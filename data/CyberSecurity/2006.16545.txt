TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

1

Adversarial Deep Ensemble: Evasion Attacks and
Defenses for Malware Detection

Deqiang Li and Qianmu Li

0
2
0
2

n
u
J

0
3

]

R
C
.
s
c
[

1
v
5
4
5
6
1
.
6
0
0
2
:
v
i
X
r
a

Abstract—Malware remains a big threat to cyber security,
calling for machine learning based malware detection. While
promising, such detectors are known to be vulnerable to evasion
attacks. Ensemble learning typically facilitates countermeasures,
while attackers can leverage this technique to improve attack
effectiveness as well. This motivates us to investigate which
kind of robustness the ensemble defense or effectiveness the
ensemble attack can achieve, particularly when they combat with
each other. We thus propose a new attack approach, named
mixture of attacks, by rendering attackers capable of multiple
generative methods and multiple manipulation sets, to perturb
a malware example without ruining its malicious functionality.
This naturally leads to a new instantiation of adversarial training,
which is further geared to enhancing the ensemble of deep
neural networks. We evaluate defenses using Android malware
detectors against 26 different attacks upon two practical datasets.
Experimental results show that the new adversarial training
signiﬁcantly enhances the robustness of deep neural networks
against a wide range of attacks, ensemble methods promote
the robustness when base classiﬁers are robust enough, and
yet ensemble attacks can evade the enhanced malware detectors
effectively, even notably downgrading the VirusTotal service.

Index Terms—Adversarial Machine Learning, Deep Neural

Networks, Ensemble, Adversarial Malware Detection.

I. INTRODUCTION

M ALWARE gains due attention from communities while

still being a big threat to cyber security. For example,
Symantec reported that 246,002,762 new malware variants
emerged in 2018 [1]. Kaspersky detected 5,321,142 malicious
Android packages in 2018 [2]. Worse yet, there is an increas-
ing number of malware variants that attempted to undermine
anti-virus tools and indeed evaded many malware detection
systems [3].

In order to relieve the severe situation, communities have
restored to machine learning techniques [4], [5]. While obtain-
ing impressive performance, the learning-based models can be
evaded by adversarial examples (see, for example, [6], [7],
[8], [9]). Interestingly, a few manipulations are enough to
perturb a malware example into an adversarial one, by which
the perturbed malware example is detected as benign rather
than malicious [10], [11]. This facilitates the research in the
context of Adversarial Malware Detection (AMD), catering
robust malware detectors against attacks.

Researchers have proposed enhancing the robustness of
classiﬁers using ensemble methods such as adversarial training

D. Li and Q. Li are with the School of Computer Science and Engineering,
Nanjing University of Science and Technology, Nanjing 210094, China
e-mail: {lideqiang,qianmu}@njust.edu.cn.

More information can be found at http://ieeexplore.ieee.org.

of ensemble [12] or ensemble adversarial training [13]. For
the counterpart, attackers can leverage ensemble methods to
promotes attack effectiveness as well such as by evading
several classiﬁers [14], [15] or by waging multiple attacks
simultaneously [16], [17]. This naturally raises the question –
how is the effectiveness of ensemble attack or the robustness
of ensemble defense when they combat with each other.
Our contributions. In this paper, we make the following
contributions. First, we propose a new attack approach, named
mixture of attacks, which enables attackers to leverage multiple
generative methods and multiple manipulation sets to produce
adversarial malware examples. To realize it, we adapt “max”
attack [16], into the AMD context, and further propose iter-
ating “max” attack in a greedy manner, so as to boost attack
effectiveness. In addition, we adapt salt and pepper noises
attack and pointwise attack [18], both of which are gradient-
free, aiming to wage effective attacks when gradients of loss
function suffer from certain issues [19].

Second, we instantiate the adversarial training [7] using
a mixture of attacks and propose applying a manipulation set
with the cardinality as large as possible. Further, we utilize this
instantiation to harden the ensemble of deep neural networks,
along with a theoretical analysis.

Third, we validate the robustness of malware detectors
against 26 evasion attacks, which are categorized into ﬁve
approaches: gradient-based, gradient-free, obfuscation, mix-
ture of attacks, and transfer attack. Speciﬁcally, there are 10
gradient-based attacks: Projected Gradient Descent (PGD)-ℓ1
[20], PGD-ℓ2 [20], PGD-ℓ∞ [21], Grosse [22], bit gradient
ascent [7], bit coordinate ascent [7], PGD-Adam [23], Gra-
dient Descent with Kernel Density Estimation (GDKDE) [6],
Fast Gradient Sign Method (FGSM) [24], and jacobian-based
saliency map attack [25]; 4 gradient-free attacks: 2 Mimicry
attacks, salt and pepper noises, and pointwise; 5 obfuscation
attacks: Java reﬂection, string encryption, variable renaming,
junk code injection, and all four techniques above combined;
3 mixtures of attacks: “max” PGDs, iterative “max” PGDs,
and iterative “max” PGDs+GDKDE, where PGDs means the
mixture contains PGD-ℓ1, PGD-ℓ2, PGD-ℓ∞, and PGD-Adam;
4 transfer attacks. We implement 6 Android malware detectors,
including Basic DNN with no efforts to harden the model, 3
hardened DNNs incorporating adversarial training with attack
rFGSM (dubbed AT-rFGSM)[7], PGD-Adam (dubbed AT-
Adam) [23], and “max” PGDs, and 2 adversarial deep en-
sembles (i.e., the hardened ensemble of deep neural networks
incorporating adversarial training). We conduct systematical

1556-6013 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.

 
 
 
 
 
 
TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

2

experiments on Drebin [26] and Androzoo [27] datasets,
centering at the aforementioned question. Our ﬁndings are that:

• The hardened models incorporating “max” PGDs can
detect more malware examples than the Basic DNN, AT-
rFGSM, and AT-Adam at the cost of lowering detection
accuracy on benign examples, and thus F1 score in the ab-
sence of attacks. Adversarial deep ensemble cannot serve
as a remedy, and even reduces the detection accuracy on
both malicious and benign examples.

• The hardened models incorporating “max” PGDs signif-
icantly outperform the Basic DNN, AT-rFGSM, and AT-
Adam against evasion attacks. However, all models can-
not defeat two types of attacks: attack mimicking benign
examples (e.g., GDKDE and Mimicry), and mixture of
attacks (e.g., iterative “max” PGDs).

• The ensemble promotes robustness against attacks when
base classiﬁers are robust enough. The usefulness of
ensemble, however, is un-deterministic when base models
are vulnerable to attacks. Interestingly, the experimental
results conﬁrm our theoretical analysis.

• VirusTotal service [28] notably suffers from the iterative
“max” PGDs+GDKDE attack. This is important because
it shows adversarial evasion attacks may be a practical
threat to cyber security.

• When attributing the important features for adversarial
deep ensemble, we observe that sub-effective features
(e.g., com.google.ads.AdActivity) are emphasized,
thus leading to its robustness against attacks while trading
off accuracy in non-adversarial settings. This implies the
necessity of robust feature extraction.

Last but not the least, we make our codes publicly available

at https://github.com/deqangss/adv-dnn-ens-malware.
Paper outline. The rest of the paper is organized as follows.
Section II reviews the related work. Section III presents the
ensemble of deep neural networks, evasion attacks (including
two attacks adapted into AMD ﬁrst time), and adversarial
training. Section IV describes the mixture of attacks and
adversarial deep ensemble. Section V evaluates attacks and
defenses. Section VI discusses certain issues we concern.
Section VII concludes the paper and shows future research.

II. RELATED WORK

We review ensemble methods from the attacker’s and the

defender’s perspectives, respectively.

A. Ensemble Attacks

There are two ensemble-based approaches improving the
effectiveness of adversarial examples: (i) by attacking multiple
classiﬁers and (ii) by using multiple attack methods.

For type (i), Liu et al. [14] suggest improving the transfer-
ability of adversarial examples by attacking an ensemble of
deep learning models rather than a single one. This is because
adversarial examples that can fool multiple models tend to
have strong transferability [14], [15]. Dong et al. [29] further

investigate three manners of organizing the base models and
demonstrate that the ensemble of averaging logits outperforms
the others for boosting the attack effectiveness.

For type (ii), Araujo et al. [17] show the difference between
ℓ2 and ℓ∞ norm-based adversarial examples geometrically.
Tram`er et al. [16] further propose attacking a classiﬁer using
multiple types of manipulations (e.g., constrained by ℓ1 or ℓ∞
norm). The empirical results show that the “max” attack lets
the attacker evade the victim effectively.

In the context of AMD, one classical means is to perturb
the discriminative features derived by random forest [30].
Recently, Al-Dujaili et al. [7] demonstrate the difference
between four attacks in a sense that deep neural network based
malware detectors enhanced by training with one attack cannot
resist the other attacks.

We apply the aforementioned two approaches together with
accommodating: (i) the discrete input domain and (ii) the
constraint of retaining malicious functionality. We exploit
the “max” strategy by permitting attackers to have multiple
generative methods and multiple manipulation sets, which is
different from the study [16] that focuses on the types of
manipulations.

B. Ensemble Defenses

Biggio et al. [31], [32] propose defending against evasion
attacks using bagging and random subspace techniques, which
can produce evenly-distributed weights. One limitation is that
the base classiﬁer is restricted to the linear algorithm. For
deep learning models, Abbasi et al. [33] propose “specialists
+ 1” ensemble, and Xu et al. [34] combine multiple feature
squeezing techniques, so as to detect adversarial examples ef-
fectively. Both defenses, however, are defeated by a following
study [35], which demonstrates that the ensemble of weak
defenses cannot mitigate evasion attacks. Tram`er et al. [13]
further introduce ensemble adversarial training, which learns
one robust model by augmenting training data with adversarial
examples transferred from multiple models. Grefenstette et
al. [12] empirically demonstrate that adversarial training of
ensemble achieves better robustness than the ensemble of ad-
versarially trained multiple classiﬁers. Pang et al. [36] suggest
enhancing the ensemble by diversifying base classiﬁers.

In the context of malware detection, Smutz and Stavrou [37]
propose leveraging ensemble classiﬁer to detect adversarial
examples that are treated as outliers. Stokes et al. [38] show the
resilience of ensemble to evasion attacks. Moreover, stacking
ensemble is utilized to hinder adversarial malware examples
[39]. All these defenses neglect the adversarial training that is
an effective means to resist evasion attacks.

As a comparison, we aim to circumvent a wide range
of evasion attacks in the AMD context. We also enhance
the ensemble model by adversarial training (i.e., adversarial
training of ensemble) [12], while incorporating a mixture of
attacks, along with a theoretical analysis of ensemble.

III. PRELIMINARIES

We present

the ensemble of deep neural networks,

the

evasion attacks, and the framework of adversarial training.

TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

3

A. Ensemble of Deep Neural Networks

Given a malware example z in the ﬁle space Z, i.e., z ∈
Z, mapped as a d dimensional feature vector (i.e., feature
representation) x ∈ X by feature extraction method φ : Z →
X , an ensemble classiﬁer f : X → Y takes x as input and
returns the predicted label 0 or 1, i.e., Y = {0, 1}, where ‘0’
means benign ﬁle and ‘1’ means malicious. We consider the
deep ensemble that is a linear combination of l Deep Neural
i=1. Let w denote a weight vector
Networks (DNNs) {Fi}l
w = (w1, w2, · · · , wl), satisfying w ∈ W with W = {w :
1⊤ · w = 1, wi ≥ 0} for i = 1, . . . , l. For waging effective
attacks, we apply logit ensemble [29], which is:

F(x) = softmax(

wiZi(x)),

s.t. w ∈ W

(1)

l

i=1
X

where Zi denotes the logits of Fi. Further, a stable version
would be implemented to transform Zi to Zi −max(Zi), so as
to avoid numerical overﬂow, where max returns the maximum
element in a vector. We obtain the predicted label by f (x) =
F(x), where arg max returns the index of maximum
arg maxj
element in a vector.

The parameters of ensemble, collectively denoted by θ, are
optimized by minimizing a cost over the joint feature vector
and label space, namely:

E

(x,y)∈X ×Y

L(F(x), y)

,

(2)

min
θ

where L : R|Y| ×Y is a loss function (e.g., cross entropy [40]).
For simplifying notations, we use F(·) (rather than F(θ; ·)) to
denote a parameterized ensemble of deep neural networks.

(cid:2)

(cid:3)

B. Evasion Attacks

1) Deﬁnition: We focus on evasion attacks in the context
of AMD, which generally conﬁnes attackers by: perturbing
malware examples in the test phase (rather than training phase)
and retaining malicious functionality [7], [11], [41]. The
terminology of adversarial malware example and adversarial
example are used interchangeably, referring to the perturbed
example that can evade the victim successfully.

Two versions of evasion attacks are used, corresponding to
the ﬁle space Z and the feature space X , respectively. In the
ﬁle space, the attacker perturbs the malware example z into
z′ in order to mislead the classiﬁer f [42]. Formally, given
malware example-label pair (z, y) and manipulation set Mz,
the attacker intents to achieve:

z′ = z ⊕ δ,

(3)

s.t. (δ ∈ Mz)∧(f (φ(z′)) 6= y) ∧ (f (φ(z)) = y)

where ⊕ refers to the operation of applying manipulations,
and manipulations in Mz retain the functionality of z (e.g,
junk codes injection).

Instead of perturbing executable malware examples directly,
an attacker could derive manipulations in the feature space
X , guiding the generation of adversarial examples in the ﬁle
space [43], [44]. Let Mx denote representation manipulations
for x = φ(z), which is derived by

Mx = {(φ(z′) − φ(z)) : z′ = z ⊕ δ} for ∀ δ ∈ Mz.

(4)

Formally, given a tuple (x, y), manipulations in the feature
space Mx, and the inverse feature extraction φ−1, the attacker
intents to achieve:

z′ = φ−1(x′),
(5)
s.t. (x′ = x + δx) ∧ (δx ∈ Mx) ∧ (f (x′) 6= y) ∧ (f (x) = y).

Notably, feature space evasion attack necessitates following
two assumptions. Assumption 1 below says the inverse feature
extraction φ−1 is solvable, and otherwise the corresponding
adversarial example only exists in theory.

Assumption 1 (solvability assumption [43]). Given z ∈ Z
and feature extraction φ, the attacker can obtain z′ = φ−1(x′)
when the representation x′ is perturbed from x for x = φ(z).
Eq.(5) shows φ−1 works upon the manipulation set Mx
which is derived by Eq.(4). It is a brute-force solution by
calculating all perturbed ﬁles in advance, incurring efﬁciency
issues. Researchers thus suggest alternatives [43], [44], [45],
[46]. One example is predetermining Mx empirically based on
the ﬁle space manipulation set Mz. This would rely on the
below assumption that the perturbed representation is bounded
by a box constraint [44]. Let ˇu and ˆu respectively denote the
lower and upper boundaries in the feature space (i.e., elements
in ˇu are not greater than corresponding elements in ˆu).
Assumption 2 (manipulation assumption [44]). x′ perturbed
from x satisﬁes x′ ∈ [ˇu, ˆu] for ∀ x ∈ X .

With regard to Assumption 2 (i.e., Mx ⊂ [ˇu − x, ˆu − x]),
we cannot modify the ﬁle z by following x′ − x partic-
ularly. One reason is features may be interdependent (i.e.,
modifying a representation value triggers other correlated ones
changed), resulting in dependent manipulations [43], [46].
However, Assumption 2 misses to capture this dependence.
In this work, we implement φ−1(x′) subject
to retaining
malicious functionality, which may break the intuition of
φ(φ−1(x′ − x)) = x′ − x. Our preliminary experiments show
that the attack effectiveness barely suffers from this side-effect
for most of the used features are independent1 (see Section
V-A3).

2) Threat Models: We consider a threat model speciﬁed
by attacker’s capability of perturbing examples and attacker’s
knowledge of the target system [6], [43], [45], [41], [47].
Attacker capability. As aforementioned earlier, an attacker is
capable of perturbing malware examples in the test phase with
malicious functionality preservation. In addition, the attacker
is permitted to generate an example that is miss-classiﬁed with
high cost (e.g., maximizing classiﬁer’s loss) [43], [7].
Attacker knowledge. There are three attack scenarios: white-
box vs. black-box vs. grey-box. In the white-box scenario, the
attacker knows everything of the targeted system, including
defense mechanisms; in the black-box scenario, the attacker
does not know internals of the victim, except for the predicted
the grey-box scenario resides in between, and we
labels;

1This observation is application-special. To accommodate various types of
feature extraction, we need to bridge the gap between the feature space attack
and the ﬁle space attack, avoiding the use of empirical Mx. We leave this
problem in further work.

TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

4

permit the attacker to know the dataset, the feature extraction
method, and the learning algorithm, but not defense methods
and learned parameters of the targeted malware detector.

3) Attack Strategies in the Literature: We consider a broad
range of methods to specify evasion attacks, which are cate-
gorized into four approaches: gradient-based attacks, gradient-
free attacks, obfuscations, and transfer attacks.

Gradient-based attacks. This approach permits attackers
to wage white-box attacks, deriving adversarial examples
using gradients of classiﬁer’s loss function. For example,
ℓp (p = 1, 2, ∞) norm based Projected Gradient Descent
(PGD) is an effective means to maximizing the classiﬁer’s loss
appropriately [21]. Note that the perturbation δx is continuous
during optimization process when following the direction of
gradients. We map the perturbed representation by looking for
the feasible nearest neighbor [20]. In addition, several related
methods are adapted or proposed in the context of AMD, in-
cluding Grosse [22], Fast Gradient Sign Method (FGSM) [24],
[7], Jacobian-based Saliency Map Attack (JSMA) [48], [25],
Bit Gradient Ascent (BGA) [7], Bit Coordinate Ascent (BCA)
[7] and PGD-Adam [23], while noting that Grosse, JSMA,
BGA, and BCA permit the feature addition only. Moreover,
another method, named Gradient Descent with Kernel Density
Estimation (GDKDE) [6], lifts perturbed examples into the
populated region of benign ones.

Gradient-free attacks. This approach permits attackers to
wage grey-box attacks. The Mimicry is usually applied, known
as perturbing a malware example to mimic the benign ones
[43], [45]. We further adapt two methods (both are originally
proposed in the context of image processing) to modify
malware examples: salt and pepper noises and pointwise [18].
Algorithm 1 shows the former one in the feature space,
which perturbs representations using salt and pepper noises
repetitively. Algorithm 2 describes the pointwise that, given
an adversarial example, reduces its degree of manipulations
while keeping its adversarial property.

Obfuscations. This approach enables attackers to wage ﬁle
space evasion attacks with knowing nothing about the victim
model (i.e., zero-query black-box attack). Typically, software
sample can be manipulated using certain techniques (e.g.,
variable renaming). Indeed, researchers have reported the
obfuscated malware examples can bypass detection [49], [45].
This inspires us to investigate this attack approach.

Transfer attacks. This approach suggests attackers waging
transfer attack when knowing certain knowledge of f , such
as a portion of features [43], [44]. The procedure mainly
has following steps: perform reverse-engineering to obtain
a surrogate model which resembles the targeted model f ;
perturb malware examples against the surrogate model; target
the classiﬁer f using the perturbed examples.

C. Minmax Adversarial Training

Adversarial training lets classiﬁers know certain attacks
proactively by augmenting the training data with adversarial
examples [50], [24], [51], [52]. In particular, it has been
proposed to consider adversarial training with the optimal

Algorithm 1: Salt and pepper noises attack in the feature
space.
Input: The feature representation-label pair (x, y); the
classiﬁer f ; the manipulation set Mx; a scalar
0 ≤ ǫmax ≤ 1; the number of scalars Ns; the
number of repetitions Nrept.

Output: The perturbed point x∗.

1 Initialize x∗ ← x;
2 repeat
3

Produce evenly spaced scalars ǫ1, . . . , ǫNs over the
range of [0, ǫmax];
for j = 1 to Ns do

Generate salt and pepper noises δx with the
maximum degree of manipulation ǫj ∗ d and
δx ∈ Mx;
Set x′ ← x + δx;
if f (x′) 6= y then

Set x∗ ← x′ and ǫmax ← ǫj;
break;

end

10
11 until Nrept is reached;
12 return x∗.

Algorithm 2: Pointwise attack in the feature space.
Input: The feature representation-label pair (x, y); the
classiﬁer f ; the adversarial representation
x∗ = (x∗
set XM.

d); the perturbed representation

1, . . . , x∗

Output: The perturbed point ˇx∗.

1 repeat
2

Set ˇx∗ ← x∗;
Shufﬂe the list of indices [d] = h1, · · · , di to another
list [(d)] = h(1), · · · , (d)i randomly;
for i = 0 to d do

if x∗

(i) = x(i) then
continue;

Modify x∗ locally by setting x∗
if (x∗ /∈ XM) ∨ (f (x∗) = y) then

(i) ← x(i);

reset x∗

(i) ← ˇx∗

(i);

4

5

6

7

8

9

3

4

5

6

7

8

9

end
10
11 until ˇx∗ = x∗;
12 return ˇx∗.

attack, which in a sense corresponds to the worst-case scenario
and therefore leads to classiﬁers that are robust against the
non-optimal ones, namely the minmax adversarial training [7]:

E

(x,y)∈X ×Y

min
θ

L(F(θ; x), y) + max
x′∈XM

L(F(θ; x′), y)

,

(cid:18)

(cid:19)
(6)
where θ denotes the parameters of a DNN F and XM ⊆ [ˇu, ˆu]
denotes a predetermined set of perturbed representations. It is
worthy reminding that the inner maximization is intractable
because of the non-convex DNN, resulting in local maxima.

TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

5

IV. METHODOLOGY

We elaborate the mixture of attacks and adversarial deep
ensemble, of which the adversarial deep ensemble relies on a
mixture of attacks.

A. Mixture of Attacks

We propose mixture of attacks, an ensemble based approach,
permitting attackers to perturb a malware example via multiple
attack methods and multiple manipulation sets. Though this
setting gives attackers more freedom, it is practical in the
context of AMD. For instance, researchers suggest perturbing
Android Packages by adding instructions into the ﬁle of
AndroidManifest.xml [11], while addition operation can be
applied to the ﬁle of classes.dex as well [25] and moreover,
certain objects (e.g., string) can be hidden [45].

1) Overall Idea: With regard to Assumption 1 and 2 (which
are empirically handled in Section V), we consider feature
space evasion attacks. Let H denote the space of generative
method and ∆x denote the space of manipulation set such that

Deﬁnition 1 (generative method). A generative method h ∈ H
takes as input the representation x with a constraint Mx ∈
∆x, and returns a perturbed representation x′ = h(Mx; x).
We characterize the “strength” of an attack upon h and Mx
via some scoring measurements, wherein the classiﬁer loss
L(F(h(Mx; x)), y) is leveraged for a given (x, y) tuple. The
higher loss value indicates a stronger attack.

The mixture of attacks has the same objective as the
aforementioned approaches (e.g., gradient-based), aiming to
maximize a score when manipulating malware examples. In
contrast to attack strategies that design a generative method
upon a manipulation set, a mixture of attacks attempts to
construct n generative methods {hi}n
i=1 and m manipulation
sets {Mj
j=1, and then combine them to wage an attack,
where n ≥ 1 and m ≥ 1.

x}m

2) Two Attack Strategies: In order to realize the mixture of

attacks, we apply two straightforward strategies as follows:

• “Max” strategy: Given a representation-label pair (x, y),
i=1, and m manipulation sets
j=1, the attacker attempts to choose a generative
Mx, both of

n generative methods {hi}n
{Mj
x}m
method, say
h, and a manipulation set, say
which joint to produce the optimal attack, namely that

Algorithm 3: Iterative “max” attack in the feature space.
Input: The feature representation-label pair (x, y); n

i=1; m manipulation sets
j=1; the score measurement L; the number

generation methods {hi}n
{Mj
of iterations N ; a small constant ε > 0.

x}m

Output: The perturbed point x′.

0 ← x;
1 Initialize x′
2 for k = 1 to N do

3

4

5

6

7

8

9

10

11

for i = 1 to n do

for j = 1 to m do

Calculate hi(Mj

x; x′

k−1);

end

end
Mx via Eq.(7);
Select
h and
Set x′ ←
Mx; x);
h(
e
f
Set x′
k ← x′;
e
f
k, y) − L(x′
if |L(x′
return x′;

k−1, y)| < ε then

12
13 end
14 return x′.

proceed with taking as input the resulting point calculated by
“max” attack, as shown in line 10. The procedure halts until
the predetermined number of iteration is reached (line 2) or the
convergence criterion is met (line 11, the change of score is
less than a small scalar such as ε = 10−9). The iterative case
improves the attack effectiveness greedily, resulting in more
directions being explored. In the future, one may consider
more strategies to improve the “max” attack.

B. Adversarial Deep Ensemble

We enhance the robustness of deep ensemble by adversarial
training technique incorporating the “max” attack. In the end,
we instantiate the minmax training (see Eq.6) as:

E

(x,y)∈X ×Y

min
θ

L(F(x), y) + max
h;Mx

(cid:18)

L(F(h(Mx; x)), y)

,

(cid:19)
(8)

s.t. h ∈ H ∧ Mx ∈ ∆x.

e
Mx = arg max
h;

L(F(h(Mx; x)), y)

f

s.t., h ∈ {hi}n
f

e

h;Mx
i=1 ∧ Mx ∈ {Mj

x}m

j=1

(7)

Because of the information barrier between the defender and
the attacker, we shall use the notations H and ∆x to replace
attacker’s empirical set {hi}n
j=1, respectively.
In contrast to the former study [7], there are three differences:

i=1 and {Mj

x}m

• Iterative “max” strategy: The attacker performs the “max”
strategy iteratively with each round adding perturbations
on the resulting example came from the last iteration (in
the ﬁrst iteration, perturbations are applied on x).

Algorithm 3 uniﬁes and summarizes the two strategies. We
calculate perturbed examples using two loops corresponding to
generative methods {hi}n
j=1
(line 3 - line 7). For line 8 and line 9, we select the optimal
combination of attack method and manipulation set to perturb
an example. Herein, parts of the algorithm (from line 3 to line
9) belongs to the “max” attack. The iterative case continues to

i=1 and manipulation sets {Mj

x}m

• H contains multiple approximate maximizers.
• ∆x contains a huge number of manipulation sets.
• F is a deep ensemble (rather than a single DNN).
First, we may apply all possibly approximate maximizers
to enhance the malware detectors, for the aim of defending
against a wide range of attacks. Owing to the efﬁciency issue,
we choose and use the gradient-based maximizers.

Second, let ˇMx and ˆMx denote two manipulation sets. The
theorem below suggests producing adversarial examples upon
the union of all manipulation sets, namely Mx =

∆x.

S

TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

6

Theorem 1. Upon two manipulation sets ˇMx and ˆMx, given
a representation-label pair (x, y), the generative method h
perturbs x by maximizing the classiﬁer loss L, which leads to
L(F(h( ˇMx; x)), y) ≤ L(F(h( ˆMx; x)), y) when ˇMx ⊆ ˆMx.
Theorem 1 can be easily proved. Given ˇMx ⊆ ˆMx, we
derive X ˇM = {x + ˇδx : ˇδx ∈ ˇMx} and X ˆM = {x + ˆδx : ˆδx ∈
ˆMx}, and further get X ˇM ⊆ X ˆM. Due to h( ˇMx; x) ∈ X ˇM,
we reason h( ˇMx; x) ∈ X ˆM and obtain L(F(h( ˇMx; x)), y) ≤
L(F(h( ˆMx; x)), y). Recalling the Assumption 2 (i.e., box
constraint), we apply Mx =
∆x. Rigorously, the theorem
works when the generative method is the exact solution to
S
maximizing the loss L, which is an intricate problem (see
discussion in Section III-C). Nevertheless, our preliminary
experiments show that the projected gradient descent based
maximizers follow this theorem well.

Third, we design a robust ensemble as below.
1) Base Classiﬁers: The generalization error of ensemble
drops signiﬁcantly when base classiﬁers are effective and in-
dependent [53]. We consider diversifying base classiﬁers using
the approach of data sample manipulation [54]. Speciﬁcally,
we have each base classiﬁer perceive adversarial examples that
are produced by an approximate maximizer. This means we
regularize the base classiﬁers using adversarial training but
incorporating distinct attacks. Formally, the objective, denoted
by Jens, is

l

Jens = J + γ

Ji,

(9)

where J denotes the Eq.(8), γ is a factor to balance the two
items, and Ji is the regularization for ith base classiﬁer:

i=1
X

M for ErrorF(X ∗

between F∗ and F. Without confusion, we drop x∗ for ηZ,x∗
and X ∗
M), leading to the compactness ηZ and
ErrorF. Instead of the logits error, one may consider others
(e.g., error upon softmax). Without loss of generalization, we
herein aim to accommodate the logit ensemble, but the result
obtained below can be extended to other ensembles.

We additionally make a hypothesis of F’s base classiﬁers
being non-negatively correlated, i.e., E(ηZi )E(ηZj ) ≥ 0 for
i, j ∈ {1, . . . , l} and i 6= j, in a sense that all DNNs are
learned from the same dataset and to solve similar tasks (which
is validated in Section VI). Let E(ˇηZ)2 denote the smallest
error achieved by one of base DNNs. We present the following
theorem:

Theorem 2. Given the deep ensemble F with E(ηZi )E(ηZj ) ≥
0 for i, j ∈ {1, . . . , l} and i 6= j, the error of the ensemble
satisﬁes ErrorF ≥ (E(ˇηZ)2/l).

Proof. We derive:

ErrorF = E(ηZ)2 = E

(wiηZi)2

l

i=1
X

l

l

!

l

=

i=1
X

w2
i

E(ηZi )2 +

wiwjE(ηZi )E(ηZj )

.

(12)

i=1
X

j6=i
X

(cid:0)

(cid:1)

The optimal weights for the problem

min
w∈W

l

i=1
X

w2
i

E(ηZi )2

Ji = min

θi

E
(x,y)∈X ×YL(Fi(θi; hi(Mx; x)), y).

(10)

is

Here θi (i = 1, . . . , l − 1) denotes the parameters of base
DNN Fi, Mx =
∆x, and hi is an approximate maximizer.
In order to accommodate the unperturbed examples, the lth
base classiﬁer is learned from the pristine training set.

S

2) Combination: We optimize weights w by the Eq.(8),
when given the DNNs F1, F2, · · · , Fl with frozen parameters.
The optimal weights can be solved by Lagrange multiplier
[55], [56]. In order to make the optimization compatible to
that of DNN, we leverage projected gradient descent:

wi+1 = ProjW

wi − β · ▽wJ(F)

,

(11)

(cid:0)

V

(cid:1)

where β > 0 is the learning rate and ProjW projects V into
the space W. We use the algorithm of Duchi et al. [56] to
conduct the projection.

{z

|

}

3) Analysis: We present a theoretical analysis of the deep
ensemble against evasion attacks. Speciﬁcally, we quantify the
robustness, by comparing to an ideal DNN, using a relaxation
of averaging mean square error over logits. Formally, given an
M ⊆ XM, an ideal DNN F∗ and a
adversarial example set X ∗
learned DNN F, the error is deﬁned as
M (Z∗(x∗

M) = Ex∗∈X ∗

ErrorF(X ∗

i ) − Z(x∗

i ))2 ,

where Z∗ and Z denote the logits of DNN F∗ and F,
respectively. Let ηZ,x∗ = Z∗(x∗) − Z(x∗) denote the offset

l

wi = (

i=1
X

1
E(ηZi )2 )−1

1

E(ηZi )2 (i = 1 · · · , l).

(13)

Substituting Eq. (13) into Eq. (12), we obtain:

l

ErrorF ≥

w2
i

E(ηZi )2 ≥

i=1
X
This leads the theorem follows.

1

l
i=1

1
E(ηZ

i )2

≥

E(ˇηZ)2
l

.

P

From Theorem 2, we draw:

Insight 1. The error of ensemble ErrorF could be smaller
than the best base DNN,
than
any individual classiﬁer that is enclosed into the ensemble;
The error of ensemble could be arbitrarily large when base
classiﬁers cannot resist the evasion attacks.

thus showing more robust

V. EXPERIMENTS AND EVALUATION

A. Experimental Setup

In this section, we describe data pre-processing, classiﬁers
(i.e., defenses) training, and manipulations in ﬁle and feature
spaces.

 
TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

7

1) Data Pre-Processing: We validate the effectiveness of
attacks and defenses using Android malware detectors on
Drebin [26] and Androzoo [27] datasets. The Drebin dataset
[26] contains 5,615 malicious Android packages and SHA256
values of 123,453 benign examples. Based on the given
SHA256 values, we downloaded 42,333 benign APKs from the
APK markets, including 29,252 applications from GooglePlay
store, 7,552 applications from AppChina, and 5,529 from other
resources such as Anzhi. Androzoo [27] is an APK repository.
To obtain the recent ﬁles, we downloaded 134,976 APKs that
have the attached date from July 1st to December 31st in
2017. We sent all APKs to the VirusTotal service, which is
an ensemble of over 70 anti-virus scanners (e.g., Kaspersky,
McAfee, FireEye, Comodo, etc.). Based on the feedback, we
obtain 15,467 malware examples and 91,295 benign examples.
An APK is labeled as malicious if there are at least ﬁve anti-
virus scanners say it is malicious, and is labeled as benign if
no scanners detect it. We split both datasets respectively into
three disjoint sets for training (60%), validation (20%), and
testing (20%).

Feature extraction. APK is a zip ﬁle which contains An-
droidManifest.xml, classes.dex, and others (e.g., res). The
AndroidManifest.xml describes an APK’s information, such
as the package name and permission announcement. The
functionality is built into classes.dex which is understandable
by Java Virtual Machine (JVM).

Following prior adversarial learning studies [26], [45], [22],
[25], we use the Drebin features, which consist of 8 subsets
of features, including 4 subsets of features extracted from
AndroidManifest.xml (denoted by S1, S2, S3, S4, respectively)
and 4 subsets of features extracted from the disassembled dex-
code (denoted by S5, S6, S7, S8, respectively). More speciﬁ-
cally, (i) S1 contains the features that are related to the access
of an APK to the hardware of a smartphone (e.g., camera,
touchscreen, or GPS module); (ii) S2 contains the features
that are related to the permissions requested by the APK
in question; (iii) S3 contains the features that are related to
the application components (e.g., activities, service, receivers,
etc.); (iv) S4 contains the features that are related to the APK’s
communications with the operating system; (v) S5 contains
the features that are related to the critical system API calls,
which cannot run without appropriate permissions or the root
privilege; (vi) S6 contains the features that correspond to the
used permissions; (vii) S7 contains the features that are related
to the API calls that can access sensitive data or resources on
a smartphone; (viii) S8 contains the features that are related to
IP addresses, hostnames and URLs found in the disassembled
code.

In order to extract features of the applications, we utilize the
Androgurad 3.3.5, which is a static APK analysis toolkit[57].
Note that 141 APKs in Drebin dataset cannot be analyzed.
Moreover, a feature selection is conducted to remove those
low-frequency features for the sake of computational efﬁciency
[45]. As a result, we keep 10,000 features at top frequencies.
The APK is mapped into the feature space as a binary feature
vector, where ‘1’ (‘0’) corresponds to a feature represent the
presence (absence) in an APK.

2) Training Classiﬁers: We train six classiﬁers: (i) the
basic DNN with no effort to enhance the model (dubbed
Basic DNN); (ii) enhanced DNN incorporating Adversarial
Training with the inner maximizer solved by iterative FGSM
using randomized “rounding” (dubbed AT-rFGSM) [7]; (iii)
enhanced DNN incorporating Adversarial Training with the
inner maximizer solved by Adam optimizer (dubbed AT-
Adam) [23]; (iv) enhanced DNN incorporating Adversarial
Training with the inner maximizer solved by a Mixture of
Attacks (dubbed AT-MA); (v) Adversarial Deep Ensemble with
the inner maximizer solved by the Mixture of Attacks (see
Eq.(8), dubbed ADE-MA); (vi) the ADE-MA with diversity
promoted (see Eq.(9), dubbed dADE-MA).

Hyper-parameter settings. We use DNNs with two fully-
connected hidden layers (each layer having neurons 160) and
the ReLU activation function. For outer minimization (see
Eq.(8)), all classiﬁers are optimized by using Adam with
epochs 150, mini-batch size 128, and learning rate 0.001.
For the inner maximization, the iterative FGSM is imple-
mented as ℓ∞ norm based PGD attack with step size 0.01
and iterations 100. The Adam-based maximizer is set up
with the step size 0.02, iterations 100, and random starting
points [23]. The mixture of attacks is realized as the “max”
attack, which has four maximizers: ℓ1 norm based PGD attack
with step size 1.0 and iterations 50, ℓ2 norm based PGD
attack with step size 1.0 and iterations 100, ℓ∞ norm based
PGD attack with aforementioned settings, and Adam based
PGD attack with aforementioned settings. Notably, dADE-MA
has an extra hyper-parameter γ. We experimentally justify
this hyper-parameter and choose γ = 1 on Drebin dataset
and γ = 0.1 on Androzoo dataset. All attacks perturb the
feature representations upon a manipulation set Mx (will be
elaborated later).

Model selection. We learn the classiﬁers using Drebin and
Androzoo training datasets, respectively. A selected model is
the one that obtains the best “accuracy” on the validation
set. The “accuracy” is the percentage of examples being
classiﬁed correctly. For adversarial training, an additional term
is considered, which is the accuracy of correctly classifying
adversarial examples produced by the corresponding inner
maximizers. The selected model is used for evaluation.

3) Specifying Manipulations: We specify manipulations
applied to Android applications and estimate the perturbations
for Drebin [26] feature representations accordingly.

Manipulation set Mz in the ﬁle space. Given an APK,
we consider both incremental and decremental manipulations.
For incremental manipulations, the attacker can insert some
manifest features (e.g., request extra permissions and hard-
ware, state additional activities, services, Intent-ﬁlter, etc.).
However, some objects are hard to insert, such as content-
provider, because the absence of Uniform Resource Identiﬁer
(URI) will corrupt an APK. With respect to the .dex ﬁle,
junk codes (e.g., null OpCode, debugging information, dead
functions or classes) can be injected without destroying the
APK example. The similar means can be performed for the
string (e.g., IP address) injection, as well.

TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

8

the
When the attacker uses decremental manipulations,
APK’s information in the xml ﬁles can be changed (e.g.,
package name). However, it is impossible to remove activity
entirely because an activity may represent a class implemented
in the classes.dex code. Nevertheless, we can rename an ac-
tivity and change its relevant information (e.g., activity label),
while noting that the related components in the .dex should
be modiﬁed accordingly. The other components (e.g., service,
provider and receiver) also can be modiﬁed in the similar
fashion. The method names and class names that are deﬁned
by developers could be replaced by random strings, too. Note
that the corresponding statement, instantiation, reference, and
announcements should be changed accordingly. Moreover,
user-speciﬁed strings can be obfuscated using encryption and
the cipher-text will be decrypted at running time. Further,
the attacker can hide public and static system APIs using
Java reﬂection and encryption together. This is shown by the
example in List 1, which retrieves the device ID and sends the
content outside the phone via SMS. All of the modiﬁcations
mentioned above only obfuscate an APK without changing its
functionalities.

TelephonyManager telecom = // default ;
String str = telecom.getDeviceId();
String encrypt_str = "EAQMVmZdGUV/VxdAVV9T";
// plain text: sendTextMessage
String mtd_name =

DecryptString.convertToString(encrypt_str);

SmsManager smgr = SmsManager.getDefault();
Method send_sms = null;
send_sms =

smgr.getClass().getMethod(mtd_name,
String.class, String.class, String.class,
PendingIntent.class, PendingIntent.class);

send_sms.invoke(smgr, "+50 1234567", null,

str, null, null);

Listing 1: Java code snippet
“sendTextMessage”.

to hide the API method

One challenge is that the attacker needs to perform ﬁne-
grained manipulations on compiled ﬁles automatically at scale,
while preserving the functionalities of malware samples. This
is important because a small mistake in a malware example can
render the ﬁle un-executable. The preservation of malicious
functionalities may be estimated by using a dynamic malware
analysis tool, (e.g., Sandbox).
Manipulation set Mx in the feature space. The aforemen-
tioned manipulations modify static Android features such as
API calls. We observe that two kinds of perturbations can be
applied to the feature representations as follows:

• Flipping ‘0’ to ‘1’: The attacker can increase the repre-
sentation values of appropriate objects, such as compo-
nents (e.g., activity), system APIs, and IP address.

• Flipping ‘1’ to ‘0’: The attacker can ﬂip ‘1’ to ‘0’ by
removing or hiding objects (e.g., activity name, public or
static APIs.)

Table I summarizes the operations in the Drebin feature
space. We observe that the operations are not applicable to
S6 because this subset of features rely upon S2 and S5,
meaning that modiﬁcations on S2 or S5 may cause changes

Table I: Overview of manipulations in the feature space, where
X(✗) indicates that the operation of ﬂipping ‘0’ to ‘1’ or
ﬂipping ‘1’ to ‘0’ can (cannot) be performed on features in
the corresponding subset.

Feature sets (# of features)

manifest

dexcode

S1 Hardware (17)
S2 Requested permissions (247)
S3 Application components (8,619)
S4 Intents (866)

S5 Restricted API calls (118)
S6 Used permission (20)
S7 Suspicious API calls (19)
S8 Network addresses (94)

0 → 1
X
X
X
X

X
✗
X
X

1 → 0

✗
✗
X
✗

X
✗
X
X

of representation on S6. In addition, we highlight that the
manipulation set is larger than two recent studies [25], [11]
that only consider ﬂipping ‘0’ to ‘1’ in the feature space.

B. Evaluating the Effectiveness of Attacks and Defenses

We evaluate the attacks and defenses with centering at the
earlier aforementioned question that is disintegrated as four
sub-questions in the following:

• RQ1: How is the accuracy of adversarial deep ensemble
for detecting malware examples in the absence of attacks?
• RQ2: How is the robustness of adversarial deep ensemble
against a broad range of attacks, and how is the usefulness
of ensemble against attacks?

• RQ3: How is the accuracy of anti-virus scanners under

the mixture of attacks?

• RQ4: Why the enhanced classiﬁers can (cannot) defend

against certain attacks?

Metrics. The effectiveness of classiﬁers is measured by ﬁve
standard metrics: False-Positive Rate (FNR), False-Negative
Rate (FNR), Accuracy (Acc), balanced Accuracy (bAcc) [58]
and F1 score [59]. The balanced Accuracy and F1 score are
considered because of the imbalanced dataset.
RQ1: How is the accuracy of adversarial deep ensemble
for detecting malware examples in the absence of attacks?
For answering RQ1, we evaluate the above mentioned six
classiﬁers (i.e., Basic DNN, AT-rFGSM, AT-Adam, AT-MA,
ADE-MA, and dADE-MA) on Drebin and Androzoo datasets,
respectively.

Table II summarizes the results. We observe that when
compared with the Basic DNN, the adversarial training based
defenses achieve lower FNRs (at most a 2.13% decrease on
Drebin dataset and 1.42% on Androzoo) but higher FPR
(at most a 4.64% increase on Drebin dataset and 3.84% on
Androzoo). For these defenses, AT-MA achieves the lowest
FNR (1.59% on Drebin dataset and 1.32% on Androzoo)
and ADE-MA encounters the highest FPR (4.96% on Drebin
dataset and 4.32% on Androzoo). This can be explained as
follows: by injecting adversarial malware examples into the
training set, the learning process makes the model search for
malware examples in a bigger space, resulting in the drop in
FNR and increase in FPR. AT-MA, ADE-MA, and dADE-MA
attain the similar FNR and FPR in the absence of attacks, due
to the fact that the same generative methods are leveraged.

TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

9

Table II: Effectiveness of the classiﬁers when there are no
adversarial attacks.

Dataset

Classiﬁer

Effectivenss (%)

FNR

FPR

Acc

Basic DNN
AT-rFGSM [7]
AT-Adam [23]
AT-MA
ADE-MA
dADE-MA

Basic DNN
AT-rFGSM [7]
AT-Adam [23]
AT-MA
ADE-MA
dADE-MA

3.72
2.74
3.27
1.59
1.59
1.95

2.74
2.05
1.61
1.32
1.45
1.57

0.32
2.45
1.45
3.66
4.96
3.69

0.48
0.66
0.96
2.13
4.32
3.65

99.28
97.51
98.34
96.58
95.44
96.52

99.18
99.13
98.94
97.99
96.11
96.66

bAcc

97.98
97.40
97.64
97.37
96.72
97.18

98.39
98.65
98.72
98.27
97.11
97.39

F1

96.93
90.23
93.22
87.18
83.61
86.94

97.26
97.11
96.51
93.60
88.28
89.77

Drebin

Andro-
zoo

Moreover, these three defenses increase the balanced accuracy
to their achieved accuracy as more malware examples are
detected. Interestingly, both ensemble based defenses tend to
have higher FNR and FPR than AT-MA. The underlying reason
might be that adversarial deep ensemble focuses on more
perturbed examples that are outliers. In summary, we draw:

Insight 2. In the absence of attacks, the hardened models
using mixture of attacks can detect more malware examples
than the Basic DNN and the other hardened models (because
of their smaller FNR), at the price of small side-effect in the
FPR, classiﬁcation accuracy and balanced accuracy, but no-
table side-effect in F1 score; adversarial ensemble exacerbates
this situation, further lowering the effectiveness a little in terms
of all measurements.

RQ2: How is the robustness of adversarial deep ensemble
against a broad range of attacks, and how is the usefulness of
ensemble against attacks? For answering RQ2, we randomly
select 800 malware examples from the test set to wage evasion
attacks, attempting to fool the aforementioned six classiﬁers,
namely Basic DNN, AT-rFGSM, AT-Adam, AT-MA, ADE-
MA, and dADE-MA.

For gradient-based methods, in the settings of Grosse [22],
BGA [7], BCA [7], JSMA [25], and ℓ1 norm-based PGD
(dubbed PGD-ℓ1), we perturb one feature per time with the
maximum iterations 100. For GDKDE [6], PGD-Adam [23],
and ℓ∞ norm-based PGD attack (dubbed PGD-ℓ∞), we iterate
the algorithm with the maximum iterations 1,000 and step size
0.01. The ℓ2 norm-based PGD attack (dubbed PGD-ℓ2) is set
with the maximum iterations 1,000 and step size 1.0.

For gradient-free attacks, we wage salt and pepper noises
attack (dubbed Salt+Pepper) with Nrept = 10, ǫmax = 1,
and Ns = 1, 000. Moreover, let Mimicry×Nben denote a
mimicry attack, in which we use Nben benign examples to
guide the perturbation of a malware example, leading to Nben
perturbed examples; then, we select the one from these Nben
perturbed example that causes the miss-classiﬁcation with the
smallest perturbations. Pointwise takes the resultant examples
of Mimicry×Nben as input (dubbed Pointwise×Nben).

The obfuscations are implemented via the AVPASS which
is a tool to obfuscate Android applications [49]. We apply ﬁve

attacks: Java reﬂection, string encryption, variable renaming,
junk code injection, and the four techniques above combined.
In order to wage mixture of attacks, we leverage four PGD
attacks (PGD-adam, PGD-ℓ1, PGD-ℓ2, and PGD-ℓ∞), thus
denoted “max” attack as “max” PGDs. The iterative version is
denoted as I-“max” PGDs with iteration N = 5 and ε = 10−9.
Furthermore, by jointing the PGDs and GDKDE, we wage I-
“max” PGDs+GDKDE attack.

When performing transfer attacks for the targeted model, we
treat the other ﬁve classiﬁers as surrogate models individually.
This means that, given a malware example, we perturb it upon
the other ﬁve models, respectively. All the perturbed examples
will be sent to query the targeted model.

Table III reports the accuracy of classiﬁers against attacks
on Drebin and Androzoo datasets. From the upper half of
Table III, we make the following observations. First, three
defenses (AT-MA, ADE-MA, and dADE-MA) signiﬁcantly
enhance the robustness of DNNs, achieving the accuracy of
≥ 90.13% under 19 attacks, ≥ 88.25% under 18 attacks, and
≥ 89.75% under 19 attacks, respectively. These achievements
considerably outperform the Basic DNN, AT-rFGSM, and AT-
Adam except for a lower accuracy (smaller than 8%) than AT-
Adam under the GDKDE attack and a small lower (< 1.99%)
than AT-rFGSM under an obfuscation attack.

Second, ADE-MA and dADE-MA improve the robustness
against gradient-free attacks, obfuscation attacks, and transfer
attacks. When came to the gradient-based attacks and mixture
of attacks, however, both defense models are not useful,
and even hinder the robustness in some cases. For instance,
compared to the AT-MA, neither of the two ensembles mitigate
the I-“max” PGDs effectively (a 26.88% decrease for ADE-
MA and 12.38% decrease for dADE-MA).

Third, all defenses suffer from the GDKDE (≤ 78.13%),
PGD-ℓ∞ (≤ 84.5%), Mimicry×30 (≤ 83%), Pointwise (≤
81.75%) and the three mixtures of attacks (≤ 79.63%). For
GDKDE and Mimicry, the reason may be that these gener-
ative methods produce adversarial representations that have
similar data distribution as benign examples, leading that no
learning-based classiﬁers can detect these attacks effectively.
Pointwise further promotes the attack effectiveness when using
Mimicry×30 as the initial attack. For PGD-ℓ∞, the reason may
be that the corresponding maximizer does not sufﬁce to obtain
adversarial examples in the training phase. This further leads
to the effectiveness of “max” PGDs.

From the lower half of Table III, we additionally make the
following observations. Fourth, the effectiveness of gradient-
based attacks is not comparative to the gradient-free attacks
(e.g., Mimicry), which counters the results in Drebin dataset.
This may be that the feature representations of malicious
examples are closer to benign ones in the Androzoo dataset
than that in Drebin.

Fifth, though AT-MA achieves higher accuracy of detecting
most of gradient-based attacks, its robustness is lower than
AT-Adam under the PGD-ℓ2 attack and also lower than AT-
rFGSM under the PGD-ℓ∞ attack. This can be explained that
AT-MA induces a loss over the four attacks, which in turn
leads to lower robustness than hardened models that focus on
an attack solely.

TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

10

Table III: Effectiveness of the six classiﬁers, including Basic DNN, Adversarial Training (AT)-rFGSM, AT-Adam, AT-Mixture
of Attacks (MA), Adversarial Deep Ensemble (ADE)-MA, and diversiﬁed ADE-MA (dADE-MA), against adversarial evasion
attacks.

Dataset

Attack Type

Attack Name

Accuracy (%)

Basic DNN

AT-rFGSM AT-Adam AT-MA

ADE-MA

dADE-MA

No Attack

−

Gradient-based
attacks

Drebin

Gradient-free
attacks

Grosse [22]
BGA [7]
BCA [7]
JSMA [25]
FGSM [7]
GDKDE [6]
PGD-Adam [23]
PGD-ℓ1 [20]
PGD-ℓ2 [20]
PGD-ℓ∞ [20]

Salt+Pepper
Mimicry ×1
Mimicry ×30
Pointwise ×30

Obfuscation

Mixture of
attacks (MA)

Transfer
attacks

Java reﬂection
String encryption
Variable renaming
Junk code injection
All techniques combined

“max” PGDs
I-“max” PGDs
I-“max” PGDs+GDKDE

GDKDE
PGD-ℓ1
PGD-ℓ∞
I-“max” PGDs+GDKDE

No Attack

−

Gradient-based
attacks

Androzoo

Gradient-free
attacks

Grosse [22]
BGA [7]
BCA [7]
JSMA [25]
FGSM [7]
GDKDE [6]
PGD-Adam [23]
PGD-ℓ1 [20]
PGD-ℓ2 [20]
PGD-ℓ∞ [20]

Salt+Pepper
Mimicry ×1
Mimicry ×30
Pointwise ×30

Obfuscation

Mixture
of attacks

Transfer
attacks

Java reﬂection
String encryption
Variable renaming
Junk code injection
All techniques combined

“max” PGDs
I-“max” PGDs
I-“max” PGDs+GDKDE

GDKDE
PGD-ℓ1
PGD-ℓ∞
I-“max” PGDs+GDKDE

96.63

0.00
0.00
0.00
0.00
0.00
0.00
0.38
0.00
0.63
0.00

78.25
53.88
10.63
9.50

96.63
96.42
96.84
95.70
90.66

0.00
0.00
0.00

36.75
17.03
34.35
23.10

97.75

22.63
23.00
22.63
41.25
37.75
1.13
50.25
22.63
51.13
22.63

10.00
0.13
0.00
0.00

97.98
95.15
96.82
31.43
13.79

22.63
22.63
0.63

6.95
20.63
48.20
3.45

97.25

58.50
97.25
60.75
65.25
97.25
66.75
93.50
41.50
96.00
48.75

96.25
90.63
62.13
59.38

97.25
96.93
97.47
99.26
99.60

30.13
22.75
4.50

88.83
96.08
96.65
95.53

98.13

39.00
98.13
39.00
47.63
98.13
10.38
85.13
37.88
74.00
97.75

70.38
21.75
2.75
1.75

97.98
95.59
97.27
69.52
34.48

37.25
36.00
3.13

48.63
81.20
96.15
89.08

96.63

63.63
96.63
63.38
63.63
96.63
78.13
89.25
49.00
89.38
74.13

94.00
87.00
60.75
59.00

96.63
96.55
96.58
98.81
99.40

45.00
15.00
14.88

87.60
92.68
95.25
94.38

98.63

26.00
98.63
26.00
43.50
98.63
2.63
55.13
26.00
82.25
78.25

87.88
13.25
0.75
0.38

98.85
95.59
97.50
68.57
37.93

26.00
25.75
0.25

42.13
70.03
91.70
75.55

98.38

92.00
98.38
92.00
92.00
98.38
70.13
96.63
90.13
96.00
72.38

95.00
95.75
80.38
79.25

98.38
98.21
98.35
99.11
98.21

71.13
65.13
63.13

92.25
97.35
96.75
94.23

98.75

92.25
98.75
92.13
92.50
98.75
30.63
93.50
89.88
76.50
52.38

69.75
58.13
22.88
19.88

99.14
96.92
97.73
86.67
48.28

42.25
29.75
19.13

77.35
93.53
97.53
85.55

98.38

88.25
98.25
89.63
89.25
98.38
76.13
94.75
85.25
94.50
81.75

97.63
98.13
83.00
81.75

98.38
98.34
98.35
99.70
99.40

71.00
38.25
36.13

95.30
98.20
97.33
97.33

98.25

89.25
98.13
89.63
89.38
98.25
54.50
95.75
88.38
95.63
92.13

81.13
69.38
48.88
46.75

96.54
94.71
96.82
98.10
100.0

83.88
59.63
36.50

74.70
96.95
98.83
93.93

98.25

89.88
98.25
93.13
91.38
98.25
71.13
96.63
89.75
96.50
84.50

95.75
94.75
77.75
76.50

98.25
98.08
98.23
99.56
97.61

79.63
52.75
51.00

92.58
97.05
97.18
96.93

98.13

89.38
98.13
91.63
91.38
98.13
50.88
97.25
88.50
96.38
88.13

97.00
71.00
46.25
40.63

96.25
94.71
96.59
99.05
96.55

82.75
72.75
30.13

76.73
96.88
98.80
97.15

TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

11

100

95

90

85

80

100

90

80

70

100

95

90

85

80

100

90

80

70

)

%

(

i

n
b
e
r
D
n

y
c
a
r
u
c
c
A

)

%

(

i

n
b
e
r
D
n

y
c
a
r
u
c
c
A

)

%

(

o
o
z
o
r
d
n
A
n
o

y
c
a
r
u
c
c
A

)

%

(

o
o
z
o
r
d
n
A
n
o

y
c
a
r
u
c
c
A

Grosse

BGA

BCA

JSMA

FGSM

1 5 20 40 60 80 100

1 5 20 40 60 80 100

1 5 20 40 60 80 100

1 5 20 40 60 80 100

0

Basic DNN
AT-rFGSM
AT-Adam
AT-MA
ADE-MA
dADE-MA
1

GDKDE

PGD-Adam

PGD-ℓ1

PGD-ℓ2

PGD-ℓ%

0

1

0

5

0

2

0

0

4

0

0

0

6

0

8

0

0

0

0

1

0

1

0

5

0

2

0

0

4

0

0

0

6

0

8

0

0

0

0

1

1

5

0

2

0

4

0

6

0

8

0

0

1

0

1

0

5

0

2

0

0

4

0

0

0

6

0

8

0

0

0

0

1

0

1

0

5

0

0

2

0

4

0

0

0

6

0

8

0

0

0

0

1

Grosse

BGA

BCA

JSMA

FGSM

1 5 20 40 60 80 100

1 5 20 40 60 80 100

1 5 20 40 60 80 100

1 5 20 40 60 80 100

0

1

GDKDE

PGD-Adam

PGD-ℓ1

PGD-ℓ2

PGD-ℓ∞

0

1

0

5

0

0

2

0

0

4

0

0

6

0

8

0

0

0

0

1

0

1

0

5

0

2

0

0

4

0

0

0

6

0

8

0

0

0

0

1

1

5

0

2

0

4

0

6

0

8

0

0

1

Iterations

0

1

0

5

0

0

2

0

0

4

0

0

6

0

0

1

8

0

0

0

0

1

0

5

0

0

2

0

0

4

0

0

6

0

8

0

0

0

0

1

Figure 1: Accuracy of classiﬁers against gradient-based attacks with different iterations on Drebin and Androzoo datasets.

Figure 1 depicts the accuracy of classiﬁers against gradient-
based attacks with different iterations. With more details, we
further make the observations as follows. Sixth, once the
iterations exceed a certain extent, GDKDE can evade all
hardened models effectively. Moreover, the hardened models,
incorporating adversarial training with “max” PGDs, cannot
thwart the PGD-ℓ∞ attack as effectively as the PGD-ℓ1 attack.
Seventh, ADE-MA tends to achieve better accuracy than the
AT-MA when attacks are launched with small iterations, while
this situation exchanges when iterations are increased to a large
extent. It is worth noting that AT-MA attains the best accuracy
on the unperturbed malware examples in the Androzoo dataset,
resulting in the following phenomenon that, along with the
iteration increased, AT-MA obtains higher accuracy than ADE-
ME at the start of curves, while lower accuracy in the middle,
and back to higher accuracy in the end. To some extent, this
conﬁrms our theoretical analysis of ensemble that promotes
the robustness when base classiﬁers are robust enough.

Eighth, under most of the attacks such as Groose, BCA,
the defense of dADE-MA serves as
JSMA and PGD ℓ1,
a remedy for ADE-MA against attacks at
large iterations
(about > 60). This explains why dADE-MA circumvents more
attacks than ADE-MA (see Table III). In summary, we draw

insights:

Insight 3. The hardened models incorporating mixture of
attacks can defend against a broad range of attacks effectively,
but remaining vulnerable to mimicry attacks and mixtures of
attacks; Ensemble promotes the robustness against an attack
when base classiﬁers are robust enough.

RQ3: How is the accuracy of anti-virus scanners under the
mixture of attacks? For answering RQ3, we wage transfer
attacks to target the VirusTotal. The surrogate model is dADE-
MA and the generative method is I-“max” PGDs+GDKDE.
We perturb the randomly selected 800 malware examples
from Drebin test set. Apktool is leveraged to perform reverse
engineering [60]. We ﬁnally obtain 800 perturbed malware
examples, along with their unperturbed versions, which are
together queried VirusTotal service.

Figure 2 exhibits the experimental results of attacking Virus-
Total. From the box-plot of Figure 2a, we observe that malware
examples are predicted as malicious conﬁdently except that
about 1% of them (∼8 ﬁles in 800 examples) being detected
by below 24 scanners, while adversarial attacks notably affect
the prediction, by noting that most of the perturbed examples
are detected by lower than 25 scanners. Nonetheless, no

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

12

100

)

%

(

y
c
a
r
u
c
c
A

80

60

40

20

0

60

s
r
e
n
n
a
c
S

40

f
o

20

r
e
b
m
u
N

0

No attack
Attack

No attack

Attack

a
r
i
v
A

e
e
f
A
c
M

o
d
o
m
o
C

y
k
s
r
e
p
s
a
K

e
r
u
c
e
S
-
F

t
f
o
s
o
r
c
i
M

c
e
t
n
a
m
y
S

2
3
D
O
N
-
T
E
S
E

(a)

(b)

Figure 2: Waging transfer attack against VirusTotal service. (a)
Box plot of the number of anti-virus scanners that predict a
queried example as malicious when applied 800 pristine mal-
ware examples (i.e., no attack) and their 800 perturbed ones
(i.e., attack). We perturb the malware examples using I-“max”
PGDs+GDKDE attack against dADE-MA. (b) Accuracy of 8
scanners when applied the (un)perturbed examples.

attacks can evade VirusTotal thoroughly. Figure 2b showcases
8 famous scanners. We observe that the attack barely affects
the Kaspersky, ESET-NOD32, and F-Secure. McAfee, Co-
modo, and Symantec, however, present vulnerability to these
adversarial examples. In summary, we draw:

Insight 4. The transfer version of mixture of attacks can
downgrade the VirusTotal service and evade certain anti-virus
scanners effectively.

RQ4: Why the enhanced classiﬁers can (cannot) defend
against certain attacks? For answering RQ4, we statistically
attribute “important” features for the defender and the attacker,
respectively. Here the “important” feature is the one that has a
large inﬂuence on the classiﬁcation accuracy. To this purpose,
we present a case study on the defense dADE-MA and the
attack I-“max” PGDs+GDKDE on the Drebin dataset. For the
attacker, we intuitively investigate the features perturbed by the
adversary with high-frequencies. For the defender, an intuitive
solution is deﬁcient due to the intricate structure of deep
ensemble. Therefore, we introduce an alternative using feature
selection technique via a surrogate model. The procedure
proceeds as follows: (i) train a surrogate model on the training
set
to mimic dADE-MA; (ii) rank the features based on
feature importance extracted from the surrogate model; (iii)
mask feature representations based on the ranking result by
setting non-important features value as 0; (iv) send the masked
feature representations to dADE-MA and calculate the change
of accuracy; (v) repeat step (iii)-(iv) and terminate it until a
predetermined number of important features is reached or the
accuracy is lower than a threshold; (vi) reﬁne the importance
upon the retained features using permutation importance [61].
We use random forest to learn the surrogate model, aiming
to obtain feature importance coarsely. In step (iii)-(v), we
leverage binary search to speed up ﬁltering out trivial features.
Table IV demonstrates the top 20 important features for the
classiﬁer dADE-MA and the attack I-“max” PGDs+GDKDE,
respectively. We make the following observations. First, 18
the detection of benign examples,
features (90%) beneﬁt

which may be induced by the imbalanced dataset. Second, 8
important features (40%) of the classiﬁer are manipulated by
I-“max” PGDs+GDKDE frequently, in particular the feature
getNetworkInfo, which promotes malware detection. Third,
features of the classiﬁer that rank at top are neglected by
this attack. Two reasons could account for this: the attack is
failed to search for some of these features; both malicious
and benign applications use these features frequently, such
as android.permission.INTERNET for accessing internet
service. Finally, com.google.ads.AdActivity is at the 5th
place. Though counter-intuitive, it may be that adversarial
deep ensemble enforces the model to focus on this neutral
feature. The above observations collaboratively explain why
dADE-MA obtains an accuracy of 51% against I-“max”
PGDs+GDKDE attack and sacriﬁces detection accuracy in the
absence of attacks.

Insight 5. The hardened model is prone to use sub-effective
features, so as to gain robustness against attacks but a little
trading off accuracy in absence of attacks.

VI. DISCUSSION

Functionality Estimation. We emulate malware behaviors by
Cuckoodroid [62] that is an automated static and dynamical
analysis toolkit for APKs. Due to the efﬁciency issue, we
randomly select 10 APKs from 800 perturbed examples that
are sent to VirusTotal, along with their original versions, to
conduct the estimation. We observe that two malware exam-
ples and their perturbed versions cannot run on the emulator,
and thus exclude them from the testing. For other perturbed
applications, 3 of them execute successfully on the emulator,
2 apps can be deployed into Android runtime but failed to run,
and the remained 3 apps cannot be installed. This shows more
research is needed to solve the problem of retaining malicious
functionality.
Small vs. large degree of manipulations. In this study, we
let attackers sufﬁce iterations to maximize the classiﬁer’s loss
when perturbing malware examples as long as the malicious
functionality is preserved. In term of ℓ1 norm, some attacks
perturb malware examples slightly, for example the JSMA
attack against the Basic DNN with the average perturbations
4.48; some others perturb malware examples to a large extent,
for example the PGD-ℓ∞ attack against the Basic DNN with
the average perturbations 2,772.87. In addition, the I-“max”
PGDs+GDKDE attack perturbs malware examples with the
average perturbations 2,600.02, 53.23, and 45.16 when target-
ing the Basic DNN, AT-Adam, and dADE-MA, respectively.
Validating the hypothesis of theoretical analysis. We em-
pirically justify the hypothesis (see Section IV-B3) that base
classiﬁers of ensemble are non-negatively correlated under
adversarial example attacks. In this end, we directly let per-
turbed examples pass through a deep ensemble and mea-
sure the Pearson correlation coefﬁcient between any two
base models upon the logit belonging to the label ‘1’. The
ideal classiﬁer is treated as a constant and thus neglected
by the applied measurement. We perturb the 800 malware
examples selected from Drebin datasets using Mimicry×30
and I-“max” PGDs+GDKDE attacks against ADE-MA. There

 
 
 
TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

13

Table IV: Top 20 important features for the defense of dADE-MA vs. I-“max” PGDs+GDKDE attack. We further report
whether the feature facilitates the classiﬁcation accuracy for benign (−) or malicious (+) examples and, whether the feature
is frequently manipulated by ﬂipping ‘1’ to ‘0’ (↓) or ﬂipping ‘0’ to ‘1’ (↑).

Defender

+/− Attacker

↑ / ↓

android/widget/VideoView;→start
android/widget/MediaPlayer;→start
android.permission.ACCESS COARSE LOCATION
android.permission.INTERNET
com.google.ads.AdActivity

android.permission.ACCESS FINE LOCATION
android/location/LocationManager;→requestLocationUpdates
android/location/LocationManager;→removeUpdates
android/net/ConnectivityManager;→getActiveNetworkInfo
getSystemService

android.permission.ACCESS NETWORK STATE
android/location/LocationManager;→getBestProvider
android.permission.READ CONTACTS
android/net/ConnectivityManager;→getNetworkInfo
getPackageInfo

android/widget/VideoView;→setVideoURI
android.permission.WAKE LOCK
android.permission.SEND SMS
android/widget/VideoView;→setVideoPath
printStackTrace

−
−
−
−
−

−
−
−
−
−

−
−
−
+
−

−
−
+
−
−

com.airpush.android.PushServiceStart61159
.httpserver.HttpServerService
android/telephony/TelephonyManager;→getLine1Numb
android/telephony/TelephonyManager;→listen
android/location/LocationManager;→removeUpdates

android/location/LocationManager;→requestLocationUpdates
android/location/LocationManager;→getLastKnownLocation
android/net/ConnectivityManager;→getNetworkInfo
android.permission.ACCESS FINE LOCATION
android.permission.READ CONTACTS

android/app/ActivityManager;→getRunningTasks
android.permission.ACCESS COARSE LOCATION
android/telephony/TelephonyManager;→getDeviceId
android.permission.MOUNT UNMOUNT FILESYSTEMS
android/telephony/TelephonyManager;→getCellLocation

getPackageInfo
android/net/ConnectivityManager;→getActiveNetworkInfo
android/net/wiﬁ/WiﬁManager;→isWiﬁEnabled
sendTextMessage
android.permission.WRITE SETTINGS

↑
↑
↓
↑
↑

↑
↑
↓
↑
↑

↑
↑
↓
↑
↑

↑
↑
↑
↓
↑

are 5 base models in ADE-MA, each attack resulting in 10
correlation coefﬁcients wherein the mean value is 0.4 ± 0.16
(0.16 is the standard deviation) under Mimicry attack and
0.18 ± 0.22 under I-“max” PGDs+GDKDE. Moreover, we
conduct the same estimation for dADE-MA. The results are
0.56 ± 0.15 under Mimicry attack and 0.39 ± 0.17 under I-
“max” PGDs+GDKDE. Most of the observations conﬁrm our
statement except for several cases in ADE-MA.

VII. CONCLUSION AND FUTURE WORK

We have studied the usefulness of ensemble for both the
defender and the attacker in the context of adversarial malware
detection. We propose the mixture of attacks and adversarial
deep ensemble. The adversarial deep ensemble can defend
against a broad range of evasion attacks, while cannot thwart
mimicry attacks and mixtures of attacks. Ensemble methods
promote the robustness against evasion attacks when base
classiﬁers are robust enough. For the attacker, the ensemble
methods notably improve the attack effectiveness.

We hope this paper will inspire more research into the
context of adversarial malware detection. Future research prob-
lems are plentiful, such as: intriguing properties of different
adversarial malware examples, seeking effective attacks, robust
feature extraction, malicious functionality estimation, defense
validation metrics and further designing robust defenses.

REFERENCES

[1] Symantec, “Internet security threat report 2019 (istr),” Symantec, Tech.

Rep., February 2019.

[2] V. Chebyshev. (2019, March 5) Mobile malware evolution. [Online].

Available: https://securelist.com/
(2018)

[3] CISCO.

Cisio @ONLINE.

[Online].

Available:

https://www.cisco.com

[4] Y. Ye, T. Li, and et al., “A survey on malware detection using data
mining techniques,” ACM Comput. Surv., vol. 50, no. 3, pp. 41:1–41:40,
2017.

[5] Y. Fan, S. Hou, and et al., “Gotcha - sly malware!: Scorpion A
metagraph2vec based malware detection system,” in Proceedings of
KDD’2018, 2018, pp. 253–262.

[6] I. C. B. Biggio and D. M. et al., “Evasion attacks against machine
learning at test time,” in Machine Learning and Knowledge Discovery
in Databases: European Conference. Springer, 01 2013, pp. 387–402.
[7] A. Al-Dujaili, A. Huang, E. Hemberg, and U.-M. OReilly, “Adversarial
deep learning for robust detection of binary encoded malware,” in 2018
IEEE Security and Privacy Workshops (SPW).
IEEE, 2018, pp. 76–82.
[8] S. Hou, Y. Ye, Y. Song, and M. Abdulhayoglu, “Make evasion harder:
An intelligent android malware detection system,” in Proceedings of the
Twenty-Seventh IJCAI, 2018, pp. 5279–5283.

[9] L. Chen, Y. Ye, and T. Bourlai, “Adversarial machine learning in
malware detection: Arms race between evasion attack and defense,” in
EISIC’2017, 2017, pp. 99–106.

[10] F. Kreuk, A. Barak, S. Aviv-Reuven, M. Baruch, B. Pinkas, and
J. Keshet, “Deceiving end-to-end deep learning malware detectors using
adversarial examples,” in CoRR, 2018.

[11] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel,
“Adversarial perturbations against deep neural networks for malware
classiﬁcation,” arXiv preprint arXiv:1606.04435, 2016.

[12] E. Grefenstette, R. Stanforth, B. O’Donoghue, J. Uesato, G. Swirszcz,
robustness and
adversarially-trained ensembles,” arXiv preprint

and P. Kohli, “Strength in numbers: Trading-off
computation via
arXiv:1811.09300, 2018.

[13] F. Tram`er, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and
P. McDaniel, “Ensemble adversarial training: Attacks and defenses,”
arXiv preprint arXiv:1705.07204, 2017.

[14] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transfer-
able adversarial examples and black-box attacks,” arXiv preprint
arXiv:1611.02770, 2016.

[15] H. Kwon, Y. KIM, K.-W. Park, H. Yoon, and D. Choi, “Advanced ensem-
ble adversarial example on unknown deep neural network classiﬁers,”
IEICE Transactions on Information and Systems, vol. E101.D, pp. 2485–
2500, 10 2018.

[16] F. Tram`er and D. Boneh, “Adversarial

training and robustness for

multiple perturbations,” arXiv preprint arXiv:1904.13000, 2019.
[17] A. Araujo, R. Pinot, and et al., “Robust neural networks using random-
ized adversarial training,” arXiv preprint arXiv:1903.10219, 2019.
[18] L. Schott, J. Rauber, M. Bethge, and W. Brendel, “Towards the ﬁrst
adversarially robust neural network model on mnist,” arXiv preprint
arXiv:1805.09190, 2018.

[19] A. Athalye, N. Carlini, and D. A. Wagner, “Obfuscated gradients give a
false sense of security: Circumventing defenses to adversarial examples,”
CoRR, vol. abs/1802.00420, 2018.

[20] D. Li, Q. Li, Y. Ye, and S. Xu, “Enhancing deep neural networks against
adversarial malware examples,” arXiv preprint arXiv:2004.07919, 2020.
[21] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” arXiv preprint
arXiv:1706.06083, 2017.

TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY

14

in adversarial settings,” in 2016 EuroS&P.

[48] N. Papernot, P. McDaniel, and et al., “The limitations of deep learning
IEEE, 2016, pp. 372–387.
[49] J. Jung, C. Jeon, and et al., “AVPASS: Leaking and Bypassing Antivirus
Detection Model Automatically,” in Black Hat USA Brieﬁngs (Black Hat
USA), Las Vegas, NV, Jul. 2017.

[50] L. Xu, Z. Zhan, S. Xu, and K. Ye, “An evasion and counter-evasion
study in malicious websites detection,” in CNS, 2014 IEEE Conference
on.

IEEE, 2014, pp. 265–273.

[51] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial machine learning

at scale,” arXiv preprint arXiv:1611.01236, 2016.

[52] L. Xu, Z. Zhan, S. Xu, and K. Ye, “Cross-layer detection of malicious
websites,” in Third ACM Conference on Data and Application Security
and Privacy (CODASPY’13), 2013, pp. 141–152.

[53] W. Hoeffding, “Probability inequalities for sums of bounded random
variables,” Journal of the American statistical association, vol. 58, no.
301, pp. 13–30, 1963.

[54] Z.-H. Zhou, Ensemble methods: foundations and algorithms. Chapman

and Hall/CRC, 2012.

[55] D. P. Bertsekas, Constrained optimization and Lagrange multiplier

methods. Academic press, 2014.

[56] J. Duchi, S. Shalev-Shwartz, and et al., “Efﬁcient projections onto the
l 1-ball for learning in high dimensions,” in Proceedings of the 25th
ICML. ACM, 2008, pp. 272–279.

[57] A. Desnos.

(2019) Androguard @ONLINE.

[Online]. Available:

https://github.com/androguard/androguard

[58] K. H. Brodersen, C. S. Ong, K. E. Stephan, and J. M. Buhmann,
“The balanced accuracy and its posterior distribution,” in 2010 20th
International Conference on Pattern Recognition, 2010, pp. 3121–3124.
[59] M. Pendleton, R. Garcia-Lebron, J.-H. Cho, and S. Xu, “A survey on
systems security metrics,” ACM Comput. Surv., vol. 49, no. 4, pp. 1–35,
Dec. 2016.

[60] (2019,

Apktool.
May)
https://ibotpeaches.github.io/Apktool

[Online].

Available:

[61] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.

5–32, 2001.

[62] I. Revivo and O. Caspi, “Cuckoodroid,” in Black Hat USA, Las Vegas,

NV, Jul. 2017.

[63] B. Kolosnjaji, A. Demontis, and et al., “Adversarial malware binaries:
Evading deep learning for malware detection in executables,” in 2018
26th European Signal Processing Conference (EUSIPCO), Sep. 2018,
pp. 533–537.

[22] K. Grosse, N. Papernot, and et al., “Adversarial examples for malware
detection,” in European Symposium on Research in Computer Security.
Springer, 2017, pp. 62–79.

[23] D. Li, Q. Li, Y. Ye, and S. Xu, “Enhancing robustness of deep neural
networks against adversarial malware samples: Principles, framework,
and application to aics2019 challenge,” in The AAAI-19 Workshop on
Artiﬁcial Intelligence for Cyber Security (AICS), 2019, 2019.

[24] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing

adversarial examples (2014),” arXiv preprint arXiv:1412.6572.

[25] X. Chen, C. Li, D. Wang, S. Wen, J. Zhang, S. Nepal, Y. Xiang, and
K. Ren, “Android hiv: A study of repackaging malware for evading
machine-learning detection,” IEEE Transactions on Information Foren-
sics and Security, vol. 15, pp. 987–1001, 2020.

[26] D. Arp, M. Spreitzenbarth, and et al., “Drebin: Effective and explainable
detection of android malware in your pocket.” in Ndss, vol. 14, 2014,
pp. 23–26.

[27] K. Allix, T. F. Bissyand´e, J. Klein, and Y. Le Traon, “Androzoo:
Collecting millions of android apps for the research community,” in
Proceedings of the 13th International Conference on Mining Software
Repositories, ser. MSR ’16. New York, NY, USA: ACM, 2016, pp. 468–
471. [Online]. Available: http://doi.acm.org/10.1145/2901739.2903508

[28] (2019, May) Virustotal. [Online]. Available: https://www.virustotal.com
[29] Y. Dong, F. Liao, and et al., “Boosting adversarial attacks with momen-

tum,” in Proceedings of the CVPR, 2018, pp. 9185–9193.

[30] C. Smutz and A. Stavrou, “Malicious pdf detection using metadata and
structural features,” in Proceedings of the 28th annual computer security
applications conference. ACM, 2012, pp. 239–248.

[31] B. Biggio, G. Fumera, and F. Roli, “Multiple classiﬁer systems for robust
classiﬁer design in adversarial environments,” International Journal of
Machine Learning and Cybernetics, vol. 1, no. 1-4, pp. 27–41, 2010.

[32] ——, “Multiple classiﬁer systems under attack,” in International Work-
shop on Multiple Classiﬁer Systems. Springer, 2010, pp. 74–83.
[33] M. Abbasi and C. Gagn´e, “Robustness to adversarial examples through

an ensemble of specialists,” in ICLR 2017 workshop, 2017.

[34] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial
examples in deep neural networks,” arXiv preprint:1704.01155, 2017.
[35] W. He, J. Wei, X. Chen, N. Carlini, and D. Song, “Adversarial example
defense: Ensembles of weak defenses are not strong,” in 11th USENIX
Workshop on Offensive Technologies (WOOT 17).
Vancouver, BC:
USENIX Association, Aug. 2017.

[36] T. Pang, K. Xu, C. Du, N. Chen, and J. Zhu, “Improving adversarial ro-
bustness via promoting ensemble diversity,” in International Conference
on Machine Learning, 2019, pp. 4970–4979.

[37] C. Smutz and A. Stavrou, “When a tree falls: Using diversity in ensemble

classiﬁers to identify evasion in malware detectors.” in NDSS, 2016.

[38] J. W. Stokes, D. Wang, M. Marinescu, M. Marino, and B. Bussone,
“Attack and defense of dynamic analysis-based, adversarial neural
malware classiﬁcation models,” 12 2017.

[39] J. Parikh. (2018, August) Protecting the protector:hardening machine
[Online]. Available:

learning defenses against adversarial attacks.
https://www.blackhat.com/us-18/speakers/Jugal-Parikh.html

[40] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against deep learning systems
using adversarial examples,” arXiv preprint, 2016.

[41] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise of
adversarial machine learning,” Pattern Recognition, vol. 84, pp. 317–
331, 2018.

[42] W. Xu, Y. Qi, and D. Evans, “Automatically evading classiﬁers: A case

study on pdf malware classiﬁers,” in NDSS, January 2016.

[43] P. L. Nedim rndic, “Practical evasion of a learning-based classiﬁer: A
case study,” in Security and Privacy (SP), 2014 IEEE Symposium on.
IEEE, 2014, pp. 197–211.

[44] A. Demontis, M. Melis, and et al., “Why do adversarial attacks transfer?
explaining transferability of evasion and poisoning attacks,” in 28th
USENIX Security Symposium, USENIX Security 2019, Santa Clara, CA,
USA, August 14-16, 2019, N. Heninger and P. Traynor, Eds. USENIX
Association, 2019, pp. 321–338.

[45] A. Demontis, M. Melis, and et al., “Yes, machine learning can be more
secure! a case study on android malware detection,” IEEE Transactions
on Dependable and Secure Computing, vol. 16, no. 4, pp. 711–724, July
2019.

[46] F. Pierazzi, F. Pendlebury, J. Cortellazzi, and L. Cavallaro, “Intriguing
properties of adversarial ml attacks in the problem space,” arXiv preprint
arXiv:1911.02142, 2019.

[47] B. Biggio, G. Fumera, and F. Roli, “Security evaluation of pattern
classiﬁers under attack,” IEEE TKDE, vol. 26, pp. 984–996, 04 2014.

