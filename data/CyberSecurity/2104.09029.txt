Benchmarking the Benchmark - Analysis of Synthetic NIDS Datasets

Siamak Layeghy, Marcus Gallagher, Marius Portmann

School of ITEE, The University of Queensland, Brisbane, Australia

1
2
0
2

r
p
A
9
1

]
I

N
.
s
c
[

1
v
9
2
0
9
0
.
4
0
1
2
:
v
i
X
r
a

Abstract

Network Intrusion Detection Systems (NIDSs) are an increasingly important tool for the pre-
vention and mitigation of cyber attacks. Over the past years, a lot of research eﬀorts have aimed
at leveraging the increasingly powerful models of Machine Learning (ML) for this purpose. A
number of labelled synthetic datasets generated have been generated and made publicly available
by researchers, and they have become the benchmarks via which new ML-based NIDS classiﬁers
are being evaluated. Recently published results show excellent classiﬁcation performance with
these datasets, increasingly approaching 100 percent performance across key evaluation metrics
such as accuracy, F1 score, etc. Unfortunately, we have not yet seen these excellent academic
research results translated into practical NIDS systems with such near-perfect performance.

This motivated our research presented in this paper, where we analyse the statistical proper-
ties of the benign traﬃc in three of the more recent and relevant NIDS datasets, (CIC, UNSW,
...). As a comparison, we consider two datasets obtained from real-world production networks,
one from a university network and one from a medium size Internet Service Provider (ISP). Our
results show that the two real-world datasets are quite similar among themselves in regards to
most of the considered statistical features. Equally, the three synthetic datasets are also rela-
tively similar within their group. However, and most importantly, our results show a distinct
diﬀerence of most of the considered statistical features between the three synthetic datasets
and the two real-world datasets. Since ML relies on the basic assumption of training and test
datasets being sampled from the same distribution, this raises the question of how well the
performance results of ML-classiﬁers trained on the considered synthetic datasets can translate
and generalise to real-world networks. We believe this is an interesting and relevant question
which provides motivation for further research in this space.

Keywords: Network traﬃc, feature distribution, IDS Dataset, NetFlow, Traﬃc Characteristics

1. Introduction

Network Intrusion Detection Systems (NIDSs) are an important defence mechanism to pro-
tect computer networks against an increasing diversity, sophistication and volume of cyber at-
tacks. Due to the tremendous progress in Machine Learning (ML) over the last few years, in

Preprint submitted to Arxiv.org

April 20, 2021

 
 
 
 
 
 
particular Deep Neural Networks (DNNs), there has been a lot of recent research into lever-
aging the power of novel ML models for Network Intrusion Detection Systems. In particular
supervised ML methods have shown great potential over traditional signature-based NIDS. As
with any supervised ML problem, the availability of high quality labelled datasets is absolutely
critical. For many years, the KDD99 dataset [1] has been the most widely used benchmark
dataset for the evaluation of NIDSs. However, it is well established that the dataset has signiﬁ-
cant limitations [2] the main one being its age, it was created over two decades ago. Given that
Youtube, Facebook, Spotify, mainstream cloud computing and smartphones did not exist when
the dataset was created, one can appreciate that the pattern of network traﬃc has undergone a
profound change since then.

Furthermore, the type and sophistication of network attacks have undergone an equally dra-
matic change in the last 20+ years 1. The need for more recent and relevant NIDS datasets
has been clearly identiﬁed [2], and has lead to the development of a range of new datasets over
the last few years. In contrast to ML application areas such as image classiﬁcation, where high
quality benchmark datasets can relatively easily be generated, this is a much harder problem in
the context of NIDSs. Ideally, we would have datasets collected from real production networks,
with realistic network patterns of benign traﬃc, together with a wide range of correctly labelled
attack traﬃc. Since such ideal NIDS datasets are not readily available, researchers have recently
developed a range of new synthetic datasets, which have become the new benchmarks. These
synthetic datasets are typically generated in a controlled and relatively small simulation or test-
bed environment, where both normal traﬃc and attack traﬃc are created and labelled. Each
of these datasets typically has its own dedicated feature set, which is collected and represented
in a ﬂow-based format. Over the past few years, researchers have extensively used these syn-
thetic datasets to evaluate a wide range of new proposed ML-based intrusion detection models
and methodologies. Recently published results show increasingly excellent classiﬁcation perfor-
mance, approaching 100% across the key performance metrics, such as accuracy, F1-score, etc.
Consequently, one could assume that the problem of ML-based NIDS has been largely solved.
Arguably, this is not quite the case, and the excellent results achieved in recently published
academic research have not yet translated into practical, near-perfect intrusion detection sys-
tems deployed in real-world production networks. This apparent gap has motivated our research
presented in this paper.

ML generally assumes that statistical properties of training data are the same as for testing
data. Therefore, in order for the performance of an ML-classiﬁer trained and evaluated on
synthetic datasets to generalise and translate to a real network scenario, the statistical properties
of both datasets would have to be similar. Our aim was therefore to compare the statistical
properties of synthetic NIDS datasets with those of real world network traﬃc. Our aim was
therefore to compare the statistical properties of synthetic datasets with those of network traﬃc
obtained from real, large-scale production networks. In our analysis, we focused on benign (non-

1It is quite suprising that the KDD99 dataset is still used today???

2

attack) traﬃc, due to the lack of attack labels in the production network traﬃc available to us.
For our analysis, we have considered three recently published and widely used synthetic datasets
UNSW-NB15 [2], CIC-IDS2017 [3] and TON-IOT [4]. The datasets contain 44 to 85 number of
features in diﬀerent formats. We further used two datasets from large-scale production networks,
collected in 2019, one from a medium sized Australian ISP, and the other from the University of
Queensland network with ∼100 and ∼700 ﬂows per second respectively. The real-world datasets
were in Netﬂow/IPFIX format, which is widely available in

In order to enable the comparison of the ﬁve datasets, we require them to be in the same
format. For this, we leveraged our previous work [5], where we converted synthetic NIDS datasets
from their proprietary feature set and format to the standard Netﬂow/IPFIX format 2. In [6],
we argue the beneﬁts of having a standard feature set and dataset format, and also show that,
somewhat surprisingly, ML-classiﬁers achieve higher performance using the Netﬂow feature set
and format, compared to the original version.

For our comparison, we considered 9 practically relevant statistical features, such as ﬂow
duration, ﬂow size, packet size, plus a number of IP address and port number related features.
We compared the statistical distributions via box plots and CDF of each feature across the ﬁve
considered datasets. We further quantiﬁed the distance of the diﬀerent feature distributions
using the Wasserstein metric [7]. Finally, we calculated and visualised the embedding of the 9
features into a 2 dimensional feature space, using four diﬀerent embedding algorithms.

Our analysis provided the following key ﬁndings. The two real-world datasets, despite the
fact that they had been collected from quite diﬀerent networks, exhibit a high degree of similarity
in the traﬃc feature distributions. Similarly, the synthetic datasets are quite similar amongst
themselves in regards to most traﬃc features. However, and most interestingly, our analysis
found a highly signiﬁcant diﬀerence between the synthetic datasets and the real-world datasets
in regards to most of the considered feature statistics.

To the best of our knowledge, this paper provides the ﬁrst analysis of recent synthetic NIDS
datasets and their comparison to real word traﬃc, We believe our results are relevant due to the
extensive use of these synthetic datasets as a benchmark to evaluate ML-based NIDS models and
algorithms, and they motivate future research into the development of new datasets that more
closely match the properties of traﬃc in large scale real-world networks. This is an important
goal in order to allow the translation and generalisation of the excellent NIDS performance
achieved in academic research into NIDSs that are increasingly practically relevant and widely
deployed in practical settings.

The rest of this paper is organised as follows. After explaining the background in the next
section, the three synthetic datasets along with the two real-world network traﬃc datasets that
are used as references for this purpose, are introduced in Section 3. The next section provides
various qualitative comparisons between features distributions of the synthetic datasets and the
real-world datasets. Then, Section 5 quantiﬁes the diﬀerences of feature distributions among

2Datasets are available here ...

3

two groups of datasets. Section 6 provides the related works in the ﬁeld and Section 7 concludes
the paper and provides the insights from our study.

2. Background

2.1. Network Intrusion Detection Systems

The main approaches for building network-based intrusion detection systems (IDS) include
misuse detection or rule-based, anomaly detection, and a third hybrid approach, which tries to
combine beneﬁts of the ﬁrst two approaches [8]. In misuse detection systems, detection is based
on the known attack signatures or rules. In anomaly detection, most of the methods deﬁne a
normal traﬃc model and then identify anomalies as deviations from the normal model. While
misuse-based IDSs have low false positive ratios, they fail to detect new types of attacks /
intrusions since they have designed to detect speciﬁc types of intrusions. Anomaly-based IDSs,
on the other hand, have higher false positive ratios but are potentially capable of detecting
unknown attacks / intrusions. The anomaly-detection systems are implemented using various
techniques, and hence there are various categories of anomaly-based IDS [9].

The main approaches for the network anomaly detection include Statistical, Machine Learn-
ing, Soft Computing, Knowledge-based, and Combination Learner methods. In the statistical
methods, generally a model is ﬁtted to the given data (usually normal behaviour), either via
parametric or non-parametric techniques. Then, by applying statistical inference tests, the prob-
ability that an instance is generated by the model is calculated. If such probability is low, the
instance is considered anomaly [10]. In the soft computing approach, which includes methods
based on Genetic Algorithms (GA), Fuzzy Set Theoretic (FST), Rough Set (RS), and Ant Colony
and Artiﬁcial Immune Systems (AIS), persistant features of data are detected and categorized
without environmental feedback. For instance, in methods based on GA, heuristic search tech-
niques based on evolutionary ideas are used to learn the user proﬁles, and in methods based on
FST, fuzzy rules are used to determine the likelihood of speciﬁc or general network attacks [8].
In knowledge-based methods, network events are matched against predeﬁned rules or patterns
of attacks, i.e. attack signatures. Several approaches have been taken in the knowledge-based
methods including expert systems, rule-based, ontology-based, logic-based and state-transition
analysis [10]. The main ingredient of all combination learner methods, which include ensemble-
based, fusion and hybrid approaches, is combining advantages of diﬀerent methods to enhance
the performance of anomaly detection. In the case of ensemble-base and fusion approaches vari-
ous classiﬁers are combined and the hybrid approach enhances the anomaly detection by getting
the advantages of misuse detection methods [8].

Among diﬀerent methods utilised for implementing the anomaly-detection IDS, systems
based on machine learning have been very common [11]. This approach includes two meth-
ods clustering / outlier-based, and classiﬁcation-based. While clustering-based methods do not
require labelled datasets for their training / operation, their evaluation necessitates using la-
belled datasets. Training and evaluation of the second sub-category of machine learning based

4

approach, i.e. classiﬁcation-based methods, require labelled datasets in which classes / cate-
gories of data are known and included in the dataset as labels. This allows machine learning
algorithms to learn the patterns of various classes and accordingly classify the dataset records.
In the ﬁeld of anomaly detection, these classes are mainly normal and anomaly classes. The
anomaly class can be further divided into diﬀerent types of anomalies such as network failures,
intrusions and other types of attacks.

3. Datasets Explored in This Study

In this study we have used three Synthetic / testbed-based IDS benchmark datasets which

we are compared with two real-world network traﬃc records.

3.1. Synthetic Datasets

The Synthetic / testbed-based datasets selected for this study are chosen from the most

recent IDS benchmark datasets that published from 2015 till 2019.

3.1.1. UNSW-NB15 Dataset

This is the oldest dataset among the three selected datasets, which is published in 2015 by
the Cyber Range Lab of the Australian Centre for Cyber Security (ACCS) [2]. The dataset
has consisted of 2,540,044 ﬂows including 87.35% benign and 12.65% attack ﬂows. The traﬃc
ﬂows are represented in 49 features generated by Argus and Bro-IDS from traﬃc of a test bed
in which 9 diﬀerent attack types are combined with normal background testbed traﬃc. This
dataset is also published in the form of raw pcap ﬁles.

3.1.2. CIC-IDS2017 Dataset

This dataset has been generated by Canadian Institute for Cybersecurity in 2017 [3]. The
main priority in generating this dataset is stated by authors as having realistic background traﬃc.
They have used their own developed tool for network analysis, CICFlowMeter, to generate a
dataset in which ﬂows are labeled based on time stamp, source, and destination IPs, source and
destination ports, protocols and attack. They have also provided the dataset represented by
extracted features deﬁnition. They have included 21 diﬀerent network attack in this dataset,
which includes most of the major network breaches such as DoS, DDoS, diﬀerent scans, Botnet,
and various types of Web and Inﬁltration attacks. They ran individual class of attacks separately
and provided them in a separate CSV ﬁles, as for normal background traﬃc of their test bed.
The dataset is also published in raw pcap format.

3.1.3. TON-IOT Dataset

This is a newer dataset published by ACCS in 2019, encompassing a broader scope compared
to their previous dataset [2], which includes traﬃc records from their testbed network and IoT
devices along with logs of the operating systems [4]. The network traﬃc records, explored in
our work, are represented in the form of 22,339,021 network ﬂows using 44 features extracted by

5

Bro-IDS. The background / benign traﬃc constitutes 3.56% of dataset and the remaining ﬂows
(96.44%) are 9 diﬀerent attack records. This dataset is also published in the raw pcap format.

3.2. Real-World Network Traﬃc Records

In order to compare the above testbed-based datasets with real-world traﬃc, we collected
two sets of network traﬃcs. Since we are looking for the aspects of network traﬃc that must be
valid in any network type, we selected two completely diﬀerent networks. This is an important
basis in our study to avoid biased judgement and be able to generalise the results to any network
type. As such, the two network traﬃc records explained in this section, not only are collected
from two diﬀerent types of networks, but they also belong to two diﬀerent organization with
various administration requirements and ruling polices who serve diﬀerent types of customers.
In addition, these two sets of network traﬃc have been recorded in two diﬀerent time period,
2017 and 2019, to maximise the possibility of result generalisation.

3.2.1. Internet Service Provider Traﬃc Dataset

The ﬁrst real-world dataset utilised in this study is the NetFlow records from an Australian
Internet Service Provider (ISP) backbone network. This ISP is a provider of enterprise-grade
managed services such as Internet, MPLS, VoIP, and cloud with dual point of presences (PoPs)
in all major Australian capital cities, New Zealand, Hong Kong and Manilla. Most of the
customers of the ISP are businesses and companies with multiple oﬃces, sometimes thousands
of oﬃces around the country, that use ISP’s services for the connectivity and other network and
Internet services. For collecting the traﬃc records we used the nprobe [12] software on a server
to extract NetFlow records with about 20 ﬁelds. The whole traﬃc from ISP’s backbone network
was mirrored and aggregated toward the nprobe server where after extracting NetFlow records
stored them in another collector server. These records reﬂect the whole traﬃc in the monitored
part of ISP’s backbone network without any sampling or exemptions. We have collected these
NetFlow records for about 30 days in June 2017 and it includes about 400GB of ﬂow records.

3.2.2. University of Queensland (UQ) Dataset

The next real-world dataset we have used in this study includes NetFlow records collected
form the LAN of faculty of Engineering, Architecture and Information Technology (EAIT), The
University of Queensland. The EAIT faculty consists of ﬁve schools including Architecture,
Chemical Engineering, Civil Engineering, Information Technology and Electrical Engineering,
and Mechanical and Mining Engineering. We used similar setup for collecting this dataset, i.e.
using nprobe NetFlow exporter with the same set of ﬁelds that we used on ISP NetFlows. The
data collection in this site started on early Feb. 2019 and continued for about 50 days. The
recorded data includes all the traﬃc ﬂows in the monitored part of the network including all
wired and wireless communications, servers’ traﬃc and all workstations in all subsidiary schools
of EAIT faculty, totally about 4TB.

Table 1 shows a summary information relating the ﬁve datasets studied in this paper. This
information includes the type of dataset, i.e. is it a synthetic or real-world dataset, the percentage

6

Table 1: Summary information of datasets studied in this paper

Dataset

Synthetic /
Real-world

Attack (%)

Number of
Features

Year

Format

Generation
Tool

UNSW-NB15 [2]

Synthetic

12.65

CIC-IDS2017 [3]

Synthetic

28.16

TON IOT [4]

Synthetic

96.44

ISP [Our]

Real-World

UQ [Our]

Real-World

0

0

49

85

44

20

20

2015

2017

2019

proprietary
ﬂow format

Argus / Bro

proprietary
ﬂow format

CICFlowMeter

proprietary
ﬂow format

Security
Onion / Bro

2017

NetFlow

nprobe

2019

NetFlow

nprobe

of attack and benign ﬂows from total records of the dataset, number of features, the year dataset
is collected / published, size, type and the tools used to generate / collect the dataset.

4. Statistical Analysis of Network Traﬃc Features

As discussed in Section 6.2, anomalies typically change statistical distribution of some traﬃc
features [13, 14, 15]. Hence, we cannot expect to acquire similar results from the comparison of
statistical distributions of these benchmark datasets containing anomalies / attacks with that
of the real-world traﬃc records with unknown attack inclusion status. So, we needed to make
sure that the selected parts of both groups of datasets are attack free (normal traﬃc).

As such, we had two issues to address before starting our analysis. First, to select attack free
parts from benchmark datasets, and second, to make sure the selected parts of the real-world
traﬃc does not include attacks. The ﬁrst problem was relatively easy, as normal / background
traﬃc were provided in separate ﬁles in UNSW-NB15 [2], CIC-IDS2017 [3] and TON IOT [4],
and we used the provided metadata to exclude all the attack ﬂows. The second task was rather
diﬃcult.
Initially, we asked for the attack / anomaly logs in the ISP and our university IT
department related to the selected part of the traﬃc. Then, we applied our under-development
AI-based anomaly detection algorithms on these recordings. Finally, any detected anomalies by
algorithms were investigated in complete details and discussed with the corresponding network
administration for ﬁnal decision. In this way, an anomaly-free part of the traﬃc was selected
equivalent to 24 hours of both ISP and UQ traﬃc records.

Table 2 shows the list of traﬃc features we have used in this statistical distributions com-
parison. The ﬁrst column is the name of feature, the second column is the NetFlow (V9) ﬁelds
used to calculate the feature, and the third column is the formula / equation applied for the
calculation. Since some of the needed NetFlow ﬁelds were not provided in the original published
benchmark datasets, we had to use their provided packet captures (pcap ﬁles) to generate the

7

Table 2: List of Traﬃc features investigated in this study, along with NetFlow ﬁelds utilised to calculate these
features and how they are calculated

Feature
flow duration

flow size (in Bytes)

packet time (average)

packet size

number of source IPs per
destination IP
number of source IPs per
destination PORT
number of destination IPs per
source PORT
number of destination Ports per
source port
number of L7 protocols per
destination port

NetFlow Fields
FIRST SWITCHED(FS),
LAST SWITCHED(LS)
IN BYTES(IB),
OUT BYTES(OB)
FIRST SWITCHED(FS),
IN PKTS(IP),
LAST SWITCHED(LS),
OUT PKTS(OP)

IN BYTES(IB),
OUT BYTES(OB),
IN PKTS(IP),
OUT PKTS(OP)

SRC IP, DST IP

SRC IP, DST PORT

DST IP, SRC PORT

SRC PORT, DST PORT

L7_PROTO, DST PORT

How to Calculate
Ls - FS

IB + OB

LS - FS
IP + OP

IB + OB
IP + OP

COUNT(SRC IP)
per DST IP
COUNT(SRC IP)
per DST PORT
COUNT(DST IP)
per SRC PORT
COUNT(DST PORT)
per SRC PORT
COUNT(L7_PROTO)
per DST PORT

NetFlow records. For this, after determining the attack free parts of the benchmark datasets,
as explained above, the corresponding pcap ﬁles were identiﬁed and fed to nprobe [12] software
to generate NetFlow records.

All the NetFlow ﬁelds utilised in our study have previously been used for characterising
anomaly traﬃc in at least one or more studies [13, 14, 15], except the L7 PROTO that was not
easily provided by devices in the past. As such, comparing these datasets with real-world traﬃc,
in terms of statistical distributions of these features, provides a realistic measure of similarity
to the real-world traﬃc. This is a missing meaningful benchmarking of these datasets that can
indicate their suitability for benchmarking the IDS algorithms and systems in the real-world
scenarios.

In this study we provide two sets of comparisons.

Initially we qualitatively investigate
distribution of these features by comparing their boxplots and Cumulative Distribution Functions
(CDFs), and later their embeddings, in this section. This enables us to observe the diﬀerence in
distribution of these features between the real-world traﬃc and testbed-based datasets. Then we
quantify these qualitative comparisons using quantitative distance metrics in the next section.

8

4.1. Qualitative Comparison of Feature Distributions

In this section, we are providing boxplot and CDF of all the features listed in Table 2 for all
ﬁve datasets, including the three testbed-based datasets and two real-world traﬃc records, side
by side.

4.1.1. Feature Variability Within the Dataset

Before comparing the feature distributions between groups, we investigated the feature dis-
tribution within various samples of the same dataset. Figure 1 shows two examples of comparing
feature distribution within the individual datasets. Figure 1-a and b show the distribution of
ﬂow durations for diﬀerent days of ISP and sampled UQ datasets (∼ 20 sample per minute)
respectively, and Figure 1-c and d show distribution of ﬂow sizes for diﬀerent days of ISP and

(a)

(c)

(b)

(d)

Figure 1: Comparing the ﬂow durations and ﬂow sizes within the two real world datasets. a) and b) show the ﬂow
durations of ISP, and sampled UQ dataset, and c) and d) show their ﬂow sizes correspondingly. In each ﬁgure,
the selected sample day is highlighted in blue color

9

(a)

(b)

Figure 2: Comparing the ﬂow duration in ﬁve datasets a) boxplots, and b) CDFs

sampled UQ datasets respectively. In all ﬁgures, the red circles show the median, the red crosses
show the mean, and the lines / bars show the standard deviation (STD) of the feature. The y
axis is shown in logarithmic scale for the ﬂow sizes (1-c and d) due to the presence of very large
ﬂows, which has compressed the main part of the feature range.

As seen, while there are variations in the distribution of features among diﬀerent days of both
datasets, the main characteristics of distributions are similar, i.e. means, and medians are very
close and STDs are almost in the same range. This clearly indicates that distribution of these
features is an inherent quality of these datasets and its comparison is indeed meaningful. In
addition, the it shows that the statistical parameters of selected sample day from each dataset,
which is highlighted in blue, is not signiﬁcantly diﬀerent from the rest of the dataset.

4.1.2. Flow Duration

Figure 2 shows the distribution of ﬂow duration for all ﬁve datasets. In Figure 2-a the three
left-most boxplots are the three synthetic datasets and two right-most are the real-world traﬃc
records. In all boxplots shown in this paper, the whiskers indicate a maximum distance of 1.5
times IQR above and below IQR. As seen, while the two real-world datasets have completely
diﬀerent IQR from all three synthetic datasets, they have signiﬁcant overlap. This can be further
conﬁrmed by Figure 2-b in which the CDFs of two real-world traﬃc records move close together
and far from the three synthetic datasets all the way to the right corner of the ﬁgure.

4.1.3. Flow Size (in Bytes)

The next feature distribution compared between these ﬁve dataset is the ﬂow size (in Bytes).
Figure 3 compares the distribution of ﬂow sizes in (a) by boxplots and in (b) by CDFs. While
in Figure 3-a there is a meaningful distinction between IQRs of the two synthetic datasets
TON IOT and UNSW NB15, and two real-world datasets, the IQR of CIC has a major overlap
In addition, the overlap between IQRs of the two real-world
with two real-world datasets.

10

CIC_IDSTON_IOTUNSW_NB15       ISPUQ0246810121416Flow Duration (Sec)0100101102Flow Duration (Sec)0.00.20.40.60.81.0Cumulative FrequencyCIC_IDSTON_IOTUNSW_NB15ISPUQ(a)

(b)

Figure 3: Comparing the ﬂow sizes (in Bytes) in ﬁve datasets a) boxplots, and b) CDFs

datasets is still signiﬁcant. This can be further conﬁrmed by investigating the CDF of ﬂow size
in these datasets as shown in Figure 3-b. Here, we have used logarithmic scale for the horizontal
axis. This is because of the very large outlier values that make the main part of the curves
very tiny. The conclusion from ﬁgure 3-a can be conﬁrmed here as well. The two real-world
datasets close together from the beginning to the end, CIC also closely moves together with
them, but TON IOT and UNSW NB15 have separate path. It worth to mention that we have
also computed similar graphs for the distribution of ﬂow sizes in number of packets, i.e. the
total number of packets in ﬂows (IP + OP), but since the result were very similar to ﬂow sizes
in Bytes (Figure 3) we gave up to include them.

4.1.4. Packet Time (average)

Figure 4 shows the distribution of packet times for the ﬁve datasets. The average packet
time / duration is computed by dividing ﬂow duration by total number of packets in ﬂow, as
shown in the Table 2. Since it takes into account, at the same time, the ﬂow duration and
number of packets, it reﬂects traﬃc characteristics in two dimensions time and volume. Again,
the similarity between two real-world datasets, and their distance to synthetic datasets is very
clear.

4.1.5. Packet Size

Figure 5 shows packet size distribution for the ﬁve datasets. Although it is hard to separate
the real-world and synthetic datasets in Figure 5-a, due to the major overlap between IQRs of
both groups, further investigation, as seen in Figure 5-b, reveals a pattern similar to previous
features. While the distinction between groups is clear, the diﬀerence, in this case, is not as
signiﬁcant as in the case of the previous features. Though, the phenomena of two real-world
datasets moving close together, separate from others, from beginning of the CDF till its end, is

11

CIC_IDSTON_IOTUNSW_NB15       ISPUQ020000400006000080000Flow Size (Byte)101102103104105106107108109Flow Size (Byte)0.00.20.40.60.81.0Cumulative FrequencyCIC_IDSTON_IOTUNSW_NB15ISPUQ(a)

(b)

Figure 4: Comparing the packet times in ﬁve datasets a) boxplots, and b) CDFs

still observed. The horizontal axis in Figure 5-b is again in logarithmic scale, to highlight the
main part of the graph, which is compressed due to the outliers with very large values.

4.1.6. Number of Source IPs per Destination IP

The next row of the Table 2 indicates a feature that is computed by counting number of
source IP addresses per each destination IP address. This feature is speciﬁcally important for
the detection and identiﬁcation of DDoS attacks. Figure 6 shows the distribution of this feature
for the ﬁve datasets in (a) using boxplots, and in (b) using CDFs. As in Figure 6-a, the two
real-world datasets have a completely similar IQR that is totally diﬀerent from UNSW NB15
and TON IOT, and the IQR of CIC dataset is overlapping with the real-world datasets. Figure

(a)

(b)

Figure 5: Comparing the packet sizes in ﬁve datasets a) boxplots, and b) CDFs

12

CIC_IDSTON_IOTUNSW_NB15       ISPUQ01234567Packet Duration (Sec)0100101Packet Duration (Sec)0.00.20.40.60.81.0Cumulative FrequencyCIC_IDSTON_IOTUNSW_NB15ISPUQCIC_IDSTON_IOTUNSW_NB15       ISPUQ0200400600800Packet Size (Byte)102103104Packet Size (Byte)0.00.20.40.60.81.0Cumulative FrequencyCIC_IDSTON_IOTUNSW_NB15ISPUQ(a)

(b)

Figure 6: Comparing the number of source IPs per destination IP in ﬁve datasets a) boxplots, and b) CDFs

6-b also conﬁrms this by showing that the CDF curves of the two real-world datasets move
close together from beginning to the end. The CDF curve of CIC starts close to the real-world
datasets but somewhere in the end of the range separates from them. However, the CDF curves
of the UNSW NB15 and TON IOT take a distinct path from early beginning to the end. These
results are completely inline with the previous results, i.e. results of the previous features. As
mentioned earlier, the results in this section includes the initial analysis and we have done the
precise statistical tests for all feature distribution comparisons, that their results will be provided
the results in the next subsection.

(a)

(b)

Figure 7: Comparing the number of source IP addresses per destination port in ﬁve datasets a) boxplots, and b)
CDFs

13

CIC_IDSTON_IOTUNSW_NB15       ISPUQ0510152025Number of Source Ips per Dest IP100101102103104Number of Source IPs per Dest IP0.00.20.40.60.81.0Cumulative FrequencyCIC_IDSTON_IOTUNSW_NB15ISPUQCIC_IDSTON_IOTUNSW_NB15       ISPUQ050100150200250300350Number of Source Ips per Dest Port100101102103104105Number of Source IPs per Dest Port0.00.20.40.60.81.0Cumulative FrequencyCIC_IDSTON_IOTUNSW_NB15ISPUQ4.1.7. Number of Source IPs per Destination Port

This feature is again important in detecting DoS and DDoS and scanning attacks. The
feature distribution for the ﬁve datasets are depicted in Figure 7-a in boxplots and in Figure 7-b
in CDFs. The distinction between the real-world and synthetic datasets is very clear in both
boxplots and CDF curves. In Figure 7-a, the three synthetic datasets (red boxplots) have very
small IQR, in contrast to two real-world datasets (blue boxplots), which have large overlapped
IQR. The CDF curves have exactly the same situation, i.e. the two real-world datasets are
relatively close together from beginning all the way to the end, and distinct from synthetic
datasets.

4.1.8. Number of Destination IPs per Source Port

Again, this feature is important in detecting DoS, DDoS and scanning attacks. The boxplots
and CDF curves of the feature distribution for the ﬁve datasets are depicted in Figure 8-a and 8-
b respectively. The situation of feature distribution is completely similar to the previous feature.
The two real-world datasets have very close distributions in both boxplots and CDFs, and they
are distinct from the synthetic datasets.

4.1.9. Number of Destination Ports per Source Port

This feature is also important in detecting DDoS and scanning attacks. The feature distri-
bution for the ﬁve datasets are depicted in boxplots (Figure 9-a ) and CDF curves (Figure 9-b).
Like the two previous features, the distributions of the feature in two real-world datasets are
very close in both boxplots and CDFs, and they are distinct from the synthetic datasets with an
exception of UNSW NB15 which gets close to the real-world datasets in parts of its CDF curve.

(a)

(b)

Figure 8: Comparing the number of destination IP addresses per source port in ﬁve datasets a) boxplots, and b)
CDFs

14

CIC_IDSTON_IOTUNSW_NB15       ISPUQ0200400600800Number of Dest Ips per Source Port100101102103104Number of Dest IPs per Source Port0.00.20.40.60.81.0Cumulative FrequencyCIC_IDSTON_IOTUNSW_NB15ISPUQ(a)

(b)

Figure 9: Comparing the number of destination ports per source port in ﬁve datasets a) boxplots, and b) CDFs

4.1.10. Number of L7 Protocols per Destination Port

This is the last feature used in this study for the comparison of the synthetic datasets with
the real-world traﬃc. While due to the lack of availability of L7 protocol in the ﬁelds exported
by NetFlow exporters in the past, it has not been utilised in the anomaly detection studies
referred in our study, it is an important feature in network security.
Intuitively, many well-
known protocols have been used with speciﬁc destination ports, such as the HTTP and port 80.
As such, the number of L7 protocols utilised with each port can be attributed to the normal
behavior, i.e. characteristics, of a network and changes in its distribution can be an indicator of
abnormal network situation.

The distribution of this feature for the ﬁve datasets are shown in Figure 10. The boxplots

(a)

(b)

Figure 10: Comparing the number of L7 protocols per destination port in ﬁve datasets a) boxplots, and b) CDFs

15

CIC_IDSTON_IOTUNSW_NB15       ISPUQ01020304050607080Number of Dest Ports per Source Port100101102103104105Number of Dest Ports per Source Port0.00.20.40.60.81.0Cumulative FrequencyCIC_IDSTON_IOTUNSW_NB15ISPUQCIC_IDSTON_IOTUNSW_NB15       ISPUQ0.02.55.07.510.012.515.0Number of L7-Protos Dest Port100101Number of L7-Protos per Dest Port0.00.20.40.60.81.0Cumulative FrequencyCIC_IDSTON_IOTUNSW_NB15ISPUQare shown in Figure 10-a in which the three synthetic datasets are depicted in red color in
the left most side and the two real-world datasets are depicted in blue in the right-most side
of the ﬁgure. As seen, the IQRs of the two real-world datasets are totally separate from the
synthetic datasets. Although the real-world datasets seem to have non-overlapping IQRs, their
both diﬀerences to synthetic datasets’ IQRs are stronger than their diﬀerence. Similar situation
can be understood from the CDF curves as shown in Figure 10-b that conﬁrms relative closeness
of the real-world datasets compared to synthetic datasets. While this intuitive analysis gives us
a sense of the feature distribution comparison between these ﬁve datasets, the results provided
by running Kruskal-Wallis test in the next subsection will provide the ﬁnal ﬁndings.

4.2. Comparing Dimensionality Reduced Feature Distributions

Here we visualize the feature distributions after applying various methods of embedding, on
the set of all nine features listed in Table 2, and compare the ﬁve datasets. For this purpose, we
use four diﬀerent methods of dimensionality reduction techniques including Linear Discriminant
Analysis (LDA), Multi-dimensional Scaling(MDS), Spectral Embedding and Principle Com-
ponent Analysis (PCA). The resulting embeddings of features after applying dimensionality
reduction on each dataset are plotted in Figure 11-a, b, c and d respectively. Embeddings of
each dataset is plotted using a diﬀerent marker and color to illustrate the diﬀerences of their
distributions.

In Figure 11-a, LDA is the dimensionality reduction method applied to the features. The
horizontal axis represents the ﬁrst embedded component and the vertical axis represents the
second embedded component. As per previous ﬁgures, dark and light blue colors are used for
UQ and ISP and the orange, light and dark red are used for UNSW˙NB15, TON˙IOT and
CIC˙IDS respectively. As seen, the embedded points from the two real-world datasets are vastly
spread in the direction of the ﬁrst embedded component, while the embedded points of the
synthetic datasets are mostly located in the area of small values. Similar results can be seen in
Figure 11-d, where PCA is applied for dimensionality reduction and the horizontal and vertical
axes represent the ﬁrst and second principle components respectively.

The other two embedding show another pattern in which the real-world and synthetic
datasets are separated across both embedded components.
In Figure 11-b and Figure 11-c,
which illustrate the results of MDS and spectral embedding dimensionality reduction methods,
the horizontal and vertical axes represent the ﬁrst and second embedded component respectively.
In both ﬁgures, the embedded points of the synthetic datasets are mostly accumulated in a small
area of the surface, while the embedded points of the real-world datasets are spread in much
larger area. These results are another indication of the vast diﬀerences between the statistical
features of the real-world datasets and the normal part of these synthetic datasets.

16

(a)

(c)

(b)

(d)

Figure 11: Embedding samples all the ﬁve datasets using a) Linear Discriminant Analysis (LDA), b) Multi-
dimensional Scaling(MDS), c) Spectral Embedding, and d) Principle Component Analysis (PCA)

5. Quantifying the Distance Between Distributions

In the previous section we have shown that distributions of traﬃc characteristics features
of the benign / attack-free parts of the three selected IDS datasets are statistically diﬀerent
from the real-world traﬃc. This distribution diﬀerence is visualised for several features that are
commonly used for implementing anomaly and intrusion detection systems, in terms of their
CDF and boxplot comparison. While these visualizations are clear indications of the diﬀerence
between the two groups of datasets, they do not provide a measure of how an individual dataset
is far from the other one.

In this section, we are quantifying the diﬀerence between these distributions. There are a
range of metrics that can be used to measure the distance between two distributions / prob-
ability density functions as reviewed in [16]. Each metrics comes with its assumptions about
distributions, which deﬁnes the accuracy of the measurement. The distance metric that we are

17

(a)

(b)

(c)

(d)

(e)

(f)

Figure 12: Wasserstein distances between the samples of the ﬁve datasets in terms of a) ﬂow duration, b) ﬂow
size, c) (average) packet time, d) packet size, e) number of destination ports per source port, and f) number of
destination IP addresses per source port

using in this section is Wasserstein distance, which is also known as the Earth Mover’s distance,
and is commonly used in the machine learning applications. The Wasserstein distance W of two
distribution u and v is given by [7]

W (u, v) = inf

π∈Γ(u,v)

(cid:90)

R×R

|x − y|dπ(x, y)

(1)

where Γ(u, v) is the set of (probability) distributions on R × R whose marginals are u and v on
the ﬁrst and second factors respectively. The inf stands for inﬁmum, also known as the greatest
lower bound. If S is a subset of set T (partially ordered), then inf (S) is the greatest element of
T which is less than or equal to all elements of S. Th Wasserstein distance of u and v can also
be stated in terms of their CDFs, U and V as [7]

|U − V |

(2)

W (u, v) =

(cid:90) +∞

−∞

18

UQ       ISPTON_IOTUNSW_NB15CIC_IDSUQ    ISPTON_IOTUNSW_NB15CIC_IDS00.250.480.510.170.2500.620.650.410.480.6200.0950.360.510.650.09500.410.170.410.360.4100.00.10.20.30.40.50.6Wasserstein DistanceUQ       ISPTON_IOTUNSW_NB15CIC_IDSUQ    ISPTON_IOTUNSW_NB15CIC_IDS00.0830.260.30.110.08300.290.30.0820.260.2900.220.290.30.30.2200.340.110.0820.290.3400.000.050.100.150.200.250.30Wasserstein DistanceUQ       ISPTON_IOTUNSW_NB15CIC_IDSUQ    ISPTON_IOTUNSW_NB15CIC_IDS00.0710.520.570.290.07100.480.530.250.520.4800.090.310.570.530.0900.370.290.250.310.3700.00.10.20.30.40.5Wasserstein DistanceUQ       ISPTON_IOTUNSW_NB15CIC_IDSUQ    ISPTON_IOTUNSW_NB15CIC_IDS00.10.250.210.210.100.270.210.150.250.2700.360.250.210.210.3600.330.210.150.250.3300.000.050.100.150.200.250.300.35Wasserstein DistanceUQ       ISPTON_IOTUNSW_NB15CIC_IDSUQ    ISPTON_IOTUNSW_NB15CIC_IDS00.0140.780.760.820.01400.790.770.830.780.7900.160.250.760.770.1600.140.820.830.250.1400.00.10.20.30.40.50.60.70.8Wasserstein DistanceUQ       ISPTON_IOTUNSW_NB15CIC_IDSUQ    ISPTON_IOTUNSW_NB15CIC_IDS00.0420.690.740.690.04200.670.710.670.690.6700.290.120.740.710.2900.310.690.670.120.3100.00.10.20.30.40.50.60.7Wasserstein Distancewhich in the case of our study can be applied to CDFs of features computed in Section 4.1.

Figure 12 shows the Wasserstein distances between each pair of the ﬁve datasets that are used
in this study, in the form of a heatmap diagram, for six features. In Figure 12-a the distance
between ﬂow duration distributions is measured. The value of the Wasserstein distances are
shown on each entry, in addition to visualising by color. Since the Wasserstein distance metric
is symmetric, distance from distribution a to b is the same as distance from b to a. For instance,
the distance between UNSW˙NB15 and UQ and vice versa is 0.51. As seen, except CIC-IDS,
the distances between synthetic datasets and the real-world datasets is bigger than real-world
to real-world distances.

Figure 12-b, c, d, e, and f show the Wasserstein distances for the distribution of ﬂow size,
packet time, packet size, number of destination ports per source ports and number of destination
IPs per source port, respectively. The diﬀerences between the real-world and all synthetic
datasets are more signiﬁcant for the rest of features.

In order to summarise the overall diﬀerences between these datasets in a single number,
the Wasserstein distances of all features are calculated and averaged. Figure 13-a shows the
averaged Wasserstein distance over all features listed in Table 2. Since the main subject in this
paper is the distance between the real-world and synthetic datasets, these values are summarised
in Figure 13-b. The horizontal axis indicates the distance to UQ dataset, and the vertical axis
indicates distance to ISP dataset. As distance of UQ to UQ dataset is zero, it is placed on
vertical axis (x = 0), similarly the ISP dataset is placed on horizontal axis (y = 0). In this way,
the coordinates of each dataset represents its averaged distances to the two real-world datasets.
As seen, the CIC-IDS dataset is closest dataset to UQ and ISP, and TON˙IOT and UNSW˙NB15
are placed in farther locations. The main purpose of this representation is to clearly show the

(a)

(b)

Figure 13: Averaged distances between datasets over all 9 features in Table 2 using a) heatmap , and b) distance
to UQ and ISP

19

UQ       ISPTON_IOTUNSW_NB15CIC_IDSUQ    ISPTON_IOTUNSW_NB15CIC_IDS00.120.390.430.320.1200.390.490.370.390.3900.310.290.430.490.3100.320.320.370.290.3200.00.10.20.30.4Wasserstein Distance0.00.10.20.30.4Wesserstein Distance (to UQ)0.00.10.20.30.40.5Wesserstein Distance (to ISP)0.1250.1250.3920.3890.4330.4940.3150.370UQISPTON_IOTUNSW_NB15CIC_IDSdistances between the real-world and synthetic datasets. While the two real-world datasets are
placed in small distances from each other, all the three synthetic datasets are in much farther
distances.

6. Related Works

In order to select the previous works regarding the subject, we have considered three aspects.
First the role of network traﬃc characteristics, i.e features of network traﬃc that aﬀect normal
network behavior. Then, we looked for those features that are considered for detecting and
identifying network traﬃc anomalies. Finally, we looked for other works in the ﬁeld of IDS
dataset evaluation and summarised these works method and how they evaluated the publicly
available IDS datasets.

6.1. Network Traﬃc Characteristics

Many previous works that discuss characteristics of network traﬃc such as [17] and [18]
focus on traﬃc volume variations in time to explain main features of network traﬃc. In [19],
they use Principle Component Analysis (PCA) to investigate the the origin-destination ﬂows of
network as an essential part of network traﬃc modelling, and ﬁnding solution to a wide variety
of problems including traﬃc engineering, capacity planning and anomaly detection. While
timeseries and time variations of traﬃc volume play a signiﬁcant role in many aspects of network
such as design and implementation, there are other features of network traﬃc that are equally
important when discussing traﬃc characteristics. This has been clearly illustrated in two other
previous studies [20] and [21] which investigate network traﬃc not only via its timeseries, but
also by analysing its statistical distributions. In [20] by collecting a petabyte of measurement
data over two months, they discovered and reported traﬃc characteristics patterns. They not
only investigated the features related to traﬃc volume, but they also studied other features such
as ﬂow duration and ﬂow inter-arrival time.

In[21], they studied the traﬃc characteristics of 10 distinct datacentre networks with diﬀer-
ent organisational administrations such as university, enterprise, and cloud services providers.
They studied the traﬃc patterns of these datacentres by investigating the ﬂow and packet-level
properties of various layer-7 applications, and the impact of these applications on network con-
gestion and link and network utilisation. In their study, they investigated a range of packet and
ﬂow statistical distributions such as number of ﬂows per second, ﬂow inter-arrival time, ﬂow
size, ﬂow duration, and packet size. Unlike previous works, they not only took into account the
time domain distribution of the traﬃc features, but they also considered the statistical measures
such as Cumulative Distribution Function (CDF). In addition, they investigated these measures
per layer 7 applications, and provided the distribution of corresponding statistical measures per
various layer 7 applications. They use these measures to understand the normal patterns of
network traﬃc in various levels such as edge, aggregation and core links.

20

6.2. Network Anomalies Characteristics

The next group of studies investigate the network traﬃc characteristics to understand abnor-
mal network behavior and detect anomalies. In this group of studies, various statistics, measures
and distributions of several network traﬃc features have been utilised for detecting abnormal
network behaviors. They not only take into account a broader range of network traﬃc features,
they also consider the statistical measures and distributions of these features. Furthermore,
some of the studies in this group apply their method on traﬃc records collected from the real
world, mainly backbone networks, which is a stronger evaluation of their proposed method.

In [13], this argument is explained by illustrating the role and importance of the distribu-
tions of packet features, such as IP addresses and ports observed in ﬂow traces, in detecting
and identifying the structure of a wide range of network anomalies. They state that clustering
network traﬃc based on the distribution of these features creates meaningful clusters of anoma-
lies and such clusters can be utilised for detecting new anomalies and automatic classiﬁcation
of anomalies. In this way, by investigating these distributions, they are able to not only detect
the volume-based anomalies, but also detect many other anomalies that do not change traﬃc
volume signiﬁcantly. They validate their proposed methods on data collected from two back-
bone networks (Abilene and Geant) and conclude that feature distributions is a key ingredient
in general network anomaly detection framework.

In [14], they monitored anomalies and collected network traﬃc records of the same backbone
networks (Abilene and Geant), in order to determine which network parameters aﬀect detectabil-
ity of network anomalies, and how these parameters inﬂuence the characteristics of anomalies.
They recorded 3 weeks of traﬃc and routing data from both networks and detected three speciﬁc
anomalies by applying Kalman ﬁlter. While they stated that detecting anomalies is signiﬁcantly
dependent on the network design, monitoring infrastructure, and anomaly-detection technique,
they investigated variations of the entropy of four features of network traﬃc, namely source
and destination IP addresses and source and destination (L4) Ports. They concluded that it is
not possible to detect all anomalies in a network, based on a single method for Internet-Wide
anomaly detection.

In the last study of this group, based on the idea proposed in [13], using the traﬃc fea-
ture distribution for anomaly detection, histograms of eight network traﬃc features have been
investigated [15]. These features include the source and destination IP addresses, source and
destination (L4) ports, TCP ﬂags, (L4) protocol number, packet size, and ﬂow duration. While
they have listed the possible beneﬁts of each feature for detecting speciﬁc type of anomaly,
they stated that combination of features can reveal changes in the network traﬃc, which would
be invisible otherwise. They applied their proposed method on the data collected from one
datacentre, and one campus network and an IDS benchmark dataset.

The results of these studies, collectively, show that network traﬃc features distribution play a
signiﬁcant role in the detection of network anomalies in both real world and benchmark datasets.
However, this has rarely been investigated in the evaluation of publicly available datasets. In
the next subsection, we brieﬂy explore the available review papers that have evaluated these
benchmark datasets.

21

6.3. IDS Dataset Evaluation

While there are many publicly available IDS benchmark datasets that have been frequently
utilised for the evaluation of new and existing IDS algorithms, studies that evaluate these
datasets themselves are very rare. In this subsection we explore the two studies that we have
found in this space.

The ﬁrst study investigates usefulness of DARPA dataset for the evaluation of IDSs [22].
They use two signature-based IDSs, Snort [23] and Cisco IDS, along with two anomaly detection
methods to evaluate DARPA datasets based on the methodology proposed by MIT Lincoln
Laboratory for IDS evaluation. Their results indicate that this dataset is useful for the evaluation
of intrusion detection systems. Since this evaluation includes only a single dataset, and it is
rather an old study investigating even an older dataset, the results cannot be generalised for
other datasets.

The report in [24] is the only study we found evaluating multiple IDS datasets. They
systematically evaluate 11 publicly accessible IDS datasets published between 1998 till 2016, and
conclude that most of these datasets are out of date and unreliable for the evaluation of the IDS
algorithms. By investigating the shortcoming of the existing datasets, they provide a framework
for the evaluation of IDS datasets. This framework consists of 11 features which a benchmark
IDS dataset should possess, and hence deﬁne a corresponding score for each of their studied
benchmark datasets. These features include complete network conﬁguration, complete traﬃc,
labeled dataset, complete interaction, complete capture, available protocols, attack diversity,
anonymity, heterogeneity, feature set, and metadata. The main approach in deﬁning these
features, which are proposed based on the observations of the existing benchmark datasets, is
to prevent the previous dataset shortcomings. However, it is not discussed how having these
features guarantees the ﬁtness or enhances the ﬁtness of a synthetic IDS dataset for the real-
world evaluation scenarios.

As such, we are proposing for the ﬁrst time, a criteria for a benchmark dataset, that can
be used to evaluate the similarity of a synthetic / testbed-based IDS dataset to the real-world
network traﬃc records. We explain this methodology in the coming sections by ﬁrst explaining
the datasets utilised in our study.

7. Conclusion

The benchmark datasets for Network Intrusion Detection Systems (NIDSs) are commonly
used by academic and industrial network security researchers for the evaluation of the anomaly
detection and NIDS algorithms. In most of the methods based on the machine learning tech-
niques, statistical features of the network traﬃc play a signiﬁcant role in the performance of
the classiﬁcation of normal and anomalous traﬃc. However, these benchmark datasets have
never been analysed in terms of the statistical features of their traﬃc records. The statistical
distributions of the attack / anomaly traﬃc features are usually diﬀerent from the normal traf-
ﬁc. As such, the normal / benign traﬃc records of these datasets are the main parts which

22

should simulate / mimic normal / benign the real-world traﬃc in which the anomaly detection
algorithms are supposed to work.

The main purpose of this paper is to introduce tools and methodologies that can measure
how realistic is the evaluation of an NIDS algorithm via a synthetic dataset. Currently, this has
not been addressed in the network security research ﬁeld. This paper, tries to address this gap
not only by proposing the required tool and methodology, but also by applying the proposed
methodology on three recent NIDS datasets, and comparing them to two diverse real-world
network traﬃc datasets.

We initially propose nine traﬃc features that can be used for this purpose. Then, we illustrate
the statistical distributions of these features for the synthetic and real-world datasets. We also
use four diﬀerent dimensionality reduction methods to embed the set of nine features in a 2-
dimensional Euclidean space and visualise the resulting embeddings for all datasets. In both
cases, the statistical distributions and dimensionality reduced visualisations, there are signiﬁcant
distinctions between the two groups of datasets, the real world and synthetic. In addition, the
diﬀerences among members of each group is much less signiﬁcant, e.g. the two real-world datasets
are more similar together than to the synthetic datasets, despite their diverse origin.

While the illustrated distributions clearly indicate the diﬀerences in the traﬃc features of
the synthetic and real-world datasets, diﬀerences between the real-world datasets and each of
the synthetic datasets varies from one dataset to another. We propose a metric to quantify these
diﬀerences between statistical distributions of features of each pair of datasets. The proposed
metric also clearly indicates that diﬀerence between the two real-world datasets, despite the
fact that they represent two diﬀerent types of networks from two diﬀerent organisation and
geographical-location, is much less than to either of synthetic datasets.

Based on these results, we believe that the evaluation of anomaly detection and NIDS algo-
rithms using synthetic datasets, which are not created in the context of real-world traﬃc, does
not guarantee their classiﬁcation performance in the real-world scenarios. The addressing of
this gap constitutes the future work of our research where we are proposing a methodology for
creating NIDS benchmark datasets that their traﬃc feature distributions comply with that of
the real-world traﬃc.

References

[1] University of California, Irvine, “KDD Cup 1999 Data.” http://kdd.ics.uci.edu/

databases/kddcup99/kddcup99.html, 1999. Accessed: 2020-07-30.

[2] N. Moustafa and J. Slay, “UNSW-NB15: A Comprehensive Data set for Network Intrusion
Detection systems (UNSW-NB15 Network Data Set),” in Military communications and
information systems conference (MilCIS) (pp. 1-6). IEEE., pp. 1–6, IEEE, 2015.

[3] I. Sharafaldin, A. H. Lashkari, and A. A. Ghorbani, “Toward generating a new intrusion
detection dataset and intrusion traﬃc characterization,” ICISSP 2018 - Proceedings of

23

the 4th International Conference on Information Systems Security and Privacy, vol. 2018-
Janua, pp. 108–116, 2018.

[4] N. Moustafa, “ToN IoT datasets.” http://dx.doi.org/10.21227/fesz-dm97, 2019.

[5] M. Sarhan, S. Layeghy, N. Moustafa, and M. Portmann, “Netﬂow datasets for machine
learning-based network intrusion detection systems,” in Big Data Technologies and Appli-
cations, (Cham), pp. 117–135, Springer International Publishing, 2021.

[6] M. Sarhan, S. Layeghy, N. Moustafa, and M. Portmann, “Towards a standard feature set

of nids datasets.” https://arxiv.org/abs/2101.11315, 2021.

[7] A. Ramdas, N. G. Trillos, and M. Cuturi, “On wasserstein two-sample testing and related

families of nonparametric tests,” Entropy, vol. 19, no. 2, p. 47, 2017.

[8] Monowar H. Bhuyan, Dhruba Kumar Bhattacharyya, and Jugal Kumar Kalita, “Network
Anomaly Detection: Methods, Systems and Tools,” IEEE Communications Surveys & Tu-
torials, vol. 16, no. 1, pp. 303–336, 2014.

[9] Hung-Jen Liao, Chun-Hung Richard Lin, Ying-Chih Lin, Kuang-Yuan Tung, H.-j. Liao,
C.-h. R. Lin, Y.-c. Lin, and K.-y. Tung, “Intrusion detection system: A comprehensive
review,” Journal of Network and Computer Applications, vol. 36, no. 1, pp. 16–24, 2013.

[10] Monowar H. Bhuyan, Dhruba K. Bhattacharyya, and Jugal K. Kalita, Network Traﬃc
Anomaly Detection and Prevention Concepts, Techniques, and Tools. Springer, 2017.

[11] M. H. Bhuyan, D. K. Bhattacharyya, and J. K. Kalita, “Network Anomaly Detection:
Methods, Systems and Tools,” IEEE Communications Surveys & Tutorials, vol. 16, no. 1,
pp. 303–336, 2014.

[12] Ntop, “nProbe, An Extensible NetFlow v5/v9/IPFIX Probe for IPv4/v6.” https://www.

ntop.org/guides/nprobe/cli_options.html, 2017.

[13] A. Lakhina, M. Crovella, and C. Diot, “Mining anomalies using traﬃc feature distributions,”

SIGCOMM Comput. Commun. Rev., vol. 35, no. 4, pp. 217–228, 2005.

[14] A. Soule, H. Ringberg, F. Silveira, J. Rexford, and C. Diot, “Detectability of traﬃc anoma-
lies in two adjacent networks,” in International Conference on Passive and Active Network
Measurement, vol. 4427 LNCS, pp. 22–31, Springer Berlin Heidelberg, 2007.

[15] Andreas Kind, Marc Ph. Stoecklin, and Xenofontas Dimitropoulos, “Histogram-based traﬃc
anomaly detection,” IEEE Transactions on Network and Service Management, vol. 6, no. 2,
pp. 110–121, 2009.

24

[16] Sung-Hyuk Cha, “Comprehensive Survey on Distance/Similarity Measures between Prob-
ability Density Functions,” International Journal of Mathematical Models and Methods in
Applied Sciences, vol. 1, no. 4, pp. 300–307, 2007.

[17] Kevin Thompson, Gregory J. Miller, and Rick Wilder, “Wide-area Internet traﬃc patterns

and characteristics,” IEEE Network, vol. 11, no. 6, pp. 10–23, 1997.

[18] L. Liu, H. H. Jiang, W. Z. Rui, and J. Wang, “Study on the characteristics of network
traﬃc based on STFT,” Proceedings - 2015 2nd International Conference on Information
Science and Control Engineering, ICISCE 2015, pp. 485–488, 2015.

[19] Anukool Lakhina, Konstantina Papagiannaki, Mark Crovella, Christophe Diot, Eric D.
Kolaczyk, and Nina Taft, “Structural analysis of network traﬃc ﬂows,” SIGMETRICS
Perform. Eval. Rev., vol. 32, no. 1, pp. 61–72, 2004.

[20] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and R. Chaiken, “The nature of dat-
acenter traﬃc: Measurements & analysis,” Proceedings of the ACM SIGCOMM Internet
Measurement Conference, IMC, pp. 202–208, 2009.

[21] Theophilus Benson, Aditya Akella, and David A Maltz, “Network traﬃc characteristics
of data centers in the wild,” in Proceedings of the 10th ACM SIGCOMM conference on
Internet measurement, pp. 267–280, ACM, 2010.

[22] C. Thomas, V. Sharma, and N. Balakrishnan, “Usefulness of DARPA dataset for intrusion
detection system evaluation,” Data Mining, Intrusion Detection, Information Assurance,
and Data Networks Security 2008, vol. 6973, no. March 2008, p. 69730G, 2008.

[23] M. Roesch, “Snort – Lightweight Intrusion Detection for Networks,” in Proceedings of LISA
99: 13th Systems Administration Conference, (Seattle, Washington, USA), pp. 229–238,
1999.

[24] A. Gharib, I. Sharafaldin, A. H. Lashkari, and A. A. Ghorbani, “An Evaluation Framework
for Intrusion Detection Dataset,” in ICISS 2016 - International Conference on Information
Science and Security 2016, no. Cic, pp. 0–4, IEEE, 2017.

25

