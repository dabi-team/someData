1
2
0
2

r
a

M
3
2

]

R
C
.
s
c
[

1
v
5
6
3
2
1
.
3
0
1
2
:
v
i
X
r
a

Risk Analysis and Policy Enforcement of Function Interactions
in Robot Apps

Yuan Xu1, Tianwei Zhang2, Yungang Bao1

1ICT, CAS

2Nanyang Technological University

ABSTRACT
Robot apps are becoming more automated, complex and diverse.
An app usually consists of many functions, interacting with each
other and the environment. This allows robots to conduct various
tasks. However, it also opens a new door for cyber attacks: adver-
saries can leverage these interactions to threaten the safety of ro-
bot operations. Unfortunately, this issue is rarely explored in past
works.

We present the ﬁrst systematic investigation about the function
interactions in common robot apps. First, we disclose the potential
risks and damages caused by malicious interactions. We introduce
a comprehensive graph to model the function interactions in ro-
bot apps by analyzing 3,100 packages from the Robot Operating
System (ROS) platform. From this graph, we identify and catego-
rize three types of interaction risks. Second, we propose RTron,
a novel system to detect and mitigate these risks and protect the
operations of robot apps. We introduce security policies for each
type of risks, and design coordination nodes to enforce the policies
and regulate the interactions. We conduct extensive experiments
on 110 robot apps from the ROS platform and two complex apps
(Baidu Apollo and Autoware) widely adopted in industry. Evalu-
ation results indicated RTron can correctly identify and mitigate
all potential risks with negligible performance cost. To validate the
practicality of the risks and solutions, we implement and evaluate
RTron on a physical UGV (Turtlebot) with real-word apps and en-
vironments.

1 INTRODUCTION
The robotics technology is rapidly integrated into every aspect of
our life. Diﬀerent types of robots and applications were designed
to assist humans with many dangerous or tedious jobs. A robot app
usually consists of multiple processes (a.k.a. nodes), with each one
focusing on one speciﬁc function, e.g., localization, path planning.
They interact with each other to complete the end-to-end task.

To ease the development of robot apps, many companies ex-
pose interfaces of massive functions for their products (e.g. Ford
Open XC [1], Dji Onboard SDK [2], UR Application Builder [3]).
Developers can then use these functions to create new apps. Al-
ternatively, public platforms are introduced, where functions are
developed in a crowd-sourcing manner by third-party developers
and distributed through the open-source function markets. The
most mainstream platform is the Robot Operating System (ROS)
[4], which provides thousands of open-source robot functions. Func-
tions from this platform have been widely adopted in the research
community and many commercial products, such as Dji Matrice
200 drone [2], PR2 humanoid [5] and ABB manipulator [6].

However, these functions can be the Achilles’ Heel of robot apps,
threatening the safety of robot operations. There are two reasons

1

that facilitate this hazard. (1) Public platforms like ROS allow third-
party developers to share their functions. Diﬀerent from other well-
developed app stores (e.g., mobile devices [7, 8], PCs [9–11], IoT
[12–14]), the ROS platform does not enforce any security inspec-
tion over the submitted code. An adversary can easily upload mali-
cious functions to the platform for users to download. This threat
is practical but severe: we successfully demonstrate the feasibility
of uploading malicious functions to the ROS platform and compro-
mising any robot apps built upon them (§ 7.4 and Appendix F.1). (2)
Function nodes in a robot app have dynamic and frequent interac-
tions with each other and the physical environment. Even one ma-
licious node can aﬀect the states and operations of the entire app,
leading to severe privacy breach and physical damages [15, 16]. For
instance, Chrysler Corporation recalled 1.4 million vehicles in 2015
due to a software vulnerability in its Uconnect dashboard comput-
ers [17]. An adversary could exploit it to hack into a jeep remotely
and take over the dashboard functions.

To ensure the safety of robot apps, it is critical to protect the in-
teractions among various functions inside the apps. We are inter-
ested in two questions: What potential risks and security incidents
can a malicious interaction bring? How can we detect and mitigate
malicious interactions? Unfortunately, there are currently few stud-
ies focusing on the interactions in robot apps. Security analysis of
interactions in IoT systems have been explored [18–27]. As robot
apps have more complex and distinct features, it is hard to apply
the above methods to the robot ecosystem, as discussed in § 8.

In this paper, we present the ﬁrst study to explore the func-
tion interactions in common robot apps from the perspective of
security and safety. We make three contributions to answer the
above two questions. First, we introduce a comprehensive interac-
tion graph to model all the interactions in robot apps. Although
numerous apps have been implemented for various robot devices
and tasks, there are still no systematic summaries about the char-
acteristics of function node interactions. To achieve this, we im-
plement a rule-based tool to automatically analyze 3100 packages
from the ROS platform, categorize them into 17 types and build a
graph to cover all possible interactions. We also select 110 popular
robot apps from the ROS platform to verify our interaction graph.
This model lays a foundation for our risk analysis and mitigation in
this paper, and also aims to enhance people’s understanding about
the characteristics of robot apps for other purposes in the future.
Second, we analyze potential risks from those interactions based
on our graph model. We classify these risks into three types. (1)
General Risk: it happens when multiple function nodes share same
states, and malicious nodes attempt to compromise the states by
sending wrong messages. (2) Robot-Speciﬁc Risk: this is caused by
the conﬂict between the robot’s velocity and the frame rate of the
image recognition function. (3) Mission-Speciﬁc Risk: this refers to
the violation of users’ expectation regarding the safe and secure

 
 
 
 
 
 
behaviors of the robot system. We provide detailed analysis and
examples to show the possible consequences of each risk.

Third, we introduce RTron, a novel system to detect and mit-
igate risks caused by suspicious interactions in robot apps. The
core of RTron is a set of coordination nodes, which are used to
regulate the interactions and enforce security policies. We design
a coordinate node with some security policies to mitigate each
type of risks. Speciﬁcally, RTron includes two stages. At the devel-
opment stage, it generates the interaction graph from the source
code of the robot app, and help developers discover all high-risk
function nodes, which may trigger potential malicious interactions.
Based on the generated risk information, RTron deploys coordi-
nation nodes along with these high-risk function nodes. This is
achieved without changing the original function node. At the oper-
ation stage, RTron deploys a security service to keep monitoring
all the information from the coordination nodes. A visualized inter-
face is provided to end users to observe the high-risk interactions.
If a risk occurs, the corresponding coordination node will enforce
the desired policy conﬁgured by users during the app launch to
mitigate it.

We conduct extensive experiments to evaluate the eﬀectiveness,
eﬃciency and practicality of RTron. (1) We select 110 robot apps
from the ROS platform, covering 24 robots of 4 types. RTron can
correctly identify all potential risks from three types of vulnerable
interactions, with negligible overhead at both the oﬄine and online
stages. (2) We perform large-scale evaluations on more complex
and practical robot apps: we select 2 apps from the ROS platform
for the home and autorace scenarios, each containing 10 functions
to perform 6 tasks; we also deploy 2 self-driving apps (Autoware
[28] and Apollo [29]), which are widely adopted in the autonomous
vehicle industry. RTron successfully identiﬁes 198 high-risk inter-
actions in these 4 apps, and mitigates them promptly and eﬀec-
tively. (3) We demonstrate a practical end-to-end attack, from up-
loading malicious functions to the public ROS platform, developing
vulnerable robot apps, to compromising a physical robot (Turtlebot
UGV) and environment. We show this threat can be eliminated by
RTron.

2 BACKGROUND & THREAT MODEL
2.1 Interaction in Robot Apps
Robot apps run on the embedded computer of a robot device to
interpret sensory data collected from the environment, and make
the corresponding action decisions. The workﬂow of a robot app
can be represented as a directed graph, where each node repre-
sents a certain function, and edges represent the dependencies of
the functions in this app. Figure 1 shows a navigation app as an
example. This robot app is composed of three major processing
stages [30]: (1) Perception: the robot extracts estimated states of
the environment and the device from raw sensory data. It uses
the Localization node to determine the device position, and the
CostmapGen node to model the surroundings. (2) Planning: the ro-
bot determines the long-range actions. It uses the Path Planning
node to ﬁnd the shortest path, and the Exploration node to search
for all accessible regions. The Exploration node also exposes an
external service for users to launch a navigation mission. (3) Con-
trol: the robot processes the execution action and forwards these

ENV

Navigation App

Costmap
Gen

Localiza
tion

Explore
Map

Global
Map

Pose

Explora
tion

Goal

Path
Planning

Velocity
Driver

Velocity

Path
Tracking

Local
Map

Path

SENSORS

PERCEPTION

PLANNING

CONTROL

ACTUATORS

Figure 1: An example of the navigation app.

motions to the actuators. It uses the Path Tracking node to pro-
duce velocity commands following the planned path, and the Velocity
Driver node to convert the velocity to instructions for the motor
to drive the wheels.

One big feature of robot apps is the high interactions among
various function nodes in the workﬂow. Based on the triggered
events, the interactions can be classiﬁed into two groups:
Direct interaction (solid line). This denotes the interaction be-
tween two functions (ellipses), which are directly connected in the
workﬂow and sharing common robot states (squares). Robot states
are deﬁned as the collection of all aspects and knowledge of the de-
vice that can impact future behaviors [31], e.g., position, orienta-
tion, explored maps. The computation of one function can change
some robot states, which will aﬀect the computation of another
function. For instance, in Figure 1, the action of Path Planning
is triggered by the event that Localization generates the robot’s
current position and orientation. Then the two nodes have direct
interaction over the robot states of position and orientation.
Indirect interaction (dotted line). This refers to the dependency
of two functions, which are not connected in the workﬂow, but
can interact with each other via the environmental context. One
node in the app can issue actions to change the environmental
context (e.g., obstacles, space, etc.), which will further inﬂuence
another node. In the navigation app, the functions in the Control
stage generate commands to control the robot to change the phys-
ical environment. This triggers the functions in the Perception to
conduct new computations. For instance, the map created by the
CostmapGen function depends on the action from the Path Tracking
function. As a result, these two function nodes are indirectly inter-
acted, although they are not directly connected in the workﬂow.

2.2 Robot App Platform
In robotics, the most popular app platform is Robot Operating Sys-
tem (ROS) [4]. Both the research community and industry widely
adopt ROS as the foundation or the testbed for their apps, such as
Dji Matrice 200 drone [2], PR2 humanoid [5] and ABB manipula-
tor [6]. In this paper, we mainly focus on the ROS platform. Our
methods and conclusions can be generalized to other platforms as
well.

The ROS platform oﬀers two kinds of services. First, it provides
robot core libraries, which act as the middleware between robot
apps and hardware. These core libraries support hardware abstrac-
tion, message passing mechanisms and device drivers for hundreds
of sensors and motors. Second, the ROS platform maintains thou-
sands of robot code repositories (a.k.a. repos) for distributed version
control, code management and sharing. A repo commonly includes
one or more packages maintained on the hosting site (e.g. GitHub,

Cloud

ROS Platform

3rd-party 
functions

Download
1

4
Launch 
Mission

customized 
functions

Topics

Development

Developer

Integration
2

APP
ROS Core Lib

3

Deploy Robot Apps

Services

5

Mission
Execution

Operation

User

Phone

Feedback
6

Robot

Environment

Figure 2: The lifecycle of robot app development (blue parts)
and operation (green parts).

BitBUcket, GitLab). The developers can add their repos to the ROS
platform through sending a pull request to the ROS maintainer. If
it succeeds, both the repos and included packages can get speciﬁc
indexes for other developers to download and use. Detailed infor-
mation regarding the ROS platform and repos can be found in Ap-
pendix B.1. This work mainly targets this service, and shows that
untrusted repos from the ROS platform can signiﬁcantly threaten
the robot apps built from them.

Figure 2 illustrates the key concepts and components in the life-
cycle of robot app development and operation. First, the design of
the app is decomposed into several necessary functions. Among
them core functions (white ellipses) need to be customized by the
developer, while non-core functions (black ellipses) can be down-
loaded from ROS code repos (❶). Then the developer uses ROS core
libraries to organize these functions as an app workﬂow (❷) and
deploys the app to the robot (❸). Each function is abstracted as a
ROS node and connected with others through ROS Topics. The ROS
topics are many-to-many named buses that store the robot or envi-
ronment states. Each topic is implemented by the publish-subscribe
messaging protocol: some nodes can subscribe to a topic to obtain
relevant data, while some nodes can publish data to a topic.

The robot communicates with end users through ROS Services.
The ROS services are a set of interfaces of the robot app exposed
to end users. Each service is implemented by the Remote Procedure
Call (RPC) protocol and allows users to launch tasks or adjust func-
tion parameters. Once the robot receives a mission from the user’s
phone (❹), it executes the mission and interacts with the surround-
ing environment at runtime (❺). The user will receive the notiﬁca-
tion from the robot when all tasks are completed (❻).

2.3 Threat Model and Problem Scope
In this paper, we consider a threat model where some nodes of
a robot app are untrusted. Those adversarial nodes aim to com-
promise the robot’s operations, forcing it to perform dangerous
actions. This can result in severe security and safety issues to ma-
chines, humans and environments [32–34].

This threat model is drawn from three observations. First, the
ROS platform is open for everyone to upload and share their code
repos. Diﬀerent from app stores of other ecosystems [7–14], the
ROS platform does not have any security check over the submitted
code. As a result, an adversarial developer can insert malicious code
to a repo and publish it to the ROS platform for other users to down-
load. This has been highlighted in the design document of ROS2
Robotic Systems Threat Model [35]: “third-party components releas-
ing process create additional security threats (third-party component
may be compromised during their distribution)”. We also conﬁrm

the feasibility and practicality of this threat by successfully pub-
lishing malicious packages to the ROS platform, in an end-to-end
attack demonstrated in § 7.4 and Appendix F.1. Second, the quality
of third-party function code is not guaranteed. A lot of functions
in the ROS platform are in a lack of coding standards or speciﬁca-
tions. They may also contain software bugs that can be exploited
by an adversary to compromise the nodes at runtime [32, 36]. By
inspecting the latest commit logs in the Robot Vulnerability Data-
base [37], 17 robot vulnerabilities and 834 bugs (e.g., no authenti-
cation, uninitialized variables, buﬀer overﬂow) were discovered in
the repos of 51 robot components, 37 robots and 34 vendors in the
ROS platform. Most of them are still not addressed yet. Third, the
high interactions among nodes in a robot app can amplify the at-
tack damage. If an adversary controls one node, it is possible that
he can aﬀect other nodes directly or indirectly, and then the entire
app. The existence of untrusted nodes can also cause data races or
deadlocks when the interaction synchronization is not well han-
dled.

Given this threat model, our goal is to design a methodology and
system, which can identify and mitigate the safety risks caused by
the malicious nodes inside robot apps. We focus on the protection
of node interactions (both direct and indirect) instead of the opera-
tion of individual nodes. We further assume the underlying OS and
ROS core libraries are trusted: the operational ﬂow and data trans-
mission are well protected, and the isolation scheme is correctly
implemented so the malicious nodes are not able to hijack the hon-
est ones or the privileged systems. How to enhance the security
of the ROS core libraries [32, 38–40] and mitigate vulnerabilities
from networks [41–44], sensors [45–52, 52–56], actuators [57, 58]
and controllers [59] are orthogonal to our work.

3 FUNCTION INTERACTION ANALYSIS
In this section, we aim to draw an interaction graph to model all the
functions in the ROS platform and their communications. Up to the
date of writing, the ROS platform contains 941 repos with around
3,100 packages. A function is typically composed of multiple pack-
ages, while a repo is usually developed for one speciﬁc function.
Hence, we ﬁrst implement an automatic tool to categorize these
repos based on the function type they can achieve1 (§ 3.1). Then
we build an interaction graph based on this categorization (§ 3.2),
which is further veriﬁed by common robot apps (§ 3.3).

3.1 Categorization of Robot Functions
We ﬁrst build an automatic tool to perform large-scale analysis on
all the repos from the ROS platform. Past works adopted Natural
Language Processing and code dependency clustering techniques
to analyze smartphone and home-based IoT apps [60–66]. How-
ever, it is challenging to apply these techniques to the ROS apps.
First, a large portion of repos have poor code quality and the de-
scriptions are not well documented, which can hardly reﬂect the
characteristics of the functions. According to our analysis, 340 out

1There are a few repos which can realize more than one functions. Our tool splits
them into sub-repos with each one focusing on one function, and then categorizes
them.

Table 1: Categorization of repos in the ROS platform.

Domain

Perception
(17.6%)

Planning
(10%)

Control
(10%)

Drivers
(18.4%)

Others
(44%)

Function Type
Preprocessing
Localization
Mapping
Recognition
Path Planning
Goal Planner
Path Tracking
Teleoperation
Speech Generation
Switcher
Mobile
Manipulator
Speaker
Sensors
Visualization
Support
Extension

# of repos Example repos
84 (7.4%)
35 (3.1%)
31 (2.7%)
50 (4.4%)
103 (9.1%)
11 (1%)
68 (6%)
30 (2.6%)
9 (0.8%)
6 (0.5%)
38 (3.3%)
37 (3.3%)
6 (0.5%)
128 (11.3%)
169 (14.9%)
111 (9.8%)
219 (19.3%)

lidar_camera_calibration
slam_karto
homer_mapping
jsk_recognition
asr_ftc_local_planner
behaviortree_planner
teb_local_planner_tutorials
joy_teleop
homer_tts
iot_bridge
ackermann_controller
agile_grasp
xbot_talker
xsens_driver
rqt_reconﬁgure
aws_ros1_common
ros_pytest

of 941 repos do not provide the function descriptions in the docu-
ments, while 14 repos even use Japanese or German for the descrip-
tion. Second, the dependencies across repos do not provide accu-
rate or useful information for clustering. On one hand, most repos
achieve simple functions (e.g. localization, teleoperation) without
importing any other repos. On the other hand, some common re-
pos are imported by various repos without any connections. For
instance, cv_bridge calls the OpenCV library to process images.
It can be imported by the recognition, QR-based localization ror
vision-based mapping.

Instead, we adopt the rule-based approach to analyze and cate-
gorize repos based on their names, manifest ﬁles and related doc-
uments (e.g. README ﬁle or Wiki page). We adopt the Stanford
TokensRegex [67] to implement such an automatic tool. Detailed
implementation is described in Appendix A. In our evaluation, this
tool can successfully identify the function types of 88.52% ROS re-
pos. The rest repos (11.48%) which have too blur language features
to be identiﬁed are then analyzed manually, with very minor eﬀort.
We believe our tool can adapt to future ROS repos as well.

Table 1 illustrates our analysis results about all function types of
repos under the ROS1 kinetic version in the platform: 941 repos are
spit into 1135 ones as some repos implement two or more functions.
Then they are categorized into 17 function types in ﬁve domains.
The ﬁrst three domains include main functions for computing
the robot tasks. In the Perception domain, the Preprocessing func-
tion has the largest proportion, which converts the raw sensory
data or calibrates multiple data to the desired representation. Then
the processed data are transmitted to the Localization and Mapping
functions to form the knowledge of navigation, or transmitted to
the Recognition function to generate the corresponding informa-
tion. In the Planning domain, functions are used to parse a high-
level task to a set of low-level actions based on the knowledge
from the environment. For example, the Path Planning function
receives the coordinate of the destination and computes a collision-
free path to reach that position. The Goal Planner function encap-
sulates each action into a basic unit and deﬁnes the corresponding
logic execution sequence. The planned actions are then forwarded
to the functions in the Control domain to drive the actuators. The
actuators can be commonly classiﬁed into three categories: wheels
or rotors in mobile robots, manipulator and speaker. The ﬁrst two

actuators are driven by the Path Tracking function, which gen-
erates adaptive velocities to control these robots navigating in the
real world. The Speech Generation function creates audios for
the speaker to respond to users’ requests. This function also in-
cludes packages from many cloud providers. The switcher func-
tion is used to switch on/oﬀ certain actuators for some speciﬁc
tasks.

In addition to the three main domains, we also identify two
more categories. In the Drivers domain, the functions allow the ro-
bot to interface with various actuators. The Others domain has the
largest number of repos. Speciﬁcally, the Visualization function
provides a GUI plugin for users to control the robot. The Support
function builds a bridge between ROS and many third-part plat-
forms, making the robot apps compatible with AI frameworks, mo-
bile apps, and cloud services. The Extension function provides the
wrappers of core libraries in ROS to ease the robot app develop-
ment.

3.2 Building an Interaction Graph
With the above categorization, we analyze the message types be-
tween diﬀerent repos, and generate an abstract interaction graph,
as shown in Figure 3. From the ﬁgure, we can observe that func-
tions in Perception, Planning and Control domains constitute all the
computational nodes (the gray ellipse) in the interaction graph2.
As discussed in § 2.1, function nodes share robot states through
direct interactions. The robot states are generally estimated val-
ues the function node computes based on the sensory data. We
use a solid arrow to represent the direct interaction between two
nodes, which is implemented in a topic pattern. Speciﬁcally, two
functions are connected if the source node publishes message types
which are subscribed by the destination node. The message type
published/subscribed by adjacent nodes is labeled above the arrow.
For example, the published message type of the Path Planning
function is ‘nav_msgs/Path’, which is the same as the subscribed
message type of Path Tracking function. Thus, a connection ap-
pears from the Path Planning to path Tracking.

In addition, function nodes also have indirect interactions via
the physical environment. The action commands generated from
the function nodes in the Control domain can alter the robot’s sur-
rounding environment, which will also aﬀect some function nodes
as they need to re-estimate the latest robot states and determine
the actions at the next moment. We use dotted arrows to represent
such indirect interactions caused by the changes of environmental
contexts. For example, after the Path Tracking node generates ve-
locity and drives the robot to a new context, the obstacle’s position
relative to the robot has also changed. Then the function nodes in
Perception need to update the robot states (e.g. map and pose). If an
obstacle is added to an unknown map, the Path Planning node
may need to re-plan a new path.

With those direct and indirect interactions, Figure 3 gives a com-
plete closed-loop robot operation. Speciﬁcally, function nodes in
the Perception domain receive physical environmental information
from sensors and convert it to robot internal estimated states (e.g.

2According to the sensor types supported by ROS [69], we further split the
Recognition function into several recognition subfunctions, e.g. temperature/illumi-
nance/humidity recognition.

Env State

Hardware

Drivers

Perception

Planning

Control

<$/75$

6$#7$%"+1%$

4551#*,",-$

01#*2*+3

6"%?$+
CDE$-+

CD’+"-5$

812*/(
.$,’/%

6$#7$%"+1%$(
.$,’/%

4551#*,",-$(
.$,’/%

01#*2*+3(
.$,’/%

!"#$%"

&"’$%()*’+",-$(
.$,’/%

49:

;<.

.$,’/%(
)%*O$%

.$,’/%(
)%*O$%

.$,’/%(
)%*O$%

.$,’/%(
)%*O$%

.$,’/%(
)%*O$%

.$,’/%(
)%*O$%

.$,’/%(
)%*O$%

.$,’/%(
)%*O$%

<%$(
<%/-$’’*,?

<%$(
<%/-$’’*,?

<%$(
<%/-$’’*,?

!"#$%&

’()*(#+",#(

4..,)$%+%5(

<%$(
<%/-$’’*,?

-(.+/0(
1,)$2$"3

4 ) + & (

4)+&(

4)+&(

6 + 7( #! 5+%

6+7(#!5+%
4:;

8 9 7 $ / 9 %

<%$(
<%/-$’’*,?

<%$(
<%/-$’’*,?

<%$(
<%/-$’’*,?

<%$(
<%/-$’’*,?

.7$$-=(
>$-/?,*@/,

6$#7$%"+1%$(
>$-/?,*@/,

4551#*,",-$(
>$-/?,*@/,

01#*2*+3(
>$-/?,*@/,

.-$,$(
>$-/?,*@/,

B"-$(
>$-/?,*@/,

CDE$-+(
>$-/?,*@/,

4%"(#(7"
4%"(#(7"
4%"(#(7"

4%"(#(7"

4 % " ( # ( 7 "
4% "(#( 7"
4% "(#( 7"

B"-$
>$-/?,*@/,
897(

&/-"5*F"@/,(
G(9"77*,?

.7$$-=(
;$,$%"@/,

?,2$9=+"+

!"#$%&

Drivers

.7$"A$%(
)%*O$%

Hardware

Robot State

.7$"A$%(
8-+1"+/%

.7$"A*,?

;/"5(
<5",,$%

>99.

.H*+-=$%

@)2

C+=$%(
)%*O$%

C+=$%((
8-+1"+/%

C,KCLI(
M,/D

%
9
/
+
%
/
7
(
=

(
7
9
8

<"+=(
<5",,*,?

8+"<

<"+=(
6%"-A*,?

A(.95$"3

9/D*5$(
)%*O$%

9/D*5$(
8-+1"+/%

6$5$N(
C7$%"@/,

A(.95$"3

9",*715"+/%(
)%*O$%

9",*715"+/%(
8-+1"+/%

</’$I(
J$5/-*+3

Figure 3: The interaction graph of robot functions.
Table 2: Analysis of open-source robot apps from the ROS showcase website [68].

App Categories
Remote Control
Panorama
2D/3D Mapping
Navigation
SLAM
Exploration
Follower
Manipulation
Face/Person Detection
Object/Scene Detection
Object Search
Gesture Recognition
Voice Interaction
Autonomous Driving

Set of function nodes

# of apps Robot Type
23 (20.8%) MB, MM, HR, MAV Teleoperation+Mobile/Manipulator Driver
2 (1.8%)
8 (7.3%)
22 (20%)
11 (10%)
5 (4.5%)
8 (7.3%)
8 (7.3%)
8 (7.3%)
5 (4.5%)
1 (1%)
3 (2.7%)
5 (4.5%)
1 (1%)

Preprocessing
Preprocessing+Mapping+Teleoperation+Mobile Driver
Preprocessing+Localization+Path Planning+Path Tracking+Mobile Driver
Preprocessing+Localization+Mapping+Path Planning+Path Tracking+Mobile Driver
Preprocessing+Localization+Mapping+Goal Planner+Path Planning+Path Tracking+Mobile Driver
Preprocessing+Recognition+Mobile Driver
Preprocessing+Localization+Path Planning+Path Tracking+Manipulator Driver
Preprocessing+Recognition
Preprocessing+Recognition
Preprocessing+Localization+Recognition+Goal Planner+Path Planning+Path Tracking+Mobile Driver ROSbot 2.0 PRO
Preprocessing+Recognition+Manipulator Driver
Preprocessing+Recognition+Speech Generation+Speaker/Mobile/Manipulator Driver
Preprocessing+Localization+Recognition+Goal Planner+Path Planning+Path Tracking+Mobile Driver Turtlebot3

MB
MB
MB, MM, MAV
MB
MB
MB
MM
MB, MM, MAV
MM
MM
HR, MAV
MB, HR
MB

Example Robot
Caster
Turtlebot3
Xbot
Tiago++
Roch
Turtlebot2
Magni Silver
LoCoBot
ARI
Tiago

COEX Clover
Qtrobot

pose). Function nodes in the Planning domain generate long-term
plans (e.g. path) based on robot’s knowledge. All data ﬂows con-
verge to the function nodes in the Control domain, which output
action commands (e.g. velocity) to control related actuators. Then
the actuators can alter the surrounding environment, and force the
function nodes in the Perception to repeat the above procedure.

3.3 App Analysis
Our interaction graph is built from the analysis of individual func-
tion nodes. We verify its comprehensiveness and correctness us-
ing end-to-end robot apps. We collect 110 robot apps from the
ROS showcase website [68], covering 24 diﬀerent robots includ-
ing mobile base (MB), mobile manipulator (MM), micro aerial ve-
hicle (MAV) and humanoid robot (HR). Table 2 summarizes the cat-
egories of these apps, numbers and the applicable robot types. All
these apps can be illustrated using our interaction model, as shown
in the fourth column. Detailed descriptions of the commonly used
apps can be found in Appendix B.2.

Note that most of these apps analyzed above are designed for the
research purpose, e.g., algorithm analysis and function testing. An
industrial/commercial app needs to perform more complex tasks
with a variety of unexpected emergencies in the real world. As a
result, it is common to integrate multiple research apps for better
robustness and functionality. For example, the Navigation app can-
not recognize traﬃc lights and handle emergencies (e.g., intruders,
accidents). We need to integrate Object/Scene Recognition to rec-
ognize these conditions and take corresponding safe operations.

-!".&

/&+&)+&/$
)(0/*1(0

99

!"#$
%&’()*+,

*!".&2
3&’"+&/$0(/&

!"#4%&’2
3&’"+&/$0(/&

!)#$%&’&()*$+,-.$/$51

!2#$+3"34/56&2,72$+,-.

5+"+&

6&3)&61(0$
&%&0+

99

)(0+3(’$
")1(0

&%&0+23&’"+&/$
0(/&

")1(023&’"+&/$
0(/&

!"#$%&’&()*$+,-.$/$01
:,(&24$;’4&()2<3’$+,-.

!8#$0,--,3’/56&2,72$+,-.
;’8,(&24$;’4&()2<3’$+,-.

Figure 4: Three types of interaction risk.

The combinations of these apps can still be modeled using our in-
teraction graph, as we will demonstrate in § 7.

4 RISK ANALYSIS
We analyze safety risks caused by malicious function nodes and
interactions. We classify these risks into three categories (Figure
4 and Table 4). We describe how each risk can incur unexpected
behaviors to threaten the robot safety.

4.1 General Risk (GR)
GR is caused by a direct interaction. It occurs when multiple func-
tion nodes share the same robot states. If one node is malicious, it
can intentionally change the robot states to wrong values to aﬀect
the robot operation. Based on the interaction graph, there are two
conditions to trigger the GR. First, two or more function nodes are
connected to the same successor node, and at least one of them is

untrusted. Second, the transmitted message types among the above
function nodes need to be the same. This guarantees that all these
nodes share the same robot state through the direct interaction.

According to the number of topics, GR can be further divided
into two types. (1) General Risk with Single Topic (GR-ST): multi-
ple high-risk nodes publish to one same topic, subscribed by the
successor node (Figure 4a). (2) General Risk with Multiple Topics
(GR-MT): both the indegree and outdegree of the topic are equal
to 1. There can be multiple parallel topics with the same message
type subscribed by the successor function (Figure 4b).

4.2 Robot-Speciﬁc Risk (RSR)
RSR happens in an indirect interaction, due to the conﬂict behav-
iors related to the robotic mobility characteristic. This mobility fea-
ture requires the robot to recognize real-time environment condi-
tions (e.g. obstacle avoidance, traﬃc light) and react to them promptly.
The robot’s maximal velocity is determined by its reaction time,
which further depends on two factors [70, 71]. The ﬁrst factor is
the processing time for collision avoidance, which is the end-to-
end latency from obstacle detection to velocity control. The sec-
ond factor is the frame rate of the Image Recognition function.
The faster the robot is, the larger frame rate this function requires
to respond to the rapid changes of the environment. In this paper,
we only focus on the second factor as the processing latency is
the safety issue of the internal function node (i.e. Path Tracking)
rather than the interaction between two nodes.

Figure 4c shows the mechanism of RSR. There are two types
of high-risk function nodes: (1) the image-related node is used to
understand the current detected conditions through image recog-
nition. (2) The max_vel-related node outputs the maximal velocity
value to the corresponding topic based on the current condition.
These two nodes aﬀect each other via an indirect interaction (dot-
ted line). The maximal velocity and image frame rate should satisfy
certain conditions to guarantee the robot can function correctly. If
either node is malicious and produces anomalous output (too large
maximal velocity or too small frame rate), the requirement can be
compromised, bringing catastrophic eﬀects in some tasks.

4.3 Mission-Speciﬁc Risk (MSR)
MSR refers to the violation of users’ expectations regarding the
safe and secure behaviors of a robot system. It exists in the indirect
interaction between an event-related node and action-related node
(Figure 4d), when there are conﬂicts between them, regulated by
some scenario-speciﬁc rules. Although some GRs and RSRs may
also lead to the violation of these rules, the causes and mitigation
strategies are totally diﬀerent. So it is necessary to discuss MSR
separately. There are two types of high-risk nodes in MSR: (1) the
event-related ones include all the nodes in the Perception domain
except Preprocessing. The robot uses those nodes to understand
the conditions of the physical environment. (2) The action-related
ones include all the nodes in the Control domain which can directly
interact with the actuator drivers. They are used to actively change
the actual states of both the robot and environment. If either of
these nodes are malicious, the robot and task can be compromised
with unexpected consequences.

Table 3: Examples of Mission-Speciﬁc Risks and Rules.
Scenario

Description

Domestic

Warehouse

City

Specialized

The companion robot must send an alert when a user is in danger.
The robotic vacuum must be turned oﬀ when a user is sleeping.
The manipulator must not grasp objects that exceed its limited weight.
The AGV must recharge when the battery level is below a threshold.
The mobile vehicle must follow the traﬃc rule.
The mobile vehicle must maintain a safe distance with passengers.
The ﬁreﬁghter robot must send an alert when detecting the wounded.
The precision of the surgery robot must be above a speciﬁed threshold.

The rules to prevent MSR are determined by the missions and
usage scenarios, which are usually speciﬁed by users. Table 3 lists
some examples of MSRs and the corresponding rules in four scenar-
ios. (1) In a domestic context, robots are designed to manage var-
ious human-centric tasks, e.g., house cleaning, baby-sitting. They
are required not to disturb human’s normal life. (2) In a warehouse
context, industrial robots are introduced to achieve high automa-
tion and improve the productivity, such as manipulator and au-
tonomous ground vehicle (AGV). These robots are required to com-
plete each subtask correctly, eﬃciently and safely. (3) In a city
context, autonomous vehicles and delivery robots move at high
speeds in the transportation system, and handle complex events
from outdoor dynamic environment. Thus, they need to obey the
transportation rules and ensure the safety of passengers and public
assets. (4) Robots are also deployed in many specialized scenarios
to conduct professional missions. For example, rescue robots are
used to search for survivors or extinguish ﬁres. Medical robots are
used in hospitals to diagnose and treat patients. Military robots are
designed in battleﬁelds to destroy enemies or constructions. These
robots need to follow the rules related to their speciﬁc missions.

4.4 Summary of Risks from Each Domain
An arbitrary malicious node in the robot app can incur the above
risks. We discuss the potential risks and consequences caused by
malicious functions in each domain.
Perception. If a node in the Perception domain is untrusted, the
robot states will be estimated as wrong values. Following the di-
rect interactions, the robot will take anomalous actions, which vi-
olate the rules of MSR. Moreover, since the Recognition function
typically adopts sensor fusion to reduce uncertainty caused by the
physical limit of diﬀerent sensors, such threat can cause GR as well.
For instance, an autonomous vehicle is navigating in a highway.
A malicious Preprocessing function intentionally sends wrong
sensory data to the Object Recognition function to cause optical
illusions, e.g., recognizing a turn right sign as a stop sign. This will
violate the traﬃc rule: “vehicles cannot stop in a highway”.
Planning. A malicious node in the Planning domain can interrupt
the current task, or reset the robot states to wrong values. In a com-
mon robot app, there can be multiple Global Planner functions
for diﬀerent goals based on various events from the Recognition
functions3. This gives the malicious node chances to win the com-
petition against other goals and compromise the robot states (GR).
Besides, the malicious node can also directly modify the goal to
make the robot take anomalous actions in a speciﬁc event (MSR).
For instance, a robot vacuum is executing the cleaning task in
a living room. The Global Planner function is compromised and
controlled by an adversary to set a new destination goal as the

3We merge all these interactions in our interaction graph (Figure 3)

Risk Domain Threat Coordination Node

Executor Policy

Table 4: Summary of risks, threats and mitigation for function interactions.

GR

Perception,
Planning, Control

GRCN

Developer

RSR

Control

RSRCN

Developer

End User

Parameter
Block Bit
Timeout

Block
FIFO_Queue
Priority_Queue Timeout, Priority
Preemption
Block
Safe
Constrain

Description
Allow/block the action of chosen ﬂow.
Choose the action based on ﬁfo order with time limit.
Choose the action based on priority order with time limit.
Choose the action based on priority order.
Allow/block the velocity control action of chosen ﬂow.

Priority
Block Bit
Threshold, Priority Adjust max velocity based on fps data.
Max_vel_limit

Limit adjustable max velocity limit with a user-deﬁned value.

MSR

Perception,
Planning, Control

MSRCN

End User

Block

Block Bit

Allow/block the action of chosen ﬂow.

master bedroom for stealing privacy. This can violate a possible
MSR rule: “the robot vacuum cannot enter the bedroom”. If the
robot does not have enough power to clean the master bedroom,
this will violate the MSR rule: “the AGV must recharge when the
battery level is below a speciﬁed threshold.” (Table 3).
Control. If a function in the Control domain is malicious, the ad-
versary can launch attacks in three ways. First, the function can
interrupt or suspend other actions from diﬀerent interactions (GR).
Second, it can increase the velocity to cause failures of image-related
recognition functions through the indirect interaction (RSR). Third,
it can directly control the robot to take unexpected actions in a spe-
ciﬁc scenario (MSR).

For instance, in a task of searching dangerous goods or wounded
persons, the robot device receives images through the equipped
camera at a certain frame rate. If the max_vel node is malicious and
intentionally increases the maximal velocity, there will be no or
less correlation between adjacent frames. The Image Recognition
function may fail to process each frame promptly, and frames con-
taining safety-related information (e.g. drug, thief) can be missed.

5 MITIGATION METHODOLOGY
We present a novel methodology to mitigate the malicious function
interactions. The core of our solution is a set of coordination nodes
(§ 5.1) and security policies (§ 5.2), as summarized in Table 4.

5.1 Coordination Node
The coordination nodes are deployed inside the robot apps to regu-
late the interactions and enforce the desired security policies. They
are designed to be general for diﬀerent types of robots, function
nodes and risks. Developers can deploy them into apps without
modifying the internal function code. Users can adjust conﬁgura-
tions based on their demands. We design three types of coordina-
tion nodes, to mitigate three types of risks respectively (Figure 5).
General Risk Coordination Node (GRCN). This node is inserted
between the high-risk nodes and their successor node (Figure 5a).
The published topics of each high-risk node need to be remapped
to the subscribed topic of this GRCN to create new data ﬂows, and
the published topic of the GRCN need to be mapped to the sub-
scribed topic of the successor node. Thus, the GRCN can control
each data ﬂow from the high-risk nodes based on various policies.
Robot-Speciﬁc Risk Coordination Node (RSRCN). This node
needs to coordinate the conﬂict between the image-related node
and max_vel-related node (Figure 5b). We use the same method to
insert the RSRCN between the max_vel-related node and its suc-
cessor node. To collect the frame rate from the image-related node,

!"#$%&
’()*+,"(!)-"*&
./012304

5*

!"#$%&’&(")$*+,-$.//(0+’"1/’$2/0&$3&4)/56&’7

,<+&
!"*8’"(

:!);$

8!);$=
($%)’$#&*"#$

#$’$5’$#&
5"*#8-"*

==

,<+

!)6&
7$%"58’9

!)6&
7$%"58’9

!)6>7$%=
($%)’$#&*"#$

5*

!8#$*/8/79:4&;+<;$*+,-$.//(0+’"1/’$2/0&$3&4)/56&’7

+’)’$

$7$*’=($%)’$#&
*"#$

<$(5$<-"*&
$7$*’

==

5"*’("%&
)5-"*

5"*’("%&
)5-"*

)5-"*=($%)’$#&
*"#$

5*

!;#$>+,,+/’9:4&;+<;$*+,-$.//(0+’"1/’$2/0&$3&4)/56&’7

Figure 5: Three types of coordination nodes (purple circles).

we insert a fps_monitor node to subscribe to the detected condi-
tion topic published by the image-related node. This fps_monitor
node measures the frequency of the triggered event and publishes
the frame rate to the fps topic. The RSRCN subscribes to this fps
topic and uses it as reference for max velocity adjustment.
Mission-Speciﬁc Risk Coordination Node (MSRCN). This node
needs to allow/block the actions taken under wrong conditions
(Figure 5c). Thus, it is deployed between each action-related node
and its successor, and subscribes to all perception event topics of
event-related nodes. In this way, the MSRCN can collect all per-
ception events in the app and obtain the control of each action. It
is worth noting that there can be multiple GRCNs for each inter-
action, but the numbers of both RSRCN and MSRCN are always
one.

5.2 Security Policies
To mitigate the malicious interactions in an app, each type of coor-
dination nodes implements a set of policies. Table 4 lists the poli-
cies we have built along with the descriptions and parameters for
GRCN, RSRCN and MSRCN. Each policy needs to be conﬁgured
by either the developer or end user, as shown in the “Executor”
column.
GRCN Policies. GRCN aims to coordinate data ﬂows from diﬀer-
ent high-risk nodes. We use four types of policies to adapt to diﬀer-
ent scenarios. Speciﬁcally, the block policy is used when the user
wants to stop the current action immediately in case of emergency.
When multiple high-risk nodes publish control commands, the pre-
emption policy will choose the action with the highest priority. For
example, both the Safe Control and Path Tracking nodes pub-
lish velocity to the Mobile Driver node. However, the safe control

action should be taken ﬁrst because it is responsible for ensuring
user’s safety. FIFO_Queue and Priority_Queue policies are used for
high-risk nodes with high requirements of completion time, such
as search, rescue and obstacle avoidance.
RSRCN Policies. RSRCN aims to resolve the conﬂicts between
data ﬂows from the image-related (iﬂow) and max_vel-relate (vﬂow)
nodes. We use three types of policies to adjust the maximal velocity
of the robot. Block policy allows/blocks the action from vﬂow and
does not aﬀect the action from iﬂow. Safe policy uses thresholds
to bridge the maximal velocity with fps. Based on the fact that a
higher velocity requires a faster processing capability, we assume
the maximal velocity is proportional to the fps. Then the threshold
serves as a scale factor and can be conﬁgured by users. Constrain
policy sets a maximal velocity limit to ensure safety in complex
and dynamic environments. This is particularly useful when users
want the robots to work at low speeds psychologically even though
they drive within safe speed ranges.
MSRCN Policies. MSRCN aims to coordinate the conﬂicts between
the data ﬂows from the event-related node (eﬂow) and action-relate
node (aﬂow). We only adopt block policy to decide whether the ac-
tion should be taken under some speciﬁc conditions. However, the
block bits of eﬂow and aﬂow are diﬀerent. Bit 0/1 in aﬂow denotes
that the actions are allowed/blocked, while Bit 0/1 in eﬂow repre-
sents whether the condition event is triggered or not. Thus, end
users can control all the actions under arbitrary conditions.

To reduce the complexity of conﬁguring our methodology for
unexperienced end users, we delegate part of the policy selection
and parameter conﬁguration tasks to the developers. It is reason-
able because some risks are derived from the race condition while
the others are caused by falling short of user’s expectation. Specif-
ically, the developers enforce appropriate policies for each GRCN
and set the corresponding parameters. Moreover, the developers
also preset the parameters in the block and safe policies for RSRCN
based on the robot’s characteristics. On the other hand, the end
users only have the control of policy selection in RSRCN and MSRCN.
The parameters they need to conﬁgure are just max_vel_limit in
RSRCN and block bit in MSRCN. Table 4 shows the role of end
users and developers for each policy (the “Executor” column).

6 SYSTEM DESIGN
We design RTron, a novel end-to-end system equipped with the
above mitigation. Given a potential vulnerable robot app, the de-
veloper ﬁrst utilizes RTron to add necessary coordination nodes
to the app without modifying the original function node, and set
up some security policies. Then the end user can safely launch the
patched app on the robot, and conﬁgure other policies before the
task starts. Figure 6 gives the overview of RTron. It consists of two
components: (1) an App Instrumentor for developers to detect po-
tential risks in robot apps and deploys coordination nodes (§ 6.1);
(2) a Security Service that visualizes and conﬁgures the coordina-
tion nodes to mitigate risks at runtime (§ 6.2).

6.1 App Instrumentor
The goal of this module is to instrument the target app’s source
code to make it compatible with RTron. It patches an app with

!"#$%

App Instrumentor (Development)

Security Service (Operation)

Potential Risk 
Discovery

!

"

Developer
Conﬁg.

Coordination Node 
Deployment

Data 
Collector

Risk 
Controller

cn

#

cn

%

dc

$

cn

rc

&

cn

Policies

’

User
Conﬁg.

Robot App 
Simulation

Instrumented 
Robot App

Runtime

Figure 6: RTron system overview.

Algorithm 1: Algorithm for Potential Risk Discovery

Input: 𝑁
𝑇
𝑁 𝑝
𝑗
𝑇 𝑠
𝑖
𝑇 𝑝
𝑖

Output: 𝑅𝑁

1 foreach topics 𝑡 𝑗 ∈ 𝑇 do
if num(𝑁 𝑝
𝑗 ) >1 then
2
𝑔𝑟 ← n𝑁 𝑝
𝑅𝑁 𝑆𝑇
𝑗 o;

3

⊲ A set of nodes in a robot app
⊲ A set of topics in a robot app
⊲ A set of nodes publish to the topic 𝑗
⊲ A set of topics subscribed by the node 𝑖
⊲ A set of topics published by the node 𝑖
⊲ Risk nodes in a robot app

4

5

6
7

8

9
10

11

if (‘max_vel’ ∈ 𝑡 𝑗 .𝑛𝑎𝑚𝑒) ∧ (𝑡 𝑗 .𝑡 𝑦𝑝𝑒 == ‘std_msgs/Float64’) then

𝑅𝑁 𝑚𝑎𝑥

𝑟𝑠𝑟 ← n𝑁 𝑝
𝑗 o;
foreach string 𝑠𝑛 ∈ EVENT_MSG_TYPE do

if (𝑠𝑛 ∈ 𝑡 𝑗 .𝑡 𝑦𝑝𝑒) ∨ (‘detect’ ∈ 𝑡 𝑗 .𝑛𝑎𝑚𝑒) then
𝑚𝑠𝑟 ← n𝑁 𝑝
𝑗 o;
foreach string 𝑠𝑛 ∈ ACTION_MSG_TYPE do

𝑅𝑁 𝑒𝑣𝑒𝑛𝑡

if 𝑠𝑛 ∈ 𝑡 𝑗 .𝑡 𝑦𝑝𝑒 ∨ (‘goal’ ∈ 𝑡 𝑗 .𝑛𝑎𝑚𝑒) then

𝑅𝑁 𝑎𝑐𝑡𝑖𝑜𝑛

𝑚𝑠𝑟 ← n𝑁 𝑝
𝑗 o;

12 foreach node 𝑛𝑖 ∈ 𝑁 do
13

sort node’s subscriptions 𝑇 𝑠
foreach subscription 𝑠𝑘 ∈ 𝑇 𝑠

𝑖 by 𝑇 𝑠
𝑖 do

𝑖 .𝑡 𝑦𝑝𝑒;

if 𝑠𝑘 .𝑡 𝑦𝑝𝑒 == 𝑠𝑘+1.𝑡 𝑦𝑝𝑒 then
𝑔𝑟 ← {𝑛𝑖 };

𝑅𝑁 𝑀𝑇

foreach subscription 𝑠𝑘 ∈ 𝑇 𝑠

𝑖 do

if 𝑠𝑘 .𝑡 𝑦𝑝𝑒 == ‘sensor_msgs/Image’ then
foreach publication 𝑝𝑚 ∈ 𝑇 𝑝

𝑖 do

foreach string 𝑠𝑛 ∈ RECOG_TOPIC_NAME do

if 𝑠𝑛 ∈ 𝑝𝑚 .𝑛𝑎𝑚𝑒 then
𝑟𝑠𝑟 ← {𝑛𝑖 };

𝑅𝑁 𝑖𝑚𝑎𝑔𝑒

14
15

16

17

18

19
20
21

22

certain coordination nodes to collect events and actions from high-
risk function nodes, and guard the robot at runtime. Two subcom-
ponents are introduced to identify high-risk function nodes, and
the locations to deploy the coordination nodes, respectively.
Potential Risk Discovery. This submodule is designed to help
developers identify high-risk function nodes in a robot app. It ﬁrst
simulates the lifecycle of the target app and automatically gener-
ates the interaction graph oﬄine. Then it traverses all function
nodes (black circles in Figure 6) in the graph and identiﬁes three
types of high-risk function nodes: GR node 𝑅𝑁𝑔𝑟 , RSR node 𝑅𝑁𝑟𝑠𝑟
and MSR node 𝑅𝑁𝑚𝑠𝑟 (❶). Algorithm 1 describes our identiﬁca-
tion strategy. We conclude one rule to discover each type of risky
nodes:

GR Rule: we identify the topics in the graph whose indegree
is greater than 1. All nodes that publish to these identiﬁed topics
are denoted as 𝑅𝑁 𝑠𝑡
𝑔𝑟 with single topic (Lines 1-3). The node with

gr policy
block bit
timeout
priority

a
t
a
d
a
t
e
m

cn ID
node type
node description
trigger time
risk info
policy

obj

rc

dc

rsr policy
block bit
timeout
priority
threshold
max_vel_limit

msr policy
block bit
timeout

gr cn

rsr cn

msr cn

Figure 7: Risk model of three types of risks in Data Collector.

more than one subscribed topics of the same message type can be
integrated to 𝑅𝑁 𝑚𝑡

𝑔𝑟 with multiple topics (Lines 12-16).
RSR Rule: to identify the image-related node 𝑅𝑁 𝑖𝑚𝑎𝑔𝑒

and max_
vel-related node 𝑅𝑁 𝑚𝑎𝑥
, RTron checks the topic name and type of
𝑟𝑠𝑟
each subscribed or published message (Lines 4-5,17-22). It searches
the key words (e.g., ‘detect’, ‘people’ and ‘face’) in the RECOG_TOPIC
_NAME string list. Evaluations in § 7 indicate this key word search-
ing can eﬀectively identify the RSR nodes.

𝑟𝑠𝑟

𝑚𝑠𝑟

𝑚𝑠𝑟

MSR Rule: to identify the event-related node 𝑅𝑁 𝑒𝑣𝑒𝑛𝑡

and action-
related node 𝑅𝑁 𝑎𝑐𝑡𝑖𝑜𝑛
, RTron checks if the message type of each
topic (Lines 6-11) is in the EVENT_MSG_TYPE or ACTION_MSG_
TYPE lists since message types typically use standard ROS naming
conventions [72]. The complete lists of EVENT_MSG_TYPE and
ACTION_MSG_TYPE are presented in Appendix C.
Coordination Node Deployment. The collected information of
potential risks is used to conﬁgure the coordination node setting(❷).
This includes a set of topics and parameters. Topics represent the
state transition between two function nodes: the subscribed and
published topics specify the predecessor and successor nodes of
each coordination, respectively. The parameters are used to expose
an interface to the end user for conﬁguring each policy. With these
conﬁguration ﬁles, a Coordination Node Deployment submodule is
designed to deploy coordination nodes into the app automatically
(❸). Meanwhile, the developers check the details of the risks, select
the optional policies for GRCN and conﬁgure related parameters.
Taking GRCN as an example (Figure 13(a) in Appendix D). GRCN
monitors velocity data from three risky nodes: Navigation Control,
Tele-operation and Safe Control. The data transmission of
each node is marked as ﬂow1, ﬂow2 and ﬂow3. The developer can
select the Priority_Queue policy after the app is launched, and set
ﬂow3 from Safe Control as the highest priority, indicating its
velocity action should be always taken ﬁrst. However, if the co-
ordination node cannot receive the responding actions before the
user-deﬁned timeout (i.e. 0.2s), it will transmit the velocity action
of ﬂow2 with the second priority.

6.2 Security Service
This module aims at visualizing and mitigating risks of malicious
interactions at runtime. It consists of two subcomponents deployed
along with the robot app.
Data Collector. When the robot executes the app within the en-
vironment, all coordination nodes in the instrumented app keep
forwarding their information to this submodule (❹). Such infor-
mation is stored as a risk model, which consists of metadata and
a set of policy parameters (❺). As shown in Figure 7, the meta-
data records basic information of a coordination node, including its

ID, node type, node description, trigger time and risk information.
They manage each coordination node and visualize to the users for
risk display and policy conﬁguration.
Risk Controller. This submodule visualizes risk information and
enforces policies from users to each coordination node. Right after
the app is launched on the robot, the Risk Controller obtains all the
information of each coordination node from the Data Collector. It
then conﬁgures each coordination node by sending user-deﬁned
policy parameters (❼). When the Data Collector receives an event
and the corresponding coordination nodes’ actions at runtime, the
Risk Controller evaluates them against a collection of security poli-
cies. Some policies are mandatory, while some are optional, de-
pending on the real-world demands (e.g. task or scenario) of end
users.

The Risk Controller provides an interface for end users to check
the details of risks and select the optional policies (❻). Appendix
D presents the user consoles for three types of coordination nodes.
The console displays a visual representation of the rule violation
and available policies based on the risk model of each node. Specif-
ically, the node type, node description, trigger time and risk infor-
mation are summarized as the violated rule, violation cause, vio-
lation details respectively. Note that the end users only have full
control of policy selection for RSRCN and MSRCN, and parameter
conﬁgurations for two speciﬁc policies.

Taking RSRCN as an example (Figure 13(b) in Appendix D). End
users can check the current violation information and reset the
corresponding policy parameters at runtime. When a robot moves
from an obstacle-free environment (e.g., Highway) to a complex
environment (e.g. downtown area), users can select the Constrain
policy in an RSRCN to limit the robot’s maximal velocity.

7 EVALUATION
We aim to answer the following questions:
• Can RTron eﬀectively detect three types of interaction risks?
What is the relationship between the interaction risks and task
characteristics in each robot app? (§ 7.1)

• How many coordination nodes are required to deploy in a typ-
ical robot app? How to conﬁgure the policy for an end user
under various environmental contexts? (§ 7.2)

• What is the performance overhead of RTron? (§ 7.3)
• How practical is it to launch end-to-end attacks and perform
risk mitigation on physical robots and environments? (§ 7.4)

Target Workloads. In addition to a total of 110 open-source single-
functional apps from the ROS showcase website [68], we perform
analysis of more complex apps (Figure 8):
• Home scenario: home-based apps and robots are used to ac-
company people and conduct housework. These tasks include
teleoperation, chat, food/drink delivery, cleaning, safe detec-
tion, and object search. We use four ROS apps (Remote Control,
Face/Person Detection, Object Search and Voice Interaction) of
RosBot 2.0 Pro [73] to develop one home app (Figure 8a).

• AutoRace scenario [74]: this type of apps is designed for compe-
tition of autonomous driving robot platforms. To ensure that
the robot can drive on the track safely, there are six neces-
sary missions for the robot to execute, including lane detection
& control, traﬃc light detection, sign detection, parking, level

6 Tasks

Level 
Crossing

1 Teleoperation
3 Food Delivery
5 Safe Detection

2 Chat
4 House Cleaning
6 Object Search

Parking

Lane

Sign

6 Tasks
1 Lane Detection & Control
2 Traffic Light Detection
3 Sigh Detection  4 Parking
5 Level Crossing Detection
6 Tunnel Driving

(cid:37)(cid:82)(cid:85)(cid:85)(cid:72)(cid:74)(cid:68)(cid:86)(cid:36)(cid:89)(cid:72)
(cid:48)(cid:68)(cid:83)

Jaguar2015XE

(cid:37)(cid:82)(cid:85)(cid:85)(cid:72)(cid:74)(cid:68)(cid:86)(cid:36)(cid:89)(cid:72)
(cid:48)(cid:68)(cid:83)

Jaguar2015XE

ROSBot
2.0 Pro

Person

12 Functions
Preprocessing, Localization, Mapping, 
Recognition, Path Planning, Global 
Planner, Path Tracking, Teleoperation, 
Speech Generation, Mobile Driver, 
Speaker Driver, Sensors Driver

Tunnel

Turtlebot3

9 Functions
Preprocessing, Localization, 
Mapping, Recognition, Path 
Planning, Global Planner, 
Path Tracking, Mobile Driver, 
Sensors Driver

Traffic Lights

(b) Autorace

(c) Autoware

(d) Baidu Apollo

Figure 8: Four simulated scenarios in the Gazebo/LGSVL.

3 Tasks

1 Lane Detection & Control
2 Traffic Light Detection
3 City Driving

9 Functions
Preprocessing, Localization, 
Mapping, Recognition, Path 
Planning, Global Planner, 
Path Tracking, Mobile Driver, 
Sensors Driver

AutoRace
Home

Apollo
Autoware

21

18

(a) Home

35

20

18

14

13

13

8

s
n
o
i
t
c
a
r
e
t
n

I

k
s
R

i

f
o
t
n
u
o
m
A

40

30

20

10

0
Risk
Type

5

GR
ST

5

3

2

1

4

1

3

4

5

3

1

1

GR
MT

RSR
Image

RSR
Max_vel

MSR
Event

MSR
Action

Figure 9: Numbers of high-risk nodes in four robot apps.

crossing detection and tunnel driving. We use the open-source
Autonomous Driving app of Turtlebot3 [75] which can realize
all six tasks in the autorace scenario (Figure 8b).

• Autonomous driving scenario: we consider two mainstream self-
driving apps: Autoware [28] and Apollo [29], which have been
fully deployed and tested in physical autonomous vehicles. These
two apps are more complex than the AutoRace scenario, with
a richer set of self-driving modules composed of sensing, com-
puting, and actuation capabilities (Figure 8c and 8d).

Experimental Setup. Since this paper focuses on the software
risks in robot apps, we mainly use simulation to validate our so-
lution. Implementation and evaluation on physical robots will be
demonstrated in § 7.4. We choose the Gazebo simulator [76] and
ROS Kinetic in the home and autorace scenarios, which run on a
server equipped with 1.6GHz 4-core Intel i5 processor and Nvidia
MX110 GPU. In the autonomous driving scenario, we use the LGSVL
simulator [77] with ROS Indigo for Apollo 3.5, and ROS Melodic for
Autoware 1.14, running on a server with 4.2GHz 8-core Intel i7 and
Nvidia GTX 1080 GPU. We use Rviz [78] to visualize 3D informa-
tion from both the simulator and robot apps.

7.1 Risk Identiﬁcation
Single-functional Apps. We successfully extract all the GRs and
MSRs from all 110 open-source apps. GRs are identiﬁed by check-
ing the nodes and topics based on their topology relationship. Some
GRs are ignored when they publish messages to the log/visualiza-
tion topic, which will not bring risks to the robot app. MSRs are
identiﬁed by inspecting if the standardized topic types are matched.
Diﬀerent from the GR rule, the RSR rule involves the identi-
ﬁcation of speciﬁc topic names and types. We choose 15 image-
related apps (e.g., Face/Person Detection, Object/Scene Detection,
Object Search, Autonomous Driving) and 1 max_vel-related app
(Autonomous Driving). We successfully discover all 20 image-related
and 4 max_vel-related RSRs from these apps.
Multi-functional Apps. RTron is also scalable for analysis of
more complex apps. As examples, we show the interaction graph
of the home-based app produced by RTron in Figure 16 , and the

high-risk node for each type in the home and autorace apps in
Table 11 in Appendix E.

RTron successfully identiﬁes 198 risk interactions in the four
target apps. Figure 9 lists the numbers of extracted nodes with re-
spect to each risk type. We can observe the numbers of risk interac-
tions in the autorace (blue bar) and autoware (yellow bar) apps are
larger than home (red bar) and apollo (green bar) apps, although
the home app has the largest number of functions. This is caused by
the diﬀerences in the internal structure of each robot app. In the
home scenario, each task is relatively independent. However, in
the autorace and autoware apps, all tasks are organized as a mono-
lithic component to control the robot to drive safely. To achieve
this, these two apps need to recognize various scenes from sensory
images and take the corresponding actions. Consequently, the high
dependency among those tasks increases the number of GRs. More-
over, the requirement of image and scene recognition increases the
number of image-related RSRs and event-related MSRs. Apollo is
a special case where the number of topics is far smaller than the
other apps, thus the number of risks is also the smallest (Table 7).

7.2 Risk Mitigation
CN Analysis. RTron uses the extracted risk information to de-
ploy CNs. For GRs, the number of GRCNs depends on the number
of high-risk interactions linked to the same node. Thus, RTron
checks the GR information of “Pub Node” and deploys the GRCN
between high-risk nodes and their pub node. For RSRs, since RSR-
CNs directly publish velocity messages to the Mobile Driver func-
tion, the number of RSRCN is always 1. The subscriptions of RSRCN
is related to the number of image-related nodes and max_vel-related
nodes. Besides, as described in § 5.1, each image-related node should
be assigned to an fps_monitor node to generate the processing
rate of the image recognition process. So the number of required
fps_monitor nodes depends on the number of image-related nodes.
For MSRs, the number of MSRCNs is 1, as all event-related and
action-related nodes publish corresponding messages to the MSRCN,
which then sends the action message to all related actuator driver
nodes.

Table 5 lists the numbers of three types of CNs in the four robot
apps. We observe that GRCNs account for a large portion of the
total added nodes. Due to a large number of RSR image-related
interactions, the autorace app has more fps_monitor nodes than
the home app. The complete interaction graph with added CNs is
shown in Figure 17, Appendix E for the example of the home app.
Policy Selection. RTron implements a variety of policies for three
types of CNs. How to select the appropriate policy for each CN is
critical for the secure operation of robot apps. We use the home
app as an example to illustrate the guideline for policy selection.

Table 5: Numbers of CNs in four complex robot apps.

Scenario

Home
AutoRace
Apollo
Autoware

GRCN
Perception Planning Control

RSR
FMN CN

8
16
4
11

3
2
1
3

1
4
1
2

2
5
3
1

1
1
1
1

MSR
CN
1
1
1
1

Table 6: High-risk interacted topics and features of three
GRCN types in the home app.

CN Type

Perception

Planning

Interacted Topics
‘/explore_server/status’, ‘/move_base/status’,
‘tf’, ‘tf_static’, ‘/camera/rgb/image_raw’,
‘/camera/depth/image_raw’,
‘/move_base/global_costmap/footprint’,
‘/move_base/local_costmap/footprint’
‘/move_base/goal’, ‘/move_base/cancel’,
‘/move_base_simple/goal’

Control

‘/cmd_vel’

Feature

State
Parallelization

Goal
Queuing
Action
Preemption

GRCN : this is designed to coordinate direct high-risk interac-
tions between multiple connected nodes. Based on the types of in-
teracted topics, we classify GRCN into three categories: perception,
planning and control. As shown in Table 6, the messages of inter-
acted topics in perception are related to the sensory information
(e.g. images) or preprocessed robot states (e.g. footprints, status).
Typically, multiple messages with the same type are published to
the same target node, and processed in parallel for either sensor fu-
sion or state monitoring. Thus, there is no contention among these
messages.

Messages of the interacted topics in planning or control con-
tend with each other to get the long-term and instant control of
the robot. Speciﬁcally, when a message of a new planning goal is
received, the robot must ﬁrst complete the previous goal before
executing the current one. For example, an object search task is
launched after the search_manager node publishes a goal to the
/move_base_simple/goal topic. An adversary can use a malicious
rviz node to send another arbitrary destination to this topic (Fig-
ure 16 in the appendix). The object search task will be immediately
interrupted and then the robot is controlled to reach the designated
position. Thus, a GRCN with the ‘FIFO_Queue’ or ‘Priority_Queue’
policy can delay such malicious actions without task interruption.
Diﬀerent from the planning messages, the control messages need
to control the robot immediately. End users can select the ‘Preemp-
tion’ policy of GRCN for coordination. For instance, the malicious
teleop_twist_keyboard node can ﬂood the /cmd_vel topic while
the robot is following a planned path to the destination. Then the
topic receives the messages from both teleop_twist_keyboard
and move_base nodes simultaneously, which causes the robot to
switch velocity in the two target directions. By assigning the high-
est priority to the move_base-related velocity control interaction
(i.e. /cmd_vel), the move_base node can control the robot ﬁrst.

RSRCN: end users are not recommended to set the ‘Block’ or
‘Safe’ policy. These two options should be chosen by app devel-
opers after extensive evaluations. Instead, users can choose the
‘Constrain’ policy to set a maximal velocity value to limit the ro-
bot’s speed. This is very eﬀective and safe, especially when the
robot’s working environment is highly complex and dynamic, and
the task completion time is not very critical. For example, if an

Application

Table 7: Processing time of potential risk discovery.
Processing Time (s)
RSR MSR
GR
0.057
0.113
0.114
0.011
0.035
0.035
0.152
0.299
0.308
0.498
0.727
0.764
0.753
1.086
1.12
1.927
3.199
3.121
2.105
4.049
4.075
0.306
0.606
0.631
1.747
2.931
2.945

Teleoperation [79]
Voice Interaction [80]
Mapping [81]
Navigation [82]
Exploration [83]
Home
AutoRace [74]
Apollo [29]
Autoware [28]

Topic
Number
17
7
25
63
84
125
112
39
218

Node
Number
4
6
6
8
10
21
25
21
38

)
s
m

(
y
c
n
e
t
a
L

6

4

2

0

1

2

3

4

5

6

7

8

9

10

CN Number
Figure 10: Overhead of CNs in an end-to-end data ﬂow.

adversary compromises the move_base node and increases the ro-
bot’s speed to a dangerous level, this can cause a potential traﬃc
accident. By setting an appropriate threshold in the ‘Safe’ policy or
max_vel_limit in the ‘Constrain’ policy, the robot will slow down
its speed without object detection failures.

MSRCN: although there is only one policy option, users can cus-
tomize diﬀerent rules to allow/block the actions of speciﬁc robots
under speciﬁc conditions. Taking the home app as an example, the
MSRCN receives messages from three event-related topics (/objects,
/person_detector/detections, /odom) and two action-related
topics (/audio/audio, /cmd_vel). Users can set a rule to disal-
low the robot’s movement when it detects the target object. This
can identify and mitigate the interruption of the object search task
caused by the malicious rviz node mentioned above.

7.3 Performance Overhead
Oﬀline overhead. We evaluate the risk discovery stage of RTron
in terms of processing time for identifying high-risk nodes in a ro-
bot app. Table 7 reports the performance results of 9 robot apps
with diﬀerent numbers of topics and nodes. We repeat each exper-
iment for 20 times to calculate the average latency. We conclude
that the risk discovery has negligible overhead as an oﬄine pro-
cess. The results also show that the processing time is aﬀected by
the number of topics and nodes. This is because the risk identiﬁca-
tion depends on the traversal of either nodes or topics (Algorithm
1). Speciﬁcally, there are two iterations in the process of both GR
and RSR discovery and one iteration of topics in the process of MSR
discovery. Thus, discovering GR takes similar time as RSR, which is
longer than MSR. One exception is the autorace app, which has the
largest processing time, but fewer nodes and topics than the home
app. This is because there are more high-risk GR interactions in
the autorace app (Table 5), which add extra work (i.e. related topic
type and name match) in the node iteration process.

GR Nodes
/move_base
/tb3_safe_control

Pub Topic Name
/cmd_vel
/cmd_vel

Wireless 
Access Point

Turtlebot3

Original
Destination

Unstable
Area
Ghost Dest

MSR Nodes
[Event] /turtlebot3_core
[Action] /tb3_monitor

Pub Topic Name
/odom
/move_base_simple/goal

Turtlebot3

/teleop_twist_keyboard
: Turn Right

GRCN Policy
GRCN Flows
flow1
flow2

Preemption
Parameter
2
1

Unstable
Area

MSRCN Policy
MSRCN Flows
eflow
aflow

Block
Parameter
1
0

(a) GR Case

Turtlebot3

(b) MSR Case

Figure 11: GR and MSR experiments on turtlebot3.

Turtlebot3
/move_base: Straight

Runtime overhead. This includes the overhead from the coordi-
nation nodes and security service. The security service is only re-
sponsible for risk monitoring and policy conﬁguration of each co-
ordination node, without any interference on the execution of the
robot app. Much like IoT policy enforcement systems [20, 21], we
ignore the overhead of this process since users manually conﬁgure
the policy for each CN only at the mission launch stage or scenario
change condition. The coordination nodes are distributed among
function nodes in the robot app, which can increase the end-to-end
latency from the perception to the control stages. Although there
are dozens of nodes in a typical robot app, these nodes work in a
parallel multi-ﬂow mode. To achieve real time, typically each data
ﬂow includes fewer than 10 nodes. So we consider the overhead
of end-to-end latency within 10 coordination nodes. As shown in
Figure 10, the extra latency incurred by 10 coordination nodes is
around 5ms. This is trivial even for the autonomous driving app
with the strongest real-time constraint: according to the industry
standards published by Mobileye [84] and design speciﬁcations
from Udacity [85], the latency for processing tragic condition in
an autonomous driving app should be within 100 ms, which is far
larger than the overhead of coordination nodes.

7.4 Case Studies in the Real World
To demonstrate the practicality of the considered threats and pro-
posed solution, we implement and evaluate several scenarios in a
physical device, i.e., Turtlebot3. Figure 11 shows our settings and
real-world environment. The Turtlebot3 is an open-source mobile
base equipped with a Raspberry Pi CPU@1.3GHz, 1 GB memory
and a 360 Laser Distance Sensor (LDS), running ubuntu 16.04 and
ROS kinetic. It is connected to a server (Intel i7 CPU@4.2GHz with
16GB of RAM) for computation oﬄoading and mission launching.
Attack Method. We develop two normal tools /tb3_safe_control
and tb3_monitor to monitor and control the robot’s movement.
We insert some malicious codes in these tools which send wrong
control commands. The detailed implementation of our attacks is
described in Appendix F.1. We encapsulate these tools into two
ROS packages and successfully upload them to the ROS platform
as a developer4. This validates our threat model that an adversary
can easily share malicious packages in the ROS platform. Next, we

4To avoid raising ethical concerns, we add an extra trigger such that the attacks hap-
pen only when the MAC address of the robot matches a predeﬁned one. This ensures
that the malicious package will not aﬀect normal users. The repo is uploaded to the
ROS platform from Dec. 5, 2020 to the date of writing on the website: link removed
for anonymity, as it contains the author’s information

download these two packages as another developer, and imple-
ment them on the Turtlebot device. Below we describe the mali-
cious behaviors and how our system can mitigate them with two
cases5.
GR Case. The /tb3_safe_control node generates malicious ve-
locity commands during the robot’s navigation at certain moments.
In Figure 11a, the robot plans a straight route in the corridor. Dur-
ing its movement, the /move_base node computes the real-time
velocity and publishes it to the /cmd_vel topic to drive the ro-
bot to the destination. Due to the shared state between these two
nodes, the malicious node compromises the robot and creates a
crash through publishing continuous “turn right” commands to the
shared topic. In RTron, the developer can choose Preemption pol-
icy in the GRCN and set diﬀerent priorities to each ﬂow. Then the
malicious node is not able to interrupt the normal navigation be-
haviors in this case.
MSR Case. The tb3_monitor node sends malicious goal commands
during the robot’s navigation at certain moments. As shown in Fig-
ure 11b, it generates a wrong destination in an unstable area far
away from the wireless access point. As part of the computations
is oﬄoaded to the remote server, the robot running into this area
will lose network connection, and malfunction. In our experiment,
after the destination is changed, the robot navigates into the un-
stable area and ﬁnally stops under the poor network condition. In
RTron, we choose Block policy in the MSRCN and set diﬀerent pa-
rameters to each ﬂow. The developer uses position from the /odom
topic to implement a function node to check whether the robot
moves in the unstable area. This node is connected with MSRCN
and marked as an eﬂow. In this way, any suspicious destination
within the unstable area would be blocked.

8 RELATED WORKS
Robotic Security. Existing research on robotic security has mainly
focused on traditional security issues in robot systems, e.g., net-
work communication [38–40], denial-of-service attacks [32] and
software vulnerabilities [59, 86–89]. In addition, adversaries can
also spoof the sensory data ([45–56]), fake the actuator signals [58],
or tamper with the micro-controller input [59].

In this paper, we focus on a new type of security issue in robot
apps, caused by malicious interactions. We are the ﬁrst to demon-
strate the feasibility and severity of this threat, as well as a possible
defense solution against it.
Interaction Risk Mitigation. Prior works studied the interaction
risks in IoT apps [18–27]. Users adopt operation rules following

5Considering the potential physical damages caused by vehicle’s high speeds, the RSR
case is implemented in the simulator in Appendix F.2

the “If-This-Then-That” (IFTTT) trigger-action programming par-
adigm [90, 91] to express automation behaviors among IoT devices.
These methods translate the rules to the interaction graph, and ver-
ify if conﬂicts or policy violations can occur between interactions.
There are three major diﬀerences between the interaction risks
of robot apps and IoT apps. (1) For interaction modeling, robot
apps not only inherit all the interactions from IoT apps, but also
enjoy robot-speciﬁc ones, e.g., direct interactions via sharing in-
ternal states, indirect interactions caused by mobility. Robot apps
need to cooperate with multiple functions, and require more com-
plicated rules than the IFTTT model in IoT apps. (2) For risk iden-
tiﬁcation, IoT apps are implemented by verifying if the interaction
between diﬀerent rules violates user-deﬁned policies. However, ro-
bot apps have not only such risks (MSR), but also new ones (GR
and RSR) due to data competition and mobility. (3) For risk mitiga-
tion, diﬀerent from the simple “allow/block” policy adopted in IoT
works, coordination of each type of risks needs a set of diﬀerent
conﬁgurable policies to mitigate malicious function interactions.
All these distinct features of robot apps require new studies about
the risk analysis and mitigation solutions, as we present in this
paper.

9 CONCLUSION
Function interaction provides great ﬂexibility and convenience for
robot app development. However, it also introduces potential risks
that can threaten the safety of robot operations. This is exacerbated
by the fact that current robot app stores do not provide security
inspection over the function packages. We present the ﬁrst study
towards the safety issues caused by suspicious function interaction
in robot apps. We introduce a novel end-to-end system and method
to enforce security policies and protect the function interactions in
robot apps. We hope this study can open a new direction for robot-
ics security, and increase people’s awareness about the importance
of function interaction protection.

[19] C.-J. M. Liang, Z. L. Lei Bu, J. Zhang, S. Han, B. F. Karlsson, D. Zhang, and
F. Zhao, “Systematically debugging iot control system correctness for building
automation,” in Proceedings of the 3rd ACM International Conference on Systems
for Energy-Eﬃcient Built Environments (BuildSys@SenSys), 2015.

[20] W. Ding and H. Hu, “On the safety of iot device physical interaction control,” in
ACM Conference on Computer and Communications Security (CCS), 2018.
[21] Z. B. Celik, G. Tan, and P. D. McDaniel, “Iotguard: Dynamic enforcement of
security and safety policy in commodity iot,” in Annual Network and Distributed
System Security Symposium (NDSS), 2019.

[22] Z. B. Celik, P. D. McDaniel, and G. Tan, “Soteria: Automated iot safety and secu-

rity analysis,” in Annual Technical Conference (ATC), 2018.

[23] D. T. Nguyen, C. Song, Z. Qian, S. V. Krishnamurthy, E. J. M. Colbert, and P. D.
McDaniel, “Iotsan: fortifying the safety of iot systems,” in Conference on Emerg-
ing Network Experiment and Technology (CoNEXT), 2018.

[24] H. Chi, Q. Zeng, X. Du, and J. Yu, “Cross-app interference threats in smart homes:

Categorization, detection and handling,” in CoRR abs/1808.02125, 2018.

[25] L. Bu, W. Xiong, C.-J. M. Liang, S. Han, D. Zhang, S. Lin, and X. Li, “Systemati-
cally ensuring the conﬁdence of real-time home automation iot systems,” ACM
Transactions on Cyber-Physical Systems (TCPS), vol. 2, no. 3, pp. 22:1–22:23, 2018.
[26] L. Zhang, W. He, J. Martinez, N. Brackenbury, S. Lu, and B. Ur, “Autotap: syn-
thesizing and repairing trigger-action programs using ltl properties,” in Interna-
tional Conference on Software Engineering (ICSE), 2019.

[27] Q. Wang, P. Datta, W. Yang, S. Liu, A. Bates, and C. A. Gunter, “Charting the
attack surface of trigger-action iot platforms,” in ACM Conference on Computer
and Communications Security (CCS), 2019.

[28] “The autoware.ai project,” https://github.com/Autoware-AI/autoware.ai, 2020.
[29] “Baidu apollo,” https://github.com/ApolloAuto/apollo, 2020.
[30] Siciliano, Bruno, and O. Khatib, Springer handbook of robotics.

Secaucus, NJ,

USA: Sprinter-Verlag New York, Inc.: Springer, 2016.

[31] S. Thrun, W. Burgard, and D. Fox, Probabilistic Robotics. The MIT Press, 2005.
[32] B. Dieber, B. Breiling, S. Taurer, S. Kacianka, S. Rass, and P. Schartner, “Security
for the robot operating system,” IEEE Trans. Robotics and Autonomous Systems,
vol. 98, pp. 192–203, 2017.

[33] T. Denning, C. Matuszek, K. Koscher, J. R. Smith, and T. Kohno, “A spotlight on
security and privacy risks with future household robots: attacks and lessons,” in
Ubiquitous Computing (UbiComp), 2009.

[34] C. Cerrudo and L. Apa, “Hacking robots before skynet,” IOActive Website, 2017.
[35] “Ros 2 robotic systems threat model,” https://design.ros2.org/articles/ros2_threat_model.html,

2020.

[36] J. R. Mcclean and C. Farrar, “A preliminary cyber-physical security assessment

of the robot operating system (ros),” in Proceedings of SPIE, 2013.

[37] “Robot vulnerability database (rvd),” https://github.com/aliasrobotics/RVD/,

2020.

[38] R. Toris, C. A. Shue, and S. Chernova, “Message authentication codes for secure
remote non-native client connections to ros enabled robots,” in International
Conference on Technologies for Practical Robot Applications (TePRA), 2014.
[39] R. Dóczi, B. S. Ferenc Kis, V. Poser, G. Kronreif, E. Josvai, and M. Kozlovszky,
“Increasing ros 1.x communication security for medical surgery robot,” in IEEE
International Conference on Systems, Man and Cybernetics (SMC), 2016.

REFERENCES
[1] “Openxc platform,” http://openxcplatform.com/, 2020.
[2] “Dji onboard sdk,” https://developer.dji.com/onboard-sdk/, 2020.
[3] “Application builder,” https://www.universal-robots.com/builder/, 2020.
[4] “Open source robot operating system,” http://www.ros.org/, 2019.
[5] “Ros pr2 package,” http://wiki.ros.org/Robots/PR2/, 2020.
[6] “Ros abb package,” http://wiki.ros.org/abb/, 2020.
[7] “Google play,” https://play.google.com/store/, 2020.
[8] “App store,” https://www.apple.com/ios/app-store/, 2020.
[9] “Windows apps - microsoft store,” https://www.microsoft.com/en-us/store/apps/windows/,

2020.

[10] “Ubuntu appstore,” https://ubuntu.com/blog/tag/appstore/, 2020.
[11] “The mac app store,” https://www.apple.com/uk/osx/apps/app-store//, 2020.
[12] “Samsung smartthings,” https://www.smartthings.com/, 2020.
[13] “Apple homekit,” https://developer.apple.com/homekit/, 2020.
[14] “Google weave project,” https://developers.google.com/weave/, 2020.
[15] N. DeMarinis, S. Tellex, V. P. Kemerlis, G. D. Konidaris, and R. Fonseca, “Scan-
ning the internet for ros: A view of security in robotics research,” in International
Conference on Robotics and Automation (ICRA), 2019.

[16] D. Quarta, M. Pogliani, M. Polino, A. M. Zanchettin, and S. Zanero, “Rogue
robots: Testing the limits of an industrial robot’s security,” Politecnico di Mi-
lano, Tech. Rep., 2017.
hack,
jeep

1.4m vehicles

[17] “After

chrysler

bugﬁx,”

recalls

for

https://www.wired.com/2015/07/jeep-hack-chrysler-recalls-1-4m-vehicles-bug-fix/,
2015.

[18] C.-J. M. Liang, B. F. Karlsson, N. D. Lane, F. Zhao, J. Zhang, Z. Pan, Z. Li, and
Y. Yu, “Sift: building an internet of safe things,” in International Symposium on
Information Processing in Sensor Networks (IPSN), 2015.

[40] B. Dieber, S. Kacianka, S. Rass, and P. Schartner, “Application-level security for
ros-based applications,” in International Conference on Intelligent RObots and Sys-
tems (IROS), 2016.

[41] V. Matellán, J. Balsa, F. Casado, C. Fernández, and F. J. R. Lera, “Cybersecurity
in autonomous systems: Evaluating the performance of hardening ros,” in XVII
Workshop En Agentes Físicos, 2016.

[42] T. Bonaci, J. Herron, T. Yusuf, J. Yan, T. Kohno, and H. J. Chizeck, “To make a
robot secure: An experimental analysis of cyber security threats against teleop-
erated surgical robots,” in CoRR abs/1504.04339, 2015.

[43] N. M. Rodday, R. de Oliveira Schmidt, and A. Pras, “Exploring security vulnera-
bilities of unmanned aerial vehicles,” in IEEE/IFIP Network Operations and Man-
agement Symposium (NOMS), 2016.

[44] J. Chen, Z. Feng, J.-Y. Wen, B. Liu, and L. Sha, “A container-based dos attack-
resilient control framework for real-time uav systems,” in Design, Automation,
and Test in Europe (DATE), 2019.

[45] N. O. Tippenhauer, C. Pöpper, K. B. Rasmussen, and S. Capkun, “On the require-
ments for successful gps spooﬁng attacks,” in ACM Conference on Computer and
Communications Security (CCS), 2011.

[46] N. Nighswander, B. M. Ledvina, J. Diamond, R. Brumley, and D. Brumley, “Gps
software attacks,” in ACM Conference on Computer and Communications Security
(CCS), 2012.

[47] K. C. Zeng, S. Liu, Y. Shu, D. Wang, H. Li, Y. Dou, G. Wang, and Y. Yang, “All
your gps are belong to us: Towards stealthy manipulation of road navigation
systems,” in USENIX Security Symposium (USENIX Security 18), 2018.

[48] J. S. Warner and R. G. Johnston, “A simple demonstration that the global posi-
tioning system (gps) is vulnerable to spooﬁng,” Journal of Security Administra-
tion, vol. 25, no. 2, pp. 19–27, 2002.

[79] “Rosbot teleoperation app,” https://husarion.com/tutorials/ros-tutorials/3-simple-kinematics-for-mobile-robot/,

2020.

[80] “Xiaoqiang voice interaction app,” https://community.bwbot.org/topic/492/,

2020.

[81] “Rosbot slam app,” https://husarion.com/tutorials/ros-tutorials/6-slam-navigation/,

2020.

[82] “Rosbot navigation app,” https://husarion.com/tutorials/ros-tutorials/7-path-planning/,

2020.

[83] “Rosbot exploration app,” https://husarion.com/tutorials/ros-tutorials/8-unknown-environment-exploration/,

2020.

[84] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Reinforcement learning for
autonomous driving,” in NIPS Workshop on Learning, Inference and Control of
Multi-Agent Systems, 2016.

[85] “An open source self-driving car,” https://www.udacity.com/self-driving-car/,

2020.

[86] B. B. Madan, M. Banik, and D. Bein, “Securing unmanned autonomous systems
from cyber threats,” Journal of Defense Modeling & Simulation, vol. 16, no. 2, pp.
119–135, 2016.

[87] M. Hooper, Y. Tian, R. Zhou, B. Cao, A. P. Lauf, L. Watkins, W. H. Robinson,
and W. Alexis, “A review on cybersecurity vulnerabilities for unmanned aerial
vehicles,” in Military Communications Conference (MILCOM), 2016.

[88] C. G. L. Krishna and R. R. Murphy, “A review on cybersecurity vulnerabilities for
unmanned aerial vehicles,” in IEEE International Symposium on Safety, Security,
and Rescue Robotics (SSRR), 2017.

[89] T. Kim, C. H. Kim, J. Rhee, F. Fei, Z. Tu, G. Walkup, X. Zhang, X. Deng, and D. Xu,
“Rvfuzzer: Finding input validation bugs in robotic vehicles through control-
guided testing,” in USENIX Security Symposium (USENIX Security 19), 2019.
[90] C. Nandi and M. D. Ernst, “Automatic trigger generation for rule-based smart

homes,” in PLAS@CCS, 2016.

[91] T. Yu, V. Sekar, S. Seshan, Y. Agarwal, and C. Xu, “Handling a trillion (unﬁx-
able) ﬂaws on a billion devices: Rethinking network security for the internet-of-
things,” in HotNets, 2015.

[49] S.-H. Seo, B.-H. Lee, S.-H. Im, and G.-I. Jee, “Eﬀect of spooﬁng on unmanned
aerial vehicle using counterfeited gps signal,” Journal of Positioning, Navigation,
and Timing, vol. 4, no. 2, p. 57–65, 2015.

[50] H. Shin, D. Kim, Y. Kwon, and Y. Kim, “Illusion and dazzle: Adversarial opti-
cal channel exploits against lidars for automotive applications,” in International
Workshop on Cryptographic Hardware and Embedded Systems (CHES), 2017.
[51] Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A. Chen, K. Fu, and
Z. M. Mao, “Adversarial sensor attack on lidar-based perception in autonomous
driving,” in ACM Conference on Computer and Communications Security (CCS),
2019.

[52] D. Davidson, H. Wu, R. Jellinek, V. Singh, and T. Ristenpart, “Controlling uavs
with sensor input spooﬁng attacks,” in Workshop on Oﬀensive Technologies
(WOOT), 2016.

[53] Y. Son, H. Shin, D. Kim, Y.-S. Park, J. Noh, K. Choi, J. Choi, and Y. Kim, “Rocking
drones with intentional sound noise on gyroscopic sensors,” in USENIX Security
Symposium (USENIX Security 15), 2015.

[54] T. Trippel, O. Weisse, W. Xu, P. Honeyman, and K. Fu, “Walnut: Waging doubt
on the integrity of mems accelerometers with acoustic injection attacks,” in Eu-
roS&P, 2017.

[55] Y. Tu, Z. Lin, I. Lee, and X. Hei, “Injected and delivered: Fabricating implicit
control over actuation systems by spooﬁng inertial sensors,” in USENIX Security
Symposium (USENIX Security 18), 2018.

[56] Y. Shoukry, P. D. Martin, P. Tabuada, and M. B. Srivastava, “Non-invasive spoof-
ing attacks for anti-lock braking systems,” in International Workshop on Crypto-
graphic Hardware and Embedded Systems (CHES), 2013.

[57] F. Fei, Z. Tu, R. Yu, T. Kim, X. Zhang, D. Xu, and X. Deng, “Cross-layer retroﬁtting
of uavs against cyber-physical attacks,” in IEEE International Conference on Ro-
botics and Automation (ICRA), 2018.

[58] H. Choi, W.-C. Lee, Y. Aafer, F. Fei, Z. Tu, X. Zhang, D. Xu, and X. Xinyan, “De-
tecting attacks against robotic vehicles: A control invariant approach,” in ACM
Conference on Computer and Communications Security (CCS), 2018.

[59] D. Quarta, M. Pogliani, M. Polino, F. Maggi, A. M. Zanchettin, and S. Zanero,
“An experimental security analysis of an industrial robot controller,” in IEEE
Symposium on Security and Privacy (S&P), 2017.

[60] X. Pan, Y. Cao, X. Du, B. He, G. Fang, R. Shao, and Y. Chen, “Flowcog: Context-
aware semantics extraction and analysis of information ﬂow leaks in android
apps,” in USENIX Security Symposium (USENIX Security 18), 2018.

[61] E. Fernandes, J. Jung, and A. Prakash, “Security analysis of emerging smart home

applications,” in IEEE Symposium on Security and Privacy (S&P), 2016.

[62] R. Xu, H. Saïdi, and R. J. Anderson, “Aurasium: Practical policy enforcement
for android applications,” in USENIX Security Symposium (USENIX Security 12),
2012.

[63] X. yong Zhou, S. Demetriou, D. He, M. Naveed, X. Pan, X. Wang, C. A. Gunter,
and K. Nahrstedt, “Identity, location, disease and more: inferring your secrets
from android public resources,” in ACM Conference on Computer and Communi-
cations Security (CCS), 2013.

[64] E. Fernandes, J. Paupore, A. Rahmati, D. Simionato, M. Conti, and A. Prakash,
“Flowfence: Practical data protection for emerging iot application frameworks,”
in USENIX Security Symposium (USENIX Security 16), 2016.

[65] Y. M. P. Pa, S. Suzuki, K. Yoshioka, T. Matsumoto, T. Kasama, and C. Rossow,
“Iotpot: Analysing the rise of iot compromises,” in USENIX Workshop on Oﬀensive
Technologies (WOOT), 2015.

[66] X. yong Zhou, Y. Lee, N. Zhang, M. Naveed, and X. Wang, “The peril of frag-
mentation: Security hazards in android device driver customizations,” in IEEE
Symposium on Security and Privacy (S&P), 2014.

[67] “Stanford tokensregex,” https://nlp.stanford.edu/software/tokensregex.html,

2020.

[68] “Robots that you can use with ros.” https://robots.ros.org/, 2020.
[69] “Sensors supported by ros.” http://wiki.ros.org/Sensors/, 2020.
[70] B. Boroujerdian, H. Genc, S. Krishnan, W. Cui, A. Faust, and V. J. Reddi,
“Mavbench: Micro aerial vehicle benchmarking,” in Annual IEEE/ACM Interna-
tional Symposium on Microarchitecture (MICRO), 2018.

[71] S.-C. Lin, Y. Zhang, C.-H. Hsu, M. Skach, M. E. Haque, L. Tang, and J. Mars, “The
architectural implications of autonomous driving: Constraints and acceleration,”
in International Conference on Architectural Support for Programming Languages
and Operating Systems (ASPLOS), 2018.

[72] “Ros messages,” http://wiki.ros.org/Messages/, 2020.
[73] “Rosbot 2.0 pro,” https://store.husarion.com/collections/dev-kits/products/rosbot-pro/,

2020.

[74] “Turtlebot3 autorace,” https://emanual.robotis.com/docs/en/platform/turtlebot3/autonomous_driving,

2020.

[75] “Turtlebot3,” https://emanual.robotis.com/docs/en/platform/turtlebot3/overview/,

2020.

[76] “Gazebo 3d robot simulator.” http://gazebosim.org/, 2020.
[77] “Lgsvl simulator.” https://www.lgsvlsimulator.com/, 2020.
[78] “Rviz 3d visualization tool for ros.” https://www.stereolabs.com/docs/ros/rviz/,

2020.

A IMPLEMENTATION OF CODE ANALYSIS
As discussed in the § 3.1, we require an understanding of how many
function nodes in the interaction graph. In this section, we describe
our approach to automatically map various repos in the ROS plat-
form to several functions.

A.1 Key Information Extraction
To identify the function type of a ROS repo, three particular at-
tributes of the repos can be inspected:

(1) the repo name, which can directly reﬂect the functionality

of this repo;

(2) the manifest ﬁle (i.e. package.xml), which shows a func-

tional brief of the repo.

(3) the related document (i.e. README ﬁle), which presents
the detailed information of this package, including func-
tion description, topics and services;

Listing 1 shows an example of the three attributes in rrt_exploration
repo. We regard the package name as one piece of key information
and extract other two pieces of key information from the manifest
ﬁle and related document. Speciﬁcally, we identify the text within
the key tags (i.e. <description></description>) as key information
in the manifest ﬁle. For the related document, we need ﬁrst ﬁlter
out the useless interference, such as the installation command and
requirements introduction in the example. Then we use the remain-
ing description of the repo as the key information.

image p r o c e s s i n g

I t

t h e

t h a t

t h a t

i m p le m e nts a m u l t i − r o b o t map

i m p le m e nts a m u l t i − r o b o t
a l s o ha s

*** the repo name ***
r r t _ e x p l o r a t i o n
*** the manifest ﬁle ***
< d e s c r i p t i o n >A ROS p a c k a g e
RRT− b a s e d map e x p l o r a t i o n a l g o r i t h m .
image − b a s e d f r o n t i e r d e t e c t i o n t h a t u s e s
f r o n t i e r p o i n t s < / d e s c r i p t i o n >
t o e x t r a c t
. . . . . .
*** the related document ***
I t
i s a ROS p a c k a g e
e x p l o r a t i o n a l g o r i t h m f o r m o b i l e r o b o t s .
t h e R a p i d l y − E x p l o r i n g Random T r e e ( RRT ) a l g o r i t h m .
u s e s oc c u p a nc y g i r d s a s a map r e p r e s e n t a t i o n . The p a c k a g e
ha s 5 d i f f e r e n t ROS n o d e s :
( 1 ) G l o b a l RRT f r o n t i e r p o i n t d e t e c t o r node .
( 2 ) L o c a l RRT f r o n t i e r p o i n t d e t e c t o r node .
. . . . . .
1 . R e q u i r e m e n t s
The p a c k a g e ha s been t e s t e d on b oth ROS K i n e t i c and ROS
J a d e .
s h o u l d work on o t h e r d i s t r i b u t i o n s
I n d i g o ,
$ sudo apt-get install ros-kinetic-gmapping
. . . . . .
2 .
Download t h e p a c k a g e and p l a c e
i n y ou r wor k s p a c e . And the n c o m p i l e u s i n g c a t k i n _ m a k e .
. . . . . .

I n s t a l l a t i o n

i s b a s e d on

t h e / s r c

i n s i d e

f o l d e r

l i k e

I t

I t

i t

i t

Listing 1: An example of the three attributes

A.2 Function Classiﬁcation
To categorize 941 repos into 17 types of robotic functions in Table
1, we ﬁrst select 500 repos as a training set and use our expert ex-
perience to manually conclude the rules for each type from three
pieces of key information in each repo. Then we use string regular
expression to match these rules with tokens in the key informa-
tion over above the rest of 441 repos. Finally, we manually analyze

Table 8: The number of successful identiﬁcations in 941 re-
pos.

Function Type

Preprocessing
Localization
Mapping
Recognition
Path Planning
Goal Planner
Path Tracking
Teleoperation
Speech Generation
Switch
Mobile
Manipulator
Speaker
Sensor
Visualization
Support
Extension
Function Node

Repo
Name
18
17
15
21
44
5
8
7
2
3
2
2
0
10
98
14
23
154

Manifest
File
41
13
11
23
49
4
37
21
1
2
26
30
0
77
60
45
72
335

ReadMe
File
13
3
4
4
5
0
11
1
5
1
4
2
5
16
11
25
18
74

Manual

12
2
1
2
5
2
12
1
1
0
6
3
1
25
0
27
106
73

Automation
Rate
85.71%
94.29%
96.77%
96.00%
95.15%
81.82%
82.35%
96.67%
88.89%
100.00%
84.21%
91.89%
83.33%
80.47%
100.00%
75.68%
51.60%
88.52%

these 341 repos to verify the correctness of our automatic classi-
ﬁcation. Taking the Visualization function as an example, the
rule is to determine whether the key information contains one of
two kinds of tokens. One token is the function-related words and
their variants, such as visualize and simulation. The other token is
the function-related tool names, such rqt and gazebo. These tools
are designed to visualize the robot and diﬀerent tasks. Due to the
fact that the more information probably hides more inferences, we
identify the function types from repo name, manifest ﬁle and re-
lated documents successively, which means the matching process
is ﬁnished if succeeds in one piece of key information. The rest of
repos cannot be detected automatically will be analyzed manually.
Table 8 shows the results of our automated function classiﬁca-
tion. The automation rate is the radio of the successfully identi-
ﬁed number to the sum of repos in this function. We can observer
that the automation rate of most function types is more than 80%
except the Support and Extension functions. Fortunately, the re-
pos in the Visualization, Support and Extension belong to the
Others domain, they are independent of the function nodes in the
interaction graph. Thus, considering the repos in the ﬁrst 14 func-
tion types, the automation rate of function node can reach 88.52%.
Among these successfully automated classiﬁed repos, the accuracy
of our classiﬁcation can achieve 99.82%.

B THE DESCRIPTION OF ROS PLATFORM

AND APPS

B.1 The Relationship of ROS Elements
Figure 12 shows the relationship of diﬀerent elements in the ROS
platform. As described in § 2.2, the ROS platforms maintains a list
of ROS indexes (i.e. repo names), each index link to the source code
of this repo in the hosting site. A repo commonly consists of one
or multiple ROS packages, and its related document (i.e. ReadMe).
Each package concludes source code and a manifest ﬁle (i.e. pack-
age.xml). The manifest ﬁle is used to describe its version, descrip-
tion and dependence. A robotic function can be implemented by
one or multiple packages. It means one repo can have two or more

The ROS Platform

1. mqtt_bridge
2. rqt_image_view
3. rtt_exploration
4. wge100_driver
5. jsk_recognition
(cid:258)(cid:258)

Repo
Name

link to the 
hosting site

One Repo

Package 1
Package 2
Package 3
Package 4
ReadME
(cid:258)(cid:258)

Related Document

Multiple
Repos

Function 1

Manifest 
File

Package 1

src
package.xml
(cid:258)(cid:258)

Function 1

Function 2

Function 4

ROS 
Application

Implement

Implement

Function 2

Function 3

Figure 12: The relationship among the app, repo, package
and function.

functions. These functions are then integrated with some functions
in other repos or customized by users to consist a ROS application.

B.2 Descriptions of Commonly Used Apps
We describe the top ﬁve robot apps in ROS platform that are most
commonly used, according to Table 2.
Remote control. This type of apps is designed to control the ro-
bot remotely from smartphones, joysticks or keyboards. It uses
Teleoperration to receive the signals from the remote controller
and Mobile/Manipulator Driver to transfer these signals to each
actuator’s control command.
2D/3D mapping. These apps aim to create a 2D/3D map of un-
known environment through remote control. End users use Tele
operration to move the robot to explore the unknown zones. Dur-
ing the exploration, Preprocessing sends structural sensory data
to Mapping for map creation.
Navigation. This type of apps instructs a robot to navigate through
an obstacle-ﬁlled known environment and reach a speciﬁed desti-
nation. These apps use Localization to estimate the robot’s po-
sition, and Path Planning to compute a collision-free path from
its position to the destination. Then, Path Tracking is called to
follow the path until the robot achieves the goal or the mission
fails.
SLAM. These apps can be regarded as the combination of Mapping
and Navigation. To reach an arbitrary destination in an obstacle-
ﬁlled unknown environment, the apps use Mapping and Localization
simultaneously to transfer the unknown map to a known one and
locate its position.
Face/Person Detection. These apps receive images from cameras
(Preprocessing) and apply the OpenCV face/person detector based
on an Adaboost cascade of Haar features/HOG (Recognition). They
publish regions of interests (ROIs) of the detection and a debug im-
age, showing the processed image with the ROIs that is likely to
contain faces or persons.

C MISSION-SPECIFIC RISK MESSAGE TYPE
We present the description of the EVENT_MSG_TYPE in Table 9
and ACTION_MSG_TYPE in Table 10.

Table 9: Description of EVENT_MSG_TYPE.

Message Type
sensor_msgs/
BatteryState
sensor_msgs/
Temperature

sensor_msgs/
RelativeHumidity

sensor_msgs/
MagneticField

sensor_msgs/
FluidPressure

sensor_msgs/
NavSatFix
sensor_msgs/
Illuminance
nav_msgs/
Odometry

Description
Measurement of the battery state (voltage,
charge, etc).

Measurement of the temperature.

Deﬁnes the ratio of partial pressure of water
vapor to the saturated vapor pressure at a
temperature.
Measurement of the Magnetic Field vector at
a speciﬁc location.
Measurement of the pressure inside of a ﬂuid
(air, water, etc), atmospheric or barometric
pressure.
Measurement for any Global Navigation
Satellite System (latitude, longitude, etc).
Measurement of the single photometric
illuminance.
Measurement of an estimate of a position
and velocity in free space (pose, twist, etc).

Table 10: Description of ACTION_MSG_TYPE.

Actuator Message Type

Mobile

Manipulator

Speaker

geometry_msg/
Twist

control_msgs/
FollowJoint
TrajectoryAction
audio_common
_msg/AudioData

Description
This expresses the velocity in
free space broken into its linear
and angular parts.

This deﬁnes the joint trajectory
to follow.

This deﬁnes the audio data to
speak.

D THE RTRON USER CONSOLE
We present the description of the end user console of general risk
coordination node, robot-speciﬁc risk coordination node and mission-
speciﬁc risk coordination node in Figure 13(a), Figure 13(b) and
Figure 13(c), respectively.

E THE APP INTERACTION GRAPH
Figure 16 shows a complete interaction graph of home app. Gray el-
lipses denote the function nodes; while rectangles represent topics.
Each two nodes are connected through topics. We use black arrows
to denote these interactions. The interactions of GR-ST and GR-MT
are marked with blue and red, receptively. We also use purple rect-
angles with/without diagonal stripes to denote MSR event-related
and MSR action-related topics. The RSR image-related nodes is de-
picted in yellow ellipses. Table 11 gives one example of the identi-
ﬁed high-risk node for each type in two apps.

Figure 17 shows the coordination node distribution in the com-
plete interaction graph of home app. We use green, red and cyan
ellipses to denote GR, RSR and MSR CN receptively. The redirected
interactions between normal/high-risk nodes and CN are marked
with the same color of each type of CN.

!"#$%&’(#)*(+
,"#$%-’(#)*(+%."//),0
!"#$%&#’()%*+,-(4*$&3$,(9,$#)".F()#’.1#$(C#0+(+G%1,(9,$#)".F(+.%.,(
!"#$%.,5(1*$,-(2,’,1%$(1*$,(84*$&3$,(’#5,+(3*D$"+G(.#(#’,(’#5,:

,"#$%-’(#)*(+%1$2)’#.
;1"22,1(&4,-(<=<=>=?><=(@<-=A-=@
C#0<

!"#$%&’(#)*(+
,"#$%-’(#)*(+%."//),0

!"#$%&’(#)*(+
,"#$%-’(#)*(+%."//),0

!"#$%&#’()%*+,-(4"+4%.)G(D,.0,,’(H3+(%’5(4%I(9,$#)".F()#’.1#$(
!"#$%.,5(1*$,-(1#D#.6+3,)"7)(1"+/(84%IJ9,$6(%’5("4%2,61,$%.,5(’#5,+:
,"#$%-’(#)*(+%1$2)’#.
;1"22,1(&4,-(<=<=>=?><=(@<-=A-=?

!"#$%&#’()%*+,-(.%/,(%)&#’+(%.(01#’2(3$%),+(#1(01#’2(&4,(
!"#$%.,5(1*$,-(4"++"#’6+3,)"7)(1"+/(8,9,’.6(%’5(%)&#’61,$%.,5(’#5,+:
,"#$%-’(#)*(+%1$2)’#.
;1"22,1(&4,-(<=<=>=?><=(@<-=A-=B

+)-’F)*(+%
7(+2,(#

C#0@

9,$#)".F

2$#$G
(6$,)*(+

9,$#)".F

.)?$%
7(+2,(#

C#0A

9,$#)".F

;)<=-$#%
,$#)2$1%4(5

9C#0@-(3%.G(.1%)/"’2

E/)F$%
,$#)2$1%4(5

"C#0@-(5,.,).(3%1/"’2
"C#0<-(5,.,).($%’,

3-$+2%
,$#)2$1%4(5

,C#0@-(D%E,1F(4#’".#1
,C#0<-(5,.,).(3%1/"’2

:7*(+%
,$#)2$1%4(5

%C#0@-(4#9,
%C#0<-(21%+3

6(#’7’$.%(6*(+.

6(#’7’$.%(6*(+.

8#(79

D,$$/6*(+

HEHI=J"$"$

D,’(,’20=J"$"$

8#(79

>)?$

@(+.2,)’+

)$")/

)$")/

)$")/

)$")/

)$")/

)$")/

)$")/

8#(79%8’2

((C#0@-((=
((C#0<-((@
((C#0A-((=

D,’(,’20
((C#0@-((@
((C#0<-((<
((C#0A-((A

A’/$("2%K.L
((C#0@-((=><
((C#0<-((=>@
((C#0A-((=>=M

D,’(,’20C%A’/$("2
((C#0@-((@K(=><
((C#0<-((<K(=>@
((C#0A-((AK(=>=M

8#(79%8’2
((9C#0@-((=

AB,$.B(#1C%D,’(,’20
((9C#0@-((@>@K(A
(("C#0<-(((A><K(@
(("C#0A-(((@>LK(<

;)<=-$#=#’/’2
((9C#0@-((@><

6(#’7’$.%(6*(+.

8#(79

)$")/

8#(79%8’2
((,C#0@-((@(((((%C#0@-(((@
((,C#0@-((=(((((,C#0<-(((@

(a) Developer console of GRCN

(b) End user console of RSRCN

(c) End user console of MSRCN

Figure 13: Developer and end user console of each risk in RTron. The red solid rectangle denotes a button for the end users.
The blue/red box represents policy-related conﬁguration parameters for the developers/end users.

Table 11: Examples of high-risk nodes in the Home and AutoRace apps. Texts marked in red are for risk identiﬁcation in our
system.

Scenario

Risk Type

High-Risk Nodes

Sub Topic Name

Home

AutoRace

GR-ST

GR-MT

RSR-Image
MSR-Event
MSR-Action

GR-ST

GR-MT

RSR-Image
RSR-Max_vel
MSR-Event
MSR-Action

/move_base
/teleop_twist_keyboard
/gazebo
/gazebo
/ﬁnd object 3d
/move_base
/rosbot_tts
/detect_tunnel
/rviz
/detect/lane
/detect_traﬃc_light
/detect_sign
/detect_parking
/core_node_controller
/detect_tunnel

-
-
-
-
/camera/rgb/image raw
/odom
-
-
-
-
-
/camera/image_compensated
-
/detect/tunnel_stamped
-

Sub Topic Type

-
-
-
-
sensor_msgs/Image
nav_msgs/Odometry
-
-
-
-
-
sensor_msgs/Image
-
std_msgs/UInt8
-

Pub Topic Name

Pub Topic Type

Pub Node

/cmd_vel
/cmd_vel
/camera/depth/image_raw
/camera/rgb/image_raw
/objects
-
audio_common_msgs/AudioData
/move_base_simple/goal
/move_base_simple/goal
/detect/lane
/control/max_vel
/detect/traﬃc_sign
/control/max_vel
-
/cmd_vel

geometry_msgs/Twist
geometry_msgs/Twist
sensor_msgs/Image
sensor_msgs/Image
std msgs/Float32MultiArray
-
/rosbot_audio/audio
geometry_msgs/PoseStamped
geometry_msgs/PoseStamped
std_msgs/Float64
std_msgs/Float64
std_msgs/UInt8
std_msgs/Float64
-
geometry_msgs/Twist

/gazebo
/gazebo
/ﬁnd_object_3d
/ﬁnd_object_3d
/search manager
-
/rosbot_audio
/move_base_simple/goal
/move_base_simple/goal
/control/lane
/control/lane
/core_mode_decider
/control_lane
-
/gazebo

collision

/control/max_vel
: 2m/s

RSR Nodes
[Max_vel] /tb3_monitor
[Image] /move_base

Pub Topic Name
/control_max_vel
/detect/objects

RSRCN Policy
RSRCN Flows
vflow

Constrain
Parameter
0.22

Turtlebot3
/control/max_vel: 
0.22m/s

Figure 14: RSR experiment on Turtlebot3 in the simulation.

F THE ATTACK IMPLEMENTATION AND RSR

CASE

F.1 The Attack Implementation
As § 7.4 describes, the malicious codes of three types of risks are
hidden in the two normal robot functional packages: /tb3_safe_control
and tb3_monitor. The /tb3_safe_control provides commands
for safe teleoperation with diﬀerent input devices. It use LaserScan
information to estimated the distance between the robot and obsta-
cles, and stop the robot’s movement within a customized safe dis-
tance. The tb3_monitor package provides commands to monitor
nodes’ information and robot’s states in real time.

Figure 15 illustrates the attack and its consequences. The mali-
cious code of GR attack is added in the /tb3_safe_control pack-
age. The malicious codes of RSR and MSR attack are all hidden in
the tb3_monitor package. It’s worth noting that we add speciﬁc
triggering logics (Lines 2) in each attack to avoid raising ethical
concerns. The triggering condition is the success match between
the default MAC address and local host MAC address. Since the
MAC address is unique of diﬀerent devices, the malicious codes
can only work in our robotic devices. Moreover, we set time match-
ing process to make the attack launch at speciﬁc time, other than
at the beginning. This can make attacks more hidden.

Figure 15(a) shows the related code snippets in the GR attack.The
gr_attack function is invoked by a callback function of LaserScan
Topic. In each iteration, the function starts by searching the gr-
related vulnerable node, i.e. ‘move_base’. If exists, it means the
move-related control topic ‘cmd_vel’ exists and the robot is exe-
cuting a navigation task with a great probability. Thus, we send a
Twist-type move command with -0.2 z-axis angular velocity to the
victim topic. This would lead to the robot suddenly turn right and
crash to the obstacles while navigating in a collision-free path.

Figure 15(b) and (c) present the malicious code snippets in the
RSR and MSR attack respectively. Both rsr_attack and msr_attack
functions are invoked while each traversal of all topics of one node.
Speciﬁcally, once the ‘control/max_vel’ topic exists, the malicious

1 d e f g r _ a t t a c k ( ) :
2

i f mac == HOST_MAC:

3

4

5

6

7

8

9

10

11

12

i f

c u r _ t i m e _ m i n == 15 and
n o d e _ e x i s t ( ' move_base ' ) :

1 d e f
2

r s r _ a t t a c k ( ) :

i f mac == HOST_MAC:

t w i s t = T w i s t ( )
t w i s t . l i n e a r . x = t w i s t . l i n e a r . y
= t w i s t . l i n e a r . z = 0 . 0
t w i s t . a n g u l a r . x = t w i s t . a n g u l a r

. y = 0 . 0

t w i s t . a n g u l a r . z = − 0 . 2

g r _ a t k _ p u b = r o s p y . P u b l i s h (

' c m d_v e l ' , T wi s t ,
q u e u e _ s i z e = 1 0 )
g r _ a t k _ p u b . p u b l i s h ( t w i s t )

3

4

5

6

7

8

9

10

11

12

c u r _ t i m e _ m i n == 30 and
t o p i c _ e x i s t ( ' c o n t r o l / m a x_v e l ' )

i f

:

m a x_v e l = F l o a t 6 4 ( )
m a x_v e l . d a t a = 2

r s r _ a t k _ p u b = r o s p y . P u b l i s h e r (
' c o n t r o l / m a x_v e l ' , F l o a t 6 4 ,
q u e u e _ s i z e = 1 )

r s r _ a t k _ p u b . p u b l i s h ( m a x_v e l )

1 d e f m s r _ a t t a c k ( ) :
2

i f mac == HOST_MAC:

3

4

5

6

7

8

9

10

11

12

13

14

i f

c u r _ t i m e _ m i n == 45 and
t o p i c _ e x i s t ( ' m o v e _ b a s e _ s i m p l e /

g o a l ' ) :

g o a l = Pos e S ta m p e d ( )
g o a l . h e a d e r . stamp = now ( )
g o a l . h e a d e r . f r a m e _ i d = " map "
g o a l . p os e . p o s i t i o n = MAL_LOC

m s r _a tk _p u b = r o s p y . P u b l i s h e r (

' m o v e _ b a s e _ s i m p l e / g o a l ' ,
PoseStamped , q u e u e _ s i z e = 1 )

m s r _a tk _p u b . p u b l i s h ( g o a l )

(a) Malicious codes of GR attack

(b) Malicious codes of RSR attack

(c) Malicious codes of MSR attack

Figure 15: Malicious codes in our tb3_safe_teleop and tb3_monitor packages.

process would send a max velocity control command with 2 m/s to
the victim topic. Similarly, if the ‘move_base_simple/goal’ topic
exists, a goal with a malicious location will be launched to the vic-
tim topic and the robot would move to the dangerous destination.

F.2 The RSR Case Study in Simulation
We use the same simulation setup described in Section 7. The tb3_monitor
node sends malicious max velocity conﬁguring commands druing
the robot’s navigation at speciﬁc moments. It increases the max
velocity value through publishing the malicious messages to the
/control_max_vel topic. In Figure 14, the initial max velocity is
0.22m/s and the robot moves safely. At one moment, this value is
increased to 2m/s. Then the robot moves too fast to detect the ob-
stacle and a collision occurs. In RTron, we choose Constrain policy
in the RSRCN and set 0.22 to the vﬂow. In this way, the max veloc-
ity of the robot is ﬁxed at 0.22m/s and cannot be changed by the
attacker.

a
n
d
l
o
g
n
o
d
e

(
i
.
e

.

/
r
o
s
n
o
d
e
)
a
r
e
d
e
l
e
t
e
d
i
n
t
h
e
ﬁ
g
u
r
e

.

F
i
g
u
r
e

1
6

:

T
h
e

I
n
t
e
r
a
c
t
i
o
n
G
r
a
p
h
o
f
R
o
b
o
t

A
p
p
l
i
c
a
t
i
o
n
s

i
n
H
o
m
e

S
c
e
n
a
r
i
o

.

T
h
e

s
u
b
s
c
r
i
p
t
i
o
n
s
o
f

v
i
s
u
a
l
i
z
a
t
i
o
n
n
o
d
e

(
i
.
e

.

/
r
v
i
z
)

/joint_state_controller_spawner

/clicked_point

/explore_client

/exploreation_polygon_maker

rosbot_asr

/rosbot_asr/text

/rosbot_nlp

audio

/audio/audio_capture

/audio/audio

/rosbot_asr

MSR Action-Related Topic

GR MT Interaction

Normal Topic

Normal Interaction

MSR Event-Related Topic

GR ST Interaction

rosbot_tts

/rosbot_tts/text

/rosbot_tts

robsbot_audio

/rosbot_audio/audio_status

/rosbot_audio/audio

camera

/rosbot_audio

Normal Node

RSR Image-Related Node

move_base_simple

/rviz

/move_base_simple/goal

/teleop_twist_keyboard

/cmd_vel

/gazebo

/camera/depth/camera_info

/depthImage_to_laserscan

/proj_scan

/camera/depth/image_raw

/ﬁnd_object_3d

/objects

/camera/rgb/image_raw

/person_detector

/person_detector/detections

/joint_states

/robot_state_publisher

obstacles

/obstacles/checked

/obstacles/pending

explore_server

/laser_broadcaster

/tf

/search_manager

/explore_server/status

/gmapping_node

/map

/tf_static

/explore_server/cancel

/explore_server/explore_costmap/explore_boundary/frontiers

/explore_server

/explore_server/explore_costmap/costmp_updates

/explore_server/explore_costmap/costmp

/explore_server/explore_costmap/footprint

/odom

/scan

move_base

/move_base/NavfnROS/plan

/move_base/global_costmap/footprint

/move_base

/move_base/local_costmap/costmap_updates

/move_base/Trajectory/PlannerROS/global_plan

/move_base/local_costmap/costmap

/move_base/Trajectory/PlannerROS/local_plan

/move_base/global_costmap/costmap_updates

/move_base/global_costmap/costmap

/move_base/locall_costmap/footprint

/move_base/status

/move_base/goal

/move_base/cancel

/move_base/max_vel

F
i
g
u
r
e
1
7

:

T
h
e
C
o
o
r
d
i
n
a
t
i
o
n
N
o
d
e
D
i
s
t
r
i
b
u
t
i
o
n
i
n
t
h
e

I
n
t
e
r
a
c
t
i
o
n
G
r
a
p
h
o
f
R
o
b
o
t

A
p
p
l
i
c
a
t
i
o
n
s

i
n
H
o
m
e
S
c
e
n
a
r
i
o

.

/joint_state_controller_spawner

/clicked_point

/explore_client

/exploreation_polygon_maker

rosbot_asr

/rosbot_asr/text

/rosbot_nlp

rosbot_tts

/rosbot_tts/text

audio

/audio/audio_capture

/audio/audio

/rosbot_asr

/rosbot_tts

robsbot_audio

/rosbot_audio/audio_status

/rosbot_audio/audio

/rosbot_audio

Normal Topic

Normal Interaction

Normal Node

MSR Event-Related Topic

GR Redirect Interaction

RSR Image-Related Node

MSR Action-Related Topic

RSR Redirect Interaction

/msr/audio

MSR Redirect Interaction

GR J

RSR @

MSR F

/gr_cn_(cid:6)

/rviz

camera

/camera/depth/camera_info

/depthImage_to_laserscan

/proj_scan

/rsr_fps_monitor_(cid:15)

/msr_cn_(cid:16)

/rsr_fps_monitor_(cid:14)

/msr/
cmd_vel

/camera/depth/image_raw

/ﬁnd_object_3d

/objects

/gr_cn_(cid:4)

/camera/rgb/image_raw

/person_detector

/person_detector/detections

move_base_simple

/move_base_simple/goal

/teleop_twist_keyboard

/cmd_vel

/gazebo

/joint_states

/robot_state_publisher

/gr_cn_(cid:5)

obstacles

/obstacles/checked

/obstacles/pending

explore_server

/gr_cn_(cid:7)

/gr_cn_(cid:1)

/odom

/scan

/laser_broadcaster

/gr_cn_3

/tf_static

move_base

/move_base/NavfnROS/plan

/move_base/global_costmap/footprint

/move_base

/move_base/local_costmap/costmap_updates

/tf

/gr_cn_!

/search_manager

/explore_server/status

/gmapping_node

/map

/gr_cn_(cid:2)

/explore_server/cancel

/explore_server/explore_costmap/explore_boundary/frontiers

/explore_server

/explore_server/explore_costmap/costmp_updates

/gr_cn_(cid:0)

/explore_server/explore_costmap/costmp

/explore_server/explore_costmap/footprint

/move_base/Trajectory/PlannerROS/global_plan

/move_base/local_costmap/costmap

/move_base/Trajectory/PlannerROS/local_plan

/move_base/global_costmap/costmap_updates

/move_base/global_costmap/costmap

/move_base/locall_costmap/footprint

/move_base/status

/move_base/goal

/move_base/cancel

/gr_cn_(cid:9)

/move_base/max_vel

/rsr_cn_(cid:13)

/rsr/objects/fps

/rsr/person/fps

/gr_cn_(cid:3)

/gr_cn_(cid:11)

(cid:8)
(cid:10)
(cid:12)
(cid:17)
(cid:18)
(cid:19)
