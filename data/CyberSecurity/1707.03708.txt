7
1
0
2

t
c
O
6
1

]

R
C
.
s
c
[

2
v
8
0
7
3
0
.
7
0
7
1
:
v
i
X
r
a

Proactive Defense Against Physical Denial of Service
Attacks using Poisson Signaling Games

Jeffrey Pawlick and Quanyan Zhu(cid:63)

New York University Tandon School of Engineering
Department of Electrical and Computer Engineering
6 MetroTech Center, Brooklyn, NY 11201. {jpawlick,quanyan.zhu}@nyu.edu

Abstract. While the Internet of things (IoT) promises to improve areas such as
energy efﬁciency, health care, and transportation, it is highly vulnerable to cyber-
attacks. In particular, distributed denial-of-service (DDoS) attacks overload the
bandwidth of a server. But many IoT devices form part of cyber-physical systems
(CPS). Therefore, they can be used to launch “physical” denial-of-service attacks
(PDoS) in which IoT devices overﬂow the “physical bandwidth” of a CPS. In this
paper, we quantify the population-based risk to a group of IoT devices targeted
by malware for a PDoS attack. In order to model the recruitment of bots, we de-
velop a “Poisson signaling game,” a signaling game with an unknown number of
receivers, which have varying abilities to detect deception. Then we use a version
of this game to analyze two mechanisms (legal and economic) to deter botnet
recruitment. Equilibrium results indicate that 1) defenders can bound botnet ac-
tivity, and 2) legislating a minimum level of security has only a limited effect,
while incentivizing active defense can decrease botnet activity arbitrarily. This
work provides a quantitative foundation for proactive PDoS defense.

1

Introduction to the IoT and PDoS Attacks

The Internet of things (IoT) is a “dynamic global network infrastructure with self-
conﬁguring capabilities based on standard and interoperable communication protocols
where physical and virtual ‘things’ have identities, physical attributes, and virtual per-
sonalities” [2]. The IoT is 1) decentralized, 2) heterogeneous, and 3) connected to the
physical world. It is decentralized because nodes have “self-conﬁguring capabilities,”
some amount of local intelligence, and incentives which are not aligned with the other
nodes. The IoT is heterogeneous because diverse “things” constantly enter and leave
the IoT, facilitated by “standard and interoperable communication protocols.” Finally,
IoT devices are connected to the physical world, i.e., they are part of cyber-physical
systems (CPS). For instance, they may inﬂuence behavior, control the ﬂow of trafﬁc,
and optimize home lighting.

(cid:63) This work is partially supported by an NSF IGERT grant through the Center for Interdisci-
plinary Studies in Security and Privacy (CRISSP) at New York University, by the grant CNS-
1544782, EFRI-1441140, and SES-1541164 from National Science Foundation (NSF) and
de-ne0008571 from the Department of Energy.

 
 
 
 
 
 
Fig. 1. Conceptual diagram of a PDoS attack. 1) Attack sponsor hires botnet herder. 2) Botnet
herder uses server to manage recruitment. 3) Malware scans for vulnerable IoT devices and begins
cascading infection. 4) Botnet herder uses devices (e.g., HVAC controllers) to deplete bandwidth
of a cyber-physical service (e.g., electrical power).

1.1 Difﬁculties in Securing the Internet of Things

While the IoT promises gains in efﬁciency, customization, and communication ability, it
also raises new challenges. One of these challenges is security. The social aspect of IoT
devices makes them vulnerable to attack through social engineering. Moreover, the dy-
namic and heterogeneous attributes of the IoT create a large attack surface. Once com-
promised, these “things” serve as vectors for attack. The most notable example has been
the Mirai botnet attack on Dyn in 2016. Approximately 100,000 bots—largely belong-
ing to the (IoT)—attacked the domain name server (DNS) for Twitter, Reddit, Github,
and the New York Times [15]. A massive ﬂow of trafﬁc overwhelmed the bandwidth of
the DNS.

1.2 Denial of Cyber-Physical Service Attacks

Since IoT devices are part of CPS, they also require physical “bandwidth.” As an ex-
ample, consider the navigation app Waze [1]. Waze uses real-time trafﬁc information to
ﬁnd optimal navigation routes. Due to its large number of users, the app also inﬂuences
trafﬁc. If too many users are directed to one road, they can consume the physical band-
width of that road and cause unexpected congestion. An attacker with insider access to
Waze could use this mechanism to manipulate transportation networks.

Another example can be found in healthcare. Smart lighting systems (which deploy,
e.g., time-of-ﬂight sensors) detect falls of room occupants [22]. These systems alert
emergency responders about a medical situation in an assisted living center or the home
of someone who is aging. But an attacker could potentially trigger many of these alerts
at the same time, depleting the response bandwidth of emergency personnel.

Such a threat could be called a denial of cyber-physical service attack. To distin-
guish it from a cyber-layer DDoS, we also use the acronym PDoS (Physical Denial of
Service). Figure 1 gives a conceptual diagram of a PDoS attack. In the rest of the pa-
per, we will consider one speciﬁc instance of a PDoS attack, although our analysis is
not limited to this example. We consider the infection and manipulation of a popula-
tion of IoT-based heating, ventilation, and air conditioning (HVAC) controllers in order

1234to cause a sudden load shock to the power grid. Attackers either disable demand re-
sponse switches used for reducing peak load [6], or they unexpectedly activate inactive
loads. This imposes risks ranging from frequency droop to load shedding and cascading
failures.

Fig. 2. PDoS defense can be designed at multiple layers. Malware detection and active defense
can combat initial infection, secure IoT design and strategic trust can reduce the spread of the
malware, and CPS can be resilient and physically-aware. We focus on detection and active de-
fense.

1.3 Modeling the PDoS Recruitment Stage

Defenses against PDoS can be designed at multiple layers (Fig. 2). The scope of this pa-
per is limited to defense at the stage of botnet recruitment, in which the attacker scans a
wide range of IP addresses, searching for devices with weak security settings. Mirai, for
example, does this by attempting logins with a dictionary of factory-default usernames
and passwords (e.g. root/admin, admin/admin, root/123456) [12]. Devices in
our mechanism identify these suspicious login attempts and use active defense to learn
about the attacker or report his activity.

In order to quantify the risk of malware infection, we combine two game-theoretic
models known as signaling games [14,7] and Poisson games [18]. Signaling games
model interactions between two parties, one of which possesses information unknown
to the other party. While signaling games consider only two players, we extend this
model by allowing the number of target IoT devices to be a random variable (r.v.) that
follows a Poisson distribution. This captures the fact that the malware scans a large
number of targets. Moreover, we allow the targets to have heterogeneous abilities to
detect malicious login attempts.

1.4 Contributions and Related Work

We make the following principle contributions:

1. We describe an IoT attack called a denial of cyber-physical service (PDoS).

Malware DetectionActive DefenseResilient GridSecureIoT DesignStrategic Trust ManagementPhysically-Aware Security2. We develop a general model called Poisson signaling games (PSG) which quantiﬁes

one-to-many signaling interactions.

3. We ﬁnd the pure strategy equilibria of a version of the PSG model for PDoS.
4. We analyze legal and economic mechanisms to deter botnet recruitment, and ﬁnd
that 1) defenders can bound botnet activity, and 2) legislating a minimum level of
security has only a limited effect, while incentivizing active defense, in principle,
can decrease botnet activity arbitrarily.

Signaling games are often used to model deception and trust in cybersecurity [16,19,21].
Poisson games have also been used to model malware epidemics in large populations
[11]. Wu et al. use game theory to design defense mechanisms against DDoS attacks
[24]. But the defense mechanisms mitigate the actual the ﬂood of trafﬁc against a target
system, while we focus on botnet recruitment. Bensoussan et al. use a susceptible-
infected-susceptible (SIS) model to study the growth of a botnet [5]. But IoT devices
in our model maintain beliefs about the reliability of incoming messages. In this way,
our paper considers the need to trust legitimate messages. Finally, load altering attacks
[17,4] to the power grid are an example of PDoS attacks. But PDoS attacks can also
deal with other resources.

In Section 2, we review signaling games and Poisson games. In Section 3, we com-
bine them to create Poisson signaling games (PSG). In Section 4, we apply PSG to
quantify the population risk due to PDoS attacks. Section 5 obtains the perfect Bayesian
Nash equilibria of the model. Some of these equilibria are harmful for power companies
and IoT users. Therefore, we design proactive mechanisms to improve the equilibria in
Section 6. We underline the key contributions in Section 7.

2 Signaling Games and Poisson Games

This section reviews two game-theoretic models: signaling games and Poisson games.
In Section 3, we combine them to create PSG. PSG can be used to model many one-to-
many signaling interactions in addition to PDoS.

2.1 Signaling Games with Evidence

Signaling games are a class of dynamic, two-player, information-asymmetric games
between a sender S and a receiver R (c.f. [14,7]). Signaling games with evidence extend
the typical deﬁnition by giving receivers some exogenous ability to detect deception1
[20]. They are characterized by the tuple

ΦSG = (cid:0)X, M, E, A, qS, δ, uS, uR(cid:1) .

First, S posses some private information unknown to R. This private information is
called a type. The type could represent, e.g., a preference, a technological capability, or a

1 This is based on the idea that deceptive senders have a harder time communicating some
messages than truthful senders. In interpersonal deception, for instance, lying requires high
cognitive load, which may manifest itself in external gestures [23].

malicious intent. Let the ﬁnite set X denote the set of possible types, and let x ∈ X denote
one particular type. Each type occurs with a probability qS(x), where qS : X → [0, 1]
such that (s.t.) ∑
x∈X

qS (x) = 1 and ∀x ∈ X, qS(x) ≥ 0.

Based on his private information, S communicates a message to the receiver. The
message could be, e.g., a pull request, the presentation of a certiﬁcate, or the execution
of an action which partly reveals the type. Let the ﬁnite set M denote the set of possible
messages, and let m ∈ M denote one particular type. In general, S can use a strategy in
which he chooses various m with different probabilities. We will introduce notation for
these mixed strategies later.

In typical signaling games (e.g. Lewis signaling games [14,7] and signaling games
discussed by Crawford and Sobel [7]), R only knows about x through m. But this sug-
gests that deception is undetectable. Instead, signaling games with evidence include a
detector2 which emits evidence e ∈ E about the sender’s type [20]. Let δ : E → [0, 1] s.t.
δ(e | x, m) = 1 and δ(e | x, m) ≥ 0. Then δ(e | x, m)
for all x ∈ X and m ∈ M, we have ∑
e∈E

gives the probability with which the detector emits evidence e given type x and message
m. This probability is ﬁxed, not a decision variable. Finally A be a ﬁnite set of actions.
Based on m and e, R chooses some a ∈ A. For instance, R may choose to accept or reject
a request represented by the message. These can also be chosen using a mixed-strategy.
In general, x, m, and a can impact the utility of S and R. Therefore, let uS : M × A →
R|X| be a vector-valued function such that uS (m, a) = (cid:2)uS
x∈X . This is a column
vector with entries uS
x(m, a). These entries give the utility that S of each receiver of type
x ∈ X obtains for sending a message m when the receiver plays action a. Next, deﬁne
the utility function for R by uR : X × M × A → R, such that uR(x, m, a) gives the utility
that R receives when a sender of type x transmits message m and R plays action a.

x (m, a)(cid:3)

2.2 Poisson Games

Poisson games were introduced by Roger Myerson in 1998 [18]. This class of games
models interactions between an unknown number of players, each of which belongs to
one type in a ﬁnite set of types. Modeling the population uncertainty using a Poisson r.v.
is convenient because merging or splitting Poisson r.v. results in r.v. which also follow
Poisson distributions.

Section 3 will combine signaling games with Poisson games by considering a sender
which issues a command to a pool of an unknown number of receivers, which all re-
spond at once. Therefore, let us call the players of the Poisson game “receivers,” al-
though this is not the nomenclature used in the original game. Poisson games are char-
acterized by the tuple

ΦPG = (cid:0)λ,Y, qR, A, ˜uR(cid:1) .

First, the population parameter λ > 0 gives the mean and variance of the Poisson
distribution. For example, λ may represent the expected number of mobile phone users

2 This could literally be a hardware or software detector, such as email ﬁlters which attempt
to tag phishing emails. But it could also be an an abstract notion meant to signify the innate
ability of a person to recognize deception.

within range of a base station. Let the ﬁnite set Y denote the possible types of each re-
ceiver, and let y ∈ Y denote one of these types. Each receiver has type y with probability
qR(y), where ∑
y∈Y

qR(y) = 1 and ∀y ∈ Y, qR(y) > 0.

Because of the decomposition property of the Poisson r.v., the number of receivers
of each type y ∈ Y also follows a Poisson distribution. Based on her type, each receiver
chooses an action a in the ﬁnite set A. We have deliberately used the same notation
as the action for the signaling game, because these two actions will coincide in the
combined model.

Utility functions in Poisson games are deﬁned as follows. For a ∈ A, let ca ∈ Z+ (the
set of non-negative integers) denote the count of receivers which play action a. Then let
c be a column vector which contains entries ca for each a ∈ A. Then c falls within the
set Z(A), the set of all possible integer counts of the number of players which take each
action.

y∈Y

y (a, c)(cid:3)

. The entries ˜uR

Poisson games assume that all receivers of the same type receive the same utility.
Therefore, let ˜uR : A × Z(C) → R|Y | be a vector-valued function such that ˜uR (a, c) =
(cid:2) ˜uR
y (a, c) give the utility that receivers of each type y ∈ Y ob-
tain for playing an action a while the vector of the total count of receivers that play each
action is given by c. Note that this is different from the utility function of receivers in
the signaling game. Given the strategies of the receivers, c is also distributed according
to a Poisson r.v.

3 Poisson Signaling Games

Figure 3 depicts Poisson signaling games (PSG). PSG are characterized by combining
ΦSG and ΦPG to obtain the tuple

SG = (cid:0)X,Y, M, E, A, λ, q, δ,U S,U R(cid:1) .
ΦPG

3.1 Types, Actions, and Evidence, and Utility

As with signaling games and Poisson games, X denotes the set of types of S, and Y
denotes the set of types of R. M, E, and A denote the set of messages, evidence, and
actions, respectively. The Poisson parameter is λ.

The remaining elements of ΦPG

SG are slightly modiﬁed from the signaling game or
Poisson game. First, q : X × Y → [0, 1]2 is a vector-valued function such that q (x, y)
gives the probabilities qS(x), x ∈ X, and qR(y), y ∈ Y, of each type of sender and receiver,
respectively.

As in the signaling game, δ characterizes the quality of the deception detector. But
receivers differ in their ability to detect deception. Various email clients, for example,
may have different abilities to identify phishing attempts. Therefore, in PSG, we de-
ﬁne the mapping by δ : E → [0, 1]|Y |, s.t. the vector δ (e | x, m) = [δy (e | x, m)]y∈Y gives
the probabilities δy(e | x, m) with which each receiver type y observes evidence e given

sender type x and message m. This allows each receiver type to observe evidence with
different likelihoods3.

Fig. 3. PSG model the third stage of a PDoS attack. A sender of type x chooses an action m which
is observed by an unknown number of receivers. The receivers have multiple types y ∈ Y. Each
type may observe different evidence e ∈ E. Based on m and e, each type of receiver chooses an
action a.

The utility functions U S and U R are also adjusted for PSG. Let U S : M × Z(A) →
R|X| be a vector-valued function s.t. the vector U S (m, c) = (cid:2)U S
x∈X gives the
utility of senders of each type x for sending message m if the count of receivers which
choose each action is given by c. Similarly, let U R : X × M × A × Z(A) → R|Y | be a
vector-valued function s.t. U R (x, m, a, c) = (cid:2)U R
y∈Y gives the utility of re-
ceivers of each type y ∈ Y. As earlier, x is the type of the sender, and m is the message.
But note that a denotes the action of this particular receiver, while c denotes the count
of overall receivers which choose each action.

y (x, m, a, c)(cid:3)

x (m, c)(cid:3)

3.2 Mixed-Strategies and Expected Utility

Next, we deﬁne the nomenclature for mixed-strategies and expected utility functions.
For senders of each type x ∈ X, let σS
x(m)
gives the probability with which he plays each message m ∈ M. For each x ∈ X, let ΣS
x
denote the set of possible σS

x : M → [0, 1] be a mixed strategy such that σS

x. We have

(cid:40)

ΣR
x =

¯σ | ∑
m∈M

¯σ (m) = 1 and ∀m ∈ M, ¯σ (m) ≥ 0

.

(cid:41)

3 In fact, although all receivers with the same type y have the same likelihood δy(e | x, m) of
observing evidence e given sender type x and message m, our formulation allows the receivers
to observe different actual realizations e of the evidence.

Action 𝑎∈𝐴𝑆Action: 𝑚∈𝑀Action 𝑎∈𝐴Action 𝑎∈𝐴Action 𝑎∈𝐴Type 𝑦∈𝑌Type 𝑥∈𝑋𝑆Type 𝑦∈𝑌Type 𝑦∈𝑌Type 𝑦∈𝑌Evidence 𝑒∈𝐸𝑒∈𝐸𝑒∈𝐸𝑒∈𝐸𝑅𝑅𝑅𝑅For receivers of each type y ∈ Y, let σR

y : A → [0, 1] denote a mixed strategy such that
y (a | m, e) gives the probability with which she plays action a after observing message

σR
m and action e. For each y ∈ Y, the function σR

y belongs to the set

(cid:40)

ΣR
y =

¯σ | ∑
a∈A

¯σ (a) = 1 and ∀a ∈ A, ¯σ (a) ≥ 0

.

(cid:41)

In order to choose her actions, R forms a belief about the sender type x. Let µR
y (x | m, e)
denote the likelihood with which each R of type y who observes message m and ev-
idence e believes that S has type x. In equilibrium, we will require this belief to be
consistent with the strategy of S.

Now we deﬁne the expected utilities that S and each R receive for playing mixed
x × ΣR → R.
strategies. Denote the expected utility of a sender of type x ∈ X by ¯U S
Notice that all receiver strategies must be taken into account. This expected utility is
given by

x : ΣS

x (σS
¯U S

x, σR) = ∑

∑
c∈Z(A)

m∈M

x (m) P (cid:8)c | σR, x, m(cid:9)U S
σS

x (m, c).

Here, P{c | σR, x, m} is the probability with which the vector c gives the count of re-
ceivers that play each action. Myerson shows that, due to the aggregation and decompo-
sition properties of the Poisson r.v., the entries of c are also Poisson r.v. [18]. Therefore,
P{c | σR, x, m} is given by

P (cid:8)c | σR, x, m(cid:9) = ∏

a∈A

eλa λca
a
ca!

, λa = λ∑
y∈Y

∑
e∈E

qR (y) δy (e | x, m) σR

y (a | m, e) .

(1)

Next, denote the expected utility of each receiver of type y ∈ Y by ¯U R

y ×ΣR → R.
y ) gives the expected utility when this particular receiver plays
y and the population of all types of receivers plays the mixed-

y : ΣR

Here, ¯U R
y (θ, σR | m, e, µR
mixed strategy θ ∈ ΣR
strategy vector σR. The expected utility is given by

y (θ, σR | m, e, µR
¯U R

y ) = ∑
x∈X

∑
a∈A

∑
c∈Z(A)
y (x | m, e) θ (a | m, e) P (cid:8)c | σR, x, m(cid:9)U R
µR

y (x, m, a, c),

(2)

where again P{c | σR, x, m} is given by Eq. (1).

3.3 Perfect Bayesian Nash Equilibrium

First, since PSG are dynamic, we use an equilibrium concept which involves perfec-
tion. Strategies at each information set of the game must be optimal for the remaining
subgame [8]. Second, since PSG involve incomplete information, we use a Bayesian
concept. Third, since each receiver chooses her action without knowing the actions of
the other receivers, the Poisson stage of the game involves a ﬁxed point. All receivers
choose strategies which best respond to the optimal strategies of the other receivers.

Perfect Bayesian Nash equilibrium (PBNE) is the appropriate concept for games with
these criteria [8].

Consider the two chronological stages of PSG. The second stage takes place among
the receivers. This stage is played with a given m, e, and µR determined by the sender
(and detector) in the ﬁrst stage of the game. When m, e, and µR are ﬁxed, the interaction
y : ΣR → P (ΣR
between all receivers becomes a standard Poisson game. Deﬁne BRR
y )
(where P (S) denotes the power set of S) such that the best response of a receiver of
type y to a strategy proﬁle σR of the other receivers is given by the strategy or set of
strategies

BRR
y

(cid:0)σR | m, e, µR
y

(cid:1) (cid:44) arg max

¯U R
y

(cid:0)θ, σR | m, e, µR
y

(cid:1) .

(3)

θ∈ΣR
y

The ﬁrst stage takes place between the sender and the set of receivers. If we ﬁx the
set of receiver strategies σR, then the problem of a sender of type x ∈ X is to choose σS
x
to maximize his expected utility given σR. The last criteria is that the receiver beliefs
µR must be consistent with the sender strategies according to Bayes’ Law. Deﬁnition 1
applies PBNE to PSG.

Deﬁnition 1. (PBNE) Strategy and belief proﬁle (σS∗, σR∗, µR) is a PBNE of a PSG if
all of the following hold [8]:

∀x ∈ X, σS∗

x ∈ arg max
x ∈ΣS
σS
x

x (σS
¯U S

x, σR∗),

∀y ∈ Y, ∀m ∈ M, ∀e ∈ E, σR∗

y ∈ BRR
y

(cid:0)σR∗ | m, e, µR
y

(cid:1) ,

∀y ∈ Y, ∀m ∈ M, ∀e ∈ E, µR

y (d | m, e) ∈

δy (e | d, m) σS
∑
˜x∈X

δy (e | ˜x, m) σS

d (m) qS (d)
˜x (m) qS ( ˜x)

(4)

(5)

(6)

,

if ∑
˜x∈X
have µR

δy (e | ˜x, m) σS
y (l | m, e) = 1 − µR

˜x (m) qS ( ˜x) > 0, and µR
y (d | m, e) .

y (d | m, e) ∈ [0, 1] , otherwise. We also always

Equation (4) requires the sender to choose an optimal strategy given the strategies
of the receivers. Based on the message and evidence that each receiver observes, Eq. (5)
requires each receiver to respond optimally to the proﬁle of the strategies of the other
receivers. Equation (6) uses Bayes’ law (when possible) to obtain the posterior beliefs
µR using the prior probabilities qS, the sender strategies σS, and the characteristics δy,
y ∈ Y of the detectors [20].

4 Application of PSG to PDoS

Section 3 deﬁned PSG in general, without specifying the members of the type, message,
evidence, or action sets. In this section, we apply PSG to the recruitment stage of PDoS
attacks. Table 1 summarizes the nomenclature.

S refers to the agent which attempts a login attempt, while R refers to the device.
Let the set of sender types be given by X = {l, d}, where l represents a legitimate
login attempt, while d represents a malicious attempt. Malicious S attempt to login to

many devices through a wide IP scan. This number is drawn from a Poisson r.v. with
parameter λ. Legitimate S only attempt to login to one device at a time. Let the receiver
types be Y = {k, o, v}. Type k represents weak receivers which have no ability to detect
deception and do not use active defense. Type o represents strong receivers which can
detect deception, but do not use active defense. Finally, type v represents active receivers
which can both detect deception and use active defense.

Table 1. Application of PSG to PDoS Recruitment

Set
Type x ∈ X of S
Type y ∈ Y of R
Message m ∈ M of S
Evidence e ∈ E
Action a ∈ A of R

Elements
l : legitimate, d : malicious
k : no detection; o : detection; v : detection & active defense
m = {m1, m2, . . .}, a set of |m| password strings
b : suspicious, n : not suspicious
t : trust, g : lockout, f : active defense

4.1 Messages, Evidence Thresholds, and Actions

Messages consist of sets of consecutive unsuccessful login attempts. They are denoted
by m = {m1, m2, . . .}, where each m1, m2, . . . is a string entered as an attempted pass-
word4. For instance, botnets similar to Mirai choose a list of default passwords such as
[12]

m = {admin, 888888, 123456, default, support} .

Of course, devices can lockout after a certain number of unsuccessful login attempts.
Microsoft Server 2012 recommends choosing a threshold at 5 to 9 [3]. Denote the lower
end of this range by τL = 5. Let us allow all attempts with |m| < τL. In other words, if
a user successfully logs in before τL, then the PSG does not take place. (See Fig. 5.)

The PSG takes place for |m| ≥ τL. Let τH = 9 denote the upper end of the Microsoft
range. After τL, S may persist with up to τH login attempts, or he may not persist. Let p
denote persist, and w denote not persist. Our goal is to force malicious S to play w with
high probability.

For R of types o and v, if S persists and does not successfully log in with |m| ≤
τH login attempts, then e = b. This signiﬁes a suspicious login attempt. If S persists
and does successfully login with |m| ≤ τH attempts, then e = n, i.e., the attempt is not
suspicious5.

If a user persists, then the device R must choose an action a. Let a = t denote
trusting the user, i.e., allowing login attempts to continue. Let a = g denote locking the

4 A second string can also be considered for the username.
5 For strong and active receivers, δy (b | d, p) > δy (b | l, p) , y ∈ {o, v}. That is, these receivers
are more likely to observe suspicious evidence if they are interacting with a malicious sender
than if they are interacting with a legitimate sender. Mathematically, δk(b | d, p) = δk(b | l, p)
signiﬁes that type k receivers do not implement a detector.

device to future login attempts. Finally, let a = f denote using an active defense such as
reporting the suspicious login attempt to an Internet service provider (ISP), recording
the attempts in order to gather information about the possible attacker, or attempting to
block the offending IP address.

4.2 Characteristics of PDoS Utility Functions

The nature of PDoS attacks implies several features of the utility functions U S and U R.
These are listed in Table 2. Characteristic 1 (C1) states that if S does not persist, both
players receive zero utility. C2 says that R also receives zero utility if S persists and
R locks down future logins. Next, C3 states that receivers of all types receive positive
utility for trusting a benign login attempt, but negative utility for trusting a malicious
login attempt. We have assumed that only type v receivers use active defense; this is
captured by C4. Finally, C5 says that type v receivers obtain positive utility for using
active defense against a malicious login attempt, but negative utility for using active
defense against a legitimate login attempt. Clearly, C1-C5 are all natural characteristics
of PDoS recruitment.

Table 2. Characteristics of PDoS Utility Functions

#

C1

C2

C3

C4

C5

Notation
∀x ∈ X, y ∈ Y , a ∈ A, c ∈ Z (A) ,
U S
y (x, w, a, c) = 0.

x (w, c) = U R
∀x ∈ X, y ∈ Y, c ∈ Z (A) ,
U R
y (x, p, g, c) = 0.
∀y ∈ Y, c ∈ Z (A) ,

U R

y (d, p,t, c) < 0 < U R

y (l, p,t, c).

∀x ∈ X, c ∈ Z (A) ,

k (x, p, f , c) = U R
U R

o (x, p, f , c) = −∞.

∀c ∈ Z (A) ,
v (l, p, f , c) < 0 < U R

U R

v (d, p, f , c).

4.3 Modeling the Physical Impact of PDoS Attacks

The quantities ct , cg, and c f denote, respectively, the number of devices that trust, lock
down, and use active defense. Deﬁne the function Z : Z(A) → R such that Z(c) de-
notes the load shock that malicious S cause based on the count c. Z(c) is clearly non-
decreasing in ct , because each device that trusts the malicious sender becomes infected
and can impose some load shock to the power grid.

The red (solid) curve in Fig. 4 conceptually represents the mapping from load shock
size to damage caused to the power grid based on the mechanisms available for regu-
lation. Small disturbances are regulated using automatic frequency control. Larger dis-
turbances can signiﬁcantly decrease frequency and should be mitigated. Grid operators
have recently offered customers load control switches, which automatically deactivate

Fig. 4. Conceptual relationship between load shock size and damage to the power grid. Small
shocks are mitigated through automatic frequency control or demand-side control of ﬂexible
loads. Large shocks can force load shedding or blackouts.

appliances in response to a threshold frequency decrease [10]. But the size of this vol-
untary demand-side control is limited. Eventually, operators impose involuntary load
shedding (i.e., rolling blackouts). This causes higher inconvenience. In the worst case,
transient instability leads to cascading failures and blackout [9].

The yellow and orange dashed curves in Fig. 4 provide two approximations to Z(c).
The yellow curve, ˜Zlin(c), is linear in ct . We have ˜Zlin(c) = ωt
d is a positive
real number. The orange curve, ˜Zstep(c), varies according to a step function, i.e., Z(c) =
Ωt
d is a positive real number and 1{•} is the indicator function. In this
paper, we derive solutions for the linear approximation. Under this approximation, the
utility of malicious S is given by

d1{ct >τt }, where Ωt

dct , where ωt

d (m, c) = ˜Zlin(c) + ωg
U S

dcg + ω f

d c f = ωt

dct + ωg

dcg + ω f

d c f .

where ωg
down or uses active defense, respectively.

d < 0 and ω f

d < 0 represent the utility to malicious S for each device that locks

Using ˜Zlin(c), the decomposition property of the Poisson r.v. simpliﬁes ¯U S

x, σR).
We show in Appendix A that the sender’s expected utility depends on the expected
values of each of the Poisson r.v. that represent the number of receivers who choose
each action ca, a ∈ A. The result is that

x (σS

x (σS
¯U S

x, σR) = λσS

x (p) ∑
y∈Y

∑
e∈E

∑
a∈A

qR (y) δy (e | x, p) σR

y (a | p, e) ωa
x.

(7)

Next, assume that the utility of each receiver does not depend directly on the actions
of the other receivers. (In fact, the receivers are still endogenously coupled through
the action of S.) Abusing notation slightly, we drop c (the count of receiver actions)
in U R
y ).
Equation (2) is now

y (x, m, a, c) and σR (the strategies of the other receivers) in ¯U R

y (θ, σR | m, e, µR

¯U R
y

(cid:0)θ | m, e, µR
y

(cid:1) = ∑

x∈X

∑
a∈{t, f }

y (x | m, e) θ (a | m, e)UR
µR

y (x, m, a) .

(0,0)Power Impact 𝑍(𝑐)Load Shock Size ∝𝑐𝑡Automatic Frequency ControlDemand-side control of flexible loadsAutomatic load sheddingDemand-side control of less flexible loadsInstability and blackout5 Equilibrium Analysis

In this section, we obtain the equilibrium results by parameter region. In order to sim-
plify analysis, without loss of generality, let the utility functions be the same for all
receiver types (except when a = f ), i.e., ∀x ∈ X, U R
v (x, p,t).
Also without loss of generality, let the quality of the detectors for types y ∈ {o, v} be
the same: ∀e ∈ E, x ∈ X, δo(e | x, p) = δv(e | x, p).

k (x, p,t) = U R

o (x, p,t) = U R

Fig. 5. Model of a PSG under Lemma 1. Only one of many R is depicted. After the types x and
y, of S and R, respectively, are drawn, S chooses whether to persist beyond τL attempts. Then R
chooses to trust, lockout, or use active defense against S based on whether S is successful. Lemma
1 determines all equilibrium strategies except σS∗
v (• | p, b) , marked by
the blue and red items.

o (• | p, b) , and σR∗

d (•) , σR∗

5.1 PSG Parameter Regime

We now obtain equilibria for a natural regime of the PSG parameters. First, assume that
legitimate senders always persist: σS
l (p) = 1. This is natural for our application, because
IoT HVAC users will always attempt to login. Second, assume that R of all types trust
login attempts which appear to be legitimate (i.e., give evidence e = n). This is satisﬁed
for

qS (d) <

U R
k (l, p,t)
k (l, p,t) −U R
U R
Third, we consider the likely behavior of R of type o when a login attempt is suspi-
cious. Assume that she will lock down rather than trust the login. This occurs under the
parameter regime

k (d, p,t)

(8)

.

qS (d) >

˜U R
o (l, p,t)
o (l, p,t) − ˜U R

˜U R

o (d, p,t)

,

(9)

Legitimate𝑥=𝑙Malicious𝑥=𝑑𝑚<𝜏𝐿𝑚<𝜏𝐿𝑚≥𝜏𝐻→𝑚=𝑝𝑚≥𝜏𝐿NOGAMENOGAME𝑚≥𝜏𝐿𝑚≥𝜏𝐻→𝑚=𝑝𝑚<𝜏𝐻→𝑚=𝑤Success→𝑒=𝑛Failure→𝑒=𝑏Success→𝑒=𝑛Failure→𝑒=𝑏𝑎=𝑡𝑎=𝑡𝑎=𝑡𝑎=𝑔𝑎=𝑓𝑎=𝑡𝑎=𝑔𝑎=𝑓ZEROUTILITYSUCCESSFUL LOGINSUCCESSFUL LOGINLOCKOUT USERACTIVE DEFENSEAGAINSTUSERINFECTIONINFECTIONLOCKOUTATTACKERACTIVE DEFENSEAGAINST ATTACKERusing the shorthand notation

o (l, p,t) = U R
˜U R

o (l, p,t) δ0 (b | l, p) ,

o (d, p,t) = U R
˜U R

o (d, p,t) δ0 (b | d, p) .

The fourth assumption addresses the action of R of type v when a login attempt is
suspicious. The optimal action depends on her belief µR
o (d | p, b) that S is malicious.
The belief, in turn, depends on the mixed-strategy probability with which malicious S
persist. We assume that there is some σS
d(p) for which R should lock down (a = g). This
is satisﬁed if there exists a real number φ ∈ [0, 1] such that, given6 σS

d(p) = φ,

v (t | p, b, µR
¯U R

v ) > 0,

v ( f | p, b, µR
¯U R

v ) > 0.

(10)

This simpliﬁes analysis, but can be removed if necessary.

Lemma 1 summarizes the equilibrium results under these assumptions. Legitimate
S persist, and R of type o lock down under suspicious login attempts. All receiver types
trust login attempts which appear legitimate. R of type k, since she cannot differenti-
ate between login attempts, trusts all of them. The proof follows from the optimality
conditions in Eq. (4-6) and the assumptions in Eq. (8-10).

Lemma 1. (Constant PBNE Strategies) If σS
following equilibrium strategies are implied:

d(p) = 1 and Eq. (8-10) hold, then the

σS∗
l (p) = 1, σR∗

o (g | p, b) = 1, σR∗

k (t | p, b) = 1,

o (t | p, n) = σR∗
σR∗

v (t | p, n) = σR∗
Figure 5 depicts the results of Lemma 1. The remaining equilibrium strategies to be
obtained are denoted by the red items for S and the blue items for R. These strategies
are σR∗
d (p) depends on whether R of
type o and type v will lock down and/or use active defense to oppose suspicious login
attempts.

d (p). Intuitively, σS∗

v (• | p, b), and σS∗

o (• | p, b), σR∗

k (t | p, n) = 1.

5.2 Equilibrium Strategies

The remaining equilibrium strategies fall into four parameter regions. In order to delin-
eate these regions, we deﬁne two quantities.

Let T DR
type v if σS
some probability. Equation (3) can be used to show that

v (U R
d(p) = 1. If qS(d) > T DR

v , δv) denote a threshold which determines the optimal action of R of
v , δv), then the receiver uses active defense with

v (U R

T DR
v

(cid:0)U R

v , δv

(cid:1) =

˜U R
v (l, p, f )
v (l, p, f ) − ˜U R

˜U R

v (d, p, f )

,

where we have used the shorthand notation:

v (l, p, f ) := U R
˜U R

v (l, p, f ) δv (b | l, p) ,

v (d, p, f ) := U R
˜U R

v (d, p, f ) δv (b | d, p) .

6 We abuse notation slightly to write ¯U R

v (a | m, e, µR

y ) for the expected utility that R of type v

obtains by playing action a.

Next, let BPS
d
m = p, i.e., for persisting. We have

(cid:0)ωd, qR, δ(cid:1) denote the beneﬁt which S of type d receives for choosing

BPS
d

(cid:0)ωd, qR, δ(cid:1) := ∑

y∈Y

∑
e∈E

qR (y) δy (e | d, p) σR

y (a | p, e) ωa
d.

∑
a∈A

If this beneﬁt is negative, then S will not persist. Let BPS
d
the beneﬁt of persisting when receivers use the pure strategies:

(cid:0)ωd, qR, δ | ak, ao, av

(cid:1) denote

k (ak | p, b) = σR
σR

o (ao | p, b) = σR

v (av | p, b) = 1.

We now have Theorem 1, which predicts the risk of malware infection in the remaining
parameter regions. The proof is in Appendix B.

Theorem 1. (PBNE within Regions) If σS
v (• | p, b), and σS∗
σR∗

d (p) vary within the four regions listed in Table 3.

d(p) = 1 and Eq. (8-10) hold, then σR∗

o (• | p, b),

In the status quo equilibrium, strong and active receivers lock down under suspi-
cious login attempts. But this is not enough to deter malicious senders from persisting.
We call this the status quo because it represents current scenarios in which botnets in-
fect vulnerable devices but incur little damage from being locked out of secure devices.
This is a poor equilibrium, because σS∗
d (p) = 1.

In the active deterrence equilibrium, lockouts are not sufﬁcient to deter malicious
v , R of type v use active defense. This is
d (p) < 1. In this equilibrium, R of type o always locks

S from fully persisting. But since qS(d) > T DR
enough to deter malicious S : σS∗
down: σR∗

o (g | p, b) = 1. R of type v uses active defense with probability

σR∗
v ( f | p, b) =

dqR (k) + ωg
ωt
(cid:17)
(cid:16)
d − ω f
ωg

d

(cid:0)qR (o) + qR (v)(cid:1)
d
qR (v) δv (v | d, p)

,

(11)

and otherwise locks down: σR∗
reduced probability

v (g | p, b) = 1 − σR∗

v ( f | p, b) . Deceptive S persist with

σS∗
d (p) =

(cid:18)

1
qS (d)

˜U R
v (l, p, f )
v (l, p, f ) − ˜U R

˜U R

v (d, p, f )

(cid:19)

.

(12)

In the resistant attacker equilibrium, qS(d) > T DR

v . Therefore, R of type v use active
defense. But BPS
d (• |t, g, f ) > 0, which means that the active defense is not enough to
deter malicious senders. This is a “hopeless” situation for defenders, since all available
means are not able to deter malicious senders. We still have σS∗

d (p) = 1.

In the vulnerable attacker equilibrium, there is no active defense. But R of type
o and type v lock down under suspicious login attempts, and this is enough to deter
malicious S, because BPS
d (• |t, g, g) < 0. R of types o and v lock down with probability

o (g | p, b) = σR∗
σR∗

v (g | p, b) =

ωt
d

(qR (0) + qR (v)) δo (b | d, p) (cid:0)ωt

d − ωg

d

(cid:1) ,

(13)

and trust with probability σR∗
persist with reduced probability

o (t | p, b) = σR∗

v (t | p, b) = 1 − σR∗

o (g | p, b) . Deceptive S

σS∗
d (p) =

(cid:18)

1
qS (d)

˜U R
o (l, p,t)
o (l, p,t) − ˜U R

˜U R

o (d, p,t)

(cid:19)

.

(14)

The status quo and resistant attacker equilibria are poor results because infection
of devices is not deterred at all. The focus of Section 6 will be to shift the PBNE to the
other equilibrium regions, in which infection of devices is deterred to some degree.

Table 3. Equilibrium Regions of the PSG for PDoS

qS(d) < T DR

v (•) qS(d) > T DR

v (•)

0 < σR∗
0 < σR∗

Vulnerable Attacker
σS∗(p) < 1
o (t | p, b), σR∗
v (t | p, b), σR∗

o (g | p, b) < 1
v (g | p, b) < 1
Active Deterrence
σS∗(p) < 1
σR∗
o (g | p, b) = 1
Status Quo
0 < σR∗
σS∗(p) = 1
v (g | p, b),
o (g | p, b) = 1 σR∗
σR∗
v ( f | p, b) < 1
σR∗
v (g | p, b) = 1 Resistant Attacker

σS∗(p) = 1
σR∗
o (g | p, b) = 1
σR∗
v ( f | p, b) = 1

BPS

d (• |t, g, g) < 0,

BPS

d (• |t, g, f ) < 0

BPS

d (• |t, g, g) > 0,

BPS

d (• |t, g, f ) < 0

BPS

d (• |t, g, g) > 0,

BPS

d (• |t, g, f ) > 0

6 Mechanism Design

The equilibrium results are delineated by the quantities qS, T DR
These quantities are functions of the parameters qS, qR, δo, δv, ωd, and U R
v . Mechanism
design manipulates these parameters in order to obtain a desired equilibrium. We dis-
cuss two possible mechanisms.

v , δv) and BPS

v (U R

d (ωd, qR, δ).

6.1 Legislating Basic Security

Malware which infects IoT devices is successful because many IoT devices are poorly
secured. Therefore, one mechanism design idea is to legally require better authentica-
tion methods, in order to decrease qR(k) and increase qR(o).

The left-hand sides of Figs. 6-8 depict the results. Figure 6(a) shows that decreas-
ing qR(k) and increasing qR(o) moves the game from the status quo equilibrium to the
vulnerable attacker equilibrium. But Fig. 7(a) shows that this only causes a ﬁxed de-
crease in σS∗
d (p), regardless of the amount of decrease in qR(k). The reason, as shown
in Fig. 8(a), is that as qR(o) increases, it is incentive-compatible for receivers to lock

Fig. 6. Equilibrium transitions for (a) legal and (b) active defense mechanisms. The equilibrium
numbers signify: 1-status quo, 2-resistant attacker, 3-vulnerable attacker, 4-active deterrence.

Fig. 7. Malware persistence rate for (a) legal and (b) active defense mechanisms.

Fig. 8. Probabilities of opposing malicious S. Plot (a): probability that R lock down with the legal
mechanism. Plot (b): probability that R use active defense.

0.80.850.90.95qR(o)11.522.53Equilibrium Theorem NumberEquilibrium Regiongd=-0.3gd=-0.5gd=-0.701020304050URv(d,p,f)1234Equilibrium Theorem NumberEquilibrium Regionfd=-12fd=-14fd=-160.80.850.90.95qR(o)0.80.850.90.9511.05S*d(p)Persistence Rate of Malwaregd=-0.3gd=-0.5gd=-0.701020304050URv(d,p,f)0.20.40.60.81S*d(p)Persistence Rate of Malwarefd=-12fd=-14fd=-160.80.850.90.95qR(o)0.880.90.920.940.960.981Ro(g|p,b) = Rv(g|p,b)Lock Out Probability of R of Types o and vgd=-0.3gd=-0.5gd=-0.701020304050URv(d,p,f)00.511.522.5Rv(f|p,b)10-3Active Def. Prob. of R of Type vfd=-12fd=-14fd=-16down with progressively lower probability σR∗
y (g | p, b), y ∈ {o, v}. Rather than forcing
malicious S to not persist, increasing qR(o) only decreases the incentive for receivers to
lock down under suspicious login attempts.

6.2

Incentivizing Active Defense

One reason for the proliferation of IoT malware is that most devices which are secure
(i.e., R of type y = o) do not take any actions against malicious login attempts except
to lock down (i.e., to play a = g). But there is almost no cost to malware scanners for
making a large number of login attempts under which devices simply lock down. There
is a lack of economic pressure which would force σS∗

d (p) < 1, unless qR(0) ≈ 1.

This is the motivation for using active defense such as reporting the activity to an
ISP or recording the attempts in order to gather information about the attacker. The
right hand sides of Figs. 6-8 show the effects of providing an incentive U R
v (d, p, f ) for
active defense. This incentive moves the game from the status quo equilibrium to either
the resistant attacker equilibrium or the vulnerable attacker equilibrium, depending on
whether BPS
d (• |t, g, f ) is positive (Fig. 6(b)). In the vulnerable attacker equilibrium,
the persistence rate of malicious S is decreased (Fig. 7(b)). Finally, Fig. 8(b) shows that
only a small amount of active defense σR∗
v ( f | p, b) is necessary, particularly for high
values of7 ω f
d .

7 Discussion of Results

The ﬁrst result is that the defender can bound the activity level of the botnet. Recall that
the vulnerable attacker and active deterrence equilibria force σS∗
d (p) < 1. That is, they
decrease the persistence rate of the malware scanner. But another interpretation is pos-
sible. In Eq. (14) and Eq. (12), the product σS∗
d (p) qS (d) is bounded. This product can
be understood as the total activity of botnet scanners: a combination of prior probability
of malicious senders and the effort that malicious senders exert8. Bensoussan et al. note
that the operators of the Conﬁker botnet of 2008-2009 were forced to limit its activity
[5,13]. High activity levels would have attracted too much attention. The authors of [5]
conﬁrm this result analytically, using a dynamic game based on an SIS infection model.
Interestingly, our result agrees with [5], but using a different framework.

Secondly, we compare the effects of legal and economic mechanisms to deter re-
cruitment for PDoS. Figures 6(a)-8(a) showed that σS∗
d (p) can only be reduced by a
ﬁxed factor by mandating security for more and more devices. In this example, we
found that strategic behavior worked against legal requirements. By comparison, Figs.
6(b)-8(b) showed that σS∗
d (p) can be driven arbitrarily low by providing an economic
incentive U R

v (d, p, f ) to use active defense.

v ( f | p, b) = 1 for ω f

7 In Fig. 8(b), σR∗
d = −12.
8 A natural interpretation in an evolutionary game framework would be that σS∗

d (p) = 1, and
qS(d) decreases when the total activity is bounded. In other words, malicious senders con-
tinue recruiting, but some malicious senders drop out since not all of them are supported in
equilibrium.

Future work can evaluate technical aspects of mechanism design such as improving
malware detection quality. This would involve a non-trivial trade-off between a high
true-positive rate and a low false-positive rate. Note that the model of Poisson signaling
games is not restricted PDoS attacks. PSG apply to any scenario in which one sender
communicates a possibly malicious or misleading message to an unknown number of
receivers. In the IoT, the model could capture the communication of a roadside location-
based service to a set of autonomous vehicles, or spooﬁng of a GPS signal used by
multiple ships with automatic navigation control, for example. Online, the model could
apply to deceptive opinion spam in product reviews. In interpersonal interactions, PSG
could apply to advertising or political messaging.

A Simpliﬁcation of Sender Expected Utility

Each each component of c is distributed according to a Poisson r.v. The components are
P{ca | σR, x, m}. Recall that S receives zero utility
independent, so P{c | σR, x, m} = ∏
a∈A
when he plays m = w. So we can choose m = p :

x (σS
¯U S

x, σR) = σS

x (p) ∑
c∈Z(A)

∏
a∈A

P (cid:8)ca | σR, x, p(cid:9) (cid:0)ωt

xct + ωg

xcg + ω f

x c f

(cid:1) .

Some of the probability terms can be summed over their support. We are left with

x (σS
¯U S

x, σR) = σS

x (p) ∑
a∈A

caP (cid:8)ca | σR, x, p(cid:9) .

ωa
x ∑
ca∈Z+

(15)

The last summation is the expected value of ca, which is λa. This yields Eq. (7).

B Proof of Theorem 1

o (g | p, b) and σR∗

The proofs for the status quo and resistant attacker equilibria are similar to the proof for
Lemma 1. The vulnerable attacker equilibrium is a partially-separating PBNE. Strate-
gies σR∗
v (g | p, b) which satisfy Eq. (13) make malicious senders exactly
indifferent between m = p and m = w. Thus, they can play the mixed-strategy in Eq.
(14), which makes strong and active receivers exactly indifferent between a = g and
a = t. The proof of the vulnerable attacker equilibrium follows a similar logic.

References

1. Free community-based mapping, trafﬁc and navigation app. Waze Mobile, [Online]. Avail-

able: https://www.waze.com/.

2. Visions and challenges for realising the internet of things. Technical report, CERP IoT

Cluster, European Commission, 2010.
2014.

threshold,

lockout

3. Account

Microsoft TechNet,

[Online]. Available:

https://technet.microsoft.com/en-us/library/hh994574(v=ws.11).aspx.

4. Sajjad Amini, Hamed Mohsenian-Rad, and Fabio Pasqualetti. Dynamic load altering attacks

in smart grid. In Innovative Smart Grid Technol. Conf., pages 1–5. IEEE, 2015.

5. Alain Bensoussan, Murat Kantarcioglu, and SingRu Celine Hoe. A game-theoretical ap-
In Decision and Game

proach for ﬁnding optimal strategies in a botnet defense model.
Theory for Security, pages 135–148. Springer, 2010.

6. Tyler Byers. Demand response and the iot: Using data to maximize customer bene-
ﬁt, 2017. Comverge Blog, [Online]. Available: http://www.comverge.com/blog/february-
2017/demand-response-and-iot-using-data-to-maximize-cus/.

7. Vincent P Crawford and Joel Sobel. Strategic information transmission. Econometrica: J

Econometric Soc., pages 1431–1451, 1982.

8. D. Fudenberg and J. Tirole. Game Theory, volume 393. 1991.
9. J Duncan Glover, Mulukutla S Sarma, and Thomas Overbye. Power System Analysis &

Design, SI Version. Cengage Learning, 2012.

10. D. J. Hammerstrom. Part II. Grid friendly appliance project. In GridWise Testbed Demon-

stration Projects. Paciﬁc Northwest National Laboratory, 2007.

11. Yezekael Hayel and Quanyan Zhu. Epidemic protection over heterogeneous networks using
evolutionary poisson games. IEEE Trans. Inform. Forensics and Security, 12(8):1786–1800,
2017.

12. Ben Herzberg, Dima Bekerman, and Igal Zeifman. Breaking down mirai: An IoT DDoS
Incapsula Blog, Bots and DDoS, Security, 2016.
[Online] Available:

botnet analysis.
https://www.incapsula.com/blog/malware-analysis-mirai-ddos-botnet.html.

13. Kelly Jackson Higgins. Conﬁcker botnet ’dead in the water,’ researcher says, 2010.
Dark Reading, [Online]. Available: http://www.darkreading.com/attacks-breaches/conﬁcker-
botnet-dead-in-the-water-researcher-says/d/d-id/1133327?

14. David Lewis. Convention: A philosophical study. John Wiley & Sons, 2008.
15. Robinson Meyer. How a bunch of hacked dvr machines took down twitter and reddit. The

Atlantic, October 2016.

16. Monireh Mohebbi Moghaddam and Quanyan Zhu. A game-theoretic analysis of deception
over social networks using fake avatars. In Decision and Game Theory for Security. Springer,
2016.

17. Amir-Hamed Mohsenian-Rad and Alberto Leon-Garcia. Distributed internet-based load al-
tering attacks against smart power grids. IEEE Trans. Smart Grid, 2(4):667–674, 2011.

18. Roger B Myerson. Population uncertainty and poisson games.

Int. J. Game Theory,

27(3):375–392, 1998.

19. Jeffrey Pawlick, Sadegh Farhang, and Quanyan Zhu. Flip the cloud: Cyber-physical signaling
In Decision and Game Theory for

games in the presence of advanced persistent threats.
Security, pages 289–308. Springer, 2015.

20. Jeffrey Pawlick and Quanyan Zhu. Deception by design: Evidence-based signaling games
for network defense. In Workshop on the Economics of Inform. Security and Privacy, Delft,
The Netherlands, 2015.

21. Jeffrey Pawlick and Quanyan Zhu. Strategic trust in cloud-enabled cyber-physical systems
with an application to glucose control. IEEE Trans Inform. Forensics and Security, To appear.
22. Richard J Radke, Tianna-Kaye Woodstock, MH Imam, Arthur C Sanderson, and Sandipan
Mishra. Advanced sensing and control in the smart conference room at the center for lighting
enabled systems and applications. In SID Symp. Digest of Tech. Papers, volume 47, pages
193–196. Wiley Online Library, 2016.

23. Aldert Vrij, Samantha A Mann, Ronald P Fisher, Sharon Leal, Rebecca Milne, and Ray
Bull. Increasing cognitive load to facilitate lie detection: The beneﬁt of recalling an event in
reverse order. Law and Human Behavior, 32(3):253–265, 2008.

24. Qishi Wu, Sajjan Shiva, Sankardas Roy, Charles Ellis, and Vivek Datla. On modeling and
simulation of game theory-based defense mechanisms against DoS and DDoS attacks. In
Proc. Spring Simulation Multiconf., page 159. Society for Comput. Simulation Intl., 2010.

