7
1
0
2

t
c
O
6
1

]

R
C
.
s
c
[

2
v
8
0
7
3
0
.
7
0
7
1
:
v
i
X
r
a

Proactive Defense Against Physical Denial of Service
Attacks using Poisson Signaling Games

Jeffrey Pawlick and Quanyan Zhu(cid:63)

New York University Tandon School of Engineering
Department of Electrical and Computer Engineering
6 MetroTech Center, Brooklyn, NY 11201. {jpawlick,quanyan.zhu}@nyu.edu

Abstract. While the Internet of things (IoT) promises to improve areas such as
energy efï¬ciency, health care, and transportation, it is highly vulnerable to cyber-
attacks. In particular, distributed denial-of-service (DDoS) attacks overload the
bandwidth of a server. But many IoT devices form part of cyber-physical systems
(CPS). Therefore, they can be used to launch â€œphysicalâ€ denial-of-service attacks
(PDoS) in which IoT devices overï¬‚ow the â€œphysical bandwidthâ€ of a CPS. In this
paper, we quantify the population-based risk to a group of IoT devices targeted
by malware for a PDoS attack. In order to model the recruitment of bots, we de-
velop a â€œPoisson signaling game,â€ a signaling game with an unknown number of
receivers, which have varying abilities to detect deception. Then we use a version
of this game to analyze two mechanisms (legal and economic) to deter botnet
recruitment. Equilibrium results indicate that 1) defenders can bound botnet ac-
tivity, and 2) legislating a minimum level of security has only a limited effect,
while incentivizing active defense can decrease botnet activity arbitrarily. This
work provides a quantitative foundation for proactive PDoS defense.

1

Introduction to the IoT and PDoS Attacks

The Internet of things (IoT) is a â€œdynamic global network infrastructure with self-
conï¬guring capabilities based on standard and interoperable communication protocols
where physical and virtual â€˜thingsâ€™ have identities, physical attributes, and virtual per-
sonalitiesâ€ [2]. The IoT is 1) decentralized, 2) heterogeneous, and 3) connected to the
physical world. It is decentralized because nodes have â€œself-conï¬guring capabilities,â€
some amount of local intelligence, and incentives which are not aligned with the other
nodes. The IoT is heterogeneous because diverse â€œthingsâ€ constantly enter and leave
the IoT, facilitated by â€œstandard and interoperable communication protocols.â€ Finally,
IoT devices are connected to the physical world, i.e., they are part of cyber-physical
systems (CPS). For instance, they may inï¬‚uence behavior, control the ï¬‚ow of trafï¬c,
and optimize home lighting.

(cid:63) This work is partially supported by an NSF IGERT grant through the Center for Interdisci-
plinary Studies in Security and Privacy (CRISSP) at New York University, by the grant CNS-
1544782, EFRI-1441140, and SES-1541164 from National Science Foundation (NSF) and
de-ne0008571 from the Department of Energy.

 
 
 
 
 
 
Fig. 1. Conceptual diagram of a PDoS attack. 1) Attack sponsor hires botnet herder. 2) Botnet
herder uses server to manage recruitment. 3) Malware scans for vulnerable IoT devices and begins
cascading infection. 4) Botnet herder uses devices (e.g., HVAC controllers) to deplete bandwidth
of a cyber-physical service (e.g., electrical power).

1.1 Difï¬culties in Securing the Internet of Things

While the IoT promises gains in efï¬ciency, customization, and communication ability, it
also raises new challenges. One of these challenges is security. The social aspect of IoT
devices makes them vulnerable to attack through social engineering. Moreover, the dy-
namic and heterogeneous attributes of the IoT create a large attack surface. Once com-
promised, these â€œthingsâ€ serve as vectors for attack. The most notable example has been
the Mirai botnet attack on Dyn in 2016. Approximately 100,000 botsâ€”largely belong-
ing to the (IoT)â€”attacked the domain name server (DNS) for Twitter, Reddit, Github,
and the New York Times [15]. A massive ï¬‚ow of trafï¬c overwhelmed the bandwidth of
the DNS.

1.2 Denial of Cyber-Physical Service Attacks

Since IoT devices are part of CPS, they also require physical â€œbandwidth.â€ As an ex-
ample, consider the navigation app Waze [1]. Waze uses real-time trafï¬c information to
ï¬nd optimal navigation routes. Due to its large number of users, the app also inï¬‚uences
trafï¬c. If too many users are directed to one road, they can consume the physical band-
width of that road and cause unexpected congestion. An attacker with insider access to
Waze could use this mechanism to manipulate transportation networks.

Another example can be found in healthcare. Smart lighting systems (which deploy,
e.g., time-of-ï¬‚ight sensors) detect falls of room occupants [22]. These systems alert
emergency responders about a medical situation in an assisted living center or the home
of someone who is aging. But an attacker could potentially trigger many of these alerts
at the same time, depleting the response bandwidth of emergency personnel.

Such a threat could be called a denial of cyber-physical service attack. To distin-
guish it from a cyber-layer DDoS, we also use the acronym PDoS (Physical Denial of
Service). Figure 1 gives a conceptual diagram of a PDoS attack. In the rest of the pa-
per, we will consider one speciï¬c instance of a PDoS attack, although our analysis is
not limited to this example. We consider the infection and manipulation of a popula-
tion of IoT-based heating, ventilation, and air conditioning (HVAC) controllers in order

1234to cause a sudden load shock to the power grid. Attackers either disable demand re-
sponse switches used for reducing peak load [6], or they unexpectedly activate inactive
loads. This imposes risks ranging from frequency droop to load shedding and cascading
failures.

Fig. 2. PDoS defense can be designed at multiple layers. Malware detection and active defense
can combat initial infection, secure IoT design and strategic trust can reduce the spread of the
malware, and CPS can be resilient and physically-aware. We focus on detection and active de-
fense.

1.3 Modeling the PDoS Recruitment Stage

Defenses against PDoS can be designed at multiple layers (Fig. 2). The scope of this pa-
per is limited to defense at the stage of botnet recruitment, in which the attacker scans a
wide range of IP addresses, searching for devices with weak security settings. Mirai, for
example, does this by attempting logins with a dictionary of factory-default usernames
and passwords (e.g. root/admin, admin/admin, root/123456) [12]. Devices in
our mechanism identify these suspicious login attempts and use active defense to learn
about the attacker or report his activity.

In order to quantify the risk of malware infection, we combine two game-theoretic
models known as signaling games [14,7] and Poisson games [18]. Signaling games
model interactions between two parties, one of which possesses information unknown
to the other party. While signaling games consider only two players, we extend this
model by allowing the number of target IoT devices to be a random variable (r.v.) that
follows a Poisson distribution. This captures the fact that the malware scans a large
number of targets. Moreover, we allow the targets to have heterogeneous abilities to
detect malicious login attempts.

1.4 Contributions and Related Work

We make the following principle contributions:

1. We describe an IoT attack called a denial of cyber-physical service (PDoS).

Malware DetectionActive DefenseResilient GridSecureIoT DesignStrategic Trust ManagementPhysically-Aware Security2. We develop a general model called Poisson signaling games (PSG) which quantiï¬es

one-to-many signaling interactions.

3. We ï¬nd the pure strategy equilibria of a version of the PSG model for PDoS.
4. We analyze legal and economic mechanisms to deter botnet recruitment, and ï¬nd
that 1) defenders can bound botnet activity, and 2) legislating a minimum level of
security has only a limited effect, while incentivizing active defense, in principle,
can decrease botnet activity arbitrarily.

Signaling games are often used to model deception and trust in cybersecurity [16,19,21].
Poisson games have also been used to model malware epidemics in large populations
[11]. Wu et al. use game theory to design defense mechanisms against DDoS attacks
[24]. But the defense mechanisms mitigate the actual the ï¬‚ood of trafï¬c against a target
system, while we focus on botnet recruitment. Bensoussan et al. use a susceptible-
infected-susceptible (SIS) model to study the growth of a botnet [5]. But IoT devices
in our model maintain beliefs about the reliability of incoming messages. In this way,
our paper considers the need to trust legitimate messages. Finally, load altering attacks
[17,4] to the power grid are an example of PDoS attacks. But PDoS attacks can also
deal with other resources.

In Section 2, we review signaling games and Poisson games. In Section 3, we com-
bine them to create Poisson signaling games (PSG). In Section 4, we apply PSG to
quantify the population risk due to PDoS attacks. Section 5 obtains the perfect Bayesian
Nash equilibria of the model. Some of these equilibria are harmful for power companies
and IoT users. Therefore, we design proactive mechanisms to improve the equilibria in
Section 6. We underline the key contributions in Section 7.

2 Signaling Games and Poisson Games

This section reviews two game-theoretic models: signaling games and Poisson games.
In Section 3, we combine them to create PSG. PSG can be used to model many one-to-
many signaling interactions in addition to PDoS.

2.1 Signaling Games with Evidence

Signaling games are a class of dynamic, two-player, information-asymmetric games
between a sender S and a receiver R (c.f. [14,7]). Signaling games with evidence extend
the typical deï¬nition by giving receivers some exogenous ability to detect deception1
[20]. They are characterized by the tuple

Î¦SG = (cid:0)X, M, E, A, qS, Î´, uS, uR(cid:1) .

First, S posses some private information unknown to R. This private information is
called a type. The type could represent, e.g., a preference, a technological capability, or a

1 This is based on the idea that deceptive senders have a harder time communicating some
messages than truthful senders. In interpersonal deception, for instance, lying requires high
cognitive load, which may manifest itself in external gestures [23].

malicious intent. Let the ï¬nite set X denote the set of possible types, and let x âˆˆ X denote
one particular type. Each type occurs with a probability qS(x), where qS : X â†’ [0, 1]
such that (s.t.) âˆ‘
xâˆˆX

qS (x) = 1 and âˆ€x âˆˆ X, qS(x) â‰¥ 0.

Based on his private information, S communicates a message to the receiver. The
message could be, e.g., a pull request, the presentation of a certiï¬cate, or the execution
of an action which partly reveals the type. Let the ï¬nite set M denote the set of possible
messages, and let m âˆˆ M denote one particular type. In general, S can use a strategy in
which he chooses various m with different probabilities. We will introduce notation for
these mixed strategies later.

In typical signaling games (e.g. Lewis signaling games [14,7] and signaling games
discussed by Crawford and Sobel [7]), R only knows about x through m. But this sug-
gests that deception is undetectable. Instead, signaling games with evidence include a
detector2 which emits evidence e âˆˆ E about the senderâ€™s type [20]. Let Î´ : E â†’ [0, 1] s.t.
Î´(e | x, m) = 1 and Î´(e | x, m) â‰¥ 0. Then Î´(e | x, m)
for all x âˆˆ X and m âˆˆ M, we have âˆ‘
eâˆˆE

gives the probability with which the detector emits evidence e given type x and message
m. This probability is ï¬xed, not a decision variable. Finally A be a ï¬nite set of actions.
Based on m and e, R chooses some a âˆˆ A. For instance, R may choose to accept or reject
a request represented by the message. These can also be chosen using a mixed-strategy.
In general, x, m, and a can impact the utility of S and R. Therefore, let uS : M Ã— A â†’
R|X| be a vector-valued function such that uS (m, a) = (cid:2)uS
xâˆˆX . This is a column
vector with entries uS
x(m, a). These entries give the utility that S of each receiver of type
x âˆˆ X obtains for sending a message m when the receiver plays action a. Next, deï¬ne
the utility function for R by uR : X Ã— M Ã— A â†’ R, such that uR(x, m, a) gives the utility
that R receives when a sender of type x transmits message m and R plays action a.

x (m, a)(cid:3)

2.2 Poisson Games

Poisson games were introduced by Roger Myerson in 1998 [18]. This class of games
models interactions between an unknown number of players, each of which belongs to
one type in a ï¬nite set of types. Modeling the population uncertainty using a Poisson r.v.
is convenient because merging or splitting Poisson r.v. results in r.v. which also follow
Poisson distributions.

Section 3 will combine signaling games with Poisson games by considering a sender
which issues a command to a pool of an unknown number of receivers, which all re-
spond at once. Therefore, let us call the players of the Poisson game â€œreceivers,â€ al-
though this is not the nomenclature used in the original game. Poisson games are char-
acterized by the tuple

Î¦PG = (cid:0)Î»,Y, qR, A, ËœuR(cid:1) .

First, the population parameter Î» > 0 gives the mean and variance of the Poisson
distribution. For example, Î» may represent the expected number of mobile phone users

2 This could literally be a hardware or software detector, such as email ï¬lters which attempt
to tag phishing emails. But it could also be an an abstract notion meant to signify the innate
ability of a person to recognize deception.

within range of a base station. Let the ï¬nite set Y denote the possible types of each re-
ceiver, and let y âˆˆ Y denote one of these types. Each receiver has type y with probability
qR(y), where âˆ‘
yâˆˆY

qR(y) = 1 and âˆ€y âˆˆ Y, qR(y) > 0.

Because of the decomposition property of the Poisson r.v., the number of receivers
of each type y âˆˆ Y also follows a Poisson distribution. Based on her type, each receiver
chooses an action a in the ï¬nite set A. We have deliberately used the same notation
as the action for the signaling game, because these two actions will coincide in the
combined model.

Utility functions in Poisson games are deï¬ned as follows. For a âˆˆ A, let ca âˆˆ Z+ (the
set of non-negative integers) denote the count of receivers which play action a. Then let
c be a column vector which contains entries ca for each a âˆˆ A. Then c falls within the
set Z(A), the set of all possible integer counts of the number of players which take each
action.

yâˆˆY

y (a, c)(cid:3)

. The entries ËœuR

Poisson games assume that all receivers of the same type receive the same utility.
Therefore, let ËœuR : A Ã— Z(C) â†’ R|Y | be a vector-valued function such that ËœuR (a, c) =
(cid:2) ËœuR
y (a, c) give the utility that receivers of each type y âˆˆ Y ob-
tain for playing an action a while the vector of the total count of receivers that play each
action is given by c. Note that this is different from the utility function of receivers in
the signaling game. Given the strategies of the receivers, c is also distributed according
to a Poisson r.v.

3 Poisson Signaling Games

Figure 3 depicts Poisson signaling games (PSG). PSG are characterized by combining
Î¦SG and Î¦PG to obtain the tuple

SG = (cid:0)X,Y, M, E, A, Î», q, Î´,U S,U R(cid:1) .
Î¦PG

3.1 Types, Actions, and Evidence, and Utility

As with signaling games and Poisson games, X denotes the set of types of S, and Y
denotes the set of types of R. M, E, and A denote the set of messages, evidence, and
actions, respectively. The Poisson parameter is Î».

The remaining elements of Î¦PG

SG are slightly modiï¬ed from the signaling game or
Poisson game. First, q : X Ã— Y â†’ [0, 1]2 is a vector-valued function such that q (x, y)
gives the probabilities qS(x), x âˆˆ X, and qR(y), y âˆˆ Y, of each type of sender and receiver,
respectively.

As in the signaling game, Î´ characterizes the quality of the deception detector. But
receivers differ in their ability to detect deception. Various email clients, for example,
may have different abilities to identify phishing attempts. Therefore, in PSG, we de-
ï¬ne the mapping by Î´ : E â†’ [0, 1]|Y |, s.t. the vector Î´ (e | x, m) = [Î´y (e | x, m)]yâˆˆY gives
the probabilities Î´y(e | x, m) with which each receiver type y observes evidence e given

sender type x and message m. This allows each receiver type to observe evidence with
different likelihoods3.

Fig. 3. PSG model the third stage of a PDoS attack. A sender of type x chooses an action m which
is observed by an unknown number of receivers. The receivers have multiple types y âˆˆ Y. Each
type may observe different evidence e âˆˆ E. Based on m and e, each type of receiver chooses an
action a.

The utility functions U S and U R are also adjusted for PSG. Let U S : M Ã— Z(A) â†’
R|X| be a vector-valued function s.t. the vector U S (m, c) = (cid:2)U S
xâˆˆX gives the
utility of senders of each type x for sending message m if the count of receivers which
choose each action is given by c. Similarly, let U R : X Ã— M Ã— A Ã— Z(A) â†’ R|Y | be a
vector-valued function s.t. U R (x, m, a, c) = (cid:2)U R
yâˆˆY gives the utility of re-
ceivers of each type y âˆˆ Y. As earlier, x is the type of the sender, and m is the message.
But note that a denotes the action of this particular receiver, while c denotes the count
of overall receivers which choose each action.

y (x, m, a, c)(cid:3)

x (m, c)(cid:3)

3.2 Mixed-Strategies and Expected Utility

Next, we deï¬ne the nomenclature for mixed-strategies and expected utility functions.
For senders of each type x âˆˆ X, let ÏƒS
x(m)
gives the probability with which he plays each message m âˆˆ M. For each x âˆˆ X, let Î£S
x
denote the set of possible ÏƒS

x : M â†’ [0, 1] be a mixed strategy such that ÏƒS

x. We have

(cid:40)

Î£R
x =

Â¯Ïƒ | âˆ‘
mâˆˆM

Â¯Ïƒ (m) = 1 and âˆ€m âˆˆ M, Â¯Ïƒ (m) â‰¥ 0

.

(cid:41)

3 In fact, although all receivers with the same type y have the same likelihood Î´y(e | x, m) of
observing evidence e given sender type x and message m, our formulation allows the receivers
to observe different actual realizations e of the evidence.

Action ğ‘âˆˆğ´ğ‘†Action: ğ‘šâˆˆğ‘€Action ğ‘âˆˆğ´Action ğ‘âˆˆğ´Action ğ‘âˆˆğ´Type ğ‘¦âˆˆğ‘ŒType ğ‘¥âˆˆğ‘‹ğ‘†Type ğ‘¦âˆˆğ‘ŒType ğ‘¦âˆˆğ‘ŒType ğ‘¦âˆˆğ‘ŒEvidence ğ‘’âˆˆğ¸ğ‘’âˆˆğ¸ğ‘’âˆˆğ¸ğ‘’âˆˆğ¸ğ‘…ğ‘…ğ‘…ğ‘…For receivers of each type y âˆˆ Y, let ÏƒR

y : A â†’ [0, 1] denote a mixed strategy such that
y (a | m, e) gives the probability with which she plays action a after observing message

ÏƒR
m and action e. For each y âˆˆ Y, the function ÏƒR

y belongs to the set

(cid:40)

Î£R
y =

Â¯Ïƒ | âˆ‘
aâˆˆA

Â¯Ïƒ (a) = 1 and âˆ€a âˆˆ A, Â¯Ïƒ (a) â‰¥ 0

.

(cid:41)

In order to choose her actions, R forms a belief about the sender type x. Let ÂµR
y (x | m, e)
denote the likelihood with which each R of type y who observes message m and ev-
idence e believes that S has type x. In equilibrium, we will require this belief to be
consistent with the strategy of S.

Now we deï¬ne the expected utilities that S and each R receive for playing mixed
x Ã— Î£R â†’ R.
strategies. Denote the expected utility of a sender of type x âˆˆ X by Â¯U S
Notice that all receiver strategies must be taken into account. This expected utility is
given by

x : Î£S

x (ÏƒS
Â¯U S

x, ÏƒR) = âˆ‘

âˆ‘
câˆˆZ(A)

mâˆˆM

x (m) P (cid:8)c | ÏƒR, x, m(cid:9)U S
ÏƒS

x (m, c).

Here, P{c | ÏƒR, x, m} is the probability with which the vector c gives the count of re-
ceivers that play each action. Myerson shows that, due to the aggregation and decompo-
sition properties of the Poisson r.v., the entries of c are also Poisson r.v. [18]. Therefore,
P{c | ÏƒR, x, m} is given by

P (cid:8)c | ÏƒR, x, m(cid:9) = âˆ

aâˆˆA

eÎ»a Î»ca
a
ca!

, Î»a = Î»âˆ‘
yâˆˆY

âˆ‘
eâˆˆE

qR (y) Î´y (e | x, m) ÏƒR

y (a | m, e) .

(1)

Next, denote the expected utility of each receiver of type y âˆˆ Y by Â¯U R

y Ã—Î£R â†’ R.
y ) gives the expected utility when this particular receiver plays
y and the population of all types of receivers plays the mixed-

y : Î£R

Here, Â¯U R
y (Î¸, ÏƒR | m, e, ÂµR
mixed strategy Î¸ âˆˆ Î£R
strategy vector ÏƒR. The expected utility is given by

y (Î¸, ÏƒR | m, e, ÂµR
Â¯U R

y ) = âˆ‘
xâˆˆX

âˆ‘
aâˆˆA

âˆ‘
câˆˆZ(A)
y (x | m, e) Î¸ (a | m, e) P (cid:8)c | ÏƒR, x, m(cid:9)U R
ÂµR

y (x, m, a, c),

(2)

where again P{c | ÏƒR, x, m} is given by Eq. (1).

3.3 Perfect Bayesian Nash Equilibrium

First, since PSG are dynamic, we use an equilibrium concept which involves perfec-
tion. Strategies at each information set of the game must be optimal for the remaining
subgame [8]. Second, since PSG involve incomplete information, we use a Bayesian
concept. Third, since each receiver chooses her action without knowing the actions of
the other receivers, the Poisson stage of the game involves a ï¬xed point. All receivers
choose strategies which best respond to the optimal strategies of the other receivers.

Perfect Bayesian Nash equilibrium (PBNE) is the appropriate concept for games with
these criteria [8].

Consider the two chronological stages of PSG. The second stage takes place among
the receivers. This stage is played with a given m, e, and ÂµR determined by the sender
(and detector) in the ï¬rst stage of the game. When m, e, and ÂµR are ï¬xed, the interaction
y : Î£R â†’ P (Î£R
between all receivers becomes a standard Poisson game. Deï¬ne BRR
y )
(where P (S) denotes the power set of S) such that the best response of a receiver of
type y to a strategy proï¬le ÏƒR of the other receivers is given by the strategy or set of
strategies

BRR
y

(cid:0)ÏƒR | m, e, ÂµR
y

(cid:1) (cid:44) arg max

Â¯U R
y

(cid:0)Î¸, ÏƒR | m, e, ÂµR
y

(cid:1) .

(3)

Î¸âˆˆÎ£R
y

The ï¬rst stage takes place between the sender and the set of receivers. If we ï¬x the
set of receiver strategies ÏƒR, then the problem of a sender of type x âˆˆ X is to choose ÏƒS
x
to maximize his expected utility given ÏƒR. The last criteria is that the receiver beliefs
ÂµR must be consistent with the sender strategies according to Bayesâ€™ Law. Deï¬nition 1
applies PBNE to PSG.

Deï¬nition 1. (PBNE) Strategy and belief proï¬le (ÏƒSâˆ—, ÏƒRâˆ—, ÂµR) is a PBNE of a PSG if
all of the following hold [8]:

âˆ€x âˆˆ X, ÏƒSâˆ—

x âˆˆ arg max
x âˆˆÎ£S
ÏƒS
x

x (ÏƒS
Â¯U S

x, ÏƒRâˆ—),

âˆ€y âˆˆ Y, âˆ€m âˆˆ M, âˆ€e âˆˆ E, ÏƒRâˆ—

y âˆˆ BRR
y

(cid:0)ÏƒRâˆ— | m, e, ÂµR
y

(cid:1) ,

âˆ€y âˆˆ Y, âˆ€m âˆˆ M, âˆ€e âˆˆ E, ÂµR

y (d | m, e) âˆˆ

Î´y (e | d, m) ÏƒS
âˆ‘
ËœxâˆˆX

Î´y (e | Ëœx, m) ÏƒS

d (m) qS (d)
Ëœx (m) qS ( Ëœx)

(4)

(5)

(6)

,

if âˆ‘
ËœxâˆˆX
have ÂµR

Î´y (e | Ëœx, m) ÏƒS
y (l | m, e) = 1 âˆ’ ÂµR

Ëœx (m) qS ( Ëœx) > 0, and ÂµR
y (d | m, e) .

y (d | m, e) âˆˆ [0, 1] , otherwise. We also always

Equation (4) requires the sender to choose an optimal strategy given the strategies
of the receivers. Based on the message and evidence that each receiver observes, Eq. (5)
requires each receiver to respond optimally to the proï¬le of the strategies of the other
receivers. Equation (6) uses Bayesâ€™ law (when possible) to obtain the posterior beliefs
ÂµR using the prior probabilities qS, the sender strategies ÏƒS, and the characteristics Î´y,
y âˆˆ Y of the detectors [20].

4 Application of PSG to PDoS

Section 3 deï¬ned PSG in general, without specifying the members of the type, message,
evidence, or action sets. In this section, we apply PSG to the recruitment stage of PDoS
attacks. Table 1 summarizes the nomenclature.

S refers to the agent which attempts a login attempt, while R refers to the device.
Let the set of sender types be given by X = {l, d}, where l represents a legitimate
login attempt, while d represents a malicious attempt. Malicious S attempt to login to

many devices through a wide IP scan. This number is drawn from a Poisson r.v. with
parameter Î». Legitimate S only attempt to login to one device at a time. Let the receiver
types be Y = {k, o, v}. Type k represents weak receivers which have no ability to detect
deception and do not use active defense. Type o represents strong receivers which can
detect deception, but do not use active defense. Finally, type v represents active receivers
which can both detect deception and use active defense.

Table 1. Application of PSG to PDoS Recruitment

Set
Type x âˆˆ X of S
Type y âˆˆ Y of R
Message m âˆˆ M of S
Evidence e âˆˆ E
Action a âˆˆ A of R

Elements
l : legitimate, d : malicious
k : no detection; o : detection; v : detection & active defense
m = {m1, m2, . . .}, a set of |m| password strings
b : suspicious, n : not suspicious
t : trust, g : lockout, f : active defense

4.1 Messages, Evidence Thresholds, and Actions

Messages consist of sets of consecutive unsuccessful login attempts. They are denoted
by m = {m1, m2, . . .}, where each m1, m2, . . . is a string entered as an attempted pass-
word4. For instance, botnets similar to Mirai choose a list of default passwords such as
[12]

m = {admin, 888888, 123456, default, support} .

Of course, devices can lockout after a certain number of unsuccessful login attempts.
Microsoft Server 2012 recommends choosing a threshold at 5 to 9 [3]. Denote the lower
end of this range by Ï„L = 5. Let us allow all attempts with |m| < Ï„L. In other words, if
a user successfully logs in before Ï„L, then the PSG does not take place. (See Fig. 5.)

The PSG takes place for |m| â‰¥ Ï„L. Let Ï„H = 9 denote the upper end of the Microsoft
range. After Ï„L, S may persist with up to Ï„H login attempts, or he may not persist. Let p
denote persist, and w denote not persist. Our goal is to force malicious S to play w with
high probability.

For R of types o and v, if S persists and does not successfully log in with |m| â‰¤
Ï„H login attempts, then e = b. This signiï¬es a suspicious login attempt. If S persists
and does successfully login with |m| â‰¤ Ï„H attempts, then e = n, i.e., the attempt is not
suspicious5.

If a user persists, then the device R must choose an action a. Let a = t denote
trusting the user, i.e., allowing login attempts to continue. Let a = g denote locking the

4 A second string can also be considered for the username.
5 For strong and active receivers, Î´y (b | d, p) > Î´y (b | l, p) , y âˆˆ {o, v}. That is, these receivers
are more likely to observe suspicious evidence if they are interacting with a malicious sender
than if they are interacting with a legitimate sender. Mathematically, Î´k(b | d, p) = Î´k(b | l, p)
signiï¬es that type k receivers do not implement a detector.

device to future login attempts. Finally, let a = f denote using an active defense such as
reporting the suspicious login attempt to an Internet service provider (ISP), recording
the attempts in order to gather information about the possible attacker, or attempting to
block the offending IP address.

4.2 Characteristics of PDoS Utility Functions

The nature of PDoS attacks implies several features of the utility functions U S and U R.
These are listed in Table 2. Characteristic 1 (C1) states that if S does not persist, both
players receive zero utility. C2 says that R also receives zero utility if S persists and
R locks down future logins. Next, C3 states that receivers of all types receive positive
utility for trusting a benign login attempt, but negative utility for trusting a malicious
login attempt. We have assumed that only type v receivers use active defense; this is
captured by C4. Finally, C5 says that type v receivers obtain positive utility for using
active defense against a malicious login attempt, but negative utility for using active
defense against a legitimate login attempt. Clearly, C1-C5 are all natural characteristics
of PDoS recruitment.

Table 2. Characteristics of PDoS Utility Functions

#

C1

C2

C3

C4

C5

Notation
âˆ€x âˆˆ X, y âˆˆ Y , a âˆˆ A, c âˆˆ Z (A) ,
U S
y (x, w, a, c) = 0.

x (w, c) = U R
âˆ€x âˆˆ X, y âˆˆ Y, c âˆˆ Z (A) ,
U R
y (x, p, g, c) = 0.
âˆ€y âˆˆ Y, c âˆˆ Z (A) ,

U R

y (d, p,t, c) < 0 < U R

y (l, p,t, c).

âˆ€x âˆˆ X, c âˆˆ Z (A) ,

k (x, p, f , c) = U R
U R

o (x, p, f , c) = âˆ’âˆ.

âˆ€c âˆˆ Z (A) ,
v (l, p, f , c) < 0 < U R

U R

v (d, p, f , c).

4.3 Modeling the Physical Impact of PDoS Attacks

The quantities ct , cg, and c f denote, respectively, the number of devices that trust, lock
down, and use active defense. Deï¬ne the function Z : Z(A) â†’ R such that Z(c) de-
notes the load shock that malicious S cause based on the count c. Z(c) is clearly non-
decreasing in ct , because each device that trusts the malicious sender becomes infected
and can impose some load shock to the power grid.

The red (solid) curve in Fig. 4 conceptually represents the mapping from load shock
size to damage caused to the power grid based on the mechanisms available for regu-
lation. Small disturbances are regulated using automatic frequency control. Larger dis-
turbances can signiï¬cantly decrease frequency and should be mitigated. Grid operators
have recently offered customers load control switches, which automatically deactivate

Fig. 4. Conceptual relationship between load shock size and damage to the power grid. Small
shocks are mitigated through automatic frequency control or demand-side control of ï¬‚exible
loads. Large shocks can force load shedding or blackouts.

appliances in response to a threshold frequency decrease [10]. But the size of this vol-
untary demand-side control is limited. Eventually, operators impose involuntary load
shedding (i.e., rolling blackouts). This causes higher inconvenience. In the worst case,
transient instability leads to cascading failures and blackout [9].

The yellow and orange dashed curves in Fig. 4 provide two approximations to Z(c).
The yellow curve, ËœZlin(c), is linear in ct . We have ËœZlin(c) = Ï‰t
d is a positive
real number. The orange curve, ËœZstep(c), varies according to a step function, i.e., Z(c) =
â„¦t
d is a positive real number and 1{â€¢} is the indicator function. In this
paper, we derive solutions for the linear approximation. Under this approximation, the
utility of malicious S is given by

d1{ct >Ï„t }, where â„¦t

dct , where Ï‰t

d (m, c) = ËœZlin(c) + Ï‰g
U S

dcg + Ï‰ f

d c f = Ï‰t

dct + Ï‰g

dcg + Ï‰ f

d c f .

where Ï‰g
down or uses active defense, respectively.

d < 0 and Ï‰ f

d < 0 represent the utility to malicious S for each device that locks

Using ËœZlin(c), the decomposition property of the Poisson r.v. simpliï¬es Â¯U S

x, ÏƒR).
We show in Appendix A that the senderâ€™s expected utility depends on the expected
values of each of the Poisson r.v. that represent the number of receivers who choose
each action ca, a âˆˆ A. The result is that

x (ÏƒS

x (ÏƒS
Â¯U S

x, ÏƒR) = Î»ÏƒS

x (p) âˆ‘
yâˆˆY

âˆ‘
eâˆˆE

âˆ‘
aâˆˆA

qR (y) Î´y (e | x, p) ÏƒR

y (a | p, e) Ï‰a
x.

(7)

Next, assume that the utility of each receiver does not depend directly on the actions
of the other receivers. (In fact, the receivers are still endogenously coupled through
the action of S.) Abusing notation slightly, we drop c (the count of receiver actions)
in U R
y ).
Equation (2) is now

y (x, m, a, c) and ÏƒR (the strategies of the other receivers) in Â¯U R

y (Î¸, ÏƒR | m, e, ÂµR

Â¯U R
y

(cid:0)Î¸ | m, e, ÂµR
y

(cid:1) = âˆ‘

xâˆˆX

âˆ‘
aâˆˆ{t, f }

y (x | m, e) Î¸ (a | m, e)UR
ÂµR

y (x, m, a) .

(0,0)Power Impact ğ‘(ğ‘)Load Shock Size âˆğ‘ğ‘¡Automatic Frequency ControlDemand-side control of flexible loadsAutomatic load sheddingDemand-side control of less flexible loadsInstability and blackout5 Equilibrium Analysis

In this section, we obtain the equilibrium results by parameter region. In order to sim-
plify analysis, without loss of generality, let the utility functions be the same for all
receiver types (except when a = f ), i.e., âˆ€x âˆˆ X, U R
v (x, p,t).
Also without loss of generality, let the quality of the detectors for types y âˆˆ {o, v} be
the same: âˆ€e âˆˆ E, x âˆˆ X, Î´o(e | x, p) = Î´v(e | x, p).

k (x, p,t) = U R

o (x, p,t) = U R

Fig. 5. Model of a PSG under Lemma 1. Only one of many R is depicted. After the types x and
y, of S and R, respectively, are drawn, S chooses whether to persist beyond Ï„L attempts. Then R
chooses to trust, lockout, or use active defense against S based on whether S is successful. Lemma
1 determines all equilibrium strategies except ÏƒSâˆ—
v (â€¢ | p, b) , marked by
the blue and red items.

o (â€¢ | p, b) , and ÏƒRâˆ—

d (â€¢) , ÏƒRâˆ—

5.1 PSG Parameter Regime

We now obtain equilibria for a natural regime of the PSG parameters. First, assume that
legitimate senders always persist: ÏƒS
l (p) = 1. This is natural for our application, because
IoT HVAC users will always attempt to login. Second, assume that R of all types trust
login attempts which appear to be legitimate (i.e., give evidence e = n). This is satisï¬ed
for

qS (d) <

U R
k (l, p,t)
k (l, p,t) âˆ’U R
U R
Third, we consider the likely behavior of R of type o when a login attempt is suspi-
cious. Assume that she will lock down rather than trust the login. This occurs under the
parameter regime

k (d, p,t)

(8)

.

qS (d) >

ËœU R
o (l, p,t)
o (l, p,t) âˆ’ ËœU R

ËœU R

o (d, p,t)

,

(9)

Legitimateğ‘¥=ğ‘™Maliciousğ‘¥=ğ‘‘ğ‘š<ğœğ¿ğ‘š<ğœğ¿ğ‘šâ‰¥ğœğ»â†’ğ‘š=ğ‘ğ‘šâ‰¥ğœğ¿NOGAMENOGAMEğ‘šâ‰¥ğœğ¿ğ‘šâ‰¥ğœğ»â†’ğ‘š=ğ‘ğ‘š<ğœğ»â†’ğ‘š=ğ‘¤Successâ†’ğ‘’=ğ‘›Failureâ†’ğ‘’=ğ‘Successâ†’ğ‘’=ğ‘›Failureâ†’ğ‘’=ğ‘ğ‘=ğ‘¡ğ‘=ğ‘¡ğ‘=ğ‘¡ğ‘=ğ‘”ğ‘=ğ‘“ğ‘=ğ‘¡ğ‘=ğ‘”ğ‘=ğ‘“ZEROUTILITYSUCCESSFUL LOGINSUCCESSFUL LOGINLOCKOUT USERACTIVE DEFENSEAGAINSTUSERINFECTIONINFECTIONLOCKOUTATTACKERACTIVE DEFENSEAGAINST ATTACKERusing the shorthand notation

o (l, p,t) = U R
ËœU R

o (l, p,t) Î´0 (b | l, p) ,

o (d, p,t) = U R
ËœU R

o (d, p,t) Î´0 (b | d, p) .

The fourth assumption addresses the action of R of type v when a login attempt is
suspicious. The optimal action depends on her belief ÂµR
o (d | p, b) that S is malicious.
The belief, in turn, depends on the mixed-strategy probability with which malicious S
persist. We assume that there is some ÏƒS
d(p) for which R should lock down (a = g). This
is satisï¬ed if there exists a real number Ï† âˆˆ [0, 1] such that, given6 ÏƒS

d(p) = Ï†,

v (t | p, b, ÂµR
Â¯U R

v ) > 0,

v ( f | p, b, ÂµR
Â¯U R

v ) > 0.

(10)

This simpliï¬es analysis, but can be removed if necessary.

Lemma 1 summarizes the equilibrium results under these assumptions. Legitimate
S persist, and R of type o lock down under suspicious login attempts. All receiver types
trust login attempts which appear legitimate. R of type k, since she cannot differenti-
ate between login attempts, trusts all of them. The proof follows from the optimality
conditions in Eq. (4-6) and the assumptions in Eq. (8-10).

Lemma 1. (Constant PBNE Strategies) If ÏƒS
following equilibrium strategies are implied:

d(p) = 1 and Eq. (8-10) hold, then the

ÏƒSâˆ—
l (p) = 1, ÏƒRâˆ—

o (g | p, b) = 1, ÏƒRâˆ—

k (t | p, b) = 1,

o (t | p, n) = ÏƒRâˆ—
ÏƒRâˆ—

v (t | p, n) = ÏƒRâˆ—
Figure 5 depicts the results of Lemma 1. The remaining equilibrium strategies to be
obtained are denoted by the red items for S and the blue items for R. These strategies
are ÏƒRâˆ—
d (p) depends on whether R of
type o and type v will lock down and/or use active defense to oppose suspicious login
attempts.

d (p). Intuitively, ÏƒSâˆ—

v (â€¢ | p, b), and ÏƒSâˆ—

o (â€¢ | p, b), ÏƒRâˆ—

k (t | p, n) = 1.

5.2 Equilibrium Strategies

The remaining equilibrium strategies fall into four parameter regions. In order to delin-
eate these regions, we deï¬ne two quantities.

Let T DR
type v if ÏƒS
some probability. Equation (3) can be used to show that

v (U R
d(p) = 1. If qS(d) > T DR

v , Î´v) denote a threshold which determines the optimal action of R of
v , Î´v), then the receiver uses active defense with

v (U R

T DR
v

(cid:0)U R

v , Î´v

(cid:1) =

ËœU R
v (l, p, f )
v (l, p, f ) âˆ’ ËœU R

ËœU R

v (d, p, f )

,

where we have used the shorthand notation:

v (l, p, f ) := U R
ËœU R

v (l, p, f ) Î´v (b | l, p) ,

v (d, p, f ) := U R
ËœU R

v (d, p, f ) Î´v (b | d, p) .

6 We abuse notation slightly to write Â¯U R

v (a | m, e, ÂµR

y ) for the expected utility that R of type v

obtains by playing action a.

Next, let BPS
d
m = p, i.e., for persisting. We have

(cid:0)Ï‰d, qR, Î´(cid:1) denote the beneï¬t which S of type d receives for choosing

BPS
d

(cid:0)Ï‰d, qR, Î´(cid:1) := âˆ‘

yâˆˆY

âˆ‘
eâˆˆE

qR (y) Î´y (e | d, p) ÏƒR

y (a | p, e) Ï‰a
d.

âˆ‘
aâˆˆA

If this beneï¬t is negative, then S will not persist. Let BPS
d
the beneï¬t of persisting when receivers use the pure strategies:

(cid:0)Ï‰d, qR, Î´ | ak, ao, av

(cid:1) denote

k (ak | p, b) = ÏƒR
ÏƒR

o (ao | p, b) = ÏƒR

v (av | p, b) = 1.

We now have Theorem 1, which predicts the risk of malware infection in the remaining
parameter regions. The proof is in Appendix B.

Theorem 1. (PBNE within Regions) If ÏƒS
v (â€¢ | p, b), and ÏƒSâˆ—
ÏƒRâˆ—

d (p) vary within the four regions listed in Table 3.

d(p) = 1 and Eq. (8-10) hold, then ÏƒRâˆ—

o (â€¢ | p, b),

In the status quo equilibrium, strong and active receivers lock down under suspi-
cious login attempts. But this is not enough to deter malicious senders from persisting.
We call this the status quo because it represents current scenarios in which botnets in-
fect vulnerable devices but incur little damage from being locked out of secure devices.
This is a poor equilibrium, because ÏƒSâˆ—
d (p) = 1.

In the active deterrence equilibrium, lockouts are not sufï¬cient to deter malicious
v , R of type v use active defense. This is
d (p) < 1. In this equilibrium, R of type o always locks

S from fully persisting. But since qS(d) > T DR
enough to deter malicious S : ÏƒSâˆ—
down: ÏƒRâˆ—

o (g | p, b) = 1. R of type v uses active defense with probability

ÏƒRâˆ—
v ( f | p, b) =

dqR (k) + Ï‰g
Ï‰t
(cid:17)
(cid:16)
d âˆ’ Ï‰ f
Ï‰g

d

(cid:0)qR (o) + qR (v)(cid:1)
d
qR (v) Î´v (v | d, p)

,

(11)

and otherwise locks down: ÏƒRâˆ—
reduced probability

v (g | p, b) = 1 âˆ’ ÏƒRâˆ—

v ( f | p, b) . Deceptive S persist with

ÏƒSâˆ—
d (p) =

(cid:18)

1
qS (d)

ËœU R
v (l, p, f )
v (l, p, f ) âˆ’ ËœU R

ËœU R

v (d, p, f )

(cid:19)

.

(12)

In the resistant attacker equilibrium, qS(d) > T DR

v . Therefore, R of type v use active
defense. But BPS
d (â€¢ |t, g, f ) > 0, which means that the active defense is not enough to
deter malicious senders. This is a â€œhopelessâ€ situation for defenders, since all available
means are not able to deter malicious senders. We still have ÏƒSâˆ—

d (p) = 1.

In the vulnerable attacker equilibrium, there is no active defense. But R of type
o and type v lock down under suspicious login attempts, and this is enough to deter
malicious S, because BPS
d (â€¢ |t, g, g) < 0. R of types o and v lock down with probability

o (g | p, b) = ÏƒRâˆ—
ÏƒRâˆ—

v (g | p, b) =

Ï‰t
d

(qR (0) + qR (v)) Î´o (b | d, p) (cid:0)Ï‰t

d âˆ’ Ï‰g

d

(cid:1) ,

(13)

and trust with probability ÏƒRâˆ—
persist with reduced probability

o (t | p, b) = ÏƒRâˆ—

v (t | p, b) = 1 âˆ’ ÏƒRâˆ—

o (g | p, b) . Deceptive S

ÏƒSâˆ—
d (p) =

(cid:18)

1
qS (d)

ËœU R
o (l, p,t)
o (l, p,t) âˆ’ ËœU R

ËœU R

o (d, p,t)

(cid:19)

.

(14)

The status quo and resistant attacker equilibria are poor results because infection
of devices is not deterred at all. The focus of Section 6 will be to shift the PBNE to the
other equilibrium regions, in which infection of devices is deterred to some degree.

Table 3. Equilibrium Regions of the PSG for PDoS

qS(d) < T DR

v (â€¢) qS(d) > T DR

v (â€¢)

0 < ÏƒRâˆ—
0 < ÏƒRâˆ—

Vulnerable Attacker
ÏƒSâˆ—(p) < 1
o (t | p, b), ÏƒRâˆ—
v (t | p, b), ÏƒRâˆ—

o (g | p, b) < 1
v (g | p, b) < 1
Active Deterrence
ÏƒSâˆ—(p) < 1
ÏƒRâˆ—
o (g | p, b) = 1
Status Quo
0 < ÏƒRâˆ—
ÏƒSâˆ—(p) = 1
v (g | p, b),
o (g | p, b) = 1 ÏƒRâˆ—
ÏƒRâˆ—
v ( f | p, b) < 1
ÏƒRâˆ—
v (g | p, b) = 1 Resistant Attacker

ÏƒSâˆ—(p) = 1
ÏƒRâˆ—
o (g | p, b) = 1
ÏƒRâˆ—
v ( f | p, b) = 1

BPS

d (â€¢ |t, g, g) < 0,

BPS

d (â€¢ |t, g, f ) < 0

BPS

d (â€¢ |t, g, g) > 0,

BPS

d (â€¢ |t, g, f ) < 0

BPS

d (â€¢ |t, g, g) > 0,

BPS

d (â€¢ |t, g, f ) > 0

6 Mechanism Design

The equilibrium results are delineated by the quantities qS, T DR
These quantities are functions of the parameters qS, qR, Î´o, Î´v, Ï‰d, and U R
v . Mechanism
design manipulates these parameters in order to obtain a desired equilibrium. We dis-
cuss two possible mechanisms.

v , Î´v) and BPS

v (U R

d (Ï‰d, qR, Î´).

6.1 Legislating Basic Security

Malware which infects IoT devices is successful because many IoT devices are poorly
secured. Therefore, one mechanism design idea is to legally require better authentica-
tion methods, in order to decrease qR(k) and increase qR(o).

The left-hand sides of Figs. 6-8 depict the results. Figure 6(a) shows that decreas-
ing qR(k) and increasing qR(o) moves the game from the status quo equilibrium to the
vulnerable attacker equilibrium. But Fig. 7(a) shows that this only causes a ï¬xed de-
crease in ÏƒSâˆ—
d (p), regardless of the amount of decrease in qR(k). The reason, as shown
in Fig. 8(a), is that as qR(o) increases, it is incentive-compatible for receivers to lock

Fig. 6. Equilibrium transitions for (a) legal and (b) active defense mechanisms. The equilibrium
numbers signify: 1-status quo, 2-resistant attacker, 3-vulnerable attacker, 4-active deterrence.

Fig. 7. Malware persistence rate for (a) legal and (b) active defense mechanisms.

Fig. 8. Probabilities of opposing malicious S. Plot (a): probability that R lock down with the legal
mechanism. Plot (b): probability that R use active defense.

0.80.850.90.95qR(o)11.522.53Equilibrium Theorem NumberEquilibrium Regiongd=-0.3gd=-0.5gd=-0.701020304050URv(d,p,f)1234Equilibrium Theorem NumberEquilibrium Regionfd=-12fd=-14fd=-160.80.850.90.95qR(o)0.80.850.90.9511.05S*d(p)Persistence Rate of Malwaregd=-0.3gd=-0.5gd=-0.701020304050URv(d,p,f)0.20.40.60.81S*d(p)Persistence Rate of Malwarefd=-12fd=-14fd=-160.80.850.90.95qR(o)0.880.90.920.940.960.981Ro(g|p,b) = Rv(g|p,b)Lock Out Probability of R of Types o and vgd=-0.3gd=-0.5gd=-0.701020304050URv(d,p,f)00.511.522.5Rv(f|p,b)10-3Active Def. Prob. of R of Type vfd=-12fd=-14fd=-16down with progressively lower probability ÏƒRâˆ—
y (g | p, b), y âˆˆ {o, v}. Rather than forcing
malicious S to not persist, increasing qR(o) only decreases the incentive for receivers to
lock down under suspicious login attempts.

6.2

Incentivizing Active Defense

One reason for the proliferation of IoT malware is that most devices which are secure
(i.e., R of type y = o) do not take any actions against malicious login attempts except
to lock down (i.e., to play a = g). But there is almost no cost to malware scanners for
making a large number of login attempts under which devices simply lock down. There
is a lack of economic pressure which would force ÏƒSâˆ—

d (p) < 1, unless qR(0) â‰ˆ 1.

This is the motivation for using active defense such as reporting the activity to an
ISP or recording the attempts in order to gather information about the attacker. The
right hand sides of Figs. 6-8 show the effects of providing an incentive U R
v (d, p, f ) for
active defense. This incentive moves the game from the status quo equilibrium to either
the resistant attacker equilibrium or the vulnerable attacker equilibrium, depending on
whether BPS
d (â€¢ |t, g, f ) is positive (Fig. 6(b)). In the vulnerable attacker equilibrium,
the persistence rate of malicious S is decreased (Fig. 7(b)). Finally, Fig. 8(b) shows that
only a small amount of active defense ÏƒRâˆ—
v ( f | p, b) is necessary, particularly for high
values of7 Ï‰ f
d .

7 Discussion of Results

The ï¬rst result is that the defender can bound the activity level of the botnet. Recall that
the vulnerable attacker and active deterrence equilibria force ÏƒSâˆ—
d (p) < 1. That is, they
decrease the persistence rate of the malware scanner. But another interpretation is pos-
sible. In Eq. (14) and Eq. (12), the product ÏƒSâˆ—
d (p) qS (d) is bounded. This product can
be understood as the total activity of botnet scanners: a combination of prior probability
of malicious senders and the effort that malicious senders exert8. Bensoussan et al. note
that the operators of the Conï¬ker botnet of 2008-2009 were forced to limit its activity
[5,13]. High activity levels would have attracted too much attention. The authors of [5]
conï¬rm this result analytically, using a dynamic game based on an SIS infection model.
Interestingly, our result agrees with [5], but using a different framework.

Secondly, we compare the effects of legal and economic mechanisms to deter re-
cruitment for PDoS. Figures 6(a)-8(a) showed that ÏƒSâˆ—
d (p) can only be reduced by a
ï¬xed factor by mandating security for more and more devices. In this example, we
found that strategic behavior worked against legal requirements. By comparison, Figs.
6(b)-8(b) showed that ÏƒSâˆ—
d (p) can be driven arbitrarily low by providing an economic
incentive U R

v (d, p, f ) to use active defense.

v ( f | p, b) = 1 for Ï‰ f

7 In Fig. 8(b), ÏƒRâˆ—
d = âˆ’12.
8 A natural interpretation in an evolutionary game framework would be that ÏƒSâˆ—

d (p) = 1, and
qS(d) decreases when the total activity is bounded. In other words, malicious senders con-
tinue recruiting, but some malicious senders drop out since not all of them are supported in
equilibrium.

Future work can evaluate technical aspects of mechanism design such as improving
malware detection quality. This would involve a non-trivial trade-off between a high
true-positive rate and a low false-positive rate. Note that the model of Poisson signaling
games is not restricted PDoS attacks. PSG apply to any scenario in which one sender
communicates a possibly malicious or misleading message to an unknown number of
receivers. In the IoT, the model could capture the communication of a roadside location-
based service to a set of autonomous vehicles, or spooï¬ng of a GPS signal used by
multiple ships with automatic navigation control, for example. Online, the model could
apply to deceptive opinion spam in product reviews. In interpersonal interactions, PSG
could apply to advertising or political messaging.

A Simpliï¬cation of Sender Expected Utility

Each each component of c is distributed according to a Poisson r.v. The components are
P{ca | ÏƒR, x, m}. Recall that S receives zero utility
independent, so P{c | ÏƒR, x, m} = âˆ
aâˆˆA
when he plays m = w. So we can choose m = p :

x (ÏƒS
Â¯U S

x, ÏƒR) = ÏƒS

x (p) âˆ‘
câˆˆZ(A)

âˆ
aâˆˆA

P (cid:8)ca | ÏƒR, x, p(cid:9) (cid:0)Ï‰t

xct + Ï‰g

xcg + Ï‰ f

x c f

(cid:1) .

Some of the probability terms can be summed over their support. We are left with

x (ÏƒS
Â¯U S

x, ÏƒR) = ÏƒS

x (p) âˆ‘
aâˆˆA

caP (cid:8)ca | ÏƒR, x, p(cid:9) .

Ï‰a
x âˆ‘
caâˆˆZ+

(15)

The last summation is the expected value of ca, which is Î»a. This yields Eq. (7).

B Proof of Theorem 1

o (g | p, b) and ÏƒRâˆ—

The proofs for the status quo and resistant attacker equilibria are similar to the proof for
Lemma 1. The vulnerable attacker equilibrium is a partially-separating PBNE. Strate-
gies ÏƒRâˆ—
v (g | p, b) which satisfy Eq. (13) make malicious senders exactly
indifferent between m = p and m = w. Thus, they can play the mixed-strategy in Eq.
(14), which makes strong and active receivers exactly indifferent between a = g and
a = t. The proof of the vulnerable attacker equilibrium follows a similar logic.

References

1. Free community-based mapping, trafï¬c and navigation app. Waze Mobile, [Online]. Avail-

able: https://www.waze.com/.

2. Visions and challenges for realising the internet of things. Technical report, CERP IoT

Cluster, European Commission, 2010.
2014.

threshold,

lockout

3. Account

Microsoft TechNet,

[Online]. Available:

https://technet.microsoft.com/en-us/library/hh994574(v=ws.11).aspx.

4. Sajjad Amini, Hamed Mohsenian-Rad, and Fabio Pasqualetti. Dynamic load altering attacks

in smart grid. In Innovative Smart Grid Technol. Conf., pages 1â€“5. IEEE, 2015.

5. Alain Bensoussan, Murat Kantarcioglu, and SingRu Celine Hoe. A game-theoretical ap-
In Decision and Game

proach for ï¬nding optimal strategies in a botnet defense model.
Theory for Security, pages 135â€“148. Springer, 2010.

6. Tyler Byers. Demand response and the iot: Using data to maximize customer bene-
ï¬t, 2017. Comverge Blog, [Online]. Available: http://www.comverge.com/blog/february-
2017/demand-response-and-iot-using-data-to-maximize-cus/.

7. Vincent P Crawford and Joel Sobel. Strategic information transmission. Econometrica: J

Econometric Soc., pages 1431â€“1451, 1982.

8. D. Fudenberg and J. Tirole. Game Theory, volume 393. 1991.
9. J Duncan Glover, Mulukutla S Sarma, and Thomas Overbye. Power System Analysis &

Design, SI Version. Cengage Learning, 2012.

10. D. J. Hammerstrom. Part II. Grid friendly appliance project. In GridWise Testbed Demon-

stration Projects. Paciï¬c Northwest National Laboratory, 2007.

11. Yezekael Hayel and Quanyan Zhu. Epidemic protection over heterogeneous networks using
evolutionary poisson games. IEEE Trans. Inform. Forensics and Security, 12(8):1786â€“1800,
2017.

12. Ben Herzberg, Dima Bekerman, and Igal Zeifman. Breaking down mirai: An IoT DDoS
Incapsula Blog, Bots and DDoS, Security, 2016.
[Online] Available:

botnet analysis.
https://www.incapsula.com/blog/malware-analysis-mirai-ddos-botnet.html.

13. Kelly Jackson Higgins. Conï¬cker botnet â€™dead in the water,â€™ researcher says, 2010.
Dark Reading, [Online]. Available: http://www.darkreading.com/attacks-breaches/conï¬cker-
botnet-dead-in-the-water-researcher-says/d/d-id/1133327?

14. David Lewis. Convention: A philosophical study. John Wiley & Sons, 2008.
15. Robinson Meyer. How a bunch of hacked dvr machines took down twitter and reddit. The

Atlantic, October 2016.

16. Monireh Mohebbi Moghaddam and Quanyan Zhu. A game-theoretic analysis of deception
over social networks using fake avatars. In Decision and Game Theory for Security. Springer,
2016.

17. Amir-Hamed Mohsenian-Rad and Alberto Leon-Garcia. Distributed internet-based load al-
tering attacks against smart power grids. IEEE Trans. Smart Grid, 2(4):667â€“674, 2011.

18. Roger B Myerson. Population uncertainty and poisson games.

Int. J. Game Theory,

27(3):375â€“392, 1998.

19. Jeffrey Pawlick, Sadegh Farhang, and Quanyan Zhu. Flip the cloud: Cyber-physical signaling
In Decision and Game Theory for

games in the presence of advanced persistent threats.
Security, pages 289â€“308. Springer, 2015.

20. Jeffrey Pawlick and Quanyan Zhu. Deception by design: Evidence-based signaling games
for network defense. In Workshop on the Economics of Inform. Security and Privacy, Delft,
The Netherlands, 2015.

21. Jeffrey Pawlick and Quanyan Zhu. Strategic trust in cloud-enabled cyber-physical systems
with an application to glucose control. IEEE Trans Inform. Forensics and Security, To appear.
22. Richard J Radke, Tianna-Kaye Woodstock, MH Imam, Arthur C Sanderson, and Sandipan
Mishra. Advanced sensing and control in the smart conference room at the center for lighting
enabled systems and applications. In SID Symp. Digest of Tech. Papers, volume 47, pages
193â€“196. Wiley Online Library, 2016.

23. Aldert Vrij, Samantha A Mann, Ronald P Fisher, Sharon Leal, Rebecca Milne, and Ray
Bull. Increasing cognitive load to facilitate lie detection: The beneï¬t of recalling an event in
reverse order. Law and Human Behavior, 32(3):253â€“265, 2008.

24. Qishi Wu, Sajjan Shiva, Sankardas Roy, Charles Ellis, and Vivek Datla. On modeling and
simulation of game theory-based defense mechanisms against DoS and DDoS attacks. In
Proc. Spring Simulation Multiconf., page 159. Society for Comput. Simulation Intl., 2010.

