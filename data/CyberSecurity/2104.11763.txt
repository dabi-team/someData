Leveraging Sharing Communities to Achieve Federated Learning for
Cybersecurity∗

Frank W. Bentrem† , Michael A. Corsello† , and Joshua J. Palm†

Abstract. Automated cyber threat detection in computer networks is a major challenge in cybersecurity. The
cyber domain has inherent challenges that make traditional machine learning techniques problematic,
speciﬁcally the need to learn continually evolving attacks through global collaboration while main-
taining data privacy, and the varying resources available to network owners. We present a scheme to
mitigate these diﬃculties through an architectural approach using community model sharing with a
streaming analytic pipeline. Our streaming approach trains models incrementally as each log record
is processed, thereby adjusting to concept drift resulting from changing attacks. Further, we designed
a community sharing approach which federates learning through merging models without the need
to share sensitive cyber-log data. Finally, by standardizing data and Machine Learning processes in
a modular way, we provide network security operators the ability to manage cyber threat events and
model sensitivity through community member and analytic method weighting in ways that are best
suited for their available resources and data.

Key words. cybersecurity, federated learning, streaming machine learning

AMS subject classiﬁcations. 68Q85, 68T05, 68U01

1. Introduction. Computers have been subject to malicious attacks for many years. How-
ever, the sophistication and impacts of those attacks have grown dramatically. Successful,
timely detection of such attacks is essential to enable defenders to prevent or respond to an
attack to mitigate or minimize damage. The consequences from such attacks include substan-
tial ﬁnancial losses, leaked personal data, and the release of proprietary information or state
secrets. The immense human and computational resources required to analyze network logs is
prohibitive and often still misses novel attacks. As a result, there is a growing need to incorpo-
rate advanced techniques such as machine learning in the cyber defender’s toolbox. However,
the cyber domain presents key challenges to traditional batch machine learning techniques
[1, 2].

Problem 1.1 The continuous evolution of threat vectors requires continual learning.

Problem 1.2 The volume and sensitivity of network log data prevents long-term retention

and sharing across organizations.

Problem 1.3 Barriers-to-entry prevent small or more vulnerable organizations from adopt-

ing complex or expensive technology.

Problem 1.4 Feedback from network security operators is important to identify novel

attacks and mitigate as quickly as possible.

∗Submitted to the editors March 5, 2021; accepted for publication March 25, 2021; revised April 15, 2021.
Funding: This work was funded by the Defense Advanced Research Projects Agency under contract no. W911NF-

18-C-0019 through subcontract under the University of Virginia.

†Commonwealth Computer Research, Inc., Charlottesville, VA (frank.bentrem@ccri.com,

michael.corsello@ccri.com, joshua.palm@ccri.com, https://ccri.com).

1

1
2
0
2

r
p
A
7
2

]

R
C
.
s
c
[

2
v
3
6
7
1
1
.
4
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
2

F. W. BENTREM, M. A. CORSELLO, AND J. J. PALM

In this paper, we present solutions to these problems through an architecture that (1)
is streaming, (2) is federated, and (3) manages feedback. The paper is organized in the
following way. Our targeted constraints are in section 2, our system architecture is in section 3,
the implemented machine learning algorithms are in section 4, and the summary follows in
section 5.

2. Targeted Constraints. Individual organizations that train models to recognize cyber
threats are severely data limited, however, privacy concerns prevent them from directly sharing
raw network data logs. Even anonymized data may collectively reveal sensitive information.
We therefore propose a federated learning system [3] where trained standardized models are
shared across organizations through their parameters only without sharing any raw data. The
models are then merged in a way to improve performance compared to individually trained
models. Details on the sharing and merging of trained models are provided in section 4.

In order to entice a large group of organizations to participate in a collaborative learning
enterprise, we lower barriers-to-entry by minimizing the computational requirements regard-
ing platform, bandwidth, processing power, and storage. Our design choices were guided by
this constraint, where we preferred small algorithms, minimal sharing, and streaming process-
ing. Some modiﬁcations to the machine learning algorithms designed for traditional batch
processing are needed to enable streaming learning. (For a similar approach see[4].)

3. System Architecture. To achieve the simultaneous goals of supporting real-time ana-
lytics of streaming log data and ad hoc oﬄine analytics, we propose a simple directed acyclic
graph (DAG) computational model. This model assumes a well-deﬁned data model to fa-
cilitate “pluggable” analytics and multi-organizational data federation. The compute ﬂow is
based upon a small number of autonomous boxes that are connected in a speciﬁc, simple way
as illustrated in Figure 1.

Figure 1. Core computation DAG.

The ﬂow of data through the DAG ensures data at each stage is consistent and provides
guarantees of how, when and where data is both shared and integrated. Sharing occurs at
“stores” where sharable data is integrated across organizations, as shown in Figure 2, in
the live stream using well-known model combiners speciﬁc to each analytic shared. In this
way, the development of a new analytic involves the creation of a local training processor,
a model combining routine, and a prediction processor. Extending the sharing through the
use of communities will enable data to be aggregated across community members and then

LEVERAGING SHARING COMMUNITIES TO ACHIEVE . . .

3

Figure 2. Computation DAG extended with feedback stores where sharing occurs.

re-shared as the “community consensus” for further integration at member sites.

4. Machine Learning Algorithms. Finally, any number of prediction algorithms can be
plugged into the streaming framework along with methods to merge the separately trained
models. We built a prototype for our described system using three example classes of machine
learning algorithms, Example 4.1 (neural network), Example 4.2 (naive Bayes), and Example
4.3 (Random Forest), which illustrate how a large number of diﬀerent classiﬁcation algorithms
might be implemented. The input data for the machine learning algorithms were obtained
using existing raw HTTP data logs that were preprocessed to obtain 81-dimensional vectors
x (features), most of which were processed according to the method described by Oprea, et
al.[5] Limited malicious labels were derived from VirusTotal and FireEye™ where available,
and the host traﬃc ranking was used as a proxy for benign labels. The streaming federated
algorithm with scheduled model sharing is described in Algorithm 4.1.

Algorithm 4.1 Streaming federated algorithm
while not past time for scheduled sharing do

Receive HTTP record R
Preprocess to get input vector x
if R contains label information then

Extract labels y
Perform training iteration with x

else

Perform prediction on x

y

−→

end if
end while
Exchange model parameters with collaborators
Merge models
Repeat from top

4

F. W. BENTREM, M. A. CORSELLO, AND J. J. PALM

Example 4.1 (Neural Network) To maintain a small model size, for our neural network
model, we avoided deep learning and implemented a multilayer perceptron with ﬁve hidden
layers of size [64, 32, 16, 8, 4] and output size two for the malicious and benign scores. To
merge neural network models we perform a weighted average for the weights and biases of
each node in the network. So for each weight wi and bias bi, where i is the model index, we
obtain the merged weight w(cid:48) and bias b(cid:48) through the weighted averages

(4.1)

w(cid:48) =

a, w

(cid:104)

(cid:105)

and b(cid:48) =

a, b

,

(cid:105)

(cid:104)

where a is the (averaging) weights for the shared models with
the weights and biases for all shared models at a given node.

||1 = 1, and w and b are the
a

||

Example 4.2 (Naive Bayes) For the Naive Bayes algorithm, we generate two histograms
hk,i for each feature i, one histogram for the benign labels and one for the malicious labels,
hk,i||1 represent the cumulative numbers of benign
where k
||
and malicious records in the histograms. Then the benign and malicious likelihoods
Lk and
evidence

. Let Nk =
benign, malicious
}

for each feature i are given by

∈ {

E

(4.2)

Lk,i =

hk,i
Nk

and

Ei =

k hk,i
k Nk

,

(cid:80)
(cid:80)

respectively. The probabilities p(k
|
being benign (k = benign) or malicious (k = malicious) are given by

x) for a log record with feature vector x = (x1, x2, . . . , xn)

(4.3)

p(k

|

x) =

Nk
k Nk

n

(cid:89)i=1

Lk,i(xi)
Ei(xi)

,

(cid:80)

Lk,i(xi) are the likelihoods and

where
Ei(xi) is the evidence for the histogram bin corresponding
to the value xi of feature i. The proof for (4.3) is given in ??. Model merging is accomplished
by simply summing the histograms from the shared models.

Example 4.3 (Random Forest) As an example of an ensemble model, the Random Forest
algorithm implements some number m of constituent models that are combined for prediction.
The Random Forest algorithm is trained in the normal way. To merge the shared ensembles,
we select a sampling of constituent models from each shared ensemble so that we maintain m
models in the merged ensemble.

5. Summary. We presented an automated cyber threat detection system that leverages
sharing communities for collaborative, federated learning with a streaming architecture. The
computation DAG addresses the need for data privacy and low barriers-to-entry. We com-
pleted a prototype with streaming machine learning algorithms with the capability to merge
shared models. The results from our internal testing demonstrate the feasibility of our ap-
proach and the eﬀectiveness of sharing and merging shared models, however a detailed per-
formance analysis is contingent on more robust labeling.

Acknowledgments. We would like to acknowledge the University of Virginia PCORE-

CHASE team for their support.

LEVERAGING SHARING COMMUNITIES TO ACHIEVE . . .

5

REFERENCES

[1] G. Apruzzese, M. Colajanni, L. Ferretti, A. Guido, and M. Marchetti, On the eﬀectiveness of
machine and deep learning for cyber security, in 2018 10th international conference on cyber Conﬂict
(CyCon), IEEE, 2018, pp. 371–390.

[2] M. Conti, T. Dargahi, and A. Dehghantanha, Cyber threat intelligence: challenges and opportunities,

in Cyber Threat Intelligence, Springer, 2018, pp. 1–6.

[3] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T. Suresh, and D. Bacon, Federated
learning: Strategies for improving communication eﬃciency, arXiv preprint arXiv:1610.05492, (2016).
[4] J. Montiel, M. Halford, S. M. Mastelini, G. Bolmier, R. Sourty, R. Vaysse, A. Zouitine, H. M.
Gomes, J. Read, T. Abdessalem, and A. Bifet, River: machine learning for streaming data in
python, 2020, https://arxiv.org/abs/2012.04740.

[5] A. Oprea, Z. Li, R. Norris, and K. Bowers, Made: Security analytics for enterprise threat detection,

in Proceedings of the 34th Annual Computer Security Applications Conference, 2018, pp. 124–136.

SUPPLEMENTARYMATERIALS:LeveragingSharingCommunitiestoAchieveFederatedLearningforCybersecurity∗FrankW.Bentrem†,MichaelA.Corsello†,andJoshuaJ.Palm†SM1.ProofofNaiveBayesProbabilitiesinEquation(4.3).ForarefresheronBayes’Ruleandindependencesee[SM2],andageneraldiscussionontheNaiveBayesalgorithmcanbefoundinFriedman,etal.[SM1]Forsimplicity,weassumethateachfeatureinthefeaturevectorx=(x1,x2,...,xn)isstatisticallyindependentofeveryotherfeatureinx.RemarkSM1.1Whilethisassumptionisoftennotstrictlytrue,itisgenerallyfoundtoproducewell-performingclassiﬁcationalgorithmsinpractice.LetSbethesetofclassesfortheclassiﬁcationproblem,e.g.S={benign,malicious}.Foreachfeaturei,wegenerateahistogramhk,iwithidenticalbinsforeverylabelclassk∈S.LetNk=||hk,i||1representthecumulativenumberofrecordsineachclass.ThentheclasslikelihoodsLk,iandevidenceEiforeachfeatureiaregivenby(SM1.1)Lk,i=hk,iNk,andEi=Pkhk,iPkNk,respectively.ClaimSM1.1.Foralogrecordwithmutuallyindependentfeaturesinitsfeaturevectorx,theprobabilityp(k|x)ofitbelongingtoclasskisgivenby(SM1.2)p(k|x)=NkPkNknYi=1Lk,i(xi)Ei(xi)inthelimitNk→∞,whereLk,i(xi)arethelikelihoodsandEi(xi)istheevidenceforthehistogrambincorrespondingtothevaluexioffeaturei.Proof.FromBayes’Rulewiththeassumptionofindependence[SM1],theprobabilityofofalogrecordwithfeaturevectorxbelongingtoclassk,is(SM1.3)p(k|x1,...,xn)=1Zp(k)nYi=1p(xi|k),whereZ=p(x)=Pkp(k)p(x|k).Herep(k)isthepriorprobabilityandp(xi|k)isthelikelihood.Bydeﬁnition,inthelimitofalargenumberofrecords,p(k)=Nk/PkNk.Throughthesamelimit,thelikelihoodisthefractionofrecordsofaclassthatfallintoagiven∗SubmittedtotheeditorsMarch5,2021;acceptedforpublicationMarch25,2021;revisedApril15,2021.Funding:ThisworkwasfundedbytheDefenseAdvancedResearchProjectsAgencyundercontractno.W911NF-18-C-0019throughsubcontractundertheUniversityofVirginia.†CommonwealthComputerResearch,Inc.,Charlottesville,VA(frank.bentrem@ccri.com,michael.corsello@ccri.com,joshua.palm@ccri.com,https://ccri.com).SM1SM2F.W.BENTREM,M.A.CORSELLO,ANDJ.J.PALMhistogrambin,sothatp(xi|k)=hk,i(xi)/Nk=Lk,i(xi),wherehk,i(xi)isthehistogrambinvalueforthefeaturevaluexi.Equation(SM1.2)isthenequivalentto(SM1.4)p(k|x)=Z−1NkPkNknYi=1Lk(xi).Againundertheassumptionofmutualindependence,(SM1.5)p(x)=nYi=1p(xi),soZ=nYi=1Pkhk,i(xi)PkNk=nYi=1E(xi).Substituting(SM1.5)into(SM1.4)resultsin(SM1.6)p(k|x)=NkPkNknYi=1Lk(xi)E(xi),whichistheresultclaimed.REFERENCES[1]J.Friedman,T.Hastie,R.Tibshirani,etal.,Theelementsofstatisticallearning,SpringerseriesinstatisticsNewYork,2001.[2]L.MetcalfandW.Casey,Cybersecurityandappliedmathematics,Syngress,2016.