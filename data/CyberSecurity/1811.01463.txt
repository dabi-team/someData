Security for Machine Learning-based Systems: 
Attacks and Challenges during Training and Inference 

Faiq Khalid*, Muhammad Abdullah Hanif *, Semeen Rehman‚Ä†, Muhammad Shafique* 
*Institute of Computer Engineering, Vienna University of Technology (TU Wien), Austria 
‚Ä†Institute of Computer Technology, Vienna University of Technology (TU Wien), Austria 
{faiq.khalid; muhammad.hanif; semeen.rehman; muhammad.shafique}@tuwien.ac.at 

Abstract ‚Äî The exponential increase in dependencies between 
the cyber and physical world leads to an enormous amount of data 
which  must  be  efficiently  processed  and  stored.  Therefore, 
computing  paradigms  are  evolving  towards  machine  learning 
(ML)-based  systems  because  of  their  ability  to  efficiently  and 
accurately process the enormous amount of data.  Although ML-
based  solutions  address  the  efficient  computing  requirements  of 
big  data,  they  introduce  (new)  security  vulnerabilities  into  the 
systems,  which  cannot  be  addressed  by  traditional  monitoring-
based  security  measures.  Therefore,  this  paper  first  presents  a 
brief  overview  of  various  security  threats  in  machine  learning, 
their respective threat models and associated research challenges 
to  develop  robust  security  measures.  To  illustrate  the  security 
vulnerabilities of ML during training, inferencing and hardware 
implementation, we demonstrate some key security threats on ML 
using LeNet and  VGGNet for  MNIST and German Traffic Sign 
Recognition  Benchmarks  (GTSRB),  respectively.  Moreover, 
based on the security analysis of ML-training, we also propose an 
attack  that  has  a  very  less  impact  on  the  inference  accuracy. 
Towards the end, we highlight the associated research challenges 
in developing security measures and provide a brief overview of 
the techniques used to mitigate such security threats.   

Keywords‚ÄîMachine  Learning,  Neural  Networks,  Deep 
Learning,  DNNs,  Security,  Attacks,  Attack  Surface,  Autonomous 
Vehicle, Traffic Sign Detection.  

I. 

INTRODUCTION 

Due  to  the  exponential  growth  in  complex  integration  and 
interconnection  of  the  physical  and  cyber  domains  with  humans,  the 
number  of  connected  devices  increases  enormously.  Several  market 
analysts and corresponding surveys forecast that by 2025, the connected 
devices (20 billion in 2017) will surpass 75 billion [1][2][3], as shown 
in Fig. 1. Although these connected devices are revolutionizing several 
automation, 
like  healthcare, 
applications  domains 
autonomous  vehicles,  transport  systems  and  many  other  but  they 
generate  and  collect  an  enormous  amount  of  data,  for  example,  on 
average,  300  hours  of  video  content  is  uploaded  on  YouTube  every 
minute [4], 95 million [5] and 300 million [5] photos are uploaded daily 
on Instagram [4] and Facebook [4], respectively, as shown in Fig. 1. 
Moreover,  several  surveys  also  predict  that  by  2025,  this  data  is 
expected  to  surpass  the  160 Zettabyte  mark  and  by  2020,  for  every 
person  on  earth,  1.7  MB  data  will  be  generated  every  second  [6]. 
Therefore,  there  is  a  dire  need  to  efficiently  process  and  store  this 
enormous  amount of  data,  which  leads  to  the  following  fundamental 
research challenges: 

industrial 

1)  How to increase the computing capability to process this data in a 

highly energy-efficient manner?  

2)  How  to  increase  the  storing  capability  to  store  this  data  in 
interpretable form with minimum energy and area overhead?  

To address these research challenges, researchers have been exploring  

Fig. 1: Increasing trend of connected devices and corresponding measured 
data for processing [2] (Source for logos: google images). 

several  novel  computing  architectures,  methodologies,  frameworks, 
algorithms  and  tools.  However,  Machine  learning  (ML)  algorithms, 
especially  (deep)  neural  networks  have  emerged  as  one  of  the  most 
popular computing paradigms due to their ability to efficiently handle 
such gigantic amounts of data [3]. ML algorithms not only addressed 
the  processing  requirements  of  the  huge  data  but  they  have  also 
revolutionized several application domains, like smart cities, intelligent 
transportation  systems,  smart  grids,  autonomous  vehicles, healthcare, 
social  networks  (Facebook,  Twitter,  Youtube,  Instagram,  etc.)  and 
many  more,  as  shown  in  Fig.  2,  by  extracting  some  of  the  hidden 
features. 

Fig. 2: Applications of Machine Learning Algorithms (Source for images: 
google images). 

Unlike  the  traditional  computing  algorithms,  ML  algorithms 
dynamically change the computational flow with respect to the input 

How to efficientlyand securelyprocess and store this enormous amount of data while ensuring the privacy?6.807.007.207.407.607.808.008.208.400102030405060708020152016201720182019202020212022202320242025World Population (Billion)Connected Devices (Billions)By 2025, there will be 75 Billionof connected devices6.807.007.207.407.607.808.008.208.4002040608010012014016018020152016201720182019202020212022202320242025World Population (Billion)Generated Data (Zettabytes)By 2025, there will be 160 Zettabyte of measured dataNumber of connected devicesGenerated/measured dataWorld PopulationBy 2025, there will be 4,785interactions per connected person in one day [6]By 2020, for every person on earth, 1.7 MB data will be generated every second [6]0.4 million hours of video uploaded daily300 million photos uploaded daily95 millionphotos uploaded daily5.5 billion searches daily1.6 million packets ships daily.Sonar~10-100KB/secAutonomous Vehicles 4,000 GB per day (Intel) [7]Radar~10-100KB/secGPS~50KB/secLiDAR~10-70MB/secCamera~20-40MB/sec25 Billion daily forecast requests.Autonomous DrivingCancer DetectionNatural Language ProcessingImage ClassificationObject Detection & LocalizationMachine TranslationStrategy gamesForex/Stocks Trading 
 
 
data,  which  increases  the  energy  overhead.  Moreover,  due  to  the 
unpredictability of the computing in hidden layers of neural networks, 
these algorithms possess several security vulnerabilities which result in 
increased system vulnerability towards security threats [8]-[11]. Some 
example  are:  Amazon echo  hacking [8], Facebook  chatbots  [8], self-
driving bus crashes (on its very first day in Las Vegas) [13]. These real-
world  incidents  highlight  the  security  risks  involved  in  ML-based 
systems and raise the fundamental research question: How to securely 
train and implement ML algorithms?  

To  address  this  question,  a  comprehensive  set  of  vulnerability 
analyses  and  countermeasures  is  required.  Several  techniques  for 
security  vulnerability  analysis  and  countermeasures  have  been 
proposed  based  on  the  manufacturing/design  cycle  of  ML-based 
systems which is composed of training, hardware implementation of 
trained  model  (e.g.,  LeNet  and  VGGNet  trained  for  MNIST  and 
GTSRB  dataset)  and  inference.  Since,  each  stage  possesses  its  own 
security vulnerabilities, their corresponding methodologies for security 
analysis  and  countermeasures  have  different  impact  on  ML-based 
systems.  For  example,  traffic  sign  detection  in  autonomous  vehicles 
requires  a  sophisticated  and powerful pre-processing  methodology to 
remove  random  environmental  uncertainties  [14].  However,  face 
detection  inside  a  highly  secure  building  requires  a  relatively  less 
powerful  preprocessing  methodology  because  of  the  controlled 
environment.  

A.  Our Contributions 

To address these fundamental security challenges in ML, this paper 

makes the following contributions:  

1)  A brief yet comprehensive overview of the machine learning security 
including various security threats at different stages (Section II.B) 
and respective threat models (Section II.A). 

2)  Associated research challenges to develop robust security measures 

for security threats at different stages of machine learning. 

3)  A  comprehensive  experimental  analysis  of  different  security 
vulnerabilities and potential threats during training and inference.   
4)  A  brief  discussion  on  the  possible  countermeasures  for  security 

vulnerabilities in machine learning.   

II.  SECURITY  FOR MACHINE LEARNING 

To  provide  a  better  understanding  of  the  security  for  ML-based 
systems,  in  this  section,  we  provide  a  comprehensive  overview  of 
possible threat models and associated  security attacks/vulnerabilities 
during  the  different  stages  of  the  design/manufacturing  cycle  of  the 
ML-based systems.    

A.  Threat Models 

to 

identify 

the  potential  threat  factors, 

To develop security measures for ML-based systems, the foremost 
step  is 
i.e.,  attacker, 
design/manufacturing stage, attack mechanism and intention of attack. 
Therefore, a precise threat model should be defined, which provides the 
information  about  the  capabilities  and  goals  of  an  attacker  under 
realistic assumptions. Hence, first we provide a brief overview of the 
manufacturing cycle of ML-based applications/systems.  

The manufacturing/design cycle is defined as all the possible steps 
which are involved in training, testing and deployment of the ML-based 
application/systems, as shown in Fig. 3 [15][17]. Based on the different 
resource  requirements  and  potential  application  users,  the  following 
actors are part of the manufacturing/design cycle. 

1)  3rd  Party  (3P)  Cloud  Platforms:  If  the  IP  providers  or 
manufacturers do not have enough  computing resources  to  fulfill 
the  requirements  of the larger datasets  and neural networks,  e.g., 
ResNet  [16],  then  3rd  party  (3P)  cloud  platforms  are  used  [17]. 
However,  it  comes  with  security  vulnerabilities,  i.e.,  IP  stealing, 

manipulation  of  the  training  dataset  and  models/architectures. 
Therefore,  cloud  platforms  can  be  declared  as  untrusted  in  the 
following two different cases. 
a)  If the cloud platform provider is untrusted, it can manipulate the 
training dataset and baseline neural networks or ML algorithms.  
b)  Even if the cloud platform provider is trusted, a man-in-middle 
[18] attack can be performed by another client to steal the IP, 
i.e., the trained network or even to manipulate the IP or affect 
the training process.  

2)  IP Providers: The other actor in the manufacturing cycle is the IP 
provider  which  can  also  be  untrusted  because  it  can  poison  the 
training  datasets  and  can  also  manipulate  baseline  ML 
models/architectures  or  other  hyper-parameters,  which  are  not 
accessible to 3P cloud providers. 

3)  Manufacturers:  If  the  manufacturers  are  untrusted  then  the 

following security vulnerabilities can be introduced:  
a)  Malicious hardware during the hardware implementation.  
b)  Side-channel (SC) attacks to steal the IP which can be a trained 

ML algorithm or a dataset. 

c)  Trained algorithm by performing local training.  

4)  Users:  Even  after  the  deployment  of  the  ML-based  systems,  the 

following security vulnerabilities can still be exploited: 
a)  Users can perform  side-channel attacks  to  extract  the IP, i.e., 

trained ML algorithm.  

b)  During inference, an attacker can also compromise the security 
of the ML-based system by manipulating the inference data or 
their corresponding hardware.   

Fig.  3:  Security  threats  with  respect  to  different  actors  involved  in  the 
manufacturing cycle of ML-based Applications. 

Thus,  based  on  the  trustworthiness  of  all  actors  involved  in  the 
design/manufacturing cycle, there can be 15 possible threat models for 
ML-based systems. For instance, if any of the above-mentioned factors 
are untrusted then it can be considered as a potential threat model. 

B.  Security Threats 
  Based on the above-mentioned threat models, each actor can exploit 
or introduce security vulnerabilities in the ML-based system. Therefore, 
for  developing  security  measures,  the  next  step  after  defining  the 
possible threat model is to identify the potential threats, their activation 
methodologies  (i.e.,  data  manipulation,  malicious  hardware  and 
software  intrusions)  and  corresponding  payloads  (i.e.,  confidence 
targeted 
in  classification), 
reduction 
misclassification). Therefore, with respect to the manufacturing cycle, 
the following possible security threats can be identified.  
1)  Training:  During  training,  an  attacker  can  poison  the  training 
dataset and can also manipulate the tools/architecture/model [15], 
e.g., adding parallel layers or neurons, to perform security attacks, 
as shown in Fig. 4. However, in case of outsourced training, remote 
side-channel or cyber-attacks can be used to steal the IPs.   

random  or 

(ambiguity 

Trained DNNSecurity Threats‚ùëData Poisoning‚ùëModel Modification‚ùëIP StealingTraining3P Cloud PlatformsSecurity Threats‚ùëInference Data Poisoning‚ùëSide-channel attacks‚ùëHardware IntrusionEnd User‚Ä¢Face Recognition ‚Ä¢Traffic Sign Detection ‚Ä¢Character RecognitionSecurity Threats‚ùëData Poisoning‚ùëModel Modification‚ùëIP Stealing‚ùëHardware Intrusion‚ùëSide-channel attacksDataset GenerationManufacturerReal-time testingReal-time DataValidationDataset Splitting DNN (structure, Type, Initialization )Trained DNNSecurity Threats‚ùëData Poisoning‚ùëModel Modification‚ùëIP Stealing‚ùëHardware IntrusionIP ProviderValidation DatasetTraining Dataset 
 
 
(i.e., 10), the Top1 error of the LeNet is 0.8% which is not acceptable 
in testing the LeNet.  

2)  Hardware  Implementation:  Similarly,  manipulation  of  the 
hardware  implementation  of  the  trained  ML  model  (hardware 
Trojans) and IP stealing (i.e., side-channel, remote cyber-attacks) 
can be performed at hardware level [15], as shown in Fig. 4.  
3)  Inference: In this phase, there can be the following types of attacks: 
a)  The user can attack (i.e., side-channel, remote cyber-attacks) the 

deployed ML-based system to steal the IP.  

b)  Other possible attackers can be in-direct beneficiaries who can 
either  manipulate  the  inference  data  or  intrude  the  hardware 
[19]. Moreover, attackers can also perform side-channel attacks 
for IP stealing (see Fig. 4). 

Fig. 5: Impact of noise intensity (Salt & Pepper and Gaussian) on LeNet 
for MNIST dataset with 1% to 40% intruded number of samples. 

Therefore, to incorporate the effect on inference accuracy, we propose 
an attack which does not intrude the dataset, but it extends the dataset 
with  certain  malicious  samples,  as  shown  inFig.  6.  To  illustrate  the 
effectiveness  of  this  attack,  we  implement  this  attack  on  LeNet  and 
VGGNet for MNIST and GTSRB datasets, respectively, and compare 
it with traditional dataset poisoning attacks. 

Fig.  4:  An  Overview  of  Security  Threats/Attacks  and  their  respective 
payloads  for  Machine  Learning  Algorithms  during  Training,  Inference, 
and their respective Hardware Implementations [15]. 

III.  SECURITY ANALYSIS OF ML-BASED SYSTEMS 

To  illustrate  the  significance  of  security  vulnerabilities  in  ML-
based  systems,  we  present  a  detailed  analysis  of  some  of  the  most 
common  security  vulnerabilities  in  ML,  i.e.,  data  poisoning  (during 
training) and adversarial examples (during inference).  

A.  Training Attacks 

Training is one of the fundamental steps in developing ML-based 
systems and requires a lot of computational resources that encourages 
outsourced  training  on  3P  cloud  platforms.  However,  outsourcing 
comes  with  security  vulnerabilities,  i.e.,  data  poisoning,  IP  (training 
data  or  trained  model)  stealing  attacks  or  intrusions  in  baseline  ML 
models/algorithms.  

1)  Random Misclassification Attack  

To demonstrate the security vulnerabilities during the training, we 
demonstrate a random misclassification attack through data poisoning 
in the MNIST [20] dataset during the training of LeNet [21] and in the 
German Traffic Sign Recognition Benchmarks (GTSRB) dataset [23] 
during the training of VGGNet [22], respectively. However, to perform 
the  data  poisoning  attack  on  MNIST  and  GTSRB,  the  following 
research challenges need to be addressed: 

1)  What is the optimal intensity of data poisoning (noise) to perform 
random  misclassification  while  maintaining  the  testing/targeted 
accuracy? 

2)  What is the optimal number of intruded samples? 

To address the above-mentioned challenges, we analyzed LeNet for 
MNIST dataset with different number of intruded samples (1% to 40%) 
and with different noise types and intensities (i.e.,  salt & pepper and 
Gaussian noise), as shown in Fig. 5. The analysis shows that even with 
1% (700/70000) of intruded samples and minimum salt & pepper noise 

Fig.  6:  Our  experimental  setup  for  random  misclassification  attacks  on 
LeNet and VGGNet for MNIST and GSRB dataset. 

LeNet with MNIST: First, we demonstrate the proposed attack (Attack 
2 in Fig. 6) by training LeNet with only 3% (2100) intruded samples 
that  are  appended  with  MNIST.  Similarly,  we  also  performed  the 
traditional attack (Attack 1 in Fig. 6) in which LeNet was trained on 
MNIST with similar number of intruded samples (3%, i.e., 2100). To 
demonstrate the effectiveness of these attacks, we perform the inference 
on intruded and un-intruded LeNets, as shown in Fig. 7. The analysis 
of these attacks shows that in both attacks the intruded LeNet randomly 
misclassifies the poisoned data sample (with label 0 as 8). However, the 
effect  on  inference  accuracy  of  the  proposed attack (1.3%) is  less  as 
compared to the traditional data poisoning attack (1.8%), as shown in 
Fig. 7.   

Training Dataset ValidationDataset Data Acquisition Training ValidationInference DataOutsourced TrainingPASSFAILSecurity Threats‚ùëTraining Data Poisoning‚ùëParallel Layers/ModelTrainingAttacker‚Äô Goals: Confidence Reduction, Random Misclassification, Targeted Misclassification, IP StealingSecurity Threats‚ùëSide Channel Attacks‚ùëHardware IntrusionsHardware ImplementationSecurity Threats‚ùëInference Data Poisoning‚ùëSide Channel Attacks‚ùëHardware IntrusionsInference/RuntimeValidatedModelHW DesignValidatedDesignClassification1011011101101111101101MemoryData inData outPower SupplyCloud Attacks+‚ûîGaussian NoiseSalt and Pepper20400123451234520400)3-10x Error (1 Top78910113-10x 8.010.58.59.510.011.03-10x 8.010.58.59.510.011.011.5)3-10x Error (1 Top789101112T = T1 + T2T2T1T2‚ÄôT2‚ÄôT1T2T1 + T2 + T2‚ÄôT1 +T2‚ÄôAttack 1 Trained Model17Attack 2 TrainingInferencingT = T1 + T2Trained Model17TrainingInferencingT2‚ÄôT1T2T1 +T2‚ÄôT = T1 + T2Trained ModelTrainingInferencingT = T1 + T2T2T1T2‚ÄôT1 + T2 + T2‚ÄôTrained ModelTrainingInferencing 
 
 
 
  
 
VGGNet with GTSRB:  Similarly, we also performed the proposed 
training data poisoning attack (Attack 2 in Fig. 6) and traditional attack 
(Attack 1 in Fig. 6) on VGGNet with GTSRB, as shown in  Fig. 10. 
The experimental analysis shows that the impact of the proposed attack 
on inference accuracy is 20% less than the traditional attacks, as shown 
by the output distribution of 4% Salt & pepper noise in Fig. 10. 

analysis, we consider a threat model in which an attacker has the access 
to the dataset (images) right after the camera, as shown in Fig. 8. 

Fig.  8:  Threat  model  and  experimental  setup  for  inference  attacks,  i.e., 
adversarial examples. 

1)  Adversarial Examples 

Based on the threat model (Fig. 8), there can be several possible 
attacks. However, one of the most common attacks is to generate the 
adversarial examples [27][28]. The key goal of any adversarial example 
is to add an imperceptible noise into the data (images) that can force the 
ML algorithm to misclassification. To achieve this goal, an adversarial 
example  typically  follows  the  two-step  methodology,  as  discussed 
below and shown in Fig. 9. 

1)  In the first step, an attacker chooses the target image/images or target 
output  class/classes  (in  case  of  targeted  misclassification)  and 
defines the optimization goals, i.e., correlation coefficients, accuracy 
or other parameters to analyze imperceptibility. 

2)  In the second step, a random noise is introduced in the target image 
to compute the imperceptibility based on the defined optimization 
goals.  If  optimal  imperceptibility  is  achieved,  then  the  intruded 
image is considered as an adversarial image; otherwise, the noise is 
updated based on imperceptibility parameters  and a new image is 
generated.  

Fig. 9: Basic methodology to generate an adversarial example. 

To  analyze  the  impact  of  adversarial  examples  on  inference,  in  this 
paper,  we  demonstrate  two  of  the  most  commonly  used  adversarial 
attacks from the open-source Cleverhans library [29][30], i.e., L-BFGS 
Method [24][25] and Fast Gradient Sign Method (FSGM) [26][27]. 

Fig.  7:  Random  misclassification  attack  (i.e.,  introducing  Salt  &  Pepper 
noise  in  3%  data  samples  of  the  MNIST  dataset)  on  LeNet  during  the 
training phase. 

Based on the experimental analysis, we can conclude that randomly 
introduced noise in the training samples can be destructive because it 
reduces the confidence or misclassifies the input data. For example, the 
collision  avoidance  in  autonomous  vehicles  can  be  fooled  by 
performing the random misclassification which leads to accidents. 

B.    Inference Attacks 
In  the  development/manufacturing  cycle  of  ML-based  systems,  like 
the traditional systems,  the inference stages of  ML algorithms come 
with  security  vulnerabilities,  i.e.,  manipulation  of  data  acquisition 
block,  communication  channels  and  side-channel  analysis 
to 
manipulate  the  inference  data  and  leaking  IP  (inference  data  and 
trained model). Remote cyber-attacks and side-channel attacks come 
with high computational costs and are therefore less frequently used.   
Consequently,  to  illustrate  the  impact  of  security  vulnerabilities  on 
inference,  we  demonstrate  a  couple  of  inference  data  poisoning 
attacks, 
i.e.,  adversarial  examples  (Limited-memory  Broyden‚Äì
Fletcher‚ÄìGoldfarb‚ÄìShanno  (L-BFGS)  Method  [24][25]  and  Fast 
Gradient  Sign  Method  (FSGM)  [26][27]).  In  this  experimental 

Fig. 10: Random misclassification attack, i.e., introducing the salt and pepper noise in German Traffic Sign Recognition Benchmarks (GTSR).

Original Labels081725,338,647,15661,8,579,186,991,7,2Security Attack: Random Misclassification during Training# of Total Samples: 70000; # Intruded Samples: 3%; Intrusion: Salt & Pepper Noise, Dataset: MNIST; Network: LeNet; Accuracy Loss: 1.8% (Attack1), 1.3% (Attack 2)Intruded Labels98.798.798.298.298.298.797.898.298.297.8Class Precision (%)Attack 1Attack 2081725,33847,15661,8,579,186,091,799.299.298.999.298.999.298.798.998.998.9L1L2L3L4L5L6Convolutional LayersFC LayersInput Label = StopOutputLabel = Speed Limit (60km/h)Modified ImageTarget ImageIntruded ImageNoiseAdversarial ExamplesOptimizations w.r.tAttack techniques99.470.3150.1580.020.0016020406080100ABCDE92.1251.941.3361.772.6641020406080100ABCDE44.62313.658.177.2625.63401020304050ABCDE87.6122.093.11.895.023020406080100ABCDE78.263.674.581.6811.256020406080100ABCDENo Noise1%Salt & Pepper2%Salt & Pepper3%Salt & Pepper4%Salt & PepperNoiseTraining Data PoisoningAttack 1 Intrude the certain number of samples in training datasetAttack 2 Append the certain number of  intruded samples in training dataset99.470.3150.1580.020.0016020406080100ABCDE95.4981.040.681.071.56020406080100ABCDE92.6121.032.61.092.5020406080100ABCDE86.262.973.151.0546.325020406080100ABCDE68.6238.542.055.1415.273020406080100ABCDEA: StopB: YieldC: Dangerous Curve to the RightD:Priority RoadE: Speed Limit (60km/h)Top5 ClassesDataset: German Traffic Sign Recognition BenchmarksNetwork: VGGNetNoise:Salt & Pepper# of Samples: 50000 # of Classes: 40Experimental Setup 
 
 
 
 
basic principle of the L-BFGS method is to achieve the optimization 
goal as defined in Equation 1. 

5)  How to validate the correctness and fairness of the ML hardware 

implementation of the trained ML model? 

ùëÄùëñùëõùëñùëöùëñùëßùëí ‚Äñùëõùëúùëñùë†ùëí‚Äñ2   ‚Üí  ùëì(ùë• + ùëõùëúùëñùë†ùëí)   ‚â†  ùëì(ùë•)   (1) 

  Where,  noise  represents  the  perturbations  and  minimizing  it 
represents  its  imperception.  To  illustrate  the  effectiveness  of  this 
method,  we  demonstrated  this  attack  on  the  VGGNet  trained  on  the 
GTSRB, as shown in Fig. 11. This experimental analysis shows that by 
introducing adversarial noise to the image, the input is misclassified, 
i.e., from a stop sign to speed limit 60km/h.  

Fig. 11: An adversarial image generated by L-BFGS Method. 

Fast Gradient Sign Method (FSGM): Although the L-BFGS method 
generates  adversarial  examples  with imperceptible noise,  it utilizes  a 
basic linear search algorithm to update the noise for optimization which 
makes  it  computationally  expensive  and  slow  [26].  Therefore, 
Goodfellow et al. proposed a Fast Gradient Sign Method to generate 
adversarial examples which is faster and requires less computations as 
it performs one step gradient update along the direction of the sign of 
gradient at each pixel [27]. Their proposed imperceptive noise can be 
defined as:    

ùúÇ  =  ùúñ ùõªùë• ùêΩ(ùúÉ, ùë•, ùëì)  

 (2) 

  Where,  ùúñ  and  ùúÇ  are  the  magnitude  of  the  perturbation  and  the 
imperceptible  noise,  respectively.  J  is  the  cost  minimizing  function 
(based  on  original  image  x,  classification  function  f  and  cost  with 
respect to target class ùúÉ) obtained through stochastic gradient descent. 
The generated adversarial example can be computed by adding ùúÇ into 
the targeted image. To analyze this vulnerability, we demonstrated this 
attack on VGGNet trained on GTSRB, as shown in Fig. 12.  

Fig. 12: An adversarial image generated by Fast Gradient Sign Method. In 
this  experiment,  we  assume  the  value  for  ‚Äú ùùê ‚Äù  is  0.007  to  make  it 
imperceptible, as mentioned in [27]. 

IV.  RESEARCH CHALLENGES FOR DESIGN AND DEVELOPMENT 
OF ML-BASED SYSTEM/APPLICATIONS 

Based  on  the  above-mentioned  security  threats/vulnerabilities 
(Section II.B), the following research challenges need to be addressed 
for developing secure/robust ML-based systems:  

1)  How to securely generate the training dataset and ensure its privacy 

(especially, in outsourced design/implementation)?  

2)  How to obfuscate the training dataset and hyper-parameters of the 
underlying ML algorithm to ensure privacy during the outsourced 
training period? 

3)  How  to  ensure  the  security  of  the  data  acquisition  during  the 

inference stage?  

4)  How to ensure the security of the pre-processing, ML algorithm and 

post processing hardware implementation? 

6)  How  to  protect  and  obfuscate  the  IPs  (i.e.,  trained  ML  model, 
dataset) from IP stealing  attacks,  i.e., side channel, remote cyber-
attacks, shared cache attacks? 

7)  How to securely execute ML algorithms on third-party hardware 

accelerators? 

V.  COUNTERMEASURES FOR SECURITY VULNERABILITIES IN 
MACHINE LEARNING  

To address the above-mentioned challenges (Section IV), several 
countermeasures have been proposed, i.e., interactive proof (SafetyNets 
[33][34]), privacy-preserving predictions (SecureML [31], DeepNano 
[32]), encryption of dataset and trained ML models (CryptoNets [36]), 
classifier protections etc. In this section, we briefly discuss the several 
possible countermeasures.  

Table 1 shows the summary of the potential security vulnerabilities 
and their corresponding countermeasures. Based on the manufacturing 
cycles, these countermeasures can be classified into the following three 
categories: 

Table 1: Summary of the potential security attacks and respective possible 
countermeasures with respect to manufacturing/design cycle. 

Security 
Vulnerabilities 

Training Data 
Manipulation 

Cloud IP 
Stealing 
Attacks 

Hardware 
Trojans 

SC- IP Stealing 

Cyber-Attacks 
for IP Stealing 
Inference Data 
Manipulation  

HW/SW IP 
stealing 
Attacks 

i

g
n
n
i
a
r
T

n
o
i
t
a
t
n
e
m
e
l
p
m

I

W
H

e
c
n
e
r
e
f
n
I

Potential Countermeasures 

‚Ä¢  Data Encryption, 
‚Ä¢  Redundant Outsourced Training 
‚Ä¢  Transfer  Learning  (local  training  of  trained 
model which is obtained by outsourced training) 

‚Ä¢  Data Encryption 
‚Ä¢  Baseline ML-Model Obfuscation 
‚Ä¢  HW/SW Side-channel Analysis 
‚Ä¢  Online communication monitors 
‚Ä¢  Cyber Security Measures 
‚Ä¢  Online Monitoring 
‚Ä¢  HW/SW Side-channel Analysis 
‚Ä¢  Formal Verification and validation of hardware 

implementation 

‚Ä¢  Online property checkers 
‚Ä¢  HW/SW Side-channel Analysis 
‚Ä¢  SC parameter-based runtime monitoring setup 
‚Ä¢  Online property checkers 
‚Ä¢  Online communication monitors 
‚Ä¢  Cyber Security Measures 
‚Ä¢  Data Encryption 
‚Ä¢  Sophisticated Pre-processing 
‚Ä¢  Online communication monitors 
‚Ä¢  Cyber Security Measures 
‚Ä¢  HW/SW Side-channel Analysis 
‚Ä¢  SC parameter-based runtime monitoring setup 

there  are 

1)  Training:  Depending  upon  the  targeted  vulnerabilities  and 
several  possible 

corresponding  methodologies, 
countermeasures:  
a)  Encryption:  In  this  approach,  the  training  dataset  set  is 
encrypted  before  training  (local/outsource),  i.e.,  cryptoNets 
[36][37][38].  However,  this  countermeasure  comes  with 
additional hardware for encrypting the inference dataset.  
b)  Transfer  Learning-based  Local  Training:  To  mitigate  the 
manipulations  of  weight/model,  the  dataset  is  split  into  two 
parts, one for outsourced training and other one is to locally train 
the  model,  which  can  be  used  to  overwrite  the  outsourced 
trained  weights/model  by  performing  the  transfer  learning 
[39][40].  

c)  Redundant  Training:  To  mitigate  the  manipulation  of  trained 
model/data,  the  training  is  outsourced  to  multiple  3P  cloud 

Stop SignConfidence: 99.47%  Adversarial Noise by L-BFGS Speed Limit (60km/h)Confidence: 85.68%  =+Stop SignConfidence: 99.47%  Adversarial Noise by FGSMSpeed Limit (60km/h)Confidence: 92.31%  + 0.007 x = 
 
 
 
 
 
 
 
platforms. Triple/multi-modular redundancy is used to identify 
the intrusions while testing [41].  

2)  Hardware  Implementation:  Hardware  intrusions  in  ML-based 
systems  are  similar  to  traditional  hardware-attacks,  therefore, 
typical hardware  security techniques  can be  applied, i.e.,  runtime 
anomaly detection using side-channel and communication analysis, 
formal  method-based  analysis 
traditional 
obfuscation techniques to mitigate hardware IP stealing. 

([42]-[44]),  and 

3)  Inference:  Similarly,  traditional  obfuscation  techniques,  runtime 
anomaly detection, side channel analysis and security measures for 
remote cyber-attacks [45] can be applied to mitigate the IP stealing. 
data, trained model and hardware manipulation. Development and 
selection of  appropriate  security  measures  for  inference is a  very 
complex  and tedious process because of  the system‚Äôs energy and 
design constraints, especially in battery operated components of a 
CPS  [46].  For  example,  to  avoid  the  data  poisoning  attacks, 
encryption  is  one  of  the  possible  countermeasures,  but  due  to 
limited energy resources applicability of this is limited.  

VI.  CONCLUSION 

In  this  paper,  we  first  discuss  and  identify  the  possible  security 
threats  in  machine  learning  with  respect  to  threat  models,  attack 
methodologies  and  payloads.  Moreover,  to  analyze  the  security 
vulnerabilities  for  identifying  the  potential  countermeasures,  we 
demonstrate some of the security threats (Training data poisoning and 
adversrial  examples  (L-BFGS  and  FSGM))  on  the  LeNet  and  the 
VGGNet  for  the  MNIST  and  the  German  Traffic  Sign  Recognition 
Benchmarks (GTSRB), respectively. We also propose a training data 
poisoning attack which has relatively less impact on inference accuracy. 
Finally,  we  provide  an  overview  of  possible  security  measures  and 
highlight  respective  research  challenges  in  developing  these  security 
measures.   

[1]  Robert  E  Stroud  CGEIT  CRISC,  ‚ÄúIoT  Opportunities,  Trends,  and  Momentum‚Äù, 

REFERENCES 

[2] 

Forrester, 2017. 
Statista, ‚ÄúInternet of Things (IoT) connected devices installed base worldwide from 
[Online] 
billions)‚Äù, 
2015 
https://www.statista.com/statistics/471264/iot-number-of-connected-devices-
worldwide/ 

2018, 

2025 

June 

(in 

to 

[3]  R. F, Babiceanu et al. ‚ÄúBig Data and virtualization for manufacturing cyber-physical 
systems:  A  survey  of  the  current  status  and  future  outlook‚Äù,  in  Computers  in 
Industry, 81,2016, pp. 128-137. 
‚ÄúSocial Media Statistics 2018‚Äù https://dustn.tv/social-media-statistics/  

[4] 
[5]  Marchop, https://merchdope.com/youtube-statistics/  
[6]  Domo, ‚ÄúData Never Sleep 6.0‚Äù, https://www.domo.com/learn/data-never-sleeps-6  
[7] 
P.  Nelson,  ‚ÄúJust  one  autonomous  car  will  use  4,000  GB  of  data/day‚Äù,  Network 
world, 
2018 
https://www.networkworld.com/article/3147892/internet/one-autonomous-car-
will-use-4000-gb-of-dataday.html  

[online], 

July 

31st 

[8]  Melis, Marco, et al. "Is deep  learning safe for robot  vision? adversarial examples 

[9] 

against the icub humanoid." arXiv preprint arXiv:1708.06939 (2017). 
Papernot, Nicolas, et al. "The limitations of deep learning in adversarial settings." 
Security and Privacy (EuroS&P), IEEE European Symposium on. IEEE, 2016. 
[10]  Zhang,  Xinyang  et  al.  "Modular  Learning  Component  Attacks:  Today's  Reality, 

Tomorrow's Challenge." arXiv preprint arXiv:1708.07807 (2017). 

[11]  Papernot, Nicolas, et al. "Practical black-box attacks against machine learning." Asia 

[16]  He,  K.,  Zhang,  X.,  Ren,  S.  and  Sun,  J.,  2016.  Deep  residual  learning  for  image 

recognition. IEEE CVPR 2018, pp. 770-778. 

[17]  F.  Kriebel  et  al., ‚ÄúRobustness  for  Smart  Cyber  Physical  Systems  and  Internet-of-
Things: From Adaptive Methods to Reliability and Security for Machine Learning‚Äù, 
ISVLSI 2018, pp. 581-586. 

[18]  Umamaheshwari,  S.,  and  J.  N.  Swaminathan.  "Man-In-Middle  Attack/for  a  Free 

Scale Topology." IEEE ICCCI 2018, pp. 1-4.  

[19]  Salem, Ahmed, Yang Zhang, Mathias Humbert, Mario Fritz, and Michael Backes. 
"ML-Leaks:  Model  and  Data  Independent  Membership  Inference  Attacks  and 
Defenses on Machine Learning Models." arXiv preprint arXiv:1806.01246 (2018). 

[20]  LeCun, Yann et al., http://yann.lecun.com/exdb/mnist/, May 2018. 
[21]  LeCun,  Yann.  "LeNet-5,  convolutional  neural  networks."  URL:  http://yann. 

lecun.com/exdb/lenet (2015): 20. 

[22]  Wang,  L.,  Guo,  S.,  Huang,  W.,  &  Qiao, Y.  (2015).  Places205-vggnet  models  for 

scene recognition. arXiv preprint arXiv:1508.01667. 

[23]  J. Stallkamp et al., "The German traffic sign recognition benchmark: a multi-class 

classification competition." IJCNN, pp. 1453-1460. IEEE, 2011. 

[24]  Szegedy,  C.,  Zaremba,  W.,  Sutskever,  I., Bruna,  J.,  Erhan,  D.,  Goodfellow,  I.,  & 
Fergus,  R.  (2013).  Intriguing  properties  of  neural  networks.  arXiv  preprint 
arXiv:1312.6199. 

[25]  Carlini,  N.,  &  Wagner,  D.  (2016).  Towards  evaluating  the  robustness  of  neural 

networks. arXiv preprint arXiv:1608.04644. 

[26]  Ashok, Anubhav, Sree Harsha Kalli, and Prachee Sharma. "Fooling a Deep Neural 

Network Forever." 

[27]  Grosse, Kathrin, et al. "On the (statistical) detection of adversarial examples." arXiv 

preprint arXiv:1702.06280 (2017). 

[28]  Huang,  L.,  Joseph,  A.  D.,  Nelson,  B.,  Rubinstein,  B.  I.,  &  Tygar,  J.  D.  (2011, 
October). Adversarial machine learning. In Proceedings of the 4th ACM workshop 
on Security and artificial intelligence (pp. 43-58). ACM. 

[29]  Goodfellow, Ian, et al. "cleverhans v0. 1: an adversarial machine learning library." 

arXiv preprint (2016). 

[30]  Papernot, N., Carlini, N., Goodfellow, I., Feinman, R., Faghri, F., Matyasko, A., ... 
&  Garg,  A.  (2016).  cleverhans  v2.  0.0:  an  adversarial  machine  learning  library. 
arXiv preprint arXiv:1610.00768. 

[31]  Mohassel, Payman, and Yupeng Zhang. "SecureML: A system for scalable privacy-
preserving machine learning." 2017 38th IEEE Symposium on Security and Privacy 
(SP). IEEE, 2017. 

[32]  Bo≈æa,  Vladim√≠r,  Bro≈àa  Brejov√°,  and  Tom√°≈°  Vina≈ô.  "DeepNano:  deep  recurrent 
neural networks for base calling in MinION nanopore reads." PloS one 12.6 (2017): 
e0178751. 

[33]  Ghodsi,  Z.,  Gu,  T.,  &  Garg,  S.  (2017).  Safetynets:  Verifiable  execution  of  deep 
neural  networks  on  an  untrusted  cloud.  In  Advances  in  Neural  Information 
Processing Systems (pp. 4672-4681). 

[34]  Gu, Tianyu, et al.. "BadNets: Identifying Vulnerabilities in the Machine Learning 

Model Supply Chain." arXiv preprint arXiv:1708.06733 (2017). 

[35]  G.  Katz  et  al.  ‚ÄúReluplex:  An  efficient  SMT  solver  for  verifying  deep  neural 

networks‚Äù, CAV. Springer, 2017, pp. 97-117. 

[36]  Gilad-Bachrach,  Ran,  et  al.  "Cryptonets:  Applying  neural  networks  to  encrypted 
data  with  high  throughput  and  accuracy."  International  Conference  on  Machine 
Learning. 2016. 

[37]  Sharif, Mahmood, et al. "Accessorize to a crime: Real and stealthy attacks on state-
of-the-art face recognition." Proceedings of the 2016 ACM SIGSAC Conference on 
Computer and Communications Security. ACM, 2016. 

[38]  Hesamifard  et  al.,    ‚ÄúCryptoDL:  Deep  Neural  Networks  over  Encrypted  Data.‚Äù 

preprint arXiv:1711.05189 (2017). 

[39]  F. J. Gonz√°lez-Serrano et al., ‚ÄúSupervised machine learning using encrypted training 

data.‚Äù International Journal of Information Security (2017): pp. 1-13. 

[40]  Deng,  J.,  Dong,  W.,  Socher,  R.,  Li,  L.  J.,  Li,  K.,  &  Fei-Fei,  L.  (2009,  June). 

Imagenet: A large-scale hierarchical image database. IEEE CVPR, pp. 248-255. 

[41]  T.  Li  et  al.,  ‚ÄúOutsourced  privacy-preserving  classification  service  over  encrypted 

data‚Äù, Journal of Network and Computer Applications (2018). 

Conference on Computer and Communications Security. ACM, 2017. 

[42]  D.  Gopinath  et  al.,  ‚ÄúDeepsafe:  A  data-driven  approach  for  checking  adversarial 

[12]  A. griffin. ‚ÄúAI failures‚Äù, THE INDEPENDENT TECH 
[13]  J.  B.  white,  ‚ÄúSelf-driving  bus  crashes  on  very  first  day‚Äù,  THE  INDEPENDENT 

TECH 

[14]  M. N. Asiedu, et al. "Image processing and machine learning techniques to automate 
diagnosis  of  Lugol's  iodine  cervigrams  for  a  low-cost  point-of-care  digital 
colposcope."  Optics  and  Biophotonics  in  Low-Resource  Settings  IV.  Vol.  10485, 
2018. 

[15]  M. A. Hanif et al., ‚ÄúRobust Machine Learning Systems: Reliability and Security for 

Deep Neural Networks‚Äù, IOLTS 2018, pp. 257-260. 

robustness in neural networks‚Äù, preprint arXiv:1710.00486 (2017). 

[43]  X. Huan et al., ‚ÄúSafety verification of deep neural networks‚Äù, CAV. Springer, 2017. 
[44]  Rosenberg et al.,  "DeepAPT:  nation-state APT attribution  using end-to-end deep 

neural networks.‚Äù ICANN. Springer, 2017, pp. 91-99. 

[45]  X. Liu et al., ‚ÄúPrivacy-Preserving Outsourced Support Vector Machine Design for 

Secure Drug Discovery.‚Äù IEEE TCC (2018). 

[46]  ‚Äú2017 

10 
https://medium.com/syncedreview/2017-in-review-10-ai-failures-4a88da1bdf01 

Failures‚Äù, 

Review: 

AI 

in 

[online]  

 
