Maintaining Data Integrity in Fog Computing Based
Critical Infrastructure Systems

Abdulwahab Alazeb, Brajendra Panda
Dept. of Computer Science and Computer Engineering
University of Arkansas
Fayetteville AR, USA
afalazeb,bpanda@uark.edu

0
2
0
2

n
a
J

7
1

]

R
C
.
s
c
[

2
v
3
4
4
4
0
.
1
0
0
2
:
v
i
X
r
a

Abstract—The evolution of the utilization of technologies in
nearly all aspects of life has produced an enormous amount of
data essential in a smart city. Therefore, maximizing the beneﬁts
of technologies such as cloud computing, fog computing, and the
Internet of things is important to manage and manipulate data in
smart cities. However, certain types of data are sensitive and risky
and may be inﬁltrated by malicious attacks. As a result, such
data may be corrupted, thereby causing concern. The damage
inﬂicted by an attacker on a set of data can spread through an
entire database. Valid transactions that have read corrupted data
can update other data items based on the values read. In this
study, we introduce a unique model that uses fog computing in
smart cities to manage utility service companies and consumer
data. We also propose a novel technique to assess damage to data
caused by an attack. Thus, original data can be recovered, and
a database can be returned to its consistent state as no attacking
has occurred.

Index Terms—Fog Databases, Malicious Transactions, Affected

Transactions, Data Integrity

I. INTRODUCTION

The future of the internet is in the Internet of Things (IoT),
evidenced by the signiﬁcant increase in wearable technology,
smart homes and buildings, connected vehicles, and smart
grids. The number of connected IoT devices in use by 2025
is estimated at nearly 75 billion [1], producing an enormous
amount of data predicted to total more than 79 zettabytes
[2]. Limitations and restrictions in bandwidth, as well as this
rapid growth in the amount of data produced, make the current
information system architecture inadequate for managing and
moving that volume of data to the cloud. In many scenarios,
with increasing use of IoT devices, it would be impractical to
do so. Additionally, contemporary society has incorporated a
staggering number of sensitive and real-time IoT applications
as integral parts of daily life. Connected car technologies,
video conference applications, health monitoring, and real-
time production line monitoring, are all applications requiring
low-latency and location awareness in order to provide high-
quality services for this technology-driven era [3].

IoT devices, such as smart meters in modern smart cities,
will not only produce a massive amount of data, but hetero-
geneous data that will a need to be processed in real-time [4].
This data will not be valuable enough without exploiting the
maximum beneﬁts from other technologies. It is unmanageable
and nearly impossible for a cloud to handle the many tasks,

of processing and aggregating, analyzing and storing for such
volumes of disparate data [5]. For that reason, fog computing
was presented by Cisco [6].

Fog computing is trending today because it has several
unique characteristics that, not only establish it as an extension
of the cloud, but also provide privileges in addition to, and
complementing. those of the cloud. This complement to cloud
computing, in an age of cloud processing, is signiﬁcant in
allowing the performance of resource, intensive, and extended
term analytics [7].

In the potential for smart cities, fog computing will help
the IoT and smart meter devices process the data and make
quick decisions to take the right action within a critical time-
frame and to aggregate only the indispensable data for the
cloud. Services and utility companies, such as water and
electric companies, can exploit fog computing technology to
manage and analyze the volume of consumers data. There are
many existing studies on how to expand the efﬁciency of fog
computing in smart cities and to solve technical issues related
to the large volume of data that needs fusion and integration to
the cloud [8]. Moreover, while security and privacy issues have
been addressed by many researchers, other aspects need more
attention, such as a case in which a protection system fails
during a cyberattack and costumers data need to be recovered.
This study aims to detect all transactions that are affected by
any malicious transaction, recover the correct value of data,
and ensure the integrity of consumer data in a fog computing
environment in smart cities.

II. LITERATURE REVIEW

As a new network technology, fog computing has attracted
many researchers. Quite a number of papers have presented a
general survey or study of the challenges, issues, and future
direction of fog computing [9]. Other works write about the
architecture of the fog system [10]. Security and privacy issues
are also a hot topic in this ﬁeld [11], [12].

Many researchers have written about the use of fog com-
puting in smart grids and cities to improve the quality of
services provided to consumers and to solve security and
privacy matters. Zhu et al. [13] proposed a new architecture for
using fog computing in smart cities. They presented a privacy
enhancement scheme that would use techniques such as blind

 
 
 
 
 
 
signature to provide anonymous authentication on consumer
data. They also proposed applying extra encryption techniques
on the smart meter readings of all consumer data at times. This
can be done when each fog node is aggregated to the cloud.
Although Zhu et al. claimed that their proposed scheme would
enhance the privacy and security of consumer data, their model
is still vulnerable to electronic and insider attacks.

Lyu et al. [11] proposed a new framework to securely
aggregate smart meter readings through fog nodes and to
the cloud. To ensure consumer data privacy, their data will
be encrypted after adding some statistical noise using the
Gaussian noise technique. Furthermore, Aazam et al. [10]
discussed the architecture of the Industrial Internet of Things,
which is the use of the IoT in the manufacturing industry for
such applications as smart sensors, actuators, and robots. It
is important for fog computing to be a solution that provides
essential support closer to end users to ensure local and real-
time processing for sensitive and complex tasks.

Much research has been conducted in the area of damage
assessment and data recovery. Researchers have proposed
models and mechanisms that try to recover data after cyber-
attacks. The column dependency based approach introduced
by Chakraborty et al. [14], observes the relationship between
transactions to determine which transactions have been af-
fected by malicious attacks and need to be recovered. In
this way the time-consuming recovery of data after attacks
will take less time than traditional approaches. Chakraborty
et al. proposed a recovery method that will take the affected
transactions as input and perform the recovery in two stages:
compensation and re-execution. Their experiments indicated
that when malicious transactions increase in the database, the
second stage of their recovery scheme is also increased.

Liu [15] aimed to improve the efﬁciency of damage as-
sessment and repair in distributed database systems. They ﬁrst
identiﬁed the challenges and complications that those systems
face and then proposed an algorithm for distributed damage
assessment and repair. On each site, they adopted a Local
Damage Assessment and Recovery (DAR) Executor to scan
the local log to detect and clean any sub-transactions that were
affected by a malicious transaction. Also, on each site there
is a Local DAR Manager that cooperates with the Executor to
ensure global coordination between all sites on the system by
generating a coordinator for any cleaning transaction.

Panda et al. [16] used the data dependency-based approach
to assess the damage that could occur from electronic attacks
and then to return the database to a consistent state. They intro-
duced two algorithms. In the ﬁrst one, damage assessment and
recovery algorithms are executed at the same time, blocking
the system until the whole procedure is complete and causing
signiﬁcant delays as a result. The second algorithm addresses
the delays since the system should be available soon after all
the affected and damaged data have been detected and blocked.
Alazeb and Panda [12] introduced two different models
for using fog computing in a healthcare environment. The
ﬁrst model
is an architecture that uses fog modules with
heterogeneous data, and the second model uses fog modules

with homogeneous data. For each model,
they propose a
unique methodology to assess the damage caused by mali-
cious transactions so that original data may be recovered and
affected transactions identiﬁed for future investigation.

III. MODEL

In this section, a unique architecture for using a distributed
fog node system in smart cities to manage the consumer data of
utility services will be proposed. Then, cooperative algorithms
will be proffered for identifying, assessing, recovering, and
restoring all the damaged and affected data created by an
attack. The goal is the restoration of a reliable database. In
the proposed model, it is assumed that the Intrusion Detection
System (IDS) is responsible for detecting malicious transac-
tions in the system and providing a list of those transactions
to the damage assessment algorithms. Each fog node in the
proposed architecture must have its own log ﬁle and use a
strict serializable history. All operations in the log ﬁle need
to be in the same order in the history. The log ﬁles cannot
be user modiﬁed at any time. Since the log ﬁles will contain
a record of every modiﬁcation to the value of any data item
that is updated by write operations, all read operations are
also required to be stored in the log ﬁles to identify the data
dependencies between the operations and the transactions and
among the detections of the victim fog nodes in the systems.

A. Model Nations

A description of the notations to be used in the proposed

model can be found in Table I.

TABLE I
NOTATION USED IN OUR PROPOSED APPROACH DESCRIPTION

Notation
pub fog

usc fog
MT L

DA Table

DI L

DIT Fogx

VIT Fogx

wi(A, v1, v2)

ri(A, v)

Description
The public fog nodes that are accessed by customers
and utilities providers.
The private fog node for each utility service company.
The list of detected malicious transactions done by
IDS.
The damage audit table, which is a data structure that
will be created by the damage assessment algorithms
to collect data about transactions that are needed to
do the data recovery, such as the valid and invalid
read data items, data written, and accessed fogs.
The damage item list that will contain all damaged
data items that are identiﬁed by our proposed damage
assessment algorithms.
The Fogx damage item table, where x is the ID of the
secondary affected fog node, which reads any damage
data item from another fog node.
The valid data items table that will be created by
algorithm 3 or 4 to add to it all recovered data items
for the secondary affected fog node Fogx. It will be
sent to Fogx to use it as input on algorithm 4.
The write operation of the transaction Ti; v1 is the
before image, which represents the old value of the
data item A before any updating. And v2 is the after
image, which is the new value of data item A after it
is updated.
The read operation of transaction Ti where A is the
data item and v is the current value of A.

some essential computations, such as calculating the daily bill
and average daily customer usage. These computations by the
utilities are important in improving the quality of services in
each city as the need for expansion of services in peak seasons
may become evident and shortages avoided. Utilities may
use data to plan fuel purchases or raise consumer awareness
regarding consumption and conservation.

C. The Proposed Damage Assessment Algorithms

1) Algorithm 1: The Main Damage Assessment Algorithm:
The IDS is responsible for identifying the attacking trans-
actions and sending a list of them to the victim fog node.
Whenever one or more malicious transactions are found on
any fog node in the system, the IDS will detect them and
send them as a list (MT L) to that fog node to be used as
input in the proposed schemes. Once the fog node receives
the list, it will launch Algorithm 1, which is the main damage
assessment algorithm.

As soon as Algorithm 1 is launched, it will create the
damage audit table (DA Table) and damage data item list
(DI L). Both will be initialized to null. Then, the algorithm
will scan the local log ﬁle of the victim fog node, Fog1,
beginning from the ﬁrst attacking transaction of (MT L) list,
Ti. Ti will be added as a new record into DA Table since it
is the ﬁrst attacking transaction. If the attacking transaction
updates at least one data item, then this data item will be
damaged, and any other updating transactions that read this
data will be affected as well. It is important to collect and
store all data items that have been updated and damaged by
the attacking transactions. Then, all transactions that have
read those damaged data items can be identiﬁed, and data
dependency can be declared between the transactions and the
fog nodes in the entire system.

One of the main functionalities of this algorithm will be
the collection of data before damage occurs, and store those
images, which represent the pre-attack value of the data item,
in the written data column on the DA Table. These images
will be used later in the recovery algorithm. Simultaneously,
those damaged data items will be added to the damaged item
list to determine data dependency.

Also, the algorithm will examine every transaction in the
log ﬁle following, the ﬁrst attack, to determine whether any
other transaction is an attacking transaction, or a data access
transaction from another fog node, or an updating write
transaction. In the case where the transaction is an attacking
transaction, the algorithm will perform as in the ﬁrst attacking
transaction. However, if the transaction is an access transaction
from another fog node (Fogx), the algorithm will check every
data item that has been read by Fogx. A new damage item table
for Fogx (DIT Fogx) will be created and all damaged data
items that have been read by Fogx as well as the transaction
identiﬁcation will be added to the DIT Fogx.

Since a fog node may access the same data multiple times,
it is essential to know the transaction ID; this will make it easy
to ﬁnd on its log ﬁle and conﬁrm that the damaged data items
were not corrected later on in the fogx by valid updating. In

Fig. 1. The proposed architecture.

B. The Proposed Architecture

In the proposed model, each smart city will have sev-
eral public fog nodes (pub fog), which will be efﬁciently
distributed to guarantee the quality of service at each point
of the entire city. Private fog nodes will be included, with
at least one private fog node for utility service companies
(usc fog), such as water, electricity, and gas utilities. The
usc fog nodes should be effectively located in the center of
the whole distributed system to ensure a reliable connection to
all pub fog nodes and provide different routes should one of
the pub fog nodes disconnect for any reason. Consumers will
be able to send queries to pub fog nodes only. Data may be
retrieved from the local database if available there. Otherwise,
the queries will be forwarded to the appropriate usc fog node.
Consumers are not allowed to directly connect the usc fog
nodes for security reasons. All queries related to those nodes
will come through the pub fog nodes. Customer utility usage
data will be collected from smart homes and buildings using
IoT devices and smart meters. Usage data will be sent to the
nearest efﬁcient pub fog nodes based on several factors, such
as location and load balance. It is assumed that each pub fog
node in the system will have the ability to perform some
essential data operations, such as calculating customer average
usage over a speciﬁc time frame or aggregating the totals
of selected data values. Those operations are fundamental
to optimization of the network bandwidth since the data
sent over the network will be diminished by aggregating the
necessary data. Additionally, as most customer data will be
processed locally, at the edge of the network, it will enhance
privacy and security by reducing sensitive data transmittal.
Each utility usc fog node receiving the data will also perform

Algorithm 1 The Main Damage Assessment Algorithm

1: Create a new DA table and initialize to null
2: Create a new DI L and initialize to null
3: for every Ti the local log starting from the ﬁrst attacking

4:

5:
6:
7:
8:

9:
10:
11:
12:
13:

14:
15:
16:

17:
18:
19:

20:
21:
22:
23:

24:
25:
26:
27:

28:
29:
30:

31:

transactions of MT L do

if Ti is attacking transaction then

add it as a new record into DA Table
for every wi (A, v1, v2) do

add (A, v1) pair to data written column
add A to the DI L if it is not there

else if Ti is transaction from another fog node x then

for every data item A read by Ti do

if A ∈ DI L then

if DIT Fogx does not exist then

Create a new DIT Fogx where x is the
ID of aimed fog node that reads the affected transaction

Mark Ti as affected transaction
Add Ti and A into DIT Fogx
Update the last column of DA Table

else if Ti is updating transaction then

add it as a new record into DA Table
for every ri (A, v) do
if A ∈ DI L then

add A to invalid read column

else

add (A, v) to valid read column

for every wi (A, v1, v2) do

if invalid read column of Ti (cid:54)= ∅ then
add (A, v2) to data written column
add A to the DI L if it is not there

else check If (A ∈ DI L)

add (A, v2) to data written column
delete A from DI L

if ci is found & (both invalid read and data written

columns of Ti) = ∅ then

else if ai is found then

delete the record of Ti from DA Table

32:
33:
34:
35: Send DIT Fogx to Fogx to do further detection
36: Send DA Table & DI L for data recovery (algorithm 3)

delete the record Ti from DA Table

the meantime, the DA Table will be updated indicating that
Fogx has read the damaged data item, so when the recovery
algorithm has successfully corrected the value of the damaged
data item, it will send the correct value to Fogx to use as input
for its own recovery algorithm. If the transaction is an updating
transaction (Tw), and not an attacking transaction belonging
to the malicious transaction list, then it must be added to
the DA Table and examined to accomplish two goals. The
ﬁrst goal is to determine data dependency. All read operations
must be checked to conﬁrm whether Tw has read any of the
damaged data items that already exist to the damaged item list.
If so, those damaged data items will be added to the invalid
read column of Tw, and undamaged data items will be added

to the valid read column. Then, all the write operations will
be checked to determine whether any have read damaged data
items. If so, that means the damage has spread and the written
data item is also corrupted. Therefore, it will be added to the
damaged item list, if it is not already there.

However, if the transaction Tw updates any data item, (A),
without reading any items from the damaged item list, then
the data item (A) will be further checked evaluate its inclusion
in the damaged item list. If (A) was updated without reading
a corrupted transaction, that means it is a valid write and the
data item (A) has been refreshed, so (A) must be removed
from the damaged item list as in steps(28-30). The new value
will be added to the data written column, accomplishing the
second goal of adding the non-attacking updating transaction
to the DA Table.

Finally, all data items in the main victim fog node will
be available for use except the damaged data items on DI L.
Therefore, system availability will be increased. The damage
item table DIT fogx will be sent
to fogx to do further
detection, while the damage audit table and damage item list
will be sent to Algorithm 3, which is the main data recovery
algorithm.

2) Algorithm 2: Secondary Fog Node Damage Assessment
Algorithm: This algorithm will be like Algorithm 1, with some
differences. The main difference in the input of this algorithm
is the DIT Fogx, which is one of the outputs of Algorithm
1 if an affected fog node reads any damaged data item from
the main victim fog node. Assume Fog1 is the main victim
fog node in the system, and it was attacked and maliciously
updated in the transaction Ti, which wrote the data item(Z).
Later, Fogx accessed Fog1 via the transaction Tj to read (Z)
and update other data items, (N) and (M), on its database.
Here we call Fogx the secondary affected fog node. Once the
secondary affected fog node in our example, Fogx receives
the DIT Fogx containing (Z) as a damaged data item , it will
create a new damage audit table and initialize it to null. Note
that this algorithm will use the received DIT Fogx to store
and track the damaged data items instead of creating a new
damage item list.

The algorithm will scan the log ﬁle and start from the
ﬁrst affected transaction from the received table. Therefore,
whenever an affected transaction that belongs to DIT fogx
is found, the steps (3-12) will insert it as new record to the
damage audit table and check each read operations if its belong
to DIT fogx then add it to the invalid read column; otherwise,
it will be added to the valid read column. Moreover, for the
write operations, the updated data items along with its new
values will be added to the data written column as well as
they will be added to the DIT fogx table if they are not there.
In our example, data items N and M will be added to the
DIT fogx and the data written column in DA Table. In a like
manner, the damage item table, if there is one, will be sent to
fogy while the damage audit table and damage item list will
be sent to Algorithm 4, which is data recovery algorithm for
the secondary fog node. The process continues until all the
affected transactions in the entire system are detected.

Algorithm 2 Secondary Fog Node Damage Assessment

Algorithm 3 The Main Recovery Algorithm

Once Fogx receives the DIT Fogx

1: Create a new DA table and initialize to null
2: for every Ti in the local log starting from the ﬁrst affected

transaction on DIT Fogx do

if invalid read column of Ti (cid:54)= ∅ then

1: for each record in the DA Table do
2:
3:
4:

for every data item A in invalid read column do
ﬁnd the last updated (A, v) pair in data written

if Ti ∈ DIT Fogx && mark as affected then

column of DA Table from the former records

3:

4:
5:
6:
7:
8:
9:

10:
11:

12:

13:
14:
15:
16:
17:

18:
19:
20:

21:
22:
23:
24:

25:
26:
27:

28:
29:
30:
31:

32:
33:
34:

35:

36:
37:

add a record for Ti into DA Table
for every ri (A, v) do

if A ∈ DIT Fogx then

add A to invalid read column

else

add (A, v) to valid read column

for every wi (A, v1, v2) do

add (A, v2) pair to data written column
add A into DIT Fogx if it is not there
else if Ti is transaction from another fog node y then

for every data item A read by Ti do

if A ∈ DIT Fogx then

if DIT Fogy does not exist then

Create a new DIT Fogy where y is the
ID of aimed fog node that reads the affected transaction

Mark Ti as affected transaction
Add Ti and A into DIT Fogy
Update the last column of DA Table

else if Ti is updating transaction then

add it as a new record into DA Table
for every ri (A, v) do

if A ∈ DIT Fogx then

add A to invalid read column

else

add (A, v) to valid read column

for every wi (A, v1, v2) do

if invalid read column of Ti (cid:54)= ∅ then
add (A, v2) to data written column
add a record of Ti into DIT Fogx with

data item A if it is not there

else check If (A ∈ DIT Fogx)

add (A, v2) to data written column
delete A from DIT Fogx

if ci is found & (both invalid read and data written

columns of Ti) = ∅ then

delete the record of Ti from DA Table

else if ai is found then

delete the record Ti from DA Table

38:
39: Send DIT Fogy to Fogy to do further detection
40: Send DA Table & DIT Fogx for recovery (algorithm 4)

D. The Proposed Data Recovery Algorithms

1) Algorithm 3: The Main Data Recovery Algorithm:
Immediately after Algorithm 1 has accomplished its task, it
will send the DA Table and DI L to Algorithm 3 for data
recovery. Once Algorithm 3 receives the DA Table, it will
scan the records that read invalid data items. When a data

5:

6:

7:
8:

9:
10:
11:

12:

add (A, v) to valid read column
delete A from invalid read column
for every A in data written column do

recalculate the value of A using values in the

valid read column

if any fogx is existing in fog ID column then

if VIT Fogx does not exist then

Create a new VIT Fogx where x is the ID

of aimed fog node that reads the affected transaction

Add Ti and the (A, v) pair which is the correct

value of A into VIT Fogx
13: Send VIT Fogx to Fogx node
14: for every A in DI L do
15:

check the new log that has just been created while the

recovery process was in progress

if A is not modiﬁed in the log then

scan data written column of DA Table upward to

ﬁnd last updated value of A

substitute the value of A in the database with v

16:
17:

18:

item record is found in the invalid read column, the algorithm
will perform three steps:

• Step 1: Scan the data written column upward, beginning
at the former record of DA Table, for each data item (A)
found in the invalid read column. Once the last updated
and correct value of (A) is found, this value will be added
as a pair (A, v) to the valid read column and data item
(A) will be deleted from the invalid read column. This
continues until all data items in the invalid read column
in the same record have been recovered.

• Step 2: Recompute each data item in the data written
column in the same record by reading the new values
from the valid read column. Successful completion of
Steps 1 and 2 should result in all data items in that record
having the correct values.

• Step 3: Check the last column in the same record, which
is the fog ID column, to determine if any data item has
been read by another fog node in the system; if so, a
new Valid Data Items table (VIT Fogx) will be created
for each affected fog node. Then, the transaction ID along
with the correct new value of each accessed data item will
be added to the VIT Fogx.

the records in the DA Table have been
As soon as all
examined and all three steps are successfully completed, the
VIT Fogx will be sent to the corresponding fog node, Fogx.
2) Algorithm 4: Secondary Fog Node Data Recovery Al-
gorithm: This algorithm will be like Algorithm 3, with two
primary differences. The ﬁrst is that Algorithm 4 input will

be the DA Table and the DIT Fogx from Algorithm 2 for
the same fog node and the VIT Fogx from another fog node,
Fog1. Secondly, this algorithm will check every record on
the received DA Table. When a transaction that is marked
affected is found, then for every data item (A) in the invalid
column, the (A, v) pair from the corresponding transaction on
the VIT Fogx is copied to the valid data column on DA Table
and deleted from the invalid column of DA Table. After all
the damaged data items have been deleted on each record of
the DA Table, the data written column will be checked to
discern if it is empty. If not, then the value v of each data
item (A) in the data written column must be recalculated using
the new values in the valid read column. However, the same
procedure used in Algorithm 3 will be employed if the record
has a transaction with some data items on the invalid read
column. The process continues until all affected data items in
the system are recovered to a consistent state.

E. An Example

To clarify the proposed scheme, consider the following
example. There are two fog nodes in our smart city. Fog1 is a
pub fog node collecting consumer data via smart meters. Fogx
is a private fog node used by the utility company to manage
aggregated data and calculate consumer bills and consumption.
Consider the following log schedules for each one of them:

SFog.1= r1(A, 5) r1(B, 4) w1(C, 11, 9) w1(G, 3, 9) r2(B,
4)c1 r2(G, 9) w2(A, 5, 13) w2(D, 0, 13)c2 fogx.r3(G, 9)c3 w4(A,
13, 5) w4(G, 9, 3)c4 r5(D, 13) r5(A, 5) r5(C, 9) w5(D, 2, 27)c5
r6(B, 4) w6(B, 4, 4) r6(D, 16) w6(D, 16, 20) r6(A, 5) w6(A,
5, 25)c6 fogx.r7(D, 20)c7 r8(C, 9)c8 w9 (C, 9, 11)c9 r10(A, 25)
r10(C, 11) w10(E, 10, 36)c10 fogx.r11(E, 36)c11

SFog.x = r9(K, 3) r9(fog1.T3.G, 9) w9(K, 3, 12) c9 r10(M,
10) r10(K, 12) w10(M, 10, 22)c10 r14(fog1.T7.D, 20) r14(L, 4)
w14(N, 17, 24) c14 r16(fog1.T11.E, 36) w16(P, 4 , 36)c16

Now, suppose the IDS detects the ﬁrst transaction, T1, on
the Fog1 schedule is an attacking transaction and data items
(C) and (G) are detected as having been maliciously updated.
The IDS will send T1 as the list MT L to Fog1. Once Fog1,
the primary victim fog node in the system, receives the list, it
will launch Algorithm 1. Then, it will create a new DA Table
and a new DI L. Consequently, the log ﬁle of Fog1 will be
scanned, beginning with the ﬁrst attacking transaction on the
MT L, which is T1.

Whenever an attacking transaction is found, such as T1 in
this example, it will be added to the DA Table as a new
record. All the write operations of T1 will also be checked, so
whenever a data item is found, it will be added along with its
old value (before image) as a pair to the data written column,
and the data items will be added to the damaged items list.
In our example, the pairs (C, 11) and (G, 3) are added to the
data written column (Table II) while the data items (C) and (G)
will be added to DI L. This will be the case for all attacking
transactions that belong to MT L.

The algorithm will examine the next transaction in the log
ﬁle, which is T2. Since T2 is an updating transaction, it will
be added into the DA Table. Consequently, every reading

Algorithm 4 Secondary Fog Node Recovery Algorithm

Once Fogx receives the VIT Fogx
1: for each record in the DA Table do
if Ti is affected transaction then
2:
3:
4:

for every data item A in invalid read column do
copy the (A, v) pair from corresponding trans-
action on the VIT Fogx to the valid data column on
DA Table

5:

6:

7:
8:

9:

10:
11:

12:
13:

14:
15:

16:
17:
18:

19:

if

delete A from invalid read column
the data written column (cid:54)= ∅ then
for every A in data written column do

recalculate the value of A using values in

the valid read column

else if Ti is updating transaction & invalid read column

(cid:54)= ∅ then

for every A in invalid read column do

ﬁnd the last updated (A, v) pair in data written

column of DA Table from the former records

add (A, v) to valid read column
delete A from invalid read column
for every A in data written column do

recalculate the value of A using values in the

valid read column

if any fogy is existing in fog ID column then

if VIT Fogy does not exist then

Create a new VIT Fogy where y is the ID

of aimed fog node that reads the affected transaction

Add Ti and the (A, v) pair which is the correct

value of A into VIT Fogy
20: Send VIT Fogy to Fogy node
21: for every A in DIT Fogx do
22:

check the new log that has just been created while the

recovery process was in progress

if A is not modiﬁed in the log then

scan data written column of DA Table upward to

ﬁnd last updated value of A

substitute the value of A in the database with v

23:
24:

25:

operation in T2 will be examined to ascertain if it read any
damaged data item from the DI L; if so, the data item will
be added to the invalid read column, as apparent with (G).
Otherwise, it will be added, as a pair with its value, to the valid
read column as (B, 4) in the example. The next transaction is
T3. Fogx has read the data item (G), which is a damaged
data item. So, the Fog ID column will be marked and a new
Damage Items Table will be created for Fogx (DIT Fogx),
adding data item (G) to the table with the transaction ID
(Table III).

The algorithm will also ﬁnd that damaged data items (A)
and (G) have been refreshed in T4 and updated without reading
any other damaged data items. Therefore, they will be added,
with their new values,
to the data that were written and
removed from the DI L. The process continues until the end
of the log is reached. Then, the DIT Fogx will be sent to

Fogx to be used as input for Algorithm 2, while the DA Table
and DI L will be sent to Algorithm 3, which is the primary
recovery algorithm. All data items on DI L will be blocked
from being used until they recover.

Once Fogx receives the DIT Fogx, it will launch Algorithm
2 and use the DIT Fogx as input in further assessment and
detection processes. Note that this table will add any damaged
data items that are detected in Fogx, ( Table V). Thus, a new
DA Table will be created. It will scan the local log of Fogx,
starting from the ﬁrst affected transaction on the DIT Fogx,
which is T9. T9 will be added to the new DA Table and for
every read operation, the data item will be examined to ﬁnd
out if it belongs to the DIT Fogx; if so, it will be added to the
invalid read column. Otherwise, it will be added to the valid
read column along with its value. Therefore, the data item
fog1.T3.G will be added to the invalid read column, while the
pair (K, 3) will be added to the valid read column (Table IV).
However, it will do the same thing for the write operations as
in Algorithm 1, so the updated data item (K) along with its
value (K, 12) will be added to the data written column.

Meanwhile,

the data item (K), will be added into the
DIT Fog2 (Table V), since it becomes affected by reading the
damaged data item fog1.T3.G. For T10, the updating occurs
after reading the damaged data item (K), so the scenario (the
same as shown in T2 in Fog1) will be repeated (Table IV).
Consequently, the process continues until the end of the log
is reached. Once reached, the DA Table for Fogx will be sent
to Algorithm 4 to conduct data recovery. And all data items
on Table V will be blocked until they recover.

As soon as Fog1 has completed the damage assessment
algorithm, Algorithm 1, and sent the DA Table and DI L
to Algorithm 3 to proceed with data recovery, which will be
launched immediately, and received the DA Table and DI L
as inputs, it will scan the DA Table from its beginning and
search for any transactions that read invalid data items. For
example, T2 read invalid data item (G), and the algorithm
looks for the last valid update value of (G), which must be
the closest transaction before T2. Therefore, T1 must have the
latest updated correct value of (G), which is (3). The pair (G,
3) will be copied to the valid read column, and (G) will be
removed from the invalid read column. After that, T2 will be
recalculated using the new values (Table VI). (Note that in
this example any transaction where write operations are found
after read operations, all values of the read operations will be
added together.)

Following T2, T3 will be processed in the same manner. As
Fogx has read the damaged data item (G), a new Valid Data
Item Table for Fogx (VIT Fogx) will be created and added to
the transaction ID, T3, and the correct value of (G), which is
(G, 3) will be added (Table VII), and so on until the end of
the DA Table. After that, VIT Fogx will be sent to Fogx to
be used as an input in Algorithm 4 to recover the data.

Once Fogx receives the VIT Fogx, it will launch Algorithm
4 and use VIT Fogx along with it is own DA Table to process
data recovery. Then, every record in the DA Table will be
checked. Since the ﬁrst record, T9 in this example, must be

then VIT Fogx should
an affected transaction from Fog1,
have the correct and valid value of the damaged data item.
Therefore, the new value of data item (G) will be copied
from VIT Fogx to the valid read column of the DA Table
and removed from the invalid read column. After that, T9 will
be recalculated using the new values (Table VIII). The rest of
the algorithm will be almost the same as Algorithm 3.

TABLE II
THE DAMAGE AUDIT TABLE FOR FOG1

Valid read

Invalid

Fog ID

Data written
(C, 11), (G, 3)
(A, 13), (D, 13)

(A, 5), (G, 3)
(D, 27)
(D, 20), (A, 25)

(B, 4)

(A, 5)
(B, 4), (A, 5)

(C, 11)
(E, 36)

(C, 11)

T Id
T1
T2
T3
T4
T5
T6
T7
T9
T10
T11

G
G

C, D
D
D

A
E

Fogx

Fogx

Fogx

TABLE III
FOGx DAMAGE ITEM TABLE CREATED BY FOG1

Transaction Id
fog1.T3
fog1.T7
fog1.T11

Damaged Data Items
G
D
E

TABLE IV
THE DAMAGE AUDIT TABLE FOR FOGx

T Id
T9
T10
T14
T16

Data written
(K, 12)
(M, 22)
(N, 24)
(P, 36)

Valid read
(K, 3)
(M, 10)
(L, 4)

Invalid
fog1.T3.G
K
fog1.T7.D
fog1.T11.E

Fog ID

TABLE V
DIT FOGx WITH ALL DAMAGED DATA ITEMS THAT ARE FOUND ON FOGx

Transaction Id
fog1.T3
fog1.T7
fog1.T11
T9
T10
T14
T16

Damaged Data Items
G
D
E
K
M
N
P

TABLE VI
DA TABLE FOR FOG1 AFTER DAMAGED DATA HAVE BEEN RECOVERED

Data written
(C, 11), (G, 3)
(A, 7), (D, 7)

(A, 5), (G, 3)
(D, 23)
(D, 27), (A, 32)

(C, 11)
(E, 43)

T Id
T1
T2
T3
T4
T5
T6
T7
T9
T10
T11

Valid read

Invalid

Fog ID

(B, 4), (G, 3)
(G, 3)

(A,5), (C, 11), (D, 7)
(B, 4), (A, 5), (D, 23)
(D, 27)

(C, 11), (A, 32)
(E, 43)

Fogx

Fogx

Fogx

[10] M. Aazam, S. Zeadally, and K. A. Harras, “Deploying fog computing
in industrial internet of things and industry 4.0,” IEEE Transactions on
Industrial Informatics, vol. 14, no. 10, pp. 4674–4682, 2018.

[11] L. Lyu, K. Nandakumar, B. Rubinstein,

J. Bedo, and
M. Palaniswami, “Ppfa: privacy preserving fog-enabled aggregation in
smart grid,” IEEE Transactions on Industrial Informatics, vol. 14, no. 8,
pp. 3733–3744, 2018.

Jin,

J.

[12] A. Alazeb and B. Panda, “Ensuring data integrity in fog computing based
health-care systems,” in International Conference on Security, Privacy
and Anonymity in Computation, Communication and Storage, pp. 63–77,
Springer, 2019.

[13] L. Zhu, M. Li, Z. Zhang, C. Xu, R. Zhang, X. Du, and N. Guizani,
“Privacy-preserving authentication and data aggregation for fog-based
smart grid,” IEEE Communications Magazine, vol. 57, no. 6, pp. 80–
85, 2019.

[14] A. Chakraborty, A. K. Majumdar, and S. Sural, “A column dependency-
based approach for static and dynamic recovery of databases from
malicious transactions,” International Journal of Information Security,
vol. 9, no. 1, pp. 51–67, 2010.

[15] P. Liu and M. Yu, “Damage assessment and repair in attack resilient
distributed database systems,” Computer Standards & Interfaces, vol. 33,
no. 1, pp. 96–107, 2011.

[16] B. Panda and J. Giordano, “Reconstructing the database after electronic
attacks,” in Database Security XII, pp. 143–156, Springer, 1999.

TABLE VII
VIT FOGx SENT FROM FOG1

Transaction Id
fog1.T3
fog1.T7
fog1.T11

Valid Data Items
(G,3)
(D,27)
(E,43)

TABLE VIII
DA TABLE FOR FOGx AFTER DAMAGED DATA HAVE BEEN RECOVERED

T Id
T9
T10
T14
T16

Data Written
(K, 6)
(M, 16)
(N, 31)
(P, 43)

Valid Read
(K,3), (fog1.T3.G, 3)
(M, 10), (K, 6)
(L, 4), (fog1.T7.D, 27)
(fog1.T11.E, 43)

Invalid

fogID

IV. CONCLUSION

Intrusion detection is one of the main phases that must be
included to ensure the security and reliability of any comput-
ing system. This phase uses software or device to observe
the system for any malicious activity or policy violation.
However, detection systems sometimes fail to detect several
malicious transactions on time, leading to data damage. There-
fore, intrusion detection must be complemented by another
phase, namely, damage assessment and data recovery, which
ensures the integrity and availability of system data. This
phase identiﬁes any further affected transactions and ensures
that the database returns to a consistent state. In this paper,
we have introduced a novel architecture model for applying
fog technology to smart cities. Working with the nature and
characteristics of the model, we propose a unique method
of assessing and recovering damaged data. As part of future
work, we plan to evaluate the proposed model by simulating
the whole environment and examining the performance of our
algorithms.

REFERENCES

[1] Statista Research Department, “Internet of things (iot) connected devices
installed base worldwide from 2015 to 2025 (in billions),” 2016.
[2] A. Holst, “Data volume of internet of things (iot) connections worldwide

in 2018 and 2025 (in zettabytes),” 2019.

[3] B. Zhang, N. Mor, J. Kolb, D. S. Chan, K. Lutz, E. Allman,
J. Wawrzynek, E. Lee, and J. Kubiatowicz, “The cloud is not enough:
Saving iot from the cloud,” in 7th {USENIX} Workshop on Hot Topics
in Cloud Computing (HotCloud 15), 2015.

[4] M. Marjani, F. Nasaruddin, A. Gani, A. Karim, I. A. T. Hashem, A. Sid-
diqa, and I. Yaqoob, “Big iot data analytics: architecture, opportunities,
and open research challenges,” IEEE Access, vol. 5, pp. 5247–5261,
2017.

[5] F. Y. Okay and S.

¨Ozdemir, “A fog computing based smart grid
model,” 2016 International Symposium on Networks, Computers and
Communications (ISNCC), pp. 1–6, 2016.

[6] F. Bonomi, R. Milito, J. Zhu, and S. Addepalli, “Fog computing and its
role in the internet of things,” in Proceedings of the ﬁrst edition of the
MCC workshop on Mobile cloud computing, pp. 13–16, ACM, 2012.

[7] A. Yousefpour, C. Fung, T. Nguyen, K. Kadiyala, F. Jalali, A. Niakan-
lahiji, J. Kong, and J. P. Jue, “All one needs to know about fog computing
and related edge computing paradigms: A complete survey,” Journal of
Systems Architecture, 2019.

[8] C. Perera, Y. Qin, J. C. Estrella, S. Reiff-Marganiec, and A. V. Vasilakos,
“Fog computing for sustainable smart cities: A survey,” ACM Computing
Surveys (CSUR), vol. 50, no. 3, p. 32, 2017.

[9] R. Mahmud, R. Kotagiri, and R. Buyya, “Fog computing: A taxonomy,
survey and future directions,” in Internet of everything, pp. 103–130,
Springer, 2018.

