2
2
0
2

y
a
M
1

]

R
C
.
s
c
[

1
v
6
6
5
0
0
.
5
0
2
2
:
v
i
X
r
a

Adversarial Planning

VALENTIN VIE, Pennsylvania State University, USA
RYAN SHEATSLEY, Pennsylvania State University, USA
SOPHIA BEYDA, Pennsylvania State University, USA
SUSHRUT SHRINGARPUTALE, Pennsylvania State University, USA
KEVIN CHAN, Army Research Laboratory, USA
TRENT JAEGER, Pennsylvania State University, USA
PATRICK MCDANIEL, Pennsylvania State University, USA

Planning algorithms are used in computational systems to direct autonomous behavior. In a canonical
application for example, planning for autonomous vehicles is used to automate the static or continuous
planning towards performance, resource management, or functional goals (e.g., arriving at the destination,
managing fuel consumption). Existing planning algorithms assume non-adversarial settings; a least cost plan
is developed based on available environmental information (i.e., the input instance). Yet, it is unclear how such
algorithms will perform in the face of adversaries attempting to thwart the planner. In this paper, we explore
the security of planning algorithms as used in cyber- and cyber-physical systems. We present two adversarial
planning algorithmsâ€“one static and one adaptiveâ€“that perturb input planning instances to maximize cost (often
substantially so). We evaluate the performance of the algorithms against two dominant planning algorithms
used in commercial applications (D* Lite and Fast Downward) and show both are vulnerable to extremely
limited adversarial action. Here, experiments show that the adversary is able to increase plan costs in 66.9% of
instances by only removing a single action from the actions space (D* Lite) and render 70% of the instances
from an international planning competition unsolvable by removing only three actions (Fast Forward). Finally,
we show that finding an optimal perturbation in any search-based planning system is NP-hard.

ACM Reference Format:
Valentin Vie, Ryan Sheatsley, Sophia Beyda, Sushrut Shringarputale, Kevin Chan, Trent Jaeger, and Patrick
McDaniel. 2022. Adversarial Planning. 1, 1 (May 2022), 25 pages.

1 INTRODUCTION
The science of planning is about creating a planâ€”a set of actionsâ€”to achieve a goal. Applications
of planning algorithms are found in robotics, aerospace, and industrial processes, where they are
used to find the most optimal solution to a given problem. For example, planning algorithms have
been used for unmanned vehicles [1], natural language generation [2], greenhouse logistics [3],
manufacturing [4], network vulnerability analysis [5], and navigation [6].

Plans are, in most instances, sequences of steps called actions. Each action represents an atomic
operation that an agent performs to achieve some sub-goal within the domain, e.g., moving a step
forward or loading or unloading a device. A planning algorithm receives an input planning problem
consisting of states and operations and outputs a sequence of actions, which, when executed from
the initial state, gets the agent to a goal state. The plan cost is the sum of the costs of the actions in
the plan. Planning algorithms are designed to minimize the plan cost (which can include multiple
metrics such as distance traveled, resources, etc.). A common visualization is to create a graph with
states as nodes and actions as edges connecting two states (Fig. 1).

Authorsâ€™ addresses: Valentin Vie, vav4@cse.psu.edu, Pennsylvania State University, Westgate Building, W378, University
Park, Pennsylvania, USA, 16801; Ryan Sheatsley, sheatsley@psu.edu, Pennsylvania State University, USA; Sophia Beyda,
beyda@psu.edu, Pennsylvania State University, USA; Sushrut Shringarputale, sps5394@psu.edu, Pennsylvania State Univer-
sity, USA; Kevin Chan, kevin.s.chan.civ@mail.mil, Army Research Laboratory, USA; Trent Jaeger, tjaeger@cse.psu.edu,
Pennsylvania State University, USA; Patrick McDaniel, mcdaniel@cse.psu.edu, Pennsylvania State University, USA.

2022. XXXX-XXXX/2022/5-ART
https://doi.org/

, Vol. 1, No. 1, Article . Publication date: May 2022.

 
 
 
 
 
 
2

Vie et al.

Fig. 1. State-space viewed as a graph with actions linking the different states. A planner finds a way to reach
a goal state

In practice, planners (often called agents) are embedded in larger systems of components including
sensors (e.g., motion detection, LIDAR), classification or logic systems (e.g., machine learning-based
image recognition, monotonic reasoning), logic/software driven mechanical actuators (e.g., break
systems, batteries), and embedded operating systems [7, 8]. For example, autonomous vehicles
simultaneously use local and global planners to make short-term (e.g., motion planning through
a busy intersection) and long term (e.g., route planning) decisions on how to safely direct the
vehicle to its destination [9]. While the security of many of these components and the system as a
whole have been explored in many contexts [10, 11], few, if any, previous efforts have attempted to
understand how these systems can be subverted by an adversary attacking the planning process.
This work considers an adversary attempting to subvert a system by attacking the planner with
the goal of reducing the effectiveness of the system (e.g., inducing long indirect routing) or in the
degenerate case preventing the successful completion of the plan entirely (e.g., preventing the
vehicle from reaching its destination). Here, we explore a threat model in which an adversary is
able to remove a number of actions from the set of actions in the action space available to the
agent. We consider two adversarial strategies: one where the adversary perturbs the elements
of the input planning instance (called a static or offline attack) and another where the adversary
adaptively counteracts an executing plan (called an online attack). Here we build upon security
efforts at identifying worst case inputs to complex systems and algorithms, e.g., in adversarial
machine learning [12â€“14] or fuzzing [15, 16].

The consequences of inefficient or unachievable plans (the outcome of an attack) can be dire.
For example, a poorly designed motion plan in an autonomous vehicle can lead to unsafe condi-
tions or accidents [17]. Poor or sub-optimal planning in chemical manufacturing (called chemical
production scheduling) can lead to production quality problems, induce equipment failures, or
reduce efficiency [18]. Planning failures in transportation systems or logistics can cause widespread
outages and lead to travelers being stranded [19] (see Section 5.2 in which we simulate a realistic
attack on the Munich airport planning systems). In short, we posit any system that depends on
a viable and efficient plan can be undermined by an adversary with the ability to make small
perturbations to the plan space. Similar to work in adversarial machine learning, the adversaryâ€™s
goal to (a) find a perturbation that achieves the negative outcome (increasing plan execution cost)
while (b) minimizing the size of the perturbation (number of actions removed).

, Vol. 1, No. 1, Article . Publication date: May 2022.

Initial state{At (1,1)}State 1{At (1,2)}Action: go north cost: 1State 4Action ... cost: 5Action ...cost: 1Goal state{At (5,5)}State 2{At (2,1)}State 3{At (0,1)}Action: go right cost: 2Action: go leftcost: 1.............................Goal StateAdversarial Planning

3

In this paper, we develop and evaluate two adversarial algorithms that manipulate real-world
planning systems to induce sub-optimal (i.e., more costly) plans. We introduce the window-heuristic
as an approximation function to predict the expected plan cost impact made by removing part
of the action space (called an adversarial perturbation of the input planning instance). Several
adversarial algorithms are presented, and intuition and examples are provided.

We empirically evaluate our approach on the D* Lite algorithm [6] (a path-finding algorithm)
and the Fast Downward planner [20], two of the most widely used planning algorithms used on
industrial systems. The D* Lite algorithm has been widely used for autonomous vehicle navigation
for its capacity to adapt to changes in the environment. The original version (D*) was, for example,
used by DARPA for its Unmanned Ground Vehicle program [1] and for Mars rover prototypes [6].
The Fast Downward planner is a classical planning system based on forward heuristic search [20]
and was used by two winning planners of the sequential-satisfying track to find low-cost plans in a
fixed amount of time of the 9th International Planning Competition (IPC).

Our experiments of D* Lite planning show that 66.9% of randomly selected planning instances
in a size 15 Ã— 15 maze1 have an increased cost or become unsolvable with a single perturbation
(82.7% with two). Unsolvable is defined as the inability for the agent to find a valid path to the
goal state. In a second set of experiments, we evaluate the performance of the window-heuristic
on the international 2014 IPC competition planning instances. Interestingly, for some competition
domains, we found that 70% of instances become unsolvable if an adversary can remove only three
actions out of over 500 available.

We make the following contributions in this work:

â€¢ We develop an algorithm to find adversarial changes in STRIPS-written tasks and path-finding
problems. We use our window-heuristic to create a table of adversarial changes which we
use to perturb an planning instance (Section 4).

â€¢ The attacks are applied to two of the dominant planning systems used in commercial appli-

cations: D* Lite and the Fast Downward planner in real-world settings (Section 5).

â€¢ The online attack achieved an 82% success rate at inducing cost while the offline attack

reached up to a 100% success rate, depending upon the domain considered.

Section 6 further presents the high-level results of a extended survey of the security of fielded
planning systems. In this, we explore the realism of the proposed threat model and provide concrete
examples of how (and why) such perturbations can be achieved in real systems and applications
such as autonomous vehicles, manufacturing, and data center management.

2 BACKGROUND
Planning Algorithms - The objective of a planning algorithm is to output a plan containing
different actions to achieve a goal. This objective can take different forms depending on the
problem. For example, given a Rubikâ€™s cube, the goal is to have one color on each face. For the
air cargo transportation domain, the goal is to deliver all packages to their destination airports
(Fig. 2). A planning algorithm outputs a sequence of actions ğ´1, ğ´2, ..., ğ´ğ‘› which, when executed
from the initial state ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ , gets the agent to a goal state ğ‘†ğ‘”ğ‘œğ‘ğ‘™ . A state is defined as the configuration
of the environment. There can be as many states as the environment requires. For example, a
Rubikâ€™s cube contains more than 43 quintillion different states. If we call ğ‘“ the transition function,
then we have the following state trajectory: ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ = ğ‘†0, ğ‘†1 = ğ‘“ (ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡, ğ´1), ..., ğ‘†ğ‘”ğ‘œğ‘ğ‘™ = ğ‘“ (ğ‘†ğ‘›âˆ’1, ğ´ğ‘›) = ğ‘†ğ‘›.
The solution is said to be optimal if the total cost (cid:205)ğ‘›âˆ’1
ğ‘ (ğ‘†ğ‘–, ğ´ğ‘–+1) is minimized, where ğ‘ (ğ‘†ğ‘–, ğ´ğ‘–+1)
ğ‘–=0

1For reference, routing in this grid is equivalent to real world taxi route planning during periods of high congestion (e.g.,
New Years eve) in mid-town New York City (i.e., bounded by 8th Avenue (W), 59th (N), the East River (E), and Times
Square(S)).

, Vol. 1, No. 1, Article . Publication date: May 2022.

4

Vie et al.

Fig. 2. air cargo transportation: planning to deliver the packages to the destination with the minimum
number of actions (Load, Unload, Fly).

(:action LOAD

:parameters (?c - cargo ?p - plane

?a - airport)

:precondition (and (At ?c ?a) (At ?p ?a))
:effect (and (In ?c ?p) (not (At ?c ?a))))

(:action LOAD_c1_p1_LAX

:parameters (c1 - cargo p1 - plane

LAX - airport)

:precondition (and (At c1 LAX) (At p1 LAX))
:effect (and (In c1 p1) (not (At c1 LAX))))

Fig. 3. (Top) Non-grounded Load action for air cargo transportation domain. (Bottom) A grounded Load
operator.

is the cost of action ğ´ğ‘–+1 from state ğ‘†ğ‘– . The cost function is chosen by the user depending on the
criterion under optimization, e.g., time, resources, etc.

We distinguish two categories of planning systems: online and offline. We define a planning
system as offline when the state trajectory to the goal state is computed before the execution begins.
The entire plan is computed beforehand, which leaves little room for adaptation but leaves enough
time to optimize the solution. On the other hand, with an online planner, an agent can react to
changes in the environment (miscalculation, error, obstacle discovery, etc.). The plan is updated as
the agent moves. Examples of these types of planning systems are the Mars Roverâ€™s navigation
(offline) and a Teslaâ€™s navigation system (online).
STRIPS Notation - STRIPS is a standard language to describe classical planning problems. Simply
put, a planning problem articulated in STRIPS is a triple (ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘†ğ‘”ğ‘œğ‘ğ‘™, O). See Appendix A for an
example of a STRIPS instance.

The first item ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ is the initial state represented as a collection of variables known as atoms
or predicates. For example, to specify that cargo1 is at LAX airport (Fig. 2), we would describe the
state with the predicate (At, cargo1, LAX). The second element ğ‘†ğ‘”ğ‘œğ‘ğ‘™ is a characterization of the

, Vol. 1, No. 1, Article . Publication date: May 2022.

SFOATLLAXORDJFKp1p2p3c4c2c3c1Adversarial Planning

5

goal state, defined as a set of predicates. In air cargo transportation goal is the final position of
the cargo. The last item O is a finite set of grounded actions. A grounded action is an action in
which all parameters have been assigned to real objects, i.e., the action has been fully specified.
A non-grounded action has at least one parameter not bound to a specific object. We call actions
operators that can transform a state into (potentially) another state by impacting the cost of the
plan. Only deterministic actions are considered, meaning there is no uncertainty about the outcome
of an action. Three sets of predicates define an operator: parameters, preconditions, and effects.
The parameters specify the objectsâ€™ type. As demonstrated in Figure 3, c has to be cargo, p a plane,
and a an airport. The preconditions must be satisfied for the action to be executed. For example, in
Figure 3, in order to load a unit of cargo into a plane, the cargo must be at the same airport as the
plane. The effects impact the input state modifying its predicates, they define the consequences of
the action on the state. The set O can contain thousands of grounded operators. For example, in
Figure 2, there would be one grounded Load operator for each combination of cargo, plane, and
airport. Consequently, we mostly use non-grounded actions to define a planning task. For example,
in Figure 3, a could be any airport (SFO, LAX, etc.). If two planning problems share the same
non-grounded actions, they belong to the same domain, e.g., instances of the Rubikâ€™s cube have
different starting positions, but are always solved by rotating the faces (i.e., the same operators).
Finally, within the scope of this paper, we introduce an adversarial change as a grounded operator
being removed from the set of operators O, with the goal of increasing the cost. The goal for the
adversary is to find ğ‘˜ grounded actions to remove from O to increase the cost of the plan. In the
offline attack, the removal happens before the agent starts planning whereas in the online attack,
the removal happens while the agent is executing the plan. We consider ğ‘˜ to never exceed ten and
|O| can have an arbitrary size (usually thousands of grounded actions).
General Purpose Planners - Since we develop a general heuristic to find effective adversarial
changes in any domain, we work with domain-independent planners. General purpose planners
generally perform worse than domain-specific ones because they lack the domain-specific knowl-
edge to prune the search for a solution [21]. However, they offer a convenient way to solve planning
tasks without domainâ€”and planningâ€”expertise. These planners are divided into three main cate-
gories [22]. Currently, the most popular and effective approaches to solving a deterministic planning
problem are: (1) performing a search using a planning graph [23], (2) translating it to a Boolean
satisfiability (SAT) problem and using a SAT solver [24], or (3) executing a forward or backward
search in the state-space with a heuristic [25]. We build the window-heuristic (Section 4) on top of
the last category of planners: forward heuristic search planners. We also test the attack on forward
heuristic search planners although our techniques apply to other classes of planners as well.

In more detail, the planners we focus on here perform a forward or backward search in the
state-space: a graph where the nodes are the states and the directed edges are the grounded actions
(See Fig. 1). The goal is to find a minimum cost path from the initial state to the goal state in the
state-space. The search is guided by a heuristic by estimating the distance from a particular state
to the goal state. Heuristics can be extracted from the problem directly or can be specific to a
domain. For path-finding problems, a common practice is to use the Manhattan distance or the
Euclidean distance. The competition planner Fast Downward [26] can run with different search
algorithms (e.g., A* and Best-first search) and different heuristics (e.g., FF heuristic and Additive
heuristic). While this work focuses on forward/backward search, our preliminary analysis of other
general-purpose planners suggests vulnerability to adversarial manipulation is a function of the
instance and less on the specific planning algorithm. We defer that analysis to future work.

, Vol. 1, No. 1, Article . Publication date: May 2022.

6

Vie et al.

3 THREAT MODEL
We develop an algorithm to adversely influence planning systems. Given a planning task, we want
to output a set of adversarial changes to decrease the cost of the initial plan. The metric used to
measure the cost of the plan changes between the different applications of planning systems. It
can be the computation time, the algorithmic complexity, the resource requirements, or the cost
function. For instance, the complexity is critical for online planning algorithms such as on-board
planners in unmanned vehicles, where the energy resources are limited [27].

Specifically, an adversary will seek to come up with adversarial changes that will impact the cost
of the plan. The computation time needed to find a plan may also be affected by the adversarial
changes. Indeed, a planning system has to potentially explore deeper in the state-space to find an
acceptable solution. The ultimate goal for the adversary is to make sure the agent will never reach
the goal state without the agent knowing it. In this way, the cost of the plan is infinite, and the
planner can loop almost forever. Note that perturbing a plan is not always feasible; it depends on
the adversaryâ€™s capabilities and the instance considered.

3.1 Adversarial Capabilities
The strength of an adversary is defined by the information and capabilities at their disposal. We
consider two kinds of attacks: online and offline. In the offline case, the adversary perturbs the
input planning instance given to the agentâ€™s planner. For example, given a task in the air cargo
transportation domain, the agent needs to find a series of actions reaching the goal, without
(Load cargo1 in plane1 at SFO) and (Fly plane1 from SFO to JFK). In the online case, the
adversary removes actions from O during the execution of the plan (while the agent is interacting
with the environment).

We call ğ»ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ the search heuristic used by the agent. Additionally, for an online planning system,
we define ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ as the current state of the agent at time ğ‘¡ğ‘– and ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ as the next state of the agent at
time ğ‘¡ğ‘–+1. These values do not exist for an offline planner because the plan is computed beforehand.
We explore adversaries threat models including: (ğ‘) Agentâ€™s Heuristic and Informed, Online
- This adversary knows the search heuristic used by the agentâ€™s planner (Euclidean, Manhattan,
etc.). Knowing the next state of the agent, ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ , means our adversary is informed. The adversary
knows ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ , ğ‘†ğ‘”ğ‘œğ‘ğ‘™ , ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ and ğ»ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ . (ğ‘) Agentâ€™s Heuristic, Online - This adversary knows the
search heuristic used by the agent. However, ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ is unknown. The adversary can only guess
ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ using the state with the best cost estimate given by ğ»ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ . This guess can be incorrect if the
agentâ€™s search algorithm is non-deterministic. Here, the adversary knows ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ , ğ‘†ğ‘”ğ‘œğ‘ğ‘™ , and ğ»ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ .
(ğ‘) Black-Box, Online - This adversary does not know anything concerning the agentâ€™s planner.
Here, the adversary knows ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ and ğ‘†ğ‘”ğ‘œğ‘ğ‘™ . (ğ‘‘) Agentâ€™s Heuristic, Offline - This adversary
knows the heuristic used by the planning system of the agent. The adversary knows ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ , ğ‘†ğ‘”ğ‘œğ‘ğ‘™ , and
ğ»ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ . (ğ‘’) Black-Box, Offline - This adversary does not know anything concerning the agentâ€™s
planner, only ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ and ğ‘†ğ‘”ğ‘œğ‘ğ‘™ . The adversarial changes are specified at the beginning of the agentâ€™s
computation. The agent needs to find a plan taking these changes into account.

4 APPROACH
This section presents the window-heuristic to find adversarial changes. The window-heuristic
outputs a set of ğ‘˜ grounded actions to remove the instance (O) to increase the cost of the plan.

One could use the min-cut algorithm to find a minimum cut of the state space. Here, we would
partition the initial state and the goal state(s). The min-cut algorithm removes edges, which
represent grounded actions in the state-space. When all the edges from the cut are removed by an
adversary, there would be no path between ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ and ğ‘†ğ‘”ğ‘œğ‘ğ‘™ , meaning no plan actually exists and the

, Vol. 1, No. 1, Article . Publication date: May 2022.

Adversarial Planning

7

adversary has succeeded. Unfortunately, the min-cut algorithm does not guarantee to cut less than
ğ‘˜ edges, i.e., less than the number of grounded actions that an adversary is able to prevent. The
min-cut problem can be solved in polynomial time in the size of the input graph [28]. However,
in the general case, the state-space has an exponential number of nodes in the length of the task
definition, thus making the min-cut algorithm inappropriate for finding adversarial examples.

One might alternately try all possible changes and keep the most adversarial one, i.e., brute-force.
More formally, given a task ğ‘‡ = (ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘†ğ‘”ğ‘œğ‘ğ‘™, O) we do the following: (1) Compute a plan for task
ğ‘‡ . (2) For every action ğ´ğ‘– in this plan, compute a solution of (ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘†ğ‘”ğ‘œğ‘ğ‘™, O \ {ğ´ğ‘– }). (3) Keep the
most adversarial action ğ´ ğ‘— , i.e., the one that maximally increases the cost of the initial plan. This
brute-force approach guarantees to find the best adversarial change if the plan computed in (1)
is optimal. However, it requires that we run a planner ğ‘ + 1 times, where ğ‘ is the length of the
initial plan. This is not practical because the search for a solution is generally computationally
intensive [29, 30].

Even the most sophisticated planners can fail to find an existing solution. Moreover, the brute-
force approach only outputs a single adversarial change. In order to output two adversarial changes,
an adversary would have to run the planner once for every pair of actions in the initial plan,
i.e., ğ‘ (ğ‘ âˆ’ 1) + 1. To output ğ‘˜ adversarial changes, the adversary would have to run their planner
ğ‘‚ (ğ‘ğ‘˜ ) times. Additionally, if the adversary does not find a solution to the initial problem during
step (1), it is impossible to run step (2). Hence, brute-forcing is not practical in the general case: it
is computationally intensive and assumes that an adversary has a planner as equally sophisticated
as the agent.

We develop an approximation algorithm to output adversarial changes. We formally show that
finding the ğ‘˜ adversarial changes is at least NP-Hard (Appendix B). The window-heuristic enables
us to bring planning instances of arbitrary size into a tractable scope. The intuition is as follows: our
strategy is to store known successful attacks from smaller (tractable) problems and project them
onto larger (intractable) planning instances. Similar methods have been shown to enhance planning
algorithms by reducing the number of nodes explored [31]. Here we model a large problem as a
graph. Here, the goal for an adversary is to identify a small region of that graph (i.e., a window)
without being able to see the entire graph. To achieve this, the adversary will walk through the
large graph to observe if the current region matches a previously observed region. Once a match is
found, the adversary executes a known attack on the region.

Formally, we define a window as a connected sub-graph of the state-space, parameterized by ğ‘› (a
state-space is a graph where the nodes are the states and the directed edges are the grounded actions).
Hence, a window contains ğ‘› nodes (i.e., states) and at least ğ‘› âˆ’ 1 edges (i.e., grounded actions) as it
is connected. Figure 4c is a visual representation of a window for the air cargo transportation
domain. The formulation of our attack is divided into two parts: a generation phase and an execution
phase.
Window Generation - The adversary creates a table of windows, that are known to be adversarial
(i.e., when applied, the cost of the plan increases). To create the table, the adversary generates
several simpler planning instances (all from the same domain). The instances should be simple
enough for the adversary to brute force them within the reduced state-space, i.e., run step (1) and
(2) from the previously explained brute-force algorithm (Section 4). When the most adversarial
action is found, we extract a window around it (Section 4.1 and 4.2).
Window-Heuristic Execution - Once the table is generated, the adversary is given a planning
task and searches for adversarial changes with the window-heuristic (As described in Section 4.3).
The adversary runs their own planning algorithm, computes a solution, and applies the window-
heuristic. The adversary slides the windows from the table on the state-space. A match occurs

, Vol. 1, No. 1, Article . Publication date: May 2022.

8

Vie et al.

(a)

(b)

(c)

Fig. 4. (a) An adversarial 3 Ã— 3 window in path-finding. Placing a wall where the â€œXâ€ is will prevent the agent
from reaching the goal if it approaches from the bottom or the right of the window. As the only way to
reach the goal is left, the overall path length is likely increased. (b) the (non-adversarial) window is not likely
to increase the cost of the plan. (c) An adversarial window (ğ‘› = 4, 4 states, 3 actions) from the air cargo
domain: removing the last actionâ€”Unload c5 from p2 at PHX makes planning fail.

when the windows are isomorphically equivalent (further discussed in Section 4.2), and thus, we
output the associated grounded action (as shown in Fig. 7).

4.1 The Window-View
An adversary links a window and an adversarial change. We say we apply the window when we
remove the grounded action associated with it from the set of grounded actions, O. Intuitively,
when we attack a planning instance, we apply a window when we see a matching one in the
state-space of the arbitrary problem. Windows can take different shapes and sizes depending on
the class domain.

For path-finding domains, we choose a window to be a local ğ‘› Ã— ğ‘› node view of the agentâ€™s
surroundings (Fig. 4a, 4b). We define a wall to be a node the agent cannot reach. The adversarial
change associated with the ğ‘› Ã— ğ‘› view is a wall at the center of the window. We apply the window
when we see the same arrangement of walls in the environment. In this sense, to apply a window
means to add a wall at the center of the window.

For STRIPS-written problems, a window is a succession of ğ‘› states linked by ğ‘› âˆ’ 1 actions. We
apply a window when we see an equivalent succession of the ğ‘› âˆ’ 1 first states in the state-space.
The adversary applies a window by removing the last grounded action in that window from O
(i.e., the ğ‘› âˆ’ 1ğ‘¡â„). As shown in Figure 4c, preventing (previously loaded) cargo from being unloaded
is likely to be adversarial.

Window-size is chosen empirically. Ideally, it should be a function of the considered domain to
maximize the success rate of the adversary. In the rest of this paper, we set ğ‘› = 3 for path-finding
domains and ğ‘› = 4 for STRIPS domains. Assuming a fixed number of entries in the table and a
larger window size, finding an equivalent window is less likely to happen because it needs to be
found over the entire sub-graph described by the window. On the other hand, with larger window

, Vol. 1, No. 1, Article . Publication date: May 2022.

(At, c5, PHL)(At, p2, PHL)(In, c5, p2)  (At, p2, PHL)Load c5 in p2(In, c5, p2)  (At, p2, PHX)Fly p2 to PHX(At, c5, PHX) (At, p2, PHX)Unload c5 from p2Adversarial Planning

9

sizes, the probability to perturb the plan when a match is found is increased. Indeed, the larger the
window, the more alike the sub-graph on which we match the window has to be. The adversarial
change within the window is more specialized and thus has a better chance to be adversarial.

4.2 The Table of Advantageous Windows
To create a table containing the most effective adversarial windows, the adversary generates several
random simple problems and extracts the most adversarial windows with an exhaustive search.
Then, the adversary adds them to the table only if no other equivalent window (i.e., isomorphic) is
already in the table. Without an equivalence relation the adversary would end up with, potentially,
an exponential number of entries in the table. The adversary can also limit the number of predicates
in each node of a window using a normalization process. When we extract a window for STRIPS
instances, we capture ğ‘› states. Each of those states contains hundreds of predicates to describe
the entire environment. However, because the environment does not change drastically within a
window, many predicates remain unchanged across the ğ‘› states. Finally, while creating the table,
we also compute how frequently a window is adversarial. We can threshold the table to only keep
the windows with the highest frequency. In doing so, we get a higher probability to increase the
cost of the plan when we apply a window. Intuitively, adversarial windows that are frequently
observed in smaller problems are more likely to increase the cost in larger problems.

Note that selecting random examples to generate windows is appropriate when the adversary
has no knowledge of the instances expected at run time. In truth, table creation could be improved
(perhaps vastly) by using examples of instances (or similar instances) likely to be encountered by
the target at run time. Indeed, in practice an intelligent adversary would collect known instances
and â€œtrainâ€ the window heuristic generation to find advantageous windows representative of those
encountered by the victim system. We leave actively investigating other training approaches to
future work.
Graph Isomorphism - The predicates describing a state are grounded, meaning they do not contain
any free variable. A state would contain the predicate (At, p1, JFK) instead of (At, plane, airport).
This notation is dependent on how we choose to name the airports and the planes. We need an
equivalence relation that does not rely on the objectsâ€™ names. Consider the following predicates
(At, p1, JFK) and (At, plane1, airport1). Renaming p1 to plane1 and JFK to airport1 gives us
the equivalence. For STRIPS-written tasks, we say that window ğ‘¤1 is isomorphically equivalent
to window ğ‘¤2 if (1) there is a bijection ğ‘“ between the name of the objects such that ğ‘¤1 = ğ‘“ (ğ‘¤2),
plane1 = ğ‘“ (p1) and (2) ğ‘¤1 and ğ‘¤2 share the same non-grounded operators (Fig. 5). For path-finding,
we say that two windows are equivalent if there is a rotation ğ‘Ÿ such that ğ‘¤1 = ğ‘Ÿ (ğ‘¤2).
Normalizing - Normalizing is a process we only perform for STRIPS windows because path-finding
states are not defined with predicates. We normalize to remove constant predicates in an extracted
window. Consider a window (ğ‘› = 4) containing four states and three grounded actions extracted
from the air cargo transportation domain. The planning task can contain an arbitrary number
of plane objects. Each plane would need a predicate to indicate its position: (At, plane, airport).
However, if a plane does not move, the predicates concerning its position do not change and are
repeated across the four states. Thus, they are not relevant to the evolution of the environment
within the four states. In Figure 6, none of the three actions affect plane p1, so we remove all the
predicates concerning p1 in the window. The only predicates that are going to change between
the four states are the ones modified (i.e., added, deleted) by the three grounded actions. More
formally, we call the four states in the window ğ‘†âŸ¦1;4âŸ§. Each ğ‘†ğ‘– is a set of predicates. We introduce
Î” = (cid:209)ğ‘–=âŸ¦1;4âŸ§ ğ‘†ğ‘– and we define âˆ€ğ‘– âˆˆ âŸ¦1; 4âŸ§, Ë†ğ‘†ğ‘– = ğ‘†ğ‘– \ Î”. The Ë†ğ‘†ğ‘– are the new normalized states where
we removed the redundant predicates.

, Vol. 1, No. 1, Article . Publication date: May 2022.

10

Vie et al.

Fig. 5. Two equivalent windows for the air cargo transportation domain. The actions are similar (Fly, Fly,
Unload) and there is a bijection between the objects. The bijection is the following: ğ‘“ (ğ‘1) = ğ‘3, ğ‘“ (ğ½ ğ¹ğ¾) = ğ‘†ğ¹ğ‘‚,
ğ‘“ (ğ‘ƒğ»ğ‘‹ ) = ğ¿ğ´ğ‘†, ğ‘“ (ğ‘2) = ğ‘1, ğ‘“ (ğ‘ƒğ»ğ¿) = ğ‘ƒğ»ğ‘‹ , and ğ‘“ (ğ‘4) = ğ‘5.

Fig. 6. The window before normalization (top) and after (bottom). All the predicates that werenâ€™t modified by
the three actions were removed, e.g., (At, p1, PHX) is removed.

Thresholding - With the normalizing process and isomorphisms, we drastically decrease the size
and the number of entries needed in the table. We introduce the threshold in order to only keep
the windows that appeared the most in the table. Indeed, a window might be adversarial for 50%
of the problems, and another one might be adversarial for 1% of the problems. Still, both of those
windows appear in the table with the same importance. To address this, once the table is filled, we
empirically select only the most frequently observed windows. If a window is adversarial once over
a thousand tasks, there is no need to record itâ€”it is too specific.

The formal process to create the table is the following (Algorithm 1). (1) Generate several random
simple tasks from of the domain considered. (2) Find the most malicious windows using a brute-force
approach for all of those (line 4 to 10). (3) Extract a window around the adversarial change with
the highest cost increase. (4) Normalize the window if needed and add it to the table (line 12 to 18).
Finally, (5) threshold the table to only keep the most adversarial windows (line 21).

, Vol. 1, No. 1, Article . Publication date: May 2022.

(At, p3, SFO)(At, p1, PHX)(In, c5, p1)(At, p3, LAS)(At, p1, PHX)(In, c5, p1)Fly p3 to LAS(At, p3, LAS)(At, p1, SFO)(In, c5, p1)Fly p1 to SFOUnload c5 from p1(At, p3, LAS)(At, p1, SFO)(At, c5, SFO)(At, p1, JFK)(At, p2, PHL)(In, c4, p2)(At, p1, PHX)(At, p2, PHL)(In, c4, p2)Fly p1 to PHX(At, p1, PHX)(At, p2, JFK)(In, c4, p2)Fly p2 to JFKUnload c4 from p2(At, p1, PHX)(At, p2, JFK)(At, c4, JFK)(At, p1, PHX)(At, p2, PHL)(At, p3, PHL)(At, p4, PHX)(At, c1, PHX)(In, c2, p4)(At, c3, JFK)(In, c4, p3)Unload c4 from p3 at PHLLoad c1 in p4 at PHXFly p2 from PHL to JFK(At, p1, PHX)(At, p2, PHL)(At, p3, PHL)(At, p4, PHX)(At, c1, PHX)(In, c2, p4)(At, c3, JFK)(At, c4, PHL)(At, p1, PHX)(At, p2, PHL)(At, p3, PHL)(At, p4, PHX)(In, c1, p4)(In, c2, p4)(At, c3, JFK)(At, c4, PHL)(At, p1, PHX)(At, p2, JFK)(At, p3, PHL)(At, p4, PHX)(In, c1, p4)(In, c2, p4)(At, c3, JFK)(At, c4, PHL)(At, p2, PHL)(At, c1, PHX)(In, c4, p3)(At, p2, PHL)(At, c1, PHX)(At, c4, PHL)Unload c4 from p3 at PHL(At, p2, PHL)(In, c1, p4)(At, c4, PHL)Load c1 in p4 at PHXFly p2 from PHL to JFK(At, p2, JFK)(In, c1, p4)(At, c4, PHL)Adversarial Planning

11

Algorithm 1: Constructing a table of advantageous adversarial windows using N random
simple problems. For each of those problem we extract the most adversarial window and
add it to the table.
1 for ğ‘– â† 1 to ğ‘ do
2

Generate a random problem ğ‘ƒ;
Solve problem ğ‘ƒ with a planner;
Let ğ¶ be its cost and ğ‘ƒğ¿ the plan;
Let ğ‘ğ‘’ğ‘ ğ‘¡ğ´ğ‘‘ğ‘£ğ¶ be the best adversarial cost;
Let ğ‘ğ‘’ğ‘ ğ‘¡ğ´ğ‘‘ğ‘£ğ´ğ‘ğ‘¡ be the best adversarial action;
foreach action ğ´ in the plan ğ‘ƒğ¿ do

Solve problem ğ‘ƒ without the action ğ´ allowed;
Update ğ‘ğ‘’ğ‘ ğ‘¡ğ´ğ‘‘ğ‘£ğ¶ and ğ‘ğ‘’ğ‘ ğ‘¡ğ´ğ‘‘ğ‘£ğ´ğ‘ğ‘¡;

end
if ğ‘ğ‘’ğ‘ ğ‘¡ğ´ğ‘‘ğ‘£ğ¶ > ğ¶ then

Take a window ğ‘Š around ğ‘ğ‘’ğ‘ ğ‘¡ğ´ğ‘‘ğ‘£ğ´ğ‘ğ‘¡;
Normalize the window ğ‘Š ;
if ğ‘Š has an equivalent in the table then

Add 1 to the number of occurrences of ğ‘Š ;

else

Add ğ‘Š to the table;

end

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

end

19
20 end
21 Threshold the table;
22 return the table;

4.3 The Window-heuristic
We now explain how to use the table to run the window-heuristic and output adversarial changes.
An adversary and the agent are given an arbitrary size problem (ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘†ğ‘”ğ‘œğ‘ğ‘™, O). The goal for the
agent is to find an optimal plan. The goal for an adversary is to output the best set of grounded
actions to perturb the plan. We distinguish the offline case (STRIPS-written problems) from the
online case (path-finding). Those two cases differ in the attack scenario: in the offline case, the goal
for an adversary is to output ğ‘˜ adversarial changes before the agent starts planning. For the online
case, an adversary applies these changes directly and perturbs the agentâ€™s environment while the
agent is planning. That means the agent can react (find another plan) to an adversaryâ€™s changes.
Offline Window-heuristic - This procedure is detailed in Algorithm 2. An adversary explores
the state-space by running a separate planner, to find the solution to the input task (Recall, the
adversary knows at least the initial and goal states). In doing so, an adversary expects to expand
the state-space in the same direction(s) as the agent. If an adversary successfully predicts the
state-space expansion, applying windows hindering that expansion will be adversarial for the
agent. The search starts from ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ and stops either when an adversaryâ€™s planner returns or when ğ‘˜
adversarial changes have been found. Essentially, when an adversary recognizes a window from
the table during the state-space expansion, the adversarial change associated is applied. During
the search, we extract a window around the next state to be expanded by the adversaryâ€™s planner:
ğ‘†ğ‘ğ‘‘ğ‘£
ğ‘›ğ‘’ğ‘¥ğ‘¡ (line 4). We check if this window has a match in the table. If it does, we apply the adversarial

, Vol. 1, No. 1, Article . Publication date: May 2022.

12

Vie et al.

Algorithm 2: Offline window-heuristic: Given an offline planning instance it outputs a set
of adversarial changes.
Input: Number of adversarial changes allowed ğ‘˜;

The initial state ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ ;
The goal state characterization ğ‘†ğ‘”ğ‘œğ‘ğ‘™ ;
The set of operators O;
The advantageous windows table ğ‘Š ğ‘¡ğ‘ğ‘ğ‘™ğ‘’;

Output: A set of grounded actions ğ‘†;

1 ğ‘†ğ‘’ğ‘¥ğ‘ğ‘ğ‘›ğ‘‘ğ‘’ğ‘‘ = ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ ;
2 while ğ‘†ğ‘’ğ‘¥ğ‘ğ‘ğ‘›ğ‘‘ğ‘’ğ‘‘ â‰  ğ‘†ğ‘”ğ‘œğ‘ğ‘™ and |ğ‘† | < ğ‘˜ do
3

Find ğ‘†ğ‘ğ‘‘ğ‘£
ğ‘›ğ‘’ğ‘¥ğ‘¡ with search algorithm;
Take a window ğ‘Š around ğ‘†ğ‘ğ‘‘ğ‘£
ğ‘›ğ‘’ğ‘¥ğ‘¡ ;
Normalize ğ‘Š ;
Search for an equivalent of ğ‘Š in ğ‘Š ğ‘¡ğ‘ğ‘ğ‘™ğ‘’;
if an equivalent has been found then

Extract the adversarial change from ğ‘Š ;
Add it to the the set ğ‘†;
Remove the action from O;
Find a new ğ‘†ğ‘ğ‘‘ğ‘£

ğ‘›ğ‘’ğ‘¥ğ‘¡ with search algorithm;

4

5

6

7

8

9

10

11

12

end
ğ‘†ğ‘’ğ‘¥ğ‘ğ‘ğ‘›ğ‘‘ğ‘’ğ‘‘ = ğ‘†ğ‘ğ‘‘ğ‘£
ğ‘›ğ‘’ğ‘¥ğ‘¡

13
14 end
15 return ğ‘†;

change to the planning task (line 10). In Figure 7, we show how the adversarial change associated
with the window recognized is removed from the plan.
Online Window-heuristic - For the online case, we do not need to pre-determine a set of ad-
versarial changes. The adversarial changes are applied at run-time and the agent needs to adapt.
The window-heuristic for an online planning task runs about the same way except an adversary
does not run a separate planner. Indeed, there is no need to predict the agentâ€™s entire state-space
expansion because we assume an adversary knows ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ at all times. ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ is the position of
the agent and is updated as the agent moves (i.e. executes actions in the environment). We only
need to predict the next state to be visited by the agent ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ where ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ is defined as:

ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ =

arg min
ğ‘† âˆˆğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘ ğ‘œğ‘Ÿ (ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ )

[ğ»ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ (ğ‘†)]

(1)

Each time the agent moves and updates ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ , an adversary computes an estimation of ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡
(line 3). In order to estimate ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ , an adversary uses the search heuristic ğ»ğ‘ğ‘‘ğ‘£. The state ğ‘†ğ‘ğ‘‘ğ‘£
ğ‘›ğ‘’ğ‘¥ğ‘¡
predicted by an adversary is ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ â€™s neighbor with the lowest ğ»ğ‘ğ‘‘ğ‘£ image:

ğ‘†ğ‘ğ‘‘ğ‘£
ğ‘›ğ‘’ğ‘¥ğ‘¡ =

arg min
ğ‘† âˆˆğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘ ğ‘œğ‘Ÿ (ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ )

[ğ»ğ‘ğ‘‘ğ‘£ (ğ‘†)]

(2)

An estimation of ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ is calculated, the adversary can apply the same mechanism as the offline
case. If the adversary recognizes a window around ğ‘†ğ‘ğ‘‘ğ‘£
ğ‘›ğ‘’ğ‘¥ğ‘¡ , we apply the adversarial change (line 9).

, Vol. 1, No. 1, Article . Publication date: May 2022.

Adversarial Planning

13

Algorithm 3: Online window-heuristic: Given an online planning instance it perturbs the
agentâ€™s plan at run-time.
Input: Number of adversarial changes allowed ğ‘˜;

The current state of the agent ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ ;
The goal state characterization ğ‘†ğ‘”ğ‘œğ‘ğ‘™ ;
The set of operators O;
The lookup table ğ‘Š ğ‘¡ğ‘ğ‘ğ‘™ğ‘’;

1 ğ´ğ‘ â† 0;
2 while ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ â‰  ğ‘†ğ‘”ğ‘œğ‘ğ‘™ and ğ´ğ‘ < ğ‘˜ do
3

Estimate ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ using ğ»ğ‘ğ‘‘ğ‘£, ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ and ğ‘†ğ‘”ğ‘œğ‘ğ‘™ ;
Take a window ğ‘Š around ğ‘†ğ‘ğ‘‘ğ‘£
ğ‘›ğ‘’ğ‘¥ğ‘¡ ;
Normalize ğ‘Š ;
Search for an equivalent of ğ‘Š in ğ‘Š ğ‘¡ğ‘ğ‘ğ‘™ğ‘’;
if an equivalent has been found then

Extract the adversarial change from ğ‘Š ;
Apply the adversarial change;
ğ´ğ‘ â† ğ´ğ‘ + 1;
// The agent has to re-plan.

end
Wait for ğ‘†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ to change;
// The agent has re-planned and moved.

4

5

6

7

8

9

10

11

12

13 end

Planning type

Domain

Agentâ€™s planner

Threat model

Success Rate

Online
Online
Online
Offline
Offline
Offline
Offline
Offline
Offline
Offline

Maze
Maze
Maze
Barman
Floortile
Hiking
Tetris
Airport
Openstacks
Data-network

D* Lite
D* Lite
D* Lite
Fast Downward
Fast Downward
Fast Downward
Fast Downward
Fast Downward
Fast Downward
Fast Downward

Agentâ€™s heuristic and Informed
Agentâ€™s heuristic
Black-box
Black-box
Black-box
Black-box
Black-box
Black-box
Black-box
Black-box

66.86% - 82.65%
57.53% - 79.73%
60.41% - 82.75%
85.71%
95.00%
75.00%
70.59%
100.00%
100.00%
41.67%

Table 1. All threat models and domains considered for the evaluation. Ranges in the success rates signify
the 1 wall to 2 wall success rate while concrete values are when an adversary has up to 4 grounded actions.

5 EVALUATION
In this section, we evaluate our approach in several planners and domains for both online and
offline attacks. Table 1 summarizes the experiments and results. We begin by exploring D*Lite. We
ask:

(1) Given an arbitrary planning task, how often can an adversary increase the cost? (i.e., the

success rate)

(2) Given a set of instances what is the average cost increase an adversary can expect to generate?
(3) What is the minimum knowledge an adversary needs in order to perturb a planning algorithm?

, Vol. 1, No. 1, Article . Publication date: May 2022.

14

Vie et al.

Fig. 7. An adversary expands the state space starting with the initial state and discovers ğ‘†1 and ğ‘†2. Assume
ğ»ğ‘ğ‘‘ğ‘£ (ğ‘†1) < ğ»ğ‘ğ‘‘ğ‘£ (ğ‘†2), then ğ‘†1 is the next state to expand. After discovering ğ‘†1â€™s successors, an adversary
checks if the windows ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ âˆ’ ğ´1 âˆ’ ğ‘†1 âˆ’ ğ´3 âˆ’ ğ‘†3 and ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ âˆ’ ğ´1 âˆ’ ğ‘†1 âˆ’ ğ´4 âˆ’ ğ‘†4 have a match in the table.
Assuming no match is found, it continues until a match is discovered at ğ‘†2 âˆ’ ğ´5 âˆ’ ğ‘†5 âˆ’ ğ´6 âˆ’ ğ‘†6.

First, note that not all planning tasks are sensitive to adversarial changes. There may be multiple
disjoint plans to reach a goal state. Still, as we found, an adversaryâ€”depending on their knowledgeâ€”
can expect a success rate from 70% to 95% with up to 4 adversarial changes. This value highly
depends on the domain and the generation phase parameters (threshold, number of tasks generated,
nature of tasks generated, window size...). On average, with up to 4 adversarial changes, an adversary
will produce a plan from 1 to 10 steps longer (when the task is still solvable). For the black-box
online attack (defined in Section 3.1) for path-finding tasks in mazes, we achieve a success rate of
56.52% with one wall and 78.98% with two walls (Section 5.1).

5.1 D*Lite
We evaluate the success rate of the window-heuristic for an agent using the path-finding algorithm
D*Lite[6] in a maze. Here are the details of the experimental setup:

â€¢ The agent uses D*Lite combined with the Euclidean distance to guide the search for a solution:
â„ğ¸ğ‘¢ğ‘ğ‘™ğ‘–ğ‘‘ğ‘’ğ‘ğ‘› (ğ‘ ) = âˆšï¸(ğ‘ ğ‘¥ âˆ’ ğ‘”ğ‘œğ‘ğ‘™ğ‘¥ )2 + (ğ‘ ğ‘¦ âˆ’ ğ‘”ğ‘œğ‘ğ‘™ğ‘¦)2. When the agentâ€™s heuristic is unknown (Black-
box scenario), an adversary uses the Manhattan distance: â„ğ‘€ğ‘ğ‘›â„ğ‘ğ‘¡ğ‘¡ğ‘ğ‘› (ğ‘ ) = |ğ‘ ğ‘¥ âˆ’ ğ‘”ğ‘œğ‘ğ‘™ğ‘¥ | + |ğ‘ ğ‘¦ âˆ’
ğ‘”ğ‘œğ‘ğ‘™ğ‘¦ |.

â€¢ There is only one goal state in the maze (ğ‘†ğ‘”ğ‘œğ‘ğ‘™ ). The goal for the agent is to reach it from the

initial state (ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ ).

, Vol. 1, No. 1, Article . Publication date: May 2022.

Initial StateS1S2S3S4S8S9S5S6S7A1A3A4A6A9A8A2A5A7Adversarial Planning

15

(a)

(b)

(c)

Fig. 8. The three bar graphs show the success of an adversary placing walls in mazes. The threat models are
the following (a) Agentâ€™s heuristic and Informed (b) Agentâ€™s heuristic (c) Black-box. The bars on the left (resp.
right) describe the success rate of an adversary capable of placing one wall (resp. two walls).

â€¢ The agent is not allowed to move in diagonals, the only actions authorized are to move up,

down, left, or right.

The goal of an adversary is to select the optimal location in a maze to place a wall in order to
maximally increase the length of the path. The first step is to create the table of windows explained
in Section 4.2. Recall that the creation of the table is an offline process. To do so, we generate 500
random mazes (size 15 Ã— 15 - wall frequency 0.25) and we brute-force the best adversarial wall. For
each tile on the initial path, we try to place a wall and we compute the path cost of the modified
maze. We extract a 3 Ã— 3 window around the most adversarial wall and we add it to the table.
Appendix C, shows the empirical settings for the generation phase that gave the best experimental
results (success rate, average path cost increase).

Once the table of advantageous adversarial windows is created, we can run the window-heuristic.
The agent is given a maze instance and tries to reach a goal position. The adversary places walls as
the agent moves increasing the number of steps required for the agent. We distinguish the three
different online scenarios.
Agentâ€™s Heuristic and Informed, Online - We start with a powerful adversary knowing ğ»ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡
and ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ . Figure 8a shows an increase the cost of the plan by adding 1 wall in 66.86% of all cases.
Note that not all mazes can have their cost perturbed; sometimes one wall is not enough if two or
more disjoint but equally optimal paths exist. We reach a success rate of 82.65% when the adversary
has the opportunity to place two walls in the way of the agent. On average, we increase the path by
2.48 steps when the adversary adds 1 wall and by 4.62 steps when allowed to place up to 2 walls.
Agentâ€™s Heuristic, Online - This scenario is harder for the adversary and more realistic. The
adversary only has access to the heuristic the agent is using, its current position, and the goal state.
Since we do not know which way the agent is going in that case, the adversary has to estimate
the next move of the agent using ğ»ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ . With one wall we modify on average 57.53% (Fig. 8b) of
the plans and 79.73% with two walls (all sizes of mazes considered). As we can expect the efficiency
drops, but by 5-10% which is still a notable success rate because statistically, the adversary still
manages to increase the cost. We increase it by 2.27 steps with 1 wall and by 4.44 with 2 walls.
Black-Box, Online - The adversary does not have access to the heuristic the agent is using.
The adversary has to create the table and predict the ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ using a different heuristic. We choose
ğ»ğ‘ğ‘‘ğ‘£ = â„ğ‘€ğ‘ğ‘›â„ğ‘ğ‘¡ğ‘¡ğ‘ğ‘› â‰  ğ»ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ . This will answer the question about the transferability (between ğ»ğ‘ğ‘‘ğ‘£
and ğ»ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ ) of the attack. The agent however solves the maze using the Euclidean distance. With 1

, Vol. 1, No. 1, Article . Publication date: May 2022.

16

Vie et al.

wall the adversary achieves a success rate of 60.41% and increase the cost on average by 2.53 steps
(Fig. 8c). With 2 walls we have a high success rate of 82.75% and a cost increase of 5.48 steps.
Takeaways - To summarize, we found that an adversary equipped with the window-heuristic is
able to successfully increase the plan cost. The adversary needs little knowledge to perturb these
kinds of path-finding problems. The difference between the success rate of the black-box and the
informed scenario is small. Intuitively, the estimation of ğ‘†ğ‘›ğ‘’ğ‘¥ğ‘¡ in the black-box scenario is often
correct, given the limited number of actions the agent can take. Also, the window-heuristic scales
well with the problem size and the success rate stays constant with different maze sizes.

All of the experiments here considered an online setting. We now move to an evaluation of the

offline setting with an adversary running the heuristic before the agent starts to plan.

5.2 Fast Downward
We show the efficiency of the window-heuristic on diverse tasks solved by the Fast Downward
(FD) planner. The planner solves STRIPS instances with a large variety of settings. The user can
specify the search strategy (e.g., A*, Greedy search, hill-climbing...) and the search heuristicâ€”called
evaluator (e.g., FF, CEA, and Landmark-count). Fast Downward also translates the STRIPS instance
into other data structures to enhance the search for a solution. The Fast Downward planner solves
a planning instance in three phases: translation, knowledge compilation, and search. During the
translation phase, the FD planner performs grounding of predicates and operators [20], basically
creating the set of grounded operators, O. Concretely, our algorithm removes ğ‘˜ grounded actions
from O at translation phase. In practice, an adversary would have to prevent those actions.

The planner the adversary can run is a simple forward search planner running breadth-first
search (BFS) or A* coupled with a single heuristic (Additive Cost [25], etc.). This self-made planner
run by the adversary is less sophisticated compared to FD. We evaluate an attack in which the
adversary does not know how the FD planner works; only the domain specifications, ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ and ğ‘†ğ‘”ğ‘œğ‘ğ‘™
are known. This attack is considered to be black-box offline.

We evaluate the window-heuristic a in the following steps: (ğ‘) We generate tables as described
in Table 2 (ğ‘) and run the FD planner to output a plan without the adversary interfering. The agent
runs FD with the FF heuristic [32] and context-enhanced additive heuristic [33] with lazy best-first
search and preferred operators.2 (ğ‘) We run the window-heuristic with the adversaryâ€™s planner to
output ğ‘˜ adversarial change(s), i.e., the grounded operator(s) that will be removed from O (ğ‘‘) We
run FD again (same settings) with the limited set of operators. (ğ‘’) Finally, we compare the cost of
the plan with and without an adversary.

We benchmark tasks and domains including several FD-based planners that performed the best

during the 2014 IPC competition as well as several real-world applications:

â€¢ The airport domain tasks aim to control the ground traffic at an airport. Airplanes must
reach their destination gate. There is outbound and inbound traffic; the former are airplanes
that must take off, the latter are airplanes that have just landed and have to park [34]. The
instances we ran were based on the Munich Airport3.

â€¢ The data-network domain tackles distributed computing related problems. In a given network
of servers, each server can produce data by processing some existing data and sends this
to other servers on the network. The goal is to process data dispatched across servers with
variable hardware and connection capabilities while minimizing the processing cost.

2This was one of the best configurations available according to the benchmarks run by Helmert in 2006 [20].
3Hatzack developed a realistic simulation tool, which he supplied to the IPC organizers to generate the domain instances.
The simulator included Frankfurt, Zurich, and Munich airports. Frankfurt and Zurich proved too large for IPC purposes [34].

, Vol. 1, No. 1, Article . Publication date: May 2022.

Adversarial Planning

17

Domain

Problem Threshold Window Algorithm

Barman
Floortile
Hiking
Tetris
Airport
Openstacks
Data-network

200
200
100
200
15
10
15

10
1
3
1
5
0
0

5
35
11
24
8
34
19

A*
A*
BFS
A*
A*
A*
A*

ğ»ğ‘ğ‘‘ğ‘£

Additive cost
Additive cost
âˆ…
â„ğ¶ğ‘šğ‘œğ‘›ğ‘ƒğ‘Ÿğ‘’ğ‘‘
Additive cost
Additive cost
â„ğ¶ğ‘šğ‘œğ‘›ğ‘ƒğ‘Ÿğ‘’ğ‘‘

Table 2. The generation processâ€™ settings across domains. The problem corresponds to the number of problems
generated to fill the table, the window is the number of windows kept in the table, and the algorithm is the
algorithm used by the adversary. â„ğ¶ğ‘šğ‘œğ‘›ğ‘ƒğ‘Ÿğ‘’ğ‘‘ (ğ‘ ) is defined as |ğ‘”ğ‘œğ‘ğ‘™ğ‘ğ‘Ÿğ‘’ğ‘‘ | âˆ’ |ğ‘”ğ‘œğ‘ğ‘™ğ‘ğ‘Ÿğ‘’ğ‘‘ âˆ© ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ |.

â€¢ Finally, the Openstacks domain is based on the â€œminimum maximum simultaneous open
stacksâ€ combinatorial optimization problem. A manufacturer has orders for a combination of
different products and can only make one product at a time, e.g., schedule order completion
(NP-hard).

(a)

(b)

(c)

(d)

(e)

Fig. 9. Domains: (a) barman, (b) floortile, (c) hiking, (d) tetris and (e) data-network. We benchmarked
the window-heuristic using all the tasks available in the optimizing track. We show the success rate of the
window-heuristic depending on the number of adversarial changes allowed (adversarial budget).

The results of our experiments are multifaceted. First, the most readily perturbed domains
(i.e., most vulnerable to attack) were the ones where the goal cannot be divided into independent
sub-goals. Assuming a planning instance can be divided into multiple independent sub-planning
tasks with sub-goals, an agent can find a sub-plan to reach each sub-goal independently. The agent
then assembles each sub-plan in an arbitrary order to create the final plan. An adversarial change
is likely to perturb the sub-plan to achieve one of the sub-goals, but not the other sub-plans. For

, Vol. 1, No. 1, Article . Publication date: May 2022.

18

Vie et al.

Domain

1

Number of adversarial changes allowed
2

7

5

4

6

3

8

9

10

3.64
0.21
Barman
3.95
2.6
Floortile
1.55
2.2
Hiking
10.47
9.18
Tetris
4.72 âˆâˆ—
Airport
âˆâˆ—
âˆ
Openstacks
87.67
Data-network 56.75

4.86
4.65
1.05
15.00
âˆ
âˆ
34.42

8.57
5.05
1.05
16.47
âˆ
âˆ
73.83

5.21
4.55
1.05
14.18
âˆ
âˆ
72.83

2.71
4.05
1.05
15.00
âˆ
âˆ
-7.00

10.29
4.05
1.05
16.24
âˆ
âˆ
4.67

11.71
3.55
1.05
16.53
âˆ
âˆ
146.08

7.93
3.35
1.05
18.65
âˆ
âˆ
115.00

14.5
3.35
1.05
23.29
âˆ
âˆ
120.16

Table 3. Average path cost impact. The tasks becoming unsolvable are ignored (ğ‘ğ‘œğ‘ ğ‘¡ = +âˆ). In red, the
adversary decreases the cost of the plan (on average). Entries marked with âˆ— mean up to that point all
problems benchmarked become unsolvable, the plans have infinite cost and the average cost change is
infinite.

example, given a task in the air cargo transportation domain with two packages to deliver (c1,
c2). Each delivery is a sub-goal: we can deliver c1 first and then c2 or the opposite. An adversarial
change only perturbs one of the sub-goals; the agentâ€™s planner just has to find another sub-goal-plan.
This requires less work from the agent than finding another global plan. This is why the success rate
of the data-network domain remains around 50-60%. On the other hand, tasks from the floortile
and hiking domain cannot be divided into independent sub-goals and are consequently highly
perturbed (Figure 9). Figure 9 excludes the airport and openstacks domains as they become
unsolvable early on in the number of adversarial changes allowed.

Because the cost of removing an action from the plan may have real cost (see Section 6), the
number of changes may represent and important success parameter. For example, in Figure 9, the
data-network domain does not seem to be a successful attack (58.32% success rate with 4 grounded
actions to remove). However, over few data-network instances, an adversary would increase the
cost on average by 73.83 units with four adversarial changes.

Table 3 shows that, surprisingly, an adversary can also have a beneficial effect on the cost of the
planâ€”the introduction of an adversary improves the plan by decreasing its cost. Intuitively, the
introduction of the instance perturbation forces the planner (which does not guarantee optimality)
to explore areas of the state space incorrectly pruned by an aggressive planning algorithm. When
the agent explores these areas containing a shorter way to reach a goal state, the cost is decreased.
Usually, we expect the agentâ€™s heuristic to guide the agent to these cost-effective areas. However,
for some instances like the ones belonging to the data-network domain, the search heuristic is
sometimes ineffective because it produces plateaus. The agent usually breaks the ties by randomly
selecting the next state, which makes its behavior unpredictable. Applying adversarial changes
modifies the plateaus and may be favorable for the agent. This explains why the adversary has
sometimes such a beneficial effect for instances in the data-network domain.
Takeaways - We find that an adversary can be successful without knowing the agentâ€™s planner.
The success rate of the adversary decreases if the tasks perturbed can be divided into sub-tasks
with sub-goals. Overall, two adversarial changes are generally sufficient to efficiently perturb an
agentâ€™s task.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Adversarial Planning

19

6 REALIZING ATTACKS
One of the key questions one might ask is how an adversary can practically alter the plan instance.
Here we highlight the results of our ongoing survey of the security of real-world planning sys-
tems and demonstrate scenarios in which planning instances can be (and have been) targeted by
adversaries.
Transportation systems - Next-generation transportation systems use vehicle area networks to
exchange motion, location and hazard information (e.g., V2X [35]). These vehicle to vehicle messages
are used by internal planning systems in autonomous cars to determine how to make local and
global decisions [7, 8]. However, such messages can simply misreport the state of the environment,
therein allowing selfish behavior [36]. Here, the misreports will alter the maneuver (action) space of
the receiving victim cars, and therein alter the planning inputs as posited throughout. Anecdotally, a
recent low-tech attack on route planning was demonstrated by a performance artist Simon Weckert,
who created a virtual traffic jam by carrying 99 phones on an otherwise empty street. This fake
â€œcongestionâ€ (perturbation of the plan space) was avoided by mapping software of those in the
nearby area[37].
Motion planning - Motion planning in vision-based robotic systems is used at multi-scales to plan
for the movement or manipulation of objects within the environment [38]. However, it is known
that vision systems used in robotics can be profoundly affected by changes in light. In particular
darkness, shadows and reflection, possibly caused by an adversary, can inhibit the scene or object
interpretation/perception which can vastly alter the plan space [39].
Chemical manufacturing - Similar to other industrial manufacturing planning systems, chemical
production scheduling is the process of scheduling, delivering and retrieving chemical components
through a plant. Such planning is highly dependent on the correct understanding of the available
resource inventory and equipment states as recorded in plant databases [18]. An adversary who is
able to compromise the DB server (through techniques such as phishing, APT, or exploiting host
vulnerabilities) can alter the database to manipulate the planning of the production schedule.
Data center/cloud management - Data centers migrate virtual machines and containers to,
among other goals, balance load, reduce resource usage, and provide isolation for sensitive com-
putation. These migrations are most often coordinated using a discrete or continuous resource
planner [40]. Any adversary who is able to occupy a VM host or surrounding infrastructure and
generate network and/or computational load will change the resource signature and alter the data
center planning instance.

7 RELATED WORK
The inception of planning began with breadth-first search, designed by Moore et al. in 1959 [41]
to solve mazes. Later, Heuristic search was introduced in 1965 to better navigate the search space
by using a function to estimate the distance from the current position to the goal [42]. With the
creation of A* [43] and the development of the STRIPS language [44] (which introduced planning
formalism), state-space search arose. In 1986, Pednault et al. created the ADL language [45] with
looser requirements for problem formulation than STRIPS, which enabled state-space search
algorithms to tackle even more realistic problems.

Also falling within the domain of AI, machine learning algorithms have been similarly shown
to be vulnerable to malicious inputs [12, 13, 46]. Here several works have demonstrated how
slight but carefully calculated perturbations in an input (called adversarial examples) could control
deep learning systems. This emerging area of research inspired us to investigate the impact of
introducing an adversary to deterministic planning problems. Indeed, our research demonstrated

, Vol. 1, No. 1, Article . Publication date: May 2022.

20

Vie et al.

similar phenomena: small changes to the planning instance can have drastic consequences on the
output of a planner.

Other past, related efforts have explored the use of machine learning for path-finding. Xiang
et al. [47] found adversarial examples on a reinforcement learning technique, as applied to path-
finding. They performed similar experiments on mazes: adding walls on the way of the agent. Their
agent used a Q-learning algorithm [48]â€”a reinforcement learning techniqueâ€”to move to the goal.
Here, they focused on general mazes and did not employ any notion of windows as we explored
throughout. They focused only on path-planning, while we create a general method that can be
applied to any planning instance that can be expressed in the PDDL language [49] (or a subset of it,
e.g., STRIPS).

Thematically close but with different goals, Culberson and Schaeffer [31] introduced pattern
databases as an optimization for planners. Pattern databases store the exact number of moves
required to solve various sub-goals of a planning instance. This method enhances single-agent
search by reducing the total number of nodes explored. Similar to our work, the algorithm tries to
match patterns from a table (i.e., graph isomorphism) during a search through the state space.

8 CONCLUSION
This paper has explored adversarial capabilities in planning systems. We introduced an adversarial
heuristic and algorithm for an adversary to identify malicious modifications to the environment that
disrupt the plan to induce high cost or prevent the goal state from being reached. This algorithm
can be adapted to any kind of deterministic, single-goal planning problem online or offline and
scale with the size of the planning instance. For some domains the approach is successful in 60% to
95% instances if the adversary knows the desired goal and state of the agent.

In future work, we plan to explore defenses and measures of robustness. We will also explore
more complicated planning domains (e.g., multiple goals, irreversible actions, etc.) and methods for
training adversarial planning heuristics. As well as the practical impacts of manipulated planners
in situ. By experimenting and measuring the real-world impacts of manipulated planners, we can
understand the methods and degree to which planning system are vulnerable in the wild.

REFERENCES
[1] Anthony Stentz and Martial Hebert. A complete navigation system for goal acquisition in unknown environments.

Autonomous Robots, 2(2):127â€“145, Jun 1995.

[2] Alexander Koller and JÃ¶rg Hoffmann. Waking up a sleeping rabbit: On natural-language sentence generation with FF.

In ICAPS, 2010.

[3] Malte Helmert and Hauke Lasinger. The scanalyzer domain: Greenhouse logistics as a planning problem. In Proceedings
of the 20th International Conference on Automated Planning and Scheduling, ICAPS 2010, Toronto, Ontario, Canada, May
12-16, 2010, pages 234â€“237, 2010.

[4] William I. Bullers, Shimon Y. Nof, and Andrew B. Whinston. Artificial intelligence in manufacturing planning and

control. A I I E Transactions, 12(4):351â€“363, 1980.

[5] Mark S Boddy, Johnathan Gohde, Thomas Haigh, and Steven A Harp. Course of action generation for cyber security

using classical planning. In ICAPS, pages 12â€“21, 2005.

[6] Sven Koenig and Maxim Likhachev. D*lite. In Proceedings of the Eighteenth National Conference on Artificial Intelligence
and Fourteenth Conference on Innovative Applications of Artificial Intelligence, July 28 - August 1, 2002, Edmonton, Alberta,
Canada., pages 476â€“483, 2002.

[7] S. Behere and M. Torngren. A functional architecture for autonomous driving. In 2015 First International Workshop on

Automotive Software Architecture (WASA), pages 3â€“10, 2015.

[8] W. Zong, C. Zhang, Z. Wang, J. Zhu, and Q. Chen. Architecture design and implementation of an autonomous vehicle.

IEEE Access, 6:21956â€“21970, 2018.

[9] Wilko Schwarting, Javier Alonso-Mora, and Daniela Rus. Planning and decision-making for autonomous vehicles.

Annual Review of Control, Robotics, and Autonomous Systems, 1, may 2018.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Adversarial Planning

21

[10] V. L. L. Thing and J. Wu. Autonomous vehicle security: A taxonomy of attacks and defences. In 2016 IEEE International
Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber,
Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData), pages 164â€“170, Dec 2016.

[11] M. Amoozadeh, A. Raghuramu, C. Chuah, D. Ghosal, H. M. Zhang, J. Rowe, and K. Levitt. Security vulnerabilities of
connected vehicle streams and their impact on cooperative driving. IEEE Communications Magazine, 53(6):126â€“132,
June 2015.

[12] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium

on Security and Privacy (SP), pages 39â€“57. IEEE, 2017.

[13] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. The
limitations of deep learning in adversarial settings. 2016 IEEE European Symposium on Security and Privacy (EuroS&P),
pages 372â€“387, 2016.

[14] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale.

arXiv preprint

arXiv:1611.01236, 2016.

[15] Peng Chen and Hao Chen. Angora: Efficient fuzzing by principled search. In 2018 IEEE Symposium on Security and

Privacy (SP), pages 711â€“725. IEEE, 2018.

[16] Cornelius Aschermann, Sergej Schumilo, Tim Blazytko, Robert Gawlik, and Thorsten Holz. Redqueen: Fuzzing with

input-to-state correspondence. In NDSS, volume 19, pages 1â€“15, 2019.

[17] Brian Paden, Michal ÄŒÃ¡p, Sze Zheng Yong, Dmitry Yershov, and Emilio Frazzoli. A survey of motion planning and

control techniques for self-driving urban vehicles. IEEE Transactions on Intelligent Vehicles, 1, April 2016.

[18] Kamarizan Kidam and Markku Hurme. Analysis of equipment failures as contributors to chemical process accidents.

Process Safety and Environmental Protection, 91(1):61 â€“ 78, 2013.

[19] Robert Wall and Alison Slider. "u.s. airlines report delays caused by system fault: Faa said it was aware that several

airlines were experiencing issues with a flight-planning program". The Wall Street Journal, April 2019.

[20] Malte Helmert. The fast downward planning system. Journal of Artificial Intelligence Research, 26:191â€“246, 2006.
[21] Andreas Junghanns and Jonathan Schaeffer. Domain-dependent single-agent search enhancements. In Proceedings of
the 16th International Joint Conference on Artifical Intelligence - Volume 1, IJCAIâ€™99, pages 570â€“575, San Francisco, CA,
USA, 1999. Morgan Kaufmann Publishers Inc.

[22] Stuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall Press, Upper Saddle River, NJ,

USA, 3rd edition, 2009.

[23] Avrim L. Blum and Merrick L. Furst. Fast planning through planning graph analysis. Artificial Intelligence, 90(1):281 â€“

300, 1997.

[24] Henry Kautz and Bart Selman. BLACKBOX: A new approach to the application of theorem proving to problem solving.

In AIPS98 Workshop on Planning as Combinatorial Search, volume 58260, pages 58â€“60, 1998.
[25] Blai Bonet and HÃ©ctor Geffner. Planning as heuristic search. Artificial Intelligence, 129:5â€“33, 2001.
[26] Miles Tracy Karen Scarfone, Wayne Jansen. Description of participant planners of the deterministic track. The Eighth

International Planning Competition, June 2014.

[27] Conor McGann, Frederic Py, Kanna Rajan, Hans Thomas, Richard Henthorn, and Rob Mcewen. T-REX: A model-based

architecture for AUV control. The International Conference on Automated Planning and Scheduling, 2007.

[28] David Karger. Global min-cuts in RNC and other ramifications of a simple mincut algorithm. In Proceedings of the

Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 21â€“30, 01 1993.

[29] Kutluhan Erol, Dana S. Nau, and V.S. Subrahmanian. Complexity, decidability and undecidability results for domain-

independent planning. Artificial Intelligence, 76(1):75 â€“ 88, 1995. Planning and Scheduling.

[30] Tom Bylander. The computational complexity of propositional STRIPS planning. Artificial Intelligence, 69(1):165 â€“ 204,

1994.

[31] Joseph C Culberson and Jonathan Schaeffer. Pattern databases. Computational Intelligence, 14(3):318â€“334, 1998.
[32] JÃ¶rg Hoffmann and Bernhard Nebel. The ff planning system: Fast plan generation through heuristic search. J. Artif.

Int. Res., 14(1):253â€“302, May 2001.

[33] Malte Helmert and Hector Geffner. Unifying the causal graph and additive heuristics. In Proceedings of the Eighteenth
International Conference on Automated Planning and Scheduling, ICAPS 2008, Sydney, Australia, September 14-18, 2008,
pages 140â€“147, 2008.

[34] Stefan Edelkamp, Roman Englert, JÃ¶rg Hoffmann, Frederico dos S. Liporace, Sylvie ThiÃ©baux, and Sebastian TrÃ¼g.
Engineering benchmarks for planning: the domains used in the deterministic part of IPC-4. J. Artif. Intell. Res.,
26:453â€“541, 2006.

[35] On-Board System Requirements for V2V Safety Communications, mar 2016.
[36] J. Petit and S. E. Shladover. Potential cyberattacks on automated vehicles. IEEE Transactions on Intelligent Transportation

Systems, 16(2):546â€“556, April 2015.

[37] Kate Cox. How to virtually block a road: Take a walk with 99 phones, February 2020.

, Vol. 1, No. 1, Article . Publication date: May 2022.

22

Vie et al.

[38] Kamal Gupta and Angel P. Pobil. Practical Motion Planning in Robotics: Current Approaches and Future Directions. John

Wiley & Sons, Inc., USA, 1998.

[39] John M. Hollerbach, William B. Thompson, and Peter Shirley. The convergence of robotics, vision, and computer

graphics for user interaction. The International Journal of Robotics Research, 18(11):1088â€“1100, 1999.

[40] Zoha Usmani and Shailendra Singh. A survey of virtual machine placement techniques in a cloud data center. Procedia

Computer Science, 78:491â€“498, 12 2016.

[41] Edward F. Moore. The shortest path through a maze. Proceedings of the International Symposium on the Theory of

Switching, Harvard University Press., pages 285â€“292, 1959.

[42] Allen Newell and George Ernst. The search for generality. In Proc. IFIP Congress, volume 65, pages 17â€“24, 1965.
[43] Peter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of minimum cost

paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100â€“107, 1968.

[44] Richard E. Fikes and Nils J. Nilsson. STRIPS: A new approach to the application of theorem proving to problem solving.

Artificial Intelligence, 2(3):189 â€“ 208, 1971.

[45] Edwin PD Pednault. ADL: Exploring the middle ground between strips and the situation calculus. Kr, 89:324â€“332,

1989.

[46] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. CoRR, abs/1607.02533,

2016.

[47] Y. Xiang, W. Niu, J. Liu, T. Chen, and Z. Han. A pca-based model to predict adversarial examples on q-learning of path

finding. In 2018 IEEE Third International Conference on Data Science in Cyberspace (DSC), pages 773â€“780, June 2018.

[48] Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3):279â€“292, May 1992.
[49] Maria Fox and Derek Long. PDDL2.1: an extension to PDDL for expressing temporal planning domains. CoRR,

abs/1106.4561, 2011.

[50] Amotz Bar-Noy, Samir Khuller, and Baruch Schieber. The complexity of finding most vital arcs and nodes. Technical
report, University of Maryland at College Park, 1995. Univ. of Maryland Institute for Advanced Computer Studies
Report No. UMIACS-TR-95-96.

[51] Cristina Bazgan, Till Fluschnik, AndrÃ© Nichterlein, Rolf Niedermeier, and Maximilian Stahlberg. A more fine-grained
complexity analysis of finding the most vital edges for undirected shortest paths. CoRR, abs/1804.09155, 2018.
[52] Peta Masters and Sebastian Sardina. Deceptive path-planning. In Proceedings of the Twenty-Sixth International Joint

Conference on Artificial Intelligence, IJCAI-17, pages 4368â€“4375, 2017.

[53] A. E. Howe. Improving the reliability of artificial intelligence planning systems by analyzing their failure recovery.

IEEE Transactions on Knowledge & Data Engineering, 7:14â€“25, 02 1995.

[54] Naveed Arshad, Dennis Heimbigner, and Alexander L. Wolf. A planning based approach to failure recovery in
distributed systems. In Proceedings of the 1st ACM SIGSOFT Workshop on Self-managed Systems, WOSS â€™04, pages 8â€“12,
New York, NY, USA, 2004. ACM.

9 ACKNOWLEDGMENTS
The authors would like to thank Nicolas Papernot for his insightful comments and the parallels he
pointed out with adversarial machine learning. We also would like to thank Rachel King, Blaine
Hoak, and Berkay Celik for helping us during the writing and formatting phase. This material is
based upon work supported by, or in part by, the National Science Foundation under Grant No.
CNS-1805310 and Grant No. Grant No. CNS-1900873.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Adversarial Planning

23

A SAMPLE TASK FROM THE AIR CARGO TRANSPORTATION DOMAIN
The air cargo transportation domain specifies a class of task where cargos must be delivered to
a destination airport minimizing the cost of the plan. The domain is described by three different
operator Load, Unload and Fly each with unit cost. The Load operator is used to load a cargo in an
airplane, Unload is the inverse operator. Finally, Fly is used to freight a cargo from one airport to
another when previously loaded into a plane.

(:action LOAD

:parameters (?c - cargo ?p - plane

?a - airport)

:precondition (and (At ?c ?a) (At ?p ?a))
:effect (and (In ?c ?p) (not (At ?c ?a))))

(:action UNLOAD

:parameters (?c - cargo ?p - plane

?a - airport)

:precondition (and (In ?c ?p) (At ?p ?a))
:effect (and (At ?c ?a) (not (In ?c ?p))))

(:action FLY

:parameters (?p - plane ?from - airport
?to - airport)

:precondition (and (At ?p ?from))
:effect (and (At ?p ?to) (not (At ?p ?from))))

Fig. 10. The air cargo transportation domain.

Now we define a sample task belonging to the domain: we give the initial state and a specification
of the goal state. Figure 11 describes an initial state with two cargos, one at SFO and one at JFK.
The goal is to switch the position of those using the two planes.

(:objects

p1 p2 - plane
c1 c2 - cargo
SFO JFK - airport)

(:init

(At c1 SFO)
(At c2 JFK)
(At p1 SFO)
(At p2 JFK))

(:goal

(and (At c1 JFK) (At c2 SFO)))

Fig. 11. A problem from the air cargo transportation domain.

B FINDING ADVERSARIAL EXAMPLES IS NP-HARD
Given a planning instance, we show that finding ğ‘˜ adversarial changes to increase the length of
the optimal plan is an NP-Hard problem. We define the Adversarial Change Problem (ADVCP) as

, Vol. 1, No. 1, Article . Publication date: May 2022.

24

Vie et al.

follows:
Input - A planning instance I = (ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘†ğ‘”ğ‘œğ‘ğ‘™, O) where O is a set of grounded operators with
non-negative cost and an integer ğ‘˜ âˆˆ N.
Output - A set of ğ‘˜ grounded actions whose removal from O maximize the cost increase of the
optimal planâ€”ğ‘˜ adversarial changes.
The decision problem associated with ADVCP (D-ADVCP) is the following. â€œGiven a planning
instance I = (ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘†ğ‘”ğ‘œğ‘ğ‘™, O) and two integers (ğ‘˜, â„) âˆˆ N2, is there ğ‘˜ grounded actions whose
removal from O makes the length of the optimal plan at least â„â€.

We also introduce the Most Vital Arcs Problem (MVAP):
Input - A graph ğº = (ğ‘‰ , ğ¸) (directed or undirected), an integer ğ‘˜ âˆˆ N, and two nodes (ğ‘ , ğ‘¡) âˆˆ ğ‘‰ 2.
Each edge in ğ‘’ âˆˆ ğ¸ has a non-negative cost of ğ‘ (ğ‘’).
Output - A set of ğ‘˜ edgesâ€”arcsâ€”whose removal maximize the length increase of the optimal path
between ğ‘  and ğ‘¡ in ğº.

The decision problem associated with the MVAP (D-MVAP) is the following. â€œGiven ğº = (ğ‘‰ , ğ¸),
(ğ‘˜, â„) âˆˆ N2 and (ğ‘ , ğ‘¡) âˆˆ ğ‘‰ 2, is there ğ‘˜ edges whose removal makes the length of the shortest path
from ğ‘  to ğ‘¡ at least â„?â€.

We prove that ADVCP is an NP-Hard problem by reducing D-MVAP to D-ADVCP. D-MVAP
is known to be NP-Hard [50] [51]. We first introduce the polynomial time reduction between
instances of the two decision problems. In a second time, we show that the answer for an instance
of D-ADVCP is â€œyesâ€ if and only if the answer of the corresponding instance of D-MVAP is â€œyesâ€.
Suppose we are given an instance of the D-MVAP consisting of a graph ğº = (ğ‘‰ , ğ¸), (ğ‘˜, â„) âˆˆ N2
two integers and (ğ‘ , ğ‘¡) âˆˆ ğ‘‰ 2 two nodes. Let |ğ‘‰ | = ğ‘› and assume the nodes in ğ‘‰ are labelled
1, 2, 3, ..., ğ‘›. We create the following equivalent D-ADVCP instance I (planning instance).

â€¢ We define ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ as the state with the following set of predicate: {(Node, s)}
â€¢ A state will be recognized as a goal state ğ‘†ğ‘”ğ‘œğ‘ğ‘™ if its predicates include the following set of

predicates: {(Node, t)}

â€¢ If node ğ‘– is connected to ğ‘— with an edge in ğº, we add the following operator ğ‘œğ‘– ğ‘— to O. The

cost of this operator is set to the same cost as the ğ‘–-to-ğ‘—-edge cost.

(:grounded-action Oij

:precondition (Node, i)
:effect (and (Node, j) (not (Node, i))))

The resulting planning instance can be constructed in polynomial time in the size of ğ‘‰ and ğ¸.
We now show that the answer for an instance of the D-ADVCP is â€œyesâ€ if and only if the answer of
the corresponding D-MVAP instance is â€œyesâ€.

Assuming the answer is â€œyesâ€ for a D-ADVCP instance I = (ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘†ğ‘”ğ‘œğ‘ğ‘™, O) where ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ =
{(Node, s)} and ğ‘†ğ‘”ğ‘œğ‘ğ‘™ = {(Node, t)}. Then removing ğ‘˜ grounded actions from O makes the length
of the optimal plan at least â„. We call ğº I the graph given by the inverse reduction. ğº I is exactly
the state space graph generated by I. The cost of Iâ€™s optimal plan is the same as the cost of an
optimal path from ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡ to ğ‘†ğ‘”ğ‘œğ‘ğ‘™ in ğº I. The graph ğº I, (ğ‘˜, â„) and (ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘†ğ‘”ğ‘œğ‘ğ‘™ ): equivalent D-MVAP
instance will be a â€œyesâ€.

If ğº = (ğ‘‰ , ğ¸), (ğ‘˜, â„) âˆˆ N2 and (ğ‘ , ğ‘¡) âˆˆ ğ‘‰ 2 (D-MVAP instance) is a â€œyesâ€, there exist ğ‘˜ edges whose
removal makes the length of the shortest path from ğ‘  to ğ‘¡ at least â„. The equivalent planning
instance Iğº defined by the reduction has the same state space graph as ğº. Then, removing the ğ‘˜
equivalent grounded actions from O in Iğº will increase the cost of the optimal plan by at least â„.

, Vol. 1, No. 1, Article . Publication date: May 2022.

Adversarial Planning

25

Hence ADVCP is an NP-Hard problem. Note that we defined the ADVCP problem such that
I = (ğ‘†ğ‘–ğ‘›ğ‘–ğ‘¡, ğ‘†ğ‘”ğ‘œğ‘ğ‘™, O) would be given as an input. However, in real-life, the set O is not given, instead
we give a set of non-grounded operators with a set of objects to ground them. Unfortunately, O
can grow exponentially with the number of non-grounded operators and the number of objects. In
the end, finding adversarial changes is finding an NP-hard problemâ€™s solution for an input with
exponential size.

C TABLE GENERATION PARAMETERS

Threat model

Threshold Window Algorithm

AHI
Agentâ€™s heuristic
Black-box

0
0
10

231
231
45

D*Lite
D*Lite
D*Lite

ğ»ğ‘ğ‘‘ğ‘£
â„ğ¸ğ‘¢ğ‘ğ‘™ğ‘–ğ‘‘ğ‘’ğ‘ğ‘›
â„ğ¸ğ‘¢ğ‘ğ‘™ğ‘–ğ‘‘ğ‘’ğ‘ğ‘›
â„ğ‘€ğ‘ğ‘›â„ğ‘ğ‘¡ğ‘¡ğ‘ğ‘›

Table 4. The table generation processâ€™ settings for the different scenarios in the Maze domain. The window
corresponds to the number of windows kept in the table, the algorithm is the one used by the adversary, and
the AHI is the agentâ€™s heuristic and informed.

D DEFENSES
It is natural to ask what defenses would mitigate this kind of adversarial planning. Consider that
the agent will always follow what seems to be the shortest way to reach the goal. If we were to
implement a new kind of planner resisting adversarial changes, we would face two contradictory
incentives. (1) Try to find the shortest plan to the goal state and the plan becomes predictable or (2)
find a less predictable plan that can be worse in terms of resources than the initial one. The first
trade-off is the one followed by most planners and we showed it was sensible to an adversary. The
second choice is exactly what an adversary wants because the plan is in the end worse than the
initial one. This tension between these two contradictory goals makes it hard to come up with a
defense against adversarial changes.

Deception and secrecy might be the only way to prevent an adversary from interfering the plan.
In [52], researchers worked on deceptive path planning: finding a path such that an observer cannot
determine the goal the agent wants to reach until the last steps. Without a clear description of the
goal state, an adversary is unable to predict the agentâ€™s plan.

If an adversary still succeeds in finding an effective adversarial change, a way to limit its impact is
to use failure recovery techniques. Howe [53] studied plan resilience and error recovery at planning
and execution time. Arshad et al. [54] investigated failure recovery for distributed system using
planning. Their approach automates failure recovery by defining an acceptable recovered state as a
goal. Then their system runs a planner to get from the current failure state to the recovered state.
However that planner can also be vulnerable to adversarial changes.

, Vol. 1, No. 1, Article . Publication date: May 2022.

