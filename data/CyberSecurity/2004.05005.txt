i

Adversarial Attacks on Machine Learning
Cybersecurity Defences in Industrial Control
Systems

Eirini Anthi1,*, Lowri Williams1, Matilda Rhode1, Pete Burnap1, Adam Wedgbury2
1Cardiff University, School of Computer Science & Informatics, Cardiff, UK
2Airbus, Newport, UK

0
2
0
2

r
p
A
0
1

]

G
L
.
s
c
[

1
v
5
0
0
5
0
.
4
0
0
2
:
v
i
X
r
a

Abstract—The proliferation and application of machine learn-
ing based Intrusion Detection Systems (IDS) have allowed for
more ﬂexibility and efﬁciency in the automated detection of
cyber attacks in Industrial Control Systems (ICS). However, the
introduction of such IDSs has also created an additional attack
vector; the learning models may also be subject to cyber attacks,
otherwise referred to as Adversarial Machine Learning (AML).
Such attacks may have severe consequences in ICS systems, as
adversaries could potentially bypass the IDS. This could lead
to delayed attack detection which may result in infrastructure
damages, ﬁnancial loss, and even loss of life. This paper explores
how adversarial learning can be used to target supervised models
by generating adversarial samples using the Jacobian-based
Saliency Map attack and exploring classiﬁcation behaviours. The
analysis also includes the exploration of how such samples can
support the robustness of supervised models using adversarial
training. An authentic power system dataset was used to support
the experiments presented herein. Overall,
the classiﬁcation
performance of two widely used classiﬁers, Random Forest and
J48, decreased by 16 and 20 percentage points when adversarial
samples were present. Their performances improved following
adversarial training, demonstrating their robustness towards
such attacks.

Index Terms—industrial control systems, supervised machine
learning, adversarial machine learning, attack detection, intru-
sion detection system

I. INTRODUCTION

I NDUSTRIAL Control Systems (ICS) play a key role in

Critical National Infrastructure (CNI) concepts such as
manufacturing, power/smart grids, water treatment plants, gas
and oil reﬁneries, and health-care. Historically, ICS networks
and their components were protected from cyber attacks as
they ran on proprietary hardware and software, and were
connected in isolated networks with no external connection to
the Internet [1]. However, as the world is becoming more inter-
connected, there has been a need to connect ICS components
together and to other networks, allowing remote access and
monitoring functionalities. As a result, ICS are now subject to
a range of security vulnerabilities [1].

Given the importance of these systems, they have become
an attractive target to an attacker. As these systems control
operations in the physical world, the cyber attacks against
them may have major consequences for the environment they

*Corresponding author: anthies@cardiff.ac.uk

operate in, and subsequently, its users. It is therefore under-
standable that the security issues surrounding such systems
have become a global issue. Thus, designing robust, secure,
and efﬁcient mechanisms for detecting and defending cyber
attacks in ICS networks is more important than ever [2].

Although there exist several security mechanisms for tradi-
tional IT systems, their integration into ICS systems is chal-
lenging mainly for two reasons; a) ICS devices are resource
constrained, and b) they include legacy systems and devices
that do not support modern security measures. Subsequently,
complementary security solutions, such as passive process data
monitoring, are promising [3]. This has led to a substantial in-
crease in research focusing on ICS tailored Intrusion Detection
Systems (ICS). Such intrusion systems operate by observing
the network or sensor data in order to detect attacks and
anomalies that may affect ICS.

the trained models may also be subject

Due to their efﬁciency in detecting attacks, there has been
a substantial increase in the application and integration of
machine learning within IDSs (e.g. [1], [4]–[10]). However,
the introduction of such systems has introduced an additional
to
attack vector;
attacks. The act of deploying attacks towards machine learning
based systems is known as Adversarial Machine Learning
(AML). The aim is to exploit the weaknesses of the pre-
trained model which has “blind spots” between data points it
has seen during training. More speciﬁcally, by automatically
introducing slight perturbations to the unseen data points the
model may cross a decision boundary and classify the data
as a different class. As a result, the model’s effectiveness can
be reduced as it is presented with unseen data points that it
cannot associate target values to, subsequently increasing the
number of misclassiﬁcations.

The existence of such techniques means that infrastructures
which incorporate machine learning based IDSs may be at risk
of being vulnerable to cyber attacks. In the context of ICS,
AML can be used to manipulate data from actuators or other
devices by including perturbations to cause malicious data
to be classiﬁed as being benign, consequently bypassing the
IDS. This could lead to delayed attack detection, information
leakage, ﬁnancial loss, and even loss of life. It is therefore
understandable that as machine learning based detection mech-
anisms become more widely deployed, the adversary incentive
for defeating them increases. As a result, it is evident that

 
 
 
 
 
 
machine learning based IDSs must be extensively evaluated
against AML attacks.

To the best of our knowledge, this is the ﬁrst study which
investigates the behaviour of supervised models against AML,
as well as the defence of such attacks in the context of ICS.
The main contributions of the work presented in this paper are
the empirical investigations into:

• generating adversarial samples from a power system

dataset

• the behaviour of supervised machine learning algorithms
against adversarial samples for intrusion detection in an
ICS system

• how adversarial training can support the robustness of

such models

The study uses a representative power system dataset and
was designed as follows (see Figure 1): 1) randomly split
the power system dataset into training and testing set, each
containing 60% and 40% data points respectively, 2) evaluate
a range of supervised machine learning models and identify
which are the best performing, 3) generate adversarial samples
using the Jacobian-based Saliency map method, 4) evaluate
the performance of the trained models in 2 on the generated
adversarial samples in 3, 5) include a percentage of adversarial
samples from 3 in the training data and re-train and evaluate
the models.

Fig. 1: An overview of the study design

The remainder of this paper is structured as follows: Section
II discusses the relevant work in this research area, Section III
discusses the power system testbed and the generated dataset
which is used to support the experiments in this paper, Section
IV evaluates the performance of a range of supervised classi-

ii

ﬁers, Section V discusses AML and the methodology followed
to generate adversarial samples, Section VII investigates the
effectiveness of adversarial training as a defence mechanisms,
and ﬁnally VIII concludes the paper.

II. RELATED WORK

There has been a substantial increase in machine learning
based IDSs for a range of ICS systems. Table I presents a sum-
mary of the existing ICS systems and associated supervised
learning approaches to attack detection and classiﬁcation in
these contexts. To date, there has been less focus on AML
in this context. Such research has mainly focused on email
spam classiﬁers, malware detection, and very recently there
has been interest in AML against network IDSs for traditional
networks (e.g. [11]–[13]).

More speciﬁcally, both Nelson et al. [14] and Zhou et al.
[15] demonstrate that an adversary can exploit and successfully
bypass the machine learning methods employed in spam ﬁlters
by modifying a small percentage of the original training data.
Moreover, Grosse et al. [16] evaluate the robustness of a
neural network trained on the DREBIN Android malware
dataset. They report that it is possible to confuse the model
by perturbing a small amount of the features in the training
set. Such an attack is considered to be a white box attack,
as in order to be successful, the adversary needs to have
access or knowledge of the dataset and the features it includes.
Furthermore, Hu and Tan [17] proposed a more advanced
adversarial technique which uses the concept of Generative
Adversarial Networks (GAN) to successfully attack malware
classiﬁers without requiring any knowledge of the data and
the system. This is known as a black box attack.

In the context of ICSs,

there exist only a handful of
investigations into AML attacks. Speciﬁcally, Zizzo et al. [18]
showcased a simple AML attack against an Long Short-Term
Memory (LSTM) classiﬁer which was applied on an ICS
dataset. However, this work is at a preliminary stage as the
adversarial samples were generated by manually selecting the
feature/acutator values to be perturbed. Yaghoubi and Fainekos
[19] proposed a gradient based search approach which was
evaluated on a Simulink model of a steam condenser. However,
this approach is efﬁcient only against a handful of systems
that may speciﬁcally employ Recurrent Neural Networks
(RNN) with smooth activation functions. Finally, Erba et
al. [3] demonstrated two types of real-time evasion attacks,
again using Recurrent Neural Network models, and used an
autoencoder to generate adversarial samples, which is com-
putationally complex. Neither of these aforementioned works
investigate defence methods against AML. Conclusively, it is
evident that there is room to investigate AML and the defence
against such attacks for current IDSs in ICS systems that are
supported by supervised learning. Moreover, as Table I shows,
Recurrent Neural Networks are yet to gain prominence in
attack detection in an ICS context - with algorithms such as
Naive Bayes, Random Forest, SVM, and J48 being much more
widely used. We therefore base our experiments in defending
against AML on these methods as the state of the art in ML-
driven attack detection methods for ICS.

iii

Work
[4]
[20]
[21]
[22]
[23]
[24]
[25]
[6]
[26]
[27]
[28]
[29]
[30]
[31]
[32]
[33]
[5]
[34]
[1]
[10]
[8]
[9]
[35]

Dataset
Gas Pipeline
Power System
Power system (synthetic)
SWaT
SCADA/ICS
Gas Pipeline
SCADA/Modbus
SCADA Testbed
Power Grid, Water Plant, Gas Plant
Wind Turbines
SCADA Testbed
Power System
SCADA
SCADA Testbed
Power System
Wind Turbine
SCADA Testbed
SCADA Testbed
SWaT
ICS Testbed
SWaT
SCADA Network Trafﬁc
ICS Testbed

Machine Learning Models
Naive Bayes, Random Forest, SVM, J48, OneR
OneR, Random Forest, Naive Bayes, SVM, JRipper + Adaboost
Naive Bayes, Random Forests, SVM
SVM, J48, Random Forest
J48, Naive Bayes
SVM, Random Forest
Decision Tree, K-Nearest Neighbor, SVM, OCSVM
Random Forest, J48, Logistic Regression, Naive Bayes
J48, Random Forest, Naive Bayes, SVM, JRipper + Adaboost
SVM
SVM, Decision Tree, and Random Forest
SVM, J48, Neural Network
Naive Bayes, BayesNet, J48
Decision Tree, Random Forest
Random Forest
Decision Trees (J48, Random Forest, CART, Ripper, etc.)
Bayesian Network
Long Short Term Memory (RNN)
1D Convolutional Networks
Neural Network (Error-back propagation and Levenberg-Marquardt)
Long Short Term Memory (RNN)
One-Class SVM
Long Short Term Memory (RNN)

TABLE I: Summary of current work on Intrusion Detection Systems in Industrial Control Systems

III. INDUSTRIAL CONTROL SYSTEM CASE STUDY: POWER
SYSTEM

More speciﬁcally,

the components of the power system

include:

Mississippi State University and Oak Ridge National Labo-
ratory implemented a scaled-down version of a power system
framework. Although this system is relatively small, it captures
the core function and is considered as being a representative
example of a larger power system [36]. Figure 2 illustrates in
more detail the power system framework conﬁguration and the
components used for generating the datasets in which support
the experiments in this paper.

Fig. 2: Power System Framework Testbed [37]

• G1 and G2 are the main generators.
• R1, R2, R3, and R4 are the Intelligent Electronic Devices
(IEDs) responsible for switching the breakers (BR1, BR2,
BR3, BR4), which are automatically operated electrical
switches designed to protect electrical circuits from dam-
age caused by excess current from an overload or short
circuit, on and off.

• Each IED automatically controls one breaker (e.g. R1

controls BR1, R2 controls BR2, etc.)

• The IEDs use a distance protection scheme which trips
the breaker on detected faults (whether they are valid or
invalid) since they have no internal validation to detect
the difference.

• Operators can also manually issue commands to the IEDs
to manually trip the breakers. The manual override is
used when performing maintenance on the lines or other
system components.

• There are also other network monitoring devices con-
nected on the testbed, such as SNORT and Syslog servers.

A. Dataset

A total of 15 datasets containing both benign and malicious
data points were generated from the power system testbed
by [36]. These data points have been further categorised
into three main classes; ‘no event’ instances, ‘natural event’
instances, and ‘attack event’ instances. Both the ‘no event’
and ‘natural event’ instances are grouped together to represent
benign activity. To generate the malicious data, attacks from 5
scenarios were deployed on the power system. These attacks
are described as follows:

1) Short-circuit fault. This is a short in a power line
and can occur in various locations along the line. The
location is indicated by the percentage range.

2) Line maintenance. One or more relays are disabled on

a speciﬁc line to do maintenance for that line.

3) Remote tripping command injection attack. This is
an attack that sends a command to a relay which causes
a breaker to open. It can only be done once an attacker
has penetrated outside defenses.

4) Relay setting change attack. Relays are conﬁgured
with a distance protection scheme. The attacker changes
the setting to disable the relay function so that the relay
will not trip for a valid fault or a valid command.

5) Data injection attack. A valid fault

is imitated by
changing values to parameters such as the current,
voltage, and sequence components. This attack aims to
blind the operator and causes a black out.

For the purposes of the work described in this paper, all 15
datasets were used. The dataset consisted of 55,663 malicious
and 22,714 benign data points.

IV. SUPERVISED MACHINE LEARNING

To explore how well supervised classiﬁcation algorithms
can learn to detect cyber attacks in an ICS environment,
the performance of supervised machine learning when the
corresponding data discussed in Section III-A was used to train
the classiﬁcation model and evaluated. The following Sections
report the features present in the power systems dataset, as well
as describing the methodology behind selecting and training
the best performing supervised classiﬁers.

A. Feature Selection

In order to perform machine learning classiﬁcation experi-
ments, it is essential to identify which attributes best describe
the dataset. In this case, the data points within the power
system dataset contain attributes associated with synchropha-
sor measurements and basic network security mechanisms. A
synchrophasor measurement unit is a device which measures
the electrical waves on an electricity grid, using a common
time source for synchronization. The dataset contains a total
of 128 features [37]. These features are described in more
detail as follows:

• 29 types of measurements from each synchrophasor mea-
surement unit. In this speciﬁc power system testbed, there
are 4 PMUs. Therefore, the dataset contains a total of 116
synchrophasor measurement columns.

• 12 types of measurements of control panel logs, snort
alerts, and relay logs of the 4 synchrophasor measurement
unit and relay.

Table IV-A summarises the features included in the dataset,
as well as their corresponding descriptions. More speciﬁcally,
the index of each feature is in the form of “R#-Signal
Reference”. The “R ‘#’ ” speciﬁes the type of measurement
from the synchrophasor measurement unit. For instance, “R1-
PA1:VH” corresponds to the “Phase A voltage phase angle”
measured by “PMU R1”.

iv

Feature
PA1-PA3:VH
PM1: V -PM3:V
PA4:IH - PA6:IH
PM4: I PM6: I
PA7:VH PA9:VH
PM7: V PM9: V
PA10:VH - PA12:VH
PM10: V - PM1
F
DF
PA:Z
PA:ZH
S

Description
PA1:VH PA3:VH Phase A
C Voltage Phase Angle
Phase A - C Current Phase Angle
Phase A - C Current Phase Magnitude
Pos. Neg. Zero Voltage Phase Angle
Pos. Neg. Zero Voltage Phase Magnitude
Pos. Neg. Zero Current Phase Angle
Pos. Neg. Zero Current Phase Magnitude
Frequency for relays
Frequency Delta (dF/dt) for relays
Appearance Impedance for relays
Appearance Impedance Angle for relays
Status Flag for relays

TABLE II: Features included as part of the power system
dataset

B. Model Training

To explore how well supervised machine learning algo-
rithms can detect cyber attacks in an ICS environment, the
corresponding power system dataset was used to evaluate a
range of state-of-the-art classiﬁers. In the case of identifying
whether a datapoint
is malicious or benign, classiﬁcation
is evaluated relative to the training dataset, producing four
outputs:

• true positives (TP) - data points are predicted as being

malicious, when they are indeed malicious.

• true negatives (TN) - data points are predicted as being

benign, when they are indeed benign.

• false positives (FP) - data points are predicted as being

malicious, when in fact, they are benign.

• false negatives (FN) - data points are predicted as being

benign, when in fact, they are malicious.

Subsequently, these output are used to evaluate the classiﬁ-
cation performance of the trained model using Precision (P),
Recall (R), and F1-score (F). Such measures are calculated
using the equations in Equation 1.

(1)

P =

, R =

P · R
P + R

, F = 2 ·

T P
T P + FP

T P
T P + F N
The “no free lunch” theorem suggests that there is no univer-
sally best learning algorithm [38]. In other words, the choice of
an appropriate algorithm should be based on its performance
for that particular problem and the properties of data that
characterize the problem. In this case, a variety of classiﬁers
distributed as part of Weka [39] were evaluated using 10-fold
cross-validation using their default hyper-parameters.

To conform to other comparable IDSs in ICS systems in
Table I, the classiﬁers were also selected based on their ability
to support a high-dimensional feature space. The classiﬁers
included:

• Generative models that consider conditional dependencies
in the dataset or assume conditional independence (e.g.
Bayesian Network, Naive Bayes).

• Discriminative models that aim to maximise information
gain or directly maps data to their respective classes
without modeling any underlying probability or structure
of the data (e.g. J48 Decision Tree, Support Vector
Machine).

Classiﬁer
Zero R
BayesNet
Naive Bayes
SVM
Adaboost + JRip
Random Forest
J48

P
0.50
0.66
0.67
-
-
0.94
0.87

R
0.70
0.61
0.31
-
-
0.93
0.87

F
0.58
0.63
0.19
-
-
0.93
0.87

Time (s)
0.20
8.28
54.00
28,800.00
28,800.00
247.00
480.00

TABLE III: Weighted average results following cross-
validation

To support classiﬁcation experiments, a random subset of
approximately 60% of the dataset described in Section III-A
was selected for training, with the remaining 40% selected for
testing. Figure 3 reports the distributions of data points across
the target values in both the training and testing datasets.

Fig. 3: Distribution of data points across both training and
testing datasets

Table III illustrates the results for each classiﬁer. Previous
work which have used a very small sample of this power
system dataset to support their classiﬁcation experiments have
shown that the ensemble classiﬁer which combines both the
Adaboost and JRipper models was found to be the best per-
forming [40]. However, in this work, when both the ensemble
classiﬁer and Support Vector Machine (SVM) classiﬁer were
applied, both models were still training following approx-
imately 2 days of running. In this case, the models were
stopped and their classiﬁcation performances were omitted in
the reporting of the results herein. Conversely, with F1-scores
of 0.93 and 0.87, the classiﬁers with the highest performances
were Random Forest and Weka’s implementation of the J48
decision tree method with no pruning respectively.

V. ADVERSARIAL MACHINE LEARNING

To reiterate, the aim of AML is to automatically introduce
perturbations to the unseen data points in order to confuse the
pre-trained model. The following sections introduce the types
of AML attacks, as well as the methods used to automatically
generate adversarial samples.

v

A. Adversarial Attack Types

Depending on the phase and aspect of the machine learning
model that is being targeted, AML attacks can be described
in terms of four primary vectors: [13], [41]:

• The Inﬂuence of an attack’s affects the classiﬁer’s de-
cision. Attacks can be further categorised as causative
attacks, which occur during the learning phase (poison
attacks), or exploratory attacks, which target the trained
model during the testing phases (evasion attacks).

• Security Violations affect either the integrity of the
model when the adversarial samples cause misclassiﬁca-
tions, or when the high rate of misclassiﬁcations causes
the model to become unusable.

• Speciﬁcity refers to targeted attacks, where the adver-
sarial samples aim to target a speciﬁc target value, or
indiscriminate attacks, where the samples do not target a
speciﬁc target value.

• Privacy refers to attacks where the adversary’s goal is to

extract information from the classiﬁer.

Papernot et al. [42] further categorise adversarial attacks

based on:

• Their complexity. The consequences of such attacks can
range from slightly reducing the conﬁdence of a model’s
predictions to causing it to misclassify all unseen data
points.

• The knowledge an adversary may have. A white box
attack refers to when an attacker has useful knowledge
related to the learning model, such as it’s architecture, the
network’s trafﬁc it reads, and the features used to support
its training. It is considered as being a black box attack
when an adversary has no information about the internal
workings of the target model.

Given that we have access to the full training dataset and
its features, and we don’t know the target model, the AML
approach presented in this work is classiﬁed as a being a grey-
box attack.

B. Adversarial Sample Generation Methods

There are various methods by which adversarial samples
can be generated. Such methods vary in complexity, the speed
of their generation, and their performance. An unsophisticated
approach towards crafting such samples is to manually perturb
the input data points. However, manual perturbations are
slow to generate and evaluate by comparison with automatic
approaches. Two of the most popular techniques towards auto-
matically generating perturbed samples include the Fast Gra-
dient Sign Method (FGSM) and the Jacobian based Saliency
Map Attack (JSMA), presented by Goodfellow et al. [43] and
Papernot et al. [42] respectively.

Both methods rely on the methodology, that when adding
small perturbations (δ) to the original sample (X), the resulting
sample (X*) can exhibit adversarial characteristics (X* = X +
δ) [11] in that X* is now classiﬁed differently by the targeted
model. Moreover, both methods are also usually applied by
using a pre-trained MLP as the underlying model for the
adversarial sample generation.

Dataset
Original test data
θ = 0.1, γ = 0.1
θ = 0.9, γ = 0.9

R1-PA1:VH
0.764515
0.765000
1.000000

R2:DF
0.361399
0.361000
0.538000

R2-PM11:I
0.008482
0.008480
0.008600

R3-PM5:I
0.026826
0.026800
0.026800

Predicted
1
6,149
21,122

0
2,840
1,240

Actual

0
1

Predicted
1
3,115
30,670

0
10,610
2,631

Actual

0
1

TABLE IV: An example of how features are perturbed using
JSMA

Random Forest

J48

TABLE V: Confusion matrices for the original test set (Be-
nign = 0, Malicious = 1)

vi

The FGSM method aims to target each of the features of
the input data by adding a speciﬁed amount of perturbation.
The perturbation noise is computed by the gradient of the
cost function J with respect to the input data. Let θ represent
the model parameters, x are the inputs to the model, y are
the labels associated with the input data, (cid:15) is a value which
represents the extent of the noise to be applied, and J(θ,x,y)
is the cost function used to train the targeted neural network.

x∗ = x + (cid:15) sign (∇x J(θ, x, y))

(1)

On the other hand, the JSMA method generates perturba-
tions using saliency maps. A saliency map identiﬁes which
features of the input data are the most relevant to the model
decision being one class or another; these features if altered are
most likely affect the classiﬁcation of the target values. More
speciﬁcally, an initial percentage of features (θ) is chosen
to be perturbed by a (γ) amount of noise. Then, the model
establishes whether the added noise has caused the targeted
model to misclassify or not. If the noise has not affected the
model’s performance, another set of features is selected and a
new iteration occurs until a saliency map appears which can
be used to generate an adversarial sample.

Given that the JSMA method may take a few iterations to
generate adversarial samples, the FGSM is computationally
faster [42]. However, as opposed to FGSM which alters each
feature, JSMA is a more complex and elaborate approach
which represents more realistic attacks as it progressively
alters a small percentage of features at a time. This allows for
more realistic and ﬁner grained AML attacks, as adversaries
are able to deﬁne both the percentage of features to perturb
and the amount of perturbation to include when generating the
adversarial samples.

This work presents the use of JSMA in a grey-box attack,
in which the attacker has no knowledge of the target model
but has access to the full dataset and knowledge of features.
Despite not knowing the target model, we can approximate
samples that will cause the target model to misclassify using
another model due to the transferability of adversarial samples
across machine learning models [44].

In this case, the adversarial samples used in the experiments
herein were generated using the JSMA method. A pre-trained
MLP was used as the underlying model for the generation.
The code implementation used to create the adversarial data
was based on the CleverHans project [42]. Table IV shows the
transformation of the features of a malicious data point using
the JSMA method.

VI. EVALUATING SUPERVISED MODELS ON
ADVERSARIAL SAMPLES

Both the trained Random Forest and J48 models presented
in Section IV-B were ﬁrst evaluated against the original testing
dataset. The F1-scores achieved by both classiﬁers were 0.67
and 0.66 respectively. The confusion matrix in Table V shows
how the predicted classes for each data point in the original
testing dataset compare against the actual ones. In compar-
ison to the Random Forest model, J48 demonstrated a high
percentage of correct predictions, thus less often miclassifying
the data points.

To explore how different combinations of the JSMA param-
eters affect the performance of the trained classiﬁers, adver-
sarial samples were generated from all malicious data points
present in the testing data by using a range of combinations of
θ and γ. The adversarial samples were joined with the benign
testing data points and subsequently presented to the trained
models. Figures 4 and 5 report the overall weighted-averaged
F1-scores for all adversarial combinations of JSMA’s θ and γ
parameters.

Fig. 4: Random Forest classiﬁcation performance (F1-score)
on adversarial samples generated using JSMA

In comparison to Random Forest, the classiﬁcation perfor-
mance of the J48 model achieved a decrease in F1-scores
across the majority of the θ and γ parameters. This may
indicate that J48 may be more sensitive, subsequently mis-
classifying malicious data points as benign. However, when
θ = 0.3, γ = 0.2 and θ = 0.2, γ = 0.7, the model achieves
a higher classiﬁcation performance of 0.69 (an increase of 3
percentage points). This may indicate that the generation of

vii

Predicted
1
5,327
10,037

0
3,662
12,325

Actual

0
1

Predicted
1
5,327
18,259

0
3,662
4,103

Actual

0
1

θ = 0.1 γ = 0.3

θ = 0.3 γ = 0.2

TABLE VI: Confusion matrices after applying J48 to adver-
sarial testing samples (Benign = 0, Malicious = 1)

Predicted
1
6,149
12,630

0
2,840
9,732

Predicted
1
6,149
21,338

0
2,840
1,024

Actual

0
1

Actual

0
1

θ = 0.2 γ = 0.4

θ = 0.5 γ = 0.9

TABLE VII: Confusion matrices after applying Random
Forest to adversarial testing samples (Benign = 0, Malicious
= 1)

include adversarial training and adversarial sample detection.
The former has been explored in the ﬁeld of visual computing,
where Goodfellow et al. [45] demonstrated that re-training
the neural network on a dataset containing both the original
and adversarial samples signiﬁcantly improves its efﬁciency
against adversarial samples. The latter technique involves the
implementation of mechanisms that are capable of detecting
the presence of such samples using direct classiﬁcation, neu-
ral network uncertainty, or input processing [18]. However,
these detection mechanisms have been found to be weak in
defending AML [18], [46].

Subsequently, in this paper, the robustness of supervised
machine learning classiﬁers against AML is further evaluated
using adversarial training. In this case, a random sample of
20% of the adversarial data points in the testing dataset which
signiﬁcantly decreased the model’s performance (Random
Forest: θ = 0.2, γ = 0.4 and J48: θ = 0.1, γ = 0.3) were
included in the original training dataset.

The experiments described in Sections IV-B and VI were
repeated by retraining the models with the newly generated
training data and applying such models on all unseen adversar-
ial samples. Both the Random Forest and J48 models achieved
cross-validation F1-scores of 0.94 and 0.89 respectively.

Figures 6 and 7 report the overall weighted-averaged F1-
scores for all adversarial combinations of JSMA’s θ and γ
parameters following adversarial training. The results demon-
strated that for both classiﬁers, including adversarial samples
in the training data increased their classiﬁcation performances.
More speciﬁcally, Random Forest and J48 achieved F1-scores
of 0.76 and 0.80 respectively, an increase of 2 and 11 percent-
age points in comparison to the classiﬁcation performances
reported in Figures 4 and 5.

The classiﬁcation performances demonstrated by the Ran-
dom Forest model achieves a greater overall
increase in
comparison to the J48 model. That is, for Random Forest,
the classiﬁcation performance for all combinations were im-
proved. Whereas for J48, only around 30% of the classiﬁcation
performances increased signiﬁcantly. This may imply that
Random Forest is a more robust model towards classifying
adversarial samples of all combinations of JSMA’s θ and γ

Fig. 5: J48 classiﬁcation performance (F1-score) on adversar-
ial samples generated using JSMA

some adversarial samples has made such data points more
distinct in discriminating between the target values.

Conversely, the classiﬁcation performance of the Random
Forest model achieved an increase in F1-scores for the major-
ity of θ and γ pairs. This may indicate that Random Forest may
be a more robust classiﬁer in correctly discriminating between
malicious and benign data points. However, when θ = 0.2, γ
= 0.4, the model’s classiﬁcation performance decrease by 16
percentage points (F1-score = 0.57). Based on the dataset used
in the experiments presented in this paper, θ = 0.2, γ = 0.4
would be the optimal parameter an adversary would use to
successfully reduce the accuracy of a machine learning based
IDS, subsequently diverting malicious data points.

These ﬁndings demonstrate the importance of parameter-
tuning in applying JSMA for generating adversarial examples.
The JSMA model is likely to be more robust under white-box
conditions as it was designed but these results indicate that
with careful parameter tuning, this approach can be adapted
to work under black-box conditions. Although the F1-scores
increase in some instances, the attacker is primarily interested
in their malicious data points being classiﬁed as benign, such
that an increase in F1-score is not necessarily undesirable from
the attacker’s perspective.

The confusion matrices in Tables VI and VII provide a better
insight into the performance of the classiﬁers across the experi-
ments. In comparison to the original classiﬁcation distributions
in Table V, both classiﬁers demonstrate a signiﬁcant increase
in false positives. That is, data points with an actual target
value of malicious have been misclassiﬁed as being benign.
On the other hand, when θ = 0.5, γ = 0.9, the Random Forest
model’s true positive distribution increases, which may explain
as to why its F1-score also increases.

VII. DEFENDING ADVERSARIAL MACHINE LEARNING

A few methods towards defending AML attacks have been
proposed in the literature. Two of the most popular techniques

parameters. This is intuitive given Strauss et al.’s [7] demon-
stration that ensemble machine learning algorithms are more
robust against adversarial techniques and Random Forests are
ensembles of decision trees (such as J48).

Fig. 6: Random Forest classiﬁcation performance following
adversarial training (θ = 0.2, γ = 0.4)

Fig. 7: J48 classiﬁcation performance following adversarial
training (θ = 0.1, γ = 0.3)

viii

Thus, it is evident that understanding the applicability of these
attacks in ICS systems is necessary in order to develop more
robust machine learning based IDSs.

This paper explores how adversarial learning can be used
to target supervised models by generating adversarial samples
and exploring classiﬁcation behaviours. To support the experi-
ments presented herein, an authentic power system dataset was
used to train and test widely used supervised machine learning
classiﬁers. The testing data was presented to a JSMA in order
to generate adversarial samples with a range of combinations
that affect the amount of noise and the number of features
to perturb. Such samples were evaluated against two of the
best performing classiﬁers, Random Forest and J48. Overall,
the classiﬁcation performance for both models decreased by
16 and 20 percentage points when adversarial samples were
present.

The analysis also includes the exploration of how such
samples can support the robustness of supervised models using
adversarial training. A random sample of 20% of the generated
adversarial data points were included in the original training
dataset. The models were retrained and applied on all unseen
adversarial samples. Overall, the classiﬁcation performance
of the Random Forest model reported a greater increase in
comparison to the J48 model. This demonstrates that Random
Forest is a more robust model towards classifying adversarial
samples of all combinations of JSMA parameters on the given
dataset.

IX. FUTURE WORK

Although the experiments described in this paper have
demonstrated that adversarial samples can successfully be gen-
erated using JSMA and affect the classiﬁcation performance of
state-of-the-art supervised models, it is important to note that
there are several other methods of generating such samples to
consider (e.g. Iterative Gradient Sign, Carlini Wagner, Genera-
tive Adversarial Networks). In this case, as part of future work,
this study can be extended further to include different models
as a source for generating adversarial samples. Moreover,
AML should be further investigated against other models such
as LSTMs.

Finally, the robustness of the supervised models was demon-
strated using adversarial training. It is also important to note
that this method may not always be sufﬁcient as it is difﬁcult
to anticipate all possible types of adversarial machine learning
attacks against a given system. Therefore, there is a need to
investigate other possible defense mechanisms.

VIII. CONCLUSION

Due to their effectiveness and ﬂexibility, machine learning
based IDSs are now recognised as fundamental
tools for
detecting cyber attacks in ICS systems. Nevertheless, such
systems are vulnerable to attacks that may severely undermine
or mislead their capabilities, commonly known as Adversarial
Machine Learning (AML). Such attacks may have severe
consequences in ICS infrastructures, as adversaries could
potentially modify malicious data points in order to bypass the
IDS, causing delayed attack detection and extensive damages.

REFERENCES

[1] M. Kravchik and A. Shabtai, “Detecting cyber attacks in industrial
control systems using convolutional neural networks,” in Proceedings of
the 2018 Workshop on Cyber-Physical Systems Security and PrivaCy.
ACM, 2018, pp. 72–83.

[2] Y. Ashibani and Q. H. Mahmoud, “Cyber physical systems security:
Analysis, challenges and solutions,” Computers & Security, vol. 68, pp.
81–97, 2017.

[3] A. Erba, R. Taormina, S. Galelli, M. Pogliani, M. Carminati, S. Zanero,
and N. O. Tippenhauer, “Real-time evasion attacks with physical con-
straints on deep learning-based anomaly detectors in industrial control
systems,” arXiv preprint arXiv:1907.07487, 2019.

[4] J. M. Beaver, R. C. Borges-Hink, and M. A. Buckner, “An evaluation of
machine learning methods to detect malicious scada communications,”
in 2013 12th International Conference on Machine Learning and Ap-
plications, vol. 2.

IEEE, 2013, pp. 54–59.

[5] J. Bigham, D. Gamez, and N. Lu, “Safeguarding scada systems
with anomaly detection,” in International Workshop on Mathematical
Methods, Models, and Architectures for Computer Network Security.
Springer, 2003, pp. 171–182.

[6] M. A. Teixeira, T. Salman, M. Zolanvari, R. Jain, N. Meskin, and
M. Samaka, “Scada system testbed for cybersecurity research using
machine learning approach,” Future Internet, vol. 10, no. 8, p. 76, 2018.
[7] T. Strauss, M. Hanselmann, A. Junginger, and H. Ulmer, “Ensemble
methods as a defense to adversarial perturbations against deep neural
networks,” arXiv preprint arXiv:1709.03423, 2017.

[8] J. Goh, S. Adepu, M. Tan, and Z. S. Lee, “Anomaly detection in
cyber physical systems using recurrent neural networks,” in 2017 IEEE
18th International Symposium on High Assurance Systems Engineering
(HASE).

IEEE, 2017, pp. 140–145.

[9] L. A. Maglaras and J. Jiang, “Intrusion detection in scada systems
using machine learning techniques,” in 2014 Science and Information
Conference.

IEEE, 2014, pp. 626–631.

[10] O. Linda, T. Vollmer, and M. Manic, “Neural network based intrusion
detection system for critical infrastructures,” in 2009 international joint
conference on neural networks.

IEEE, 2009, pp. 1827–1834.

[11] M. Rigaki, “Adversarial deep learning against intrusion detection clas-

siﬁers,” 2017.

[12] B. Biggio, G. Fumera, and F. Roli, “Multiple classiﬁer systems un-
der attack,” in International Workshop on Multiple Classiﬁer Systems.
Springer, 2010, pp. 74–83.

[13] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. D.
Tygar, “Adversarial machine learning,” in Proceedings of the 4th ACM
workshop on Security and artiﬁcial intelligence. ACM, 2011, pp. 43–
58.

[14] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. Rubinstein, U. Saini,
C. A. Sutton, J. D. Tygar, and K. Xia, “Exploiting machine learning to
subvert your spam ﬁlter.” LEET, vol. 8, pp. 1–9, 2008.

[15] Y. Zhou, M. Kantarcioglu, B. Thuraisingham, and B. Xi, “Adversarial
support vector machine learning,” in Proceedings of the 18th ACM
SIGKDD international conference on Knowledge discovery and data
mining, 2012, pp. 1059–1067.

[16] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel,
“Adversarial examples for malware detection,” in European Symposium
on Research in Computer Security. Springer, 2017, pp. 62–79.
[17] W. Hu and Y. Tan, “Generating adversarial malware examples for black-
box attacks based on gan,” arXiv preprint arXiv:1702.05983, 2017.
[18] G. Zizzo, C. Hankin, S. Maffeis, and K. Jones, “Adversarial machine
learning beyond the image domain,” in 2019 56th ACM/IEEE Design
Automation Conference (DAC).

IEEE, 2019, pp. 1–4.

[19] S. Yaghoubi and G. Fainekos, “Gray-box adversarial testing for control
systems with machine learning components,” in Proceedings of the 22nd
ACM International Conference on Hybrid Systems: Computation and
Control, 2019, pp. 179–184.

[20] T. H. Morris, Z. Thornton, and I. Turnipseed, “Industrial control system
simulation and data logging for intrusion detection system research,” 7th
Annual Southeastern Cyber Security Summit, pp. 3–4, 2015.

[21] S. D. Anton, S. Kanoor, D. Fraunholz, and H. D. Schotten, “Evaluation
of machine learning-based anomaly detection algorithms on an industrial
modbus/tcp data set,” in Proceedings of the 13th International Confer-
ence on Availability, Reliability and Security, 2018, pp. 1–9.

[22] A. Robles-Durazno, N. Moradpoor, J. McWhinnie, and G. Russell,
“A supervised energy monitoring-based machine learning approach for
anomaly detection in a clean water supply system,” in 2018 International
Conference on Cyber Security and Protection of Digital Services (Cyber
Security).

IEEE, 2018, pp. 1–8.

[23] I. Ullah and Q. H. Mahmoud, “A hybrid model for anomaly-based
intrusion detection in scada networks,” in 2017 IEEE International
Conference on Big Data (Big Data).

IEEE, 2017, pp. 2160–2167.

[24] R. L. Perez, F. Adamsky, R. Soua, and T. Engel, “Machine learning for
reliable network attack detection in scada systems,” in 2018 17th IEEE
International Conference On Trust, Security And Privacy In Computing
And Communications/12th IEEE International Conference On Big Data
Science And Engineering (TrustCom/BigDataSE). IEEE, 2018, pp. 633–
638.

[25] H. Qu, J. Qin, W. Liu, and H. Chen, “Instruction detection in scada/mod-
bus network based on machine learning,” in International Conference
on Machine Learning and Intelligent Communications. Springer, 2017,
pp. 437–454.

ix

[26] J. Yeckle and S. Abdelwahed, “An evaluation of selection method in
the classiﬁcation of scada datasets based on the characteristics of the
data and priority of performance,” in Proceedings of the International
Conference on Compute and Data Analysis, 2017, pp. 98–103.

[27] E. Hoxha, Y. Vidal Segu´ı, and F. Pozo Montero, “Supervised classiﬁca-
tion with scada data for condition monitoring of wind turbines,” in 9th
ECCOMAS Thematic Conference on Smart Structures and Materials,
2019, pp. 263–273.

[28] I. Fraz˜ao, P. H. Abreu, T. Cruz, H. Ara´ujo, and P. Sim˜oes, “Denial of
service attacks: detecting the frailties of machine learning algorithms
in the classiﬁcation process,” in International Conference on Critical
Information Infrastructures Security. Springer, 2018, pp. 230–235.
[29] H. Lahza, K. Radke, and E. Foo, “Applying domain-speciﬁc knowledge
to construct features for detecting distributed denial-of-service attacks
on the goose and mms protocols,” International Journal of Critical
Infrastructure Protection, vol. 20, pp. 48–67, 2018.

[30] J. R. Werling, “Behavioral proﬁling of scada network trafﬁc using
machine learning algorithms,” AIR FORCE INSTITUTE OF TECH-
NOLOGY WRIGHT-PATTERSON AFB OH GRADUATE SCHOOL
OF , Tech. Rep., 2014.

[31] I. A. Siddavatam, S. Satish, W. Mahesh, and F. Kazi, “An ensemble
learning for anomaly identiﬁcation in scada system,” in 2017 7th
International Conference on Power Systems (ICPS).
IEEE, 2017, pp.
457–462.

[32] D. Wang, X. Wang, Y. Zhang, and L. Jin, “Detection of power grid
disturbances and cyber-attacks based on machine learning,” Journal of
Information Security and Applications, vol. 46, pp. 42–52, 2019.
[33] I. Abdallah, V. Dertimanis, H. Mylonas, K. Tatsis, E. Chatzi, N. Dervilis,
K. Worden, and E. Maguire, “Fault diagnosis of wind turbine structures
using decision tree learning algorithms with big data,” Safety and
Reliability–Safe Societies in a Changing World, pp. 3053–3061, 2018.
[34] J. Gao, L. Gan, F. Buschendorf, L. Zhang, H. Liu, P. Li, X. Dong,
and T. Lu, “Lstm for scada intrusion detection,” in 2019 IEEE Paciﬁc
Rim Conference on Communications, Computers and Signal Processing
(PACRIM).

IEEE, 2019, pp. 1–5.

[35] C. Feng, T. Li, and D. Chana, “Multi-level anomaly detection in
industrial control systems via package signatures and lstm networks,” in
2017 47th Annual IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN).

IEEE, 2017, pp. 261–272.

[36] S. Pan, T. Morris, and U. Adhikari, “Classiﬁcation of disturbances and
cyber-attacks in power systems using heterogeneous time-synchronized
data,” IEEE Transactions on Industrial Informatics, vol. 11, no. 3, pp.
650–662, 2015.

[37] “Powersystem dataset readme.pdf,” (Accessed on 03/18/2020).
[38] D. H. Wolpert, “The supervised learning no-free-lunch theorems,” in

Soft computing and industry. Springer, 2002, pp. 25–42.

[39] “Weka 3 - data mining with open source machine learning soft-
ware in java,” https://www.cs.waikato.ac.nz/ml/weka/, (Accessed on
06/03/2018).

[40] R. C. B. Hink, J. M. Beaver, M. A. Buckner, T. Morris, U. Adhikari,
and S. Pan, “Machine learning for power system disturbance and cyber-
attack discrimination,” in 2014 7th international symposium on resilient
control systems (ISRCS).

IEEE, 2014, pp. 1–8.

[41] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar,
“Can machine learning be secure?” in Proceedings of the 2006 ACM
Symposium on Information, computer and communications security.
ACM, 2006, pp. 16–25.

[42] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,” in
2016 IEEE European Symposium on Security and Privacy (EuroS&P).
IEEE, 2016, pp. 372–387.

[43] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in
Advances in neural information processing systems, 2014, pp. 2672–
2680.

[44] N. Papernot, P. McDaniel, and I. Goodfellow, “Transferability in ma-
chine learning: from phenomena to black-box attacks using adversarial
samples,” arXiv preprint arXiv:1605.07277, 2016.

[45] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing

adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.

[46] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated gradients give a
false sense of security: Circumventing defenses to adversarial examples,”
arXiv preprint arXiv:1802.00420, 2018.

