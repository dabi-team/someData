1 

Greening Cloud-Enabled Big Data Storage 
Forensics: Syncany as a Case Study 

Yee-Yang Teing, Ali Dehghantanha, Senior Member, IEEE, Kim-Kwang Raymond Choo, Senior 
Member, IEEE, Mohd Taufik Abdullah, and Zaiton Muda 

 

the 

investigation  process  by 

forensics  researchers  are  moving 

Abstract—The  pervasive  nature  of  cloud-enabled  big  data 
storage solutions introduces  new challenges in the identification, 
collection,  analysis,  preservation  and  archiving  of  digital 
evidences. Investigation of such complex platforms to locate and 
recover traces of criminal activities is a time-consuming process. 
Hence,  cyber 
towards 
streamlining 
locating  and 
documenting  residual  artefacts  (evidences)  of  forensic  value  of 
users’ activities on cloud-enabled big data platforms, in order to 
reduce  the  investigation  time  and  resources  involved  in  a  real-
world investigation.  In this paper, we seek to determine the data 
remnants  of  forensic  value  for  Syncany  private  cloud  storage 
service,  a  popular  storage  engine  for  big  data  platforms.  We 
demonstrate the types and the locations of the artefacts that can 
be forensically recovered. Findings from this research contribute 
to  an  in-depth  understanding  of  cloud-enabled  big  data  storage 
forensics, which can result in reduced time and resources spent in 
real-world investigations involve Syncany-based cloud platforms.  

Index  Terms—Green  forensics,  big  data  forensics,  cloud 

forensics, Syncany forensics 

I.  INTRODUCTION 

B 

ig  data  and  cloud  computing  are  two  information  and 
communications technology (ICT) trends in recent years. 
For  example,  a  2013  survey  by  the  International  Data 
Corporation  (IDC)  predicted 
the  cloud  computing 
spending will exceed USD 107 billion, and will drive 17% of 
the  IT  product  expenditures  by  2017  [1].  Gartner  [2]  also 
predicted that the cloud computing market will be worth more 

that 

Yee-Yang Teing is with the Department of Computer Science, Faculty of 
Computer  Science  and  Technology,  Universiti  Putra  Malaysia,  Serdang, 
43400 Selangor, Malaysia, as well as the School of Computing, Science and 
Engineering,  University  of  Salford,  Salford,  Greater  Manchester  M5  4WT, 
UK (e-mail: teingyeeyang@gmail.com).  

Ali  Dehghantanha  is  with  the  School  of  Computing,  Science  and 
Engineering,  University  of  Salford,  Salford,  Greater  Manchester  M5  4WT, 
UK (e-mail: A. Dehghantanha@salford.ac.uk). 

Kim-Kwang  Raymond  Choo  is  with  the  Department  of  Information 
Systems  and  Cyber  Security,  University  of  Texas  at  San  Antonio,  San 
Antonio,  TX  78249-0631,  USA,  as  well  as  the  Information  Assurance 
Research Group, University of South Australia, Adelaide, SA 5001, Australia 
(e-mail: raymond.choo@fulbrightmail.org). 

Zaiton  Muda  and  Mohd  Taufik  Abdullah  are  with  the  Department  of 
Computer Science, Faculty of Computer Science and Technology, Universiti 
Putra  Malaysia, 
(e-mail: 
zaitonm@upm.edu.my and taufik@upm.edu.my).  

Selangor,  Malaysia 

Serdang, 

43400 

than USD 1.1 trillion by 2017 as more big data platforms and 
big  data  analytical  solutions  are  deployed  over  cloud  storage 
services. 

There  are, however, situations  where  public clouds are  not 
suitable to host the big data platforms (e.g. inadequate privacy 
protection for data). Public cloud users are often billed for the 
resources consumed, including the storage input output (I/O), 
CPUs  and  memory,  and  the  cost  can  become  prohibitively 
expensive  in  storing,  accessing  and  analysing  high  velocity, 
variety,  veracity  and  volume  data.  There  were  also  concerns 
about  Cloud  Service  providers  (CSPs)  being  able  to  infer  or 
profile users based on the hosted / stored data [3]–[5]. Thus, it 
is unsurprising that cloud and big data security and privacy are 
current research focus [6]–[15]. Data owners in industries such 
as  healthcare  and  banking  are  also  subject  to  exacting 
regulatory requirements,  which restrict  the  outsourcing of the 
data for storage and analysis. The concerns are compounded in 
the 
the  PRISM  and  MUSCULAR 
surveillance  programs  that  allegedly  enable  government  and 
law  enforcement  agencies  to  tap  into  CSPs’  data  centres 
without  a  search  warrant  [16].  Therefore,  in-house  private 
dedicated  cloud  storage  services  have  become  an  ideal 
solution for big data platforms. 

leakage  of 

recent 

Cloud forensics, an emerging research trend, has real-world 
implications  for  both  criminal 
investigations  and  civil 
litigations  [17].  Due  to  the  nature  of  cloud-enabled  big  data 
storage  solutions  (e.g.  data  physically  stored  in  distributed 
servers), identification of residual evidences may be a ‘finding 
a  needle  in  a  haystack’  exercise  [18]–[21].  Even  if  the  data 
tools, 
could  be 
techniques  and  practices  are  unlikely  to  be  adequate  [22]. 
These 
cross-jurisdictional 
investigations,  which  could  prohibit  the  transfer  of  evidential 
data  due  to  the  lack  of  cross-nation  legislative  agreements  in 
place  [23]–[26].  Overcoming  these  investigation  challenges 
demands  significant  time  and  resources  of  an  investigation 
team [27].  

traditional  evidence  collection 

escalated 

located, 

issues 

are 

in 

In this paper, we extend our previous work [28]1 seeking to 
identify  potential  artefacts  that  remain  on  client  devices  and 
cloud servers involving the use of Syncany as a private cloud 
storage solution supporting big-data platforms. We attempt to 
answer following questions: 

1. What residual artefacts can be recovered from the hard disk 
and  memory  of  using  Syncany  desktop  clients,  and  the 

1  This  paper  is  an  extended  conference  version  of  [28],  with  more  than 

50% new content. 

 
 
 
 
 
 
locations  of  the  data  remnants  on  a  Windows  8.1,  Ubuntu 
14.04.1 LTS (Ubuntu), and Mac OS X Mavericks 10.9.5 (Mac 
OS) client device? 
2. What  data  of  forensics  interest  can  be  recovered  from  a 
cloud  server  hosting  Syncany  private  cloud  storage  service 
and  the  location  of  the  data  remnants  on  an  Ubuntu  14.04.1 
LTS server? 
3. What  data  of  forensics  interest  can  be  collected  from  the 
network traffic communications between Syncany clients and 
servers? 

We  describe  related  works  and  Syncany  private  cloud 
service  in  the  next  two  sections.  Section   IV  outlines  our 
research  methodology  and  experiment  environment  setup.  In 
Sections   V  and   VI,  we  discuss  findings  from  the  clients  and 
servers,  respectively.  Section   VII  summarises  the  research 
findings.  Finally,  in  Section   VIII,  we  conclude  the  paper and 
outline potential future research areas.  

II.  RELATED WORK  

to 

logging  mechanism,  designed 

Due  to  the  underlying  legal  challenges  and  complexity 
involved  in  cross-jurisdictional  cloud  forensic  investigations, 
Marty  [29]  and  Shields  et  al.  [30]  proposed  a  proactive 
log 
application-level 
information  of  forensics  interest  which  can  also  be  used  in 
incident  response.  Forensic  researchers  such  as  Dykstra  and 
Sherman [31], Gebhardt
and Reiser [32], Quick et al. [25], and 
Martini  and  Choo  [33]  have  also  presented  frameworks  and 
prototype  implementations  to  support  collection  of  evidential 
materials  using  Application  Programming  Interfaces  (API) 
from  a  variety  of  cloud  storage  platforms.  Although  data 
collection  using  APIs  could  potentially  reduce  interactions 
with  the  CSPs,  it  could  be  limited  by  the  APIs’  features  set 
[33]. 

Quick  and  Choo  [34]  and  Teing  et  al.  [35]  studied  the 
integrity of data downloaded from the web and desktop clients 
of  Dropbox,  Google  Drive,  Skydrive,  and  Symform  and 
identified  that  the  act  of  downloading  data  from  client 
applications does not tamper the contents of the files, despite 
changes  in  file  creation/modification  times.  Other  cloud 
applications such as Evernote [36], Amazon S3 [36], Dropbox 
[36],  [37],  Google  Drive  [36],  [38],  Microsoft  Skydrive  [39], 
Amazon  Cloud  Drive  [40],  BitTorrent  Sync  [41],  SugarSync 
[42],  Ubuntu  One  [43],  huBic  [44],  Syncany  [28],  Symform 
[35],  as  well  as  mobile  cloud  apps  [45],  [46]  have  also  been 
examined.  These  forensics  studies  located  artefacts  of  client 
applications from the user settings and application data resided 
on  the  media  storage  through  keyword  search.  Quick  and 
Choo  [37]–[39] also identified that data  erasing tools such as 
Eraser  and  CCleaner  do  not  completely  remove  the  data 
remnants  from  Dropbox,  Google  Drive,  and  Microsoft 
SkyDrive.  

The  first  cloud  forensic  framework  was  proposed  by 
Martini  and  Choo  [47],  which  was  derived  based  upon  the 
frameworks  of  McKemmish  [48]  and  NIST  [49].  The 
framework  was  used  to  investigate  ownCloud  [50],  Amazon 
EC2 [51], VMWare [33], and XtreemFS [52]. Quick et al. [25] 
further extended and validated the four-stage framework using 
SkyDrive,  Dropbox,  Google  Drive,  and  ownCloud.  Chung  et 

2 

al.  [36]  proposed  a  methodology  for  cloud  investigation  on 
Windows,  Mac  OS, 
iOS,  and  Android  devices.  The 
methodology was then used to investigate Amazon S3, Google 
Docs,  and  Evernote.  Scanlon  et  al. 
[53]  outlined  a 
methodology  for  remote  acquisition  of  evidence  from 
decentralised  file  synchronisation  network  which  utilised  to 
investigate BitTorrent Sync [54], [55]. In another study, Teing 
et al. [56] proposed a methodology for investigating the newer 
BitTorrent Sync application (version 2.0) or any third party or 
Original  Equipment  Manufacturer  (OEM)  applications. Do  et 
al. [57] proposed an adversary model for digital forensics, and 
they demonstrated how such an adversary model can be used 
to investigate mobile devices (e.g. Android smartwatch  –  Do 
et al. [58] and apps). Ab Rahman et al. [59], on the other hand, 
proposed  a  conceptual  forensic-by-design  framework 
to 
integrate forensics tools and best practices in the development 
of  cloud  systems.  Tzuen  et.al.  [27],  discussed  about 
opportunities 
streamlining  digital 
investigation  by  referring  to  research  that  identified  and 
documented residual artefacts. 

challenges 

and 

in 

The  majority  of  existing  published  works  in  the  field  of 
cloud forensics have focused on client investigations on public 
cloud  infrastructures.  This  paper  is  one  of  few  studies  which 
focus  on  both  client  and  server  forensics.  Such  a  study  may 
assist in reducing the carbon  footprints of investigation cases 
involving private cloud storage solutions.  

III.  BACKGROUND 

Syncany  is  a  popular  open  source  and  cross-platform  cloud 
storage  solution  written  in  Java  and  supports  data  storage  on 
different  backend  media  (e.g.,  FTP,  Box.net,  WebDAV  and 
SFTP, Amazon S3, Google Storage, IMAP, Local, Picasa and 
Rackspace  Cloud  Files).  Hence,  it  does  not  require  a  server-
side  software  to  be  installed  for  deployment  [60].  The  data 
synchronisation  is  facilitated  by  the  sync  link  or  repo2  URL 
(similar to the concept of BitTorrent Sync, with the difference 
being  that  Syncany  uses  a  central  storage  infrastructure). 
Syncany  encrypts  the  sync  files  locally  (using  128-bit 
AES/GCM  encryption)  before  uploading  them  to  the  central 
(offsite) storage (by default); hence, only clients in possession 
of  the  password  can  access  repositories.  The  data  model 
consists of three main entities as follows [60]: 

  Versioning: Syncany captures different versions of a 
file  and  keeps  track  of  the  changes  using  metadata 
such  as  date,  time,  size  and  checksums.  There  are 
three  primary  versioning  concepts,  which  are 
database  versions,  file  histories,  and  file  versions.  A 
database  version  represents  the  point  in  time  when 
the  file  tree  is  captured.  Each  database  version 
contains  a  list  of  file  histories,  representing  the 
identity  of  a  file.  Each  file  history  contains  a 
collection  of 
the 
incarnations of a file. 
  Deduplication/Chunking: 

data 
deduplication technique to break individual files into 

file  versions, 

representing 

Syncany 

uses 

2 In Syncany, a repository (or commonly known as a repo) refers to a dumb 
storage  i.e.,  a  folder  storing  the  packaged  data  and  metadata  about  a  sync 
folder in the backend system [60]. 

 
 
 
small  chunks  on 
the  client.  The  chunks  are 
represented  in  data  blobs  (each  about  8-32  KB  in 
size), which are identified by their checksum. 

  Multichunking:  Individual  chunks  are  grouped  into 
multichunks, compressed and encrypted before being 
uploaded to the offsite storage. 

Fig.    1  shows  an  example  of  logical  data  model  of  a 
Syncany repository. The entities are  stored locally  in the 
form 
the 
org.syncany.database package and tables are stored in the 
local HSQLDB-based database in .XML format [61].  

plain-old-Java-object 

(POJOs) 

of 

in 

Syncany uses a command line interface by default, but users 
can  manually  install  a  plugin  which  supports  graphical  user 
interface  (GUI)  as  well.  Syncany  commands  of  forensic 
interest are as follows [62]: 

 

 

 

 

It 

link 

creates 

structured 

sy  init:  To  initialise  the  repository  for  a  new  sync 
directory. 
sync-directory-specific 
a 
config.xml  and repo file. The  former holds the  local 
configuration  information  while  the  latter  contains 
the  chunking/crypto  details  required  to  initialise  the 
remote  repository.  This  command  also  generates  the 
sync link in two formats: a commonly used encrypted 
link  structured  such  as  syncany://storage/1/<master-
salt>/<encrypted-config>,  where  both  <master-salt> 
and  <encrypted-config>  are  base58  encoded;  a 
plaintext 
as 
such 
syncany://storage/1/not-encrypted/<plaintext-
config>,  where  the  <plaintext-config>  is  a  base58-
encoded  representation  of  the  storage/connection 
config. 
sy connect: To connect to an existing repository using 
the  sync  link  or  manually  using  the  repo  URL.  This 
command  is  similar  to  sy  init,  with  the  difference 
being that it downloads the repo files from  a remote 
storage. 
sy  status:  Lists  changes  made  to  the  local  sync  files 
by  comparing  the  local  file  tree  (e.g.,  last  modified 
dates and file sizes) with the local database. 
sy  up:  Detects  changes  in  the  local  sync  directory 
(using  the  ‘sy  status’  command),  indexes  new  files 
and  uploads  changes  to  the  remote  repository  (using 
the  ‘sy  up’  command).  File  changes  are  packaged 
into  new  multichunks  and  uploaded  to  an  offsite 

3 

 

 

storage, alongside the delta metadata database. 
sy ls-remote: Queries the remote storage and lists the 
client  database  versions  that  have  not  yet  been 
downloaded/processed. 
sy  down:  Detects  file  changes  made  by  other  clients 
(as  identified  by  the  ‘sy  ls-remote’  command).  The 
command first downloads the metadata of relevance. 
Then, it evaluates which multichunks are changed or 
required.  Finally,  it  downloads  and  arranges  the 
multichunks  according  to  the  vector  clocks,  when 
necessary. 

IV.  RESEARCH METHODOLOGY 

Similar  to  Quick  and  Choo  [37]–[39]  and  Teing  et  al.  [35], 
[63]  Our  test  environments  consist  of  four  (4)  VMware 
Workstations  (VMs),  one  (1)  for  server  and  three  (3)  for 
desktop  clients  as  detailed  in  Table  I.  The  VMs  were  hosted 
using  VMware  Fusion  Professional  version  7.0.0  (2103067) 
on a Macbook Pro (Late 2012) running Mac OS X Mavericks 
10.9.5,  with  a  2.6GHz  Intel  Core  i7  processor  and  16GB  of 
RAM.  As  explained  by  Quick  and  Choo  [37]–[39],  virtual 
environments  are  preferred 
for  examination  of  cloud 
applications.  It  is  noteworthy  that  the  Syncany  application 
does  not  require  an  account,  and  hence  no  user  account  was 
The 3111th email 
created. 
messages of the Berkeley Enron email dataset (downloaded fr
om http://bailando.sims.berkeley.edu/enron_email.html)  were 
used to create a set of sample files and saved in .RTF, .TXT, 
.DOCX, 
.PDF  formats, 
providing a basis for replication of the experiments in future. 
The set of sample files were placed in a new directory on the 
clients’  workstations,  before  being  uploaded  to  the  servers, 
which  subsequently  being  downloaded  to  the  corresponding 
client devices. 

.JPG  (print  screen), 

.ZIP,  and 

In  our  experiments,  we  conducted  a  predefined  set  of 
activities  to  simulate  various  real  world  scenarios  of  using 
included 
Syncany  private  cloud  storage  service.  These 
installation and uninstallation of the client applications as well 
as  accessing,  uploading,  downloading,  viewing,  deleting,  and 
unsyncing  the  sync  files.  A  snapshot  was  undertaken  of  the 
VM  workstations  prior  and  after  each  experiment.  Wireshark 
was  deployed  on 
the 
communication  network  traffic.  For  the  purpose  of  this 
research, we used WebDAV as the carrying protocol to enable 

the  host  machine 

to  capture 

Fig.  1. Logical data model of a Syncany repository (Adapted from [12]). 

 
 
 
Server configurations 

Ubuntu Server 
Operating system : Ubuntu 14.04.1 LTS 
Virtual memory size: 1GB 
Virtual storage size: 20GB 
Web application: Apache 2 
Storage type: WebDAV 
Database server: HSQL Database Engine 2.3.0 
IP address/URL: http://172.16.38.180/webdav 

TABLE II 
 SYSTEM CONFIGURATIONS FOR SYNCANY FORENSICS 

4 

Client configurations 

Windows Client 
Operating system: Windows 8.1 Professional (Service Pack 2, 64-bit, build 9600). 
Virtual memory size: 2GB 
Virtual storage size: 20GB 
Client application: Syncany 0.4.6-alpha 
Plugins installed: WebDAV and GUI 
Ubuntu Client 
Operating system: Ubuntu 14.04.1 LTS 
Virtual memory size: 1GB 
Virtual storage size: 20GB 
Client application: Syncany 0.4.6-alpha 
Plugins installed: WebDAV and GUI 
Mac OS Client 
Operating system: Mac OS X Mavericks 10.9.5 
Virtual memory size: 1GB 
Virtual storage size: 60GB 
Client application: Syncany 0.4.6-alpha 
Plugins installed: WebDAV and GUI 

TABLE I 
 TOOLS PREPARED FOR SYNCANY FORENSICS 

Usage 

Tool 
FTK Imager Version 3.2.0.0 
dd version 1.3.4-1 
Autopsy 3.1.1 

raw2vmdk 
HxD Version 1.7.7.0 
Volatility 2.4 

SQLite  Browser  Version 
3.4.0 
Photorec 7.0 
File juicer 4.45 
BrowsingHistoryView v.1.60 
Thumbcacheviewer  Version 
1.0.2.7 
Windows 
Version 1.0 
Console Version 10.10 (543) 
Windows 
2.6.0.0 
NTFS Log Tracker 
SQL Workbench/J Build 118. 

Event  Viewer 

File  Analyser 

To create a forensic image of the .VMDK files. 
To produce a bit-for-bit image of the .VMEM files. 
To  parse  the  file  system,  produce  directory  listings,  as  well  as  extracting/analysing  the  files,  Windows  registry,  swap 
file/partition, and unallocated space of the forensic images. 
To create .VMDK files from raw dd image. 
To conduct keyword searches in the unstructured datasets. 
To  extract  the  running  processes  and  network  information  from  the  physical  memory  dumps,  dumping  files  from  the 
memory  space  of  the  Syncany  client  applications,  and  detecting  the  memory  space  of  a  string  (using  the  ‘pslist’, 
‘netstat’/’netscan’, ‘memdump’ and ‘yarascan’ functions). 
To view the contents of SQLite database files. 

To data carve the unstructured datasets. 
To extract files from files. 
To analyse the web browsing history. 
To examine the Windows thumbnail cache. 

To view the Windows event logs. 

To view log files. 
To analyse the Windows prefetch and link files. 

To parse and analyse the $LogFile, $MFT, and $UsnJrnl New Technology File System (NTFS) files. 
To view the contents of Hyper SQL Database  (HSQLDB). 

the study of the network’s inner workings. Other forensic tools 
prepared  for  experiments  are  highlighted  in  Table  II.  The 
experiments  were  repeated  at  thrice  (at  different  dates)  to 
ensure consistency of findings. 

The reliability of digital evidences rest upon the ability of the 
practitioners  to  adhere  to  generally  accepted  principles, 
standards,  guidelines,  procedures,  and  best  practices  [49], 
[64].  In  this  research,  we  adopted  the  cloud  forensics 
framework  suggested  by  Martini  and  Choo  [47]  for  our 
investigation as follow: 

1.  Evidence  source  identification  and  preservation.  We 
identified  the  physical  hardware  of  interest,  which  contained 
the virtual disk (VMDK) and memory (VMEM)  files in each 
VM  folder.  Then,  we  created  a  forensic  copy  of  the  VMDK 
and  VMEM  files  in  E01  container  and  raw  image  file  (dd) 
formats  respectively.  For  all  the  forensic  images  created,  we 
calculated the MD5 and a SHA1 hash values for each original 
copy and subsequently verified for each working copy.  

2.  Collection.  We  extracted  test  data  that  matched  the  search 
terms  ‘syncany  and  ‘enron’  in  the  hard  disk  images.  These 
included SQLite databases, Apple Property List (PLIST) files, 
event logs, shortcuts, thumbnail cache, and Windows prefetch, 
$MFT,  $LogFile  and  $UsnJrnl  system  files.  Data  from  the 
physical  memory  dumps  were  collected  using  Volatility, 
Photorec  file  carver,  and  HxD  Hex  Editor;  network  captures 
Wireshark and Netminer. Similar to the first stage, for all the 
extracted  files,  we  calculated  the  MD5  and  a  SHA1  hash 
values  for  each  original  copy  and  subsequently  verified  for 
each working copy. 

3. Examination and analysis. This phase is concerned with the 
assessment  and  extraction  of  the  evidential  information  from 
the collected data. We seek to recover the following artefacts 
of evidential value from client devices: 

  Sync and file  management  metadata – The property 
to 

information  generated  by 

the  Syncany  client 

 
 
 
 
 
facilitate the synchronisation process with the hardware 
hosting  the  cloud  environment.  These  include  the 
node/peer  IDs,  usernames,  device  names,  folder/file 
IDs, share keys (if any), and other property information 
that  can 
file-to-user 
mappings. 

file-to-device  and 

facilitate 

  Authentication and encryption metadata – Artefacts 
describing  the  encryption  algorithm,  public/private 
keys  or  certificates,  encryption  salt, 
initialization 
vectors  (IVs),  password  hashes,  login  URL,  and 
(possibly)  plaintext  credentials  that  could  provide  the 
opportunity  to  login  to  the  user’s  online  account  or 
undertake offline brute-force attacks to obtain the login 
password.   

  Cloud  transaction  logs  –  Records  of  user  actions 
(install,  uninstall,  create  user  account,  file  upload,  file 
download,  file  modification,  file  delete,  login,  logoff, 
etc.)  made  to  the  cloud  instance,  which  are  often 
accompanied  by  the  directory  path  and  the  timestamp 
information useful for timeline analysis. 

  Data  storage  –  Actual  storage  or  temporary  holding 
place  (e.g.,  the  application  generated  sync/archive 
folders, OS and web browser’s thumbnail caches, trash 
folders, and unallocated space) that may be relied upon 
for  recovering  the  synced  files  in  case  the  cloud 
environment is not accessible or the actual copies have 
been deleted. 

  Network  captures  –  Copies  of  network  packets 
transmitted between the client and the hardware hosting 
the  cloud  environment 
the  digital 
fingerprints of the user actions. 

that  contain 

  Volatile  memory  dumps  –  Volatile  memory  is  a 
potential source of many residual artefacts contributing 
to  extended  investigation  of  above  mentioned  items 
[65]. 

We seek to recover the following artefacts of evidential value 
from the server: 

  Administrative  and  file  management  metadata  – 
The server application generated instances that provide 
copies  of  the  sync  and  file  management  as  well  as 
authentication and encryption metadata of the clients. 
  Cloud  logging  and  authentication  data:  Records  of 
cloud  transaction  and  authentication  events/requests 
made by the clients to the cloud hosting instance, which 
are  often  accompanied  with  identifying  information 
such  as  node  IDs,  logical  addresses,  session  IDs,  and 
timestamps 
provenance 
determination. 

that  will 

facilitate 

  Data repositories - The data uploaded by the users to 
the  server  (typically  present  in  encrypted  chunks  or 
blocks),  which  may  assist  a  practitioner  in  recovering 
the synced files from the cloud hosting environment. 

4.  Reporting  and  presentation.  This  final  phase  relates  to  the 
preparation and presentation of the information resulting from 
the analysis phase. As observed by Cahyani et al. [66]–[68], it 
is  important  for  forensics  examiners  to  articulate  and  explain 
complex and technical  forensic terminologies to the  judiciary 

5 

and juries in order for them to  understand “how these crimes 
were  committed,  what  digital  evidence  is  and  where  it  may 
exist,  and  how  the  process  of  digital  evidence  collection  was 
undertaken by the forensic investigators” [69]. 

V.  SYNCANY ANALYSIS ON DESKTOP CLIENTS 

‘Syncany’ 

user-specific  

Inspection of the directory listings determined that artefacts of 
the  Syncany  client  could  be  predominantly  located  in  the 
hidden ‘.syncany’ directory specific to the sync folders as well 
as 
directory in 
%Users%\<User Profile>\AppData\Roaming\, /home/<User 
Profile>/.config/, and /Users/<User Profile>/.config/  on  the 
Windows,  Ubuntu,  and  Mac  OSX  clients  (respectively).  The 
former  stores  folder-specific  configuration  while  the  latter 
manages  the  user-specific  configuration.  Both  the  ‘.syncany’ 
and  ‘syncany’  directories  remained  after  uninstallation  of  the 
client applications. 

A.  Sync and File Management Metadata 

Analysis  of  the  ‘syncany’  located  the  last  used  PID  for  the 
daemon  process  in  the  %Syncany%/daemon.pid  file,  which 
could  be  useful  information  for  file-to-process  mapping  i.e., 
dumping the memory space using the ‘memdump’ function of 
Volatility.  The  /%Syncany%/daemon.xml  file  held  the  folder 
metadata  meant  for  the  daemon  process  to  identify  sync 
directories.  Each  sync  folder  creates  a  ‘folder’  subtag  in  the 
‘folders’  property  to  hold  the  directory  path  and  information 
about whether the folder is enabled in the ‘path’ and ‘enabled’ 
entries. 

in 

and 

names 

display 

Examination  of  the  ‘.syncany’  directories  revealed  20-
character  machine 
the 
/%.syncany%/config.xml  file,  in  the  ‘machineName’  and 
‘displayName’ properties (respectively). The machine name is 
the  random 
identify  a 
local  machine  name  used 
computer/user  for a  repository,  while  the display  name is  the 
human readable user name for  given local machine [62]. The 
file  also  described  the  type  of  backend  storage  in  use  in  the 
‘type’ entry of the ‘connection’ property. Knowing the type of 
backend storage in use may enable a practitioner to extrapolate 
the  potential  artefacts  in  a  Syncany  investigation  e.g.,  if  the 
WebDAV  protocol  is  used,  the  web  server  logs  can  be  a 
potential source of relevant information. 

to 

in 

table 

located 

The  server’s  displayed  name  was 

the 
/%syncany%/logs/syncany.log,  from  the  log  entry  “1-10-15 
18:52:20.274 | PluginSettingsP | main           | INFO : Setting 
field 'username' with value 'syncanyserver'”. Further details of 
the sync and file management metadata could be located in the 
‘FILEVERSION_FULL’ 
the 
/%.syncany%/db/local.db database. Specifically, we recovered 
the  unique  identifications  (ID),  directory  paths,  file  sizes, 
SHA1  checksums,  and  permission  information  (in  POSIX 
format) for the sync folders and files specific to a repository in 
the 
‘SIZE’, 
‘FILECONTENT_CHECKSUM’,  and  ‘POSIXPERMS’  table 
fields. The file checksum could enable identification of a sync 
file outside the sync folder i.e., when the file has been moved 
from the sync folder. 

‘FILEHISTORY_ID’, 

‘PATH’, 

of 

 
B.  Authentication and Encryption Metadata 

are 

and 

encryption  metadata 

%.syncany%/config.xml, 

Our  findings  suggested  that  the  files  of  interest  for  the 
in 
authentication 
/%syncany%/config.xml, 
and 
/%.syncany%/master  files.  The  /%syncany%/config.xml  file 
records  the  masterkey  and  salt  used  to  encrypt  the  sensitive 
information  in  the  /%.syncany%/config.xml  file  [62],  in  the 
‘configEncryptionKey’  tag  of  the  ‘userConfig’  property; 
useful when seeking to brute force the repo password stored in 
the  ‘password’  tag  of  the  ‘connection’  property  in  the 
the 
/%.syncany%/config.xml 
/%.syncany%/config.xml  file  holds  the  plain  text  masterkey 
and  salt  used  to  derive  the  encryption  keys  for  the  folder-
specific  data, 
the 
‘masterKey’ property [62]. The master key could be located in 
/%.syncany%/master  file  as  well.  We  also  located  the  URL 
and  server’s  display  name  for  the  repository  in  the  ‘url’  and 
‘username’ tags, respectively. 

the  ‘Key’  and  ‘Salt’  entries  of 

Meanwhile, 

file. 

in 

C.  Cloud Transaction History 

Recalling 

sub-directory. 

Inspections  of  the  directory  listings  revealed  the  sync  folder 
initialisation/addition  time  from  the  creation  time  of  the 
the 
‘.syncany’ 
‘FILEVERSION_FULL’ table of the %.syncany%/db/local.db 
database,  we  observed  that  records  of  the  last  modified  and 
updated  time  could  be  located  for  the  folder-specific  sync 
directories/files  in  the  ‘LASTMODIFIED’  and  ‘UPDATED’ 
table  fields,  respectively.  Sync  directories/files  that  were 
modified  could  be  differentiated  from  the  ‘VERSION’  table 
column given the value of more than 1. Additionally, the view 
tables  also  held  the  directory/file  modification  status  in  the 
sync 
table 
‘STATUS’ 
the 
directories/files  were  newly 
(new),  modified 
(CHANGED),  or  deleted  (deleted).  We  could  also  determine 
machine  names  for  the  clients  that  made  changes  (e.g.,  add 
and remove files or sub-directories) to the repositories as well 
as the database local times associated with the changes, in the 
and 
‘DATABASEVERSION_CLIENT’ 
fields, 
‘DATABASEVERSION_LOCALTIME’ 
respectively 
the 
  2). 
(see  Fig. 
/%syncany%/logs/syncany.log revealed the client application’s 
access times as well as file creation, modification, and deletion 
times  of  the  sync  folders/files,  along  with  the  corresponding 
property  information.  Details  of  the  log  entries  of  forensic 

indicating  whether 
added 

  Examination  of 

field, 

table 

6 

Further examination of the Windows client determined that 
prefetch  files  could  be  differentiated  from  ‘SYNCANY-
LATEST-X86_64-2.EXE-FDCA2B01.pf’.  The  prefetch  files 
provided information about the filename and full path for the 
executable  file,  number  of  times  the  application  has  been 
loaded,  last  run  time,  and  other  associated  timestamps.  We 
also  located  shortcuts  to  the  loader  files  %Program  Files 
(x86)%\Syncany\bin\launcher.vbs 
and  %Program  Files 
(x86)%\Syncany\unins000.exe at %ProgramData%\Microsoft\
Windows\Start Menu\Programs\Syncany\Syncany.lnk and %Pr
ogramData%\Microsoft\Windows\Start Menu\Programs\Sync
any\Uninstall  Syncany.lnk  (respectively),  providing  potential 
for  alternative  methods  for 
the  application 
installation and last run timestamps. 

identifying 

in 

the 

log  entry  “Oct 

the  sync  folders/files 

On  the  Ubuntu  client,  we  observed  that  the  installation 
created the log entry “2015-10-01 18:32:39 install syncany:all 
<none>  0.4.6.alpha”  in  the  dpkg  log.  The  installation  also 
created 
  1  17:45:41  ubuntu 
AptDaemon.Worker:  INFO:  Installing  local  package  file: 
/home/UbuntuPc/Desktop/syncany-latest.deb”  in  the  system 
log  (Syslog).  Additionally,  analysis  of  the  Zeitgeist  database 
(/.local/share/zeistgeist/activity.sqlite)  revealed  the  directory 
paths  for 
the  ‘subj_url’  and 
‘subj_current_uri’  table  fields  of  the  ‘event_view’  table  and 
original  directory  path  in  the  ‘subj_origin_uri’  table  field, 
which are undoubtedly useful in real world investigations. If a 
sync  (download)  file  has  been  moved  from  the  sync  folder, 
then  there  are  records  remaining  in  the  Zeitgeist  database  to 
enable  identification  of  the  sync  files.  We  also  recovered 
records  of  the  types  of  actions  made  to  the  folders/files  of 
relevance  (e.g.,  Access,  Create,  Move,  Leave,  and  Delete; 
each action creates a record in the database) in ‘interpretation’ 
table  column,  alongside  the  timestamps  associated  with  the 
/home/<User 
actions.  Analysis 
Profile>/.local/share/recently-used.xbel  log  located  caches  of 
sync  directory paths  accessed by the client application  which 
included  the  last  added,  modified,  and  visited  times  in  the 
‘bookmark’  property.  Fig.  3  shows  that  the  directory  path  as 
well  as  last  added,  modified,  and  added  times  could  be 
differentiated  from  the  ‘href’,  ‘visited’,  ‘modified’,  and 
‘added’  entries  in  the  ‘bookmark’  property.  The  figure  also 
illustrates  that  a  practitioner  could  identify  the  number  of 
times the client application has accessed a sync folder and the 
application  name  from  the  ‘count’  and  ‘name’  entries  of  the 
‘bookmark:applications’ property. 

the  Gnome’s 

of 

interest are in Table III. 

Fig.  2. Portion of the ‘FILEVERSION_FULL’ table recovered in our research. 

 
 
 
Relevance 
practitioner 

a 

in 

Assists 
the 
identification  of  the  server’s  display 
name. 
Assists  a  practitioner  in  identifying 
the  repository  initiation  time  for  a 
sync  folder,  alongside  the  directory 
path and repository URL. 

Assists  a  practitioner  in  identifying 
the  time  when  a  sync  folder  is 
connected  to  an  existing  repository, 
including the directory path. 
Assists  a  practitioner  in  identifying 
the sync file addition time, including 
the  property  information  such  as the 
filename, file version, as well as last 
modified and updated times. 
Assists  a  practitioner  in  identifying 
the 
time, 
including  the  property  information 
such  as  the  directory  name,  folder 
version, as well as last modified and 
updated times 
Enables a practitioner in determining 
the deletion time of a sync folder. 
By  searching  for  the  ‘file’  tag,  a 
the 
practitioner 
filenames  associated  with  a  sync 
folder.  

folder  addition 

identify 

sync 

can 

TABLE III 
ENTRIES OF PARTICULAR INTEREST IN  SYNCANY.LOG 

Examples of log entries 

7 

 

 

 

 

 

 

 

 

1-10-15 18:52:20.274  |  PluginSettingsP  |  main                   |  INFO  :  Setting  field  'username'  with 
value 'syncanyserver' 

1-10-15 18:52:27.196 | ManagementReque | main           | SEVE : Executing InitOperation for 
folder /home/suspectpc/SyncanyUbuntuClient ... 
1-10-15  18:52:34.807  |  WebdavTransferM  |  IntRq/SyncanyU  |  INFO  :  WebDAV:  Uploading 
local 
to 
http://172.16.38.180/webdav/UbuntuRepo/master ...  
1-10-15 19:32:38.646 | ManagementReque | main           | SEVE : Executing ConnectOperation 
for folder /home/suspectpc/SyncanyWindowsDownloadToUbuntu . 

/home/suspectpc/SyncanyUbuntuClient/.syncany/master 

file 

13 

Dec 

1-10-15  19:26:14.015  |  Indexer                  |  Thread-68            |  INFO  :        *  Added  file  version:    
size=30967, 
[version=1,  path=Enron3111.zip, 
FileVersion 
lastModified=Sat 
linkTarget=null, 
08:35:00 
checksum=75a666ba87fef0f8425a71edcd621d0a4367aa47, updated=Thu Oct 01 19:26:14 PDT 
2015, posixPermissions=rw-r--r--, dosAttributes=--a-] 
1-10-15 19:42:18.751 | FileSystemActio | NotifyThread   | INFO :     with winning version   : 
FileVersion  [version=1,  path=WindowsToUbuntu,  type=FOLDER,  status=NEW,  size=4096, 
lastModified=Mon Sep 28 21:40:44 PDT 2015, linkTarget=null, checksum=null, updated=Thu 
Oct 01 19:42:10 PDT 2015, posixPermissions=rwxr-xr-x, dosAttributes=----] 

status=NEW, 
2014, 

type=FILE, 
PST 

1-10-15  20:14:17.091  |  AppIndicatorTra  |  PySTDIN                |  INFO  :  Python  Input  Stream: 
Removing folder '/home/UbuntuPc/SyncanyUbuntuClient' ... 
1-10-15  19:26:17.032  |  AppIndicatorTra  |  Timer-0                |  INFO  :  Sending  message: 
<updateRecentChangesGuiInternalEvent> 

   <recentChanges> 
<file>/home/suspectpc/SyncanyUbuntuClient/Enron3111.zip</file> 
<file>/home/suspectpc/SyncanyUbuntuClient/Enron3111.txt</file> 
<file>/home/suspectpc/SyncanyUbuntuClient/Enron3111.rtf</file> 
… 
   </recentChanges> 
</updateRecentChangesGuiInternalEvent> 

D.  Data Storage 

The  sync  directories  could  be  discerned  from  directories 
containing  the  hidden  ‘.syncany’  sub-directory.  Analysis  of 
the thumbnail cache located thumbnail images for the synced 
files  in  %AppData%\Local\Microsoft\Windows\Explorer\  and 
/home/<User  Profile>/.cache/thumbnails/large/ 
the 
Windows  and  Ubuntu  clients  (respectively).  The  thumbnail 
cache  could  be  differentiated  from  the  thumbnail  header 
property ‘jtEXtThumb::URI file:’ that referenced the directory 
path for the sync folder in the latter. The files deleted (locally 
and  remotely)  could  also  be  recovered  from  the  unallocated 

of 

space/partition. 

E.  Network Analysis 

Examination  of  the  network  traffic  determined  that  only  the 
server’s IP address could be recovered. A closer inspection of 
the  network  packets  revealed  copies  of  the  response  body  of 
the  WebDAV  protocol’s  PROPFIND  HTTP  method  (i.e., 
“PROPFIND  /webdav/<Repo  Name>/actions/  HTTP/1.1”) 
which  contained  the  URLs  as  well  as  last  creation  and 
modified  times  of  the  repositories  (see  Fig.    4).  It  was  also 
possible  to  recover  the  master  key  salt  from  the  PROPFIND 
/webdav/<Repo  Name>/master 
HTTP  methods 
/webdav/<Repo  Name>/master 
and 
HTTP/1.1” 

“PUT 
“GET 

Fig. 3. Portion of the recently-used.xbel log for the Syncany Ubuntu client application. 

 
 
 
 
8 

Fig.  4. An excerpt of the response body of the PROFIND HTTP method for the Syncany repository. 

the 

repository 

initialisation  or  connection, 

HTTP/1.1”.  Since  the  master  key  salt  is  only  created  once 
during 
the 
timestamp  of  which  could  reflect  the  sync  folder  creation  or 
addition time. Undertaking data carving of the network traffic 
recovered 
is 
noteworthy  that  the  repository  is  encrypted  by  default,  and 
the  client 
hence  accessing 
application and encryption password.  

the  repository  may  require 

the  full  repositories 

intact.  However, 

it 

F.   Memory Analysis 

Examinations  of  the  running  processes  using  the  ‘pslist’ 
function  of  Volatility  indicated  the  process  names,  process 
identifiers (PID), parent process identifiers (PPID), as well as 
process initiation and termination times. Although the process 
name was masqueraded with ‘java.exe’, we could differentiate 
the  PID  from  the  last  used  PID  recorded  in  the  daemon.pid 
and  syncany.log  files.  Examinations  of  the  memory  dumps 
using the ‘netscan’ or ‘netstat’ function of Volatility recovered 
the network information associated with the processes such as 
the host and server’s IP addresses, port numbers, socket states, 
and protocols, providing an alternative method for recovery of 
the network information. 

Undertaking  data  carving  of  the  memory  space  of  the 
Syncany  daemon  process,  we  recovered  the  files  of  forensic 
interest such as config.xml, syncany.log and local.db database 
intact.  The  presence  of  the  data  remnants  in  plain  text  also 
suggested that it is possible to texts from the files based on the 
log file entries and tag names for the  configuration files. The 
%.syncany%/config.xml  file  could  also  be  potentially  carved 
from  unstructured  datasets,  using  the  header  and  footer 
information of “3C 63 6F 6E 66 69 67 3E 0A…3C 2F 70 61 
73 73 77 6F 72 64 3E 0A 20 20 20 3C 2F 63 6F 6E 6E 65 63 
74  69  6F  6E  3E  0A  3C  2F  63  6F  6E  66  69  67  3E”  in  hex 
format  or  “<config>….  </connection>…</config>”  in  non-
unicode  string  format,  but  the  findings  may  be  subject  to 
software 
the 
for 
‘FILEVERSION_FULL’  table  of  the  local.db  database,  a 
search 
the 
/%.syncany%/config.xml  file)  could  enable  future  searches  of 
the raw cell data in unstructured datasets. 

the  machine  name 

updates.  As 

(identified 

records 

from 

from 

the 

for 

VI.  SYNCANY ANALYSIS ON THE UBUNTU SERVER 

Examinations  of  the  Syncany  servers  determined  that  the 
repositories  were  fully  encrypted.  Other  than  the  master  file 

in  possession  of 

(storing the master key salt for the sync data), it appeared that 
accessing  the  sync  data  is  only  possible  with  the  Syncany 
the  repository  password.  The 
client 
repositories  did  not  necessitate  the  need  for  the  server 
environment,  and  hence  it  was  possible  to  rebuild  the  sync 
folders externally (e.g., in our forensic workstation). This can 
be done by running the ‘sy connect’ command or choosing the 
‘Connect  to  an  existing  online  storage’  option  on  the  GUI  – 
similarly to the process of connecting to a repository from the 
desktop clients.  

An inspection of the Apache server log determined that the 
log  entries  for  the  HTTP  requests  made  by  the  clients  to  the 
repositories  were  in  the  format  “<Client’s  IP  address>  - 
<Server’s  name>  [The  time  when  the  request  was  received] 
"<Request  line>"  <Status  code>  <Size  of  the  repository 
object  returned  to  the  client>  "  –"  "<User  agent>"”.  For 
example,  we  were  able 
repository 
initialisation  time  from  the  log  entry  “172.16.38.132  - 
"PUT 
syncanyserver 
"-" 
/webdav/UbuntuRepo/master  HTTP/1.1" 
"Sardine/UNAVAILABLE"”. 

-0700] 
201  480 

[01/Oct/2015:18:52:34 

to  determine 

the 

VII.  REPORTING AND PRESENTATION 

A timeline of the data from the various sources was outlined to 
demonstrate  the  cloud  transactions  from  the  clients  and  the 
servers (see Table  IV). The complete timeline analysis is too 
broad to present fully in this paper; therefore, we only outline 
the  timeline  for  the  Enron3111.txt  sample  file  from  the 
Ubuntu client. 

VIII.  CONCLUDING REMARKS 

In  this  paper,  we  suggested  an  investigation  approach  for 
Syncany, a cloud-enabled big data platform, with the aims of 
reducing  the  required  time  and  resources  required  in  a  real 
investigation.  Specifically,  we  described  the  data  remnants 
recovered  from  the  use  of  Syncany  private  cloud  storage 
service as a backbone for big data storage, such as config.xml, 
local.db,  and  syncany.log  files  on  client  devices.  These  files 
could  provide  the  sync,  file  management,  authentication,  and 
encryption  metadata  essential  for  identifying  the  synced  files 
and  cloud  hosting  instance  as  well  as  correlating  the  cloud 
transaction 
for 
timelining  user  activities.  The  creation  of  the  sync-folder-
specific  ‘.syncany’  sub-directory suggested that a  practitioner 

file  versioning  datasets) 

records 

(e.g., 

 
 
9 

Source 

Key Date 

Autopsy file List 

Autopsy file list 

Dpkg log 

syncany.log 

syncany.log 

Autopsy file list 

local.db 

syncany.log 

2015-10-01 
01:14:41 
2015-10-01 
10:26:10 
2015-10-01 
18:32:39 
2015-10-01 
18:34:12 
2015-10-01 
18:52:27 

2015-10-01 
18:52:28 
2015-10-01 
19:26:14 
2015-10-01 
19:26:14 

local.db 

local.db 

Autopsy file list 

syncany.log 

2015-10-01 
19:42:10 
2015-10-01 
19:42:10 
2015-10-01 
19:42:18 
2015-10-01 
19:42:18 

TABLE  IV 
TIMELINE FOR SYNCANY FORENSICS 

Event 
Type 
Created 

Comment 

Created /home/UbuntuPc/SyncanyUbuntuClient. 

Created 

Created /home/UbuntuPc/SyncanyUbuntuClient/Enron3111.txt. 

Created 

Last run 

Initialised 

Installed Syncany Ubuntu client application version 0.4.6.alpha - 
“2015-10-01 18:32:39 install syncany:all <none> 0.4.6.alpha” 

1-10-15 18:34:12.064 | DaemonOperation | Thread-5       | INFO : Starting 
daemon operation with action RUN ... 
Initialised repo for /home/suspectpc/SyncanyUbuntuClient - 

 “1-10-15 18:52:27.196 | ManagementReque | main           | SEVE : Executing 
InitOperation for folder /home/suspectpc/SyncanyUbuntuClient…” 

Initialised  Created repo /home/UbuntuPc/SyncanyUbuntuClient/.syncany. 

Created 

Added 

Node ‘UbuntuPc” (Machine Name=’UYCrwWXGXKvbOYKtZBGc’) added 
/home/UbuntuPc/SyncanyUbuntuClient/Enron3111.txt. 
Added /home/UbuntuPc/SyncanyUbuntuClient/Enron3111.txt - 

“1-10-15 19:26:14.004 | Indexer         | Thread-68      | INFO :    * Added file 
version:    FileVersion [version=1, path=Enron3111.txt, type=FILE, 
status=NEW, size=2734, lastModified=Sat Dec 13 08:33:11 PST 2014, 
linkTarget=null, checksum=ceadc4b7d47af4125a68e03ec3141cb3fde407ff, 
updated=Thu Oct 01 19:26:14 PDT 2015, posixPermissions=rw-r--r--, 
dosAttributes=--a-]” 

Created 

Created 

Created 

Node ‘WindowsPc’ (Machine Name=‘PqPKcLWznojHZVLslgNo’) added 
/home/UbuntuPc/SyncanyUbuntuClient/WindowsToUbuntu. 
Node ‘WindowsPc’ (Machine Name=‘PqPKcLWznojHZVLslgNo’) added 
/home/UbuntuPc/SyncanyUbuntuClient/WindowsToUbuntu/Enron3111.txt. 
Created /home/UbuntuPc/SyncanyUbuntuClient/WindowsToUbuntu. 

Added 

Added /home/UbuntuPc/SyncanyUbuntuClient/WindowsToUbuntu - 

“1-10-15 19:42:18.751 | FileSystemActio | NotifyThread   | INFO :     with 
winning version   : FileVersion [version=1, path=WindowsToUbuntu, 
type=FOLDER, status=NEW, size=4096, lastModified=Mon Sep 28 21:40:44 
PDT 2015, linkTarget=null, checksum=null, updated=Thu Oct 01 19:42:10 
PDT 2015, posixPermissions=rwxr-xr-x, dosAttributes=----]” 

syncany.log 

2015-10-01 
19:42:18 

Added 

Added 
/home/UbuntuPc/SyncanyUbuntuClient/WindowsToUbuntu/Enron3111.txt - 

“1-10-15 19:42:18.848 | FileSystemActio | NotifyThread   | INFO :    + 
NewFileSystemAction [file1=null, file2=FileVersion [version=1, 
path=WindowsToUbuntu/Enron3111.txt, type=FILE, status=NEW, 
size=2734, lastModified=Sat Dec 13 08:33:11 PST 2014, linkTarget=null, 
checksum=ceadc4b7d47af4125a68e03ec3141cb3fde407ff, updated=Thu 
Oct 01 19:42:10 PDT 2015, posixPermissions=rw-r--r--, dosAttributes=--a-]]” 

Autopsy file List 

Autopsy file list; 
Trashinfo 
local.db 

2015-10-01 
19:42:18 
2015-10-01 
19:48:29 
2015-10-01 
19:48:32 

Created 

Deleted 

Deleted 

Created 
/home/UbuntuPc/SyncanyUbuntuClient/WindowsToUbuntu/Enron3111.txt. 
Deleted 
/home/UbuntuPc/SyncanyUbuntuClient/WindowsToUbuntu/Enron3111.txt. 
Node ‘UbuntuPc” (Machine Name=’UYCrwWXGXKvbOYKtZBGc’) deleted 
/home/UbuntuPc/SyncanyUbuntuClient/WindowsToUbuntu/Enron3111.txt. 

can identify the sync directory and the associated timestamps 
from  the  directory  listing,  as  well  as  other  OS-generated 
instances  such  as  shortcuts,  event  logs,  $LogFile,  $MFT, 
$UsnJrnl,  registry  (‘RecentDocs’,  ‘UserAssist’,  ‘Run’,  and 
‘ComDig32’  etc.),  Zeitgeist  and  recently-used.xbel  logs,  and 
thumbnail cache. 

Our examination of the physical memory indicated that the 
memory dumps can provide a potential alternative method for 
recovery of the applications cache, logs, and HTTP requests in 
plain text. However, the encryption password was not located 
(on both the hard disk and memory dumps), suggesting that a 
practitioner  can  only  obtain  the  encryption  password  via  an 
offline  brute-force  attack  or  directly 
the  user. 
Nevertheless,  a  practitioner  must  keep  in  mind  that  memory 
changes  frequently  according  to  user  activities  and  will  be 

from 

wiped as soon as the system is shut down. Hence, obtaining a 
memory  snapshot  of  a  compromised  system  as  quickly  as 
possible  increases  the  likelihood  of  obtaining  the  encryption 
key. 

Analysis of the network traffic produced unique identifiable 
information  such  as  URL  references  to  the  repositories  and 
HTTP  requests  for  the  cloud  transactions.  We  observed  that 
the sync data were fully encrypted. This should perhaps not be 
surprising  as  Syncany  fully  encrypts  the  sync  data  before 
uploading to the offsite storage [62]. However, it is important 
to  note  that  the  artefacts  may  vary  according  to  different 
network protocol implementations e.g., if the HTTPS protocol 
is used, then the HTTP requests will be fully encrypted.  

As  expected,  examinations  of 

the  Syncany  servers 
determined  that  the  offsite  repositories  were  fully  encrypted, 

 
 
 
10 

Artefact Category 
Sync and file management 
metadata 
Authentication and encryption 
metadata 
Cloud transaction history 

Data storage 

Network analysis 

Memory analysis 

TABLE V 
SUMMARY OF FINDINGS FROM THE SYNCANY PRIVATE CLOUD STORAGE SERVICE 

Sources of Information 

 
 
 
 
 
 

 
 
 
 
 
 
 
 

Sync-folder-specific %.syncany%/config.xml and %.syncany%/db/local.db 
User-specific %syncany%/logs/syncany.log, %syncany%/daemon.xml, and %Syncany%/daemon.pid 
Sync-folder-specific /%.syncany%/config.xml file 
User-specific /%syncany%/config.xml file 
Folder-specific %.syncany%/db/local.db database 
SYNCANY-LATEST-X86_64-2.EXE-FDCA2B01.pf, Syncany.lnk, and Uninstall Syncany.lnk on the 
Windows client 
The Zeitgeist, recently-used.xbel, and system and dpkg logs of the Ubuntu client 
Directories containing the ‘.syncany’ sub-directory 
Copies of the thumbnail images for synced files in the thumbnail cache 
IP addresses of the client and server 
The timestamp information associated with the cloud transactions 
Copies of the clear text PROPFIND HTTP method 
Copies of the encrypted repositories 
Copies of the %.syncany%/db/local.db, %syncany%/logs/syncany.log, and %syncany%/daemon.xml in 
plain text 

and  it  appeared  that  accessing  the  sync  data  is  only  possible 
with the client application and the encryption password. In the 
case when physical acquisition is not feasible for the server, a 
practitioner  can  facilitate  the  data  collection  with  logical 
acquisition of the data repositories. While the machine names 
could  be  located  for  the  nodes  associated  with  a  Syncany 
repository, only the  web application log  could provide  the IP 
addresses of the nodes. A summary of our findings is outlined 
in Table V. 

Collectively,  our  research  highlighted  the  challenges  that 
could arise during forensic investigations of cloud-enabled big 
data  solutions.  While  the  use  of  deduplication/chunking  and 
encryption  technologies  can  benefit  users  by  providing  an 
efficient  and  secure  means  for  managing  big  data,  evidence 
collection  and  analysis  may  necessitate  intercepting  and 
collecting  password  and  utilisation  of  other  vendor-specific 
applications.  This  can  be  subject  to  potential  abuse  by  cyber 
criminals seeking to hide their tracks. Without the cooperation 
of  the  user  (suspect),  forensics  endeavours  may  end  up  an 
exercise  in  futility.  Therefore,  we  suggest  the  vendor  to 
implement  a  forensically  friendly  logging  mechanism  (e.g., 
providing information about who accesses the data, what data 
has  been  accessed,  from  where  did  the  user  access  the  data, 
and  when  did  the  user  access  the  data)  that  supports  the 
collection of the raw log data outside the encrypted datasets by 
default. In a recent work, for example, Ab Rahman et al. [59] 
highlighted 
forensic-by-design  and 
importance  of 
presented a conceptual forensic-by-design framework. 

the 

Future  work  would  include  extending  this  study  to  other 
private cloud storage services such as Seafile to have an up-to-
date  and  comprehensive  forensic  understanding  and  facilitate 
in greening investigation of big data platforms. 

REFERENCES 

[1] 

[2] 

International  Data  Corporation  (IDC),  “Worldwide  and  Regional 
Public  IT  Cloud  Services  2013–2017  Forecast,”  2014.  [Online]. 
http://www.idc.com/getdoc.jsp?containerId=242464. 
Available: 
[Accessed: 20-Nov-2015]. 
Gartner,  “Forecast:  IT  Services,  2011-2017,  4Q13  Update,”  2013. 
[Online].  Available:  https://www.gartner.com/doc/2637515/forecast-
it-services--q. [Accessed: 26-May-2016]. 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

K.  K.  R.  Choo  and  R.  Sarre,  “Balancing  Privacy  with  Legitimate 
Surveillance and Lawful Data Access,”  IEEE Cloud Comput., vol. 2, 
no. 4, pp. 8–13, Jul. 2015. 
S. Nepal, R. Ranjan, and K. K. R. Choo, “Trustworthy Processing of 
Healthcare Big Data in Hybrid Clouds,” IEEE Cloud Comput., vol. 2, 
no. 2, pp. 78–84, Mar. 2015. 
A.  Dehghantanha  and  K.  Franke,  “Privacy-respecting  digital 
investigation,”  in  Proceedings  of  2014  Twelfth  Annual  International 
Conference on Privacy, Security and Trust (PST), 2014, pp. 129–138. 
K.-K. R. Choo, “Cloud computing: Challenges and future directions,” 
Australian Institute of Criminology, report, 2010. 
K.-K. R. Choo, “Organised crime groups in cyberspace: a typology,” 
Trends Organ. Crime, vol. 11, no. 3, pp. 270–295, Jul. 2008. 
N.  H.  Ab  Rahman  and  K.-K.  R.  Choo,  “A  survey  of  information 
security incident handling in the cloud,” Comput. Secur., vol. 49, no. 
C, pp. 45–69, Mar. 2015. 
L.  Li,  R.  Lu,  K.  K.  R.  Choo,  A.  Datta,  and  J.  Shao,  “Privacy-
Preserving-Outsourced  Association  Rule  Mining  on  Vertically 
Partitioned Databases,” IEEE Trans. Inf. Forensics Secur., vol. 11, no. 
8, pp. 1847–1861, Aug. 2016. 

[10]  R. Jiang, R. Lu, and K.-K. R. Choo, “Achieving high performance and 
privacy-preserving  query  over  encrypted  multidimensional  big 
metering data,” Future Gener. Comput. Syst. 

[11]  X. Liu, R. Choo, R. Deng, R. Lu, and J. Weng, “Efficient and Privacy-
Preserving  Outsourced  Calculation  of  Rational  Numbers,”  IEEE 
Trans. Dependable Secure Comput., vol. PP, no. 99, pp. 1–1, 2016. 

[12]  X.  Liu,  R.  H.  Deng,  K.-K.  R.  Choo,  and  J.  Weng,  “An  Efficient 
Privacy-Preserving  Outsourced  Calculation  Toolkits  with  Multiple 
Keys,” IEEE Trans. Inf. Forensics Secur. Press, 2016. 

[13]  Y. Hu, J. Yan, and K.-K. R. Choo, “PEDAL: a dynamic analysis tool 
for  efficient  concurrency  bug  reproduction  in big data  environment,” 
Clust. Comput., vol. 19, no. 1, pp. 153–166, Jan. 2016. 

[14]  L. Zhao, L. Chen, R. Ranjan, K.-K. R. Choo, and J. He, “Geographical 
information  system  parallelization  for  spatial  big  data  processing:  a 
review,” Clust. Comput., vol. 19, no. 1, pp. 139–152, Nov. 2015. 
[15]  Y.  Yang,  H.  Zhu,  H.  Lu,  J.  Weng,  Y.  Zhang,  and  K.-K.  R.  Choo, 
“Cloud  based  data  sharing  with  fine-grained  proxy  re-encryption,” 
Pervasive Mob. Comput., vol. 28, pp. 122–134, Jun. 2016. 

[16]  B. G. Soltani Ashkan and A. Peterson, “How we know the NSA had 
access  to  internal  Google  and  Yahoo  cloud  data,”  The  Washington 
Post, 04-Nov-2013. 

[17]  M.  Damshenas,  A.  Dehghantanha,  R.  Mahmoud,  and  S.  bin 
Shamsuddin,  “Forensics  investigation  challenges  in  cloud  computing 
environments,”  in  Proceedings  of  2012  International  Conference  on 
Cyber  Security,  Cyber  Warfare  and  Digital  Forensic  (CyberSec), 
2012, pp. 190–194. 
J.  M.  Cauthen,  “Executing  Search  Warrants  in  the  Cloud,”  FBI,  10-
Jul-2014. 
Available: 
[Online]. 
https://leb.fbi.gov/2014/october/executing-search-warrants-in-the-
cloud. [Accessed: 26-May-2016]. 

[18] 

 
 
 
[19]  D.  Quick  and  K.-K.  R.  Choo,  “Big  forensic  data  reduction:  digital 
forensic images and electronic evidence,” Clust. Comput., vol. 19, no. 
2, pp. 1–18, Mar. 2016. 

[20]  D. Quick and K.-K. R. Choo, “Impacts of increasing volume of digital 
forensic  data:  A  survey  and  future  research  challenges,”  Digit. 
Investig., vol. 11, no. 4, pp. 273–294, Dec. 2014. 

[21]  D.  Quick  and  K.-K.  R.  Choo,  “Data  reduction  and  data  mining 
framework for digital forensic evidence: Storage, intelligence, review, 
and archive.,” Trends Issues Crime Crim. Justice, vol. 480, pp. 1–11, 
Sep. 2014. 
F.  Daryabar  and  A.  Dehghantanha,  “A  review  on  impacts  of  cloud 
computing and digital forensics,” Int. J. Cyber-Secur. Digit. Forensics 
IJCSDF, vol. 3, no. 4, pp. 183–199, 2014. 

[22] 

[23]  C. Hooper, B. Martini, and K.-K. R. Choo, “Cloud computing and its 
implications for cybercrime investigations in Australia,” Comput. Law 
Secur. Rev., vol. 29, no. 2, pp. 152–163, Apr. 2013. 

[24]  National Institute of Standards and Technology (NIST), “NIST Cloud 
Computing  Forensic  Science  Challenges.”  National  Institute  of 
Standards and Technology, 2014. 

[25]  D. Quick, B. Martini, and R. Choo, Cloud Storage Forensics, 1st ed. 

Syngress, 2013. 

[26]  B. Martini and K.-K. R. Choo, “Cloud Forensic Technical Challenges 
and  Solutions:  A  Snapshot,”  IEEE  Cloud Comput.,  vol.  1, no.  4,  pp. 
20–25, Nov. 2014. 

[27]  Y.  TzeTzuen,  A.  Dehghantanha,  A.  Seddon,  and  S.  H.  Mohtasebi, 
“Greening Digital Forensics: Opportunities and Challenges,” in Signal 
Processing and Information Technology, 2011, pp. 114–119. 
[28]  Y.-Y.  Teing,  A.  Dehghantanha,  K.-K.  R.  Choo,  Z.  Muda,  M.  T. 
Abdullah, and W.-C. Chai, “A Closer Look at Syncany Windows and 
Ubuntu  Clients’  Residual  Artefacts,”  in  Security,  Privacy  and 
Anonymity in Computation, Communication and Storage, G. Wang, I. 
Ray, J. M. A. Calero, and S. M. Thampi, Eds. Springer  International 
Publishing, 2016, pp. 342–357. 

[29]  R. Marty, “Cloud Application Logging for Forensics,” in Proceedings 
of the 2011 ACM Symposium on Applied Computing, New York, NY, 
USA, 2011, pp. 178–184. 

[31] 

[30]  C.  Shields,  O.  Frieder,  and  M.  Maloof,  “A  system  for  the  proactive, 
continuous,  and  efficient  collection  of  digital  forensic  evidence,” 
Digit. Investig., vol. 8, Supplement, pp. S3–S13, Aug. 2011. 
J.  Dykstra  and  A.  T.  Sherman,  “Design  and  implementation  of 
FROST:  Digital  forensic  tools  for  the  OpenStack  cloud  computing 
platform,” Digit. Investig., vol. 10, pp. S87–S95, Aug. 2013. 
[32]  T.  Gebhardt  and  H.  P.  Reiser,  “Network  Forensics  for  Cloud 
Computing,”  in  Distributed  Applications  and  Interoperable  Systems, 
J. Dowling and F. Taïani, Eds. Springer Berlin Heidelberg, 2013, pp. 
29–42. 

[33]  B.  Martini  and  K.-K.  R.  Choo,  “Remote  Programmatic  vCloud 
Forensics: A Six-Step Collection Process and a Proof of Concept,” in 
Proceedings of 13th IEEE International Conference on Trust, Security 
and  Privacy  in  Computing  and  Communications  (TrustCom  2014), 
2014, pp. 935–942. 

[34]  D.  Quick  and  K.-K.  R.  Choo,  “Forensic  collection  of  cloud  storage 
data:  Does  the  act  of  collection  result  in  changes  to  the  data  or  its 
metadata?,” Digit. Investig., vol. 10, no. 3, pp. 266–277, Oct. 2013. 

[35]  Y.-Y.  Teing,  D.  Ali,  K.-K.  R.  Choo,  M.  Conti,  and  T.  Dargahi, 
“Forensic  Investigation  of  Cooperative  Storage  Cloud  Service: 
Symform as a Case Study,” J. Forensics Sci., vol. [In Press], pp. 1–14, 
2016. 

[36]  H. Chung, J. Park, S. Lee, and C. Kang, “Digital forensic investigation 
of  cloud  storage  services,”  Digit.  Investig.,  vol.  9,  no.  2,  pp.  81–95, 
Nov. 2012. 

[37]  D.  Quick  and  K.-K.  R.  Choo,  “Dropbox  analysis:  Data  remnants  on 

user machines,” Digit. Investig., vol. 10, no. 1, pp. 3–18, Jun. 2013. 

[38]  D.  Quick  and  K.-K.  R.  Choo,  “Google  Drive:  Forensic  Analysis  of 
Data  Remnants,”  J  Netw  Comput  Appl,  vol.  40,  pp.  179–193,  Apr. 
2014. 

[40] 

[39]  D.  Quick  and  K.-K.  R.  Choo,  “Digital  droplets:  Microsoft  SkyDrive 
forensic data remnants,” Future Gener. Comput. Syst., vol. 29, no. 6, 
pp. 1378–1394, Aug. 2013. 
J. S. Hale, “Amazon Cloud Drive forensic analysis,”  Digit. Investig., 
vol. 10, no. 3, pp. 259–265, Oct. 2013. 
J.  Farina,  M.  Scanlon,  and  M.-T.  Kechadi,  “BitTorrent  Sync:  First 
Impressions  and  Digital  Forensic  Implications,”  Digit.  Investig.,  vol. 
11, Supplement 1, pp. S77–S86, May 2014. 

[41] 

11 

[42]  M.  Shariati,  A.  Dehghantanha,  and  K.-K.  R.  Choo,  “SugarSync 
forensic analysis,” Aust. J. Forensic Sci., vol. 48, no. 1, pp. 1–23, Apr. 
2015. 

[43]  M.  Shariati,  A.  Dehghantanha,  B.  Martini,  and  K.-K.  R.  Choo, 
“Chapter  19  -  Ubuntu  One  investigation:  Detecting  evidences  on 
client machines,” in The Cloud Security Ecosystem, Boston: Syngress, 
2015, pp. 429–446. 

[44]  B.  Blakeley,  C.  Cooney,  A.  Dehghantanha,  and  R.  Aspin,  “Cloud 
Storage  Forensic:  hubiC  as  a  Case-Study,”  in  2015  IEEE  7th 
International  Conference  on  Cloud  Computing  Technology  and 
Science (CloudCom), 2015, pp. 536–541. 

[45]  B. Martini, Q. Do, and K.-K. R. Choo, “Mobile Cloud Forensics: An 
Analysis  of  Seven  Popular  Android  Apps,”  ArXiv150605533  Cs, pp. 
309–345, 2015. 
F. Daryabar, A. Dehghantanha, B. Eterovic-Soric, and K.-K. R. Choo, 
“Forensic investigation of OneDrive, Box, GoogleDrive and Dropbox 
applications on Android and iOS devices,” Aust. J. Forensic Sci., vol. 
48, no. 6, pp. 1–28, Mar. 2016. 

[46] 

[47]  B.  Martini  and  K.-K.  R.  Choo,  “An  integrated  conceptual  digital 
forensic framework for cloud computing,” Digit. Investig., vol. 9, no. 
2, pp. 71–80, Nov. 2012. 

[48]  R.  McKemmish,  What  is  forensic  computing?  Canberra:  Australian 

Institute of Criminology, 1999. 

[49]  K. Kent, S. Chevalier, and T. Grance, “Guide to Integrating Forensic 

Techniques into Incident.” 2006. 

[50]  B. Martini and K.-K. R. Choo, “Cloud storage forensics: ownCloud as 
a case study,” Digit. Investig., vol. 10, no. 4, pp. 287–299, Dec. 2013. 
[51]  N.  Thethi  and  A.  Keane,  “Digital  forensics  investigations  in  the 
in  Proceedings  of  2014  IEEE  International  Advance 

Cloud,” 
Computing Conference (IACC), 2014, pp. 1475–1480. 

[52]  B.  Martini  and  K.-K.  R.  Choo,  “Distributed  filesystem  forensics: 
XtreemFS  as  a  case  study,”  Digit.  Investig.,  vol.  11, no.  4, pp.  295–
313, Dec. 2014. 

[53]  M. Scanlon, J. Farina, and M.-T. Kechadi, “BitTorrent Sync: Network 

Investigation Methodology,” Sep. 2014. 

[54]  M.  Scanlon,  J.  Farina,  N.  A.  L.  Khac,  and  T.  Kechadi,  “Leveraging 
Decentralization to Extend the Digital Evidence Acquisition Window: 
Case  Study  on  BitTorrent  Sync,”  ArXiv14098486 Cs, pp.  1–14,  Sep. 
2014. 
J.  Farina,  M.  Scanlon,  and  M.-T.  Kechadi,  “BitTorrent  Sync:  First 
Impressions  and  Digital  Forensic  Implications,”  Digit.  Investig.,  vol. 
11, Supplement 1, pp. S77–S86, May 2014. 

[55] 

[56]  Y.-Y.  Teing,  A.  Dehghantanha,  K.-K.  R.  Choo,  and  L.  T.  Yang, 
“Forensic  investigation  of  P2P  cloud  storage  services  and  backbone 
for  IoT  networks:  BitTorrent  Sync  as  a  case  study,”  Comput.  Electr. 
Eng., vol. 22, no. 6, pp. 1–14, 2016. 

[57]  Q.  Do,  B.  Martini,  and  K.-K.  R.  Choo,  “A  Forensically  Sound 
Adversary Model for Mobile Devices,” PLoS ONE, vol. 10, no. 9, p. 
e0138449, Sep. 2015. 

[58]  Q. Do, B. Martini, and K.-K. R. Choo, “Is the data on your wearable 
device  secure?  An  Android  Wear  smartwatch  case  study,”  Softw. 
Pract. Exp., p. n/a-n/a, Jan. 2016. 

[60] 

[59]  N.  H.  Ab  Rahman,  N.  D.  W.  Cahyani,  and  K.-K.  R.  Choo,  “Cloud 
incident  handling  and  forensic-by-design:  cloud  storage  as  a  case 
study,” Concurr. Comput. Pract. Exp., Jan. 2016. 
P.  C.  Heckel,  “Syncany  explained:  idea,  progress,  development  and 
future  (part  1),”  Philipp’s  Tech  Blog,  18-Oct-2013.  [Online]. 
https://blog.heckel.xyz/2013/10/18/syncany-explained-
Available: 
idea-progress-development-future/. [Accessed: 26-May-2016]. 
P. C. Heckel, “Deep into the code of Syncany - command line client, 
application  flow  and  data  model  (part  2),”  Philipp’s  Tech  Blog,  14-
Feb-2013. 
Available: 
[Online]. 
https://blog.heckel.xyz/2014/02/14/deep-into-the-code-of-syncany-cli-
application-flow-and-data-model/. [Accessed: 26-May-2016]. 
Syncany, 
https://syncany.readthedocs.io/en/latest/. [Accessed: 26-May-2016]. 

“Syncany  User  Guide.” 

[Online].  Available: 

[62] 

[61] 

[63]  Y.-Y.  Teing,  D.  Ali,  K.-K.  R.  Choo,  and  M.  Yang,  “Forensic 
investigation  of  P2P  cloud  storage  services  and  backbone  for  IoT 
networks: BitTorrent Sync as a case study,” Comput. Electr. Eng., no. 
2016, 2016. 
S.  Wilkinson,  “ACPO  Good  Practice  Guide  for  Digital  Evidence.” 
Association of Chief Police Officers (ACPO), 2012. 

[64] 

[65]  T.  Y.  Yang,  A.  Dehghantanha,  K.-K.  R.  Choo,  and  Z.  Muda, 
“Windows Instant Messaging App Forensics: Facebook and Skype as 
Case Studies,” PLOS ONE, vol. 11, no. 3, p. e0150300, Mar. 2016. 

 
12 

on 

Internet  Technology 
Investigation 

Kim-Kwang  Raymond  Choo  (SM'15)  received  the  Ph.D.  in 
Information  Security  in  2006  from  Queensland  University  of 
the  Cloud 
Technology,  Australia.  He  currently  holds 
Technology  Endowed  Professorship  at  The  University  of 
Texas  at  San  Antonio,  and  is  an  associate  professor  at 
University  of  South  Australia.  He  has  served  as  the  Special 
Issue  Guest  Editor  of  ACM  Transactions  on  Embedded 
Computing  Systems  (2016;  DOI:  10.1145/3015662),  ACM 
(2016;  DOI: 
Transactions 
10.1145/3013520),  Digital 
(2016;  DOI: 
10.1016/j.diin.2016.08.003),  Future  Generation  Computer 
Systems  (2016;  DOI:  10.1016/j.future.2016.04.017),  IEEE 
Cloud  (2015;  DOI:  10.1109/MCC.2015.84),  IEEE  Network 
(2016;  DOI:  10.1109/MNET.2016.7764272)  Journal  of 
DOI: 
Computer 
and 
10.1016/j.jcss.2016.09.001),  Multimedia 
(2017;  DOI:  10.1007/s11042-016-4081-z), 
Applications 
Pervasive 
(2016;  DOI: 
10.1016/j.pmcj.2016.10.003),  etc.  He  is  the  recipient  of 
various  awards  including  ESORICS  2015  Best  Paper  Award, 
Winning  Team  of  the  Germany's  University  of  Erlangen-
Nuremberg  (FAU)  Digital  Forensics  Research  Challenge 
2015,  and  2014  Highly  Commended  Award  by  the  Australia 
New  Zealand  Policing  Advisory  Agency,  Fulbright 
Scholarship  in  2009,  2008  Australia  Day  Achievement 
Medallion,  and  British  Computer  Society's  Wilkes  Award  in 
2008. He is a Fellow of the Australian Computer Society. 

and  Mobile  Computing 

Sciences 

System 

(2017; 

Tools 

and 

[66]  N. D. W. Cahyani, B. Martini, and K.-K. R. Choo, “Effectiveness of 
multimedia  presentations  in  improving  understanding  of  technical 
terminologies and concepts: a pilot study,”  Aust. J. Forensic Sci., pp. 
1–17, Jan. 2016. 

[67]  N.  D.  W.  Cahyani,  B.  Martini,  and  K.-K.  R.  Choo,  “Do  Multimedia 
Presentations Enhance Judiciary’s Technical Understanding of Digital 
Forensic  Concepts?  An  Indonesian  Case  Study,”  in  Proceedings  of 
49th  Annual  Hawaii  International  Conference  on  System  Sciences 
(HICSS 2016), Hawaii, 2016, pp. 5617–5626. 

[68]  N. D. W. Cahyani, B. Martini, K.-K. R. Choo, and A. M. N. Al-Azhar, 
“Forensic  data  acquisition  from  cloud-of-things  devices:  windows 
Smartphones  as  a  case  study,”  Concurr.  Comput.  Pract.  Exp.,  Jan. 
2016. 

[69]  N. D. W. Cahyani, B. Martini, and K. K. R. Choo, “Using Multimedia 
Presentations to  Enhance  the  Judiciary’s  Technical  Understanding  of 
Digital Forensic Concepts: An Indonesian Case Study,” in 2016 49th 
Hawaii International Conference on System Sciences (HICSS), 2016, 
pp. 5617–5626. 

Yee-Yang  Teing  is  a  research  fellow  at 
the  Putra  University  of  Malaysia,  and 
holds  a  Bachelor  of  Computer  Forensics 
(First  Class  Honours).  He  is  a  Certified 
Ethical Hacker (CEH), Computer Hacking 
Forensic 
and 
Certified  Security  Analyst  (ECSA).  His 
research  interests  include  cybercrime  investigations,  malware 
analysis and network security.  

Investigator 

(CHFI) 

 Ali  Dehghantanha 
is  a  Marie-Curie 
International  Incoming  Fellow  in  Cyber 
Forensics  and  a  fellow  of  the  UK  Higher 
Education  Academy  (HEA).  He  has  served 
for  many  years  in  a  variety  of  research  and 
industrial  positions.  Other  than  Ph.D.  in 
Cyber  Security  he  holds  many  professional 
certificates such as GXPN, GREM, CISM, CISSP, and CCFP. 
He  has  served  as  an  expert  witness,  cyber  forensics  analysts 
and  malware  researcher  with  leading  players  in  Cyber-
Security and E-Commerce. 

 
 
 
 
 
  
  
 
 
 
 
    
    
 
 
