Improving Radioactive Material Localization by
Leveraging Cyber-Security Model Optimizations

Ryan Sheatsley

Matthew Durbin

Azaree Lintereur

Patrick McDaniel

2
2
0
2

b
e
F
1
2

]

G
L
.
s
c
[

1
v
7
8
3
0
1
.
2
0
2
2
:
v
i
X
r
a

Abstract

One of the principal uses of physical-space sensors in pub-
lic safety applications is the detection of unsafe conditions
(e.g., release of poisonous gases, weapons in airports, tainted
food). However, current detection methods in these applica-
tions are often costly, slow to use, and can be inaccurate in
complex, changing, or new environments. In this paper, we
explore how machine learning methods used successfully in
cyber domains, such as malware detection, can be leveraged to
substantially enhance physical space detection. We focus on
one important exemplar application–the detection and local-
ization of radioactive materials. We show that the ML-based
approaches can signiﬁcantly exceed traditional table-based
approaches in predicting angular direction. Moreover, the de-
veloped models can be expanded to include approximations of
the distance to radioactive material (a critical dimension that
reference tables used in practice do not capture). With four
and eight detector arrays, we collect counts of gamma-rays
as features for a suite of machine learning models to localize
radioactive material. We explore seven unique scenarios via
simulation frameworks frequently used for radiation detection
and with physical experiments using radioactive material in
laboratory environments. We observe that our approach can
outperform the standard table-based method, reducing the
angular error by 37 % and reliably predicting distance within
2.4 %. In this way, we show that advances in cyber-detection
provide substantial opportunities for enhancing detection in
public safety applications and beyond.

1 Introduction

The integration of computation and sensing has revolution-
ized the management of physical spaces [12]. For exam-
ple, new capabilities enable smart buildings that reduce en-
ergy use and lessen carbon footprints, smart homes which
ease our personal lives, and smart infrastructures which sup-
port semi-autonomously secured spaces. Collectively, these
Cyber-Physical Systems (CPS) are driving massive innova-

tion, and in particular, advancing public safety in many do-
mains. Security—the protection of physical spaces from ad-
versaries who wish to manipulate or harm the space or those
who reside in it—is one of the important areas being advanced.
Speciﬁcally, detection of adversarial entities, actions, or dan-
gerous states is the principal use of physical-space sensors.

One of the most well known detection problems in physical
security is the localization of radioactive materials. The use
of nuclear technology has grown since the discovery of radia-
tion [1] and it is now found in many applications, including
power, medicine, and space. As the use of nuclear technol-
ogy increases, so does the potential for misuse of radioactive
materials. The canonical domain for discussing the detec-
tion of rogue radioactive materials is the container shipping
industry (speciﬁcally, cargo inspection). Shipping contain-
ers are critical infrastructure, yet represent an ideal mode of
transportation for adversaries due to their low cost [20] and
the presence of large amounts of metal (and other shielding
materials) which attenuates radioactive signals, substantially
reducing the efﬁcacy of radiation detectors to extract rele-
vant signals from the surrounding noise [10]. As yet another
example, urban search applications during large public gath-
erings (e.g., the Super Bowl, or Times Square during New
Year’s Eve) face similar challenges; the surrounding build-
ings can cause severe signal attenuation and impede search
effectiveness.

Over the past few decades, a conventional technique for
source localization (known as Directional Gamma-ray De-
tection) has relied on the use of pre-populated datasets (i.e.,
reference tables) calibrated at speciﬁc distances in laboratory
environments [11, 28]. In a process thematically similar to
collecting malware signatures from known samples, the table
is built using location templates. When a location needs to be
screened, gamma ray detectors, placed in a ﬁxed geometry,
are used to acquire counts. The distribution of counts across
the detectors are compared against these reference tables to
predict if a source is present, and the corresponding detector-
to-source angle. Even with these methods, a secondary phase
is often necessary, where responders (paired with portable

1

 
 
 
 
 
 
detectors) manually search for the radioactive material on
foot [15,26,36]. A central limitation to this process is compar-
ing the readings from the detectors to the reference tables: in
non-trivial cases, attenuation, scattering, shielding, and other
naturally occurring phenomena can signiﬁcantly deviate the
characteristics of the acquired gamma-ray signals. These fac-
tors limit the utility of reference tables, and are further exac-
erbated as the true distance to the radioactive source diverges
from the calibration distance of the reference table.

Localization of radioactive materials has a striking similar-
ity to one of the most fundamental problems in cyber-security:
detection. Similar to malware, spam, or network intrusion
detection, an adversary hides a malicious artifact, and the
challenge falls upon the defender to use information from
the environment (often overburdened by noise) to detect and
locate that artifact. For the past four decades, the security
community has developed techniques that reveal information-
rich artifacts and methods to amplify desirable signals from
the surrounding noise. Our insight is that the application of
techniques from cyber-security to radioactive material local-
ization has the potential to produce signiﬁcant advancements
in physical security.

From a computer security perspective, the physical phe-
nomena described earlier (e.g., attenuation, scattering, and
shielding) inject noise into the readings. In fact, the presence
of this noise causes naive application of machine learning to
yield results worse than even the relatively inaccurate table-
based approaches. We posit that applying and adapting feature
scaling techniques used in cyber detection domains will mit-
igate the impact of noise and amplify the signal of interest.
Speciﬁcally, we apply unit norm scaling (regularly used in
spam detection) & robust feature standardization (often used
in domains that have features with broad scales, i.e., network
intrusion detection) to be able to discern the signal of inter-
est from the noisy environment. With these techniques, we
signiﬁcantly improve the ability to localize radioactive ma-
terials. Further, we explore the abilities of machine learning
models to estimate the distance to radioactive material, thus
quantifying both angle and distance.

One of the new opportunities afforded by the novel appli-
cation of cyber-security techniques to this physical domain is
that we are no longer bound to predicting directionality exclu-
sively; we explore the abilities of machine learning models to
estimate the distance to radioactive materials and show that
they are effective in a suite of different environments. The
application and adaptation of these techniques present a new
capability that has not been achieved for radiation detection
applications. This paper represents a signiﬁcant step forward
in Directional Gamma-ray Detection with the development
of a novel framework to predict distance as well as direction
(and thus location) with a stationary detection system.

In this paper, we present techniques which have the
potential to advance the current capabilities for locating
radioactive materials. We apply and adapt data curation

techniques used successfully in cyber detection domains,
and tune machine learning models to localize radioactive
sources. We assess the approach using the Monte-Carlo-based
radiation transport framework (Monte Carlo N-Particle
Transport Code [9]) and physical experiments using ra-
dioactive sources in laboratory settings. We design the experi-
ments to include obstructions that affect radioactive signals,
which can serve as a proof-of-concept for cargo inspection
and urban search scenarios. An overview of our approach is
shown in Figure 1.

We evaluate this approach with six different models on
seven datasets, of which ﬁve are simulated and two are experi-
mental (collected in a laboratory environment). With the simu-
lated experiments we ﬁnd that the cyber-inspired approach re-
duces the angular error by 37 % (4.9°, 95% CI±0.07 from the
reference table to 3.1°, 95% CI ± 0.04 with our approach) and
we can predict distance within 2.4 %, 95% CI ± 0.54 of the
source’s location (up to 15 m). For the laboratory experiments,
we reduce the angular error by 26 % (8.5°, 95% CI ± 0.22
from the reference table to 6.3°, 95% CI ± 0.17 with unit
norm scaling) and predict distance within 13.0 %, 95% CI ±
3.74 of the radioactive materials (up to 3 m). Our contribu-
tions are:

• We present techniques adapted from cyber-security de-
tection to exploit the use of gamma-ray signals for accu-
rate radioactive material localization.

• We demonstrate that our approach surpasses the tradi-
tional table-based approach, incurring an average angu-
lar error of 3.1°, 95% CI ± 0.04 vs. 4.9°, 95% CI ± 0.07,
respectively.

• We extend the standard deﬁnition of localization to in-
clude distance. Here, the posited approach can predict
distance within 2.4 %, 95% CI ± 0.54 of a simulated ra-
dioactive source when the source strength is known.

• We perform experiments with real, radioactive sources
to validate our ﬁndings in complex laboratory environ-
ments. Our approaches surpass the table-based method,
incurring an average angular error of 6.3°, 95% CI ±
0.17 vs. 8.5°, 95% CI ± 0.22, respectively. Moreover,
we can predict distance within 13.0 %, 95% CI ± 3.74
of real radioactive materials.

• We provide seven new datasets (including simulated
and real data) that we make public for future research
in this important domain, curated for use with machine
learning.

2 Problem Deﬁnition

Threat Model. Our problem is essentially a game of hide
and seek: an adversary places a radioactive source and the
objective, as the defender, is to conﬁrm its existence and de-
termine its location. We assume a complex environment, that

2

Figure 1: Localizing Radioactive Materials - We apply data curation techniques, shown to be successful in cyber-detection
applications, to counts collected from radioactive signals to predict both the angle to and distance from a radioactive source.

is, it contains obstructions (e.g., buildings) that interfere with
(and thus, obfuscate) the signal produced by the radioactive
source. Further, we assume a stationary environment: the
adversary is non-adaptive and the radioactive source and the
radiation detector are stationary (however, the distance and an-
gle between the source and the radiation detector can change
across different experimental scenarios), as are the surround-
ing obstructions in the environment. We also assume that the
adversary has no ability to intervene with the operation of the
radiation detector and that it is operating optimally (and there-
fore trust the produced readings to be as accurate as physical
phenomena permit).

To some extent, we also assume no a priori knowledge of
the radioactive material being used by the adversary (which
we detail in Section 4). While the experiments are done ex-
clusively with one material (due to availability, safety, and
applicability), the method with which we detect and localize
a radioactive source is agnostic to any gamma ray emitting
isotope used by an adversary. The intuition behind this is
straightforward: while different gamma ray emitting isotopes
exhibit unique radioactive signatures, they all emit quanta
within certain energy regions. Therefore, for this approach,
detecting a particular isotope is simply a function of which
portion of the energy region is scanned. Thus, a takeaway of
this work is that responders can be “blind”, in some sense, to
the speciﬁc isotope used by an adversary.

Detector Setup. The detection of a radioactive source relies
upon the use of a detector array. The radiation produced by the
source interacts with the detectors to generate a signal. This
signal is then captured and used to produce a histogram cor-
responding to the energy deposited via different interactions.
The detection system is comprised of four or eight detectors,
each of which collect individual energy histograms. Here we
sub-sample counts over a range of energies which are the
inputs (i.e., features) to the machine learning models.

As mentioned in Section 1, there are a suite of environ-
mental factors that can negatively affect the readings of the
detector (aiding the adversary). Figure 2 highlights some of
the main sources of these environmental factors, and how
they impact the signal. Notably, there are three central phe-
nomena produced by obstructions: attenuation, scatter, and
shielding. We describe in more detail how these phenomena
affect the readings in Section 3, but for the purposes of the
problem deﬁnition, these broadly just reduce the signal or

Figure 2: Detection in a complex environment - The principal
objective is to detect and localize radioactive sources in com-
plex environments that induce undesirable phenomena, e.g.,
shielding, attenuation, scatter, and background noise.

amplify noise.

Machine Learning for Security. Machine learning has been
successful in computer security detection applications includ-
ing network intrusion detection, malware, and zero-day vul-
nerabilities [17, 29, 32]. However, machine learning has not
been applied to radioactive source search scenarios, which
shares many parallels with domains within computer security.
A central factor for deploying machine learning in security-
sensitive domains is feature scaling [23, 30, 31]. In network
intrusion detection, there are many different kinds of features,
which can contain outliers that can negatively affect standard-
ization [30]. We ﬁnd that detecting radioactive materials faces
a similar burden, in that the distributions of gamma-ray counts
can contain strong outliers (i.e., sources of noise). By apply-
ing robust feature standardization techniques that account
for these outliers, we improve the accuracy of many learning
algorithms.

As a second optimization, we take inspiration from tech-
niques used traditionally for spam detection: unit norm scal-
ing. Speciﬁcally, we observe that much like analyzing the
relative frequency of words in emails, learning algorithms are
likely to be more accurate in localizing radioactive materials
with relative detector counts rather than gross signals.

3

ΘScatterObstructionShieldingAttenuationObstruction60CoSophistication of ApproachSuccessSpamBlacklistsWord-based FiltersNaïve MLML with Unit-norm ScalingRadiationDetectionElevated CountsReference TablesNaïve MLML with CPS TechniquesIntrusion DetectionSignaturesAnomaliesNaïve MLML with Robust StandarizationMalwareSignaturesNaïve MLML with OpcodesSandboxesRadiationCountsPhysical DomainSignalPreprocessingFeaturesAngle & DistancePhysical DomainLocationMachineLearningΘScatterObstructionShieldingAttenuation3 Radioactivity in the Physical World

Radiation, which is energy in transit, can be either electri-
cally charged (e.g., electrons, protons, and alpha particles) or
uncharged (gamma rays, x-rays, and neutrons). Uncharged
radiation poses unique detection challenges, but is not eas-
ily shielded [16]. In this work, gamma-rays are the principal
phenomena of interest as they are not readily shielded by
thin metals (i.e., shipping containers), unlike charged parti-
cles [33]. Also, gamma-rays produce unique energy signa-
tures [14] which can be used to classify the radioactive source,
analogous to signatures produced by malware in intrusion de-
tection systems. Generally, gamma-rays that pass through a
detector interact in one of three ways: the photoelectric effect,
Compton scattering, or pair production. These interactions
produce readings that eventually become the features of the
approach.

Poisson Statistics. Absent of physical phenomena (and any
detector deﬁciencies), one of the most fundamental challenges
in interpreting detector readings is that they are burdened by
Poisson statistics. The underlying uncertainty complicates
accurate interpretation of the readings; the stochastic nature
of radioactive decay means that the exact same experiment
repeated twice in a row will yield different results. This
fact gives a fundamental insight into what makes localizing
radioactive sources a challenging problem.

In the simplest scenario (no obstructions, line-of-sight to
the radioactive source, and ideal detector characteristics), the
phenomena above, coupled with Poisson statistics, can have a
notable affect on detector results, which table-based analysis
approaches have difﬁculty rectifying. Much like network ad-
versaries who obfuscate their signature to frustrate detection
systems by mixing benign requests in the midst of malicious
ones, noise produced by scattering and attenuation (as well as
the fundamental uncertainty) can have a non-trivial negative
effect on this approach with respect to localizing radioactive
sources.

3.1 Existing Approaches

The two phase search procedure, which relies on detection
and localization, is a demanding process, both in time and
labor, given that responders must triangulate the radioactive
source manually. Currently deployed techniques aim to com-
bine both phases: by analyzing minute differences between
counts received across detectors in an array of ﬁxed geometry,
the angle to the radioactive source can be determined (within
some error). This problem is known as directional gamma-ray
detection. Most conventional techniques use pre-populated
datasets (i.e., reference tables) of known source locations, cal-
ibrated at a speciﬁc distance in lab environments [28] (shown
in Figure 4). However, these approaches suffer in non-trivial
cases where attenuation, scattering, shielding, and other natu-
rally occurring phenomena affect the detected signals. More,

these methods still require responders to manually search in
the suspected direction of the radioactive source. Thus, they
are susceptible to human-error, inaccuracies of the reference
tables, and are bound by the number of responders that can
be equipped with portable detectors to triangulate the source.
For this work, we measure the net counts for each indi-
vidual detector and compute their differences to predict both
angle and distance (which gives us localization). This com-
bines two previously disjoint stages, enabling responders to
quickly identify the location of the radioactive material. We
hypothesize there is latent information that characterizes the
environment which enables the location of the radioactive
material to be determined. Consider that if one detector re-
ceives more counts than another, it is likely the source is in
the direction of the detector with the highest counts. However,
if an obstruction is directly in front of this detector, then the
neighboring detectors may receive more counts. The chal-
lenge here is to capture these subtle situations–tools used in
detection in computer security environments are effective at
pulling out this embedded information, and we exploit this
observation in our analysis.

4 Approach

Localizing radioactive materials in noisy environments shares
many of the same challenges observed in the cyber-security
detection space. Here, we brieﬂy detail the radioactive source
used, some relevant characteristics of the detector, the frame-
work used in the simulated experiments, describe the feature
scaling adaptations, and present the machine learning algo-
rithms used.

4.1 Material Detected

Cobalt-60 (60Co) is the radioactive isotope used in the sim-
ulations and laboratory experiments. Cobalt-60 is a relevant
isotope to study, as it can be found in many domains, includ-
ing medicine, industry, food, and nuclear power [13, 18, 34].
The widespread use of 60Co means that, in practice, it is an
isotope responders often wish to locate.. Most importantly,
while we use 60Co in the experiments, we emphasize that
these techniques are not speciﬁc to this isotope; many ra-
dioactive isotopes have characteristic peaks similar to 60Co,
simply at different energies [16].

4.2 The Detectors

In gamma-ray spectroscopy, there are two main detector types:
scintillators and semiconductors. Though most semiconduc-
tors offer better resolution and improved intrinsic efﬁciency
(i.e., a high probability of interaction with gamma-rays), they
are expensive, and some requiring cooling to liquid nitro-
gen temperatures (−196 °C) [27]. Such requirements were
impractical for this work.

4

Thus, we use thallium-doped sodium iodide (NaI(Tl)) scin-
tillation detectors, popular in many ﬁeld applications [19].
There are a handful of properties that make NaI(Tl) detec-
tors useful for experimentation, namely: room-temperature
operation, high efﬁciency, and large photofraction (i.e., the
fraction of incident photons fully absorbed). The popularity
and accessibility of the detector makes it an attractive choice
for evaluating the applicability of this approach.

4.3 Monte Carlo N-Particle Transport Code

MONTE CARLO N-PARTICLE TRANSPORT CODE (MCNP)
is a Monte Carlo method simulator for radiation transport1.
It uses Monte Carlo methods to simulate interactions (i.e.,
absorption, and scattering) as radiation propagates through
a medium. Monte Carlo methods are considered to be the
de facto standard for applications in radiation analysis due
to their ability to accurately model radiation transport and
interactions [9]. Due to its ability to simulate nearly any en-
vironment, MCNP is used in many ﬁelds, including medicine,
detector design, reactor design, radiography, material pene-
tration tests, radiation dosimetry, among others [9]. Figure 3
showcases the capacity of MCNP to model real radioactive
phenomena–the simulated detector responses map nearly 1:1
onto the laboratory detector readings. Differences between
simulated and laboratory readings are largely attributed to
the effects of gain-shift [4, 25], stemming from temperature
and other environmental inﬂuences. Gain shift has the effect
of “shifting” the spectra, which can lead to counts artiﬁcially
being added or subtracted to the reading. As each detector
experiences differing amounts of gain shift, these small ef-
fects can propagate to notable differences in the normalized
input features. To partially mitigate this, energy calibrations
are regularly performed.

4.4 Feature Selection

With a popular isotope and effective detector, we return to the
central goal: localization of radioactive sources in complex en-
vironments. To achieve this goal, we ﬁrst ask a basic question:
what can we measure? We defer to historically successful
techniques to answer this question.

As described in Section 3, prior work has combined detec-
tion and direction (i.e., angle) into a single phase by analyzing
slight variances in the responses of each detector in a ﬁxed
geometry array [6]. Here, we use gamma-ray counts as fea-
tures, and extend the analysis to localizing (i.e., predict both
angle and distance) radioactive sources. In this way, we seek
to combine all phases of detection and localization into one
step, averting cost for any special equipment and saving time
by avoiding a manual foot search.

1“Radiation Transport” software simulates the propagation of radiation

through space and its interactions in media.

Figure 3: Detector responses as a function of angle for lab-
oratory experiments and simulations - MCNP is capable of
modeling radioactive phenomena with striking accuracy and
precision. Error bars on response values are included, but
comparable in size to the plot markers. MCNP is the de facto
tool used for radiation analysis, particularly large-scale exper-
iments that are difﬁcult to safely execute in live laboratory
settings.

4.5 Feature Optimization & Cyber-security

The abundance of research in localizing radioactive sources
has many answers to the question asked above. However,
there is yet another question that we must ask: how do we ﬁlter
noise without dampening the signal of interest? To answer
this question, our intuition leads us to our central hypothesis:
techniques used in the cyber-security detection space can be
useful in localizing radioactive materials. We describe the
techniques and relevant adaptations below2.
Robust Feature Standardization. For many learning algo-
rithms, standardizing feature scales is an important prerequi-
site. A common technique is to subtract the mean and scale to
unit variance. However, standard techniques can be negatively
affected by features that have highly skewed distributions, like
those seen in network intrusion detection [30]. Instead, sub-
tracting the median and scaling features according to some
quantile range has been shown to produce better results for
features with outliers. We standardize the features via:

ˆx =

x − ˜x
max Q3 − max Q1

(1)

where x is the original feature, ˆx is a standardized feature, ˜x is
the median value, and max Qi is the maximum value for the
ith quantile. By scaling based on the maximum value in some

2It is worth noting that we tried other data manipulation techniques that
had marginal (or sometimes even negative) effect on the accuracy of the
models, namely: 0-1 rescaling, normalization (i.e., mean centered at 0), and
standardization (i.e., mean centered at 0 and a standard deviation of 1).

5

0255075100125150175Angle(Degrees)0.150.200.250.300.350.40DetectorResponse(arb.unit)Detector0Detector1Detector2Detector3Simulatedquantile, we mitigate the negative inﬂuence outliers may have
on the accuracy of some learning techniques.
Unit Norm Scaling. lp normalization is an arguably uncom-
mon feature scaling technique (l2 regularization on model
parameters is a common use of lp norms in machine learning).
If we represent a dataset as an M × N matrix of M samples
and N features, then most feature scaling techniques operate
across all samples (i.e., M × 1)–that is, one particular feature
for all samples is scaled in some manner. However, unit norm
differs in that it operates across one sample (i.e., 1 × N). We
can formulate unit norm scaling as:

ˆx =

x
(cid:107)x(cid:107)lp

(2)

where ˆx is a lp norm scaled input. Our intuition for using
unit norm scaling follows successful applications of natural
language processing towards spam detection: a natural objec-
tive for spam detection is to determine the term frequency of
words (or n-grams) in an email. For example, it is difﬁcult
to draw any conclusions if the bigram “free money” appears
a handful of times in email. However, stronger conclusions
can be drawn if “free money” was the most common bigram.
We apply this same reasoning to predicting the angle to a
radioactive source: whether or not a detector receives 10 or
100 counts is hardly useful; what is more important is that
a detector received the most counts (which is then the most
likely direction to the radioactive source). This insight leads
to substantial increases in angle prediction accuracy.

4.6 Reference Tables & Machine Learning

Here, we describe the reference tables and learning algorithms
used in this work. The techniques presented here were speciﬁ-
cally chosen, as they offer unique advantages over one another,
such as interpretability, scalability, and accuracy. Further de-
tails are presented in Appendix A.
Reference Tables. As described in Section 3, reference tables
are commonly used for directional gamma-ray detection. To
build a reference table: detectors are setup in a ﬁxed geometry,
a known source is selected and placed at a ﬁxed distance from
the detector array, and the relative counts for each detector are
recorded at varying source angles. These reference tables are
ostensibly a closed-form approximation of the phenomena as
they encode the response of a detector as a function of angle
to the source, as shown in Figure 4.

Once the reference table is calibrated, the responses of the
detectors to an unknown source are compared to the reference
table. Often with a least-squares regression analysis, where
the angle to the unknown source is predicted by:

θγ = arg min

(Γ(θ) − x)2

θ

(3)

Figure 4: MCNP simulated detector responses as a function of
source angle - reference tables are built from these responses
and are then used to approximate unknown sources. Calcu-
lated error bars are comparable in size to line thickness, and
are thus excluded.

a vector of detector counts for a particular observation. Equa-
tion 3 leads us to two observations: 1) differences between
the calibration environment and real world environment (such
as the presence of obstructions) will lead to discrepancies
in the relative detector responses, potentially leading respon-
ders in the wrong direction, and 2) often more severely, slight
variations in distance can produce profoundly different distri-
butions of counts than any observation for which the reference
table was calibrated.

Logistic Regression. Logistic regression (LR), akin to linear
regression, computes a weighted sum of input features with an
additional bias term, and applies the logistic function to this
sum [35]. While more sophisticated techniques have emerged,
each have limitations. We include logistic regression models
in this work to investigate if simpler models sufﬁce to perform
localization tasks accurately and quickly.

Support-Vector Machines. Prior to the inception of deep
learning, support-vector machines (SVMs) dominated ma-
chine learning benchmarks across many domains [3]. SVMs
are attractive as they can form non-linear decision boundaries,
which may be necessary given the noisiness of this domain.

k-Nearest Neighbors. k-Nearest Neighbors (kNN) is a non-
parametric approach [2]. As the observed count distributions
in this domain can change rapidly in a variety of unique
environments, kNN is particularly useful as it does not make
any assumptions about the underlying data.

where θγ is the angle predicted by the reference table, Γ(θ)
are the calibrated detector responses for some angle θ, and x is

Decision Trees. Decision Trees (DT) are ﬂexible machine
learning algorithms that are commonly used today [24]. They

6

050100150200250300350Angle(Degrees)0.30.40.50.60.70.80.91.0DetectorResponse(arb.unit)Detector1Detector2Detector3Detector4Detector5Detector6Detector7Detector8require minimal data preparation, have low performance over-
heads, and offer intuitive explanations of the formed decision
boundaries. Decision trees are appealing in this domain not
only as another non-parametric technique, but also because
of their white-box design: the learned decisions are easy to
interpret, which can be useful in understanding subtle changes
in the decision process as a function of the environment.
Deep Neural Networks. Deep neural networks represent a
state-of-the-art class of learning techniques that have demon-
strated success in the most challenging machine learning
benchmarks [7]. Deﬁnitions vary, but generally speaking,
deep neural networks often refer to any class of artiﬁcial
neural networks with multiple layers between the input and
output layers. For this work, the “deep neural network” is
a fully-connected feedforward network, with some number
of hidden layers, trained with back-propagation, using the
rectiﬁer (colloquially, “ReLU”) as the activation function.

5 Evaluation

In this section, we evaluate the approach on seven datasets:
ﬁve were simulated and two were measured3. The experi-
ments were performed on a Dell Precision T7600 with Intel
Xeon E5-2630 and NVIDIA Geforce TITAN X. We used
Scikit-learn [22] for data curation and for instantiating the
machine learning models. We ask:

• Is the naive machine learning solution sufﬁcient for lo-

calizing radioactive sources?

• Can our optimizations approximate or exceed the perfor-

mance of existing table-based angle predictions?

• When source strength is known, can distance be pre-
dicted and, if so, how is the accuracy affected as a func-
tion of distance from the radioactive source?

Summary: The simulation and physical laboratory experi-
ments demonstrate that the developed techniques outperform
the reference table for predicting angle by 37 % (down from
4.9° to 3.1°) and estimate distance within 2.4 % of the dis-
tance to a radioactive source.

5.1 Experimental Scale & Parameters

The simulated and laboratory datasets contain a radioactive
source that is located 1–15 m and 1–3 m away from the de-
tector array center, respectively. Simulated acquisitions were
approximated as 14 s counts of a 1 and 10 µCi 60Co source,
while laboratory acquisitions were 5 minute counts of an
approximately 1 µCi 60Co source. With scaling (discussed

below), these are common parameters in this ﬁeld, and serve
as a proof-of-concept for this analysis method.

The experiments contain three factors that are scaled in
a manner that makes the datasets especially challenging: at-
tenuation, measurement time, and distance. These datasets
serve as benchmarks for evaluating approaches on real ra-
dioactive material subject to all forms of physical phenomena
and environmental factors.

Attenuation. Radioactive sources used by medical or indus-
trial entities can be on the order of ∼100 Ci (around eight
orders of magnitude stronger than the real source used in this
work) [13, 21]. These sources have the potential to be used by
the adversary, and thus have activities similar to those which
these methods could be used to localize. Separate simulations
were conducted with the four detector array and 60Co sources
of various activities to gauge the scaling of obstruction thick-
ness and attenuation. Results showed that the attenuation
effect of a single cinder block (10 cm of solid concrete) on
a 1 µCi source is approximately the same as the attenuation
effect of 150 cm of solid concrete on a 1 Ci source. Thus,
the experiments model a challenging scenario where a weak
radioactive source is in a thick concrete building.

Sample Time. We also modeled the measurement time in
the simulated datasets to be comparable to the times used
in realistic search scenarios [16]. Recall, radioactive sources
decay at a particular rate—that is, a 1 Ci source sampled for 5
minutes will produce (theoretically) identical results to a 1
2 Ci
source sampled for 10 minutes (assuming a half-life much
greater than the measurement time). Since these sampling
times are comparable to realistic scenarios, but with a source
potentially 8 orders of magnitude weaker than those in real
scenarios, predicting angle and distance is challenging from a
scaling perspective.

Distance. Most sources radiate in an isotopic manner, and
thus fall subject to many laws in signal processing, notably the
inverse-square law. This means that the intensity perceived
by a detector decreases squarely with distance. Additional
simulations were conducted to gauge the scaling effects on
distance, to put the laboratory measurements in perspective
to real world expectations. Results isolating the effects of
geometry alone showed that the counts we receive with the
1 µCi 60Co source in ﬁve minutes at 3 m are comparable to
the counts we would get from a 1 Ci 60Co source in a single
second at 100 m. So while the sources used in the labora-
tory experiments are challenging to detect at 0.5 to 3 m away
(with short data acquisition times), they more than scale to
parameters useful for actual source search applications.

5.2 Datasets

3The radioactive 60Co source used in the laboratory experiments was
a low activity source. At approximately 1 µCi, it is safe to handle with the
appropriate safety procedures, which were deﬁned and strictly followed for
all the measurements.

The ﬁrst task was to create datasets of simulated and real-
world measured gamma-ray counts in various settings to
enable evaluation of the detection algorithm. We generated

7

seven, which we will refer to as S-Dataset 1, S-Dataset 2
106/7, S-Dataset 3 106/7 for the simulated experiments (from
MCNP), L-Dataset 1, and L-Dataset 2 for the laboratory ex-
periments. The 106 and 107 variants of S-Dataset 2 and 3
describes the number of simulated gamma-rays per trial. The
different numbers of simulated gamma-rays correspond to
different source strengths or measurements times, and repre-
sent differing levels of statistics. All scenarios use the same
radioactive source (60Co) at varying distances, angles, and
obstruction locations, as described below and summarized in
Table 1.

Simulated Datasets. The datasets were generated with an
8-detector array and a source at varying distances between 1
and 15 m. S-Datasets 1 & 2 contain 72, 000 samples where
the source is uniformly rotated a full 360° around the detector
array (at roughly 1° increments), while S-Dataset 3 contains
27, 000 samples and the radioactive source is only rotated
90°. For each trial, either 106 or 107 gamma-rays were sim-
ulated, corresponding to a 14 s count of a 1 µCi or a 10 µCi
60Co source. S-Datasets 2 & 3 contained a solid concrete
obstruction that mimics the effects of a concrete building. In
S-Dataset 2, the obstruction was stationary; in S-Dataset 3,
the obstruction was randomly placed between ten locations.
Table 1 provides experiment details.

While one million gamma-rays may sound signiﬁcant, re-
call the isotropic nature of radiation and the variety of physical
phenomena described in Section 3; in reality, less than 0.1 %
of these gamma-rays will cause some interaction (either pos-
itively or negatively) with the detector array 3 m from the
source.

Laboratory Datasets. The two laboratory datasets were ac-
quired with a 4-detector array setup (shown in Figure 6) and
a radioactive source at varying distances between 0.5 and
3 m. Both datasets contain 125 samples where a 3 µCi 60Co
radioactive source is rotated 90° at (roughly) 15° increments
around the detectors. L-Dataset 1 has no obstructions and L-
Dataset 2 contains concrete obstructions at ﬁxed locations. A
summary of the data is presented in Table 1, and a photograph
of the detector setup and accompanying block diagram are
shown in Figures 5 & 6.

While 125 samples per dataset may seem small, it is both
signiﬁcant in this context and sufﬁcient. A single sample often
requires approximately 5 minutes to collect (i.e., nearly 11
hours of data collection for one dataset). Moreover, through-
out this entire process, we regularly performed energy cali-
brations on the detector, and periodically acquired separate
background radiation spectra to make the readings as accurate
as possible4. Finally, some learning models (e.g., SVMs) are
performant on small datasets. Thus, these relatively small lab-
oratory datasets represent a challenge in localizing radioactive
sources when data may be severely limited.

4Detectors in reality experience what is known as “gain shift”–the energy
spectrum for radiation slowly changes overtime from a variety of environ-

Figure 5: Detection in Radioactive Environments - The
gamma-rays from the 60Co source interact with the detec-
tor array, consisting of NaI detectors which emit light upon
gamma-ray interactions, photomultiplier tubes (PMTs) to con-
vert the light into an analog pulse, and a digitizer to convert
the analog pulse into a digital signal. The digital signal is then
processed by spectroscopy software to convert the signals into
the “counts”, which are used as input to the machine learning
models.

Figure 6: Setup of L-Dataset 2 - This is a photograph of the
detector array in the laboratory. The 60Co source is attached to
the cardboard tube. The concrete blocks are the obstructions
and cause the behavior of the gamma-rays produced by the
60Co source to be representative of an urban environment.

5.3 Experiment Overview

This section details experiments exploring how the proposed
models predict angle and distance as compared to the ref-
erence tables. In the following ﬁgures, the accuracy is the
number of samples where the exact angle (or distance5) was
predicted correctly over the total number of samples. We
highlight these results as, in real scenarios, even ±5° angular
tolerance may be unacceptable (particularly if the radioactive

mental factors. Additionally, the background radiation can vary with time
and location. For radioactive source search scenarios, a single calibration is
often sufﬁcient, however, data collection for scientiﬁc use, such as this work,
requires recalibrating the detector for gain shift and background radiation
regularly to obtain accurate measurements.

8

Nal(Tl) DetectorPhotomultiplerTubePoint-like sourceDetector ArrayDigitizerComputer with Spectroscopy SoftwareAnodeSignal(a) Predicting Angle

Figure 7: Localizing radioactive sources with naive machine learning - Applying machine learning to raw detector counts
produced average results for predicting angle. Estimating distance was acceptable for some learning techniques.

(b) Predicting Distance

Dataset

S-Dataset 1
S-Dataset 2
S-Dataset 3
L-Dataset 1
L-Dataset 2

Obstruction
Size (m)

Obstruction
Location

Angle (°) Distance (m) Num. Trials

1 × 2 × 5
0.5 × 2 × 5

Fixed
Moving

0.5 × 2 × 5

Fixed

0–360
0–360
0–90
0–90
0–90

1–15
1–15
1–15
0.5–3
0.5–3

72,000
72,000
27,000
125
125

Table 1: Dataset statistics for the experiments.

source is estimated to be far away).

5The distances are binned (and approximated) via the Freedman–Diaconis
Estimator, which is an outlier-resilient, optimal binning algorithm [8]. Im-
portantly, the estimator suggests bin widths so that the difference between
the empirical and theoretical probability distributions are minimal. Reported
accuracy is the number of samples where the bin was predicted over the total

Naive Machine Learning. The naive approach to this prob-
lem is to directly use the raw counts collected by the detector
as inputs to learning models. As shown in Figure 7 (a), the re-
sults are average: the reference table had average angular error
of 4.9°, 95% CI±0.07 while the best model (k-Nearest Neigh-
bor) had an average angular error of 4.2°, 95% CI ± 0.11.The
logistic regression model and deep neural network were un-
able to predict neither angle nor distance correctly. For the
distance at which the reference table was calibrated (around
200 cm; where the peaks are), the reference table outper-
formed all of the models. As the distance increased, the
reference table quickly became inaccurate, unlike Decision

number of samples. For the simulations and laboratory experiments, 42 and
8 bins were created, i.e., 35 and 3.75 cm per bin, respectively.

9

0.00.20.40.60.81.0Accuracy(%)L-Dataset1rt=ReferenceTablelr=LogisticRegressiondt=DecisionTreessvm=SupportVectorMachinesknn=k-NearestNeighborsdnn=DeepNeuralNetworkS-Dataset2(107)S-Dataset3(107)0100200Distance(cm)0.00.20.40.60.81.0Accuracy(%)L-Dataset205001000Distance(cm)S-Dataset105001000Distance(cm)S-Dataset2(106)05001000Distance(cm)S-Dataset3(106)0.00.20.40.60.81.0Accuracy(%)L-Dataset1lr=LogisticRegressiondt=DecisionTreessvm=SupportVectorMachinesknn=k-NearestNeighborsdnn=DeepNeuralNetworkS-Dataset2(107)S-Dataset3(107)0100200Distance(cm)0.00.20.40.60.81.0Accuracy(%)L-Dataset205001000Distance(cm)S-Dataset105001000Distance(cm)S-Dataset2(106)05001000Distance(cm)S-Dataset3(106)(a) Predicting Angle

Figure 8: Localizing radioactive sources with cyber-security detection techniques - applying unit norm scaling and robust outlier
standardization lead to signiﬁcant improvements for most of the models.

(b) Predicting Distance

Trees and k-Nearest Neighbors. However, neither of these
algorithms eclipsed the accuracy of the reference table at
any particular distance compared to the maximal accuracy
of the reference table (i.e., the distance it was calibrated for).
For the laboratory experiments, the best models performed
worse: the reference table had an average angular error of
8.5°, 95% CI ± 0.22, while the best model had an average
angular error of 12.9°, 95% CI ± 0.37. Thus, for predicting
angle, the simple application of machine learning yields re-
sults worse than reference tables at their calibrated distance,
marginally better at other distances for the simulated experi-
ments, and explicitly worse for the measured datasets.

For predicting distance (Figure 7 (b)), the naive approach
produced impressive results with some of the algorithms for
scenarios in which the source strength is assumed to be known.

The best model (k-Nearest Neighbors) could predict distance
within 2.5 %, 95% CI ± 0.55 of the distance to a source. How-
ever, for the simulated data, most of the learning techniques
were not able to predict distance at all, while decision trees
and k-Nearest Neighbor produced a sigmoid-like curve for the
60Co 106 simulation. Perhaps not surprisingly, these models
can estimate (nearly perfectly) the distance to sources that are
exceedingly close, yet struggle for sources that are relatively
far away.

Applying Cyber-security Detection Techniques. As de-
tailed in Section 4, the radioactive environment is inherently
burdened by noise, similar to intrusion detection domains,
and thus we suspected that a cyber detection approach would
readily apply in this domain. Figure 8 (a) demonstrates the

10

0.00.20.40.60.81.0Accuracy(%)L-Dataset1rt=ReferenceTablelr=LogisticRegressiondt=DecisionTreessvm=SupportVectorMachinesknn=k-NearestNeighborsdnn=DeepNeuralNetworkS-Dataset2(107)S-Dataset3(107)0100200Distance(cm)0.00.20.40.60.81.0Accuracy(%)L-Dataset205001000Distance(cm)S-Dataset105001000Distance(cm)S-Dataset2(106)05001000Distance(cm)S-Dataset3(106)0.00.20.40.60.81.0Accuracy(%)L-Dataset1lr=LogisticRegressiondt=DecisionTreessvm=SupportVectorMachinesknn=k-NearestNeighborsdnn=DeepNeuralNetworkS-Dataset2(107)S-Dataset3(107)0100200Distance(cm)0.00.20.40.60.81.0Accuracy(%)L-Dataset205001000Distance(cm)S-Dataset105001000Distance(cm)S-Dataset2(106)05001000Distance(cm)S-Dataset3(106)results. Immediately, we can see signiﬁcant improvements:
many of the models now exceed the reference table accu-
racy, even at the distance at which the reference table was
calibrated. After applying unit norm scaling, the average
angular error for the best model (k-Nearest Neighbors) is
3.1°, 95% CI ± 0.04 (down from 4.2°), an improvement from
the reference table by 37 % (down from 4.9°). For the lab-
oratory datasets, we also see improvements: we reduce the
angular error to 6.3°, 95% CI ± 0.17 (down from 12.9°), an
improvement from the reference table by 26 %.

For predicting distance, we applied robust feature stan-
dardization. Like network intrusion detection, this domain
is inherently noisy and contains outliers that may negatively
inﬂuence standard feature scaling techniques. Figure 9 shows
a sample distribution of detector counts with the quartile
ranges we scale from in the experiments and Figure 8 (b)
shows the results. A small improvement is made to the
overall accuracy after applying robust feature standardiza-
tion (from 2.5 %, 95% CI ± 0.55 to 2.4 %, 95% CI ± 0.54
for the best models) and substantial gains for the other
models (e.g., logistic regression, support vector machines,
and deep neural networks) as shown in Figure 8. For the
laboratory experiments, robust feature standardization also
yielded small improvements (from 13.5 %, 95% CI ± 3.66 to
13.0 %, 95% CI ± 3.74).
We highlight key takeaways of this work:

• Our approach can far surpass the capabilities of refer-
ence tables even for the distance at which the table was
calibrated. This demonstrates that: 1) calibrations do not
lend themselves well to the complex nature of problems
in real environments, and 2) model-based approaches,
paired with cyber-security detection techniques, are ef-
fective tools for localizing radioactive sources.

• Our approach accurately estimates distance to a radioac-
tive source. Prior to this work, techniques either required
mobile detectors (either ﬁrst responders on foot or vehi-
cles) to triangulate radioactive sources manually; now,
we are void of these limitations. We can localize a ra-
dioactive source simultaneously at the time it is detected.
While initial trials beneﬁted from an apriori knowledge
of the source strength, similar standardization and ad-
ditional cyber-physical security techniques are being
explored to apply distance predictions to sources of un-
known strength.

• Perhaps not surprisingly, obstructions have a tangible
impact on modeling radioactive behavior: most of the
models observed an ∼10% decrease in accuracy in the
most challenging datasets where the locations of ob-
structions varied. Radioactive source search scenarios in
reality will likely observe similar challenges given that
no environment is identical.

• An order of magnitude increase in particle counts (106 to

107) was especially helpful to increase model accuracy
at the longest distances (i.e., greater than 8.4 m). In other
words, high activity sources (or longer acquisition times)
can be localized accurately over long distances.

6 Observations

Unit Norm Scaling. One of the most signiﬁcant improve-
ments we observed for angle prediction was the application of
unit norm scaling. There are multiple reasons why this tech-
nique was so effective: much like detecting spam in emails,
the absolute frequency of words is hardly useful; instead, it
is often more interesting to see how frequent some words are
used relative to one another. The intuition is straightforward:
if the bulk of an email contains words that are commonly asso-
ciated with spam, then the email is most likely spam as well;
that is to say, long emails that contain 80 % “spam words,” for
example, are fundamentally no different (in terms of spam or
not) than short emails with a similar relative amount of spam
words. We follow this same reasoning for localizing radioac-
tive materials: when receiving a sum total of 1000 counts
or 100 counts, if a particular detector receives the majority
then, in both cases, the radioactive source is most likely in
front of this particular detector. This has the added beneﬁt of
augmenting the training set–learning approaches no longer
have to disentangle that 1000 counts or 100 is relatively mean-
ingless for predicting angle, as those two situations are being
treated identically. These observations give insight into why
this feature scaling technique was so effective.

Robust Outlier Standardization. We found that scaling the
features in a manner robust to outliers was effective for pre-
dicting distance. While using the raw counts was acceptable
for sources that were close to the detectors, we noticed that
the accuracy of the models decreased quickly as the distance
linearly increased (i.e., the Inverse-square law in practice).
We observed that robustly scaling features helped maintain
the accuracy of the models over longer distances. Figure 9 led
us to our insight: we aim to emphasize the signal from within
the two dotted lines as the bulk of the counts indicated that
the source was directly behind the detector in this example.
However, due, in part, to the physical phenomena described
in Section 3, the detector observed a small increase in counts
directly in front of it (i.e., at 0°). Thus, we hypothesized that
mitigating the inﬂuence of these outliers would aid in pre-
dicting distance. The results demonstrate that this insight was
indeed helpful.

Estimating Distance. While these approaches are relatively
accurate at predicting distance, estimating distance is difﬁ-
cult, especially with a stationary system. Today, there is a
focus on using mobile systems to localize radioactive sources
(e.g., Mobile Urban Radiation Search (MURS) [5]). By taking
multiple samples at different locations, mobile systems can
exploit basic triangulation algorithms to estimate the distance

11

eclipse the accuracy of table-based approaches at distances
for which the table was calibrated. Yet, curated applications,
such as unit-norm scaling and robust standardization, can pro-
duce results which surpass table-based approaches across all
evaluated distances. This work demonstrates that the complex
signals produced during source localization efforts can beneﬁt
from machine learning approaches with curated application
of signal ampliﬁcation techniques. Future efforts will focus
on more robust distance prediction techniques and greater
collection of physical measurements in controlled laboratory
settings. Through our application of cyber-security princi-
ples, we introduced a state-of-the-art approach in localizing
radioactive sources in complex physical scenarios.

References

[1] Nuclear Development : Nuclear Energy Today. page

114.

[2] N. S. Altman. An Introduction to Kernel and Nearest-
Neighbor Nonparametric Regression. The American
Statistician, 46(3):175–185, 1992.

[3] Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N.
Vapnik. A Training Algorithm for Optimal Margin Clas-
siﬁers. In Proceedings of the Fifth Annual Workshop
on Computational Learning Theory, COLT ’92, pages
144–152, New York, NY, USA, 1992. Association for
Computing Machinery. event-place: Pittsburgh, Penn-
sylvania, USA.

[4] R. Casanovas, J.J. Morant, and M. Salvadó. Temper-
ature peak-shift correction methods for NaI(Tl) and
LaBr3(Ce) gamma-ray spectrum stabilisation. Radi-
ation Measurements, 47(8):588–595, August 2012.

[5] Joseph C. Curtis, Reynold J. Cooper, Tenzing H.
Joshi, Bogdan Cosofret, Thomas Schmit, John Wright,
Jonathan Rameau, Daisei Konno, Daniel Brown, Forrest
Otsuka, Eric Rappeport, Matthew Marshall, and Julia
Speicher. Simulation and validation of the Mobile Ur-
ban Radiation Search (MURS) gamma-ray detector re-
sponse. Nuclear Instruments and Methods in Physics
Research Section A: Accelerators, Spectrometers, Detec-
tors and Associated Equipment, 954:161128, February
2020.

[6] Matthew Durbin, Ryan Sheatsley, Christopher Balbier,
Tristan Grieve, Patrick McDaniel, and Azaree Lintereur.
Development of Machine Learning Algorithms for Di-
rectional Gamma Ray Detection. In Proceedings of the
Institute of Nuclear Materials Management, June 2019.

[7] Terrence L. Fine, S. L. Lauritzen, M. Jordan, J. Lawless,
and V. Nair. Feedforward Neural Network Methodology.
Springer-Verlag, Berlin, Heidelberg, 1st edition, 1999.

Figure 9: Demonstrating Outliers - This ﬁgure demonstrates
a photocount distribution for one of the detectors. The dotted
lines mark the quantile ranges of the data that we scale to.
This gives intuition into why robust feature standardization is
helpful: in this ﬁgure, the radioactive source is directly behind
the detector (as shown by the peak at 180°), yet there is a
noticeable increase in photocount readings directly in front
of the detector (i.e., 0° and 359°)–these increased counts are
manifestations of noise in part from the physical phenomena.

to a source (perhaps with more accuracy than our initial ap-
proaches). A natural limitation of this approach is the cost,
requirement of a mobile environment, and dependency on
multiple samples for triangulation. However, the fact that the
success of our approaches are agnostic to these requirements
yields a unique opportunity; much like how predictions with
the stationary system studied beneﬁted from the application
of cyber techniques, the same is likely true for the techniques
used in systems such as MURS. Broadly speaking, the over-
lap between radiation detection and detection in computing
environments suggests that a large class of approaches are
likely to beneﬁt from cyber-inspired approaches, as observed
with the system used in this work.

7 Conclusion

In this paper, we investigated new analysis approaches for
radioactive source localization. We explored how techniques
from the cyber-security detection domains can surpass tradi-
tional table-based approaches and extend the standard deﬁni-
tion of localization to include distance. We observed through
both simulated and physical laboratory experiments that our
techniques surpassed the angular accuracy of table-based ap-
proaches, reducing the angular error by 37 % and reliably
predicting distance within 2.4 %.

Moreover, we observed how naive applications of machine
learning either produced inaccurate predictions or failed to

12

050100150200250300350Angle(Degrees)100200300400500Photocounts[8] David Freedman and Persi Diaconis. On the his-
togram as a density estimator:L2 theory. Zeitschrift
für Wahrscheinlichkeitstheorie und Verwandte Gebiete,
57(4):453–476, December 1981.

[19] Sanjoy Mukhopadhyay, Richard Maurer, Ron Wolff,
Ethan Smith, Paul Guss, and Stephen Mitchell. Net-
worked Gamma Radiation Detection System for Tactical
Deployment. page 9.

[9] John T. Goorley, Michael R. James, Thomas E. Booth,
Jeffrey S. Bull, Lawrence J. Cox, Joe W. Jr. Durkee,
Jay S. Elson, Michael Lorne Fensin, Robert A. III
Forster, John S. Hendricks, H. Grady III Hughes, Rus-
sell C. Johns, Brian C. Kiedrowski, Roger L. Martz,
Stepan G. Mashnik, Gregg W. McKinney, Denise B.
Pelowitz, Richard E. Prael, Jeremy Ed Sweezy, Laurie S.
Waters, Trevor Wilcox, and Anthony J. Zukaitis. Initial
MCNP6 Release Overview - MCNP6 version 1.0. Tech-
nical Report LA-UR-13-22934, 1086758, June 2013.

[10] Matthew D. Grypp, Craig M. Marianno, John W. Pos-
ton, and Gentry C. Hearn. Design of a spreader bar
crane-mounted gamma-ray radiation detection system.
Nuclear Instruments and Methods in Physics Research
Section A: Accelerators, Spectrometers, Detectors and
Associated Equipment, 743:1 – 4, 2014.

[11] D. Hanna, L. Sagnières, P.J. Boyle, and A.M.L.
MacLeod. A directional gamma-ray detector based on
scintillator plates. Nuclear Instruments and Methods in
Physics Research Section A: Accelerators, Spectrome-
ters, Detectors and Associated Equipment, 797:13–18,
October 2015.

[12] A. Humayed, J. Lin, F. Li, and B. Luo. Cyber-Physical
Systems Security—A Survey. IEEE Internet of Things
Journal, 4(6):1802–1831, December 2017.

[13] Industrial Applications and Chemistry Section Inter-
national Atomic Energy Agency, Vienna (Austria).
Gamma irradiators for radiation processing. Techni-
cal Report 83-909690-6-8, IAEA, International Atomic
Energy Agency (IAEA), 2006. INIS-XA–862.

[14] Harold Elford Johns and John Robert Cunningham. The
physics of radiology. Charles C. Thomas, Springﬁeld,
Ill., U.S.A, 4th ed edition, 1983.

[15] Raymond T. Klann, Jason Shergur, and Gary Mattesich.
Current State of Commercial Radiation Detection Equip-
ment for Homeland Security Applications. Nuclear
Technology, 168(1):79–88, October 2009.

[16] Glenn F. Knoll. Radiation detection and measurement.

Wiley, New York, 3rd ed edition, 2000.

[17] Miroslav Kubat, Robert C Holte, and Stan Matwin. Ma-
chine Learning for the Detection of Oil Spills in Satellite
Radar Images. page 21.

[18] G R Malkoske and J Slack. COBALT-60 PRODUC-
TION IN CANDU POWER REACTORS. page 6.

[20] CONGRESS OF THE UNITED STATES CONGRES-
SIONAL BUDGET OFFICE. Scanning and Imaging
Shipping Containers Overseas: Costs and Alternatives.
Technical report, CBO, June 2016.

[21] P Ortiz, M Oresegun, and J Wheatley. Lessons from

Major Radiation Accidents. page 10.

[22] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine Learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

[23] Danijela Proti´c. Anomaly-Based Intrusion Detection:
Feature Selection and Normalization Inﬂuence to the
Machine Learning Models Accuracy. 1(3):7, 2018.

[24] J. R. Quinlan. Induction of decision trees. Machine

Learning, 1(1):81–106, March 1986.

[25] P L Reeder and D C Stromswold. Performance of Large
NaI(Tl) Gamma-Ray Detectors Over Temperature -50ºC
to +60ºC. page 46.

[26] Alan L. Remick, John L. Crapo, and Charles R.
Woodruff. U.S. NATIONAL RESPONSE ASSETS
FOR RADIOLOGICAL INCIDENTS:. Health Physics,
89(5):471–484, November 2005.

[27] Gopal B Saha. Physics and Radiobiology of Nuclear

Medicine. 2001.

[28] Chris Schrage, Nathan Schemm, Sina Balkir, Michael W.
Hoffman, and Mark Bauer. A Low-Power Directional
Gamma-Ray Sensor System for Long-Term Radiation
Monitoring. IEEE Sensors Journal, 13(7):2610–2618,
July 2013.

[29] Robin Sommer and Vern Paxson. Outside the Closed
World: On Using Machine Learning for Network Intru-
sion Detection. In 2010 IEEE Symposium on Security
and Privacy, pages 305–316, Oakland, CA, USA, 2010.
IEEE.

[30] Jungsuk Song, Hiroki Takakura, Yasuo Okabe, and
Yongjin Kwon. A Robust Feature Normalization
Scheme and an Optimized Clustering Method for
Anomaly-Based Intrusion Detection System.
In Ra-
mamohanarao Kotagiri, P. Radha Krishna, Mukesh Mo-
hania, and Ekawit Nantajeewarawat, editors, Advances

13

in Databases: Concepts, Systems and Applications, vol-
ume 4443, pages 140–151. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2007.

[31] Muhammad Tahir, Mingchu Li, Naeem Ayoub, and
Muhammad Aamir. Efﬁcacy Improvement of Anomaly
Detection by Using Intelligence Sharing Scheme. Ap-
plied Sciences, 9(3):364, January 2019.

[32] Chih-Fong Tsai, Yu-Feng Hsu, Chia-Ying Lin, and Wei-
Yang Lin. Intrusion detection by machine learning: A
review. Expert Systems with Applications, 36(10):11994–
12000, December 2009.

[33] U.S.NRC. NRC: Radiation Basics.

[34] Stuart Walker. EPA Facts about Cobalt-60. Technical

report, U.S. EPA.

[35] Raymond E. Wright. Logistic regression. In Reading
and understanding multivariate statistics., pages 217–
244. American Psychological Association, Washington,
DC, US, 1995.

[36] K. P. Ziock. The Lost Source, Varying Backgrounds
and Why Bigger May Not Be Better. In AIP Conference
Proceedings, volume 632, pages 60–70, Washington,
DC (USA), 2002. AIP. ISSN: 0094243X.

14

A Appendix

We provide a brief summary of the evaluated learning tech-
niques and show our hyperparameters used in the evaluation.
Logistic Regression. Logistic regression (LR), akin to linear
regression, computes a weighted sum of input features with
an additional bias term, and applies the logistic function to
this sum [35]. We can deﬁne a logistic regression model as
follows:

(cid:96) = (1 + e(−θ(cid:62)·x))−1

(4)

where θ represents the weights of the model and x represents
our input vector. While more sophisticated techniques have
emerged, i.e.,deep learning, each have their own limitations.
We include logistic regression models in this work to inves-
tigate if simpler models sufﬁce to perform localization tasks
accurately and quickly
Support-vector Machines. Prior to the inception of deep
learning, support-vector machines (SVMs) dominated ma-
chine learning benchmarks across many domains [3]. For
binary classiﬁcation, SVMs seek to ﬁnd two hyper-planes that
satisfy:

θ(cid:62) · x + b (cid:81) {−1, 1}

(5)

where θ represents the weights of the model, x represents
our input vector, and {−1, 1} encode the two classes. The
algorithm then seeks to maximize the difference between
the two hyperplanes, while satisifying the above constraints.
SVMs are attractive as they can form non-linear decision
boundaries, which may be necessary given the noisiness of
this domain.
k-Nearest Neighbors. The k-nearest neighbors algorithm
(kNN) is a non-parametric approach, used ubiquitously in
academia and industry. As the observed count distributions
in our domain can change rapidly in a variety of unique en-
vironments, kNN is particularly useful as it does not make
any assumptions about the underlying data. We can deﬁne
the kNN algorithm as follows (using the Euclidean metric for
distance):

(cid:113)

( ˆx − x)2

ˆy = arg min
y:(x,y)∼D

(6)

where ˆy is predicted class, (x, y) ∼ D represents the input-
class pairs for the observed distribution, and ˆx represents a
new observation at the time of inference. As a non-parametric

model, we expected kNN to exhibit adequate accuracy, amor-
tized across a variety of unique scenarios.

Decision Trees. Decision Trees (DT) are ﬂexible machine
learning algorithms that are commonly used today [24]. They
require minimal data preparation, have low performance over-
heads, and offer intuitive explanations of the formed decision
boundaries. Decision trees often follow a binary if-then-else
structure whose rules are built by minimizing:

T (i,t) =

|x| : xi ≤ t
|DT |

· G(i, ≤ t) +

|x| : xi > t
|DT |

· G(i, > t)

(7)

where i a feature, t is the threshold for i, x are input vectors
from distribution DT partitioned at node T , and G(i,t) (i.e.,
the Gini impurity score) is:

G(i, (cid:81) t) = 1 −∑

y

(cid:32)

|x| : xi (cid:81) t, (x, y) ∼ DT
|x| : xi (cid:81) t

(cid:33)2

where (x, y) ∼ DT represents the input-class pairs for the
observed distribution. Decision trees are appealing in this
domain not only as another non-parametric technique, but
also because of their white-box design: the learned decisions
are easy to interpret, which can be useful in understanding
subtle changes in the decision process as a function of the
environment.

Deep Neural Networks. Deep neural networks represent a
state-of-the-art class of learning techniques that have demon-
strated success in the most challenging machine learning
benchmarks [7]. Deﬁnitions vary, but generally speaking,
deep neural networks often refer to any class of artiﬁcial
neural networks with multiple layers between the input and
output layers. We can formalize a deep neural network (with
ReLU as the activation function) as:

P(x) = S(P(cid:96)(max(0, θ(cid:62)

(cid:96) · x + b)))

(8)

where x is an input vector, P(cid:96) is the (cid:96)th iterate of P (i.e.,
function composition) where (cid:96) is the number of layers in the
network, θ(cid:96) are the weights for the (cid:96)th layer, b is a vector of
biases, and S is deﬁned as the softmax layer. Given the compu-
tational complexity required of artiﬁcial neural networks with
many hidden layers, we interested if deep neural networks
were expressive enough to be effective in our most complex
scenarios.

15

Logistic Regression

Decision Trees

Solver: L-BFGS
Epochs: 100
1.0 L2 Regularization
Criterion: Gini
Max Depth: 10

Support Vector Machines Kernel: RBF

k-Nearest Neighbors

Deep Neural Network

Gamma: 1/(# features * variance)
1.0 L2 Regularization
Neighbors: 5
Algorithm: Ball Tree (Leaf size of 30)
Distance: Minkowski
Layers: 3 with 15 neurons (fully-connected)
Activation: Relu
Epochs: 300
Optimizer: Adam
10−4 L2 Regularization

Table 2: Model Hyperparameters

16

