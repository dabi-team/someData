1
2
0
2

g
u
A
7
2

]
I

A
.
s
c
[

1
v
6
7
2
2
1
.
8
0
1
2
:
v
i
X
r
a

IJCAI-21 1st International Workshop on Adaptive Cyber Defense

End-To-End Anomaly Detection for Identifying Malicious Cyber Behavior
through NLP-Based Log Embeddings
Andrew Golczynski and John A. Emanuello
Laboratory for Advanced Cybersecurity Research
National Security Agency

Abstract

Rule-based IDS (intrusion detection systems) are
being replaced by more robust neural IDS, which
demonstrate great potential in the ﬁeld of Cyber-
security. However, these ML approaches continue
to rely on ad-hoc feature engineering techniques,
which lack the capacity to vectorize inputs in ways
that are fully relevant to the discovery of anoma-
lous cyber activity. We propose a deep end-to-
end framework with NLP-inspired components for
identifying potentially malicious behaviors on en-
terprise computer networks. We also demonstrate
the efﬁcacy of this technique on the recently re-
leased DARPA OpTC data set.

1 Introduction

Automated IDS (intrusion detection systems) have been
of interest to researchers and cybersecurity professionals for
several decades [Denning, 1987]. Earlier generations of IDS
utilities leveraged manual rules and traditional machine learn-
ing techniques paired with ad-hoc feature engineering to re-
produce expert capabilities given limited processing power
and data availability. However, these hard-coded features and
signature-based rules failed to be generally robust (especially
when new threats are introduced), and traditional machine
learning models generally lacked the capacity to sufﬁciently
discern anomalous behaviors in large enterprise computer
networks [Verma and Bridges, 2018].
Improved IDS im-
plemented via neural-based machine learning has been long-
expected, and may have the capacity to address these issues
[Lunt, 1990], but such systems have only become feasible in
the past several years due to recent improvements in hardware
and data collection. Autoencoder-based anomaly detection
has been applied in this space with some success, enabling
the discovery of malicious activity under the assumption that
such activity will differ markedly from the ordinary users of
a network or other system [Hashemi and Keller, 2020].

With such modern approaches coming to bear, we can now
expect the development of a more-capable family of neural
IDS. However, there remain several issues in the design and
implementation of these systems; in particular, autoencoder-

based IDS typically still rely on the frail ad-hoc feature1 en-
gineering used by traditional IDS. To truly bring these neural
systems to fruition, a more automated and robust approach to
record vectorization must be developed.

To that end, this paper details our work, which was in-
spired by natural language processing (NLP) techniques, to
provide autonomous feature generation of network and host
log metadata vectorization. By treating logs’ entry ﬁelds
as tokens in a text corpus, we seek to remove the assump-
tions made by traditional feature engineering approaches, in-
stead allowing our utility to learn vector representations of
the in our log ﬁles, leading to natural embeddings of records
describing system activities in a manner relevant to the de-
scription and discovery of anomalous behavior in a network
(e.g. which IPs commonly commonly communicate with
each other and which dlls are commonly loaded for partic-
ular applications). Furthermore, we train these embeddings
in-line with our anomaly detection utility, forming an end-to-
end architecture with embeddings that are speciﬁcally trained
to improve performance on our downstream anomaly detec-
tor, ensuring relevance and performance.

leveraged in this paper,

In this work, we will provide a brief overview of
the DARPA Operationally Transparent Computing (OpTC)
including discussing
dataset
some of the relevant aspects of data labeling and pre-
processing/engineering of the data. We will then discuss our
architectural motivations and choices, along with implemen-
tation, training and testing details for our proof-of-concept
utility. The results of this work will then be presented and
discussed, followed by concluding remarks.

1.1 Background and Related Work

A typical traditional approach to log-based intrusion detec-
tion is provided by Verma and Bridges [Verma and Bridges,
2018] – the authors develop a metric space for describing
Windows host logs, but their methodology relies on ad-hoc
features and metrics rather than a learned/neural approach.
Other authors, such as Chen et al., have worked to create more
automated feature engineering using NLP-based vectoriza-
tion, but their intersection with NLP also relies on non-neural

1We differentiate ﬁelds, which are found in raw log data, and
features, which refer to the output of some sort of process and which
give a meaningful representation of the raw data.

 
 
 
 
 
 
traditional approaches, namely term frequency inverse doc-
ument frequency (TF-IDF) vectorizations of log ﬁles [Chen
et al., 2019], which lack the ability to dynamically learn ﬁt-
for-purpose representations of data. Di Mauro et al recently
provided a survey of neural approaches to intrusion detection,
showing the potential of neural techniques in this space, but
none of the referenced works looks to modern word embed-
dings as a solution to our noted feature engineering problem
[Mauro et al., 2020].

Statistical anomaly detection approaches, such as those
in Heard & Rubin-Delanchy [Heard and Rubin-Delanchy,
2016], and Whitehouse et al. [Whitehouse et al., 2016] do
show promise, but do not appeal to neural network based ap-
proaches such as Nguyen et al. [Nguyen et al., 2019], which
have a wider capacity to learn more complex representations
of baseline cyber behavior.

Here, the idea is to train the neural network architecture,
such as an autoencoder (AE). AEs are trained in an unsuper-
vised fashion, wherein the target function to be learned is an
approximate identity f : Rn → Rn on the input space[Good-
fellow et al., 2016]. These networks are usually decom-
posed into non-linear encoder h : Rn → Rm and decoder
g : Rm → Rn functions such that m << n. By construc-
tion, AEs learn salient information about the training data and
reconstruction will fail on outliers or data drawn from distri-
butions vastly different from that of the training set, making
them ideal for anomaly detection (using (L2) reconstruction
error (cid:107)x − f (x)(cid:107)2 as an anomaly score)[Cozzolino and Ver-
doliva, 2016; Liao et al., 2018].

Our work borrows from the modern family of neural word
embedding techniques, developed by researchers such as
[Bengio et al., 2006] and Mikolov et al.
Bengio et al.
[Mikolov et al., 2013a; Mikolov et al., 2013b] in their devel-
opment of the word2vec-style utilities and their related ex-
tensions to embed words in a vector space in which semantic
relationships between words are encoded as geometric prop-
erties in the embedded space. While their original intention
was strictly rooted in text analysis, other works have shown
that the word2vec style of embedding can be applied to other
domains to produce coherent and useful embeddings – for
[Perozzi et al., 2014], wherein
instance, see Perozzi et al.
the authors leverage word2vec as a method for embedding
graph representations, as well as Liu et al., wherein a simi-
lar vectorization approach (log2vec) is utilized to embed log
entries as nodes of a graph [Liu et al., 2019]. An appli-
cation of this approach can be found in Ring et al., which
leverages word2vec-style embeddings to embed the space of
IP addresses for network trafﬁc analysis [Ring et al., 2017].
Similarly, Ramstr¨om et al. and Wong et al. use a word2vec-
inspired vectorization of netﬂow features as input for an AE-
based anomaly detection scheme [Ramstr¨om, 2019; Wong
and Emanuello, 2021]. The latter work also demonstrated
that the AEs were quite robust to the presence of data corre-
sponding to cyber attacks in the training data, which is likely
due to the AE’s ability to ignore characteristics of rare events.

2 Data

The availability of quality benchmark datasets concern-
ing malicious cyber activity is a perennial challenge for re-
searchers in this space – cyber activity is generally complex
and multifaceted, meaning that any one dataset can only de-
scribe a small portion of malicious cyber behavior. Addition-
ally, labeling such data sets can be a challenge, as combing
through millions of logs to determine which logs are actu-
ally associated with legitimate malicious activity is a daunting
task at the best of times, leading to labeling that can be incom-
plete or incorrect. Some datasets [Moustafa and Slay, 2015]
rely on synthetic data to provide greater variety and accuracy,
but these may fail to capture the nuances of both normal user
activity and actual attack patterns, and may overly-simplify
downstream machine learning challenges.

For this work, we relied on the Operationally Transpar-
ent Cyber Dataset created by Defense Advanced Research
Projects Agency (DARPA) [DARPA, 2019], which was de-
signed to help address some of the above issues. These data
consist of network and host logging collected from a network
of hundreds of Windows hosts over a one-week period, rep-
resenting normal (benign) user activity. Over a three-day pe-
riod within this week, a set of red team actors also worked to
perform various penetration tests against the network, seek-
ing to inﬁltrate the network, ensure persistence, and carry out
increasingly-complex attacks over time. The data we sought
to use consisted of the following:

1. Combined network and host logs in the ECAR (Ex-
tended Cyber Analytic Repository) format, aggregated
by host and time.

2. An description of the attacks carried out by the red team
actors, with timestamps, Windows process IDs, host-
names, and general notes.

With over 17B events (from approximately 500 hosts) cor-
responding to detailed, security-oriented network and host
logging, along with detailed descriptions of the red team ac-
tivities on the network, the open source OpTC dataset pro-
vides: (1) a more realistic scenario (multi-staged redteam at-
tacks) on which to test our cyber activity detection task; and
(2) a means for others to validate our results and to compare
our technique with theirs2.

2.1 Data Format

The ECAR data format is an extension of Mitre’s CAR
format, which consists of nested JSON dictionaries, with
each record primarily describing an object (e.g. a process,
thread, registry key, etc.) and the action taken on that ob-
ject (e.g.
starting a process, modifying a registry value,
etc.). Additional object/action-speciﬁc properties are popu-
lated on the record to provide necessary context as well (e.g.
a record pertaining to the editing of a registry value would
have an optional property describing the new value being set)
[MITRE, 2020]. The ECAR format simply adds additional
object/action pairs that provide a greater level of detail than

2See Anjum et. al for detailed analysis of the data and a discus-
sion on its utility for ML-based cybersecurity research activities[An-
jum et al., 2021].

the original CAR [DARPA, 2019]. Overall, the ECAR data
contains 58 unique keys in its nested structure, with each
record only populating a minority of those ﬁelds.

The OpTC ECAR dataset consists of roughly 10 terabytes
of data uncompressed, with tens of millions of records per
host across the collection period [Anjum et al., 2021]. This
provides fertile ground for our modeling approaches, while
still having a relatively-constrained vocabulary designed for
unambiguous logging rather than expressive natural
lan-
guage.

2.2 Train/Test Split and Labeling

Fundamentally, our goal is to train an anomaly detector on
baseline activity that can then be applied to later activity to
detect abnormal behavior. To mirror this need, we used the
days of benign activity as our training data set, and reserved
the days containing both benign and red team activity for test-
ing, with the goal of detecting the red team activity in our test
set.

However, the records in the ECAR data are not inherently
labeled with benign/malicious tags. To address this, we de-
veloped labels for the data using the description of the mali-
cious activities provided by the red team actors on their active
days. For our purposes, an ECAR record is considered to be
malicious if it is one of the malicious agent processes noted
by the red team, or was spawned by a malicious process. Ad-
ditionally, the data was searched for any miscellaneous ma-
licious network activity that was not directly associated with
the noted processes – while the majority of the records we la-
beled as ”malicious” were discovered via the former standard,
a small minority were caught through the latter (speciﬁcally,
activity associated with ping sweeps performed on the net-
work were problematic, given that the processes performing
such actions were associated with system utilities, rather than
being spawned directly by the malicious agents).

This labeling approach, it should be noted, casts a broad
net – all logs associated with the clearly-malicious activity
are discovered by this approach, but generic activity that is in-
cidentally associated with the malign activity is also caught.
While this produces somewhat noisy labels, our main con-
cern is not to achieve 100 percent accurate classiﬁcation on
a record-by-record basis, but rather to catch the overarching
attacks as described by activity across multiple logs with a
high degree of precision – primarily, we aim to provide the
SOC admin strong evidence of behavior indicative of an at-
tack, rather than providing them with all activities that may be
associated with an attack. Given this goal, the incidentally-
malicious logs being labeled as malign is of minimal concern
to us.

3 Data Pre-processing

For this work, we created a vocabulary consisting of terms
that were tokenized and/or pre-processed versions of ﬁeld
values3 from the ECAR records. Our pre-processing philos-

3In keeping with terminology associated dictionary data types
such as JSON, we will use the terms key (often inter-changeably
with ﬁeld) and the precise instances these ﬁelds may take on as in
logs as values.

ophy was to have as light as touch as possible, which should
allow us to avoid the pitfalls associated with the largely ad-
hoc feature engineering approaches that are standard in IDS.
A proof of concept that only alters its input minimally should
show that the text processing techniques are responsible for
the bulk of the learning, rather than the guilty knowledge of
the researchers being the driving factor behind our efﬁcacy.
In total, our pre-processing consisted of the following steps:

1. Dropping unnecessary features

Of the ﬁfty-eight keys in the ECAR ﬁle, only twenty-
seven were kept for processing. The remaining ﬁelds
were determined to be insufﬁciently relevant to our cho-
sen task/approach, and were dropped (see Appendix A.
for the complete list of keys that we used.)

2. Feature preﬁxing

Some terms may have speciﬁc meanings depending on
which feature with which they are associated. For ex-
ample, the number 443 has a speciﬁc meaning in the
dest port ﬁeld, but that meaning would not be preserved
in other ﬁelds. To ensure that these meanings are re-
spected, values of select features were preﬁxed with
their feature name (e.g.
”443” would be mapped to
the term ”PORT 443” when it is found in the dest port
ﬁeld).

3. Connection time bucketing

Network activity records contain start and end times for
the connection; to discretize these values, connection
durations were calculated and bucketed into SMALL,
MEDIUM, and LARGE buckets based on manual in-
spection of the distribution of connection durations.

4. Path/ﬁle name extraction

File paths and registry keys can contain machine spe-
ciﬁc sets of directories, even when considering a com-
mon item (e.g. ”xyz.dll” might be found in separate di-
rectory locations on separate machines, even though the
underlying ﬁle is the same). To correct for this, such
paths are reduced to only the ﬁle name. Similarly, when
parsing command line input, only the name of the exe-
cuted program is maintained (i.e. the full path and any
arguments are dropped).

5. /24 CIDR subnet extraction

IPv4 addresses were, in many cases, too sparse across
the dataset to be well-represented, so we opted to re-
move the last octet from each address and only use the
/24 subnet. While this did reduce the resolution of the
representation of our addresses, we gained additional ro-
bustness by aggregating rare terms into a smaller number
of representations.

6. Emphemeral port aggregation

Ports used for outgoing network trafﬁc are typically ar-
bitrarily chosen from the high end (greater than 49151)
of the range of valid ports. These choices carry no
real meaning, and needlessly expand the vocabulary
space, and so such ports were replaced with a generic
”EPHEMERAL PORT” token.

7. Removal of rare terms

After performing the above tokenization, terms that ap-
peared fewer than ten times (determined by examination
of the distribution of term frequencies) over our train-
ing corpus were discarded and replaced with a generic
”OBSCURE TERM” token.

8. Conversion to vocabulary indices

All remaining vocabulary tokens were indexed, and each
token was replaced with its index. Empty ﬁelds (e.g. ﬁle
path ﬁelds are empty for network trafﬁc events) were
replaced with a ”NULL TERM” token.

Given the sheer volume and variability of our data, we ad-
ditionally elected to focus proposed deep learning training
regimen on each of four hosts (201-204) in the dataset for a 3-
day period directly preceded the red team’s efforts. Similarly,
for each of the four models we limited our test data to the
same host used to train the model, and used records from the
full three days of the red team’s work, during which hosts 201
and 203 were attacked at some point, while 202 and 204 never
appear to have been targeted by the red team. This collection
of pre-processing steps left us with a training set of roughly
15 million records per host, comprised of 27 ﬁelds, and a vo-
cabulary space of roughly 6000 terms per host. Our test set
(similarly pre-processed) was roughly 9 million records per
host, with roughly 37,000 and 19,000 of those being labeled
as malicious on hosts 201 and 203, respectively.

4 End-to-End Record Vectorization and

Anomaly Detection Model

To reiterate, the goal of this work is to build an anomaly
detection scheme capable of discovering novel cyberthreat ac-
tivity, that would otherwise be missed by traditional IDS. Our
NLP-inspired approach to anomaly detection requires us to
embed our pre-processed records (27-dimensional vectors of
vocabulary indices, with each dimension drawn from a rele-
vant ﬁeld in the ECAR records) into a semantically-relevant
vector space, which is the input to our autoencoder-based
anomaly detector at inference. To that end, we sought to ﬁrst
vectorize the elements of our vocabulary, which would then
allow us to construct relevant vector representations of the
full documents composed of those elements.

4.1 Model Architecture and Loss

Instead of relying on embeddings trained on an unsuper-
vised word2vec task, our architecture trains the word vec-
tors inline with our autoencoder, allowing the relevant down-
stream task to exert pressure on the embeddings instead, pro-
ducing embeddings speciﬁcally-tuned for use in the anomaly
detection task. Our ultimate model architecture passes all
words (one-hot encodings, here represented by sparse indices
yi) in the record through a common embedder, and generates
a record representation (V) by concatenating those word vec-
tors. These record vectors are then passed through a single-
layer autoencoder (our downstream task), which attempts to
compress and decompress our record vector, creating an ap-
proximation of our encoded input ( ˆV). Loss on this autoen-

Figure 1: Model architecture, with general data ﬂow

coding task is calculated using the square of the L2 norm of
the difference between our input and reconstructed vectors.

A dense extractor layer is then applied to the autoencoder’s
output, which attempts to predict the original input word-by-
word as probability distributions (ˆyi) over the training vocab-
ulary, with our loss being given as the total cross entropy
across each term in the record. Additionally, we found that
adding a loss term of the square of the L2 norm of the dif-
ference between the autoencoder’s input and output helped
to stabilize the training process and ensure consistent internal
representations of the records.

To ensure that both losses operated at a similar scale, a
scaling parameter α (typically on the order of α = 5) was
used to weight the output of the internal autoencoder’s re-
construction loss. Additionally, the cross entropy loss of the
outer encoder was weighted by the log of the prevalences of
the correct words in the training corpus (we refer to the log
of the prevalence of a given term as wlabel). Given that the
corpus is very sparse, with a majority of the terms being the
NULL token, this weighting encourages the word embedding
to avoid overvaluing such common terms. The overall loss
for a given record is summarized as follows:

Loss = ((cid:80)27

i=1 wyi ∗ CEL(ˆyi, yi)) + α ∗

(cid:13)
(cid:13)
(cid:13)V − (cid:98)V

(cid:13)
2
(cid:13)
(cid:13)
2

Where the CEL between yi and ˆyi is calculated as the cross
entropy loss between the one-hot encoding associated with
the sparse index yi and the predicted distribution ˆyi.

4.2 Model Training

Our model was trained on a single AWS EC2 g3xlarge in-
stance, using Keras/Tensorﬂow 2.4.1. A batch size of 32 was
found to be optimal – larger batches were tested, but the train-
ing process generally failed to produce workable results.

Batches were generated by randomly sampling from the
training corpus, with each record being weighted by the log

of the prevalence of its rarest term. This encourages the train-
ing process to sample records with rarer terms to train on, en-
suring that all terms in our vocabulary are given meaningful
embeddings, regardless of prevalence.

Using the Adam optimizer, training was allowed to pro-
ceed for at most 200,000 batches, though 50,000 batches was
found to produce workable results. Training required roughly
an hour using this architecture and setup, but loss stabilized
relatively early (within the ﬁrst 30,000 batches).

5 Testing, Evaluation, and Results

This area of research is unfortunately lacking in baseline
techniques and measurements for performance (a problem
that is especially exaggerated by our use of a new data set
with few previous related works). We therefore seek to pro-
vide a general baseline for the efﬁcacy of our method, and to
contextualize our results in the light of potential practical ap-
plications of our work. Our testing methodology is inspired
by the ideal end use case for our prototype – namely, we fore-
see these types of utilities being used to alert administrators at
times when network and host logs are exhibiting anomalous
behavior, and to provide prioritized lists of anomalies for in-
vestigation/action in a SOC/administrative setting. Because
of these use cases, overall accuracy is not a useful measure
for our performance, as we have a large class imbalance (less
than 1 percent of our test data is labeled as anomalous), and as
was previously stated, our goal is not necessarily to correctly
classify all of our malicious trafﬁc as anomalous, as some of
that trafﬁc may be only incidentally labeled as malicious due
to its association with a redteam process. Instead, we choose
to provide two main results that address the above use cases:
a qualitative view of changes in the network’s cross-entropy
loss over time, and measurements of precision using various
error levels as classiﬁcation thresholds. We feel that these re-
sults best promote our goal of allowing administrative users
to know when an attack is taking place, while also providing
a manageable number of true positives, with a high degree of
certainty (i.e. high precision).

The ﬁrst of these use cases (our temporal alerting system)
is seen in ﬁgures 2 and 3. In a SOC, an administrator may ﬁnd
it useful to be able to determine when a given host’s activity
has changed from benign to anomalous, as this may indicate
the onset of an attack. To achieve this, we gather our test data
into 60-minute buckets, calculate the percentage of records in
each bucket over the 99.9th percentile of the associated train-
ing data’s error distribution, and display the resulting time se-
ries in ﬁgure. To show the correlation between this bucketed
reconstruction error metric and the actual red team activity
on the box, we additionally plot a histogram of the amount
of red team activity in each of the sixty-minute buckets. Con-
sider the ﬁgure for host 201: note that the reconstruction error
is relatively stable and low when there is no red team activity
on the box, indicating that the embeddings and autoencoder
are able to generalize to unseen new data from the same host.
Once the red team begins working, we see that the percentage
of high-loss records clearly spikes upwards, indicating some
degree of anomalous activity on the host. Once the red team
pivots to a new host, we see that the loss returns to its pre-

(a) Host 0201

(b) Host 0203

Figure 2: A comparison of the amount of red team activity in a given
60-minute bucket, and the amount of activity over the 99.9th per-
centile of the error distribution for the training data on the associated
host. The dashed gray line is set at the average expected level of the
graph - 0.1 percent. This ﬁgure includes only the attacked hosts.

vious normal levels. In an operationalized tool, this would
enable alerting of network defenders, prompting further in-
vestigation into the causes of the spike.

To remark on one item of note: the spikes in our metrics at
t=1500 and t=2880 are correlated with large (on the order of
a 10-fold increase) spikes in the volume of WMI (Windows
Management Instrumentation) activity on each host. This is
anomalous in and of itself, and it is true that the red team
made broad use of WMI between these two spikes to pivot
throughout the network, but it is unclear if this activity can
be explicitly tied to the red team actors due to the coarse-
grain nature of the red team’s ground truth documentation. As
such, we have elected to treat these spikes as non-malicious
anomalies.

Our second metric provides a more quantitative view of
our results by indicating the precision of classifying the top
q percent (in terms of cross entropy loss) of the data set as
anomalous. This evaluation is performed as follows:

1. Pre-process and vectorize test data by embedding the as-
sociated terms and concatenating those embeddings

2. Pass those test vectors through our trained network, and

calculate the cross-entropy loss for each record

3. For each quantile, q, between the 95th and 100th per-
centile of our test loss distribution, classify every record
with a loss above q as being anomalous.

(a) Host 0202

(a) Host 0201

(b) Host 0204

Figure 3: A comparison of the amount of red team activity in a given
60-minute bucket, and the amount of activity over the 99.9th per-
centile of the error distribution for the training data on the associated
host. The dashed gray line is set at the average expected level of the
graph - 0.1 percent. This ﬁgure includes only the unaffected hosts,
so no malicious activity is expected.

4. For each threshold, q, calculate the precision of our clas-
siﬁcation. Recall is additionally calculated for complete-
ness.

Figure 4 shows the classiﬁcation precision by quantile
threshold for each of the attacked hosts– by setting a thresh-
old at the 99.9th percentile of our training reconstruction error
distributions, we can see that our classiﬁcation system pro-
vides at least 66 percent precision; the host that saw the most
involved attack (that is, host 201 - host 203 simply exhibited
beaconing behavior), we saw 80 percent precision. In a realis-
tic tool built on such work, this would ensure that examining
a ranked list of anomalies discovered by the system would
quickly yield obvious true anomalies. For additional context,
this classiﬁcation method would have had a 0.082 percent and
0.096 percent false discovery rate on the unattacked hosts 202
and 204, respectively, which is in-line with the 99.9 percentile
threshold, indicating that the error distributions are consis-
tent across the training and test sets, and that ordinary activity
should not be overly-given to produce false positives.

To provide a better view of the range and distribution of the
reconstruction errors, we additionally provide a histogram of
the errors (ﬁgures 5 and 6), segregated by class. This shows
a clear separation between a large portion of the malicious
activity, and the majority of the remaining activity.

Finally, to provide a more qualitative context for our re-
sults, we additionally provide a visualization of the word vec-

(b) Host 0203

Figure 4: Test precision and recall by quantile threshold. The verti-
cal dotted line marks the 99.9th percentile of the error distribution
of the associated host’s benign training data. Note that 80 and 66
percent precision with 66 and 86 percent recall can be achieved at
these thresholds for hosts 201 and 203, respectively.

tor embeddings learned by the system for host 201 (ﬁgure 7).
This is done by extracting the embeddings from the model,
reducing their dimensionality using T-SNE, and labeling the
emergent structure discovered within the visualization.

Figure 7 shows that there is clear role-based clustering of
terms, both network-related (e.g. IP addresses and ports) and
host-related (executables, logs, registry information, etc.).
The large gray cluster in 7 largely contains embeddings of
terms that refer to ﬁles and resources being manipulated by
the system in vairous ways. For instance, DLL modules
loaded by the system, Microsoft Ofﬁce ﬁles modiﬁed by the
Ofﬁce suite’s software, and .tmp ﬁles can be found in the clus-
ter. These ﬁle types, among others, combine into subclusters,
which we show in ﬁgure 8. Because ﬁles can be manipulated
in many ways (e.g. DLL ﬁles can be loaded as modules, or
they can be moved to a new directory), the clustering is less
clear than the independent clusters noted in 7, but the role-
based groupings are still clear. Our training process’s gener-
ation of these semantically-relevant embeddings shows that

our autoencoding process is learning relevant patterns for the
compression and reconstruction of our records, meaning that
the measures discussed above (precision, etc.) are relevant to
our end goal.

6 Future Work

This work has shown that there is some promise in this
general end-to-end approach to anomaly detection, but fur-
ther investigation of several key issues is still needed.

1. Vocabulary improvement: Generally, we want to remove
as much ad-hoc feature engineering as possible from our
process to ensure that we do not require or depend on
strict assumptions or models. However, our current ap-
proach requires a higher-than-necessary amount of vo-
cabulary engineering (e.g.
rare term thresholding and
combination, term discretization, ﬁle path simpliﬁcation
etc.). Working to remove these ad-hoc aspects of our
feature engineering regimen is a high priority.

2. Network architecture improvement: our current archi-
tecture is suited for its role as a proof-of-concept, but it
is hardly state of the art – future work should work to
improve our design by improving the following:

• Increasing the depth/capacity of our autoencoder as

appropriate to capture broader structure.

• Encouraging the development of structure in the la-
tent space of our autoencoder (e.g. clustering, topo-

logical structure, etc.) to strengthen the meanful-
ness of the latent space (and therefore, the mean-
infulness of the conclusions drawn on the basis of
that space).

• Exploring alternate methods for record embedding
beyond simple concatenation, such as doc2vec-
style record embeddings. Additionally, aggregating
records across time by process ID or user should be
explored as a method for contextualized, behavior-
based analysis. Appropriately, the application of
recurrent, convolutional, and attentional networks
should be explored in this space.

3. Online training: Our current approach does not account
for the streaming nature of our data source, and so could
not truly be utilized as an effective operational tool. Fu-
ture work should explore methods for timely training to
prevent model staleness, as well as methods for account-
ing for the appearance of new vocabulary and phenom-
ena over time.

4. Model explainability: Our work does not focus on ex-
plainability, but given its intended nature as a portion of
a human/machine teaming utility, explainability should
be developed with some urgency. Analysis of the con-
tributing components of the autoencoder’s reconstruc-
tion error could provide an intuition as to why a given
record is considered to be anomalous. Alternately, ex-
amining the components of the end-to-end cross entropy
loss in the model could yield similar outcomes.

(a) Host 201

(a) Host 202

(b) Host 203

Figure 5: A histogram of reconstruction errors on the attacked hosts,
segregated by class: benign and malicious – note that the two popu-
lations have been normalized to keep them on the same scale in each
case.

(b) Host 204

Figure 6: A histogram of reconstruction errors on the benign hosts –
note that the distribution has been normalized.

network and host log data. Red team activity in our test set
was clearly detected, with the raw reconstruction error distri-
butions and temporal summarizations providing obvious indi-
cations of anomalous activity. Our classiﬁcation method ex-
hibits the desired degree of precision, showing the potential
for practical implementations based on this work. Addition-
ally, the embeddings trained through this end-to-end process
show clear signs of semantically-relevant structure, indicat-
ing that our architecture and training process is capable of
learning coherent descriptions of a given host’s activity.

A List of ECAR Keys Used in Experiments

1. action

2. command line

3. dest ip

4. dest port

5. direction

6. ﬁle path

7. image path

8. info class

9. key

10. l4protocol

11. logon id

12. module path

13. new path

14. object

15. parent image path

16. path

17. principal

18. requesting domain

19. requesting logon id

20. requesting user

21. sid

22. task name

23. type

24. user

25. user name

26. value

27. duration (calculated from ﬂow start and end times)

References
[Anjum et al., 2021] Md. Monowar Anjum, Shahrear Iqbal,
and Benoit Hamelin. Analyzing the usefulness of the darpa
optc dataset in cyber threat detection research. Proceed-
ings of the 26th ACM Symposium on Access Control Mod-
els and Technologies, Jun 2021.

Figure 7: A TSNE visualization of the word embeddings learned by
the model. Labels are provided for relevant categories of words.

Figure 8: A breakdown of the large cluster in our TSNE visualiza-
tion – note the clustering of of common types of ﬁles, including
Microsoft Ofﬁce suite ﬁles, DLLs, .tmp ﬁles created by the system,
and .mum/.cat ﬁles associated with the Microsoft update process.

7 Conclusion

In this work, we demonstrated an end-to-end autoencoder
model that leverages techniques borrowed from natural lan-
guage processing to detect anomalies in a broad collection of

[Mikolov et al., 2013a] Tomas Mikolov, Kai Chen, Greg S.
Corrado, and Jeffrey Dean. Efﬁcient Estimation of Word
Representations in Vector Space, 2013.

[Mikolov et al., 2013b] Tom´as Mikolov, Ilya Sutskever, Kai
Chen, Greg Corrado, and Jeffrey Dean. Distributed Rep-
resentations of Words and Phrases and their Composition-
ality. CoRR, abs/1310.4546, 2013.

[MITRE, 2020] MITRE. Data Model — MITRE Cyber An-

alytics Repository, 2020.

[Moustafa and Slay, 2015] N. Moustafa and J. Slay. Unsw-
nb15: A Comprehensive Data Set for Network Intrusion
Detection Systems (unsw-nb15 Network Data Set).
In
2015 Military Communications and Information Systems
Conference (MilCIS), pages 1–6, 2015.

[Nguyen et al., 2019] Q. P. Nguyen, K. W. Lim, D. M. Di-
vakaran, K. H. Low, and M. C. Chan. GEE: A Gradient-
based Explainable Variational Autoencoder for Network
Anomaly Detection. In 2019 IEEE Conference on Com-
munications and Network Security (CNS), pages 91–99,
2019.

[Perozzi et al., 2014] Bryan Perozzi, Rami Al-Rfou, and
Steven Skiena. Deepwalk: Online Learning of Social Rep-
resentations. In Proceedings of the 20th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and Data
Mining, pages 701–710, 2014.

[Ramstr¨om, 2019] Kasper Ramstr¨om. Botnet Detection on
Flow Data using the Reconstruction Error from Autoen-
coders Trained on Word2vec Network Embeddings. PhD
thesis, Uppsala Universitet, 2019.

[Ring et al., 2017] M. Ring, A. Dallmann, D. Landes, and
A. Hotho. IP2Vec: Learning Similarities Between IP Ad-
dresses. In 2017 IEEE International Conference on Data
Mining Workshops (ICDMW), pages 657–666, 2017.
[Verma and Bridges, 2018] M. E. Verma and R. A. Bridges.
Deﬁning a Metric Space of Host Logs and Operational
Use Cases. In 2018 IEEE International Conference on Big
Data (Big Data), pages 5068–5077, 2018.

[Whitehouse et al., 2016] M. Whitehouse, M. Evangelou,
and N. M. Adams. Activity-based temporal anomaly de-
tection in enterprise-cyber security. In 2016 IEEE Confer-
ence on Intelligence and Security Informatics (ISI), pages
248–250, 2016.

[Wong and Emanuello, 2021] V. Wong and J. Emanuello.
Robustness of ML-Enhanced IDS to Stealthy Adversaries.
In AI/ML for Cybersecurity: Challenges, Solutions, and
Novel Ideas at SIAM Data Mining 2021, 2021.

[Bengio et al., 2006] Yoshua Bengio, Holger Schwenk,
Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc
Gauvain. Neural Probabilistic Language Models, pages
137–186. Springer Berlin Heidelberg, Berlin, Heidelberg,
2006.

[Chen et al., 2019] Qian Chen, Sheikh Rabiul Islam, Henry
Haswell, and Robert A. Bridges. Automated Ransomware
Behavior Analysis: Pattern Extraction and Early Detec-
tion. In Feng Liu, Jia Xu, Shouhuai Xu, and Moti Yung,
editors, Science of Cyber Security, pages 199–214, Cham,
2019. Springer International Publishing.

[Cozzolino and Verdoliva, 2016] D. Cozzolino and L. Ver-
doliva.
Single-image Splicing Localization through
Autoencoder-based Anomaly Detection. In 2016 IEEE In-
ternational Workshop on Information Forensics and Secu-
rity (WIFS), pages 1–6, 2016.

[DARPA, 2019] DARPA. Operationally Transparent Cyber

(OpTC) Data Release, 2019.

[Denning, 1987] D. E. Denning. An Intrusion-Detection
Model. IEEE Transactions on Software Engineering, SE-
13(2):222–232, 1987.

[Goodfellow et al., 2016] Ian Goodfellow, Yoshua Bengio,
and Aaron Courville. Deep Learning. The MIT Press,
2016.

[Hashemi and Keller, 2020] Mohammad J. Hashemi and
Eric Keller. Enhancing Robustness Against Adversarial
Examples in Network Intrusion Detection Systems, 2020.
[Heard and Rubin-Delanchy, 2016] N. Heard and P. Rubin-
Delanchy. Network-wide Anomaly Detection via the
In 2016 IEEE Conference on Intel-
Dirichlet Process.
ligence and Security Informatics (ISI), pages 220–224,
2016.

[Liao et al., 2018] D. Liao, C. Chen, W. Tsai, H. Chen,
Y. Wu, and S. Chang. Anomaly Detection for Semiconduc-
tor Tools Using Stacked Autoencoder Learning. In 2018
International Symposium on Semiconductor Manufactur-
ing (ISSM), pages 1–4, 2018.

[Liu et al., 2019] Fucheng Liu, Yu Wen, Dongxue Zhang,
Xihe Jiang, Xinyu Xing, and Dan Meng. Log2vec: A
Heterogeneous Graph Embedding Based Approach for
In Proceed-
Detecting Cyber Threats within Enterprise.
ings of the 2019 ACM SIGSAC Conference on Computer
and Communications Security, CCS ’19, page 1777–1794,
New York, NY, USA, 2019. Association for Computing
Machinery.

[Lunt, 1990] Teresa F. Lunt. Detecting Intruders in Com-
puter Systems. In Proceedings, Sixth Annual Symposium
and Technical Displays on Physical and Electronic Secu-
rity, July 30 - August 2 1990.

[Mauro et al., 2020] M. D. Mauro, G. Galatro, and A. Li-
otta. Experimental Review of Neural-Based Approaches
IEEE Transactions
for Network Intrusion Management.
on Network and Service Management, 17(4):2480–2495,
2020.

