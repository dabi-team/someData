8
1
0
2

g
u
A
7
1

]

R
C
.
s
c
[

1
v
0
7
7
5
0
.
8
0
8
1
:
v
i
X
r
a

Reinforcement Learning for Autonomous Defence in Software-Deﬁned
Networking

Yi Han1[0000−0001−6530−4564], Benjamin I.P. Rubinstein1[0000−0002−2947−6980], Tamas Abraham2[0000−0003−2466−7646],
Tansu Alpcan1[0000−0002−7434−3239], Olivier De Vel2, Sarah Erfani1[0000−0003−0885−0643], David Hubczenko2,
Christopher Leckie1[0000−0002−4388−0517], and Paul Montague2

1 School of Computing and Information Systems, The University of Melbourne
2 Defence Science and Technology Group
{yi.han, benjamin.rubinstein, tansu.alpcan, sarah.erfani, caleckie}@unimelb.edu.au
{tamas.abraham, olivier.devel, david.hubczenko, paul.montague}@dst.defence.gov.au

Abstract. Despite the successful application of machine learning (ML) in a wide range of domains, adaptability—
the very property that makes machine learning desirable—can be exploited by adversaries to contaminate training
and evade classiﬁcation. In this paper, we investigate the feasibility of applying a speciﬁc class of machine learning
algorithms, namely, reinforcement learning (RL) algorithms, for autonomous cyber defence in software-deﬁned
networking (SDN). In particular, we focus on how an RL agent reacts towards different forms of causative attacks
that poison its training process, including indiscriminate and targeted, white-box and black-box attacks. In addition,
we also study the impact of the attack timing, and explore potential countermeasures such as adversarial training.

Keywords: Adversarial reinforcement learning, Software-Deﬁned Networking, cyber security, adversarial training.

1

Introduction

Machine learning has enjoyed substantial impact on a wide range of applications, from cyber-security (e.g., network
security operations, malware analysis) to autonomous systems (e.g., decision-making and control systems, computer
vision). Despite the many successes, the very property that makes machine learning desirable—adaptability—is a
vulnerability to be exploited by an economic competitor or state-sponsored attacker. Attackers who are aware of the
ML techniques being deployed can contaminate the training data to manipulate a learned ML classiﬁer in order to
evade subsequent classiﬁcation, or can manipulate the metadata upon which the ML algorithms make their decisions
and exploit identiﬁed weaknesses in these algorithm—so called Adversarial Machine Learning [13, 37, 18].

This paper focuses on a speciﬁc class of ML algorithms, namely, reinforcement learning (RL) algorithms, and
investigates the feasibility of applying RL for autonomous defence in computer networks [14], i.e., the ability to “ﬁght
through” a contested environment—in particular adversarial machine learning attacks—and ensure critical services
(e.g., email servers, ﬁle servers, etc.) are preserved to the fullest extent possible.

For example, consider a network as shown in Fig. 1 that consists of 32 nodes, one (node 3.8) of whom connects to
the critical server, two (nodes 3.9 and 4.5) connect to potential migration destinations, and three (nodes 1.5, 2.7 and
3.6) connect to the attacker’s hosts. The attacker aims to propagate through the network, and compromise the critical
server. We aim to prevent this and preserve as many nodes as possible through the following RL approach:

– We ﬁrst train two different types of RL agents: Double Deep Q-Networks (DDQN) [34] and Asynchronous Ad-
vantage Actor-Critic (A3C) [53]. The agents observe network states, and select actions such as “isolate”, “patch”,
“reconnect”, and “migrate”. The agents gradually optimise their actions for different network states, based on
the received rewards for maintaining critical services, costs incurred when shutting down non-critical services or
migrating critical services.

– Once a working agent is obtained, we then investigate different ways by which the attacker may poison the training
process of the RL agent. For example, the attacker can falsify part of the reward signals, or manipulate the states
of certain nodes, in order to trick the agent to take non-optimal actions, resulting in either the critical server
being compromised, or signiﬁcantly fewer nodes being preserved. Both indiscriminate (untargeted) and targeted,
white-box and black-box attacks are studied.

 
 
 
 
 
 
– We also explore possible countermeasures—e.g., adversarial training—that make the training less vulnerable to

causative/poisoning attacks.

– To make use of the developed capacity for autonomous cyber-security operations, we build our experimental plat-
form around software-deﬁned networking (SDN) [3], a next-generation tool chain for centralising and abstracting
control of reconﬁgurable networks. The SDN controller provides a centralised view of the whole network, and
is directly programmable. As a result, it is very ﬂexible for managing and reconﬁguring various types of net-
work resources. Therefore, in our experiments the RL agents obtain all network information and perform different
network operations via the SDN controller.

– Our results demonstrate that RL agents can successfully identify the optimal actions to protect the critical server,
by isolating as few compromised nodes as possible. In addition, even though the causative attacks can cause the
agent to make incorrect decisions, adversarial training shows great potential for mitigating the negative impact.

Fig. 1: An example network setup. The attacker propagates through the network to compromise the critical server, and
the defender applies RL to prevent the critical server from compromise and to preserve as many nodes as possible.

The remainder of the paper is organised as follows: Section 2 brieﬂy introduces the fundamental concepts in
reinforcement learning and SDN; Section 3 deﬁnes the research problem; Section 4 introduces in detail the different
forms of proposed attacks against RL; Section 5 presents the experimental results on applying RL for autonomous
defence in SDN, and the impact of those causative attacks; Section 6 overviews previous work on adversarial machine
learning (including attacks against reinforcement learning) and existing countermeasures; Section 7 concludes the
paper, and offers directions for future work.

2 Preliminaries

Before deﬁning the research problems investigated in this paper, we ﬁrst brieﬂy introduce the basic concepts in rein-
forcement learning and software-deﬁned networking.

2.1 Reinforcement Learning

In a typical reinforcement learning setting [73], an agent repeatedly interacts with the environment: at each time step
t, the agent (1) observes a state st of the environment; (2) chooses an action at based on its policy π—a mapping from

2

Subnet 1Subnet 2Subnet 4Subnet 31.11.22.12.23.13.24.14.21.31.41.51.62.32.42.52.62.72.83.33.43.53.63.73.83.94.34.44.54.64.74.84.9Attacker’s hostCritical serverPossible migration destinationthe observed states to the actions to be taken; and (3) receives a reward rt and observes next state st+1. This process
continues until a terminal state is reached, and then a new episode restarts from a certain initial state.
The agent’s objective is to maximise its discounted cumulative rewards over the long run: Rt = (cid:205)∞
γ ∈ (0, 1] is the discount factor that controls the trade-off between short-term and long-term rewards.

τ=t γτ−trτ, where

Under a given policy π, the value of taking action a in state s is deﬁned as: Qπ(s, a) = E[Rt |st = s, at = a, π].
Similarly, the value of state s is deﬁned as: V π(s) = E[Rt |st = s, π]. In this paper, we mainly focus on two widely cited
RL algorithms: Double Deep Q-Networks (DDQN) [34] and Asynchronous Advantage Actor-Critic (A3C) [53].

Q-Learning Q-learning [73] approaches the above problem by estimating the optimal action value function Q∗(s, a) =
maxπ Qπ(s, a). Speciﬁcally, it uses the Bellman equation Q∗(s, a) = Es(cid:48)[r + γ maxa(cid:48) Q∗(s(cid:48), a(cid:48))] to update the value
iteratively. In practice, Q-learning is commonly implemented by function approximation with parameters θ: Q∗(s, a) ≈
Q(s, a; θ). At each training iteration i, the loss function is deﬁned as: Li(θi) = E[(r + γ maxa(cid:48) Q(s(cid:48), a(cid:48); θi−1) −
Q(s, a; θi))2].

Deep Q-Networks (DQN) Classic Q-learning networks suffer from a number of drawbacks, including (1) the i.i.d.
(independent and identically distributed) requirement of the training data being violated as consecutive observations
are correlated, (2) unstable target function when calculating Temporal Difference (TD) errors, and (3) different scales
of rewards. Deep Q networks (DQN) [54] overcome these issues by (1) introducing experience replay, (2) using a
target network that ﬁxes its parameters (θ−) and only updates at regular intervals, and (3) clipping the rewards to the
range of [−1, 1]. The loss function for DQN becomes: Li(θi) = E[(r + γ maxa(cid:48) Q(s(cid:48), a(cid:48); θ−

i ) − Q(s, a; θi))2].

Double DQN (DDQN) To further solve the problem of value overestimation, Hasselt et al. [34] generalise the Double
Q-learning algorithm [33] proposed in the tabular setting, and propose Double DQN (DDQN) that separates action
selection and action evaluation, i.e., one DQN is used to determine the maximising action and a second one is used to
estimate its value. Therefore, the loss function is: Li(θi) = E[(r + γQ(s(cid:48), arg maxa(cid:48) Q(s(cid:48), a(cid:48); θi); θ−

i ) − Q(s, a; θi))2].

Prioritised Experience Replay Experience replay keeps a buffer of past experiences, and for each training it-
eration, it samples uniformly a batch of experiences from the buffer, i.e., each experience is treated equally re-
gardless of importance. Prioritised experience replay [70] assigns higher sampling probability to transitions that
do not ﬁt well with the current estimation of the Q function. For DDQN, the error of an experience is deﬁned as
|r + γQ(s(cid:48), arg maxa(cid:48) Q(s(cid:48), a(cid:48); θ); θ−) − Q(s, a; θ)|. Experiences with larger errors are more likely to be chosen, and the
errors will be updated at the end of each training iteration.

Asynchronous Advantage Actor-Critic (A3C) Mnih et al. [53] propose an asynchronous variant of the classical
actor-critic algorithm, which estimates both the state value function V(s; θv) and a policy π(a|s; θ p). Speciﬁcally, the
asynchronous advantage actor-critic (A3C) algorithm uses multiple threads to explore different parts of the state space
simultaneously, and updates the global network in an asynchronous way. In addition, instead of using discounted
returns to determine whether an action is good, A3C estimates the advantage function so that it can better focus on
where the predictions are lacking.

2.2 Software-Deﬁned Networking

In conventional online applications, the majority of the communication occurs directly between the client and the
server. However, due to the wide use of content-delivery networks (CDN) and cloud services, it has become common
for modern applications to access multiple servers and databases before data is ﬁnally returned to the client, resulting
in a signiﬁcantly higher proportion of “east-west” trafﬁc. In addition, users nowadays may access an application with
any type of device, e.g., smartphones, tablets and laptops, from anywhere around the world. These changes have
contributed to an ever-increasing strain on traditional networks, largely due to conventional switches having their own
control unit, making network reconﬁguration time consuming.

3

In order to better serve today’s dynamic and high-bandwidth applications, a new architecture called Software-
Deﬁned Networking (SDN) has emerged [3]. There are three layers in the SDN architecture: (1) the application layer
includes applications that deliver services. These applications communicate their network requirements to the con-
troller via northbound APIs; (2) the SDN controller translates these requirements into low-level controls, and sends
them through southbound APIs to the infrastructure layer; (3) the infrastructure layer comprises network switches that
control forwarding and data processing.

One major advantage of SDN is that it decouples network control and forwarding functions, rendering the con-
troller directly programmable. As a result, network resources can be conveniently managed, conﬁgured and optimised
using standardised protocols. For example, Openﬂow [2] is a commonly-used protocol between the controller and the
underlying switches. Note that in the simplest implementation, the controller follows a centralised design where it has
a global view of the entire network. However, in order to manage large-scale networks, hierarchical and distributed
designs can also be adopted.

There have been a number of proprietary and open-source SDN controller software platforms, such as Cisco’s
Open SDN Controller [5], Floodlight [6], NOX/POX [8] and Open vSwitch [4]. In this paper, we have opted to use
OpenDaylight [49, 9], which is the largest open-source SDN controller today and which is updated regularly.

Applying RL in SDN One main challenge SDN faces arises from highly-dynamic trafﬁc patterns, which motivate
a requirement for the network to be reconﬁgured frequently. It has been demonstrated that RL is an ideal tool to
accomplish such a task [69, 48, 46, 38, 77, 68, 51, 27, 40, 20]. For example, Salahuddin et al. [69] propose a roadside
unit (RSU) cloud to enhance trafﬁc ﬂow and road safety. This RSU cloud is implemented using SDN, and leverages
reinforcement learning to better cope with the dynamic service demands from the transient vehicles. In this way. the
reconﬁguration overhead can be minimised over the long run: increasing/decreasing the number of service centres,
migrating services from one centre to another, for example. Mao et al. [48] design a job scheduling algorithm, called
DeepRM, for computing clusters that have similar demands as SDN management. DeepRM uses colour images to
represent system resources, e.g., CPU, RAM, as well as the resources required by each job slot, and exploits policy
gradient methods to minimise average job slowdown.

3 Problem Statement

In this paper, we seek to answer the question: Can reinforcement learning be used for autonomous defence in SDN?
We start with a scenario that does not consider the attacker poisoning the training process, and then investigate the
impact of adversarial reinforcement learning. While we also brieﬂy discuss potential countermeasures, we largely
leave defences to future work.

3.1 Reinforcement Learning Powered Autonomous Defence in SDN
Consider a network of N nodes (e.g., Fig. 1), H = {h1, h2, ..., hN }, where HC ⊂ H is the set of critical servers to be
protected (blue nodes in Fig. 1), HM ⊂ H is the set of possible migration destinations for h ∈ HC (green nodes), and
HA ⊂ H is the set of nodes that have initially been compromised (red nodes). The attacker aims to propagate through
the network, and penetrate the mission critical servers, while the defender/SDN controller monitors the system state,
and takes appropriate actions in order to preserve the critical servers and as many non-critical nodes as possible.

Reﬂecting suggestions from past work, we consider a defender adopting RL. In this paper, we start with a simpli-
ﬁed version, and make the following assumptions (Section 7 explains how they may be replaced): (1) each node (or
link) only has two states: compromised/uncompromised (or on/off); (2) both the defender and the attacker know the
complete network topology; (3) the defender has in place a detection system that can achieve a detection rate of 90%,
with no false alarms (before the causative attacks); (4) the attacker needs to compromise all nodes on the path (i.e.,
cannot "hop over" nodes).

Given these assumptions, in each step the defender:

1. Observes the state of the network—whether a node is compromised, and whether a link is switched on/off, e.g.,
there are 32 nodes and 48 links in Fig. 1, so one state contains an array of 80 0s/1s, where 0 means the node is
uncompromised or the link is switched off, and 1 means the node is compromised or the link is switched on;

4

2. Takes an action that may include: (i) isolating and patching a node; (ii) reconnecting a node and its links; (iii)
migrating the critical server and selecting the destination; and (iv) taking no action. Note that, in this scenario, the
defender can only take one type of action at a time, and if they decide to isolate/reconnect, only one node can be
isolated/reconnected at a time. In the example of Fig. 1, there are 68 (32+32+3+1) possible actions altogether;
3. Receives a reward based on (i) whether the critical servers are compromised; (ii) the number of nodes reachable
from the critical servers; (iii) the number of compromised nodes; (iv) migration cost; and (v) whether the action is
valid, e.g., it is invalid to isolate a node that has already been isolated, or to migrate a server to the current location.

Meanwhile, the attacker carefully chooses the nodes to compromise. For example, in the setting of Fig. 1, they
infect a node only if it (1) is closer to the “backbone” network (nodes on the dashed circle); (2) is in the backbone
network; or (3) is in the target subnet. Table 1 summarises this problem setting.

Table 1: Problem description: RL powered autonomous defence in SDN

State

Defender
(1) Whether each node is compromised;
(2) Whether each link is turned on/off.

Actions (1) Isolate and patch a node;

(2) Reconnect a node and its links;
(3) Migrate the critical server and select the destination;
(4) Take no action
(1) Preserve the critical servers;
(2) Keep as many nodes uncompromised and reachable
from the critical servers as possible.

Goals

Attacker

Compromise a node that satisﬁes certain conditions, e.g.,
the node (1) is closer to the “backbone” network; (2) is
in the backbone network; or (3) in the target subnet.

Compromise the critical servers.

Alternatively, if we look from a game perspective, the problem can be deﬁned as a two-player zero-sum security
game [11] between the attacker (A) and the defender (D): G = {P, ASi, Ui, i ∈ { A, D}}, where P = {A, D}, AS is the
action set, and U is the utility. Speciﬁcally, in the problem setting discussed here, while the defender can choose one
of the four types of actions, the attacker has a pure/deterministic strategy (we will replace this and model the attacker
as another agent [66] in future work). In addition, the defender’s utility U D is the discounted accumulative rewards
Rt = (cid:205)∞
τ=t γτ−trτ, γ ∈ (0, 1], and since it is a zero-sum game, U A = −U D. Considering tractability of computing
equilibria (especially for much larger and more complex networks), we are interested in approximating the optimal
response for the defender using RL algorithms.

3.2 Causative Attacks against RL Powered Autonomous Defence System

As an online system, the autonomous defence system continues gathering new statistics, and keeps training/updating
its model. Therefore, it is necessary and crucial to analyse the impact of an adversarial environment, where malicious
users can manage to falsify either the rewards received by the agent, or the states of certain nodes. In other words, this
is a form of causative attack that poisons the training process, in order for the tampered model to take sub-optimal
actions. In this paper, we investigate the two forms of attacks below.
1. Flipping reward signs. Suppose that without any attack, the agent would learn the following experience (s, a, s(cid:48), r),
where s is the current system state, a is the action taken by the agent, s(cid:48) is the new state, and r is the reward. In
our scenario, we permit the attacker to ﬂip the sign of a certain number of rewards (e.g., 5% of all experiences),
and aim to maximise the loss function of the RL agent. This is an extreme case of the corrupted reward channel
problem [28], where the reward may be corrupted due to sensor errors, hijacks, etc.

2. Manipulating states. Again, consider the case where the agent learns an experience (s, a, s(cid:48), r) without any at-
tack. Furthermore, when the system reaches state s(cid:48), the agent takes the next optimal action a(cid:48). The attacker is
then allowed to introduce one false positive (FP) and one false negative (FN) reading in s(cid:48), i.e., one uncompro-
mised/compromised node is reported as compromised/uncompromised to the defender. As a result, instead of
learning (s, a, s(cid:48), r), the agent ends up observing (s, a, s(cid:48) + δ, r (cid:48)) (where δ represents the FP and FN readings), and
consequently may not take a(cid:48) in the next step.

5

4 Attack Mechanisms

This section explains in detail the mechanisms of the attacks introduced above.

4.1 Attack I: Maximise Loss Function by Flipping Reward Signs

Recall that the DDQN agent aims to minimise the loss function: Li(θi) = E[(r + γQ(s(cid:48), arg maxa(cid:48) Q(s(cid:48), a(cid:48); θi); θ−
i ) −
Q(s, a; θi))2]. In the ith training iteration, θi is updated according to the gradient of ∂Li/∂θi. The main idea for the
ﬁrst form of attack is to falsify certain rewards based on ∂Li/∂r, in order to maximise the loss Li.

Speciﬁcally as shown in Algorithm 1, after the agent samples a batch of experiences for training, we calculate the
gradient of ∂Li/∂r for each of them, and ﬂip the sign of experience with the largest absolute value of the gradient
|∂Li/∂r | that satisﬁes r · ∂Li/∂r < 0 (if r · ∂Li/∂r > 0 ﬂipping the sign decreases the loss function).

Algorithm 1: Attack I – Flipping reward signs

: The list of sampled experiences LE ; The loss function Li of the RL agent

Input
Output : The tampered experiences L (cid:48)
E

1 for experience (sj, aj, s(cid:48)

j, rj ) in LE do

Calculate g = ∂Li/∂rj ;
if |g| > maxG and g · rj < 0 then

maxG = |g|;
maxIdx = j;

2

3

4

5

6 (smaxI dx, amaxI dx, s(cid:48)
7 return L (cid:48)
E

maxI dx, rmaxI dx) ← (smaxI dx, amaxI dx, s(cid:48)

maxI dx, −rmaxI dx);

4.2 Attack II: Prevent Agent from Taking Optimal/Speciﬁc Actions by Manipulating States

Our experimental results show that the above form of attack is indeed effective in increasing the agent’s loss function.
However, it only delays the agent from learning the optimal actions. Therefore, the second form of attack directly
targets the value function Q (against DDQN agent) or the policy π (against A3C agent).

1. Indiscriminate attacks. For each untampered experience (s, a, s(cid:48), r), indiscriminate attacks falsify the states of
two nodes in the new state s(cid:48), in order to prevent the agent from taking the next optimal action a(cid:48) that has been
learned so far (which may be different from the ﬁnal optimal action for the given state), i.e., against DDQN agent
the attacks minimise maxa(cid:48) Q(s(cid:48) + δ, a(cid:48)), while against A3C agent the attacks minimise maxa(cid:48) π(a(cid:48)|s(cid:48) + δ).
2. Targeted attacks. Targeted attacks aim to prevent the agent from taking a speciﬁc action (in our case, we ﬁnd
that this is more effective than tricking the agent to take a speciﬁc action). As an extreme case, this paper allows
the attacker to know the (ﬁnal) optimal action a∗ that the agent is going to take next (a∗ may be different from
a(cid:48)), and they seek to minimise the probability of the agent taking that action: for DDQN, the attacks minimise
Q(s(cid:48) + δ, a∗); for A3C, the attacks minimise π(a∗|s(cid:48) + δ).

The details of the above two types of attacks are presented in Algorithm 2 (Note: Algorithm 2 is for the attacks
against DDQN. Due to similarity, the algorithm for attacks against A3C is omitted). In addition, we also consider the
following variants of the attacks:

1. White-box attacks vs. Black-box attacks. In white-box attacks, the attacker can access the model under training
to select the false positive and false negative nodes, while in black-box attacks, the attacker ﬁrst trains surrogate
model(s), and then uses them to choose the FPs and FNs.

6

2. Limit on the choice of FPs and FNs. The above attacks do not set any limit on the choice of FPs and FNs, and
hence even though the attacker can only manipulate the states of two nodes each time, overall, they still need to
be able to control a number of nodes, which is not practical. Therefore, we ﬁrst run unlimited white-box attacks,
identify the top two nodes that have been selected most frequently as FPs and FNs respectively, and only allow
the attacker to manipulate the states of those nodes.

3. Limit on the timing of the attack. The last type of attacks only introduces FPs and FNs in the ﬁrst m steps (e.g.,

m = 3) in each training episode.

Algorithm 2: Attack II – Manipulating states

Input

: The original experience (s, a, s(cid:48), r); The list of all nodes LN ; Target action at (at = −1 for indiscriminate attack);
The main DQN Q

Output : The tampered experience (s, a, s(cid:48) + δ, r (cid:48))

1 if at == −1 then

// indiscriminate attack
at = arg maxa(cid:48) Q(s(cid:48), a(cid:48));

2

3 for node n in LN do
4

if n is compromised then

5

6

7

8

9

10

11

12

13

14

15

mark n as uncompromised;
if Q(s(cid:48) + δ, at ) < minQF N then

// δ represents the FP and/or FN readings
F N = n;
minQF N = Q(s(cid:48) + δ, at );

restore n as compromised;

else

mark n as compromised;
if Q(s(cid:48) + δ, at ) < minQF P then

FP = n;
minQF P = Q(s(cid:48) + δ, at );
restore n as uncompromised;

16 Change node F N to uncompromised;
17 Change node FP to compromised;
18 return (s, a, s(cid:48) + δ, r (cid:48))

5 Experimental Veriﬁcation

This section begins with a discussion of the experimental results obtained when applying RL to autonomous defence in
a SDN environment without considering causative attacks. We then analyse the impact of the two forms of attacks ex-
plained in Section 4. Finally, we discuss adopting adversarial training as a potential countermeasure, and present some
preliminary results. Experiments on causative attacks were performed on eight servers (equivalent to two Amazon
EC2 t2.large instances and six t2.xlarge instances [1]), and each set of experiments was repeated 15 to 25 times.

5.1 Autonomous Defence in a SDN

For our experiments, as shown in Fig. 1, we created a network with 32 nodes and 48 links using Mininet [7], one
of the most popular network emulators. OpenDaylight [49, 9] serves as the controller, and monitors the whole-of-
network status. The RL agent retrieves network information and takes appropriate operations by calling corresponding

7

APIs provided by OpenDaylight. In the setup, the three nodes in red, i.e., nodes 1.5, 2.7 and 3.6, have already been
compromised. Node 3.8 is the critical server to be protected, and it can be migrated to node 3.9 or 4.5.

We trained a DDQN (with Prioritised Experience Relay) agent and an A3C agent. We set the length of training
such that the reward per episode for both agents reached a stable value well before training ended. The two agents
learned two slightly different responses: the DDQN agent decides to ﬁrst isolate node 3.6, then 1.3, 2.2 and ﬁnally 2.1,
which means 21 nodes are preserved (see Fig. 2a); while the A3C agent isolates nodes 1.5, 3.3, 2.2 and 2.1, keeping
20 nodes uncompromised and reachable from the critical server (see Fig. 2b).

(a) DDQN

(b) A3C

Fig. 2: Optimal results without causative attacks (nodes in the blue/green shade are preserved)

5.2 Attack I: Flipping Reward Sign

This subsection presents the results of the ﬁrst form of attack that ﬂips the reward sign. In our experiments, we limit
the total number of tampered experiences to 5% of all experiences obtained by the agent, and also set the number of
tampered experiences per training iteration to the range of [1, 5].

As can be seen in Fig. 3, the attack is effective in increasing the agent’s loss function. However, our results also
suggest that this form of attack only delays the training as the agent still learns the optimal actions (although the delay
can be signiﬁcant).

5.3 Attack II: Manipulate State—Indiscriminate Attacks

Unlimited White-Box Attacks We start with unlimited indiscriminate white-box attacks, the case where the attacker
has full access to the model under training. For each experience (s, a, s(cid:48), r) obtained by the agent, they can manipulate
the states of any two nodes in s(cid:48), i.e., one false positive and one false negative, in order to prevent the agent from
taking the next optimal action a(cid:48) that has been learned so far (note that it may be different from the ﬁnal optimal
action). Speciﬁcally, for the DDQN agent, the attacker minimises maxa(cid:48) Q(s(cid:48) + δ, a(cid:48)); for the A3C agent, the attacker
minimises maxa(cid:48) π(a(cid:48)|s(cid:48) + δ).

Fig. 4 presents the results we obtained during our experiments. The leftmost bars in Figs. 4a and 4b suggest
that the unlimited indiscriminate white-box attacks are very effective against both DDQN and A3C. Speciﬁcally, the
average number of preserved nodes decreases from 21 and 20 to 3.3 and 4.9, respectively.

8

Subnet 1Subnet 2Subnet 4Subnet 31.11.22.12.23.13.24.14.21.31.41.51.62.32.42.52.62.72.83.33.43.53.63.73.83.94.34.44.54.64.74.84.9Attacker’s hostCritical serverPossible migration destinationSubnet 1Subnet 2Subnet 4Subnet 31.11.22.12.23.13.24.14.21.31.41.51.62.32.42.52.62.72.83.33.43.53.63.73.83.94.34.44.54.64.74.84.9Attacker’s hostCritical serverPossible migration destinationFig. 3: Cumulative loss before and after ﬂipping reward sign attacks (against DDQN)

White-Box Attacks with Limits on the Choices of False Positive and False Negative As pointed out in Subsec-
tion 3.2, even though the attacker only manipulates the states of two nodes each time, overall, they still need to be
able to control a number of nodes, which is unlikely in practice. We calculate the number of times that each node is
chosen in the above unlimited attacks, and ﬁnd that some nodes are selected more frequently than others (Fig. 5; the
histograms for the A3C case are omitted due to similarity).

Therefore, when poisoning the DDQN agent, we limit the false positive nodes to {3.5 (node ID 18), 4.1 (node ID
23)}, and limit the false negative nodes to {2.7 (node ID 12), 3.6 (node ID 19)}. We use node 4.1 instead of 3.4 (node
ID 17), as otherwise both selected nodes would be from the target subnet and directly connected to the target, which
is unlikely in real situations. In the A3C case, the false positive and false negative nodes are limited to {1.3 (node ID
2), 3.5 (node ID 18)}, and {2.7 (node ID 12), 1.5 (node ID 4)}, respectively.

The second and third bars in Figs. 4a and 4b show that the limit has an obvious negative impact on the attack,
especially the limit on the false negative nodes. Still, less than half of the nodes can be preserved on average, compared
with the scenarios without attacks.

Black-Box Attacks with Limits on the Choices of False Positive and False Negative Nodes In our black-box
attacks (both intra- and cross-models), the attacker does not have access to the training model. Instead, they train their
own agents ﬁrst, and use the surrogate models to poison the training of the target models by choosing false positive
and false negative nodes. Speciﬁcally, we have trained a few DDQN and A3C agents with a different number of hidden
layers from the target model, and observed that these surrogates can still prevent the critical server from compromising.
As illustrated by the rightmost two bars in Figs. 4a and 4b, black-box attacks are only slightly less effective than
the counterpart white-box attacks despite the surrogate using a different model. This lends support that transferability
also exists between RL algorithms, i.e., attacks generated for one model may also transfer to another model.

5.4 Attack II: Manipulate State—Targeted Attacks

In the targeted attacks considered here, the attacker is assumed to know the sequence of ﬁnal optimal actions, and
attempts to minimise the probability of the agent following that sequence. It should be pointed out that we have also
studied the case where the attacker instead maximises the probability of taking a speciﬁc non-optimal action for each
step, but our results suggested that this is less effective.

We ﬁnd that in targeted attacks, certain nodes are also selected more frequently as a false positive and false negative.
In this scenario, we limit false positive nodes to (1) {3.5 (node ID 18), 4.1 (node ID 23)} against DDQN, (2) {2.6 (node
ID 11), 1.4 (node ID 3)} against A3C, and limit false negative nodes to (1) {1.5 (node ID 4), 2.1 (node ID 6)} against
DDQN, (2) {4.1 (node ID 23), 2.4 (node ID 9)} against A3C.

Our results, summarised in Fig. 6, indicate that (1) compared with the results on indiscriminate attacks, targeted
attacks work better, especially against DDQN (fewer nodes are preserved on average), as the attacker is more knowl-

9

0.511.522.53No. of training episode10502004006008001000Cumulative loss Attack I: flipping reward sign Without attackTraining starts(a) Indiscriminate attacks against DDQN

(b) Indiscriminate attacks against A3C

Fig. 4: Indiscriminate attacks against DDQN & A3C. The bars indicates the percentage of attacks (left y−axis) that
(1) have no impact; (2) cause fewer nodes to be preserved; and (3) cause the critical server to be compromised. The
line marked by “×” indicates the average number of preserved servers (right y−axis). The ﬁve types of attacks are: (1)
white-box, no limit on FNs&FPs; (2) white-box, with limits on FP but not on FN, (3) white-box, with limits on both
FP and FN; (4) black-box, same algorithm, with limits on both FPs and FNs; (5) black-box, different algorithm, with
limits on both FPs and FNs.

edgeable in this case; (2) similar to the indiscriminate case, black-box attacks achieve comparable results to the white-
box attacks, further demonstrating the transferability between DDQN and A3C.

5.5 Timing Limits for the Attacks

The attacks discussed so far allowed the attacker to poison every experience obtained by the agent. A possible lim-
itation on this assumption is to examine whether these attacks can remain successful when the attacker can only
manipulate part of the experiences. Therefore, in this subsection we shall look at attacks that poison only a subset (the
ﬁrst three steps) in each training episode.

Figs. 7a and 7b depict the results of the time-limited version of (cross-model) black-box attacks against DDQN
and white-box attacks against A3C (both with limit on the choices of FPs and FNs), respectively. The results suggest
that even though the time limit has a negative impact in every scenario studied, the attacks are still effective.

10

  No limit, Whitebox FP{18,23},   Whitebox  FP{18,23},  FN{12,19},    Whitebox   FP{18,23},   FN{12,19},      Blackbox      (DDQN)   FP{18,23},   FN{12,19},     Blackbox       (A3C)020406080100Percentage of attacks048121620No. of preserved servers  Have no impact Cause fewer nodes to be preserved Cause the critical server to be compromised Average number of preserved servers  No limit, Whitebox FP{2,18},  Whitebox  FP{2,18},  FN{12,4},  Whitebox   FP{2,18},   FN{12,4},     Blackbox      (A3C)   FP{2,18},   FN{12,4},    Blackbox    (DDQN)020406080100Percentage of attacks048121620No. of preserved servers  Have no impact Cause fewer nodes to be preserved Cause the critical server to be compromised Average number of preserved servers(a) False positive

(b) False negative

Fig. 5: Histograms of the false positive and false negative nodes being selected against DDQN. N.B.: The node IDs
{0, 1, ..., 31} are ordered and mapped to the node sequence {1.1, 1.2, ..., 1.6, 2.1, ..., 2.8, 3.1, ..., 3.9, 4.1, ..., 4.9}

5.6 Discussion on Countermeasures

In supervised learning problems, adversarial training [74, 31, 75] has the defender select a target point (x, y) from
the training set, modify x to x∗ (i.e., generates an adversarial sample), and then inject (x∗, y) back into the training
set, under the implicit assumption that the true label y should not change given the instance has been only slightly
perturbed.

In our RL setting, while the attacker manipulates the observed states to minimise the probability of the agent taking
the optimal action a in state s, the defender can construct adversarial samples that counteract the effect. For example,
for each experience (s, a, s(cid:48), r), the defender can increase r by a small amount, e.g., 5% of the original value, given that
r is positive and a is not chosen randomly (the probability of choosing an action randomly decreases as the training
proceeds). The rationale behind this modiﬁcation is that when the poisoning attack starts out, it is likely that a is still
the optimal action (that has been learned so far) for state s. If r is positive it means that action a is a relatively good
option for s, and since the attacker has poisoned the state to prevent the agent from taking a, we slightly increase r to
encourage the agent to take action a.

We have tested the above idea against indiscriminate white-box attacks with a limit on the choices of FPs and FNs
against DDQN. Speciﬁcally, for each experience (s, a, s(cid:48), r) whose r is positive and a is not selected randomly, we
change it to (s, a, s(cid:48), min(1.0, 1.05r)). Note that our experimental results suggest that adding 5% error to the reward
signal when there is no attack will not prevent the agent from learning the optimal actions, although it may cause some
delay. The results in Fig. 8 indicate that adversarial training can make the training process much less vulnerable.

However, the results are still preliminary, and we plan to further investigate other forms of adversarial training. For
example, Pinto et al. [66] model all potential disturbances as an extra adversarial agent, whose goal is to minimise the
discounted reward of the leading agent. They formulate the policy learning problem as a two player zero-sum game,
and propose an algorithm that optimises both agents by alternating learning one agent’s policy with the other policy
being ﬁxed. In addition, we will also study the impact of the loss function, prioritised experience replay, ensemble
adversarial training [75] and other, more intrusive types of attacks, where the adversary is aware of the defence method,
and attacks the defended model.

6 Related Work

This section reviews ways in which attackers can target machine learning systems and current defence mechanisms.
We ﬁrst present a taxonomy on attacks against (primarily) supervised classiﬁers, and then summarise recent work

11

(a) Targeted attacks against DDQN

(b) Targeted attacks against A3C

Fig. 6: Targeted attacks against DDQN & A3C. The bars indicate the percentage of attacks (left y−axis) that (1) have
no impact; (2) cause fewer nodes to be preserved; and (3) cause the critical server to be compromised. The line marked
by “×” indicates the average number of preserved servers (right y−axis).

that applies/modiﬁes these attacks to manipulate RL systems. Finally, we review existing countermeasures against
adversarial machine learning.

6.1 Taxonomy of Attacks against Machine Learning Classiﬁers

Barreno et al. [13] develop a qualitative taxonomy of attacks against machine learning classiﬁers based on three axes:
inﬂuence, security violation and speciﬁcity.

– The axis of inﬂuence concerns the attacker’s capabilities: in causative attacks, the adversary can modify the train-
ing data to manipulate the learned model; in exploratory attacks, the attacker does not poison training, but carefully
alters target test instances to ﬂip classiﬁcations. The resulting malicious instances which resemble legitimate data
are called adversarial samples.

– The axis of security violation indicates the consequence desired by the attacker: integrity attacks are an example of
deception attacks (achieving uncertainty, incompleteness, etc.), resulting in indecision, delayed decisions, poor or
even wrong decisions, in decision-making systems. In this type of attack, the malicious instances bypass the ﬁlter
as false negatives. In contrast, availability attacks seek to cause a denial-of-service against legitimate instances.
– The axis of speciﬁcity refers to the target of the attacks: indiscriminate attacks aim to degrade the classiﬁer’s

performance overall, while targeted attacks focus on a speciﬁc type of instance, or a speciﬁc instance.

12

  FP{18,23},  FN{4,6},  Whitebox   FP{18,23},   FN{4,6},    Blackbox   (DDQN)   FP{18,23},   FN{4,6},   Blackbox    (A3C)020406080100Percentage of attacks048121620No. of preserved servers  Have no impact Cause fewer nodes to be preserved Cause the critical server to be compromised Average number of preserved servers  FP{11,3},  FN{23,9},  Whitebox   FP{11,3},   FN{23,9},    Blackbox   (A3C)   FP{11,3},   FN{23,9},   Blackbox    (DDQN)020406080100Percentage of attacks048121620No. of preserved servers  Have no impact Cause fewer nodes to be preserved Cause the critical server to be compromised Average number of preserved servers(a) Time limited cross-model black-box attacks against DDQN

Fig. 7: Attacks against DDQN&A3C with time limit. The attacker only poisons the ﬁrst three steps per training episode.

(b) Time limited white-box attacks against A3C

Table 2 uses the taxonomy to classify previous work on adversarial machine learning against classiﬁers. As can be
seen, more focus has been paid to exploratory integrity attacks. Presently, the Fast Gradient Sign Method (FGSM)
attack [31] is widely studied, and the C&W attack [22] is the most effective found so far on the application domains
tested, mostly in computer vision. Both of these attack methods can be used for targeted or indiscriminate attacks.

With further examination of the attacker’s capabilities, in addition to the potential control over the training data,
a powerful attacker may also know the internal architecture and parameters of the classiﬁer. Therefore, a fourth di-
mension can be added to the above taxonomy according to attacker information: in white-box attacks, the adversary
generates malicious instances against the target classiﬁer directly; while in black-box attacks, since the attacker does
not possess full knowledge about the model, they ﬁrst approximate the target’s model by training over a dataset from
a mixture of samples obtained by observing the target’s performance, and synthetically generating inputs and label
pairs. Then if the reconstructed model generalises well, the crafted adversarial examples against this model can be
transferred to the target network and induce misclassiﬁcations. Papernot et al. [63, 62] have demonstrated the effec-
tiveness of the black-box attack in certain speciﬁc domains. Speciﬁcally, they investigate intra- and cross-technique
transferability between deep neural networks (DNNs), logistic regression, support vector machines (SVMs), decision
trees and the k-nearest neighbour algorithm.

6.2 Attacks against Reinforcement Learning

In more recent studies, several papers have begun to study whether attacks against classiﬁers can also be applied to
RL-based systems. Huang et al. [39] have shown that deep RL is vulnerable to adversarial samples generated by the

13

  Indiscriminate    FP{18,23},    FN{12,19},     Blackbox       (A3C)   Targeted    FP{18,23},   FN{4,6},   Blackbox    (A3C)020406080100Percentage of attacks048121620No. of preserved servers  Have no impact Cause fewer nodes to be preserved Cause the critical server to be compromised Average number of preserved servers No time limit counterpart Indiscriminate    FP{2,18},    FN{12,4},     Whitebox   Targeted    FP{11,3},   FN{23,9},   Whitebox020406080100Percentage of attacks048121620No. of preserved servers  Have no impact Cause fewer nodes to be preserved Cause the critical server to be compromised Average number of preserved servers No time limit counterpartFig. 8: Adversarial training against indiscriminate white-box attacks with limits on the choices of FPs and FNs (against
DDQN), FP ∈ {18, 23}, F N ∈ {12, 19}

Fast Gradient Sign Method [31]. Their experimental results demonstrate that both white-box and black-box attacks are
effective, even though the less knowledge the adversary has, the less effective the adversarial samples are.

Behzadan & Munir [15] establish that adversaries can interfere with the training process of DQNs, preventing the
victim from learning the correct policy. Speciﬁcally, the attacker applies minimum perturbation to the state observed
by the target, so that a different action is chosen as the optimal action at the next state. The perturbation is generated
using the same techniques proposed against DNN classiﬁers. In addition, the authors demonstrate the possibility of
policy manipulation, where the victim ends up with choosing the actions selected by the adversarial policy.

Lin et al. [47] propose two kinds of attacks against deep reinforcement learning agents. In strategically-timed
attacks, instead of crafting the state at each time step, the adversary identiﬁes a subset of most vulnerable steps, and
uses the C&W attack [22] to perturb the corresponding states. In enchanting attacks, the adversary uses sampling to
iteratively ﬁnd a sequence of actions that will take the agent to the target state, and craft the current state so that the
agent will follow the next required action.

6.3 Adversarial Machine Learning Defences

A number of countermeasures have been proposed since the discovery of adversarial samples. These can be roughly
categorised into two classes: data-driven defences and learner robustiﬁcation.

Data-driven Defences This class of defences are data driven – they either ﬁlter out the malicious data, inject adver-
sarial samples into the training dataset, or manipulate features via projection. These approaches are akin to black-box
defences since they make little to no use of the learner.

– Filtering instances. These counter-measures assume that the poisoning data in the training dataset or the adversarial
samples against the test dataset either exhibit different statistical features, or follow a different distribution. There-
fore, they propose to identify and ﬁlter out the injected/perturbed data. For example, Laishram and Phoha [42]
design a method called Curie to protect SVM classiﬁers against poisoning attacks that ﬂip labels in the training
dataset. Steinhardt, Koh and Liang [72] study the worst-case loss of both data dependent and independent saniti-
sation methods. Metzen et al. [52] propose to train a detector network that takes input from intermediate layers of
a classiﬁcation network, and ﬁlters out adversarial samples. Feinman et al. [29] use (i) kernel density estimates in
the feature space of a ﬁnal hidden layer, and (ii) Bayesian neural network uncertainty estimates to detect the adver-
sarial samples against DNNs. Li et al. [45] apply principal component analysis (PCA) on the convolutional layers
of an original convolutional neural network (CNN), and use the extracted statistics to train a cascade classiﬁer that
can distinguish between valid and adversarial data.

14

           Before  adversarial training            After  adversarial training020406080100Percentage of attacks048121620No. of preserved servers  Have no impact Cause fewer nodes to be preserved Cause the critical server to be compromised Average number of preserved serversTable 2: Taxonomy of attacks on machine learners, with representative past work. As the taxonomy was designed for
supervised learners, we include attacks on reinforcement learning in Section 6.2.

Availability
Newsome et al. [60]: manipulate
training set of classiﬁers for worms
and spam to block legitimate
instances;
Chung & Mok [24]: generate
harmful signatures to ﬁlter out
legitimate network trafﬁc;
Nelson et al. [58]: exploit statistical
machine learning against a popular
email spam ﬁlter
Newsome et al. [60]; Chung & Mok
[24]; Nelson et al. [58]

Moore et al. [55]: provide
quantitative estimates of
denial-of-service activity

Moore et al. [55]

Causative

Targeted
(training set
manipulation for
speciﬁc errors)

Exploratory

Indiscriminate
(training set
manipulation to
maximise overall
error rate)
Targeted (test
instance
manipulation for
speciﬁc errors)

Indiscriminate
(test instance
manipulation for
misclassiﬁca-
tion)

Integrity
Rubinstein et al. [67]: boiling frog attacks against the PCA
anomaly detection algorithm;
Li et al. [44]: poison training data against collaborative
ﬁltering systems;
Mei & Zhu [50]: identify the optimal training set to
manipulate different machine learners;
Burkard & Lagesse [19]: targeted causative attack on
Support Vector Machines that are learning from a data
stream

Biggio et al. [18]: inject crafted training data to increase
SVM’s test error;
Xiao et al. [78]: label ﬂips attack against SVMs;
Koh & Liang [41]: minimise the number of crafted training
data via inﬂuence analysis
Nelson et al. [59]: probe a classiﬁer to determine good
attack points;
Papernot et al. [64]: exploits forward derivatives to search
for the minimum regions of the inputs to perturb;
Goodfellow et al. [31]: design the “fast gradient sign
method” (FGSM) to generate adversarial samples;
Carlini & Wagner [22]: propose the C&W method for
creating adversarial samples;
Han & Rubinstein [32]: improve the gradient descent
method by replacing with gradient quotient
Biggio et al. [17]: use gradient descent method to ﬁnd
attack instances against SVMs;
Szegedy et al. [74] demonstrate that changes imperceptible
to human eyes can make DNNs misclassify an image;
Goodfellow et al. [31];
Papernot et al. [63, 62]: attack the target learner via a
surrogate model;
Moosavi-Dezfooli et al. [56, 57]: propose DeepFool that
generates universal perturbations to fool multiple DNNs;
Carlini & Wagner [22];
Nguyen et al. [61]: produce images that are unrecognisable
to humans, but can be recognised by DNNs;
Han & Rubinstein [32]

– Injecting data. Goodfellow et al. [31] attribute the existence of adversarial samples to the “blind spots” of the
training algorithm, and propose injecting adversarial examples into training, in order to improve the generalisation
capabilities of DNNs [74, 31] – akin to active learning in non-adversarial settings. Tramer et al. [75] extend such
adversarial training methods by incorporating perturbations generated against other models.

– Projecting data. Previous work has shown that high dimensionality facilitates the generation of adversarial sam-
ples, resulting in an increased attack surface. For example, Wang, Gao and Qi [76] theorise that a single unnec-
essary feature can ruin the robustness of a model. To counter this, data can be projected into lower-dimensional
space before testing. Speciﬁcally, Bhagoji et al. [16] and Zhang et al. [80] propose defence methods based on di-
mensionality reduction via principal component analysis (PCA), and reduced feature sets, respectively. Das et al.
[25] demonstrate that JPEG compression can be used as a pre-processing step to defend against evasion attacks in
computer vision, potentially because JPEG compression removes high-frequency signal components, which helps

15

remove imperceptible perturbations. However, these results contradict those obtained by Li and Vorobeychik [43],
which suggests that more features should be used when facing adversarial evasion.

Learner Robustiﬁcation Rather than focusing solely on training and test data, this class of methods—which are
white-box in nature—aim to design models to be less susceptible to adversarial samples in the ﬁrst place.

– Stabilisation. Zheng et al. [81] design stability training that modiﬁes the model’s objective function by adding
a stability term. Their experimental results demonstrate that such a modiﬁcation stabilises DNNs against small
perturbations in the inputs. Papernot et al. [65] provide further examples using a distillation strategy against a
saliency-map attack. However, this method has been shown to be ineffective by Carlini and Wagner [21]. Hosseini
et al. [36] propose to improve adversarial training by adding an additional “NULL” class, and attempt to classify
all adversarial samples as invalid.

– Moving target. Sengupta et al. [71] apply moving target defences against exploratory attacks: instead of using
a single model for classiﬁcation, the defender prepares a pool of models, and for each image to be classiﬁed,
one trained DNN is picked following some speciﬁc strategy. The authors formulate the interaction as a Repeated
Bayesian Stackelberg Game, and show that their approach can decrease the attack’s success rate, while maintaining
high accuracy for legitimate users.

– Robust statistics. Another avenue that has remained relatively unexplored is to leverage ideas from robust statis-
tics, such as inﬂuence functions, break-down points, and M-estimators with robust loss functions (such as the
Huber loss) that place diminishing cost to increasingly erroneous predictions. Rubinstein et al. [67] were the ﬁrst
to leverage robust statistics in adversarial learning settings for cyber-security, by applying a robust form of princi-
pal components analysis that optimises median absolute deviations instead of variance to defend against causative
attacks on network-wide volume anomaly detection. Recently, interest in the theoretical computer science com-
munity has turned to robust estimation in high dimensions, e.g., Diakonikolas et al. [26].

Lessons Learned Despite many defences proposed, several recent studies [35, 23] point out that most of these meth-
ods (i) unrealistically assume that the attacker is not aware of the defence mechanism, and (ii) only consider relatively
weak attacks, e.g., FGSM [31]. Negative results are reported on the effectiveness of these methods against adaptive
attackers that are aware of the defence and act accordingly, and against the C&W attack [22] (empirically the most
efﬁcient exploratory attack proposed so far).

Speciﬁcally, He et al. [35] demonstrate that the following recently proposed methods cannot defend against adap-
tive adversaries: (i) feature squeezing [79], (ii) specialists+1 ensemble method [10], and (iii) ensemble of methods
in the papers [52, 29, 30]. Carlini and Wagner [23] show that ten detection methods for adversarial samples can be
defeated, either by the C&W attack [22], or white-box/black-box attacks. The authors conclude that the most effective
defence so far for DNNs is to apply randomness to the DNN, because it makes generating adversarial samples against
the target classiﬁer as difﬁcult as generating transferable adversarial samples. More recently, Athalye et al. [12] show
that defences relied on obfuscated gradients can also be circumvented.

While most of the previous work on adversarial machine learning focused on the vision domain, this paper applies

RL in cyber security, and studies how it reacts against different forms of causative integrity attacks.

7 Conclusions and Future Work

In this paper, we demonstrated the feasibility of developing autonomous defence in SDN using RL algorithms. In
particular, we studied the impact of different forms of causative attacks, and showed that even though these attacks
might cause RL agents to take sub-optimal actions, adversarial training could be applied to mitigate the impact.

For future work, we plan to (1) use a trafﬁc generator to introduce background trafﬁc between nodes, and use
network performance metrics to replace the current binary states; (2) consider different types of network trafﬁc, so
that the actions of the RL agent could include partial isolation in terms of blocking certain protocols between nodes;
(3) change full observability of the network status to partial observability—the defender may have limited resources,
and the attacker may not know the entire topology; and (4) remove limiting assumptions, e.g., the attacker having to
compromise all nodes along the path to the critical server.

16

Bibliography

[1] Amazon EC2 Instance Types — Amazon Web Services (AWS), https://aws.amazon.com/ec2/

instance-types/

[2] OpenFlow. http://archive.openﬂow.org/wp/learnmore/ (2011)
[3] SDN architecture. Tech.

rep.

(Jun 2014), https://www.opennetworking.org/wp-content/

uploads/2013/02/TR_SDN_ARCH_1.0_06062014.pdf

[4] Open vSwitch. http://openvswitch.org/ (2016)
[5] Cisco Open SDN Controller. https://www.cisco.com/c/en/us/products/cloud-systems-management/open-sdn-

controller/index.html (2017)

[6] Floodlight OpenFlow Controller. http://www.projectﬂoodlight.org/ﬂoodlight/ (2017)
[7] Mininet: An Instant Virtual Network on your Laptop. http://mininet.org/ (2017)
[8] NOX/POX. https://github.com/noxrepo (2017)
[9] OpenDaylight. https://www.opendaylight.org/ (2017)
[10] Abbasi, M., Gagné, C.: Robustness to Adversarial Examples through an Ensemble of Specialists. eprint

arXiv:1702.06856 (2017), http://arxiv.org/abs/1702.06856

[11] Alpcan, T., Baar, T.: Network Security: A Decision and Game-Theoretic Approach. Cambridge University Press,

New York, NY, USA, 1st edn. (2011)

[12] Athalye, A., Carlini, N., Wagner, D.: Obfuscated Gradients Give a False Sense of Security: Circumventing
Defenses to Adversarial Examples. arXiv:1802.00420 [cs] (Feb 2018), http://arxiv.org/abs/1802.
00420, arXiv: 1802.00420

[13] Barreno, M., Nelson, B., Joseph, A.D., Tygar, J.D.: The Security of Machine Learning. Machine Learning 81(2),

121–148 (Nov 2010), https://doi.org/10.1007/s10994-010-5188-5

[14] Beaudoin, L.: Autonomic computer network defence using risk states and reinforcement learning. Ph.D. thesis,

University of Ottawa (Canada) (2009)

[15] Behzadan, V., Munir, A.: Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks. eprint

arXiv:1701.04143 (2017), https://arxiv.org/abs/1701.04143

[16] Bhagoji, A.N., Cullina, D., Mittal, P.: Dimensionality Reduction as a Defense against Evasion Attacks on Ma-

chine Learning Classiﬁers. arXiv:1704.02654 (2017), https://arxiv.org/abs/1704.02654

[17] Biggio, B., Corona, I., Nelson, B., Rubinstein, B.I., Maiorca, D., Fumera, G., Giacinto, G., Roli, F.: Security
evaluation of support vector machines in adversarial environments. In: Support Vector Machines Applications,
pp. 105–153. Springer International Publishing (2014), https://arxiv.org/abs/1401.7727

[18] Biggio, B., Nelson, B., Laskov, P.: Poisoning Attacks against Support Vector Machines. In: Proceedings of the
29th International Coference on International Conference on Machine Learning. pp. 1467–1474. Omnipress,
Edinburgh, Scotland (2012), http://arxiv.org/abs/1206.6389

[19] Burkard, C., Lagesse, B.: Analysis of Causative Attacks Against SVMs Learning from Data Streams. In: Pro-
ceedings of the 3rd ACM on International Workshop on Security And Privacy Analytics. pp. 31–36. IWSPA ’17,
ACM, New York, NY, USA (2017), http://doi.acm.org/10.1145/3041008.3041012

[20] Cao, S., Wang, Y., Xu, C.: Service Migrations in the Cloud for Mobile Accesses: A Reinforcement Learning Ap-
proach. In: Proceedings of the 2017 International Conference on Networking, Architecture, and Storage (NAS).
pp. 1–10 (Aug 2017)

[21] Carlini, N., Wagner, D.: Defensive Distillation is Not Robust to Adversarial Examples. arXiv:1607.04311 (2016)
[22] Carlini, N., Wagner, D.: Towards Evaluating the Robustness of Neural Networks. eprint arXiv:1608.04644

(2016), http://arxiv.org/abs/1608.04644, arXiv: 1608.04644

[23] Carlini, N., Wagner, D.: Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods.

eprint arXiv:1705.07263 (2017), http://arxiv.org/abs/1705.07263, arXiv: 1705.07263

[24] Chung, S.P., Mok, A.K.: Advanced Allergy Attacks: Does a Corpus Really Help? In: Recent Advances in In-
trusion Detection. pp. 236–255. Lecture Notes in Computer Science, Springer, Berlin, Heidelberg (Sep 2007),
https://link.springer.com/chapter/10.1007/978-3-540-74320-0_13

[25] Das, N., Shanbhogue, M., Chen, S.T., Hohman, F., Chen, L., Kounavis, M.E., Chau, D.H.: Keeping the Bad Guys
Out: Protecting and Vaccinating Deep Learning with JPEG Compression. eprint arXiv:1705.02900 (May 2017),
http://arxiv.org/abs/1705.02900, arXiv: 1705.02900

[26] Diakonikolas, I., Kamath, G., Kane, D.M., Li, J., Moitra, A., Stewart, A.: Robust Estimators in High Dimen-
sions without the Computational Intractability. In: Proceedings of the 2016 IEEE 57th Annual Symposium on
Foundations of Computer Science (FOCS). pp. 655–664 (Oct 2016)

[27] Duggan, M., Duggan, J., Howley, E., Barrett, E.: A reinforcement learning approach for the scheduling of live
migration from under utilised hosts. Memetic Computing pp. 1–11 (Dec 2016), https://link.springer.
com/article/10.1007/s12293-016-0218-x

[28] Everitt, T., Krakovna, V., Orseau, L., Hutter, M., Legg, S.: Reinforcement Learning with a Corrupted Reward

Channel. eprint arXiv:1705.08417 (2017), http://arxiv.org/abs/1705.08417

[29] Feinman, R., Curtin, R.R., Shintre, S., Gardner, A.B.: Detecting Adversarial Samples from Artifacts. eprint

arXiv:1703.00410 (2017), http://arxiv.org/abs/1703.00410, arXiv: 1703.00410

[30] Gong, Z., Wang, W., Ku, W.S.: Adversarial and Clean Data Are Not Twins. eprint arXiv:1704.04960 (2017),

http://arxiv.org/abs/1704.04960

[31] Goodfellow,

I.J., Shlens, J., Szegedy, C.: Explaining and Harnessing Adversarial Examples. eprint

arXiv:1412.6572 (2014), http://arxiv.org/abs/1412.6572

[32] Han, Y., Rubinstein, B.I.P.: Adequacy of the Gradient-Descent Method for Classiﬁer Evasion Attacks.

arXiv:1704.01704 (Apr 2017), http://arxiv.org/abs/1704.01704, arXiv: 1704.01704

[33] Hasselt, H.V.: Double Q-learning. In: Lafferty, J.D., Williams, C.K.I., Shawe-Taylor, J., Zemel, R.S., Culotta, A.
(eds.) Advances in Neural Information Processing Systems 23, pp. 2613–2621. Curran Associates, Inc. (2010),
http://papers.nips.cc/paper/3964-double-q-learning.pdf

[34] Hasselt, H.V., Guez, A., Silver, D.: Deep Reinforcement Learning with Double Q-learning. eprint

arXiv:1509.06461 (Sep 2015), http://arxiv.org/abs/1509.06461, arXiv: 1509.06461

[35] He, W., Wei, J., Chen, X., Carlini, N., Song, D.: Adversarial Example Defenses: Ensembles of Weak Defenses are
not Strong. eprint arXiv:1706.04701 (2017), http://arxiv.org/abs/1706.04701, arXiv: 1706.04701
[36] Hosseini, H., Chen, Y., Kannan, S., Zhang, B., Poovendran, R.: Blocking Transferability of Adversarial Exam-
ples in Black-Box Learning Systems. eprint arXiv:1703.04318 (2017), http://arxiv.org/abs/1703.
04318, arXiv: 1703.04318

[37] Huang, L., Joseph, A.D., Nelson, B., Rubinstein, B.I., Tygar, J.: Adversarial machine learning. In: Proceedings

of the 4th ACM Workshop on Security and Artiﬁcial Intelligence. pp. 43–58. ACM (2011)

[38] Huang, R., Chu, X., Zhang, J., Hu, Y.H.: Energy-efﬁcient Monitoring in Software Deﬁned Wireless Sensor
Networks Using Reinforcement Learning: A Prototype. Int. J. Distrib. Sen. Netw. 2015, 1:1–1:1 (Jan 2015),
https://doi.org/10.1155/2015/360428

[39] Huang, S., Papernot, N., Goodfellow, I., Duan, Y., Abbeel, P.: Adversarial Attacks on Neural Network Policies.

eprint arXiv:1702.02284 (2017), https://arxiv.org/abs/1702.02284

[40] Kim, S., Son, J., Talukder, A., Hong, C.S.: Congestion prevention mechanism based on Q-leaning for efﬁcient
routing in SDN. In: Proceedings of the 2016 International Conference on Information Networking (ICOIN). pp.
124–128 (Jan 2016)

[41] Koh, P.W., Liang, P.: Understanding Black-box Predictions via Inﬂuence Functions. arXiv:1703.04730 [cs, stat]

(Mar 2017), http://arxiv.org/abs/1703.04730, arXiv: 1703.04730

[42] Laishram, R., Phoha, V.V.: Curie: A method for protecting SVM Classiﬁer from Poisoning Attack.

arXiv:1606.01584 [cs] (Jun 2016), http://arxiv.org/abs/1606.01584, arXiv: 1606.01584

[43] Li, B., Vorobeychik, Y.: Feature Cross-substitution in Adversarial Classiﬁcation. In: Proceedings of the
2014 NIPS. pp. 2087–2095. NIPS’14, MIT Press, Cambridge, MA, USA (2014), http://dl.acm.org/
citation.cfm?id=2969033.2969060

[44] Li, B., Wang, Y., Singh, A., Vorobeychik, Y.: Data Poisoning Attacks on Factorization-Based Collaborative Filter-
ing. eprint arXiv:1608.08182 (2016), http://adsabs.harvard.edu/abs/2016arXiv160808182L
[45] Li, X., Li, F.: Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics.

arXiv:1612.07767 [cs] (Dec 2016), http://arxiv.org/abs/1612.07767, arXiv: 1612.07767

[46] Lin, S.C., Akyildiz, I.F., Wang, P., Luo, M.: QoS-Aware Adaptive Routing in Multi-layer Hierarchical Software
Deﬁned Networks: A Reinforcement Learning Approach. In: Proceedings of the 2016 IEEE International Con-
ference on Services Computing (SCC). pp. 25–33 (Jun 2016)

18

[47] Lin, Y.C., Hong, Z.W., Liao, Y.H., Shih, M.L., Liu, M.Y., Sun, M.: Tactics of Adversarial Attack on Deep
Reinforcement Learning Agents. eprint arXiv:1703.06748 (Mar 2017), http://arxiv.org/abs/1703.
06748, arXiv: 1703.06748

[48] Mao, H., Alizadeh, M., Menache, I., Kandula, S.: Resource Management with Deep Reinforcement Learning. In:
Proceedings of the 15th ACM Workshop on Hot Topics in Networks. pp. 50–56. HotNets’16, ACM, New York,
NY, USA (2016), http://doi.acm.org/10.1145/3005745.3005750

[49] Medved, J., Varga, R., Tkacik, A., Gray, K.: OpenDaylight: Towards a Model-Driven SDN Controller architec-
ture. In: Proceeding of IEEE International Symposium on a World of Wireless, Mobile and Multimedia Networks
2014. pp. 1–6 (Jun 2014)

[50] Mei, S., Zhu, X.: Using Machine Teaching to Identify Optimal Training-set Attacks on Machine Learners. In:
Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence. pp. 2871–2877. AAAI’15, AAAI
Press, Austin, Texas (2015), http://dl.acm.org/citation.cfm?id=2886521.2886721

[51] Mestres, A., Rodriguez-Natal, A., Carner, J., Barlet-Ros, P., Alarcón, E., Solé, M., Muntés-Mulero, V., Meyer,
D., Barkai, S., Hibbett, M.J., Estrada, G., Ma‘ruf, K., Coras, F., Ermagan, V., Latapie, H., Cassar, C., Evans,
J., Maino, F., Walrand, J., Cabellos, A.: Knowledge-Deﬁned Networking. SIGCOMM Comput. Commun. Rev.
47(3), 2–10 (Sep 2017), http://doi.acm.org/10.1145/3138808.3138810

[52] Metzen, J.H., Genewein, T., Fischer, V., Bischoff, B.: On Detecting Adversarial Perturbations. eprint

arXiv:1702.04267 (2017), http://arxiv.org/abs/1702.04267, arXiv: 1702.04267

[53] Mnih, V., Badia, A.P., Mirza, M., Graves, A., Harley, T., Lillicrap, T.P., Silver, D., Kavukcuoglu, K.: Asyn-
chronous Methods for Deep Reinforcement Learning. In: Proceedings of the 33rd International Conference on
International Conference on Machine Learning - Volume 48. pp. 1928–1937. ICML’16, JMLR.org, New York,
NY, USA (2016), http://dl.acm.org/citation.cfm?id=3045390.3045594

[54] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M.A.: Playing Atari
with Deep Reinforcement Learning. CoRR abs/1312.5602 (2013), http://arxiv.org/abs/1312.5602
[55] Moore, D., Shannon, C., Brown, D.J., Voelker, G.M., Savage, S.: Inferring Internet Denial-of-service Activity.
ACM Trans. Comput. Syst. 24(2), 115–139 (May 2006), http://doi.acm.org/10.1145/1132026.
1132027

[56] Moosavi-Dezfooli, S.M., Fawzi, A., Fawzi, O., Frossard, P.: Universal Adversarial Perturbations. eprint

arXiv:1610.08401 (2016), http://arxiv.org/abs/1610.08401

[57] Moosavi-Dezfooli, S.M., Fawzi, A., Frossard, P.: DeepFool: A Simple and Accurate Method to Fool Deep Neural

Networks. In: CVPR. pp. 2574–2582 (2016), https://arxiv.org/abs/1511.04599

[58] Nelson, B., Barreno, M., Chi, F.J., Joseph, A.D., Rubinstein, B.I., Saini, U., Sutton, C.A., Tygar, J.D., Xia,
K.: Exploiting Machine Learning to Subvert Your Spam Filter. In: Proceedings of the First USENIX Work-
shop on Large-scale Exploits and Emergent Threats (LEET’08) (2008), http://blaine-nelson.com/
research/pubs/Nelson-Barreno-LEET-2008.pdf

[59] Nelson, B., Rubinstein, B.I., Huang, L., Joseph, A.D., Lee, S.J., Rao, S., Tygar, J.: Query strategies for evad-
ing convex-inducing classiﬁers. Journal of Machine Learning Research 13(May), 1293–1332 (2012), http:
//www.jmlr.org/papers/v13/nelson12a.html

[60] Newsome, J., Karp, B., Song, D.: Paragraph: Thwarting Signature Learning by Training Maliciously. In: Pro-
ceedings of the 9th International Conference on Recent Advances in Intrusion Detection. pp. 81–105. RAID’06,
Springer-Verlag, Berlin, Heidelberg (2006), http://dx.doi.org/10.1007/11856214_5

[61] Nguyen, A., Yosinski, J., Clune, J.: Deep Neural Networks are Easily Fooled: High Conﬁdence Predictions for

Unrecognizable Images. In: CVPR. pp. 427–436 (2015), https://arxiv.org/abs/1412.1897

[62] Papernot, N., McDaniel, P., Goodfellow, I.: Transferability in Machine Learning: from Phenomena to Black-Box
Attacks using Adversarial Samples. eprint arXiv:1605.07277 (2016), http://arxiv.org/abs/1605.
07277

[63] Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z.B., Swami, A.: Practical Black-Box Attacks against
Deep Learning Systems using Adversarial Examples. eprint arXiv:1602.02697 (2016), https://arxiv.
org/abs/1602.02697v3

[64] Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z.B., Swami, A.: The Limitations of Deep Learning in
Adversarial Settings. In: Proceedings of the European Symposium on Security & Privacy. pp. 372–387 (2016),
https://arxiv.org/abs/1511.07528

19

[65] Papernot, N., McDaniel, P., Wu, X., Jha, S., Swami, A.: Distillation as a Defense to Adversarial Perturbations
against Deep Neural Networks. eprint arXiv:1511.04508 (2015), http://arxiv.org/abs/1511.04508,
arXiv: 1511.04508

[66] Pinto, L., Davidson, J., Sukthankar, R., Gupta, A.: Robust Adversarial Reinforcement Learning. eprint

arXiv:1703.02702 (2017), https://arxiv.org/abs/1703.02702

[67] Rubinstein, B.I., Nelson, B., Huang, L., Joseph, A.D., Lau, S.h., Rao, S., Taft, N., Tygar, J.: ANTIDOTE: Un-
derstanding and defending against poisoning of anomaly detectors. In: Proceedings of the 9th ACM SIGCOMM
Conference on Internet Measurement. pp. 1–14. ACM (2009), https://people.eng.unimelb.edu.
au/brubinstein/papers/imc206-rubinstein.pdf

[68] Salahuddin, M.A., Al-Fuqaha, A., Guizani, M.: Reinforcement learning for resource provisioning in the vehicular

cloud. IEEE Wireless Communications 23(4), 128–135 (Aug 2016)

[69] Salahuddin, M.A., Al-Fuqaha, A., Guizani, M.: Software-Deﬁned Networking for RSU Clouds in Support of
the Internet of Vehicles. IEEE Internet of Things Journal 2(2), 133–144 (Apr 2015), http://ieeexplore.
ieee.org/abstract/document/6949072/

[70] Schaul, T., Quan, J., Antonoglou, I., Silver, D.: Prioritized Experience Replay. CoRR abs/1511.05952 (2015),

http://arxiv.org/abs/1511.05952

[71] Sengupta, S., Chakraborti, T., Kambhampati, S.: Securing Deep Neural Nets against Adversarial Attacks with
Moving Target Defense. eprint arXiv:1705.07213 (May 2017), http://arxiv.org/abs/1705.07213,
arXiv: 1705.07213

[72] Steinhardt, J., Koh, P.W., Liang, P.: Certiﬁed Defenses for Data Poisoning Attacks. eprint arXiv:1706.03691 (Jun

2017), http://arxiv.org/abs/1706.03691, arXiv: 1706.03691

[73] Sutton, R.S., Barto, A.G.: Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, ﬁrst edn.

(1998)

[74] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R.: Intriguing Properties of

Neural Networks. eprint arXiv:1312.6199 (2013), http://arxiv.org/abs/1312.6199

[75] Tramèr, F., Kurakin, A., Papernot, N., Boneh, D., McDaniel, P.: Ensemble Adversarial Training: Attacks and De-
fenses. eprint arXiv:1705.07204 (May 2017), http://arxiv.org/abs/1705.07204, arXiv: 1705.07204
[76] Wang, B., Gao, J., Qi, Y.: A Theoretical Framework for Robustness of (Deep) Classiﬁers against Adversarial
Examples. eprint arXiv:1612.00334 (2016), http://arxiv.org/abs/1612.00334, arXiv: 1612.00334
[77] Wang, Z., Chen, C., Li, H.X., Dong, D., Tarn, T.J.: A novel incremental learning scheme for reinforcement learn-
ing in dynamic environments. In: Proceedings of the 12th World Congress on Intelligent Control and Automation
(WCICA). pp. 2426–2431 (Jun 2016)

[78] Xiao, H., Xiao, H., Eckert, C.: Adversarial Label Flips Attack on Support Vector Machines. In: Proceedings of
the 20th European Conference on Artiﬁcial Intelligence. pp. 870–875. ECAI’12, IOS Press, Amsterdam, The
Netherlands, The Netherlands (2012), https://doi.org/10.3233/978-1-61499-098-7-870
[79] Xu, W., Evans, D., Qi, Y.: Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. eprint

arXiv:1704.01155 (2017), http://arxiv.org/abs/1704.01155

[80] Zhang, F., Chan, P.P.K., Biggio, B., Yeung, D.S., Roli, F.: Adversarial Feature Selection Against Evasion At-
tacks. IEEE Transactions on Cybernetics 46(3), 766–777 (Mar 2016), http://ieeexplore.ieee.org/
document/7090993/

[81] Zheng, S., Song, Y., Leung, T., Goodfellow, I.: Improving the Robustness of Deep Neural Networks via Stability

Training. eprint arXiv:1604.04326 (2016), https://arxiv.org/abs/1604.04326

20

