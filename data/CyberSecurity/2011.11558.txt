2
2
0
2

t
c
O
1
1

]
E
M

.
t
a
t
s
[

2
v
8
5
5
1
1
.
1
1
0
2
:
v
i
X
r
a

Beta-CoRM: A Bayesian Approach for n-gram Proﬁles
Analysis

Jos´e A. Perusqu´ıa1, Jim E. Griﬃn2, Cristiano Villa3

IIMAS – Universidad Nacional Aut´onoma de M´exico1
Department of Statistical Science – University College London2
School of Mathematics, Statistics and Physics – Newcastle University3

Abstract

n-gram proﬁles have been successfully and widely used where long sequences of
potentially diﬀering lengths are analysed for clustering or classiﬁcation. Mostly, ma-
chine learning algorithms have been used for this purpose but, despite their superb
predictive performance, these methods cannot discover hidden structure or provide a
full probabilistic representation of the data. That is why in this paper we centre our
attention on a novel class of Bayesian generative models designed for n-gram proﬁles
used as binary attributes. The ﬂexibility of our modelling allows us to consider a
straightforward approach to feature selection in this generative model. Furthermore,
we derive a slice sampling algorithm for a fast inferential procedure which is applied
to both synthetic and real data scenarios and shows that feature selection can improve
classiﬁcation accuracy.

Key words: Bayesian statistics, cyber security, feature selection, labeled data ,n-grams

1

Introduction

Classiﬁcation, as the statistical problem of identifying the group to which an observation belongs,
is at the core of many ﬁelds of research. From a practical point of view, the statistical methods and
algorithms used for this purpose depend, among other things, on the type of data, the number of
groups (if known a priori ) and if we have any prior knowledge on how the groups look, that is, if we
have access to labeled data. Although the process of labelling can be expensive, and perhaps quite
complicated, it provides the data with a rich structure that can be exploited to eﬃciently learn the
underlying generative process of the data and provide, among other things, accurate classiﬁcation
algorithms. In this direction, for certain applications, where observations are characterised by a set

1

 
 
 
 
 
 
of binary attributes, some of the most widely used models belong to the class of machine learning
algorithms; such as (boosted) decisions trees, random forests and support vector machine among
others.

The algorithms just mentioned, and in particular the boosted methods, have become quite
popular because of their excellent predictive performance; however, one major drawback of these
algorithms is that they can not be used for other tasks, such as ﬁnding unobserved structure in
the data or providing a probabilistic representation of uncertainty for which Bayesian inference is
particularly attractive. That is why in this paper we centre our attention on a novel class of Bayesian
supervised learning models designed for observations characterised by a set of binary attributes. In
particular, we are interested in n-gram proﬁles, which have been used for a long time in many ﬁelds
of research such as natural language processing (see e.g. Cavnar and Trenkle, 1994), cyber security
(see e.g. Kolter and Maloof, 2004; Geng et al., 2011) and genomics (see e.g. Tomovi´c et al., 2006),
to mention a few.

Broadly speaking, an n-gram is a contiguous sequence of n elements, that depending on the
ﬁeld of interest, can be letters, numbers, words, computer commands, etc. These are used to
summarise observations such as documents or computer programs. From a theoretical point of
view, summarising by n-grams sequences has the advantage that more information is retained
compared to using single elements as attributes. This is something that has been widely exploited
in natural language processing applications, like language detection, spelling correction or speech
recognition, where one of the main objectives is to predict the upcoming word given the previous n
words under Markovian assumptions. Prediction of new elements is not the only use of n-grams, and
their presence/absence or their frequencies can also be used as features to characterise observations.
These features are often used for clustering and classiﬁcation tasks such as anomaly detection.

Although n-grams have yielded good results in many ﬁelds of research, it is important to ac-
knowledge that there are still some theoretical and computational drawbacks that need to be kept
in mind when using them. For instance, the biggest computational challenge is the curse of di-
mensionality since even for small data sets the number of n-grams can easily reach the order of
hundreds of thousands or even billions (see e.g. Raﬀ et al., 2016), and hence, make it impossible to
use all n-grams in an analysis. That is why a feature selection procedure is required as a ﬁrst step.
Deciding how many and which n-grams to use is vital and has lead to an interesting and active
area of research. Within the computer science community, some of the most common criteria are
either frequency-based, e.g. using the features that appear at least K times (Geng et al., 2011), or
information-based, e.g. using the M most important features with respect to the mutual informa-
tion gain (Kolter and Maloof, 2004). Others have proposed more complex feature selection such as
Raﬀ et al. (2016), where a three step method was used to discover the best n-grams for classiﬁcation
tasks. We take the large number of n-grams as motivation for a Bayesian nonparametric approach
which yields a ﬂexible model whose complexity grows with the size of the data and propose a post-
hoc feature selection procedure which leads to improved out-of-sample classiﬁcation accuracy in our
examples.

2

In this paper we centre our attention on a new class of Bayesian models speciﬁcally built to
be used with n-gram proﬁles when the observations are grouped and assumed to be completely
characterised by the presence/absence of the n-grams, used as features. Our approach is based on
the underlying theory of compound random measures (CoRM’s) (Griﬃn and Leisen, 2016), which
allows us to deﬁne a hierarchy across groups controlled by a beta global distribution and a beta score
distribution at a group level. The beta-CoRM approach proposed here is a ﬂexible probabilistic
model for which a slice sampling method for the posterior and predictive inference is also derived.
CoRM’s are particularly attractive for n-gram proﬁles, since they allow us to construct correlated
measures that characterise and diﬀerentiate the groups. This is especially useful for ﬁnding the
most inﬂuential features in each group from a common list of features. Furthermore, the structure
of our model allows us to introduce a generalisation that can be used for the vital task of feature
selection in the high-dimensional setting of n-grams.

The remainder of the paper is as follows: in Section 2 we provide a more detailed description of
n-gram proﬁles and their applications; in Section 3 and Section 4 we present our proposed model and
the generalisation to allow a feature selection step; in Section 5 we detail the posterior inference
of the model where a slice sampling technique is developed for a faster update mechanism; in
Section 6 we present a prior sensitivity analysis, paying special attention to the generalised version;
in Section 7 we present the results of applying our proposed methodologies to real-case applications
found in the cyber security realm; lastly, in Section 8 we provide some general conclusions and
future work.

2 n-gram Proﬁles

Historically, n-grams have played a key role in many applications related to natural language process-
ing (Suen, 1979) since they tend to be robust against spelling errors, they contain more information
than using one element by itself, and are relatively simple to obtain and interpret (Tomovi´c et al.,
2006; Geng et al., 2011). Although n-gram models have yielded good results in most of the areas of
research where they have been used, they also represent a computational challenge because in cer-
tain applications the number of n-grams explodes. This is something that has been seen especially
in cyber security applications related to malware detection and classiﬁcation, where the number of
n-grams reach the order of millions and even billions (see e.g. Kolter and Maloof, 2004; Raﬀ et al.,
2016). For this reason, there is a need to ﬁrst decide which n-grams to use which can be done
through frequency-based, information-based or more complex feature selection methods.

Now, turning back our attention to n-grams, we can provide a meaningful deﬁnition by consid-
ering a set of T tokens (e.g. words, letters or computer commands) then an n-gram is a contiguous
sequence of n elements of T . In this way, the i-th observation (e.g. document or computer program)
is a list of Ki tokens for which there exist at most Ki − n + 1 diﬀerent n-grams. Hence, for a data
composed of N observations, we are then able to extract a set of at most (cid:80)N
i=1(Ki − n + 1) unique
n-grams which are further used as features to create proﬁles that completely identify groups and
observations within them. Thus, being able to accurately model these proﬁles, is vital to under-

3

standing the structure of the data for replication purposes and for classiﬁcation of new observations.
In practice two main approaches for this modelling have been considered: 1) modelling the number
of times each n-gram appears or 2) reducing the data to the presence or absence of each n-gram, that
is, binary features. In this paper, we will consider the latter, which is natural in the applications
that we consider.

Our interest mainly centres on analysing labeled data where we have access to a group indicator
for each observation and so N observations are grouped into d groups. For these N observations, we
are then able to extract a set of M diﬀerent n-grams that are assumed to completely characterise
the groups and the observations through their presence/absence. Since these are binary attributes
we can think of the data as a grouped binary matrix, that is, a binary matrix containing d sub
matrices with the j-th group having nj observations so that the total number of rows is (cid:80)
j nj = N
and the total number of columns is given by M . So, if we denote this grouped binary matrix by X,
we can deﬁne its entries as Xkji ∈ {0, 1} where i ∈ {1, ..., M }, j ∈ {1, ..., d} and k ∈ {1, ..., nj}, that
is, the rows of X are indexed by the pair (j, k) which respectively indicate the group and observation
within that group so that Xkji = 1 indicates that the k-th observation of the j-th group contains
the i-th n-gram and Xkji = 0 otherwise. We will write Xkj = (Xkj1, . . . , XkjM ).

The grouped binary matrices play a vital role in the Bayesian models we present in the next
two sections. As we thoroughly describe in the upcoming sections, the models considered allow us
to provide an individual probabilistic characterisation of the importance of each n-gram at a group
level which is used to diﬀerentiate groups. Furthermore, our modelling framework provides us with
the tools to perform feature selection to ﬁnd an optimal set of n-grams which provide the largest
diﬀerences between the groups and hence, maximise discrimination among them.

3 Beta-CoRM

of

d-dimensional

In this section, we present a novel Bayesian approach to supervised learning that builds on a special
type
measures
(CRM’s) (Kingman, 1967), known as compound random measures (CoRM’s) (Griﬃn and Leisen,
2016). Since their introduction in Kingman (1967), CRM’s have become essential for most Bayesian
nonparametric models. One of their key properties is their almost sure discreteness, which means
that their realisations are discrete with probability 1. In other words, if X is a CRM then,

completely

random

vector

of

X =

∞
(cid:88)

i=1

Jiδxi,

(1)

where both the locations, xi’s, and the jumps, Ji’s, are random. Moreover, X is characterised by
the L´evy measure ν that contains all the information about the distribution of the xi’s and the Ji’s.
This characteristic allows us to use CRM’s to model data generated by a discrete distribution or to
use them as the basic building block in mixture models.

4

A CoRM deﬁnes d correlated measures by perturbing the jumps of a CRM, that is, if µj

represents the j-th random measure then,

µj =

∞
(cid:88)

i=1

mjiJiδxi

m1i, ..., mdi

iid∼ h,

where the mji’s are the perturbation coeﬃcients that identify speciﬁc features of the j-th random
measure and h is called a score distribution. Therefore, CoRM’s are completely characterised
through the distribution h and the directing L´evy measure ν of the CRM. For a complete theoretical
study on CoRM’s and some of their applications the reader can refer to Griﬃn and Leisen (2016,
2018).

3.1 Construction

From the broad description given above, compound random measures are particularly attractive
for grouped data and hence, for supervised learning. Since our interest centres on the probabilistic
modelling of binary matrices, a natural approach would be to consider a beta-Bernoulli model. In a
Bayesian nonparametric framework this can be achieved by choosing a beta process (BP) B, as the
directing CRM on a suitable space Ω. One of the main features of this stochastic process is that it
concentrates the jumps in (0, 1) and hence they can be used as the parameters for Bernoulli random
variables. As a CRM, the beta process is completely characterised by its L´evy measure given by

ν(dω, dp) = c(ω)p−1(1 − p)c(ω)−1dpB0(dω),

where c(ω) is a concentration function and B0 is a ﬁnite ﬁxed measure on Ω.
In practice, it is
common to consider c(ω) = c, so that c is a concentration parameter. As for B0 this measure can
be continuous, discrete or a mix of both types. In the Bayesian nonparametric literature the most
common choice is absolutely continuous. This choice is particularly useful for factorial models like
the Indian buﬀet process (and related models) where the interest centres on inference about the
inﬁnite number of unknown latent factors (see e.g. Griﬃths and Ghahramani, 2005, 2011; Thibaux
and Jordan, 2007)

For an n-gram proﬁles analysis a factorial model could also be used to discover some latent
structure; however, in this paper we are interested in modelling the data directly. In order to do so,
a discrete base measure on the set of n-grams might be a more suitable choice, that is,

B0 =

∞
(cid:88)

i=1

qiδωi,

(2)

where the set of ωi’s work as labels to distinguish the diﬀerent n-grams, contrary to factorial models
where each ωi usually represents a latent factor that needs to be inferred. This is a particularly in-
teresting approach since the beta process will share the same atoms as B0 with corresponding jumps

5

pi sampled from a beta distribution (cqi, c(1 − qi)) and hence, B has the following representation

B =

∞
(cid:88)

i=1

piδωi.

(3)

By doing so, the set of jumps pi can be thought as the probability that an observation regardless
of the class has the corresponding n-gram, and for each of the d correlated groups these weights
are perturbed by the scores mji. The perturbed coeﬃcients mjipi represent the probability of
observing each n-gram for each of the d groups; choosing a score distribution on (0, 1) ensures that
these probabilities are properly deﬁned. For the purposes of this paper we restrict our attention to a
beta score distribution and in particular, to a beta(a, 1), that as we shall see in Section 4 represents
an interesting approach to feature selection when working with a generative model. In this way, for
the j-th group we have a marginal process given by

Bj =

∞
(cid:88)

i=1

mjipiδωi

mji

ind∼ beta(a, 1).

(4)

Finally, the generative process is fully described by assuming that each observation Xkj in group

j follows a Bernoulli process with corresponding base measure Bj, so that for k = 1, . . . , nj,

Xkj =

∞
(cid:88)

i=1

xkjiδωi

xkji ∼ Ber(mjipi).

(5)

We believe that this Bayesian model provides an interesting approach to supervised learning
with n-gram proﬁles. The structure of compound random measures allows us to represent the
importance of each feature for each of the classes through the perturbation coeﬃcients. This is
particularly useful in situations where we expect to diﬀerentiate classes through diﬀerences found
in some of the features.

3.2 Properties

Now that the model has been described it is important to analyse its properties to fully understand
the generative process and the role of the hyperparameters in the learning process. For the directing
discrete beta process we can obtain both the expectation and the variance, this will provide an
insight into the role of the the jumps qi and the concentration parameter c.

Proposition 1 Let B be a beta process with discrete base measure as in (3) and (2) respectively.
Then

1. E(B) = B0 and

2. Var(B) = 1
c+1

(cid:80)∞

i=1 qi(1 − qi).

The proof is straightforward and can be found in the Appendix A. The properties stated in
Proposition 1 provide important information about the parameters of the directing beta process

6

and the role they have in the generative process. For instance, the qi’s represents our prior knowledge
on the global probabilities, and the concentration parameter c, controls the similarity between the
a.s.→ qi, therefore, large values of c should be
pi’s and the qi’s. As c → ∞, Var(pi) → 0 and hence, pi
used when there is a strong prior belief that the qi’s are good estimates of the pi’s. On the other
hand, for values of c close to 0 then pi will be either close to 1 or 0 with probabilities qi and (1 − qi)
respectively.

For the correlated measures Bj’s, that characterise each group, useful properties can also be
derived, like their expectation and their variance. Moreover, since the shared jump pi introduces
dependence between the jump heights in each measure, the covariance and the correlation at each
location ωi can also be obtained. All this information is grouped in the following proposition, which
proof can be found in the Appendix A.

Proposition 2 Let B a beta process deﬁned as in Proposition 1 and Bj and Bk denote the j-th
and the k-th measure deﬁned as in (4), then

1. E(Bj|B) = a

a+1 B and hence E(Bj) = a

a+1 B0.

2. For i (cid:54)= l then Cov(Bj(dωi), Bj(dωl)) = 0.

3. For a ﬁxed feature ωi

Var(Bj(dωi)) =

(cid:18) aqi
a + 2

(cid:19) (cid:18) (1 − qi)(a + 1)2 + qi(c + 1)

(cid:19)

(c + 1)(a + 1)2

then for j (cid:54)= k,

and

Cov(Bj(dωi), Bk(dωi)) =

(cid:18) a

(cid:19)2 qi(1 − qi)

a + 1

c + 1

Corr(Bj(dωi), Bk(dωi)) =

a(a + 2)(1 − qi)
(a + 1)2(1 − qi) + qi(c + 1)

.

From these properties we can immediately obtain the probability of an observation having the

feature ωi, which does not depend on c and is given by

P(xkji = 1) = E(Bj(dωi)) =

a
a + 1

qi.

Then the joint distribution can be further generalised to consider all groups by simply obtaining
the d-th moment of a beta distribution with parameters (cqi, c(1 − qi)), yielding

P





d
(cid:89)

j=1


 = E

xkji = 1





d
(cid:89)

j=1



Bj(dωi)

 =

(cid:18) a

(cid:19)d

a + 1

E(pd

i ) =

(cid:18) a

(cid:19)d d−1
(cid:89)

a + 1

j=0

cqi + j
c + j

.

Finally, it is also interesting to notice that the covariance is the diﬀerence between the joint prob-
ability of two observations in diﬀerent groups having the feature ωi and the distribution assuming

7

independence, that is, for the n-th and m-th observation in the j-th and k-th group respectively,

Cov(Bj(dωi), Bk(dωi)) =

(cid:18) a

a + 1

(cid:19)2 (cid:18) cq2

i + qi
c + 1

(cid:19)

−

(cid:18) a

(cid:19)2

a + 1

q2
i .

4 Feature Selection

Now that the discrete beta-CoRM approach for grouped binary matrices has been fully described,
we present a generalisation of the model that yields a natural feature selection procedure, which will
be particularly useful if the feature space is a high-dimensional object and our goal is discrimination
about d groups. This approach arises naturally by noting that the density of the beta(a,1) score
distribution can be written as

axa−1 = xa
0

axa−1
xa
0

1(0,x0)(x) + (1 − xa
0)

axa−1
1 − xa
0

1(x0,1)(x)

= (1 − w)f (x) + wg(x),

where w = 1 − xa
0, and f (x) and g(x) are truncated beta distributions on (0, x0) and (x0, 1),
respectively. For small x0, this representation mimics the form of a spike-and-slab prior (see e.g.
Mitchell and Beauchamp, 1988; George and McCulloch, 1997; Ishwaran and Rao, 2005) with w the
probability of ”including” a variable and g(x) the slab distribution. The cumulative distribution
function (cdf) of g(x) is

G(x) =

xa − xa
0
1 − xa
0

,

and we deﬁne

˜G(x) = lim
a↓0

G(x) =

log(x0) − log(x)
log(x0)

,

with the corresponding probability density function (pdf)

˜g(x) =

1
log(1/x0)

1
x

,

x > x0.

Therefore, for ﬁxed, small x0, we can understand the prior distribution as a spike-and-slab prior
where a controls the size of the spike with a small value of a implying a small value of w. If a is
close to zero, the pdf of the slab is approximately ˜g(x). To have a better understanding of the shape
parameter a in the beta-CoRM model, in Figure 1 we present a graphical representation of the cdf
of the slab distribution for diﬀerent values of a and with x0 = .00001. Now, since the beta(a, 1)
random variables moderate pi, it can be immediately appreciated that for small a the prior expects
some of the scores mji to be close to zero and hence the respective products mjipi to be close to
zero as well and with w controlling the proportion close to ”zero”.

With this nice spike-and-slab interpretation of the beta distribution in mind, we can then pro-
vide a natural way to generalise the beta-CoRM model to taking into account a feature selection

8

Figure 1: Cumulative distribution function of the slab distribution for diﬀerent values of the
shape parameter a and x0 = .00001.

procedure by allowing each variable ωi to have a unique shape parameter ai, that is,

pi ∼ beta(cqi, c(1 − qi))

mji ∼ beta(ai, 1)

xkji ∼ Ber(mjipi)

j ∈ 1, . . . , d

k ∈ 1, . . . , nj,

with all the ai’s having a common distribution. Then the posterior estimates of these shape pa-
rameters should be used to determine which are the best discriminative features by noting that
features with a ”small” shape parameter should be preferred since they yield more distinguishable
groups which is particularly useful for supervised classiﬁcation. As we shall see in Section 6, this
adjustment can lead to better classiﬁcation performance.

5 Posterior Inference

Due to the discrete nature of the model, the joint posterior distribution of the correlated random
measures {Bj}j and the directing beta process B, is the product of the posterior distribution of the
random variables associated to each atom, that is, pi and the set of scores m1i, ..., mdi. Therefore,
we can analyse the posterior density on each atom. So, if we consider the i-th feature, the posterior
density (up to proportionality) is given by





d
(cid:89)

(pimji)x·ji(1 − pimji)nj −x·ji

j=1





(cid:16)

(1 − pi)c(1−qi)−1(cid:17) d

(cid:89)

pcqi−1
i

ma−1
ji

j=1

=

(cid:16)

pcqi+x··i−1
i

(1 − pi)c(1−qi)−1(cid:17) d

(cid:89)

mx·ji+a−1

ji

(1 − pimji)nj −x·ji,

(6)

j=1

where x·ji = (cid:80)nj
j=1 x·ji. From (6) we can immediately appreciate that due to
the presence of the d terms (1 − mjipi), the joint and the conditional distributions do not have a

k=1 xkji and x··i = (cid:80)d

9

known form. Therefore, a Gibbs sampling algorithm cannot be applied directly to this posterior
distribution. One way to address this issue is with the introduction of a set of latent variables {ukji}
that allows us to deﬁne an artiﬁcial measure Bkj as the base measure for the k-th observation in
the j-th group, that is,

Bkj =

Xkj =

(cid:88)

i
(cid:88)

i

ykjipiδωi

xkjiδωi

ykji = 1{ukji < mji} ∼ Ber(mji)

xkji ∼ Ber(ykjipi).

(7)

This approach is based on the idea of slice sampling (Damien et al., 1999), where a set of
latent variables that preserve the marginal distribution are introduced. Slice sampling schemes
have become widely used in Bayesian nonparametric models since they yield eﬃcient computational
methods for the inﬁnite dimensional objects that are at their core. The reader can refer to Griﬃn
and Holmes (2010) and the references therein for an overview on the computational issues found
in some nonparametric models and the approaches used to address them. As for the normalised
completely random measures, the slice sampling technique is useful in order to introduce a random
truncation point and hence, consider only a random ﬁnite number of jumps. In our case the slice
sampling approach that we propose allows us to create these inﬁnite activity measures {Bkj}k,j,
that yield a suitable augmented likelihood from which we can recover the original one by integrating
out the latent variables as stated in the following Lemma.

Lemma 1 The discrete beta-CoRM deﬁned by equations (3), (4) and (5) is equivalent to the aug-
mented model in (7).

In order to prove Lemma 1 it is suﬃcient to note that the likelihood for a speciﬁc observation
xkji is a mixture of a degenerate distribution and a Bernoulli distribution with parameter pi with
respective weights (1 − mji) and mji. The complete details can be found in the Appendix A. Now,
with this form of the likelihood, the complete augmented posterior (up to proportionality) is given
by

(cid:16)

pcqi−1
i

(1 − pi)c(1−qi)−1(cid:17)





d
(cid:89)

nj
(cid:89)

j=1

k=1

(cid:0)δxkji
0

(cid:1)(1−ykji)

pxkjiykji
i

(1 − pi)(1−xkji)ykji







×



(cid:34)

d
(cid:89)

j=1

ma−1
ji

nj
(cid:89)

k=1

mykji
ji

(1 − mji)1−ykji

(cid:35)
 .

(8)

From (8) we can immediately notice that the conditional posterior distributions are

pi|{xkji}, {ykji} ∼ beta





d
(cid:88)

nj
(cid:88)

xkjiykji + cqi,

d
(cid:88)

nj
(cid:88)

(1 − xkji)ykji + c(1 − qi)





j=1

k=1

j=1

k=1

10

(cid:32)

mji|{ykji} ∼ beta

a +

nj
(cid:88)

k=1

ykji, 1 + nj −

(cid:33)

ykji

nj
(cid:88)

k=1

ykji|xkji, mji, pi ∼




δ1



Ber

(cid:16) (1−pi)mji
1−pimji

(cid:17)

if

if

xkji = 1

xkji = 0

j ∈ {1, ..., d}

k ∈ {1, ..., nj}.

Hence, a straightforward Gibbs sampling algorithm can be used for the posterior inference. Clearly,
it is also important to notice that this slice sampling technique is also valid when we use individual
shape parameters ai rather than a global parameter a. In this case, however, the posterior distri-
bution of the scores {mji}d
k=1 ykji and the
rest remains the same. The full conditional distributions of the other parameters are as follows.

j=1 is a beta distribution with shape parameter ai + (cid:80)nj

5.1 Full conditional distribution of c

For the prior speciﬁcation of the concentration parameter we can recall from Section 3.2 that c is
a positive parameter that primarily modulates the variance of the weights of the beta process, so
a.s.= qi ∀i. Then, using (8) we have that the full conditional density of c is
that when c → ∞, pi
proportional to

f (c|p, q, θc) ∝

(cid:32) M
(cid:89)

pcqi−1
i

(1 − pi)c(1−qi)−1

(cid:33)

B(cqi, c(1 − qi))

i=1

f (c|θc)

where f (c|θc) is the prior distribution, θc the set of hyperparameters and M the number of active
features. Clearly, this full conditional distribution does not have a closed form and to sample from
it we follow the adaptive random walk Metropolis Hasting (Atchad´e and Rosenthal, 2005) on log c
so that a suitable Gaussian distribution can be used as the proposal distribution.

Full conditional distribution of a

For the full conditional distribution of the shape parameter a we ﬁrst notice that the full conditional
density is proportional to

f (a|M, θa) ∝ aM d exp



a

d
(cid:88)

M
(cid:88)

j=1

i=1



log mji

f (a|θa),

where f (a|θa) is the prior distribution and θa the corresponding hyperparameters. In this case and
contrary to the concentration parameter we can directly observe that if a ∼ gamma(λ, α) then we
obtain a conjugate model so that the full conditional distribution of a is gamma with parameters
(λ + M d, α − (cid:80)d
i=1 log mji). Of course, we acknowledge that other prior distributions could
be used and a Metropolis Hasting step can be used to sample from the posterior.

(cid:80)M

j=1

11

Full conditional distribution of ai

The full conditional distribution for the individual shape parameters ai follows the same reasoning
and steps as for the global parameter a by noting that in this case the full conditional density of
each ai is proportional to

f (ai|M, θai) ∝ ad

i exp

ai





log mji

f (ai|θai),

d
(cid:88)

j=1

where f (ai|θai) is the common distribution of the shapes with hyperparameters θai. We can then
notice that for computational ease a gamma(α, β) prior could be used so that the full conditional of
each shape is a gamma distribution with parameters (α + d, β − (cid:80)d
j=1 log mji). In this case however,
we are also interested in analysing the eﬀects of diﬀerent priors since there is an inherent interest on
doing feature selection. In particular, we are interested on scale-mixture gamma priors that allow
us to deﬁne local and global shrinkage priors while still having a conjugate posterior for the shape
parameters, such as the gamma-gamma prior introduced in Griﬃn and Brown (2017), that is,

ai|λ, αi ∼ gamma (λ, αi)

αi|φ, κ ∼ gamma (φ, κ) ,

(9)

where κ is a scale parameter, λ controls the behaviour of the distribution close to zero and φ controls
the tail of the distribution. It is worth noticing that if λ = 1 we recover the Lomax distribution
(Lomax, 1954) which is a heavy-tailed distribution closely related to the Pareto types I and II
distributions. More general scale-mixture gamma priors such as the double gamma prior (Bitto
and Fr¨uhwirth-Schnatter, 2019) or the triple gamma prior (Cadonna et al., 2020) could also be
considered.

6 Prior Sensitivity Analysis

To analyse the eﬀects of the prior on the beta-CoRM models proposed in the previous sections,
we consider a synthetic data set composed of 5 imbalanced groups with 250 total observations
and 300 binary features as graphically represented in Figure 2. We are interested in looking at
the computational cost of posterior inference and the impact of feature selection on the predictive
performance of the models. It is important to notice that for the predictive performance, the test
set has been generated using the same parameters as the synthetic data. Furthermore, and for
illustrative purposes, on the feature selection process we obtain the optimal number of features by
using the true labels of the test set. Of course, we acknowledge that on real-world applications this
is not possible and hence, to ﬁnd the“best” features we could rely on a cross validation procedure.

12

Figure 2: Synthetic data set composed of 5 imbalanced groups separated by the red lines
with 250 total observations and 300 binary features.

6.1 Prior Selection

Since interest mainly centres on the prior structure given to the shape parameter(s) a (a(cid:48)
is), we
consider the same vague gamma prior on the concentration parameter c for the diﬀerent prior setups.
As for the shape parameters, we can recall from Section 5.1 that we are mainly interested in scale-
mixture gamma priors and gamma vague priors since they allow us to have a conjugate structure
on some of the parameters while introducing a shrinkage structure ideal for feature selection. In
particular, we centre our attention on vague gamma priors for λ and α and on the gamma-gamma
prior structure described in (9), since it allows us to study quite interesting setups for diﬀerent
values of (λ, φ, κ) such as the Lomax prior with λ = 1, the objective Lomax prior (Walker and
Villa, 2021) with λ = φ = κ = 1 and a half-Cauchy type prior with λ = φ = 0.5. With respect to
the posterior inference, it is important to mention that for all these models we consider an eﬀective
sample size of 1000 observations obtained from the posterior distribution using the MCMC scheme
detailed in Section 5, with 251,000 iterations, a burn-in of 1000 and a thinning of 250 to ensure
samples with low-correlation for the parameters of the beta-CoRM models. As of the computational
time, and despite the large number of simulations, the four models required around 12 minutes to
ﬁnish the posterior inference.

6.2 Posterior Inference Analysis

With respect to the posterior inference analysis, there are several aspects of the simulations results
that we would like to contrast among the ﬁve prior structures considered. Firstly, in Table 1 we
present the median and the 95% credible interval for the only common parameter, that is, the
concentration parameter c. It can be noted that quite consistent results are obtained with the only
exception of the objective Lomax prior that yields values slightly shifted to the left.

Now we turn our attention to which might be the most interesting parameter for the beta-CoRM
model which is the shape parameter a and the shape parameters for the generalised versions. For
this the ﬁrst thing we would like to notice is the median and 95% credible interval in the beta-CoRM
model displayed in Table 2.

13

Model/Prior
Beta-CoRM/vague gamma
Gen. beta-CoRM/vague gamma
Gen. beta-CoRM/objective Lomax
Gen. beta-CoRM/Lomax
Gen. beta-CoRM/half-Cauchy

Lower Interval Median Upper Interval
5.7047
5.7098
5.4119
5.7183
5.8403

4.8134
4.7233
4.4747
4.7065
4.8882

6.8216
6.8284
6.4652
6.8587
7.0668

Table 1: Median and 95% credible interval comparison for the concentration parameter.

Model
Beta-CoRM

Lower Interval Median Upper Interval
1.6397

1.5317

1.7519

Table 2: Median and 95% credible interval comparison for the shape parameter

From these results, we can immediately appreciate a highly-concentrated posterior distribution.
This is certainly an interesting result since we can now compare this global behaviour against the
individual shape parameters of the generalised beta-CoRM models with diﬀerent prior structures.
To this end, in Figure 3 we graphically present the posterior mean of the shape parameters for the
generalised beta-CoRM model with vague gamma prior, objective Lomax prior, Lomax prior and
half-Cauchy type prior. We note how with the vague gamma prior we obtain shape parameters
within the credible interval of a. In contrast, for the gamma-gamma prior cases, we are able to see a
more deﬁned shrinkage eﬀect ideal for the feature selection purposes of the generalised beta-CoRM.
However, it is clear that this eﬀect is more pronounced for the objective Lomax and the half-Cauchy
type priors, whereas for the Lomax prior we get a less variable behaviour for the shape parameters.
This is something that can be explained by the prior choices on (φ, κ) and the posterior behaviour
on the ones that are not ﬁxed as seen in Table 3 and Table 4.

Model/Prior
Gen. beta-CoRM/objective Lomax
Gen. beta-CoRM/Lomax
Gen. beta-CoRM/half-Cauchy

Lower Interval Median Upper Interval
1
185.4517
0.5

1
727.0159
0.5

1
37.7230
0.5

Table 3: Median and 95% credible interval comparison for φ.

Model/Prior
Gen. beta-CoRM/objective Lomax
Gen. beta-CoRM/Lomax
Gen. beta-CoRM/half-Cauchy

Lower Interval Median Upper Interval
1
370.5593
1.8380

1
1432.916
2.3846

1
73.0926
1.4220

Table 4: Median and 95% credible interval comparison for κ.

14

Vague gamma

Objective Lomax

Lomax

Half-Cauchy

Figure 3: Posterior mean estimates of the shape parameters ai’s for the generalised beta-
CoRM with diﬀerent hyperpriors.

6.3 Feature Selection and Predictive Performance

Now that we have compared the posterior inference of the beta-CoRM models and the impact of
the prior we can turn our attention to the feature selection procedure of the generalised beta-CoRM
models. To this end we are mainly interested on the optimal number of features, the threshold at
which they are found and of course, the impact on the predictive performance. Firstly, in Table 5 we
present the results on the ﬁrst two characteristics mentioned above for the generalised beta-CoRM
models. Finally, to fully understand the impact of the prior in Table 6 we present the predictive
performance of the ﬁve beta-CoRM models using four widely used metrics in the literature when
dealing with imbalanced sets (see e.g. Olson and Delen, 2008), which are: accuracy (Acc.), precision
(Prec.), recall (Rec.) and the F1-score (F1). For this synthetic data set we are then able to see
that the best results with respect to the predictive performance of the model correspond to the
generalised beta-CoRM with vague gamma and Lomax priors.

Model/Prior
Gen. beta-CoRM/vague gamma
Gen. beta-CoRM/objective Lomax
Gen. beta-CoRM/Lomax
Gen. beta-CoRM/half-Cauchy

Optimal Features Threshold
1.646140
2.441105
2.804267
122.388788

238
215
280
234

Table 5: Optimal number of features and threshold for the generalised beta-CoRM model
with diﬀerent priors.

15

Model/Prior
Beta-CoRM/vague gamma
Gen. beta-CoRM/vague gamma
Gen. beta-CoRM/objective Lomax
Gen. beta-CoRM/Lomax
Gen. beta-CoRM/half-Cauchy

Acc.(%) Prec.(%) Rec.(%) F1(%)
85.84
88.57
86.60
87.85
86.24

85.87
88.42
86.06
87.32
85.59

86.48
89.57
87.61
88.97
87.57

85.60
87.60
86.40
87.20
86.40

Table 6: Computational cost and predictive performance of the ﬁve beta-CoRM models.

7 Application to Cyber Security

The use of n-grams as binary features has been considered for detecting masquerade attacks using
UNIX commands (see e.g. Geng et al., 2011), for detecting and classifying malicious software (see
e.g. Kolter and Maloof, 2004; Pekta¸s et al., 2011) using the binary content of the malware and for
characterising the behaviour of a program using the kernel calls in order to detect anomalies (see e.g.
Marceau, 2001). From a Bayesian perspective, these applications just as other cyber security tasks
represent a relatively new, interesting and challenging area of research since most of the methods
used to detect the attacks belong to the class of machine learning supervised models. For the
application considered in this paper we centre our attention on the malware classiﬁcation problem
for which we are able to straightforward apply and see the advantages of the beta-CoRM models in
particular of the generalised versions that allows us to perform a feature selection process to select
an optimal set of features and reduce the uncertainty of the data.

7.1 Malware Detection and Classiﬁcation

Malware is a computational term that is commonly used to describe any software speciﬁcally de-
signed to disrupt, damage or gain access to a computer system. Traditionally, the use of antivirus
software has been essential in order to detect malicious code and to keep the computer systems
protected. Antivirus software usually makes use of the blacklisting method, where a new program
is scanned in search of signatures of known malware and if found, the program is disabled and a
warning is ﬂagged. This approach is eﬀective for detecting known threats; however, it has been
proved to be less eﬀective with new threats and with slight modiﬁcations made to the original code
to avoid recognition (McGraw and Morrisett, 2000). A thorough review of statistical methods to
deal with intrusion detection, including malware detection and classiﬁcation, can be found in Pe-
rusqu´ıa et al. (2022). In recent years, machine learning and statistical approaches have been used
as an alternative to the blacklisting method and several approaches have been proposed in order to
detect malicious code. One of such approaches is precisely by analysing the executable content of
the malware through n-gram proﬁles (see e.g. Kolter and Maloof, 2004; Pekta¸s et al., 2011).

It is important to remark that malware detection is not the only task required when dealing
with malicious software. In order to understand their infectious process, their potential threat level
and therefore, how to be well-protected against these malicious software, there is a need to correctly

16

identify the family to which they belong. Accurate classiﬁcation may also speed-up the process of
reverse-engineering to ﬁx computer systems that were infected as well as for developing security
patches to prevent more computers to become infected. Of course, this classiﬁcation task can also
be done through an n-gram proﬁle analysis of the hexadecimal code.

Before introducing the data used it is important to remark that in computer science, the byte
is the basic unit of information for storage and processing, and it is most commonly represented by
a sequence of 8 binary digits or bits. Every instruction given to a computer can be broken down
into sequences of bytes, which form the instruction’s binary code. These binary sequences can be
expressed in a more condensed form using the hexadecimal notation that is, each byte is represented
as the combination of two elements of the set {0, 1, ..., 9, A, B, ..., F }. For example, considering the
hexadecimal representation of the code extract of a malicious software, given by 00 00 1C 40 2A 28,
and n = 4 we take all the possible sequences of 4 contiguous bytes to create the set of 4-grams, that
is {00001C40, 001C402A, 1C402A28}. This data processing is then performed in the complete code
for all the malicious and benign software (if applicable).

7.1.1 Data

The data used is a subset of the data released as part of the Microsoft Malware Classiﬁcation
Challenge (Ronen et al., 2018) hosted at Kaggle in 2015. The data is composed of 842 malware
representing a mix of nine diﬀerent families. For each of these 842 malware we have the label
representing the true family and the ﬁle with the hexadecimal representation of the binary code.
These malware were further split into a training set composed of 590 observations and a test set
of 292 elements. Taking into account this 590 malware the number of unique 4-grams reached the
order of 10 million that is why we decided to centre our attention on the 4-grams that appeared
at least once in each family yielding a unique set of 826 features. This extreme criterion allows us
to consider 4-grams that are likely to appear in new observations which is something vital to take
into consideration. Of course, other criteria could be used to reduce the number of initial features;
however, this is something outside the scope of this paper. In Figure 4 both the training and the
test set are graphically represented.

Training Data

Test Data

Figure 4: Malware data set split into training set (left) and test set (right). For each plot
the nine families are separated by the red horizontal lines. The black dots represent that a
feature is present within the respective malware hexadecimal code.

17

From Figure 4 we are able to directly observe a clear and deﬁned structure in some of the families,
such as family three, where there are some predominant features that could help us discriminate
new malware. However, it is also true that some of the families are not that well deﬁned like families
one, six and nine. This of course might be due to the way the initial set of features has been chosen,
however, it also provides us with a nice motivation for the generalised beta-CoRM models since then
we should be able to extract an optimal set of features to create a reﬁned n-gram proﬁles analysis.

7.1.2 Posterior Inference Results

In this section we present and discuss some interesting results of the four beta-CoRM models
considered so far. In this case since the data set is almost 6.5 times bigger than the synthetic data
we only considered 101,000 iterations for all the algorithms with a burn-in of 1000 and a thinning
of 100 for an eﬀective sample size of 1000.
In this direction it is compelling to remark that all
the algorithms roughly required 30 minutes to run contrary to the synthetic data where it was
observed that the beta-CoRM and the generalised beta-CoRM with an objective Lomax prior and
half-Cauchy prior required half the time to ﬁnish the posterior inference routines. Now, as for some
of the results ﬁrst we would like to compare the posterior samples of some common parameters.

Firstly, in Table 7 we present the median and the 95% credible interval of the concentration
parameter c for all the models considered so far. We can then appreciate a slight diﬀerence between
the median and the intervals of the beta-CoRM models with vague gamma priors and the generalised
version with gamma-gamma priors.

Model/Prior
Beta-CoRM/vague gamma
Gen. beta-CoRM/vague gamma
Gen. beta-CoRM/objective Lomax
Gen. beta-CoRM/Lomax
Gen. beta-CoRM/half-Cauchy

Lower Interval Median Upper Interval
2.2292
2.2252
2.1691
2.1873
2.1587

2.0161
2.0273
1.9517
1.9814
1.9485

2.4829
2.4512
2.4140
2.4285
2.3963

Table 7: Median and 95% credible interval comparison for the concentration parameter.

Now we centre our attention on the shape parameter for the beta-CoRM and the shape param-
eters for the generalised versions, since as detailed throughout the paper these parameters play a
vital role in the feature selection step. In Table 8 we ﬁrst present the median and credible interval
of the shape parameter for the beta-CoRM model, from which we can notice that the posterior
distribution exhibits a low variance.

Model
Beta-CoRM

Lower Interval Median Upper Interval
0.7599

0.7792

0.7407

Table 8: Median and 95% credible interval comparison for the shape parameter

Lastly, in Figure 5 we present the posterior mean estimates of the shape parameters ai’s for the
generalised beta-CoRM under the four prior structures considered so far. We can then again notice

18

how the generalised beta-CoRM model with vague gamma priors yields shape parameters, a(cid:48)
is,
within the credible interval of the shape parameter a of the beta-CoRM. Then and just as expected,
the gamma-gamma prior exhibits a shrinkage behaviour which certainly helps the motivation behind
the feature selection process. In particular, we can again appreciate how the eﬀect of having quite
isolated shapes is more predominant for the objective Lomax and half-Cauchy type prior, whereas
for the Lomax the variability is more controlled.

Vague gamma

Objective Lomax

Lomax

Half-Cauchy

Figure 5: Posterior mean estimates of the shape parameters ai’s for the generalised beta-
CoRM with diﬀerent hyperpriors.

7.1.3 n-gram Proﬁles Analysis and Feature Selection

Now that we have a better sense on the inference, we can proceed to the analysis of the n-gram
proﬁles produced by the beta-CoRM models. First, in Figure 6 we display the graphical represen-
tation of the posterior predictive probabilities for each of the 826 4-grams across the nine families
of malware. It is immediate to notice that the ﬁve 4-gram proﬁles are quite similar and visually
it is diﬃcult to see any meaningful diﬀerences among each other. To have a better sense of this
we obtain the pair-wise Euclidean distance of the proﬁles and present the results in Table 9. We
can appreciate that beta-CoRM models with vague gamma priors yield the most similar proﬁles
followed by the model with objective Lomax and half-Cauchy type prior.

Of course, it is important to remember that the main advantage of the generalised models is
that they allow us to ﬁnd an optimal number of features to reduce the uncertainty in the data
and which can be further used to improve the predictive performance of the model. To this end
and contrary to the synthetic data we obtain the optimum threshold and the optimal features by
using the training set as our validation set as well. With this in mind, we present in Table 10 the
results on the training set including the maximum accuracy achieved, the threshold used and the

19

Proﬁle
1
2
3
4
5

1
0
0.1704
0.4214
0.3702
0.4370

2
0.1704
0
0.4160
0.3623
0.4304

3
0.4214
0.4160
0
0.1939
0.1722

4
0.3702
0.3623
0.1939
0
0.2028

5
0.4370
0.4304
0.1722
0.2028
0

Table 9: Euclidean distance of the n-gram proﬁles of the beta CoRM (Proﬁle 1) and the
gen. beta-CoRM models with vague gamma (Proﬁle 2), objective Lomax (Proﬁle 3), Lomax
(Proﬁle 4) and half-Cauchy (Proﬁle 5) priors.

Beta-CoRM

Vague gamma

Objective Lomax

Lomax

Half-Cauchy

Figure 6: 4-gram proﬁles represented by the posterior predictive probabilities for the beta-
CoRM and the gen. beta-CoRM with diﬀerent hyperpriors.

number of features required. Clearly, with the generalised versions we are able to better explain the
data with the objective Lomax prior being the best one. Furthermore, we can again appreciate the
similarities between the models with vague gamma priors and the ones having the gamma-gamma
prior structure.

With respect to the feature selection it is further interesting to notice that the four generalised
models coincide in 98 features, and this number increases to 119 common features if we just consider

20

Model/Prior
Beta-CoRM/Vague Gamma
Gen. beta-CoRM/Vague Gamma
Gen. beta-CoRM/Objective Lomax
Gen. beta-CoRM/Lomax
Gen. beta-CoRM/half-Cauchy

Accuracy (%) Threshold Features

80.51
85.59
85.93
84.75
85.08

N/A
0.7565
0.6094
0.6152
0.5922

826
116
124
129
124

Table 10: Prediction results on the training set by the generalised beta-CoRM models with
vague gamma, objective Lomax, Lomax and half-Cauchy type priors respectively.

the generalised beta-CoRM models with gamma-gamma priors. To fully understand the process of
feature selection on this noisy data set we present in Figure 7 the data restricted to the optimal
ﬁgures for the four generalised beta-CoRM models. Then we are able to see better deﬁned groups
with the exception of the ﬁrst, sixth and last family. Nevertheless the models are still able to
identify some inﬂuencing features in the groups, which we can exploit to improve the predictive
performance of the original model on the test set as seen in Table 11.

Model/Prior
beta-CoRM/vague gamma
Gen. beta-CoRM/vague gamma
Gen. beta-CoRM/objective Lomax
Gen. beta-CoRM/Lomax
Gen. beta-CoRM/half-Cauchy

Acc. (%) Prec. (%) Rec. (%) F1 (%)
80.44
84.00
84.11
83.36
84.07

82.23
84.82
84.85
84.14
84.84

80.16
83.73
84.13
83.33
83.73

81.22
84.86
84.97
84.35
85.09

Table 11: Predictive performance of the ﬁve beta-CoRM models.

8 Conclusions

n-gram proﬁles are an important tool that has been widely used in many ﬁelds of research to
discover unique properties that characterise the observations and the classes to which they belong.
Depending on the application the modelling of n-gram proﬁles can either exploit the presence or
absence or their frequencies. In this paper, we have centred our attention on the former since this
approach has been used for cyber security applications that clearly represent a challenging and
interesting opportunity for Bayesian statistics due to the growing demand of robust algorithms and
methods capable of defending computer networks. Of course, as we have established throughout the
paper, the use of n-grams can yield a large number of features to be considered which is unfeasible
from a practical point of view. To address this, we have designed a ﬂexible model that allows us to
consider a suitable and straightforward generalisation to perform a feature selection procedure to
ﬁnd an optimal subset of features.

Furthermore, we have considered a deep prior sensitivity analysis for which we have discovered
how certain priors exhibit more clearly the shrinkage eﬀect required for the feature selection. Al-

21

Vague gamma

Objective Lomax

Lomax

Half-Cauchy

Figure 7: Restricted data on the optimal features (blue) for generalised beta-CoRM model
with diﬀerent hyperpriors.

though, it is clear that after a long time running the posterior simulations we have reached quite
consistent results regardless of the prior chosen. Of course, it is important to remark that other
non-conjugate shrinkage priors could be used; however, the computational cost might increase. This
is something that we need to always keep in mind whenever dealing with applications like cyber
security where fast and scalable algorithms are required. Finally, and from a cyber security point
of view there is deﬁnitely more interesting research and applications where we could exploit the
beta-CoRM models. For example, we have seen that n-gram proﬁles have also been used to detect
masquerade attacks, where supervised learning models able to learn malicious activity without any
samples, that is one class algorithms, are required. We believe that the beta-CoRM models could
be used as well in this kind of applications; however, this might require a slightly diﬀerent approach
since in this paper we are assuming we have access to both benign and malicious observations.

Acknowledgments

The authors would like to acknowledge in ﬁrst place the University of Kent where most of the results
presented in this paper were carried out. Secondly, we would like to thank the anonymous reviewers
for their insightful and helpful comments on the ﬁrst version of the paper.

22

A Proofs

Proof of Proposition 1

Proving the properties established in Proposition 1. only requires to remark that pi are beta dis-
tributed with parameters (cqi, c(1 − qi)). Therefore E(pi) = qi, and using the monotone convergence
theorem we get

E(B) = E

(cid:33)

piδωi

=

(cid:32) ∞
(cid:88)

i=1

∞
(cid:88)

i=1

E(pi)δωi =

∞
(cid:88)

i=1

qiδωi = B0.

Following the same monotone convergence reasoning and using the fact that E(p2
it can be shown that

i ) = qi(1−qi)

c+1 + q2
i

E(B2) = B2

0 +

1
c + 1

From which the variance can be obtained directly.

Proof of Proposition 2

∞
(cid:88)

i=1

qi(1 − qi).

The ﬁrst property is straightforward since it follows the same reasoning as in Proposition 1, and
the second property is a direct consequence of the scores and jumps being mutually independent.
Now, for the variance,

Var(Bj(dωi)) = E(B2

j (dωi)) − E(Bjd(ωi))2 = E(m2

i ) − E(mjipi)2

(cid:18)

(cid:18)

a
(a + 1)2(a + 2)
a
(a + 1)2(a + 2)

+

+

(cid:19) (cid:18) qi(1 − qi)

c + 1

a2
(a + 1)2
a2
(a + 1)2
(cid:18)
(cid:19)

+

jip2
(cid:19) (cid:18) qi(1 − qi)

c + 1

(cid:19) (cid:18) qi(1 − qi)

(cid:19)

c + 1
a
(a + 1)2(a + 2)

(cid:19)

q2
i

(cid:19) (cid:18) (1 − qi)(a + 1)2 + qi(c + 1)

(cid:19)

(c + 1)(a + 1)2

.

(cid:18) a

a + 2
(cid:18) aqi
a + 2

=

=

=

=

(cid:19)

(cid:18) a

(cid:19)2

+ q2
i

−

q2
i

(cid:18)

+

a + 1
a
(a + 1)2(a + 2)

(cid:19)

q2
i

From the previous expressions it can be clearly appreciated that for a ﬁxed feature, the variance
is the same across families, that is Var(Bj(dωi)) = Var(Bk(dωi)). This will be useful in order to
obtain the correlation. But ﬁrst, for the covariance between Bj(dωi) and Bk(dωi) we have from the
ﬁrst property in this proposition that

E(Bj(dωi)) =

(cid:18) a

(cid:19)

a + 1

B0(dωi) =

(cid:18) a

(cid:19)

a + 1

qi = E(Bk(dωi)),

and

E(Bj(dωi)Bk(dωi)) = E(mjimkip2

i ) =

(cid:18) a

(cid:19)2

a + 1

E(p2

i ) =

(cid:18) a

(cid:19)2 (cid:18) qi(1 − qi)

a + 1

c + 1

(cid:19)

,

+ q2
i

23

therefore,

Cov(Bj(dωi), Bk(dωi)) = E(Bj(dωi)Bk(dωi)) − E(Bj(dωi))E(Bk(dωi))

=

=

(cid:18) a

(cid:19)2 (cid:18) qi(1 − qi)

a + 1

c + 1

(cid:19)

−

+ q2
i

(cid:18) a

(cid:19)2

a + 1

q2
i

(cid:18) a

(cid:19)2 (cid:18) qi(1 − qi)

(cid:19)

a + 1

c + 1

.

Hence, the correlation is given by

Corr(Bj(dωi), Bk(dωi)) =

Cov(Bj(dωi), Bk(dωi))
Var(Bj(dωi))

=

=

(cid:18) a

(cid:19)2 (cid:18) qi(1 − qi)

a + 1

c + 1

(cid:19) (cid:18)

(cid:19) (cid:18) a + 2
aqi

(c + 1)(a + 1)2
(1 − qi)(a + 1)2 + qi(c + 1)

(cid:19)

a(a + 2)(1 − qi)
(1 − qi)(a + 1)2 + qi(c + 1)

.

Proof of Lemma 1.

Let us consider ﬁrst the augmented model. In this case it is straightforward to see that conditioned
a.s.= 0 and conditioned on ykji = 1 we have that xkji ∼ Ber(pi). With
on ykji = 0 we have that xkji
this in mind, the augmented likelihood is

d
(cid:89)

nj
(cid:89)

j=1

k=1

(cid:0)δxkji
0

(cid:1)(1−ykji) (cid:16)

pxkji
i

(1 − pi)(1−xkji)(cid:17)ykji

,

and the posterior distribution is proportional with respect to the latent variables to

d
(cid:89)

nj
(cid:89)

j=1

k=1

(cid:0)δxkji
0

(cid:1)(1−ykji) (cid:16)

pxkji
i

(1 − pi)(1−xkji)(cid:17)ykji

mykji
ji

(1 − mji)(1−ykji).

Integrating out the latent variables yields,

d
(cid:89)

nj
(cid:89)

j=1

k=1

δxkji
0

(1 − mji) + mjipxkji

i

(1 − pi)(1−xkji).

(10)

Therefore, we can appreciate that the marginal posterior is the product of the mixture of a de-
generate distribution and a Bernoulli distribution with corresponding weights (1 − mji) and mji.
This expression at ﬁrst sight does not resemble the posterior distribution for the beta-CoRM model.
However, it is suﬃcient to notice that from (10) we obtain

δxkji
0

(1 − mji) + mjipxkji

i

(1 − pi)(1−xkji) =




1 − mji + mji(1 − pi) = 1 − mjipi



mjipi

24

if xkji = 0

if xkji = 1.

Hence, we recover the original posterior distribution.

References

Atchad´e, Y. F. and Rosenthal, J. S. (2005). On adaptive Markov chain Monte Carlo algorithms.

Bernoulli, 11(5):815 – 828.

Bitto, A. and Fr¨uhwirth-Schnatter, S. (2019). Achieving shrinkage in a time-varying parameter

model framework. Journal of Econometrics, 210(1):75 – 97.

Cadonna, A., Fr¨uhwirth-Schnatter, S., and Knaus, P. (2020). Triple the Gamma—A Unifying
Shrinkage Prior for Variance and Variable Selection in Sparse State Space and TVP Models.
Econometrics, 8(20):20.

Cavnar, W. b. and Trenkle, J. M. (1994). N-Gram-Based Text Categorization. In In Proceedings of
SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, pages 161
– 175.

Damien, P., Wakeﬁeld, J., and Walker, S. (1999). Gibbs sampling for Bayesian non-conjugate and
hierarchical models by using auxiliary variables. Journal of the Royal Statistical Society: Series
B (Statistical Methodology), 61(2):331 – 344.

Geng, D., Odaka, T., Kuroiwa, J., and Ogura, H. (2011). An N-Gram and STF-IDF model for
masquerade detection in a UNIX environmente. Journal in Computer Virology, 7(2):133 – 142.

George, E. I. and McCulloch, R. E. (1997). Approaches for bayesian variable selection. Statistica

Sinica, 7:339 – 373.

Griﬃn, J. and Brown, P. (2017). Hierarchical Shrinkage Priors for Regression Models. Bayesian

Analysis, 12(1):135 – 159.

Griﬃn, J. and Holmes, C. (2010). Computational issues arising in Bayesian nonparametric hier-
archical models. In Nils Lid Hjort, Chris Holmes, P. M. and Walker, S. G., editors, Bayesian
Nonparametrics, pages 208 – 222. Cambridge University Press.

Griﬃn, J. and Leisen, F. (2018). Modelling and Computation Using NCoRM Mixtures for Density

Regression. Bayesian Analysis, 13(3):897 – 916.

Griﬃn, J. E. and Leisen, F. (2016). Compound Random Measures and their use in Bayesian non-
parametrics. Journal of the Royal Statistical Society Series B-Statistical Methodology, 79(2):525
– 545.

Griﬃths, T. L. and Ghahramani, Z. (2005). Inﬁnite Latent Feature Models and the Indian Buﬀet
Process. In Proceedings of the 18th International Conference on Neural Information Processing
Systems, NIPS’05, pages 475 – 482, Cambridge, MA, USA. MIT Press.

25

Griﬃths, T. L. and Ghahramani, Z. (2011). The Indian Buﬀet Process: An Introduction and

Review. Journal of Machine Learning Research, 12(32):1185 – 1224.

Ishwaran, H. and Rao, J. S. (2005). Spike and slab variable selection: Frequentist and Bayesian

strategies. Annals of Statistics, 33(2):730 – 773.

Kingman, J. F. C. (1967). Completely Random Measure. Paciﬁc Journal of Mathematics, 21(1):59

– 78.

Kolter, J. Z. and Maloof, M. A. (2004). Learning to detect malicious executables in the wild. In
Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, KDD ’04, pages 470 – 478. Association for Computing Machinery.

Lomax, K. (1954). Business Failures: Another Example of the Analysis of Failure Data. Journal of

the American Statistical Association, 49(268):847 – 852.

Marceau, C. (2001). Characterizing the Behavior of a Program Using Multiple-Length N-Grams.
In Proceedings of the 2000 Workshop on New Security Paradigms, NSPW ’00, page 101 – 110.

McGraw, G. and Morrisett, G. (2000). Attacking Malicious Code: A Report to the Infosec Research

Council. IEEE Software, 17(5):33 – 41.

Mitchell, T. J. and Beauchamp, J. J. (1988). Bayesian Variable Selection in Linear Regression.

Journal of the American Statistical Association, 83(404):1023 – 1032.

Olson, D. and Delen, D. (2008). Advanced Data Mining Techniques. Springer Berlin Heidelberg.

Pekta¸s, A., Eris, M., and Acarman, T. (2011). Proposal of n-gram based algorithm for malware clas-
siﬁcation. SECURWARE 2011 - 5th International Conference on Emerging Security Information,
Systems and Technologies, pages 14 – 18.

Perusqu´ıa, J. A., Griﬃn, J. E., and Villa, C. (2022). Bayesian Models Applied to Cyber Security

Anomaly Detection Problems. International Statistical Review, 90(1):78 – 99.

Raﬀ, E., Zak, R., Cox, R., Sylvester, J., Yacci, P., Ward, R., Tracy, A., Mclean, M., and Nicholas, C.
(2016). An investigation of byte n-gram features for malware classiﬁcation. Journal of Computer
Virology and Hacking Techniques, pages 1 – 20.

Ronen, R., Radu, M., Feuerstein, C., Yom-Tov, E., and Ahmadi, M. (2018). Microsoft Malware

Classiﬁcation Challenge. arXiv:1802.10135.

Suen, C. Y. (1979). n-Gram Statistics for Natural Language Understanding and Text Processing.

IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-1(2):164 – 172.

26

Thibaux, R. and Jordan, M. I. (2007). Hierarchical Beta Processes and the Indian Buﬀet Process. In
Meila, M. and Shen, X., editors, Proceedings of the Eleventh International Conference on Artiﬁcial
Intelligence and Statistics, volume 2 of Proceedings of Machine Learning Research, pages 564 –
571. PMLR.

Tomovi´c, A., Janiˇci´c, P., and Keˇselj, V. (2006). n-Gram-based classiﬁcation and unsupervised
hierarchical clustering of genome sequences. Computer Methods and Programs in Biomedicine,
81(2):137 – 153.

Walker, S. G. and Villa, C. (2021). An Objective Prior from a Scoring Rule. Entropy, 23(7):18.

27

