Minimax Theorems for Finite Blocklength Lossy

Joint Source-Channel Coding over an AVC

1

Anuj S. Vora, Ankur A. Kulkarni

Abstract

Motivated by applications in the security of cyber-physical systems, we pose the ﬁnite blocklength communication

problem in the presence of a jammer as a zero-sum game between the encoder-decoder team and the jammer, by

allowing the communicating team as well as the jammer only locally randomized strategies. The communicating

team’s problem is non-convex under locally randomized codes, and hence, in general, a minimax theorem need not

hold for this game. However, we show that approximate minimax theorems hold in the sense that the minimax and

maximin values of the game approach each other asymptotically. In particular, for rates strictly below a critical

threshold, both the minimax and maximin values approach zero, and for rates strictly above it, they both approach

unity. We then show a second order minimax theorem, i.e., for rates exactly approaching the threshold with along

a speciﬁc scaling, the minimax and maximin values approach the same constant value, that is neither zero nor one.

Critical to these results is our derivation of ﬁnite blocklength bounds on the minimax and maximin values of the

game and our derivation of second order dispersion-based bounds.

9
1
0
2

l
u
J

1
1

]
T
I
.
s
c
[

1
v
4
2
3
5
0
.
7
0
9
1
:
v
i
X
r
a

Cyber-physical systems consist of physical entities that are remotely controlled via communication

I. INTRODUCTION

channels. Examples of such systems are smart city infrastructure, modern automobiles, power grids and

nuclear power plants where a background cyber layer [2] is deployed to carry time-critical information.

Efﬁcient and secure transmission of information over the cyber layer is fundamental to the functioning of

such systems. To guarantee reliable communication in the block coding paradigm, it generally requires an

increasingly large number of uses of the channel, which induces delays in the transmission. Since the systems

above are sensitive to delays, it is of utmost importance to study these systems in the ﬁnite blocklength

regime. On another count, the presence of the cyber layer in these systems makes them vulnerable to

adversarial attacks which can have catastrophic consequences [3], [4]. Motivated by these two factors –

limited delay and the possibility of jamming – in this paper, we study a point-to-point ﬁnite blocklength

lossy communication problem in presence of a jammer.

The authors are with the Systems and Control Engineering group at the Indian Institute of Technology Bombay, Mumbai, 400076. They can

be reached at anujvora@iitb.ac.in, kulkarni.ankur@iitb.ac.in. An earlier version of this work was presented at the National

Conference on Communications in Bangalore [1].

 
 
 
 
 
 
2

We consider a setting where a sender and a receiver are communicating over a channel whose state

is controlled by an active jammer. The jammer can vary the state arbitrarily, i.e., at every instance of

transmission, with the goal of disrupting communication. The standard formulation of this problem considers

this setting only from the vantage point of the communicating team, by seeking coding strategies that are

robust against any action of the jammer. However, since both the communicating team and the jammer

have strategic capabilities, it is logical that one adopts a neutral point of view where one allows both the

communicating team and the jammer to act strategically. To this end, we formulate the above problem as a

zero-sum game where the sender-receiver team aims to minimize the loss in communication by designing

suitable codes, while the jammer tries to maximize it by choosing channel states.

Critical to our contribution is the deﬁnition of allowable strategies for the communicating team and the

jammer; in particular the notion of randomization we permit. We allow the strategy for the communicating

team to be a stochastic code, that is, a pair of encoder and decoder, that are allowed only local randomization.

Generally, for a communication problem, it is of interest to determine codes which are deterministic. In

the game theoretic parlance, this corresponds to pure strategies for the communicating team. However, for

reasons discussed later, ﬁnding a deterministic code for this problem appears to require analysis which is

beyond the scope of this paper. Consequently, we look for randomized strategies. Game-theory provides

us with two possible kinds of randomizations of pure strategies (see e.g., [5]). A mixed strategy is a

random choice of a pure strategy; in the communication theoretic parlance this corresponds to a randomly

chosen pair of encoder and decoder, or random code. However, implementing such a code requires shared

randomness between the encoder and the decoder, which is may not be feasible in cyber physical systems

where the encoder and decoder are decentralized with the channel as a sole medium of communication.

Another notion of randomization in game theory is behavioral strategies – under such a strategy, an action

is chosen at random given the information at the time. In the information theory literature these are referred

to as stochastic codes. Under such a strategy, the encoder and decoder randomize locally using their own

private source of randomness. We adopt this notion of randomization in this paper for the communicating

team. For the jammer, we allow it to randomize over the choice of state sequences of the channel.

Unfortunately, the consideration of stochastic codes creates signiﬁcant hurdles in further analysis. First,

having formulated the problem as a zero-sum game, one is obligated to analyze it using the concept of a

saddle point [5]. The game is said to admit a saddle point if the minimum loss that the communicating

team can incur in the worst case over all jammer strategies (called the upper value of the game), equals

the maximum loss that the jammer can induce in the worst case over all strategies of the communicating

team (called the lower value of the game). However, for each ﬁxed action of the jammer, the optimization

problem for the communicating team under stochastic codes is necessarily nonconvex due to the non-classical

information structure [6]. As a consequence a saddle point need not exist for the resulting zero-sum game.

3

This puts in jeopardy any further game theoretic analysis.

Second, the construction of a stochastic code entails some intrinsic challenges. To compute an upper

bound for the upper value of the game, we require an achievability scheme for the joint source-channel

setting. However, constructing a deterministic code for the joint source-channel setting could possibly require

determining the existence of a deterministic code of the AVC for the maximum probability of error criterion.

The latter problem is unsolved in general and in some cases is equivalent to determining the zero error

capacity of a DMC [7].

Our main results shows that despite the nonconvexity above, near-minimax theorems hold for this game

in the sense that as the blocklength becomes large, the upper and lower values of the game come arbitrarily

close. In particular, we show that there exists a threshold such that if the asymptotic rate is strictly below

the threshold, the upper and lower values tend to zero, whereas, for rates strictly above the threshold, the

upper and lower values tend to unity. We then consider a reﬁned regime such that the asymptotic rate is

exactly equal to the threshold, but allowing a backoff from the threshold that changes with the blocklength

along a speciﬁc scaling. In this case, the upper and lower values tend to a same constant whose value is

neither zero nor one. This shows that a minimax theorem holds even in this novel, reﬁned regime.

The upper value of the game corresponds to lossy joint source-channel coding over an arbitrarily varying

channel. The lower value, on the other hand, corresponds to ﬁnding a probability distribution on state

sequences such that the minimum loss over the resulting (not necessarily memoryless) channel is maximized.

Our results are obtained by constructing a converse for the latter problem to lower bound the lower value,

and an achievability scheme for the AVC to upper bound the upper value. These bounds yield second order

dispersion bounds for the rate of communication which have been of signiﬁcant interest in the recent past

(see, e.g., Polyankskiy et al. [8], Kostina and Verd´u [9] and their numerous follow ups). In our context, the

dispersion bounds yield the aforementioned reﬁned or “second order” minimax theorem.

Our achievability scheme exploits the stochastic coding available for joint source-channel coding. A

separate source-channel coding scheme for the lossless source-channel setting can be constructed by taking

the composition of a source code and a channel code constructed independently of each other. Although

the scheme is sufﬁcient for the minimax theorems with classical notions of rate, it is not sharp enough to

provide us the second order minimax theorem we seek. We develop a joint source-channel coding scheme

using ideas from [10] and [9] to give a stochastic code that yields the desired theorem. To construct this

achievability scheme we ﬁrst derive a reduced random code which is a code with a uniform distribution

over a smaller number of deterministic codes. Using a deterministic channel code for average probability

of error and the above reduced random code, we construct a joint source-channel code which only requires

local randomness at the encoder thereby giving a stochastic code. We use the deterministic channel code

presented in [11] for our construction.

4

The lower bound uses the linear programming relaxation method from [12]. Recently, the authors in [12]

showed that although the point-to-point communication problem (without a jammer) is non-convex in the

space of stochastic codes, it nevertheless possesses a hidden convexity. Speciﬁcally, they demonstrate that

for large blocklengths, the problem can be approximated arbitrarily closely by a linear programming (LP)

based relaxation. This suggests that an approximate minimax theorem could hold for our problem. Our

results validate this intuition. They show that the LP relaxation is tight even in adversarial settings, adding

to the list of cases where the LP relaxation method has yielded tight results [13], [14].

To the best of our knowledge, the formulation we consider here has not been studied before. To be sure,

game-theoretic formulation of communication in presence of jammer has been studied previously, but in a

somewhat different sense. For instance [15], [16] consider mutual information as the payoff function and

prove the existence of saddle point strategies. The control community has studied problem in the continuous

alphabet, e.g., the authors in [17] consider the problem of communicating a sequence of Gaussian random

variables in presence of a jammer and pose a zero-sum game with the mean squared error as the payoff

function. In the context of the AVC, the coding theorems for the Gaussian AVC for peak and average

power constraints on the input and the jammer were proved in [18]. Implicit in their coding theorems is

an approximate minimax theorem for the zero-sum game. A zero-sum game formulation similar to our

paper is also discussed in [10, Ch. 12], where the authors discuss and formulate different instances of

the communication over AVC as a zero-sum game. The closest to our setting, is the problem studied in

[19], [20] by the second author of the present paper. There the authors consider only the channel coding

problem and the action of the jammer is ﬁxed throughout the transmission, making the upper value problem

equivalent to coding for the compound channel; moreover, [19], [20] do not consider second order minimax

theorems.

Lossless joint source-channel coding over an AVC was studied by us in the ‘conference version’ of this

paper [1]. The present paper differs from [1] in the following aspects. We consider lossy joint source-

channel setting in this paper unlike in [1] where we considered only the lossless setting. Moreover, we

derive dispersion based bounds on the rate, which pave the way for the second order minimax theorem.

This paper is organized as follows. We formulate the problem in Section III. The lower bound is derived

in Section IV and the upper bound is derived in Section V. The corresponding asymptotic analysis is done

in Section VI and Section VII concludes the paper.

A. Notation

II. PRELIMINARIES

All random variables in this paper are discrete and are deﬁned on an underlying probability space with
measure P. Random variables are represented with uppercase letters X and their instances are denoted by

Caligraphic letters

Y
distributions on a space

X

,

lower case letters x; unless otherwise stated these are vectors whose length will be understood from the
context. Blackboard letters X, Y etc. are used to represent the corresponding single-letter random variable.

5

etc. denote spaces of single-letter random variables. The set of all probability

X

is denoted by

X

(

P

X

) and a particular distribution is represented as PX.
i:xi=
n

), given by Tx(

(

)

|{

•}|

. The

•

≡

) denotes the set of all types of sequence in

The type of a sequence x

joint type of x, y is denoted as Tx,y. The set

⊆ P
n. The set of sequences with type P is denoted as T (P ).

(

∈ X

n is the empirical distribution Tx ∈ P
)
X

Pn(

X

X

For any expression

, the indicator function I

is unity if the expression holds true, otherwise it

P

B

{B}
is zero. The probability of an event A under the measure induced by a distribution P is denoted by
E[X]]2.

is denoted as Var(X) := E[X

P (x). The variance of a random variable X
X, we deﬁne (PX ×

−
X)(y) :=
x). The complementary Gaussian distribution function is denoted as Q. Note that this is

{
∈
For any distributions PX and PY

X)(x, y) := PX (x)PY

x) and (PX PY

P
x PX(x)PY

A
}

∈ X

X(y

PY

:=

{

I

A

|

x

x

}

|

|

|

|

X(y

|

|

different than the conditional distribution functions deﬁned as QX
P
distribution functions will be deﬁned within the context and used accordingly. The exp and log are with

Y for some random variables X, Y . These

|

respect to base 2.

B. Arbitrarily Varying Channel

The arbitrarily varying channel (AVC) was ﬁrst introduced in [21] where the authors consider a channel

whose law changes arbitrarily with every transmission. The AVC can be modelled as follows. Consider a

family of channels

:=

PY

{

V

X,Θ=θ ∈ P
|
and

(

Y|X

), θ

∈ T }

denoted by the ﬁnite sets

, respectively, where each channel is indexed by the parameter θ, called

X
the state of the channel, drawn from a ﬁnite space

Y

T
Thus, the resulting channel can be modeled as the following discrete memoryless AVC. Speciﬁcally, the

. We assume the channel behaviour is memoryless.

, having common input spaces and output spaces

probability of receiving the sequence y

n on sending the input sequence x in

of states is θ

n is given as PY

X,Θ(y

|

|

∈ T

n
i=1 PY

|

X,Θ(yi|

xi, θi).

A deterministic channel code is deﬁned as a pair of functions (fC, ϕC) given as

Q

∈ Y
x, θ) =

fC :

n, ϕC :

n

,

W → X

Y
N. The probability of error for each message m

→ W

=

1, . . . , M

with M

{

W
n under the deterministic channel code (fC, ϕC) is given as

∈

}

where

θ

∈ T

n when the sequence

X

(1)

and state sequence

∈ W

em,θ(fC, ϕC) :=

ϕ(y)

I

{

= m
}

PY

|

X,Θ(y

|

fC(m), θ).

(2)

n

y
X
∈Y

The maximum and average probability of error over all messages for this code are deﬁned as

e(fC, ϕC) := max

n

θ

∈T

max
1,...,M
∈{

m

}

em,θ(fC, ϕC),

¯e(fC, ϕC) := max

n

θ

∈T

1
M

M

m=1
X

em,θ(fC, ϕC),

(3)

6
6

respectively.

Unlike the ordinary discrete memoryless channel (DMC), the capacity of the AVC depends on the error

criteria as well as the type of codes used by the encoder and decoder. Further, in some cases, no closed

form expression to the capacity of the AVC is known. In particular, it was shown in [7] that computing the

deterministic code capacity of a certain class of AVCs under the maximum probability of error criterion

amounts to computing the zero error capacity of a DMC, which is a signiﬁcantly hard problem. However,

the capacity of the AVC is computable for maximum as well as average probability of error criteria if the

encoder and decoder are allowed to randomize their actions.

A stochastic code is deﬁned as a pair of conditional distributions QX

W ∈ P
The maximum and average probability of error under a stochastic code (QX

|

(

n

X
|W
W , Qc
W

|

|

), Qc
W

Y ∈ P

(

W|Y

n).

|

Y ) are deﬁned as

e(QX

W , Qc
W

|

Y ) := max
θ
∈T

n

|

max
1,...,M
∈{

}

w

I

{

w

= w

QX

W (x
|

|

}

w)PY

X,Θ(y

|

|

x, θ)Qc
W

Y (

w

|

|

y),

Xx,y, bw
I

{

b
= w

w

M

1
M

QX

W (x
|

|

}

w)PY

X,Θ(y

x, θ)Qc
W

|

|

b
y).

Y (

w

|

|

¯e(QX

W , Qc
W

|

Y ) := max
θ
∈T

n

|

Xx,y, bw
Let e be the probability of error criterion (maximum or average). Let R = log M

w=1
X

b

be the rate of commu-

b

n

nication. For

ǫn}n

{

1 →

≥

0, the rate R is said to be achievable if there exists a sequence of (M, n) codes

of rate R (deterministic or random) such that the probability of error e

ǫn for sufﬁciently large n. The

≤

supremum of all such rates is deﬁned as the capacity of the channel and is denoted as C.

In case of the AVC, the capacity C could be zero under certain types of codes and error criteria. We state

the conditions under which the capacity of the AVC is positive. An AVC is said to be non-symmetrizable

if there does not exist any distribution PΘ

(

X

|

∈ P

T |X

) such that

x

∀

∈ X

, x′

∈ X

, y

∈ Y

, we have

X(θ

PΘ

|

|

x)PY

X,Θ(y

|

|

x′, θ) =

X(θ

PΘ

|

|

x′)PY

X,Θ(y

|

|

x, θ).

Xθ
∈T

Xθ
∈T

In this paper, it is assumed that the AVC under study is non-symmetrizable. For a non-symmetrizable AVC,

the stochastic code capacity for maximum (and average) probability of error criterion is given as [22]

min
(
∈P
T
where I(X; YqΘ) is the mutual information between X and YqΘ and YqΘ is the output of the averaged
X,Θ=θ when X is the input of the channel. For further discussion on
channel (qΘPY

C = max
(
PX
X

qΘ(θ)PY

X,Θ) :=

∈P

qΘ

)

)

(4)

I(X; YqΘ),

|

θ

∈T

|

the AVC, the reader is referred to ([10], Ch. 12) and [23].

P

C. Rate distortion function

Let

be a space of messages and let dS :

S

S × S →
where it is required to express k-length strings from
given by E[dS(S,

d for a ﬁxed distortion level d > 0.

S)]

S

∞

[0,

) be a distortion function. Consider a problem

k in M messages such that the average distortion

≤

b

6
6
A deterministic source code is deﬁned as a pair of functions (fS, ϕS) given as

fS :

k

S

→ W

, ϕS :

W → S

k,

7

(5)

where

=

1, . . . , M

with M

N.

W

∈
The distortion for the k-length sequences is deﬁned as d(S,

}

{

Si) > d. Let the rate
. For a given distortion level d, the rate R is said to be achievable if there exists
P
b
d. The inﬁmum of all such rates is deﬁned as the rate distortion

i=1 dS(Si,

S) := 1
k

E[d(S,

S)]

b

k

be deﬁned as R = log M

k
a code, such that limk

function and is given as

b

→∞

≤

R(d) =

min
PbS|S,E[dS(S,

bS)]

d

≤

I(S,

S),

(6)

where S

is distributed as PS

(

) and

∈ S
rate-distortion theory can be found in ([24], Ch. 10).

∈ P

∈ S

S

S

b

b

is distributed as PbS
|

(

S ∈ P

S|S

). More details on the

D. Information quantities

In this section we deﬁne few information quantities that will be used subsequently in the paper. The

information density is deﬁned as

iX;YqΘ (x; y) = log

where

(qΘPY
(PXqΘPY

X,Θ)(y

x)
|
X,Θ)(y)

|

|

, x

, y

,

∈ Y

∈ X

(7)

(qΘPY

X,Θ)(y

|

|

x) =

Xθ
∈T

qΘ(θ)PY

X,Θ(y

|

|

x, θ), (PXqΘPY

X,Θ)(y) =

|

PX(x)qΘ(θ)PY

X,Θ(y

|

|

x, θ).

,θ
Xx
∈T
∈X

The information density for vector valued random variables (X, Y ) is denoted as iX;Yq and is deﬁned
analogously. The d-tilted information is deﬁned as

jS(s, d) = log

E

1

−

d

exp(λ∗
h

λ∗d(s,

S))

, s

,

∈ S

(8)

where the expectation is with respect to the unconditional distribution PbS∗ that achieves the minimum in

(6) and λ∗ =

R′(d). To deﬁne the d-tilted information for the vector valued random variables (S,

−

deﬁne the following.

i

b

RS(d) =

inf
S|S :E[d(S,
P b

b
S)]

d

I(S;

S),

where d(s,

≤
s) is the distortion function deﬁned earlier. Further, the d-tilted information for random variables

b

S is deﬁned as,

b

jS(s, d) := log

E

d

exp(λ∗
h

1

−

,

λ∗d(s,

S))

i

b

(10)

S), we

b

(9)

where the expectation is with respect to the distribution P b

S∗ that achieves the inﬁmum in the (9) and

8

λ∗ =

R′S(d). Further discussion on the d-tilted information can be found in [25].

−

We deﬁne the following set of capacity achieving distributions

ΠΘ =

qΘ ∈ P

(

T

) : max

PX

(cid:27)
The source and channel dispersions are deﬁned as

(cid:26)

I(X; YqΘ) = C

, ΠX =

PX

(

X

) : min
qΘ

∈ P

I(X; YqΘ) = C

.

(cid:27)

(cid:26)

VS = Var (jS(S, d)) ,

(11)

∈

V +
C = min
ΠX
PX

Var(iX;YqΘ (X; Y)), V −C = max

max
ΠΘ
qΘ
∈
where S is distributed as PS and (X, Y) are distributed as PX
assume that there exists a unique capacity achieving state distribution q∗Θ ∈
the above deﬁned channel dispersions are equal V −C = V +

C =: VC.

×

qΘ

|

ΠΘ

Var(iX;YqΘ (X; Y)),

min
ΠX
PX
∈
∈
X,Θ). For computing asymptotics, we
(qΘPY

(12)

ΠΘ. In this case, we have that

III. PROBLEM FORMULATION

), is to be

(

S

for every
N according to

V

n, n

∈ X

∈
S

Consider a ﬁnite family of channels

:=

PY

), θ

. Let

be a ﬁnite space. Suppose

a random source message S

k, k

∈ P
communicated over this family of channels where a jammer can choose a channel from the set

∈ S

∈

X,Θ=θ ∈ P

(

{

V
N, generated i.i.d. according a ﬁxed distribution PS

∈ T }

Y|X

S

|

transmission. An encoder encodes the message S into the channel input string X

a law QX

n

(

k). The channel output string Y

n is decoded by the decoder to

|

|S
k

X
(

Y ∈ P

n). Together (QX

S ∈ P
to a law Q b
Y ) is termed as a stochastic code. An error is said to occur
S
|
if the distortion between the decoded sequence and the source sequence exceeds a predeﬁned level d, that
Si) > d, where dS is the distortion function deﬁned in Section II-C and
) is the maximum allowable distortion level. A jammer selects the channels used for transmission

is, when d(S,
d

b
S) := 1
k

i=1 dS(Si,

S, Q b
S
|

∈ Y

∈ S

[0,

|Y

S

b

k

|

k according

∈

∞

b

P

b

by choosing a random state sequence Θ

n distributed according to q

(

n); Θ

denotes the

∈ T

∈ P

T

∈ T

single-letterized random variable. We assume that the encoder and decoder do not know the actions of

the jammer and that the jammer also does not have any information about the actions of the encoder and

decoder or the source message.

We assume the channel behaviour is memoryless. Thus, the resulting channel is given by the equation

X,Θ(y

PY

|

|

x, θ) =

n
i=1 PY

X,Θ(yi|

|

xi, θi), which governs the probability of receiving the output sequence

y = (y1, . . . , yn) when the input sequence is x = (x1, . . . , xn) and the state sequence is θ = (θ1, . . . , θn).
The rate of communication in this setting is deﬁned as R = k
n .

Q

The probability of error is given as

P(d(S,

S) > d) =

Xs,x,y,bs,θ

b

d(s,

s) > d

I

{

}

q(θ)PS(s)QX

s)PY

S(x
|

|

|

X,Θ(y

x, θ)Q bS
|

|

Y (

s

|

y).

(13)

b

b

We assume that the encoder and decoder aim to minimize the probability of error by choosing stochastic

9

codes (QX

Y ) while the jammer tries to maximize it by choosing the distribution q. Thus, for every pair
of (k, n), we have a zero-sum game between the encoder-decoder team and the jammer with the probability

S, Q bS
|

|

of error as the payoff. The relevant background on zero-sum games can be found in [5, Ch. 4].

The minimax or the upper value of the game is given by

ν(k, n) = min
QX|S,Q b

S|Y

max
q

P(d(S,

S) > d)

(
b
Y ∈ P
and the maximin or the lower value of the game is given by

S ∈ P

k), Q bS
|

QX

|S

X

s.t

(

n

|

k

S

|Y

n), q

(

T

∈ P

n),

ν(k, n) = max

q

min
QX|S,Q b

S|Y

P(d(S,

S) > d)

s.t

Clearly, we have that ν(k, n)

QX

|

S ∈ P
ν(k, n).

≥

(

n

X

|S

k), Q bS
|

b
Y ∈ P

(

S

k

|Y

n), q

(

T

∈ P

n).

It can be observed that the minimax problem is a lossy joint source-channel coding problem over an

AVC with stochastic codes, since the encoder and decoder search for stochastic codes which minimize the

worst case probability of error. Further, it is optimal for the jammer to pick a deterministic sequence of

states since the probability of error given by equation (13) is linear in the distribution q. In a joint source-

channel coding problem over a DMC without a jammer, asymptotically vanishing probability of error can
be achieved for rates below C′
R(d) , where C ′ is the capacity of the DMC and R(d) is the rate distortion
function. Further, the probability of error goes to one for rates above C′
R(d) [26]. In this paper, we show that
, and

the above result extends to this game, where ν(k, n) and ν(k, n) approach each other as k, n

→ ∞

the value they approach depends on the asymptotic value of the rate k
n .

The main results of this paper are the following minimax theorems. Both upper and lower values tend

to zero if limk,n

→∞

k

n < C

R(d) as shown in the following result.

Theorem 3.1: Consider a sequence (k, n) such that limk,n

→∞

k

n < C

R(d) . Then,

lim
→∞

k,n

ν(k, n) = lim

ν(k, n) = 0.

(14)

k,n

→∞

When limk,n

→∞

k

n > C

R(d) , the upper and lower values tend to unity as shown in the following result.
k

Theorem 3.2: Consider a sequence (k, n) such that limk,n

n > C

R(d) . Then,

→∞

lim
→∞

k,n

ν(k, n) = lim

ν(k, n) = 1.

(15)

k,n

→∞

We then consider a ﬁner regime. The sequence (k, n) is such that k

particular, the sequence is parameterized by ρ

k
n

∈

=

R given as,

C
R(d)

+

ρ
√n

.

n →

C
R(d) along a speciﬁc scaling. In

(16)

For the above sequence, we have the following result.

Theorem 3.3: Let V +

C = V −C = VC. Then, for the sequence (k, n) chosen as (16), we have

10

lim
→∞

k,n

ν(k, n) = lim

k,n

→∞

ν(k, n) = Q



.

(17)

−
VC + C

ρR(d)
R(d) VS(d) 




q

The above result states that the upper and lower values of the game tend to an intermediate value between

zero and unity, unlike in Theorem 3.1 and Theorem 3.2. This non-extremal value is achieved when we take a

sequence with the limit as C

R(d) , which is the threshold of reliable communication in the joint source-channel
setting. This insight into the ﬁner asymptotics is possible only due to the higher order dispersion bounds

that we derive in the latter sections

IV. LOWER BOUND ON THE MAXIMIN VALUE

We proceed to derive the aforementioned results by computing ﬁnite blocklength bounds for ν(k, n)

and ν(k, n). In this section, we derive a lower bound on ν(k, n), by relaxing the inner minimization over

(QX

S, Q b
S
|

|

Y ) in the maximin problem. For each q

∈ P

(

n), the minimization can be written as

s)
b

≡

PS(s)QX

s)PYq
S(x
b
|

X(y

|

x)

|

T
s) > d

Q(s, x, y,

s)

}

SC(q) min
QX|S,Q b

S|Y

d(s,

I

{

Xs,x,y,bs
Q(s, x, y,

s.t

×
b
s) = 1

S(x
|
y) = 1

s

|

y)

s
|
b

≥

0

x QX
bs Q b
P
S
|
s), Q bS
S(x
P
|
|
x, θ).

|
Y (
Y (

|
X,Θ(y

QX

|
y),

Q bS
|

s

|

b

Y (
s,

y,

∀

∀

∀

s, x, y,

s,

where PYq

X(y

x) :=

n q(θ)PY

θ

|

|

b
The above problem is non-convex in the space of the distributions (QX

Y ) [27]. A particular line
of approach for such problems is to derive a convex relaxation by containing the non-convex feasible

S, Q b
S
|

P

∈T

b

|

|

|

region within a convex set. We consider a linear programming relaxation presented in [12] derived by a

lift-and-project like method.

In this method, we linearize the objective and the constraints by replacing the bi-product terms

y) with an auxiliary variable V (s, x, y,

s). Further, we append constraints which imply that

QX

S(x
|

s)Q bS
|
the variable V (s, x, y,

Y (

s

|

|

S(x
|
b
drop the requirement that V (s, x, y,
s)

QX

s)

≡

|

b

y). Finally, to derive a relaxation of the problem SC(q), we

y). Effectively, we approximate the non-convex

|

s

s)Q b
Y (
S
|
|
S(x
QX
|
|
b

≡

b
s
Y (

s)Q bS
|

b

b

feasible region of the problem SC(q) by a polytope, thereby lifting the problem into a higher dimensional

space. This exercise results in a linear program which is given as

11

LP(q) min
QX|S,Q b

S|Y ,
V Xs,x,y,bs

d(s,

s) > d

I

{

}

¯Q(s, x, y,

s)

¯Q(s, x, y,
b

s)

≡

PS(s)PYq

b

X(y

|
|
V (s, x, y,

x)

s)

×
b
s) = 1 : γS
S(x
|
y) = 1 : γC
y) = 0 : λS
s) = 0 : λC

P

s)
P
−

|

s

x QX
bs Q bS
|
Q b
S
|
QX

|
Y (
s
Y (
|
b
S(x
|
−
b
y)
s
s), Q bS
|
|
V (s, x, y,
b

|
Y (

s)

≥

≥

0

0

n

R, λS
q :

k

S

× S

× Y

→

k
b

s,

y,

∀
b
∀
s,

s, y)

q (s)
q (y)
q (s,
∀
q (x, s, y)
b
∀

x, s, y,
∀
b
s,
s, x, y,

s, y,

s.t

x V (s, x, y,
bs V (s, x, y,
QX

s)
b
S(x
|
b

|

P

P

s, x, y,

∀
R and λC
q :

s,
b
k
b

S

× X

n

n

× Y

→

R are

where the functions γS
q :

Lagrange multipliers.

k

S

→

R, γC
q :

n

Y

→

Any feasible point of the problem LP(q) is given by the tuple (QX

Y , V ) and the corresponding
Y ). Successively
repeating the exercise results in increasingly tighter convex relations of the original problem. Further details

feasible point of SC(q) is derived by projecting this point onto the space of (QX

S, Q b
S
|

S, Q bS
|

|

|

on the lift-and-project method can be found in ([28], ch. 5).

Clearly, we have

OPT(SC(q))

≥

OPT(LP(q)),

q

∀

(

T

∈ P

n).

Now we derive the dual problem of LP(q) and using weak duality, we bound optimal value of LP(q) thereby

bounding OPT(SC(q)). The corresponding dual program is given as follows.

γS
q (s) +

γC
q (y)

q,λC
q

DP(q) max
q ,γC
q ,λS
γS
γS
q (s)
γC
q (y)
λS
q (s,

s.t

s
X

−

y λC
s λS
P
−
s, y) + λC
P

y
X
q (x, s, y)
s, y)
q (s,
q (x, s, y)
b

≤

≤

≤

0

0

Π(s, x, y,

x, s,

∀

s, y,

∀
s)

∀
b

b

s, x, y,

(I)

(II)

s, (III)

b

x). From weak duality it follows that OPT(SC(q))

≥

where Π(s, x, y,

s)

I

{

≡

d(s,

s) > d

PS(s)PYq
b

|

}

X(y

|

q and from [19] we get

OPT(LP(q)) = OPT(DP(q))
b

∀

b
ν(k, n) = max

OPT(SC(q))

max
q

≥

OPT(LP(q)) = max

q

OPT(DP(q))

max
q

≥

FEAS(DP(q)),

(18)

q

where FEAS(DP(q)) is the objective function of DP(q) evaluated at a feasible point. Thus, to compute a

lower bound on the minimax, as well as, the maximin value of the zero-sum game, it is sufﬁcient to derive

a feasible solution of the DP(q). The following Theorem gives one such construction and computes the

dual cost of DP(q) for the above feasible solution to get a lower bound for the maximin value.

Theorem 4.1: The value ν(k, n) is lower bounded as

12

ν(k, n)

max
q

≥

OPT(DP(q))

max
,U
q,PY q

≥

sup
γ>0 "

s
X

PS(s) min

x "

P

jS(s, d)
(cid:16)

iX;Y q

|

−

U (x; Y

U)

|

≤

γ

(cid:17)

+ exp(jS(s, d)

γ)

−

X(u

PU

|

|

x)PY q

U (y

u)

|

|

U

u=1
X

y
X

N, PY q ∈ P
1, . . . , U
where U
}
d-tilted information as deﬁned in Section II-D.

∈ U

, U

:=

∈

{

×

I

jS(s, d)
n

iX;Y q

|

−

U (x; y

|

u) > γ

# −

o

,

U
exp(γ) #
(19)

n), iX;Y q

(

Y

U (x; y

|

|

u) = log

PYq |X,U (y
PY q |U (y

x,u)
u)

|
|

and jS(s, d) is the

Proof : Deﬁne a random variable U taking values in

:=

1, . . . , U
}

{

U

such that

U

x) =

PYq

X(y

|

|

u=1
X
Consider the following dual variables for DP(q)

X(u

PU

|

|

x)PYq

X,U (y

|

|

x, u).

(20)

λS
q (s,

s, y)

I
≡ −

{

d(s,

s)

d

}

≤

PS(s)

exp(γ
P

−

U
u=1 PY q

U (y
|
|
jS(s, d))

u)

,

b
λC
q (x, s, y)

γS
q (s)

≡

≡

U
b

PU

PS(s)

min
x

P

y
X
θ q(θ)PY

X(u

x) min

|

|

u=1
X
q (x, s, y), γC
λC

q (y)

PYq

|

X,U (y

|

x, u),

(cid:26)

exp(

γ)

−

≡ −

U

u=1
X

PY q
exp(γ

|

U (y
u)
|
jS(s, d))

−

PY q

U (y

|

|

u),

,

(cid:27)

n

where γ > 0, PY q

U (y

|

|

u) :=

Θ,U (y

|

|

θ, u) and PY

|

Θ,U is any distribution in

(

P

Y

|T

n,

).

U

From the proof of Theorem 5.3 in [12], it follows that the above choice of dual variables are feasible for

the DP(q). Further, the dual cost is given as

min
x

s
X

y
X

λC
q (x, s, y) +

y
X

U

exp(

γ)

−

−

U

u=1
X

PY q

U (y

u)

|

|

X(u

PU

|

|

x) min

PYq

(cid:26)

X,U (y

|

|

x, u),

PY q
exp(γ

|

U (y
u)
|
jS(s, d))

−

X(u

PU

|

|

x)PYq

X,U (y

|

|

x, u)I

(

PYq

X,U (y
|
U (y
|

|
PY q

|

x, u)
u) ≤

(cid:27)

− P
P
exp(jS(s, d))

exp(γ) )

y

U
u=1 PY q
exp(γ)

U (y

u)

|

|

min
x

PS(s)

y
X

u=1
X

U

PS(s) min

x "

y
u=1
X
X
U

s
X

s
X

≥

=

+

1
jS(s, d))

exp(γ

−

X(u

PU

|

|

x)PY q

U (y

|

|

u) I

(

u=1
X

y
X

PYq

X,U (y
|
U (y
|

|
PY q

|

x, u)
u)

>

exp(jS(s, d))

exp(γ) )# −

U
exp(γ)

.

13

U
exp(γ)

.

# −

Taking iX;Y q

U (x; y

|

|

u) = log

PYq |X,U (y
PY q |U (y

x,u)
u)

|
|

, we get

s
X

=

λC
q (x, s, y) +

min
x

y
X

exp(

γ)

−

−

y
X

PS(s) min

P

x "

U (x; Y

U)

|

−

iX;Y q
(cid:16)
U

|

s
X

U

PY q

u=1
X
jS(s, d)

|

U (y

u)

|

γ

≤ −

(cid:17)

+ exp(jS(s, d)

γ)

−

X(u

PU

|

|

x)PY q

U (y

|

|

u) I

iX;Y q

U (x; y

|

u)

|

−

jS(s, d) >

γ

−

u=1
X

y
X

n

o

Taking supremum over γ and maximum over U, PY q and q in the above equation, we get the expression
on the RHS of (19). The required bound follows from the equation (18).

The linear programming relaxation gives a lower bound on the upper and lower values of the game. We

now derive an upper bound by constructing an achievability code for the joint source-channel setup.

V. UPPER BOUND ON THE MINIMAX VALUE

To construct an upper bound on the minimax value ν(k, n), we construct a stochastic joint source-

channel code. Recall that a stochastic joint source-channel code is a pair of distributions (QX

S, Q bS
|

|

Y ) with

QX

n

(

k) and Q b
S
|

S ∈ P
|
In this paper, we consider a stochastic code involving a stochastic encoder and a deterministic decoder. To

Y ∈ P

|Y

|S

X

S

k

(

n).

construct the stochastic code, we consider a random joint source-channel code and a deterministic channel

code. Using the idea of random code reduction, we derive another random code with uniform distribution

over a smaller number of codes. The encoder randomly chooses a code from this ensemble to communicate

the source message. Further, the encoder uses the deterministic code to communicate the index across the

channel to the decoder. The decoder then decodes the index with the deterministic code and then uses the

code corresponding to the index to decode the source message.

To construct the stochastic joint source-channel code, we ﬁrst consider a deterministic channel code which

gives a guarantee on the average probability of error.

A. Deterministic channel code for average probability of error

In this section, we ﬁrst consider a deterministic channel code which is independent of the source code.

Let (fc, ϕc) be a channel code as deﬁned in Section II-B.

We have the following result from Theorem 1 in [11] which provides for the existence of a deterministic

code for average probability of error.

Theorem 5.1: Let PX ∈ P

(

X

n) and Z(x, ¯x, y)

0, 1

}

∈ {

be a function such that

Z(x, ¯x, y)Z(¯x, x, y) = 0

x

∀

∈ X

n, ¯x

∈ X

n, y

∈ Y

n

(21)

14

Θ

S

fS

W

fC

X

PY

X,Θ

|

Y

ϕC

W

c

ϕS

S

b

Fig. 1: Source-Channel Coding Setup

and

n

n is a typical set. Let (X, ¯X, Y )

A ⊂ X

× Y
channel code (fc, ϕc), such that

PX ×

PX ×

PY

|

∼

X,Θ. Then, there exists a deterministic

¯e(fc, ϕc)

n)

2 ln(3
M

|T |

≤ r
+2M log e P(Z(X, ¯X, Y ) = 0, (X, Y )

+ min
PX

max
θ
∈T

n

P((X, Y ) /
(cid:16)

θ)

.

∈ A|

∈ A|

θ) + 2 log 3

n max
¯x
∈X

n

|T |

P(Z(X, ¯x, Y ) = 0, (X, Y )

θ)

∈ A|

(22)

(cid:17)
We thus have a deterministic channel code that gives an upper bound on the average probability of error.

We now construct a random joint source-channel code.

B. Random joint source-channel code

The joint source-channel coding setup is given in Figure 1. Let the output random variable of the source
N, and the input random variable of the source decoder

, where M

1, . . . , M

encoder be W

=

∈ W

{

}

∈

be

W

. The source code is a pair of functions (fS, ϕS) as deﬁned in Section II-C. The channel code

is a pair of functions (fC, ϕC) as deﬁned in Section II-B. The composition of the two codes gives the joint

∈ W

c

source-channel code (f, ϕ) deﬁned as

f := (fC

fS) :

◦

k

S

→ X

n, ϕ := (ϕS

ϕC) :

◦

n

Y

→ S

k.

A random joint source-channel code is a pair of random variables (F, Φ) taking values in the set

(f, ϕ)

f :

|

{

k

n, ϕ :

→ X

S
Y
as ed,θ(f, ϕ) :=

k

n

→ S
I

s,y

. The probability of error for the deterministic joint source-channel code is given
}
d(s, ϕ(y)) > d
{

f (s), θ). Correspondingly, the error for the random

PS(s)PY

X,Θ(y

}

|

joint source-channel code (F, Φ)
P

∼

|
ψ is given as

ed,θ(ψ) := E [ed,θ(F, Φ)] , ed(ψ) := max
∈T

θ

n

E [ed,θ(ψ)] .

(23)

We now construct a random joint source-channel code which is similar to the construction of a deter-

ministic code given in [9] for a joint source-channel setting with a non-adversarial DMC.

Theorem 5.2: There exists a random code ψ such that the probability of error is upper bounded as

ed(ψ)

min
PL|S

max
θ
∈T

n

≤

E

exp

(cid:16)

h

−

(cid:16)

iX;Yq∗ (X; Y )

+

log L

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

θ

|

i

(cid:17)

+ E

(1

h

P bS (

−

d(S)))L

B

i(cid:17)

,

(24)

15

(25)

,

where (S, L,

S, X, Y )

PS ×

∼

PL

S ×

|

P b

S ×

PX ×

PY

|

X,Θ=θ,

b

iX;Yq∗ (x; y) = log

(q∗PY
(PXq∗PY

X,Θ)(y

x)
|
X,Θ)(y)

|

|

where (X, Y )

i=1 q∗Θ(θi), q∗Θ ∈
Proof : We construct a random code ψ as follows. First, we construct a random source code (FS, ΦS). Then,

X,Θ=θ, q∗(θ) =

PX ×

ΠΘ and

d(s) :=

∈ S

PY

s)

≤

∼

B

}

{

s

.

|

k : d(s,

d

n

Q

for a particular instance of the source code (fS, ϕS), we construct a random channel code (FC, ΦC).

b

b

Source encoder - Generate M codewords from the set

k each according to P bS ∈ P

(

S

S

k). Let the

ith codeword be denoted as

Si. Further, for a given source message s

k, generate a random variable

L

∼

PL

S=s ∈ P

|

(N

|S

k) such that L

b

≤

M. The encoder encodes the source message s as

FS(s) =

m, L
}

min
{
L




d < mini<m d(s,

Sm)

d(s,
≤
d < mini=1,...,L d(s,

b

b

Si)

(26)

where d is the distortion function and d is the maximum distortion level as deﬁned in Section II-C.



Source decoder - The source decoder decodes the output m of the channel decoder as ΦS(m) =

Sm.

We now construct a random channel code for a given instance of source code (FS, ΦS) = (fS, ϕS).
b
m=1 each distributed according to PX ∈ P
(
X
{
For a given output m of the source encoder, the channel encoder encodes the output as FC(m) = Xm.

Channel encoder - Deﬁne M random variables

Xm}

M

n).

∈ S

Si),

b

Channel decoder - For the channel decoding function, we deﬁne a random variable U

as follows

1, . . . , M + 1

}

∈ {

(27)

fS(S)

d(S, ϕS

fS(S))

◦

d

≤

M + 1 else.

U =



X,Θ(y


X(y

x) :=

Let PYq∗
output y of the channel, the channel decoder decodes the string y as
Q

x, θ), where q∗(θ) =

n q∗(θ)PY

P

∈T

|

|

θ

|

|

n

i=1 q∗Θ(θi), q∗Θ ∈

ΠΘ. For the observed

ΦC(y) = m

arg max
1,...,M
j
∈{

}

PU

|

SM (j
b

|

∈

sM )PYq∗

X(y

|

|

Xj),

(28)

where PU

|

b
SM ∈ P

1, . . . , M + 1

(

k)M

.

b

}|

S

{
(cid:0)

Let ψ be the distribution induced by the source codebook distribution P bS and the channel codebook
. We now compute the probability of error

distribution PX on the set of joint source-channel codes

f, ϕ

(cid:1)

{

}

under this random code ψ for a ﬁxed θ

n. Let the source and the channel codebooks be

(

s1, . . . ,

sM )

∈ S

k and xM = (x1, . . . , xM )

n. The probability of error is bounded above as

b

b

P

d(S,

S) > d

sM , xM , θ

|

d(S, ϕS

fS(S)) > d

sM

|

◦

(cid:16)

b

b

(cid:0)
ϕC(Y )

= fS(S)

(cid:1)
sM , xM , θ, d(S, ϕS

b

|

◦

fS(S))

d

,

≤

where the ﬁrst term is the source coding error and the second term is the channel decoding error when
there is no error at the source encoder. The source coding error is given as P

fS(S)) > d

d(S, ϕS

sM

=

b

(cid:1)

∈ T

∈ X
P

(cid:17)

≤
+ P

(cid:0)

sM =

b

(29)

◦

|

(cid:0)

(cid:1)

b

6
d(S, ϕS

{

◦

fS(S)) > d

=

U > L
}

{

}

. The channel decoding error can be further

16

P

sM

since

U > L
|
computed as follows.
b

(cid:0)

(cid:1)

P

ϕC(Y )

= fS(S)

sM , xM , θ, d(S, ϕS

(cid:0)
= P

ϕC(Y )

= U

|
sM , xM , θ
b

|

fS(S))

◦

d

≤

(cid:1)

PU

SM (m
b
|

|

sM )P
b

(cid:1)
ϕC(Y )

xM , θ

= m
|

M
(cid:0)

m=1
X
M

=

=

m=1
X

PU

bSM (m
|

|

(cid:0)

b
sM )P

PU
|
PU

≤

[m′
fS(S))

◦

(cid:1)
sM )PYq∗
sM )PYq∗
b

bSM (m′
|
SM (m
b
|
|
d implies that fS(S) = U.

X(Y
|
X(Y

Xm′)
Xm) ≥

|

|

|

b

where (30) follows since for d(S, ϕS
b

Averaging over codebooks (

sM , xM ), we get that

(30)

(31)

1

|

xM , θ

,

!

P bSM (

sM )PXM (xM )P
b

d(S,

S) > d

sM , xM , θ

= P

d(S,

S) > d

θ

= ed,θ(ψ),

(32)

bsM ,xM
X
sM ) =

where P b

SM (

b
M
i=1 P b
S(

si),

(cid:16)
si ∈ S

|

b

b

k and PXM (xM ) =

|

(cid:16)

(cid:17)
M
i=1 PX(xi), xi ∈ X

b

(cid:17)
n. P

ed,θ(ψ) follows since the random code is induced by the distributions PXM and P bSM .

Q

Q
Substituting the above in (29), we get

b

b

b

d(S,

S) > d

(cid:16)

b

=

θ

|

(cid:17)

ed,θ(ψ)

≤

P(U > L) +

P bSM (

sM )

bsM
X

b

M

m=1
X

PU

SM (m
b
|

|

sM )P

b

PU
|
PU

M

[m′=1

SM (m′
b
SM (m
b
|

|

sM )PYq∗
|
sM )PYq∗
b

X(Y
|
X(Y

|

Xm′)
|
Xm) ≥

|

.

1

θ

|

!

(33)

b

Following the line of arguments given in Theorem 7 from [9], we get

M

P bSM (

sM )

PU

bSM (m
|

|

sM )P

bsM
X
E

≤

b

exp

m=1
X
iX;Yq∗ (X; Y )

b
−

log L
|

+

M

[m′=1
θ

|

PU
|
PU

SM (m′
b
|
SM (m
b
|

|

sM )PYq∗
sM )PYq∗
b

|

|

X(Y
X(Y

|

|

Xm′)
Xm) ≥

1

θ

|

!

b

(cid:2)
and P(U > L) = E

(1

h
ed,θ(ψ)

−|
P(

(cid:0)
−

B

d(S)))L

(cid:3)
. Substituting in (33), we get

(cid:1)

E

exp

≤

−|

i
iX;Yq∗ (X; Y )

log L
|

+

θ

|

−

+ E

1

−

P bS(

d(S))

B

L

.

i

(cid:1)

Taking minimum over the distributions PL

S and taking the maximum over the states θ, we get the required

(cid:2)

(cid:0)

|

(cid:1)(cid:3)

h(cid:0)

bound.

We further state a weaker bound by choosing L =

to Theorem 8 in [9].

γ
d(S)) ⌋
B

P b
S(

⌊

where γ > 0 is chosen arbitrarily according

Theorem 5.3: There exists a random joint source-channel code ψ such that

ed(ψ)

inf
γ>0

max
θ
∈T

n  

≤

E

exp

"

 −

iX;Yq∗ (X; Y )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

P b
S(

γ
d(S))

B

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

θ

! |

# −

γ

e1
−

,

!

(36)

(34)

(35)

6
6
6
 
 
 
where (S,

S, X, Y ) are as deﬁned in Theorem 5.2.

17

Using the deterministic channel code and the random joint source-channel code, we now construct a

b

stochastic joint source-channel code.

C. Stochastic source-channel code and upper bound

In this section, we present an approach to construct a stochastic joint source-channel code. For that, we

consider a lemma which is a version of the random code reduction lemma given in [10, Ch. 4].

Lemma 5.4: Let (F, Φ)

∼

ψ, and let K be a positive integer such that

Then there exist K deterministic joint source-channel codes (fi, ϕi)K

i=1, such that

K >

ed(ψ)

−

log(

n)
log(1 + ed(ψ))

|T |

.

1
K

K

i=1
X

ed,θ(fi, ϕi) < ed(ψ)

θ

∀

∈ T

n.

(37)

(38)

We now construct a joint source-channel code (QX

S, ϕ), where QX

S is a stochastic encoder and ϕ is a

|
deterministic decoder. The construction is according to Theorem 12.13 in [10]. We consider K deterministic

|

codes, (fi, ϕi)K

i=1 deﬁned as fi :

k

n and ϕi :

n

→ X
chosen code from this ensemble is used to communicate the messages from

→ S

Y

S

k, satisfying the equation (38). A randomly

deterministic code (

f ,

ϕ) deﬁned as

f :

1, . . . , K

{

dn,

ϕ :

dn

} → X

Y

→ {

(22), where dn is a function of n. We use (
b
a random variable i

1, . . . , K

b

b
∈ {

Given s

k, the encoder chooses the input string X

∈ S

dn+n

ϕ :

Y

k decodes y = (

y, ¯y)

dn+n as

∈ Y

→ S

∈ X

b

f ,

ϕ) to communicate the index i of the chosen code. Consider

}

, distributed uniformly and independent of any other random variable.
f (i), fi(s)). The decoder

dn+n randomly as X = (

b

b

b

k. Further, we consider the

satisfying the equation

S
1, . . . , K

}

b
ϕ(y) =

n.

s if (

ϕ(

y), ϕi(¯y)) = (i, s) for some i

0 else,
b

b






where

y

dn and ¯y

∈ Y

∈ Y

Using this stochastic joint source-channel code, we now bound the lower value of the zero-sum game.

b

Note that since we encode the index of the choice of code in the input sequence of length dn, the rate of

communication is given as

k
dn+n .

Theorem 5.5: The minimax value of the game, ν(k, n), is bounded above as

ν(k, n) = min
QX|S,Q b

S|Y

max
q

P(d(S,

S) > d)

dn)

2 ln(3
|T |
K

+ min
PXa

≤ r

max
θa (cid:20)

b
P((Xa, Ya) /

θa) + 2K log e P(Z(Xa, ¯Xa, Ya) = 0, (Xa, Ya)

θa)

∈ A|

∈ A|

+ max
dn
¯xa
∈X

2 log 3

|T |

dnP(Z(Xa, ¯xa, Ya) = 0, (Xa, Ya)

∈ A|

θa)

(cid:21)

E

exp

max

 −

θb "

where

+ inf
γ>0

iX;Yq∗ (Xb; Yb)
"
(cid:12)
(cid:12)
(cid:12)
¯Xa ∼
PXa,
(cid:12)
X
X
× Y
n and PXa ∈ P
X,Θ=θb, with θb ∈ T
PXbPY
X
Proof : The maximum probability of error is bounded as follows

(cid:12)
(cid:12)
(cid:12)
PXaPY
(cid:12)
dn) and PXb ∈ P

B
(Xa, Ya)

P bS(

θb

∼

−

X

∋

∋

dn

dn

dn

(

(

|

γ

e1
−

,

#

# −

! |
X,Θ=θa, with θa ∈ T

|

dn,

n

n

X

× Y

n) are any distributions.

+

γ
d(S))

18

(39)

(Xb, Yb)

∋

∼

max
∈T

n+dn

θ

P(d(S,

S) > d

|

Θ = θ) = max
n+dn

θ

K

b
S) > d,
P(d(S,

ϕ(Ya) = i
|

K

1
K

∈T

i=1
X
i = i, Θ = θ)

+

1
K

i=1
X

n+dn  
1
K

≤

θ

max
∈T

≤

max
dn
θa
∈T

+ max
θb∈T

n

1
K

i,ya
X
I

i,s,yb
X

b
K

b
ϕ(Ya)

P(

1
K

i=1
X
I

{

b
ϕ(ya)

i = i, Θ = θ) +

= i
|

PY

= i
}

X,Θ(ya|

|

f (i), θa)

b

d(s, ϕi(yb)) > d

b
PS(s)PY

}

X,Θ(yb|

|

fi(s), θb).

{

P(d(S,

S) > d

i = i, Θ = θ)

|

b

!

1
K

K

i=1
X

P(d(S, ϕi(Yb)) > d

i = i, Θ = θ)

|

!

Using (22) from Theorem 5.1 we bound the ﬁrst term and using equation (38) from Lemma 5.4 we bound

the second term to get

max
∈T

n+dn

θ

P(d(S,

S) > d

Θ = θ)

|

b

2 ln(3
|T |
K

dn)

+ min
PXa

max
θa (cid:20)

P((Xa, Ya) /

θa)

∈ A|

2 log 3

|T |

dnP(Z(Xa, ¯xa, Ya) = 0, (Xa, Ya)

θa)

∈ A|

≤ r
+ max
dn
¯xa
∈X

+2K log e P(Z(Xa, ¯Xa, Ya) = 0, (Xa, Ya)

∈ A|

θa)

(cid:21)

+ inf
γ>0

E

max

θb "

exp

"

 −

iX;Yq∗ (Xb; Yb)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

P bS(

γ
d(S))

B

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

θb

! |

# −

γ

e1
−

.

#
(40)

Using maxθ

∈T

dn+n P(d(S,

S) > d

Θ = θ) = maxq

|

(
T

∈P

dn+n)

P(d(S,

S) > d), we get the required result.

Having derived the bounds for the lower and upper values of the game, we now proceed to derive the

b

b

rate of convergence for upper and lower values of the zero-sum game.

VI. ASYMPTOTICS AND MINIMAX THEOREMS

We now use the bounds derived in the earlier sections to compute the limits of the upper and lower

values. Throughout this section we assume that there exists a unique capacity achieving state distribution
q∗Θ ∈

ΠΘ whereby V −C = V +

C =: VC and VC > 0.

6
6
Let (X, Y)

∼
we assume that

where

PX

×

θ

∈T

qΘ(θ)PY

X,Θ=θ, with qΘ ∈ P

|

(

T

). Then for all such distributions PX

P

iX;Y

q∗
Θ

(x; Y) <

,

∞

19

),

(

X

∈ P

(41)

iX;Y

q∗
Θ

(x; y) = log

with q∗Θ ∈

ΠΘ. Further, we also assume that

(q∗ΘPY
(PXq∗ΘPY

X,Θ)(y

x)
|
X,Θ)(y)

|

|

, x

, y

∈ X

∈ Y

jS(s, d) <

,

∞

s

∀

∈ S

(42)

where jS(s, d) is as deﬁned in section II-D.

A. Dispersion based asymptotics of the upper bound

In this section we compute the limit of the lower bound by using Theorem 5.5. We ﬁrst bound the error

terms due to the average probability of error in equation (39).

1) Average error: Let P ∗X

5.5, we deﬁne the set

A ⊆ X

∈
dn

× Y

ΠX and deﬁne P ∗X(x) :=
dn as follows. Let T n

dn. To use the Theorem

dn
i=1 P ∗X(xi) for x
dn

∈ X
i=1 Tθ(θ′i) with Tθ ∈ Pn(
n.

n, y

∈ X

∈ Y

X,Θ)(y

θ (θ′) :=
Q
x)
|
X,Θ)(y)

|

Q

, x

(T n
θ PY
(P ∗XT n

|
θ PY

iX ∗;YT n

θ

(x; y) = log

Then the set

A

is deﬁned as

A

=

(x, y)

n

dn

∈ X

× Y

dn : iX ∗;YT n

θ

(x; y) > γ for some Tθ ∈ Pn(

T

)

.

o

). Deﬁne

T

(43)

(44)

V0 := sup
(
T
∈P

qΘ

)

qΘ(θ)PY

X,Θ=θ.

|

θ

∈T

Var

iX∗;YqΘ (X; Y)

,

(cid:0)

(cid:1)

Further, let

where (X, Y)

P ∗X

∼

×

P

The following Lemma gives a necessary condition for an AVC to be non-symmetrizable. The lemma

below follows from [11, Lemma 6].

Lemma 6.1: Let X, X′

∼

PX and PX(x) > 0 for all x

. Let Dη be the set deﬁned as

∈ X

where (PX

{
QX′Θ ×
PX
×
||
η∗ be deﬁned as

and D(QXX′ΘY

×

|

PY
QX′Θ ×

PY

|

Dη =

QXX′ΘY

(

∈ P

X × X × T × Y

) : D(QXX′ΘY

PX

||

QX′Θ ×

PY

|

×

X,Θ)

η

,

}

≤

(45)

X,Θ)(x, x′, θ, y) = PX(x)QX′Θ(x′, θ)PY

x, θ), QX′Θ is the marginal of QXX′ΘY

X,Θ(y

|

|

X,Θ) is the relative entropy between QXX′ΘY and PX

QX′Θ ×

PY

|

×

X,Θ. Let

η∗ = inf

{

η : QXX′ΘY

∈

Dη, QX′XΘ′Y

∈

Dη for some QXX′ΘΘ′Y

(

∈ P

X × X × T × T × Y

)

.

}

(46)

20

If the AVC is non-symmetrizable, then η∗ > 0.

Using the above lemma, we now construct the function Z :

dn

dn

dn

X

× X

× Y

→ {

0, 1

}

is deﬁned

as follows. Since the AVC is non-symmetrizable, from Lemma 6.1 we have η∗ > 0. Choose η such that

0 < η < η∗. Set

Z(xa, ¯xa, ya) =

1 if (xa, ya)

∈ A

0 otherwise,




& either (¯xa, ya) /

∈ A

or

θa ∈ T

dn s. t. Txa,¯xa,θa,ya ∈

∃

Dη

(47)

where recall that Txa,¯xa,θa,ya is the joint type of xa, ¯xa, θa, ya. Thus, the function Z satisﬁes (21) and we
dn, ya ∈ Y

dn. Since otherwise, there exists (xa, ¯xa, ya)

have Z(xa, ¯xa, ya)Z(¯xa, xa, ya) = 0

xa, ¯xa ∈ X



∀

dn

dn

dn such that Z(xa, ¯xa, ya)Z(¯xa, xa, ya) = 1. This implies (xa, ya)

and (¯xa, ya)

∈
and

× Y

× X

X
hence, there exist joint types Txa,¯xa,θa,ya ∈
deﬁnition of η∗, we get η∗

≤

Dη and T¯xa,xa,θ′

a,ya ∈

Dη for some θa, θ′a ∈ T

∈ A

∈ A

dn. Thus, from the

η. However, this is a contradiction since η is chosen to be strictly lesser than

η∗. Using this function Z we get the following upper bound. The proof is in the Appendix.

Theorem 6.2: For the above choice of

, Z, K and dn, we have that

A

dn)

2 ln(3
|T |
K

r

+ min
PXa

max
dn
θa
∈T

P((Xa, Ya) /
(cid:20)

∈ A|

θa) + 2K log e P(Z(Xa, ¯Xa, Ya) = 0, (Xa, Ya)

θa)

∈ A|

2 log 3

+ max
dn
¯xa
∈X
2 ln(3
|T |
K

≤ r

dnP(Z(Xa, ¯xa, Ya) = 0, (Xa, Ya)

|T |

∈ A|

θa)

(cid:21)

dn)

+

V0

log √dn
dn

−

dn

δ

(cid:16)

(cid:17)

2 +

2 log e
√dn

+

2 log 3

|T |

dn(dn + 1)|X |
exp(dn(η))

2

Θ

|

||Y|

.

(48)

2) Random code error: In this section, we bound the error terms in (39) corresponding to the random

joint source-channel code. The proof is in the Appendix.

Theorem 6.3: The random code probability in (39) is bounded as

E

inf
γ>0

max
θb∈T

n "

nC

Q

≤

exp

 −

"
kR(d)

iX;Yq∗ (Xb; Yb)
(cid:12)
(cid:12)
(cid:12)
Γ(k)
(cid:12)
−
−
nVC + kVS(d) !

B
n + k

+

−

γ
d(S))

log

+

P bS(

B
K0 + 2
√k

,

θb

#

! |

+ e1
−

γ

#

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(49)

where B, K0 > 0 are constants and Γ(k) = ¯c log k + c + log

p

1
2 log k + 1

, ¯c, c > 0.

Putting together the above two theorems, we get the following result.

(cid:0)

(cid:1)

Theorem 6.4: The upper value is bounded as

ν(k, n)

≤

+

nC

Q

kR(d)

Γ(k)

−
−
nVC + kVS(d) !

+

B
√n + k

+

K0 + 2
√k

dn)

p
2 ln(3
|T |
K

+

2 log e
√dn

+

r

V0

log √dn
dn

−

dn

δ

(cid:16)

2 log 3

2 +

|T |

dn(dn + 1)|X |
exp(dn(η))

2

Θ

|

||Y|

,

(50)

(cid:17)

 
 
where Γ(k), K0 and B are as in Theorem 6.3.

Proof : The required bound follows by substituting the bounds in (48) from Theorem 6.2 and (49) from

Theorem 6.3 in (39).

We now choose a sequence of (k, n) for which the upper and lower values of the game tend to zero. Take

21

,
(cid:25)
N is such that K satisﬁes the equation (37), δ > 0 and

K = c0n, dn =

−

(cid:24)

log K
δ
C

where c0 ∈

is the ceiling function.

⌈•⌉

The following corollary gives the dispersion bound for the rate of communication.

Corollary 6.5: [Dispersion based achievability] Let ǫ > 0. Consider a sequence of (k, n) satisfying

kR(d)

nC

−

Γ(k)

−

≥

nVC + kVS(d) Q−

1 (ǫ) .

Then, we have that ν(k, n)

ν(k, n)

Proof : Take (k, n) such that nC

≤

ǫ.
≤
kR(d)

−

δ(k, n) =

B
√n + k

+

r

dn)

2 ln(3
|T |
K

+

p

(cid:16)
and K, dn are as in (51). Substituting in (50), we get that

≥
p
2 log e
√dn

nVC + kVS(d) Q−

1 (ǫ

−

δ(k, n)) + Γ(k), where

+

dn

δ

V0

log √dn
dn

−

2 log 3

2 +

|T |

dn(dn + 1)|X |
exp(dn(η))

(cid:17)

2

Θ

|

||Y|

nC

Q

kR(d)

Γ(k)

−
−
nVC + kVS(d) !

+

B
√n + k

+

r

dn)

2 ln(3
|T |
K

+

2 log e
√dn

+

p
2 log 3

+

|T |

dn(dn + 1)|X |
exp(dn(η))

2

Θ

|

||Y|

ǫ.

≤

Since δ(k, n)

0 as k, n

→

→ ∞

, the result follows.

V0

log √dn
dn

−

dn

δ

(cid:16)

2

(cid:17)

(51)

(52)

(53)

B. Dispersion based asymptotics of the lower bound

In this section, we compute the limit of the lower bound of the probability of error. The proof is in the

Appendix.

Theorem 6.6: The lower value ν(k, n) is bounded as

ν(k, n)

max
,U
q,PY q

≥

sup
γ>0 "

s
X

PS(s) min

x "

P

jS(s, d)
(cid:16)

iX;Y q

|

−

U (x; Y

U)

|

≤

γ

(cid:17)

U

+ exp(jS(s, d)

γ)

PU

X(u

|

|

x)PY q

U (y

|

|

u) I

jS(s, d)

iX;Y q

|

−

U (x; y

|

u) > γ

−

y
u=1
X
X
kR(d) + γ(n)

nC
nVC + kVS(d)

−

Q

≥

K3 ! −
where K1, K2, K3, B′ > 0 are constants and γ(n) = K4 log(n + 1), K4 > 0.

√n + k −

p

−

K1
k −

K2
√n −

1
√n + 1

,

n
B′

U
exp(γ) #

# −

o

(54)

 
 
We also have the following corollary that gives the second order dispersion bounds on the rate.

Corollary 6.7: [Dispersion based converse] Let ǫ > 0. Consider a sequence of (k, n) satisfying

kR(d)

nC

−

nVC + kVS(d) Q−

1 (ǫ)

γ(n).

−

≤

p

Then, we have that ν(k, n)

ν(k, n)

ǫ.

≥

≥

Proof : Take (k, n) such that nC

kR(d) + γ(n)

−

nVC + kVS(d) Q−

1 (ǫ + δ(k, n)), where

≤

22

(55)

δ(k, n) =

K1
k

+

K2
√n

p
+

1
√n + 1

+

B′
√n + k

.

Substituting in (54), we get that

Q

kR(d) + γ(n)

nC
nVC + kVS(d)

−

K3 ! −

−
, the result follows.

Since δ(k, n)

0 as k, n
p

→

→ ∞

K1
k −

K2
√n −

1

B′

√n + 1 −

√n + k ≤

ǫ.

(56)

C. Proof of the minimax theorems

We are now prepared to derive the asymptotic results stated in Section III. We consider sequences of

(k, n) and study the limiting behaviour of the ﬁnite blocklength bounds along these sequences.

We ﬁrst prove Theorem 3.1.

Proof of Theorem 3.1: Consider the bound from Theorem 6.3. Take a sequence (k, n)

lim k

n < C

R(d) . Then, we have that

such that

↑ ∞

C

Q

lim
→∞

k,n

√n





−

k

n R(d) + Γ(k)
VC + k

n

n VS(d) 






= 0.

and for K and dn chosen as (51), we have





q

Further, as k, n

→ ∞

B
n + k

+

K0 + 2
√k

+

r

dn)

2 ln(3
|T |
K

+

2 log e
√dn

+

Thus, ν(k, n)

→

0 and hence ν(k, n)

0.

→

V0

log √dn
dn

−

dn

δ

(cid:16)

2 log 3

2 +

|T |

dn(dn + 1)|X |
exp(dn(η))

2

Θ

|

||Y|

0.

→

(cid:17)

We now come to the proof of Theorem 3.2.

Proof of Theorem 3.2: Consider the bound in Theorem 6.6. Take a sequence (k, n)

C
R(d) . Then, we have that

such that lim k

n >

↑ ∞

Q

lim
→∞

k,n

√n









q

C

k

n R(d) + γ(n)
nVS(d)

−
VC + k

n

K3
n

−

= 1.









 
23

Further, as k, n

→ ∞

K1
k −

K2
√n −

−

1

B′

√n + 1 −

√n + k →

0.

Thus, ν(k, n)

→

1 and hence ν(k, n)

1.

→

Note that proving the coincidence of the upper and lower values as in Theorem 3.1 and Theorem 3.2 can

be shown without dispersion bounds (see e.g., the arguments in [1]). Consequently, these claims hold even

when the capacity achieving state distribution is not unique.

To prove Theorem 3.3, we consider a reﬁned deﬁnition of the rate with O( 1

√n ) backoff from C

R(d) . Recall

from (16) that the sequence is given as

ρ
√n
R is ﬁxed. The non-asymptotic formulae we have derived allow us to evaluate the upper and

C
R(d)

(57)

k
n

+

=

,

where ρ

∈

lower value of the game along this reﬁned deﬁnition of the rate. Along this sequence, the upper and lower

value now both tend to the same value, which depends on ρ. This value is in general neither 0 or 1. This

coincidence in ﬁner asymptotics hinges on there being a unique capacity achieving distributions for the

jammer. We discuss this in the following section.

Proof of Theorem 3.3: Substituting (57) in the lower bound given by (54), we get the following Gaussian

term in the inequality

Q

kR(d) + γ(n)

nC
nVC + kVS(d)

−

K3 !

−

Taking limit k, n

p

, we get

→ ∞

= Q



ρR(d) + γ(n)
√n

−
R(d) + ρ

VC + ( C

√n )VS(d)



q

since the other terms in (54) tend to zero asymptotically.



q

lim
→∞

k,n

ν(k, n)

Q



≥

−
VC + C

ρR(d)
R(d) VS(d) 


,

K3
n

−

.





(58)

(59)

Similarly, substituting (57) in (50), we get the following Gaussian term in the inequality.

nC

Q

kR(d)

Γ(k)

−
−
nVC + kVS(d) !

Taking the limit as k, n

p
→ ∞

, we get

= Q





q

ρR(d)

Γ(k)
√n

−
VC + ( C

−
R(d) + ρ

√n )VS(d) 


.

(60)

since the other terms in (50) vanish asymptotically. This completes the proof.

lim
→∞

k,n

ν(k, n)

Q



≥

−
VC + C

ρR(d)
R(d) VS(d) 




q

,

(61)

 
 
24

D. Non-uniqueness of channel dispersion

In a general AVC setting, there could be multiple pairs of distributions (PX, qΘ) that achieve the capacity

in (4) and consequently the sets ΠX and ΠΘ could be non-singleton sets. When the capacity achieving input
and the state distributions are non-unique, the channel dispersions V +

C and V −C deﬁned in Section II-D need

not be equal. The dispersions are equal if either one of the capacity achieving input or state distributions

is unique.

In this paper, we computed the dispersion bounds for stochastic joint source-channel coding over an

AVC under the assumption that the capacity achieving state distribution is unique. Recently, authors in

[11] and [29] computed the second order dispersion bounds for the rate of communication over an AVC for

deterministic and random codes respectively. They showed that in the case of non-unique capacity achieving

distributions, the achievability and converse bounds for the rate are not equal. Thus, it is natural to suppose

that in the case of non-unique capacity achieving distributions, the ‘ﬁner’ minimax theorem may not hold

and thereby the achievability and converse bounds for the rate may not match in the joint source-channel

setting. Validating this would require improved joint source-channel coding strategies which is a point of

future work.

VII. CONCLUSION

We formulated the adversarial communication problem as zero-sum game between the encoder-decoder

team and the jammer where the encoder-decoder attempt to minimize the probability of error while the

jammer tries to maximize it. The problem is non-convex in the space of strategies of the encoder-decoder

team and hence a minimax theorem need not hold. However, we showed that an approximate minimax

theorem holds for the game. We derived ﬁnite blocklength upper and lower bounds for the minimax and

maximin values of the game and showed that asymptotic minimax theorems hold as the blocklength tends

C

to inﬁnity. In particular, for rates below C
R(d) , the values tend to unity. For rates tending exactly to C
the same constant under a technical assumption on the uniqueness of capacity achieving distributions.

R(d) , the upper and lower values tend to zero and for rates above
√n ) backoff, the values tend to

R(d) with a O( 1

APPENDIX A

We begin with the following central limit theorem due to Berry and Esseen (see [8]).

Theorem A.1 (Berry-Esseen CLT): Fix n

we have

N. Let Wi be independent random variables. Then, for t

∈

1
n

n

i=1
X

P
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Wi > Dn + t

r

Vn
n ! −

Q(t)

Bn
√n

,

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

R,

∈

(62)

 
where Q is the complementary Gaussian function, and

25

Dn =

1
n

n

E[Wi], Vn =

i=1
X

Proof of Theorem 6.2:

Var[Wi], An =

1
n

n

i=1
X

1
n

n

i=1
X

E[
|

Wi −

E[Wi|

3]], Bn =

c0An
V 3/2
n

, c0 > 0.

(63)

We weaken the bound in (39) by taking PXa(xa) =

dn
i=1 P ∗X(xi), P ∗X

ΠX. Let

∈

A

, Z, K and dn be

as deﬁned in (43), (47) and (51) respectively. Let Ui

:= iX∗;YTθ
Q

θ

∈T

Tθ(θ)PY

X,Θ=θ ∀

|

i. Since the channel is memoryless, we have iX ∗;Yq∗ (Xa; Ya) =

taking γ = log(√dnK) we can write the following, P
P
From Lemma 12.10 in [10], we have C

i=1 Ui ≤
iX ∗;Yq∗ (Xa; Ya)
γ
P
≤
(Xai; Yai)] = E[Ui] for all Tθ ∈ Pn(
(cid:1)
(cid:0)
T
≤
E[Ui]. Also, substituting for log K from equation (51), we get

E[iX;Y

(cid:16)P

T n
θ

dnC

≤

dn
i=1

(Xai; Yai), where (Xai, Yai)
×
dn
i=1 Ui. Thus,
log(√dnK)

= P

P ∗X

∼

dn

(cid:17)
) and hence

P

P

dn

i=1
X

Ui ≤

log

dn + log K

p

P

P

! ≤

≤

Ui ≤

log

√dn
exp dnδ

+

dn

i=1
X
dn

E[Ui])

(Ui −

log

≥

dn

E[Ui]

!

i=1
X
exp dnδ
√dn !

,

(64)

where the last equation follows from triangle inequality. Using Chebyshev’s inequality and taking supremum

 (cid:12)
i=1
(cid:12)
X
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

over θa, we get

sup
dn

∈T

θa

dn

i=1
X

P

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E[Ui])

(Ui −

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dnδ

−

≥

log

dn

p

! ≤

sup
dn

∈T

θa

(dnδ

V0

−

dn
log √dn)2

Var

iX∗;YTθa

(Xa; Ya)

(cid:16)

(cid:17)

(65)

≤

dn

δ

(cid:16)

log √dn
dn

−

2 ,

(cid:17)

where (65) follows from equation (44).

The bounds on the other two terms given as P

P(Z(Xa, ¯xa, Ya) = 0, (Xa, Ya)
Theorem 4 in [11]. Using (65) and above bounds, we get the required bound.

¯Xa = ¯xa, θa)

(dn + 1)|X |
(cid:0)

||Y| exp(
(cid:1)

∈ A|

−

≤

|

Z(Xa, ¯Xa, Ya
Θ

2

= 0, (Xa, Ya)

θa)

∈ A|

≤

1
√dnK

and

dnη) follows from the proof of

Proof of Theorem 6.3: To derive the bound, we construct a random joint source-channel code (F, Φ)

or equivalently a distribution ψ on the set of codes
distributions PX and P bS. Let P ∗X be a distribution from the set ΠX that achieves the V +
n. Further, take P b
PX(x) :=
S(

k
i=1 P ∗bS (
optimal in (6). Clearly, with these choice of distributions, we have that
b

n
i=1 P ∗X(xi) for x

s) =

∈ X

∈ S

f, ϕ

si),

Q

Q

b

{

}

s

n

C in (12). Deﬁne
k, where P ∗bS achieves the

. From Theorem 5.2, it sufﬁces to choose the

iX ∗;Yq∗ (X; Y ) =

i=1
X

iX∗;Y

q∗
Θ

(Xi; Yi), jS(S, d) =

j=1
X

k

b
jS(Sj, d).

 
 
(66)

(67)

(68)

(cid:21)

Recall the error terms corresponding to the random code from Theorem 5.5. Writing the maximization

over θb ∈ T

n as maximization over q

(

T

∈ P

n), we can write the bound as

26

where q(θ) =

Further, deﬁne the set

Q

D

iX ∗;Yq∗ (Xb; Yb)

E

exp

max
(
T
∈P

q

n

"

n) "
i=1 qi(θi) with qi ∈ P
as

 −

(cid:12)
(cid:12)
(cid:12)
), θ
(cid:12)

∈ T

(

T

log

−

P bS(

γ
d(S))

B

n. Let h(Xb, Yb, S) :=

+

+ e1
−

γ

,

!#

(cid:12)
(cid:12)
(cid:12)
n
j=1 iX∗;Y
(cid:12)

q∗
Θ

#

(Xbj; Ybj)

log

−

γ
d(S))

P b
S(

B

:=

s
(

D

∈ S

k : log

1
d(s)) ≤

B

P bS(

i=1
X

k

jS(si, d) +

¯c

P

1
2

−

log k + c

,

)

(cid:19)

(cid:18)

where ¯c, c are constants deﬁned in [9]. Deﬁne the random variable Wl as

Wl = Wl(n, k) := 


iX∗;Y

q∗
Θ

(Xbl; Ybl)

jS(Sl

n, d)

−

−

can be written as

n

if l

≤
if n < l

n + k

≤

The expectation E

exp

h(Xb, Yb, S)

− |

+

|

I

E

(cid:2)
exp

(cid:20)

(cid:0)
h(Xb, Yb, S)
− |
(cid:0)

n+k

+
|

Wl −

E

exp

≤







− (cid:12)
Xl=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
h(Xb, Yb, S)



(cid:1)(cid:3)
S
{

∈ D}

+ E

exp

(cid:21)

(cid:20)

+

− |
(cid:0)

(cid:1)
log(k(¯c

−

1

2) exp(c)γ)

h(Xb, Yb, S)

+
|

I

{

S /

∈ D}

(cid:1)

K0
√k

,

+









(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the ﬁrst term in (68) follows by using the deﬁnition of the set
using E

P(S /

exp

I

) and applying Lemma 5 from [9].

and the second term follows by

D

∈ D
We deﬁne the following moments to be used in bounding the ﬁrst term in (68) using Berry-Esseen CLT.

− |

≤

∈ D}
(cid:3)

(cid:2)

(cid:0)

S /
{

+
|

(cid:1)

Dn+k(q) =

1
n + k

n+k

E[Wl], Vn+k(q) =

n+k

Var[Wl],

1
n + k

Xl=1
n+k

Xl=1
3], Bn+k(q) =

(69)

, c0 > 0.

An+k(q) =

c0An+k(q)
V 3/2
n+k(q)
Xl=1
Note that the moments are computed with respect to the distribution PX ×
we deﬁne the following set

1
n + k

E[Wl|

Wl −

E[
|

P

θ q(θ)PY

X,Θ=θ ×

|

PS. Next,

=

(x, y, s)

(

H

n

n

∈ X

× Y

× S

k :

1
n + k

n+k

Wl >

Dn+k(q)

Xl=1

tk,n

−

r

Vn+k(q)
n + k !)

,

where tk,n > 0 will be chosen later. For the sake of brevity, we deﬁne the term in the exp as follows

g(Xb, Yb, S) =

n+k

Xl=1

Wl −

log(k(¯c

−

1

2) exp(c)γ),

Γn+k(q) = (n + k)

Dn+k(q)

tk,n

−

r

Vn+k(q)
n + k ! −

log(k(¯c

−

1

2) exp(c) log γ).

(70)

(71)

(72)

 
 
27

which can be written as

Thus, we can write (68) as E

exp

g(Xb, Yb, S)

E

exp

g(Xb, Yb, S)

E

(cid:2)
≤

− |
(cid:0)
exp

(cid:2)

(cid:0)

Γn+k(q)

|

− |

− |
(cid:0)

+
|
+

(cid:1)
I

I

(cid:2)
(Xb, Yb, S)
{
(Xb, Yb, S)
{

(cid:1)

+
|
(cid:1)(cid:3)
+ E

where the ﬁrst term in (73) follows by using the deﬁnition of the set

(cid:3)

∈ H}

exp

g(Xb, Yb, S)

− |

(cid:3)
(cid:2)
+ E [I

(cid:0)

(Xb, Yb, S) /
{

∈ H}

∈ H}

(Xb, Yb, S) /
{

∈ H}

+

I

(cid:1)

(cid:3)

(73)

|
] ,

and the second term follows by

H

1. Using the above results and (68), we get the following bound.

using exp(

+)

≤

−| • |

E

exp

max
(
T
∈P

q

n) "

"

E

 −

(cid:12)
(cid:12)
(cid:12)
(cid:12)
− |

iX ∗;Yq∗ (Xb; Yb)

log

−

P b
S(

γ
d(S))

B

+

+ e1
−

γ

≤

q

max
(
T
∈P

n)

exp

Γn+k(q)

+
|

I

{

(Xb, Yb, S)

∈ H}

(cid:1)
To upper bound the ﬁrst term in (74), we compute the maximum of exp

(cid:0)

(cid:2)

(cid:3)

!#

(cid:12)
(cid:12)
(cid:12)
+ max
(cid:12)
(
T
∈P

q

n)

#

P ((Xb, Yb, S) /

∈ H

) + e1
−

γ +

K0
√k

.

(74)

Γn+k(q)

− |

over all distributions.

+

|

(cid:1)

Since exp(

−| • |

+) is a decreasing function with respect to Γn+k(q), we get

(cid:0)

exp

max
(
T
∈P

n)

q

− |
(cid:0)

Γn+k(q)

+
|

exp

≤

(cid:1)

To compute the minimum, we consider the following

min
(
T
∈P

n)

q

Γn+k(q)

 −

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

!

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(75)

Dn+k(q) =

Vn+k(q) =

1
n + k

1
n + k

E[iX∗;Y

q∗
Θ

(Xbi; Ybi)]

k
n + k

−

R(d),

Var(iX∗;Y

q∗
Θ

(Xbi; Ybi)) +

k
n + k

VS(d),

n

i=1
X
n

i=1
X

where the moments are with respect to P ∗X

qi(θ)PY

|

θ

∈T

X,Θ=θ. Thus, the minimum is given as

(n + k)

Dn+k(q)

min
(
T
∈P

n)

q

tk,n

−

r

kR(d)

−

(n + k) max
ΠΘ

qi∈

−

tk,n

r

Vn+k(q)
n + k

+ O(1), (76)

where the maximum in the second term in (76) is restricted to ΠΘ by using Lemma 63 and Lemma 64 in

×
Vn+k(q)
n + k !

P
= nC

[8]. Since P ∗X

ΠX and q∗Θ ∈

∈

ΠΘ,

ΠΘ|

|

(n + k)

min
(
T
∈P

q

n)

Dn+k(q)

−

tk,n

r
kR(d)

= 1, we have maxqi∈
Vn+k(q)
n + k !

= nC

−

ΠΘ Vn+k(q) = nVC. Thus, we have that

kR(d)

tk,n

−

nVC + kVS(d) + O(1),

(77)

p

We choose tk,n as tk,n = (nC

minq

(
T

∈P

n) Γn+k(q)

≥

¯c log k

log γ

c)/

nVC + kVS(d). Thus, we get

−

−
1
2 log k + O(1). Substituting in (74), we get the following upper bound

p

−

−

max
(
T
∈P

n)

q

1
√k

P ((Xb, Yb, S)

Q(tk,n) + max
n)
(
T

∈P

q

≤

∈ H
Bn+k(q)
√n + k

) + max
(
q
∈P
T
K0 + 2
√k

+

P ((Xb, Yb, S) /

∈ H

) + e1
−

γ +

K0
√k

n)

,

(78)

where (78) follows by taking γ = 1

2 loge k + 1 and bounding P ((Xb, Yb, S) /

∈ H

) using Berry-Esseen CLT.

 
 
Recall that Bn+k(q) is given as Bn+k(q) = c0An+k(q)
3/2
n+k(q)
V
Bn+k(q) as from the deﬁnition of An+k(q) and Vn+k(q) as

. Assuming minl

1,...,n+k

∈{

}

Var[Wl]

= 0, we can bound

28

Bn+k(q)

≤

c0 maxl E[
|

Wl −
(minl Var[Wl])3/2

E[Wl]
|

3]

.

(79)

From (41) and (42), we have that the RHS is ﬁnite for all q and independent of k, n. Thus, we have that

maxq

(
T

∈P

n) Bn+k(q) is a ﬁnite constant. Substituting tk,n from above, taking maxq

n) Bn+k(q)

(
T

∈P

≤

B with

B > 0 and using (78), we get the required bound.

Proof of Theorem 6.6: From (19), it sufﬁces to construct q, P ¯Yq, the random variable U and γ to get a
ΠΘ. Let U be the number of types
lower bound on ν(k, n). Take q(θ) = q∗(θ) =

n

i=1 q∗Θ(θi) where q∗Θ ∈

be indices corresponding to each of the types. Thus, for a given sequence

) and

=

1, . . . , U
}

X
n, U maps it to its type which is denoted by some index u

U

{

Q

. Further, let PY q

U be deﬁned as

|

∈ U

in

x

Pn(
∈ X

PY q

U (y

|

|

u) = (PXq∗PY

|

X,Θ)(y) =

PX (x)q∗(θ)PY

X,Θ(y

x, θ),

(80)

|

|

Xx,θ
) being the type corresponding to the index u. Thus,

where PX (x) =

we have that iX;Y q
Q
|

|

n
i=1 Tx(xi), x
u) =
U (x; Y

n with Tx ∈ Pn(
∈ X
n
i=1 iX;Y

iX;Y

P
q∗ (x′; y) := log

X
q∗ (xi; Yi), jS(s, d) =
x′)
|
X,Θ)(y)

(q∗ΘPY
(Txq∗ΘPY

X,Θ)(y

|

|

P
, x′

k

j=1 jS(sj, d), where

, y

.

∈ Y

∈ X

(81)

Since q is taken as an i.i.d. distribution, effectively, we have a channel where

Y

X=xi,Θ=θi when the input is x = (x1, . . . , xn). Thus, the LHS of (54) is a converse
of the standard DMC without a jammer with the channel given as the above averaged channel. Following

q∗Θ(θi)PY

θi∈T

∼

|

n
i=1

Q

P

the line of arguments given in Appendix C of [9], we get the following inequality.

PS(s) min

x "

P

jS(s, d)
(cid:16)

iX;Y q

|

−

U (x; Y

U)

|

≤

γ

(cid:17)

max
,U
q,PY q

sup
γ>0 "

s
X
+ exp(jS(s, d)

U

γ)

u=1
X

y
X

−

X(u

PU

|

|

x)PY q

U (y

|

|

k

jS(Sj, d))

γ

n

P

iX;Y

q∗
Θ

(x∗i ; Yi)

n
K1
k −

K2
√n −

u) I

jS(s, d)

iX;Y q

|

−

U (x; y

|

u) > γ

U
exp(γ) #

# −

o

≥

! −
where the second term in the LHS is bounded below by zero, K1 and K2 are some constants and x∗ =

j=1
X

i=1
X

≤ −

−

−

(n + 1)|X |−

1 exp(

γ),

(82)

(x∗1, . . . , x∗n) is a sequence such that its type Tx∗ minimizes

min
n(
X
∈P

Tx

Tx −

) |

P ∗X

|

(83)

where P ∗X

ΠX. Let

∈

Wl = Wl(n, k) :=






iX;Y

q∗
Θ

jS(Sn

(x∗l ; Yl)
l, d)

−

n

if l

≤
if n < l

n + k.

≤

6
 
Deﬁne the following moments of the random variable Wl.

Dn+k =

An+k =

1
n + k

1
n + k

n+k

Xl=1
n+k

Xl=1

E[Wl], Vn+k =

n+k

Var[Wl],

1
n + k

Xl=1
3]], B′n+k =

E[
|

Wl −

E[Wl|

c0An+k
V 3/2
n+k

, c0 > 0.

From Berry-Esseen CLT, we have

n+k

P

γ
Wl ≤ −

! ≥

Q



Dn+k + γ
n+k
Vn+k
n+k

B′n+k
√n + k

.



−

Xl=1


We also have the following inequalities from the Appendix C of [9].

q



Dn+k ≤

Vn+k ≥

n
n + k
n
n + k

C

−

VC +

k
n + k
k
n + k

R(d),

VS(d)

K3
n + k

,

−

29

(84)

(85)

(86)

where K3 > 0 is some constant. Further, from (41) and (42), we can show that An+k is bounded and hence

B′n+k is bounded by a constant B′ > 0. Using (85) and (86), we get

Q



Dn+k + γ
n+k
Vn+k
n+k

Q



≥

Substituting the above in (84), taking γ =



q


|X | −

1
2

(cid:0)

(cid:1)

kR(d) + γ

nC
nVC + kVS(d)

−

K3 !

−

.

p

log(n+1) and using (82), we get the required bound.

REFERENCES

[1] A. S. Vora and A. A. Kulkarni, “A minimax theorem for ﬁnite blocklength joint source-channel coding over an AVC,” in Twenty-ﬁfth

National Conference on Communications (NCC).

IEEE, 2019, pp. 1–6.

[2] A. Humayed, J. Lin, F. Li, and B. Luo, “Cyber-physical systems securitya survey,” IEEE Internet of Things Journal, vol. 4, no. 6, pp.

1802–1831, 2017.

[3] J. Slay and M. Miller, “Lessons learned from the maroochy water breach,” in International Conference on Critical Infrastructure Protection.

Springer, 2007, pp. 73–82.

[4] R. Langner, “Stuxnet: Dissecting a cyberwarfare weapon,” IEEE Security & Privacy, vol. 9, no. 3, pp. 49–51, 2011.

[5] M. Maschler, E. Solan, and S. Zamir, Game Theory. Cambridge University Press, 2013.

[6] A. A. Kulkarni and T. P. Coleman, “An optimizer’s approach to stochastic control problems with nonclassical information structures,”

IEEE Transactions on Automatic Control, vol. 60, no. 4, pp. 937–949, 2015.

[7] R. Ahlswede, “A note on the existence of the weak capacity for channels with arbitrarily varying channel probability functions and its

relation to shannon’s zero error capacity,” The Annals of Mathematical Statistics, vol. 41, no. 3, pp. 1027–1033, 1970.

[8] Y. Polyanskiy, H. V. Poor, and S. Verd´u, “Channel coding rate in the ﬁnite blocklength regime,” Information Theory, IEEE Transactions

on, vol. 56, no. 5, pp. 2307–2359, 2010.

[9] V. Kostina and S. Verd´u, “Lossy joint source-channel coding in the ﬁnite blocklength regime,” Information Theory, IEEE Transactions

on, vol. 59, no. 5, pp. 2545–2575, 2013.

[10]

I. Csiszar and J. K¨orner, Information theory: coding theorems for discrete memoryless systems. Cambridge University Press, 2011.

 
 
30

[11] O. Kosut and J. Kliewer, “Finite blocklength and dispersion bounds for the arbitrarily-varying channel,” arXiv preprint arXiv:1801.03594,

2018.

[12] S. T. Jose and A. A. Kulkarni, “Linear programming-based converses for ﬁnite blocklength lossy joint source-channel coding,” IEEE

Transactions on Information Theory, vol. 63, no. 11, pp. 7066–7094, 2017.

[13] ——, “Linear programming based converses for ﬁnite-blocklength joint-source channel coding,” IEEE Transactions on Information Theory,

vol. 63, no. 11, pp. 7066–7094, 2017.

[14] ——, “Improved ﬁnite blocklength converses for Slepian-Wolf coding via linear programming,” accepted (currently in press), IEEE

Transactions on Information Theory, 2018.

[15] J. M. Borden, D. M. Mason, and R. J. McEliece, “Some information theoretic saddlepoints,” SIAM journal on control and optimization,

vol. 23, no. 1, pp. 129–143, 1985.

[16] M. Hegde, W. Stark, and D. Teneketzis, “On the capacity of channels with unknown interference,” IEEE Transactions on Information

Theory, vol. 35, no. 4, pp. 770–783, 1989.

[17] T. Bas¸ar et al., “A complete characterization of minimax and maximin encoder-decoder policies for communication channels with

incomplete statistical description,” IEEE Transactions on Information Theory, vol. 31, no. 4, pp. 482–489, 1985.

[18] B. Hughes and P. Narayan, “Gaussian arbitrarily varying channels,” IEEE Transactions on Information Theory, vol. 33, no. 2, pp. 267–284,

1987.

[19] S. T. Jose and A. A. Kulkarni, “On a game between a delay-constrained communication system and a ﬁnite state jammer,” in Decision

and Control (CDC), 2018 IEEE 57th Annual Conference, to appear.

[20] ——, “Shannon meets von neumann: A minimax theorem for channel coding in the presence of a jammer,” Under review with IEEE

Transactions on Information Theory; arXiv: https://arxiv.org/abs/1811.07358, 2018.

[21] D. Blackwell, L. Breiman, and A. Thomasian, “The capacities of certain channel classes under random coding,” The Annals of Mathematical

Statistics, vol. 31, no. 3, pp. 558–567, 1960.

[22] R. Ahlswede, “Elimination of correlation in random codes for arbitrarily varying channels,” Probability Theory and Related Fields, vol. 44,

no. 2, pp. 159–175, 1978.

[23] A. Lapidoth and P. Narayan, “Reliable communication under channel uncertainty,” IEEE Transactions on Information Theory, vol. 44,

no. 6, pp. 2148–2177, 1998.

[24] T. M. Cover and J. A. Thomas, Elements of information theory.

John Wiley & Sons, 2012.

[25] V. Kostina and S. Verd´u, “Fixed-length lossy compression in the ﬁnite blocklength regime,” Information Theory, IEEE Transactions on,

vol. 58, no. 6, pp. 3309–3338, 2012.

[26] C. E. Shannon, “Coding theorems for a discrete source with a ﬁdelity criterion,” IRE Nat. Conv. Rec, vol. 4, no. 142-163, p. 1, 1959.

[27] A. A. Kulkarni and T. P. Coleman, “An optimizer’s approach to stochastic control problems with nonclassical information structures,”

IEEE Transactions on Automatic Control, vol. 60, no. 4, pp. 937–949, 2015.

[28] M. Conforti, G. Cornu´ejols, and G. Zambelli, Integer programming. Springer, 2014, vol. 271.

[29] O. Kosut and J. Kliewer, “Dispersion of the discrete arbitrarily-varying channel with limited shared randomness,” in Information Theory

(ISIT), 2017 IEEE International Symposium on.

IEEE, 2017, pp. 1242–1246.

