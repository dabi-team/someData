1
2
0
2

b
e
F
0
2

]

G
L
.
s
c
[

2
v
4
1
1
7
0
.
1
1
0
2
:
v
i
X
r
a

Targeted Query-based Action-Space Adversarial
Policies on Deep Reinforcement Learning Agents

Xian Yeow Lee, Yasaman Esfandiari, Kai Liang Tan, Soumik Sarkar
Department of Mechanical Engineering
Iowa State University
Ames, IA 50011
{xylee, yasesf, kailiang, soumiks}@iastate.edu

Abstract

Advances in computing resources have resulted in the increasing complexity of
cyber-physical systems (CPS). As the complexity of CPS evolved, the focus has
shifted from traditional control methods to deep reinforcement learning-based
(DRL) methods for control of these systems. This is due to the difﬁculty of
obtaining accurate models of complex CPS for traditional control. However, to
securely deploy DRL in production, it is essential to examine the weaknesses of
DRL-based controllers (policies) towards malicious attacks from all angles. In this
work, we investigate targeted attacks in the action-space domain, also commonly
known as actuation attacks in CPS literature, which perturbs the outputs of a
controller. We show that a query-based black-box attack model that generate
optimal perturbations with respect to an adversarial goal can be formulated as
another reinforcement learning problem. Thus, such an adversarial policy can
be trained using conventional DRL methods. Experimental results showed that
adversarial policies which only observe the nominal policy’s output generate
stronger attacks than adversarial policies that observe the nominal policy’s input
and output. Further analysis reveals that nominal policies whose outputs are
frequently at the boundaries of the action space are naturally more robust towards
adversarial policies. Lastly, we propose the use of adversarial training with transfer
learning to induce robust behaviors into the nominal policy, which decreases the
rate of successful targeted attacks by half.

1

Introduction

Learning-based approaches, speciﬁcally deep learning, are increasingly being applied to dynamical
systems and control. This trend is especially prevalent in cyber-physical systems (CPS), which are
capable of generating an enormous amount of data from monitoring systems and sensor readings.
Since deep learning models (deep neural networks) are trained in an end-to-end fashion by leveraging
the information-rich data, these methods have gained traction for solving various tasks. Examples of
deep learning being applied in CPS include fault and anomaly detection [1, 2], system monitoring [3],
and supporting human-machine interaction procedures [4, 5].

Deep Reinforcement Learning (DRL), which combines deep neural networks as function approx-
imators with reinforcement learning algorithms for sequential decision making have also enjoyed
tremendous breakthroughs in recent years, due to algorithmic and computing resource advancements.
For example, DRL has successfully been applied to problems such as inverse design [6, 7], trafﬁc
resource management [8, 9], recommendation systems [10] and even image processing [11]. Within
the area of CPS, RL 1 have mainly been applied in place of traditional controllers, where the RL agent

1The terms DRL and RL are used interchangeably in this manuscript.

Presented at Deep RL Workshop, NeurIPS 2020.

 
 
 
 
 
 
makes decisions based on a set of observations (e.g., sensor readings, states of the system), and the
decisions get converted into physical actions. While traditional control methods are well-developed,
extensive knowledge of the system’s behavior is often required to design a suitable controller. As
such, traditional control methods are often challenging to scale to complex, high-dimensional systems.
On the other hand, RL methods do not require knowledge of the system but only requires access
to the actual system or a surrogate of the system to learn a good control policy. Subsequently, a
control policy with complex behaviors can emerge from training an RL agent as a CPS controller.
More importantly, obtaining a highly accurate model for real-world, complex CPS is often non-trivial.
Therefore, RL-approaches which leverage the abundance of data to learn a control policy which
can be adapted to real systems is extremely useful. Various case studies have proposed using RL as
controllers for complex, high-dimensional CPS, such as robotic manipulation and navigation [12, 13].

Unfortunately, RL-based control has also been proven to be vulnerable to a host of malicious
adversarial attacks [14, 15]. These attacks seek to inject perturbations of various forms through
the multiple channels in which the controller interacts with the environment. In CPS applications,
especially safety-critical systems, the cost of such a successful attack on the RL controller can be
extremely high. Thus, it is vital to understand the various mechanisms in which an RL controller
can be attacked to mitigate such scenarios. In this paper, we focus on targeted action-space attacks
where the perturbations are injected into the RL-controller’s output signal. Since action-space attacks
(also known as actuation attacks) are relatively less studied in the context of RL-based controls,
this work serves to ﬁll in such a gap. Speciﬁcally, we contribute to the following in this paper: We
propose a query-based black-box DRL-based targeted attack model that perturbs another RL agent to
an adversarially-deﬁned goal. We demonstrate that full-state information is not required for to learn
a successful attack. Instead, only actuation information and information related to the adversarial
goal is sufﬁcient for learning effective attacks. Furthermore, we study the characteristics of the
nominal policies and empirically show that policies that favor extreme actions are naturally more
robust towards attacks. Additionally, we study the efﬁcacy of different adversarial training schemes
and conclude that for our proposed attack model, a combination of transfer learning and adversarial
training is required to robustify nominal policies against such attacks.

2 Related Works

The vulnerability of control systems to adversarial attacks has been shown by many research studies
with different applications. The security concerns for control systems has been discussed as early
as 1989 when attack detection and system recovery were proposed in [16] and [17]. As CPS have
speciﬁc vulnerabilities dependant on their structure, the general approach is to analyse the adverse
effect of particular attacks on speciﬁc systems. For example, in distributed multi-agent systems, there
exists attacks which disrupt the states of the system without changing the system’s outputs (stealthy
attacks), for which several algorithms have been proposed [18–20].

Reinforcement learning agents have recently been found susceptible to attacks as well. Their
vulnerability has been discussed in [21, 22], where the authors proposed a method to attack a
DRL agent’s state-space uniformly at each every time step. Several other researchers have also
proposed state-space attack and defence strategies for DRL agents [20, 23–26]. While the above
studies focus on white-box attacks, [27] attempted to ﬁnd optimal black-box attacks and found
that smooth policies are naturally more resilient. Also, [28] used sequence-to-sequence models
to predict a sequence of actions that a trained agent will take in the future and showed that the
black-box attacks can make a trained agent malfunction after a speciﬁc time delay. Another study
by [29] also proved the vulnerability of RL agents to action-space attacks by framing the problem
as a constrained-optimization problem of minimizing the total reward by taking into account the
dynamics of the agent. Meanwhile, [30] proposed that action-space robustness is a new criteria for
real world use of RL and devised an algorithm which makes RL agents more robust to action-space
noise. [31] also adversarially trained DRL agents using Projected Gradient Sign (PGD) method [32]
to provide action-space robustness. Another branch of studying the robustness of DRL agent focuses
on adversarial policies in which another DRL agent is introduced in the system to learn an adversarial
policy which makes the nominal DRL agent robust both in natural and adversarial environments [25].
Additionally, [33] used adversarial policy training methods in a multi-agent system to train an
adversarial agent to attack the nominal agent. In this paper, we focus on a similar adversarial policy
training method to train a policy to mount targeted attacks on the nominal agent in the action-space.

2

3 Methodology

3.1 Deep Reinforcement Learning

In this section, we provide a brief background on DRL algorithms. Speciﬁcally, we focus on model-
free policy-iteration based methods that are tailored for continuous control. Let st and at denote
the (continuous, possibly high-dimensional) state and action at time t respectively. Given an at
that is selected according to policy π at st, a corresponding reward signal, rt is computed using the
reward function, rt = R(st, at). The reward function is typically predeﬁned and embedded in the
environment and remains unknown to the RL agent. The RL agent recursively interacts with the
environment with at and transitions into the next state, st+1 = E(st, at), until a deﬁned horizon
t = T , or a completion condition is met. Interacting with the environment creates a sequence of
state-action vectors, also known as a trajectory τ = (st, at, st+1, at+1, ...). The goal of the RL agent
is to learn an optimal policy π∗ so that the discounted cumulative future reward of the entire trajectory,
R(τ ) = (cid:80)T
t=0 γtrt, is maximized. The discount factor, γ ∈ (0, 1), allows the RL agent to either
favor short-term rewards or maximize potentially larger rewards in the future.

Policy-based methods directly learns a state-action mapping π that maximizes the expected return
J(π) = E[R(τ )]. Here, π is also represented with a deep neural network parameterized by θ, where
θ is optimized using gradient ascent, i.e.:

θk+1 = θk + α∇θJ(πθ)|θk
In practice, the policy returns a distribution over actions via a representation of a parameterized
distribution (e.g., Gaussian), where the optimal parameters of the distribution (µ, σ) are estimated by
the neural network. The optimal action a∗
t ∼ π∗
a∗

t is then sampled from the distribution.
θ , π∗ = arg max

E[R(τ )]

(1)

(2)

π

Variants of policy optimization algorithms used in DRL include Proximal Policy Optimization
(PPO) [34] and Asynchronous Methods for Deep Reinforcement Learning [35].

3.2 Targeted Adversarial Perturbations

In the following sections, we refer to the RL agent
subjected to targeted adversary attacks as the nominal
policy, πnom, and the RL agent mounting the targeted
attack on the nominal policy as the adversarial pol-
icy, πadv. Previous works have demonstrated the
vulnerability of RL algorithms to optimization-based
action-space perturbations [29]. However, such ap-
proaches, called "white-box attacks," requires knowl-
edge of πnom’s architecture and weights for com-
puting the attack. Moreover, while information from
πnom may be useful for computing untargeted attacks,
it is considerably non-trivial to compute a targeted
attack via optimization-based approaches. This is be-
cause πnom only encodes information about optimal
actions with respect to the nominal goal. In other
words, a query on πnom will not reveal information
on which action is optimal with respect to some ar-
bitrarily deﬁned adversarial goal. This motivates us
to propose an alternative approach where we train a
separate RL agent to learn an adversarial policy to predict perturbations which guide πnom to an
adversarial goal, without requiring knowledge of πnom’s internal structure, i.e., a black-box attack.
To formulate our targeted attack model, we begin with the assumption that the adversary has access
to the following: 1) A working copy or surrogate of the trained nominal policy and environment. 2)
Nominal policy’s input and/or output channels (readings from sensors and actuators). 3) Capability
to modify the nominal policies output channel (perturbing the actions)

Figure 1: Overview of our proposed frame-
work for training an adversarial policy, πadv
to mount targeted attacks in the action-space
domain on a trained nominal policy πnom.

Next, we consider the objective of the targeted attack model, which is to compute an optimal sequence
of perturbations to add to πnom’s actuation such that πnom arrives at an adversarial goal, conditioned

3

only on πnom’s input/output information. Since we have assumed πnom is trained sufﬁciently and
thus can be considered a stationary policy during deployment, the interactions of πnom with the
environment and corresponding state transitions are essentially another Markov Decision Process
(MDP) with stationary dynamics. As such, we can cast the objective of the targeted attack model as
another RL formulation, where the interactions of πnom with the actual environment are combined
into an adversarial training environment for πadv. In this adversarial environment, the goal of πadv is
to maximize the reward with respect to the adversarial goal, subject to environment’s state transitions
and πnom’s actions. Formally, the objective of the the targeted attack model can be expressed as:

max Radv =

T
(cid:88)

t=0

ˆR( ˆst, δt)

where :
ˆst = {st, at, dadv}, δt ∼ πadv( ˆst)
st = E(st−1, at−1 + δt−1), at ∼ πnom(st)

(3)

In the formulation above, ˆst denotes the state observed by πadv at time step t, which consists of
concatenated vectors of the nominal state observation st, nominal action at sampled from πnom
and a distance measure to the adversary goal dadv,t. Additionally, δt denotes the perturbations
sampled from πadv and E(.) signiﬁes state transitions of the environment due to the previous states,
actions and perturbations. Without loss of generalizability, the reward function that we used in our
experiments for the formulation above is expressed by:

ˆR =

(cid:26)(cid:107)dadv,t−1(cid:107) − (cid:107)dadv,t(cid:107) − I(λ) if dadv > 0
if dadv = 0

1

(4)

where I(λ) is a penalty indicator term that penalizes πadv if πnom reaches the nominal goal. In our
experiments, we maintain I(λ) as 1 if the agent is at the nominal goal; otherwise, I(λ) is 0. We
highlight that the reward function shown in Equation 4 is just one possible form that works for
training πadv. Nonetheless, other forms of reward function may be crafted, without affecting the
RL formulation of the targeted attack model. With the targeted attack’s objective formulated as a
RL problem, we can now obtain the attack model, represented by πadv via conventional RL training
algorithms. A pseudocode for training πadv using conventional RL algorithms is also shown in Alg.1
in Supplementary Materials.

3.3 Differences from other attack models

We begin with a brief contrast of our attack model with previously proposed attack models and
discuss their respective beneﬁts and drawbacks. In state-space attacks, the attack model perturbs or
modiﬁes the input of the RL agent such that the agent is fooled into taking wrong actions. Hence,
under the assumption of optimal perturbations, the adversary can consistently trick πnom to take
actions that eventually lead it to an adversarial goal without any resistance. In contrast, action-space
attacks do not aim to fool πnom but add perturbations to πnom’s output such that the resultant actions
lead the agent to the adversarial goal. Since the observations of πnom are not corrupted and with the
assumption that πnom ≡ π∗, πnom will output an action such that for every δt added to at, the action
at+1 selected by πnom will still be optimal. In other words, a sufﬁciently trained πnom will tend
to provide a correcting force at the next time step, which may cancel out the effect of the δt. This
renders the targeted attack model in the action-space more challenging and possibly more limited in
efﬁcacy than observation space perturbations.

Another comparison that we would like to discuss is the differences between optimization-based
attacks and learning-based attacks. In optimization-based attacks, the optimal perturbations are
typically computed via gradient-based optimization strategies. The advantage of these attacks is
that they can be computed online, with relatively light compute. However, these methods often
require access to internal information of πnom, such as network architectures and weights, which
may be non-trivial to obtain. Additionally, the knowledge encoded within πnom typically does not
contain accurate information with regard to other arbitrary goals; hence mounting targeted attacks
is not straight-forward. On the other hand, our proposed learning-based method circumvents both
disadvantages of optimization-based methods. Access to input/output information is signiﬁcantly

4

less challenging to obtain that πnom’s network architecture and weights information, hence making
such black-box attacks more plausible. Furthermore, by parameterizing the attack model with
another goal-conditioned policy, we can learn the optimal perturbations that are directly related to
the adversarial goals, which enables us to carry out targeted attacks. Nonetheless, learning-based
methods do require additional off-line training time and larger computational resources. The efﬁcacy
of learning-based attacks is also highly dependent on the training process, which assumes that the
training data distribution, in this case the interactions between πnom and the environment, is constant.
If πnom gets re-trained signiﬁcantly, we hypothesize that the existing πadv would not work very
well and will have to be re-trained as well. However, it is still imperative to examine learning-based
targeted attacks since the risk of CPS subjected to such attacks exists, and the subsequent effect of
being driven to an adversarial state can be costly.

4 Results and discussion

4.1 Experimental descriptions

To test the efﬁcacy of our attack models, we perform experiments on two platforms, speciﬁcally
Point-Goal and Car-Goal, found in the suite of environments proposed by [36]. In both environments,
the task for the πnom is to navigate to a randomly generated goal. To obtain πnom as a testing
platform for mounting our attacks, we train πnom with the PPO algorithm on the two environments
for 5E6 steps. After obtaining the trained πnom, we trained πadv to mount the targeted attacks on
πnom. The training of πnom was also performed using PPO with 5E6 steps.

4.2 Adversarial policy for targeted attack

In this section, we provide the results of training πadv to learn the optimal perturbations, which guides
πnom towards an adversarial goal for ﬁve different seeds in the two different environments. We ﬁrst
demonstrate the rewards of training πnom in the ﬁrst column of Fig. 2. As observed, the training
of πnom converged and is capable of maintaining a high reward across different seeds. Having a
sufﬁciently trained πnom allows us to conﬁdently test the effectiveness of πadv as an attack model
and attribute the πnom failures to the efﬁcacy of the proposed attack.

Figure 2: Summary of training rewards for nominal and adversarial policies in Point-Goal and
Car-Goal environments. (a) High training rewards for πnom illustrates that agents are well-trained
in both environments (b) Training rewards for πadv that observes πnom’s state and actions. (c)
Training rewards for πadv that only observes πnom’s actions. Comparing (b) and (c), we observe that
perturbations generated by πadv that only observes πnom’s actions are more effective.
Next, we present the training rewards of πadv trained in two variants of adversarial environments. The
ﬁrst variant of πadv, denoted as StateAware in the second column of Fig. 2, observes πnom’s inputs

5

Table 1: Mean success rates/episode of πadv in perturbing πnom to an adversarial goal during
evaluation. (cid:107)dG(cid:107) denotes the minimum distance an adversarial goal is generated from the nominal
goal. As (cid:107)dG(cid:107) increases, the adversary success rate generally decreases as it is harder to perturb the
πnom to the adversarial goal.

Point-Goal

Car-Goal

Min. (cid:107)dG(cid:107) Goal

StateAware

StateU naware

StateAware

StateU naware

0.5

1.0

1.5

Adversary
Nominal

Adversary
Nominal

Adversary
Nominal

6.22 ± 5.29
2.66 ± 2.90

5.88 ± 5.26
2.32 ± 2.85

5.30 ± 5.17
1.84 ± 2.58

11.12 ± 4.52
0.78 ± 1.08

11.28 ± 4.31
0.50 ± 1.04

11.30 ± 4.04
0.58 ± 0.90

1.46 ± 1.90
3.62 ± 3.36

1.52 ± 2.10
3.12 ± 3.27

1.32 ± 1.75
3.26 ± 3.42

5.46 ± 5.53
4.74 ± 4.40

5.20 ± 5.76
4.54 ± 4.22

4.76 ± 4.04
3.92 ± 5.57

such as position readings, velocity readings, distance to the nominal goal, the actuation outputs, and
distance to the adversarial goal. The second variant of πadv, denoted as StateU naware, observes
only πnom’s actuation output and distance to the adversarial goal. The training rewards of this
variant is shown in the third column of Fig. 2. Interestingly, we observed that the πadv trained in
StateU naware, which consists of a reduced set of observations, achieved higher overall rewards. In
the Point-Goal environments, we see that only two of the ﬁve πadv was able to achieve high rewards
in StateAware. In contrast, four of the ﬁve πadv trained in the StateU naware environment were
capable of converging to high rewards. A similar trend is also observed in the Car-Goal environment,
which has more complex dynamics. In the StateAware variant of the training environment, only one
πadv shows signs of learning during the 5 million training steps 2. However, in the StateU naware
variant, two of the πadv showed signs of learning at earlier stages of training and achieved signiﬁcantly
higher rewards.

For validation, we considered three scenarios categorized by the minimum distance an adversarial
goal is generated from the nominal goal, denoted by (cid:107)dG(cid:107). These three scenarios ensure that we
evaluate πadv’s ability on different levels of difﬁculty, where the adversarial goal may be near or
far from the nominal goal. For each scenario, each πadv is evaluated for ten episodes, with each
episode having a 1,000 steps. A new pair of adversarial and nominal goal at least (cid:107)dG(cid:107) apart is
randomly generated each time πnom reaches either the adversarial or nominal goal. The number of
time πnom reaches either one of the goals is averaged across all episodes and seeds and tabulated
in Table 1. Overall, we observed that the success rates of πadv in generating perturbations that lead
πnom to the adversarial goal are higher for the Point-Goal environment compared to the Car-Goal
environment. This is expected since the dynamics of the Car-Goal environment is more complex and
thus harder to compute a perturbation which controls πnom. However, for both environments and
across all scenarios, the StateU naware variant of πadv is signiﬁcantly more effective in mounting
the targeted attacks. Such observation is counter-intuitive since this variant of πadv has access to
less information on πnom. This could be attributed to the fact that sufﬁcient information is already
embedded in the actions of πnom for πadv to learn the optimal δt. Consequently, additional state
information of πnom only acts as detrimental noise to πadv’s learning process.

Overall, these results suggest that πnom trained with conventional RL strategies are susceptible to
targeted action-space attacks. More importantly, our results showed that with our proposed attack
model, πadv only requires access to the πnom output (actions) rather than both input and output
(state and actions) channels to learn an effective targeted attack. In terms of CPS applications, this
is especially crucial since requiring access to only the πnom actions lowers the barrier of entry for
mounting such targeted attacks.

4.3 On robustness of nominal policy

In this section, we provide some insights into what makes a particular policy naturally robust or
susceptible to targeted action-space attacks. Having demonstrated that πadv that was trained only on
the output channels of πnom is superior over πadv that was trained on the input and output channels,

2Codes are available at https://github.com/xylee95/targeted_adversarial_policies

6

Figure 3: Investigation of different πnom behaviors in Car-Goal reveals that a policy that frequently
output actions that are at the limits of the entire action space, i.e., 1 or -1, is naturally more robust
towards action-space targeted attacks (low adversarial rewards). Colored data points corresponds to
original seeds shown in Fig. 2 and gray data points corresponds to additional random seeds.

we focus the consequent studies only on the StateU naware variant of πadv. As illustrated in the
third column of the Car-Goal environment in Fig. 2, it is clear that certain trajectories of πadv have
a more challenging time learning the optimal perturbations to achieve high rewards. To test if this
phenomenon is driven by some characteristics of the underlying πnom, we trained another 15 seeds
of πnom and πadv to observe the training behavior of πadv.

In the environments that we used, the range of actions that πnom can take is bounded between
[-1,1]. We evaluated each πnom’s behavior under three scenarios of (cid:107)dG(cid:107), as shown in Table 1, and
aggregated the number of actions in both action dimensions that are either -1 or 1 over the entire
trajectory of 1,000 steps. In Fig. 3, we visualize the ﬁnal adversarial training rewards of πadv as a
function of the average number of actions that πnom takes that is at the bounds of the action-space.
Each data point in the ﬁgure represents a unique πnom trained on a different seed. As observed, we
see a clear negative correlation between the number of actions at the action-space limits with the
adversarial reward. A πnom that frequently outputs actions that are inside the range of [-1,1] tend to
be more susceptible to targeted attacks, as reﬂected by higher adversarial rewards. In contrast, a πnom
that frequently chooses actions that are at the bounds of [-1,1] tend to be more robust to targeted
attacks mounted by πadv. This conﬁrms the hypothesis that certain πnom are inherently robust to
such targeted adversarial attacks and the common underlying factor among these policies is that they
frequently output actions that are either maximum or minimum (similar to a on-off control policy).
This is in contrast to previously made observations that identiﬁed policies which are more smooth
with respect to state observations are more robust to state space attacks [27].

4.4 Adversarial training schemes

Figure 4: Training curves of πnom robustiﬁed via adversarial training with (a) untrained πnom and
πadv (b) untrained πnom and trained πadv and (c) trained πnom πadv. In (c) the weights of the trained
πnom were loaded and further ﬁne-tuned by training the agent with targeted attacks. Increasing
rewards in (c) reﬂects the capability of the nominal policy to counter targeted attacks while methods
(a) and (b) shows no signs of robustiﬁcation.

To induce the behavior of taking extreme actions in πnom to be robust to targeted adversarial
perturbations, we consider training πnom with adversarial training that involves the inclusion of
adversarial examples in the training process of the policy. Here, we consider three variants of
adversarial training schemes and show that only one variant successfully robustiﬁes πnom.

7

In the ﬁrst variant, we assumed both πnom and πadv are initially untrained and trained both policies in
tandem. In this scheme, πnom learns to maximize its reward by reaching the nominal goal, while πadv
also learns to maximize the adversarial reward by learning the optimal δt. This variant of training
scheme is ideal since we do not assume access to a trained πadv and only require the alternative
training of two policies with different objectives. Nonetheless, a drawback of this scheme is that
the training process of both policies might not converge since both policies are continually being
updated. This also breaks the assumption that the interactions between πnom and the environment is
an MDP with stationary dynamics in the context of training πadv. For the second variant, we assume
that we have access to a trained πadv and train πnom with the targeted attacks mounted. In this
formulation, πadv is ﬁxed, and πnom learns a policy that maximizes its reward while being subjected
to the perturbations generated by πadv. Lastly, in the third variant, we assume that both πnom and
πadv are already trained, and we further robustify πnom by ﬁne-tuning tits weights via adversarial
training. This third variant is similar to the second variant, except that πnom isn’t trained from scratch
but instead trained using a form of transfer learning where the weights of the trained πnom are used
as a starting point for further ﬁne-tuning.

In Fig. 4, we show the results of robustifying πnom via the three variants of adversarial training
schemes. As seen in Fig. 4(a) and (b), the ﬁrst two variants failed to robustify πnom. Nonetheless,
the third method of adversarial training, which leverages the prior weights of a pre-trained πnom
as a starting point, displayed signiﬁcant learning improvements when compared with the previous
methods. We highlight that the rewards of πnom shown in Fig. 4 are the rewards achieved by πnom
while being subjected to targeted attacks by πadv. Additionally, only random seeds of πnoms that
were initially vulnerable to targeted attacks were selected for these adversarial training schemes.
This ensures that the improvements in πnom seen in Fig. 4(c) can be solely attributed to the effects
of adversarial training. For additional results on the effects of combining transfer learning with
adversarial learning to robustify πnom, please refer to Sec. 2 in the Supplementary Materials.

5 Conclusions and future work

In this study, we proposed a learning-based black-box targeted attack model that perturbs the output
of a RL agent, πnom, to lead the agent to an adversarial goal. We show that such an attack model
can be represented by another RL agent, πadv, which learns the optimal perturbations given only
the input and/or output channels of πnom. Experimental results reveal that our proposed attack
model is feasible and the πadv which only observes the output channel, can mount a more effective
targeted attack. This reveals a vulnerability in systems where the decisions are controlled by a
RL agent, even in situations where knowledge πnom’s weights and architecture are unknown or
well-protected. Additionally, we observed that πnom are naturally robust when they frequently output
action magnitudes that are at the limits of the action range. These results led us to propose a defense
scheme that combines adversarial training with transfer learning to induce such behaviors into πnom
to robustify them against such attacks while maintaining a comparable performance under nominal
conditions. The results of training πnom via the proposed defense scheme showed that the success
rate of πadv in driving πnom to the adversarial goal decreased by half. Collectively, our experiments
illustrate a potential vulnerability in RL agents from targeted attacks in the action-space. Defensive
schemes such as adversarial training for πnom provide a certain degree of robustness towards such
attacks, although it is by no means a fully effective method.

While we have shown that policies that frequently output extreme actions are more robust towards
adversarial attacks and presented one method to induce such behaviors into policies, we acknowledge
that such a robustness-inducing mechanism may not be ideal. For one thing, policies that emit such
actions closely imitate the behaviors of on-off controllers (bang-bang controllers). In the context
of real CPS, such controllers often have negative practical implications such as reduced lifespan of
physical parts due to an increase in friction forces, fatigue, and more. Hence, a future direction is to
develop methods to robustify the policies towards action-space attacks while maintaining a smoother
control behavior. Additionally, while adversarial training does help in robustifying πnom, the success
of robustiﬁcation is also highly dependent on the random seeds. We hypothesize the failure to
robustify certain πnom via adversarial training may be because the perturbations generated by πadv is
too strong that πnom fails to obtain a useful learning signal from all the bad trajectories. Thus, future
works may investigate advanced adversarial training schemes such as curriculum learning, which
increases the severity of attacks on a ﬁxed schedule.

8

Broader Impact

Our proposed method for mounting targeted action space attacks highlights a broad, existing vulner-
ability of systems deployed with RL. Although RL is applied in a wide range of applications, we
foresee that the impacts of such attacks are especially high in cyber-physical systems in which the
decisions of the RL algorithm corresponds to physical actions or actuation.

While it is clearly unethical for the proposed attacks to be carried out with malicious intents, we
hope that highlighting the existence of such attacks to the community will create an awareness.
Consequently, this awareness can hopefully spur the development of RL algorithms with built-in
safety mechanisms. With that in mind, we believe that researchers involved in ML security and
development of new RL algorithms will directly beneﬁt from the results of this study. Additionally,
industrial practitioners who are leveraging RL algorithms may also gain a new perspective on the
robustness and security of their systems. Nonetheless, it is possible that our proposed methods can be
used with malicious intent and we hope that our discussions on robustiﬁcation of RL algorithms via
adversarial training discourage such attempts. Since the data used in the results was purely simulated,
we believe the concern of biases in data is not applicable to this study.

Acknowledgments and Disclosure of Funding

This work was supported in part by NSF grant CNS-1845969.

References

[1] Yuan Luo, Ya Xiao, Long Cheng, Guojun Peng, and Danfeng Daphne Yao. Deep learning-
based anomaly detection in cyber-physical systems: Progress and opportunities. arXiv preprint
arXiv:2003.13213, 2020.

[2] Farnaz Seyyed Mozaffari, Hadis Karimipour, and Reza M Parizi. Learning based anomaly
detection in critical cyber-physical systems. In Security of Cyber-Physical Systems, pages
107–130. Springer, 2020.

[3] Xian Yeow Lee, Sourabh K Saha, Soumik Sarkar, and Brian Giera. Automated detection of part
quality during two-photon lithography via deep learning. Additive Manufacturing, 36:101444,
2020.

[4] Kin Gwn Lore, Nicholas Sweet, Kundan Kumar, Nisar Ahmed, and Soumik Sarkar. Deep value
of information estimators for collaborative human-machine information gathering. In 2016
ACM/IEEE 7th International Conference on Cyber-Physical Systems (ICCPS), pages 1–10.
IEEE, 2016.

[5] Hongyi Liu, Tongtong Fang, Tianyu Zhou, Yuquan Wang, and Lihui Wang. Deep learning-based
multimodal control interface for human-robot collaboration. Procedia CIRP, 72:3–8, 2018.

[6] Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de

novo drug design. Science advances, 4(7):eaap7885, 2018.

[7] Xian Yeow Lee, Aditya Balu, Daniel Stoecklein, Baskar Ganapathysubramanian, and Soumik
Sarkar. A case study of deep reinforcement learning for engineering design: Application to
microﬂuidic devices for ﬂow sculpting. Journal of Mechanical Design, 141(11), 2019.

[8] Kathy Jang, Eugene Vinitsky, Behdad Chalaki, Ben Remer, Logan Beaver, Andreas A Ma-
likopoulos, and Alexandre Bayen. Simulation to scaled city: zero-shot policy transfer for
trafﬁc control via autonomous vehicles. In Proceedings of the 10th ACM/IEEE International
Conference on Cyber-Physical Systems, pages 291–300, 2019.

[9] Kai Liang Tan, Subhadipto Poddar, Soumik Sarkar, and Anuj Sharma. Deep reinforcement
learning for adaptive trafﬁc signal control. In Dynamic Systems and Control Conference, volume
59162, page V003T18A006. American Society of Mechanical Engineers, 2019.

9

[10] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and
Zhenhui Li. Drn: A deep reinforcement learning framework for news recommendation. In
Proceedings of the 2018 World Wide Web Conference, WWW ’18, page 167–176, Republic and
Canton of Geneva, CHE, 2018. International World Wide Web Conferences Steering Committee.

[11] Ryosuke Furuta, Naoto Inoue, and Toshihiko Yamasaki. Pixelrl: Fully convolutional network

with reinforcement learning for image processing. IEEE Transactions on Multimedia, 2019.

[12] S. Gu, E. Holly, T. Lillicrap, and S. Levine. Deep reinforcement learning for robotic manipula-
tion with asynchronous off-policy updates. In 2017 IEEE International Conference on Robotics
and Automation (ICRA), pages 3389–3396, 2017.

[13] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi. Target-
driven visual navigation in indoor scenes using deep reinforcement learning. In 2017 IEEE
International Conference on Robotics and Automation (ICRA), pages 3357–3364, 2017.

[14] Tong Chen, Jiqiang Liu, Yingxiao Xiang, Wenjia Niu, Endong Tong, and Zhen Han. Adversarial
attack and defense in reinforcement learning-from ai security view. Cybersecurity, 2(1):11,
2019.

[15] Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar Janjua, Ala Al-Fuqaha,
Dinh Thai Hoang, and Dusit Niyato. Challenges and countermeasures for adversarial attacks on
deep reinforcement learning. arXiv preprint arXiv:2001.09684, 2020.

[16] S. Longhi and A. Monteriù. Fault detection and isolation of linear discrete-time periodic systems
using the geometric approach. IEEE Transactions on Automatic Control, 62(3):1518–1523,
2017.

[17] Michèle Basseville, Igor V Nikiforov, et al. Detection of abrupt changes: theory and application,

volume 104. prentice Hall Englewood Cliffs, 1993.

[18] Kyriakos G Vamvoudakis, Joao P Hespanha, Bruno Sinopoli, and Yilin Mo. Detection in
adversarial environments. IEEE Transactions on Automatic Control, 59(12):3209–3223, 2014.

[19] Ziyang Guo, Dawei Shi, Karl Henrik Johansson, and Ling Shi. Optimal linear cyber-attack on
remote state estimation. IEEE Transactions on Control of Network Systems, 4(1):4–13, 2016.

[20] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun.
Tactics of adversarial attack on deep reinforcement learning agents. In Proceedings of the 26th
International Joint Conference on Artiﬁcial Intelligence, IJCAI’17, page 3756–3762. AAAI
Press, 2017.

[21] Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy
induction attacks. In International Conference on Machine Learning and Data Mining in
Pattern Recognition, pages 262–275. Springer, 2017.

[22] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial

attacks on neural network policies. ICLR Workshop, 2017.

[23] Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary.
In Proceedings of the 17th
Robust deep reinforcement learning with adversarial attacks.
International Conference on Autonomous Agents and MultiAgent Systems, pages 2040–2042,
2018.

[24] Jianwen Sun, Tianwei Zhang, Xiaofei Xie, Lei Ma, Yan Zheng, Kangjie Chen, and Yang Liu.
Stealthy and efﬁcient adversarial attacks against deep reinforcement learning. In AAAI, 2020.

[25] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust
policy learning: Active construction of physically-plausible perturbations. In 2017 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pages 3932–3939. IEEE,
2017.

[26] Aaron Havens, Zhanhong Jiang, and Soumik Sarkar. Online robust policy learning in the
presence of unknown adversaries. In Advances in Neural Information Processing Systems,
pages 9916–9926, 2018.

10

[27] Alessio Russo and Alexandre Proutiere. Optimal attacks on reinforcement learning policies.

arXiv preprint arXiv:1907.13548, 2019.

[28] Yiren Zhao, Ilia Shumailov, Han Cui, Xitong Gao, Robert Mullins, and Ross Anderson. Black-
box attacks on reinforcement learning agents using approximated temporal information. In
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Workshops (DSN-W), pages 16–24. IEEE, 2020.

[29] Xian Yeow Lee, Sambit Ghadai, Kai Liang Tan, Chinmay Hegde, and Soumik Sarkar. Spa-
tiotemporally constrained action space attacks on deep reinforcement learning agents. In AAAI,
pages 4577–4584, 2020.

[30] Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and
applications in continuous control. In International Conference on Machine Learning, pages
6215–6224, 2019.

[31] Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Soumik Sarkar, et al. Robustifying
reinforcement learning agents via action space adversarial training. In 2020 American Control
Conference (ACC), pages 3959–3964. IEEE, 2020.

[32] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.

[33] Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell.
Adversarial policies: Attacking deep reinforcement learning. International Conference on
Learning Representations, 2019.

[34] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

[35] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-
crap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. In International conference on machine learning, pages 1928–1937,
2016.

[36] Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep rein-

forcement learning. 2019.

[37] Yasuhiro Fujita, Toshiki Kataoka, Prabhat Nagarajan, and Takahiro Ishikawa. Chainerrl: A

deep reinforcement learning library. arXiv preprint arXiv:1912.03905, 2019.

6 Supplementary Materials

6.1 Environment Descriptions

The environments we used as a testing platform is a modiﬁed version of the environments proposed by
in [36]. The environments are modiﬁed such two goals, corresponding to the nominal and adversarial
goal are generated for each new trajectory. Fig. 5 shows a sample screenshot of each environment.
In Point-Goal, the RL agent is represented as a simple robot that can actuate forward/backward
and rotate clockwise/anti-clockwise. In Car-Goal, the RL agent is represented by a robot with two-
independent wheels and a free-rolling wheel, with the same translation and rotation ability. While both
environments have the same action dimensions (translation and rotation), the Car-Goal environment
represents a more challenging task with more complex system dynamics. Under nominal conditions,
the agent observes its state, which consists of a set of sensor readings and LiDAR information that
represents the distance of the agent relative to its goal. The agent then outputs an action bounded
between the range of [-1,1] for each actuation dimension.

11

Figure 5: Screenshots of environments used in experiments. Green area, brown area and red object
represents the nominal goal, adversarial goal and the agent respectively. Left image shows a birds-eye
view of the entire space for Point-Goal environment. Right image shows a zoomed-in view of
Car-Goal environment with LiDAR readings visualized as a circular halo above the agent.

Table 2: Effects of combining transfer learning with adversarial training on πnom’s average success
rate of reaching nominal and adversarial goals. The success rate of the targeted adversarial attacks on
πnom decreases by almost half after adversarial training. However, adversarial training also decreases
the general performance πnom under nominal conditions.

Min.
(cid:107)dG(cid:107)

Goals

No Adv. Training With Adv. Training No Adv. Training
with No Attack
with Attack

with Attack

Adv. Training
with No Attack

0.5

1.0

1.5

Adversary
Nominal

Adversary
Nominal

Adversary
Nominal

10.82± 3.01
0.96 ± 0.848

10.6 ± 3.35
0.80 ± 1.10

10.1 ± 3.17
0.66 ± 0.84

6.04 ± 4.89
5.72 ± 4.61

5.68 ± 4.74
5.48 ± 4.31

5.10 ± 4.64
5.38 ± 4.20

N.A.
11.5 ± 1.97

N.A.
11.3 ± 2.01

N.A.
10.9 ± 1.57

N.A.
9.44 ± 3.21

N.A.
9.18 ± 3.12

N.A.
8.88 ± 3.34

6.2 Additional Results

In this section, we provide additional results of πnom’s performance pre- and post- adversarial
training for different scenarios during evaluation. Comparing πnom without adversarial training
and πnom after adversarial training that is subjected to targeted attacks, we observe that the number
of adversarial goals reached decreases almost by half, while the number of nominal goals reached
also increased signiﬁcantly. This illustrates the effectiveness of an adversarial training scheme in
providing a certain degree of robustness to πnom. Nevertheless, such adversarial training schemes
also comes with a cost to the general performance of πnom. Comparing the nominal performance
(i.e., performance without attacks) of πnom with and without adversarial training, we note that the
number of nominal goals reached is slightly lower for πnom that has been adversarially trained.
However, similar observations have also been made with adversarial training schemes for RL in
the state/observation space [31] as well as with supervised deep learning tasks [32]. This reveals
a trade-off between a policy’s performance and ability to generalize to more scenarios, such as
adversarial situations.

Next, we show some anecdotal observations on the mechanisms of robustifying πnom via adversarial
training. In Fig. 6(a), we visualize the average number of extreme actions that πnom outputs pre-
adversarial training (in circular markers) and post-adversarial training (in cross markers) for the
ﬁve seeds shown in Fig.4(c) of the main manuscript. We observe that the number of extreme
actions chosen by πnom increased signiﬁcantly for the three seeds that achieved high rewards during
adversarial training. These results highlight the role of adversarial training as one possible method
for obtaining such policies. Furthermore, this also validates our previous conclusion that a policy that
frequently outputs extreme actions is naturally more robust towards targeted attacks. For the sake of
completeness, we also tried training new πadvs to mount targeted attacks on the robustiﬁed πnom
that has been adversarially trained. The results, shown in Fig. 6(b), reveals that πnom that shows
successful signs of robustiﬁcation during adversarial training (seeds 0, 50, 120) remains robust in

12

Figure 6: (a) Effects of adversarial training on πnom’s behaviour. Circles denote πnom before adver-
sarial training and crosses denote the same set of πnom after adversarial training. The average number
of extreme actions that πnom takes in a single trajectory increased signiﬁcantly after adversarial
training for seeds that were successfully robustiﬁed. (b) Training curves of new πadv for attacking
robustiﬁed πnom that were previously vulnerable to targeted attacks. Observe that once πnom are
successfully robustiﬁed, as shown in Fig.4(c) of the main manuscript for seeds (0, 50, 120), it is
difﬁcult to learn new πadv that effectively mounts the attacks.

the presence of new πadv. Nonetheless, πnom that did not get robustiﬁed during adversarial training
remains vulnerable to new πadv.

6.3 Training algorithm and details

Algorithm 1: Algorithm for training πadv.

1 Initialize πadv, trained πnom, E N ; while steps < N do
2

Initialize st;
while t < T do

3

4

5

6

7

8

9

at ∼ πnom(st);
Construct ˆst = (st, at, dadv) or ˆst = (at, dadv);
δt ∼ πadv ( ˆst);
st+1 = E(st, at + δt)

if time for update then
Update πadv

Table 3: Parameters used to train πnom and πadv.

Parameter

Optimizer
Learning Rate
Entropy Coefﬁcient
Batch Size
Update Interval

Value

Adam
3E-4
0
1024
2048

Relevant details for training the πnom and πadv are provided in this section. To train the πnom,
we parameterize the agent with two deep neural networks, one representing the policy and one
representing the value function. The policy networks consists of three fully-connected layers with the
ﬁrst and second layer having 64 hidden units. The output of the policy network is parameterized with
a Gaussian distribution where the mean and variance of the distribution being estimated by the ﬁnal
fully-connected layer with the same dimensions as the action-space. The value function network also
consists of three fully connected layers with 128, 64 and 1 hidden units respectively, with the output
of the ﬁnal layer being an estimate of the state’s value. A hyperbolic tangent activation function is
used in between all layers. The same network architecture was also used to train πadv as well. Both

13

πnom and πadv were trained using PPO with the parameters shown in Table 3. All experiments and
codes were implemented in ChainerRL [37].

14

