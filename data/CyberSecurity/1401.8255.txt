Quantitative Analysis of Active Cyber Defenses Based on
Temporal Platform Diversity∗

Kevin M. Carter
MIT Lincoln Laboratory
244 Wood St.
Lexington, MA 02420
kevin.carter@ll.mit.edu

Hamed Okhravi
MIT Lincoln Laboratory
244 Wood St.
Lexington, MA 02420
hamed.okhravi@ll.mit.edu

James Riordan
MIT Lincoln Laboratory
244 Wood St.
Lexington, MA 02420
james.riordan@ll.mit.edu

4
1
0
2

n
a
J

1
3

]

R
C
.
s
c
[

1
v
5
5
2
8
.
1
0
4
1
:
v
i
X
r
a

ABSTRACT
Active cyber defenses based on temporal platform diversity
have been proposed as way to make systems more resistant
to attacks. These defenses change the properties of the plat-
forms in order to make attacks more complicated. Unfortu-
nately, little work has been done on measuring the eﬀective-
ness of these defenses. In this work, we use four diﬀerent
approaches to quantitatively analyze these defenses; an ab-
stract analysis studies the algebraic models of a temporal
platform diversity system; a set of experiments on a test
bed measures the metrics of interest for the system; a game
theoretic analysis studies the impact of preferential selection
of platforms and derives an optimal strategy; ﬁnally, a set
of simulations evaluates the metrics of interest on the mod-
els. Our results from these approaches all agree and yet are
counter-intuitive. We show that although platform diversity
can mitigate some attacks, it can be detrimental for others.
We also illustrate that the beneﬁt from these systems heav-
ily depends on their threat model and that the preferential
selection of platforms can achieve better protection.

Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous;
D.2.8 [Software Engineering]: Metrics—complexity mea-
sures, performance measures

General Terms
Security, Measurement

Keywords
Operating systems security, dependable and fault-tolerant
systems and networks, metrics, evaluation, experimentation

∗This work is sponsored by the Department of Defense under
Air Force Contract FA8721-05-C-0002. Opinions, interpre-
tations, conclusions and recommendations are those of the
author and are not necessarily endorsed by the United States
Government.

INTRODUCTION

1.
Developing secure systems is diﬃcult and costly. The high
cost of eﬀectively mitigating all vulnerabilities and the far
lesser cost of exploiting a single one creates an environ-
ment which advantages cyber attackers. New cyber defense
paradigms have been proposed to re-balance the landscape
and create uncertainty for the attackers [18]. One such
paradigm is active defense based on temporal platform di-
versity.

Temporal platform diversity (or simply platform diversity)
techniques dynamically change the properties of a comput-
ing platform in order to complicate attacks. Platform prop-
erties refer to hardware and operating system (OS) attributes
such as instruction set architecture (ISA), stack direction,
calling convention, kernel version, OS distribution, and ma-
chine instance. Various platform diversity techniques have
been proposed in the literature. Emulation-based techniques
change the calling sequence and instruction set presented
to an application [31]; multivariant execution techniques
change properties such as stack direction or machine descrip-
tion using compiler generated diversity and virtualization
[27, 26, 13, 9]; migration-based techniques change the hard-
ware and operating system of an application using contain-
ers and compiler-based checkpointing [19]; server diversiﬁ-
cation techniques rotate a server across multiple platforms
and software stacks using network proxies [24]; self cleans-
ing techniques change the machine instance by continuously
rotating across many virtual machines and re-imaging the
inactive ones [3, 10, 2]. The key point to these techniques
is that platform diversity does not remove vulnerabilities, it
just makes them more complicated to exploit.

Unfortunately little work has been done on understanding
and quantifying the impact of temporal platform diversity
on the security of a system. The following important ques-
tions often remained unanswered: how often should the plat-
form change? How diverse are various platforms? What is
the quantitative impact of platform diversity on attacks?
What are the important parameters and factors for the ef-
fectiveness of a platform diversity system? Should the next
platform or platform conﬁguration be selected uniformly or
preferentially? What is the optimal strategy for controlling
a platform diversity system?

In this work, we take various approaches to quantitatively
analyze and study active cyber defenses based on platform
diversity. We use a migration-based technique (i.e. Talent

 
 
 
 
 
 
[19]) in our study and quantitatively measure its proper-
ties using four approaches. First, we perform an abstract
analysis of platform diversity using algebraic models, com-
binatorics, and Markov models. Second, we implement the
platform diversity system on a testbed and perform experi-
ments with it using real exploits to quantitatively measure
its properties. Third, we analyze the temporal migration
patterns using game theory and derive an optimal strategy
for selecting the next platform for ﬁve operating systems:
CentOS, FreeBSD, Debian, Fedora, and Gentoo. Fourth, we
model the platform diversity system running the above op-
erating systems with and without the optimal strategy and
simulate its impact on an attacker trying to compromise the
system.

The quantitative results from our four approaches are con-
sistent and in some cases surprising and counter-intuitive.
Our ﬁndings indicate that although platform diversity can
help protect a system against some attacks (speciﬁcally, per-
sistent service disruption attacks), it can negatively impact
security for others (speciﬁcally, fast, single compromise at-
tacks). We show that the beneﬁt of platform diversity tech-
niques heavily depends on their threat models and that op-
erational requirements and temporal parameters must be
understood and ﬁne tuned for these techniques to be ef-
fective. Moreover, we prove that preferential selection of
platforms can achieve better protection and derive the op-
timal strategy for determining temporal patterns (i.e. the
next platform).

The rest of the paper is organized as follows. Section 2 pro-
vides a brief overview of platform diversity techniques and
Talent. Section 3 describes the threat model used through-
out the paper. Section 4 discusses our abstract analysis
approach and its results. Section 5 discusses our testbed
experiments and measurements performed on a real system.
Section 6 provides the game theoretic analysis of temporal
patterns and the optimal solution. Section 7 explains our
simulation approach and results. Section 8 enumerates a
number of lessons learned and discusses our ﬁndings. We
discuss the related work in Section 9 before concluding the
paper in Section 10.

2. PLATFORM DIVERSITY BACKGROUND
In this section, we brieﬂy describe the active defense tech-
niques based on platform diversity. We provide enough back-
ground for understanding the rest of the paper. More details
about each technique can be found in its original publica-
tion.

Temporal platform diversity techniques change platform prop-
erties in order to make attacks more complicated. They of-
ten rely on temporal changes (e.g. VM rotation), diversity
(e.g. multivariant execution), or both (e.g. migration-based
techniques) to protect a system. These techniques are often
implemented using machine-level or operating system-level
virtualization, compiler-based code diversiﬁcation, emula-
tion layers, checkpoint/restore techniques, or a combination
thereof. Emulation-based techniques such as Genesis [31] of-
ten use an application-level virtual machines such as Strata
[29] or Valgrind [17] to implement instruction set diversity.
In some cases, multiple instances are executed and a monitor
compares their results. Multivariant execution techniques

such as Reverse stack [25] (also called N-variant systems
[7]) use compiler-based techniques to create diverse applica-
tion code by replacing sets of instructions with semantically
equivalent ones. Migration-based techniques such as Talent
[19] use operating system-level virtualization (containers) to
move an application across diverse architectures and oper-
ating systems. Platform diversity can also be achieved at a
higher abstraction level by switching between diﬀerent im-
plementations of servers [24]. These techniques either do
not preserve the state (e.g. a web server) or they preserve
it using high level conﬁguration ﬁles (e.g. DNS server). Fi-
nally, self-cleansing techniques such as SCIT [3] only change
the current instance of the platform without diversifying it.
The main goal, in this case, is bringing the platform to its
pristine state and removing persistence of attacks.

In this work, we use a migration-based platform diversity
technique, Talent, in our analyses. Since Talent provides
both temporal changes (periodic migrations) and diversity
(multiple operating systems), it is a good candidate for cap-
turing the major eﬀects and dynamics in our quantitative
analysis.

2.1 Talent
Trusted dynamic logical heterogeneity system (Talent) [19]
is a framework for live-migrating critical applications across
heterogeneous platforms. Talent has several design goals:
i) heterogeneity at the instruction set architecture level, ii)
heterogeneity at the operating system level, iii) preserva-
tion of the state of the application, including the execution
state, open ﬁles and sockets, and iv) working with a general-
purpose, system language such as C. Talent uses two key
concepts, operating-system-level virtualization and portable
checkpoint compilation, to address the challenges involved
in using heterogeneous platforms, including binary incom-
patibility and the loss of state and environment.

2.1.1 Environment Migration
The environment of an application refers to the ﬁlesystem,
conﬁguration ﬁles, open ﬁles, network connections, and open
devices. Many of the environment parameters can be pre-
served using virtual machine migration. However, virtual
machine migration can only be accomplished using a homo-
geneous operating system and hardware.

Talent uses operating-system-level virtualization to sandbox
an application and migrate the environment. In operating-
system-level virtualization, the kernel allows for multiple iso-
lated user-level instances. Each instance is called a container
(jail or virtual environment). This type of virtualization can
be thought of as an extended chroot in which all resources
(devices, ﬁlesystem, memory, sockets, etc.) are virtualized.
Hence, the semantic information that is often lost in hard-
ware virtualization is readily available in operating-system-
level virtualization.

Talent is implemented using the OpenVZ [14]. When migra-
tion is requested, Talent migrates the container of the appli-
cation from the source machine to the destination machine.
This is done by synchronizing the ﬁlesystem of the contain-
ers, migrating the IP address of the source container’s vir-
tual network interface, and transferring the state of each
TCP socket (sk_buff structure of the kernel). IPC and sig-

nal migration is supported in Talent through the underlying
virtualization layer.

2.1.2 Process Migration
Migrating the environment is only one step in backing up
the system because the state of running programs must also
be migrated. To do this, a method to checkpoint running
applications must be implemented.

Talent uses a portable checkpoint compiler (CPPC [22]) to
migrate a process state. CPPC allows Talent to checkpoint
the state of the binary on one architecture and recover the
state on a diﬀerent architecture. To perform checkpointing,
the code is precompiled to ﬁnd restart-relevant variables.
These variables and their memory locations are then reg-
istered in the checkpointing tool. When checkpointing, the
process is paused and the values of the memory locations are
dumped into a ﬁle. The checkpointing operation must occur
at safe points in the code to generate a consistent view. At
restart, the memory of the destination process is populated
with the desired variable values from the checkpoint ﬁle.
Some portions of the code are re-executed in order to con-
struct the entire state. The checkpoint ﬁle itself must have
a portable format to achieve portability across 32-bit/64-bit
architectures and little/big endian machines. Talent uses
the HDF5 portable ﬁle format [1] for its checkpoints.

Talent has been implemented on Intel Xeon 32-bit, Intel
Core 2 Quad 64-bit, and AMD Opteron 64-bit processors. It
has also been tested with Gentoo, Fedora (9, 10, 11, 12, and
17), CentOS (4, 5, and 6.3), Debian (4, 5, and 6), Ubuntu
(8 and 9), SUSE (10 and 11), and FreeBSD 9 operating
systems.

3. THREAT MODEL
We discuss multiple threat models in this paper but anal-
ysis shows that they share common features. To make the
analysis more precise, we explicitly describe the core threat
model in this section. Variations upon the core threat model
are described in the other sections as appropriate.

In our model, the defender has a number of diﬀerent plat-
forms to run a critical application. The attacker has a set of
exploits (attacks) that are applicable against some of these
platforms, but not the others. We call the platforms for
which the attacker has an exploit “vulnerable” and the oth-
ers “invulnerable.” In a strict systems security terminol-
ogy, vulnerable does not imply exploitable; without loss of
generality, we only consider exploitable vulnerabilities. An
alternative interpretation of this threat model is that the
vulnerabilities are exploitable on some platforms, but not
on the other ones.

The defender does not know which platforms are vulnera-
ble and which are invulnerable, nor does he have detection
capabilities for the deployed exploits. This scenario, for ex-
ample, describes the use of zero-day exploits by attackers,
for which no detection mechanism exists by deﬁnition.

Since there is little attempt to isolate the inactive platforms
in platform diversity systems, we assume that all platforms
are accessible by the attacker, and the attacker attempts to
exploit each one. Moreover, we assume that the defender

does not have a recovery or re-imaging capability to restore
a compromised platform. The reason for this assumption is
two fold: ﬁrst, typical recovery methods (e.g. clean installa-
tion of the operating system) can take a long time to com-
plete and unless the defender has many spare platforms, it
is hard to accomplish it eﬀectively. A large number of spare
platforms also implies large hardware cost for these systems
(e.g. SCIT [3]). Second, and more importantly, the goal
of this paper is to study the impact of temporal platform
changes, not the eﬀectiveness of recovery capabilities being
used. Hence, once exploited, a platform remains vulnerable
and the attacker requires no staging time in the future.

The attacker’s goal is what creates the variations in our
threat model. For example, one success criteria may be for
the adversary to compromise the system for a given period
of time to cause irreversible damage (e.g. crash a satellite),
while a diﬀerent success criteria gives the attacker gradual
gain the longer the system is compromised (e.g. exﬁltration
of information). These variations and their impact on the
eﬀectiveness of temporal platform diversity are discussed in
the subsequent sections.

4. ABSTRACT ANALYSIS
Much of the analysis of one system using temporal platform
diversity as a security strategy applies to any such system.
In this section we will specify an abstraction that generically
describes temporal platform diversity, which is useful as a
security strategy in protecting against attacks that have a
temporal requirement. We analyze a number of cases where
this holds. To better convey the meanings, we use simple
phrases which are examples of these cases to refer to them:
crash the satellite, sneaking past a sensor, and exﬁltration
over a diﬃcult channel.

1. Crash the Satellite - once the attacker has controlled
the system continuously for a speciﬁed time period,
the game is over and the attacker has won.

2. Sneaking past a sensor - the attacker needs control for
a speciﬁed time period for an attack to succeed. Once
the attacker has control for the period, the attacker’s
payoﬀ function starts to increase from zero. The sce-
nario can either be continuous or ﬁnite duration.

3. Exﬁltration over a diﬃcult channel - the attacker needs
control for a speciﬁed time period for an attack to suc-
ceed. Once the attacker has control for the period,
the attackers payoﬀ function starts to increase from a
positive value.

We can categorize the problem space according to a number
of properties:

• the attackers temporal requirement can either be ag-

gregate or continuous,

• the attackers payoﬀ can be either fractional or binary

(all or nothing),

• the period of usage (equivalently attack window) can

either be of ﬁxed duration or ongoing, and

m
n
sk
v (cid:0)sk(cid:1)
¬v (cid:0)sk(cid:1)
P (cid:0)v (cid:0)sk(cid:1)(cid:1)
Pvv
Pii

number of vulnerable states
number of invulnerable states
state at migration step k
state at migration step k is vulnerable
state at migration step k is not vulnerable
probability that v (cid:0)sk(cid:1)
P (cid:0)v (cid:0)sk+1(cid:1) |v (cid:0)sk(cid:1)(cid:1)
P (cid:0)¬v (cid:0)sk+1(cid:1) |¬v (cid:0)sk(cid:1)(cid:1)

Table 1: Notation describing platform diversity system

• the migration models include random with repeat, ran-

dom without repeat, and periodic permutation

We will deﬁne the abstract model of a temporal platform
diversity system S as a system that migrates through a ﬁnite
ﬁxed collection of states {si}. Each state either has or does
not have a property exploitable by the attacker which we
call vulnerable. In the ﬁrst approximation to the model we
assume that the states are fully independent. We will use
the notation presented in Table 1.

4.1 Attacker Aggregate Control
When the attacker requires only aggregate control, there are
two main subcategories according to the attacker’s payoﬀ.
The fractional case is trivially determined by the ratio of
m and n. In the binary case, wherein the attacker wins by
controlling a speciﬁed fraction of the vulnerable time, the
defender may optimize via an initial subselection of states
in a process reminiscent of gerrymandering. For example, if
m = 3 and n = 2 and the attacker wants to control greater
than 50% of the time, then the defender should simply ex-
pect to lose should all platforms be utilized. By contrast if
the defender randomly subselects two then the defender can
reduce the attacker’s expectation of winning to

C (3, 2)
C (5, 2)

=

3
10

= 30%,

of platforms is used. Our later experiment will use 3 vul-
nerable and 2 invulnerable platforms which is a suﬃciently
small number to have a strong inﬂuence upon the conditional
probabilities.

This reduces to the Markov chain

V

Pvv

Pv

start

1−Pii

1−Pvv

P k=1−Pv

I

Pii

which can be used to conﬁrm that the steady state

4.3 Attacker Fractional Payoff Model
The steady state of attacker control of the system can be
modeled using Markov chains with states I and V refer-
ring to invulnerable and vulnerable respectively. While the
simple Markov model describing the transitions {I, V } −→
{I, V } describes the base behavior of the system, it does not
naturally capture the notion of repeated vulnerable states.
We can adapt this chain to one with a richer collection of
states
(cid:8)I, IV, IV 2, . . . , IV n−1, V n(cid:9) −→ (cid:8)I, IV, IV 2, . . . , IV n−1, V n(cid:9)

which support runs of length n. The probability of invul-
nerable to invulnerable transition is given by

(cid:16)

Pii = P

¬v (si+1) | ¬v

sk(cid:17)(cid:17)
(cid:16)

=

n − 1
m + n − 1

and the probability of vulnerable to vulnerable transition is
given by

Pvv = P

(cid:16)

v (si+1) | v

(cid:16)

sk(cid:17)(cid:17)

=

m − 1
m + n − 1

. The Markov model looks like

y!(x−y)! is the combinatorial choice func-
where C (x, y) =
tion. Here the value of 2 as the number of platforms chosen.

x!

1−P¬v

IV

Pvv

(cid:47) IV 2

Generally, if p is the percentage of time that the attacker
requires for success and we subselect j platforms from the
total m + n, then the probability of attacker success is

Psuccess =

min(m, j)
(cid:88)

i=(cid:100)p·j(cid:101)

C (m, i) · C (n, j − i)
C (m + n, j)

,

in the steady-state model.

4.2 Attacker Continuous Control
When the attacker requires continuous control, the defender
can use the subselection strategy as above as well as leverag-
ing conditional probabilities. These conditional probabilities
are given in Table 2.

m+n > m−j
Here, we observe that m
m+n−j so long as n and j are
both greater than zero. As such, migrating without imme-
diate repeat, while not inﬂuencing the fraction of vulnera-
ble platforms selected, tends to reduce successful sequences.
We note that the inﬂuence is greater when a smaller number

1−Pvv

1−Pvv

1−Pv

I

Pii

1−Pvv

IV n−1

Pvv

V n

Pvv

which has the (n + 1) × (n + 1) Markov transition matrix is
given by

1 − Pvv

Pvv

Pvv

Pvv














Pii
1 − vvv
1 − Pvv
1 − Pvv

1 − Pvv
1 − Pvv
1 − Pvv














.

. . .

Pvv
Pvv

(cid:102)
(cid:102)
(cid:6)
(cid:6)
(cid:55)
(cid:55)
(cid:39)
(cid:39)
(cid:100)
(cid:100)
(cid:70)
(cid:70)
(cid:113)
(cid:113)
(cid:47)
(cid:35)
(cid:35)
(cid:50)
(cid:50)
(cid:77)
(cid:77)
(cid:111)
(cid:111)
(cid:123)
(cid:123)
(cid:77)
(cid:77)
Repeat Vuln ¬Vuln P (v (Si+1)) P (cid:0)v (si+1) | v (cid:0)sk(cid:1)(cid:1) P (cid:0)v (si+j+1) | v (si+j) & . . . &v (cid:0)sk(cid:1)(cid:1)
Without m
m

With

n
n

m−1
m+n−1
m
m+n

m−j
m+n−j
m
m+n

m
m+n
m
m+n

Table 2: Conditional Probabilities

This transition matrix has the steady state eigen-vector

av · Pvv

av · P 2

vv · · ·

av · P n−1

vv

av · (cid:80)∞

i=n P i
vv

(cid:105)

(cid:104)

n
m+n

where

av =

m
m + n

(cid:18) 1 − Pvv
Pvv

·

(cid:19)

.

Figure 1: Windows Of Opportunity

i=n P i

v = m

m+n − av · (cid:80)n−1

This can be used to compute the steady state behavior of
the system. If the attacker success begins after n steps then
the steady state is given by the right most term in the eigen
vector av · (cid:80)∞
v. If the attacker
success includes the steps leading to a run of n steps then we
must also include vulnerable states weighted by the proba-
bility that they will become a run of n vulnerable states and
the contribution to the run: the probability that IV n−1 will
become V nis PV , the probability that IV n−2 will become
V nis 2 · P 2
V and so forth. Reducing that equation, we ﬁnd
that the expected period of attack control L(n) is

i=0 P i

L(n) = 1 −

(1 − P¬v)−1 + (1 − Pv) (cid:80)n−1

i=0 i · P i−1

v

(1 − P¬v)−1 + (1 − Pv)−1

which is one minus the percentage of time that the defender
is in control.

4.4 Attacker Binary Payoff Model
In the binary payoﬀ model with random selection (with or
without immediate repeats), the attacker will eventually win
so long as it is combinatorially possible in the same manner
that a person ﬂipping a coin will eventually observe a se-
quence of ten, or ninety-two, heads in a row. Here metrics
might reasonably be based in the mean time until attacker
victory. These can be analyzed in a fashion similar to the
steady state model:

I {1···∞}

1−Pv

1−P n−1
vv

1−P n−1
vv

P n−1
vv

P∗

start

V {1···n−1}I {1···∞}

Pv P n−1
vv

P

(n−1)
vv

V n

1

end
(cid:0)1 − P n−1

(cid:1). We can use this to evaluate the
where P∗ = Pv
expected time L(cid:48)(n) to attack compromise as the probabilis-

vv

Figure 2: Probability

tically weighted sum of all path lengths

L(cid:48)(n) =n +

1 − Pv
+
1 − Pii
vv − 1(cid:1) ·
1 − n · P n−1
(cid:0)1 − P n−1

(cid:0)P 1−n
(cid:32)

vv + (n − 1) · P n
vv
(cid:1) · (1 − Pvv)

vv

(1)

.
(cid:33)

+

1
1 − Pii

The full derivation can be found in Appendix A. Hence, in
scenarios such as ‘crash the satellite’, Eq. (1) computes the
expected time before the adversary is able to take down the
service.

4.5 Finite Duration
While the eﬀect of a ﬁnite duration scenario with random
attacker starting time does not strictly concern temporal
platform diversity systems, the eﬀect needs to be explained
in order to interpret the experimental results. Let d be the
duration of the trial, a be the period that the attacker must
be present.

Then the probability that the attack succeeds is given by

Psuccess = min

1, max

0,

(cid:18)

(cid:18)

(cid:19)(cid:19)

d − a
s

As a function of a, the variable region is a line of slope
− a
s . This is approximated by the graph with one platform
where there the only deterministic inﬂuence is this window
of opportunity (see Figure 4).

4.6 Fractional Effect
The ﬁnal deterministic eﬀect is a sort of fractional eﬀect
which occurs when the required attacker duration a passes

(cid:39)
(cid:39)
(cid:42)
(cid:42)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:79)
(cid:79)
(cid:118)
(cid:118)
(cid:6)
(cid:6)
(cid:15)
(cid:15)
between various multiples of the duration of each platform.
For example, if the duration of the attacker passes from
being a bit less than a single platform duration to a bit more,
then the random selection process means that instead of a
single vulnerable platform showing up, two need to show up
consecutively. The same thing happens as we transition from
two to three and so on. The result is a sort of downward step
in the graphs which is smoothed but the the initial attack
step (e.g. if there is one vulnerable state but the attacker
arrives in its middle, it might be an inadequate duration for
the attacker).

Speciﬁc examples of attacks for this form will be given in ...

1. continuous attacker control / aggregate attacker con-

trol

2. periodic permutational / random with repeat / ran-

dom without repeat

The platform migration model used in the experiment is
random without immediate repeat (e.g. for platforms are
{A, B, C} the the sequence A → B → A → C is permissible
while A → A → B → C is not).

The fractional eﬀect can be observed in our experiments in
Figure 4.

5. EXPERIMENTS
5.1 Experiment Setup
To perform the experiments, a notional application with C
back-end and GUI front-end has been ported to Talent. On
the test bed, we have a pool of ﬁve diﬀerent platforms: Fe-
dora on x86, Gentoo on x86, Debian on x86 64, FreeBSD on
x86, and CentOS on x86. The application runs for a random
amount of time on a platform before migrating to a diﬀerent
one (i.e. inter-migration delay).

The attacker’s goal is to control the active platform for some
time T . Since in a real scenario the vulnerability of the plat-
form is unknown, we may consecutively migrate to multiple
vulnerable platforms, in which case the attacker wins. To
implement this scenario on the test bed, we launch two real
exploits against Talent. The ﬁrst is the TCP MAXSEG ex-
ploit which triggers a divide by zero vulnerability in net/
ipv4/tcp.c (CVE-2010-4165) to perform a DoS attack on
the platform. Only the Gentoo platform is vulnerable to this
attack. The second attack is the Socket Pairs exploit which
triggers a garbage collection vulnerability in net/unix/
garbage.c (CVE-2010-4249) to saturates the CPU usage
and ﬁle descriptors. The Fedora and CentOS platforms are
vulnerable to this attack. Our Debian and FreeBSD plat-
forms are not vulnerable to these exploits.

In each conﬁguration, we select N ∈ (1, 5) platforms. For
each sample, the application randomly migrates across those
N platforms without immediate repeat. In the case of N = 1
(baseline), the application remains on the same platform
during the entire run. Without loss of generality, the inter-
migration delay (tmig) is chosen randomly and uniformly
from 20-30 seconds. Although we have no reason to believe

Figure 3: A 3-platform sample

that these are the appropriate values for a real-world ap-
plication, we will show later that the actual values of the
inter-migration delay (tmig) and attacker’s goal (T ) are in-
consequential to our experiments and can be parametrized.

One or both exploits become available to the attacker at
random times during each sample. As a result, zero to three
platforms can be breached (zero when the exploit is not ef-
fective against the set of platforms and three when both
exploits are available and Fedora, CentOS, and Gentoo are
in the pool of platforms). When the exploit is launched, its
payload reaches all of the platforms in the selected set at
once (not one after another). This approach tries to model
the behavior of network-based exploits that propagate to all
machines within a network very rapidly. Each sample runs
for 15 minutes, leading to 300 collected samples for each
conﬁguration. We also collect a central log which includes
a timestamp, the status of each platform (up or down), and
the active platform and a local log (for veriﬁcation purposes)
which also includes ﬁner-grained CPU load for each plat-
form.

Figure 3 illustrates one sample with 3 platforms. The red
arrows show when exploits are launched. In this case, plat-
forms 2 and 5 are vulnerable to the exploits.

5.2 Experiment Results
We calculate the value of the metric, which is the percentage
of time that the attacker is in control for longer than T and
present these results in Fig. 4. At the ﬁrst look, they seem
completely counter-intuitive. For small attacker goals (T ),
fewer platforms actually perform better. This is due to the
fact that in situations where the attacker wins quickly, more
platforms present a larger attack surface. As a result, the
attacker wins if she can compromise any of the platforms.
In other words,

T
timg

→ 1 : Attacker wins iﬀ any platform is vulnerable

For the baseline case (1 platform), there is no change in the
platform, so the primary eﬀect being observed is the ﬁnite
duration eﬀect discussed in Section 4.5. The one platform
line, in fact, approximates Fig. 2.

In addition, notice that the fractional eﬀect (i.e. downward
steps) discussed in Section 4.6 can also be observed for two
or more platforms.

The value of diversity can only be observed for attacker

P2 P5 P2 P1 P2 P5 P1 Exploit 1 Exploit 2 time and migrating to a platform that is most likely to break the
chain of adversary persistence. Intuitively, if the defender
knows an adversary has been able to exploit some unknown
vulnerability on the current platform, the proper course of
action is to migrate to the platform least similar.

Let us formalize this notion by recalling S = {s1, . . . , sm+n}
as the set of m + n available platforms and v(s) as a Boolean
function specifying whether or not platform s is vulnerable
to some available exploit.
In general, this function is un-
known. Without loss of generality, we discretize time such
that an adversary needs to be present for K = T /tmig in-
tervals, where tmig is the speciﬁed time between migrations.
For simplicity of analysis, we assume that the time required
to migrate platforms is negligible. This time is indeed unim-
portant for our analysis, however it would be if one were to
optimize tmig, which is an area for future work.

Let us ﬁrst study the case where K = 2, for which a ﬁrst-
order Markov chain accurately models the system, then move
to the general case. At each interval k, an optimal system
will migrate to the platform which solves the following:

sk = arg min

j

(cid:16)

P

v(sj)|v(sk−1)

(cid:17)

,

(2)

where sk ∈ S is the platform presented during interval k. In-
tuitively, this selects the platform statistically most diverse
from the current one. This similarity can be deﬁned in a
variety of methods, such as common lines of code or shared
modules. If there existed some mapping f : S → R2, Eq. (2)
would be solved by selecting the point that is furthest away.

Solving the general case for any given K, one should assume
each of the past K − 1 platforms have been vulnerable, and
therefore exploited. As discussed in Section 4, this becomes
a Markov chain of order K − 1, and the optimal platform at
each interval k is determined by solving

sk = arg min

j

(cid:16)

(cid:17)
v(sj)|v(sk−1), v(sk−2), . . . , v(sk−K+1)

.

P

(3)
This formulation selects the platform that is jointly most
dissimilar from the set of K − 1 prior platforms. Once again
assuming ∃f : S → X = [x1, . . . , xn], where X ∈ R2, Eq. (3)
selects the point which maximizes the area of the polygon
constructed by joining the set of previous platforms

xk = arg max

j

area(xj, xk−1, . . . , xk−K+1).

(4)

We note that Eq. (3) can be solved directly, without map-
ping to a Euclidean space, as there are several methods of
computing area between points given just the distances be-
tween them. For example, with K = 3, Heron’s formula can
be used to compute the area of a triangle.

6.1 Deterministic Strategy
It is important to note that Eq. (3) results in a periodic
scheduling strategy, which can be seen in Eq. (4). As each
sample point xi is deterministically deﬁned, there will exist
a set of K points that form the polygon with the largest
area. Regardless of which platform the system is initiated
with, the migration pattern will eventually devolve into a pe-
riodic rotation across these K platforms. We note that this
determinism is acceptable under the adversary model, as

Figure 4: The portion of time that the attacker is in control

goals that are large with respect to the inter-migration time
(T (cid:29) tmig). This is an important parameter when deploy-
ing temporal platform diversity systems; the inter-migration
time must be selected short enough based on the service re-
quirements of the system. For example, if the system has
to survive and provide service within 5 minutes (i.e. the at-
tacker goal is disrupting service longer than T = 5 minutes),
the inter-migration time must be tmig << 5 min. In other
words,

T
timg

→ N : Attacker wins iﬀ all platforms are vulnerable

6. OPTIMAL CONTROL
While understanding the optimal number of platforms will
yield improved performance, the question of which platforms
to deploy is of critical importance. Recall that in Section 3
we describe a threat model in which the defender has no
knowledge of the vulnerability states of the available plat-
forms. However, even if there exists some vague sense of
‘platform A is more secure than platform B’, it is not always
the case that one should deploy platform A. The requirement
for adversary persistence results in the system security being
achieved through platform diversity.

As an illustration, suppose we have at our disposal three dif-
ferent operating systems – FreeBSD, Fedora, and CentOS –
and we are able to deploy any two of these in a platform mi-
gration system. It would be far better to deploy FreeBSD in
combination with either Fedora or CentOS than leveraging
the combination of Fedora and CentOS. This is because the
latter two OSes are distributions of RedHat Linux and share
much of the same kernel code and device drivers. As such,
a vulnerability on Fedora is much more likely to exist on
CentOS (and vice versa) than on FreeBSD, which is Unix-
based but not a Linux distribution. Hence, a single exploit
is unlikely to work on both FreeBSD and Fedora/CentOS;
this increases the cost for an adversary to compromise the
system.

Note that the above illustrative example makes no mention
of the security level of the individual platforms. Recall that
the goal of the defender is not to globally minimize system
vulnerability, but to prevent persistent vulnerability for a
duration of length T . This is best accomplished by, at each
migration time, assuming the current platform is vulnerable

0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0 50 100 150 200 250 300 Portion of Time the Adversary Succeeds Adversary's Goal (time to disrupt service in seconds) 1 Platform 2 Platforms 3 Platforms 4 Platforms 5 Platforms Simulation Testbed Measurment CentOS Fedora Debian Gentoo FreeBSD
1.0
0.6645
0.8067
0.6973
FreeBSD 0.0368

0.6973
0.8658
0.6202
1.0
0.0330

0.0368
0.0324
0.0385
0.0330
1.0

0.8067
0.5928
1.0
0.6202
0.0385

0.6645
1.0
0.5928
0.8658
0.0324

CentOS
Fedora
Debian
Gentoo

Table 3: Code Similarity Scores, S(i, j), including device
drivers

the adversary launches exploits against all platforms simul-
taneously. Hence, randomization oﬀers no beneﬁt to system
security in this case.

This relation is important to note, as the typical thought
process has been that platform diversity is achieved through
randomization. In fact, the opposite is true; randomization
and diversity are two diﬀerent criterion that are often or-
thogonal to one another. A strategy that optimizes for ran-
domization would uniformly select from the available plat-
forms, and would almost surely – as k → ∞ – select con-
secutive platforms that are highly similar and vulnerable to
the same exploits. The logic behind this fact was discussed
in Section 4.4. Contrarily, a strategy that optimizes for di-
versity returns a deterministic schedule which minimizes the
probability of consecutive platforms with similar vulnerabil-
ities, according to some measure of similarity.

7. SIMULATION
We now demonstrate the optimality of our presented con-
trol strategies through a Monte Carlo simulation, leveraging
the pool of ﬁve platforms presented in Section 5 . We use
the Measures of Software Similarity (MOSS)1 [28] tool to
compute a similarity score between each pair of operating
systems. The results are presented in Table 3, where each
similarity is on a scale of (0-1) where 1.0 implies identical
code and 0.0 implies entirely distinct code. The input for
each operating system was the kernel code and a set of stan-
dard device drivers. As discussed earlier, one should notice
that FreeBSD is highly dissimilar to the 4 Linux distribu-
tions presented.

During this simulation, we set K = 3, such that the ad-
versary must be present for 3 consecutive time intervals.
This could be viewed, for example, as T = 90 seconds with
tmig = 30 seconds. Rather than require an extensive set of
vulnerability exploits, we deﬁne our adversary by randomly
selecting one of the ﬁve available platforms as vulnerable,
v(s(cid:48)). Next, we determine the vulnerability status of each
other platform i by performing a Bernoulli trial with a prob-
ability of success equal to the similarity score between s(cid:48) and
si. The intuition behind this setup is that the greater the
similarity between platform code, the more likely they share
a vulnerability. While this is not a direct mapping, the in-
tuition enables a robust analysis, and diﬀerent measures of
similarity could be used.

Figure 5: Fraction of time presenting a vulnerable system

Diversity Uniform Random 3
0.541

0.493

0.539

Table 4: Mean vulnerability rate (1-AUC)

selecting which platform to migrate to:

• Diversity: Optimizing diversity through Eq. (3)

• Uniform: Uniform random selection at each interval
from the fully available set without immediate repeat

• Random 3 : Uniform random selection of K = 3 plat-
forms prior to the trial, then periodic rotation between
them

In order to optimize diversity, we deﬁne Eq. (3) with Heron’s
formula to compute the area of a triangle, deﬁning the dis-
tance between points as d(i, j) = 1 − S(i, j), where S(i, j) is
the similarity score between si and sj given in Table 3.

In Fig. 5 we plot the cumulative distribution function (CDF)
of the fraction of time the selected platform was vulnerable,
computed across all 500 MC trials.
In this plot, a curve
trending towards the upper-right corner is preferred, as it
demonstrates that a vulnerable system was presented less
often. We see that optimizing diversity generally selects less
vulnerable systems than either of the other methods; the
mean rate of vulnerability for the three methods is shown in
Table 4. While diversity does show statistically signiﬁcant
improvement over other strategies, the conclusion from this
evaluation would be that none of the strategies are particu-
larly viable. Indeed, in Section 4 we determined that K was
the optimal number of platforms to use, yet its performance
is nearly identical to leveraging more. As discussed earlier,
however, this is a ﬂawed metric that doesn’t represent this
attacker model.

During each of 500 Monte Carlo (MC) trials lasting 100
time intervals each, we select the vulnerable platforms in
the above manner and evaluate three diﬀerent methods of

1http://theory.stanford.edu/∼aiken/moss/

We now demonstrate the CDFs of the two metrics which
are appropriate for evaluating the given threat models in
Fig. 6. First, we study the case for which a single com-
promise is considered success for the adversary and failure
for the defender (e.g. ‘Crash the Satellite’). In Fig. 6a, we

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91Fraction of TimeValue of CDF  DiversityUniformRandom 3(a) Time to ﬁrst compromise

(b) Fraction of time compromised

Figure 6: Evaluation metrics on diﬀerent migration strategies

plot the CDF for the time it takes before the system is fully
compromised, that is, K vulnerable platforms are presented
consecutively. Noting that these results are from the same
trials as Fig. 5, we see a signiﬁcant diﬀerence in performance
with the appropriate metric. Optimizing for diversity results
in system compromise only 2% of the time, while uniform
selection eventually results in system compromise in 79% of
the trials. We note that because the diversity optimization
strategy results in periodic scheduling, if system compro-
mise is going to happen, it does so nearly instantaneously
– either at k = K or shortly thereafter (once the determin-
istic solution is reached). This is due to the fact that the
system can only be compromised if all of the platforms in
the resultant solution are vulnerable; nothing changes after
the solution is reached. The same can be said about the
Random 3 strategy, but we note that performance is signif-
icantly reduced due to the increased probability of drawing
3 platforms that are highly similar. Contrarily, the Uniform
strategy will almost surely result in system compromise as
k → ∞, if there exists K vulnerable platforms in the fully
available set.

Finally, we study the threat model for which an adversary
achieves progressive gains the longer they are present on
the system (e.g. ‘Exﬁltration over a Diﬃcult Channel’ and
’Sneak Past a Sensor’). Recall that in these scenarios, the
defender loss begins – or continues – after the adversary is
present for K consecutive intervals. As such, an appropriate
evaluation metric is the fraction of time that the system is
in a compromised state, which we plot the CDF for in Fig.
6b. Once again, optimizing diversity yields far superior per-
formance to other methods. In 98% of the trials, the system
was never compromised (note this matches Fig. 6a), while in
that other 2% it was compromised for the entire duration.
Meanwhile, uniform migration results in system compromise
on average 26% of the time, and only in less than 2% of the
trials did it result in less compromise than diversity opti-
mization (13% of trials when comparing to Random 3).

is important to take note of and emphasizes the need to
develop a deployment strategy that is optimal towards the
speciﬁc threat model of interest. Given the threat model
requiring adversary persistence, deterministically optimiz-
ing for diversity shows superior performance to attempting
to achieve security through randomization. We note that
while the eﬀects discovered in these simulations match the
theoretical results presented in Section 4 as well as in the
experimental results in Section 5.2.

8. LESSONS LEARNED AND DISCUSSIONS
Our work in analyzing and testing temporal moving target
technologies has provided three main lessons.

The ﬁrst is that the methodology of producing threat de-
rived metrics, as in [23], can be extended to evaluate base
technologies. In the cases, technologies are designed to ad-
dress a speciﬁc problem or set of problem. As such security
technologies implicitly carry the set of threats which they
are intended to address. These threats can be used to de-
rive metrics.

While the threats addressed by temporal moving target tech-
nologies vary, there remains a certain similarity in the pro-
duced metrics and, perhaps more importantly, in the data
needed to evaluate them. This allows the transformation of
data gathered from one set of, potentially expensive or time
consuming, tests in a speciﬁc usage scenario to diﬀerent us-
age scenarios.

The second is the surprising result that security, as measured
using the threat derived metrics, is not a strictly increasing
Indeed, the minimal diversity
as a function of diversity.
required to cover the attackers required duration provides
the optimal solution in the continuous requirement case. In
the aggregate case, the optimal diversity is determined via a
process similar to gerrymandering: that is that the attacker
just barely loses in the majority of sub-selections.

One can see that when evaluating with the appropriate met-
rics, the diﬀerence in performance becomes signiﬁcant. This

The ﬁnal lesson is that when we have information concerning
diversity between individual platforms, the optimal result is

010203040506070809010000.10.20.30.40.50.60.70.80.91Time (k)Value of CDF  DiversityUniformRandom 300.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91Fraction of TimeValue of CDF  DiversityUniformRandom 3view these analyses as complimentary, where [6] applies to
the attacker/defender evolution, while the work we present
here applies to the steady state.

10. CONCLUSION
In this paper, we have quantitatively studied cyber defenses
based on temporal platform diversity. Although the exper-
iments were focused on a migration-based platform diver-
sity system, much of the analyses apply to any such system.
Our abstract analysis has studied the major properties in a
platform diversity system including the temporal eﬀects, the
payoﬀ models, and the window of opportunity using combi-
natorics and Markov models. Additionally, the impact of
ﬁnite duration and fractional eﬀects has been studied. The
experiments have collected data from a testbed implementa-
tion of such a system in which all of the abstract eﬀects can
be observed. The game theoretic analysis has studied the
impact of preferential platform selection on the eﬀectiveness
of such systems and derived an optimal operational strategy.
Finally, by simulating the model of the system, we evaluate
the impact of diﬀerent strategies on the eﬀectiveness of tem-
poral platform diversity.

Our results suggest that while platform diversity is useful
for mitigating some attacks, it is of critical importance to
understand the threat model one aims to defend against.
Designing and deploying defensive techniques that have not
considered the threat model may result in the illusion of se-
curity, when one may actually be increasing their attack sur-
face. Moreover, coupling a platform diversity system with
a control mechanism that uses optimal strategies can sig-
niﬁcantly improve its eﬀectiveness. No one technology will
defend against all attacks, so understanding the proper op-
eration and strengths of a platform diversity system enables
network defenders ensure that other techniques are deployed
for areas of weakness.

The future work in this domain will focus on performing
more experiments with such systems, extending the analysis
to other platform diversity techniques and other random-
ization and diversity approaches, and analyzing the second
order behavior such as adaptive adversaries who change tac-
tics based on the deployed defenses.

11. ACKNOWLEDGEMENTS
Special thanks to Mark Rabe of MIT Lincoln Laboratory for
his help in setting up the experimental test bed.

provided by maximizing platform dissimilarity while mini-
mizing the diversity in the sense of platform count. Indeed,
even given knowledge of the probability of vulnerability in
the various platforms, the most secure ensemble might not
be comprised of the most secure individual members.

9. RELATED WORK
Various platform diversity techniques have been proposed in
the literature. As mentioned earlier, The Self-Cleansing In-
trusion Tolerance (SCIT) project rotates virtual machines to
reduce the exposure time. SCIT-web server [4] and SCIT-
DNS [32] preserve the session information and DNS mas-
ter ﬁle and keys, respectively, but not the internal state of
the application. The Resilient Web Service (RWS) Project
[11] uses a virtualization-based web server system that de-
tects intrusions and periodically restores them to a pristine
state. Certain forms of server rotation have been proposed
by Blackmon and Nguyen [5] and by Rabbat, et al. [21] in
an attempt to achieve high availability servers.

High-level forms of temporal platform changes have been
proposed by Petkac and Badger [20] and Min and Choic [16]
to build intrusion tolerant systems although the diversiﬁca-
tion strategy is not as detailed in these eﬀorts.

Compiler-based multivariant [12, 25, 27, 26, 13] and N-
variant systems [7] propose another way of achieving plat-
form diversity.

Holland, et al propose diversifying machine descriptions us-
ing a virtualization layer [9]. A similar approach with more
speciﬁc diversiﬁcation strategy based on instruction sets and
calling sequences has been proposed by Williams et al [31].

Wong and Lee [30] use randomization in the processor to
combat side-channel attacks on caches.

On the evaluation side, Manadhata and Wind [15] propose
a formal model for measuring a system’s attack surface that
can be used to compare diﬀerent platforms. Evans et al
[8] develop models to measure the eﬀectiveness of diversity-
based moving target technique. They evaluate the proba-
bility of attack success given the time duration of attack
probing, construction, and launch cycles and the entropy of
randomness in the target system. They evaluate the impact
of various attacks on moving target systems including cir-
cumvention, deputy, brute force, entropy reduction, probing,
and incremental attacks.

Colbaugh and Glass [6] use game theory to analyze strategies
for deploying moving target defenses against adaptive adver-
saries, concluding with the result that uniform randomiza-
tion is optimal. While these results may seem counter to
our own, they are not. Their threat model did not require
adversary persistence and the defense strategy was designed
to minimize predictability. Speciﬁcally, the more an adver-
sary is able to observe a defense, they are more likely to
adapt to it.
In the case of temporal platform migration,
this implies that the adversary will expend resources to-
wards the development of exploits speciﬁc to the systems
observed. While their adversary starts with no capabilities
and develops them over time, we instead assume the adver-
sary already has access to a full suite of exploits. One can

APPENDIX
A. BINARY PAYOFF DERIVATION
Let Lv (n) be the expected length of a sequence beginning
with a vulnerable state that requires n consecutive vulner-
able states to fail. Let LI (n) be the equivalent construct
for sequences beginning with an invulnerable state. Then
the mean time until attacker victory is L(n) = Pv · Lv (n) +
(1 − Pv) · Li (n). Due to the Markov property we know that
Li(n) = E
+ Lv(n). Also due to the Markov
property,

I {1...∞}(cid:17)
(cid:16)

Lv(n) =P n−1

vv

· E (V n) +
(cid:1) ·

(cid:16)

E

(cid:0)1 − P n−1

vv

(cid:16)

V {1...n−1}I {1...∞}(cid:17)

(cid:17)

+ Lv (n)

We can compute the expected lengths of the sequence I {1...∞}
as

(cid:16)

I {1...∞}(cid:17)

E

=

and of V {1...n−1} as E

(cid:80)∞

ii

j=1 j · (1 − Pii) · P j−1
(cid:80)∞
j=1 (1 − Pii) · P j−1
V {1...n−1}(cid:17)
(cid:16)

=

ii

=

1
1 − Pii

(cid:80)n−1

i=1 i · (1 − Pvv) · P i−1
(cid:80)n−1
i=1 (1 − Pvv) · P i−1

vv

vv

=

1 − n · P n−1
(cid:0)1 − P n−1

vv + (n − 1) · P n
vv
(cid:1) · (1 − Pvv)

vv

which combines to give the expected time until attacker com-
promise

L(n) =n +

1 − Pv
1 − Pii

+

(cid:0)P 1−n

vv − 1(cid:1)





1 − n · P n−1
1 − P n−1

(cid:16)

vv

(cid:17)

vv + (n − 1) · P n
vv

· (1 − Pvv)





+

1
1 − Pii

B. REFERENCES
[1] HDF4 Reference Manual. The HDF Group, February
2010. ftp://ftp.hdfgroup.org/HDF/Documentation/
HDF4.2.5/HDF425 RefMan.pdf.

[2] D. Arsenault, A. Sood, and Y. Huang. Secure, resilient
computing clusters: Self-cleansing intrusion tolerance
with hardware enforced security (scit/hes). In
Proceedings of the The Second International
Conference on Availability, Reliability and Security,
ARES ’07, pages 343–350, Washington, DC, USA,
2007. IEEE Computer Society.

[3] A. Bangalore and A. Sood. Securing web servers using

self cleansing intrusion tolerance (scit). In
Dependability, 2009. DEPEND ’09. Second
International Conference on, pages 60 –65, june 2009.
[4] A. K. Bangalore and A. K. Sood. Securing web servers

using self cleansing intrusion tolerance (scit). In
Proceedings of the 2009 Second International
Conference on Dependability, pages 60–65, 2009.
[5] S. Blackmon and J. Nguyen. High-availability ﬁle

server with heartbeat. System Admin, The Journal for
UNIX and Linux Systems Administration, 10(9), 2001.

[6] R. Colbaugh and K. Glass. Predictability-oriented

defense against adaptive adversaries. In Proceedings of
the IEEE Intl. Conference on Systems, Man, and
Cybernetics, COEX, pages 2721–2727, 2012.

[7] B. Cox, D. Evans, A. Filipi, J. Rowanhill, W. Hu,
J. Davidson, J. Knight, A. Nguyen-Tuong, and
J. Hiser. N-variant systems: a secretless framework for
security through diversity. In Proceedings of the 15th
conference on USENIX Security Symposium - Volume
15, USENIX-SS’06, Berkeley, CA, USA, 2006.
USENIX Association.

[8] D. Evans, A. Nguyen-Tuong, and J. C. Knight.

Eﬀectiveness of moving target defenses. In Moving
Target Defense, pages 29–48. 2011.

[9] D. A. Holland, A. T. Lim, and M. I. Seltzer. An

architecture a day keeps the hacker away. SIGARCH
Comput. Archit. News, 33(1):34–41, Mar. 2005.
[10] Y. Huang, D. Arsenault, and A. Sood. Incorruptible
system self-cleansing for intrusion tolerance. In
Performance, Computing, and Communications
Conference, 2006. IPCCC 2006. 25th IEEE
International, pages 4 pp. –496, april 2006.
[11] Y. Huang and A. Ghosh. Automating intrusion

response via virtualization for realizing uninterruptible
web services. In Network Computing and Applications,
2009. NCA 2009. Eighth IEEE International
Symposium on, pages 114 –117, july 2009.

[12] T. Jackson, B. Salamat, A. Homescu, K. Manivannan,

G. Wagner, A. Gal, S. Brunthaler, C. Wimmer, and
M. Franz. Compiler-generated software diversity. In
Moving Target Defense, pages 77–98. 2011.

[13] T. Jackson, B. Salamat, G. Wagner, C. Wimmer, and

M. Franz. On the eﬀectiveness of multi-variant
program execution for vulnerability detection and
prevention. In Proceedings of the 6th International
Workshop on Security Measurements and Metrics,
MetriSec ’10, pages 7:1–7:8, New York, NY, USA,
2010. ACM.

[14] K. Kolyshkin. Virtualization in linux. White paper,

OpenVZ, September 2006.

[15] P. K. Manadhata and J. M. Wing. A formal model for

a system’s attack surface. In Moving Target Defense,
pages 1–28. 2011.

[16] B. J. Min and J. S. Choi. An approach to intrusion

tolerance for mission-critical services using
adaptability and diverse replication. Future Gener.
Comput. Syst., 20(2):303–313, Feb. 2004.

[17] N. Nethercote and J. Seward. Valgrind: a framework
for heavyweight dynamic binary instrumentation. In
Proceedings of the 2007 ACM SIGPLAN conference
on Programming language design and implementation,
PLDI ’07, pages 89–100, New York, NY, USA, 2007.
ACM.

[18] F. Networking, I. T. Research, and D. (NITRD).

Federal Cybersecurity Game-change R&D Themes,
2012. http://cybersecurity.nitrd.gov/page/federal-
cybersecurity-1.

[19] H. Okhravi, A. Comella, E. Robinson, and J. Haines.

Creating a cyber moving target for critical
infrastructure applications using platform diversity.
International Journal of Critical Infrastructure
Protection, 5(1):30 – 39, 2012.

[20] M. Petkac and L. Badger. Security agility in response

to intrusion detection. In in 16th Annual Computer
Security Applications Conference (ACSAC, page 11,
2000.

[21] R. Rabbat, T. McNeal, and T. Burke. A

high-availability clustering architecture with data
integrity guarantees. In IEEE International
Conference on Cluster Computing, pages 178–182,
2001.

[22] G. Rodr´ıguez, M. J. Mart´ın, P. Gonz´alez, J. Touri˜no,
and R. Doallo. Cppc: a compiler-assisted tool for
portable checkpointing of message-passing
applications. Concurr. Comput. : Pract. Exper.,
22(6):749–766, Apr. 2010.

[23] R.P. Lippmann, J.F. Riordan, T.H. Yu, and K.K.

Watson. Continuous Security Metrics for Prevalent
Network Threats: Introduction and First Four
Metrics. Technical report, MIT Lincoln Laboratory,
May 2012.

[24] A. Saidane, V. Nicomette, and Y. Deswarte. The

design of a generic intrusion-tolerant architecture for
web servers. Dependable and Secure Computing, IEEE
Transactions on, 6(1):45 –58, jan.-march 2009.
[25] B. Salamat, A. Gal, and M. Franz. Reverse stack

execution in a multi-variant execution environment. In
In Workshop on Compiler and Architectural
Techniques for Application Reliability and Security,
2008.

[26] B. Salamat, A. Gal, T. Jackson, K. Manivannan,
G. Wagner, and M. Franz. Multi-variant program
execution: Using multi-core systems to defuse
buﬀer-overﬂow vulnerabilities. In Complex, Intelligent
and Software Intensive Systems, 2008. CISIS 2008.
International Conference on, pages 843 –848, march
2008.

[27] B. Salamat, T. Jackson, G. Wagner, C. Wimmer, and
M. Franz. Runtime defense against code injection
attacks using replicated execution. Dependable and
Secure Computing, IEEE Transactions on, 8(4):588
–601, july-aug. 2011.

[28] S. Schleimer, D. S. Wilkerson, and A. Aiken.

Winnowing: local algorithms for document
ﬁngerprinting. In Proceedings of the 2003 ACM
SIGMOD international conference on Management of
data, SIGMOD ’03, pages 76–85, New York, NY,
USA, 2003. ACM.

[29] K. Scott and J. Davidson. Strata: A Software

Dynamic Translation Infrastructure. Technical Report
CS-2001-17, 2001.

[30] Z. Wang and R. B. Lee. New cache designs for

thwarting software cache-based side channel attacks.
In Proceedings of the 34th annual international
symposium on Computer architecture, ISCA ’07, pages
494–505, New York, NY, USA, 2007. ACM.
[31] D. Williams, W. Hu, J. W. Davidson, J. D. Hiser,

J. C. Knight, and A. Nguyen-Tuong. Security through
diversity: Leveraging virtual machine technology.
IEEE Security and Privacy, 7(1):26–33, Jan. 2009.

[32] A. S. Yih Huang, David Arsenault. Incorruptible

self-cleansing intrusion tolerance and its application to
dns security. AJournal of Networks, 1(5):21–30,
September/October 2006.

