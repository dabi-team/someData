9
1
0
2

y
a
M
4
2

]

G
L
.
s
c
[

1
v
3
5
0
0
1
.
5
0
9
1
:
v
i
X
r
a

Federated Forest

Yang Liu1,2 , Yingting Liu3,∗, Zhijie Liu4,∗, Junbo Zhang1,2,†, Chuishi Meng1,2 , Yu Zheng1,2
1JD Intelligent Cities Research, Beijing, China
2JD Intelligent Cities Business Unit, Beijing, China
3University of Science and Technology of China, Hefei, China
4Beijing Normal University, Beijing, China
{liuyang21cn,yingting6,msjunbozhang,msyuzheng}@outlook.com
201621210044@mail.bnu.edu.cn, chuishimeng@gmail.com

Abstract

Most real-world data are scattered across different companies or government orga-
nizations, and cannot be easily integrated under data privacy and related regulations
such as the European Union’s General Data Protection Regulation (GDPR) and
China’ Cyber Security Law. Such data islands situation and data privacy & secu-
rity are two major challenges for applications of artiﬁcial intelligence. In this paper,
we tackle these challenges and propose a privacy-preserving machine learning
model, called Federated Forest, which is a lossless learning model of the traditional
random forest method, i.e., achieving the same level of accuracy as the non-privacy-
preserving approach. Based on it, we developed a secure cross-regional machine
learning system that allows a learning process to be jointly trained over different
regions’ clients with the same user samples but different attribute sets, processing
the data stored in each of them without exchanging their raw data. A novel predic-
tion algorithm was also proposed which could largely reduce the communication
overhead. Experiments on both real-world and UCI data sets demonstrate the
performance of the Federated Forest is as accurate as the non-federated version.
The efﬁciency and robustness of our proposed system had been veriﬁed. Overall,
our model is practical, scalable and extensible for real-life tasks.

1

Introduction

Artiﬁcial intelligence has made great progress in recent years thanks to the large amount of data
collected in different domains. Unfortunately, the data has also arisen to be the largest bottleneck
for the implementation of AI methods. In real-world applications, the big data are scattered across
different companies or government organizations and stored in the form of data islands, in other
words, data across different domains cannot be shared with each other. For companies the data is
among one of the most important assets of companies which cannot be easily shared. Governments’
data are highly secured and mostly not utilized. Besides, people now are highly sensitive about data
privacy. Data breaches happen occasionally and most countries now either have data privacy-related
legislation enacted or being drafted. In 2018, the European Union enacted the General Data Protection
Regulation (GDPR) [26]. The GDPR provides individuals with more control over their personal data
and states strict principles and absolute transparencies on how businesses should handle these data.
Any type of tracking or record of personal data must be authorized by the customer before collection
and business must clearly state their intentions and plans for the data. Faced with the difﬁculties and
restrictions, the question becomes if it is worth investing effort to make use of the scattered data.

∗Equal contribution. The research was done when the second and third authors were interns at JD Intelligent

Cities Research.

†Junbo Zhang is the corresponding author.

Preprint. Under review.

 
 
 
 
 
 
The answer is yes. Academia, companies and governments could all beneﬁt from resolving the data
islands situation. The joint-models are able to improve many current services and products, and
support more potential applications, including but not limited to medical study, targeted marketing,
urban anomalies detection and risk management, as shown in Figure 1. For example, banks could
train joint-models with e-commerce companies to achieve a precised customer proﬁling and improve
their marketing strategies. Government organizations could work with ride-hailing companies to have
a better understanding of city’s daily trafﬁc ﬂow and adjust the road planing based on it.

Consequently, the question becomes how can
we train the joint-models. Faced with the chal-
lenges of data islands and data privacy & secu-
rity, the current available methods cannot com-
pletely solve the problems. Because of this,
developing new methods to bridge the gap be-
tween real-world applications and data islands
becomes an urgent problem. In 2016, a new
approach named federated learning [23, 20, 19]
was proposed, which mainly focuses on build-
ing privacy-preserved machine learning mod-
els when data are distributed in different places.
Federated learning has provided a new approach
to look at the current problems, and shwon the possibility of real-life applications.

Figure 1: New Era of Machine Learning

Inspired by their work, we proposed a novel privacy-preserving tree-based machine learning model,
named Federated Forest (FF). Based on it, we developed a secure cross-regional machine learning
system, which is capable of conquering the challenges described above. Our contributions are
four-folds:

• Secured privacy. Data privacy is fully protected by redesigning the tree building algorithms,
applying encryption methods and establishing a third-party trusty server. The contents and amount
of information exchange are limited to a minimum, and each participant is blind to others.

• Lossless (accurate). Our model is based on the methodologies of CART [3] and bagging [2], and
ﬁts the vertical federated setting. We experimentally proved that our model can achieve the same
level of accuracy as the non-federated approach that brings the data into one place.

• Efﬁciency. An efﬁcient communication mechanism was implemented with methods of MPI [1]
for sharing of the intermediate values. A fast prediction algorithm was designed and it’s weakly
correlated (scale-free) to the number of domains and trees, maximum tree depth and sample size.
• Practicability and scalability. Our model supports both classiﬁcation and regression tasks, and is
strongly practical, extensible and scalable for real-life applications. The experiments on real-world
data sets had proved our model’s accuracy, efﬁciency and robustness.

2 Related Work

2.1 Federated Learning

Federated learning [23, 20, 19] was ﬁrst proposed to solve the problems that rich data are generated
from user devices, but due to regulations it’s difﬁcult to build models from the data. The solution is to
keep the data on user devices and train a shared model by aggregating locally calculated intermediate
results in neural networks. In [5] they proposed a new recommender system which applies federated
learning to meta-learning. Federated learning has also been applied to solve multi-task problems
in [28] and a loss-based AdaBoost method was developed in [16]. [14] introduced a vertically-
aggregated federated learning method. In their work, each data provider possessed unique features,
and sample IDs are aligned between them. They jointly learned a logistic regression model to secure
the data privacy and keep modeling accurate. In addition, a modular benchmarking framework
for federated settings was presented in the work of [4]. Although many research products have
been coming out, the deﬁnition of federated learning was still blurry until the work of [29]. They
categorized current federated learning methods into three types, horizontal federated learning, vertical
federated learning and federated transfer learning. Following this survey, the same team introduced a
new framework known as secure federated transfer learning [22] to build models for target-domain

2

party by leveraging rich labels from source-domain party, as the data sets of the two parties are
different in both sample space and feature space. In [6] they reviewed the tree-boosting method and
applied it to the vertical federated setting. A lossless framework was proposed and it was able to
keep information of each private data provider from being revealed. In [30] they presented a novel
reinforcement learning approach that considers the privacy requirement and builds Q-network for
each agent with the help of other agents. To make the federated machine learning more practical, they
are pushing to build a Federated AI Ecosystem such that the partners can fully exploit their data’s
value and promote vertical applications. An IEEE standard Guide For Architectural Framework And
Application Of Federated Machine Learning [12] was also initialized and is being drafted.

2.2 Data Privacy Protection

In federated learning, there are two major encryption methods applied for protecting data privacy and
security, which are differential privacy [8] and homomorphic encryption [10]. The idea of differential
privacy is to add properly calibrated noise to the algorithm or the data, with examples including
[11, 24]. This approach will not affect computational efﬁciency too much but may weaken model
performance. Homomorphic encryption is a method that supports secure multiplication and addition
on encrypted data, and once the result is decrypted, it should match the output of operations on
the corresponding raw data. The work of [15, 21, 18] all used this approach. There are two major
drawbacks of homomorphic encryption. First, the complexity of the algorithm is high and it will
be intensely time consuming for frequent use. Second, it does not support operations of non-linear
functions, such as Sigmoid and Logarithmic function, and approximations are necessary. In the work
of [14] they used the Taylor expansion to approximate the Sigmoid function and [17] used least
squares method. In theory these approaches could work but in our practice the results were not ideal.

3 Problem Formulation

3.1 Data Distribution

In our work, we focus on the vertical federated learning problems, in which all participants have the
same sample space but different feature space, as shown in Figure 2. Consider each company or
government organization as a regional data domain, denoted as Di, then the overall data domain is
D = D1 ∪ D2 ∪ · · · ∪ DM , where 1 ≤ i ≤ M . M is the number of regional domains. We denote
the feature space of Di as Fi, then the entire feature space F is F = F1 ∪ F2 ∪ · · · ∪ FM . During
the modeling process, all features’ true names were encoded to protect privacy. For any i and j, if
i (cid:54)= j and 1 ≤ i, j ≤ M , then Fi ∩ Fj = ∅. In our work, all domains have the same number of
samples and the sample IDs were aligned across domains. One master machine was deployed as
the parameter server and multiple client machines were used, where each contains one regional data
domain. The labels y were provided by one of the clients, which we assume to be client 1. Then the
labels were copied to the master and clients in encrypted forms. Two things to notice here: 1) In
reality, M is usually small and even M = 5 means there are ﬁve different organizations modeling
together, which could be rare. The model design can be totally different for large M . 2) We are not
going to talk about the methods of ID alignment since it is another research topic, discussed in work
such as [25]. The notations appeared in this paper are also shown in Table 3.

3.2 Problem Statement

The formal statement of the problem is given as below:

Given: Regional domain Di and encrypted label y on each client i, 1 ≤ i ≤ M .

Learn: A Federated Forest, such that for each tree in the forest: 1) a complete tree model T is held
on master; 2) a partial tree model Ti is stored on each client i, 1 ≤ i ≤ M .

Constraint: The performance (accuracy, f1-score, MSE, e.t.c.) of the Federated Forest must be
comparable to the non-federated random forest.

3

4 Methodology

Here we present the framework of Federated
Forest, which is based on the CART tree [3] and
bagging [2], and is able to deal with both classiﬁ-
cation and regression problems. The framework
is shown as in Figure 2 and details of the algo-
rithms are given in the following subsections.

4.1 Model Building

Algorithm.
In our work, each tree is built by
all parties working together and the tree struc-
ture is stored on the master node and every client.
However, each tree only stores the split informa-
tion with respect to their own features. We ﬁrst present the client-side Federated Forest algorithm in
Algorithm 1, and in Algorithm 2 we described how the master coordinates the modeling process.

Figure 2: Federated Forest

Following the bagging paradigm, the mas-
ter node ﬁrst randomly selects a subset
of features and samples from the entire
data. Then the master will notify each
client the selected features and sample
IDs privately. For the selected features,
master will notify each client privately.
For example, if ten features are chosen
by the master and client 1 only possesses
three of them, then client 1 will only know
these three features were selected. It will
never know how many features were cho-
sen globally, not to mention what the fea-
tures were. During the tree construction,
the pre-pruning conditions are frequently
checked. If the conditions are satisﬁed, the
clients and master will create leaf nodes
accordingly.

ALGORITHM 1: Federated Forest – Client
Input

:Data set Di on client i;
Local features Fi = ∅ or Fi = {fA, fB, · · · };
Encrypted label y;

Output :Partial Federated Forest Model on Client i
while tree_build is True do

(cid:48)

(cid:48)

Receive F
i ⊂ Fi and D
Function TreeBuild (D

i ⊂ Di for current tree building;
i, F
Create empty tree node;
if the pre-pruning condition is satisﬁed then

i , y)

(cid:48)

(cid:48)

Mark current node as leaf node;
Assign leaf label by voting;
return leaf node;
p, f ∗ ← −∞, N one;
i (cid:54)= ∅ then
if F
Compute impurity improvement p for any

(cid:48)

(cid:48)

i and ﬁnd local maximum pi;

f ∈ F
Record local best split feature f ∗ and split
threshold;

else

right subtrees to master;

Receive sample indices of left and right subtrees;

Send encrypted pi to master;
if receive the split message from master then

/* Global best split feature is from itself */
is_selected ← True;
Split samples and send sample indices of left and

If the termination condition is not trig-
gered, all clients enter the splitting state,
and the best split feature of the current
tree node will be selected by comparing
the impurity improvements. First, each
client i ﬁnds the local optimal split feature
f ∗
i . Then the master collects all local op-
timal features and corresponding impurity
improvements, allowing the global best
feature to be found. Second, the master
notiﬁes the client who provided the global
best feature. The corresponding client will
split the samples and send the data parti-
tion results (sample IDs that fall into left
and right subtrees) to the master for distri-
bution. For the current tree node, only the
client that provides the best split feature
will save the details of this split. The other clients are only aware that the selected feature is not
contributed by themselves. The split information such as threshold and split feature are also unknown
to them. Last, the subtrees are recursively created and the current tree node is returned. In modeling,
if the child trees nodes are created successfully, the parent node doesn’t need to save the sample IDs
for the subtrees. Otherwise, if the connection is down, the modeling can be easily recovered from the
break point.

left_subtree ← TreeBuild (D
right_subtree ← TreeBuild (D
if is_selected is True then

Save subtrees to tree node;
return tree node;

return Partial Federated Forest Model on Client i;

Save f ∗ and split threshold to tree node;

Append current tree to forest;

i_lef t, F
i_right, F

i , yright);

i , ylef t);

(cid:48)

(cid:48)

(cid:48)

(cid:48)

4

Model Storage. A tree predictive
model is composed of two parts, tree
structure and split information such as
feature and threshold used for each split.
Since the forest is built with all clients
working together, the structure of each
tree on every client is the same. However,
for a given tree node, the client may or
may not store the detailed information.
Only the master server is able to store
the complete model. For each tree node,
the client will store the corresponding
split threshold only if it provided the
split feature. If not, the client will store
the current node but only
nothing at
keep the node structure. We denoted
the complete tree nodes as T , the one
saved on master, and denoted the tree
nodes without full details stored by ith
client as Ti. Since the tree structure is
consistent, we consider Ti ⊂ T , and
T1 ∩ T2 ∩ · · · ∩ TM = L, where L is
the leaf node sets. The complete tree
T is the union of all partial trees, that
T = T1 ∪ T2 ∪ · · · ∪ TM .

4.2 Model Prediction

ALGORITHM 2: Federated Forest – Master
Input

:Indices of D;
Encoded features F = F1 ∪ F2 ∪ · · · ∪ FM ;
Encrypted label y;

Output :Complete Federated Forest Model
/*Build trees for forest recurrently*/
while tree_build is True do

Broadcast randomly selected samples D
Randomly select features F
(cid:48)
Function TreeBuild (D

;
i from Fi and send to client i;
, F

, y)

(cid:48)

(cid:48)

(cid:48)

Create empty tree node;
if the pre-pruning condition is satisﬁed then

i=1 and related information

Mark current node as leaf node;
Assign leaf label by voting;
return leaf node;
Receive encrypted {p}M
from all clients;
Take j = argmax({p}M
Receive split indices from client j and broadcast;
lef t, F
left_subtree ← TreeBuild (D
right_subtree ← TreeBuild (D
right, F
Save subtrees and split info to tree node;
return tree node;

i=1) and notify client j;

, yright);

, ylef t);

(cid:48)

(cid:48)

(cid:48)

(cid:48)

Append current tree to forest;

return Complete Federated Forest Model;

ALGORITHM 3: Federated Forest Prediction – Client
Input

:Partial federated forest model saved on ith client;
Encoded features Fi on ith client;
Test set Dtest

i

on ith client;
i of leaf l on Ti, l ∈ L

Output :Samples IDs Sl
while TreePrediction is True do

Function TreePredict (Ti, Dtest

, Fi)

i

if is_leaf is True then

Return sample IDs Sl

i and leaf label;

else

if Ti keeps the split info of current node then

Split samples into subtrees;
left_subtree ← TreePredict (Ti_lef t, Fi,
Dtest
right_subtree ← TreePredict (Ti_right,
Fi, Dtest

i,lef t);

i_right);

else

);

left_subtree ← TreePredict (Ti_lef t, Fi,
Dtest
i
right_subtree ← TreePredict (Ti_right,
Fi, Dtest

);
Return left and right subtrees;

i

Under the vertical federated setting [29],
the classical approach of prediction in-
volves multiple rounds of communication
between the master and clients, even for
only one sample. When the number of
trees, maximum tree depth and sample
size are large, the communication require-
ments for predicting will become a serious
burden. To address this problem, we de-
signed a novel prediction method which
takes the advantage of our distributed
model storage strategy. Our method only
needs one round of collective communica-
tion for each tree and even for the overall
forest. We ﬁrst present the prediction al-
gorithm of the client side in Algorithm 3,
and in Algorithm 4, we described how the
master server coordinates each client to
achieve the ﬁnal predictions.

i , S2

i , · · · , Sl

Send Si = {S1

First, each client uses the locally stored
model to predict samples. For the tree Ti
on ith client, each sample enters Ti from
the root node, and ﬁnally falls into one or
several leaf nodes through the binary tree. When the sample travels through each node, if the model
stores the split information at this node, then this sample is determined to enter the left or right subtree
by checking the split threshold. If the model does not have split information at this node, the sample
simultaneously enters both left and right subtrees.

i, · · · } to master;

Secondly, the path determination of the tree node is performed recursively until each sample falls
into one or several leaf nodes. When this process is ﬁnished, each leaf node of tree Ti on client i will

5

keep a batch of samples. We use Sl
model Ti, where l ∈ L. L is the set of leaf nodes of the tree Ti.

i to represent the samples that fall into the leaf node l of the tree

i}M

Thirdly, for each leaf l ∈ L, the master
will take the intersection on {Sl
i=1, and
the result will be Sl. Then the sample sets
Sl owned by each leaf node on complete
tree T are already associated with ﬁnal
predictions. Here we gave a formal propo-
sition on our new prediction method so it
can be mathematically deﬁned:

ALGORITHM 4: Federated Forest Prediction – Master
:Sample IDs S of test set Dtest
Input
Output : Prediction of Federated Forest
while TreePrediction is True do

Gather {S1, S2, · · · , Si, · · · };
Obtain {S1, S2, · · · , Sl, · · · }, where
Sl = Sl
Return label of leaf l for samples in Sl, l ∈ L;

2 ∩ · · · ∩ Sl

1 ∩ Sl

M ;

proposition 1. For samples S fall into
one or multiple leaves on tree Ti, then
for any leaf l of the complete tree T , the
sample IDs Sl in leaf l can be obtained by taking intersection of {Sl

Calculate forest predictions by voting on the results of trees;
return Final Predictions;

i}M

i=1, that Sl = Sl

1∩Sl

2∩· · ·∩Sl

M .

The proof is provided in Appendix 7. After obtaining the label values for each sample on all trees,
we can easily achieve ﬁnal predictions. In this approach, we only need one round of communication
for each tree, or even only one round for the entire forest.

4.3 Privacy Protection

Here we have categorized our efforts on the privacy protection into ﬁve parts:

Identities. In real world tasks, we often face situations where IDs of samples are tied to persons’ real
identities. Because of this, we have to encrypt the identities before the ID alignment. An example
approach could be like following: First all clients use an agreed hash method to transform the sample
IDs and generate new hashed IDs. Then Message-Digest Algorithm 5 (MD5) can be applied on the
hashed IDs and generate irreversibly encrypted IDs.

Labels. For classiﬁcation problems, even labels are encoded, we could still guess the true values,
especially for binary classiﬁcation. For regression problems, even though labels can be encrypted
with homomorphic encryption, it will be extremely time consuming for modeling. In practical tasks,
there will be a trade-off between the security protection and the computational efﬁciency.

Features. On each client, local features were encoded before given to the master for global feature
sampling. So the master will not know the real meaning of features.

Communication. Encryption methods such as RSA and AES can be applied to secure everything
(model intermediate values, sample indices, e.t.c.) communicated during the training and prediction.

Model Storage. The entire model was distributed across all clients. For each node, the client would
store the corresponding split information only if the split feature is on local machine. If not, it only
stored the structure of the current node. Clients knew nothing about each other including whose
features were selected and at which tree nodes. Master can optionally keep a copy of the entire model.

5 Experimental Studies

5.1 Experimental Setup

In this section, we used 9 benchmark data sets, including one real-world data set target marketing and
8 public data sets from UCI [7, 27, 9, 13], as shown in Table 1. Different sample sizes and feature
spaces were considered, and the accuracy, efﬁciency and robustness of our proposed framework
were tested for both classiﬁcation and regression problems. In our experiments we did not pursue
absolute accuracy and instead tested whether the performance of our methods is at the same level
as the non-federated approach, i.e., lossless. The target marketing data set was collected from
two totally different domains. One of them was from an e-commerce company and contains 84
features, and the other one was from a bank which provided 11 features. Before modeling all the
sensitive information was protected. Three main series of experiments were conducted in this section,
including experiments with two data providers, experiments with multiple data providers, and analysis
of prediction efﬁciency. The details of each test are given in the following subsections.

6

Table 1: Classiﬁcation and regression experiments

Classiﬁcation
target marketing
ionosphere
spambase
parkinson [27]
kdd cup 99
waveform
gene
Regression
year prediction
Superconduct [13]

RF1
0.870
0.864
0.844
0.849
0.974
0.745
0.975
RF1
10.47
19.74

RF2
0.848
0.828
0.831
0.849
0.965
0.743
0.975
RF2
10.72
17.49

F-LR
0.862
0.873
0.873
0.829
-
-
-
F-LR
9.56
17.52

NonFF
-
0.908 ± 0.019
0.943 ± 0.005
0.859 ± 0.018
0.995 ± 0.001
0.826 ± 0.008
0.988 ± 0.005
NonFF
9.537 ± 0.003
15.369 ± 0.118

FF
0.890 ± 0.014
0.896 ± 0.030
0.928 ± 0.020
0.857 ± 0.013
0.995 ± 0.009
0.822 ± 0.012
0.982 ± 0.006
FF
9.555 ± 0.061
15.411 ± 0.163

p-value
-
0.211
0.065
0.744
0.012
0.029
0.229
p-value
0.058
0.186

5.2 Experiments with Two-Party Scenarios

In this part, exposed UCI data sets were vertically and randomly separated by feature dimension
and placed on two different client servers (M = 2), each containing half of the feature space from
original data. For target marketing, it was also placed on two different client servers, of which each
contained several business domains. The experiments in this section are summarized as following:

• Federated Logistic/Linear Regression (F-LR): We jointly trained logistic/linear regression mod-

els, where data is kept locally and the model is partly stored in each client.

• Non-Federated Forest (NonFF): All data were integrated together for Random Forest modeling.
• Random Forest 1 (RF1): Partial data from the 1st client was used to build a random forest model.
• Random Forest 2 (RF2): Partial data from the 2nd client was used to build a random forest model.
• Federated Forest (FF): This is our proposed model, which two parties jointly learn a random

forest. Data were kept locally and model was partly stored in each client.

We conducted the experiments on both classiﬁcation and regression problems, and present the results
of accuracy and RMSE in Table 1. We found that the performance of RF1 and RF2 were obviously
worse than the NonFF and FF. Both RF1 and RF2 can be considered as modeling with data from
one business domain, and the insufﬁcient feature space resulted in imperfect study of the global
knowledge. We also found in most tests that the regression models didn’t perform very well. For the
test on target marketing, since direct aggregation of data between two institutions was not allowed,
we only ran tests for RF1, RF2, F-LR and FF. The results show that FF performs as expected and a
better accuracy is achieved by building models on different domains.

For most of the data sets, NonFF and FF outperformed the other methods. In our method, we were
building each tree by processing globally on every regional domain, which was same to the tree built
by aggregating raw data together. Z-Test was applied to verify the lossless of our method compared
with NonFF, of which the null hypothesis is that the means from two populations are equal at a given
level of signiﬁcance. For each data set, 40 rounds of tests on the NonFF and FF were performed
and the p-value of each Z-Test is given in Table 1. If the p-value ≥ 0.05, the null hypothesis cannot
be rejected at the 0.05 level and there is no signiﬁcant difference between the outputs of NonFF
and FF. If 0.01 ≤ p-value < 0.05, the null hypothesis cannot be rejected at the 0.01 level. And
statistically, we consider there exists a slight but acceptable difference for this range of p-value. The
null hypothesis should be rejected if p-value < 0.01 with a signiﬁcant difference between the means.
By examining the p-value of each data set, we can ﬁnd that there are six of them proved to have no
signiﬁcant difference between the results of NonFF and FF, and for the rest data sets the differences
are slight. No null hypotheses were rejected.

Overall, we can safely conﬁrm that the Federated Forest is a lossless solution for both classiﬁcation
and regression problems, which achieves the same performance as the non-federated random forest.

5.3 Experiments with Multi-Party Scenario

In this part, we ran tests on the parkinson data set to verify whether the Federated Forest is capable of
conjoining more than two domains effectively and if a reasonable improvement on accuracy could

7

be achieved. We chose parkinson to run the test since it already contains eight clearly categorized
sub-domains. As for tests of training and prediction efﬁciency, we duplicated data for ten times. In the
tests, each time we added one domain into the federated model, and we recorded the accuracy, training
and prediction time. As shown in Figure 3, the accuracy of Federated Forest improved consistently.
The training execution time was almost linearly with respect to to the number of domains, which is to
be expected because all features are be examined in tree building. For the prediction time, though more
domains and features were added, the difference in execution time was negligible. The results demon-
strate that our new prediction algorithm is very effective when handling multiple regional domains.

5.4 Prediction Efﬁciency

In this part, we compared the efﬁciency of our
new prediction method with the classical pre-
diction approach. We used target marketing,
spambase and waveform data sets as the exam-
ples. We ran all the tests for 20 times and report
the average results, as shown in Figures 4, 5 and
6. The solid lines with dot marker represent the results of classical prediction method, and the dash
lines with x marker represent our proposed prediction method.

Figure 3: Accy. & Exec. Time vs. # of Domains

Firstly, we set the maximum tree depth to 4 and
changed the number of estimators from 8 to 32,
and the results were shown in Figure 4. It can
be seen that our method produced a strong im-
provement on the prediction efﬁciency. Though
the execution time of both methods increased
linearly respect to the number of estimators, the
slope varied dramatically between our method
and the classical prediction method. For the clas-
sical method, there are multiple rounds of communication in each node during prediction. But in our
method, there is only one round of communication for each tree.

Figure 4: Prediction Time vs. Number of Estimators

Secondly, we set the number of estimators to
8, and adjusted the maximum tree depth from
4 to 16. As shown in Figure 5, our method
outperformed the classical prediction method
again. By increasing the maximum tree depth,
the growth rate of prediction time for both meth-
ods gradually slowed down and stabilized. This
is because by setting the maximum depth to a
large number, the tree building may early stop
due to pre-pruning and the actual tree depth will be smaller. In our method, no matter how deep the
tree is or how many leaf nodes are created, communication was only executed once for each tree.

Figure 5: Prediction Time vs. Max Depth

Finally, we ﬁxed the number of estimators and
maximum tree depth, and changed the test sam-
ple rate from 0.1 to 0.4, as shown in Figure 6.
Because the classical approach has a strong lin-
ear correlation with the sample size, we found
that its results presented a linear growth trend.
Meanwhile the execution time of our method
changed very slowly, which shows our method
is robust to prediction sample size.

Figure 6: Prediction Time vs. Test Sample Size

Overall, our new prediction method had been proved to be highly efﬁcient.

6 Conclusions

In this paper, we proposed a novel tree-based machine learning model, called Federated Forest, which
is lossless with respect to the model accuracy and protects data privacy. A secure cross-regional

8

machine learning system was developed based on it, which allows a learning model to be jointly
trained across different clients with the same user samples but different attribute sets. The raw
data on each client are not exposed and exchanged to other clients during the modeling. A novel
prediction algorithm was proposed which could largely reduce the communication overhead and
improve the prediction efﬁciency. Data privacy was secured by redesigning the tree algorithms,
deploying encryption methods and establishing a third-party trusted server. Raw data will never be
directly exchanged, only limited amount of intermediate values between each party. We performed
experiments on both real-world and UCI data sets, showing the superior performance in classiﬁcation
and regression tasks, and the proposed Federated Forest was proven to be as accurate as the non-
federated random forest that requires gathering the data into one place. The efﬁciency and robustness
of our proposed system have also been veriﬁed. Overall, the Federated Forest overcomes the
challenges of the data islands problem and privacy protection in a brand new approach, and it can be
deployed for real-world applications.

Acknowledgement

Special thanks to Chentian Jin for valuable discussions and feedback.

References

[1] Blaise Barney. 2019. Message Passing Interface (MPI). Lawrence Livermore National Labora-

tory. Available at https://computing.llnl.gov/tutorials/mpi.

[2] Leo Breiman. 1996. Bagging Predictors. Machine Learning 24, 2 (01 Aug 1996), 123–140.

https://doi.org/10.1023/A:1018054314350

[3] L. Breiman, J. Friedman, C.J. Stone, and R.A. Olshen. 1984. Classiﬁcation and Regression

Trees. Taylor & Francis.

[4] Sebastian Caldas, Peter Wu, Tian Li, Jakub Koneˇcný, H. Brendan McMahan, Virginia Smith, and
Ameet Talwalkar. 2018. LEAF: A Benchmark for Federated Settings. arXiv:cs.LG/1812.01097

[5] Fei Chen, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. 2018. Federated Meta-Learning for

Recommendation. arXiv:cs.LG/1802.07876

[6] Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, and Qiang Yang. 2019. SecureBoost:

A Lossless Federated Learning Framework. arXiv:cs.LG/1901.08755

[7] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http://archive.

ics.uci.edu/ml

[8] Cynthia Dwork. 2006. Differential Privacy. In Automata, Languages and Programming, Michele
Bugliesi, Bart Preneel, Vladimiro Sassone, and Ingo Wegener (Eds.). Springer Berlin Heidelberg,
Berlin, Heidelberg, 1–12.

[9] Kelwin Fernandes, Pedro Vinagre, and Paulo Cortez. 2015. A Proactive Intelligent Decision
Support System for Predicting the Popularity of Online News. In Progress in Artiﬁcial Intel-
ligence, Francisco Pereira, Penousal Machado, Ernesto Costa, and Amílcar Cardoso (Eds.).
Springer International Publishing, Cham, 535–546.

[10] Craig Gentry. 2009. A fully homomorphic encryption scheme. Ph.D. Dissertation. Stanford

University. crypto.stanford.edu/craig.

[11] Robin C. Geyer, Tassilo Klein, and Moin Nabi. 2017. Differentially Private Federated Learning:

A Client Level Perspective. arXiv:cs.CR/1712.07557

[12] Federated Machine Learning Working Group. 2019. P3652.1 - Guide for Architectural Frame-
work and Application of Federated Machine Learning. Available at https://standards.
ieee.org/project/3652_1.html.

[13] Kam Hamidieh. 2018. A data-driven statistical model for predicting the critical temperature
https:

of a superconductor. Computational Materials Science 154 (2018), 346 – 354.
//doi.org/10.1016/j.commatsci.2018.07.052

9

[14] Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume
Smith, and Brian Thorne. 2017. Private federated learning on vertically partitioned data via
entity resolution and additively homomorphic encryption. arXiv:cs.LG/1711.10677

[15] Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume
Smith, and Brian Thorne. 2017. Private federated learning on vertically partitioned data via
entity resolution and additively homomorphic encryption. arXiv:cs.LG/1711.10677

[16] Li Huang, Yifeng Yin, Zeng Fu, Shifa Zhang, Hao Deng, and Dianbo Liu. 2018.
LoAdaBoost:Loss-Based AdaBoost Federated Machine Learning on medical Data.
arXiv:cs.LG/1811.12629

[17] Miran Kim, Yongsoo Song, Shuang Wang, Yuhou Xia, and Xiaoqian Jiang. 2018. Secure
logistic regression based on homomorphic encryption: Design and evaluation. JMIR medical
informatics 6, 2 (2018), e19.

[18] Sangwook Kim, Masahiro Omori, Takuya Hayashi, Toshiaki Omori, Lihua Wang, and Seiichi
Ozawa. 2018. Privacy-Preserving Naive Bayes Classiﬁcation Using Fully Homomorphic
Encryption. In Neural Information Processing, Long Cheng, Andrew Chi Sing Leung, and
Seiichi Ozawa (Eds.). Springer International Publishing, Cham, 349–358.

[19] Jakub Konecn`y, H Brendan McMahan, Daniel Ramage, and Peter Richtárik. 2016. Federated Op-
timization: Distributed Machine Learning for On-Device Intelligence. arXiv:cs.LG/1610.02527

[20] Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh,
and Dave Bacon. 2016. Federated Learning: Strategies for Improving Communication Efﬁciency.
arXiv:cs.LG/1610.05492

[21] Trieu Phong Le, Yoshinori Aono, Takuya Hayashi, Lihua Wang, and Shiho Moriai. 2018.
Privacy-Preserving Deep Learning via Additively Homomorphic Encryption. IEEE Transactions
on Information Forensics & Security PP, 99 (2018), 1–1.

[22] Yang Liu, Tianjian Chen, and Qiang Yang. 2018. Secure Federated Transfer Learning.

arXiv:cs.LG/1812.03337

[23] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y
Arcas. 2016. Communication-Efﬁcient Learning of Deep Networks from Decentralized Data.
arXiv:cs.LG/1602.05629

[24] H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2017. Learning Differen-

tially Private Recurrent Language Models. arXiv:cs.LG/1710.06963

[25] Richard Nock, Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Giorgio Patrini, Guillaume
Smith, and Brian Thorne. 2018. Entity Resolution and Federated Learning get a Federated
Resolution. arXiv:cs.DB/1803.04035

[26] General Data Protection Regulation. 2016. Regulation (EU) 2016/679 of the European Parlia-
ment and of the Council of 27 April 2016 on the protection of natural persons with regard to
the processing of personal data and on the free movement of such data, and repealing Directive
95/46. Ofﬁcial Journal of the European Union (OJ) 59, 1-88 (2016), 294.

[27] C Okan Sakar, Gorkem Serbes, Aysegul Gunduz, Hunkar C Tunc, Hatice Nizam, Betul Erdogdu
Sakar, Melih Tutuncu, Tarkan Aydin, M Erdem Isenkul, and Hulya Apaydin. 2019. A compara-
tive analysis of speech signal processing algorithms for Parkinson´s disease classiﬁcation and the
use of the tunable Q-factor wavelet transform. Applied Soft Computing 74 (2019), 255–263.

[28] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. 2017. Federated

Multi-Task Learning. arXiv:cs.LG/1705.10467

[29] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated Machine Learning:
Concept and Applications. ACM Transactions on Intelligent Systems and Technology (TIST) 10,
2 (2019), 12.

[30] Hankz Hankui Zhuo, Wenfeng Feng, Qian Xu, Qiang Yang, and Yufeng Lin. 2019. Federated

Reinforcement Learning. arXiv:cs.LG/1901.08277

10

APPENDIX

Reproducibility

Our model is implemented with Python 3.6, Scikit-learn 0.20, Numpy 1.15.4, python-paillier 1.4.1
and mpi4py 3.0.0. We train/evaluate our model on servers each with 4 CPU cores and Centos 7.0.
The information of all used data sets are given in Table 2.

Table 2: Data sets

Classiﬁcation
target marketing
ionosphere
spambase
parkinson [27]
kddcup99
waveform
gene
Regression
year prediction
Superconduct [13]

Size
156198
351
4601
756
4M
5000
801
Size
515345
21263

Features
95(11/84)
34
57
754
42
21
20531
Features
90
81

Classes
2
2
2
2
23
3
5
Range
1922-2011
0.0002-185

Pseudo-code for FF-Regressor

The main difference between regression and classiﬁcation problem lies in the generation of leaf node
result and the ﬁnal predictions. The following is the pseudo-code of regression problem, where the
difference from the classiﬁcation problem is in the line 7 of Algorithm 5, line 9 of Algorithm 6 and
line 5 of Algorithm 8.

Notations In Proof

• Sample IDs are denoted as S, and Sl

i contains the sample IDs which fall into leaf l of tree Ti. Sl

denotes the sample set of leaf node l in the complete binary tree model T .

• The test sample set is H, and the single sample is h ∈ H.

• Wi is the set of decision making paths of sample h that goes through the binary tree to fall into the
leaf node of Ti. For the tree Ti, it is possible that h falls into more than one leaf, due to our model
storage strategy.

• w∗ is the decision making path of the sample h that goes through the complete binary tree to fall
into the leaf node in T . For the complete tree T , if sample h fall into one leaf, then it cannot fall
into another leaf. It means that any leaf l and g in T , Sl ∩ Sg = ∅.

• The complete tree T on master is deﬁned as T = T1 ∪ T2 ∪ · · · ∪ TM .
• Detailed descriptions of notations are shown in Table 3.

Proof of the Proposition 1

For the prediction process, samples S will go through the client tree Ti and fall into one or multiple
leaves. For any leaf l of the complete tree T , the sample IDs Sl in leaf l can be obtained by taking
intersection of {Sl

i=1, that Sl = Sl

2 ∩ · · · ∩ Sl

1 ∩ Sl

i}M

M .

Proof. In order to prove Sl = Sl

1 ∩ Sl

2 ∩ · · · ∩ Sl

M , we will prove:

• Sl ⊆ Sl

1 ∩ Sl

2 ∩ · · · ∩ Sl
M

• Sl ⊇ Sl

1 ∩ Sl

2 ∩ · · · ∩ Sl
M

11

ALGORITHM 5: Federated Forest – Client
Input

:Data set Di on client i;
Local features Fi = ∅ or Fi = {fA, fB, · · · };
Homomorphic encrypted label y;
Output :Partial Federated Forest Model on Client i

1 while tree_build is True do
Receive F
i ⊂ Fi and D
Function TreeBuild (D

2

3

(cid:48)

(cid:48)

i ⊂ Di for current tree building;
i, F

i , y)

(cid:48)

(cid:48)

Create empty tree node;
if the pre-pruning condition is satisﬁed then

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

Mark current node as leaf node;
Assign leaf label by averaging;
return leaf node;

(cid:48)

end
p, f ∗ ← −∞, N one;
if F
i (cid:54)= ∅ then
Compute impurity improvement p for any f ∈ F
Record local best split feature f ∗ and split threshold;

(cid:48)

i and ﬁnd local maximum pi;

end
Send encrypted pi to master;
if receive the split message from master then

/* Global best split feature f ∗
is_selected ← True;
Split samples and send sample indices of left and right subtrees to master;

global is from itself */

else

Receive sample indices of left and right subtrees;

end
left_subtree ← TreeBuild (D
right_subtree ← TreeBuild (D
if is_selected is True then

(cid:48)

i_lef t, Fi
i_right, F

(cid:48)

(cid:48), ylef t);

(cid:48)

i , yright);

Save f ∗ and split threshold to tree node;

end
Save subtrees to tree node;
return tree node;

end
Append current tree to forest;
return Partial Federated Forest Model on Client i;

32
33 end

Proof of Sl ⊆ Sl

1 ∩ Sl

2 ∩ · · · ∩ Sl

M :

For any sample h in the leaf l of the complete tree T , h ∈ Sl. w∗ denotes its decision making
path from root to leaf node. For model Ti on each client i, if the model stores split information at
the current node, it is determined according to the threshold whether this sample enters the left or
right subtree. If the current model does not store split information at this node, the sample enters
left and right subtrees simultaneously. Therefore for sample h, its decision making path w∗ on
the complete tree T must be subset of its decision making path Wi on any client i. Then we have
w∗ ⊆ Wi, 1 ≤ i ≤ M , which is equivalent to h ∈ Sl
i, 1 ≤ i ≤ M . Because of this we can safely say
that h ∈ Sl

M for any h in Sl. Then we can prove that Sl ⊆ Sl

2 ∩ · · · ∩ Sl

1 ∩ Sl

M .

1 ∩ Sl
Proof of Sl ⊇ Sl

2 ∩ · · · ∩ Sl
1 ∩ Sl

2 ∩ · · · ∩ Sl

M :

Assume that sample h doesn’t belong to leaf node l but belongs to g in complete model T , which is
h /∈ Sl and h ∈ Sg. Besides, we assume h ∈ Sl
1 ∩ Sl
=⇒ h ∈ Sg
2 ∩ · · · ∩ Sg

M , obtained by the above proof.

2 ∩ · · · ∩ Sl

1 ∩ Sg

M .

12

ALGORITHM 6: Federated Forest – Master
Input

:Indices of D;
Encoded features F = F1 ∪ F2 ∪ · · · ∪ FM ;
Encrypted label y;

Output :Complete Federated Forest Model

1 /*Build trees for forest recurrently*/
2 while tree_build is True do

Broadcast randomly selected samples D
Randomly select features F
Function TreeBuild (D

;
i from Fi and send to client i;
, F

, y)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

Create empty tree node;
if the pre-pruning condition is satisﬁed then

Mark current node as leaf node;
Assign leaf label by averaging;
return leaf node;

i=1 and related information from all clients;
i=1) and notify client j;

end
Receive encrypted {p}M
Take j = argmax({p}M
Receive split indices from client j and broadcast;
lef t, F
left_subtree ← TreeBuild (D
right, F
right_subtree ← TreeBuild (D
Save subtrees and split info to tree node;
return tree node;

, ylef t);

, yright);

(cid:48)

(cid:48)

(cid:48)

(cid:48)

end
Append current tree to forest;
return Complete Federated Forest Model;

21
22 end

ALGORITHM 7: Federated Forest Prediction – Client
Input

:Partial federated forest model saved on ith client;
Encoded features Fi on ith client;
Test set Dtest

i

on ith client;
i of leaf l on Ti, l ∈ L

Output :Samples IDs Sl

1 while TreePrediction is True do
2

Function TreePredict (Ti, Dtest

i

, Fi)

if is_leaf is True then

Return sample IDs Sl

i and leaf label;

else

if Ti keeps the split info of current node then

Split samples into subtrees based on threshold;
left_subtree ← TreePredict (Ti_lef t, Fi, Dtest
i_lef t);
right_subtree ← TreePredict (Ti_right, Fi, Dtest

i_right);

else

left_subtree ← TreePredict (Ti_lef t, Fi, Dtest
);
right_subtree ← TreePredict (Ti_right, Fi, Dtest

i

);

i

end
Return left and right subtrees;

end
Send Si = {S1

i , S2

i , · · · , Sl

i, · · · } to master;

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

end
return;

18
19 end

13

ALGORITHM 8: Federated Forest Prediction – Master
Input
Output : Prediction of Random Forest

:Sample IDs S of test set Dtest;

1 while TreePrediction is True do
2

Gather {S1, S2, · · · , Si, · · · };
Obtain {S1, S2, · · · , Sl, · · · }, where Sl = Sl
Return label of leaf l for samples in Sl, l ∈ L;

3

4
5 end
6 Calculate forest predictions by averaging the results of trees;
7 return Final Predictions;

1 ∩ Sl

2 ∩ · · · ∩ Sl

M ;

Table 3: Notations

Notation Description

M
Di
N
D
Fi
F
y
Ti
T
L
l, g
O
S
Sl
i
Sl
h
H
Wi
w∗
k

number of regional domains
data set held by client i
total number of samples in training
entire data set D = {D1, D2, · · · , DM }
feature space of Di
entire feature space of D, F = F1 ∪ F2 ∪ · · · ∪ FM
labels
partial decision/regression tree stored on ith client
complete tree T = T1 ∪ T2 ∪ · · · ∪ TM
leaf nodes set of the entire tree
leaf node of the current tree, l, g ∈ L
lowest common ancestor of l, g in T
the sample IDs of entire data set D
the sample IDs which fall into leaf l of tree Ti
the sample IDs which fall into leaf l of complete tree T
single test sample
entire test sample set
the set of decision making paths of sample h on Ti
decision making path of sample h on T
maxmium tree depth

=⇒ h ∈ (Sg
=⇒ h ∈ (Sg

1 ∩ Sg
1 ∩ Sl

2 ∩ · · · ∩ Sg
1) ∩ (Sg
2 ∩ Sl

M ) ∩ (Sl
1 ∩ Sl
2) ∩ · · · ∩ (Sg

2 ∩ · · · ∩ Sl
M ∩ Sl
M )

M )

That is to say, sample h will fall into the leaf node g and l at the same time in every model stored on
client.
∵ In the same binary tree structure, the path from a child node to the root node is ﬁxed and unique.
Under the complete tree structure, the path set of the leaf node g and l up to the root node is wl ∪ wj.
And the lowest common ancestor node exists and is uniquely set to O.
So (wl ∪ wj) ⊆ Wi =⇒ (wl ∪ wj) ∈ (W1 ∩ W2 ∩ · · · ∩ WM )
So no platform stores the information of the node O.

=⇒ T (cid:54)= T1 ∪ T2 ∪ · · · ∪ TM

This contradicts to T = T1 ∪ T2 ∪ · · · ∪ TM .

Therefor the hypothesis doesn’t hold.
=⇒ h /∈ Sl =⇒ h /∈ Sl

2 ∩ · · · ∩ Sl
M

=⇒ Sl ⊇ Sl

1 ∩ Sl

1 ∩ Sl
2 ∩ · · · ∩ Sl
M

14

In summary, we can prove Sl = Sl

1 ∩ Sl

2 ∩ · · · ∩ Sl

M .

Communication Complexity Analysis

Here we give a brief analysis on communication complexity. There are mainly three types of
communication during the training, where M is the number of regional domains:

• Send and receive. Master sends randomly selected features to each client in every turn for tree
building and the client who saves the global optimal feature sends the sample split indices of this
feature to master when building the node. The communication complexity is O(1).

• Broadcast. Master broadcasts sample indices for each tree node construction. The communication

complexity is O(M ).

• Gather. Master gathers and compares the impurity improvement of features at every turn for node
building. It also gathers sample sets of all leaves on each tree stored by clients in the prediction
process. The communication complexity is O(M ).

Since the maximum depth is k, in a tree, there are at most 2k−1 − 1 intermediate nodes and 2k−1
leaf nodes. Take the process of building a tree for example, the communication complexity of the
whole system in training phase is O(2k(M + 1)). For the prediction phase, if not optimized, the
communication complexity is O(2k−1M ), otherwise, the optimized communication complexity is
O(M ).

15

