Mutually exciting point process graphs
for modelling dynamic networks

Francesco Sanna Passino and Nicholas A. Heard

Department of Mathematics, Imperial College London
180 Queen’s Gate, SW7 2AZ, London

Abstract

A new class of models for dynamic networks is proposed, called mutually exciting point process
graphs (MEG). MEG is a scalable network-wide statistical model for point processes with dyadic marks,
which can be used for anomaly detection when assessing the signiﬁcance of future events, including
previously unobserved connections between nodes. The model combines mutually exciting point pro-
cesses to estimate dependencies between events and latent space models to infer relationships between
the nodes. The intensity functions for each network edge are characterised exclusively by node-speciﬁc
parameters, which allows information to be shared across the network. This construction enables estima-
tion of intensities even for unobserved edges, which is particularly important in real world applications,
such as computer networks arising in cyber-security. A recursive form of the log-likelihood function for
MEG is obtained, which is used to derive fast inferential procedures via modern gradient ascent algo-
rithms. An alternative EM algorithm is also derived. The model and algorithms are tested on simulated
graphs and real world datasets, demonstrating excellent performance.

Keywords — dynamic network, Hawkes process, self-exciting process, statistical cyber-security.

1 Introduction

Dynamic networks are encountered in many domains, representing, for example, interactions in social net-
works, messaging applications, or computer networks. Event data from dynamic networks are observed
. . . are event times and the dyadic marks
t1
as triplets (t1, x1, y1), . . . , (tm, xm, ym), where 0
(xk, yk) denote the source and destination nodes, each belonging to a set of nodes V =
of
size n. The sequence of graph edges (x1, y1), . . . , (xm, ym) induces a directed network adjacency matrix
n×n where Aij = 1 if node i connected to node j at least once during the entire observa-
A =
}
tion period, and Aij = 0 otherwise. This article presents a new class of models for the arrival of connection
events between nodes in a network, called mutually exciting graphs (MEG). The MEG model builds upon
mutually exciting point processes and latent space models.

1, . . . , n
{

} ∈ {

0, 1

Aij

t2

≤

≤

≤

}

{

1
2
0
2
c
e
D
2
2

]
I
S
.
s
c
[

3
v
7
2
5
6
0
.
2
0
1
2
:
v
i
X
r
a

Mutually exciting point processes have been already successfully used for a variety of different appli-
cations: modelling of earthquakes (Ogata, 1988), ﬁnancial markets (Bowsher, 2007), criminal activities
(Mohler et al., 2011; Stomakhin et al., 2011), and popularity of tweets (Zhao et al., 2015; Chen and Tan,
2018). Let t1, t2, . . . , tm denote an increasing sequence of observed event times, and N (t) = (cid:80)m
1[0,t](tk)
the corresponding counting process, representing the number of events observed up to time t. A counting
process can be characterised by its conditional intensity function λ(t) = limδ→0 E[N (t + δ)
t]/δ,
t of the process up to time t. For
representing the expected rate of event times conditioned on the history
self-exciting processes, the conditional intensity λ(t) is assumed to depend on the last r observed arrival
times:

N (t)

|H

k=1

H

−

λ(t) = λ +

ω(t

tk),

−

(1.1)

N (t)
(cid:88)

k>N (t)−r

1

 
 
 
 
 
 
Mutually exciting point process graphs for modelling dynamic networks

≥

R+ is a baseline intensity level and ω(
·

) is usually chosen to be a scaled exponential function: ω(t) = β exp

0 and θ > 0. Usually, β is referred to as jump and β + θ as decay rate. Alternative choices of ω(

) is a non-increasing and non-negative excitation function.
where λ
∈
For simplicity, ω(
, where
}
·
β
)
·
are nonparametric step functions (Price-Williams and Heard, 2020), or the power-law ω(t) = θ(t + γ)−1−δ,
0, β, δ > 0 and θ < δβδ (Ozaki, 1979). In the literature, two extreme cases for the intensity in
where θ
(1.1) are usually considered: r = 1, corresponding to a ﬁrst order Markov-like structure, and r =
, called
a Hawkes process (Hawkes, 1971). Intuitively, if r = 1, the intensity only depends on the time elapsed
since the last event. On the other hand, if r =
, the conditional intensity depends on all observed events,
downweighted according to the elapsed time. If r = 0, the model reduces to a simple Poisson process, such
that all inter-arrival times are independent and exponentially distributed with rate λ.

(β + θ)t

{−

∞

∞

≥

O

O{

nnz(A)
}

(n2) parameters, or

In large graphs, simultaneously modelling all the edge processes using individual intensities of the form
(1.1) is computationally challenging, and ignores possible correlations between different edges and nodes.
Inference would require estimating
parameters if the graph is sparse,
where nnz(
) denotes the number of non-zero entries in a matrix. This is not feasible in most real-world
·
applications. Furthermore, this approach would not parameterise new edges appearing after the model train-
ing period. Hence, traditional statistical models for networks, for example latent space models (Hoff et al.,
(n) parameters. Inspired by the literature on
2002), aim to reduce the representation of the network to
latent space models for network adjacency matrices, here a dynamic graph is modelled through the edge-
speciﬁc point processes with intensity functions parametrised by node-speciﬁc latent features. In standard
latent space network models, the probability of a link between two nodes is expressed as a function of node-
Rd, such that P(Aij = 1) = f (ai, bj), for some kernel function f . In this
speciﬁc latent vectors ai, bj
work, it is assumed that the arrival times on each observed network edge can be modelled using a mutually
exciting point process depending on node-speciﬁc characteristics.

O

∈

The related literature on mutually exciting point processes is vast, although mostly focusing on univari-
ate and multivariate point processes; limited attention is devoted to using such processes for modelling large
dynamic graphs. Hawkes processes are traditionally used to estimate causal interactions within multivari-
ate processes (Linderman and Adams, 2014), because of their appealing theoretical properties in terms of
Granger causality and directed information (Etesami et al., 2016; Eichler et al., 2017). Hawkes processes
have also been used in Fox et al. (2016) to analyse e-mail networks, primarily focusing on point processes on
each node. Blundell et al. (2012) proposed Hawkes processes to model reciprocating relationships between
graph communities. Miscouridou et al. (2018) extend this approach, proposing Hawkes process models for
temporal interaction data with reciprocation, using compound completely random measures. The approach
proposed in this paper is also related to Perry and Wolfe (2013), who consider directed interactions within
dynamic networks as a multivariate point process using a Cox multiplicative intensity model, with covariates
depending on the history of the process. The MEG model proposed in this work is different from alterna-
tive methodologies proposed in the literature, since it uses mutually exciting processes at the edge level,
parametrised only by node-speciﬁc features.

Furthermore, dynamic models for network snapshots observed at discrete points in time have also been
proposed in the literature. In particular, the methodology proposed in this work could be related to dynamic
latent space models, which are based on latent feature representations of each node, evolving according to
a temporal dynamics. Examples are Sarkar and Moore (2006); Krivitsky and Handcock (2014); Sewell and
Chen (2015); Durante and Dunson (2016); Lee et al. (2021). The MEG model proposed in this article extends
the latent feature framework to a continuous time setting, using node-speciﬁc latent vectors to parametrise
point processes on each edge.

The remainder of this article is structured as follows: Section 2 introduces the MEG model, followed
by a description of the related inferential procedures in Section 3. Section 4 discusses simulation from the
model and the calculation of p-values for each network event. Results on simulated and real-world computer
networks are discussed in Section 5.

Sanna Passino, F. and Heard N. A.

2

Mutually exciting point process graphs for modelling dynamic networks

2 Mutually exciting point process graphs

The main contribution proposed in this article is a mutually exciting graph model (MEG) for dynamic net-
work point processes, deﬁned by an n
.
}
×
Each entry λij(t) is the conditional intensity of the counting process Nij(t) = (cid:80)m
1[0,t]×{i}×{j}(tk, xk, yk)
of events occurring on the graph edge (i, j), such that λij(t) = limδ→0 E[Nij(t + δ)
t]/δ. For
|H
generality, it is assumed that for each edge (i, j) there exists a changepoint τij
0 after which the edge
becomes observable. In the simplest case, τij = 0 for all i and j.

n time-varying matrix of non-negative functions λ(t) =

λij(t)
{

Nij(t)

k=1

≥

−

To parameterise λ(t), each entry is represented as an additive model with three non-negative components.
The ﬁrst, denoted αi(t), characterises the process of arrival times involving i as source node; the second,
βj(t), corresponds to arrivals for which j is the destination node; the third, γij(t), is an interaction term
which will also be parameterised by node-speciﬁc parameters, giving:

λij(t) = αi(t) + βj(t) + γij(t),

t

τij.

≥

(2.1)

Note that the intensity function (2.1) resembles the link function used in additive and multiplicative effect
network models for network adjacency matrices, proposed in Hoff (2021).
Deﬁne the source and destination counting processes as Ni(t) = (cid:80)m
1[0,t]×{j}(tk, yk). Furthermore, let (cid:96)i1, (cid:96)i2, . . . denote the indices
k=1

j(t) =
(cid:80)m
of the arrival times
such that i appears as source node, and (cid:96)(cid:48)
for which j is
the destination node. To allow self excitation of both source and destination nodes, the latent functions αi(t)
and βj(t) are assigned a similar form to the conditional intensity (1.1):

j2, . . . denote the event indices

1[0,t]×{i}(tk, xk) and N (cid:48)

k : xk = i
}
{

k : yk = j
{

j1, (cid:96)(cid:48)

k=1

}

αi(t) = αi +

Ni(t)
(cid:88)

k>Ni(t)−r

ωi(t

−

t(cid:96)ik ),

βj(t) = βj +

N (cid:48)
j (t)
(cid:88)

k>N (cid:48)

j (t)−r

ω(cid:48)
j(t

t(cid:96)(cid:48)

jk

),

−

(2.2)

where α = (α1, . . . , αn), β = (β1, . . . , βn)
i are
node-speciﬁc, non-increasing excitation functions from R+ to R+. For simplicity, the excitation functions
assume the following scaled exponential form, for non-negative parameters µi, µ(cid:48)

+ are node-speciﬁc baseline intensity levels, and ωi, ω(cid:48)

∈

Rn

j, φi, φ(cid:48)

Rn
+:

j ∈

ωi(t) = µi exp

{−

(µi + φi)t
}

, ω(cid:48)

j(t) = µ(cid:48)

j exp

{−

(µ(cid:48)

j + φ(cid:48)

j)t
}

.

(2.3)

Scaled exponential excitation functions have signiﬁcant computational advantages for inference in MEG
models: in particular, the log-likelihood can be expressed in recursive form and evaluated in linear time,
which speeds up inference and allows the methodology to scale to large graphs. These aspects will be more
extensively discussed in Section 2.1.

Similarly, let (cid:96)ij1, (cid:96)ij2, . . . be the indices

of the events observed on the edge (i, j).
k : xk = i, yk = j
{
The interaction term γij(t) in (2.1) assumes a similar form to (2.2), but with a background rate obtained as
N:
the inner product between two node-speciﬁc d-dimensional baseline parameter vectors γi, γ(cid:48)

Rd

}

j ∈

+, d

∈

γij(t) = γ

(cid:124)
i γ(cid:48)

j +

Nij (t)
(cid:88)

k>Nij (t)−r

ωij(t

t(cid:96)ijk ).

−

(2.4)

The excitation function ωij(t) is also expressed as a sum of scaled exponential functions, parameterised by
four node-speciﬁc, non-negative latent d-vectors νi, ν(cid:48)

j, θi, θ(cid:48)

Rd
+:

j ∈

ωij(t) =

d
(cid:88)

(cid:96)=1

νi(cid:96)ν(cid:48)

j(cid:96) exp

{−

(θi(cid:96) + νi(cid:96))(θ(cid:48)

j(cid:96) + ν(cid:48)

j(cid:96))t
}

.

(2.5)

The inner product baseline and products within the scaled exponential excitation functions are inspired by
random dot product graph models (see, for example, Athreya et al., 2018) for link probabilities. This choice

Sanna Passino, F. and Heard N. A.

3

Mutually exciting point process graphs for modelling dynamic networks

(a) αi(t)

(b) βi(t)

(c) γij(t)

(d) λij(t)

Figure 1: Cartoon of a 1-dimensional MEG model (2.1)–(2.5) for r = 1 and r =

. Event times are marked on
the x-axis. Events with source node i and destination node j are denoted by triangles ((cid:78)); other events
with source node i are are denoted with circles (
), and other events with destination node j are denoted by
•
squares ((cid:4)). For each event time, a jump in the corresponding intensities is observed.

∞

is helpful to obtain closed form expression for inference in MEG models, as discussed in Section 3.1. Alter-
native options, inspired by other latent space models, could also be used for the baseline, such as
(Hoff et al., 2002).

γ(cid:48)
2
j(cid:107)

γi

−

(cid:107)

∞

j for r =

j + νiν(cid:48)

j = 0.3, θ(cid:48)

j = 0.6, ν(cid:48)

j = 0.8, φ(cid:48)

A cartoon example of the intensity λij(t) for the d = 1 dimensional MEG model with scaled exponential
functions is given in Figure 1, with αi = 0.2, µi = 0.5, φi = 0.5, βj = 0.1, µ(cid:48)
j = 0.2, γi =
0.8, νi = 0.9, θi = 1.1, γ(cid:48)
j = 0.2. In Figure 1d, the edge intensity function jumps at
each event time involving source node i or destination node j, or both. In particular, larger jumps in λij(t),
of size µi + µ(cid:48)
, are observed when events are observed on the edge (i, j) (triangles). The
intensity also increases if events are observed from source node i (circles, cf. Figure 1a) or to destination
node j (squares, cf. Figure 1b), with jumps of size µi and µ(cid:48)
. For r = 1, the intensity
λij(t) is bounded by construction at αi + βj + γiγ(cid:48)

j + µi + µ(cid:48)
A key feature of the model is the representation of the intensity (2.1) with only node-speciﬁc parame-
ters Ψ = (α, µ, φ, β, µ(cid:48), φ(cid:48), γ, ν, θ, γ(cid:48), ν(cid:48), θ(cid:48)). This construction allows estimation of intensities even for
unobserved edges. Therefore, in practical applications, when a new link is observed, it is possible to imme-
diately provide an estimate of the intensity of the process on that edge. This is a substantial difference with
respect to models based on edge-speciﬁc parameters for the edge intensities. Such models do not perform
well for scoring new links, since the score for a new observation could only be based on a prior guess of the
intensity, whereas MEG “borrows strength” from events observed on similar nodes and edges in the graph,
providing an informed estimate of the intensity function on the newly observed edge.
T =

j respectively for r =
j + νiν(cid:48)
j.

For a sequence of observed events

, with event times in [0, T ], the

∞

H

(x1, y1, t1), . . . , (xm, ym, tm)
}
{

Sanna Passino, F. and Heard N. A.

4

024681000.51tαi(t)r=∞r=1024681000.511.52tβj(t)r=∞r=1024681000.20.40.60.8tγij(t)r=∞r=1024681001234tλij(t)r=∞r=1Mutually exciting point process graphs for modelling dynamic networks

log-likelihood (Daley and Vere-Jones, 2002) of a generic MEG model is:

log L(

H

T ; Ψ) =

n
(cid:88)

n
(cid:88)

(cid:40) nij
(cid:88)

i=1

j=1

k=1

log λij(t(cid:96)ijk )

(cid:41)

λij(t)dt

.

(cid:90) T

−

τij

(2.6)

where nij is the number of events observed on edge (i, j). Explicit forms of the likelihood function (2.6)
with d = 1 and r = 1 or r =

, are presented in the Appendix.

∞

2.1 Computational issues with the calculation of the likelihood

∞

For r =
, the main computational burden associated with the calculation of the log-likelihood (2.6) is
the double summation over each (i, j) pair of the sum of intensities λij(t(cid:96)ijk ) for the events t(cid:96)ijk , and the
summations required in (2.2) and (2.4) to evaluate that intensity for each event. This section discusses a
recursive form of the log-likelihood (2.6) for the MEG model with r =
, which can be evaluated in linear
time on each active edge, signiﬁcantly reducing the computational requirements. This is a signiﬁcant com-
putational advantage of scaled exponential excitation functions, which makes them particularly appealing for
practical applications. Assume sequences of arrival times ti1 <
< tiNi(T ) involving i as source node, and
t(cid:48)
j1 <
j (T ) such that j is the destination of the connection. Within each pair of sequences, assume
Ni(T ), N (cid:48)
min
that a subset of nij
events is observed on the edge (i, j), and denote the indices of
{
such events as uij,1, . . . , uij,nij and u(cid:48)
ij,nij . The terms in the ﬁrst summation in the log-likelihood
(2.6) can then be written as:

j(T )
}
ij,1, . . . , u(cid:48)

< t(cid:48)

· · ·

· · ·

jN (cid:48)

∞

≤

log λij(t(cid:96)ijk ) = log

αi + µi

(cid:26)

uij,k−1
(cid:88)

e

−(µi+φi)(t(cid:96)ijk

−tih)

+ βj + µ(cid:48)
j

u(cid:48)
ij,k−1
(cid:88)

h=1

−(µ(cid:48)
e

j +φ(cid:48)

j )(t(cid:96)ijk

−t(cid:48)

jh)

h=1
d
(cid:88)

(cid:124)
i γ(cid:48)

j +

q=1

+ γ

νiqν(cid:48)
jq

k−1
(cid:88)

h=1

e

−(νiq+θiq)(θ(cid:48)

jq+ν(cid:48)

jq)(t(cid:96)ijk

−t(cid:96)ijh

)

(cid:27)
.

(2.7)

Using a technique similar to the method proposed in Ogata (1978), it is possible to calculate (2.7) in linear
ij(k)
time using a recursive formulation of the inner summations. For k
and ˜ψijq(k) as follows:

, deﬁne ψij(k), ψ(cid:48)
}

1, 2, . . . , nij

∈ {

ψij(k) =

uij,k−1
(cid:88)

h=1

−(µi+φi)(t(cid:96)ijk
e

−tih)

,

ψ(cid:48)

ij(k) =

u(cid:48)
ij,k−1
(cid:88)

e

h=1

−(µ(cid:48)

j +φ(cid:48)

j )(t(cid:96)ijk

−t(cid:48)

jh)

,

˜ψijq(k) =

k−1
(cid:88)

h=1

−(νiq+θiq)(ν(cid:48)
e

jq+θ(cid:48)

jq)(t(cid:96)ijk

−t(cid:96)ijh

)

, q = 1, . . . , d.

(2.8)

Using (2.7) and (2.8), the ﬁrst term of the log-likelihood (2.6) becomes:

nij
(cid:88)

k=1

log λij(t(cid:96)k ) =

nij
(cid:88)

k=1

log






αi + βj + γiγ(cid:48)

j + µiψij(k) + µ(cid:48)

jψ(cid:48)

ij(k) +

d
(cid:88)

q=1

νiqν(cid:48)
jq

˜ψijq(k)






.

(2.9)

The expression can be evaluated in linear time using the recursive equations for ψij(k), ψ(cid:48)
presented in the following proposition, proved in the Appendix.

ij(k) and ˜ψijq(k)

Proposition 1. The terms ψij(k), ψ(cid:48)

ij(k) and ˜ψij(k) can be written recursively as follows:

ψij(k) = e

−(µi+φi)(t(cid:96)ijk

−t(cid:96)ij,k−1

)

[1 + ψij(k

1)] +

−

uij,k−1
(cid:88)

h=uij,k−1+1

−(µi+φi)(t(cid:96)ijk

−tih)

,

e

Sanna Passino, F. and Heard N. A.

5

Mutually exciting point process graphs for modelling dynamic networks

ψ(cid:48)

ij(k) = e

−(µ(cid:48)

j +φ(cid:48)

j )(t(cid:48)
(cid:96)(cid:48)
k

−t(cid:48)
(cid:96)(cid:48)
k−1

) (cid:2)1 + ψ(cid:48)

ij(k

1)(cid:3) +

−

˜ψijq(k) = e

−(νiq+θiq)(ν(cid:48)

jq+θ(cid:48)

jq)(t(cid:96)ijk

−t(cid:96)ij,k−1

1 + ˜ψijq(k

) (cid:104)

h=u(cid:48)

ij,k−1+1
(cid:105)
1)

.

−

u(cid:48)
ij,k−1
(cid:88)

−(µ(cid:48)
e

j +φ(cid:48)

j )(t(cid:96)ijk

−t(cid:48)

jh)

,

2.2 Extension to undirected and bipartite graphs

The proposed modelling framework has been presented for directed graphs, but could be easily extended to
undirected and bipartite networks. For undirected graphs A = A(cid:124)
, hence there is no distinction between
source and destination nodes. Therefore, βj(t) in (2.1) could be simply be replaced by αj(t), and γij(t)
modiﬁed as follows:

γij(t) = γ

(cid:124)
i γj +

Nij (t)
(cid:88)

d
(cid:88)

k>Nij (t)−r

(cid:96)=1

νi(cid:96)νj(cid:96) exp

(θi(cid:96) + νi(cid:96))(θj(cid:96) + νj(cid:96))t
}

.

{−

Furthermore, bipartite graphs can be considered as special cases of directed graphs, where the node set
V2 = ∅, and all
V = V1
the edges are of the form (i, j) with i
V2. Therefore, the intensity function (2.1) and the
V1 and j
corresponding components (2.2) and (2.4) still hold.

V2 is divided into two sets V1 and V2 of cardinality n1 and n2, such that V1

∪

∩

∈

∈

3 Inference via maximum likelihood estimation

Inference in Hawkes processes is usually carried out using maximum likelihood estimation (MLE) via the
EM algorithm or gradient ascent methods, since it is not possible to optimise the likelihood analytically.
Similar issues arise for the log-likelihood (2.6) for MEG models. Only a small subset of the parameters has
a closed-form solution for the MLE: the start times τij. If τij in (2.6) is unknown, the maximum likelihood
estimates is simply ˆτij = t(cid:96)ij1 if at least one event is observed on the edge, and ˆτij =
otherwise.
Intuitively, this is reasonable: the best guess about the start time of activity on an edge simply corresponds to
the ﬁrst observation on that edge. A formal proof of the result is provided in the Appendix. For maximising
(2.6) with respect to the remaining parameters Ψ, two strategies are deployed: the Expectation-Maximisation
algorithm (EM, Dempster et al., 1977), and the adaptive moment estimation method (Adam, Kingma and Ba,
2015).

∞

∈

→

∞

∞

α1, . . . , αn

[0, min
{

, an inﬁnitely fast decaying rate φi

Note that issues with MLE might arise when the parameters lie at the boundaries of the parameter space.
For example, for a non-negative ﬁnite jump 0 < µi <
would
→ ∞
make the resulting process a simple Poisson process for αi(t) in (2). Similarly, if 0 < φi <
, a jump size
µi
0 would make αi(t) again correspond to a Poisson process with rate αi. Similar considerations can
be made about the parameters of the excitation functions of the remaining components in (2). Additionally,
identiﬁability issues are observed if further constraints are not imposed on the parameters: for example,
subtracting a constant c
) for all αi, and adding the same constant to all βj, returns
the same log-likelihood function (2.6). For identiﬁability, at least one value of αi or βj must be kept ﬁxed.
Identiﬁability issues also arise from the interaction term: the inner product γ
j is invariant to orthogonal
R+,
transformations of γi and γ(cid:48)
the interaction parameters νi, θi, ν(cid:48)
j/c.
This leads to highly multimodal log-likelihood functions, and to a non-unique MLE. This problem is in-
consequential for prediction, since the predictive distribution of new events depends upon a function of the
parameters which is identiﬁable. Similarly, for assessing robustness of parameter estimation procedures,
identiﬁable transformations of the parameters can be considered: for example, the sums αi + βj in the main
effects model are identiﬁable, or the products (νi + θi)(ν(cid:48)
j) for d = 1. An example will be given in
Section 5.1.

j preserving non-negativity of the vectors. Similarly, for a constant c

j produce the same excitation function as cνi, cθi, ν(cid:48)

j/c and θ(cid:48)

j and θ(cid:48)

j + θ(cid:48)

(cid:124)
i γ(cid:48)

∈

}

Sanna Passino, F. and Heard N. A.

6

Mutually exciting point process graphs for modelling dynamic networks

3.1

Inference via the EM algorithm

An EM algorithm can be conveniently implemented following a network-wide extension of the procedure
of Fox et al. (2016), after adopting a simple reparametrisation of the log-likelihood (2.6). In particular, the
scaled exponential decay rates µi + φi, µ(cid:48)
j, νiq + θiq and ν(cid:48)
jq,
where ˜φi > µi, ˜φ(cid:48)
j, ˜θiq > νi, and ˜θ(cid:48)
j > µ(cid:48)
j. Similarly, the jumps µi, µ(cid:48)
jq are expressed as the
product between the decay rates ˜φi, ˜φ(cid:48)
jq and the ratios between the jump and decay rates, denoted:

j + φ(cid:48)
jq > ν(cid:48)
j, ˜θiq and ˜θ(cid:48)

jq are rewritten as ˜φi, ˜φ(cid:48)

j, ˜θiq and ˜θ(cid:48)

j, νiq and ν(cid:48)

jq + θ(cid:48)

˜µi =

µi
µi + φi

,

˜µ(cid:48)
j =

µ(cid:48)
j
j + φ(cid:48)
µ(cid:48)
j

,

˜νiq =

νiq
νiq + θiq

,

˜ν(cid:48)
jq =

ν(cid:48)
jq
jq + θ(cid:48)
ν(cid:48)
jq

,

{−

= ˜µi ˜φi exp

(µi + φi)t
}

where such parameters lie in [0, 1]. For example, under the two equivalent parametrisations, ωi(t) =
µi exp
. The vector of all parameters can then be equivalently rewrit-
ten as ˜Ψ = (α, ˜µ, ˜φ, β, ˜µ(cid:48), ˜φ(cid:48), γ, ˜ν, ˜θ, γ(cid:48), ˜ν(cid:48), ˜θ(cid:48)), using the updated notation. Furthermore, consider the
sequence of arrival times ti1 <
j (T ) such that
< tijNij (T ) denote the events on
j is the destination of the connection. Similarly, let the sequence tij1 <
the edge (i, j). Using this revised notation, the conditional intensity function (2.1) for an edge, for t
τij,
is:

{−
< tiNi(T ) involving i as source node, and t(cid:48)

˜φit
}

j1 <

< t(cid:48)

· · ·

· · ·

· · ·

jN (cid:48)

≥

λij(t) = αi +

Ni(t)
(cid:88)

k>Ni(t)−r

ωi(t

−

tik) + βj +

N (cid:48)
j (t)
(cid:88)

k>N (cid:48)

j (t)−r

ω(cid:48)
j(t

−

t(cid:48)
jk) +

d
(cid:88)

q=1

γiqγ(cid:48)

jq +

Nij (t)
(cid:88)

d
(cid:88)

k>Nij (t)−r

q=1

ωijq(t

tijk),

−

{

}

∞

→

jqt

r, N (cid:48)

+min

j(t)
}

r, Ni(t)
}
{

(3.1)
R+,
) in (2.4) has been expressed as a sum of d functions ωijq : R+
where the excitation function ωij(
·
˜θiq ˜θ(cid:48)
˜θ(cid:48)
where ωijq(t) = ˜νiq ˜θiq ˜ν(cid:48)
from (2.5). Therefore, conditional on t, the subsequent event
jq exp
jq
{−
+
on the edge (i, j) could be interpreted as the offspring of one of the 2+d+min
r, Nij(t)
d min
components of the intensity (3.1), each corresponding to a non-homogeneous Poisson pro-
}
{
cess in (t,
). In other words, λij(t) is written as a superimposition of conditional intensities of different
processes, where the event allocations are missing data, giving a branching structure to the event hierarchy.
For missing data problems, the traditional approach in statistics is to deploy the EM algorithm, which
in this setting requires to introduce latent binary variables to reconstruct the branching structure. For events
generated from the background rates αi, βj and γiqγ(cid:48)
jq, q = 1, . . . , d (also known as immigrant events in the
literature), the corresponding latent variables are denoted by the letter b. In particular b
equals
1 if tij(cid:96) is a background event obtained from the Poisson process with rate αi, and 0 otherwise. Similarly,
(β)
(γ)
b
ij(cid:96)q denote whether the event tij(cid:96) is a background event from Poisson processes with rates βj and
ij(cid:96) and b
γiqγ(cid:48)
jq respectively. On the other hand, for the events that are not generated from the background rates, the
(α)
ij(cid:96)k = 1 if tij(cid:96) is offspring of the
corresponding latent variables are denoted with the letter z. In particular, z
(β)
k-th event such that node i is source, and 0 otherwise; a similar reasoning applies for z
ij(cid:96)k, which instead
considers the sequence of events such that node j is destination. As before, it is necessary to introduce a
(γ)
further subscript for the interaction term: z
ij(cid:96)kq = 1 if tij(cid:96) is offspring of the k-th event on the edge (i, j),
from the q-th additive component of the intensity. If such latent variables are known, it is possible to write
in simple form the complete data log-likelihood, which also includes the information about the branching
structure:

(α)
ij(cid:96) ∈ {

0, 1
}

log L(

H

T ; ˜Ψ, B, Z) =

n
(cid:88)

n
(cid:88)

(cid:40) nij
(cid:88)

(cid:34)
(α)
ij(cid:96) log(αi) +
b

Ni(tij(cid:96))
(cid:88)

i=1

j=1

(cid:96)=1

k>Ni(tij(cid:96))−r

z

(α)

ij(cid:96)k[log(˜µi ˜φi)

˜φi(tij(cid:96)

−

tik)]

−

+ b

(β)
ij(cid:96) log(βi) +

N (cid:48)

j (tij(cid:96))
(cid:88)

z

k>N (cid:48)

j (tij(cid:96))−r

(β)

ij(cid:96)k[log(˜µ(cid:48)

j

˜φ(cid:48)
j)

−

˜φ(cid:48)
j(tij(cid:96)

−

t(cid:48)
jk)] +

(cid:32)
(γ)
ij(cid:96)q[log(γiq) + log(γ(cid:48)
b

jq)]

d
(cid:88)

q=1

Sanna Passino, F. and Heard N. A.

7

Mutually exciting point process graphs for modelling dynamic networks

Nij (tij(cid:96))
(cid:88)

z

+

k>Nij (tij(cid:96))−r

(γ)

ij(cid:96)kq[log(˜νiq ˜θiq) + log(˜ν(cid:48)

jq

˜θ(cid:48)
jq)

−

˜θiq ˜θ(cid:48)

jq(tij(cid:96)

(cid:33)(cid:35)

(cid:90) T

tijk)]

−

−

τij

(cid:41)
.

λij(t)dt

(3.2)

The E-step of the EM algorithm consists in calculating EB,Z|HT ,Ψ∗
, the expected
}
value of the complete data log-likelihood (3.2) with respect to the distribution of the latent indicators B and
Z, conditional on the observations

log L(
{
T and parameter values ˜Ψ∗. From (3.2), this reduces to calculating:

T ; ˜Ψ, B, Z)

H

H

(·)
· = P
ξ

B,Z|HT , ˜Ψ∗

(cid:110)
(·)
· = 1
b

(cid:12) ˜Ψ∗(cid:111)
(cid:12)

,

(·)
· = P

ζ

B,Z|HT , ˜Ψ∗

(cid:110)
z

(·)
· = 1

(cid:12) ˜Ψ∗(cid:111)
(cid:12)

,

known as responsibilities. Such probabilities are simply represented by the relative contributions of different
components to the conditional intensity (3.1):

ξ

ξ

ξ

(α)
ij(cid:96) ∝
(β)
ij(cid:96) ∝
(γ)
ij(cid:96)q ∝

αi,

βj,

γiqγ(cid:48)

jq,

ζ

(α)
ij(cid:96)k ∝
(β)
ζ
ij(cid:96)k ∝
(γ)
ij(cid:96)kq ∝

ζ

˜µi ˜φi exp
˜φ(cid:48)
˜µ(cid:48)
j exp
j
˜νiq ˜θiq ˜ν(cid:48)
jq

{−

˜φi(tij(cid:96)
˜φ(cid:48)
j(tij(cid:96)

{−
˜θ(cid:48)
jq exp

−

tik)
}
t(cid:48)
jk)
−
˜θiq ˜θ(cid:48)

1(tik,∞)(tij(cid:96)),
1(t(cid:48)
jk,∞)(tij(cid:96)),
}
jq(tij(cid:96)

tijk)
}

−

{−

1(tijk,∞)(tij(cid:96)),

(3.3)

with normalising constant λij(tij(cid:96)), cf. (2) and (3.1), calculated using parameter values ˜Ψ∗.

At the M-step, the expectation E

calculated at the E-step is maximised
with respect to ˜Ψ, and updated parameter estimates are obtained. For most of the parameters in the MEG
model with scaled exponential excitation function, the maxima are analytically available, and their form
depends on the choice of r. For r =

T ; ˜Ψ, B, Z)
}

log L(
{

B,Z|HT , ˜Ψ∗

H

:
∞

ˆ˜µi =

(cid:80)n

j=1

(cid:80)ni

)

}

,

,

ˆαi =

ˆβj =

ˆγiq =

(cid:80)n

j=1

(cid:80)nij

(cid:80)n

j=1(T

(cid:80)n

i=1

(cid:80)n

i=1(T

−
(cid:80)n

(cid:80)n

j=1 γ(cid:48)

j=1
jq(T

−
(cid:80)nij

(α)
ij(cid:96)
T, τij

(cid:96)=1 ξ
min
{
(β)
(cid:96)=1 ξ
ij(cid:96)
T, τij
min
{
(cid:80)nij
(cid:96)=1 ξ
min
{

−

(α)
ij(cid:96)k

j=1

(cid:80)n

(cid:80)nij
(cid:96)=1

(cid:80)Ni(tij(cid:96))
k=1

ζ
k=1[e− ˜φi min{T,max{τij −tik,0}}
(cid:80)nij
(cid:96)=1

(cid:80)N (cid:48)
k=1

j (tij(cid:96))

(cid:80)n

ζ

i=1
j min{T,max{τij −t(cid:48)

jk,0}}

−
(β)
ij(cid:96)k

(cid:80)Nij (tij(cid:96))
k=1

(γ)
ζ
ij(cid:96)kq
e−˜θiq ˜θ(cid:48)
jq(T −tijk)]

−

,

e− ˜φi(T −tik)]

,

,

e− ˜φ(cid:48)

j (T −t(cid:48)

jk)]

(3.4)

)

}
(γ)
ij(cid:96)q
T, τij

ˆ˜µ(cid:48)
j =

ˆ˜νiq =

,

)
}

(cid:80)n

i=1

(cid:80)n(cid:48)

j

k=1[e− ˜φ(cid:48)
(cid:80)nij
(cid:96)=1

(cid:80)n

j=1

(cid:80)n

j=1 ˜ν(cid:48)
jq

(cid:80)nij

k=1[1

−

and similarly for ˆγ(cid:48)
equations can be obtained. For example, again for r =

jq and ˆ˜ν(cid:48)

jq. For the remaining parameters, an exact solution is not available, but recursive

˜φi =

˜θiq =

(cid:80)n

j=1{

(cid:80)nij
(cid:96)=1

(cid:80)Ni(tij(cid:96))
k=1

ζ

(α)
ij(cid:96)k(tij(cid:96)

(cid:80)n

j=1

(cid:80)nij

(cid:96)=1{

(cid:80)Nij (tij(cid:96))
k=1

(γ)
ζ
ij(cid:96)kq

:
∞
(cid:80)Ni(tij(cid:96))
k=1
(cid:80)ni

(cid:80)n

j=1

(cid:80)nij
(cid:96)=1

(α)
ij(cid:96)k

ζ

−
(cid:80)n

tik) + ˜µi
(cid:80)nij
(cid:96)=1

j=1
˜θ(cid:48)
jq(tij(cid:96)

k=1[(T
(cid:80)N (tij(cid:96))
k=1
tijk) + ˜νiq ˜ν(cid:48)
jq

−
(γ)
ij(cid:96)kq
˜θ(cid:48)
jq(T

ζ

−

tik)e− ˜φi(T −tik)

ijke− ˜φiτ +
τ +
ijk ]

−

,

}

tij(cid:96))e−˜θiq ˜θ(cid:48)

jq(T −tij(cid:96))

}

−

,

(3.5)

where τ +
}}
procedure is summarised in Algorithm 1.

T, max
{

ijk = min

tik, 0

τij

−

{

. Similar equations are available for ˜φ(cid:48)

j and ˜θ(cid:48)

jq. The full iterative

3.2

Inference via gradient ascent methods

The EM algorithm proposed in the previous section has appealing statistical properties, but it is not scalable
for large networks or for large numbers of events, since it requires tnij[2 + d + Ni(T ) + N (cid:48)
j(T ) + dnij] ad-
ditional latent variables to be deﬁned for each edge, which is not feasible in most practical applications. On
the other hand, the log-likelihood in (2.6) was shown to have a recursive expression for r =
, which also

∞

Sanna Passino, F. and Heard N. A.

8

Mutually exciting point process graphs for modelling dynamic networks

Algorithm 1: EM algorithm for optimisation of the log-likelihood (2.6).

Input: initial parameter values ˜Ψ0.
Output: model parameters ˜Ψ corresponding to a local maximum of log L(

T ; ˜Ψ).

H

1 for k = 1, 2, . . . do

2

3

(·)
E-step: calculate responsibilities ξ
·
M-step: calculate ˜Ψk+1 = argmax ˜Ψ
T ; ˜Ψ, B, Z)
(3.4) and (3.5) iteratively, using the most recent parameter estimates,

and ζ
E
B,Z|HT , ˜Ψk {

(·)
· using (3.3) with parameters ˜Ψk,

log L(

H

; for r =
}

, apply

∞

4 until convergence in log L(

H

T ; ˜Ψ).

∈

holds for its gradient with respect to the parameters Ψ. Therefore, in order to make the inferential procedure
scalable, gradient-based optimisation methods appear to be suitable. Gradient ascent methods are usually
based on computing the gradient of the log-likelihood function, and iteratively updating the parameter val-
R+, also known as
ues in the direction of steepest ascent given by the gradient, for a given step size η
learning rate. One of the main issues of standard gradient ascent for high-dimensional parameter estimation
is the choice of the learning rate. The adaptive moment estimation method (Adam, Kingma and Ba, 2015)
is a popular gradient ascent optimisation algorithm widely used in the machine learning and deep learning
communities, which adaptively selects and adjusts learning rates for each parameter. Its convergence prop-
erties have been extensively studied (Reddi et al., 2018; Chen et al., 2019; Zou et al., 2019). In Adam, the
step sizes are adjusted via exponentially weighted moving averages (EWMA) of the estimated gradient and
square gradient (respectively denoted m and v, with decay rates ρ1, ρ2
[0, 1]). Such averages provide
estimates for the ﬁrst and second moment of the gradient respectively; these estimates are then corrected for
bias and used to update the parameters in a similar fashion to standard gradient ascent, after adding a small
R+ (usually known as smoothing parameter) to the estimate of the second moment, in order to
offset ε
avoid computational issues when its value vanishes towards zero. Considering the high-dimensional maxi-
mum likelihood estimation of MEG models, Adam appears to be a suitable inferential choice. Alternative
gradient ascent techniques for optimisation are surveyed in Ruder (2016). In this work, Adam is imple-
mented after adopting a simple re-parametrisation and optimising the logarithm of each parameter, since are
all constrained to be positive. The resulting optimisation procedure is detailed in Algorithm 2. The gradient
g = ∂
T ; Ψ) of the likelihood (2.6) with respect to Ψ inherits a recursive form from (2.9), and
therefore it can be calculated in linear time. Explicit forms for d = 1 and r =
are derived in the Appendix.

∂Ψ log L(

H

∈

∈

∞

Algorithm 2: Adam algorithm for optimisation of the log-likelihood (2.6).

Input: step size η

R+, decay rates ρ1, ρ2

∈
parameter values Ψ0.

(0, 1), smoothing parameter ε

∈

R+, initial

∈

Output: model parameters Ψ corresponding to a local maximum of log L(

T ; Ψ).
1 Initialise estimates of the ﬁrst and second moment of the gradient: m0 = 0, v0 = 0,
2 for k = 1, 2, . . . do
3

calculate gradient gk = ∂
update EWMA estimate of ﬁrst moment: mk = ρ1mk−1 + (1
Ψk−1)
update second moment: vk = ρ2vk−1 + (1
(cid:18)(cid:113)

, evaluated at Ψk−1,
ρ1)(gk
(gk

(cid:12)
T ; Ψ)
(cid:12)Ψ=Ψk−1

∂Ψ log L(

×
Ψk−1)],
(cid:19)(cid:27)

H

H

(cid:26)

Ψk−1),

−
ηmt

ρ2)[(gk
(cid:14)(1

×
ρk
1)

−

−
×
vt/(1

×
ρk
2) + ε

−

,

update parameters: Ψk = Ψk−1

exp

×

4

5

6

7 until convergence in log L(

T ; Ψ).

H

Sums, products, quotients, exponentials, and square roots are applied element-wise.

Sanna Passino, F. and Heard N. A.

9

Mutually exciting point process graphs for modelling dynamic networks

4 Simulation and assessment of the goodness-of-ﬁt

In order to validate the inferential procedure, it is necessary to simulate data from the MEG model (2.1),
which can be interpreted as an extended multivariate Hawkes process where some of the parameters are
shared across the individual processes. Therefore, simulating MEG models is possible under the framework
described in Ogata (1981), and follows the standard technique of simulation via thinning. The procedure is
described in Algorithm 3.

Algorithm 3: Simulation of a MEG in [0, T ].
1 set t(cid:63) = 0,
2 repeat
3

4

5

6

i=1

(cid:80)n

j=1 λij(t(cid:63)

+), where t(cid:63)

set λ(cid:63) = (cid:80)n
generate the inter-arrival time q =
obtain the candidate arrival time t(cid:63)
assign the arrival time t(cid:63) to the edge (i, j) with probability λij(t(cid:63)
edge with probability 1

−
←
(cid:80)n
j=1 λij(t(cid:63)

+ denotes the limit from the right,
log(u)/λ(cid:63), where u

−)/λ(cid:63), where t(cid:63)

t(cid:63) + q,

(cid:80)n

i=1

∼

Uniform[0, 1],

7 until t(cid:63) > T ;

−

−)/λ(cid:63), and do not assign to any

− denotes the limit from the left.

Furthermore, it is possible to assess the performance of the inferential procedure by evaluating the
goodness-of-ﬁt from out-of-sample events. If the model parameters are estimated only from the event times
obtained in [0, T (cid:63)], with T (cid:63) < T , using Algorithm 2, the goodness-of-ﬁt can then be evaluated from the event
times in (T (cid:63), T ]. Goodness-of-ﬁt measures can be calculated from functions of the compensator function for
the model. Given the conditional intensity λij(t), the compensator Λij(t) is:

(cid:90) t

Λij(t) =

λij(s)ds.

τij

Examples of compensator functions for some MEG models, for t = T , can be found in the Appendix. Given
arrival times t1, . . . , tnij on the edge (i, j), under the null hypothesis of correct speciﬁcation of the condi-
tional intensity λij(t), by time rescaling theorem (see, for example, Brown et al., 2002) Λij(t1), . . . , Λij(tnij )
are event times of a homogeneous Poisson process with unit rate. It follows that the upper tail p-values

pijk = exp

{−

Λij(tk) + Λij(tk−1)

= exp

}

(cid:40)

(cid:90) tk

−

tk−1

(cid:41)

λij(s) ds

(4.1)

follow a standard uniform distribution under the null hypothesis. Therefore, given the estimates of the
conditional intensity functions obtained from the arrival times in [0, T (cid:63)], approximately uniform p-values for
the test event times in (T (cid:63), T ] should be observed if the model is speciﬁed and estimated correctly.

5 Applications and results

In this section, the MEG model is tested on simulated network data and on two real world computer network
datasets:
the Enron e-mail network, and a bipartite graph obtained from network ﬂow data collected at
Imperial College London. Across the experiments, the decay rates (ρ1, ρ2) in Algorithm 2 have been set to
(0.9, 0.99), and ε = 10−8.

5.1 Simulated events on small fully connected graphs

In order to evaluate Algorithm 1 and 2 and their performance at estimating MEG models, simulated net-
work data are initially used. In this section, a small fully connected directed graph with n = 2 is con-
and τij = 0 are generated: (i) MEGs with
sidered. Two types of mutually exciting graphs with r =

∞

Sanna Passino, F. and Heard N. A.

10

Mutually exciting point process graphs for modelling dynamic networks

(a) Baseline

(b) Jump

(c) Decay

(d) KS scores

Figure 2: Histograms (with corresponding kernel density estimates) of parameter estimates and boxplots of KS scores
obtained using EM and Adam from 100 simulations of 3,000 events on a fully connected MEG with n = 2,
, α = [0.01, 0.05], β = [0.07, 0.03], µ = [0.2, 0.15], µ(cid:48) = [0.1, 0.25],
λij(t) = αi(t) + βj(t), r =
∞
φ = [0.8, 0.85], φ(cid:48) = [0.9, 0.75].

λij(t) = αi(t) + βj(t), αi(t) and βj(t) as in (2.2) and (2.3), with α = [0.01, 0.05], β = [0.07, 0.03],
µ = [0.2, 0.15], µ(cid:48) = [0.1, 0.25], φ = [0.8, 0.85], φ(cid:48) = [0.9, 0.75], and (ii) MEGs with λij(t) = γij(t),
cf. (2.4), γ = [0.1, 0.5], γ(cid:48) = [0.1, 0.3], ν = [0.6, 0.4], ν(cid:48) = [0.5, 0.25], θ = [0.4, 0.6], θ(cid:48) = [0.5, 0.75].
For each of the two MEG models, 3,000 events are simulated using Algorithm 3, and the process parameters
are estimated from the simulated events via EM (Algorithm 1) and Adam (Algorithm 2, with η = 0.05),
initialising the parameters at random from a uniform distribution in (0.1, 1). The estimation procedure is
repeated 5 times from different random initialisation points, and the ﬁnal estimates corresponding to the
highest log-likelihood are retained. Using the estimated parameters, the p-values (4.1) are then calculated
for all simulated events. Finally, the Kolmogorov-Smirnov (KS) score against the uniform distribution is cal-
culated on those p-values. The procedure is then repeated 100 times, obtaining a set of parameter estimates
for each simulated MEG.

The results are plotted in Figures 2 and 3, which report the histograms and kernel density estimates of
identiﬁable transformations of the parameters for each of the four network edges, obtained using the EM and
Adam algorithms, compared to the true value of the parameters. Furthermore, the ﬁgures report the boxplots
of the KS scores.

Overall, it appears that the results obtained using Adam are only marginally worse than those obtained
using the EM algorithm. In particular, the distributions of estimates obtained using the two methodologies

Sanna Passino, F. and Heard N. A.

11

Mutually exciting point process graphs for modelling dynamic networks

(a) Baseline

(b) Jump

(c) Decay

(d) KS scores

Figure 3: Histograms (with corresponding kernel density estimates) of parameter estimates and boxplots of KS scores
obtained using EM and Adam from 100 simulations of 3,000 events on a fully connected MEG with n = 2,
, γ = [0.1, 0.5], γ(cid:48) = [0.1, 0.3], ν = [0.6, 0.4], ν(cid:48) = [0.5, 0.25], θ = [0.4, 0.6],
λij(t) = γij(t), r =
θ(cid:48) = [0.5, 0.75].

∞

appear to be roughly centred around the true value of the parameters, but the EM estimates appear to be
slightly more precise and accurate when compared to Adam. In both cases, the KS scores are extremely
small, demonstrating an excellent ﬁt. Furthermore, it is possible to compare the performance of the two
inferential algorithms when the number of events increases: Figure 4 reports the histograms of estimates of
the decay µ + φ obtained using only 250, 500, 1,000 and 2,000 of the 3,000 simulated events on each graph.
The performance of both estimation procedures improves in terms of KS scores and variance of estimates
when more observations are available.

The main advantage of Adam over the EM algorithm for r =

is mainly given by the recursive form
of the log-likelihood, which enables calculations in linear time on each edge for the gradients. On the other
hand, the EM algorithm requires a quadratic number of additional latent variables deﬁned on each edge,
which becomes unsustainable for efﬁcient inference on large graphs. Therefore, for the remainder of this
work, Adam will be used, primarily because of computational reasons.

∞

5.2 Simulated events on Erd˝os-Rényi graphs

In order to further evaluate estimation of MEG models, events on an Erd˝os-Rényi graph are also simulated.
First, an adjacency matrix is simulated from an Erd˝os-Rényi graph with n = 10 nodes, such that Aij
∼
Bernoulli(p), with p = 1/4. For the edges such that Aij = 1, then τij = 0, otherwise if Aij = 0,

Sanna Passino, F. and Heard N. A.

12

Mutually exciting point process graphs for modelling dynamic networks

(a) m = 250

(b) m = 500

(c) m = 1,000

(d) m = 2,000

Figure 4: Histograms (with corresponding kernel density estimates) of estimates for µ + φ and boxplots of KS
scores obtained using EM and Adam from 100 simulations from the same model as Figure 2, with
m

250, 500, 1,000, 2,000

events.

∈ {

}

∞

. Second, m = 2,500 event times are generated from a MEG model with r =

then τij =
using
∞
Algorithm 3, with parameters in Ψ sampled at random from uniform distributions, restricted to the following
(10−2, 1),
ranges: αi, βj
j(cid:96) ∈
ν(cid:48)
and θi(cid:96) = 1
j(cid:96). In the simulation, the expected number of events per active edge is
m/[pn(n
6 = 120 parameters, with learning rate η = 0.1,
after a random initialisation from the same uniform distributions used in the data generating process. The
entire procedure is repeated 100 times.

(10−5, 10−4), µi, µ(cid:48)
νi(cid:96), θ(cid:48)
j(cid:96) = 1

111. Algorithm 2 is used to estimate 2n

(10−2, 10−1), γi(cid:96), γ(cid:48)

(10−5, 10−1), νi(cid:96), ν(cid:48)

∈
−
1)]

j, φi, φ(cid:48)

j(cid:96) ∈

j ∈

≈

−

−

×

A second simulation is conducted for an Erd˝os-Rényi graph graph with n = 20 nodes and p = 1/4,
simulating m = 10,000 events from a MEG model with interaction term only, corresponding to λij(t) =
Aijγij(t), with r = 1 and d = 5. A minor modiﬁcation is made to the range of the uniform distributions
(10−2, 1).
for sampling some of the interaction term parameters: γi(cid:96), γ(cid:48)
Despite the simpler form of the intensity functions λij(t), more parameters must be estimated (2n
3d =
600) compared to the ﬁrst simulation, and the expected number of connections per edge is only 105. As
before, 100 MEGs are generated, and Adam (Algorithm 2) is used to estimate the parameters, with learning
rate η = 10−3. The resulting boxplots of the KS test obtained for the two simulations are plotted in Figure 5.
Both boxplots demonstrate that the algorithm is able to recover sensible estimates of the parameter values,
resulting in small KS scores, corresponding to a good model ﬁt.

(10−5, 10−1), νi(cid:96), ν(cid:48)

j(cid:96), θi(cid:96), θ(cid:48)

j(cid:96) ∈

j(cid:96) ∈

×

5.3 Enron e-mail network

The Enron e-mail network collection is a record of e-mails exchanged between the employees of Enron
Corporation before its bankruptcy. These data have already been demonstrated to be well-modelled as self-
exciting point processes by Fox et al. (2016). In this article, the version of these data1 used in Priebe et al.
(2005) is analysed, where e-mails recorded multiple times have been used only once, and e-mails with
incorrectly recorded sent times (coded in the data with 9pm, 31 December 1979) have been removed. After
such pre-processing, the e-mail data consist of 34,427 distinct triplets (xk, yk, tk), corresponding to messages

1The data are freely available at http://www.cis.jhu.edu/~parky/Enron/.

Sanna Passino, F. and Heard N. A.

13

Mutually exciting point process graphs for modelling dynamic networks

Figure 5: Boxplots of the Kolmogorov-Smirnov scores obtained for the two simulations described in Section 5.2.

exchanged between n = 184 employees between November 1998 and June 2002, forming a total of 3,007
graph edges. Note that some of the emails are sent to multiple receivers, and only 18,031 unique event times
are observed, implying that on average each e-mail is sent to approximately 1.90 nodes.

Because an e-mail can have multiple recipients, and because the event times are recorded to the nearest
second, the likelihood (2.6) must be adapted slightly to handle tied arrival times. An approach used by Price-
Williams and Heard (2020, Section 8.1) is followed, with the arrivals modelled by an analogous discrete time
process: In particular, arrivals at time t are assumed to contribute to the the intensities λij(
) from time t + dt
·
onwards, where dt is the sampling interval, equal to one second in this example. The p-values of the process
are approximated using (4.1), following Fox et al. (2016).

The model is trained on 30,704 e-mails sent before 1st December, 2001, and tested on the remaining
3,723 e-mails. In the training set, 2,720 edges are observed, and 811 in the test set, of which 287 are not
observed in the training period. One of the advantages of the proposed methodology is the possibility to
score events for such new links.

A range of MEG models are ﬁtted to the training data, using different combinations of r and d for
characterising main effects and interactions. A good conﬁguration for the initial parameter values is obtained
N (cid:48)
j (T )
through utilising the quantities ui = Ni(T )
nT , corresponding to the average rate of incoming
nT
and outgoing connections observed for each node. In particular, good results and convergence are obtained
setting initial values αi = µi = ui, φi = 3ui, βj = µ(cid:48)
j = u(cid:48)
j. For the interaction term, the
10−4. If d > 1,
j(cid:96) = νi(cid:96) = ν(cid:48)
initial values used to obtain the results are γi(cid:96) = γ(cid:48)
10−5 is added to the interaction parameters. In general, the
then Gaussian noise with standard deviation 2
algorithm is fairly robust to different initialisations if the scale of the parameters is similar to the choices
above. The learning rate η is set to 0.1.

j = 3u(cid:48)
j, and φ(cid:48)
j(cid:96) = 10−4, and θi(cid:96) = θ(cid:48)

and u(cid:48)

j(cid:96) = 5

j =

·

·

Three strategies are used for estimation of τij: (i) Using the MLE ˆτij = t(cid:96)ij1; (ii) Setting τij = 0;
(iii) Setting τij = 0 if Aij = 1, and τij =
if Aij = 0. The MLE approach (i) has a drawback: the p-values
(4.1) for the ﬁrst observation on each edge are always 1. This implies that the KS scores are bounded below
by 2720/30704

0.0885 for the training set and 287/3723

0.0770 for the test set.

∞

The KS scores obtained on the training and test sets after ﬁtting different MEG models are reported
in Table 1. The best performance (KS score 0.0152) is achieved when a Markov process is used for the
interaction term, with d = 5 or d = 10, combined with a Hawkes process for the main effects, setting τij
using option (iii). The same model achieves the best performance when alternative strategies for estimation
of τij are used. If τij is set to its MLE (i), then the lower bound for the KS score on the training set is
attained. In general, setting τij using option (iii) seems to outperform competing strategies for estimation of
τij in terms of KS scores. More importantly, overall the results demonstrate that the interaction term plays a
key role in obtaining a good ﬁt on the observed event times.

The results on the training set can also be compared to alternative node-based models from the literature.

Sanna Passino, F. and Heard N. A.

14

≈

≈

0.010.020.030.040.050.060.070.080.09Kolmogorov-SmirnovscoresSimulation2Simulation1Mutually exciting point process graphs for modelling dynamic networks

Table 1: Training and test Kolmogorov-Smirnov scores on the Enron e-mail network for different conﬁgurations of the

MEG model.

KS scores (train & test)

τij

↓

Interactions γij(

)
·

↓

Absent

Absent

–

–

τij = t(cid:96)ij1 (MLE)

τij = 0

(cid:40)

0,

τij =

Aij = 1
, Aij = 0

∞

Poisson
(r = 0)

Markov
(r = 1)

Hawkes
(r =
)
∞

d = 1
d = 5
d = 10
d = 1
d = 5
d = 10
d = 1
d = 5
d = 10

Absent

Poisson
(r = 0)

Markov
(r = 1)

Hawkes
(r =
)
∞

d = 1
d = 5
d = 10
d = 1
d = 5
d = 10
d = 1
d = 5
d = 10

Absent

Poisson
(r = 0)

Markov
(r = 1)

Hawkes
(r =
)
∞

d = 1
d = 5
d = 10
d = 1
d = 5
d = 10
d = 1
d = 5
d = 10

0.4252 0.4221
0.3490 0.3851
0.3339 0.3763
0.1662 0.2029
0.0916 0.1875
0.0885 0.1743
0.2640 0.2755
0.2304 0.2904
0.2461 0.2923

–

–

0.7039 0.7926
0.5623 0.7059
0.5354 0.6853
0.3135 0.3324
0.0760 0.1664
0.0775 0.1649
0.2871 0.2486
0.1939 0.2167
0.2029 0.2395

–

–

0.5158 0.6038
0.4269 0.5516
0.4035 0.5413
0.1950 0.2115
0.0709 0.1222
0.0619 0.1029
0.1870 0.2084
0.1377 0.1805
0.1556 0.2023

Main effects αi(

) and βj(
·

)
·

↓

Poisson (r = 0) Markov (r = 1) Hawkes (r =

0.4530 0.4133
0.3946 0.4179
0.3498 0.3953
0.3347 0.3688
0.1491 0.1945
0.0910 0.1684
0.0885 0.1848
0.2825 0.2887
0.2284 0.2760
0.2521 0.2865

0.7678 0.7983
0.6627 0.7753
0.5646 0.7206
0.5332 0.6739
0.3004 0.3326
0.0825 0.1584
0.0793 0.1546
0.2333 0.2449
0.1885 0.2246
0.2158 0.2470

0.5590 0.5941
0.4812 0.5864
0.4309 0.5641
0.4084 0.5565
0.1600 0.2017
0.0746 0.1008
0.0627 0.1079
0.1816 0.2049
0.1374 0.1840
0.1588 0.2046

0.3678 0.3484
0.3434 0.3574
0.3165 0.3677
0.3112 0.3470
0.1305 0.1777
0.0885 0.1628
0.0885 0.1696
0.2538 0.2637
0.2271 0.2774
0.2413 0.3091

0.7456 0.7360
0.6543 0.7148
0.5748 0.7008
0.5725 0.6952
0.3262 0.3240
0.0855 0.1782
0.0816 0.1606
0.2485 0.2379
0.2010 0.2137
0.2207 0.2339

0.4112 0.3667
0.3742 0.3602
0.3553 0.3598
0.3430 0.3537
0.1504 0.1422
0.0696 0.0917
0.0634 0.0836
0.1783 0.1747
0.1391 0.1642
0.1546 0.1863

)
∞
0.4443 0.3586
0.4255 0.3560
0.3491 0.3613
0.3376 0.3575
0.1702 0.1874
0.0916 0.1746
0.0885 0.1743
0.2599 0.2871
0.2420 0.2981
0.2498 0.3129

0.7058 0.6046
0.7059 0.6050
0.7060 0.6053
0.7060 0.6059
0.2027 0.1999
0.0495 0.0924
0.0402 0.0971
0.1749 0.1991
0.1467 0.1994
0.1606 0.1943

0.4593 0.2758
0.4197 0.2808
0.3938 0.2803
0.3659 0.2810
0.1309 0.1445
0.0152 0.0848
0.0213 0.0800
0.1719 0.1879
0.1553 0.2154
0.1640 0.2082

For example, Fox et al. (2016) propose the following node-speciﬁc intensity function for sending e-mails:

λi(t) = αi +

N (cid:48)
i (t)
(cid:88)

k=1

µi exp

(µi + φi)(t

{−

t(cid:48)
ik)
}

,

−

(5.1)

where the intensity jumps according to the event times of the received e-mails, cf. (2.2) and (2.3). Despite
the present article using a slightly different number of e-mails, the Kolmogorov-Smirnov score obtained on
the training data using (5.1) is 0.2806, which corresponds almost exactly to the result in Fox et al. (2016),
demonstrating that the MEG appears to have superior performance for the Enron network. The parameters
of (5.1) are estimated by direct optimisation using the Nelder-Mead method on the negative log-likelihood
function for each source node. Nearly identical results to Fox et al. (2016) are also obtained from ﬁtting an
independent Poisson processes λi(t) = αi on each source node, with KS score 0.4088. Finally, independent
Hawkes process models of the form (1.1) are also ﬁtted to each source node, obtaining a KS score of 0.2499
which is signiﬁcantly outperformed by the best conﬁguration of the MEG model. Since the MEG model KS
score outperforms the value obtained using (5.1), it could be inferred that users tend to respond to multiple
e-mails in sessions, and not necessarily immediately after an individual e-mail is received.

5.4

Imperial College London NetFlow data

Many enterprises routinely collect network ﬂow (NetFlow) data, representing summaries of connections
between internet protocol (IP) addresses (see, for example, Hofstede et al., 2014), which should be monitored
for detecting unusual network activity, security breaches, and potential intrusions. Modelling arrival times in

Sanna Passino, F. and Heard N. A.

15

Mutually exciting point process graphs for modelling dynamic networks

Figure 6: Connections to the ICL Virtual Learning Environment from two clients.

computer networks is complicated by several factors: events tend to appear in bursts, they might be recorded
multiple times, and exhibit polling at regular intervals (Heard et al., 2014). In computer network security, it
is particularly important to assess the signiﬁcance of observing new links, corresponding to connections on
previously unobserved edges (Metelli and Heard, 2019). New links might be indicative of lateral movement,
which is a common behaviour of network attackers (Neil et al., 2013): intruders might move across the
network with the purpose of escalating credentials, establishing connections which were previously unseen or
unexpected. Therefore, correctly modelling new connections, and consequently providing reliable anomaly
scores, is paramount for network security. The proposed MEG framework for modelling point processes on
networks simultaneously addresses two fundamental tasks in network security: monitoring the normality of
observed trafﬁc, and anomaly detection for unusual new connections.

A bipartite dynamic network has been constructed from a subset of NetFlow data collected at Imperial
College London (ICL). The network consists of 1,951,067 arrival times recorded to the nearest millisecond,
observed between 20th January 2020, and 9th February 2020, recorded from n1 = 173 clients hosted within
the Department of Mathematics at ICL, connecting to n2 = 6,083 internet servers connecting on ports 80
and 443 (corresponding to unencrypted and encrypted web trafﬁc), forming a total of 156,186 unique edges.
The periodic and automated activity has been ﬁltered by considering only edges such that the percentage
of arrival times observed between 7am and 12am is larger than 99%, corresponding to the building opening
hours. To learn connectivity patterns, the MEG model is trained on the ﬁrst two weeks of data, corresponding
to 1,299,372 events, and tested on 651,695 events observed in the ﬁnal week. The number of unique edges
observed in the training period is 115,600, and 70,408 in the test set; only 29,822 edges are observed in both
time windows, which implies that 40,586 new edges are observed in the test set.

As discussed in Section 1, computer network data are observed in bursts and exhibit periodic behaviour.
Figure 6 gives an example of the connections from two of the clients to the ICL Virtual Learning Environment
(VLE) server. Each session begins at an hour consistent with human behaviour, while the frequency of
subsequent connections within each session is likely to be due to automated activity and page refreshing.

The models have been initialised using a similar initialisation scheme to Section 5.3, with learning rate
N (cid:48)
j (T )
n2T , the chosen initial values are αi = µi = ui, φi =
10−4.

η = 0.5. In particular, setting ui = Ni(T )
3ui, βj = µ(cid:48)
j)1/2, νi(cid:96) = ν(cid:48)
j, and φ(cid:48)
As before, Gaussian noise is added to the interaction parameters if d > 1.

n1T and u(cid:48)
j =
j, γi(cid:96) = (ui)1/2, γ(cid:48)

j(cid:96) = 10−4, and θi(cid:96) = θ(cid:48)

j(cid:96) = (u(cid:48)

j = 3u(cid:48)

j = u(cid:48)

j(cid:96) = 5

·

The likelihood for the Hawkes process is highly multimodal, and more sensitive to the initial values of the
parameters than the Markov process with r = 1. Therefore, the parameters for the Hawkes process models
are initialised with the optimal values obtained from the corresponding Markov process models, which seems
to lead to fast convergence. The Kolmogorov-Smirnov scores calculated on the training and test set arrival
times for different MEG models are reported in Table 2. The parameter τij is set according to option (iii)
from Section 5.3, which was observed to have the best performance on the Enron data.

Sanna Passino, F. and Heard N. A.

16

03691215182124Timeofday14710131619Day03691215182124Timeofday14710131619DayMutually exciting point process graphs for modelling dynamic networks

Table 2: KS scores on the ICL NetFlow data for different conﬁgurations of the MEG model.

KS scores (train & test)

Interactions γij(
·

)

↓

Absent

Poisson
(r = 0)

Markov
(r = 1)

Hawkes
(r =
)
∞

d = 1
d = 5
d = 10

d = 1
d = 5
d = 10

d = 1
d = 5
d = 10

Main effects αi(

) and βj(
·

)
·

↓

Absent

Poisson (r = 0) Markov (r = 1) Hawkes (r =

–

–

0.7351 0.7148

0.6678 0.6489

0.7328 0.7157
0.7295 0.7167
0.7260 0.7174

0.2194 0.1723
0.1024 0.1080
0.0843 0.0764

0.1080 0.0802
0.1576 0.1819
0.1584 0.1935

0.7325 0.7150
0.7313 0.7123
0.7289 0.7140

0.2242 0.1657
0.0896 0.0805
0.0871 0.0761

0.0747 0.1182
0.1532 0.2126
0.1546 0.2112

0.6672 0.6480
0.6673 0.6487
0.6680 0.6493

0.2038 0.1440
0.0728 0.0738
0.0850 0.0843

0.1082 0.0794
0.1677 0.2143
0.1619 0.2206

)
∞
0.7312 0.6950

0.7316 0.6960
0.7275 0.6967
0.7270 0.6969

0.1645 0.1281
0.1041 0.0899
0.1100 0.0883

0.0884 0.1262
0.2307 0.2383
0.2388 0.2503

(a) Training set

(b) Test set

Figure 7: Q-Q plots for the training and test p-values obtained from different MEG models, with main effects αi(t)

and βj(t) with r = 1, and different parameters for the interaction term γij(t), speciﬁed in the legend.

The best performance (KS score 0.0728) is achieved by a Markov process with r = 1 for both the main
effects and interactions, and latent dimensionality d = 5 for the parameters of the interaction term. Corre-
sponding Q-Q plots for some of the models are plotted in Figure 7. Overall, the table and plots demonstrate
that correctly modelling the arrival times requires inclusion within the model of an interaction term with a
self-exciting component. Because of the extremely bursty behaviour of NetFlow arrival times, the Markov
process model for main effects and interactions intuitively appears to be a suitable choice.

Finally, for the best performing model the corresponding KS scores are calculated individually for each
edge, and plotted in Figure 8 as a function of the number of connections on the edge. Clearly, the model has
a better performance at scoring arrival times on more active edges.

6 Conclusion

The mutually-exciting graph (MEG), a novel network-wide model for point processes with dyadic marks has
been proposed. MEG uses mutually exciting point processes to model intensity functions, and borrows ideas
from latent space models to infer relationships between the nodes. Edge-speciﬁc intensities are obtained only

Sanna Passino, F. and Heard N. A.

17

0.00.20.40.60.81.0Theoreticalquantiles0.00.20.40.60.81.0ObservedquantilesUniformdistributionr=0,d=1r=1,d=1r=1,d=5r=1,d=10r→∞,d=10.00.20.40.60.81.0Theoreticalquantiles0.00.20.40.60.81.0ObservedquantilesMutually exciting point process graphs for modelling dynamic networks

(a) Training set

(b) Test set

Figure 8: Scatterplot of the KS scores, calculated for each edge, versus the logarithm of the total number of connec-

tions on the edge, for the best performing model in Table 2.

via node-speciﬁc parameters, which is useful for large and sparse graphs. Importantly, the proposed model
is able to predict events observed on new edges. Inference is performed via maximum likelihood estimation,
optimised using the EM algorithm, or numerically via modern gradient ascent methods. The model has
been tested on simulated data and on two data sources related to computer networking: ICL NetFlow data
and the Enron e-mail network. MEG appears to have excellent goodness-of-ﬁt on training and testing data,
resulting in low Kolmogorov-Smirnov scores even on very large and heterogeneous data like network ﬂows.
Furthermore, for the Enron e-mail network, MEG greatly outperforms results previously obtained in the
literature on the same data. The model has been speciﬁcally motivated by cyber-security applications, where
scoring observations on new links is particularly important for network security. Within this context, MEG
might be used to complement existing techniques for modelling sequences of edges on dynamic networks
(Sanna Passino and Heard, 2019), providing a network-wide method for scoring arrival times.

The model could potentially be extended to admit an increasing number of nodes. Node-speciﬁc param-
eter values could be assigned after clustering similar nodes into groups, and allocating new nodes to one
such community. For example, the initial parameter values for new nodes could correspond to the centroid
of the corresponding community-speciﬁc parameter values. The initial cluster structure could be established
from the connectivity patterns in the adjacency matrix (via spectral clustering or modularity maximisation),
or from additional labels available for the nodes (for example, in cyber-security applications, subnets, geo-
graphical location, or machine type).

Code

A python library to reproduce the results in this paper, and a bash script to obtain the Enron e-mail network
data, are available in the GitHub repository fraspass/meg.

Acknowledgements

This work is funded by the Microsoft Security AI research grant “Understanding the enterprise: Host-based
event prediction for automatic defence in cyber-security". The authors thank Dr Melissa J. M. Turcotte for
helpful discussions and comments.

Sanna Passino, F. and Heard N. A.

18

Mutually exciting point process graphs for modelling dynamic networks

References

Athreya, A., Fishkind, D. E., Tang, M., Priebe, C. E., Park, Y., Vogelstein, J. T., Levin, K., Lyzinski, V., Qin,
Y. and Sussman, D. L. (2018) Statistical inference on random dot product graphs: a survey. Journal of
Machine Learning Research, 18, 1–92.

Blundell, C., Beck, J. and Heller, K. (2012) Modelling reciprocating relationships with hawkes processes.
In Advances in Neural Information Processing Systems 25 (eds. F. Pereira, C. Burges, L. Bottou and
K. Weinberger), 2600–2608. Curran Associates.

Bowsher, C. G. (2007) Modelling security market events in continuous time: Intensity based, multivariate

point process models. Journal of Econometrics, 141, 876 – 912.

Brown, E., Barbieri, R., Ventura, V., Kass, R. and Frank, L. (2002) The time-rescaling theorem and its

application to neural spike train data analysis. Neural computation, 14, 325–346.

Chen, F. and Tan, W. H. (2018) Marked self-exciting point process modelling of information diffusion on

Twitter. Annals of Applied Statistics, 12, 2175–2196.

Chen, X., Liu, S., Sun, R. and Hong, M. (2019) On the convergence of a class of Adam-type algorithms for

non-convex optimization. In International Conference on Learning Representations.

Daley, D. and Vere-Jones, D. (2002) An Introduction to the Theory of Point Processes – Volume I: Elementary

Theory and Methods. Probability and Its Applications. Springer.

Dempster, A. P., Laird, N. M. and Rubin, D. B. (1977) Maximum likelihood from incomplete data via the

em algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39, 1–22.

Durante, D. and Dunson, D. B. (2016) Locally adaptive dynamic networks. Annals of Applied Statistics, 10,

2203–2232.

Eichler, M., Dahlhaus, R. and Dueck, J. (2017) Graphical modeling for multivariate hawkes processes with

nonparametric link functions. Journal of Time Series Analysis, 38, 225–242.

Etesami, J., Kiyavash, N., Zhang, K. and Singhal, K. (2016) Learning network of multivariate Hawkes
processes: A time series approach. In Proceedings of the Thirty-Second Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’16, 162–171. AUAI Press.

Fox, E., Short, M., Schoenberg, F., Coronges, K. and Bertozzi, A. (2016) Modeling e-mail networks and
inferring leadership using self-exciting point processes. Journal of the American Statistical Association,
111, 564–584.

Hawkes, A. (1971) Spectra of some self-exciting and mutually exciting point processes. Biometrika, 58,

83–90.

Heard, N. A., Rubin-Delanchy, P. T. G. and Lawson, D. J. (2014) Filtering automated polling trafﬁc in com-
puter network ﬂow data. Proceedings - 2014 IEEE Joint Intelligence and Security Informatics Conference,
JISIC 2014, 268–271.

Hoff, P. (2021) Additive and Multiplicative Effects Network Models. Statistical Science, 36, 34–50.

Hoff, P. D., Raftery, A. E. and Handcock, M. S. (2002) Latent space approaches to social network analysis.

Journal of the American Statistical Association, 97, 1090–1098.

Hofstede, R., ˇCeleda, P., Trammell, B., Drago, I., Sadre, R., Sperotto, A. and Pras, A. (2014) Flow monitoring
explained: From packet capture to data analysis with NetFlow and IPFIX. IEEE Communications Surveys
Tutorials, 16, 2037–2064.

Sanna Passino, F. and Heard N. A.

19

Mutually exciting point process graphs for modelling dynamic networks

Kingma, D. P. and Ba, J. (2015) Adam: a method for stochastic optimization. In 3rd International Conference

on Learning Representations, ICLR (eds. Y. Bengio and Y. LeCun). San Diego, CA, USA.

Krivitsky, P. N. and Handcock, M. S. (2014) A separable model for dynamic networks. Journal of the Royal

Statistical Society: Series B, 76, 29–46.

Lee, W., McCormick, T. H., Neil, J., Sodja, C. and Cui, Y. (2021) Anomaly detection in large scale networks

with latent space models. Technometrics, 0, 1–23.

Linderman, S. W. and Adams, R. P. (2014) Discovering latent network structure in point process data. In
Proceedings of the 31st International Conference on International Conference on Machine Learning -
Volume 32, ICML’14, II–1413–II–1421.

Metelli, S. and Heard, N. A. (2019) On Bayesian new edge prediction and anomaly detection in computer

networks. Annals of Applied Statistics, 13, 2586–2610.

Miscouridou, X., Caron, F. and Teh, Y. W. (2018) Modelling sparsity, heterogeneity, reciprocity and commu-
nity structure in temporal interaction data. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, NIPS’18, 2349–2358.

Mohler, G. O., Short, M. B., Brantingham, P. J., Schoenberg, F. P. and Tita, G. E. (2011) Self-exciting point

process modeling of crime. Journal of the American Statistical Association, 106, 100–108.

Neil, J., Hash, C., Brugh, A., Fisk, M. and Storlie, C. B. (2013) Scan statistics for the online detection of

locally anomalous subgraphs. Technometrics, 55, 403–414.

Ogata, Y. (1978) The asymptotic behaviour of maximum likelihood estimators for stationary point processes.

Annals of the Institute of Statistical Mathematics, 30, 243–261.

Ogata, Y. (1981) On Lewis’ simulation method for point processes.

IEEE Transactions on Information

Theory, 27, 23–31.

Ogata, Y. (1988) Statistical models for earthquake occurrences and residual analysis for point processes.

Journal of the American Statistical Association, 83, 9–27.

Ozaki, T. (1979) Maximum likelihood estimation of Hawkes’ self-exciting point processes. Annals of the

Institute of Statistical Mathematics, 31, 145–155.

Perry, P. and Wolfe, P. (2013) Point process modelling for directed interaction networks. Journal of the Royal

Statistical Society: Series B (Statistical Methodology), 75, 821–849.

Price-Williams, M. and Heard, N. A. (2020) Nonparametric self-exciting models for computer network traf-

ﬁc. Statistics and Computing, 30, 209–220.

Priebe, C. E., Conroy, J. M., Marchette, D. J. and Park, Y. (2005) Scan statistics on Enron graphs. Computa-

tional & Mathematical Organization Theory, 11, 229–247.

Reddi, S. J., Kale, S. and Kumar, S. (2018) On the convergence of Adam and beyond.

In International

Conference on Learning Representations.

Ruder, S. (2016) An overview of gradient descent optimization algorithms. arXiv e-prints.

Sanna Passino, F. and Heard, N. A. (2019) Modelling dynamic network evolution as a Pitman-Yor process.

Foundations of Data Science, 1, 293–306.

Sarkar, P. and Moore, A. W. (2006) Dynamic social network analysis using latent space models. In Advances

in Neural Information Processing Systems 18, 1145–1152.

Sanna Passino, F. and Heard N. A.

20

Mutually exciting point process graphs for modelling dynamic networks

Sewell, D. K. and Chen, Y. (2015) Latent space models for dynamic networks. Journal of the American

Statistical Association, 110, 1646–1657.

Stomakhin, A., Short, M. B. and Bertozzi, A. L. (2011) Reconstruction of missing data in social networks

based on temporal patterns of interactions. Inverse Problems, 27.

Zhao, Q., Erdogdu, M. A., He, H. Y., Rajaraman, A. and Leskovec, J. (2015) Seismic: A self-exciting point
process model for predicting tweet popularity. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, KDD ’15, 1513–1522.

Zou, F., Shen, L., Jie, Z., Zhang, W. and Liu, W. (2019) A sufﬁcient condition for convergences of adam and
rmsprop. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 11119–
11127. IEEE Computer Society.

A Calculation of the log-likelihood in MEG models

j(cid:96), ν(cid:48)

j(cid:96), θ(cid:48)
j(cid:96)). Assume two sequences of arrival times t1 <
< t(cid:48)
n(cid:48)
j

According to the choice of the excitation function and the parameter r, the log-likelihood (2.6) takes different
forms. Here, examples are given for MEG models with d = 1, r = 1 or r =
, with scaled exponential exci-
tation functions. For simplicity, since d = 1, the second subscript (cid:96) is dropped from the triplets (γi(cid:96), νi(cid:96), θi(cid:96))
and (γ(cid:48)
< tni involving i as source node, and
t(cid:48)
such that j is the destination of the connection. Within the two sequences of arrival times
1 <
· · ·
1, . . . , t(cid:48)
t1, . . . , tni and t(cid:48)
n(cid:48)
j
edge (i, j). Denote the indices of such events as (cid:96)1, . . . , (cid:96)nij and (cid:96)(cid:48)
¯tk = max
k = max
th : th < t(cid:96)k }
{
{
the log-likelihood (2.6) is:

. Deﬁne
. For the edge (i, j), assuming r = 1, the ﬁrst part of

ni, n(cid:48)
min
j}
{
nij , such that t(cid:96)k = t(cid:48)
(cid:96)(cid:48)
k

, assume that a subset of nij events, with nij

, is observed on the

h < t(cid:48)
(cid:96)(cid:48)
k }

≤
1, . . . , (cid:96)(cid:48)

h : t(cid:48)
t(cid:48)

and ¯t(cid:48)

· · ·

∞

nij
(cid:88)

k=1

log λij(t(cid:96)k ) =

nij
(cid:88)

k=1

log

(cid:110)

αi + µie−(µi+φi)(t(cid:96)k

−¯tk) + βj + µ(cid:48)
je

−(µ(cid:48)

j +φ(cid:48)

j )(t(cid:48)
(cid:96)(cid:48)
k

−¯t(cid:48)

k)

+ γiγ(cid:48)

j + νiν(cid:48)

je−(νi+θi)(ν(cid:48)

j +θ(cid:48)

j )(t(cid:96)k

−t(cid:96)k−1

)(cid:111)
.

(A.1)

For Hawkes process models, the main computational burden associated with the calculation of the likelihood
is the double summation arising in the ﬁrst term of (2.6) when r =
. The term in the ﬁrst sum in (2.6) can
be written as:

∞

nij
(cid:88)

k=1

log λij(t(cid:96)k ) =

nij
(cid:88)

k=1

(cid:26)

log

αi + µi

(cid:96)k−1
(cid:88)

h=1

e−(µi+φi)(t(cid:96)k

−th) + βj + µ(cid:48)
j

(cid:96)(cid:48)
k−1
(cid:88)

e

−(µ(cid:48)

j +φ(cid:48)

j )(t(cid:48)
(cid:96)(cid:48)
k

−t(cid:48)

h)

+ γiγ(cid:48)

j + νiν(cid:48)
j

k−1
(cid:88)

h=1

e−(νi+θi)(θ(cid:48)

j +ν(cid:48)

j )(t(cid:96)k

h=1
(cid:27)

−t(cid:96)h

)

.

(A.2)

Using a technique similar to the method proposed in Ogata (1978), it is possible to calculate (A.2) in linear
time using a recursive formulation of the inner summations. For k
ij(k) and
˜ψij(t) as follows:

, deﬁne ψij(k), ψ(cid:48)

2, . . . , nij

∈ {

}

ψij(k) =

(cid:96)k−1
(cid:88)

h=1

e−(µi+φi)(t(cid:96)k

−th),

ψ(cid:48)

ij(k) =

(cid:96)(cid:48)
k−1
(cid:88)

h=1

˜ψij(k) =

k−1
(cid:88)

h=1

e−(νi+θi)(ν(cid:48)

j +θ(cid:48)

j )(t(cid:96)k

−t(cid:96)h

),

−(µ(cid:48)
e

j +φ(cid:48)

j )(t(cid:48)
(cid:96)(cid:48)
k

−t(cid:48)

h)

,

Sanna Passino, F. and Heard N. A.

21

Mutually exciting point process graphs for modelling dynamic networks

assuming the initial conditions ˜ψij(1) = 0 and

ψij(1) =

(cid:96)1−1
(cid:88)

h=1

e−(µi+φi)(t(cid:96)1 −th),

ψ(cid:48)

ij(1) =

(cid:96)(cid:48)
1−1
(cid:88)

h=1

−(µ(cid:48)
e

j +φ(cid:48)

j )(t(cid:48)
(cid:96)(cid:48)
1

−t(cid:48)

h)

.

Note that the subscript (i, j) for ψij(k) and ψ(cid:48)
represent a short hand notation for (cid:96)ijk and (cid:96)(cid:48)

ij(k) is required since (cid:96)k and (cid:96)(cid:48)
k are edge-speciﬁc values, and
ijk. Using (2.8), the ﬁrst term (A.2) of the likelihood becomes:

nij
(cid:88)

k=1

log λij(t(cid:96)k ) =

nij
(cid:88)

k=1

log

(cid:110)

αi + βj + γiγ(cid:48)

j + µiψij(k) + µ(cid:48)

jψ(cid:48)

ij(k) + νiν(cid:48)
j

(cid:111)

˜ψij(k)

.

Proposition 2. The terms ψij(k), ψ(cid:48)

ij(k) and ˜ψij(k) can be written recursively as follows:

ψij(k) = e−(µi+φi)(t(cid:96)k

−t(cid:96)k−1

) [1 + ψij(k

1)] +

−

(cid:96)k−1
(cid:88)

e−(µi+φi)(t(cid:96)k

−th),

(A.3)

ψ(cid:48)

ij(k) = e

−(µ(cid:48)

j +φ(cid:48)

j )(t(cid:48)
(cid:96)(cid:48)
k

−t(cid:48)
(cid:96)(cid:48)
k−1

) (cid:2)1 + ψ(cid:48)

ij(k

−

1)(cid:3) +

˜ψij(k) = e−(νi+θi)(ν(cid:48)

j +θ(cid:48)

j )(t(cid:96)k

−t(cid:96)k−1

) (cid:104)

1 + ˜ψij(k

−

h=(cid:96)k−1+1
(cid:96)(cid:48)
k−1
(cid:88)

k−1+1

h=(cid:96)(cid:48)
(cid:105)
1)

.

−(µ(cid:48)

j +φ(cid:48)

j )(t(cid:48)
(cid:96)(cid:48)
k

−t(cid:48)

h)

,

e

Proof. The result is proved here for ψij(k).

ψij(k) =

(cid:96)k−1
(cid:88)

h=1

e−(µi+φi)(t(cid:96)k

−th) =

(cid:96)k−1
(cid:88)

h=1

e−(µi+φi)(t(cid:96)k

−th) +

(cid:96)k−1
(cid:88)

h=(cid:96)k−1+1

e−(µi+φi)(t(cid:96)k

−th)

= e−(µi+φi)(t(cid:96)k

−t(cid:96)k−1

)+(µi+φi)(t(cid:96)k

−t(cid:96)k−1

)

(cid:96)k−1
(cid:88)

h=1

e−(µi+φi)(t(cid:96)k

−th) +

(cid:96)k−1
(cid:88)

h=(cid:96)k−1+1

e−(µi+φi)(t(cid:96)k

−th)

= e−(µi+φi)(t(cid:96)k

−t(cid:96)k−1

)

(cid:96)k−1
(cid:88)

h=1


(cid:96)k−1
(cid:88)

h=(cid:96)k−1+1



e−(µi+φi)(t(cid:96)k−1

−th) +

e−(µi+φi)(t(cid:96)k

−th)

= e−(µi+φi)(t(cid:96)k

−t(cid:96)k−1

)

1 +

e−(µi+φi)(t(cid:96)k−1

−th)

 +

(cid:96)k−1−1
(cid:88)

h=1

(cid:96)k−1
(cid:88)

h=(cid:96)k−1+1

e−(µi+φi)(t(cid:96)k

−th),

which, using the deﬁnition of ψij(k
but the summation is splitted at (cid:96)(cid:48)
j)(t(cid:48)
φ(cid:48)
(cid:96)(cid:48)
k −

t(cid:48)
(cid:96)(cid:48)
k−1

)

−

k−1, and the ﬁrst summation is multiplied and divided by exp

. For ˜ψij(k), the decomposition is analogous to standard Hawkes processes.
}

ij(k) follows similar steps,
j +

(µ(cid:48)

{−

1), gives the result (A.3). The proof for ψ(cid:48)

The second part of the log-likelihood (2.6) is equivalent to Λij(T ) and follows from integration of λij(t)

over the observation period. For r = 1:

(cid:90) T

0

λij(t)dt = (αi + βj + γiγ(cid:48)

j)(T

min
{

−

T, τij

)
}

νiν(cid:48)
j
(νi + θi)(ν(cid:48)
ni(cid:88)

µi
µi + φi

k=1

−

−

nij
(cid:88)

k=1

(cid:104)
e−(νi+θi)(ν(cid:48)

j +θ(cid:48)

j )(t(cid:96)k+1

−t(cid:96)k

)

(cid:105)
1

−

j + θ(cid:48)
j)

1[τij ,∞)(tk)

(cid:104)

e−(µi+φi)(tk+1−tk)

(cid:105)

1

−

Sanna Passino, F. and Heard N. A.

22

Mutually exciting point process graphs for modelling dynamic networks

µ(cid:48)
j
µ(cid:48)
j + φ(cid:48)
j
µi
µi + φi
µ(cid:48)
j
j + φ(cid:48)
µ(cid:48)
j

−

−

−

n(cid:48)
j
(cid:88)

1[τij ,∞)(t(cid:48)
k)

(cid:104)
e−(µ(cid:48)

j +φ(cid:48)

j )(t(cid:48)

k+1−t(cid:48)
k)

k=1
(cid:104)
e−(µi+φi)(min{th:th≥τij }−max{th:th≤τij })

(cid:105)

1

−

e−(µi+φi)(τij −max{th:th≤τij })(cid:105)

−

(cid:104)

e−(µ(cid:48)

j +φ(cid:48)

j )(min{t(cid:48)

h:t(cid:48)

h≥τij }−max{t(cid:48)

h:t(cid:48)

h≤τij })

−

e−(µ(cid:48)

j +φ(cid:48)

j )(τij −max{t(cid:48)

h:t(cid:48)

h≤τij })(cid:105)

where tni+1 = t(cid:48)
j +1 = t(cid:96)nij +1 = T . Similarly, for r =
n(cid:48)

:

∞

(cid:90) T

0

λij(t)dt = (αi + βj + γiγ(cid:48)

j)(T

min
{

−

T, τij

ni(cid:88)

(cid:104)

e−(µi+φi)(T −tk)

−

)
}
e−(µi+φi) min{T,max{τij −tk,0}}(cid:105)

(cid:104)

e−(µ(cid:48)

j +φ(cid:48)

j )(T −t(cid:48)
k)

e−(µi+φi) min{T,max{τij −t(cid:48)

k,0}}(cid:105)

−

nij
(cid:88)

k=1

(cid:104)
e−(νi+θi)(ν(cid:48)

j +θ(cid:48)

j )(T −t(cid:96)k

(cid:105)

1

.

)

−

j + θ(cid:48)
j)

µi
µi + φi

−

k=1
n(cid:48)
j
(cid:88)

µ(cid:48)
j
µ(cid:48)
j + φ(cid:48)
j
k=1
νiν(cid:48)
j
(νi + θi)(ν(cid:48)

−

−

,

(A.4)

(A.5)

Note that (A.4) and (A.5) are monotonically decreasing functions in τij for any choice of the remaining
parameters, with constraint τij < t(cid:96)ij1, where t(cid:96)ij1 is the ﬁrst arrival time on the edge (i, j). Furthermore, τij
does not explicitly appear in the ﬁrst part of the likelihood, cf. (A.1) and (A.2). Therefore, using (2.6), the
maximum likelihood estimate for τij is simply ˆτij = t(cid:96)ij1 if at least one event is observed on the edge, and
ˆτij =

otherwise. If τij is set to its MLE, then the last two lines of (A.4) cancel out.

For the p-values in (4.1), for r =

and τij = 0, the difference between the compensators is calculated

sequentially at the observed times t1, . . . , tnij using ψij(k), ψ(cid:48)

ij(k) and ˜ψij(k):

∞

∞

Λij(tk)

−

Λij(tk−1) = (αi + βj + γiγ(cid:48)

j)(tk

tk−1)

−
Ni(tk)

[ψij(k)

−

ψij(k

−

−

1) + Ni(tk−1)]

µi
µi + φi
µ(cid:48)
j
j + φ(cid:48)
µ(cid:48)
j
νiν(cid:48)
j
(νi + θi)(ν(cid:48)

(cid:2)ψ(cid:48)

−

−

−

ij(k)

N (cid:48)

j(tk)

−

(cid:104) ˜ψij(k)

j + θ(cid:48)
j)

ψ(cid:48)

ij(k

−

1) + N (cid:48)

j(tk−1)(cid:3)

Nij(tk)

˜ψij(k

−

−

(cid:105)
1) + Nij(tk−1)

.

−

−

An expression similar to (A.4) can be used for Λij(tk)

Λij(tk−1) when r = 1.

−

B Calculation of the gradient for r =

∞

In the derivations of the gradient, the following notation is used:

χij(k) = αi + βj + γiγ(cid:48)

j + µiψij(k) + µ(cid:48)

jψ(cid:48)

ij(k) + νiν(cid:48)
j

˜ψij(k).

The partial derivative of log L(

H

T ; Ψ) with respect to αi and γi takes the following form:

∂
∂αi

log L(

H

T ; Ψ) =

n
(cid:88)

j=1

(cid:34)

1[τij ,∞)(T )

(T

−

T, τij
min
{

}

−

) +

(cid:35)

χij(k)−1

,

nij
(cid:88)

k=1

Sanna Passino, F. and Heard N. A.

23

Mutually exciting point process graphs for modelling dynamic networks

∂
∂γi

log L(

H

T ; Ψ) =

n
(cid:88)

j=1

(cid:34)

γ(cid:48)
j

1[τij ,∞)(T )

(T

−

min
{

−

T, τij

) +
}

(cid:35)

χij(k)−1

.

nij
(cid:88)

k=1

Similar equations can be derived for the partial derivatives with respect to βj and γ(cid:48)
j.

The calculations of the partial derivatives with respect to the parameters µi, φi, µ(cid:48)
cursive structure deﬁned in the previous section, since the expressions ψij(k) and ψ(cid:48)
(µi, φi) and (µ(cid:48)

j) respectively. For µi and φi:

j, φ(cid:48)

j and φ(cid:48)
j use the re-
ij(k) are functions of

∂
∂µi

log L(

H

T ; Ψ) =

1
µi + φi

n
(cid:88)

j=1

1[τij ,∞)(T )

ni(cid:88)

(cid:26) φi

µi + φi

k=1

(cid:104)
e−(µi+φi)(T −tk)

e−(µi+φi)τ +

ijk

(cid:105)

−

(cid:104)
(T

µi

−

tk)e−(µi+φi)(T −tk)

ijke−(µi+φi)τ +
τ +

ijk

−

(cid:105) (cid:27)

n
(cid:88)

j=1

1[τij ,∞)(T )

nij
(cid:88)

k=1

(cid:20)

1
χij(k)

ψij(k) + µi

(cid:21)

ψij(k)

,

∂
∂µi

−

+

∂
∂φi

log L(

H

T ; Ψ) =

µi
µi + φi

−

n
(cid:88)

j=1

1[τij ,∞)(T )

ni(cid:88)

(cid:26) 1

(cid:104)

µi + φi

k=1

e−(µi+φi)(T −tk)

e−(µi+φi)τ +

ijk

(cid:105)

−

(cid:104)

+

(T

+ µi

tk)e−(µi+φi)(T −tk)

ijke−(µi+φi)τ +
τ +

ijk

−

(cid:105) (cid:27)

1[τij ,∞)(T )

nij
(cid:88)

k=1

1
χij(k)

∂
∂φi

ψij(k),

−
n
(cid:88)

j=1

T, max
ijk = min
{

where τ +
τij
{
respect to µi and φi is computed recursively as follows:
(cid:26) ∂
∂φi

ψij(k) = e−(µi+φi)(t(cid:96)k

ψij(k) =

∂
∂µi

∂
∂φi

−t(cid:96)k−1

tk, 0

}}

−

)

ψij(k

1)

(t(cid:96)k −

−

−

t(cid:96)k−1) [1 + ψij(k

(cid:27)

1)]

−

. In the above expression, the partial derivative of ψij(k) with

(cid:96)k−1
(cid:88)

−

h=(cid:96)k−1+1

(t(cid:96)k −

th)e−(µi+φi)(t(cid:96)k

−th),

Similar considerations can be made for the partial derivatives with respect to (νi, θi) and (ν(cid:48)
j). In this
case, the recursive form stems from ˜ψij(k), which is function of the two pairs of parameters. For νi and θi:

j, θ(cid:48)

∂
∂νi

log L(

H

T ; Ψ) =

n
(cid:88)

j=1

1[τij ,∞)(T )ν(cid:48)
j
νi + θi

νi(T

−

−

t(cid:96)k )e−(νi+θi)(ν(cid:48)

j +θ(cid:48)

j )(T −t(cid:96)k

(cid:40)

nij
(cid:88)

θi

(cid:104)

(νi + θi)(ν(cid:48)

k=1
(cid:41)

)

+

n
(cid:88)

nij
(cid:88)

j + θ(cid:48)
j)
1[τij ,∞)(T )ν(cid:48)
j
χij(k)

e−(νi+θi)(ν(cid:48)

j +θ(cid:48)

j )(T −t(cid:96)k

(cid:105)

1

)

−

(cid:20)
˜ψij(k) + νi

(cid:21)
˜ψij(k)

,

∂
∂νi

∂
∂θi

log L(

H

T ; Ψ) =

νi
νi + θi

−

n
(cid:88)

j=1

1

(cid:104)

(νi + θi)(ν(cid:48)

j + θ(cid:48)
j)

e−(νi+θi)(ν(cid:48)

j +θ(cid:48)

j )(T −t(cid:96)k

(cid:105)
1

)

−

+ (T

−

t(cid:96)k )e−(νi+θi)(ν(cid:48)

j +θ(cid:48)

j )(T −t(cid:96)k

1
χij(k)

∂
∂θi

˜ψij(k).

j=1

1[τij ,∞)(T )ν(cid:48)
j

k=1
nij
(cid:88)

(cid:40)

(cid:41)

)

+ νi

k=1
nij
(cid:88)

n
(cid:88)

j=1

k=1

Sanna Passino, F. and Heard N. A.

24

Mutually exciting point process graphs for modelling dynamic networks

The recursive equations for the partial derivative of ˜ψij(k) with respect to νi and θi are equivalent. For θi:

∂
∂θi

˜ψij(k) = e−(νi+θi)(ν(cid:48)

j +θ(cid:48)

j )(t(cid:96)k

−t(cid:96)k−1

)

(cid:26) ∂
∂θi

˜ψij(k

1)

−

−

(ν(cid:48)

j + θ(cid:48)

j)(t(cid:96)k −

(cid:104)

1 + ˜ψij(k

t(cid:96)k−1)

(cid:105)(cid:27)

1)

,

−

Similarly to the previous cases, the initial condition is:

∂
∂θi

˜ψij(1) = 0.

Sanna Passino, F. and Heard N. A.

25

