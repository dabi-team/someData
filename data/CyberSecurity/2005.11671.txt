1
2
0
2

g
u
A
1
3

]

R
C
.
s
c
[

3
v
1
7
6
1
1
.
5
0
0
2
:
v
i
X
r
a

Arms Race in Adversarial Malware Detection: A Survey

DEQIANG LI, Nanjing University of Science and Technology, China
QIANMU LI, Nanjing University of Science and Technology, China
YANFANG (FANNY) YE, Case Western Reserve University, USA
SHOUHUAI XU, University of Colorado Colorado Springs, USA

Malicious software (malware) is a major cyber threat that has to be tackled with Machine Learning (ML)
techniques because millions of new malware examples are injected into cyberspace on a daily basis. However,
ML is vulnerable to attacks known as adversarial examples. In this paper, we survey and systematize the field
of Adversarial Malware Detection (AMD) through the lens of a unified conceptual framework of assumptions,
attacks, defenses, and security properties. This not only leads us to map attacks and defenses to partial
order structures, but also allows us to clearly describe the attack-defense arms race in the AMD context. We
draw a number of insights, including: knowing the defender’s feature set is critical to the success of transfer
attacks; the effectiveness of practical evasion attacks largely depends on the attacker’s freedom in conducting
manipulations in the problem space; knowing the attacker’s manipulation set is critical to the defender’s
success; the effectiveness of adversarial training depends on the defender’s capability in identifying the most
powerful attack. We also discuss a number of future research directions.

CCS Concepts: • Security and privacy → Malware and its mitigation; • Theory of computation →
Adversarial learning;

Additional Key Words and Phrases: Malware Detection, Adversarial Machine Learning, Evasion Attacks,
Poisoning Attacks

ACM Reference Format:
Deqiang Li, Qianmu Li, Yanfang (Fanny) Ye, and Shouhuai Xu. 2021. Arms Race in Adversarial Malware
Detection: A Survey. ACM Comput. Surv. 1, 1 (September 2021), 35 pages. https://doi.org/10.1145/nnnnnnn.
nnnnnnn

1 INTRODUCTION
Malware (malicious software) is a big cyber threat and has received a due amount of attention.
For instance, Kaspersky reports that 21,643,946 unique malicious files were detected in the year
2018, 24,610,126 in 2019, and 33,412,568 in 2020 [47, 72]. A popular defense against malware is
to use signature-based detectors [44], where a signature is often extracted by malware analysts
from known malware examples. This approach has two drawbacks: signatures are tedious to
extract and can be evaded [34] by a range of techniques (e.g., encryption, repacking, polymorphism
[7, 21, 63, 86, 106, 111, 139]). This incompetence has motivated the use of Machine Learning (ML)

Work was partly done when Shouhuai Xu was affiliated with University of Texas at San Antonio, One UTSA Circle, San
Antonio, Texas 78249, USA.
Authors’ addresses: Deqiang Li, Nanjing University of Science and Technology, China; Qianmu Li, Nanjing University of
Science and Technology, China; Yanfang (Fanny) Ye, Case Western Reserve University, 10900 Euclid Ave. Cleveland, Ohio,
44106, USA; Shouhuai Xu, University of Colorado Colorado Springs, 1420 Austin Bluffs Pkwy, Colorado Springs, Colorado,
80918, USA, sxu@uccs.edu.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2021 Association for Computing Machinery.
0360-0300/2021/9-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

 
 
 
 
 
 
:2

D. Li, Q. Li, F. Ye, and S. Xu

based malware detectors, which can be automated to some degree and can possibly detect new
malware examples (via model generalization or knowledge adaptation [8, 11, 43, 45, 56, 57, 64, 66,
84, 121, 137, 138, 143]). More recently, Deep Learning (DL) has been used for malware detection
(see, e.g., [99, 103, 125]).

While promising, ML-based malware detectors are vulnerable to attacks known as adversarial
examples [16, 60, 114]. There are two kinds of attacks. One is evasion attack, where the attacker
perturbs test examples to adversarial examples to evade malware detectors [2, 16, 39, 53, 57, 91, 120,
127]. The other is poisoning attack, where the attacker manipulates the training dataset for learning
malware detectors [30, 40, 118]. These attacks usher in the new field of Adversarial Malware
Detection (AMD) [16, 27, 36, 39, 40, 53, 114, 118, 127, 133, 134].

The state-of-the-art in AMD is that there are some specific results scattered in the literature
but there is no systematic understanding. This is true despite that there have been attempts at
systematizing the related field of Adversarial Machine Learning (AML) [9, 26, 60, 140], which
however cannot be automatically translated to AMD. This is so because malware detection has
three unique characteristics which are not exhibited by the other application domains (e.g., image or
audio processing). (i) There are no common, standard feature definitions because both attackers and
defenders can define their own features to represent computer files. As a consequence, attackers can
leverage this “freedom” in feature definition to craft adversarial examples. (ii) Malware features are
often discrete rather than continuous and program files are often highly structured with multiple
modalities. This means that arbitrarily perturbing malware files or their feature representations
might make the perturbed files no more executable. This also means that the discrete domain
makes perturbation a non-differentiable and non-convex task. (iii) Any meaningful perturbation
to a malware example or its feature representation must preserve its malicious functionality. For
example, the Android Package Kit (APK) requires that the used permissions are publicized in
the AndroidManifest.xml, meaning that removing permissions in this manifest file would incur
a runtime error. The preceding (ii) and (iii) make both the attacker’s and defender’s tasks more
challenging than their counterparts where small perturbations are not noticeable (e.g., images).
Our Contributions. We propose a conceptual framework for systematizing the AMD field through
the lens of assumptions, attacks, defenses, and security properties. In specifying these, we seek
rigorous definitions whenever possible, while noting that these definitions have been scattered in
the literature. Rigorous definitions are important because they can serve as a common reference for
future studies. The framework allows us to map the known attacks and defenses into some partial
order structures and systematize the AMD attack-defense arms race.

We make a number of observations, including: (i) the indiscriminate attack that treats malicious
examples as equally important has been extensively investigated, but targeted and availability
attacks are much less investigated; (ii) the evasion attack is much more extensively studied than the
poisoning attack; (iii) there is no silver-bullet defense against evasion and poisoning attacks; (iv)
sanitizing examples is effective against black-box and grey-box attacks, but not white-box attacks;
(v) AMD security properties have been evaluated empirically rather than rigorously; (vi) there is
no theoretical evidence to support that the effectiveness of defense techniques on the training set
can generalize to other adversarial examples.

We draw a number of insights, including: (i) knowing defender’s feature set is critical to the
success of transfer attacks, highlighting the importance of keeping defender’s feature set secret
(e.g., by randomizing defender’s feature set); (ii) the effectiveness of practical evasion attacks largely
depends on the attacker’s degree of freedom in conducting manipulations in the problem space
(i.e., a small degree of freedom means harder to succeed); (iii) effective defenses often require the
defender to know the attacker’s manipulation set, explaining from one perspective why it is hard

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:3

to design effective defenses; (iv) effectiveness of adversarial training depends on the defender’s
capability in identifying the most powerful attack.

Finally, we discuss a number of future research directions, which hopefully will inspire and

encourage many researchers to explore them.
Related Work. The closely related prior work is Maiorca et al. [81], which surveys previous studies
in adversarial malicious PDF document detection. In contrast, we consider the broader context
of AMD and propose novel partial orders to accommodate AMD assumptions, attacks, defenses,
and properties. There are loosely-related prior studies, which survey prior AML studies (but not
focusing on AMD), including [9, 10, 17, 26, 79, 94, 105, 140]. For example, Yuan et al. [140] survey
attack methods for generating adversarial examples, while briefly discussing evasion attacks in the
AMD context; Barreno et al. [9, 10] propose a taxonomy of AML attacks (causative vs. exploratory
attacks, integrity vs. availability attacks, and targeted vs. indiscriminate attacks); Biggio et al. [17]
propose a defense framework for protecting Support Vector Machines (SVMs) from evasion attacks,
poisoning attacks and privacy violations; Papernot et al. [94] systematize AML security and privacy
with emphasis on demonstrating the trade-off between detection accuracy and robustness.
Paper Outline. Section 2 describes our survey and systematization methodology and framework.
Section 3 applies our framework to systematize the literature AMD studies. Section 4 discusses
future research directions. Section 5 concludes the paper.

2 SURVEY AND SYSTEMATIZATION METHODOLOGY
Terminology, Scope and Notations. In the AMD context, a defender I aims to use ML to detect
or classify computer files as benign or malicious; i.e., we focus on binary classification. An attacker
A attempts to make malicious files evade I’s detection by leveraging adversarial files (interchange-
ably, adversarial examples). Adversarial malware examples are often generated by perturbing or
manipulating malware examples, explaining why we will use the two terms, perturbation and
manipulation, interchangeably. Adversarial attacks can be waged in the training phase of a ML
model (a.k.a., poisoning attack) or in the test phase (a.k.a., evasion attack). It is worth mentioning
that the privacy violation attack [60] is waged in addition to the preceding two attacks because A
can always probe I’s detectors. A file, benign and malicious alike, is adversarial if it is intentionally
crafted to (help malicious files) evade I’s detection, and non-adversarial otherwise. We focus on I
using supervised learning to detect malicious files, which may be adversarial or non-adversarial
because they co-exist in the real world with no self-identification. This means that we do not
consider the large body of malware detection literature that does not cope with AMD, which has
been addressed elsewhere (e.g., [101]). Table 1 summarizes the main notations used in the paper.

2.1 Brief Review on ML-based Malware Detection
Let Z be the example space of benign/malicious adversarial/non-adversarial files. Let Y = {+, −}
or Y = {1, 0} be the label space of binary classification, where +/1 (−/0) means a file is malicious
(benign). Let D = Z × Y be the file-label (example-label) space. For training and evaluating
a classifier in the absence of adversarial files, I is given a set 𝐷 ⊂ D of non-adversarial be-
nign/malicious files as well as their ground-truth labels. I splits 𝐷 into three disjoint sets: a
training set 𝐷𝑡𝑟𝑎𝑖𝑛 = {(𝑧𝑖, 𝑦𝑖 )}𝑛
𝑖=1, a validation set for model selection, and a test set for evaluation.
Each file 𝑧𝑖 ∈ Z is characterized by a set 𝑆 of features and represented by a numerical vector
x𝑖 = (𝑥𝑖,1, . . . , 𝑥𝑖,𝑑 ) in the 𝑑-dimensional feature space X = R𝑑 , which accommodates both continu-
ous and discrete feature representations [1, 55, 56, 69, 113, 121]. The process for obtaining feature
representation x𝑖 of 𝑧𝑖 ∈ Z is called feature extraction, denoted by a function 𝜙 : Z → X with
x𝑖 ← 𝜙 (𝑆, 𝑧𝑖 ). Because 𝜙 can be hand-crafted (denoted by 𝜙𝑐 ), automatically learned (denoted by

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:4

Notation
R (R+)
A, I
P
𝑧, 𝑧 ′ ∈ Z
𝑆
x, x′ ∈ X

Y, 𝑦
D = Z × Y
𝐷𝑡𝑟𝑎𝑖𝑛 ⊂ D, 𝑛
𝐷𝑡𝑒𝑠𝑡
𝐷𝑝𝑜𝑖𝑠𝑜𝑛, 𝐷 ′

𝑝𝑜𝑖𝑠𝑜𝑛

O (𝑧, 𝑧 ′)

𝛿
M, ZM ⊆ Z

M, XM ⊆ X

Γ(𝑧, 𝑧 ′)

𝐶 (x, x′)

𝛿𝑧 ∈ M
𝛿x ∈ M
𝜙 : Z → X
𝜑, 𝑓

𝐹𝜃 : X → R
𝐿 : R × Y → R
EL, WR, AT

D. Li, Q. Li, F. Ye, and S. Xu

Table 1. Main notations used in the paper

𝑝𝑜𝑖𝑠𝑜𝑛 is set of adversarial file-label pairs obtained by perturbing non-

Meaning
the set of (positive) real numbers
attacker and defender (treated as algorithms)
the probability function
Z is example space; 𝑧 ′ is obtained by perturbing 𝑧
defender I’s feature set for representing files
X = R𝑑 is 𝑑-dimensional feature space; x, x′ ∈ X are respectively feature
representations of 𝑧, 𝑧 ′ ∈ Z
Y is the label space of binary classification, Y = {+/1, −/0}; 𝑦 ∈ Y
the file-label (i.e., example-label) space
the training set in file-label space; 𝑛 = |𝐷𝑡𝑟𝑎𝑖𝑛 |
the test set in file-label space
𝐷 ′
adversarial files in 𝐷𝑝𝑜𝑖𝑠𝑜𝑛 ⊂ D
O (𝑧, 𝑧 ′) : Z × Z → {true, false} is an oracle telling if two files have the same
functionality or not
a manipulation for perturbing files with preserving their functionalities
M is manipulation set in the problem space; ZM is set of adversarial files
generated using M
M is feature manipulation set; XM is set of adversarial feature vectors generated
using M
Γ(𝑧, 𝑧 ′) : Z × Z → R+ measures the degree of manipulation for perturbing
𝑧 ∈ Z into 𝑧 ′ ∈ Z
𝐶 (x, x′) : X × X → R+ is the function measuring the cost incurred by changing
feature vector x to x′
𝛿𝑧 is a set of manipulations of 𝑧 w.r.t. 𝑧 ′
𝛿x = x′ − x is a perturbation vector of x w.r.t. x′
feature extraction function; x ← 𝜙 (𝑧), x′ ← 𝜙 (𝑧 ′)
𝜑 : X → R is classification function; 𝑓 : Z → R is classifier 𝑓 = 𝜑 (𝜙 (·)); by
abusing notation a little bit, we also use “+ ← 𝑓 (𝑧)” to mean that 𝑓 predicts 𝑧
as malicious when 𝑓 (𝑧) ≥ 𝜏 for a threshold 𝜏
machine learning algorithm with parameters 𝜃
loss function measuring prediction error of 𝐹𝜃
defense techniques: Ensemble Learning, Weight Regularization, Adversarial
Training

VL, RF, IT, CD, SE defense techniques: Verifiable Learning, Robust Feature, Input Transformation,

BE, OE, BP, OP
GO,SF,MI
TR,HS,GM,MS

𝐴1, . . . , 𝐴5

𝐴6, . . . , 𝐴9

RR, CR, DR, TR

Classifier ranDomization, Sanitizing Examples
attack tactics: basic and optimal evasion; basic and optimal poisoning
attack techniques: Gradient-based Optimization, Sensitive Features, MImicry
attack techniques: TRansferability, Heuristic Search, Generative Model, Mixture
Strategy
the 5 attributes under I’s control; they are known to A at respective degrees
𝑎1, . . . , 𝑎5
the 4 attributes under A’s control; they are known to I at respective degree
𝑎6, . . . , 𝑎9
security properties: Representation Robustness, Classification Robustness, De-
tection Robustness, Training Robustness

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:5

𝜙𝑎), or a hybrid of both [13], we unify them into 𝜙 such that 𝜙 (𝑆, 𝑧) = 𝜙𝑎 (𝜙𝑐 (𝑆, 𝑧)); when only
manual (automatic) feature extraction is involved, we can set 𝜙𝑎 (𝜙𝑐 ) as the identity map. There are
two kinds of features: static features are extracted via static analysis (e.g., strings, API calls [4, 129]);
dynamic features are extracted via dynamic analysis (e.g., instructions, registry activities [42, 64]).

Fig. 1. Illustration of ML-based malware detector.

As highlighted in Figure 1, I uses {(𝑧𝑖, 𝑦𝑖 )}𝑛

𝑖=1 to learn a malware detector or classifier 𝑓

:
Z → [0, 1], where 𝑓 (𝑧) = 𝜑 (𝜙 (𝑆, 𝑧)) is composed of feature extraction function 𝜙 : Z → X
and classification function 𝜑 : X → [0, 1]. Note that 𝑓 (𝑧) ∈ [0, 1], namely 𝜑 (x) ∈ [0, 1] with
x ← 𝜙 (𝑆, 𝑧), can be interpreted as the probability that 𝑧 is malicious (while noting that calibration
may be needed [90]). For a given threshold 𝜏 ∈ [0, 1], we further say (by slightly abusing notations)
𝑧 is labeled by 𝑓 as +, or + ← 𝑓 (𝑧), if 𝑓 (𝑧) ≥ 𝜏, and labeled as − or − ← 𝑓 (𝑧) otherwise. In practice,
𝑓 is often specified by a learning algorithm 𝐹 with learnable parameter 𝜃 (e.g., weights) and a
hand-crafted feature extraction 𝜙𝑐 ; then, 𝜃 is tuned to minimize the empirical risk associated with
a loss function 𝐿 : [0, 1] × Y → R measuring the prediction error of 𝐹𝜃 [124] (e.g., cross-entropy
[49]), namely

min
𝜃

L (𝜃, 𝐷𝑡𝑟𝑎𝑖𝑛) = min

𝜃

1
𝑛

∑︁

(𝑧𝑖,𝑦𝑖 ) ∈𝐷𝑡𝑟𝑎𝑖𝑛

(𝐿(𝐹𝜃 (𝜙𝑐 (𝑆, 𝑧𝑖 )), 𝑦𝑖 )) .

(1)

Example 1: The Drebin malware detector. Drebin is an Android malware detector trained from
static features [8]. Table 2 summarizes Drebin’s feature set, which includes 4 subsets of features

Table 2. Drebin features

Feature set

𝑆1 Hardware components
𝑆2 Requested permissions
𝑆3 App components
𝑆4 Filtered intents
𝑆5 Restricted API calls
𝑆6 Used permissions
𝑆7 Suspicious API calls
𝑆8 Network addresses

Manifest

Dexcode

𝑆1, 𝑆2, 𝑆3, 𝑆4 extracted from the AndroidManifest.xml and another 4 subsets of features 𝑆5, 𝑆6, 𝑆7, 𝑆8
extracted from the disassembled DEX code files (recalling that DEX code is compiled from a program
written in some language and can be understood by the Android Runtime). Specifically, 𝑆1 contains
features related to the access of an Android package (APK) to smartphone hardware (e.g., camera,

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

 ?Malicious/Benign filesNewfileTrainingTestingExtractingPredicting 𝑧𝑖 𝑖=1𝑛 𝜙 𝑧 𝐱  𝐱𝑖 𝑖=1𝑛 𝜑 MaliciousBenignFeature extractionClassification function 𝑦𝑖 𝑖=1𝑛 Ground truthMalicious/BenignDecision ?Training filesNewfileTrainingTestingExtractingPredicting 𝑧𝑖 𝑖=1𝑛 𝜙 𝑧 𝐱  𝐱𝑖 𝑖=1𝑛 MaliciousBenignFeature extraction 𝑦𝑖 𝑖=1𝑛 Ground truthMalicious/BenignDecision𝜑 Classification function𝑛  𝑥𝑛,1 𝑥𝑛,𝑑  𝑥2,1 𝑥2,𝑑  𝑥1,1 𝑥1,𝑑 :6

D. Li, Q. Li, F. Ye, and S. Xu

touchscreen, or GPS module); 𝑆2 contains features related to APK’s requested permissions listed
in the manifest prior to installation; 𝑆3 contains features related to application components (e.g.,
activities, service, receivers, and providers.); 𝑆4 contains features related to APK’s communications
with the other APKs; 𝑆5 contains features related to critical system API calls, which cannot run
without appropriate permissions or the root privilege; 𝑆6 contains features corresponding to the
used permissions; 𝑆7 contains features related to API calls that can access sensitive data or resources
in a smartphone; and 𝑆8 contains features related to IP addresses, hostnames and URLs found in
the disassembled codes. The feature representation is binary, meaning 𝜙 = 𝜙𝑐 : Z ↦→ {0, 1}𝑑 with
|𝑆 | = 𝑑 and x = (𝑥1, . . . , 𝑥𝑑 ), where 𝑥𝑖 = 1 if the corresponding feature is present in the APK 𝑧 and
𝑥𝑖 = 0 otherwise. A file 𝑧 in the feature space looks like the following:

(cid:27)

(cid:27)

𝑆2

(cid:170)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:172)

x = 𝜙 (𝑧) →

· · ·
0
1
· · ·
1
0
· · ·

· · ·
permission::SEND_SMS
permission::READ_CONTACTS
· · ·
api_call::getDeviceID
api_call::setWifiEnabled
· · ·

(cid:169)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:171)
Drebin uses a linear Support Vector Machine (SVM) to learn classifiers.
Example 2: The MalConv malware detector. MalConv [99] is Convolutional Neural Network
(CNN)-based Windows Portable Executable (PE) malware detector learned from raw binary pro-
grams (i.e., end-to-end detection) [67]. Figure 2 depicts its architecture. The sequence of binary code
is transformed into byte values (between 0 to 255) with the maximum length bounded by 𝑁𝑚𝑎𝑥
(e.g., 𝑁𝑚𝑎𝑥 = 221 bytes or 2MB). Each byte is further mapped into a real-valued vector using the
embedding [35]. The CNN layer and pooling layer learn abstract representations. The embedding,
CNN and pooling layers belong to feature extraction 𝜙𝑎, and the fully-connected and softmax layers
belong to the classification operation 𝜑.

𝑆5

Fig. 2. MalConv architecture [99].

2.2 Framework
We systematize AMD studies through the lens of four aspects: (i) the assumptions that are made;
(ii) the attack or threat model in terms of attacker A’s objective and A’s input, with the latter
including A’s information about the defender I and A’s own; (iii) the defense in terms of I’s
objective and I’s input, with the latter including I’s information about A and I’s own; (iv) the
security properties that are at stake. These four aspects are respectively elaborated below.

2.2.1

Systematizing Assumptions. Five assumptions have been made in the AMD literature.
Assumption 1 below says that the data samples in 𝐷 are Independent and Identically Distributed
(IID), which is a strong assumption and researchers have started to weaken it [52, 110].

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

EXE11maxkkNbbbbEmbeddingmkkeeee11Convolutional layerConvolutional layerTemporal max poolingFully connectedSoftmaxByte valuexxArms Race in Adversarial Malware Detection: A Survey

:7

Assumption 1 (IID assumption; see, e.g., [107]). Computer files in training data and testing data

are independently drawn from the same distribution.

Assumption 2 below is adapted from AML context, where humans can serve as an oracle O for
determining whether two images are the same [126]. In the AMD context, O can be instantiated as
(or approximated by) malware analysts [2, 27, 65, 114] or automated tools (e.g., Sandbox [36, 134]),
with the latter often using heuristic rules produced by malware analysts (e.g., YARA [3]).

Assumption 2 (Oracle assumption; adapted from [126]). There is an oracle O : Z × Z →
{true, false} that tells if two files 𝑧, 𝑧 ′ ∈ Z have the same functionality or not; true ← O (𝑧, 𝑧 ′) if
and only if 𝑧 and 𝑧 ′ have the same functionality.

Assumption 3 below says that there is a way to measure the degree of manipulations by which

one file is transformed to another.

Assumption 3 (Measurability assumption [36, 68]). There is a function Γ(𝑧, 𝑧 ′) : Z × Z → R+
that measures the degree of manipulations according to which a file 𝑧 ′ ∈ Z can be derived from the
file 𝑧 ∈ Z.

Since Assumption 3 is often difficult to validate, Γ(𝑧, 𝑧 ′) may be replaced by a function that
quantifies the degree of manipulation that can turn feature representation x into x′, where x =
𝜙 (𝑆, 𝑧) and x′ = 𝜙 (𝑆, 𝑧 ′). This leads to:

Assumption 4 (Smoothness assumption [13]). There is a function 𝐶 (x, x′) : X × X → R+ such

that 𝐶 (𝜙 (𝑆, 𝑧), 𝜙 (𝑆, 𝑧 ′)) ≈ 0 when (Γ(𝑧, 𝑧 ′) ≈ 0) ∧ (true ← O (𝑧, 𝑧 ′)).

Assumption 5 below says that the inverse of feature extraction, 𝜙 −1, is solvable so that a perturbed

representation x′ can be mapped back to a legitimate file.

Assumption 5 (Invertibility assumption [76]). Feature extraction 𝜙 is invertible, meaning that

given x′, the function 𝜙 −1 : X → Z produces 𝑧 ′ = 𝜙 −1(x′).

2.2.2

Recall that the feature extraction function 𝜙 may be composed of a hand-crafted 𝜙𝑐 and an
automated 𝜙𝑎, where 𝜙𝑐 may be neither differentiable nor invertible [16, 97]. This means x′ may
not be mapped to a legitimate file. Researchers tend to relax the assumption by overlooking the
interdependent features [76, 114], while suffering from the side-effect x′ ≠ 𝜙 (𝜙 −1(x′)) [97, 114].
Systematizing Attacks. We systematize attacks from two perspectives: attacker’s objective
(i.e., what the attacker attempts to accomplish) and attacker’s input (i.e., what leverages the attacker
can use). Whenever possible, we seek rigorous definitions to specify the attacker’s input, while
noting that these definitions have been scattered in the literature. We believe this specification is
important because it can serve as a common reference for future studies. To demonstrate this, we
discuss how to apply it to formulate a partial-order structure for comparing attacks.
Attacker’s Objective. There are three kinds of objectives: (i) Indiscriminate, meaning A attempts
to cause as many false-negatives as possible [6, 28, 53, 100, 127, 136]; (ii) Targeted, meaning A
attempts to cause specific false-negatives (i.e., making certain malicious files evade the detection
[40, 118]); (iii) Availability, meaning A attempts to frustrate defender I by rendering I’s classifier
𝑓 unusable (e.g., causing substantially high false-positives [20, 30, 40, 87, 98]).
Attacker’s Input. Table 3 highlights the attributes we define to describe A’s input, including: five
attributes 𝐴1, . . . , 𝐴5 that are under I’s control (indicated by 1) but may be known to A at some
extent 𝑎1, . . . , 𝑎5, respectively; and four attributes 𝐴6, . . . , 𝐴9 that are under A’s control (indicated
by 1). These attributes are elaborated below.

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:8

D. Li, Q. Li, F. Ye, and S. Xu

Table 3. Attributes for specifying A’s and I’s input.

Attacker A’s input Defender I’s input
Attributes
Attributes under I’s control but may be known to A to some extent
𝐴1: Training set 𝐷𝑡𝑟𝑎𝑖𝑛
𝐴2: Defense technique
𝐴3: Feature set 𝑆
𝐴4: Learning algorithm 𝐹𝜃
𝐴5: Response
Attributes under A’s control but may be known to I to some extent
𝐴6: Manipulation set M
𝐴7: Attack tactic
𝐴8: Attack technique
𝐴9: Adversarial examples

𝑎1 ∈ [0, 1]
𝑎2 ∈ {0, 1}
𝑎3 ∈ [0, 1]
𝑎4 ∈ [0, 1]
𝑎5 ∈ {0, 1}

𝑎6 ∈ [0, 1]
𝑎7 ∈ {0, 1}
𝑎8 ∈ {0, 1}
𝑎9 ∈ [0, 1]

1
1
1
1
1

1
1
1
1

(i) 𝐴1: it describes I’s training set 𝐷𝑡𝑟𝑎𝑖𝑛 for learning classifier 𝑓 . We use 𝑎1 ∈ [0, 1] to represent
the extent at which 𝐷𝑡𝑟𝑎𝑖𝑛 is known to A. Let ˆ𝐷𝑡𝑟𝑎𝑖𝑛 be the training files that are known to A.
Then, 𝑎1 = | ˆ𝐷𝑡𝑟𝑎𝑖𝑛 ∩ 𝐷𝑡𝑟𝑎𝑖𝑛 |/|𝐷𝑡𝑟𝑎𝑖𝑛 |.

(ii) 𝐴2: it describes I’s techniques, which can be Ensemble Learning (EL), Weight Regularization
(WR), Adversarial Training (AT), Verifiable Learning (VL), Robust Feature (RF), Input Transformation
(IT), Classifier ranDomization (CD), Sanitizing Examples (SE). Let 𝐴2 ∈ {EL,WR,AT,VL,RF,IT,CD,SE}
and 𝑎2 ∈ {0, 1} such that 𝑎2 = 0 means A does not know I’s techniques and 𝑎2 = 1 means A
knows I’s technique. The techniques are defined as follows. Definition 1 says that I constructs
multiple classifiers and uses them collectively in malware detection.

Definition 1 (ensemble learning or EL [142]). Let H be I’s classifier space. Given 𝐾 classifiers
𝑖=1 where 𝑓𝑖 ∈ H and 𝑓𝑖 : Z → [0, 1], let 𝑓𝑖 be assigned with weight 𝜔𝑖 with (cid:205)𝐾
𝑖=1 𝜔𝑖 = 1 and

{𝑓𝑖 }𝐾
𝜔𝑖 ≥ 0. Then, 𝑓 = (cid:205)𝐾

𝑖=1 𝜔𝑖 𝑓𝑖 .

Definition 2 says that I uses regularization (e.g., ℓ2 regularization [89] or dropout [112]) to

decrease model’s sensitivity to adversarial examples.

Definition 2 (weight regularization or WR [49]). Given a regularization item Ω (e.g., con-
[L (𝜃, 𝐷𝑡𝑟𝑎𝑖𝑛) + Ω(𝜃 )], where

straints imposed on the learnable parameters), the empirical risk is min
L is defined in Eq. (1).

𝜃

Definition 3 says that I proactively makes its classifier 𝑓 perceive some information about
adversarial files. That is, I augments the training set by incorporating adversarial examples that
may be produced by I, A, or both.

Definition 3 (adversarial training or AT [53]). Let 𝐷 ′ denote a set of adversarial file-label

pairs. Then, I tunes model parameters by minimizing the empirical risk: min
where 𝛽 ≥ 0 denotes a balance factor.

𝜃

[L (𝜃, 𝐷𝑡𝑟𝑎𝑖𝑛) + 𝛽L (𝜃, 𝐷 ′)],

Definition 4 says that I intentionally over-estimates the error incurred by A’s manipulations

and then minimizes it.

Definition 4 (verifiable learning or VL [130]). Given (𝑧, 𝑦) ∈ 𝐷𝑡𝑟𝑎𝑖𝑛 and a manipulation set
ˆM known by I, let 𝑧 ( ˆM) denote the upper and lower boundaries on
ˆM. Then, this defense technique
minimizes the following loss function derived from Eq.(1): 𝐿(𝐹𝜃 (𝜙𝑐 (𝑆, 𝑧)), 𝑦)+𝛽𝐿(𝐹𝜃 (𝜙𝑐 (𝑆, 𝑧 ( ˆM))), 𝑦).

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:9

Definition 5 says that I uses a set of features 𝑆 ∗ ⊆ 𝑆 that can lead to higher detection capability

against adversarial example attacks.

Definition 5 (robust feature or RF; adapted from [141]). Given a training set 𝐷𝑡𝑟𝑎𝑖𝑛 ∪ 𝐷 ′

that contains (adversarial) file-label pairs, the set of robust feature set 𝑆 ∗ is
𝐿((cid:101)𝐹𝜃 (𝜙𝑐 ( ˜𝑆, 𝑧)), 𝑦),

𝑆 ∗ = arg min

∑︁

˜𝑆 ⊂𝑆

(𝑧,𝑦) ∈𝐷𝑡𝑟𝑎𝑖𝑛∪𝐷′

where (cid:101)𝐹𝜃 is 𝐹𝜃 or a simplified learning algorithm that is computationally faster than 𝐹𝜃 [141].

Definition 6 says that I aims to use non-learning methods (e.g., de-obfuscation as shown in

Proguard [14]) to offset A’s manipulations.

Definition 6 (input transformation or IT, adapted from [28]). Let IT : Z → Z denote
an input transformation in the file space. Given file 𝑧 and transformation IT, the classifier is 𝑓 =
𝜑 (𝜙 (IT(𝑧))).

Definition 7 says that I randomly chooses 𝑚 classifiers and uses their results for prediction.
That is, I aims to randomize the feature representation used by 𝑓 , the learning algorithm, and/or
response to A’s queries (to prevent A from inferring information about 𝑓 ).

Definition 7 (classifier randomization or CD; adapted from [65]). Given I’s classifier
space H and an input file 𝑧, I randomly selects 𝑚 classifiers from H with replacement, say {𝑓𝑖 }𝑚
𝑖=1.
Then, 𝑓 = 1
𝑚

𝑖=1 𝑓𝑖 (𝑧).

(cid:205)𝑚

Instead of enhancing malware detectors, Definition 8 provides an alternative that detects the

adversarial examples for further analysis.

Definition 8 (sanitizing examples or SE; adapted from [23, 30]). I aims to detect adversarial

files by using function flag : Z → {yes, no} to flag a file as adversarial (yes) or not (no).

(iii) 𝐴3: it describes I’s feature set 𝑆. We use 𝑎3 ∈ [0, 1] to represent the extent at which A

knows about 𝑆. Let ˆ𝑆 denote the features that are known to A. Then, 𝑎3 = | ˆ𝑆 ∩ 𝑆 |/|𝑆 |.

(iv) 𝐴4: it describes I’s learning algorithm 𝐹𝜃 , the set of trainable parameters 𝜃 , and hyperpa-
rameters (which are set manually, e.g., 𝛽 in Definition 3) [40, 88]. We use 𝑎4 ∈ [0, 1] to represent
that A knows an 𝑎4 degree about 𝐴4, where 𝑎4 = 0 means A knows nothing and 𝑎4 = 1 means A
knows everything.

(v) 𝐴5: it describes I’s response to A’s query to 𝑓 (if applicable), which is relevant because A
can learn useful information about 𝑓 by observing 𝑓 ’s responses [119]. We define 𝑎5 ∈ {0, 1} such
that 𝑎5 = 0 means there is a limit on the response that can be made by A to 𝑓 (referred as LQ) and
𝑎5 = 1 means there is no limit (referred as FQ).

(vi) 𝐴6: it describes A’s manipulation set in the problem space, which describes perturbations

for generating adversarial files (adapted from perturbation set in the AML literature [123]):
M = {𝛿 : (𝑧 ′ ← A (𝑧, 𝛿)) ∧ (true ← O (𝑧, 𝑧 ′)) ∧ (𝑧 ∈ Z) ∧ (𝑧 ′ ≠ 𝑧)}.

M is application-specific. For instance, an Android Package Kit (APK) permits adding codes
or renaming class names [31, 39, 53, 76], a Windows Portable Executable (PE) permits adding
codes or changing PE section names [5, 37, 38, 68], and a Portable Document Format (PDF) file
permits appending dead-code at its end [114] or add new instructions [25, 134]. This means that a
perturbation 𝛿 ∈ M can be a tuple specifying an operator (e.g., addition or removal), an object (e.g.,
a feature used by I), and other kinds of information (e.g., perturbation location in a file).

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:10

D. Li, Q. Li, F. Ye, and S. Xu

Since it is often infeasible to enumerate the entire manipulation set, A may leverage an empirical
one (cid:102)M [31, 39, 53, 76, 97, 114], which can be defined in the problem or feature space. Manipulations
in the problem space must not violate the relevant constraints (e.g., adding APIs in an APK should
not cause the request of unauthorized permissions). Manipulations in the feature space facilitate
efficient computing via gradient-based methods as long as the inverse feature mapping 𝜙 −1 is
available. Furthermore, we can use the manipulation set M to define a feature manipulation set M:
(2)

M = {𝛿x = x′ − x : (x = 𝜙 (𝑧)) ∧ (x′ = 𝜙 (𝑧 ′)) ∧ (𝑧 ′ ← A (𝑧, 𝛿)) ∧ (𝛿 ∈ M) ∧ (𝑧 ∈ Z)}.

In order to compute M efficiently, one strategy is to estimate a feature-space analog of (cid:102)M, denoted
by (cid:101)M [109, 114]. This however demands resolving the invertibility Assumption 5.

(vii) 𝐴7: it describes A’s attack tactics. We consider two tactics: classifier evasion and classifier
poisoning. For the evasion attack, we consider three variants: basic evasion (BE), optimal evasion
1 (OE1) and optimal evasion 2 (OE2). For the poisoning attack, we consider two variants: basic
poisoning (BP) and optimal poisoning (OP). Correspondingly, we have 𝐴7 ∈ {BE, OE1, OE2, BP, OP}.
These tactics are elaborated below, while noting that they do not explicitly call oracle O because
definitions of manipulation sets M already assure that manipulations preserve functionalities of
non-adversarial files.

As shown in Definition 9, the basic evasion attack is that A uses a set of perturbations 𝛿𝑧 ⊆ M
to manipulate a malicious file 𝑧, which is classified by I’s classifier 𝑓 as + ← 𝑓 (𝑧), to an adversarial
file 𝑧 ′ such that − ← 𝑓 (𝑧 ′).

Definition 9 (basic evasion or BE [53]). A looks for 𝛿𝑧 ⊆ M to achieve the following for 𝑧 ∈ Z

with + ← 𝑓 (𝑧):

− ← 𝑓 (𝑧 ′) where (𝑧 ′ ← A (𝑧, 𝛿𝑧)) ∧ (𝛿𝑧 ⊆ M).

As shown in Definition 10, the attacker attempts to minimize the degree of perturbations. In other
words, this attack tactic is the same as BE, except that A attempts to minimize the manipulation
when perturbing a non-adversarial file 𝑧 ∈ Z into an adversarial file 𝑧 ′ ∈ Z.

Definition 10 (optimal evasion 1 or OE1; adapted from [24]). A attempts to achieve the

following for 𝑧 ∈ Z with + ← 𝑓 (𝑧):
min
𝑧′

Γ(𝑧 ′, 𝑧) s.t. (𝑧 ′ ← A (𝑧, 𝛿𝑧)) ∧ (𝛿𝑧 ⊆ M) ∧ (− ← 𝑓 (𝑧 ′)).

As shown in Definition 11, the attacker attempts to maximize I’s loss for waging high-confidence

evasion attacks, while noting the small perturbations may be incorporated.

Definition 11 (optimal evasion 2 or OE2; adapted from [16]). A attempts to achieve the

following for 𝑧 ∈ Z with + ← 𝑓 (𝑧):

max
𝑧′

𝐿(𝐹𝜃 (𝜙𝑐 (𝑆, 𝑧 ′)), +) s.t. (𝑧 ′ ← A (𝑧, 𝛿𝑧)) ∧ (𝛿𝑧 ⊆ M) ∧ (− ← 𝑓 (𝑧 ′)).

Let 𝐷 ′

𝑝𝑜𝑖𝑠𝑜𝑛 ⊂ D be a set of adversarial file-label pairs obtained by manipulating non-adversarial
files in 𝐷𝑝𝑜𝑖𝑠𝑜𝑛. Let 𝐷 ′
𝑝𝑜𝑖𝑠𝑜𝑛 be the contaminated training data for learning a
classifier 𝑓 ′ with parameters 𝜃 ′. As shown in Definition 12, the basic poisoning attack is that the
attacker aims to make 𝑓 ′ mis-classify the files in a dataset 𝐷𝑡𝑎𝑟𝑔𝑒𝑡 , while accommodating the attacks
that A manipulates labels of the files in 𝐷𝑝𝑜𝑖𝑠𝑜𝑛 [95].

𝑡𝑟𝑎𝑖𝑛 ← 𝐷𝑡𝑟𝑎𝑖𝑛 ∪ 𝐷 ′

Definition 12 (basic poisoning or BP [9]). Given a set 𝐷𝑡𝑎𝑟𝑔𝑒𝑡 of files where + ← 𝑓 ( (cid:164)𝑧) for
(cid:164)𝑧 ∈ 𝐷𝑡𝑎𝑟𝑔𝑒𝑡 and a set 𝐷𝑝𝑜𝑖𝑠𝑜𝑛 of non-adversarial files, A attempts to perturb files in 𝐷𝑝𝑜𝑖𝑠𝑜𝑛 to
𝑝𝑜𝑖𝑠𝑜𝑛 = {(A (𝑧, 𝛿𝑧), A (𝑦)) : ((𝑧, 𝑦) ∈ 𝐷𝑝𝑜𝑖𝑠𝑜𝑛) ∧ (𝛿𝑧 ⊆ M)∧ (A (𝑦) ∈ {+, −})}
adversarial ones 𝐷 ′

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:11

such that classifier 𝑓 ′ learned from 𝐷 ′
𝑝𝑜𝑖𝑠𝑜𝑛 mis-classifies the files in 𝐷𝑡𝑎𝑟𝑔𝑒𝑡 .
𝑡𝑟𝑎𝑖𝑛 ← 𝐷𝑡𝑟𝑎𝑖𝑛 ∪ 𝐷 ′
Formally, the attacker intents to achieve the following for ∀ (cid:164)𝑧 ∈ 𝐷𝑡𝑎𝑟𝑔𝑒𝑡 : − ← 𝑓 ′( (cid:164)𝑧) where 𝑓 ′ is learned
from 𝐷 ′

𝑡𝑟𝑎𝑖𝑛 ← 𝐷𝑡𝑟𝑎𝑖𝑛 ∪ 𝐷 ′

𝑝𝑜𝑖𝑠𝑜𝑛.

As shown in Definition 13, the optimal poisoning attack is the same as BF, except that A attempts
to maximize the loss when using classifier 𝑓 ′ with parameter 𝜃 ′ to classify files in 𝐷𝑡𝑎𝑟𝑔𝑒𝑡 . Definition
𝑝𝑜𝑖𝑠𝑜𝑛 | [116] or bounds on the degree of
13 can have multiple variants by considering bounds on |𝐷 ′
perturbations |𝛿𝑧 | [118].

Definition 13 (optimal poisoning or OP [87]). Given 𝐷𝑝𝑜𝑖𝑠𝑜𝑛, A perturbs 𝐷𝑝𝑜𝑖𝑠𝑜𝑛 into 𝐷 ′

𝑝𝑜𝑖𝑠𝑜𝑛

for achieving:

max
𝐷′
𝑝𝑜𝑖𝑠𝑜𝑛

L (𝜃 ′, 𝐷𝑡𝑎𝑟𝑔𝑒𝑡 ) where 𝜃 ′ ← arg min

𝜃

L (𝜃, 𝐷𝑡𝑟𝑎𝑖𝑛 ∪ 𝐷 ′

𝑝𝑜𝑖𝑠𝑜𝑛).

(viii) 𝐴8: it describes A’s attack techniques, such as Gradient-based Optimization (GO), Sensitive
Features (SF), MImicry (MI), TRansferability (TR), Heuristic Search (HS), Generative Model (GM),
and Mixture Strategy (MS). We denote this by 𝐴8 ∈ {GO, SF, MI, TR, HS, GM, MS}. Let A have
a classifier ˆ𝑓 , which consists of a hand-crafted feature extraction ˆ𝜙𝑐 and a parameterized model
ˆ𝐹 ˆ𝜃 . Let A also have an objective function 𝐿A : [0, 1] × Y → R, which measures ˆ𝑓 ’s error or A’
failure in evasion [16, 24]. Note that ˆ𝑓 and 𝐿A can be the same as, or can mimic (by leveraging A’s
knowledge about I’s attributes 𝐴1, . . . , 𝐴5), I’s classifier 𝑓 and loss function 𝐿, respectively.

The attack technique specified by Definition 14 is that A solves the feature-space optimization
problems described in Definitions 10, 11 and 13 by using some gradient-based optimization method
and then leverages the invertibility Assumption 5 to generate adversarial malware examples.

Definition 14 (Gradient-based Optimization or GO, adapted from [24, 87]). Let x ←
ˆ𝜙𝑐 ( ˆ𝑆, 𝑧) and x′ ← x + 𝛿x. The feature-space optimization problem in Definition 10 can be written as
(3)

𝐶 (x, x + 𝛿x) s.t. (𝛿x ∈ [u, u]) ∧ ( ˆ𝐹 ˆ𝜃 (x′) < 𝜏),

min
𝛿x

where u and u are respectively the lower and upper bounds on M (e.g., 𝛿x ∈ [−x, 1 − x] for binary
representation x). The feature-space optimization problem in Definition 11 can be written as
(cid:16) ˆ𝐹 ˆ𝜃 (x + 𝛿x), +

s.t. (𝛿x ∈ [u, u]).

𝐿A

(4)

(cid:17)

max
𝛿x

The feature-space optimization problem specified in Definition 13 can be written as

E( (cid:164)𝑧, (cid:164)𝑦) ∈𝐷𝑡𝑎𝑟𝑔𝑒𝑡 𝐿A ( ˆ𝐹 ˆ𝜃 ′ ( ˆ𝜙𝑐 ( ˆ𝑆, (cid:164)𝑧), (cid:164)𝑦)), ∀(𝑧, 𝑦) ∈ 𝐷𝑝𝑜𝑖𝑠𝑜𝑛

(5)

max
𝛿x ∈ [u,u]
ˆ𝜃 ′ ← arg min

ˆ𝜃

where

E

(𝑧𝑡 ,𝑦𝑡 ) ∈ ˆ𝐷𝑡𝑟𝑎𝑖𝑛∪{ ( ˆ𝜙 −1

𝑐 ( ˆ𝜙𝑐 (𝑧)+𝛿x),𝑦′) }

𝐿A

(cid:16) ˆ𝐹 ˆ𝜃 ( ˆ𝜙𝑐 ( ˆ𝑆, 𝑧𝑡 ), 𝑦𝑡 )

(cid:17)

.

In order to calculate the gradients of loss function 𝐿A with respect to 𝛿x in Eqs.(3) and (4),
inequality constraints can be handled by appending penalty items to the loss function in question
and box-constraints can be coped with by using gradient projection [24, 78]. Since x + 𝛿x is
continuous, the GO attack technique needs to map 𝛿x to a discrete perturbation vector in M, for
instance by using the nearest neighbor search [78]. The gradients of loss function 𝐿A with respect
to 𝛿x in Eq. (5) are delicate to deal with. One issue is the indirect relation between 𝐿A and 𝛿x, which
can be handled by the chain rule [73]. Another issue is the difficulty that is encountered when
computing the partial derivatives 𝜕 ˆ𝜃 ′/𝜕𝛿x [87]. For dealing with this, researchers often relax the
underlying constraints (e.g., by supposing that ˆ𝐹 ˆ𝜃 is a linear model).

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:12

D. Li, Q. Li, F. Ye, and S. Xu

The attack technique specified by Definition 15 is that A perturbs malware examples by injecting
or removing a small number of features to decrease the classification error measured by the loss
function 𝐿A as much as possible.

Definition 15 (Sensitive Features or SF, adapted from [74]). For evasion attacks, A aims to

maximize the following with respect to a given malware example-label pair (𝑧, +):

max
𝑧′

𝐿A

(cid:16) ˆ𝐹 ˆ𝜃 ( ˆ𝜙 ( ˆ𝑆, 𝑧 ′)), +

(cid:17)

s.t. (𝑧 ′ ← A (𝑧, 𝛿𝑧)) ∧ (𝛿𝑧 ⊆ M) ∧ (|𝛿𝑧 | ≤ 𝑚),

where 𝑚 is the maximum degree of manipulations.

For poisoning attacks, A aims to maximize the following with respect to the given 𝐷𝑝𝑜𝑖𝑠𝑜𝑛 and

𝐷𝑡𝑎𝑟𝑔𝑒𝑡 ,

max
𝐷′
𝑝𝑜𝑖𝑠𝑜𝑛

E( (cid:164)𝑧, (cid:164)𝑦) ∈𝐷𝑡𝑎𝑟𝑔𝑒𝑡 𝐿A ( ˆ𝐹 ˆ𝜃 ′ ( ˆ𝜙𝑐 ( ˆ𝑆, (cid:164)𝑧), (cid:164)𝑦)),

(6)

ˆ𝜃 ′ is learned from ˆ𝐷𝑡𝑟𝑎𝑖𝑛 ∪ 𝐷 ′

where
with obeying (𝑧 ′ ← A (𝑧, 𝛿𝑧)) ∧ (𝛿𝑧 ⊆ M) ∧ (𝑧 ∈ 𝐷𝑝𝑜𝑖𝑠𝑜𝑛) ∧ (|𝛿𝑧 | ≤ 𝑚).

𝑝𝑜𝑖𝑠𝑜𝑛 such that ∀𝑧 ′ ∈ 𝐷 ′

𝑝𝑜𝑖𝑠𝑜𝑛 is obtained via the perturbation 𝛿𝑧

The attack technique specified by Definition 16 is that A perturbs malware example 𝑧 by
mimicking a benign example, while noting that this attack technique can be algorithm-agnostic.

Definition 16 (MImicry or MI, adapted from [114]). Given a set of benign examples 𝐷𝑏𝑒𝑛 and

a malware example 𝑧, A aims to achieve the following minimization:

min
𝛿𝑧 ∈M

Γ(A (𝑧, 𝛿𝑧), 𝑧𝑏𝑒𝑛) s.t. (∃ 𝑧𝑏𝑒𝑛 ∈ 𝐷𝑏𝑒𝑛).

(7)

The attack technique specified by Definition 16 can be extended to accommodate the similarity
between representations in the feature space [16, 114]. The attack technique specified by Definition
17 is that A generates adversarial examples against a surrogate model ˆ𝑓 .

Definition 17 (TRansferability or TR, adapted from [91]). A learns a surrogate model

of 𝑓 from ˆ𝐷𝑡𝑟𝑎𝑖𝑛, ˆ𝑆,
example 𝑧 to 𝑧 ′ and then attacks 𝑓 with 𝑧 ′. For poisoning attacks, A contaminates
− ← ˆ𝑓 ′( (cid:164)𝑧) for ∀( (cid:164)𝑧, (cid:164)𝑦) ∈ 𝐷𝑡𝑎𝑟𝑔𝑒𝑡 , by poisoning the training set ˆ𝐷𝑡𝑟𝑎𝑖𝑛 with 𝐷 ′
with 𝐷 ′

ˆ𝑓
ˆ𝜙𝑐 and ˆ𝐹 . For evasion attacks, A achieves − ← ˆ𝑓 (𝑧 ′) by perturbing malware
ˆ𝑓 ′ such that
𝑝𝑜𝑖𝑠𝑜𝑛 and the attacks 𝑓

ˆ𝑓 to

𝑝𝑜𝑖𝑠𝑜𝑛.

The attack technique specified by Definition 18 is that A searches perturbations in M via some
heuristics, while leveraging oracle O’s responses to A’s queries and 𝑓 ’s responses to A’s queries.
Since M is defined with respect to the problem space, this attack technique does not need the
invertibility Assumption 5.

Definition 18 (Heuristic Search or HS). Let ℎ be a function taking O’s response and 𝑓 ’s

response as input. Given a malware example 𝑧, A looks for an 𝑚-length manipulation path

⟨𝑧 (0), 𝑧 (1), . . . , 𝑧 (𝑚) ⟩ s.t. 𝑧 (𝑖+1) = A (𝑧 (𝑖), 𝛿𝑧,(𝑖) ) ∧ (𝛿𝑧,(𝑖) ∈ M) ∧ (ℎ(O, 𝑓 , 𝑧, 𝑧 (𝑖) ) ≤ ℎ(O, 𝑓 , 𝑧, 𝑧 (𝑖+1) ))

where 𝑧 (0) = 𝑧.

The attack technique specified by Definition 19 is that A uses a generative model 𝐺 with
parameters 𝜃𝑔 to perturb malware representation vectors and then leverages the invertibility
Assumption 5 to turn the perturbed vector into an adversarial malware example.

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:13

Definition 19 (Generative Model or GM). Given a malware representation vector x = ˆ𝜙𝑐 ( ˆ𝑆, 𝑧),

A achieves

max
𝜃𝑔

𝐿A

(cid:16) ˆ𝐹 ˆ𝜃 (𝐺𝜃𝑔 (x)), +

(cid:17)

s.t. 𝐺𝜃𝑔 (x) ∈ [u − x, u − x]

and leverages the invertibility Assumption 5 to obtain an adversarial example 𝑧 ′ = ˆ𝜙 −1

𝑐 (𝐺𝜃𝑔 (x)).

The attack technique specified by Definition 20 is that A combines multiple perturbation methods

to perturb an example.

Definition 20 (Mixture Strategy or MS [76]). Let H𝐴 denote the space of generative methods
and W𝑎 = {w𝑎 : w𝑎 = (𝑤𝑎,1, . . . , 𝑤𝑎,𝐾 ), 𝑤𝑎,𝑖 ≥ 0} with 𝑖 = 1, . . . , 𝐾 denote the weights space. Given a
malware example 𝑧, A aims to achieve

𝐿A ( ˆ𝐹 ˆ𝜃 ( ˆ𝜙 (𝑧 ′)), +) s.t. (𝑧 ′ =

max
w𝑎

𝐾
∑︁

𝑖=1

𝑤𝑎,𝑖𝑔𝑖 (𝑧)) ∧ (O (𝑧, 𝑧 ′) = true)) ∧ (𝑔𝑖 ∈ H𝐴) ∧ (w𝑎 ∈ W𝑎).

(ix) 𝐴9: it corresponds to A’s adversarial files. Given file manipulation set M, the corresponding
set of adversarial files is defined as ZM = {A (𝑧, 𝛿𝑧) : (𝑧 ∈ Z) ∧ (𝛿𝑧 ⊆ M)}. Given feature
manipulation set M, the set of adversarial feature vectors is: XM = {x′ : (x′ = x + 𝛿x) ∧ (𝛿x ∈ M)}.

(1, 1, 1, 1, 1) White-box attack

(𝑎1, 𝑎2, 1, 1, 𝑎5)

(𝑎1, 𝑎2, 1, 𝑎4, 𝑎5)

(0, 0, 1, 1, 0)

(𝑎1, 𝑎2, 𝑎3, 1, 𝑎5)

(𝑎1, 𝑎2, 1, 0, 𝑎5)

(0, 0, 1, 𝑎4, 0)

(𝑎1, 𝑎2, 𝑎3, 𝑎4, 𝑎5)

(0, 0, 𝑎3, 1, 0)

(𝑎1, 𝑎2, 0, 1, 𝑎5)

(0, 0, 1, 0, 0)

(𝑎1, 𝑎2, 𝑎3, 0, 𝑎5)

(0, 0, 𝑎3, 𝑎4, 0)

(𝑎1, 𝑎2, 0, 𝑎4, 𝑎5)

(0, 0, 0, 1, 0)

(0, 0, 𝑎3, 0, 0)

(𝑎1, 𝑎2, 0, 0, 𝑎5)

(0, 0, 0, 𝑎4, 0)

(0, 0, 0, 0, 0) Black-box attack

Fig. 3. A portion of the partial order defined over (𝑎1, . . . , 𝑎5).

On the Usefulness of the Preceding Specification. The preceding specification can be applied
to formulate a partial order in the attribute space, which allows to compare attacks unambiguously.
Figure 3 depicts how vector (𝑎1, · · · , 𝑎5) formulates a partial order between the widely-used informal
notions of black-box attack, namely (𝑎1, 𝑎2, 𝑎3, 𝑎4, 𝑎5) = (0, 0, 0, 0, 0), and white-box attack, namely
(𝑎1, 𝑎2, 𝑎3, 𝑎4, 𝑎5) = (1, 1, 1, 1, 1); there are many kinds of grey-box attacks in between.

2.2.3

Systematizing Defenses. Similarly, we systematize defenses from two perspectives: de-
fender’s objective (i.e., what the defender aims to achieve) and defender’s input (i.e., what leverages
the defender can use). We also discuss how to apply the specification to formulate a partial-order
structure for comparing defenses.
Defender’s Objectives. I aims to detect ideally all of the malicious files, adversarial and non-
adversarial alike, while suffering from small side-effects (e.g., increasing false-positives).

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:14

D. Li, Q. Li, F. Ye, and S. Xu

Defender’s Input. As highlighted in Table 3, I’s input includes attributes 𝐴1, . . . , 𝐴5, which
are under I’s control, and the extent 𝑎6, . . . , 𝑎9 at which I respectively knows about attributes
𝐴6, . . . , 𝐴9, which are under A’s control. Note that 𝐴1, . . . , 𝐴9 have been defined above.

• We define 𝑎6 ∈ [0, 1] to represent the extent at which I knows A’s manipulation set M. Let
ˆM ⊆ M denote the subset of A’s manipulation set known to I. Then, we set 𝑎6 = | ˆM|/|M|.
• We define 𝑎7 ∈ {0, 1} such that 𝑎7 = 0 means I does not know A’s attack tactic 𝐴7 ∈

{BE, OE1, OE2, BP, OP} and 𝑎7 = 1 means I knows A’s tactic.

• We define 𝑎8 ∈ {0, 1} such that 𝑎8 = 1 (𝑎8 = 0) means the defender does (not) know I’s

attack technique 𝐴8 ∈ {GO, SF, TR, MI, HS, GM, MS}.

• We use 𝑎9 = | ˆZM |/|ZM | to represent the extent at which I knows about A’s adversarial
files, where 𝑎9 ∈ [0, 1] and ZM is A’s adversarial files and ˆZM ⊆ ZM is known to I.
On the Usefulness of the Preceding Specification. Similarly, the defense specification can be
used to formulate a partial order in the attribute space, paving the way for comparing defenses
unambiguously. Figure 4 depicts how vector (𝑎6, . . . , 𝑎9) formulates a partial order between the
widely-used informal notions of black-box defense (𝑎6, 𝑎7, 𝑎8, 𝑎9) = (0, 0, 0, 0) and white-box defense
(𝑎6, 𝑎7, 𝑎8, 𝑎9) = (1, 1, 1, 1, 1); there are many kinds of grey-box defenses in between.

(1, 1, 1, 1) White-box defense

(1, 𝑎7, 𝑎8, 𝑎9)

(1, 0, 0, 1)

(𝑎6, 𝑎7, 𝑎8, 1)

(1, 𝑎7, 𝑎8, 0)

(1, 0, 0, 𝑎8)

(𝑎6, 𝑎7, 𝑎8, 𝑎9)

(𝑎6, 0, 0, 1)

(0, 𝑎7, 𝑎8, 1)

(1, 0, 0, 0)

(𝑎6, 𝑎7, 𝑎8, 0)

(𝑎6, 0, 0, 𝑎8)

(0, 𝑎7, 𝑎8, 𝑎9)

(0, 0, 0, 1)

(𝑎6, 0, 0, 0)

(0, 𝑎7, 𝑎8, 0)

(0, 0, 0, 𝑎9)

(0, 0, 0, 0) Black-box defense

Fig. 4. A portion of the partial order defined over (𝑎6, . . . , 𝑎9).

2.3 Systematizing Security Properties
Since 𝑓 = 𝜑 (𝜙 (·)), we decompose 𝑓 ’s security properties into 𝜑’s and 𝜙’s. We consider: Repre-
sentation Robustness (RR), meaning that two similar files have similar feature representations;
Classification Robustness (CR), meaning that two similar feature representations lead to the same
label; Detection Robustness (DR), meaning that feature extraction function 𝜙 returns similar repre-
sentations for two files with the same functionality; Training Robustness (TR), meaning that small
changes to the training set does not cause any significant change to the learned classifier. With
respect to small perturbations, Definitions 21 and 22 below collectively say that when two files
𝑧 and 𝑧 ′ are similar, they would be classified as the same label with a high probability. Since the
classification function 𝜑 is linear, we can obtain a 𝜖-robust 𝜑 analytically, where 𝜖 is a small scalar
that bounds the perturbations applied to feature vectors [80]. This means that the main challenge
is to achieve robust feature extraction.

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:15

Definition 21 (RR or (𝜖, 𝜂)-robust feature extraction; adapted from [126]). Given con-
stants 𝜖, 𝜂 ∈ [0, 1], and files 𝑧, 𝑧 ′ ∈ Z such that (Γ(𝑧, 𝑧 ′) ≈ 0) ∧ (true ← O (𝑧, 𝑧 ′)), we say feature
extraction function 𝜙 is (𝜖, 𝜂)-robust if

P(𝐶 (x, x′) ≤ 𝜖) = P(𝐶 (𝜙 (𝑧), 𝜙 (𝑧 ′)) ≤ 𝜖) > 1 − 𝜂.
Definition 22 (CR or 𝜖-robust classification [12]). Given constant 𝜖 ∈ [0, 1] as in Definition

21 and any feature vectors x, x′ ∈ X, we say classification function 𝜑 is 𝜖-robust if

(𝐶 (x, x′) ≤ 𝜖) → ((𝜑 (x) > 𝜏) ∧ (𝜑 (x′) > 𝜏)).
Definition 23 specifies detection robustness, which says that feature extraction function 𝜙 returns
similar representations for two different files as long as they have the same functionality. Note that
Definitions 22 and 23 collectively produce a malware detector with detection robustness.

Definition 23 (DR or (O, 𝜂)-robust feature extraction; adapted from [2]). Given constant
𝜂 ∈ [0, 1] and two files 𝑧, 𝑧 ′ ∈ Z such that (Γ(𝑧, 𝑧 ′) >> 0) ∧ (true ← O (𝑧, 𝑧 ′)), we say feature
extraction 𝜙 is (O, 𝜂)-robust if P(𝐶 (𝜙 (𝑧), 𝜙 (𝑧 ′)) ≤ 𝜖) > 1 − 𝜂.

Suppose we impose a restriction on the adversarial files set 𝐷 ′

𝑝𝑜𝑖𝑠𝑜𝑛 | ≤ 𝛾 |𝐷𝑡𝑟𝑎𝑖𝑛 |
for some constant 𝛾 ∈ [0, 1]. Let classifier 𝑓 ′ be learned from 𝐷𝑡𝑟𝑎𝑖𝑛 ∪ 𝐷 ′
𝑝𝑜𝑖𝑠𝑜𝑛. Definition 24 says
that a classifier 𝑓 ′ learned from poisoned training set can predict as accurately as 𝑓 learned from
𝐷𝑡𝑟𝑎𝑖𝑛 with a high probability.

𝑝𝑜𝑖𝑠𝑜𝑛 such that |𝐷 ′

Definition 24 (TR or (𝛾, 𝜁 )-robust training; adapted from [116]). Given classifiers 𝑓 learned
𝑝𝑜𝑖𝑠𝑜𝑛 | ≤ 𝛾 |𝐷𝑡𝑟𝑎𝑖𝑛 |, and small constants
𝑝𝑜𝑖𝑠𝑜𝑛 where |𝐷 ′

from 𝐷𝑡𝑟𝑎𝑖𝑛 and 𝑓 ′ learned from 𝐷𝑡𝑟𝑎𝑖𝑛 ∪ 𝐷 ′
𝜁 ∈ [0, 1], we say 𝑓 ′ is (𝛾, 𝜁 )-robust if ∀𝑧 ∈ Z

(cid:16)

(𝑓 (𝑧) > 𝜏) ∧ (|𝐷 ′

𝑝𝑜𝑖𝑠𝑜𝑛 | ≤ 𝛾 |𝐷𝑡𝑟𝑎𝑖𝑛 |)

(cid:17)

→ (P(𝑓 ′(𝑧) > 𝜏) > 1 − 𝜁 ) .

3 SYSTEMATIZING AMD ARMS RACE
We systematize attacks according to A’s objective, input, assumptions, the security properties that
are broken, and the types of victim malware detectors (e.g., Windows vs. Android). Similarly, we
systematize defenses according to I’s objective, input, assumptions, the security properties that
are achieved, and the types of enhanced malware detectors (e.g., Windows vs. Android). We group
attacks (defenses) according to the attacker’s (defender’s) techniques and then summarize them
in a table according to the publication date in chronological order. For convenience, we will use
wildcard ∗ to indicate any value in a domain (e.g., [0, 1]); we will use ∨ to describe A’s and I’s
“broader” input (if applicable). For example, (0, 1, 0, 1, 0|𝐴6, . . . , 𝐴9) ∨ (1, 0, 1, 1, 1|𝐴6, . . . , 𝐴9) means
that A has either (𝑎1, 𝑎2, 𝑎3, 𝑎4, 𝑎5) = (0, 1, 0, 1, 0) or (𝑎1, 𝑎2, 𝑎3, 𝑎4, 𝑎5) = (1, 0, 1, 1, 1). Finally, we will
present the attack-defense escalation.

3.1 Systematizing Attack Literature

3.1.1 Attacks using Gradient-based Optimization (GO). Biggio et al. [16] propose solving the
problem of optimal evasion attacks by leveraging gradient-based optimization techniques. They
focus on high-confidence evasion attacks with small perturbations (cf. Definition 11). Given a
malware representation-label pair (x, 𝑦 = +), the optimization problem specified in Eq.(4) with
respect to GO is instantiated as:
max
𝛿x

𝐿A ( ˆ𝐹 ˆ𝜃 (x + 𝛿x), 𝑦 = +) = min
𝛿x

(𝐿(𝐹𝜃 (x + 𝛿x, 𝑦 = −)) − 𝛽𝑎K (x + 𝛿x))

s.t. (𝛿x ∈ [0, u]) ∧ (𝐶 (x, x + 𝛿x) ≤ 𝑚),

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:16

D. Li, Q. Li, F. Ye, and S. Xu

Table 4. Summary of AMD attacks (✓means applicable, means 0, means 1, means a value in [0, 1]).

Attack
(in chronological order)

Attack
Objective

Attack Input

Assumptions

Broken
Properties

Malware
detector

𝑛
𝑖
𝑎
𝑟
𝑡

𝐷

t
e
s
g
n
i
n
i
a
r
T

e
u
q
i
n
h
c
e
t

e
s
n
e
f
e
D

t
e
s

e
r
u
t
a
e
F

m
h
t
i
r
o
g
l
a

g
n
i
n
r
a
e
L

t
e
s
n
o
i
t
a
l
u
p
i
n
a
M

e
s
n
o
p
s
e
R

:
1
𝐴

:
2
𝐴

:
3
𝐴

:
4
𝐴

:
5
𝐴

:
6
𝐴

e
u
q
i
n
h
c
e
t
k
c
a
t
t

A

:
8
𝐴

c
i
t
c
a
t
k
c
a
t
t

A

:
7
𝐴

e
t
a
n
m

i

i
r
c
s
i
d
n
I

y
t
i
l
i
b
a
l
i
a
v
A

d
e
t
e
g
r
a
T

t
e
s

e
l
p
m
a
x
e

l
a
i
r
a
s
r
e
v
d
A

:
9
𝐴

n
o
i
t
p
m
u
s
s
a

s
s
e
n
h
t
o
o
m
S

n
o
i
t
p
m
u
s
s
a
y
t
i
l
i
b
i
t
r
e
v
n
I

n
o
i
t
p
m
u
s
s
a
y
t
i
l
i
b
a
r
u
s
a
e
M
✓

n
o
i
t
p
m
u
s
s
a

e
l
c
a
r
O

n
o
i
t
p
m
u
s
s
a
D

I
I

✓ ✓

✓ ✓
✓ ✓

✓ ✓ ✓
✓
✓

s
s
e
n
t
s
u
b
o
R
n
o
i
t
a
t
n
e
s
e
r
p
e
R

:

R
R

s
s
e
n
t
s
u
b
o
R
n
o
i
t
a
c
fi
i
s
s
a
l
C

:

R
C
✓

✓

✓

✓

✓

✓
✓

✓

✓

✓

✓

✓ ✓ ✓

✓ ✓
✓ ✓ ✓

✓

✓

✓

✓ ✓ ✓
✓ ✓ ✓

✓

✓

✓

F
D
P
✓

✓

✓

✓

✓
✓

✓

s
s
e
n
t
s
u
b
o
R
n
o
i
t
c
e
t
e
D

s
s
e
n
t
s
u
b
o
R
g
n
i
n
i
a
r
T

:

R
D

:

R
T

m
a
r
g
o
r
P
s
w
o
d
n
i
W

e
g
a
k
c
a
P
d
i
o
r
d
n
A

✓

✓

✓
✓
✓
✓

✓

✓

✓

✓

✓

✓
✓

✓

✓

✓ ✓

✓

✓
✓

✓

✓

✓

✓

✓

✓

✓

✓

✓
✓

✓

✓

✓

M OE2 MI XM

M OE2 GO XM

✓

✓

TR
TR
TR
TR

BE
BE
OE2
OE2

M BE MI ZM
XM
M
ZM
M
XM
M
M
ZM
M BE HS ZM
✓
M BE MI ZM
M BE GM XM ✓ ✓
M BE GM XM ✓ ✓
SF XM

M OE2

✓ ✓

SF XM
M OE2
SF XM
M OE2
XM
M
TR
BE
M
ZM
TR
BE
M BE HS ZM

M OP GO XM

✓

✓

✓ ✓

BE
BE

M BE HS ZM
XM
M
M
ZM
M OE2 GM ZM ✓ ✓ ✓
M OE2 GO XM

TR
TR

✓

✓

M BP

SF XM

M OE2 GO XM

M
M

BP
BP

SF
SF

XM
ZM

M OE2 GO XM
M OE2 GO XM

M
M

OE1
OE2

GO
SF

XM
XM

M OE2 MI ZM
XM
M
ZM
M

OE2
OE2

MS
MS

✓ ✓

✓

✓ ✓

✓
✓

✓ ✓

✓

✓

Smutz and Stavrou [109]

Biggio et al. [16]

Maiorca et al. [82]

Šrndić and Laskov [114]

Xu et al. [134]
Carmony et al. [25]
Hu and Tan [58]
Hu and Tan [59]

Demontis et al. [39]

Grosse et al. [53]
Chen et al. [29]

Khasawneh et al. [65]

Dang et al. [36]

Muñoz-González et al. [87]

Yang et al. [136]

Rosenberg et al. [100]

Anderson et al. [5]
Kreuk et al. [70]

Chen et al. [30]

Al-Dujaili et al. [2]

Suciu et al. [118]

Kolosnjaji et al. [68]
Suciu et al. [117]

Chen et al. [31]

Pierazzi et al. [97]

Li and Li [76]

✓

✓

✓

✓

✓
✓
✓
✓

✓

✓
✓

✓

✓

✓

✓

✓
✓

✓

✓

✓
✓

✓

✓

✓

✓

✓

where 𝛽𝑎 ≥ 0 is a balance factor and K is a density estimation function for lifting x + 𝛿x to the
populated region of benign examples. Since 𝛿x ≥ 0, the manipulation only permits object injections
to meet the requirement of preserving malicious functionalities. The attack is validated by using the
PDF malware detector and the feature representation is the number of appearances of hand-selected
keywords (e.g., JavaScript). Because the perturbation is continuous, the authors suggest searching
a discrete point close to the continuous one and aligning the point with ∇𝐿A (x + 𝛿x, 𝑦 = +). This
attack makes the invertibility Assumption 5 because it operates in the feature space. Experimental
results show that when I employs no countermeasures, knowing I’s feature set 𝑆 and learning
algorithm 𝐹 are sufficient for A to evade I’s detector. This attack and its variants have been
shown to evade PDF malware detectors [17, 102, 114, 141], PE malware detectors [68], Android

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:17

malware detectors [76], and Flash malware detectors [83]. The kernel density estimation item
makes the perturbed representation x + 𝛿x similar to the representations of benign examples,
explaining the successful evasion. In summary, the attack works under the Oracle, Measurability,
and Invertibility assumptions. A’s input is, or A can be characterized as, (𝑎1, . . . , 𝑎5|𝐴6, · · · , 𝐴9) =
(1, 1, 1, 1, 1|M, OE2, GO, XM) ∨ (0, 0, 1, ∗, 0|M, OE2, GO, XM) and A breaks the CR property.

Al-Dujaili et al. [2] propose evasion attacks against DNN-based malware detectors in the feature
space. In this attack, A generates adversarial examples with possibly large perturbations in the
feature space. More precisely, given a representation-label pair (x, 𝑦 = +), the optimization problem
of Eq.(4) with respect to GO is instantiated as: max𝛿x
𝐿(𝐹𝜃 (x + 𝛿x), 𝑦 = +) s.t. 𝛿x ∈ [0, 1 − x].
The attack has four variants, with each perturbing the representation in a different direction (e.g.,
normalized gradient of the loss function using the ℓ∞ norm). A “random” rounding operation is
used to map continuous perturbations into a discrete domain. When compared with the basic
rounding (which returns 0 if the input is smaller than 0.5, and returns 1 otherwise), the “random”
rounding means that the threshold of rounding is sampled from the interval [0, 1] uniformly. For
binary feature representation, the manipulation set Mx = [0, 1 − x] assures the flipping of 0 to
1. The effectiveness of the attack is validated using Windows malware detector in the feature
space. In summary, the attack works under the Oracle and Invertibility assumptions with A input
(𝑎1, . . . , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 1, 1, 1, 1|M, OE2, GO, XM) and breaks the DR property.

Kreuk et al. [70] propose an evasion attack in the feature space against MalConv, which is an
end-to-end Windows PE malware detector (as reviewed in Section 2.1) [99]. Given a malware
embedding code x, the optimization problem of Eq.(4) with respect to GO is instantiated as:
min𝛿x
𝐿(𝐹𝜃 ([x|𝛿x]), 𝑦 = −) s.t. ∥𝛿x∥𝑝 ≤ 𝜖, where | means concatenation and ∥ · ∥𝑝 is the 𝑝 norm
where 𝑝 ≥ 1. Because MalConv is learned from sequential data, perturbation means appending
some content to the end of a PE file. Perturbations are generated in a single step by following
the direction of the ℓ∞ or ℓ2 normalized gradients of the loss function [51, 71]. For instance, the
attack based on the ℓ∞ norm is ˜x′ = x − 𝜖 · sign(∇x(𝐿(𝐹𝜃 (x), −)), where sign(𝑥) = +1 (−1) if
𝑥 ≥ 0 (𝑥 < 0). Since the embedding operation uses a look-up table to map discrete values (0, 1,
. . ., 255) to the learned real-value vectors, the attack uses a nearest neighbor search to look for the
learned embedding code close to ˜x′. In summary, the attack works under the Oracle and Invertibility
assumptions with input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 1, 1, 1, 1|M, OE2, GO, XM) and breaks the RR
and CR properties.

Kolosnjaji et al. [68] and Suciu et al. [117] independently propose gradient-based attacks in
the feature space to evade MalConv [99]. Both studies also use the loss function exploited by
Kreuk et al. [70]. Kolosnjaji et al. [68] use the manipulation set M corresponding to appending
instructions at the end of a file. This attack proceeds iteratively and starts with randomly initialized
perturbations. In each iteration, continuous perturbations are updated in the direction of the ℓ2
normalized gradient of the loss function with respect to the input, and then a nearest neighbor
search is applied to obtain discrete perturbations. Suciu et al. [117] perturb embedding codes
in the direction of the ℓ∞ normalized gradient of the loss function, while adding instructions
in the mid of a PE file (e.g., between PE sections) while noting that appended content could be
truncated by MalConv. Both attacks work under the Oracle and Invertibility assumptions with
input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 1, 1, 1, 1|M, OE2, GO, XM) and break the RR and CR properties.
Muñoz-González et al. [87] propose the optimal poisoning OP attack in the feature space (Defini-
tion 13), which is NP-hard. In this case, the optimization problem of Eq.(5) is relaxed by supposing
that the classifier is linear to render the optimization problem tractable [20, 87, 132]. The at-
tack is waged against Windows PE malware detectors. Feature set includes API calls, actions
and modifications in the file system; each file is represented by a binary vector. The attack has

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:18

D. Li, Q. Li, F. Ye, and S. Xu

two variants: one uses white-box input, where A derives 𝐷 ′
𝑝𝑜𝑖𝑠𝑜𝑛 from I’s detector 𝑓 ; the other
uses grey-box input, where A knows I’s training set as well as feature set and trains a sur-
rogate detector. The attack works under the Oracle and Invertibility assumptions with input
(𝑎1, . . . , 𝑎5|𝐴6, · · · , 𝐴9) = (1, 1, 1, 1, 1|M, OP, GO, XM) ∨ (1, 0, 1, 0, 0|M, OP, GO, XM) and breaks TR.

3.1.2 Attacks using Sensitive Features (SF). Demontis et al. [39] propose the optimal evasion
OE2 in the feature space to perturb important features in terms of their weights in the linear
function 𝜑 (x) = w⊤x + 𝑏, where w = (𝑤1, 𝑤2, · · · , 𝑤𝑑 ) is a weight vector, 𝑏 is the bias, and 𝑑
is the dimension of feature space. The attack is waged against Drebin malware detector (which
is reviewed in Section 2.1). A manipulates the 𝑥𝑖 ’s with largest |𝑤𝑖 |’s as follows: flip 𝑥𝑖 = 1 to
𝑥𝑖 = 0 if 𝑤𝑖 > 0, flip 𝑥𝑖 = 0 to 𝑥𝑖 = 1 if 𝑤𝑖 < 0, and do nothing otherwise, while obeying
the manipulation set M corresponding to the injection or removal of features. The attack works
under the Oracle, Measurability, and Invertiblity assumptions with input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) =
(1, 1, 1, 1, 1|M, OE2, SF, XM) ∨ (0, 0, 1, ∗, 0|M, OE2, SF, XM) and breaks the CR property.

Grosse et al. [53] propose a variant of the Jacobian-based Saliency Map Attack (JSMA) [92] in
the feature space against the Drebin malware detector (which is reviewed in Section 2.1). Instead of
using SVM, Deep Neural Network (DNN) is used to build a detector. Important features are identified
by leveraging the gradients of the softmax output of a malware example with respect to the input.
A large gradient value indicates a high important feature. A only injects manifest features to
manipulate Android Packages and generates adversarial files from I’s detector 𝑓 . The attack works
under the Oracle, Measurability, and Invertiblity assumptions with input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) =
(0, 1, 1, 1, 1|M, OE2, SF, XM) and breaks the RR and CR properties.

Chen et al. [29] propose an evasion attack in the feature space by perturbing the important
features derived from a wrapper-based feature selection algorithm [27, 29, 96, 141]. The attacker’s
loss function 𝐿A has two parts: (i) the classification error in the mean squared loss and (ii) the
manipulation cost 𝐶 (x, x′) = (cid:205)𝑑
𝑑 ), and 𝑐𝑖 is
the hardness of perturbing the 𝑖th feature while preserving malware’s functionality. The attack is
waged against Windows PE malware detector that uses hand-crafted Windows API calls as features
and the binary feature representation. However, there are no details about the composition of
manipulation set. This attack works under the Oracle, Measurability, and Invertibility assumptions
with input (𝑎1, . . . , 𝑎5|𝐴7, · · · , 𝐴9) = (1, 1, 1, 1, 1|M, OE2, SF, XM)) and breaks CR.

𝑖 |, where x = (𝑥1, . . . , 𝑥𝑑 ), x′ = (𝑥 ′

𝑖=1 𝑐𝑖 |𝑥𝑖 − 𝑥 ′

1, . . . , 𝑥 ′

Chen et al. [31] propose evasion attacks in the feature space against two Android malware
detectors, MaMaDroid [84] and Drebin [8]. The manipulation set M corresponds to the injection of
manifest features (e.g., activities) and API calls. A evades MaMaDroid by using the optimal evasion
OE1 (Definition 10) and OE2 (Definition 11), and evades Drebin by using OE2. The optimization
problem of OE1 Eq.(3) is solved using an advanced gradient-based method known as C&W [24].
OE2 is solved using JSMA [92]. Because JSMA perturbs sensitive features, we categorize this
attack into the SF group. The OE2 attack works under the Oracle, Measurability and Invertibility
assumptions, with four kinds of input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, 1, 0, 0|M, OE2, SF, XM) ∨
(0, 0, 1, 0, 1|M, OE2, SF, XM) ∨ (1, 0, 1, 0, 0|M, OE2, SF, XM) ∨ (1, 0, 1, 0, 1|M, OE2, SF, XM), and breaks
the CR property. The OE1 attack works under the same assumptions with the same input except
using attack technique GO, and breaks the CR property.

Chen et al. [30] propose a basic poisoning BP attack in the feature space against Android
malware detectors. The feature set contains syntax features (e.g., permission, hardware, API) and
semantic features (e.g., sequence of pre-determined program behaviors such as getDevicedID→
URL→openConnection). The ML algorithm used is SVM, random forest, or 𝐾-Nearest Neighbor
(KNN) [22, 107]. The malware representations are perturbed using a JSMA variant [92] against the
SVM-based classifier (while noting JSMA is applicable neither to random forests nor to KNN because

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:19

they are gradient-free). Feature manipulation set M corresponds to the injection of syntax features.
A poisons I’s training set by injecting perturbed perturbations with label −. The attack works
under the Oracle, Measurability, and Invertibility assumptions with input (𝑎1, . . . , 𝑎5|𝐴6, · · · , 𝐴9) =
(1, 1, 1, 1, 1|M, BP, SF, XM) ∨ (0, 0, 0, 1, 1|M, BP, SF, XM) ∨ (1, 0, ∗, 1, 1|M, BP, SF, XM) and breaks TR.
Suciu et al. [118] propose a basic poisoning attack in both feature and problem spaces. The
authors obtain 𝐷 ′
𝑝𝑜𝑖𝑠𝑜𝑛 by applying small manipulations to non-adversarial benign files and then
obtain their labels as given by VirusTotal service [108]. A’s objective is to make I’s classifier
𝑓 mis-classify a targeted malware file 𝑧𝑚𝑎𝑙 as benign. A proceeds as follow: (i) obtain an initial
benign file 𝑧𝑏𝑒𝑛, where 𝑧𝑏𝑒𝑛 ≈ 𝑧𝑚𝑎𝑙 in the feature space with respect to the ℓ1 norm; (ii) use the
by making a small perturbation so that they have
JSMA method [92] to manipulate 𝑧𝑏𝑒𝑛 to 𝑧 ′
𝑝𝑜𝑖𝑠𝑜𝑛 and
similar feature representations; (iii) add 𝑧 ′
use 𝐷𝑡𝑟𝑎𝑖𝑛 ∪ 𝐷 ′
lowers the
𝑝𝑜𝑖𝑠𝑜𝑛 to train classifier 𝑓 ′ (Definition 15); (iv) undo the addition if 𝑧 ′
classification accuracy significantly, and accept it otherwise. The attack is waged against the Drebin
malware detector and the manipulation set corresponds to the feature injection of permission,
API, and strings. This attack works under the Oracle, Measurability, and Inversiability assump-
tions with input (𝑎1, . . . , 𝑎5|𝐴6, . . . , 𝐴9) = (∗, 0, 1, ∗, 0|M, BP, SF, XM) ∨ (1, 1, 1, 1, 1|M, BP, SF, XM) ∨
(1, 0, ∗, ∗, 0|M, BP, SF, XM) ∨ (1, 0, 1, 0, 0|M, BP, SF, XM) and breaks the TR property. The study gen-
erates adversarial malware examples, but does not test their malicious functionalities.

and its label obtained from VirusTotal to 𝐷 ′

𝑏𝑒𝑛

𝑏𝑒𝑛

𝑏𝑒𝑛

3.1.3 Attacks using MImicry (MI). Smutz and Stavrou [109] propose a mimicry attack in the
feature space to modify features of a malicious file to mimic benign ones, where A knows I’s
classifier 𝑓 . Discriminative features are identified by observing their impact on classification
accuracy. The attack perturbs features of malware examples by replacing their value with the
mean of the benign examples. The attack is leveraged to estimate the robustness of PDF malware
detectors without considering the preservation of malware functionality. The attack works under
the Measurability assumption with input (𝑎1, . . . , 𝑎5|𝐴6, · · · , 𝐴9) = (1, 1, 1, 1, 1|M, OE2, MI, X) and
breaks the CR property.

Maiorca et al. [82] propose a reverse mimicry attack against PDF malware detectors in the problem
space. Instead of modifying malicious files to mimic benign ones, A embeds malicious payload (e.g.,
JavaScript code) into a benign file. The attack can be enhanced by using parser confusion strategies,
which make the injected objects being neglected by feature extractors when rendered by PDF readers
[25]. The attack works under the Oracle assumption with input (𝑎1, . . . , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, 0, 0, 0|
M, BE, MI, ZM) and breaks the DR property.

Pierazzi et al. [97] propose a white-box evasion attack against the Drebin malware detector and
then an enhanced version of the detector in the problem space [8, 39]. They intend to bridge the
gap between the attacks in the problem space and the attacks in the feature space. In addition, four
realistic constraints are imposed on the manipulation set M, including available transformation,
preserved semantics, robustness to preprocessing, and plausibility. In order to cope with the side-
effect features when incorporating gradient information of I’s classifier, the attacker first harvests
a set of manipulations from benign files; Manipulations in the problem space are used to query
I’s feature extraction for obtaining perturbations in the feature space; an adversarial malware
example is obtained by using the manipulations corresponding to the perturbations that have a
high impact on the classification accuracy. This attack works under the Oracle assumption with
input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (1, 1, 1, 1, 1|M, OE2, MI, ZM) and breaks the DR property.

3.1.4 Attacks using TRansferability (TR). Šrndić and Laskov [114] investigate the mimicry attack
and the aforementioned gradient descent and kernel density estimation attack against the PDFrate
service, where A knows some features used by I. A makes the representation of an adversarial

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:20

D. Li, Q. Li, F. Ye, and S. Xu

malware example similar to a benign one. Manipulation set M corresponds to adding objects into
PDF files. Both attacks perturb feature vectors against a surrogate model, and then map the per-
turbed feature representations to the problem space by injecting manipulations between the body
and the trailer of a PDF file. For the mimicry attack, A uses 𝑁𝑏𝑒𝑛 > 0 benign examples to guide ma-
nipulations, resulting in 𝑁𝑏𝑒𝑛 perturbed examples. The example incurring the highest classification
error is used as an adversarial example. The attack works under the Oracle and Invertibility assump-
tions with input (𝑎1, . . . , 𝑎5|𝐴6, . . . , 𝐴9) = (0, 0, ∗, 0, 0|M, BE, TR, XM) ∨ (0, 0, ∗, ∗, 0|M, BE, TR, XM) ∨
(1, 0, ∗, 0, 0|M, BE, TR, XM)∨(1, 0, ∗, ∗, 0|M, BE, TR, XM)∨(0, 0, ∗, 0, 0|M, BE, TR, ZM)∨(0, 0, ∗, ∗, 0|M,
BE, TR, ZM)∨(1, 0, ∗, 0, 0|M, BE, TR, ZM)∨(1, 0, ∗, ∗, 0|M, BE, TR, ZM) and breaks DR. The gradient-
based attack neglects the constraint of small perturbations and works under the same assumptions
with the same input except using the attack technique OE2.

Khasawneh et al. [65] propose an evasion attack in both the feature and problem spaces against
malware detectors learned from dynamic hardware features (e.g., instruction frequency), where
A knows some features used by I. The attack proceeds as follows. A first queries I’s classifier
to obtain a surrogate model and then generates adversarial files against the surrogate model.
Manipulation set M corresponds to the injection of some features because the others (e.g., memory
access) are uncontrollable. Perturbations are conducted to the important features that are identified
by large weights in the model. The attack works under the Oracle and Invertibility assumptions
with input (𝑎1, . . . , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, ∗, 0, 1|M, BE, TR, XM) ∨ (0, 0, ∗, 0, 1|M, BE, TR, ZM) and
breaks the CR property.

Rosenberg et al. [100] propose an evasion attack in both the feature and problem spaces against a
Recurrent Neural Network (RNN) based surrogate model, which is learned from API call sequences.
In this attack, A’s training data is different from I’s, but the labels are obtained by querying I’s
detector. In order to reduce the number of queries to I’s detector, A augments its training data
using the Jacobian-based augmentation technique [91] and modifies the API sequence of an example
in the direction of the ℓ∞ normalized gradient of the loss function. Manipulation set M corresponds
to inserting no-op API calls. Experimental results show that adversarial examples generated from a
surrogate RNN model can evade SVM, DNN, and RNN detectors. The attack works under the Oracle
and Invertibility assumptions with input (𝑎1, . . . , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, 1, 0, 1|M, BE, TR, XM) ∨
(0, 0, 1, 0, 1|M, BE, TR, ZM) and breaks the RR and CR properties.

3.1.5 Attacks using Heuristic Search (HS). Xu et al. [134] propose black-box evasion attacks in
the problem space against two PDF malware detectors known as PDFrate [109] and Hidost [115],
respectively. Given a malicious file 𝑧, A uses a genetic algorithm to iteratively generate 𝑧 ′ from 𝑧
as follows: (i) A manipulates a set of candidates (or 𝑧 in the initial iteration) via object deletion,
insertion, or replacement. (ii) A queries these variants to O and 𝑓 . (iii) A succeeds when obtaining
a successful adversarial example 𝑧 ′, namely (True ← O (𝑧, 𝑧 ′)) ∧ (− ← 𝑓 (𝑧 ′)); otherwise, A uses a
score function to select candidates for the next iteration or aborts after reaching a threshold number
of iterations. The score function ℎ varies with classifiers; for PDFrate, ℎ(O, 𝑓 , 𝑧, 𝑧 ′) = 0.5 − 𝑓 (𝑧 ′) if
O (𝑧, 𝑧 ′) = true, and returns -0.5 if O (𝑧, 𝑧 ′) = false. This attack models an Oracle and works with
the input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, 0, 0, 1|M, BE, HS, ZM) and breaks the DR property.

Yang et al. [136] propose evasion attacks against Android malware detectors in the problem
space. In this attack, A also uses a genetic algorithm to perturb a malware example 𝑧 iteratively. In
each iteration, A extracts some features and calculates similarity scores between the malicious
APKs in the feature space; the features that have high impact on the similarity scores are selected;
the manipulations are to perturb the selected features. The attack works under the Oracle and
Measurability assumptions with input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, 0, 0, 1|M, Z, BE, HS, ZM)
and breaks the DR property.

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:21

Dang et al. [36] propose a black-box evasion attack against malware detectors (e.g., PDFrate)
in the problem space. Given a malicious file 𝑧, A uses the hill-climbing algorithm to iteratively
generate adversarial file 𝑧 ′ from 𝑧. In each iteration, A generates a path of variants sequentially,
each of which is perturbed from its predecessor using manipulations corresponding to object
deletion, insertion, or replacement. A score function ℎ is leveraged to select candidates, such as
ℎ(O, 𝑓 , 𝑧, 𝑧 ′) = mal𝑧′ − clf𝑧′ or ℎ(O, 𝑓 , 𝑧, 𝑧 ′) = mal𝑧′/clf𝑧′, where mal𝑧′ denotes the length of the first
example turned from malicious to benign (obtaining by using an oracle) on the manipulation path (cf.
Definition 18) and clf𝑧′ denotes the length of the first malware example that has successfully misled
the classifier 𝑓 . Both examples of interest are obtained by a binary search, effectively reducing
the number of queries to oracle O and 𝑓 . The attack models an Oracle and works with input
(𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, 0, 0, 0|M, BE, HS, ZM) and breaks the DR property.

3.1.6 Attacks using Generative Model (GM). Hu and Tan [58] propose an evasion attack against
Windows malware detectors in the feature space, by using Generative Adversarial Networks
(GAN) [50]. In this attack, A modifies the binary representation of Windows API calls made by
malicious files, namely flipping some feature values from 0 to 1. A learns a generator 𝐺𝜃𝑔
and a
discriminator from A’s training dataset. The discriminator is a surrogate detector learned from
feature vectors corresponding to A’s benign files and those produced by 𝐺𝜃𝑔
, along with labels
obtained by querying I’s detector 𝑓 . An adversarial example feature vector is generated by using
x′ = max(x, round(𝐺𝜃𝑔 (x, a))), where a is a vector of noises, round is the round function, and max
means element-wise maximum. Hu and Tan [59] also propose another evasion attack using the
Seq2Seq model [33]. Both attacks work under the IID, Oracle and Invertibility assumptions with
input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, 1, 0, 1|M, BE, GM, XM) and break the DR property.

Anderson et al. [5] propose a Reinforcement Learning (RL)-based evasion attack against Windows
PE malware detectors in the problem space. Manipulation set M is the RL action space, which
includes some bytecode injections (e.g., API insertion) and some bytecode deletion. Attacker A
learns an RL agent on A’s data, with labels obtained by querying defender I’s detector 𝑓 . The
learned agent predicts manipulations sequentially for a given malware example. Moreover, A is
restricted by only applying a small number of manipulations to a malicious PE file. Experimental
results show that the attack is not as effective as others (e.g., gradient-based methods). The attack
works under the Oracle and Measurability assumptions with input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) =
(0, 0, 0, 0, 1|M, OE2, GM, ZM) and breaks the RR and CR properties.

3.1.7 Attacks using Mixture Strategy (MS). Li and Li [76] propose evasion attacks against DNN-
based Android malware detectors in both feature and problem spaces. Given four gradient-based
attack methods, the attack looks for the best one to perturb malware representations. A can
iteratively perform this strategy to modify the example obtained in the previous iteration. Experi-
mental results show that the mixture of attacks can evade malware detectors effectively. The attack
works under the IID, Oracle and Invertibility assumptions with input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) =
(1, 1, 1, 1, 1|M, BE, MS, XM) ∨ (1, 1, 1, 1, 1|M, BE, MS, ZM) and breaks the DR property.

3.1.8 Drawing Observations and Insights. We summarize the preceding reviews with the follow-
ing observations. (i) Indiscriminate attacks have been much more extensively investigated than
targeted attacks and availability attacks. (ii) Evasion attacks have been much more extensively
studied than poisoning attacks. (iii) The Oracle assumption has been widely made. In addition, we
draw the following insights.

Insight 1. (i) Knowing the defender’s feature set is critical to the success of transfer attacks,
highlighting the importance of keeping the defender’s feature set secret (e.g., randomizing the defender’s
feature set). (ii) The effectiveness of evasion attacks largely depends on the attacker’s degree of freedom

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:22

D. Li, Q. Li, F. Ye, and S. Xu

in conducting manipulations in the problem space (i.e., a smaller degree of freedom means it is harder
for the attack to succeed).

3.2 Systematizing Defense Literature

Table 5. Summary of AMD defenses (✓means applicable, means 0, means 1, means a value in [0, 1])

Defense
(in chronological order)

Defense
Objective

Defense Input

Assumptions

Achieved
Properties

Malware
Detector

𝑛
𝑖
𝑎
𝑟
𝑡

e
u
q
i
n
h
c
e
t

𝐷

t
e
s

t
e
s

:
2
𝐴

e
r
u
t
a
e
F

e
s
n
e
f
e
D

g
n
i
n
i
a
r
T

n
o
i
t
c
e
t
e
d
e
r
a
w
l
:
a
3
m
𝐴
✓
Biggio et al. [15]
𝑆
✓
Smutz and Stavrou [110]
𝑆
✓
Zhang et al. [141]
𝑆
✓
Demontis et al. [39]
𝑆
✓
Wang et al. [127]
𝑆
✓
Grosse et al. [53]
𝑆
✓
Grosse et al. [53]
𝑆
✓
Chen et al. [29]
𝑆
✓
Khasawneh et al. [65]
𝑆
✓
Dang et al. [36]
𝑆
✓
Yang et al. [136]
𝑆
✓
Yang et al. [136]
𝑆
✓
Chen et al. [27]
𝑆
✓
Incer et al. [61]
𝑆
✓
Chen et al. [30]
𝑆
✓
Al-Dujaili et al. [2]
𝑆
✓
Chen et al. [28]
𝑆
✓
Jordan et al. [62]
𝑆
✓
Li et al. [77]
𝑆
✓
Tong et al. [122]
𝑆
✓
Li and Li [76]
𝑆
✓
Chen et al. [32]
𝑆
✓
Li et al. [78]
𝑆
𝑡𝑟𝑎𝑖𝑛 contains 𝐷𝑡𝑟𝑎𝑖𝑛 and a portion of A’s adversarial examples.

:
1
𝐴
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷 ∗
𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛
𝐷𝑡𝑟𝑎𝑖𝑛 EL+AT+RF

EL
SE
RF
WR
IT
WR
AT
AT
CD
SE
AT
SE
RF
VL
SE
AT
IT
RF
AT
RF
AT
VL

𝐷∗

m
h
t
i
r
o
g
l
a

g
n
i
n
r
a
e
L

t
e
s
n
o
i
t
a
l
u
p
i
n
a
M

c
i
t
c
a
t
k
c
a
t
t

A

e
u
q
i
n
h
c
e
t
k
c
a
t
t

A

e
s
n
o
p
s
e
R

t
e
s

e
l
p
m
a
x
e

l
a
i
r
a
s
r
e
v
d
A

:
6
𝐴

:
7
𝐴

:
8
𝐴

:
9
𝐴

:
:
4
5
𝐴
𝐴
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 LQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ
𝐹𝜃 FQ

n
o
i
t
p
m
u
s
s
a
y
t
i
l
i
b
a
r
u
s
a
e
M

n
o
i
t
p
m
u
s
s
a

s
s
e
n
h
t
o
o
m
S

n
o
i
t
p
m
u
s
s
a
y
t
i
l
i
b
i
t
r
e
v
n
I

n
o
i
t
p
m
u
s
s
a

e
l
c
a
r
O

n
o
i
t
p
m
u
s
s
a
D

I
I

✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓

✓
✓

✓
✓
✓
✓
✓

✓ ✓
✓

✓
✓

✓ ✓

✓

✓ ✓

✓ ✓
✓

s
s
e
n
t
s
u
b
o
R
g
n
i
n
i
a
r
T

:

R
T

s
s
e
n
t
s
u
b
o
R
n
o
i
t
c
e
t
e
D

:

R
D

✓

m
a
r
g
o
r
P
s
w
o
d
n
i
W

e
g
a
k
c
a
P
d
i
o
r
d
n
A

s
s
e
n
t
s
u
b
o
R
n
o
i
t
a
t
n
e
s
e
r
p
e
R

:

R
R

s
s
e
n
t
s
u
b
o
R
n
o
i
t
a
c
fi
i
s
s
a
l
C

:

R
C
✓

✓ ✓
✓
✓
✓ ✓
✓ ✓
✓
✓

✓
✓
✓

✓ ✓
✓ ✓ ✓

✓

✓

✓
✓ ✓ ✓
✓ ✓
✓ ✓ ✓
✓

✓ ✓

✓

F
D
P
✓
✓
✓

✓

✓

✓

✓

✓

✓
✓

✓

✓

✓

✓

✓
✓

✓
✓
✓

✓

✓

✓

✓

3.2.1 Defenses using Ensemble Learning (EL). Biggio et al. [15] propose a one-and-a-half-class
SVM classifier against evasion attacks, by leveraging an interesting observation (i.e., decision bound-
aries of one-class SVM classifiers are tighter than that of two-class SVM classifiers) to facilitate
outlier detection. Specifically, the authors propose an ensemble of a two-class classifier and two
one-class classifiers, and then combine them using another one-class classifier. The defense can
enhance PDF malware detectors against gradient-based attacks [16], which can be characterized as
(𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (1, 1, 1, 1, 1|M, OP2, GO, XM). However, the defense cannot thwart attacks
incurring large perturbations. Independent of this study, other researchers propose using the random
subspace and bagging techniques to enhance SVM-based malware detectors, dubbed Multiple Clas-
sifier System SVM (MCS-SVM), which leads to evenly distributed weights [18, 39]. These defenses
work under the IID assumption with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, EL, 𝑆, 𝐹𝜃, FQ |0, 1, 0, 0)
and achieves the CR property.

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:23

3.2.2 Defenses using Weight Regularization (WR). Demontis et al. [39] propose enhancing
the Drebin malware detector 𝜑 (x) = w⊤x + 𝑏 by using box-constraint weights. The inspira-
tion is that the classifier’s sensitivity to perturbations based on the ℓ1 norm is bounded by the
ℓ∞ norm of the weights. This defense hardens the Drebin detector against a mimicry attack
with input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, 1, 0, 0|M, BE, MI, XM), obfuscation attack [41] with in-
put (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, 0, 0, 0|M, BE, −, ZM), and the attack that modifies important
features [39] with input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (1, 1, 1, 1, 1|M, OE2, SF, XM), here ‘−’ means
inapplicable. Experimental results show this defense outperforms MSC-SVM [18]. The defense
works under the IID, Oracle and Measurability assumptions with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) =
(𝐷𝑡𝑟𝑎𝑖𝑛, WR, 𝑆, 𝐹𝜃, FQ |1, 1, 0, 0) and achieves CR.

Grosse et al. [53] investigate how to apply two defense techniques known as distillation [93]
and retraining [120] to enhance the DNN-based Drebin malware detector. The distillation tech-
nique can decrease a model’s generalization error by leveraging a teacher to relabel the train-
ing data represented by real-value vectors (rather than one-hot encoding). It uses retraining to
tune a learned model with respect to an augmented training set with adversarial examples. Both
defenses are estimated against a variant of JSMA and can be characterized by their input as
(𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (0, 1, 1, 1, 1|M, OE2, SF, XM). Experimental results show the two defenses
achieve limited success. The defense based on the distillation technique works under the IID
assumption with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, WR, 𝑆, 𝐹𝜃, FQ |0, 1, 0, 0) and achieves the
RR and CR properties. The defense based on the retraining technique works under the IID and
Measurability assumptions with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ |1, 1, 1, 0) and
achieves the RR and CR properties.

3.2.3 Defenses using Adversarial Training (AT). Chen et al. [29] adapt a generic retraining frame-
work proposed in the AML context [75] to enhance linear malware detectors. The defense uses a
label smoothness regularization technique to mitigate the side-effect of adversarial training [135].
The defense is evaluated using Windows malware detectors against “feature selection”-based eva-
sion attacks, which can be characterized as (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (1, 1, 1, 1, 1|M, OE2, SF, XM).
The defense works under the IID and Measurability assumptions and can be characterized as
(𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ |1, 1, 1, 0), while assuring the CR property.

Yang et al. [136] propose a defense against genetic algorithm-based evasion attacks that can
be characterized as (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (0, 0, 0, 0, 1|M, BE, HS, ZM). The defense uses three
techniques: adversarial training, sanitizing examples, and weight regularization [39]. The adversarial
training uses one half of A’s adversarial examples. The defense works under the IID assumption
and can be characterized as (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷 ′
𝑡𝑟𝑎𝑖𝑛
is the union of 𝐷𝑡𝑟𝑎𝑖𝑛 and a portion (e.g., one half) of A’s adversarial examples. The defense of
sanitizing examples is learned from manipulations used by the attacker and works under the
IID assumption with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, SE, 𝑆, 𝐹𝜃, FQ |1, 1, 1, 0). Both defenses
achieve the DR property. The defense of wight regularization is reviewed in Section 3.2.2.

𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ |0, 1, 0, ∗), where 𝐷 ′

Al-Dujaili et al. [2] adapt the idea of minmax adversarial training (proposed in the AML context)
to enhance DNN-based malware detectors. In this defense, the inner-layer optimization gener-
ates adversarial files by maximizing the classifier’s loss function; the outer-layer optimization
searches for the parameters 𝜃 (of DNN 𝐹𝜃 ) that minimize the classifier’s loss with respect to the
adversarial files. The defense enhances Windows malware detectors against attacks with input
(𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (0, 1, 1, 1, 1|M, OE2, GO, XM). Experimental results show that malware
detectors that are hardened to resist one attack may not be able to defend against other attacks. By
observing this phenomenon, researchers propose using a mixture of attacks to harden DNN-based

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:24

D. Li, Q. Li, F. Ye, and S. Xu

malware detectors [76]. The defense works under the IID assumption and can be characterized as
(𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ |1, 1, 0, 0). The defense assures the DR property.

Li et al. [77] propose a DNN-based attack-agnostic framework to enhance adversarial malware
detectors. The key idea is dubbed adversarial regularization, which enhances malware detectors via
the (approximately) optimally small perturbation. The framework wins the AICS’2019 adversarial
malware classification challenge organized by MIT Lincoln Lab researcher [131], without knowing
anything about the attack. The defense works under the IID, Measurability, and Smoothness
assumptions with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ |0, 1, 0, 0) and assures the
RR and CR properties. In the extended study [78], the authors further enhance the framework with
6 defense principles, including ensemble learning, adversarial training, and robust representation
learning. The enhanced defense is validated with 20 attacks (including 11 grey-box attacks and
9 white-box attacks) against Android malware detectors. The enhanced defense works under
the IID and Measurability assumptions with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, AT + EL +
RF, 𝑆, 𝐹𝜃, FQ |1, 1, 0, 0) and assures the DR property.

3.2.4 Defenses using Verifiable Learning (VL). Incer et al. [61] propose using monotonic malware
classifiers to defend against evasion attacks, where monotonic means 𝜑 (x) ≤ 𝜑 (x′) when x ≤ x′
[54]. Technically, this can be achieved by using (i) robust features that can only be removed
or added but not both and (ii) monotonic classification function (e.g., linear models with non-
negative weights). The resulting classifier can thwart any attack that perturbs feature values
monotonically. The defense works under the IID assumption with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) =
(𝐷𝑡𝑟𝑎𝑖𝑛, VL, 𝑆, 𝐹𝜃, FQ |1, 1, 0, 0) and assures the RR, CR and DR properties.

Chen et al. [32] propose a defense to enhance PDF malware detectors against evasion attacks, by
leveraging the observation that manipulations on PDF files are subtree additions and/or removals.
They also propose new metrics for quantifying such structural perturbations. This allows to adapt
the symbolic interval analysis technique proposed in the AML context [128] to enhance the PDF
malware detectors. The defense can cope with attacks leveraging small perturbations in the training
phase. This defense works under the IID, Measurability, and Smoothness assumptions with input
(𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, VL, 𝑆, 𝐹𝜃, FQ |1, 1, 0, 0) and achieves the RR and CR properties.

3.2.5 Defenses using Robust Features (RF). Zhang et al. [141] propose leveraging optimal adver-
sarial attacks for feature selection. The defense enhances PDF malware detectors against gradient-
based attacks, which can be characterized as (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (1, 1, 1, 1, 1|M, OE2, GO, XM).
The defense works under the IID, Measurability, and Smoothness assumptions and can be charac-
terized as (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, RF, 𝑆, 𝐹𝜃, FQ |1, 1, 0, 0). The defense assures the RR and
CR properties.

Tong et al. [122] propose refining features into invariant ones to defend against genetic algorithm-
based evasion attacks with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (0, 0, 0, 0, 1|M, BE, HS, ZM). Experi-
mental results show that adversarial training can be further leveraged to enhance the robust-
ness of the defense. The defense works under IID assumption with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9)
= (𝐷𝑡𝑟𝑎𝑖𝑛, RF, 𝑆, 𝐹𝜃, FQ |1, 1, 0, 0), and achieves RR, CR and DR.

Chen et al. [27] propose mitigating evasive attacks by filtering features according to their impor-
tance |𝑤𝑖 |/𝑐𝑖 with respect to the linear function 𝜑 (x) = w⊤x + 𝑏, where 𝑥𝑖 , 𝑤𝑖 and 𝑐𝑖 denote respec-
tively the 𝑖th component of x, w and the constraint on manipulation cost c. The defense enhances An-
droid malware detectors against three attacks: a random attack with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) =
(0, 0, 1, 0, 0|M, BE, −, XM), a variant of the mimicry attack with input (0, 0, 1, 0, 0|M, BE, MI, XM), and
the attack that modifies important features with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (1, 1, 1, 1, 1|M, BE, SF,

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:25

XM), where ‘−’ means inapplicable. The defense works under the IID, Measurability, and Smooth-
ness assumptions with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, RF, 𝑆, 𝐹𝜃, FQ |1, 1, 0, 0) and achieves
RR and CR.

Jordan et al. [62] propose a robust PDF malware detector against evasion attacks by inter-
preting JavaScript behaviors using static analysis. A PDF file is classified as malicious when it
calls a vulnerable API method or when it exhibits potentially malicious or unknown behaviors.
The defense is validated against the reverse mimicry attack [82] with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) =
(0, 0, 0, 0, 0|M, BE, MI, ZM). The defense has input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, RF, 𝑆, 𝐹𝜃, FQ |1,
1, 0, 0) and achieves RR, CR and DR.

3.2.6 Defenses using Input Transformation (IT). Wang et al. [127] propose the random feature
nullification to enhance DNN-based malware detectors against the attack of Fast Gradient Sign
Method (FGSM) [51] by nullifying (or dropping) features randomly in both training and testing
phases. This offers a probabilistic assurance in preventing a white-box attacker from deriving
adversarial files by using gradients of the loss function with respect to the input. The defense
enhances Windows malware detectors against the FGSM attack with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) =
(0, 1, 1, 1, 1|M, OE2, GO, XM). The defense works under IID assumption with input (𝐴1, · · · , 𝐴5|𝑎6,
· · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, IT, 𝑆, 𝐹𝜃, FQ |0, 1, 0, 0) and achieves CR.

DroidEye [28] defends Android malware detectors against evasion attacks by quantizing bi-
nary representations, namely transforming binary representations into real values and then us-
ing compression to reduce the effect of adversarial manipulations. The defense enhances linear
malware detectors against a “feature selection”-based attack with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) =
(1, 1, 1, 1, 1|M, OE2, SF, XM) [29] and the FGSM attack with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (0, 1, 1, 1,
1|M, OE2, GO, XM) [51]. The defense works under IID assumption with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9)
= (𝐷𝑡𝑟𝑎𝑖𝑛, IT, 𝑆, 𝐹𝜃, FQ |0, 1, 0, 0) and achieves CR.

3.2.7 Defenses using Classifier Randomization (CD). Khasawneh et al. [65] propose randomizing
classifiers (i.e., using one randomly chosen from a pool of classifiers that use heterogeneous
features) to defend against transfer attacks. The defense is validated against an attack which
perturbs important features with input (𝑎1, . . . , 𝑎5|𝑎6, · · · , 𝑎9) = (0, 0, ∗, 0, 1|M, BE, SF, ZM). The
defense works under the IID assumption with input (𝑎1, . . . , 𝑎5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, CD, 𝑆, 𝐹𝜃,
FQ |0, 1, 1, 0) and achieves the CR property.

3.2.8 Defenses using Sanitizing Examples (SE). Smutz and Stavrou [110] propose an ensemble
classifier to defend against grey-box evasion attacks by returning classification results as benign,
uncertain and malicious according to the voting result (e.g., [0%, 25%] classifiers saying malicious
can be treated as benign, [25%, 75%] saying malicious can be treated as uncertain, and [75%, 100%]
saying malicious can be treated as malicious). The defense enhances a PDF malware detector against
three types of evasion attacks: gradient-based attack [114] with input (𝑎1, . . . , 𝑎5|𝐴6, · · · , 𝐴9) =
(1, 0, ∗, ∗, 0|M, OE2, TR, ZM), mimicry attack with input (1, 0, ∗, ∗, 0|M, BE, TR, ZM), and reverse
mimicry attack with input (0, 0, 0, 0, 0|M, BE, MI, ZM) [82]. The defense works under the IID
assumption with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, SE, 𝑆, 𝐹𝜃, FQ |0, 1, 0, 0) and achieves DR.

Dang et al. [36] propose enhancing PDF malware detectors by lowering the classification thresh-
old 𝜏 and restricting the maximum query times, rendering genetic algorithm-based evasion attacks
harder to succeed. This defense works under the IID assumption with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) =
(𝐷𝑡𝑟𝑎𝑖𝑛, SE, 𝑆, 𝐹𝜃, LQ |0, 1, 0, 0) and achieves DR.

Chen et al. [30] investigate defending Android malware detectors against poisoning attacks with
input (𝑎1, . . . , 𝑎5|𝐴6, · · · , 𝐴9) = (1, 1, 1, 1, 1|M, BP, SF, XM) The idea is to filter adversarial files that

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:26

D. Li, Q. Li, F. Ye, and S. Xu

are distant from non-adversarial ones, where distance is measured by the Jaccard index, Jaccard-
weight similarity and cosine similarity. The defense works under the Measurability assumption
with input (𝐴1, · · · , 𝐴5|𝑎6, · · · , 𝑎9) = (𝐷𝑡𝑟𝑎𝑖𝑛, SE, 𝑆, 𝐹𝜃, FQ |0, 1, 0, 0) and achieves TR.

3.2.9 Drawing Observations and Insights. Summarizing the preceding discussions, we draw the
following observations. (i) Most studies focus on black-box defenses (i.e., the defender knows
little about the attacker), which is against the principle of “knowing yourself and knowing your
enemy". (ii) Most studies focus on defenses against evasion attacks rather than poisoning attacks.
(iii) There is no silver bullet defense against evasion attacks or poisoning attacks, at least for now.
(iv) Sanitizing adversarial files as outliers is effective against black-box and grey-box attacks, but not
white-box attacks. (v) The security properties achieved by defenses have been evaluated empirically
rather than rigorously proven (despite that provable security is emerging on the small degree of
perturbations; see for example [32, 48]). (vi) There is no theoretical evidence to support that the
effectiveness of defense tactics on the training set (e.g., adversarial training and verifiable learning)
can generalize to other adversarial examples. In addition, we draw the following insights:

Insight 2. (i) Effective defenses often require the defender to know the attacker’s manipulation set.
In the real world, it is hard to achieve this, explaining from one perspective why it is hard to design
effective defenses. (ii) The effectiveness of adversarial training depends on the defender’s capability in
identifying the most powerful attack.

3.3 Systematizing AMD Arms Race
Figure 5 displays AMD attack-defense arms race surrounding three malware detectors: PDFrate,
Drebin, and DNN-based detector. For a better visual effect, we group papers that proposed defense
methods in terms of a common input (𝑎6, . . . , 𝑎9). For example, we group [136],[53] and [29]
together because the defenders in both papers have input (𝑎6, . . . , 𝑎9) = (1, 1, 1, 0), while noting
that their input on (𝐴1, . . . , 𝐴5) may or may not be different. We also simplify attack and defense
inputs while preserving the critical information when an attack (defense) works for multiple inputs.
For example, (𝑎1, . . . , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, 1, 0, 0|M, OE2, SF, XM) is the critical information for
attack input (𝑎1, · · · , 𝑎5|𝐴6, · · · , 𝐴9) = (0, 0, 1, 0, 0|M, OE2, SF, XM) ∨ (0, 0, 1, 0, 1|M, OE2, SF, XM) ∨
(1, 0, 1, 0, 0|M, OE2, SF, XM) ∨ (1, 0, 1, 0, 1|M, OE2, SF, XM) because it is the weakest attack input
in the partial order formulated by these (𝑎1, . . . , 𝑎5)’s. This suggests us to focus on attack input
(0, 0, 1, 0, 0|M, OE2, SF, XM) because it is already able to break some defense and automatically
implies that a stronger input can achieve the same (while noting some special cases, see discussion
in Section 3.2.1). Multiple defense inputs are simplified in the same manner.

Arms race in PDF malware detection: We summarize two sequences of escalations caused by
PDFrate [109]. In one sequence, PDFrate is defeated by transfer attacks, which are realized by
gradient-based and mimicry methods against surrogate models [114]. These attacks trigger the
defense escalation to an ensemble detector built on top of some diversified classifiers [110]. This
defense [110] triggers attack escalation to reverse mimicry attacks [82], which trigger the defense
escalation of using robust hand-crafted features [62]. This defense represents the state-of-the-art
PDF malware detector, but still incurs a high false-positive rate. In the other sequence of arms race,
PDFrate is defeated by genetic algorithm-based attacks [134]. These attacks trigger the defense
escalation to [36] and [122]. The former defense [36] restricts the responses to attacker queries, but
can be defeated by the escalated attack that leverages the hill-climbing algorithm (also shown in
[36]). The latter defense [122] uses invariant features to thwart the attacks and represents another
state-of-the-art PDF malware detectors.

Arms race in android malware detection: Drebin is defeated by the attack that modifies a limited
number of important features [39], which also proposes the new defense to defeat the escalated

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

[31]
(0, 0, 1, 0, 0)
(M, OE1, GO, XM)
(M, OE2, SF, XM)

[39]
(0, 0, 1, ∗, 0)
(M, OE2, SF, XM)

[136]
(0, 0, 0, 0, 1)
(M, BE, HS, ZM)

[97]
(1, 1, 1, 1, 1)
(M, OE2, MI, ZM)

[53]
(0, 1, 1, 1, 1)
(M, OE2, SF, XM)

Legend:

Waging
Attack
[2]
(0, 1, 1, 1, 1)
(M, OE2, GO, XM)

Defense
Escalation

Attack
Escalation

[76]
(0, 1, 1, 1, 1)
(M, OE2, MS, XM)

[68, 70, 117]
(0, 1, 1, 1, 1)
(M, OE2, GO, XM)

F
i
g

.

5

.

A
r
m

s

r
a
c
e

i

n
A
M
D
a
tt
a
c
k
a
n
d
d
e
f
e
n
s
e

e
s
c
a
l
a
t
i
o
n
s
.

(0, 0, 0, 0, 1)
(M, BE, HS, ZM)
[134]

(0, 0, 0, 0, 0)
(M, BE, HS, ZM)
[36]

(0, 0, 0, 0, 0)
(M, BE, MI, ZM)
[25]

(0, 0, 0, 0, 0)
(M, BE, MI, ZM)
[82]

[8, 84]

PDFrate [109]

Drebin [8]
(𝐷𝑡𝑟𝑎𝑖𝑛, ∅, 𝑆, 𝐹𝜃, FQ)
(0, 0, 0, 0)

(𝐷𝑡𝑟𝑎𝑖𝑛, ∅, 𝑆, 𝐹𝜃, FQ)
(0, 0, 0, 0)

[36]:(𝐷𝑡𝑟𝑎𝑖𝑛, SE, 𝑆, 𝐹𝜃, LQ)
[77]:(𝐷𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ)
[127]:(𝐷𝑡𝑟𝑎𝑖𝑛, IT, 𝑆, 𝐹𝜃, FQ)
[53]:(𝐷𝑡𝑟𝑎𝑖𝑛, WR, 𝑆, 𝐹𝜃, FQ)
[30]:(𝐷𝑡𝑟𝑎𝑖𝑛, SE, 𝑆, 𝐹𝜃, FQ)
[110]:(𝐷𝑡𝑟𝑎𝑖𝑛, SE, 𝑆, 𝐹𝜃, FQ)
[28]:(𝐷𝑡𝑟𝑎𝑖𝑛, IT, 𝑆, 𝐹𝜃, FQ)
[15]:(𝐷𝑡𝑟𝑎𝑖𝑛, EL, 𝑆, 𝐹𝜃, FQ)
(0, 1, 0, 0)

PDFrate [109]

(𝐷𝑡𝑟𝑎𝑖𝑛, ∅, 𝑆, 𝐹𝜃, FQ)
(0, 0, 0, 0)

DNN-based malware
detector [53]
(𝐷𝑡𝑟𝑎𝑖𝑛, ∅, 𝑆, 𝐹𝜃, FQ)
(0, 0, 0, 0)

[99]

[8]

[136]:(𝐷𝑡𝑟𝑎𝑖𝑛, SE, 𝑆, 𝐹𝜃, FQ)
[53]:(𝐷𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ)
[29]:(𝐷𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ)
(1, 1, 1, 0)

(∗, 0, 1, ∗, 0)
(1, 0, 1, 0, 0)
(1, 0, ∗, ∗, 0)
(M, BP, SF, XM)
(M, BP, SF, ZM)
[118]

(1, 1, 1, 1, 1)
(M, OE2, MI, X)
[109]

(1, 1, 1, 1, 1)
(M, OE2, SF, XM)
[29]

[39]:(𝐷𝑡𝑟𝑎𝑖𝑛, WR, 𝑆, 𝐹𝜃, FQ)
[122]:(𝐷𝑡𝑟𝑎𝑖𝑛, RF, 𝑆, 𝐹𝜃, FQ)
[61]:(𝐷𝑡𝑟𝑎𝑖𝑛, RF, 𝑆, 𝐹𝜃, FQ)
[62]:(𝐷𝑡𝑟𝑎𝑖𝑛, RF, 𝑆, 𝐹𝜃, FQ)
[2]:(𝐷𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ)
[76]:(𝐷𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ)
[32]:(𝐷𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ)
[27]:(𝐷𝑡𝑟𝑎𝑖𝑛, RF, 𝑆, 𝐹𝜃, FQ)
[141]:(𝐷𝑡𝑟𝑎𝑖𝑛, RF, 𝑆, 𝐹𝜃, FQ)
(1, 1, 0, 0)

[65]:(𝐷𝑡𝑟𝑎𝑖𝑛, CD, 𝑆, 𝐹𝜃, FQ)
(0, 1, 1, 0)

[136]:(𝐷 ∗

𝑡𝑟𝑎𝑖𝑛, AT, 𝑆, 𝐹𝜃, FQ)
(0, 1, 0, ∗)

(0, 0, ∗, 0, 0)
(M, BE, TR, ZM)
[114]

(0, 0, 0, 0, 1)
(M, OE2, GO, ZM)
[5]

(0, 0, ∗, 0, 1)
(M, BE, TR, ZM)
[65]

(0, 0, 1, ∗, 0)
(M, OE2, GO, XM)
[16]

(0, 0, 1, 0, 1)
(M, BE, GM, XM)
[58, 59]

(0, 0, 1, 0, 1)
(M, BE, TR, ZM)
[100]

(0, 0, 0, 1, 1)
(M, BP, SF, XM)
[30]

(M, OP, GO, XM)
(1, 0, 1, 0, 0)
[87]

A
C
M
C
o
m
p
u
t
i
n
g

S
u
r
v
e
y
s
,

V
o
l
.

1

,

N
o

.

1

,

A
r
t
i
c
l
e

.

P
u
b
l
i
c
a
t
i
o
n
d
a
t
e
:

S
e
p
t
e
m
b
e
r

2
0
2
1

.

A
r
m

s
R
a
c
e

i

n
A
d
v
e
r
s
a
r
i
a
l

M
a
l
w
a
r
e
D
e
t
e
c
t
i
o
n

:

A
S
u
r
v
e
y

:

2
7

:28

D. Li, Q. Li, F. Ye, and S. Xu

attack. This defense [39] triggers the attack escalation to, and is defeated by, the genetic algorithm-
based attack [136] and the mimicry-alike attack [97]. The former attack [136] triggers the escalated
defense (also presented in [136]) that leverages attack mutations to detect adversarial examples
[136]. The latter attack [97] injects objects in APKs and (in principle) can be defeated by the
monotonic classifier [61, 97]. These escalated defenses represent the state-of-the-art Android
malware detectors, but still incur a high false-positive rate.

Arms race in DNN-based malware detection: The DNN-based detector [53] triggers four gradient-
based evasion attacks presented in [2], which also hardens the DNN malware detector by using an
minmax adversarial training instantiation to incorporates the ℓ∞ normalized gradient-based attack.
This escalated defense [2] triggers the mixture of attacks presented in [76]. The defense of minmax
adversarial training incorporating a mixture of attacks can defeat a broad range of attacks, but still
suffers from the mimicry attack and other mixtures of attacks [76]. As a consequence, there are no
effective defenses can thwart all kinds of attacks.

Independent arms race: There are studies that have yet to trigger cascading arms races, including:
(i) Studies [29, 30, 118, 127] propose independent attacks and then show how to defeat these attacks.
(ii) Studies [31, 58, 59, 68, 70, 87, 100, 117] propose attacks to defeat naive malware detectors. (iii)
Studies propose defenses to counter some attacks [15, 27, 28, 32, 77].

4 FUTURE RESEARCH DIRECTIONS (FDRS)
FRD 1: Pinning down the root cause(s) of adversarial malware examples. Speculations on root cause(s)
include: (i) invalidity of the IID assumption because of distribution drifting, namely that testing
files and training files are drawn from different distributions [16, 19, 52]); (ii) incompetent feature
extraction [39, 134]; (iii) high dimensionality of malware representations [48]; (iv) insufficient scale
of training data [104]; (v) low-probability “pockets” in data manifolds [120]; (vi) linearity of DNNs
[51]; and (vii) large curvature of decision boundaries [46, 85].Although these speculations may be
true, more studies are needed in order to (in)validate them.

FRD 2: Characterizing the relationship between transferability and vulnerability. In the AMD context,
an attacker may use a surrogate model to generate adversarial examples and a defender may use a
surrogate model for adversarial training. Transferability is related to the extent at which knowledge
gained by a surrogate model may be the same as, or similar to, what is accommodated by a
target model. The wide use of surrogate models in the AMD context suggests that there may be a
fundamental connection between knowledge transferability and model vulnerability.

FRD 3: Investigating adversarial malware examples in the wild. In the AMD context, it is challenging
to generate practical adversarial malware examples to correspond to perturbations conducted
in the feature space, owing to realistic constraints. On the other hand, an attacker can directly
search for manipulations in the problem space. This may cause large perturbations, putting the
value of studies on small perturbations in question. This represents a fundamental open problem
that distinguishes the field of AMD from its counterparts in other application settings. This issue
is largely unaddressed by assuming that there is an oracle for telling whether manipulated or
perturbed features indeed correspond to a malware sample or not.

FRD 4: Quantifying the robustness and resilience of malware detectors. Robustness and resilience
of malware detectors against adversarial examples need to be quantified, ideally with a provable
guarantee. For this purpose, one may adapt the reduction-based paradigm underlying the provable
security of cryptographic primitives and protocols.

FRD 5: Designing malware detectors with provable robustness and resilience guarantees. Having
understood the root cause(s) of adversarial examples, characterized the effect of transferability,

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:29

investigated the effectiveness of practical attacks, and designed metrics for quantifying the robust-
ness and resilience of malware detectors, it is imperative to investigate robust malware detectors
with provable robustness, ideally as rigorous as what has been achieved in the field of cryptography.
In this regard, robust feature extraction, adversarial learning, and verifiable learning are promising
candidates for making breakthroughs.
FRD 6: Forecasting the arms race in malware detection. Arms race is a fundamental phenomenon
inherent to the cybersecurity domain. In order to effectively defend against adversarial malware,
one approach is to deploy proactive defense, which requires the capability to forecast the arms
race between malware writers and defenders. For instance, it is important to predict how attacks
will evolve and what kinds of information would be necessary in order to defeat such attacks.

5 CONCLUSION
We have presented a framework for systematizing the field of AMD through the lens of assumptions,
attacks, defenses and security properties. This paves the way for precisely relating attacks and
defenses. We have also shown how to apply the framework to systematize the AMD literature,
including the arms race between AMD attacks and defenses. We have reported a number of insights.
The study leads to a set of future research directions. In addition to the ones described in Section
4, we mention the following two, which are discussed here because there are rarely studies on
these aspects. (i) To what extent explainability (or interpretability) of ML models can be leveraged
to cope with adversarial malware examples? It is intuitive that explainability could be leveraged
to recognize adversarial examples because they may not be explainable [37]. (ii) To what extent
uncertainty quantification can be leveraged to cope with adversarial malware examples? If the
uncertainty associated with detectors’ predictions on adversarial malware examples are inherently
and substantially higher than the uncertainty associated with non-adversarial malware examples,
this fact can be leveraged to recognize adversarial malware examples. Finally, we reiterate that the
research community should seek to establish a solid foundation for AMD. Although this foundation
can leverage ideas and techniques from AML, the unique characteristics of AMD warrant the need
of a unique foundation.

REFERENCES

[1] Tony Abou-Assaleh, Nick Cercone, Vlado Keselj, and et al. 2004. N-gram-based detection of new malicious code. In
Proceedings of the 28th Annual International Computer Software and Applications Conference, Vol. 2. IEEE Computer
Society, Hong Kong, China, 41–42.

[2] Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, and Una-May O’Reilly. 2018. Adversarial Deep Learning for Robust
Detection of Binary Encoded Malware. In 2018 IEEE Security and Privacy Workshops (SPW). IEEE, Francisco, CA, USA,
76–82.

[3] Victor M. Alvarez. 2019. Yara. Avaiabled at http://virustotal.github.io/yara/ (2019/05/02). (2019).
[4] Blake Anderson, Curtis Storlie, and Terran Lane. 2012. Improving malware classification: bridging the static/dynamic
gap. In Proceedings of the 5th ACM workshop on Security and artificial intelligence. ACM, Raleigh, NC, USA, 3–14.
[5] Hyrum S. Anderson, Anant Kharkar, Bobby Filar, David Evans, and Phil Roth. 2018. Learning to Evade Static PE
Machine Learning Malware Models via Reinforcement Learning. CoRR abs/1801.08917 (2018). http://arxiv.org/abs/
1801.08917

[6] Hyrum S Anderson, Anant Kharkar, Bobby Filar, and Phil Roth. 2017. Evading machine learning malware detection.

Black Hat (2017).

[7] Axelle Aprville and Ange Albertini. 2014. Hide android applications in images. https://www.blackhat.com/. (2014).

Online; accessed October 2014.

[8] Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon, Konrad Rieck, and CERT Siemens. 2014. DREBIN:
Effective and Explainable Detection of Android Malware in Your Pocket. In NDSS, Vol. 14. The Internet Society, San
Diego, California, USA, 23–26.

[9] Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. 2010. The security of machine learning. Machine

Learning 81, 2 (2010), 121–148.

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:30

D. Li, Q. Li, F. Ye, and S. Xu

[10] Marco Barreno, Blaine Nelson, Russell Sears, Anthony D Joseph, and et al. 2006. Can machine learning be secure?. In
Proceedings of the 2006 ACM Symposium on Information, computer and communications security. ACM, Taipei, Taiwan,
16–25.

[11] Karel Bartos, Michal Sofka, and Vojtech Franc. 2016. Optimized invariant representation of network traffic for detecting
unseen malware variants. In 25th {USENIX} Security Symposium ({USENIX} Security 16). USENIX Association, Austin,
TX, USA, 807–822.

[12] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, and Antonio Criminisi.
2016. Measuring neural net robustness with constraints. In Advances in neural information processing systems. Curran
Associates, Inc., Barcelona, Spain, 2613–2621.

[13] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives.

IEEE T-PAMI 35, 8 (2013), 1798–1828.

[14] Benjamin Bichsel, Veselin Raychev, Petar Tsankov, and Martin Vechev. 2016. Statistical Deobfuscation of Android
Applications. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. Association
for Computing Machinery, New York, NY, USA, 343–355.

[15] Battista Biggio, Igino Corona, and et al. 2015. One-and-a-half-class multiple classifier systems for secure learning
against evasion attacks at test time. In International Workshop on Multiple Classifier Systems. Springer, Günzburg,
Germany, 168–180.

[16] Battista Biggio, Igino Corona, Davide Maiorca, and et al. 2013. Evasion Attacks against Machine Learning at Test
Time. In Machine Learning and Knowledge Discovery in Databases: European Conference. Springer, Prague, Czech
Republic, 387–402.

[17] Battista Biggio, Igino Corona, Blaine Nelson, and et al. 2014. Security evaluation of support vector machines in

adversarial environments. In Support Vector Machines Applications. Springer, Cham, 105–153.

[18] Battista Biggio, Giorgio Fumera, and Fabio Roli. 2010. Multiple classifier systems for robust classifier design in

adversarial environments. Int. J. Mach. Learn. Cybern. 1, 1-4 (2010), 27–41.

[19] B. Biggio, G. Fumera, and F. Roli. 2014. Security Evaluation of Pattern Classifiers under Attack. IEEE Transactions on

Knowledge and Data Engineering 26, 4 (April 2014), 984–996. https://doi.org/10.1109/TKDE.2013.57

[20] Battista Biggio, Blaine Nelson, and Pavel Laskov. 2012. Poisoning Attacks against Support Vector Machines. In
Proceedings of the 29th International Conference on Machine Learning. icml.cc / Omnipress, Edinburgh, Scotland, UK,
105–153.

[21] Leyla Bilge and Tudor Dumitras. 2012. Before we knew it: an empirical study of zero-day attacks in the real world. In
Proceedings of the 2012 ACM conference on Computer and communications security. ACM, Raleigh, NC, USA, 833–844.

[22] Leo Breiman. 2001. Random Forests. Machine Learning 45 (2001), 5–32.
[23] Nicholas Carlini and David Wagner. 2017. Adversarial examples are not easily detected: Bypassing ten detection
methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, Dallas, TX, USA, 3–14.
[24] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of neural networks. In Security and

Privacy (SP), 2017 IEEE Symposium on. IEEE, San Jose, CA, USA, 39–57.

[25] Curtis Carmony, Xunchao Hu, and et al. 2016. Extract Me If You Can: Abusing PDF Parsers in Malware Detectors. In
23rd Annual Network and Distributed System Security Symposium. The Internet Society, San Diego, California, USA.
[26] Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopadhyay. 2018.

Adversarial Attacks and Defences: A Survey. CoRR abs/1810.00069 (2018). arXiv:1810.00069

[27] Lingwei Chen, Shifu Hou, and Yanfang Ye. 2017. SecureDroid: Enhancing Security of Machine Learning-based

Detection Against Adversarial Android Malware Attacks. In ACSAC. ACM, Orlando, FL, USA, 362–372.

[28] Lingwei Chen, Shifu Hou, Yanfang Ye, and Shouhuai Xu. 2018. DroidEye: Fortifying Security of Learning-Based
Classifier Against Adversarial Android Malware Attacks. In IEEE/ACM 2018 International Conference on Advances in
Social Networks Analysis and Mining, ASONAM. IEEE Computer Society, Barcelona, Spain, 782–789.

[29] Lingwei Chen, Yanfang Ye, and Thirimachos Bourlai. 2017. Adversarial Machine Learning in Malware Detection:
Arms Race between Evasion Attack and Defense. In European Intelligence and Security Informatics Conference, EISIC.
IEEE Computer Society, Athens, Greece, 99–106.

[30] Sen Chen, Minhui Xue, Lingling Fan, and et al. 2018. Automated poisoning attacks and defenses in malware detection

systems: An adversarial machine learning approach. computers & security 73 (2018), 326–344.

[31] X. Chen, C. Li, D. Wang, and et al. 2020. Android HIV: A Study of Repackaging Malware for Evading Machine-Learning

Detection. IEEE Transactions on Information Forensics and Security 15 (2020), 987–1001.

[32] Yizheng Chen, Shiqi Wang, Dongdong She, and Suman Jana. 2020. On Training Robust PDF Malware Classifiers. In

29th USENIX Security Symposium. USENIX Association, Virtual Conference, 2343–2360.

[33] Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine
Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014,

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:31

October 25-29, 2014. ACL, Doha, Qatar, 1724–1734.

[34] CISCO. 2018. CISIO reporter. Avaiabled at https://www.cisco.com (2018/12/02). (2018).
[35] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural
language processing (almost) from scratch. Journal of machine learning research 12, Aug (2011), 2493–2537.

[36] Hung Dang, Yue Huang, and Ee-Chien Chang. 2017. Evading classifiers by morphing in the dark. In CCS. ACM,

Dallas, TX, USA, 119–133.

[37] Luca Demetrio, Battista Biggio, and et al. 2019. Explaining Vulnerabilities of Deep Learning to Adversarial Malware
Binaries. In Proceedings of the Third Italian Conference on Cyber Security, Vol. 2315. CEUR-WS.org, Pisa, Italy.
[38] Luca Demetrio, Scott E. Coull, Battista Biggio, and et al. 2020. Adversarial EXEmples: A Survey and Experimental
Evaluation of Practical Attacks on Machine Learning for Windows Malware Detection. CoRR abs/2008.07125 (2020).
[39] Ambra Demontis, Marco Melis, Battista Biggio, and et al. 2017. Yes, machine learning can be more secure! a case

study on android malware detection. IEEE Trans. Dependable Secur. Comput. 16 (2017), 711–724.

[40] Ambra Demontis, Marco Melis, and et al. 2019. Why Do Adversarial Attacks Transfer? Explaining Transferability of
Evasion and Poisoning Attacks. In 28th USENIX Security Symposium. USENIX Association, Santa Clara, USA, 321–338.

[41] Dexguard. 2018. Dexguard @ONLINE. (December 2018). https://www.guardsquare.com/en/products/dexguard
[42] Hermann Dornhackl, Konstantin Kadletz, Robert Luh, and Paul Tavolato. 2014. Malicious behavior patterns. In
Service Oriented System Engineering (SOSE), 2014 IEEE 8th International Symposium on. IEEE, Oxford, United Kingdom,
384–389.

[43] Pang Du, Zheyuan Sun, Huashan Chen, Jin-Hee Cho, and Shouhuai Xu. 2018. Statistical Estimation of Malware
Detection Metrics in the Absence of Ground Truth. IEEE Trans. Information Foren. and Sec. 13, 12 (2018), 2965–2980.
[44] Manuel Egele, Theodoor Scholte, Engin Kirda, and Christopher Kruegel. 2012. A survey on automated dynamic

malware-analysis techniques and tools. ACM computing surveys (CSUR) 44, 2 (2012), 6.

[45] Yujie Fan, Shifu Hou, Yiming Zhang, Yanfang Ye, and Melih Abdulhayoglu. 2018. Gotcha - Sly Malware!: Scorpion A
Metagraph2vec Based Malware Detection System. In Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, KDD. ACM, London, UK, 253–262.

[46] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. 2016. Robustness of classifiers: from
adversarial to random noise. In Advances in Neural Information Processing Systems. Curran Associates, Inc., Barcelona,
Spain, 1632–1640.

[47] M. Garnaeva, F. Sinitsyn, Y. Namestnikov, and et al. 2016. Kaspersky Security Bulletin Overall Statistics for 2016.

https://media.kasperskycontenthub.com. (2016).

[48] Justin Gilmer, Luke Metz, and et al. 2018. Adversarial Spheres. In 6th International Conference on Learning Representa-

tions. OpenReview.net, Vancouver, BC, Canada.

[49] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. Vol. 1. MIT press, Cambridge, MA.
[50] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. 2014. Generative Adversarial Nets. In Advances in neural information processing systems. Curran
Associates, Inc., Montreal, Quebec, Canada, 2672–2680.

[51] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples.

In 3rd International Conference on Learning Representations. OpenReview.net, San Diego, CA, USA.

[52] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, and et al. 2017. On the (Statistical) Detection of Adversarial

Examples. CoRR abs/1702.06280 (2017). arXiv:1702.06280 http://arxiv.org/abs/1702.06280

[53] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. 2017. Adversarial
examples for malware detection. In European Symposium on Research in Computer Security. Springer, Oslo, Norway,
62–79.

[54] Maya Gupta, Andrew Cotter, and et al. 2016. Monotonic calibrated interpolated look-up tables. The Journal of Machine

Learning Research 17, 1 (2016), 3790–3836.

[55] William Hardy, Lingwei Chen, Shifu Hou, Yanfang Ye, and Xin Li. 2016. DL4MD: A deep learning framework for
intelligent malware detection. In Proceedings of the International Conference on Data Mining. Las Vegas, USA, 61.
[56] Shifu Hou, Yanfang Ye, and et al. 2017. Hindroid: An intelligent android malware detection system based on structured
heterogeneous information network. In Proceedings of the 23rd ACM SIGKDD. ACM, Halifax, NS, Canada, 1507–1515.
[57] Shifu Hou, Yanfang Ye, Yangqiu Song, and Melih Abdulhayoglu. 2018. Make Evasion Harder: An Intelligent Android
Malware Detection System. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,
IJCAI. ijcai.org, Stockholm, Sweden, 5279–5283.

[58] Weiwei Hu and Ying Tan. 2017. Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN.

CoRR abs/1702.05983 (2017). http://arxiv.org/abs/1702.05983

[59] Weiwei Hu and Ying Tan. 2018. Black-Box Attacks against RNN Based Malware Detection Algorithms. In The
Workshops of The Thirty-Second AAAI Conference on Artificial Intelligence. AAAI Press, New Orleans, USA, 245–251.

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:32

D. Li, Q. Li, F. Ye, and S. Xu

[60] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and JD Tygar. 2011. Adversarial machine
learning. In Proceedings of the 4th ACM workshop on Security and artificial intelligence. ACM, Chicago, IL, USA, 43–58.
[61] Inigo Incer, Michael Theodorides, Sadia Afroz, and David A. Wagner. 2018. Adversarially robust malware detection
using monotonic classification. In Proceedings of the Fourth ACM International Workshop on Security and Privacy
Analytics. ACM, Tempe, AZ, USA, 54–63.

[62] Alexander Jordan, François Gauthier, Behnaz Hassanshahi, and David Zhao. 2018. SAFE-PDF: Robust Detection of

JavaScript PDF Malware Using Abstract Interpretation. arXiv preprint arXiv:1810.12490 (2018).

[63] Jinho Jung, Chanil Jeon, Max Wolotsky, Insu Yun, and Taesoo Kim. 2017. AVPASS: Leaking and Bypassing Antivirus

Detection Model Automatically. https://www.blackhat.com/. (2017). Online; accessed July 2017.

[64] Kris Kendall and Chad McMillan. 2007. Practical malware analysis. Black Hat Conference, USA. (2007). Online;

access at December 2019.

[65] Khaled N Khasawneh, Nael Abu-Ghazaleh, and et al. 2017. RHMD: evasion-resilient hardware malware detectors. In
Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture. ACM, Cambridge, MA, USA,
315–327.

[66] Jin-Young Kim, Seok-Jun Bu, and Sung-Bae Cho. 2018. Zero-day malware detection using transferred generative

adversarial networks based on deep autoencoders. Information Sciences 460-461 (2018), 83 – 102.

[67] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing, EMNLP, Alessandro Moschitti, Bo Pang, and Walter Daelemans
(Eds.). ACL, Doha, Qatar, 1746–1751.

[68] Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto, Claudia Eckert, and Fabio Roli.
2018. Adversarial Malware Binaries: Evading Deep Learning for Malware Detection in Executables. In 26th European
Signal Processing Conference, EUSIPCO. IEEE, Roma, Italy, 533–537.

[69] Bojan Kolosnjaji, Ghadir Eraisha, George Webster, and et al. 2017. Empowering convolutional networks for malware
classification and analysis. In Neural Networks (IJCNN), 2017 International Joint Conference on. IEEE, Anchorage, AK,
USA, 3838–3845.

[70] Felix Kreuk, Assi Barak, Shir Aviv-Reuven, and et al. 2018. Adversarial Examples on Discrete Sequences for Beating

Whole-Binary Malware Detection. CoRR abs/1802.04528 (2018). arXiv:1802.04528 http://arxiv.org/abs/1802.04528

[71] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017. Adversarial Machine Learning at Scale. In 5th ICLR.

OpenReview.net, Toulon, Franc.

[72] Kaspersky Lab. 2018. Kaspersky @ONLINE. (May 2018). https://www.kaspersky.com/
[73] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature 521, 7553 (2015), 436.
[74] Qi Lei, Lingfei Wu, and et al. 2019. Discrete Adversarial Attacks and Submodular Optimization with Applications to

Text Classification. In Proceedings of Machine Learning and Systems 2019. mlsys.org, Stanford, CA, USA.

[75] Bo Li, Yevgeniy Vorobeychik, and Xinyun Chen. 2016. A General Retraining Framework for Scalable Adversarial

Classification. CoRR abs/1604.02606 (2016). arXiv:1604.02606 http://arxiv.org/abs/1604.02606

[76] Deqiang Li and Qianmu Li. 2020. Adversarial Deep Ensemble: Evasion Attacks and Defenses for Malware Detection.

IEEE Trans. Inf. Forensics Secur. 15 (2020), 3886–3900.

[77] Deqiang Li, Qianmu Li, Yanfang Ye, and Shouhuai Xu. 2019. Enhancing Robustness of Deep Neural Networks against
Adversarial Malware Samples: Principles, Framework, and Application to AICS’2019 Challenge. In The AAAI-19
Workshop on Artificial Intelligence for Cyber Security (AICS). Hawaii, USA. http://arxiv.org/abs/1812.08108

[78] Deqiang Li, Qianmu Li, Yanfang Ye, and Shouhuai Xu. 2021. A Framework for Enhancing Deep Neural Networks

Against Adversarial Malware. IEEE Trans. Netw. Sci. Eng. 8, 1 (2021), 736–750.

[79] Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, and Victor CM Leung. 2018. A survey on security threats and

defensive techniques of machine learning: a data driven view. IEEE access 6 (2018), 12103–12117.

[80] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep

Learning Models Resistant to Adversarial Attacks. In 6th ICLR. OpenReview.net, Vancouver, BC, Canada.

[81] Davide Maiorca, Battista Biggio, and Giorgio Giacinto. 2019. Towards Adversarial Malware Detection: Lessons Learned
from PDF-based Attacks. ACM Comput. Surv. 52, 4, Article 78 (Aug. 2019), 36 pages. https://doi.org/10.1145/3332184
[82] Davide Maiorca, Igino Corona, and Giorgio Giacinto. 2013. Looking at the Bag is Not Enough to Find the Bomb: An
Evasion of Structural Methods for Malicious PDF Files Detection. In Proceedings of the 8th ASIA CCS. ACM, New
York, NY, USA, 119–130.

[83] Davide Maiorca, Ambra Demontis, and et al. 2020. Adversarial Detection of Flash Malware: Limitations and Open

Issues. Comput. Secur. 96 (2020), 101901.

[84] Enrico Mariconti, Lucky Onwuzurike, and et al. 2017. MaMaDroid: Detecting Android Malware by Building Markov
Chains of Behavioral Models. In 24th Annual Network and Distributed System Security Symposium NDSS. The Internet
Society, San Diego, California, USA.

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:33

[85] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and et al. 2018. Robustness of Classifiers to Universal Perturbations:

A Geometric Perspective. In 6th ICLR. OpenReview.net, Vancouver, BC, Canada.

[86] Andreas Moser, Christopher Kruegel, and Engin Kirda. 2007. Limits of static analysis for malware detection. In
Computer security applications conference, 2007. ACSAC 2007. Twenty-third annual. IEEE, Miami Beach, Florida, USA,
421–430.

[87] Luis Muñoz-González, Battista Biggio, and et al. 2017. Towards poisoning of deep learning algorithms with back-
gradient optimization. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, Dallas,
TX, USA, 27–38.

[88] Annamalai Narayanan, Liu Yang, Lihui Chen, and Liu Jinliang. 2016. Adaptive and scalable android malware detection
through online learning. In Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, Vancouver, BC,
Canada, 2484–2491.

[89] Andrew Y. Ng. 2004. Feature Selection, L1 vs. L2 Regularization, and Rotational Invariance. In Proceedings of the
Twenty-First International Conference on Machine Learning. Association for Computing Machinery, New York, NY,
USA, 78.

[90] Alexandru Niculescu-Mizil and Rich Caruana. 2005. Predicting good probabilities with supervised learning. In

Proceedings of the Twenty-Second ICML, Vol. 119. ACM, Bonn, Germany, 625–632.

[91] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, and et al. 2017. Practical Black-Box Attacks against Machine
Learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. Association
for Computing Machinery, New York, NY, USA, 506–519.

[92] Nicolas Papernot, Patrick McDaniel, Somesh Jha, and et al. 2016. The limitations of deep learning in adversarial

settings. In EuroS&P, 2016 IEEE European Symposium on. IEEE, Germany, March, 372–387.

[93] Nicolas Papernot, Patrick D. McDaniel, and et al. 2016. Distillation as a Defense to Adversarial Perturbations Against
Deep Neural Networks. In IEEE Symposium on Security and Privacy. IEEE Computer Society, San Jose, USA, 582–597.
[94] Nicolas Papernot, Patrick D. McDaniel, Arunesh Sinha, and Michael P. Wellman. 2018. SoK: Security and Privacy
in Machine Learning. In 2018 IEEE European Symposium on Security and Privacy, EuroS&P. IEEE, London, United
Kingdom, 399–414.

[95] Andrea Paudice, Luis Muñoz-González, and Emil C. Lupu. 2018. Label Sanitization Against Label Flipping Poisoning

Attacks. In ECML PKDD 2018 Workshops, Proceedings, Vol. 11329. Springer, Dublin, Ireland, 5–15.

[96] Hanchuan Peng, Fuhui Long, and Chris Ding. 2005. Feature selection based on mutual information criteria of
max-dependency, max-relevance, and min-redundancy. IEEE Transactions on PAMI 27, 8 (2005), 1226–1238.
[97] F. Pierazzi, F. Pendlebury, J. Cortellazzi, and L. Cavallaro. 2020. Intriguing Properties of Adversarial ML Attacks in
the Problem Space. In IEEE Symposium on Security and Privacy. IEEE Computer Society, San Francisco, CA, USA,
1308–1325.

[98] Niels Provos and Thorsten Holz. 2007. Virtual honeypots: from botnet tracking to intrusion detection. Pearson Education,

London, England.

[99] Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro, and Charles K Nicholas. 2018. Malware
detection by eating a whole exe. In Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence. AAAI
Press, New Orleans, Louisiana, USA.

[100] Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici. 2018. Generic Black-Box End-to-End Attack Against
State of the Art API Call Based Malware Classifiers. In Research in Attacks, Intrusions, and Defenses, Michael Bailey,
Thorsten Holz, Manolis Stamatogiannakis, and Sotiris Ioannidis (Eds.). Springer International Publishing, Cham,
490–510.

[101] Christian Rossow, Christian J. Dietrich, and et al. 2012. Prudent Practices for Designing Malware Experiments: Status
Quo and Outlook. In IEEE Symposium on Security and Privacy. IEEE Computer Society, San Francisco, California,
USA, 65–79.

[102] Paolo Russu, Ambra Demontis, Battista Biggio, Giorgio Fumera, and Fabio Roli. 2016. Secure kernel machines against
evasion attacks. In Proceedings of the 2016 ACM workshop on artificial intelligence and security. ACM, Vienna, Austria,
59–69.

[103] Joshua Saxe and Konstantin Berlin. 2015. Deep neural network based malware detection using two dimensional
binary program features. In MALWARE, 2015 10th International Conference on. IEEE, Fajardo, PR, USA, 11–20.
[104] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. 2018. Adversarially
robust generalization requires more data. In Advances in Neural Information Processing Systems. Curran Associates
Inc., Montréal, Canada, 5019–5031.

[105] Alexandru Constantin Serban and Erik Poll. 2018. Adversarial Examples - A Complete Characterisation of the

Phenomenon. CoRR abs/1810.01185 (2018). http://arxiv.org/abs/1810.01185

[106] Syed Zainudeen Mohd Shaid and Mohd Aizaini Maarof. 2015. In memory detection of Windows API call hooking

technique. In (I4CT), 2015 International Conference on. IEEE, Kuching, Malaysia, 294–298.

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

:34

D. Li, Q. Li, F. Ye, and S. Xu

[107] Shai Shalev-Shwartz and Shai Ben-David. 2014. Understanding machine learning: From theory to algorithms. Cambridge

university press, New York, USA.

[108] Hispasec Sistemas. 2019. VirusTotal. Avaiabled at https://www.virustotal.com (2019/05/02). (2019).
[109] Charles Smutz and Angelos Stavrou. 2012. Malicious PDF Detection Using Metadata and Structural Features. In
Proceedings of the 28th Annual Computer Security Applications Conference. ACM, New York, NY, USA, 239–248.
[110] Charles Smutz and Angelos Stavrou. 2016. When a Tree Falls: Using Diversity in Ensemble Classifiers to Identify

Evasion in Malware Detectors. In 23rd NDSS. The Internet Society, San Diego, California, USA.

[111] Dawn Song, David Brumley, Heng Yin, and et al. 2008. BitBlaze: A new approach to computer security via binary

analysis. In International Conference on Information Systems Security. Springer, Hyderabad, India, 1–25.

[112] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a
simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research 15, 1 (2014),
1929–1958.

[113] Nedim Šrndić and Pavel Laskov. 2013. Detection of malicious pdf files based on hierarchical document structure. In
Proceedings of the 20th Annual Network & Distributed System Security Symposium, NDSS. The Internet Society, San
Diego, California, USA, 1–16.

[114] N. Šrndić and P. Laskov. 2014. Practical Evasion of a Learning-Based Classifier: A Case Study. In 2014 IEEE Symposium

on Security and Privacy. IEEE Computer Society, Berkeley, CA, USA, 197–211.

[115] Nedim Šrndić and Pavel Laskov. 2016. Hidost: a static machine-learning-based detector of malicious files. EURASIP

Journal on Information Security 2016, 1 (2016), 22.

[116] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. 2017. Certified defenses for data poisoning attacks. In Advances

in Neural Information Processing Systems. Curran Associates, Inc., Long Beach, CA, USA, 3517–3529.

[117] O. Suciu, S. E. Coull, and J. Johns. 2019. Exploring Adversarial Examples in Malware Detection. In 2019 IEEE Security

and Privacy Workshops (SPW). IEEE, San Francisco, CA, USA, 8–14.

[118] Octavian Suciu, Radu Marginean, and et al. 2018. When does machine learning {FAIL}? generalized transferability for
evasion and poisoning attacks. In 27th {USENIX} Security Symposium ({USENIX} Security 18). USENIX Association,
Baltimore, MD, USA, 1299–1316.

[119] Johan AK Suykens and Joos Vandewalle. 1999. Least squares support vector machine classifiers. Neural processing

letters 9, 3 (1999), 293–300.

[120] Christian Szegedy, Wojciech Zaremba, and et al. 2014. Intriguing properties of neural networks. In 2nd International

Conference on Learning Representations. OpenReview.net, Banff, AB, Canada.

[121] Acar Tamersoy, Kevin Roundy, and Duen Horng Chau. 2014. Guilt by association: large scale malware detection by
mining file-relation graphs. In Proceedings of the 20th ACM SIGKDD. ACM, New York, NY, USA, 1524–1533.
[122] Liang Tong, Bo Li, and et al. 2019. Improving Robustness of ML Classifiers against Realizable Evasion Attacks Using
Conserved Features. In 28th USENIX Security Symposium. USENIX Association, Santa Clara, USA, 285–302.
[123] Florian Tramèr and Dan Boneh. 2019. Adversarial Training and Robustness for Multiple Perturbations. In Advances in
Neural Information Processing Systems 32: NeurIPS 2019. Curran Associates, Inc., Vancouver, BC, Canada, 5858–5868.
[124] Vladimir Vapnik. 1991. Principles of Risk Minimization for Learning Theory. In Advances in Neural Information

Processing Systems 4. Morgan Kaufmann, Denver, Colorado, USA, 831–838.

[125] R Vinayakumar and KP Soman. 2018. DeepMalNet: Evaluating shallow and deep networks for static PE malware

detection. ICT Express (2018), 255–258.

[126] Beilun Wang, Ji Gao, and Yanjun Qi. 2017. A Theoretical Framework for Robustness of (Deep) Classifiers against

Adversarial Samples. In 5th ICLR. OpenReview.net, Toulon, France.

[127] Qinglong Wang, Wenbo Guo, Kaixuan Zhang, and et al. 2017. Adversary Resistant Deep Neural Networks with an

Application to Malware Detection. In Proceedings of the 23rd KDD. ACM, Halifax, NS, Canada, 1145–1153.

[128] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018. Formal Security Analysis of
Neural Networks using Symbolic Intervals. In 27th USENIX Security Symposium, USENIX Security, William Enck and
Adrienne Porter Felt (Eds.). USENIX Association, Baltimore, MD, USA, 1599–1614.

[129] W. Wang, X. Wang, and et a. 2014. Exploring Permission-Induced Risk in Android Applications for Malicious
Application Detection. IEEE Transactions on Information Forensics and Security 9, 11 (Nov 2014), 1869–1882.
[130] Eric Wong and J. Zico Kolter. 2018. Provable Defenses against Adversarial Examples via the Convex Outer Adversarial
Polytope. In Proceedings of the 35th ICML, Vol. 80. PMLR, Stockholmsmässan, Stockholm, Sweden, 5283–5292.
[131] AICS workshop. 2018. AICS 2019 Workshop Challenge Problem @ONLINE. (December 2018). http://www-personal.

umich.edu/~arunesh/AICS2019/challenge.html

[132] Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. 2015. Is feature selection
secure against training data poisoning?. In International Conference on Machine Learning. JMLR.org, Lille, France,
1689–1698.

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

Arms Race in Adversarial Malware Detection: A Survey

:35

[133] Li Xu, Zhenxin Zhan, Shouhuai Xu, and Keying Ye. 2014. An evasion and counter-evasion study in malicious websites

detection. In CNS, 2014 IEEE Conference on. IEEE, San Francisco, CA, USA, 265–273.

[134] Weilin Xu, Yanjun Qi, and David Evans. 2016. Automatically Evading Classifiers: A Case Study on PDF Malware
Classifiers. In 23rd Annual Network and Distributed System Security Symposium, NDSS. The Internet Society, San
Diego, California, USA.

[135] Peng Yang and Peilin Zhao. 2015. A min-max optimization framework for online graph classification. In Proceedings
of the 24th ACM International on Conference on Information and Knowledge Management. ACM, Melbourne, VIC,
Australia, 643–652.

[136] Wei Yang, Deguang Kong, Tao Xie, and Carl A Gunter. 2017. Malware detection in adversarial settings: Exploiting
feature evolutions and confusions in android apps. In Proceedings of the 33rd ACSAC. ACM, Orlando, FL, USA, 288–302.
[137] Yanfang Ye, Lifei Chen, Dingding Wang, Tao Li, Qingshan Jiang, and Min Zhao. 2009. SBMDS: an interpretable string

based malware detection system using SVM ensemble with bagging. Journal in computer virology 5, 4 (2009), 283.

[138] Yanfang Ye, Tao Li, Donald A. Adjeroh, and S. Sitharama Iyengar. 2017. A Survey on Malware Detection Using Data

Mining Techniques. ACM Comput. Surv. 50, 3 (2017), 41:1–41:40.

[139] I. You and K. Yim. 2010. Malware Obfuscation Techniques: A Brief Survey. In 2010 International Conference on
Broadband, Wireless Computing, Communication and Applications. IEEE Computer Society, Fukuoka, Japan, 297–300.
[140] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. 2019. Adversarial examples: Attacks and defenses for deep learning.

IEEE Trans. Neural Networks Learn. Syst. 30, 9 (2019), 2805–2824.

[141] Fei Zhang, Patrick PK Chan, Battista Biggio, Daniel S Yeung, and Fabio Roli. 2016. Adversarial feature selection

against evasion attacks. IEEE transactions on cybernetics 46, 3 (2016), 766–777.

[142] Zhi-Hua Zhou. 2012. Ensemble methods: foundations and algorithms. Chapman and Hall/CRC, Boca Raton, FL, USA.
[143] Ziyun Zhu and Tudor Dumitras. 2016. FeatureSmith: Automatically Engineering Features for Malware Detection by
Mining the Security Literature. In Proceedings of the 2016 ACM SIGSAC Conference on CCS. ACM, Vienna, Austria,
767–778.

ACM Computing Surveys, Vol. 1, No. 1, Article . Publication date: September 2021.

