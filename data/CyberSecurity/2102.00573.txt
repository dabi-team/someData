A Secure Learning Control Strategy via Dynamic Camouﬂaging for
Unknown Dynamical Systems under Attacks

Sayak Mukherjee1, Veronica Adetola1

1

1
2
0
2

b
e
F
1

]

Y
S
.
s
s
e
e
[

1
v
3
7
5
0
0
.
2
0
1
2
:
v
i
X
r
a

Abstract—This paper presents a secure reinforcement learning
(RL) based control method for unknown linear time-invariant
cyber-physical systems (CPSs) that are subjected to compositional
attacks such as eavesdropping and covert attack. We consider
the attack scenario where the attacker learns about the dynamic
model during the exploration phase of the learning conducted
by the designer to learn a linear quadratic regulator (LQR),
and thereafter, use such information to conduct a covert attack
on the dynamic system, which we refer to as doubly learning-
based control and attack (DLCA) framework. We propose a dy-
namic camouﬂaging based attack-resilient reinforcement learning
(ARRL) algorithm which can learn the desired optimal controller
for the dynamic system, and at the same time, can inject sufﬁcient
misinformation in the estimation of system dynamics by the
attacker. The algorithm is accompanied by theoretical guarantees
and extensive numerical experiments on a consensus multi-agent
system and on a benchmark power grid model.

Index Terms—Cyber physical systems, CPS security, reinforce-

ment learning, covert attacks, attack-resilient learning control.

I. INTRODUCTION

Security of Cyber-Physical Systems (CPSs) is becoming
one of the fundamental requirements to safeguard various
infrastructure and control systems against malicious attacks
that can lead to catastrophic failures if left unattended. Refer-
ences such as [1], [2] present overview of various theoretical
and computational aspects of attack detection, prevention and
resilient control designs. Extensive research work on the
detection and identiﬁcation of the attacks can be found in
references [3]–[5]. Various types of different attack scenarios
are considered in the literature; [6] categorizes these scenarios
based on the CPS model knowledge, disclosure of resources
and disruption of resources. More speciﬁcally, recent works
have investigated attacks such as denial-of-service attacks
[7], false data-injection attacks [8], replay attacks [9], covert
attacks [10], [11], etc. Different types of attacks lead to several
prevention and mitigation techniques involving secure state
estimation techniques [12], watermarking certain pre-speciﬁed
signals in the CPS loop [13], [14], moving target defense and
its variants [15]–[17], to name a few. Most of the literature on
CPS security focuses on the setting where the designer knows
the dynamic model of the system, and the attacker possesses
the knowledge about the dynamics with varying degree of
availability. However, with the ever increasing complexity and
dimensionality of the dynamic systems, the designer may not
have the explicit model information, and may need to learn the
dynamics or the feedback controller from the state, control and
output trajectories. In this paper, we propose a new formulation
and mitigation technique of a compositional attack performed

1 S. Mukherjee and V. Adetola are with the Optimization and Control
Group, Paciﬁc Northwest National Laboratory (PNNL), Richland, WA, USA.
Emails: (sayak.mukherjee, veronica.adetola)@pnnl.gov.

in a setting where the designer is tasked to learn an optimal
controller in a data-driven way. We have considered both the
eavesdropping and the covert attack, where the attacker can
manipulate both the controls and measurements to remain
undetected and at the same time harm the CPS by injecting
malicious inputs, to be performed in a sequential way.

Recently feedback control research for partially or fully
unknown dynamic systems has seen a tremendous growth
with the advancement on data-driven learning techniques
such as reinforcement learning (RL) [18]. In recent years,
several papers such as [19]–[23] have used RL for linear
optimal control using a variety of solution techniques such
as adaptive dynamic programming (ADP), Q-learning, actor-
critic methods, model reduction based RL, etc. More variants
of data-driven control research such as data-dependent linear
matrix inequalities [24], various distributed control designs
[25], [26], analyzing sample complexity [27], to name a few,
have been reported. We, in this paper, consider the linear
quadratic regulator problem using ADP/RL as considered
in [20], and then investigate the scenarios with malicious
attacks. In [28], learning-based secure control framework in
the presence of sensor and actuator attacks are discussed. This
work ﬁrst uses the model to perform the detection task. [29]
considers learning based attacks. We have considered a very
generic attack scenario where initially the attacker does not
possess any system dynamic information and therefore the
attacker ﬁrst eavesdrops, and after gathering sufﬁcient dynamic
information conducts a covert attack. As we consider the
problem of learning the optimal control in a secured way from
the perspective of the designer, we refer to our framework as
a doubly learning-based control and attack (DLCA) scenario.
We propose an attack-resilient reinforcement learning (ARRL)
algorithm which can learn the desired optimal controller for
the dynamic system, and at the same time, can inject sufﬁcient
misinformation in the system’s dynamics estimation to delude
the attacker.

The contributions of this paper are as follows:
• We propose a secure learning control framework, namely
DLCA, where the attacker tries to exploit the learning
methodology such as the system exploration to gather
important dynamic information and conduct malicious
covert attacks. Therefore, the vulnerability of the learning
based designs for CPS in the presence of attackers needs
to be addressed.

• Thereafter, we propose a retroﬁtted reinforcement learn-
ing design, namely, ARRL, where the designer tries to
misguide the attacker during the exploration phase of
the learning. We dynamically couple the CPS with a
nonlinear time-variant static map satisfying input-to-state
stability (ISS) conditions, which we termed as dynamic

 
 
 
 
 
 
camouﬂaging.

• The proposed method is accompanied with sufﬁcient
guarantees and numerical experiments conducted on a
consensus multi-agent system and on a benchmark power
grid model.

The rest of the paper is organized as follows. The secure
learning control problem is introduced in Section II. The
nominal ADP/RL based control design is discussed in Section
III. The attack model
is discussed in Section IV. Section
V proposes the attack resilient design and its advantages.
Numerical experiments are performed in Section VI, and
concluding remarks are provided in Section VII.

II. SECURE LEARNING CONTROL PROBLEM

We ﬁrst formalize the problem of performing secure learn-
ing control design for the CPS. We consider a linear time
invariant dynamic systems with the dynamics:

˙x = Ax + Bu, x(0) = x0,

(1)

where the x ∈ Rn denotes the states, and u ∈ Rm denotes
the control inputs. The designer is interested in computing
the optimal controller for this dynamic systems with unknown
state matrices. Therefore, we make the following assumptions
from the designer’s perspective:
Assumption 1: The model matrices A, and B are unknown.
Assumption 2: The pair (A, B) is stabilizable.
Assumption 3: The state and control measurements x(t), and
u(t) are available to the designer.

The designer is tasked with formulating a learner to solve

the following linear quadratic regulator (LQR) problem.
P. Under the assumptions 1, 2 and 3, learn the state feedback
controller u = −Kx such that the following objective is
minimized in closed loop:

minimize J(x0, u) =

(cid:90) ∞

0

(x(t)T Qx(t) + u(t)T Ru(t))dt,

(2)

where Q (cid:23) 0, R (cid:31) 0 denote the state and control penalty
weights.

times,

The standard model based solution to this problem is found
by solving the well-known algebraic Riccati equation (ARE).
In recent
the research works on adaptive dynamic
programming and reinforcement learning have looked into
formulating techniques to solve this problem without
the
knowledge of the model matrices, and only using state and
input trajectories (we will brieﬂy recapitulate such nominal
learning control design in the next section). However, stan-
dard learning control designs do not consider any adversarial
behavior from malicious entities. In this paper, we, on the
other hand, considered a scenario when the dynamic system
is under the inﬂuence of an attacker.

The attacker initially does not possess any dynamic infor-
mation about the system. Therefore, the attacker tries to extract
the dynamic information of the system during the learning, and
then conducts covert attacks, which will be discussed shortly.
Therefore, we can formalize the activities of the attacker in
two phases described as follows.

2

Attacker Act A1 (Eavesdropping): Attacker conducts the
ﬁrst phase of the attack during the exploration phase of the
learning control design. The attacker intends to learn about
the dynamic matrices A, B during this phase by gathering the
exploration input u(t), and the resultant state measurements
x(t). Although, the designer is conducting the exploration
of the dynamic system to learn the optimal control K, the
designer is unaware that
the attacker is also using such
trajectory information to learn about the system’s dynamics.
Attacker Act A2 (Covert Attack): In the next phase, the
adversary conducts a covert attack on the dynamical system.
In covert attacks, the attacker inject malicious signals at the
inputs, and then compensate the impact of the injected attack
in the measurements to conceal the attack, thereby, making
it covert to the system operator. The covert attacks are very
difﬁcult to identify, and its impact on the system can be
catastrophic. The attacker can also keep on injecting malicious
signals at the actuation such that the system continues to
operate inefﬁciently without being captured by the sensors.

Therefore, the designer now needs to propose modiﬁcations
to the learning control design. The attacker should not be able
to eavesdrop and accurately capture the dynamic information
that could allow a successful covert attack. At
the same
time, the designer should learn the optimal control solutions
corresponding to the dynamical system (1) and objective (2).
We enumerate these considerations as follows:

• Learner consideration 1: Learner needs to make the
exploration phase of the learning control secure. This
means, the measurements of u(t), and x(t) should not
be the accurate representation of the dynamical system
(1).

• Learner consideration 2: The attacker should be unable
to keep its attack on the system covert. Therefore, with-
out any external disturbance, when the dynamic system
operates under steady state condition, any malicious in-
jection will create considerable perturbations at the state
measurement channels which can be easily detected by
the operator.

• Learner consideration 3: Although, the learner camou-
ﬂage the exploration trajectory measurements, it is still
required to compute the optimal control u = −Kx cor-
responding to the original learning problem P. Therefore,
we do not compromise with the learning computation
accuracy in order to satisfy considerations 1 and 2,
which is really important from the perspective of the
implemented feedback control for the dynamic system.

We will next recapitulate the nominal adaptive dynamic pro-
gramming (ADP) based optimal control learning strategy that
can solve problem P without any malicious attack.

III. RECAPITULATION OF ADP-BASED NOMINAL
LEARNING CONTROL

The problem P with the unknown model matrices can be
solved using ADP/ RL based approaches. We use the off-
policy RL based gain computation framework for this study
as given in [20]. Here, the system is excited with exploration
signal u0, and, thereafter, the state measurements x(t) are

gathered for a sufﬁcient number of time samples described
shortly. The control input u0 should be such that the state tra-
jectory remains sufﬁciently bounded. Considering a quadratic
Lyapunov function xT P x, P (cid:31) 0, the time-derivative along
the state trajectories is computed, and subsequently using the
Kleinman’s algorithm [30] the following model-independent
trajectory relationship can be obtained for the interval [t, t+T ]:

(t+T )Pkx(t+T ) − xT
xT

t Pkxt − 2

= −

(cid:90) t+T

t

(xT ¯Qkx)dτ.

(cid:90) t+T

t

((Kkx + u0)T RKk+1x)dτ

(3)

where, ¯Qk = Q + K T
k RKk. We can solve (3) by constructing
a data-driven iteration framework that uses the time sampled
measurements of states and the controls. The learner gathers
the data matrices D = {Nxx, Mxx, Mxu0} where,

Nxx =

x ⊗ x|t1+T

,

· · ·

, x ⊗ x|tl+T

(cid:104)

t1
(cid:104)(cid:82) t1+T
t1
(cid:104)(cid:82) t1+T
t1

Mxx =

(x ⊗ x)dτ,

· · ·

Mxu0 =

(x ⊗ u0)dτ,

· · ·

(cid:105)T

,

tl
, (cid:82) tl+T
tl
, (cid:82) tl+T
tl

(x ⊗ x)dτ

(cid:105)T

,

(x ⊗ u0)dτ

(cid:105)T

.

(4)

(5)

(6)

Algorithm 1 summarizes the steps to compute the optimal

control solutions with unknown state dynamics.

Algorithm 1 Nominal Reinforcement Learning Control
1. Data gathering: Measure the state and controls (x(t) and u0) for interval
(t1, t2, · · · , tl), ti − ti−1 = T . Then construct D = {Nxx, Mxx, Mxu0 }
such that rank(Mxx Mxu0 ) = n(n + 1)/2 + nm.

2. Iteratively update the control : Starting with a stabilizing K0, update the
feedback control gain Kk iteratively (k = 0, 1, · · · ) by solving the least
square equation where vec(.) denotes standard vectorization operation -
for k = 0, 1, 2, ..
A. Solve for Pk, and Kk+1:

(cid:2)Nxx −2Mxx(In ⊗ KT

k R) − 2Mxu0 (In ⊗ R)(cid:3) ×
= −Mxxvec( ¯Qk).

(cid:21)

(cid:20) vec(Pk)
vec(Kk+1)

(7)

B. Break the iterations when ||Pk − Pk−1|| < ς, ς is a small positive
threshold.
endfor

3. Applying K on the system : Finally, apply u = −Kx, and remove u0.

Remark 1: With nominally stable system, i.e., when A is
the iterative update in (7) does not require any
Hurwitz,
initial stabilizing control. Otherwise, policy iteration based RL
techniques require a stabilizing K0 [31], mainly because of its
inception from the Newton-Kleinman updates [30].
Remark 2: In order to get unique convergent solutions of the
optimal gain, the data sample requirement is converted into the
following rank condition: rank(Mxx Mxu0 ) = n(n + 1)/2 +
nm. This is dependent on the number of unknown variables
in the least squares problem. The condition is also analogous
to the persistency of excitation condition of adaptive control
literature. Practically, one can use twice of the data samples
required by the rank condition for guaranteed convergence.
Theorem 1 [20]: When the rank condition of the Remark 2
is satisﬁed, the iterates of Pk, Kk from Algorithm 1 converge
(cid:3)
to optimal P , and K with k → ∞.

3

IV. ATTACKER MODELING

To this end, we consider an attacker which acts in two folds

as follows.
A. Attacker Act 1 - Eavesdropping:

The attacker becomes active during the exploration phase of
the learning algorithm. In this phase, the attacker eavesdrops
on the input and state channels u0(t), and x(t) to gather
the measurements for the time instants t1, ..., tl. The attacker
gathers as much as dynamic information as possible to perform
the identiﬁcation of A, B. The attacker can employ any of
the sub-space based identiﬁcation approaches. We assume
that
the attacker knows the dimensionality of the system.
The attacker constructs the surrogate model of the original
dynamical system as follows:

˙˜x = ˜A˜x + ˜Bu, ˜x(0) = ˜x0.
It can be expected that the model matrices ˜A, ˜B are identiﬁed
with high accuracy. Practically, the attacker can identify a
model which is similar to the original state space, i.e, they
are related by a similarity transformation T :

(8)

˜A = T AT −1, ˜B = T B.

(9)

In order to consider the worst-case scenario, we assume that
the attacker can successfully identify the actual state space
models. Therefore, ˜A = A, ˜B = B, i.e., T = I.
Remark 3: Why eavesdropping during exploration? The rea-
son behind such consideration is that the exploration is the
most vulnerable phase for the system to be compromised, cre-
ating a worst-case scenario. During exploration, the designer
tries to persistently excite the system such that the input-output
data is sufﬁciently rich in dynamic information. Therefore,
if the attacker can get access to such exploration data, the
attacker could easily perform the system identiﬁcation to get
accurate model estimates.

B. Attacker Act 2 - Covert Attack:

Once, the attacker has access to the system dynamic model
information, a covert attack using the manipulated input and
state channels is conducted. An attack on the control input
will result into:

˜u(t) = u(t) + ζ(t),

(10)

where ζ(t) is the malicious input signal. Thereafter,
the
attacker compensates the effect of the input attack to the state
measurement sensors of the system by manipulating:

¯x(t) = x(t) − ˜x(t),

(11)

where ¯x(t) is the measured states, and ˜x(t) is the compen-
sation signals added by the attacker. Therefore, the attacker
needs to generate ζ(t) and ˜x(t). We now state the following
Lemma which characterizes the covert nature of the attack.
Lemma 1: Consider the attack begins at t = Ta, if the attacker
uses (10), (11), and runs the dynamics:
˙˜x = A˜x + Bζ,

A :

(12)

with ˜x(Ta) = 0, then the attack will remain covert at the
measured states.

4

Proof: The solution of x(t) starting from t = Ta is
given as:

x(t) = eA(t−Ta)x(Ta) +

(cid:90) t

Ta

eA(t−τ )[B(u + ζ)]dτ,

(13)

and the output of the attacker’s internal model (12) is given
by,

˜x(t) = eA(t−Ta) ˜x(Ta) +

(cid:90) t

Ta

eA(t−τ )[Bζ]dτ.

(14)

Lemma 2: With bounded exploration inputs, and under as-
sumption 4, the system (16) will be input-to-state stable (ISS)
with respect to ψ.
Proof: For any generic dynamic system the exploration control
can be written in the form u = −Kss + u0 where (A − BKs)
is stable, and u0 is a bounded exploration such that the state
measurements remain within the stable neighborhood of the
operating point. Therefore, we could write,

˙x = (A − BKs)x + Bu0 + Bψ(t), x(t0) = x0.

(19)

The resulting state trajectories become

Therefore, the modiﬁed output seen by the designer following
from (11):

x(t) = e(A−BKs)(t−t0)x0 +

(cid:90) t

e(A−BK)(t−τ )Bu0dτ

(20)

¯x(t) = eA(t−Ta)(x(Ta) − ˜x(Ta)) +

(cid:90) t

Ta

eA(t−τ )[Bu]dτ (15)

Setting ˜x(Ta) = 0, it is evident that ¯x(t) can be treated as
legitimate state measurements, making the attack covert. (cid:3)

V. RETROFITTING THE LEARNING TO MAKE IT
ATTACK-RESILIENT

The designer has to make sure that the system identiﬁcation
by the attacker is incorrect, and at the same time, the desired
feedback control gain is being computed without sacriﬁcing
performance as enumerated in Section II. To this end, we,
hereby, propose a solution in this section. The main idea is
to modify the dynamic model in such a way that the attacker
cannot perfectly identify the system during the exploration
phase of the learning, referring to as dynamic camouﬂaging.
On the other hand, the designer has full knowledge about this
extraneous modiﬁcation such that the optimal gain K can be
correctly learned as in P.

As the designer starts the learning with the assumption
of unknown A, and B matrices, any system parameter or
actuator gains cannot be modiﬁed. Instead, we suggest to add
an input-to-state stable (ISS) coupling, dependent on the states
of the dynamic system, at an internal actuation location of the
system, which is assumed to be safe with respect to external
attacks. To check the applicability of such coupling, operators
can use the plant simulators, or historic measurements to
estimate whether the state trajectories will remain bounded.
The underlying dynamic system, therefore, is modiﬁed to:

˙x = Ax + B(u + ψ),

ψ = φ(t, x(t)).

(16)

(17)

Here ψ is assumed to be a nonlinear time varying static map,
however, one can also make this a dynamic map. We make the
following boundedness assumption on the coupling function
ψ = φ(t, x(t)).
Assumption 4: The coupling function ψ = φ(t, x(t)) satisﬁes

t0

e(A−BK)(t−τ )Bψ(τ )dτ

+

(cid:90) t

t0

As A − BKs is Hurwitz, we have, (cid:107)e(A−BKs)(t−t0)(cid:107) ≤
ke−λ(t−t0), k > 0, λ > 0. Thereafter, we can bound (cid:107)x(t)(cid:107)
as,

(cid:107)x(t)(cid:107) ≤ ke−λ(t−t0)(cid:107)x(t0)(cid:107)+
k(cid:107)B(cid:107)
λ

(supτ ∈[t0,t](cid:107)u0(τ )(cid:107) + supτ ∈[t0,t](cid:107)ψ(τ )(cid:107)).

(21)

Therefore, we can conclude that with bounded (cid:107)u0(t)(cid:107) , and
(cid:3)
assumption 4, the x(t) dynamics remains ISS.
This type of dynamics has been recently studied in our pa-
pers [32], [33] in the context of robustness of structured control
designs. Motivated from them, the formalism is found to be
suitable to make the nominal RL algorithm attack-resilient.
When the attacker eavesdrops on the input and state channels,
then u(t) and x(t) correspond to the underlying dynamics (16),
and, therefore, performing a system identiﬁcation will result
into erroneous state space representations, i.e.,

˜A (cid:54)= T AT −1, ˜B (cid:54)= T B.

(22)

In our numerical experiments we set φ(t, x(t)) = f (t)x(t)
with (cid:107)f (t)(cid:107)2 ≤ γ, f (t) (cid:54)= 0, ∀t, and to simulate the worst-
case, we assume that the identiﬁed state matrix is ˜A = A +
Bf (t), with f (t) frozen at time t = t0. We assume that the
attacker could perfectly estimate B, to create a worst-case
scenario.

To this end, the learning algorithm needs to be retroﬁtted
due the modiﬁcations as in (16). As we have intentionally
connected the coupling ψ, the designer has access to the full
measurements of ψ(t). At this stage recall the Kleinman’s
algorithm:
Theorem 2 [30]: Let K0 be such that A − BK0 is Hurwitz.
Then, for k = 0, 1, . . .
1. Solve for Pk(Policy Evaluation) :

AT

k Pk + PkAk + K T

k RKk + Q = 0, Ak = A − BKk. (23)

(cid:107)ψ(cid:107)2 = (cid:107)φ(t, x(t))(cid:107)2 ≤ γ(cid:107)x(t)(cid:107)2, γ > 0.

(18)

2. Update the control gain (Policy update):

Thereafter, we
interconnection during the exploration such that
trajectories remain bounded.

characterize

the

ISS stability of

the
the state

Kk+1 = R−1BT Pk.

(24)

Then A − BK is Hurwitz and Kk and Pk converge to optimal
(cid:3)
K, and P as k → ∞.

5

Fig. 1: Overview of the attack resilient learning methodology

The modiﬁed state dynamics (16) incorporating u = −Kkx is
given by

˙x = (A − BKk)x + B(Kkx + u) + Bψ.

(25)

We use similar exploration inputs u = u0 as before. As
considered in the derivation of Algorithm 1, we consider
similar Lyapunov function candidate xT Pkx, and compute its
derivative along the closed-loop trajectories and use Theorem
2 to replace the dependency on the model matrices as follows:

Algorithm 2 Attack-Resilient RL (ARRL) Control
1. Data gathering: Measure the state and controls and coupling variables
interval (t1, t2, · · · , tl), ti − ti−1 = T .
(x(t) u0(t) and ψ(t))
for
=
that
Then
rank(Mxx Mxu0 Mxψ) = n(n + 1)/2 + 2nm.

{Nxx, Mxx, Mxu0 , Mxψ}

construct D

such

2. Iteratively update the control : Starting with a stabilizing K0, update the
feedback control gain K iteratively (k = 0, 1, · · · ) by solving the least square
equation -
for k = 0, 1, 2, ..
A. Solve for Pk, and Kk+1:

d
dt

k Pk + PkAk)x+

(xT Pkx) = xT (AT
2(Kkx + u0)T BT Pkx + 2(ψT BT Pk)x
= −xT (Qk)x + 2(Kkx + u0)T RKk+1x+

2(ψT BT Pk)x,

(cid:2)Nxx −2Mxx(ID ⊗ KT
(cid:124)

(26)

×

k R) − 2Mxui0 (ID ⊗ R) −2Mxψ

(cid:123)(cid:122)
Θk
vec(Pk)
vec(K(k+1))
vec(BT Pk)






 = −Mxxvec(Qik)
(cid:125)

(cid:124)

(cid:123)(cid:122)
Φk

(29)

(cid:3)

(cid:125)

.

where, Qk = Q + K T
on the both sides, we have,

k RKk. Rearranging, and taking integrals

B. Break the iteartions when ||Pk − Pk−1|| < ς, ς is a small positive
threshold.
endfor

xT (t + T )Pkx(t + T ) − xT (t)Pkx(t)

(cid:90) t+T

− 2

((Kkx + u0)T RKk+1x)dτ

t
(cid:90) t+T

t

= −

(xT Qkx − 2(ψT BT Pk)x)dτ.

(27)

(27) is independent of model matrices, and constructed by
the trajectories of system states x(t), exploration control
u0(t), and the measurements of the coupling ψ(t). We can
use the properties of Kronecker product (denoted by ⊗) to
write xT Qkx = (xT ⊗ xT ) vec(Qk), ψT BT Pkx = (xT ⊗
ψT ) vec(BT Pk). The Attack-resilient RL algorithm can be
written by formulating an iterative version of (27), using
measurements of x(t), u0(t) and ψ(t) as given in Alg. 2. We
append the data matrix D as in Algorithm 1 with Mxψ where,

Mxψ =

(cid:104)(cid:82) t1+T
t1

(x ⊗ ψ)dτ,

· · ·

, (cid:82) tl+T
tl

(x ⊗ ψ)dτ

(cid:105)T

. (28)

Theorem 3: Performing Algorithm 2 with the modiﬁed system
(16) will able to recover the optimal control u = −Kx for the
actual system (1).
Proof: Performing Algorithm 2 (ARRL) using x(t), u(t), and
ψ(t) is equivalently solving the trajectory relationship (27). As

3. Applying K on the system : Finally, apply u = −Kx, and remove u0.

(27) has been constructed using Theorem 2, then any solution
from Theorem 2 will satisfy the kth iteration of the following
equation:

Θk





vec(Pk)
vec(K(k+1))
vec(BT Pk)



 = Φk.

(30)

Therefore, a solution from Theorem 2 should also sat-
isfy (30). With sufﬁcient gathering of data,
the condition
rank(Mxx Mxu0 Mxψ) = n(n + 1)/2 + 2nm is satisﬁed,
therefore, Θk will have full column rank. As such,
and,
equation (30) has a unique solution Pk, K(k+1). As this is an
unique solution, it is also the solution Pk, Kk+1 of theorem
2. Considering this equivalence of the Algorithm 2 with the
modiﬁed Kleinman update in Theorem 2, we can conclude that
the K, and P corresponding to Algorithm 1 can be recovered.
(cid:3)

The attacker then tries to launch covert attacks once the
closed-loop system is in the operating condition. However, the

where αij > 0 are the coupling coefﬁcients. We consider the
state and input matrix to be:

KAlg.2 =

measured states seen by the designer now is: ¯x(t) = x(t) −
˜x(t):

¯x(t) = eA(t−Ta)x(Ta) − e ˜A(t−Ta) ˜x(Ta)+
(cid:90) t

(eA(t−τ )[B(u + ζ)] − e ˜A(t−τ )[ ˜Bζ])dτ,

(31)

Ta

which is not a legitimate system response, even if ˜B = B,
and therefore will create undesired perturbations during the
normal operational mode of the plant depending on the energy
in attack inputs, and the error (cid:107)A − ˜A(cid:107). Therefore, a pre-
tuned set-point based detector can alert the system operator to
take further necessary actions. The designer can also design
sophisticated detectors using the nominal statistical properties
of the system, which we keep as future research.

VI. NUMERICAL SIMULATIONS

A. A Multi-agent System

We consider a multi-agent network with 6 agents taken from

[33] where each agent follows the consensus dynamics:

(cid:88)

˙xi =

j∈Ni,i(cid:54)=j

αij(xj − xi) + ui, xi(0) = xi0,

(32)

A =








−5
2
3
0
0
0

2
−6
0
0
1
3

3
0
−5
2
0
0

0
0
2
−2
0
0

0
1
0
0
−4
3








0
3
0
0
3
−6

, B = I6.

(33)

The dynamics follows a Laplacain structure with A.1n =
0 resulting into a zero eigenvalue with the rest of them
are −10.00, −8.27, −6.00, −3.00, −0.72, −0.00. We choose
initial conditions as [0.3, 0.5, 0.4, 0.8, 0.9, 0.6]T . The learned
controller is tasked to improve the damping of the slow
eigenvalues. We set Q = 10I6, R = I6. We ﬁrst experiment
with the nominal system without any retroﬁtting in the learning
design. As, n = 6, m = 6, the rank condition for the algorithm
1 requires 2 × (57) = 114 samples. The exploration has been
performed with 200 samples with 0.01 s time step. We excite
the system with the sum of sinusoids exploration. We assume
that this phase is not secured to attackers, and therefore, the
attacker could eavesdrop and could easily estimate the state
matrices. Once the data matrix D is constructed, the control
gains are computed via the iterations as given in Algorithm 1,
and then implemented to the system. Fig. 2 shows the actual
state trajectories during the initial 2 s exploration, and with
the control implementation. Fig. 3 shows the convergence of
the learning control using Algorithm 1 resulting into:

KAlg.1 =










2.3868
0.2731
0.4239
0.0342
0.0125
0.0318

0.2731
2.2564
0.0319
0.0010
0.1899
0.4100

0.0342
0.4239
0.0010
0.0319
0.3161
2.3884
0.3161
2.8112
0.0004 −0.0001
0.0017 −0.0001

0.0318
0.0125
0.4100
0.1899
0.0004
0.0017
−0.0001 −0.0001
0.4408
2.5188
2.2781
0.4408










,

which matches closely with the model-based solution. At
5 s,
the attacker starts injecting malicious signals to the
system, however, as the learning control was not secured,

(34)

6

the attacker could launch a covert attack, and therefore, the
state measurements could not able to capture any of these
malicious signals as shown in Fig. 4. However, the actual
state trajectories are heavily impacted as shown in Fig. 2. This
kind of covert attack can cause expensive state excursions, and
make the system less efﬁcient. The quadratic cost incurred
from 5 s to 10 s turns out to be 1.08 × 103 units.

Thereafter, we show the efﬁcacy of the retroﬁtted secured
learning design. During the exploration phase of the learn-
ing, we have added a functional coupling with the control
inputs in the form, u = ucontrol − f (t)x(t), f (t) = 0.3 ×
(sin(t) + cos(t) + 0.02). As the attacker does not aware of
such modiﬁcations, the system identiﬁcation performed by
the attacker using the input and state measurements during
the exploration will be erroneous. The modiﬁed algorithm
2, however,
is tasked with computing the optimal control
u = −Kx associated with the actual system dynamics A, B,
and not
that of the modiﬁed state dynamics. The control
computed with Algorithm 2 with the convergence shown in
Fig. 6 is given as:










2.3868
0.2731
0.4239
0.0342
0.0125
0.0318

0.2731
2.2564
0.0319
0.0010
0.1899
0.4100

0.0342
0.4239
0.0010
0.0319
0.3161
2.3884
2.8112
0.3161
0.0004 −0.0001
0.0017 −0.0001

0.0318
0.0125
0.4100
0.1899
0.0004
0.0017
−0.0001 −0.0001
0.4408
2.5188
2.2781
0.4408










,

(35)

which shows that the modiﬁed algorithm does not suffer in
the accuracy of computing the desired optimal control. As the
attacker is not able to capture the accurate state dynamics, the
attack does not remain covert anymore. To simulate a worst-
case identiﬁcation scenario, assume that the model identiﬁed
by the attacker uses ˜A = A − (cid:15)sc × 0.3 × 1.02 × I6 with
f (t = 0), and (cid:15)sc = 2 is a scaling factor. Thereafter, when
the attacker injects malicious attack signal at t = 5s, the
state trajectories at the measurement ports can capture such
behaviours as shown in Fig. 7. If the state trajectories hit a pre-
calibrated set point, then the system operator is being alerted
to remove the malicious enterprise.

B. A Power Grid Benchmark

We consider a 13-bus Kundur power system model as shown
in Fig. 8. To numerically simulate the model to generate data,
we model generator dynamics by the ﬂux decay model. The
state variables are denoted as δ(t), ω(t), E(t), Ef d(t) which
are the generator internal angle in radians, speed deviation
from nominal (2π × 60 radians/sec), internal voltage, and
excitation voltage in per unit, respectively. The supplementary
control signal is added with the voltage reference of automatic
voltage regulator (AVR) in the excitation dynamics. With a
100 MVA base, the rated active power generation capacity of
the generators G1-G4 are 7 p.u., 7 p.u., 7.16 p.u., and 7 p.u.,
respectively. The grid contains one inter-area mode (0.61 Hz)
and two intra-area modes (1.68, 1.56 Hz). The control design
objective is to improve the inter-area and oscillatory dynamic
performance of the grid. We have designed Q such that it
penalizes the relative difference between the generator angles
(inter-area power ﬂows depend on angular differences), and the
energy associated with other generator states. In this model,

7

Fig. 2: Actual states during exploration
(till 2 s), control implementation and
attack at 5 s for the nominal design

Fig. 3: Convergence of controller K in
Algorithm 1 for the nominal design

Fig. 4: Measured state trajectories during
exploration, control implementation, and
attack at 15 s without resilient design

Fig. 5: Actual states during exploration
(till 2 s), control implementation and
attack at 5 s for the resilient design

Fig. 6: Convergence of the retroﬁtted
controller following ARRL in Algorithm
2

Fig. 7: Measured state trajectories during
exploration, control implementation, and
attack at 5 s with ARRL

VII. CONCLUSIONS
This paper discussed a secure learning control methodology
which is resilient to adversarial actions from malicious agents.
The attacker eavesdrops the learning process and estimate
dynamic information of the CPS to conduct covert attack
subsequently. In such scenarios, we have discussed a dynamic
camouﬂaging technique during the learning to misguide the
attacker without compromising the accuracy of learning of
the optimal control. We have shown that by coupling the
dynamic system with nonlinear static time-varying functions
can provide one such dynamic camouﬂaging with adequate
guarantees. Numerical simulations conducted on a consensus
multi-agent system, and on a power system model validates
the algorithmic and theoretical considerations.

REFERENCES

[1] F. Pasqualetti, F. Dorﬂer, and F. Bullo, “Control-theoretic methods for
cyberphysical security: Geometric principles for optimal cross-layer
resilient control systems,” IEEE Control Systems Magazine, vol. 35,
no. 1, pp. 110–127, 2015.

[2] S. M. Dibaji, M. Pirani, D. B. Flamholz, A. M. Annaswamy, K. H.
Johansson, and A. Chakrabortty, “A systems and control perspective of
cps security,” Annual Reviews in Control, vol. 47, pp. 394–411, 2019.
[3] H. Li, Z. Chen, L. Wu, H.-K. Lam, and H. Du, “Event-triggered
fault detection of nonlinear networked systems,” IEEE Transactions on
Cybernetics, vol. 47, no. 4, pp. 1041–1052, 2016.

[4] E. Mousavinejad, F. Yang, Q.-L. Han, and L. Vlacic, “A novel cyber at-
tack detection method in networked control systems,” IEEE transactions
on cybernetics, vol. 48, no. 11, pp. 3254–3264, 2018.

[5] K. G. Vamvoudakis, J. P. Hespanha, B. Sinopoli, and Y. Mo, “Detection
in adversarial environments,” IEEE Transactions on Automatic Control,
vol. 59, no. 12, pp. 3209–3223, 2014.

[6] M. S. Chong, H. Sandberg, and A. M. Teixeira, “A tutorial introduction
to security and privacy for cyber-physical systems,” in 2019 18th
European Control Conference (ECC).

IEEE, 2019, pp. 968–978.

[7] Y. Yuan, Q. Zhu, F. Sun, Q. Wang, and T. Bas¸ar, “Resilient control of
cyber-physical systems against denial-of-service attacks,” in 2013 6th
International Symposium on Resilient Control Systems (ISRCS).
IEEE,
2013, pp. 54–59.

[8] C.-Z. Bai, F. Pasqualetti, and V. Gupta, “Data-injection attacks in
stochastic control systems: Detectability and performance tradeoffs,”
Automatica, vol. 82, pp. 251–260, 2017.

[9] M. Zhu and S. Martinez, “On the performance analysis of resilient
networked control systems under replay attacks,” IEEE Transactions on
Automatic Control, vol. 59, no. 3, pp. 804–808, 2013.

Fig. 8: Kundur 13-bus 4-machine power system model

we have n = 16, m = 4, and therefore, we explore for 5 s with
0.01 s time steps. The attack starts at 15 s, and we simulate till
25 s. Fig. 9 shows the actual frequency excursions during the
learning and the attack which shows considerable perturbations
caused by the adversary, however, in the measured frequencies,
as in Fig. 10, the impact of the attack is not visible, making
the attack covert. The fast convergence of the RL control
update iterations are shown in Fig. 11. The convergence is
reached with high accuracy with respect to the model-based
solutions as shown in Fig. 12. The dynamic performance
improvement of the nominal controller is shown in Fig. 13,
where the angular difference between the generators 2 and 3
characterizes the oscillations in the tie-lines connecting buses
6 and 12. Subsequently, we test the attack resilient design with
the malicious signal injection starting at 15 s. The trajectory
measurements are gathered for 8 s, and thereafter, the control
iterations of Algorithm 2 (ARRL) has been performed which
results in fast convergence as shown in Fig. 15. Most of the
entries of K matches closely with the model-based solutions,
and only few of the entries are 2 − 3% off. However, when we
test the dynamic performance of the learned ARRL control,
we can see from Fig. 17 that the optimal performance of
the nominal RL design has been recovered. Moreover, in this
scenario, the measured frequencies now capture the impact
of the the malicious attack starting from 15 s as in Fig. 14,
therefore, the attacker cannot remain covert anymore.

123456iteration012345||Kk - Kk-1||123456iteration012345||Kk - Kk-1||8

Fig. 9: Actual frequencies during
probing (till 5 s), control implementation
and attack at 15 s without ARRL

Fig. 10: Measured frequencies during
probing (till 5 s), control implementation
and attack at 15 s without ARRL

Fig. 11: Convergence of K using
nominal RL (Algorithm 1)

Fig. 12: Error % of the RL based
solution using Alg. 1 with respect to the
model based solution

Fig. 13: Improving the inter-area
dynamic performance with the nominal
RL control

Fig. 14: Measured frequencies during
probing (till 7 s), control implementation,
and attack at 15 s with ARRL

Fig. 15: Convergence of the retroﬁtted
controller following ARRL in Algorithm
2

Fig. 16: Error % of the RL based
solution using ARRL (Alg. 2) with
respect to the model based solution

Fig. 17: Improving the inter-area
dynamic performance with the ARRL
control

[10] A. O. de Sa, L. F. R. da Costa Carmo, and R. C. Machado, “Covert
attacks in cyber-physical control systems,” IEEE Transactions on In-
dustrial Informatics, vol. 13, no. 4, pp. 1641–1651, 2017.

[11] A. Barboni, H. Rezaee, F. Boem, and T. Parisini, “Detection of covert
cyber-attacks in interconnected systems: A distributed model-based
approach,” IEEE Transactions on Automatic Control, 2020.

[12] Y. Mao, S. Diggavi, C. Fragouli, and P. Tabuada, “Secure state-
reconstruction over networks subject to attacks,” IEEE Control Systems
Letters, vol. 5, no. 1, pp. 157–162, 2021.

[13] B. Satchidanandan and P. R. Kumar, “Dynamic watermarking: Active
defense of networked cyber–physical systems,” Proceedings of the IEEE,
vol. 105, no. 2, pp. 219–240, 2016.

[14] Y. Mo, S. Weerakkody, and B. Sinopoli, “Physical authentication of
control systems: Designing watermarked control inputs to detect coun-
terfeit sensor outputs,” IEEE Control Systems Magazine, vol. 35, no. 1,
pp. 93–109, 2015.

[15] A. Kanellopoulos and K. G. Vamvoudakis, “A moving target defense
control framework for cyber-physical systems,” IEEE Transactions on
Automatic Control, vol. 65, no. 3, pp. 1029–1043, 2019.

[16] C. Schellenberger and P. Zhang, “Detection of covert attacks on cyber-
physical systems by extending the system dynamics with an auxiliary
system,” in 2017 IEEE 56th Annual Conference on Decision and Control
(CDC).

IEEE, 2017, pp. 1374–1379.

[17] P. Grifﬁoen, S. Weerakkody, and B. Sinopoli, “A moving target defense
for securing cyber-physical systems,” IEEE Transactions on Automatic
Control, 2020.

[18] R. Sutton and A. Barto, Reinforcement learning - An introduction. MIT

press, Cambridge, 1998, 1998.

[19] D. Vrabie, O. Pastravanu, M. Abu-Khalaf, and F. Lewis, “Adaptive
optimal control for continuous-time linear systems based on policy
iteration,” Automatica, vol. 45, pp. 477–484, 2009.

[20] Y. Jiang and Z.-P. Jiang, “Computational adaptive optimal control for
continuous-time linear systems with completely unknown dynamics,”
Automatica, vol. 48, pp. 2699–2704, 2012.

[21] K. Vamvoudakis, “Q-learning for continuous-time linear systems: A
model-free inﬁnite horizon optimal control approach,” Systems & Con-
trol Letters, vol. 100, pp. 14–20, 2017.

[22] B. Kiumarsi, K. Vamvoudakis, H. Modares, and F. Lewis, “Optimal
and autonomous control using reinforcement learning: A survey,” IEEE
Trans. on Neural Networks and Learning Systems, 2018.

[23] S. Mukherjee, H. Bai, and A. Chakrabortty, “Reduced-dimensional
reinforcement learning control using singular perturbation approxima-
tions,” Automatica, vol. 126, p. 109451, 2021. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S000510982030649X

[24] C. De Persis and P. Tesi, “Formulas for data-driven control: Stabilization,
optimality, and robustness,” IEEE Transactions on Automatic Control,
vol. 65, no. 3, pp. 909–924, 2019.

[25] S. Mukherjee and T. L. Vu, “Reinforcement

learning of structured
control for linear systems with unknown state matrix,” arXiv preprint
arXiv:2011.01128, 2020.

[26] S. Fattahi, N. Matni, and S. Sojoudi, “Efﬁcient learning of distributed
linear-quadratic control policies,” SIAM Journal on Control and Opti-
mization, vol. 58, no. 5, pp. 2927–2951, 2020.

[27] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu, “On the sample com-
plexity of the linear quadratic regulator,” Foundations of Computational
Mathematics, pp. 1–47, 2019.

[28] Y. Zhou, K. G. Vamvoudakis, W. M. Haddad, and Z. P. Jiang, “A secure
control learning framework for cyber-physical systems under sensor and
actuator attacks,” IEEE Transactions on Cybernetics, pp. 1–13, 2020.

[29] A. Rangi, M. J. Khojasteh, and M. Franceschetti, “Learning-based
attacks in cyber-physical systems: Exploration, detection, and control
cost trade-offs,” arXiv preprint arXiv:2011.10718, 2020.

[30] D. Kleinman, “On an iterative technique for riccati equation computa-
tions,” IEEE Trans. on Automatic Control, vol. 13, no. 1, pp. 114–115,
1968.

[31] Y. Jiang and Z.-P. Jiang, Robust Adaptive Dynamic Programming.

Wiley-IEEE press, 2017.

[32] S. Mukherjee, H. Bai, and A. Chakrabortty, “On robust model-free
reduced-dimensional reinforcement learning control for singularly per-
turbed systems,” in 2020 American Control Conference (ACC).
IEEE,
2020, pp. 3914–3919.

[33] S. Mukherjee and T. L. Vu, “Imposing robust structured control con-
straint on reinforcement learning of linear quadratic regulator,” arXiv
preprint arXiv:2011.07011, 2020.

0510152025Time (sec)-50510Actual frequency trajectories (p.u.)10-3 1 2 3 412345Time (sec)-202410-50510152025Time (sec)-101Measured frequency trajectories (p.u.)10-4 1 2 3 42468iteration050100150200250300350||Kk - Kk-1||0102030405060Elements of the feedback gain matrix-4-3-2-10123Relative error between model-based and learning solutions10-405101520Time(s)-0.3-0.2-0.100.10.22 - 3 (rad)With learning controlWithout control0510152025Time (sec)-3-2-10123Measured frequency trajectories (p.u.)10-3 1 2 3 4246Time (sec)-4-2010-52468iteration050100150200250300350||Kk - Kk-1||0102030405060Elements of the feedback gain matrix-3-2-10123Relative % error between model-based and learning solutions05101520Time(s)-0.3-0.2-0.100.10.22 - 3 (rad)With ARRL based controlWithout control