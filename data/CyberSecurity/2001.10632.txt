IoT Behavioral Monitoring
via Network Traﬃc Analysis

Arunan Sivanathan

A dissertation submitted in fulﬁllment

of the requirements for the degree of

Doctor of Philosophy

School of Electrical Engineering and Telecommunications

The University of New South Wales

September 2019

0
2
0
2

n
a
J

8
2

]
I

N
.
s
c
[

1
v
2
3
6
0
1
.
1
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
Abstract

The Internet of Things (IoT) is being hailed as the next wave revolutionizing our
society. Smart homes, enterprises, and cities are increasingly being equipped with a
plethora of IoTs, ranging from smart-lights to smoke alarms and security cameras.
While IoT networks have the potential to beneﬁt society and our lives, they create
privacy and security challenges not seen with traditional IT networks. The un-
precedented scale and heterogeneity of IoT devices make today’s security measures
inapplicable to IoT networks. Due to the lack of tools for real-time visibility into
IoT network activity, operators of such smart environments are not often aware of
their IoT assets, let alone whether each IoT device is functioning properly safe from
cyber-attacks. This thesis is the culmination of our eﬀorts to develop techniques to
proﬁle the network behavioral pattern of IoTs, automate IoT identiﬁcation and clas-
siﬁcation, deduce their operating context, and detect anomalous behavior indicative
of cyber-attacks.

We begin this thesis by surveying IoT market-segments, security risks, and stake-
holder roles, while reviewing current approaches to vulnerability assessments, intru-
sion detection, and behavioral monitoring. For our ﬁrst contribution, we collect traf-
ﬁc traces and characterize the network behavior of IoT devices via attributes such as
activity cycles and signaling patterns. We develop a robust machine learning-based
inference engine trained with these attributes and demonstrate real-time classiﬁca-
tion of 28 oﬀ-the-shelf IoT devices in the lab with over 99% accuracy. Our second
contribution enhances the classiﬁcation by reducing the cost of attribute extraction
(via ﬂow-level telemetry at multiple timescales) while also identifying IoT device
states (bootup, user-interaction, and idle). Prototype implementation and evalua-
tion demonstrate the ability of our supervised machine learning method to detect
behavioral changes (including ﬁrmware updates) for ﬁve IoT devices. Our third
and ﬁnal contribution develops a modularized unsupervised inference engine that
dynamically accommodates the addition of new IoT devices and/or updates to ex-
isting ones, without requiring system-wide retraining of the model. We demonstrate
via experiments that our model can automatically detect attacks (i.e., direct, spoof-
ing, and reﬂection) and ﬁrmware changes in ten IoT devices with over 94% accuracy.

i

List of Publications

During the course of this thesis project, a number of publications have been made
based on the work presented here and are listed below for reference.

Journal Publications

1. A. Sivanathan, H. Habibi Gharakheili, and V. Sivaraman , “Detecting IoT
Behavioral Changes Using Clustering-Based Network Traﬃc Modeling”, (Un-
der review at IEEE Internet of Things Journal)

2. A. Sivanathan, H. Habibi Gharakheili, and V. Sivaraman , “Managing IoT
Cyber-Security using Programmable Telemetry and Machine Learning”, (Un-
der going revision at IEEE Transactions on Network and Service Management).

3. H. Habibi Gharakheili, A. Sivanathan, A. Hamza, and V. Sivaraman, “Network-
Level Security for the Internet of Things: Opportunities and Challenges”, Com-
puter,vol. 52(8):58-62, Aug. 2019.

4. A. Sivanathan, H. Habibi Gharakheili, F. Loi, A. Radford, C. Wijenayake,
A. Vishwanath, and V. Sivaraman, “Classifying IoT Devices in Smart Environ-
ments Using Network Traﬃc Characteristics”, IEEE Transactions on Mobile
Computing,18(8):1745-1759, Aug 2019.

Conference Publications

5. A. Sivanathan, H. Habibi Gharakheili and V. Sivaraman, “Inferring IoT
Device Types from Network Behavior Using Unsupervised Clustering”, IEEE
LCN, Osnabruck, Germany, Oct 2019.

6. S. Madanapalli, A. Sivanathan, H. Habibi Gharakheili, V. Sivaraman, S.
Patil and B. Pularikkal, “Modeling and Monitoring Wi-Fi Calling Traﬃc in

iii

Enterprise Networks Using Machine Learning”, IEEE LCN, Osnabruck, Ger-
many, Oct 2019.

7. A. Sivanathan, H. Habibi Gharakheili and V. Sivaraman, “Can We Classify
an IoT Device Using TCP Port Scan?”, IEEE ICIAfS, Colombo, Sri Lanka,
Dec 2018.

8. A. Sivanathan, F. Loi, H. Habibi Gharakheili and V. Sivaraman, “Experi-
mental Evaluation of Cybersecurity Threats to the Smart-Home”, IEEE ANTS,
Bhubaneswar, India, Dec 2017.

9. F. Loi, A. Sivanathan, H. Habibi Gharakheili, A. Radford and V. Sivara-
man, “Systematically Evaluating Security and Privacy for Consumer IoT De-
vices”, Workshop on Internet of Things Security and Privacy (IoT S&P), Dal-
las, Texas, USA, Nov 2017.

10. M. Lyu, D. Sherratt, A. Sivanathan, H. Habibi Gharakheili, A. Radford
and V. Sivaraman, “Quantifying the Reﬂective DDoS Attack Capability of
Household IoT Devices”, ACM WiSec, Boston, MA, USA, Jul 2017.

11. A. Sivanathan, D. Sherratt, H. Habibi Gharakheili, A. Radford, C. Wije-
nayake, A. Vishwanath and V. Sivaraman, “Characterizing and Classifying IoT
Traﬃc in Smart Cities and Campuses”, IEEE Infocom SmartCity17 Workshop
on Smart Cities and Urban Computing, Atlanta, GA, USA, May 2017.

12. A. Sivanathan, D. Sherratt, H. Habibi Gharakheili, V. Sivaraman and A.
Vishwanath, “Low-Cost Flow-Based Security Solutions for Smart-Home IoT
Devices”, IEEE ANTS, Bangalore, India, Nov 2016.

Patent

13. “An IoT Device Classiﬁcation Apparatus and Process”, A. Sivanathan, H.
Habibi Gharakheili, V. Sivaraman, Australian Provisional Patent, Application
No. 2018904759, ﬁled Dec 2018.

iv

Acknowledgment

First and foremost, I would like to express my sincere gratitude to my primary
supervisor, Prof. Vijay Sivaraman. Prof. Vijay, thank you for your guidance and
support throughout my Ph.D. research. It has been an absolute privilege to work
with you, and this work would not have been possible without your contagious
enthusiasm for research. I am equally grateful to my joint supervisor, Dr. Hassan
Habibi Gharakheili. Dr. Hassan, thank you for giving me valuable pointers and
ideas, shaping my research, and carefully reviewing all the manuscripts I produced
during my Ph.D. I owe you a big debt of gratitude for your time, careful attention
to detail, and challenging questions.

I would like to express my sincere thanks to Prof. Eliathamby Ambikairajah,
who oﬀered me this Ph.D. candidature at UNSW. Prof. Ambi, it wouldn’t be pos-
sible to pursue this opportunity without your recognition. I am very thankful for
the scholarship and your keen monitoring on my progress throughout my carrier.
My special thanks also go to Dr. Tharmarajah Thiruvaran, who recognized and
recommended me for this position.

This thesis is the result of three and a half years of a wonderful collaboration.
The development and execution of the ideas presented here simply would not have
been possible without the hard work, deep discussions, and shared excitement of
all my co-authors, Vijay Sivaraman, Hassan Habibi Gharakheili, Chamith Wije-
nayake, Adam Radford, Arun Vishwanath, Daniel Sherratt, Franco Loi, Minzhao
Lyu, Sharat Chandra Madanapalli, and Mohammed Ayyoob Hamza. I deeply ap-
preciate your contributions.

I graciously acknowledge and appreciate the collaboration with the teams at
Cisco Systems and the Australian Communications Consumer Action Network (AC-
CAN). Their involvement and the input provided at various stages of this work were
inevitably productive.

v

I have been extremely fortunate to meet and interact with several talented, in-
teresting, and fun fellow research colleagues, Mohammad Hossein, Ayyoob, Sharat,
Iresha, Jawad, Minzhao, and Thanchanok (Tara), who spared their time to help me
with both the happy and boring parts of the Ph.D.

I cannot forget friends who went through hard times together, cheered me on,
and celebrated each accomplishment. Anu Raghavi, Arunkumar, Gajan, Kaavya,
Kawsihen, Navaroshan, Sirojan, and Tharshini – thank you for keeping my life
nourished and fun- ﬁlled.

Words cannot express my gratitude to my parents and family, Sivanathan, Sasikala,

Abhayan, Aparajithan, and Dhanya for constantly encouraging me to pursue a doc-
toral degree. My deepest appreciation is dedicated to my wife, Anusuya. You have
been extremely caring and supportive of me throughout this entire process and have
made countless sacriﬁces to help me get to this point whilst carrying your own bur-
dens. Also, I would like to thank Anusuya’s parents and family members for their
constant moral support during this journey. Aekan and Pavinesh, because of you, I
laughed harder and smiled more.

Finally, I’m grateful to the people of Australia, who as a country has generously
oﬀered me all the necessary rights, respect, peace and safe environment, some of
which were even refused in my country of origin.

vi

Contents

Abstract

List of Publications

Acknowledgment

List of Figures

List of Tables

1 Introduction

1.1 Thesis Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 Thesis Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Survey on IoT Ecosystems

2.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2

IoT Market Segments . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2.1

Industrial IoT (IIoT) . . . . . . . . . . . . . . . . . . . . . . .

2.2.2 Consumer IoT . . . . . . . . . . . . . . . . . . . . . . . . . . .

i

iii

v

xi

xv

1

3

4

5

6

7

7

8

2.2.3 Enterprise / Commercial IoT . . . . . . . . . . . . . . . . . . 10

2.3

IoT Security Risks

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

2.3.1 Threats in IoT Network Compare to IT Network . . . . . . . 12

2.3.2 Drawbacks of Traditional Security Measures . . . . . . . . . . 12

2.3.3 Types of Cyber Attacks on IoT . . . . . . . . . . . . . . . . . 13

2.4 Perspectives and Roles of Stakeholders . . . . . . . . . . . . . . . . . 14

2.4.1 Consumers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

2.4.2 Manufacturers . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

2.4.3 Regulators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

vii

2.4.4

Insurers

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

2.5 Systematical Evaluation of Cybersecurity Threats . . . . . . . . . . . 19

2.5.1

Security Test Suite . . . . . . . . . . . . . . . . . . . . . . . . 20

2.5.2

Security Posture of IoTs

. . . . . . . . . . . . . . . . . . . . . 23

2.5.3

Security Rating of IoT Devices

. . . . . . . . . . . . . . . . . 32

2.6 Existing IoT Security Solutions

. . . . . . . . . . . . . . . . . . . . . 34

2.6.1

Signature-based Intrusion Detection . . . . . . . . . . . . . . . 35

2.6.2

Speciﬁcation-based Intrusion Detection . . . . . . . . . . . . . 35

2.6.3 Anomaly-based Instruction Detection . . . . . . . . . . . . . . 36

2.7

IoT Behavioral Monitoring . . . . . . . . . . . . . . . . . . . . . . . . 37

2.7.1

IoT Traﬃc Characterization . . . . . . . . . . . . . . . . . . . 37

2.7.2

IoT Finger-Printing and Classiﬁcation . . . . . . . . . . . . . 39

2.8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

3 IoT Traﬃc Characterization and Classiﬁcation

47

3.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

3.2

IoT Traﬃc Collection and Synthesis . . . . . . . . . . . . . . . . . . . 52

3.2.1 Experimental Test-bed . . . . . . . . . . . . . . . . . . . . . . 53

3.2.2 Trace Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54

3.3

IoT Traﬃc Characterization . . . . . . . . . . . . . . . . . . . . . . . 55

3.3.1

IoT Activity and Volume Pattern . . . . . . . . . . . . . . . . 57

3.3.2

IoT Signaling Pattern . . . . . . . . . . . . . . . . . . . . . . 58

3.4 Machine Learning Based Classiﬁcation . . . . . . . . . . . . . . . . . 65

3.4.1 Multi-Stage Device Classiﬁcation Architecture . . . . . . . . . 65

3.4.2 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . 68

3.5 Real-Time Operation in a Network . . . . . . . . . . . . . . . . . . . 73

3.5.1 Computing Attributes

. . . . . . . . . . . . . . . . . . . . . . 74

3.5.2 Training the Machine . . . . . . . . . . . . . . . . . . . . . . . 76

3.5.3

Interpreting the Output of Classiﬁer

. . . . . . . . . . . . . . 77

3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

viii

4 Behavioral Monitoring using Low-Cost Attributes

81

4.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

4.2 Traﬃc Flows and Attributes . . . . . . . . . . . . . . . . . . . . . . . 84

4.2.1 Traﬃc Trace Dataset . . . . . . . . . . . . . . . . . . . . . . . 84

4.2.2 Traﬃc Flows and Attributes . . . . . . . . . . . . . . . . . . . 86

4.2.3 Traﬃc Characteristics of IoT Devices . . . . . . . . . . . . . . 89

4.3

IoT Traﬃc Inference Engines . . . . . . . . . . . . . . . . . . . . . . . 93

4.3.1

Inference Architecture . . . . . . . . . . . . . . . . . . . . . . 93

4.3.2 Models Training and Performance Evaluation . . . . . . . . . 95

4.4 Practical and Operational Considerations . . . . . . . . . . . . . . . . 103

4.4.1 Cost of Attributes

. . . . . . . . . . . . . . . . . . . . . . . . 103

4.4.2 Use of Inference Engines in Real-Time . . . . . . . . . . . . . 108

4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

5 Behavioral Change Detection using Clustering Algorithm

113

5.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

5.2 Clustering Flow-Level Attributes

. . . . . . . . . . . . . . . . . . . . 116

5.2.1 Flow-Level Telemetry and Traﬃc Attributes . . . . . . . . . . 116

5.2.2 Attributes Clustering . . . . . . . . . . . . . . . . . . . . . . . 119

5.3 Unsupervised Classiﬁcation of IoT Devices . . . . . . . . . . . . . . . 122

5.3.1 Clustering Models: Generation, Tuning, and Testing

. . . . . 122

5.3.2 Conﬂict Resolution . . . . . . . . . . . . . . . . . . . . . . . . 124

5.3.3 Consistency Score . . . . . . . . . . . . . . . . . . . . . . . . . 127

5.3.4 Monitoring Phases

. . . . . . . . . . . . . . . . . . . . . . . . 130

5.4 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 131

5.4.1 Device Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . 131

5.4.2 Detecting Behavioral Change

. . . . . . . . . . . . . . . . . . 134

5.4.3 Detecting Attacks . . . . . . . . . . . . . . . . . . . . . . . . . 137

5.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143

6 Conclusions and Future Work

145

ix

6.1 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145

6.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147

References

149

x

List of Figures

2.1 Communication model: (a) Direct, (b) External, (c) Transit

. . . . .

9

2.2 TPLink camera POST message . . . . . . . . . . . . . . . . . . . . . 25

2.3 Control bit pattern of LiFX lightbulb . . . . . . . . . . . . . . . . . . 26

2.4 Wireshark capture of Amazon Echo . . . . . . . . . . . . . . . . . . . 27

2.5 Wireshark capture of Netatmo camera . . . . . . . . . . . . . . . . . 27

2.6 Belkin camera is pairing with user app . . . . . . . . . . . . . . . . . 28

3.1 Testbed architecture showing connected 28 diﬀerent IoT devices along
with several non-IoT devices, and telemetry collected across the in-
frastructure is fed to our classiﬁcation models.

. . . . . . . . . . . . . 52

3.2 Sankey diagram of daily network activity for two representative IoT
devices, Amazon Echo and LiFX lightbulb. A clear distinction is
observed in terms of their communication patterns, i.e. the servers
they talk to, and the port numbers and protocols used for data exchange. 56

3.3 Distribution of IoT activity pattern: (a) ﬂow volume, (b) ﬂow dura-

tion, (c) average ﬂow rate and (d) device sleep time. . . . . . . . . . . 58

3.4 Word-cloud of server ports (total count of unique ports is shown in

{sub-captions} next to the device name). . . . . . . . . . . . . . . . . 59

3.5 Word-cloud of domain names (total count of unique domains is shown

in {sub-captions} next to the device name).

. . . . . . . . . . . . . . 60

3.6 Distribution of IoT signaling pattern: (a) DNS interval, (b) NTP

interval.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62

3.7 Signature of cipher suite. . . . . . . . . . . . . . . . . . . . . . . . . . 63

3.8 Signature of cipher suite. . . . . . . . . . . . . . . . . . . . . . . . . . 64

3.9 System architecture of the multi-stage classiﬁer.

. . . . . . . . . . . . 66

xi

3.10 Confusion matrix of our IoT device classiﬁcation using all attributes

(accuracy: 99.88%, RRSE: 5.06%).

. . . . . . . . . . . . . . . . . . . 74

3.11 Merit of attributes.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 75

3.12 Operational insights for real-time implementation of our device classi-

ﬁer: (a) impact of training and (b) conﬁdence-level for correct/incorrect
classiﬁcation.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

4.1 System architecture of network telemetry and inference engines.

. . . 87

4.2 Histogram of traﬃc proﬁle to compare IoT and non-IoT devices: (a)
remote traﬃc volume over 32-minute; and (b) DNS query count over
64-minute. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89

4.3 Histogram of traﬃc proﬁle for representative IoT devices: (a) NTP
response count, (b) download volume of remote traﬃc, and (c) upload
volume of SSDP traﬃc.

. . . . . . . . . . . . . . . . . . . . . . . . . 90

4.4 Histogram of traﬃc proﬁle for IoT devices at three operating states:

(a) Amazon Echo, (b) Belkin switch, and (c) Dropcam.

. . . . . . . . 92

4.5 Hierarchical architecture of IoT traﬃc inference engines.

. . . . . . . 94

4.6 Confusion matrix of the IoT detector. . . . . . . . . . . . . . . . . . . 97

4.7 Confusion matrix of IoT classiﬁer trained by the ﬁrst three months’

worth of data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98

4.8 Time trace of IoT classiﬁer outputs with test traﬃc instances from:

(a) HP printer, and (b) Hue light-bulb.

. . . . . . . . . . . . . . . . . 99

4.9 Confusion matrix of IoT device classiﬁcation re-trained by additional

data from two weeks in February.

. . . . . . . . . . . . . . . . . . . . 100

4.10 Confusion matrix of IoT state classiﬁers: (a) Amazon Echo; (b) Belkin

switch; (c) Dropcam; and (d) LiFX bulb. . . . . . . . . . . . . . . . . 102

4.11 Information gain value of: (a) all attributes, and (b) CFS-selected

attributes, for the IoT classiﬁer.

. . . . . . . . . . . . . . . . . . . . . 104

4.12 Information gain of all attributes for state classier models: (a) Ama-
zon Echo, (b) Belkin switch, (c) Dropcamp, and (d) LiFX bulb.

. . . 105

4.13 Information gain of non-redundant attributes for state classier mod-
els: (a) Amazon Echo, (b) Belkin switch, (c) Dropcamp, and (d) LiFX
bulb. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

xii

4.14 Impact of attributes on: (a) performance; (b) cost, for the IoT classiﬁer.107

4.15 Time trace of device classiﬁer outputs with benign and attack traﬃc

of: (a) Belkin switch; and (b) Amazon Echo. . . . . . . . . . . . . . . 109

4.16 Time trace of outputs for: (a) device classiﬁer, and (b) state classiﬁer,

with benign and attack traﬃc of LiFx bulb.

. . . . . . . . . . . . . . 110

5.1 Clusters of data instances in two-dimensional space for representative

IoT devices: (a) Amazon Echo; (b) Belkin switch; and (c) LiFX bulb. 120

5.2 Distance probability of clusters: (a) C3 of LiFX bulb;(b) B3 of Belkin

switch; and (c) A1 of Amazon Echo.

. . . . . . . . . . . . . . . . . . 121

5.3 Elbow method for selecting optimal number of clusters. . . . . . . . . 123

5.4 Use of each clustering model for a test instance.

. . . . . . . . . . . . 125

5.5 Distribution of clustering probability for training instances of three

device types. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126

5.6 Architecture of our inference engine.

. . . . . . . . . . . . . . . . . . 127

5.7 Dynamics of consistency score for Smart Things instances in real-time.128

5.8 Real-time update rate of consistency score – for a given model, it falls
fast (from highest to lowest value in 3 hours) on continuous negative
outputs, and rises slowly (from lowest to highest value in 24 hours)
on continuous positive outputs.

. . . . . . . . . . . . . . . . . . . . . 129

5.9 Confusion matrix of device classiﬁcation: (a) raw output of clustering

models; and (b) reﬁned output after conﬂict resolution.

. . . . . . . . 132

5.10 Time-trace of consistency score for normal behavior in: (a) Belkin

switch; and (b) Triby speaker.

. . . . . . . . . . . . . . . . . . . . . . 134

5.11 Wireshark capture of Triby speaker packets showing outage of SIP

server.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135

5.12 Time-trace of consistency score due to ﬁrmware upgrade in Dropcam

traﬃc: (a) original model; and (b) re-trained model. . . . . . . . . . . 136

5.13 CDF: distribution of conﬁdence-level for Belkin motion instances.

. . 140

5.14 Performance comparison for traﬃc of Samsung cam during attack:

(a) one-class model; and (b) multi-class model. . . . . . . . . . . . . . 142

xiii

List of Tables

2.1 Posture of conﬁdentiality . . . . . . . . . . . . . . . . . . . . . . . . . 24

2.2 Posture of integrity and authentication . . . . . . . . . . . . . . . . . 29

2.3 Posture of access control and availability . . . . . . . . . . . . . . . . 30

2.4 Posture of reﬂection attacks . . . . . . . . . . . . . . . . . . . . . . . 32

2.5 Security rating . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

3.1 MAC address and DHCP host name of IoT devices used in our testbed. 50

3.2 Performance of the proposed IoT device classiﬁer under diﬀerent sets

of attributes.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

3.3

Impact of attributes combination on performance of classiﬁer.

. . . . 76

4.1 Flow rules speciﬁc to each device, proactively inserted into SDN

switch for real-time telemetry.

. . . . . . . . . . . . . . . . . . . . . . 88

4.2 Performance metrics of the IoT detector model.

. . . . . . . . . . . . 97

4.3 Performance metrics of the IoT classier model (after re-training).

. . 101

4.4 Flow entries (per-device) needed for non-redundant attributes set.

. . 106

5.1 Flow rules (per-device) needed for network traﬃc telemetry.

. . . . . 117

5.2 Summary of partial DATA1 (benign traﬃc): Device instances and

clustering parameters.

. . . . . . . . . . . . . . . . . . . . . . . . . . 119

5.3 Summary of DATA2: device instances and clustering parameters –
benign traces for training, and mix of benign and attack traces for
testing).

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138

5.4 Detection rate (%) of direct attacks: per model (in rows) and per

attack-type (in columns). . . . . . . . . . . . . . . . . . . . . . . . . . 139

xv

5.5 Detection rate (%) of reﬂection attacks: per model (in rows) and per

attack-type (in columns). . . . . . . . . . . . . . . . . . . . . . . . . . 140

5.6 Performance of one-class classiﬁers for mix of attack and benign traﬃc.141

xvi

xvii

Chapter 1

Introduction

Contents

1.1 Thesis Contributions . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 Thesis Organization . . . . . . . . . . . . . . . . . . . . . . . . .

3

4

The number of devices connecting to the Internet is rapidly increasing, signalling

the beginning of the era of “Internet of Things” (IoT). IoT refers to the tens of billions

of low–cost devices autonomously communicating with each other and with remote

servers on the Internet. They include everyday objects such as lights, cameras,

motion sensors, door locks, thermostats, ﬁtness trackers, power switches and house-

hold appliances. With shipments projected to reach nearly 20 billion by 2020 [1],

thousands of IoT devices are expected to ﬁnd their way in homes, enterprises, cam-

puses, and cities of the near future, engendering “smart” environments beneﬁting

our society and our lives.

While the beneﬁts of IoT devices are well understood, they have unfortunately

also become weapons of destruction in the hands of cyber-attackers. Recent in-

cidents show that the consequences of exploiting IoT vulnerabilities can be high:

an eavesdropper can illegitimately snoop into family activities, an attacker can take

control or shut down a power grid, and household devices can become launch-pads to

1

Chapter 1.

Introduction

attack popular web-services. Securing IoT devices from attack remains a formidable

challenge. The large scale and heterogeneity in IoT devices, each with its own hard-

ware, ﬁrmware, and software, makes the security vulnerabilities diverse and attack

vectors complex. The reasons for such vulnerabilities can be manifold, for exam-

ple, devices do not have any host protection because of the resource constraints,

device integrators obtain device parts from various suppliers without conducting

any systematic security testing, and device manufacturers have low motivation to

embed security in consumer IoT devices, as they are dissuaded by low margins,

time-to-market pressure, and limited skills.

Network operators are not always fully aware of their IoT assets and they lack

tools that provide visibility into device operational behavior. Obtaining visibility in

a timely manner is paramount to network operators so as to ensure that devices are

in appropriate network security segments, and device behavioral changes indicative

of cyber-attacks are able to be detected, so they can be quarantined rapidly when a

breach is identiﬁed.

This thesis begins by surveying the security challenges in the IoT ecosystem

by categorizing connected devices based on market segments, assessing the risks

and challenges compared to traditional IT networks, and recognizing the major

stakeholders and their responsibilities. We also develop a systematical approach to

evaluate the security of smart home devices, validate it empirically in the lab using

many consumer IoT devices, and compare it against threats and solutions used in

traditional IT security.

The observations above emphasize the need to develop a deep understanding of

the behavior of network traﬃc to/from IoT devices. The objective in this thesis

is therefore to develop behavioral models of IoT devices, which allows for feature

extraction, automated classiﬁcation, and anomaly detection using machine learning

algorithms. Equally important to this thesis is the empirical validation of the models

using data collected from IoT devices in the lab and in the wild. In this context,

the major contributions of this thesis are as follows:

2

1.1 Thesis Contributions

Chapter 1.

Introduction

1. Our ﬁrst contribution is to learn the unique traﬃc behavioral characteristics of

various IoT devices. We build an IoT experimental testbed instrumented with

28 consumer IoT devices and ﬁve non-IoT devices. The traﬃc characteristics

of each device are analyzed via attributes comprising activity patterns (e.g.,

distribution of ﬂow volume, ﬂow duration, traﬃc rate, and device sleep time)

and signalling patterns (e.g., server ports, domain names, cipher suites, DNS,

and NTP queries) extracted from the network traﬃc traces. These attributes

are used to develop an inference engine to classify the IoT device types using

machine learning techniques. The proposed approach is trained and validated

using the data collected over a six months period from our lab testbed, and

demonstrated to achieve over 99% accuracy.

2. Our second contribution is to develop techniques to extract ﬂow-based at-

tributes at multiple timescales using a programmable telemetry architecture

and to minimize the cost of attribute extraction. We then develop an infer-

ence engine by using these extracted ﬂow level attributes along with a multi-

stage supervised learning architecture to detect the behavioral changes of IoT

devices, distinguish IoT traﬃc from non-IoTs, classify individual types and

identify states (e.g., bootup, user-interaction, and idle) during its normal op-

erations. We then quantify the trade-oﬀ between performance and cost of our

solution, and demonstrate how our monitoring scheme can be used in real-time

operation for detecting behavioral changes (i.e., ﬁrmware upgrade or cyber-

attacks).

3. For our third contribution, we invent unsupervised, modularized machines to

achieve a per-device type classiﬁcation. This allows us to dynamically ac-

commodate changes (e.g., ﬁrmware upgrade or addition of a new type) in

an IoT network without requiring a system-wide retraining. The machines

are sensitive to minor deviations in traﬃc characteristics, allowing us to iden-

3

Chapter 1.

Introduction

tify changes arising from low-rate cyber-attacks. We validate this by launching

multi-rate attacks including port scanning, ARP spooﬁng, smurf, fraggle, TCP

SYN ﬂooding and UDP/TCP/ICMP reﬂections in our testbed. We have shown

that our machine is able detect attacks with a high true positive rate (TPR).

1.2 Thesis Organization

The rest of this thesis is organized as follows. Chapter 2 published in [2–4] surveys

the landscape of the IoT ecosystem and highlights related work and contributions

made in recent years.

In Chapter 3, published in [5, 6], we characterize the IoT

traﬃc based on the network activity and signalling pattern, and classify the device

types using machine learning-based inference engines. Chapter 4 published in [7, 8]

presents a low-cost attribute extraction architecture using the software deﬁned net-

working (SDN) paradigm and enhances the inference engine to recognize IoT devices

along with the device types and states. In Chapter 5, published in [9, 10], we en-

hance the inference to an unsupervised modular device classiﬁcation architecture to

accommodate behavioral changes of the devices while also developing a methodology

to monitor the consistency of the classiﬁer and validate the classiﬁcation framework

using benign and attack traﬃc. Chapter 6 concludes the thesis with pointers to

direction for future work.

4

Chapter 2

Survey on IoT Ecosystems

Contents

2.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2

IoT Market Segments . . . . . . . . . . . . . . . . . . . . . . . .

2.2.1

Industrial IoT (IIoT) . . . . . . . . . . . . . . . . . . . .

2.2.2 Consumer IoT . . . . . . . . . . . . . . . . . . . . . . . .

6

7

7

8

2.2.3

Enterprise / Commercial IoT . . . . . . . . . . . . . . .

10

2.3

IoT Security Risks . . . . . . . . . . . . . . . . . . . . . . . . . . 11

2.3.1 Threats in IoT Network Compare to IT Network . . . .

2.3.2 Drawbacks of Traditional Security Measures . . . . . . .

2.3.3 Types of Cyber Attacks on IoT . . . . . . . . . . . . . .

12

12

13

2.4 Perspectives and Roles of Stakeholders . . . . . . . . . . . . . . . 14

2.4.1 Consumers . . . . . . . . . . . . . . . . . . . . . . . . . .

2.4.2 Manufacturers . . . . . . . . . . . . . . . . . . . . . . . .

2.4.3 Regulators . . . . . . . . . . . . . . . . . . . . . . . . . .

2.4.4

Insurers

. . . . . . . . . . . . . . . . . . . . . . . . . . .

15

15

16

19

2.5

Systematical Evaluation of Cybersecurity Threats

. . . . . . . . 19

2.5.1

Security Test Suite . . . . . . . . . . . . . . . . . . . . .

2.5.2

Security Posture of IoTs

. . . . . . . . . . . . . . . . . .

2.5.3

Security Rating of IoT Devices

. . . . . . . . . . . . . .

20

23

32

2.6 Existing IoT Security Solutions . . . . . . . . . . . . . . . . . . . 34

5

Chapter 2. Survey on IoT Ecosystems

2.6.1

Signature-based Intrusion Detection . . . . . . . . . . . .

2.6.2

Speciﬁcation-based Intrusion Detection . . . . . . . . . .

2.6.3 Anomaly-based Instruction Detection . . . . . . . . . . .

35

35

36

2.7

IoT Behavioral Monitoring . . . . . . . . . . . . . . . . . . . . . 37

2.7.1

IoT Traﬃc Characterization . . . . . . . . . . . . . . . .

2.7.2

IoT Finger-Printing and Classiﬁcation . . . . . . . . . .

37

39

2.8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

The IoT ecosystem is in its early stages, and concerns about security and privacy

threats have been getting increasing attention. In this chapter, we summarize the

IoT ecosystem and challenges involved in protecting the device from cyber-attacks.

Our ﬁrst contribution explores the IoT ecosystem in terms of the market segments,

risks, and challenges, as well as the expected responsibilities of major stakeholders in

securing the devices. The second contribution proposes a systematical approach to

evaluate the security of the devices by exploring aspects of conﬁdentiality, integrity,

access control and the possibility to launch reﬂection attacks. Then we review

the existing IT security solutions and the challenges in adapting them to the IoT

domain. Finally, we study existing techniques for characterization, classiﬁcation,

and anomaly detection in IoT network traﬃcs. Parts of this chapter have been

published in [2], [3] and [4].

2.1 Introduction

The phrase “Internet of Things” introduced by Kevin Ashton in 1999, refers to the

Internet-connected cyber-physical systems which sense and control the physical en-

vironment without much human interventions unlike traditional general purpose

computers and smartphones [11]. Internet-connected devices have already started

to create a profound eﬀect on us by oﬀering the promise of unparalleled freedom and

ﬂexibility, be it for ﬁtness, health, eﬃciency, safety, or entertainment [12]. The num-

ber of IoT devices in use is exponentially growing – 20.4 billion Internet-connected

6

Chapter 2. Survey on IoT Ecosystems

devices will be instrumented across the globe by 2020 as per [13]. Australia’s largest

telco operator Telstra predicts that an average Australian household which had 13

Internet-connected devices in 2017 will reach 30 by 2020 [14].

2.2 IoT Market Segments

According to IDC’s forecast, spending on IoT technology is expected to surpass

US$ 1.2 trillion in 2022 with the compound annual growth of 13.6% [15]. It covers

a wide range of application domains such as smart home, wearables, automobiles,

industrial IoT, smart cities, smart agriculture, intelligent retail, energy management,

and health care. The IoT market can be folded into three main categories: 1)

Industrial IoT; 2) Consumer IoT; and 3) Enterprises / Commercial IoT. This section

provides a brief overview of these segments.

2.2.1

Industrial IoT (IIoT)

Industrial IoT (IIoT) brings the fourth industrial revolution (Industry 4.0) into

reality by facilitating remote monitoring and automation in value chains of the

manufacturing industries (e.g., transportation, oil-and-gas, mining, energy/utilities,

aviation, and logistics). IIoT includes interconnected industrial sensors, controllers,

and actuators to maximize the eﬃciency and reliability in mission-critical infras-

tructures of the industries. IIoT devices are speciﬁcally designed to tolerate rugged

environment and operate for the long-term. They are mainly instrumented in new

or legacy systems to perform relatively simple tasks like measuring the fuel levels or

monitoring the product quality with the assistance of sensors [16, 17].

Typically, IIoT devices are deployed in large scale Industrial Control Networks

(ICN) which are transparent as opposed to a corporate IT network. To maintain

interoperability and management through single management systems (e.g., ICS,

SCADA), industries tend to use homogeneous devices and similar protocols (e.g.,

7

Chapter 2. Survey on IoT Ecosystems

MQTT, DDS, CoAP) within a speciﬁc network. Also, industries mostly have an

in-house ability to maintain life cycles (e.g., updating ﬁrmware or patching vulner-

abilities) of the devices. Due to the transparency of ICNs and predictable behavior

of the IIoT devices, it is relatively easy to barricade IIoT using simple security

policies [18].

2.2.2 Consumer IoT

Consumer IoT refers to the connected gadgets built for personal use, ranging from

wearable health monitors, smart bulbs, smoke-alarms, and webcams to smart home

appliances such as fridges. These devices come in diﬀerent form factors to perform

heterogeneous functionalities. Unlike IIoT, consumer IoT manufacturers prioritize

low cost, advanced functionalities, user convenience, and elegant interfaces over per-

formance, reliability, and long-term support of the devices. Typically, these devices

collect and deal with a lot of private and sensitive user information since they work

in an environment very close to users.

The consumer IoT devices are mostly connected to small scale networks similar

to a home network where they function independently. The devices from diﬀer-

ent vendors oﬀer diﬀerent management portals or mobile apps to control the IoT

devices [19]. Fig 2.1 shows the main three communication models used by the de-

vices to exchange the data with users and the cloud-based services: 1) Direct access

model – the devices not only communicate directly with cloud services but also, can

be controlled directly through the management portals or smartphone Apps (e.g.,

LiFX bulb, HP Envy Printer); 2) External access model – the device directly com-

municates with the cloud services only. It doesn’t provide any direct interface or

API to control by users. However, users can get updates via connecting with the

cloud servers (e.g., Awair air quality monitor, Nest smoke sensor); and 3) Transit

model – this method is mostly used in low power devices which do not have the

8

Chapter 2. Survey on IoT Ecosystems

Figure 2.1: Communication model: (a) Direct, (b) External, (c) Transit

direct Internet connectivity. These devices communicate with smartphones using

low powered communication mediums such as Bluetooth or Near Field Communi-

cation (NFC). Then the smartphone relays the data to a vendor cloud server over

the Internet (e.g., Fitbit, Tile Bluetooth tracker) [20].

The interoperability of consumer IoTs is inherently limited since each manufac-

turer uses dissimilar protocols for authorization and communication. Still, consumer

IoTs use commonly accepted protocols (e.g., UPnP, Bonjour, REST, mDNS) to dis-

cover, control, and communicate with other devices. On the other hand, integration

platforms such as IFTTT, voice-activated assistances (e.g., Google Home, Ama-

zon Echo) and dedicated IoT hubs (e.g., Samsung SmartThings) enable somewhat

interoperability using local or cloud to cloud APIs.

The level of inbuilt security mechanisms of the consumer IoTs varies between

vendor to vendor. Also, consumer IoT platforms usually use third-party tools and

services, which adds another level of complexity in security. On the other hand, the

heterogeneous behavior of consumer IoT devices makes it diﬃcult to protect using

simple security policies.

9

IoTInternetIoTIoTInternetInternet(a)(b)(c)Chapter 2. Survey on IoT Ecosystems

2.2.3 Enterprise / Commercial IoT

Enterprise IoT (also referred to commercial IoT) can be found in large organiza-

tional networks such as smart buildings, retail spaces, and smart cities (e.g., smart

lighting, connected HVAC system). The characteristics of enterprise IoT overlaps

the behavior of industrial IoTs and consumer IoTs [21]. For instance, enterprise IoT,

like smart lighting system, oﬀers a user-friendly interface to consumers, whereby

they optimize the power usage of the whole organization by controlling thousands

of light bulbs.

The enterprise IoT mostly support automation protocols, namely BACnet and

Modbus, to exchange data with other devices. Although typical enterprise IoTs are

managed by the private cloud services, they may update the information to the

public cloud APIs as well (e.g., a public transportation company which equipped

GPS trackers on their buses may update the location of buses to users through

common mapping services similar to Google map).

Industrial IoT is typically managed by a centralized controller in an industry;

nevertheless, a single organizational network may contain several independent en-

terprise IoT systems and be controlled by diﬀerent departments. For example, a

smart city may have street lights which are monitored by councils and traﬃc signals

which are controlled by city police. Due to this complexity, the authentication and

access permissions are sophisticated compared to consumer or pure Industrial IoTs,

which may require support with hierarchical access control and directory services

(e.g., LDAP, Active Directory). Despite the fact that enterprise IoTs are commonly

connected behind relatively secured corporate networks, the involvement of large

people, departments, in the IoT ecosystem, may lead to insecure settings.

10

2.3 IoT Security Risks

Chapter 2. Survey on IoT Ecosystems

IoTs are being rapidly adopted as they give us the opportunity to enjoy incredible

experiences in our life. Nevertheless, they are susceptible to attack by those wishing

to harm us. Many Internet-connected devices have poor in-built security measures

that make them vulnerable, and these ﬂaws have the potential to reveal private

data and information that may further hurt or alarm us. A typical smart home

with many IoT devices is under signiﬁcant risk of cyberattack. This vulnerability

compromises data and threatens our safety. The frequency and severity of cyber-

attacks has been escalating in recent years. As each month brings new consumer

IoT devices to the market and millions of deployments in households worldwide, new

security and privacy attack vectors open up that can be exploited at a scale never

seen before.

Furthermore, search engines, such as Shodan [22] and Inseccam [23], are dis-

covering vulnerable IoT devices exposed to the Internet; openly available lists of

IoT default username-password combinations [24]; as well as the publicly available

botnet codes similar to Mirai [25, 26], which make it an eﬀortless task to launch a

cyberattack.

The attacks on IoT devices have already started to show the impact on the econ-

omy of the companies as well as the privacy of users. One-fourth of the companies,

which are rapidly moving towards IoT, reported at least $34 million security-related

losses in only the last year [27]. Studies show that 91.5% of data generated by IoT de-

vices are exposed as plain text – readable by anybody snooping the transaction [28].

It includes private and sensitive information such as medical records [29].

11

Chapter 2. Survey on IoT Ecosystems

2.3.1 Threats in IoT Network Compare to IT Network

The attacks currently targeting the IoT devices are not completely brand new in

cyberspace. Although they have been encountered in traditional IT network over

the decades, they pose new dimensions of challenges in the IoT ecosystem [30].

First of all, the scale of IoT and the number of exposed endpoints create a massive

exploitable threat surface which has never been seen in the traditional networks.

The traditional IT networks are built upon a very limited number of platforms

(i.e., applications, operating systems, and device vendors) which are well grown and

constantly undergo security evaluations by experts. However, every year hundreds of

new IoT devices are introduced to the market by newly emerging startups – mostly

who do not have expertise in security. This makes the situation worse.

Almost 90% of IoT devices closely monitor and collect some form of personal

data like location, health, habits, interests, etc [31]. Also, they interact with the

physical environment without any intervention from human users. These factors,

as a result of the security breach, possibly create severe consequences as it results

in a loss of privacy or infrastructure damage, or worse it results in safety hazards

to users [32].

In 2015, security evaluators demonstrated this by hijacking a jeep

remotely [33]. Furthermore, the IoT behind the secured enterprise networks becomes

an easy inﬁltration point for attackers [34] and this can then jeopardize other devices

in the network. The compromised ﬁsh tank sensors used to hack a casino is a good

example of this scenario [35].

2.3.2 Drawbacks of Traditional Security Measures

The present IT ecosystem is mainly protected by host-based threat protection mech-

anisms (e.g., Anti-virus) and network perimeter defenses (e.g., ﬁrewalls and instruc-

tion detection systems (IDS)). Unlike the general-purpose computers or smartphones

12

Chapter 2. Survey on IoT Ecosystems

in the IT ecosystem, IoT devices come with a very limited computational power.

Therefore they cannot run in-built protection mechanisms like anti-virus software

as well as key certiﬁcate exchanges and state of the art encrypted algorithms during

communications. Moreover, IoTs do not have enough resources like storage, battery,

and computational power to support automatic ﬁrmware upgrading and security

patching mechanism [34] like our smartphones.

On the other hand, the scale and heterogeneity of the IoT ecosystem make it

diﬃcult to protect using the existing network-level defense systems. For example,

the traditional networks can be protected for a certain degree by allowing well-known

protocols (i.e., port numbers) only through ﬁrewalls. However, the IoT ecosystem

makes it impossible because of the diverse amount of protocols and standards used

in the devices [36]. Meanwhile, the classical attack signature identiﬁcation using

IDS is also not an easy task since the attack and response patterns vary between

devices. The detailed analysis of the existing IDS will be discussed in §2.6.

IoT devices are vulnerable to indirect attacks as well. For example, in a smart

home, door locks might be conﬁgured to unlock automatically while the smoke sensor

alarm is triggered. An attacker may exploit this cross-device communication to open

the door by compromising under secured smoke sensor [37]. This kind of attack can

be preventable only if security systems distinguish the context of devices in the

network [38].

2.3.3 Types of Cyber Attacks on IoT

Cyber attacks on IoT can be categorized into 4 diﬀerent criteria: 1) attacks that

aﬀect the conﬁdentiality of the data communication of them; 2) attacks that aﬀect

the integrity and authentication of connections they establish with other entities

(local or external); 3) attacks that aﬀect the access control and availability to make

the connection with legitimate devices; and 4) attacks that use the IoT devices

13

Chapter 2. Survey on IoT Ecosystems

as reﬂectors to attack new devices. Following attacks recorded in recent history

can explain the severity of each types.

In November 2015, hackers compromised

a Hello Barbie doll and gained access to user accounts and encrypted audios [39]

(Conﬁdentiality violation). In 2016, a large scale attack used the Zigbee protocol in

the Phillips Hue lightbulb to spread a worm to control other lightbulbs [40] (integrity

violation). In 2017, a hacker with the name “Stackoverﬂowin” gained illegitimate

access to 150,000 printers by exploiting the Internet printing protocol (IPP), and

was able to send out rogue print jobs [41](access control violation).

In the same

year, a university became a victim of DDoS attack from its campus lamp posts and

vending machines [42] and one of the largest distributed denial of service (DDoS)

attack to dates, recorded in 2016, used an army of compromised IoT devices to

bring down the Dyn DNS server. It aﬀected many popular websites [43] (availability

violation)). Although we have not seen any mass scale reﬂection attacks using IoT

devices yet, the researches suggest that present IoT devices are highly capable to

reﬂect the attacks[44]. We develop a systematic approach to evaluate these 4 types

of vulnerabilities in §2.5. Later, in Chapter 5 we will validate the eﬃcacy of our

anomaly detection engine by launching DDoS (Type 3) and Reﬂection attacks (Type

4) which are commonly used for IoT devices at scale.

2.4 Perspectives and Roles of Stakeholders

It is a well-known fact that there is no one-oﬀ solution to secure all IoT devices. It

requires a lot of responsibilities and actions to be taken by various entities related to

the IoT ecosystem [45]. This section identiﬁes the main players of the IoT ecosystem

and discuss their role and responsibilities in the security domain.

14

Chapter 2. Survey on IoT Ecosystems

2.4.1 Consumers

Mostly IoT consumers do not scrutinize the security of devices during their pur-

chase. There are two main reasons for that: 1) they don’t seem to be aware of how

detailed and sensitive data are being collected by their devices nor are they aware

the consequences if that data get compromised; and 2) they don’t have the knowl-

edge to rate the safety of a device. Also, many IoT users do not follow good security

practices such as applying strong passwords – 10 out of 100 devices have never been

changed from their default username and passwords (e.g., <admin,admin>, <ad-

min,password>) [46]. They assume that device manufacturers or service providers

apply the software updates and patches until the lifetime of the device – actually,

this is a myth. Nevertheless, it is not reasonable to expect them to be tech-savvy

enough to patch the devices manually as well. Experts say, although consumers do

not have the capacity to understand the technical terms related to the security, they

can be indicated using a rating scheme similar to “energy eﬃciency star rating” in

electrical appliances [47].

2.4.2 Manufacturers

The peak demand for IoT leads the manufacturers to focus on rushing the device

to the market rather than prioritizing the security. Furthermore, manufacturers

hesitate to provide long term supports to devices, especially due to development

costs. They are more likely to release a new version with the improved functionalities

to get more proﬁts than supporting the previous version. Even though some of the

manufacturers are aware that their devices support mass scale DDoS, they don’t

give close attention towards ﬁxing the issues. The reason is those kinds of attacks

don’t impact the customers directly [48].

15

Chapter 2. Survey on IoT Ecosystems

2.4.3 Regulators

As security and privacy concern rises about the IoTs, there have been calls for gov-

ernment regulations in the IoT ecosystem. These regulations are expected to urge

manufacturers to build devices with minimum security standards. The main impli-

cation in this process is, according to the current settings, diﬀerent domains of the

IoT may fall under the diﬀerent departments’ regulations. For example, devices re-

lated to health and medical come under the rules and regulations of the Therapeutic

Goods Administration within the Department of Health. Meanwhile, services and

technologies such as telecommunications, broadcasting, radio communications, and

the Internet are regulated by the Australian Communications and Media Authority.

In the scenario of Medical grade IoT, it may require the attention of both depart-

ments [45]. On the other hand, some people believe strict government regulations

may create additional bureaucracy and stiﬂe the innovation and agility in the IoT

development [49]. Several governments have already started to propose very basic

level legislations to handle this trade-oﬀ.

United States of America

The United States Congress introduced a bill “Internet of Things (IoT) Cybersecu-

rity Improvement Act of 2019” to set minimum standards to procure and use the IoTs

for government agencies. It requested the recommendations from the National In-

stitute of Standards and Technology (NIST) to propose minimum standards on IoT

development, identity management, patching, and conﬁguration management [50].

However, this bill does not consider consumer or business use cases.

16

Chapter 2. Survey on IoT Ecosystems

UK

In 2019, the UK proposed a basic set of code of practice (CoP) to be followed on

IoT design and development [51]. It includes: 1) unique factory reset settings for

each device – cannot have universal default passwords for all devices; 2) a public

point of contact has to be provided by manufacturers to disclose the vulnerabilities;

3) requirement to explicitly state the minimum duration that a device will continue

to receive security updates or patches; and 4) a labeling system to determine the

level of security – similar to health star rating on foods or energy rating on electrical

appliances. Currently, the oﬃcials say the government is planning to impose the ﬁrst

three practices as mandatory ‘Secure by Design’ rules and the labeling system as a

voluntary scheme to improve the knowledge of consumers about the basic security

standards of devices.

Japan

The National Institute of Information and Communications Technology (NICT) of

Japan has announced a scanning over the nationwide Internet-connected devices to

identify the vulnerabilities. This project has been estimated to continue until 2022.

During the experiment, agencies especially probe the devices using the list of default

usernames and passwords without the concern of citizens and businesses to identify

the IoTs with easily guessable credentials. The owners of the devices will be notiﬁed

if the scan ﬁnds any potential security issues. Although this search can help to

uncover a portion of vulnerable devices, ﬁxing them might have implications. For

example, owners may not have enough knowledge to ﬁx the vulnerabilities [52].

17

Chapter 2. Survey on IoT Ecosystems

Europe

‘ETSI TS 103 645’ is a new cybersecurity standard for consumer Internet of Things

devices released by the European Telecommunications Standards Institute (ETSI)

in February 2019 [53]. It proposes 13 best practices to support manufacturers: no

default passwords; keeping software updated; manage vulnerability reports; securely

store security-sensitive data; communicate securely; minimize attack surfaces; en-

sure software integrity; protect personal data; be resilient to outages; make use of

telemetry data; allow users to delete personal data; make installation and mainte-

nance easy; and validate input data. It claims that these standards allow ﬂexibility

for innovation rather than being rigid rules.

Australia

Compared to other countries discussed earlier, Australia is still lagging in imposing

the legislation for the protection of IoT. In the past, the federal government has

proposed the idea of a rating logo for Internet-connected devices which is named as

“Cyber Kangaroo” [54] – assuring a basic level of quality for consumers.

However, it received criticism from the experts for various reasons. The resilience

to attack of the devices cannot be expressed by a static rating logo. The security

weaknesses of the devices may unveil over time, but the rating logo on the packaging

cannot reﬂect those changes. Also, the security rating may reﬂect diﬀerent meaning

on diﬀerent domains. For example, the consequences of an attack on a connected

car are diﬀerent from the breach on connected Barbie dolls – these cannot be rated

by a single rating scheme. The security labels also make a false impression to users

that these devices are always secure. Thus, users tend to negate the best practices,

such as changing default passwords / updating the devices immediately after the

patch available [55].

18

Chapter 2. Survey on IoT Ecosystems

2.4.4

Insurers

In spite of the precautions taken to secure the IoT, there is the possibility to still be

aﬀected by cyberattacks along the line. It may cause harm to users and aﬀect the

reputation of the manufacturers. To mitigate the impact and avoid bankruptcy, they

may invest in cyber-insurance. The increasing premium for these manufacturers who

are more likely to be vulnerable will also force them to bring security to the top of

their priority list. It is claimed that the global market size of Cyber Insurance is

estimated to grow from US$2.9 billion in 2019 to US$16.7 billion by 2024 [56].

2.5 Systematical Evaluation of Cybersecurity Threats

Emerging research work [20, 37, 44, 57–60] has focused on understanding and identi-

fying potential security and privacy threats for IoT. However, there is little research

into a systematic way for identifying security ﬂaws in existing and emerging IoT de-

vices. We believe our work is the ﬁrst to develop a systematic methodology for pro-

ﬁling the security posture of consumer IoT devices, which can lead to a security-star

rating that can inform consumers, regulators, and insurance bodies of the associated

risks.

With regards to this scenario, we develop a suite of security tests categorized

under four criteria: conﬁdentiality of data sent/received by the IoT device; integrity

and authentication of connections the IoT device establishes with other (local or

external) entities; the access control and availability of the IoT device to connection

requests; and the capability of the IoT device to participate in reﬂection attacks.

Next, we apply our automated security test suite to 20 IoT devices available in

the market today, chosen to cover a range of applications including home security

(cameras and motion sensor), health (weighing scale, blood-pressure monitor and

air-quality sensors), energy management (light-bulbs and power-switch), and en-

19

Chapter 2. Survey on IoT Ecosystems

tertainment (photo frame, printer and speaker). Finally, using the outputs of our

automated test suite, we assign a color-coded security score to each of the devices

under each of the four criteria, thereby giving an intuitive visual representation of

the device’s security posture.

2.5.1 Security Test Suite

In this section, we develop a suite of security tests to categorize threats that ex-

ploit security/privacy vulnerabilities in IoT devices under four dimensions namely

conﬁdentiality, integrity, access control and availability, and reﬂection.

Conﬁdentiality

Conﬁdentiality involves ensuring the exchanged data between endpoints cannot be

understood by unwanted snoopers. We evaluate the conﬁdentiality of exchanged

data using three measures, whether it is plaintext, encoded, or encrypted. We assess

all communication channels of a given IoT device – between: device and cloud server;

device and user App; user App and cloud server. We therefore wrote a Python script

that performs ARP spooﬁng inside the home network to intercept all traﬃc to/from

the IoT device as well as the user’s smartphone.

Encryption protocol: We use this test to determine the security protocol being

used for a particular communication channel. The security protocol is obtained

by checking the protocol ﬁeld of the packet capture on Wireshark to see if it is

identiﬁable.

Plaintext: After inspecting the protocol ﬁeld, we analyze the data ﬁeld (i.e.

payload) to check if it contains any human-readable text. This test determines

whether the data is in plaintext or not, but it does not diﬀerentiate between encoded

data and encrypted data as both are not human-readable.

20

Chapter 2. Survey on IoT Ecosystems

Entropy: Since the above tests cannot always evaluate the conﬁdentiality of

data, we use the entropy test to verify whether a certain communication is encrypted,

encoded or in plaintext. Entropy can not only be used to determine whether data

is encrypted, but also to assess the strength of encryption. The better the level of

encryption the higher the entropy as it will contain more information.

We wrote a Python script that is fed raw data from captured packets to compute

the Shannon entropy of the data one byte at a time (i.e. a value between 0 and

8) – we look at the data in bytes.

In order to have an accurate entropy value,

we use at least 100 KB worth of packets. Our entropy test veriﬁes whether the

data is encrypted in conjunction with the encryption protocol test and conﬁrms the

plaintext test. We note that the entropy test may fail to distinguish encrypted from

encoded communications specially when it is applied to traﬃc of compressed video

generated by cameras – video compression yields a high entropy value though it is

unencrypted.

Integrity

Integrity assessment ensures a given IoT device performs its intended functions with-

out any manipulation and no message to/from the device is modiﬁed without detec-

tion. We therefore test the following:

Replay attack: We feed captured packets sent from the user App to the IoT

device (using the technique mentioned in Conﬁdentiality) into our Python script

which will then replay them to the IoT device. The attack is successful if the device

performs a certain function speciﬁed in the packet. Furthermore, if packets are

in plaintext (or encoded), we modify certain ﬁelds inside the packets and replay

them to check whether the device responds to tampered packets. Replay attacks

are launched on a third of IoT devices which use plaintext or encoded packets only.

We note that other devices which communicate encrypted traﬃc using TLS/SSL are

protected against replay attacks..

21

Chapter 2. Survey on IoT Ecosystems

DNS security: We also test whether the device attempts to connect to an ille-

gitimate server. Inspecting the DNS queries and responses, we assess whether devices

uses DNSSEC. We note that DNSSEC is only oﬀered by authoritative servers, not

recursive resolvers. Authoritative servers contacted by IoT devices are managed by

their respective manufacturers.

In order to determine whether a device validates

DNSSEC certiﬁcate records or not, we spoofed the response of DNS queries made

by that device. Accepting spoofed responses by the device and attempting to con-

nect to the illegitimate server indicate that the device does not validate DNSSEC

records.

If the device is vulnerable to DNS spooﬁng, we use a python script to perform

DNS spooﬁng redirecting traﬃc to a fake server.

If the device attempts to con-

nect to this fake server, the system integrity is violated.

In addition, if it sends

information to the fake server it indicates the device does not conduct any form of

authentication.

Access control and availability

We consider the access control and availability of an IoT device to identify how easily

an attacker can gain access/control to/of the device and determine whether it is sus-

ceptible to a denial of service (DoS) attack. We start our test by scanning for ports

that are open on the device using command nmap -sS -sU -p 0- 65535 [deviceIP].

We then attempt to gain access via Telnet, SSH and HTTP using a list of known

weak login credentials – these ports were exploited recently by the Mirai botnet that

resulted in one of the largest DDoS attacks from IoTs over the Internet [61].

Denial of Service: We assess the ease of launching a DoS attack using the

following experiment. We determine how much incoming traﬃc the IoT device can

handle before it completely loses its expected functionality. We ﬂood the device with

ICMP ping requests as well as UDP packets, and determine the amount of data that

22

Chapter 2. Survey on IoT Ecosystems

is required to stop the operation of the IoT. We conduct these two tests using the

hping3 tool by issuing the command: hping3 -d 1000 -1 (1 for ICMP and 2

for UDP) -p (port) (deviceIP). We also use another python script to measure

the maximum number of concurrent TCP connections the device can handle before

it crashes – by ﬂooding the device with TCP SYN packets to initiate connections to

the list of open ports on the device.

Reﬂection attacks

Following the public announcement of the large DDoS attack fueled by IoTs in

2016 [61] many manufacturers have consequently closed their remote access ports,

or strengthened their default login credentials. We have shown that IoT devices

can still be employed to launch DDoS attacks by exploiting various protocols using

source-spoofed traﬃc [44]. Evaluating the reﬂection capability of device protocols is

important since IoT devices are increasingly contributing to DDoS attacks to popular

services providers across the Internet [62–65]. We experiment the reﬂection attacks

on three standard protocols namely ICMP, SSDP and SNMP. We write a python

script that crafts malformed packets (with spoofed source IP address) and sends;

(a) ICMP messages, (b) SSDP broadcasts, and (c) SNMP requests to a given IoT

device. For the SNMP, we further check if the device supports the SNMP public

community string that can potentially generate a larger volume of responses.

If

successful, we issue a getBulk SNMP request that sends multiple getNext requests

at once. Responding to each of these protocols reveals that the device can be used

to launch a reﬂection attack.

2.5.2 Security Posture of IoTs

We now validate our assessment methodology by applying it to 20 IoT devices that

have been recently introduced to the consumer market, ranging from cameras and

23

Chapter 2. Survey on IoT Ecosystems

Table 2.1: Posture of conﬁdentiality

Device to Server

Device to User-app

User-App to Server

Devices

Plaintext Protocol Entropy Plaintext Protocol Entropy Plaintext Protocol Entropy

Hue bulb

No

TLSv1.2

Belkin switch

Partially

Unknown

Samsung cam

Belkin cam

Awair air quality

HP printer

LiFX bulb

Canary cam

TPlink plug

Amazon Echo

SmartThings

Pixstar photo

TPlink cam

Belkin motion

NEST smoke

Netatmo cam

Dlink cam

Hello Barbie

Withings sleep

Dropcam

No

No

No

No

No

No

No

No

No

No

No

Yes

No

No

No

Unknown

Unknown

SSL

TLSv1.2

Unknown

TLSv1.2

TLSv1.2

TLSv1.2

Unknown

Unknown

IPsec

None

TLSv1.2

Unknown

TLSv1.2

7.70

7.74

7.99

7.06

7.89

7.96

7.95

7.98

7.69

7.87

7.97

7.25

8.00

5.40

7.99

7.84

7.99

Yes

Yes

None

None

5.48

5.16

No

SSL

7.95

Yes

No

None

Unknown

5.38

4.66

No

Unknown

5.33

No

No

No

No

No

No

No

No

Unknown

SSL

SSL

SSL

TLSv1.2

SSL

TLSv1.2

TLSv1.2

7.91

7.48

7.90

7.64

7.46

7.63

7.91

7.80

Yes

Yes

None

None

7.51

5.16

No

TLSv1.2

7.73

Partially

HTTP

7.97

No

No

No

No

TLSv1.2

TLSv1.2

7.54

7.98

TLSv1.2

TLSv1.2

7.63

7.94

lightbulbs to power switches and health monitoring devices. We verify our method-

ology on some devices with known security ﬂaws [57] and also evaluate the security

and privacy posture of other IoT devices with security vulnerabilities that are un-

known to us.

Conﬁdentiality

Our conﬁdentiality assessment results are shown in Table 2.1 by three measures over

three communication channels (as discussed in §2.5.1).

It can be seen that most

of the devices have fairly secure communication in two channels namely device-

to-server and mobile-app-to-server since they use secure protocols like TLS/SSL

most of the time. However, a majority of the vulnerabilities arise when the device

communicates with the mobile-App – ﬁve devices send in plaintext, only one device

uses SSL with fairly lower entropy values. Note that for some devices (e.g. Belkin

24

Chapter 2. Survey on IoT Ecosystems

Figure 2.2: TPLink camera POST message

Switch, Samsung Smart Cam), the security protocol is not identiﬁed but together

with plaintext and entropy tests, we can evaluate the conﬁdentiality of a given

channel. Considering the user privacy, we see quite a few devices such as Phillips

Hue lightbulb, Belkin power switch, HP Envy printer, TPLink camera, and Belkin

motion sensor, communicate in plaintext (some of them were discussed in [20]),

– revealing private information, for example, whether the Belkin power switch is

on/oﬀ, or when the Phillips Hue lightbulb was last used.

Our results also enable us to discover new vulnerabilities in some devices such as

the TPLink camera. Fig. 2.2, which depicts a detailed insight into packets captured

from the TPLink camera (i.e. a POST request packet payload in red text followed

by the HTTP response packet in blue text). The video/audio stream is sent in plain-

text (the video/audio header is human-readable even though its data doesn’t seem

human-readable). This data can be sniﬀed by an attacker and then used to reassem-

25

video/audio	headerHTTP	payloadvideo/audio	dataBase64	encodedChapter 2. Survey on IoT Ecosystems

Figure 2.3: Control bit pattern of LiFX lightbulb

ble the video/audio data. Surprisingly, it reveals not only the video/audio data but

also the authentication password required for logging in to the device. This pass-

word is exposed in the basic authentication ﬁeld of the packet shown in Fig. 2.2 (i.e.

YWRtaW46WvdSdGFND0=) – this is a Base64 version of “admin”. Given the password,

we are able to log into the device by simply guessing the user-name as “admin” which

is a common default credential used in many IoT devices.

The eﬃcacy of our entropy measure can be seen in the LiFX lighbulb. Our

plaintext test for this device shows that the LiFX bulb is not communicating in

a human-readable format, whereas its traﬃc data has a low entropy value of 4.56.

When taking a closer look into the LiFX packets, we are able to discover that

packets associated with certain commands (from the user App) are identical and

certain bits represent speciﬁc functions of the device, meaning that the data is just

encoded as shown in Fig. 2.3. Similarly in the TPLink power switch, we see that

the data is not in plaintext but the entropy value is 5.33, suggesting that it could

possibly be encoded or poorly encrypted. By guessing that the data is sent in JSON

format (i.e. {data}), we attempt to XOR the ﬁrst byte with the character “{” to

obtain the single byte key. We then apply the key to the encrypted message and are

able to extract the message in plaintext. This indicates a weak encryption is used

in the TPLink power switch. Note that some devices employ stronger encryption

protocols. For example, Amazon Echo uses TLSv1.2 for all traﬃc it communicates

(shown in Fig. 2.4), or Netamo camera implements IPsec, protecting the IP address

of endpoints from potential attackers (shown in Fig. 2.5).

26

Data	SizeHeaderChangeColorColourvalueSaturationBrightSlight	color	changeTime	delay	of	color	delayChapter 2. Survey on IoT Ecosystems

Figure 2.4: Wireshark capture of Amazon Echo

Lastly, we evaluate the conﬁdentiality of devices’ communication after their ini-

tial setup phase is complete. There are, however, some devices that communicate

in an insecure manner when they initially pair with the user App. For example,

Fig. 2.6 shows that Belkin camera exposes the password of the local WiFi network

in plaintext (i.e. ThisIsMyWiFiPassword in Fig. 2.6) when responding to a GET

request.

Integrity and Authentication

Our assessment results for the posture of integrity and authentication in twenty IoT

devices are shown in Table 2.2. Considering the test for replay attacks, ﬁve of our

IoT devices are susceptible such as the Philips Hue light bulb, Belkin power switch,

HP Envy printer, LiFX light bulb, and TPLink switch. Some of these exploits

have been already reported. For example, the Belkin switch was evaluated to be

insecure against replay attacks due to the lack of authentication [20] or the LiFX

lightbulb that communicates encoded messages with the user App [66]. An attacker

Figure 2.5: Wireshark capture of Netatmo camera

27

Chapter 2. Survey on IoT Ecosystems

Figure 2.6: Belkin camera is pairing with user app

can turn on/oﬀ the Belkin switch with a well-crafted fresh packet, or change the

color/brightness of the LiFX bulb using the control bit pattern shown in Fig. 2.3.

On the other hand, those IoT devices that employ secure protocols (e.g. SSL) are

protected against replay attacks such as the Awair air monitor and Amazon Echo.

Our DNS security test results show that none of 20 IoT devices implements

DNSSEC protocol that is primarily designed to prevent DNS spooﬁng attacks. This

vulnerability enables attackers to hijack the DNS query and possibly impersonate

the legitimate server to the IoT device. Even if DNS spooﬁng is successful, the victim

IoT device may protect itself by some form of authentication. According to the last

column of Table 2.2, some devices such as the Phillips Hue lightbulb and LiFX bulb

do communicate with the fake server, after a successful DNS spooﬁng. The Phillips

Hue lightbulb sends an HTTP message to the fake server that is listening on the

same port as the real server, while the LiFX bulb sends data to our fake server which

appears to be in its own unique data format (as shown in Fig. 2.3).

28

Chapter 2. Survey on IoT Ecosystems

Table 2.2: Posture of integrity and authentication

Devices

Replay Attack DNS spooﬁng Fake Server

Hue bulb

Belkin switch

Samsung cam

Belkin cam

Awair air quality

HP printer

LiFX bulb

Canary cam

TPlink plug

Amazon Echo

SmartThings

Pixstar photo

TPlink cam

Belkin motion

NEST smoke

Netatmo cam

Dlink cam

Hello Barbie

Withings sleep

Dropcam

Yes

Yes

No

No

No

Yes

Yes

No

Yes

No

No

No

No

Yes

No

No

Yes

No

No

No

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

HTTP

Fail SSL

Fail SSL

Fail SSL

Fail SSL

Fail SSL

Plaintext

Fail SSL

Fail SSL

Fail SSL

Fail SSL

Fail SSL

Fail SSL

Plaintext

Fail SSL

Fail Ipsec

Plaintext

Fail SSL

Fail SSL

Fail SSL

Access Control and Availability

Our access control and availability evaluation shown in Table 2.3 assesses the state

of ports as: “open” indicating that a service is actively accepting TCP connections

and/or UDP datagrams, “closed” indicating that the port receives and responds to

probe packets, but there is no service listening on it, and “ﬁltered” indicating a port

scanner cannot determine whether the port is open or closed (a ﬁltering method

prevents probes to reach the port). The results shown in Table 2.3 indicate that

almost all devices have some form of vulnerabilities in terms of open ports which

enable intruders to communicate with or access into the device. For example, the

Belkin smart camera exposes a large number of ports, 5 TCP and 31 UDP. Another

vulnerable device is the HP printer with 9 open TCP ports and 10 open UDP ports.

Among all these open ports, we note that the HP printer responds on a special TCP

port 9100 that is used for printing with no authorization – this vulnerability was

29

Chapter 2. Survey on IoT Ecosystems

Table 2.3: Posture of access control and availability

Vulnerable
Ports
80

Weak
Passwords
No

ICMP
DoS
Protected

UDP
DoS
Protected

No. of
TCP Con.
112

None

23Mbps

6.3Mbps

97

Devices

Open Ports
(TCP)

Open Ports
(UDP)

Hue bulb

80, 8080

1900, 5353

Belkin switch

53, 49155

Samsung cam

Belkin cam

80, 443, 554,
943,
4520,
49152

80, 81, 443,
9964, 49153

53, 1900, 3111, 7638, 13965,
14675, 17143, 19422, 22894,
23835, 26011, 27047, 38849,
40014, 41970, 42518, 43403,
47836, 53121, 53330, 55353,
65484

161, 5353

1900, 10000, 13105, 19827,
26854, 28971, 32596, 32435,
33435, 35042, 35316, 35056,
36500, 36943, 38587, 38606,
39632, 39714, 43588, 43834,
47709, 48190, 44179, 49156,
49201, 49360, 52042, 52144,
52603, 55254, 56284

Awair air quality Filtered

Filtered

HP printer

80, 443, 631,
3911,
3910,
8080,
9100,
9220, 53048

137, 161, 543, 3702, 5353,
5355, 7235, 53592, 56693,
56723

80, All
ports allow
telnet

80

None

23

80

None

LiFX bulb

Canary cam

Closed

Closed

TPlink plug

80, 9999

Amazon Echo

4070

Filtered

Closed

1040

5353

SmartThings

23, 39500

Filtered

Pixstar photo

Closed

137

TPlink cam

80, 554, 1935,
2020, 8080

Belkin motion

53, 49152

NEST smoke

Closed
ﬁltered

and

Netatmo cam

80, 5555

1068, 3702, 5353, 42941

53, 1900, 3080, 3081, 3082,
3179, 3229, 3236, 3619,
4050, 4052, 4053, 4054,
4055, 4289, 4996, 4997,
4998, 14675

17395, 17466, 17471, 18184,
18234, 18455, 18721, 18916,
19090, 19112, 19217, 19458,
19581

654, 7242, 26082, 29110,
31574, 35826, 39408, 46721,
48080, 56943

80

80

90Mbps

4.1Mbps

17

7.7Mbps

74Kbps

256

No

No

No

No

No

36Mbps

7.2Mbps

6Mbps

82Kbps

6.4Mbps

5.5Mbps

25Mbps

Protected

9.2Mbps

130Mbps

8.8Mbps

Protected

Protected

Yes

48Mbps

870Kbps

11.3Mbps

350Kbps

Protected

Protected

1

15

258

1

130

109

80

No

8.2 Mbps

45Kbps

256

Dlink cam

21, 23, 5001,
5004, 16119

1900, 5002, 5003, 10000

5004

No
Password

49Mbps

292Kbps

20

Hello Barbie

Closed

Closed

Withings sleep

22, 7685, 7888

5353

22

No

Protected

Protected

22

10Mbps

Dropcam

Closed

Closed of ﬁltered

4Mbps

recently exploited to attack more than 150000 printers [67]. On the other hand, a

device like the Awair air monitor has all ports closed, and hence is protected against

common attacks such as SYN ﬂooding.

We note that some IoT devices allow remote access via SSH (port 22), Telnet

(port 23), or HTTP(port 80). Until recently, many IoT devices had weak credentials

30

Chapter 2. Survey on IoT Ecosystems

(from a list of about 60 common defaults) that Mirai malware [25] exploited to hijack

hundreds of thousands of IoTs, launching a major DDoS attack on the Internet.

None of these 60 defaults were valid when we used them for our 20 IoT devices.

Surprisingly, we have two devices with no protection for remote access: HP printer

allows Telnet without asking for a password; and the DLink camera asks for no

credentials during SSH access – some manufacturers seemingly open remote access

ports for testing/debugging purposes.

From the DoS attack test results shown in Table 2.3, it can be seen that most

devices are susceptible to at least one form of DoS attacks, either of ICMP-, UDP-

or TCP-based. We note that the required traﬃc rate to cause a device to stop func-

tioning is not signiﬁcant in many cases especially when UDP is used (i.e. less than

1 Mbps for Belkin SmartCam, LiFX lightbulb or TPLink camera). For Samsung

Smart camera, it can handle ICMP traﬃc rate up to 90 Mbps, however it stops

functioning (the camera will not be able to transmit live video stream to the user

App), if it is bombarded by UDP-based traﬃc at a rate more than 4.1 Mbps.

Reﬂection Attacks

Lastly, we consider ICMP, SSDP and SNMP protocols by checking if a given device

reﬂects traﬃc of these types. Our results are shown in Table 2.4. We can see that

all devices, except the LiFX lightbulb, are reﬂecting ICMP traﬃc. We then test

the SSDP protocol which is commonly enabled in many IoT devices for ease of

discovery. When we use SSDP, the reﬂected traﬃc (i.e. response) is ampliﬁed by

a large factor since it contains service and presence information of the IoT device

– this makes it an attractive protocol for DDoS attackers. We observe that ﬁve

of our devices are vulnerable to SSDP reﬂection attacks – the rest of them do not

use SSDP for discovery. Lastly, we examine SNMP protocol which is not widely

used by IoT devices. Furthermore, with SNMP v2c (and v3), it is possible to use

public community strings such that the ampliﬁcation factor is signiﬁcantly high.

31

Chapter 2. Survey on IoT Ecosystems

Table 2.4: Posture of reﬂection attacks

Devices

ICMP Reﬂection SSDP Reﬂection SNMP Reﬂection

Hue bulb

Belkin switch

Samsung cam

Belkin cam

Awair air quality

HP printer

LiFX bulb

Canary cam

TPlink plug

Amazon Echo

SmartThings

Pixstar photo

TPlink cam

Belkin motion

NEST smoke

Netatmo cam

Dlink cam

Hello Barbie

Withings sleep

Dropcam

Yes

Yes

Yes

Yes

Yes

Yes

No

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

Yes

No

Yes

No

No

No

No

No

No

No

No

No

Yes

No

No

Yes

No

No

No

No

No

v2c

No

No

v1

No

No

No

No

No

No

No

No

No

No

No

No

No

No

The SNMP v2c is only available in the Samsung Smart camera. Sending a getBulk

request to the camera, it will iterate the getNext request multiple times, and hence

a larger amount of traﬃc is generated.

2.5.3 Security Rating of IoT Devices

Without doubt, hundreds of consumer IoT devices are going to emerge in the years

ahead, and their security/privacy vulnerabilities are going to be diverse. Our results

from evaluation of the twenty devices highlight the security posture of consumer

IoTs, and reveal the problems that users have to deal with.

In this section we

discuss how our methodology can be used for a security ratings system that is

beneﬁcial to consumers or insurance companies. We propose a three-level rating:

“A” being secure, “B” being moderately secure/insecure, and “C” being insecure.

Table 2.5 shows our attempt to rate each of IoT devices that we assessed their

32

Chapter 2. Survey on IoT Ecosystems

Table 2.5: Security rating

Conﬁdentiality

Integrity and
Authentication

Access Control

Reﬂection Attacks

Device
to
Server

Device
to
Application

Application
to
Server

t
x
e
t
n
i
a
l
P

A

B

A

A

A

A

A

A

A

A

A

A

A

A

A

A

C

A

A

A

l
o
c
o
t
o
r
P

A

A

A

A

A

A

A

A

A

A

C

A

A

y
p
o
r
t
n
E

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

C

A

A

A

t
x
e
t
n
i
a
l
P

C

C

A

A

A

C

A

A

A

A

A

A

C

C

A

B

A

A

A

A

l
o
c
o
t
o
r
P

y
p
o
r
t
n
E

C

C

A

A

A

C

A

A

A

A

C

C

A

C

A

A

A

A

C

C

A

A

A

C

C

A

C

A

A

A

A

C

A

A

A

A

A

A

t
x
e
t
n
i
a
l
P

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

l
o
c
o
t
o
r
P

y
p
o
r
t
n
E

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

All

y
c
a
v
i
r
P

C

C

A

A

A

C

A

A

A

A

A

A

C

C

A

A

A

A

A

A

y
a
l
p
e
R

k
c
a
t
t
A

g
n
ﬁ
o
o
p
S

S
N
D

r
e
v
r
e
S

e
k
a
F

s
t
r
o
P
n
e
p
O

)
P
C
T
(

s
t
r
o
P
n
e
p
O

)
P
D
U
(

C

C

A

A

A

C

C

A

C

A

A

A

A

A

A

A

A

A

A

A

C

C

C

C

C

C

C

C

C

C

C

C

C

C

C

C

C

C

C

A

A

A

A

A

C

A

A

A

A

A

A

A

A

A

A

A

C

C

C

C

B

C

A

A

C

C

C

A

C

C

B

C

C

A

C

A

C

C

C

C

B

C

B

A

C

C

B

C

C

C

C

C

C

A

C

B

s
t
r
o
P
e
l
b
a
r
e
n
l
u
V

C

A

C

C

A

C

A

A

C

A

C

A

C

A

A

C

C

A

C

A

s
d
r
o
w
s
s
a
P

k
a
e

W

A

A

A

A

A

A

A

A

A

A

A

A

C

A

A

A

C

A

A

A

S
o
D
P
M
C
I

B

C

C

C

C

A

C

C

C

B

C

C

C

C

C

C

S
o
D
P
D
U

C

C

C

B

C

A

B

A

C

C

C

B

B

B

B

A

C

A

P
C
T

f
o

.
o
N

s
n
o
i
t
c
e
n
n
o
C

n
o
i
t
c
e
ﬂ
e
R

P
M
C
I

n
o
i
t
c
e
ﬂ
e
R

P
D
S
S

n
o
i
t
c
e
ﬂ
e
R

P
M
N
S

g
n
i
r
t
S

y
t
i
n
u
m
m
o
C

c
i
l
b
u
P
P
M
N
S

C

C

C

C

A

C

A

A

C

C

C

A

C

C

A

C

C

A

C

A

C

C

C

C

C

C

A

C

C

C

C

C

C

C

C

C

C

C

C

C

C

C

A

C

A

A

A

A

A

A

A

A

A

C

A

A

C

A

A

A

A

A

C

A

A

C

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

C

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

A

Devices

Phillip Hue lightbulb

Belkin Switch

Samsung Smart Cam

Belkin Smart Cam

Awair air monitor

HP Envy Printer

LiFX lightbulb

Canary Camera

TPLink Switch

Amazon Echo

Samsung Smart
Things

Pixstar Photo Frame

TPLink Camera

Belkin Motion Sensor

Nest Smoke Alarm

Netamo Camera

Dlink Camera

Hello Barbie
Companion

Whithings Sleep
Monitor

Nest Drop Camera

security posture on the four dimensions – all ratings in this table are subjective

and given based on authors perceptions. One may consolidate our table by giving

weights to each dimension in the future.

We use color codes for ease of visualization, green for A rating, yellow for B

rating, and red for C rating. We also use gray color for cells where the data is not

available. For example, the encryption protocol of Belkin switch is not identiﬁed on

Wireshark for the device-to-server communication; DNS query is not performed in

Belkin motion sensor; normal functionality of the Pixtar photo frame is not aﬀected

by a DoS attack. Using our color-coded ratings table, consumers are able to quickly

visualize the security posture of individual devices. All devices display some form

of vulnerability in either of integrity, access control and reﬂection dimensions –

this raises concerns for consumers as well as for the Internet ecosystem in general.

Devices such as the Amazon Echo, Hello Barbie, Nest Dropcam, Whitings Sleep

monitor seem relatively secure by the measure of conﬁdentiality. Amazon Echo in

particular is a top-rated device in security with encrypted communication channels

and having almost all of its ports closed. On the other hand, devices such as Phillips

Hue lightbulb and the Belkin switch seem fairly poor in security. The Phillips Hue

33

Chapter 2. Survey on IoT Ecosystems

in particular communicates in plaintext to the user App, is susceptible to replay

attacks, has many open ports and can be used to launch various reﬂection attacks

to victim servers.

We recognize that security is but one concern amongst many that manufacturers

of IoT devices are dealing with. The surge in demand for IoT is leading many

manufacturers to rush to market with their product, and increasing user appeal to

gain market traction can become more paramount than ensuring fool–proof security.

No matter how it evolves, consumers would eventually demand for a rating system

(much like the energy rating system given to home appliances) that needs to be

developed by standard bodies and tracked by regulation entities. This would protect

consumers rights and incentivize manufacturers to improve the security of their

device to receive an acceptable rating that can lead to a good share of the market.

2.6 Existing IoT Security Solutions

Security and monitoring solutions for traditional IT network have been extensively

studied [68–70] in the past few decades by the research community. Those studies

include both the Host-based Intrusion Detection System (HIDS) and Network-based

Intrusion Detection System (NIDS).

Anti-virus in computers is a good example of the implementation of HIDS. It

monitors the activities of the devices based on the system log ﬁles as well as network

packets on its interfaces [71]. The host-based security systems are unlikely to be

embedded in IoT devices due to resource constraints and are not resilient enough

to maintain security standards for the long term since automatically applying the

security patches is hard.

NIDS solution protects all the devices in a network to complement device vendor

security implementation [72] by analyzing the activities of devices from the network

34

Chapter 2. Survey on IoT Ecosystems

traﬃc. NIDS monitor the activity of the devices based on the techniques such as

signature-based detection, speciﬁcation-based detection, and anomaly-based detec-

tion [73]. The advantages of NIDS over HIDS are: 1) it can be implemented using

a centralized controller and hosted in the cloud environment rather than using the

device resources; and 2) upgrading the system is easy and protection of all devices

can be handed over to security experts rather than expecting all the users to be

tech-savvy.

2.6.1 Signature-based Intrusion Detection

The signature-based attack identiﬁcation system is the commonly used technique

in the present NIDS implementations such as Bro [74], Snort [75], and commercial

hardware. They compare the traﬃc with already known attack signatures collected

from the sandbox environment and honeypots. However, the signature-based meth-

ods do not show good performance with zero-day (attacks that have never seen

before) vulnerabilities. The main implication of signature-based attack detection is

that it is not scalable in the IoT domain due to the heterogeneity of IoTs. Generat-

ing the attack signature for a growing number of IoT types is not a feasible solution,

whereas, in a traditional network, most of the devices run on similar platforms (e.g.,

Windows, Unix, Linux, Android).

2.6.2 Speciﬁcation-based Intrusion Detection

The speciﬁcation-based intrusion detection mechanism is monitoring the device

based on the rules (i.e., speciﬁcation) that deﬁne the allowed or malicious net-

work activities [69, 76]. The speciﬁcation-based detection has the ability to act as

both: 1) learn the attack characteristics and identify the attacks that follow those

speciﬁcation; or 2) learn the benign behavior of the device and detect the variations

when a traﬃc ﬂow overrule the speciﬁcation [77–80].

35

Chapter 2. Survey on IoT Ecosystems

In [81], Amaral et al. propose a speciﬁcation-based approach for a wireless

sensor network and expect the network operators to generate the speciﬁcation by

themselves. In [82] Nguyen et al. develop a protection system called “IoTSAN” which

allows the users to deﬁne speciﬁcation by semantic rules. However, generating the

speciﬁcations for every device is a tedious task. Internet Engineering Task Force

(IETF) recently released a standard called Manufacturer Usage Description (MUD)

to outline the network activities of the devices intended by the manufacturers [83].

The work in [84] proposes an IDS by automatically proﬁling each device using MUD

at the initial stage and then identiﬁes the attacks that violate the proﬁles. This

work has been extended in [85] to identify the volumetric anomalies along with the

speciﬁcation violations.

Although the speciﬁcation based IDS make better accuracy in identifying attacks

and policy violations, generating speciﬁcations for the proliferation of IoT is hard,

and the standards like MUD have not yet been adopted by manufacturers.

2.6.3 Anomaly-based Instruction Detection

The anomaly detection technique is to learn legitimate behavior from the normal

network traﬃc and identify the variations from it. Since anomaly detection just

inspects deviations from the benign traﬃc rather than the attack signatures, it has

the capability to identify the zero-day attacks as well.

Although there is an extensive body of literature [70, 86–90] in anomaly detec-

tion based instruction systems, it has achieved only a very limited amount of success

rate [91] in the traditional IT network. The reasons are manifold [92]: 1)legitimate

traﬃc shows high variability in IT networks; 2) diﬃcult to get the ground truth in

the training dataset; 3) very limited public datasets to learn the normal network

behaviors; 4) both wrongly identifying a legitimate traﬃc as illegitimate, and illegit-

imate traﬃc as legitimate incur a high cost; and 5) practical challenges in evaluating

the system.

36

Chapter 2. Survey on IoT Ecosystems

However, in the domain of IoT, these methods give some promises since the

activities of IoT devices is less complicated than servers or computers and due to

their limited functionality and following similar patterns, it is easy to characterize

the whole behavior of the device [93] from the network traﬃc.

2.7 IoT Behavioral Monitoring

Nowadays network operators lack real-time visibility into connected devices – over

40% of today’s endpoints are unknown and unmanaged by the organizations which

lead to signiﬁcant infrastructure blind spots, unauthorized access, and data leaks [18].

Based on the fact that IoT devices exhibit limited traﬃc patterns, we believe it is pos-

sible to identify and characterize their network behavior [94]. It enables the operator

to: 1) manage the assets connected in the network [5]; 2) enforce the device-speciﬁc

policies [95]; and 3) locate the vulnerable and blacklisted devices eﬀortlessly [96].

2.7.1

IoT Traﬃc Characterization

Signiﬁcant research work is carried out in the existing literature to characterize the

general Internet traﬃc [97–100]. These prior works largely focus on application

detection (e.g. Web browsing, Video streaming, Gaming, Mail, Skype VoIP, Peer-

to-Peer, etc.). However, studies focusing on characterizing IoT traﬃc (also referred

to as machine-to-machine or M2M traﬃc) are still in their infancy.

Analysis of Empirical Traces

The work in [101] is one of the ﬁrst large-scale studies to delve into the nature

of M2M traﬃc.

It is motivated by the need to understand whether M2M traﬃc

imposes new challenges for the design and management of cellular networks. The

work uses a traﬃc trace spanning one week from a tier-1 cellular network operator

37

Chapter 2. Survey on IoT Ecosystems

and compares M2M traﬃc with traditional smartphone traﬃc from a number of

diﬀerent perspectives – temporal variations, mobility, network performance, and

so on. They conclude that M2M traﬃc is substantially diﬀerent from smartphone

traﬃc as it tends to exhibit higher uplink to downlink traﬃc volume, varying diurnal

patterns, and larger round-trip times. The study informs network operators to be

cognizant of these factors when managing their networks.

In [102], Nikaein et al. note that the amount of traﬃc generated by a single

M2M device is likely to be small, but the total traﬃc generated by hundreds or

thousands of M2M devices would be substantial. These observations are to some

extent corroborated by [103, 104], which note that a remote patient monitoring

application is expected to generate about 0.35 MB per day and smart meters roughly

0.07 MB per day.

Aggregated Traﬃc Model

A Coupled Markov Modulated Poisson Processes (CMMPP) framework to capture

the behavior of a single machine-type communication, as well as the collective be-

havior of tens of thousands of M2M devices, is proposed in [105]. The complexity of

the CMMPP framework is shown to grow linearly with the number of M2M devices,

rendering it eﬀective for large-scale synthesis of M2M traﬃc.

In [106], Markus et al.

show that it is possible to split the (traﬃc) state of

an M2M device into three generic categories, namely periodic update, event-driven,

and payload exchange, and a number of modelling strategies that use these states

are developed. An illustration of model ﬁtting is shown via a use-case in ﬂeet

management comprising 1000 trucks run by a transportation company. The ﬁtting is

based on measured M2M traﬃc from a 2G/3G network. A simple model to estimate

the volume of M2M traﬃc generated in a wireless sensor network-enabled connected

home is constructed in [107]. Since the behavior of sensors is very application-

38

Chapter 2. Survey on IoT Ecosystems

speciﬁc, the work identiﬁes certain common communication patterns that can be

attributed to any sensor device.

Although all the above studies do fundamental studies in the IoT traﬃc charac-

terizations, they do not undertake a ﬁne-grained characterization in consumer IoT

devices. On this scenario, we present insights into the underlying network traf-

ﬁc characteristics using statistical attributes such as activity cycles, port numbers,

signalling patterns, and cipher suites in [5] and [6] which is discussed in Chapter 3.

2.7.2

IoT Finger-Printing and Classiﬁcation

Traﬃc ﬁngerprinting and classiﬁcation is widely used for various applications such

as network management [108, 109], QoS [110, 111], and cyber-security [112–115].

However, IoT traﬃc ﬁngerprinting and classiﬁcation methods are still in its early

stages [116]. The recent attention on IoT security and asset management has at-

tracted researchers to observe IoT devices using both active and passive ﬁngerprint-

ing techniques.

The active ﬁngerprinting comprise various techniques – identifying vendor from

OUI preﬁx of the MAC Address, device name from the host-name ﬁeld of DHCP ne-

gotiation, services oﬀered by the device using discovery protocols or actively parsing

service banners and estimating the device types by probing the ports [4]. Shodan [22]

is one of the examples that actively scan and classify (mainly by parsing service ban-

ners) the IoT devices. The main disadvantage of the active scanning approach is

they greatly impact the network and degrade the performance of connected devices.

Although the passive ﬁngerprinting techniques are not simple and straight for-

ward as active, they tend to provide a rich set of information about the devices and

their states without generating additional congestions. These passive ﬁngerprints

can be recorded by monitoring the network traﬃc (i.e., passive network teleme-

39

try) using the middle-boxes (e.g., switches) or mirroring the traﬃc to a dedicated

Chapter 2. Survey on IoT Ecosystems

inspection engine.

Passive Network Telemetry

Network traﬃc measurement has been a subject of interest to academia and indus-

try. Many diﬀerent methods have been proposed and practically used ranging from

traditional port-based counting using SNMP [117], WiFi packet sniﬃng [118–120]

and packet sampling [121] to ﬂow-based telemetry [122, 123].

Traditional ﬁngerprinting methods like sniﬃng wireless (e.g., WiFi, Zigbee, Blue-

tooth) packets require special hardware [124]. Although, these methods may reveal

valuable information in attack scenarios, they provide network operators with very

limited insights into the operation of their network.

Modern telemetry methods can be categorized into: (a) packet-based [121, 125,

126]; and (b) ﬂow-based [122, 123, 127]. sFlow [121] is one of the commonly used

methods that randomly samples (i.e., one in N) packets from the network switches.

Due to its random sampling, sFlow tends to collect packets from elephant ﬂows

(those that carry heavy traﬃc and are long in duration), and hence mice ﬂows are

likely to get missed which results in an inaccurate measurement. To address this

issue, Everﬂow [125] proposes to collect speciﬁc packets (e.g., TCP SYN, FIN, and

RST) using the match and mirror functionality of data-center switches. Planck [126]

estimates the throughput of ﬂows at very tight time-scales by mirroring traﬃc of

multiple ports to a monitoring port at which a collector performs high-rate sampling.

Overall, packet-level telemetry can only provide partial visibility into network traﬃc

ﬂows.

Commercial switches equipped with NetFlow [122] (used in Chapter 3) engines

export ﬂow records. However, there are two limitations: (a) they only export a

ﬂow record once it expires (not in real-time); and (b) updating and maintaining

40

Chapter 2. Survey on IoT Ecosystems

ﬂow records result in computational cost [128]. FlowRadar [123] overcomes the

limitations of Netﬂow by incorporating an encoded hash table (data structure for

ﬂow counters) with low memory overheads and exporting ﬂows periodically (e.g., 10

ms). However, FlowRadar is still not supported by commercial switches available on

the market. SDN APIs [127] which is commonly available on OpenFlow supported

switches, enable us to measure traﬃc ﬂows at low cost with reasonable resolutions.

Thus, we propose ﬂow-level telemetry using SDN APIs as an eﬀective solution in

Chapter 4.

Classiﬁcation and Anomaly detection

IoT traﬃc classiﬁcation has been the subject of recent researches for a variety of

purposes: 1) recognizing IoT from mixture of IoT and non-IoT devices [6, 129]; 2)

classifying the type of IoT device [130, 131]; 3) identifying the operating states [119,

132]; and 4) detecting the abnormal behaviors [93, 133] of the IoT traﬃc.

Attribute selection: The proposed classiﬁcation mechanisms in the literature

rely on a wide range of attributes from as simple as, a set of IP addresses (of servers)

that each device communicates [131], to sophisticated as the entropy of payloads

exchanged by the devices [134]. However, these attributes come in various extraction

cost and diﬀerent level of impact on the classiﬁcation process (e.g., Although the

set of IP addresses is a low-cost attribute, it is not much reliable since the IP

address belongs to an elastic IPv4 address allocation – employed for dynamic cloud

computing like Amazon AWS). Thus, the trade-oﬀ between the cost and robustness

of attributes plays a vital role in the attribute selection.

Work in [130] develops a classiﬁcation system called “IoT Sentinel” to recognize

and identify the IoT device types immediately after it connected to the network. It

employs a single attribute vector with the elements of 276 (i.e., ﬁrst 12 packets with

23 attributes for each). The proposed 23 attributes including 16 binary attributes

41

Chapter 2. Survey on IoT Ecosystems

(indicating the use of various protocols at application, transport, network, and link

layers) along with IP layer header options, remote IP address/port numbers, and

size and raw byte value of packets. In order to minimize the attribute extraction

cost, the system limits the classiﬁcation process during the device connecting phase-

only – it is not feasible for continuous monitoring. [134] proposes a technique to

improve the “IoT Sentinel” by extracting payload entropy, TCP payload length,

TCP window size in addition to the subset of above mentioned attributes for every

ﬁve packets. Although this method solves the limitation, it incurs a high cost in

attribute extraction.

Work in [135] uses over 300 attributes from each TCP session of IoT traﬃc to

classify the device type by applying majority voting for every 20 consecutive sessions.

It highlight the most important attributes as packets Time-To-Live (minimum, me-

dian, and average), the ratio of transmitted-bytes to received-bytes, and the Alexa

rank of servers which the device communicates.

In this method, the IoT devices

which rarely use TCP sessions, or which have long TCP sessions (e.g.,

[6] men-

tions Google Dropcam initiates TCP connection during the boot states and keep it

alive as long as it has network connectivity) may take a long duration to be classi-

ﬁed. Some researchers [136] argue that traﬃc attributes need to be automatically

learned (from a raw sequence of packet payloads in TCP ﬂows) instead of being

hand-crafted. We believe that the extraction of packet payloads makes it diﬃcult

to scale this method. Our ﬁrst approach to classify IoT devices (explained in Chap-

ter 3) uses attributes which can be extracted relatively easily using the network

elements that are instrumented with hardware-accelerated ﬂow-level analyzers (e.g.,

Netﬂow capable devices).

Although the attributes mentioned above provides a rich set of information about

the IoT behavior, obtaining them require specialized hardware accelerators – be-

comes more expensive, and unscalable due to the need of deep packet inspection

in real-time. To tackle this, researchers in [132] have proposed a model which uses

42

Chapter 2. Survey on IoT Ecosystems

traﬃc patterns of encrypted network ﬂows (with a server measured outside NAT) to

reveal the existence of IoT speciﬁc devices inside a home network without the need

for detailed packet or ﬂow inspection. One of our works [7] show that the IoT traﬃc

can be: 1) channeled based on either protocols; or endpoints speciﬁc ﬂows, and 2)

monitored at low cost using SDN enabled switches. This inspired us to extract the

ﬂow level measurements (i.e., byte count, packet count) corresponding to individual

devices as attributes of speciﬁc ﬂows (e.g., DNS query, DNS response, NTP query,

NTP response, and etc.) in Chapter 4. Also, we compute the attributes in diﬀerent

time-granularities since the traﬃc attributes are better in characterizing the network

behavior of IoTs in multiple time-scales [137].

Classiﬁcation & Anomaly Techniques: Machine learning techniques do a

prominent role in state-of-the-art traﬃc classiﬁcation and anomaly detection en-

gines [138]. The previous studies have used a diﬀerent kind of machine learning

techniques which can be categorized into two-fold: 1) supervised; and 2) unsuper-

vised.

Supervised classiﬁcation algorithms such as Support Vector Machines (SVM),

naive Bayes, decision trees (e.g., : C4.5, random forest), k-nearest neighbor (KNN),

and neural network/Multilayer Perceptron(MLP) are commonly used for the traﬃc

classiﬁcation purposes. The supervised classiﬁcation techniques tend to give high

performance in distinguishing the known classes due to their discriminative ability.

In the literature, these algorithms are used in two diﬀerent modes: 1) Multi-class

classiﬁcation – classify the data between tgree or more than classes (e.g., IoT de-

vice classiﬁcation/state classiﬁcation); and 2) Binary classiﬁcation – make a binary

decision (i.e., Positive or Negative).

In [109], Lippmann et al. evaluate the performance of multiple multi-class clas-

siﬁer performance (i.e., KNN, SVM, Binary decision Tree and MLP) in classifying

the computer operating systems (e.g., : Linux 2.0, Linux 2.1, MacOS9, Win9x,

WinNT, WinXP) using TCP/IP header information and concludes the KNN and

43

Chapter 2. Survey on IoT Ecosystems

Binary decision tree outperforms the rest. Despite the simplicity and reasonably

good performance on a low dimensional dataset, KNN may be prone to be aﬀected

by high dimensionality in the data and high computational cost in classiﬁcation [139].

Performance of SVM is very sensitive to the selection of hyperparameters, and it

becomes diﬃcult to train the accurate models [140]. In [116], Lopez-Martin et al.

classify the network application traﬃc (e.g., : Google, YouTube, Oﬃce 365) using

the multi-class neural network which is proven to be eﬀective in complex data struc-

tures, but it requires a large amount of data to train the system. Decision tree-based

classiﬁers are commonly used since they are easy to build discriminative models with

relatively small amount of data. However, they are prone to being over-ﬁt for the

training dataset. Random forest perfectly handles the over-ﬁtting issue using en-

semble decision trees. Chapter 3 and Chapter 4 therefore explore the use of it for

classifying the device types and their states. The main constraint of the multi-class

classiﬁcation is scalability – a high number of classes makes the classiﬁer complex

and updating the classiﬁer requires full retraining.

Unlike multi-class classiﬁers, binary classiﬁers are trained only based on two

diﬀerent classes. In the traﬃc classiﬁcation domain, this approach is used for various

purposes like distinguishing between known attack vs. benign traﬃc [141] or IoT vs.

non-IoT traﬃc, etc. In [133], Doshi et al. train the binary classiﬁer to identify the

DDoS attack traﬃcs (made by Mirai botnets) and benign traﬃc of the IoT devices.

Although this method shows high performance in detecting the trained attacks from

benign traﬃc, the eﬃcacy of unknown attack detection cannot be guaranteed. The

reason is supervised classiﬁers learn only the diﬀerences between classes rather than

proﬁling the whole behavior. On the other hand, the work in [129] proposes to

build individual binary classiﬁcation model for each class in a device classiﬁcation

problem to eliminate the complexity issue of multi-class classiﬁcation. It is achieved

by individually training each device traﬃc with the mixture of the rest of the devices

– ‘one vs. rest’ approach. It is a well-known fact that supervised machine learning

methods may suﬀer due to an unbalanced dataset (unequal number of data points

44

Chapter 2. Survey on IoT Ecosystems

between classes). This makes scalability issues in ‘one vs. rest’ binary classiﬁcation

approach when the proportion of the ‘rest’ part keeps increasing.

Unsupervised machine learning techniques are generative, which can model the

whole behavioral pattern of the data to detect the abnormal behavioral changes in

the traﬃc. Over the decade, many unsupervised network traﬃc anomaly detection

methods have been proposed for general Internet traﬃc [142–144]. They use various

algorithms such as probabilistic (e.g., Gaussian mixture models),domain-based (e.g.,

one-class SVM), and cluster-based (e.g., DBSCAN, Kmeans) [85, 145]. The work

in [146] use unsupervised clustering approaches to distinguish the botnet C&C com-

munication channels from the benign traﬃc of traditional network traﬃc as well as

the malicious activity during the attack mode. The study in [143] shows that unsu-

pervised algorithms may suﬀer due to the curse of high dimensional data. Therefore,

it proposes to reduce the attribute dimension using Principal Component Analysis

(PCA) which transform the attributes to a reduced set of uncorrelated attributes.

In the context of IoT, , [147] authors use neural network-based deep autoencoders

to detect anomalies. Similarly, work in [148] proposed a deep-learning framework

to ﬁngerprint iPads and iPhones using packets inter-arrival time. However, these

intensive packet-based approaches are computationally expensive.

In [85], Hamza

et al. propose a volumetric attack detection mechanism by monitoring the MUD-

compliant activities which require ﬁne-grained ﬂow for each device. To achieve this,

they use X-Means, which is a version of K-Means clustering algorithm.

The main negative aspect of the unsupervised learning method is they are not

discriminative as supervised machine learning algorithms – diﬃcult for classiﬁcation

purposes. One of our work [9] resolves this issue by proposing a probability-based

conﬂict resolver, which is extended to anomaly detection in Chapter 5.

45

2.8 Conclusion

Chapter 2. Survey on IoT Ecosystems

In this chapter, we have comprehensively studied the IoT ecosystem as well as the

challenges introduced by it, especially in the areas of cybersecurity and user privacy.

Initially, the market segments of IoT devices are identiﬁed based on the category of

industrial, consumer, and enterprises IoTs. The challenges in securing IoT devices

compared to traditional general-purpose devices are investigated in order to further

move on to specialized security solutions for the IoT ecosystem. We have discussed

the roles and responsibilities of key players in securing the IoT ecosystem. We pro-

posed a systematic approach to evaluate the security of IoT devices. From there we

compared the existing security solutions and the challenges to be encountered when

adapting it to the IoT domain. Finally, we set the foundation for characterization,

classiﬁcation, and anomaly detection in IoT traﬃc, which will be discussed deeply

in the following chapters.

46

Chapter 3

IoT Traﬃc Characterization and

Classiﬁcation

Contents

3.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

3.2

IoT Traﬃc Collection and Synthesis . . . . . . . . . . . . . . . . 52

3.2.1

Experimental Test-bed . . . . . . . . . . . . . . . . . . .

3.2.2 Trace Data . . . . . . . . . . . . . . . . . . . . . . . . . .

53

54

3.3

IoT Traﬃc Characterization . . . . . . . . . . . . . . . . . . . . 55

3.3.1

IoT Activity and Volume Pattern . . . . . . . . . . . . .

3.3.2

IoT Signaling Pattern . . . . . . . . . . . . . . . . . . . .

57

58

3.4 Machine Learning Based Classiﬁcation . . . . . . . . . . . . . . . 65

3.4.1 Multi-Stage Device Classiﬁcation Architecture . . . . . .

3.4.2

Performance Evaluation . . . . . . . . . . . . . . . . . .

65

68

3.5 Real-Time Operation in a Network . . . . . . . . . . . . . . . . . 73

3.5.1 Computing Attributes

. . . . . . . . . . . . . . . . . . .

3.5.2 Training the Machine . . . . . . . . . . . . . . . . . . . .

3.5.3

Interpreting the Output of Classiﬁer

. . . . . . . . . . .

74

76

77

3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

47

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

Our study in the previous chapter showed the importance of real-time visibility

into IoT network using behavioral monitoring. In this chapter, we study the network

behavioral patterns of IoT devices using traﬃc characteristics obtained at the net-

work level. Using this, we develop an inference engine to classify the device types.

Our contributions are fourfold. First, we instrument a smart environment with 28

diﬀerent IoT devices spanning cameras, lights, plugs, motion sensors, appliances and

health-monitors. We collect and synthesize traﬃc traces from this infrastructure for

a period of six months, a subset of which we release as open data for the community

to use. Second, we present insights into the underlying network traﬃc character-

istics using statistical attributes such as activity cycles, port numbers, signalling

patterns and cipher suites. Third, we develop a multi-stage machine learning based

classiﬁcation algorithm and demonstrate its ability to identify speciﬁc IoT devices

with over 99% accuracy based on their network activity. Finally, we discuss the

trade-oﬀs between cost, response time, and performance involved in deploying the

classiﬁcation framework in real-time. Parts of this chapter have been published in [5]

and [6].

3.1 Introduction

The number of devices connecting to the Internet is ballooning, ushering in the era

of the “Internet of Things” (IoT). As we mentioned in previous chapters, the pro-

liferation of IoT, creates an important problem. Operators of smart environments

can ﬁnd it diﬃcult to determine what IoT devices are connected to their network

and further to ascertain whether each device is functioning normally. This is mainly

attributed to the task of managing assets in an organization, which is typically

distributed across diﬀerent departments. For example, in a local council, lighting

sensors may be installed by the facilities team, sewage and garbage sensors by the

48

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

sanitation department and surveillance cameras by the local police division. Co-

ordinating across various departments to obtain an inventory of IoT assets is time

consuming, onerous and error-prone, making it nearly impossible to know precisely

what IoT devices are operating on the network at any point in time. Obtaining

“visibility” into IoT devices in a timely manner is of paramount importance to the

operator, who is tasked with ensuring that devices are in appropriate network secu-

rity segments, are provisioned for requisite quality of service, and can be quarantined

rapidly when breached. The importance of visibility is emphasized in Cisco’s most

recent IoT security report [18], and further highlighted by two recent events: sensors

of a ﬁshtank that compromised a casino in Jul 2017 [35], and attacks on a University

campus network from its own vending machines in Feb 2017 [42]. In both cases, net-

work segmentation could have potentially prevented the attack and better visibility

would have allowed rapid quarantining to limit the damage of the cyber-attack on

the enterprise network.

One would expect that devices can be identiﬁed by their MAC address and DHCP

negotiation. However, this faces several challenges: (a) IoT device manufacturers

typically use NICs supplied by third-party vendors, and hence the Organizationally

Unique Identiﬁer (OUI) preﬁx of the MAC address may not convey any information

about the IoT device; (b) MAC addresses can be spoofed by malicious devices; (c)

many IoT devices do not set the Host Name option in their DHCP requests [149];

indeed we found that about half the IoT devices we studied do not reveal their host

names, as shown in Table 3.1; (d) even when the IoT device exposes its host name

it may not always be meaningful (e.g. WBP-EE4C for Withings baby monitor in

Table 3.1); and lastly (e) these host names can be changed by the user (e.g. the HP

printer can be given an arbitrary host name). For these reasons, relying on DHCP

infrastructure is not a viable solution to correctly identify devices at scale.

In this chapter, we address the above problem by developing a robust frame-

work that classiﬁes each IoT device separately in addition to one class of non-IoT

49

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

Table 3.1: MAC address and DHCP host name of IoT devices used in our testbed.

DHCP host name

Awair-4594
NetCamHD

MAC address OUI
IoT device
44:65:0d:56:cc:d3 Amazon Technologies Inc.
Amazon Echo
e0:76:d0:3f:00:ae AMPAK Technology, Inc.
August Doorbell Cam
70:88:6b:10:0f:c6
Awair air quality monitor
b4:75:0e:ec:e5:a9 Belkin International Inc.
Belkin Camera
ec:1a:59:83:28:11 Belkin International Inc.
Belkin Motion Sensor
Belkin International Inc.
ec:1a:59:79:f4:89
Belkin Switch
74:6a:89:00:2e:25 Rezolt Corporation
Blipcare BP Meter
IEEE Registration Authority
7c:70:bc:5d:5e:dc
Canary Camera
30:8c:fb:2f:e4:b2
Dropcam
Dropcam
6c:ad:f8:5e:e4:61 AzureWave Technology Inc.
Google Chromecast
28:c2:dd:ﬀ:a5:2d AzureWave Technology Inc.
Hello Barbie
70:5a:0f:e4:9b:c0 Hewlett Packard
HP Printer
74:c6:3b:29:d7:1d AzureWave Technology Inc.
iHome PowerPlug
d0:73:d5:01:83:08 LIFI LABS MANAGEMENT PTY LTD LIFX Bulb
LiFX Bulb
18:b4:30:25:be:e4 Nest Labs Inc.
NEST Smoke Sensor
Netatmo Camera
70:ee:50:18:34:43 Netatmo
Netatmo Weather station 70:ee:50:03:b8:ac Netatmo
Phillip Hue Lightbulb
Pixstart photo frame
Ring Door Bell
Samsung Smart Cam
Smart Things
TP-Link Camera
TP-Link Plug
Triby Speaker
Withings Baby Monitor
Withings Scale
Withings sleep sensor

00:17:88:2b:9a:25 Philips Lighting BV
e0:76:d0:33:bb:85 AMPAK Technology, Inc.
88:4a:ea:31:66:9d Texas Instruments
00:16:6c:ab:6b:88
Samsung Electronics Co.,Ltd
SmartThings
d0:52:a8:00:67:5e Physical Graph Corporation
f4:f2:6d:93:51:f1
Little Cam
50:c7:bf:00:56:39 TP-LINK TECHNOLOGIES CO.,LTD. HS110(US)
18:b7:9e:02:20:44
00:24:e4:10:ee:4c Withings
00:24:e4:1b:6f:96 Withings
00:24:e4:20:28:c6 Withings

Chromecast
Barbie-A52D
HPE49BC0
hap-29D71D

TP-LINK TECHNOLOGIES CO.,LTD.

WBP-EE4C

WSD-28C6

Philips-hue

Invoxia

Ambarella/C100F1615229

netatmo-welcome-183443

devices with high accuracy using statistical attributes derived from network traﬃc

characteristics. Qualitatively, most IoT devices are expected to send short bursts of

data sporadically. Quantitatively, our preliminary work in [5] was one of the ﬁrst

attempts to study how much traﬃc IoT devices send in a burst and how long they

idle between activities. We also evaluated how much signaling they perform (e.g.

domain lookups using DNS or time synchronization using NTP) in comparison to

the data traﬃc they generate. This chapter signiﬁcantly expands on our prior work

by employing a more comprehensive set of attributes on trace data captured over

a much longer duration (of 6 months) from a test-bed comprising 28 diﬀerent IoT

devices.

There is no doubt that it is becoming increasingly important to understand the

nature of IoT traﬃc. Doing so helps contain unnecessary multicast/broadcast traﬃc,

reducing the impact they have on other applications. It also enables operators of

smart cities and enterprises to dimension their networks for appropriate performance

50

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

levels in terms of reliability, loss, and latency needed by environmental, health, or

safety applications. However, the most compelling reason for characterizing IoT

traﬃc is to detect and mitigate cyber-security attacks. It is widely known that IoT

devices are by their nature and design easy to inﬁltrate [2, 20, 57, 58, 150, 151]. New

stories are emerging of how IoT devices have been compromised and used to launch

large-scale attacks [152]. The large heterogeneity in IoT devices has led researchers

to propose network-level security mechanisms that analyze traﬃc patterns to identify

attacks (see [37] and our recent work [7]); success of these approaches relies on a

good understanding of what “normal” IoT traﬃc proﬁle looks like.

Our primary focus in this chapter is to establish a machine learning framework

based on various network traﬃc characteristics to identify and classify the default

(i.e. baseline) behavior of IoT devices on a network. This chapter ﬁlls an important

gap in the literature relating to classiﬁcation of IoT devices based on their network

traﬃc characteristics. Our contributions are

1. We instrument a living lab with 28 IoT devices emulating a smart environment.

The devices include cameras, lights, plugs, motion sensors, appliances and

health-monitors. We collect and synthesize data from this environment for a

period of 6 months. A subset of our data is made available for the research

community to use.

2. We identify key statistical attributes such as activity cycles, port numbers,

signaling patterns and cipher suites, and use them to give insights into the

underlying network traﬃc characteristics.

3. We develop a multi-stage machine learning based classiﬁcation algorithm and

demonstrate its ability to identify speciﬁc IoT devices with over 99% accuracy

based on their network behavior.

4. We evaluate the deployment of the classiﬁcation framework in real-time, by

examining the trade-oﬀs between costs, response time, and accuracy of the

classiﬁer.

51

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

The rest of this chapter is organized as follows: We present our IoT setup and

data traces in §3.2, and in §3.3 characterize traﬃc attributes of the various IoT

devices. In §3.4 we propose a machine learning based multi-stage device classiﬁcation

method and evaluate its performance, followed by a discussion on the real-time

operation of the proposed system in §3.5. The chapter is concluded in §3.6.

3.2 IoT Traﬃc Collection and Synthesis

In this section, we describe our smart environment infrastructure for collecting and

synthesizing traﬃc from various IoT devices.

Figure 3.1: Testbed architecture showing connected 28 diﬀerent IoT devices along
with several non-IoT devices, and telemetry collected across the infrastructure is fed
to our classiﬁcation models.

52

Energy managementBelkin MotionSensorBelkinSwitchiHomePower plugLIFXLightbulbPhillip Hue LightbulbTP LinkPower plugAppliancesGoogle ChromecastHelloBarbieHP EnvyPrinterPixstarPhotoFrameTribySpeakerCamerasAugust DoorbellBelkinCameraCanaryCameraDropCameraNetatmoCameraRing DoorbellSamsungSmart CamTP Link CameraWithingsBaby MonitorNon-IoTSmart Phones & TabletsLaptopsWired connectionWireless connectionControllers/HubsAmazonEchoSamsung SmartThingsHue BridgeZigbeeAnalyzer/ClassifiertelemetryInternetgatewayHealth-MonitorWithingsScaleAwairAir QualityMonitorBlipcareBP meterNetatmoweather stationWithingsSleep SensorNest SmokeAlarmInternal attackerExternal attackerExternal VictimInternal VictimChapter 3.

IoT Traﬃc Characterization and Classiﬁcation

3.2.1 Experimental Test-bed

A real-life architecture of a “smart environment” is depicted in Fig. 3.1 that serves a

wide range of IoT and non-IoT devices over its (wired/wireless) network infrastruc-

ture and allows them to communicate with the Internet servers via a gateway. Our

lab setup is a specialized implementation of this architecture, housed at our campus

facility, comprises one node of TP-Link Archer C7 v2 WiFi access point (repre-

senting internal switch) collocated with the Internet gateway. The TP-Link access

point, ﬂashed with the OpenWrt ﬁrmware release Chaos Calmer (15.05.1, r48532),

serves as the gateway to the public Internet. We also installed additional OpenWrt

packages on the gateway, namely tcpdump (4.5.1-4) for capturing traﬃc, bash

(4.3.39-1) for scripting, block-mount package for mounting external USB storage

on the gateway, kmod-usb-core and kmod-usb-storage (3.18.23-1) for storing

the traﬃc trace data on the USB storage.

In our lab setup, the WAN interface of the TP-Link access point is connected to

the public Internet via the university network, while the IoT devices are connected

to the LAN and WLAN interfaces respectively. Our smart environment has a total

of 28 unique IoT devices representing diﬀerent categories along with several non-IoT

devices. Here, IoT refers to speciﬁc-purpose Internet connected devices (e.g. cameras

and smoke sensors), while general-purpose devices (e.g. phones and laptops) fall into

the non-IoT category.

The IoT devices include cameras (Nest Dropcam, Samsung SmartCam, Netatmo

Welcome, Belkin camera, TP-Link Day Night Cloud camera, Withings Smart Baby

Monitor, Canary camera, August door bell, Ring door bell), switches and triggers

(iHome, TP-Link Smart Plug, Belkin Wemo Motion Sensor, Belkin Wemo Switch),

hubs (Smart Things, Amazon Echo), air quality sensors (NEST Protect smoke alarm,

Netatmo Weather station, Awair air quality monitor), electronics (Triby speaker,

PIXSTAR Photoframe, HP Printer, Hello barbie, Google Chromecast), healthcare

53

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

devices (Withings Smart scale, Withings Aura smart sleep sensor, Blipcare blood

pressure meter) and light bulbs (Phips Hue and LiFX Smart Bulb). Several non-IoT

devices were also connected to the testbed, such as laptops, mobile phones and an

Android tablet. The tablet was used to conﬁgure the IoT devices as recommended

by the respective device manufacturers.

3.2.2 Trace Data

All the traﬃc on the LAN side was collected using the tcpdump tool running on

OpenWrt [153]. It is important to have a one-to-one mapping between a physical

device and a known MAC address (by virtue of being in the same LAN) or IP address

(i.e. without NAT) in the traﬃc trace. Capturing traﬃc on the LAN allowed us to

use MAC address as the identiﬁer for a device to isolate its traﬃc from the traﬃc mix

comprising many other devices in the network. We developed a script to automate

the process of data collection and storage. The resulting traces were stored as pcap

ﬁles on an external USB hard drive of 1 TB storage attached to the gateway. This

setup permitted continuous logging of the traﬃc across several months.

We started logging the network traﬃc in our smart environment from 1-Oct-

2016 to 13-Apr-2017, i.e. over a period of 26 weeks. The raw trace data contains

packet headers and payload information. The process of data collection and storage

begins at midnight local time each day using the Cron job on OpenWrt. We wrote

a monitoring script on the OpenWrt to ensure that data collection/storage was

proceeding smoothly. The script checks the processes running on the gateway at 5

second intervals. If the logging process is not running, then the script immediately

restarts it, thereby limiting any data loss event to only 5 seconds. To make the trace

data publicly available, we set up an Apache server on a virtual machine (VM) in our

university data center and wrote a script to periodically transfer the trace data from

the previous day, stored on the hard drive, onto the VM. The trace data from two

54

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

weeks is openly available for download at: http://iotanalytics.unsw.edu.au/.

The size of the daily logs varies between 61 MB and 2 GB, with an average of 365

MB.

3.3 IoT Traﬃc Characterization

We now present our observations using passive packet-level analysis of traﬃc from

28 IoT devices over the course of 26 weeks. We study a broad range of IoT traﬃc

characteristics including activity patterns (e.g. distribution of volume/times during

active/sleep periods), and signalling (e.g. domain names requested, server-side port

numbers used and TLS handshake exchanges).

IoT traﬃc constitutes (i) traﬃc generated by the devices autonomously – e.g.

DNS, NTP, etc. that are unaﬀected by human interaction, as well as (ii) traﬃc

generated due to users interacting with the devices – e.g. Belkin Wemo sensor

responding to detection of movement, Amazon Echo responding to voice commands

issued by a user, LiFX lightbulb changing colour and intensity upon user request,

Netatmo Welcome camera detecting an occupant and instructing the LiFX light

bulb to turn on with a speciﬁc colour, and so on. Our dataset well captures these

two types of IoT traﬃc from a lab that represents a living smart environment (i.e.

covering periods over which humans are present or absent in the environment).

To provide insights into the IoT traﬃc characteristics, we show in Fig. 3.2 a

Sankey plot of network traﬃc seen over a 24 hour period for Amazon Echo and

LiFX lightbulb. These devices are chosen just for illustrative purposes. Each plot

depicts the ﬂow-level information generated by the respective device. Flows are: (a)

either unicast or multicast/broadcast, (b) destined to either local hosts (LAN) or

Internet servers (WAN), and (c) tied to protocols (TCP, UDP, ICMP or IGMP) and

port numbers.

55

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

(a) Amazon Echo.

(b) LiFX lightbulb.

Figure 3.2: Sankey diagram of daily network activity for two representative IoT
devices, Amazon Echo and LiFX lightbulb. A clear distinction is observed in terms
of their communication patterns, i.e. the servers they talk to, and the port numbers
and protocols used for data exchange.

Fig. 3.2 provides a visual aid depicting the underlying traﬃc signature exhibited

by the two devices. For example, DNS (port number 53) and NTP (port number

123) are used by both Amazon Echo and LiFX lightbulb. While Amazon Echo

uses HTTP (port number 80), HTTPS (port number 443) and ICMP (port num-

ber 0), LiFX lightbulb does not use any of these applications. Further, each device

seems to communicate to a unique port number on a WAN server; TCP 33434 for

Amazon Echo and UDP 56700 for LiFX lightbulb, as shown by the top ﬂow in

Figures 3.2a and 3.2b. Finally, we observe that Amazon Echo accesses a number

of domain names including softwareupdates.amazon.com, device-metrics-su.

amazon.com, example.org, pindorama.amazon.com and pool.ntp.org. However,

LiFX lightbulb communicates with only two domains, i.e. v2.broker.lifx.co and

pool.ntp.org.

56

Amazon EchoLANMulticastwww.meethue.com23.23.189.20pool.ntp.orgdevice-metrics-us.amazon.com224.0.0.22pindorama.amazon.com239.255.255.250www.example.orgsoftwareupdates.amazon.com192.168.1.1192.168.1.240208.67.220.2208.8.8.83850167ICMPTCPIGMPUDP33434123530WAN443190080LiFX Smart BulbBroadcastLANv2.broker.lifx.copool.ntp.org8.8.8.8192.168.1.1192.168.1.25512367UDPTCP5670053WANChapter 3.

IoT Traﬃc Characterization and Classiﬁcation

3.3.1

IoT Activity and Volume Pattern

We start with the activity pattern of IoT devices that is deﬁned by the properties of

their traﬃc ﬂows. We deﬁne four key attributes at a per-ﬂow level to characterize IoT

devices based on their network activity: ﬂow volume (i.e. sum total of download

and upload bytes), ﬂow duration (i.e. time between the ﬁrst and the last packet

in a ﬂow), average ﬂow rate (i.e. ﬂow volume divided by the ﬂow duration), and

device sleep time (i.e. time interval over which the IoT device has no active ﬂow).

We plot in Fig. 3.3 the probability distribution of the above four attributes for

a chosen set of IoT devices using the trace data collected over 26 weeks. It can be

observed from Fig. 3.3a that each IoT device tends to exchange a small amount of

data per ﬂow. For the case of the LiFX lightbulb (depicted by red bars), 26% of

ﬂows transfer between [130, 140] bytes and 20% between [120, 130] bytes. The ﬂow

volume for the Belkin motion sensor (depicted by green bars) is slightly higher; over

35% of ﬂows transfer between [2800, 3800] bytes. For the Amazon Echo (depicted

by blue bars), over 95% of ﬂows transfer less than 1000 bytes. Though we present

the ﬂow volume histogram for only a few devices, most of our IoT devices exhibit a

similar predictable pattern.

A similar pattern emerges for the ﬂow duration as well. Referring to Fig. 3.3b,

we note that the ﬂow duration of 53 seconds is seen in more than 40% of ﬂows for

Amazon Echo, while a duration of 60 seconds is seen for the LiFX lightbulb and

Belkin motion sensor with a probability of 50% and 21% respectively.

For the average ﬂow rate attribute, Fig. 3.3c shows that the mean rate is rather

small, in the bits-per-second range as one would qualitatively expect. Quantitatively,

the ﬁgure shows that the LiFX lightbulb has an average ﬂow rate of 18 bits-per-

second nearly 60% of the time. Nearly 30% of Belkin ﬂows have a bit rate in the

range 59 to 60 bits-per-second while nearly 40% Amazon Echo ﬂows have a bit range

in the range 70 to 71 bits-per-second.

57

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

(a) Flow volume.

(b) Flow duration.

(c) Average ﬂow rate.

(d) Device sleep time.

Figure 3.3: Distribution of IoT activity pattern: (a) ﬂow volume, (b) ﬂow duration,
(c) average ﬂow rate and (d) device sleep time.

Lastly, in terms of the sleep time for the devices Fig. 3.3d shows that the Belkin

motion sensor and the LiFX lightbulb exhibit a distinct sleep pattern. The duration

is 1 second and 60 seconds with probability 73% and 48% respectively. However,

multiple sleep times with small probabilities are observed for the Amazon Echo. This

is because Amazon Echo keeps its TCP connections alive and goes to sleep only when

it disconnects from the Internet. Other devices in our test-bed also perform like the

Echo and do not seem to have a dominant sleep pattern.

3.3.2

IoT Signaling Pattern

We now focus on the application layer protocols, inferred using the port numbers,

that IoT devices mostly use to communicate locally in the LAN and/or externally

with servers on the public Internet.

58

01000200030004000X: flow volume (Bytes)00.050.10.150.20.250.3Probability: P(flow volume  = x)Amazon EchoBelkin MotionLiFX lighbulb0100200300400X: flow duration (Sec)00.10.20.30.40.50.6Probability: P(flow duration  = x)Amazon EchoBelkin MotionLiFX lighbulb020406080100X: Mean rate (bps)00.20.40.60.81Probability: P(Flow rate  = x)Amazon EchoBelkin MotionLiFX lightbulb020406080X: sleep time (Sec)00.20.40.60.81Probability: P(sleep time  = x)Amazon EchoBelkin MotionLiFX lightbulbChapter 3.

IoT Traﬃc Characterization and Classiﬁcation

(a) Amazon Echo {10}.

(b) LiFX lightbulb {5}.

(c) Awair air monitor {7}.

(d) Belkin motion sensor {7}.

(e) Belkin power switch {7}.

(f) Belkin camera {9}.

(g) Netatmo weather station {4}.

(h) Non-IoT {2382}.

Figure 3.4: Word-cloud of server ports (total count of unique ports is shown in
{sub-captions} next to the device name).

Server port numbers

Fig. 3.4 shows the word cloud of server-side port numbers of all ﬂows initiated from

a variety of IoT devices. For each device, if a port is used more frequently then it is

shown by a larger font-size in the respective word cloud. Sub-captions (i.e. numbers

within {}) report the number of unique server ports for each device. It can be seen

that IoT devices each uniquely communicate with a handful of server ports whereas

non-IoT devices use a much wider range of services (i.e. 2382 unique ports are shown

in Fig. 3.4h and many of them are very infrequent). We observe that non-standard

ports 33434, 56700, 8883, and 25050 are prominently seen in traﬃc originating from

Amazon Echo, LiFX lightbulb, Awair air quality monitor, and Netatmo weather

station respectively, as shown in the top row of Fig. 3.4. Further, we note devices

from the same manufacturer share certain ports. For example, port numbers 8443

and 3478 are common between Belkin’s motion sensor, power switch, and camera,

59

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

(a) Amazon Echo {30}.

(b) Google Dropcam {5}.

(c) HP printer {6}.

(d) Belkin camera {11}.

(e) Belkin motion sensor {5}. (f) Belkin power switch {8}.

(g) Awair air quality {5}.

(h) LiFX lightbulb {2}.

(i) Non-IoT {11927}.

Figure 3.5: Word-cloud of domain names (total count of unique domains is shown
in {sub-captions} next to the device name).

as shown in Figures 3.4d-3.4f. We also note that well-known standard port numbers

such as 53 (DNS), 123 (NTP), 0 (ICMP) and 1900 (SSDP) are used by many of the

IoT devices as well as the non-IoTs with various frequencies, as shown in Fig. 3.4.

Moreover, the server-side port number of 443 (TLS/SSL) is also used by many of

the IoT devices.

DNS queries

DNS is a common application used by almost all networked devices. Since IoT

devices are custom-designed for speciﬁc purposes, they access a limited number

of domains corresponding to their vendor-speciﬁc end-point servers. We plot in

Fig. 3.5 the word cloud of domain names accessed by several IoT devices as well

as non-IoTs. It is seen that IoT devices are fairly distinguishable by the domain

names they communicate with. For example, as depicted in Figures 3.5a-3.5c,

domains such as example.com, example.net, and example.org are frequently re-

quested by Amazon Echo; sub-domains of hp.com and hpeprint.com are seen in

60

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

DNS queries from the HP printer. However, we also see that some prominent do-

main names are shared between the diﬀerent devices. For example, belkin.com and

d3gjecg2uu2faq.cloudfront.net are commonly used by Belkin devices (i.e. cam-

era, motion sensor and power switch) as shown in Figures 3.5d-3.5f; or pool.ntp.org

is prominent in traﬃc ﬂows generated from Google Dropcam, Awair air quality mon-

itor and LiFX lightbulb, as shown in Figures 3.5b-3.5h. Again considering non-IoTs

in Fig. 3.5i, we see about 12000 unique domains visited which is far diverse com-

pared to IoT devices with only a handful of domains accessed repeatedly. We also

found that IoT devices diﬀer from one other in how often the DNS protocol is used.

We have observed from our traﬃc traces that IoT devices generate DNS queries

during diﬀerent stages of its operation; for example only during the boot-up phase

(e.g. Google Dropcamp) or when interacting with a user (e.g. Hello Barbie) or

periodically (e.g. Amazon Echo). As shown in Fig. 3.6a, certain IoT devices exhibit

a characteristic signature in the frequency of their DNS queries. The LiFX light-

bulb and Amazon Echo send DNS queries very frequently (i.e. every 5 minutes) but

a device like the Belkin motion sensor requests domain names only once every 30

minutes.

NTP queries

As mentioned earlier, NTP is another popular protocol used by IoT devices because

precise and veriﬁable timing is crucial for IoT operations [154]. Many IoT devices

tend to use NTP protocol (UDP port 123) in a periodic manner in order to synchro-

nize their time with publicly available NTP servers. For example, Awair air quality

monitor, LiFX lightbulb and Google Dropcam obtain the IP address of time servers

from pool.ntp.org. We also ﬁnd that time synchronization occurs repeatedly in

our test-bed and many IoT devices exhibit a recognizable pattern in the use of NTP.

For example, the Belkin power switch, LiFX lightbulb and SmartThings hub send

NTP requests every 60, 300 and 600 seconds respectively, as shown in histogram

plot of Fig. 3.6b.

61

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

(a) Histogram of DNS interval.

(b) Histogram of NTP interval.

Figure 3.6: Distribution of IoT signaling pattern: (a) DNS interval, (b) NTP inter-
val.

Cipher suite

A number of IoT devices use TLS/SSL protocol (port number 443) to communicate

with their respective servers on the Internet [115].

In order to initiate the TLS

connection and negotiate the security algorithms with servers, devices start hand-

shaking by sending a “Client Hello” packet with a list of “cipher suites” that they can

support, in the order of their preference. For example, Figures 3.7a and 3.7b depict

cipher suites that Amazon Echo oﬀers to two diﬀerent Amazon servers. Each cipher

suite (i.e. 4-digit code) can take one of 380 possible values and represents algorithms

for key exchange, bulk encryption and message authentication code (MAC). For ex-

ample, the cipher 002f negotiated by an Amazon server uses RSA, AES_128_CBC,

and SHA protocols for key exchange, bulk encryption and message authentication,

respectively.

We ﬁnd that 17 out of the 28 IoT devices in our setup, including the Amazon

Echo, August Doorbell Cam, Awair air quality monitor, Belkin Camera, Canary

Camera, Dropcam, Google Chromecast, Hello Barbie, HP ENVY Printer, iHome,

Netatmo Welcome camera, Philips Hue lightbulb, Pixtar photoframe, Ring Door

Bell, Triby, Withings Aura smart sleep sensor and Withings Scale, use TLS/SSL for

communication. We ﬁnd that Amazon Echo uses total of ﬁve diﬀerent cipher suite

62

0500100015002000X: DNS interval (sec)00.20.40.60.81Probability: P(DNS interval  = x)← LiFX lightbulb    (300 s)← Echo  (300 s)Belkin motion→  (1800 s)← Netatmo weather    (1200 s)← Smart Things    (600 s)0500100015002000X: NTP interval (sec)00.20.40.60.81Probability: P(NTP interval  = x)← Smart Things    (600 s)← LiFX  (300 s)← Belkin    switch    (60 s)Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

(a) cs1 of Amazon Echo.

(b) cs2 of Amazon Echo.

Figure 3.7: Signature of cipher suite.

strings when communicating SSL to diﬀerent servers, Triby speaker uses two strings,

while the Pixtar photoframe uses only one string for all of its SSL communications.

We plot unique cipher suite strings from these three devices in Fig. 3.8 as discrete

signals: x-axis is the order of 4-digit cipher codes that appear in the oﬀered suite, and

y-axis is the index of the individual cipher codes (i.e. a value from {1, 2, ..., 380}).

It is seen that the collection of cipher suite signals enunciates a unique signature for

each IoT device. Exceptionally, we found that Pixtar photoframe shares its single

cipher suite with one of 18 suites that are used by August door-bell – we will see in

§3.4.2 that relying only on cipher suite attribute would not be eﬀective in classifying

Pixtar photo-frame traﬃc.

There are however many devices that rarely exchange cipher suites but instead

prefer to keep their TLS connections alive for a long period. For example, Google

Dropcam establishes a TLS connection to its own server whenever it boots up and

maintains this connection as long as it has network connectivity, while Amazon Echo

and Pixstar photoframe initiate on average 1 and 2 TLS connections respectively

every hour.

Summary: In this section, we have identiﬁed 8 key attributes based on the

underlying network traﬃc characteristics of IoT devices. They are ﬂow volume, ﬂow

duration, average ﬂow rate, device sleep time, server port numbers, DNS queries,

63

{"ciphersuite":["c014", "c00a", "0039", "0038", "0037", "0036", "0088", "0087", "0086", "0085", "c00f", "c005", "0035", "0084", "c013", "c009", "0033", "0032", "0031", "0030", "009a", "0099", "0098", "0097", "0045", "0044", "0043", "0042", "c00e", "c004", "002f", "0096", "0041", "0007", "c011", "c007", "c00c", "c002", "0005", "0004", "c012", "c008", "0016", "0013", "0010", "000d", "c00d", "c003", "000a", "0015", "0012", "000f", "000c", "0009", "00ff"] ,"negotiated cipher":"002f"}{"ciphersuite":["c030", "c02c", "c028", "c024", "c014", "c00a", "00a5", "00a3", "00a1", "009f", "006b", "006a", "0069", "0068", "0039", "0038", "0037", "0036", "0088", "0087", "0086", "0085", "c032", "c02e", "c02a", "c026", "c00f", "c005", "009d", "003d", "0035", "0084", "c02f", "c02b", "c027", "c023", "c013", "c009", "00a4", "00a2", "00a0", "009e", "0067", "0040", "003f", "003e", "0033", "0032", "0031", "0030", "009a", "0099", "0098", "0097", "0045", "0044", "0043", "0042", "c031", "c02d", "c029", "c025", "c00e", "c004", "009c", "003c", "002f", "0096", "0041", "0007", "c011", "c007", "c00c", "c002", "0005", "0004", "c012", "c008", "0016", "0013", "0010", "000d", "c00d", "c003", "000a", "0015", "0012", "000f", "000c", "0009", "00ff"] , "negotiated cipher": "c02f"}Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

(a) Amazon Echo.

(b) Triby.

(c) Pixtar photoframe.

Figure 3.8: Signature of cipher suite.

NTP queries and cipher suites. Although, some devices (e.g. Amazon Echo, or

LiFX lightbulb) can be uniquely identiﬁed by considering just one or two traﬃc

attributes such as the list of domain-names, port-numbers, or cipher suites, these

come with challenges. For example, a strong attribute like the list of cipher-suites is

observed very infrequently in the traﬃc (e.g. only once a day). As another example,

diﬀerent types of devices from the same vendor visit similar domains and use the

same port numbers to access cloud servers. Capturing aspects such as the number

of occurrences for these attributes (e.g. number of times a domain is accessed or

number of streams that use the port), in combination with other attributes, vastly

improves the prediction capability to distinguish between devices from the same

manufacturer. In the next section, we develop a multi-stage machine learning based

algorithm using combinations of these attributes to help classify IoT devices with

high accuracy.

64

020406080100n: order050100150200250300350400cipher-code [n]Echo cs1Echo cs2Echo cs3Echo cs4Echo cs5020406080100n: order050100150200250300350400cipher-code [n]Triby cs1Triby cs2020406080100n: order050100150200250300350400cipher-code [n]Pixtar csChapter 3.

IoT Traﬃc Characterization and Classiﬁcation

3.4 Machine Learning Based Classiﬁcation

In order to synthesize the attributes from our trace data, we ﬁrst convert the raw

pcap ﬁles into ﬂows on an hourly basis using the Joy tool [155]. Then, for a given

IoT device, we compute the traﬃc activity and signalling attributes deﬁned in the

previous section over the hourly instances. The number of instances for each device

obtained from the trace spanning 26 weeks varies depending on factors such as the

duration for which a device is online, or how a device generates traﬃc (autonomously

or interactively). For example, there were only 13 hourly instances for the Blipcare

BP monitor since it generates traﬃc only when the device is used by a user. On the

other hand, we collected 4177 instances for Google Dropcam.

3.4.1 Multi-Stage Device Classiﬁcation Architecture

We note that three of our attributes namely “set of domain names”, “set of remote

port numbers” and “set of cipher suites” are nominal (i.e. are not treated as numeric

values) and multi-valued (for example, {“53”:3, “123”:1, “443”:2} represents a set of

remote port numbers with three occurrences of port number 53, two occurrences

of port 123, and one occurrence of port number 443). Our remaining attributes

including ﬂow volume/duration, ﬂow rate, sleep time, and DNS/NTP intervals con-

tain single quantitative and continuous values. We therefore employ a two-stage

hierarchical architecture for our IoT classiﬁer as shown in Fig. 3.9.

In this architecture, we ﬁrst feed each multi-valued attribute to its corresponding

stage-0 classiﬁer in the form of a “bag of words”. A bag of words is a matrix whose

rows represent labeled instances, and columns represent unique words. This matrix

has M rows (i.e. total number of instances) and N columns (i.e. number of unique

words). We observed 356, 421 and 54 unique words for domain-names, remote port

numbers and cipher suite strings, as shown in Fig. 3.9. In addition to these unique

65

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

Figure 3.9: System architecture of the multi-stage classiﬁer.

words, we aggregated all corresponding words for non-IoT devices as “others” - a

column called “others” in each Stage-0 matrix represents words not seen in IoT

traﬃc. Each cell of this matrix is the number of occurrences of such unique words

in a given instance.

As shown in Fig. 3.9, each classiﬁer of Stage-0 generates two outputs, namely

a tentative class and a conﬁdence level, which together with other single-valued

quantitative attributes (i.e. ﬂow volume, duration, rate, sleep time, DNS, NTP

intervals) are fed into a Stage-1 classiﬁer that produces the ﬁnal output (i.e. the

device identiﬁcation with a conﬁdence level).

Stage-0: Bag-of-words Classiﬁers

We employ a Naive Bayes Multinomial classiﬁer to analyze each bag of words in the

stage-0 of our machine. It has been shown [156] that this classiﬁer performs well in

text classiﬁcation when dealing with a large number of unique words. During the

training phase, the classiﬁer takes the distribution of words, e.g. individual unique

66

!"#$"%%"&’’’""%"%()*+,-.!"#$%&$/00123*/20-45552,678/1,23,*55529,1:;32<0855528,,*+=,2<08>$29-0:,-21;?62<0()*+,-."#$$$&$@AAA%BAAA!B222@CA#%BCA##B22@CA$DBCA$EB(@CA"ABCA$CB22@CCFGBCCF&B222()*+,-.$$$""&$’()*+,)(-./’()*+0-1234)5’()*123/6(//7+34./896+453/1,2(9:;+453/1,2(<(2==+>)1+?2@+)>+7)13+5-.?/1=<)5>40/5A/+>)1+?2@+)>+7)13+5-.?/1=<(2==+>)1+?2@+)>+)>+0).245+52./=<)5>40/5A/+>)1+?2@+)>+0).245+52./=<(2==+>)1+?2@+)>+A47B/1+=-43/=<)5>40/5A/>)1+?2@+)>+<47B/1+6-43/=!"#$%&$’()%*($+%’*$,-).(’/!"#$%&$0%)"1,$,")(/!"#$%&$21+3(’$4-1*(/92C,/+D2E/=+F-(345).42(+A(2==4>4/1+92C,/+D2E/=+F-(345).42(+A(2==4>4/1+92C,/+D2E/=+F-(345).42(+A(2==4>4/1+4*"#(564*"#(57<(2==<)5>40/5A/G250).+’)1/=3+A(2==4>4/1Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

domain names, and computes the probability of each word given a class using:

Pr(wtrain
j

ci) =
|

N +

nl,ci,wk

train

(3.1)

1 +

nl,ci,wj

train

D
(cid:80)
l=1
N
(cid:80)
k=1

D
(cid:80)
l=1

where wj is a unique word in the training dataset (e.g. port number 56700); ci is

a class label (e.g. LiFX lightbulb); D is the total number of instances; nl,ci,wj
is the number of wj occurrences in each of instances with class label of ci; N is the

train

total number of unique words (e.g. we have N = 421 unique port numbers in our

dataset).

During the testing phase, the classiﬁer needs to compute the following probability

for all possible classes:

Pr(ci

W test) = Pr(ctrain
|

i

)

N
(cid:89)

j=1

Pr (wj

train

j

ci)ntest
|

(3.2)

where W test is a set represented by

w1 : ntest

1

, w2 : ntest

2

{

, ..., wN : ntest
N }

; ntest
j

is the

occurrence number of individual unique words wj in a given test instance; Pr(ctrain

i

)

is the presence probability of a class ci in the whole training dataset (i.e. number of

ci training instances divided by total number of all training instances). The classiﬁer

ﬁnally chooses the class that gives the maximum probability in (3.2) for a given set of

words along with their occurrences. Note that a Naive Bayes Multinomial classiﬁer

performs well if training instances are fairly distributed among various classes [156].

Stage-1 Classiﬁer

We have a stage-1 classiﬁer that takes all quantitative attributes along with the pair

of outputs from each stage-0 classiﬁer. Since the stage-1 attributes are not linearly

separable and the outputs of stage-0 classiﬁers are nominal values, we use a Random

Forest based stage-1 classiﬁer. Another reason for selecting the Random Forest is

its high tolerance to over-ﬁtting compared to other decision tree classiﬁers.

67

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

3.4.2 Performance Evaluation

We use the Weka [157] tool for our IoT device classiﬁcation. We have collected a

total of 50,378 labeled instances from our traﬃc traces. As mentioned earlier, we

have a number of instances from diﬀerent devices – those that generate traﬃc when

triggered by user interaction have small number of instances (e.g. 13 for Blipcare BP

monitor, 21 for Google Chromecast) and those that autonomously generate traﬃc

have a fairly large number of instances (e.g. 2,868 for Samsung Smart Things or

2,247 for Amazon Echo). We have randomly split instances into two groups, one

containing 70% of the instances for “training” and another containing 30% of the

instances for “testing”.

Table 3.2 shows the performance of our classiﬁer under various scenarios, each

captured by a pair of columns. For a given scenario, we measure the true positive

rate (i.e.

fraction of test instances that are correctly classiﬁed) and false positive

rate (i.e.

fraction of test instances that are incorrectly classiﬁed) for every device

corresponding to the rows in Table 3.2. We also obtain the average conﬁdence level

(i.e. a number between 0 and 1 depicted within square brackets in each cell) of our

classiﬁer for correctly classiﬁed and incorrectly classiﬁed instances. In addition, we

aggregate the performance of individual classes and compute the overall accuracy

(i.e.

total true positive rate) along with the overall root relative squared error

(RRSE) as measures of performance for our classiﬁer. These measures are reported

in the top row of each scenario in Table 3.2. Note that our objective is to achieve a

high accuracy (close to 100%) with a fairly low error (close to zero).

Performance of Stage-0: Port Numbers Attribute

The ﬁrst three columns correspond to those cases in which we consider only nominal

attributes of stage-0 (i.e. bag of words corresponding to port numbers, domain

names and cipher suites). The ﬁrst column shows that when we only use a list of

68

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

Table 3.2: Performance of the proposed IoT device classiﬁer under diﬀerent sets of
attributes.

Devices

Port Numbers

Accuracy: 92.13%
RRSE: 39.93%

Domain Names

Accuracy: 79.48%
RRSE: 57.56%

Cipher Suite
Accuracy: 36.15%
RRSE: 86.73%

Combined stage-0

Accuracy: 97.39%
RRSE: 18.24%

Final

Accuracy: 99.88%
RRSE: 5.06%

Incorrectly Classiﬁed

y
l
t
c
e
r
r
o
C

d
e
ﬁ
i
s
s
a
l
C

100.0%
[1.00]

Amazon Echo

August doorbell

99.0%
[1.00]

iHome: 0.6% [1.00]
Others: 0.4% [0.65]

y
l
t
c
e
r
r
o
C

d
e
ﬁ
i
s
s
a
l
C

99.6%
[1.00]

100.0%
[1.00]

Incorrectly Classiﬁed

Dropcam: 0.4% [0.08]

Awair air quality

97.6%
[1.00]

Non IoT: 2.0% [0.32]
Amazon Echo: 0.4% [0.53]

99.2%
[1.00]

SmartThings: 0.4% [0.49]
Dropcam: 0.4% [0.08]

95.5%
[1.00]

Belkin motion: 3.0% [0.94]
Others: 1.5% [0.67]

39.4%
[0.99]

Belkin motion: 59.8% [0.62]
Non IoT: 0.8% [1.00]

Non IoT: 0.2% [1.00]

0.0%
[-]

Belkin switch: 100.0% [0.57]

Belkin motion: 0.2% [0.77]
Others: 0.3% [0.75]

99.7%
[0.57]

Dropcam: 0.2% [0.08]
Blipcare BP: 0.1% [0.79]

99.8%
[1.00]

99.5%
[1.00]

100.0%
[1.00]

98.1%
[0.33]

SmartThings: 0.6% [0.99]
Others: 1.3% [0.59]

100.0%
[1.00]

100.0%
[1.00]

99.8%
[1.00]

95.4%
[1.00]

99.7%
[1.00]

99.4%
[1.00]

97.5%
[1.00]

100.0%
[1.00]

99.7%
[1.00]

Dropcam: 0.2% [0.08]

Dropcam: 2.0% [0.97]
Others: 2.6% [0.70]

Dropcam: 0.3% [0.08]

Belkin motion: 0.6% [1.00]

LiFX bulb: 1.9% [0.99]
Others: 0.5% [0.68]

Dropcam: 0.3% [0.08]

100.0%
[1.00]

100.0%
[0.09]

99.7%
[0.70]

SmartThings: 0.1% [0.42]
Dropcam: 0.1% [0.08]

100.0%
[1.00]

99.9%
[1.00]

97.8%
[1.00]

99.3%
[1.00]

14.5%
[1.00]

79.9%
[0.50]

99.7%
[1.00]

99.7%
[0.99]

Dropcam: 0.1% [0.08]

Dropcam: 2.2% [0.08]

Dropcam: 0.7% [0.08]

Dropcam: 73.4% [0.10]
SmartThings: 12.0% [0.43]

LiFX bulb: 20.1% [0.50]

Dropcam: 0.3% [0.08]

Dropcam: 0.3% [0.08]

Belkin cam

Belkin motion

Belkin switch

Canary cam

Dropcam

LiFX bulb

NEST smoke

Netatmo weat.

Netatmo cam

Pixstar photo

Samsung cam

SmartThings

TPlink cam

TPlink plug

Triby speaker

Withings sleep

96.8%
[1.00]

Non IoT: 1.9% [0.99]
Others: 1.2% [0.55]

Hue bulb

Chromecast

HP printer

iHome

Withings baby mon.

88.8%
[1.00]

62.5%
[1.00]

61.5%
[0.99]

79.2%
[0.90]

58.2%
[1.00]

Samsung cam: 11.1% [0.45]
Belkin motion: 0.1% [1.00]

Amazon Echo: 25.0% [0.52]
Non IoT: 12.5% [0.60]

100.0%
[1.00]

Dropcam: 38.0% [0.16]
Others: 0.6% [0.86]

Dropcam: 10.2% [0.34]
Others: 10.6% [0.42]

Non IoT: 41.8% [1.00]

Withings scale

74.8%
[0.98]

Non IoT: 15.3% [0.56]
Others: 9.9% [0.19]

3.8%
[1.00]

87.5%
[0.97]

100.0%
[1.00]

41.4%
[0.79]

Dropcam: 12.5% [0.08]

Withings sleep: 56.8% [0.96]
Dropcam: 1.8% [0.08]

Ring doorbell

Blipcare BP

Hello Barbie

Non IoT

0.6%
[0.98]

20.0%
[0.54]

0.0%
[-]

74.2%
[0.98]

Netatmo weat.: 95.8% [0.18]
Others: 3.6% [0.60]

100.0%
[0.98]

Ring doorbell: 80.0% [0.41]

Dropcam: 71.4% [0.08]
Others: 28.6% [0.50]
Triby speaker: 16.6% [0.90]
Others: 9.2% [0.69]

40.0%
[0.79]

21.4%
[1.00]

66.9%
[0.97]

HP printer: 60.0% [0.44]

Dropcam: 71.4% [0.08]
HP printer: 7.1% [0.45]
Dropcam: 29.7% [0.08]
Others: 3.4% [0.73]

y
l
t
c
e
r
r
o
C

d
e
ﬁ
i
s
s
a
l
C

100.0%
[1.00]

78.8%
[1.00]

99.2%
[0.63]

0.0%
[-]

0.0%
[-]

0.0%
[-]

100.0%
[1.00]

100.0%
[0.09]

0.0%
[-]

0.0%
[-]

0.0%
[-]

99.7%
[0.92]

0.0%
[-]

0.0%
[-]

0.0%
[-]

0.0%
[-]

0.0%
[-]

Incorrectly Classiﬁed

Pixstar photo: 21.2% [1.00]

Dropcam: 0.8% [0.08]

y
l
t
c
e
r
r
o
C

d
e
ﬁ
i
s
s
a
l
C

99.9%
[1.00]

100.0%
[1.00]

100.0%
[1.00]

Incorrectly Classiﬁed

y
l
t
c
e
r
r
o
C

d
e
ﬁ
i
s
s
a
l
C

Incorrectly Classiﬁed

HP printer: 0.1% [0.52]

99.7%
[1.00]

Non IoT: 0.1% [0.37]
Dropcam: 0.1% [0.43]

100.0%
[1.00]

100.0%
[1.00]

Dropcam: 100.0% [0.08]

97.7%
[0.99]

Non IoT: 1.5% [0.74]
Dropcam: 0.8% [1.00]

97.7%
[0.99]

Non IoT: 1.5% [0.60]
Netatmo cam: 0.8% [0.57]

Dropcam: 100.0% [0.08]

Dropcam: 100.0% [0.08]

Samsung cam: 0.3% [0.79]
Non IoT: 0.2% [1.00]

Belkin motion: 0.2% [1.00]

99.5%
[1.00]

99.8%
[1.00]

100.0%
[1.00]

74.0%
[0.96]

HP printer: 25.7% [0.52]
Others: 0.3% [0.41]

Dropcam: 100.0% [0.08]

Dropcam: 100.0% [0.08]

Dropcam: 100.0% [0.08]

100.0%
[1.00]

100.0%
[1.00]

100.0%
[1.00]

Dropcam: 0.3% [0.08]

99.8%
[1.00]

Pixstar photo: 0.1% [0.54]
Dropcam: 0.1% [0.60]

August doorbell: 99.7% [0.71]
Dropcam: 0.3% [0.08]

100.0%
[1.00]

Dropcam: 100.0% [0.08]

100.0%
[1.00]

99.8%
[1.00]

99.8%
[1.00]

100.0%
[1.00]

100.0%
[1.00]

100.0%
[1.00]

100.0%
[1.00]

100.0%
[1.00]

99.9%
[1.00]

100.0%
[1.00]

100.0%
[1.00]

Non IoT: 0.2% [0.97]

Belkin motion: 0.2% [0.93]

Hue bulb: 0.1% [0.37]

Dropcam: 100.0% [0.08]

99.8%
[1.00]

LiFX bulb: 0.1% [0.88]
Dropcam: 0.1% [0.97]

99.8%
[1.00]

LiFX bulb: 0.1% [0.71]
Dropcam: 0.1% [0.67]

Dropcam: 100.0% [0.08]

Dropcam: 100.0% [0.08]

100.0%
[1.00]

100.0%
[1.00]

18.0%
[0.57]

0.0%
[-]

42.3%
[0.33]

7.8%
[1.00]

0.0%
[-]

21.4%
[0.99]

59.5%
[0.79]

Dropcam: 82.0% [0.08]

89.8%
[1.00]

Dropcam: 9.8% [0.96]
HP printer: 0.4% [0.52]

Dropcam: 100.0% [0.08]

Dropcam: 57.7% [0.08]

Dropcam: 92.2% [0.08]

Dropcam: 100.0% [0.08]

Dropcam: 78.6% [0.08]

Dropcam: 36.3% [0.08]
Others: 4.2% [0.73]

100.0%
[1.00]

99.1%
[1.00]

100.0%
[1.00]

100.0%
[0.90]

14.3%
[0.97]

98.8%
[1.00]

Dropcam: 0.9% [0.54]

HP printer: 78.6% [0.52]
Dropcam: 7.1% [0.61]
HP printer: 1.1% [0.56]
Dropcam: 0.2% [0.75]

100.0%
[0.99]

100.0%
[1.00]

100.0%
[1.00]

100.0%
[1.00]

100.0%
[0.85]

92.9%
[0.99]

99.7%
[0.99]

Hue bulb: 7.1% [0.35]

HP printer: 0.3% [0.55]

69

98.0%
[1.00]

Netatmo weat.: 1.2% [0.37]
Others: 0.8% [0.49]

100.0%
[1.00]

41.2%
[0.99]

Dropcam: 54.8% [0.08]
Netatmo weat.: 4.0% [0.16]

99.9%
[1.00]

Non IoT: 0.1% [1.00]

Dropcam: 0.4% [0.08]

Dropcam: 11.0% [0.08]

99.6%
[1.00]

89.0%
[1.00]

Dropcam: 76.5% [0.08]

Dropcam: 99.2% [0.08]

23.5%
[1.00]

0.8%
[0.71]

100.0%
[0.98]

100.0%
[1.00]

99.9%
[1.00]

87.5%
[1.00]

Non IoT: 0.1% [0.57]

Dropcam: 12.5% [0.69]

Dropcam: 96.2% [0.08]

2.5%
[0.45]

Dropcam: 97.1% [0.08]
Others: 0.4% [0.75]

99.3%
[0.82]

Dropcam: 0.4% [0.85]
Others: 0.2% [0.39]

99.8%
[0.99]

Non IoT: 0.1% [0.67]
Dropcam: 0.1% [0.28]

100.0%
[1.00]

100.0%
[1.00]

99.9%
[1.00]

100.0%
[1.00]

99.9%
[1.00]

87.5%
[0.98]

Non IoT: 0.1% [0.84]

Non IoT: 0.1% [0.47]

Dropcam: 12.5% [0.57]

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

server-side port numbers for device classiﬁcation, a reasonable accuracy of 92.13%

is achieved, but RRSE is poor (at 39.93%). Inspecting the individual classes, we

observe that certain classes highlighted by yellow or light-green (e.g. Ring door bell,

Blipcare BP monitor, Hello Barbie, and Google chromecast) are poorly classiﬁed.

We explain the reason behind this misclassiﬁcation next.

Ring door bell: Out of 486 instances, 465 contain a single occurrence of the

DNS query (i.e. remote port number 53). We see that 95.8% of test instances are

incorrectly classiﬁed as Netatmo weather station. This is because of two reasons:

(i) there are 2451 training instances of Netatmo compared to 323 of Ring door

bell, which makes Pr(ctrain

i

) of Netatmo larger than that of Ring door bell, and (ii)

many Netatmo instances contain several (on average 4 times) occurrences of port

53 as opposed to only one for Ring Door bell, which also contributes to Pr(wj

ci)
|

of Netatmo being greater than that for Ring door bell in (3.1). Thus, Ring door

bell instances get classiﬁed as Netatmo weather station, warranting a second stage

of classiﬁcation with additional attributes for improved accuracy.

Blipcare BP monitor: It uses only two remote port numbers, namely 8777

and 53, in a total of 13 instances - the port numbers appear only once or twice

in each instance. Surprisingly, we see that 80% of Blipcare test instances are in-

correctly classiﬁed as Ring Door Bell though the remote port number of 8777 is

unique to the Blipcare BP monitor. This is because there are only a very small

number of Blipcare instances in our dataset, which results in a fairly small value

Blipcare) = 0.0294 in (3.1), and
Blipcare) = 0.0203 and Pr(“8777”
of Pr(“53”
|
|

a negligible value of Pr(Blipcaretrain) = 0.0003 in (3.2). On the other hand,

Ring) becomes very small as the remote port number 8777 is never used
Pr(“8777”
|

by the Ring Door Bell in our dataset. However, the probability of Pr(“8777”
Ring) =
|

0.0011 in (3.1) is suﬃcient enough to maximize the classiﬁer probability Pr(Ring

“53” :

|{

1, “8777” : 1

}

) in (3.2), given Pr(Ringtrain) = 0.0097.

70

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

Other devices: Server-side port numbers are empty in 72% of instances for

Hello Barbie, since it communicates with local devices instead of Internet-based

end-points. Similarly for HP printer (38%) and iHome power plug (10%). The lack

of server-side port number information explains why these devices are classiﬁed as

Dropcam, which has the highest value of Pr(Dropcamtrain) = 0.0828 in (3.2). We

note that the conﬁdence level of our stage-0 classiﬁer is fairly low (i.e. less than 0.4)

in these cases, suggesting that the classiﬁer chooses the most probable class given

empty attribute (i.e. all ntest

j

are zero).

Performance of Stage-0: Domain Names Attribute

We now focus on the stage-0 machine that uses only a bag of domain-names, which

yields an accuracy of 79.48% with a fairly high RRSE value of 57.56%, as shown in

the second column in Table 3.2. In this scenario, more classes suﬀer from misclas-

siﬁcation (i.e. those with yellow coloured cells) compared to the previous scenario

where only remote port numbers were considered. The reasons behind the misclassi-

ﬁcation are threefold: (i) since devices from the same manufacturer share a collection

of domain names, as discussed in §3.3.2, 59.8% of Belkin camera test instances are

misclassiﬁed as Belkin Motion sensor and 100% Belkin Motion sensor instances are

misclassiﬁed as Belkin switch. Similarly, 56.8% of Withings scale instances are in-

correctly classiﬁed as Withings sleep sensor, and 12% of Samsung smart cam are

misclassiﬁed as Samsung Smartthings. (ii) a signiﬁcant number of instances from

select devices contain no DNS query entries (e.g. 96.2% of HP printer, 73.4% of

Samsung Smart Cam, 71.4% of Hello Barbie, 12.5% of iHome power plug, 11% of

Hue bulb) and are thus incorrectly classiﬁed as a Dropcam, which also rarely gen-

erates DNS packets. (iii) the low number of training instances with domain names

leads to poor performance (e.g. Blipcare BP meter and Hello Barbie).

71

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

Performance of Stage-0: Cipher Suite Attribute

Considering only the cipher suite attribute, this stage-0 classiﬁer results in a fairly

low accuracy of 36.15% with a high RRSE of 86.73%, as shown in the third column

in Table 3.2. Again, the main reason for such poor performance is the scarcity of

cipher suite attribute in the training instances, though this attribute carries a very

strong signature to uniquely identify an IoT device. Note that many of the IoT

devices do not use secure communication at all and are thus devoid of this attribute

(i.e. have an empty ﬁeld for it). Unsurprisingly, instances of devices that exchange

cipher suite fairly frequently including Amazon Echo, Awiar air quality monitor,

Canary camera, Google Chromecast and Netatmo camera are correctly classiﬁed, as

shown by the dark-green color cells in the corresponding column in Table 3.2. In

addition, we ﬁnd that August doorbell cam is sharing one of its cipher suite strings

(out of total 18) with Pixstar photoframe, which has a single cipher suite string.

Thus, 21.2% of August door bell instances are misclassiﬁed as Pixstar photoframe

and almost all instances of Pixstar photoframe are classiﬁed as August doorbell.

Performance of Stage-0: Combination of Attributes

We expect the combination of the three bags of words (port numbers, domain names,

and cipher suites) to signiﬁcantly enhances the accuracy of our classiﬁer, as indeed

shown by the fourth column titled “Combined stage-0” in Table 3.2. The overall

accuracy reaches to 97.39% with RRSE of 18.24%. It can be seen that the majority

of test instances are correctly classiﬁed, except for Hello Barbie. This is because

most of the Hello Barbie attributes are empty in stage-0 and thus it is classiﬁed as

Dropcam, as mentioned earlier.

Interestingly, we see that all test instances of Blipcare BP monitor are classiﬁed

correctly though the accuracy of individual stage-0 was fairly poor. This is because

our decision-tree-based classiﬁer in stage-1 sees a strong correlation between the

72

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

outputs of stage-0 classiﬁers and the actual class of training instance, even though

those outputs (tentative class) are incorrect – e.g. having the tentative output from

remote port number classiﬁer as Ring door bell, having the tentative output from

cipher suite classiﬁer as Dropcam, and having the conﬁdence level from domain

name classiﬁer less than 0.66 collectively is a strong indication of Blipcare instance.

Overall Performance

As the last step, we incorporate the outputs from the stage-0 classiﬁers into stage-1

(without the latter having any notion of the quantitative attributes from the former),

and additionally include quantitative attributes (ﬂow volume, duration, rate, sleep

time, DNS and NTP intervals). The last column of Table 3.2 shows the overall

performance of the classiﬁcation framework. In this case, the accuracy reaches a

remarkably high value of 99.88%, with almost all classes labeled correctly with a

very small value of RRSE at 5.06%. Fig. 3.10 shows the full confusion matrix of

our classiﬁcation when all the attributes are used in conjunction, and corroborates

that the diagonal entries (corresponding to correct classiﬁcation) are all at or very

close to 100%, with just two exceptions – the Google Chromecast and the Hello

Barbie. As explained earlier, the Chromecast gets classiﬁed as the Dropcam in some

instances, while the Hello Barbie gets classiﬁed as a Hue bulb.

3.5 Real-Time Operation in a Network

Thus far, we have examined the performance of our multi-stage classiﬁer using oﬀ-

line analysis on captured traﬃc traces (i.e. pcap ﬁles). In this section, we discuss

how one can realize a real-time implementation of our system taking into account

the various stages involved in the analysis, namely attribute collection, machine

training, and interpreting the classiﬁer’s output.

73

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

Figure 3.10: Confusion matrix of our IoT device classiﬁcation using all attributes
(accuracy: 99.88%, RRSE: 5.06%).

3.5.1 Computing Attributes

Extracting the attributes on-the-ﬂy requires infrastructure that has suﬃcient visi-

bility into the traﬃc ﬂowing on the network. Flow related attributes such as ﬂow

volume, ﬂow duration and ﬂow rate can be extracted relatively easily using network

switches that are instrumented with special hardware-accelerated ﬂow-level analyz-

ers, e.g. NetFlow capable devices [158]. We therefore deem the extraction cost of

ﬂow related attributes to be fairly low, and show them via blue color bars in Fig. 3.11

that depicts the relative costs and merits of the various attributes.

74

AmazonEchoAugustdoorbellAwairairqualityBelkincamBelkinmotionBelkinswitchBlipcareBPCanarycamDropcamChromecastHelloBarbieHPprinteriHomeLiFXbulbNESTsmokeNetatmoweat.NetatmocamHuebulbPixstarphotoRingdoorbellSamsungcamSmartThingsTPlinkcamTPlinkswitchTribyspeakerWithingssleepWithingsbabymon.WithingsscalenonIoTClassiﬁedasAmazonEchoAugustdoorbellAwairairqualityBelkincamBelkinmotionBelkinswitchBlipcareBPCanarycamDropcamChromecastHelloBarbieHPprinteriHomeLiFXbulbNESTsmokeNetatmoweat.NetatmocamHuebulbPixstarphotoRingdoorbellSamsungcamSmartThingsTPlinkcamTPlinkswitchTribyspeakerWithingssleepWithingsbabymon.WithingsscalenonIoTActualinstances99.70.00.00.00.00.00.00.00.10.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.10.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.097.70.00.00.00.00.00.00.00.00.00.00.00.00.80.00.00.00.00.00.00.00.00.00.00.01.50.00.00.00.099.80.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.20.00.00.00.00.299.80.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.012.587.50.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.092.90.00.00.00.00.00.07.10.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.10.00.099.80.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.10.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.099.90.10.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.099.90.00.00.00.00.00.00.00.00.00.00.10.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.10.00.00.00.00.10.00.00.00.00.00.00.099.80.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.099.90.00.00.00.10.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.30.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.099.7Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

Figure 3.11: Merit of attributes.

Attributes including bag of port numbers, sleep-time, and frequency of DNS/NTP

requests can be extracted using ﬂow-aware network switches with extra computation

and state management. For example, remote port numbers of all ﬂows associated

with a given IoT device need to be recorded for the bag of port numbers. How-

ever, this speciﬁc state is not captured by default in commodity switches. Similarly,

time intervals between successive UDP packets of NTP/DNS should be recorded,

which requires additional computation. We therefore associate these attributes with

medium cost, and shown as yellow color bars in Fig. 3.11.

Lastly, two of our attributes, namely bag of domain names and bag of cipher

suite strings, can only be extracted by looking inside the payload of the appropriate

packets, which imposes considerable cost on processing. Thus, we associate these

attributes with high collection cost, and shown them via red color bars in Fig. 3.11.

Having understood the extraction cost of various attributes, let us now examine

the relative importance of the attributes in classifying the IoT devices. We quantify

the importance of each attribute by employing the select attributes tool in Weka

with InfoGain attribute evaluator and Ranker search method. Fig. 3.11 shows the

attributes in decreasing order of merit score. A high merit score translates to superior

strength in identifying the class of an instance. We can see that the “ﬂow-volume”

is the most important attribute, followed by “bag of remote port numbers”, “bag of

domain names” and “ﬂow duration” respectively. The sleep-time and NTP interval

are the attributes with the lowest merit.

75

flow volumeport-nums classdomain-names classflow-durationmean-flow-ratedomain-names confid.DNS intervalbag-of-CS classbag-of-CS confid.port-nums confid.sleep-timeNTP interval012345 MeritLow costMedium costHigh costChapter 3.

IoT Traﬃc Characterization and Classiﬁcation

Table 3.3: Impact of attributes combination on performance of classiﬁer.

Accuracy RRSE

all attributes

99.88%

low- and medium-cost attributes

99.68%

5.06%

7.70%

only low-cost attributes

97.85%

18.63%

Knowing the relative cost and merit of each attribute allows us to evaluate the

performance of our classiﬁer using: (a) only low cost attributes, (b) combination of

low and medium cost attributes, and (c) all attributes. The classiﬁer accuracy and

RRSE are shown in Table 3.3. It is seen that using only low-cost attributes results

in 97.85% accuracy with an RRSE value of 18.63%; the additional use of medium-

cost attributes increases accuracy to 99.68% and signiﬁcantly reduces the RRSE

error to 7.7%; while including all attributes yields an overall accuracy of 99.88% and

RRSE of 5.06%. The method can therefore be tuned to achieve appropriate balance

between attribute collection cost and accuracy/error of classiﬁcation.

3.5.2 Training the Machine

The duration of the training data set is another source of cost incurred by our

classiﬁcation. In Fig. 3.12a, we plot the accuracy of the classiﬁer on the left y-axis

and the RRSE on the right y-axis as a function of the number of days involved in

collecting the training data set. Note that the x-axis is in log-scale and each day

represents 24 instances.

It can be seen that the classiﬁer achieves an overall accuracy is 99.28% with

only one day of training and saturates at 99.76% when trained over 16 days. On

the other hand, RRSE drops from 14.43% to 7.5% when the training duration is

increased from 1 day to 16 days. It further falls to 5.82% when we train using 70%

of all instances from 128 days. As mentioned in §3.4, the RRSE value is sensitive to

the accuracy of individual classes. We therefore believe that if there is a balanced

number of instances from various classes, our classiﬁer would perform better in terms

of RRSE.

76

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

(a) Impact of training data.

(b) Conﬁdence of classiﬁer.

Figure 3.12: Operational insights for real-time implementation of our device classi-
ﬁer: (a) impact of training and (b) conﬁdence-level for correct/incorrect classiﬁca-
tion.

3.5.3

Interpreting the Output of Classiﬁer

As discussed in §3.4.1, our classiﬁer generates a conﬁdence level during the testing

phase. This can be used as a measure of reliability for our classiﬁer. If adequate

information is not provided by a test instance then the classiﬁer will choose a random

class (as discussed in §3.4.2) with a low conﬁdence level - this can be interpreted as

an “unknown” class. For example, given instances with an empty value for the cipher

suite attribute, the corresponding stage-0 classiﬁer will output Dropcam class with

a conﬁdence value of less than 10% - even for Dropcam instances that are classiﬁed

correctly the conﬁdence level is low within the same range.

We plot the CCDF of conﬁdence level of our stage-1 classiﬁer in Fig. 3.12b for

77

1248163264128Num. of training days9092.59597.5100Accuracy (%)57.51012.515RRSE (%)AccuracyRRSE30405060708090100X: confidence (%)10-210-1100CCDF: Prob [confidence > x]Correctly classifiedIncorrectly classifiedChapter 3.

IoT Traﬃc Characterization and Classiﬁcation

instances classiﬁed as correct and incorrect. It is clearly seen that the conﬁdence

level is always below 80% when an instance is incorrectly classiﬁed, as shown by the

red dotted line - the average conﬁdence level for incorrectly classiﬁed instances is

54.22%. On the other hand, our classiﬁer has an average 99.74% conﬁdence level

for instances that are correctly classiﬁed. We note that for only a negligible fraction

of correctly classiﬁed instances (i.e. 0.37%) the conﬁdence level is less than 80% as

shown by the blue dashed line. This suggests that we can comfortably rely on our

classiﬁer’s output for a device if it results in a conﬁdence level of greater than 80%,

otherwise we need to collect more traﬃc (and richer instances) from that device in

order to increase the conﬁdence level.

To demonstrate the ability of our classiﬁer in detecting changes of normal be-

havior, we have launched UDP reﬂection and TCP SYN attacks of varying rates on

the Samsung camera. When our classiﬁer is fed these attributes during the attack,

it incorrectly identiﬁes the device, but its conﬁdence-level drops to less than 50%.

We note that the conﬁdence level is 100% for normal traﬃc from Samsung camera,

as shown in the last column of Table 3.2. This is taken as a sign of anomalous

behavior that warrants further investigation by the network operator. Note that the

anomaly detection is not the primary focus of this chapter, and hence our evaluation

is limited to only one attack type. Later in Chapter 5 we will study in detail several

attacks when we evaluate our anomaly detection scheme.

3.6 Conclusion

Despite the proliferation of IoT devices in smart homes, enterprises, campuses, and

cities around the world, operators of such environments lack visibility into what IoT

devices are connected to their networks, what their traﬃc characteristics are, and

whether the devices are functioning appropriately free from security compromises.

This work is the ﬁrst to systematically characterize and classify IoT devices at run-

78

Chapter 3.

IoT Traﬃc Characterization and Classiﬁcation

time. We instrumented a smart environment with 28 unique IoT devices and col-

lected traﬃc traces continuously over 26 weeks. We then statistically characterized

the traﬃc in terms of activity cycles, signalling patterns, communication protocols

and cipher suites. We developed a multi-stage machine learning based classiﬁcation

framework that uniquely identiﬁes IoT devices with over 99% accuracy. Finally,

we evaluated the real-time operational cost, response time, and accuracy trade-oﬀs

of our classiﬁcation method. This chapter shows that IoT devices can be identi-

ﬁed with high accuracy based on their network behavior, and sets the stage for

detecting misbehaviors resulting from security breaches in the smart environment.

However, obtaining all of these characteristics using specialized hardware accelera-

tors in real-time becomes more expensive, and unscalable due to the need of deep

packet inspection. In the following chapter, we will discuss an approach to monitor

the devices with low-cost attribute extraction.

79

Chapter 4

Behavioral Monitoring using

Low-Cost Attributes

Contents

4.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

4.2 Traﬃc Flows and Attributes

. . . . . . . . . . . . . . . . . . . . 84

4.2.1 Traﬃc Trace Dataset . . . . . . . . . . . . . . . . . . . .

4.2.2 Traﬃc Flows and Attributes . . . . . . . . . . . . . . . .

4.2.3 Traﬃc Characteristics of IoT Devices . . . . . . . . . . .

84

86

89

4.3

IoT Traﬃc Inference Engines . . . . . . . . . . . . . . . . . . . . 93

4.3.1

Inference Architecture

. . . . . . . . . . . . . . . . . . .

4.3.2 Models Training and Performance Evaluation . . . . . .

93

95

4.4 Practical and Operational Considerations . . . . . . . . . . . . . 103

4.4.1 Cost of Attributes

. . . . . . . . . . . . . . . . . . . . . 103

4.4.2 Use of Inference Engines in Real-Time . . . . . . . . . . 108

4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

In the previous chapter, we distinguish the types of IoT devices based on their

network characteristics. However to secure the IoT devices, we need to detect their

anomalous activities and behavioral drifts. This turns out to be non-trivial as it (a)

81

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

requires an inference engine that monitors the ﬁne-grained activities of the devices

(b) should incur a low-cost computational cost for telemetry. The existing IoT

traﬃc monitoring techniques use specialized acceleration on network switches, or

full inspection of packets in software, which can be complex, expensive, inﬂexible,

and unscalable. In this chapter, we use an SDN paradigm combined with machine

learning to leverage the beneﬁts of programmable ﬂow-based telemetry with ﬂexible

data-driven models to manage IoT devices based on their network activity. Our

contributions are three-fold: (1) We analyze traﬃc traces of 17 real consumer IoT

devices collected in our lab over a six months period and identify a set of traﬃc ﬂows

(per-device) whose time-series attributes computed at multiple timescales (from a

minute to an hour) characterize the network behavior of various IoT device types,

and their operating states (i.e., booting, actively interacted with user, or being idle);

(2) We develop a multi-stage architecture of inference models that use ﬂow-level

attributes to automatically distinguish IoT devices from non-IoTs, classify individual

types of IoT devices, and identify their states during normal operations. We train

our models and validate their eﬃcacy using real traﬃc traces; and (3) We quantify

the trade-oﬀ between performance and cost of our solution, and demonstrate how

our monitoring scheme can be used in operation for detecting behavioral changes

(ﬁrmware upgrade or cyber attacks). Parts of this chapter have been published in

[7] and [8].

4.1 Introduction

The lack of eﬀective security on IoTs [30, 36, 150] presents a number of challenges

for network operators of large organizations who are looking to bring these devices

online at scale.

Implementing device-level security would deﬁnitely help protect

against automated attacks [159], but its eﬃcacy can vary across manufacturers and

device types depending upon devices capabilities and their mode of operation [72].

82

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

In a parallel eﬀort, IETF has approved an Internet standard called “Manufacturer

Usage Description" (MUD) [83] to protect IoT devices. This framework allows

manufacturers to formally specify the intended behavior of their devices that can be

used to generate and enforce access control lists (ACLs) [84] for IoT devices, limiting

their network behavior to only a tight set of services. Although MUD policies can

reduce the surface of attacks on IoTs they are still insuﬃcient, since ACL rules do

not restrict temporal variation of traﬃc ﬂows (e.g., traﬃc with unwanted volume or

pattern cannot be prevented if endpoints and protocols conform to MUD rules).

Therefore, it is crucial for organizations to maximize visibility into their IoT

infrastructure [6], and thus better manage security risks of these vulnerable de-

vices [18]. Network administrators need to know all connected devices and their ex-

pected operations on the network, and continuously monitor their activities ensuring

IoTs behave “normally” [18]. Existing traﬃc monitoring solutions are either purely

software-based (hence unscalable to high traﬃc rates) or customized hardware-based

(hence inﬂexible and expensive) [123]. Network operators, today, widely use Net-

Flow [122] (an embedded switch instrumentation) to obtain aggregate measurement

of traﬃc ﬂows. However, it comes at cost of CPU resources on the switch [160] for

generating, collating, and exporting ﬂow records. To reduce this overhead, opera-

tors statistically mirror packet samples (e.g., sFlow [121]) to a remote collector for

extracting ﬂow information that inevitably leads to reduced accuracy. On the other

hand, special-purpose hardware appliances (i.e., deep packet inspection engines) of-

fer both accuracy and performance in traﬃc monitoring but they are prohibitively

expensive for many network operators.

In this chapter, we aim to monitor behavior of IoT devices on the network using

a combination of Software Deﬁned Networking (SDN) telemetry and machine learn-

ing methods. We believe that the SDN paradigm by its nature provides ﬂow-level

isolation and visibility in a low-cost and scalable manner. For accurate detection

of devices and tracking of their dynamic behaviors, we employ machine learning

83

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

algorithms to learn key patterns of traﬃc ﬂows. Our ﬁrst contribution is to identify

a set of TCP and UDP ﬂows (for each IoT device) and highlight characteristics at-

tributes, computed from time-series of ﬂows at multiple time-scales, distinguishing

various IoT device types and their behavioral states (booting, active, or idle) on

the network. Our second contribution develops a multi-stage architecture consisting

of a set of inferencing models that use ﬂow-level attributes to automatically recog-

nize traﬃc of IoT devices from non-IoTs, classify types of IoT devices, and identify

operating states of each IoT during normal operation. We train our models and

validate their performance to obtain high accuracy using real traﬃc traces. Finally,

we demonstrate the eﬃcacy of our scheme in detecting network behavioral changes

due to ﬁrmware upgrade or cyber-attacks. Also, we quantify the trade-oﬀ between

performance and cost of our monitoring solution for real-time deployment.

The rest of this chapter is organized as follows: In §4.2 we present our dataset

and traﬃc ﬂows, and characterize attributes of various IoT devices and their oper-

ating states. We propose the architecture of IoT traﬃc inference and evaluate its

performance in §4.3, followed by a discussion on the operational trade-oﬀ and use

of the proposed system in §4.4. The chapter is concluded in §4.5.

4.2 Traﬃc Flows and Attributes

In this section, we begin by analyzing real traﬃc traces collected in our lab. We

then identify traﬃc attributes to distinguish IoT devices from non-IoTs, classify

individual IoTs, and determine their operating states.

4.2.1 Traﬃc Trace Dataset

We used two sets of full PCAP traﬃc traces collected from our testbed. The ﬁrst

dataset (i.e., DATA1) was collected from a network consisting of more than thirty

84

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

IoT and non-IoT devices for a duration of six months (i.e., 01-Oct-2016 to 31-Mar-

2017) [6]. We select 17 IoT devices, those whose trace was present for at least 60

days in packet traces. These devices include Amazon Echo, August doorbell, Awair

air quality, Belkin motion sensor, Belkin switch, Dropcam, HP printer, LiFX bulb,

NEST smoke sensor, Netatmo weather, Netatmo camera, Hue bulb, Samsung smart

camera, Smart Things, Triby speaker, Withings sleep sensor, and Withings scale.

Note that our dataset contains traﬃc traces of six non-IoT devices including Android

phone, Android tablet, Windows laptop, MacBook, and two iPhones.

The second dataset (i.e., DATA2) consists of traces with state annotation for

selected IoT devices including Amazon Echo, Belkin switch, Dropcam, and LiFX

bulb. We developed a software tool to automatically interact with these four devices

over two days and annotate their traﬃc traces. Annotations indicate three operating

states of IoT devices, namely “boot” (i.e., getting connected to the network), “active”

(i.e., interacting with users), and “idle” (i.e., not being booted or actively used).

The main purpose of state classiﬁcation is to monitor the activity of IoT devices

at a ﬁne-grained level, augmenting the device classiﬁcation model. We believe that

boot, active, and idle states are generic and across all IoT devices. Hence, the state

classiﬁer is able to capture minor variations in the activity patterns of a device

regardless of various attack models.

For the boot state, we used a TP-Link HS110 smart plug (whose traﬃc is not

considered in our analysis) supplying power to these four devices. We wrote a script

to automatically turn oﬀ/on this smart plug resulting a boot state for the subjected

IoT device. For the active state, we used an app called “RepetiTouch Pro” and a text

to-speech engine called espeak[161]. The former records and replays interactions of a

real user with three IoT devices including Belkin switch, Google Dropcam camera,

and LiFX lightbulb via their manufacturer app – the user interactions (i.e., turning

on/oﬀ the switch, streaming video from the camera, and turning on/oﬀ the bulb)

were recorded on an Android tablet which was connected to the local network of our

85

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

testbed. The latter periodically asks scripted questions (e.g., “How is the weather in

Sydney, Australia”) from Amazon Echo. For the idle state, we used all traﬃc traces

that were annotated as neither boot nor active, during the data collection period.

The diﬃculty of capturing all possible user interactions in the active state, led us to

limit the number of devices studied to only four.

4.2.2 Traﬃc Flows and Attributes

In Chapter 3 we showed that individual IoT devices exhibit identiﬁable patterns in

their traﬃc ﬂows such as DNS/NTP/SSDP signaling proﬁles, activity cycles, and

volume patterns. Although, these attributes contain a rich set of information to

ﬁngerprint IoTs and their activities, they incur high cost of extraction from the

network in real-time.

In this chapter, our attributes are computed for individual 2-tuple and/or 3-tuple

ﬂows (coarse-grained). Note that this diﬀers from our attributes in Chapter 3 where

IoT traﬃc attributes such as activity volume and average ﬂow rates were computed

for individual 5-tuple ﬂows (ﬁne-grained). The attributes employed in Chapter 3

provide a richer set of information, and hence the trained models yield a very high

accuracy. However, those attributes are expensive to extract and compute from

the network traﬃc in real-time. Instead, the attributes we identify and use in this

chapter are largely at aggregate level (more cost-eﬀective) but give a slightly lower

accuracy (still reasonably acceptable) when compared to results in Chapter 3.

As we mention in the Chapter 2, ﬂow-level telemetry provided by SDN APIs [127]

enables us to dynamically measure speciﬁc traﬃc ﬂows at low-cost with reasonable

resolutions. Also, we note that SDN-enabled switches that are currently available

in the market typically support a large number of ﬂow rules without experiencing

performance degradation. For example, a NoviSwitch provides massive table with

up to 1 million ﬂow rules in TCAM for wildcard matches while oﬀering up to 400

86

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

Figure 4.1: System architecture of network telemetry and inference engines.

Gbps throughput. This means that with insertion of 8 OpenFlow rules, one switch

can essentially manage monitoring of more than 100K IoT devices.

Inspired by recent proposals [7, 162] on network telemetry using SDN, we consider

a set of ﬂow rules that collectively characterize traﬃc signature of IoT devices. We

choose eight low-cost ﬂow entries that can cover a subset of most commonly used

signaling and activity patterns of the IoT device that we studied in Chapter 3. For

each device, these ﬂow rules are pro-actively inserted into SDN-enabled switch(es)

to which IoT devices are connected, as shown in Fig. 4.1. We use MAC address as

the identiﬁer of a device – one may use IP address (without NAT), physical port

number, or VLAN for a one-to-one mapping of a physical device to its traﬃc trace.

For real-time monitoring, counters of these ﬂow rules are periodically (i.e., every

minute) measured via the SDN controller that will form traﬃc attributes of each

device.

Table 4.1 shows eight ﬂow rules which we use to measure network traﬃc of each

IoT device with the following order: (1,2) DNS outgoing queries and incoming

87

IoT State ClassificationNon-IoTSmart phones & tabletsLaptopsIoTHP EnvyprinterTribyspeakerAugust doorbellDropcamNetatmocameraSamsungsmartcamAmazonEchoSmartThingsWithingsscaleAwairair-qualityNetatmoweatherWithingsSleep-sensorNestsmoke-sensorBelkinmotion-sensorBelkin switchLIFXbulbHuebulbGatewaySDNswitchesInternetInferenceenginesIoTDetectionIoT Device ClassificationFlow-level TelemetrySDN controllertraffic attributesChapter 4. Behavioral Monitoring using Low-Cost Attributes

responses on UDP 53; (3,4) NTP outgoing queries and incoming responses on UDP

123; (5) SSDP outgoing queries on UDP 1900; (6,7) other “remote” (e.g., Internet)

traﬃc outgoing from and incoming to the device that passes through the gateway;

and (8) all “local” (i.e., LAN) traﬃc incoming to the device. Note that we do

not measure incoming SSDP traﬃc to IoT devices in order to avoid capturing (and

mixing with) discovery activities of other devices on the local network. Note that

rules priority (the second last column in Table 4.1) are used to split the traﬃc of each

device into three levels: signaling packets (i.e., priority 100), other remote packets

(i.e., priority 10), and local packets (i.e., priority 1).

For each of the eight ﬂows (mentioned above), we use two key attributes [5]

namely average packet size and average rate. Also, note that traﬃc attributes

can better characterize network behavior of individual devices if they are computed

at multiple time-scales [137]. We, therefore, collect packet counts and byte counts

per each ﬂow every minute, and compute attributes at time-granularities of 1-, 2-,

4-, 8-, 16-, 32-, 64-minutes. This way, we generate fourteen attributes for each ﬂow

which means a total of 112 attributes per device.

In order to synthesize ﬂow rules, we wrote a native SDN simulator [163] that

takes an input PCAP trace, and performs packet-by-packet service (matching packet

headers against ﬂow table entries, updating statistics, applying required actions)

Table 4.1: Flow rules speciﬁc to each device, proactively inserted into SDN switch
for real-time telemetry.

Flow description

srcETH dstETH srcIP dstIP Protocol

srcPort dstPort Priority Action

DNS query (DNS
↑

DNS response (DNS

)

)
NTP query (NTP
↑

NTP response (NTP

SSDP query (SSDP
↑

outgoing remote (Rem.

↑
incoming remote (Rem.

)
incoming local (Loc.
↓

)

↓

)
↓
)

<devMAC>

*

<devMAC>

*

<devMAC>

*

<devMAC>

*

<devMAC>

*

)

<devMAC>

<gwMAC>

↓

)

<gwMAC>

<devMAC>

*

<devMAC>

17

17

17

17

17

*

*

*

*

53

*

123

*

*

*

*

53

*

123

*

1900

*

*

*

100

100

100

100

100

10

10

forward

forward

forward

forward

forward

forward

forward

1

forward

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

88

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

(a) Volume of remote traﬃc at 32-mins resolu-
tion.
Figure 4.2: Histogram of traﬃc proﬁle to compare IoT and non-IoT devices: (a)
remote traﬃc volume over 32-minute; and (b) DNS query count over 64-minute.

(b) Count of DNS query packets at 64-mins res-
olution.

inside a software SDN switch. The simulator records counters of ﬂow bytes and

packets periodically (e.g., one minute). Using the output of the simulator, we use

another script to generate instances of traﬃc attributes for each device every minute.

An instance is a vector of 112 attributes with a label (e.g., Amazon Echo:boot).

4.2.3 Traﬃc Characteristics of IoT Devices

We now highlight traﬃc characteristics of individual IoT devices that can be learned

so as to distinguish them from non-IoTs, classify their device type, and identify their

operating states.

IoT versus non-IoT: We begin with traﬃc attributes that diﬀerentiate IoT

devices from non-IoTs. Fig. 4.2 shows the probability density of two representative

attributes, namely remote traﬃc volume at 32-minute resolution, and DNS query

count at 64-minute resolution. We can see in Fig. 4.2a that IoT devices tend to

transfer a small volume of traﬃc from remote (i.e., Internet) network and 90% of

instances they download less than 500 KB every half-an-hour. However, for non-

IoTs this value is widely spread between 10 KB to 100 MB and mostly they transfer

more than 500 KB. In terms of DNS activity in Fig. 4.2b, IoTs display identiﬁable

89

100102104106X: volume of incoming remote (KB)00.050.10.150.20.25Probability: P(volume = x)IoTnon-IoT100101102103104X: count of DNS query00.050.10.150.20.25Probability: P(count = x)IoTnon-IoTChapter 4. Behavioral Monitoring using Low-Cost Attributes

(a) Count of NTP response packets (16-min).

(b) Volume of remote traﬃc (8-min).

(c) Volume of SSDP traﬃc (8-min).

Figure 4.3: Histogram of traﬃc proﬁle for representative IoT devices: (a) NTP
response count, (b) download volume of remote traﬃc, and (c) upload volume of
SSDP traﬃc.

patterns of query count mostly less than 100 (e.g., 22% of instances with four queries

per hour), while non-IoTs have a wider range of DNS query count (i.e., 10 to 3000

DNS queries over an hour) with almost equal probabilities.

IoT device types: Focusing on IoT devices, we now consider three traﬃc at-

tributes, namely NTP responses count at 16-min resolution, upload volume of remote

traﬃc at 8-min resolution, and volume of SSDP responses at 8-min resolution, as

shown in Fig. 4.3. We quantitatively compare traﬃc characteristics of four repre-

sentative IoT devices from three diﬀerent manufacturers (i.e., Amazon, Belkin, and

LiFX). It is observed from Fig. 4.3a that the LiFX bulb (depicted by solid green

lines) sends three NTP responses every 16-minute interval for more than 90% of

90

100101102X: count of NTP response00.20.40.60.81Probability: P(count = x)Amazon EchoBelkin switchBelkin motionLiFX bulb100102104106X: volume of outgoing remote (Bytes)00.20.40.60.81Probability: P(volume = x)Amazon EchoBelkin switchBelkin motionLiFX bulb100102104106X: volume of outgoing SSDP (Bytes)00.20.40.60.81Probability: P(volume = x)Amazon EchoBelkin switchBelkin motionLiFX bulbChapter 4. Behavioral Monitoring using Low-Cost Attributes

instances. This measure varies between 7 to 42 responses for Amazon Eco (depicted

by dashed red lines). For Belkin power switch and motion sensor (depicted by solid

blue and dotted pink lines), we see two signiﬁcant peaks at a count of one and two

NTP responses, each with a diﬀerent probability – Belkin switch seems more active

(compared to Belkin motion), with 70% probability of generating 2 NTP responses

at 16-minute resolution.

For download volume of remote traﬃc attribute at 8-minute resolution, shown

in Fig. 4.3b, we see a relatively unique pattern in the probability density function

for each of these four devices: for Belkin switch and Belkin motion it peaks at 573

bytes and 3KB, respectively, while the LiFX bulb and Amazon Echo each exhibits

a range of values, [0.5, 3] KB and [7, 33] KB, respectively.

Considering the upload volume of SSDP traﬃc in Fig. 4.3c, the Belkin switch

seems distinctive from Belkin motion (i.e., probability of 82% for 8 KB volume in

Belkin switch compared to 73% chance for 800 bytes volume in Belkin motion).

Amazon Echo displays a strong pattern with a peak of 100% at volume of 650 bytes.

Lastly, we observe that LiFX does not use SSDP protocol at all, and thus lacks this

attribute in its traﬃc proﬁle.

Operating states of IoT: We now look at selected traﬃc attributes of Ama-

zon Echo, Belkin switch, and Dropcam at the three operating states, shown in

Fig. 4.4. We focus on download volume of remote traﬃc for Amazon Echo since

it frequently communicates with its cloud servers; download volume of local traﬃc

for Belkin switch since it receives command from user mobile app connected to the

local network; and upload volume of remote traﬃc for Dropcam since it tends to

send videos to its cloud servers. We can see that the three operating states are fairly

distinct in chosen attributes shown in Fig. 4.4. It is observed that all three devices

exchange smaller volume of traﬃc during their idle state (shown by dashed green

lines) compared to active and boot states. As an example,for Amazon Echo, shown

in Fig. 4.4a, 75% of idle instances receive between [0.5, 1] KB from remote servers

91

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

(a) Amazon Echo: remote traﬃc (2-min).

(b) Belkin switch: local traﬃc (1-min).

(c) Dropcam: remote traﬃc (2-min)

Figure 4.4: Histogram of traﬃc proﬁle for IoT devices at three operating states: (a)
Amazon Echo, (b) Belkin switch, and (c) Dropcam.

at 2-minute resolution, while the probability density function for boot and active

instances peaks at 30 KB and 70 KB, respectively. Additionally, we observe for

Belkin switch that the volume of local traﬃc during boot state is larger than active

state. This is because this device sends SSDP discovery when it boots up and that

results in the arrival of responses from all SSDP-capable devices on the network.

Therefore, a peak at 110 KB is seen for boot state (dotted red line) in Fig. 4.4b.

92

100102104106X: volume of incoming remote (Bytes)00.10.20.30.40.50.60.7Probability: P(volume = x)BootActiveIdle100102104106X: volume of incoming local (Bytes)00.10.20.30.40.50.60.7Probability: P(volume = x)BootActiveIdle100102104106X: volume of outgoing remote (Bytes)00.10.20.30.40.50.60.7Probability: P(volume = x)BootActiveIdleChapter 4. Behavioral Monitoring using Low-Cost Attributes

4.3 IoT Traﬃc Inference Engines

In this section, we develop a multi-stage architecture to automatically infer IoT

traﬃc, train a set of models, and evaluate their performance.

4.3.1

Inference Architecture

For a given device on the network, we have three objectives: (a) to determine if

the device is IoT or non-IoT, and if it is detected as IoT; (b) to classify its device

type (e.g., Amazon Echo, Dropcam); and (c) to identify the operating state of IoT

(i.e., boot, active, idle). To meet these objectives we need a set of trained models:

a bi-class classiﬁer to distinguish IoT devices from non-IoTs (i.e., IoT detector); a

multi-class classiﬁer to determine the type of a given IoT device (i.e., IoT classiﬁer);

and a set of multi-class classiﬁers to identify IoT operating states (i.e., a state

classiﬁer for each device type). Note that state classiﬁers are specialized models

and each learns traﬃc patterns of one device in the three states of operation. State

classiﬁers tend to have narrower views, and hence become more sensitive to change

of behavior for their respective devices (compared to the device classiﬁer with a

broader view). Therefore, these specialized models are able to enhance the visibility

of network operators into subtle changes [164] in their IoT infrastructure.

There exist a number of techniques [165] such as Neural Networks, Support

Vector Machines (SVMs), and Decision Trees that can be used to train models

to infer predeﬁned classes. Neural networks have proven to be very eﬀective in

classifying input data with high dimensions, but they demand a large amount of

training data. Also, neural networks are seen as black box models since it becomes

diﬃcult to interpret their reasoning process. Performance of SVMs is very sensitive

to the selection of hyper-parameters, and hence it becomes diﬃcult to train an

accurate model. On the other hand, decision tree-based techniques are widely used

since it is easier to generate (reasonably) accurate models with a relatively small

93

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

Figure 4.5: Hierarchical architecture of IoT traﬃc inference engines.

amount of data. Importantly, they generate trees which can be readily interpreted.

Note that decision tree algorithms are prone to over-ﬁtting which can be avoided

by the use of ensemble decision trees. In this work, we employ Random Forest [166]

which builds an ensemble of decision trees, each uses a random subset of attributes.

It is best known for its performance in various classiﬁcation tasks [6, 162, 167].

Fig. 4.5 illustrates our hierarchical architecture for IoT traﬃc inference. It con-

sists of three layers of random-forest classiﬁers (i.e., an IoT detector, an IoT classi-

ﬁer, and a set of IoT state classiﬁers). Once a new device connects to the network,

the programmable switch is pushed by additional ﬂow rules (Table 4.1) pertinent

to the device. We ﬁrst feed the IoT detector model by full set of periodic ﬂow-level

attributes (i.e., at time-scales of powers of two between 1-min to 64-min). Upon

detection of an IoT device with suﬃciently high conﬁdence (say, more than 80%),

the second model (i.e., IoT classiﬁer) is called by the full set of attributes – a device

will not be checked by the second layer of inference, if it is detected as non-IoT at

the ﬁrst layer. The output of the IoT classiﬁer triggers a pertinent state classiﬁer at

the third layer of our architecture. Our state classiﬁer models consume a subset of

attributes, however, it is only up to 4-minute resolution. This is because change of

states (e.g., boot) result in short-term eﬀects on device traﬃc pattern – considering

long-term attributes may reduce the ability of the model to accurately detect the

operating state in real-time.

94

IoTDetector IoTClassifierSpecializedState Classifiersnon-IoTIoTAmazon EchoDropcamBelkin switchLiFXbulbDropcamState ClassifierAmazon EchoState Classifier…Echo bootEcho activeEcho idleMatch Flows•DNS↑ byte/pktcount•SSDP↑ byte/pktcount•DNS↓ byte/pktcount•Remote↑ byte/pkt count•NTP↑ byte/pktcount•Remote↓ byte/pkt count•NTP↓ byte/pktcount•Local↓ byte/pkt count(at 1, 2, 4, 8, 16, 32, 64-min)(at 1, 2, 4-min)…StatesDropcambootDropcamactiveDropcamidleDevice traffic[attributes][attributes][attributes]Chapter 4. Behavioral Monitoring using Low-Cost Attributes

4.3.2 Models Training and Performance Evaluation

We now label instances to train our classiﬁers, and generate models needed for the

three layers of the inference architecture, as shown in Fig. 4.5. We next evaluate

their performance using test instances. For both training and testing the traﬃc

classiﬁers we use Weka [157] tool.

Instances: Recall from §4.2.2 that our instances are computed every minute.

Since two of our models consume full-set attributes (i.e., 1-min to 64-min) we down

sample our instances by the factor of 15 to avoid over-ﬁtting for the IoT detector

and the IoT classiﬁer – it is likely to have heavily-correlated instances generated

within 15 minutes. Note that the risk of over-ﬁtting is less for the state classiﬁers

given that they only use short timescale attributes.

We have collected a total of 115,237 instances of IoT and non-IoT devices from

our DATA1 (in §4.2.1) and 10,423 instances of four IoT devices (i.e., Amazon Echo,

Belkin switch, Dropcam, and LiFX bulb) with state annotation from our DATA2 (in

§4.2.1). We have a diﬀerent number of instances across various devices in our dataset,

depending upon their presence and activity on the testbed, and their interactions

with the lab users. Among all devices, NEST smoke-sensor has the lowest number

(i.e., 865) of instances since it communicates once a day for a short period of time.

The highest count belongs to Dropccam with 11,873 instances as it was online more

than 90% of days during the 6-month period of packet capture and it frequently

communicates with its cloud-servers whenever it is on the network.

Metrics: Since classes are not evenly distributed in our datasets, we use three

metrics including weighted “precision”, “recall ”, and “F1 score” along with confusion

matrix to evaluate the performance of each model. These metrics are deﬁned as

follows:

precision =

TP
TP + FP

(4.1)

95

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

recall =

TP
TP + FN

2

F1 =

precision

recall

×
precision + recall

×

(4.2)

(4.3)

where TP is the rate of true positive, FP is the rate of false positive, and FN is

the rate of false negative. Note that F1 conveys the balance between precision and

recall values and is computed by the harmonic mean of these two values in Eq. 4.3.

All metrics take a value between 0 and 1.

In addition to the correctness of classiﬁcation, we record conﬁdence-level of our

random-forest models for all instances, correctly classiﬁed and incorrectly classiﬁed

ones.

Ideally, we expect our models to display high conﬁdence (i.e., close to 1)

when they predict a correct class for an input instance, and low conﬁdence (i.e.,

close to 0) when they predict an incorrect class. Lack of conﬁdence indicates that

the tested instance contains attributes diﬀerent from those that were learned before

(i.e., new or unseen pattern). We next look at individual models at various layers

of the inference architecture

IoT Detector: In our DATA1, there exist 1212 instances labeled as non-IoT

and 114,025 instances labeled as 17 types of IoT. For the training set, we randomly

choose 800 instances (i.e., 66%) from non-IoT and 50 instances from each class of

IoT device (i.e., a total of 850). Remaining instances in DATA1 are used to test

this bi-class classiﬁer.

Fig. 4.6 shows the confusion matrix of the IoT detector model. The rows show

actual labels (i.e., IoT or non-IoT) and columns show predicted labels – cell numbers

are in percentage. Table 4.2 shows all performance metrics of this model. It is seen

that 98.7% of IoT test instances and 97.8% of non-IoT test instances are correctly

classiﬁed, as shown by diagonal values of the confusion matrix in Fig. 4.6. Looking

at the last two columns of Table 4.2, the conﬁdence-level of this model is fairly

high on average (i.e., 0.968 and 0.947) for correct classiﬁcation and is relatively

96

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

Figure 4.6: Confusion matrix of the IoT detector.

low on average (i.e., 0.635 and 0.701) for incorrect classiﬁcation. Also, the three

performance metrics; namely precision, recall, and F1; all indicate reasonable high

values on average as 0.983, 0.982, and 0.983 respectively.

IoT Classiﬁer:

For this model, we split IoT instances of the DATA1 into chronological sets of

training and testing given a suﬃcient number of instances available in our dataset

over six-month period. This way the performance of the model is evaluated over

time. Therefore, we use instances collected during the ﬁrst three months (i.e., 01-

Oct-2016 to 31-Dec-2016) for training and the remaining issuances (i.e., collected

between 01-Jan-2017 and 31-Mar-2017) for testing.

Fig. 4.7 depicts the confusion matrix of the IoT classiﬁer. We observe that the

model performs well in predicting most of classes. For example, the correct predic-

tion rate for Amazon Echo, August doorbell, Belkin switch, or Dropcamp is more

than 97%. However, the model performance does not seem acceptable for certain

classes. For example, it is seen that 12.0% of NEST smoke-sensor instances are

Table 4.2: Performance metrics of the IoT detector model.

IoT/non-IoT TP

FN FP Precision Recall

F1

TP avg.
conﬁdence

FN avg.
conﬁdence

IoT

0.987

0.013

0.022

non-IoT

0.978

0.022

0.013

0.979

0.987

0.987

0.983

0.978

0.983

0.968

0.947

0.635

0.701

97

IoTnon-IoTIoTnon-IoT98.71.32.297.8Chapter 4. Behavioral Monitoring using Low-Cost Attributes

Figure 4.7: Confusion matrix of IoT classiﬁer trained by the ﬁrst three months’
worth of data.

misclassiﬁed as Withings scale, and 39.8% of HP printer instances are misclassiﬁed

as Belkin switch. Additionally, for the Awair air-quality sensor only 77.1% of test

instances are correctly classiﬁed while 8.6% and 13.0% are misclassiﬁed as August

doorbell and Withings scale, respectively. We note that the model displays a low

conﬁdence on average for incorrect prediction of these three classes, i.e., 0.485 for

Awair air-quality, 0.535 for HP printer, and 0.389 for NEST sensor – due to space

constraints we omit detailed table of performance metrics per individual classes.

Additionally, we ﬁnd that even though 94.0% of Hue bulb instances are correctly

classiﬁed the average conﬁdence of our model is 0.572 (i.e., undesirably low).

98

AmazonEchoAugustdoorbellAwairairqualityBelkinmotion-sensorBelkinswitchDropcamHPprinterLiFXbulbNESTsmoke-sensorNetatmoweatherNetatmocameraHuebulbSamsungsmart-camSmartThingsTribyspeakerWithingssleep-sensorWithingsscaleClassiﬁedasAmazonEchoAugustdoorbellAwairairqualityBelkinmotion-sensorBelkinswitchDropcamHPprinterLiFXbulbNESTsmoke-sensorNetatmoweatherNetatmocameraHuebulbSamsungsmart-camSmartThingsTribyspeakerWithingssleep-sensorWithingsscaleActualinstances97.70.50.00.00.60.10.20.30.00.10.00.20.00.00.20.00.30.098.50.90.00.00.00.00.00.00.10.00.00.00.00.10.30.00.08.677.10.00.00.00.00.00.00.60.00.00.00.00.30.313.00.00.00.099.60.20.20.00.00.00.00.00.00.00.00.00.00.10.00.00.00.0100.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.10.10.099.10.00.10.00.00.20.00.00.00.00.40.00.00.00.00.039.80.058.91.10.10.00.00.00.00.00.20.00.00.00.00.00.00.90.10.997.70.00.00.30.00.00.00.00.00.00.00.30.00.00.00.00.00.086.60.30.00.00.00.00.00.712.00.00.00.00.00.00.00.00.00.299.70.00.10.00.00.00.00.00.00.10.00.00.00.10.00.00.21.297.40.60.00.30.00.20.00.30.10.00.10.00.10.00.10.00.00.493.05.80.10.00.00.00.00.10.00.00.00.00.00.00.20.00.01.298.30.00.00.00.00.10.00.00.00.00.00.00.00.10.01.90.56.391.20.00.00.00.00.10.20.00.30.00.20.20.00.00.40.00.00.094.93.20.60.00.10.00.00.50.10.00.00.10.00.00.00.00.01.095.72.40.00.00.20.00.00.00.00.00.30.70.00.00.00.00.20.298.5Chapter 4. Behavioral Monitoring using Low-Cost Attributes

(a) HP printer.

Figure 4.8: Time trace of IoT classiﬁer outputs with test traﬃc instances from: (a)
HP printer, and (b) Hue light-bulb.

(b) Hue light-bulb.

To better analyze the poor performance of the model in certain classes, we plot in

Fig. 4.8 the time trace of model outputs with test traﬃc instances from HP printer

and Hue light-bulb. Each circle represents an instance and its color shows the model

conﬁdence. A color bar on the right side of plots shows the mapping of conﬁdence

values to colors – dark green indicates high conﬁdence and yellow indicates low

conﬁdence. Starting from Fig. 4.8a, we observe that instances of HP printer from

the ﬁrst week of January are mostly classiﬁed correctly and are supported by high

conﬁdence levels (i.e., dark green circles). The printer goes oﬄine for about a

month and comes back online on 11-Feb-2017 and this is when its traﬃc is mostly

misclassiﬁed as Belkin switch with consistently low conﬁdence levels from the model

(i.e., light green circles). This clearly shows that the behavior of HP printer changed

99

Jan 01Jan 15Feb 01Feb 15Mar 01Mar 15Mar 31Instance dateAmazon EchoAugust doorbellAwair air-qualityBelkin motion-sensorBelkin switchDropcamHP printerLiFX bulbNEST smoke-sensorNetatmo weatherNetatmo cameraHue bulbSamsung smart-camSmart ThingsTriby speakerWithings sleep-sensorWithings scale00%10%20%30%40%50%60%70%80%90%100%Confidence levelJan 01Jan 15Feb 01Feb 15Mar 01Mar 15Mar 31Instance dateAmazon EchoAugust doorbellAwair air-qualityBelkin motion-sensorBelkin switchDropcamHP printerLiFX bulbNEST smoke-sensorNetatmo weatherNetatmo cameraHue bulbSamsung smart-camSmart ThingsTriby speakerWithings sleep-sensorWithings scale00%10%20%30%40%50%60%70%80%90%100%Confidence levelChapter 4. Behavioral Monitoring using Low-Cost Attributes

Figure 4.9: Confusion matrix of IoT device classiﬁcation re-trained by additional
data from two weeks in February.

when it restarted in mid-February – we manually inspected traﬃc traces and veriﬁed

that it was due to a legitimate ﬁrmware upgrade (i.e., benign changes). Moving to

Fig. 4.8b, we see classiﬁer outputs for Hue bulb traﬃc instances during the whole

testing period. Though instances are mostly predicted correctly, the conﬁdence

level starts falling, from an average of 0.93 to average 0.50, on 15-Feb-2017. Again

this behavioral change led to a manual inspection by which we veriﬁed that it was

legitimate.

Given these observations, we augment our training set with two weeks of data

(i.e., from 12-Feb-2017 to 25-Feb-2017) for duration over which new legitimate traﬃc

100

AmazonEchoAugustdoorbellAwairair-qualityBelkinmotion-sensorBelkinswitchDropcamHPprinterLiFXbulbNESTsmoke-sensorNetatmoweatherNetatmocameraHuebulbSamsungsmart-camSmartThingsTribyspeakerWithingssleep-sensorWithingsscaleClassiﬁedasAmazonEchoAugustdoorbellAwairair-qualityBelkinmotion-sensorBelkinswitchDropcamHPprinterLiFXbulbNESTsmoke-sensorNetatmoweatherNetatmocameraHuebulbSamsungsmart-camSmartThingsTribyspeakerWithingssleep-sensorWithingsscaleActualinstances97.70.40.00.00.30.20.40.20.20.10.00.20.00.00.10.10.30.098.90.90.00.00.00.00.00.00.00.00.00.00.00.10.10.00.01.193.60.00.00.00.00.00.00.80.00.00.00.00.00.44.20.00.00.099.60.20.30.00.00.00.00.00.00.00.00.00.00.00.00.00.00.099.90.00.00.00.00.00.00.00.00.00.00.00.00.00.00.10.00.099.00.00.10.00.00.20.00.00.00.00.60.00.00.00.00.00.70.097.71.30.10.00.00.00.00.00.20.00.00.00.00.00.00.00.11.698.00.00.00.30.00.00.00.00.00.00.00.30.00.00.30.00.00.097.60.30.00.00.00.00.00.01.40.00.00.00.00.00.00.00.00.299.70.00.00.00.00.00.00.00.00.10.00.00.00.20.00.00.21.397.30.60.00.30.00.10.00.00.00.10.10.00.10.00.00.00.00.191.77.70.20.00.00.00.00.00.00.00.00.00.00.00.00.00.00.199.70.20.00.00.00.10.00.00.00.00.00.00.00.10.02.00.70.296.90.00.00.00.00.10.20.00.20.00.50.20.00.00.40.00.00.094.13.90.50.00.20.00.00.30.20.30.00.10.00.00.00.00.01.095.72.30.00.40.80.00.00.00.00.00.00.20.00.00.00.00.20.697.9Chapter 4. Behavioral Monitoring using Low-Cost Attributes

Table 4.3: Performance metrics of the IoT classier model (after re-training).

IoT/Non-IoT

TP

FN FP Precision Recall

F1

TP avg. conﬁdence FN avg. conﬁdence

Amazon Echo

0.977

0.023

0.000

August doorbell

0.989

0.011

0.001

Awair air-quality

0.936

0.064

0.002

Belkin motion-sensor

0.996

0.004

0.000

Belkin switch

0.999

0.001

0.001

Dropcam

HP printer

LiFX bulb

0.990

0.010

0.001

0.977

0.023

0.001

0.980

0.020

0.001

NEST smoke-sensor

0.976

0.024

0.001

Netatmo weather

0.997

0.003

0.002

Netatmo camera

0.973

0.027

0.001

Hue bulb

0.917

0.083

0.001

Samsung smart-cam 0.997

0.003

0.006

Smart Things

0.969

0.031

0.001

Triby speaker

0.941

0.059

0.001

Withings sleep-sensor

0.957

0.043

0.004

Withings scale

0.979

0.021

0.003

1.000

0.999

0.998

1.000

0.999

0.999

0.999

0.999

0.999

0.998

0.999

0.999

0.994

0.999

0.999

0.996

0.997

0.977

0.989

0.989

0.994

0.936

0.966

0.996

0.998

0.999

0.999

0.990

0.994

0.977

0.988

0.980

0.990

0.976

0.987

0.997

0.997

0.973

0.986

0.917

0.956

0.997

0.996

0.969

0.984

0.941

0.969

0.957

0.976

0.979

0.988

0.994

0.974

0.850

0.825

0.990

0.987

0.985

0.892

0.818

0.935

0.980

0.975

0.989

0.980

0.785

0.968

0.953

0.430

0.509

0.616

0.340

0.604

0.462

0.591

0.617

0.472

0.824

0.371

0.505

0.367

0.437

0.452

0.504

0.560

patterns emerged. Fig. 4.9 shows the performance of the IoT classiﬁer after it is re-

trained. It is seen that the confusion matrix is almost diagonal with the TP rate

of more than 90% for all classes (i.e., on average 97.4%). We list in Table 4.3

all performance metrics of the IoT classiﬁer after re-training. We observe that the

model average conﬁdence is boosted across all classes – speciﬁcally it reaches to

0.975 for Hue bulb instances. Also, three performance metrics consistently display

an acceptable performance of classiﬁcation with 0.998, 0.973, and 0.986 for average

precision, recall, and F1 score.

IoT State Classiﬁers: From our DATA2, we generated a diﬀerent number of

instances of four IoT devices with state labels including: Amazon Echo (boot: 208,

active: 74, idle: 1795); Belkin switch (boot: 110, active: 84, idle: 2688); Dropcam

(boot: 145, active: 98, idle: 2639); and LiFX bulb (boot: 160, active: 84, idle:

2338). To train individual device-speciﬁc models, we randomly choose 40 instances

from each of their respective states – remaining instances are used to test the models.

Note that the state classiﬁer are specialized models trained independently for each

101

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

(a) Amazon Echo.

(b) Belkin switch.

(c) Dropcam.

(d) LiFX.

Figure 4.10: Confusion matrix of IoT state classiﬁers: (a) Amazon Echo; (b) Belkin
switch; (c) Dropcam; and (d) LiFX bulb.

device type, and hence adding more devices or increasing instances of a given device

will not aﬀect the performance and robustness of existing state classiﬁers.

Fig. 4.10 shows the confusion maps of the four IoT state classiﬁers. Our ﬁrst

observation is that all four models predict very well the active state – 100.0% TP rate

in three models (Amazon Echo, Dropcamp, and LiFX bulb), and 95.5% TP rate in

Belkin switch. Next, we see that boot instances are prone to be misclassiﬁed as idle,

and vice versa (e.g., 8.6% and 7.5% of boot instances respectively in Belkin switch

and LiFX bulb are misclassiﬁed as idle). This misclassiﬁcation could be possibly

because instances pertinent to state transitions (e.g., boot to idle) are not precisely

annotated.

102

BootActiveIdleClassiﬁedasBootActiveIdleActualinstances96.40.03.60.0100.00.02.14.094.0BootActiveIdleClassiﬁedasBootActiveIdleActualinstances91.40.08.60.095.54.54.42.992.7BootActiveIdleClassiﬁedasBootActiveIdleActualinstances98.10.01.90.0100.00.01.82.296.0BootActiveIdleClassiﬁedasBootActiveIdleActualinstances92.50.07.50.0100.00.07.42.689.9Chapter 4. Behavioral Monitoring using Low-Cost Attributes

4.4 Practical and Operational Considerations

In the previous section, we evaluated the performance of our inference engines using

all traﬃc attributes of devices during their normal operation. In this section, we

ﬁrst quantify the cost of our scheme in practice and show how we can optimize

the trade-oﬀ between cost and performance. Next, we demonstrate how network

operators can interpret the outputs of inference engines, and therefore manage their

cyber-security risk.

4.4.1 Cost of Attributes

In order to quantify the cost of our scheme, we begin by examining the impact

of individual attributes on the performance of traﬃc inference. We have 112 at-

tributes for the IoT detector and the IoT classiﬁer, and 48 attributes for the IoT

state classiﬁers. Note that some of these attributes could be highly correlated, and

hence become redundant. Also, some attributes may not be very relevant to class

prediction, and hence can be removed.

Redundant Attributes: We use a selection algorithm called Correlation-based

Feature Subset (CFS) [168] with best-ﬁrst searching method. CFS is a ﬁlter that

uses a correlation-based heuristic to ﬁnd a subset of attributes with the highest merit

– i.e., attributes highly-correlated with the class, yet uncorrelated with each other.

Importance of Attributes: In decision tree-based machine learning, the In-

formation Gain (IG) method is used to measure the weight of various attributes

in accurate prediction. Important attributes carry more information (i.e., large IG

value) to distinguish classes, and unrelated attributes have no information. We now

compute the IG value of attributes used for each of the three classiﬁer types.

To better visualize the merit of various attributes, we illustrate in Fig. 4.11a

the IG values computed for all 112 attributes used by the IoT classiﬁer. Each

cell represents an attribute (i.e., rows are ﬂow counters and columns are various

103

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

(a) All attributes.

(b) Subset of nonredundant attributes.

Figure 4.11: Information gain value of: (a) all attributes, and (b) CFS-selected
attributes, for the IoT classiﬁer.

timescales), and is labeled (and color coded) by its IG value – the darker the cell,

the higher the IG value. Fig. 4.11b shows a subset of 35 attributes selected by the

CFS algorithm eliminating correlated (i.e., redundant) attributes. Note that this

subset still results in the same performance of prediction as presented in the previous

section.

We observe that the highest IG value 3.36 corresponds to “outgoing remote byte-

count over 8-minute” followed by “incoming remote byte-count over 4-minute” with

IG 3.32. Another observation is that byte-count of both incoming/outgoing remote

over mid-term timescales (i.e., 4-, 8-, 16-min) have higher information compared to

other attributes, as shown by darker cells. Also, DNS counters over longer timescales

(i.e., 32- and 64-min) display a relatively high gain of information in predicting class

of IoT devices.

Overall, attributes of two ﬂow rules related to incoming local traﬃc and outgoing

SSDP queries seem to have minimal impacts in IoT device classiﬁcation. This is

mainly because only a few of IoT devices in our lab (e.g., Hue bulb, Bekin motion,

Amazon Echo) communicate on the local network or send SSDP queries. Even

though these ﬂow rules (and associated attributes) may not seem important across

all devices, they can precisely characterize and help identify devices which use them

in their network traﬃc.

104

3.031.773.072.040.310.290.330.200.320.200.220.220.220.220.660.663.232.063.182.360.660.640.570.340.560.350.330.330.330.330.790.793.322.393.262.610.860.831.050.681.050.690.560.560.560.560.870.853.292.533.362.850.870.841.691.081.711.090.840.840.840.840.900.893.152.683.312.900.870.842.191.372.201.381.111.111.111.110.930.922.862.673.202.940.900.892.611.822.701.831.301.301.291.291.001.002.772.683.012.900.970.942.932.133.112.131.541.541.511.511.191.191 min2 min4 min8 min16 min32 min64 minRem. BRem. PRem. BRem. PLoc. BLoc. PDNS BDNS PDNS BDNS PNTP BNTP PNTP BNTP PSSDP BSSDP P0.511.522.53Information Gain3.03 3.072.04             2.063.182.36             2.39 2.610.86          0.85 2.533.36     1.71     0.90  2.68 2.900.87 2.19 2.20        2.67 2.94    2.701.83   1.29  2.772.683.01   2.932.133.11 1.541.54 1.511.191.191 min2 min4 min8 min16 min32 min64 minRem. BRem. PRem. BRem. PLoc. BLoc. PDNS BDNS PDNS BDNS PNTP BNTP PNTP BNTP PSSDP BSSDP P11.522.53Information GainChapter 4. Behavioral Monitoring using Low-Cost Attributes

(a) Amazon Echo.

(b) Belkin switch.

(c) Dropcam.

(d) LiFX bulb.

Figure 4.12: Information gain of all attributes for state classier models: (a) Amazon
Echo, (b) Belkin switch, (c) Dropcamp, and (d) LiFX bulb.

Moreover, we have analyzed the impact of attributes for two other types of

classiﬁers. Fig. 4.12 and Fig. 4.13 show the information gain value of all and non-

redundant attributes for state classiﬁcation models. We observe, for example, in

Fig. 4.13a that attributes of only for ﬂow rules (i.e., incoming remote, outgoing

remote, incoming DNS, incoming NTP) are needed for the state classiﬁcation of

Amazon Echo – there is no attribute selected for the other four ﬂows. Another

observation is that attributes over very short timescales (i.e., 1-min and 2-min) be-

come important in classifying operating states of IoT devices. Also, we note that the

variation of IG values for non-redundant attributes is less (i.e., between 0.1 and 0.3)

compared to the IoT classiﬁer model (i.e., between 0.86 and 3.36). For the IoT detec-

tor model, we found that attributes over longer timescale (i.e., outgoing/incoming

(a) Amazon Echo.

(b) Belkin switch.

(c) Dropcam.

(d) LiFX bulb.

Figure 4.13: Information gain of non-redundant attributes for state classier models:
(a) Amazon Echo, (b) Belkin switch, (c) Dropcamp, and (d) LiFX bulb.

105

0.350.310.280.230.000.000.190.200.200.200.320.320.310.310.000.000.390.360.400.340.000.000.350.350.350.350.290.290.310.310.000.000.220.220.250.170.000.000.220.220.230.230.230.230.240.240.000.001 min2 min4 minRem. BRem. PRem. BRem. PLoc. BLoc. PDNS BDNS PDNS BDNS PNTP BNTP PNTP BNTP PSSDP BSSDP P00.050.10.150.20.250.30.350.4Information Gain0.210.120.090.090.170.170.070.070.070.070.120.120.120.120.110.110.210.110.100.100.150.130.070.070.070.070.080.080.090.090.090.090.180.170.190.100.110.090.090.070.070.070.060.060.060.060.080.081 min2 min4 minRem. BRem. PRem. BRem. PLoc. BLoc. PDNS BDNS PDNS BDNS PNTP BNTP PNTP BNTP PSSDP BSSDP P0.080.10.120.140.160.180.2Information Gain0.180.130.260.190.000.000.130.130.130.130.140.140.150.150.000.000.130.120.200.160.000.000.170.170.170.170.160.160.160.160.000.000.110.110.120.100.000.000.120.120.120.120.110.110.120.120.000.001 min2 min4 minRem. BRem. PRem. BRem. PLoc. BLoc. PDNS BDNS PDNS BDNS PNTP BNTP PNTP BNTP PSSDP BSSDP P00.050.10.150.20.25Information Gain0.260.240.270.230.090.090.120.090.140.090.030.030.030.030.000.000.250.210.260.230.090.090.140.140.180.140.040.040.040.040.000.000.180.140.180.160.080.080.120.080.120.080.000.000.000.000.000.001 min2 min4 minRem. BRem. PRem. BRem. PLoc. BLoc. PDNS BDNS PDNS BDNS PNTP BNTP PNTP BNTP PSSDP BSSDP P00.050.10.150.20.25Information Gain      0.19         0.39 0.40   0.350.35   0.29                    1 min2 min4 minRem. BRem. PRem. BRem. PLoc. BLoc. PDNS BDNS PDNS BDNS PNTP BNTP PNTP BNTP PSSDP BSSDP P0.20.220.240.260.280.30.320.340.360.380.4Information Gain    0.17 0.07    0.120.120.12 0.110.21    0.13                          1 min2 min4 minRem. BRem. PRem. BRem. PLoc. BLoc. PDNS BDNS PDNS BDNS PNTP BNTP PNTP BNTP PSSDP BSSDP P0.080.10.120.140.160.180.2Information Gain  0.26    0.13     0.15    0.20    0.17                        1 min2 min4 minRem. BRem. PRem. BRem. PLoc. BLoc. PDNS BDNS PDNS BDNS PNTP BNTP PNTP BNTP PSSDP BSSDP P0.140.160.180.20.220.24Information Gain0.260.240.270.23 0.090.12         0.25    0.090.14 0.18            0.08          1 min2 min4 minRem. BRem. PRem. BRem. PLoc. BLoc. PDNS BDNS PDNS BDNS PNTP BNTP PNTP BNTP PSSDP BSSDP P0.080.10.120.140.160.180.20.220.240.26Information GainChapter 4. Behavioral Monitoring using Low-Cost Attributes

byte-count over 32-min and 64-min) have higher impact. It is important to note

that our observations and ﬁndings may change in diﬀerent environments depending

upon types of devices and their possible interactions.

Cost versus Performance: There exist two sources of cost in our inference

scheme: (1) number of ﬂow entries; and (2) space complexity of computing at-

tributes. Given the ﬁxed size of TCAM on programmable (SDN) switches, eﬃcient

management of ﬂow entries [169] becomes crucial to scale of scheme for deployment

in a network with a large number of IoT devices. Since our attributes are computed

at multiple timescales up to 64-minutes, we need to maintain the time-series of ﬂow

counters accordingly (i.e., 64 data-points each corresponds to a minute).

We, therefore, aim to reduce the cost by decreasing attributes, without signif-

icantly aﬀecting performance. Table 4.4 shows the number of ﬂow entries needed

by each inference model with non-redundant set of attributes – check-marked cells

indicate the ﬂows needed for attributes of models in each row. It clearly shows room

for optimizing our approach by dynamic management of ﬂow entries on the pro-

grammable switch. For example, the IoT detector model only needs four ﬂow rules

per device. Once a device is detected as IoT, an additional four ﬂows are needed by

the IoT classiﬁer (i.e., a total of eight ﬂows). Once the IoT device is successfully

classiﬁed, it may need a reduced number of ﬂows depending upon its specialized

state classiﬁer (some ﬂows can be removed from the switch). The state classiﬁer of

Amazon Echo, Belkin switch, Dropcam, and LiFX respectively need 4, 6, 3, and 5

ﬂow entries per each unit of device.

Table 4.4: Flow entries (per-device) needed for non-redundant attributes set.

Inference model

Rem.

IoT detector

IoT classiﬁer

State classiﬁer - Amazon Echo

State classiﬁer - Belkin Switch

State classiﬁer - Dropcam

State classiﬁer - LiFX

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

↑

Rem.

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

Loc.

↓

↓

(cid:88)

(cid:88)

(cid:88)

NTP

↑

NTP

↓

↓

(cid:88)

(cid:88)

(cid:88)

(cid:88)

v

(cid:88)

↑

SSDP

(cid:88)

(cid:88)

(cid:88)

DNS

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

↑

DNS

(cid:88)

(cid:88)

(cid:88)

106

Num. of ﬂow entries

5

8

4

6

3

5

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

(a) Performance metrics of classiﬁcation.

(b) Cost of attributes and ﬂow rules.

Figure 4.14: Impact of attributes on: (a) performance; (b) cost, for the IoT classiﬁer.

We further optimize by a careful trade-oﬀ between cost of performance. We:

(a) ﬁrst sort non-redundant attributes in descending order; (b) then accumulate

attributes one-by-one from the sorted list; and lastly (c) quantify the cost and per-

formance at each step. Let us visualize this process for the IoT classiﬁer in Fig. 4.14.

We plot performance metrics and cost signals, each as a function of cumulative set

of high-merit attributes. With 35 non-redundant attributes, it is seen in Fig. 4.14a

that average weighted precision, recall, F1 score reach to 97.5%, 97.3%, 97.4% re-

spectively, and in Fig. 4.14b that the total cost per device would reach to 8KB of

memory and 8 ﬂow entries. We note that with the top 25 attributes we can achieve

about 97% in all performance metrics which can save four ﬂow entries (i.e., 50%

saving) and reduce the space complexity to 5KB (i.e., 37% reduction).

Devices experimented in our testbed, indeed, represent majority of IoT devices

that are currently available in the market. We acknowledge that the importance of

the attributes can vary in diﬀerent environments. That’s why we begin by incor-

porating all attributes for our baseline evaluation without removing any attribute.

Later, a network operator may use our method to perform cost-beneﬁt analysis to

identify and possibly remove low-impact (less important) attributes for their envi-

ronment.

107

0 5 101520253035Number of high-merit attributes80828486889092949698100Percentage (%)PrecisionRecallF1 Score05101520253035Number of high-merit attributes02000400060008000Space complexity per device (bytes)02468Num. of flow entries per deviceSpace complexityNum. of flow entriesChapter 4. Behavioral Monitoring using Low-Cost Attributes

4.4.2 Use of Inference Engines in Real-Time

We now demonstrate how our scheme can help network operators detect behavioral

changes due to malicious network activities or cyber-attacks.

Unlike traditional non-IoT devices, behavior proﬁle of IoTs does not signiﬁcantly

change by interactions with users or environment. We discussed in §4.3.2 how legit-

imate ﬁrmware upgrades can be detected by our solution (i.e., consistent misclassi-

ﬁcation and/or low conﬁdence), as shown in Fig. 4.8. Note that sudden changes in

outputs of inference models (if persist) for given device(s) can trigger an investigation

by network administrators or inspection appliances.

We now test our classier models with attack traﬃc on IoT devices. We use a set

of publicly available PCAP traces [85] that contain both benign and attack traﬃc

(clearly annotated) corresponding to a few IoT devices we use in this work.

In Figures 4.15 and 4.16 we present results of three representative scenarios: (1)

the output label of the IoT classiﬁer changes persistently (i.e., repeatedly misclas-

sifying) accompanied by a sudden drop in conﬁdence; (2) the output label of the

IoT classiﬁer does not change, but its conﬁdence drops and persistently stays at

low levels; and (3) the output of the IoT classiﬁer remains normal (expected label

with reasonable conﬁdence), but the respective state classiﬁer mis-behaves. In these

plots, red crosses indicate time periods over which attack traﬃc is launched to the

respective IoT device, and blue circles show purely benign traﬃc instances.

Fig. 4.15a illustrates the scenario 1 for Belkin switch. This time trace displays a

situation where the IoT device experiences TCP SYN reﬂection attack twice, each

for a duration of 10 minutes. It is seen that during attack periods (shown by red

cross markers) the predicted label changes from Belkin switch to Netatmo camera

with conﬁdence less than 70%. Right after the attack, the output comes back to its

original label and the conﬁdence starts rising gradually.

108

Chapter 4. Behavioral Monitoring using Low-Cost Attributes

(a) IoT classiﬁer with Belkin switch traﬃc.

(b) IoT classiﬁer with Amazon Echo traﬃc.

Figure 4.15: Time trace of device classiﬁer outputs with benign and attack traﬃc
of: (a) Belkin switch; and (b) Amazon Echo.

Fig. 4.15b (representative of the Scenario 2) displays a time trace of our moni-

toring scheme for Amazon Echo under UDP-based DoS attack over two periods of

10-minutes each. We observe that these attacks do not change the predicted label

of the IoT classiﬁer, but cause the model conﬁdence to decay rapidly (and remains

below 80% persistently).

Lastly, Fig. 4.16a (representative of the scenario 3) illustrates a situation where

attack traﬃc is not intense (i.e., ping of death attack on LiFX bulb), and hence

does not aﬀect the broad view of the IoT classiﬁer model. However, the specialized

model of the state classiﬁer is signiﬁcantly aﬀected. During the attack periods, the

109

WithingsscaleWithingssleep-sensorTribyspeakerNetatmocameraLiFXbulbBelkinswitchAmazonEchoDevicesBenigntrafﬁcAttacktrafﬁcJun,0214:43Jun,0214:48Jun,0214:53Jun,0214:58Jun,0215:03Jun,0215:08Jun,0215:13Jun,0215:18Jun,0215:23Jun,0215:28Jun,0215:33Jun,0215:38Jun,0215:43Jun,0215:48Jun,0215:53Jun,0215:58Time0%10%20%30%40%50%60%70%80%90%100%ConﬁdencelevelWithingsscaleWithingssleep-sensorTribyspeakerNetatmocameraLiFXbulbBelkinswitchAmazonEchoDevicesBenigntrafﬁcAttacktrafﬁcOct,2310:00Oct,2310:05Oct,2310:10Oct,2310:15Oct,2310:20Oct,2310:25Oct,2310:30Oct,2310:35Oct,2310:40Oct,2310:45Oct,2310:50Oct,2310:55Oct,2311:00Oct,2311:05Oct,2311:10Oct,2311:15Time0%10%20%30%40%50%60%70%80%90%100%ConﬁdencelevelChapter 4. Behavioral Monitoring using Low-Cost Attributes

(a) IoT classiﬁer with LiFX bulb traﬃc.

(b) LiFX state classiﬁer with LiFX bulb traﬃc.

Figure 4.16: Time trace of outputs for: (a) device classiﬁer, and (b) state classiﬁer,
with benign and attack traﬃc of LiFx bulb.

LiFX bulb is persistently seen in an active state which is not normal for a light-bulb

since its activity (turning on/oﬀ or changing color) is expected to be relatively short.

Additionally, the conﬁdence of the state classiﬁer quickly falls to a level of about

50% which is not normal again.

Although drops in the conﬁdence level indicate traﬃc anomalies, it can be chal-

lenging for a network administrator to diﬀerentiate random traﬃc variations (e.g.

noises) occurring over a very short time interval from a real attack which may persist

for a considerable amount of time. Next chapter will develop a method to clearly

diﬀerentiate anomalies from short term traﬃc variations.

110

WithingsscaleWithingssleep-sensorTribyspeakerNetatmocameraLiFXbulbBelkinswitchAmazonEchoDevicesBenigntrafﬁcAttacktrafﬁcOct,2312:50Oct,2312:55Oct,2313:00Oct,2313:05Oct,2313:10Oct,2313:15Oct,2313:20Oct,2313:25Oct,2313:30Oct,2313:35Oct,2313:40Oct,2313:45Oct,2313:50Oct,2313:55Oct,2314:00Oct,2314:05Time0%10%20%30%40%50%60%70%80%90%100%ConﬁdencelevelIdleActiveBootStatesBenigntrafﬁcAttacktrafﬁcOct,2312:50Oct,2312:55Oct,2313:00Oct,2313:05Oct,2313:10Oct,2313:15Oct,2313:20Oct,2313:25Oct,2313:30Oct,2313:35Oct,2313:40Oct,2313:45Oct,2313:50Oct,2313:55Oct,2314:00Oct,2314:05Time0%10%20%30%40%50%60%70%80%90%100%ConﬁdencelevelChapter 4. Behavioral Monitoring using Low-Cost Attributes

4.5 Conclusion

This chapter developed a real-time behavioral monitoring solution for IoT devices

employing low-cost ﬂow-level telemetry with the support of OpenFlow enabled SDN

switches. We identiﬁed traﬃc ﬂows that can collectively characterize the network

behavior of IoT devices and their states such as booting, user interaction or idle.

We then trained a set of classiﬁcation models with supervised machine learning

algorithms for a three-stage inference architecture using real traﬃc traces of 17 IoT

devices collected over a period of six months. We validate their eﬃcacy in detecting

IoT devices from non-IoTs, classifying their type, and identifying their operating

state. Lastly, we showed how we balance the trade-oﬀ between cost and performance

of our scheme, and demonstrated how operators can use it to detect IoT behavioral

changes (both legitimate and malicious). While this chapter mainly focused on

low-cost attributes and optimizing the cost of network telemetry, we just scratch

the surface of anomaly detection. The following chapter improves the sensitivity

of inference engine for detecting behavioral changes in IoT network traﬃc, due to

ﬁrmware upgrades or low-rate cyber-attacks, using clustering models.

111

Chapter 5

Behavioral Change Detection using

Clustering Algorithm

Contents

5.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

5.2 Clustering Flow-Level Attributes . . . . . . . . . . . . . . . . . . 116

5.2.1

Flow-Level Telemetry and Traﬃc Attributes . . . . . . . 116

5.2.2 Attributes Clustering . . . . . . . . . . . . . . . . . . . . 119

5.3 Unsupervised Classiﬁcation of IoT Devices

. . . . . . . . . . . . 122

5.3.1 Clustering Models: Generation, Tuning, and Testing . . 122

5.3.2 Conﬂict Resolution . . . . . . . . . . . . . . . . . . . . . 124

5.3.3 Consistency Score . . . . . . . . . . . . . . . . . . . . . . 127

5.3.4 Monitoring Phases

. . . . . . . . . . . . . . . . . . . . . 130

5.4 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . 131

5.4.1 Device Classiﬁcation . . . . . . . . . . . . . . . . . . . . 131

5.4.2 Detecting Behavioral Change

. . . . . . . . . . . . . . . 134

5.4.3 Detecting Attacks . . . . . . . . . . . . . . . . . . . . . . 137

5.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143

In the previous two chapters, we developed inference engines that help network

operators automatically identify IoT assets via network-level traﬃc analysis, and

113

Chapter 5. Behavioral Change Detection using Clustering Algorithm

monitor their behavior in real-time. However, IoT manufacturers often tend to

release new ﬁrmware which improves device functionalities or even automatically

perform upgrades from cloud servers to devices that are operational in the ﬁeld.

This becomes challenging for classiﬁcation models to incorporate behavioral changes

(or new classes) dynamically without retraining the entire model.

In this chapter, we develop a modular device classiﬁcation architecture that al-

lows us to dynamically accommodate legitimate changes in IoT assets, via either the

addition of a new device proﬁle or an upgrade of existing proﬁles, without replacing

the entire set of models. Our contributions are threefold: (1) We develop an unsu-

pervised one-class clustering method for each device to detect their normal network

behavior. We use traﬃc attributes identiﬁed in the previous chapter that can be

obtained from ﬂow-level network telemetry to characterize behavior of individual

IoT devices; (2) We tune individual device-speciﬁc clustering models and use them

to classify IoT devices from network traﬃc in real-time. We enhance our classiﬁca-

tion by developing methods for automatic conﬂict resolution and model consistency

monitoring mechanism; and (3) We evaluate the eﬃcacy of our scheme by applying

it to traﬃc traces (benign and attack) from 12 real IoT devices, and demonstrate its

ability to detect behavioral changes with overall accuracy of more than 94%. Parts

of this chapter have been published in [9] and [10].

5.1 Introduction

IoT devices are typically purpose built with limited functionalities – they communi-

cate with a speciﬁc set of endpoints (i.e., servers) using a small number of TCP/UDP

ﬂows. Therefore, a growing number of traﬃc classiﬁcation proposals are emerging

based on supervised machine-learning techniques (e.g., multi-class decision-trees or

neural-networks) that use packet-level [135], ﬂow-level [136], or a combination of

packet-level and ﬂow-level [6] traﬃc attributes for monitoring IoTs behavioral pat-

114

Chapter 5. Behavioral Change Detection using Clustering Algorithm

terns on the network. In our prior work [9] we showed that generating the model

for multi-class classiﬁers becomes practically challenging when a new device type is

added to the network or the behavior of existing device types legitimately changes

(due to ﬁrmware upgrades by device manufacturers) – it is needed to regenerate the

entire model of all classes.

In order to avoid over-ﬁtting the generated model to

speciﬁc classes, we need to carefully balance (i.e., representing classes equally) the

training dataset comprising instances of all device types. However, certain devices

need much more instances to capture their normal behavior.

In this chapter, we employ a set of one-class clustering models (one per IoT de-

vice), and each can be independently trained and updated. Our ﬁrst contribution

develops an inference engine using an unsupervised one-class clustering model for

each device to detect their normal network behavior using low-cost traﬃc attributes

that can be computed from real-time ﬂow-level telemetry. Our second contribu-

tion tunes individual device-speciﬁc clustering models and uses them to classify IoT

devices types from network traﬃc in real-time. We enhance our classiﬁcation by

developing methods for automatic conﬂict resolution and monitoring consistency of

individual models. Finally, we evaluate the eﬃcacy of our scheme by applying it

to traﬃc traces (benign and attack) from 12 real IoT devices and demonstrate its

ability to detect behavioral changes with an overall accuracy of more than 94%.

The rest of this chapter is organized as follows: In §5.2, we present our dataset

and traﬃc attributes, and build unsupervised clusters to characterize network behav-

ior of individual IoT devices. In §5.3, we design and implement an inference engine

composed of classiﬁcation models to identify the IoT devices and detect anomalies.

We also devise a scoring technique to measure the consistency of those classiﬁcation

models. In §5.4, we evaluate the eﬃcacy of the inference engine in classifying device

types and detecting the attack, followed by comparison on one-class classiﬁcation

with a multi-class classiﬁcation method. The chapter is concluded in §5.5.

115

Chapter 5. Behavioral Change Detection using Clustering Algorithm

5.2 Clustering Flow-Level Attributes

In this section, we ﬁrst outline our IoT dataset, network telemetry, and traﬃc at-

tributes. Next, we show how clusters of attributes will characterize network behavior

of individual IoT devices.

5.2.1 Flow-Level Telemetry and Traﬃc Attributes

Dataset: We use two sets of packet traces for this work, namely DATA1 (be-

nign traﬃc) and DATA2 (mix of benign and attack traﬃc). The ﬁrst dataset (i.e.,

DATA1) was collected from a testbed consisting of more than 30 IoT devices for a

duration of six months (i.e., 01-Oct-2016 to 31-Mar-2017) [6]. We select 12 IoT de-

vices namely the Amazon Echo, Belkin motion sensor, Belkin switch, Dropcam, HP

printer, LiFX bulb, Netatmo weather station, Netatmo camera, Samsung camera,

Smart Things, Triby speaker, and Withings sleep sensor as those showed signiﬁcant

activities during the early period of the dataset (i.e., 1-Oct-2016 to 15-Nov-2016).

We assumed that DATA1 does not contain any attack data and the devices we used

in our testbed are not compromised. However, we allowed the devices to update

their ﬁrmware automatically. Also, we aware that these data may contain uninten-

tional connection losses for some devices due to the downtimes of cloud service. We

evaluate (in §5.3) on DATA1 the eﬃcacy of our inference engine in classifying device

proﬁles as well as detecting their behavioral changes.

Our second dataset (i.e., DATA2) contains more than eight weeks’ worth of

PCAP traces [85] collected from ten IoT devices (in a diﬀerent environment) over

two months in 2018. DATA2 includes normal traﬃc (covering boot, active, and idle

operating states) and also annotated attack traﬃc (direct and reﬂective) on these

IoT devices. For this dataset, we assumed that no attacks occurred during the days

other than the annotated ones.

In order to reduce the unintentional behavioural

116

Chapter 5. Behavioral Change Detection using Clustering Algorithm

changes, we minimize the automatic ﬁrmware updates by manually updating the

possible devices before collecting the data. We use DATA2 (in §5.4.3) to evaluate

the performance of our scheme in detecting cyber-attacks that cause behavioral

changes in real-time.

Flow-Level Telemetry and Attributes: We showed in Chapter 3 that indi-

vidual IoT devices exhibit identiﬁable patterns in their traﬃc ﬂows such as activity

cycles and volume patterns, and proﬁles of signaling protocols such as DNS, NTP,

and SSDP. To monitor IoT behavior on the network in real-time, we identify a set

of ﬂows (speciﬁc to each device) that collectively capture its entire traﬃc. These

ﬂow rules can be programmed into an SDN-enabled switch [7, 162] through which

the traﬃc of IoT devices passes – rules of diﬀerent devices are distinguished by a

match ﬁeld corresponding to device identiﬁer (i.e., MAC or IP address). Counters

of these ﬂow rules are periodically (conﬁgurable, say, every minute) measured, and

will form traﬃc attributes of individual devices.

Table 5.1 shows eight ﬂow rules that we use to measure network traﬃc of each

IoT device with the following order: (1,2) DNS outgoing queries and incoming

responses on UDP 53; (3,4) NTP outgoing queries and incoming responses on UDP

123; (5) SSDP outgoing queries on UDP 1900; (6,7) other “remote” traﬃc (e.g.,

Internet) outgoing from and incoming to the device that passes through the gateway;

and (8) all “local” traﬃc (i.e., LAN) incoming to the device. Note that we do not

monitor SSDP traﬃc incoming to IoT devices to avoid capturing (and mixing) the

Table 5.1: Flow rules (per-device) needed for network traﬃc telemetry.

Flow description srcETH dstETH srcIP dstIP srcPort srcPort proto

DNS
↑
DNS
↓
NTP
↑
NTP
↓
SSDP
↑
remote
↑
remote
↓
local

↓

<devMAC>
*

<devMAC>
*

<devMAC>

*

<devMAC>
*

<devMAC>
*

<devMAC>

<gwMAC>

<gwMAC>
*

<devMAC>

<devMAC>

*
*
*
*
*
*
*
*

*
*
*
*
*
*
*
*

*
53
*
123
*
*
*
*

53
*
123
*
1900
*
*
*

17
17
17
17
17
*
*
*

117

Chapter 5. Behavioral Change Detection using Clustering Algorithm

discovery activities of other devices on the local network. Also, we do not monitor

local traﬃc coming to IoT device as this traﬃc is assumed to have originated from

another IoT device locally – this way, activity of local ﬂows is counted only for one

device (receiver). We have used MAC address as the identiﬁer of a device – one

may use an IP address (i.e., without NAT), physical port number, or VLAN for a

one-to-one mapping of a physical device to its traﬃc trace.

We use two key attributes [5] namely average packet size and average rate

for each of the eight ﬂows mentioned above. We also note that traﬃc attributes can

better characterize individual devices if they are computed at multiple time-scales

[137] particularly in the characterization of long-range dependent traﬃc. We, there-

fore, collect per-ﬂow packet and byte counts every minute, and compute attributes

at time-granularities of 1, 2, 4, and 8 minutes. This way we generate eight attributes

for each ﬂow that means a total of 64 attributes per device.

Extracting Attributes: In order to synthesize ﬂow entries and thereby extract

attributes from the traﬃc traces, we use our native packet-level parsing tool [5]. It

takes raw PCAP ﬁles as input, develops a table of ﬂows (like in an SDN switch)

and exports byte/packet counters of each ﬂow at a conﬁgurable resolution (e.g., 60

sec). Lastly, we generate a stream of instances (a vector of attributes periodically

generated every minute) corresponding to each of the individual devices.

We begin with DATA1, and use a month’s worth of its data (i.e., 01-Oct-2016

to 31-Oct-2016) for training and the following two weeks for testing our models –

the second column in Table 5.2 summarizes the number of training/testing instances

per each device type contained in this part of DATA1. Later in §5.3, we will use the

rest of DATA1 (spanning a longer period of traﬃc traces) to show how our models

detect changes in IoT behaviors.

118

Chapter 5. Behavioral Change Detection using Clustering Algorithm

Table 5.2: Summary of partial DATA1 (benign traﬃc): Device instances and clus-
tering parameters.

Device

Amazon Echo

Belkin motion

Belkin switch

Dropcam

HP printer

LiFX bulb

Netatmo cam

Netatmo weather

Samsung cam

Smart Things

Triby speaker

Instance count

Unsupervised classiﬁer
parameters

Training
(1-month)

Testing
(2-week)

# Principal
components

#
clusters

40843

35153

40991

41089

40713

36952

40788

24896

40841

41073

31898

18694

18780

18771

18787

18693

18707

18706

17473

18696

18799

18694

10877

19

17

18

9

13

14

15

9

16

13

15

12

256

256

256

128

128

256

512

128

256

256

256

128

Withings sleep sensor

32033

5.2.2 Attributes Clustering

Our primary objective is to train a number of models (one per IoT device) where each

model recognizes traﬃc patterns of a particular device type (i.e., class) and rejects

data from all other classes – i.e., one-class classiﬁer generates “positive” outputs for

a known/normal instances, and “negative” otherwise. This approach enables us to

re-train each model independently (in case of legitimate changes). Also, it has been

shown that device-specialized models can better detect anomalous traﬃc patterns

(outliers) [85]. There are a number of algorithms for one-class classiﬁcation. One

of the most common and eﬃcient methods is K-means [113] which ﬁnds groups of

instances (i.e., “clusters”) for a given class that are similar to one another. Each

cluster is identiﬁed by its centroid, and an instance is associated with a cluster if

the instance is closer to the centroid of that cluster than any other centroids.

To provide insights into traﬃc characteristics of IoT devices, we show in Fig. 5.1

clusters of instances for three representative devices namely, the Amazon Echo,

Belkin switch, and LiFX bulb from our dataset. Note that our instances are multi-

dimensional (i.e., 64 attributes), and thus can not be easily visualized. Therefore,

119

Chapter 5. Behavioral Change Detection using Clustering Algorithm

Figure 5.1: Clusters of data instances in two-dimensional space for representative
IoT devices: (a) Amazon Echo; (b) Belkin switch; and (c) LiFX bulb.

we employ the Principal Component Analysis (PCA) to project data instances to a

two-dimensional space just for illustration purposes – data instances are shown as

dots and cluster centroids are shown as crosses. Note that only 10% of instances are

shown in each cluster for better visualization – as an example, four dots in cluster

A1 of Amazon Echo, shown in Fig. 5.1.(a), approximately represent 40 instances.

Dotted circles depict the boundary of clusters. These boundaries will be used to

determine if a test instance belongs to clusters of a class or not. As per a rule of

thumb for ﬁnding outliers [170], a boundary for each cluster is chosen in a way to

exclude data points whose distance from the centroid is relatively large (i.e., values

more than 1.5 times the interquartile range from the third quartile). In other words

we deﬁne the boundary for each cluster that covers the ﬁrst 97.5% [171] of data

points closest to the cluster center and exclude farther instances to avoid impurities

in our training dataset.

It is important to note that an actual cluster forms a contour (enclosing asso-

ciated data points) which could form a complex shape. Given that our individual

models consist of tens of clusters we approximate the shape of their contours, and

hence make our classiﬁcation scheme computationally cost-eﬀective and more eﬃ-

cient. Furthermore, K-Means algorithm attempts to partition the training dataset

into spherical clusters when it is tuned optimally (i.e., equal distance from centroids

120

-200204060Principalcomponent1-200204060Principalcomponent2(a)AmazonEchoA3:22.2%A1:0.1%A2:25.1%-200204060Principalcomponent1(b)BelkinswitchB1:77.4%B2:19.7%B3:2.1%-200204060Principalcomponent1(c)LiFXbulbC3:19.8%C2:20.0%C4:1.0%C1:38.8%Chapter 5. Behavioral Change Detection using Clustering Algorithm

Figure 5.2: Distance probability of clusters: (a) C3 of LiFX bulb;(b) B3 of Belkin
switch; and (c) A1 of Amazon Echo.

in all dimensions). This way, spherical boundaries will be easily used to determine

if a test instance belongs to clusters of a class or not.

From Fig. 5.1, it is seen that instances of Amazon Echo, Belkin switch, and LiFX

bulb are grouped into 16, 4, and 8 clusters, respectively. We observe that instance

clusters of Amazon Echo are fairly spread across the 2D space. For Belkin switch,

clusters are mainly spread across the principal-component-1 while their principal-

component-2 is limited between

−

20 and 20. Lastly, LiFX bulb instances are spread

along the principal-component-2, while limited between

20 and 20 in the principal-

−

component-1. Note that each cluster of a class has a probability (“cluster likelihood”)

of covering training instances from the corresponding device type, depending upon

device traﬃc patterns seen in the training dataset. As annotated in Fig. 5.1, highly

probable clusters for Amazon Echo are A2 (25.1%) and A3 (22.2%), for Belkin

switch are B1 (77.4%) and B2 (19.7%), and for LiFX bulb are C1 (38.8%) and

C2 (20.0%). These clusters highlight the dominant traﬃc characteristics of their

respective device.

We also note that distribution of instances within each cluster also varies across

clusters. Fig. 5.2 shows a zoomed version of one cluster from each of our three

representative IoT devices – instances are shown by green dots. Each cluster is

divided into 10 equal bands starting from the centroid to the cluster boundary. For

each band, we compute a probability that indicates the fraction of training instances

121

-7-6-5-4-3Principalcomponent1-2-1012Principalcomponent2(a)ClusterC3ofLiFX101520Principalcomponent112162024(b)ClusterB3ofBelkinswitch5101520Principalcomponent1-8-4048(c)ClusterA1ofAmazonEcho0%5%10%15%20%25%30%35%Chapter 5. Behavioral Change Detection using Clustering Algorithm

it covers. The probability of bands is color coded on a linear scale (e.g., dark blue

indicates a higher probability).

It can be seen in Fig. 5.2a that 95% of LiFX instances inside cluster C3 fall under

four central bands of this cluster. Moving to B3 of the Belkin switch in Fig. 5.2b,

we observe that 81% of instances fall in middle bands (from 4th to 8th). Lastly,

looking at a less probable cluster of Amazon A1 in Fig. 5.2c, 85% of instances are

covered by the last ﬁve bands far from the centroid. We would like to reiterate that

the 2D space is used here for illustration purposes only. In our classiﬁcation scheme,

we employ a hyper-sphere in 64-dimensional space for clustering instances of IoT

traﬃc attributes.

5.3 Unsupervised Classiﬁcation of IoT Devices

In this section, we describe the architecture of our inference engine which consists

of a set of one-class models for individual device types. Next, we develop methods

to resolve conﬂicts between multiple models for device classiﬁcation. Finally, we

develop a scoring technique to measure the consistency of the models in classifying

IoT devices, identify two monitoring phases namely initial and stable, and detect

behavioral changes.

5.3.1 Clustering Models: Generation, Tuning, and Testing

Prior to generating clustering models, we need to pre-process our raw dataset.

First, we normalize each attribute independently to avoid outweighing large-value

attributes (e.g., average bytes rate of incoming remote traﬃc at 8-min timescale)

over smaller attributes (e.g., average packet size of outgoing NTP traﬃc at 1-min

timescale) [172] as the scale of value for diﬀerent attributes varies signiﬁcantly (i.e.,

several orders of magnitude). We employ the Z-score method (i.e., computing µ

122

Chapter 5. Behavioral Change Detection using Clustering Algorithm

Figure 5.3: Elbow method for selecting optimal number of clusters.

and σ from the training dataset) to scale individual attributes. Second, we project

data instances into a lower dimension space by using PCA [173] which results in lin-

early uncorrelated principal components. This is because our data is 64-dimensional

which can be computationally expensive for real-time prediction, and also aﬀect the

clustering performance (possibly getting biased towards less signiﬁcant attributes).

The orthogonal components enable K-means to detect clusters more clearly by re-

moving redundant and noisy attributes of training dataset. We choose the number of

PCA components to retain optimum “cumulative variance” [174] for our dimension

reduction engine.

Following dimension reduction, we apply K-means algorithm with varying K

values in the power-of-2 (i.e., 2i where i = 1, ..., 10). Note that setting K to small

values would not generate an accurate model of network behavior for IoT devices,

and large values increase the computational cost in both training and testing phases.

Also, a very large K results in smaller size clusters, and hence a rigid classiﬁer which

cannot correctly detect normal (legitimate) instances with small deviations from the

training data – i.e., over-ﬁtting. We ﬁnd the optimal number of clusters using the

elbow method [175]. Fig. 5.3 shows the average square distance of instances from the

cluster centers (i.e., Inertia per instance) versus clusters count for two representative

device types. The optimal cluster number (marked by ‘

×

’ on each curve) is chosen

when the ﬁrst derivative of inertia per instance exceeds a very small negative value

123

264128256512NumberofClusters0246810InertiaperinstanceAmazonEchoDropcamChapter 5. Behavioral Change Detection using Clustering Algorithm

0.01 (almost ﬂat) when increasing clusters count. It can be seen that the model

−
for Amazon Echo needs 256 clusters for optimal performance, and this measure

is 128 clusters for Dropcam. We show in the rightmost column of Table 5.2, the

model parameters for individual device types that are obtained from the methods

mentioned above.

Having clustering models generated, we test an instance of IoT traﬃc attributes

after scaling and dimension reduction, as shown by a sequence of steps in Fig. 5.4.

The test instance is presented to all of the device-speciﬁc models to ﬁnd the near-

est centroid of each model – the minimum of euclidean distances between the test

instance and clusters centroid is chosen. Given a nearest centroid, the instance is

checked against the corresponding cluster to determine if it falls inside or outside

of that cluster boundary, and if inside, compute a conﬁdence level. To better illus-

trate this process, let us consider the two-dimensional space of clusters we discussed

earlier in Fig. 5.1. Assume that a test instance has its principal component-1 and

component-2 equal to 0 and 20, respectively. The nearest centroids to this test in-

stance are A1 of Amazon Echo, B2 of Belkin switch, and C4 of LiFX. Since the test

instance falls outside of A1 boundary, the Amazon Echo model results in a negative

output while the other two models give positive outputs. In what follows next, we

show how to select the “winner” model in case of multiple positive outputs for a test

instance.

5.3.2 Conﬂict Resolution

Each model learns the normal behavior of one device type. Diﬀerent devices may

display a slightly similar traﬃc behavior (e.g., DNS, NTP or SSDP) for a short

period of time [136]. This can result in multiple positive outputs generated by our

clustering models for an instance. We address this issue by using the “conﬁdence”

of models which give positive output: the model with the highest conﬁdence-level is

selected as the winner.

124

Chapter 5. Behavioral Change Detection using Clustering Algorithm

Figure 5.4: Use of each clustering model for a test instance.

Conﬁdence-level: We derive a probability value for test instances to be asso-

ciated with a cluster of that model - we call it “associate probability”. Given an

instance Ins receiving a positive output from a model Mi and falling in a distance

band Dl of the nearest cluster Cj (of the model Mi), the associate probability is

estimated by:

[Ins|Mi(Cj (Dl))] = P train
P test

[Cj |Mi] ×

P train
[Dl|Cj ]

(5.1)

where P train
[Cj |Mi]
is the probability of distance band Dl inside the cluster Ci – both probability

is the likelihood of the nearest cluster Cj within the model Mi and

P train
[Dl|Cj ]

values are obtained from the training dataset. We note that P train
[Cj |Mi]

is always non-

zero (by optimal tuning [176]), but it is possible to have P train

(Dl|Cj )

equal to zero when

none of the training instances fall inside a band Dl (i.e., unexplored distance bands

in the training data). To avoid a zero conﬁdence for test instances, we slightly modify

the band probability using the Laplacean prior[156], priming each band instances

count with a count of one, as given by:

P train
[Dl|Cj )] =

1 + NDl
L + NCj

(5.2)

125

Unsupervised one-class classifierAttributes instancecluster boundaryin/out?Confidence levelCheck clusterZ-Score scaler Dimension reduction engine (PCA)Test:Find nearestcentroid1234Chapter 5. Behavioral Change Detection using Clustering Algorithm

Figure 5.5: Distribution of clustering probability for training instances of three
device types.

where NDl

is the number of training instances inside the band Dl; NCj

is the

total number of instances in the cluster Cj; L is the total count of distance bands

in the cluster – we use ten bands in every cluster (L = 10).

The associate probability, to some extent, indicates the model conﬁdence. How-

ever, it becomes challenging to select the winner among multiple models giving

positive output since the number of clusters and also the distribution of distance

bands vary across models, and hence the associate probability is scaled diﬀerently.

For example, models with large number of clusters may have relatively smaller val-

, or a cluster with highly sparse bands would result in smaller values

ues of P train

(Cj |Mi)

of P train

(Dl|Cj )

.

To obtain a metric of conﬁdence for comparison across models, we scale the as-

sociate probability (computed above for a test instance) by using the distribution

of this probability in a training dataset. To better illustrate this scaling process,

we show in Fig. 5.5 the cumulative distribution function (CDF) of the associate

probability for training instances from three IoT models namely, Amazon Echo,

Netatmo cam, and Smart Things. For example, we observe that the associate prob-

ability equals to 0.5% is a high value for Amazon Echo model (shown by dotted blue

lines) keeping it above more than 99% of training instances. However, this measures

becomes 90% and 52% for Smart Things (dashed green lines) and Netatmo (solid or-

126

0.00.51.01.52.02.53.03.54.04.5X:associateprobability(%)0.00.10.20.30.40.50.60.70.80.91.0CDF:Prob[associate-prob≤x]AmazonEchoNetatmocamSmartThingsChapter 5. Behavioral Change Detection using Clustering Algorithm

Figure 5.6: Architecture of our inference engine.

ange) models, respectively. Therefore, given the associate probability (from a model

that gives positive output for a test instance) we derive the model conﬁdence-level

by computing the fraction of its training data that fall below the test instance (with

respect to the model’s empirical CDF of associate probability).

5.3.3 Consistency Score

Ideally, for monitoring individual IoT devices we expect consistent outputs to be

generated by the inference engine over time. It is important to note that a given

device which is consistently and correctly classiﬁed by a model over a period of time

(say, a week), may occasionally get missed (i.e., negative output) by its intended

model. To bootstrap the monitoring process for a newly connected (and possibly

unknown) device, we initially need this device to consistently receive positive outputs

from one of the existing models in order to accept the device and label it by a known

class (“stable state”). Once a device becomes known (accepted) and is at its stable

state, receiving negative outputs frequently from its indented model, which indicates

a change (legitimate or illegitimate) in the device behavior and thus requires further

investigations.

We develop a score (between 0 and 1) to track the consistency of our device

classiﬁcation – we call it “consistency score”. Fig 5.6 shows the architecture of our

127

Model D1Clustering modelsAttributes instanceModel D2…Model DN…Conflict ResolverInference EnginePos.[Confidence]Pos.[Confidence]Neg.Neg.Pos.Consistency ScoreD1D2…DN…0.75⇧0.01⇩…    ⇩0.01 ⇩…    ⇩Device ModelsScorePos.Neg.Neg.Neg.Neg.[Confidence]Chapter 5. Behavioral Change Detection using Clustering Algorithm

Figure 5.7: Dynamics of consistency score for Smart Things instances in real-time.

inference engine consisting of classiﬁcation followed by consistency scoring. For

instances of a given device, the consistency score is computed and tracked per each

model and updated following classiﬁcation of instance – the consistency score of a

model rises by its positive output and falls by its negative output over time. To better

understand the dynamics of this score, let us begin with an example. We take three

day’s worth of instances from Smart Things device, and present (real-time replay)

this traﬃc to four trained models including Smart Things, Netatmo camera, Amazon

Echo, and Withing Sleep sensor. We show in Fig. 5.7 the consistency score of these

four models in real-time. It can be seen that as we expect the score of intended

model Smart Things (shown by solid lines) is dominant while the other three are

negligible (and hence invisible). For Smart Things the score slowly rises and reaches

to a high level of 0.8 after about 30 hours. We also observe that sometimes the score

of the intended model falls slightly and rises again – this is because some instances

may display patterns closer to other models. We zoom in to the gray band region

(Nov 4, 11pm - Nov 5, 2am) to see the magniﬁed score of other models. It is observed

that once other models give positive output their score quickly spikes, but soon after

drops back to zero (shown by dotted red lines for Netatmo cam) as the intended

model Smart Things wins again.

128

Nov4,12amNov4,6amNov4,12pmNov4,6pmNov5,12amNov5,6amNov5,12pmNov5,6pmNov6,12amNov6,6amNov6,12pmNov6,6pmNov7,12amTime0.00.10.20.30.40.50.60.70.80.91.0ConsistencyscoreSmartThingsNetatmocamAmazonEchoWithingssleepsensor11pm11pm12am12am1am1am2am0.01000.01020.01040.50000.6000Chapter 5. Behavioral Change Detection using Clustering Algorithm

Figure 5.8: Real-time update rate of consistency score – for a given model, it falls
fast (from highest to lowest value in 3 hours) on continuous negative outputs, and
rises slowly (from lowest to highest value in 24 hours) on continuous positive outputs.

We update the consistency score with two rates: rising on positive output at

rate λr and falling on negative output at rate λf – these rates can be conﬁgured by

network operators. To update the consistency score, we use sigmoid functions which

are commonly used in processes like trust management [177] as they exhibit a soft

start and end, and are bounded within 0 and 1. The raw score is represented by

sequence

St

{

}

beginning at time t = 0, is dynamically updated by:

St =

St−1
1 + St−1

eλ
(eλ

×
×

1)

−

(5.3)

where, St−1 is the previous value of the estimated score, and λ is set dynamically

depending on the latest output of the model (i.e., λr > 0 for positive output, and

λf < 0 for negative output). Network operators can conﬁgure their λr and λf based

on their preferred policy in terms of how quickly (or slowly) they want to rise/fall

the score. Depending upon the time expected T to reach to a “target score” S∗

(between 0 and 1) from the mid-level score 0.50, we derive the value λ by:

λ =

log( S∗
1−S∗ )
T

(5.4)

129

024681012141618202224Hours0.00.10.20.30.40.50.60.70.80.91.0Consistencyscore1.5hours12hoursRisingonpositiveoutputFallingonnegativeoutputChapter 5. Behavioral Change Detection using Clustering Algorithm

In this chapter, we choose a conservative policy whereby the consistency score

rises at slower rate than it falls. Our λ values are the same for all models and

conﬁgured in such a way that it will take 12 hours to reach to a very high score 0.99

from the score 0.50 (λr = 0.0064) in case of successive positive outputs from the

model, while it will need only 1.5 hours to reach a very small score 0.01 from the

score 0.50 (λf =

−

0.0511) in case of successive negative outputs. Fig. 5.8 shows two

sample curves of consistency scores with our chosen λ values, each is monotonically

rising and falling on successive positive and negative outputs respectively. We note

that both curves saturate (reaching to ultimate values 0 and 1) in inﬁnite time, and

change very slowly beyond certain levels, i.e., above 0.99 for the rising curve and

below 0.01 for the falling curve. In other words, entering into these regions can stiﬂe

agility of our real-time monitoring (specially for detecting attacks in real-time). For

example, in order to fall from 0.999 to 0.99 it will take at least 45 minutes (half the

time needed to fall from 0.99 to 0.50). Similarly, it needs 6 hours to rise from 0.001

to 0.01. Therefore, we cap the score at 0.99 and 0.01 as our saturation levels, and

also initialize the score by S0 = 0.01.

5.3.4 Monitoring Phases

For monitoring behavior of each IoT device we consider two phases: (1) initial

phase, and (2) stable phase.

Initial phase begins once a device connects to the

network for the ﬁrst time (discovered). During this phase, our inference engine

(shown in Fig. 5.6) aims determine the device type (classiﬁcation) by asking all

existing models. To achieve this aim, every instance of the device traﬃc is fed to all

models in real-time and their outputs are obtained. If multiple models give positive

outputs, then our conﬂict resolution is applied to choose a winner model. During this

phase, the consistency score of all winner models (giving positive output) is tracked

till the score of one model reaches an acceptable level (i.e., a threshold chosen by

the network operator, say 0.90) whereby the device type is veriﬁed. At this point

130

Chapter 5. Behavioral Change Detection using Clustering Algorithm

the device gets labeled by a known class, its intended model is determined, and its

initial phase completes. Stable phase begins upon completion of the initial phase.

In the stable phase, the inference engine uses only the intended model to monitor

the real-time behavior of the device. In the next section, we see how the consistency

score of the intended model will be used to detect any change of behavior.

5.4 Performance Evaluation

We now evaluate the eﬃcacy of our inference engine. First, we evaluate the perfor-

mance of one-class models and conﬂict resolution in selecting an intended model for

a given device during its initial phase of monitoring. Once the device type is clas-

siﬁed (with suﬃciently high level of consistency), we next demonstrate behavioral

changes using temporal consistency score of the intended model during its stable

phase of monitoring. Finally, we show the eﬃcacy of our inference engine. We also

compare our one-class classiﬁcation with a multi-class classiﬁcation method.

5.4.1 Device Classiﬁcation

We begin by evaluating the performance of device classiﬁcation using part of test

instances from DATA1 (i.e., only two weeks spanning from 1-Nov-2016 to 14-Nov-

2016). We show in Fig. 5.9 the confusion matrix of classiﬁcation before and after

resolving conﬂicts. Every clustering model (listed in rows) is presented by test

instances of IoT devices (listed in columns). For a given cell, the value indicates

the percentage of instances (from the device in corresponding column) that receive

positive output from the model in the corresponding row.

Starting from raw outputs in Fig. 5.9a, it can be seen that all models correctly

detect majority of instances from their own class as shown by diagonal elements

of the confusion matrix – except Triby speaker with 88.9%, others display more

131

Chapter 5. Behavioral Change Detection using Clustering Algorithm

(a) Raw output of clustering models.

(b) Reﬁned output after conﬂict resolution.

Figure 5.9: Confusion matrix of device classiﬁcation: (a) raw output of clustering
models; and (b) reﬁned output after conﬂict resolution.

than 93.5% of correct detection (i.e., true positive). However, we observe that

models incorrectly detect device instances from other classes (i.e., false positive) as

shown by non-diagonal elements of the confusion matrix. For example, models for

132

AmazonEchoBelkinmotionBelkinswitchDropcamHPprinterLiFXbulbNetatmoweatherNetatmocamSamsungcamSmartThingsTribyspeakerWithingssleepsensorTestinstancesAmazonEchoBelkinmotionBelkinswitchDropcamHPprinterLiFXbulbNetatmoweatherNetatmocamSamsungcamSmartThingsTribyspeakerWithingssleepsensorClusteringmodels93.52.185.199.399.89.432.690.50.032.396.498.90.094.635.80.198.90.24.146.70.019.833.559.40.00.097.30.00.30.014.92.60.00.04.02.30.00.00.098.231.50.20.80.00.00.00.00.00.00.010.50.198.40.11.00.10.10.40.10.40.00.00.00.086.695.00.80.00.00.00.10.00.10.00.10.00.05.897.90.10.00.00.60.126.20.419.80.02.675.535.796.92.276.349.929.80.00.00.00.00.00.00.00.096.60.10.00.00.00.00.00.00.00.00.024.00.096.93.10.00.00.01.70.00.615.86.51.91.30.488.93.37.00.00.20.35.26.01.479.419.041.263.996.6AmazonEchoBelkinmotionBelkinswitchDropcamHPprinterLiFXbulbNetatmoweatherNetatmocamSamsungcamSmartThingsTribyspeakerWithingssleepsensorTestinstancesAmazonEchoBelkinmotionBelkinswitchDropcamHPprinterLiFXbulbNetatmoweatherNetatmocamSamsungcamSmartThingsTribyspeakerWithingssleepsensorClusteringmodels93.00.11.31.31.30.80.21.10.02.25.02.60.094.50.20.00.00.00.30.10.00.00.00.20.00.094.00.00.00.00.10.00.00.00.10.00.00.00.098.10.00.00.00.00.00.00.00.00.00.00.00.096.10.00.00.00.00.00.00.00.00.00.00.00.491.30.00.00.00.00.00.00.00.00.00.00.00.396.10.10.00.00.00.11.40.13.70.01.96.21.996.50.48.35.31.40.00.00.00.00.00.00.00.096.40.00.00.00.00.00.00.00.00.00.00.10.088.90.00.00.00.00.40.00.00.00.10.10.20.086.00.10.60.00.00.00.20.10.01.20.40.43.195.6Chapter 5. Behavioral Change Detection using Clustering Algorithm

Amazon Echo and Belkin motion incorrectly give positive output to 99.8% and 98.9%

of instances from HP printer. Considering the raw outputs of various models, we

found 70% of test instances are detected by more than one model (in addition to their

expected model), and 2% of test instances are not detected by any of the models.

Next, we select the winner model for each test instance using model conﬁdence-level

(§5.3.2).

Fig. 5.9b shows the confusion map after conﬂict resolution. It clearly shows a

signiﬁcant enhancement in performance of our device classiﬁcation by selecting the

model with the highest conﬁdence. Note that the average false positive rate has

reduced to less than 0.4% while the average true positive rate is 93.9%.

Also, we observe that the conﬂict resolver has slightly reduced the rate of true

positive for almost all models. Note that Smart Things is impacted more compared

to other models by experiencing a drop from 96.9% to 88.9% in its true positive

rate largely because of the Netatmo camera model which gives positive output with

high conﬁdence for 8.3% of Smart Things instances. Focusing on the model of

Netatmo camera, we found that its clusters overlap with a number of clusters of

several devices such as Belkin switch, LiFX, Smart things, and Triby speaker, and

hence results in false positives. This is mainly because of the aperiodic behavior

of the Netatmo camera which is event triggered – camera transmits video to its

cloud server whenever it recognizes a human face or detects a motion. As a result,

it displays a wider range of activity pattern at longer time scales, overlapping with

traﬃc patterns of other devices. For example, the average byte rate of incoming

NTP traﬃc of Netatmo camera at 8-min timescale can take a value from 0 to 700

bytes-per-min. Such a wide range overlaps with the value range of the same attribute

for LiFX bulb (varying between 8

−

between 15

−

25 bytes-per-min).

12 bytes-per-min) and Smart Things (varying

133

Chapter 5. Behavioral Change Detection using Clustering Algorithm

(a) Belkin switch.

Figure 5.10: Time-trace of consistency score for normal behavior in: (a) Belkin
switch; and (b) Triby speaker.

(b) Triby speaker.

5.4.2 Detecting Behavioral Change

In the previous subsection, we showed the eﬃcacy of our system in classifying in-

dividual device instances using an array of models. Once classiﬁed, we monitor

activity of each IoT device in real-time using its intended model. We now check how

our models highlight behavioral changes by tracking dynamics of their consistency

scores (§5.3.3). For this evaluation, we use a longer portion of DATA1 spanning

from 01-Nov-2016 to 31-Mar-2017.

Fig 5.10a shows the consistency score of our inference engine, for traﬃc instances

of Belkin switch over a period between Nov 1, 2016 and Jan 28, 2017. It is seen that

the score ramps up to 99% within the ﬁrst 48 hours – the device then goes oﬄine

for a day as shown by dashed gray lines, and comes back online on Nov 4. After

134

Nov 1, 2016Nov 8, 2016Nov15,20160.00.10.20.30.40.50.60.70.80.91.0Consistency score14daysJan 17, 2017Jan 24, 201714daysActiveOfﬂineTimeNov 1, 2016Nov 8, 20160.00.10.20.30.40.50.60.70.80.91.0Consistency score8daysSIPServerdownSIPServerup1dayNov 22, 20168daysSIPServerdownSIPServerup12hoursActiveOfﬂineTimeChapter 5. Behavioral Change Detection using Clustering Algorithm

Figure 5.11: Wireshark capture of Triby speaker packets showing outage of SIP
server.

that device instances are consistently detected by the intended model, and hence

the score remains high with minor changes over this long period.

Fig 5.10b illustrates a scenario where consistency score drops for a relatively short

period of time (due to temporary change of behavior in traﬃc of Triby speaker),

and rises afterwards. We manually inspected packet traces corresponding to these

temporary drops of the score. We found that a remote SIP server (sip.invoxia.com),

with which Triby speaker keeps a continuous TCP connection, was responding with

ACK/RST packets during those periods, as highlighted by red rows in Fig. 5.11,

indicating the expected SIP service was not operational. Note that other services

for Triby speaker were functional normally.

We also note that the behavior of a device may change permanently due to

ﬁrmware upgrade. Fig 5.12a illustrates this scenario for Dropcam. We can see that

the consistency score of Dropcam model remains high throughout Nov 2016 until

Dec 5, 2016 when the device goes oﬀ-line, as shown by dotted gray lines – a couple

of slight drops are observed on Nov 15 and Nov 22 which get restored fairly quickly

(infrequent mis-classiﬁcation is not surprising due to minor overlaps between clusters

135

TimesrcIPdstIPProtocolsrcPortdstPortInfo2016-11-01  18:11:23192.168.1.120cfapmxc001-xsaDNS5850253Standard query 0x02d1 A sip.invoxia.co2016-11-01  18:11:23cfapmxc001-xsa192.168.1.120DNS5358502Standard query response 0x02d1 A sip.invoxia.com2016-11-01  18:11:23192.168.1.120sip.invoxia.coSIP521805228Request: REGISTER sip:sip.invoxia.com2016-11-01  18:11:23sip.invoxia.co192.168.1.120SIP522852180Status: 200 OK  (1 binding) | 2016-11-01  18:11:26192.168.1.120sip.invoxia.coTCP52180522852180  >  5228 [ACK] Seq=182817 Ack=168520 Win=65360 Len=0 2016-11-01  18:12:20sip.invoxia.co192.168.1.120TCP5228521805228  >  52180 [FIN, ACK] Seq=168520 Ack=182817 Win=63063 Len=0 2016-11-01  18:12:20192.168.1.120sip.invoxia.coTCP52180522852180  >  5228 [FIN, ACK] Seq=182817 Ack=168521 Win=65360 Len=0 2016-11-01  18:12:20sip.invoxia.co192.168.1.120TCP5228521805228  >  52180 [ACK] Seq=168521 Ack=182818 Win=63063 Len=0 TSval=3211580419 TSecr=376178112016-11-01  18:12:30192.168.1.120sip.invoxia.coTCP49814522849814  >  5228 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 2016-11-01  18:12:30sip.invoxia.co192.168.1.120TCP5228498145228  >  49814 [RST, ACK] Seq=1 Ack=1 Win=0 Len=02016-11-01  18:12:31192.168.1.120sip.invoxia.coTCP498145228[TCP Retransmission] 49814  >  5228 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 2016-11-01  18:12:31sip.invoxia.co192.168.1.120TCP5228498145228  >  49814 [RST, ACK] Seq=1 Ack=1 Win=0 Len=02016-11-01  18:15:04192.168.1.120blue.invoxia.iTCP351158090[TCP Keep-Alive] 35115  >  8090 [ACK] Seq=1 Ack=1 Win=30016 Len=0 TSval=37634203 TSecr=32114465542016-11-01  18:15:04blue.invoxia.i192.168.1.120TCP809035115[TCP Keep-Alive ACK] 8090  >  35115 [ACK] Seq=1 Ack=2 Win=14480 Len=0 2016-11-01  18:15:05192.168.1.120blue.invoxia.iTCP351158090[TCP Keep-Alive] 35115  >  8090 [ACK] Seq=1 Ack=1 Win=30016 Len=0 2016-11-01  18:15:05blue.invoxia.i192.168.1.120TCP809035115[TCP Keep-Alive ACK] 8090  >  35115 [ACK] Seq=1 Ack=2 Win=14480 Len=0 2016-11-01  18:16:04192.168.1.120sip.invoxia.coTCP36682522836682  >  5228 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 2016-11-01  18:16:04sip.invoxia.co192.168.1.120TCP5228366825228  >  36682 [RST, ACK] Seq=1 Ack=1 Win=0 Len=02016-11-01  18:16:05192.168.1.120sip.invoxia.coTCP366825228[TCP Retransmission] 36682  >  5228 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 TSval=37640299 TSecr=02016-11-01  18:16:05sip.invoxia.co192.168.1.120TCP5228366825228  >  36682 [RST, ACK] Seq=1 Ack=1 Win=0 Len=02016-11-01  18:19:35192.168.1.120cfapmxc001-xsaDNS5850253Standard query 0x02d2 A sip.invoxia.co2016-11-01  18:19:35cfapmxc001-xsa192.168.1.120DNS5358502Standard query response 0x02d2 A sip.invoxia.co2016-11-01  18:19:35192.168.1.120sip.invoxia.coTCP54643522854643  >  5228 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 2016-11-01  18:19:35sip.invoxia.co192.168.1.120TCP5228546435228  >  54643 [RST, ACK] Seq=1 Ack=1 Win=0 Len=02016-11-01  18:19:36192.168.1.120sip.invoxia.coTCP546435228[TCP Retransmission] 54643  >  5228 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 2016-11-01  18:19:36sip.invoxia.co192.168.1.120TCP5228546435228  >  54643 [RST, ACK] Seq=1 Ack=1 Win=0 Len=0Chapter 5. Behavioral Change Detection using Clustering Algorithm

(a) Original model (Dropcam).

(b) Re-trained model (Dropcam).

Figure 5.12: Time-trace of consistency score due to ﬁrmware upgrade in Dropcam
traﬃc: (a) original model; and (b) re-trained model.

of various models). However, once Dropcam comes back online on Dec 25, the score

steeply drops to its lowest possible value 0.01 and stays at that level during the

whole January. It is seen that the score sometimes jumps up to 0.20 or even 0.30,

but it quickly drops back to its minimum value. We again manually inspect the

packet traces of Dropcam and found its behavior permanently changed. Note that

the Dropcam network activity is dominated by a single TLS connection which the

device establishes with its cloud server (nexus-us1.dropcam.com) [6], typically sending

packets of size 156 bytes and receiving packets of size 66 bytes. Manual inspections

revealed that the rate of packets for this ﬂow changed in both directions (while

packet sizes remained unchanged), resulting in a decrease of upstream bitrate from

1896 bps to 1120 bps and downstream bitrate from 584 bps to 424 bps. It is also

important to note that the ﬁrmware upgrade for Dropcam is done automatically

136

Nov 1, 2016Nov 8, 2016Nov15,2016Nov22,20160.00.10.20.30.40.50.60.70.80.91.0Consistency score28daysDec 27, 2016Jan 3, 2017Jan10,2017Jan17,2017Jan24,201736daysFirmwareupdateActiveOfﬂineTimeNov 1, 2016Nov 8, 2016Nov15,2016Nov22,20160.00.10.20.30.40.50.60.70.80.91.0Consistency score28daysDec 27, 2016Jan 3, 2017Jan10,2017Jan17,2017Jan24,201736daysretraining(2weeks)FirmwareupdateActiveOfﬂineTimeChapter 5. Behavioral Change Detection using Clustering Algorithm

when it reboots. Conﬁrming ﬁrmware updates (after they occur) requires manual

inspection only when our inference engine ﬂags anomalies. Note that automatic

detecting of ﬁrmware updates requires labeled instances which are not available to

us – each manufacturer uses a diﬀerent method to update device ﬁrmware remotely.

Therefore, we are not able comment on the ability of our attributes in detecting

ﬁrmware updates.

Once it is conﬁrmed, we re-train the Dropcam model using

an additional two weeks’ worth of data (between Dec 25, 2016 and Jan 07, 2017)

after this ﬁrmware upgrade – adding new instances to training dataset resulted in

an increase in the number of PCA components (from 9 to 11) for Dropcam, while

the number of clusters remained the same. Fig. 5.12b shows how consistency score

returns back to its perfect level after augmenting the Dropcam model with attributes

of the ﬁrmware upgrade – the score is shown by dashed gray lines during the two

weeks of re-training period.

5.4.3 Detecting Attacks

We now evaluate the performance of our inference engine against attack traﬃc. For

this evaluation, we use our second dataset DATA2.

It consists of well-annotated

attack and benign traﬃc of ten real IoT devices namely Amazon Echo, TPlink

switch, Belkin motion sensor, Belkin switch, LiFX bulb, Netatmo camera, Hue bulbs,

iHome switch, Samsung Smart camera, and Google Chromecast. These attacks on

IoT devices are in various types including directly targeted attacks such as ARP

spooﬁng, TCP SYN ﬂooding, Fraggle (UDP ﬂooding), and Ping of Death, and also

reﬂection attacks such as SNMP, SSDP, TCP SYN, and Smurf. Each type is at

three diﬀerent rates (i.e., low: 1 packet-per-second, medium: 10 pps, and high: 100

pps). Additionally, attacks are diversiﬁed in terms of the location of attacker being

remote or local to victim/reﬂector IoT devices. In total, DATA2 contains 200 attack

sessions, and each lasts for around 10 minutes.

137

Chapter 5. Behavioral Change Detection using Clustering Algorithm

Table 5.3: Summary of DATA2: device instances and clustering parameters – benign
traces for training, and mix of benign and attack traces for testing).

Instance count
Testing
(4-week)
27510
37216
12689
24316
25830
26181
10639
36747
35205
35761

Training
(4-week)
27102
38229
21038
17396
17329
25903
13529
38227
38211
37866

Unsupervised classiﬁer
parameters

# Principal
components
20
13
17
17
19
15
16
15
14
16

#
clusters
256
256
256
512
512
256
256
256
128
128

Device
Amazon Echo
Belkin motion
Belkin switch
Chromecast
Hue bulb
LiFX bulb
Netatmo cam
Samsung cam
TPlink switch
iHome

Note that DATA2 was collected from a diﬀerent IoT environment, and therefore

we need to regenerate our clustering models using data of IoT behaviors speciﬁc to

that environment. From DATA2 traces, we choose four weeks’ worth of data (i.e.,

May 28-31, Jun 8-19, Oct 9-19) containing pure benign traﬃc for training, and the

remaining four weeks (Jun 1-8, Jun 19-20, Oct 19-Nov 10) containing a mix of benign

and attack traﬃc for testing. Table 5.3 shows the number of instances (training and

testing) per each device as well as parameters of individual clustering models.

Let us now evaluate the eﬃcacy of individual models against traﬃc mix of attack

and benign instances. We measure four metrics: fraction of attack instances getting

negative output (TN: true negative), fraction of benign instances getting negative

output (FN: false negative), fraction of benign instances getting positive output

(TP: true positive), and fraction of attack instances getting positive output (FP:

false positive). On average, our models yield acceptable performance metrics – TN,

FN, TP, and FP equals to 92.0%, 6.1%, 93.9%, and 8.0%, respectively.

Focusing on attacks, Table 5.4 and 5.5 show the detection rate of our models for

direct and reﬂection attacks. Each attack type-location scenario in columns (e.g.,

ARP Spooﬁng R

D: remote attacker launching direct spooﬁng attack to device) is

→

repeated three times at rates 1, 10, and 100 pps.

138

Chapter 5. Behavioral Change Detection using Clustering Algorithm

Starting from Table 5.4 corresponding to direct attacks, it can be seen that the

average detection rate for ARP Spooﬁng, Ping of Death, TCP SYN ﬂooding, and

Fraggle is 84.3%, 89.4%, 91.3%, and 86.2%, respectively. However, we observe that

the Belkin motion model displays a poor performance in detecting attacks launched

from local attackers (highlighted cells). For example, the detection rates of Ping of

Death, TCP SYN ﬂooding, and Fraggle are 43.3%, 13.3%, and 3.0% respectively.

This is mainly because Belkin motion typically communicates with its mobile App

locally by UPnP messages reporting current state of the sensor, and hence local

attacks are not seen as so abnormal by the corresponding model – soon we will

further investigate and address this issue.

Moving to Table 5.5 to check the performance of models against reﬂection at-

tacks, we see the rate of detection for Smurf, SNMP, SSDP and TCP SYN reﬂection

attacks on average is 99.1%, 58.8%, 88.5%, and 92.0%, respectively. Again, we ob-

serve that some of broadcast attacks (i.e., SSDP reﬂection attack on Chromecast)

and local attacks (i.e., TCPsyn on Belkin motion and SNMP on Samsung cam)

are missed. This is primarily because we only monitor local traﬃc targeted to IoT

devices (§5.2.1), and hence broadcast traﬃc and reﬂected outgoing local traﬃc gets

missed.

It is important to note that models of Belkin motion and Hue bulb are

detecting local SSDP reﬂection attacks (L

→

D

L) only because these two devices

→

have limited processing power, and hence under local SSDP attacks their normal

operation (activity pattern of other ﬂows) gets impacted leading to abnormal be-

havior.

Table 5.4: Detection rate (%) of direct attacks: per model (in rows) and per attack-
type (in columns).

Attack
Attacker
1
Rate
Amazon Echo
100
Belkin motion 80
Belkin switch
100
Chromecast
80
Hue bulb
70
LiFX bulb
88
Netatmo cam 25
Samsung cam 100
TPlink switch
60
iHome
100

D

L
→
10
75
70
90
80
90
100
88
90
70
100

100
88
80
100
90
90
100
75
90
60
100

ARP Spooﬁng Ping of Death

TCP SYN

L

D
→
10 100

D

L
→
10

1

1

100 1

D

R

→
10

100 1

70
100

50
100

10
100

90
100

90
100

100
100

100
100

100
100

100
100

10
100
100
100

100
100
100

30
100
100
100

100
100
100

0
100
100
100

100
100
100

75
100
50
100

80
100
100

100
100
70
100

100
100
100

95
100
90
100

100
100
100

139

Fraggle

D

L
→
10
100
0

100 1
100
10

100

R

D

→
10
100

100
100

100
0

100

100

100

100

100

100

100

100

100

100

100

100

Chapter 5. Behavioral Change Detection using Clustering Algorithm

Table 5.5: Detection rate (%) of reﬂection attacks: per model (in rows) and per
attack-type (in columns).

Attack
Attacker &
Victim

Rate
Amazon Echo
Belkin motion
Belkin switch
Chromecast
Hue bulb
LiFX bulb
Netatmo cam
Samsung cam 100
TPlink switch
100
iHome

90
100

Smurf

SNMP

SSDP

TcpSynReﬂection

L
1

→

L

D
10 100 1 10 100

→

→

→

D

L

L

L
1

→

R

D
→
10 100

R
1

→

D
R
→
10 100

L

L
D
→
1 10 100

→

L
1

→

D
R
10 100

→

R
1

→

D
R
→
10 100

L
1

→

D
L
→
10 100

R
1

→

D
R
→
10 100

100
100

100
100

100
100

100
100

0

0

0

30

100

100

100

100

100

100

90

80

100

100

100

100

100

100

0
100

30
90

0
100

100
100

100
100

100
100

100
100

100
100

100
100

30
100
100
100

100
100
100

30
100
100
100

100
100
100

0
100
100
100

100
100
100

85
100
40
100

100
100
100

95
95
90
100

100
100
100

100
100
100
100

100
100
100

Figure 5.13: CDF: distribution of conﬁdence-level for Belkin motion instances.

Enhancing Detection Rate: As discussed earlier in this section, models in gen-

eral perform well for a mix of benign and attack traﬃc except in certain situations.

Among all models, we found that the Belkin motion model does not perform well

especially for attack instances, and results in a relatively high FP 32.1%. To further

investigate such performance, we look at its conﬁdence-level. Fig. 5.13 shows the

CDF of the Belkin model conﬁdence for incorrectly classiﬁed attack instances (FP)

as well as correctly classiﬁed benign instances (TP). We note that the model gives a

very low conﬁdence-level (less than 2.5%) for a majority (61%) of the FP instances

while such low conﬁdence is seen for a tiny fraction (3%) of the TP instances. Again

the acceptable conﬁdence-level will be chosen by the network operator depending

on their desired sensitivity.

In our case, choosing conﬁdence threshold 2.5%, the

performance metrics is signiﬁcantly enhanced for Belkin model – FP is improved

down to 12.5% while TP is slightly degraded (from 95.5% to 92.6%).

140

0102030405060708090100X:conﬁdence(%)0.000.100.200.300.400.500.600.700.800.901.00CDF:Prob[conﬁdence≤x]TPFPChapter 5. Behavioral Change Detection using Clustering Algorithm

Such enhancement is observed across all models after ﬁltering model outputs

with conﬁdence less than 2.5%, and thus overall TN, FN, TP, and FP reaches to

94.7%, 9.03%, 92.0%, and 5.3%, respectively across all models. We show in Table 5.6

the performance of individual models after this enhancement. We can see that every

model now displays acceptable value in performance metrics (high rate of true alarms

and low rate of false alarms).

Performance Comparison of One-Class vs. Multi-Class: Lastly, we com-

pare the performance of our one-class classiﬁer scheme versus previously studied

multi-class classiﬁers (including Chapter 3). For our comparison, we use Random

Forest algorithm (based on decision trees) to generate and tune a multi-class model

using the training instances (same as for our one-class models) from DATA2.

Before comparing the two schemes, we need our devices to operate in their stable

phase of monitoring. Note that, the ﬁrst two days of testing data contains pure

benign traﬃc from all of the ten devices. This amount of data is suﬃcient for all of

intended one-class models to be selected (consistency score of winner models exceeds

our chosen threshold 0.90). In other words, every device passes its initial phase and

enters into the stable phase. In the stable phase, the inference engine is expected

to give negative output whenever attack traﬃc instances are present and generate

positive output for pure benign traﬃc.

Table 5.6: Performance of one-class classiﬁers for mix of attack and benign traﬃc.

Detected as

attack

benign

TN (%) FN (%) TP (%) FP (%)

Amazon Echo
TPlink switch
Belkin motion
Belkin switch
LiFX bulb
Netatmo cam
Hue bulb
iHome
Samsung cam
Chromecast

98.8
95.9
87.5
99.2
99.3
94.6
97.3
100.0
92.4
81.9

6.0
5.8
7.4
8.1
7.3
6.0
15.7
7.1
6.7
10.2

94.0
94.2
92.6
91.9
92.7
94.0
84.3
92.9
93.3
89.8

1.2
4.1
12.5
0.8
0.7
5.4
2.7
0.0
7.6
18.1

141

Chapter 5. Behavioral Change Detection using Clustering Algorithm

(a) one-class clustering model.

(b) multi-class decision-tree model.

Figure 5.14: Performance comparison for traﬃc of Samsung cam during attack: (a)
one-class model; and (b) multi-class model.

Fig. 5.14 illustrates the consistency of the two schemes for a sample of traﬃc

from Samsung smart cam during a week period with 380 instances of attack traﬃc

– each instance is worth a minute of traﬃc.

In Fig. 5.14a we plot the real-time

consistency score for Samsung smart cam during attack periods – attack instances

are marked by red ‘

×

’. It can be seen that the model correctly detects attack traﬃc

instances by giving them negative outputs, causing a drop in the consistency score.

We note that sometimes the consistency score keeps falling down even when attack

ﬁnishes. This is because the impact of some attacks persist in attributes of a few

following instances (up to 8 minutes). We can see that during intense attack periods

(Jun 2 and Jun 3), the consistency score of the one-class model of Samsung camera

drops to its lowest level, well highlighting a signiﬁcant change of behavior in device

traﬃc.

On the other hand, it is seen in Fig. 5.14b that the multi-class model is insensitive

to attacks. The consistency score of the model remains high during the whole week,

and does not noticeably get aﬀected by attack instances, as shown in Fig. 5.14b.

Even though the Random Forest model gives negative output to some attack in-

stances (34.6%), but each negative output is immediately followed by a sequence

of positive outputs keeping the real-time consistency score at a very high value –

incorrectly suggesting that the device behaves normally.

142

Jun 02Jun 03Jun04Jun05Jun06Jun07Jun08Time0.00.20.40.60.81.0Consistency scoreScoreAttackJun 02Jun 03Jun04Jun05Jun06Jun07Jun08Time0.00.20.40.60.81.0Consistency scoreScoreAttackChapter 5. Behavioral Change Detection using Clustering Algorithm

We observe that the one-class clustering model has, by far, more ability to high-

light (detect) anomalies in device behavior compared to the multi-class decision-tree

model – detection rate of 92.6% compared to 34.6%. We also note that multi-class

model correctly classiﬁes 98.0% of benign instances (TP) while this metric is slightly

lower (94.7%) for one-class model.

Although the performance of one-class classiﬁer models is compared with a multi-

class classiﬁer for a limited number of devices, we have found that one-class clas-

siﬁcation is more scalable compared to multi-class classiﬁcation. Note that adding

more devices would exponentially increase the complexity of the multi-class model.

For one-class models, instead, our conﬂict resolution method ensures each classiﬁer

works independently even for a large number of devices. We note that the cost of

one-class classiﬁcation is expected to increase linearly with the number of devices.

Note that these two approaches are fundamentally diﬀerent in their way of mod-

eling: one-class models are generative (learn distribution of each class) while multi-

class models are discriminative (learn decision boundary between various classes).

As a result, one-class models become sensitive to changes in any attribute while

multi-class models become sensitive to changes in only discriminative attributes.

5.5 Conclusion

Real-time traﬃc monitoring is of paramount importance for network operators who

manage a diverse set of IoT devices. In this chapter, we developed a modular clas-

siﬁer to identify IoT devices from their network behavior using a set of clustering

models. We further ﬁne-tuned our classiﬁcation models to not only identify IoT

devices but also detect the cyber-attacks from network traﬃc. We augmented our

machine learning-based system of classiﬁers with a conﬂict resolver and a model

consistency mechanism to track the behavioral changes of devices. Finally, we eval-

uated the eﬃcacy of our system by applying it to traﬃc traces from 12 IoT devices,

143

Chapter 5. Behavioral Change Detection using Clustering Algorithm

and demonstrate its ability to detect behavioral changes and cyber-attacks with an

overall accuracy of more than 94%. There are many interesting aspects of this work

that warrant further study. We outline some directions in the next chapter.

144

Chapter 6

Conclusions and Future Work

Contents

6.1 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145

6.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147

6.1 Conclusions

The Internet of Things has become a natural extension to the physical world and its

inﬂuence is found in every aspect of our lives. Although they have endless potential

to oﬀer immense experiences and beneﬁts to the users, the rapid innovation and pro-

liferation make them vulnerable to security and privacy breaches. The recent attacks

on IoT networks clearly indicate the possible catastrophic consequences if we fail to

give adequate attention to IoT security. On the other hand, traditional IT security

mechanisms fail to protect the IoT networks due to the large-scale deployments and

heterogeneous behavior of the devices.

Smart environment operators still fail to recognize the IoT assets and detect

cyber-attacks or compromised behaviors in real-time due to the lack of tools to

enable visibility into the IoT network. This thesis is an attempt to develop novel

145

Chapter 6. Conclusions and Future Work

IoT behavioral monitoring mechanisms using network analytics to automate the IoT

device identiﬁcation and classiﬁcation. It also deduces their operating context and

detects anomalous behavior indicative of IoT cyber-attacks.

This thesis opened by highlighting the ecosystem of IoT, especially in the per-

spective of security and privacy, by considering the market segments, security risks,

challenges in protection, role of the stakeholders and vulnerability assessment meth-

ods. We have also carried out a comprehensive survey on existing IoT security

solutions and behavioral monitoring methods. Following this, three key contribu-

tions to the ﬁeld of IoT security have been presented by providing better visibility

into IoT network:

• We captured and synthesized the IoT network traﬃc traces from a testbed

which is equipped with 28 consumer IoT devices. These traces were used

to proﬁle the behavioral characteristics of IoT devices based on the activity

and signalling patterns. We developed a machine learning based classiﬁcation

framework using the attributes obtained from the traﬃc characteristics to

classify the IoT devices. The proposed classiﬁcation architecture was trained

and tested with six months worth of data collected from an IoT testbed that

showed more than 99% accuracy in classifying the IoT device types.

• We reduced the cost of attribute extraction by proposing ﬂow level programmable

telemetry at multiple timescales. We proposed a real-time IoT behavioral mon-

itoring solution that can recognize the IoT devices and their states of opera-

tions. Furthermore, we showed how an operator can further optimize the cost

of telemetry and how our inference engine can be used to identify behavioral

changes including cyber-attacks.

• We developed a modularized inference engine using an unsupervised machine

learning algorithm, which allows changes to be accommodated in the device

behaviors without system wide retraining. Additionally, we proved that unsu-

146

Chapter 6. Conclusions and Future Work

pervised machine learning models are very sensitive with regards to detecting

the behavioral changes including low-rate cyber-attacks which showed 94%

detection rate.

6.2 Future Work

This work makes signiﬁcant contribution towards IoT behavioral monitoring with

better visibility into the IoT network using network analytics. The novel mechanisms

for IoT behavioral monitoring outlined in this thesis can be improved and reﬁned

based on real-world deployment scenarios. Some of the key reﬁnements are outlined

below:

• The inference engine developed in this thesis has the ability to detect behav-

ioral changes by monitoring for anomalous network traﬃc patterns. Techniques

to identify the exact traﬃc ﬂow that is causing the anomalous activity should

be further researched and developed in the future. Identifying and quaran-

tining the anomalous ﬂows during the attack, and performing forensics on

compromised devices will be a sensible future requirement and can be a step

forward for this work [92].

• We proposed a classiﬁcation framework to deduce the operating context by

identifying states of the devices. However, we have not investigated the proba-

bility of each state and transitional probabilities among them as a part of this

work. It will also be useful to detect the attacks that exploit the legitimate

network activities of devices, which will be a challenge to the current inference

engine. For example, if a device is conﬁgured to work dependent on a web ser-

vice, an attacker may compromise the web service to exploit the device using

its dependency [178]. During this attack the traﬃc between the web service

and the device may follow a legitimate pattern and then it is diﬃcult to trace

147

Chapter 6. Conclusions and Future Work

any anomalies. Monitoring the state transitioning pattern and frequency may

be a way forward to detect the attacks.

• The ﬂow level telemetry is purely used in this thesis to monitor the IoT traﬃc

activities. Although it is diﬃcult to spoof the ﬂow patterns of a device, there

are possibilities an attacker could carefully mimic the ﬂow patterns from a com-

promised device. This issue can be mitigated by including some packet level

attributes from sampled traﬃc which ﬁngerprint the payloads (e.g., entropy

of packets) in addition to ﬂow level telemetry.

We hope other researchers will explore the future directions identiﬁed above.

148

References

[1]

I. Spectrum, Popular Internet of Things forecast of 50 billion devices by
2020 Is outdated. [Online]. Available: http://bit.ly/2MkwA06 (visited on
12/31/2017).

[2] F. Loi, A. Sivanathan, H. Habibi Gharakheili, A. Radford, and V. Sivaraman,
“Systematically Evaluating Security and Privacy for Consumer IoT Devices”,
in Proc. IoT S&P Workshop on Internet of Things Security and Privacy,
Dallas, Texas, USA, 2017.

[3] A. Sivanathan, F. Loi, H. Habibi Gharakheili, and V. Sivaraman, “Exper-
imental Evaluation of Cybersecurity Threats to the Smart-home”, in IEEE
ANTS, Bhubaneswar, India, Dec. 2017.

[4] A. Sivanathan, H. Habibi Gharakheili, and V. Sivaraman, “Can We Classify
an IoT Device Using TCP Port Scan?”, in ICIAfS 2018, Colombo, Sri Lanka,
Dec. 2018.

[5] A. Sivanathan, D. Sherratt, H. Habibi Gharakheili, et al., “Characterizing and
Classifying IoT Traﬃc in Smart Cities and Campuses”, in IEEE INFOCOM
WKSHPS, Atlanta, USA, May 2017.

[6] A. Sivanathan, H. Habibi Gharakheili, F. Loi, et al., “Classifying IoT De-
vices in Smart Environments Using Network Traﬃc Characteristics”, IEEE
Transactions on Mobile Computing, vol. 18, no. 8, pp. 1745–1759, Aug. 2019.

[7] A. Sivanathan, D. Sherratt, H. Habibi Gharakheili, V. Sivaraman, and A.
Vishwanath, “Low-cost Flow-based Security Solutions for Smart-home IoT
Devices”, in IEEE ANTS, Bangalore, India, Nov. 2016.

[8] A. Sivanathan, H. Habibi Gharakheili, and V. Sivaraman, “Managing IoT
Cyber-Security using Programmable Telemetry and Machine Learning”,
IEEE Transactions on Network and Service Management(Under review),

[9] ——, “Inferring IoT Device Types from Network Behavior Using Unsuper-
vised Clustering”, in IEEE LCN 2019, Osnabrück, Germany, Oct. 2019.

[10] ——, “Detecting IoT Behavioral Changes Using Clustering-Based Network

Traﬃc Modeling”, IEEE Internet of Things Journal(Under review),

149

References

[11] K. Ashton, “That ’ Internet of Things ’ Thing”, RFiD Journal, vol. 22, pp. 97–
114, 2009. [Online]. Available: https://www.rfidjournal.com/articles/
view?4986.

[12] J. Gubbi et al., “Internet of Things (IoT): A vision, architectural elements,
and future directions”, Future Generation Computer Systems, vol. 29, no. 7,
pp. 1645–1660, 2013.

[13] Gartner Says 8.4 Billion Connected "Things" Will Be in Use in 2017, Up 31
Percent From 2016. [Online]. Available: https://gtnr.it/2ksLwNB (visited
on 02/07/2017).

[14] Telstra unveils its Smart Home hub. [Online]. Available: https : / / www .
zdnet.com/article/telstra- unveils- its- smart- home- hub/ (visited
on 06/22/2016).

[15]

IDC Forecasts Worldwide Technology Spending on the Internet of Things to
Reach $1.2 Trillion in 2022. [Online]. Available: https://www.idc.com/
getdoc.jsp?containerId=prUS43994118 (visited on 06/18/2018).

[16] Yonomi, Consumer, Enterprise and Industrial IoT Platforms: What’s the Dif-
ference? [Online]. Available: https : / / www . yonomi . co / blog / 2018 / 5 /
10/consumer- enterprise- and- indtrial- iot- platforms- whats- the-
difference (visited on 07/22/2019).

[17] Kelltontech, IoT and IIoT - A Comparative Analysis. [Online]. Available:
https : / / www . kelltontech . com / kellton - tech - blog / how - iot -
evolving-industrial-sphere (visited on 07/22/2019).

[18] Cisco, Cisco 2017 Midyear Cybersecurity Report, 2017. [Online]. Available:

https://bit.ly/2zXh0Pg (visited on 08/30/2018).

[19] P. P. Ray, “A survey of IoT cloud platforms”, Future Computing and Infor-

matics Journal, vol. 1, no. 1-2, pp. 35–46, 2017, issn: 23147288.

[20] S. Notra et al., “An Experimental Study of Security and Privacy Risks with
Emerging Household Appliances”, in Proc. First International Workshop on
Security and Privacy in Machine-to-Machine Communications (M2MSec),
San Francisco, CA, USA, Oct. 2014.

[21] What’s the Diﬀerence Between Consumer and Industrial IoT? [Online]. Avail-

able: https://bit.ly/2MJ2inc (visited on 09/21/2016).

[22] Shodan.

[Online]. Available: https : / / www . shodan . io/ (visited on

06/01/2019).

[23]

Insecam.
06/01/2019).

[Online]. Available: http : / / www . insecam . org/ (visited on

[24] DPE - The Default Password Enumeration Project. [Online]. Available: http:

//www.toolswatch.org/dpe/ (visited on 08/06/2019).

150

References

[25] J. Gamblin. (2016). Mirai Source Code, [Online]. Available: https://github.

com/jgamblin/Mirai-Source-Code/tree/master/mirai/bot.

[26] Understanding the Mirai Botnet, Vancouver, BC, Aug. 2017, pp. 1093–1110.

[27] Digicert Inc, “State of IoT Security Survey 2018”, no. DigiCert Inc, 2018.
[Online]. Available: https://www.digicert.com/wp- content/uploads/
2018/11/StateOfIoTSecurity_Report_11_02_18_F_am.pdf.

[28] T. Greene, Study: most enterprise IoT transactions are unencrypted - CIO.
[Online]. Available: https : / / www . cio . com . au / article / 662021 /
study - most - enterprise - iot - transactions - unencrypted/ (visited on
07/22/2019).

[29] D. Wood et al., “Cleartext Data Transmissions in Consumer IoT Medical

Devices”, in Proc. IoT S&P ’17, New York, New York, USA, 2017.

[30] H. Suo et al., “Security in the internet of things: A review”, in Proc. ICCSEE,

Hangzhou, China, Mar. 2012.

[31] HP News - HP Study Reveals 70 Percent of Internet of Things Devices Vulner-
able to Attack, 2014. [Online]. Available: https://bit.ly/2CLInhz (visited
on 07/20/2019).

[32] Z. Zhang et al., “Emerging Security Threats and Countermeasures in IoT”,

in ASIA CCS ’15, 2015, pp. 1–6.

[33] Hackers Remotely Kill a Jeep on the Highway-With Me in It. [Online]. Avail-

able: https://bit.ly/1ZcoZgH (visited on 07/21/2015).

[34] B. Schneier, The Internet of Things Is Wildly Insecure - And Often Unpatch-

able, https://bit.ly/2eGBrFv.

[35] How a ﬁsh tank helped hack a casino. [Online]. Available: https://wapo.st/

2uhIr54 (visited on 07/21/2017).

[36] Q. Jing et al., “Security of the Internet of Things: perspectives and chal-

lenges”, Wireless Networks, vol. 20, no. 8, pp. 2481–2501, 2014.

[37] T. Yu et al., “Handling a trillion (unﬁxable) ﬂaws on a billion devices”, in
Proc. ACM HotNets, ser. HotNets-XIV, Philadelphia, PA, USA: ACM, Nov.
2015, pp. 1–7.

[38] M. Yamauchi et al., “Anomaly Detection for Smart Home IoT Based on Users’

Behavior”, Icce 2018, pp. 1–6, 2019.

[39] S. Gibbs. (2015). Hackers can hijack Wi-Fi Hellow Barbie to spy on your

children, [Online]. Available: https://goo.gl/p2Pfk9.

[40] E. Ronen et al., “IoT Goes Nuclear: Creating a ZigBee Chain Reaction”, Tech.
Rep., 2016. [Online]. Available: https://eprint.iacr.org/2016/1047.

151

References

[41] C. Cimpanu. (2017). A Hacker just Pwned Over 150,000 Printers Left Ex-

posed Online, [Online]. Available: https://goo.gl/1SZSUw.

[42] Data breach digest IoT calamity: the Panda Monium. [Online]. Available:

https://bit.ly/2YGaiMj (visited on 02/12/2017).

[43] N. Woolf. (2016). DDoS attack that disrupted internet was largest of its kind
in history, experts say, [Online]. Available: https://www.theguardian.com/
technology/2016/oct/26/ddos-attack-dyn-mirai-botnet.

[44] M. Lyu, D. Sherratt, A. Sivanathan, et al., “Quantifying the Reﬂective DDoS
Attack Capability of Household IoT Devices”, in Proc. ACM WiSec, Boston,
Massachusetts, Jul. 2017.

[45] V. Sivaraman et al., “Inside job - Security and privacy threats for smart-
home IoT devices ”, Sydney: Australian Communications Consumer Action
Network, 2017.

[46] Positive Technologies, Cybersecurity Threatscape, 2017. [Online]. Available:

https://bit.ly/2GRvTag.

[47] Stilgherrian, No stars for Internet of Things security | ZDNet. [Online]. Avail-
able: https://www.zdnet.com/article/no- stars- for- internet- of-
things-security/ (visited on 07/22/2019).

[48] C. Fernandes and V. Sivaraman, “It’s only the beginning: Metadata Retention
laws and the Internet of Things”, Journal of Telecommunications and the
Digital Economy, vol. 3, no. 3, pp. 47–57, 2015.

[49] M. Richardson et al., “Towards responsive regulation of the Internet of
Things: Australian perspectives”, Internet Policy Review, vol. 6, no. 1, pp. 1–
14, 2017.

[50]

InformationAge, US Congress is introducing a new bill for IoT security, Ac-
cessed: 2019-06-07, Apr. 2019. [Online]. Available: https://bit.ly/2CYE4zL.

[51] M. James, Plans announced to introduce new laws for internet connected
devices, 2019. [Online]. Available: https://www.gov.uk/government/news/
plans-announced-to-introduce-new-laws-for-internet-connected-
devices (visited on 07/20/2019).

[52]

IEEE-Spectrum, Japan to Probe IoT Devices and Then Prod Users to
Smarten Up, https://bit.ly/2I3RrBL, Accessed: 2019-06-07, Feb. 2019.

[53] P. V. Eecke and E. Dekyvere, EU: Keeping your connected devices secure:
Europe introduces IoT Cybersecurity standard | Privacy Matters. [Online].
Available: https : / / blogs . dlapiper . com / privacymatters / europe -
keeping- your- connected- devices- secure- europe- introduces- iot-
cybersecurity-standard/ (visited on 07/20/2019).

152

References

[54]

I. Mikolic-Torreira et al., Exploring Cyber Security Policy Options in Aus-
tralia. RAND Corporation, 2017. [Online]. Available: https://www.rand.
org/pubs/research%7B%5C_%7Dreports/RR2008.html.

[55] E. Chapman, Obstacles for the cyber kangaroo, 2017. [Online]. Available:
https : / / www . aspistrategist . org . au / obstacles - for - the - cyber -
kangaroo/ (visited on 07/22/2019).

[56] Analytical Research Cognizance, Cyber Insurance Market Size 2019 Global
Opportunity Analysis, Trend, Overview, Application, Key Players, Insurance
Types, Protection Technology & Industry Forecasts 2024. [Online]. Available:
https://reut.rs/2Cmwgas (visited on 02/25/2019).

[57] N. Dhanjani, Abusing the Internet of Things: Blackouts, Freakouts, and Stake-

outs. O’Reilly Media, 2015.

[58] E. Fernandes et al., “Security Analysis of Emerging Smart Home Applica-
tions”, in Proc. IEEE Symposium on Security and Privacy (SP), San Jose,
CA, USA, May 2016.

[59] P. Morgner, “Security and Privacy in the Internet of Things : Technical
and Economic Perspectives”, PhD thesis, Friedrich-Alexander-University of
Erlangen-Nürnberg, 2019.

[60] S. Notra et al., “An Experimental Study of Security and Privacy Risks with
Emerging Household Appliances”, in Proc. IEEE Conference on Communi-
cations and Network Security, San Francisco, USA, Oct. 2014.

[61] Herzberg et al. (2016). Breaking Down Mirai: An IoT DDoS Botnet Anal-
ysis, [Online]. Available: https : / / www . incapsula . com / blog / malware -
analysis-mirai-ddos-botnet.html.

[62] F. J. Ryba et al., “Ampliﬁcation and DRDoS Attack Defense - A Survey
and New Perspectives”, ArXiv:1505.07892, 2015. [Online]. Available: http:
//arxiv.org/abs/1505.07892.

[63] M. Kuhrer et al., “Hell of a Handshake : Abusing TCP for Reﬂective Ampli-

ﬁcation DDoS Attacks”, USENIX WOOT, pp. 1–6, 2014.

[64] C. Rossow, “Ampliﬁcation Hell: Revisiting Network Protocols for DDoS
Abuse”, in Proc. Network and Distributed System Security Symposium, San
Diego, California, 2014.

[65] M. Kührer et al., “Exit from Hell ? Reducing the Impact of Ampliﬁca-
tion DDoS Attacks”, 23rd USENIX Security Symposium, USENIX Sec 2014,
pp. 111–125, 2014.

[66] D. Hall. (2015). Building a LIFX packet, [Online]. Available: https : / /

community.lifx.com/t/building-a-lifx-packet/59.

[67] Ms. Smith. (2017). Hacker stackoverﬂowin pwning printers, [Online]. Avail-

able: https://goo.gl/4E4e71.

153

References

[68] U. A. Sandhu et al., “A Survey of Intrusion Detection & Prevention Tech-

niques”, in ICICM, Singapore, 2011.

[69] C. Ko et al., “Execution Monitoring of Security-Critical Programs in Dis-
tributed Systems: a Speciﬁcation-based Approach”, in Proc. IEEE Sympo-
sium on Security and Privacy, Oakland, CA, USA, May 1997.

[70] S. Mukkamala et al., “Intrusion Detection using Neural Networks and Support

Vector Machines”, in Proc. IJCNN, Honolulu, HI, USA, May 2002.

[71] P. Mell, “Understanding Intrusion Detection Systems”, in IS Management

Handbook, Auerbach Publications, 2003, pp. 409–418.

[72] V. Sivaraman et al., “Network-level Security and Privacy Control for Smart-

Home IoT Devices”, in Proc. IEEE WiMob, Abu Dhabi, UAE, Oct. 2015.

[73] L. H. Yeo et al., “Understanding Modern Intrusion Detection Systems: A
Survey”, ArXiv, Aug. 2017. [Online]. Available: http://arxiv.org/abs/
1708.07174.

[74] V. Paxson, “Bro: A System for Detecting Network Intruders in Real-time”,

Computer Network, vol. 31, no. 23-24, pp. 2435–2463, Dec. 1999.

[75] M. Roesch, “Snort - Lightweight Intrusion Detection for Networks”, in Proc.

USENIX CSA, Seattle, Washington, Nov. 1999.

[76] P. Uppuluri and R. Sekar, “Experiences with Speciﬁcation-Based Intrusion
Detection”, in Recent Advances in Intrusion Detection, Berlin, Heidelberg,
2001.

[77] R. Berthier and W. H. Sanders, “Speciﬁcation-Based Intrusion Detection for
Advanced Metering Infrastructures”, in IEEE Paciﬁc Rim International Sym-
posium on Dependable Computing, Pasadena, CA, USA, Dec. 2011.

[78] M. Surendar and A. Umamakeswari, “InDReS: An Intrusion Detection and
Response System for Internet of Things with 6LoWPAN”, in IEEE WiSP-
NET, Chennai, India, Mar. 2016.

[79] A. Le et al., “A Speciﬁcation-Based IDS for Detecting Attacks on RPL-Based

Network Topology”, Information, vol. 7, p. 25, May 2016.

[80] H. Bostani and M. Sheikhan, “Hybrid of Anomaly-based and Speciﬁcation-
based IDS for Internet of Things using Unsupervised OPF based on MapRe-
duce Approach”, Computer Communications, vol. 98, pp. 52–71, Jan. 2017.

[81] J. P. Amaral et al., “Policy and Network-based Intrusion Detection System
for IPv6-enabled Wireless Sensor Networks”, in Proc. IEEE ICC, Sydney,
NSW, Australia, Jun. 2014.

[82] D. T. Nguyen et al., “IotSan”, in Proc. Emerging Networking Experiments
and Technologies - CoNEXT, New York, New York, USA: ACM Press, Aug.
2018, pp. 191–203.

154

References

[83] E. Lear et al., “Manufacturer Usage Description Speciﬁcation”, IETF Secre-

tariat, Tech. Rep., Mar. 2019.

[84] A. Hamza et al., “Combining MUD Policies with SDN for IoT Intrusion De-
tection”, in Proc. ACM Sigcomm workshop on IoT S&P, Budapest, Hungary,
Aug. 2018.

[85] A. Hamza et al., “Detecting Volumetric Attacks on IoT Devices via SDN-
Based Monitoring of MUD Activity”, in Proc. ACM SOSR, San Jose, Cali-
fornia, USA, Apr. 2019.

[86] D. P. Vinchurkar and A. Reshamwala, “A Review of Intrusion Detection Sys-
tem Using Neural Network and Machine Learning Technique”, International
Journal of Engineering Science and Innovative Technology, vol. 1, pp. 54–63,
Nov. 2012.

[87] R. Patel et al., “A Survey and Comparative Analysis of Data Mining Tech-
niques for Network Intrusion Detection Systems”, International Journal of
Soft Computing and Engineering (IJSCE), vol. 2, pp. 265–260, Jan. 2012.

[88] L. Portnoy et al., “Intrusion Detection with Unlabeled Data Using Cluster-

ing”, in Proc. of ACM CSS Workshop on DMSA, 2001, pp. 5–8.

[89] H. H. Hosmer, “Security is Fuzzy!: Applying the Fuzzy Logic Paradigm to

the Multipolicy Paradigm”, in NSPW, 1993.

[90] P. Dokas et al., “Data mining for Network Intrusion Detection”, Proceeding

of NGDM, Jan. 2002.

[91] P. Garcia-Teodoro et al., “Anomaly-based network intrusion detection: Tech-
niques, systems and challenges”, Computers & security, vol. 28, no. 1-2,
pp. 18–28, Feb. 2009.

[92] R. Sommer and V. Paxson, “Outside the Closed World: On Using Machine
Learning for Network Intrusion Detection”, in IEEE Symposium on Security
and Privacy, Washington, DC, USA, May 2010.

[93] S. Nomm and H. Bahsi, “Unsupervised Anomaly Based Botnet Detection
in IoT Networks”, IEEE International Conference on Machine Learning and
Applications, pp. 1048–1053, Dec. 2018.

[94] Y. Amar et al., “An Analysis of Home IoT Network Traﬃc and Behaviour”,
Mar. 2018. [Online]. Available: http://arxiv.org/abs/1803.05368.

[95] D. Barrera et al., “IDIoT: Securing the Internet of Things like it’s 1994”,
Tech. Rep., 2017. [Online]. Available: http://arxiv.org/abs/1712.03623.

[96] N. Aluthge, “IoT device ﬁngerprinting with sequence-based features”, PhD

thesis, 2017.

155

References

[97] A. Moore and D. Zuev, “Internet Traﬃc Classiﬁcation Using Bayesian Analy-
sis Techniques”, SIGMETRICS Perform. Eval. Rev., vol. 33, no. 1, pp. 50–60,
Jun. 2005, issn: 0163-5999.

[98] M. Iliofotou et al., “Exploiting Dynamicity in Graph-based Traﬃc Analysis:
Techniques and Applications”, in Proc. ACM CoNEXT, Rome, Italy, Dec.
2009.

[99] D. Bonﬁglio et al., “Revealing Skype Traﬃc: When Randomness Plays with
You”, SIGCOMM Comput. Commun. Rev., vol. 37, no. 4, pp. 37–48, Aug.
2007, issn: 0146-4833.

[100] R. Ferdous et al., “On the Use of SVMs to Detect Anomalies in a Stream
of SIP Messages”, in Proc. IEEE ICMLA, Boca Raton, Florida, USA, Dec.
2012.

[101] M. Z. Shaﬁq et al., “A First Look at Cellular Machine-to-Machine Traﬃc:
Large Scale Measurement and Characterization”, in Proc. ACM Sigmetrics,
England, Jun. 2012.

[102] N. Nikaein et al., “Simple Traﬃc Modeling Framework for Machine Type

Communication”, in Proc. ISWCS, Germany, Aug. 2013.

[103] M. Jadoul, The IoT: The Network Can Make It or Break It. [Online]. Avail-
able: https : / / insight . nokia . com / iot - network - can - make - it - or -
break-it.

[104] M. Simon and Alcatel-Lucent, Architecting Networks: Supporting IoT. [On-

line]. Available: http://bit.ly/2kySsIM.

[105] M. Laner et al., “Traﬃc Models for Machine Type Communications”, in Proc.

ISWCS, Germany, Aug. 2013.

[106] L. Markus et al., Traﬃc models for machine-to-machine (M2M) communica-
tions: types and applications. Dec. 2015, pp. 133–154, isbn: 978-1-78242-102-
3.

[107] A. Orrevad, “M2M Traﬃc Characteristics: When Machines Participate in
Communication”, PhD thesis, KUNGLIGA TEKNISKA HÖGSKOLAN,
2009.

[108] M. Roughan et al.,

“Class-of-service Mapping for QoS: A Statistical
Signature-based Approach to IP Traﬃc Classiﬁcation”, in Proc. SIGCOMM
Conference on Internet Measurement, ser. IMC ’04, vol. 25, New York, NY,
USA: ACM, 2004, pp. 135–148.

[109] R. Lippmann et al., “Passive Operating System Identiﬁcation from TCP/IP
Packet Headers”, in Workshop on Data Mining for Computer Security, Cite-
seer, vol. 40, 2003.

156

References

[110] J. Zhang et al., “Network Traﬃc Classiﬁcation Using Correlation Informa-
tion”, IEEE Transactions on Parallel and Distributed Systems, vol. 24, no. 1,
pp. 104–117, Jan. 2013.

[111] S. C. Madanapalli et al., “Inferring Netﬂix User Experience from Broadband
Network Measurement”, in 2019 Network Traﬃc Measurement and Analysis
Conference (TMA), Paris, France, Jun. 2019.

[112] M. Lyu et al., “Mapping an Enterprise Network by Analyzing DNS Traﬃc”, in
Passive and Active Measurement, D. Choﬀnes and M. Barcellos, Eds., Cham:
Springer International Publishing, 2019, pp. 129–144.

[113] D. Tegeler et al., “BotFinder: Finding Bots in Network Traﬃc Without Deep
Packet Inspection”, in Proc. ACM CoNEXT, Nice, France, Dec. 2012.

[114] D. McGrew and B. Anderson, “Enhanced Telemetry for Encrypted Threat

Analytics”, in Proc. IEEE ICNP, Singapore, Nov. 2016.

[115] B. Anderson and D. McGrew, “Identifying Encrypted Malware Traﬃc with

Contextual Flow Data”, in Proc. ACM AISec, Vienna, Austria, Oct. 2016.

[116] M. Lopez-Martin et al., “Network Traﬃc Classiﬁer with Convolutional and
Recurrent Neural Networks for Internet of Things”, IEEE Access, vol. 5, 2017.

[117] D. Levi et al., “Simple Network Management Protocol (SNMP) Applications”,

United States, Tech. Rep., 2002.

[118] V. Srinivasan et al., “Protecting your daily in-home activity information from
a wireless snooping attack”, in Proc. UbiComp, Seoul, Korea, 2008.

[119] A. Acar et al., “Peek-a-Boo: I see your smart home activities, even en-
crypted!”, Tech. Rep., 2018. [Online]. Available: http://arxiv.org/abs/
1808.02741.

[120] B. Copos et al., “Is Anybody Home? Inferring Activity from Smart Home
Network Traﬃc”, in Proc. IEEE SPW, IEEE, May 2016, pp. 245–251.

[121]

sFlow. [Online]. Available: https://sflow.org/about/index.php (visited
on 06/07/2019).

[122] B. Claise, “Cisco Systems NetFlow Services Export Version 9”, RFC 3954,
Oct. 2004, pp. 1–33. [Online]. Available: https://www.rfc- editor.org/
info/rfc3954.

[123] Y. Li et al., “FlowRadar: A Better NetFlow for Data Centers”, in Proc.

USENIX NSDI, Santa Clara, CA, Mar. 2016.

[124] S. Siby, R. R. Maiti, and N. O. Tippenhauer, “IoTScanner: Detecting privacy

threats in IoT neighborhoods”, 2017, pp. 23–30.

157

References

[125] Y. Zhu et al., “Packet-Level Telemetry in Large Datacenter Networks”, in
Proc. SIGCOMM, New York, New York, USA: ACM Press, 2015, pp. 479–
491.

[126] J. Rasley et al., “Planck: Millisecond-scale Monitoring and Control for Com-
modity Networks”, SIGCOMM Comput. Commun. Rev., vol. 44, no. 4,
pp. 407–418, Aug. 2014.

[127] N. Mckeown et al., “OpenFlow: Enabling Innovation in Campus Networks”,

SIGCOMM Comput. Commun. Rev., vol. 38, no. 2, pp. 69–74, Mar. 2008.

[128] R. Hofstede et al., “Measurement Artifacts in Netﬂow Data”, in Proc. PAM,

Hong Kong, China: Springer-Verlag, 2013, pp. 1–10.

[129] Y. Meidan et al., “ProﬁlIoT: A Machine Learning Approach for IoT Device
Identiﬁcation Based on Network Traﬃc Analysis”, in Proc. ACM SAC, Mar-
rakech, Morocco, Apr. 2017.

[130] M. Miettinen et al., “IoT Sentinel Demo: Automated Device-Type Identiﬁca-
tion for Security Enforcement in IoT”, in Proc. International Conference on
Distributed Computing Systems, Atlanta, GA, USA, Jun. 2017.

[131] H. Guo and J. Heidemann, “IP-Based IoT Device Detection”, in Proc. ACM

IoT S&P, Budapest, Hungary, Aug. 2018.

[132] N. Apthorpe et al., “A Smart Home is No Castle: Privacy Vulnerabilities of

Encrypted IoT Traﬃc”, in Proc. DAT, New York,USA, Nov. 2017.

[133] R. Doshi et al., “Machine Learning DDoS Detection for Consumer Internet of
Things Devices”, in 2018 IEEE SPW, San Francisco, CA, May 2018, pp. 29–
35.

[134] B. Bezawada et al., “Behavioral Fingerprinting of IoT Devices”, in Proc.

ASHES, Toronto, Canada, 2018, pp. 41–50.

[135] Y. Meidan et al., “Detection of Unauthorized IoT Devices Using Machine
Learning Techniques”, ArXiv, 2017. [Online]. Available: http://arxiv.org/
abs/1709.04647.

[136] J. Ortiz et al., “DeviceMien: Network Device Behavior Modeling for Identify-
ing Unknown IoT Devices”, in Proc. ACM IoTDI, Montreal, Quebec, Canada,
2019.

[137] H. Jiang and C. Dovrolis, “Why is the Internet Traﬃc Bursty in Short Time
Scales?”, SIGMETRICS Perform. Eval. Rev., vol. 33, no. 1, pp. 241–252, Jun.
2005.

[138] A. L. Buczak and E. Guven, “A Survey of Data Mining and Machine Learn-
ing Methods for Cyber Security Intrusion Detection”, IEEE Communications
Surveys Tutorials, vol. 18, no. 2, pp. 1153–1176, Oct. 2016.

158

References

[139] R. K. Agrawal, “K-Nearest Neighbor for Uncertain Data”, International Jour-
nal of Computer Applications, vol. 105, no. 11, pp. 13–16, 2014, issn: 0975-
8887.

[140] S. Yue et al., “SVM classiﬁcation:Its contents and challenges”, Applied
Mathematics-A Journal of Chinese Universitiesz, vol. 18, no. 3, pp. 332–342,
Sep. 2003, issn: 1993-0445.

[141] A. Kumar et al., “EDIMA: Early Detection of IoT Malware Network Activity
Using Machine Learning Techniques”, ArXiv, 2019. [Online]. Available: http:
//arxiv.org/abs/1906.09715.

[142] K. Leung and C. Leckie, “Unsupervised Anomaly Detection in Network Intru-
sion Detection Using Clusters”, in Proc. ACSC, Newcastle, Australia, 2005.

[143] S. Zhao

et

al.,

“A Dimension Reduction Model

for Anomaly-Based Intrusion Detection in Internet of Things”,
DASC/PiCom/DataCom/CyberSciTech, Nov. 2017.

and Classiﬁer
in

[144] M. Pimentel et al., “A review of Novelty Detection”, Signal Processing, vol.

99, pp. 215–249, 2014.

[145] W. Chen et al., “Exploring a Service-based Normal Behaviour Proﬁling Sys-
tem for Botnet Detection”, in 2017 IFIP/IEEE Symposium on Integrated
Network and Service Management (IM), May 2017, pp. 947–952.

[146] G. Gu et al., “BotMiner: Clustering Analysis of Network Traﬃc for Protocol-
and Structure-independent Botnet Detection”, in Proc. of Security Sympo-
sium, San Jose, CA, Jul. 2008.

[147] Y. Meidan et al., “N-BaIoT Network-Based Detection of IoT Botnet Attacks
Using Deep Autoencoders”, IEEE Pervasive Computing, vol. 17, no. 3, pp. 12–
22, Jul. 2018.

[148] S. Aneja, N. Aneja, and M. S. Islam, “IoT Device ﬁngerprint using deep

learning”, Proc. IEEE IOTAIS 2018, pp. 174–179, 2019.

[149] S. Alexander and R. Droms, “DHCP Options and BOOTP Vendor Exten-
sions”, RFC Editor, RFC 2132, Mar. 1997, pp. 1–34. [Online]. Available:
https://tools.ietf.org/rfc/rfc2132.txt.

[150]

I. Andrea et al., “Internet of Things: Security vulnerabilities and challenges”,
in 2015 IEEE Symposium on Computers and Communication (ISCC), Jul.
2015.

[151] K. Moskvitch, “Securing IoT: In your Smart Home and your Connected En-

terprise”, Engineering Technology, vol. 12, Apr. 2017.

[152] The guardian, Why the Internet of Things is the New Magic Ingredient for
Cyber Criminals, 2016. [Online]. Available: http://bit.ly/2Z8vtT4.

[153] OpenWrt, 2016. [Online]. Available: https://openwrt.org/.

159

References

[154] M. Weiss et al., Time-Aware Applications, Computers, and Communication
Systems, 2015. [Online]. Available: http://dx.doi.org/10.6028/NIST.TN.
1867.

[155] Cisco Systems. (2017). joy Tool, [Online]. Available: https://github.com/

cisco/joy/.

[156] A. McCallum and K. Nigam, “A Comparison of Event Models for Naive
Bayes Text Classiﬁcation”, AAAI/ICML-98 Workshop on Learning for Text
Categorization, pp. 41–48, 1998.

[157] E. Frank et al., The WEKA Workbench. Online Appendix for "Data Mining:
Practical Machine Learning Tools and Techniques", Fourth Edition. Morgan
Kaufmann, 2016.

[158] E. Vyncke and C. Paggen, LAN Switch Security: What Hackers Know About

Your Switches. Cisco Press, 2008, p. 340.

[159] Zack Whittaker, Hackers are using botnets to take the hard work out of break-
ing into networks, Apr. 2018. [Online]. Available: https://zd.net/2H9SAER.

[160] SolarWinds, NetFlow Basics and Deployment Strategies, Aug. 2010. [Online].

Available: https://bit.ly/2K4EZ6Q.

[161]

eSpeak: Speech Synthesizer.
sourceforge.net/ (visited on 04/03/2019).

[Online]. Available: http : / / espeak .

[162] H. Habibi Gharakheili et al., “iTeleScope: Softwarized Network Middle-Box
for Real-Time Video Telemetry and Classiﬁcation”, IEEE Transactions on
Network and Service Management, 2019.

[163] A. Sivanathan, SDN Sim, 2018. [Online]. Available: https://github.com/

arunmir/sdn-sim.

[164] EY, Cybersecurity compromise diagnostic: Hunting for evidence of cyber at-
tackers, Apr. 2017. [Online]. Available: https://go.ey.com/2X7yTIS.

[165] R. Caruana and A. Niculescu-Mizil, “An Empirical Comparison of Supervised
Learning Algorithms”, in Proc. ICML, Pittsburgh, Pennsylvania, USA, Jun.
2006.

[166] L. Breiman, “Random Forests”, Machine Learning, vol. 45, no. 1, pp. 5–32,

Oct. 2001.

[167] J. Zhang et al., “Robust Network Traﬃc Classiﬁcation”, IEEE/ACM Trans-

actions on Networking, vol. 23, no. 4, pp. 1257–1270, Aug. 2015.

[168] M. A. Hall, “Correlation-based Feature Subset Selection for Machine Learn-
ing”, PhD thesis, University of Waikato, Hamilton, New Zealand, 1998.

[169] A. Vishnoi et al., “Eﬀective Switch Memory Management in OpenFlow Net-

works”, in Proc. ACM DEBS, Mumbai, India, May 2014.

160

References

[170] J. N. Miller, “Tutorial Review—Outliers in Experimental Data and Their

Treatment”, Analyst, vol. 118, no. 5, pp. 455–461, 1993.

[171] D. Ruan et al.,

Intelligent Data Mining: Techniques and Applications.

Springer Science & Business Media, 2005, vol. 5.

[172]

I. Bin Mohamad and D. Usman, “Standardization and Its Eﬀects on K-Means
Clustering Algorithm”, Research Journal of Applied Sciences, Engineering
and Technology, vol. 6, no. 17, pp. 3299–3303, Sep. 2010.

[173] C. Ding et al., “K-means Clustering via Principal Component Analysis”, in
Proc. Int. Conf. Machine Learning, Banﬀ, Alberta, Canada, Jul. 2004.

[174] C. H. Achen, “What Does ”Explained Variance” Explain?: Reply”, Political

Analysis, vol. 2, pp. 173–184, 1990.

[175] D. Ketchen et al., “The Application of Cluster Analysis in Strategic Manage-
ment Research: An Analysis and Critique”, Strategic Management Journal,
vol. 17, no. 6, pp. 441–458, 1996.

[176] M. K. Pakhira, “A Modiﬁed k-means Algorithm to Avoid Empty Clusters”,
International Journal of Recent Trends in Engineering, vol. 1, no. 1, pp. 220–
226, May 2009.

[177] M. Firdhous et al., “A Memoryless Trust Computing Mechanism for Cloud
Computing”, in Networked Digital Technologies, Berlin, Heidelberg, 2012,
pp. 174–185.

[178] M. Surbatovich et al., “Some recipes can do more than spoil your appetite:
Analyzing the security and privacy risks of IFTTT recipes”, in International
World Wide Web Conference, Perth, Australia, 2017.

161

