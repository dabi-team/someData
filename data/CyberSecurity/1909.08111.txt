Detecting Generalized Replay Attacks
via Time-Varying Dynamic Watermarking

Matthew Porter, Pedro Hespanhol, Anil Aswani, Matthew Johnson-Roberson, and Ram Vasudevan

1

9
1
0
2

p
e
S
7
1

]

C
O
.
h
t
a
m

[

1
v
1
1
1
8
0
.
9
0
9
1
:
v
i
X
r
a

Abstract—Cyber-physical systems (CPS) often rely on external
communication for supervisory control or sensing. Unfortunately,
these communications render the system vulnerable to cyber-
attacks. Attacks that alter messages, such as replay attacks that
record measurement signals and then play them back to the
system, can cause devastating effects. Dynamic Watermarking
methods, which inject a private excitation into control inputs to
secure resulting measurement signals, have begun addressing the
challenges of detecting these attacks, but have been restricted
to linear time invariant (LTI) systems. Though LTI models are
sufﬁcient for some applications, other CPS, such as autonomous
vehicles, require more complex models. This paper develops
a linear time-varying (LTV) extension to previous Dynamic
Watermarking methods by designing a matrix normalization
factor to accommodate the temporal changes in the system.
Implementable tests are provided with considerations for real-
world systems. The proposed method is then shown to be able to
detect generalized replay attacks both in theory and in simulation
using a LTV vehicle model.

I. INTRODUCTION

Cyber-physical systems (CPS) combine both networked
computing and sensing resources with physical control sys-
tems in an effort to increase efﬁciency, manage complexity,
or provide convenience. Whether it is industrial control ap-
plications or smart devices, CPS require secure networked
communications to operate safely and correctly. Malicious
attacks on such systems can cause devastating results [1]–
[4]. CPS are often protected by traditional cyber security
tools, but these methods are insufﬁcient due to the addition
of networked physical infrastructure. [5], [6]. A growing body
of work has started to address these challenges by developing
new detection algorithms, analyzing potentially stealthy attack
models, and ﬁnding ways of reducing the effect of attacks. One
particular detection method, Dynamic Watermarking, has been
shown to detect various attack models while making few as-
sumptions about system structure. Despite these developments,
detection algorithms, including Dynamic Watermarking, have
only focused on CPS that can be modeled as linear time
invariant (LTI) systems. While LTI models can be sufﬁcient for
steady state or slow moving applications, many emerging CPS

This work was supported by a grant from Ford Motor Company via the
Ford-UM Alliance under award N022977 and by UC Berkeley Center for
Long-Term Cybersecurity.

M. Porter and R. Vasudevan are with the Department of Mechanical
Engineering, University of Michigan, Ann Arbor, MI 48103 USA (e-
mail: matthepo@umich.edu; ramv@umich.edu).

A. Aswani and P. Hespanhol are with the Department of Industrial Engineer-
ing and Operations Research, University of California Berkeley, Berkeley, CA
94720 USA (e-mail: aaswani@berkeley.edu; pedrohespanhol@berkeley.edu).
M. Johnson-Roberson is with the Department of Naval Architecture, Uni-
versity of Michigan, Ann Arbor, MI 48103 USA (e-mail: mattjr@umich.edu).

such as autonomous vehicles require models that change over
time. This paper develops methods to accommodate such CPS
by extending Dynamic Watermarking to linear time-varying
(LTV) systems.

A. Attack Models

Attacks are divided into three categories: denial of service
(DOS) attacks, where the control or measurement signal is
stopped, direct attacks, where the plant, actuators or sensors
are physically attacked, and deception attacks, where the
control or measurement signal are altered. [7]. DOS attacks
if they stop
can be detrimental, but are trivial
all communication. Furthermore, when only a portion of
communication is stopped, their effects can be minimized
using graceful degradation [8]. The result of direct attacks
often causes anomalies in the measurement signal and can
therefore be detected by methods used to detect deception
attacks. Consequentially, this work focuses on the detection
of deception attacks.

to detect

A variety of deception attacks have been proposed. The
simplest deception attacks add noise using arbitrary or random
strategies [9]. On the other hand, bias injection attacks, the
attacker injects a constant bias into the system [10], while
routing attacks send measurement signals through a linear
transform [11]. Other deception attacks attempt to decouple the
system such that the measurements are unaltered while certain
states of the system are attacked [12]. For instance, zero-
dynamics attacks take advantage of un-observable states or
remove the effects of their attacks in the resulting measurement
signal [10], and replay attacks involve an attacker replaying
recorded measurements while possibly altering control as well
[10].

The amount of knowledge of the system dynamics and
detection scheme along with the capability of the attacker to
alter certain signals necessary to carry out these attacks varies
greatly. While random, bias injection, routing, and replay
attacks do not require any knowledge of the underlying system
dynamics, decoupling and zero-dynamics attack require almost
full knowledge. This knowledge can be difﬁcult to obtain
for non-insider attackers but it is not impossible [13], [14].
Nonetheless, this work focuses on a generalization of a replay
attack due to the simplicity of implementation and because it
has already been applied during real-world attacks [1]. Further-
more, we consider attacks that only alter measurement signals,
since many of the systems we care about use local controllers
while operating using externally received measurements.

 
 
 
 
 
 
2

B. Attack Detection Algorithms

The measurement residual, deﬁned as the difference be-
tween the measurement and the expected measurement, is used
by most detection schemes. For each detector, a metric based
on the measurement residual is generated. If at any time the
metric exceeds a user-deﬁned threshold, the detector raises
an alarm. Generally, these metrics can be separated into two
categories: those that only observe the system, called passive
methods, and those that alter the system while observing,
called active methods. While passive methods do not degrade
control performance, active methods accept a small amount of
performance degradation in exchange for the ability to detect
more complex attacks [15]–[17]. These categories can be
further subdivided into stateless metrics, which only consider
the current measurement residual, and stateful metrics, which
rely on previous measurement residuals as well.

1) Passive Methods: The χ2 detector’s metric is the inner
product of the normalized measurement residual, which fol-
lows a χ2 distribution. Due to its simplicity, the χ2 detector
has been studied in several works [18]–[21]. Though the χ2 is
widely used, it is a stateless detector. Two stateful alternatives
are the cumulative sum (CUSUM) detector and the multi-
variate exponentially weighted moving average (MEWMA)
detector. When comparing these stateful detectors to the χ2
detector, it has been shown that the stateful detectors can
often provide stronger guarantees on detection while the χ2
detector boasts both simpler implementation and generally
takes less time to detect attacks [22], [23]. While passive
detectors can detect random attacks, they are unable to detect
more sophisticated attacks such as replay attacks. In addition,
they have only been developed for LTI systems.

2) Active Methods: Most active methods fall into one of
two categories: moving target defense, which change system
parameters to keep attackers from obtaining the current con-
ﬁguration, and watermarking-based methods, which encrypt
measurement signals with a watermark that is added to the
control input.

The concept of moving target defenses is a topic of con-
tinued interest for the ﬁeld of cyber security and includes
randomizing the order of code execution and physical memory
storage locations [24]. In CPS, moving target defense can
take the form of switching between redundant measurements
[25]–[29], altering control strategy [25], [29], or by changing
plant dynamics [25]–[27], [30]–[33]. Switching measurement
signals works well when an attacker is only hacking a
few measurements, but otherwise performs similar to passive
methods. Altering the control strategy is arguably similar to
watermarking-based methods and can allow for detection of
most attack models except zero-dynamics attacks. While some
methods alter the physical plant dynamics directly [25]–[27],
others append the plant dynamics with an auxiliary system
with possibly more complex dynamics [30]–[33]. Despite the
consideration of more complex dynamics for the auxiliary
systems, moving target defense has only been applied to
systems that have LTI dynamics. Although complex dynamics
cause the behavior of the test metric to change in time,
methods for selecting a time-varying threshold involve hand

tuning. Altering plant dynamics can allow for detection of
all attack models, but the method makes certain assumptions
about the system. Note, for the auxiliary systems it is assumed
that the plant has secure knowledge of its own state, which
does not account for vulnerable networked sensors. Also, when
an auxiliary system is not used, it is assumed that the plant
dynamics are changeable.

The introduction of a watermark was ﬁrst proposed as a way
of making the χ2 detector robust to replay attacks [34] and
other more advanced attacks [35]. Here, the watermark takes
the form of independent identically distributed (IID) Gaussian
noise that is added to the control input. Robustness to replay
attacks is then achieved by properly selecting the watermark
covariance, while the χ2 detector itself remains unchanged.
Dynamic Watermarking uses a metric that relies on both the
covariance of the residuals and the correlation between the
residuals and the watermark. The covariance of the watermark
is allowed to be an arbitrary symmetric full rank matrix [36]–
[40]. In these works, the metric uses the measurement residuals
contained in a temporally sliding window. Guarantees of
detection are then made as the window size tends to inﬁnity.
Extensions to a limited subset of nonlinear systems have been
implemented [38], [41], but otherwise Dynamic Watermarking
has been limited to LTI systems. Though the addition of the
watermark causes a degradation in system performance, the
degradation can be minimized [42], [43]. Other work has
considered allowing the watermark signal to be auto-correlated
[44] or to have distributions that are not Gaussian [45], [46].
Furthermore, other forms of watermarks include intentional
package drops [47], [48], using parameterized transforms
on measurements [11], [49], [50], and B-splines added to
feed forward inputs [51]. Though Dynamic Watermarking is
unable to detect zero-dynamics attacks, it does not require the
assumption of changeable plant dynamics or locally secure
knowledge of plant state. This paper focuses on Dynamic
Watermarking as described in Hespanhol et al. [37] due to its
ability to be applied to a wide range of LTI systems including
both fully and partially observable systems.

C. Contributions

The contributions of this paper are threefold. First, the tests
used in Hespanhol et al. [37] are extended to LTV systems.
This is done using a carefully designed matrix normalization
factor to accommodate the temporal changes in the system.
These tests are then proven to detect generalized replay
attacks. Second, a model is developed for time-varying gen-
eralized replay attacks. Third, LTV Dynamic Watermarking is
applied to a simulated system to provide proof of concept.

The remainder of this paper is organized as follows. Section
I-D introduces notation. Section II reviews the methods in Hes-
panhol et al. [37] to motivate the need for LTV Dynamic Wa-
termarking. Asymptotic guarantees and implementable tests
for LTV Dynamic Watermarking are provided in Sections III
and IV respectively. Simulated results are presented in Section
V. The appendix covers statistical background for the proofs
in this paper in addition to a few larger equations that have
been removed from proofs to improve readability.

D. Notation

This section breiﬂy introduces the notation used in this
paper. The 2-norm of a vector x is denoted kxk. Similarly,
the 2-norm of a matrix X is denoted kXk. The trace of a
matrix X is denoted tr(X). Zero matrices of dimension i × j
are denoted 0i×j, and in the case that i = j, the notation is
simpliﬁed to 0i. Identity matrices of dimension i are denoted
Ii.

The Wishart distribution with scale matrix Σ and i degrees
of freedom is denoted W(Σ, i) [52, Section 7.2]. The mul-
tivariate Gaussian distribution with mean µ and covariance
Σ is denoted N (µ, Σ). The chi-squared distribution with i
degrees of freedom is denoted χ2(i). The expectation of a
random variable a is denoted E[a]. The probability of an event
E is denoted P(E). Given a sequence of random variables
{ai}∞
i=1, convergence in probability is denoted p-limi→∞ai
and almost sure convergence is denoted as-limi→∞ai [53,
Deﬁnition 7.2.1].

II. INSPIRATION FOR LTV WATERMARKING

This section describes the inspiration for LTV dynamic wa-
termarking by summarizing the method described in Hespan-
hol et al. [37] for LTI systems. Consider an LTI system with
state xn, measurement yn, process noise wn, measurement
noise zn, watermark en, additive attack vn, and stabilizing
feedback that uses the observed state ˆx

xn+1 = Axn + BK ˆxn + Ben + wn
ˆxn+1 = (A + BK + LC)ˆxn + Ben − Lyn

yn = Cxn + zn + vn

(1)
(2)

(3)

where xn, ˆxn, wn ∈ Rp, en ∈ Rq, yn, zn, vn ∈ Rr, and
x0 = 0p×1. The process noise wn, measurement noise zn,
and watermark en are mutually independent and take the
form wn ∼ N (0p×1, Σw), zn ∼ N (0r×1, Σz), and en ∼
N (0q×1, Σe). While the process and measurement noise are
unknown to the controller, the watermark signal is generated
by the controller and is known. The following assumption is
made on the controller, observer, and watermark design.

3

other and with wn and zn. Note, when Σω and Σζ are
selected such that the covariance of the measurement residual
is unaltered and the attack scaling parameter is −1, this model
describes a replay attack. While attackers may have the ability
to start and stop attacks at will, attacks that are only present
for ﬁnite time are not guaranteed to be detected. Therefore,
the
when considering asymptotic guarantees of detection,
assumption of persistence is made. To formally describe these
persistent attacks, consider the following deﬁnition.

Deﬁnition II.2. The asymptotic attack power is deﬁned as

as-lim
i→∞

1
i

i−1
n=0 v⊺

nvn.

(7)

P
Under this deﬁnition, an attack with non-zero asymptotic
power is deemed to be persistent.

The asymptotic claims of LTI dynamic watermarking take

the form of the following theorem.

Theorem II.3.
[37, Theorem 1] Consider an attacked LTI
system satisfying (1)-(3), an attack model satisfying (5)-(6),
and Σ satisfying (4). Let k′ = min{k ≥ 0 | C(A + BK)kB 6=
0r×q} be ﬁnite. If

and

as-lim
i→∞

1
i

P

as-lim
i→∞

1
i

i−1
n=0(C ˆxn − yn)(C ˆxn − yn)⊺ = Σ

(8)

i−1

n=0(C ˆxn − yn)e⊺

n−k′−1 = 0r×q,

(9)

P
then the asymptotic attack power is 0.

The delay of the watermark by k′ in (9) ensures that the
effect of the watermark is present in the measurement signal.
Note, the contrapositive of Theorem II.3 states that for attacks
with non-zero asymptotic power, (8) and (9) cannot both
be satisﬁed. Therefore, considering the LHS of (8) and (9),
generalized replay attacks of non-zero asymptotic power are
guaranteed to be detected in inﬁnite time.

To make these tests implementable in real time, a statistical
test is derived using a sliding window of ﬁxed size. At each
step, the combined partial sums in (8)-(9) take the form

Assumption II.1. Assume kA + BKk < 1, kA + LCk < 1,
and Σe is full rank.

Sn =

n+ℓ
i=n+1

The measurement residual for this system takes the form
C ˆxn − yn. When an attack is not present, the distribution of
the measurement residuals converge to a zero mean Gaussian
distribution with covariance Σ where

Σ = lim
n→∞

E[(C ˆxn − yn)(C ˆxn − yn)⊺].

(4)

Note, for a LTV system, the limit in (4) may not exist.

Next, consider a generalization of a replay attack satisfying

vn = α(Cxn + zn) + Cξn + ζn

ξn+1 = (A + BK)ξn + ωn

(5)
(6)

where α ∈ R is called the attack scaling factor, the false
state ξn ∈ Rp has process noise ωn ∈ Rp and measurement
noise ζn ∈ Rr that take the form ωn ∼ N (0p×1, Σω) and
ζn ∼ N (0r×1, Σζ), and are mutually independent with each

(C ˆxi − yi)
ei−k′−1 (cid:21)

(cid:20)

(C ˆxi − yi)⊺

e⊺
i−k′−1

. (10)

P

(cid:2)
Under the assumption of no attack, Sn converges asymptoti-
cally to the Wishart distribution with scale matrix

(cid:3)

S =

Σ
0q×r

(cid:20)

0r×q
Σe (cid:21)

(11)

and ℓ degrees of freedom as ℓ → ∞. Furthermore, for
a generalized replay attack of non-zero asymptotic power,
Theorem II.3 gives us that the scale matrix for Sn is no longer
S, since either (8) or (9) is not satisﬁed. Given the sampled
matrix Sn, the test then uses the negative log likelihood of the
scale matrix

L(Sn) = (m + q + 1 − ℓ) log(|Sn|) + tr

S−1Sn

.

(12)

Negative log likelihood values that exceed a user deﬁned
threshold, signal an attack.

(cid:0)

(cid:1)

4

For LTV systems,

the limits in (8)-(9) may not exist.
Furthermore, the sampled matrices Sn may no longer be
approximated as a Wishart distribution since the vectors used
to create it in (10) are not necessarily identically distributed.
To accommodate these changes in distribution, it is necessary
to develop a new method.

III. LTV DYNAMIC WATERMARKING

This section derives the limit-based formulation of Dynamic
Watermarking for a discrete-time LTV system. First,
the
LTV dynamics, necessary assumptions, and the resulting limit
based tests are deﬁned. Subsequently, Section III-A provides
intermediate results to prove these claims.

Consider an LTV system with state xn, measurement yn,
process noise wn, measurement noise zn, watermark en,
additive attack vn, and stabilizing feedback that uses the
observed state ˆx

xn+1 = Anxn + BnKn ˆxn + Bnen + wn

yn = Cnxn + zn + vn

(13)

(14)

where xn, ˆxn, wn ∈ Rp, en ∈ Rq, yn, zn, vn ∈ Rr, and
x0 = 0p×1. The process noise wn, measurement noise zn,
and watermark en are mutually independent and take the
form wn ∼ N (0p×1, Σw,n), zn ∼ N (0r×1, Σz,n), and
en ∼ N (0q×1, Σe). While the process and measurement
noise are unknown to the controller, the watermark signal is
generated by the controller and is known. For simplicity, deﬁne
¯An = (An + BnKn) and ¯A(n,m) = ¯An · · · ¯Am for n ≥ m and
¯A(n,n+1) = Ip. We make the following assumption.
Assumption III.1. The covariances Σe, Σw,n, and Σz,n, of
the random variables used in (13)-(14), are full rank. Further-
more, there exists positive constants ηw, ηz, η ¯A, ηB, ηC ∈ R
such that kΣw,nk < ηw, kΣz,nk < ηz, k ¯Ank < η ¯A < 1,
kBnk < ηB, and kCnk < ηC , for all n ∈ N.

The assumption of bounded full rank covariances for the
process and measurement noise are satisﬁed for most systems
by modeling error and sensor noise. Furthermore, the input and
output matrices are often constrained to be ﬁnite by sensor and
actuator limits. Since the watermark and controller are user
deﬁned, the remainder of the assumptions can be satisﬁed so
long as the system is controllable. We make the following
assumption.

Assumption III.2.

lim
i→∞

1
i

i−1
n=0 CnBn−1 6= 0r×q.

(15)

P

Here, (15) guarantees an asymptotic correlation between the
measurement signal yn and the watermark signal en−1, which
has been delayed by a single time step. This ensures that
the watermark has a persistent measurable effect on the
measurement signal, which can then be used for validation
purposes. This is similar to assuming k′ is equal to 0 for the
LTI case.

The observer and the corresponding observer error, deﬁned

as δn = ˆxn − xn, satisfy

ˆxn+1 = ( ¯An + LnCn)ˆxn + Bnen − Lnyn

(16)

δn+1 = (An + LnCn)δn − wn − Ln(zn + vn),

(17)

where ˆx0 = δ0 = 0p×1. For simplicity, deﬁne An = (An +
LnCn) and A(n,m) = An · · · Am for n ≥ m and A(n,n+1) =
Ip. Furthermore, let

¯δn+1 = An
ˆδn+1 = An

¯δn − wn − Lnzn
ˆδn − Lnvn

(18)

(19)

where ¯δ0 = ˆδ0 = 0p×1. Note that δn = ¯δn + ˆδn and that when
vn = 0r×1, ∀n we have that ˆδn = 0p×1, ∀n. Here ¯δn can be
thought of as the portion of the observer error that results from
the original noise of the system, while ˆδn is the contribution
of the attack to the observer error.

Next, consider the expected value Σδ,n = E[¯δn¯δ⊺
n | vn = 0r×1, ∀n], which can be written as

E[δnδ⊺

n] =

Σδ,n =

n
i=0 A(n−1,n−i+1)(Σw,n−i+
+ Ln−iΣz,n−iL⊺

n−i)A⊺

(n−1,n−i+1).

P

The matrix normalization factor is then deﬁned as

Vn = (CnΣδ,nC⊺

n + Σz,n)−1/2,

(20)

(21)

which exists since Σz,n is full rank. For an LTI system, the
matrix Vn = Σ−1/2 where Σ is as deﬁned in (4). For the LTV
system, the matrix normalization factor can be thought of as
a time-varying normalization for the measurement residual.
Next, we make the following assumption about the observer.

Assumption III.3. There exists positive constants ηA, ηL, ηδ,
ηV ∈ R such that kAnk < ηA < 1, kLnk < ηL, kΣδ,nk < ηδ,
and kVnk < ηV , for all n ∈ N.

If the system in (13)-(14) is observable, then the user deﬁned
controller can satisfy the assumption on An. Previous assump-
tions imply the assumptions on Ln, Σδ,n, and Vn are satisﬁed,
but the bounds here are used to simplify notation.

Next, we alter the attack deﬁned in (5)-(6) to create a time-

varying equivalent. Consider an attack vn that satisﬁes

vn = α(Cnxn + zn) + Cnξn + ζn

ξn+1 = ¯Anξn + ωn,

(22)

(23)

where α ∈ R is called the attack scaling factor, the false
state ξn ∈ Rp has process noise ωn ∈ Rp and measurement
noise ζn ∈ Rr that take the form ωn ∼ N (0p×1, Σω,n) and
ζn ∼ N (0r×1, Σζ,n) and are mutually independent with each
other and with wn and zn. Similar to the LTI case, when Σω,n
and Σζ,n are selected properly and the attack scaling parameter
is −1, this model describes a replay attack. The results of such
an attack can have devastating results as shown in Figure 1.
While an attacker could choose to allow the noise to have
unbounded covariance, the resulting attack would be trivial to
detect. Therefore, we make the following assumption about
the attack model.

Assumption III.4. When there is an attack, vn follows the
dynamics (22)-(23) with the attack scaling factor remaining
constant. Furthermore, there exists positive constants ηω, ηη ∈
R such that kΣω,nk < ηω, kΣζ,nk < ηζ , for all n ∈ N.

i−1

n=0 Vn(Cnδn − zn − Cnξn − ζn)e⊺

n−1.

5

(25)

To make asymptotic guarantees of detection, we also assume

the persistence of attacks using the following deﬁnition.

= p-lim
i→∞

1
i

Deﬁnition III.5. The asymptotic attack power is deﬁned as

p-lim
i→∞

1
i

i−1
n=0 v⊺

nvn.

(24)

Similar to prior research in Dynamic Watermarking, we ﬁrst

P

deﬁne the asymptotic tests.

Theorem III.6. Consider an attacked LTV system satisfying
the dynamics in (13)-(19). Let Vn be as deﬁned in (21). If
vn = 0r×1, for all n ∈ N, then

i−1

n=0 Vn(Cn ˆxn − yn)e⊺

n−1 = 0r×q

P

Corollary A.5 says that to show that the RHS of (25) converges
in probability to 0r×q, it is sufﬁcient to show that each term
in the sum converges in probability to 0r×q. Note that
n=0 Vn(Cnδn − Cnξn)e⊺

n−1 = 0r×q

(26)

i−1

1
i

p-lim
i→∞

P

by Corollary A.7 since en−1 is independent identically dis-
tributed with bounded covariance, and Vn(Cnδn − Cnξn) is a
bounded linear transform of a random vector that satisﬁes the
necessary auto correlation bound as a result of Theorem A.9.
Similarly,

(C1)

p-lim
i→∞

1
i

i−1

n=0 Vn(−zn − ζn)e⊺

n−1 = 0r×q

(27)

P

p-lim
i→∞

and

p-lim
i→∞

1
i

1
i

i−1
n=0 Vn(Cn ˆxn − yn)(Cn ˆxn − yn)⊺V ⊺

n = Ir. (C2)

P

Furthermore, if the attack follows the dynamics in (22)-(23)
and has non-zero asymptotic power as deﬁned in Deﬁnition
III.5, then (C1) and (C2) cannot both be satisﬁed.

From Theorem III.6, the LHS of (C1) and (C2) can be
used to guarantee detection of generalized replay attacks
with non-zero asymptotic power in inﬁnite time. Note, (C1),
(C2), and (24) use limits in probability as opposed to the
almost sure limits used in their LTI counterparts. This change
removes the guarantee of detection via the asymptotic tests
for certain pathological examples of attacks, but both forms
of convergence provide the same motivation for the statistical
tests in Section IV. Given an arbitrary real number ǫ, almost
sure convergence states that with probability 1 the sequence
will remain a distance of less than ǫ from the limit after a
ﬁnite number of steps while convergence in probability states
that the probability that an element of the sequence is within
a distance of ǫ from the limit converges to 1 as you continue
along the sequence. Since the statistical tests use a sliding
window to consider only a ﬁnite number of steps at a time,
both forms of convergence say that as the window size grows
the sequence of sample averages become more likely to be
closer to the limit when no attack is present. As a result, the
test becomes more sensitive.

A. Intermediate Results

To prove Theorem III.6, several intermediate results must
ﬁrst be provided. First, we consider the asymptotic limit (C1)
and show that it implies α is equal to 0. This allows us
to assume that α is equal to 0 for the remainder of the
intermediate results.

Theorem III.7. Consider an attacked LTV system satisfying
(13)-(19) and the attack model satisfying (22)-(23). Let Vn be
as deﬁned in (21). (C1) holds if and only if the attack scaling
factor α is equal to 0.

P

by Corollary A.7 since zn, ζn, and en−1 are mutually in-
dependent identically distributed with bounded covariances.
Therefore α = 0 implies (C1) holds.

Now assume that (C1) holds. Rearranging (C1) using (14),

(17), and (22) results in

p-lim
i→∞

1
i

i−1

n=0 Vn(Cn ˆxn − yn)e⊺

n−1 =

P

= p-lim
i→∞

1
i

i−1
n=0 Vn(Cnδn − (1 + α)zn+
− αCnxn − Cnξn − ζn)e⊺

n−1.

P

(28)

Now since (27) holds by the same argument as before, we can
use Theorem A.4 to cancel these terms resulting in
n=0 Vn(Cn ˆxn − yn)e⊺

n−1 =

i−1

1
i

p-lim
i→∞

P

= p-lim
i→∞

1
i

i−1
n=0 Vn(Cnδn − αCnxn)eT

n−1.

(29)

Expanding xn in (29) by one step using (13) then results in

P

1
i

i−1

n=0 Vn(Cn ˆxn − yn)e⊺

p-lim
i→∞
i−1
= p-lim
n=0 Vn(Cnδn − αCn(An−1xn−1+
i→∞
+ Bn−1Kn−1 ˆxn−1 + Bn−1en−1 + wn−1))e⊺

n−1 =

P
1
i

P

n−1.

(30)

Using Corollary A.7 we have that

p-lim
i→∞

1
i

P

i−1

n=0 −αVnCnBn−1(en−1e⊺

n−1 − Σe) = 0q×r.

(31)

1
i

i−1

Therefore by Theorem A.4 we have
n=0 Vn(Cn ˆxn − yn)e⊺

p-lim
i→∞
= p-lim
i→∞
+ Bn−1Kn−1 ˆxn−1 + wn−1))e⊺

P
1
i

P

n−1 =

i−1
n=0 Vn(Cnδn − αCn(An−1xn−1+

n−1 + αVnCnBn−1Σe.

(32)

Note, that all elements of

Vn(Cnδn − αCn(An−1xn−1+

+ Bn−1Kn−1 ˆxn−1 + wn−1))e⊺

n−1

(33)

Proof. (Theorem III.7) Assume that α is equal to 0. Rearrang-
ing the LHS of (C1) using (14), (17), and (22) results in

are distributed symmetrically about 0 for all n ∈ N. Consider
an element of (32) for which the corresponding element in

i−1

n=0 Vn(Cn ˆxn − yn)e⊺

n−1 =

p-lim
i→∞

1
i

P

1
i

i−1
n=0 VnCnBn−1Σe

(34)

P

6

does not converge. For each i, the probability that the matrix
element in (32) is farther away from 0 than the corresponding
element in (34) is at least 0.5. Therefore the element cannot
(cid:4)
converge in probability to 0 completing the proof.

Assuming α is equal to 0, we show that (C2) is equivalent
to another condition that is only dependent on the attack vn
and its contribution to the observer error ˆδn. Note, ˆδn is not
a computable quantity given the available knowledge of the
system, but the provided intermediate condition is an amenable
surrogate to (C2).

Theorem III.8. Consider an attacked LTV system satisfying
(13)-(19) and an attack model satisfying (22)-(23). Let Vn be
as deﬁned in (21). Assume the attack scaling factor α is equal
to 0. (C2) holds if and only if

p-lim
i→∞

1
i

i−1

n=0 Vn(Cnˆδn − vn)(Cn ˆδn − vn)⊺V ⊺

n = 0r.

(35)

P

Proof. (Theorem III.8) Expanding (C2) using (14) and (17)-
(19) gives us

p-lim
i→∞

1
i

i−1
n=0 Vn(Cn ˆxn − yn)(Cn ˆxn − yn)⊺V ⊺

n =

P

= p-lim
i→∞

1
i

i−1

n=0 Vn(Cn ¯δn − zn)(Cn ¯δn − zn)⊺V ⊺

n +

P

+ Vn(Cn ¯δn − zn)(Cn ˆδn − vn)⊺V ⊺
n +
+ Vn(Cn ˆδn − vn)(Cn ¯δn − zn)⊺V ⊺
n +
+ Vn(Cn ˆδn − vn)(Cn ˆδn − vn)⊺V ⊺
n .

By Corollary A.7 and Theorem A.9,

p-lim
i→∞

1
i

and

P

p-lim
i→∞

1
i

i−1

n=0 Vn(Cn ¯δn − zn)(Cn ¯δn − zn)⊺V ⊺

n = Ir

i−1

n=0 Vn(Cn ¯δn − zn)(Cn ˆδ − vn)⊺V ⊺

n = 0r

(36)

(37)

(38)

P

since, by the deﬁnition of Vn in (21), the expectation for each
summand in (37) is Ir, and Vn(Cn¯δn − zn) is uncorrelated
with Vn(Cnˆδ − vn). First, assume that
(C2) holds. By
Theorem A.4, it follows from (36)-(38) that (35) must hold.
Next, assume that (35) holds. By Corollary A.5, it follows
(cid:4)
from (36)-(38) that (C2) holds.

Since the attack vn, under the assumption that α is equal to
0, is only dependent on the random vectors ξn and ζn, we now
provide sufﬁcient and necessary conditions on these random
vectors for the asymptotic attack power to be 0. Similar to
Theorem III.8, these random vectors are not computable by the
controller, but the resulting conditions can be used to connect
(C2) to the asymptotic attack power.

Theorem III.9. Consider an attacked LTV system satisfying
(13)-(19) and an attack model satisfying (22)-(23). Assume
that the attack scaling factor α is equal to 0. The asymptotic
attack power as deﬁned in (24) is 0 if and only if

and

p-lim
i→∞

1
i

i−1
n=0 Cnξnξ⊺

nC⊺

n = 0r.

(40)

P

Proof. (Theorem III.9) Assume that α = 0. Using Lemma
A.10 we have that the asymptotic attack power is 0 if and
only if

p-lim
i→∞

1
i

i−1
n=0 vnv⊺

n = 0r.

(41)

Expanding the LHS of (41) using (22)-(23) we get an equva-
lent condition.

P

p-lim
i→∞

1
i

i−1
n=0 Cnξnξ⊺

nC⊺

n+

+ Cnξnζ⊺
P

n + (Cnξnζ⊺

n)⊺ + ζnζ⊺

n = 0r

(42)

Since ξn and ζn are uncorrelated, from Theorem A.9 and
Corollary A.7 we have

p-lim
i→∞

1
i

i−1
n=0 Cnξnζ⊺

n = 0r.

(43)

P

First, assume that (39) and (40) hold. By Corollary A.5 we
have that (42) must hold since, when separated, the limit for
each term converges to 0r. Next, assume that (42) holds. By
Theorem A.4 we can rewrite (42) as

p-lim
i→∞

1
i

i−1
n=0 ζnζ⊺

n + Cnξnξ⊺

nC⊺

n = 0r,

(44)

since (43) holds. Note, both terms are positive-semideﬁnite
matrices. Therefore, for an arbitrary ǫ > 0 we have that

P

P

1
i

i−1
n=0 ζnζ⊺
n

> ǫ

≤

(cid:16)(cid:13)
(cid:13)
(cid:13)

≤ P
P

1
i

(cid:13)
(cid:17)
i−1
n=0 ζnζ⊺
(cid:13)
(cid:13)
P

Furthermore, (44) implies

n + Cnξnξ⊺

nC⊺
n

(cid:16)(cid:13)
(cid:13)
(cid:13)
i−1
n=0 ζnζ⊺

P

lim
i→∞

1
i

(cid:16)(cid:13)
(cid:13)
(cid:13)
Then, by (45) and (46)

P

n + Cnξnξ⊺

nC⊺
n

> ǫ

(cid:17)

(cid:13)
(cid:13)
(cid:13)

> ǫ

(45)

(cid:17)

(cid:13)
(cid:13)
(cid:13)
= 0r, ∀ǫ > 0
(46)

P

lim
i→∞

1
i

i−1

n=0 ζnζ⊺

n

> ǫ

= 0r, ∀ǫ > 0.

(47)

(cid:13)
P
Therefore, (39) must hold. Applying Theorem A.4 to (44)
(cid:13)
(cid:13)
(cid:4)
using (39) implies (40) must also hold.

(cid:16)(cid:13)
(cid:13)
(cid:13)

(cid:17)

Next, we start to complete the connection between (C2) and
zero asymptotic attack power by proving (35) implies (39).
Furthermore, we prove a related result that makes it simpler
to prove that (35) implies (40).

Theorem III.10. Consider an attacked LTV system satisfying
(13)-(19) and an attack model satisfying (22)-(23). Let Vn be
as deﬁned in (21). Assume the attack scaling factor α is equal
to 0. If (35) holds, then (39) holds as well and

p-lim
i→∞

1
i

i−1

n=0(Cn ˆδn − Cnξn)(Cn ˆδn − Cnξn)⊺ = 0r.

(48)

P

Proof. (Theorem III.10) Assume that (35) holds. Expanding
the LHS of (35) using (22) we get

p-lim
i→∞

1
i

i−1
n=0 ζnζ⊺

n = 0

P

(39)

p-lim
i→∞

1
i

i−1

n=0 Vn(Cn ˆδn − Cnξn)(Cn ˆδn − Cnξn)⊺V ⊺

n +

P

+ Vn(Cn ˆδn − Cnξn)ζ⊺
nV ⊺
+ Vnζnζ⊺

n = 0r.

nV ⊺

n + (Vn(Cn ˆδn − Cnξn)ζ⊺

nV ⊺

n )⊺+
(49)

Using Corollary A.7 and Theorem A.9 we have

7

where mn = min{n, m}. Furthermore, there exists an m′ ∈ N
such that m′ ≤ m and

p-lim
i→∞

1
i

i−1

n=0 Cn ¯A(n−1,n−j+1)ωn−j×

i−1

n=0 Vn(Cn ˆδn − Cnξn)ζ⊺

nV ⊺

n = 0r.

(50)

P

× ω⊺

n−j

¯A⊺

(n−1,n−j+1)C⊺

n 6= 0r

(56)

p-lim
i→∞

1
i

Therefore, by applying Theorem A.4 to (49) we have

P

p-lim
i→∞

1
i

P

i−1

n=0 Vn(Cnˆδn − Cnξn)(Cn ˆδn − Cnξn)⊺V ⊺

n +

+ Vnζnζ⊺

nV ⊺

n = 0r.

(51)

Note, both terms are positive-semideﬁnite matrices. Using the
same method used on (44), we then have

for j = m′ but not for j < m′.

Proof. (Lemma III.12) First, we prove the existence of m.
Assume that (40) does not hold. Expanding the LHS of (40)
using (23) results in

p-lim
i→∞

1
i

i−1
n=0

n

j=1 Cn ¯A(n−1,n−j+1)ωn−j

×

P
×

(cid:16)P
n
j=1 Cn ¯A(n−1,n−j+1)ωn−j

⊺

(cid:17)
6= 0r.

(57)

i−1
n=0 Vnζnζ⊺

nV ⊺

n = 0r

(52)

Then, using Lemma A.10 we have that

(cid:16)P

(cid:17)

i−1
n=0

n

j=1 Cn ¯A(n−1,n−j+1)ωn−j

2

6= 0.

(58)

p-lim
i→∞

1
i

P

1
i

(cid:18)

P

i−1

n=0 Vn(Cnˆδn − Cnξn)(Cn ˆδn − Cnξn)⊺V ⊺

n = 0r.
(53)

Since (40) does not hold there exists ǫ, τ > 0 such that

P

P

(cid:13)
(cid:13)
(cid:13)

i−1
n=0

n

j=1 Cn ¯A(n−1,n−j+1)ωn−j

We complete the proof using Lemma A.11 but we must ﬁrst
provide lower bound on the eigenvalues of V ⊺
n Vn. Let λn
denote the smallest eignenvalue of V ⊺
n Vn, then λn is lower
bounded since

1

λn =

≥

k(V ⊺
n Vn)−1k
1
η2
C ηδ + ηz

> 0.

=

1
kCnΣδ,nC⊺
n + Σz,nk

≥

P

1
i

(cid:18)

P

(54)

P

(cid:13)
(cid:13)
(cid:13)

P

(cid:13)
(cid:13)
(cid:13)

for inﬁnitely many i. We prove that there exists an m such
that for each i that (59) holds we have

i−1
n=0

mn

j=1 Cn ¯A(n−1,n−j+1)ωn−j

2

> ǫ
6

(cid:13)
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

> ǫ

> τ

(cid:19)

(59)

>

(cid:19)

τ
4
(60)

p-lim
i→∞

1
i

P

and

p-lim
i→∞

1
i

P

If we assume that
(39) does not hold then applying
Lemma A.11 contradicts (52). Therefore (39) must hold.
Similarly, assuming that (48) does not hold would result in a
(cid:4)
contradiction with (53). Therefore (48) must also hold.

Next we prove that (35) implies (40) to complete the relation

between (C2) and the asymptotic attack power.

Theorem III.11. Consider an attacked LTV system satisfying
(13)-(19) and an attack model satisfying (22)-(23). Let Vn be
as deﬁned in (21). Assume the attack scaling factor α is equal
to 0. If (35) holds then (40) holds as well.

To prove Theorem III.11, we instead prove the contrapos-
itive statement by assuming that (40) does not hold, proving
that (48) does not hold either, then using Theorem III.10 we
complete the proof. To do this, we split the summation in
(40) according to the following lemma. This split allows us to
disregard the cross terms on the LHS of (48) and shows that
the remaining terms do not converge in probability to 0r.

Lemma III.12. Consider an attacked LTV system satisfying
(13)-(19) and an attack model satisfying (22)-(23). Let Vn be
as deﬁned in (21). Assume the attack scaling factor α is equal
to 0. If (40) does not hold then there exists m ∈ N for which

which is equivalent to (55) as a result of Lemma A.10. To
make statements on the truncated sum, we start by ﬁnding the
relationship between the probability in the LHS of (59) and
the probability in the LHS of (60). For each i such that (59)
holds, we apply triangle inequality to get

τ < P

1
i

i−1
n=0

mn

j=1 Cn ¯A(n−1,n−j+1)ωn−j

+

(cid:18) (cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
.
> ǫ
(cid:19)
Further expanding and applying Theorem A.1 result in

P
n
j=mn+1 Cn ¯A(n−1,n−j+1)ωn−j

P

P

(cid:19)

2

(cid:18)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(61)

τ < P

1
i

i−1
n=0

mn

j=1 Cn ¯A(n−1,n−j+1)ωn−j

(cid:18)
2
i

P

i−1
n=0

+ P

(cid:13)
(cid:13)
(cid:13)

mn

P
j=1 Cn ¯A(n−1,n−j+1)ωn−j

> ǫ
3

+

(cid:19)

2

(cid:13)
(cid:13)
×
(cid:13)

(cid:18)

×

n

P
j=mn+1 Cn ¯A(n−1,n−j+1)ωn−j

P

(cid:13)
(cid:13)
(cid:13)

> ǫ
3

(cid:13)
(cid:13)
+
(cid:13)

(cid:19)

(cid:13)
(cid:13)
(cid:13)

n

j=mn+1 Cn ¯A(n−1,n−j+1)ωn−j

(cid:13)
(cid:13)
+ P
(cid:13)

P

1
i

i−1
n=0

(cid:18)

P

P

(cid:13)
(cid:13)
(cid:13)

.

> ǫ
3
(cid:19)
(62)

2

(cid:13)
(cid:13)
(cid:13)

Focusing on the center term in the RHS of (62), we can write

p-lim
i→∞

1
i

i−1
n=0

Cn

mn
j=1

¯A(n−1,n−j+1)ωn−j

×

P

×

(cid:16)
Cn

P

mn
j=1

¯A(n−1,n−j+1)ωn−j

(cid:16)

P

6= 0r.

(55)

×

(cid:17)
⊺

(cid:17)

P

2
i

i−1
n=0

mn

j=1 Cn ¯A(n−1,n−j+1)ωn−j

×

P
n

P

(cid:13)
(cid:13)
(cid:13)

j=mn+1 Cn ¯A(n−1,n−j+1)ωn−j

P

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
> ǫ
3

≤

!

(cid:13)
(cid:13)
(cid:13)

 
8

≤ P

 r

2
i

×

r

≤ P

(cid:18)

+ P

2
i

i−1
n=0

mn

j=1 Cn ¯A(n−1,n−j+1)ωn−j

2

×

P
i−1
n=0

(cid:13)
(cid:13)
P
(cid:13)
(cid:13)
(cid:13)
(cid:13)
n
j=mn+1 Cn ¯A(n−1,n−j+1)ωn−j

2

> ǫ
3

P
2
i

P

(cid:13)
(cid:13)
i−1
(cid:13)
n=0

mn

j=1 Cn ¯A(n−1,n−j+1)ωn−j

2

(cid:13)
(cid:13)
(cid:13)
> ǫ
3

P

n

j=mn+1 Cn ¯A(n−1,n−j+1)ωn−j

(cid:13)
(cid:13)
(cid:13)

i−1
n=0

P

2
i

P

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

P

≤

!

+

(cid:19)
2
> ǫ
3
(cid:19)
(63)

,

(cid:13)
(cid:13)
(cid:13)

where the ﬁrst inequality comes from applying the Cauchy
Schwarz Inequality and the second inequality comes from
applying Theorem A.2. Then since

P

1
i

(cid:18)
≤ P

i−1
n=0

mn

j=1 Cn ¯A(n−1,n−j+1)ωn−j

2

> ǫ
3

≤

P
2
i

P

(cid:13)
(cid:13)
i−1
(cid:13)
n=0

(cid:13)
(cid:13)
j=1 Cn ¯A(n−1,n−j+1)ωn−j
(cid:13)

mn

2

(cid:19)
> ǫ
3

(cid:18)

P

and

P

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

P

1
i

(cid:18)
≤ P

i−1
n=0

n

j=mn+1 Cn ¯A(n−1,n−j+1)ωn−j

2

> ǫ
3

≤

P
2
i

(cid:13)
(cid:13)
i−1
(cid:13)
n=0

P

(cid:13)
(cid:13)
j=mn+1 Cn ¯A(n−1,n−j+1)ωn−j
(cid:13)

n

2

(cid:18)

P

P

(cid:13)
(cid:13)
(cid:13)

we can combine (62) with (63)-(65) to obtain

(cid:13)
(cid:13)
(cid:13)

(cid:19)
> ǫ
3
(cid:19)
(65)

,

τ < 2P

1
i

i−1
n=0

mn

j=1 Cn ¯A(n−1,n−j+1)ωn−j

2

> ǫ
6

+

P
i−1
n=0

(cid:13)
(cid:13)
(cid:13)

(cid:13)
P
(cid:13)
n
j=mn+1 Cn ¯A(n−1,n−j+1)ωn−j
(cid:13)

2

+ 2P

(cid:18)
1
i

(cid:18)

P

P

(cid:13)
(cid:13)
(cid:13)

(cid:19)
> ǫ
6
(cid:19)
(66)

.

(cid:13)
(cid:13)
(cid:13)

2 the ﬁrst term must be lower bounded by τ

If we can upper bound the second term in the RHS or (66)
by τ
2 completing
the proof. To provide this bound we make use of Markov’s
Inequality. To this end, we ﬁrst bound the expectation

E

1
i

(cid:18)
= E

i−1
n=0

n

j=mn+1 Cn ¯A(n−1,n−j+1)ωn−j

2

=

P

(cid:13)
(cid:13)
i−1
(cid:13)
n=0

n
j=mn+1

(cid:13)
(cid:13)
Cn ¯A(n−1,n−j+1)ωn−j
(cid:13)

(cid:19)
⊺

×

P

P
Cn ¯A(n−1,n−j+1)ωn−j

(cid:0)

≤

!

(cid:1)

1
i

≤ E
(cid:16)
×

i−1
n=0

∞
j=m+1
Cn+j ¯A(n+j−1,n+1)ωn
P

P

(cid:0)

Cn+j ¯A(n+j−1,n+1)ωn

(cid:1)

⊺

×

(cid:1)

P
1
i

×

(cid:0)

≤ 1
i

i−1
(cid:0)
n=0

∞
j=m+1 pη2

(cid:1) (cid:17)
ω = pη2
Cη2(j−1)
η2

A1

C η2m
A1 η2
ω
1−η2
A1

,

(67)

P

P

where the the ﬁrst equality comes from expanding the norm
and ignoring uncorrelated terms, the ﬁrst inequality comes
from rearranging the summation and allowing the second
summation to go to inﬁnity, the second inequality comes from
distributing the expectation and upper bounding each element,

p-lim
i→∞

1
i

P
1
i

= p-lim
i→∞

×

P
ˆδn −
(cid:16)

p-lim
i→∞

1
i

P
1
i

= p-lim
i→∞

P
ˆδn −

×

P
ˆδn −

− Cn

× ω⊺

n−m′ ¯A⊺

and the ﬁnal equality comes from evaluating the summations.
Since ηA1 < 1, we can choose m sufﬁciently large such that

pη2

C η2m
A1 η2
1−η2
A1

ω

< τ ǫ
24 .

(68)

Using Markov’s inequality [54, Equation 5.31] we have that

P

1
i

(cid:18)

n

i−1
n=0

j=mn+1 Cn ¯A(n−1,n−j+1)ωn−j
(cid:13)
C η2m
≤ 6pη2
A1 η2
P
P
(cid:13)
A1)ǫ < τ
ω
(cid:13)
(1−η2

4

2

> ǫ
6

≤

(cid:19)

(69)

(cid:13)
(cid:13)
(cid:13)

which completes the proof for the existence of m.

Next, we prove the existence of m′. Consider the expansion

of (55)

p-lim
i→∞

1
i

i−1
n=0 Cn

P

mn
j=1
× ω⊺
P

mn
k=1
¯A⊺

¯A(n−1,n−j+1)ωn−j×

P
n−k

(n−1,n−k+1)C⊺

n 6= 0.

(64)

(cid:19)

Considering the summands where j 6= k we have that
n=0 Cn ¯A(n−1,n−j+1)ωn−j×

i−1

1
i

p-lim
i→∞

P

× ω⊺

n−k

¯A⊺

(n−1,n−k+1)C⊺

n = 0

(70)

(71)

by Theorem A.6 since ωn is independent and the dynamics
are bounded and stable. If we further assume that there does
not exist an m′ for which (56) holds then by Theorem A.1
we have that (55) does not hold which is a contradiction.
Therefore, the set of integers less that or equal to m for
which (56) holds, is a non-empty ﬁnite set. The smallest
(cid:4)
element of this set then satisﬁes the conditions for m′.

Now returning to prove the Theorem.

Proof. (Theorem III.11) WLOG, in this proof, we allow sum-
mations to reference variables with negative index by assuming
these values to be 0r to ease notation. Assume that (35) holds
but (40) does not. Since (40) does not hold, m′ be chosen
such that it satisﬁes the description in Lemma III.12. From
Theorem III.10 we have that (35) implies (48). Expanding the
LHS of (48) using (23) gives us

i−1

n=0(Cn ˆδn − Cnξn)(Cn ˆδn − Cnξn)⊺ =

i−1
n=0

Cn

ˆδn −

n
j=1

¯A(n−1,n−j+1)ωn−j

×

(cid:16)
n
k=1

(cid:16)

¯A(n−1,n−k+1)ωn−k

P

⊺

C⊺
n

= 0r.

(cid:17)
(72)

(cid:17)

(cid:17)
By separating the index m′ we can write

P

i−1

n=0(Cn ˆδn − Cnξn)(Cn ˆδn − Cnξn)⊺ =

i−1
n=0 Cn

ˆδn −

¯A(n−1,n−j+1)ωn−j

×

!

1≤j≤n
j6=m′

P

¯A(n−1,n−k+1)ωn−k

0≤k≤n
k6=m′

⊺

!

C⊺

n+

¯A(n−1,n−j+1)ωn−j

×

1≤j≤n
j6=m′
(n−1,n−m′+1)C⊺

P

!
n − Cn ¯A(n−1,n−m′+1)ωn−m′×

 
 
 
 
⊺

!

C⊺

n+

auto-correlation is bounded by (156) in the appendix. Fur-
thermore, expanding the LHS of (79) using (19) gives us

9

(73)

×

!

(74)

i−1

n=0 −Cnˆδnω⊺

n−m′ ¯A⊺

(n−1,n−m′+1)C⊺

n =

i−1
n=0 Cn

n−1
j=0 A(n−1,j+1)Ljζj

1
i

p-lim
i→∞
= p-lim
i→∞
+

P
1
i

P

n
(cid:0) P
k=1 A(n−1,n−k+1)Ln−kCn−k×
n
¯A(n−k−1,n−ℓ+1)ωn−ℓ
×
ℓ=k+1
(n−1,n−m′+1)C⊺
n = 0r.
(cid:1)

×
P
n−m′ ¯A⊺
× ω⊺
P

(80)

To prove that (79) holds, we use Corollary A.5 on (80) and
show that each term converges to 0r. Note, by Theorem A.6,

i−1

n=0(Cn ˆδn − Cnξn)(Cn ˆδn − Cnξn)⊺ =

p-lim
i→∞

1
i
× ω⊺
P

i−1
n=0 Cn
n−m′ ¯A⊺

n−1
j=0 A(n−1,j+1)Ljζj

×

(n−1,n−m′+1)C⊺
(cid:0) P

n = 0r,

(cid:1)

(81)

¯A(n−1,n−k+1)ωn−k

×

ˆδn −

0≤k≤n
k6=m′
+ Cn ¯A(n−1,n−m′+1)ωn−m′×
× ω⊺

P
n−m′ ¯A⊺
(n−1,n−m′+1)C⊺

n = 0r.

For now suppose that

p-lim
i→∞

1
i

i−1
n=0 −Cn

ˆδn −

1≤j≤n
j6=m′
(n−1,n−m′+1)C⊺

P

n = 0r.

P
× ω⊺

n−m′ ¯A⊺

¯A(n−1,n−j+1)ωn−j

Then by Theorem A.4 we have that

p-lim
i→∞

1
i

P
1
i

= p-lim
i→∞

i−1
n=0 Cn

ˆδn −

¯A(n−1,n−j+1)ωn−j

×

!

1≤j≤n
j6=m′

P

P
ˆδn −

×

¯A(n−1,n−k+1)ωn−k

0≤k≤n
k6=m′
+ Cn ¯A(n−1,n−m′+1)ωn−m′×
× ω⊺

P
n−m′ ¯A⊺
(n−1,n−m′+1)C⊺
Furthermore, by our choice of m′ we have that

n = 0r.

!

⊺

C⊺

n+

p-lim
i→∞

i−1

1
i
× ω⊺
P

n=0 Cn ¯A(n−1,n−m′+1)ωn−m′×
n−m′ ¯A⊺
(n−1,n−m′+1)C⊺

n 6= 0r,

and since the terms are all positive-semideﬁnite matrices

i−1

n=0 Cn ¯A(n−1,n−m′+1)ωn−m′ ×
(n−1,n−k+1)C⊺
≤

¯A⊺

> ǫ

n−k

n

× ω⊺
P

P

1
i

(cid:0)(cid:13)
(cid:13)
≤ P

1
i

i−1
n=0 Cn

ˆδn −

(cid:13)
(cid:1)
¯A(n−1,n−j+1)ωn−j
(cid:13)
1≤j≤n
j6=m′

P

¯A(n−1,n−k+1)ωn−k

⊺

!

C⊺

n+

 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
×

P
ˆδn −

0≤k≤n
k6=m′
+ Cn ¯A(n−1,n−m′+1)ωn−m′×

P

× ω⊺

n−m′ ¯A⊺

(n−1,n−m′+1)C⊺

n

×

!

(77)

This implies that (73) cannot hold which contradicts (35).
Therefore (40) must hold since otherwise there exists an m′
satisfying Lemma A.11.

To complete the proof, we now show that (74) indeed holds.

by Corollary A.5 this is equivalent to proving

p-lim
i→∞

1
i

i−1
n=0 Cn
n−m′ ¯A⊺

1≤j≤n
j6=m′
P
(n−1,n−m′+1)C⊺

× ω⊺
P

n = 0r

¯A(n−1,n−j+1)ωn−j×

and

p-lim
i→∞

1
i

i−1

n=0 −Cnˆδnω⊺

n−m′ ¯A⊺

(n−1,n−m′+1)C⊺

n = 0r.

(78)

(79)

P

Note, (78) holds by Corollary A.7 since all ωn are mutually
independent, kCn ¯A(n−1,n−m′+1)k ≤ kCnk < ηC , and the

> ǫ

.
!

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

since kCn ¯A(n−1,n−m′+1)k ≤ ηC , ζn and ωn are mutually
independent, and the auto-correlation is bounded by (157) in
the appendix. Furthermore, considering the portion of ˆδn not
dependent on ωn−m′, by Theorem A.6,

(75)

1
i

i−1
n=0 Cn

n
j=1 A(n−1,n−j+1)Ln−jCn−j ×

p-lim
i→∞
×

n
P
k=j+1
k6=m′
n−m′ ¯A⊺
× ω⊺
P

¯A(n−j−1,n−k+1)ωn−k×

P

(n−1,n−m′+1)C⊺

n = 0r,

(82)

(76)

since ωn are independent, kCn ¯A(n−1,n−m′+1)k ≤ ηC , and the
auto-correlation is bounded by (158) in the appendix. Now if

p-lim
i→∞

1
i

i−1
n=0 Cn

m′−1
j=1 A(n−1,n−j+1)Ln−jCn−j×

× ¯A(n−j−1,n−m′+1)ωn−m′×
P
× ω⊺

P
n−m′ ¯A⊺

n−1,n−k+1C⊺

n = 0r,

(83)

we have completed the proof. To show this, we show that the
trace of the matrix converges to 0 for each value of j.

p-lim
i→∞

1
i

i−1
n=0

n−m′ ¯A⊺
ω⊺

(n−1,n−m′+1)C⊺

n×

P

(cid:0)

× CnA(n−1,n−j+1)Ln−jCn−j ×
× ¯A(n−j−1,n−m′+1)ωn−m′
1
i

Cn ¯A(n−1,n−m′+1)ωn−m′

i−1
n=0

≤

(cid:1)

≤ p-lim
i→∞

2

1/2

×

P
1
i

(cid:13)
i−1
(cid:13)
n=0

(cid:0)
×
× Cn−j ¯A(n−j−1,n−m′+1)ωn−m′

(cid:13)
CnA(n−1,n−j+1)Ln−j×
(cid:13)
1/2

P

(cid:0)

2

(cid:1)

(cid:13)
(cid:13)

(84)

where the inequality follow from the Cauchy Schwarz Inequal-
ity. Let ǫ, τ > 0 be chosen arbitrarily. Note that by Markov’s
Inequality

(cid:1)

(cid:13)
(cid:13)

P

1
i

i−1
n=0

Cn ¯A(n−1,n−m′+1)ωn−m′

2

≥

(cid:16)

P

(cid:13)
(cid:13)

≥ 2η2

C η2(m′ −1)
A1
1−τ

ηω

(cid:13)
≤
(cid:13)

i−1

n=0kCn ¯A(n−1,n−m′ +1)ωn−m′k2
ηω

(cid:17)

(1−τ )E
h

1
i P

≤

2η2
Cη2(m′ −1)
ηω

C η2(m′ −1)

A1

A1

A1

C η2(m′ −1)
ηω

= 1−τ
2 .

≤ (1−τ )η2
2η2

i

≤

(85)

 
 
 
 
 
 
10

Fig. 1. Desired and attacked trajectory of an LTV car model showing attack start and detection (Left); Corresponding LTV Dynamic Watermarking test metric
showing attack start and detection (right)

Furthermore by our choice of m′, we have that there exists an
N such that i > N implies

Under our assumption of non-zero asymptotic power,
contrapositive implies that (C2) does not hold.

the
(cid:4)

1
i

P

i−1
n=0

Cn−j ¯A(n−j−1,n−m′+1)ωn−m′
ǫ2
C η2(m′ −1)
η2(j−1)
A2
Finally, applying Theorem A.3

(cid:13)
≤
(cid:13)

η2
Lηω

P

2η4

(cid:17)

(cid:16)

A1

(cid:13)
≥ τ +1
2 .
(cid:13)

2

≤

P

1
i

i−1
n=0

Cn ¯A(n−1,n−m′+1)ωn−m′

1/2

2

×

(cid:18)(cid:18)

P
×

(cid:13)
(cid:13)

1
i

i−1
n=0

(cid:19)
CnA(n−1,n−j+1)Ln−j×

(cid:13)
(cid:13)

(cid:18)

P

(cid:13)
(cid:13)

× Cn−j ¯A(n−j−1,n−m′+1)ωn−m′

1/2

2

≥ P

1
i

i−1
n=0

Cn−j ¯A(n−j−1,n−m′+1)ωn−m′

(cid:19)

(cid:13)
(cid:13)

(cid:18)

P
≤

(cid:13)
ǫ2
(cid:13)
C η2(m′ −1)
η2(j−1)
A2
A1

2η4

+

1 − P

(cid:18)
× ωn−m′

(cid:18)
2

1
i

i−1
n=0

P
≥ 2η2

(cid:13)
(cid:13)
Cη2
A1ηω

(cid:13)
(cid:13)

+

η2
Lηω (cid:19)
Cn ¯A(n−1,n−m′+1)×

≥

τ + 1
2

+ 1 −

(cid:13)
1 − τ
(cid:13)
2

− 1 = τ.

Therefore (83) must hold.

− 1 ≥

(cid:19)(cid:19)

(87)

(88)

(cid:4)

Having proven several intermediate results, we are now able

to formally prove Theorem III.6.

Proof. (Theorem III.6) When no attack is present, (C1) holds
using Theorem III.7 since α is equal to 0. Furthermore, (C2)
holds since δ = δ.

Now assume that an attack of non-zero asymptotic power

is present and consider the following cases.
Case 1 (α 6= 0): Using Theorem III.7, (C1) does not hold.
Case 2 (α = 0): Note, (C2) implies zero asymptotic attack
power as follows.

(C2) ⇐⇒

Thm. III.8

(35)

=⇒
Thm. III.10
=⇒
Thm. III.11

(39)

(40)

⇐⇒
Thm. III.9

(cid:18)

zero asymptotic
attack power

(cid:19)

(86)

IV. IMPLEMENTABLE STATISTICAL TESTS

While Section III provides a necessary background for LTV
Dynamic Watermarking, inﬁnite limits are not well suited for
real time attack detection. This section derives a statistical test
using a sliding window approach. Let

ψn =

(cid:20)

Vn(Cn ˆxn − yn)
en−1

(cid:21)

(89)

and

≤ ǫ

≥

(cid:19)

2

Qn = [ψn−ℓ . . . ψn][ψn−ℓ . . . ψn]⊺.

(90)

where ℓ+1 is the window size, ℓ ∈ N, and ℓ ≥ q +r −1. Note,
ψn is asymptotically uncorrelated and identically distributed
such that ψn ∼ N (0q+r×1, S), for n = 1, 2, 3, · · · where

S =

Ir
0q×r
(cid:20)

0r×q
Σe (cid:21)

.

(91)

Therefore, under the assumption of no attack, the distribution
of Qn approaches a Wishart distribution with ℓ + 1 degrees of
freedom and scale matrix S as ℓ goes to inﬁnity. Furthermore,
for a generalized replay attack with non-zero asymptotic
power, Theorem III.6 proves that the scale matrix for Qn is no
longer S since either (C1) or (C2) is not satisﬁed. The Wishart
distribution can then be used to deﬁne a statistical test using
the negative log likelihood of the scale matrix S given the
sampled matrix Qn:

L(Qn) = (q + r − ℓ) log(|Qn|) + tr(S−1Qn).

(92)

In theory, if the process and measurement noise covariances
Σw,n and Σz,n are known, Vn can be calculated using (20)-
(21). In practice, these covariances are difﬁcult to estimate
which can lead to error in the estimate of Vn. To reduce this
error, Vn can be directly estimated using an ensemble average
of i realizations such that

Vn ≈

1
i

i

j=1(Cn ˆx(j)

n − y(j)

n )(Cn ˆx(j)

n − y(j)

n )⊺

(cid:16)

P

−1/2

(93)

(cid:17)

11

the measurement and process noise matching that of the true
system. The results of this attack on the system, and the ability
of LTV Dynamic Watermarking to quickly detect it, are shown
in Figure 1.

VI. CONCLUSION

This paper derives Dynamic Watermarking for LTV sys-
tems, and provides asymptotic guarantees in addition to im-
plementable tests. A LTV generalized replay attack is deﬁned
and shown to be detectable by the Dynamic Watermarking
method developed in this work. Furthermore, a vehicle model
with LTV Dynamic Watermarking is simulated to provide
proof of concept of the implementable tests. Using these
simulations, the LTV Dynamic Watermarking is compared to
its LTI counterpart and is shown to provide a more consistent
test metric.

REFERENCES

[1] R. Langner, “Stuxnet: Dissecting a cyberwarfare weapon,” IEEE Security

& Privacy, vol. 9, no. 3, pp. 49–51, May 2011.

[2] M. Abrams and J. Weiss, “Malicious control system cyber security attack

case study - Maroochy water services, australia,” MITRE, 2008.

[3] R. M. Lee, M. J. Assante, and T. Conway, “German steel mill cyber

attack,” Industrial Control Systems, vol. 30, p. 62, 2014.

[4] ——, “Analysis of the cyber attack on the ukrainian power grid,”
Electricity Information Sharing and Analysis Center (E-ISAC), 2016.
[5] H. Sandberg, S. Amin, and K. H. Johansson, “Cyberphysical security in
networked control systems: An introduction to the issue,” IEEE Control
Systems Magazine, vol. 35, no. 1, pp. 20–23, 2015.

[6] A. A. C´ardenas, S. Amin, and S. Sastry, “Research challenges for the
security of control systems,” in Proceedings of the 3rd Conference on
Hot Topics in Security, ser. HOTSEC’08. Berkeley, CA, USA: USENIX
Association, 2008, pp. 1–6.

[7] A. A. C´ardenas, S. Amin, and S. Sastry, “Secure control: Towards sur-
vivable cyber-physical systems,” in The 28th International Conference
on Distributed Computing Systems Workshops, June 2008, pp. 495–500.
[8] S. Amin, A. A. C´ardenas, and S. Sastry, “Safe and secure networked
control systems under denial-of-service attacks,” in International Work-
shop on Hybrid Systems: Computation and Control. Berlin, Heidelberg:
Springer, 2009, pp. 31–45.

[9] Y. Liu, P. Ning, and M. K. Reiter, “False data injection attacks against
state estimation in electric power grids,” ACM Transactions on Informa-
tion and System Security (TISSEC), vol. 14, no. 1, pp. 13:1–13:33, Jun.
2011.

[10] A. Teixeira, D. P´erez, H. Sandberg, and K. H. Johansson, “Attack models
and scenarios for networked control systems,” in Proceedings of the 1st
International Conference on High Conﬁdence Networked Systems, ser.
HiCoNS ’12. New York, NY, USA: ACM, 2012, pp. 55–64.

[11] R. M. Ferrari and A. M. Teixeira, “Detection and isolation of routing
attacks through sensor watermarking,” in 2017 Annual American Control
Conference (ACC), May 2017, pp. 5436–5442.

[12] R. S. Smith, “A decoupled feedback structure for covertly appropriating
networked control systems,” IFAC Proceedings Volumes, vol. 44, no. 1,
pp. 90 – 95, 2011.

[13] Y. Yuan and Y. Mo, “Security in cyber-physical systems: Controller
design against known-plaintext attack,” in 54th IEEE Conference on
Decision and Control (CDC), Dec 2015, pp. 5814–5819.

[14] D. Umsonst, E. Nekouei, A. M. Teixeira, and H. Sandberg, “On
the conﬁdentiality of linear anomaly detector states,” in 2019 Annual
American Control Conference (ACC), July 2019, pp. 397–403.

[15] M. Porter, A. Joshi, P. Hespanhol, R. Vasudevan, and A. Aswani,
“Simulation and real-world evaluation of attack detection schemes,” in
2019 Annual American Control Conference (ACC), July 2019, pp. 551–
558.

[16] S. Weerakkody, O. Ozel, P. Grifﬁoen, and B. Sinopoli, “Active detection
for exposing intelligent attacks in control systems,” in 2017 IEEE
Conference on Control Technology and Applications (CCTA), Aug 2017,
pp. 1306–1312.

Fig. 2. Simulated LTI and LTV Dynamic Watermarking test metrics for LTV
car model under no attack

where the superscript (j) is the index of the realization. This
approximation is appropriate since by the weak law of large
numbers we have that when no attack is present

p-lim
i→∞

1
i

i

j=1(Cn ˆx(j)

n − y(j)

n )(Cn ˆx(j)

n − y(j)

n )⊺ =

P

= CnΣδ,nC⊺

n + Σz,n

(94)

and Vn is deﬁned as in (21).

V. SIMULATED RESULTS

To provide proof of concept, we use a simpliﬁed car model

x
y
ψ
v
˙ψ

v cos(ψ)
v sin(ψ)
˙ψ
a
¨ψ





=





,

(95)
























where the car has ground plane coordinates (x, y), heading
ψ, forward velocity v, and angular velocity ˙ψ. Using the
desired trajectory shown in Figure 1, (95) is linearized and
discretized using a step size of 0.05 and zero order hold on
the current state and input. The controller and observer for
the resulting LTV system are found using a linear quadratic
regulator (LQR) to stabilize the system. Furthermore,
the
process and measurement noise covariances are chosen such
that they scale linearly with the velocity.

To compare LTI and LTV Dynamic Watermarking, a time
invariant matrix normalization factor is calculated using the
average of the residual covariance, while the time-varying
matrix normalization factor is calculated using (20)-(21). For
both cases, we run 100 simulations with a window size of
20 and calculate the test metric and the average test metric
as shown in Figure 2. Note, while the LTV Dynamic Water-
marking metric remains consistent over the entire simulation,
the LTI counterpart has a repeatable time-varying pattern.

Using the un-attacked data, a threshold for the LTV case is
found such that the rate at which false alarms occur does not
exceed once per every 50 seconds of run time. Next consider
an attack model satisfying (13)-(19), with α equal to −1 and

12

[17] S. Weerakkody, B. Sinopoli, S. Kar, and A. Datta, “Information ﬂow
for security in control systems,” in 55th IEEE Conference on Decision
and Control (CDC), Dec 2016, pp. 5065–5072.

[18] Y. Mo, E. Garone, A. Casavola, and B. Sinopoli, “False data injection
attacks against state estimation in wireless sensor networks,” in 49th
IEEE Conference on Decision and Control (CDC), Dec 2010, pp. 5967–
5972.

[19] Y. Mo and B. Sinopoli, “Integrity attacks on cyber-physical systems,”
in Proceedings of the 1st International Conference on High Conﬁdence
Networked Systems, ser. HiCoNS ’12. New York, NY, USA: ACM,
2012, pp. 47–54.

[20] C. Kwon, W. Liu, and I. Hwang, “Security analysis for cyber-physical
systems against stealthy deception attacks,” in 2013 Annual American
Control Conference (ACC), June 2013, pp. 3344–3349.

[21] N. Hashemi and J. Ruths, “Generalized chi-squared detector for lti
systems with non-gaussian noise,” in 2019 Annual American Control
Conference (ACC), July 2019, pp. 404–410.

[22] C. Murguia and J. Ruths, “CUSUM and Chi-squared Attack Detection
of Compromised Sensors,” in 2016 IEEE Conference on Control Appli-
cations (CCA), Sep 2016, pp. 474–480.

[23] D. Umsonst and H. Sandberg, “Anomaly Detector Metrics for Sensor
Data Attacks in Control Systems,” in 2018 Annual American Control
Conference (ACC), June 2018, pp. 153–158.

[24] S. Jajodia, A. K. Ghosh, V. Swarup, C. Wang, and X. S. Wang, Mov-
ing target defense: creating asymmetric uncertainty for cyber threats.
Springer Science & Business Media, 2011, vol. 54.

[25] A. M. Teixeira, I. Shames, H. Sandberg, and K. H. Johansson, “Reveal-
ing stealthy attacks in control systems,” in 2012 50th Annual Allerton
Conference on Communication, Control, and Computing (Allerton), Oct
2012, pp. 1806–1813.

[26] M. A. Rahman, E. Al-Shaer, and R. B. Bobba, “Moving target defense
for hardening the security of the power system state estimation,” in
Proceedings of the First ACM Workshop on Moving Target Defense, ser.
MTD ’14. New York, NY, USA: ACM, 2014, pp. 59–68.

[27] J. Tian, R. Tan, X. Guan, and T. Liu, “Hidden moving target defense
in smart grids,” in Proceedings of the 2nd Workshop on Cyber-Physical
Security and Resilience in Smart Grids, ser. CPSR-SG’17. New York,
NY, USA: ACM, 2017, pp. 21–26.

[28] J. Giraldo, A. A. C´ardenas, and R. G. Sanfelice, “A moving target
defense to detect stealthy attacks in cyber-physical systems,” in 2019
Annual American Control Conference (ACC), July 2019, pp. 391–396.
[29] A. Kanellopoulos and K. Vamvoudakis, “Switching for unpredictability:
A proactive defense control approach,” in 2019 Annual American
Control Conference (ACC), July 2019, pp. 4338–4343.

[30] S. Weerakkody and B. Sinopoli, “Detecting integrity attacks on control
systems using a moving target approach,” in 54th IEEE Conference on
Decision and Control (CDC), Dec 2015, pp. 5820–5826.

[31] C. Schellenberger and P. Zhang, “Detection of covert attacks on cyber-
physical systems by extending the system dynamics with an auxiliary
system,” in 56th IEEE Conference on Decision and Control (CDC), Dec
2017, pp. 1374–1379.

[32] M. Ghaderi, K. Gheitasi, and W. Lucia, “A novel control architecture
for the detection of false data injection attacks in networked control
systems,” in 2019 Annual American Control Conference (ACC), July
2019, pp. 139–144.

[33] P. Grifﬁoen, S. Weerakkody, and B. Sinopoli, “An optimal design of a
moving target defense for attack detection in control systems,” in 2019
Annual American Control Conference (ACC), July 2019, pp. 4527–4534.
[34] Y. Mo and B. Sinopoli, “Secure control against replay attacks,” in
2009 47th Annual Allerton Conference on Communication, Control, and
Computing (Allerton), Sept 2009, pp. 911–918.

[35] S. Weerakkody, Y. Mo, and B. Sinopoli, “Detecting integrity attacks
on control systems using robust physical watermarking,” in 53rd IEEE
Conference on Decision and Control (CDC), Dec 2014, pp. 3757–3764.
[36] B. Satchidanandan and P. R. Kumar, “Dynamic Watermarking: Active
Defense of Networked Cyber-Physical Systems,” Proceedings of the
IEEE, vol. 105, no. 2, pp. 219–240, 2017.

[37] P. Hespanhol, M. Porter, R. Vasudevan, and A. Aswani, “Dynamic
watermarking for general lti systems,” in 56th IEEE Conference on
Decision and Control (CDC), Dec 2017, pp. 1834–1839.

[38] B. Satchidanandan and P. Kumar, “Defending cyber-physical systems
from sensor attacks,” in International Conference on Communication
Systems and Networks. Springer, 2017, pp. 150–176.

[39] P. Hespanhol, M. Porter, R. Vasudevan, and A. Aswani, “Statistical
watermarking for networked control systems,” in 2018 Annual American
Control Conference (ACC), June 2018, pp. 5467–5472.

[40] J. Rubio-Hernan, L. De Cicco, and J. Garcia-Alfaro, “Event-triggered
integrity attacks,” in

watermarking control
to handle cyber-physical
Nordic Conference on Secure IT Systems. Springer, 2016, pp. 3–19.

[41] W. H. Ko, B. Satchidanandan, and P. R. Kumar, “Theory and im-
plementation of dynamic watermarking for cybersecurity of advanced
transportation systems,” in 2016 IEEE Conference on Communications
and Network Security (CNS), 2016, pp. 416–420.

[42] Y. Mo, R. Chabukswar, and B. Sinopoli, “Detecting integrity attacks
on scada systems,” IEEE Transactions on Control Systems Technology,
vol. 22, no. 4, pp. 1396–1407, July 2014.

[43] M. Hosseini, T. Tanaka, and V. Gupta, “Designing optimal watermark
signal for a stealthy attacker,” in European Control Conference (ECC),
June 2016, pp. 2258–2262.

[44] Y. Mo, S. Weerakkody, and B. Sinopoli, “Physical authentication of
control systems: Designing watermarked control inputs to detect coun-
terfeit sensor outputs,” IEEE Control Systems Magazine, vol. 35, no. 1,
pp. 93–109, Feb 2015.

[45] B. Satchidanandan and P. R. Kumar, “On the design of security-
guaranteeing dynamic watermarks,” IEEE Control Systems Letters,
vol. 4, no. 2, pp. 307–312, April 2020.

[46] P. Hespanhol, M. Porter, R. Vasudevan, and A. Aswani, “Sensor
switching control under attacks detectable by ﬁnite sample dynamic
watermarking tests,” arXiv preprint arXiv:1909.00014, 2019.

[47] O. Ozel, S. Weerakkody, and B. Sinopoli, “Physical watermarking
for securing cyber physical systems via packet drop injections,” in
2017 IEEE International Conference on Smart Grid Communications
(SmartGridComm), Oct 2017, pp. 271–276.

[48] S. Weerakkody, O. Ozel, and B. Sinopoli, “A bernoulli-gaussian physical
watermark for detecting integrity attacks in control systems,” in 2017
55th Annual Allerton Conference on Communication, Control, and
Computing (Allerton), Oct 2017, pp. 966–973.

[49] R. M. Ferrari and A. M. Teixeira, “Detection and isolation of replay
attacks through sensor watermarking,” IFAC-PapersOnLine, vol. 50,
no. 1, pp. 7363–7368, 2017.

[50] A. M. Teixeira and R. M. Ferrari, “Detection of sensor data injection
attacks with multiplicative watermarking,” in European Control Confer-
ence (ECC), June 2018, pp. 338–343.

[51] R. Romagnoli, S. Weerakkody, and B. Sinopoli, “A model inversion
based watermark for replay attack detection with output tracking,” in
2019 Annual American Control Conference (ACC), July 2019, pp. 384–
390.

[52] T. Anderson, An Introduction to Multivariate Statistical Analysis, ser.

Wiley Series in Probability and Statistics. Wiley, 2003.

[53] G. Grimmett and D. Stirzaker, Probability and random processes, 3rd ed.

Oxford university press, 2001.

[54] P. Billingsley, Probability and Measure, 3rd ed. Wiley, 1995.
[55] D. R. Brillinger, Time series: data analysis and theory.

Siam, 1981,

vol. 36.

APPENDIX

This section outlines the relevant background in statistics
used in the paper and provides a few longer equations removed
from proofs for readability.

A. Statistical Background

First, we provide inequalities for functions of random vari-

ables using the following three theorems.

Theorem A.1. Let (ai)s
then

i=1 be a ﬁnite set of random variables

P (

s
i=1 ai > ǫ) ≤

s
i=1

P

ai > ǫ
s

.

Proof. Assume ai < ǫ

P

P

(cid:1)
s ∀i. This would imply that
s
ǫ
s = ǫ.
i=1 ai <

s
i=1

(cid:0)

Therefore,

P

P

{

s
i=1 ai > ǫ} ⊆

s
i=1

ai > ǫ
s

P

S

(cid:8)

(cid:9)

(96)

(97)

(98)

Furthermore,
P (

s

i=1 ai > ǫ) ≤ P

P

≤

s
i=1
P

s
(cid:0)S
i=1

≤

ai > ǫ
s
ai > ǫ
(cid:8)
s

.
(cid:9)(cid:1)

(99)

where the ﬁrst inequality comes from the inclusion of the
P
events and the ﬁnal inequality comes from Boole’s Inequality.
(cid:4)

(cid:0)

(cid:1)

Theorem A.2. Let (ai)s
then

i=1 be a ﬁnite set of random variables

P (

s
i=1 |ai| > ǫ) ≤

s
i=1

P

|ai| > ǫ

1
s

.

(100)

Q
Proof. Assume |ai| < ǫ

1

(cid:16)
s ∀i. This would imply that

P

(cid:17)

(101)
The remainder of the proof follows closely to Theorem A.1. (cid:4)
Q

s = ǫ.

Q

s
i=1 |ai| <

s
i=1 ǫ

1

Theorem A.3. Let a and b be random variables then for ǫ, γ >
0 we have

P (|ab| < ǫ) ≥ P (|a| < γ) + P

Proof. Note that

|b| < ǫ
γ

(cid:16)

(cid:17)

− 1.

(102)

P(|ab| < ǫ) ≥ P

{|a| < γ} ∩

|b| < ǫ
γ

(103)

since |a| < γ and |b| < ǫ/γ implies |ab| < ǫ. By expanding
the RHS of (103) using inclusion exclusion and bounding the
union term by 1, we get

n

o(cid:17)

(cid:16)

P(|ab| < ǫ) ≥ P(|a| < γ) + P

|b| < ǫ
γ

− 1.

(cid:16)

(cid:17)

(104)

(cid:4)

It is often helpful to split a probabilistic limit into com-
ponents of the underlying random variable. While this is not
possible for all cases, we provide sufﬁcient conditions here.

Theorem A.4. Given sequences of random variables ai and
bi, and constants a and b, suppose that

p-lim
i→∞

ai + bi = a + b and p-lim
i→∞

ai = a

then

p-lim
i→∞

bi = b.

(105)

(106)

Proof. Assume (105) holds. Given an ǫ > 0, we have that

P (kbi − bk > ǫ) ≤ P
+ P

kai − ak > ǫ
2

(cid:0)

kai − a + bi − bk > ǫ
2

+

(107)

(cid:1)

where the inequality comes from triangle inequality and
Theorem A.1. Since both terms in this upper bound converge
to zero, their sum, must as well. Therefore, (106) must hold. (cid:4)

(cid:0)

(cid:1)

Similarly we can combine probabilistic limits as follows.

Corollary A.5. Consider sequences of random variables ai
and bi and constants a and b. If

13

(109)

then

ai + bi = a + b.

p-lim
i→∞
i = −ai, a′ = −a, b′

Proof. Let a′
i = ai + bi and b′ = a + b.
Note, (108) implies (105) is satisﬁed. Therefore, using Theo-
rem A.4

p-lim
i→∞

(ai + bi) = p-lim
i→∞

b′
i = b′ = a + b.

(110)

(cid:4)

Since many of the limits in this paper deal with the average
outer product of random vectors, it
is important to know
how and when these limits converge. The following theorem
provides sufﬁcient conditions for convergence.

Theorem A.6. Consider the sequences of vectors (fi)∞
i=1 and
(gi)∞
i=1 where fi ∼ N (0s×1, Σf,i) and gi ∼ N (0t×1, Σg,i).
Let η and ǫ be scalar values such that 0 < η < ∞ and ǫ > 1.
If

kE[fjf ⊺
∀ i, j ∈ N, then

i ]k, kE[gjg⊺

i ]k, kE[fjg⊺

i ]k < η

ǫ|i−j| ,

p-lim
i→∞

1
i

i

j=1 fjg⊺

j − E[fjg⊺

j ] = 0s×t.

(111)

(112)

P

Proof. For (112) to hold, each of the element must also
converge to 0 with probability 1. Therefore we will consider
an arbitrary element and show it converges using an inequality
derived from Chebyshev’s inequality. Selecting the element in
an arbitrary row m and column n such that 0 ≤ m ≤ s and
0 ≤ n ≤ t, let

h⊺
m =

01×(m−1)

1 01×(s−m)

(113)

and

(cid:2)

h⊺
n =

01×(n−1)

1 01×(t−n)

(cid:3)

,

(114)

ρi = 1
i

(cid:2)
i
j=1 h⊺

then the sum for this single element can be written as
mfig⊺

i hn − h⊺
m
In order to use Chebyshev’s inequality we must ﬁrst bound the
second moment of ρi. We start by expanding ρ2
i using (115)
and canceling like terms to get

E[fjg⊺

j ]hn.

(115)

P

(cid:3)

m

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

=

i
j=1

(116)

E[ρ2
i ]

k hn]+

mfkg⊺
.

j hnh⊺
k ]hn

1
i2
− h⊺
P
m

mfjg⊺
E[fkg⊺

i
k=1
E[fjg⊺
P

E[h⊺
j ]hnh⊺
Expanding the expectation in the ﬁrst term using [55, Equation
2.3.8] and once again canceling like terms results in
E[fkg⊺
k ]hnh⊺
m
.
k ]hn

i
k=1 h⊺
m
k ]hmh⊺
Distributing the norm across the addition and multiplication
using triangle inequality and the sub-multiplicative property
of the 2 norm we then get the upper bound

1
i2
+ h⊺
P
m

E[fjf ⊺
P

E[fjg⊺

E[gjg⊺

j ]hn+

E[ρ2
i ]

(117)

i
j=1

=

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

n

E[ρ2
i ]

≤ 1
i2
× kE[fkg⊺
(cid:12)
(cid:12)

i
j=1

i

k=1 khmk2khnk2kE[fjg⊺

j ]k + khmk2khnk2kE[fjf ⊺
P

P

k ]k×
k ]k kE[gjg⊺

k ]k.

(118)

p-lim
i→∞

bi = b and p-lim
i→∞

ai = a

(108)

(cid:12)
(cid:12)

14

Applying the bounds in (111) and the fact that khmk =
khnk = 1 we can further upper bound resulting in

E[ρ2
i ]

≤ 1
i2

i
j=1

i
k=1

2η2
ǫ2|j−k|

Furthermore,
E[ρ2
i ]

(cid:12)
(cid:12)

≤ 1
i2

(cid:12)
(cid:12)

P

i
j=1

∞
k=1

P
4η2
ǫ2k = 4η2

i(1− 1

ǫ2 ) .

(119)

(120)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

where the inequality comes from the summation in (120)
containing all of the summands in (119) and the fact that all
summands are non-negative.

P

P

Finally, using this bound and applying Chebyshev’s Inequal-
ity [54, Equation 5.32] we have that, for an arbitrary choice
of β > 0,

P (|ρi| > β) ≤

E[ρ2
i ]
β2 =

4η2
iβ2(1− 1

ǫ2 ) .

(121)

Therefore, ρi converges to 0 with probability 1. Since the
matrix element was chosen arbitrarily, (112) must hold. (cid:4)

As a direct result of Theorem A.6, we can also make similar
claims for Gaussian sequences that have been multiplied by
bounded linear transformations.

i=1 and (gi)∞

Corollary A.7. Consider a pair of sequences of vectors
(fi)∞
i=1 where fi ∼ N (0s×1, Σf,i) and gi ∼
N (0t×1, Σg,i). Furthermore, consider the sequences of time
i=1, where Ti ∈ Rs′×s and
varying matrices (Ti)∞
Ui ∈ Rt′×t. Assume that

i=1 and (Ui)∞

kTik ≤ ηT and kUik ≤ ηU .

(122)

Let η, ǫ ∈ R such that 0 < η < ∞ and ǫ > 1. If
i ]k < η

i ]k, kE[gjg⊺

i ]k, kE[fjg⊺

ǫ|i−j| ,

kE[fjf ⊺
∀ i, j ∈ N, then

(123)

p-lim
i→∞

1
i

i

j=1 Tjfjg⊺

j U ⊺

j − ETj

P

fjg⊺
j

U ⊺

j = 0s′×t′.

(cid:2)

(cid:3)

(124)

Proof. We prove this result by showing that the bounded linear
transform generates new sequences that satisfy the conditions
described in Theorem A.6. Let

i = Tifi ∀i and g′
f ′
i ∼ N (0s′×1, TiΣf,iT ⊺
i ) and g′

(125)
i = Uigi ∀i
i ∼ N (0t′×1, UiΣg,iU ⊺
i ).

then f ′
Furthermore, we have that
jf ′⊺

kE[f ′

i ]k ≤ kTjkkTikkE[fjf ⊺

i ]k < η2

T η
ǫ|i−j|

(126)

where the ﬁrst inequality comes from the submultiplicative
property of the spectral norm and the second from applying
(123) and (122). Similarly,

and kE[f ′

jg′⊺

i ]k < ηU ηT η
ǫ|i−j| .

(127)

kE[g′

jg′⊺
Let η′ = max{η2

i ]k < η2
U η
ǫ|i−j|
U η, η2
i ]k, kE[g′

T η, ηU ηT η} and ǫ′ = ǫ then
i ]k < η′
jg′⊺

jg′⊺

jf ′⊺

kE[f ′

i ]k, kE[f ′
which satisﬁes the conditions for using Theorem A.6 which
implies that

(128)

ǫ′|i−j|

i
j=1 f ′

jg′⊺

j − E[f ′

jg′⊺

j ] = 0s′×t′ ,

(129)

p-lim
i→∞

1
i

P

which completes the proof since
j ] = Tjfjg⊺

j − E[f ′

jg′⊺
f ′

jg′⊺

j U ⊺

j − TjE[fjg⊺

j ]U ⊺
j .

(130)

(cid:4)

To use Theorem A.6 and Corollary A.7, we provide sufﬁ-
cient conditions for a Gaussian sequence to satisfy conditions
(111) and (123).

Theorem A.8. Consider the Gaussian process

ai+1 = Miai + bi

(131)

where a0 = 0s×1 and bi are independent gaussian distributed
random vaiables such that bi ∼ N (0s×1, Σb,i). If ∃ǫ1, ǫ2 such
that kMik < ǫ1 < 1 and kΣb,ik < ǫ2 < ∞ ∀ i then
kE [aja⊺

(132)

where η = ǫ2
1−ǫ2
1

and ǫ = 1
ǫ1

ǫ|i−j] ,

i ]k < η
.

Proof. Consider the LHS of (132) when i = j. We can expand
ajaT

j using (131) iterativley to get
kE[aja⊺
j ]k =
i=1 Mj−1 . . . Mj−i+1Σb,j−iM ⊺
j

=

j−i+1 . . . M ⊺

j−1

P

(cid:13)
(cid:13)
(cid:13)

We upper bound this norm as follows

kE[aja⊺

j ]k ≤

<

j
i=1 kMj−1k . . . kMj−i+1k×
× kΣb,j−ik kM ⊺
P
i=1 ǫ2ǫ2(j−1)
i

j−i+1k . . . kM ⊺

≤ ǫ2
1−ǫ2
1

1

,

j−1k

.
(133)
(cid:13)
(cid:13)
(cid:13)

(134)

P

where the ﬁrst inequality comes from applying triangle in-
equality and the sub-multiplicative property of the spectral
norm and the second inequality comes from applying the
bounds on kMik and kΣb,ik and then bounding the resulting
geometric series.

We now focus on (132) for when i 6= j. Consider the

following which has been expanded using (131)

j ]k = kE[aja⊺
kE[aj+ia⊺
i
k=1 Mj+i−1 . . . Mj+i−k+1bj+i−k)⊺]k.
+
Since E[ajbj+i−k] = 0 ∀ k ≤ i, this simpliﬁes to

j+i]k = kE[aj(Mj+i−1 . . . Mjaj+

(135)

j ]k = kE[aja⊺
= kE[aja⊺

j+i]k
j ]M ⊺
(136)
where the inequality comes from (134) and kMik < ǫ1. (cid:4)

j+i−1k < η
ǫi ,

j . . . M ⊺

P
kE[aj+ia⊺

Next, we show that, when α being equal to 0, the full system

state satisﬁes the conditions of Theorem A.8.

Theorem A.9. Consider an attacked LTV system satisfying
the dynamics in (13)-(19) and the attack model in (22)-(23).
Assume the attack scaling factor α is equal to 0. Then ∃ η > 0
and ǫ > 1 such that

xn
¯δn
ˆδn
ξn

xn+i
¯δn+i
ˆδn+i
ξn+i

⊺



















E 
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)















(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

< η
ǫi .

(137)

Proof. We prove this result using Theorem A.8. First note that
using (13)-(19), (22)-(23), and assuming α = 0 we can write

xn+1
¯δn+1
ˆδn+1
ξn+1













= Mn 

xn
¯δn
ˆδn
ξn

en
wn
zn
ζn
ωn



















+ Tn





(138)





15

where the inequality comes from triangle inequality and the
equality comes from the matrix bnb⊺
n being singular. This
implies that

P

1
i

i−1

n=0 b⊺

nbn

> ǫ

≥ P

1
i

i−1
n=0 bnb⊺
n

> ǫ

(cid:16)(cid:12)
(cid:12)
(cid:12)

P

(cid:17)

(cid:12)
(cid:12)
(cid:12)

(cid:16)(cid:13)
(cid:13)
(cid:13)

P

(cid:13)
(cid:13)
(cid:13)

Since the LHS of (148) converges to zero as i → ∞ as a
result of our assumption, the RHS must do so as well which
directly implies the LHS of (146) holds.

.
(148)
(cid:17)

where

and

¯An BnKn BnKn
0p
0p
0p

0p
An
0p

An
0p
0p

Mn = 





0p
0p
−LnCn
¯An







Now assume that the LHS of (146) holds. Then since

(139)

1
i

i−1
n=0 b⊺

nbn = tr

1
i

i−1
n=0 bnb⊺
n

,

(149)

and for the matrix to converge it must also converge element-
(cid:4)
wise, we have that the RHS of (146) also holds.

P

P

(cid:16)

(cid:17)

Tn = 

Bn
0p×r
Ip
0p×q −Ip −Ln
0p
0p×q
0p
0p×q

0p×r
0p×r
0p×r −Ln
0p×r
0p×r




Let ǫ1 = max{ηA1, ηA2} then

0p
0p
0p
Ip



.





(140)

kMnk < ǫ1 < 1

(141)

since the eigenvalues of upper block diagonal matrices are the
set of eigenvalues of the block elements on the diagonal and
k ¯Ank < ηA1 < 1 and kAnk < ηA2 < 1. Furthermore, denote

bn = Tn

n w⊺
e⊺
n

z⊺
n

n ω⊺
ζ⊺
n

then bn ∼ N (0, Σb,n) where

(cid:2)

(142)

⊺

,

(cid:3)

0q×p
Σe
0p×q Σw,n
0r×q
0r×q
0p×q

0q×r
0p×r
0r×p Σz,n
0r
0r×p
0p×r
0p

0q×p
0q×r
0p
0p×r
0r×p
0r
Σζ,n
0r×p
0p×r Σω,n

Σb,n = Tn











T ⊺
n .

(143)







Since Bn, Ln, Σe, Σw,n, Σz,n, Σζ,n, and Σω,n are all bounded
we have that kΣb,nk < ǫ2 for some 0 ≤ ǫ2 < ∞. Denoting

Next, we show that if conditions such as (C2) do not hold,
linear transforms of the limit also do not converge to zero
given the conditions in the following lemma hold.
Lemma A.11. Consider a family of matrices Rn ∈ Rt×s
with full column rank. Assume there exists η ∈ R such that
0 < η ≤ λn, where λn is the smallest eigenvalue of RT
n Rn.
Furthermore, consider a sequence of random vectors fn ∼
N (0s×1, Σf ) such that Σf,n is positive semi-deﬁnite. If

∞

i=1 kE[fnf ⊺

n+i]k < ∞ ∀n

and

then

P

p-lim
i→∞

1
i

P

i−1
n=0 fnf ⊺

n 6= 0s,

p-lim
i→∞

1
i

i−1
n=0 Rnfnf ⊺

n R⊺

n 6= 0t.

(150)

(151)

(152)

Proof. (Lemma A.11) Assume that (150)-(151) holds, but

P

p-lim
i→∞

1
i

i−1
n=0 Rnfnf ⊺

n R⊺

n = 0t.

Applying Lemma A.10 we have that

P

an =

x⊺
n

¯δ⊺
n

ˆδ⊺
n

⊺

,

ξn

(144)

p-lim
i→∞

1
i

i−1
n=0 f ⊺

n R⊺

nRnfn = 0.

we are able to complete the proof using Theorem A.8.

(cid:2)

(cid:3)

(cid:4)

This implies that

P

Since the asymptotic attack power uses the inner product of
vn while most other limits use outer products, we relate these
limits in the following Lemma.

Lemma A.10. Consider a sequence of random vectors
(bn)∞

n=0 such that bn ∈ Rs.

if and only if

p-lim
i→∞

1
i

i−1
n=0 bnb⊺

n = 0s

P

p-lim
i→∞

1
i

i−1
n=0 b⊺

nbn = 0.

(145)

(146)

P
Proof. Assume that the RHS of (146) holds. Note that

i−1

n=0 bnb⊺

n

1
i

(cid:13)
(cid:13)
(cid:13)

P

≤ 1
i

(cid:13)
(cid:13)
(cid:13)

i−1

n=0 kbnb⊺

nk = 1
i

i−1
n=0 b⊺

nbn.

(147)

P

P

p-lim
i→∞
n fn ≤ λnf ⊺

η
i
P
n fn ≤ f ⊺

i−1
n=0 f ⊺

n fn = 0

since ηf ⊺
nRnfn. Since the limit is
not affected by the constant η, and using Lemma A.10, this
(cid:4)
contradicts (151). Therefore, (152) must hold.

n R⊺

B. Ommited Equations

The following equations were ommited from the proof of

Theorem III.11 to improve readability. First, note that

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

" 

P

×

1≤j≤n
j6=m′

¯A(n−1,n−j+1)ωn−j

×

!

¯A(n+i−1,n+i−k+1)ωn+i−k

1≤k≤n+i
k6=m′

P

⊺

!

=

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(153)

(154)

(155)

 
hP
=

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
P

≤

and

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

" 

×

16

=

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯A(n−1,n−j+1)Σω,n−j ¯A⊺

(n+i−1,n−j+1)

1≤j≤n
j6=m′
P
∞
j=1 η2j−4+i

A1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
P

,

≤

ηω = ηi−2

(156)

A2 ηω
1−η2
A1
where the ﬁrst equality comes from evaluating the expectation,
the inequality comes from distributing the norm using triangle
inequality and the subadditivity of the spectral norm, bounding
the individual terms, and allowing the summation to include
j = m′ and go to inﬁnity, and the ﬁnal inequality comes from
evaluating the summation. Using similar reasoning, we also
have

E

n−1
j=0

n+i−1

k=0 A(n−1,j+1)Ljζj ζ⊺

k L⊺

kA⊺

(n+i−1,k+1)

n−1
P

j=0 A(n−1,j+1)LjΣζj L⊺
j=0 η2(n−1−j)
n−1
ηi
A2η2
P

j A⊺
Lηζ ≤ ηi

A2

(n+i−1,j+1)
A2η2
1−η2

Lηζ
A2

.

≤

(cid:13)
(cid:13)
(cid:13)

=

i(cid:13)
(cid:13)
(cid:13)
(157)

Cn

n
j=1 A(n−1,n−j+1)Ln−jCn−j ×

P
n
k=j+1
k6=m′

P

¯A(n−j−1,n−k+1)ωn−k

×

!

×

Cn+i

n+i
j=1 A(n+i−1,n+i−j+1)Ln+i−jCn+i−j×

P
¯A(n+i−j−1,n+i−k+1)ωn+i−k

×

n+i
k=j+1
k6=m′

P
n
j=1

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n+i
ℓ=1 CnA(n−1,n−j+1)Ln−jCn−j×

P
×

P
n
k=max{j+1,ℓ+1}
k6=m′
P
(n+i−ℓ−1,n−k+1)C⊺
× ¯A⊺

n+i−ℓL⊺

n+i−ℓ×

¯A(n−j−1,n−k+1)Σω,n−k×

⊺

!

=

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

× A⊺

(n+i−1,n+i−ℓ+1)C⊺

n+i

≤

×

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
η2k−j−ℓ−2+i
A

ηω ≤

≤

n
j=1
×

P

n
ℓ=1 η4

C η2

Lηℓ+j−2

A

n
P
k=max{j+1,ℓ+1}
k6=m′

≤ ηi−4

P
C η2
A η4

Lηω2

∞
j=1

∞
ℓ=j

∞

k=ℓ+1 η2k = 2ηi

C η2
Aη4
Lηω
(1−η2
A)3

.
(158)

P

P

P

 
