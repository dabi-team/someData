1
2
0
2

p
e
S
3
2

]

R
C
.
s
c
[

1
v
2
4
5
1
1
.
9
0
1
2
:
v
i
X
r
a

ADVERSARIALuscator: An Adversarial-DRL based
Obfuscator and Metamorphic Malware Swarm
Generator

Mohit Sewak
Microsoft R&D, India
mohit.sewak@microsoft.com

Sanjay K. Sahay, Hemant Rathore
BITS Pilani, Goa, India
{ssahay, hemantr}@goa.bits-pilani.ac.in

Abstract

Advanced metamorphic malware and ransomware, by using ob-
fuscation, could alter their internal structure with every attack.
If
such malware could intrude even into any of the IoT network, then
even if the original malware instance get detected, by that time it
can still infect the entire network. The IoT era also required Industry
4.0 grade AI based defense against such advanced malware. But AI
algorithm need a lot of training data, and it is challenging to obtain
training data for such evasive malware. Therefore, in this paper, we
present ADVERSARIALuscator, a novel system that uses specialized
(adversarial) deep reinforcement learning to obfuscate malware at the
opcode level and create multiple metamorphic instances of the same.
To the best of our knowledge, ADVERSARIALuscator is the ﬁrst-
ever system that adopts the Markov Decision Process based approach
to convert and ﬁnd a solution to the problem of creating individual
obfuscations at the opcode level. This is important as the machine
language level is the least at which functionality could be preserved
so as to mimic an actual attack eﬀectively. ADVERSARIALuscator
is also the ﬁrst-ever system to use eﬃcient continuous action control
capable deep reinforcement learning agents like the Proximal Policy
Optimization in the area of cyber security. Experimental results indi-
cate that ADVERSARIALuscator could raise the metamorphic prob-
ability of a corpus of malware by ≥ 0.45. Additionally, more than

1

 
 
 
 
 
 
33% of metamorphic instances generated by ADVERSARIALuscator
were able to evade even the most potent IDS and penetrate the tar-
get system, even when the defending IDS could detect the original
malware instance. Hence ADVERSARIALuscator could be used to
generate data representative of a swarm of very potent and coordi-
nated AI based metamorphic malware attack. The so generated data
and simulations could be used to bolster the defenses of an IDS against
an actual AI based metamorphic attack from advanced malware and
ransomware.

1

Introduction

Industry 4.0 aims to proliferate networks of edge devices and autonomous
control. Such a scenario is to the greatest advantage of the malware de-
velopers, especially the developers of metamorphic malware. Metamorphic
malware are the second generation, advanced malware that could alter their
internal structure using techniques like obfuscation after each attack [1]. Even
if the original malware instance is known or detected a while later, its still
challenging to detect, locate and quarantine all its metamorphic instances of
the malware in the network. Threat scenario becomes even more challenging
if the metamorphic instances are from diﬀerent original malware or a cross
between other known malware.

For any traditional, or even classical Machine Learning (ML) based Intru-
sion Detection Systems (IDS) [2, 3], even a single such metamorphic attack
scenario is akin to an extreme threat scenario arising from multiple simul-
taneous zero-day attack and has the potential of compromising the entire
network and the Factory/Industry setup. Though it may be possible to use
advanced Deep Learning (DL) based IDS [4, 5], but such complex DL based
IDS need a lot of labeled data for training before they can detect the obfus-
cated instance generated by a metamorphic malware. Though, there exist
synthetic adversarial data generators that could generate perturbations at
the binary level that can evade a DL based IDS [6–10]; but training against
the data generated by such systems could potentially cause more harm to the
IDS instead of incrementally training them against metamorphic instances
of known malware [9]. This is because the lowest level at which an actual
code level obfuscations could be identically represented and the ﬁle function-
ality could be identify-ably preserved is at the assembly language (sequence

2

of opcodes) level. But owing to the length and the possible combination of
opcodes that makes a unique malware, to simulate a metamorphic malware
generator even at the opcode level requires solving a very complex Markov
Decision Process (MDP) with very high cardinality action-space. To the best
of our knowledge, there exists no system that claims to generate AI driven
obfuscations at the opcode level.

Therefore, in this paper we present a novel system named ADVER-
SARIALuscator, for Adversarial Deep Reinforcement Learning based
obfuscator and Metamorphic Malware Swarm Generator, that could solve
such a complex MDP to generate swarms of metamorphic malware obfus-
cated at the opcode level. Earlier in our initial work named DOOM [11] we
solved this MDP which has limited unique DRL agents that could not be used
to propagate a swarm attack. On the other hand, ADVERSARIALuscator
is a matured, multi-agent implementation of a complete metamorphic swarm
generation system. The metamorphic malware swarm data generated by
ADVERSARIALuscator and the simulations obtained from the MDP envi-
ronment we created can help bolster defenses of any (even networked) IDS
against metamorphic malware swarm attacks. The ADVERSARIALuscator
system can not only be used for on online IDS, but could also scan ﬁles,
directories and databases.

The remaining of the paper is organized as follows. In section 2 we cover
the background and motivation, whereas in section 3 we discuss the related
work on several aspects of ADVERSARIALuscator. Next, in section 4, 5 and
6 we cover the details of the architecture, the custom RL environment and
the DRL agent for ADVERSARIALuscator. Then, in section 7 we describe
the results of the various experiments carried out on the setup as described.
We discuss the implications of the achieved results in section 8 and ﬁnally
conclude the paper in section 9.

2 Background and Motivation

In this section we cover the background and motivation related to diﬀerent
aspects of obfuscation and ADVERSARIALuscator.

3

2.1 Obfuscation vs. Binary Perturbations

As opposed to generating features with perturbations of binary sequences in
the network payload that could makes a malicious payload be classiﬁed as
non-malicious, the ADVERSARIALuscator system creates obfuscations of
actual malware at the opcode level. Doing so the ADVERSARIALuscator
system not only mimics an actual metamorphic malware (at feature vector
level) which an existing system is more likely to actually encounter than ran-
dom perturbations but is also compatible with Intrusion Detection Systems
(IDS) that could also be used for malware detection at a ﬁle system level and
provide many other beneﬁts as we discuss next.

Instead of working at the binary sequence level, many advanced IDS,
work at a more semantically informative opcode sequence level to be more
eﬀective in identifying such advanced threats. If suﬃcient training data is
available, some IDS could even create defence against existing instances of
Metamorphic malware as well [4].

Since most of ML and DL techniques work on the implicit or explicit
assumption of identifying patterns in a speciﬁc distribution of training data,
therefore if the inference (new detection candidate samples) data distribution
is signiﬁcantly diﬀerent from that of their training data, these techniques may
not only provide an ambiguous classiﬁcation, but in some cases may also
provide an incorrect classiﬁcation. In the context to an IDS this mean that
the IDS may not only fail to detect the incoming malware as a threat, or at
least as doubtful (and thus warranting additional static or dynamic analysis),
but may even pass it as a non-malicious sample. Obfuscation could create
such potent metamorphic malware samples which may seemingly come from
a diﬀerent (desirably non-malicious) distribution and may evade even most
of the advanced ML/ DL based IDS.

The IDS’ that work with binary-sequence based features, could be easily
evaded by data generated by creating synthetic perturbations at a binary
level [6–10]. Such generated perturbations/ noise added to a malicious binary
data could create synthetic samples, which to an IDS seem to have come
from a non-malicious binary network traﬃc payload and hence corrupt the
IDS’ detection. The intention behind such mechanism is that the samples
that evade the IDS may be used to re-train the IDS to augment its defenses
against a new malware or a new instance (supposedly obfuscated) of an
existing malware. The underlying implicit assumption being that some of the
samples created by these perturbations may have features that are identical

4

to that of an actual new malware or an obfuscation of an existing malware,
and hence re-training with this additional data will have a monotonically
desirable impact on the performance (deﬁnitely increase or at least will not
deteriorate the performance) of the IDS.

These implicit assumptions may not only prove to be unfounded some-

times, but may even be counter intuitive at other times. This is because:

• As indicated in [9] these methods, especially the ones based on gradient-
attack based techniques (like GANs) make re-training of IDS ineﬀec-
tive.

• Such generated perturbations may not represent an actual opcode/
instruction and their insertion point in the binary sequence may not
correspond to a logical start or a logical end of an opcode. Such mech-
anism generate synthetic binary sequences and insert them in a man-
ner that may not represent a similar one generated from the compiler
for an original ﬁle (malicious or non-malicious). Hence despite claims
of functionality-preservation, synthetic data generated by such systems
may never actually represent an actual malware that a developer would
create and which can infect a system.

• Moreover since each executing processor architecture understand dif-
ferent set of opcode and each opcode may produce a very unique
set of binary sequence, so despite claims in the respective papers of
functionality-preservation, the random sequence of binary data pro-
duced may not even be comprehend-able by the target executing pro-
cess architecture.

• Lastly when the IDS is trained with so much data from a distribution
that may not represent features of a likely obfuscated malware that
the IDS would probably face in real deployment, it may even make the
IDS over-ﬁt [12] or reduce the proportion of actual malware sample
data from its training, thus reducing its eﬀectiveness against real new
inference data.

Another critical drawback with the entire approach is that it is entirely
dependant on retraining IDS’s classiﬁer with synthetic data for improving the
IDS’s detection capabilities against (new or) metamorphic malware. Whereas
a more potent technique to enable the IDS to detect the obfuscated malware

5

could be to de-obfuscate [13] the malware before sending it (or its extracted
features) to the IDS’s classiﬁer. Classiﬁes if just one of the sub-component
of the IDS. The other sub-components being the feature extractor and op-
tionally a feature transformer, pre-processor etc. As indicated earlier, in
some cases it may be counter-intuitive to re-train a classiﬁer on synthetic
data; therefore sub-components or sub-systems could be created/ trained
that could try to solve the problem of metamorphic malware detection with-
out mandating re-training of the IDS’s classiﬁer.

2.2 Uses of Obfuscations created by ADVERSARI-

ALuscator

The ADVERSARIALuscator system creates metamorphic malware features
at the opcode level. One additional beneﬁt of creating obfuscations at this
level is that these could be used not only to re-train the IDS’s classiﬁer, but
may also be used to augment other sub-components of the IDS as well (like
the feature extractor or pre-processor), thus alleviating the need to tamper
with a classiﬁer that is well trained on actual malware data distribution.
Also ancillary (internal or external to IDS) sub-systems could be created that
could de-obfuscate the obfuscation of a metamorphic malware to normalize it
to its (existing) base malware variant before sending it to the IDS’s classiﬁer
for detection.

Creating a normalized representation of the metamorphic instance of spe-
ciﬁc malware variant is not only important from a cyber-forensic perspective,
but is also critically required for some advanced multi-nominal IDS and other
similar systems that also detect the variant of the malware (besides class)
for further action. Therefore the opcode level obfuscations generated by the
ADVERSARIALuscator system could be used in either of these ways:

• For improving the IDS’s classiﬁer detection by re-training it with train-
ing data additionally augmented with the obfuscated metamorphic in-
stances of existing malware.

• For training/ augmenting other internal sub-systems of the IDS with
capabilities that could de-obfuscate the incoming ﬁle’s extracted fea-
tures before sending it to IDS’s classiﬁer.

• For creating/ training other external sub-systems for normalizing obfus-
cations of diﬀerent variant of existing malware to augment an existing

6

IDS without mandating the IDS to be even aware of their existence.

3 Related Work

In this section we cover the related work on diﬀerent aspects of obfuscation
and ADVERSARIALuscator.

3.1 DRL-Agent as IDS Adversary

One reason why most of the existing perturbation-creation based systems still
work at binary level is that many of these systems work on the basis of ad-
versarial machine learning systems like the Generative Adversarial Networks
(GANs) [7, 9, 10, 14]. In such an adversarial setting, there exists a ‘Discrimi-
nator Network’ (D) which mimics an IDS’s classiﬁer and tries to detect any
new sample created by another ‘Generator Network’ (G) of the system. In
a typical GAN both the D and G networks are comprised of Convolutional
Neural Networks (CNNs) [15], and it is intuitive to transform a binary net-
work traﬃc as a CNN or other similar input layer so it makes it easier to
work at binary sequence level with GAN based adversarial techniques.

Some advanced systems in this category could use Reinforcement Learn-
ing (RL) [12] or even Deep Reinforcement Learning (DRL) [16] agents [8]
to replace the G network. But even these Adversarial RL/DRL based sys-
tems also work at the binary network traﬃc level. The reason for even these
to to work at the binary level could be that most of the popular RL algo-
rithms (like Q Learning) or even DRL algorithms (like Deep Q Networks)
could ﬁnd eﬃcient solutions to only low cardinality action space based RL
tasks. Therefore these systems could only learn a policy comprising of just
a few selected actions that the agent could take. These actions could be like
adding or removing a very speciﬁc sequence of binary instructions. In most
of such systems the sequence of binaries that could be inserted/ deleted is
also either very abstract or otherwise restricted. This further adds bias to
the created data even if the resulting binary sequences remains operation
after such alterations.

But training an Adversarial DRL/RL based agent for this purpose is not
a trivial task. The instruction-set for any architecture may contain thou-
sands of opcodes. Therefore the ‘Markov Decision Process’ (MDP) that
could mimic the obfuscation of a malware holistically at the sophistication

7

level represented by inserting any number of junk instructions in diﬀerent
combination represents a very high cardinality observation and action space.
Solving an MDP require a RL agent that could ﬁnd eﬃcient solution to such
RL task. Hence popular RL or DRL algorithms like the ones employed in
the existing literature in this ﬁeld are not suitable to learn such obfuscation
task at the required sophistication level.

The ADVERSARIALuscator system hence has to use specialized DRL
agents to learn this RL task and create the desired obfuscations. Experimen-
tal results indicate that the obfuscated malware created by ADVERSARIALuscator
could evade (classiﬁed as non-malicious or ambiguous) even the most potent
malware intrusion detection systems and hence could mimic even an extreme
‘zero day attack’ from multiple simultaneous metamorphic malware. To the
best of our information, ADVERSARIALuscator is the ﬁrst system that
takes a Markov Decision Process based approach to convert and ﬁnd a solu-
tion to the problem of opcode level obfuscation, and it is also the ﬁrst ever
system to use continuous action control capable deep reinforcement learning
agents the like Proximal Policy Optimization (PPO) in the area of malware
generation and defense.

The PPO agents acts as (multiple) G networks in the adversarial setup.
For enacting as the D networks, we use the complete IDS (including pre-
processing, feature-selection and transformation) that claimed to provide the
most superior performance (accuracy of 99.21%with an FPR of 0.19%) [4]
over a standardized malware dataset with mixed types and generation of
malware. We also use the associated malware data on which this performance
was achieved [17] to train ADVERSARIALuscator.

3.2 Code/opcode level obfuscation

There have been many attempts to generate obfuscations at the code level
[18], but these are not scalable. Later eﬀorts were also made to use machine
learning models [19] to automate to some extent the obfuscation mechanism.
Most of these methods does not replicate the advance multiple-simultaneous
botnet metamorphic attacks required to train a deep reinforcement learning
adversary. There has been attempt to use CNN based Generative Adversar-
ial Networks (GANs) [7, 9, 10, 14] as well but these constitutes mostly using
supervised training approaches on speciﬁc algorithms. Recently Deep Rein-
forcement Learning (especially Q Learning) has been utilized [8] to alter the
binary code of the ﬁle to evade attacks. Such systems are not only limited

8

to very small action-space problems (here limited to adding some speciﬁc
4-bit code) but even these could not be used to mimic an actual obfusca-
tions (which require a code/opcode level treatment). To the best of our
knowledge our system is the ﬁrst ever to work directly on the numerous of
opcodes (hence requires solving a large-action space reinforcement problem)
to mimic obfuscations that could be used generate data to train any solution
(even deep learning or reinforcement learning based) that works on opcode
frequency features for malware detection.

4 Architecture of the ‘ADVERSARIALusca-

tor’

The architecture of the ADVERSARIALuscator System is shown in ﬁgure 1.
This architecture broadly consists of four subsystems:

1. The opcode repository of the original malware ﬁles (collected from Mali-
cia dataset [17]) and its subsequent obfuscated instances (as produced
by the the ADVERSARIALuscator system) and corresponding meta-
data (like original malware mapping, similarity scores with normalized
malware feature vector etc.).

2. Repository of existing trained IDS to act as the adversary (‘Discrimi-

nant Network’ as in GAN).

3. A custom RL environment (to emulate the MDP for agents to learn

against).

4. The Obfuscating DRL Agent(s).

In ﬁgure 1, the DRL agent interacts with the environment to train against
episodic tasks which can alter the presented opcode frequency. Such alter-
ations could be strictly additive corresponding to a net addition of opcode
instructions thus mimicking junk instruction/ code insertion for obfusca-
tion [20]. The agent may even choose choose to insert opcodes which though
are present in the instruction set of the architecture but not present in the
ﬁle.

During the initialization of an episode the environment fetches a randomly
selected malware variant’s opcode frequency feature vector from the malware

9

feature vector repository. Then the environment scores the obtained feature
vector using the available IDS. The IDS acts as an adversary and provides
the the (initial) non-malicious (prediction) probability (Pnon-malicious) of the
selected malware variant. This is stored in the metadata to compute the
reduction in Pnon-malicious in each episode by the corresponding agent. The
environment then serves this feature vector (called observation as in RL) to
the agent to propose an action. The action proposed by the agent is the
sequence index of the opcode whose frequency it proposes to be increased,
thus representing a real-life action of inserting new opcodes/ instructions to
the existing malware.

The environment in each training step subsequently invokes the avail-
able IDS to obtain the probability of the generated opcode frequency vector
(modiﬁed as per the agent’s ‘action’) to have come from a ‘Non-Detectable
Malware File’ (NDMF) ﬁle (thus representing a successful obfuscation of
malware to non-malicious) (PNDMF = 1 − PDMF, where DMF stands for
‘Detectable Malware File’). A decent increment in PNDMF from the initial
state (most of the malicious ﬁles as used in this system had a PDMF ≈ 1.0,
(limPDMF→0) by the IDS to a level where the IDS’s classiﬁer could not disam-
biguate the ﬁle instance is considered as the ﬁrst success criteria. The second
success criteria being taking the PDMF to the other extreme so that the IDS
starts classifying the resulting feature vector as to come from a non-malicious
ﬁle with very high probability. The DRL agents trains over multiple such
training episodes to update and reﬁne an action-policy to obfuscate any mal-
ware variant to achieve both or at-least the ﬁrst success criteria.

Multiple such DRL agents could be instantiated and trained with varying
degree of dissimilarity from the other DRL agents so that each could learn
to obfuscate a given malware using a slightly diﬀerent action-policy and sub-
sequently producing multiple dissimilar metamorphic instances of the same
malware variant. There are diﬀerent methods to introduce such dissimilari-
ties. Some of these methods could range from one extreme of changing the
complete underlying algorithm of the DRL agent, to other extreme where
a the dissimilarities could be achieved by simply changing random number
seed of various instances of the same DRL agent. The system thus creates
the required collection of multiple metamorphic instances that could mimic
a very powerful and sophisticated attack on high value assets like defence
installations, nuclear plants, strategic ﬁnancial systems, and government as-
sets. Therefore such created obfuscations could represent an ideal dataset
to be used for creating a defensive mechanism and solution to avert such

10

11

Figure 1: Agent Training Process Flow

sophisticated attacks on similar high value assets.

4.1

safeOpcodeObfuscation Mechanism

There are many malware creation tool-kits available in public domain, which
are already being used to create very potent malware variants. Such tool-
kits do not use advanced AI to create virtually undetectable obfuscation,
whereas the ADVERSARIALuscator system is capable of generating such
sophisticated obfuscations virtually undetectable even with multiple ML/ DL
detection systems. Hence the ADVERSARIALuscator system or an imple-
mentation of the same in the wrong hand could potentially create huge neg-
ative impact. Therefore to obviate such actions we have purposely designed
the process such that the obfuscation part of the system would work only at
the opcode level and could not be used to create a malicious executable.

Despite it could have been possible to use the extracted opcode sequence
as-is instead of frequency vector. By opting for the later we have kept the
obfuscation process purposely intractable in this system such that it works
on abstraction of extracted opcode frequencies (instead of actual opcode se-
quences). If the original opcode sequence is not archived then the frequencies
once extracted could not be re-sequenced to create a valid assembly (.asm)
ﬁle so as to generated an malicious executable.

Since the objective of the ADVERSARIALuscator system is to strengthen

the cyber-defense, we have taken the extra precautions that ADVERSARIALuscator
(at least without major post-processing or reconﬁguration) could not be used
by cyber-attack-perpetrators to defeat the very objective and use ADVERSARIALuscator
to generate a ﬂurry of multiple simultaneous ‘zero day’ metamorphic malware
attack which the existing IDS may not be able to defend against.

4.2

functionalityPreserving Metamorphosism

As reasoned in section 1, most of the systems that work at binary level,
despite their respective claims, cannot produce a metamorphic malware (or
even claim to produce an obfuscated malware) that mimics an actual program
compiled from a real compiler and that is compatible with the downstream
execution architecture, or is even compatible with its instructions.

ADVERSARIALuscator instead works at the opcode level, identiﬁes the
malware at the instruction level and also create obfuscations at the instruc-
tion level. The functionality of a given program is indicated by the sequence

12

of the instructions available in its assembly produced by the compiler. As ex-
plained in section 4, ADVERSARIALuscator only inserts junk instructions
and does not remove the existing to preserve the functionality even at the
instruction level. Doing so ADVERSARIALuscator does not only preserves
the intended functionality of the program, but even risks that it could be
used to create an actual malware. Therefore despite the desired outcome of
functionality preservation, we had to also the safety mechanism as described
in section 4.1 to ensure that where for all the theoretical purposes of the
claims and experiments the functionality is preserved, in practice it remains
intractable.

5 Custom Reinforcement Learning Environ-

ment

The environment serves a major role in reinforcement learning. Here its role
is to present a current-state of the opcode frequency vector of the malware ﬁle
from the repository to the agent to act upon, and then give it an appropriate
reward and the corresponding next-state. The next-state here represents the
resultant opcode frequency vector after changing the speciﬁc opcode´s fre-
quency as suggested by the agent in the current step. The next-state for the
current-step becomes the current-state for the next-step in the environment.
The current-state, action, reward, next-state (S-A-R-S) cycle contin-
ues until a terminal-state is reached (for an episodic task) or until a (pa-
rameterized) predeﬁned number of steps are exhausted. On reaching such
scenario, the environment resets and re-instantiates itself with the opcode
frequency vector of a randomly chosen new malware ﬁle from the reposi-
tory and resets other necessary variables like initial PDMF, turns completed,
total episode reward, total discounted episode reward, is complete (current
episode completion) ﬂag as in the algorithm 2. From the ‘reset’ to the episode
completion, the environment responds in each step as given in algorithm 3.

5.1 The structure of ‘State’

The state in our experiment comprises of a vector of whole numbers corre-
sponding to the set of opcode frequency for a given ﬁle. We use the same
unique opcode set as used by Sewak et. al. [4]. We also use the same IDS
which they claimed to have produced the best performance and in their

13

work. Their IDS claimed an accuracy of 99.21%with an False Positive Rate
of 0.19% on the Malicia dataset [17] which is the best performance available
in literature on any standardized malware dataset. This dataset along with
the collected non-malicious ﬁles for the work resulted in a set of 1612 unique
opcodes (instruction set). Correspondingly we have a state comprising of
1612 dimension ‘Box’ Space with a permissible range of [0, 10000] ∈ Z1612.
The ‘Box’ Space is an ‘Open AI’s ‘Gym’ [21] compatible API reference class
for deﬁning the state for a Reinforcement Learning environment such that it
could be used against any standardized setup/ agent.

5.2 The design of ‘Action’

The ‘action’ in our reinforcement learning problem is the speciﬁc opcode in-
dex that needs to be altered in a given step of an episode. We used a large
cardinality discrete action control capable agent. The agent determines which
opcodes’ frequency should be altered and in which direction it should be al-
tered (increased or decreased). The magnitude of alteration could be a ﬁxed
constant or a trainable parameter. In this approach we have Nobservation × 2
actions, the ﬁrst N actions corresponding to a an increase in the speciﬁc
opcode frequency by a constant Cincrement, and the next N actions represent
an act of decreasing the corresponding opcode frequency by a ﬁxed amount
Cdecrement, where Cincrement, Cdecrement ∈ N.
In our implementation we have
kept Cincrement = Cdecrement = 5. Also since from the perspective of obfus-
cation, the easiest way of creating multiple obfuscation often increases the
opcode frequency by adding junk code, instructions etc. [20]. Therefore, to
mimic this eﬀect we allow the agent’s action only a net increase in individual
opcode frequency from their initial level (as in original malware). An action
with a net eﬀect of decreasing an individual opcode frequency below its orig-
inal level results in returning the the same state as before the action and a
commensurate reward.

5.3 The ‘Reward’ function

How and what the agent learns is to a considerable degree dependant upon
the reward/ penalty criteria and the magnitude thereof. Our ﬁrst objec-
tive is that the agent should learn to alter the opcode frequency so as to
substantially enhance the probability that the opcode frequency feature vec-
tor belongs to an NDMF (considers an non-malicious) instead of to a DMF

14

(malicious) ﬁle.

For a binomial classiﬁer the unbiased cut-oﬀ threshold for separating the
two class is midway i.e. the 0.5 probability mark as in equation 1. That is
given: opcode ∈ W1612 :

P(opcodeﬁle | f ile ⊆ {non-malicious ﬁles})

= P(opcodeﬁle | f ile ⊆ {malicious ﬁles}) = 0.5 (1)
Owing to the IDS’ established performance, most of the malicious ﬁles’ op-
codes have a Pnon-malicious ≈ 0.0 to start with. We penalize any resulting
opcode frequency feature vector that has predicted Pnon−malicious ≤ 0.5 and
reward the ones with Pnon−malicious ≥ 0.5 proportionally. That is in each step
the reward given to the agent is:

Given: Pnon-malicious = P(opcodeﬁle | f ile ⊆ {non − maliciousf iles}) We

have reward: R = Pnon-malicious − 0.5.

But this reward criteria when in isolation has a drawback that it en-
courages long trajectories resulting in greater positive rewards instead of
favoring a shorter trajectory that quickly reaches a very high Pnon-malicious.
One way to ensure that distant rewards are not favored much is to increase
the ‘discounting-factor’ (γ). But since the γ could only be altered by agent’s
algorithm and is not in the scope of the environment’s, so the reward mech-
anism cannot use lowering the discounting-factor enough to ensure that high
instantaneous-rewards are more favorable than lower cumulative-discounted-
rewards. So to overcome this drawback, we have another (instantaneous)
reward component. This reward component is given by the environment to
the agent (in addition to the one stated above), when the agent to manage to
alter the opcode frequency enough such that the ﬁle is almost unambiguously
classiﬁed as non-malicious (second objective). This reward is high enough
to easily surpass even multiple cumulative (even discounting given γ < 1)
rewards. This criteria occurs when the Pnon-malicious ≥ Pthreshold. Where,
Pthreshold is a high/ threshold probability of non-malicious (say 0.90). There-
fore, as a step function now the reward can be given as equation 2 below.

reward =






Pnon-malicious − 0.5,

Rgoal,

if Pnon-malicious
≤ Pthreshold
otherwise

(2)

Where Rgoal could either be constant or a variable dependant upon the max-
imum steps allowed in the episode. Each new episode starts from the reset

15

Figure 2: UML Diagram for the ‘ObfuscationEnvironment’ Class

of environment. On reset, a random malware ﬁle’s opcode is retrieved and
the episode continues until the goal is reached (achieving high non-malicious
probability threshold) or exhausting the maximum permissible steps for the
particular episode. Here we set Rgoal = Max Permissible Steps in an episode
so that we could balance the needs for setups that have larger episode lengths
to allow for slow convergence of complex agents with too many optimize-able
parameters and have max permissible step adaptive Rgoal such that it is al-
ways greater than any cumulative reward over even a long episode. Alterna-
tively, or additionally, we could give a large penalty (negative reward) if the
max permissible steps is exhausted without the agent managing a Rgoal. We
did not use this second mechanism in our experiment.

5.4 The custom ‘ObfuscationEnvironment’ design and

algorithm

Combining the above three aspects of the environment as covered in the
previous sub-sections, namely the ‘State’ (5.1), the ‘Action’ (5.2), and the
‘Reward Function’ (5.3, we create a class for the malware obfuscation envi-
ronment, the UML design of the same is given in ﬁg 2.

The two main role and corresponding methods of any custom environment
are to reset an episode (using the ‘reset()’ method) and to take the action

16

init

suggested by the agent and in return deliver an appropriate ‘reward’ and the
subsequently changed state as the result of the action. This is done using the
(python magic function to instantiate
the ‘step(action)’ action. The
a class) instantiates the environment instance with custom parameters to
train a unique DRL agent instance. Besides the environment parameters
the agent’s approximation function (DL model), and training conﬁguration
(epochs, batch size, weight initialization etc.) also determines the uniqueness
of the thus trained agent. Besides these three methods for whose algorithm
the pseudo-code is given below, there are other helper and utility methods
deﬁned in our custom environment implementation, but since these are not
essentially required for a standard out-of-box reinforcement learning agent,
we do not provide the pseudo-code of these here. The pseudo-code of the
initialization method is given as 1, for the reset method is given as 2, and for
the step method is given as 3.

6 Proximal Policy Optimization Agent Algo-

rithm

Given the constraints as covered in section 5.2 earlier, we have a discrete
action control requirement involving very high cardinality action space. Some
of the most popular DRL agents for discrete action agents like the ‘Deep Q
Networks’ (DQN) [22, 23], ‘Double DQN’ (DDQN) [24], and the ‘Dueling
DQN’ (DDQN) [22] though could manage large state-space, but perform
poorly for large/ continuous action space.

Deterministic Policy Gradient (DPG) [25] based DRL approaches like
the ‘Deep Deterministic Policy Gradient’ (DDPG) [26] claims to deliver the
best in class performance on RL tasks involving large, and even continuous
action-space. The problem with such algorithms is that their line search
based policy gradient update (used during optimization) either generates
too big updates (which for updates involving non-linear trajectory makes
the update go beyond the target) or makes the learning too slow. Since
in the DRL paradigm non-linear gradients are quite common so algorithms
based on line search gradient update does not prove very robust and cannot
‘Trust Region
provide guarantees of near monotonic policy improvements.
Policy Optimization’ (TRPO) [27] an algorithm based on Trust-Region based
policy update using ‘Minorize-Maximization’ (MM) (second order) Gradient

17

Algorithm 1 init () method
Input:
NT = max allowed turns in any episode: int,
NO = cardinality of unique opcode set: int,
IncrementStep = any given opcode change step:int,
RFsuccess = reward for successful completion of an episode: ﬂoat,
RFthreshold = reward function parameter for minimum non-malicious
probability threshold: ﬂoat,
RSeed = random number seed for environment to instantiate the
environment: int,
γ = discounting factor for future rewards: ﬂoat
ClassMalwareDetect = Malware Prediction System Class: Object
Output:
None

procedure init
values of the parameters to train a unique (DRL) agent instance

(cid:46) Initialize the Environment Class with appropriate

max turns ← NT
cardinality of observation space ← NO
cardinality of action space ← NO
f requency increment step ← IncrementStep
reward f or successf ul obf uscation ← RFsuccess
threshold non − malicious prediction probability ← RFthreshold
seed ← RSeed
discounting f actor ← γ
F unctionP redict ← predict method of ClassM alwareDetect
is current episode completed ← T rue
Set random number seed for implementation platform as seed

18

Algorithm 2 reset() method
Input:
None
Output:
array of opcode frequency: int[0...NO]

procedure reset

(cid:46) Resets the environment for a new episode to begin

Get the opcode of a random malware ﬁle from the malware repository
malware f ilename ← ﬁlename of the malware ﬁle
initialobservation ← vector of opcode f requency
turns completed ← 0
is current episode completed ← F alse
total episode reward ← 0.0
total discounted episode reward ← 0.0
return (initial observation)

(cid:46) used in performance evaluation
(cid:46) for plotting and evaluation

Algorithm 3 step() method
Input:
action index : int
Output:
reward : ﬂoat
new state: int[0...NO]

procedure step (cid:46) Step within an ongoing episode. Implements the action suggested
by the agent and gives the corresponding reward and the new state to the agent.

if is current episode completed = True then

reset()

new observation ← current observation
new observation[action index] ← new observation[action index] + IncrementStep
benign probability ← F unctionP redict(current observation)
reward ← benign probability − 0.5
total episode reward ← γ ∗ total episode reward + reward
total discounted episode reward ← γ ∗ total discounted episode reward + reward
total turns played ← total turns played + 1
(cid:46) for plotting and evaluation
current observation ← new observation
if benign probability ≥ RFthreshold then
is current episode completed ← T rue
store the new observation in obfuscated malware achieve
compute similarity of new observation array with the initial observation array

return (reward, new observation)

19

update claims to solve this problem and provide guarantee for near monotonic
general (stochastic) policy improvement even for non-linear policies like that
approximated by (deep) neural networks.

Additionally TRPO uses a mechanism called ‘Importance Sampling’ to
compute the expectancy of policy from previous trajectories instead of only
the current trajectory to stabilize policy gradient. This method has an un-
derlying assumption that the previous trajectory’s distribution (Q(x)) is not
very diﬀerent from the current trajectory’s distribution (P (x)).

The policy gradient for a Stochastic Policy Gradient [28] method and

associated algorithms like Actor Critic [29] looks like below:

∇θ(Jθ) = Eτ ∼πθ(τ )[∇θ log πθ(τ )r(τ )]

(3)

In equation 3, the trajectory τ over which the samples for computing ex-
pectancy is gathered (to update the gradient ∇ of the policy-value-function
J), is also the same (current) trajectory of the policy π (parameterized over
θ) as used in the update.

But in the case of TRPO using importance sampling and the past trajec-
tory for sampling, this policy-value-function update looks as below equation
4.

∇θ(cid:48)(Jθ(cid:48)) = Eτ ∼πθ(τ )[

∇θ(cid:48) log πθ(cid:48)(

T
(cid:88)

t=1

t(cid:48)=1

t
(cid:89)

πθ(cid:48)
πθ

)(

T
(cid:88)

t(cid:48)=t

r)]

(4)

In the case of TRPO, the second order gradient update computation is very
expensive, and hence for real size-able tasks it is seldom use.

We use the PPO DRL agent algorithm (PPO) [30]. PPO is an improve-
ment over TRPO [27]. The PPO [30] algorithm works similar to TRPO and
is much easier to compute as it uses a linear-variant of the gradient update
called the ‘Fisher Information Matrix’ (FIM). In equation 4, the trajectory is
being sampled from the policy as it existed in previous time-step (πθ = Q(x)),
whereas the expectancy over such collected samples are used to update the
policy at next time-step (πθ(cid:48) = P (x)). When the ratio of expectancy over
the two trajectory distributions vary signiﬁcantly P (x)
Q(x) as in the case of linear
gradient update in PPO, the previously stated assumption may not hold,
leading to high variance in policy updates. To avoid this there are two meth-
ods that the PPO algorithm recommends. The ﬁrst one use a ‘Adaptive KL
Penalty’ and the second one use ‘Objective Clipping’.

The recommended value of the clipping-factor ((cid:15)) in the ‘Objective Clip-
ping’ is (cid:15) = 0.2 [30]. In the ‘Objective Clipping’ mechanism if the probability

20

ratio between the two trajectory’s policies is not in the range [(1 − (cid:15)), (1 + (cid:15))]
the ‘estimated advantage’ is clipped. The DL function approximator that we
use for the PPO algorithm’s actor and critic network comprise of 2 hidden
layers each, with each hidden layer having 64 neurons and a tanh activation.

7 Experimental Results

Generating metamorphic malware using obfuscation as opcode level resulted
in a high-cardinality action-space MDP, that the PPO algorithm based DRL
agent was able to learn an eﬃcient policy to solve.

The resulting trained agents could obfuscate most of the malware and
uplift their metamorphic (Pnon-malicious) probability of miss-classiﬁcation error
to a substantial degree to fail or evade even the best IDS which were even
trained using the corresponding original malware variants.

Figure 3: Training Statistics for a sample Agent

As shown in ﬁgure 3, the mean PN DM F was uplifted by ≥ 0.45 (from
almost 0.0) indicating that the IDS could not eﬀectively detect them as

21

malicious unambiguously anymore. Also as shown in ﬁgure 4 almost 1/3rd
samples achieved a PN DM F ≥ 0.5 (more likely to be non-malicious rather
than malicious). The probability density function (PDF) distribution is left
skewed, indicating that most of the samples were eﬀectively obfuscated by
the agent.

Another interesting observation is on the nature of opcode similarity be-
tween the original malware variants and their obfuscated version of the same.
We use Pearson product-moment correlation coeﬃcients between the opcode
vectors to generate this similarity. The correlation is taken from the corre-
lation matrix R, whose relationship with the co-variance matrix C, which is
as given in the equation below:

Rij =

Cij
(cid:112)Cii ∗ Cjj

(5)

Figure 4: opcode Similarity for a sample Agent

In ﬁgure 4:

22

Figure 5: Training Statistics for Agent - 2

23

• The ﬁrst sub-plot shows that the resultant metamorphic instance’s op-
code frequency feature with that of the original non-obfuscated variant
across episodes and also overlays the resulting PN DM F of the resulting
metamorphic vector.

• The second plot (histogram) indicates that the opcode frequency vec-
tor for the obfuscated variant is very similar to the original malware
variant.

• The third plot is a similar histogram as sub-plot-2, but ﬁltered for ob-
fuscations that achieved a metamorphic probability > 0.5. It indicates
that even for episodes where a high PN DM F was achieved, the resulting
opcode frequency vector is not very diﬀerent from that of the original
malware variant.

8 Discussion

The insights from these observations are very critical as it indicates that
with minimal eﬀorts (adding additional opcode instructions using methods
like junk-code insertion etc.) new obfuscations with very high metamorphic
probability could be achieved with our system. Another important observa-
tion is that even with a net additive obfuscation the resulting metamorphic
feature vector is very similar to that of the original and hence the resultant
ﬁle-size (a function of total instructions) would also be very similar. This
observation is important as many advanced IDS, besides featuring multiple
classiﬁers, also has an unsupervised routing sub-component that helps im-
prove the IDS’ performance by routing the candidate ﬁle to the appropriate
classiﬁer based on the unsupervised sub-components routing. This routing
sub-component could use ML techniques like clustering [31], or could even
use simple heuristics based on ﬁle-size [32].
If the ﬁle-size (for heuristic
based) or opcode similarity (for clustering based) changes substantially then
the resulting IDS may route the metamorphic variant to a diﬀerent classier
with which the evasion probabilities may not hold as high as indicated the
lab settings. But having a high feature (and hence ﬁle-size) similarity, the
resulting obfuscations from the ADVERSARIALuscator are much likely to
be routed to the same classiﬁer of an IDS, and hence the evasion probabilities
would not be impacted.

24

Figure 6: opcode Similarity for Agent - 2

25

9 Conclusion

We designed and developed a novel system that uses specialized (adversarial)
deep reinforcement learning to obfuscate malware at the opcode level and cre-
ate multiple metamorphic instances of the same and named it ADVERSAR-
IALuscator. To the best of our knowledge, ADVERSARIALuscator is the
ﬁrst-ever system that adopts the Markov Decision Process based approach to
convert and ﬁnd a solution to the problem of creating individual obfuscations
at the opcode level. This is important as the machine language level is the
least at which functionality could be preserved so as to mimic an actual at-
tack eﬀectively. ADVERSARIALuscator is also the ﬁrst-ever system to use
eﬃcient continuous action control capable deep reinforcement learning Exper-
imental results indicate that ADVERSARIALuscator could raise the meta-
morphic probability of a corpus of malware by ≥ 0.45. Additionally, more
than 33% of metamorphic instances generated by ADVERSARIALuscator
were able to evade even the most potent IDS and penetrate the target sys-
tem, even when the defending IDS could detect the original malware instance.
Hence, ADVERSARIALuscator could be used to generate a swarm of very
potent metamorphic malware. Such an advanced attack could eﬀectively
mimic an extreme ‘zero-day attack’ from multiple simultaneous metamorphic
malware and hence can create ideal data required to bolster the defences of
an IDS against an actual and potential AI based malware attack. However,
we believe that incorporating some of the very recent developments in Multi-
Agent Reinforcement Learning (MARL) can lead to the development of an
even more potent system with capabilities of coordinated swarms of malware,
and the work in this direction is in progress.

References

[1] Yanfang Ye, Tao Li, Donald Adjeroh, and S Sitharama Iyengar. A survey
on malware detection using data mining techniques. ACM Computing
Surveys (CSUR), 50(3):1–40, 2017.

[2] Hemant Rathore, Sanjay K Sahay, Ritvik Rajvanshi, and Mohit Sewak.
Identiﬁcation of signiﬁcant permissions for eﬃcient android malware de-
tection.
In International Conference on Broadband Communications,
Networks and Systems, pages 33–52. Springer, 2020.

26

[3] Hemant Rathore, Sanjay K Sahay, Shivin Thukral, and Mohit Sewak.
Detection of malicious android applications: Classical machine learning
vs. deep neural network integrated with clustering. In International Con-
ference on Broadband Communications, Networks and Systems, pages
109–128. Springer, 2020.

[4] Mohit Sewak, Sanjay K Sahay, and Hemant Rathore. Comparison of
deep learning and the classical machine learning algorithm for the mal-
ware detection. In IEEE/ACIS SNPD, pages 293–296. IEEE, 2018.

[5] M. Sewak, Sanjay Sahay, and H. Rathore. An overview of deep learn-
ing architecture of deep neural networks and autoencoders. Journal of
Computational and Theoretical Nanoscience, 17:182–188, 2020.

[6] Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca,
Giorgio Giacinto, Claudia Eckert, and Fabio Roli. Adversarial malware
binaries: Evading deep learning for malware detection in executables.
CoRR, abs/1803.04173, 2018.

[7] Hemant Rathore, Sanjay K Sahay, Piyush Nikam, and Mohit Sewak.
Robust android malware detection system against adversarial attacks
using q-learning. Information Systems Frontiers, pages 1–16, 2020.

[8] D. Wu, B. Fang, J. Wang, Q. Liu, and X. Cui. Evading machine learn-
ing botnet detection models via deep reinforcement learning. In IEEE
International Conference on Communications, pages 1–6, 2019.

[9] Weiwei Hu and Ying Tan. Generating adversarial malware examples for

black-box attacks based on GAN. CoRR, abs/1702.05983, 2017.

[10] Zilong Lin, Yong Shi, and Zhi Xue.

IDSGAN: generative adversar-
ial networks for attack generation against intrusion detection. CoRR,
abs/1809.02077, 2018.

[11] Mohit Sewak, Sanjay K Sahay, and Hemant Rathore. Doom: a novel
adversarial-drl-based op-code level metamorphic malware obfuscator for
the enhancement of ids. In ACM International Conference on Pervasive
and Ubiquitous Computing (UbiComp), pages 131–134, 2020.

27

[12] Hyrum S. Anderson, Anant Kharkar, Bobby Filar, David Evans, and
Phil Roth. Learning to evade static PE machine learning malware mod-
els via reinforcement learning. CoRR, abs/1801.08917, 2018.

[13] Vadim Kotov and Michael Wojnowicz. Towards generic deobfuscation

of windows API calls. CoRR, abs/1802.04466, 2018.

[14] Muhammad Usama, Muhammad Asim, Siddique Latif, Junaid Qadir,
and Ala I. Al-Fuqaha. Generative adversarial networks for launching and
thwarting adversarial attacks on network intrusion detection systems.
International Wireless Communications & Mobile Computing Confer-
ence, pages 78–83, 2019.

[15] Mohit Sewak, Md. Rezaul Karim, and Pradeep Pujari. Practical Convo-
lutional Neural Networks: Implement Advanced Deep Learning Models
Using Python. Packt Publishing, 2018.

[16] Mohit Sewak. Deep Reinforcement Learning - Frontiers of Artiﬁcial

Intelligence. Springer, 2019.

[17] Antonio Nappa, M Zubair Raﬁque, and Juan Caballero. The malicia
identiﬁcation and analysis of drive-by download operations.

dataset:
International Journal of Information Security, 14(1):15–33, 2015.

[18] Jean Borello and Ludovic M´e. Code obfuscation techniques for meta-
morphic viruses. Journal in Computer Virology, 4(3):211–220, 2008.

[19] Priti Desai and Mark Stamp. A highly metamorphic virus generator.

IJMIS, 1:402–427, 2010.

[20] Chandan Kumar Behera and D Lalitha Bhaskari. Diﬀerent obfuscation
techniques for code protection. Procedia Computer Science, 70:757–763,
2015.

[21] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,
John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR,
abs/1606.01540, 2016.

[22] Volodymyr Mnih et al. Human-level control through deep reinforcement

learning. Nature, 518(7540):529–533, February 2015.

28

[23] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioan-
nis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari
with deep reinforcement learning. CoRR, abs/1312.5602, 2013.

[24] Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement

learning with double q-learning. CoRR, abs/1509.06461, 2015.

[25] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra,
and Martin Riedmiller. Deterministic policy gradient algorithms.
In
International Conference on Machine Learning (ICML), pages 387–395.
PMLR, 2014.

[26] Timothy P. Lillicrap et al. Continuous control with deep reinforcement

learning. CoRR, abs/1509.02971, 2015.

[27] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and
Pieter Abbeel. Trust region policy optimization. In ICML, 2015.

[28] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Man-
sour. Policy gradient methods for reinforcement learning with function
approximation. In International Conference on Neural Information Pro-
cessing Systems, pages 1057–1063. MIT Press, 1999.

[29] Volodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex
Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray
Kavukcuoglu. Asynchronous methods for deep reinforcement learning.
CoRR, abs/1602.01783, 2016.

[30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and
CoRR,

Proximal policy optimization algorithms.

Oleg Klimov.
abs/1707.06347, 2017.

[31] Hemant Rathore, Sanjay K Sahay, Palash Chaturvedi, and Mohit Sewak.
Android malicious application classiﬁcation using clustering. In Interna-
tional Conference on Intelligent Systems Design and Applications, pages
659–667. Springer, 2018.

[32] Sanjay K Sahay, Ashu Sharma, and Hemant Rathore. Evolution of mal-
ware and its detection techniques. In Information and Communication
Technology for Sustainable Development, pages 139–150. 2020.

29

