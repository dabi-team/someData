Robustness Testing of Data and Knowledge Driven
Anomaly Detection in Cyber-Physical Systems

Xugui Zhou, Maxﬁeld Kouzel, Homa Alemzadeh
University of Virginia, Charlottesville, VA 22904, USA {xz6cz, mak3zaa, ha4d@virginia.edu}

2
2
0
2

y
a
M
3

]

G
L
.
s
c
[

2
v
3
8
1
9
0
.
4
0
2
2
:
v
i
X
r
a

Abstract—The growing complexity of Cyber-Physical Systems
(CPS) and challenges in ensuring safety and security have led
to the increasing use of deep learning methods for accurate
and scalable anomaly detection. However, machine learning
(ML) models often suffer from low performance in predicting
unexpected data and are vulnerable to accidental or malicious
perturbations. Although robustness testing of deep learning mod-
els has been extensively explored in applications such as image
classiﬁcation and speech recognition, less attention has been paid
to ML-driven safety monitoring in CPS. This paper presents the
preliminary results on evaluating the robustness of ML-based
anomaly detection methods in safety-critical CPS against two
types of accidental and malicious input perturbations, generated
using a Gaussian-based noise model and the Fast Gradient Sign
Method (FGSM). We test the hypothesis of whether integrating
the domain knowledge (e.g., on unsafe system behavior) with the
ML models can improve the robustness of anomaly detection
without sacriﬁcing accuracy and transparency. Experimental
results with two case studies of Artiﬁcial Pancreas Systems (APS)
for diabetes management show that ML-based safety monitors
trained with domain knowledge can reduce on average up to
54.2% of robustness error and keep the average F1 scores high
while improving transparency.

Index Terms—adversarial machine learning, safety, resilience,

anomaly detection, cyber-physical system, medical device.

I. INTRODUCTION
Deep learning (DL) methods are increasingly used for
anomaly detection [1]–[3] and attack recovery [4], [5] in
safety-critical CPS, such as medical devices and autonomous
vehicles. The ML-based anomaly detection methods are often
preferred to model-based techniques due to their easier imple-
mentation, powerful capability in capturing the relationship
between input and output or approximating dynamic models
of the control systems, and high accuracy in predicting unseen
data that shares similar features with the training data.

However, the effectiveness of the ML models heavily relies
on the quantity and quality of the training data [6], especially
in safety-critical applications where sufﬁcient and representa-
tive data (e.g., anomaly examples) is difﬁcult or expensive to
collect. This limited data availability impedes the development
of accurate ML models for anomaly detection. Accidental
and malicious perturbations in the ML input may also cause
misclassiﬁcations or incorrect predictions [7], [8] and lead to
catastrophic consequences in safety-critical applications [9].
Further, the complex architectures of DL networks and their
black-box nature lead to a lack of transparency and intractabil-
ity and make them hard to verify [10], [11].

Although the robustness of DL models against adversarial
perturbations has been extensively studied for computer vi-

sion and speech recognition applications, less attention has
been paid to the robustness of anomaly detection models
against perturbations on multivariate time-series data [12],
[13]. Previous works have studied the differences between
adversarial machine learning in CPS vs. cyberspace systems
[14] and the challenges of implementing adversarial attacks
in CPS [8]. However, robustness testing and possible methods
to improve the robustness of safety monitoring have not been
well investigated in CPS.

In this paper, we adopt the adversarial example crafting
techniques to evaluate the robustness of DL-based safety mon-
itors in CPS. We focus on answering two research questions:
• RQ1: How robust are the state-of-the-art ML safety mon-
itors against accidental and malicious input perturbations?
• RQ2: Does the integration of domain knowledge help

with improving the robustness of ML monitors?

Speciﬁcally, we evaluate the robustness of the ML-based
safety monitors in detecting unsafe control actions issued
by a CPS controller in the presence of accidental Gaussian
noise and adversarial perturbations affecting their inputs (sen-
sors and control commands). For the integration of domain
knowledge with ML models, we extract the context-speciﬁc
speciﬁcation of unsafe control actions in a given CPS using
a control-theoretic hazard analysis method [2], [15], [16] and
add the extracted safety properties as a regularization term in
a semantic loss function [17], [18] that guides the training
process. Fig. 1 presents the overall framework.

The main contributions of the paper are as follows:

• Assessing the resilience of ML-based anomaly detection
models against both accidental and malicious perturbations
on multivariate time-series input using a Gaussian-based
noise model and the widely-used fast gradient sign method
(FGSM) for adversarial example generation.

• Integrating domain knowledge into ML monitors as a se-
mantic loss function and evaluating the robustness in com-
parison to two state-of-the-art baseline monitors, Multi-layer
Perceptron (MLP) and Long Short Term Memory (LSTM).
• Evaluating the robustness of the data-driven vs. combined
data and knowledge driven monitors using datasets collected
from two closed-loop Artiﬁcial Pancreas Systems (APS)
with different controllers and patient glucose simulators.
Our results show that the ML monitors with semantic loss
function can reduce on average up to 54.2% of robustness
error and keep average F1 scores high while improving
transparency.

1

 
 
 
 
 
 
(a)

(b)

Fig. 1: (a) Overall Framework. (b) An Example APS Simulation Trace with Safety Monitor.

II. PRELIMINARIES
The core of CPS are controllers that monitor and control
physical processes (e.g., patient’s dynamics) by estimating the
current physical state (e.g., blood glucose (BG) and insulin
on board (IOB)) based on sensor measurements and sending
control commands (e.g., rate of insulin injection) to the
actuators. We consider a safety monitor that is integrated with
a CPS controller (e.g., an APS controller) and observes the
sensor data and control commands to evaluate whether the
control commands issued in a given system context might be
unsafe and lead to hazards and to stop their delivery to the
actuators (see Fig. 1) [2].

A. Machine Learning Based Safety Monitoring

We model the task of detecting an unsafe control action as

(1)

a context-speciﬁc conditional event, as shown below:
yt = p(∃t(cid:48) ∈ [t, t + T ] : xt(cid:48) ∈ Xh|f (Xt), f (Ut))
where, f (·) represents an aggregation function (e.g., average,
Euclidean norm, or regression) over a window of sensor
measurements Xt or control actions Ut. Given the control
action sequence Ut executed under the system state sequence
Xt, an ML-based monitor outputs a binary yt that classiﬁes Ut
to safe or unsafe depending on whether a hazard is expected
to occur within T timesteps from time t. Fig. 1(b) shows an
example simulation trace from monitoring an APS system.

B. Context-Dependent Safety Speciﬁcation

For integrating the domain knowledge on the safety of the
control actions issued by a controller, we adopt an approach
based on the high-level control-theoretic hazard analysis [15]
for speciﬁcation of context-dependent safety requirements [2].
Table I shows an example set of context-dependent safety
speciﬁcations for the APS case study described as Signal
Temporal Logic (STL [19]) formulas. Each formula speciﬁes
the system context (based on controller inputs and estimated
state variables) under which a control action ut is potentially
unsafe and might lead to a safety hazard Hi, if issued by the
controller. These formulas can be also synthesized into logic
to design a rule-based safety monitor solely based on domain
knowledge and are applicable to any controller with the same
functional speciﬁcation.

2

C. Integrating Domain Knowledge with ML

We encode the STL formulas generated for detecting unsafe
control actions as a custom semantic loss function that penal-
izes the ML model during the training process, if the prediction
does not match with any of the unsafe control action formulas.
The new loss function is as follows:

loss = lossex + w

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

yt − I

(cid:32)

(cid:95)

Φh

f (µ(Xt)) |= Φh

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(2)

where lossex is the baseline ML model loss function (e.g.,
cross-entropy loss), w is a weight parameter that determines
the degree that system context and safety speciﬁcation would
interfere with the training process, yt is the output prediction
of the ML model, and I(·) is an indicator function indicating
whether the aggregated values of the estimated state variables
for a measurement window, f (µ(Xt)), satisfy any of the
unsafe control action speciﬁcations Φh listed in Table I.

This approach is generalizable as the safety speciﬁcations
are generated using a high-level hazard analysis method that
is not limited to any speciﬁc CPS. In addition, the semantic
loss function can be applied to any ML model.

III. ROBUSTNESS TESTING
We consider the accidental or malicious perturbations on
ML-based monitor inputs as small changes that cannot be
detected by the current methods for sensor/input error detec-
tion and attack detection, such as invariant detection [20] or

STL Description of Safety Context
(BG > BGT ∧ BG(cid:48) > 0) ∧ (IOB(cid:48) < 0) ∧ u1
(BG > BGT ∧ BG(cid:48) > 0) ∧ (IOB(cid:48) = 0) ∧ u1
(BG > BGT ∧ BG(cid:48) < 0) ∧ (IOB(cid:48) > 0) ∧ u1
(BG > BGT ∧ BG(cid:48) < 0) ∧ (IOB(cid:48) < 0) ∧ u1
(BG > BGT ∧ BG(cid:48) < 0) ∧ (IOB(cid:48) = 0) ∧ u1
(BG < BGT ∧ BG(cid:48) < 0) ∧ (IOB(cid:48) > 0) ∧ u2
(BG < BGT ∧ BG(cid:48) < 0) ∧ (IOB(cid:48) < 0) ∧ u2
(BG < BGT ∧ BG(cid:48) < 0) ∧ (IOB(cid:48) = 0) ∧ u2
(BG > BGT ) ∧ u3
(BG < 70) ∧ ¬u3
(BG > BGT ∧ BG(cid:48) > 0) ∧ (IOB(cid:48) <= 0) ∧ u4
(BG < BGT ∧ BG(cid:48) < 0) ∧ (IOB(cid:48) >= 0) ∧ u4

TABLE I: Context Dependent Safety Speciﬁcations for APS
Rule
Φh
1
2
3
4
5
6
7
8
9
10
11
12
* BGT: BG target value; BG(cid:48) = dBG/dt, IOB(cid:48) = dIOB/dt;
* u1,2,3,4 :decrease_insulin, increase_insulin, stop_insulin, keep_insulin;
* H1 : Too much insulin is infused, which will reduce the BG and might lead
to hypoglycemia; H2 : Too little insulin is infused, which causes the BG to
increase and could lead to hyperglycemia.

Implied
Hazard Type
H2
H2
H2
H2
H2
H1
H1
H1
H2
H1
H2
H1

0200400600800100050100150200(mg/dL)BG1012(Unit,mg/dL/min)Insulin_rateIOB02004006008001000Time (minute)0.00.51.01.5  Monitor AlertsHazardsTo implement the FGSM attacks, the attacker would need
including the model

full access to the target ML model,
structure and parameters.

Black-box Attacks: We also test the robustness of ML
monitors against black-box attacks [30] where the attacker
does not have full access to the ML safety monitor or its model
structure. The attacker’s capabilities are limited to sending
queries to the model and the knowledge of features used by
the model.

Previous studies have shown that adversarial examples can
transfer across ML models with different architectures [28],
[31], [32]. So for a black-box attack, the attacker can ﬁrst
train a substitute model using the input/output data from
the target safety monitor, then generate white-box adversarial
perturbations based on the substitute model, expecting that
they are transferable to the target model.

We use a two-layer MLP (128-64) as the substitute model
and generate the adversarial perturbations using the same
FGSM approach. Similar to white-box FGSM attacks, the
black-box FGSM attacks also target the multivariate input data.

IV. EXPERIMENTAL SETUP AND RESULTS

We used an open-source simulation environment that inte-
grates the closed-loop simulation of two example APS control
systems [2] to evaluate different ML-based safety monitors.
Speciﬁcally,
integrates two widely-used APS controllers
(OpenAPS [33] and Basal-Bolus [34]) with two different
patient glucose simulators, including Glucosym [35] and UVA-
Padova Type 1 Diabetes Simulator [36], simulating 20 differ-
ent diabetic patient proﬁles.

it

We ran the experiments on an x86_64 PC with an Intel Core
i9 CPU @ 3.50GHz and 32GB RAM running Linux Ubuntu
LTS. We used TensorFlow v.2.5.0 to train our ML models.

A. Baseline ML Monitors

We evaluated two ML architectures, the Multi-layer Percep-
tron (MLP) and Long Short Term Memory (LSTM). The MLP
monitor consisted of two fully connected layers, comprising
256 and 128 neutrons, followed by a fully-connected layer
with ReLU activation and a ﬁnal softmax layer to obtain
the hazard probabilities. The LSTM monitor was a two-layer
(128-64) stacked LSTM with an input time step of 6 (i.e.,
30 minutes of data), followed by a fully-connected layer with
softmax activation. We trained both models using the Adam
optimizer with a sparse categorical cross-entropy loss function
and a default learning rate of 0.001.

B. Metrics

We use the following metrics for our experiments:

TABLE II: Confusion Matrix for Sequential Data with Tolerance
Window δ

Ground Truth Positive

t(cid:48)=t−δ(cid:48)
t

PP (cid:80)t
PN (cid:80)t
t(cid:48)=t G(t(cid:48)) > 0
* PP: Predicted positive; PN: Predicted negative; P (t)/G(t): Prediction/Ground truth at time t;

P (t(cid:48)) > 0 && (cid:80)t+δ
P (t(cid:48)) == 0 && (cid:80)t+δ

t(cid:48)=t G(t(cid:48)) == 0

t(cid:48)=t G(t(cid:48)) == 0

t(cid:48)=t G(t(cid:48)) > 0

t(cid:48)=t−δ(cid:48)
t

Ground Truth Negative
P (t) > 0 && (cid:80)t+δ
P (t) == 0 && (cid:80)t+δ

t − δ(cid:48)

t: Start time of a window δ, ending with a positive ground truth, that includes t.

Fig. 2: An Example FGSM Attack on a Baseline Monitor with the
Keep_Insulin Injection Command Issued by the Controller.
change detection techniques (e.g., Cumulative Sum Control
Chart (CUSUM) [8], [21]), but can lead to misclassiﬁcation
results and severe consequences for the CPS being monitored.
For example, an attacker can remotely login to an insulin pump
and change the output control commands [22], or due to a
malfunction, the pump can deliver an incorrect insulin dosage
[23]. If these small perturbations are also sent to the ML-
based monitor, either event could evade the detection by the
monitor [24] and result in severe complications such as hypo-
or hyperglycemia [25] and potential death.

Simulating Environment Noise on Input Data: To gener-
ate small accidental perturbations, we add a randomly gen-
erated error value from a Gaussian distribution with zero
mean and small standard deviation (less than one standard
deviation) to the sensor data. More aggressive deviations might
be easily detected by the existing CPS anomaly detection
techniques [20], [21], [26].

We use Gaussian noise to simulate the environment noise
on input data because (1) Gaussian is a reasonable assumption
for any process or system that’s subject to the Central Limit
Theorem [27] and (2) Gaussian noise is entirely described by
second-order statistics, which are relatively easy to measure.
Further investigation of the generalization of the proposed
methods against a broader range of noises is out of the scope
of this paper.

White-box Attacks: Fast Gradient Sign Method (FGSM)
[12] is a simple but effective method widely used in generating
adversarial images using the gradients of a neural network,
and is reported to be also effective in non-image domain [28].
For a speciﬁc input x and loss function J(·, ·), the adversarial
sample xadv can be generated using the following equations:

xadv = x + ∆x
∆x = (cid:15) ∗ sign(∇x(J(x, ¯y))

(3)

(4)

where ∆x is the generated perturbation on the input x with
label ¯y, and (cid:15) is a small constant parameter that limits the
strength of perturbations at each dimension. The resulting
adversarial input maximizes the loss function using the L∞
norm [29]. Unlike Gaussian noise, which is only applied to
sensor data, we inject FGSM attacks to the multivariate time
series input data (both sensor and control commands).

An example of adversarial input generated using FGSM is
shown in Fig. 2. Despite only a minute change to the input,
the output of the targeted neural network changes from unsafe
with 93.39% conﬁdence to safe with 99.98% conﬁdence.

3

TABLE III: Overall Performance of Each ML Models without Noises.

Simulator

Glucosym
(OpenAPS)

T1DS2013
(Basal-Bolus)

Model
Rule-based
MLP
LSTM
MLP-Custom
LSTM-Custom
Rule-based
MLP
LSTM
MLP-Custom
LSTM-Custom

No. Sim.
8800
8800
8800
8800
8800
8800
8800
8800
8800
8800

No. Sample
1.32E+06
1.32E+06
1.32E+06
1.32E+06
1.32E+06
1.32E+06
1.32E+06
1.32E+06
1.32E+06
1.32E+06

ACC
0.87
0.97
0.98
0.98
0.97
0.61
0.94
0.99
0.96
0.98

F1
0.73
0.89
0.93
0.91
0.86
0.56
0.71
0.95
0.82
0.90

Prediction Accuracy represents the performance of the
ML-based safety monitors in accurate prediction of hazards,
measured using precision, recall, accuracy (ACC), and F1
score calculated using a Sample Level with Tolerance Window
metric [2], where a tolerance window before the start time
of hazard (th) is used for calculation of the metrics. Table II
shows the confusion matrix with a tolerance window.

Prediction Robustness Error measures how robust the ML
model predictions are against the perturbations and is deﬁned
as the number of samples that fail to keep the same predicted
output classes after adding perturbations, over the total number
of samples of each class in the dataset

[37], [38]:

robustness error =

(cid:80)N

i I(fθ(xi) (cid:54)= fθ(xi + ∆x))
j Nj

(cid:80)

(5)

where Nj, N is the total number of samples in class j and
all the classes, respectively, and ∆x is the input perturbation
on the ML model θ. The indicator function I(·) equals one
when the prediction fθ(xi) is not the same as fθ(xi + ∆x).

C. Performance of ML based Safety Monitors

Table. III presents the overall performance of the baseline
Rule-based and ML monitors (MLP and LSTM) and monitors
customized with a semantic loss function based on domain
knowledge (MLP-Custom and LSTM-Custom) in detecting
anomalies in both simulators in the absence of any pertur-
bations. We see that MLP monitors trained with the custom
loss function achieved higher F1 scores than both baseline
MLP monitors and the pure Rule-based monitor, indicating
the advantage of combining domain knowledge and data-
driven techniques. Fig. 3 shows an example of the decision
boundaries learned by the MLP and MLP-Custom monitors.
Although the LSTM models trained with the custom loss
function did not improve the F1 score of baseline LSTM
monitors, they achieved comparable accuracy.

Fig. 4: Example Distributions of Test Dataset with/without Adding
Gaussian Noise N (µ = 0, σ2 = (0.5std)2).

Besides, the integration of domain knowledge also improves
the ML explainability by offering simple rules to check the
output of the ML model.

D. Robustness against Gaussian Noise

An example distribution of the input dataset with/without

Gaussian noises is presented in Fig. 4.

Fig. 5 shows the performance of the ML models (averaged
F1 Score over all the patient proﬁles) in presence of different
levels of input noise (σ = [0.1, 0.25, 0.5, 0.75, 1.0]std) with
both Glucosym and T1DS2013 simulators.

We can see from Fig. 5 that both MLP and LSTM models’
performance decreases after adding Gaussian noises in both
simulators. However, for the Glucosym simulator, the LSTM
monitor has a more signiﬁcant drop in F1 score than the
MLP monitor (36.8% vs. 9.1%), indicating that the LSTM
model is less robust against small input perturbations. For the
T1DS2013 simulator, the LSTM monitor has less drop in F1
score due to the difference in sensor data distribution (see Fig.
4) and the higher percentage of faulty samples (39.3% vs. 33.9
%).

We also observe that the F1 score of the MLP monitor
increases with the rising levels of added noise (higher de-
viations). This might be because of the large number of new
alarms generated due to noisy input that increased the initially
low recall of baseline MLP monitor and also resulted in
decreased precision, as shown in Fig. 6.

(a) Glucosym Simulator with OpenAPS Controller.

Fig. 3: An Example of Decision Boundaries of the Baseline MLP
Monitor (Left) and the MLP-Custom Monitor (Right).

Fig. 5: F1 Score of the ML Models in presence of Gaussian Noise
N (µ = 0, σ2).

(b) T1DS2013 Simulator with Basal-Bolus Controller.

4

100120140160180200220240BG(mg/dL)21012IOB(unit)01100120140160180200220240BG(mg/dL)21012IOB(unit)010501001502002503000500010000150002000025000GlucosymOriginalGlucosymNoisyT1DS2013OriginalT1DS2013Noisy(a) Glucosym Simulator with OpenAPS Controller.

(b) T1DS2013 Simulator with Basal-Bolus Controller.
Fig. 8: F1 Score of each ML Models Against FGSM Attacks.

no memory capacity [39], and the perturbation does not
accumulate over a sequence of input data.

After retraining the baseline monitors with the proposed
custom loss function, we can observe the improvements of
both the MLP monitor and the LSTM monitor in all simula-
tors, demonstrating the effectiveness of the custom loss func-
tion in improving ML models’ robustness against adversarial
attacks. Besides, the LSTM-Custom monitors have higher F1
scores than MLP-Custom monitors due to their advantage in
dealing with time-series data [40]. Therefore, by combining
the advantage of integrating domain knowledge and the beneﬁt
of the LSTM model structure, the LSTM-Custom monitors
maintained the highest F1 scores for both APS systems.

F. Robustness Error Evaluation

We also evaluate the robustness of the ML monitors by
comparing their outputs using the robustness error metric (Eq.
5). Fig. 9 shows the heat-map of each ML model’s robustness
error against the Gaussian noise and FGSM attacks.

We make the following observations:
• Baseline ML monitors are more vulnerable to FGSM
attacks (with larger robustness errors) than simulated en-
vironment noise in the input data in both APS simulators.
• The baseline LSTM monitor is more sensitive to both
FGSM attacks and Gaussian noise due to the disturbance
on both their current sensor data and short memory [39],
[40].

• Almost all the ML models have more signiﬁcant ro-
bustness error against FGSM attacks in the T1DS2013
simulator than in the Glucosym simulator, which might
result from different patient proﬁles and the more straight-
forward controller
(Basal-Bolus [34]) used with the
T1DS2013 simulator.

• The ML models customized with semantic loss functions
have the least robustness error almost in all the situa-
tions and reduce on average up to 22.2% and 54.2% of
the robustness error against Gaussian noise and FGSM
attacks across models and simulators. This further attests
the advantage of the integrating domain knowledge in
keeping the model predictions stable and robust.

Fig. 6: Precession and Recall of MLP Monitors in T1DS2013
Simulator Against Gaussian Noise N (µ = 0, σ2). The MLP model
retrained with the custom loss function is marked with *.

In both simulators,

the monitors retrained with custom
semantic loss functions reduced the performance drop and kept
the F1 score high with noisy input data. This demonstrates the
advantage of the integrated domain knowledge in overcoming
disturbance in the input data as well as the robustness and
generalizability in maintaining stable performance for anomaly
detection with different ML models and different controllers.

E. Robustness against White-box FGSM Attacks

An example of adversarial inputs generated by the white-
box FGSM attack is shown in Fig. 7. Fig. 8 presents the overall
F1 score of each ML model (averaged over all the patients) in
the presence of white-box FGSM attacks with the adversarial
degree (cid:15) ranging from 0.01 to 0.2. We observe that the F1
scores for both the MLP and LSTM baseline monitors drop
with adversarial inputs in both simulators, indicating these
ML models’ incapability to keep stable performance under
adversarial attacks as well as the effectiveness of white-box
FGSM attacks in fooling ML models for anomaly detection.
We also observe that the baseline LSTM monitor has a
relatively larger decrease in F1 score than the baseline MLP
monitor in the T1DS2013 simulator, which might be because
of the differences in the generated adversarial signals (see Fig.
7) and in the neural network architectures between the MLP
and LSTM models. FGSM attacks may have less inﬂuence
on the MLP models since MLPs are model networks with

Fig. 7: Example Input Data of
the MLP and LSTM Models
with/without White-box FGSM Attacks ((cid:15) = 0.2). Each simulation
step equals 5 minutes in the actual system.

5

050010001500MLP, Simulation Steps50100150200BG(mg/dL)OriginalNoisy050010001500MLP, Simulation Steps1012IoB(Unit)OriginalNoisy050010001500LSTM, Simulation Steps50100150200BG(mg/dL)OriginalNoisy050010001500LSTM, Simulation Steps1012IoB(unit)OriginalNoisyFig. 9: Robustness Error of ML Monitors Against Gaussian Noise (Zero Mean and 0.1-1 Standard Deviation) and FGSM Attacks.

of the authors’ knowledge, this paper is the ﬁrst to study
the impact of adversarial attacks on multivariate time series
anomaly detection in CPS.

Adversarial defense methods: The adversarial

training
using the adversarial perturbations on input data is one of the
most commonly used methods to defend ML models against
adversarial examples [44]–[46]. However, it suffers from a
high cost to generate adversarial examples, and it is impossible
to cover all possible adversary techniques that attackers might
utilize [47]. Gradient masking is another common technique
for reducing the ML models’ sensitivity to small input per-
turbations, which works by adding a penalty term to the
original prediction loss to produce near-zero gradients [48],
[49]. However, previous works have shown that this approach
scariﬁes the ML model accuracy [47]. Our work deviates from
these works by integrating domain knowledge with the ML
model as a semantic loss that can improve the ML monitor’s
robustness and transparency while keeping the accuracy and
F1 score high.

VI. CONCLUSION

In this paper, we adapt two strategies to craft accidental
and adversarial perturbations on multivariate time series data
used for anomaly detection in CPS. We also propose a method
to protect ML safety monitors against such perturbations by
integrating domain knowledge through customizing ML mod-
els with semantic loss functions.We evaluate the robustness
of two different ML monitors with and without the proposed
protection using the data collected from two closed-loop
APS systems. Robustness testing of ML monitors showed
large reductions in model performance, despite the models
performing very well on unaltered data. The incorporation
of domain knowledge signiﬁcantly improved robustness when
compared with baseline models without sacriﬁcing accuracy
or transparency. Experimental results showed that baseline
LSTM models were more susceptible to attacks than the
MLP models, but the LSTM models customized with semantic
loss had the least robustness error against all perturbations.
These ﬁndings warrant a more comprehensive investigation
of robustness testing and defense strategies for ML anomaly
detection models in CPS.

ACKNOWLEDGMENT

This work was partially supported by the Commonwealth of
Virginia under Grant CoVA CCI: C-Q122-WM-02 and by the
National Science Foundation (NSF) under Grant No. 1748737.

Fig. 10: Robustness Error of ML Monitors Against Black-box Attacks

G. Robustness against Black-box Attacks

We evaluate the robustness of the ML monitors against
black-box attacks generated using a substitute MLP model.
Fig. 10 shows the robustness error heat-map for the ML
monitors against the black-box FGSM attacks. We see that
the baseline LSTM models keep the robustness error below
0.23, which is 2.04 times less than the robustness error against
white-box FGSM attacks. On the other hand, the baseline MLP
models and customized ML models keep the robustness error
very small. The inclusion of the custom loss function reduced
the robustness error to less than 10% of its original value.

V. RELATED WORK

DNN-based anomaly detection in CPS: Previous efforts
on anomaly detection using Deep Neural Networks have
achieved considerable accuracy with well-tuned parameters or
complex model structures [1], [4], [41]–[43] that suffer from a
lack of generalization and transparency. In this paper, we show
that the integration of the domain knowledge using a semantic
loss function into the anomaly detection model can offer a
way to verify the model predictions and improve transparency
while keeping the accuracy high.

Adversarial attack methods: Generating adversarial exam-
ples to test the robustness of classiﬁers has been an active area
of research [12], [13]. However, most previous works focused
on the image classiﬁcation and speech recognition domains.
Few recent works have adopted these advanced attack methods
in the non-image domain, such as testing regression models
with ﬁnance and power consumption data [28] [32]. There are
also some works looking into attacks speciﬁcally for CPS,
such as testing a universal adversarial method [38] on the
NASA turbofan dataset or crafting adversarial examples for
CPS with sensor constraints [14]. Nevertheless, to the best

6

0.10.250.50.751.0Gaussian Noise (0±std*)MLP-GlucosymMLP-T1DS2013MLP-Custom-GlucosymMLP-Custom-T1DS2013LSTM-GlucosymLSTM-T1DS2013LSTM-Custom-GlucosymLSTM-Custom-T1DS2013Robustness Error0.010.020.040.060.080.000.010.020.030.040.010.020.050.070.090.010.010.030.050.060.030.070.130.180.220.010.030.050.070.100.010.030.060.080.100.010.010.030.050.070.00.10.20.30.40.50.010.050.10.150.2FGSM ()MLP-GlucosymMLP-T1DS2013MLP-Custom-GlucosymMLP-Custom-T1DS2013LSTM-GlucosymLSTM-T1DS2013LSTM-Custom-GlucosymLSTM-Custom-T1DS2013Robustness Error0.000.010.030.050.080.000.030.110.170.190.000.010.030.050.070.010.040.120.180.230.000.030.060.110.170.050.310.390.460.470.000.010.020.030.030.000.010.010.020.020.00.10.20.30.40.50.010.050.10.150.2FGSM ()MLP-GlucosymMLP-T1DS2013MLP-Custom-GlucosymMLP-Custom-T1DS2013LSTM-GlucosymLSTM-T1DS2013LSTM-Custom-GlucosymLSTM-Custom-T1DS2013Robustness Error0.000.000.010.010.010.000.000.000.010.010.000.000.010.010.020.000.000.010.010.010.000.010.020.060.110.000.040.100.200.230.000.000.010.020.020.000.000.010.010.010.00.10.20.30.40.5REFERENCES

[1] Y. Luo, Y. Xiao, L. Cheng et al., “Deep Learning-based Anomaly
Detection in Cyber-physical Systems: Progress and Opportunities,” ACM
Computing Surveys, vol. 54, no. 5, pp. 106:1–106:36, May 2021.
[2] X. Zhou, B. Ahmed, J. H. Aylor et al., “Data-driven design of context-
aware monitors for hazard prediction in artiﬁcial pancreas systems,” in
2021 51st Annual IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN), 2021, pp. 484–496.

[3] A. Ding, P. Murthy, L. Garcia et al., “Mini-me, you complete me!
data-driven drone security via dnn-based approximate computing,” in
24th International Symposium on Research in Attacks, Intrusions and
Defenses, 2021, pp. 428–441.

[4] H. Choi, S. Kate, Y. Aafer et al., “Software-based realtime recovery from
sensor attacks on robotic vehicles,” in 23rd International Symposium on
Research in Attacks, Intrusions and Defenses (RAID 2020). USENIX
Association, Oct. 2020, pp. 349–364.

[5] P. Dash, G. Li, Z. Chen et al., “Pid-piper: Recovering robotic vehicles
from physical attacks,” in 51st Annual IEEE/IFIP International Confer-
ence on Dependable Systems and Networks (DSN), 2021, pp. 26–38.
[6] M. M. Rahman, R. M. Voyles, J. Wachs et al., “Sequential prediction
with logic constraints for surgical robotic activity recognition,” in
2021 30th IEEE International Conference on Robot Human Interactive
Communication (RO-MAN), 2021, pp. 468–475.

[7] J. Guo, Q. Zhang, Y. Zhao et al., “RNN-Test: Towards Adversarial
Testing for Recurrent Neural Network Systems,” IEEE Transactions on
Software Engineering, pp. 1–1, 2021.

[8] Y. Jia, J. Wang, C. M. Poskitt et al., “Adversarial attacks and mitigation
for anomaly detectors of cyber-physical systems,” International Journal
of Critical Infrastructure Protection, vol. 34, p. 100452, Sep. 2021.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S1874548221000445

[9] F. O. Olowononi, D. B. Rawat, and C. Liu, “Resilient Machine Learning
for Networked Cyber Physical Systems: A Survey for Machine Learning
Security to Securing Machine Learning for CPS,” IEEE Communications
Surveys Tutorials, vol. 23, no. 1, pp. 524–552, 2021, conference Name:
IEEE Communications Surveys Tutorials.

[10] U. Ehsan, Q. V. Liao, M. Muller et al., “Expanding explainability:
towards social transparency in ai systems,” in Proceedings of the 2021
CHI Conference on Human Factors in Computing Systems, 2021, pp.
1–19.

[11] C. M. Cutillo, K. R. Sharma, L. Foschini et al., “Machine intelligence
in healthcare—perspectives on trustworthiness, explainability, usability,
and transparency,” NPJ digital medicine, vol. 3, no. 1, pp. 1–5, 2020.

[12] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing

adversarial examples,” arXiv 1412.6572, Dec. 2014.

[13] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial examples
in the physical world,” in Artiﬁcial intelligence safety and security.
Chapman and Hall/CRC, 2018, pp. 99–112.

[14] J. Li, Y. Yang, J. S. Sun et al., “ConAML: Constrained Adversarial
Machine Learning for Cyber-Physical Systems,” in Proceedings of
the 2021 ACM Asia Conference on Computer and Communications
Security. Virtual Event Hong Kong: ACM, May 2021, pp. 52–66.
[Online]. Available: https://dl.acm.org/doi/10.1145/3433210.3437513

[15] N. Leveson and J. Thomas, “An STPA primer,” Cambridge, MA, 2013.
[16] X. Zhou, A. Schmedding, H. Ren et al., “Strategic safety-critical
attacks against an advanced driver assistance system,” in 2022 52nd
Annual IEEE/IFIP International Conference on Dependable Systems and
Networks (DSN), 2022.

[17] J. Xu, Z. Zhang, T. Friedman et al., “A semantic loss function for
deep learning with symbolic knowledge,” in International conference
on machine learning. PMLR, 2018, pp. 5502–5511.

[18] X. Zhou, B. Ahmed, J. H. Aylor et al., “Knowledge and data driven
synthesis of runtime monitors for cyber-physical systems,” in under
review of IEEE Transactions on Dependable and Secure Computing
(TDSC), 2021.

[19] E. Bartocci, J. Deshmukh, A. Donzé et al., “Speciﬁcation-based moni-
toring of cyber-physical systems: a survey on theory, tools and applica-
tions,” in Lectures on Runtime Veriﬁcation, 2018, pp. 135–175.
[20] S. Adepu and A. Mathur, “Using Process Invariants to Detect
IFIP
Cyber Attacks on a Water Treatment System,” in 31st
International
Information Security and Privacy Conference (SEC),
vol. AICT-471, May 2016, pp. 91–104. [Online]. Available: https:
//hal.inria.fr/hal-01369545

[21] A. A. Cárdenas, S. Amin, and Z.-S. Lin et al., “Attacks against process
control systems: Risk assessment, detection, and response,” in ASIACCS,
2011, p. 12.

[22] “Medtronic recalls minimed insulin pump models for cybersecurity
https://www.healio.com/news/endocrinology/20190627/

risks,”
medtronic-recalls-minimed-insulin-pump-models-for-cybersecurity-
risks.

[23] “Fda issues class i recall of certain medtronic insulin pumps,” https:

//beyondtype1.org/medtronic-recall/.

[24] “Medical device cyber attacks: Tv plot or dangerous

reality?”

https://www.drugwatch.com/news/2019/07/11/medical-device-cyber-
attacks-tv-plot-or-dangerous-reality/.

[25] L. Marcovecchio, “Complications of acute and chronic hyperglycemia,”

US Endocrinology, vol. 13 (1), 2017.

[26] A. Mehmed, W. Steiner, and A. Causevic, “Formal veriﬁcation of an
approach for systematic false positive mitigation in safe automated
driving system,” April 2020.

[27] H. Fischer, A history of the central limit theorem: From classical to

modern probability theory. Springer, 2011.

[28] G. R. Mode and K. A. Hoque, “Adversarial Examples in Deep
Learning for Multivariate Time Series Regression,” in 49th Annual
IEEE Applied Imagery Pattern Recognition (AIPR) workshop 2020,
Sep. 2020. [Online]. Available: http://arxiv.org/abs/2009.11911

[29] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial
examples in deep neural networks,” in Network and Distributed Systems
Security Symposium (NDSS) 2018, 2017.

[30] S. Guo, J. Zhao, X. Li et al., “A Black-Box Attack Method against
Machine-Learning-Based Anomaly Network Flow Detection Models,”
Security and Communication Networks, vol. 2021, Jan. 2021. [Online].
Available: https://doi.org/10.1155/2021/5578335

[31] W. Zhou, X. Hou, Y. Chen et al., “Transferable adversarial perturba-
tions,” in Proceedings of the European Conference on Computer Vision
(ECCV), 2018, pp. 452–467.

[32] G. R. Mode and K. A. Hoque, “Crafting Adversarial Examples for
Deep Learning Based Prognostics (Extended Version),” 2020. [Online].
Available: https://arxiv.org/abs/2009.10149

[33] “Principles of an open artiﬁcial pancreas system (openaps),” https:

//openaps.org/reference-design.

[34] E. Chertok Shacham, H. Kﬁr, N. Schwartz et al., “Glycemic control with
a basal-bolus insulin protocol in hospitalized diabetic patients treated
with glucocorticoids: a retrospective cohort,” BMC Endocr Disord, vol.
18(75), pp. 1472–6823, 2018.

[35] “Glucosym,” https://github.com/Perceptus/GlucoSym.
[36] C. D. Man, F. Micheletto, D. Lv et al., “The uva/padova type 1 diabetes
simulator: new features,” Journal of diabetes science and technology,
vol. 8, no. 1, pp. 26–34, 2014.

[37] X. Zhang and D. Evans, “Cost-sensitive robustness against adversarial
examples,” Seventh International Conference on Learning Representa-
tions, 2018.

[38] A. Basak, P. Rathore, S. H. Nistala et al., “Universal Adversarial Attack
on Deep Learning Based Prognostics,” in 2021 20th IEEE International
Conference on Machine Learning and Applications (ICMLA), Dec. 2021,
pp. 23–29.

[39] W. Dong, H. Sun, J. Tan et al., “Short-term regional wind power
forecasting for small datasets with input data correction, hybrid
neural network, and error analysis,” Energy Reports, vol. 7, pp.
7675–7692, 2021. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S2352484721011665

[40] Z. C. Lipton, D. C. Kale, C. Elkan et al., “Learning to Diagnose
with LSTM Recurrent Neural Networks,” 2015. [Online]. Available:
https://arxiv.org/abs/1511.03677

[41] M. Yasar and H. Alemzadeh, “Real-time context-aware detection of
unsafe events in robot-assisted surgery,” in 2020 50th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN),
2020, pp. 385–397.

[42] J.-H. Moon, J.-H. Yu, and K.-A. Sohn, “An ensemble approach to
anomaly detection using high- and low-variance principal components,”
Computers and Electrical Engineering, vol. 99, p. 107773, Apr. 2022.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S0045790622000714

[43] M. Rigaki and S. Garcia, “A Survey of Privacy Attacks in Machine
Learning,” arXiv:2007.07646 [cs], Apr. 2021, arXiv: 2007.07646.
[Online]. Available: http://arxiv.org/abs/2007.07646

7

[44] F. Tramèr, A. Kurakin, N. Papernot et al., “Ensemble adversarial
training: Attacks and defenses,” arXiv preprint arXiv:1705.07204, 2017.
[45] Y. Ganin, E. Ustinova, H. Ajakan et al., “Domain-adversarial training
of neural networks,” The journal of machine learning research, vol. 17,
no. 1, pp. 2096–2030, 2016.

[46] E. Wong, L. Rice,

free: Revisiting adversarial
on Learning Representations, 2020.
//openreview.net/forum?id=BJx040EFvH

and J. Z. Kolter,

than
“Fast
training,” in International Conference
[Online]. Available: https:

is better

[47] N. Papernot, P. McDaniel, A. Sinha et al., “Towards the science of
security and privacy in machine learning,” 2016. [Online]. Available:
https://arxiv.org/abs/1611.03814

[48] S. Rifai, P. Vincent, X. Muller et al., “Contractive auto-encoders:
Explicit invariance during feature extraction,” in ICML, 2011, pp. 833–
840. [Online]. Available: https://icml.cc/2011/papers/455_icmlpaper.pdf
[49] S. Gu and L. Rigazio, “Towards deep neural network architectures
robust to adversarial examples,” in 3rd International Conference on
Learning Representations, 2015. [Online]. Available: http://arxiv.org/
abs/1412.5068

8

