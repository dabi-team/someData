9
1
0
2

g
u
A
5
1

]
E
M

.
t
a
t
s
[

1
v
9
2
0
6
0
.
8
0
9
1
:
v
i
X
r
a

Pearson Distance is not a Distance

Victor Solo
Department of Electrical Engineering, University of New South Wales

August 19, 2019

Abstract

The Pearson distance between a pair of random variables X, Y with correlation
ρxy, namely, 1-ρxy, has gained widespread use, particularly for clustering, in areas
In all these
such as gene expression analysis, brain imaging and cyber security.
applications it is implicitly assumed/required that the distance measures be metrics,
thus satisfying the triangle inequality. We show however, that Pearson distance is
not a metric. We go on to show that this can be repaired by recalling the result,
ρxy is a metric. We similarly show that
(well known in other literature) that
p
, which is invariant to the sign of ρxy, is not a
ρxy|
a related measure of interest, 1
metric but that q1

ρ2
xy is. We also give generalizations of these results.

− |

−

1

−

Keywords: distance, Pearson correlation, metric.

1

 
 
 
 
 
 
Distance measures have a long history of application in Statistics as well as in many

other areas. Some of the earliest applications in Statistics are to cluster analysis, Gower

(1967),Sokal and Sneath (1963) and multidimensional scaling, Torgerson (1958),Kruskal and Wish

(1977),Gower (1966). More recently there has been an explosion of interest in distance

measures for application to gene expression data, Gibbons and Roth (2002),d’Haeseleer

(2005),Jaskowiak et al. (2014). Other recent sources of interest include brain imaging

Walther et al. (2016) and cyber security, Weller-Fahy et al. (2015).

Note that we are concerned here with measures of distance between random variables,

not with measures of distance between probability measures - such as Hellinger distance.

To prevent confusion between ’distance’ and ’metric’, henceforth we will instead use the

pair ’dissimilarity’ and ’metric’, following terminology of Gower and Legendre (1986).

Given a ﬁnite set of random variables Xk, k1,

, N1N, a pairwise dissimilarity be-

· · ·

•

tween Xi, Xj is a functional of the bivariate distribution satisfying: (i) symmetry: dij = dji;
(ii) strict positivity i.e. dij ≥
measure is a functional of the bivariate distribution which satisiﬁes

0 with equality iﬀ i = j. A normalised pairwise similarity

1, sii = 1 and

sij| ≤

|

symmetry. Associated to any normalised similarity measure is a dissimilarity measure

dij = 1

sij.

−

A dissimilarity measure dij is a metric if it obeys the triangle inequality: dij ≤

dik +dkj

•
for all 1

i, j, k

N.

≤

≤

But why do we need the metric property? Most of the applications above are concerned

with clustering which needs to be done consistently. To partition variables into two or

more clusters coherently, we need to ensure that the variables in one cluster are closer to

each other than they are to the variables in another cluster. If X is close to Y (so that dxy

is small) and Z is close to Y (so that dzy is small) then to put X,Y,Z in the same cluster

we need to know that X is close to Z; so we need to ensure dxz is small. The simplest

way to guarantee this is to require the triangle inequality holds, otherwise we can have dxz
arbitrarily large. Thus clustering is made coherent by ensuring dij is a metric 1

Continuing, clearly the pairwise correlation ρij is a normalised similarity measure and

we will focus on similarity/dissimilarity measures that are functions of correlation. Of

1An even stronger condition, not pursued here, is the ultrametric condition dxz

max(dxy, dyz).

≤

2

particular interest will be four measures of dissimilarity:

1. Pearson dissimilarity: 1

ρij.

−

2.

P

|

|

earson dissimilarity: 1

− |

3. √Pearson dissimilarity:

p
4. P2earson dissimilarity: q1

1

.

ρij|
ρij.

−

ρ2
ij.

−

Numerous dissimilarity measures have been applied to gene expression data but Pear-

son dissimilarity is one of the main ones: Gibbons and Roth (2002)[deﬁnition on p1575],

d’Haeseleer (2005)[Table 1], Jaskowiak et al. (2014)[comments below equation (11)], Weller-Fahy et al.

(2015)[equation (41)]. The story is similar in brain imaging, Walther et al. (2016)[section

2.2] and cyber security, Weller-Fahy et al. (2015)[equation (41)].

Gower (1966) discussed normalised similarity informally, showing that

1
p

sij is a

−

metric if the matrix [sij] is positive semi-deﬁnite. These results are formally proved in

Gower and Legendre (1986). While Gower (1966) does not mention correlation explicitly

we may say that it has been known for at least half a century that √Pearson dissimilarity,

is a metric. But what of Pearson dissimilarity?

In section II we use elementary arguments to show that Pearson dissimilarity is not

a metric and also provide an elementary proof that √Pearson dissimilarity is a metric.

earson dissimilarity
In section III we use similar elementary arguments to show that
is not a metric, while P2earson dissimilarity, is. In section IV contains conclusions. For

|

|

P

completeness, in the appendix, we discuss more general results from the metric preserving

literature and the work of Gower and Legendre (1986).

1 Pearson Dissimilarity

We give elementary analyses of √Pearson dissimilarity and Pearson dissimilarity.

Result I. √Pearson dissimilarity is a Metric.

Proof. Let (X, Y, Z) = (X1, X2, X3) be zero mean random variables each with unit variance.

3

Then consider that

X

|
(X

Y

−
Y )2

⇒
E(X

−

−

⇒

Y )2

E(X

⇒ p
⇒ q2(1

Y )2

−

ρx,y)

−

| ≤ |

X

−

Y

Z

+

Z

−
|
|
Z)2 + (Z

−
Z)2 + E(Z

|
Y )2 + 2

X

Z

Z

Y

|
−
Y )2 + 2E(

||
X

−
Z

|
Z

(X

−
E(X

≤

≤

−

≤
= [

−
E(X

p

E(X

≤ p
≤ q2(1

−

E(X

Z)2 + E(Z

Y )2 + 2

−

−
E(Z

p
Y )2]2

−

Z)2 +

−

p

E(Z

Z)2 +

p

−
ρx,z) + q2(1

Y )2

−
ρz,y).

−

|
−
E(X

||
Z)2

−

Y

)

|
E(Z

−

p

Y )2

−

and the proof is complete.

Result II. Pearson dissimilarity is not a Metric.

Proof. Let U, V, W be zero mean random variables with:

U independent of V, W and for 0 < θ < π
2

var(V ) = var(W ) = sin2(θ), corr(V, W ) =

cos2(θ) and var(U) = cos2(θ).

−

Then set (X, Y, Z) = (U + V, U + W, U) so that var(X) = 1 = var(Y ). Thus

ρy,z = ρx,z =

E(XZ)
var(X)var(Z)

=

p

cos2(θ)
cos(θ)

|

|

=

|

cos(θ)

.

|

However

cos(θ)

|

|

= cos(θ) since 0 < θ < π

2 . Continuing

ρx,y = E(U + V )(U + W ) = E(U 2) + E(V W )

= var(U) + cov(V, W )

= var(U) + corr(V, W )

var(V )var(W )

p
cos2(θ)sin2(θ) = cos4(θ)

= cos2(θ)

−

Now we show the triangle inequality can be violated i.e.

(1

−

ρx,y) > (1

1

≡

−

ρx,z) + (1
−
−
cos4(θ) > 2(1

cos2(θ))(1 + cos2(θ)) > 2(1

cos(θ))

cos(θ))

−

−

ρy,z) = 2(1

ρx,z)

−

(1 + cos(θ))(1 + cos2(θ)) > 2

(1

≡

−

≡

4

This certainly holds for

θ

|

| ≤

π
4 and the claim is established. Not that the violation is not

inﬁnitesimal, it is gross, covering over half the range of θ values.

2 P2earson and

P

|

|

earson Dissimilarities

Here we give elementary analyses of P2earson and
Result III. P2earson dissimilarity is a metric.

P

|

|

earson dissimilarity.

Proof. This result is due to Innocenti and Materassi (2008) but we give an elementary

proof for completeness. Let Xk, k1,

, N1N and Yk, k1,

· · ·

· · ·

, N1N be two independent

random vectors each with zero mean, unit variances and the same correlation matrix [ρij].

Set Zk = XkYk, k1,

· · ·

, N1N. Then the Zk have zero means and unit variances since

E(Zk) = E(XkYk) = E(Xk)E(Yk) = 0
k ) = E(X 2
k) = E(X 2
var(Zk) = E(Z 2

k Y 2

k )E(Y 2

k ) = 1.

Their correlations are, for i

= j

E(ZiZj) = E(XiYiXjYj) = E(XiYi)E(XjYj) = ρ2

ij

Now result I applies to the set of random variables

Zk}

{

and the result follows.

Result IV.

P

|

|

earson dissimilarity is not a metric.

Proof. We use the construction from Theorem 2.2. We show the triangle inequality is

violated. We need

1

1

−

≡

≡

> 1

ρx,y|
− |
cos4(θ) > 2

− |

+ 1

ρx,z|
2cos(θ)

ρy,z|

− |

−

2cos(θ) > 1 + cos4(θ)

cos(θ)(2

−

≡

cos3(θ)) > 1

One can easily check that this holds for π

6 ≤

π
4 ; again a gross violation.

θ

≤

5

6
3 Conclusion

In this paper, motivated by the widespread use of distance measures for clustering in nu-

merous applications we explained why coherent clustering necessitates the metric property.

We have then given elementary analyses of four distance measures followed by an appendix

which sketches more general results.

The most widely used measure, Pearson distance = 1

ρxy, is not a metric whereas

−

it has long been known, that a simple modiﬁcation, √Pearson =

popular sign invariant modiﬁcation of Pearson distance,
metric whereas a simple modiﬁcation P2earson =

1
p

−

1
p

P

−
earson = 1

|
|
ρ2
xy is a metric.

ρxy is a metric. A
ρxy|

is also not a

− |

4 Appendix: General Methods

The results of Gower and Legendre (1986) provide a general approach to proving the metric

property of a dissimilarity measure associated with a normalised similarity measure. And

methods from the metric preserving function literature, Corazza (1999) provide general

tools for proving when transformations of metrics succeed or fail to preserve the metric

property. We state some of the basic results and then apply them to the current setting.

Result S1. Gower and Legendre (1986). If [sij] is a normalised similarity matrix then

the dissimilarity dij =

1

p

−

sij is a metric if [sij] is positive semi-deﬁnite.

This immediately implies result I.

To continue we introduce:

A function f (x) is metric preserving if whenever dij is a metric so is f (dij).

•

Note that we need only consider f (x) for x

0 since metrics are non-negative.

≥

We now have the following results.

Result M1. Corazza (1999). If g(x) is strictly convex and passes through the origin

i.e. g(0) = 0 then it is not metric preserving.

Result M2. Corazza (1999). If f(0) = 0 and f(x) is concave and strictly increasing

then it is metric preserving.

From result M1 we immediately get:

6

Result II∗. No strictly convex function of (a)

1

p

−

ρij or of (b) q1

−

ρ2
ij, which passes

through the origin, is a metric.

Result II follows from result II∗a if we take g(x) = x2. Result IV follows from result

II∗b as follows. Set

dij = q1

ρ2
ij ⇒

−

1

ρij|

− |

= 1

− q1

−

d2
ij = g(dij)

and note that g(d) = 0 iﬀ d = 0. Next

g′(d) =

d

−

√1

g′′(d) =

d2 ⇒

1
d2(1

> 0

d2)

−

√1

−

so that g(d) is strictly convex and result IV follows.

Result M2 immediately delivers:

Result IVast. Any strictly increasing concave function of

1

p

−

ρij or of q1

ρ2
ij,

−

that passes through the origin, is a metric.

Example 4.1. Since f (x) = √x is concave then

dij = (1

p

−

ρij)1/4 is a metric.

References

Corazza, P. (1999), ‘Introduction to metric preserving fuctions’, Amer. Math. Monthly

104, 309323.

d’Haeseleer, P. (2005), ‘How does gene clustering work?’, Nature Biotechnology 23, 1499–

1501.

Gibbons, F. and Roth, F. (2002), ‘Judging the quality of gene expression-based clustering

methods using gene annotation’, Genome Research 12, 15741581.

Gower, J. (1966), ‘Some distance properties of latent root and vector methods used in

multivariate analysis’, Biometrika 53, 325–338.

Gower, J. (1967), ‘A comparison of some methods of cluster analysis’, Biometrics 23, 623–

637.

7

Gower, J. C. and Legendre, P. (1986), ‘Metric and euclidean properties of dissimilarity

coeﬃcients’, Journal of Classiﬁcation 3, 5–48.

Innocenti, G. and Materassi, D. (2008), Econometrics as sorcery, Technical report, Dipar-

timento di Sistemi e Informatica, Universit di Firenze.

Jaskowiak, P., Campello, R. and Costa, I. (2014), ‘On the selection of appropriate distances

for gene expression data clustering’, BMC Bioinformatics 15(Suppl 2):S2.

Kruskal, J. and Wish, M. (1977), Multidimensional Scaling, Sage Publications, Beverly

Hills. CA.

Sokal, R. and Sneath, P. (1963), Principles of Numerical Taxonomy, W. H. Freeman and

Co., San Francisco and London.

Torgerson, W. (1958), Theory and Methods of Scaling, J. Wiley, New York.

Walther, A., Nili, H., Ejaz, N., Alink, A., Kriegeskorte, N. and Diedrichsen, J. (2016), ‘Reli-

ability of dissimilarity measures for multi-voxel pattern analysis’, Neuroimage 137, 188–

200.

Weller-Fahy, D., Borghetti, B. and Sodemann, A. (2015), ‘A survey of distance and simi-

larity measures used within network intrusion anomaly detection’, IEEE Comm. Surveys

& Tutorials 17, 70–91.

8

