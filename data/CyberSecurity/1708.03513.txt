Early-Stage Malware Prediction Using Recurrent Neural Networks

Matilda Rhodea,∗, Pete Burnapa, Kevin Jonesb

aSchool of Computer Science and Informatics, Cardiﬀ University
bAirbus Group

8
1
0
2

n
u
J

8
1

]

R
C
.
s
c
[

3
v
3
1
5
3
0
.
8
0
7
1
:
v
i
X
r
a

Abstract

Static malware analysis is well-suited to endpoint anti-virus systems as it can be conducted
quickly by examining the features of an executable piece of code and matching it to previously
observed malicious code. However, static code analysis can be vulnerable to code obfuscation
techniques. Behavioural data collected during ﬁle execution is more diﬃcult to obfuscate,
but takes a relatively long time to capture - typically up to 5 minutes, meaning the malicious
payload has likely already been delivered by the time it is detected.

In this paper we investigate the possibility of predicting whether or not an executable
is malicious based on a short snapshot of behavioural data. We ﬁnd that an ensemble of
recurrent neural networks are able to predict whether an executable is malicious or benign
within the ﬁrst 5 seconds of execution with 94% accuracy. This is the ﬁrst time general types
of malicious ﬁle have been predicted to be malicious during execution rather than using a
complete activity log ﬁle post-execution, and enables cyber security endpoint protection to
be advanced to use behavioural data for blocking malicious payloads rather than detecting
them post-execution and having to repair the damage.

Keywords: malware detection, intrusion detection, recurrent neural networks, machine
learning, deep learning

1. Introduction

Automatic malware detection is necessary to process the rapidly rising rate and volume
of new malware being generated. Virus Total, a free tool which can be used to evaluate
whether ﬁles are malicious, regularly approaches one million new, distinct ﬁles for analysis
each day1[1].

Commonly, automatic malware detection used in anti-virus systems compares (features
extracted from) the code of an incoming ﬁle to a known list of malware signatures. However,
this form of ﬁltering using static data is unsuited to detecting completely new (“zero-day”)

∗Corresponding author
Email addresses: rhodem@cardiff.ac.uk (Matilda Rhode), burnapp@cardiff.ac.uk (Pete Burnap),

kevin.jones@airbus.com (Kevin Jones)
10.935 million on 2nd December 2017

 
 
 
 
 
 
malware unless it shares code with previously known strains [2]. Obfuscating the code,
now common practice among malware authors, can even enable previously seen malware to
escape detection [3].

Malware detection research has evolved to respond to the inadequacies of static detection.
Behavioural analysis (dynamic analysis) examines a sample ﬁle in a virtual environment
whilst it is being executed. Behavioural analysis approaches assume that malware cannot
avoid leaving a measurable footprint as a result of the actions necessary for it to achieve
its aims. However, executing the malware incurs a time penalty by comparison with static
analysis. Whilst dynamic data can lead to more accurate and resilient detection models than
static data ([4], [5], [6]), in practice behavioural data is rarely used in commercial endpoint
anti-virus systems due to this time penalty. It is inconvenient and ineﬃcient to wait for
several minutes whilst a single ﬁle is analysed, and ultimately, the malicious payload has
likely been delivered by the end of the analysis window so the opportunity to block malicious
actions has been missed.

To avoid waiting, some approaches monitor “live” activity on the local network or the
machine. These detection systems tend either to look for traits that signify a particular type
of malware (e.g. ransomware) or to ﬂag deviations from a baseline of “normal” behaviour.
These two approaches suﬀer from speciﬁc ﬂaws. Searching for particular behaviours is
analogous to the traditional methods of comparing incoming ﬁles with known variants, and
may miss detecting new types of malware. Whilst anomaly detection is prone to a high false-
positive rate as any activity that deviates from a “normal” baseline is deemed malicious.
In practice anomalous activity is often investigated by human analysts, making the model
vulnerable to exploitation. An attacker could bring about lots of anomalous behaviour such
that the human analysts are ﬂooded with investigation requests, reducing the chances of the
activity created by the attack itself from being detected.

We propose a behaviour-based model to predict whether or not a ﬁle is malicious using the
ﬁrst few seconds of ﬁle execution with a view to developing a tool that could be incorporated
into an end-point solution. Though general malicious and benign ﬁles comprise a wide range
of software and potential behaviours, our intuition is that malicious activity begins rapidly
once a malicious ﬁle begins execution because this reduces the overall runtime of the ﬁle
and thus the window of opportunity for being disrupted (by a detection system, analyst,
or technical failure). As far as we are aware this is the ﬁrst paper attempting to predict
malicious behaviour for various types of malware based on early stage activity.

We feed a concise feature set of ﬁle machine activity into an ensemble of recurrent neural
networks and ﬁnd that we achieve a 94% accurate detection of benign and malicious ﬁles
5 seconds into execution. Previous dynamic analysis research collects data for around 5
minutes per sample.

The main contributions of this paper are:

1. We propose a recurrent neural network (RNN) model to predict malicious behaviour
using machine activity data and demonstrate its capabilities are superior to other
machine learning solutions that have previously been used for malware detection
2. We conduct a random search of hyperparameter conﬁgurations and provide details

2

of the conﬁgurations leading to high classiﬁcation accuracy, giving insight into the
methods required for optimising our malware detection model

3. We investigate the capacity of our model to detect malware families and variants which
it has not seen previously - simulating ‘zero day’ and advanced persistent threat (APT)
attacks that are notoriously diﬃcult to detect

4. We conduct a case-study using 3,000 ransomware samples and show that our model
has high detection accuracy (94%) at 1 second into execution without prior exposure
to examples of ransomware, and investigate the combinations of features most relevant
to the model decisions

2. Related Work

Automatic malware detection models typically use either code or behaviour based fea-
tures to represent malicious and benign samples. Each of these approaches has its beneﬁts
and drawbacks, such that research continues to explore detection methods using both kinds
of data.

Hybrid approaches use both static and dynamic data, closer approximating the methods
used by anti-virus engines; why analyse the behaviour of a ﬁle if it matches a known malware
signature? But unless static detection is used purely to ﬁlter out known malwares, any
dependence on static methods in a hybrid approach leaves the model open to the same
weaknesses as a purely static model.

Static data. Static data, derived directly from code, can be collected quickly. Though
signature-based methods fail to detect obfuscated or entirely new malware, researchers have
extracted other features for static detection. Saxe and Berlin [7] distinguish malware from
benignware using a deep feed-forward neural network with a true-positive rate of 95.2% using
features derived from code. However, the true-positive rate falls to 67.7% when the model
is trained using ﬁles only seen before a given date and tested using those discovered for the
ﬁrst time after that date, indicating the weakness of static methods in detecting completely
new malwares. Damodoran et al. [5] conducted a comparative study of static, behavioural
and hybrid detection models for malware detection and found behavioural data to give the
highest area under the curve (AUC) value, 0.98, using Hidden Markov Models with a dataset
of 785 samples. Additionally, Grosse et al.[6] show that, in the case of Android software,
static data can be obfuscated to cause a classiﬁer previously achieving 97% accuracy to
fall as low as 20% when classifying obfuscated samples. Training using obfuscated samples
allowed a partial recovery of accuracy, but accuracy did not improve beyond random chance.

Dynamic data. Methods using dynamic data assume that malware must enact the be-
haviours necessary to achieve their aims. Typically, these approaches capture behaviours
such as API calls to the operating system kernel. Tobiyama et al.[8] use RNNs to extract
features from 5 minutes of API call log sequences which are then fed into a convolutional
neural network to obtain 0.96 AUC score with a dataset of 170 samples. Firadusi et al. [9]
compare machine learning algorithms trained on API calls and achieve an accuracy of 96.8%

3

using correlation-based feature selection and a J48 decision tree. The 250 benign samples
used for the experiment are all collected from the WindowsXP System32 directory, which
is likely to give a higher degree of homogeneity than benign software encountered in the
wild. Ahmed et al.
[10] detect malware using API call streams and associated metadata
with a Naive Bayes classiﬁer, achieving 0.988 AUC, again with the 100 benign samples be-
ing WindowsXP 32-bit system ﬁles. Both Tian et al.
[11] and Hansen et al. use Random
Forests trained on API calls and associated metadata to achieve 97% accuracy and a 98%
F-Score respectively. Huang and Stokes [12] achieve the highest accuracy in the literature,
99.64%, using System API calls and features derived from those API calls using a shallow
feed-forward neural network. Table 1 outlines the dataset sizes and recording time for the
related literature. The median dataset size for binary classiﬁcation is 1,300 samples. Huang
[13] are outliers with much larger datasets, both ob-
and Stokes [12] and Pascanu et al.
tained through access to the corpus of samples held privately by the authors’ companies.
The majority of research does not mention a time-cap on ﬁle execution, in these cases we
may presume that the ﬁles are executed until activity stops. The median data capture time
frame for those reported is 5 minutes (see Table 1).

Ref. Malicious

samples

Benign
samples

Reported time collecting dynamic
data
Binary classiﬁcation

[8]
[9]

81
220

[10]

416

[5]

745

69
250

100

40

[11]
[14]2
[13]

1,368
5,000
25,000

465
837
25,000

[12]

2.85m

3.65m

5 minutes
No time cap mentioned - implicit
full execution
No time cap mentioned - implicit
full execution
Fixed time and 5-10 minutes men-
tioned but overall time cap not ex-
plicitly stated
30 seconds
3.33 minutes (200 seconds)
At least 15 steps - exact time unre-
ported
No time cap mentioned - implicit
full execution

[15]

4,753

Malware family classiﬁcation
n/a

No time cap mentioned - implicit
full execution

Table 1: Reported data sample sizes and times collecting dynamic behavioural data per sample

Time-eﬃciency dynamic analysis methods. Existing methods to reduce dynamic data record-
ing time focus on eﬃciency. The core concept is only to record dynamic data if it will improve
accuracy, either by omitting some ﬁles from dynamic data collection or by stopping data

4

collection early. Shibahara et al.
[16] decide when to stop analysis for each sample based
on changes in network communication, reducing the total time taken by 67% compared
with a “conventional” method that analyses samples for 15 minutes each. Neugschwandtner
et al. [17] used static data to determine dissimilarity to known malware variants using a
clustering algorithm. If the sample is suﬃciently unlike any seen before, dynamic analysis
is carried out. This approach demonstrated an improvement in classiﬁcation accuracy by
comparison with randomly selecting which ﬁles to dynamically analyse, or selecting based
on sample diversity. Similarly, Bayer et al. [18] create behavioural proﬁles to try and iden-
tify polymorphic variants of known malware, reducing the number of ﬁles undergoing full
dynamic analysis by 25%. Approaches to date still allow some ﬁles to be run for a long
dynamic execution time, whereas here we investigate a blanket cut-oﬀ of dynamic analysis
for all samples, with a view to this analysis being run in an endpoint anti-virus engine.

RNNs for malware detection. We propose using a recurrent neural network (RNN) for pre-
dicting malicious activity as as they are able to process time-series data, thus capturing
information about change over time as well as the raw input feature values. Kolsnaji et
al [19] sought to detect malware families with deep neural networks, including recurrent
networks, to classify malware into families using API call sequences. By combining a con-
volutional neural network with long-short-term memory (LSTM) cells, the authors were
able to attain a recall of 89.4%, but do not address the binary classiﬁcation problem of
distinguishing malware from benignware. Pascanu et al. [13] did conduct experiments into
whether ﬁles were malicious or benign using RNNs and Echo State Networks. The authors
found that Echo State Networks performed better with an accuracy of around 95% (error
rate of 5%) but did not attempt to predict malicious behaviour from initial execution.

Ransomware detection. In Section 5.4 we test our model on a corpus of 3,000 ransomware
samples. Early prediction is particularly useful for types of malware from which recovery
is diﬃcult and/or costly. Ransomware encrypts user ﬁles and withholds the decryption key
until a ransom is paid to the attackers. This type of attack cannot be remedied without
ﬁnancial loss unless a backup of the data exists. Recent work on ransomware detection by
Scaife et al. [20] uses features from ﬁle system data, such as whether the contents appears to
have been encrypted, and number of changes made to the ﬁle type. The authors were able
to detect and block all of the 492 ransomware samples tested with less than 33% of user data
being lost in each instance. Continella et al. [21] propose a self-healing system, which detects
malware using ﬁle system machine activity (such as read/write ﬁle counts), the authors were
able to detect all 305 ransomware samples tested, with a very low false-positive rate. These
two approaches use features selected speciﬁcally for their ability to detect ransomware, but
this requires knowledge of how the malware operates. Our approach seeks to use features
which can be used to detect any malware family, including those which have not been seen
before. That is to say, we will demonstrate the eﬀectiveness of detecting ransomware without
dependence on ransomware-speciﬁc training data. The key purpose of this ﬁnal experiment
is to show that our general model of malware detection is able to detect general types of
malware as well as time-critical samples such as ransomware.

5

3. Methods

Dynamically collected data is more robust to obfuscation methods than statically col-
lected data ([5], [6]), but dynamic collection takes longer.
In order to advance malware
detection to a more predictive model that can respond in seconds we propose a model which
uses only short sequences of the initial dynamic data to investigate whether this is suﬃcient
to judge a ﬁle as malicious with a high degree of accuracy.

We use 10 machine activity data metrics as feature inputs to the model. We take a
snapshot of the metrics every second for 20 seconds whilst the sample executes, starting at
0s, such that at 1s, we have two feature sets or a sequence length of 2. Though API calls
to the operating system kernel are the most popular behavioural features used in dynamic
malware detection, there are several reasons why we have chosen machine activity features as
inputs to the model instead. Firstly, recent work has shown that API calls are vulnerable to
manipulation, causing neural networks to misclassify samples ([22], [23]). As Burnap et al.
[24] argue“malware cannot avoid leaving a behavioural footprint” of machine activity, future
work will necessarily examine the robustness of machine activity to adversarial crafting, but
this is outside the scope of this paper. A key advantage of continuous data such as machine
activity metrics is that the model is able to infer information from completely unseen input
values; any unseen data values in the test set will still have numerical relevance to the data
from the training set as it will have a relative value that can be mapped onto the learned
model. API calls on the other hand are categorical, such the meaning of unseen API call
cannot be interpolated against existing data. Practically, categorical features require an
input vector with a placeholder for each category to record whether it is present or not.
Hundreds or even thousands ([12]) of API calls can be collected, leading to a very large
input vector, which in turn makes the model slower to train. Being categorical, any API
calls not present in the training data will have no placeholder in the input vector at the
classiﬁcation stage even if they appear in later test samples. The machine activity data we
collected are continuous numeric values, allowing for a large number of diﬀerent machine
states to be represented in a small vector of size 10.

As illustrated in Figure 1, to collect our activity data we executed Portable Executable
(PE) samples using Cuckoo Sandbox [25], a virtualised sandboxing tool. While executing
each sample we extracted machine activity metrics using a custom auxiliary module reliant
on the Python Psutil library[26]. The metrics captured were: system CPU usage, user CPU
use, packets sent, packets received, bytes sent, bytes received, memory use, swap use, the
total number of processes currently running and the maximum process ID assigned.

6

Figure 1: High-level model overview

As the data are sequential, we chose an algorithm capable of analysing sequential data.
Making use of the time-series data means that the rate and direction of change in features
as well as the raw values themselves are all inputs to the model. Recurrent Neural Networks
(RNNs) and Hidden Markov Models are both able to capture sequential changes, but RNNs
hold the advantage in situations with a large possible universe of states and memory over
an extended chain of events [27], and are therefore better suited to detecting malware using
machine activity data.

RNNs can create temporal depth in the same way that neural networks are deep when
multiple hidden layers are used. Until the development of the LSTM cell by Hochreiter
and Schmidhuber in 1997, RNNs performed poorly in classifying long sequences, as the
updates required to tune the weights between neurons would tend to vanish or explode [28].
LSTM cells can hold information back from the network until such a time as it is relevant
or “forget” information, thus mitigating the problems surrounding weight updates. The
success of LSTM has prompted a number of variants, though few of these have signiﬁcantly
improved on the classiﬁcation abilities of the original model [29]. Gated Recurrent Units
(GRUs) [30], however, have been shown to have comparable classiﬁcation to LSTM cells,
and in some instances can be faster to train [31], for this potential training speed advantage,
we use GRU units.

An appropriate architecture and learning procedure of a neural network is usually integral
to a successful model. These attributes are captured by hyperparameter settings, which are
often hand-crafted. Due to the rapid evolution of malware, we anticipate that the RNN
should be re-trained regularly with newly discovered samples, thus the architecture may
need to change too. As it needs to be carried out multiple times, this process should be
automated. We chose to conduct a random search of the hyperparameter space as it can
easily be parallelised (unlike a grid search), it is trivial to implement, and has been found
to be more eﬃcient at ﬁnding good conﬁgurations than grid search [32]. We chose the
conﬁguration which performed best on a 10-fold cross-validation over the training set for
our ﬁnal model conﬁguration, the hyperparameter search space and ﬁnal conﬁguration is
detailed in Table 2 for reproducibiltiy.

7

Possible values
1, 2, 3
True, False
1 – 500
1 – 500
0 – 0.5 (0.1 increments)

Hyperparameter
Depth
Bidirectional
Hidden neurons
Epochs
Dropout rate
Weight regularisation None, l1, l2, l1 and l2
None, l1, l2, l1 and l2
Bias regularisation
32, 64, 128, 256
Batch size

Best conﬁguration
3
True
74
53
0.3
l2
None
64

Table 2: Possible hyperparameter values and the hyperparameters of the best-perfoming conﬁguration on
the training set

4. Dataset

4.1. Samples

We initially obtained 1,000 malicious and 600 “trusted” Windows7 executables from
VirusTotal [33] along with 800 trusted samples from the system ﬁles of a fresh Windows7 64-
bit installation. We then downloaded a further 4,000 Windows 7 applications from popular
free software sources, such as Softonic [34], PortableApps [35] and SourceForge [36]. We
included the online download ﬁles as they are a better representation the typical workload
of an anti-virus system than Windows system ﬁles.

We used the VirusTotal API [33] as a proxy to label the downloaded software as benign
or malicious. VirusTotal runs ﬁles through around 60 anti-virus engines and reports the
number of engines that detected the ﬁle as malicious. Similar to [7], for malicious samples,
we omitted any ﬁles that were deemed malicious by less than 5 engines in the VirusTotal
API as the labelling of these ﬁles is contentious. Files not labelled as malicious by any
of the anti-virus engines were deemed ’trusted’ as there is no evidence to suggest they are
malware. We therefore consider these as benign samples. This has the limitation of not
detecting previously unseen malware but our samples are selected from an extended time
period historically so it is likely that it would be reported as malware at some point in this
period if it were actually malicious.

The ﬁnal dataset comprised 2,345 benign and 2,286 malicious samples, which is consistent
with dataset sizes in this ﬁeld of research e.g. ([9], [11], [37], [10], [5], [8], [38]). We used a
further 2,876 ransomware samples obtained from the VirusShare online malware repository
[39] for the ransomware case study in Section 5.4.

We were also able to extract the date that VirusTotal had ﬁrst seen each ﬁle and the
families and variants that each anti-virus engine classiﬁed the malware samples. The dates
that the ﬁles were ﬁrst seen ranged from 2006 to 2017. We split the test and training set
ﬁles according to the date ﬁrst seen to mimic the arrival of completely new software. The
training set only comprised samples ﬁrst seen by VirusTotal before 11:15 on 10th October
2017 and the test set only samples after this date, which produced a test set of 500 samples
(206 trusted and 316 malicious). We choose this date and time as it gave a number of each
malicious and benign samples that is is line with the sample size in the existing literature.

8

The total instances of the diﬀerent malware families is documented in Table 3. The
“disputed” class represents those malware for which a family could not be determined be-
cause the anti-virus engines did not produce a majority vote in favour of one type. We also
found the precise variants where possible, and have listed the numbers of advanced persistent
threat malware (APTs) and ransomware in each category as APTs are notoriously diﬃcult
for static engines to detect and the ransomware case-study in Section 5.4 required removal
of all ransomware from the training set.

Family
Trojan
Virus
Adware
Backdoor
Bot
Worm
Rootkit
Disputed
Total

Total (apt)(ransomware)
1,382 (0)(76)
407 (20)(56)
180 (0)(51)
123 (7)(0)
76
24
11
83
2,239

Table 3: Number of instances of diﬀerent malware families in dataset

4.2. Input Features

Table 4 outlines the minimum and maximum values of the 10 inputs we collected for
malware and benignware respectively. Though the inter-quartile ranges of values are gener-
ally similar (See Figure 2) The benign data sees a far greater number of outliers in RAM use
(memory and swap) and packets being received. The malicious data has a large number of
outliers in total number of processes, but the benign samples have outliers in the maximum
assigned process ID, indicating that malicious ﬁles in this dataset try to carry out lots of
longer processes simultaneously, whereas benign ﬁles will carry out a number of quick actions
in succession.

Data preprocessing. Prior to training and classiﬁcation, we normalise the data to improve
model convergence speed in training. By keeping data between 1 and -1, the model is able to
converge more quickly, as the neurons within the network operate within this numeric range
[40]. We achieve this by normalising around the zero mean and unit variance of the training
data. For each feature, i , we establish the mean, µi, and variance, σi, of the training data.
These values are stored, after which every feature, xi is scaled:

xi − µi
σi

9

Total Processes
Max. Process ID
CPU User (%)
CPU System (%)
Memory Use (MB)
Swap Use (MB)
Packets Sent (000s)
Packets Received (000s)
Bytes Received (MB)
Bytes Sent (MB)

Benign
Min. Max.
43
3,020
0
0
941
941
0.3
2.9
4
0.4

57
26,924
100
100
8,387
14,040
110
737
1,116
1,434

Malicious
Min. Max.
44
3,020
0
0
939
941
0.3
2.9
4
0.4

137
5,084
100
100
1,957
1,956
129
192
266
1,188

Table 4: Minimum and maximum values of each input feature for benign and malicious samples

Figure 2: Frequency distributions of input features for benign and malicious samples

10

5. Experimental Results

For reproducibility, the code used to implement the following experiments can be found at
https://github.com/mprhode/malware-prediction-rnn. Information on the data sup-
porting the results presented here, including how to access them, can be found in the Cardiﬀ
University data catalogue at http://doi.org/10.17035/d.2018.0050524986. We used
Keras [41] to implment the RNN experiments, ScikitLearn [42] to implement all other ma-
chine lerning algorithms and trained the models using an Nvidia GTX1080 GPU. The Virtual
Machine used 8GB RAM, 25GB storage, and a single CPU core running 64-bit Windows
7. We installed Python 2.7 on the machine along with a free oﬃce software suite (Libre-
Oﬃce), browser (Google Chrome) and PDF reader (Adobe Acrobat). The virutal machine
was restarted between each sample execution to ensure that malicious and benign ﬁles alike
began from the same machine set-up.

5.1. Hyperparameter conﬁguration

Each layer of a neural network learns an abstracted representation of the data fed in
from the previous layer. There must be a suﬃcient number of neurons in each layer and
a suﬃcient number of layers to represent the distinctions between the output classes. The
network can also learn to represent the training data too closely, causing the model to overﬁt.
Choosing hyperparameters is about ﬁnding a nuanced, but generalisable representation of
the data. Table 2 details the search space and ﬁnal hyperparameters selected for the models
in the later experiments. Although there are only 8 parameters to tune, but there are 576
million diﬀerent possible conﬁgurations. As well as the hyperparameters above, we randomly
select the time into execution of data. Although the goal is to ﬁnd the best classiﬁer for
the shortest amount of time, selecting an arbitrary time such as 5 or 10 seconds into ﬁle
execution may only produce models capable of high accuracy at that sequence length. We
do not know whether a model will increase monotonically in accuracy with more data or
peak at a particular time into the ﬁle execution. Randomising the time into execution
used for training and classiﬁcation reduces the chances of having a blinkered view of model
capabilities.

Without regularisation measures, the representations learned by a neural network can
fail to generalise well. For regularisation, we try using dropout as well as l1 and l2 regu-
larisation on the weight and bias terms in the network in our search space. Dropout [43]
randomly omits a pre-deﬁned percentage of nodes each training epoch, which commonly
limits overﬁtting. l1 regularisation penalises weights growing to large values whilst l2 reg-
ularisation allows a limited number of weights to grow to large values. Our random search
indicated that a dropout rate of 0.1-0.3 produced the best results on the training set, but
weight regularisation was also prevalent in the best-performing conﬁgurations.

Bidirectional RNNs use two layers in every hidden layer, one processing the time series
progressively, and the second processing regressively. Pasacnu et al. [13] found good results
using a bidirectional RNN, as the authors were concerned that the start of a ﬁle’s processes
may be forgotten by a progressive sequence as if the LSTM cell forgets it in favour of new

11

data, the regressive sequence ensures that the initial data remains prevalent in decision-
making. We also found that many of the the best-scoring conﬁgurations used a bidirectional
architecture.

A model depth of 2 or 3 gave the best results. The number of hidden neurons was 50
or more in each layer to give any accuracy above 60%. All conﬁgurations used the “Adam”
weight updating rule [44] as it learns to adjust the rate at which weights are updated during
training.

5.2. Predicting malware using early-stage data

Our goal is to predict malware using behavioural analysis quickly enough that user ex-
perience would not (signiﬁcantly) suﬀer from the time delay. If the model is accurate within
a short time, this sandbox-based analysis could be integrated into an endpoint antivirus
system.

We tested RNNs against other machine learning algorithms used for behavioural malware
classiﬁcation: Random Forest, J48 Decision Tree, Gradient Boosted Decision Trees, Support
Vector Machine (SVM), Naive Bayes, K-Nearest Neighbour and Multi-Layer Perceptron
algorithms (as in [11], [9], [45], [46]). Previous research indicates that Random Forest,
Decision Tree or SVM are likely to perform the best of those considered.

To mimic the challenge of analysing new incoming samples, we have derived a test set
using only the samples that were ﬁrst seen by VirusTotal after 11:15 on 10th October 2017.
This does not account for variants of the same family being present in both the test and
training set, but we explore this question in Section 5.3.

Figure 3 shows the accuracy trend as execution time progresses for the 10-fold cross
validation on the training set and on the test set. Random Forest achieves the highest
accuracy over the 20 seconds of execution on the training set (see Table 5), but the RNN
achieves the highest accuracy on the unseen test set (see Table 6) and outperforms all
other algorithms on the unseen test set after 1 second of execution (see lower graph in
Figure 3). This could be because the training set is quite homogeneous and so relatively
easy for the Random Forest to learn, but it is unable to generalise as well as the RNN to
the completely new ﬁles in the test set. The RNN cannot usefully learn from 0 seconds
as there is no sequence to analyse so accuracy is equivalent to random guess. Using just
1 snapshot (at 0 seconds) of machine activity data, the SVM performs best on the test
set and is able to classify 80% of unseen samples correctly. But after 1 second the RNN
performs consistently better than all other algorithms. Using 4 seconds of data the RNN
correctly classiﬁes 91% of unseen samples, and achieves 96% accuracy at 19 seconds into
execution, whereas the highest accuracy at any time predicted by any other algorithm is 92%
(see Table 7). The RNN improves in accuracy as the amount of sequential data increases.
Although peak accuracy occurs at 19 seconds, the predictive accuracy gains per second
begin to diminish after 4 seconds . From 0 to 4 seconds accuracy improves by 41 percentage
points (11 percentage points from 1 second to 4 seconds) but only by 5 points from 4 to 19
seconds. Our results indicate that dynamic data from just a few seconds of execution can
be used to predict whether or not a ﬁle is malicious. At 4 seconds we are able to accurately
classify 91% of samples, which constitutes an 8 percentage point loss from the state of the

12

art dynamic detection accuracy[12] in exchange for a 04:56 minutes time saved from the
typically documented data recording time per sample (see Table 1), making our model a
plausible addition to endpoint anti-virus detection systems.

Figure 3: Classiﬁcation accuracy for diﬀerent machine learning algorithms and a recurrent neural network
as time into ﬁle execution increases

Accuracy (%) Time (s) FP (%) FN (%)
Classiﬁer
95.29
RandomForest
85.01
MultiLayerPerceptron
86.3
KNearestNeighbors
82.39
SVM
93.41
DecisionTree
83.94
AdaBoost
NaiveBayes
77.44
GradientBoostedDecisionTrees 95.81
87.75
RNN

5.03
21.3
17.53
24.5
7.87
19.78
29.78
5.44
10.93

4.5
9.83
10.96
10.62
5.72
12.03
10.7
3.32
15.15

19
20
20
10
20
2
2
19
20

Table 5: Highest average accuracy over 10-fold cross validation on training set during ﬁrst 20 seconds of
execution with corresponding false positive rate (FP) and false negative rate (FN)

13

Classiﬁer
RandomForest
MultiLayerPerceptron
KNearestNeighbors
SVM
DecisionTree
AdaBoost
NaiveBayes
GradientBoostedDecisionTrees
RNN

Accuracy (%) Time (s) FP (%) FN (%)
92.05
91.07
90.38
90.57
89.17
87.82
76.25
92.62
96.01

12.29
12.98
15.12
14.39
17.22
17.72
21.13
11.08
4.72

4.29
5.53
4.66
5.13
5.22
7.24
24.74
4.33
3.17

20
18
18
20
12
19
0
20
19

Table 6: Highest accuracy on unseen test set during ﬁrst 20 seconds of execution with corresponding false
positive rate (FP) and false negative rate (FN)

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

80

85

87

91

93

94

93

95

95

94

95

94

95

95

95

95

94

95

96

93

12

14

16

14

10

33

17

9

2

2

9

3

10

2

5

3

7

2

9

2

6

2

9

2

6

4

7

3

7

2

6

4

9

3

7

3

5

3

7

5

Time
(s)
Acc.
(%)
FN
(%)
FP
(%)

Table 7: RNN prediction Accuracy (Acc.), false negative rate (FN) and false positive rate (FP) on test set
from 1 to 20 seconds into ﬁle execution time

5.3. Simulation of zero-day malware detection

Dividing the test and training set by date ensures that the two groups are distinct sets
of ﬁles. However, a slight variant on a known strain is technically a new ﬁle. We were able
to extract information about the malware families and variants and want to test how well
the model performs when confronted with a completely new family or variant.

Table 8 gives the numbers in the test set for the families and those variants for which there
were more than 100 instances in the dataset. Dinwod, Eldorado, Zusy and Wisdomeyes are
Trojans; Kazy and Scar are Viruses. We also collected all of those variants listed as advanced
persistent threats (APTs) for as signature based systems struggle to detect these especially if
previously unseen. The APTs and some of the high-level families have less than 100 samples
and as such the results are unlikely to be indicative for the general population of that family
but we test them anyway for comparison.

To avoid contamination from those samples that were disputed, these are removed from
the dataset for the following experiments. For each family in Table 8, we trained a completely
new model without any samples from the family of interest.

The test set is entirely malicious, which means accuracy is an appropriate metric as it is
just the rate of correct detection from the only class of interest. Table 9 gives the predictive
accuracy over time for diﬀerent families and for APTs, and Table 10 gives the predictive

14

Family/Variant
Trojan
Virus
Adware
Backdoor
Bot
Worm
Rootkit
Dinwod
Artemis
Eldorado
Zusy
Wisdomeyes
Kazy
Scar
APTs

Total
1,382 (0)(76)
407 (20)(56)
180 (0)(51)
123 (7)(0)
76
24
11
265
228
209
135
132
116
101
27

Table 8: Test accuracy diﬀerence between family omitted and included in training set

accuracies for the ﬁve variants for which we collected over 100 samples. Perhaps surprisingly,
we see high classiﬁcation accuracies across these two sets of results. The families are detected
with lower accuracy in general. For the Trojans particularly, during the ﬁrst few seconds,
accuracy is actually worse than random chance. Because so much of the dataset set is
comprised of Trojans, removing these from training halves the number of malware samples,
so this may account for the particularly poor performance. The accuracy does increase
signiﬁcantly between 1 and 3 seconds of execution. This is probably because Trojans are
deﬁned by their delivery mechanism, and the model has not been trained on any examples of
this form of malware delivery. The model has, however, seen malicious behaviour from other
families, which may be similar to some of the later behaviours by the Trojans, accounting
for the signiﬁcant rise in accuracy. Though the Worms are actually detected with a 100%
accuracy at each second, there were only 24 Worm samples in the dataset.

Family

Trojan
Virus
Adware
Backdoor
Bot
Worm
Rootkit
APT

1
11.16
91.26
90.68
91.3
93.06
100.0
100.0
96.3

2
49.67
89.58
90.0
91.21
91.55
100.0
75.0
96.3

3
70.23
82.7
83.33
80.0
92.86
100.0
75.0
88.46

Time(s)

4
68.07
83.0
84.11
83.53
84.85
100.0
75.0
92.0

5
73.86
83.54
59.59
82.28
90.16
100.0
100.0
100.0

6
69.33
88.89
85.71
79.73
85.71
100.0
75.0
94.74

7
55.63
84.56
87.22
87.32
80.0
100.0
100.0
94.74

8
57.75
86.31
66.41
82.61
86.36
100.0
100.0
100.0

9
60.18
84.38
77.31
79.69
88.1
100.0
66.67
94.74

10
56.24
82.26
73.5
80.7
87.5
100.0
100.0
89.47

Table 9: Classiﬁcation accuracy on diﬀerent malware families with all instances of that family removed from
training set

15

Variant

1
90.57
94.3

Dinwod
Eldorado
Wisdomeyes 92.59
91.18
Zusy
89.74
Kazy
92.08
Scar

2
89.43
93.3
90.91
89.63
82.76
92.08

3
78.11
92.0
83.72
85.94
85.22
75.25

Time(s)

4
91.32
86.42
91.34
82.11
86.49
78.22

5
93.96
90.07
89.83
81.74
87.88
62.63

6
98.87
82.01
92.63
85.19
94.94
81.82

7
99.25
74.81
94.44
85.29
87.5
89.69

8
98.11
81.75
84.52
88.66
88.89
81.44

9
98.08
85.48
90.36
90.43
91.43
86.46

10
97.31
83.61
87.34
85.56
89.71
88.42

Table 10: Classiﬁcation accuracy on diﬀerent malware variants with all instances of that variant removed
from training set

Figure 4: Comparative detection accuracy on various malware families with examples of the family omitted
from the training set

16

Figure 5: Comparative detection accuracy on various malware variants with examples of the variant omitted
from the training sett

The variants tend to achieve a higher predictive accuracy than the families. Other than
Dinwod, all families score lower at 10 seconds than at 1 second. Each variant is a kind of
Trojan or Virus, but the model was trained on other types of Trojan and Virus. This can help
explain the slight drop in accuracy over the ﬁrst 10 seconds. It is the delivery mechanism
which the variants have in common with samples in the training set, so the period over
which this occurs (the ﬁrst few seconds) gives the best predictive accuracy. Every variant
was detected with over 89% accuracy during the ﬁrst second of execution, despite the model
having no exposure to that variant previously.

If the model is able to score well on a family without ever having seen a sample from
that family, the model may hold a robustness against zero days, and support our hypothesis
that malware do not exhibit wildly diﬀerent behavioural activity from one another as their
goals are not wildly divergent, even if the attack vector mechanisms are.

5.4. Ransomware Case Study

Early prediction that a sample is malicious enables defensive techniques to move from
recovery to prevention. This is particularly desirable for malware such as ransomware, from
which data recovery is only possible by paying a ransom if a backup does not exist. We
obtained an additional 2,788 ransomware samples from the VirusShare website [39] to test
the predictive capability of our model.

Reports in the wake of the high proﬁle ransomware attacks, e.g. WannaCry/WannaDecryptor

worm in May 2017, were reported to be preventable if a patch released two months earlier
had been installed [47]. Endpoint users cannot be relied on to carry out security updates
as the primary defence against new malware. We test our method by removing the 183

17

ransomware samples and the disputed-family samples from our original dataset and train a
new model on the remaining samples, we then test how well the model is able to detect the
VirusShare samples and the removed 183 samples.

The model is able to detect 94% of samples at 1 second into execution without having
seen any ransomware previously. When we include half of the ransomware samples in the
training set, this rises to 99.86% (see Table 11).

Samples
Training Set

in

Omitted
Half included

Time(s)

1
94.19
99.86

2
93.72
99.1

3
90.94
97.96

4
92.02
98.83

5
86.77
98.29

6
92.46
97.89

7
89.55
98.78

8
87.62
99.29

9
77.88
97.96

10
87.52
96.46

Table 11: Classiﬁcation accuracy on ransomware for one model which has not been trained on ransomware
(omitted), and for one which has (half included)

In Figure 6 there is a clear distinction in the accuracy trend over execution time between
the model which has been trained on some of the relevant family. The model which has
never seen ransomware before starts to drop in accuracy after the initial few seconds. Again
we believe this is because the model is recognising the delivery mechanism at the start of
execution, which will be common to other types of malware in the training set, though
the later malicious behaviour is is less recognisable to the model by comparison with the
later behaviour of the other types of malware it has seen. The model trained with half of
the samples knows how ransomware behaves after a few seconds and so maintains a high
detection accuracy.

It would be interesting to see if the model at 1 second and the model at 5 seconds rely on
diﬀerent input features to reach accurate predictions. It is diﬃcult to penetrate the decision
making process of a neural network; the architecture presented here has 1,344 neurons almost
4 million trainable parameters, but we can turn the input features on and oﬀ and see the
eﬀect of combinations of features on classiﬁcation accuracy. By setting the inputs to zero,
which is the normalised mean of the training data, we can turn a feature “oﬀ”. By turning
oﬀ all the features and then turning them back on sequentially, we can see which features
are needed to gain a certain level of accuracy.

In Table 12, we can see that with just two features, both the 1 second and the 5 sec-
onds models trained with and without ransomware are able to beat 50% accuracy (random
chance). The model trained using ransomware is able to correctly detect more than 99%
of ransomware samples as malicious using just the number of packets sent and either the
number of packets or number of bytes received. Unlike the model trained with ransomware,
which draws accurate conclusions from packet data and total processes, when no ransomware
is included in the training set, memory usage is also a prominent feature in accurate de-
tection. Comparing to the broader families, in classifying Adware, Trojans and Viruses,
memory and packets a single input feature allowed the model to achieve more than 50%
accuracy, Trojans are the only family for which memory contributes to scoring above 50%
at the one-second model, when combined with packets sent and swap. As Trojans comprise

18

Figure 6: Classiﬁcation accuracy on ransomware for one model which has not been trained on ransomware
(omitted), and for one which has (half included)

the majority of the dataset it makes sense that the most relevant features for classifying
them help to deﬁne what constitutes malware to the model.

The accuracy in identifying unseen families highlights the presence of shared dynamic
characteristics between diﬀerent malware types. The broad families, which detail the mal-
ware infection mechanism particularly help to identify malware early on. Whilst new mal-
ware variants are likely to appear, new delivery mechanisms are far less common and help
to distinguish unseen families from benignware.

5.5. Improving prediction accuracy with an ensemble classiﬁer

As well as accuracy, the values of the model predictions increase with time into ﬁle exe-
cution. Therefore we now propose an ensemble method, using the top three best performing
conﬁgurations found in the hyperparameter search space during the previous experiments,
to try and improve the classiﬁcation conﬁdence earlier in the ﬁle execution. Accuracy does
not increase monotonically in our ﬁrst conﬁguration, and of the best three conﬁgurations on

19

# Fea-
tures
on

1

2

Ransomware omitted from training set
1 second model

5 second model

Ransomware in training set
1 second model

5 second model

Max.
Acc.
00.03

98.92

Features
on
tx bytes

Max.
Acc.
40.82

Features
on
memory

Max.
Acc.
89.36

97.54

memory
and
rx
bytes

99.80

rx bytes
and
rx
packets

Max.
Acc.
14.95

71.15

Features
on
total
pro-
cesses
rx bytes
and
tx
bytes

Features
on
rx pack-
ets

tx pack-
ets
and
{rx
pack-
ets,
bytes}

rx

Table 12: Maximum accuracy scores in predicting ransomware with only one and two features turned on for
a model not trained on ransomware and for a model trained on ransomware

the 10-fold cross-validation, no single conﬁguration consistently achieved the highest accu-
racy at each second, the conﬁguration used in the previous sections was the conﬁguration
that scored the highest accuracy at 1 second.

We take the best-scoring conﬁgurations on the training set across the ﬁrst 5 seconds,
which are 3 distinct hyperparameter sets (one model was the best at 1 and 2 seconds, one
at 3 and 5 seconds) and take the maximum of the predictions of these three RNNs before
thresholding at 0.5 to give a ﬁnal malicious/benign label. The conﬁguration details are in
Table 13, conﬁguration “A” is the same as has been used in the previous experiments.

B
1

A
Hyperparameter
3
Depth
True True
Bidirectional
74
Hidden neurons
53
Epochs
0.3
Dropout rate
Weight regularisation l2
Bias regularisation
Batch size

358
112
0.1
l2

64

None None None
64

64

C
2
False
195
39
0.1
l1

Table 13: Highest accuracy-scoring conﬁgurations during ﬁrst 5 seconds in 10-fold cross validation on training
set

To combine the predictions of conﬁgurations A, B and C we take the maximum value
of the three to bias the predictions in favour of detecting malware (labelled as 1) over
benignware (labelled as 0). An ensemble of models does tend to boost accuracy, increasing
detection from 92% to 94% at 5 seconds, and the maximum accuracy from conﬁguration A

20

alone, 96%, is reached at 9 seconds instead of at 19 seconds (see Table 14). The results in
Table 14 show that the accuracy score improves or matches the highest scoring model of
conﬁgurations A, B and C for 12 of the ﬁrst 20 seconds. Model A, the original conﬁguration,
only bests the ensemble accuracy once. We tested whether the ensemble scores improved
predictive conﬁdence on the individual samples compared with the predictions of the best-
scoring model. We can measure predictive conﬁdence by rewarding those correct predictions
closer to 1 or 0 more highly, i.e. a prediction of 0.9 is better than 0.8 when the sample is
malicious. The equation for predictive conﬁdence is as follows:

where b is the true label and p is the predicted label.

conf idence = 1 − |b − p|

Time (s) Highest accuracy
of conﬁgurations
A, B and C
79.69 (C)
85.6 (A)
87.52 (A, C)
91.54 (A)
92.38 (B)
94.09 (A)
94.92 (A)
94.25 (A)
94.97 (A)
95.53 (C)
95.91 (C)
95.46 (C)
95.16 (A)
95.93 (C)
96.1 (C)
95.62 (C)
95.34 (C)
96.67 (C)
96.51 (C)
93.81 (A)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

Ensemble acc. (%) Ensemble FP (%) Ensemble FN (%)

79.5
83.69*
88.48*
91.92*
93.95*
95.28*
95.12*
95.48*
96.02*
95.11*
96.13*
95.46*
95.6*
95.93*
95.87*
96.54*
96.5*
96.43*
96.26*
94.85*

33.5
25.73
15.05
8.74
3.4
4.37
4.85
4.88
4.39
5.45
4.95
5.47
5.97
5.03
4.57
4.08
3.06
4.12
4.23
8.22

12.03
10.16
9.21
7.64
7.84
4.97
4.9
4.26
3.68
4.48
3.04
3.82
3.15
3.29
3.77
2.94
3.86
3.1
3.3
3.31

Table 14: Ensemble accuracy (acc.), false positive rate (FP) and false negative rate (FN) compared with
highest accuracy of conﬁgurations A, B and C. Those marked with a “*” signify predictions that were
statistically signiﬁcantly more conﬁdent by at the conﬁdence level of 0.01

Using a one-sided T-test, we found that the conﬁdence of predictions from the ensemble
method were signiﬁcantly higher (at 0.01 conﬁdence level) for every second after 1 second,
malicious predictions are likely be more conﬁdent as we are taking the maximum value of the
three models, but it is interesting that taking the maximum of the benign samples does not

21

out weigh the increase in conﬁdence. This indicates that three models are more conﬁdent
about benign samples than malicious ones. A further beneﬁt of the ensemble approach is
the reduction in the false negative rate. The minimum false negative rate for Model A was
4.5%, but here the false positive rate is at least 3 percentage points lower than for model
A during the ﬁrst 7 seconds, and remains lower than Model A’s global minimum for the
remaining 20 seconds.

If the gains in accuracy for the ensemble classiﬁer are due to diﬀerences in the features
learned by the network, this could help to protect against adversarial manipulation of data.
We attempt to interpret what conﬁgurations A, B, and C are using to distinguish malware
and benignware. These preliminary tests seek to gauge whether it is possible to analyse the
decisions made by the trained neural networks.

By setting the test data for a feature (or set of features) to zero, we can approximate
the absence of that information between samples. We assess the overall impact of turning
features “oﬀ” by observing the fall in accuracy and dividing it by the number of features
turned oﬀ. A single feature incurring a 5 percentage point loss attains an impact factor of
-5, but two features creating the same loss would be awarded -2.5 each. Finally, we take the
average across impact scores to assess the importance of each feature when a given number
of features are switched oﬀ.

Figure 7 gives the impact factors for each feature at 4 seconds into ﬁle execution. Intu-
itively, the more features omitted, the higher the impact factors become. Interestingly, there
are some very small gains in accuracy for conﬁgurations A and B when only one feature is
missing but no more than 0.2 percentage points. For each of the conﬁgurations, CPU use
on the system has the highest impact factor. It is most integral for conﬁguration A, which
is also the best-scoring model. The CPU use in conﬁguration A does not really see an in-
crease in its impact factor as we remove more input features, but for conﬁguration B, all
features attain higher impact factors the more are removed. We can infer that conﬁguration
B has learned a representation of the data which combines the inputs to decide whether the
output is malicious or benign, whereas conﬁguration A appears to have learned at least one
representation of CPU system use as a predictor of malware.

The diﬀerence between the impact scores and their emphasis can help us to see which
features are most predictive at diﬀerent time steps (at 4 seconds this is CPU usage) and to
understand how an ensemble classiﬁer is able to outperform the predictions of its compo-
nents. As all three models suﬀer the biggest loss from CPU usage, if an adversary knew this
she might be able to manipulate CPU system use to avoid detection. Future work should
examine the decision processes of networks to detect potential weaknesses that could be
exploited to evade detection. The ensemble oﬀers a small increase in accuracy but more
importantly, this analysis can help to understand ways in which the models may be manip-
ulated, by biasing results towards malicious predictions (taking the maximum prediction)
we introduce a form of safety-net against the manipulation of a single model.

22

6. Limitations and Future work

Our results indicate that behavioural data can provide a good indication of whether or
not a ﬁle is malicious based only on its initial behaviours, even when the model has not
been exposed to a particular malware variant before. Dynamic analysis could reasonably
be incorporated into endpoint antivirus systems if the analysis only takes a few seconds per
ﬁle. Further challenges which must be addressed before this is possible include:

6.1. Other ﬁle types and operating systems

So far we have only examined Windows7 executables. Though Windows7 is the most
prevalent operating system globally [48] and Windows executables are the most commonly
submitted ﬁle to VirusTotal [1], we should extend these methods to see if the model is
capable of detecting malicious PDFs, URLs and other potential vehicles for malware, as
well as applications which run on other operating systems.

6.2. Robustness to adversarial samples

The robustness of this approach is limited if adversaries know that the ﬁrst 5 seconds
are being used to determine whether a ﬁle will run in the network. By planting long sleeps
or benign behaviour at the start of a malicious ﬁle, adversaries could avoid detection in
the virtual machine. We hypothesised that malicious executables begin attempting their
objectives as soon as possible to mitigate the chances of being interrupted, but this would
be likely to change if malware authors knew that only subsections of activity were the basis of
anti-virus system decisions. We envisage future work examining a sliding-window approach
to behavioral prediction.

The sliding-window approach will take snapshots (of 5 seconds) of data and monitor
machine activity on a per-process basis to try and predict whether or not a ﬁle is malicious.
This would run in the background as the ﬁle is executed in a live environment. The advantage
of this approach is that we eliminate the waiting time before a user is allowed to access the
ﬁle. The challenges in implementing these next steps are recalibration for endpoint machines
(see Section 6.4 below) and suﬃciently quick killing of the malicious process once it has been
detected, i.e. before the malicious payload is executed.

Despite the future worry that executables could be amended to avoid detection by the
model proposed in this paper, this does not invalidate the use of our proposed method.
Whilst some attacks may be altered speciﬁcally to evade an behavioral early-detection sys-
tem, this would be in response the attacker knowing that the target in question was employ-
ing these types of defence. However, there would still be many malwares without benign
behaviour injections at the start of the ﬁle. We continue to use signature-based detection
in antivirus systems despite the use of static obfuscation techniques, because it is still an
invaluable method for quickly detecting previously seen malwares. The model proposed here
indicates that we can quickly detect unseen variants, and we hope that future research will
evaluate the robustness of the sliding window approach using adversarially crafted samples.

23

6.3. Process blocking

If a live monitoring system is implemented, processes predicted to be malicious will need
to be terminated. Future work should examine the ability of the model to block once the
classiﬁer anticipates malicious activity, and to investigate whether the malicious payload has
been executed.

6.4. Portability to other machines and operating systems

The machine activity metrics are speciﬁc to the context of the virtual machine used in this
experiment. To move towards adoption in an endpoint anti-virus system, the RNN should
be retrained on the input data generated by a set of samples on the target machine. Though
this recalibration will take a few hours at the start of the security system installation, it will
only need to be performed when hardware is upgraded (once per machine for most users)
and opens the possibility of porting the model to other operating systems, including other
versions of Windows.

Though we have not tested the portability of the data between machines, i.e. training
with data recorded on one machine and testing with data recorded on another, it is easy to see
cases in which this will not work. Some metrics, such as CPU usage are relative (measured
as a percentage of total available processing power) and so will change dramatically with
hardware capacities. For example, a ﬁle requiring 100% of CPU capacity on one machine
may use just 30% on another with more cores. However, we see no reason why the model
cannot be re-calibrated to a new machine. There is cause for concern if the hardware means
that the granularity of the data falls below that which is used in this paper. For example a
very small amount of RAM could limit the memory usage such that the useful information
that one sample uses 1.1MB and another 1.2MB are both capped at 1MB, thus appearing
the same to the model. Whilst the experiments in this paper are conducted in a virtual
machine and the memory, storage and processing power can be replicated, we hope that
future work will extend this model to run live in the background on the intended recipient
machine. Since the hardware capacities of a typical modern computer are greater than those
for the virtual machine used here, this may in turn provide more granularity in the data and
possibly allow the model to learn a better representation of the diﬀerence between malicious
and benign software. The diﬀerent results that we would be likely to see on a more powerful
machine oﬀer a potential advantage in training but also necessitate re-calibration on a per-
machine basis. Since this is a one-oﬀ time cost, it is not a major limitation of the proposed
solution.

7. Conclusions

Dynamic malware detection methods are often preferred to static detection as the latter
are particularly susceptible to obfuscation and evasion when attackers manipulate the code
of an executable ﬁle. However, dynamic methods previously incurred a time penalty due
to the need to execute the ﬁle and collect its activity footprint before making a decision
on its malicious status. This meant the malicious payload had likely already been executed
before the attack was detected. We have developed a novel malware prediction model based

24

on recurrent neural networks (RNNs) that signiﬁcantly reduces dynamic detection time, to
less than 5 seconds per ﬁle, whilst retaining the advantages of a dynamic model. This oﬀers
the new ability to develop methods that can predict and block malicious ﬁles before they
execute their payload completely, preventing attacks rather than having to remedy them.

Through our experimental results we have shown that it is possible to achieve a detection
accuracy of 94% with just 5 seconds of dynamic data using an ensemble of RNNs and an
accuracy of 96% in less than 10 seconds, whilst typical ﬁle execution time for dynamic
analysis is around 5 minutes.

The best RNN network conﬁgurations discovered through random search each employed
bidirectional hidden layers, indicating that making use of the input features progressing as
well as regressing in time aided distinction between malicious and benign behavioural data.
A single RNN was capable of detecting completely unseen malware variants with over
89% accuracy for the 6 diﬀerent variants tested at just 1 second into ﬁle execution. The
accuracy tended to fall a little after the ﬁrst 2 seconds, implying that the model was best
able to recognise the infection mechanism at a family level (e.g. Trojan, Virus) given that
this would be the ﬁrst activity to occur. The RNN was less accurate at detecting malware at
a family level when that family had been omitted from the training data (11% accuracy at
1 second detecting Trojans), further indicating that the model was easily able to detect new
variants, provided it had been exposed to examples of that family of infection mechanisms.
Our ransomware use case experiment supported this theory further, as the RNN was able to
detect ransomware, which shares common infection mechanisms with other types of attack
such as Trojans, with 94% accuracy, without being exposed to any ransomware previously.
However, this accuracy fell as time into ﬁle execution increased, again implying that the
model was easily able to detect a malicious delivery mechanism, better than the activity
itself. After exposure to ransomware, the model accuracy remained above 96% for the ﬁrst
10 seconds.

The RNN models outperformed other machine learning classiﬁers in analysing the unseen
test set, though the other algorithms performed competitively on the training set. This
indicates that the RNN was more robust against overﬁtting to the training set than the
other algorithms and had learnt a more generalisable representation of the diﬀerence between
malicious and benign ﬁles. This is particularly important in malware detection as adversaries
are constantly developing new malware strains and variants in an attempt to evade automatic
detection.

To date this is the ﬁrst analysis of the extent to which general malware executable ﬁles
can be predicted to be malicious during its execution rather than using the complete log
ﬁle post-execution, we anticipate that future work can build on these results to integrate
ﬁle-speciﬁc behavioural detection into endpoint anti-virus systems across diﬀerent operating
systems.

References

[1] VirusTotal, Statistics - virustotal, [Data for 26 April 2017] (2017).

URL https://www.virustotal.com/en/statistics/

25

[2] P. Vinod, R. Jaipur, V. Laxmi, M. Gaur, Survey on malware detection methods, in: Proceedings of
the 3rd Hackers’ Workshop on computer and internet security (IITKHACK’09), 2009, pp. 74–79.
[3] I. You, K. Yim, Malware obfuscation techniques: A brief survey, in: 2010 International Conference on
Broadband, Wireless Computing, Communication and Applications, 2010, pp. 297–300. doi:10.1109/
BWCCA.2010.85.

[4] L. Nataraj, V. Yegneswaran, P. Porras, J. Zhang, A comparative assessment of malware classiﬁcation
using binary texture analysis and dynamic analysis, in: Proceedings of the 4th ACM Workshop on
Security and Artiﬁcial Intelligence, AISec ’11, ACM, New York, NY, USA, 2011, pp. 21–30. doi:
10.1145/2046684.2046689.
URL http://doi.acm.org/10.1145/2046684.2046689

[5] A. Damodaran, F. D. Troia, C. A. Visaggio, T. H. Austin, M. Stamp, A comparison of static, dynamic,
and hybrid analysis for malware detection, Journal of Computer Virology and Hacking Techniques
13 (1) (2017) 1–12. doi:10.1007/s11416-015-0261-z.

[6] K. Grosse, N. Papernot, P. Manoharan, M. Backes, P. D. McDaniel, Adversarial examples for mal-
ware detection, in: Computer Security - ESORICS 2017 - 22nd European Symposium on Research
in Computer Security, Oslo, Norway, September 11-15, 2017, Proceedings, Part II, 2017, pp. 62–79.
doi:10.1007/978-3-319-66399-9_4.
URL https://doi.org/10.1007/978-3-319-66399-9_4

[7] J. Saxe, K. Berlin, Deep neural network based malware detection using two dimensional binary program
features, in: 2015 10th International Conference on Malicious and Unwanted Software (MALWARE),
2015, pp. 11–20. doi:10.1109/MALWARE.2015.7413680.

[8] S. Tobiyama, Y. Yamaguchi, H. Shimada, T. Ikuse, T. Yagi, Malware detection with deep neural
network using process behavior, in: 2016 IEEE 40th Annual Computer Software and Applications
Conference (COMPSAC), Vol. 2, 2016, pp. 577–582. doi:10.1109/COMPSAC.2016.151.

[9] I. Firdausi, C. lim, A. Erwin, A. S. Nugroho, Analysis of machine learning techniques used in behavior-
based malware detection, in: 2010 Second International Conference on Advances in Computing, Control,
and Telecommunication Technologies, 2010, pp. 201–203. doi:10.1109/ACT.2010.33.

[10] F. Ahmed, H. Hameed, M. Z. Shaﬁq, M. Farooq, Using spatio-temporal information in api calls with
machine learning algorithms for malware detection, in: Proceedings of the 2nd ACM Workshop on
Security and Artiﬁcial Intelligence, ACM, 2009, pp. 55–62.

[11] R. Tian, R. Islam, L. Batten, S. Versteeg, Diﬀerentiating malware from cleanware using behavioural
analysis, in: Malicious and Unwanted Software (MALWARE), 2010 5th International Conference on,
IEEE, 2010, pp. 23–30.

[12] W. Huang, J. W. Stokes, Mtnet: A multi-task neural network for dynamic malware classiﬁcation,
in: Proceedings of the 13th International Conference on Detection of Intrusions and Malware, and
Vulnerability Assessment - Volume 9721, DIMVA 2016, Springer-Verlag New York, Inc., New York,
NY, USA, 2016, pp. 399–418. doi:http://dx.doi.org/10.1007/978-3-319-40667-1\_20.

[13] R. Pascanu, J. W. Stokes, H. Sanossian, M. Marinescu, A. Thomas, Malware classiﬁcation with re-
current networks, in: 2015 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2015, pp. 1916–1920. doi:10.1109/ICASSP.2015.7178304.

[14] S. S. Hansen, T. M. T. Larsen, M. Stevanovic, J. M. Pedersen, An approach for detection and family
classiﬁcation of malware based on behavioral analysis, in: 2016 International Conference on Computing,
Networking and Communications (ICNC), 2016, pp. 1–5. doi:10.1109/ICCNC.2016.7440587.

[15] B. Kolosnjaji, A. Zarras, G. Webster, C. Eckert, Deep learning for classiﬁcation of malware system call
sequences, in: Australasian Joint Conference on Artiﬁcial Intelligence, Springer, 2016, pp. 137–149.
[16] T. Shibahara, T. Yagi, M. Akiyama, D. Chiba, T. Yada, Eﬃcient dynamic malware analysis based on
network behavior using deep learning, in: 2016 IEEE Global Communications Conference (GLOBE-
COM), 2016, pp. 1–7. doi:10.1109/GLOCOM.2016.7841778.

[17] M. Neugschwandtner, P. M. Comparetti, G. Jacob, C. Kruegel, Forecast: skimming oﬀ the malware
cream, in: Proceedings of the 27th Annual Computer Security Applications Conference, ACM, 2011,
pp. 11–20.

26

[18] U. Bayer, E. Kirda, C. Kruegel, Improving the eﬃciency of dynamic malware analysis, in: Proceedings

of the 2010 ACM Symposium on Applied Computing, ACM, 2010, pp. 1871–1878.

[19] B. Kolosnjaji, A. Zarras, G. Webster, C. Eckert, Q. Bai, Deep Learning for Classiﬁcation of Malware
System Call Sequences, Springer International Publishing, Cham, 2016, pp. 137–149. doi:10.1007/
978-3-319-50127-7\_11.
URL http://dx.doi.org/10.1007/978-3-319-50127-7_11

[20] N. Scaife, H. Carter, P. Traynor, K. R. Butler, Cryptolock (and drop it): stopping ransomware attacks
on user data, in: Distributed Computing Systems (ICDCS), 2016 IEEE 36th International Conference
on, IEEE, 2016, pp. 303–312.

[21] A. Continella, A. Guagnelli, G. Zingaro, G. De Pasquale, A. Barenghi, S. Zanero, F. Maggi, Shieldfs: a
self-healing, ransomware-aware ﬁlesystem, in: Proceedings of the 32nd Annual Conference on Computer
Security Applications, ACM, 2016, pp. 336–347.

[22] I. Rosenberg, E. Gudes, Bypassing system calls–based intrusion detection systems, Concurrency and

Computation: Practice and Experience 29 (16).

[23] I. Rosenberg, A. Shabtai, L. Rokach, Y. Elovici, Generic black-box end-to-end attack against rnns and

other API calls based malware classiﬁers, CoRR abs/1707.05970. arXiv:1707.05970.
URL http://arxiv.org/abs/1707.05970

[24] P. Burnap, R. French, F. Turner, K. Jones, Malware classiﬁcation using self organising feature maps

and machine activity data, Computers & Security 73 (2018) 399–410.

[25] D. Oktavianto, I. Muhardianto, Cuckoo Malware Analysis, Packt Publishing, 2013.
[26] P. S. Foundation, Psutil python library (2017).
URL https://pypi.python.org/pypi/psutil

[27] Z. C. Lipton, A critical review of recurrent neural networks for sequence learning, CoRR abs/1506.00019.

URL http://arxiv.org/abs/1506.00019

[28] Y. Bengio, P. Simard, P. Frasconi, Learning long-term dependencies with gradient descent is diﬃcult,

IEEE transactions on neural networks 5 (2) (1994) 157–166.

[29] K. Greﬀ, R. K. Srivastava, J. Koutn´ık, B. R. Steunebrink, J. Schmidhuber, Lstm: A search space

odyssey, IEEE transactions on neural networks and learning systems.

[30] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, Y. Bengio, Learning
phrase representations using rnn encoder–decoder for statistical machine translation, in: Proceedings
of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association
for Computational Linguistics, Doha, Qatar, 2014, pp. 1724–1734.
URL http://www.aclweb.org/anthology/D14-1179

[31] J. Chung, C. Gulcehre, K. Cho, Y. Bengio, Empirical evaluation of gated recurrent neural networks on

sequence modeling, in: NIPS 2014 Workshop on Deep Learning, December 2014, 2014.

[32] J. Bergstra, Y. Bengio, Random search for hyper-parameter optimization, Journal of Machine Learning

Research 13 (Feb) (2012) 281–305.

[33] G. Sood, virustotal: R Client for the virustotal API, r package version 0.2.1 (2017).
[34] Softonic.com (2017).

URL https://en.softonic.com/

[35] Porteableapps.com (2017).

URL https://portableapps.com/

[36] Sourceforge.net (2017).

URL https://sourceforge.net/

[37] Z. Yuan, Y. Lu, Y. Xue, Droiddetector: android malware characterization and detection using deep

learning, Tsinghua Science and Technology 21 (1) (2016) 114–123.

[38] M. Imran, M. T. Afzal, M. A. Qadir, Using hidden markov model for dynamic malware analysis:
First impressions, in: 2015 12th International Conference on Fuzzy Systems and Knowledge Discovery
(FSKD), 2015, pp. 816–821. doi:10.1109/FSKD.2015.7382048.

[39] Virusshare.com (2017).

URL https://virusshare.com/

27

[40] Y. A. LeCun, L. Bottou, G. B. Orr, K.-R. M¨uller, Eﬃcient backprop, in: Neural networks: Tricks of

the trade, Springer, 2012, pp. 9–48.

[41] F. Chollet, Keras (2015).

URL https://github.com/fchollet/keras

[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay,
Scikit-learn: Machine learning in python, Journal of Machine Learning Research 12 (2011) 2825–2830.
[43] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout: a simple way
to prevent neural networks from overﬁtting., Journal of Machine Learning Research 15 (1) (2014)
1929–1958.

[44] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, CoRR abs/1412.6980. arXiv:

1412.6980.
URL http://arxiv.org/abs/1412.6980

[45] W.-C. Wu, S.-H. Hung, Droiddolphin: a dynamic android malware detection framework using big data
and machine learning, in: Proceedings of the 2014 Conference on Research in Adaptive and Convergent
Systems, ACM, 2014, pp. 247–252.

[46] Y. Fang, B. Yu, Y. Tang, L. Liu, Z. Lu, Y. Wang, Q. Yang, A new malware classiﬁcation approach
based on malware dynamic analysis, in: J. Pieprzyk, S. Suriadi (Eds.), Information Security and
Privacy, Springer International Publishing, Cham, 2017, pp. 173–189.

[47] D. o. H. UK Government National Audit Oﬃce, Investigation: Wannacry cyber attack and the nhs

(Oct 2017).
URL https://www.nao.org.uk/wp-content/uploads/2017/10/Investigation-WannaCry-cyber-attack-and-the-NHS.
pdf

[48] NetMarketShare.com, Windows7 market share (2017).

URL https://www.netmarketshare.com/windows-market-share?options=

28

Figure 7: Impact scores for features with 1, 2 and 3 features turned oﬀ 4 seconds into ﬁle execution

29

