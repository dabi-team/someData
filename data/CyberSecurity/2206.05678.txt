2
2
0
2

n
u
J

2
1

]

C
D
.
s
c
[

1
v
8
7
6
5
0
.
6
0
2
2
:
v
i
X
r
a

Security of Machine Learning-Based Anomaly
Detection in Cyber Physical Systems

Zahra Jadidi∗, Shantanu Pal∗, Nithesh Nayak K∗, Arawinkumaar Selvakkumar∗, Chih-Chia Chang∗,
Maedeh Beheshti§§, Alireza Jolfaei∗∗
∗School of Computer Science, Queensland University of Technology, Brisbane, QLD 4000, Australia
§§Critical Path Institute, Tucson, AZ 85718, USA
∗∗School of Computer Science, Macquarie University, Sydney, NSW 2109, Australia
zahra.jadidi@qut.edu.au, shantanu.pal@qut.edu.au, nithesh.nayakkinnimulky@connect.edu.au, mbeheshti@c-path.org,
chihchia.chang@connect.qut.edu.au, arawinkumaar.selvakkumar@connect.qut.edu.au, alireza.jolfaei@mq.edu.au

Abstract—With the emergence of the Internet of Things (IoT)
and Artiﬁcial Intelligence (AI) services and applications in the
Cyber Physical Systems (CPS), the methods of protecting CPS
against cyber threats is becoming more and more challenging.
Various security solutions are implemented to protect CPS net-
works from cyber attacks. For instance, Machine Learning (ML)
methods have been deployed to automate the process of anomaly
detection in CPS environments. The core of ML is deep learning.
However, it has been found that deep learning is vulnerable to
adversarial attacks. Attackers can launch the attack by applying
perturbations to input samples to mislead the model, which
results in incorrect predictions and low accuracy. For example,
the Fast Gradient Sign Method (FGSM) is a white-box attack
that calculates gradient descent oppositely to maximize the loss
and generates perturbations by adding the gradient to unpolluted
data. In this study, we focus on the impact of adversarial attacks
on deep learning-based anomaly detection in CPS networks
and implement a mitigation approach against the attack by
retraining models using adversarial samples. We use the Bot-IoT
and Modbus IoT datasets to represent the two CPS networks.
We train deep learning models and generate adversarial samples
using these datasets. These datasets are captured from IoT and
Industrial IoT (IIoT) networks. They both provide samples of
normal and attack activities. The deep learning model trained
with these datasets showed high accuracy in detecting attacks.
An Artiﬁcial Neural Network (ANN) is adopted with one input
layer, four intermediate layers, and one output layer. The output
layer has two nodes representing the binary classiﬁcation results.
To generate adversarial samples for the experiment, we used a
function called the ‘fast gradient method’ from the Cleverhans
library. The experimental result demonstrates the inﬂuence of
FGSM adversarial samples on the accuracy of the predictions and
proves the effectiveness of using the retrained model to defend
against adversarial attacks.

Index Terms—Cyber physical systems, Machine learning, Se-

curity, Attacks, Defence, Internet of Things.

I. INTRODUCTION

Cyber Physical Systems (CPS) are interconnected computer
systems that connect physical devices to the cyber world
to operate a process efﬁciently [1]. The emergence of the
Internet of Things (IoT) in CPS plays a key role in the fourth
industrial revolution (i.e., Industry 4.0) [2]. There has been
a signiﬁcant trend to deploy IoT applications and services
to monitor and control CPS [3]. Alongside the exponential
increase in the number of IoT devices in CPS, networks is

the growth of unknown vulnerabilities and threats posing a
big security challenge in the present preventive systems. This
challenge in the CPS network increases the surface of attacks
allowing adversaries to gain control of unsecured devices [4]
the Intrusion Detection Systems (IDS)
[5]. Towards this,
provides successful ways to classify and identifying attacks
in CPS networks. Traditional IDS use different methods to
identify and detect different attacks, for example,
through
statistical-based and patterned detection. New features are also
applied in IDS enhancing their detection capabilities through
behavioural patterns without relying on built-in signatures.
Machine Learning (ML) techniques, on the other hand, are
integrated into IDS to support automated decision making in
the classiﬁcation and identiﬁcation of any greater array of
attacks. In particular, a recent study [6] shows that the majority
of the state-of-the-art IDS are built on systems, e.g., Support
Vector Machines (SVM), Random Forest, and Decision Trees.
However, other studies [7] [8] also show that deep learning-
based IDS has more advantages over conventional IDS because
of its performance in large datasets and automated extraction
of complex representation from data.

In regard to this context, the enormous usage of different
types of ML and deep learning models have attracted adver-
sarial entities by implementing noise and disrupting prediction
outcomes that cause a result to be invalid or incorrect. This is
known as ‘Adversarial Machine Learning’ (AML), which can
exploit the weakness of a pre-trained model by manipulating
data and network trafﬁc that traverse through IDS devices
[9]. The perturbations engaged by the adversary in input data
can reduce the model’s effectiveness in its deciding bound-
aries wherein malicious packets are misclassiﬁed as benign.
Following this notion, gradient-based adversarial attacks [10]
are proven to be highly effective in adding interference in
gradient inputs of the model. Examples of these adversarial
attacks are the Fast Gradient Sign Method (FGSM) [11], the
Jacobian-based Saliency Map Attack (JSMA) [12], Deepfool
[13], and Carlini Wagner attack (CW) [14]. To this end, to
enhance the capability of a deep learning-based IDS, different
models, approaches, and techniques are used to determine
its effectiveness in preventing such adversarial attacks from
disrupting its decision making.

 
 
 
 
 
 
An Artiﬁcial Neural Network (ANN) is one of the well-
established methods to represent the classiﬁcation of either
numeric or image data [15]. ANNs have been deployed by
many papers to perform anomaly detection in CPS networks
[16] [17]. However, deep learning-based anomaly detection
methods are vulnerable to AML attacks. In accordance with
the vulnerability of the deep learning methods, there is a
huge demand for ﬁnding defensive techniques to improve
robustness against adversarial attacks. However, in the existing
studies, the results still show a signiﬁcant gap in addressing
the gradient-based adversarial attacks.

To address this issue, in this paper, we investigate the impact
of FGSM attacks on deep learning-based anomaly detection
in CPS networks and present a defensive strategy for these
attacks. To the best of our knowledge, unlike other studies
in this domain, we intend to investigate FGSM-based attacks
in CPS networks including Industrial IoT systems. The major
contributions of the paper can be summarized as follows:

• We explore the effects of adversarial attacks on deep
learning-based anomaly detection in a CPS network using
real-world Industrial IoT datasets.

• Based on our ﬁndings, we design a framework to address
the impact of FGSM adversarial attacks on deep learning-
based anomaly detection in CPS networks.

• We implement the framework to mitigate the attacks by
retraining our novel neural network-based solution using
adversarial samples.

• We evaluated the model results using our generated
adversarial attacks with variation in noise. Our results
show how the model robustness improved 92.8% after
using the proposed defence strategy.

The rest of the paper is organized as follows: In Section II,
we discuss related work. In Section III, we present a detailed
methodology used in our design. In Section IV, we show the
results achieved from the experiments and provide a discussion
of the signiﬁcant ﬁndings. Section V discusses the defence
strategies. Finally, in Section VI, we conclude the paper with
future work.

II. RELATED WORK

Several studies have been conducted to apprehend and
mitigate the effects of cyber attacks on CPS using ML [18]–
[20]. Nevertheless, there has been less emphasis on AML
attacks to ML-based solutions. The vulnerabilities of ML-
based solutions have been reviewed in various areas, e.g.,
spam detection, malware detection, and intrusion detection in
computer networks. There are different types of attacks that
can exploit these vulnerabilities, e.g. FGSM and Generative
Adversarial Networks (GANs).

Proposal [21] demonstrates a type of poisoning attack called
label ﬂipping that can reduce a performance metrics of a
classiﬁcation model by ﬂipping labels on the sample data. In
addition, proposal [22] evaluates the robustness of different
classiﬁcation models by implementing a malware classiﬁcation
system through ML. The results of this experiment show that
the models, when loaded with FGSM attack, can gravely

affect the performance results, and when adversarial training
is executed, the model becomes robust with the detection. In
[23], the authors demonstrate a study by using the GAN model
to generate adversarial Distributed Denial of Service (DDoS)
attacks that are successful in evading IDS because of their
polymorphic ability to change signatures. The papers discussed
above studied AML attacks in typical IT networks, and they
did not work on CPS networks with industrial devices which
follow different behavioural patterns. In our study, we focus
speciﬁcally on the CPS networks.

In the context of CPS,

there is not sufﬁcient research
on AML attacks on ML-based IDS. AML attacks in CPS
networks have been reviewed by a number of papers [24]–
[26]. In health systems immunity and vulnerability, while
integrated with deep learning algorithms, proposal [27] in-
vestigates different adversarial attacks like FGSM and Basic
Iterative Method (BIM) that show the effects of the amount
of perturbation versus the performance variation. An encode-
decode model has been proposed by [28] to provide evidence
of the robustness against FGSM only on a limited number
of IoT datasets. The authors of [29] propose an AML-based
partial-model attack. This attack could be performed by chang-
ing a small part of the device data. They show that ML-based
analysis in IoT systems are vulnerable to AML attacks even if
the adversaries manipulate a small portion of the device data.
The authors of [30] use the Bot-IoT dataset to implement
several adversarial attacks, namely FGSM, BIM and Projected
Gradient Descent (PGD), against two deep learning methods
Forward-Feed Neural Network (FNN) and Self-Normalizing
Neural Network (SNN), to compare the results of various
gradient-based adversarial attacks. The authors demonstrate
that adversarial samples have a different impact on both deep
learning models and show that SNN has more resiliency
against adversarial samples. Additionally, the demonstrations
show that when dataset features are normalized, performance
metrics are better but are more susceptible to adversarial
attacks. However, the defence strategy against these attacks
was not proposed in this paper. Proposal [31] also implements
a deep learning algorithm-based IDS that uses multi-layer
perceptron FNN against different gradient-based attacks, and
the metrics show that the FNN had been gravely affected by
the initiated adversarial attacks.

The aforementioned proposals did not consider AML at-
tacks in Industrial IoT which are known as important com-
ponents of CPS. In this study, we will investigate the impact
of AML attacks on both IoT and Industrial IoT. In addition,
unlike these studies,
in this paper, we focus on creating
different variants of gradient-based attacks with the help of
instance in hardening the defence of a
ML and use that
classiﬁcation model continuously by implementing an iterative
retraining approach. In summary, unlike the above-mentioned
proposals, we focus on implementing an architecture that can
self-sustain its defences against adversarial attacks in CPS at
scale. Additionally, for experimental purposes, the datasets that
have been used in most of the previous studies do not include
Industrial IoT in CPS networks. They also do not investigate

the required defence strategy for Industrial IoT in the context
of CPS networks, which we consider in this paper. Further,
in this paper, we particularly address this gap by proposing a
defensive strategy against FGSM attacks in IoT and Industrial
IoT networks.

III. METHODOLOGY

In this section, we detail the methodology. Before discussing
it
in detail, we present how our proposed model would
function. In Fig. 1, we illustrate the functional components of
the proposed model. Initially, we discuss the datasets (i.e., IoT
and Industrial IoT datasets) used in this study to investigate
the impact of FGSM attacks on deep learning-based security
solutions. Then, the deep learning-based anomaly detection
and FGSM model will also be explained.

Fig. 1. The functional components of the proposed model.

A. Datasets

In our experiment, we have used two datasets, namely Bot-
IoT and Modbus. These datasets are provided by the Cyber
Range Lab of UNSW Canberra, Australia. A brief introduction
of each of them are as follows [32]:

1) Bot-IoT Dataset: This dataset was comprised of over 72-
million IoT network trafﬁc records originally [33]. In order to
make the data processing more efﬁcient, 5% of the dataset
was extracted resulting in approximately 3.6 million records
[33]. The dataset has 19 features, and in this study, we selected
the 10 best features as shown in Table I. The selection was
based on the Correlation Coefﬁcient and Joint Entropy Score
discussed in [33]. Five classes are contained in the dataset
including (i) DDoS, (ii) DoS, (iii) Reconnaissance, (iv) Theft
and (v) Normal. All the classes except Normal were labelled
as “1”, which stands for attack, while normal records were
labelled as “0”.

2) Modbus Dataset: This is a benchmark dataset for Indus-
try 4.0/IoT and Industrial IoT (IIoT) for evaluating the ﬁdelity
and efﬁciency of various cybersecurity applications based on
Artiﬁcial Intelligence (AI). It comprised around 50 thousand

TABLE I
SELECTION OF THE TEN BEST FEATURES FROM BOT-IOT DATASET [33].

Features

Description

seq
stddev
N IN Conn P SrcIP
min
state number
mean
N IN Conn P DstIP
drate
srate
max

Argus sequence number
Standard deviation of aggregated records
Total number of packets per source IP
Minimum duration of aggregated records
Numerical representation of transaction state
Average duration of aggregated records
Total number of packet per destination IP
Destination-to-source packets per second
Source-to-destination packets per second
Maximum duration of aggregated records

records. It is one of the datasets from the TON IoT Telemeter
dataset [34], and contains records captured by Modbus sensors.
This dataset is composed of 8 features as shown in Table II and
categorized into four classes containing (i) DoS, (ii) DDoS,
(iii) Backdoor attack and (iv) Normal.

TABLE II
IOT MODBUS DATASET FEATURES AND DESCRIPTION [34].

Features

Description

Timestamp of sensor reading data
ts
Date of logging Modbus register’s data
date
time
Time of logging Modbus register’s data
FC1 Read Input Register Modbus function code that is responsi-

FC2 Read Discrete Value Modbus function code that is in charge

ble for reading an input register

FC3 Read Holding Register Modbus function code that is responsi-

of reading a discrete value

FC1 Read Coil

type

ble for reading a holding register
Modbus function code that is responsi-
ble for reading a coil
A tag with normal or attack subclasses,
e.g., DoS, DDoS, and backdoor attacks

B. Data Pre-processing

The original Bot-IoT dataset was comprised of unhandled
data which this experiment needs to address, as shown in Ta-
ble III. Redundant and irrelevant features in the data can cause
a problem in the classiﬁcation model in predicting outcomes,
especially when dealing with large dimension datasets [35].
To appropriately conduct this experiment, the Bot-IoT dataset
goes through different data pre-processing techniques that help
identify relevant data from insigniﬁcant information. As the
ﬁrst step of the data pre-processing, the dataset has to be
standardized using functions that will remove redundant and
unused columns and rows in the dataset. Secondly, the dataset
undergoes the process of Synthetic Minority Oversampling
Technique (SMOTE) [36] that uses random samples to balance
the distribution of the output labels and addresses the effects
of imbalanced data, as shown in Table III. Unlike the Bot-IoT
dataset, the Modbus dataset has class distribution close to each
other, and there was no signiﬁcant difference in results after
balancing, due to which we do not apply any data balancing
technique for the Modbus dataset.

Datasets(CPS)Data Pre-processingDeep Learning-based AnomalyDetection1. Evaluate the vulnerability of deep     learning against FGSM attacks.2. Improve the robustness of deep     learning in FGSM attacks.GeneratingAdversarialSamplesFinally, once the Bot-IoT dataset distribution is balanced,
it is prepared to convert categorical values into numerical
data, since Modbus dataset is normalized without the need
of data balancing. Once the dataset is normalized, it is split
into training and testing datasets in our classiﬁcation model,
as shown in Table IV.

In order

the function
to generate adversarial samples,
‘fast_gradient_method’ from CleverHans v2.1.0 [39]
is used, which is a library for adversarial examples. As the
function was written following the equation mentioned above,
all the values of variables in the equation can be changed via
the parameters of the function.

TABLE III
IMBALANCED AND BALANCED DATASETS.

Dataset Name Output Labels
Bot-IoT

Modbus

Attack
Normal
Attack
Normal

Imbalanced Dataset Balanced Dataset

Total Samples
293,447
370
13,886
17,476

Total Samples
293,447
293,447
...
...

TABLE IV
TRAINING AND TESTING DATA AMOUNT.

Total

Training Samples
70%

Testing Samples
30%

Bot-IoT
(imbalanced)
Bot-IoT
(balanced)
Modbus

2,934,817

2,054,371

880,446

5,868,894
51,106

4,108,225
21,953

1,760,669
9,409

C. Deep Learning-Based Anomaly Detection

In this study,

the classiﬁcation model used was ANN
networks obtained from [37]. ANN is composed of one input
layer, four hidden layers and one output layer. The underlying
model architectures are the same, due to variation in features
of each dataset the input layer is different (Bot-IoT dataset
has 10 nodes, while the Modbus dataset has 8 nodes). The
hidden layers are activated with TanH and they are composed
of 20, 60, 80 and 90 nodes, respectively. Both output layers use
Sigmoid to activate and generate binary results (1 for attack
or 0 for normal).

D. Generating Adversarial Samples

In recent studies performed regarding adversarial attacks
on ML, gradient-based attacks pose a severe threat to neural
networks [38]. Thus, a common gradient-based attack was em-
ployed to demonstrate how badly it can affect the classiﬁcation
model’s accuracy and use that instance to create more devas-
tating adversarial attacks to be used in the proposed approach.
Recall, we use FGSM for generating adversarial examples
[11]. FGSM is performed based on Eq. 1, which maximize the
loss by calculating the gradient descent oppositely. In Eq. 1,
(cid:101)x stands for adversarial samples, x represents the input data,
and J is the loss function. As shown in the function, (cid:15) is a
crucial variable that can signiﬁcantly affect the output. The (cid:15)
value is used to add noise which leads to worse performance.

(cid:101)x = x + (cid:15) · sign(∇xJ(w, x, y))

(1)

IV. RESULTS AND DISCUSSION

This section will discuss the evaluation metrics employed
for the experiment. We also present the results and discuss the
signiﬁcant ﬁndings.

A. Evaluation Metrics

Evaluating speciﬁc metrics must be well deﬁned to ﬁnd out
how well the proposed approach is. The study used the eval-
uation metrics indicators that assess the experimental results,
i.e., the accuracy, precision, recall, and F1-score, Eqs. 2, 3,
and 4. where (TP) is true positive, (TN) is true negative, (FP)
is false positive, and (FN) shows false negative.

P recision =

T P
T P + F P

Recall =

T P
T P + F N

F 1 Score =

2(P recision · Recall)
P recision + Recall

(2)

(3)

(4)

These evaluation metrics are summarized succinctly using
the confusion matrix which evaluates model proﬁciency with
a prediction for the class.

B. Results

In this section, we present the performance of the ANN
classiﬁer under FGSM attacks for both Bot-IoT and Modbus
datasets. Also, experiments have been conducted for the Bot-
IoT dataset for scenarios handling imbalanced datasets. The
functional codes of our solution can be found in the following
repository [40]. The results show how the imbalanced data
resulted in an over-ﬁtting model. We have trained the model
for three scenarios, two with the Bot-IoT dataset (balanced and
unbalanced), and another one with the Modbus dataset. The
results of these three data scenarios can be seen in Fig. 2, Fig. 4
and Fig. 6. We can also observe from Fig. 2 and Fig. 4 the
improvements between the imbalanced dataset and balanced
dataset, which manifest the need for the data balance in the
Bot-IoT dataset.

These three models were subjected to adversarial attacks
with different epsilon values (noise) ranging from 0 to 1. In
order to assess the classiﬁcation model’s robustness, this ex-
periment used a common gradient-based attack called FGSM.
The FGSM attack served as an initial attack to illustrate how
this type of adversarial attack can gravely affect an outcome
of a classiﬁcation model. When all these models were tested
for FGSM attack, the results clearly show how the adversarial
attack has deteriorated the model performance.

TABLE V
EVALUATION RESULTS IN BOT-IOT AND MODBUS DATASETS.

Datasets

Testing Dataset

TP

TN

FP

FN

Bot-IoT dataset Original dataset
(imbalanced)

FGSM epsilon=0
FGSM epsilon=0.2
FGSM epsilon=0.4
FGSM epsilon=0.6
FGSM epsilon=0.8
FGSM epsilon=1

Bot-IoT dataset Original dataset
(balanced)

FGSM epsilon=0
FGSM epsilon=0.2
FGSM epsilon=0.4
FGSM epsilon=0.6
FGSM epsilon=0.8
FGSM epsilon=1

Modbus dataset Original dataset

FGSM epsilon=0
FGSM epsilon=0.2
FGSM epsilon=0.4
FGSM epsilon=0.6
FGSM epsilon=0.8
FGSM epsilon=1

0
0
41
98
111
111
111
879,760
879,760
577
577
577
577
577
5,243
5,243
4,796
806
0
0
0

880,335
880,335
879,400
852,893
712,199
412,583
63,022
875,348
875,348
802,486
397,280
35,262
4976
4976
3,815
3,815
2,222
218
203
203
203

0
0
935
27,442
168,136
467,752
817,313
4,987
4,987
77,849
483,055
845,073
875,359
875,359
351
351
1,944
3,948
3,963
3,963
3,963

111
111
70
13
0
0
0
574
574
879,757
879,757
879,757
879,757
879,757
0
0
447
4,437
5,243
5,243
5,243

TABLE VI
EVALUATION MATRICS IN BOTH BOT-IOT AND MODBUS DATASETS.

Datasets

Testing dataset

Precision Recall
(%)

(%)

F1-Score

Bot-IoT dataset Original dataset
(imbalanced)

FGSM epsilon=0
FGSM epsilon=0.2
FGSM epsilon=0.4
FGSM epsilon=0.6
FGSM epsilon=0.8
FGSM epsilon=1

Bot-IoT dataset Original dataset
(balanced)

FGSM epsilon=0
FGSM epsilon=0.2
FGSM epsilon=0.4
FGSM epsilon=0.6
FGSM epsilon=0.8
FGSM epsilon=1

Modbus dataset Original dataset

FGSM epsilon=0
FGSM epsilon=0.2
FGSM epsilon=0.4
FGSM epsilon=0.6
FGSM epsilon=0.8
FGSM epsilon=1

0
0
4.2
0.36
0.07
0.02
0.01
99.44
99.44
0.74
0.12
0.07
0.07
0.07
93.73
93.73
71.16
16.95
0
0
0

0
0
36.94
88.29
100
100
100
99.93
99.93
0.07
0.07
0.07
0.07
0.07
100
100
91.47
15.37
0
0
0

0
0
7.54
0.71
0.13
0.05
0.03
99.68
99.68
0.12
0.08
0.07
0.07
0.07
96.76
96.76
80.05
16.12
0
0
0

Tables V and VI show FGSM attacks with different epsilon
values in Bot-IoT and Modbus datasets. The impact of epsilon
values on the accuracy of our ANN classiﬁer for all the three
models is shown in Fig. 3, Fig. 5 and Fig. 7.

V. DEFENCE STRATEGIES

We implement a popular defensive approach known as
Adversarial training. Adversarial training is the approach of
training an ML model with adversarial samples to increase
the model’s ability to detect adversarial attacks. An experi-
ment conducted by [11] showed that adversarial training can
minimise the impact of the attack when the data is perturbed
by the adversary. Adversarial training can help strengthen the
robustness of the model and can fend off future adversarial
attacks. We use a defensive approach by retraining the model
including adversarial samples in the train and test datasets

Fig. 2. Confusion matrix in imbalanced Bot-IoT dataset.

Impact of Epsilon values on the prediction accuracy (for imbalanced

Fig. 3.
Bot-IoT dataset).

in the form of percentages. These percentages act as the
percentage of adversarial samples in the dataset. This defensive
approach is related to our study because it demonstrates the
model’s ability to reduce the impact caused by the attack.
With adversarial training, our model can produce better results
than when it was trained with an unpolluted dataset. When the
model was trained with more adversarial samples, it became
robust and classiﬁed correctly. The percentage of adversarial
samples is shown in Table VII. From our experiments, we
observe the rise in precision when the percentage of adversarial
samples in an unpolluted dataset increases. We can see the
results in Tables VIII and IX. These results depict that adding
an adversarial sample into the training has proven to be
an excellent defensive strategy against adversarial attacks.
However, the train and test split are kept the same as in
previous experiments, replacing the 10%, and 20% of the
training samples with adversarial attacked samples, with the
highest epsilon value (i.e., 1.0) for the FGSM attack. We have
added 10% and 20% of adversarial samples while training the
model. Adding more samples to the training data and using
different adversarial attacks is a different direction of research.

figure 4figure 5Fig. 4. Confusion matrix of ANN in balanced Bot-IoT.

Fig. 6. Confusion matrix of ANN in Modbus dataset.

Fig. 5.
Bot-IoT dataset).

Impact of Epsilon values on the prediction accuracy (for balanced

Fig. 7.
dataset).

Impact of Epsilon values on the prediction accuracy (for Modbus

VI. CONCLUSION AND FUTURE WORK

Cyber Physical Systems (CPS) transformed the interaction
with the physical world by connecting physical devices to the
Internet and the cyber world. Deep learning-based intrusion
detection has been widely used in such CPS environments to
learn the baseline of normal behaviour and detect abnormal ac-
tivities. However, advanced deep learning-based solutions are
vulnerable to Adversarial Machine Learning (AML) attacks. In
this paper, we addressed this issue and investigated the impact
of AML attacks on intrusion detection for both IoT (Inter-
net of Things) and large-scale Industrial IoT networks. For
experimental purposes, two real-world datasets (i.e., Bot-IoT
and Modbus) were used to evaluate the results. These datasets
depicted actual industry scenarios where there are class im-
balance scenarios. Therefore, we performed pre-processing of
datasets to manage imbalanced data distribution. Fast Gradient
Sign Method (FGSM) based adversarial attacks were studied
in this paper, and the results showed that the performance of an
Artiﬁcial Neural Networks (ANN) classiﬁer is reduced under
such FGSM attacks. Then, a defence strategy was proposed
to improve the robustness of the deep learning by retraining it
with adversarial samples. The results showed the improvement
of deep learning robustness in FGSM samples. For future

work in defensive strategies, adding adversarial samples from
other types of adversarial attacks like Generative Adversarial
Networks (GAN) would also be reviewed.

ACKNOWLEDGMENT

The authors acknowledge the support of the Commonwealth

of Australia and Cybersecurity Research Centre Limited.

REFERENCES

[1] D. Pivoto, L. de Almeida, R. Righi, J. Rodrigues, A. Lugli, and
A. Alberti, “Cyber-physical systems architectures for industrial internet
of things applications in industry 4.0: A literature review,” Journal of
manufacturing systems, vol. 58, pp. 176–192, 2021.

[2] J. Yaacoub, O. Salman, H. Noura, N. Kaaniche, A. Chehab, and
M. Malli, “Cyber-physical systems security: Limitations, issues and
future trends,” Microprocessors and microsystems, vol. 77, 2020.
[3] A. Sharma, V. Burman, and S. Aggarwal, “Role of iot in industry 4.0,”
in Advances in Energy Technology, Springer, pp. 517–528, 2022.
[4] S. Pal, M. Hitchens, and V. Varadharajan, “On the design of security
mechanisms for the internet of things,” in Eleventh International Con-
ference on Sensing Technology (ICST), IEEE, pp. 1–6, 2017.

[5] S. Pal and Z. Jadidi, “Analysis of security issues and countermeasures
for the industrial internet of things,” App. Sciences, vol. 11, no. 20, 2021.
[6] K. Costa, J. Papa, C. Lisboa, R. Munoz, and V. Albuquerque, “Internet
of things: A survey on machine learning-based intrusion detection
approaches,” Computer Networks, vol. 151, pp. 147–157, 2019.

[7] M. Garadi, A. Mohamed, A. Ali, X. Du, I. Ali, and M. Guizani, “A
survey of machine and deep learning methods for internet of things (iot)
security,” IEEE Communications S&T, vol. 22, pp. 1646–1685, 2020.

figure 6figure 7figure 8figure 9TABLE VII
THE COMPOSITION OF THE DATA FOR DEFENCE EXPERIMENTS.

Total

Adversarial

Original

Experiment 1

Experiment 2

Training Data
Testing Data
Training Data
Testing Data

70%
30%
70%
30%

10%
100%
20%
100%

90%
0%
80%
0%

TABLE VIII
CONFUSION MATRICES OF THE EXPERIMENTS.

Testing dataset name

TP

TN

FP

FN

Experiment 1 (10% adversarial samples)

Bot-IoT
(imbalanced)
Bot-IoT
(balanced)
Modbus

0

880,335

0

879,779
5,216

875,447
3,102

4,888
1,064

Experiment 2 (20% adversarial samples)

Bot-IoT
(imbalanced)
Bot-IoT
(balanced)
Modbus

0

880,335

0

879,642
5,216

875,367
3,102

4,968
1,064

111

555
27

111

692
27

TABLE IX
CLASSIFICATION REPORT OF EXPERIMENT WITH BOT-IOT AND MODBUS
DATASETS.

Testing dataset name

Precision
(%)

Recall
(%)

F1-Score

Experiment 1 (10% adversarial samples)

Bot-IoT
(imbalanced)
Bot-IoT
(balanced)
Modbus

0

99.35
0.05

0

0

99.95
0.04

99.65
0.04

Experiment 2 (20% adversarial samples)

Bot-IoT
(imbalanced)
Bot-IoT
(balanced)
Modbus

0

99.44
83.06

0

0

99.92
99.49

99.68
90.53

[8] G. Haylett, Z. Jadidi, and K. Thanh, “System-wide anomaly detection
of industrial control systems via deep learning and correlation analysis,”
in AI Applications and Innovations, Springer, pp. 362–373, 2021.
[9] E. Anthi, L. Williams, A. Javed, and P. Burnap, “Hardening machine
learning denial of service (dos) defences against adversarial attacks in
iot smart home networks,” Computers & Security, 2021.

[10] U. Ozbulak, M. Gasparyan, W. Neve, and A. Messem, “Perturbation
analysis of gradient-based adversarial attacks,” Pattern Recognition
Letters, vol. 135, pp. 313–320, 2020.

[11] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing

adversarial examples,” arXiv Preprint, 2014.

[12] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,”
in Euro S&P, IEEE, pp. 372–387, 2016.

[13] S. Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple and accurate
method to fool deep neural networks,” in computer vision and pattern
recognition conference, IEEE, pp. 2574–2582, 2016.

[14] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in IEEE symposium on security and privacy (SP), IEEE, pp.
39–57, 2017.

[15] O. Abiodun, A. Jantan, A. Omolara, K. Dada, N. Mohamed, and
H. Arshad, “State-of-the-art in artiﬁcial neural network applications: A
survey,” Heliyon, vol. 4, no. 11, 2018.

[16] D. Wu, et al., “Anomaly detection based on rbm-lstm neural network
for cps in advanced driver assistance system,” ACM Transactions on
Cyber-Physical Systems, vol. 4, no. 3, pp. 1–17, 2020.

[17] Y. Luo, Y. Xiao, L. Cheng, G. Peng, and D. Yao, “Deep learning-based
anomaly detection in cyber-physical systems: Progress and opportuni-
ties,” ACM Computing Surveys (CSUR), vol. 54, no. 5, pp. 1–36, 2021.
[18] S. Kim and K. Park, “A survey on machine-learning based security

design for cyber-physical systems,” App. Sci., vol. 11, no. 12, 2021.

[19] C. Ahmed, M. Umer, B. Binte Liyakkathali, M. Jilani, and J. Zhou,
“Machine learning for cps security: applications, challenges and rec-
ommendations,” in Machine Intelligence and Big Data Analytics for
Cybersecurity Applications, Springer, pp. 397–421, 2021.

[20] Z. Jadidi, A. Dorri, R. Jurdak, and C. Fidge, “Securing manufacturing
using blockchain,” in International Conference on Trust, Security and
Privacy in Computing and Communications (TrustCom), IEEE, pp.
1920–1925, 2020.

[21] H. Zhang, N. Cheng, Y. Zhang, and Z. Li, “Label ﬂipping attacks against

naive bayes on spam ﬁltering systems,” App. Intelli., pp. 1–12, 2021.

[22] S. Patil, V. Varadarajan, D. Walimbe, S. Gulechha, S. Shenoy, A. Raina,
and K. Kotecha, “Improving the robustness of ai-based malware detec-
tion using adversarial machine learning,” Algo., vol. 14, no. 10, 2021.
[23] R. Chauhan and S. Heydari, “Polymorphic adversarial ddos attack on
ids using gan,” in International Symposium on Networks, Computers and
Communications (ISNCC), IEEE, pp. 1–6, 2020.

[24] Y. Song, T. Liu, T. Wei, X. Wang, Z. Tao, and M. Chen, “Fda: Federated
defense against adversarial attacks for cloud-based iiot applications,”
IEEE Trans. on Industrial Infor., vol. 17, no. 11, pp. 7830–7838, 2020.
[25] S. Dankwa and L. Yang, “Securing iot devices: A robust and efﬁcient
deep learning with a mixed batch adversarial generation process for
captcha security veriﬁcation,” Electronics, vol. 10, no. 15, 2021.
[26] H. Qiu, T. Dong, T. Zhang, J. Lu, G. Memmi, and M. Qiu, “Adversarial
attacks against network intrusion detection in iot systems,” IEEE Internet
of Things Journal, vol. 8, no. 13, pp. 10327–10335, 2020.

[27] G. Mode and K. Hoque, “Crafting adversarial examples for deep learning

based prognostics (extended version),” arXiv Preprint, 2020.

[28] Z. Yang, I. Abbasi, F. Algarni, S. Ali, and M. Zhang, “An iot time
series data security model for adversarial attack based on thermometer
encoding,” Security and Communication Networks, 2021.

[29] Z. Luo, S. Zhao, Z. Lu, Y. Sagduyu, and J. Xu, “Adversarial machine
learning based partial-model attack in iot,” in 2nd ACM Workshop on
Wireless Security and Machine Learning, pp. 13–18, 2020.

[30] O. Ibitoye, O. Shaﬁq, and A. Matrawy, “Analyzing adversarial attacks
against deep learning for intrusion detection in iot networks,” in Globe-
Com, IEEE, 2019, pp. 1–6.

[31] Z. Wang, “Deep learning-based intrusion detection with adversaries,”

IEEE Access, vol. 6, pp. 38367–38384, 2018.

[32] UNSW-Canberra-Australia, “The ton iot datasets.” [Online]. Available:
https://research.unsw.edu.au/projects/toniot-datasets(January2022)
[33] N. Koroniotis, N. Moustafa, E. Sitnikova, and B. Turnbull, “Towards
the development of realistic botnet dataset in the internet of things
for network forensic analytics: Bot-iot dataset,” Future Generation
Computer Systems, vol. 100, pp. 779–796, 2019.

[34] A. Alsaedi, N. Moustafa, Z. Tari, A. Mahmood, and A. Anwar, “Ton iot
telemetry dataset: A new generation dataset of iot and iiot for data-driven
intrusion detection systems,” IEEE Access, vol. 8, 2020.

[35] H. Abdulwahab, S. Ajitha, and M. ASaif, “Feature selection techniques
in the context of big data: taxonomy and analysis,” Applied Intelligence,
pp. 1–46, 2022.

[36] R. Pears, J. Finlay, and A. Connor, “Synthetic minority over-sampling
for predicting software build outcomes,” arXiv

technique (smote)
Preprint, 2014.

[37] P. Papadopoulos, O. Essen, N. Pitropakis, C. Chrysoulas, A. Mylonas,
and W. Buchanan, “Launching adversarial attacks against network intru-
sion detection systems for iot,” Journal of Cybersecurity and Privacy,
vol. 1, no. 2, pp. 252–273, 2021.

[38] R. Mahfuz, R. Sahay, and A. Gamal, “Mitigating gradient-based adver-

sarial attacks via denoising and compression,” arXiv Preprint, 2021.

[39] N. Papernot, et al., “Technical report on the cleverhans v2. 1.0 adver-

sarial examples library,” arXiv Preprint, 2016.

[40] “Github-code.” [Online]. github.com/NitheshNayak/AnomalyDetection\

CyberPhysicalSystems.git

