Utilizing Noise Addition for Data Privacy, an Overview 

Kato Mivule 
Computer Science Department 
Bowie State University 
14000 Jericho Park Road Bowie, MD 20715 
Mivulek0220@students.bowiestate.edu 

taken 

into  account 

Abstract  â€“  The  internet  is  increasingly  becoming  a 
standard  for  both  the  production  and  consumption  of 
data  while  at  the  same  time  cyber-crime  involving  the 
theft  of  private  data  is  growing.  Therefore  in  efforts  to 
securely  transact  in data,  privacy  and  security  concerns 
must  be 
the 
confidentiality  of  individuals  and  entities  involved  is  not 
compromised, and that the data published is compliant to 
privacy  laws.  In  this  paper,  we  take  a  look  at  noise 
addition as one of the data privacy providing techniques. 
Our  endeavor  in this  overview  is  to  give  a  foundational 
perspective  on  noise  addition  data  privacy  techniques, 
provide  statistical  consideration 
for  noise  addition 
techniques and look at the current state of the art in the 
field, while outlining future areas of research.   

to  ensure 

that 

Keywords:    Data  Privacy,  Security,  Noise  Addition, 
Data Perturbation 

1. Introduction 

Large  data  collection  organizations  such  as  the 
Census Bureau often release statistics to the public in the 
form  of  statistical  databases,  often  transformed  to  some 
extent,  omitting  sensitive  information  such  as  personal 
identifying  information  (PII).  Researchers  have  shown 
that  with  such  publicly  released  statistical  databases  in 
conjunction with supplemental data, adversaries are able 
to  launch  inference  attacks  and  reconstruct  identities  of 
individuals  or  an  entity's  sensitive  information  [1]. 
Therefore  while  data  de-identification  is  essential,  it 
should be taken as an initial step in the process of privacy 
preserving  data  publishing  but  other  methods  such  as 
noise addition should strongly be considered after PII has 
been  removed  from  data  sets  to  ensure  greater  levels  of 
confidentiality  [1]  [2].  A  generalized  data  privacy 
procedure  would  involve  both  data  de-identification  and 
perturbation as shown in Figure 1. 

Figure 1: Generalized Data Privacy with Noise Addition   

2. Background 

In  this  section  we  take  a  look  at  some  of  the 
terms used in the noise addition procedure. Data Privacy 
and  Confidentiality  is  the  protection  of  an  entity  or  an 
individual against illegitimate information revelation. [1]. 
Data  Security  is  concerned  with  legitimate  accessibility 
of  data  [2].  Data  de-identification  is  the  removal  of 
personally  identifiable  information  (PII)  from  a  data  set 
[3] [4]. Data de-identification process also referred to as 
data  anonymization,  data  sanitization,  and  statistical 
disclosure  control  (SDC),  is  a  process  in  which  PII 
attributes are excluded or denatured to such an extent that 
when  the  data  is  made  public,  a  person's  identity,  or  an 
entity's  sensitive  data,  cannot  be  reconstructed  [5]  [6]. 
Statistical  disclosure  control  methods  are  classified  as 
non-perturbative and perturbative, with the former being 
a procedure in which original data is not denatured, while 
with 
is  denatured  before 
publication  to  provide  confidentiality  [1].  Therefore  de-
identification  of  data  ensures  to  some  extent  that 
sensitive and personal data does not suffer from inference 
and  reconstruction  attacks,  which  are  methods  of  attack 
in  which  isolated  pieces  of  data  are  used  to  infer  a 
supposition about a person or an entity [7].   

latter,  original  data 

the 

Data  utility  verses  privacy  is  how  useful  a  published 
dataset  is  to  the  consumer  of  that  publicized  dataset.  In 
most instances,  when publishers of large data sets do so, 
they  ensure  that  PII  is  removed  and  data  is  distorted  by 
noise  addition  techniques.  However,  in  doing  so,  the 
original  data  suffers  loss  of  some  of  its  statistical 
properties  even  while  confidentiality  is  granted,  thus 
making the dataset almost  meaningless to the user of the 
published  dataset.  Therefore  a  balance  between  privacy 
and  utility  needs  is  always  sought  [24]  [25]  [26].  Data 
privacy  scholars  have  noted  that  achieving  optimal  data 
privacy while not shrinking data utility is an ongoing NP-
hard  task  [27].  Statistical  databases  are  non-changing 
data  sets  often  published  in  aggregated  format  [28]. 
While  data  de-identification  will  ensure  the  removal  of 
PII  attributes,  it  has  been  deemed  a  novice  method  by 
researchers; the remaining sanitized data set could still be 
compromised  and  used  to  reconstruct  an  individual's 
identity or an entity's sensitive data [1] [2]. Therefore the 
remaining  confidential  attributes  that  contain  sensitive 
information for example salary, student's GPA, need to be 
transformed to such  an extent that they  cannot be linked 

 
 
 
 
 
 
with  outside  information  in  an  inference  attack.  It  is  in 
this  context  that  we  focus  on  noise  addition  as  a 
transform 
to 
perturbation  methodology 
numerical attributes to grant confidentiality.   

that  seeks 

3. Related work 

in 

With  an  increasing  interest  in  data  privacy  and 
security  research,  a  number  of  surveys  have  been  done 
articulating  the  progress  and  state  of  the  art  in  the  data 
privacy  and  security  research  field.  In  their  survey  on 
data  privacy  and  security,  Santos  et  al.,  present  an 
overview  on  state  of  the  art  in  data  security  techniques, 
placing  emphasis  on  data  security  solutions  for  data 
warehousing  [40].  Furthermore, 
their  overview, 
Matthews  and  Harel,  offer  a  more  comprehensive 
summary  of  current  statistical  disclosure  limitation 
techniques,  noting  that  that the  balance  between  privacy 
and  utility  is  still  being  sought  with  data  privacy 
enhancing  techniques  [41].  Additionally  Joshi  and  Kuo, 
offer an outline of state of the art data privacy techniques 
in  Online  Social  Networks,  in  which  they  note  how  a 
balance is always pursued between privacy requirements 
for  users  and  using  private  data  for  advertisements  [42]. 
Yet  still,  in  their  review,  Ying-hua  et  al.,  take  a  closer 
look at the current data privacy preserving techniques in 
data  mining,  providing  advantages  and  disadvantages  of 
various data privacy procedures [43]. While a number of 
current  overviews  on  data  privacy  focus  on  the  general 
data  privacy  enhancing  techniques,  in  this  paper,  we 
focus  on  noise  addition  methods  while  providing 
statistical considerations for data perturbation.   

4. Noise Addition   

transform 

In  this  section,  we  take  a  look  at  noise  addition 
perturbation  methods 
confidential 
that 
attributes  by  adding  noise  to  provide  confidentiality. 
Noise  addition  works  by  adding  or  multiplying  a 
stochastic  or 
to  confidential 
randomized  number 
quantitative  attributes.  The  stochastic  value  is  chosen 
from  a  normal  distribution  with  zero  mean  and  a 
diminutive standard deviation [10] [11]. 

4.1. Additive Noise   

Work  on  additive  noise  was  first  publicized  by 

Kim [12] with the general expression that 

ğ‘ = ğ‘‹ + ğœ€ 

(1) 
Where  Z  is  the  transformed  data  point,  X  is  the  original 
data  point  and  É›  is  the  random  variable  (noise)  with  a 
distribution ğ‘’~ğ‘ (0, ğœ2). This is then added to X.  The  X 
is  then  replaced  with  the  Z  for  the  data  set  to  be 
published.[13]  With  stochastic  noise,  random  data  is 
added 
the 
distinguishing  values,  an  example  includes  increasing  a 
student's GPA by a diminutive percentage, say from 3.45 
to  3.65  GPA  [14].  In  their  work  on  additive  noise, 
Domingo-Ferrer et al., outline that in additive noise, also 
referred to as white noise, concealment by additive noise 
anticipates  that  the  variable  of  measurements  ğ‘¥ğ‘—  of  the 

to  confidential  attributes 

to  conceal 

original  data  set  ğ‘‹ğ‘—   is  continuously  replaced  by  the 
variable, 

ğ‘§ğ‘— = ğ‘¥ğ‘— +  ğ‘—   

(2) 
Where  j  is  the  variable  of  normally  distributed  noise 
2)  ,  such 
acquired  from  a  random  variable:  ğœ€ğ‘—~ğ‘(0, ğœğ‘—
that 
ğ¶ğ‘œğ‘£(ğœ€ğ‘¡, ğœ€ğ‘™ ),  for  all  t!  =  l  thus  the  method  preserves  the 
mean  and  covariance.  [20]  Therefore  additive  noise  can 
be expressed in a simple format as follows [21]:   
ğ‘ = ğ‘‹ + ğœ€   

(3) 
Z  is  masked  data  value  to  be  published,  after  the 
transformation  X  +  É›.  X  is  the  original  unmasked  data 
value  in  the  raw  data  set.  É›  (epsilon)  is  the  random 
variable  (noise)  added  to  X,  whose  distribution  is 
ğœ€~ğ‘(0, ğœ2).  Ciriani  et  al.,  note  that  additive  noise  also 
known  as  uncorrelated  noise,  preserves  the  mean  and 
covariance  of  the  original  data  but  the  correlation 
coefficients  and  variances  are  not  sustained.  Another 
variation  of  additive  noise  is  correlated  additive  noise 
that  keeps  the  mean  and  allows  the  sustenance  of 
correlation coefficients in the original data [22].   

4.2. Multiplicative Noise   

that  multiplicative  noise 

Multiplicative noise is another type of stochastic 
noise  outlined  by  Kim  and  Winkler  [23]  in  which  they 
describe 
is  rendered  by 
generating random numbers with a mean = 1, which then 
are  used  as  noise  and  multiplied  to  the  original  data set. 
Each  data  element  is  multiplied  by  a  random  number 
with  a  short  Gaussian  distribution,  with  mean  = 1  and  a 
small variance:   

ğ‘Œğ‘— = ğ‘‹ğ‘—ğœ€ğ‘—   

(4) 
Where Y is the perturbed data; X is the original data; E is 
the  generated  random  variable  (noise)  with  a  normal 
distribution with mean Âµ and variance Ïƒ [23]. 

4.3 Logarithmic multiplicative noise   

Kim and Winkler [23] describe another variation 
of  multiplicative noise, in  which a logarithmic  alteration 
is taken on the original data: 

ğ‘Œğ‘— =  ğ‘™ğ‘›ğ‘‹ğ‘—  

(5) 

  The  random  number  (noise)  is  then  generated  and  then 
added to the altered data [23]: 

ğ‘ğ‘— =   ğ‘Œğ‘— + ğœ€ğ‘—     

(6) 

Where X is the original data; Y is the logarithmic altered 
data; Z is the logarithmic altered data with noise added to 
it;  ğ‘’ ğ‘¥ is  the  exponential  function  used  to  calculate  the 
antilog.   

4.4. Differential Privacy   

In  this  section,  we  take  a  look  at  Differential 
privacy,  a  current  state  of  the  art  data  perturbation 
method  that  utilizes  Laplace  noise  addition  techniques 
and was proposed by Dwork (2006). Differential privacy   
is  the  latest  state-of-the-art  methodology  in  data  privacy 
that  enforces  confidentiality  by  returning  perturbed 
aggregated  query  results  from  databases,  such  that  users 
of the databases cannot discern if particular data item has 

 
 
 
 
 
 
 
 
 
 
 
been  altered  or  not.  This  means  that  with  the  perturbed 
results of the query, an attacker cannot derive information 
about any data item in the database [33]. The database in 
this  case  is  a  collection  of  rows  that  represent  each 
individual  entity  we  seek  to  provide  concealment.  [34] 
According to Dwork (2008), two databases D1 and D2 are 
considered  identical  or  similar,  if  they  differ  or  disagree 
in  only  one  element  or  row  that  is  ğ·1 âˆ† ğ·2  = 1 . 
Therefore,  a  procedure  ğ‘ğ‘›   that  grants  confidentiality, 
satisfies  ï¥-differential  privacy  if  the  result  to  any  same 
query  run  on  database  D1  and  again  run  on  database  D2 
should  probabilistically  be  similar,  and  as  long  as  those 
results satisfy the following requirement: [36] 

ğ‘ƒ[ğ‘ğ‘›(ğ·1)âˆˆğ‘…]
ğ‘ƒ[ğ‘ğ‘›(ğ·2)âˆˆğ‘…]

  â‰¤   ğ‘’ğœ€      

(7) 

Where D1 and D2 are the two databases 

ï‚·  P  is  the  probability  of  the  perturbed  query 

ï‚· 

ï‚· 

ï‚· 

the  privacy  granting  procedure 

results D1 and D2 respectively. 
qn() 
is 
(perturbation).     
qn(D1)  is  the  privacy  granting  procedure  on 
query results from database D1. 
qn(D2)  is  the  privacy  granting  procedure  on 
query results from database D2. 

ï‚·  R  is  the  perturbed  query  results  from  the 

databases D1 and D2 respectively. 
ğ‘’ ğœ€ï€  is the exponential ï¥ epsilon value.   

ï‚· 

Therefore  to  satisfy  differential  privacy,  the  probability 
of  the  perturbed  query  results  D1  divided  by  the 
probability  of  the  perturbed  query  results  D2  should  be 
less or equal to an exponential ï¥ epsilon value. That is to 
say,  if  we  run  the  same  query  on  database  D1,  and  then 
run  the  same  query  again  on  database  D2,  our  query 
results should probabilistically be similar. If the condition 
can  be  mitigated  in  the  presence  or  absence  of  the  most 
influential  observation  for  a  particular  query,  then  this 
condition  will  also  be  mitigated 
for  any  other 
observation.  The  consequence  of  the  most  dominant 
observation  for  a  given  query  is  given  by  âˆ†ğ‘“   and 
assessed in the following way: 

âˆ†ğ‘“ = ğ‘€ğ‘ğ‘¥|ğ‘“(ğ·1) âˆ’  ğ‘“(ğ·2)|   

(8) 

For  all  possible  realizations  of  D1  and  D2,  Where  f(D1) 
and  f(D2)  represent  the  true  responses  to  the  query  from 
D1  and  D2  [33]  [34]  [35]  [36].  According  to  Dwork 
(2006), the results to a query are presented as noise in the 
following way: 

ğ‘“(ğ‘¥) +  ğ¿ğ‘ğ‘ğ‘™ğ‘ğ‘ğ‘’(0, ğ‘)   

(9) 

Where b is defined as follows for Laplace noise: 

ğ‘ =

âˆ†ğ‘“

ğœ€

(10) 

X represents a particular realization of the database, while 
f(x)  represents  the  true  response  to  the  query,  the 
response  would  satisfy  ï¥-differential  privacy.  The  Î”f 
must  look  at  all  possible  realizations  of  D1  and  D2  [33] 
[34] [35] [36] [37]. We  could  take  an  example  in  which 
we query the GPA of students at Bowie State University. 
If  our  Min  GPA  in  the  database  is  2.0,  for  smallest 
possible  GPA,  and  our  Max  GPA  is  4.0  for  largest 
possible  GPA,  we  then  calculate  Î”f  as 2.0.  We  choose  a 
small  ï¥  value  of  0.01.  The  parameter  b  of  the  Laplace 
noise  is  set  to  Î”f/ï¥  =  2.0/0.01  =  200.  Thus  we  have 
Laplace  (0,  200)  noise  distribution.  Therefore 
the 
unperturbed results of the query + Noise from Laplace (0, 
200)  =  Perturbed  query  results  satisfying  ï¥-differential 
privacy.  [34]  It  has  been  noted  by  researchers  that  a 
smaller  ï¥  epsilon  value  creates  greater  privacy  by  the 
procedure.  However,  utility  risks  degeneration  with  a 
much  smaller  ï¥  epsilon  value  [38].  For  example,  ï¥  at 
0.0001,  will  give  b  as  20000,  Laplace  (0,  20000)  noise 
distribution.   

Figure  2:  A  general  Differential  Privacy  satisfying 
procedure. 

General steps for differential privacy shown in Figure 2: 

ï‚·  Run query on database 
ï‚·  Calculate the most influential observation 
ï‚·  Calculate the Laplace noise distribution   
ï‚·  Add  Laplace  noise  distribution  to  the  query 

results 

ï‚·  Publish perturbed query results. 

4.5. Differential privacy pros and cons 

Differential  privacy  grants  across-the-board 
privacy, and easy to implement  with SQL for  aggregated 
data  publication  [39].  However,  utility  is  a  challenge  as 
statistical  properties  change  with  a  much  smaller  ï¥ï€¬ï€ as 
Laplace noise addition takes into account the outliers and 
most influential observation. [38] More noise to the data 
at  the  level  of  the  most  influential  observation  only 
renders  the  data  useless  thus  balance  between  privacy 
and utility still a challenge [34] [37]. 

4.6. Statistical background for Noise addition   

In  this  section,  we  take  a  look  at  statistical 
considerations  for  data  perturbation  utilizing  noise 
addition.  With  noise  addition,  transformed  data  has  to 
keep  the  same  statistical  properties  as  the  original  data. 
Therefore  consideration  has  to  be  made  for  statistical 
characteristics  such  as  normal  distribution,  mean, 
variance,  standard  deviation,  covariance,  and  correlation 

 
 
           
 
 
 
 
 
     
 
 
 
 
for both original and perturbed data sets.   

The Mean Î¼, is the average of values after their total sum 
has  been  taken.  In  this  case  we  would  look  at  the 
summation  of  values  then  we  divide  them  by  the  n,  the 
quantity  of  values;  the  mathematical  statement  then  for 
the Mean Î¼, is straight forward: [16] 

ğœ‡ =  

1

ğ‘›

ğ‘›
  âˆ‘
ğ‘˜=0

ğ‘¥ğ‘–

(11) 

The  Normal  Distribution,  also  known  as  the  Gaussian 
distribution,  used  in  calculating  the  noise  addition,  is  a 
bell shaped continuous probability distribution used as an 
estimation  to  depict  real-valued  stochastic  variables  that 
agglomerate  around  a  single  mean.  The  formula  for 
normal distribution is as follows:[15] 

ğ‘“(ğ‘¥) =

1

âˆš(2 ğœ‹ ğœ2  )

  Ã—   ğ‘’âˆ’((ğ‘¥âˆ’ ğœ‡)2/2ğœ2)  

   (12) 

in 

the  bell  curve,  while 

The  parameter  Î¼  represents  the  mean,  the  point  of  the 
the  parameter  Ïƒ2 
peak 
representing  the  variance,  the  width  of  the  distribution. 
The annotation N (Î¼, Ïƒ2) represents a normal distribution 
with  mean  Î¼  and  variance  Ïƒ2.  Therefore  X~ğ‘(ğœ‡, ğœ2)  is 
representative of X distributed N (Î¼, Ïƒ2). The distribution 
with Î¼ = 0 and Ïƒâ€‰2 = 1 is cited to as the standard normal. 

The  Variance  Ïƒ2,  in noise  addition,  is  a  measure  of  how 
data distributes itself in approximation to the mean value. 
The expression for variance is given by: [17] 

ğœ2 =

âˆ‘(ğ‘‹âˆ’ ğœ‡)2  
(12) 
ğ‘
Where  Ïƒ2  is  the  variance,  Âµ  is  the  mean,  X  being  the 
single data values, N as the number of values, and âˆ‘ (X â€“ 
Âµ)2 as  the  summing  up  of  all    data  values  X  minus  the 
mean Âµ squared.   

The  Standard  Deviation,  Ïƒ,  is  a  measure  of  how 
distributed  data  is  from  the  normal,  thus  we  would  say 
standard  deviation  is  how  data  points  are  deviated  from 
the  mean.  The  mathematical  expression  is  simply  the 
square root of the variance Ïƒ2: [18] 

ğœ =   âˆšğœ2   

(13) 

Covariance:  With  noise  addition,  the  measurement  of 
how  affiliated  original  data  and  perturbed  data  are,  is 
crucial.  Covariance,  Cov(X,  Y),  is  a  calculation  of  how 
affiliated the deviations between the data points  X and Y 
are.  If  the  covariance  is  positive,  then  the  X  and  Y  data 
points'  inclination  is  to  increase  together,  else  if  the 
covariance  is  negative,  then  the  tendency  is  that  for  the 
two  data  points  X  and  Y,  one  lessens  while  the  other 
gains. However, if the covariance is zero, then this would 
signal  that  the  data  points  are  each  autonomous.  The 
expression for covariance is given as follows: [19]   

ğ¶ğ‘œğ‘£ ğ‘¥ğ‘¦ =  

1

ğ‘

 âˆ‘ ğ‘¥ğ‘–ğ‘¦ğ‘– âˆ’ ğ‘¥ğ‘¦   

(14) 

Correlation  ğ‘Ÿğ‘¥ğ‘¦   also  known  as  the  Pearson  product, 
calculates the capability and inclination of an additive or 
linear  relation  between  two  data  points.  The  correlation 
ğ‘Ÿğ‘¥ğ‘¦   is  dimensionless,  autonomous  of  the  parts  in  which 
the data points x and y are calculated [19]. If  ğ‘Ÿğ‘¥ğ‘¦   is = -1, 
then  ğ‘Ÿğ‘¥ğ‘¦   indicates a negative linear relation between the 
data  points  x  and  y.  If  ğ‘Ÿğ‘¥ğ‘¦   =  0,  then  the  linear  relation 
between  the  two  data  points  x  and  y  does  not  exist, 
however,  a  regular  nonlinear  relation  might  exist.  If  ğ‘Ÿğ‘¥ğ‘¦ 
= +1, then there is a strong linear relation between x and 
y. The expression used for correlation is: [19]   

ğ¶ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› =   ğ‘Ÿğ‘¥ğ‘¦ =

ğ¶ğ‘œğ‘£ ğ‘¥ğ‘¦
ğœğ‘¥ğœğ‘¦

(15) 

4.7. Signal Noise Ratio (SNR)   

In this section, we take  a look at SNR in relation to 
data  perturbation  using  noise  addition,  with  the  aim  that 
SNR  could  be  employed  to  achieve  optimal  data  utility 
while preserving privacy, by  measuring how  much noise 
we  need  to  optimally  obfuscate  data.  In  electronic 
signals, SNR is used to calculate a signal tainted by noise 
by  approximating the signal power to noise power ratio, 
basically  the  ratio  of  the  power  of  the  signal  without 
noise over the power of the noise. 

 ğ‘  =

 ğ‘– ğ‘› ğ‘™    ğ‘–ğ‘›  

ğ‘ğ‘œğ‘–      ğ‘– ğ‘›  

(16) 

With data perturbation, we could further borrow from the 
definition  of  SNR  employed  in  Image  Processing  were 
the ratio of mean to standard deviation of a signal is used,   
and  typically  SNR  is  computed  as  the  ratio  of  the  mean 
pixel  value  to  the  standard  deviation  of  the  pixel  values 
in a certain vicinity [29] [30]. 

 ğ‘  =  

ğœ‡

ğœ

(17) 

The  parameter  Î¼  in  this  case  represents  the  mean  of  the 
signal  and  the  parameter  Ïƒ  as  the  standard  deviation  of 
the  noise.  A  presumed  threshold  for  SNR  in  image 
processing  is  based  on  the  Rose  Criterion  which 
stipulates  that  an  SNR  of  5  is  desirable  in  order  to 
differentiate image details  with 100 per cent confidence. 
Therefore,  an  SNR  of  less  than  5  per  cent  will  result  in 
less  than  100  per  cent  confidence  in  recognizing 
particulars of an image [31].   

5. Illustration   

In  this  section,  we  provide  an  example  of  data 
perturbation with noise addition for illustrative purposes. 
We  follow  a  simple  algorithm  in  implementing  noise 
addition 
provide 
confidentiality in a published data set. The first step is the   

perturbation  methodology 

to 

 
  
 
 
   
 
 
 
 
 
 
 
     
 
 
      
 
 
 
   
 
 
de-identification  of  the  data  set  by  the  removal 
of  PII,  after  which  we  apply  noise  addition.  In  our 
implementation,  we  created  a  data  set  of  10  records  for 
illustrative  purposes  and  then  applied  the  algorithm 
below.  The  original  data  set  contained  PII,  we  de-
identified  the  original  data  set,  after  which  we  applied 
additive  noise  to  the  numerical  attributes,  and  we  then 
plotted  the  results  in  a  graph,  comparing  the  statistical 
properties of the original and perturbed data. 

Steps for De-identification and Noise Addition   
1. For all values of the data set to be published, 
2. Do data de-identification   
2.1. Find PII 
2.2 Remove PII 

3. For remaining data void of PII to be published, 

3.1. Find quantitative attributes in the data set 
3.2. Apply additive noise to the quantitative data 
values 

4. Publish data set 

5.1. Results of Illustration 

Table  1:  Original  Data  Set  (All  data  for  illustrative 
purposes).     

Table 2: Result after de-identification on original data. 

Table 4: Results of the Normal Distribution of Original Perturbed Scholarship Amount. 

Table 3: Random noise between 1000 and 9000 added to Scholarship attribute. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
7. References 

[1]  V.  Ciriani,  et  al,  2007.  Secure  Data  Management  in 
Decentralized  System,  Springer,  ISBN  0387276947, 
2007, pp 291-321. 

[2]  D.E  Denning  and  P.J  Denning,  1979.  Data Security, 
ACM Computing Surveys, Vpl. II, No. 3, September 
1, 1979. 

[3]  US  Department  of  Homeland  Security,  2008. 
Handbook  for  Safeguarding  Sensitive  Personally 
Identifiable  Information  at  The  Department  of 
[Online]. 
Homeland  Security,  October  2008. 
Available 
at: 
http://www.dhs.gov/xlibrary/assets/privacy/privacy_
guide_spii_handbook.pdf 

[4]  E.  Mccallister  and  K.  Scarfone,  2010.  Guide  to 
Protecting 
the  Confidentiality  of  Personally 
Identifiable Information ( PII ) Recommendations of 
the  National  Institute  of  Standards  and  Technology, 
NIST Special Publication 800-122, 2010. 

[5]  S.R.  Ganta,  et  al,  2008.  Composition  attacks  and 
auxiliary  information  in  data  privacy,  Proceeding  of 
the 14th ACM SIGKDD international conference on 
Knowledge discovery and   
- 
SIGKDD â€™08, 2008, p. 265. 

mining 

data 

[6]  A.  Oganian,  and  J.  Domingo-Ferrer,  2001.  On  the 
complexity  of  optimal  microaggregation 
for 
statistical  disclosure  control,  Statistical  Journal  of 
for 
the United Nations    Economic  Commission 
Europe, Vol. 18, No. 4. (2001), pp. 345-353. 

[7]  K.F.  Brewster,  1996.  The  National  Computer 
Security  Center  (NCSC)  Technical  Report  -  005V 
olume 1/5 Library No. S-243,039, 1996. 

[8]  P.  Samarati,  2001.  Protecting  Respondentâ€™s  Privacy 
in  Microdata  Release. 
IEEE  Transactions  on 
Knowledge  and  Data  Engineering  13,  6  (Nov./Dec. 
2001): pp. 1010-1027. 

[9]  L.  Sweeney,  2002.  k-anonymity:  A  Model  for 
Journal  on 
and  Knowledge-based 

Protecting  Privacy. 
Uncertainty,  Fuzziness 
Systems 10, 5 (Oct. 2002): pp. 557-570.   

International 

[10] Md  Zahidul  Islam,  Privacy  Preservation  in  Data 
Mining  Through  Noise  Addition,  PhD  Thesis, 
School  of  Electrical  Engineering  and  Computer 
Science,  University  of  Newcastle,  Callaghan,  New 
South Wales 2308, Australia, November 2007 

[11] Mohammad Ali Kadampur, Somayajulu D.V.L.N.,  A 
Noise Addition Scheme in Decision Tree for, Privacy 
Preserving  Data  Mining, 
JOURNAL  OF 
COMPUTING,  VOLUME  2,  ISSUE  1,  JANUARY 
2010, ISSN 2151-9617 

[12] Jay  Kim,  A  Method  For  Limiting  Disclosure  in 
Microdata 
and 
Transformation, Proceedings of the Survey Research 
Methods,  American  Statistical  Association,  Pages 
370-374, 1986. 

Random 

Based 

Noise 

[13] J.  Domingo-Ferrer,  F.  SebÃ©,  and  J.  CastellÃ -Roca, 
â€œOn  the  Security  of  Noise  Addition  for  Privacy  in 
in  Statistical 
Statistical  Databases,â€ 

in  Privacy 

Figure  3:  Results  of  the  normal  distribution  of  original 
and perturbed scholarship amount 

Covariance  between  Original  Scholarship  Data  set  and 
Perturbed Scholarship Data set = 1055854875.465. Since 
Covariance  is  positive,  it  shows  that  the  two  data  sets 
move together in the same direction. Correlation between 
Original Scholarship Data set  and Perturbed Scholarship 
Data set = 0.999. Since Correlation is a strong positive, it 
shows  a  relationship  between 
two  data  sets, 
increasing and decreasing together.   

the 

6. Conclusion 

We  have  taken  a  look  at  data  perturbation 
utilizing noise addition as a methodology used to provide 
privacy  for  published  data  sets.  We  also  took  a  look  the 
statistical  considerations  when  utilizing  noise  addition. 
We  provided  an  illustrative  example  showing  that  de-
identification  of  data  when  done  in  concert  with  noise 
addition would add more to the privacy of published data 
sets  while  maintaining  the  statistical  properties  of  the 
original data set. However, generating perturbed data sets 
that are statistically close to the original data sets is still a 
challenge as consideration has to be made for the tradeoff 
between utility and privacy; the more close the perturbed 
data  is  to  the  original,  the  less  confidential  that  data  set 
becomes,  and  the  more  distant  the  perturbed  data  set  is 
from the original, the more secure but then, utility of the 
data  set  might  be  lost  when  the  statistical  characteristics 
of the origin data set are lost. Noise generation certainly 
affects  the  level  of  perturbation  on  the  original  data  set. 
Yet  still,  striking  the  right  balance  between  privacy  and 
utility  remains  a  factor.  While  state  of  the  art  data 
perturbation  techniques  such  as  differential  privacy 
provide  hope  for  achieving  greater  confidentiality, 
achieving  optimal  data  privacy  while  not  shrinking  data 
utility  is  an  ongoing  NP-hard  task.  Therefore  more 
research needs to be done on how optimal privacy could 
be  achieved  without  degrading  data  utility.  Another  area 
of  research  is  how  noise  addition  techniques  could  be 
optimally  applied  in  the  cloud  and  mobile  computing 
arena, given the ubiquitous computing era.   

 
 
 
Databases,  vol.  3050,  Springer  Berlin  /  Heidelberg, 
2004, p. 519. 

[14] Huang  et  al,  Deriving  Private  Information  from 
Interest  Group  on 

Randomized  Data,  Special 
Management of Data - SIGMOD 2005 June 2005. 
[15] Lyman Ott and Michael Longnecker, An introduction 
to  statistical  methods  and  data  analysis,  Cengage 
Learning, 
0495017582, 
9780495017585, Pages 171-173 

2010, 

ISBN 

[16] Martin  Sternstein,  Barron's  AP  Statistics,  Barron's 
0764140892, 

Educational Series, 2010, ISBN 
Pages 49-51. 

[17] Chris  Spatz,  Basic  Statistics:  Tales  of  Distributions, 
Cengage  Learning,  2010,  ISBN  0495808911,  Page 
68. 

[18] David  Ray  Anderson,  Dennis  J.  Sweeney,  Thomas 
for  Business  and 
ISBN 

Arthur  Williams,  Statistics 
Economics,  Cengage  Learning,  2008, 
0324365055, Pages 95. 

[19] Michael J. Crawley, Statistics: an introduction using 
R,  John  Wiley  and  Sons,  2005,  ISBN  0470022973, 
Pages 93-95. 

[20] J.  Domingo-Ferrer  and  V.  Torra  (Eds.),  On  the 
Security of Noise  Addition for Privacy in Statistical 
Databases,  LNCS  3050,  pp.  149â€“161,  2004.# 
Springer-Verlag Berlin Heidelberg 2004. 

[21] Ruth  Brand,  Microdata  Protection  Through  Noise 
Addition,  LNCS  2316,  pp.  97â€“116,  2002.  Springer-
Verlag Berlin Heidelberg 2002. 

[22] [22]  Ciriani  et  al, Microdata  Protection,Secure  Data 
Management  in  Decentralized  System,    pages  291-
321, Springer, 2007. 

[23] Jay  J.  Kim  and  William  E.  Winkler,  Multiplicative 
Noise  for  Masking  Continuous  Data,  Research 
Report  Series,  Statistics  #2003-01,  Statistical 
Research Division, U.S. Bureau of the Census. 
[24] Rastogi  et  al,  The  boundary  between  privacy  and 
utility  in  data  publishing,  VLDB  ,September  2007, 
pp. 531-542.   

[25] Sramka  et  al,  A  Practice-oriented  Framework  for 
Measuring  Privacy  and  Utility  in  Data  Sanitization 
Systems, ACM, EDBT 2010. 

[26] Sankar,  S.R.,  Utility  and  Privacy  of  Data  Sources: 
and  Reveal 

Can  Shannon  Help  Conceal 
Information?, presented at CoRR, 2010.   

[27] Wong,  R.C.,  et  al,  Minimality  attack  in  privacy 
preserving  data  publishing,  VLDB,  2007.  pp.543-
554.   

[28] Adam,  N.R.  and  Wortmann,  J.C.,  A  Comparative 
Methods  Study  for  Statistical  Databases:  Adam  and 
Wortmann, ACM Comp. Surveys,    vol.21, 1989. 
[29] Jeffrey  J.  Goldberger,  Practical  Signal  and  Image 
Processing  in  Clinical  Cardiology,  Springer,  2010, 
Page 28-42 

[30] John  L.  Semmlow,  Biosignal  and  biomedical  image 
processing:  MATLAB-based  applications,  Volume 
22  of  Signal  processing  and  communications  CRC 
Press, 2004, ISBN 9780824750688, Page 11.   

[31] Jerrold  T.  Bushberg,  The  essential  physics  of 
medical  imaging,  Edition  2,  Lippincott  Williams  & 
Wilkins, 2002,  ISBN 0683301187,  9780683301182, 
Page 278-280. 

[32] Narayanan,  A.  and  Shmatikov,  V.,  2010.  Myths  and   
fallacies of "personally identifiable    information". In 
Proceedings of Commun. ACM. 2010, 24-26.   

[33] Dwork, C., Differential Privacy, in ICALP, Springer, 

2006 

[34] Muralidhar,  K.,  and  Sarathy,  R.,  Does  Differential 
Privacy Protect  Terry  Grossâ€™ Privacy?,  In Privacy in 
Statistical Databases, Vol. 6344 (2011), pp. 200-209. 
[35] Muralidhar,  K.,  and  Sarathy,  R.,  Some  Additional 
Insights  on  Applying  Differential  Privacy  for 
Numeric  Data,  In  Privacy  in  Statistical  Databases, 
Vol. 6344 (2011), pp. 210-219. 

[36] Dwork,  C.,  Differential  Privacy:  A  Survey  of 
Results,  In  Theory  and  Applications  of  Models  of 
Computation TAMC , pp. 1-19, 2008 

[37] M.  S.  Alvim,  M.  E.  AndrÃ©s,  K.  Chatzikokolakis,  P. 
Degano,  and  C.  Palamidessi,  "Differential  privacy: 
on  the  trade-off  between  utility  and  information 
leakage,"  Aug. 
[Online].  Available: 
http://arxiv.org/abs/1103.5188   

2011. 

[38] Fienberg,  S.E.,  et  al,  Differential  Privacy  and  the 
for  Multi-dimensional 
in  Statistical 

Risk-Utility 
Contingency  Tables 
Databases, Vol. 6344 (2011), pp. 187-199. 

In  Privacy 

Tradeoff 

[39] A.  Haeberlem,  B.C.  Pierce,  and  A.  Narayan, 
"Differential  privacy  under  fire,"  in  Proceedings  of 
the 20th USENIX Security Symposium, Aug. 2011.   
[40] Santos, R.J.; Bernardino,  J.;  Vieira,  M.;  ,  "A  survey 
in  data  warehousing:  Issues, 
on  data  security 
challenges 
- 
International  Conference  on  Computer  as  a  Tool 
(EUROCON),  2011  IEEE  ,  vol.,  no.,  pp.1-4,  27-29 
April 2011 

and  opportunities,"  EUROCON 

[41] Joshi,  P.;  Kuo,  C.-C.J.;  ,  "Security  and  privacy  in 
online  social  networks:  A  survey,"  Multimedia  and 
Expo  (ICME),  2011  IEEE  International  Conference 
on , vol., no., pp.1-6, 11-15 July 2011 

[42] Matthews,  Gregory 

J.,  Harel,  Ofer,  Data 
confidentiality:  A  review  of  methods  for  statistical 
disclosure  limitation  and  methods  for  assessing 
privacy, Statistics Surveys, 5,   
(2011), 1-29 (electronic). 

,  "State-of-the-art 

[43] Liu  Ying-hua;  Yang  Bing-ru;  Cao  Dan-yang;  Ma 
Nan; 
in  distributed  privacy 
preserving  data  mining,"  Communication  Software 
and Networks (ICCSN), 2011 IEEE 3rd International 
Conference  on  ,  vol.,  no.,  pp.545-549,  27-29  May 
2011 

 
