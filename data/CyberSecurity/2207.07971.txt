A Survey of Decision Making in Adversarial Games

Xiuxian Li, Min Meng, Yiguang Hong, and Jie Chen

1

2
2
0
2

l
u
J

6
1

]
T
G
.
s
c
[

1
v
1
7
9
7
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Game theory has by now found numerous applica-
tions in various ﬁelds, including economics, industry, jurispru-
dence, and artiﬁcial intelligence, where each player only cares
about its own interest in a noncooperative or cooperative manner,
but without obvious malice to other players. However, in many
practical applications, such as poker, chess, evader pursuing, drug
interdiction, coast guard, cyber-security, and national defense,
players often have apparently adversarial stances, that is, selﬁsh
actions of each player inevitably or intentionally inﬂict loss
or wreak havoc on other players. Along this line, this paper
provides a systematic survey on three main game models widely
employed in adversarial games, i.e., zero-sum normal-form and
extensive-form games, Stackelberg (security) games, zero-sum
differential games, from an array of perspectives, including basic
knowledge of game models, (approximate) equilibrium concepts,
problem classiﬁcations, research frontiers, (approximate) optimal
strategy seeking techniques, prevailing algorithms, and practical
applications. Finally, promising future research directions are
also discussed for relevant adversarial games.

Index Terms—Adversarial games, zero-sum games, Stackel-

berg games, differential games, Nash equilibrium, regret.

I. INTRODUCTION

Game theory has long been a powerful and conventional
paradigm for modeling complex and intelligent interactions
among a group of players and improving decision making for
selﬁsh players, since the seminal work [1]–[3] by John von
Neumann, John Nash, and others. Hitherto, it has found a
vast range of real-world applications in a variety of domains,
including economics, biology, ﬁnance, computer science, pol-
itics, and so forth, where each individual player is only
concerned with its own interest [4]–[6]. It played an extremely
important role even during the Cold War in the 60s, and has
been employed by many national institutions in defense, such
as United States Agencies for security control [7].

Adversarial games are a class of particularly important game
models, where players deliberately compete with each other
while simultaneously achieving their own utility maximization.
To date, adversarial games have been an orthodox framework
for shaping high-efﬁcient decision making in numerous re-
alistic applications, such as poker, chess, evader pursuing,
drug interdiction, coast guard, cyber-security, and national

This work was supported by National Natural Science Foundation of
China under Grant 62003243, Grant 62103305 and Grant 62088101, Shang-
hai Municipal Commission of Science and Technology No. 19511132100,
19511132101, and Shanghai Municipal Science and Technology Major
Project, No. 2021SHZDZX0100.

The authors are with the Department of Control Science and Engineer-
ing, College of Electronics and Information Engineering, and the Shang-
hai Research Institute for Intelligent Autonomous Systems, Tongji Uni-
versity, Shanghai, China (Email: xxli@ieee.org, mengmin@tongji.edu.cn,
yghong@tongji.edu.cn, chenjie206@tognji.edu.cn).

X. Li and M. Meng are also with the Institute for Advanced Study, Tongji

University, Shanghai, China.

team 1
n1 players
u1i

team 2
n2 players
u2i

(cid:266)(cid:266)(cid:266)(cid:266)

(cid:266)(cid:266)

team j

team m
nm players
umi

(cid:266)(cid:266)

framework of adversarial games with simultaneous
Fig. 1. A general
or sequential moves, perfect or imperfect
information, and symmetric or
asymmetric information, where triangles denote players and there exist
m teams, within which team members play in a cooperative manner,
while the play among teams is adversarial and usually zero-sum,
i.e.,
Pm
j=1 uij (xij , x−ij) = 0 for all strategies with the subscript ij
representing the j-th player in i-th team whose strategy and utility function
are denoted as xij and uij , respectively. And x−ij is the strategy proﬁle of
all players except the j-th player in team i.

i=1 Pni

defense, etc. For example, in Texas Hold’em poker, which has
been one of primary competitions as a benchmark for testing
researchers’ proposed algorithms in game theory and artiﬁcial
intelligence (AI) held by international well-known conferences
such as AAAI, multiple players compete against each other
to win the game by seeking sophisticated strategy techniques
[8]. Generally speaking, adversarial games enjoy several main
features as follows: 1) hardness of efﬁcient and fast algorithms
design with limited computing resources and/or samples; 2)
imperfect information for many practical problems, that is,
some information is private to one or more players which,
however, is hidden from other players, such as the card game
of poker; 3) large models, including large action spaces and
information sets, for example, the adversary space in the road
networks security problem is of the order 1018 [9]; 4) in-
complete information for a multitude of real-life applications,
that is, one or more agents do not know what game is being
played (e.g., the number of players, the strategies available
to each player, and the payoff for each strategy), and in this
case, the very game being played is generally represented with
players’ uncertainties, like uncertain payoff functions with
uncertain parameters; and 5) possible dynamic trait, i.e., the
played game is sometimes time-varying, instead of static, for
example, a poacher may have different poaching strategies in
a wildlife park as the environment varies with seasons. It is
worth pointing out that incomplete information is understood
distinctly from imperfect information here, as distinguished
by some researchers, although they are interchangeably used
in some literature. In addition, other possible characteristics
include bounded rationality, where players may be not fully

 
 
 
 
 
 
rational, such as arbitrarily random lone wolf attacks by
terrorists. However, it is noteworthy that not all adversarial
games are with imperfect and/or incomplete information, for
example, the game of Go has both perfect and complete
information, since it has explicit game rules and all chess
pieces’ positions are visible to both players at all times as well
as the actions of the opponent, which has been well solved by
well-known AI agents, such as AlphaGo and AlphaZero [10]–
[12].

As the competitive feature is ubiquitous in a large num-
ber of real-world applications, adversarial games have been
extensively investigated until now [13]–[18]. For example,
the authors in [13] provided a broad survey of technical
advances in Stackelberg security games (SSG) in 2018, the
authors in [14] reviewed some main Nash equilibrium (NE)
computing algorithms for extensive-form games with imper-
fect information based on counterfactual regret minimization
(CFR) methods, the authors in [15] reviewed a combined
use of game theory and optimization algorithms along with
a new categorization for researches conducted in this area, the
authors in [16] reviewed distributed online optimization, fed-
erated optimization from the perspective of privacy-preserving
mechanisms, and cooperative/non-cooperative games from two
facets, i.e., minimizing global costs and minimizing individ-
ual costs, and the authors in [17] surveyed recent advances
of decentralized online learning, including decentralized on-
line optimization and online game, from the perspectives of
problem classiﬁcations, performance metrics, state-of-the-art
performance results, and potential research directions in future.
Additionally,
in consideration of the importance of game
theory in national defense, some reviews of game theory in
defense applications were succinctly provided in [18], [19],
and a survey of defensive deception based on game theory
and machine learning (ML) approaches was presented in [20].
Nonetheless, a thorough overview for adversarial games from
the perspectives of the basic models’ knowledge, equilibrium
concepts, optimal strategy seeking techniques, research fron-
tiers, and prevailing algorithms is still lacking.

Motivated by the above facts,

this survey aims to pro-
vide a systematic review on adversarial games from sev-
eral dimensions, including the models of three main mod-
els frequently employed in adversarial games (i.e., zero-sum
normal-form and extensive-form games, Stackelberg (security)
games, and zero-sum differential games), (approximate) opti-
mal strategy concepts (i.e., NE, correlated equilibrium, coarse-
correlated equilibrium, strong Stackelberg equilibrium, team-
maxmin equilibrium, and corresponding approximate ones),
(approximate) optimal strategy computing techniques (e.g.,
CFR methods, AI methods), state-of-the-art results, prevail-
ing algorithms, potential applications, and promising future
research directions. To the best of our knowledge, this survey
is the ﬁrst systematic overview on adversarial games, generally
providing an orthogonal and complementary component for
the aforementioned survey papers, which may aid researchers
and practitioners in relevant domains. Please note that the three
game models are not mutually exclusive, but may overlap
for the same game from different viewpoints. For example,
Stackelberg games and differential games can also be zero-

2

sum games, etc. In addition, there actually exist other models
leveraged for adversarial games, such as Bayesian games,
Markov games (or stochastic games), signaling games, behav-
ioral game theory and evolutionary game theory. However, we
are not ambitious to review all of them in this survey, since
each of them is of independent interest and pretty abundant in
existing diverse materials.

The structure of this survey is organized as follows. The
detailed game models and solution concepts are introduced
in Section II, the existing main literature is reviewed along
with state-of-the-art results in Section III, some prevailing al-
gorithms are expounded in Section IV, an array of applications
are presented in Section V, promising future research direc-
tions are discussed in Section VI, and ﬁnally the conclusion
is drawn in Section VII.
Notations: Deﬁne [n]

:= {1, 2, . . . , n} be the set of n
positive numbers for an integer n. Denote by R, Rd, and
Rd
+ the sets of real numbers, d-dimensional real vectors, and
nonnegative d-dimensional real vectors, respectively. For a
ﬁnite set S with m elements, deﬁne ∆(S) = ∆m := {x ∈
m
Rm
i=1 xi = 1} (i.e., the simplex of dimension m − 1),
and |S| be the cardinality of S. Let P and E denote the
mathematical probability and expectation, respectively. Let x⊤
denote the transpose of x, and h·, ·i be the inner product. 0
and 1 denote vectors or matrices of all entries 0 and 1 with
compatible dimension in the context, respectively, sometimes
with explicit subscript being the dimension.

+ :

P

II. MODELS OF ADVERSARIAL GAMES

This section provides three main models for adversarial
games, i.e., zero-sum normal-form and extensive-form games,
Stackelberg (security) games and differential games, along
with solution concepts in these game models, and a general
framework of adversarial games is illustrated in Fig. 1.

A. Zero-Sum Normal-Form and Extensive-Form Games

Normal-form and extensive-form games are two widely
employed game models, accounting for simultaneous or se-
quential actions committed by the players in a game.

Normal-Form Games

(NFGs). A normal-form (or
strategic-form) game is denoted by a tuple (N, A, u) [4],
where N := [n] is a ﬁnite set of players. In the meantime,
A := A1 × · · · × An is the action proﬁle set for all players,
where Ai is the set of pure actions or strategies available to
player i ∈ [n], and a = (a1, . . . , an) ∈ A is a joint action
proﬁle. Moreover, u := (u1, . . . , un), where ui : Ai → R is
a real-valued utility (or payoff) function for player i. Also, a
mixed strategy/policy for player i is a probability distribution
over its action set Ai, denoted by πi ∈ ∆(Ai), and πi(ai)
denotes the probability for player i to commit an action
ai ∈ Ai. The expected utility ui(πi, π−i) of player i can
be expressed as Ea∼π(ui(a)), where π := (π1, . . . , πn) is
the joint (mixed) action proﬁle and π−i denotes the joint
action proﬁle of all players except player i. Similarly, let
a−i be the joint (pure) action proﬁle of all players except
player i, and denote ui by ui(ai, a−i) for manifesting the
dependency of a joint pure action proﬁle. The social welfare

n
is deﬁned as SW (a) :=
i=1 ui(a) for a pure action proﬁle
a ∈ A, whose mixed strategy correspondence is given as
SW (π) := Ea∼πSW (a). In addition, the game is called
constant-sum if for any action proﬁle a ∈ A, it holds that
n
i=1 ui(a) = cs for a constant cs, and called zero-sum if

P

cs = 0, as an illustration in Fig. 2.
P

1

2

u1(x1,x-1)+(cid:266)+un(xn,x-n)=0

n

(cid:266)(cid:266) 

3

Fig. 2. A schematic illustration of zero-sum games with n players.

Note that for the case with continuous action sets, generally
assumed closed and convex, they are usually called continuous
games.

In what follows, the extensive-form games with imperfect
information are introduced, which can reduce to ones with
perfect information when information sets of each player is a
singleton [5].

Imperfect-Information Extensive-Form Games

(II-
EFGs). An II-EFG is a tuple (N, H, Z, A, P, µ, u, I), where
N = [n] is a ﬁnite set of n players, H is a set of histories
(i.e., nodes), representing the possible sequence of actions,
and Z ⊆ H denotes the set of terminal nodes, which have no
further actions and award a value to each player. Outside of
N , a different “player” exists, denoted c, representing chance
decisions. Moreover, the empty sequence ∅ is included in H,
standing for a unique root node. At a nonterminal node h ∈ H,
A(h) := {a : (h, a) ∈ H} is the action function assigning a
set of available actions at h (here A(h) is different from A in
normal-form games, which should be clear from the context),
and P (h) is the player function assigning a player to the
node h who takes an action at that node with P (h) = c if
chance determines the action at h. And h ⊏ h′ means that
h is led to h′ by a sequence of actions, i.e., h is a preﬁx
of h′. u = (u1, . . . , un) is the set of utility functions, where
ui : Z → R is the utility function of player i. If there is a
n
i=1 ui(z) = cs for all z ∈ Z, then the
constant cs such that
game is called a constant-sum game, and a zero-sum game
when cs = 0.

P

The main feature “imperfect information” is represented
by information sets (infosets) for all players. Speciﬁcally,
I = (I1, . . . , In) is the set of information sets, where Ii is
a partition of Hi := {h : P (h) = i, h ∈ H} satisfying that
A(h1) = A(h2) and P (h1) = P (h2) for any h1, h2 ∈ Ii,j for
some Ii,j ∈ Ii. That is, all nodes in the same infoset of Ii
are indistinguishable to player i. Note that each node h ∈ H
is only in one infoset for each player. When all players can
remember all historical information, it is called perfect recall.
Formally, let h, h′, g, g′ be histories such that h ⊏ h′, g ⊏ g′,
and then perfect recall means that if g and h do not share an

infoset and each is not a preﬁx of the other, then h′ and g′
also do not share an infoset.

3

P
ui(π′

A normal-form plan (or pure strategy) of player i is a
tuple ai ∈ Ξi := ×Ii,j ∈Ii A(Ii,j ), which assigns an action to
each infoset of player i. A normal-form strategy xi means a
probability distribution over Ξi, i.e., xi ∈ ∆(Ξi). A behavioral
strategy πi (or simply, strategy) is a probability distribution
over A(Ii,j ) for each infoset of player i. A joint strategy
proﬁle π is composed of all players’ strategies πi, i ∈ [n],
i.e., π = (π1, . . . , πn), with π−i representing all the strategies
except πi. Denote by p(Ii,j , a) (or p(h, a)) the probability of a
speciﬁc action a at infoset Ii,j , and pπ(h) the reach probability
of history h if all the players select their actions according to
π. For a strategy proﬁle, player i has its total expected payoff
h∈Z pπ(h)ui(h). Denote by Σi the set of all
as ui(π) =
possible strategies for player i.

i

i , π∗

i , π∗

−i) − ui(πi, BR(πi)), where (π∗

A best response for player i to π−i is a strategy BR(π−i) :=
i, π−i). In a two-player zero-sum game, the
arg maxπ′
exploitability e(πi) of a strategy πi, deﬁned as e(πi) :=
ui(π∗
−i) is a Nash
equilibrium, as deﬁned later. In multi-player games, the total
exploitability (or NashConv) of a strategy proﬁle π is deﬁned
i∈[n] ui(BR(π−i), π−i) − ui(πi, π−i), and
as [21] e(π) :=
the average exploitability (or simply exploitability) is deﬁned
as e(π)/|N |, which is leveraged to measure how much can
be gained by unilaterally deviating to their best response,
generally interpreted as a distance from a Nash equilibrium.
Note that besides the above normal-form and extensive-
form games, other classes of games may be conducive as well
in adversarial games, such as Markov games (or stochastic
games) [22], where the game state changes according to a
transition probability based on the current game state and
players’ actions, Bayesian games [23], which models game
uncertainties with incomplete information, and so forth.

P

In what follows, some solution concepts for related games

are introduced.

The Nash equilibrium is the most widely adopted notion in

the literature [2].

Deﬁnition 1 (ǫ-Nash Equilibrium (ǫ-NE)). For both normal-
1 , . . . , π∗
form and extensive-form games, a strategy π = (π∗
n)
is called an ǫ-NE for a constant ǫ ≥ 0 if

ui(π∗

i , π∗

−i) ≥ ui(πi, π∗

−i) − ǫ,

∀πi, i ∈ N

(1)

is a best response of π∗
−i), ∀i ∈ [n].

that is, the gain is at most ǫ if any player changes its own
strategy solely. Moreover, it is called an NE when ǫ = 0, that
is, π∗
−i for any player i ∈ [n], i.e.,
i
i = BR(π∗
π∗
It is well known that there exists at least one NE in mixed
strategies for games with ﬁnite number of players and ﬁnite
number of pure strategies for each player [2].

Even though NE may exist for many games and it
is
computationally efﬁcient for two-player zero-sum games, it
is well known by complexity theory that approximating an
NE in κ-player (κ ≥ 3) zero-sum games and even two-
player nonzero-sum games is computationally hard, that is,
is PPAD-complete for general games [24]–[26]. As an
it
alternative, (coarse) correlated equilibrium is often considered

4

for normal-form games in the literature, which is efﬁciently
computable in all normal-form games, as deﬁned in the
following [27].

Deﬁnition 2 (ǫ-Correlated Equilibrium (ǫ-CE)). For a normal-
form game (N, A, u), an ǫ-CE is a probability distribution µ
over ×i∈[n]Ai if for each player i ∈ [n] and any swap function
φi : Ai → Ai (usually called strategy modiﬁcation),

Ea∼µ[ui(ai, a−i)] ≥ Ea∼µ[ui(φi(ai), a−i)] − ǫ.

(2)

That is, no player can gain more payoff by unilaterally
deviating its action privately informed by a coordinator who
samples a joint action a = (a1, . . . , an) from that distribution.
Furthermore, another relevant notion is deﬁned below [28].

Deﬁnition 3 (ǫ-Coarse Correlated Equilibrium (ǫ-CCE)). For
a normal-form game (N, A, u), an ǫ-CCE is a probability
distribution µ over ×i∈[n]Ai if for each player i ∈ [n] and
all actions a′

i ∈ Ai,

Ea∼µ[ui(ai, a−i)] ≥ Ea∼µ[ui(a′

i, a−i)] − ǫ.

(3)

The above condition looks like almost the same as that for
ǫ-CE, except the removal of the conditioning on the action ai,
by arbitrarily selecting an action a′
i on their own, instead of
following the action ai advised by the coordinator. For NE,
CE, and CCE, it is known that they are payoff equivalent
to each other in two-player zero-sum games by the minimax
theorem [29]. Recently, the notions of CE and CCE have been
extended to extensive-form games in [30], [31], which however
have been less studied by now.

where UT stands for the team’s utility deﬁned by UT(s) :=
l∈Z′ uT(l)c(l) if at least one terminal node is achieved by
the joint plan s (i.e., Z ′ ⊆ Z is nonempty) with the chance
P
c(l) determined by chance nodes, and UT(s) = 0 otherwise.

A TME is generally unique and it is an NE which maximizes
the team’s utility. In addition, the concept of ǫ-TME can be
similarly deﬁned, at which both the team and the adversary can
gain at most ǫ if any player unilaterally changes its strategy.
Besides the aforementioned optimal strategy concepts, it is
worth noting that there are other notions as well, such as
subgame perfect NE [4] and α-rank [34], which however are
beyond this survey.

B. Stackelberg Games

Stackelberg games (SGs, or leader-follower games) can date
back to Stackelberg competition introduced in [35] to model a
strategic game between two ﬁrms, the leader and the follower,
where the leader can take actions ﬁrst. SGs, as games with
sequential actions and asymmetric information, have many
practical applications, for example, PROTECT, a system that
the United States Coast Guard utilizes to assign patrols in
Boston, New York, and Los Angeles [36], and ARMOR, an
assistant deployed in Los Angeles International Airport in
2007 for randomly scheduling checkpoints on the roadways
entering the airport. In what follows, general Stackelberg
games and Stackelberg security games [37] are introduced,
where the second one is an important special case of general
SGs.

In an II-EFG, let us consider the case where all the players
in T := {1, . . . , n − 1} are cooperative, thus forming a team,
who take actions independently and play against an adversary
i∈T ui,
n, and ui = uj, ∀i, j ∈ T and un = −uT = −
called a zero-sum single-team single-adversary extensive-form
team game (or simply zero-sum team game (TG)) [32]. Before
is
introducing the notion of team-maxmin equilibrium,
necessary to ﬁrst prepare some essentials. Let Si denote the
set of action sequences of player i, where an action sequence
of player i, deﬁned by a node h ∈ H, is the ordered set of
actions of player i that are on the path from the root to h.
Let ∅ be the dummy sequence to the root. A realization plan
ri : Si → [0, 1] is a function mapping each action sequence
to a probability, satisfying

P

it

ri(∅) = 1,

ri(si, a) = ri(si),

∀Ii,j ∈ Ii, si = seqi(Ii,j ),

Xa∈A(Ii,j )

ri(s′

i) ≥ 0,

∀s′

i ∈ Si,

(4)

where seqi(Ii,j ) denotes the action sequence leading to Ii,j .
With the above preparations, the team-maxmin equilibrium,

ﬁrst introduced in [33], is deﬁned as follows [32].

Deﬁnition 4 (Team-Maxmin Equilibrium (TME)). A TME is
deﬁned as

arg max

r1,...,rn−1

min
rn

n

UT(s)

ri(si),

(5)

s=×i∈N si,s∈S
X

i=1
Y

leader

followers

1

2 (cid:266)(cid:266) 

p

Fig. 3. A schematic illustration of general Stackelberg games, where directed
edges mean that the leader commits an action ﬁrst and then the followers play
actions in response to the leader’s action.

General Stackelberg Game (GSG). A GSG consists of a
leader, who commits an action ﬁrst, and p followers, who can
observe and learn the leader’s strategy and then take actions
in response to the leader’s strategy, see Fig. 3. Denote by F ,
Al, Af the sets of p followers, the leader’s pure strategies,
and each follower’s pure strategies, respectively. The leader
knows the probability of facing follower k ∈ F , denoted
as ̟k ∈ [0, 1]. Denote by x ∈ ∆(Al) the mixed strategy
of the leader, where the i-th component xi represents the
probability of choosing the i-th pure strategy by the leader.
Let qk
j ∈ {0, 1} denote the decision of follower k ∈ F to
qk
j = 1 for
take a pure strategy j ∈ Af such that
all k ∈ F . Note that it is enough for the rational followers
to only consider pure strategies [38]. For the leader and each
follower k ∈ F , the utilities (or payoffs, rewards) of the leader
and the follower are captured by a pair of matrices (Rk, Ck),
where Rk ∈ R|Al|×|Af | is the utility matrix of the leader when

j∈Af

P

5

choosing a strategy arbitrarily close to the equilibrium that
makes the follower prefer the desired strategy [40]. When
the tie-breaking rule is in favor of the followers, then the
equilibrium is called weak Stackelberg equilibrium (WSE),
which however does not always exist [41]. Moreover, the
concept of SSE can be similarly deﬁned for SSGs.

C. Zero-Sum Differential Games

Differential games (DGs), also known as dynamic games
[41], are a natural extension of sequential games to continuous-
time scenario, which are expressed by differential equations
and ﬁrst introduced by Isaacs [42]. DGs can be regarded as an
extension of optimal control [43], which usually has a single
decision maker with a single objective function, while multiple
players are involved in a DG with noncooperative objectives.
Since this survey is concerned with adversarial games, zero-
sum DGs (mostly involving two players in the literature) are
considered here, although many other types of DGs emerge in
the literature, including nonzero-sum differential games, mean-
ﬁeld games, differential graphical games, Dynkin games, and
so on [44], [45].

A two-player zero-sum differential game (TP-ZS-DG) is

described by a dynamical system as

t ∈ [t0, T ]

˙x(t) = f (t, x(t), u(t), v(t)),
x(t0) = x0, u(t) ∈ U, v(t) ∈ V,

(10)
where x(t) ∈ Rd is the state vector at time t, t0 is the initial
time, x0 is the initial state, U ⊆ Rm1, V ⊆ Rm2 are control
constraints for players 1 and 2, respectively, u(t) and v(t) are
control actions (or signals) for player 1 and 2, respectively,
and f : [t0, T ] × Rd × U × V → Rd is the dynamics, as
illustrated in Fig. 4.

facing follower k, and Ck ∈ R|Al|×|Af | is the utility matrix
of follower k ∈ F . Then, the expected utilities of the leader
and follower k can be, respectively, given as

Ul(x, q) =

U k

f (x, qk) =

̟kxiqk

j Rk
ij,

i∈Al X
X

j∈Af X
k∈F
xiqk

j Ck
ij,

i∈Al X
j∈Af
X

(6)

(7)

where q := (q1, . . . , q|F |) and qk := (qk
k ∈ F .

1 , . . . , qk

|Af |) for each

Stackelberg Security Game (SSG). In SSG, as a speciﬁc
case of GSG, the leader and followers are viewed as the
defender and attackers, where the defender aims to schedule a
limited number of m security resources to protect (or cover)
a subset of n targets from the attackers’ attacks, with m < n.
The notations F, Al, Af , ̟k, x, qk
j are the same deﬁned as in
the above GSG. Noting that |Af | = n in this case, the leader’s
pure strategy set Al is now composed of all possible subsets of
at most m targets that can be safeguarded simultaneously, and
qk
j ∈ {0, 1} indicates whether attacker k ∈ F attacks target
j ∈ [n]. Let cj ∈ [0, 1] be the probability of coverage of target
i∈Al,j∈i xi, where j ∈ i connotes
j ∈ [n] such that cj =
that the target j is covered by pure strategy i. When facing
attacker k ∈ F who attacks target j ∈ [n], the defender’s
utility is Dk
u(j)
if the target is uncovered or unprotected. The utility of attacker
k ∈ F is Ak
c (j) when attacking target j that is covered, or
Ak
u(j) when attacking target j that is uncovered. It is generally
assumed that Dk
u(j) and Ak
c (j), which are
in line with the common sense. The expected utilities for the
defender and attacker k ∈ F is, respectively, expressed as

c (j) if the target is covered or protected, or Dk

c (j) ≥ Dk

u(j) ≥ Ak

P

Ud(x, q) =

j∈Af X
k∈F
X

̟kqk

j [cjDk

c (j) + (1 − cj)Dk

u(j)],

U k

a (x, qk) =

j∈Af
X

j [cjAk
qk

c (j) + (1 − cj)Ak

u(j)].

(8)

(9)

1,u(t)

ẋ(t)=f(t,x(t),u(t),v(t))

2,v(t)

A most widely adopted solution for GSG and SSG is the
so-called strong Stackelberg equilibrium, which always exists
in all Stackelberg games [37], [39]. Recall that it is enough
for each follower to play pure strategies.

Deﬁnition 5 (Strong Stackelberg Equilibrium (SSE)). A strat-
egy proﬁle (x∗, {qk∗}k∈F ) for a GSG forms an SSE, if

1) x∗ is optimal for the leader:

(x∗)⊤Rkqk∗ ≥ x⊤RkRk(x), ∀x ∈ ∆(Al), ∀k ∈ F

where Rk(x) denotes the attacker k’s best response
against x.

2) Each follower k always plays a best-response, i.e.,

(x∗)⊤Ckqk∗ ≥ (x∗)⊤Ckqk, ∀ feasible qk.

3) Each follower k breaks ties in favor of the leader:

(x∗)⊤Rkqk∗ ≥ (x∗)⊤RkRk(x∗), ∀k ∈ F.

Fig. 4. A schematic illustration of two-player differential games.

For different setups in the literature, distinct cost functions
are generally employed, most of which, however, are either
based on or variants of an essential and important cost func-
tion, as given below:

J(u(·), v(·)) =

f0(t, x(t), u(t), v(t))dt + φ(x(T )), (11)

T

t0

Z

where f0 : [t0, T ] × Rd × U × V → R is the running cost
(or stage cost) and φ : Rd → R is the terminal cost (or ﬁnal
cost).

With (11), the goal of DG (10) is for player 1 to minimize

the cost J, while player 2 aims at maximizing it, i.e.,

min
u(·)∈U

max
v(·)∈V

J(u(·), v(·)).

(12)

The tie-breaking rule is reasonable in cases of indifference
since the leader can often induce the favorable equilibrium by

For (12), the optimal cost of J is called the value of the
game, expressed as a value function ψ(t, x). Moreover, the

6

TABLE I
Main features of various adversarial games.

Game models

Player numbers

Action order

Information

Dynamics

Zero-sum NFG

Zero-sum EFG

GSG and SSG

Zero-sum DG

≥ 2

mostly simultaneous

symmetric

discrete-time
continuous-time

≥ 2 (perfect information)
mostly 2 (imperfect information)
mostly one-leader
p-follower
mostly 2

sequential

symmetric

mostly discrete-time

sequential

asymmetric

mostly discrete-time

mostly simultaneous

mostly symmetric

continuous-time

solution notion is still the NE as in normal-form and extensive-
form zero-sum games, also called minimax equilibrium (or
minimax point, saddle point) in the literature since the studied
problem is in fact a saddle point game (or saddle point
problem/optimization).

Note that dynamics (10) is deterministic. In the meantime,
stochastic DGs have also been addressed in the literature,
described by stochastic differential equations with the standard
Brownian motion [44]. It is also noteworthy that the above
DGs are usually studied under a set of assumptions, such as the
compactness of U, V , and the Lipschitz continuity of f, f0, φ,
among others [45].

Finally, the main features of the aforementioned games are

summarized in Table I.

III. RESEARCH CLASSIFICATION AND FRONTIERS

This section aims to succinctly summarize the relevant
literature for zero-sum games, GSGs, SSGs, and TP-ZS-DGs
along with the emerging state-of-the-art research. However, the
relevant literature on adversarial games is too vast to cover it
all, and thus only the literature of our interest is reviewed
throughout this survey.

A. Zero-Sum Games (ZSGs)

Both normal-form and extensive-form ZSGs studied in the
literature can be generally categorized into the following
main aspects: bilinear games, saddle point problems, multi-
player ZSGs, team games, and imperfect-information ZSGs,
as discussed below.

1) Bilinear Games. Bilinear games are simple models for
delineating two-player games, generally in normal-form
as [46]: maximizing utilities x⊤Ay and x⊤By for
players 1 and 2, respectively, where A ∈ Rm×n and
B ∈ Rm×n are payoff matrices, subject to strategy sets
x ∈ X := {x ∈ Rm : R1x = r1, x ∈ Rm
+ } and
y ∈ Y := {y ∈ Rn : R2y = r2, y ∈ Rn
+} with some
R1 ∈ Rk1×m, R2 ∈ Rk2×n and r1 ∈ Rk1 , r2 ∈ Rk2 . A
bilinear game is usually denoted by the payoff matrices
pair (A, B), which is zero-sum when B = −A, and
as an important notion, the rank of a game (A, B) is
deﬁned as the rank of matrix A + B. Several interest-
ing games can be viewed as special cases of bilinear
games, such as bimatrix games [47]–[49], where R1 =
1⊤
m, R2 = 1⊤
n and r1 = r2 = 1, imitation games (a
special case of bimatrix games with B = I) [50], and the
Colonel Blotto game (i.e., two colonels simultaneously

allocate their troops across different battleﬁelds) [51]. In
addition, multi-player polymatrix games [52] can also be
equivalently transformed to bilinear games [46]. Gener-
ally speaking, the existing literature mainly focuses on
the computational complexity and polynomial-time al-
gorithm design for approximating NE of bilinear games
[53], bimatrix games [54], polymatrix games [55], and
the Colonel Blotto game [56]. Recently, it is shown that
NE computation in two-player nonzero-sum games with
rank ≥ 2 is PPAD-hard [57], [58]. And computing a
s-approximate NE is PPAD-hard even for imitation
1/nc
games for any c > 0 [50], where ns is the number of
moves available to the players, and a polynomial-time
algorithm was developed for ﬁnding an approximate NE
in [50]. Also, computing an NE in a tree polymatrix
game with twenty actions per player is PPAD-hard [55],
and a polynomial-time algorithm for 1/3-approximate
NE in bimatrix games was proposed in [54], which is the
state-of-the-art in the literature. For the Colonel Blotto
game, efﬁcient and simple algorithms have been recently
provided in [59]–[61], and meanwhile, various scenarios
have been extended for this game, including dynamic
Colonel Blotto game [62], generalized Colonel Blotto
and generalized lottery Blotto games [63], and multi-
player cases [61], [64]. Furthermore, bilinear games are
generalized to hidden bilinear games in [65], where the
inputs controlled by players are ﬁrst processed by a
smooth function, i.e., a hidden layer, before coming into
the conventional bilinear games.

2) Saddle Point Problems (SPPs). SPPs are also called
saddle point optimization, min-max/minimax games,
or min-max/minimax optimization in the literature.
The formulation of a general SPP [66] is given as
minx∈X maxy∈Y f (x, y), where X ⊆ Rm and Y ⊆ Rn
are closed and convex, possibly the entire Euclidean
spaces or their compact subsets. For general SPPs,
besides zero-sum bilinear games, other two types have
been extensively considered, that is, non-bilinear SPPs
and bilinear SPPs. A non-bilinear SSP [67], [68] is
expressed as minx∈X maxy∈Y f (x) + Θ(x, y) − h(y),
where Θ is a general coupling function, and as a special
case, when Θ(x, y) = x⊤Cy with C ∈ Rm×n, the
game is called a bilinear SPP [69]–[71] due to the
bilinear coupling. The existing research mainly centers
on equilibrium existence, computational and sampling
complexity, and efﬁcient algorithm design, for instance,

as done in the aforementioned recent works. Mean-
while, various scenarios have been investigated in the
literature, including projection-free methods by apply-
ing the Frank-Wolfe algorithm [72], [73], nonconvex-
nonconcave general SPPs [74], [75], linear last-iterate
convergence [76], SSPs with adversarial bandits and
delays [77], periodic zero-sum bimatrix games with
continuous strategy spaces [78], compositional SSPs
[79], decentralized setup [80], and hidden general SPPs
[81], where the controlled inputs are ﬁrst fed into smooth
functions whose outputs are then treated as inputs for the
traditional general SPPs. Finally, it is noteworthy that
the general SPPs with sequential actions have also been
studied, called min-max Stackelberg games, for example,
the recent work [82] with dependent feasible sets.
3) Multi-Player Zero-Sum Games (MP-ZSGs). The above
discussed games usually involve two players. It is well
known that approximating an NE in multi-player zero-
sum games and even two-player nonzero-sum games
is PPAD-complete [24]–[26]. Moreover, it
is known
that multi-player symmetric zero-sum games might have
only asymmetric equilibria, which is consistent with
that of two-player and multi-player symmetric nonzero-
sum games, but in contrast with the case in two-player
symmetric zero-sum games that always have symmetric
equilibria (if equilibria exist) [83]. In the literature,
most of works focus on multi-player zero-sum polyma-
trix games (also called network matrix games in some
works), where the utility of each player is composed of
the sum of utilities gained by playing with its neighbors
in an undirected graph [52]. The authors in [84] gener-
alized von Neumann’s minimax theorem to multi-player
zero-sum polymatrix games, thus, implying convexity
of equilibria, polynomial-time tractability, and conver-
gence of no-regret learning algorithms to NEs, and last-
iterate convergence was studied in [85] for multi-player
polymatrix zero-sum games. O(1/T ) time-average con-
vergence was established by using alternating gradient
descent in [86], where T is the time horizon. Moreover,
it is shown that for continuous-time algorithms, time-
average convergence may fail even in a simple periodic
multi-player zero-sum polymatrix game or replicator
dynamics, but being Poincar´e recurrent in [87], [88].
What’s more, it is realized that mutual cooperations
among players may beneﬁt more than pursuing selﬁsh
exploitability, and in this case, team/alliance formation
is also studied in the literature, for example, [89], where
it was demonstrated that team formation may be seen as
a social dilemma. Additionally, other pertinent research
encompasses multi-player general-sum games [90]–[92]
and machine learning based studies [93], etc.

4) Team Games (TGs). Generically, team games refer to
those games where at least one team exists with the
cooperation of team members with communications
either before the play, or during the play, or simul-
taneously before and during the play, or without any
communications. In general, team games in the literature
can be classiﬁed from two perspectives. One perspective

7

depends upon the team number, i.e., one-team games (or
adversarial team games) [94], where players in the team
enjoying the same utility function play against an adver-
sary independently, and two-team games [95] consisting
of two teams in a game. The other perspective is on
perfect-information and imperfect-information games.
For team games, TME is an important solution concept,
for which it is known that computing a TME is FNP-
hard and inapproximable in additive sense [96], [97].
Even though, efﬁcient algorithms for computing a TME
in perfect-information zero-sum NFGs have been devel-
oped until now, e.g., [94]. Meanwhile, a class of zero-
sum two-team games in perfect-information normal-
form was studied in [95], where ﬁnding an NE is shown
to be CLS-hard, i.e., unlikely to have a polynomial-
time NE computing algorithm. Moreover, as two-team
games, two-network zero-sum games are also addressed,
where each network is thought of as a team [98]–[100].
the
For imperfect-information zero-sum team games,
researchers have investigated a variety of scenarios cen-
tering around the computational complexity and efﬁcient
algorithms, such as one-team games [32], [101], [102],
one-team game with two members in the team [103], the
computation of team correlated equilibrium in two-team
games [104].

5) Imperfect-Information ZSGs (II-ZSGs). Unlike perfect-
information games, such as Chess, Go and backgammon,
II-ZSGs, involving individual players’ primate informa-
tion that is hidden to other players, are more challenging
due to information privacy and uncertainty, especially
for large games with large action spaces and/or infosets.
For example, the game of heads-up (i.e., two-player)
limit Texas Hold’em poker, with over 1014 infosets, has
been a challenging problem for AI for over 10 years,
before essentially solved by Cepheus [105], the ﬁrst
computer program for handling imperfect information
games that are played competitively by humans. Also,
the game of no-limit Texas Hold’em poker has more
than 10161 infosets, for which DeepStack [106] and
Libratus [107] are the ﬁrst line of AI agents/algorithms
to defeat professional humans in heads-up no-limit Texas
Hold’em poker. As such, most of research focuses on the
computing of NEs in two-player II-ZSGs in the literature
[108], [109], aiming to develop efﬁcient superhuman AI
agents in face of the challenges of imperfect informa-
tion, large models and uncertainties. To handle large
games with imperfect information, several techniques
have been successively proposed, for exmaple, pruning,
abstraction, and search [110]–[112]. Roughly speaking,
pruning aims to avoid traversing the whole game tree for
an algorithm while simultaneously ensuring the same
convergence, including regret-based pruning, dynamic
thresholding, best-response pruning, and so on [113].
Abstraction aims to generate a smaller version of the
original game by bucketing similar infosets or actions,
while maintaining as much as possible the strategic
features of the original game [114], mainly including in-
formation abstraction and action abstraction. Meanwhile,

search tries to improve upon the (approximate) solution
of a game abstraction, which may be far from the true
solution of the original game, by seeking a more precise
equilibrium solution for a faced subgame, such as depth-
limited search [111], [115]. Moreover, it has been shown
recently that some two-player poker games can be rep-
resented as perfect-information sequential Bayesian ex-
tensive games with efﬁcient implementation [116]. The
authors in [117] recently bridged several standing gaps
between NFG and EFG learning by directly transferring
desirable properties in NFGs to EFGs, guaranteeing si-
multaneously last-iterate convergence, lower dependence
on the game size, and constant regret in games. Besides,
bandit feedback is of practical importance in real-world
applications for II-ZSGs [118], [119], where only the
interactive trajectory and the payoff of the reached ter-
minal node can be observed without prior knowledge of
the game, such as the tree structure, the observation/state
space, and transition probabilities (for Markov games)
[120]. On the other hand, multi-player II-ZSGs are more
challenging and thus have been less researched except
for a handful of works, for example, Pluribus [121], the
ﬁrst multi-player poker agent, has defeated top humans
in six-player no-limit Texas Hold’em poker (the most
prevalent poker in the world) [122], and other endeavors
[119], [123]–[126]. Aside from deterministic methods,
AI approaches have achieved great success in II-SSGs
based on reinforcement learning, deep neural networks
and so on [106], [120], [127]–[138], for instance, Al-
phaGo (the ﬁrst AI agent to achieve superhuman level
in Go) [10], AlphaZero (with initial training independent
of human data and Go-speciﬁc features, reaching state-
of-the-art performance in Go, Chess and Shogi with
minimal domain knowledge) [11], and DeepStack [106],
to name a few. More details can refer to a recent survey
for AI in games [139]. Note that other closely related
research subsumes imperfect-information general-sum
games with full and bandit feedback [140]–[142], two-
player zero-sum Markov games [143], and multi-player
general-sum Markov games [144].

It should be noted that incomplete information is also im-
portant in adversarial games, mainly comprising of Bayesian
games (cf. a recent survey [145]).

B. Stackelberg Games

Stackelberg games are roughly summarized from four per-
spectives, i.e., GSGs, SSGs, continuous Stackelberg games,
and incomplete-information Stackelberg games.

1) GSGs. The research on GSGs mainly lies in three as-
pects, i.e., computational complexity, solution methods,
and their applications. For computational complexity,
when only having one follower in GSGs, it is known
that the problem can be solved in polynomial time,
while it is NP-hard for the multiple followers case [38].
Regarding solution methods, there are an array of pro-
posed methods in the literature, but primarily depending
upon approaches for coping with linear programming

8

(LP) and mixed integer linear programming (MILP),
including cutting plane methods, enumerative methods,
and hybrid methods, among others [146], [147]. Note
that both GSGs and SSGs can be formulated as bilevel
optimization problems [146], [147], where bilevel op-
timization has a hierarchical structure with two level
optimizations, one lower level optimization (follower)
nested in another upper level optimization (leader) as
constraints, which is an active research area unto itself
[148]. As for practical applications, a multitude of real-
world problems have been tackled using Stackelberg
games, such as economics [149], smart grid [150], [151],
wireless networks [152], dynamic inspection problems
[153], industrial internet of things [154], etc. It should
be noted that other relevant cases have also been studied
in the literature, such as multi-leader cases [155]–[159],
the case with bounded rationality [160], and general-sum
games [161], etc.

2) SSGs. In general, SSGs can be classiﬁed by the func-
tionality of security resources. To be speciﬁc, when
every resource is capable of protecting every target, it is
called homogeneous resources, and when resources are
restricted to protecting only some subset of targets, it
is called heterogeneous resources. Meanwhile, resources
can also be distinguished by how many targets they are
able to cover simultaneously, and in this case, a notion,
called schedule, is assigned to a resource with the size of
the schedule being deﬁned to be the number of targets
that can be simultaneously covered by the resource,
including the case with size 1 [162] and greater than 1
[163]. For these scenarios, the computational complexity
was addressed in [164] when existing a single attacker,
as shown in Table II. With regard to solution methods,
similar methods for solving GSGs can be applied to
handle SSGs. Moreover, the practical applications of
SSGs encompass wildlife protection [165], passenger
screening at airports [166], crime prevention [167],
cyber-security [168], information security [169], border
patrol [170], [171], and so forth. In the meantime, there
are other scenarios addressed in the literature, like multi-
defender cases [172], [173], Bayesian generalizations
[174], and the case with bounded rationality [175] and
ambiguous information [176], etc.

TABLE II
Complexity results with a single attacker [164].

SSGs

Homogeneous resources

Heterogeneous resources

Size of schedule

1

P

P

2

P
NP-hard

≥ 3
NP-hard
NP-hard

3) Continuous Stackelberg games. This sort of games mean
Stackelberg games with continuous strategy spaces. In
general, there exist two players, a leader and a follower,
who have cost functions f1 : Ω → R and f2 : Ω →
R with Ω := X × Y , respectively, where X ⊆ Rd1
and Y ⊆ Rd2 are closed convex and possibly compact

strategy sets for the leader and the follower, respectively.
Then, the problem can be formally written as

min
x∈X

{f1(x, y) : y ∈ arg min
y∈Y

f2(x, y)},

(13)

where the follower still takes actions in response to
the leader after the leader makes its decision ﬁrst. In
this case, a strategy x∗ ∈ X of the leader is called a
Stackelberg equilibrium strategy [177] if

f1(x∗, y) ≤ sup

f1(x, y), ∀x ∈ X1 (14)

sup
y∈BR(x∗)
y∈BR(x)
where BR(x) = {y ∈ Y : f2(x, y) ≤ f2(x, y′), ∀y′ ∈
Y } is the best response of the follower against x.
Along this line, a hierarchical Stackelberg v/s Stackel-
berg game was studied in [178], where the ﬁrst general
existence result for the games’ equilibria is established
without positing single-valuedness assumption on the
equilibrium of the follower-level game. Furthermore, the
connections between the NE and Stackelberg equilib-
rium were addressed in [177], where convergent learn-
ing dynamics are also proposed by using Stackelberg
gradient dynamics that can be regarded as a sequential
variant of the conventional gradient descent algorithm,
and both zero-sum and general-sum games are con-
sidered therein. Additionally, as a special case of the
above game (13), min-max Stackelberg games are paid
attention to as well, where the problem is of the form
minx∈X maxy∈Y f (x, y) with f : Ω → R being the cost
function. This problem has been investigated in the liter-
ature, especially for the case with dependent strategy set
[82], [179], i.e., inequality constraints g(x, y) ≥ 0 are
imposed for the follower for some function g : Ω → R,
for which the prominent minimax theorem [29] does not
hold any more.

4) Incomplete-Information Stackelberg Games. Incomplete
information means that the leader can only access partial
information or cannot access any information of the
followers’ utility functions, moves, or behaviors. This is
in contrast with the traditional Stackelberg games, where
the followers’ information is available to the leader.
This weak scenario has been extensively considered in
recent years motivated by practical applications. For
example, the authors in [180] studied situations in which
only partial information on the attacker behavior can
be observed by the leader. And a single-leader-multiple-
followers SSG was considered in [181] with two types of
misinformed information, i.e., misperception and decep-
tion, for which a stability criterion is provided for both
strategic stability and cognitive stability of equilibria
based on hyper NE. Additionally, one of interesting di-
rections is information deceptions of the follower, that is,
the follower is inclined to deceive the follower by send-
ing misinformation, such as fake payoffs, to the leader
in order to beneﬁt itself as much as possible, while,
at the same time, the leader needs to distinguish the
deception information for minimizing its loss incurred
by the deception. Recently, an interesting result on the
nexus between the follower’s deception and the leader’s

9

maximin utility is obtained for optimally deceiving the
leader in [182], that is, through deception, almost any
(fake) Stackelberg equilibrium can be induced by the
follower if and only if the leader procures at least their
maximin utility at this equilibrium.

C. Zero-Sum Differential Games

According to the existing literature, zero-sum DGs are
categorized by ﬁve main dimensions, which however are not
mutually exclusive, but from different angles of studied prob-
lems, i.e., linear-quadratic DGs, DGs with nonlinear dynamical
systems, Stackelberg DGs, stochastic DGs, and terminal time
and state constraint.

1) Linear-Quadratic DGs. This relatively simple model has
been widely studied for DGs, where dynamical systems
are linear differential equations and cost functions are
quadratic [183], [184]. In general, linear-quadratic DGs
are analytically and numerically solvable, which can
ﬁnd a variety of applications in reality, such as pursuit-
evasion problem [185], [186]. Recently, singular linear-
quadratic DGs were studied in [187], which cannot
be handled either using the Isaacs MinMax principle
or the Bellman-Isaacs equation approach, and to solve
this problem, an interception differential game was in-
troduced with appropriate regularized cost functional
and dual representation. The authors in [188] studied
a linear-quadratic-Gaussian asset defending differential
game where the state information of the attacker and the
defender is not accessible to each other, but the trajec-
tory of a moving asset is known by them. Meanwhile,
a two-player linear-quadratic-Gaussian pursuit-evasion
DG was investigated in [189] with partial information
and selected observations, where the state of one player
can be observed any time preferred by the other player
and the cost function of each player consists of the direct
cost of observing and the implicit cost of exposing his
state. A linear-quadratic DG with two defenders and two
attackers against a stationary target was considered in
[190]. Two-player mean-ﬁeld linear-quadratic stochastic
DGs in an inﬁnite horizon was investigated in [191],
where the existence of both open-loop and closed-
loop saddle points is studied by resorting to coupled
generalized algebraic Riccati equations.

2) Nonlinear DGs. The DGs with nonlinear state dynamics
have also been taken into account in the literature, given
that many practical applications cannot be dealt with by
linear-quadratic DGs. For example, the authors in [192]
considered a class of nonlinear TP-ZS-DGs by appealing
to an adaptive dynamic programming. TP-ZS-DGs were
addressed in [193] by proposing an approximate opti-
mal critic learning algorithm based on policy iteration
of a single neural network. Nonlinear DGs were also
considered with time delays [194]–[196] and fractional-
order systems [197], and then were studied in [198]
with the dynamical system depending on the system’s
distribution and the random initial condition. Besides
two players, multi-player zero-sum DGs with uncertain

nonlinear dynamics were considered and tackled using a
new iterative adaptive dynamic programming algorithm
in [199].

3) Stackelberg DGs. Motivated by the fact of sequential
actions in some practical applications, like Stackelberg
games, DGs with sequential actions, called Stackelberg
DGs, have been broadly addressed in the literature. For
instance, a linear-quadratic Stackelberg DG was consid-
ered in [200] with mixed deterministic and stochastic
controls, where the follower can select adapted random
processes as its controller. The Stackelberg DG was
employed to ﬁght terrorism in [201]. Then, the authors
in [202] investigated two classes of state-constrained
Stackelberg DGs with a nonzero running cost and state
constraint, for which Hamilton-Jacobi equations are es-
tablished.

4) Stochastic DGs. In many realistic problems, the dynam-
ics of a concerned system may not be completely mod-
elled, but undergoing some uncertainties and/or noises,
and thereby, stochastic differential equations have been
leveraged to model the system dynamics in stochas-
tic DGs [203], [204]. In this respect, the authors in
[205] considered two-person zero-sum stochastic linear-
quadratic DGs, along with the investigation of the open-
loop saddle point and the open-loop lower and upper
values. A class of stochastic DGs with ergodic payoff
were studied in [206], where it is not necessary for
the diffusion system to be non-degenerate. In addition,
linear-quadratic stochastic Stackelberg DGs were taken
into consideration in [207] with asymmetric roles for
players, [208] for jump-diffusion systems, [209] without
the solvability assumption on the associated Riccati
equations, and [210] with model uncertainty. And a
Stackelberg stochastic DG with nonlinear dynamics and
asymmetric noisy observation was addressed in [211].
5) Terminal Time and State Constraint. A basic classiﬁca-
tion of zero-sum DGs can be made based on terminal
time and state constraint, that is, whether the terminal
time is ﬁnite (including two cases, i.e., a ﬁxed constant
or a variable to be speciﬁed) or inﬁnite, and whether the
system state is unconstrained or constrained. Along this
line, the case with ﬁxed terminal time and unconstrained
state was ﬁrst addressed [212], and the state-constrained
case with ﬁxed terminal time was also studied [213].
Meanwhile, the case with the terminal time being a
variable was investigated in the literature, such as [214]
without state constraints and [215], [216] in presence of
state constraints but with zero running-cost. Recently,
the case with nonzero state constraint and underdeter-
mined terminal time was investigated in [202]. Besides
the above ﬁnite horizon cases, the inﬁnite horizon case
has also been considered in the literature, e.g., [191],
[217].

Last, it is worth pointing out that other possible forms of
zero-sum DGs exist in the literature, such as the case with
continuous and/or impulse controls [217], mean-ﬁeld DGs
[218], [219], risk-sensitive zero-sum DGs [204] and so forth.

10

IV. PREVAILING ALGORITHMS AND APPROACHES

This section aims at encapsulating some main efﬁcient al-
gorithms and approaches for handling the reviewed adversary
games as discussed in Section II.

A. Zero-Sum Normal- and Extensive-Form Games

The bundle of algorithms can be roughly divided into two
parts according to their applicabilities to normal-form games
or imperfect-information extensive-form games.

For normal-form games, a large number of algorithms have
so far been proposed, e.g., regret matching (RM for short, ﬁrst
proposed by Hart and Mas-Colell in 2000 [220]), RM+ [221],
ﬁctitious play [222], [223], double oracle [224], online double
oracle [49], and among others. Wherein, the most prevalent
algorithms are based on regret learning, usually called no-
regret (or sublinear) learning algorithms, depending external
and internal regrets in general, as deﬁned below.

The external regret and internal regret [225] for each player

i ∈ [n] are, respectively, deﬁned as

(15)

(16)

RE
i

:= max
ai∈Ai

T

[ui(ai, πt

−i) − ui(πt)],

t=1
X
T

RI

i := max
a′
i,ai∈Ai

t=1
X

1

i=aA
at
i

[ui(a′

i, at

−i) − ui(at)],

where the superscript t stands for the iteration number, T is
the time horizon, and 1E is the indicator function with an
event E. Generally speaking, the external regret measures the
greatest regret for not playing actions ai’s, and the internal
regret indicates the greatest regret for not swapping to action a′
i
when each time actually playing action aA
i . Note that weighted
external and internal regrets are also deﬁned by adding a
weight at each time t [226], and other regrets are considered
as well in the literature, including swap regret [91] and several
dynamic/static NE-based regrets [17], [227]–[230].

With regrets at hand, it is now ready to present two of most
widely employed algorithms, i.e., optimistic (or predictive)
follow the regularized leader (Optimistic FTRL for brevity)
and optimistic mirror descent (OMD for short) [85], which
are, respectively, given as

xt+1 = arg max
x∈X

α

x, mt +

gt

− R(x)

,

(17)

n

D

τ =1
X

E

o

t

and

xt+1 = arg max
x∈X
ˆxt+1 = arg max
ˆx∈X

{αhx, mti − DR(x, ˆxt)},

{αhˆx, gti − DR(ˆx, ˆxt)},

(18)

where X is a generic closed convex constraint set, α > 0 is
the stepsize, gt is a subgradient of a function f t returned by
the environment after the player commits an action at time t,
mt is a subgradient prediction, often assuming mt = gt in
the literature, and R(x) is a strongly convex function, serving
as the base function for deﬁning the Bregman divergence
DR(x, y) := R(x)−R(y)−h∇R(y), x−yi for any x, y ∈ Rd.
Note that many widely employed algorithms, such as op-
timistic gradient descent ascent (OGDA) [76] and optimistic

multiplicative weights update (OMWU, or optimistic hedge)
[231], are special cases or variants of optimistic FTRL and
OMD, and other different efﬁcient algorithms also exist such
as optimistic dual averaging (OptDA) [232], greedy weights
[226], and so forth.

For imperfect-information games, the most popular algo-
rithms are counterfactual regret minimization (CFR) [233],
whose details are introduced as follows, with the same no-
tations as in extensive-form games in Section II-A.

Recalling that pπ(h) denotes the reach probability of history
h with strategy proﬁle π. For an infoset J ∈ I, let pπ(J)
denote the probability of reaching the infoset J via all possible
h∈J pπ(h). And denote by
histories in J, i.e., pπ(J) =
pπ
i (J) the reach probability of infoset J for player i according
P
to the strategy π, i.e., pπ
i (J) = ΠJ ′·a′⊑J,P (J)=ip(J ′, a′), and
pπ
−i(J) the counterfactual reach probability of infoset J, i.e.,
the probability of reaching J with strategy proﬁle π except
that the probability of reaching J is treated as 1 by the current
actions of player i, i.e., without the contribution of player i to
reach J. Meanwhile, pπ(h, z) denotes the probability of going
from history h to a nonterminal node z ∈ Z. Then, for player
i ∈ [n], the counterfactual value at a nonterminal history h is
deﬁned as

νπ
i (h) :=

−i(h)pπ(h, z)ui(z),
pπ

(19)

the counterfactual value of an infoset J is deﬁned as

z∈Z,h⊏z
X

νπ
i (J) :=

νπ
i (h),

and the counterfactual value of an action a is deﬁned as

h∈J
X

νπ
i (J, a) :=

pπ
−i(h)

pπ(h · a, z)ui(z)
i

.

h∈J h
X

z∈Z
X
The instantaneous regret at
iteration t and counterfactual
regret at iteration T for action a in infoset J are, respectively,
deﬁned as

i (J, a) := νπt
rt

i (J, a) − νπt

i (J),

T

RT

i (J, a) :=

rt
i (J, a),

(22)

(23)

t=1
X
where πt is the joint strategy proﬁle leveraged at iteration t.
(J, a) := max{RT
i (J, a), 0}, applying
regret matching by Hart and Mas-Colell [220] can generate
the strategy update as

By deﬁning RT,+

i

(20)

(21)

πT +1
i

(J, a) =

(J,a)

RT ,+
i (J,a) ,
i
ζT
1
|A(J)| ,
a∈A(J) RT,+

(

i

i (J, a) > 0

if ζT
otherwise

(24)

i (J, a) :=

with ζT
(J, a), and (24) is the essen-
tial CFR method for player i’s strategy selection. Moreover, it
is known that the CFR method can guarantee the convergence
to NEs for the average strategy of players, i.e.,
t=1 pπt

i (J, a)

P

T

,

∀i ∈ [n].

(25)

i (J)
Hitherto, various famous variants of CFR have been devel-
oped with superior performance, including CFR+ [221], [234],

P

P

i (J)πt
T
t=1 pπt

¯πT
i (J, a) :=

11

discounted CFR (DCFR) [235], linear CFR (LCFR) [236],
exponential CFR (ECFR) [237], AutoCFR [238], etc. More
details can be found in [14], [112], [239].

Meanwhile, lots of AI methods have been brought forward
in the literature [93], such as policy space response oracles
(PSRO) [21], [240], neural ﬁctitious self-play [127], deep
CFR [236], single deep CFR [241], uniﬁed deep equilibrium
ﬁnding (UDEF) [136], player of games (PoG) [133], neural
auto-curricula (NAC) [137], and so forth. Among these meth-
ods, PSRO has been an effective approach in recent years,
which uniﬁes ﬁctitious play and double oracle algorithms.
Nonetheless, UDEF provides a uniﬁed framework of PSRO
and CFR, which are generally considered independently with
their own advantages, and thus UDEF are superior to both
PSRO and CFR as demonstrated by experiments on Leduc
poker [136]. The recently-developed PoG algorithm has uni-
ﬁed several previous approaches by integrating guided search,
self-play learning, and game-theoretic reasoning, and demon-
strated theoretically and experimentally the achievement of
strong empirical performance in large perfect and imperfect
information games, which defeats state-of-the-art in heads-
up no-limit Texas Hold’em poker (Slumbot) [133]. Moreover,
NAC, as a meta-learning algorithm proposed recently in [137],
provides a potential future direction to develop general multi-
agent reinforcement learning (MARL) algorithms solely from
data, since it can learn its own objective solely from the
interactions with environment, without the need of human-
designed knowledge about game theoretic principles, and it
can decide by itself what
i.e., who to
compete with, should be during training. Furthermore, it is
shown that NAC is comparable or even superior to the state-
of-the-art population-based game solvers, such as PSRO, on
a series of games, like Games of Skill, differentiable Lotto,
non-transitive Mixture Games, Iterated Matching Pennies, and
Kuhn poker [137].

the meta-solution,

Finally, it is worth pointing out that by CFR methods, it
can guarantee the convergence to NEs in the sense of the
empirical distribution (i.e., time-average) of play, but generally
failing to converge for the day-to-day play (i.e., the last-iterate
convergence) [242], [243], although it does converge in the
sense of last-iterate in two-player zero-sum games [85]. In
this respect, the last-iterate convergence is of also importance
to be explored as demonstrated in economics, and so on [76],
[85], [244]–[246].

B. Stackelberg Games

GSGs and SSGs can be expressed as bilevel linear program-
ming (BLP) or mixed integer linear programming (MILP),
which can be further transformed or relaxed as linear program-
ming (LP) [146]. As mentioned in Section III-B, solving GSGs
and SSGs is generally NP-hard, and most existing solution
methods are variants of solution approaches for MILP and
LP, including cutting plane methods, enumerative methods,
hybrid methods, and so on [147]. Some of most widely used
approaches in the literature are introduced in the sequel.

1) Multiple LP Approach. This approach is proposed in
[38], most widely employed for those easy problems

that can be solved in polynomial time, including the
case with a single follower type for GSGs [38], further
improved upon in [247] by merging LPs into a single
MILP. And this approach has also been improved to deal
with SSGs in [164], generally pretty efﬁcient in the case
with size 1 of the schedule and the case with size 2 of
the schedule but for homogeneous resources, as shown
in Table II.

2) Benders Decomposition. Benders decomposition method
is developed in [248], which is effective to handle
general MILP problems. The crux of this method is
to divide the original problem into two other problems,
that is, one is called master problem by relaxing some
constraints and the other is called subproblem, along
with a separation problem that is the dual of the sub-
problem. Then, the solution seeking procedure involves
the solving of the master problem ﬁrstly, followed by
solving the separation problem, and ﬁnally checking the
feasibility and optimality conditions for the subproblem
with different contingent operations. Moreover, this ap-
proach can be improved upon by combining with other
techniques, such as Farkas’ lemma [249] and normalized
cut [250], leading to a recent efﬁcient algorithm, called
normalized Benders decomposition [147], etc.

3) Branch and Cut. Branch & cut method, as a hybrid
methods, combines the cutting plane method [251] with
the branch and bound method [252]. This approach is
pretty effective for resolving various (mixed) integer pro-
gramming problems while still ensuring the optimality.
In general, branch and cut algorithm is in the same spirit
of the branch and bound scheme, but appending new
constraints when necessary in each node by resorting to
cutting plane approaches [147].

4) Cut and Branch. This method is similar to the branch
and cut approach, and the difference lies in that the extra
cuts are only added in the root node. Meanwhile, only
the branching constraints are added to the other nodes.
It is found in [147] that with variables in R in master
problem and stabilization, cut and branch is superior to
other methods in some sense.

5) Gradient Descent Ascent. Gradient descent ascent, i.e.,
the classical gradient descent and ascent algorithm [253],
is the most noticeable algorithm for solving continuous
Stackelberg games, where descent and ascent opera-
tions are, respectively, performed for the leader and the
follower, but in a sequential order, and other methods
mostly rest on this algorithm [82], [177]. For example,
the max-oracle gradient-descent algorithm [82] is a
variant of gradient descent ascent, where the ascent
operation in the follower is directly replaced with an
approximate best response provided by a max-oracle.
Finally, it is worth pointing out that AI methods have also
been leveraged to cope with Stackelberg games, e.g., [254]
and a survey [255] for reference.

C. Zero-Sum Differential Games

Among the methods for solving zero-sum DGs, the viscosity
solution approach is the most widely exploited one, for which

12

it
is known that a value function is the solution of the
Hamilton-Jacobi-Isaacs (HJI) equations. In the sequel, this
approach is introduced for DGs (10) and (11), and other
detailed cases can be found in [45], [256].

For DGs (10) and (11), the Hamiltonian is deﬁned as

H(t, x, ω) = min
u∈U

max
v∈V

{hf (t, x, u, v), ωi + f0(t, x, u, v)},

t ∈ [t0, T ], x, ω ∈ Rd

(26)

and the HJI equation is given as

∂tψ(t, z) + H(t, z, ∂zψ(t, z)) = 0,
ψ(T, z) = φ(z),

t ∈ [t0, T ), z ∈ Rd

(27)

where the second condition is called the terminal condition,
ψ : [t0, T ] × Rd → R is a function, and ∂t, ∂z represent the
subgradients with respect to t, z, respectively.

national 
national 
defense
defense

poker
poker

applications 
applications 
of 
of 
adversarial 
adversarial 
games
games

StarCraft
StarCraft

security
security

politics
politics

(cid:266)(cid:266)
(cid:266)(cid:266)

Fig. 5. A schematic illustration of applications of adversarial games.

Let Ψ denote the set of functions ψ : [t0, T ] × Rd → R
satisfying the continuity condition in t and the Lipschitz
condition on every bounded subset of Rd in x. From [195], it is
known that if a function ψ ∈ Ψ is coinvariantly differentiable
at each point (t, z) ∈ [t0, T ] × Rd, satisﬁes HJI equation (27),
and ∂tψ, ∂zψ ∈ Ψ, then ψ is the value function of differential
game (10) and (11), and the optimal control strategies for two
players are given as

u∗(t, z) ∈ arg min
u∈U
v∗(t, z) ∈ arg max
v∈V

max
v∈V
min
u∈U

χ(t, z, u, v),

χ(t, z, u, v),

(28)

where

χ(t, z, u, v) := hf (t, z, u, v), ∂zψ(t, z)i + f0(t, z, u, v). (29)

Moreover, it should be noted that AI methods have also been
applied to solve differential games, for example, reinforcement
learning was employed to deal with multi-player nonlinear dif-
ferential games [257], where a novel two-level value iteration-
based integral reinforcement learning algorithm was proposed
only depending upon partial information of system dynamics.

V. APPLICATIONS

This section provides some practical applications for ad-
versarial games. As a matter of fact, adversarial games have
been leveraged to solve a large volume of realistic problems
including poker
in the literature, as illustrated in Fig. 5,

202

V. Bucarey et al.

We are concerned with the speciﬁc actions that Carabineros can take during

night shift patrols. The region is divided into several police precincts. Due to

the vast expanses and harsh landscape at the border to patrol and the lack of

manpower, for the purpose of the defender actions under consideration, a num-

ber of these precincts are paired up when planning the patrol. Furthermore,

Carabineros have identiﬁed a set of locations along the border of the region

that can serve as vantage points from where to conduct surveillance with tech-

nical equipment such as night goggles and heat sensors (Figs.

and ). A night

shift action consists in deploying a joint detail with personnel from two paired

precincts to conduct vigilance from 22h00 to 04h00 at the vantage point located

within the territory of the paired precincts. Due to logistical constraints, for a

given precinct pair, Carabineros deploys a joint detail from every precinct pair

to a surveillance location once a week.

Fig. 3. A Carabinero conducts
surveillance

Fig. 4. Harsh border landscape

13

[133], StarCraft [258], politics [259], infrastructure security
[13], pursuit-evasion problems [186], border defense [19],
[170], [260], national defense [18], communication scheduling
[261], autonomous driving [262], homeland security [263], etc.
In what follows, we provide three well known examples to
illustrate applications.

realm of the paired precincts. Meanwhile, in consideration
of logistical constraints, a joint detail is deployed for every
Carabineros requires a schedule indicating the optimal deployment of details
precinct pair to a surveillance location once a week. Fig.
to vantage points for a given week. Figure depicts a defender strategy in a game
7 illustrates the case with 3 pairings, 7 precincts and 10
with = 3 pairings,
shows a
locations.
tabular representation of the implemented strategy for that week.

= 10 locations. Table

= 7 precincts and

Example 1 (Radar Jamming). Radar jamming is one of
widely studied applications of zero-sum games in modern
electronic warfare [264], [265]. In radar jamming, there exist
two players, one radar who aims to detect a target in a
probability as high as possible, and one jammer who aims
at minimizing the radar’s detection by jamming it. Therefore,
the two players are diametrically opposed, and the scenario
forms a two-player zero-sum game (cf. Fig. 6 for a schematic
the target,
illustration). Usually, according to the type of
some kinds of utility functions can be constructed in distinct
scenarios of jamming, and some constraints can be described
mathematically relying on physical limitations, such as jammer
power, spatial extent of jamming, and threshold parameter and
reference window size for the radar. For example, a Swerling
Type II target is assumed in [266] in presence of Rayleigh
distributed clutter, for which certain utility functions are built
for cell averaging and order-statistic constant false alarm rate
(CFAR) processors in three scenarios of jamming, i.e., ungated
range noise, range-gated noise, and false-target jamming.

Radar range 

Jammer 

Radar 

Target 

Fig. 6. A schematic illustration of radar jamming.

Example 2 (Border Patrols). It is an important task for a
country to secure national borders to avoid illicit behaviors
of drugs, contraband, and stowaway, etc. In this spirit, border
patrols are introduced here as one application of SSGs, which
is proposed by Carabineros de Chile [170], [171], to thwart
drug trafﬁcking, contraband and illegal entry. To this end, both
day and night shift patrols along the border are arranged by
Carabineros according to distinct requirements.

The night shift patrols are specially focused on. To make it
practically implementable, the region is partitioned into some
police precincts, some of which are paired up when scheduling
the patrol, because of the vast expanses and harsh landscape
at the border and the manpower limitation. In addition, a
set of vantage locations have been identiﬁed by Carabineros
along the border of the region, which are suited for conducting
surveillance with high-tech equipments, like heat sensors and
night goggles. A night shift action means the deployment of a
joint detail with personnel from two paired precincts to carry
out vigilance overnight at the vantage locations within the

Fig. 7. Feasible schedule for a week, where stars and squares mean precinct
headquarters and border outposts, respectively, cited from [170].

Fig. 5. Feasible schedule for a week

Example 3 (Pursuit-Evasion Problems). Pursuit-evasion prob-
lems are one of prevalent applications of zero-sum DGs, which
have been widely applied to many practical problems, such
as surveillance and navigation, in robotics and aerospace
and so forth. In pursuit-evasion problems, there usually exist
a collection of pursuers and evaders (one pursuer and one
evader in the simplest case) possibly with a moving target or
stationary target set/area, and the pursuers aim to capture or
intercept the evaders who have opposed objectives [186]. As
a concrete example, consider a case where there exists one
pursuer (or defender) which protects a maritime coastline or
border from the attacking by two slower aircraft (or evaders).
The pursuer needs to sequentially pursue the evaders and
strives to intercept them as far as possible from the coastline.
Meanwhile, the two evaders can collaborate and strive to
minimize their combined distance to the coastline before they
are intercepted. For this problem, a regular solution was
provided for the differential game in [267].

VI. POSSIBLE FUTURE DIRECTIONS

In view of some challenges in adversary games, this section
attempts to present potential research directions in future, as
discussed in the sequel.

• Efﬁcient Algorithms Design. Even though a wide range
of algorithms have been proposed in the literature, as
introduced above, efﬁcient, fast and optimal algorithms
with limited computing, storage, and memory capabilities
are still the overarching research directions in (adversar-
ial) games and artiﬁcial intelligence, which are far from
fully explored, including a plethora of scenarios, e.g.,
equilibrium computation [226], real-time strategy (RTS)
making [268], exploiting suboptimal opponents [269],
attack resiliency [270], and so forth.

• Last-Iterate Convergence. In general, no-regret learning
can guarantee the convergence of the empirical distri-

 
 
 
 
 
 
bution of play (i.e., time-average convergence) for each
player to the set of NEs. However, the last-iterate con-
vergence fails in general [242], [243], although restricted
classes of games indeed have the last-iterate convergence
by no-regret learning algorithms, such as two-player zero-
sum games [85]. Note that the last-iterate convergence
is important in many practical applications, for exam-
ple, generative adversarial networks (GANs) [271] and
economics [231], which have been receiving a growing
interest in recent years [272].

• Imperfect Information. Imperfect information, as a pos-
sible main feature of many practical adversarial games,
inﬂicts a major challenge in adversarial games, which is
still being actively under explored, although an array of
works have focused on it, e.g., [115], [273].

• Large Games. For adversarial games with large action
spaces and/or infosets, practical limitations, such as lim-
ited computing resources, impose the need of efﬁcient
algorithms design amenable to implementation with lim-
ited computation, storage and even communication [274].
• Incomplete Information. Incomplete information is an-
other main hallmark of many adversarial games, which
is one of challenge sources. Generally speaking, game
uncertainties, such as parameter uncertainties, action out-
come uncertainty, underlying world state uncertainty, can
be subsumed in the category of incomplete information,
and the main studied models are Bayesian and interval
models [145], [275], [276].

• Bounded Rationality. Completely rational players are of-
ten assumed in the study of games. Nonetheless, irrational
players naturally appear in practice, which has triggered
an increasing interest in games with bounded rationality,
e.g., behavior models such as lens-QR models, prospect
theory inspired models and quantal response models
[277]–[279].

• Dynamic Environments. Most of games have been in-
vestigated as static ones, i.e., with time-invariant game
rules. However, due to possible dynamic characteristics
of the environment within which players compete, online
game (or time-varying game) is imperative for further
attention in future, where each player’s utility function is
time-varying or even adversarial without any distribution
assumptions [17], [227]–[230].

• Hybrid Games. It is known that many realistic adversarial
games involve both continuous and discrete physical
dynamics that govern players’ motion or changing rules,
which can be framed in the framework of hybrid games
[280], [281]. In this respect, how to combine the game
theory with control dynamics is an important yet chal-
lenging research area.

• AI in Games. Recent years have witnessed great progress
in the success of AI methods applied in games, which
can integrate some advanced approaches of reinforcement
learning, neural networks, meta-learning, and so on [135],
[282]–[284]. With the advent of modern high-tech and
big-data complex missions, AI methods provide an ef-
fective manner to commit real-time strategies by solely
exploiting ofﬂine or real-time streaming data [139].

14

VII. CONCLUSION

Adversarial games play a signiﬁcant role in practical appli-
cations, for which this survey provided a systematic overview
on it from three main categories, i.e., zero-sum normal- and
extensive-form games, Stackelberg (security) games, and zero-
sum differential games. To this end, several distinct angles
have been employed to anatomize adversarial games, ranging
from game models, solution concepts, problem classiﬁcation,
research frontiers, prevailing algorithms and real-world appli-
cations to potential future directions. In general, this survey
has attempted to summarize the past research in an intact
manner, although the existing references are too vast to cover
in its entirety. To our best knowledge, this survey is the ﬁrst to
present a systematic overview on adversarial games. Finally,
future possible directions have been also discussed.

REFERENCES

[1] J. von Neumann and O. Morgenstern, Theory of Games and Economic

Behavior, 2nd ed. Princeton University Press, 1947.

[2] J. F. Nash, “Equilibrium points in n-person games,” Proceedings of the
National Academy of Sciences, vol. 36, no. 1, pp. 48–49, 1950.
[3] J. Nash, “Non-cooperative games,” Annals of Mathematics, vol. 54,

no. 2, pp. 286–295, 1951.

[4] D. Fudenberg and J. Tirole, Game Theory. MIT Press, 1991.
[5] M. J. Osborne and A. Rubinstein, A Course in Game Theory. MIT

Press, 1994.

[6] T. Bas¸ar and G. Zaccour, Handbook of Dynamic Game Theory.

Springer International Publishing, 2018.

[7] R. J. Aumann, M. Maschler, and R. E. Stearns, Repeated Games with

Incomplete Information. MIT Press, 1995.

[8] N. Bard, J. Hawkin, J. Rubin, and M. Zinkevich, “The annual computer
poker competition,” AI Magazine, vol. 34, no. 2, pp. 112–112, 2013.
[9] T. H. Nguyen, D. Kar, M. Brown, A. Sinha, A. X. Jiang, and M. Tambe,
“Towards a science of security games,” in Mathematical Sciences with
Multidisciplinary Applications, 2016, pp. 347–381.

[10] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot et al., “Mastering the game of Go with deep neural
networks and tree search,” Nature, vol. 529, no. 7587, pp. 484–489,
2016.

[11] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering the
game of Go without human knowledge,” Nature, vol. 550, no. 7676,
pp. 354–359, 2017.

[12] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,
M. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al., “A general
reinforcement learning algorithm that masters chess, shogi, and Go
through self-play,” Science, vol. 362, no. 6419, pp. 1140–1144, 2018.
[13] A. Sinha, F. Fang, B. An, C. Kiekintveld, and M. Tambe, “Stackelberg
security games: Looking beyond a decade of success,” in International
Joint Conference on Artiﬁcial Intelligence (IJCAI), Stockholm, Swe-
den, 2018, pp. 5494–5501.

[14] H. Li, X. Wang, F. Jia, Y. Li, and Q. Chen, “A survey of Nash
equilibrium strategy solving based on CFR,” Archives of Computational
Methods in Engineering, vol. 28, no. 4, pp. 2749–2760, 2021.
[15] M. K. Sohrabi and H. Azgomi, “A survey on the combined use of
optimization methods and game theory,” Archives of Computational
Methods in Engineering, vol. 27, no. 1, pp. 59–80, 2020.

[16] J. Wang, Y. Hong, J. Wang, J. Xu, Y. Tang, Q.-L. Han, and J. Kurths,
“Cooperative and competitive multi-agent systems: From optimization
to games,” IEEE/CAA Journal of Automatica Sinica, vol. 9, no. 5, pp.
763–783, 2022.

[17] X. Li, L. Xie, and N. Li, “A survey of decentralized online learning,”

arXiv preprint arXiv:2205.00473, 2022.

[18] E. Ho, A. Rajagopalan, A. Skvortsov, S. Arulampalam, and M. Pi-
raveenan, “Game theory in defence applications: A review,” Sensors,
vol. 22, no. 3, p. 1032, 2022.

[19] D. Shishika and V. Kumar, “A review of multi-agent perimeter defense
games,” in International Conference on Decision and Game Theory for
Security, College Park, USA, 2020, pp. 472–485.

[20] M. Zhu, A. H. Anwar, Z. Wan, J.-H. Cho, C. A. Kamhoua, and
M. P. Singh, “A survey of defensive deception: Approaches using
game theory and machine learning,” IEEE Communications Surveys
& Tutorials, vol. 23, no. 4, pp. 2460–2493, 2021.

[21] M. Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. P´erolat,
D. Silver, and T. Graepel, “A uniﬁed game-theoretic approach to
multiagent reinforcement learning,” in Advances in Neural Information
Processing Systems, vol. 30, Long Beach, CA, USA, 2017.

[22] M. L. Littman, “Markov games as a framework for multi-agent
reinforcement learning,” in Machine Learning Proceedings, 1994, pp.
157–163.

[23] S. Zamir et al., “Bayesian games: Games with incomplete information,”

Tech. Rep., 2008.

[24] X. Chen, X. Deng, and S.-H. Teng, “Settling the complexity of
computing two-player Nash equilibria,” Journal of the ACM (JACM),
vol. 56, no. 3, pp. 1–57, 2009.

[25] C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou, “The complex-
ity of computing a Nash equilibrium,” SIAM Journal on Computing,
vol. 39, no. 1, pp. 195–259, 2009.

[26] A. Rubinstein, Hardness of Approximation Between P and NP. Morgan

& Claypool, 2019.

[27] R. J. Aumann, “Subjectivity and correlation in randomized strategies,”

Journal of Mathematical Economics, vol. 1, no. 1, pp. 67–96, 1974.

[28] J. Hannan, “Approximation to Bayes risk in repeated play,” Contribu-
tions to the Theory of Games, vol. 3, no. 2, pp. 97–139, 1957.
[29] J. V. Neumann, “Zur theorie der gesellschaftsspiele,” Mathematische

Annalen, vol. 100, no. 1, pp. 295–320, 1928.

[30] G. Farina, T. Bianchi, and T. Sandholm, “Coarse correlation in
extensive-form games,” in AAAI Conference on Artiﬁcial Intelligence,
vol. 34, no. 2, 2020, pp. 1934–1941.

[31] A. Celli, S. Coniglio, and N. Gatti, “Computing optimal coarse corre-
lated equilibria in sequential games,” arXiv preprint arXiv:1901.06221,
2019.

[32] A. Celli and N. Gatti, “Computational

results for extensive-form
adversarial team games,” in AAAI Conference on Artiﬁcial Intelligence,
vol. 32, no. 1, 2018.

[33] B. von Stengel and D. Koller, “Team-maxmin equilibria,” Games and

Economic Behavior, vol. 21, no. 1-2, pp. 309–321, 1997.

[34] S. Omidshaﬁei, C. Papadimitriou, G. Piliouras, K. Tuyls, M. Row-
land, J.-B. Lespiau, W. M. Czarnecki, M. Lanctot, J. Perolat, and
R. Munos, “α-rank: Multi-agent evaluation by evolution,” Scientiﬁc
Reports, vol. 9, no. 1, pp. 1–29, 2019.

[35] H. Von Stackelberg, Marktform und gleichgewicht.

Springer-Verlag,

Berlin, 1934.

[36] B. An, F. Ord´o˜nez, M. Tambe, E. Shieh, R. Yang, C. Baldwin,
J. DiRenzo III, K. Moretti, B. Maule, and G. Meyer, “A deployed
quantal response-based patrol planning system for the U.S. coast
guard,” Interfaces, vol. 43, no. 5, pp. 400–420, 2013.

[37] C. Casorr´an, B. Fortz, M. Labb´e, and F. Ord´o˜nez, “A study of general
and security Stackelberg game formulations,” European Journal of
Operational Research, vol. 278, no. 3, pp. 855–868, 2019.

[38] V. Conitzer and T. Sandholm, “Computing the optimal strategy to
commit to,” in Proceedings of the 7th ACM conference on Electronic
Commerce, Michigan, USA, 2006, pp. 82–90.

[39] G. Leitmann, “On generalized Stackelberg strategies,” Journal of
Optimization Theory and Applications, vol. 26, no. 4, pp. 637–643,
1978.

[40] H. von Stackelberg, Market Structure and Equilibrium.

Springer

Science & Business Media, 2011.

[41] T. Bas¸ar and G. J. Olsder, Dynamic Noncooperative Game Theory.

SIAM, 1998.

[42] R. Isaacs, Differential Games. Wiley, New York, 1965.
[43] F. L. Lewis, D. Vrabie, and V. L. Syrmos, Optimal Control.

John

Wiley & Sons, 2012.

[44] R. Buckdahn, P. Cardaliaguet, and M. Quincampoix, “Some recent
aspects of differential game theory,” Dynamic Games and Applications,
vol. 1, no. 1, pp. 74–114, 2011.

[45] A. Friedman, Differential Games. Courier Corporation, 2013.
[46] J. Garg, A. X. Jiang, and R. Mehta, “Bilinear games: Polynomial time
algorithms for rank based subclasses,” in International Workshop on
Internet and Network Economics, Singapore, 2011, pp. 399–407.
[47] C. E. Lemke and J. T. Howson, Jr, “Equilibrium points of bimatrix
games,” Journal of the Society for Industrial and Applied Mathematics,
vol. 12, no. 2, pp. 413–423, 1964.

[48] I. Anagnostides and P. Penna, “Solving zero-sum games through
alternating projections,” arXiv preprint arXiv:2010.00109, 2021.

15

[49] L. C. Dinh, Y. Yang, Z. Tian, N. P. Nieves, O. Slumbers, D. H. Mguni,
H. B. Ammar, and J. Wang, “Online double oracle,” arXiv preprint
arXiv:2103.07780, 2021.

[50] A. Murhekar, “Approximate Nash equilibria of imitation games: Algo-
rithms and complexity,” in International Conference on Autonomous
Agents and Multiagent Systems, 2020, pp. 887–894.

[51] E. Borel, “La th´eorie du jeu et

les ´equations int´egralesa noyau
sym´etrique,” Comptes rendus de l’Acad´emie des Sciences, vol. 173,
no. 1304-1308, p. 58, 1921.

[52] J. T. Howson Jr, “Equilibria of polymatrix games,” Management

Science, vol. 18, no. 5-part-1, pp. 312–318, 1972.

[53] G. Sengodan and C. Arumugasamy, “Linear complementarity problems
and bilinear games,” Applications of Mathematics, vol. 65, no. 5, pp.
665–675, 2020.

[54] A. Deligkas, M. Fasoulakis, and E. Markakis, “A polynomial-time
algorithm for 1/3-approximate Nash equilibria in bimatrix games,”
arXiv preprint arXiv:2204.11525, 2022.

[55] A. Deligkas, J. Fearnley, and R. Savani, “Tree polymatrix games are

PPAD-hard,” arXiv preprint arXiv:2002.12119, 2020.

[56] S. Seddighin, “Campaigning via LPs: Solving Blotto and Beyond,”
Ph.D. dissertation, University of Maryland, College Park, 2019.
[57] R. Mehta, “Constant rank two-player games are PPAD-hard,” SIAM

Journal on Computing, vol. 47, no. 5, pp. 1858–1887, 2018.

[58] S. Boodaghians, J. Brakensiek, S. B. Hopkins, and A. Rubinstein,
“Smoothed complexity of 2-player Nash equilibria,” in Annual Sympo-
sium on Foundations of Computer Science, 2020, pp. 271–282.
[59] S. Behnezhad, A. Blum, M. Derakhshan, M. Hajiaghayi, C. H. Pa-
padimitriou, and S. Seddighin, “Optimal strategies of Blotto games:
Beyond convexity,” in Proceedings of ACM Conference on Economics
and Computation, Phoenix, AZ, USA, 2019, pp. 597–616.

[60] S. Behnezhad, S. Dehghani, M. Derakhshan, M. Hajiaghayi, and
S. Seddighin, “Fast and simple solutions of Blotto games,” Operations
Research, DOI: 10.1287/opre.2022.2261, 2022.

[61] D. Beaglehole, “An efﬁcient approximation algorithm for the Colonel

Blotto game,” arXiv preprint arXiv:2201.10758, 2022.

[62] V. Leon and S. R. Etesami,

learning for dynamic
Colonel Blotto game with a budget constraint,” arXiv preprint
arXiv:2103.12833, 2021.

“Bandit

[63] D. Q. Vu, P. Loiseau, and A. Silva, “Approximate equilibria in
generalized Colonel Blotto and generalized Lottery Blotto games,”
arXiv preprint arXiv:1910.06559, 2019.

[64] E. Boix-Adser`a, B. L. Edelman, and S. Jayanti, “The multiplayer
Colonel Blotto game,” Games and Economic Behavior, vol. 129, pp.
15–31, 2021.

[65] E.-V. Vlatakis-Gkaragkounis, L. Flokas, and G. Piliouras, “Poincar´e
recurrence, cycles and spurious equilibria in gradient-descent-ascent
for non-convex non-concave zero-sum games,” in Advances in Neural
Information Processing Systems, vol. 32, Vancouver, BC, Canada,
2019, pp. 1–12.

[66] G. Zhang, Y. Wang, L. Lessard, and R. B. Grosse, “Near-optimal
local convergence of alternating gradient descent-ascent for minimax
optimization,” in International Conference on Artiﬁcial Intelligence and
Statistics, 2022, pp. 7659–7679.

[67] E. Y. Hamedani and N. S. Aybat, “A primal-dual algorithm with
line search for general convex-concave saddle point problems,” SIAM
Journal on Optimization, vol. 31, no. 2, pp. 1299–1329, 2021.
[68] V. Tominin, Y. Tominin, E. Borodich, D. Kovalev, A. Gasnikov, and
P. Dvurechensky, “On accelerated methods for saddle-point problems
with composite structure,” arXiv preprint arXiv:2103.09344, 2021.
[69] G. Xie, Y. Han, and Z. Zhang, “DIPPA: An improved method for
bilinear saddle point problems,” arXiv preprint arXiv:2103.08270,
2021.

[70] D. Kovalev, A. Gasnikov, and P. Richt´arik, “Accelerated primal-dual
gradient method for smooth and convex-concave saddle-point problems
with bilinear coupling,” arXiv preprint arXiv:2112.15199, 2021.
[71] K. K. Thekumparampil, N. He, and S. Oh, “Lifted primal-dual method
for bilinearly coupled smooth minimax optimization,” arXiv preprint
arXiv:2201.07427, 2022.

[72] G. Gidel, T. Jebara, and S. Lacoste-Julien, “Frank-Wolfe algorithms
for saddle point problems,” in International Conference on Artiﬁcial
Intelligence and Statistics, Florida, USA, 2017, pp. 362–371.

[73] C. Chen, L. Luo, W. Zhang, and Y. Yu, “Efﬁcient projection-free algo-
rithms for saddle point problems,” in Advances in Neural Information
Processing Systems, vol. 33, 2020, pp. 10 799–10 808.

[74] H. Li, Y. Tian, J. Zhang, and A. Jadbabaie, “Complexity lower bounds
for nonconvex-strongly-concave min-max optimization,” in Advances
in Neural Information Processing Systems, vol. 34, 2021, pp. 1–13.

[75] Y.-P. Hsieh, P. Mertikopoulos, and V. Cevher, “The limits of min-max
optimization algorithms: Convergence to spurious non-critical sets,” in
International Conference on Machine Learning, 2021, pp. 4337–4348.
[76] C.-Y. Wei, C.-W. Lee, M. Zhang, and H. Luo, “Linear last-iterate
convergence in constrained saddle-point optimization,” in International
Conference on Learning Representations, 2021, pp. 1–12.

[77] I. Bistritz, Z. Zhou, X. Chen, N. Bambos, and J. Blanchet, “No
weighted-regret learning in adversarial bandits with delays,” Journal
of Machine Learning Research, vol. 23, pp. 1–43, 2022.

[78] T. Fiez, R. Sim, S. Skoulakis, G. Piliouras, and L. Ratliff, “Online
learning in periodic zero-sum games,” vol. 34, 2021, pp. 1–13.
[79] H. Gao, X. Wang, L. Luo, and X. Shi, “On the convergence of stochas-
tic compositional gradient descent ascent method,” in International
Joint Conference on Artiﬁcial Intelligence, 2021, pp. 1–7.

[80] A. Beznosikov, G. Scutari, A. Rogozin, and A. Gasnikov, “Distributed

saddle-point problems under data similarity,” vol. 34, 2021.

[81] E.-V. Vlatakis-Gkaragkounis, L. Flokas, and G. Piliouras, “Solving
min-max optimization with hidden structure via gradient descent as-
cent,” in Advances in Neural Information Processing Systems, vol. 34,
2021, pp. 1–14.

[82] D. Goktas and A. Greenwald, “Convex-concave min-max Stackel-
berg games,” in Advances in Neural Information Processing Systems,
vol. 34, 2021.

[83] D. Xefteris, “Symmetric zero-sum games with only asymmetric equi-

libria,” Games and Economic Behavior, vol. 89, pp. 122–125, 2015.

[84] Y. Cai and C. Daskalakis, “On minmax theorems for multiplayer
games,” in Proceedings of Annual ACM-SIAM Symposium on Discrete
Algorithms, San Francisco, California, 2011, pp. 217–234.

[85] I. Anagnostides,

I. Panageas, G. Farina, and T. Sandholm, “On
last-iterate convergence beyond zero-sum games,” arXiv preprint
arXiv:2203.12056, 2022.

[86] J. P. Bailey, “o(1/t) time-average convergence in a generalization of
multiagent zero-sum games,” arXiv preprint arXiv:2110.02482, 2021.
[87] T. Fiez, R. Sim, S. Skoulakis, G. Piliouras, and L. Ratliff, “Online
learning in periodic zero-sum games: von Neumann vs Poincar´e.”
[88] S. Skoulakis, T. Fiez, R. Sim, G. Piliouras, and L. Ratliff, “Evolutionary
game theory squared: Evolving agents in endogenously evolving zero-
sum games,” in AAAI Conference on Artiﬁcial Intelligence, 2021, pp.
1–9.

[89] E. Hughes, T. W. Anthony, T. Eccles, J. Z. Leibo, D. Balduzzi, and
Y. Bachrach, “Learning to resolve alliance dilemmas in many-player
zero-sum games,” arXiv preprint arXiv:2003.00799, 2020.

[90] S. Ganzfried, “Fast complete algorithm for multiplayer Nash equilib-

rium,” arXiv preprint arXiv:2002.04734, 2020.

[91] I. Anagnostides, C. Daskalakis, G. Farina, M. Fishelson, N. Golowich,
and T. Sandholm, “Near-optimal no-regret
learning for corre-
lated equilibria in multi-player general-sum games,” arXiv preprint
arXiv:2111.06008, 2021.

[92] I. Anagnostides, G. Farina, C. Kroer, A. Celli, and T. Sandholm, “Faster
no-regret learning dynamics for extensive-form correlated and coarse
correlated equilibria,” arXiv preprint arXiv:2202.05446, 2022.
[93] G. Gidel, “Multi-player games in the era of machine learning,” Ph.D.

dissertation, Universit´e de Montr´eal, 2020.

[94] Y. Zhang and B. An, “Converging to team-maxmin equilibria in zero-
sum multiplayer games,” in International Conference on Machine
Learning, 2020, pp. 11 033–11 043.

[95] F. Kalogiannis, E.-V. Vlatakis-Gkaragkounis, and I. Panageas, “Team-
work makes von Neumann work: Min-max optimization in two-team
zero-sum games,” arXiv preprint arXiv:2111.04178, 2021.

[96] K. A. Hansen, T. D. Hansen, P. B. Miltersen, and T. B. Sørensen,
“Approximability and parameterized complexity of minmax values,” in
International Workshop on Internet and Network Economics, 2008, pp.
684–695.

[97] C. Borgs, J. Chayes, N. Immorlica, A. T. Kalai, V. Mirrokni, and C. Pa-
padimitriou, “The myth of the folk theorem,” Games and Economic
Behavior, vol. 70, no. 1, pp. 34–43, 2010.

[98] B. Gharesifard and J. Cort´es, “Distributed convergence to Nash equi-
libria in two-network zero-sum games,” Automatica, vol. 49, no. 6, pp.
1683–1692, 2013.

[99] Y. Lou, Y. Hong, L. Xie, G. Shi, and K. H. Johansson, “Nash
equilibrium computation in subnetwork zero-sum games with switching
communications,” IEEE Transactions on Automatic Control, vol. 61,
no. 10, pp. 2920–2935, 2015.

16

[101] Y. Zhang and B. An, “Computing team-maxmin equilibria in zero-sum
multiplayer extensive-form games,” in AAAI Conference on Artiﬁcial
Intelligence, vol. 34, no. 02, 2020, pp. 2318–2325.

[102] L. Carminati, F. Cacciamani, M. Ciccone, and N. Gatti, “Public
information representation for adversarial team games,” arXiv preprint
arXiv:2201.10377, 2022.

[103] G. Farina, A. Celli, N. Gatti, and T. Sandholm, “Faster algorithms
for optimal ex-ante coordinated collusive strategies in extensive-form
zero-sum games,” arXiv preprint arXiv:2009.10061, 2020.

[104] B. H. Zhang and T. Sandholm, “Team correlated equilibria in zero-
sum extensive-form games via tree decompositions,” arXiv preprint
arXiv:2109.05284, 2021.

[105] O. Tammelin, N. Burch, M. Johanson, and M. Bowling, “Solving
heads-up limit Texas Hold’em,” in International Joint Conference on
Artiﬁcial Intelligence, 2015, pp. 645–652.

[106] M. Moravˇc´ık, M. Schmid, N. Burch, V. Lis`y, D. Morrill, N. Bard,
T. Davis, K. Waugh, M. Johanson, and M. Bowling, “DeepStack:
Expert-level artiﬁcial intelligence in heads-up no-limit poker,” Science,
vol. 356, no. 6337, pp. 508–513, 2017.

[107] N. Brown and T. Sandholm, “Superhuman AI for heads-up no-limit
poker: Libratus beats top professionals,” Science, vol. 359, no. 6374,
pp. 418–424, 2018.

[108] R. Munos, J. Perolat, J.-B. Lespiau, M. Rowland, B. De Vylder,
M. Lanctot, F. Timbers, D. Hennes, S. Omidshaﬁei, A. Gruslys et al.,
“Fast computation of Nash equilibria in imperfect information games,”
in International Conference on Machine Learning, 2020, pp. 7119–
7129.

[109] G. Farina, C. Kroer, and T. Sandholm, “Better regularization for
sequential decision spaces: Fast convergence rates for Nash, correlated,
and team equilibria,” arXiv preprint arXiv:2105.12954, 2021.
[110] N. Brown and T. Sandholm, “Safe and nested subgame solving for
imperfect-information games,” in Advances in Neural Information
Processing Systems, vol. 30, 2017, pp. 1–11.

[111] N. Brown, T. Sandholm, and B. Amos, “Depth-limited solving for
imperfect-information games,” in Advances in Neural Information
Processing Systems, vol. 31, 2018, pp. 1–12.

[112] N. Brown, “Equilibrium ﬁnding for

imperfect-
information games,” Ph.D. dissertation, Carnegie Mellon University,
2020.

large adversarial

[113] T. A. Marsland, “A review of game-tree pruning,” ICGA Journal, vol. 9,

no. 1, pp. 3–19, 1986.

[114] T. Sandholm, “Solving imperfect-information games,” Science, vol.

347, no. 6218, pp. 122–123, 2015.

[115] M. Schmid, “Search in imperfect information games,” arXiv preprint

arXiv:2111.05884, 2021.

[116] V. Kovaˇr´ık, D. Milec, M. ˇSustr, D. Seitz, and V. Lis`y, “Fast algorithms
for poker require modelling it as a sequential Bayesian game,” arXiv
preprint arXiv:2112.10890, 2021.

[117] G. Farina, C.-W. Lee, H. Luo, and C. Kroer, “Kernelized multiplica-
tive weights for 0/1-polyhedral games: Bridging the gap between
learning in extensive-form and normal-form games,” arXiv preprint
arXiv:2202.00237, 2022.

[118] L. Meng and Y. Gao, “Generalized bandit regret minimizer frame-
work in imperfect information extensive-form game,” arXiv preprint
arXiv:2203.05920, 2022.

[119] Y. Bai, C. Jin, S. Mei, and T. Yu, “Near-optimal

learning of
information,” arXiv preprint

extensive-form games with imperfect
arXiv:2202.01752, 2022.

[120] T. Kozuno, P. M´enard, R. Munos, and M. Valko, “Model-free learning
for two-player zero-sum partially observable Markov games with
perfect recall,” arXiv preprint arXiv:2106.06279, 2021.

[121] N. Brown and T. Sandholm, “Superhuman AI for multiplayer poker,”

Science, vol. 365, no. 6456, pp. 885–890, 2019.

[122] A. Blair and A. Safﬁdine, “AI surpasses humans at six-player poker,”

Science, vol. 365, no. 6456, pp. 864–865, 2019.

[123] B. Wu, “Hierarchical macro strategy model for MOBA game AI,” in
AAAI Conference on Artiﬁcial Intelligence, vol. 33, no. 1, 2019, pp.
1206–1213.

[124] Y. Tian, Q. Gong, and Y. Jiang, “Joint policy search for multi-
agent collaboration with imperfect information,” in Advances in Neural
Information Processing Systems, vol. 33, 2020, pp. 19 931–19 942.

[100] S. Huang, J. Lei, Y. Hong, and U. V. Shanbhag, “No-regret distributed
learning in two-network zero-sum games,” in Proceedings of IEEE
Conference on Decision and Control, Austin, Texas, 2021, pp. 924–
929.

[125] S. Ganzfried, C. Laughlin, and C. Moreﬁeld, “Parallel algorithm for
Nash equilibrium in multiplayer stochastic games with application to
naval strategic planning,” in International Conference on Distributed
Artiﬁcial Intelligence, 2020, pp. 1–13.

[126] Y. Weilin, H. Zhenzhen, L. Junren, X. Jiahui, J. Xiang, C. Shaofei,
Z. Wanpeng, and C. Jing, “Imperfect information game in multiplayer
no-limit Texas Hold’em based on mean approximation and deep
CFVnet,” in Proceedings of China Automation Congress, 2021, pp.
2459–2466.

[127] J. Heinrich and D. Silver, “Deep reinforcement learning from self-play
in imperfect-information games,” arXiv preprint arXiv:1603.01121,
2016.

[128] H. Li, K. Hu, S. Zhang, Y. Qi, and L. Song, “Double neural counter-
factual regret minimization,” in International Conference on Learning
Representations, 2019, pp. 1–13.

[129] F. Farnia and A. Ozdaglar, “Do GANs always have Nash equilibria?” in
International Conference on Machine Learning, 2020, pp. 3029–3039.
[130] A. Gruslys, M. Lanctot, R. Munos, F. Timbers, M. Schmid, J. Perolat,
D. Morrill, V. Zambaldi, J.-B. Lespiau, J. Schultz et al., “The advantage
regret-matching actor-critic,” arXiv preprint arXiv:2008.12234, 2020.
[131] D. Ye, G. Chen, W. Zhang, S. Chen, B. Yuan, B. Liu, J. Chen, Z. Liu,
F. Qiu, H. Yu et al., “Towards playing full MOBA games with deep
reinforcement learning,” in Advances in Neural Information Processing
Systems, vol. 33, 2020, pp. 621–632.

[132] D. Ye, Z. Liu, M. Sun, B. Shi, P. Zhao, H. Wu, H. Yu, S. Yang, X. Wu,
Q. Guo et al., “Mastering complex control in MOBA games with deep
reinforcement learning,” in AAAI Conference on Artiﬁcial Intelligence,
vol. 34, no. 04, 2020, pp. 6672–6679.

[133] M. Schmid, M. Moravcik, N. Burch, R. Kadlec, J. Davidson, K. Waugh,
N. Bard, F. Timbers, M. Lanctot, Z. Holland et al., “Player of games,”
arXiv preprint arXiv:2112.03178, 2021.

[134] P. Phillips, “Reinforcement learning in two-player zero-sum simulta-
neous action games,” arXiv preprint arXiv:2110.04835, 2021.
[135] H. Fu, W. Liu, S. Wu, Y. Wang, T. Yang, K. Li, J. Xing, B. Li,
B. Ma, Q. Fu et al., “Actor-critic policy optimization in a large-scale
imperfect-information game,” in International Conference on Learning
Representations, 2021, pp. 1–12.

[136] X. Wang, J. Cerny, S. Li, C. Yang, Z. Yin, H. Chan, and B. An,
“A uniﬁed perspective on deep equilibrium ﬁnding,” arXiv preprint
arXiv:2204.04930, 2022.

[137] X. Feng, O. Slumbers, Z. Wan, B. Liu, S. McAleer, Y. Wen, J. Wang,
and Y. Yang, “Neural auto-curricula in two-player zero-sum games,”
in Advances in Neural Information Processing Systems, vol. 34, 2021.
[138] X. Feng, O. Slumbers, Y. Yang, Z. Wan, B. Liu, S. McAleer, Y. Wen,
and J. Wang, “Discovering multi-agent auto-curricula in two-player
zero-sum games,” arXiv preprint arXiv:2106.02745, 2021.

[139] Q. Yin,

J. Yang, W. Ni, B. Liang, and K. Huang, “AI

in
games: Techniques, challenges and opportunities,” arXiv preprint
arXiv:2111.07631, 2021.

[140] A. Celli, A. Marchesi, T. Bianchi, and N. Gatti, “Learning to correlate
in multi-player general-sum sequential games,” in Advances in Neural
Information Processing Systems, vol. 32, 2019.

[141] A. Celli, A. Marchesi, G. Farina, and N. Gatti, “No-regret learning
dynamics for extensive-form correlated equilibrium,” in Advances in
Neural Information Processing Systems, vol. 33, 2020, pp. 7722–7732.
[142] Z. Song, S. Mei, and Y. Bai, “Sample-efﬁcient learning of correlated
equilibria in extensive-form games,” arXiv preprint arXiv:2205.07223,
2022.

[143] C.-Y. Wei, C.-W. Lee, M. Zhang, and H. Luo, “Last-iterate convergence
of decentralized optimistic gradient descent/ascent in inﬁnite-horizon
competitive Markov games,” in Annual Conference on Learning The-
ory, 2021, pp. 4259–4299.

[144] W. Mao and T. Bas¸ar, “Provably efﬁcient reinforcement

learning
in decentralized general-sum Markov games,” Dynamic Games and
Applications, pp. 1–22, 2022.

[145] D. R. Insua, F. Ruggeri, R. Soyer, and S. Wilson, “Advances in
Bayesian decision making in reliability,” European Journal of Opera-
tional Research, vol. 282, no. 1, pp. 1–18, 2020.

[146] C. Casorr´an-Amilburu, “Formulations and algorithms for general and
security Stackelberg games,” Ph.D. dissertation, Universit´e libre de
Bruxelles; Universidad de Chile, 2017.

[147] I. A. Arriagada Fritz, “Benders decomposition based algorithms for
general and security Stackelberg games,” Master’s thesis, Universidad
de Chile, 2021.

[148] S. Dempe, Bilevel Optimization: Theory, Algorithms and Applications.

TU Bergakademie Freiberg, 2018, vol. 3.

17

[150] S. Maharjan, Q. Zhu, Y. Zhang, S. Gjessing, and T. Basar, “Dependable
demand response management in the smart grid: A Stackelberg game
approach,” IEEE Transactions on Smart Grid, vol. 4, no. 1, pp. 120–
132, 2013.

[151] M. Yu and S. H. Hong, “A real-time demand-response algorithm for
smart grids: A Stackelberg game approach,” IEEE Transactions on
Smart Grid, vol. 7, no. 2, pp. 879–888, 2015.

[152] D. Yang, G. Xue, J. Zhang, A. Richa, and X. Fang, “Coping with a
smart jammer in wireless networks: A Stackelberg game approach,”
IEEE Transactions on Wireless Communications, vol. 12, no. 8, pp.
4038–4047, 2013.

[153] C. Guzman, J. Riffo, C. Telha, and M. Van Vyve, “A sequential
Stackelberg game for dynamic inspection problems,” European Journal
of Operational Research, 2021.

[154] Y. Jiang, Y. Zhong, and X. Ge, “IIoT data sharing based on blockchain:
A multi-leader multi-follower Stackelberg game approach,” IEEE In-
ternet of Things Journal, vol. 9, no. 6, pp. 4396–4410, 2021.
[155] S. Leyffer and T. Munson, “Solving multi-leader-common-follower
games,” Optimisation Methods & Software, vol. 25, no. 4, pp. 601–
623, 2010.

[156] H. Zhang, Y. Xiao, L. X. Cai, D. Niyato, L. Song, and Z. Han, “A
multi-leader multi-follower Stackelberg game for resource management
in LTE unlicensed,” IEEE Transactions on Wireless Communications,
vol. 16, no. 1, pp. 348–361, 2016.

[157] L. Mallozzi and R. Messalli, “Multi-leader multi-follower model with

aggregative uncertainty,” Games, vol. 8, no. 3, p. 25, 2017.

[158] T. D. Tran and L. B. Le, “Resource allocation for multi-tenant network
slicing: A multi-leader multi-follower Stackelberg game approach,”
IEEE Transactions on Vehicular Technology, vol. 69, no. 8, pp. 8886–
8899, 2020.

[159] M. Castiglioni, A. Marchesi, and N. Gatti, “Committing to correlated
strategies with multiple leaders,” Artiﬁcial Intelligence, vol. 300, p.
103549, 2021.

[160] J. Pita, M. Jain, M. Tambe, F. Ord´onez, and S. Kraus, “Robust
solutions to Stackelberg games: Addressing bounded rationality and
limited observations in human cognition,” Artiﬁcial Intelligence, vol.
174, no. 15, pp. 1142–1171, 2010.

[161] Y. Bai, C. Jin, H. Wang, and C. Xiong, “Sample-efﬁcient learning of
Stackelberg equilibria in general-sum games,” in Advances in Neural
Information Processing Systems, vol. 34, 2021.

[162] C. Kiekintveld, M. Jain, J. Tsai, J. Pita, F. Ord´onez, and M. Tambe,
“Computing optimal randomized resource allocations for massive se-
curity games,” in Proceedings of International Conference on Au-
tonomous Agents and Multiagent Systems, Budapest, Hungary, 2009,
pp. 689–696.

[163] M. Jain, J. Tsai, J. Pita, C. Kiekintveld, S. Rathi, M. Tambe, and
F. Ord´onez, “Software assistants for randomized patrol planning for
the LAX airport police and the federal air marshal service,” Interfaces,
vol. 40, no. 4, pp. 267–290, 2010.

[164] D. Korzhyk, V. Conitzer, and R. Parr, “Complexity of computing
optimal Stackelberg strategies in security resource allocation games,”
in AAAI conference on Artiﬁcial Intelligence, Georgia, USA, 2010, pp.
805–810.

[165] F. Fang and T. H. Nguyen, “Green security games: Apply game theory
to addressing green security challenges,” ACM SIGecom Exchanges,
vol. 15, no. 1, pp. 78–83, 2016.

[166] M. Brown, A. Sinha, A. Schlenker, and M. Tambe, “One size does
not ﬁt all: A game-theoretic approach for dynamically and effectively
screening for threats,” in AAAI Conference on Artiﬁcial Intelligence,
vol. 30, no. 1, Arizona, USA, 2016.

[167] C. Zhang, S. Gholami, D. Kar, A. Sinha, M. Jain, R. Goyal, and
M. Tambe, “Keeping pace with criminals: An extended study of
designing patrol allocation against adaptive opportunistic criminals,”
Games, vol. 7, no. 3, p. 15, 2016.

[168] P. Dasgupta, J. B. Collins, and R. Mittu, Adversary-Aware Learning

Techniques and Trends in Cybersecurity. Springer, 2021.

[169] E. Galinkin, “Information security games: A survey,” arXiv preprint

arXiv:2103.12520, 2021.
[170] V. Bucarey, C. Casorr´an,

´O. Figueroa, K. Rosas, H. Navarrete, and
F. Ord´o˜nez, “Building real Stackelberg security games for border
patrols,” in International Conference on Decision and Game Theory
for Security, Vienna, Austria, 2017, pp. 193–212.

[149] T. Li and S. P. Sethi, “A review of dynamic Stackelberg game models,”
Discrete & Continuous Dynamical Systems-B, vol. 22, no. 1, p. 125,
2017.

[171] V. Bucarey, C. Casorr´an, M. Labb´e, F. Ordo˜nez, and O. Figueroa,
“Coordinating resources in Stackelberg security games,” European
Journal of Operational Research, vol. 291, no. 3, pp. 846–861, 2021.

[172] J. Lou and Y. Vorobeychik, “Equilibrium analysis of multi-defender
security games,” in International Joint Conference on Artiﬁcial Intel-
ligence (IJCAI), Buenos Aires, Argentina, 2015, pp. 596–602.

[173] D. Mutzari, Y. Aumann, and S. Kraus, “Robust solutions

multi-defender
arXiv:2204.14000, 2022.

Stackelberg

security

games,”

arXiv

for
preprint

[174] Y. Li, V. Conitzer, and D. Korzhyk, “Catcher-evader games,” in
International Joint Conference on Artiﬁcial Intelligence (IJCAI), New
York, USA, 2016, pp. 329–337.

[175] B. Wang, Y. Zhang, Z.-H. Zhou, and S. Zhong, “On repeated Stack-
elberg security game with the cooperative human behavior model for
wildlife protection,” Applied Intelligence, vol. 49, no. 3, pp. 1002–1015,
2019.

[176] W. Ma, W. Liu, K. McAreavey, X. Luo, Y. Jiang, J. Zhan, and Z. Chen,
“A decision support framework for security resource allocation under
ambiguity,” International Journal of Intelligent Systems, vol. 36, no. 1,
pp. 5–52, 2021.

[177] T. Fiez, B. Chasnov, and L. J. Ratliff, “Convergence of learning
dynamics in Stackelberg games,” arXiv preprint arXiv:1906.01217,
2019.

[178] A. A. Kulkarni and U. V. Shanbhag, “An existence result for hier-
archical Stackelberg v/s Stackelberg games,” IEEE Transactions on
Automatic Control, vol. 60, no. 12, pp. 3379–3384, 2015.

[179] D. Goktas, J. Zhao, and A. Greenwald, “Robust no-regret learning in
min-max Stackelberg games,” arXiv preprint arXiv:2203.14126, 2022.
[180] M. Mafﬁoli, “Dealing with partial information in follower’s behavior

identiﬁcation,” Master’s thesis, Politecnico di Milano, 2019.

[181] Z. Cheng, G. Chen, and Y. Hong, “Single-leader-multiple-followers
Stackelberg security game with hypergame framework,” IEEE Trans-
actions on Information Forensics and Security, vol. 17, pp. 954–969,
2022.

[182] G. Birmpas, J. Gan, A. Hollender, F. J. Marmolejo-Coss´ıo, N. Rajgopal,
and A. A. Voudouris, “Optimally deceiving a learning leader
in
Stackelberg games,” Journal of Artiﬁcial Intelligence Research, vol. 72,
pp. 507–531, 2021.

[183] D. Lukes and D. Russell, “A global theory for linear-quadratic dif-
ferential games,” Journal of Mathematical Analysis and Applications,
vol. 33, no. 1, pp. 96–123, 1971.

[184] J. Engwerda, “Linear quadratic differential games: An overview,”
Advances in Dynamic Games and Their Applications, pp. 1–34, 2009.
[185] J. Shinar, V. Turetsky, V. Y. Glizer, and E. Ianovsky, “Solvability
of linear-quadratic differential games associated with pursuit-evasion
problems,” International Game Theory Review, vol. 10, no. 04, pp.
481–515, 2008.

[186] I. E. Weintraub, M. Pachter, and E. Garcia, “An introduction to pursuit-
evasion differential games,” in Proceedings of American Control Con-
ference (ACC), Denver, CO, USA, 2020, pp. 1049–1066.

[187] A. Gibali and O. Kelis, “An analytic and numerical investigation of a

differential game,” Axioms, vol. 10, no. 2, p. 66, 2021.

[188] Y. Huang, J. Chen, and Q. Zhu, “Defending an asset with partial
information and selected observations: A differential game framework,”
in Proceedings of IEEE Conference on Decision and Control (CDC),
Austin, Texas, 2021, pp. 2366–2373.

[189] Y. Huang and Q. Zhu, “A pursuit-evasion differential game with
strategic information acquisition,” arXiv preprint arXiv:2102.05469,
2021.

[190] E. Garcia, D. W. Casbeer, M. Pachter, J. W. Curtis, and E. Doucette,
“A two-team linear quadratic differential game of defending a target,”
in Proceedings of American Control Conference (ACC), Denver, CO,
USA, 2020, pp. 1665–1670.

[191] X. Li, J. Shi, and J. Yong, “Mean-ﬁeld linear-quadratic stochastic differ-
ential games in an inﬁnite horizon,” arXiv preprint arXiv:2007.06130,
2020.

[192] H. Zhang, Q. Wei, and D. Liu, “An iterative adaptive dynamic pro-
gramming method for solving a class of nonlinear zero-sum differential
games,” Automatica, vol. 47, no. 1, pp. 207–214, 2011.

[193] R. Song, J. Li, and F. L. Lewis, “Robust optimal control for disturbed
nonlinear zero-sum differential games based on single NN and least
squares,” IEEE Transactions on Systems, Man, and Cybernetics: Sys-
tems, vol. 50, no. 11, pp. 4009–4019, 2019.

[194] N. Y. Lukoyanov, “Functional Hamilton-Jacobi type equations with ci-
derivatives in control problems with hereditary information,” Nonlinear
Functional Analysis and Applications, vol. 8, no. 4, pp. 535–555, 2003.
[195] A. Plaksin, “On Hamilton-Jacobi-Bellman-Isaacs equation for time-
delay systems,” IFAC-PapersOnLine, vol. 52, no. 18, pp. 138–143,
2019.

18

[196] W. Meng and J. Shi, “A linear quadratic stochastic Stackelberg differ-
ential game with time delay,” arXiv preprint arXiv:2012.14145, 2020.
[197] M. I. Gomoyunov, “Dynamic programming principle and Hamilton-
Jacobi-Bellman equations for fractional-order systems,” SIAM Journal
on Control and Optimization, vol. 58, no. 6, pp. 3185–3211, 2020.

[198] J. Moon and T. Basar, “Zero-sum differential games on the Wasserstein

space,” arXiv preprint arXiv:1912.06084, 2019.

[199] D. Liu and Q. Wei, “Multiperson zero-sum differential games for a
class of uncertain nonlinear systems,” International Journal of Adaptive
Control and Signal Processing, vol. 28, no. 3-5, pp. 205–231, 2014.

[200] J. Shi and G. Wang, “A linear-quadratic Stackelberg differential game
with mixed deterministic and stochastic controls,” arXiv preprint
arXiv:2004.00653, 2020.

[201] A. E.-M. A. Megahed, “The Stackelberg differential game for counter-

terrorism,” Quality & Quantity, vol. 53, no. 1, pp. 207–220, 2019.

[202] D. Lee and C. J. Tomlin, “Hamilton-Jacobi equations for two classes of
state-constrained zero-sum games,” arXiv preprint arXiv:2106.15006,
2021.

[203] R. Elliott and M. Davis, “Optimal play in a stochastic differential
game,” SIAM Journal on Control and Optimization, vol. 19, no. 4,
pp. 543–554, 1981.

[204] J. Moon, T. E. Duncan, and T. Bas¸ar, “Risk-sensitive zero-sum differ-
ential games,” IEEE Transactions on Automatic Control, vol. 64, no. 4,
pp. 1503–1518, 2018.

[205] J. Sun, “Two-person zero-sum stochastic linear-quadratic differential
games,” SIAM Journal on Control and Optimization, vol. 59, no. 3,
pp. 1804–1829, 2021.

[206] J. Li, W. Li, and H. Zhao, “On the value of a general stochastic dif-
ferential game with ergodic payoff,” arXiv preprint arXiv:2106.15894,
2021.

[207] J. Shi, G. Wang, and J. Xiong, “Linear-quadratic stochastic Stackel-
berg differential game with asymmetric information,” Science China
Information Sciences, vol. 60, no. 9, pp. 1–15, 2017.

[208] J. Moon, “Linear-quadratic stochastic Stackelberg differential games
for jump-diffusion systems,” SIAM Journal on Control and Optimiza-
tion, vol. 59, no. 2, pp. 954–976, 2021.

[209] J. Sun, H. Wang, and J. Wen, “Zero-sum Stackelberg stochastic linear-
quadratic differential games,” arXiv preprint arXiv:2109.14893, 2021.
[210] J. Huang, S. Wang, and Z. Wu, “Robust Stackelberg differential game
with model uncertainty,” IEEE Transactions on Automatic Control,
DOI: 10.1109/TAC.2021.3097549, 2021.

[211] Y. Zheng and J. Shi, “Stackelberg stochastic differential game with
asymmetric noisy observations,” International Journal of Control, pp.
1–21, 2021.

[212] L. C. Evans and P. E. Souganidis, “Differential games and repre-
sentation formulas for solutions of Hamilton-Jacobi-Isaacs equations,”
Indiana University Mathematics Journal, vol. 33, no. 5, pp. 773–797,
1984.

[213] A. Altarovici, O. Bokanowski, and H. Zidani, “A general Hamilton-
Jacobi framework for non-linear state-constrained control problems,”
ESAIM: Control, Optimisation and Calculus of Variations, vol. 19,
no. 2, pp. 337–357, 2013.

[214] I. M. Mitchell, A. M. Bayen, and C. J. Tomlin, “A time-dependent
Hamilton-Jacobi formulation of reachable sets for continuous dynamic
games,” IEEE Transactions on Automatic Control, vol. 50, no. 7, pp.
947–957, 2005.

[215] K. Margellos and J. Lygeros, “Hamilton-Jacobi formulation for reach-
avoid differential games,” IEEE Transactions on Automatic Control,
vol. 56, no. 8, pp. 1849–1861, 2011.

[216] J. F. Fisac, M. Chen, C. J. Tomlin, and S. S. Sastry, “Reach-avoid
problems with time-varying dynamics, targets and constraints,” in Pro-
ceedings of International Conference on Hybrid Systems: Computation
and Control, Seattle, Washington, 2015, pp. 11–20.

[217] B. E. Asri and H. Lalioui, “Deterministic differential games in inﬁnite
horizon involving continuous and impulse controls,” arXiv preprint
arXiv:2107.03524, 2021.

[218] J. Moon, “Linear-quadratic mean-ﬁeld stochastic zero-sum differential

games,” Automatica, vol. 120, p. 109067, 2020.

[219] J. Sun, H. Wang, and Z. Wu, “Mean-ﬁeld linear-quadratic stochastic
differential games,” Journal of Differential Equations, vol. 296, pp.
299–334, 2021.

[220] S. Hart and A. Mas-Colell, “A simple adaptive procedure leading to
correlated equilibrium,” Econometrica, vol. 68, no. 5, pp. 1127–1150,
2000.

[221] O. Tammelin, “Solving large imperfect

information games using

CFR+,” arXiv preprint arXiv:1407.5042, 2014.

[222] G. W. Brown, “Iterative solution of games by ﬁctitious play,” Activity
Analysis of Production and Allocation, vol. 13, no. 1, pp. 374–376,
1951.

[223] S. Ganzfried, “Fictitious play outperforms counterfactual regret mini-

mization,” arXiv preprint arXiv:2001.11165, 2020.

[224] H. B. McMahan, G. J. Gordon, and A. Blum, “Planning in the
presence of cost functions controlled by an adversary,” in International
Conference on Machine Learning, Washington, USA, 2003, pp. 536–
543.

[225] X. Xu and Q. Zhao, “Distributed no-regret

learning in multiagent
systems: Challenges and recent developments,” IEEE Signal Processing
Magazine, vol. 37, no. 3, pp. 84–91, 2020.

[226] H. Zhang, A. Lerer, and N. Brown, “Equilibrium ﬁnding in
normal-form games via greedy regret minimization,” arXiv preprint
arXiv:2204.04826, 2022.

[227] K. Lu, G. Li, and L. Wang, “Online distributed algorithms for seeking
generalized Nash equilibria in dynamic environments,” IEEE Transac-
tions on Automatic Control, vol. 66, no. 5, pp. 2289–2296, 2020.
[228] M. Meng, X. Li, Y. Hong, J. Chen, and L. Wang, “Decentralized online
learning for noncooperative games in dynamic environments,” arXiv
preprint arXiv:2105.06200, 2021.

[229] M. Meng, X. Li, and J. Chen, “Decentralized Nash equilibria
feedback,” arXiv preprint

learning for online game with bandit
arXiv:2204.09467, 2022.

[230] M. Zhang, P. Zhao, H. Luo, and Z.-H. Zhou, “No-regret learning in
time-varying zero-sum games,” arXiv preprint arXiv:2201.12736, 2022.
[231] C. Daskalakis, M. Fishelson, and N. Golowich, “Near-optimal no-regret
learning in general games,” Advances in Neural Information Processing
Systems, vol. 34, pp. 1–13, 2021.

[232] Y.-G. Hsieh, K. Antonakopoulos, and P. Mertikopoulos, “Adaptive
learning in continuous games: Optimal regret bounds and convergence
to Nash equilibrium,” in Annual Conference on Learning Theory, 2021,
pp. 2388–2422.

[233] M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione, “Regret
minimization in games with incomplete information,” in Advances in
Neural Information Processing Systems, vol. 20, 2007, pp. 1–8.
[234] M. Bowling, N. Burch, M. Johanson, and O. Tammelin, “Heads-up
limit hold’em poker is solved,” Science, vol. 347, no. 6218, pp. 145–
149, 2015.

[235] N. Brown and T. Sandholm, “Solving imperfect-information games
via discounted regret minimization,” in AAAI Conference on Artiﬁcial
Intelligence, vol. 33, no. 01, 2019, pp. 1829–1836.

[236] N. Brown, A. Lerer, S. Gross, and T. Sandholm, “Deep counterfactual
regret minimization,” in International Conference on Machine Learn-
ing, 2019, pp. 793–802.

[237] H. Li, X. Wang, S. Qi, J. Zhang, Y. Liu, Y. Wu, and F. Jia, “Solv-
ing imperfect-information games via exponential counterfactual regret
minimization,” arXiv preprint arXiv:2008.02679v2, 2020.

[238] H. Xu, K. Li, H. Fu, Q. Fu, and J. Xing, “AutoCFR: Learning to design
counterfactual regret minimization algorithms,” in AAAI Conference on
Artiﬁcial Intelligence, 2022, pp. 1–8.

[239] T. W. Neller and M. Lanctot, “An introduction to counterfactual regret
minimization,” in Proceedings of Model AI Assignments, The Fourth
Symposium on Educational Advances in Artiﬁcial Intelligence, vol. 11,
2013.

[240] P. Muller, S. Omidshaﬁei, M. Rowland, K. Tuyls, J. Perolat, S. Liu,
D. Hennes, L. Marris, M. Lanctot, E. Hughes, Z. Wang, G. Lever,
N. Heess, T. Graepel, and R. Munos, “A generalized training approach
learning,” in International Conference on Learning
for multiagent
Representations, 2020, pp. 1–13.

[241] E. Steinberger, “Single deep counterfactual regret minimization,” arXiv

preprint arXiv:1901.07621, 2019.

[242] P. Mertikopoulos, C. Papadimitriou, and G. Piliouras, “Cycles in
adversarial regularized learning,” in Proceedings of Annual ACM-SIAM
Symposium on Discrete Algorithms, New Orleans, LA, USA, 2018, pp.
2703–2717.

[243] E.-V. Vlatakis-Gkaragkounis, L. Flokas, T. Lianeas, P. Mertikopoulos,
and G. Piliouras, “No-regret learning and mixed Nash equilibria: They
do not mix,” in Advances in Neural Information Processing Systems,
Virtual, 2020, pp. 1380–1391.

[244] C. Daskalakis and I. Panageas, “Last-iterate convergence: Zero-
sum games and constrained min-max optimization,” arXiv preprint
arXiv:1807.04252, 2018.

[245] J. Abernethy, K. A. Lai, and A. Wibisono, “Last-iterate convergence
rates for min-max optimization,” arXiv preprint arXiv:1906.02027,
2019.

19

[246] N. Golowich, S. Pattathil, C. Daskalakis, and A. Ozdaglar, “Last iterate
is slower than averaged iterate in smooth convex-concave saddle point
problems,” in Annual Conference on Learning Theory, 2020, pp. 1758–
1784.

[247] V. Conitzer and D. Korzhyk, “Commitment to correlated strategies,”
in AAAI Conference on Artiﬁcial Intelligence, California, USA, 2011,
pp. 632–637.

[248] J. F. Benders, “Partitioning procedures for solving mixed-variables
programming problems,” Numerische Mathematik, vol. 4, no. 1, pp.
238–252, 1962.

[249] J. Farkas, “Theorie der einfachen Ungleichungen,” Journal f¨ur die reine

und angewandte Mathematik, vol. 1902, no. 124, pp. 1–27, 1902.

[250] M. Fischetti, D. Salvagnin, and A. Zanette, “Minimal infeasible sub-

systems and Benders cuts.”

[251] R. E. Gomory, “Outline of an algorithm for integer solutions to linear
programs,” Bulletin of the American Mathematical Society, vol. 64, pp.
275–278, 1958.

[252] A. H. Land and A. G. Doig, “An automatic method of solving discrete
programming problems,” Econometrica, vol. 28, no. 3, pp. 497–520,
1960.

[253] S. Ruder, “An overview of gradient descent optimization algorithms,”

arXiv preprint arXiv:1609.04747, 2016.

[254] S. Gottipati and P. Paruchuri, “A genetic algorithm approach to
compute mixed strategy solutions for general Stackelberg games,” in
IEEE Congress on Evolutionary Computation, Krakow, Poland, 2021,
pp. 1648–1655.

[255] G. De Nittis and F. Trovo, “Machine learning techniques for Stack-
elberg security games: A survey,” arXiv preprint arXiv:1609.09341,
2016.

[256] H. V. Tran, Hamilton-Jacobi Equations: Theory and Applications.

American Mathematical Soc., 2021, vol. 213.

[257] M. Li, J. Qin, N. M. Freris, and D. W. C. Ho, “Multi-player
Stackelberg-Nash Game for nonlinear system via value iteration-
based integral reinforcement learning,” IEEE Transactions on Neural
Networks and Learning Systems, vol. 30, no. 4, pp. 1429–1440, 2022.
[258] S. Ontan´on, G. Synnaeve, A. Uriarte, F. Richoux, D. Churchill,
and M. Preuss, “A survey of real-time strategy game AI research
and competition in StarCraft,” IEEE Transactions on Computational
Intelligence and AI in Games, vol. 5, no. 4, pp. 293–311, 2013.
[259] S. Davidai and M. Ongis, “The politics of zero-sum thinking: The
relationship between political ideology and the belief that life is a
zero-sum game,” Science Advances, vol. 5, no. 12, pp. 1–10, 2019.

[260] A. Von Moll, E. Garcia, D. Casbeer, M. Suresh, and S. C. Swar,
“Multiple-pursuer, single-evader border defense differential game,”
Journal of Aerospace Information Systems, vol. 17, no. 8, pp. 407–
416, 2020.

[261] X. Gao, E. Akyol, and T. Basar, “Communication scheduling and
remote estimation with adversarial intervention,” IEEE/CAA Journal
of Automatica Sinica, vol. 6, no. 1, pp. 32–44, 2019.

[262] X. Na and D. Cole, “Theoretical and experimental

investigation
of driver noncooperative-game steering control behavior,” IEEE/CAA
Journal of Automatica Sinica, vol. 8, no. 1, pp. 189–205, 2021.
[263] L. A. Albert, A. Nikolaev, and S. H. Jacobson, “Homeland security

research opportunities,” IISE Transactions, pp. 1–23, 2022.

[264] X. Song, P. Willett, S. Zhou, and P. B. Luh, “The MIMO radar and
jammer games,” IEEE Transactions on Signal Processing, vol. 60,
no. 2, pp. 687–699, 2011.

[265] H. Li, Z. Han, W. Pu, L. Liu, K. Li, and B. Jiu, “Counterfactual regret
minimization for anti-jamming game of frequency agile radar,” arXiv
preprint arXiv:2202.10049, 2022.

[266] D. J. Bachmann, R. J. Evans, and B. Moran, “Game theoretic analysis
of adaptive radar jamming,” IEEE Transactions on Aerospace and
Electronic Systems, vol. 47, no. 2, pp. 1081–1100, 2011.

[267] E. Garcia, A. Von Moll, D. W. Casbeer, and M. Pachter, “Strategies
for defending a coastline against multiple attackers,” in Proceedings of
IEEE Conference on Decision and Control (CDC), Nice, France, 2019,
pp. 7319–7324.

[268] L. H. S. Lelis, “Planning algorithms for zero-sum games with exponen-
tial action spaces: A unifying perspective,” in International Conference
on International Joint Conferences on Artiﬁcial Intelligence, 2021, pp.
4892–4898.

[269] Q. Liu, Y. Wang, and C. Jin, “Learning Markov games with adversarial
opponents: Efﬁcient algorithms and fundamental limits,” arXiv preprint
arXiv:2203.06803, 2022.

[270] S. Banik and S. D. Bopardikar, “Attack-resilient path planning using
dynamic games with stopping states,” IEEE Transactions on Robotics,
vol. 38, no. 1, pp. 25–41, 2021.

20

[271] K. Wang, C. Gou, Y. Duan, Y. Lin, X. Zheng, and F.-Y. Wang, “Gen-
erative adversarial networks: Introduction and outlook,” IEEE/CAA
Journal of Automatica Sinica, vol. 4, no. 4, pp. 588–598, 2017.
[272] C.-W. Lee, C. Kroer, and H. Luo, “Last-iterate convergence in
extensive-form games,” in Advances in Neural Information Processing
Systems, vol. 34, 2021, pp. 1–13.

[273] J. Perolat, R. Munos, J.-B. Lespiau, S. Omidshaﬁei, M. Rowland, P. Or-
tega, N. Burch, T. Anthony, D. Balduzzi, B. De Vylder et al., “From
Poincar´e recurrence to convergence in imperfect information games:
Finding equilibrium via regularization,” in International Conference
on Machine Learning, 2021, pp. 8525–8535.

[274] H. Henderson, “Cybered competition, cooperation, and conﬂict in a
game of imperfect information,” The Cyber Defense Review, vol. 6,
no. 3, pp. 43–60, 2021.

[275] G. Costikyan, Uncertainty in Games. MIT Press, 2013.
[276] L. Xu, “Learning and planning under uncertainty for green security,”
in International Joint Conference on Artiﬁcial Intelligence, 2021, pp.
1–3.

[277] D. Kar, F. Fang, F. Delle Fave, N. Sintov, and M. Tambe, “A
game of thrones: When human behavior models compete in repeated
Stackelberg security games,” in Proceedings of the 2015 International
Conference on Autonomous Agents and Multiagent Systems, 2015, pp.
1381–1390.

[278] W. N. Caballero, B. J. Lunday, and R. P. Uber, “Identifying behaviorally
robust strategies for normal form games under varying forms of
uncertainty,” European Journal of Operational Research, vol. 288,
no. 3, pp. 971–982, 2021.

[279] P. Tsiotras, “Bounded rationality in learning, perception, decision-
making, and stochastic games,” in Handbook of Reinforcement Learn-
ing and Control, 2021, pp. 491–523.

[280] A. Platzer, “Differential game logic,” ACM Transactions on Computa-

tional Logic, vol. 17, no. 1, pp. 1–51, 2015.

[281] M. Iyer and B. Gilby, “Modeling an adversarial poacher-ranger hybrid

game.”

[282] N. Brown, A. Bakhtin, A. Lerer, and Q. Gong, “Combining deep
reinforcement learning and search for imperfect-information games,”
in Advances in Neural Information Processing Systems, vol. 33, 2020,
pp. 17 057–17 069.

[283] K. Li, H. Xu, M. Zhang, E. Zhao, Z. Wu, J. Xing, and K. Huang,
“OpenHoldem: An open toolkit for large-scale imperfect-information
game research,” arXiv preprint arXiv:2012.06168, 2020.

[284] I. Oh, S. Rho, S. Moon, S. Son, H. Lee, and J. Chung, “Creating pro-
level AI for a real-time ﬁghting game using deep reinforcement learn-
ing,” IEEE Transactions on Games, DOI: 10.1109/TG.2021.3049539,
2021.

