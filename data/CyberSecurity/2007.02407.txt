1
2
0
2

r
a

M
3
1

]

G
L
.
s
c
[

3
v
7
0
4
2
0
.
7
0
0
2
:
v
i
X
r
a

Adversarial Machine Learning Attacks and Defense Methods
in the Cyber Security Domain

Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach

Ben-Gurion University of the Negev

March 16, 2021

Abstract

In recent years machine learning algorithms, and more speciﬁcally deep learning algorithms, have been
widely used in many ﬁelds, including cyber security. However, machine learning systems are vulnerable
to adversarial attacks, and this limits the application of machine learning, especially in non-stationary,
adversarial environments, such as the cyber security domain, where actual adversaries (e.g., malware de-
velopers) exist. This paper comprehensively summarizes the latest research on adversarial attacks against
security solutions based on machine learning techniques and illuminates the risks they pose. First, the ad-
versarial attack methods are characterized based on their stage of occurrence, and the attacker’s goals and
capabilities. Then, we categorize the applications of adversarial attack and defense methods in the cyber
security domain. Finally, we highlight some characteristics identiﬁed in recent research and discuss the
impact of recent advancements in other adversarial learning domains on future research directions in the
cyber security domain. This paper is the ﬁrst to discuss the unique challenges of implementing end-to-end
adversarial attacks in the cyber security domain, map them in a uniﬁed taxonomy, and use the taxonomy
to highlight future research directions.

CCSDescription: General and reference -> Surveys and overviews

Keywords: adversarial learning; adversarial machine learning; evasion attacks; poisoning attacks; deep

learning; adversarial examples; cyber security.

1 Introduction

The growing use of machine learning, and particularly deep learning, in ﬁelds like computer vision and
natural language processing (NLP) has been accompanied by increased interest in the domain of adversarial
machine learning (a.k.a. adversarial learning), i.e., attacking and defending machine learning models algo-
rithmically (Huang et al. (2011)). Of special interest are adversarial examples, which are samples modiﬁed
in order to be misclassiﬁed by the classiﬁer attacked.

Most of the research in adversarial learning has focused on the computer vision domain, and more
speciﬁcally in the image recognition domain. This research has centered mainly on convolutional neural
networks (CNNs), which are commonly used in the computer vision domain (Akhtar and Mian (2018); Qiu
et al. (2019)). However, in recent years, adversarial example generation methods have increasingly been

1

 
 
 
 
 
 
Accepted as a long survey paper at ACM CSUR 2021

utilized in other domains, including natural language processing (e.g., Gao et al. (2018)). These attacks
have also been used recently in the cyber security domain (e.g., Rosenberg et al. (2018)). This domain
is particularly interesting, because it is rife with adversaries (e.g., malware developers who want to evade
machine and deep learning-based next generation antivirus products, spam ﬁlters, etc.). Adversarial learning
methods have already been executed against static analysis deep neural networks1.

The main goal of this paper is to illuminate the risks posed by adversarial learning to cyber security
solutions that are based on machine learning techniques. This paper contains: (1) an in-depth discussion
about the unique challenges of adversarial learning in the cyber security domain (Section 2), (2) a summary
of the state-of-the-art adversarial learning research papers in the cyber security domain, categorized by
application (Sections 5 and 6) and characterized by our uniﬁed taxonomy (deﬁned in Section 4), (3) a
discussion of the challenges associated with adversarial learning in the cyber security domain and possible
future research directions (Section 7), including issues relating to existing defense methods (and the lack
thereof), and (4) a summary of the theoretical background on the adversarial methods used in the cyber
security domain (Section 3). We focus on adversarial attacks and defenses against classiﬁers used in the
cyber security domain, and not on other topics, such as attacks on models’ interpretability (Kuppa and Le-
Khac (2020)) or methods to assist the model’s interpretability (Ross and Doshi-Velez (2018)).

The main contributions of this paper are as follows:
1. We focus on a wide range of adversarial learning applications in the cyber security domain (e.g.,
malware detection, speaker recognition, cyber-physical systems, etc.), introduce a new uniﬁed taxonomy
and illustrate how existing research ﬁts into this taxonomy, providing a holistic overview of the ﬁeld. In
contrast, previous work focused mainly on speciﬁc domains, e.g., malware detection or network intrusion
detection.

2. Using our taxonomy, we highlight research gaps in the cyber security domain that have already
been addressed in other adversarial learning domains (e.g., Trojan neural networks in the image recognition
domain) and discuss their impact on current and future trends in adversarial learning in the cyber security
domain.

3. We discuss the unique challenges that attackers and defenders face in the cyber security domain,
challenges which do not exist in other domains (e.g., image recognition). For instance, in the cyber security
domain, the attacker must verify that the original functionality of the modiﬁed malware remains intact. Our
discussion addresses the fundamental differences between adversarial attacks performed in the cyber security
domain and those performed in other domains.

2 Preliminary Discussion: The Differences Between Adversarial At-

tacks in the Computer Vision and Cyber Security Domains

Most adversarial attacks published, including those published at academic cyber security conferences, have
focused on the computer vision domain, e.g., generating a cat image that would be classiﬁed as a dog by
the classiﬁer. However, the cyber security domain (e.g., malware detection) seems to be a more relevant
domain for adversarial attacks, because in the computer vision domain, there is no real adversary (with
a few exceptions, e.g., terrorists who want to tamper with autonomous cars’ pedestrian detection systems
(Eykholt et al. (2018)), deepfakes (Liu et al. (2020)) which might cause fake news or ﬁnancial fraud, etc.).
In contrast, in the cyber security domain, there are actual adversaries with clear targeted goals. Examples
include ransomware developers who depend on the ability of their ransomware to evade anti-malware prod-
ucts that would prevent both its execution and the developers from collecting the ransom money, and other

1 https://skylightcyber.com/2019/07/18/cylance-i-kill-you/

Accepted as a long survey paper at ACM CSUR 2021

types of malware that need to steal user information (e.g., keyloggers), spread across the network (worms),
or perform any other malicious functionality while remaining undetected.

A key step in deﬁning the proper taxonomy for the cyber security domain is answering this question:
Given the obvious relevance of the cyber security domain to adversarial attacks, why do most adversarial
learning researchers focus on computer vision? In addition to the fact that image recognition is a popular
machine learning research topic, another major reason for the focus on computer vision is that performing an
end-to-end adversarial attack in the cyber security domain is more difﬁcult than performing such an attack in
the computer vision domain. The differences between adversarial attacks performed in those two domains
and the challenges that arise in the cyber security domain are discussed in the subsections that follow.

2.1 Keeping (Malicious) Functionality Intact in the Perturbed Sample

Any adversarial executable ﬁle must preserve its malicious functionality after the sample’s modiﬁcation.
This might be the main difference between the image classiﬁcation and malware detection domains, and
pose the greatest challenge. In the image recognition domain, the adversary can change every pixel’s color
(to a different valid color) without creating an “invalid picture” as part of the attack. However, in the cyber
security domain, modifying an API call or arbitrary executable’s content byte value might cause the modiﬁed
executable to perform a different functionality (e.g., changing a WriteFile() call to ReadFile() ) or even crash
(if you change an arbitrary byte in an opcode to an invalid opcode that would cause an exception). The same
is true for network packets; perturbing a network packet in order to evade the network intrusion detection
system (NIDS) while keeping a valid packet structure is challenging.

In order to address this challenge, adversaries in the cyber security domain must implement their own
methods (which are usually feature-speciﬁc) to modify features in a way that will not break the functionality
of the perturbed sample, whether it is an executable, a network packet, or something else. For instance, the
adversarial attack used in Rosenberg et al. (2018) generates a new malware PE (portable executable) with a
modiﬁed API call trace in a functionality-preserving manner.

2.2 Small Perturbations Are Not Applicable for Discrete Features

In the computer vision domain, gradient-based adversarial attacks, e.g., the fast gradient sign method (FGSM)
(Section 3), generate a minimal random modiﬁcation to the input image in the direction that would most sig-
niﬁcantly impact the target classiﬁer prediction. A ‘small modiﬁcation’ (a.k.a. perturbation) can be, for
example, changing a single pixel’s color to a very similar color (a single pixel’s color can be changed from
brown to black to fool the image classiﬁer).

However, the logic of a ‘small perturbation’ cannot be applied to many cyber security features. Consider
a dynamic analysis classiﬁer that uses API calls. An equivalent to changing a single pixel’s color would be to
change a single API call to another API call. Even if we disregard what such a modiﬁcation would do to the
executable’s functionality (mentioned in the previous subsection), would one of the following be considered
a ‘small perturbation’ of the WriteFile() API call: (1) modifying it to ReadFile() (a different operation for
the same medium), or (2) modifying it to RegSetValueEx() (a similar operation for a different medium)?
The use of discrete features (e.g., API calls) which are not continuous or ordinal severely limits the use of
gradient-based attack methods (Section 3). The implications of this issue will be discussed in Section 7.6.1.

Accepted as a long survey paper at ACM CSUR 2021

2.3 Executables are More Complex than Images

An image used as input to an image classiﬁer (usually a convolutional neural network, CNN) is represented
as a ﬁxed size matrix of pixel colors. If the actual image has different dimensions than the input matrix, the
picture will usually be resized, clipped, or padded to ﬁt the dimension limits.

An executable, on the other hand, has a variable length: executables can range in size from several
kilobytes to several gigabytes. It is also unreasonable to expect a clipped executable to maintain its original
classiﬁcation. Let’s assume that we have a 100MB benign executable into which we inject a shellcode in a
function near the end-of-ﬁle. If the shellcode is clipped in order to ﬁt the malware classiﬁer’s dimensions,
there is no reason that the ﬁle would be classiﬁed as malicious, because its benign variant would be clipped
to the exact same form.

In addition, the execution path of an executable may depend on the input, and thus, the adversarial
perturbation should support any possible input that the malware may encounter when executed in the target
machine.

Finally, in the cyber security domain, classiﬁers usually use more than a single feature type as input
(e.g., phishing detection using both URLs and connected server properties as was done in Shirazi et al.
(2019)). ]). A non-exhaustive list of features used in the cyber security domain is presented in Figure 2.
Some feature types are easier to modify without harming the executable’s functionality than others. For
instance, in the adversarial attack used in Rosenberg et al. (2018), appending printable strings to the end of a
malware PE ﬁle is much easier than adding API calls to the PE ﬁle using a dedicated framework built for this
purpose. In contrast, in an image adversarial attack, modifying each pixel has the same level of difﬁculty.
The implications of this issue are discussed in Section 7.6.1.

While this is a challenge for malware classiﬁer implementation, it also affects adversarial attacks against
malware classiﬁers. For instance, attacks in which you have a ﬁxed input dimension, (e.g., a 28*28 matrix
for MNIST images), are much easier to implement than attacks for which you need to consider the variable
ﬁle size.

3 Adversarial Learning Methods Used in the Cyber Security Domain

This section includes a list of theoretical adversarial learning methods that are used in the cyber security
domain but were inspired by attacks from other domains. Due to space limitations, this is not a complete
list of the state-of-the-art prior work in other domains, such as image recognition or NLP. Only methods
that have been used in the cyber security domain are mentioned. A more comprehensive list that includes
methods from other domains can be found, e.g., in Qiu et al. (2019).

The search for adversarial examples as a similar minimization problem is formalized in Szegedy et al.

(2014) and Biggio et al. (2013):

argr min f (x + r) (cid:54)= f (x) s.t. x + r ∈ D

(1)

The input x, correctly classiﬁed by the classiﬁer f , is perturbed with r such that the resulting adversarial
example, x + r, remains in the input domain D but is assigned a different label than x. To solve Equation 1,
we need to transform the constraint f (x + r) (cid:54)= f (x) into an optimizable formulation. Then we can easily use
the Lagrange multiplier to solve it. To do this, we deﬁne a loss function Loss() to quantify this constraint.
This loss function can be the same as the training loss, or different, e.g., hinge loss or cross-entropy loss.

Accepted as a long survey paper at ACM CSUR 2021

Gradient-Based Attacks

In gradient-based attacks, adversarial perturbations are generated in the direction of the gradient, i.e., in
the direction with the maximum effect on the classiﬁer’s output (e.g., FGSM; Equation 4). Gradient-based
attacks are effective but require adversarial knowledge about the targeted classiﬁer’s gradients. Such attacks
require knowledge about the architecture of the target classiﬁer and are therefore white-box attacks.

When dealing with malware classiﬁcation tasks, differentiating between malicious ( f (x) = 1) and benign
( f (x) = −1), as done by SVM, Biggio et al. (2013) suggested solving Equation 1 using gradient ascent. To
minimize the size of the perturbation and maximize the adversarial effect, the white-box perturbation should
follow the gradient direction (i.e., the direction providing the greatest increase in model value, from one
label to another). Therefore, the perturbation r in each iteration is calculated as:

r = ε∇xLoss f (x + r, −1) s.t. f (x) = 1

(2)

where ε is a parameter controlling the magnitude of the perturbation introduced. By varying ε, this method
can ﬁnd an adversarial sample x + r.

Szegedy et al. (2014) views the (white-box) adversarial problem as a constrained optimization problem,
i.e., ﬁnd a minimum perturbation in the restricted sample space. The perturbation is obtained by using
box-constrained L-BFGS to solve the following equation:

argr min(d ∗ |r| + Loss f (x + r, l)) s.t. x + r ∈ D

(3)

where d is a term added for the Lagrange multiplier.

Goodfellow et al. (2015) introduced the white-box FGSM attack. This method optimizes over the L∞
norm (i.e., reduces the maximum perturbation on any input feature) by taking a single step to each element
of r in the direction opposite the gradient. The intuition behind this attack is to linearize the cost function
Loss() used to train a model f around the neighborhood of the training point x with a label l that the adversary
wants to force the misclassiﬁcation of. Under this approximation:

r = εsign(∇xLoss f (x, l))

(4)

Kurakin et al. (2016) extended this method with the iterative gradient sign method (iGSM). As its name
suggests, this method applies FGSM iteratively and clips pixel values of intermediate results after each step
to ensure that they are close to the original image (the initial adversarial example is the original input):

x(cid:48)

n+1 = Clip (cid:8)x(cid:48)

n + εsign(∇xLoss f (x(cid:48)

n, l))(cid:9)

(5)

The white-box Jacobian-based saliency map approach (JSMA) was introduced by Papernot et al. (2016b).
This method minimizes the L0 norm by iteratively perturbing features of the input which have large adver-
sarial saliency scores. Intuitively, this score reﬂects the adversarial goal of taking a sample away from its
source class and moves it towards a chosen target class.

First, the adversary computes the Jacobian of the model:

where component (i, j) is the deriva-

(cid:104) ∂ f j
∂ xi

(cid:105)
(x)

i, j

tive of class j with respect to input feature i. To compute the adversarial saliency map, the adversary then
computes the following for each input feature i:

Accepted as a long survey paper at ACM CSUR 2021

S(x,t)[i] =


0

∂ ft (x)
∂ xi



i f ∂ ft (x)
∂ xi
|∑ j(cid:54)=t

∂ f j(x)
∂ xi

> 0

< 0 or ∑ j(cid:54)=t
∂ f j(x)
|
∂ xi

otherwise

(6)

where t is the target class that the adversary wants the machine learning model to assign. The adversary then
selects the input feature i with the highest saliency score S(x,t)[i] and increases its value. This process is
repeated until misclassiﬁcation in the target class is achieved or the maximum number of perturbed features
has been reached. This attack creates smaller perturbations with a higher computing cost than the attack
presented in Goodfellow et al. (2015).

The Carlini-Wagner (C&W) attack Carlini and Wagner (2017b) formulates the generation of adversarial
examples as an optimization problem: ﬁnd some small change r that can be made to an input x + r that will
change its classiﬁcation, such that the result is still in the valid range. They instantiate the distance metric
with an Lp norm (e.g., can either minimize the L2, L0 or L1 distance metric), deﬁne the cost function Loss()
such that Loss(x + r) ≥ 0 if and only if the model correctly classiﬁes x + r (i.e., gives it the same label that
it gives x), and minimize the sum with a trade-off constant c which is chosen by modiﬁed binary search:

argr min (cid:0)||r||p + c ∗ Loss f (x + r,t)(cid:1) s.t. x + r ∈ D

(7)

where the cost function Loss() maximizes the difference between the target class probability and the class
with the highest probability. It is deﬁned as:

max (cid:0)argi(cid:54)=t max( f (x + r, i)) − f (x + r,t), −k(cid:1)

(8)

where k is a constant to control the conﬁdence.

Moosavi-Dezfooli et al. (2016) proposed the DeepFool adversarial method to ﬁnd the closest distance
from the original input to the decision boundary of adversarial examples. DeepFool is an untargeted attack
technique optimized for the L2 distance metric. An iterative attack by linear approximation is proposed in
order to overcome the nonlinearity at a high dimension. If f is a binary differentiable classier, an iterative
method is used to approximate the perturbation by considering that f is linearized around x + r in each
iteration. The minimal perturbation is provided by:

argr min (||r||2) s.t. f (x + r) + ∇x f (x + r)T ∗ r = 0

(9)

This result can be extended to a more general Lp norm, p ∈ [0, ∞).

Madry et al. (2018) proposed a projected gradient descent (PGD) based adversarial method to gener-
ate adversarial examples with minimized empirical risk and the trade-off of a high perturbation cost. The
model’s empirical risk minimization (ERM) is deﬁned as E(x, y)∼D[Loss(x, y, θ )], where x is the original
sample, and y is the original label. By modifying the ERM deﬁnition by allowing the adversary to perturb the
input x by the scalar value S, ERM is represented by minθ ρ(θ ) : ρ(θ ) = E(x, y)∼D[maxδ ∈SLoss(x + r, y, θ )],
where ρ(θ ) denotes the objective function. Note that x + r is updated in each iteration.

Chen et al. (2018a) presented the elastic net adversarial method (ENM). This method limits the total
absolute perturbation across the input space, i.e., the L1 norm. ENM produces the adversarial examples by
expanding an iterative L2 attack with an L1 regularizer.

Accepted as a long survey paper at ACM CSUR 2021

Papernot et al. (2016c) presented a white-box adversarial example attack against RNNs. The adversary

iterates over the words x[i] in the review and modiﬁes it as follows:

x[i] = arg min

z

||sign(x[i] − z) − sign(J f (x)[i, f (x)])|| s.t. z ∈ D

(10)

where f (x) is the original model label for x, and J f (x)[i, j] = ∂ f j
(x). sign(J f (x)[i, f (x)]) provides the direc-
∂ xi
tion one has to perturb each of the word embedding components in order to reduce the probability assigned
to the current class and thus change the class assigned to the sentence. However, the set of legitimate word
embeddings is ﬁnite. Thus, one cannot set the word embedding coordinates to any real value. Instead, one
ﬁnds the word z in dictionary D such that the sign of the difference between the embeddings of z and the
original input word is closest to sign(J f (x)[i, f (x)]). This embedding considers the direction closest to the
one indicated by the Jacobian as most impactful on the model’s prediction. By iteratively applying this
heuristic to a word sequence, an adversarial input sequence misclassiﬁed by the model is eventually found.

Score-Based Attacks

Score-based attacks are based on knowledge of the target classiﬁer’s conﬁdence score. Therefore, these are
gray-box attacks.

The zeroth-order optimization (ZOO) attack was presented in Chen et al. (2017b). ZOO uses hinge loss

in Equation 8:

max (cid:0)argi(cid:54)=t max(log ( f (x + r, i))) − log ( f (x + r,t)) , −k(cid:1)

(11)

where the input x, correctly classiﬁed by the classiﬁer f , is perturbed with r, such that the resulting adver-
sarial example is x + r.

ZOO uses the symmetric difference quotient to estimate the gradient and Hessian:

∂ f (x)
∂ xi

≈

f (x + h ∗ ei) − f (x − h ∗ ei)
2h

∂ 2 f (x)
∂ x2
i

≈

f (x + h ∗ ei) − 2 f (x) + f (x − h ∗ ei)
h2

(12)

(13)

where ei denotes the standard basis vector with the i-th component as 1, and h is a small constant.

Using Equations 11 and 12, the target classiﬁer’s gradient can be numerically derived from the conﬁdence
scores of adjacent input points, and then a gradient-based attack is applied, in the direction of maximum
impact, in order to generate an adversarial example.

Decision-Based Attacks

Decision-based attacks only use the label predicted by the target classiﬁer. Thus, these are black-box attacks.

Accepted as a long survey paper at ACM CSUR 2021

Generative Adversarial Network (GAN)

An adversary can try to generate adversarial examples based on a GAN, a generative model introduced in
Goodfellow et al. (2014). A GAN is designed to generate fake samples that cannot be distinguished from the
original samples. A GAN is composed of two components: a discriminator and a generator. The generator
is a generative neural network used to generate samples. The discriminator is a binary classiﬁer used to
determine whether the generated samples are real or fake. The discriminator and generator are alternately
trained so that the generator can generate valid adversarial records. Assuming we have the original sample set
x with distribution pr and input noise variables z with distribution pz, G is a generative multilayer perception
function with parameter g that generates fake samples G(z); another model D is a discriminative multilayer
perception function that outputs D(x), which represents the probability that model D correctly distinguishes
fake samples from the original samples. D and G play the following two player minimax game with the
value function V (G; D):

min
G

max
D

V (G, D) = E

[log (D(X))] + E

Z∼pz

[log (1 − D(G(Z))]

(14)

X∼pr

In this competing fashion, a GAN is capable of generating raw data samples that look close to the real

data.

The Transferability Property

Many black-box attacks presented in this paper (e.g., Rosenberg et al. (2018); Sidi et al. (2019); Yang
et al. (2018)) rely on the concept of adversarial example transferability presented in Szegedy et al. (2014):
Adversarial examples crafted against one model are also likely to be effective against other models. This
transferability property holds even when the models are trained on different datasets. This means that the
adversary can train a surrogate model, which has similar decision boundaries as the original model and
perform a white-box attack on it. Adversarial examples that successfully fool the surrogate model would
most likely fool the original model as well (Papernot et al. (2017)).

The transferability between DNNs and other models, such as decision tree and SVM models, was exam-
ined in Papernot et al. (2016a). A study of the transferability property using large models and a large-scale
dataset was conducted in Liu et al. (2017), which showed that while transferable non-targeted adversarial
examples are easy to ﬁnd, targeted adversarial examples rarely transfer with their target labels. However, for
binary classiﬁers (commonly used in the cyber security domain), targeted and non-targeted attacks are the
same.

The reasons for the transferability are unknown, but a recent study Ilyas et al. (2019) suggested that
adversarial vulnerability is not “necessarily tied to the standard training framework but is rather a property
of the dataset (due to representation learning of non-robust features)”; this also clariﬁes why transferability
happens regardless of the classiﬁer architecture. This can also explain why transferability is applicable to
training phase attacks (e.g., poisoning attacks) as well (Mu˜noz Gonz´alez et al. (2017)).

4 Taxonomy

Adversarial learning in cyber security is the modeling of non-stationary adversarial settings like spam ﬁlter-
ing or malware detection, where a malicious adversary can carefully manipulate (or perturb) the input data,
exploiting speciﬁc vulnerabilities of learning algorithms in order to compromise the (targeted) machine
learning system’s security.

Accepted as a long survey paper at ACM CSUR 2021

Figure 1: Chronological overview of the taxonomy

A taxonomy for the adversarial domain in general exists (e.g., Barreno et al. (2010)) and inspired our
taxonomy. However, the cyber security domain has a few unique challenges, described in the previous
section, necessitating a different taxonomy to categorize the existing attacks, with several new parts, e.g.,
the attack’s output, attack’s targeting, and perturbed features.

Our proposed taxonomy is shown in Figure 1. The attacks are categorized based on seven distinct attack

characteristics, which are sorted by four chronological phases of the attack:

1) Threat Model - The attacker’s knowledge and capabilities, known prior to the attack. The threat model

includes the training set access and attacker’s knowledge.

2) Attack Type - These characteristics are a part of the attack implementation. The attack type includes

the attacker’s goals, the targeted phase, and the attack’s targeting.

3) The features modiﬁed (or perturbed) by the attack.
4) The attack’s output.
A more detailed overview of our proposed taxonomy, including possible values for the seven characteris-
tics, is shown in Figure 2. The seven attack characteristics (attacker’s goals, attacker’s knowledge, attacker’s
training set access, targeted phase, attack’s targeting, perturbed features, and attack’s output) are described
in the subsections that follow.

We include these characteristics in our taxonomy for the following reasons:
1) These characteristics are speciﬁc to the cyber domain (e.g., perturbed features and attack’s output).
2) These characteristics are especially relevant to the threat model, which plays a much more critical
role in the cyber security domain, where white-box attack are less valuable than in other domains, since
the knowledge of adversaries in the cyber security domain about the classiﬁer architecture is usually very
limited (e.g., attacker’s knowledge, the attacker’s training set access, and the targeted phase).

3) These characteristics highlight missing research in the cyber security domain, which exists in other

domains of adversarial learning. Such research is speciﬁed in Section 7 7 (e.g., attack’s targeting).

4) These characteristics exist in many domains but have a different emphasis (and are therefore more
important) in the cyber security domain (for example, if we analyze the attacker’s goal characteristic, avail-
ability attacks are of limited use in other domains, but they are very relevant in the cyber security domain).

4.1 Attacker’s Goals

This characteristic of the attack is sometimes considered part of the attack type. An attacker aims to achieve
one or more of the following goals (a.k.a. the CIA triad): (1) Conﬁdentiality - Acquire private information
by querying the machine learning system, e.g., stealing the classiﬁer’s model (Tram`er et al. (2016)), (2)
Integrity - Cause the machine learning system to perform incorrectly for some or all input; for example,
to cause a machine learning-based malware classiﬁer to misclassify a malware sample as benign (Srndic
and Laskov (2014)), and (3) Availability - Cause a machine learning system to become unavailable or block
regular use of the system; for instance, to generate malicious sessions which have many of the features of

Accepted as a long survey paper at ACM CSUR 2021

Figure 2: Detailed overview of the taxonomy

regular trafﬁc, causing the system to classify legitimate trafﬁc sessions as malicious and block legitimate
trafﬁc (Chung and Mok (2006)).

4.2 Attacker’s Knowledge

This attack characteristic is sometimes considered part of the threat model. Attacks vary based on the amount
of knowledge the adversary has about the classiﬁer he/she is trying to subvert: (1) Black-Box attack - Re-
quires no knowledge about the model beyond the ability to query it as a black-box (a.k.a. the oracle model),
i.e., inserting an input and obtaining the output classiﬁcation, (2) Gray-Box attack - Requires some (limited)
degree of knowledge about the targeted classiﬁer. While usually this consists of the features monitored by
the classiﬁer, sometimes it is other incomplete pieces of information like the output of the hidden layers of
the classiﬁer or the conﬁdence score (and not just the class label) provided by the classiﬁer, (3) White-Box
attack - The adversary has knowledge about the model architecture and even the hyperparameters used to
train the model, and (4) Transparent-Box attack - In this case, the adversary has complete knowledge about
the system, including both white-box knowledge and knowledge about the defense methods used by de-
fender (see Section 7.6). Such knowledge can assist the attacker in choosing an adaptive attack that would
be capable of bypassing the speciﬁc defense mechanism (e.g., Tramer et al. (2020)).

While white-box attacks tend to be more efﬁcient than black-box attacks (sometimes by an order of

Accepted as a long survey paper at ACM CSUR 2021

magnitude Rosenberg and Gudes (2016)), the knowledge required is rarely available in real-world use cases.
However, white-box knowledge can be gained either through internal knowledge or by using a staged attack
to reverse engineer the model beforehand Tram`er et al. (2016). Each type of attack (black-box, gray-box,
etc.) has a query-efﬁcient variant in which the adversary has only a limited number of queries (in each
query the adversary inserts input into the classiﬁer and obtains its classiﬁcation label), and not an unlimited
amount of queries, as in the variants mentioned above. A query-efﬁcient variant is relevant in the case of
cloud security services (e.g., Rosenberg et al. (2020b)). In such services, the attacker pays for every query
of the target classiﬁer and therefore aims to minimize the number of queries made to the cloud service when
performing an attack. Another reason for minimizing the number of queries is that many queries from the
same computer might arouse suspicion of an adversarial attack attempt, causing the cloud service to stop
responding to those queries. Such cases require query-efﬁcient attacks.

4.3 Attacker’s Training Set Access

Another important characteristic of an attack, sometimes considered part of the threat model, is the access
the adversary has to the training set used by the classiﬁer (as opposed to access to the model itself, mentioned
in the previous subsection). The attacker’s training set access is categorized as follows: (1) None - no access
to the training set, (2) Read data from the training set (entirely or partially), (3) Add new samples to the
training set, and (4) Modify existing samples (modifying either all features or just speciﬁc features, e.g., the
label). For instance, poisoning attacks require add or modify permissions.

4.4 Targeted Phase

This attack characteristic is sometimes considered part of the attack type. Adversarial attacks against ma-
chine learning systems occur in two main phases of the machine learning process: (1) Training Phase attack
- This attack aims to introduce vulnerabilities (to be exploited in the classiﬁcation phase) by manipulat-
ing training data during the training phase. For instance, a poisoning attack can be performed by inserting
crafted malicious samples labeled as benign to the training set as part of the baseline training phase of a
classiﬁer, (2) Inference Phase attack - This attack aims to ﬁnd and subsequently exploit vulnerabilities in
the classiﬁcation phase. In this phase, the attacker modiﬁes only samples from the test set. For example, an
evasion attack involves modifying the analyzed malicious sample’s features in order to evade detection by
the model. Such inputs are called adversarial examples.

Note that attacks on online learning systems (for instance, anomaly detection systems Clements et al.
(2019)) combine both training phase and inference phase attacks: the attack is an evasion attack, but if it
succeeds, the classiﬁer learns that this trafﬁc is legitimate, making additional such attacks harder to detect
by the system (i.e., there is a poisoning effect). Such attacks would be termed inference attacks in this paper,
since in this case, the poisoning aspect is usually a by-product, and is not the attacker’s main goal. Moreover,
even if the poisoning aspect is important to the attacker, it would usually be successful only if the evasion
attack works, so evasion is the primary goal of the attacker in any case.

4.5 Attack’s Targeting

This characteristic is sometimes considered part of the attack type. Each attack has a different targeting,
deﬁning the trigger conditions or the desired effect on the classiﬁer: (1) Label-Indiscriminate attack - Always
minimizes the probability of correctly classifying a perturbed sample (the adversarial example), (2) Label-
Targeted attack - Always maximizes the probability of a speciﬁc class to be predicted for the adversarial

Accepted as a long survey paper at ACM CSUR 2021

example (different from the predicted class for the unperturbed sample), and (3) Feature-Targeted attack -
The malicious behavior of these attacks are only activated by inputs stamped with an attack trigger, which
might be the existence of a speciﬁc input feature or group of feature values in the adversarial example.

Attacks can be both feature and label targeted. Note that in the cyber security domain, many classiﬁers
are binary (i.e., they have two output classes: malicious and benign, spam and ham, anomalous or not, etc.).
For binary classiﬁers, label-indiscriminate and label-targeted attacks are the same, because in these cases,
minimizing the probability of the current class (label-indiscriminate attack) is equivalent to maximizing the
probability of the only other possible class.

4.6 Perturbed Features

As mentioned in Section 2.3, in the cyber security domain, classiﬁers and other machine learning systems
often use more than one feature type. Thus, attackers who want to subvert those systems should consider
modifying more than a single feature type. We can therefore characterize the different adversarial attacks in
the cyber security domain by the features being modiﬁed/perturbed or added. Note that the same feature type
might be modiﬁed differently depending on the sample’s format. For instance, modifying a printable string
inside a PE ﬁle might be more challenging than modifying a word within the content of an email content,
although the feature type is the same. Thus, this classiﬁcation is not simply a feature type but a tuple of
feature type and sample format (for instance, printable strings inside a PE ﬁle). The following is a partial
list (e.g., the work of Rosenberg et al. (Rosenberg et al., 2020a) contains 2,381 features, so the full list cannot
be included) of such tuples used in the papers reviewed in our research: PCAP (packet capture; part of a
network session) statistical features (e.g., number of SYN requests in a certain time window), PCAP header
(e.g., IP or UDP) ﬁelds, PE header ﬁelds, printable strings inside a PE ﬁle, binary bytes inside a PE ﬁle, PE
executed API calls (during a dynamic analysis of the PE ﬁle), and words inside an email or characters inside
a URL.

4.7 Attack’s Output

As discussed in Section 2.1, in contrast to image-based attacks, most adversarial attacks in the cyber domain
require the modiﬁcation of a feature’s values. While in some domains, such as spam detection, modifying
a word in an email is non-destructive, modifying, e.g., a ﬁeld in a PE header metadata, might result in an
unrunnable PE ﬁle. Thus, there are two type of attacks: (1) Feature-Vector attack - Such attacks obtain
a feature vector as an input and output another perturbed feature vector. However, such an attack doesn’t
generate a sample which can be used by the attacker and is usually only a hypothetical attack that would not
be possible in real life, and (2) End-to-End attack - This attack generates a functional sample as an output.
Thus, this is a concrete real-life attack. This category is further divided into many subgroups based on the
sample type produced, e.g., a valid and runnable PE ﬁle, a phishing URL, a spam email, etc.

For instance, most trafﬁc anomaly detection attacks reviewed in this paper are feature vector attacks.
They use statistical features aggregating packet metadata, but the authors do not show how to generate the
perturbed packet. In contrast, the attack used by Rosenberg et al. (2018) to add API calls to a malicious
process uses a custom framework that generates a new binary that adds those API calls. Thus, this is an
end-to-end attack. In some image-based domains, e.g., face recognition systems (Section 5.6.1), end-to-
end attacks can be further categorized as those that generate images (e.g., Liu et al. (2018)) and those that
generate physical elements that can be used to generate multiple relevant images (e.g., Sharif et al. (2016)).

Accepted as a long survey paper at ACM CSUR 2021

5 Adversarial Attacks in the Cyber Security Domain

Our paper addresses adversarial attacks in the cyber security domain. An overview of this section is provided
in Tables 1-7. Target classiﬁer abbreviations are speciﬁed in Appendix A. The attack type includes the
attacker’s goals, the targeted phase, and the attack’s targeting. The threat model includes the attacker’s
knowledge and training set access. Unless otherwise mentioned, a gray-box attack requires knowledge of
the target classiﬁer’s features, the attack’s targeting is label-indiscriminate, and the attacker’s training set
access is none. Some of the columns are not a part of our taxonomy (Section 4) but provide additional
relevant information that may be helpful for understanding the attacks, such as the target classiﬁers.

Each of the following subsections represents a speciﬁc cyber security domain that uses adversarial learn-
ing and discusses the adversarial learning methods used in this domain. While there are other domains in
cyber security, we focused only on domains in which substantial adversarial learning research has been per-
formed. Due to space limits, this review paper covers only the state of the art in the abovementioned areas
and not all adversarial attacks, especially in large and diverse domains, such as biometric or cyber-physical
systems. The strengths and weaknesses of each adversarial attack are analyzed throughout this section.

For the reader’s convenience, we have summarized the analysis in Tables 1-7 using the following Boolean

parameters:

(1) Reasonable attacker knowledge – Is the attack a gray-box or black-box attack, both of which re-
quire a reasonable amount of knowledge (+ value in Tables 1-7) or a white-box attack, which requires an
unreasonable amount of knowledge (- value in Tables 1-7) in the cyber security domain?

(2) End-to-end attack – Does the attack have an end-to-end attack output (deﬁned in Section 4.7)? Such
attacks are considered more feasible attacks (+ value in Tables 1-7), while feature vector attacks (deﬁned in
Section 4.7) are considered less feasible (- value in Tables 1-7).

(3) Effective attack – Is the attack effective (success rate greater than 90%) for the attack use case (+

value in Tables 1-7), or ineffective (success rate lower than 90%; - value in Tables 1-7)?

(4) Representative dataset - Is the dataset used representative of the relevant threats in the wild (+ value

in Tables 1-7), or is it just a subset (or old variants) of those threats (- value in Tables 1-7)?

(5) Representative features - Are the features used in the classiﬁers being attacked similar to those being

used in real-life security products (+ value in Tables 1-7) or not (- value in Tables 1-7)?

The mathematical background for the deep learning classiﬁers is provided in Appendix A, and the math-
ematical background for the commonly used adversarial learning attacks in the cyber security domain is
provided in Section 3.

Note that while the classiﬁers the attacker tries to subvert are mentioned brieﬂy in order to provide
context helpful for understanding the attack, a complete list of the state-of-the-art prior work is not provided
due to space limits. A more comprehensive list can be found, e.g., in Berman et al. (2019). Cases in
which an adversarial attack does not exist for a speciﬁc application type are omitted. This paper also does
not review adversarial attacks in non-cyber domains, such as image recognition (with the exception of the
face recognition domain which is addressed in Section 5.6.1, which is cyber related). It also does not cover
papers related to cyber security, but not to adversarial learning, such as the use of machine learning to bypass
CAPTCHA.

5.1 Malware Detection and Classiﬁcation

Next generation antivirus (NGAV) products, such as Cylance, CrowdStrike, SentinelOne, and Microsoft
ATP use machine and deep learning models instead of signatures and heuristics, allowing them to detect
unseen and unsigned malware but also leaving them open to attacks against such models.

Accepted as a long survey paper at ACM CSUR 2021

Malware classiﬁers can either use static features gathered without running the code (e.g., n-gram byte se-
quence, strings, or structural features of the inspected code) or dynamic features (e.g., CPU usage) collected
during the inspected code execution.

While using static analysis provides a performance advantage, it has a major disadvantage: since the
code is not executed, the analyzed code might not reveal its “true features.” For example, when looking
for speciﬁc strings in the ﬁle, one might not be able to catch polymorphic malware, in which those features
are either encrypted or packed, and decrypted only during runtime by a speciﬁc bootstrap code. Fileless
attacks (code injection, process hollowing, etc.) are also a problem for static analysis. Thus, dynamic
features, extracted at runtime, can be used. The most prominent dynamic features that can be collected
during malware execution are the sequences of API calls (Kolbitsch et al. (2009)), particularly those made to
the OS, which are termed system calls. Those system calls characterize the software behavior and are harder
to obfuscate during execution time without harming the functionality of the code. The machine learning
techniques (and thus the attacks of them) can be divided into two groups: traditional (or shallow) machine
learning and deep learning techniques. A summary of the attacks in the malware detection sub-domain is
shown in Tables 1 and 2.

5.1.1 Attacking Traditional (Shallow) Machine Learning Malware Classiﬁers

Srndic and Laskov (2014) implemented an inference integrity gray-box evasion attack against PDFRATE,
a random forest classiﬁer for static analysis of malicious PDF ﬁles, using PDF structural features, e.g., the
number of embedded images or binary streams within the PDF. The attack used either a mimicry attack in
which features were added to the malicious PDF to make it “feature-wise similar” to a benign sample, or
created an SVM representation of the classiﬁer and subverted it using a method that follows the gradient of
the weighted sum of the classiﬁer’s decision function and the estimated density function of benign examples.
This ensures that the ﬁnal result lies close to the region populated by real benign examples. The density
function must be estimated beforehand, using the standard technique of kernel density estimation, and then
the transferability property is used to attack the original PDFRATE classiﬁer using the same PDF ﬁle. Li
et al. (2020b) performed an inference integrity gray-box attack against the same classiﬁer by using GAN-
generated feature vectors and transforming them back into PDF ﬁles. The main advantage of these attacks
is the fact that they are end-to-end attacks that produce a valid (malicious) PDF ﬁle, which evade detection.
The main problem is that very few malware have PDF ﬁle type. PE ﬁles, which are more common, were not
covered in this work.

Ming et al. (2015) used an inference integrity replacement attack, replacing API calls with different
functionality-preserving API subsequences (so gray-box knowledge about the monitored APIs is required)
to modify the malware code. They utilized a system-call dependence graph (SCDG) with the graph edit
distance and Jaccard index as clustering parameters of different malware variants and used several SCDG
transformations on their malware source code to move it to a different cluster. Their transformations can
cause similar malware variants to be classiﬁed as a different cluster, but they didn’t show that the attack
can cause malware to be classiﬁed (or clustered) as a benign program, which is usually the attacker’s main
goal. Xu et al. (2020) also implemented an inference integrity gray-box attack against a SCDG-based APK
malware classiﬁer, using n-strongest nodes and FGSM (see Section 3) methods. The main advantage of
these attacks is the fact that they are end-to-end attacks that produce a valid (malicious) binary (PE or APK,
respectively) ﬁle, which evade detection. The main problem is that the features used (SCDG) are not used
by real-life NGAV products.

Suciu et al. (2018b) and Chen et al. (2018b) used a training integrity poisoning attack against a linear
SVM classiﬁer trained on the Drebin dataset Arp et al. (2014) for Android malware detection. This attack

Accepted as a long survey paper at ACM CSUR 2021

Table 1: Summary of Adversarial Learning Approaches in Malware Detection (Part 1)

?
e
g
d
e
l
w
o
n
k

r
e
k
c
a
t
t
a

e
l
b
a
n
o
s
a
e
R

?
k
c
a
t
t
a
d
n
e
-
o
t
-
d
n
E

?
k
c
a
t
t
a

e
v
i
t
c
e
f
f
E

Citation

Year

Target
Classi-
ﬁer

Attack
Type

Attack’s
Output

Threat
Model

Perturbed
Features

Srndic and
Laskov
(2014); Li
et al.
(2020b)

Ming et al.
(2015)

Suciu et al.
(2018b)

2020

RF

Inference
integrity

PDF ﬁle
(end-to-
end)

Gray-
box

+

+

+

Static
structural
PDF
features

2015

SCDG

2018

SVM

Inference
integrity

Training
integrity

PE ﬁle
(end-to-
end)

Feature
vector

+

+

Gray-
box

Executed
API calls

+

+

-

-

Static
Android
manifest
features

Gray-
box;
add
train-
ing
set
ac-
cess

?
t
e
s
a
t
a
d

e
v
i
t
a
t
n
e
s
e
r
p
e
R

-

-

-

?
s
e
r
u
t
a
e
f

e
v
i
t
a
t
n
e
s
e
r
p
e
R

+

-

+

Dang et al.
(2017)

2017

SVM,
RF

Inference
integrity

Anderson
et al. (2018)

2018 GBDT

Inference
integrity

PDF ﬁle
(end-to-
end)

PE ﬁle
(end-to-
end)

Query-
efﬁcient
gray-
box

Static
structural
PDF
features

Black-
box

+

+

+

-

+

+

+

-

+

+

-

+

-

-

+

+

-

-

+

-

Operations
(e.g.,
packing)
performed
on a PE
ﬁle

Static
Android
manifest
features

Static
Android
manifest
features

Grosse et al.
(2017)

2017

FC
DNN

Inference
integrity

Feature
vector

White-
box

Xu et al.
(2020)

2020

SCDG

Inference
integrity

Feature
vector

Gray-
box

Kreuk et al.
(2018b);
Kolosnjaji
et al. (2018)

2018

1D
CNN

Inference
integrity

PE ﬁle
(end-to-
end)

White-
box

PE ﬁle’s
raw bytes

-

+

+

+

+

Accepted as a long survey paper at ACM CSUR 2021

Table 2: Summary of Adversarial Learning Approaches in Malware Detection (Part 2)

?
e
g
d
e
l
w
o
n
k

r
e
k
c
a
t
t
a

e
l
b
a
n
o
s
a
e
R

+

+

+

+

+

-

+

+

?
t
e
s
a
t
a
d

e
v
i
t
a
t
n
e
s
e
r
p
e
R

+

+

-

-

-

-

-

+

?
k
c
a
t
t
a

e
v
i
t
c
e
f
f
E

+

+

+

+

+

+

+

+

?
k
c
a
t
t
a
d
n
e
-
o
t
-
d
n
E

+

+

-

+

-

-

-

+

?
s
e
r
u
t
a
e
f

e
v
i
t
a
t
n
e
s
e
r
p
e
R

+

+

-

+

+

-

+

+

+

+

+

+

+

Citation

Year

Target
Classiﬁer

2018

1D CNN

Attack
Type

Inference
integrity

2020

GBDT, FC
DNN

Inference
integrity

Attack’s
Output

PE ﬁle
(end-to-
end)

PE ﬁle
(end-to-
end)

Threat
Model

Black-
box

Perturbed
Features

PE ﬁle’s
raw bytes

Black-
box

PE header
metadata

Suciu
et al.
(2018a)

Rosenberg
et al.
(2020a);
Rosen-
berg and
Meir
(2020)

Hu and
Tan
(2017b)

Abusnaina
et al.
(2019)

Hu and
Tan
(2017a)

Rosenberg
et al.
(2018)

2018

2018

Rosenberg
et al.
(2020b)

2017

RF, LR, DT,
SVM, MLP

Inference
integrity

Feature
vector

Gray-
box

API calls’
unigrams

Xu et al.
(2016)

2016 SVM, RF, CNN Inference
integrity

PDF ﬁle
(end-to-
end)

Gray-
box

Liu et al.
(2019)

2019

FC DNN, LR,
DT, RF

Inference
integrity

Feature
vector

Gray-
box

Static
structural
PDF
features

Static
Android
manifest
features

2019

CNN

Inference
integrity

Feature
vector

White-
box

CFG
features

2017

LSTM

Inference
integrity

Feature
vector

Gray-
box

Executed
API calls

LSTM, GRU,
FC DNN, 1D
CNN, RF,
SVM, LR,
GBDT

LSTM, GRU,
FC DNN, 1D
CNN, RF,
SVM, LR,
GBDT

Inference
integrity

PE ﬁle
(end-to-
end)

Gray-
box

Executed
API calls,
printable
strings

Inference
integrity

PE ﬁle
(end-to-
end)

Query-
efﬁcient
gray-
box

Executed
API calls,
printable
strings

Accepted as a long survey paper at ACM CSUR 2021

requires gray-box knowledge of the classiﬁer’s features and training set add access. The poisoning was done
by adding static features (permissions, API calls, URL requests) from the target to existing benign instances.
Mu˜noz Gonz´alez et al. (2017) used a training integrity poisoning attack against logistic regression, MLP, and
ADALINE classiﬁers, for spam and ransomware detection, by using back-gradient optimization. This attack
requires gray-box knowledge of the classiﬁer’s features and training set add and read access. A substitute
model is built and poisoned, and the poisoned samples are effective against the target classiﬁer as well, due
to the transferability property. The main problem with these poisoning attacks is that they require a powerful
attacker who is able to inject samples into the training set of the malware classiﬁer. While such a scenario is
possible in some cases (e.g., supply chain attacks), this is usually not a common case, making such attacks
less feasible.

Dang et al. (2017) utilized the rate of feature modiﬁcations from a malicious sample and a benign known
sample as the score and used a hillclimbing approach to minimize this score, bypassing SVM and random
forest PDF malware classiﬁers based on static features in a query-efﬁcient manner. Thus, their inference
integrity attack is a query-efﬁcient gray-box attack. This attack was the ﬁrst attempt to perform a query-
efﬁcient attack in the cyber security domain. However, the classiﬁers bypassed were not state-of-the-art
deep classiﬁers.

In Anderson et al. (2018, 2017), the features used by the gradient boosted decision tree classiﬁer included
PE header metadata, section metadata, and import/export table metadata. In Anderson et al. (2018, 2017),
an inference integrity black-box attack which trains a reinforcement learning agent was presented. The
agent is equipped with a set of operations (such as packing) that it may perform on the PE ﬁle. The reward
function was the evasion rate. Through a series of games played against the target classiﬁer, the agent
learns which sequences of operations are likely to result in detection evasion for any given malware sample.
The perturbed samples that bypassed the classiﬁer were uploaded to VirusTotal and scanned by 65 anti-
malware products. Those samples were detected as malicious by 50% of anti-malware products that detected
the original unperturbed samples. This means that this attack works against real-world security products
(although the authors did not mention which ones were affected). However, unlike other attacks, this attack’s
effectiveness is less than 25% (as opposed to 90% for most other adversarial attacks), showing that additional
research is needed in order for this approach to be practical in real-life use cases.

5.1.2 Attacking Deep Neural Network Malware Classiﬁers

Rosenberg et al. (2020a); Rosenberg and Meir (2020) used the EMBER dataset and PE structural features
(see Table 8) to train a substitute FC DNN model and used explainability machine learning algorithms (e.g.,
integrated gradients) to detect which of the 2,381 features have high impact on the malware classiﬁcation and
can also be modiﬁed without harming the executable’s functionality (e.g., ﬁle timestamp). These features
were modiﬁed in a gray-box inference integrity attack, and the mutated malware bypassed not only the
substitute model, but also the target GBDT classiﬁer, which used a different subset of samples and features.
The main advantage of these attacks are that they bypassed an actual, targeted, real-life NGAV. The main
limitation of this attack is that it is not fully automatic - human intervention is required to select the features
that are perturbed.

Grosse et al. (2017); Grosse et al. (2016) presented a white-box inference integrity attack against an
Android static analysis fully connected DNN malware classiﬁer. The static features used in the DREBIN
dataset (see Table 8) were from the AndroidManifest.xml ﬁle and included permissions, suspicious API
calls, activities, etc. The attack is a discrete FGSM (see Section 3) variant, which is performed iteratively
in the following two steps until a benign classiﬁcation is made: (1) compute the gradient of the white-box
model with respect to the binary feature vector x, and (2) ﬁnd the element in x whose modiﬁcation from zero

Accepted as a long survey paper at ACM CSUR 2021

to one (i.e., only feature addition and not removal) would cause the maximum change in the benign score and
add this feature to the adversarial example. The main advantage of this attack is that it provides a methodical
way of dealing with discrete features, commonly used in the cyber security domain, and evaluates this attack
against many DNN architectures. The main problem is that the white-box assumption is unrealistic in many
real-life scenarios.

Kreuk et al. (2018b) implemented an inference integrity attack against MalConv, a 1D CNN, using the
ﬁle’s raw byte content as features (Raff et al. (2018)). The additional bytes are selected by the FGSM method
(see Section 3) and are inserted between the ﬁle’s sections. Kolosnjaji et al. (2018) implemented a similar
attack and also analyzed the bytes which are the most impactful features (and are therefore added by the
attack), showing that a large portion of them are part of the PE header metadata. Suciu et al. (2018a) trans-
formed this white-box gradient-based attack to a black-box decision-based attack by appending bytes from
the beginning of benign ﬁles, especially from their PE headers, which, as shown in Kolosnjaji et al. (2018),
are prominent features. The main insight of these attacks is that even classiﬁers that use raw memory bytes
as features (leveraging deep architecture’s representation learning) are vulnerable to adversarial examples.
The main disadvantages is that such classiﬁers are rarely used by real-life NGAVs.

Hu and Tan (2017b) perturbed static API call unigrams by performing a gray-box inference integrity
attack. If n API types are used, the feature vector dimension is n. A generative adversarial network (GAN;
Appendix A) was trained, where the discriminator simulates the malware classiﬁer while the generator tries
to generate adversarial samples that would be classiﬁed as benign by the discriminator, which uses labels
from the black-box model (a random forest, logistic regression, decision tree, linear SVM, MLP, or an
ensemble of all of these). However, this is a feature vector attack: the way to generate an executable with
the perturbed API call trace was not presented, making this attack infeasible in real life.

Xu et al. (2016) generated adversarial examples that bypass PDF malware classiﬁers, by modifying static
PDF features. This was done using an inference integrity genetic algorithm (GA), where the ﬁtness of the
genetic variants is deﬁned in terms of the target classiﬁer’s conﬁdence score. The GA is computationally
expensive and was evaluated against SVM, random forest, and CNN classiﬁers using static PDF structural
features. This gray-box attack requires knowledge of both the classiﬁer’s features and the target classiﬁer’s
conﬁdence score. Liu et al. (2019) used the same approach to bypass an IoT Android malware detector. The
bypassed fully connected DNN, logistic regression, decision tree, and random forest classiﬁers were trained
using the DREBIN dataset.

Abusnaina et al. (2019) trained an IoT malware detection CNN classiﬁer using graph-based features (e.g.,
shortest path, density, number of edges and nodes, etc.) from the control ﬂow graph (CFG) of the malware
disassembly. They used white-box attacks: C&W, DeepFool, FGSM, JSMA (see Section 3), the momentum
iterative method (MIM), projected gradient descent (PGD), and virtual adversarial method (VAM). They also
added their own attack, graph embedding, and augmentation, which adds a CFG of a benign sample to the
CFG of a malicious sample via source code concatenation. The problem with this attack is that CFG takes a
long time to generate, and therefore graph-based features are rarely used by real-life malware classiﬁers.

Hu and Tan (2017a) proposed a gray-box inference integrity attack using an RNN GAN to generate
invalid APIs and inserted them into the original API sequences to bypass an LSTM classiﬁer trained on the
API call trace of the malware. A substitute RNN is trained to ﬁt the targeted RNN. Gumbel-Softmax, a one-
hot continuous distribution estimator, was used to smooth the API symbols and deliver gradient information
between the generative RNN and the substitute RNN. Null APIs were added, but while they were omitted to
make the adversarial sequence generated shorter, they remained in the loss function’s gradient calculation.
This decreases the attack’s effectiveness, since the substitute model is used to classify the Gumbel-Softmax
output, including the null APIs’ estimated gradients, so it does not simulate the malware classiﬁer exactly.
The gray-box attack output is a feature vector of the API call sequence that might harm the malware’s

Accepted as a long survey paper at ACM CSUR 2021

functionality (e.g., by inserting the ExitProcess() API call in the middle of the malware code), making this
attack infeasible in real-life scenarios.

Rosenberg et al. (2018) presented a gray-box inference integrity attack that adds API calls to an API call
trace used as input to an RNN malware classiﬁer, in order to bypass a classiﬁer trained on the API call trace
of the malware. A GRU substitute model was created and attacked, and the transferability property was
used to attack the original classiﬁer. The authors extended their attack to hybrid classiﬁers combining static
and dynamic features, attacking each feature type in turn. The target models were LSTM variants, GRUs,
conventional RNNs, bidirectional and deep variants, and non-RNN classiﬁers (including both feedforward
networks, like fully connected DNNs and 1D CNNs, and traditional machine learning classiﬁers, such as
SVM, random forest, logistic regression, and gradient boosted decision tree). The authors presented an
end-to-end framework that creates a new malware executable without access to the malware source code.
The variety of classiﬁers and the end-to-end framework ﬁts real-life scenarios, but the focus only on strings’
static features is limiting.

A subsequent work (Rosenberg et al. (2020b)) presented two query-efﬁcient gray-box inference integrity
attacks against the same classiﬁers, based on benign perturbations generated using a GAN that was trained
on benign samples. One of the gray-box attack variants requires the adversary to know which API calls
are being monitored, and the other one also requires the conﬁdence score of the target classiﬁer in order to
operate an evolutionary algorithm to optimize the perturbation search and reduce the number of queries used.
This attack is generic for every camouﬂaged malware and does not require a per malware pre-deployment
phase to generate the adversarial sequence (either using a GAN, as in Hu and Tan (2017a), or a substitute
model, as in Rosenberg et al. (2018)). Moreover, the generation is done at runtime, making it more generic
and easier to deploy.

5.2 URL Detection

Webpages are addressed by a uniform resource locator (URL). A URL begins with the protocol used to
access the page. The fully qualiﬁed domain name (FQDN) identiﬁes the server hosting the webpage. It
consists of a registered domain name (RDN) and preﬁx referred to as subdomains. A phisher has full control
of the subdomain and can set them to any value. The RDN is constrained, since it has to be registered with
a domain name registrar. The URL may also have a path and query components which also can be changed
by the phisher at will.

Consider this URL example: https://www.amazon.co.uk/ap/signin?encoding=UTF8. We can identify
the following components: protocol = https; FQDN = www.amazon.co.uk; RDN = amazon.co.uk; path and
query = /ap/signin?encoding=UTF8. A summary of the attacks in the URL detection sub domain is shown
in Table 3.

Since URLs can be quite long, URL shortening services have started to appear. In addition to shortening

the URL, these services also obfuscate them.

5.2.1 Phishing URL Detection

Phishing refers to the class of attacks where a victim is lured to a fake web page masquerading as a target
website and is deceived into disclosing personal data or credentials. Phishing URLs seem like legitimate
URLs and redirect the users to phishing web pages, which mimic the look and feel of their target websites
(e.g., a bank website), in the hopes that the user will enter his/her personal information (e.g., password).

Bahnsen et al. (2018) performed an inference integrity attack to evade a character-level LSTM-based
phishing URL classiﬁer (Bahnsen et al. (2017)), by concatenating the effective URLs from historical attacks

Accepted as a long survey paper at ACM CSUR 2021

Table 3: Summary of Adversarial Learning Approaches in URL Detection

?
e
g
d
e
l
w
o
n
k

r
e
k
c
a
t
t
a

e
l
b
a
n
o
s
a
e
R

+

+

?
t
e
s
a
t
a
d

e
v
i
t
a
t
n
e
s
e
r
p
e
R

+

+

?
k
c
a
t
t
a

d
n
e
-
o
t
-
d
n
E

+

-

?
k
c
a
t
t
a

e
v
i
t
c
e
f
f
E

+

+

Attack
Type

Inference
integrity

Inference
integrity

Attack’s
Output

URL
(end-to-
end)

Feature
vector

Threat
Model

Perturbed
Features

Gray-
box

URL
characters

Gray-
box

All
features
used by
the
classiﬁers

Citation

Year

Target
Classiﬁer

2018

LSTM

2019 State-of-the-art

phishing
classiﬁers

Bahnsen
et al.
(2018)

Shirazi
et al.
(2019)

AlEroud
and
Karabatis
(2020)

Anderson
et al.
(2016)

Sidi et al.
(2019)

2020

RF, NN, DT,
LR, SVM

Inference
integrity

2016

RF

Inference
integrity

2019

CNN, LSTM,
BLSTM

Inference
integrity

URL
(end-to-
end)

URL
(end-to-
end)

URL
(end-to-
end)

Black-
box

URL
characters

+

+

+

-

Black-
box

URL
characters

Black-
box

URL
characters

+

+

+

+

+

+

+

-

?
s
e
r
u
t
a
e
f

e
v
i
t
a
t
n
e
s
e
r
p
e
R

-

+

-

-

-

Accepted as a long survey paper at ACM CSUR 2021

(thus, this is a gray-box attack). Then, from this full text, sentences with a ﬁxed length were created.
An LSTM model used those sentences as a training set in order to generate the next character. Once the
model generated a full prose text, it was divided by http structure delimiters to produce a list of pseudo
URLs. Each pseudo URL was assigned a compromised domain, such that the synthetic URLs take the
form: http://+compromised domain+pseudo URL. While this attack is indeed effective, the concatenation
of benign URLs can be signed, making this attack less evasive for real-life classiﬁers then the generative
attacks (e.g., AlEroud and Karabatis (2020)) mentioned below.

Shirazi et al. (2019) generated adversarial examples using all possible combinations of the values of
the features (e.g., website reputation) used by state-of-the-art phishing classiﬁers, such as Verma and Dyer
(2015), making this a more realistic attack. However, the attack requires knowledge about the features
being used by the classiﬁer, making it a gray-box inference integrity attack. Such knowledge is not always
accessible to the attacker, making this attack less feasible in real-life scenarios.

Phishing URLs were generated by a text GAN in Trevisan and Drago (2019); Anand et al. (2018), in
order to augment the phishing URL classiﬁer’s training set and improve its accuracy. AlEroud and Karabatis
AlEroud and Karabatis (2020) used the phishing URLs generated as adversarial examples in an inference
integrity attack, in order to bypass the target classiﬁer. It remains unclear whether the attack mentioned in
AlEroud and Karabatis (2020) is robust enough to bypass GAN-based defenses, such as the defense methods
presented in Trevisan and Drago (2019); Anand et al. (2018).

5.2.2 Domain Generation Algorithm (DGA) URL Detection

DGAs are commonly used malware tools that generate large numbers of domain names that can be used
for difﬁcult to track communications with command and control servers operated by the attacker. The large
number of varying domain names makes it difﬁcult to block malicious domains using standard techniques
such as blacklisting or sinkholing. DGAs are used in a variety of cyber attacks, including ransomware,
spam campaigns, theft of personal data, and implementation of distributed denial-of-service (DDoS) attacks.
DGAs allow malware to generate any number of domain names daily, based on a seed that is shared by the
malware and the threat actor, allowing both to synchronize the generation of domain names.

Sidi et al. (2019) used a black-box inference integrity attack, training a substitute model to simulate the
DGA classiﬁer on a list of publicly available DGA URLs. Then that attacker iterates over every character
in the DGA URL. In each iteration, the results of the feedforward pass of the substitute model are used to
compute the loss with regard to the benign class. The attacker performs a single backpropagation step on
the loss in order to acquire the Jacobian-based saliency map, which is a matrix that assigns every feature
in the input URL with a gradient value (JSMA; see Section 3). Features (characters) with higher gradient
values in the JSMA would have a more signiﬁcant (salient) effect on the misclassiﬁcation of the input, and
thus each character would be modiﬁed in turn, making the substitute model’s benign score higher. Finally,
URLs that evade detection by the substitute model would also evade detection by the target DGA classiﬁer
due to the transferability property (see Section 3). Despite the fact that this attack conﬁrms the feasibility
of transferability in the DGA URL detection sub-domain, the use of (only) character-level features does not
accurately represent real-life classiﬁers.

Anderson et al. (2016) performed an inference integrity black-box attack that used a GAN to produce
domain names that current DGA classiﬁers would have difﬁculty identifying. The generator was then used
to create synthetic data on which new models were trained. This was done by building a neural language
architecture, a method of encoding language in a numerical format, using LSTM layers to act as an autoen-
coder. The autoencoder is then repurposed such that the encoder (which takes in domain names and outputs
an embedding that converts a language into a numerical format) acts as the discriminator, and the decoder

Accepted as a long survey paper at ACM CSUR 2021

(which takes in the embedding and outputs the domain name) acts as the generator. Anderson et al. attacked
a random forest classiﬁer trained on features deﬁned in Schiavoni et al. (2014); Antonakakis et al. (2012);
Yadav et al. (2012); Yadav et al. (2010). The features of the random forest DGA classiﬁer are unknown to
the attacker. They include: the length of domain name; entropy of character distribution in domain name;
vowel to consonant ratio; Alexa top 1M n-gram frequency distribution co-occurrence count, where n = 3, 4,
or 5; n-gram normality score; and meaningful character ratio. The fact that this attack bypasses a classiﬁer
which uses many features, as speciﬁed above, makes it more suitable for real-life scenarios.

5.3 Network Intrusion Detection

A security system commonly used to secure networks is a network intrusion detection system (NIDS). An
NIDS is a device or software that monitors all trafﬁc passing a strategic point for malicious activities. When
such an activity is detected, an alert is generated. Typically an NIDS is deployed at a single point, for
example, at the Internet gateway. A summary of the attacks in the network intrusion detection sub-domain
is provided in Table 4.

Clements et al. (2019) conducted a white-box inference integrity attack against Kitsune (Mirsky et al.
(2018)), an ensemble of autoencoders used for online network intrusion detection. Kitsune uses packet
statistics which are fed into a feature mapper that divides the features between the autoencoders, to ensure
fast online training and prediction. The RMSE output of each autoencoder is fed into another autoencoder
that provides the ﬁnal RMSE score used for anomaly detection. This architecture can be executed on small,
weak routers.

Clements et al. used four adversarial methods: FGSM, JSMA, C&W, and ENM (see Section 3). The
attacker uses the LP distance on the feature space between the original input and the perturbed input as the
distance metric. Minimizing the L0 norm correlates to altering a small number of extracted features. This
method has two main limitations: (1) The threat model assumes that the attacker knows the target classiﬁer’s
features, architecture, and hyperparameters. This makes this attack a white-box attack, rather than black-box
attack. This is a less realistic assumption in real-life scenarios. (2) Modiﬁcation is done at the feature level
(i.e., modifying only the feature vector) and not at the sample level (i.e., modifying the network stream). This
means that there is no guarantee that those perturbations can be performed without harming the malicious
functionality of the network stream. The fact that some of the features are statistical makes the switch from
vector modiﬁcation to sample modiﬁcation even more challenging.

Lin et al. (2018) generated adversarial examples using a GAN, called IDSGAN, in which the GAN’s
discriminator obtains the labels from the black-box target classiﬁer. The adversarial examples are evaluated
against several target classiﬁers: SVM, naive Bayes, MLP, logistic regression, decision tree, random forest,
and k-nearest neighbors classiﬁers. This attack assumes knowledge about the target classiﬁer’s features,
making it a gray-box inference integrity attack. The features include individual TCP connection features
(e.g., the protocol type), domain knowledge-based features (e.g., a root shell was obtained), and statistical
features of the network sessions (like the percentage of connections that have SYN errors within a time
window). All features are extracted from the network stream (the NSL-KDD dataset was used; see Table
8). This attack generates a statistical feature vector, but the authors do not explain how to produce a real
malicious network stream that has those properties.

Yang et al. (2018) trained a DNN model to classify malicious behavior in a network using the same
features as Lin et al. (2018), achieving performance comparable to state-of-the-art NIDS classiﬁers, and
then showed how to add small perturbations to the original input to lead the model to misclassify malicious
network packets as benign while maintaining the maliciousness of these packets, this attack assumes that
an adversary without internal information on the DNN model is trying to launch black-box attack. Three

Accepted as a long survey paper at ACM CSUR 2021

Table 4: Summary of Adversarial Learning Approaches in Network Intrusion Detection

?
e
g
d
e
l
w
o
n
k

r
e
k
c
a
t
t
a

e
l
b
a
n
o
s
a
e
R

-

+

+

+

-

-

+

+

-

-

-

-

-

-

-

+

-

-

?
k
c
a
t
t
a
d
n
e
-
o
t
-
d
n
E

?
k
c
a
t
t
a

e
v
i
t
c
e
f
f
E

?
t
e
s
a
t
a
d

e
v
i
t
a
t
n
e
s
e
r
p
e
R

+

-

-

-

-

-

+

?
s
e
r
u
t
a
e
f

e
v
i
t
a
t
n
e
s
e
r
p
e
R

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

Protocol
statistical
features

Statistical
and
protocol
header
features

Similar to
Lin et al.,
but
modiﬁes
only non-
impactful
features
like send
time

Statistical
and
protocol
header
features

Features
from SDN
messages

Citation

Year

Target
Classiﬁer

Attack
Type

Threat
Model

Perturbed
Features

2019

Autoencoder
ensemble

Inference
integrity

White-
box

Attack’s
Output

Feature
vector

2018

SVM, NB,
MLP, LR, DT,
RF, KNN

Inference
integrity

Feature
vector

Gray-
box

2018

DNN

Inference
integrity

Feature
vector

Gray-
box

Same as
Lin et al.

2017 DT, RF, SVM

Inference
integrity

Feature
vector

Gray-
box

Same as
Lin et al.

2018

MLP

Inference
integrity

Feature
vector

White-
box

Same as
Lin et al.

2018

MLP

2019 DAGMM, AE,

AnoGAN,
ALAD,
DSVDD,
OC-SVM, IF

Inference
integrity

Inference
integrity

Feature
vector

White-
box

Same as
Lin et al.

PCAP ﬁle
(end-to-
end)

Query-
efﬁcient
gray-
box

2019 FC DNN, SNN

Inference
integrity

Feature
vector

Gray-
box

2019 MLP, CNN,

LSTM

Feature
vector

White-
box

Inference
integrity,
training
availabil-
ity

Clements
et al.
(2019)

Lin et al.
(2018)

Yang
et al.
(2018)

Rigaki
and
Elragal
(2017)

Warzynski
and
Kolaczek
(2018)

Wang
(2018)

Kuppa
et al.
(2019)

Ibitoye
et al.
(2019)

Huang
et al.
(2019)

Accepted as a long survey paper at ACM CSUR 2021

different black-box attacks were attempted by the adversary: an attack based on zeroth order optimization
(ZOO; see Section 3), an attack based on a GAN (similar to the one proposed by Lin et al. (2018)), and an
attack on which a substitute model is trained which is followed by a C&W attack (see Section 3) performed
against the substitute model. Applying the generated adversarial example against the target classiﬁer suc-
ceeds due to the transferability property (see Section 3). This paper has the same limitations as Lin et al.
(2018): this gray-box inference integrity attack assumes knowledge about the target classiﬁer’s features and
also generates only the feature vectors and not the samples themselves.

In their gray-box inference integrity attack, Rigaki and Elragal Rigaki and Elragal (2017) used the same
NSL-KDD dataset (see Table 8). Both FGSM and JSMA (see Section 3) attacks were used to generate
adversarial examples against an MLP substitute classiﬁer, and the results were evaluated against decision
tree, random forest, and linear SVM classiﬁers. This paper has the same limitations as Lin et al. (2018): this
attack assumes knowledge about the target classiﬁer’s features and also generates only the feature vectors
and not the samples themselves.

Warzynski and Kolaczek (2018) performed a white-box inference integrity feature vector attack against
an MLP classiﬁer trained on the NSL-KDD dataset (see Table 8). They used a white-box FGSM attack (see
Section 3). Wang (2018) added three more white-box attacks: JSMA, DeepFool, and C&W (see Section 3).
The Lp distance and the perturbations are in the feature space in both cases. This attack is not an end-to-end
attack, so again it cannot be used to generate malicious network streams that bypass real-life classiﬁers.

Kuppa et al. (2019) proposed a query-efﬁcient gray-box inference integrity attack against deep unsu-
pervised anomaly detectors, which leverages a manifold approximation algorithm for query reduction and
generates adversarial examples using spherical local subspaces while limiting the input distortion and KL
divergence. Seven state-of-the-art anomaly detectors with different underlying architectures were evaluated:
a deep autoencoding Gaussian mixture model, an autoencoder, anoGAN, adversarially learned anomaly de-
tection, deep support vector data description, one-class support vector machines, and isolation forests (see
Section 3), which is a more diverse set of classiﬁers than other attacks mentioned in this section, making
this attack more applicable, regardless of the classiﬁer deployed. All classiﬁers were trained on the CSE-
CIC-IDS2018 dataset and features (see Table 8). This dataset is more recent than the NSL-KDD dataset
used in much of the research mentioned in this section and better represent today’s threats. Unlike other
papers discussed in this section, the authors generated a full PCAP ﬁle (and not just feature vectors). They
also only modiﬁed features that could be modiﬁed without harming the network stream (e.g., time-based
features), so they actually created adversarial samples and not just feature vectors. However, they did not
run the modiﬁed stream in order to verify that the malicious functionality remains intact.

Ibitoye et al. (2019) attacked a fully connected DNN and a self-normalizing neural network classiﬁer
(an SNN is a DNN with a SeLU activation layer; Klambauer et al. (2017)) trained on the BoT-IoT dataset
and features (see Table 8), using FGSM (see Section 3), the basic iteration method, and the PGD at the
feature level. They showed that both classiﬁers were vulnerable, although SNN was more robust to adver-
sarial examples. This attack is unique in terms of the dataset and architectures used and demonstrates the
susceptibility of IoT SNN-based classiﬁers to adversarial attacks.

Huang et al. (2019) attacked port scanning detectors in a software-deﬁned network (SDN). The detectors
were MLP, CNN, and LSTM classiﬁers trained on features extracted from Packet-In messages (used by port
scanning tools like Nmap in the SDN) and switch monitoring statistic messages (STATS). The white-box
inference integrity attacks used were FGSM and JSMA (see Section 3). The JSMA attack was also (suc-
cessfully) conducted on regular trafﬁc packets (JSMA reverse) to create false negatives, creating noise and
confusion in the network (a white-box training availability attack). While this attack requires permissions
not available to a regular attacker (knowledge of the classiﬁer’s architecture, etc.), it shows the susceptibility
of port scanning classiﬁers to adversarial attacks.

Accepted as a long survey paper at ACM CSUR 2021

5.4 Spam Filtering

The purpose of a spam ﬁlter is to determine whether an incoming message is legitimate (i.e., ham) or unso-
licited (i.e., spam). Spam detectors were among the ﬁrst applications to use machine learning in the cyber
security domain and therefore were the ﬁrst to be attacked. A summary of the attacks in the spam ﬁltering
sub domain is shown in Table 5.

Huang et al. (2011) attacked SpamBayes Robinson (2003), which is a content-based statistical spam ﬁlter
that classiﬁes email using token counts. SpamBayes computes a spam score for each token in the training
corpus based on its occurrence in spam and non-spam emails. The ﬁlter computes a message’s overall spam
score based on the assumption that the token scores are independent, and then it applies Fisher’s method
for combining signiﬁcance tests to determine whether the email’s tokens are sufﬁciently indicative of one
class or the other. The message score is compared against two thresholds to select the label: spam, ham (i.e.,
non-spam), or unsure.

Huang et al. (2011) designed two types of training availability attacks. The ﬁrst is an indiscriminate
dictionary attack, in which the attacker sends attack messages that contain a very large set of tokens—the
attack’s dictionary. After training on these attack messages, the victim’s spam ﬁlter will have a higher spam
score for every token in the dictionary. As a result, future legitimate email is more likely to be marked as
spam, since it will contain many tokens from that lexicon. The second attack is a targeted attack - the attacker
has some knowledge of a speciﬁc legitimate email he/she targets for incorrect ﬁltering. Nelson et al. (2008)
modeled this knowledge by letting the attacker know a certain fraction of tokens from the target email, which
are included in the attack message. Availability attacks like these are quite rare in the adversarial learning
landscape and open up interesting attack options, such as adversarial denial-of-service attacks (see Section
7.2).

Biggio et al. (2014) evaluated the robustness of linear SVM and logistic regression classiﬁers to a white-
box inference integrity attack where the attacker adds the most impactful good words and found that while
both classiﬁers have the same accuracy for unperturbed samples, the logistic regression classiﬁer outper-
forms the SVM classiﬁer in robustness to adversarial examples. The white-box approach used in this attack
is less feasible in real-life scenarios.

Br¨uckner et al. (2012) modeled the interaction between the defender and the attacker in the spam ﬁltering
domain as a static game in which both players act simultaneously, i.e., without prior information about their
opponent’s move. When the optimization criterion of both players depends not only on their own action
but also on their opponent’s move, the concept of a player’s optimal action is no longer well-deﬁned, and
thus the cost functions of the learner (the defender) and the data generator (the attacker) are not necessarily
antagonistic. The authors identiﬁed the conditions under which this prediction game has a unique Nash
equilibrium and derived algorithms that ﬁnd the equilibrial prediction model. From this equation, they
derived new equations for the Nash logistic regression and Nash SVM using custom loss functions. The
authors showed that both the attacker and the defender are better off respectively attacking and using the
Nash classiﬁers. While this attack took a different (more game theory focused) approach then other more
“conventional” attacks, its effectiveness is no different.

Sethi and Kantardzic (2018) trained several classiﬁers (linear SVM, k-nearest neighbors, SVM with RBF
kernel, decision tree, and random forest) on several datasets, including SPAMBASE for spam detection and
NSL-KDD (see Table 8) for network intrusion detection. They presented a gray-box inference integrity and
conﬁdentiality attack and a query-efﬁcient gray-box anchor point inference integrity attack which is effective
against all models. The uniqueness of this study lies in its focus on query-efﬁciency (thus making this attack
more feasible) and on the conﬁdentiality attack (trying to reverse engineer the attacked model, i.e., a model
inversion attack), which are quite rare in the spam ﬁltering domain.

Accepted as a long survey paper at ACM CSUR 2021

Table 5: Summary of Adversarial Learning Approaches in Spam Filtering

?
e
g
d
e
l
w
o
n
k

r
e
k
c
a
t
t
a

e
l
b
a
n
o
s
a
e
R

?
t
e
s
a
t
a
d

e
v
i
t
a
t
n
e
s
e
r
p
e
R

?
s
e
r
u
t
a
e
f

e
v
i
t
a
t
n
e
s
e
r
p
e
R

?
k
c
a
t
t
a
d
n
e
-
o
t
-
d
n
E

?
k
c
a
t
t
a

e
v
i
t
c
e
f
f
E

+

-

+

+

+

Attack’s
Output

Feature
vector

Threat
Model

Gray-
box

Perturbed
Features

Email
words or
same as
Lin et al.

Email
(end-to-
end)

Email
(end-to-
end)

Email
(end-to-
end)

Email
(end-to-
end)

Email
(end-to-
end)

Feature
vector

Gray-
box

Email
words

+

+

+

+

+

Email
words

Email
words

Email
words

Email
words

Email
words

-

+

+

+

-

+

+

+

+

-

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

White-
box

Gray-
box

Gray-
box

Gray-
box

Gray-
box;
add and
read
training
set
access

Citation

Year

Target
Classiﬁer

SVM, kNN,
DT, RF

2018

Attack
Type

Inference
integrity
and
conﬁ-
dential-
ity

Training
avail-
ability

Inference
integrity

Inference
integrity

Inference
integrity

2011 Bayesian spam

ﬁlter

2014

SVM, LR

2012

NB, SVM

2018 NB, LSTM, 1D

CNN

2018

LSTM, 1D
CNN

Inference
integrity

2017

LR, MLP

Training
integrity

Sethi and
Kan-
tardzic
(2018)

Huang
et al.
(2011);
Nelson
et al.
(2008)

Biggio
et al.
(2014)

Br¨uckner
et al.
(2012)

Kuleshov
et al.
(2018)

Lei et al.
(2018)

Mu˜noz
Gonz´alez
et al.
(2017)

Accepted as a long survey paper at ACM CSUR 2021

Generalized attack methods, which are effective against several NLP classiﬁers, are a recent trend.
Kuleshov et al. (2018) implemented such a generalized black-box inference integrity attack to evade NLP
classiﬁers, including spam ﬁltering, fake news detection, and sentiment analysis. The greedy attack ﬁnds
semantically similar words (enumerating all possibilities to ﬁnd words with a minimal distance and score
difference from the original input) and replacing them in sentences with a high language model score. Three
classiﬁers were evaded: NB, LSTM, and a word-level 1D CNN.

Lei et al. (2018) did the same while using a joint sentence and word paraphrasing technique to maintain
the original semantics and syntax of the text. They attacked LSTM and a word-level 1D CNN trained on
same datasets used by Kuleshov et al. (2018), providing a more effective attack on many datasets, including
a spam ﬁltering dataset.

This interesting approach of generalization can be extended in the future by applying other NLP-based

attacks in the domain of spam adversarial examples.

5.5 Cyber-Physical Systems (CPSs) and Industrial Control Systems (ICSs)

Cyber-physical systems (CPSs) and industrial control systems (ICSs) consist of hardware and software com-
ponents that control and monitor physical processes, e.g., critical infrastructure, including the electric power
grid, transportation networks, water supply networks, nuclear plants, and autonomous car driving. A sum-
mary of the attacks in the CPS sub-sdomain is provided in Table 6.

Specht et al. (2018) trained a fully connected DNN on the SECOM dataset, recorded from a semi-
conductor manufacturing process, which consists of 590 attributes collected from sensor signals and vari-
ables during manufacturing cycles. Each sensor data entry is labeled as either a normal or anomalous pro-
duction cycle. They used the FGSM white-box inference integrity feature vector attack to camouﬂage ab-
normal/dangerous sensor data so it appears normal. This attack uses a white-box approach, making it less
feasible in real-life scenarios.

Ghafouri et al. (2018) conducted a gray-box inference integrity attack on a linear regression-based
anomaly detector, a neural network regression anomaly detector, and an ensemble of both, using the TE-
PCS dataset, which contains sensor data describing two simultaneous gas-liquid exothermic reactions for
producing two liquid products. There are safety constraints that must not be violated (e.g., safety limits
for the pressure and temperature of the reactor), corresponding to the data. For the linear regression-based
anomaly detector, the problem of ﬁnding adversarial examples of sensor data can be solved using a mixed-
integer linear programming (MILP) problem. In order to bypass the neural network regression and ensemble,
an iterative algorithm is used. It takes small steps in the direction of increasing objective function. In each
iteration, the algorithm linearizes all of the neural networks at their operating points and solves the problem
using MILP as before. The mathematical approach taken in this attack (MILP) can be used in the future to
attack different models, e.g., auto-regressive models, with greater efﬁciency than current approaches.

Clark et al. (2018) used a Jaguar autonomous vehicle (JAV) to emulate the operation of an autonomous
vehicle. The driving and interaction with the JAV environment used the Q-learning reinforcement learning
algorithm. JAV acts as an autonomous delivery vehicle and transports deliveries from an origination point to
a destination location. The attacker’s goal is to cause the JAV to deviate from its learned route to an alternate
location where it can be ambushed. A white-box inference integrity attack was chosen with the goal of
compromising the JAV’s reinforcement learning (RL) policy and causing the deviation. The attack was
conducted by inserting an adversarial data feed into the JAV via its ultrasonic collision avoidance sensor.
This attack uses a white-box approach, making it less feasible in real-life scenarios. However, it attacks
reinforcement learning models which are frequently used in CPSs.

Accepted as a long survey paper at ACM CSUR 2021

Table 6: Summary of Adversarial Learning Approaches in Cyber-Physical Systems

?
e
g
d
e
l
w
o
n
k

r
e
k
c
a
t
t
a

e
l
b
a
n
o
s
a
e
R

-

+

-

+

+

-

+

?
t
e
s
a
t
a
d

e
v
i
t
a
t
n
e
s
e
r
p
e
R

+

+

+

+

+

?
k
c
a
t
t
a

e
v
i
t
c
e
f
f
E

+

+

+

+

+

?
s
e
r
u
t
a
e
f

e
v
i
t
a
t
n
e
s
e
r
p
e
R

+

+

+

+

+

+

+

+

+

+

+

?
k
c
a
t
t
a
d
n
e
-
o
t
-
d
n
E

-

-

-

-

-

-

-

Citation

Year

Target
Classiﬁer

2018

FC DNN

Attack
Type

Inference
integrity

Attack’s
Output

Feature
vector

Threat
Model

Perturbed
Features

White-
box

Sensor
signals

Specht
et al.
(2018)

Ghafouri
et al.
(2018)

Clark
et al.
(2018)

Feng
et al.
(2017)

Erba
et al.
(2019)

Yaghoubi
and
Fainekos
(2019)

Li et al.
(2020a)

2018

LR, DNN

Inference
integrity

Feature
vector

Gray-
box

Sensor
data

2018 RL Q-Learning

Inference
integrity

Feature
vector

White-
box

Ultrasonic
collision
avoidance
sensor
data

2017

LSTM

Inference
integrity

Feature
vector

Gray-
box

Sensor
data

2019 Autoencoders

2019

RNN

Inference
in-
tegrity/
avail-
ability

Inference
integrity

Feature
vector

Gray-
box

Sensor
data

Feature
vector

White-
box

Continuous
sensor
data

2020

FC DNN

Inference
integrity

Feature
vector

Gray-
box

Sensor
data

Accepted as a long survey paper at ACM CSUR 2021

Feng et al. (2017) presented a gray-box inference integrity attack against a LSTM anomaly detector us-
ing a GAN (see Appendix A) with a substitute model as a discriminator. Two use cases were evaluated: gas
pipeline and water treatment plant sensor data. Li et al. (2020a) presented a gray-box inference integrity at-
tack against the same dataset, but used a constraint-based adversarial machine learning to adhere the intrinsic
constraints of the physical systems, modeled by mathematical constraints derived from normal sensor data.
This attack targets LSTM classiﬁers, which are commonly used in ICS, making this an important attack.

Erba et al. (2019) demonstrated inference integrity and availability attacks against an autoencoder-based
anomaly detection system of water treatment plant sensor data. Access to both the ICS features and benign
sensor data (to train a substitute model) is assumed, making this attack a gray-box attack. This attack
enumerates all possible operations for every sensor. This attack targets autoencoder classiﬁers, which are
commonly used to detect anomalies, as done in ICSs, making this an important attack.

Yaghoubi and Fainekos (2019) presented a gray-box inference integrity attack against a steam condenser
with an RNN-based anomaly detector that monitors continuous (e.g., signal) data. The attack uses gradient-
based local search with either uniform random sampling or simulated annealing optimization to ﬁnd the data
to modify.

5.6 Biometric Systems

In this subsection, we focus on attacks that target the most commonly used biometric systems that leverage
machine learning: face, speech, and iris recognition systems. While these attacks can be viewed as computer
vision (or audio) attacks, we focus only on such attacks that affect cyber security authentication thus allowing
unauthorized users access to the protected systems. Many studies have focused on adversarial attacks against
other types of biometric systems, for instance: handwritten signature veriﬁcation (Hafemann et al. (2019)),
EEG biometrics ( ¨Ozdenizci et al. (2019)), and gait biometric recognition (Prabhu et al. (2017)). However,
as previously mentioned, these are not discussed here due to space limitations. A summary of the attacks
against biometric systems is presented in Table 7.

5.6.1 Face Recognition

Sharif et al. (2016) presented an inference integrity attack against face recognition systems. The target clas-
siﬁer for the white-box attack was VGG-Face, a 39 layer CNN (Parkhi et al. (2015)). The target classiﬁer
for the black-box attack was the Face++ cloud service. Instead of perturbing arbitrary pixels in the facial
image, this attack only perturbed the pixels of eyeglasses which were worn by the attacker so the attacker
would either be classiﬁed as another person (label-target attack) or not be classiﬁed as himself (indiscrim-
inate attack). Both attacks are feature-targeted. Sharif et al. used a commodity inkjet printer to print the
front plane of the eyeglass frames on glossy paper, which they then afﬁxed to the actual eyeglass frames
when physically performing attacks. This makes this end-to-end attack interesting, as it can be used (in its
black-box variant) in real-life scenarios.

Both Chen et al. (2017a) and Liu et al. (2018) used a black-box training integrity poisoning attack against
face recognition systems. The CNN target classiﬁers were VGG-Face (Parkhi et al. (2015)) and DeepID (Sun
et al. (2014)) respectively. Liu et al. used a non-physical image, such as a square appearing in the picture,
as the Trojan trigger in the picture to be labeled as a different person. In Chen et al. (2017a), the poisoned
facial images contained a physical accessory as the key; a photo of a person taken directly from the camera
can then become a backdoor when the physical accessory is worn; thus, this is both a feature-targeted and a
label-targeted attack. Both attacks require training set add access. Despite the fact that these are white-box
attacks, the usage of feature-targeting ﬁts nicely with some forms of real-life attacks, such as supply-chain

Accepted as a long survey paper at ACM CSUR 2021

Table 7: Summary of Adversarial Learning Approaches in Biometric Systems

?
e
g
d
e
l
w
o
n
k

r
e
k
c
a
t
t
a

e
l
b
a
n
o
s
a
e
R

?
t
e
s
a
t
a
d

e
v
i
t
a
t
n
e
s
e
r
p
e
R

?
s
e
r
u
t
a
e
f

e
v
i
t
a
t
n
e
s
e
r
p
e
R

?
k
c
a
t
t
a
d
n
e
-
o
t
-
d
n
E

?
k
c
a
t
t
a

e
v
i
t
c
e
f
f
E

+

+

+

+

+

Perturbed
Fea-
tures

Image’s
pixels

Image’s
pixels

-

+

+

+

+

Image’s
pixels

-

+

+

+

+

+

-

+

+

-

+

+

+

+

+

+

+

+

+

+

+

-

+

+

-

+

+

+

+

+

Mel-
spectrum
fea-
tures
and
MFCCs

Raw
wave-
forms

Raw
wave-
forms

Mel-
spectrum
fea-
tures

Image’s
pixels

pixels

Iris

codes

Citation

Year

Target
Classiﬁer

2016

CNN

Sharif
et al.
(2016)

Biometric
Applica-
tion

Face
recogni-
tion

Inference
integrity

Attack
Type

Attack’s
Output

Threat
Model

Liu et al.
(2018)

2018

CNN

Face
recogni-
tion

Training
integrity

Chen et al.
(2017a)

2017

CNN

Face
recogni-
tion

Training
integrity

Kreuk
et al.
(2018a)

2018 LSTM/GRU Speaker
recogni-
tion

Inference
integrity

Feature
vector

Physical
eyeglasses
(end-to-
end)

Image
(non-
physical
end-to-
end)

Physical
accessory
(end-to-
end)

Feature-
targeted
white-box
and
black-box

Feature-
targeted
white-
box; add
training
set access

Feature-
targeted
white-
box; add
training
set access

White-box
or
black-box

Black-box

Black-box

Gong and
Poellabauer
(2017)

2017 Mixed
CNN-
LSTM

Du et al.
(2019)

2019

CNN

Cai et al.
(2018)

2018

CNN

2018

CNN

Wang
et al.
(2018)

Taheri

et al.

(2019)

et al.

(2019a,b)

Speaker
recogni-
tion

Speaker
recogni-
tion

Speaker
recogni-
tion

Face
recogni-
tion, Iris
recogni-
tion

recogni-

tion, iris

recogni-

tion

nition

Inference
integrity

Inference
integrity

Raw
waveform
(end-to-
end)

Raw
waveform
(end-to-
end)

Inference
integrity

Feature
vector

Gray-box

Gray-box

Inference
integrity

integrity

Inference

integrity

Image
(non-
physical
end-to-
end)

Image

(non-

physical

end-to-

end)

Feature

vector

2019

CNN

Fingerprint

Inference

White-box

Image’s

-

+

+

+

+

Soleymani

2019

CNN

Iris recog-

Gray-box

+

-

+

+

-

Accepted as a long survey paper at ACM CSUR 2021

attacks, where a powerful adversary wants to exploit a very speciﬁc scenario (e.g., evade detection of a
person only when he/she is holding a key).

5.6.2 Speaker Veriﬁcation/Recognition

Note that in this subsection, we only discuss speaker recognition and veriﬁcation system adversarial attacks.
Kreuk et al. (2018a) presented white-box inference integrity attacks on an LSTM/GRU classiﬁer that
was either trained on the YOHO or NTIMIT datasets using two types of features: Mel-spectrum features and
MFCCs. They also presented two black-box inference integrity attacks, using the transferability property. In
the ﬁrst one, they generated adversarial examples with a system trained on NTIMIT dataset and performed
the attack on a system that was trained on YOHO. In the second attack, they generated the adversarial
examples with a system trained using Mel-spectrum features and performed the attack on a system trained
using MFCCs. All of the attacks used the FGSM attack, and the attack output was a feature vector and not a
complete audio sample. This, and the fact that this attack uses a white-box approach, making it less feasible
in real-life scenarios

Gong and Poellabauer (2017) trained a WaveRNN model (a mixed CNN-LSTM model) on raw wave-
forms (the IEMOCAP dataset’s utterances; see Table 8) for speaker recognition, as well as emotion and
gender recognition. They used a substitute waveCNN model and performed a black-box inference integrity
attack using FGSM on the raw waveforms, rather than on the acoustic features, making this an end-to-end
attack that does not require an audio reconstruction step. The use of a gray-box approach and raw waveform
features makes this attack feasible in real-life scenarios.

Du et al. (2019) used six state-of-the-art speech command recognition CNN models: VGG19, DenseNet,
ResNet18, ResNeXt, WideResNet18, and DPN-92, all adapted to the raw waveform input. The models
were trained for speaker recognition on the IEMOCAP dataset (see Table 8) and for speech recognition,
sound event classiﬁcation, and music genre classiﬁcation using different datasets. The black-box inference
integrity attack used FGSM or particle swarm optimization (PSO) on the raw waveforms. The use of a
gray-box approach and raw waveform features and the evaluation against many classiﬁers make this attack
feasible in real-life scenarios.

Cai et al. (2018) trained a CNN classiﬁer that performs multi-speaker classiﬁcation using Mel-spectrograms

as input. They used a Wasserstein GAN with gradient penalty (WGAN-GP) to generate adversarial exam-
ples for an indiscriminate gray-box inference integrity attack and also used a WGAN-GP with a modiﬁed
objective function for a speciﬁc speaker for a targeted attack. The attack output is a feature vector of Mel-
spectrograms and not an audio sample. This attack uses a white-box approach, making it less feasible in
real-life scenarios.

5.6.3 Iris and Fingerprint Systems

Wang et al. (2018) performed an indiscriminate black-box inference integrity attack, leveraging the fact that
many image-based models, including face recognition and iris recognition models, use transfer learning, i.e.,
they add new layers on top of pretrained layers which are trained on a different model (a teacher model with a
known architecture) and are used to extract high-level feature abstractions from the raw pixels. For instance,
the face recognition model’s teacher model can be VGG-Face (Parkhi et al. (2015)), while an iris model’s
teacher model can be VGG16. By attacking the teacher model using white-box attacks, such as C&W, the
target model (student model), for which the architecture is unknown, is also affected. This approach can be
used to attack many classiﬁers, especially those in domains where transfer learning using a known model is
common, like the computer vision domain.

Accepted as a long survey paper at ACM CSUR 2021

Taheri et al. (2019) trained a CNN classiﬁer on the CASIA dataset of images of iris and ﬁngerprint
data. They implemented a white-box inference integrity attack using the FGSM, JSMA, DeepFool, C&W,
and PGD methods to generate the perturbed images. This attack uses a white-box approach, making it less
feasible in real-life scenarios.

Soleymani et al. (2019a,b) generated adversarial examples for code-based iris recognition systems, us-
ing a gray-box inference integrity attack. However, conventional iris code generation algorithms are not
differentiable with respect to the input image. Generating adversarial examples requires backpropagation of
the adversarial loss. Therefore, they used a deep autoencoder substitute model to generate iris codes that
were similar to iris codes generated by a conventional algorithm (OSIRIS). This substitute model was used
to generate the adversarial examples using FGSM, iGSM, and Deepfool attacks. The attack of iris codes can
serve as the initial step toward end-to-end adversarial attacks against biometric systems.

6 Adversarial Defense Methods in the Cyber Security Domain

Our taxonomy focuses on the attack side, but every attack is accompanied by a corresponding defense
method.

If adversarial attacks are equivalent to malware attacking a computer (a machine learning model), then
defense methods can be viewed as anti-malware products. However, most defense methods have been evalu-
ated in the image recognition domain for CNNs and in the NLP domain for RNNs. Due to space limitations,
we cannot provide a complete list of the state-of-the-art prior work in those domains. A more comprehensive
list can be found, e.g., in Qiu et al. (2019).

Several papers presenting attacks in the cyber security domain (e.g., Grosse et al. (2017); Sidi et al.
(2019)) discuss the fact that the attack is effective even in the presence of well-known defense methods
that were evaluated and found effective in the computer vision domain (e.g., distillation and adversarial
retraining). However, only a few defense methods were developed speciﬁcally for the cyber security domain
and its unique challenges, like those described in Section 2. Furthermore, cyber security classiﬁers usually
have a different architecture than computer vision classiﬁers, against which most of the methods presented
in prior research were evaluated.

Adversarial defense methods can be divided into two subgroups: (1) detection methods - methods used
to detect adversarial examples, and (2) robustness methods - methods used to make a classiﬁer more robust
to adversarial examples, without explicitly trying to detect them.

Each defense method is either attack-speciﬁc (i.e., it requires adversarial examples generated by the
attack in order to mitigate the attack) or attack-agnostic (i.e., it works against all types of attack methods,
without the need to have a dataset of adversarial examples generated by those attacks). Attack-agnostic
defense methods are more generic and are therefore preferable.

In the malware detection sub-domain, Chen et al. (2018b) suggested an attack-agnostic method to make
an Android malware classiﬁer robust to poisoning attacks. Their method includes two phases: an ofﬂine
training phase that selects and extracts features from the training set and an online detection phase that
utilizes the classiﬁer trained by the ﬁrst phase. These two phases are intertwined through a self-adaptive
learning scheme, in which an automated camouﬂage detector is introduced to ﬁlter the suspicious false
negatives and feed them back into the training phase. Stokes et al. (2017) evaluated three attack-agnostic
robustness defense methods: weight decay, an ensemble of classiﬁers, and distillation (neural network com-
pression, resulting in gradient masking) for a dynamic analysis malware classiﬁer based on a non-sequence
based deep neural network. Rosenberg et al. (2019) tried to defend an API call-based RNN classiﬁer and

Accepted as a long survey paper at ACM CSUR 2021

compared their own RNN attack-agnostic robustness defense method, termed sequence squeezing, to four
robustness defense methods inspired by existing CNN-based defense methods: adversarial retraining (re-
training the classiﬁer after adding adversarial examples), defense GAN (generating several samples using a
GAN and using the closest one to the original input as the classiﬁer’s input), nearest neighbor classiﬁcation,
and RNN ensembles. They showed that sequence squeezing provides the best trade-off between training and
inference overhead (which is less critical in the computer vision domain) and adversarial robustness.

For DGA detection, Sidi et al. (Sidi et al., 2019) evaluated the robustness defense methods of adversar-
ial retraining (attack-speciﬁc) and distillation (attack-agnostic), showing that they improve the robustness
against adversarial attacks.

For CPS defense, Kravchic and Shabtai Kravchik and Shabtai (2019) suggested detecting adversarial
attacks in ICS data using 1D CNNs and undercomplete autoencoders (UAEs), an attack-agnostic method.
The authors demonstrate that attackers must sacriﬁce much of their attack’s potential damage to remain
hidden from the detectors. Ghafouri et al. (2018) presented robust linear regression and neural network
regression-based anomaly detectors for CPS anomalous data detection by modeling a game between the
defender and attacker as a Stackelberg game in which the defender ﬁrst commits to a collection of thresholds
for the anomaly detectors, and the attacker then computes an optimal attack. The defender aims to minimize
the impact of attacks, subject to a constraint typically set to achieve a target number of false alarms without
consideration of attacks.

For spam detection, Alzantot et al. (2018) evaluated the attack-speciﬁc robustness defense methods of
adversarial retraining against RNN classiﬁers, showing that such methods improve robustness against adver-
sarial attacks.

For biometric systems defense methods, Taheri et al. (2019) presented an attack-speciﬁc detection ar-
chitecture that includes shallow and deep neural networks to defend against biometric adversarial examples.
The shallow neural network is responsible for data preprocessing and generating adversarial samples. The
deep neural network is responsible for understanding data and information, as well as for detecting adver-
sarial samples. The deep neural network gets its weights from transfer learning, adversarial training, and
noise training. Specht et al. (2018) suggested an attack-speciﬁc robustness defense method using an iter-
ative adversarial retraining process to mitigate adversarial examples for semi-conductor anomaly detection
system of sensor data. Soleymani et al. (2019a) used an attack-agnostic robustness defense method involv-
ing wavelet domain denoising of the iris samples, by investigating each wavelet sub-band and removing the
sub-bands that are most affected by the adversary.

In our opinion, other defense methods proposed for the computer vision domain could inspire similar
defense methods in the cyber domain. However, their effectiveness in the cyber security domain would need
to be evaluated, since as discussed above, the cyber security domain is different and has unique challenges.
Furthermore, in the cyber security domain further emphasis should be put on the defense method overhead
(as done, e.g., in Rosenberg et al. (2019)), since cyber security classiﬁers usually perform their classiﬁcation
in real-time, meaning that low overhead is critical, unlike in the computer vision domain. Finally, we
believe that te research attention should be given to attack-agnostic defense methods, as attack-speciﬁc
defense methods (e.g., adversarial retraining) provide a very narrow defense against the growing variety of
adversarial attacks presented in this study.

Accepted as a long survey paper at ACM CSUR 2021

7 Current Gaps and Future Research Directions for Adversarial Learn-

ing in the Cyber Security Domain

In this section, we highlight gaps in our taxonomy, presented in Section 4, which are not covered by the
applications presented in Sections 5 and 6, despite having a required functionality. Each such gap is pre-
sented in a separate subsection below. For each gap, we summarize the progress made on this topic in other
domains of adversarial learning, such as the computer vision domain, and extrapolate future trends in the
cyber security domain from it.

7.1 Attack’s Targeting Gap: Feature-Targeted Attacks and Defenses

Poisoning integrity attacks place mislabeled training points in a region of the feature space far from the
rest of the training data. The learning algorithm labels such a region as desired, allowing for subsequent
misclassiﬁcations at test time. However, adding samples to the training set might cause misclassiﬁcation of
many samples and thus would raise suspicion of an attack, while the adversary might want to evade just a
speciﬁc sample (e.g., a dedicated APT).

In non-cyber security domains, a feature-targeted attack, also known as a Trojan neural network attack
(Liu et al. (2018)) or backdoor attack (Gu et al. (2019), Chen et al. (2017a)), is a special form of poisoning
attack, which aims to resolve this problem. A model poisoned by a backdoor attack should still perform
well on most inputs (including inputs that the end user may hold out as a validation set) but cause targeted
misclassiﬁcations or degrade the accuracy of the model for inputs that satisfy some secret, attacker-chosen
property, which is referred to as the backdoor trigger. For instance, adding a small rectangle to a picture
would cause it to be classiﬁed with a speciﬁc target label Liu et al. (2018). Such attacks were performed
on face recognition Chen et al. (2017a); Liu et al. (2018), trafﬁc sign detection (Gu et al. (2019)), sentiment
analysis, speech recognition, and autonomous driving (Liu et al. (2018)) datasets.

However, such attacks have not been applied yet in the cyber security domain, despite the fact that such
attacks have interesting use cases. For instance, such an attack might allow only a speciﬁc nation-state APT
to bypass the malware classiﬁer, while still detecting other malware, leaving the defender unaware of this
feature-targeted attack.

Defenses against such attacks are also required. In the image recognition domain, Wang et al. (2019)
generates a robust classiﬁer by pruning backdoor-related neurons from the original DNN. Gao et al. (2019)
detects a Trojan attack during run-time by perturbing the input and observing the randomness of predicted
classes for perturbed inputs on a given deployed model. A low entropy in predicted classes implies the
presence of a Trojaned input. Once feature-targeted attacks are published in the cyber security domain,
defense methods to mitigate them will follow.

7.2 Attacker’s Goals Gap: Resource Exhaustion-Based Availability Attacks

There are two availability attack variants. One is the subversion attack, in which the system is fooled so
it does not serve legitimate users. Such adversarial attacks have been implemented in the cyber security
domain by fooling the classiﬁer into considering legitimate trafﬁc as malicious, thus blocking it (e.g., Huang
et al. (2011); Nelson et al. (2008)). The other variant is the resource exhaustion attack, in which all of
a system’s available resources are used in order to prevent legitimate users from using them. The second
variant is very common in the cyber security domain (e.g., distributed denial-of-service attacks on websites,
zip bombs, etc.) but not in adversarial attacks in this domain.

Accepted as a long survey paper at ACM CSUR 2021

Given recent advancements in the computer vision domain, e.g., sponge examples Shumailov et al.
(2020), which are adversarial examples whose classiﬁcation requires 10-200 times the resources of classify-
ing a regular sample, we believe it is only a matter of time before such examples will reach the cyber security
domain, e.g., PE ﬁles that take a very long to classify.

7.3 Attacker’s Goals and Knowledge Gap: Conﬁdentiality Attacks via Model Queries

and Side-Channels

Reverse engineering (reversing) of traditional (non-ML-based) anti-malware is a fundamental part of a mal-
ware developer’s modus operandi. So far, conﬁdentiality attacks have only been conducted against image
recognition models and not against cyber security related models, e.g., malware classiﬁers. However, per-
forming such attacks in the cyber security domain might provide the attacker with enough data to perform
more effective white-box attacks, instead of black-box ones.

In the image recognition domain, conﬁdentiality attacks have been conducted by querying the target
model. Tram`er et al. (2016) formed a query-efﬁcient gray-box (the classiﬁer type should be known) score-
based attack. The attack used equation solving to recover the model’s weights from sets of observed sample-
conﬁdence score pairs (x, h(x)) retrieved by querying the target model. For instance, a set of n such pairs is
enough to retrieve the n weights of a logistic regression classiﬁer using n-dimensional input vectors. Wang
and Gong (2018) used a similar approach to retrieve the model’s hyperparameters (e.g., the factor of the
regularization term in the loss function equation).

In non-cyber domains, conﬁdentiality attacks have also been conducted via side-channel information.
Duddu et al. (2018) used timing attack side-channels to obtain neural network architecture information.
Batina et al. (2019) used electromagnetic radiation to get neural network architecture information from
embedded devices. Hua et al. (2018) used both a timing side-channel and off-chip memory access attempts
during inference to discover on-chip CNN model architecture.

7.4 Perturbed Features Gap: Exploiting Vulnerabilities in Machine and Deep Learn-

ing Frameworks

In non-ML based cyber security solutions, vulnerabilities are commonly used to help attackers reach their
goals (e.g., use buffer overﬂows to run adversary-crafted code in a vulnerable process). A vulnerability in the
underlying application or operating system is a common attack vector for disabling or bypassing a security
product. This trend is starting to be seen in the adversarial learning domain against image classiﬁers, however
such attacks have not been demonstrated against malware classiﬁers. Such vulnerabilities are specialized,
and should be researched explicitly for the proper cyber use cases in order to be effective.

In the image recognition domain, Xiao et al. (2018) discovered security vulnerabilities in popular deep
learning frameworks (Caffe, TensorFlow, and Torch). Stevens et al. (2017) did the same for OpenCV (a com-
puter vision framework), scikit-learn (a machine learning framework), and Malheur Rieck et al. (2011) (a
dynamic analysis framework used to classify unknown malware using a clustering algorithm). By exploiting
these frameworks’ implementations, attackers can launch denial-of-service attacks that crash or hang a deep
learning application, or control-ﬂow hijacking attacks that lead to either system compromise or recognition
evasions.

We believe that future adversarial attacks can view the deep learning framework used to train and run
the model as a new part of the attack surface, leveraging vulnerabilities in the framework, which can even
be detected automatically by a machine learning model (as reviewed in Ghaffarian and Shahriari (2017)).
Some of these vulnerabilities can be used to add data to input in a way that would cause the input to be

Accepted as a long survey paper at ACM CSUR 2021

misclassiﬁed, just as adversarial perturbation would, but by subverting the framework instead of subverting
the algorithm. This can be viewed as an extension of the perturbed features attack characteristics in our
taxonomy.

7.5 Attack’s Output Gap: End-to-End Attacks in Complex Format Sub Domains

As discussed in Section 4.7, only end-to-end attacks can be used to attack systems in the cyber security
domain. Some sub-domains, such as email, have a simple format and therefore it is easier to map from
features (words) back to a sample (email) and create an end-to-end attack. However, in more complex sub-
domains, such as NIDS and CPS, the mapping from features to a full sample (e.g., a network stream) is
complex. As can be seen in Sections 5.3 and 5.5, only a small number of attacks in the NIDS and CPS
sub-domains (less than 10%) are end-to-end attacks.

We predict that the trend seen in the computer vision domain, where after several years of feature vector
attacks (e.g., Goodfellow et al. (2015)), end-to-end attacks followed Eykholt et al. (2018), will also be seen
in the cyber security domain. There will likely be three directions for such end-to-end attacks: 1) Adding
new features to an existing sample, e.g., Li et al. (2020b); Rosenberg et al. (2018); Srndic and Laskov (2014).
2) Modifying only a subset of features that can be modiﬁed without harming the functionality of an existing
sample, e.g., Rosenberg et al. (2020a); Kuppa et al. (2019). 3) Using cross-sample transformations (e.g.,
packing) that would change many features simultaneously Rosenberg et al. (2020a); Anderson et al. (2018).

7.6 Adversarial Defense Method Gaps

The gaps in the domain of defense methods against adversarial attacks in the cyber security domain is acute,
because this domain involves actual adversaries: malware developers who want to evade next generation
machine and deep learning-based classiﬁers. Such attacks have already been executed in the wild against
static analysis deep neural networks Cyl ([n.d.]). We have mapped two gaps speciﬁc to the cyber security
domain, which are shown below.

7.6.1 Metrics to Measure the Robustness of Classiﬁers to Adversarial Examples

Several papers (Katz et al. (2017); Weng et al. (2018); Peck et al. (2017)) suggested measuring the robustness
of machine learning systems to adversarial attacks by approximating the lower bound on the perturbation
needed for any adversarial attack to succeed; the larger the perturbation, the more robust the classiﬁer.
However, these papers assume that the robustness to adversarial attacks can be evaluated by the minimal
perturbation required to modify the classiﬁer’s decision. This raises the question of whether this metric is
valid in the cyber security domain.

Section 2.2 leads us to the conclusion that minimal perturbation is not necessarily the right approach
for adversarial learning in the cyber security domain. As already mentioned in Biggio and Roli (2018),
maximum conﬁdence attacks, such as the Carlini and Wagner (C&W) attack (Section 3), are more effective.
However, this is not the complete picture.

As mentioned in Section 2.3, in the cyber security domain, classiﬁers usually use more than a single
feature type as input (e.g., both PE header metadata and byte entropy in Saxe and Berlin (2015)). Certain
feature types are easier to modify without harming the executable functionality than others. On the other
hand, an attacker can add as many strings as needed; in contrast to images, adding more strings (i.e., a larger
perturbation) is not more visible to the user than adding less strings, since the executable ﬁle is still a binary
ﬁle.

Accepted as a long survey paper at ACM CSUR 2021

This means that we should not only take into account the impact of a feature on the prediction, but also
the difﬁculty of modifying this feature type. Unfortunately, there is currently no numeric metric to assess
the difﬁculty of modifying features. Currently, we must rely on the subjective opinion of experts who assess
the difﬁculty of modifying each feature type, as shown in Katzir and Elovici Katzir and Elovici (2018).
When such a metric becomes available, combining it with the maximum impact metric would be a better
optimization constraint than minimal perturbation.

In conclusion, both from an adversary’s perspective (when trying to decide which attack to use) and from
the defender’s perspective (when trying to decide which classiﬁer would be the most robust to adversarial
attack), the metric of evaluation currently remains an open question in the cyber security domain.

7.6.2 Defense Methods Combining Domain-Speciﬁc and Domain-Agnostic Techniques

Domain-speciﬁc constraints and properties have been leveraged in the computer vision domain. For instance,
the feature squeezing method mentioned in Xu et al. (2018) applied image-speciﬁc dimensionality reduction
transformations to the input features, such as changing the image color depth (e.g., from 24 bit to 8 bit), in
order to detect adversarial examples in images.

We argue that the same approach can be applied in the cyber security domain, as well. In the cyber
security domain, there are very complex constraints on the raw input. For instance, a PE ﬁle has a very
strict format (in order to be loadable by the Windows operating system), with severe limitations on the
PE header metadata values, etc. Such constraints can be leveraged to detect “domain-speciﬁc anomalies”
more accurately. For example, the attacks used to evade malware classiﬁers in Rosenberg and Meir (2020);
Rosenberg et al. (2020a) involved concatenation of strings from the IAT to the EOF/PE overlay; while such
concatenation generates a valid PE, one might ask whether it makes sense for such strings to be appended to
the EOF instead of, e.g., being in the data section or a resource section.

7.6.3 Defense Methods Robust to Unknown and Transparent-Box Adversarial Attacks

There are two main challenges when developing a defense method:

The ﬁrst challenge is creating a defense method which is also robust against transparent-box attacks, i.e.,

attackers who know what defense methods are being used and select their attack methods accordingly.

In the computer vision domain, Tramer et al. (2020); Carlini and Wagner (2017a) showed that many
different types of commonly used defense methods (e.g., detection of adversarial examples using statistical
irregularities) are rendered useless by a speciﬁc type of adversarial attack. He et al. (2017) showed the same
for feature squeezing, and Athalye et al. (2018); Hashemi et al. (2018); Ilyas et al. (2017) presented similar
results against Defense-GAN.

Similar research should be conducted in the cyber security domain. For instance, attackers can make
their attack more robust against RNN subsequence model ensembles presented in Rosenberg et al. (2019)
by adding perturbations across the entire API call sequence and not just until the classiﬁcation changes.

The second challenge is creating defense methods that are effective against all attacks and not just spe-
ciﬁc ones, termed attack-agnostic defense methods in Rosenberg et al. (2019). However, the challenge of
ﬁnding a metric to evaluate the robustness of classiﬁers to adversarial attacks in the cyber security domain,
already discussed in Section 7.6.1, makes the implementation of attack-agnostic defense methods in the
cyber security domain more challenging than in other domains.

Accepted as a long survey paper at ACM CSUR 2021

8 Conclusions

In this paper, we reviewed the latest research on a wide range of adversarial learning attacks and defenses in
the cyber security domain (e.g., malware detection, network intrusion detection, etc.).

One conclusion based on our review is that while feature vector adversarial attacks in the cyber security
domain are possible, real-life attacks (e.g., against next generation antivirus software) are challenging. This
is due to the unique challenges that attackers and defenders are faced with in the cyber security domain; these
challenges include the difﬁculty of modifying samples end-to-end without damaging the malicious business
logic and the need to modify many feature types with various levels of modiﬁcation difﬁculty.

From the gaps in our taxonomy discussed above and from the recent advancements in other domains
of adversarial learning, we identiﬁed some directions for future research in adversarial learning in the cy-
ber security domain. One direction focuses on the implementation of feature-triggered attacks that would
work only if a certain trigger exists, leaving the system’s integrity unharmed in other cases, thus making it
harder to detect the attack. Another possible direction is performing conﬁdentiality attacks involving model
reversing via queries or side-channels. A third research direction aims at expanding the attack surface of
adversarial attacks to include the vulnerabilities in the relevant machine learning framework and designing
machine learning models to detect and leverage them. From the defender’s point of view, more robust de-
fense methods against adversarial attacks in the cyber security domain should be the focus of future research.
A ﬁnal conclusion derived from our review is that adversarial learning in the cyber security domain
is becoming more and more similar to the cat and mouse game played in the traditional cyber security
domain, in which the attackers implement more sophisticated attacks to evade the defenders and vice versa.
A key takeaway is that defenders should become more proactive in assessing their system’s robustness to
adversarial attacks, the same way penetration testing is proactively used in the traditional cyber security
domain.

References

[n.d.].

Cylance,

I Kill You!

https://skylightcyber.com/2019/07/18/

cylance-i-kill-you. Accessed: 2019-08-24.

2018a. 25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, Cali-
fornia, USA, February 18-21, 2018. The Internet Society. https://www.ndss-symposium.org/
ndss2018/

2018b. 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.
net/group?id=ICLR.cc/2018/Conference

Ahmed Abusnaina, Aminollah Khormali, Hisham Alasmary, Jeman Park, Afsah Anwar, and Aziz Mohaisen.
2019. Adversarial Learning Attacks on Graph-based IoT Malware Detection Systems. In 39th IEEE
International Conference on Distributed Computing Systems, ICDCS 2019, Dallas, TX, USA, July 7,
Vol. 10. 2019.

Naveed Akhtar and Ajmal S. Mian. 2018. Threat of Adversarial Attacks on Deep Learning in Computer
Vision: A Survey. IEEE Access 6 (2018), 14410–14430. https://doi.org/10.1109/ACCESS.
2018.2807385

Accepted as a long survey paper at ACM CSUR 2021

Ahmed AlEroud and George Karabatis. 2020. Bypassing Detection of URL-based Phishing Attacks Using
Generative Adversarial Deep Neural Networks. In Proceedings of the Sixth International Workshop on
Security and Privacy Analytics. 53–60.

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei Chang.
2018. Generating Natural Language Adversarial Examples. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018,
Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for Computational
Linguistics, 2890–2896. https://aclanthology.info/papers/D18-1316/d18-1316

A. Anand, K. Gorde, J. R. Antony Moniz, N. Park, T. Chakraborty, and B. Chu. 2018. Phishing URL De-
tection with Oversampling based on Text Generative Adversarial Networks. In 2018 IEEE International
Conference on Big Data (Big Data). 1168–1177. https://doi.org/10.1109/BigData.2018.
8622547

Hyrum S. Anderson, Anant Kharkar, Bobby Filar, David Evans, and Phil Roth. 2018. Learning to Evade
Static PE Machine Learning Malware Models via Reinforcement Learning. CoRR abs/1801.08917 (2018).
arXiv:1801.08917 http://arxiv.org/abs/1801.08917

Hyrum S. Anderson, Anant Kharkar, Bobby Filar, and Phil Roth. 2017. Evading Machine Learning Malware

Detection. In Black Hat US.

Hyrum S. Anderson and Phil Roth. 2018. EMBER: An Open Dataset for Training Static PE Malware
Machine Learning Models. CoRR abs/1804.04637 (2018). arXiv:1804.04637 http://arxiv.org/
abs/1804.04637

Hyrum S. Anderson, Jonathan Woodbridge, and Bobby Filar. 2016. DeepDGA: Adversarially-Tuned Do-
main Generation and Detection. In Proceedings of the 2016 ACM Workshop on Artiﬁcial Intelligence
and Security, AISec@CCS 2016, Vienna, Austria, October 28, 2016, David Mandell Freeman, Aikaterini
Mitrokotsa, and Arunesh Sinha (Eds.). ACM, 13–21. https://doi.org/10.1145/2996758.
2996767

Manos Antonakakis, Roberto Perdisci, Yacin Nadji, Nikolaos Vasiloglou, Saeed Abu-Nimeh, Wenke Lee,
and David Dagon. 2012. From Throw-Away Trafﬁc to Bots: Detecting the Rise of DGA-Based Mal-
ware. In Presented as part of the 21st USENIX Security Symposium (USENIX Security 12). USENIX,
https://www.usenix.org/conference/usenixsecurity12/
Bellevue, WA, 491–506.
technical-sessions/presentation/antonakakis

Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon, and Konrad Rieck. 2014. DREBIN:
Effective and Explainable Detection of Android Malware in Your Pocket. In 21st Annual Network
and Distributed System Security Symposium, NDSS 2014, San Diego, California, USA, Febru-
https://www.ndss-symposium.org/ndss2014/
ary 23-26, 2014. The Internet Society.
drebin-effective-and-explainable-detection-android-malware-your-pocket

Anish Athalye, Nicholas Carlini, and David A. Wagner. 2018. Obfuscated Gradients Give a False Sense
of Security: Circumventing Defenses to Adversarial Examples, See Dy and Krause (2018), 274–283.
http://proceedings.mlr.press/v80/athalye18a.html

A. C. Bahnsen, E. C. Bohorquez, S. Villegas, J. Vargas, and F. A. Gonz´alez. 2017. Classifying phish-
ing URLs using recurrent neural networks. In 2017 APWG Symposium on Electronic Crime Research
(eCrime). 1–8. https://doi.org/10.1109/ECRIME.2017.7945048

Accepted as a long survey paper at ACM CSUR 2021

Alejandro Correa Bahnsen, Ivan Torroledo, Luis David Camacho, and Sergio Villegas. 2018. DeepPhish :

Simulating Malicious AI.

Marco Barreno, Blaine Nelson, Anthony D. Joseph, and J. D. Tygar. 2010. The security of machine learning.
Machine Learning 81, 2 (2010), 121–148. https://doi.org/10.1007/s10994-010-5188-5

Lejla Batina, Shivam Bhasin, Dirmanto Jap, and Stjepan Picek. 2019. CSI NN: Reverse Engineer-
ing of Neural Network Architectures Through Electromagnetic Side Channel. In 28th USENIX Secu-
rity Symposium (USENIX Security 19). USENIX Association, Santa Clara, CA, 515–532. https:
//www.usenix.org/conference/usenixsecurity19/presentation/batina

Daniel Berman, Anna Buczak, Jeffrey Chavis, and Cherita Corbett. 2019. A Survey of Deep Learn-
ing Methods for Cyber Security. Information 10 (04 2019), 122. https://doi.org/10.3390/
info10040122

Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ˇSrndi´c, Pavel Laskov, Giorgio Gi-
In Machine
https:

acinto, and Fabio Roli. 2013. Evasion Attacks against Machine Learning at Test Time.
Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg, 387–402.
//doi.org/10.1007/978-3-642-40994-3_25

B. Biggio, G. Fumera, and F. Roli. 2014. Security Evaluation of Pattern Classiﬁers under Attack. IEEE
Transactions on Knowledge and Data Engineering 26, 4 (April 2014), 984–996. https://doi.org/
10.1109/TKDE.2013.57

Battista Biggio and Fabio Roli. 2018. Wild Patterns: Ten Years After the Rise of Adversarial Machine
Learning. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Se-
curity, CCS 2018, Toronto, ON, Canada, October 15-19, 2018, David Lie, Mohammad Mannan, Michael
Backes, and XiaoFeng Wang (Eds.). ACM, 2154–2156. https://doi.org/10.1145/3243734.
3264418

Michael Br¨uckner, Christian Kanzow, and Tobias Scheffer. 2012. Static Prediction Games for Adversarial
Learning Problems. J. Mach. Learn. Res. 13, 1 (Sept. 2012), 2617–2654. http://dl.acm.org/
citation.cfm?id=2503308.2503326

Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N.
Chang, Sungbok Lee, and Shrikanth S. Narayanan. 2008. IEMOCAP: interactive emotional dyadic motion
capture database. Lang. Resour. Evaluation 42, 4 (2008), 335–359. https://doi.org/10.1007/
s10579-008-9076-6

Wilson Cai, Anish Doshi, and Rafael Valle. 2018. Attacking Speaker Recognition With Deep Generative

Models. ArXiv abs/1801.02384 (2018).

Nicholas Carlini and David Wagner. 2017a. Adversarial Examples Are Not Easily Detected. In Proceedings
of the 10th ACM Workshop on Artiﬁcial Intelligence and Security - AISec 2017. ACM Press. https:
//doi.org/10.1145/3128572.3140444

Nicholas Carlini and David A. Wagner. 2017b. Towards Evaluating the Robustness of Neural Networks. In
2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017. IEEE
Computer Society, 39–57. https://doi.org/10.1109/SP.2017.49

Accepted as a long survey paper at ACM CSUR 2021

Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. 2018a. EAD: Elastic-Net Attacks to
Deep Neural Networks via Adversarial Examples. In Proceedings of the Thirty-Second AAAI Conference
on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative Applications of Artiﬁcial Intelligence (IAAI-18),
and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans,
Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q. Weinberger (Eds.). AAAI Press,
10–17. https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16893

Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017b. ZOO: Zeroth Order Op-
timization based Black-box Attacks to Deep Neural Networks without Training Substitute Models. In
Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security - AISec 17. ACM Press.
https://doi.org/10.1145/3128572.3140448

Sen Chen, Minhui Xue, Lingling Fan, Shuang Hao, Lihua Xu, Haojin Zhu, and Bo Li. 2018b. Automated
poisoning attacks and defenses in malware detection systems: An adversarial machine learning approach.
Computers & Security 73 (2018), 326 – 344. https://doi.org/10.1016/j.cose.2017.11.
007

Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017a. Targeted Backdoor Attacks on
Deep Learning Systems Using Data Poisoning. CoRR abs/1712.05526 (2017). arXiv:1712.05526 http:
//arxiv.org/abs/1712.05526

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the Properties of
Neural Machine Translation: Encoder–Decoder Approaches. In Proceedings of SSST-8, Eighth Workshop
on Syntax, Semantics and Structure in Statistical Translation. Association for Computational Linguistics.
https://doi.org/10.3115/v1/w14-4012

Simon P. Chung and Aloysius K. Mok. 2006. Allergy attack against automatic signature generation. Lecture
Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes
in Bioinformatics), Vol. 4219 LNCS. Springer Verlag, Germany, 61–80.

George W. Clark, Michael V. Doran, and William Glisson. 2018. A Malicious Attack on the Machine
Learning Policy of a Robotic System. 2018 17th IEEE International Conference On Trust, Security And
Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science
And Engineering (TrustCom/BigDataSE) (2018), 516–521.

Joseph Clements, Yuzhe Yang, Ankur A. Sharma, Hongxin Hu, and Yingjie Lao. 2019. Rallying Ad-
versarial Techniques against Deep Learning for Network Security. CoRR abs/1903.11688 (2019).
arXiv:1903.11688 http://arxiv.org/abs/1903.11688

Hung Dang, Yue Huang, and Ee-Chien Chang. 2017. Evading Classiﬁers by Morphing in the Dark. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS
2017, Dallas, TX, USA, October 30 - November 03, 2017, Bhavani M. Thuraisingham, David Evans,
Tal Malkin, and Dongyan Xu (Eds.). ACM, 119–133. https://doi.org/10.1145/3133956.
3133978

Tianyu Du, Shouling Ji, Jinfeng Li, Qinchen Gu, Ting Wang, and Raheem A. Beyah. 2019. SirenAttack:

Generating Adversarial Audio for End-to-End Acoustic Systems. ArXiv abs/1901.07846 (2019).

Vasisht Duddu, Debasis Samanta, D. Vijay Rao, and Valentina E. Balas. 2018. Stealing Neural Networks
via Timing Side Channels. CoRR abs/1812.11720 (2018). arXiv:1812.11720 http://arxiv.org/
abs/1812.11720

Accepted as a long survey paper at ACM CSUR 2021

Jennifer G. Dy and Andreas Krause (Eds.). 2018. Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018. Proceedings
of Machine Learning Research, Vol. 80. PMLR. http://proceedings.mlr.press/v80/

Alessandro Erba, Riccardo Taormina, Stefano Galelli, Marcello Pogliani, Michele Carminati, Stefano
Zanero, and Nils Ole Tippenhauer. 2019. Real-time Evasion Attacks with Physical Constraints on
Deep Learning-based Anomaly Detectors in Industrial Control Systems. CoRR abs/1907.07487 (2019).
arXiv:1907.07487 http://arxiv.org/abs/1907.07487

Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash,
Tadayoshi Kohno, and Dawn Song. 2018. Robust Physical-World Attacks on Deep Learning Visual
Classiﬁcation. In Computer Vision and Pattern Recognition (CVPR).

Cheng Feng, Tingting Li, Zhanxing Zhu, and Deeph Chana. 2017. A Deep Learning-based Framework for

Conducting Stealthy Attacks in Industrial Control Systems. ArXiv abs/1709.06397 (2017).

J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi. 2018. Black-Box Generation of Adversarial Text Sequences to
Evade Deep Learning Classiﬁers. In 2018 IEEE Security and Privacy Workshops (SPW). 50–56. https:
//doi.org/10.1109/SPW.2018.00016

Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C. Ranasinghe, and Surya Nepal. 2019.
STRIP: A Defence against Trojan Attacks on Deep Neural Networks. In Proceedings of the 35th An-
nual Computer Security Applications Conference (San Juan, Puerto Rico) (ACSAC ’19). Association for
Computing Machinery, New York, NY, USA, 113–125. https://doi.org/10.1145/3359789.
3359790

Seyed Mohammad Ghaffarian and Hamid Reza Shahriari. 2017. Software Vulnerability Analysis and Dis-
covery Using Machine-Learning and Data-Mining Techniques: A Survey. ACM Comput. Surv. 50, 4,
Article 56 (Aug. 2017), 36 pages. https://doi.org/10.1145/3092566

Amin Ghafouri, Yevgeniy Vorobeychik, and Xenofon D. Koutsoukos. 2018. Adversarial Regression for
Detecting Attacks in Cyber-Physical Systems. In Proceedings of the Twenty-Seventh International Joint
Conference on Artiﬁcial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden., J´erˆome Lang
(Ed.). ijcai.org, 3769–3775. https://doi.org/10.24963/ijcai.2018/524

Yuan Gong and Christian Poellabauer. 2017. Crafting Adversarial Examples For Speech Paralinguistics

Applications. ArXiv abs/1711.03280 (2017).

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C.
Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In Advances in Neural Information
Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, Decem-
ber 8-13 2014, Montreal, Quebec, Canada, Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D.
http://papers.nips.cc/paper/
Lawrence, and Kilian Q. Weinberger (Eds.). 2672–2680.
5423-generative-adversarial-nets

I. J. Goodfellow, J. Shlens, and C. Szegedy. 2015. Explaining and Harnessing Adversarial Examples. Inter-

national Conference on Learning Representations (ICLR) (Dec. 2015). arXiv:1412.6572 [stat.ML]

A. Graves and J. Schmidhuber. [n.d.]. Framewise phoneme classiﬁcation with bidirectional LSTM networks.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005. IEEE. https:
//doi.org/10.1109/ijcnn.2005.1556215

Accepted as a long survey paper at ACM CSUR 2021

K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel. 2016. Adversarial Pertur-
ArXiv e-prints (June 2016).

bations Against Deep Neural Networks for Malware Classiﬁcation.
arXiv:1606.04435 [cs.CR]

Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. 2017. Ad-
versarial Examples for Malware Detection. In Computer Security – ESORICS 2017. Springer International
Publishing, 62–79. https://doi.org/10.1007/978-3-319-66399-9_4

Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2019. BadNets: Evaluating Backdooring
Attacks on Deep Neural Networks. IEEE Access 7 (2019), 47230–47244. https://doi.org/10.
1109/ACCESS.2019.2909068

Luiz G. Hafemann, Robert Sabourin, and Luiz Eduardo Soares de Oliveira. 2019. Characterizing and Eval-
uating Adversarial Examples for Ofﬂine Handwritten Signature Veriﬁcation. IEEE Transactions on Infor-
mation Forensics and Security 14 (2019), 2153–2166.

Mohammad Hashemi, Greg Cusack, and Eric Keller. 2018. Stochastic Substitute Training: A Gray-box
Approach to Craft Adversarial Examples Against Gradient Obfuscation Defenses. In Proceedings of the
11th ACM Workshop on Artiﬁcial Intelligence and Security, CCS 2018, Toronto, ON, Canada, October
19, 2018, Sadia Afroz, Battista Biggio, Yuval Elovici, David Freeman, and Asaf Shabtai (Eds.). ACM,
25–36. https://doi.org/10.1145/3270101.3270111

Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. 2017. Adversarial Exam-
ple Defense: Ensembles of Weak Defenses are not Strong. In 11th USENIX Workshop on Offensive
Technologies, WOOT 2017, Vancouver, BC, Canada, August 14-15, 2017., William Enck and Collin
https://www.usenix.org/conference/woot17/
Mulliner (Eds.). USENIX Association.
workshop-program/presentation/he

Sepp Hochreiter and J ˜AŒrgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8

(nov 1997), 1735–1780. https://doi.org/10.1162/neco.1997.9.8.1735

Weiwei Hu and Ying Tan. 2017a. Black-Box Attacks against RNN based Malware Detection Algorithms.
ArXiv e-prints abs/1705.08131 (2017). arXiv:1705.08131 http://arxiv.org/abs/1705.08131

Weiwei Hu and Ying Tan. 2017b. Generating Adversarial Malware Examples for Black-Box Attacks Based
on GAN. ArXiv e-prints abs/1702.05983 (2017). arXiv:1702.05983 http://arxiv.org/abs/
1702.05983

Weizhe Hua, Zhiru Zhang, and G. Edward Suh. 2018. Reverse Engineering Convolutional Neural Net-
works Through Side-channel Information Leaks. In Proceedings of the 55th Annual Design Automa-
tion Conference (San Francisco, California) (DAC ’18). ACM, New York, NY, USA, Article 4, 6 pages.
https://doi.org/10.1145/3195970.3196105

Chi-Hsuan Huang, Tsung-Han Lee, Lin-huang Chang, Jhih-Ren Lin, and Gwoboa Horng. 2019. Adversarial
Attacks on SDN-Based Deep Learning IDS System. In Mobile and Wireless Technology 2018, Kuinam J.
Kim and Hyuncheol Kim (Eds.). Springer Singapore, Singapore, 181–191.

Ling Huang, Anthony D. Joseph, Blaine Nelson, Benjamin I. P. Rubinstein, and J. D. Tygar. 2011. Adver-
sarial machine learning. In Proceedings of the 4th ACM Workshop on Security and Artiﬁcial Intelligence,
AISec 2011, Chicago, IL, USA, October 21, 2011, Yan Chen, Alvaro A. C´ardenas, Rachel Greenstadt,

Accepted as a long survey paper at ACM CSUR 2021

and Benjamin I. P. Rubinstein (Eds.). ACM, 43–58. https://doi.org/10.1145/2046684.
2046692

Olakunle Ibitoye, M. Omair Shaﬁq, and Ashraf Matrawy. 2019. Analyzing Adversarial Attacks Against
Deep Learning for Intrusion Detection in IoT Networks. CoRR abs/1905.05137 (2019). arXiv:1905.05137
http://arxiv.org/abs/1905.05137

Andrew Ilyas, Ajil Jalal, Eirini Asteri, Constantinos Daskalakis, and Alexandros G. Dimakis. 2017. The
Robust Manifold Defense: Adversarial Training using Generative Models. CoRR abs/1712.09196 (2017).
arXiv:1712.09196 http://arxiv.org/abs/1712.09196

Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.
2019. Adversarial examples are not bugs, they are features. arXiv preprint arXiv:1905.02175 (2019).

Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. 2017. Reluplex: An Efﬁ-
cient SMT Solver for Verifying Deep Neural Networks. In Computer Aided Veriﬁcation, Rupak Majumdar
and Viktor Kunˇcak (Eds.). Springer International Publishing, Cham, 97–117.

Ziv Katzir and Yuval Elovici. 2018. Quantifying the resilience of machine learning classiﬁers used for cyber
security. Expert Systems with Applications 92 (2018), 419 – 429. https://doi.org/10.1016/
j.eswa.2017.09.053

G¨unter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. 2017. Self-Normalizing
Neural Networks. In Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, Is-
abelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N.
http://papers.nips.cc/paper/
Vishwanathan, and Roman Garnett (Eds.). 971–980.
6698-self-normalizing-neural-networks

Clemens Kolbitsch, Paolo Milani Comparetti, Christopher Kruegel, Engin Kirda, Xiaoyong Zhou, and Xi-
aoFeng Wang. 2009. Effective and Efﬁcient Malware Detection at the End Host. In Proceedings of the
18th Conference on USENIX Security Symposium (Montreal, Canada) (SSYM’09). USENIX Association,
Berkeley, CA, USA, 351–366. http://dl.acm.org/citation.cfm?id=1855768.1855790

Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto, Claudia Eckert,
and Fabio Roli. 2018. Adversarial Malware Binaries: Evading Deep Learning for Malware Detection in
Executables. In 26th European Signal Processing Conference, EUSIPCO 2018, Roma, Italy, September
3-7, 2018. IEEE, 533–537. https://doi.org/10.23919/EUSIPCO.2018.8553214

Nickolaos Koroniotis, Nour Moustafa, Elena Sitnikova, and Benjamin Turnbull. 2018. Towards the De-
velopment of Realistic Botnet Dataset in the Internet of Things for Network Forensic Analytics: Bot-
IoT Dataset. CoRR abs/1811.00701 (2018). arXiv:1811.00701 http://arxiv.org/abs/1811.
00701

Moshe Kravchik and Asaf Shabtai. 2019. Efﬁcient Cyber Attacks Detection in Industrial Control Sys-
tems Using Lightweight Neural Networks. CoRR abs/1907.01216 (2019). arXiv:1907.01216 http:
//arxiv.org/abs/1907.01216

Felix Kreuk, Yossi Adi, Moustapha Ciss´e, and Joseph Keshet. 2018a. Fooling End-To-End Speaker Veriﬁ-
cation With Adversarial Examples. 2018 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) (2018), 1962–1966.

Accepted as a long survey paper at ACM CSUR 2021

Felix Kreuk, Assi Barak, Shir Aviv-Reuven, Moran Baruch, Benny Pinkas, and Joseph Keshet. 2018b.
Adversarial Examples on Discrete Sequences for Beating Whole-Binary Malware Detection. CoRR
abs/1802.04528 (2018). arXiv:1802.04528 http://arxiv.org/abs/1802.04528

Volodymyr Kuleshov, Shantanu Thakoor, Tingfung Lau, and Stefano Ermon. 2018. Adversarial Exam-
https://openreview.net/forum?id=

ples for Natural Language Classiﬁcation Problems.
r1QZ3zbAZ

Aditya Kuppa, Slawomir Grzonkowski, Muhammad Rizwan Asghar, and Nhien-An Le-Khac. 2019. Black
Box Attacks on Deep Anomaly Detectors. In Proceedings of the 14th International Conference on Avail-
ability, Reliability and Security (Canterbury, CA, United Kingdom) (ARES ’19). ACM, New York, NY,
USA, Article 21, 10 pages. https://doi.org/10.1145/3339252.3339266

A. Kuppa and N. A. Le-Khac. 2020. Black Box Attacks on Explainable Artiﬁcial Intelligence(XAI) methods
in Cyber Security. In 2020 International Joint Conference on Neural Networks (IJCNN). 1–8. https:
//doi.org/10.1109/IJCNN48605.2020.9206780

Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2016. Adversarial examples in the physical world.

ArXiv abs/1607.02533 (2016).

Qi Lei, Lingfei Wu, Pin-Yu Chen, Alexandros G. Dimakis, Inderjit S. Dhillon, and Michael Witbrock.
2018. Discrete Attacks and Submodular Optimization with Applications to Text Classiﬁcation. CoRR
abs/1812.00151 (2018). arXiv:1812.00151 http://arxiv.org/abs/1812.00151

Jiangnan Li, Jin Young Lee, Yingyuan Yang, Jinyuan Stella Sun, and Kevin Tomsovic. 2020a. ConAML:
Constrained Adversarial Machine Learning for Cyber-Physical Systems. arXiv:2003.05631 [cs.CR]

Yuanzhang Li, Yaxiao Wang, Ye Wang, Lishan Ke, and Yu-an Tan. 2020b. A Feature-vector Generative

Adversarial Network for Evading PDF Malware Classiﬁers. Information Sciences (2020).

Zilong Lin, Yong Shi, and Zhi Xue. 2018. IDSGAN: Generative Adversarial Networks for Attack Generation
against Intrusion Detection. CoRR abs/1809.02077 (2018). arXiv:1809.02077 http://arxiv.org/
abs/1809.02077

Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008. Isolation Forest. In Proceedings of the 8th IEEE
International Conference on Data Mining (ICDM 2008), December 15-19, 2008, Pisa, Italy. IEEE Com-
puter Society, 413–422. https://doi.org/10.1109/ICDM.2008.17

Xiaolei Liu, Xiaojiang Du, Xiaosong Zhang, Qingxin Zhu, Hao Wang, and Mohsen Guizani. 2019. Ad-
versarial Samples on Android Malware Detection Systems for IoT Systems. Sensors 19, 4 (2019), 974.
https://doi.org/10.3390/s19040974

Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2017. Delving into Transferable Adversar-
International Conference on Learning Representations (ICLR)

ial Examples and Black-box Attacks.
abs/1611.02770. arXiv:1611.02770 http://arxiv.org/abs/1611.02770

Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang.
2018. Trojaning Attack on Neural Networks. In 25nd Annual Network and Distributed System Security
Symposium, NDSS 2018, San Diego, California, USA, February 18-221, 2018. The Internet Society.

Accepted as a long survey paper at ACM CSUR 2021

Zhengzhe Liu, Xiaojuan Qi, and Philip H.S. Torr. 2020. Global Texture Enhancement for Fake Face Detec-
tion in the Wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion (CVPR).

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018.
https://

Towards Deep Learning Models Resistant to Adversarial Attacks, See DBL (2018b).
openreview.net/forum?id=rJzIBfZAb

Jiang Ming, Zhi Xin, Pengwei Lan, Dinghao Wu, Peng Liu, and Bing Mao. 2015. Replacement At-
tacks: Automatically Impeding Behavior-Based Malware Speciﬁcations. In Applied Cryptography and
Network Security - 13th International Conference, ACNS 2015, New York, NY, USA, June 2-5, 2015,
Revised Selected Papers (Lecture Notes in Computer Science), Tal Malkin, Vladimir Kolesnikov, Al-
https:
lison Bishop Lewko, and Michalis Polychronakis (Eds.), Vol. 9092. Springer, 497–517.
//doi.org/10.1007/978-3-319-28166-7_24

Yisroel Mirsky, Tomer Doitshman, Yuval Elovici, and Asaf Shabtai. 2018. Kitsune: An Ensem-
http:

ble of Autoencoders for Online Network Intrusion Detection, See DBL (2018a).
//wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/
ndss2018_03A-3_Mirsky_paper.pdf

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016. DeepFool: A Simple and
Accurate Method to Fool Deep Neural Networks. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 2574–
2582. https://doi.org/10.1109/CVPR.2016.282

Luis Mu˜noz Gonz´alez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C.
Lupu, and Fabio Roli. 2017. Towards Poisoning of Deep Learning Algorithms with Back-gradient Opti-
mization. In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security (Dallas, Texas,
USA) (AISec ’17). ACM, New York, NY, USA, 27–38. https://doi.org/10.1145/3128572.
3140451

Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D. Joseph, Benjamin I. P. Rubinstein, Udam
Saini, Charles A. Sutton, J. Doug Tygar, and Kai Xia. 2008. Exploiting Machine Learning to Subvert
Your Spam Filter. In First USENIX Workshop on Large-Scale Exploits and Emergent Threats, LEET ’08,
San Francisco, CA, USA, April 15, 2008, Proceedings, Fabian Monrose (Ed.). USENIX Association.
http://www.usenix.org/events/leet08/tech/full_papers/nelson/nelson.pdf

Ozan ¨Ozdenizci, Ye Wang, Toshiaki Koike-Akino, and Deniz Erdogmus. 2019. Adversarial Deep Learning

in EEG Biometrics. IEEE Signal Processing Letters 26 (2019), 710–714.

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami.
2017. Practical Black-Box Attacks Against Machine Learning. In Proceedings of the 2017 ACM on Asia
Conference on Computer and Communications Security (Abu Dhabi, United Arab Emirates) (ASIA CCS
’17). ACM, New York, NY, USA, 506–519. https://doi.org/10.1145/3052973.3053009

Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami.
2016b. The Limitations of Deep Learning in Adversarial Settings. In 2016 IEEE European Symposium
on Security and Privacy (EuroS&P). IEEE. https://doi.org/10.1109/eurosp.2016.36

Accepted as a long survey paper at ACM CSUR 2021

Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. 2016c. Crafting adversarial
input sequences for recurrent neural networks. In MILCOM 2016 - 2016 IEEE Military Communications
Conference. IEEE. https://doi.org/10.1109/milcom.2016.7795300

Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. 2016a. Transferability in Machine Learn-
ing: from Phenomena to Black-Box Attacks using Adversarial Samples. ArXiv e-prints abs/1605.07277
(2016). arXiv:1605.07277 http://arxiv.org/abs/1605.07277

O. M. Parkhi, A. Vedaldi, and A. Zisserman. 2015. Deep Face Recognition. In British Machine Vision

Conference.

Jonathan Peck, Joris Roels, Bart Goossens, and Yvan Saeys. 2017. Lower Bounds on the Robustness to
Adversarial Perturbations. In Proceedings of the 31st International Conference on Neural Information
Processing Systems (Long Beach, California, USA) (NIPS’17). Curran Associates Inc., USA, 804–813.
http://dl.acm.org/citation.cfm?id=3294771.3294848

Vinay Uday Prabhu, UnifyID, and San Dewayne Francisco. 2017. Vulnerability of deep learning-based gait

biometric recognition to adversarial perturbations.

Shilin Qiu, Qihe Liu, Shijie Zhou, and Chunjiang Wu. 2019. Review of Artiﬁcial Intelligence Adversarial
Attack and Defense Technologies. Applied Sciences 9 (03 2019), 909. https://doi.org/10.
3390/app9050909

Alec Radford, Luke Metz, and Soumith Chintala. 2016. Unsupervised Representation Learning with Deep
Convolutional Generative Adversarial Networks. In 4th International Conference on Learning Repre-
sentations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, Yoshua
Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.06434

Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro, and Charles K. Nicholas. 2018.
Malware Detection by Eating a Whole EXE. In The Workshops of the The Thirty-Second AAAI Confer-
ence on Artiﬁcial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018. (AAAI Workshops),
https://aaai.org/ocs/index.php/WS/AAAIW18/
Vol. WS-18. AAAI Press, 268–276.
paper/view/16422

Konrad Rieck, Philipp Trinius, Carsten Willems, and Thorsten Holz. 2011. Automatic analysis of malware
behavior using machine learning. Journal of Computer Security 19, 4 (2011), 639–668. https://
doi.org/10.3233/JCS-2010-0410

Maria Rigaki and Ahmed Elragal. 2017. Adversarial Deep Learning Against Intrusion Detection Classiﬁers.

Gary Robinson. 2003. A statistical approach to the spam problem. Linux Journal 2003 (01 2003).

Ishai Rosenberg and Ehud Gudes. 2016. Bypassing system calls-based intrusion detection systems. Con-
currency and Computation: Practice and Experience 29, 16 (nov 2016), e4023. https://doi.org/
10.1002/cpe.4023

Ishai Rosenberg and Shai Meir. 2020. Bypassing NGAV for Fun and Proﬁt. Black Hat Europe (2020).

Ishai Rosenberg, Shai Meir, Jonathan Berrebi, Ilay Gordon, Guillaume Sicard, and Eli (Omid) David. 2020a.
Generating End-to-End Adversarial Examples for Malware Classiﬁers Using Explainability. In 2020 In-
ternational Joint Conference on Neural Networks (IJCNN). 1–10. https://doi.org/10.1109/
IJCNN48605.2020.9207168

Accepted as a long survey paper at ACM CSUR 2021

Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach. 2019. Defense Methods Against Adversarial
Examples for Recurrent Neural Networks. CoRR abs/1901.09963 (2019). arXiv:1901.09963 http:
//arxiv.org/abs/1901.09963

Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach. 2020b. Query-Efﬁcient Black-Box Attack
Against Sequence-Based Malware Classiﬁers. In ACSAC ’20: Annual Computer Security Applications
Conference, Virtual Event / Austin, TX, USA, 7-11 December, 2020. ACM, 611–626. https://doi.
org/10.1145/3427228.3427230

Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici. 2018. Generic Black-Box End-to-End
Attack Against State of the Art API Call Based Malware Classiﬁers. In Research in Attacks, Intrusions,
and Defenses - 21st International Symposium, RAID 2018, Heraklion, Crete, Greece, September 10-
12, 2018, Proceedings (Lecture Notes in Computer Science), Michael Bailey, Thorsten Holz, Manolis
Stamatogiannakis, and Sotiris Ioannidis (Eds.), Vol. 11050. Springer, 490–510. https://doi.org/
10.1007/978-3-030-00470-5_23

Andrew Ross and Finale Doshi-Velez. 2018. Improving the Adversarial Robustness and Interpretability of
Deep Neural Networks by Regularizing Their Input Gradients. Proceedings of the AAAI Conference on Ar-
tiﬁcial Intelligence 32, 1 (Apr. 2018). https://ojs.aaai.org/index.php/AAAI/article/
view/11504

Lukas Ruff, Nico G¨ornitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Robert A. Vandermeulen, Alexander
Binder, Emmanuel M¨uller, and Marius Kloft. 2018. Deep One-Class Classiﬁcation, See Dy and Krause
(2018), 4390–4399. http://proceedings.mlr.press/v80/ruff18a.html

Joshua Saxe and Konstantin Berlin. 2015. Deep neural network based malware detection using two di-
mensional binary program features. In 2015 10th International Conference on Malicious and Unwanted
Software (MALWARE). IEEE. https://doi.org/10.1109/malware.2015.7413680

Stefano Schiavoni, Federico Maggi, Lorenzo Cavallaro, and Stefano Zanero. 2014. Phoenix: DGA-Based
Botnet Tracking and Intelligence. In Detection of Intrusions and Malware, and Vulnerability Assessment,
Sven Dietrich (Ed.). Springer International Publishing, Cham, 192–211.

Thomas Schlegl, Philipp Seeb¨ock, Sebastian M. Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. 2017.
Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery. In
Information Processing in Medical Imaging - 25th International Conference, IPMI 2017, Boone, NC,
USA, June 25-30, 2017, Proceedings (Lecture Notes in Computer Science), Marc Niethammer, Mar-
tin Styner, Stephen R. Aylward, Hongtu Zhu, Ipek Oguz, Pew-Thian Yap, and Dinggang Shen (Eds.),
Vol. 10265. Springer, 146–157. https://doi.org/10.1007/978-3-319-59050-9_12

Bernhard Sch¨olkopf, Robert C. Williamson, Alexander J. Smola, John Shawe-Taylor, and John C. Platt.
1999. Support Vector Method for Novelty Detection. In Advances in Neural Information Processing
Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 - December 4, 1999], Sara A.
Solla, Todd K. Leen, and Klaus-Robert M¨uller (Eds.). The MIT Press, 582–588. http://papers.
nips.cc/paper/1723-support-vector-method-for-novelty-detection

M. Schuster and K.K. Paliwal. 1997. Bidirectional recurrent neural networks. IEEE Transactions on Signal

Processing 45, 11 (1997), 2673–2681. https://doi.org/10.1109/78.650093

Accepted as a long survey paper at ACM CSUR 2021

Tegjyot Singh Sethi and Mehmed Kantardzic. 2018. Data driven exploratory attacks on black box classiﬁers
in adversarial domains. Neurocomputing 289 (2018), 129 – 143. https://doi.org/10.1016/j.
neucom.2018.02.007

Iman Sharafaldin, Arash Habibi Lashkari, and Ali Ghorbani. 2018. Toward Generating a New Intrusion
Detection Dataset and Intrusion Trafﬁc Characterization. 108–116. https://doi.org/10.5220/
0006639801080116

Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. 2016. Accessorize to a Crime: Real
and Stealthy Attacks on State-of-the-Art Face Recognition. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security (Vienna, Austria) (CCS ’16). ACM, New York,
NY, USA, 1528–1540. https://doi.org/10.1145/2976749.2978392

Hossein Shirazi, Bruhadeshwar Bezawada, Indrakshi Ray, and Charles Anderson. 2019.

Adversar-
https://doi.org/10.1007/

ial Sampling Attacks Against Phishing Detection.
978-3-030-22479-0_5

83–101.

Ilia Shumailov, Yiren Zhao, Daniel Bates, Nicolas Papernot, Robert Mullins, and Ross Anderson. 2020.

Sponge Examples: Energy-Latency Attacks on Neural Networks. arXiv:2006.03463 [cs.LG]

Lior Sidi, Asaf Nadler, and Asaf Shabtai. 2019. MaskDGA: A Black-box Evasion Technique Against
DGA Classiﬁers and Adversarial Defenses. CoRR abs/1902.08909 (2019). arXiv:1902.08909 http:
//arxiv.org/abs/1902.08909

Sobhan Soleymani, Ali Dabouei, J. Dawson, and N.M. Nasrabadi. 2019a. Defending Against Adversarial

Iris Examples Using Wavelet Decomposition. (08 2019).

Sobhan Soleymani, Ali Dabouei, Jeremy Dawson, and Nasser M. Nasrabadi. 2019b. Adversarial Examples

to Fool Iris Recognition Systems. ArXiv abs/1906.09300 (2019).

Felix Specht, Jens Otto, Oliver Niggemann, and Barbara Hammer. 2018. Generation of Adversarial Ex-
amples to Prevent Misclassiﬁcation of Deep Neural Network based Condition Monitoring Systems for
Cyber-Physical Production Systems. In 16th IEEE International Conference on Industrial Informatics,
INDIN 2018, Porto, Portugal, July 18-20, 2018. IEEE, 760–765. https://doi.org/10.1109/
INDIN.2018.8472060

Nedim Srndic and Pavel Laskov. 2014. Practical Evasion of a Learning-Based Classiﬁer: A Case Study. In
2014 IEEE Symposium on Security and Privacy, SP 2014, Berkeley, CA, USA, May 18-21, 2014. IEEE
Computer Society, 197–211. https://doi.org/10.1109/SP.2014.20

Rock Stevens, Octavian Suciu, Andrew Ruef, Sanghyun Hong, Michael W. Hicks, and Tudor Dumitras.
2017. Summoning Demons: The Pursuit of Exploitable Bugs in Machine Learning. CoRR abs/1701.04739
(2017). arXiv:1701.04739 http://arxiv.org/abs/1701.04739

Jack W. Stokes, De Wang, Mady Marinescu, Marc Marino, and Brian Bussone. 2017. Attack and Defense
of Dynamic Analysis-Based, Adversarial Neural Malware Classiﬁcation Models. CoRR abs/1712.05919
(2017). arXiv:1712.05919 http://arxiv.org/abs/1712.05919

Octavian Suciu, Scott E. Coull, and Jeffrey Johns. 2018a. Exploring Adversarial Examples in Malware De-
tection. In Proceedings of the AAAI Symposium on Adversary-Aware Learning Techniques and Trends in

Accepted as a long survey paper at ACM CSUR 2021

Cybersecurity (ALEC 2018) co-located with the Association for the Advancement of Artiﬁcial Intelligence
2018 Fall Symposium Series (AAAI-FSS 2018), Arlington, Virginia, USA, October 18-20, 2018. (CEUR
Workshop Proceedings), Joseph B. Collins, Prithviraj Dasgupta, and Ranjeev Mittu (Eds.), Vol. 2269.
CEUR-WS.org, 11–16. http://ceur-ws.org/Vol-2269/FSS-18_paper_44.pdf

Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor Dumitras. 2018b. When Does
Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks. In 27th USENIX
Security Symposium (USENIX Security 18). USENIX Association, Baltimore, MD, 1299–1316. https:
//www.usenix.org/conference/usenixsecurity18/presentation/suciu

Yi Sun, Xiaogang Wang, and Xiaoou Tang. 2014. Deep Learning Face Representation from Predicting
10,000 Classes. In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR ’14). IEEE Computer Society, Washington, DC, USA, 1891–1898. https://doi.org/10.
1109/CVPR.2014.244

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and
Rob Fergus. 2014. Intriguing properties of neural networks. International Conference on Learning Rep-
resentations (ICLR) abs/1312.6199. arXiv:1312.6199 http://arxiv.org/abs/1312.6199

Shayan Taheri, Milad Salem, and Jiann-Shiun Yuan. 2019. RazorNet: Adversarial Training and Noise
Training on a Deep Neural Network Fooled by a Shallow Neural Network. Big Data and Cognitive
Computing 3 (07 2019), 43. https://doi.org/10.3390/bdcc3030043

Mahbod Tavallaee, Ebrahim Bagheri, Wei Lu, and Ali A. Ghorbani. 2009. A Detailed Analysis of the
KDD CUP 99 Data Set. In Proceedings of the Second IEEE International Conference on Computational
Intelligence for Security and Defense Applications (Ottawa, Ontario, Canada) (CISDA’09). IEEE Press,
Piscataway, NJ, USA, 53–58. http://dl.acm.org/citation.cfm?id=1736481.1736489

Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. 2020. On Adaptive Attacks to

Adversarial Example Defenses. arXiv:2002.08347 [cs.LG]

Florian Tram`er, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. 2016. Stealing Ma-
chine Learning Models via Prediction APIs. In 25th USENIX Security Symposium, USENIX Secu-
rity 16, Austin, TX, USA, August 10-12, 2016., Thorsten Holz and Stefan Savage (Eds.). USENIX
https://www.usenix.org/conference/usenixsecurity16/
Association, 601–618.
technical-sessions/presentation/tramer

Martino Trevisan and Idilio Drago. 2019. Robust URL Classiﬁcation With Generative Adversarial Net-
works. SIGMETRICS Perform. Eval. Rev. 46, 3 (Jan. 2019), 143–146. https://doi.org/10.
1145/3308897.3308959

Rakesh Verma and Keith Dyer. 2015. On the Character of Phishing URLs: Accurate and Robust Statistical
Learning Classiﬁers. In Proceedings of the 5th ACM Conference on Data and Application Security and
Privacy (San Antonio, Texas, USA) (CODASPY ’15). ACM, New York, NY, USA, 111–122. https:
//doi.org/10.1145/2699026.2699115

B. Wang and N. Z. Gong. 2018. Stealing Hyperparameters in Machine Learning. In 2018 IEEE Symposium

on Security and Privacy (SP). 36–52. https://doi.org/10.1109/SP.2018.00038

Accepted as a long survey paper at ACM CSUR 2021

Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y. Zhao.
2019. Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. In 2019 IEEE
Symposium on Security and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019. IEEE, 707–
723. https://doi.org/10.1109/SP.2019.00031

Bolun Wang, Yuanshun Yao, Bimal Viswanath, Haitao Zheng, and Ben Y. Zhao. 2018. With Great Train-
ing Comes Great Vulnerability: Practical Attacks against Transfer Learning. In 27th USENIX Secu-
rity Symposium (USENIX Security 18). USENIX Association, Baltimore, MD, 1281–1297. https:
//www.usenix.org/conference/usenixsecurity18/presentation/wang-bolun

Z. Wang. 2018. Deep Learning-Based Intrusion Detection With Adversaries. IEEE Access 6 (2018), 38367–

38384. https://doi.org/10.1109/ACCESS.2018.2854599

Arkadiusz Warzynski and Grzegorz Kolaczek. 2018. Intrusion detection systems vulnerability on adversar-
ial examples. In 2018 Innovations in Intelligent Systems and Applications, INISTA 2018, Thessaloniki,
Greece, July 3-5, 2018. IEEE, 1–4. https://doi.org/10.1109/INISTA.2018.8466271

Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca
Daniel. 2018. Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach. In
International Conference on Learning Representations. https://openreview.net/forum?id=
BkUHlMZ0b

Q. Xiao, K. Li, D. Zhang, and W. Xu. 2018. Security Risks in Deep Learning Implementations. In 2018 IEEE
https://doi.org/10.1109/SPW.2018.

Security and Privacy Workshops (SPW). 123–128.
00027

Peng Xu, Bojan Kolosnjaji, Claudia Eckert, and Apostolis Zarras. 2020. MANIS: evading malware detection
system on graph structure. In Proceedings of the 35th Annual ACM Symposium on Applied Computing.
1688–1695.

Weilin Xu, David Evans, and Yanjun Qi. 2018. Feature Squeezing: Detecting Adversarial Examples
http://wp.internetsociety.org/ndss/

in Deep Neural Networks, See DBL (2018a).
wp-content/uploads/sites/25/2018/02/ndss2018_03A-4_Xu_paper.pdf

Weilin Xu, Yanjun Qi, and David Evans. 2016.

Automatically Evading Classiﬁers: A Case
Study on PDF Malware Classiﬁers. In 23rd Annual Network and Distributed System Security
Symposium, NDSS 2016, San Diego, California, USA, February 21-24, 2016. The Internet So-
http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/
ciety.
2017/09/automatically-evading-classifiers.pdf

S. Yadav, A. K. K. Reddy, A. L. N. Reddy, and S. Ranjan. 2012. Detecting Algorithmically Generated
Domain-Flux Attacks With DNS Trafﬁc Analysis. IEEE/ACM Transactions on Networking 20, 5 (Oct
2012), 1663–1677. https://doi.org/10.1109/TNET.2012.2184552

Sandeep Yadav, Ashwath Kumar Krishna Reddy, A.L. Narasimha Reddy, and Supranamaya Ranjan. 2010.
Detecting Algorithmically Generated Malicious Domain Names. In Proceedings of the 10th ACM SIG-
COMM Conference on Internet Measurement (Melbourne, Australia) (IMC ’10). ACM, New York, NY,
USA, 48–61. https://doi.org/10.1145/1879141.1879148

Accepted as a long survey paper at ACM CSUR 2021

Shakiba Yaghoubi and Georgios Fainekos. 2019. Gray-box Adversarial Testing for Control Systems with
Machine Learning Components. In Proceedings of the 22Nd ACM International Conference on Hybrid
Systems: Computation and Control (Montreal, Quebec, Canada) (HSCC ’19). ACM, New York, NY,
USA, 179–184. https://doi.org/10.1145/3302504.3311814

K. Yang, J. Liu, C. Zhang, and Y. Fang. 2018. Adversarial Examples Against the Deep Learning Based Net-
work Intrusion Detection Systems. In MILCOM 2018 - 2018 IEEE Military Communications Conference
(MILCOM). 559–564. https://doi.org/10.1109/MILCOM.2018.8599759

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. SeqGAN: Sequence Generative Adver-
sarial Nets with Policy Gradient. In Proceedings of the Thirty-First AAAI Conference on Artiﬁcial
Intelligence, February 4-9, 2017, San Francisco, California, USA., Satinder P. Singh and Shaul
http://aaai.org/ocs/index.php/AAAI/
Markovitch (Eds.). AAAI Press, 2852–2858.
AAAI17/paper/view/14344

Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar. 2018. Ad-
versarially Learned Anomaly Detection. In IEEE International Conference on Data Mining, ICDM 2018,
https://doi.org/10.
Singapore, November 17-20, 2018. IEEE Computer Society, 727–736.
1109/ICDM.2018.00088

Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Dae-ki Cho, and Haifeng Chen.
2018. Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection, See DBL
(2018b). https://openreview.net/forum?id=BJJLHbb0-

Accepted as a long survey paper at ACM CSUR 2021

A Deep Learning Classiﬁers: Mathematical and Technical Background

A.1 Deep Neural Networks (DNNs)

Neural networks are a class of machine learning models made up of layers of neurons (elementary computing
units).

A neuron takes an n-dimensional feature vector x = [x1, x2...xn] from the input or the lower level neuron

and outputs a numerical output y = [y1, y2...ym], such that

y j = φ (∑n

i=1

w jixi + b j)

(15)

to the neurons in higher layers or the output layer. For the neuron j, y j is the output and b j is the bias term,
while w ji are the elements of a layer’s weight matrix. The function φ is the nonlinear activation function,
such as sigmoid(), which determines the neuron’s output. The activation function introduces nonlinearities
to the neural network model. Otherwise, the network remains a linear transformation of its input signals.
Some of the success of DNNs is attributed to these multi-layers of nonlinear correlations between features,
which are not available in popular traditional machine learning classiﬁers, such as SVM, which has at most
a single nonlinear layer using the kernel trick.

A group of m neurons forms a hidden layer which outputs a feature vector y. Each hidden layer takes
the previous layer’s output vector as the input feature vector and calculates a new feature vector for the layer
above it:

yl = φ (W lyl−1 + bl)

(16)

where yl, W l and bl are the output feature vector, the weight matrix, and the bias of the l-th layer, respectively.
Proceeding from the input layer, each subsequent higher hidden layer automatically learns a more complex
and abstract feature representation which captures a higher level structure.

A.1.1 Convolutional Neural Networks (CNNs)

CNNs are a type of DNN. Let xi be the k-dimensional vector corresponding to the i-th element in the
sequence. A sequence of length n (padded when necessary) is represented as: x[0 : n − 1] = x[0] ⊥ x[1] ⊥
x[n − 1], where ⊥ is the concatenation operator.
In general, let x[i : i + j] refer to the concatenation of
words x[i], x[i + 1], ..., x[i + j] . A convolution operation involves a ﬁlter w, which is applied to a window
of h elements to produce a new feature. For example, a feature ci is generated from a window of words
x[i : i + h − 1] by:

ci = φ (W x[i : i + h] + b)

(17)

where b is the bias term and φ is the activation function. This ﬁlter is applied to each possible win-
dow of elements in the sequence {x[0 : h − 1], x[1 : h], ..., x[n − h : n − 1]} to produce a feature map: c =
[c0, c1, ..., cn−h]. We then apply a max over time pooling operation over the feature map and take the maxi-
mum value: ˆc = max(c) as the feature corresponding to this particular ﬁlter. The idea is to capture the most
important feature (the one with the highest value) for each feature map.

We described the process by which one feature is extracted from one ﬁlter above. The CNN model uses
multiple ﬁlters (with varying window sizes) to obtain multiple features. These features form the penultimate

Accepted as a long survey paper at ACM CSUR 2021

layer and are passed to a fully connected softmax layer whose output is the probability distribution over
labels.

CNNs have two main differences from fully connected DNNs:

1. CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent
layers. The architecture thus ensures that the learned ”ﬁlters” produce the strongest response to a
spatially local input pattern. Stacking many such layers leads to nonlinear “ﬁlters” that become in-
creasingly “global.” This allows the network to ﬁrst create representations of small parts of the input
and assemble representations of larger areas from them.

2. In CNNs, each ﬁlter is replicated across the entire input. These replicated units share the same param-
eterization (weight, vector, and bias) and form a feature map. This means that all of the neurons in a
given convolutional layer respond to the same feature (within their speciﬁc response ﬁeld). Replicat-
ing units in this way allows for features to be detected regardless of their position in the input, thus
constituting the property of translation invariance. This property is important in both vision problems
and with sequence input, such as API call traces.

A.2 Recurrent Neural Networks (RNNs)

A limitation of neural networks is that they accept a ﬁxed sized vector as input (e.g., an image) and produce
a ﬁxed sized vector as output (e.g., probabilities of different classes). Recurrent neural networks can use
sequences of vectors in the input, output, or both. In order to do that, the RNN has a hidden state vector, the
context of the sequence, which is combined with the current input to generate the RNN’s output.

Given an input sequence [x1, x2...xT ], the RNN computes the hidden vector sequence[h1, h2...hT ] and the

output vector sequence [y1, y2...yT ] by iterating the following equations from t = 1 to T :

ht = φ (W xhxt +W hhxt−1 + bh)

yt = W hyht + bo

(18)

(19)

where the W terms denote weight matrices (e.g., W xh is the input hidden weight matrix), the b terms denote
bias vectors (e.g., bh is the hidden bias vector), and φ is usually an element-wise application of an activation
function. DNNs without a hidden state, as speciﬁed in Equation 18, reduce Equation 19 to the private case
of Equation 16, known as feedforward networks.

A.2.1 Long Short-Term Memory (LSTM)

Standard RNNs suffer from both exploding and vanishing gradients. Both problems are caused by the
RNNs’ iterative nature, in which the gradient is essentially equal to the recurrent weight matrix raised to a
high power. These iterated matrix powers cause the gradient to grow or shrink at a rate that is exponential
in terms of the number of time steps T . The vanishing gradient problem does not necessarily cause the
gradient to be small; the gradient’s components in directions that correspond to long-term dependencies
might be small, while the gradient’s components in directions that correspond to short-term dependencies is
large. As a result, RNNs can easily learn the short-term but not the long-term dependencies. For instance,
a conventional RNN might have problems predicting the last word in: “I grew up in France...I speak ﬂuent
French” if the gap between the sentences is large.

Accepted as a long survey paper at ACM CSUR 2021

The LSTM architecture (Hochreiter and Schmidhuber (1997)), which uses purpose-built memory cells
to store information, is better at ﬁnding and exploiting long-range context than conventional RNNs. The
LSTM’s main idea is that instead of computing ht from ht−1 directly with a matrix-vector product followed
by a nonlinear transformation (Equation 18), the LSTM directly computes (cid:52)ht , which is then added to ht−1
to obtain ht . This implies that the gradient of the long-term dependencies cannot vanish.

A.2.2 Gated Recurrent Unit (GRU)

Introduced in Cho et al. (2014), the gated recurrent unit (GRU) is an architecture that is similar to LSTM, but
reduces the gating signals from three (in the LSTM model: input, forget, and output) to two. The two gates
are referred to as an update gate and a reset gate. Some research has shown that a GRU RNN is comparable
to, or even outperforms, LSTM in many cases, while using less training time.

A.2.3 Bidirectional Recurrent Neural Networks (BRNNs)

One shortcoming of conventional RNNs is that they are only able to make use of previous context. It is
often the case that for malware events the most informative part of a sequence occurs at the beginning of
the sequence and may be forgotten by standard recurrent models. Bidirectional RNNs (Schuster and Paliwal
(1997)) overcome this issue by processing the data in both directions with two separate hidden layers, which
−→
h t , the
are then fed forward to the same output layer. A BRNN computes the forward hidden sequence
←−
h t , and the output sequence yt by iterating the backward layer from t = T to
backward hidden sequence
1 and the forward layer from t = 1 to T , and subsequently updating the output layer. Combining BRNNs
with LSTM results in bidirectional LSTM (Graves and Schmidhuber ([n.d.])), which can access long-range
context in both input directions.

A.3 Generative Adversarial Networks (GANs)

A GAN is a combination of two deep neural networks: a classiﬁcation network (the discriminator) which
classiﬁes between real and fake inputs and a generative network (the generator) that tries to generate fake
inputs that would be misclassiﬁed as genuine by the discriminator (Goodfellow et al. (2014)), eventually
reaching a Nash equilibrium. The end result is a discriminator which is more robust against fake inputs.

GANs are only deﬁned for real-valued data, while RNN classiﬁers use discrete symbols. The discrete
outputs from the generative model make it difﬁcult to pass the gradient update from the discriminative model
to the generative model.

Modeling the data generator as a stochastic policy in reinforcement learning can be done to bypass the

generator differentiation problem (Yu et al. (2017)).

A.4 Autoencoders (AEs)

Autoencoders are widely used for unsupervised learning tasks, such as learning deep representations or
dimensionality reduction. Typically, a traditional deep autoencoder consists of two components, the encoder
and the decoder. Let us denote the encoder’s function as fθ : X → H, and denote the decoder’s function as
gω : H → X, where θ , ω are parameter sets for each function, X represents the data space, and H represents
the feature (latent) space. The reconstruction loss is:

L(θ , ω) =

1
N

||X − gω ( fθ (X)) ||2

(20)

Accepted as a long survey paper at ACM CSUR 2021

where L(θ , ω) represents the loss function for the reconstruction.

A.5 Deep Autoencoding Gaussian Mixture Model (DAGMM)

The DAGMM (Zong et al. (2018)) uses two different networks, a deep autoencoder and a Gaussian mixture
model (GMM) based estimator network to determine whether a sample is anomalous or not.

A.6 AnoGAN

AnoGAN (Schlegl et al. (2017)) is GAN-based method for anomaly detection. This method involves training
a DCGAN (Radford et al. (2016)) and using it to recover a latent representation for each test data sample at
inference time. The anomaly score is a combination of reconstruction and discrimination components.

A.7 Adversarially Learned Anomaly Detection (ALAD)

ALAD (Zenati et al. (2018)) is based on a bidirectional GAN anomaly detector, which uses reconstruction
errors from adversarially learned features to determine if a data sample is anomalous. ALAD employs
spectral normalization and additional discriminators to improve the encoder and stabilize GAN training.

A.8 Deep Support Vector Data Description (DSVDD)

DSVDD (Ruff et al. (2018)) trains a deep neural network while optimizing a data-enclosing hypersphere in
the output space.

A.9 One-Class Support Vector Machine (OC-SVM)

The OC-SVM (Sch¨olkopf et al. (1999)) is kernel-based method that learns a decision boundary around
normal examples.

A.10

Isolation Forest (IF)

An isolation forest (Liu et al. (2008)) is a partition-based method which isolates anomalies by building trees
using randomly selected split values.

B Commonly Used Datasets in The Cyber Security Domain

One of the major issues in cyber security related research is that, unlike the computer vision domain, there
are very few publicly available datasets containing malicious samples, as such samples are either private and
sensitive (e.g., benign emails for spam ﬁlters) or can cause damage (e.g., PE malware samples). Moreover,
the cyber security domain is rapidly evolving, so older datasets do not always accurately represent the rele-
vant threats in the wild. The lack of common datasets makes the comparison of attacks and defenses in the
cyber security domain challenging.

Table 8 lists some of the datasets that are publicly available in the cyber security domain and commonly

used to train the attacked classiﬁers mentioned in this paper.

Accepted as a long survey paper at ACM CSUR 2021

Table 8: Commonly Used Datasets in the Cyber Security Domain

Dataset’s
Name

Citation

Year

DREBIN Arp et al.

2014

(2014)

EMBER Anderson
and Roth
(2018)

2018

Input
Type

APK
Malware

PE
Malware

NSL-
KDD

Tavallaee
et al.
(2009)

2009 Network
Stream
(PCAP)

CSE-
CIC-
IDS2018

Sharafaldin
et al.
(2018)

2018 Network
Stream
(PCAP)

BoT-
IoT

2018

Koroniotis
et al.
(2018)

Trec07p Kuleshov

2007

et al.
(2018)

Parkhi
et al.
(2015)

2015

VGG
Face

IEMOCAP Busso
et al.
(2008)

2008

IoT
Network
Stream
(PCAP)

Spam
Emails
(HTML)

Images
and corre-
sponding
face
detections

Speech
(audio)

Number of Samples

Extracted Features/Raw Data

5,600

Permissions, API calls, URL requests

PE structural features (byte histogram,
byte entropy histogram, string related
features, COFF header features, section
features, imports features, exports
features)

Individual TCP connection features (e.g.,
the protocol type), domain
knowledge-based features (e.g., a root
shell was obtained) and statistical features
of the network sessions (e.g., the
percentage of connections that have SYN
errors in a time window)

The dataset includes the captured network
trafﬁc and system logs of each machine,
along with 80 statistical network trafﬁc
features, such as the duration, number of
packets, number of bytes, length of packets

2,000,000

4,900,000

The attacking
infrastructure
includes 50
machines, and the
victim organization
has 5 departments
and includes 420
machines and 30
servers

72,000,000

Raw data (PCAPs)

50,200 spam emails
and 25,200 ham
(non-spam) emails

Raw data (HTML)

2,600

Raw data (images)

10 actors: 5 male and
5 female; 12 hours of
audio

Raw waveforms (audio)

