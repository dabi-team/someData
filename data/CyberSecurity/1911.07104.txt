RSM-GAN: A Convolutional Recurrent GAN for Anomaly Detection in
Contaminated Seasonal Multivariate Time Series

Farzaneh khoshnevisan 1, Zhewen Fan 2
1North Carolina State University
2Intuit Inc.
fkhoshn@ncsu.com, zhewen fan@intuit.com

9
1
0
2

v
o
N
6
1

]

G
L
.
s
c
[

1
v
4
0
1
7
0
.
1
1
9
1
:
v
i
X
r
a

Abstract

Robust anomaly detection is a requirement for monitoring
complex modern systems with applications such as cyber-
security, fraud prevention, and maintenance. These sys-
tems generate multiple correlated time series that are highly
seasonal and noisy. This paper presents a novel unsuper-
vised deep learning architecture for multivariate time se-
ries anomaly detection, called Robust Seasonal Multivariate
Generative Adversarial Network (RSM-GAN). It extends re-
cent advancements in GANs with adoption of convolutional-
LSTM layers and an attention mechanism to produce state-
of-the-art performance. We conduct extensive experiments
to demonstrate the strength of our architecture in adjusting
for complex seasonality patterns and handling severe lev-
els of training data contamination. We also propose a novel
anomaly score assignment and causal inference framework.
We compare RSM-GAN with existing classical and deep-
learning based anomaly detection models, and the results
show that our architecture is associated with the lowest false
positive rate and improves precision by 30% and 16% in real-
world and synthetic data, respectively. Furthermore, we re-
port the superiority of RSM-GAN regarding accurate root
cause identiﬁcation and NAB scores in all data settings.

Introduction
Detecting anomalies in real-time data sources is becoming
increasingly important thanks to the steady rise in the com-
plexity of modern systems. Examples of these systems are
an AWS Cloudwatch service that tracks metrics such as CPU
Utilization and EC2 usage, or an enterprise data encryp-
tion process where multiple encryption keys coexist and are
monitored. Anomaly detection (AD) applications include
cyber-security, data quality maintenance, and fraud preven-
tion. An effective AD algorithm needs to be accurate and
timely to allow operators to take preventative and correc-
tive measures before any catastrophic failure happens. Time
series forecasting techniques such as Autoregressive Inte-
grated Moving Average (ARIMA) (Bianco 2010) as well
as Statistical Process Control (SPC) (Barlow 1992) were
popular algorithms for such applications. However, a com-
plex system often outputs multiple correlated information
sources. These conventional AD techniques are not adequate

to capture the inter-dependencies among multivariate time
series (MTS) generated by the same system. As a result,
many unsupervised density or distance-based models such
as K-Nearest Neighbors (Fabrizio and Pizzuti 2002) have
been developed. However, these models usually ignore the
temporal dependency and seasonality in time series. The im-
portance of modeling temporal dependencies in time series
has been well studied (Brockwell 2013), and failing to cap-
ture them results in model mis-speciﬁcation and a high false
positive rate (FPR) (Qiu et al. 2012). Seasonality is hard
to model due to its irregular and complex nature. Most al-
gorithms such as (Bianco 2010) make a simplistic assump-
tion that there exists only one seasonal component such as
a weekly or monthly seasonality, while in real-world com-
plex systems, multiple seasonal patterns can occur simul-
taneously. Not accurately accounting for seasonality in AD
can also lead to false detection (Thury 1992).
Recent advancement in computation has afforded rapid de-
velopment in deep learning-based AD techniques. Auto-
encoder based models coupled with Recurrent Neural Net-
work (RNN) are well suited for capturing temporal and spa-
tial dependencies and they detect anomalies by inspecting
the reconstruction errors (Malhotra et al. 2016). Generative
Adversarial Networks (GANs) is another well-studied deep
learning framework. The intuition behind using GANs for
AD is to learn the data distribution, and in case of anomalies,
the generator would fail to reconstruct input and produce
large loss. GANs have enjoyed success in image AD (Akcay
2018; Zenati 2018; Schlegl et al. 2017), but have yet been
applied to the MTS structure. Despite such advancements,
none of the previous deep learning AD models addressed
the seasonality problem. Furthermore, most deep learning
model rely on the assumption that the training data is nor-
mal with no contamination. However, real-world data gener-
ated by a complex system often contain noise or undetected
anomalies (contamination). Lastly, MTS anomaly detection
task should not end at simply ﬂagging the anomalous time
points; a well-designed causal inference can help analysts
narrow down the root cause(s) contributing to the irregular-
ity, for them to take a more deliberate action.
To address the aforementioned problems in MTS AD, we
propose an unsupervised adversarial learning architecture

 
 
 
 
 
 
fully adopted for MTS anomaly detection tasks, called Ro-
bust Seasonal Multivariate Generative Adversarial Networks
(RSM-GAN). Motivated by (Akcay 2018; Zhang 2019), we
ﬁrst convert the raw MTS input into multi-channel corre-
lation matrices with image-like structure, and employ con-
volutional and recurrent neural network (Convolutional-
LSTM) layers to capture the spatial and temporal depen-
dencies. Simultaneous training of an additional encoder ad-
dresses the issue of training data contamination. While train-
ing the GAN, we exploit Wasserstein loss with gradient
penalty (Gulrajani et al. 2017) to achieve stable training and
during our experiment it reduces the convergence time by
half. Additionally, we propose a smoothed attention mecha-
nism to model multiple seasonality patterns in MTS. In test-
ing phase, residual correlation matrices along with our pro-
posed scoring and causal inference framework are utilized
for real-time anomaly detection. We conduct extensive em-
pirical studies on synthetic datasets as well as an encryption
key dataset. The results show superiority of RSM-GAN for
timely and precise detection of anomalies over state-of-the-
art baseline models.
The contributions of our work can be summarized as fol-
lows: (1) we propose a convolutional recurrent Wasserstein
GAN architecture (RSM-GAN), and extend the scope of
GAN-based AD from image to MTS tasks; (2) we model
seasonality as part of the RSM-GAN architecture through a
novel smoothed attention mechanism; (3) we apply an ad-
ditional encoder to handle the contaminated training data;
(4) we propose a scoring and causal inference framework to
accurately and timely identify anomalies and to pinpoint un-
speciﬁed numbers of root cause(s). The RSM-GAN frame-
work enables system operators to react to abnormalities
swiftly and in real-time manner, while giving them critical
information about the root causes and severity of the anoma-
lies.

Related Work
MTS anomaly detection has long been an active research
area because of its critical importance in monitoring high
risk tasks. There are 3 main types of detection methods: 1)
classical time series analysis (TSA) based methods; 2) clas-
sical machine learning based methods; and 3) deep learn-
ing based methods. The TSA-based models include Vector
Autoregression (VAR) (Lutkepohl 2007), and latent state
based models like Kalman Filters (Kalman 1960). These
models are prone to mis-speciﬁcation, and are sensitive
to noisy training data. Classical machine learning meth-
ods can be further categorized into distance based methods
such as the k-Nearest Neighbor (kNN) (Fabrizio and Pizzuti
2002), classiﬁcation based methods such as One-Class SVM
(M.Manevitz and Yousef 2001), and ensemble methods such
as Isolation Forest (Liu 2008). These general purpose AD
methods do not account for temporal dependencies nor the
seasonality patterns that are ubiquitous in MTS, therefore,
their performance is often lacking.
Deep learning models have garnered much attention in re-
cent years, and there have been two main types of algorithms
used in AD domain. One is autoencoder based (Han, Pei, and
Kamber 2011). For example, (Zong 2018) investigated the

use of Gaussian classiﬁers with auto-encoders to model den-
sity distributions in multi-dimensional data. (Zhang 2019)
proposed a convolutional LSTM encoder-decoder struc-
ture to capture the temporal dependencies in time series,
while assigning root causes for the anomalies. These mod-
els achieved better performance compared to the classical
machine learning models. However, they do not model sea-
sonality patterns, and they are built under the assumption
that the training data do not contain contamination. Further-
more, they did not fully explore the power of a discrimi-
nator and a generator which has shown to have a superior
performance in computer vision domain. This leads to the
other type of deep learning algorithms: generative adversar-
ial networks (GANs). Several recent studies demonstrated
that the use of GANs has great promise to detect anoma-
lies in images by mapping high-dimensional images to low
dimensional latent space (Akcay 2018; Schlegl et al. 2017;
2019). However, these models have an unrealistic assump-
tion that the training data is contamination free. A weak la-
beling to inspect this condition would make such algorithms
not being fully unsuprvised. Further, applying GANs to
data structures other than images is challenging and under-
explored. To the best of our knowledge, we are among the
ﬁrst to extend the applications of GANs to MTS.

Methodology
1 , ..., X T

n ) ∈ Rn×T , where n is
We deﬁne an MTS X = (X T
the number of time series, and T is the length of the histori-
cal training data. We aim to predict two AD outcomes: 1) the
time points t after T that are associated with anomalies, and
2) time series xi, i ∈ {1, .., n} causing the anomalies. In the
following section, we ﬁrst describe how we reconstruct the
input MTS to be consumed by a convolutional GAN. Then
we introduce the RSM-GAN framework by decomposing it
into three components: the architecture, the inner-structure,
and the attention mechanism for seasonality adjustment. Fi-
nally, after the model is trained, we describe how we develop
an anomaly scoring and casual inference procedure to iden-
tify anomalies and the root causes in the test data.

RSM-GAN Framework
MTS to Image Conversion To extend GAN to MTS and
to capture inter-correlation between multiple time series, we
convert the MTS into an image-like structure through the
construction of the so-called multi-channel correlation ma-
trix (MCM), inspired by (Song et al. 2018; Zhang 2019).
Speciﬁcally, we deﬁne multiple windows of different sizes
W = (w1, ..., wc), and calculate the pairwise inner product
(correlation) of time series within each window. For a spe-
ciﬁc time point t, we generate c matrices (channels) of shape
n×n, where each element of matrix Sw
t for a window of size
w is calculated by this formula:

sij =

(cid:80)w

δ=0 xt−δ
i
w

· xt−δ
j

(1)

In this work, we select windows W = (5, 10, 30). This re-
sults in 3 channels of n × n correlation matrices. To convert
the span of MTS into this shape, we consider a step size

ss = 5. Therefore, X is transformed to S = (S1, ..., SM ) ∈
RM ×c×n×n, where M = (cid:98) T
(cid:99) steps presented by MCMs.
ss
Finally, to capture the temporal dependency between con-
secutive steps, we stack h = 4 previous steps to the current
step t to prepare the input to the GAN-based model. Later,
we extend MCM to also capture seasonality unique to MTS.

RSM-GAN Architecture The idea behind using a GAN
to detect anomalies is intuitive. During training, a GAN
learns the distribution of the input data. Then, if anomalies
are present during testing, the networks would fail to recon-
struct the input, thus produce large losses. A GAN also ex-
ploits the power of the discriminator to optimize the net-
work more efﬁciently towards training the distribution of
input. However, in most GAN literature the training data
is explicitly assumed to be normal with no contamination.
(Berg, Ahlberg, and Felsberg 2019) have shown in a study
that simultaneous training of an encoder with GAN im-
proves the robustness of the model against contamination.
To this end, we adopt an encoder-decoder-encoder structure
(Akcay 2018), with the additional encoder, to capture the
training data distribution in both original and latent space. It
improves the robustness of the model to training noise, be-
cause the joint encoder forces similar inputs to lie close to
each other also in the latent space. Speciﬁcally, in Figure 1
the generator G has autoencoder structure that the encoder
(GE) and decoder (GD) interact with each other to mini-
mize the reconstruction or contextual loss: the l2 distance
between input x and reconstructed input x(cid:48). Furthermore,
an additional encoder E is trained jointly with the genera-
tor to minimize the latent loss: the l2 distance between la-
tent vector z and reconstructed latent vector z(cid:48). Finally, the
discriminator D is tasked to distinguish between the origi-
nal input x and the generated input G(x). Following the re-
cent improvements on GAN-based image AD (Zenati 2018;
Schlegl et al. 2017), we use feature matching loss for the
adversarial training. Feature matching exploits the internal
representation of the input x induced by an intermediate
layer in D. Assuming that the function f (·) will produce
such representation, the discriminator aims to maximize the
distance between f (x) and f (x(cid:48)) to effectively distinguish
between original and generated inputs. At the same time, the
generator battles against the adversarial loss to confuse the
discriminator. With multiple loss components and training
objectives, we employ the Wasserstein GAN with gradient
penalty (WGAN-GP) (Gulrajani et al. 2017) to 1) enhance
the training stability, and 2) converge faster and more op-
timally. The ﬁnal objective functions for the generator and
discriminator (critic) are as following:

LD = max
w∈W

Ex∼px[fw(x)] − Ex∼px [fw(G(x))]

(2)

LG = min
G

min
E

(cid:16)

w1Ex∼px (cid:107) x − G(x) (cid:107)2 +
w2Ex∼px (cid:107) GE(x) − E(G(x)) (cid:107)2 +
(cid:17)
w3Ex∼px [fw(G(x))]

(3)

Figure 1: GAN architecture with loss deﬁnitions

Figure 2: Inner structure of convolutional recurrent encoders
and convolutional decoder (with n = 10) (Zhang 2019)

Where w1, w2, and w3 are weights controlling the effect of
each loss on the total objective. We employ Adam optimizer
(Kingma and Ba 2014) to optimize the above losses.
In the next section, we describe how we design the inter-
nal structure of each network in RSM-GAN to capture the
spatial as well temporal dependencies in our input data.

Internal Encoder and Decoder Structure
In addition to
the convolutional layers in the encoders, we add RNN lay-
ers to jointly capture the spatial patterns and temporality of
our MCM input by using convolutional-LSTM (convLSTM)
gates. We apply convLSTM to every convolutional layer due
to its optimal mapping to the latent space (Zhang 2019). The
convolutional decoder applies multiple deconvolutional lay-
ers in reverse order to reconstruct MCM at current time step.
Starting from the last convLSTM output, decoder applies de-
convolutional layer and concatenates the output with convL-
STM output of the previous step. The concatenation output
is further an input to the next deconvolutional layer, and so
on. Figure 2 illustrates the detailed inner-structure of the en-
coder and decoder networks.
The second encoder E follows the same structure as the gen-
erator’s encoder GE to reconstruct latent space z(cid:48). Input to
the discriminator is the original or generated MCM of each
time step. Therefore, internal structure of the discriminator
consist of three simple convolutional layers, with the last
layer representing f (·).

Seasonality Adjustment via Attention Mechanism The
construction of the initial MCM does not take into consid-
eration of the seasonality. We propose to ﬁrst stack previ-
ous seasonal data points to the input data, and then let the
convLSTM model temporal dependencies through attention
mechanism. Speciﬁcally, in addition to h previous immedi-
ate steps, we add mi previous seasonal steps per seasonal
pattern i. To illustrate, assume the input has both the daily
and weekly seasonality. For a certain time t, we stack MCMs
of up to m1 days ago at the same time, and up to m2 weeks
ago at the same time. Additionally, to account for the fact
that seasonal patterns are often not exact, we smooth the sea-
sonal steps by averaging over steps in a neighboring window
of 30 minutes.
Moreover, even though the h previous steps are closer to the
current time step, but the previous seasonal steps might be
a better indicator to reconstruct the current step. Therefore,
we further apply an attention mechanism to the convLSTM
layers, and let the model decide the importance of all prior
steps based on the similarity rather than recency. Attention
weights are calculated based on the similarity of the hidden
state representations in the last layer, by the following for-
mula:

H(cid:48)

t =

(cid:88)

i∈(t−N,t)

αiHi, αi = softmax

(cid:16) V ec(Ht)T V ec(Hi)
X

(cid:17)

(4)
Where N = h + Σmi, V ec(·) denotes the vector, and
X = 5 is the rescaling factor. Figure 3 presents the structure
of the described smoothed attention mechanism. Finally, to
make our model even more adaptable to real-world datasets
that often exhibit holiday effects, we multiply the attention
weight αi by a binary bit bi ∈ {0, 1}, where bi = 0 in case
of holidays or other exceptional behavior in previous steps.
This way, we eliminate the effect of undesired steps from the
current step.

Figure 3: Smoothed attention mechanism

Testing Phase
Anomaly Score Assignment After training the RSM-
GAN, an anomaly score is assigned based on the residual
MCM of the ﬁrst channel (context) and latent vector of the

RSM-GAN. We deﬁne broken tiles as the elements of the
contextual or latent residual matrix that have error value
of greater than θb. (Zhang 2019) deﬁned a scoring method
based on the number of broken tiles in contextual or la-
tent residual matrix (contextb and latentb). However, this
method is more sensitive to severe anomalies, and lower-
ing the threshold results in high FPR. We propose a root
cause-based counting procedure. Since each row/column in
the contextual residual matrix is associated with a time se-
ries, the ones with the largest number of broken tiles are
contributing the most to the anomalies. Therefore, by deﬁn-
ing a threshold θh ≤ θb, we only count the number of bro-
ken tiles in rows/columns with more than half broken. We
name our new scoring method contexth. The above thresh-
olds θ = β × η.996(Etrain), which is calculated based on
99.6th percentile of error in the training residual matrices,
and the best β is captured by grid search on the validation
set.

Root Cause Framework Large errors associated with el-
ements of rows/columns of the residual MCM are indica-
tive of anomalous behavior in those time series. To identify
those abnormal time series, we need a root-cause scoring
system to assign a score to each time series based on sever-
ity of its associating errors. We present 3 different methods:
1) number of broken tiles (using the optimized θ from previ-
ous step), 2)the weighted sum of broken tiles based on their
absolute error, and 3) the sum of absolute errors.
Furthermore, the number of root causes, k, is unknown in
real-life applications. (Zhang 2019) used an arbitrary num-
ber of 3. Here, we propose to use an elbow method (Ketchen
1996) to ﬁnd the optimal k number of time series from the
root cause scores. In this approach, by sorting the scores
and plotting the curve, we aim to ﬁnd the point where the
amount of errors become very small and close to each other.
Basically, for each point ni on the score curve, we ﬁnd the
point with maximum distance from a vector that connects
the ﬁrst and last scores. Time series associated with the
scores greater than this point are identiﬁed as root causes.

Experimental Setup

Data
To evaluate different aspects of RSM-GAN, we conduct a
comprehensive set of experiments by generating synthetic
time series with multiple settings, along with a real-world
encryption key dataset.

Synthetic Data To simulate data with different seasonality
and contamination, we ﬁrst generate sinusoidal-based waves
of length T and periodicity F :

S(t, F ) =

(cid:26) sin[(t − t0)/F ] + 0.3 × (cid:15)t
cos[(t − t0)/F ] + 0.3 × (cid:15)t

srand = 0
srand = 1

(5)
Where srand is 0 or 1, t0 ∈ [10, 100] is shift in phase
and they are randomly selected for each time series. (cid:15)t ∼
N (0, 1) is the random noise, and F ∈ [60, 100] is the peri-
odicity or seasonality. Ten time series with 2 months worth
of data by minute sampling frequency are generated, or

T = 80, 640. Each time series with combined seasonality
is generated by:

S(t) = S(t, Frand) + S(t, Fday) + S(t, Fweek)

(6)

2π

60×24 and Fweek =

Where Fday = 2π
60×24×7 . To simu-
late anomalies with varying duration and intensity, we shock
time series with a random duration ([5, 60] minutes), direc-
tion, and number of root causes ([2, 6]). Each experiment is
conducted with different seasonality patterns and contami-
nation settings.

Encryption Key Data Our encryption-key dataset con-
tains 7 time series generated from a project’s encryption pro-
cess. Each time series represents the number of requests for
a speciﬁc encryption key per minute. The dataset contains
4 months of data or T = 156, 465. Four anomalies with
various length and scales are identiﬁed in the test sequence
by a security expert, and we randomly injected 5 additional
anomalies into both the train and test sequences.

Baseline Models
Three baseline models are used for comparison. Two are
classical machine learning models, i.e., One-class SVM
(OC-SVM) (M.Manevitz and Yousef 2001) and Isolation
Forest (Liu 2008). We also compare our model performance
with that of MSCRED (Zhang 2019) with the same input as
ours. MSCRED is run in a sufﬁcient number of epochs and
its best performance is reported.

Evaluation Metrics
In addition to precision, recall, false positive rate, and
F1 score, we include the Numenta Anomaly Benchmark
(NAB) score (Lavin 2015). NAB is a standard open source
framework for evaluating real-time AD algorithms. The
NAB assigns score to each positive detection based on their
relative position to the anomaly window by a scaled sigmoid
function (between -1 and 1). Speciﬁcally, it assigns a posi-
tive score to the earliest detection within anomaly window
(1 to the beginning of the window) and negative score to de-
tections after the window (false positives). Additionally, it
assigns a negative score (-1) to the missed anomalies. NAB
score is more comprehensive than standard metrics because
it also rewards timely detection. Early detection is critical for
high-stake AD tasks such as cyber-attack monitoring. Also,
it penalizes false positives as they get farther from the true
anomaly window due to high cost of the manual inspection
of the system.
In our experiments, the ﬁrst half of the time series are used
for training the model and the remainder for evaluation.
RSM-GAN is implemented in Tensorﬂow and trained in 300
epochs, in batches of size 32, on an AWS Sagemaker in-
stance with four 16GB GPUs. All the results on synthetic
data are produced by an average over ﬁve runs.

Result and Discussion

Anomaly Score Assignment
We ﬁrst evaluate our new score assignment method contexth
against the 3 other methods described before. Table 1 re-
ports the performance of RSM-GAN on synthetic MTS with

no contamination and seasonality, using different scoring
methods. The reported threshold is the optimum thresh-
old obtained by the grid search. As we can see, our pro-
posed contexth method results in more precise predictions
and has the highest NAB score. Speciﬁcally, contexth im-
proves the precision and FPR by 6.2% and 0.08% compared
to the contextb method. Scoring based on the latent resid-
ual loss results in the lowest performance. Also, combin-
ing the methods by calculating a weighted sum of context
and latent-based scores does not help improving the per-
formance. Further, this performance comparison maintains
the same for other more complex synthetic settings. Thus,
contexth will be the scoring method reported in the subse-
quent sections.

Table 1: Model performance with different anomaly score
assignment methods

Score
latentb
contextb
contexth
combined

Threshold
0.0099
0.0019
0.00026
-

Precision Recall
0.819
0.958
0.916
0.916

0.648
0.784
0.846
0.767

F1
0.723
0.862
0.880
0.835

FPR
0.0040
0.0023
0.0015
0.0025

NAB Score
0.460
0.813
0.859
0.721

Root Cause Identiﬁcation Assessment
RSM-GAN detects root causes using context-based residual
matrix. In this section, we compare the results of MSCRED
and RSM-GAN using 3 root cause scoring methods. Root
causes are identiﬁed based on the average of errors per time
series in an anomaly window and by applying the aforemen-
tioned elbow-based identiﬁcation method. Precision, recall
and F1 scores are averaged over all detected anomalies.

Table 2: Root cause identiﬁcation performance with differ-
ent root cause scoring methods

Model

MSCRED

RSM-GAN

Scoring
Number of broken (NB)
Weighted broken (WB)
Absolute error (AE)
Number of broken (NB)
Weighted broken (WB)
Absolute error (AE)

Precision Recall
0.7933
0.6866
0.7066
0.8500
0.8666
0.8666

0.5154
0.5071
0.5504
0.4960
0.6883
0.6883

F1
0.6249
0.5834
0.6188
0.6264
0.7672
0.7672

The synthetic data used in this experiment has two combined
seasonal patterns and ten anomalies in the training data. Ta-
ble 2 shows root cause identiﬁcation performance of RSM-
GAN and MSCRED. Overall, RSM-GAN outperforms MS-
CRED. As the results suggest, the NB method performs the
best for MSCRED. However, for RSM-GAN the WB and
AE methods leads to the best performance. Since the same
result holds for other settings, we report NB for MSCRED
and AE for RSM-GAN in subsequent sections.

Contamination Tolerance Assessment
In this section, we assess the robustness of RSM-GAN with
different levels of contamination in training data, and the re-
sults are compared to the baseline models. In this experi-
ment, the level of contamination starts with no contamina-
tion and at each subsequent level, we add 5 more random
anomalies with varying duration to the training data. The

Table 3: Model Performance with different levels of training data contamination

Contamination

No contamination
train: 0 (0)
test: 10 (%0.82)

Mild contamination
train: 5 (%0.43)
test: 10 (%0.76)

Medium contamination
train: 10 (%0.82)
test: 10 (%0.85)

Severe contamination
train: 15 (%1.19)
test: 15 (%1.18)

Model
OC-SVM
Isolation Forest
MSCRED
RSM-GAN
OC-SVM
Isolation Forest
MSCRED
RSM-GAN
OC-SVM
Isolation Forest
MSCRED
RSM-GAN
OC-SVM
Isolation Forest
MSCRED
RSM-GAN

Precision Recall
1.0000
1.0000
0.8450
0.9166
1.0000
1.0000
0.6029
0.7500
1.0000
1.0000
0.7143
0.8442
1.0000
1.0000
0.7290
0.8774

0.1581
0.0326
0.8000
0.8461
0.2810
0.3134
0.6949
0.8906
0.4611
0.6311
0.6548
0.8553
0.5691
0.8425
0.5493
0.8692

F1
0.2730
0.0631
0.8219
0.8800
0.4387
0.4772
0.6457
0.8143
0.6311
0.7739
0.6832
0.8497
0.7254
0.9145
0.6265
0.8732

FPR
0.0473
0.2640
0.0018
0.0015
0.0218
0.0187
0.0023
0.0009
0.0113
0.0056
0.0036
0.0014
0.0102
0.0025
0.0080
0.0018

NAB Score Root Cause Recall

-8.4370
-51.4998
0.7495
0.8598
-3.3411
-2.7199
0.2721
0.8865
-1.2351
-0.1250
0.2712
0.8511
-0.3365
0.6667
0.0202
0.8872

-
-
0.7533
0.6333
-
-
0.5483
0.7700
-
-
0.6217
0.8083
-
-
0.6611
0.8133

percentages presented in the Contamination column in Ta-
ble 3 shows the proportions of the anomalous time points in
train/test time span.
Results in Table 3 suggest that our proposed model outper-
forms all baseline models at all contamination levels for all
metrics except of the recall. Note that the %100 recall for
classic baseline models is at the expense of FPR as high as
26.4%, especially for the less severe contamination. Further-
more, comparison of the NAB scores shows that our model
has more timely detections and false positives are within a
window of the anomalies.
Lastly, as we can see, the MSCRED performance drops dras-
tically as the contamination level increases. This is because
the encoder-decoder structure of this model cannot handle
high levels of contamination while training.

Seasonality Adjustment Assessment

In this section, we assess the performance of our pro-
posed attention mechanism for capturing seasonality in
MTS. In many of the real-world AD applications, time
series might contain a single or multiple seasonal pat-
terns (daily/weekly/monthly/etc.), with the effect of special
events, like holidays. The performance of RSM-GAN is as-
sessed in different seasonality settings. In the ﬁrst three ex-
periments, synthetic MTS (2 months, sampled per minute)
are generated with no training data contamination and no
seasonality, then daily and weekly seasonality patterns are
added one by one. In the last experiment, we simulate 3
years of hourly data, and add special patterns for the time
steps related to the US holidays in both the train and test
sets. The test set of each experiment is contaminated with
10 random anomalies.
Comparing the results in Table 4, RSM-GAN shows con-
sistent performance thanks to the attention mechanism cap-
turing the seasonality patterns. All the other baseline mod-
els, especially MSCRED’s performance deteriorated with
increased complexity of the seasonal patterns. Precision is
the main metric that drops drastically for the baseline mod-
els as we add more seasonality. This is because they do not
account for changes due to seasonality and identify them as
anomalies, which also led to high FPR.
In the last experiment in Table 4, all of the abnormalities in-

Figure 4: Performance comparison on synthetic data with
weekly and monthly seasonality and holiday effect

jected to holidays are wrongly ﬂagged by the baseline mod-
els as anomalies, since no holiday adjustment is incorpo-
rated in these models. This resulted in low precision and
high FPR for those models. In RSM-GAN, multiplying the
binary vectors of holidays with the attention weights enables
it to account for the holidays, which leads to the best perfor-
mance in almost all metrics. Figure 4 shows the ground truth
anomaly labels (bottom), and the anomaly scores assigned
by each model to each time step while testing. It is evident
that our model accurately accounts for the holidays (18 Feb,
19 May, 4 Jul) and has much lower FPR.

Performance on Real-world dataset

This section evaluates our model on a real-world encryp-
tion key dataset. A cursory look shows that this dataset is
noisy, and contains both daily and weekly seasonality. To
be comprehensive, we also create a synthetic dataset with
similar patterns, i.e., daily and weekly seasonality as well
as medium contamination (10 anomalies) in the training set.
From Table 5, we make the following observations: 1) RSM-
GAN consistently outperforms all the baseline models in
terms of detection and root cause identiﬁcation recall for
both the encryption key and the synthetic dataset. 2) Not
surprisingly, for all the models, performance on the syn-
thetic data is better than that of encryption key data. It is
due to the excessive irregularities and noise in the encryp-
tion key data, and errors arising from ground truth labeling
by experts. 3) The plots in Figure 5 illustrates the anomaly
scores assigned to each time point in test dataset by each al-

Table 4: Model performance on synthetic data with different seasonal patterns and no contamination

Seasonality

Random seasonality

Daily seasonality

Daily and weekly
seasonality

Weekly and monthly
seasonality
with holidays

Model
OC-SVM
Isolation Forest
MSCRED
RSM-GAN
OC-SVM
Isolation Forest
MSCRED
RSM-GAN
OC-SVM
Isolation Forest
MSCRED
RSM-GAN
OC-SVM
Isolation Forest
MSCRED
RSM-GAN

Precision Recall
0.9819
1.0000
0.8451
0.9167
1.0000
1.0000
0.7912
0.7935
0.9487
0.9487
0.7143
0.6750
0.9444
0.8889
0.7059
0.8108

0.4579
0.0325
0.8000
0.8462
0.1770
0.1387
0.7347
0.9012
0.1883
0.1783
0.6548
0.9000
0.2361
0.2783
0.0860
0.6522

F1
0.6245
0.0630
0.8219
0.8800
0.3008
0.2436
0.7619
0.8439
0.3142
0.3002
0.6832
0.7714
0.3778
0.4238
0.1534
0.7229

FPR
0.0097
0.2646
0.0019
0.0015
0.0532
0.0710
0.0033
0.0010
0.0400
0.0428
0.0036
0.0008
0.0425
0.0321
0.0983
0.0063

NAB Score Root Cause Recall

-8.6320
-51.606
0.7495
0.8598
-9.5465
-13.107
0.3775
0.5175
-6.9745
-7.5278
0.2712
0.5461
-1.7362
-1.0773
-5.1340
0.5617

-
-
0.7533
0.6333
-
-
0.7467
0.6717
-
-
0.6217
0.4650
-
-
0.6067
0.8667

Table 5: Model performance on encryption key and synthetic seasonal MTS with contamination

Dataset

Encryption
key

Synthetic

Model
OC-SVM
Isolation Forest
MSCRED
RSM-GAN
OC-SVM
Isolation Forest
MSCRED
RSM-GAN

Precision Recall
0.2977
0.4649
0.2442
0.4405
0.9185
0.9610
0.7403
0.8438

0.1532
0.3861
0.1963
0.6852
0.6772
0.7293
0.6228
0.8884

F1
0.2023
0.4219
0.2176
0.5362
0.7772
0.8221
0.6746
0.8649

FPR
0.0063
0.0028
0.0055
0.0011
0.0038
0.0033
0.0043
0.0010

NAB Score Root Cause Recall

-17.4715
-6.9343
-1.1047
0.2992
-2.7621
-2.2490
0.2753
0.8986

-
-
0.4709
0.5093
-
-
0.6600
0.7870

gorithm. As we can see, even though isolation forest has the
highest recall rate, it also detects many false positives not
related to the actual anomaly windows, leading to negative
NAB scores. As mentioned before, irrelevant false positives
are costly in real-world applications. 4) By comparing our
model to MSCRED in Figure 5a and Figure 5b, we can see

(a) Encryption Key Dataset

(b) Synthetic Dataset

Figure 5: Anomaly score assignment of different algorithms.
The bottom plot is the ground truth labels.

that MSCRED not only has much higher FPR, but it also
fails to capture some anomalies. We conjecture it is because
MSCRED’s encoder-decoder structure is not as robust to the
training data contamination, nor does it model the seasonal-
ity patterns.

Conclusion

In this work, we presented the challenges in MTS anomaly
detection and proposed a novel GAN-based MTS anomaly
detection framework (RSM-GAN) to solve those challenges.
RSM-GAN takes advantage of the adversarial learning to
accurately capture the temporal and spatial dependencies in
the data, while simultaneously deploying an additional en-
coder to handle even severe levels of training data contami-
nation. The novel attention mechanism in the recurrent layer
of RSM-GAN enables the model to handle complex sea-
sonal patterns often found in the real-world data. Further-
more, training stability and optimal convergence of the GAN
is attained through the use of Wasserstein GAN with gradi-
ent penalty. We conducted extensive empirical studies and
results show that our architecture together with a new score
and causal inference framework lead to an exceptional per-
formance over state-of-the-art baseline models on both syn-
thetic and real-world datasets.

References

Akcay, S. 2018. Semi-supervised anomaly detection via ad-
versarial training. In Asian Conference on Computer Vision,
622–637.
Barlow, R. 1992. Foundations of statistical quality control.

Song, D.; Xia, N.; Cheng, W.; Chen, H.; and Tao, D. 2018.
Deep r-th root of rank supervised joint binary embedding
for multivariate time series retrieval. In Proceedings of the
24th ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining, 2229–2238. ACM.
Thury, G. 1992. Granger causality for time-series anomaly
detection.
Zenati, H. 2018. Estimate and replace: Efﬁcient gan-based
anomaly detection. arXiv preprint arXiv:1802.06222.
Zhang, C. 2019. A deep neural network for unsupervised
anomaly detection and diagnosis in multivariate time series
data. In The Thirty-Third AAAI Conference on Artiﬁcial In-
telligence, 1409–1416.
Zong, B. 2018. Deep autoencoding gaussian mixture model
for unsupervised anomaly detection. In ICLR.

2013. Time series: theory and methods.

In ACurrent Issues in Statistical Inference: Essays in Honor
of D. Basu, 99–112.
Berg, A.; Ahlberg, J.; and Felsberg, M. 2019. Unsuper-
vised learning of anomaly detection from contaminated im-
age data using simultaneous encoder training. arXiv preprint
arXiv:1905.11034.
Bianco, A. 2010. Outlier detection in regression models
with arima errors using robust estimates. In Journal of Fore-
cast, 20, 565–579.
Brockwell, P.
Springer Science & Business Media.
Fabrizio, A., and Pizzuti, C. 2002. Fast outlier detection
In European Conference on
in high dimensional spaces.
Principles of Data Mining and Knowledge Discovery, 15–
27. Springer.
Gulrajani, I.; Ahmed, F.; Arjovsky, M.; Dumoulin, V.; and
Courville, A. C. 2017.
Improved training of wasserstein
gans. In Advances in neural information processing systems,
5767–5777.
Han, J.; Pei, J.; and Kamber, M. 2011. Data mining: con-
cepts and techniques. Elsevier.
Kalman, R. 1960. A new approach to linear ﬁltering and
prediction problems. In ASMEJournal of Basic Engineering,
82, Series D, 35–45.
Ketchen, J. 1996. The application of cluster analysis in
strategic management research: An analysis and critique. In
Strategic Management Journal. 17 (6).
Kingma, D. P., and Ba, J. 2014. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980.
Lavin, A. 2015. Evaluating real-time anomaly detection
algorithms – the numenta anomaly benchmark. In IEEE In-
ternational Conference on Machine Learning and Applica-
tions.
Liu, F. 2008. Isolation forests. In In Proceedings of Inter-
national Conference on Data Mining, 413–422.
Lutkepohl, H. 2007. New introduction to multiple time se-
ries analysis. Springer.
Malhotra, P.; Ramakrishnan, A.; Anand, G.; Vig, L.; Agar-
wal, P.; and Shroff, G.
Lstm-based encoder-
decoder for multi-sensor anomaly detection. arXiv preprint
arXiv:1607.00148.
M.Manevitz, and Yousef, M. 2001. One-class svms for doc-
ument classiﬁcation. In J. Mach. Learn.Res, 139154.
Qiu, H.; Liu, Y.; A Subrahmanya, N.; and Li, W. 2012.
Granger causality for time-series anomaly detection. 1074–
1079.
Schlegl, T.; Seeb¨ock, P.; Waldstein, S. M.; Schmidt-Erfurth,
U.; and Langs, G. 2017. Unsupervised anomaly detection
with generative adversarial networks to guide marker dis-
In International Conference on Information Pro-
covery.
cessing in Medical Imaging, 146–157. Springer.
Schlegl, T.; Seeb¨ock, P.; Waldstein, S. M.; Langs, G.; and
f-anogan: Fast unsupervised
Schmidt-Erfurth, U. 2019.
anomaly detection with generative adversarial networks.
Medical image analysis 54:30–44.

2016.

