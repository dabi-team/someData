Characterizing Honeypot-Captured Cyber Attacks:
Statistical Framework and Case Study

Zhenxin Zhan, Maochao Xu, and Shouhuai Xu

1

6
1
0
2

r
a

M
4
2

]

R
C
.
s
c
[

1
v
3
3
4
7
0
.
3
0
6
1
:
v
i
X
r
a

Abstract—Rigorously characterizing the statistical properties
of cyber attacks is an important problem. In this paper, we
propose the ﬁrst statistical framework for rigorously analyzing
honeypot-captured cyber attack data. The framework is built
on the novel concept of stochastic cyber attack process, a new
kind of mathematical objects for describing cyber attacks. To
demonstrate use of the framework, we apply it to analyze a low-
interaction honeypot dataset, while noting that the framework
can be equally applied to analyze high-interaction honeypot data
that contains richer information about the attacks. The case
study ﬁnds, for the ﬁrst time, that Long-Range Dependence
(LRD) is exhibited by honeypot-captured cyber attacks. The
case study conﬁrms that by exploiting the statistical properties
(LRD in this case), it is feasible to predict cyber attacks (at
least in terms of attack rate) with good accuracy. This kind of
prediction capability would provide sufﬁcient early-warning time
for defenders to adjust their defense conﬁgurations or resource
allocations. The idea of “gray-box” (rather than “black-box”)
prediction is central to the utility of the statistical framework, and
represents a signiﬁcant step towards ultimately understanding
(the degree of) the predictability of cyber attacks.

Index Terms—Cyber security, cyber attacks, stochastic cy-
ber attack process, statistical properties, long-range dependence
(LRD), cyber attack prediction

I. INTRODUCTION

Characterizing statistical properties of cyber attacks not only
can deepen our understanding of cyber threats but also can
lead to implications for effective cyber defense. Honeypot is
an important tool for collecting cyber attack data, which can be
seen as a “birthmark” of the cyber threat landscape as observed
from a certain IP address space. Studying this kind of data
allows us to extract useful information about, and even predict,
cyber attacks. Despite the popularity of honeypots, there is no
systematic framework for rigorously analyzing the statistical
properties of honeypot-captured cyber attack data. This may
be attributed to that a systematic framework would require
both a nice abstraction of cyber attacks and fairly advanced
statistical techniques.

In this paper, we make three contributions. First, we pro-
pose, to our knowledge, the ﬁrst statistical framework for sys-
tematically analyzing and exploiting honeypot-captured cyber
attack data. The framework is centered on the concept we
call stochastic cyber attack process, which is a new kind of

Copyright (c) 2013 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending a request to pubs-permissions@ieee.org
Zhenxin Zhan and Shouhuai Xu are with the Department of Computer
Science, University of Texas at San Antonio, San Antonio, TX 78249. Emails:
jankins.ics@gmail.com (Zhenxin Zhan), shxu@cs.utsa.edu
(Shouhuai Xu; corresponding author)

Maochao Xu is with the Department of Mathematics, Illinois State Univer-

sity, Normal, IL 61790. Email: mxu2@ilstu.edu

mathematical objects that can naturally model cyber attacks.
This concept can be instantiated at multiple resolutions, such
as: network-level (i.e., considering all attacks against a net-
work as a whole), victim-level (i.e., considering all attacks
against a computer or IP address as a whole), port-level (i.e.,
the defender cares most about the attacks against certain ports
or services). This concept catalyzes the following fundamental
questions: (i) What statistical properties do stochastic cyber
attack processes exhibit (e.g., are they Poisson)? (ii) What
are the implications of these properties and, in particular, can
we exploit them to predict the incoming attacks (prediction
capability is the core utility of the framework)? (iii) What
caused these properties? Thus, the present paper formulates a
way of thinking for rigorously analyzing honeypot data.

Second, we demonstrate use of the framework by applying
it to analyze a dataset, which is collected by a low-interaction
honeypot of 166 IP addresses for ﬁve periods of time (220 days
cumulative). Findings of the case study include: (i) Stochastic
cyber attack processes are not Poisson, but instead can exhibit
Long-Range Dependence (LRD) — a property that is not
known to be exhibited by honeypot data until now. This ﬁnding
has profound implications for modeling cyber attacks. (ii) LRD
can be exploited to predict the incoming attacks at least in
terms of attack rate (i.e., number of attacks per time unit).
This is especially true for network-level stochastic cyber attack
processes. This shows the power of “gray-box” prediction,
where the prediction models accommodate the LRD property
(or other statistical properties that are identiﬁed). (iii) Although
we cannot precisely pin down the cause of the LRD exhibited
by honeypot data, we manage to rule out two possible causes.
We ﬁnd that the cause of LRD exhibited by cyber attacks might
be different from the cause of LRD exhibited by benign trafﬁc
(see Section IV-E).

Third, the framework can be equally applied to analyze both
low-interaction and high-interaction honeypot data, while the
latter contains richer information about attacks and allows
even ﬁner-resolution analysis. Thus, we plan to make our
statistical framework software code publicly available so that
other researchers or even practitioners, who have (for example)
high-interaction honeypot data that often cannot be shared
with third parties, can analyze their data without learning the
advanced statistic skills.

The paper is organized as follows. Section II brieﬂy reviews
some statistical preliminaries including prediction accuracy
measures, while some detailed statistical techniques are de-
ferred to the Appendix. Section III describes the framework.
Section IV discusses the case study and its limitations. Section
V discusses the limitation of the case study (which is imposed
by the speciﬁc dataset) and the usefulness of the framework

 
 
 
 
 
 
2

in a broader context. Section VI discusses related prior work.
Section VII concludes the paper with future research direc-
tions.

II. STATISTICAL PRELIMINARIES

A. Long-Range Dependence (LRD)

A stationary time sequence {Xt : t ≥ 0}, which instantiates
a stochastic cyber attack process {Xt : t ≥ 0}, is said to
possess LRD [1], [2] if its autocorrelation function

ρ(h) = Cor(Xt, Xt+h) ∼ h−βL(h),

h → ∞,

(1)

for 0 < β < 1, where h is called “lag”, L(·) is a slowly
L(ix)
L(x) = 1 for all
varying function meaning that limx→∞
i > 0. Intuitively, LRD says that a stochastic process exhibits
persistent correlations, namely that the rate of autocorrelation
decays slowly (i.e., slower than an exponential decay). Quan-
titatively speaking, the degree of LRD is expressed by Hurst
parameter (H), which is related to the parameter β in Eq.
(1) as β = 2 − 2H [3]. This means that for LRD, we have
1/2 < H < 1 and the degree of LRD increases as H → 1. In
the Appendix, we brieﬂy review six popular Hurst-estimation
methods that are used in this paper.

Since 1/2 < H < 1 is necessary but not sufﬁcient for
LRD, we need to eliminate the so-called “spurious LRD” as
we focus on the LRD property in this paper. Spurious LRD can
be caused by non-stationarity [4], or more speciﬁcally caused
by (i) short-range dependent time series with change points
in the mean or (ii) slowly varying trends with random noise
[5], [6]. We eliminate spurious LRD processes by testing the
null hypothesis (denoted by H0) that a given time series is
a stationary LRD process against the alternative hypothesis
(denoted by Ha) that it is affected by change points or a
smoothly varying trend [5]. One test is for t ≥ 0:

H0 : Xt is stationary with LRD
Ha : Xt = Zt + µt with µt = µt−1 + ψtηt

where Zt is a stationary short-memory process [7], µ0 = 0,
ψt is a Bernoulli random variable, and ηt is a white (i.e.,
Gaussian) noise process. The other alternative is:

• LRD-aware model FARIMA(p, d, q): This is the well-
known Fractional ARIMA model where 0 < d < 1/2
and H = d + 1/2 [2], [3], [8]. Speciﬁcally, a stationary
process Xt is called FARIMA(p, d, q) if

φ(B)(1 − B)dXt = ψ(B)(cid:15)t,

for some −1/2 < d < 1/2, where

φ(x) = 1 −

p
(cid:88)

j=1

φjxj,

ψ(x) = 1 +

q
(cid:88)

j=1

ψjxj,

B is the back shift operator deﬁned by BXt = Xt−1,
B2Xt = Xt−2, and so on.

C. Measures of Prediction Accuracy

Suppose Xm, Xm+1, . . . , Xz are observed data (e.g., the
attack rate Xt for m ≤ t ≤ z), and Ym, Ym+1, . . . , Yz are the
predicted data. We can deﬁne prediction error et = Xt − Yt
for m ≤ t ≤ z. Recall the popular statistic PMAD (Percent
Mean Absolute Deviation):

PMAD =

(cid:80)z
(cid:80)z

t=m |et|
t=m Xt

,

which can be seen as the overall prediction error. We also
deﬁne a variant of it, called underestimation error, which
considers only the underestimations as follows:

PMAD(cid:48) =

(cid:80)z
(cid:80)z

t=m et
t=m Xt

f or et > 0 and corresponding Xt.

Underestimation error is useful especially when the defender is
willing to over-provision some defense resources and is more
concerned with the attacks that can be overlooked because of
insufﬁcient provisioning of defense resources (e.g., when the
attack rate is high and beyond the processing capacity of the
defender’s provisioned defense resources, the defender may
have to skip examining the trafﬁc in order not to disrupt the
services in question). It is also convenient to use the following
overall accuracy measure (OA for short) and underestimation
accuracy measure (UA for short):

Ha: Xt = Zt + (cid:96)(t/n),

OA = 1 − PMAD,

UA = 1 − PMAD(cid:48).

where Zt is as in the previous test, (cid:96)(·) ∈ [0, 1] is a Lipschitz
continuous function [5], and n is the sample size.

B. Two Statistical Models for Predicting Incoming Attacks

We call a model LRD-less if it cannot accommodate LRD
and LRD-aware if it can accommodate LRD. Let (cid:15)t be
independent and identical normal random variables with mean
0 and variance σ2

(cid:15) . We consider two popular models.

• LRD-less model ARMA(p, q): This is the autoregressive

moving average process of orders p and q with

Yt =

p
(cid:88)

i=1

φiYt−i + (cid:15)t +

q
(cid:88)

j=1

θj(cid:15)t−j.

It is one of the most popular models in time series [7].

III. THE STATISTICAL FRAMEWORK

A. The Concept of Stochastic Cyber Attack Processes

Concept at the right level of abstraction is often important.
For describing and modeling cyber attacks, stochastic cyber
attack processes (often called attack processes for short in the
rest of paper) are a natural abstraction because cyber attack
events in principle formulate Point Processes [9]. Formally, a
stochastic cyber attack process is described as {Xt : t ≥ 0},
where Xt is the random variable (e.g., attack rate) at time
t. Rigorously characterizing the mathematical/probabilistic
properties of stochastic cyber attack processes is an important
problem for theoretical cyber security research, and may not
be possible before we have good understandings about their
statistical properties — the present paper is one such effort.

Stochastic cyber attack processes can be instantiated at
multiple resolutions. For example, network-level attack pro-
cesses accommodate cyber attacks against networks of inter-
est; victim-level attack processes accommodate cyber attacks
against individual computers or IP addresses; port-level attack
processes accommodate cyber attacks against individual ports.
The distinction of model resolution is important because a
high-level (i.e., low-resolution) attack process may be seen as
the superposition of multiple low-level (i.e., high-resolution)
attack processes, which may help explain the cause, or rule out
some candidate causes, of a property exhibited by the high-
level process (see Step 5 in Section III-B below for general
description and Section IV-E for case study).

(a) Illustration of victim-level stochastic cyber attack processes with respect
to individual victim IP addresses, where dots represent attack events and (for
example) attacks against victim IP 1 arrive at time t1, . . . , t9.

(b) Elaboration of a victim-level attack process with respect to victim IP 1.

Fig. 1.

Illustration of victim-level stochastic cyber attack processes

Figure 1(a) illustrates the attacks against individual victim
IP addresses, where dots on the same time axis formulate a
victim-level attack process. Figure 1(b) further shows that a
victim is attacked by N attackers (or attacking computers) at
some ports and the attacks arrive at time t1, . . . , t9.

B. The Framework

The framework is presented as a 5-step procedure. Step 1
(data pre-processing) is presented for completeness because
the data may be collected by software or hardware. Step 2
(basic statistical analysis) serves the purpose of providing hints
for Step 3 (advanced statistical analysis for identifying statisti-
cal properties of attack processes), which in turn serves as the
base for Step 4 (“gray-box” prediction) and Step 5 (exploring
the cause of the newly identiﬁed statistical properties).

It

Step 1: Data pre-processing:

is now a common
practice to treat honeypot-captured data as attacks because
there are no legitimate services and the honeypot computers
passively wait for incoming events. Honeypot-captured cyber
attack data is often organized according to the honeypot IP
addresses. Pre-processing mainly deals with two issues. First,
we may need to differentiate the attack trafﬁc corresponding
to the production ports that are associated to some honeypot
programs/services, and the attack trafﬁc corresponding to the
non-production ports that are not associated to any services.
Second, in order to analyze statistical properties exhibited
by honeypot-captured cyber attack data, we advocate using

3

ﬂows, rather than IP packets, to represent attacks because of
the following. (i) For low-interaction honeypots data, attack
payload is often missing and information about attacks is often
captured from the perspective of communication behaviors.
This suggests that ﬂow is appropriate for analyzing honeypot-
captured cyber attack data. (ii) Flow-based intrusion detection
is complementary to the traditional packet-based intrusion
detection. For example, ﬂow-based abstraction can be used
to detect attacks such as DoS (denial-of-service), scan, worm
[10], [11]. (iii) Flow-based abstraction can deal with encrypted
attack payload [12], which cannot be dealt with by packet-level
analysis.

The concept of ﬂow accommodates both TCP and UDP.
There are COTS devices that can readily extract ﬂows. How-
ever, when honeypot data is collected by software in the
format of pcap data, we need to parse it and re-assemble into
ﬂows. Since ﬂow assembly is a standard technique, in what
follows we only brieﬂy review the assembly of TCP ﬂows.
A TCP ﬂow is uniquely identiﬁed from honeypot-collected
raw pcap data via the attacker’s IP address, the port used
by the attacker, the victim IP address in the honeypot, and
the port that is under attack. An unﬁnished TCP handshake
can also be treated as a ﬂow (attack) because an unsuccessful
handshake can be caused by events such as:
in
question is busy (i.e., the connection is dropped). For ﬂows
that do not end with the FIN ﬂag (which would indicate safe
termination of TCP connection) or the RST ﬂag (which would
indicate unnatural termination of TCP connection), we need to
choose two parameters in the pre-processing. One parameter
is the ﬂow timeout time, meaning that a ﬂow is considered
expired when no packet of the ﬂow is received during a
time window. For example, 60 seconds would be reasonable
for low-interaction honeypots that provide limited interactions
[13], but a longer time may be needed for high-interaction
honeypots. The other parameter is the ﬂow lifetime, meaning
that a ﬂow is considered expired when a ﬂow lives longer than
a pre-determined lifetime, which can be set as 300 seconds
for low-interaction honeypots [13] but a longer time may be
needed for high-interaction honeypots.

the port

Step 2: Basic statistical analysis: The basic statistics
of cyber attack data can offer hints for advanced statistical
analysis. For stochastic cyber attack processes, the primary
statistic is the attack rate, which describes the number of
attacks that arrive at unit time (e.g., minute or hour or day).
Note that attack rate can be instantiated at various resolutions
of attack processes, such as: network-level attack rate, victim-
level attack rate and port-level attack rate. The secondary
statistic is the attack inter-arrival time, which describes the
time intervals between two consecutive attack events. By
investigating the min, mean, median, variance and max
of these statistics, we can identify outliers and obtain hints
about the properties of the attack processes. For example,
if the attack events are bursty, an attack process may not
be Poisson, which can serve as a hint for further advanced
statistical analysis.

Step 3: Advanced statistical analysis: Identifying sta-
tistical properties of attack processes: This step is to
identify statistical properties of attack processes at resolutions

of interest. A particular question that should be asked is: Are
the attack processes Poisson? Recall that the Poisson process
counts the number of events that occur during time intervals,
where “events” in the context of this paper are the attacks
observed by honeypots. If not Poisson, what properties do
they exhibit? It would be ideal that the attack processes are
Poisson because we can easily characterize Poisson processes
with very few parameters, and because there are many mature
methods and techniques for analyzing them. For example, we
can use the property — the superposition of Poisson processes
is still a Poisson process [14] — to simplify problems when
we consider attack processes at multiple resolutions/levels.
In many cases, attack processes may not be Poisson. For
characterizing such processes, we need to use advanced sta-
tistical methods, such as Markov process, L´evy process, and
time-series methods [9], [15]. This step is crucial because
identifying advanced statistical properties can pave the way for
answering the next questions. This step can be quite involved
in terms of statistical skills when the attack processes are not
Poisson.

Step 4: Exploiting the statistical properties: This step
addresses the following question: How can we exploit the
statistical properties of stochastic cyber attack processes to
do useful things? One exploitation is to conduct “gray-box”
prediction of the incoming attacks, at least in terms of attack
rate at the appropriate resolution (which in turn depends on
the data is collected by low-interaction or high-interaction
if an
honeypot). By “gray-box” prediction we mean that
attack process exhibits a certain property that is identiﬁed
in Step 3 (e.g., Long-Range Dependence [1], [2] or Short-
Range Dependence [15], [3]), the prediction model should
accommodate the property as well. Algorithm 1 describes a
general “gray-box” prediction algorithm, where {X1, . . . , Xt}
is the sequence of attack rates observed at time 1, . . . , t, and h
is the number of steps (e.g., hours) we want to predict ahead
of time. In addition to “gray-box” prediction, Algorithm 1 is
novel also because it selects the best model (from a family of
models) at each prediction step, which is important because
there may be no single model that can ﬁt the observed data
well at all steps.

Algorithm 1 Prediction Algorithm
INPUT: observed attack rates {X1, . . . , Xt}, h (steps ahead)
OUTPUT: prediction results Yt+h, Yt+h+1, . . .

1: repeat
2:

Fit {X1, . . . , Xt} to obtain the best model Mt from a
family of models that accommodate the newly identiﬁed
statistical properties (i.e., “gray-box” prediction) with
respect to an appropriate model selection criterion (e.g.,
Akaike information criterion (AIC) [7])
Use Mt to predict Yt+h, the number of attacks that will
arrive during the (t + h)th step

3:

4: Xt+1 ← newly observed attack rate at time t + 1
t ← t + 1 {observing more data as t evolves}
5:
6: until no need to predict further

We note that although the prediction is geared toward

4

honeypot-oriented trafﬁc, it can be useful for defending pro-
duction networks as well. This is because when honeypot-
captured attacks are increasing (or decreasing), the attack rate
with respect to production networks might also be increasing
(or decreasing) as long as the honeypots are approximately
uniformly deployed in the IP address space in question. This
can be achieved by blending honeypot IP addresses into
production IP addresses. Since being able to predict incoming
attacks (especially hours ahead of time) is always appealing,
this would give incentives to deploy honeypots as such. As a
result, it is possible to characterize the relation between the
attack trafﬁc into the honeypot IP addresses and the attack
trafﬁc into the production IP addresses.

(a) Decomposition of a victim-level attack process into multiple port-level
attack processes, where the attack process corresponding to Port 1 describes
the attacks that arrive at time t2 and t5, the attack process corresponding to
Port 2 describes the attacks that arrive at time t1, t6 and t9, etc.

(b) Attacker-level attack process can be derived from victim-level attack
process by ignoring the subsequent attacks launched by the same attacker.
In this example, the attacker-level attack process corresponding to the victim
describes the attacks that arrive at time t1, t2, t3, t4.

Fig. 2. Two approaches to exploring causes of statistical properties

Step 5: Exploring cause of the statistical properties:

This step aims to address the following question: What caused
the statistical properties exhibited by stochastic cyber attack
processes? This question is interesting because it reﬂects a
kind of “natural” phenomenon in cyberspace. It would be ideal
that one can mathematically prove the cause of a property.
This type of “theoretical proof” approach is often difﬁcult, as
witnessed by the outcome of the past two decades of effort
at studying the long-range dependence exhibited by benign
Internet trafﬁc (see Section VI). Therefore, we advocate the
“experimental” approach, which includes the following two
speciﬁc methods. The ﬁrst method is to study the decomposed
lower-level (i.e., higher-resolution) stochastic cyber attack
processes. For example, in order to investigate whether or not
a certain property is caused by another certain property of the
low-level (i.e., high-resolution) processes, we can decompose
a victim-level attack process into port-level attack processes
that correspond to the individual ports of the victim. This is
illustrated in Figure 2(a), where the victim-level attack process
is decomposed into M port-level attack processes.

The second method is to investigate whether or not a certain
property is caused by the intense (consecutive) attacks that
are launched by individual attackers. For this purpose, we can
consider the attacks against each victim that are launched by
distinct attackers. As illustrated in Figure 2(b), even though
an attacker launched multiple consecutive attacks against a
victim, we only need to consider the ﬁrst attack. If the attacker-
level attack processes do not exhibit
is
exhibited by the victim-level attack processes, we can conclude
that the property is probably caused by the intensity of the
attacks that are launched by individual attackers.

the property that

IV. CASE STUDY

To demonstrate use of the framework, we conduct a case
study by applying it to analyze a dataset that was collected by a
low-interaction honeypot. As mentioned above, the framework
can be equally applied to analyze high-interaction honeypot
data.

A. Data Pre-Processing

The dataset for our case study was collected by a honeypot,
which ran four popular low-interaction honeypot software
programs: Dionaea [16], Mwcollector [17], Amun [18], and
Nepenthes [19]. The vulnerable services offered by all four
honeypot programs are SMB, NetBIOS, HTTP, MySQL and
SSH, each of which is associated to a unique TCP port.
These are the production ports. Each honeypot IP address was
assigned to one of these programs and was completely isolated
from the other honeypot IP addresses. A single honeypot
computer was assigned with multiple IP addresses to run
multiple honeypot software programs. A dedicated computer
was used to collect the raw network trafﬁc as pcap ﬁles,
which were timestamped at the resolution of microsecond.
Table I summarizes the dataset, which corresponds to 166
victim/honeypot IP addresses for ﬁve periods of time. These
periods are not strictly consecutive because of network/system
maintenance etc.

Period

Dates

I
II
III
IV
V

11/04/2010 - 12/21/2010
02/09/2011 - 02/27/2011
03/12/2011 - 05/06/2011
05/09/2011 - 05/30/2011
06/22/2011 - 09/12/2011

Duration
(days)
47
18
54
21
80

# victim IPs

166
166
166
166
166

TABLE I
DATA DESCRIPTION

In our pre-processing, we resolve the two issues described
in the framework as follows. First, we disregard the attacks
against the non-production ports because such TCP connec-
tions are often dropped. Note that the speciﬁc attacks against
the production ports are dependent upon the vulnerabilities
emulated by the honeypot programs (e.g., Microsoft Windows
Server Service Buffer Overﬂow MS06040 and Workstation
Service Vulnerability MS06070 for the SMB service). Since
low-interaction honeypots do not capture sufﬁcient informa-
tion for precisely recognizing the speciﬁc attacks, we do not
look into speciﬁc attack types. Second, for ﬂows that do not

5

end with the FIN ﬂag (indicating safe termination of TCP
connection) or the RST ﬂag (indicating unnatural termination
of TCP connection), we use the following two parameter
values: 60 seconds for the ﬂow timeout time and 300 seconds
for the ﬂow lifetime.

B. Basic Statistical Analysis

We consider the per-hour attack rate at three resolutions: the
honeypot network, individual victim IP address, and individual
production port of each victim. The choice of per-hour is
natural, while noting that per-day attack rate is not appropriate
because each period is no more than 80 days. Since the
numbers of victim-level and port-level attack processes are
much larger than the number of network-level attack processes,
different methods are used to represent their basic statistics.

Basic statistics of network-level attack processes: For
network-level attack processes, it is feasible and appropriate
to plot the time series of the attack rate (per hour), namely the
total number of attacks against the honeypot network of 166
victims. Figure 3 plots the time series of attacks. We make the
following observations. First, the ﬁve periods exhibit different
attack patterns. For example, Periods I, II and V are relatively
stationary. Second, there are some extremely intense attacks
during some hours in Periods III and IV. The speciﬁc hour
corresponding to the extreme value in Period III is Apr 01,
2011, 12 Noon (US Eastern Time); the attacks are against the
SSH services. It is evident that the attacks are brute-forcing
password. The peak of attacks during Period IV occurs at
May 16, 2011, 3 AM (US Eastern Time). The intense attacks
are against the HTTP service. We ﬁnd no information from
the Internet whether or not there are worm/botnet outbreaks
that correspond to the peaks. Third, although the ﬁve plots
exhibit some change-points, a formal statistical analysis (using
the method reviewed in Section II for removing spurious
LRD) shows that change-points exist only in Period III, which
correspond to the largest attack rate. This means that visual
observations can be misleading and rigorous statistical analysis
is perhaps necessary.

Table II describes the basic statistics of the network-level
attack rate. On average, the victim network is least intensively
attacked during Period IV because the average per-hour attack
rate is about 9861, which is smaller than the average attack
rate during the other periods. The variances of attack rates
are much larger than the corresponding mean attack rates,
which hints that these processes are not Poisson. As we show
via formal statistical analysis in Section IV-C, these processes
actually exhibit LRD instead.

I
II
III
IV
V

Period MIN
2572
5155
6732
637
1417

Mean Median
28263
29594
19579
6528
15248.5
TABLE II
BASIC STATISTICS OF NETWORK-LEVEL ATTACK PROCESSES.

Variance
401243263.2
167872819.0
72436071.5
93209085.3
205276388.4

30963.2
31576.8
20382.3
9861.1
18960.2

Max
151189
98527
196210
89718
120221

Basic statistics of victim-level attack processes: For
victim-level attack processes, we consider the attack rate or

6

(a) Period I

(b) Period II

(c) Period III

(d) Period IV

(e) Period V

Fig. 3. Time series plots of the network-level attack processes. The x-axis indicates the relative time with respect to the start time for each period (unit:
hour). The y-axis indicates attack rate, namely the number of attacks (per hour) arriving at the honeypot.

the number of attacks (per hour) arriving at a victim. Since
there are 166 victims in each period, we cannot afford to plot
time series of victim-level attack processes.

MAX(·)

I
II
III
IV
V

LB
8
43
3
1
8.5

LB
247
335
125
41
274

LB
32.1
49.8
11.5
3.5
34.0

Period

Mean(·)

Median(·)

Variance(·)
LB
1589.9
1466.5
254.0
29.7
1225.6

UB
1810.4
1412.0
1513.5
1663.4
2228.8

UB
3219758.8
1553585.6
676860.7
2808045.2
4639659.1

UB
1327
1112
1490
1184
1526.5
TABLE III
BASIC STATISTICS OF VICTIM-LEVEL ATTACK PROCESSES: ATTACK RATE
(PER HOUR). FOR A SPECIFIC PERIOD AND A SPECIFIC STATISTIC
X ∈ {Mean, Median, Variance, MAX}, LB (UB) STANDS FOR THE
LOWER-BOUND OR MINIMUM (UPPER-BOUND OR MAXIMUM) OF
STATISTIC X AMONG ALL THE VICTIMS AND ALL THE HOURS. IN OTHER
WORDS, THE LB AND UB VALUES REPRESENT THE MINIMUM AND
MAXIMUM PER-HOUR ATTACK RATE OBSERVED DURING AN ENTIRE
PERIOD AND AMONG ALL THE VICTIMS.

UB
14403
10995
5287
7793
12267

Table III summaries the observed lower-bound (minimum)
and upper-bound (maximum) values of per-hour attack rate
for each statistic among the 166 victims. By taking Period
I as an example, we observe the following. The average
per-hour attack rate (among all the victims and among all
the median per-
the hours) is 32–1810 attacks per hour;
hour attack rate is 8–1327 attacks per hour; the maximum
number of attacks against a single victim can be up to 14403.
Boxplots of the four statistics, which are not included for
the sake of saving space, show that the ﬁve periods exhibit
somewhat similar (homogeneous) statistical properties. For
example, each statistic has many outliers in each period. By
looking into all individual victim-level attack processes, we
ﬁnd that among all the 830 victim-level attack processes (166
victims/period × 5 periods = 830 victims), the variance of
attack rate is at least 3.5 times greater than the mean attack rate
corresponding to the same victim. This fact — the variance is
much larger than the mean attack rate — hints that Poisson
models may not be appropriate for describing victim-level
attack processes. This suggests us to conduct formal statistical
tests, which will be presented in Section IV-C.

Basic statistics of port-level attack processes: For
port-level attack processes, Table IV summarizes the lower-
bound (minimum value) and upper-bound (maximum value)
for each statistic. By taking Period I as an example, we
observe the following. There can be no attacks against some
production ports during some hours, which explains why the
Mean per-hour attack rate can be 0. On the other hand, a port

(speciﬁcally, port 445 at Nov 6, 2010, 9 AM US Eastern time)
can be attacked by 14363 attacks within one hour. Like what
is observed from the victim-level attack processes, we observe
that the variance of attack rate is much larger than the mean
attack rate. This means that the port-level attack processes are
not Poisson. Indeed, as we will see in Section IV-E, many
port-level attack processes are actually heavy-tailed.

Period

Mean(·)

Median(·)

Variance(·)

MAX(·)

I
II
III
IV
V

LB
0
0
0
0
0

LB
0
0
0
0
0

LB
0
0
0
0
0

UB
1740.7
1251.5
1482.1
1613.4
2169.8

UB
3249318.9
1545078.5
661847.3
2588396.6
4629744.3

UB
1196
948
1458
1142
1448.5
TABLE IV
BASIC STATISTICS OF PORT-LEVEL ATTACK PROCESSES: ATTACK-RATE
(PER HOUR). AS IN TABLE III, LB AND UB VALUES REPRESENT THE
MINIMUM AND MAXIMUM PER-HOUR ATTACK RATE OBSERVED DURING
AN ENTIRE PERIOD AND AMONG ALL PRODUCTION PORTS OF THE
VICTIMS.

UB
14363
10992
5275
6961
12267

LB
1
1
1
1
1

C. Identifying Statistical Properties of Attack Processes

We now characterize the statistical properties exhibited by
network-level and victim-level attack processes. In particular,
we want to know they exhibit similar (if not exactly the same)
or different properties. In the above, we are already hinted that
the attack processes are not Poisson. In what follows we aim
to pin down their properties.

Network-level attack processes exhibit LRD: The hint
that network-level attack processes are not Poisson suggests
us to identify their properties. It turns out that the network-
level attack processes exhibit LRD as demonstrated by their
Hurst parameters. Table V describes the six kinds of Hurst pa-
rameters corresponding to the network-level attack processes.
Although the Hurst parameters suggest that they all exhibit
LRD, a further analysis shows the LRD exhibited in Period
III is spurious because it was caused by the non-stationarity
of the process. Therefore, 4 out of the 5 network-level attack
processes exhibit (legitimate) LRD.

Victim-level attack processes exhibit LRD: For the
830 (166 victims/period ×5 periods =830) victim-level attack
processes, we ﬁrst rigorously show that they are not Poisson.
Assume that the attack inter-arrival times are independent
and identically distributed exponential random variables with
distribution

F (x) = 1 − e−λx, λ > 0, x ≥ 0.

Per
1.03
0.75
0.63
1.07
1.03

RS
0.80
0.74
0.74
1.05
0.74

AGV
0.95
0.59
0.52
0.97
0.78

Period
I
II
III
IV
V

Box Wave
0.75
1.00
0.84
0.97
0.65
0.63
1.22
0.97
0.80
0.80

Peng
0.88
0.86
0.65
0.95
0.74
TABLE V
THE ESTIMATED HURST PARAMETERS FOR NETWORK-LEVEL ATTACK
PROCESSES. THE SIX ESTIMATION METHODS ARE REVIEWED IN APPENDIX
A-A. NOTE THAT A HURST PARAMETER VALUE BEING NEGATIVE OR
BEING GREATER THAN 1 MEANS THAT EITHER THE ESTIMATION METHOD
IS NOT SUITABLE OR THE ATTACK PROCESS IS NON-STATIONARY.

LRD?
Yes
Yes
No
Yes
Yes

To test
the exponential distribution, we ﬁrst estimate the
unknown parameter λ by the maximum likelihood method.
Then, we compute the Kolmogorov-Smirnov (KS), Cram´er-
von Mises (CM), and Anderson-Darling (AD) test statistics
[20], [21] (cf. Appendix A-C for a review) and compare them
against the respective critical values.

KS

Period
(days) min max
0.54
0.13
0.50
0.06
0.65
0.06
0.81
0.04
0.65
0.08

I
II
III
IV
V
CV

CM

AD

min
482.30
47.08
163.71
3.44
323.39

max
59543.87
20437.82
51434.32
31376.27
214543.54

min max
inf
inf
inf
inf
inf

inf
298.73
1103.70
22.83
inf
1.13

0.01

0.22
TABLE VI
MINIMUM VALUES OF THE THREE TEST STATISTICS FOR ATTACK
INTER-ARRIVAL TIME (UNIT: SECOND) CORRESPONDING TO THE
VICTIM-LEVEL ATTACK PROCESSES, WHERE min AND max REPRESENT
THE MINIMAL AND MAXIMAL MINIMUM VALUES AMONG ALL
VICTIM-LEVEL ATTACK PROCESSES IN A PERIOD, AND Inf MEANS THE
VALUE IS EXTREMELY LARGE.

Table VI reports the minimum test statistics, where the
critical values for the test statistics are based on signiﬁcance
level .05 and obtained from [22], [23]. Since the values are
far from the critical values, there is no evidence to support
the exponential distribution hypothesis. Because the minimum
test statistics violate the exponential distribution assumption
already, greater test statistics must violate the exponential
distribution assumption as well.

(a) QQ-plot of inter-arrival time of
victim-level attack process that ex-
hibits the minimum KS, CM and AD
value simultaneously

(b) Boxplot of Hurst parameters of
attack rate of the victim-level attack
processes corresponding to the 5 pe-
riods

Fig. 4. Victim-level attack processes are not Poisson but exhibit LRD

We also use QQ-plot to evaluate the goodness-of-ﬁt of expo-
nential distributions for the attack inter-arrival time of victim-
level attack processes that simultaneously exhibit the minimum

7

test statistics in Table VI. This is the victim from Period IV
with HKS = 0.04, HCM = 3.44 and HAD = 22.83. If the attack
inter-arrival time corresponding to this particular victim does
not exhibit the exponential distribution, we conclude that no
attack inter-arrival time in this dataset exhibits the exponential
distribution. The QQ plot is displayed in Figure 4(a). We
observe a large deviation in the tails. Hence, exponential
distribution cannot be used as the distribution of attack inter-
arrival times, meaning that all the victim-level attack processes
are not Poisson.

Given that the victim-level attack processes are not Poisson,
we suspect
they might exhibit LRD as well. Figure 4(b)
shows the boxplots of Hurst parameters of attack rate. We
observe that Periods I and II have relatively large Hurst
parameters, suggesting stronger LRD. Table VII summarizes
the minimums and maximums of the estimated Hurst param-
eters of attack rates. Consider Period I as an example, we
observe that the attack processes corresponding to 163 (out of
the 166) victims have average Hurst parameters falling into
[.6, 1] and thus suggest LRD, where the average is taken over
the six kinds of Hurst parameters. However, only 159 (out
of the 163) victim-level attack processes exhibit legitimate
LRD because the other 4 (out of the 163) victim-level attack
processes are actually spurious LRD (i.e., caused by the
non-stationarity of the processes). We also observe that in
Period III, there are only 87 victim-level attack processes that
exhibit LRD. Overall, 70% victim-level attack processes, or
159 + 116 + 87 + 125 + 89 = 576 out of 166 × 5 = 830 attack
processes, exhibit LRD.

Port-level attack processes exhibit LRD: Table VIII
summarizes the Hurst parameters of port-level attack pro-
cesses. We observe that there are respectively 316, 397, 399,
328, 406 port-level attack processes that exhibit LRD. Since
there are 5 production ports per victim and 166 victims, there
are 830 port-level attack processes per period. Since there are
5 periods of time, there are 4150 port-level attack processes in
total (830 ports/period × 5 periods=4150 ports). This means
that (316 + 397 + 399 + 328406)/4150 = 44.5% port-level
attack processes exhibit LRD.

Summary: In summary, we observe that 80% (4 out of
5) network-level attack processes exhibit LRD, 70% victim-
level attack processes exhibit LRD, and 44.5% port-level
attack processes exhibit LRD. This means that defenders
should expect that the burst of attacks will sustain, and that
cyber attack processes should be modeled using LRD-aware
stochastic processes.

D. Exploiting LRD to Predict Attack Rates

Assuming that the attacks arriving at honeypots are repre-
sentative of, or related to, the attacks arriving at production
networks (perhaps in some non-trivial fashion that can be
the
identiﬁed given sufﬁcient data), being able to predict
number of incoming attacks hours ahead of time can give
the defenders sufﬁcient early-warning time to prepare for
the arrival of attacks. Intuitively,
is good
at prediction in this context should accommodate the LRD
property. This is conﬁrmed by our study described below.

the model

that

8

RS

Per

Box

AGV

Period

I
II
III
IV
V

1.01
0.94
0.95
1.13
1.01

0.98
0.98
0.96
1.00
0.99

0.46
0.40
0.30
0.12
0.14

0.66
0.56
0.53
0.49
0.45

Peng
min max min max min max min max min max
1.15
0.53
1.32
0.49
0.98
0.65
1.32
0.40
1.30
0.52

0.55
1.39
0.33
1.69
0.43
1.22
0.42
1.74
1.43
0.57
TABLE VII
THE ESTIMATED HURST PARAMETERS FOR ATTACK RATE (PER HOUR) OF THE VICTIM-LEVEL ATTACK PROCESSES. THE SIX ESTIMATION METHODS ARE
REVIEWED IN APPENDIX A-A. NOTE THAT A HURST VALUE BEING NEGATIVE OR BEING GREATER THAN 1 MEANS THAT EITHER THE ESTIMATION
METHOD IS NOT SUITABLE OR THE PROCESS IS NON-STATIONARY. THE COLUMN “# OF VICTIMS W/ ¯H ∈ [.6, 1]” REPRESENTS THE TOTAL NUMBER OF
VICTIM-LEVEL ATTACK PROCESSES WHOSE AVERAGE HURST PARAMETERS ∈ [.6, 1] (WHERE AVERAGE IS AMONG THE SIX KINDS OF HURST
PARAMETERS), WHICH SUGGESTS THE PRESENCE OF LRD. THE COLUMN “# OF VICTIMS W/ LRD” INDICATES THE TOTAL NUMBER OF VICTIM-LEVEL
ATTACK PROCESSES THAT EXHIBIT LRD RATHER THAN SPURIOUS LRD. (THE SAME NOTATIONS WILL BE USED IN THE DESCRIPTION OF TABLES VIII
AND XIII.)

Wave
min max
0.96
0.40
1.33
-0.55
1.02
0.33
1.47
-0.34
1.18
-0.16

# victims w/
¯H ∈ [.6, 1]
163
130
93
126
158

# victims w/
LRD
159
116
87
125
89

0.73
0.53
0.44
0.33
0.47

1.14
1.37
1.06
1.45
1.22

Period

I
II
III
IV
V

Peng

RS
min max
1.01
0.41
1.50
0.23
1.01
0.14
1.17
0.25
1.14
0.43

0.39
1.55
0.26
1.68
0.34
1.28
0.29
1.70
1.52
0.40
TABLE VIII
THE ESTIMATED HURST PARAMETERS FOR PORT-LEVEL ATTACK RATE (PER HOUR) OF THE PORT-LEVEL ATTACK PROCESSES.

Per
min max min max min max
1.48
-0.15
1.45
0.18
1.07
0.27
1.50
0.24
1.41
0.42

Wave
min max
1.00
-0.18
1.38
-0.60
1.00
0.08
1.72
-1.10
1.43
-1.07

AGV
min max
0.98
-0.18
0.97
0.04
0.96
-0.02
1.00
0.05
0.99
0.12

# ports w/
¯H ∈ [.6, 1]
349
419
422
339
528

total # of
ports
830 + 0
829 + 1
830 + 0
828 + 2
830 + 0

0.38
0.32
0.38
0.18
0.45

1.23
1.51
1.08
1.57
1.40

Box

# ports w/
LRD
316
397
399
328
406

Prediction results for network-level attack processes:
In order to evaluate the accuracy of the prediction results,
we use Algorithm 2, which is an instantiation of Algorithm
1 while considering prediction errors for evaluation purpose.
Let {X1, . . . , Xn} be the time series of observed attack
rates. Algorithm 2 uses portion of the observed attack rates
{X1, . . . , Xn} for ﬁtting a prediction model and compares
the predicted attack rates to the observed attack rates for
computing the prediction accuracy, where h be an input
parameter indicating the number of steps (i.e., hours) we
will predict ahead of time, and p be another input parameter
indicating location of the prediction starting point. In order
to build reliable models, we set p = 50% meaning that 50%
of the observed data is used as the training data for building
models.

Algorithm 2 Prediction Evaluation Algorithm
INPUT: observed attack rates {X1, . . . , Xn}, h (hours ahead),
p ∈ (0, 1) indicates prediction start point
OUTPUT: prediction accuracy

1: t ← (cid:98)n ∗ p(cid:99)
2: while t ≤ n − h do
3:

Fit {X1, . . . , Xt} to obtain an optimal model Mt as
follows: ﬁt the data to 25 models FARIMA(p, d, q) with
varying parameters p and q (which uniquely determine
parameter d) and select the best ﬁtting model based
on the AIC criterion [7].{The case of ARMA(p, q) is
similar.}
Use Mt to predict Yt+h, the number of attacks that will
arrive during the (t + h)th step
Compute prediction error et+h = Xt+h − Yt+h
t ← t + 1

5:
6:
7: end while
8: Compute PMAD, PMAD(cid:48), OA, UA as deﬁned in Section

4:

II-C

9: return PMAD, PMAD(cid:48), OA, UA

Now we report

the prediction results, while comparing
the LRD-aware FARIMA model and the LRD-less ARMA
model. Table IX describes the prediction error of the network-
level attack processes. We observe the following. First, for
Periods I and II, both 1-hour ahead and 5-hour ahead FARIMA
prediction errors are no greater than 22%. However, the 10-
hour ahead FARIMA prediction is pretty bad. This means that
LRD-aware FARIMA can effectively predict the attack rate
even ﬁve hours ahead of time. This would give the defender
enough early-warning time.

Second, Period III network-level attack process exhibits spu-
rious LRD. However, both the LRD-aware FARIMA and the
LRD-less ARMA models can predict incoming attacks up to 5
hours ahead of time. Indeed, the prediction error of FARIMA

PMAD

PMAD(cid:48)

Period

FARIMA ARMA FARIMA ARMA

0.179
0.217
0.298
0.548
0.517

0.446
0.363
0.273
0.526
0.529

1-hour ahead prediction (h = 1, p = 0.5)
0.173
I
0.149
II
0.305
III
0.126
IV
V
0.424
5-hour ahead prediction (h = 5, p = 0.5)
0.292
I
0.420
II
0.246
III
0.226
IV
0.414
V

0.556
0.351
0.272
0.838
0.555

0.206
0.212
0.297
0.847
0.526

10-hour ahead prediction (h = 10, p = 0.5)

0.157
0.149
0.312
0.106
0.411

0.314
0.411
0.250
0.207
0.417

I
II
III
IV
V

0.314
0.277
0.202
0.282
0.402

0.869
1.024
1.00
0.648
0.982

0.801
1.034
1.002
0.627
0.952
TABLE IX
PREDICTION ERROR OF NETWORK-LEVEL ATTACK PROCESSES USING THE
LRD-AWARE FARIMA AND THE LRD-LESS ARMA, WHERE PREDICTION
ERRORS ARE DEFINED IN SECTION II. p = 0.5 MEANS THAT WE START
PREDICTING IN THE MIDPOINT OF EACH NETWORK-LEVEL ATTACK
PROCESS.

0.281
0.284
0.201
0.490
0.412

is slightly greater than the prediction error of ARMA. This
reiterates that if an attack process does not exhibit LRD,
it is better not to use LRD-aware prediction models; if an
attack process exhibits LRD, LRD-aware prediction models
should be used. This highlights the advantage of “gray-box”
prediction over “black-box” prediction, which demonstrates
the principal utility of the statistical framework.

Third, although Period IV exhibits LRD, even its 1-hour
ahead FARIMA prediction is not good enough, with prediction
error greater than 50%. While it is unclear what caused this
effect, we note that the underestimation error PMAD(cid:48) for 5-
hour ahead prediction is still reasonable for Period IV (22.6%
for FARIMA and 20.7% for ARMA). This means that if one
is willing to over-provision defense resources to some extent,
then the prediction for Period IV is still useful.

Fourth, Period V resists both prediction models in terms of
both overall prediction error PMAD and underestimation error
PMAD(cid:48). The fundamental cause of the effect is unknown at the
moment, and is left for future studies. Nevertheless, we suspect
that Extreme Value Theory could be exploited to address this
problem.

Prediction results for victim-level attack processes:

Since there are 166 victims per period, there are 830 victim-
level attack processes for which we will do prediction. Recall
that 70% victim-level attack processes exhibit LRD. We use
Table X to succinctly present the prediction results, which
to 10-hour ahead predictions during the
are with respect
last 100 hours of each time period. We make the following
observations. First, the LRD-aware FARIMA model performs
better than the LRD-less ARMA model. For example, among
the 152 (out of the 159) victim-level attack processes in
Period I that exhibit LRD and are amenable to prediction (i.e.,
the Maximum Likelihood Estimator actually converges; the
Estimator does not converge for 159-152=7 LRD processes
though), FARIMA can predict for 29 victim-level attack pro-
cesses about their 10-hour ahead attack rates with at least 70%

9

overall accuracy (OA), while ARMA can only predict for 13
victim-level attack processes at the same level of accuracy. If
the defender is willing to over-provision some resources and
mainly cares about the underestimation error (which could
cause overlooking of attacks), FARIMA can predict for 40
victim-level attack processes while ARMA can predict for 35.
Second, the victim-level attack processes in Period I exhibit
LRD and render more to prediction when compared with the
victim-level attacks processes in the other periods, which also
exhibit LRD. For non-LRD processes, neither FARIMA nor
ARMA can provide good predictions. This may be caused by
the the non-stationary of the non-LRD processes. We plan to
investigate into these issues in the future.

Summary: It is feasible to predict network-level attacks
even 5 hours ahead of time. For attack processes that exhibit
LRD, LRD-aware models can predict their attack rates better
than LRD-less models do. However, there are LRD processes
that can resist the prediction of even LRD-aware models. This
hints that new prediction models are needed.

E. Exploring (Non)Causes of LRD

Despite intensive studies in other settings, the fundamental
cause of LRD is still mysterious. One known possible cause of
LRD is the superposition of heavy-tailed processes [24], [25],
[26]. Another candidate cause of LRD is that some attackers
launch intense (consecutive) attacks (e.g., brute-forcing SSH
passwords). Now we examine the two candidate causes as
described in the framework.

LRD exhibited by network-level attack processes is

to know whether or not

not caused by heavy-tailed victim-level attack processes:
the LRD exhibited by
We want
the 4 network-level attack processes during Periods I, II,
IV and V are caused by the superposition of heavy-tailed
victim-level attack processes. That is, we want to know how
many victim-level attack processes during each of the four
periods are heavy-tailed. We ﬁnd that among the vector of
(166, 166, 166, 166) victim-level attack processes during Peri-
ods I, II, IV and V, the vector of victim-level attack processes
that exhibit heavy-tails is correspondingly (101, 0, 24, 31), by
using the POT method that is reviewed in Appendix A-B. This
means that Period I is the only period during which majority of
victim-level attack processes exhibit heavy-tails. A few or even
none processes in the three other periods exhibited heavy-tails.
This suggests that LRD exhibited by the network-level attack
processes does not have the same cause as what is believed
for benign trafﬁc [27].

LRD exhibited by victim-level attack processes is not
caused by heavy-tailed port-level attack processes: Now
we investigate whether or not the LRD exhibited by victim-
level attack processes is caused by that the underlying port-
level attack processes exhibit heavy-tails, a property brieﬂy
reviewed in Appendix A-B. Table XI shows that only 8% port-
level attack processes, or 56 + 80 + 47 + 3 + 32 = 218 out
of the (159 + 116 + 87 + 125 + 89 = 576) victims × 5
ports/victim = 2880 port-level attack processes, exhibit heavy-
tails. Moreover, only 29 (out of the 576) victim-level attack
processes have 2 or 3 port-level attack processes that exhibit

10

I

II

III

Period

total # of victims
((x1, x2)/(y))

# of victims
w/ average
OA ≥ 80%
ARMA
FARIMA
1
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

LRD: (152,152)/(159)
non-LRD: (7,7)/(7)
LRD: (109,109)/(116)
non-LRD: (50,49)/(50)
LRD: (82,82)/(87)
non-LRD: (79,79)/(79)
LRD: (118,118)/(125)
non-LRD: (41,39)/(41)
LRD: (73,73)/(89)
non-LRD: (77,61)/(77)

# of victims
# of victims
w/ average
w/ average
OA ≥ 60%
OA ≥ 70%
ARMA
FARIMA
ARMA
FARIMA
66
81
13
29
6
6
4
4
8
9
2
3
2
0
0
0
9
8
4
4
0
0
0
0
6
5
2
2
0
0
0
0
1
2
0
0
0
1
1
0
TABLE X
NUMBER OF VICTIM-LEVEL ATTACK PROCESSES THAT CAN BE PREDICTED BY THE LRD-AWARE FARIMA MODEL MORE ACCURATELY THAN THE
LRD-LESS ARMA MODEL. FOR THE COLUMN “TOTAL # OF VICTIMS ((x1, x2)/(y)),” y IS THE TOTAL NUMBER OF VICTIMS THAT EXHIBITED LRD OR
NON-LRD, x1 (OR x2) IS TOTAL NUMBER OF VICTIMS (OUT OF THE y VICTIMS) FOR WHICH THE MAXIMUM LIKELIHOOD ESTIMATOR (MLE) USED IN
THE FARIMA (ARMA) ALGORITHM CONVERGES (I.E., y − x1 AND y − x2 VICTIMS CANNOT BE PREDICTED BECAUSE THE MLE DOES NOT
CONVERGE). THE COLUMN “# OF VICTIMS W/ AVERAGE OA (OR UA) ≥ z%” REPRESENTS THE AVERAGE NUMBER OF VICTIMS (OUT OF THE x1 OR x2
VICTIMS THAT CAN BE PREDICTED), FOR WHICH THE AVERAGE PREDICTION ACCURACY IS AT LEAST z% IN TERMS OF OVERALL-ACCURACY OA (OR
UNDERESTIMATION-ACCURACY UA), WHERE AVERAGE IS OVER ALL PREDICTIONS.

# of victims
w/ average
UA ≥ 70%
ARMA
FARIMA
35
40
6
7
6
12
2
6
19
23
7
10
6
4
0
2
3
2
0
1

# of victims
w/ average
UA ≥ 60%
ARMA
FARIMA
68
89
7
7
15
26
5
13
43
50
24
31
14
11
1
4
4
16
15
24

# of victims
w/ average
UA ≥ 80%
ARMA
FARIMA
4
13
4
1
1
2
1
4
5
9
0
0
3
2
0
1
1
0
0
0

IV

V

Period

total # of
victims
exhibiting
LRD
159
116
87
125
89

I
II
III
IV
V

4
0
0
0
0
0
TABLE XI
FOR VICTIM-LEVEL ATTACK PROCESSES EXHIBITING LRD, SOME PORT-LEVEL ATTACK PROCESSES EXHIBIT HEAVY-TAILS.

.11
.40
.22
.43
.30

1
50
78
39
3
29

2
6
11
6
0
1

5
0
0
0
0
0

.11
.22
.18
.35
.25

# of victims with
certain # of sub-processes
exhibiting heavy-tail
3
0
1
2
0
2

total #
of ports
exhibiting
heavy-tail
62
103
57
3
37

# of ports
w/ shape
value
∈ (.5, 1)
1
50
2
1
5

# of ports
w/ shape
value
≥ 1
0
0
0
0
1

# of victims w/
sub-processes
exhibiting
heavy-tail
56
80
47
3
32

Shape
mean
value

Standard
deviation

heavy-tails. Further, there is only 1 port-level attack process
that exhibits inﬁnite mean because the shape value ≥ 1, and
there are 1+50+2+1+5 = 59 port-level attack processes that
exhibit inﬁnite variance because their shape values ∈ (.5, 1).
The above observations also hint that unlike in the setting
of benign trafﬁc [27], LRD exhibited by victim-level attack
processes is not caused by the superposition of heavy-tailed
port-level attack processes.

LRD exhibited by victim-level attack processes is not
caused by individual
intense attacks: Now we examine
whether or not LRD is caused by the individual attackers that
launch intense attacks. For this purpose, we consider attacker-
level attack processes, which model the attacks against each
victim that are launched by distinct attackers. In other words,
we only consider the ﬁrst attack launched by each attacker,
while disregarding the subsequent attacks launched by the
same attacker.

Table XII describes the observed lower-bound and upper-
bound of the four statistics regarding the attacker-level pro-
cesses, where the bounds are among all victims within a period
of time. By taking Period II as an example, we observe the
following: on average there are between 48 and 100 attackers
against one individual victim within one hour, and there can
be up to 621 attackers against one individual victim within one
hour. Further, attacks in Periods III and IV exhibit different
behaviors from the other three periods. From the boxplots of
the basic statistic, which are not presented for the sake of
saving space, we observe that the attackers’ behaviors are
the
actually very different

in the 5 periods. In particular,

attacker-level attack processes in Period II have many outliers
in terms of the four statistics, meaning that the attack rate
during this period varies a lot.

Period

I
II
III
IV
V

Mean(·)
LB
30.2
48.6
11.1
1.9
33.4

Median(·)
UB
LB
45
4
93
42
29
2
23
1
105
8
TABLE XII
BASIC STATISTICS OF ATTACK RATE OF THE ATTACKER-LEVEL ATTACK
PROCESSES (PER HOUR).

Variance(·)
LB
1498.1
1195.1
223.6
26.32
1132.7

MAX(·)
LB
225
306
64
40
266

UB
4094.3
6298.3
270.8
92.7
7465.2

UB
67.8
100.8
33.0
23.8
127.9

UB
432
621
100
65
605

In order to see whether or not the attacker-level attack
processes still exhibit LRD, we describe their Hurst parameters
in Table XIII. Using Period I as an example, we observe that
the attacker-level attack processes corresponding to 153 (out
of the 166) victims suggest LRD because their average Hurst
parameter ∈ [.6, 1], where the average is taken over the six
Hurst estimation methods. Moreover, none of the 153 attacker-
level processes exhibit spurious LRD. Using Period V as
another example, we observe that all 166 attacker-level attack
processes have average Hurst parameter ∈ [.6, 1], but only 77
attacker-level attack processes exhibit LRD while the other 89
attacker-level attack processes exhibit spurious LRD (caused
by non-stationarity of the processes). The above discussion
suggests that LRD exhibited by victim-level attack processes
is not caused by the intense (consecutive) attacks launched by
individual attackers, simply because most (or many) attacker-

Period

RS

AGV

Peng

Per

Box

Wave

min
0.593
0.570
0.776
0.657
0.495

max
0.977
0.883
0.994
0.920
0.758

min
0.851
0.616
0.364
0.273
0.563

max
0.958
0.950
0.747
0.955
0.727

min
0.896
0.689
0.630
0.690
0.499

max
1.111
1.070
0.748
0.872
0.806

min
1.174
0.710
0.460
0.559
0.898

max
1.334
1.152
0.679
1.206
1.114

min
0.942
0.663
0.608
0.612
0.660

max
1.185
1.242
0.746
0.952
0.977

min
0.582
-0.360
0.389
0.288
0.567

max
0.843
0.728
0.668
1.004
0.931

I
II
III
IV
V

# victims w/
¯H ∈ [.6, 1]
153
92
163
166
166

# victims w/
LRD
153
77
103
165
77

TABLE XIII
THE ESTIMATED HURST PARAMETERS OF THE ATTACK RATE OF ATTACKER-LEVEL ATTACK PROCESSES (PER HOUR).

11

level attack processes also exhibit LRD.

Summary: The LRD exhibited by stochastic cyber attack
processes is neither necessarily caused by the superposition of
heavy-tailed processes, nor necessarily caused by the intense
attacks launched by individual attackers. While we ruled out
these two candidate causes, it is an interesting and challenging
future work to precisely pin down the cause of LRD in this
context.

V. DISCUSSION

In this section we discuss the limitation of the case study

and the usefulness of the statistical framework.

A. Limitation of the Case Study

The case study has three limitations that are imposed by the
speciﬁc dataset. First, the dataset, albeit over 47+18+54+21+
80 = 220 days in total (5 periods of time), only corresponds to
166 honeypot IP addresses. We wish to have access to bigger
datasets. Still, this paper explores an important direction in
cyber security research, especially the feasibility of predicting
incoming attacks. Fortunately, the statistical framework can be
adopted by researchers to analyze their (bigger) datasets.

Second,

the dataset

is attack-agnostic in the sense that
we know the ports/services the attackers attempt to attack,
but not the speciﬁc attacks because the data was collected
using low-interaction honeypots. Although this issue can be
resolved by using high-interaction honeypots [28], there are
legitimate concerns about high-interaction honeypots from a
legal perspective. Nevertheless, the framework is equally appli-
cable to analyze high-interactive honeypot data. For example,
there might be researchers who have collected high-interaction
honeypot data and are not allowed to share the data with
others. These researchers can adopt the framework to analyze
their data at a ﬁner resolution (e.g., the attack level that an
attack process can accommodate one or multiple families of
attacks).

Third, the data is collected using honeypot rather than using
production network. For real-life adoption of the prediction
capability presented in the paper, attack trafﬁc would be
blended into the production trafﬁc. Whether or not the blended
trafﬁc also exhibits LRD is an interesting future study topic.
The main challenge again is the legal and privacy concerns in
collecting such data.

B. Usefulness of the Statistical Framework

The usefulness of the statistical framework (or analysis
methodology) can be seen from the following perspectives.

First, the framework has descriptive power because it aims to
study the advanced statistical properties exhibited by the cyber
attack data that instantiates stochastic cyber attack process.
The advanced statistical properties are not known a prior.
In order to obtain hints for the kinds of advanced statisti-
cal properties that may be relevant, the framework starts at
studying basic statistical properties exhibited by the data. The
hinted statistical properties are rigorously examined by using
advanced statistical techniques, which fall into the framework
of Hypothesis Testing (e.g., whether LRD is exhibited or not
is tested based on the values of the Hurst parameters that are
estimated using the rigorous statistical methods reviewed in
Appendix A-A). Indeed, the framework guided us to identify
the relevance of LRD in this aspect of cyber security, which
is not known until now.

Second, the framework has predictive power because, as
conﬁrmed by the case study, it allows to exploit the newly
identiﬁed advanced statistical properties to predict the attack
rate possibly hours ahead of time. This kind of property-
inspired “gray-box” prediction, rather than “black-box” pre-
diction, allows the defender to proactively provision defense
resources. Although the speciﬁc dataset used in our case
study is collected by a low-interaction honeypot, the concept
of stochastic cyber attack process can equally describe the
attacks that are observed at high-interaction honeypots. Since
high-interaction honeypots can collect more information about
attacks, the framework can be equally applied to analyze the
data with respect to speciﬁc attacks. As a result, we can predict
the arrival rate of speciﬁc attacks (i.e., attack-centric rather
than computer/network-centric). Moreover, the framework in
principle could model and predict the emergence of new at-
tacks (e.g., zero-day exploits), assuming the data exhibits LRD
or other relevant statistical properties that can be exploited for
predicting the emergence of new attacks (e.g., the probability
that a zero-day attack will arrive at a honeypot, assuming that
the data indeed contains new or zero-day attacks). Although
we do not have access to high-interaction honeypot data,
there would be researchers/practitioners who have access to
such data. This explains why we are automating our analysis
methodology, and will release the software package so that
other researchers/practitioners can use our software package
as is, or can enhance it to incorporate more analysis methods
to better serve their purposes.

Third, the framework can be adapted to describe attacks
against production networks/computers, because identifying
statistical properties of the trafﬁc would allow the defender
to detect anomalies. For example, suppose the trafﬁc during
the past days does not exhibit LRD property (or any other

is installed to ﬁlter out

relevant statistical property) but
the trafﬁc today exhibits
LRD property, then this hints possible attacks today. Suppose
the known
further that a ﬁrewall
attacks against the production networks/computers. Then the
change in statistical properties exhibited by the trafﬁc hints
the presence of new (possibly zero-day) attacks against the
production networks/computers. These hints serve as clues for
further forensics examinations. Note that for further forensics
examination of the actually attacks, we need the detailed
information about the trafﬁc. Such information is not cap-
tured by low-interaction honeypots, but can be captured by
high-interaction honeypots and production defense systems.
This explains the limitation of our case study, although the
limitation is not inherent to our framework (as it is imposed
by the speciﬁc dataset collected by low-interaction honeypot).
Fourth, consider the scenario that the honeypot IP addresses
are randomly scattered into a production network (rather than
being allocated to a consecutive chunk of IP addresses). This
would be the ideal scenario for deploying honeypots, because
it can be hard for the attacker to ﬁgure out which IP addresses
are the honeypot IP addresses. This is true especially when the
honeypot IP addresses are shufﬂed frequently and randomly
is a high-interaction one. In this
and when the honeypot
case, the attacks arrive at the honeypot IP addresses would
be comparable to the attacks that arrive at the production IP
addresses. The attacks arrive at the honeypot IP addresses can
be equally investigated using the framework presented in the
paper. This means that we can predict the arrival rate of the
attacks that will come to the honeypot IP addresses possibly
hours ahead of time. This also means that we can expect the
rate of attacks that will arrive at the production IP addresses
hours ahead of time. When the predicted attack rate is high,
the defender would need to provision more defense resources
to inspect the packets that target the production IP addresses
(e.g., for deep packet inspection). The prediction capability
gives the defender early-warning that possibly intensive attacks
will arrive in the near future. This kind of early-warning
capability is desired for real-life defense. Furthermore, the
above discussion equally applies to the case that known attacks
have been ﬁltered out by ﬁrewalls, meaning that the prediction
can be with respect to unknown (i.e., new or zero-day) attacks.

VI. RELATED WORK

We discuss the related prior work from several perspectives.
In terms of analyzing honeypot-captured cyber attack data,
there have been at least two complementary approaches. One
approach is to visualize cyber attack data, such as using
neural projection techniques to visualize the ports observed
in honeypot data [29]. However, the widely used approach
is statistical analysis. Within this approach, existing studies
mainly focused on the following aspects: (i) analyzing at-
tackers’ probing activities [30]; (ii) grouping attacks (e.g.,
[31], [13], [32], [33], [34]); (iii) characterizing Internet threats
[35], [36] such as ﬁtting the attack inter-arrival time via a
mixture of Pareto and Exponential distributions. These studies
are often based on ﬂow-level processing of data, so do we
in this paper. In contrast to these studies, we systematically

12

study the identiﬁcation, exploitation and cause of statistical
properties exhibited by honeypot data, such as LRD that is
shown to be exhibited by honeypot data for the ﬁrst time in
the present paper. To our knowledge, our framework is the ﬁrst
formal statistical analysis of honeypot-captured cyber attack
data. In particular, our study of predicting cyber attacks (in
terms of attack rate) would represent a signiﬁcant step toward
the ultimate goal of quantitatively understanding/predicting
cyber attacks.

In terms of using honeypots to improve defense, we note
that honeypots have been used to help detect various attacks in-
cluding DoS (denial-of-service) [37], worms [38], [10], botnets
[39], [40], [41], Internet-Messaging threats [42], generating
attack signatures [43], [44], and detecting targeted attacks [45].
These studies are important, but are orthogonal to the focus
of the present paper.

In terms of the LRD phenomenon, we note that LRD
was ﬁrst observed in benign trafﬁc about two decades ago
and there has been a large body of literature on this topic
(e.g., [24], [25], [26], [1]). There have been studies on the
effect of injecting abnormal events (which are not necessarily
attacks) into benign trafﬁcs that exhibit LRD. The injection
of abnormal events may disrupt the LRD exhibited by the
benign trafﬁc (see, e.g., [46]). There also have been studies on
the effect of injecting attacks into benign trafﬁcs that exhibit
LRD (in terms of number of bytes and number of packets).
The injection of attack events may not disrupt the LRD (i.e.,
the “blended” trafﬁcs still exhibit LRD) [47]. In the setting
of spams, the correlation co-efﬁcient of inter-arrival time of
spams that are sent by a group of spammers may decrease
slowly, which hints that the inter-arrival time may exhibit LRD
[48] — although this was not formally statistically analyzed
there. In contrast
the studies mentioned above, we
investigate LRD exhibited by attack rate via rigorous statistical
methods: auto-correlation serves as a hint of possible LRD,
Hurst parameters serves as the ﬁrst rigorous step of examining
LRD, and non-stationarity analysis eliminates spurious LRDs.
To our knowledge, we are the ﬁrst to report that LRD is
exhibited by honeypot-captured cyber attack data.

to all

Putting data-driven analysis of cyber attacks into a broader
context, we note that there have been studies on characterizing
blackhole-collected trafﬁc data (e.g., [49], [50]) or one-way
trafﬁc in live networks [51]. Still,
there are no advanced
statistical framework for analyzing such blackhole or one-way
trafﬁc data. More speciﬁcally, these studies differ from ours
in (i) honeypot-captured cyber attack data includes two-way
communications, whereas blackhole-collected data mainly cor-
responds to one-way communications; (ii) we rigorously ex-
plore statistical properties such as LRD, whereas their studies
do not pursue such rigorous statistical analysis. Nevertheless,
it is possible that our analysis framework can be adapted to
analyze blackhole data.

VII. CONCLUSION AND FUTURE WORK

We introduced the novel concept of stochastic cyber attack
process, which offers a new perspective for studying cyber
in particular, can be instantiated at multiple
attacks and,

resolutions such as network-level, victim-level and port-level.
We then proposed a statistical framework that is centered
on identifying, exploiting (for “gray-box” prediction) and ex-
ploring (for cause analysis) the advanced statistical properties
of stochastic cyber attack processes. In order to demonstrate
use of the framework, we applied it to analyze some low-
interaction honeypot data. The ﬁndings of the case study
include: (i) majority of the attack processes exhibit LRD; (ii)
LRD-aware models can predict the attack rates (especially for
network-level attack processes) even 5 hours ahead of time,
which would give the defender sufﬁcient early-warning time.
The prediction power of the “gray-box” prediction models,
when compared with “black-box” prediction models, rewards
the effort spent for analyzing the advanced statistical properties
of stochastic cyber attack processes.

The present study introduces a range of interesting problems
for future research. First, we need to further improve the
prediction accuracy, despite that
the LRD-aware FARIMA
model can predict better than the LRD-less ARMA models.
For this purpose, we plan to study some advanced models
that can accommodate high volatilities. It is known in the
literature that GARCH model may be able to accommodate
high volatilities, which has some correlation to LRD. This
hints that FARIMA+GARCH models may be able to ﬁt and
predict
the attack rates better. The other possible way to
improve prediction is to incorporate the Extreme Value Theory
into the FARIMA process because the FARIMA process may
not be able to capture the extremely large attack rate (i.e.,
the spikes). Second, although our study only ruled out two
candidate causes, it is important to rigorously explain the
fundamental cause of LRD as exhibited by honeypot-captured
cyber attacks. This is a difﬁcult problem in general. Third,
the victim-level attack processes and network-level attack
processes exhibit similar phenomena (i.e., LRD). This hints a
sort of scale-invariance that, if turns out to hold, would have
extremely important implications (for example) in achieving
scalable analysis of cyber attacks. Fourth, Wagener et al. [52]
recently introduced the concept of adaptive high-interaction
honeypots to interact with the attacker strategically, which is
reminiscent of [53]. It would be interesting to characterize the
statistical properties of such new variants of honeypots. Fifth,
one reviewer points out
the following interesting research
problem: Can we use the information about the prediction
errors to directly adjust the prediction results? In order to
answer this question, we need to study the statistical properties
of the prediction errors.

ACKNOWLEDGEMENT

This study was IRB-approved. We thank the anonymous
reviewers for their comments that helped us improve the paper.
This work was supported in part by ARO Grant #W911NF-13-
1-0141 and AFOSR Grant #FA9550-09-1-0165. Any opinions,
ﬁndings, and conclusions or recommendations expressed in
this material are those of the author(s) and do not necessarily
reﬂect the views of the funding agencies.

13

REFERENCES

[1] G. Samorodnitsky, “Long range dependence,” Foundations and Trends

in Stochastic Systems, vol. 1, no. 3, pp. 163–257, 2006.

[2] W. Willinger, M. Taqqu, W. Leland, and V. Wilson, “Self-similarity
in high-speed packet trafﬁc: analysis and modeling of ethernet trafﬁc
measurements,” Statistical Sci., vol. 10, pp. 67–85, 1995.

[3] J. Beran, Statistics for Long-Memory Processes. Chapman and Hall,

1994.

[4] T. Mikosch and C. Starica, “Nonstationarities in ﬁnancial time series,
the long-range dependence, and the igarch effects,” The Review of
Economics and Statistics, vol. 86, no. 1, pp. 378–390, February 2004.
[5] Z. Qu, “A test against spurious long memory,” Boston University - De-
partment of Economics, Boston University - Department of Economics
- Working Papers Series WP2010-051, 2010.

[6] X. Shao, “A simple test of changes in mean in the possible presence of
long-range dependence,” Journal of Time Series Analysis, vol. 32, no. 6,
pp. 598–606, November 2011.

[7] J. Cryer and K. Chan, Time Series Analysis With Applications in R. New

York: Springer, 2008.

[8] P. Abry and D. Veitch, “Wavelet analysis of long-range-dependent
trafﬁc,” IEEE Transactions on Information Theory, vol. 44, no. 1, pp.
2–15, 1998.

[9] D. Daley and D. Vere-Jones, An Introduction to the Theory of Point

Processes, Volume 1 (2nd ed.). Springer, 2002.

[10] F. Dressler, W. Jaegers, and R. German, “Flow-based worm detection
using correlated honeypot logs,” Proc. 2007 ITG-GI Conference Com-
munication in Distributed Systems (KiVS), pp. 1–6, 2007.

[11] O. Thonnard, J. Viinikka, C. Leita, and M. Dacier, “Automating the
analysis of honeypot data (extended abstract),” in Proc. Recent Advances
in Intrusion Detection (RAID’08), 2008, pp. 406–407.

[12] A. Sperotto, G. Schaffrath, R. Sadre, C. Morariu, A. Pras, and B. Stiller,
“An overview of ip ﬂow-based intrusion detection,” IEEE Communica-
tions Surveys & Tutorials, vol. 12, no. 3, pp. 343–356, 2010.

[13] S. Almotairi, A. Clark, G. Mohay, and J. Zimmermann, “Characteriza-
tion of attackers’ activities in honeypot trafﬁc using principal component
analysis,” in Proc. IFIP International Conference on Network and
Parallel Computing, 2008, pp. 147–154.

[14] P. Embrechts, C. Kluppelberg, and T. Mikosch, Modelling Extremal

Events for Insurance and Finance. Springer, Berlin, 1997.

[15] B. Peter and D. Richard, Introduction to Time Series and Forecasting.

Springer, 2002.

[16] http://dionaea.carnivore.it/.
[17] https://alliance.mwcollect.org/.
[18] http://amunhoney.sourceforge.net/.
[19] P. Baecher, M. Koetter, M. Dornseif, and F. Freiling, “The nepenthes
platform: An efﬁcient approach to collect malware,” in Proc. Symposium
on Recent Advances in Intrusion Detection (RAID’06), 2006, pp. 165–
184.

[20] G. Shorak and J. Wellner, Empirical Processes with Applications to

Statistics. Springer, 1986.

[21] R. D’Agostino and M. Stephens, Tests Based on EDF Statistics.

Springer, 1986.

[22] M. Chandra, N. Singpurwalla, and M. Stephens, “Kolmogorov statistics
for tests of ﬁt for the extreme value and weibull distributions,” J. Amer.
Statist. Assoc., vol. 74, pp. 729–735, 1981.

[23] V. Choulakian and M. Stephens, “Goodness-of-ﬁt tests for the general-

ized pareto distribution,” Technometrics, vol. 43, pp. 478–484, 2001.

[24] W. Leland, M. Taqqu, W. Willinger, and D. Wilson, “On the self-similar
nature of ethernet trafﬁc (extended version),” IEEE/ACM Trans. Netw.,
vol. 2, no. 1, pp. 1–15, 1994.

[25] W. Leland and D. Wilson, “High time-resolution measurement and anal-
ysis of lan trafﬁc: Implications for lan interconnection,” in INFOCOM,
1991, pp. 1360–1366.

[26] W. Willinger, M. Taqqu, R. Sherman, and D. Wilson, “Self-similarity
through high-variability: statistical analysis of ethernet lan trafﬁc at the
source level,” IEEE/ACM Trans. Netw., vol. 5, no. 1, pp. 71–86, 1997.
[27] S. Resnick, Heavy-Tail Phenomena: Probabilistic and Statistical Mod-

eling. Springer, 2007.

[28] V. Nicomette, M. Kaˆaniche, E. Alata, and M. Herrb, “Set-up and
deployment of a high-interaction honeypot: experiment and lessons
learned,” Journal in Computer Virology, vol. 7, no. 2, pp. 143–157,
2011.

[29] A. Herrero, U. Zurutuza, and E. Corchado, “A neural-visualization ids

for honeynet data,” Int. J. Neural Syst., vol. 22, no. 2, 2012.

[30] Z. Li, A. Goyal, Y. Chen, and V. Paxson, “Towards situational awareness
of large-scale botnet probing events,” Information Forensics and Secu-
rity, IEEE Transactions on, vol. 6, no. 1, pp. 175–188, march 2011.

[31] S. Almotairi, A. Clark, G. Mohay, and J. Zimmermann, “A technique
for detecting new attacks in low-interaction honeypot trafﬁc,” in Proc.
International Conference on Internet Monitoring and Protection, 2009,
pp. 7–13.

[32] A. Clark, M. Dacier, G. Mohay, F. Pouget, and J. Zimmermann, “Internet
attack knowledge discovery via clusters and cliques of attack traces,”
Journal of Information Assurance and Security, vol. 1, no. 1, pp. 21–
32, 2006.

[33] S. Almotairi, A. Clark, M. Dacier, C. Leita, G. Mohay, V. Pham,
O. Thonnard, and J. Zimmermann, “Extracting inter-arrival time based
behaviour from honeypot trafﬁc using cliques,” in 5th Australian Digital
Forensics Conference, 2007, pp. 79–87.

[34] G. Conti and K. Abdullah, “Passive visual ﬁngerprinting of network
attack tools,” in Proc. 2004 ACM workshop on Visualization and data
mining for computer security, 2004, pp. 45–54.

[35] E. Alata, M. Dacier, Y. Deswarte, M. Kaaˆaniche, K. Kortchinsky,
V. Nicomette, V. Pham, and F. Pouget, “Collection and analysis of attack
data based on honeypots deployed on the internet,” in Proc. Quality of
Protection - Security Measurements and Metrics, 2006, pp. 79–91.
[36] M. Kaˆaniche, Y. Deswarte, E. Alata, M. Dacier, and V. Nicomette,
“Empirical analysis and statistical modeling of attack processes based
on honeypots,” CoRR, vol. abs/0704.0861, 2007.

[37] Y. Gao, Z. Li, and Y. Chen, “A dos resilient ﬂow-level intrusion de-
tection approach for high-speed networks,” in Proc. IEEE International
Conference on Distributed Computing Systems (ICDCS’06), 2006, pp.
39–.

[38] D. Dagon, X. Qin, G. Gu, W. Lee, J. Grizzard, J. Levine, and H. Owen,
“Honeystat: Local worm detection using honeypots,” in Proc. Recent
Advances in Intrusion Detection (RAID’04), 2004, pp. 39–58.

[39] W. Strayer, D. Lapsley, R. Walsh, and C. Livadas, “Botnet detection
based on network behavior,” in Botnet Detection, ser. Advances in
Information Security. Springer, 2008, vol. 36, pp. 1–24.

[40] C. Livadas, R. Walsh, D. Lapsley, and W. Strayer, “Using machine
trafﬁc,” in Proc. IEEE LCN

learning techniques to identify botnet
Workshop on Network Security (WoNS’2006), 2006, pp. 967–974.
[41] V. Pham and M. Dacier, “Honeypot trace forensics: The observation
viewpoint matters,” Future Generation Comp. Syst., vol. 27, no. 5, pp.
539–546, 2011.

[42] I. Polakis, T. Petsas, E. Markatos, and S. Antonatos, “A systematic
characterization of im threats using honeypots,” in NDSS, 2010.
[43] C. Kreibich and J. Crowcroft, “Honeycomb: creating intrusion detec-
tion signatures using honeypots,” SIGCOMM Comput. Commun. Rev.,
vol. 34, no. 1, pp. 51–56, 2004.

[44] G. Portokalidis and H. Bos, “Sweetbait: Zero-hour worm detection and
containment using low- and high-interaction honeypots,” Comput. Netw.,
vol. 51, no. 5, 2007.

[45] K. Anagnostakis, S. Sidiroglou, P. Akritidis, K. Xinidis, E. Markatos,
and A. Keromytis, “Detecting targeted attacks using shadow honeypots,”
in Proc. USENIX Security Symposium, 2005.

[46] D. Nash and D. Ragsdale, “Simulation of self-similarity in network
utilization patterns as a precursor to automated testing of intrusion
detection systems,” Trans. Sys. Man Cyber. Part A, vol. 31, no. 4, pp.
327–331, 2001.

[47] B. AsSadhan, H. Kim, and J. Moura, “Long-range dependence analysis
of control and data planes network trafﬁc,” in Proc. Saudi International
Innovation Conference (SIIC’08), 2008.

[48] F. Li and M. Hsieh, “An empirical study of clustering behavior of
spammers and groupbased anti-spam strategies,” in Third Conference
on Email and AntiSpam (CEAS’06), 2006, pp. 27–28.

[49] R. Pang, V. Yegneswaran, P. Barford, V. Paxson, and L. Peterson,
“Characteristics of internet background radiation,” in Proc. ACM Internet
Measurement Conference (IMC’04), 2004, pp. 27–40.

[50] E. Wustrow, M. Karir, M. Bailey, F. Jahanian, and G. Huston, “Internet
background radiation revisited,” in Proc. ACM Internet Measurement
Conference (IMC’10), 2010, pp. 62–74.

[51] E. Glatz and X. Dimitropoulos, “Classifying internet one-way trafﬁc,”

in Internet Measurement Conference, 2012, pp. 37–50.

[52] G. Wagener, R. State, T. Engel, and A. Dulaunoy, “Adaptive and self-
conﬁgurable honeypots,” in Proc. IFIP/IEEE International Symposium
on Integrated Network Management, 2011, pp. 345–352.

[53] W. Cheswick, “An evening with berferd, in which a cracker is lured,
endured, and studied,” in Proc. Winter USENIX Conference, 1992.

14

[54] M. Taqqu, V. Teverovsky, and W. Willinger, “Estimators for long range
dependence: An empirical study,” Fractals, vol. 3, no. 4, pp. 785–798,
1995.

[55] W. Rea, M. Reale, and J. Brown, “Estimators for long range dependence:

An empirical study,” arXiv: 0901.0762v1, 2009.

APPENDIX A
REVIEW OF SOME STATISTICAL TECHNIQUES

A. Methods for Estimating Hurst Parameters

We used six popular methods (cf. [3] for details) for esti-
mating the Hurst parameter, which is a well-accepted practice
[54], [55].
1) RS method: For a time series {Xt, t ≥ 1}, with partial
sum Yt = (cid:80)t
i −
(cid:0) 1
Y 2
t , the R/S statistic is deﬁned as
t
(cid:18)

i=1 Xi and sample variance S2

i=1 X 2

t = 1
t

(cid:80)t

(cid:19)(cid:21)

(cid:1)2

(cid:19)

(cid:18)

(cid:20)

max
0≤t≤n

Yt −

Yn

− min
0≤t≤n

Yt −

Yn

.

R
S

(n) =

1
Sn

t
n

t
n

For LRD series, we have
(cid:21)

(cid:20) R
S

E

(n)

∼ CH nH , n → ∞

where CH is a positive, ﬁnite constant independent of n.
2) AGV (aggregated variance) method: Divide time series
{Xt, t ≥ 1} into blocks of size m. The block average is

X (m)(k) =

1
m

km
(cid:88)

i=(k−1)m+1

Xi,

k = 1, 2 . . . .

Take the sample variance of X (m)(k) within each block, which
is an estimator of Var(X (m)). For LRD series, we have β =
2H − 2 and

(cid:16)

X (m)(cid:17)

Var

∼ cm−β, m → ∞,

where c is a ﬁnite positive constant independent of m.
3) Peng method: The series is broken up into blocks of size
m. Compute partial sums Y (i), i = 1, 2 . . . , m within blocks.
Fit a least-square line to the Y (i)’s and compute the sample
variance of the residuals. This procedure is repeated for each
of the blocks, and the resulting sample variances are averaged.
The resulting number is proportional to m2H for LRD series.
4) Per (Periodogram) method: One ﬁrst calculates

I(λ) =

1
2πN

(cid:12)
(cid:12)
N
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j=1

Xjeijλ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

where λ is the frequency, N is the number of terms in the
series, and Xj is the data. A LRD series should have a
periodogram proportional to λ1−2H for λ ≈ 0. A regression
of the logarithm of the periodogram on the logarithm of the
frequency gives coefﬁcient 1 − 2H.
5) Box (Boxed Periodogram) method: This method was de-
veloped to deal with the problem that most points, which are
used to estimate H, reside on the right-hand side of the graph.
6) Wave (Wavelet) method: Wavelets can be thought of as akin
to Fourier series but using waveforms other than sine waves.
The estimator used here ﬁts a straight line to a frequency
spectrum derived using wavelets [8].

15

Zhenxin Zhan is a PhD candidate in
the Department of Computer Science,
University of Texas at San Antonio.
He received M.S. degree in Computer
Science from the Huazhong University
of Science and Technology, China, in
2008. His primary research interests are

in cyber attack analysis and detection.

Maochao Xu received his PH.D. in
Statistics from Portland State University
in 2010. He is an Assistant Professor of
Mathematics at the Illinois State Uni-
versity. His research interests include
Applied Statistics, Extreme value the-
ory, Cyber security, and Risk analysis
in actuary and insurance. He currently serves as an associate
editor for Communications in Statistics.

Shouhuai Xu is an Associate Pro-
fessor in the Department of Computer
Science, University of Texas at San
Antonio. His research interests include
cryptography and cybersecurity model-
ing & analysis. He earned his PhD in
Computer Science from Fudan Univer-
sity, China. More information about his research can be found
at www.cs.utsa.edu/∼shxu.

B. Heavy-tail Distributions

A random variable X is said to belong to the Maximum
Domain of Attraction (MDA) of the extreme value distribution
Hξ if there exists constants cn ∈ R+, dn ∈ R such that its
distribution function F that satisﬁes

lim
n→∞

F n(cnx + dn) = Hξ(x).

In statistics, X is said to follow a heavy-tailed distribution
if F ∈ MDA(Hξ). There are many methods for estimating
parameter α [14], [27]. A widely-used method is called Point
Over Threshold (POT). Let X1, . . . , Xn be independent and
identically distributed random variables from F ∈ MDA(Hξ),
then we may choose a high threshold u such that

lim
u→xF

sup
0<x<xF −u

| ¯Fu(x) − ¯Gξ,β(µ)(x)| = 0,

where xF is the right end poind point of X, and

Fu(x) = P (X − u ≤ x|X > u),

x ≥ 0,

and ¯Gξ,β(µ) = 1 − Gξ,β(µ)
generalized Pareto distribution (GPD)

is the survival function of

¯Gξ,β(µ)(x) =






(cid:18)

(cid:19)−1/ξ

1 + ξ

x
β
exp{−x/β},

,

ξ (cid:54)= 0

ξ = 0

where x ∈ R+ if ξ ∈ R+, and x ∈ [0, −β/ξ] if ξ ∈ R−. The
POT method states that if X1, . . . , Xn are heavy-tailed data,
then [Xi −u|Xi > u] follows a generalized Pareto distribution.

C. Goodness-of-ﬁt Test Statistics

We use

three popular goodness-of-ﬁt

statistics:
Kolmogorov-Smirnov (KS), Cram´er-von Mises (CM), and
Anderson-Darling (AD). Let X1, . . . , Xn be independent and
identical random variables with distribution F . The empirical
distribution Fn is deﬁned as

test

Fn(x) =

1
n

n
(cid:88)

i=1

I(Xi ≤ x),

where I(Xi ≤ x) is the indicator function:

I(Xi ≤ x) =

(cid:26) 1, Xi ≤ x,
o/w.

0,

The KS test statistic is deﬁned as

√

KS =

n sup
x

|Fn(x) − F (x)| .

The CM test statistic is deﬁned as

(cid:90)

CM = n

(Fn(x) − F (x))2dF (x).

The AD test statistic is deﬁned as

(cid:90)

AD = n

(Fn(x) − F (x))2w(x)dF (x),

where w(x) = [F (x)(1 − F (x))]−1.

