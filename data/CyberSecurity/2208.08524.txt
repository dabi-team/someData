DF-Captcha: A Deepfake Captcha for Preventing Fake Calls

A draft academic paper based on the provisional patent submitted January 1st 2022 under
provisional Number 63/302,086

2
2
0
2

g
u
A
7
1

]

R
C
.
s
c
[

1
v
4
2
5
8
0
.
8
0
2
2
:
v
i
X
r
a

Yisroel Mirsky
Oﬀensive AI Research Lab
Ben-Gurion University
yisroel@post.bgu.ac.il
https://offensive-ai-lab.github.io/

August 19, 2022

Abstract

Social engineering (SE) is a form of deception that aims to trick people into giving access
to data, information, networks and even money. For decades SE has been a key method for
attackers to gain access to an organization, virtually skipping all lines of defense. For example,
numerous organizations have been inﬁltrated through spear phishing attacks where targeted
emails were written to entice the receiver to click on a malicious link or open an infected
attachment. Attackers also regularly use SE to scam innocent people by making threatening
phone calls which impersonate an authority or by sending infected emails which look like they
have been sent from a loved one. SE attacks will likely remain a top attack vector for criminals
because humans are the weakest link in cyber security.

Unfortunately, the threat will only get worse now that a new technology called deepfakes as
arrived. A deepfake is believable media (e.g., videos) created by an AI. Although the technology
has mostly been used to swap the faces of celebrities, it can also be used to ‘puppet’ diﬀerent
personas. Recently, researchers have shown how this technology can be deployed in real-time to
clone someone’s voice in a phone call or reenact a face in a video call. Given that any novice
user can download this technology to use it, it is no surprise that criminals have already begun
to monetize it to perpetrate their SE attacks.

In this paper, we propose a lightweight application which can protect organizations and
individuals from deepfake SE attacks. Through a challenge and response approach, we leverage
the technical and theoretical limitations of deepfake technologies to expose the attacker. Existing
solutions are too heavy as an end-point solution and can be evaded by a dynamic attacker. In
contrast, our approach is lightweight and breaks the reactive arms race, putting the attacker at
a disadvantage.

Keywords— Deepfakes, social engineering, phishing, cybersecurity

1

 
 
 
 
 
 
1

Introduction

A deepfake is content, generated by an artiﬁcial intelligence, that is authentic in the eyes of a human being.
The word deepfake is a combination of the words ‘deep learning’ and ‘fake’ and primarily relates to content
generated by an artiﬁcial neural network, a branch of machine learning.

The most common form of deepfakes involve the generation and manipulation of human imagery. This
technology has creative and productive applications. For example, realistic video dubbing of foreign ﬁlms,1
education though the reanimation of historical ﬁgures [1], and virtually trying on clothes while shopping.2
There are also numerous online communities devoted to creating deepfake memes for entertainment,3 such
as music videos portraying the face of actor Nicolas Cage.

However, despite the positive applications of
deepfakes, the technology is infamous for its uneth-
ical and malicious aspects. At the end of 2017, a
Reddit user by the name of ‘deepfakes’ was using
deep learning to swap faces of celebrities into porno-
graphic videos, and was posting them online4. The
discovery caused a media frenzy and a large num-
ber of new deepfake videos began to emerge there-
after. In 2018, BuzzFeed released a deepfake video
of former president Barak Obama giving a talk on
the subject. The video was made using the Red-
dit user’s software (FakeApp), and raised concerns
over identity theft, impersonation, and the spread
of misinformation on social media.

Overall, there are a number of diﬀerent uses for
deepfakes. However, many of them are criminal and
unethical. Fig. presents an information trust chart
which summarizes how deepfakes can be used to mis-
lead people. In this paper, we are concerned with
hoaxes (the upper left quadrant).

1.1 The Threat

Figure 1: A deepfake information trust chart.

Criminal activities usually motivated with monetary gain. Therefore, in the coming years we can expect to
see deepfakes being weaponized for monetization. This even more likely considering that the technology has
already been proven as an eﬀective attack tool for humiliation, misinformation, and defamation. Moreover,
deepfake technologies are becoming more practical [2], eﬃcient [3], and easily accessible.5

The most likely scenario is that deepfakes will be used in social engineering (SE) attacks. An SE attack
is where an attacker uses psychological manipulation to trick users into making security mistakes or giving
away sensitive information. For example, enticing a user to open an email attachment or transfer money in
an phone scam.

SE is so eﬀective that 98% of all cyber attacks rely on it [4]. This is because humans are the weakest
link in cyber security. We are inherently gullible when enticed, threatened, or lured into a false pretext in
a social encounter. Cyber criminals know this and exploit these vulnerabilities to achieve their goals. As a
result SE attacks aﬀect everybody: the public sector, private sector, and the individual at home.

A major concern is, what happens when criminals start using deepfakes to exploit our trust? Imagine
you receive a call from your mother who is in trouble and urgently needs some money transferred. The

1https://variety.com/2019/biz/news/

ai-dubbing-david-beckham-multilingual-1203309213/

2https://www.forbes.com/sites/forbestechcouncil/2019/05/21/gans-and-deepfakes-could-revolutionize-the-

fashion-industry/

3https://www.reddit.com/r/SFWdeepfakes/
4https://www.vice.com/en us/article/gydydm/gal-gadot-fake-ai-porn
5https://github.com/aerophile/awesome-deepfakes

2

III. EntertainmentI. HoaxIV. TrustedII. PropagandaIntention to misleadTruthA Trusting News Ecosystem against Fake News from Humanity and Technology Perspectives Scams & Fraud:Trickery via spoofing, falsifying audit records, generating artwork, …Tampering of Evidence:Medical, forensic, court, …Harming Credibility:Revenge porn, political sabotage via generated videos or articles, …Inspired by Source: fb https://newsroom.fb.com/news/2018/05/inside-feed-facing-facts/#watchnowDeepfakeInformation Trust ChartAltering Published Movies:Comedy, satire, …Editing & Special Effects:Generating actors in movies, …Art & Demonstration:Animating dead characters, generated portraits, technology demos,  …Authentic Content:Credible Multimedia / Data MisdirectionGenerated discourse to amplify events / facts, …Political Warfare:Tone change of articles, content loosely based on facts, conspiracy…Corruption:Increased xenophobia, …Figure 2: An illustrative example of how a deepfake social engineering attack can impact critical
infrastructure.

caller sounds exactly like her, but the situation seems a bit out of place. Under stress and frustration, she
hands the phone over to your father who conﬁrms the situation. Without hesitation, many would transfer
the money –even though they talking to a stranger.

Now consider state-actors with considerable amounts of time and resources. They could target workers
at power plants and other critical infrastructure by posing as their superiors. Over a phone call they could
convince the worker to change a conﬁguration or setting which would lead to a cyber breach or a catastrophic
failure (Fig. 2). The attackers could even target soldiers or oﬃcers on duty leading to the compromise of
our national security.

These kinds of SE attacks would require the rendering of deepfakes in real-time. Although most methods
cannot be rendered in real-time, researchers have recently shown that this is indeed possible. For example,
in [3] the authors show how one’s voice can be cloned with only ﬁve seconds of audio.6 Furthermore, the
authors in [5] propose a ﬁrst order motion model which can be used to reenact (puppet) any still image in
real-time (Fig. 3).

Deepfake SE attacks are tangible threats.

In 2019, a company was scammed out of $250K when an
associate was tricked over the phone to transfer money. The call was later conﬁrmed to be deepfake of the
CEO’s voice [6]. After that, a widow was scammed out of nearly 300k$ when conned in a false romance by a
real-time deepfake video [7]. In 2020, cybercriminals stole $35 million in a bank heist where a bank manager
received a deepfake phone call in the voice of a company director [8]. In 2021 European MPs participated
in Zoom meetings with a real-time deepfake of a Russian opposition leader [9]. Even more recently, in 2022
the FBI warned the public of cybercriminals which are using real-time deepfakes to perform interviews and
gain access to organization in elaborate social engineering attacks [10]. At the rate which these attacks are
progressing, it is only a matter of time before they become common place.

1.2 The Proposed Solution

In this paper, we propose the DF-Captcha: a challenge-response turing test against real-time deepfake
technologies. The concept is that when a victim receives a suspicious audio or video call, the victim can
send the caller a challenge which can be automatically veriﬁed. What makes this technique powerful is that
the challenge is easy for a human to perform but extremely hard for a deepfake model to generate. For
example, to expose facial reenactment, the challenge might be to have the caller move his/her head to an
oblique angle, press on the nose, or simply turn around.

6For this paper, we have prepared an audio demo of this attack where 3 seconds of voice is used to clone a CEO’s

voice and use it to make a call: https://tinyurl.com/nzsszh54

3

The nature of these challenges is that they target limitations in deep learning technology. As a result,
models which attempt them either do not perform the action or generate large artifacts which can easily be
detected.

The limitations stem from two main challenges for the attacker:

1. It is hard for the attacker to procure a dataset which captures many diﬀerent non-related activities,
situations, and phyisical simulations all while perfecting the related task (e.g., facial reenactment).

2. Current deep learning technology does not multitask well (it cannot generate hyper realistic content
all while managing temporal physics, material simulations, anatomy, etc.) Although it is foreseeable
that the limitations may diminish as deep learning matures, the victim can easily add more challenges
requiring the attacker to train every model to be an expert at every single one. This puts the defender
one step ahead of the attacker.

DF-Captcha has several advantages over existing defenses (described in detail in section 6):

Robust. It does not rely on a single artifact that can reﬁned (past works). Moreover, it ‘forces’ the attacker

to exhibit large artifacts, and the artifacts appear regardless of the deep fake’s quality.

Resilient. This defense is hard for attacker to evade. The attacker cannot cover all concepts (tasks) with

high ﬁdelity –concepts, loss functions, etc. It is also easy for defender to introduce new challenges.

Eﬃcient. Only the relevant content needs to be analyzed (not every frame in the video/audio)

Flexible. The responses can be automatically or manually veriﬁed.

2 Overview of Proposed Method

Mitigating Real-time Deepfakes. To keep ahead of the game, we must be proactive and consider the
adversary’s next step.
If we just consider the weaknesses of the current attacks then the adversary can
evade detection and will always have the upper hand. Instead, we suggest that an eﬀective defense targets
the adversary’s limitations. With this strategy in mind, we propose a simple yet eﬀective defense where
suspicious callers must complete a ‘deepfake Turing test’ to be veriﬁed as a real persona. The test builds
on the ﬁrst research objective since the challenges posed to the deepfake are explicitly designed to stress the
deepfake’s limitations. As a result large artifacts will be created which can be easily detected by the victim
or automatically via light anomaly detection models. The core research objectives are to design a extensible
set of challenges which are both non-intrusive to a legitimate caller and highly eﬀective in exposing the
real-time deepfake.

The key diﬀerence between our approach and past works is that none of the current existing works utilize
or exploit the limitations of deepfakes their advantage. Doing so not only strengthens the position of the
defender in detecting the attack, but also provides some guarantees on doing so as well. For example, today
there does not exist any deepfake technology which can render the head at oblique poses or handle voice
conversion with a wide range of accents or inﬂection not found in the training set of the target. Although
this seems trivial, and we know that deepfakes will continue to improve, we also know that there will always

Figure 3: Still frames taken from a real-time deepfake reenactment video of a CEO. The right of
each image is the driver (attacker) and the left is the resulting frame. The deepfake was generated
using [5] with a single image of the CEO.

4

Figure 4: The process in which the proposed defense detects and protects victims from real-time
deepfake SE attacks.

be limitations which we can exploit. By forcing the attacker to reveal these limitations, strong artifacts will
be presented which we will be able to easily detect with light weight models (e..g, light networks, classical
machine learning such as SVMs, or statistics). As such, part of this work will be to (1) enumerate the
limitations of deepfakes today and how that can be compiled into non-intrusive challenges and (2) identify
the limitations of deepfakes of the future based on concrete theoretical bounds of GANs because of their
dependence on their training sets (i.e., they cannot generate content or behaviors not found in their data).
Furthermore, our proposed approach will alleviate the concern of a victim ignoring the warning of a detector
(e.g., as a false positive). This is because we will force the attacker to reveal defects which are obvious to
the victim.

The signiﬁcance of the research is that it will prepare us to handle the threat of deepfakes before they
become mainstream. Defense researchers will also beneﬁt from our work by guiding their eﬀorts according
to our ﬁndings. Moreover, society will beneﬁt from a free, extensible, open-source application which can be
used to prevent these attacks on the public sector, private sector, and individuals of all ages. To the best of
our knowledge, the proposed tool is the ﬁrst method designed to detect real-time deepfakes, and the concept
of using a deepfake Turing test is also novel.

This method can prepare society for a new and advanced social engineering attack which is emerging.
Although attacks and technologies evolve overtime, this method is modular enabling the defender consider
new limitations by adding new tests. Therefore the system has longevity since it can be easily extendable.

3 Description of the Method

The proposed defense is similar to a Turing test. A Turing test is a test of a machine’s ability to exhibit
intelligent behavior which is indistinguishable from that of a human. These test have been used the past for
cyber security. For example, to prevent myriads of bots from attacking Internet services, CAPTCHAs are
used where the user (human or bot) must solve a visual in order to prove that the user is human.

In this research we propose a ‘deepfake Turing test’ where a user is challenged to produce audio or video
content, and the requested content (i.e., the challenge) is a known limitation of generative AI. Following this
approach, a call screening application can be designed to help users identify deepfake calls. This defense
works as follows (illustrated in Fig. 4):

1. The defense is triggered if a video or audio call is suspicious. This is determined by one or more
indicators conﬁgured by the user: (1) the caller is new and hasn’t been veriﬁed yet, (2) the caller
has indicative network history such as malicious past behaviors, (3) the caller is masking his/her true

5

ChallengesDetectorSuspicious?ChallengeResponsePASSFAILEvidence12345identity (e.g., the call originates from the Tor network), (4) the liveness [11] of the caller is very low or
other obvious artifacts, (5) the user (victim) has requested a test mid-call due to suspicious behaviors
or to ensure authenticity before carrying out a caller’s requests.

2. A challenge is sent to the caller using a prerecorded message. The challenge is automatically selected
based on the type of call (audio or video), the call quality, the level of suspicion, and the caller’s
perceived status (e.g., indoors, outdoors, sitting, etc.)

3. The caller performs the requested challenge and the content is captured.

4. The detector (1) identiﬁes and extracts the response content (i.e., the few frames or a second of audio
containing the performance of the challenge) and (2) applies a light weight anomaly detector to the
response content.

5. The user is notiﬁed of the result (pass or fail) and is given (1) a degree of certainty measured by the
model’s conﬁdence, and (2) the evidence in the form of the the capture challenge content. The user
can then decide whether to proceed with the call, examine the evidence closer him/herself, or request
another challenge.

The Challenges. We have identiﬁed three categories of challenges which stress the limits of a model’s
training set and technology: physical, out-of-distribution, and audio. The following is an initial set example
challenges which we have identiﬁed. These challenges produce large artifacts in real-time deepfakes and are
easy for a human to perform.

A technology challenge (or simulation challenge) aims to push the limits of a deepfake’s capability. They
involve activities where the action cannot be done with existing deepfake technology or cannot be done well
by a model which has been optimized for reenactment (e.g., facial puppetry). Examples include: tongue
motions, poke cheek or nose, fold ear, vibrate lips, crease shirt, stroke hair, show an object, remove glasses,
wave hand quickly (motion blur), drop/bounce object, hold objects between ﬁngers, and interact with a
select object in the background.
Examples of technology challenges:

1. Drop object

2. Bounce Object

3. Fold shirt

4. Stroke hair

5. Interact with background scenery

6. Spill water

An out-of-distribution (OOD) challenge aims to exploit the limited training set of the attacker’s model.
If the attacker has limited data on the victim (i.e., less than 60 hours of video from all angles and expressions)
then the model will struggle to extrapolate the spatial content. This also holds true when using zero or few-
shot learning [12, 13, 14, 15] where a generalized model trained on many other people’s data is ﬁne tuned on
the target. For example, stand up, move close to camera, move head to oblique angles, leave screen, perform
occlusions (e.g., hold hand in front of face), open mouth, and perform hand expressions.
Examples of OOD challenges:

1. Pick up requested object

2. Hand expressions

3. Tongue motion

4. Fold ear

5. Face occlusions

6. Remove glasses

6

An audio challenge is designed to perform a technological or OOD test on the voice of the caller. For
example, sing a few notes, repeat phrase with the given rhythm, use a diﬀerent accent, talk with a given
tone or speed, speak very close to microphone, whisper, and clear throat.
Examples of technology challenges:

1. Mimic phrase

2. hum tune

3. Sing part of song

4. Repeat accent

5. Change tone or speed

6. Clear throat

7. Whistle

The tool can be extended to include more challenges as time goes on.

The Detector. The objective of the detector is to locate and evaluate the response of the caller. To make
the detector eﬃcient and practical, we perform this process in two steps. First the response is extracted
from the captured content in the form of a short audio clip or sequence of frames R. The coarse location of
the response can be located by searching for the anticipated activity with a light weight machine learning
tool. For example, to ﬁnd when the head is turned to an oblique angle we can predict the caller’s pose using
open source software like [16]. Next, R is passed through an anomaly detector trained on clean content of
other people performing the same challenge. Finally, the model predicts whether R is legitimate or not and
then passes R with its conﬁdence score to the user.

We are not concerned that an attacker may use adversarial machine learning [17] to trick our models.
This is because (1) the media channels are lossy due to compression which is an eﬀective mitigation against
these techniques, and (2) the work the attacker could do cause the detector to retrieve the wrong content as
R which will result in an anomaly anyways.

For the anomaly detection model we will experiment with a variety of algorithms and architectures to
ﬁnd the best trade-oﬀ between performance and eﬃciency (speed and resources). Initially, we will utilize
our experience in lightweight neural networks for anomaly detection to perform the task [18, 19]. We will
also try using once-class SVMs and statistical models as a baseline. We expect that these light models will
perform very well given that we know the exact location of the response and that the signals from artifacts
will be very strong. Lastly, the set of challenges are extensible making it easy to add new ones as novel
deepfake technologies are released and new limitations are exposed.

4 Preliminary Results

We implemented the real-time deepfake presented in [5] and reenacted a target photo. We achieved a
realistic live video deepfake at 35 fps. Next, we performed a sample of the challenges listed in section 3. Fig.
5 presents some screenshots of these challenges. The preliminary results show that our intuition is correct:
deepfake attacks are limited and can be easily detected when forced to perform a deepfake Turing test.

5 Strengths of the Method

In this research project, we aim to address the following research questions:

• This defense will eﬀectively mitigate the threat since it is much easier for the defender to develop and

add new challenges then it is for the attacker to develop better deepfake generative technology.

• Since a realistic deepfake must focus its training eﬀort of facial quality, there exists a signiﬁcant range
of poses, expressions, actions, and simulations which cannot all be captured by the generative model.
Therefore, there is a high likelihood that a suﬃciently sized subset of these challenges will be easy for
a human to perform.

7

Figure 5: Preliminary results demonstrating the weaknesses of real-time deepfakes to various chal-
lenges.

• Detection is practical because it is possible to limit the detection to moments which most likely contain
the response, thus reducing the number of false positives. Moreover, by using anomaly detection, we
can use a single model for all existing and new challenges, making the detector practical. Finally, by
leveraging the weaknesses of the attacker’s deepfake model we can increase the probability of detecting
the artifacts making the approach accurate.

• Attackers are vulnerable because generative AI cannot create realistic content for expressions that do
not appear in the training set (e.g., generate the back of the head, or the victim’s singing voice). This
is because there are a wide variety of expressions and interactions and it is extremely challenging for
an attacker to collect all of these aspect with the victim. Moreover, generalizing with media from
other people will still require the model to extrapolate the missing information leading to blurry or
inconsistent content.

• Even if the attacker has lots of media on the target (and can make a hyper realistic deepfake of
him/her) the defense will work well. This is because some of the challenges are not data dependent,
but rather technology dependent (e.g., they stress the limits of occlusions, temporal coherence, physics,
synthesis of arbitrary objects, etc.)

6 Related Work - Current Countermeasures

Defenses against deepfakes have been a subject research in the scientiﬁc community for some time. A
summary and systematization of the deepfake detection methods can be found in Table 1. The primary goal
of these works have been to identify deepfakes by either ﬁnding artifacts or by using generic classiﬁers.

6.1 Artifact-based Detection

Deepfakes often generate artifacts which may be subtle to humans, but can be easily detected using machine
learning. In 2014, researchers had this hypothesis and monitored physiological signals, such as heart rate,
to detect computer generated faces [20]. Regarding deepfakes, [21] monitored irregular eye blinking patterns
and [22] monitored blood volume patterns (pulse) under the skin.

Inconsistencies are also a revealing factor.

In [23] and [24], the authors noticed that video dubbing
attacks can be detected my correlating the speech to landmarks around the mouth. In [25] it was shown that
similar artifacts appear when predicting the facial landmarks. With large amounts of data on the target,
mannerisms and other behaviors can be monitored for anomalies. For example, in [26] the authors protect
world leaders from a wide variety of deepfake attacks by modeling their recorded stock footage.

Some artifacts appear where the generated content was blended back into the frame. The authors of
[27, 28, 29, 30, 31] use edge detectors, quality measures, and frequency analysis to detect artifacts in the

8

pasted content and borders. In, [32] the authors detect deepfakes by decomposing them into to their sources
while identifying the content’s boundary. Some works identify and visualize the tampered regions by either
predicting masks learned from a ground truth, or by mapping the neural activations to the raw image
[33, 34, 35, 36].

The content of a fake face can be anomalous in context to the rest of the frame. For example, residuals
from face warping processes [37, 38, 39], lighting [40], and varying ﬁdelity [41] indicate the presence of
generated content. In [42] and [43], the authors found that GANs leave unique ﬁngerprints and show how
it is possible to classify the generator given the content, even in the presence of compression and noise. In
[44] the authors analyze a camera’s unique sensor noise (PRNU) to detect pasted content.

Realistic temporal coherence is challenging to generate, and some authors capitalize on the resulting
artifacts to detect the fake content. For example, [45] uses an RNN to detect artifacts such as ﬂickers and
jitter, and [46] uses an LSTM on the face region only. In [47] the optical ﬂow between frames is analyzed,
and in [48] a classiﬁer is trained on the two frames directly.

Most works on voice clone detection fall under this category as well. Unfortunately, very little work
has been done to detect deepfake voices. Previous works have focused on voice synthesized using non-deep
learning approaches [49]. As a result, these methods have trouble identifying the same artifact found made
by deep learning models. This is because deep learning models are able to generate ﬂuid audio and because
the artifacts lie in other feature sets (e.g., behavior).

The ﬁrst detection model for deepfake audio was published in 2019 [49]. There the authors proposed a
bispectral analysis method for detecting AI-synthesized fake voices. They observed that speciﬁc and unusual
spectral correlation exhibited in the fake voices synthesized with DNNs. They explore these bispectral arti-
facts using higher-order polyspectral features for discriminating fake voices. Other works use deep learning
models, such as recurrent neural networks, in a generic way to classify the audio samples [50, 51]. Finally,
in [52] the authors propose DeepSonar which observes the neural activation of a voice classiﬁcation network.
If the activation pattern appears anomalous, then the voice sample is considered fake.

6.2 Undirected Approaches

Instead of focusing on a speciﬁc artifact, some authors train deep neural networks as generic classiﬁers, and
In [58, 59, 60], it was shown that
let the network decide which features to analyze [53, 54, 55, 56, 57].
deep neural networks tend to perform better than traditional image forensic tools on compressed imagery.
Alternatively, to overcome noise and other distortions, the authors of [61] measure the neural activation
(coverage) of a face recognition network to obtain a stronger signal from than just using the raw pixels. In
[62] the authors detect deepfakes by measuring an input’s embedding distance to real samples using an ED’s
latent space. To improve performance of their models, some add noise to their training data [63] and others
try to develop models which generalize better to unseen attacks/generators through disentanglement [64]
and semi-supervised learning [65].

6.3 Challenge Response Approach

In [66] the authors propose rtCaptcha.
In this paper the authors detect fake video calls by assuming
that attackers cannot generate content fast enough. They have the user speak a written captcha and
rely on the assumption that the attacker will not be able decipher it and then pass it through his voice
generator in time. This approach works against attackers who are using still images of the victim, prerecorded
audio/video recordings of the victims, and older deepfake technologies which required oﬄine production.
However deepfake technology has advanced to the level where high quality attacks can be performed in real-
time audio and video are processed per frame with negligible reaction delay –bypassing rtCaptcha completely.
Instead, we aim to identify real-time deepfake attacks through a captcha which exploits the weaknesses a
real-time deep fake technology: making it hard for the attacker and easy for the defender to keep up.

6.4 The Scientiﬁc Gap

The issue with current approaches is that they are not suitable for detecting real-time deepfake attacks. This
is because they (1) cannot be run in real-time, (2) cannot be run on a smartphone (where we expect an SE

9

Table 1: Summary of Deepfake Detection Methods

Type Modality Content

Method

Eval. Dataset

Performance*

t
n
e
m
t
c
a
n
e
e
R

t
n
e
m
e
c
a
l
p
e
R

e
g
a
m

I

o
e
d
V

i

o
i
d
u
A

e
r
u
t
a
e
F

t
r
a
P
y
d
o
B

e
c
a
F

e
g
a
m

I

l
e
d
o
M

a
e
r
A
d
e
t
c
e
ﬀ
A
s
e
t
a
c
i
d
n
I

n
o
i
t
u
l
o
s
e
R
t
u
p
n
I

]
1
4
[

I

I

T
M
T
e
k
a
f
p
e
e
D

Classic ML

• • •
[28] 2017
• • •
[27] 2017
[25] 2018 • • • •
[41] 2018 • •
•
[31] 2019
[29] 2019 • • • •
•
[26] 2019 • •

• •

Deep Learning

•

•

[30] 2018 • • • •
[21] 2018 • •
[60] 2018 • • • •
[62] 2018
• • •
[54] 2018 • • •
[45] 2018 • •
[58] 2018 • • •
[64] 2018 • • • •
•
[53] 2018 • •
•
[46] 2019 • •
[33] 2019 • • • •
[56] 2019
• • •
[34] 2019 • • • •
[35] 2019 • • • •
[38] 2019 • • • •
[39] 2019
•
[57] 2019 • • • •
[36] 2019 • • • •
[63] 2019 • • • •
[65] 2019 • • • •
[55] 2019 • • •
[22] 2019 • •
•
[37] 2019 • • • •
•
[47] 2019 • •
•
[?] 2019 •
•
[24] 2019 •
•
[48] 2019 •
[42] 2019 • • •
[61] 2019 • • • •
[32] 2019 • • • •
[69] 2020 ◦ • •
[70] 2020 • • •
[71] 2020 • • •
[72] 2020 • •
[73] 2020 • • •
[74] 2020 • • •
[75] 2020 • •
[76] 2020 • •
[77] 2020 • •
[78] 2020 • • •
[79] 2020 • • •
[80] 2020 • •
[81] 2020 • • •
[82] 2020 • • •

•

•
•
•

•

•

•
•

•
•

•

•
•
•
•

•
•
•
•

•
•
•
•
•

•
•
•

•

•

•

SVM-RBF
SVM
SVM
SVM
• SVM, Kmeans...
SVM
SVM

250x250
*
*

128x128 •

1024x1024
*
*

•

CNN
LSTM-CNN
Capsule-CNN
ED-GAN
CNN
CNN-LSTM
CNN
CNN AE
CNN
CNN-LSTM
CNN-DE
CNN

256x256
224x224
128x128
128x128
1024x1024
299x299
256x256
256x256
256x256
224x224
• 256x256

-

•
•

•

•

•
•

CNN
CNN
CNN+HMN
FCN
CNN
CNN
CNN
CNN
CNN
CNN
LSTM
LSTM-DNN
CNN
CNN
SVM+VGGnet
CNN

• CNN AE GAN • 256x256
CNN+Attention • 299x299
128x128
*
224x224
256x256
128x128
224x224
1024x1024
128x128
• 224x224 •
224x224
*
*
256x256
128x128
224x224
64x64
64x64
-
299x299
224x224
224x224
128x128
*
128x128
256x256
64x64
224x224
112x112
100x100
224x224

• CNN ResNet
AREN-CNN
ED-CNN
CNN
LSTM
Siamese CNN
Ensemble
*
OC-VAE
ABC-ResNet

•
• • HRNet-FCN •
• •
• •
•

PP-CNN
ED-CNN
ED-LSTM

• •
•
•
•
•
•

•

• •

•
•

• •
• •

•

•

•
•

•
•

•

•

]
5
3
[

D
F
F
D

]
7
6
[

s
c
i
s
n
e
r
o
F
e
c
a
F

]
9
5
[

+
+
s
c
i
s
n
e
r
o
F
e
c
a
F

]
8
6
[

W
F
F

]
9
3
[

F
D
-
b
e
l
e
C

•

•

•
• •

•

•

B
D
e
k
a
f
p
e
e
D
r
e
h
t
O

•
•

•

m
o
t
s
u
C

•

•

•

•
•

•

•
•
•
•

•
•

• •

•

•

•

•
•

•
•
•
•

• • •
•

•

•

•

•

C
C
A

R
E
E

C
U
A

92.9

100

99.4

99.3
92

97.1
94.4
90.5

96.9
92.8
98.5
99.2

99.4
98.1
94.7
86.4

96

81.6

97
99.6
85

0.97

0.98

0.99

0.81

0.99

0.99
0.99
0.64

94

18.2

3.33

13.33

8.18

3.11

93.2

22
16.4

0.53

•
•

•

•

• •

•
•
•

•
•
•

•

•

•

• •
•
•
• •
•

•
•

•

99.2
20.86 0.86
0.92
0.99

Prec.= 0.93

0.92

1.00
99.73

?

• Avrg.
98.52
•

89.6
94.29
• TPR=0.91
99.65
98.26
TPR=0.89

•

Statistics & Steganalysis

[44] 2018
[40] 2019
[43] 2019 • • •

•
•

•
•

•

•
•

PRNU
Statistics
PRNU

•

1280x720
-
*

• TPR=1 FPR= 0.03
•
•

90.3

*Only the best reported performance, averaged over the test datasets, is displayed to capture the ‘best-case’ scenario.

attack to be received), (3) assume a clean lossless channel [51, 50, 52] which is not the case for phone calls or
VoIP call (whatsapp etc) where compression rates can vary based on connectivity, and (4) do not handle the
human side of the issue, where a victim may ignore the warnings due to the urgency of the attacker under
the false pretex. However, the most fundamental issue is that all of these methods perpetuate an endless
arms race. This is because for every artifact or detector, the adversary can tune his/her model to evade
detection, putting the defender at a disadvantage.

Evading Artifact-based Detectors. To evade an artifact-based detector, the adversary only needs to
mitigate a single ﬂaw to evade detection. For example, the attacker’s model can generate the biological
signals monitored by [21, 22] by adding a discriminator which monitors these signals. To avoid anomalies
in extensive the neuron activation [61], the adversary can add a loss which minimizes neuron coverage.
Methods which detect abnormal poses and mannerisms [26] can be evaded by reenacting the entire head
and by learning the mannerisms from the same databases. Models which identify blurred content [30] are
aﬀected by noise and sharpening GANs [83, 84], and models which search for the boundary where the face
was blended in [32, 27, 28, 29, 30, 31] do not work on deepfakes passed through reﬁner networks, which use
in-painting, or those which output full frames (e.g., [85, 86, 87, 88] and many more). Finally, solutions which

10

search for forensic evidence [42, 43, 44] can be evaded (or at least raise the false alarm rate) by passing the
generated content through ﬁlters, or by performing physical replication or compression.

Evading Deep Learning Classiﬁers. There are a number of detection methods which apply deep learning
directly to the task of deepfake detection (e.g., [53, 54, 55, 56, 57]). However, an adversary can use adversarial
machine learning to evade detection by adding small perturbations to the generated image. Advances in
adversarial machine learning has shown that these attacks transfer across multiple models regardless of the
training data used [89]. Recent works have shown how these attacks not only work on deepfakes classiﬁers
[90] but also work with no knowledge of the classiﬁer or it’s training set [91].

In summary, existing approaches either cannot be applied to real-time deepfakes or simply do not address

a dynamic adversary which will evade their defenses.

7 Conclusion

Real-time deepfake attacks are starting to be used in social engineering attacks against companies and
individuals. As time goes on, cyber criminals will increase their usage of this technology as a reliable means
to manipulate people. Current detectors can either be evaded or cannot be used against real-time deepfakes.
To counter this threat, we poposed DF-Captcha: an eﬃcient and practical defense tool which can be used
to identify deepfake callers through a challenge-response mechanism which we call a deepfake Turing test.
By forcing deepfake generators to push their limitations, we can identify attacks with a much higher rate of
success and with using fewer resources. More importantly, we put the defenders at an advantage since it is
easier to craft challenges then it is to develop deepfake technologies.

Acknowledgments

This work was supported by the U.S.-Israel Energy Center managed by the Israel-U.S. Binational Industrial
Research and Development (BIRD) Foundation and the Zuckerman STEM Leadership Program.

8 Bibliography

References

[1] Dami Lee. Deepfake salvador dal´ı takes selﬁes with museum visitors - the verge. https://bit.ly/

3cEim4m, 5 2019.

[2] deepfakes/faceswap: Deepfakes software for all. https://github.com/deepfakes/faceswap, 2017.

(Accessed on 01/27/2020).

[3] Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Patrick Nguyen, Ruoming Pang,
Ignacio Lopez Moreno, Yonghui Wu, et al. Transfer learning from speaker veriﬁcation to multispeaker
text-to-speech synthesis. In Advances in neural information processing systems, pages 4480–4490, 2018.

[4] PurpleSec. 2020 cyber security statistics: The ultimate list of stats, data & trends — purplesec.
https://purplesec.us/resources/cyber-security-statistics/. (Accessed on 02/10/2021).

[5] Aliaksandr Siarohin, St´ephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order
motion model for image animation. In Advances in Neural Information Processing Systems 32, pages
7135–7145. Curran Associates, Inc., 2019.

[6] Jesse Demiani. A voice deepfake was used to scam a ceo out of $243,000 - forbes. https://bit.ly/

38sXb1I, 9 2019.

[7] Romance scammer used deepfakes to impersonate a navy admiral and bilk widow out of nearly
https://www.thedailybeast.com/romance-scammer-used-deepfakes-to-impersonate-a-navy-

$300,000.
admiral-and-bilk-widow-out-of-nearly-dollar300000. (Accessed on 02/09/2021).

11

[8] Fraudsters

cloned

company

lice
huge-bank-fraud-uses-deep-fake-voice-tech-to-steal-millions/?sh=23254d367559,
(Accessed on 08/17/2022).

$35 million
po-
director’s
https://www.forbes.com/sites/thomasbrewster/2021/10/14/
2020.

heist,

voice

bank

ﬁnd.

in

[9] European mps

targeted

by

— russia — the
european-mps-targeted-by-deepfake-video-calls-imitating-russian-opposition,
(Accessed on 08/17/2022).

opposition
https://www.theguardian.com/world/2021/apr/22/
2021.

guardian.

imitating

deepfake

russian

video

calls

[10] Fbi:

Scammers are interviewing for remote jobs using deepfake tech — mashable.

https:

//mashable.com/article/deepfake-job-interviews-fbi#:~:text=Deepfakes%20involve%
20using%20AI%2Dpowered,say%20whatever%20you’d%20like., 2022. (Accessed on 08/17/2022).

[11] Kavita Kavita, Gurjit Singh Walia, and Rajesh Rohilla. A contemporary survey of unimodal liveness
detection techniques: Challenges & opportunities. In 2020 3rd International Conference on Intelligent
Sustainable Systems (ICISS), pages 848–855. IEEE, 2020.

[12] Jessica Lee, Deva Ramanan, and Rohit Girdhar. Metapix: Few-shot video retargeting. arXiv preprint

arXiv:1910.04742, 2019.

[13] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky. Few-shot adversarial learn-

ing of realistic neural talking head models. arXiv preprint arXiv:1905.08233, 2019.

[14] Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. Few-shot
video-to-video synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2019.

[15] Shaoanlu. fewshot-face-translation-gan: Generative adversarial networks integrating modules from fu-
nit and spade for face-swapping. https://github.com/shaoanlu/fewshot-face-translation-GAN,
2019.

[16] Daniil Osokin. Real-time 2d multi-person pose estimation on cpu: Lightweight openpose. arXiv preprint

arXiv:1811.12004, 2018.

[17] Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopad-

hyay. Adversarial attacks and defences: A survey. arXiv preprint arXiv:1810.00069, 2018.

[18] Yisroel Mirsky, Tomer Doitshman, Yuval Elovici, and Asaf Shabtai. Kitsune: An ensemble of au-
In The Network and Distributed System Security

toencoders for online network intrusion detection.
Symposium (NDSS) 2018, 2018.

[19] Yair Meidan, Michael Bohadana, Yael Mathov, Yisroel Mirsky, Asaf Shabtai, Dominik Breitenbacher,
and Yuval Elovici. N-baiot—network-based detection of iot botnet attacks using deep autoencoders.
IEEE Pervasive Computing, 17(3):12–22, 2018.

[20] Valentina Conotter, Ecaterina Bodnari, Giulia Boato, and Hany Farid. Physiologically-based detection
In 2014 IEEE International Conference on Image Processing

of computer generated faces in video.
(ICIP), pages 248–252. IEEE, 2014.

[21] Yuezun Li, Ming-Ching Chang, and Siwei Lyu.

In ictu oculi: Exposing ai created fake videos by
detecting eye blinking. In 2018 IEEE International Workshop on Information Forensics and Security
(WIFS), pages 1–7. IEEE, 2018.

[22] Umur Aybars Ciftci and Ilke Demir. Fakecatcher: Detection of synthetic portrait videos using biological

signals. arXiv preprint arXiv:1901.02212, 2019.

[23] Pavel Korshunov and Sebastien Marcel. Speaker inconsistency detection in tampered video. In 2018

26th European Signal Processing Conference (EUSIPCO), pages 2375–2379. IEEE, 2018.

[24] Pavel Korshunov et al. Tampered speaker inconsistency detection with phonetically aware audio-visual

features. In International Conference on Machine Learning, number CONF, 2019.

12

[25] Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using inconsistent head poses. In ICASSP
2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
8261–8265. IEEE, 2019.

[26] Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki Nagano, and Hao Li. Protecting world
leaders against deep fakes. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pages 38–45, 2019.

[27] Akshay Agarwal, Richa Singh, Mayank Vatsa, and Afzel Noore. Swapped! digital face presentation
attack detection via weighted local magnitude pattern. In 2017 IEEE International Joint Conference
on Biometrics (IJCB), pages 659–665. IEEE, 2017.

[28] Ying Zhang, Lilei Zheng, and Vrizlynn LL Thing. Automated face swapping and its detection. In 2017
IEEE 2nd International Conference on Signal and Image Processing (ICSIP), pages 15–19. IEEE, 2017.

[29] Zahid Akhtar and Dipankar Dasgupta. A comparative evaluation of local feature descriptors for deep-

fakes detection.

[30] Huaxiao Mo, Bolin Chen, and Weiqi Luo. Fake faces identiﬁcation via convolutional neural network. In

Proceedings of the 6th ACM Workshop on Information Hiding and Multimedia Security. ACM, 2018.

[31] Ricard Durall, Margret Keuper, Franz-Josef Pfreundt, and Janis Keuper. Unmasking deepfakes with

simple features. arXiv preprint arXiv:1911.00686, 2019.

[32] Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, and Baining Guo. Face x-ray

for more general face forgery detection. arXiv preprint arXiv:1912.13458, 2019.

[33] Huy H Nguyen, Fuming Fang, Junichi Yamagishi, and Isao Echizen. Multi-task learning for detecting

and segmenting manipulated facial images and videos. arXiv preprint arXiv:1906.06876, 2019.

[34] Mengnan Du, Shiva Pentyala, Yuening Li, and Xia Hu. Towards generalizable forgery detection with

locality-aware autoencoder. arXiv preprint arXiv:1909.05999, 2019.

[35] Joel Stehouwer, Hao Dang, Feng Liu, Xiaoming Liu, and Anil Jain. On the detection of digital face

manipulation. arXiv preprint arXiv:1910.01717, 2019.

[36] Jia Li, Tong Shen, Wei Zhang, Hui Ren, Dan Zeng, and Tao Mei. Zooming into face forensics: A

pixel-level analysis. arXiv preprint arXiv:1912.05790, 2019.

[37] Yuezun Li and Siwei Lyu. Exposing deepfake videos by detecting face warping artifacts.
Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019.

In IEEE

[38] Yuezun Li and Siwei Lyu. Dsp-fwa: Dual spatial pyramid for exposing face warp artifacts in deepfake

videos. https://github.com/danmohaha/DSP-FWA, 2019. (Accessed on 12/18/2019).

[39] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. Celeb-df: A new dataset for deepfake

forensics. arXiv preprint:1909.12962, 2019.

[40] Jeremy Straub. Using subject face brightness assessment to detect ‘deep fakes’(conference presentation).
In Real-Time Image Processing and Deep Learning 2019, volume 10996, page 109960H. International
Society for Optics and Photonics, 2019.

[41] Pavel Korshunov and Sebastien Marcel. Deepfakes: a new threat to face recognition? assessment and

detection. arXiv preprint arXiv:1812.08685, 2018.

[42] Ning Yu, Larry S Davis, and Mario Fritz. Attributing fake images to gans: Learning and analyzing gan

ﬁngerprints. In Proceedings of the IEEE International Conference on Computer Vision, 2019.

[43] Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, and Giovanni Poggi. Do gans leave artiﬁcial
ﬁngerprints? In 2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),
pages 506–511. IEEE, 2019.

[44] Marissa Koopman, Andrea Macarulla Rodriguez, and Zeno Geradts. Detection of deepfake video ma-

nipulation. In Conference: IMVIP, 2018.

13

[45] David Guera and Edward J Delp. Deepfake video detection using recurrent neural networks. In IEEE

Conference on Advanced Video and Signal Based Surveillance (AVSS), pages 1–6. IEEE, 2018.

[46] Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAlmageed, Iacopo Masi, and Prem Natarajan.
Recurrent-convolution approach to deepfake detection-state-of-art results on faceforensics++. arXiv
preprint arXiv:1905.00582, 2019.

[47] Irene Amerini, Leonardo Galteri, Roberto Caldelli, and Alberto Del Bimbo. Deepfake video detection
In Proceedings of the IEEE International Conference on Computer

through optical ﬂow based cnn.
Vision Workshops, pages 0–0, 2019.

[48] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In Proceedings

of the IEEE International Conference on Computer Vision, pages 5933–5942, 2019.

[49] Ehab A AlBadawy, Siwei Lyu, and Hany Farid. Detecting ai-synthesized speech using bispectral analysis.

In CVPR Workshops, pages 104–109, 2019.

[50] Akash Chintha, Bao Thai, Saniat Javid Sohrawardi, Kartavya Bhatt, Andrea Hickerson, Matthew
Wright, and Raymond Ptucha. Recurrent convolutional structures for audio spoof and video deepfake
detection. IEEE Journal of Selected Topics in Signal Processing, 14(5):1024–1037, 2020.

[51] Tianxiang Chen, Avrosh Kumar, Parav Nagarsheth, Ganesh Sivaraman, and Elie Khoury. General-
ization of audio deepfake detection. In Proceedings of the Odyssey Speaker and Language Recognition
Workshop, Tokyo, Japan, pages 1–5, 2020.

[52] Run Wang, Felix Juefei-Xu, Yihao Huang, Qing Guo, Xiaofei Xie, Lei Ma, and Yang Liu. Deepsonar:
Towards eﬀective and robust detection of ai-synthesized fake voices. In Proceedings of the 28th ACM
International Conference on Multimedia, pages 1207–1216, 2020.

[53] Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. Mesonet: a compact facial video
forgery detection network. In 2018 IEEE International Workshop on Information Forensics and Security
(WIFS), pages 1–7. IEEE, 2018.

[54] Nhu-Tai Do, In-Seop Na, and Soo-Hyung Kim. Forensics face detection from gans using convolutional

neural network, 2018.

[55] Shahroz Tariq, Sangyup Lee, Hoyoung Kim, Youjin Shin, and Simon S Woo. Detecting both machine
and human created fake face images in the wild. In Proceedings of the 2nd International Workshop on
Multimedia Privacy and Security, pages 81–87. ACM, 2018.

[56] Xinyi Ding, Zohreh Raziei, Eric C Larson, Eli V Olinick, Paul Krueger, and Michael Hahsler. Swapped
face detection using deep learning and subjective assessment. arXiv preprint arXiv:1909.04217, 2019.

[57] Tharindu Fernando, Clinton Fookes, Simon Denman, and Sridha Sridharan. Exploiting human so-
cial cognition for the detection of fake and fraudulent faces via memory networks. arXiv preprint
arXiv:1911.07844, 2019.

[58] Francesco Marra, Diego Gragnaniello, Davide Cozzolino, and Luisa Verdoliva. Detection of gan-
In 2018 IEEE Conference on Multimedia Information

generated fake images over social networks.
Processing and Retrieval (MIPR), pages 384–389. IEEE, 2018.

[59] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niess-
ner. Faceforensics++: Learning to detect manipulated facial images. arXiv preprint:1901.08971, 2019.

[60] Huy H Nguyen, Junichi Yamagishi, and Isao Echizen. Capsule-forensics: Using capsule networks to
detect forged images and videos. In ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 2307–2311. IEEE, 2019.

[61] Run Wang, Lei Ma, Felix Juefei-Xu, Xiaofei Xie, Jian Wang, and Yang Liu. Fakespotter: A simple

baseline for spotting ai-synthesized fake faces. arXiv preprint arXiv:1909.06122, 2019.

[62] Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang Hua. Towards open-set identity preserving
face synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2018.

14

[63] Xinsheng Xuan, Bo Peng, Wei Wang, and Jing Dong. On the generalization of gan image forensics. In

Chinese Conference on Biometric Recognition, pages 134–141. Springer, 2019.

[64] Davide Cozzolino, Justus Thies, Andreas Rossler, Christian Riess, Matthias Niessner, and Luisa Ver-
doliva. Forensictransfer: Weakly-supervised domain adaptation for forgery detection. arXiv preprint
arXiv:1812.02510, 2018.

[65] Xiaoguang Tu, Hengsheng Zhang, Mei Xie, Yao Luo, Yuefei Zhang, and Zheng Ma. Deep transfer across

domains for face anti-spooﬁng. arXiv preprint arXiv:1901.05633, 2019.

[66] Erkam Uzun, Simon Pak Ho Chung, Irfan Essa, and Wenke Lee. rtcaptcha: A real-time captcha based

liveness detection system. In NDSS, 2018.

[67] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niess-
ner. Faceforensics: A large-scale video dataset for forgery detection in human faces. arXiv preprint
arXiv:1803.09179, 2018.

[68] Ali Khodabakhsh, Raghavendra Ramachandra, Kiran Raja, Pankaj Wasnik, and Christoph Busch. Fake
face detection methods: Can they be generalized? In 2018 International Conference of the Biometrics
Special Interest Group (BIOSIG), pages 1–6. IEEE, 2018.

[69] Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, and Baining Guo. Face x-ray
In Proceedings of the IEEE/CVF Conference on Computer

for more general face forgery detection.
Vision and Pattern Recognition, pages 5001–5010, 2020.

[70] Xurong Li, Kun Yu, Shouling Ji, Yan Wang, Chunming Wu, and Hui Xue. Fighting against deepfake:
Patch&pair convolutional neural networks (ppcnn). In Companion Proceedings of the Web Conference
2020, pages 88–89, 2020.

[71] Yuval Nirkin, Lior Wolf, Yosi Keller, and Tal Hassner. Deepfake detection based on the discrepancy

between the face and its context. arXiv preprint arXiv:2008.12262, 2020.

[72] Iacopo Masi, Aditya Killekar, Royston Marian Mascarenhas, Shenoy Pratik Gurudatt, and Wael
arXiv preprint

AbdAlmageed. Two-branch recurrent network for isolating deepfakes in videos.
arXiv:2008.03412, 2020.

[73] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated
images are surprisingly easy to spot... for now. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, volume 7, 2020.

[74] Zhiqing Guo, Gaobo Yang, Jiyou Chen, and Xingming Sun. Fake face detection via adaptive residuals

extraction network. arXiv preprint arXiv:2005.04945, 2020.

[75] Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha. Emo-
arXiv preprint

tions don’t lie: A deepfake detection method using audio-visual aﬀective cues.
arXiv:2003.06711, 2020.

[76] Shruti Agarwal, Hany Farid, Ohad Fried, and Maneesh Agrawala. Detecting deep-fake videos from
phoneme-viseme mismatches. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops, pages 660–661, 2020.

[77] Irene Amerini and Roberto Caldelli. Exploiting prediction error inconsistencies through lstm-based
classiﬁers to detect deepfake videos. In Proceedings of the 2020 ACM Workshop on Information Hiding
and Multimedia Security, pages 97–102, 2020.

[78] Chih-Chung Hsu, Yi-Xiu Zhuang, and Chia-Yen Lee. Deep fake image detection based on pairwise

learning. Applied Sciences, 10(1):370, 2020.

[79] Md Shohel Rana and Andrew H Sung. Deepfakestack: A deep ensemble-based learning technique for
deepfake detection. In 2020 7th IEEE International Conference on Cyber Security and Cloud Com-
puting (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud
(EdgeCom), pages 70–75. IEEE, 2020.

15

[80] Oscar de Lima, Sean Franklin, Shreshtha Basu, Blake Karwoski, and Annet George. Deepfake detection

using spatiotemporal convolutional networks. arXiv preprint arXiv:2006.14749, 2020.

[81] Hasam Khalid and Simon S Woo. Oc-fakedect: Classifying deepfakes using one-class variational au-
toencoder. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
Workshops, pages 656–657, 2020.

[82] Steven Fernandes, Sunny Raj, Rickard Ewetz, Jodh Singh Pannu, Sumit Kumar Jha, Eddy Ortiz, Iustina
Vintila, and Margaret Salter. Detecting deepfake videos using attribution-based conﬁdence metric. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,
pages 308–309, 2020.

[83] Seyed Ali Jalalifar, Hosein Hasani, and Hamid Aghajan. Speech-driven facial reenactment using condi-

tional generative adversarial networks. arXiv preprint arXiv:1803.07461, 2018.

[84] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep con-
volutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 1646–1654, 2016.

[85] Yuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan: Subject agnostic face swapping and reenactment.
In Proceedings of the IEEE International Conference on Computer Vision, pages 7184–7193, 2019.

[86] Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. Faceshifter: Towards high ﬁdelity and

occlusion aware face swapping. arXiv preprint arXiv:1912.13457, 2019.

[87] Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network

for pose-guided human video generation. arXiv preprint arXiv:1910.09139, 2019.

[88] Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, and Tamara L Berg. Dance dance generation:

Motion transfer for internet videos. arXiv preprint arXiv:1904.00129, 2019.

[89] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016.

[90] Paarth Neekhara, Shehzeen Hussain, Malhar Jere, Farinaz Koushanfar, and Julian McAuley. Adver-
sarial deepfakes: Evaluating vulnerability of deepfake detectors to adversarial examples. arXiv preprint
arXiv:2002.12749, 2020.

[91] Nicholas Carlini and Hany Farid. Evading deepfake-image detectors with white-and black-box attacks.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,
pages 658–659, 2020.

16

