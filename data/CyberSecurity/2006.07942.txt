Duplicity Games for Deception Design with an
Application to Insider Threat Mitigation

Linan Huang, Student Member, IEEE, and Quanyan Zhu, Member, IEEE

1

1
2
0
2

t
c
O
4
1

]
T
G
.
s
c
[

3
v
2
4
9
7
0
.
6
0
0
2
:
v
i
X
r
a

Abstract—Recent incidents such as the Colonial Pipeline
ransomware attack and the SolarWinds hack have shown
that traditional defense techniques are becoming insufﬁcient
to deter adversaries of growing sophistication. Proactive and
deceptive defenses are an emerging class of methods to defend
against zero-day and advanced attacks. This work develops
a new game-theoretic framework called the duplicity game
to design deception mechanisms that consist of a generator,
an incentive modulator, and a trust manipulator, referred
to as the GMM mechanism. We formulate a mathematical
programming problem to compute the optimal GMM mecha-
nism, quantify the upper limit of enforceable security policies,
and characterize conditions on user’s identiﬁability and
manageability for cyber attribution and user management.
We develop a separation principle that decouples the design
of the modulator from the GMM mechanism and an equiv-
alence principle that turns the joint design of the generator
and the manipulator into the single design of the manipulator.
A case study of dynamic honeypot conﬁgurations is pre-
sented to mitigate insider threats. The numerical experiments
corroborate the results that the optimal GMM mechanism
can elicit desirable actions from both selﬁsh and adversarial
insiders and consequently improve the security posture of
the insider network. In particular, a proper modulator can
reduce the incentive misalignment between the players and
achieve win-win situations for the selﬁsh insider and the
defender. Meanwhile, we observe that the defender always
beneﬁts from faking the percentage of honeypots when the
optimal generator is presented.

Index Terms—Bayesian persuasion, proactive defense,
insider threat, cyber deception, cyber

mechanism design,
attribution, cyber trust, incentive mechanism

I. INTRODUCTION

C YBER deception is an emerging proactive defense

technique against increasingly sophisticated attacks,
insider
including Advanced Persistent Threats (APTs),
threats, and supply chain attacks. Defensive deception
technologies, such as Moving Target Defense (MTD) [1]
and honeypots [2], create uncertainties and misinformation

This paper has been accepted for publication in IEEE Transactions on

Information Forensics and Security

This work is partially supported by grants SES-1541164, ECCS-
1847056, CNS-2027884, and BCS-2122060 from National Science Foun-
dation (NSF), DOE-NE grant 20-19829 and grant W911NF-19-1-0041
from Army Research Ofﬁce (ARO).

L. Huang and Q. Zhu are with the Department of Electrical and
Computer Engineering, New York University, Brooklyn, NY, 11201,
USA. E-mail:{lh2328,qz494}@nyu.edu

Digital Object Identiﬁer 10.1109/TIFS.2021.3118886

for adversaries to misdirect their perception and decision
processes [3]. Among many success stories from industry,
it has been shown in [4] that deception technology has
successfully reduced the data breach costs by 51.4% and
per analyst costs by 32% for the Security Operations
Center (SOC). An important application of cyber decep-
tion is to defend systems from insider threats. Harmful
behaviors of inadvertent insiders or insiders with mali-
cious intentions can lead to compromises of sensitive data
and disruptions in the organization’s normal operations
[5]. Deception technologies provide promising proactive
solutions to detect unwarranted behaviors and deter the
insiders from wrongdoing, e.g., [6].

The design of successful defensive deception relies on
a formal approach that quantiﬁes the strategic interactions
including a defender,
of the three classes of players,
users, and adversaries. A useful framework to design
cyber deception mechanisms needs to capture three main
features. First, the defender, the users, and the adversaries
are strategic players with clear but imperfectly aligned
objectives or incentives. Second, the defender cannot dis-
tinguish adversaries from the normal users. For example,
the defender does not know who is an adversarial insider
when designing a security policy for the network. Apart
from this, the defender cannot distinguish the type of
users in the network concerning their objectives, resources,
and trust values. Third, a sophisticated adversary behaves
stealthily and intelligently, e.g., by conducting successful
reconnaissance or acting like a normal user to gain access
or trust.

In this work, we propose Duplicity Games (DG) as a
mechanism design framework for defensive deception to
elicit desirable security outcomes when a defender, normal
users, and adversaries interact to attain their individual
objectives. A DG is a two-stage game between a defender
and a normal/adversarial user with two-sided asymmetric
information. The defender, or the defensive deceiver, has
private information of the system state. The user has a
private type, which characterizes the user’s objectives,
trustworthiness, and attributes, e.g., normal or adversarial.
the defender designs
At
three composable components of the mechanism, i.e., a
generator, an incentive modulator, and a trust manipu-
lator. The generator is a mechanism that stochastically

the ﬁrst stage of the game,

Copyright © 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes,creating new collective works, for resale or redistribution to servers or lists, or reuse of any
copyrighted component of this work in other works.

 
 
 
 
 
 
generates signals or security policies based on the system’s
private information and system constraints. The modulator
reshapes the user’s incentive by creating constrained utility
transfers between two players. The manipulator distorts
the user’s prior belief over the unknowns. These three
components are together referred to as the GMM mecha-
nism. After the mechanism is designed and implemented,
the user observes the security policies, updates his trust
through the Bayesian rule, and then responds to the GMM
mechanism by taking an action that serves his objective.
The optimal design of the GMM mechanisms anticipates
the behaviors of different types of users under a given set
of security policies and elicits desirable security behaviors.
The GMM mechanisms we introduce here represent a class
of multi-dimensional security mechanisms that control
the security policies, the (dis)incentives, and the digital
footprints (e.g., feature patterns and conﬁgurations of
honeypots and normal servers).

We formulate the design problem into a mathematical
programming problem, where the anticipated behavioral
outcomes of the users follow the Incentive-Compatible
(IC) constraint and the Modulation-Feasible (MF) con-
straint. We use concaviﬁcation techniques as in [7], [8]
to provide a graphical analysis and interpretation of the
GMM mechanism. We observe that the user’s expected
posterior utility can be fully characterized by Piece-Wise
Linear and Convex (PWLC) functions. This observation
leads to a signiﬁcantly reduced number of enforceable
security policies and enables an efﬁcient implementation
of the GMM mechanism. Finally, we show a fundamental
separation principle in which the defender can design
the modulator independently, and an equivalence principle
where the joint design of the generator and the manipulator
is equivalent to the single design of the manipulator.

For further elaboration, we use the DG framework to
study insider threats and design mitigation strategies to
deter and prevent misbehavior in corporate networks. The
corporate network defender can adaptively conﬁgure hon-
eypots and normal servers to counter ﬁngerprinting (i.e.,
generator), modify the complexity of the authentication
process to change user’s incentives (i.e., modulator), and
misreport the percentage of honeypots to make use of
the user’s trust (i.e., manipulator). The design of the
GMM mechanism leads to a set of multi-faceted socio-
technical solutions for insider threats, which formalizes the
management guidelines for insider threats recommended in
[5]. From the generator design, we propose the concept of
the motive threshold to assess the average motive of the
entire insider population and the concept of the deterrence
threshold to measure the adequacy of the honeypots. From
the modulator design, we illustrate how the proper design
of the authentication cost can reduce the misalignment
between the insiders’ and the defender’s incentives. From
the manipulator design, we ﬁnd that the manipulation of

2

the insiders’ initial beliefs can harm the defender when
there are no deceptive generators, but create an advantage
when the optimal generator is applied.

A. Related Work

1) Game Theory for Cyber Deception: Game theory
has been widely applied for proactive defense and cy-
ber deception to enhance the security of cyber-physical
systems [9]–[12]. Games of incomplete information pro-
vide a natural paradigm to quantify the uncertainty and
misinformation induced by the deception. Exemplary
game models include signaling games [13], [14], dynamic
Bayesian games with ﬁnite [15] or inﬁnite states [16],
(Bayesian) Stackelberg security games [17]–[19], and par-
tially observable stochastic games [20]. These incomplete-
information games focus on ﬁnding signals and behaviors
at the equilibrium for a given mechanism and information
structure. In this work, we further aim to design the
mechanism and exploit the information asymmetry, which
proactively enhances cyber security.

2) Incentive Mechanisms and Information Design in
Cyber Systems: There is rich literature on incentive mech-
anisms designed to enhance security [21], [22], efﬁciency
[23], [24], privacy [25] of cyber-physical systems. They
are applied to wide applications, including crowdsourcing
[23], [25], mobile sensing [22], cloud computing [24],
cyber insurance [26], and security as service [21], [27].
These incentive mechanisms mainly focus on designing
the payoff rules and the allocation rules to incentivize
participants’ behaviors in the designer’s favor. Besides
incentive design, previous works have also investigated
information design by disclosing information strategically,
which has been applied to wildlife protection [28], con-
gestion mitigation [29], and honeypot conﬁguration [30].
DGs broaden the scope of these two classes of mechanisms
to the joint design of information, incentive, and trust to
achieve desirable equilibrium outcomes.

3) Insider Threat Mitigation and Incentive Design:
Previous works, e.g., [5], [31], have proposed guidelines
to establish effective inside threat mitigation programs.
Game-theoretic models have been developed to detect
insider threats [32] and identify the best response strategy
[33]. The recent work [34] has incorporated organizational
culture and the existing defensive mechanisms into the
game model. These works provide a quantitative under-
standing of insider threats but overlook the human aspects,
such as compliance and incentives, which are fundamental
and challenging problems for insider threat mitigation.
The authors in [35] have used signaling games to model
compliant and non-compliant insiders and adopted a feed-
back loop to control their compliance. This work uses
honeypots as a way to detect and monitor the misbehavior
of the insiders and aims to formalize the design of such
guidelines, e.g., detection, incentives, and penalties.

TABLE I: Summary of notations for DG-GMM.

Variable
N, M, K
b(·) ∈ ∆X
bU (·|θ ) ∈ ∆X
bD(·|x) ∈ ∆Θ
p0 := [p0
1, · · · , p0
N ]
p := [p1, · · · , pN ]
U ) or a∗
a∗
θ (bπ
θ (p)

¯vD(π, p0)

˜vD(p0) = ¯vD(π 0, p0)

VD(p0) = ¯vD(π ∗, p0)

s{a1,a2,··· ,aM }

Meaning
Number of states, types, and actions.
True probability distribution of the state.
User’s initial belief of the state under θ .
Defender’s initial belief of the type at x.
Common prior belief in vector form.
Common posterior belief in vector form.
Optimal response action of a type-θ user to
maximize his expected posterior utility.
Defender’s expected posterior utility under
generator π and common prior belief p0.
Defender’s prior utility where generator π 0
contains zero information.
Defender’s optimal posterior utility where
generator π ∗ is optimal.
Security policy that requires the user of
type θl ∈ Θ to take action al ∈ A for all
l ∈ {1, 2, · · · , M}.

TABLE II: Summary of notations in the case study.

Variable Meaning
p0,H
D
p0,H
U
pH
U
qg
qb
rU φ 0
tg(φ 0)
tb(φ 0)

Defender’s prior belief of a node being a honeypot.
Insider’s prior belief of a node being a honeypot.
Insider’s posterior belief of a node being a honeypot.
Percentage of selﬁsh insiders.
Percentage of adversarial insiders.
Insider’s authentication cost.
Decision thresholds of the selﬁsh insiders.
Decision thresholds of the adversarial insiders.

B. Notations and Organization of the Paper

Calligraphic letter A deﬁnes a set. The notation ∆A
represents the set of probability distribution over A
and |A | represents its cardinality. We summarize main
notations for the general model and the case study in
Table I and Table II, respectively. The rest of the paper
is organized as follows. Section II introduces the DG
model. We present the mathematical programming and the
concaviﬁcation method in Section III and IV, respectively.
Section V presents a case study of honeypot conﬁguration
to mitigate insider threats and Section VI concludes the
paper.

II. DUPLICITY GAME MODEL

We present a motivating example of insider threat
mitigation in Section II-A. Then, we present the structure
of DG in Section II-B and the timeline of the GMM
mechanism design in Section II-C, respectively. Finally,
we illustrate the relation of the DG-GMM mechanism to
the Bayesian persuasion framework in Section II-D.

A. Motivating Example of Insider Threat Mitigation

Insider threats have been a long-standing problem in
cybersecurity. Due to their information, privilege, and
resource advantages over external attackers, insider threats

3

can circumvent classical defense techniques such as in-
trusion prevention and detection systems. As a result,
defensive deception methods, such as honeypots, have
been used for insider threat detection and mitigation (see
e.g., [6], [36]). Theoretically, honeypots are assumed to
achieve a zero false-positive rate and low false-negative
rate by generating decoys accessed only by attackers. This
assumption may not hold for insider threats. On the one
hand, non-adversarial insiders who are curious or error-
prone can access honeypots, which intensiﬁes alert fatigue.
On the other hand, adversarial insiders can access the
internal information and ﬁngerprint honeypots [37], [38]
using features such as open ports, protocols, and error
responses. To address these two challenges, we need to
conﬁgure the honeypot and the normal servers strategi-
cally. The conﬁguration needs to elicit desirable behaviors
from both adversarial and non-adversarial insiders even
though they have the same insider information. This work
introduces three conﬁguration methods that can be used
independently or jointly; i.e., conﬁgure the feature pattern
adaptively (see Example 1 for details), prolong or shorten
the authentication time to change insiders’ incentives, and
misreport the percentage of honeypots to make use of the
insiders’ trust.

1) Categorization of Insiders’ Motives: An insider’s
motive can be roughly classiﬁed into seven subcategories
based on the VERIS Community Database (VCDB) [39].
We divide these subcategories of motives into three classes
of motives: selﬁsh, adversarial, and unintentional. They
make up 12%, 26%, and 62%, respectively. The class of
selﬁsh motives includes fun, convenience, fear, or ideol-
ogy. The adversarial motives include espionage, ﬁnancial
gain, or grudge. The category of unintentional motives
refers to the negligent insiders who take no notice of the
deceptive conﬁguration and make habitual decisions. The
incentives of unintentional insiders are often uncontrol-
lable through incentives. Our incentive design mechanism
here focuses on the class of the selﬁsh insiders, who
seek self-interest, and the adversarial ones, who seek to
sabotage the organization.

2) Corporate Network with Insiders and Honeypots:
Fig. 1 illustrates a corporate network with honeypots
(denoted by xH ) and normal servers (denoted by xN) as
nodes. The SOC, or the defender, can privately determine
the percentage, the location, and the conﬁguration of hon-
eypots in the corporate network. The goal of the defender
is to elicit desirable behaviors from the selﬁsh insiders
(denoted by θ g) and the adversarial
insiders (denoted
by θ b). Both types of insiders can take harmful actions
intentionally yet for different reasons or motives. For
example, selﬁsh insiders may violate security rules and
abuse their privileges to save time and effort in ﬁnishing
their tasks. They do not seek to sabotage the organization
as the adversarial ones do. For each node in the corporate

4

and bU through a virtual trust manipulator. For example,
the defender can determine the percentage of honeypots
to be b(xH ) but report the percentage as bU (xH |θ ) to
the type-θ users who determine the percentage of honey-
pots based on the report without additional information.
The trust manipulator is overt
if the user’s perceived
state distribution equals the true one for all types, i.e.,
bU (x|θ ) = b(x), ∀x ∈ X , ∀θ ∈ Θ. Otherwise,
the trust
manipulator is said to be covert as the defender stealthily
manipulates users’ initial beliefs.

3) Information Structure: The information structure
consists of a ﬁnite set of signals S and a generator
π ∈ Π : X (cid:55)→ ∆S . With a slight abuse of notation, we
use π(s|x) to represent the probability of signal s ∈ S
at state x ∈ X . In Example 1 below, the signal can be
interpreted as the feature patterns, including protocols,
ports, the response time, and the error response.

Example 1 (Dynamic Feature Pattern Conﬁgurations).
To defend against honeypot ﬁngerprinting, dynamic [40]
and adaptive [41], [42] conﬁgurations have been adopted
in honeypots. The SOC can also conﬁgure normal servers
and disguise them as honeypots by generating honeypot-
related features [43].

Suppose that there are J features that both honeypots
and normal servers can generate. Denote the value of fea-
ture j ∈ {1, · · · , J} by e j ∈ E j, where E j is a ﬁnite set. For
example, the error response feature can take a binary value
e j ∈ E j = {0, 1} based on whether an abnormal error
message appears under intentionally erroneous requests
[38]. We refer to the tuple of J features as the feature
pattern denoted by s = (e1, · · · , eJ) ∈ S := ∏J
E j. Then,
the feature pattern of each node changes dynamically
accordingly to the generator π ∈ Π; i.e., a honeypot
and a normal server generate feature pattern s ∈ S
with frequency π(s|xH ) and π(s|xN), respectively. Insiders
can use these feature patterns as the digital footprint to
ﬁngerprint a node’s state, either a honeypot or a normal
server. The DG still applies to the case when the SOC
cannot conﬁgure normal server. In that case, the decision
variable π(·|xN) will be taken as ﬁxed.

j=1

4) Utility Transfer: The utility transfer consists of
a scaling factor γ ∈ [0, ∞) and an incentive modulator
c ∈ C : A (cid:55)→ R which modiﬁes the utilities of the defender
and the user to be ˆvD(x, θ , a) = vD(x, θ , a) + γc(a) and
ˆvU (x, θ , a) = vU (x, θ , a) − c(a), respectively, for all x ∈
X , θ ∈ Θ, a ∈ A . Besides monetary (dis)incentives, c(a)
can also represent the additional cost or beneﬁt of taking
action a ∈ A . For example, it captures the authentication
time to access a normal server or a honeypot. The defender
can determine the authentication time to incentivize the
user (i.e., c(a) < 0) or disincentivize him (i.e., c(a) > 0)
to take the action a ∈ A . Although the modulator c is type-
independent, its inﬂuence on users is type-dependent. For

Fig. 1: An example corporate network consists of normal
servers and honeypots. The light blue background shows
the region of the internal network.

network, an insider can either access it (denoted by action
aAC) or not (denoted by action aDO).

B. Game Elements

The DG consists of four elements; i.e., the basic game
(X , Θ, A , vD, vU , b ∈ ∆X ), the belief statistics (bD(·|x) ∈
∆Θ, bU (·|θ ) ∈ ∆X ), the information structure (S , π ∈ Π),
and the utility transfer (γ, c ∈ C ).

1) Basic Game: The DG consists of two players i ∈
{D,U}, a defender i = D (hereafter she) and a user i = U
(hereafter he). Deﬁne the ﬁnite sets of N states, M types,
and K actions as X := {x1, · · · , xN}, Θ := {θ1, · · · , θM},
and A := {aDO, a1, · · · , aK−1}, respectively. Action aDO ∈
A is the drop-out action. It indicates that the user chooses
not to participate in the game and takes no action.

The game has two-sided asymmetric information. The
defender can privately observe or know the realization of
the state x ∈ X from a probability distribution b ∈ ∆X .
For example, in the corporate network in Fig. 1, b(xH )
and b(xN) represent the percentages of honeypots and
normal servers, respectively. The user does not know each
node’s state, i.e., whether a honeypot or a normal server.
The user has a private type θ ∈ Θ that represents his
motive, capacity, rationality, or risk perception. The user’s
behaviors are abstracted as an action a ∈ A . The defender
can observe the user’s action by monitoring and logging
but she cannot observe the user’s type; e.g., whether the
user accesses the conﬁdential data by accident (i.e., the
unintentional type), out of self-interest (i.e., the selﬁsh
type), or for adversarial purposes (i.e.,
the adversarial
type). The utility functions of the defender and the user,
denoted by vi : X × Θ × A (cid:55)→ R, i ∈ {D,U}, depend on
the state, type, and action.

2) Belief Statistics: The user’s initial belief of the state
under type θ ∈ Θ is bU (·|θ ) ∈ ∆X . Since the user does
not know the true state distribution b(·), his perceived
state distribution bU can be different from the true one.
The defender’s belief of the user’s type at state x ∈ X
is bD(·|x) ∈ ∆Θ. In the game, the defender can design b

5

Fig. 2: Timeline for the GMM mechanism design.

example, a curiosity-driven insider may lose interest and
give up accessing conﬁdential data under a long authen-
tication delay or a convoluted multi-factor authentication
process. However, an adversarial insider can be persistent
if the data access leads to a comparably high ﬁnancial
return. Deﬁnition 1 deﬁnes a special utility structure where
one action ak ∈ A yields the highest beneﬁt for the user
of type θ ∈ Θ regardless of the state values. For a user
with a dominant action, a generator does not inﬂuence the
user’s belief and action.
Deﬁnition 1. An action ak ∈ A dominates (resp.
is
dominated) under type θ ∈ Θ if ˆvU (x, θ , ak) ≥ (resp. ≤
) ˆvU (x, θ , a), ∀a ∈ A , ∀x ∈ X .

C. Timeline for the GMM Mechanism Design

As shown in Fig. 2, the GMM mechanism design in
DGs has two stages to achieve the intended outcomes of
the defensive deception. At stage one, the defender designs
(resp. observes) the generator π ∈ Π,
the manipulator
b ∈ ∆X , bU (·|θ ) ∈ ∆X , ∀θ ∈ Θ, and the modulator c ∈ C
if these components can (resp. cannot) be designed. Based
on the realized state value x, the generator generates a
signal s ∈ S with probability π(s|x). In the insider threat
example, the defender conﬁgures the feature pattern s with
probability π(s|xH ) (resp. π(s|xN)) when the node is a
honeypot (resp. normal server). At stage two, the user of
type θ ∈ Θ receives the signal s ∈ S and obtains his
posterior belief bπ
U of the state using the Bayesian rule,
i.e.,

bπ
U (x|θ , s) :=

bU (x|θ )π(s|x)
∑x(cid:48)∈X bU (x(cid:48)|θ )π(s|x(cid:48))

, ∀x ∈ X .

(1)

the user of type θ ∈ Θ takes a best-response
U ) ∈ A to maximize his expected

Then,
action denoted by a∗
θ (bπ
posterior utility under the posterior belief bπ

U , i.e.,
U (·|θ ,s)[ ˆvU (x, θ , a)].

(2)

a∗
θ (bπ

U ) ∈ arg max
a∈A

Ex∼bπ

The utility of the users is a way to capture the user
θ can represent how an in-

θ . For example, a∗

behavior a∗

sider routinely follows the security rules or abuses his
privilege for personal gain. The defender’s goal
is to
determine the optimal GMM mechanism to proactively
prevent undesirable user behaviors and improve the secu-
rity posture. This objective is achieved by maximizing her
expected posterior utility ¯vD that captures the outcomes of
the user’s behaviors, i.e., ¯vD(π, b, bU , c) := Ex∼b(·)Es∼π(·|x)
E
θ ∼bD(·|x)[ ˆvD(x, θ , a∗
U ))]. Different generators provide
the user with different amounts of information about the
state. Two extreme cases are deﬁned in Deﬁnition 2.
A signal from a zero-information generator denoted by
π 0 ∈ Π does not change the user’s belief, i.e., bπ0
U (x|θ , s) =
bU (x|θ ), ∀s ∈ S , ∀x ∈ X , ∀θ ∈ Θ. Meanwhile, a signal
from a full-information generator deterministically reveals
the state to the user.

θ (bπ

Deﬁnition 2 (Zero- and Full-Information Generators).
A generator π ∈ Π contains zero information if π(s|x) =
π(s|x(cid:48)), ∀s ∈ S , ∀x, x(cid:48) ∈ X . It contains full information if
the mapping π : X (cid:55)→ S is injective.

Readers can refer to Section V for a case study of insider

threat that illustrates the two-stage GMM design.

D. Relation to Bayesian Persuasion

DG-GMM mechanism design can be viewed as a
generalized class of the Bayesian persuasion framework
[8] with heterogeneous receivers, two-sided asymmetric
information, and a joint design of information, incentive,
and trust. If the user’s type set Θ is a singleton and the
defender cannot design the modulator and the manipulator,
then DG-GMM degenerates to the Bayesian persuasion
framework. The consolidation of the modulator and the
manipulator into the mechanism gives the defender a
higher degree of freedom to improve the performance
in the deception design. It yet increases the computation
complexity as illustrated in Section III and causes the
violation of Bayesian plausibility in Section II-D1.

1) Violation of Bayesian Plausibility: The concept of
Bayesian plausibility has been deﬁned in [8], which states

Design/ObserveSignal 𝑠∈𝒮or Security policy 𝑠{"!,…,""}∈𝒮Utility𝑣%&(𝑥,𝜃,𝑎)Belief updateDefenderGenerator 𝜋Manipulator 𝑏,𝑏’Modulator 𝑐HoneypotsNormal serversUserAction𝑎∈𝒜Defensive DeceptionPrivate type 𝜃∈ΘPrivate state  𝑥∈𝒳that the expected posterior belief should equal the prior
belief for all π ∈ Π. However, we show in Lemma 1
that the trust manipulator can violate Bayesian plausibility
when the user of type θ ∈ Θ holds a different initial belief
as the defender, i.e., ∃x ∈ X : b(x) (cid:54)= bU (x|θ ).

Lemma 1 (Bayesian Plausibility). For all π ∈ Π and θ ∈
Θ, the user’s expected posterior probability be
U (x|θ ) :=
∑s∈S ∑x(cid:48)∈X b(x(cid:48))π(s|x(cid:48))bπ
U (x|θ , s) is always a valid prob-
ability measure yet is Bayesian plausible if and only if
the defender and the user have the same initial belief
b(x) = bU (x|θ ), ∀x ∈ X .

receiving s,

Proof. A generator π ∈ Π generates s with proba-
bility ∑x(cid:48)∈X b(x(cid:48))π(s|x(cid:48)). After
the user
of type θ obtains his posterior belief bπ
U (x|θ , s) ac-
the expected posterior probabil-
cording to (1). Thus,
ity ∑s∈S ∑x(cid:48)∈X b(x(cid:48))π(s|x(cid:48))bπ
U (x|θ , s) is a valid prob-
ability measure over x. The Bayesian plausibility re-
∑x(cid:48)∈X b(x(cid:48))π(s|x(cid:48))
quires be
∑x(cid:48)∈X bU (x(cid:48)|θ )π(s|x(cid:48)) π(s|x)bU (x|θ ) =
bU (x|θ ), ∀x ∈ X , under all π ∈ Π, which is equivalent to
the condition b(x) = bU (x|θ ), ∀x ∈ X .

U (x|θ ) = ∑s∈S

III. GMM DESIGNS BY MATHEMATICAL
PROGRAMMING

θ (bπ

θ (bπ

In Section III, we provide an integrated design of the
GMM mechanism by mathematical programming. We ﬁrst
elaborate on the relationship between signals and the user’s
best-response action to introduce the notion of security
policies. Each signal s from generator π ∈ Π updates the
user’s belief via (1) and consequently induces the user of
U ) ∈ A .
type θ ∈ Θ to take the best-response action a∗
Regardless of the signal set S and the generator π, these
signals can elicit at most |A ||Θ| = KM distinct outcomes;
U ) is al if his type
i.e., the user’s best-response action a∗
is θl for all permutations of θl ∈ Θ, al ∈ A . We can aggre-
gate signals in S based on their elicited actions and divide
the entire signal set S into KM mutually exclusive subsets
{a1,a2,··· ,aM }, al ∈ A , l ∈ {1, 2, ..., M}. Then,
denoted as S
the signals in subset S
{a1,a2,··· ,aM } can be interpreted as
the security policy that requires the user of type θl to take
action al for all l ∈ {1, 2, · · · , M}. Without loss of general-
ity, we use one aggregated signal s{a1,a2,··· ,aM } to represent
the signals in the set S
{a1,a2,··· ,aM}. Then, the total number
of signals are |S | = KM, and π(·|x) ∈ ∆S is a probability
distribution over KM security policies for each state x ∈ X .
The set Π naturally contains two feasibility constraints,
i.e., π(s{a1,··· ,aM}|x) ≥ 0, ∀s{a1,··· ,aM} ∈ S , ∀x ∈ X , and
∑s{a1,··· ,aM }∈S π(s{a1,··· ,aM}|x) = 1, ∀x ∈ X . In Example 2
below, we continue to use the insider threat scenario in
Section II-A to illustrate how we obtain security policies
based on the feature patterns.

6

Example 2 (Security Policies based on Feature Pat-
terns). For binary action set A = {aDO, aAC} and binary
type set Θ = {θ g, θ b}, the feature patterns in Example
1 can be aggregated into KM = 4 categories of security
policies. They are s{aDO,aDO} (i.e., both types of insiders
choose aDO), s{aDO,aAC} (i.e., selﬁsh insiders choose aAC
while adversarial insiders choose aDO), s{aAC,aDO} (i.e.,
adversarial
insiders choose aAC while selﬁsh insiders
choose aDO), and s{aAC,aAC} (i.e., both types of insiders
choose aAC).

i.e., ∑x∈X bπ

We can rewrite (2) concerning security policies
U (x|θl, s{a1,··· ,aM })[ ˆvU (x, θl, al) −
as follows,
ˆvU (x, θl, ah)] ≥ 0, ∀s{a1,··· ,aM} ∈ S , ∀ah ∈ A , ∀θl ∈ Θ. The
defender’s expected posterior utility ¯vD(π, b, bU , c) can
be equivalently represented as ∑x∈X b(x) ∑s{a1,··· ,aM }∈S
π(s{a1,··· ,aM}|x) ∑θl ∈Θ bD(θl|x) ˆvD(x, θl, al). Replacing bπ
U
with (1), we formulate the GMM mechanism design as
the following constrained optimization COP.

(COP):

r :=

sup
π∈Π,b,bU ,c∈C

¯vD(π, b, bU , c)

[ ˆvU (x, θl, al) − ˆvU (x, θl, ah)]π(s{a1,··· ,aM}|x)

(IC) ∑
x∈X
bU (x|θl) ≥ 0, ∀s{a1,··· ,aM } ∈ S , ∀ah ∈ A , ∀θl ∈ Θ.

(MF) c(aDO) = 0.

The decision variables π, b, bU , and c are vectors of
dimension N × KM, N, N × M, and K, respectively. The
feasibility constraint contained in Π and the Incentive-
Compatible (IC) constraint induce N × KM + 1 and KM ×
K × M constraints, respectively.

Denote b∗, b∗

U , π ∗, c∗ as the maximizers of COP and r as
the value of the objective function under the maximizers.
The (IC) constraint requires all security policies from the
generator to be compatible with the user’s incentives; i.e.,
the user receives the maximum beneﬁt on average when
taking the action required by the security policy. A security
policy cannot be generated if it is not incentive-compatible.
Based on the (IC) constraint, we deﬁne the credible and the
optimal generators in Deﬁnition 3 and enforceable security
policies in Deﬁnition 4.

Deﬁnition 3 (Credible and Optimal Generators). A
generator π ∈ Π is called credible if it satisﬁes (IC). A
credible generator is called optimal if it maximizes COP.

Deﬁnition 4 (Enforceable Security Policies). For a given
generator π ∈ Π, a security policy s{a1,··· ,aM} ∈ S is
enforceable (resp. unenforceable) if ∃x ∈ X such that
π(s{a1,··· ,aM}|x) (cid:54)= 0 (resp. π(s{a1,··· ,aM}|x) = 0, ∀x ∈ X ).

The Modulation-Feasible (MF) constraint results from
the fact
the defender cannot modulate the user’s
incentive if the user does not participate in the game.
Although the co-domain of c is R, Theorem 1 shows that

that

the optimal utility transfer c∗ ∈ C has to remain bounded
due to the user’s potential
threat of taking the drop-
out action aDO. We deﬁne the following shorthand nota-
tions for Theorem 1, i.e., c(θ , a) := maxx∈X vU (x, θ , a) −
vU (x, θ , aDO), ¯r = maxx∈X Eθ ∼bD[maxa∈A vD(θ , x, a)] and
r = minx∈X Eθ ∼bD[mina∈A vD(θ , x, a)].

(Feasibility
and

Theorem 1
COP
bound
¯r + γ maxa∈A ,θ ∈Θ c(θ , a)} and the lower bound is r.

and Design Capacity).
upper
bounded.
is max{maxx∈X Eθ ∼bD[vD(x, θ , aDO)],

feasible
r

is
of

The

Proof. We ﬁrst prove the feasibility. Deﬁne short-
hand notation a∗,l := arg maxa∈A Ex∼bU (x|θl )[vU (x, θl, a) −
c(a)], ∀l ∈ {1, · · · , M}, as the optimal action of the user
of type θl ∈ Θ under any feasible prior belief bU (x|θl)
and modulator c ∈ C . Then, the zero-information generator
π 0(s(a∗,1,··· ,a∗,M )|x) = 1, ∀x ∈ X , is a feasible solution to
COP.

We prove the boundedness in two steps. We ﬁrst
consider c(a) = 0, ∀a ∈ A . Since all decision variables
b, π, bD are probability measures, we obtain the upper
bound ¯r and the low bound r of r. In the second step,
we turn the modulator c into a free decision variable
with the (MF) constraint. Since c(a) = 0, ∀a ∈ A ,
is
the maximum value of COP does
a feasible solution,
not increase. Thus, the value of r is bounded. To show
that the value of ¯r is bounded in step two, we focus
on action a j ∈ A ,
that results in a non-
negative maximizer c∗(a j). On the one hand, if c(θ , a j) ≤
0, ∀θ ∈ Θ, then the drop-out action aDO dominates for
all types and r = maxb∈∆X Ex∼bEθ ∼bD[vD(x, θ , aDO)] ≤
maxx∈X Eθ ∼bD[vD(x, θ , aDO)]. On the other hand, if there
exists a type θ ∈ Θ where c(θ , a j) > 0 and c∗(a j) ≥
c(θ , a j), then the user of type θ will choose the drop-out
action aDO. Thus, r ≤ γ maxa∈A ,θ ∈Θ c(θ , a).

if it exists,

The upper and lower bounds provide the design ca-
the GMM mechanism. COP is unbounded
pacity of
the (MF) constraint as the defender can ar-
without
the value of r by
bitrarily increase (resp. decrease)
letting c(a) be an arbitrarily large (resp. small) con-
If c(a) = 0, ∀a ∈ A , we can transform COP
stant.
into a Linear Program (LP) by introducing the follow-
ing variables, i.e., η(s{a1,··· ,aM}, x) := b(x)π(s{a1,··· ,aM}|x)
and ηU (θ , s{a1,··· ,aM}, x) := bU (x|θ )π(s{a1,··· ,aM}|x). These
take non-negative values and satisfy
new variables
i.e., ∑x∈X ,s{a1,··· ,aM }∈S η =
the following constraints,
1 and ∑x∈X ,s{a1,··· ,aM }∈S ηU = 1, ∀θ ∈ Θ. After we
have solved the LP, we can obtain the initial beliefs
by b(x) = ∑s{a1,··· ,aM }∈S η(s{a1,··· ,aM}, x) and bU (x|θ ) =
∑s{a1,··· ,aM }∈S ηU (θ , s{a1,··· ,aM }, x) for all x ∈ X , θ ∈ Θ.

IV. GRAPHICAL ANALYSIS OF GMM DESIGNS

7

In Section III, we aggregate signals into KM equivalent
security policies to relate them with the user’s best-
response action. In Section IV, we directly analyze the
posterior belief and the action as each signal uniquely
determines a posterior belief. Throughout Section IV, we
focus on the overt trust manipulator deﬁned in Section
II-B2, i.e., bU (x|θ ) = b(x), ∀x ∈ X , θ ∈ Θ. Deﬁne p0
j :=
b(x j), ∀ j ∈ {1, · · · , N}, and the common prior belief in the
vector form as p0 := [p0
1, · · · , p0
N]. Since different types of
users have the same initial beliefs, the posterior beliefs
are also the same. Denote p j ∈ [0, 1] as the user’s pos-
terior belief under state x j ∈ X , ∀ j ∈ {1, · · · , N}. Deﬁne
the belief vector p := [p1, · · · , pN] and the utility vector
ˆvU (θ , a) := [ ˆvU (x1, θ , a), · · · , ˆvU (xN, θ , a)](cid:48) where notation
(cid:48) denotes the matrix transpose. For both the prior and the
posterior belief vectors, the total probability is one, i.e.,
∑N

n=1 p0
Section IV-A provides the optimal generator design
under the benchmark case where the defender can neither
modify the user’s incentive, i.e., c(a) = 0, ∀a ∈ A , nor ma-
nipulate their initial beliefs. Section IV-B incorporates the
modulator and the manipulator into the GMM mechanism
design.

n = 1 and ∑N

n=1 pn = 1.

A. Generator Design under the Benchmark Case

We rewrite (2) in its matrix form as a∗

θ (p) ∈ arg maxa∈A
pˆvU (θ , a). Since pˆvU (θ , a) is an afﬁne function of p for
any action a ∈ A , maximizing pˆvU (θ , a) over a in the
convex domain p ∈ ∆X results in a Piece-Wise Linear and
Convex (PWLC) function as summarized in Proposition 1.
The proof of convexity follows directly from the fact that
a∗
θ (p) is the point-wise maximum of a group of afﬁne
functions over p.

Proposition 1. The user’s expected posterior utility under
a give type θ ∈ Θ, i.e., maxa∈A pˆvU (θ , a), is continuously
PWLC with respect to vector p ∈ ∆X .

We visualize maxa∈A pˆvU (θ , a) under a binary state set
in Fig. 3. When N = 2, we can use the ﬁrst element p1
as the x-axis to uniquely represent the posterior belief p ∈
∆X . The four belief thresholds, i.e., 0,tθ
2 , and 1, divide
the entire belief region of p1 ∈ [0, 1] into three sub-regions.
The user of type θ takes action aK−1 if his posterior belief
belongs to the sub-region p1 ∈ [0,tθ
1 ], action a1 if p1 ∈
[tθ
2 ], and action aDO if p1 ∈ [tθ
1 ,tθ
2 , 1]. Although action a2
is not dominated under type θ based on Deﬁnition 1, it is
inactive over p1 ∈ [0, 1].

1 ,tθ

For a high-dimensional state space N ≥ 3, the user’s
entire belief region ∆X is an N − 2 simplex. For each
type θ , we can divide the entire belief region into at most
K sub-regions C θ
ai := {p ≥ 0|p(cid:48)[ˆvU (θ , ai) − ˆvU (θ , a j)] ≥

8

Illustration of KM = 4 convex polytopes C{a1,a1},
Fig. 4:
C{aDO,a1}, C{aDO,aDO}, and C{a1,aDO} in blue (horizontal
stripes), green (downward diagonal stripes), grey (vertical
stripes), and orange (upward diagonal stripes), respec-
tively. Each point in the equilateral triangle represents a
belief p = [p1, p2, p3] ∈ ∆X .

belief regions. The results can be extended to N > 3 as
a variant of the hyperplane arrangement problem [44].
We summarize the above result in Proposition 2; i.e., the
number of belief region partitions grows in a polynomial
rate denoted by χ(K, M, N) rather than the exponential rate
of KM, where χ(K, M, N) is a polynomial function of K, M
for each N.

Proposition 2 (Upper Limit of Enforceable Policies).
For any credible generator, at most χ(K, M, N) security
policies are enforceable.

Remark 1. Solely dependent on the user’s utility vector
ˆvU , the belief partition ∆X = ∪a1∈A ,··· ,aM∈A C
{a1,··· ,aM}
characterizes the user’s incentive under different types. If
C
{a1,··· ,aM } = /0, then the security policies that require the
user of type θl to take action al for any l ∈ {1, · · · , M}
are unenforceable as they violate the user’s incentive.
Proposition 2 illustrates that the number of enforceable
security policies cannot exceed a threshold determined
by K, M, N; i.e., among all |S | = KM potential security
policies, the defender can choose at most χ(K, M, N) ones
to be compatible with the user’s incentive.

1) Cyber Attribution and Type Identiﬁcation: The hon-
eypot example motivates us to investigate the condition
under which public security policies elicit different actions
from different
types of users. The condition is useful
for cyber attribution, i.e., tracing observable actions back
to the user’s private types. Since each security policy
uniquely determines a posterior belief for a given genera-
tor, we deﬁne type identiﬁability concerning the posterior
belief in Deﬁnition 5.

Deﬁnition 5 (Identiﬁable Types). Two different types

Fig. 3: The expected posterior utility of the user of
type θ ∈ Θ versus posterior belief p1 ∈ [0, 1]. The solid
lines represent the utility maxa∈A ∑N
n=1 pn ˆvU (xn, θ , a) as a
PWLC function of p1.

i, j := C θl

2 , 1] and C θ
a2

a1 ∩ · · · ∩ C θM

{a1,··· ,aM} := C θ1

aDO is the interval [tθ

0, ∀a j ∈ A . Then, ∆X = ∪i∈{DO,1,··· ,K−1}C θ
ai . If the pos-
terior belief falls into the sub-region C θ
ai , the user of type
θ takes ai as his best-response action. Take Fig. 3 as an
example, C θ
is the empty
set. As a direct result of the deﬁnition of convexity, sets
C θ
ai , ∀i ∈ {DO, 1, · · · , K − 1}, are convex and connected.
We have illustrated the belief region partition under any
given type θ ∈ Θ. Since the user has M possible types,
we further divide the belief region into ﬁner sub-regions.
Let C
aM be the sub-region of the
posterior belief under which the best-response action of the
user of type θl, ∀l ∈ {1, · · · , M}, is action al ∈ A . In par-
ticular, deﬁne C l,h
ai ∩ C θh
a j as the belief region where
the user takes action ai when his type is θl and a j when his
type is θh for all i, j ∈ {DO, 1, · · · , K − 1} and l (cid:54)= h, ∀l, h ∈
{1, · · · , M}. Based on the deﬁnition, C l,h
j,i . Since
the intersection of any collection of convex sets is convex,
{a1,··· ,aM} and C l,h
C
i, j are all convex and connected sets, i.e.,
convex polytopes. We visualize these convex polytopes in
Fig. 4 when there are two types M = 2, two actions K = 2,
and three states N = 3. The belief region ∆X is an N − 2
simplex, i.e., an equilateral triangle. Under type θ1, the
belief region is divided into C θ1
aDO = C{aDO,a1} ∪ C{aDO,aDO}
and C θ1
a1 = C{a1,a1} ∪ C{a1,aDO}. Under type θ2, the belief
region is divided into C θ2
aDO = C{aDO,aDO} ∪ C{a1,aDO} and
C θ2
a1 = C{a1,a1} ∪ C{aDO,a1}. Since there are only two types,
we have C 1,2

i, j ≡ C h,l

1,DO = C{a1,aDO}.
Among KM possible sets C

{a1,··· ,aM}, ∀al ∈ A , l ∈
{1, · · · , M}, most of them are empty. Take N = 2 as an
example, K actions can generate at most K(K −1)/2 belief
thresholds over p1 ∈ (0, 1) for each type as shown in
Fig. 3. Thus, the whole belief region p1 ∈ [0, 1] can be
divided into at most MK(K − 1)/2 + 1 regions under M
types. When N = 3, the belief region is an equilateral
triangle as shown in Fig. 4. For each given type, K actions
represent K planes. Projecting these planes vertically onto
the equilateral triangle, we obtain at most K(K − 1)/2
lines. Thus, these lines under M types can divide the
equilateral triangle into at most MK(K−1)
+ 1)/2

( MK(K−1)
2

2

𝑎=𝑎!𝑎=𝑎"#𝑎=𝑎$%!𝑎=𝑎&1Utility<latexit sha1_base64="EifKk2OMkzZweTzzY4FCgnd5ylY=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMeiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0kPS9frniVt05yCrxclKBHI1++as3iFkaoTRMUK27npsYP6PKcCZwWuqlGhPKxnSIXUsljVD72fzUKTmzyoCEsbIlDZmrvycyGmk9iQLbGVEz0sveTPzP66YmvPYzLpPUoGSLRWEqiInJ7G8y4AqZERNLKFPc3krYiCrKjE2nZEPwll9eJa1a1bus1u4vKvWbPI4inMApnIMHV1CHO2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAByo2f</latexit>p1<latexit sha1_base64="yQbb92lFZ2G4RWLghdk+NupKkcw=">AAAB8XicdVDLSgNBEJz1GeMr6tHLYBA8hZ0gedyCXjxGMA9M1jA7mU2GzM4uM71CWPIXXjwo4tW/8ebfOJtEUNGChqKqm+4uP5bCgOt+OCura+sbm7mt/PbO7t5+4eCwbaJEM95ikYx016eGS6F4CwRI3o01p6EvecefXGZ+555rIyJ1A9OYeyEdKREIRsFKtzAgd30Yc6CDQtEtua5LCMEZIdWKa0m9XiuTGiaZZVFESzQHhff+MGJJyBUwSY3pETcGL6UaBJN8lu8nhseUTeiI9yxVNOTGS+cXz/CpVYY4iLQtBXiufp9IaWjMNPRtZ0hhbH57mfiX10sgqHmpUHECXLHFoiCRGCKcvY+HQnMGcmoJZVrYWzEbU00Z2JDyNoSvT/H/pF0ukUqpfH1ebFws48ihY3SCzhBBVdRAV6iJWoghhR7QE3p2jPPovDivi9YVZzlzhH7AefsEqkeQ7w==</latexit>t✓1<latexit sha1_base64="2QaoVXQgpUa7SR/GBbDv4aBOI3M=">AAAB8XicdVDLSgNBEJz1GeMr6tHLYBA8hZ0gedyCXjxGMA9M1jA7mU2GzM4uM71CWPIXXjwo4tW/8ebfOJtEUNGChqKqm+4uP5bCgOt+OCura+sbm7mt/PbO7t5+4eCwbaJEM95ikYx016eGS6F4CwRI3o01p6EvecefXGZ+555rIyJ1A9OYeyEdKREIRsFKtzAo3/VhzIEOCkW35LouIQRnhFQrriX1eq1MaphklkURLdEcFN77w4glIVfAJDWmR9wYvJRqEEzyWb6fGB5TNqEj3rNU0ZAbL51fPMOnVhniINK2FOC5+n0ipaEx09C3nSGFsfntZeJfXi+BoOalQsUJcMUWi4JEYohw9j4eCs0ZyKkllGlhb8VsTDVlYEPK2xC+PsX/k3a5RCql8vV5sXGxjCOHjtEJOkMEVVEDXaEmaiGGFHpAT+jZMc6j8+K8LlpXnOXMEfoB5+0Tq9KQ8A==</latexit>t✓20𝜃!𝜃"𝑎!𝑎"#𝒞!$%	#&𝒞!&	#&𝒞!&	#’𝒞!,$%	!,"=𝒞{(!,("#}𝒞{("#,("#}𝒞{(!,(!}𝒞{("#,(!}𝑎!𝑎"#𝒞!$%	#’𝐩=[1,0,0]𝐩=[0,0,1]𝐩=[0,1,0]l, h ∈ {1, · · · , M} are identiﬁable under a posterior belief
p ∈ ∆X if ∃i, j ∈ {DO, 1, · · · , K − 1} and i (cid:54)= j such that
p ∈ C l,h
i, j .

The posterior beliefs under which two different types
l, h ∈ {1, · · · , M} are identiﬁable constitute a belief region
that may not be connected. This belief region solely
depends on the user’s utility vector ˆvU as the ﬁnest belief
partition ∆X = ∪a1∈A ,··· ,aM ∈A C
{a1,··· ,aM} solely depends
on ˆvU . Intuitively,
the size of the region is reduced
as the utilities of the users of type θl and θh become
better aligned. Deﬁnition 6 deﬁnes two extremes of utility
alignment.

Deﬁnition 6 (Completely (Mis)aligned Utilities). Two
different types of users have completely aligned (resp.
misaligned) utilities, or equivalently zero (resp. full) utility
misalignment, if they are unidentiﬁable (resp. identiﬁable)
under all posterior belief p ∈ ∆X .

If two utilities have the same (resp. opposite) values,
then they are completely aligned (resp. misaligned). If
two types of users’ utilities are completely aligned (resp.
misaligned), then the security policies that procure them
to take different actions (resp. the same action) are not
enforceable under any credible generators. Proposition 3
shows that the results are translation- and scale-invariant.

Proposition 3 (Alignment under Linear Dependence).
Consider linearly dependent utilities of two types l, h ∈
there exist a scaling factor
{1, · · · , M} of users; i.e.,
U (θl, θh) ∈ R and translation factors ρt
ρ s
U (x, θl, θh) ∈
R, ∀x ∈ X , such that ˆvU (x, θl, a) = ρ s
U (θl, θh) ˆvU (x, θh, a)+
U (x, θl, θh), ∀x ∈ X , a ∈ A . Two utilities are completely
ρt
aligned (resp. misaligned) if and only if ρ s
U (θl, θh) ≥ 0
(resp. < 0).

given

p ∈ ∆X and
i ∈ A such that ∑N

Proof. For
θl ∈ Θ,
any
there exists an action a∗
n=1 pn
i ) − ˆvU (xn, θl, ak)] ≥ 0, ∀ak ∈ A . Then,
[ ˆvU (xn, θl, a∗
U (θl, θh) ∑N
ρ s
n=1 pn[ ˆvU (xn, θh, a∗
i ) − ˆvU (xn, θh, ak)] ≥ 0,
∀ak ∈ A , and the user of type θh ∈ Θ at any posterior
belief p has the same best-response action a∗
if and only
i
if ρ s

U (θl, θh) ≥ 0.

2) Characterization of the Optimal Generator: Under
a zero-information generator π 0 ∈ Π, the user’s posterior
belief equals the prior belief p0 and we can rewrite the
θ (bπ0
user’ best-response action a∗
θ (p0). Since
variables bU , c are not designable in the benchmark case,
we omit them in function ¯vD and rewrite the defender’s
expected posterior utility as ¯vD(π, p0). Since the users
make decisions based on their prior beliefs, we refer
to the expected posterior utility ¯vi of player i ∈ {D,U}
as his prior utility ˜vi when the generator contains zero

U ) in (2) as a∗

information. In particular, the defender’s prior utility ˜vD is
a function of the prior belief p0, i.e.,

9

˜vD(p0) := ¯vD(π 0, p0) = E

x∼p0E
θ (p0)].
We obtain the piece-wise linear structure of the defender’s
prior utility ˜vD in Proposition 4. The solid lines in Fig. 5
illustrate ˜vD.

θ ∼bD(·|x)[ ˆvD(x, θ , a∗

Proposition 4. The defender’s prior utility ˜vD is a (possi-
bly discontinuous) piece-wise linear function of the com-
mon prior belief vector p0 ∈ ∆X with at most χ(K, M, N)
pieces.

that

˜vD is linear with respect

Proof. The piece-wise linear structure follows from the
to p0 inside each
fact
convex polytope C
{a1,··· ,aM }, ∀al ∈ A , l ∈ {1, · · · , M}. As
a result of Proposition 2, the upper bound of the number
of different convex polytopes is χ(K, M, N). Since the
polytopes are determined based on the user’s prior utility
rather than the defender’s, ˜vD is possibly discontinuous at
the boundaries of these polytopes.

Fig. 5: The defender’s expected posterior utility versus
prior belief p0
1 with and without the modulator in orange
and blue, respectively. We denote orange lines and nota-
tions in bold. The solid lines indicate that the defender’s
prior utility ˜vD is discontinuous and piece-wise linear
under three belief regions, i.e., [0,tθ1
1 , 1].
The dashed lines represent the defender’s optimal posterior
utility VD.

1 ], and [tθ2

1 ], [tθ1

1 ,tθ2

The defender’s expected posterior utility ¯vD is a function
of π ∈ Π and p0 ∈ ∆X . Thus, the defender’s optimal
posterior utility VD(p0) := supπ∈Π ¯vD(π, p0) is a function
of p0 ∈ ∆X . Based on Theorem 1, there exists an optimal
generator π ∗ ∈ Π that achieves the optimal posterior utility,
i.e., VD(p0) = ¯vD(π ∗, p0) = r. Denote the convex hull of
function ˜vD as co( ˜vD). Then, we can use the concaviﬁ-
cation technique introduced in [7], [8] to show that the
defender’s optimal posterior utility VD(p0) is the concave
closure of her prior utility ˜vD(p0) over the entire belief re-
gion p0 ∈ ∆X , i.e., VD(p0) = sup{z ∈ R|(p0, z) ∈ co( ˜vD)}.
We visualize the concaviﬁcation process under the bi-
nary state space N = 2 in Fig. 5. Suppose that there are

1Utility2413413<latexit sha1_base64="tPdfEyZMPK4/ja0N95FE1NvvTWg=">AAACIHicdVDLSgMxFM34rPVVdekmWAQXUiZF2roT3bisYKswHYdMmrbBzIPkjlCG+RQ3/oobF4roTr/GTNsBFT0QOJxzbnJz/FgKDbb9Yc3NLywuLZdWyqtr6xubla3tro4SxXiHRTJS1z7VXIqQd0CA5Nex4jTwJb/yb89y/+qOKy2i8BLGMXcDOgzFQDAKRvIqzbQ3ucRRQ99N7Zpt24SQw5yQZsM25Pi4VSetDDxyk/ZgxIF6JMu8SrUI4yKMizAmuWVQRTO0vcp7rx+xJOAhMEm1dogdg5tSBYJJnpV7ieYxZbd0yB1DQxpw7aaT3TK8b5Q+HkTKnBDwRP0+kdJA63Hgm2RAYaR/e7n4l+ckMGi5qQjjBHjIpg8NEokhwnlbuC8UZyDHhlCmhNkVsxFVlIHptGxKKH6K/yfdeo00avWLo+rJ6ayOEtpFe+gAEdREJ+gctVEHMXSPHtEzerEerCfr1XqbRues2cwO+gHr8wv3xp/Q</latexit>t✓11<latexit sha1_base64="MRemQBgNbHQ/RhaZ9e0/B2c3Pvo=">AAACIHicdVDLSgMxFM34rPVVdekmWAQXUiZF2roT3bisYKswHYdMmrbBzIPkjlCG+RQ3/oobF4roTr/GTNsBFT0QOJxzbnJz/FgKDbb9Yc3NLywuLZdWyqtr6xubla3tro4SxXiHRTJS1z7VXIqQd0CA5Nex4jTwJb/yb89y/+qOKy2i8BLGMXcDOgzFQDAKRvIqzbQ3ucRRQ99N7Zpt24SQw5yQZsM25Pi4VSetDDxyk/ZgxIF69SzzKtUijIswLsKY5JZBFc3Q9irvvX7EkoCHwCTV2iF2DG5KFQgmeVbuJZrHlN3SIXcMDWnAtZtOdsvwvlH6eBApc0LAE/X7REoDrceBb5IBhZH+7eXiX56TwKDlpiKME+Ahmz40SCSGCOdt4b5QnIEcG0KZEmZXzEZUUQam07Ipofgp/p906zXSqNUvjqonp7M6SmgX7aEDRFATnaBz1EYdxNA9ekTP6MV6sJ6sV+ttGp2zZjM76Aeszy/5TJ/R</latexit>t✓210<latexit sha1_base64="v7faQWrN6Lwo+a3I0mOFIOyq9hE=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMeiF48VTFtoY9lsN+3SzSbsToRS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O4W19Y3NreJ2aWd3b/+gfHjUNEmmGfdZIhPdDqnhUijuo0DJ26nmNA4lb4Wj25nfeuLaiEQ94DjlQUwHSkSCUbSSn/a8R7dXrrhVdw6ySrycVCBHo1f+6vYTlsVcIZPUmI7nphhMqEbBJJ+WupnhKWUjOuAdSxWNuQkm82On5MwqfRIl2pZCMld/T0xobMw4Dm1nTHFolr2Z+J/XyTC6DiZCpRlyxRaLokwSTMjsc9IXmjOUY0so08LeStiQasrQ5lOyIXjLL6+SZq3qXVZr9xeV+k0eRxFO4BTOwYMrqMMdNMAHBgKe4RXeHOW8OO/Ox6K14OQzx/AHzucPJUGOQQ==</latexit>p012𝒕𝟏𝜽𝟐𝒕𝟏𝜽𝟏1 where 0 < tθ1
1 ∈ [tθ2

two types of users and each type θ ∈ {θ1, θ2} has a single
1 < tθ2
belief threshold denoted by tθ
1 < 1.
Consider a common prior belief p0
1 , 1] denoted by
node 1’s abscissa. Then, the defender’s prior utility ˜vD(p0
1)
is denoted by node 1’s ordinate. The defender can improve
the utility from node 1’s ordinate to at most node 4’s
ordinate by adopting the optimal generator π ∗ ∈ Π as
follows. Generator π ∗ generates two signals s2 ∈ S and
s3 ∈ S with proper probabilities under different states
so that the user’s posterior belief is node 2’s abscissa
when observing policy s2 and node 3’s abscissa when
observing s3. Based on the Bayesian plausibility condition
in Section II-D, the defender’s optimal posterior utility
VD(p0
1) can be represented as the linear interpolation of
the ordinates of nodes 2 and 3, i.e., node 4’s ordinate.
The same reasoning applies to all feasible common prior
1] ∈ ∆X , the
beliefs p0
defender’s optimal posterior utility VD(p0) is the concave
closure of her prior utility ˜vD(p0) and VD(p0) ≥ ˜vD(p0).
Although we need at least |S | = KM security policies
to represent all the permutations of actions under differ-
ent types, Fig. 5 shows that the defender can achieve
her optimal posterior utility by generating two different
security policies with proper probabilities when N = 2.
Proposition 5 generalizes the result to N > 2 and shows
that the generator only needs to generate a small number
of security policies to achieve her optimal posterior utility.
If ˜vD(p0) = VD(p0) and p0 is further an interior point of
any convex polytope C
{a1,··· ,aM}, ∀al ∈ A , l ∈ {1, · · · , M},
then there exist inﬁnitely many credible generators that
achieve VD(p0).

1 ∈ [0, 1]. Therefore, for all [p0

1, 1 − p0

Proposition 5 (Efﬁciency of the Optimal Generator).
For any DG with common prior belief p0 ∈ ∆X , there
exist either one or inﬁnitely many optimal generators to
achieve the optimal posterior utility VD(p0). For each state
x ∈ X , there exists one optimal generator π ∗(·|x) ∈ ∆S
that generates at least KM − N security policies with zero
probability.

Proof. Since COP under the benchmark case is a linear
program, the optimal solution is either unique or innu-
merable. If N = 2, the convex hull consists of pieces of
line segments where each line segment can be determined
uniquely by its two endpoints. If N = 3, the convex hull as
a polygon consists of ﬁnite pieces of triangles where each
triangle can be determined uniquely by its three endpoints.
We can extend to any ﬁnite N where the convex hull
consists of pieces of (N − 1)-simplex where each piece
can be determined uniquely by N endpoints. Thus, for any
p0 ∈ ∆X , it requires at most N points to achieve VD(p0),
which corresponds to N distinct security policies.

Remark 2. Proposition 5 shows that the defender does not
need to apply all enforceable security policies to achieve

10

the optimal posterior utility; i.e., the optimal generator is
efﬁcient and generates at most N security policies for each
state x ∈ X .

We deﬁne the trust margin under a credible generator
π ∈ Π in Deﬁnition 7. The maximum trust margin is
achieved when the optimal generator π ∗ ∈ Π is applied.
The trust margin can be negative if generator π is not
well designed. However, the maximum trust margin is
non-negative as it is the difference between the defender’s
optimal posterior utility and prior utilities, i.e., VD(p0) −
˜vD(p0). Based on whether the maximum trust margin
is zero or positive, Deﬁnition 8 deﬁnes the user to be
unmanageable or manageable.

Deﬁnition 7 (Trust Margin). We deﬁne ¯vD(π, p0) −
˜vD(p0) as the trust margin under the common prior belief
p0 ∈ ∆X and a credible generator π ∈ Π.

Deﬁnition 8 (Manageability). The user is manageable
(resp. unmanageable) under prior belief p0 if the maximum
trust margin is greater than (resp. equals) zero.

Intuitively, a user is manageable if he shares the same
utility with the defender but unmanageable if he has an
D ∈ R to represent the
opposite utility. We introduce ρ s
user’s level of maliciousness. Theorem 2 investigates how
the user’s level of maliciousness affects his manageability.

Theorem 2 (Manageability and Level of maliciousness).
Let the common prior belief be state-independent, i.e.,
bD(θ |x) = ˆbD(θ ), ∀θ ∈ Θ, ∀x ∈ X , and two players’ utili-
ties be linearly dependent, i.e., there exist a scaling factor
D(x, θ ) ∈ R, such that
D ∈ R and translation factors ρt
ρ s
D(x, θ ), ∀x ∈ X , θ ∈ Θ, a ∈
ˆvD(x, θ , a) = ρ s
D ˆvU (x, θ , a) + ρt
A . Then, the following two statements hold.
(a) The defender’s trust margin is zero for all p0 ∈ ∆X
D ≤ 0. The

and credible generators if and only if ρ s
optimal generator contains zero information.

(b) The defender’s trust margin is non-negative for all
p0 ∈ ∆X and credible generators if and only if
ρ s
D > 0. Moreover, the optimal generator contains full
information. If p0 is an interior point of the (N − 1)-
simplex and there exists at least one θ ∈ Θ under
which no actions dominate, then the defender’s trust
margin is positive.

D ˆvU (x, θ , a∗

the given conditions,

˜vD(p0) = E
Proof. Under
θ ∼ˆbD
E
E
E
θ (p0)) + ρt
D(x, θ )] = ρ s
x∼p0[ρ s
θ ∼ˆbD
x∼p0
D
E
θ (p0))] + E
x∼p0[ρt
[ ˆvU (x, θ , a∗
D(x, θ )].
Proposition
θ ∼ˆbD
1 has shown that E
x∼p0[ ˆvU (x, θ , a∗
θ (p0))]
is a PWLC
function of p0 for each θ ∈ Θ. Since ˆbD(θ ) ≥ 0, ∀θ ∈ Θ,
combination E
θ (p0))]
the
is also PWLC. The term E
is a
θ ∼ˆbD
linear function of p0. Thus,
˜vD is a piece-wise linear
and concave (resp. linear) function of p0 if and only if

E
x∼p0[ ˆvU (x, θ , a∗
E
x∼p0[ρt
D(x, θ )]

linear

θ ∼ˆbD

D < 0 (resp. ρ s
ρ s
D = 0). If ˜vD is concave or linear over
the entire belief region ∆X , its convex hull is itself.
Thus, VD(p0) = ˜vD(p0) for all p0 ∈ ∆X and any zero-
information generator is optimal. Similarly, ˜vD is PWLC
if and only if ρ s
D > 0, and any full-information generator
is optimal. If there exists at least one θ ∈ Θ under which
no actions dominate, then ˜vD is strictly convex over the
entire belief region. Thus, we have VD(p0) < ˜vD(p0) when
p0 is an interior point of the (N − 1)-simplex.

Theorem 2 shows that when two players’ utilities are
linearly dependent, the user’s manageability depends on
the sign of the scaling factor ρ s
D rather than its value. Thus,
the user’s level of maliciousness has a threshold impact on
the manageability and the threshold is 0.

B. Incentive Modulator and Trust Manipulator

We illustrate the modulator design and the manipulator
design in Section IV-B1 and IV-B2, respectively. The
GMM mechanism design is presented in Section IV-B3.

1) Joint Design of Generator and Modulator: The
modulator incentivizes unmanageable users and increases
the security and efﬁciency of the networks. Under the
binary state N = 2, Fig. 5 illustrates the defender’s prior
utility with the modulator in orange solid lines. The
orange solid lines are different from the blue ones in
two folds. From the user’s perspective,
the modulator
changes the user’s expected utility under different ac-
tions and thus results in translations of the dashed lines
in Fig. 3. Those translations change the belief region
partition, e.g., the right shifts of tθ1
in Fig. 5.
1
From the defender’s perspective, the modulator modiﬁes
her utility in each new belief regions, and the value
x∼p0E
of the modiﬁcation is E
θ (p0))]. If the
defender’s belief is independent of state, i.e., bD(θ |x) =
ˆb(θ ), ∀θ ∈ Θ, ∀x ∈ X , then the defender’s utility change
E
θ (p0))] = γE
x∼p0E
is a
constant with respect to p0 in each new belief region.
When the state space is binary as shown in Fig. 5, it means
that designing c introduces translations but not rotations
to each segment of the function ˜vD.

θ ∼bD(·|x)[γc(a∗

θ ∼bD(·|x)[γc(a∗

θ ∼ˆbD(·)[c(a∗

and tθ2
1

θ (p0))]

The joint design of the modulator and the generator
results in the new convex hull denoted by the dashed
blue lines in Fig. 5. Based on both players’ perspectives,
the optimal design needs to strike a balance between
incentivizing users to change their belief region partitions
and the costs to provide the incentives. Take Fig. 5 as an
example, we observe that the modulator incurs costs to the
defender for all actions, i.e., c(a) ≤ 0, ∀a ∈ A . Thus, in
all three belief regions, the defender’s prior utilities with
the modulator, represented by the solid orange lines, are
lower than the ones without the modulator, represented
by the solid blue lines. However, the beneﬁt of the user’s
incentive change outweighs the costs; i.e., the defender’s

11

optimal posterior utility VD(p0
blue to node 4 in orange.

1) increases from node 4 in

2) Joint Design of Generator and Manipulator: The
manipulator directly distorts the user’s prior belief to elicit
desirable behaviors. When the generator cannot be de-
signed, the manipulator design is equivalent to the process
of ﬁnding the initial belief p0
g := arg maxp0∈∆X ˜vD(p0)
that achieves the global maximum of the prior utility ˜vD.
Proposition 6 proves the existence of the optimal distorted
belief p0
g.

Proposition 6. For any given ˆvD, ˆvU of two players, there
g ∈ ∆X at the boundary of the
exists an initial belief p0
{a1,··· ,aM }, ∀al ∈ A , l ∈ {1, · · · , M}, such
convex polytopes C
that p0

g = arg maxp0∈∆X ˜vD(p0).

Proof. For each ˆvD, ˆvU , the global maximum ˜vD(p0
g) =
maxp0∈∆X ˜vD(p0) exists and has a ﬁnite value due to The-
orem 1. Proposition 5 shows that the global maximum is
either unique or inﬁnite. In either case, at least one global
maximum is at the boundary of the convex polytopes
due to the piece-wise linear property stated in Proposition
4.

When the optimal generator is applied, the joint de-
sign of the manipulator and the generator is equiva-
¯p0
g :=
to the process of ﬁnding the initial belief
lent
arg maxp0∈∆X VD(p0) that achieves the global maximum
of VD. Based on the piece-wise linear property of ˜vD in
Proposition 4, the prior utility ˜vD and its concave closure
VD share the same global maximum. Thus, p0
g and
the optimal generator contains zero information. Take Fig.
5 as an example, p0
1 ] achieves the global
maximum denoted by node 2’s ordinate, and node 2 is
on both the solid and the dashed lines. These results are
summarized in Theorem 3.

1 , 1 − tθ2

g = [tθ2

g = ¯p0

Theorem 3. The design of optimal overt manipulator
changes the common initial belief p0 into p0
g. The de-
fender’s optimal posterior utility has the value of ˜vD(p0
g) =
g) and is independent of the initial belief p0 ∈ ∆X . In
VD(p0
the joint design of the overt manipulator and the generator,
the optimal generator contains zero information.

g = ¯p0

3) Design of the GMM Mechanism: We incorporate the
modulator design into the joint design of the generator
and the manipulator to complete the GMM mechanism
design. Based on the analysis in Section IV-B2, the ﬁrst
step of the GMM design is to determine the optimal
modulator c∗ ∈ C that results in the prior utility function
with the largest value of the global maximum, i.e., c∗ =
arg maxc[maxp0∈∆X ˜vD(p0)]. With the given modulator c∗,
the second step of the design is to reduce the problem to
the joint design of modulator and manipulator presented
in Section IV-B2.

Remark 3 (Separation Principle). The two-step design of
the GMM mechanism shows that the defender can design
the optimal modulator c∗ ∈ C independently.

We identify the equivalence principle in Remark 4 based
on the results in Theorem 3. If the overt manipulator allows
the defender to manipulate the initial belief arbitrarily, then
the optimal generator contains zero information; i.e., the
defender no longer needs the optimal generator to achieve
her optimal posterior utility. Note that the equivalence
principle does not mean that the generator is redundant.
When the belief manipulation is not arbitrary and under
practical constraints (e.g.,
the belief changes within a
limited range), the joint design of the two components
can yield better performance than the single design of the
manipulator.

Remark 4 (Equivalence Principle). For any given modu-
lator c ∈ C , the joint design of the generator and the overt
manipulator results in the same outcomes as the single
design of the overt manipulator does.

V. CASE STUDY

In Section V, we illustrate how the defender can use
the DG to mitigate insider threats where honeypots are
conﬁgured adaptively to detect and deter misbehavior.

A. Model Description

We have Θ = {θ b, θ g}, X = {xH , xN}, and A =
{aDO, aAC} based on the running example introduced in
Section II-A, Example 1, and Example 2. The true per-
centage of honeypots p0,H
D := b(xH ) ∈ [0, 1], is only known
to the SOC. Thus, the insiders’ perceived honeypot per-
centage p0,H
U := bU (xH |θ ) ∈ [0, 1], ∀θ ∈ Θ, can be different
from the true percentage.

Table III lists the utilities of the SOC and the insiders.
The column represents the binary state of a node, and
the row represents the insiders’ actions. In each matrix
entry, we list the payoffs resulting from the selﬁsh (resp.
adversarial) insiders on the left (resp. right) of the semi-
colon. When the insider chooses not to access a node,
we calibrate the payoffs to be 0 for both the SOC and
the insiders. The other four possible scenarios are listed
as follows. First, a selﬁsh insider’s access to a normal
server maintains the organization’s normal operation and
results in a positive reward rD > 0 (resp. rU > 0) on
the selﬁsh insider).
average to the organization (resp.
Second, when an adversarial insider accesses a normal
server, he disrupts the normal operation and compromises
conﬁdential data, which brings him a reward of φ N
U rU > 0
and incurs a security loss of φ N
D rD < 0 to the organization.
Third, if an adversarial insider accesses a honeypot, he is
detected and prohibited from data theft. Meanwhile, the
SOC obtains valuable threat intelligence. We use φ H
D > 0

12

and φ H
U < 0 to represent the degrees of the SOC’s gain
and the adversarial
insider’s loss, respectively. Finally,
once a selﬁsh insider accesses the honeypot, the SOC
has to quarantine the insider and investigate the incident,
which incurs a suspension of normal services as well as
an investigation cost. Meanwhile, the selﬁsh insider also
receives penalties and additional security training sessions.
We use φ g
U rU < 0 to represent the cost for
the SOC and the selﬁsh insider, respectively.

DrD < 0 and φ g

Selﬁsh θ g; Adversarial θ b
No Access aDO
Access aAC

Honeypot xH
0 ; 0

riφ g
i

; riφ H
i

Normal Server xN
0 ; 0
ri ; riφ N
i

TABLE III: Two players’ utilities vi(x, θ , a), i ∈ {D,U}.

Compared to a computing system that precisely fol-
lows its instructions, human insiders alter their behaviors
the
in response to (dis)incentives. In this case study,
(dis)incentives refer to the insider’s authentication cost
c(aAC) := rU φ 0 to access a node, where the ratio φ 0 ∈ R
takes the value of 0 in the default setting. We assume
that the SOC can increase (i.e., φ 0 < 0) or decrease (i.e.,
φ 0 > 0) an insider’s authentication cost at no additional
cost, i.e., γ = 0. The revenues, losses, and costs can be
quantiﬁed in dollars and their values vary for different
security scenarios.

1) Threshold Policy Analysis:

the selﬁsh and the adversarial

In this case study,
insiders share the same
both selﬁsh and adversarial
prior belief p0,H
U ∈ [0, 1]. Hence they share the same
posterior belief denoted by pH
U ∈ [0, 1] and adopt
the following threshold policies. Deﬁne the decision
thresholds of
insid-
ers as tg(φ 0) := max{min{(1 − φ 0)/(1 − φ g
U ), 1}, 0} and
U − φ 0)/(φ N
tb(φ 0) := max{min{(φ N
U ), 1}, 0}, respec-
tively. Since both denominators are positive, i.e., 1 − φ g
U >
1 and φ N
U > 0, the selﬁsh insider (resp. the adversarial
insider) chooses to access a node if and only if the node
is unlikely to be a honeypot, i.e., pH
U <
tb(φ 0)). If a selﬁsh (resp. adversarial) insider accesses a
D (φ g
node, his expected utility rU (1 − φ 0 + p0,H
U − 1)) (resp.
U ))) decreases linearly in p0,H
rU (φ N
D (φ H
D ,
i.e., the true percentage of honeypots.

U < tg(φ 0) (resp. pH

U − φ 0 + p0,H

U − φ H

U − φ N

U −φ H

Since the selﬁsh and adversarial insiders share the same
insider information, the difference in their decision thresh-
olds results purely from their incentive misalignment.
Given the insiders’ utility matrices, the SOC can change
their incentives and elicit desirable behaviors by a proper
design of the authentication cost determined by the ratio
φ 0. If φ 0 ≤ φ g
U < 0), then the selﬁsh
(resp. adversarial) insider chooses aAC for all security
scenarios. If φ 0 ≥ 1 (resp. φ 0 ≥ φ N
U > 0), then the selﬁsh
(resp. adversarial) insider chooses aDO for all security
scenarios. Since the deceptive honeypot conﬁguration can
possibly change insiders’ behaviors only if φ 0 is in the

U < 0 (resp. φ 0 ≤ φ H

U , φ H

U ), max(1, φ N

region [min(φ g
U )], we refer to the region
as the incentivized region of φ 0. As a special case of
Proposition 2, Corollary 1 shows that security policies
s{aDO,aAC} and s{aAC,aDO} cannot be both enforceable for
any node in the corporate network.
Corollary 1. If φ g
U < 0, then for all φ 0 ∈ R
U > 0, φ H
and credible conﬁguration π ∈ Π, either π(s{aDO,aAC}|x) =
0, ∀x ∈ {xH , xN}, or π(s{aAC,aDO}|x) = 0, ∀x ∈ {xH , xN}.

U < 0,φ N

B. Numerical Results

D = −1, and φ N

Following the insider categorization in Section II-A1,
we re-weight the percentage from the VCDB and adopt
qg := bD(θ g|x) = 0.32 and qb := bD(θ b|x) = 0.68 for all
x ∈ {xN, xH } as the benchmark value of the insiders’ type
statistics. Based on the analysis in Section V-A1, the values
of rU do not affect the insiders’ actions, and the value
of rD only scales the SOC’s utility by a constant. Thus,
we normalize rU = rD = 1. We consider φ g
U = φ g
D = −0.3,
φ H
U = −φ N
U = −φ H
D = 0.9 as the benchmark
values. Then, the selﬁsh insider has the same utility as the
SOC, i.e., vD(x, θ g, a) = vU (x, θ g, a), ∀x ∈ {xH , xN}, ∀a ∈
{aAC, aDO}, while the adversarial insider has an exactly
opposite utility to the one of the SOC, i.e., vD(x, θ b, a) =
−vU (x, θ b, a), ∀x ∈ {xH , xN}, ∀a ∈ {aAC, aDO}. In Section
V-B1, the SOC cannot change the authentication cost, i.e.,
c(aAC) = 0. In Sections V-B1 and V-B2, the insider has
the correct prior belief of the honeypot percentage, i.e.,
U = p0,H
p0,H
D .
1) Security Posture under the Optimal Generator:
Fig. 6a shows how the SOC’s normalized revenue ˜vD
without the optimal generator is affected by the percent-
ages of honeypots and the selﬁsh insiders, respectively.
The maximum (resp. minimum) value of ˜vD is achieved
when insiders are all selﬁsh (resp. adversarial) and no
honeypots are applied. The two decision thresholds tb(0)
and tg(0) divide the percentage of honeypots into three
regions, i.e., high, medium, and low, in which the insiders’
behaviors and the SOC’s normalized revenue ˜vD have
different characteristics.

If the intended security outcomes are not achieved due
to the insiders’ misbehavior, the SOC can apply the opti-
mal generator to elicit desirable behaviors and reduce the
cyber risks of the organization. To illustrate the effective-
ness of the optimal generator, we plot the maximum trust
margin in Fig. 6b. Fig. 6b corroborates Theorem 2; i.e.,
when all insiders are adversarial (resp. selﬁsh), no (resp.
all) credible generators, including the optimal one, can
improve the SOC’s normalized revenue for any percentage
of honeypots p0,H
D ∈ [0, 1]. The ﬂat region represented
D − φ H
by qg ∈ [0, (φ N
D ∈
[0, min(tb(0),tg(0))] identiﬁes two critical thresholds. On
the one hand, we refer to (φ N
D − φ H
D )
as the insider’s motive threshold that is used to quantify

D )] and p0,H

D − 1 + φ N

D − 1 + φ N

D )/(φ g

D )/(φ g

D − φ H

D − φ H

13

(a) Prior utility ˜vD.

(b) Maximum trust margin.

Fig. 6: SOC’s utilities vs. p0,H

D ∈ [0, 1] and qg ∈ [0, 1].

the average motive of the entire insider population. If
the percentage of adversarial insiders exceeds the mo-
tive threshold, then insiders’ behaviors are on average
destructive to the organization. On the other hand, we
refer to min(tb(0),tg(0)) as the deterrence threshold that
measures the adequacy of the honeypots. If the percentage
of honeypots is below the deterrence threshold, then the
SOC does not have a sufﬁcient number of honeypots
to create a credible threat for the insiders not to access
nodes in the corporate network. Based on Deﬁnition 8,
the insiders are unmanageable in the ﬂat region.

For the other regions, the insiders are manageable, and
the optimal generator can effectively reduce the cyber
risk of the organization. The increase depends on the
percentage of selﬁsh insiders and honeypots. When the
percentage of honeypots is tg(0) and insiders are all selﬁsh,
the organization’s revenue with the optimal generator
is 114 times higher than the one without the optimal
generator. Averaged over the entire region of qg ∈ [0, 1]
and p0,H
the organization’s revenue with the
optimal generator is 35.6% higher than the one without the
optimal generator. The results in Fig. 6 demonstrate that
the optimal generator design provides a constructive way
to quantify the accuracy of the information that the SOC
should reveal to the insiders to establish trust with them,

D ∈ [0, 1],

while in the meantime, retain her information advantage
to elicit desirable insider behaviors and maximize the or-
ganization’s well-being. These results provide a guideline
to address the challenges identiﬁed in 2c and 2d of Table
2 in [31].

2) Security Posture under Various Modulators: In Sec-
tion V-B2, we investigate how the (dis)incentives affect the
insiders’ behaviors and the security posture of the insider
network. In Fig. 7, we plot the decision thresholds of
selﬁsh and adversarial insiders in blue and red, respec-
tively. Since the blue line has a steeper slope than the
red line, Fig. 7 demonstrates that the same authentication
cost affects the selﬁsh insiders more signiﬁcantly than the
adversarial ones. As deﬁned in Deﬁnition 5, two types
of insiders are identiﬁable under posterior belief pH
U if
pH
U ∈ [tb(φ 0),tg(φ 0)]. Furthermore, a larger difference in
the two thresholds, i.e., tg(φ 0) −tb(φ 0), indicates a higher
incentive misalignment between selﬁsh and adversarial
insiders.

Fig. 7: The adversarial and the selﬁsh insiders’ decision
thresholds tb(φ 0) and tg(φ 0) in the red dashed line and the
blue solid line, respectively. The difference tg(φ 0)−tb(φ 0)
denoted in the black dotted line represents their utility
misalignment.

Fig. 8a illustrates the organization’s original payoff ˜vD
without a generator. The selﬁsh insider and the SOC
achieve a win-win situation at the region φ 0 ∈ [0.5, 0.74]
as they both achieve their maximum payoffs at that region.
The adversarial insider and the SOC cannot achieve a win-
win situation for all φ 0 ∈ R as adversarial insiders seeking
to compromise sensitive data and sabotage the organization
have a completely misaligned payoff structure. Fig. 8b
illustrates the organization’s improved payoff VD when
the optimal generator is applied. The results show that
the optimal generator can always increase the payoffs of
the selﬁsh insiders and the organization regardless of the
(dis)incentives represented by φ 0 ∈ R. Win-win situations
still exist (resp. do not exist) for the SOC and the selﬁsh
(resp. adversarial) insider.

3) Security Posture under the Covert and Overt Trust
In Section V-B3, the SOC can generate

Manipulators:

14

(a) Players’ prior utilities.

(b) Optimal posterior utilities.

Fig. 8: Utilities of the SOC, selﬁsh insiders, and adver-
sarial insiders in the dotted black, the solid blue, and the
dashed red lines, respectively.

and p0,H

U (cid:54)= p0,H

ambiguous or fake reports of the honeypot percentage so
that the insiders’ initial beliefs of the honeypot percentage
deviate from the truth, i.e., p0,H
D . Figs. 9a and
the
9b illustrate the SOC’s payoffs with and without
optimal generator, respectively, under different values of
p0,H
D . In Fig. 9a, the insiders’ initial beliefs fall
U
into the following three regions. If p0,H
U ∈ [tg(0), 1], both
types of insiders choose not to access the node. Then,
the SOC’s normalized payoff ˜vD is zero regardless of the
true percentage of honeypots p0,H
U ∈ [tb(0),tg(0)],
selﬁsh insiders choose aAC and adversarial insiders choose
aDO. Then, reducing the percentage of honeypots increases
the SOC’s normalized payoff ˜vD as it reduces the false
alarm rate when selﬁsh insiders access the honeypots. If
p0,H
U ∈ [0,tb(0)], both types of insiders choose to access
the node. Then, reducing the percentage of honeypots also
increases the SOC’s normalized payoff ˜vD. However, the
increase rate is lower than the one in the second region as
the two types of insiders take the same action and are not
identiﬁable.

D . If p0,H

D , p0,H

These results illustrate that without a deceptive genera-
tor, the SOC may not always beneﬁt from faking the per-
centage of honeypots. On the contrary, when the optimal
generator is applied in Fig. 9b, the SOC can beneﬁt from
a fake percentage of honeypots for all p0,H
U ∈ [0, 1].
Moreover, the beneﬁt of faking honeypot percentage is a
non-decreasing function of |p0,H
U |. Thus, the SOC
obtains a higher payoff VD with the optimal generator
when there is a larger mismatch between the true and
the fake percentages of honeypots. The maximum value
of VD is achieved when the true percentage of honeypots
is zero and the SOC makes the insiders believe that the
percentage of honeypots exceeds tb(0). Averaged over the
true percentage p0,H
U ∈ [0, 1],
the SOC’s payoff with the optimal generator, i.e., VD is
59.3% higher than her original payoff ˜vD.

D ∈ [0, 1] and the fake one p0,H

D − p0,H

-1-0.500.51Value of 000.20.40.60.81Decision Thresholds-1-0.500.51Value of 000.20.40.60.8Prior Utility-1-0.500.51Value of 000.20.40.60.8Optimal Posterior Utility15

design of the generator and manipulator into one single
design of the manipulator. We have applied the DG to
a case study where the defender dynamically conﬁgures
the honeypot to mitigate insider threats in a corporate
network. The numerical results have shown that the GMM
mechanism manages to elicit desirable actions from both
selﬁsh and adversarial insiders and reduce the cyber risk
of the organization. In particular, the optimal generator
itself can increase the defender’s payoff by 35.6% on
average. Equipped with the trust manipulator that fakes
the honeypot percentage, the optimal generator can further
increase the defender’s payoff by 59.3% on average.

REFERENCES

[1] S. Jajodia, A. K. Ghosh, V. Swarup, C. Wang, and X. S. Wang,
Moving target defense: creating asymmetric uncertainty for cyber
threats. Springer Science & Business Media, 2011, vol. 54.
[2] M. Bringer, C. Chelmecki, and H. Fujinoki, “A survey: Recent
advances and future trends in honeypot research,” International
Journal of Computer Network and Information Security, vol. 4,
no. 10, p. 63, 2012.

[3] E. Al-Shaer, J. Wei, W. Kevin, and C. Wang, Autonomous Cyber

Deception. Springer, 2019.

[4] “Cyber deception signiﬁcantly reduces data breach costs & im-
proves soc efﬁciency,” DECEPTIVE DEFENSE, INC., Tech. Rep.,
08 2020.

[5] S. Harris, “Insider threat mitigation guide,” Cybersecurity and

Infrastructure Security Agency, Tech. Rep.

[6] L. Spitzner, “Honeypots: Catching the insider threat,” in 19th
Annual Computer Security Applications Conference, 2003. Pro-
ceedings.

IEEE, 2003, pp. 170–179.

[7] R. J. Aumann, M. Maschler, and R. E. Stearns, Repeated games

with incomplete information. MIT press, 1995.

[8] E. Kamenica and M. Gentzkow, “Bayesian persuasion,” American

Economic Review, vol. 101, no. 6, pp. 2590–2615, 2011.

[9] Y. Zhao, L. Huang, C. Smidts, and Q. Zhu, “Finite-horizon semi-
markov game for time-sensitive attack response and probabilistic
risk assessment in nuclear power plants,” Reliability Engineering
& System Safety, p. 106878, 2020.

[10] M. H. Manshaei, Q. Zhu, T. Alpcan, T. Bacs¸ar, and J.-P. Hubaux,
“Game theory meets network security and privacy,” ACM Comput-
ing Surveys (CSUR), vol. 45, no. 3, pp. 1–39, 2013.

[11] L. Huang, J. Chen, and Q. Zhu, “A large-scale markov game
approach to dynamic protection of interdependent infrastructure
networks,” in International Conference on Decision and Game
Theory for Security. Springer, 2017, pp. 357–376.

[12] J. Pawlick and Q. Zhu, Game Theory for Cyber Deception: From

Theory to Applications. Springer Nature, 2021.

[13] J. Pawlick, E. Colbert, and Q. Zhu, “Modeling and analysis of leaky
deception using signaling games with evidence,” IEEE Transactions
on Information Forensics and Security, vol. 14, no. 7, pp. 1871–
1886, 2018.

[14] H. Sasahara and H. Sandberg, “Epistemic signaling games for cyber
deception with asymmetric recognition,” IEEE Control Systems
Letters, vol. 6, pp. 854–859, 2022.

[15] L. Huang and Q. Zhu, “A dynamic games approach to proactive
threats in cyber-

defense strategies against advanced persistent
physical systems,” Comput. & Secur., vol. 89, p. 101660, 2020.

[16] ——, “A dynamic game framework for rational and persistent robot
deception with an application to deceptive pursuit-evasion,” IEEE
Transactions on Automation Science and Engineering, pp. 1–15,
2021.

[17] X. Feng, Z. Zheng, D. Cansever, A. Swami, and P. Mohapatra,
“A signaling game model for moving target defense,” in IEEE
conference on computer communications.

IEEE, 2017, pp. 1–9.

(a) Prior utility ˜vD.

(b) Optimal posterior utility.

Fig. 9: SOC’s utilities vs. p0,H

D ∈ [0, 1] and p0,H

U ∈ [0, 1].

VI. CONCLUSION

In this work, we have presented a class of duplicity
games (DG) to design defensive deception mechanisms
for proactive network security. The deception mechanism
is referred to as the GMM mechanism as it consists of the
following three modular design components. The generator
provides users an appropriate amount of information to
procure different types of users to take actions that are fa-
vorable to the defender. The incentive modulator modiﬁes
the users’ utilities to make their incentives better aligned
with the defender’s. The trust manipulator makes use of
users’ trust to impart to them the initial beliefs that can
lead to desirable security outcomes.

We have formulated and analyzed the DG using math-
ematical programming and graphical approaches. It has
the defender requires at most N en-
been shown that
forceable security policies from the entire KM ones to
achieve the optimal security posture, which illustrates the
efﬁciency of the GMM mechanism. We have proposed the
concept of trust margin to measure how difﬁcult it is for a
defender to elicit the desired behavioral outcome. A user is
unmanageable when the maximum trust margin is zero, as
no deceptive mechanisms can affect the user’s behaviors.
We have identiﬁed a separation principle for the modulator
design and an equivalence principle that turns the joint

[18] E. Cranford, C. Lebiere, C. Gonzalez, S. Cooney, P. Vayanos, and
M. Tambe, “Learning about cyber deception through simulations:
Predictions of human decision making with deceptive signals in
stackelberg security games.” in CogSci, 2018.

[19] H. Xu, R. Freeman, V. Conitzer, S. Dughmi, and M. Tambe,
“Signaling in bayesian stackelberg games.” in AAMAS, 2016, pp.
150–158.

[20] K. Hor´ak, Q. Zhu, and B. Boˇsansk`y, “Manipulating adversary’s
belief: A dynamic game approach to deception by design for
proactive network security,” in GameSec, 2017, pp. 273–294.
[21] P. Naghizadeh and M. Liu, “Opting out of incentive mechanisms:
A study of security as a non-excludable public good,” IEEE
Transactions on Information Forensics and Security, vol. 11, no. 12,
pp. 2790–2803, 2016.

[22] Y. Zhang, H. Zhang, S. Tang, and S. Zhong, “Designing secure and
dependable mobile sensing mechanisms with revenue guarantees,”
IEEE Transactions on Information Forensics and Security, vol. 11,
no. 1, pp. 100–113, 2016.

[23] J. Lu, Y. Xin, Z. Zhang, X. Liu, and K. Li, “Game-theoretic design
of optimal two-sided rating protocols for service exchange dilemma
in crowdsourcing,” IEEE Transactions on Information Forensics
and Security, vol. 13, no. 11, pp. 2801–2815, 2018.

[24] C. Jiang, Y. Chen, Q. Wang, and K. R. Liu, “Data-driven auction
mechanism design in iaas cloud computing,” IEEE Transactions on
Services Computing, vol. 11, no. 5, pp. 743–756, 2018.

[25] Z. Zhang, S. He, J. Chen, and J. Zhang, “Reap: An efﬁcient
incentive mechanism for reconciling aggregation accuracy and indi-
vidual privacy in crowdsensing,” IEEE Transactions on Information
Forensics and Security, vol. 13, no. 12, pp. 2995–3007, 2018.
[26] R. Zhang and Q. Zhu, “FlipIn : A game-theoretic cyber insurance
framework for incentive-compatible cyber risk management of
internet of things,” IEEE Transactions on Information Forensics
and Security, vol. 15, pp. 2026–2041, 2020.

[27] J. Chen and Q. Zhu, “Security as a service for cloud-enabled
internet of controlled things under advanced persistent
threats:
a contract design approach,” IEEE Transactions on Information
Forensics and Security, vol. 12, no. 11, pp. 2736–2750, 2017.
[28] Z. Rabinovich, A. X. Jiang, M. Jain, and H. Xu, “Information
disclosure as a means to security,” in Proceedings of the 2015
International Conference on Autonomous Agents and Multiagent
Systems. Citeseer, 2015, pp. 645–653.

[29] S. Das, E. Kamenica, and R. Mirka, “Reducing congestion through
information design,” in 2017 55th annual allerton conference on
communication, control, and computing (allerton).
IEEE, 2017,
pp. 1279–1284.

[30] K. Hor´ak, B. Boˇsansk´y, P. Tom´aˇsek, C. Kiekintveld, and
C. Kamhoua, “Optimizing honeypot strategies against dynamic
lateral movement using partially observable stochastic games,”
Computers & Security, vol. 87, p. 101579, 2019.

[31] A. P. Moore, W. Novak, M. Collins, R. Trzeciak, and M. Theis,
“Effective insider threat programs: understanding and avoiding
potential pitfalls,” Software Engineering Institute White Paper,
Pittsburgh, 2015.

[32] I. Kantzavelou and S. Katsikas, “A game-based intrusion detection
mechanism to confront internal attackers,” Computers & Security,
vol. 29, no. 8, pp. 859–874, 2010.

[33] “Game-theoretic modeling and analysis of insider threats,” Inter-
national Journal of Critical Infrastructure Protection, vol. 1, pp.
75–80, 2008.

[34] C. Joshi, J. R. Aliaga, and D. R. Insua, “Insider threat modeling:
An adversarial risk analysis approach,” IEEE Transactions on
Information Forensics and Security, vol. 16, pp. 1131–1142, 2021.
[35] W. A. Casey, Q. Zhu, J. A. Morales, and B. Mishra, “Compli-
ance control: Managed vulnerability surface in social-technological
systems via signaling games,” in Proceedings of the 7th ACM
CCS International Workshop on Managing Insider Security Threats,
2015, pp. 53–62.

[36] M. M. Yamin, B. Katt, K. Sattar, and M. B. Ahmad, “Implementa-
tion of insider threat detection system using honeypot based sensors
and threat analytics,” in Future of Information and Communication
Conference. Springer, 2019, pp. 801–829.

16

[37] R. Dahbul, C. Lim, and J. Purnama, “Enhancing honeypot deception
capability through network service ﬁngerprinting,” in Journal of
Physics: Conference Series, vol. 801, no. 1.
IOP Publishing, 2017,
p. 012057.

[38] S. Morishita, T. Hoizumi, W. Ueno, R. Tanabe, C. Ga˜n´an, M. J.
van Eeten, K. Yoshioka, and T. Matsumoto, “Detect me if you. . .
oh wait. an internet-wide view of self-revealing honeypots,” in
2019 IFIP/IEEE Symposium on Integrated Network and Service
Management (IM).
IEEE, 2019, pp. 134–143.

[39] Verizon.

(2017) Vocabulary for event

recording and incident

sharing (veris). [Online]. Available: http://veriscommunity.net/
[40] L. Shi, Y. Li, T. Liu, J. Liu, B. Shan, and H. Chen, “Dynamic
distributed honeypot based on blockchain,” IEEE Access, vol. 7,
pp. 72 234–72 246, 2019.

[41] G. Wagener, R. State, T. Engel, and A. Dulaunoy, “Adaptive
and self-conﬁgurable honeypots,” in 12th IFIP/IEEE International
Symposium on Integrated Network Management (IM 2011) and
Workshops.

IEEE, 2011, pp. 345–352.

[42] L. Huang and Q. Zhu, “Adaptive honeypot engagement through
reinforcement learning of semi-markov decision processes,” in In-
ternational Conference on Decision and Game Theory for Security.
Springer, 2019, pp. 196–216.

[43] N. C. Rowe, E. J. Custy, and B. T. Duong, “Defending cyberspace
with fake honeypots.” JCP, vol. 2, no. 2, pp. 25–36, 2007.

[44] P. Orlik and H. Terao, Arrangements of hyperplanes.

Springer

Science & Business Media, 2013, vol. 300.

Linan Huang (S’16) received the B.Eng. de-
gree (Hons.) in Electrical Engineering from
Beijing Institute of Technology, China, in 2016.
He is currently pursuing a Ph.D. degree at the
Laboratory for Agile and Resilient Complex
Systems, Tandon School of Engineering, New
York University, NY, USA. His research inter-
ests include dynamic decision-making of the
multi-agent system, mechanism design, artiﬁ-
cial intelligence, security, and resilience for the
cyber-physical systems.

Quanyan Zhu (SM’02-M’14) received B. Eng.
in Honors Electrical Engineering from McGill
University in 2006, M. A. Sc. from the Uni-
versity of Toronto in 2008, and Ph.D. from
the University of Illinois at Urbana-Champaign
(UIUC) in 2013. After stints at Princeton Uni-
versity, he is currently an associate professor
at the Department of Electrical and Computer
Engineering, New York University (NYU). He
is an afﬁliated faculty member of the Center
for Urban Science and Progress (CUSP) and
Center for Cyber Security (CCS) at NYU. His current research interests
include game theory, machine learning, cyber deception, and cyber-
physical systems.

