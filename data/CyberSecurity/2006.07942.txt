Duplicity Games for Deception Design with an
Application to Insider Threat Mitigation

Linan Huang, Student Member, IEEE, and Quanyan Zhu, Member, IEEE

1

1
2
0
2

t
c
O
4
1

]
T
G
.
s
c
[

3
v
2
4
9
7
0
.
6
0
0
2
:
v
i
X
r
a

Abstractâ€”Recent incidents such as the Colonial Pipeline
ransomware attack and the SolarWinds hack have shown
that traditional defense techniques are becoming insufï¬cient
to deter adversaries of growing sophistication. Proactive and
deceptive defenses are an emerging class of methods to defend
against zero-day and advanced attacks. This work develops
a new game-theoretic framework called the duplicity game
to design deception mechanisms that consist of a generator,
an incentive modulator, and a trust manipulator, referred
to as the GMM mechanism. We formulate a mathematical
programming problem to compute the optimal GMM mecha-
nism, quantify the upper limit of enforceable security policies,
and characterize conditions on userâ€™s identiï¬ability and
manageability for cyber attribution and user management.
We develop a separation principle that decouples the design
of the modulator from the GMM mechanism and an equiv-
alence principle that turns the joint design of the generator
and the manipulator into the single design of the manipulator.
A case study of dynamic honeypot conï¬gurations is pre-
sented to mitigate insider threats. The numerical experiments
corroborate the results that the optimal GMM mechanism
can elicit desirable actions from both selï¬sh and adversarial
insiders and consequently improve the security posture of
the insider network. In particular, a proper modulator can
reduce the incentive misalignment between the players and
achieve win-win situations for the selï¬sh insider and the
defender. Meanwhile, we observe that the defender always
beneï¬ts from faking the percentage of honeypots when the
optimal generator is presented.

Index Termsâ€”Bayesian persuasion, proactive defense,
insider threat, cyber deception, cyber

mechanism design,
attribution, cyber trust, incentive mechanism

I. INTRODUCTION

C YBER deception is an emerging proactive defense

technique against increasingly sophisticated attacks,
insider
including Advanced Persistent Threats (APTs),
threats, and supply chain attacks. Defensive deception
technologies, such as Moving Target Defense (MTD) [1]
and honeypots [2], create uncertainties and misinformation

This paper has been accepted for publication in IEEE Transactions on

Information Forensics and Security

This work is partially supported by grants SES-1541164, ECCS-
1847056, CNS-2027884, and BCS-2122060 from National Science Foun-
dation (NSF), DOE-NE grant 20-19829 and grant W911NF-19-1-0041
from Army Research Ofï¬ce (ARO).

L. Huang and Q. Zhu are with the Department of Electrical and
Computer Engineering, New York University, Brooklyn, NY, 11201,
USA. E-mail:{lh2328,qz494}@nyu.edu

Digital Object Identiï¬er 10.1109/TIFS.2021.3118886

for adversaries to misdirect their perception and decision
processes [3]. Among many success stories from industry,
it has been shown in [4] that deception technology has
successfully reduced the data breach costs by 51.4% and
per analyst costs by 32% for the Security Operations
Center (SOC). An important application of cyber decep-
tion is to defend systems from insider threats. Harmful
behaviors of inadvertent insiders or insiders with mali-
cious intentions can lead to compromises of sensitive data
and disruptions in the organizationâ€™s normal operations
[5]. Deception technologies provide promising proactive
solutions to detect unwarranted behaviors and deter the
insiders from wrongdoing, e.g., [6].

The design of successful defensive deception relies on
a formal approach that quantiï¬es the strategic interactions
including a defender,
of the three classes of players,
users, and adversaries. A useful framework to design
cyber deception mechanisms needs to capture three main
features. First, the defender, the users, and the adversaries
are strategic players with clear but imperfectly aligned
objectives or incentives. Second, the defender cannot dis-
tinguish adversaries from the normal users. For example,
the defender does not know who is an adversarial insider
when designing a security policy for the network. Apart
from this, the defender cannot distinguish the type of
users in the network concerning their objectives, resources,
and trust values. Third, a sophisticated adversary behaves
stealthily and intelligently, e.g., by conducting successful
reconnaissance or acting like a normal user to gain access
or trust.

In this work, we propose Duplicity Games (DG) as a
mechanism design framework for defensive deception to
elicit desirable security outcomes when a defender, normal
users, and adversaries interact to attain their individual
objectives. A DG is a two-stage game between a defender
and a normal/adversarial user with two-sided asymmetric
information. The defender, or the defensive deceiver, has
private information of the system state. The user has a
private type, which characterizes the userâ€™s objectives,
trustworthiness, and attributes, e.g., normal or adversarial.
the defender designs
At
three composable components of the mechanism, i.e., a
generator, an incentive modulator, and a trust manipu-
lator. The generator is a mechanism that stochastically

the ï¬rst stage of the game,

Copyright Â© 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes,creating new collective works, for resale or redistribution to servers or lists, or reuse of any
copyrighted component of this work in other works.

 
 
 
 
 
 
generates signals or security policies based on the systemâ€™s
private information and system constraints. The modulator
reshapes the userâ€™s incentive by creating constrained utility
transfers between two players. The manipulator distorts
the userâ€™s prior belief over the unknowns. These three
components are together referred to as the GMM mecha-
nism. After the mechanism is designed and implemented,
the user observes the security policies, updates his trust
through the Bayesian rule, and then responds to the GMM
mechanism by taking an action that serves his objective.
The optimal design of the GMM mechanisms anticipates
the behaviors of different types of users under a given set
of security policies and elicits desirable security behaviors.
The GMM mechanisms we introduce here represent a class
of multi-dimensional security mechanisms that control
the security policies, the (dis)incentives, and the digital
footprints (e.g., feature patterns and conï¬gurations of
honeypots and normal servers).

We formulate the design problem into a mathematical
programming problem, where the anticipated behavioral
outcomes of the users follow the Incentive-Compatible
(IC) constraint and the Modulation-Feasible (MF) con-
straint. We use concaviï¬cation techniques as in [7], [8]
to provide a graphical analysis and interpretation of the
GMM mechanism. We observe that the userâ€™s expected
posterior utility can be fully characterized by Piece-Wise
Linear and Convex (PWLC) functions. This observation
leads to a signiï¬cantly reduced number of enforceable
security policies and enables an efï¬cient implementation
of the GMM mechanism. Finally, we show a fundamental
separation principle in which the defender can design
the modulator independently, and an equivalence principle
where the joint design of the generator and the manipulator
is equivalent to the single design of the manipulator.

For further elaboration, we use the DG framework to
study insider threats and design mitigation strategies to
deter and prevent misbehavior in corporate networks. The
corporate network defender can adaptively conï¬gure hon-
eypots and normal servers to counter ï¬ngerprinting (i.e.,
generator), modify the complexity of the authentication
process to change userâ€™s incentives (i.e., modulator), and
misreport the percentage of honeypots to make use of
the userâ€™s trust (i.e., manipulator). The design of the
GMM mechanism leads to a set of multi-faceted socio-
technical solutions for insider threats, which formalizes the
management guidelines for insider threats recommended in
[5]. From the generator design, we propose the concept of
the motive threshold to assess the average motive of the
entire insider population and the concept of the deterrence
threshold to measure the adequacy of the honeypots. From
the modulator design, we illustrate how the proper design
of the authentication cost can reduce the misalignment
between the insidersâ€™ and the defenderâ€™s incentives. From
the manipulator design, we ï¬nd that the manipulation of

2

the insidersâ€™ initial beliefs can harm the defender when
there are no deceptive generators, but create an advantage
when the optimal generator is applied.

A. Related Work

1) Game Theory for Cyber Deception: Game theory
has been widely applied for proactive defense and cy-
ber deception to enhance the security of cyber-physical
systems [9]â€“[12]. Games of incomplete information pro-
vide a natural paradigm to quantify the uncertainty and
misinformation induced by the deception. Exemplary
game models include signaling games [13], [14], dynamic
Bayesian games with ï¬nite [15] or inï¬nite states [16],
(Bayesian) Stackelberg security games [17]â€“[19], and par-
tially observable stochastic games [20]. These incomplete-
information games focus on ï¬nding signals and behaviors
at the equilibrium for a given mechanism and information
structure. In this work, we further aim to design the
mechanism and exploit the information asymmetry, which
proactively enhances cyber security.

2) Incentive Mechanisms and Information Design in
Cyber Systems: There is rich literature on incentive mech-
anisms designed to enhance security [21], [22], efï¬ciency
[23], [24], privacy [25] of cyber-physical systems. They
are applied to wide applications, including crowdsourcing
[23], [25], mobile sensing [22], cloud computing [24],
cyber insurance [26], and security as service [21], [27].
These incentive mechanisms mainly focus on designing
the payoff rules and the allocation rules to incentivize
participantsâ€™ behaviors in the designerâ€™s favor. Besides
incentive design, previous works have also investigated
information design by disclosing information strategically,
which has been applied to wildlife protection [28], con-
gestion mitigation [29], and honeypot conï¬guration [30].
DGs broaden the scope of these two classes of mechanisms
to the joint design of information, incentive, and trust to
achieve desirable equilibrium outcomes.

3) Insider Threat Mitigation and Incentive Design:
Previous works, e.g., [5], [31], have proposed guidelines
to establish effective inside threat mitigation programs.
Game-theoretic models have been developed to detect
insider threats [32] and identify the best response strategy
[33]. The recent work [34] has incorporated organizational
culture and the existing defensive mechanisms into the
game model. These works provide a quantitative under-
standing of insider threats but overlook the human aspects,
such as compliance and incentives, which are fundamental
and challenging problems for insider threat mitigation.
The authors in [35] have used signaling games to model
compliant and non-compliant insiders and adopted a feed-
back loop to control their compliance. This work uses
honeypots as a way to detect and monitor the misbehavior
of the insiders and aims to formalize the design of such
guidelines, e.g., detection, incentives, and penalties.

TABLE I: Summary of notations for DG-GMM.

Variable
N, M, K
b(Â·) âˆˆ âˆ†X
bU (Â·|Î¸ ) âˆˆ âˆ†X
bD(Â·|x) âˆˆ âˆ†Î˜
p0 := [p0
1, Â· Â· Â· , p0
N ]
p := [p1, Â· Â· Â· , pN ]
U ) or aâˆ—
aâˆ—
Î¸ (bÏ€
Î¸ (p)

Â¯vD(Ï€, p0)

ËœvD(p0) = Â¯vD(Ï€ 0, p0)

VD(p0) = Â¯vD(Ï€ âˆ—, p0)

s{a1,a2,Â·Â·Â· ,aM }

Meaning
Number of states, types, and actions.
True probability distribution of the state.
Userâ€™s initial belief of the state under Î¸ .
Defenderâ€™s initial belief of the type at x.
Common prior belief in vector form.
Common posterior belief in vector form.
Optimal response action of a type-Î¸ user to
maximize his expected posterior utility.
Defenderâ€™s expected posterior utility under
generator Ï€ and common prior belief p0.
Defenderâ€™s prior utility where generator Ï€ 0
contains zero information.
Defenderâ€™s optimal posterior utility where
generator Ï€ âˆ— is optimal.
Security policy that requires the user of
type Î¸l âˆˆ Î˜ to take action al âˆˆ A for all
l âˆˆ {1, 2, Â· Â· Â· , M}.

TABLE II: Summary of notations in the case study.

Variable Meaning
p0,H
D
p0,H
U
pH
U
qg
qb
rU Ï† 0
tg(Ï† 0)
tb(Ï† 0)

Defenderâ€™s prior belief of a node being a honeypot.
Insiderâ€™s prior belief of a node being a honeypot.
Insiderâ€™s posterior belief of a node being a honeypot.
Percentage of selï¬sh insiders.
Percentage of adversarial insiders.
Insiderâ€™s authentication cost.
Decision thresholds of the selï¬sh insiders.
Decision thresholds of the adversarial insiders.

B. Notations and Organization of the Paper

Calligraphic letter A deï¬nes a set. The notation âˆ†A
represents the set of probability distribution over A
and |A | represents its cardinality. We summarize main
notations for the general model and the case study in
Table I and Table II, respectively. The rest of the paper
is organized as follows. Section II introduces the DG
model. We present the mathematical programming and the
concaviï¬cation method in Section III and IV, respectively.
Section V presents a case study of honeypot conï¬guration
to mitigate insider threats and Section VI concludes the
paper.

II. DUPLICITY GAME MODEL

We present a motivating example of insider threat
mitigation in Section II-A. Then, we present the structure
of DG in Section II-B and the timeline of the GMM
mechanism design in Section II-C, respectively. Finally,
we illustrate the relation of the DG-GMM mechanism to
the Bayesian persuasion framework in Section II-D.

A. Motivating Example of Insider Threat Mitigation

Insider threats have been a long-standing problem in
cybersecurity. Due to their information, privilege, and
resource advantages over external attackers, insider threats

3

can circumvent classical defense techniques such as in-
trusion prevention and detection systems. As a result,
defensive deception methods, such as honeypots, have
been used for insider threat detection and mitigation (see
e.g., [6], [36]). Theoretically, honeypots are assumed to
achieve a zero false-positive rate and low false-negative
rate by generating decoys accessed only by attackers. This
assumption may not hold for insider threats. On the one
hand, non-adversarial insiders who are curious or error-
prone can access honeypots, which intensiï¬es alert fatigue.
On the other hand, adversarial insiders can access the
internal information and ï¬ngerprint honeypots [37], [38]
using features such as open ports, protocols, and error
responses. To address these two challenges, we need to
conï¬gure the honeypot and the normal servers strategi-
cally. The conï¬guration needs to elicit desirable behaviors
from both adversarial and non-adversarial insiders even
though they have the same insider information. This work
introduces three conï¬guration methods that can be used
independently or jointly; i.e., conï¬gure the feature pattern
adaptively (see Example 1 for details), prolong or shorten
the authentication time to change insidersâ€™ incentives, and
misreport the percentage of honeypots to make use of the
insidersâ€™ trust.

1) Categorization of Insidersâ€™ Motives: An insiderâ€™s
motive can be roughly classiï¬ed into seven subcategories
based on the VERIS Community Database (VCDB) [39].
We divide these subcategories of motives into three classes
of motives: selï¬sh, adversarial, and unintentional. They
make up 12%, 26%, and 62%, respectively. The class of
selï¬sh motives includes fun, convenience, fear, or ideol-
ogy. The adversarial motives include espionage, ï¬nancial
gain, or grudge. The category of unintentional motives
refers to the negligent insiders who take no notice of the
deceptive conï¬guration and make habitual decisions. The
incentives of unintentional insiders are often uncontrol-
lable through incentives. Our incentive design mechanism
here focuses on the class of the selï¬sh insiders, who
seek self-interest, and the adversarial ones, who seek to
sabotage the organization.

2) Corporate Network with Insiders and Honeypots:
Fig. 1 illustrates a corporate network with honeypots
(denoted by xH ) and normal servers (denoted by xN) as
nodes. The SOC, or the defender, can privately determine
the percentage, the location, and the conï¬guration of hon-
eypots in the corporate network. The goal of the defender
is to elicit desirable behaviors from the selï¬sh insiders
(denoted by Î¸ g) and the adversarial
insiders (denoted
by Î¸ b). Both types of insiders can take harmful actions
intentionally yet for different reasons or motives. For
example, selï¬sh insiders may violate security rules and
abuse their privileges to save time and effort in ï¬nishing
their tasks. They do not seek to sabotage the organization
as the adversarial ones do. For each node in the corporate

4

and bU through a virtual trust manipulator. For example,
the defender can determine the percentage of honeypots
to be b(xH ) but report the percentage as bU (xH |Î¸ ) to
the type-Î¸ users who determine the percentage of honey-
pots based on the report without additional information.
The trust manipulator is overt
if the userâ€™s perceived
state distribution equals the true one for all types, i.e.,
bU (x|Î¸ ) = b(x), âˆ€x âˆˆ X , âˆ€Î¸ âˆˆ Î˜. Otherwise,
the trust
manipulator is said to be covert as the defender stealthily
manipulates usersâ€™ initial beliefs.

3) Information Structure: The information structure
consists of a ï¬nite set of signals S and a generator
Ï€ âˆˆ Î  : X (cid:55)â†’ âˆ†S . With a slight abuse of notation, we
use Ï€(s|x) to represent the probability of signal s âˆˆ S
at state x âˆˆ X . In Example 1 below, the signal can be
interpreted as the feature patterns, including protocols,
ports, the response time, and the error response.

Example 1 (Dynamic Feature Pattern Conï¬gurations).
To defend against honeypot ï¬ngerprinting, dynamic [40]
and adaptive [41], [42] conï¬gurations have been adopted
in honeypots. The SOC can also conï¬gure normal servers
and disguise them as honeypots by generating honeypot-
related features [43].

Suppose that there are J features that both honeypots
and normal servers can generate. Denote the value of fea-
ture j âˆˆ {1, Â· Â· Â· , J} by e j âˆˆ E j, where E j is a ï¬nite set. For
example, the error response feature can take a binary value
e j âˆˆ E j = {0, 1} based on whether an abnormal error
message appears under intentionally erroneous requests
[38]. We refer to the tuple of J features as the feature
pattern denoted by s = (e1, Â· Â· Â· , eJ) âˆˆ S := âˆJ
E j. Then,
the feature pattern of each node changes dynamically
accordingly to the generator Ï€ âˆˆ Î ; i.e., a honeypot
and a normal server generate feature pattern s âˆˆ S
with frequency Ï€(s|xH ) and Ï€(s|xN), respectively. Insiders
can use these feature patterns as the digital footprint to
ï¬ngerprint a nodeâ€™s state, either a honeypot or a normal
server. The DG still applies to the case when the SOC
cannot conï¬gure normal server. In that case, the decision
variable Ï€(Â·|xN) will be taken as ï¬xed.

j=1

4) Utility Transfer: The utility transfer consists of
a scaling factor Î³ âˆˆ [0, âˆ) and an incentive modulator
c âˆˆ C : A (cid:55)â†’ R which modiï¬es the utilities of the defender
and the user to be Ë†vD(x, Î¸ , a) = vD(x, Î¸ , a) + Î³c(a) and
Ë†vU (x, Î¸ , a) = vU (x, Î¸ , a) âˆ’ c(a), respectively, for all x âˆˆ
X , Î¸ âˆˆ Î˜, a âˆˆ A . Besides monetary (dis)incentives, c(a)
can also represent the additional cost or beneï¬t of taking
action a âˆˆ A . For example, it captures the authentication
time to access a normal server or a honeypot. The defender
can determine the authentication time to incentivize the
user (i.e., c(a) < 0) or disincentivize him (i.e., c(a) > 0)
to take the action a âˆˆ A . Although the modulator c is type-
independent, its inï¬‚uence on users is type-dependent. For

Fig. 1: An example corporate network consists of normal
servers and honeypots. The light blue background shows
the region of the internal network.

network, an insider can either access it (denoted by action
aAC) or not (denoted by action aDO).

B. Game Elements

The DG consists of four elements; i.e., the basic game
(X , Î˜, A , vD, vU , b âˆˆ âˆ†X ), the belief statistics (bD(Â·|x) âˆˆ
âˆ†Î˜, bU (Â·|Î¸ ) âˆˆ âˆ†X ), the information structure (S , Ï€ âˆˆ Î ),
and the utility transfer (Î³, c âˆˆ C ).

1) Basic Game: The DG consists of two players i âˆˆ
{D,U}, a defender i = D (hereafter she) and a user i = U
(hereafter he). Deï¬ne the ï¬nite sets of N states, M types,
and K actions as X := {x1, Â· Â· Â· , xN}, Î˜ := {Î¸1, Â· Â· Â· , Î¸M},
and A := {aDO, a1, Â· Â· Â· , aKâˆ’1}, respectively. Action aDO âˆˆ
A is the drop-out action. It indicates that the user chooses
not to participate in the game and takes no action.

The game has two-sided asymmetric information. The
defender can privately observe or know the realization of
the state x âˆˆ X from a probability distribution b âˆˆ âˆ†X .
For example, in the corporate network in Fig. 1, b(xH )
and b(xN) represent the percentages of honeypots and
normal servers, respectively. The user does not know each
nodeâ€™s state, i.e., whether a honeypot or a normal server.
The user has a private type Î¸ âˆˆ Î˜ that represents his
motive, capacity, rationality, or risk perception. The userâ€™s
behaviors are abstracted as an action a âˆˆ A . The defender
can observe the userâ€™s action by monitoring and logging
but she cannot observe the userâ€™s type; e.g., whether the
user accesses the conï¬dential data by accident (i.e., the
unintentional type), out of self-interest (i.e., the selï¬sh
type), or for adversarial purposes (i.e.,
the adversarial
type). The utility functions of the defender and the user,
denoted by vi : X Ã— Î˜ Ã— A (cid:55)â†’ R, i âˆˆ {D,U}, depend on
the state, type, and action.

2) Belief Statistics: The userâ€™s initial belief of the state
under type Î¸ âˆˆ Î˜ is bU (Â·|Î¸ ) âˆˆ âˆ†X . Since the user does
not know the true state distribution b(Â·), his perceived
state distribution bU can be different from the true one.
The defenderâ€™s belief of the userâ€™s type at state x âˆˆ X
is bD(Â·|x) âˆˆ âˆ†Î˜. In the game, the defender can design b

5

Fig. 2: Timeline for the GMM mechanism design.

example, a curiosity-driven insider may lose interest and
give up accessing conï¬dential data under a long authen-
tication delay or a convoluted multi-factor authentication
process. However, an adversarial insider can be persistent
if the data access leads to a comparably high ï¬nancial
return. Deï¬nition 1 deï¬nes a special utility structure where
one action ak âˆˆ A yields the highest beneï¬t for the user
of type Î¸ âˆˆ Î˜ regardless of the state values. For a user
with a dominant action, a generator does not inï¬‚uence the
userâ€™s belief and action.
Deï¬nition 1. An action ak âˆˆ A dominates (resp.
is
dominated) under type Î¸ âˆˆ Î˜ if Ë†vU (x, Î¸ , ak) â‰¥ (resp. â‰¤
) Ë†vU (x, Î¸ , a), âˆ€a âˆˆ A , âˆ€x âˆˆ X .

C. Timeline for the GMM Mechanism Design

As shown in Fig. 2, the GMM mechanism design in
DGs has two stages to achieve the intended outcomes of
the defensive deception. At stage one, the defender designs
(resp. observes) the generator Ï€ âˆˆ Î ,
the manipulator
b âˆˆ âˆ†X , bU (Â·|Î¸ ) âˆˆ âˆ†X , âˆ€Î¸ âˆˆ Î˜, and the modulator c âˆˆ C
if these components can (resp. cannot) be designed. Based
on the realized state value x, the generator generates a
signal s âˆˆ S with probability Ï€(s|x). In the insider threat
example, the defender conï¬gures the feature pattern s with
probability Ï€(s|xH ) (resp. Ï€(s|xN)) when the node is a
honeypot (resp. normal server). At stage two, the user of
type Î¸ âˆˆ Î˜ receives the signal s âˆˆ S and obtains his
posterior belief bÏ€
U of the state using the Bayesian rule,
i.e.,

bÏ€
U (x|Î¸ , s) :=

bU (x|Î¸ )Ï€(s|x)
âˆ‘x(cid:48)âˆˆX bU (x(cid:48)|Î¸ )Ï€(s|x(cid:48))

, âˆ€x âˆˆ X .

(1)

the user of type Î¸ âˆˆ Î˜ takes a best-response
U ) âˆˆ A to maximize his expected

Then,
action denoted by aâˆ—
Î¸ (bÏ€
posterior utility under the posterior belief bÏ€

U , i.e.,
U (Â·|Î¸ ,s)[ Ë†vU (x, Î¸ , a)].

(2)

aâˆ—
Î¸ (bÏ€

U ) âˆˆ arg max
aâˆˆA

Exâˆ¼bÏ€

The utility of the users is a way to capture the user
Î¸ can represent how an in-

Î¸ . For example, aâˆ—

behavior aâˆ—

sider routinely follows the security rules or abuses his
privilege for personal gain. The defenderâ€™s goal
is to
determine the optimal GMM mechanism to proactively
prevent undesirable user behaviors and improve the secu-
rity posture. This objective is achieved by maximizing her
expected posterior utility Â¯vD that captures the outcomes of
the userâ€™s behaviors, i.e., Â¯vD(Ï€, b, bU , c) := Exâˆ¼b(Â·)Esâˆ¼Ï€(Â·|x)
E
Î¸ âˆ¼bD(Â·|x)[ Ë†vD(x, Î¸ , aâˆ—
U ))]. Different generators provide
the user with different amounts of information about the
state. Two extreme cases are deï¬ned in Deï¬nition 2.
A signal from a zero-information generator denoted by
Ï€ 0 âˆˆ Î  does not change the userâ€™s belief, i.e., bÏ€0
U (x|Î¸ , s) =
bU (x|Î¸ ), âˆ€s âˆˆ S , âˆ€x âˆˆ X , âˆ€Î¸ âˆˆ Î˜. Meanwhile, a signal
from a full-information generator deterministically reveals
the state to the user.

Î¸ (bÏ€

Deï¬nition 2 (Zero- and Full-Information Generators).
A generator Ï€ âˆˆ Î  contains zero information if Ï€(s|x) =
Ï€(s|x(cid:48)), âˆ€s âˆˆ S , âˆ€x, x(cid:48) âˆˆ X . It contains full information if
the mapping Ï€ : X (cid:55)â†’ S is injective.

Readers can refer to Section V for a case study of insider

threat that illustrates the two-stage GMM design.

D. Relation to Bayesian Persuasion

DG-GMM mechanism design can be viewed as a
generalized class of the Bayesian persuasion framework
[8] with heterogeneous receivers, two-sided asymmetric
information, and a joint design of information, incentive,
and trust. If the userâ€™s type set Î˜ is a singleton and the
defender cannot design the modulator and the manipulator,
then DG-GMM degenerates to the Bayesian persuasion
framework. The consolidation of the modulator and the
manipulator into the mechanism gives the defender a
higher degree of freedom to improve the performance
in the deception design. It yet increases the computation
complexity as illustrated in Section III and causes the
violation of Bayesian plausibility in Section II-D1.

1) Violation of Bayesian Plausibility: The concept of
Bayesian plausibility has been deï¬ned in [8], which states

Design/ObserveSignal ğ‘ âˆˆğ’®or Security policy ğ‘ {"!,â€¦,""}âˆˆğ’®Utilityğ‘£%&(ğ‘¥,ğœƒ,ğ‘)Belief updateDefenderGenerator ğœ‹Manipulator ğ‘,ğ‘â€™Modulator ğ‘HoneypotsNormal serversUserActionğ‘âˆˆğ’œDefensive DeceptionPrivate type ğœƒâˆˆÎ˜Private state  ğ‘¥âˆˆğ’³that the expected posterior belief should equal the prior
belief for all Ï€ âˆˆ Î . However, we show in Lemma 1
that the trust manipulator can violate Bayesian plausibility
when the user of type Î¸ âˆˆ Î˜ holds a different initial belief
as the defender, i.e., âˆƒx âˆˆ X : b(x) (cid:54)= bU (x|Î¸ ).

Lemma 1 (Bayesian Plausibility). For all Ï€ âˆˆ Î  and Î¸ âˆˆ
Î˜, the userâ€™s expected posterior probability be
U (x|Î¸ ) :=
âˆ‘sâˆˆS âˆ‘x(cid:48)âˆˆX b(x(cid:48))Ï€(s|x(cid:48))bÏ€
U (x|Î¸ , s) is always a valid prob-
ability measure yet is Bayesian plausible if and only if
the defender and the user have the same initial belief
b(x) = bU (x|Î¸ ), âˆ€x âˆˆ X .

receiving s,

Proof. A generator Ï€ âˆˆ Î  generates s with proba-
bility âˆ‘x(cid:48)âˆˆX b(x(cid:48))Ï€(s|x(cid:48)). After
the user
of type Î¸ obtains his posterior belief bÏ€
U (x|Î¸ , s) ac-
the expected posterior probabil-
cording to (1). Thus,
ity âˆ‘sâˆˆS âˆ‘x(cid:48)âˆˆX b(x(cid:48))Ï€(s|x(cid:48))bÏ€
U (x|Î¸ , s) is a valid prob-
ability measure over x. The Bayesian plausibility re-
âˆ‘x(cid:48)âˆˆX b(x(cid:48))Ï€(s|x(cid:48))
quires be
âˆ‘x(cid:48)âˆˆX bU (x(cid:48)|Î¸ )Ï€(s|x(cid:48)) Ï€(s|x)bU (x|Î¸ ) =
bU (x|Î¸ ), âˆ€x âˆˆ X , under all Ï€ âˆˆ Î , which is equivalent to
the condition b(x) = bU (x|Î¸ ), âˆ€x âˆˆ X .

U (x|Î¸ ) = âˆ‘sâˆˆS

III. GMM DESIGNS BY MATHEMATICAL
PROGRAMMING

Î¸ (bÏ€

Î¸ (bÏ€

In Section III, we provide an integrated design of the
GMM mechanism by mathematical programming. We ï¬rst
elaborate on the relationship between signals and the userâ€™s
best-response action to introduce the notion of security
policies. Each signal s from generator Ï€ âˆˆ Î  updates the
userâ€™s belief via (1) and consequently induces the user of
U ) âˆˆ A .
type Î¸ âˆˆ Î˜ to take the best-response action aâˆ—
Regardless of the signal set S and the generator Ï€, these
signals can elicit at most |A ||Î˜| = KM distinct outcomes;
U ) is al if his type
i.e., the userâ€™s best-response action aâˆ—
is Î¸l for all permutations of Î¸l âˆˆ Î˜, al âˆˆ A . We can aggre-
gate signals in S based on their elicited actions and divide
the entire signal set S into KM mutually exclusive subsets
{a1,a2,Â·Â·Â· ,aM }, al âˆˆ A , l âˆˆ {1, 2, ..., M}. Then,
denoted as S
the signals in subset S
{a1,a2,Â·Â·Â· ,aM } can be interpreted as
the security policy that requires the user of type Î¸l to take
action al for all l âˆˆ {1, 2, Â· Â· Â· , M}. Without loss of general-
ity, we use one aggregated signal s{a1,a2,Â·Â·Â· ,aM } to represent
the signals in the set S
{a1,a2,Â·Â·Â· ,aM}. Then, the total number
of signals are |S | = KM, and Ï€(Â·|x) âˆˆ âˆ†S is a probability
distribution over KM security policies for each state x âˆˆ X .
The set Î  naturally contains two feasibility constraints,
i.e., Ï€(s{a1,Â·Â·Â· ,aM}|x) â‰¥ 0, âˆ€s{a1,Â·Â·Â· ,aM} âˆˆ S , âˆ€x âˆˆ X , and
âˆ‘s{a1,Â·Â·Â· ,aM }âˆˆS Ï€(s{a1,Â·Â·Â· ,aM}|x) = 1, âˆ€x âˆˆ X . In Example 2
below, we continue to use the insider threat scenario in
Section II-A to illustrate how we obtain security policies
based on the feature patterns.

6

Example 2 (Security Policies based on Feature Pat-
terns). For binary action set A = {aDO, aAC} and binary
type set Î˜ = {Î¸ g, Î¸ b}, the feature patterns in Example
1 can be aggregated into KM = 4 categories of security
policies. They are s{aDO,aDO} (i.e., both types of insiders
choose aDO), s{aDO,aAC} (i.e., selï¬sh insiders choose aAC
while adversarial insiders choose aDO), s{aAC,aDO} (i.e.,
adversarial
insiders choose aAC while selï¬sh insiders
choose aDO), and s{aAC,aAC} (i.e., both types of insiders
choose aAC).

i.e., âˆ‘xâˆˆX bÏ€

We can rewrite (2) concerning security policies
U (x|Î¸l, s{a1,Â·Â·Â· ,aM })[ Ë†vU (x, Î¸l, al) âˆ’
as follows,
Ë†vU (x, Î¸l, ah)] â‰¥ 0, âˆ€s{a1,Â·Â·Â· ,aM} âˆˆ S , âˆ€ah âˆˆ A , âˆ€Î¸l âˆˆ Î˜. The
defenderâ€™s expected posterior utility Â¯vD(Ï€, b, bU , c) can
be equivalently represented as âˆ‘xâˆˆX b(x) âˆ‘s{a1,Â·Â·Â· ,aM }âˆˆS
Ï€(s{a1,Â·Â·Â· ,aM}|x) âˆ‘Î¸l âˆˆÎ˜ bD(Î¸l|x) Ë†vD(x, Î¸l, al). Replacing bÏ€
U
with (1), we formulate the GMM mechanism design as
the following constrained optimization COP.

(COP):

r :=

sup
Ï€âˆˆÎ ,b,bU ,câˆˆC

Â¯vD(Ï€, b, bU , c)

[ Ë†vU (x, Î¸l, al) âˆ’ Ë†vU (x, Î¸l, ah)]Ï€(s{a1,Â·Â·Â· ,aM}|x)

(IC) âˆ‘
xâˆˆX
bU (x|Î¸l) â‰¥ 0, âˆ€s{a1,Â·Â·Â· ,aM } âˆˆ S , âˆ€ah âˆˆ A , âˆ€Î¸l âˆˆ Î˜.

(MF) c(aDO) = 0.

The decision variables Ï€, b, bU , and c are vectors of
dimension N Ã— KM, N, N Ã— M, and K, respectively. The
feasibility constraint contained in Î  and the Incentive-
Compatible (IC) constraint induce N Ã— KM + 1 and KM Ã—
K Ã— M constraints, respectively.

Denote bâˆ—, bâˆ—

U , Ï€ âˆ—, câˆ— as the maximizers of COP and r as
the value of the objective function under the maximizers.
The (IC) constraint requires all security policies from the
generator to be compatible with the userâ€™s incentives; i.e.,
the user receives the maximum beneï¬t on average when
taking the action required by the security policy. A security
policy cannot be generated if it is not incentive-compatible.
Based on the (IC) constraint, we deï¬ne the credible and the
optimal generators in Deï¬nition 3 and enforceable security
policies in Deï¬nition 4.

Deï¬nition 3 (Credible and Optimal Generators). A
generator Ï€ âˆˆ Î  is called credible if it satisï¬es (IC). A
credible generator is called optimal if it maximizes COP.

Deï¬nition 4 (Enforceable Security Policies). For a given
generator Ï€ âˆˆ Î , a security policy s{a1,Â·Â·Â· ,aM} âˆˆ S is
enforceable (resp. unenforceable) if âˆƒx âˆˆ X such that
Ï€(s{a1,Â·Â·Â· ,aM}|x) (cid:54)= 0 (resp. Ï€(s{a1,Â·Â·Â· ,aM}|x) = 0, âˆ€x âˆˆ X ).

The Modulation-Feasible (MF) constraint results from
the fact
the defender cannot modulate the userâ€™s
incentive if the user does not participate in the game.
Although the co-domain of c is R, Theorem 1 shows that

that

the optimal utility transfer câˆ— âˆˆ C has to remain bounded
due to the userâ€™s potential
threat of taking the drop-
out action aDO. We deï¬ne the following shorthand nota-
tions for Theorem 1, i.e., c(Î¸ , a) := maxxâˆˆX vU (x, Î¸ , a) âˆ’
vU (x, Î¸ , aDO), Â¯r = maxxâˆˆX EÎ¸ âˆ¼bD[maxaâˆˆA vD(Î¸ , x, a)] and
r = minxâˆˆX EÎ¸ âˆ¼bD[minaâˆˆA vD(Î¸ , x, a)].

(Feasibility
and

Theorem 1
COP
bound
Â¯r + Î³ maxaâˆˆA ,Î¸ âˆˆÎ˜ c(Î¸ , a)} and the lower bound is r.

and Design Capacity).
upper
bounded.
is max{maxxâˆˆX EÎ¸ âˆ¼bD[vD(x, Î¸ , aDO)],

feasible
r

is
of

The

Proof. We ï¬rst prove the feasibility. Deï¬ne short-
hand notation aâˆ—,l := arg maxaâˆˆA Exâˆ¼bU (x|Î¸l )[vU (x, Î¸l, a) âˆ’
c(a)], âˆ€l âˆˆ {1, Â· Â· Â· , M}, as the optimal action of the user
of type Î¸l âˆˆ Î˜ under any feasible prior belief bU (x|Î¸l)
and modulator c âˆˆ C . Then, the zero-information generator
Ï€ 0(s(aâˆ—,1,Â·Â·Â· ,aâˆ—,M )|x) = 1, âˆ€x âˆˆ X , is a feasible solution to
COP.

We prove the boundedness in two steps. We ï¬rst
consider c(a) = 0, âˆ€a âˆˆ A . Since all decision variables
b, Ï€, bD are probability measures, we obtain the upper
bound Â¯r and the low bound r of r. In the second step,
we turn the modulator c into a free decision variable
with the (MF) constraint. Since c(a) = 0, âˆ€a âˆˆ A ,
is
the maximum value of COP does
a feasible solution,
not increase. Thus, the value of r is bounded. To show
that the value of Â¯r is bounded in step two, we focus
on action a j âˆˆ A ,
that results in a non-
negative maximizer câˆ—(a j). On the one hand, if c(Î¸ , a j) â‰¤
0, âˆ€Î¸ âˆˆ Î˜, then the drop-out action aDO dominates for
all types and r = maxbâˆˆâˆ†X Exâˆ¼bEÎ¸ âˆ¼bD[vD(x, Î¸ , aDO)] â‰¤
maxxâˆˆX EÎ¸ âˆ¼bD[vD(x, Î¸ , aDO)]. On the other hand, if there
exists a type Î¸ âˆˆ Î˜ where c(Î¸ , a j) > 0 and câˆ—(a j) â‰¥
c(Î¸ , a j), then the user of type Î¸ will choose the drop-out
action aDO. Thus, r â‰¤ Î³ maxaâˆˆA ,Î¸ âˆˆÎ˜ c(Î¸ , a).

if it exists,

The upper and lower bounds provide the design ca-
the GMM mechanism. COP is unbounded
pacity of
the (MF) constraint as the defender can ar-
without
the value of r by
bitrarily increase (resp. decrease)
letting c(a) be an arbitrarily large (resp. small) con-
If c(a) = 0, âˆ€a âˆˆ A , we can transform COP
stant.
into a Linear Program (LP) by introducing the follow-
ing variables, i.e., Î·(s{a1,Â·Â·Â· ,aM}, x) := b(x)Ï€(s{a1,Â·Â·Â· ,aM}|x)
and Î·U (Î¸ , s{a1,Â·Â·Â· ,aM}, x) := bU (x|Î¸ )Ï€(s{a1,Â·Â·Â· ,aM}|x). These
take non-negative values and satisfy
new variables
i.e., âˆ‘xâˆˆX ,s{a1,Â·Â·Â· ,aM }âˆˆS Î· =
the following constraints,
1 and âˆ‘xâˆˆX ,s{a1,Â·Â·Â· ,aM }âˆˆS Î·U = 1, âˆ€Î¸ âˆˆ Î˜. After we
have solved the LP, we can obtain the initial beliefs
by b(x) = âˆ‘s{a1,Â·Â·Â· ,aM }âˆˆS Î·(s{a1,Â·Â·Â· ,aM}, x) and bU (x|Î¸ ) =
âˆ‘s{a1,Â·Â·Â· ,aM }âˆˆS Î·U (Î¸ , s{a1,Â·Â·Â· ,aM }, x) for all x âˆˆ X , Î¸ âˆˆ Î˜.

IV. GRAPHICAL ANALYSIS OF GMM DESIGNS

7

In Section III, we aggregate signals into KM equivalent
security policies to relate them with the userâ€™s best-
response action. In Section IV, we directly analyze the
posterior belief and the action as each signal uniquely
determines a posterior belief. Throughout Section IV, we
focus on the overt trust manipulator deï¬ned in Section
II-B2, i.e., bU (x|Î¸ ) = b(x), âˆ€x âˆˆ X , Î¸ âˆˆ Î˜. Deï¬ne p0
j :=
b(x j), âˆ€ j âˆˆ {1, Â· Â· Â· , N}, and the common prior belief in the
vector form as p0 := [p0
1, Â· Â· Â· , p0
N]. Since different types of
users have the same initial beliefs, the posterior beliefs
are also the same. Denote p j âˆˆ [0, 1] as the userâ€™s pos-
terior belief under state x j âˆˆ X , âˆ€ j âˆˆ {1, Â· Â· Â· , N}. Deï¬ne
the belief vector p := [p1, Â· Â· Â· , pN] and the utility vector
Ë†vU (Î¸ , a) := [ Ë†vU (x1, Î¸ , a), Â· Â· Â· , Ë†vU (xN, Î¸ , a)](cid:48) where notation
(cid:48) denotes the matrix transpose. For both the prior and the
posterior belief vectors, the total probability is one, i.e.,
âˆ‘N

n=1 p0
Section IV-A provides the optimal generator design
under the benchmark case where the defender can neither
modify the userâ€™s incentive, i.e., c(a) = 0, âˆ€a âˆˆ A , nor ma-
nipulate their initial beliefs. Section IV-B incorporates the
modulator and the manipulator into the GMM mechanism
design.

n = 1 and âˆ‘N

n=1 pn = 1.

A. Generator Design under the Benchmark Case

We rewrite (2) in its matrix form as aâˆ—

Î¸ (p) âˆˆ arg maxaâˆˆA
pË†vU (Î¸ , a). Since pË†vU (Î¸ , a) is an afï¬ne function of p for
any action a âˆˆ A , maximizing pË†vU (Î¸ , a) over a in the
convex domain p âˆˆ âˆ†X results in a Piece-Wise Linear and
Convex (PWLC) function as summarized in Proposition 1.
The proof of convexity follows directly from the fact that
aâˆ—
Î¸ (p) is the point-wise maximum of a group of afï¬ne
functions over p.

Proposition 1. The userâ€™s expected posterior utility under
a give type Î¸ âˆˆ Î˜, i.e., maxaâˆˆA pË†vU (Î¸ , a), is continuously
PWLC with respect to vector p âˆˆ âˆ†X .

We visualize maxaâˆˆA pË†vU (Î¸ , a) under a binary state set
in Fig. 3. When N = 2, we can use the ï¬rst element p1
as the x-axis to uniquely represent the posterior belief p âˆˆ
âˆ†X . The four belief thresholds, i.e., 0,tÎ¸
2 , and 1, divide
the entire belief region of p1 âˆˆ [0, 1] into three sub-regions.
The user of type Î¸ takes action aKâˆ’1 if his posterior belief
belongs to the sub-region p1 âˆˆ [0,tÎ¸
1 ], action a1 if p1 âˆˆ
[tÎ¸
2 ], and action aDO if p1 âˆˆ [tÎ¸
1 ,tÎ¸
2 , 1]. Although action a2
is not dominated under type Î¸ based on Deï¬nition 1, it is
inactive over p1 âˆˆ [0, 1].

1 ,tÎ¸

For a high-dimensional state space N â‰¥ 3, the userâ€™s
entire belief region âˆ†X is an N âˆ’ 2 simplex. For each
type Î¸ , we can divide the entire belief region into at most
K sub-regions C Î¸
ai := {p â‰¥ 0|p(cid:48)[Ë†vU (Î¸ , ai) âˆ’ Ë†vU (Î¸ , a j)] â‰¥

8

Illustration of KM = 4 convex polytopes C{a1,a1},
Fig. 4:
C{aDO,a1}, C{aDO,aDO}, and C{a1,aDO} in blue (horizontal
stripes), green (downward diagonal stripes), grey (vertical
stripes), and orange (upward diagonal stripes), respec-
tively. Each point in the equilateral triangle represents a
belief p = [p1, p2, p3] âˆˆ âˆ†X .

belief regions. The results can be extended to N > 3 as
a variant of the hyperplane arrangement problem [44].
We summarize the above result in Proposition 2; i.e., the
number of belief region partitions grows in a polynomial
rate denoted by Ï‡(K, M, N) rather than the exponential rate
of KM, where Ï‡(K, M, N) is a polynomial function of K, M
for each N.

Proposition 2 (Upper Limit of Enforceable Policies).
For any credible generator, at most Ï‡(K, M, N) security
policies are enforceable.

Remark 1. Solely dependent on the userâ€™s utility vector
Ë†vU , the belief partition âˆ†X = âˆªa1âˆˆA ,Â·Â·Â· ,aMâˆˆA C
{a1,Â·Â·Â· ,aM}
characterizes the userâ€™s incentive under different types. If
C
{a1,Â·Â·Â· ,aM } = /0, then the security policies that require the
user of type Î¸l to take action al for any l âˆˆ {1, Â· Â· Â· , M}
are unenforceable as they violate the userâ€™s incentive.
Proposition 2 illustrates that the number of enforceable
security policies cannot exceed a threshold determined
by K, M, N; i.e., among all |S | = KM potential security
policies, the defender can choose at most Ï‡(K, M, N) ones
to be compatible with the userâ€™s incentive.

1) Cyber Attribution and Type Identiï¬cation: The hon-
eypot example motivates us to investigate the condition
under which public security policies elicit different actions
from different
types of users. The condition is useful
for cyber attribution, i.e., tracing observable actions back
to the userâ€™s private types. Since each security policy
uniquely determines a posterior belief for a given genera-
tor, we deï¬ne type identiï¬ability concerning the posterior
belief in Deï¬nition 5.

Deï¬nition 5 (Identiï¬able Types). Two different types

Fig. 3: The expected posterior utility of the user of
type Î¸ âˆˆ Î˜ versus posterior belief p1 âˆˆ [0, 1]. The solid
lines represent the utility maxaâˆˆA âˆ‘N
n=1 pn Ë†vU (xn, Î¸ , a) as a
PWLC function of p1.

i, j := C Î¸l

2 , 1] and C Î¸
a2

a1 âˆ© Â· Â· Â· âˆ© C Î¸M

{a1,Â·Â·Â· ,aM} := C Î¸1

aDO is the interval [tÎ¸

0, âˆ€a j âˆˆ A . Then, âˆ†X = âˆªiâˆˆ{DO,1,Â·Â·Â· ,Kâˆ’1}C Î¸
ai . If the pos-
terior belief falls into the sub-region C Î¸
ai , the user of type
Î¸ takes ai as his best-response action. Take Fig. 3 as an
example, C Î¸
is the empty
set. As a direct result of the deï¬nition of convexity, sets
C Î¸
ai , âˆ€i âˆˆ {DO, 1, Â· Â· Â· , K âˆ’ 1}, are convex and connected.
We have illustrated the belief region partition under any
given type Î¸ âˆˆ Î˜. Since the user has M possible types,
we further divide the belief region into ï¬ner sub-regions.
Let C
aM be the sub-region of the
posterior belief under which the best-response action of the
user of type Î¸l, âˆ€l âˆˆ {1, Â· Â· Â· , M}, is action al âˆˆ A . In par-
ticular, deï¬ne C l,h
ai âˆ© C Î¸h
a j as the belief region where
the user takes action ai when his type is Î¸l and a j when his
type is Î¸h for all i, j âˆˆ {DO, 1, Â· Â· Â· , K âˆ’ 1} and l (cid:54)= h, âˆ€l, h âˆˆ
{1, Â· Â· Â· , M}. Based on the deï¬nition, C l,h
j,i . Since
the intersection of any collection of convex sets is convex,
{a1,Â·Â·Â· ,aM} and C l,h
C
i, j are all convex and connected sets, i.e.,
convex polytopes. We visualize these convex polytopes in
Fig. 4 when there are two types M = 2, two actions K = 2,
and three states N = 3. The belief region âˆ†X is an N âˆ’ 2
simplex, i.e., an equilateral triangle. Under type Î¸1, the
belief region is divided into C Î¸1
aDO = C{aDO,a1} âˆª C{aDO,aDO}
and C Î¸1
a1 = C{a1,a1} âˆª C{a1,aDO}. Under type Î¸2, the belief
region is divided into C Î¸2
aDO = C{aDO,aDO} âˆª C{a1,aDO} and
C Î¸2
a1 = C{a1,a1} âˆª C{aDO,a1}. Since there are only two types,
we have C 1,2

i, j â‰¡ C h,l

1,DO = C{a1,aDO}.
Among KM possible sets C

{a1,Â·Â·Â· ,aM}, âˆ€al âˆˆ A , l âˆˆ
{1, Â· Â· Â· , M}, most of them are empty. Take N = 2 as an
example, K actions can generate at most K(K âˆ’1)/2 belief
thresholds over p1 âˆˆ (0, 1) for each type as shown in
Fig. 3. Thus, the whole belief region p1 âˆˆ [0, 1] can be
divided into at most MK(K âˆ’ 1)/2 + 1 regions under M
types. When N = 3, the belief region is an equilateral
triangle as shown in Fig. 4. For each given type, K actions
represent K planes. Projecting these planes vertically onto
the equilateral triangle, we obtain at most K(K âˆ’ 1)/2
lines. Thus, these lines under M types can divide the
equilateral triangle into at most MK(Kâˆ’1)
+ 1)/2

( MK(Kâˆ’1)
2

2

ğ‘=ğ‘!ğ‘=ğ‘"#ğ‘=ğ‘$%!ğ‘=ğ‘&1Utility<latexit sha1_base64="EifKk2OMkzZweTzzY4FCgnd5ylY=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMeiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0kPS9frniVt05yCrxclKBHI1++as3iFkaoTRMUK27npsYP6PKcCZwWuqlGhPKxnSIXUsljVD72fzUKTmzyoCEsbIlDZmrvycyGmk9iQLbGVEz0sveTPzP66YmvPYzLpPUoGSLRWEqiInJ7G8y4AqZERNLKFPc3krYiCrKjE2nZEPwll9eJa1a1bus1u4vKvWbPI4inMApnIMHV1CHO2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAByo2f</latexit>p1<latexit sha1_base64="yQbb92lFZ2G4RWLghdk+NupKkcw=">AAAB8XicdVDLSgNBEJz1GeMr6tHLYBA8hZ0gedyCXjxGMA9M1jA7mU2GzM4uM71CWPIXXjwo4tW/8ebfOJtEUNGChqKqm+4uP5bCgOt+OCura+sbm7mt/PbO7t5+4eCwbaJEM95ikYx016eGS6F4CwRI3o01p6EvecefXGZ+555rIyJ1A9OYeyEdKREIRsFKtzAgd30Yc6CDQtEtua5LCMEZIdWKa0m9XiuTGiaZZVFESzQHhff+MGJJyBUwSY3pETcGL6UaBJN8lu8nhseUTeiI9yxVNOTGS+cXz/CpVYY4iLQtBXiufp9IaWjMNPRtZ0hhbH57mfiX10sgqHmpUHECXLHFoiCRGCKcvY+HQnMGcmoJZVrYWzEbU00Z2JDyNoSvT/H/pF0ukUqpfH1ebFws48ihY3SCzhBBVdRAV6iJWoghhR7QE3p2jPPovDivi9YVZzlzhH7AefsEqkeQ7w==</latexit>tâœ“1<latexit sha1_base64="2QaoVXQgpUa7SR/GBbDv4aBOI3M=">AAAB8XicdVDLSgNBEJz1GeMr6tHLYBA8hZ0gedyCXjxGMA9M1jA7mU2GzM4uM71CWPIXXjwo4tW/8ebfOJtEUNGChqKqm+4uP5bCgOt+OCura+sbm7mt/PbO7t5+4eCwbaJEM95ikYx016eGS6F4CwRI3o01p6EvecefXGZ+555rIyJ1A9OYeyEdKREIRsFKtzAo3/VhzIEOCkW35LouIQRnhFQrriX1eq1MaphklkURLdEcFN77w4glIVfAJDWmR9wYvJRqEEzyWb6fGB5TNqEj3rNU0ZAbL51fPMOnVhniINK2FOC5+n0ipaEx09C3nSGFsfntZeJfXi+BoOalQsUJcMUWi4JEYohw9j4eCs0ZyKkllGlhb8VsTDVlYEPK2xC+PsX/k3a5RCql8vV5sXGxjCOHjtEJOkMEVVEDXaEmaiGGFHpAT+jZMc6j8+K8LlpXnOXMEfoB5+0Tq9KQ8A==</latexit>tâœ“20ğœƒ!ğœƒ"ğ‘!ğ‘"#ğ’!$%	#&ğ’!&	#&ğ’!&	#â€™ğ’!,$%	!,"=ğ’{(!,("#}ğ’{("#,("#}ğ’{(!,(!}ğ’{("#,(!}ğ‘!ğ‘"#ğ’!$%	#â€™ğ©=[1,0,0]ğ©=[0,0,1]ğ©=[0,1,0]l, h âˆˆ {1, Â· Â· Â· , M} are identiï¬able under a posterior belief
p âˆˆ âˆ†X if âˆƒi, j âˆˆ {DO, 1, Â· Â· Â· , K âˆ’ 1} and i (cid:54)= j such that
p âˆˆ C l,h
i, j .

The posterior beliefs under which two different types
l, h âˆˆ {1, Â· Â· Â· , M} are identiï¬able constitute a belief region
that may not be connected. This belief region solely
depends on the userâ€™s utility vector Ë†vU as the ï¬nest belief
partition âˆ†X = âˆªa1âˆˆA ,Â·Â·Â· ,aM âˆˆA C
{a1,Â·Â·Â· ,aM} solely depends
on Ë†vU . Intuitively,
the size of the region is reduced
as the utilities of the users of type Î¸l and Î¸h become
better aligned. Deï¬nition 6 deï¬nes two extremes of utility
alignment.

Deï¬nition 6 (Completely (Mis)aligned Utilities). Two
different types of users have completely aligned (resp.
misaligned) utilities, or equivalently zero (resp. full) utility
misalignment, if they are unidentiï¬able (resp. identiï¬able)
under all posterior belief p âˆˆ âˆ†X .

If two utilities have the same (resp. opposite) values,
then they are completely aligned (resp. misaligned). If
two types of usersâ€™ utilities are completely aligned (resp.
misaligned), then the security policies that procure them
to take different actions (resp. the same action) are not
enforceable under any credible generators. Proposition 3
shows that the results are translation- and scale-invariant.

Proposition 3 (Alignment under Linear Dependence).
Consider linearly dependent utilities of two types l, h âˆˆ
there exist a scaling factor
{1, Â· Â· Â· , M} of users; i.e.,
U (Î¸l, Î¸h) âˆˆ R and translation factors Ït
Ï s
U (x, Î¸l, Î¸h) âˆˆ
R, âˆ€x âˆˆ X , such that Ë†vU (x, Î¸l, a) = Ï s
U (Î¸l, Î¸h) Ë†vU (x, Î¸h, a)+
U (x, Î¸l, Î¸h), âˆ€x âˆˆ X , a âˆˆ A . Two utilities are completely
Ït
aligned (resp. misaligned) if and only if Ï s
U (Î¸l, Î¸h) â‰¥ 0
(resp. < 0).

given

p âˆˆ âˆ†X and
i âˆˆ A such that âˆ‘N

Proof. For
Î¸l âˆˆ Î˜,
any
there exists an action aâˆ—
n=1 pn
i ) âˆ’ Ë†vU (xn, Î¸l, ak)] â‰¥ 0, âˆ€ak âˆˆ A . Then,
[ Ë†vU (xn, Î¸l, aâˆ—
U (Î¸l, Î¸h) âˆ‘N
Ï s
n=1 pn[ Ë†vU (xn, Î¸h, aâˆ—
i ) âˆ’ Ë†vU (xn, Î¸h, ak)] â‰¥ 0,
âˆ€ak âˆˆ A , and the user of type Î¸h âˆˆ Î˜ at any posterior
belief p has the same best-response action aâˆ—
if and only
i
if Ï s

U (Î¸l, Î¸h) â‰¥ 0.

2) Characterization of the Optimal Generator: Under
a zero-information generator Ï€ 0 âˆˆ Î , the userâ€™s posterior
belief equals the prior belief p0 and we can rewrite the
Î¸ (bÏ€0
userâ€™ best-response action aâˆ—
Î¸ (p0). Since
variables bU , c are not designable in the benchmark case,
we omit them in function Â¯vD and rewrite the defenderâ€™s
expected posterior utility as Â¯vD(Ï€, p0). Since the users
make decisions based on their prior beliefs, we refer
to the expected posterior utility Â¯vi of player i âˆˆ {D,U}
as his prior utility Ëœvi when the generator contains zero

U ) in (2) as aâˆ—

information. In particular, the defenderâ€™s prior utility ËœvD is
a function of the prior belief p0, i.e.,

9

ËœvD(p0) := Â¯vD(Ï€ 0, p0) = E

xâˆ¼p0E
Î¸ (p0)].
We obtain the piece-wise linear structure of the defenderâ€™s
prior utility ËœvD in Proposition 4. The solid lines in Fig. 5
illustrate ËœvD.

Î¸ âˆ¼bD(Â·|x)[ Ë†vD(x, Î¸ , aâˆ—

Proposition 4. The defenderâ€™s prior utility ËœvD is a (possi-
bly discontinuous) piece-wise linear function of the com-
mon prior belief vector p0 âˆˆ âˆ†X with at most Ï‡(K, M, N)
pieces.

that

ËœvD is linear with respect

Proof. The piece-wise linear structure follows from the
to p0 inside each
fact
convex polytope C
{a1,Â·Â·Â· ,aM }, âˆ€al âˆˆ A , l âˆˆ {1, Â· Â· Â· , M}. As
a result of Proposition 2, the upper bound of the number
of different convex polytopes is Ï‡(K, M, N). Since the
polytopes are determined based on the userâ€™s prior utility
rather than the defenderâ€™s, ËœvD is possibly discontinuous at
the boundaries of these polytopes.

Fig. 5: The defenderâ€™s expected posterior utility versus
prior belief p0
1 with and without the modulator in orange
and blue, respectively. We denote orange lines and nota-
tions in bold. The solid lines indicate that the defenderâ€™s
prior utility ËœvD is discontinuous and piece-wise linear
under three belief regions, i.e., [0,tÎ¸1
1 , 1].
The dashed lines represent the defenderâ€™s optimal posterior
utility VD.

1 ], and [tÎ¸2

1 ], [tÎ¸1

1 ,tÎ¸2

The defenderâ€™s expected posterior utility Â¯vD is a function
of Ï€ âˆˆ Î  and p0 âˆˆ âˆ†X . Thus, the defenderâ€™s optimal
posterior utility VD(p0) := supÏ€âˆˆÎ  Â¯vD(Ï€, p0) is a function
of p0 âˆˆ âˆ†X . Based on Theorem 1, there exists an optimal
generator Ï€ âˆ— âˆˆ Î  that achieves the optimal posterior utility,
i.e., VD(p0) = Â¯vD(Ï€ âˆ—, p0) = r. Denote the convex hull of
function ËœvD as co( ËœvD). Then, we can use the concaviï¬-
cation technique introduced in [7], [8] to show that the
defenderâ€™s optimal posterior utility VD(p0) is the concave
closure of her prior utility ËœvD(p0) over the entire belief re-
gion p0 âˆˆ âˆ†X , i.e., VD(p0) = sup{z âˆˆ R|(p0, z) âˆˆ co( ËœvD)}.
We visualize the concaviï¬cation process under the bi-
nary state space N = 2 in Fig. 5. Suppose that there are

1Utility2413413<latexit sha1_base64="tPdfEyZMPK4/ja0N95FE1NvvTWg=">AAACIHicdVDLSgMxFM34rPVVdekmWAQXUiZF2roT3bisYKswHYdMmrbBzIPkjlCG+RQ3/oobF4roTr/GTNsBFT0QOJxzbnJz/FgKDbb9Yc3NLywuLZdWyqtr6xubla3tro4SxXiHRTJS1z7VXIqQd0CA5Nex4jTwJb/yb89y/+qOKy2i8BLGMXcDOgzFQDAKRvIqzbQ3ucRRQ99N7Zpt24SQw5yQZsM25Pi4VSetDDxyk/ZgxIF6JMu8SrUI4yKMizAmuWVQRTO0vcp7rx+xJOAhMEm1dogdg5tSBYJJnpV7ieYxZbd0yB1DQxpw7aaT3TK8b5Q+HkTKnBDwRP0+kdJA63Hgm2RAYaR/e7n4l+ckMGi5qQjjBHjIpg8NEokhwnlbuC8UZyDHhlCmhNkVsxFVlIHptGxKKH6K/yfdeo00avWLo+rJ6ayOEtpFe+gAEdREJ+gctVEHMXSPHtEzerEerCfr1XqbRues2cwO+gHr8wv3xp/Q</latexit>tâœ“11<latexit sha1_base64="MRemQBgNbHQ/RhaZ9e0/B2c3Pvo=">AAACIHicdVDLSgMxFM34rPVVdekmWAQXUiZF2roT3bisYKswHYdMmrbBzIPkjlCG+RQ3/oobF4roTr/GTNsBFT0QOJxzbnJz/FgKDbb9Yc3NLywuLZdWyqtr6xubla3tro4SxXiHRTJS1z7VXIqQd0CA5Nex4jTwJb/yb89y/+qOKy2i8BLGMXcDOgzFQDAKRvIqzbQ3ucRRQ99N7Zpt24SQw5yQZsM25Pi4VSetDDxyk/ZgxIF69SzzKtUijIswLsKY5JZBFc3Q9irvvX7EkoCHwCTV2iF2DG5KFQgmeVbuJZrHlN3SIXcMDWnAtZtOdsvwvlH6eBApc0LAE/X7REoDrceBb5IBhZH+7eXiX56TwKDlpiKME+Ahmz40SCSGCOdt4b5QnIEcG0KZEmZXzEZUUQam07Ipofgp/p906zXSqNUvjqonp7M6SmgX7aEDRFATnaBz1EYdxNA9ekTP6MV6sJ6sV+ttGp2zZjM76Aeszy/5TJ/R</latexit>tâœ“210<latexit sha1_base64="v7faQWrN6Lwo+a3I0mOFIOyq9hE=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMeiF48VTFtoY9lsN+3SzSbsToRS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O4W19Y3NreJ2aWd3b/+gfHjUNEmmGfdZIhPdDqnhUijuo0DJ26nmNA4lb4Wj25nfeuLaiEQ94DjlQUwHSkSCUbSSn/a8R7dXrrhVdw6ySrycVCBHo1f+6vYTlsVcIZPUmI7nphhMqEbBJJ+WupnhKWUjOuAdSxWNuQkm82On5MwqfRIl2pZCMld/T0xobMw4Dm1nTHFolr2Z+J/XyTC6DiZCpRlyxRaLokwSTMjsc9IXmjOUY0so08LeStiQasrQ5lOyIXjLL6+SZq3qXVZr9xeV+k0eRxFO4BTOwYMrqMMdNMAHBgKe4RXeHOW8OO/Ox6K14OQzx/AHzucPJUGOQQ==</latexit>p012ğ’•ğŸğœ½ğŸğ’•ğŸğœ½ğŸ1 where 0 < tÎ¸1
1 âˆˆ [tÎ¸2

two types of users and each type Î¸ âˆˆ {Î¸1, Î¸2} has a single
1 < tÎ¸2
belief threshold denoted by tÎ¸
1 < 1.
Consider a common prior belief p0
1 , 1] denoted by
node 1â€™s abscissa. Then, the defenderâ€™s prior utility ËœvD(p0
1)
is denoted by node 1â€™s ordinate. The defender can improve
the utility from node 1â€™s ordinate to at most node 4â€™s
ordinate by adopting the optimal generator Ï€ âˆ— âˆˆ Î  as
follows. Generator Ï€ âˆ— generates two signals s2 âˆˆ S and
s3 âˆˆ S with proper probabilities under different states
so that the userâ€™s posterior belief is node 2â€™s abscissa
when observing policy s2 and node 3â€™s abscissa when
observing s3. Based on the Bayesian plausibility condition
in Section II-D, the defenderâ€™s optimal posterior utility
VD(p0
1) can be represented as the linear interpolation of
the ordinates of nodes 2 and 3, i.e., node 4â€™s ordinate.
The same reasoning applies to all feasible common prior
1] âˆˆ âˆ†X , the
beliefs p0
defenderâ€™s optimal posterior utility VD(p0) is the concave
closure of her prior utility ËœvD(p0) and VD(p0) â‰¥ ËœvD(p0).
Although we need at least |S | = KM security policies
to represent all the permutations of actions under differ-
ent types, Fig. 5 shows that the defender can achieve
her optimal posterior utility by generating two different
security policies with proper probabilities when N = 2.
Proposition 5 generalizes the result to N > 2 and shows
that the generator only needs to generate a small number
of security policies to achieve her optimal posterior utility.
If ËœvD(p0) = VD(p0) and p0 is further an interior point of
any convex polytope C
{a1,Â·Â·Â· ,aM}, âˆ€al âˆˆ A , l âˆˆ {1, Â· Â· Â· , M},
then there exist inï¬nitely many credible generators that
achieve VD(p0).

1 âˆˆ [0, 1]. Therefore, for all [p0

1, 1 âˆ’ p0

Proposition 5 (Efï¬ciency of the Optimal Generator).
For any DG with common prior belief p0 âˆˆ âˆ†X , there
exist either one or inï¬nitely many optimal generators to
achieve the optimal posterior utility VD(p0). For each state
x âˆˆ X , there exists one optimal generator Ï€ âˆ—(Â·|x) âˆˆ âˆ†S
that generates at least KM âˆ’ N security policies with zero
probability.

Proof. Since COP under the benchmark case is a linear
program, the optimal solution is either unique or innu-
merable. If N = 2, the convex hull consists of pieces of
line segments where each line segment can be determined
uniquely by its two endpoints. If N = 3, the convex hull as
a polygon consists of ï¬nite pieces of triangles where each
triangle can be determined uniquely by its three endpoints.
We can extend to any ï¬nite N where the convex hull
consists of pieces of (N âˆ’ 1)-simplex where each piece
can be determined uniquely by N endpoints. Thus, for any
p0 âˆˆ âˆ†X , it requires at most N points to achieve VD(p0),
which corresponds to N distinct security policies.

Remark 2. Proposition 5 shows that the defender does not
need to apply all enforceable security policies to achieve

10

the optimal posterior utility; i.e., the optimal generator is
efï¬cient and generates at most N security policies for each
state x âˆˆ X .

We deï¬ne the trust margin under a credible generator
Ï€ âˆˆ Î  in Deï¬nition 7. The maximum trust margin is
achieved when the optimal generator Ï€ âˆ— âˆˆ Î  is applied.
The trust margin can be negative if generator Ï€ is not
well designed. However, the maximum trust margin is
non-negative as it is the difference between the defenderâ€™s
optimal posterior utility and prior utilities, i.e., VD(p0) âˆ’
ËœvD(p0). Based on whether the maximum trust margin
is zero or positive, Deï¬nition 8 deï¬nes the user to be
unmanageable or manageable.

Deï¬nition 7 (Trust Margin). We deï¬ne Â¯vD(Ï€, p0) âˆ’
ËœvD(p0) as the trust margin under the common prior belief
p0 âˆˆ âˆ†X and a credible generator Ï€ âˆˆ Î .

Deï¬nition 8 (Manageability). The user is manageable
(resp. unmanageable) under prior belief p0 if the maximum
trust margin is greater than (resp. equals) zero.

Intuitively, a user is manageable if he shares the same
utility with the defender but unmanageable if he has an
D âˆˆ R to represent the
opposite utility. We introduce Ï s
userâ€™s level of maliciousness. Theorem 2 investigates how
the userâ€™s level of maliciousness affects his manageability.

Theorem 2 (Manageability and Level of maliciousness).
Let the common prior belief be state-independent, i.e.,
bD(Î¸ |x) = Ë†bD(Î¸ ), âˆ€Î¸ âˆˆ Î˜, âˆ€x âˆˆ X , and two playersâ€™ utili-
ties be linearly dependent, i.e., there exist a scaling factor
D(x, Î¸ ) âˆˆ R, such that
D âˆˆ R and translation factors Ït
Ï s
D(x, Î¸ ), âˆ€x âˆˆ X , Î¸ âˆˆ Î˜, a âˆˆ
Ë†vD(x, Î¸ , a) = Ï s
D Ë†vU (x, Î¸ , a) + Ït
A . Then, the following two statements hold.
(a) The defenderâ€™s trust margin is zero for all p0 âˆˆ âˆ†X
D â‰¤ 0. The

and credible generators if and only if Ï s
optimal generator contains zero information.

(b) The defenderâ€™s trust margin is non-negative for all
p0 âˆˆ âˆ†X and credible generators if and only if
Ï s
D > 0. Moreover, the optimal generator contains full
information. If p0 is an interior point of the (N âˆ’ 1)-
simplex and there exists at least one Î¸ âˆˆ Î˜ under
which no actions dominate, then the defenderâ€™s trust
margin is positive.

D Ë†vU (x, Î¸ , aâˆ—

the given conditions,

ËœvD(p0) = E
Proof. Under
Î¸ âˆ¼Ë†bD
E
E
E
Î¸ (p0)) + Ït
D(x, Î¸ )] = Ï s
xâˆ¼p0[Ï s
Î¸ âˆ¼Ë†bD
xâˆ¼p0
D
E
Î¸ (p0))] + E
xâˆ¼p0[Ït
[ Ë†vU (x, Î¸ , aâˆ—
D(x, Î¸ )].
Proposition
Î¸ âˆ¼Ë†bD
1 has shown that E
xâˆ¼p0[ Ë†vU (x, Î¸ , aâˆ—
Î¸ (p0))]
is a PWLC
function of p0 for each Î¸ âˆˆ Î˜. Since Ë†bD(Î¸ ) â‰¥ 0, âˆ€Î¸ âˆˆ Î˜,
combination E
Î¸ (p0))]
the
is also PWLC. The term E
is a
Î¸ âˆ¼Ë†bD
linear function of p0. Thus,
ËœvD is a piece-wise linear
and concave (resp. linear) function of p0 if and only if

E
xâˆ¼p0[ Ë†vU (x, Î¸ , aâˆ—
E
xâˆ¼p0[Ït
D(x, Î¸ )]

linear

Î¸ âˆ¼Ë†bD

D < 0 (resp. Ï s
Ï s
D = 0). If ËœvD is concave or linear over
the entire belief region âˆ†X , its convex hull is itself.
Thus, VD(p0) = ËœvD(p0) for all p0 âˆˆ âˆ†X and any zero-
information generator is optimal. Similarly, ËœvD is PWLC
if and only if Ï s
D > 0, and any full-information generator
is optimal. If there exists at least one Î¸ âˆˆ Î˜ under which
no actions dominate, then ËœvD is strictly convex over the
entire belief region. Thus, we have VD(p0) < ËœvD(p0) when
p0 is an interior point of the (N âˆ’ 1)-simplex.

Theorem 2 shows that when two playersâ€™ utilities are
linearly dependent, the userâ€™s manageability depends on
the sign of the scaling factor Ï s
D rather than its value. Thus,
the userâ€™s level of maliciousness has a threshold impact on
the manageability and the threshold is 0.

B. Incentive Modulator and Trust Manipulator

We illustrate the modulator design and the manipulator
design in Section IV-B1 and IV-B2, respectively. The
GMM mechanism design is presented in Section IV-B3.

1) Joint Design of Generator and Modulator: The
modulator incentivizes unmanageable users and increases
the security and efï¬ciency of the networks. Under the
binary state N = 2, Fig. 5 illustrates the defenderâ€™s prior
utility with the modulator in orange solid lines. The
orange solid lines are different from the blue ones in
two folds. From the userâ€™s perspective,
the modulator
changes the userâ€™s expected utility under different ac-
tions and thus results in translations of the dashed lines
in Fig. 3. Those translations change the belief region
partition, e.g., the right shifts of tÎ¸1
in Fig. 5.
1
From the defenderâ€™s perspective, the modulator modiï¬es
her utility in each new belief regions, and the value
xâˆ¼p0E
of the modiï¬cation is E
Î¸ (p0))]. If the
defenderâ€™s belief is independent of state, i.e., bD(Î¸ |x) =
Ë†b(Î¸ ), âˆ€Î¸ âˆˆ Î˜, âˆ€x âˆˆ X , then the defenderâ€™s utility change
E
Î¸ (p0))] = Î³E
xâˆ¼p0E
is a
constant with respect to p0 in each new belief region.
When the state space is binary as shown in Fig. 5, it means
that designing c introduces translations but not rotations
to each segment of the function ËœvD.

Î¸ âˆ¼bD(Â·|x)[Î³c(aâˆ—

Î¸ âˆ¼bD(Â·|x)[Î³c(aâˆ—

Î¸ âˆ¼Ë†bD(Â·)[c(aâˆ—

and tÎ¸2
1

Î¸ (p0))]

The joint design of the modulator and the generator
results in the new convex hull denoted by the dashed
blue lines in Fig. 5. Based on both playersâ€™ perspectives,
the optimal design needs to strike a balance between
incentivizing users to change their belief region partitions
and the costs to provide the incentives. Take Fig. 5 as an
example, we observe that the modulator incurs costs to the
defender for all actions, i.e., c(a) â‰¤ 0, âˆ€a âˆˆ A . Thus, in
all three belief regions, the defenderâ€™s prior utilities with
the modulator, represented by the solid orange lines, are
lower than the ones without the modulator, represented
by the solid blue lines. However, the beneï¬t of the userâ€™s
incentive change outweighs the costs; i.e., the defenderâ€™s

11

optimal posterior utility VD(p0
blue to node 4 in orange.

1) increases from node 4 in

2) Joint Design of Generator and Manipulator: The
manipulator directly distorts the userâ€™s prior belief to elicit
desirable behaviors. When the generator cannot be de-
signed, the manipulator design is equivalent to the process
of ï¬nding the initial belief p0
g := arg maxp0âˆˆâˆ†X ËœvD(p0)
that achieves the global maximum of the prior utility ËœvD.
Proposition 6 proves the existence of the optimal distorted
belief p0
g.

Proposition 6. For any given Ë†vD, Ë†vU of two players, there
g âˆˆ âˆ†X at the boundary of the
exists an initial belief p0
{a1,Â·Â·Â· ,aM }, âˆ€al âˆˆ A , l âˆˆ {1, Â· Â· Â· , M}, such
convex polytopes C
that p0

g = arg maxp0âˆˆâˆ†X ËœvD(p0).

Proof. For each Ë†vD, Ë†vU , the global maximum ËœvD(p0
g) =
maxp0âˆˆâˆ†X ËœvD(p0) exists and has a ï¬nite value due to The-
orem 1. Proposition 5 shows that the global maximum is
either unique or inï¬nite. In either case, at least one global
maximum is at the boundary of the convex polytopes
due to the piece-wise linear property stated in Proposition
4.

When the optimal generator is applied, the joint de-
sign of the manipulator and the generator is equiva-
Â¯p0
g :=
to the process of ï¬nding the initial belief
lent
arg maxp0âˆˆâˆ†X VD(p0) that achieves the global maximum
of VD. Based on the piece-wise linear property of ËœvD in
Proposition 4, the prior utility ËœvD and its concave closure
VD share the same global maximum. Thus, p0
g and
the optimal generator contains zero information. Take Fig.
5 as an example, p0
1 ] achieves the global
maximum denoted by node 2â€™s ordinate, and node 2 is
on both the solid and the dashed lines. These results are
summarized in Theorem 3.

1 , 1 âˆ’ tÎ¸2

g = [tÎ¸2

g = Â¯p0

Theorem 3. The design of optimal overt manipulator
changes the common initial belief p0 into p0
g. The de-
fenderâ€™s optimal posterior utility has the value of ËœvD(p0
g) =
g) and is independent of the initial belief p0 âˆˆ âˆ†X . In
VD(p0
the joint design of the overt manipulator and the generator,
the optimal generator contains zero information.

g = Â¯p0

3) Design of the GMM Mechanism: We incorporate the
modulator design into the joint design of the generator
and the manipulator to complete the GMM mechanism
design. Based on the analysis in Section IV-B2, the ï¬rst
step of the GMM design is to determine the optimal
modulator câˆ— âˆˆ C that results in the prior utility function
with the largest value of the global maximum, i.e., câˆ— =
arg maxc[maxp0âˆˆâˆ†X ËœvD(p0)]. With the given modulator câˆ—,
the second step of the design is to reduce the problem to
the joint design of modulator and manipulator presented
in Section IV-B2.

Remark 3 (Separation Principle). The two-step design of
the GMM mechanism shows that the defender can design
the optimal modulator câˆ— âˆˆ C independently.

We identify the equivalence principle in Remark 4 based
on the results in Theorem 3. If the overt manipulator allows
the defender to manipulate the initial belief arbitrarily, then
the optimal generator contains zero information; i.e., the
defender no longer needs the optimal generator to achieve
her optimal posterior utility. Note that the equivalence
principle does not mean that the generator is redundant.
When the belief manipulation is not arbitrary and under
practical constraints (e.g.,
the belief changes within a
limited range), the joint design of the two components
can yield better performance than the single design of the
manipulator.

Remark 4 (Equivalence Principle). For any given modu-
lator c âˆˆ C , the joint design of the generator and the overt
manipulator results in the same outcomes as the single
design of the overt manipulator does.

V. CASE STUDY

In Section V, we illustrate how the defender can use
the DG to mitigate insider threats where honeypots are
conï¬gured adaptively to detect and deter misbehavior.

A. Model Description

We have Î˜ = {Î¸ b, Î¸ g}, X = {xH , xN}, and A =
{aDO, aAC} based on the running example introduced in
Section II-A, Example 1, and Example 2. The true per-
centage of honeypots p0,H
D := b(xH ) âˆˆ [0, 1], is only known
to the SOC. Thus, the insidersâ€™ perceived honeypot per-
centage p0,H
U := bU (xH |Î¸ ) âˆˆ [0, 1], âˆ€Î¸ âˆˆ Î˜, can be different
from the true percentage.

Table III lists the utilities of the SOC and the insiders.
The column represents the binary state of a node, and
the row represents the insidersâ€™ actions. In each matrix
entry, we list the payoffs resulting from the selï¬sh (resp.
adversarial) insiders on the left (resp. right) of the semi-
colon. When the insider chooses not to access a node,
we calibrate the payoffs to be 0 for both the SOC and
the insiders. The other four possible scenarios are listed
as follows. First, a selï¬sh insiderâ€™s access to a normal
server maintains the organizationâ€™s normal operation and
results in a positive reward rD > 0 (resp. rU > 0) on
the selï¬sh insider).
average to the organization (resp.
Second, when an adversarial insider accesses a normal
server, he disrupts the normal operation and compromises
conï¬dential data, which brings him a reward of Ï† N
U rU > 0
and incurs a security loss of Ï† N
D rD < 0 to the organization.
Third, if an adversarial insider accesses a honeypot, he is
detected and prohibited from data theft. Meanwhile, the
SOC obtains valuable threat intelligence. We use Ï† H
D > 0

12

and Ï† H
U < 0 to represent the degrees of the SOCâ€™s gain
and the adversarial
insiderâ€™s loss, respectively. Finally,
once a selï¬sh insider accesses the honeypot, the SOC
has to quarantine the insider and investigate the incident,
which incurs a suspension of normal services as well as
an investigation cost. Meanwhile, the selï¬sh insider also
receives penalties and additional security training sessions.
We use Ï† g
U rU < 0 to represent the cost for
the SOC and the selï¬sh insider, respectively.

DrD < 0 and Ï† g

Selï¬sh Î¸ g; Adversarial Î¸ b
No Access aDO
Access aAC

Honeypot xH
0 ; 0

riÏ† g
i

; riÏ† H
i

Normal Server xN
0 ; 0
ri ; riÏ† N
i

TABLE III: Two playersâ€™ utilities vi(x, Î¸ , a), i âˆˆ {D,U}.

Compared to a computing system that precisely fol-
lows its instructions, human insiders alter their behaviors
the
in response to (dis)incentives. In this case study,
(dis)incentives refer to the insiderâ€™s authentication cost
c(aAC) := rU Ï† 0 to access a node, where the ratio Ï† 0 âˆˆ R
takes the value of 0 in the default setting. We assume
that the SOC can increase (i.e., Ï† 0 < 0) or decrease (i.e.,
Ï† 0 > 0) an insiderâ€™s authentication cost at no additional
cost, i.e., Î³ = 0. The revenues, losses, and costs can be
quantiï¬ed in dollars and their values vary for different
security scenarios.

1) Threshold Policy Analysis:

the selï¬sh and the adversarial

In this case study,
insiders share the same
both selï¬sh and adversarial
prior belief p0,H
U âˆˆ [0, 1]. Hence they share the same
posterior belief denoted by pH
U âˆˆ [0, 1] and adopt
the following threshold policies. Deï¬ne the decision
thresholds of
insid-
ers as tg(Ï† 0) := max{min{(1 âˆ’ Ï† 0)/(1 âˆ’ Ï† g
U ), 1}, 0} and
U âˆ’ Ï† 0)/(Ï† N
tb(Ï† 0) := max{min{(Ï† N
U ), 1}, 0}, respec-
tively. Since both denominators are positive, i.e., 1 âˆ’ Ï† g
U >
1 and Ï† N
U > 0, the selï¬sh insider (resp. the adversarial
insider) chooses to access a node if and only if the node
is unlikely to be a honeypot, i.e., pH
U <
tb(Ï† 0)). If a selï¬sh (resp. adversarial) insider accesses a
D (Ï† g
node, his expected utility rU (1 âˆ’ Ï† 0 + p0,H
U âˆ’ 1)) (resp.
U ))) decreases linearly in p0,H
rU (Ï† N
D (Ï† H
D ,
i.e., the true percentage of honeypots.

U < tg(Ï† 0) (resp. pH

U âˆ’ Ï† 0 + p0,H

U âˆ’ Ï† H

U âˆ’ Ï† N

U âˆ’Ï† H

Since the selï¬sh and adversarial insiders share the same
insider information, the difference in their decision thresh-
olds results purely from their incentive misalignment.
Given the insidersâ€™ utility matrices, the SOC can change
their incentives and elicit desirable behaviors by a proper
design of the authentication cost determined by the ratio
Ï† 0. If Ï† 0 â‰¤ Ï† g
U < 0), then the selï¬sh
(resp. adversarial) insider chooses aAC for all security
scenarios. If Ï† 0 â‰¥ 1 (resp. Ï† 0 â‰¥ Ï† N
U > 0), then the selï¬sh
(resp. adversarial) insider chooses aDO for all security
scenarios. Since the deceptive honeypot conï¬guration can
possibly change insidersâ€™ behaviors only if Ï† 0 is in the

U < 0 (resp. Ï† 0 â‰¤ Ï† H

U , Ï† H

U ), max(1, Ï† N

region [min(Ï† g
U )], we refer to the region
as the incentivized region of Ï† 0. As a special case of
Proposition 2, Corollary 1 shows that security policies
s{aDO,aAC} and s{aAC,aDO} cannot be both enforceable for
any node in the corporate network.
Corollary 1. If Ï† g
U < 0, then for all Ï† 0 âˆˆ R
U > 0, Ï† H
and credible conï¬guration Ï€ âˆˆ Î , either Ï€(s{aDO,aAC}|x) =
0, âˆ€x âˆˆ {xH , xN}, or Ï€(s{aAC,aDO}|x) = 0, âˆ€x âˆˆ {xH , xN}.

U < 0,Ï† N

B. Numerical Results

D = âˆ’1, and Ï† N

Following the insider categorization in Section II-A1,
we re-weight the percentage from the VCDB and adopt
qg := bD(Î¸ g|x) = 0.32 and qb := bD(Î¸ b|x) = 0.68 for all
x âˆˆ {xN, xH } as the benchmark value of the insidersâ€™ type
statistics. Based on the analysis in Section V-A1, the values
of rU do not affect the insidersâ€™ actions, and the value
of rD only scales the SOCâ€™s utility by a constant. Thus,
we normalize rU = rD = 1. We consider Ï† g
U = Ï† g
D = âˆ’0.3,
Ï† H
U = âˆ’Ï† N
U = âˆ’Ï† H
D = 0.9 as the benchmark
values. Then, the selï¬sh insider has the same utility as the
SOC, i.e., vD(x, Î¸ g, a) = vU (x, Î¸ g, a), âˆ€x âˆˆ {xH , xN}, âˆ€a âˆˆ
{aAC, aDO}, while the adversarial insider has an exactly
opposite utility to the one of the SOC, i.e., vD(x, Î¸ b, a) =
âˆ’vU (x, Î¸ b, a), âˆ€x âˆˆ {xH , xN}, âˆ€a âˆˆ {aAC, aDO}. In Section
V-B1, the SOC cannot change the authentication cost, i.e.,
c(aAC) = 0. In Sections V-B1 and V-B2, the insider has
the correct prior belief of the honeypot percentage, i.e.,
U = p0,H
p0,H
D .
1) Security Posture under the Optimal Generator:
Fig. 6a shows how the SOCâ€™s normalized revenue ËœvD
without the optimal generator is affected by the percent-
ages of honeypots and the selï¬sh insiders, respectively.
The maximum (resp. minimum) value of ËœvD is achieved
when insiders are all selï¬sh (resp. adversarial) and no
honeypots are applied. The two decision thresholds tb(0)
and tg(0) divide the percentage of honeypots into three
regions, i.e., high, medium, and low, in which the insidersâ€™
behaviors and the SOCâ€™s normalized revenue ËœvD have
different characteristics.

If the intended security outcomes are not achieved due
to the insidersâ€™ misbehavior, the SOC can apply the opti-
mal generator to elicit desirable behaviors and reduce the
cyber risks of the organization. To illustrate the effective-
ness of the optimal generator, we plot the maximum trust
margin in Fig. 6b. Fig. 6b corroborates Theorem 2; i.e.,
when all insiders are adversarial (resp. selï¬sh), no (resp.
all) credible generators, including the optimal one, can
improve the SOCâ€™s normalized revenue for any percentage
of honeypots p0,H
D âˆˆ [0, 1]. The ï¬‚at region represented
D âˆ’ Ï† H
by qg âˆˆ [0, (Ï† N
D âˆˆ
[0, min(tb(0),tg(0))] identiï¬es two critical thresholds. On
the one hand, we refer to (Ï† N
D âˆ’ Ï† H
D )
as the insiderâ€™s motive threshold that is used to quantify

D )] and p0,H

D âˆ’ 1 + Ï† N

D âˆ’ 1 + Ï† N

D )/(Ï† g

D )/(Ï† g

D âˆ’ Ï† H

D âˆ’ Ï† H

13

(a) Prior utility ËœvD.

(b) Maximum trust margin.

Fig. 6: SOCâ€™s utilities vs. p0,H

D âˆˆ [0, 1] and qg âˆˆ [0, 1].

the average motive of the entire insider population. If
the percentage of adversarial insiders exceeds the mo-
tive threshold, then insidersâ€™ behaviors are on average
destructive to the organization. On the other hand, we
refer to min(tb(0),tg(0)) as the deterrence threshold that
measures the adequacy of the honeypots. If the percentage
of honeypots is below the deterrence threshold, then the
SOC does not have a sufï¬cient number of honeypots
to create a credible threat for the insiders not to access
nodes in the corporate network. Based on Deï¬nition 8,
the insiders are unmanageable in the ï¬‚at region.

For the other regions, the insiders are manageable, and
the optimal generator can effectively reduce the cyber
risk of the organization. The increase depends on the
percentage of selï¬sh insiders and honeypots. When the
percentage of honeypots is tg(0) and insiders are all selï¬sh,
the organizationâ€™s revenue with the optimal generator
is 114 times higher than the one without the optimal
generator. Averaged over the entire region of qg âˆˆ [0, 1]
and p0,H
the organizationâ€™s revenue with the
optimal generator is 35.6% higher than the one without the
optimal generator. The results in Fig. 6 demonstrate that
the optimal generator design provides a constructive way
to quantify the accuracy of the information that the SOC
should reveal to the insiders to establish trust with them,

D âˆˆ [0, 1],

while in the meantime, retain her information advantage
to elicit desirable insider behaviors and maximize the or-
ganizationâ€™s well-being. These results provide a guideline
to address the challenges identiï¬ed in 2c and 2d of Table
2 in [31].

2) Security Posture under Various Modulators: In Sec-
tion V-B2, we investigate how the (dis)incentives affect the
insidersâ€™ behaviors and the security posture of the insider
network. In Fig. 7, we plot the decision thresholds of
selï¬sh and adversarial insiders in blue and red, respec-
tively. Since the blue line has a steeper slope than the
red line, Fig. 7 demonstrates that the same authentication
cost affects the selï¬sh insiders more signiï¬cantly than the
adversarial ones. As deï¬ned in Deï¬nition 5, two types
of insiders are identiï¬able under posterior belief pH
U if
pH
U âˆˆ [tb(Ï† 0),tg(Ï† 0)]. Furthermore, a larger difference in
the two thresholds, i.e., tg(Ï† 0) âˆ’tb(Ï† 0), indicates a higher
incentive misalignment between selï¬sh and adversarial
insiders.

Fig. 7: The adversarial and the selï¬sh insidersâ€™ decision
thresholds tb(Ï† 0) and tg(Ï† 0) in the red dashed line and the
blue solid line, respectively. The difference tg(Ï† 0)âˆ’tb(Ï† 0)
denoted in the black dotted line represents their utility
misalignment.

Fig. 8a illustrates the organizationâ€™s original payoff ËœvD
without a generator. The selï¬sh insider and the SOC
achieve a win-win situation at the region Ï† 0 âˆˆ [0.5, 0.74]
as they both achieve their maximum payoffs at that region.
The adversarial insider and the SOC cannot achieve a win-
win situation for all Ï† 0 âˆˆ R as adversarial insiders seeking
to compromise sensitive data and sabotage the organization
have a completely misaligned payoff structure. Fig. 8b
illustrates the organizationâ€™s improved payoff VD when
the optimal generator is applied. The results show that
the optimal generator can always increase the payoffs of
the selï¬sh insiders and the organization regardless of the
(dis)incentives represented by Ï† 0 âˆˆ R. Win-win situations
still exist (resp. do not exist) for the SOC and the selï¬sh
(resp. adversarial) insider.

3) Security Posture under the Covert and Overt Trust
In Section V-B3, the SOC can generate

Manipulators:

14

(a) Playersâ€™ prior utilities.

(b) Optimal posterior utilities.

Fig. 8: Utilities of the SOC, selï¬sh insiders, and adver-
sarial insiders in the dotted black, the solid blue, and the
dashed red lines, respectively.

and p0,H

U (cid:54)= p0,H

ambiguous or fake reports of the honeypot percentage so
that the insidersâ€™ initial beliefs of the honeypot percentage
deviate from the truth, i.e., p0,H
D . Figs. 9a and
the
9b illustrate the SOCâ€™s payoffs with and without
optimal generator, respectively, under different values of
p0,H
D . In Fig. 9a, the insidersâ€™ initial beliefs fall
U
into the following three regions. If p0,H
U âˆˆ [tg(0), 1], both
types of insiders choose not to access the node. Then,
the SOCâ€™s normalized payoff ËœvD is zero regardless of the
true percentage of honeypots p0,H
U âˆˆ [tb(0),tg(0)],
selï¬sh insiders choose aAC and adversarial insiders choose
aDO. Then, reducing the percentage of honeypots increases
the SOCâ€™s normalized payoff ËœvD as it reduces the false
alarm rate when selï¬sh insiders access the honeypots. If
p0,H
U âˆˆ [0,tb(0)], both types of insiders choose to access
the node. Then, reducing the percentage of honeypots also
increases the SOCâ€™s normalized payoff ËœvD. However, the
increase rate is lower than the one in the second region as
the two types of insiders take the same action and are not
identiï¬able.

D . If p0,H

D , p0,H

These results illustrate that without a deceptive genera-
tor, the SOC may not always beneï¬t from faking the per-
centage of honeypots. On the contrary, when the optimal
generator is applied in Fig. 9b, the SOC can beneï¬t from
a fake percentage of honeypots for all p0,H
U âˆˆ [0, 1].
Moreover, the beneï¬t of faking honeypot percentage is a
non-decreasing function of |p0,H
U |. Thus, the SOC
obtains a higher payoff VD with the optimal generator
when there is a larger mismatch between the true and
the fake percentages of honeypots. The maximum value
of VD is achieved when the true percentage of honeypots
is zero and the SOC makes the insiders believe that the
percentage of honeypots exceeds tb(0). Averaged over the
true percentage p0,H
U âˆˆ [0, 1],
the SOCâ€™s payoff with the optimal generator, i.e., VD is
59.3% higher than her original payoff ËœvD.

D âˆˆ [0, 1] and the fake one p0,H

D âˆ’ p0,H

-1-0.500.51Value of 000.20.40.60.81Decision Thresholds-1-0.500.51Value of 000.20.40.60.8Prior Utility-1-0.500.51Value of 000.20.40.60.8Optimal Posterior Utility15

design of the generator and manipulator into one single
design of the manipulator. We have applied the DG to
a case study where the defender dynamically conï¬gures
the honeypot to mitigate insider threats in a corporate
network. The numerical results have shown that the GMM
mechanism manages to elicit desirable actions from both
selï¬sh and adversarial insiders and reduce the cyber risk
of the organization. In particular, the optimal generator
itself can increase the defenderâ€™s payoff by 35.6% on
average. Equipped with the trust manipulator that fakes
the honeypot percentage, the optimal generator can further
increase the defenderâ€™s payoff by 59.3% on average.

REFERENCES

[1] S. Jajodia, A. K. Ghosh, V. Swarup, C. Wang, and X. S. Wang,
Moving target defense: creating asymmetric uncertainty for cyber
threats. Springer Science & Business Media, 2011, vol. 54.
[2] M. Bringer, C. Chelmecki, and H. Fujinoki, â€œA survey: Recent
advances and future trends in honeypot research,â€ International
Journal of Computer Network and Information Security, vol. 4,
no. 10, p. 63, 2012.

[3] E. Al-Shaer, J. Wei, W. Kevin, and C. Wang, Autonomous Cyber

Deception. Springer, 2019.

[4] â€œCyber deception signiï¬cantly reduces data breach costs & im-
proves soc efï¬ciency,â€ DECEPTIVE DEFENSE, INC., Tech. Rep.,
08 2020.

[5] S. Harris, â€œInsider threat mitigation guide,â€ Cybersecurity and

Infrastructure Security Agency, Tech. Rep.

[6] L. Spitzner, â€œHoneypots: Catching the insider threat,â€ in 19th
Annual Computer Security Applications Conference, 2003. Pro-
ceedings.

IEEE, 2003, pp. 170â€“179.

[7] R. J. Aumann, M. Maschler, and R. E. Stearns, Repeated games

with incomplete information. MIT press, 1995.

[8] E. Kamenica and M. Gentzkow, â€œBayesian persuasion,â€ American

Economic Review, vol. 101, no. 6, pp. 2590â€“2615, 2011.

[9] Y. Zhao, L. Huang, C. Smidts, and Q. Zhu, â€œFinite-horizon semi-
markov game for time-sensitive attack response and probabilistic
risk assessment in nuclear power plants,â€ Reliability Engineering
& System Safety, p. 106878, 2020.

[10] M. H. Manshaei, Q. Zhu, T. Alpcan, T. BacsÂ¸ar, and J.-P. Hubaux,
â€œGame theory meets network security and privacy,â€ ACM Comput-
ing Surveys (CSUR), vol. 45, no. 3, pp. 1â€“39, 2013.

[11] L. Huang, J. Chen, and Q. Zhu, â€œA large-scale markov game
approach to dynamic protection of interdependent infrastructure
networks,â€ in International Conference on Decision and Game
Theory for Security. Springer, 2017, pp. 357â€“376.

[12] J. Pawlick and Q. Zhu, Game Theory for Cyber Deception: From

Theory to Applications. Springer Nature, 2021.

[13] J. Pawlick, E. Colbert, and Q. Zhu, â€œModeling and analysis of leaky
deception using signaling games with evidence,â€ IEEE Transactions
on Information Forensics and Security, vol. 14, no. 7, pp. 1871â€“
1886, 2018.

[14] H. Sasahara and H. Sandberg, â€œEpistemic signaling games for cyber
deception with asymmetric recognition,â€ IEEE Control Systems
Letters, vol. 6, pp. 854â€“859, 2022.

[15] L. Huang and Q. Zhu, â€œA dynamic games approach to proactive
threats in cyber-

defense strategies against advanced persistent
physical systems,â€ Comput. & Secur., vol. 89, p. 101660, 2020.

[16] â€”â€”, â€œA dynamic game framework for rational and persistent robot
deception with an application to deceptive pursuit-evasion,â€ IEEE
Transactions on Automation Science and Engineering, pp. 1â€“15,
2021.

[17] X. Feng, Z. Zheng, D. Cansever, A. Swami, and P. Mohapatra,
â€œA signaling game model for moving target defense,â€ in IEEE
conference on computer communications.

IEEE, 2017, pp. 1â€“9.

(a) Prior utility ËœvD.

(b) Optimal posterior utility.

Fig. 9: SOCâ€™s utilities vs. p0,H

D âˆˆ [0, 1] and p0,H

U âˆˆ [0, 1].

VI. CONCLUSION

In this work, we have presented a class of duplicity
games (DG) to design defensive deception mechanisms
for proactive network security. The deception mechanism
is referred to as the GMM mechanism as it consists of the
following three modular design components. The generator
provides users an appropriate amount of information to
procure different types of users to take actions that are fa-
vorable to the defender. The incentive modulator modiï¬es
the usersâ€™ utilities to make their incentives better aligned
with the defenderâ€™s. The trust manipulator makes use of
usersâ€™ trust to impart to them the initial beliefs that can
lead to desirable security outcomes.

We have formulated and analyzed the DG using math-
ematical programming and graphical approaches. It has
the defender requires at most N en-
been shown that
forceable security policies from the entire KM ones to
achieve the optimal security posture, which illustrates the
efï¬ciency of the GMM mechanism. We have proposed the
concept of trust margin to measure how difï¬cult it is for a
defender to elicit the desired behavioral outcome. A user is
unmanageable when the maximum trust margin is zero, as
no deceptive mechanisms can affect the userâ€™s behaviors.
We have identiï¬ed a separation principle for the modulator
design and an equivalence principle that turns the joint

[18] E. Cranford, C. Lebiere, C. Gonzalez, S. Cooney, P. Vayanos, and
M. Tambe, â€œLearning about cyber deception through simulations:
Predictions of human decision making with deceptive signals in
stackelberg security games.â€ in CogSci, 2018.

[19] H. Xu, R. Freeman, V. Conitzer, S. Dughmi, and M. Tambe,
â€œSignaling in bayesian stackelberg games.â€ in AAMAS, 2016, pp.
150â€“158.

[20] K. HorÂ´ak, Q. Zhu, and B. BoË‡sansk`y, â€œManipulating adversaryâ€™s
belief: A dynamic game approach to deception by design for
proactive network security,â€ in GameSec, 2017, pp. 273â€“294.
[21] P. Naghizadeh and M. Liu, â€œOpting out of incentive mechanisms:
A study of security as a non-excludable public good,â€ IEEE
Transactions on Information Forensics and Security, vol. 11, no. 12,
pp. 2790â€“2803, 2016.

[22] Y. Zhang, H. Zhang, S. Tang, and S. Zhong, â€œDesigning secure and
dependable mobile sensing mechanisms with revenue guarantees,â€
IEEE Transactions on Information Forensics and Security, vol. 11,
no. 1, pp. 100â€“113, 2016.

[23] J. Lu, Y. Xin, Z. Zhang, X. Liu, and K. Li, â€œGame-theoretic design
of optimal two-sided rating protocols for service exchange dilemma
in crowdsourcing,â€ IEEE Transactions on Information Forensics
and Security, vol. 13, no. 11, pp. 2801â€“2815, 2018.

[24] C. Jiang, Y. Chen, Q. Wang, and K. R. Liu, â€œData-driven auction
mechanism design in iaas cloud computing,â€ IEEE Transactions on
Services Computing, vol. 11, no. 5, pp. 743â€“756, 2018.

[25] Z. Zhang, S. He, J. Chen, and J. Zhang, â€œReap: An efï¬cient
incentive mechanism for reconciling aggregation accuracy and indi-
vidual privacy in crowdsensing,â€ IEEE Transactions on Information
Forensics and Security, vol. 13, no. 12, pp. 2995â€“3007, 2018.
[26] R. Zhang and Q. Zhu, â€œFlipIn : A game-theoretic cyber insurance
framework for incentive-compatible cyber risk management of
internet of things,â€ IEEE Transactions on Information Forensics
and Security, vol. 15, pp. 2026â€“2041, 2020.

[27] J. Chen and Q. Zhu, â€œSecurity as a service for cloud-enabled
internet of controlled things under advanced persistent
threats:
a contract design approach,â€ IEEE Transactions on Information
Forensics and Security, vol. 12, no. 11, pp. 2736â€“2750, 2017.
[28] Z. Rabinovich, A. X. Jiang, M. Jain, and H. Xu, â€œInformation
disclosure as a means to security,â€ in Proceedings of the 2015
International Conference on Autonomous Agents and Multiagent
Systems. Citeseer, 2015, pp. 645â€“653.

[29] S. Das, E. Kamenica, and R. Mirka, â€œReducing congestion through
information design,â€ in 2017 55th annual allerton conference on
communication, control, and computing (allerton).
IEEE, 2017,
pp. 1279â€“1284.

[30] K. HorÂ´ak, B. BoË‡sanskÂ´y, P. TomÂ´aË‡sek, C. Kiekintveld, and
C. Kamhoua, â€œOptimizing honeypot strategies against dynamic
lateral movement using partially observable stochastic games,â€
Computers & Security, vol. 87, p. 101579, 2019.

[31] A. P. Moore, W. Novak, M. Collins, R. Trzeciak, and M. Theis,
â€œEffective insider threat programs: understanding and avoiding
potential pitfalls,â€ Software Engineering Institute White Paper,
Pittsburgh, 2015.

[32] I. Kantzavelou and S. Katsikas, â€œA game-based intrusion detection
mechanism to confront internal attackers,â€ Computers & Security,
vol. 29, no. 8, pp. 859â€“874, 2010.

[33] â€œGame-theoretic modeling and analysis of insider threats,â€ Inter-
national Journal of Critical Infrastructure Protection, vol. 1, pp.
75â€“80, 2008.

[34] C. Joshi, J. R. Aliaga, and D. R. Insua, â€œInsider threat modeling:
An adversarial risk analysis approach,â€ IEEE Transactions on
Information Forensics and Security, vol. 16, pp. 1131â€“1142, 2021.
[35] W. A. Casey, Q. Zhu, J. A. Morales, and B. Mishra, â€œCompli-
ance control: Managed vulnerability surface in social-technological
systems via signaling games,â€ in Proceedings of the 7th ACM
CCS International Workshop on Managing Insider Security Threats,
2015, pp. 53â€“62.

[36] M. M. Yamin, B. Katt, K. Sattar, and M. B. Ahmad, â€œImplementa-
tion of insider threat detection system using honeypot based sensors
and threat analytics,â€ in Future of Information and Communication
Conference. Springer, 2019, pp. 801â€“829.

16

[37] R. Dahbul, C. Lim, and J. Purnama, â€œEnhancing honeypot deception
capability through network service ï¬ngerprinting,â€ in Journal of
Physics: Conference Series, vol. 801, no. 1.
IOP Publishing, 2017,
p. 012057.

[38] S. Morishita, T. Hoizumi, W. Ueno, R. Tanabe, C. GaËœnÂ´an, M. J.
van Eeten, K. Yoshioka, and T. Matsumoto, â€œDetect me if you. . .
oh wait. an internet-wide view of self-revealing honeypots,â€ in
2019 IFIP/IEEE Symposium on Integrated Network and Service
Management (IM).
IEEE, 2019, pp. 134â€“143.

[39] Verizon.

(2017) Vocabulary for event

recording and incident

sharing (veris). [Online]. Available: http://veriscommunity.net/
[40] L. Shi, Y. Li, T. Liu, J. Liu, B. Shan, and H. Chen, â€œDynamic
distributed honeypot based on blockchain,â€ IEEE Access, vol. 7,
pp. 72 234â€“72 246, 2019.

[41] G. Wagener, R. State, T. Engel, and A. Dulaunoy, â€œAdaptive
and self-conï¬gurable honeypots,â€ in 12th IFIP/IEEE International
Symposium on Integrated Network Management (IM 2011) and
Workshops.

IEEE, 2011, pp. 345â€“352.

[42] L. Huang and Q. Zhu, â€œAdaptive honeypot engagement through
reinforcement learning of semi-markov decision processes,â€ in In-
ternational Conference on Decision and Game Theory for Security.
Springer, 2019, pp. 196â€“216.

[43] N. C. Rowe, E. J. Custy, and B. T. Duong, â€œDefending cyberspace
with fake honeypots.â€ JCP, vol. 2, no. 2, pp. 25â€“36, 2007.

[44] P. Orlik and H. Terao, Arrangements of hyperplanes.

Springer

Science & Business Media, 2013, vol. 300.

Linan Huang (Sâ€™16) received the B.Eng. de-
gree (Hons.) in Electrical Engineering from
Beijing Institute of Technology, China, in 2016.
He is currently pursuing a Ph.D. degree at the
Laboratory for Agile and Resilient Complex
Systems, Tandon School of Engineering, New
York University, NY, USA. His research inter-
ests include dynamic decision-making of the
multi-agent system, mechanism design, artiï¬-
cial intelligence, security, and resilience for the
cyber-physical systems.

Quanyan Zhu (SMâ€™02-Mâ€™14) received B. Eng.
in Honors Electrical Engineering from McGill
University in 2006, M. A. Sc. from the Uni-
versity of Toronto in 2008, and Ph.D. from
the University of Illinois at Urbana-Champaign
(UIUC) in 2013. After stints at Princeton Uni-
versity, he is currently an associate professor
at the Department of Electrical and Computer
Engineering, New York University (NYU). He
is an afï¬liated faculty member of the Center
for Urban Science and Progress (CUSP) and
Center for Cyber Security (CCS) at NYU. His current research interests
include game theory, machine learning, cyber deception, and cyber-
physical systems.

