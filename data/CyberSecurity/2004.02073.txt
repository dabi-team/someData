Model-free Reinforcement Learning for
Non-stationary Mean Field Games

Rajesh K Mishra, Deepanshu Vasal, and Sriram Vishwanath

0
2
0
2

r
p
A
5

]

Y
S
.
s
s
e
e
[

1
v
3
7
0
2
0
.
4
0
0
2
:
v
i
X
r
a

Abstract— In this paper, we consider a ﬁnite horizon, non-
stationary, mean ﬁeld game (MFG) with a large population of
homogeneous players, sequentially making strategic decisions,
where each player is affected by other players through an
aggregate population state termed as mean ﬁeld state. Each
player has a private type that only it can observe, and a mean
ﬁeld population state representing the empirical distribution
of other players’ types, which is shared among all of them.
Recently, authors in [1] provided a sequential decomposition
algorithm to compute mean ﬁeld equillibrium (MFE) for such
games which allows for the computation of equilibrium policies
for them in linear time than exponential, as before. In this paper,
we extend it for the case when state transitions are not known, to
propose a reinforcement learning algorithm based on Expected
Sarsa with a policy gradient approach that learns the MFE
policy by learning the dynamics of the game simultaneously.
We illustrate our results using cyber-physical security example.

I. INTRODUCTION
There is an increasing number of applications today that
involve large scale interactions among strategic agents, such
as smart grid, autonomous vehicles, cyber-physical systems,
Internet of Things (IoT), renewable energy markets, electric
vehicle charging, ride sharing apps, ﬁnancial markets, crypto-
currencies and many more. Such applications can be modeled
through MFGs introduced by Huang et al [2] and Lasry and
Lions [3], where each player is affected by other players not
individually, but through a ‘mean ﬁeld’ that is an aggregate
of the population states.

Multi-agent systems have become ubiquitous owing to
their variety of applications. In the recent decade, several
multi-agent reinforcement learning (MARL) approaches have
been proposed to learn optimal strategies for the agents in
the system. However, these approaches scale poorly with
size and the dynamic nature of the environment makes the
learning of these strategies rather difﬁcult. In contrast, a
planning framework based on the mean ﬁeld approximation
has shown potential due to the remarkable property wherein
the agents decouple from one another with the increase in
their number. As such the agents interact only through the
mean ﬁeld which makes dynamics of the system tractable.
MARL systems work well with probabilistic models but fail
with large number of agents and this is where the mean ﬁeld
concept has proven useful.

mean ﬁeld approximation has been in use in game the-
ory applications to study large number of non-cooperative
players. In such games, given the mean ﬁeld states, players’
MFE strategies are computed backward recursively through
dynamic programming (or HJB equation in continuous time),
whereas given players’ strategies, mean ﬁeld states are com-
puted forward recursively using Mckean Vlasov equation (or

Fokker-Plank equation). Overall, MFE strategies and mean
ﬁeld states are coupled across time through a ﬁxed-point
equation. Recently, in [1], the author presented a sequential
decomposition algorithm that computes such equilibrium
strategies by decomposing this ﬁxed-point equation across
time, and reducing the complexity of this problem from
exponential to linear in time. It involves solving a smaller
ﬁxed-point equation for each time instant t.

The concept of a generalized MFG is discussed by authors
in [4], where they prove the uniqueness and existence of
Nash equillibrium (NE). They also propose a Q-learning
algorithm with Boltzmann policy and analyze its conver-
gence properties and complexity. In [5], the authors pro-
pose a posterior sampling based approach where each agent
samples a transition probability from a previous transition
and converges to the optimal oblivious strategy for MFGs.
Authors in [6] consider a multi-agent setting with agents
coupled by the average action of the agents. They show that
the mean ﬁeld Q algorithm that converges to the NE. In [7],
the authors propose reinforcement learning (RL) algorithms
for stationary MFGs that compute MFE and social-welfare
optimal solution strategies. In [8], the authors consider a
non stationary MFG and propose a ﬁctitious play iterative
learning algorithm to devise optimal strategies for mean ﬁeld
states. They argue that if the agent plays the best response to
the observed population state ﬂow, they eventually converge
to the NE.

Most prior research assume a full knowledge of the
dynamics of the system. They also assume a stationary mean
ﬁeld approximation while computing the MFE. In our paper,
however, we consider a non stationary MFG with no prior
knowledge of the system dynamics, to derive the optimal
policy as a function of the mean ﬁeld state. The main
contribution of our paper is an RL algorithm that solves
the ﬁxed point equation that was proposed by the authors
in [1] while learning the dynamics of the Markov decision
process (MDP) simultaneously. We prove the convergence of
our algorithm to the MFE analytically. Finally, we show con-
vergence to the mean ﬁeld equilibrium strategy for a stylized
problem of Malware Spread where the strategy derived from
our algorithm coincides with the optimal strategy obtained
assuming the full knowledge of the system.

The paper is structured as follows. In Section II, we
present our model. In Section III, we present the sequential
decomposition algorithm with backward recursion presented
in [1] to compute MFE of the game. In Section IV, we
present our reinforcement learning algorithm and prove its
convergence to MFE polices. In Section VI, we present

 
 
 
 
 
 
a cyber-physical system security example. We conclude in
Section VII.

A. Notation

t , a2

t , . . . , ai−1

to mean (cid:0)a1

We use uppercase letters for random variables and low-
ercase for their realizations. For any variable, subscripts
represent time indices and superscripts represent player iden-
tities. We use notation −i to denote all players other than
the player i i.e. −i = {1, 2, . . . , i − 1, i + 1, . . . , N }. We
use notation at:t(cid:48) to represent the vector (at, at + 1, . . . , at(cid:48))
when t(cid:48) ≥ t or an empty vector if t(cid:48) < t. We use
(cid:1). We remove
a−i
t
superscripts or subscripts if we want to represent the whole
vector, for example at represents (a1
t ). We denote
the indicator function of any set A by 1{A}. For any ﬁnite
set S, P(S) represents space of probability measures on
S and |S| represents its cardinality. We denote by P σ (or
Eσ) the probability measure generated by (or expectation
with respect to) strategy proﬁle σ and the space for all such
strategies as Kσ. We denote the set of real numbers by R. All
equalities and inequalities involving random variables are to
be interpreted in a.s. sense.

t , . . . , aN

, . . . , aN
t

, ai+1
t

t

II. MODEL
We consider a ﬁnite horizon discrete-time large population
sequential game with N homogeneous players, where N
tends to ∞. We denote the set of homogeneous players
by N and the set of time periods by T . In each time
instant t ∈ T , a player i ∈ N observes a private type
xi
t ∈ X = {1, 2, · · · , Nx} and a common observation
of the mean ﬁeld population state zt ∈ Z, where zt =
(zt(1), zt(2), . . . , zt(Nx)) is the fraction of population hav-
ing type x ∈ X at time t given as.

t ∼ σi

: Z t × X t → P(A) deﬁned over the space
where σi
t
Kσ which implies Ai
1:t). In the case of a
ﬁnite time-horizon game, GT , each player wants to choose
the strategy ˜σ that maximizes its total expected discounted
reward over a time-horizon T , discounted by discount factor
0 < δ ≤ 1, given as

t(·|z1:t, xi

t = E˜σt:T
J ˜σ

(cid:34) T

(cid:88)

k=t

δk−tR (cid:0)X i

k, Ai

k, zk

(cid:35)

.

(cid:1) |zt, xi

t

(3)

III. PRELIMINARIES

A. Solution concept: Markov perfect equillibrium (MPE)

The Nash equilibrium of GT is deﬁned as strategies ˜σ =
t)i∈N ,t∈T that satisfy, for all i ∈ N ,

(˜σi

E˜σi,˜σ−i

(cid:34) T

(cid:88)

t=1

δt−1R (cid:0)X i

t , Ai

t, Zt

(cid:35)

(cid:1)

≥ Eσi,˜σ−i

δt−1R (cid:0)X i

t , Ai

t, Zt

(cid:35)

(cid:1)

.

(4)

(cid:34) T

(cid:88)

t=1

For sequential games, however, a more appropriate equi-
librium concept is MPE [9], which is used in this paper.
We note that although a Markov perfect equilibrium is also
a Nash equilibrium of the game, the opposite might not be
true always. An MPE (˜σ) satisﬁes sequential rationality such
that for GT , ∀i ∈ N , ∀t ∈ T , ∀hi
t is the
space of all possible mean ﬁeld trajectories till time t, and
∀σi ∈ Kσ,

t, where Hi

t ∈ Hi

(cid:34) T

(cid:88)

E˜σi ˜σ−i

δn−tR (cid:0)X i

n, Ai

n, Zn

(cid:35)

(cid:1) |z1:t, xi

1:t

n=t

≥ Eσi ˜σ−i

(cid:34) T

(cid:88)

δn−tR (cid:0)X i

n, Ai

n, Zn

(cid:1) |z1:t, xi

1:t

(cid:35)

.

(5)

zt(x) =

1
N

N
(cid:88)

i=1

1

{xi

t=x}

(1)

n=t

B. A solution concept: MFE

i=1 zt(i) = 1. Consequently,

with (cid:80)Nx
the player i takes
an action ai
t ∈ A = {1, 2, · · · , Na} based on policy
(cid:1) which is
(cid:1), and receives a reward R (cid:0)xi
(cid:0).|z1:t, xi
t, ai
σi
t
t, action ai
a function of its current type xi
t and the common
observation zt. Player i’s type evolves as a controlled Markov
process,

t, zt

t

(cid:1) .

(cid:0)xi

(2)

t, ai

t, ai

t, zt, wi
t

xi
t+1 = fx
The random variables (wi
t)i∈N ,t∈T are assumed to be mu-
tually independent across players and across time. We can
also express the above update of xi
through a kernel,
t
t+1 ∼ τ (·|xi
xi
t, zt), where τ (·|·) represents the transition
probabilities of the MDP. In this paper, we assume that τ is
unknown. We develop a model-free RL algorithm to derive
the equilibrium policy where τ is used to simulate the model
and generate samples for learning. The idea of MFGs is
to approximate the ﬁnite population game with an inﬁnite
population game such the the mean ﬁeld state zt converges
to the statistical MFE of the game [7].

In any time period t, player i observes (z1:t, xi

takes action ai

t according to a behavioral strategy σi = (σi

1:t) and
t)t,

MFE can be deﬁned as the combination of the optimal
policy ˜σ := {˜σt}t∈T and the mean ﬁeld states z := {zt}t∈T
that satisfy the following:

1) A policy ˜σ for some z1:T such that

(cid:34) T

(cid:88)

E˜σi

δk−tR (cid:0)X i

k, Ai

k, Zk

(cid:35)

(cid:1) |z1:T , xi

1:t

k=t

≥ Eσi

(cid:34) T

(cid:88)

δk−tR (cid:0)X i

k, Ai

k, Zk

(cid:1) |z1:T , xi

1:t

(cid:35)

.

(6)

k=t

2) A function Φ[z] : Sz → 2Sσ such that

Φ (z) = {˜σ ∈ Sσ : ˜σ is optimal for z}

(7)

3) A mapping Λ : Sσ → Sz with ˜σ ∈ Sσ, z = Λ (˜σ) is

constructed recursively as,

zt+1(y) =

(cid:88)

xt,at

τ (y|x, a, zt) ˜σt(a|zt, xt)zt(dxt) (8)

Then, the pair (˜σ, z) can be called a MFE which is a good
approximation for the MPE when the population grows large.

C. A methodology to compute MFE

In this section, we summarize the backward recursive
methodology based on sequential decomposition that would
be used to compute the MPE. It is worth noting that, [1]
provides a sequential decomposition algorithm used for the
case when model of the MDP us known. Here, we modify
the algorithm so that it could be used for the model-free
case as well. We consider Markovian equilibrium strategies
of player i which depend on the common information zt at
time t, and on its current private type xi
t. Equivalently, player
i takes action of the form Ai
t). Similar to the
common agent approach [10], an alternate and equivalent
way of deﬁning the strategies of the players is as follows. We
consider a common ﬁctitious agent that views the common
information zt and generates a prescription function γi
:
t
X → P(A) as a function of zt through an equilibrium gener-
t : Z → (X → P(A)) such that γi
ating function θi
t[zt].
Then action Ai
t is generated by applying this prescription
t on player i’s current private information xi
function γi
t, i.e.
t(·|zt, xi
t(·|xi
t ∼ γi
Ai
We are only interested in symmetric equilibria of such
t) i.e. there is no

t ∼ γt(·|xi
games such that Ai
dependence of i on the strategies of the players.

t) = θt[zt](·|xi

t). Thus Ai

t[zt](·|xi

t(·|zt, xi

t) = θi

t ∼ σi

t ∼ σi

t = θi

t).

For a given symmetric prescription function γt = θ[zt], the
statistical mean ﬁeld zt evolves according to the discrete-time
McKean Vlasov equation, ∀y ∈ X :

zt+1(y) =

(cid:88)

(cid:88)

x∈X

a∈A

zt(x)γt(a|x)τ (y|x, a, zt) ,

(9)

which implies

zt+1 = φ (zt, γt) .

(10)

D. Backward recursive algorithm for GT

This section summarizes the proposed a novel model-free
algorithm to compute the optimum policy function ˜θt as
a function of mean ﬁeld zt where equilibrium generating
function (˜θt)t∈[T ] is deﬁned as ˜θt : Z → {X → P(A)},
and for each zt, we generate ˜γt = ˜θt[zt]. In addition,
we generate an action value function Qt deﬁned as Qt :
Π × Z × X × A → R that captures the expected sum of
returns at time t following certain action from a state and
then continuing with the optimal policy ˜σt+1 from time t+1
onwards. As per our knowledge, this is the ﬁrst attempt to
solve a ﬁxed point equation using the action value function
in a model-free algorithm and determining the corresponding
equilibrium policy when the mean ﬁeld is non-stationary. The
algorithm can be summarized as follows:

1) Initialize ∀zT +1, xi

T +1 ∈ X , Ai

T +1 ∈ A, ˜γT +1 ∈ Π,

a) Compute Qt, ∀xi

t ∈ X , ∀ai

t ∈ A, and ∀˜γt ∈ Π

as,

Qt(zt, xi

t,at, ˜γt)
δE(cid:2)Vt+1

(cid:52)

= R (cid:0)zt, xi
(cid:0)zt+1, X i

t, ai
t

t+1

(cid:1) +
(cid:1) |zt, xi

t

(cid:3),

(13)

where the expectation in (13) is with respect
to the random variable X i
t+1 through the mea-
sure τ (cid:0)xi
(cid:1). The mean state zt+1 =
t+1|xi
φ (zt, ˜γt).

t, ai

t, zt

b) Set ˜θt[zt] = ˜γt, where ˜γt is the solution to the
t ∈ X and

following ﬁxed-point equation at all xi
∀i ∈ N
(cid:0)·|xi

(cid:1) ∈

˜γt

t

arg max
t)
γt(·|xi

Eγt(·|xi

t) (cid:2)Qt

(cid:0)zt, xi

t, Ai

t, ˜γt

(cid:1) |zt, xi

t

(cid:3)

(14)

c) The value function Vt is computed ∀xi

t through the measure γt(ai

where expectation in (14) is with respect to ran-
t|xi
dom variable Ai
t).
t ∈ X as,
(cid:1) |zt, xi
(cid:3)
t
(15)

(cid:1) = E˜γt(·|xi

(cid:0)zt, xi

(cid:0)zt, xi

t) (cid:2)Qt

t, Ai

t, ˜γt

Vt

t

˜σi
t

(cid:0)ai

(cid:1) = ˜θ[zt] (cid:0)ai

Then, an equilibrium strategy is deﬁned as
t|xi
t|z1:t, xi
t
1:t

(16)
where ˜θ[zt] = ˜γt. The proof for the existence of the solution
to the ﬁxed point equation in (14) which is the MPE of the
game has already been provided in [1] and will be revisited
when we prove the convergence of our algorithm.

(cid:1) ,

IV. REINFORCEMENT ALGORITHM

In this section, we describe our proposed RL algorithm
that computes the optimal policy which maximizes the ex-
pected sum of returns as speciﬁed in (3). The optimal policy
˜σ is deﬁned for a discretized set of mean states z ∈ Z, given
as ˜θ [z]. At any time t, and for any current mean state zt ∈ Z,
˜θt [zt] maps to a function ˜γt that prescribes the probabilistic
t, an agent i should take, given the state xi
action ai
t.

We implement

the RL algorithm based on Expected
Sarsa [11] and without the explicit knowledge of the the
t+1|xi
transition probabilities τ (xi
t). The algorithm basi-
cally computes the Qt-values at each instant and then learns
the optimal policy ˜γt by solving the ﬁxed point equation
in (14). We use Expected Sarsa to update the Qt values,
from the current reward rt and Vt+1, the value at the future
state. This update can be expressed as

t, ai

Qt (zt, xt, at, ˜γt) = (1 − α) Qt

α (cid:0)rt + δVt+1

(cid:0)zt, xi

(cid:1) +
t, ai
t
(cid:0)zt+1, xi

t+1

(cid:1)(cid:1)

(17)

VT +1

(cid:2)zT +1, xi
T +1
˜θT +1 [zT +1]

(cid:3) (cid:52)

= 0,
(cid:52)
= 0.

(11)

(12)

2) For t = T, T − 1 . . . 1 and ∀zt, ˜θt [zt] is generated

through the following steps.

The Q-values are a function not only of the states and
the actions but also of the current mean ﬁeld state zt and
the current optimal policy ˜θ [zt] = ˜γt. The current optimal
policy and current mean state determine the next mean state
(cid:1)
zt+1 which determines the value function Vt+1
t+1
at the future state. Therefore, the functions Qt is deﬁned

(cid:0)zt+1, xi

t+1

(cid:0)zt+1, xi

over all possible equilibrium policies ˜γt (·|xt) ∈ Π, where
Π is the space of all possible strategies from a given
(cid:1) is determined
state xt. The value function Vt+1
using functional approximation. In addition, due to the non-
stationarity of the mean states, the equation in (14), which
is used to solve for the optimal policy, is not just a single
step optimization, but a ﬁxed point equation which needs
to be solved with repeated iterations. In our paper, we use
a policy gradient approach to solve for the optimal policy
at each mean state and for every time iteration repeatedly.
The entire RL algorithm described here is summarized in
Algorithm 1.

At each time instant t,

the policy iteration algorithm
computes the equilibrium policy based on the action value
function Qt through a policy gradient approach. In other
words, the solution to the ﬁxed point equation in (14) is the
policy where Qt has the highest gradient. Given that the the
function Qt is a function of the optimal policy itself, the
new found policy changes the Qt. Therefore, this process
is repeated over several iterations in order to arrive at the
required prescription function ˜γt. This is repeated at all the
mean states zt ∈ Z so that we get the ﬁnal equilibrium
function ˜θ [z].

V. CONVERGENCE

In this section, we prove the convergence of the proposed
RL algorithm to the equilibrium strategy of the statistical
MFG. Using backward recursion and sequential decomposi-
tion, the RL algorithm is able to arrive at the equilibrium
strategy for player i at each time t. In other words, we
show that the a player i has no incentive to deviate from the
equilibrium strategy ˜σ given that the other players are playing
the equilibrium strategy. Before proving convergence, we
establish two lemmas related to the main theorems. In the
ﬁrst lemma we show that the value function Vt captures the
expected sum of rewards accumulated by playing the ˜σ at
time t by the ith player. We follow it with the proof of the
next lemma establishing the optimality of the V value over
the expected sum of rewards accumulated by playing any
other strategy other than ˜σ.

Lemma 1: ∀t ∈ [T ], ∀zt, xi

t ∈ X ,

Vt

(cid:0)zt, xi

t

(cid:1) = E˜σt:T

(cid:34) T

(cid:88)

k=t

= J ˜σ
t

δk−tR (cid:0)xi

k, Ai

k, zk

(cid:35)

(cid:1) |zt, xi

t

(18)

(19)

where ˜σt is the equilibrium policy at time t and J ˜σ
is the
t
accumulated optimal returns from t till T by following the
equilibrium policy.

Algorithm 1: Equilibrium Policy
Input:
L: Batch Size for Sarsa
I: Policy Iterations
T: Time Length
Output:
˜σ: Optimal Policy

1 Initialize: VT +1
2 Initialize: ˜θT +1
3 for t = T. . . 1 do
4

for zt ∈ Z, ˜γt ∈ Π do

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

Next mean state: zt+1 ∼ φ (zt, ˜γt)
Optimum policy: ˜γt+1 = ˜θt+1 [zt+1]
for l = 1, 2, . . . L do

for (cid:0)xi

(cid:1) ∈ X × A do
l+1 ∼ τ (cid:0).|xi

l, ai
l
Sample: xi
Sarsa Target:
l, ai
G = R(xi
(cid:1) =
(cid:0)zt, xi
l, ai
l, ˜γt
Qt
(cid:0)zt, xi
(1 − α) Qt

l, ai

l, ai

l, zt

(cid:1)

(cid:1) + αG

l, ˜γt

l, zt) + δVt+1

(cid:0)zt+1, xi

l+1

(cid:1)

end

end

end
for z ∈ Z do

for n = 1 . . . I do

˜γn = PG (Qt (zt, ·, ·, ˜γn−1))
Increment: n = n + 1

end
˜θ [zt] = ˜γn

end
for z ∈ Z do

for xi ∈ X do
˜γ = ˜θ [zt]
Vt

(cid:0)zt, xi(cid:1) = E˜γ (cid:2)Qt

end

(cid:0)zt, xi, Ai, ˜γ(cid:1)(cid:3)

end

27
28 end
29 ˜σ = ˜θ[zt] ∀zt
Result: ˜σ

which is true from (13). For any mean state zt, ˜γt =
˜σt (·|·, zt), therefore, we have,

VT

(cid:0)zT , xi

T

(cid:1) = E˜γT (cid:2)R (cid:0)xi

T , Ai
T

(cid:1)(cid:3) .

(22)

which is the maximum returns the agents can receive at t =
T because ˜γt is the solution to the ﬁxed point equation in (14)
that maximizes (22).

Now assuming that the proposition is true for t = t + 1,

Proof: We prove the lemma using the theory of

we get,

mathematical induction.
At t = T , from (15),

Vt+1

VT

(cid:0)zT , xi

T

(cid:1) =E˜σT (cid:2)QT
=E˜σT (cid:2)R (cid:0)xi

(cid:0)zT , xi
T , Ai

T , Ai
T , zT

T , ˜σT (·|·, zt)(cid:1)(cid:3)
(cid:1)(cid:3) ,

= E˜σt+1:T

(20)

(21)

(cid:0)zt+1, xi

(cid:1) = J ˜σ

t+1

t+1
(cid:34) T

(cid:88)

k=t+1

δk−(t+1)R (cid:0)xi

k, Ai

k, zk

(cid:1) |zt+1, xi

t+1

(23)
(cid:35)

(24)

Vt

(cid:0)zt, xi

At time t = t, we have,
(cid:1) =E˜σt (cid:2)Qt
=E˜σt[R (cid:0)xi
δVt+1

(cid:0)zt, xi
t, zt, Ai
t
(cid:0)zt+1, xi

t

t, Ai

t+1

t, ˜σt (·|·, zt)(cid:1) |zt, xi
(cid:1) +
(cid:1) |zt, xi
t]

t

(25)

(cid:3)

(26a)

(26b)

(26a) is from the deﬁnition in (15) while (26b) is from the
deﬁnition in (13). Using the assumption in (24), we get,

(cid:34)

Vt

(cid:0)zt, xi

t

(cid:1) = E˜σt

R (cid:0)xi

t, Ai

t, zt

(cid:1) +

δE˜σt+1:T

(cid:34) T

(cid:88)

k=t+1

δk−(t+1)R (cid:0)xi

k, zk

(cid:1) |zt+1, xi

t+1

(cid:35)

Proof: We prove it through the technique of mathe-
matical induction and will use the results that were proved
before in Lemma 1 and Lemma 2.

For the base case, we consider t = T . The expected sum
of returns, when the player i follows the equilibrium policy
˜σ is given as

JT = E˜σT (cid:2)R (cid:0)xi
(cid:0)zT , xi

= VT

T , Ai
(cid:1) ,

T

T , zT

(cid:1) (cid:12)
(cid:12)z1:T , xi

1:T

(cid:3)

which is true from Lemma 1.

Now, from Lemma 2, we have,
(cid:0)zT , xi
T , Ai

T (cid:2)QT
T (cid:2)R (cid:0)xi

VT ≥Eσi
=Eσi

T ,˜σ−i

T ,˜σ−i

T , Ai
T , ˜γT
(cid:1) (cid:12)
(cid:12)z1:T , xi
T , zT

(cid:1) |zt, xi
t
(cid:3)

1:T

(cid:3)

(34)

(35)

(36)

(37)

(cid:35)

|zt, xi
t

(27)

Assuming that the condition in (33) holds at t = t + 1, we
get,

Using the the expression for expected sum of returns in (3),
we get,

Vt

(cid:0)zt, xi

t

(cid:1) = E˜σt [R (cid:0)xi

t, zt, Ai
t

(cid:1) + δJt+1]

(28)

(cid:34) T

(cid:88)

E˜σ

k=t+1

≥ Eσi ˜σ−i

δk−t−1R (cid:0)xi

k, Ai

k, zk

(cid:35)

(cid:1) (cid:12)
(cid:12)z1:t+1, xi

1:t+1

(cid:34) T

(cid:88)

k=t+1

δk−t−1R (cid:0)xi

k, Ai

k, zk

(cid:35)

(cid:1) (cid:12)
(cid:12)z1:t+1, xi

1:t+1

(38)

(cid:3)

(29)

We need to prove that the expression in (33) holds for t = t
as well. Let us represent the left hand side of (33) as L, i.e.

L = E˜σ

(cid:34) T

(cid:88)

n=t

δk−tR (cid:0)X i

k, Ai

k, Zk

(cid:35)

(cid:1) (cid:12)
(cid:12)z1:t, xi
1:t

(39)

The expectation at t + 1 is independent of the rewards at
time t, therefore, we can rewrite (39) as

(cid:1).

L = E˜σt

(cid:34)
R (cid:0)xi

t, ai

t, zt

(cid:1) +

Lemma 2: ∀i ∈ N ,t ∈ [T ],∀zt, xi
(cid:0)zt, xi

(cid:1) ≥ Eσi

t (cid:2)Qt

(cid:0)zt, xi

t,˜σ−i

Vt

t ∈ X , ai
t, Ai

t, ˜γt

t ∈ A,
(cid:1) |zt, xi

t

t

where zt+1 = φ (zt, ˜σt (·|zt, ·)).

Proof: Given that at time t + 1 the equilibrium policy
is ˜σt+1 is the solution to the ﬁxed point equation in (14),
let us assume that the player i plays a different policy (cid:98)σt+1
such that ∀xi
(cid:0)·|xi

(cid:1) |zt, xi

(cid:0)zt, xi

E (cid:2)Qt

(30)

(cid:3)

t, Ai

t, ˜γt

t+1

t

(cid:98)γt

t+1 ∈ X ,
(cid:1) /∈ arg max
γt
(cid:0)·|xi

(cid:1) = (cid:98)γt

t

t

with (cid:98)σt

(cid:0)·|zt, xi

Now, the equation on the other side of the inequality in(29)
can be changed assuming (cid:98)σ as the suboptimal policy as
follows:

Eσi
=E(cid:98)γi

t,˜σ−i

t ,˜σ−i

t (cid:2)Qt
t (cid:2)Qt

(cid:0)zt+1, X i
(cid:0)zt, xi
t, Ai

t , Ai
t, ˜γt

t, ˜γt
(cid:1) (cid:12)
(cid:12)zt, xi

(cid:1) (cid:12)
(cid:12)zt, xi
t, ai
t

t, ai
t
(cid:3)

(cid:3)

(31a)

Considering (30), we can proceed as,

E(cid:98)γi
≤E˜γi
=Vt

t ,˜σ−i

t (cid:2)Qt
t (cid:2)Qt
(cid:1)

t ,˜σ−i
(cid:0)zt, xi

t

(cid:0)zt, X i
t , Ai
(cid:0)zt+1, X i

t, ˜γt
t , Ai

(cid:1) (cid:12)
t, ai
(cid:12)zt, xi
t
(cid:1) (cid:12)
(cid:12)zt, xi

t+1, ˜γt

(cid:3)

t, ai
t

(cid:3)

(32a)

(32b)

where the last statement is from the deﬁnition in (13).

Theorem 1: A strategy (˜σ) constructed from the above

algorithm is an MFE of the game. i.e

δk−tR (cid:0)X i

k, Ai

k, Zk

(cid:35)

(cid:1) (cid:12)
(cid:12)z1:t, xi
1:t

E˜σ

(cid:34) T

(cid:88)

k=t

≥ Eσi,˜σ−i

δk−tR (cid:0)X i

k, Ai

k, Zk

(cid:35)

(cid:1) (cid:12)
(cid:12)z1:t, xi
1:t

(33)

(cid:34) T

(cid:88)

k=t

δE˜σt+1:T

(cid:34) T

(cid:88)

k=t+1

δk−t−1R (cid:0)X i

k, Ai

k, Zk

(cid:35)(cid:35)

(cid:1) (cid:12)
(cid:12)z1:t+1, xi

1:t+1

(40)

Using Lemma 1, and the deﬁnition of Vt in (15) we get,
(cid:1) + δVt+1
t, ˜σt (·|zt, ·)(cid:1)(cid:3)

(cid:0)z1:t+1, xi

t, zt
t, ai

L =E˜σt (cid:2)R (cid:0)xi
=E˜σt (cid:2)Qt
(cid:0)zt, xi
=Vt

t, ai
(cid:0)zt, xi
(cid:1)

1:t+1

(cid:1)(cid:3)

(41a)

(41b)

(41c)

t

Now, from Lemma 2,

L ≥Eσi
=Eσi

t,˜σ−i

t,˜σ−i

t (cid:2)Qt
t (cid:2)R(xi

(cid:0)zt, xi
t, Ai

t, Ai
t, ˜γt
t, zt) + δVt+1

(cid:1) |zt, xi

t

(cid:3)

(cid:0)zt+1, X i

t+1

(cid:1) |zt, xi

(42a)
(cid:3)

t
(42b)

which is true as per the deﬁnition in (13). The expression can
now be expanded using Lemma 1 and use the assumption we
made in (38).
(cid:34)

L ≥Eσi

t,˜σ−i

t

R(zt, xi

t, ai

t)+

(cid:34) T

(cid:88)

δE˜σ

δk−tR (cid:0)xi

k, Ai

k, zk

(cid:35) (cid:35)

(cid:1) |zt, xi

t

(43a)

k=t
(cid:34)
R(zt, xi

t, ai

t)+

≥Eσi

t,˜σ−i

t

δEσi ˜σ−i

(cid:34) T

(cid:88)

k=t+1

δk−t−1R (cid:0)xi

k, Ai

k, zk

(cid:35) (cid:35)

(cid:1) (cid:12)
(cid:12)z1:t+1, xi

1:t+1

=Eσi,˜σ−i

(cid:34) T

(cid:88)

k=t

δk−tR (cid:0)X i

k, Ai

k, zk

(cid:1) (cid:12)
(cid:12)z1:t, xi

1:t, ai
1:t

(43b)

(cid:35)

(43c)

Theorem 2: Let ˜σ be an MFE of the mean ﬁeld game.
Then there exists an equilibrium generating function ˜θ that
satisﬁes (14) in the backward recursion algorithm such that
˜σ is deﬁned using ˜θ.

Proof: Given that ˜σ is the MFE of the game, the

equilibrium generating function such that

˜θ[z] = ˜σ (·|z, ·)

∀z

(44)

At time t, let us assume that ˜γt
be the prescription
function that gives the equilibrium action but does not satisfy
the ﬁxed point equation in (14), i.e.

(cid:16)

= ˜θ [zt]

(cid:17)

˜γt /∈ arg max

γt

Eγt (cid:2)Qt

(cid:0)zt, xi

t, Ai

t, ˜γt

(cid:1)(cid:3)

(45)

Let us assume that (cid:98)γt be a solution to the ﬁxed point equation
in the RL algorithm, then
(cid:0)zt, xi

(cid:1)(cid:3) ≥ E˜γt (cid:2)Qt

E(cid:98)γt (cid:2)Qt

(cid:0)zt, xi

(46)

(cid:1)(cid:3)

t, Ai

t, ˜γt

t, Ai

t, ˜γt

But, we know
E(cid:98)γt (cid:2)Qt
= E(cid:98)σt (cid:2)Rt

(cid:0)zt, xi
(cid:0)xi

t, Ai
t, Ai
(cid:34) T

(cid:88)

t, ˜γt
t, zt

(cid:1)(cid:3)
(cid:1) + Vt+1

(cid:0)zt+1, xi

t+1

(cid:1)(cid:3)

(47a)

(47b)
(cid:35)

= E (cid:98)σt,˜σt+1:T

δn−tR (cid:0)X i

n, Ai

n, Zn

(cid:1) (cid:12)
(cid:12)z1:t, xi

1:t, ai
1:t

n=t

≥ E˜γt (cid:2)Qt
(cid:0)zt, xi
= Vt

t

(cid:0)zt, xi
(cid:1)

t, Ai

t, ˜γt

(cid:1)(cid:3)

(47c)

(47d)

(47e)

which is contradictory on the consideration that ˜σ is a MPE
of the game.

Assumption 1: Let the reward function R (cid:0)xi

t, ai

(cid:1) and
t, zt
(cid:1) be continuous

the state transition matrix τ (cid:0)xi
functions in zt.

t+1|xi

t, ai

t, zt

Theorem 3: Under the assumption 1, there exists a solu-

tion to the ﬁxed point equation in (14).

Proof: It has already been shown that when the reward
function is bounded, there exists a MFE for ﬁnite horizon
games. It can also been shown that the solutions to the ﬁxed
point equations are MFE of the game. Thus there exists an
equilibrium policy for the game achieved at each time t.

VI. NUMERICAL EXAMPLE

A. Security of cyber-physical system: Malware Spread

We consider a security problem in a cyber physical net-
work with positive externalities. It is a discrete version of the
malware problem presented in [12], [13], [14], [15]. Some
other applications of this model include ﬂu vaccination, entry
and exit of ﬁrms, investment, network effects. In this model,
we suppose there are large number of cyber-physical nodes
where each node has a private state xi
t ∈ {0, 1} where xi
t = 0
represent ‘healthy’ state and xi
t = 1 is the infected state.
t ∈ {0, 1}, where ai
Each node can take an action ai
t = 0
implies “do nothing” whereas ai
t = 1 implies “repair”. The
dynamics(= Qx (·|·)) are given by

(cid:40)

xi
t+1 =

t + (1 − xi
xi
0

t)wi
t

for ai
for ai

t = 0
t = 1

where wi

t ∈ {0, 1} is a binary valued random variable with
P (wi
t = 1) = q representing the probability of a node getting
infected. Thus if a node doesn’t do anything, it could get
infected with certain probability, however, if it takes repair
action, it comes back to the healthy state. Each node gets a
reward

r(xi

t, ai

t, zt) = −(k + zt(1))xi

t − λai
t.

(48)

where zt(1) is the mean ﬁeld population state being 1 at
time t, λ is the cost of repair and (k + zt(1)) represents
the risk of being infected. We pose it as an inﬁnite horizon
discounted dynamic game. We consider parameters k =
0.2, λ = 0.5, δ = 0.9, q = 0.9 for numerical results presented
in Figures 1-3.

Fig. 1. γ(1|0): Probability of choosing action 1, given xi = 0

The learning parameter α for the sarsa update was set at
0.1. The overall length of the time-horizon T was chosen to
be 60 iterations long.

Figure 1 and Figure 2 show the equilibrium policies ˜γ
at different values of the mean state zt for states xi
t = 0
and xi
t = 1. The plotted graphs are the probabilities with
which we choose action ai
t = 1. The plots of our algorithm
are compared across the true strategy that was obtained by
assuming the knowledge of the dynamics of MDP and then
solving the ﬁxed point equation. The strategies estimated

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.6Our AlgorithmTrue Optimal[10] A. Nayyar, A. Mahajan, and D. Teneketzis, “Decentralized Stochastic
Control with Partial History Sharing: A Common Information Ap-
proach,” IEEE Transactions on Automation Control, vol. 58, no. 7,
2013.

[11] H. Van Seijen, H. Van Hasselt, S. Whiteson, and M. Wiering, “A
theoretical and empirical analysis of expected sarsa,” in 2009 IEEE
Symposium on Adaptive Dynamic Programming and Reinforcement
Learning.

IEEE, 2009, pp. 177–184.

[12] M. Huang and Y. Ma, “Mean ﬁeld stochastic games with binary
actions: Stationary threshold policies,” in 2017 IEEE 56th Annual
Conference on Decision and Control (CDC).
IEEE, 2017, pp. 27–32.
[13] ——, “Mean ﬁeld stochastic games: Monotone costs and threshold
policies,” in 2016 IEEE 55th Conference on Decision and Control
(CDC).

IEEE, 2016, pp. 7105–7110.

[14] ——, “Mean ﬁeld stochastic games with binary action spaces and

monotone costs,” arXiv preprint arXiv:1701.06661, 2017.

[15] L. Jiang, V. Anantharam, and J. Walrand, “How bad are selﬁsh invest-
ments in network security?” IEEE/ACM Transactions on Networking,
vol. 19, no. 2, pp. 549–560, 2010.

Fig. 2. γ(1|1): Probability of choosing action 1, given xi = 1

using the proposed RL algorithm coincides with the true
strategies establishing the accuracy of our algorithm.

VII. CONCLUSION

We considered a ﬁnite horizon discrete-time sequential
MFG with inﬁnite homogeneous players. The players had
access to their private type and the common information of
mean population state. A ﬁxed point decomposition method
was suggested in an earlier paper that computes the equilib-
rium strategy at different mean states but with the knowledge
of the dynamics of the MDP. Here, we proposed a RL
algorithm that employs Expected Sarsa to learn the dynamics
of the game and solve the ﬁxed point equation, iteratively, to
arrive at the equilibrium strategy. In the end, we implement
our algorithm on a practical cyber-physical application to
demonstrate that the algorithm does converge to the same
optimal policy that was obtained when dynamics of the game
was known. We also analytically show the convergence of
our algorithm to the MFE of the game. To the best of our
knowledge, this is the ﬁrst RL algorithm to learn optimal
policies of non-stationary mean ﬁeld games.

REFERENCES

[1] D. Vasal, “Signaling equilibria in mean-ﬁeld games,” pp. 1–21, 2019.
[2] M. Huang, R. P. Malham´e, P. E. Caines et al., “Large population
stochastic dynamic games: closed-loop mckean-vlasov systems and the
nash certainty equivalence principle,” Communications in Information
& Systems, vol. 6, no. 3, pp. 221–252, 2006.

[3] J.-M. Lasry, P.-L. Lions, J.-M. Lasry, and P.-L. Lions, “Mean ﬁeld

games,” Japan. J. Math, vol. 2, pp. 229–260, 2007.

[4] X. Guo, A. Hu, R. Xu, and J. Zhang, “Learning mean-ﬁeld games,” in
Advances in Neural Information Processing Systems, 2019, pp. 4967–
4977.

[5] N. Tiwari, A. Ghosh, and V. Aggarwal, “Reinforcement learning for

mean ﬁeld game,” arXiv preprint arXiv:1905.13357, 2019.

[6] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean ﬁeld
multi-agent reinforcement learning,” arXiv preprint arXiv:1802.05438,
2018.

[7] J. Subramanian, “Reinforcement

learning in stationary mean-ﬁeld
games,” Proceedings of
the 18th International Conference of Au-
tonomous Agents and Multi-Agent Systems, AAMAS’19, pp. 251–259,
2019.

[8] R. Elie, J. P´erolat, M. Lauri`ere, M. Geist, and O. Pietquin, “Ap-
proximate ﬁctitious play for mean ﬁeld games,” arXiv preprint
arXiv:1907.02633, 2019.

[9] E. Maskin and J. Tirole, “Markov perfect equilibrium. I. Observable
actions,” Journal of Economic Theory, vol. 100, no. 2, pp. 191–219,
2001.

00.10.20.30.40.50.60.70.80.910.60.650.70.750.80.850.90.951Our AlgorithmTrue Optimal