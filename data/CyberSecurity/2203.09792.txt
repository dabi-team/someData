AdIoTack: Quantifying and ReÔ¨Åning Resilience of
Decision Tree Ensemble Inference Models against
Adversarial Volumetric Attacks on IoT Networks

2
2
0
2

r
a

M
8
1

]

G
L
.
s
c
[

1
v
2
9
7
9
0
.
3
0
2
2
:
v
i
X
r
a

Arman Pashamokhtari, Gustavo Batista, and Hassan Habibi Gharakheili
UNSW Sydney, Australia
Emails: {a.pashamokhtari,g.batista,h.habibi}@unsw.edu.au

Abstract‚ÄîMachine Learning-based techniques have shown
success in cyber intelligence. However, they are increasingly
becoming targets of sophisticated data-driven adversarial attacks
resulting in misprediction, eroding their ability to detect threats
on network devices. In this paper, we present AdIoTack1, a
system that highlights vulnerabilities of decision trees against
adversarial attacks, helping cybersecurity teams quantify and
reÔ¨Åne the resilience of their trained models for monitoring and
protecting Internet-of-Things (IoT) networks. In order to assess
the model for the worst-case scenario, AdIoTack performs white-
box adversarial learning to launch successful volumetric attacks
that decision tree ensemble network behavioral models cannot
Ô¨Çag. Our Ô¨Årst contribution is to develop a white-box algorithm
that takes a trained decision tree ensemble model and the proÔ¨Åle
of an intended network-based attack (e.g., TCP/UDP reÔ¨Çection)
on a victim class as inputs. It then automatically generates recipes
that specify certain packets on top of the indented attack packets
(less than 15% overhead) that together can bypass the inference
model unnoticed. We ensure that the generated attack instances
are feasible for launching on Internet Protocol (IP) networks
and effective in their volumetric impact. Our second contribution
develops a method to monitor the network behavior of connected
devices actively, inject adversarial trafÔ¨Åc (when feasible) on behalf
of a victim IoT device, and successfully launch the intended
attack. Our third contribution prototypes AdIoTack and validates
its efÔ¨Åcacy on a testbed consisting of a handful of real IoT
devices monitored by a trained inference model. We demonstrate
how the model detects all non-adversarial volumetric attacks on
IoT devices while missing many adversarial ones. The fourth
contribution develops systematic methods for applying patches to
trained decision tree ensemble models, improving their resilience
against adversarial volumetric attacks. We demonstrate how our
reÔ¨Åned model detects 92% of adversarial volumetric attacks.

I.

INTRODUCTION

IoT adoption is on the rise in both consumer and business
mainstreams. Still, more than half of the connected IoT devices
are found vulnerable [36] to a wide range of sophisticated
cyber threats like botnets, malware, phishing, or DDoS attacks.
According to a report recently published by Nokia [34], IoTs
saw a 100% increase in infections in 2020 over the previous
year. These vulnerabilities at scale can lead to signiÔ¨Åcant
disruption of critical enterprise operations [35], [48], [11] or
exÔ¨Åltration of sensitive data [29]. The lack of a real-time and
detailed inventory of connected IoT assets, deployed in large
numbers, leads enterprises to operate their network partially
blind [41], [19]. This leaves vulnerable devices unmonitored
and hence exposes their organization to grave risks [2].

1Funding for this project was provided by CyAmast Pty Ltd.

While manufacturers are assumed (expected) to embed
appropriate safeguards in the devices for securing them, many
IoT devices have shown [36], [34] to be unprotected and can be
compromised with little effort from attackers. This paper ad-
vocates ‚Äúnetwork-level‚Äù security measures, instead of ‚Äúdevice-
level‚Äù [44]. We note that embedded security implementation
can be highly variable across various IoT devices depending
on manufacturers, device capabilities, and mode of operation.
Therefore, the network-level monitoring approach comes with
a number of advantages including: (a) it can be applied to a
range of heterogeneous IoT devices; (b) it can be implemented,
operated, and upgraded in the cloud by network operators with
no dependency to device manufacturers; and (c) it can augment
any device-level security implemented by the manufacturer,
providing an extra layer of protection.

Given the speed and complexity of modern cyber threats,
network security teams are increasingly applying machine
learning (ML) techniques to network trafÔ¨Åc (packets and/or
Ô¨Çows) of IoT devices to model their network behavior [12].
ML-based models [40] are used on the network (running on
general computers fed by trafÔ¨Åc features) to automatically clas-
sify assets from identiÔ¨Åable patterns in their network activity
and detect anomalous behaviors [18], [19], [42], indicative
of compromise, Ô¨Årmware upgrade, or emerging novel attacks.
Learning-based methods offer the ability to respond to situ-
ations not explicitly encountered before, replacing processes
that would have required formidable manual analysis by human
experts.

In the context of IoT cybersecurity, well-trained ML mod-
els have proven to effectively capture the intended behavior
of IoT devices on a per-type basis. These models can Ô¨Çag
deviations in the volume and/or frequency of network activity
without being impacted by limited patterns of certain known
attacks [33], [42], [18]. This approach is successful primarily
because IoT devices display a Ô¨Ånite set of activities (with
reasonably identiÔ¨Åable patterns) on the network during their
regular operation. These behavioral characteristics present an
opportunity to train models with purely benign instances
obtained from IoT network trafÔ¨Åc. The trained models would
have the ability to distinguish clearly the ‚Äúbounded‚Äù set
of benign behavior from an ‚Äúunbounded‚Äù set of malicious
(anomalous/unintended) behavior resulted from of a network-
based cyber attack. This gives a signiÔ¨Åcant advantage to ML-
based methods against traditional signature-based ones to infer
from IoT network trafÔ¨Åc.

 
 
 
 
 
 
Traditionally, the ‚Äúsecurity of ML-based models‚Äù has not
been the main objective for designing algorithms and de-
veloping inference systems in various domains, especially in
cybersecurity. Therefore, potential adversaries with certain in-
centives aim to subvert the ML model either during training or
operation which is known as adversarial attack. Attackers may
poison the training instances to inÔ¨Çuence the resulted model.
They may attempt to carefully manipulate network trafÔ¨Åc at
run-time to Ô¨Çip predictions, yielding a poor performance of
the inference model in distinguishing the malicious instances.

Several countermeasures have been proposed, under the
umbrella of Adversarial Machine Learning (AML), with practi-
cal successes in the area of machine vision and image recogni-
tion [47], [46], [13]. However, in the context of cybersecurity,
it is in its nascent stage of development to the best of our
knowledge. A recent report by McAfee [26] highlights a few
malware families that have bypassed machine learning engines
in 2018. It predicts how cyber-criminals will be increasingly
employing artiÔ¨Åcial intelligence techniques to evade detection.

The primary objective of adversarial cyber-attackers is to
Ô¨Ånd loopholes in ML-based network security models like
trafÔ¨Åc classiÔ¨Åers, anomaly detectors, or intrusion detection
systems. To fortify inference models against these targeted
attacks, cyber-security teams need to quantify the resilience
of their trained model and reÔ¨Åne any loopholes. This paper
focuses on developing techniques for launching volumetric
attacks (known as adversarial evasion attack) on IoT devices
without being noticed by a decision tree ensemble inference
that continuously monitors the network trafÔ¨Åc. To
model
make these ideas more concrete, let us consider a scenario
whereby an attacker aims to launch a TCP SYN reÔ¨Çection
attack (common in large-scale DDoS attacks sourced from IoT
devices [25]) with 1000 attack packets.

The attacker sends 1000 TCP SYN packets with spoofed
source IP address to a target IoT device which replies (reÔ¨Çects)
them to the victim destination (the source entity speciÔ¨Åed in the
SYN packets) with 1000 TCP SYN-ACK packets. IoT devices
typically generate a small amount of trafÔ¨Åc volume on their
limited set of TCP/UDP Ô¨Çows; thus, reÔ¨Çection attacks with
high rates can be detected by inference models relatively easily.
That said, attackers with sufÔ¨Åcient prior knowledge would
have the ability to launch their intended attack (without being
detected) by precisely generating and injecting some adver-
sarial network trafÔ¨Åc (say, 50 NTP packets along with 5 DNS
packets and an SSDP packet) in addition to the intended attack
packets (1000 TCP SYN packets). To humans, this additional
trafÔ¨Åc seems to be irrelevant and random; however, this well-
crafted ‚Äúrecipe‚Äù subverts the model‚Äôs internal decision-making,
leading it to accept this network trafÔ¨Åc instance as benign.
Finding these attack recipes would depend on the inference
model‚Äôs trafÔ¨Åc features and detection algorithm, which requires
a well-developed adversarial learning algorithm to generate
them precisely. It is important to note that this paper uses the
term ‚Äúattack‚Äù in two different contexts. First is the network-
based volumetric attack (e.g., TCP SYN reÔ¨Çection) on IoT
devices, whereby target IoT devices reÔ¨Çect the incoming attack
trafÔ¨Åc towards an external victim. Second is the adversarial
machine learning attack which is a technique that utilizes
prior knowledge to subvert the ML-based inference model, and
hence the volumetric attacks can go undetected.

In the literature, researchers have studied adversarial at-
tacks on IoT networks [16], [45], [20], [39], [6]. Prior works
primarily focus on developing adversarial models that can learn
how to bypass neural network-based attack detection models.
In contrast, the literature has not devoted much attention to
decision tree-based models. Unlike neural networks, decision
thus, well-known
tree-based models are not differentiable;
approaches like [17], [37], [10] are not compatible with their
structure. Additionally, given the dynamics of network trafÔ¨Åc
and some constraints on Internet Protocol (IP) packets, adver-
sarial attacks in the context of network security (unlike image
processing) become relatively more challenging. Another gap
in the current literature is that adversarial attacks, to the best
of our knowledge, have never been executed in a real network
protected by an ML-based inference model. Lastly, no prior
systematic attempt has been made to improve the resilience
of decision trees against these sophisticated attacks without
manipulating the training process which requires re-training
the model.

This paper presents AdIoTack, a systematic approach for
learning and launching adversarial attacks on IoT networks
protected by decision tree ensemble models. We make four
main contributions: (1) We develop a novel adversarial learning
algorithm named ofÔ¨Çine learning that automatically gener-
ates adversarial attack ‚Äúrecipes‚Äù given an intended volumet-
ric attack on a target class, subverting a trained decision
tree ensemble model (¬ßIII). We consider two representative
network-based volumetric attacks, namely TCP SYN and
SSDP reÔ¨Çection, which are widely used on IoT devices for
launching DoS/DDoS attacks [21], [25], [28], [32]. Adversarial
recipes specify certain overhead packets (2% for TCP SYN
reÔ¨Çection and 14% for SSDP reÔ¨Çection) to be injected for an
intended volumetric attack to go unnoticed; (2) We develop an
online execution method that launches the adversarial instances
(learned from ofÔ¨Çine learning) on a real IoT device network
monitored by a decision tree ensemble inference model (¬ßIV);
(3) We demonstrate the performance of AdIoTack by applying
it to our testbed comprising of nine consumer IoT devices. We
show our online attack execution has at least a 95% chance of
successfully launching an adversarial attack when the attacker
has the prior knowledge of the inference model cycles (¬ßV);
and, (4) We develop a method for patching decision trees
(without a need for re-training), making them more robust
against adversarial volumetric attacks while maintaining the
inference accuracy for benign trafÔ¨Åc (¬ßVI). Our reÔ¨Åned model
can detect all adversarial SSDP reÔ¨Çection attacks with low
(less than 200 attack packets per minute), medium (between
200 and 700 attack packets per minute), and high (greater than
700 attack packets per minute) impact rates. For adversarial
SYN reÔ¨Çection attacks, the reÔ¨Åned model detects 63% of low,
75% of medium, and 100% of high impact attacks.

II. RELATED WORK

Cyber Intelligence for IoT Infrastructure: The cyber-
security of IoT networks differs from that of non-IoT or
traditional IT networks where general-purpose computers,
smartphones, or tablets display an unbounded range of network
behaviors reÔ¨Çecting their users‚Äô online activity. IoT devices,
in contrast, have a limited range of activities (with slight
variations but predictable) in the trafÔ¨Åc pattern. While mod-
eling benign behavior of non-IoTs is complicated (or even

2

impossible), IoT devices‚Äô intended activity can be formally
deÔ¨Åned and enforced on the network. A ‚Äúwhitelist‚Äù of IoT
behaviors, speciÔ¨Åed in the form of MUD (Manufacturer Usage
Description) proÔ¨Åles, has been employed to detect anomalies
[19]. Researchers have employed various ML techniques to
learn from the benign behavior of IoT devices to monitor their
health and detect malicious incidents [33], [42], [18], [38].
The use of pure benign instances from IoT network trafÔ¨Åc
in training inference models, without the inclusion of known
attack (malicious) instances, enables them to become more
robust against unseen and morphing cyber-attacks.

Adversarial Attacks: Adversarial attacks were Ô¨Årst studied
in the context of image recognition, where the objective
is to create an adversarial copy of a benign image that is
indistinguishable to human eyes. However, the image classiÔ¨Åer
mispredicts the adversarial copy. Adversarial learning in cy-
bersecurity differs from that in the image recognition domain
in (1) in image recognition, the similarity of the adversarial
instance and the benign instance, expected to look identical
to human eyes,
is the primary constraint of the problem.
However, this constraint is relaxed in cybersecurity and trafÔ¨Åc
inference problems; (2) there is no requirement for the impact
or intensity of intended attacks in image recognition problems,
i.e., any adversarial instances (with any level of deviations from
the benign instance) are accepted as long as they subvert the
model; (3) Practicality of adversarial attacks (i.e., executing
them in a real environment) is of concerns in cybersecurity
since every adversarial instance (a set of numerical values)
may not be necessarily realized as network packets.

Adversarial attack on differentiable models like neural net-
works has been widely studied both in image recognition and
cybersecurity research problems, while non-differential mod-
els like decision tree-based models remain relatively under-
learning techniques (computing
explored. Also, adversarial
gradients of the model‚Äôs loss function) for differentiable mod-
els cannot be readily applied to non-differentiable models.
Work in [14] developed a method that replaces last few layers
of a neural network model with a Random Forest, hide the
gradients of the neural network and perhaps protecting it
against adversarial attacks.

These prior works have studied evasion attacks on decision
tree-based models [8], [22], [49]. Authors in [8] has shown
vulnerability of Gradient Boosting and Random Forest models
against adversarial evasion attacks using methods of [9], [22].
They also developed a robustness technique to avoid adver-
sarial attacks by bringing adversarial instances to the training
phase to change the ensemble model with this purpose that the
model still has high accuracy for attack inputs. Work in [22]
developed a mixed-integer linear programming (MIPS) tech-
nique for solving the adversarial attack optimization problem
on tree ensembles. All existing works, applied their methods
on public dataset like MNIST [27]; in contrast, to the best of
our knowledge, our paper is the Ô¨Årst in developing techniques
for adversarial attacks against tree ensembles in the context of
IoT cybersecurity.

Adversarial Attack Approaches in IoT: Existing works in
IoT cybersecurity primarily adopt techniques from the image
recognition domain with little adaptation and contextualization.
Hence, some of the fundamental challenges and differences
(discussed above) remain unaddressed. Authors of [6] employ

reinforcement learning (RL) to generate adversarial instances
against
their multi-class inference model (four classes of
cyberattacks plus a class of benign trafÔ¨Åc). In that work, the ad-
versary iteratively adds random noises to samples of an attack
dataset and presents them to the inference model until they get
classiÔ¨Åed as benign. However, randomly generated numerical
instances do not necessarily represent realistic cyberattacks.
Authors of [1] studied the impact of adversarial attacks on
models that detect malware-infected IoT applications. Their
neural network-based model infers from graph-like features
of the binary Ô¨Åle of applications. Work in [16] trained a set
of binary-class (malicious versus benign) inference models,
each per unit of IoT devices. The authors used GAN (Genera-
tive Adversarial Networks), a well-known adversarial learning
technique for neural networks (initially designed for image
classiÔ¨Åcation context). The primary objective is to improve
the resilience of their model during training by including
adversarial instances in the training set. Another work [20]
focused on neural networks for detecting network-based at-
tacks. Their detection is done by a multi-class inference model
(with four classes of cyberattacks and one class of benign
trafÔ¨Åc). Their objective was to manipulate numerical instances
of either attack or benign for which the model makes incorrect
predictions. The authors directly borrowed techniques from
image recognition and did not demonstrate the feasibility of
subverting the model on real network. Importantly, they do not
attempt to reÔ¨Åne the vulnerable inference model.

Our work distinguishes from the prior adversarial attack
studies on decision tree-based models by (a) inter-dependency
among many features in this paper versus tweaking only
one feature in prior works; (2) considering the impact of
the volumetric network attacks and their feasibility on real
networks; (3) existing works did not explore the sequence order
of decision trees to Ô¨Ånd adversarial instances. We, instead,
analyze how the order our approach processes the decision
trees in a forest model can affect the richness of adversarial
instances; (4) our robustness technique is done post-training
which does not require re-training the model and is not a
function of a set of known adversarial instances.

Prior works in IoT cybersecurity were directly adopted
from those in image recognition with their primary focus on
neural network models. In this paper, we demonstrate our
method of generating and launching adversarial attacks on a
live network of IoT devices. To the best of our knowledge,
no prior work has focused on decision tree-based models in
this context while considering the practical challenges; and
this is the Ô¨Årst time that an adversarial attack is executed on
a real network of IoT devices. Also, our patching techniques
systematically reÔ¨Åne trained decision trees to detect adversarial
volumetric attacks on data networks whereas previous works
[8], [5] targeted other application domains like image recog-
nition.

III. ADIOTACK: SYSTEM ARCHITECTURE AND
ADVERSARIAL LEARNING

This section describes our AdIoTack system,

including
major functional decisions and system components (¬ßIII-A),
ofÔ¨Çine learning (¬ßIII-B),
the core algorithms (¬ßIII-C) and
consistency veriÔ¨Åcation (¬ßIII-D).

3

(cid:0)

(cid:0)

(details discussed in ¬ßIII-B). For AdIoTack (shown by shaded
region), there are two essential phases, namely: (a) learning
and (b) execution (launching).

In the learning phase (steps

(cid:3)
(cid:3)
(cid:0)
(cid:1)2 in Fig. 1), the ofÔ¨Çine
(cid:1)1 and
(cid:2)
(cid:2)
learning module uses the inference model (note the white-
box scenario) to learn blind spots of the model to generate
(cid:3)
adversarial recipes (step
(cid:1)3 ). Each adversarial recipe consists
(cid:2)
of a number of conditions over features of the inference model
(e.g., 10 < f1, f2 ‚â§ 50, and 20 < f3 < 40); thus, each recipe
can generate several adversarial instances that conform to the
recipe‚Äôs conditions. For example, an adversarial instance for
the mentioned recipe could be f1 = 15, f2 = 30, and f3 = 25.
(cid:3)
For execution, network telemetry (step
(cid:1)4 ) similar to inputs of
(cid:2)
the inference model must be collected, so that an ‚Äúappropriate‚Äù
adversarial recipe can be selected based on the current state
of the network. To obtain real-time telemetry and execute the
intended adversarial attack, network trafÔ¨Åc of IoT devices is
snooped. As a part of our threat model, we assume a ‚Äúmalicious
agent‚Äù like an infected smartphone or a computer is already
inside the local network (the red box inside the IoT network
on the top left) for this purpose.

(cid:0)

To obtain network telemetry in real-time, the malicious
agent can passively (e.g., snifÔ¨Ång) or actively (e.g., man-in-
the-middle via ARP poisoning) monitor the behavior of victim
IoT devices. The passive approach is stealthier to perform but
could be practically challenging in some cases, like when the
malicious agent and its victims are on different physical access
mediums (wireless versus wired). For AdIoTack, we employ
the active mode for adversarial network monitoring. Also, our
method only analyzes packet metadata (headers), so it has no
issue with encrypted payloads.

In the next step, the online engine starts a search for feasi-
ble adversarial recipes, given the current state of the network.
From those candidate recipes, the online engine selects the
closest one to the current state of the network to minimize
the amount of adversarial packets (overhead) to be injected
(in real-time to the network trafÔ¨Åc) on behalf of the victim.
Eventually, the intended volumetric attack is launched (step
(cid:3)
(cid:1)5 ) by judiciously crafting network trafÔ¨Åc matching the chosen
(cid:2)
recipe. Note that the local victim will reÔ¨Çect/amplify the attack
trafÔ¨Åc onto an ultimate victim on the Internet. It is important to
note that the adversarial packets (overhead trafÔ¨Åc) are injected
on behalf of local victims for their reÔ¨Çected/ampliÔ¨Åed trafÔ¨Åc
to go undetected.

(cid:0)

B. OfÔ¨Çine Adversarial Learning

OfÔ¨Çine adversarial

learning is a process of generating
adversarial recipes which: (a) result
in desired malicious
impact, and (b) can go undetected i.e., the trafÔ¨Åc inference
model raises no anomaly Ô¨Çag. The results of ofÔ¨Çine adversarial
learning show to what extent the given model is vulnerable to
adversarial volumetric attacks.

We employ ML models‚Äô classiÔ¨Åcation score as a measure
for detecting behavioral changes. We deÔ¨Åne a classiÔ¨Åcation
score threshold for each class (device type) according to the
training results. At run-time, any instance with a classiÔ¨Åcation
score below the given threshold is Ô¨Çagged as an anomaly. We
can deÔ¨Åne the adversarial learning problem formally as below:

4

Fig. 1. System architecture of AdIoTack.

A. Threat Model: Functional Decisions and
System Components

The main objective of AdIoTack is to help cyber-security
teams quantify the resilience of their decision tree-based mod-
els against adversarial volumetric attacks that target integrity
of the inference models. Given a well-trained trafÔ¨Åc inference
model that is able to detect ‚Äúnon-adversarial‚Äù attacks relatively
easily, AdIoTack generates adversarial instances that the model
mispredicts as benign. Note that ML models may be tested for
other aspects like their availability, i.e., whether they remain
operational while being Ô¨Çooded by malicious requests [3],
which is beyond the scope of this paper. In terms of inÔ¨Çuence,
adversarial attacks may be either causative or exploratory
[3]. In causative attacks, training data instances are poisoned
in order to manipulate the inference logic. In exploratory
attacks which happens post-training, well-crafted malicious
trafÔ¨Åc instances that resemble benign instances are used to
bypass the model while launching an attack. Our focus in
AdIoTack is on exploratory attacks.

One can quantify the resilience of an ML model in three
different scenarios: (a) White-box is the worst-case scenario
when the knowledge of the model‚Äôs structure, algorithm, and
features is used to evaluate the model against adversarial
attacks ‚Äì testing and improving the model‚Äôs performance
by a white-box approach would result in the best resilience
[7]; (b) Grey-box, when the model is evaluated with limited
knowledge of the model (say, partial access to the training
data); (c) Black-box, when the model is attacked without any
prior knowledge, and it can only be probed e.g., through API
calls. This paper aims to help cyber-security teams evaluate
and reÔ¨Åne their decision tree-based models (self-assessment).
Therefore, we develop AdIoTack in a white-box setup to
maximise effectiveness in a worst-case scenario.

Fig. 1 shows the architecture of our AdIoTack system. On
the top left, we see a network of IoT devices (green boxes)
whose trafÔ¨Åc is monitored periodically (e.g., every one minute)
by a decision tree ensemble model (i.e., trees T1 to Tn). The
network router extracts the required telemetry (e.g., packet/byte
count of different trafÔ¨Åc Ô¨Çows) from the network trafÔ¨Åc of
each IoT device and passes it to the ML inference model
(bottom left). The model continuously infers the most probable
class of the connected devices from the received telemetry. We
use the classiÔ¨Åcation score of the ML model to compute a
device-speciÔ¨Åc threshold to distinguish benign behavior from
malicious. That way, if the model yields a score below the
is an indication for malicious behavior
given threshold,

it

IoT networkDev#ADev#BDev#CnetworktelemetryML inference modeloffline learningreal-time network telemetryattack commandonlinelaunching engineadversarialrecipes13254AdIoTackT1T2T3‚Ä¶Tnwithout the need for data normalization or scaling prior to
training; (iii) Random Forest algorithm is reasonably resilient
to the overÔ¨Åtting problem [4]; (iv) Decision tree-based models
often yield good results in terms of accuracy and explainability
which means their internal decision-making process is visible
during testing, which is highly desirable for cyber-analysts
(particularly for IoT network security [30], [15], [31], [40],
[43]) in taking remedial and/or preventive actions. In contrast,
competitors like neural networks do not provide insights
into their internal process of inference; (v) Random Forest
classiÔ¨Åers are more robust than k-nearest-neighbors (k-NN),
decision trees, and AdaBoost [23] against adversarial evasion
attacks, hence, become more difÔ¨Åcult to evade.

C. The AdIoTack Algorithms

AdIoTack focuses on performing adversarial attacks on a
given decision tree ensemble model. The model consists of
several decision trees. Weighted or unweighted voting gives
the Ô¨Ånal classiÔ¨Åcation, i.e., the fraction of trees yielding the
Ô¨Ånal output determines the prediction and classiÔ¨Åcation score.

In order to explain how voting works, let us consider an
example relevant to our device class inference problem. Let
say, if a benign trafÔ¨Åc instance x belongs to IoT device class
‚ÄúA‚Äù (ground-truth), and say 95% of the trees in the ensemble
model classify it correctly, the Ô¨Ånal prediction would be ‚ÄúA‚Äù
with the score of 0.95. Now, assume ‚ÄúA‚Äù is under a cyberattack,
having a trafÔ¨Åc instance ÀÜx. The model is expected to give a
lower score [40] for ÀÜx since because of the deviation from
benign instance x there will be disagreement among various
trees, each inferring from a speciÔ¨Åc set of features. Certain
features of ÀÜx will signiÔ¨Åcantly deviate from their expected
normal range, leading a portion of trees to classify ÀÜx as a
class other than ‚ÄúA‚Äù, and other features may remain relatively
unchanged hence some other trees predict the class of ÀÜx as
‚ÄúA‚Äù. Therefore, the attack can be Ô¨Çagged with a good chance.
In adversarial attack, we aim to Ô¨Ånd a recipe which has the
same malicious impact of ÀÜx in such a way that the majority
of decision trees predict it as ‚ÄúA‚Äù, resulting in high score and
implying it is still benign.

In the context of decision trees, we deÔ¨Åne an adversarial
path to be the path from the root node of a given decision
tree to a leaf with the label of the victim device that we aim
to launch an attack on it. Each adversarial path is associated
with a set of conditions that are met across the path nodes.
Fig. 3-left shows how the adversarial path is found on an
illustrative decision tree. Grey circles are decision nodes, each
checking a certain condition (e.g., f1 ‚â§ œÑ1), proceeding to
subsequent branch (left or right, depending on the check
result). Adversarial path x‚àó is a complete path from the root to
a leaf node, landing on the target victim class ‚ÄúA‚Äù. AdIoTack
performs tree traversals using the pre-order method ‚Äì the root
node is Ô¨Årst visited, then recursively a pre-order traversal of
the left subtree is performed, followed by a recursive pre-order
traversal of the right subtree.

Note that searching the adversarial path needs to be ex-
tended to all decision trees inside the ensemble model where
consistency of conditions across trees becomes a non-trivial
challenge. Fig. 3 shows an illustrative example of searching for
an adversarial path across two decision trees while conditions
(of paths in the two trees) need to be consistent. Assume

Fig. 2.

Inputs and outputs of our adversarial learning process.

‚àÄd ‚àà D, {x | pred(x) = d and score(x) ‚â• Td}
s.t. ‚àÄf ‚àà F, xf ‚â• fmin

This equation deÔ¨Ånes a set of adversarial instances X for
each IoT device type d ‚àà D. The output of the inference
model for each adversarial instance x ‚àà X, pred(x), is d
and the classiÔ¨Åcation score, score(x), is greater than or equal
to the classiÔ¨Åcation threshold of the corresponding class, Td.
Therefore, every adversarial instance x can bypass the infer-
ence model‚Äôs detection as it satisÔ¨Åes the minimal classiÔ¨Åcation
score. Also, we deÔ¨Åne a set of constraints to guarantee that X
fulÔ¨Ålls the requirements of the desired attack. As the scope of
this paper is volumetric attacks, we deÔ¨Åne these constraints as a
set of lower bound conditions for each feature f that belongs to
the target feature set F. The set F contains the relevant features
that would be affected by the desired attack, e.g., TCP packet
and byte count features in TCP SYN reÔ¨Çection attack.

Fig. 2 illustrates inputs and outputs of our ofÔ¨Çine adver-
sarial learning process. For illustration purpose we only show
two features, f1 and f2. Suppose we want to test whether
the model is vulnerable to a volumetric attack on a victim
device (e.g., Amazon Echo), where the attack offers more
than 1000 packets over f1. The desired attack impact
is
captured by a set of conditions for the target feature(s) (in our
example, f1 > 1000). The ofÔ¨Çine adversarial learning process
then searches over a given tree ensemble model in order to
determine recipes that the ensemble model predicts them as the
class of the victim device with a classiÔ¨Åcation score of greater
than or equal to the given threshold. Adversarial recipes can be
seen as a region in an N dimension space, being N the number
of features required by the inference model. Conditions on
target features (f1 in our example) speciÔ¨Åed by the adversarial
recipes must be consistent with those conditions given as input
for the intended attack (i.e., f1 > 1000). In other words, any
point in an adversarial recipe region is an adversarial instance
that satisÔ¨Åes the conditions of the intended attack.

logistic regression, and SVM,

We can use different search methods to Ô¨Ånd X depending
on the underlying inference model. For models like neural
this optimization
networks,
problem can be solved using gradient-based methods such as
gradient descent, fast gradient sign [17], Jacobian saliency
map [37], or auto projected gradient descent [10]. While
differentiable models like neural networks have been widely
applied to image processing tasks, non-differentiable models
like decision trees have received less attention. In cybersecurity
applications, instead, decision tree-based models seem more
attractive with some interesting properties: (i) faster training
of O(n log n); (ii) better handling of imbalanced datasets [24]

5

inferencing model <Decision-Tree ensemble>offline adversarial learningvictim device <Class>target attack<feature rules>f1f2ùëì!"#$ùëì!"%&ùëì‚Äô"#$ùëì‚Äô"%&adversarialrecipescore threshold<number>Searching adversarial paths in two decision trees: The adversary starts traversing decision tree #1 with no prior rule ‚Äì a path to target class ‚ÄúA‚Äù is
Fig. 3.
found ((cid:88)). Moving to decision tree #2, the Ô¨Årst path becomes inconsistent with rules of the chosen path from decision tree #1 (√ó), and thus an alternative path
is sought and found ((cid:88)).

we want to target class ‚ÄúA‚Äù with a cyberattack that requires
{f1 > 1000} (i.e., target rule). Below, we show the initial
recipe to begin the attack:

Recipe : f1 > 1000 ‚áí A

Starting from decision tree #1 (on the left), AdIoTack Ô¨Ånds
the Ô¨Årst path leading to a leaf with the target class ‚ÄúA‚Äù (by
traversing the tree in pre-order mode). The adversarial path
on decision tree #1 consists of two conditions: {f1 > 100
and f2 > 50}, which are consistent with the initial recipe;
therefore, we can add them to the recipe:

Recipe : f1 > 1000 ‚àß f1 > 100 ‚àß f2 > 50 ‚áí A

which can be simpliÔ¨Åed (merged) to:

Recipe : f1 > 1000 ‚àß f2 > 50 ‚áí A

Moving on to decision tree #2, the Ô¨Årst possible path (in
pre-order search) to the target class ‚ÄúA‚Äù requires {f1 ‚â§ 50},
which is inconsistent with {f1 > 1000} in the current recipe ‚Äì
this path cannot be taken, and hence an alternative path needs
to be sought. The other path to the other leaf with class ‚ÄúA‚Äù
(on decision tree #2) results in two new conditions: {f3 > 10
and f2 ‚â§ 60}, not violating the current recipe determined by
the path from decision tree #1. The updated recipe is shown
below:

Recipe : f1 > 1000 ‚àß f3 > 10 ‚àß 50 < f2 ‚â§ 60 ‚áí A

To generate an adversarial recipe that drives the majority
vote to the target class, AdIoTack needs to keep track of
conditions and progressively augment them as new adversarial
paths are found across individual decision trees. Our search
procedure visits each tree in a greedy manner, i.e., it skips a
tree if it could not Ô¨Ånd any consistent adversarial path on it.
Also, the order of the trees inÔ¨Çuences the output of the method.
We address this problem by running the algorithm multiple
times with different tree ordering to generate new recipes.

Having these intuitive illustrations of the adversarial attack
method, let us formally develop it by Algorithms 1 and 2.
Algorithm 1 searches for an adversarial recipe for a given
target device. This algorithm expects the inference model
(IM), a set of target rules (T R) that are deÔ¨Åned based on

R ‚Üê (T R ‚áí T D)
for t in IM.trees do

Algorithm 1 Adversarial evasion attack on tree ensemble
model.
1: IM ‚Üê inference model
2: T R ‚Üê set of target rules
3: T D ‚Üê target device class
4: Td ‚Üê classiÔ¨Åcation score threshold of T D
5: function FINDRECIPE(IM, T R, T D, Td)
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: end function

end for
x‚àó ‚Üê PROJECT(R)
if IM.PRED(x‚àó) = T D & IM.SCORE(x‚àó) ‚â• Td then

AdvP ath ‚Üê FINDADVPATH(t.root, T D, R, [ ])
if AdvP ath (cid:54)= Null then

R ‚Üê MERGE(R, AdvP ath)

return Null

return R

(cid:46) Recipe

end if

end if

else

the target cyberattack, the target class of victim IoT device
(T D), and the classiÔ¨Åcation score threshold (Td) as inputs.

In essence, Algorithm 1 iterates over all

trees in the
ensemble model (IM). Notice the tree order in a decision
tree ensemble model is arbitrary, and different orders may
give different results (new recipes or no recipe at all). In ¬ßV,
we evaluate how different orders can affect the number of
generated recipes. For each tree, the algorithm calls the func-
tion FindAdvPath in Algorithm 2 to Ô¨Ånd an adversarial path
consistent with the current recipe generated so far. If such a
path is found, the corresponding adversarial path‚Äôs rules are
merged with the current recipe rules (Merge). The following
examples show how Merge function works:

Merge(f ‚â§ œÑ1, f ‚â§ œÑ2, œÑ1 ‚â§ œÑ2) = f ‚â§ œÑ1, or

Merge(f > œÑ1, f > œÑ2, œÑ1 > œÑ2) = f > œÑ1

As there is no guarantee that FindAdvPath will Ô¨Ånd an
adversarial path for every tree, we need to validate the efÔ¨Åcacy
of the Ô¨Ånal recipe (whether it can bypass the model or not).
We may Ô¨Ånd adversarial paths effective on only a small
subset of the decision trees, or even no tree at all. In this
case, the resulted adversarial instances will become ineffective
in yielding the target class and/or the score threshold. We
validate the efÔ¨Åcacy of a recipe by generating a representative

6

ùëì1ùëì2ùëì3‚â§100ùëì3Decision Tree #1Decision Tree #2> 100BDDCBBAACAùë•‚àóùë•‚àó‚úÖ‚úÖ‚òíconsistency?search adversarial path> 10‚â§10> 50‚â§50> 10‚â§10ùëì2> 60‚â§60ùëì1> 50‚â§50else

end if

return P

return Null

if n is leaf then

(cid:46) Adversarial path

if n.label = target then

Algorithm 2 Searching for a consistent adversarial path.
1: function FINDADVPATH(n, T D, R, P )
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

r ‚Üê FINDADVPATH(n.lef t, T D, R, [P ‚àß n.cond])
if r (cid:54)= Null then
return r

end if
r ‚Üê Null
if ISCONSISTENT(n.cond, R) & ISCONSISTENT(n.cond, P ) then

ISCONSISTENT(¬¨n.cond, R) & ISCONSISTENT(¬¨n.cond, P )

end if

end if
if
then

r ‚Üê FINDADVPATH(n.right, T D, R, [P ‚àß ¬¨n.cond])

17:
end if
18:
return r
19:
20: end function

adversarial instance, x‚àó, using Project function. We note
that every instance of a recipe would fall under the desired
adversarial paths determined by Algorithm 1; therefore, if x‚àó
can bypass the model, any other adversarial instance generated
from the given recipe can do the same. The instance x‚àó is
presented to the model to obtain the prediction. If it is classiÔ¨Åed
as the target class with a classiÔ¨Åcation score greater than or
equal to the speciÔ¨Åed threshold (Td), the algorithm approves
and returns the recipe.

Algorithm 2 (invoked from within Algorithm 1) searches
for a consistent adversarial path (P ) to a leaf with the target
class label (T D) on a given tree. The algorithm is a direct
application of pre-order traversal for binary trees. During a
search, we check for consistency (IsConsistent) between
the nodes‚Äô condition and the recipe (explained in III-D). This
helps to prune the search space, reducing the average execution
time. Algorithm 2 assumes binary decision trees; therefore, the
nodes are described by only three Ô¨Åelds (left, right and
cond). The left subtree is associated with the condition cond
and the right subtree with logical negation of it (¬¨cond).

Note that an adversary may choose to stop searching when
a certain level of majority across trees (i.e., classiÔ¨Åcation
score) is obtained. Our algorithm, instead, aggressively aims
to maximize its chance of subverting the model by (greedily)
searching all possible trees. Given this greedy nature of our
ofÔ¨Çine adversarial learning, its time complexity is O(T .N .F),
proportional to the number trees (T ), maximum number of
nodes in the trees (N ), and total number of features (F). It
is important to note that our search across trees progresses
only in a forward direction. Hence, in some cases, it may not
yield any consistent adversarial path on a decision tree (in the
middle of iterating over trees). In other words, if a consistent
path on a tree is not found, then that tree is skipped without
going backward, seeking alternative adversarial paths on the
previous trees, and updating the recipe correspondingly.

D. Verifying Consistency of Conditions

Our adversarial learning algorithm is general and can be
applied to any decision tree ensemble model. However, certain
constraints may be needed when employing the algorithm

7

Fig. 4. Local adversary (an infected machine) collects trafÔ¨Åc features before
launching an online attack.

for speciÔ¨Åc domain problems. Given the primary use-case of
this paper is network-based volumetric attacks, three types of
consistency checks are performed before taking a new branch
(left or right subtree) while searching for an adversarial path:

Single-feature consistency: This type of consistency is
required when different conditions are imposed on a single
feature across trees. Let us consider two illustrative conditions
f ‚â§ œÑ1 and f > œÑ2. They are considered consistent if œÑ2 < œÑ1,
where f denotes a feature that the inference model uses. A
single-feature consistency check is required in any problem
of adversarial machine learning. However, in the context of
cybersecurity, we identiÔ¨Åed two additional checks essential
for launching attacks; otherwise, the ofÔ¨Çine learning algorithm
may generate impractical adversarial recipes that cannot be
launched during the online phase.

Frame size consistency: This constraint

is speciÔ¨Åc to
the use-case of this paper. Frame size is measured in bytes
and has a minimum and maximum length, depending on the
implemented technology. For example, an Ethernet frame must
be at least 64 bytes for collision detection to work and can be
a maximum of 1518 bytes to avoid IP fragmentation. This
expected characteristic may get violated by a condition. For
example, let us consider a set of conditions on two features of
incoming (‚Üì) DNS trafÔ¨Åc for a given adversarial recipe:

(cid:40) C1 : 9 <

(cid:7)
‚ÜìDNS packet count
(cid:6)

(cid:7)
‚ÜìDNS byte count ‚â§ 100
(cid:6)

(cid:5)

(cid:4)

(cid:4)

(cid:5)

C2 :

These translate into an average size of incoming DNS
frames to be less than 10 bytes, which is impossible on
Ethernet networks ‚Äì sending at least ten frames even with
no payload would result in 640 bytes of trafÔ¨Åc, violating the
condition C2 above.

Boundary consistency: Average frame size obtained by
the ratio of byte count and packet count must be rounded to
integer values (cannot be Ô¨Çoat values). There are certain corner
cases where packet count and byte count conditions become
inconsistent. The following conditions exemplify this situation:

(cid:7)
(cid:40) C3 : 10 <
‚ÜìDNS packet count ‚â§ 15
(cid:6)
(cid:7)
‚ÜìDNS byte count ‚â§ 1, 000
(cid:6)

C4 : 998 <

(cid:5)

(cid:5)

(cid:4)

(cid:4)

victim IoT devicedefault gatewayARP poisoningAdIoTackIcvictim‚Äôs  current feature vectorlocal IoT devicesARP poisoningmaintains a record (feature vector Ic) of the trafÔ¨Åc features
in the current epoch. Given Ic, AdIoTack would Ô¨Ånd feasible
adversarial recipes towards the end of each epoch. A feasible
recipe has to be an upper bound to vector Ic (Fig. 5 shows
it in a 2D space). If the victim displays network activities
(Ic) exceeding a threshold (f < œÑ ) speciÔ¨Åed by a recipe,
then Ic cannot be projected to that recipe anymore simply
because we cannot reduce the amount of trafÔ¨Åc that already has
been exchanged by the device but we can increase it. Among
all feasible recipes, AdIoTack selects the closest adversarial
instance to Ic, minimizing the overhead packets to be injected.
This strategy requires AdIoTack to be aware of the inference
model‚Äôs timing cycles, i.e., when it is called for prediction.
In our evaluation (¬ßV), we will explain how AdIoTack has the
chance to execute successful attacks even without syncing with
cycles of the inference model.

In this paper, we use linear search to Ô¨Ånd feasible recipes.
Though we do not encounter a challenge in terms of the time
complexity of the linear search in our evaluation, there might
be situations where a simple linear search becomes inefÔ¨Åcient.
To achieve certain response time, one may attempt to optimize
the search process and/or prune the generated recipes. Both of
these objectives are beyond the scope of this paper.

We use an example in Table I to better clarify our method
for executing a volumetric TCP SYN reÔ¨Çection attack on
Google Chromecast. The top two rows indicate lower-bound
and upper-bound feature values speciÔ¨Åed by the adversarial
recipe for launching a successful attack. Note that the symbol
‚Äú*‚Äù indicates unbounded features (no lower/upper limits).
The third row shows the current feature vector of Google
Chromecast over the last 60 seconds. The bottom row shows
the adversarial instance comprising the intended volumetric
attack trafÔ¨Åc (bold cells) reÔ¨Çected by Google Chromecast and
overhead packets (underlined cells) injected by AdIoTack.

To successfully launch the adversarial volumetric attack,
AdIoTack needs to inject certain spoofed packets (the overhead
trafÔ¨Åc speciÔ¨Åed by the closest recipe) on behalf of the victim,
so that the inference model does not detect any deviation in
the behavior of the victim IoT device. Table II summarizes
certain packet metadata Ô¨Åelds to be modiÔ¨Åed along with their
spoofed value for a given set of trafÔ¨Åc features in the speciÔ¨Åc
model we studied. GW and VIC stand for gateway and victim,
respectively. WAN IP and LAN IP respectively refer to external
(public) and internal (private) IP address. Also, ‚Äú*‚Äù highlights
a wildcard value for the respective Ô¨Åeld.

Practical Challenges of Network-Based Attacks: We
discuss a few practical challenges to overcome for successful
execution of adversarial attack instances in what follows: (i)
in order to intercept bidirectional trafÔ¨Åc exchanged between
the victim and its local network, AdIoTack needs to employ
Gratuitous ARP replies to send a broadcast spoofed ARP reply
(on behalf of the victim device) to all local devices. That way,
all local trafÔ¨Åc destined to the victim device is forwarded to
AdIoTack; and, (ii) there might be cases whereby AdIoTack
and victim may not share the same network interface (e.g., one
is wired and the other one is wireless). In that case, AdIoTack
will have to send the spoofed trafÔ¨Åc via the default gateway,
which will disrupt
the MAC-learning tables (wireless vs.
wired) of the default gateway‚Äôs interfaces due to the spoofed
source MAC address in those packets (Table II) ‚Äì this can lead

Fig. 5. Finding feasible recipes‚Äô region and the closest adversarial instance
given current state Ic. For illustration purpose, only two features are shown.

(cid:7)
‚ÜìDNS packet count = 11
(cid:6)

(cid:5)

(cid:4)

(cid:4)

(cid:5)

Choosing the combination of
(cid:7)
‚ÜìDNS byte count = 999 gives a minimum average frame
and
(cid:6)
size of 91 B (equally sized). At a high level, this combination
would violate C4 since 11 DNS incoming messages of size
91 B each will amount to 1001 B. Indeed, one may choose
to solve this inequality problem by judiciously setting packet
sizes (e.g., nine packets of each 100 B and a packet of 99
B) ‚Äì this is beyond the scope of this paper. We only consider
recipes that allow adversarial attacks of equal packet size.

Note that in a single tree, conditions inside each path (from
root to a leaf) satisfy single-feature consistency by default.
Still, even conditions within a given path of a single tree may
violate frame size and boundary consistencies.

IV. LAUNCHING ADVERSARIAL VOLUMETRIC ATTACK

This section describes how we use adversarial recipes to
launch real volumetric attacks on an operational IoT network
whose trafÔ¨Åc is continuously monitored by a trained inference
model. To clarify the method better, we explain this section
with a set of the Ô¨Çow-based features that the model uses
for inference. Inspired by [40], we choose packet count and
byte count of certain network Ô¨Çows: ‚ÜìDNS, ‚ÜëDNS, ‚ÜìNTP, ‚ÜëNTP,
‚ÜëSSDP, ‚ÜìLAN, ‚ÜìWAN, and ‚ÜëWAN, proven to be reÔ¨Çective of IoT
behavior, during Ô¨Åxed time windows (of length one minute);
where ‚Üì and ‚Üë indicate incoming and outgoing directions,
respectively.

This phase aims to project the current state (trafÔ¨Åc feature
vector) of a victim IoT device to an adversarial
instance
using one of the adversarial recipes obtained in the ofÔ¨Çine
learning phase. A naive approach is to launch a cyberattack
blindly based on a random recipe. This approach is not
guaranteed to bypass the inference model given the variability
in volume and frequency of trafÔ¨Åc sent/received by the victim
IoT devices during the last time window (i.e., current state).
It is important to note that malicious packets (pertinent to the
intended network attack) get added to the benign trafÔ¨Åc of the
victim device. Hence, a more effective strategy is needed that
considers the current state of the variable network.

A well-known method to track network trafÔ¨Åc and create
a feature vector for the victim device is poisoning the ARP
tables (discussed in ¬ßIII) of local IoT devices and default
gateway. Therefore, AdIoTack can sit in the middle of these
entities by forwarding trafÔ¨Åc between them. Fig. 4 illustrates
the monitoring phase before launching the attack. AdIoTack

8

f1f2recipe #1recipe #4recipe #2recipe #5closest adversarial instanceICfeasible adversarialrecipes regionrecipe #3TABLE I.

AN EXAMPLE OF A FEASIBLE SYN REFLECTION ATTACK ON GOOGLE CHROMECAST.

t
k
p

S
N
D

‚Üì

2
*
0
3

e
t
y
b

S
N
D

‚Üì

119
1,666
0
120

t
k
p

S
N
D

‚Üë

2
*
0
3

e
t
y
b

S
N
D

‚Üë

194
*
0
195

t
k
p

P
T
N

‚Üì

*
*
0
1

e
t
y
b

P
T
N

‚Üì

45
*
0
46

t
k
p

P
T
N

‚Üë

*
1
0
0

e
t
y
b

P
T
N

‚Üë

*
*
0
0

t
k
p

P
D
S
S

‚Üë

7
9
0
8

e
t
y
b

P
D
S
S

‚Üë

4,353
4,553
0
4,354

t
k
p

N
A
L

‚Üì

94
148
0
95

e
t
y
b

N
A
L

‚Üì

9,303
13,797
0
9,304

t
k
p

N
A
W

‚Üì

88
*
46
1,000

e
t
y
b

N
A
W

‚Üì

45,568
*
125,857
74,000

t
k
p

N
A
W

‚Üë

10
*
51
1,000

e
t
y
b

N
A
W

‚Üë

5,650
*
5,752
74,000

Condition (>)
Condition (‚â§)
Current state (Ic)
Adversarial instance

TABLE II.

SPOOFED METADATA FOR EACH FEATURE.

Feature

‚Üì DNS

‚Üë DNS

‚Üì NTP

‚Üë NTP

‚Üë SSDP

‚Üì LAN

‚Üì WAN

‚Üë WAN

src
MAC

dst
MAC

src IP

dst IP

src Port

dst Port

GW

VIC

GW

VIC

VIC

*

GW

VIC

VIC

GW

VIC

GW

*

*

VIC

*

VIC

VIC

VIC

LAN IP

VIC WAN IP

VIC

*

VIC

*

*

VIC

VIC

GW

VIC

WAN IP

53

*

123

*

*

*

*

*

*

53

*

123

1900

*

*

*

TABLE III.

THREE ENSEMBLE MODELS PERFORMANCE ON THE

TESTING DATASET.

Model
Random Forest
Gradient Boosting
AdaBoost

Accuracy
96%
91%
99%

False positive
9%
0.1%
5%

to mis-forwarding of other packets to/from the victim (wireless
LAN instead of wired LAN). Disrupting the default gateway‚Äôs
MAC tables results in dropping future packets with the same
MAC address, this time as the destination Ô¨Åeld. To avoid
this, AdIoTack sends a speciÔ¨Åc ICMP packet with destination
broadcast MAC address (ff:ff:ff:ff:ff:ff) and victim‚Äôs
IP address. The victim‚Äôs reply will revert the MAC tables to
their correct state.

V. EVALUATION RESULTS

This section evaluates the performance of AdIoTack in its
two phases, namely ofÔ¨Çine adversarial learning, and online
adversarial execution. We begin by training three well-known
decision tree ensemble models namely Random Forest, Gradi-
ent Boosting, and AdaBoost classiÔ¨Åers.

A. The Inference Model

Fig. 6. Number of generated adversarial recipes for three ensemble models
for attack impact of 1000 packets.

data from February, March and the Ô¨Årst half of April for
training, and the second half of April and May for testing.

We use the Python Scikit Learn library to train multi-
class classiÔ¨Åers by three representative ensemble decision-tree
algorithms, namely Random Forest, Gradient Boosting, and
AdaBoost, to predict device class label. These models provide
us with a classiÔ¨Åcation score used as a baseline threshold for
detecting misbehavior. Table III shows the accuracy and false
positive rate of each model. For detecting misbehaviors, we
use classiÔ¨Åcation scores‚Äô ¬µ and œÉ for each class of IoT device
(shown in Table IV for Random Forest model) obtained from
the training dataset in such a way that if for a given instance
the model‚Äôs classiÔ¨Åcation score for its predicted label is below
¬µ ‚àí œÉ (of that label), the instance is considered as malicious.
Table IV shows the Random Forest model‚Äôs classiÔ¨Åcation score
for each class of IoT devices in our testbed obtained from the
training dataset. We use ¬µ and œÉ values obtained from the
training for detecting IoT devices‚Äô misbehavior in such a way
that if for a given instance the model‚Äôs classiÔ¨Åcation score for
its predicted label is below ¬µ ‚àí œÉ (of that label), the instance
is considered as malicious.

Inspired by [40], we consider Ô¨Çow-level features from net-
work trafÔ¨Åc, periodically computed over one-minute windows.
That way, no need for inspecting packet payloads. TrafÔ¨Åc
features include statistics (packet and byte counts) of ‚ÜìDNS,
‚ÜëDNS, ‚ÜìNTP, ‚ÜëNTP, ‚ÜëSSDP, ‚ÜìLAN, ‚ÜìWAN, and ‚ÜëWAN; where
‚Üë indicates upstream trafÔ¨Åc from each IoT device, and ‚Üì
highlights downstream trafÔ¨Åc to each IoT device. This means
a total of 16 features.

We train our model with a dataset collected from our
testbed (comprising nine IoT devices), during February-May
2020. Our full dataset has 954,384 benign instances. We use

B. AdIoTack OfÔ¨Çine Instances

In this part, we evaluate the performance of AdIoTack in
ofÔ¨Çine adversarial learning with a view to launch two well-
known network-based attacks, namely TCP SYN and SSDP
reÔ¨Çection attacks.

Let us begin by showing that our ofÔ¨Çine learning method
is generalizable by applying it to the three popular ensemble
models. Fig. 6 indicates the number of adversarial recipes
for SSDP and SYN reÔ¨Çection attacks per inference model.
Random Forest seems to be more robust to adversarial attacks

9

Random ForestGradient BoostingAdaBoostinference model010203040# adversarial recipesSYN reflection attackSSDP reflection attackTABLE IV.

EXPECTED CLASSIFICATION SCORE (¬µ AND œÉ) OF THE

RANDOM FOREST MODEL OBTAINED FROM TRAINING.

IoT class
Amazon Echo
Belkin motion
Belkin switch
Chromecast
Hue bulb
LiFX bulb
Netatmo camera
Samsung camera
TP-Link switch

Benign
œÉ
0.10
0.06
0.18
0.16
0.01
0.15
0.14
0.002
0.10

¬µ
0.94
0.96
0.86
0.90
0.99
0.72
0.88
1.00
0.86

Fig. 8. Number of generated adversarial recipes per IoT device for two
representative attacks (result of a 20-permutation run).

algorithm with 20 permutations. Belkin switch and Chromecast
in both types the attacks. Samsung smart camera has no recipe
for SSDP reÔ¨Çection, TP-Link has no recipe for SYN reÔ¨Çection
attack, and Hue lightbulb has no recipe for both of the attacks,
indicating tight constraints for those classes.

Before experimenting with our adversarial recipes in an op-
erational network of IoT devices, we evaluate the performance
of our inference model and efÔ¨Åcacy of ofÔ¨Çine adversarial in-
stances by replaying various unseen datasets in three scenarios,
namely: (1) a purely benign dataset is expected to receive
high classiÔ¨Åcation scores from the model, (2) syntactically
changed features of the benign instances (to represent non-
adversarial SYN reÔ¨Çection attack) are expected to receive
low scores from the model (highlighting a detection), and
(3) synthetically changed features of the benign instances (to
represent adversarial SYN reÔ¨Çection attack) are expected to
receive high scores from the model (highlighting a miss).

Fig. 9 shows the results for two representative IoT devices
over an hour. As expected, almost all benign instances are
classiÔ¨Åed with a high score (i.e., above the threshold shown by
dashed blue line), highlighting the expected performance under
normal situations (benign trafÔ¨Åc). The dotted red curve shows
the model‚Äôs score for non-adversarial attack instances, which
the model detects a majority of them (receiving scores below
the threshold). Finally, the purple curve highlights the model‚Äôs
inability to detect adversarial attacks (i.e., giving scores above
the threshold, meaning benign). In some epochs like at 5:03pm
for Amazon Echo and 5:08pm for Belkin switch, there is no
recipe in the feasible region to project the current instance,
thus missing data points.

C. AdIoTack Online Evaluation

To demonstrate adversarial attacks in a realistic scenario,
we deploy our inference model in a system that can classify
trafÔ¨Åc in real-time. It receives a stream of Ô¨Çow-based telemetry
and predicts trafÔ¨Åc features of individual connected devices
every minute. To collect real-time telemetry, we use an SDN-
enabled home gateway in our setup, shown in Fig. 10. We
installed Open Virtual Switch (OVS) on the gateway of our
IoT network. Ryu, an open-source SDN controller, is installed
on another machine that communicates with this OVS. The

Fig. 7. Number (min, max, avg) of generated adversarial recipes for different
permutation counts for attack impact of 1000 packets.

compared to the other two models, corroborating with the ob-
servations in [23]. Because of its relative robustness, we choose
Random Forest for the rest of the experimental evaluation in
this paper.

We now quantify the impact of the sequence order of
trees in generating adversarial recipes. We deÔ¨Åne permutation
as a random shufÔ¨Çe of the trees inside the ensemble model,
which we need to iterate through them in the same order as
the permutation suggests. We show that various permutations
can result in different adversarial recipes. Fig. 7 shows the
min, max, and average number of unique recipes for different
permutation counts by running the algorithm Ô¨Åve times for
each permutation value. For example, if the permutation count
is three, we run the ofÔ¨Çine learning with three different permu-
tations for each device over Ô¨Åve rounds and count the number
of unique recipes at each round. The plot shows an overall
increasing trend which means as we try different permutations,
there is a high chance of Ô¨Ånding new recipes. Also, the plot
suggests that SSDP reÔ¨Çection can generate more recipes than
SYN reÔ¨Çection, in which the increasing trend seems to become
smoother after three permutations. The average time taken
for generating adversarial recipes per permutation is about
1.5 minutes for SSDP reÔ¨Çection and 2.2 minutes for SYN
reÔ¨Çection attack. This is probably because four features (packet
and byte count of Ô¨Çows ‚ÜìWAN and ‚ÜëWAN) are affected by SYN
reÔ¨Çection attack, whereas only two features (packet and byte
count for ‚ÜëSSDP) are affected by the SSDP reÔ¨Çection attack.

Fig. 8 illustrates the number of generated recipes for each
IoT device in our testbed by running the ofÔ¨Çine learning

10

12345678910# permutations036912151821242730# adversarial recipesSSDP reflection attackSYN reflection attackAmazon EchoBelkin motionBelkin switchChromecastLiFX lightNetatmo cam.Samsung cam.TP-Link switchdevice020406080100120# adversarial recipesSSDP reflection attackSYN reflection attackFig. 9.
scenarios: benign (green), synthetic adversarial SYN reÔ¨Çection attack (purple) and synthetic non-adversarial SYN reÔ¨Çection attack (red).

Time trace of model performance against replayed instances of two representative IoT devices: (a) Amazon Echo, and (b) Belkin switch, in three

(a) Amazon Echo.

(b) Belkin switch.

regions, the model gives scores lower than expected threshold
to non-adversarial
instances across the three representative
devices.

During the online evaluation, we found that it is impossible
to execute any adversarial attack on three devices, namely
Samsung smart camera, Philips Hue lightbulb, and Amazon
Echo. For Samsung smart camera, the reason is that no feasible
adversarial recipe was found for Ic (device‚Äôs current feature
vector) to be projected on as Ic during our experiment violated
the upper bound of the rule on ‚ÜëSSDP feature. For the other two
devices, the reason relies on the network aspect. Philips Hue
lightbulb responds to the corrective ping message sourced by
AdIoTack. However, the response does not revert the gateway‚Äôs
MAC table; probably because Philips Hue is not a standalone
device and relies on a separate bridge for communication.
Amazon Echo did not respond to the ping message, hence
the disrupted MAC table issue stopped us from launching the
attack.

During this evaluation, we assumed that AdIoTack knows
the inference cycles of the model i.e., when the model fetches
the Ô¨Çow counters from the SDN controller and classiÔ¨Åes the
trafÔ¨Åc. This is a valid assumption as we already declared our
work to be a white-box approach. Still, we further evaluate
AdIoTack online execution in scenarios where inference cycles
are unknown. Fig. 12 illustrates this evaluation for Belkin mo-
tion sensor and Chromecast. We launched adversarial attacks
on these devices with four different timings. Time-shift of T
means that the attacker is T seconds ahead of the inference
model. We need to note that the time window in this evaluation
is 60 seconds meaning that the model becomes online every
60 seconds.

Unsurprisingly, Fig. 12 shows that adversarial attacks with
the time shift of zero are 100% successful
in bypassing
the model. Fig. 12 shows that for Chromecast,
time-shift
almost has no effect on the success of adversarial attacks;
however, for Belkin motion sensor, it has signiÔ¨Åcant effects.
The results suggest that a time shift of 30 seconds leads to
the minimum success rate (23%), but when it gets closer to
the inference model‚Äôs time cycle, i.e., time shift of 45 seconds,
the success rate signiÔ¨Åcantly rises (92%). The key takeaway is
that regardless of when AdIoTack launches the attack, there is
always a chance to bypass the model.

Fig. 10. Prototype of AdIoTack.

Ryu controller initializes the setup by sending a Ô¨Åxed set of
Ô¨Çow rules speciÔ¨Åc to each IoT device on the network, of which
the inference model needs the corresponding Ô¨Çow counters as
features (features described in ¬ßV-A). Every minute, the model
receives Ô¨Çow counters from the controller and classiÔ¨Åes indi-
vidual devices. The model also detects unexpected behavior
by way of classiÔ¨Åcation score (explained in ¬ßV-A).

Fig. 11 shows the performance of AdIoTack in the online
execution phase for three representative devices with SYN
reÔ¨Çection attack. Our entire evaluation lasted for 35 minutes
with three stages: between 2:40pm and 2:48pm (highlighted
in green), no attack was executed to observe the behavior
of our inference model in normal operation; between 2:48pm
and 3:10pm (highlighted in orange), we performed adversarial
SYN reÔ¨Çection attacks in each epoch; lastly, between 3:10pm
and 3:15pm (highlighted in red), we performed non-adversarial
SYN reÔ¨Çection attack with the same impact of the adversarial
ones launched in the middle stage. The upper subplots show
the inference model‚Äôs classiÔ¨Åcation score at the end of each
epoch. The score threshold of each class is shown by a constant
dashed red line. The lower subplots show the number of attack
packets received by the ultimate target machine (victim server)
residing on the WAN interface of the gateway (outside the local
IoT network).

It can be seen that the inference model displays an accept-
able performance under normal operation (i.e., pure benign
trafÔ¨Åc, without any attack). Moving to the orange regions, we
observe that all adversarial reÔ¨Çection attacks are successful
(classiÔ¨Åcation scores above the threshold), except during an
epoch for Chromecast in Fig. 11(b). Lastly, focusing on the red

11

5:00pm5:10pm5:20pm5:30pm5:40pm5:50pm6:00pmtime0.000.250.500.751.00classification scorebenignadversarialnon-adversarialscore threshold5:00pm5:10pm5:20pm5:30pm5:40pm5:50pm6:00pmtime0.000.250.500.751.00classification scorebenignadversarialnon-adversarialscore thresholdInternetinferencemodelAdIoTackLocal networkIoT devicesRyu controllerOpenFlow rulesflowcountersminutelyflow featuresWANtarget(a) Belkin switch.

(b) Chromecast.

(c) Netatmo camera.

Fig. 11. Time trace of model performance and attack trafÔ¨Åc on three IoT devices in three scenarios: normal operation (green), adversarial attack (orange) and
non-adversarial attack (red).

VI. REFINING RESILIENCE OF DECISION TREES AGAINST
ADVERSARIAL VOLUMETRIC ATTACKS

We have so far highlighted the vulnerability of decision
tree-based models against systematically crafted adversarial
attacks. In this section, we develop a method called patching
to reÔ¨Åne the model post-training, making decision trees robust
against adversarial volumetric attacks.

Decision trees trained purely by benign trafÔ¨Åc instances
are inherently vulnerable (given their structure) to volumetric
attacks. Fig. 13 illustrates an illustrative decision tree with four
leaves and three decision nodes on three features. For each leaf,
we deÔ¨Åne bounded and unbounded feature sets. For a given
leaf l, its bounded feature set includes feature f , if inside the
path from the tree‚Äôs root to l, there is at least one decision
node on f which must take the left branch (i.e., f ‚â§ œÑ ) to
reach l. Otherwise, if there is no such a decision node for f ,
we consider f as an unbounded feature for l. For example,
in Fig. 13, consider the leaf with label ‚ÄòC‚Äô. To reach this leaf
from the root, the following conditions must be met: f1 > 100
and f3 ‚â§ 70. Thus if an instance has f1 = +‚àû, f2 = +‚àû,
and f3 = 70, it still reaches the leaf with no issue as there
is no upper boundary check for f1 and there is no upper or
lower boundary check for f2 at all.

In the literature, there exist methods [8], [5] for developing
robust decision trees against adversarial attacks. The current
techniques manipulate the training process of decision trees
by changing the original model to make sure if an adversarial

attack happens, the model still gives the correct output label
which is desirable in image classiÔ¨Åcation which requires a
given set of adversarial instances to be provided before reÔ¨Åning
the model i.e., the model still might be vulnerable to another
set of adversarial instances. That way, they sacriÔ¨Åce the predic-
tion accuracy for benign instances, in order to make decision
trees robust in adversarial scenarios. Also, given their primary
focus is on image recognition, they do not have a notion of
volumetric attacks, and thus, unbounded features remain loose.
Our method, however, focuses on a different problem (i.e.,
volumetric cyberattacks), and aims to reÔ¨Åne a trained model.
Therefore, it does not interfere with the training process and
does not require re-training. Our method receives a trained
decision tree-based model and then patches the unbounded
features with no dependency on any adversarial recipes. If
the training dataset gives a correct representation of devices‚Äô
trafÔ¨Åc characteristics by at least capturing the maximum trafÔ¨Åc
volume for each class correctly, the patched model will have
the exact same accuracy/false positive over benign instances
as the original model. However, if there are data instances in
the testing dataset that have higher trafÔ¨Åc features‚Äô value than
the limits which were captured from the training dataset, our
method would Ô¨Çag them an as malicious (i.e., higher false
positives than the original unpatched model).

We deÔ¨Åne a patch for feature f on leaf l as follows:
patchl,f = f ‚â§ max(lf )

This patch can be seen as a new decision node added

12

0.00.20.40.60.81.0classification score2:40pm2:50pm3pm3:10pmtime02505007501000attack packet count0.00.20.40.60.81.0classification score2:40pm2:50pm3pm3:10pmtime02505007501000attack packet count0.00.20.40.60.81.0classification score2:40pm2:50pm3pm3:10pmtime02505007501000attack packet countFig. 12. Effect of time-shift for two IoT devices.

(a) Belkin motion sensor.

(b) Chromecast.

Fig. 13. Bounded and unbounded features for each leaf in a decision tree.

Fig. 14. Essential patching performed on individual leaves of the vulnerable
decision tree from Fig. 13.

just before reaching the leaf, that checks the upper bound
of f for l. The upper bound is calculated based on the
training dataset used for training the original model. As
an example, based on our training dataset, Amazon Echo
sends a maximum of four DNS packets every minute; thus,
max(Amazon Echo‚ÜëDN S pkt) = 4. Any instance x reaching
leaf l, which xf > max(lf ), goes to a new leaf that yields x
as a malicious instance.

13

Fig. 15. Detection rate of Essential and Additional patching for adversarial
SYN and SSDP reÔ¨Çection attacks.

There are two noteworthy points: (1) Because IoT devices
send/receive a low volume of trafÔ¨Åc and have repetitive be-
havioral patterns, individual devices‚Äô maximum trafÔ¨Åc volume
can be captured [43]. However, this could be challenging for
personal computers and smartphones which their trafÔ¨Åc volume
highly depends on users‚Äô activities. (2) Our method reduces
the impact of adversarial volumetric attacks signiÔ¨Åcantly. How-
ever, low-impact attacks (i.e., within the normal behavior range
of IoT devices) can still bypass the model.

There are two scenarios where feature f can become
unbounded for leaf l: (1) When there is a decision node on f ,
but its right branch (i.e., f > œÑ ) is taken on the path from the
tree‚Äôs root to l (e.g., f1 for leaf ‚ÄòC‚Äô in Fig. 13); and (2) When
there is no decision node on f through the path at all (e.g., f2
for leaf ‚ÄòC‚Äô in Fig. 13). We develop two patching techniques
to address these two scenarios. Essential Patching solves the
Ô¨Årst problem by traversing a given vulnerable decision tree
and patching leaves with unbounded features through paths
from the root. Fig. 14 shows the result of essential patching

0.00.20.40.60.81.0time shift = 00.00.20.40.60.81.0time shift = 150.00.20.40.60.81.0time shift = 305:40pm5:45pm5:50pmtime0.00.20.40.60.81.0time shift = 45classification score0.00.20.40.60.81.0time shift = 00.00.20.40.60.81.0time shift = 150.00.20.40.60.81.0time shift = 305:40pm5:45pm5:50pmtime0.00.20.40.60.81.0time shift = 45classification scoref1ABCD> 100‚â§ 100‚â§ 50> 70> 50‚â§ 70f2f3f1f2f1f3-f3f2f3f1, f2f1 , f2 , f3f1ABCD> 100‚â§ 100‚â§ 50> 70> 50‚â§ 70f2f3f2B>max(ùêµ!!)‚â§ max(ùêµ!!)f1‚â§ max(ùê∂!")C> max(ùê∂!")f3f1DD>max(ùê∑!#)> max(ùê∑!")‚â§max(ùê∑!#)‚â§ max(ùê∑!")2004006008001000# attack packet count5060708090100detection rate (%)Adversarial SSDP reflection attack (Essential patch only)Adversarial SYN reflection attack (Essential and Additional patch)Adversarial SYN reflection attack (Essential patch only)robust against SSDP reÔ¨Çection attacks with any impact. The
improvement becomes more evident as the impact increases.
More importantly, no recipe exists for volumetric attacks with
an impact above 600 packets.

VII. CONCLUSION

In this paper, we developed AdIoTack, a systematic way
of quantifying and reÔ¨Åning the resilience of decision tree en-
semble models against data-driven adversarial attacks. We Ô¨Årst
developed a white-box algorithm that automatically generates
recipes of volumetric network-based attacks that can bypass the
inference model unnoticed. Our algorithm takes the intended
attack on a victim class and the model as inputs. We developed
a systematic method to successfully launch the intended attack
on real networks. We next prototyped AdIoTack and validated
its efÔ¨Åcacy on a real testbed of real IoT devices monitored
by a trained Random-Forest model. We demonstrated how
the model detects all non-adversarial volumetric attacks on
IoT devices while missing many adversarial ones. Finally, we
developed systematic methods to patch loopholes in trained
decision tree ensemble models. We demonstrated how our
reÔ¨Åned model detects 92% of adversarial volumetric attacks.

REFERENCES

[2]

[1] A. Abusnaina, A. Khormali, H. Alasmary, J. Park, A. Anwar, and
A. Mohaisen, ‚ÄúAdversarial Learning Attacks on Graph-based IoT Mal-
ware Detection Systems,‚Äù in Proc. IEEE ICDCS, Dallas, USA, Jul 2019.
J. Anand, A. Sivanathan, A. Hamza, and H. H. Gharakheili, ‚ÄúPARVP:
Passively Assessing Risk of Vulnerable Passwords for HTTP Authenti-
cation in Networked Cameras,‚Äù in Proc. ACM Workshop on Descriptive
Approaches to IoT Security, Network, and Application ConÔ¨Åguration
(DAI-SNAC), Virtual Event, Germany, Dec 2021.

[3] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar, ‚ÄúThe Security of
Machine Learning,‚Äù Springer Machine Learning, vol. 81, p. 121‚Äì148,
May 2010.

[4] L. Breiman, ‚ÄúRandom Forests,‚Äù Springer Machine learning, vol. 45,

no. 1, pp. 5‚Äì32, Oct 2001.

[5] S. Calzavara, C. Lucchese, G. Tolomei, S. A. Abebe, and S. Orlando,
‚ÄúTreant: Training Evasion-aware Decision Trees,‚Äù Data Mining and
Knowledge Discovery, vol. 34, no. 5, pp. 1390‚Äì1420, 2020.

[6] G. Caminero, M. L. Mart¬¥ƒ±n, and B. Carro, ‚ÄúAdversarial Environment
Reinforcement Learning Algorithm for Intrusion Detection,‚Äù Computer
Networks, vol. 159, pp. 96‚Äì109, Aug 2019.

[7] N. Carlini and D. Wagner, ‚ÄúAdversarial Examples Are Not Easily
Detected: Bypassing Ten Detection Methods,‚Äù in Proc. ACM AISec,
Dallas, USA, Nov 2017.

[8] H. Chen, H. Zhang, D. Boning, and C. Hsieh, ‚ÄúRobust Decision Trees
Against Adversarial Examples,‚Äù in Proc. ICML, Long Beach, CA, USA,
Jun 2019.

[9] M. Cheng, T. Le, P.-Y. Chen, J. Yi, H. Zhang, and C.-J. Hsieh,
‚ÄúQuery-EfÔ¨Åcient Hard-label Black-box Attack:An Optimization-based
Approach,‚Äù in proc. of ICLR, Vancouver, Canada, Apr 2018.

[10] F. Croce and M. Hein, ‚ÄúReliable Evaluation of Adversarial Robustness
with an Ensemble of Diverse Parameter-free Attacks,‚Äù arXiv e-prints,
p. arXiv:2003.01690, Mar 2020.

‚ÄúUniversity
light

by
attacked
bulbs & 5,000

[11] CSO,
smart
[Online].
university-attacked-by-its-own-vending-machines-smart-light-bulbs-and-5-000-iot-devices.
html

vending machines,
2017.
Feb
https://www.csoonline.com/article/3168763/

Available:

its
IoT

devices,‚Äù

own

Fig. 16. Number of recipes generated for SYN reÔ¨Çection attack before and
after patching the model.

performed on the tree from Fig. 13. Adversarial instances will
be routed to the red leaves, which results in true detection of
the attack.

Although essential patching is necessary to Ô¨Åx some un-
bounded features, it does not cover the second scenario where
some features do not even exist on some paths. For example,
leaf ‚ÄúA‚Äù in Fig. 14, still can be targeted with volumetric attack
on f3 = +‚àû. In fact, some adversarial attacks can be captured
solely with essential patching (e.g., SSDP reÔ¨Çection attack
in Fig. 15). To overcome this problem, we deÔ¨Åne Additional
Patching which patches all leaves for a given feature. This way,
regardless of the trees‚Äô structure, we make sure all leaves are
patched against adversarial volumetric attacks on the desired
features. Additional patching can be done in different ways;
for example, one may choose to patch all leaves for all possible
features to create maximum robustness, making the model
complex and slow. Another one may use expert knowledge
to select speciÔ¨Åc vulnerable features to limit the complexity
of the model while making it robust against attacks over
those features that are more likely to be targeted by attackers.
After applying the essential patching to our original model,
we quantify the accuracy and false-positive over the testing
dataset and they are 96% and 0.09%, respectively, which are
exactly the same as the original model.

Fig. 15 shows the performance of our patching methods
over adversarial SSDP and SYN reÔ¨Çection attacks with a
range of attack impacts (100 to 1,000 packet count). Note
that these attack instances are created based on adversarial
recipes, bypassing the unpatched model, as shown in the
previous section. Essential patching is sufÔ¨Åcient to detect all
SSDP reÔ¨Çection attacks while detecting 86% of the high-
impact SYN reÔ¨Çection attacks. Having an additional patch over
‚Üë WAN packet count feature increases the detection rate for low,
medium, and high impact attacks; and detects all attacks with
an impact greater than 636 packets. The average impact of
SYN reÔ¨Çection attacks that can still bypass the model is 117
packets across our IoT devices, with the maximum impact
of 636 packets for Chromecast. Compared to the vulnerable
model, which has no limit over the attack packet count (i.e.,
unlimited impact), this impact shows a signiÔ¨Åcant improvement
in terms of robustness.

Finally, we validate the efÔ¨Åcacy of our patched model
by re-running the adversarial ofÔ¨Çine learning function on the
reÔ¨Åned model. Fig. 16 compares the number of generated
recipes in the patched model versus the original (unpatched)
model. The patched model (green bars) has become more

[12] Cyberedge, ‚ÄúCyberthreat Defense Report,‚Äù https://cyber-edge.com/wp-
content/uploads/2020/03/CyberEdge-2020-CDR-Report-v1.0.pdf, 2020.
[13] G. S. Dhillon, K. Azizzadenesheli, Z. C. Lipton, J. Bernstein, J. KossaiÔ¨Å,
A. Khanna, and A. Anandkumar, ‚ÄúStochastic Activation Pruning for
Robust Adversarial Defense,‚Äù arXiv e-prints, p. arXiv:1803.01442, Mar
2018.

14

2004006008001000impact02468101214# adversarial recipesunpatched modelpatched model[32] NETSCOUT Security, ‚ÄúA Deeper Look at IoT Weaponization,‚Äù 2020.

[Online]. Available: https://bit.ly/3pr3NJT

[33] T. D. Nguyen, S. Marchal, M. Miettinen, H. Fereidooni, N. Asokan,
and A. Sadeghi, ‚ÄúD¬®IoT: A Federated Self-learning Anomaly Detection
System for IoT,‚Äù in Proc. IEEE ICDCS, Dallas, USA, Jul 2019.
[34] Nokia, ‚ÄúThreat Intelligence Report 2020,‚Äù Computer Fraud & Security,

vol. 2020, no. 11, 2020.

[35] OPTIV,

Florida Water

‚ÄúAttempted
IoT/OT

derscores
line]. Available:
attempted-Ô¨Çorida-water-supply-tampering-underscores-iotot-security

Tampering Un-
[On-
https://www.optiv.com/explore-optiv-insights/blog/

Challenges,‚Äù

Security

Supply

2021.

Feb

[36] Palo Alto Networks, ‚ÄúUnit42 IoT Threat Report,‚Äù Apr 2020. [Online].

Available: https://start.paloaltonetworks.com/unit-42-iot-threat-report

[37] N. Papernot, P. D. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, ‚ÄúThe Limitations of Deep Learning in Adversarial Settings,‚Äù
in Proc. IEEE EuroS&P, Saarbr¬®ucken, Germany, Mar 2016.
[38] A. Pashamokhtari, N. Okui, Y. Miyake, M. Nakahara,

and
H. Habibi Gharakheili, ‚ÄúInferring Connected IoT Devices from IPFIX
Records in Residential ISP Networks,‚Äù in IEEE LCN, Virtual Event,
Canada, Oct 2021.

[39] Y. E. Sagduyu, Y. Shi, and T. Erpek, ‚ÄúIoT Network Security from
the Perspective of Adversarial Deep Learning,‚Äù in Proc. IEEE SECON,
Boston, USA, Jun 2019.

[40] A. Sivanathan, H. H. Gharakheili, and V. Sivaraman, ‚ÄúManaging IoT
Cyber-Security using Programmable Telemetry and Machine Learning,‚Äù
IEEE Transactions on Network and Service Management, vol. 17, no. 1,
pp. 60‚Äì74, 2020.

[41] A. Sivanathan, H. Habibi Gharakheili, F. Loi, A. Radford, C. Wije-
nayake, A. Vishwanath, and V. Sivaraman, ‚ÄúClassifying IoT Devices
in Smart Environments Using Network TrafÔ¨Åc Characteristics,‚Äù IEEE
TMC, vol. 18, no. 8, pp. 1745‚Äì1759, Aug 2019.

[42] A. Sivanathan, H. Habibi Gharakheili, and V. Sivaraman, ‚ÄúDetecting
Behavioral Change of IoT Devices using Clustering-Based Network
TrafÔ¨Åc Modeling,‚Äù IEEE Internet of Things Journal, vol. 7, no. 8, pp.
7295‚Äì7309, Aug 2020.

[43] A. Sivanathan, D. Sherratt, H. H. Gharakheili, A. Radford, C. Wije-
nayake, A. Vishwanath, and V. Sivaraman, ‚ÄúCharacterizing and Clas-
sifying IoT TrafÔ¨Åc in Smart Cities and Campuses,‚Äù in Proc. IEEE
INFOCOM Workshops, Atlanta, USA, May 2017.

[44] Verimatrix, ‚ÄúIoT security for

today‚Äôs connected world.‚Äù [Online].

Available: https://www.verimatrix.com/markets/internet-of-things
[45] P. V¬®ah¬®akainu, M. Lehto, and A. Kariluoto, ‚ÄúIoT-based Adversarial
Attack‚Äôs Effect on Cloud Data Platform Service in Smart Building‚Äôs
Context,‚Äù in Proc. ICCWS, Norfolk, USA, Mar 2020.

[46] H. Wang and C. Yu, ‚ÄúA Direct Approach to Robust Deep Learning
Using Adversarial Networks,‚Äù arXiv e-prints, p. arXiv:1905.09591, May
2019.

[47] C. Xie, Y. Wu, L. Maaten, A. L. Yuille, and K. He, ‚ÄúFeature Denoising
for Improving Adversarial Robustness,‚Äù in Proc. IEEE CVPR, CA,
USA, Jun 2019.

[48] ZDNet, ‚ÄúRansomware attack halts production at IoT maker Sierra Wire-
less,‚Äù Mar 2021. [Online]. Available: https://www.zdnet.com/article/
ransomware-attack-halts-production-at-iot-maker-sierra-wireless/
[49] C. Zhang, H. Zhang, and C.-J. Hsieh, ‚ÄúAn EfÔ¨Åcient Adversarial Attack

for Tree Ensembles,‚Äù in Proc. NeurIPS, Virtual, Dec 2020.

[14] Y. Ding, L. Wang, H. Zhang, J. Yi, D. Fan, and B. Gong, ‚ÄúDefending
Against Adversarial Attacks Using Random Forests,‚Äù in Proc. CVPR,
Long Beach, CA, USA, Jun 2019.

[15] R. Doshi, N. Apthorpe, and N. Feamster, ‚ÄúMachine Learning DDoS
Detection for Consumer Internet of Things Devices,‚Äù in Proc. IEEE
S&P Workshops, San Francisco, CA, USA, May 2018.

[17]

[16] A. Ferdowsi and W. Saad, ‚ÄúGenerative Adversarial Networks for
Distributed Intrusion Detection in the Internet of Things,‚Äù in Proc. IEEE
GLOBECOM, Waikoloa, USA, Dec 2019.
I. J. Goodfellow, J. Shlens, and C. Szegedy, ‚ÄúExplaining and Harnessing
Adversarial Examples,‚Äù arXiv e-prints, p. arXiv:1412.6572, Dec 2014.
[18] A. Hamza, H. Habibi Gharakheili, T. A. Benson, and V. Sivaraman,
‚ÄúDetecting Volumetric Attacks on IoT Devices via SDN-Based Moni-
toring of MUD Activity,‚Äù in Proc. ACM SOSR, CA, USA, Apr 2019.
[19] A. Hamza, D. Ranathunga, H. Habibi Gharakheili, T. A. Benson,
M. Roughan, and V. Sivaraman, ‚ÄúVerifying and Monitoring IoTs Net-
work Behavior using MUD ProÔ¨Åles,‚Äù IEEE Transactions on Dependable
and Secure Computing, pp. 1‚Äì1, 2020.

[20] O. Ibitoye, O. ShaÔ¨Åq, and A. Matrawy, ‚ÄúAnalyzing Adversarial Attacks
Against Deep Learning for Intrusion Detection in IoT Networks,‚Äù in
Proc. IEEE GLOBECOM, Waikoloa, USA, Dec 2019.
IBM X-Force Research, ‚ÄúThe weaponization of IoT devices,‚Äù 2017.
[Online]. Available: https://www.ibm.com/downloads/cas/6MLEALKV
[22] A. Kantchelian, J. D. Tygar, and A. D. Joseph, ‚ÄúEvasion and Hardening

[21]

of Tree Ensemble ClassiÔ¨Åers,‚Äù in Proc. ICML, NY, USA, Jun 2016.

[23] Z. Katzir and Y. Elovici, ‚ÄúQuantifying the Resilience of Machine
Learning ClassiÔ¨Åers Used for Cyber Security,‚Äù Expert Systems with
Applications, vol. 92, pp. 419‚Äì429, Feb 2018.

[24] T. M. Khoshgoftaar, M. Golawala, and J. V. Hulse, ‚ÄúAn Empirical Study
of Learning from Imbalanced Data Using Random Forest,‚Äù in Proc.
IEEE ICTAI, Patras, Greece, Oct 2007.

[25] M. K¬®uhrer, T. Hupperich, C. Rossow, and T. Holz, ‚ÄúHell of a Hand-
shake: Abusing TCP for ReÔ¨Çective AmpliÔ¨Åcation DDoS Attacks,‚Äù in
Proc. USENIX WOOT, San Diego, USA, Aug. 2014.

[26] M. Labs,

‚ÄúMcAfee Labs

2019 Threats Predictions Report,‚Äù

https://www.mcafee.com/blogs/other-blogs/mcafee-labs/mcafee-labs-
2019-threats-predictions/, 2018.

[27] Y. LeCun, C. Cortes, and C. J. Burges, ‚ÄúThe MNIST database of
handwritten digits,‚Äù 1998. [Online]. Available: http://yann.lecun.com/
exdb/mnist/

[28] M. Lyu, D. Sherratt, A. Sivanathan, H. H. Gharakheili, A. Radford, and
V. Sivaraman, ‚ÄúQuantifying the ReÔ¨Çective DDoS Attack Capability of
Household IoT Devices,‚Äù in Proc. ACM WiSec, Boston, USA, Jul 2017.
‚ÄúNew Vulnerability Allows DDoS Attack
2020.
https://www.cpomagazine.com/cyber-security/

and Data ExÔ¨Åltration
[Online]. Available:
new-vulnerability-allows-ddos-attack-and-data-exÔ¨Åltration-on-billions-of-devices/

[29] C. Magazine,

of Devices,‚Äù

on Billions

Jun

[30] H. Mahmudul, M. M. Islam, M. I. I. Zarif, and M. M. A. Hashem,
‚ÄúAttack and Anomaly Detection in IoT Sensors in IoT Sites Using
Machine Learning Approaches,‚Äù Elsevier Internet of Things, vol. 7, p.
100059, Sep 2019.

[31] M. Miettinen, M. S, I. Hafeez, T. Frassetto, N. Asokan, A. R. Sadeghi,
and S. Tarkoma, ‚ÄúIoT SENTINEL: Automated Device-Type IdentiÔ¨Åca-
tion for Security Enforcement in IoT,‚Äù in Proc. IEEE ICDCS, Atlanta,
USA, June 2017.

15

