2
2
0
2

p
e
S
1

]

R
C
.
s
c
[

1
v
0
7
1
0
0
.
9
0
2
2
:
v
i
X
r
a

CPS Attack Detection under Limited Local Information in Cyber Security: A
Multi-node Multi-class Classification Ensemble Approach

JUNYI LIU‚àó, Weiyang College, Tsinghua University, China
YIFU TANG‚àó, Department of Mathematics, Zhili College, Tsinghua University, China
HAIMENG ZHAO, Zhili College, Tsinghua University, China
XIEHENG WANG, Tsinghua University, China
FANGYU LI‚Ä†, Beijing University of Technology, China
JINGYI ZHANG‚Ä†, Department of Industrial Engineering, Center for Statistical Science, Tsinghua University, China

Cybersecurity breaches are the common anomalies for distributed cyber-physical systems (CPS). However, the cyber security breach

classification is still a difficult problem, even using cutting-edge artificial intelligence (AI) approaches. In this paper, we study the

multi-class classification problem in cyber security for attack detection. A challenging multi-node data-censoring case is considered.

In such a case, data within each data center/node cannot be shared while the local data is incomplete. Particularly, local nodes contain

only a part of the multiple classes. In order to train a global multi-class classifier without sharing the raw data across all nodes, the

main result of our study is designing a multi-node multi-class classification ensemble approach. By gathering the estimated parameters

of the binary classifiers and data densities from each local node, the missing information for each local node is completed to build

the global multi-class classifier. Numerical experiments are given to validate the effectiveness of the proposed approach under the

multi-node data-censoring case. Under such a case, we even show the out-performance of the proposed approach over the full-data

approach.

Additional Key Words and Phrases: federated learning, ensemble learning, multi-class classification, cyber security

ACM Reference Format:

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang. 2022. CPS Attack Detection under Limited Local

Information in Cyber Security: A Multi-node Multi-class Classification Ensemble Approach. 1, 1 (September 2022), 22 pages.

1 INTRODUCTION

Over the past few decades, collecting and storing data in multiple local data centers/nodes rather than one single node

has become more common in various areas due to the fast-developing technologies in data collection and processing

[1‚Äì8]. Such multi-node data then brings challenging environments in data analysis and integration [6, 9], and has a

‚àóBoth authors contributed equally to this research.
‚Ä†Corresponding author.

Authors‚Äô addresses: Junyi Liu, liujunyi20@mails.tsinghua.edu.cn, Weiyang College, Tsinghua University, Beijing, China; Yifu Tang, tangyf20@icloud.com,
Department of Mathematics, Zhili College, Tsinghua University, Beijing, China; Haimeng Zhao, Zhili College, Tsinghua University, Beijing, China,
haimengzhao@icloud.com; Xieheng Wang, Tsinghua University, Beijing, China, wxh19@mails.tsinghua.edu.cn; Fangyu Li, Beijing University of
Technology, Beijing, China, fangyu.li@bjut.edu.cn; Jingyi Zhang, Department of Industrial Engineering, Center for Statistical Science, Tsinghua
University, 30 Shuangqing Rd, Beijing, China, jingyizhang@tsinghua.edu.cn.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

¬© 2022 Association for Computing Machinery.
Manuscript submitted to ACM

Manuscript submitted to ACM

1

 
 
 
 
 
 
2

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang

higher risk of being attacked by hackers [10, 11]. As a result, cyber security problems have gained more attention in

many fields, such as the autonomous driving system [12, 13], the AI-assisted diagnosis and treatment system [14, 15],

and the Internet of Things (IoT) system [11, 16, 17]. In cyber security problems, attack detection is of particular interest

[10, 11]. There are a large number of works done for attack detection in the field of cyber security. Aitor Belenguer et al.

[18] review applications of Federated Learning in Intrusion Detection. They introduce the development of intrusion

detection systems and works including centralized learning, federated learning, and FEDAVG algorithm. They discuss

the limitation of current works and future technologies. Many of the reviewed works associate the intrusion detection

problem with the classification problem. Besides, [19] compare various machine learning methods for intrusion detection

systems, including supervised learning, unsupervised learning, and reinforcement learning. They discuss both binary

classification and multi-class classification scenarios according to the IoT-23 dataset. Classification problems play an

important role in intrusion detection systems since different types of attacks could be seen as different classes. And

among all the aforementioned attack-detection approaches, one of the main tasks is multi-class classification.

Fig. 1. Illustrations of existing multi-node multi-class classification approaches. Data in each local node only contains one type of
attack. Based on the local data, only the binary classifiers can be obtained, denoted as ùëê1, ùëê2 and ùëê3. Our goal is to obtain the global
classifier ùëê.

Although extensive approaches have been developed for federated or decomposition multi-class classification in

the past few years, most of the approaches require either of the following assumptions. (1) Data in each local node

Manuscript submitted to ACM

CPS Attack Detection under Limited Local Information in Cyber Security: A Multi-node Multi-class Classification
Ensemble Approach

3

contains complete information for the multi-class classification, i.e., each node contains all classes of data so that a

multi-class classifier can be calculated within each local node [20, 21], and then the global classifier can be obtained

through a federated average. (2) Data are shared across all local nodes so that the multi-class classifier can be obtained

by either the one-vs-all decomposition scheme or the one-vs-one decomposition scheme, i.e., the multi-class classifier

is obtained through an ensemble of multiple binary classifiers [22‚Äì24]. Figure 1 illustrates the two typical types of

existing approaches. Panel (a) in figure 1 shows the case in a federated approach, where assumption (1) is satisfied,

and the local nodes can only access their own data. Panel (b) and (c) in figure 1 show the cases for an one-vs-all

and an one-vs-one decomposition approach, respectively. In these cases, assumption (2) is satisfied. However, such

assumptions may not hold in real-world applications. For example, in disease screening studies, data collected from a

basic medical center may only contain records from normal people and mild patients. On the other hand, in specialist

hospitals, the majority of the records are from severe patients and critically ill patients. Such cases are also popular in

intrusion-detection studies in IoT. In such studies, different attackers may affect different parts of the network, and

carry out different types of attacks. As a result, within a certain node, there may only exist one typical type of attack

[25, 26]. We now consider a toy example illustrated in figure 2. Suppose there are three local nodes connected to a

central node. Within each node, there exists normal data, shown as the green signals. There also exist three types of

attackers, each of which is specified for a different local node. The attacked data is shown as the blue, red, and black

signals, respectively. In such cases, information in each local node is incomplete, we thus call the local nodes ‚Äúcensoring

Fig. 2. An illustration of the attack detection setting with censoring nodes. Data in each local node only contains one type of attack.
Based on the local data, only the binary classifiers can be obtained, denoted as ùëê1, ùëê2 and ùëê3. Our goal is to obtain the global classifier
ùëê.

nodes‚Äù. Because of the data censoring, each local node can only train classifiers for its existing classes based on its

data, such as the three local classifiers shown in figure 2. However, in real-world applications, no one can guarantee

Manuscript submitted to ACM

4

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang

that data from the missing classes will never appear in the future. For such data, the pre-trained classification model

fails. The information for the missing classes needs to be obtained to compensate for the classification failure for local

nodes and to obtain a global/complete classifier. One naive approach is pooling data from different nodes together

to obtain the global multi-class classifier for all classes. However, in practice, data privacy and communication cost

are crucial concerns. Sharing data across all nodes is infeasible [5, 6]. There is an urgent need to learn the missing

knowledge from other nodes without accessing their raw data. On the other hand, the data censoring is related to the

practical network structures and node features, it is unlikely to obtain information on all possible combinations of pairs

of classes. Therefore, the existing ensembling strategies described in [22‚Äì24] are also infeasible.

Our contributions. To combat the obstacles, we develop a novel multi-node multi-class classification ensemble

approach with a combination of local classification with density estimation for multi-node attack detection, especially

when data is censored within each local node. The approach is inspired by the ensemble learning approaches which

integrate weak learners obtained in each local node to build a strong learner. Similarly, in the proposed multi-node

multi-class classification ensemble approach, the ‚Äúweak learners‚Äù can be regarded as the binary classifiers trained within

each node using the local data; and the ‚Äústrong learner‚Äù is the final multi-class classifier. Unlike traditional ensemble

learning approaches, in which the local tasks within each node are based on full data, in the proposed approach, the

local tasks are some ‚Äúpartial‚Äù tasks of the global multi-class classification task based on the censored local data. By

combining local classification and density estimation, we obtain the informative parameter estimations for each class.

We then complete the missing information of unobserved classes for each local node through communication with such

estimations. The completed missing information can help each local node defend against potential attacks in the future,

even if such types of attacks have not appeared in the past. In addition, we show the proposed method is not specific

to the attack detection problems. It is potentially suitable for general multi-class classification problems under data

censoring with different types of data structures. In general, we summarize the contribution of the paper from three

aspects.

(1) We propose a novel multi-node multi-class classification problem setting with censoring local nodes.

(2) We discuss several data/class censoring structures that are common in real-world applications.

(3) We propose an ensemble approach to aggregate local binary classifiers into a global multi-class classifier that is

suitable for censoring nodes without sharing local data.

(4) The one-shot ensemble and the flexibility in the assumption of data structure make it possible to extend

the proposed approach to a general multi-node multi-class classification problem under a federated learning

framework.

2 PROBLEM AND METHODOLOGY

2.1 Traditional single-node and multi-node classification

ùëõ ùëó
Suppose ùêΩ different nodes, denoted as Node1, . . . , NodeùêΩ , are connected to a central node Nodeùëê . Data {ùíôùëñ ùëó , ùë¶ùëñ ùëó }
ùëñ=1
are collected from Nodeùëó , where ùíôùëñ ùëó ‚àà Rùëë denotes the ùëë-dimensional covariate, and ùë¶ùëñ ùëó denotes the response label.
We assume that there are ùëÄ + 1 classes in total, i.e., ùë¶ùëñ ùëó ‚àà {0, 1, . . . , ùëÄ }. For example, in medical research, ùë¶ = 0
always represents the control group, and ùë¶ = 1, . . . , ùëÄ represent ùëÄ case groups. Another example is the Intrusion
Detection System study, where ùë¶ = 0 represents normal/un-attacked records and ùë¶ = 1, . . . , ùëÄ represent ùëÄ attacks. Let
M ùëó ‚äÇ {0, 1, . . . , ùëÄ } be the subset of classes exist in Nodeùëó . When we consider the classification problem within a single

Manuscript submitted to ACM

CPS Attack Detection under Limited Local Information in Cyber Security: A Multi-node Multi-class Classification
Ensemble Approach

5

Nodeùëó , the local classifier ùëê (ùëö ùëó )

ùëó

(ùíô), ùëö ùëó ‚àà M ùëó can be obtained through minimizing the following loss function,

ùëê (ùëö ùëó )
ùëó

, ùëö ùëó ‚àà M ùëó = arg

min
,,ùëö ùëó ‚ààM ùëó

ùëê (ùëö ùëó )
ùëó

ùëõ ùëó
‚àëÔ∏Å

‚àí

‚àëÔ∏Å

ùëñ=1

ùëö ùëó ‚ààM ùëó

Ô£±Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥
Ô£≥

1(ùë¶ùëñ ùëó = ùëö ùëó ) log ùëê (ùëö ùëó )

ùëó

(ùíôùëñ ùëó )

Ô£ºÔ£¥Ô£¥Ô£Ω
Ô£¥Ô£¥
Ô£æ

s.t. ‚àëÔ∏Å
ùëö ùëó ‚ààM ùëó

ùëê (ùëö ùëó )
ùëó

= 1.

(1)

For a traditional multi-node classification, the final goal is to obtain the ùëÄ + 1-class classifier ùëê (ùëö) (ùíô) := ùëÉ (ùë¶ = ùëö|ùíô) for
ùëö = 0, 1, . . . , ùëÄ, which minimizes the cross-entropy loss as follows,

ùëê (0), . . . , ùëê (ùëö) = arg min

ùëê (0) ,...,ùëê (ùëö)

ùêΩ
‚àëÔ∏Å

ùëõ ùëó
‚àëÔ∏Å

ùëÄ
‚àëÔ∏Å

‚àí

ùëó=1

ùëñ=1

ùëö=0

1(ùë¶ùëñ ùëó = ùëö) log ùëê (ùëö) (ùíôùëñ ùëó )

s.t.

ùëÄ
‚àëÔ∏Å

ùëö=0

ùëê (ùëö) = 1,

(2)

Ô£±Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥
Ô£≥

Ô£ºÔ£¥Ô£¥Ô£Ω
Ô£¥Ô£¥
Ô£æ

where the function 1(ùë¶ùëñ ùëó = ùëö) is the indicator function that equals 1 when ùë¶ùëñ ùëó = ùëö, otherwise 0.

2.2 Ensemble multi-node multi-class classification

Recall that in federated approaches, M ùëó = {0, 1, . . . , ùëÄ }, thus the loss function for a global multi-class classification
problem is simply a summation of all the loss functions of the local multi-class classification problems. It is easy to
see that the global multi-class classifier can be obtained through a combination of all local ùëê ùëó ‚Äôs. On the other hand,
in decomposition approaches, even though ùëê ùëó ‚Äôs within each local node are binary classifiers, the shared data and the
complete information between one class to all other classes make it possible to expand the binary ùëê ùëó ‚Äôs to a global
multi-class classifier. However, we consider the cases when the local nodes are censoring nodes. Typically, we focus

on the cases when each local node only contains data in two classes. For example, if we follow the attack detection
problem setting illustrated in figure 2, we assume within each local node, there exists un-attacked data (ùë¶ = 0) together
with only one type of attack data (ùë¶ = ùëö, ùëö ‚àà {1, . . . , ùëÄ }). According to the proposed data-censoring setting, ùëê ùëó ‚Äôs within
each local node are binary one-vs-one classifiers. The access to the information of the unobserved classes for each

local node is prevented by its censored data and the lack of enough pair-wise binary classifiers. Without the complete

pair-wise information for all possible combinations of classes, it is challenging to obtain a feasible and effective global

multi-class classifier based on the aggregation of limited local classifiers, especially when local data are not allowed to

be shared. We thus need to extract more features from the local data, other than the estimated parameters for local

classifiers, to communicate across all nodes so that the local nodes can complete their missing information.

It is obvious that the global multi-class classifier ùëê (0), . . . , ùëê (ùëö) for classes 0, 1, . . . , ùëÄ depends on the information of
the following two aspects: (1) the information of the response label ùë¶ for all classes, i.e. ùëÉ (ùë¶ = ùëö) for ùëö = 0, 1, . . . , ùëÄ, and
(2) the information of the predictor ùíô for all classes. In each local node, we lack both of the aforementioned information

for the missing classes. We thus need to borrow information from other nodes to fill in the missing parts. Intuitively,
the information of the response label ùëÉ (ùë¶ = ùëö) can be provided from the local classifiers ùëê (ùëö)
‚Äôs, and the information
of ùíô is obtained by integrating the local density estimation of ùíôùëñ ùëó in each node. Specifically, the global classifier ùëê (ùëö)
can be regarded as a summation of conditional probabilities ùëÉ (ùë¶ = ùëö, data ‚àà Nodeùëó |ùíô) for ùëó = 1, . . . , ùêΩ . In addition,
when the probability ùëÉ (data ‚àà Nodeùëó ) and the distribution of ùíô in ùëó-th node are given, the conditional probability
ùëÉ (ùë¶ = ùëö, data ‚àà Nodeùëó |ùíô) is proportional to ùëÉ (ùë¶ = ùëö|data ‚àà Nodeùëó , ùíô), which is the local classifier ùëê (ùëö)
. Motivated by
this intuition, letting ùëì ( ùëó)
ùëÅ = ùëÉ (data ‚àà Nodeùëó ), ùëó = 1, 2, . . . , ùêΩ , we derive
the ensembling of local classifiers as

be the distribution of ùíô in ùëó-th node, and ùëù ( ùëó)

ùíô

ùëó

ùëó

ùëê (ùëö) =

ùêΩ
‚àëÔ∏Å

ùëó=1

ùë§ ùëó,ùëöùëê (ùëö)
ùëó

,

Manuscript submitted to ACM

6

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang

where the weight ùë§ ùëó,ùëö ‚àù 1(ùëö ‚àà M ùëó )ùëî

(cid:16)

ùëì ( ùëó)
ùíô

, ùëù ( ùëó)
ùëÅ

(cid:17)

with 1(¬∑) denoting the indicating function, and ùëî(¬∑, ¬∑) denoting some

unknown function. One possible choice of function ùëî is ùëî
follows.
Main result Suppose the local classifier for class ùë¶ = ùëö obtained by minimizing (1) in node ùëó is ÀÜùëê (ùëö)
(cid:16)
ùëù ( ùëó)
ÀÜùëê (ùëö)
ùëó
ùëÅ

as above. Let ùë§ ùëó =

. If ùë§ ùëó ‚â• 1
ùêΩ

(cid:17) ùêΩ ‚àí1

ùëó

, then we have that the following integrated classifier

(cid:16)

ùëì ( ùëó)
ùíô

, ùëù ( ùëó)
ùëÅ

(cid:17)

= ùëì ( ùëó)
ùíô

ùëù ( ùëó)
ùëÅ . We then state our main result as

. Define M ùëó , ùëì ( ùëó)

ùíô

and

ùíô ùëù ( ùëó )
ùëì ( ùëó )
ùëÅ
ùíô ùëù ( ùëó )
ùëì ( ùëó )

ùëÅ

ùëó =1

(cid:205)ùêΩ

is a feasible solution of (2). The result is proved in Appendix A.

ÀÜùëê (ùëö) =

ùêΩ
‚àëÔ∏Å

ùëó=1

1(ùëö ‚àà M ùëó )ùë§ ùëó ÀÜùëê (ùëö)

ùëó

(3)

ùëÅ can be estimated using ÀÜùëù ( ùëó)

2.3 Gaussian mixture model for single-node density estimation
In practice, ùëù ( ùëó)
is then the density estimation for ùëì ( ùëó)
Specifically, we assume ùíô in node ùëó follows a mixture of ùêæ Gaussian components, i.e.

ùëõ ùëó
(cid:205)ùëó ùëõ ùëó , where ùëõ ùëó is the sample size in ùëó-th node. The remaining problem
. In this paper, we use the Gaussian mixture model (GMM) [27] to obtain ÀÜùëì ( ùëó)
.

ùëÅ =

ùíô

ùíô

ùëì ( ùëó)
ùíô

(ùíô) =

ùêæ
‚àëÔ∏Å

ùëò=1

ùúã ( ùëó)
ùëò

ùëìùíô |ùëß ùëó =ùëò (ùíô),

(4)

, Œ£( ùëó)

( ùëó)
ùëò

ùëò ) comes from the ùëò-th Gaussian component, and ùëß ùëó is the latent variable in node ùëó with
where ùíô |ùëß ùëó = ùëò ‚àº ùëÅ (ùùÅ
ùëß ùëó = ùëò indicating that the data comes from the ùëò-th Gaussian component, and ùúã ( ùëó)
ùëò = ùëÉ (ùëß ùëó = ùëò). Given the number of
components ùêæ, model (4) can be estimated through the expectation maximization (EM) algorithm [27, 28]. In GMM, a
typical problem is choosing a proper number of components ùêæ [28]. In practice, we use a scree-plot of the likelihood
score to determine the proper number of components.

2.4 Main algorithm

Following the strategies above, the Ensemble Multi-node Multi-class Classification (EMMC) algorithm is detailed in

algorithm 1.

Algorithm 1 Ensemble Multi-node Multi-class Classification Algorithm

Input: data collected from ùêΩ nodes, {ùíôùëñ ùëó , ùë¶ùëñ ùëó }
for ùëó in 1, . . . , ùêΩ do

ùëõ ùëó
ùëñ=1

, ùëó = 1, . . . , ùêΩ .

Within ùëó-th node:
Step 1: Fit a binary classification model based on {ùíôùëñ ùëó , ùë¶ùëñ ùëó }
Step 2: Fit a GMM for ùíôùëñ ùëó , and obtain ÀÜùëì ( ùëó)
using EM algorithm.
ùíô
ùëõ ùëó
(ùíô), ÀÜùëì ( ùëó)
Step 3: Send ÀÜùëê (ùëö)
(cid:205)ùëó ùëõ ùëó to the central node.
ùíô

, and ÀÜùëù ( ùëó)

ùëÅ =

ùëõ ùëó

ùëó

ùëñ=1. and obtain ÀÜùëê (ùëö)

ùëó

(ùíô).

end for
Within the central node:
Step 4: Calculate the global classifier ÀÜùëê (ùëö) (ùíô) as in (3), for ùëö = 0, 1, . . . , ùëÄ.
Output: ÀÜùëê (ùëö) (ùíô), ùëö = 0, 1, . . . , ùëÄ.

Manuscript submitted to ACM

CPS Attack Detection under Limited Local Information in Cyber Security: A Multi-node Multi-class Classification
Ensemble Approach

7

3 ILLUSTRATION OF CONCEPT

In this section, we use some examples to illustrate the proposed problem setting and approach. Particularly, we consider

three typical structures for censoring local nodes. We then apply the proposed ensemble approach to both simulated

examples and real-world datasets under each structure. The experiments were conducted in Python using a Windows

computer with 16GB of memory and an 8 core CPU.

3.1 Simulated illustration

In this section, we use a simulated toy example to illustrate the proposed approach. Consider data with four classes
(ùëÄ = 3). Suppose the data for Class ùëö comes from a mixture distribution of ùêæùëö Gaussian components with mean ùùÅùëò,ùëö
and covariance matrix Œ£ùëò,ùëö, and each component is mixed with weight ùúãùëò,ùëö. The specific parameter settings are listed
in table 1.

Table 1. Parameter settings for each class in the simulated illustration.

class

class 0

class 1

class 2

class 3

Component
1
2
1
2
3
1
2
1
2
3

ùùÅùëò,ùëö
(‚àí6, ‚àí1)‚ä§
(‚àí8, 2)‚ä§
(3, 6)‚ä§
(4, 4)‚ä§
(5, 6)‚ä§
(0, ‚àí4)‚ä§
(0, ‚àí2)‚ä§
(‚àí1, 5)‚ä§
(‚àí2, 4)‚ä§
(‚àí2, 5)‚ä§

Œ£ùëò,ùëö ùúãùëò,ùëö
1/2
1/2
4/15
6/15
5/15
9/13
4/13
6/15
5/15
4/15

ùêº2

We show the effectiveness of the proposed method through examples under three typical settings of data structure,

which are detailed as follows.

I (The ‚Äústar‚Äù structure) Consider there are three nodes (ùêΩ = 3). Data {ùíô, ùë¶} within node ùëó comes from Class 0

and Class ùëó, i.e., ùë¶ ‚àà {0, ùëó }, ùëó = 1, 2, 3.

II (The ‚Äúring‚Äù structure) Consider there are four nodes (ùêΩ = 4). Data {ùíô, ùë¶} within node ùëó comes from Class ùëó ‚àí 1

and Class ùëó, i.e., ùë¶ ‚àà { ùëó ‚àí 1, ùëó }, ùëó = 1, . . . , 4, where we define ‚Äúùë¶ = 4‚Äù is equivalent to ‚Äúùë¶ = 0‚Äù.

III (The ‚Äúfully-connected‚Äù structure) Consider there are six nodes (ùêΩ = 6). Within node ùëó, we generate data
{ùíô, ùë¶} with ùë¶ ‚àà { ùëó1, ùëó2}, ùëó = 1, . . . , 6, where { ùëó1, ùëó2} is one of all the possible combinations of 0, 1, 2, and 3, with
ùëó1 ‚â† ùëó2.

We applied the proposed method to the simulated training data. For the first setting, we generated 14500, 13500, and

14500 samples within each node. Figure 3 shows the result of density estimation within each node. In figure 3, the left,

middle, and right columns each represent one single node respectively. The upper row shows the scree-plots of the
likelihood scores. Based on the scree-plots, we chose the number of Gaussian components within each node to be ùêæ = 3.
The bottom row of figure 3 visualizes the ground truth and estimated densities. We see that the generated samples from

the estimated density (shown as the red ‚Äú+‚Äùs) perfectly match the samples generated from the true density (shown as

the gray dots). Such an observation indicates the effectiveness of the density estimation through GMM. Similarly, based

Manuscript submitted to ACM

8

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang

on the scree-plots of the likelihood scores, we chose ùêæ = 3, 2, 2, 3 for each node in the second setting, respectively, and
ùêæ = 3, 2, 2, 3, 3, 3 for each node in the third setting, respectively.

Fig. 3. Screeplot of the likelihood score for each node in the first setting.

After the density estimation through GMM, we applied the logistic regression model to the local data to build the

binary classifier for each local node. Finally, we aggregated the estimated parameters of all local binary classifiers and

the local densities to construct the global multi-class classifier as in EMMC algorithm 1.

We generated 100, 150, 100, and 150 samples from Class 0, 1, 2, and 3, respectively, as the test data. We then carried

out the classification tasks for the testing data under each of the settings. Specifically, we calculated both the precision

and recall values defined as follows:

and

Precision =

TruePositive
TruePositive + FalsePositive

,

Recall =

TruePositive
TruePositive + FalseNegative

.

The results are listed in table 2. The values outside the brackets are the mean precision and recall based on 50 replicates,

and the values within the brackets are the corresponding standard deviations. From table 2, we observe that under all

three settings of data structure, the proposed method can provide a proper multi-class classifier with a high accuracy

based on the binary classifiers provided by each local node.

Table 2. Mean testing accuracy for each class under three settings with standard deviation in the brackets.

Class 0

Class 1

Class 2

Class 3

Precision

99.9%(.29%)
99.8%(.39%)
99.9%(.29%)

98.62%(.84%)
99.27%(.46%)
99.53%(.42%)

100%(0%)
100%(0%)
100%(0%)

99.13%(.72%)
98.68%(.92%)
99.2%(.49%)

Recall

100%(0%)
100%(0%)
100%(0%)

99.2%(.65%)
98.67%(.94%)
99.2%(.50%)

99.8%(.40%)
99.9%(.30%)
100%(0%)

98.6%(.87%)
99.2%(.50%)
99.47%(.50%)

I
II
III

I
II
III

In practice, the three types of structures may suitable for different scenarios. In the cases of attack detection, it is
obvious that the class ùë¶ = 0, i.e., the normal/un-attacked data exists within all local nodes. Thus the data structure
Manuscript submitted to ACM

CPS Attack Detection under Limited Local Information in Cyber Security: A Multi-node Multi-class Classification
Ensemble Approach

9

is by nature the ‚Äústar‚Äù structure. In the following section, the proposed approach was applied to two real-world

attack-detection datasets for further illustration.

3.2 An example for the NUSW-NB15 dataset

UNSW-NB15 is a dataset for cyber security research provided by [29]. The dataset contains normal/un-attacked data

and data of nine types of attacks: Fuzzers, Analysis, Backdoors, DoS, Exploits, Generic, Reconnaissance, Shellcode, and
Worms. Denote the nine types of attacks as ùë¶ = 1, . . . , 9, respectively. We applied the proposed multi-node multi-class
classification ensemble approach to see if the nine classes with ùë¶ = 1, . . . , 9 and the normal data (ùë¶ = 0) can be identified.
Besides the training data, UNSW-NB15 also provides a test dataset. Recall that the proposed approach is based on the

estimation of the data distribution, so a basic assumption is that there exists no distribution drift between the training

and test data. However, in practice, whether there exists a distribution drift between the training data and the testing

data is unknown. We thus need to consider a hypothesis testing for the distribution drift first. Denote the distribution
of the training data and the testing data as ùëùùëá ùëü
ùíô and
ùíô can be measured by the Kullback‚ÄìLeibler (KL) divergence ùêæùêø(ùëùùëá ùëü
ùëùùëá ùëí
ùíô , the KL
divergence ùêæùêø(ùëùùëá ùëü

ùíô , respectively. The difference between the distributions ùëùùëá ùëü

ùíô ) [30]. It is known that if ùëùùëá ùëü

ùíô and ùëùùëá ùëí

ùíô = ùëùùëá ùëí

ùíô ‚à•ùëùùëá ùëí

ùíô ‚à•ùëùùëá ùëí

ùíô ) = 0. We considered the following hypothesis testing,
ùíô ‚à•ùëùùëá ùëí

ùíô ) = 0; ùêªùëé : ùêæùêø(ùëùùëá ùëü

ùêª0 : ùêæùêø(ùëùùëá ùëü

ùíô ‚à•ùëùùëá ùëí

ùíô ) > 0.

Notice ùêæùêø(ùëùùëá ùëü
ùíô ‚à•ùëùùëá ùëí
ùíô ) is calculated based on the true distributions, which are unobserved. We can only obtain the
estimation of ùêæùêø(ùëùùëá ùëü
ùíô ‚à•ùëùùëá ùëí
ùíô ), denoted as (cid:99)ùêæùêø from the observed samples. Moreover, the distribution of (cid:99)ùêæùêø under the null
hypothesis (the ‚Äúnull distribution‚Äù) can be complicated and difficult to parameterize. We thus applied the non-parametric
testing by obtaining the empirical null distribution of (cid:99)ùêæùêø through randomly splitting the pooled dataset of the training
and test data repeatedly, and calculating the (cid:99)ùêæùêøs between the split datasets. We repeated the splitting 100 times and
calculated the proportion of the (cid:99)ùêæùêøs being greater than the original one as the testing p-value. We also carried out the
hypothesis testing on each class separately, and the results are listed in table 3. From the results, we can see that there

is a significant distribution drift between the training and test data. Specifically, the differences is significant in the

normal data and the data of attack Fuzzers, Analysis, Backdoor, and Generic. Therefore, we only use the given training

dataset in the experiment.

Table 3. P-values for the hypothesis testings of the distribution drifts between the training and testing datasets.

Class

Overall Normal

Fuzzers Analysis Backdoor DoS Exploits Generic Reconnaissance

Shellcode Worms

P-value

0.02

0.02

0.00

0.00

0.00

0.36

0.48

0.00

0.30

0.62

0.26

Since the proposed approach is based on an ensemble of the local classifiers, we need to check the sample size of

each type of attack to see if it is enough to obtain a good local classifier. The sample size for each type of attack is

listed in table 4. We observe that for the attack ‚ÄúWorms‚Äù, there are only 44 records, which is too small compared to

the rest of attacks. Thus we abandoned it in the experiment. Moreover, data balancing is also important for training

a good classifier. However, the dataset at hand highly skewed. Therefore, we balanced each type of attacks using

the following strategy: if the sample size of the attack is less than 1000, we keep the data untouched; if the sample

size is greater than 1000, we randomly pick out 1000 representatives. Note that NUSW-NB15 is a dataset for attack
detection, so we considered the ‚Äústar‚Äù structure, i.e. there are nine local nodes (J=9), and data {ùíô, ùë¶} within node ùëó
Manuscript submitted to ACM

10

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang

satisfies ùë¶ ‚àà {0, ùëó }, ùëñ = 1, . . . , 8. Again, to ensure the balance between the normal and the attacked data within each local
node, we sub-sampled the normal data according to the sample size of each type of attack, and assigned them to all the

nodes.

Table 4. Sample sizes for each type of attacks.

Attack

Fuzzers Analysis Backdoors DoS

Exploits Generic Reconnaissance

Shellcode Worms

Samplesize

6062

677

583

4089

11132

18871

3496

378

44

We randomly picked 75% of the pre-processed dataset to train the proposed classifier, and used the remaining 25% as

the testing set. We calculated the precision and recall of each class in the testing dataset for both the balanced data and

the original imbalanced data (with ‚ÄúWorms‚Äù deleted) to see the effect of the data balancing. The comparison is shown in

figure 4. Panel (a) shows the precisions and panel (b) shows the recalls. The yellow lines are the proportion of each

class in the training data, with the dashed lines representing the imbalanced data and the solid lines representing the

balanced data. The sample size for Class 0 is a combination of all nodes, thus the proportion is greater than other classes.

We observe that after balancing, the lines become much flatter. The groups of bars represent the test precisions/recalls

of each class. The red/purple bars show the ensemble/full-data results for the balanced data, and the brown/blue bars

show the ensemble/full-data results for the imbalanced data. We can observe that even though the sample sizes become

smaller than the original ones for some classes after balancing, the results are much better than those of the imbalanced

data. Such an observation verifies the necessity of data balancing. We then focus on the balanced data from now on.

Fig. 4. Comparison of the testing precisions and recalls of the ensemble (EMMC)/full-data (Full) approaches for the imbalanced and
balanced data.

Similar to the simulated example, we plot the scree-plots of the likelihood score, and chose ùêæ = 35 for the GMM in
each local node. For the local binary classifiers, we applied four commonly used machine learning approaches: Logistic

Regression (LR), Random Forest (RF), Support Vector Classifier (SVC), and Naive Bayes with Gaussian (NB). After 50

replicates, the testing precisions and recalls for each class are listed in table 5. The results show that the proposed

approach is feasible and effective when the training dataset is balanced.

Manuscript submitted to ACM

CPS Attack Detection under Limited Local Information in Cyber Security: A Multi-node Multi-class Classification
Ensemble Approach

11

Table 5. Mean classification precision and recall using different local classifiers with standard deviation of the balanced dataset

Normal

Analysis

Backdoor

DoS

Exploits

Fuzzers

Generic

Reconnaissance

Shellcode

Precision

LR 99.98%(.29%)
100%(0%)
RF

SVC 99.60%(.24%)
NB 100%(0%)

96.25%(1.91%)
96.18%(1.82%)
96.28%(1.85%)
96.08%(1.96%)

87.63%(3.61%)
87.15%(3.99%)
86.84%(4.20%)
87.44%(3.89%)

80.70%(3.81%)
81.21%(3.02%)
80.39%(3.43%)
81.07%(4.23%)

LR 96.57%(.37%)
96.38%(4.73%)
RF
SVC 96.69%(.44%)
NB 96.77%(.38%)

98.78%(1.31%)
99.13%(1.14%)
97.86%(1.90%)
99.48%(.88%)

95.40%(1.90%)
95.66%(3.39%)
95.21%(3.33%)
93.83%(3.60%)

85.25%(2.84%)
83.27%(4.49%)
82.98%(4.57%)
84.81%(3.82%)

81.67%(5.94%)
80.87%(5.28%)
78.98%(7.08%)
80.15%(7.97%)
Recall
80.80%(4.64%)
81.43%(5.78%)
77.44%(4.98%)
78.90%(7.3%)

88.55%(2.34%)
88.31%(4.14%)
88.50%(2.57%)
86.91%(5.29%)

95.70%(1.08%)
95.54%(1.02%)
95.38%(1.05%)
95.69%(.91%)

91.56%(6.29%)
92.77%(5.58%)
91.08%(10.60%)
89.90%(10.48%)

91.42%(3.37%)
89.87%(3.53%)
89.63%(4.9%)
91.22%(3.50%)

87.50%(7.30%)
88.37%(5.50%)
87.12%(8.03%)
85.69%(9.32%)

98.59%(1.80%)
99.01%(1.07%)
98.37%(1.50%)
98.88%(1.23%)

99.58%(.79%)
99.62%(.96%)
98.97%(2.94%)
98.80%(2.24%)

99.54%(1.01%)
99.40%(1.82%)
99.83%(.54%)
100%(0)

3.3 An example for The EdgeIIoT dataset

In [31], a new comprehensive and realistic cyber security dataset of IoT and IIoT, dubbed Edge-IIoT, is introduced.

The dataset is constructed for machine-learning-based intrusion detection systems. Twelve attack scenarios, namely

"Backdoor" , "DDoS-HTTP" , "DDoS-ICMP" , "DDoS-TCP" , "DDoS-UDP" , "Password" , "Port-Scanning" ,"Ransomware",
"SQL-injection" , "Uploading" , "Vulnerability-scanner" and "XSS", are covered in the dataset (ùëÄ = 12). We denote the
response label as ùë¶ = 1, . . . , 12 for each attack, respectively. Similar to the previous example, we consider the ‚Äústar‚Äù
structure: in the ùëó-th node, we consider a binary classification task for ùë¶ = 0 (the normal data) and ùë¶ = ùëó.

We randomly picked 75% of the dataset to train the classifiers and used the remaining 25% as the testing set. Again,

we observe that the attacked data are imbalanced for different classes, such skewness lowers the accuracy of the local

classifiers, as shown in figure 5. To balance the dataset, we randomly selected 1000 normal data and 1000 attacked data

for each node. Based on the scree-plots of the log-likelihood scores, the number of Gaussian components in GMM were
set to be ùêæ = 15. For local binary classifiers, we applied the machine learning approaches support vector machine (SVM),
k-nearest neighbor (kNN), Random Forest (RF), and Logistic Regression (LR) as in [31]. Averaged over 50 replicates, the

precisions and recalls for each class are listed in table 6. The results again support the feasibility and effectiveness of

the proposed approach.

Fig. 5. Comparison of the testing precisions and recalls of the ensemble (EMMC)/full-data (Full) approaches for the imbalanced and
balanced data.

Manuscript submitted to ACM

12

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang

Table 6. Mean classification precision and recall using different local classifiers with standard deviation of the balanced dataset

Benign

Backdoor

DDoS-HTTP DDoS-ICMP DDoS-TCP DDoS-UDP

Password

Port-Scanning Ransomware

SQL-injection

Uploading

Vulnerability-scanner XSS

SVM 99.89%(.04%)
kNN 99.86%(.09%)
99.89%(.06%)
RF
LR 99.89%(.06%)

99.89%(.21%)
99.77%(.47%)
100%(0)
100%(0)

100%(0)
100%(0)
100%(0)
100%(0)

SVM 99.99%(.01%)
kNN 99.96%(.11%)
100%(0)
RF
LR 100%(0)

99.90%(.27%)
99.88%(.30%)
99.96%(.11%)
99.98%(.08%)

99.76%(.41%)
99.61%(.47%)
99.76%(.41%)
99.74%(.45%)

100%(0)
100%(0)
100%(0)
100%(0)

100%(0)
100%(0)
100%(0)
100%(0)

100%(0)
100%(0)
100%(0)
100%(0)

100%(0)
100%(0)
100%(0)
100%(0)

100%(0)
99.79%(.91%)
100%(0)
100%(0)

100%(0)
100%(0)
100%(0)
100%(0)

Precision

100%(0)
100%(0)
100%(0)
100%(0)
Recall
99.83%(.23%)
99.73%(.32%)
99.85%(.23%)
99.85%(.23%)

100%(0)
100%(0)
100%(0)
100%(0)

100%(0)
100%(0)
100%(0)
100%(0)

99.98%(.08%)
99.97%(.08%)
100%(0)
100%(0)

100%(0)
100%(0)
100%(0)
100%(0)

100%(0)
100%(0)
100%(0)
100%(0)

100%(0)
100%(0)
100%(0)
100%(0)

100%(0)
100%(0)
100%(0)
100%(0)

99.98%(.08%)
100%(0)
99.98%(.08%)
100%(0)

99.93%%(.14%)
99.93%%(.14%)
99.93%(.14%)
99.91%(.16%)

99.85%(.22%)
99.94%(.14%)
99.88%%(.22%)
99.89%%(.21%)

99.68%(.34%)
99.56%(.53%)
99.56%(.48%)
99.68%(.34%)

99.80%(.29%)
99.76.31%(.29%)
99.76%(.29%)
99.62%(.04%)

When dealing with general multi-class classification problems, such as classification for digital numbers (e.g. the

MNIST dataset), we have no obvious justification on assuming a ‚Äústar‚Äù structure any more. The other two structures

(‚Äúring‚Äù and ‚Äúfully-connected‚Äù), or some structures (e.g., each node containing two classes randomly) are also possible.

For simplicity, we focus on the ‚Äústar‚Äù structure of real-world datasets for attack detection in the following sections. We

refer to Appendix B for general classification problems with the ‚Äúring‚Äù and ‚Äúfully-connected‚Äù structures.

4 AN ANALYSIS OF THE MQTT PROTOCOL-BASED DATASET

Message Queuing Telemetry Transport (MQTT) protocol is one of the most commonly used protocol in IoT [32, 33].
We consider the dataset provided by [34], which is the first simulated dataset under MQTT protocol 1. In this dataset,
four different types of attacks are considered: Aggressive scan (Scan-A), User Datagram Protocol (UDP) scan (Scan-sU),
Sparta SSH brute-force (Sparta), and MQTT brute-force attack (MQTT-BF). Therefore, we have ùëÄ = 4, with ùë¶ = 1, 2, 3, 4
indicating the Scan-A, Scan-sU, Sparta, and MQTT-BF attacks, respectively. Besides the multi-class dataset, datasets

for each type of attack are also provided. We call such datasets the ‚Äúbinary‚Äù datasets. Each binary dataset contains

records for the normal data and one certain type of attack. Moreover, among the simulated packet-based features,

unidirectional-based features, and bidirectional-based features, we focus on the bidirectional-based features, which can

provide the highest classification accuracy in previous related works in the experimental studies.

4.1 Feature subspace for each type of attacks

We first try to understand the mechanism of each type of attack by extracting the ‚Äúfeature subspace‚Äù for each type

of attack. Here we refer to the subspace spanned by certain linear combinations of original features as the ‚Äúfeature

subspace‚Äù, which differs most between the attacked and normal data.

4.1.1

Feature subspace extracting. To extract the feature subspace, we applied the principal optimal transport direction

(POTD) approach [35]. We summarize the POTD approach as follows. Firstly, an optimal transport plan [36‚Äì39] is
calculated between the attack data ùíô |ùë¶ = ùëö, ùëö = 1, 2, 3, 4 and the normal data ùíô |ùë¶ = 0. Secondly, a ‚Äúdisplacement matrix‚Äù
ùëÉùëö,0, ùëö = 1, 2, 3, 4, which represents the empirical transport plan from ùíô |ùë¶ = ùëö to ùíô |ùë¶ = 0, is used to measure the
difference between the attack and normal data. POTD approach then extracts the major directions of ùëÉùëö,0 using the
principal component analysis (PCA). The major directions, denoted as ùêµùëö, ùëö = 1, 2, 3, 4, are proved to be the directions
that differ most between the attack and normal data. The feature subspace for each type of attack is defined as the
subspace spanned by ùêµùëö.

Based on the scree-plots for the PCA of ùëÉùëö,0 shown in figure 6, the first four principal components can explain over
90% of the information of the original matrix ùëÉùëö,0. We thus set the dimension for direction matrix ùêµùëö to be ùëë = 4, for
all ùëö = 1, . . . , 4. The direction matrices for the four types of attacks are shown in figure 7. From the figure, we observe

1https://ieee-dataport.org/open-access/mqtt-internet-things-intrusiondetection-dataset

Manuscript submitted to ACM

CPS Attack Detection under Limited Local Information in Cyber Security: A Multi-node Multi-class Classification
Ensemble Approach

13

Fig. 6. Scree-plots for the PCA of the displacement matrix ùëÉùëö,0. Panel (a): ùëö = 1; Panel (b): ùëö = 2; Panel (c): ùëö = 3; and Panel (d):
ùëö = 4.

that the feature subspaces are specified for different types of attacks. We further study the specificity of the feature

subspaces using more statistical tools.

Fig. 7. Heatmaps of the four direction matrices.

Manuscript submitted to ACM

14

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang

4.1.2

Specificity of feature subspaces. We show the specificity of the four extracted feature subspaces by comparing

the distributions of the Wasserstein distances [36‚Äì39] between the attack and normal data after projecting data onto

each feature subspace. Specifically, we obtain the empirical distributions as follows,

(1) Randomly sample 1000 attacked samples and 1000 normal samples from the four binary datasets.

(2) Project the random samples onto each feature subspace.

(3) Calculate the Wasserstein distance between the projected attack and normal samples.

(4) Repeat steps (1)-(3) for 100 times.

Intuitively, if the extracted feature subspaces are specified for the corresponding attack types, we expect that the

empirical distribution of the Wasserstein distance between the attacked data and the normal data after projecting

onto the corresponding feature subspace will be clearly separated from those projected onto all other subspaces. The

histograms for the Wasserstein distances are shown in Figure 8. From the figure, we observe that it‚Äôs clear that after
projecting, the Wasserstein distances between the ùëö-th attack and the normal data in the feature subspace spanned by
ùêµùëö are significantly larger than others (ùëö = 1, 2, 3, 4). Such an observation matches the aforementioned intuition. It
verifies the specificity of the four extracted feature subspaces.

Fig. 8. Comparison of empitical distributions of the Wasserstein distance between the attacked data and normal data after projecting
onto four subspaces. Panel (a): scan_A; Panel (b): scan_sU; Panel (c): Sparta; and Panel (d): MQTT BF.

Manuscript submitted to ACM

CPS Attack Detection under Limited Local Information in Cyber Security: A Multi-node Multi-class Classification
Ensemble Approach

15

4.2 Attack detection

The specificity shown above implies that the four types of attacks may be under different mechanisms. As a result,

attack detection using a multi-class classification approach becomes more feasible. Similar to the data pre-processing

in [40], we excluded the IP addresses and protocol from the datasets. We considered each binary dataset as a node in

the experimental studies. In other words, we considered the ‚Äústar‚Äù structure as in the previous simulation studies and

real-world data illustrations. In such a structure, there are four nodes (J = 4). Each node only contains information to

identify one of the Scan-A, Scan-sU, Sparta, and MQTT-BF attacks.

Fig. 9. Screeplots of the log-likelihood scores for data in four nodes.

Based on the scree-plots of the log-likelihood score shown in figure 9, we chose ùêæ = 15 in GMM when estimating
the local data density for all nodes. We then calculated the binary classifiers within each node based on the remaining

28 bidirectional-based features (we refer to [40] for the detailed description of each feature). We applied the logistic

regression (LR), the random forests (RF), and the support vector machine with RBF kernel (SVM) to build the local

binary classifiers. Again, 75% of the data within each local node were randomly picked as the training data for obtaining

the local binary classifiers. The remained data for all nodes were pooled together as the multi-class testing data. We

compared the testing precisions and recalls for the multi-class classifiers obtained through the proposed ensemble

approach and the full-data approach. The results are listed in table 7. The values outside the brackets are the mean

precision and recall based on 50 replicates, and the values within the brackets are the corresponding standard deviations.

Manuscript submitted to ACM

16

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang

From the table, we observe that the ensemble approach performs similarly to the full-data approach for all three

classification models. Such an observation validates the feasibility of the ensemble approach, which can indeed obtain

the global multi-class classifier while data in each node is incomplete.

Table 7. Mean (SD) of the testing precision and recall for each type of attack under the ensemble approach and full-data approach
with local classification approaches (ùëë = 28).

Ensemble Multi-node Multi-class Classification

Benign

Sparta

Scan-sU

Scan-A

MQTT-BF

LR 99.97%(.008%)
99.97%(.009%)
RF
SVM 99.97% (.01%)
kNN 99.97%(.009%)

100%(0)
100%(0)
100%(0)
100%(0)

LR 100%(0)
100%(0)
RF
SVM 100%(0)
kNN 100%(0)

99.90%(.05%)
99.89%(.05%)
99.90%(.06%)
99.90%(.07%)

LR 99.99%(.002%)
100%(0%)
RF
SVM 100%(0%)
kNN 99.99%(.004%)

100%(0%)
100%(0%)
100%(0%)
100%(0%)

LR 100%(0%)
100%(0%)
RF

SVM 99.99%(.002%)
kNN 99.99%(.002%)

99.99%(.01%)
100%(0%)
100%(0%)
99.99%(.01%)

Precision

100%(0)
100%(0)
100%(0)
100%(0)

Recall
99.99%(.01%)
99.99%(.009%)
99.99%(.01%)
99.99%(.01%)

Full Data

Precision
99.92%(.03%)
99.99%(.01%)
100%(0%)
100%(0%)

Recall
99.72%(.08%)
99.99%(.01%)
99.63%(.08%)
100%(0%)

100%(0)
100%(0)
100%(0)
100%(0)

100%(0)
100%(0)
100%(0)
100%(0)

99.98(.01%)
99.98%(.01%)
99.98%(.01%)
99.98%(.01%)

99.96%(.02%)
99.98%(.03%)
99.97%(.01%)
99.97%(.01%)

100%(0%)
100%(0%)
99.99%(.008%)
100%(0%)

99.53%(.13%)
99.99%(.02%)
99.41%(.12%)
99.99%(.01%)

99.97%(.02%)
99.99%(.008%)
100%(0%)
99.97%(.01%)

99.88%(..05%)
100%(0%)
100%(0%)
100%(0%)

4.3 Classification with Dimension Reduction

In the previous experiment, we applied the proposed approach on the original dataset with a dimension of 28. However,

based on the studies on the feature subspaces for each type of attack, we observe that the attacks are specified on only

four dimensions. To show the utility of the proposed approach, we conducted a dimension reduction via Principal

Component Analysis (PCA), and reduced the data dimension from 28 to four. Again, four binary classification algorithm,

namely Logistic Regression(LR), Random Forest(RF), k Nearest Neighborhood(kNN), Support Vector Machine (SVM)

were adopted for the task after dimension reduction, and the results are listed in table 8.

Manuscript submitted to ACM

CPS Attack Detection under Limited Local Information in Cyber Security: A Multi-node Multi-class Classification
Ensemble Approach

17

Table 8. Mean (SD) of the testing precision and recall for each type of attack under the ensemble approach and full-data approach
with local classification approaches after PCA (ùëë = 4).

Ensemble Multi-node Multi-class Classification

Benign

Sparta

Scan-sU

Scan-A

MQTT-BF

LR 98.76%(.37%)
99.99%(.002%)
RF
SVM 99.90%(.30%)
kNN 99.97%(.01%)

100%(0%)
100%(0%)
100%(0%)
100%(0%)

LR 99.49%(.06%)
99.58%(.03%)
RF
SVM 99.52%(.03%)
kNN 99.58%(.03%)

100%(0%)
100%(0%)
100%(0%)
100%(0)

LR 99.43%(.07%)
99.75%(.03%)
RF

SVM -
kNN 99.97%(.02%)

100%(0%)
100%(0%)
-
100%(0%)

LR 92.75%(.78%)
99.71%(.02%)
RF

SVM -
kNN 99.58%(.03%)

100%(0)
100%(0%)
-
99.98%(.02%)

Precision
99.87%(.21%)
99.99%(.01%)
99.99%(.01%)
99.99%(.01%)
Recall
99.67%(.67%)
99.99%(.003%)
99.93%(.02%)
99.94%(.03%)

Full Data

Precision
79.39%(.58%)
100%(0%)
-
100%(0%)

Recall
99.71%(.25%)
100%(0%)
-
99.97%(.03%)

99.72%(.06%)
99.99%(.006%)
99.71%(.06%
99.99%(.007%)

97.06%(.20%)
97.07%(.20%)
97.06%(.20%)
97.07%(.002%)

94.38%(1.8%)
99.99%(.007%)
99.59%(1.4%)
99.95%(.03%)

100%(0%)
99.99%(.01%)
99.99%(.01%)
99.99%(.006%)

85.22%(4.3%)
100%(0%)
-
100%(0%)

97.05%(.21%)
98.01%(.16%)
-
97.09%(.20%)

96.84%(.26%)
99.99%(.004%)
-
99.97%(.02%)

99.98%(.02%)
98.29%(.23%)
-
99.88%(.12%)

Since the Support Vector Machine method for full-data is super time-consuming compared with other methods, we

did not report the results for SVM under the full-data case. According to the results, when applying Logistic Regression

for classification, the proposed multi-node multi-class classification ensemble approach has a higher Precision compared

with the full-data approach. One possible reason is that after processing data via PCA, data may become much more

dense, the linear classifier (LR) may not be suitable for the multi-class classification. However, it may still be suitable for

the simple binary classification within a single node. In addition, GMM may also results in a more accurate density

estimation for a dense data with less dimension than for a high-dimensional data. As a result, the proposed approach

performs better than the full-data approach with LR.

5 CONCLUSION AND DISCUSSION

In this paper, we study the federated learning framework for multi-class classification. We focus on the cases when local

nodes only contain a certain part of the classes. By filling in the missing information through gathering the parameters

from local binary classifiers and local densities, we develop a multi-node multi-class classification ensemble approach.

We validate the feasibility of the proposed approach both theoretically and practically. We also discuss the impact

Manuscript submitted to ACM

18

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang

of data balancing. Actually, there are some existing weighted binary classification approaches for imbalanced data

[41, 42]. When we adopt such weighted approaches to the local classifiers, the proposed ensemble approach can be

potentially extended to a more general approach for imbalanced data. Furthermore, the proposed approach has three

interesting merits in aspects of communication complexity, privacy, and generality. Firstly, it‚Äôs one-shot in terms of

communication complexity [43, 44], in that it only requires one communication between the server and each client. In

contrast, conventional federated learning algorithms require iterative communications, which is time-consuming and

demands that different clients must be synchronized. Thanks to the density estimator, our algorithm is further free from

the need for a global public dataset or synthetic data generation/distillation, which are required by existing one-shot

federated learning approaches [45, 46]. Secondly, the proposed approach does not transmit gradient information, which

makes it intrinsically immune to common attack strategies in decentralized machine learning such as the gradient

inversion attack [47]. Thirdly, the proposed approach is general enough that all sorts of classifiers and density estimators

can be incorporated to replace the currently used binary classifiers and GMM models. This allows one to borrow

cutting-edge techniques from deep learning or even quantum machine learning communities, such as more advanced

neural networks or quantum circuits as classifiers and modern generative models as density estimators [48, 49].

APPENDIX

A PROOF OF THE MAIN RESULT

Let ùë§ ùëó =

ùíô ùëù ( ùëó )
ùëì ( ùëó )
ùëÅ
ùíô ùëù ( ùëó )
ùëì ( ùëó )

ùëÅ

ùëó =1

(cid:205)ùêΩ

. We first show the feasibility of (3).

ùëê (ùëö) (ùíô) =ùëÉ (ùë¶ = ùëö|ùíô)
ùëÅ
‚àëÔ∏Å

=

ùëÉ (ùë¶ = ùëö, data ‚àà Nodeùëó |ùíô)

ùëó=1
ùëÅ
‚àëÔ∏Å

ùëó=1
ùëÅ
‚àëÔ∏Å

ùëó=1
ùëÅ
‚àëÔ∏Å

ùëó=1
ùëÅ
‚àëÔ∏Å

ùëó=1
ùêΩ
‚àëÔ∏Å

ùëó=1

=

=

=

=

=

ùëÉ (ùë¶ = ùëö, data ‚àà Nodeùëó , ùíô)
ùëìùíô

ùëÉ (ùë¶ = ùëö, data ‚àà Nodeùëó , ùíô)
ùëÉ (data ‚àà Nodeùëó , ùíô)

√ó

ùëÉ (data ‚àà Nodeùëó , ùíô)
ùëìùíô

ùëê (ùëö)
ùëó

(ùíô) √ó

ùëì ( ùëó)
ùëù ( ùëó)
ùëÅ
ùíô
ùëìùíô

ùëê (ùëö)
ùëó

(ùíô) √ó

ùëì ( ùëó)
ùíô

ùëù ( ùëó)
ùëÅ
ùëì ( ùëó)
ùíô

ùëù ( ùëó)
ùëÅ

(cid:205)ùëÅ

ùëó=1

1(ùëö ‚àà M ùëó )ùë§ ùëóùëê (ùëö)

ùëó

(ùíô).

We then prove that when ùë§ ùëó ‚â•
Consider the term (cid:205)ùëö log ùëê (ùëö) (ùíôùëñ ùëó ) in (2). Plug-in (3), we have

/ùêΩ , ÀÜùëê (ùëö) (ùíô) = (cid:205)ùêΩ

(cid:16)
ÀÜùëê (ùëö)
ùëó

(cid:17) ùêΩ ‚àí1

ùëó=1 1(ùëö ‚àà M ùëó )ùë§ ùëó ÀÜùëê (ùëö)

ùëó

(ùíô) minimizes loss function (2).

log ÀÜùëê (ùëö) (ùíôùëñ ùëó ) =

‚àëÔ∏Å

ùëö

‚àëÔ∏Å

ùëö

log

ùêΩ
‚àëÔ∏Å

ùëó=1

1(ùëö ‚àà M ùëó )ùë§ ùëó ÀÜùëê (ùëö)

ùëó

(ùíôùëñ ùëó ) =

‚àëÔ∏Å

ùëö

log

ùêΩ
‚àëÔ∏Å

ùëó=1

ùë§ ùëó ÀÜùëê (ùëö)
ùëó

(ùíôùëñ ùëó ).

(5)

Manuscript submitted to ACM

CPS Attack Detection under Limited Local Information in Cyber Security: A Multi-node Multi-class Classification
Ensemble Approach

19

(6)

(7)

(8)

By Jensen‚Äôs inequality, we have

When ùë§ ùëó ‚â•

(cid:17) ùêΩ ‚àí1

(cid:16)
ÀÜùëê (ùëö)
ùëó

/ùêΩ , then

log

ùêΩ
‚àëÔ∏Å

ùëó=1

ùë§ ùëó ÀÜùëê (ùëö)
ùëó

(ùíôùëñ ùëó ) ‚â•

1
ùêΩ

ùêΩ
‚àëÔ∏Å

ùëó=1

log ùêΩùë§ ùëó ÀÜùëê (ùëö)

ùëó

(ùíôùëñ ùëó ).

1
ùêΩ

ùêΩ
‚àëÔ∏Å

ùëó=1

log ùêΩùë§ ùëó ÀÜùëê (ùëö)

ùëó

(ùíôùëñ ùëó ) =

ùêΩ
‚àëÔ∏Å

ùëó=1

(cid:16)

ùêΩùë§ ùëó ÀÜùëê (ùëö)
ùëó

log

(ùíôùëñ ùëó )

(cid:17)1/ùêΩ

‚â•

ùêΩ
‚àëÔ∏Å

ùëó=1

log ÀÜùëê (ùëö)
ùëó

(ùíôùëñ ùëó ).

Combining (5), (6) and (7), we have

1(ùë¶ùëñ ùëó = ùëö) log ÀÜùëê (ùëö) (ùíôùëñ ùëó ) ‚â§ ‚àí

‚àëÔ∏Å

‚àëÔ∏Å

ùêΩ
‚àëÔ∏Å

1(ùë¶ùëñ ùëó = ùëö) log ÀÜùëê (ùëö)

ùëó

(ùíôùëñ ùëó ).

‚àëÔ∏Å

‚àëÔ∏Å

‚àí

ùëñ

ùëö

What‚Äôs more, (cid:205)ùëö ÀÜùëê (ùëö) = (cid:205)ùëó ùë§ ùëó (cid:205)ùëö 1(ùëö ‚àà M ùëó ÀÜùëê (ùëö)
‚àí (cid:205)ùëñ (cid:205)ùëö 1(ùë¶ùëñ ùëó = ùëö) log ÀÜùëê (ùëö)
it is obvious that (cid:205)ùëó
ensemble estimation ÀÜùëê (ùëö) minimizes the global loss function (2).

(cid:111)

(cid:110)

ùëó

ùëó

ùëö

ùëñ

ùëó=1
) = 1. On the other hand, since ÀÜùëê (ùëö)
(ùíôùëñ ùëó )

ùëó minimizes (1) for ùëó = 1, . . . , ùêΩ ,
reaches its minimum. Together with (8), we show that the

B APPLICATION ON MNIST DATASET

MNIST [50] is a popular image dataset with 60,000 samples that contains 10 classes of handwritten digits from 0 to

9. In such a dataset, there is no obvious justification for the particular ‚Äústar‚Äù structure. We thus additionally consider

the ‚Äúring‚Äù and ‚Äúfully-connected‚Äù structures. We randomly pick 75% of the data within each node to train the local

binary classifiers. We then combine the remained data (25% for each node) together as the multi-class test data. We use

the random forest (RF) as the binary classifiers. We compare the test precision and recall of the multi-class classifiers

obtained through the proposed ensemble approach and the full-data approach. Here the ‚Äúfull-data approach‚Äù refers

to the classifier trained with the pooled data. The results averaged over 50 replicates are listed in table 9. The values

outside the brackets are the mean precision and recall based, and the values within the brackets are the corresponding

standard deviations. From the table, we observe that the ensemble approach under the ‚Äúfully-connected‚Äù structure is

slightly better than the ‚Äúring‚Äù structure. Such an observation is expected since the ‚Äúfully-connected‚Äù structure contains

more mutual information among classes. We also observe that the ensemble approach performs similarly to the full-data

approach. Such an observation validates the feasibility and effectiveness of the federated approach.

ACKNOWLEDGEMENTS

The authors would like to acknowledge support for this project from the National Key R&D Program of China (No.

2021YFA1001300).

Manuscript submitted to ACM

20

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang

Table 9. Mean (SD) of the testing precision and recall for each digit.

Ring

96.91%(3.19%)
94.21%(4.70%)
94.11%(3.02%)
93.13%(2.03%)
91.12%(3.72%)
95.57%(4.21%)
97.74%(2.26%)
98.21%(2.04%)
95.89%(4.10%)
92.17%(3.63%)

93.41%(2.30%)
97.53%(1.93%)
97.59%(2.64%)
96.23%(2.67%)
96.20%(2.64%)
96.52%(1.64%)
99.78%(0.67%)
93.48%(4.54%)
94.97%(3.82%)
96.64%(4.00%)

Fully-connected
Precision
97.19%(2.79%)
96.62%(2.38%)
96.80%(3.31%)
96.80%(3.14%)
92.97%(2.37%)
96.45%(4.54%)
97.35%(1.90%)
99.05%(1.47%)
96.59%(4.37%)
92.57%(3.65%)
Recall
94.58%(3.40%)
97.99%(2.29%)
98.64%(2.24%)
96.62%(2.95%)
97.94%(2.24%)
96.51%(2.36%)
99.77%(0.70%)
95.35%(3.30%)
95.35%(3.37%)
96.91%(3.10%)

Full-data

98.92%(1.32%)
97.17%(1.27%)
97.67%(2.17%)
97.41%(2.55%)
92.92%(2.35%)
97.17%(2.47%)
98.61%(1.27%)
99.42%(1.16%)
95.57%(3.92%)
93.31%(2.85%)

94.46%(2.57%)
97.15%(2.17%)
98.23%(1.72%)
96.63%(2.52%)
97.67%(2.51%)
96.99%(2.92%)
99.77%(0.68%)
94.67%(2.52%)
96.13%(3.41%)
97.01%(3.46%)

Digit
0
1
2
3
4
5
6
7
8
9

0
1
2
3
4
5
6
7
8
9

REFERENCES

[1] Hyung-Ho Kim, Sang-Uk Han, Min-Chan Kim, Woo Jin Hyung, Wook Kim, Hyuk-Joon Lee, Seung Wan Ryu, Gyu Seok Cho, Kyo Young Song, and
Seong Yeob Ryu. Long-term results of laparoscopic gastrectomy for gastric cancer: a large-scale case-control and case-matched korean multicenter
study. Journal of Clinical Oncology, 32(7):627‚Äì633, 2014.

[2] Jin Ye, Lulu Guo, Bowen Yang, Fangyu Li, Liang Du, Le Guan, and Wenzhan Song. Cyber‚Äìphysical security of powertrain systems in modern electric
vehicles: Vulnerabilities, challenges, and future visions. IEEE Journal of Emerging and Selected Topics in Power Electronics, 9(4):4639‚Äì4657, 2020.
[3] Fangyu Li, Rui Xie, Zengyan Wang, Lulu Guo, Jin Ye, Ping Ma, and WenZhan Song. Online distributed iot security monitoring with multidimensional

streaming big data. IEEE Internet of Things Journal, 7(5):4387‚Äì4394, 2019.

[4] Qixia Jiang, Siping Song, Jihong Zhou, Yuxiu Liu, Aihua Chen, Yuxuan Bai, Jing Wang, Zhixia Jiang, Yanhong Zhang, Haiying Liu, et al. The
prevalence, characteristics, and prevention status of skin injury caused by personal protective equipment among medical staff in fighting COVID-19:
a multicenter, cross-sectional study. Advances in wound care, 9(7):357‚Äì364, 2020.

[5] Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyridon Bakas, Mathieu N Galtier, Bennett A Landman,

Klaus Maier-Hein, et al. The future of digital health with federated learning. NPJ digital medicine, 3(1):1‚Äì7, 2020.

[6] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE Signal

Processing Magazine, 37(3):50‚Äì60, 2020.

[7] Zihang Zeng, Yiming Ma, Huihui Zeng, Peng Huang, Wenlong Liu, Mingyan Jiang, Xudong Xiang, Dingding Deng, Xin Liao, Ping Chen, et al.
Simple nomogram based on initial laboratory data for predicting the probability of ICU transfer of COVID-19 patients: Multicenter retrospective
study. Journal of medical virology, 93(1):434‚Äì440, 2021.

[8] Lulu Guo, Bowen Yang, Jin Ye, Hong Chen, Fangyu Li, Wenzhan Song, Liang Du, and Le Guan. Systematic assessment of cyber-physical security of
energy management system for connected and automated electric vehicles. IEEE Transactions on Industrial Informatics, 17(5):3335‚Äì3347, 2020.
[9] Matthias Butenuth, Guido v G√∂sseln, Michael Tiedge, Christian Heipke, Udo Lipeck, and Monika Sester. Integration of heterogeneous geospatial

data in a federated database. ISPRS Journal of Photogrammetry and Remote Sensing, 62(5):328‚Äì346, 2007.

[10] Derui Ding, Qing-Long Han, Yang Xiang, Xiaohua Ge, and Xian-Ming Zhang. A survey on security control and attack detection for industrial

cyber-physical systems. Neurocomputing, 275:1674‚Äì1683, 2018.

[11] Ansam Khraisat and Ammar Alazab. A critical review of intrusion detection systems in the internet of things: techniques, deployment strategy,

validation strategy, attacks, public datasets and challenges. Cybersecurity, 4(1):1‚Äì27, 2021.

Manuscript submitted to ACM

CPS Attack Detection under Limited Local Information in Cyber Security: A Multi-node Multi-class Classification
Ensemble Approach

21

[12] Eduardo Arnold, Omar Y Al-Jarrah, Mehrdad Dianati, Saber Fallah, David Oxtoby, and Alex Mouzakitis. A survey on 3D object detection methods

for autonomous driving applications. IEEE Transactions on Intelligent Transportation Systems, 20(10):3782‚Äì3795, 2019.

[13] Di Feng, Christian Haase-Sch√ºtz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wiesbeck, and Klaus Dietmayer. Deep
multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges. IEEE Transactions on Intelligent
Transportation Systems, 22(3):1341‚Äì1360, 2020.

[14] Shuo Jin, Bo Wang, Haibo Xu, Chuan Luo, Lai Wei, Wei Zhao, Xuexue Hou, Wenshuo Ma, Zhengqing Xu, Zhuozhao Zheng, et al. AI-assisted CT

imaging analysis for COVID-19 screening: Building and deploying a medical AI system in four weeks. MedRxiv, 2020.

[15] Athanasia Mitsala, Christos Tsalikidis, Michail Pitiakoudis, Constantinos Simopoulos, and Alexandra K Tsaroucha. Artificial intelligence in colorectal

cancer screening, diagnosis and treatment. a new era. Current Oncology, 28(3):1581‚Äì1607, 2021.

[16] Geethapriya Thamilarasu and Shiven Chawla. Towards deep-learning-driven intrusion detection for the internet of things. Sensors, 19(9):1977, 2019.
[17] S Smys, Abul Basar, Haoxiang Wang, et al. Hybrid intrusion detection system for internet of things (IoT). Journal of ISMAC, 2(04):190‚Äì199, 2020.
[18] Javier Navaridas Aitor Belenguer and Jose A. Pascual. A review of federated learning in intrusion detection systems for iot. IEEE INTERNET OF

THINGS JOURNAL, 2022.

[19] Jo√£o Vitorino, Rui Andrade, Isabel Pra√ßa, Orlando Sousa, and Eva Maia. A comparative analysis of machine learning techniques for iot intrusion

detection. In International Symposium on Foundations and Practice of Security, pages 191‚Äì207. Springer, 2022.

[20] Siwei Feng and Han Yu. Multi-participant multi-class vertical federated learning. arXiv preprint arXiv:2001.11154, 2020.
[21] Holger R Roth, Ken Chang, Praveer Singh, Nir Neumark, Wenqi Li, Vikash Gupta, Sharut Gupta, Liangqiong Qu, Alvin Ihsani, Bernardo C Bizzo,
et al. Federated learning for breast density classification: A real-world implementation. In Domain Adaptation and Representation Transfer, and
Distributed and Collaborative Learning, pages 181‚Äì191. Springer, 2020.

[22] David MJ Tax and Robert PW Duin. Using two-class classifiers for multiclass classification. In Object recognition supported by user interaction for

service robots, volume 2, pages 124‚Äì127. IEEE, 2002.

[23] Ting-Fan Wu, Chih-Jen Lin, and Ruby Weng. Probability estimates for multi-class classification by pairwise coupling. Advances in Neural Information

Processing Systems, 16, 2003.

[24] Mikel Galar, Alberto Fern√°ndez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera. An overview of ensemble methods for binary
classifiers in multi-class problems: Experimental study on one-vs-one and one-vs-all schemes. Pattern Recognition, 44(8):1761‚Äì1776, 2011.
[25] Nadia Chaabouni, Mohamed Mosbah, Akka Zemmari, Cyrille Sauvignac, and Parvez Faruki. Network intrusion detection for IoT security based on

learning techniques. IEEE Communications Surveys & Tutorials, 21(3):2671‚Äì2701, 2019.
[26] Somayye Hajiheidari, Karzan Wakil, Maryam Badri, and Nima Jafari Navimipour.

comprehensive investigation. Computer Networks, 160:165‚Äì191, 2019.

Intrusion detection systems in the internet of things: A

[27] Douglas A Reynolds. Gaussian mixture models. Encyclopedia of biometrics, 741(659-663), 2009.
[28] Christophe Biernacki, Gilles Celeux, and G√©rard Govaert. Choosing starting values for the EM algorithm for getting the highest likelihood in

multivariate gaussian mixture models. Computational Statistics & Data Analysis, 41(3-4):561‚Äì575, 2003.

[29] Nour Moustafa and Jill Slay. Unsw-nb15: a comprehensive data set for network intrusion detection systems (unsw-nb15 network data set). In 2015

Military Communications and Information Systems Conference (MilCIS), pages 1‚Äì6, 2015.

[30] Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathematical statistics, 22(1):79‚Äì86, 1951.
[31] Mohamed Amine Ferrag, Othmane Friha, Djallel Hamouda, Leandros Maglaras, and Helge Janicke. Edge-iiotset: A new comprehensive realistic

cyber security dataset of iot and iiot applications for centralized and federated learning. IEEE Access, 10:40281‚Äì40306, 2022.

[32] Meena Singh, MA Rajan, VL Shivraj, and Purushothaman Balamuralidhar. Secure MQTT for internet of things (IoT). In 2015 fifth international

conference on communication systems and network technologies, pages 746‚Äì751. IEEE, 2015.

[33] Vatsal Gupta, Sonam Khera, and Neelam Turk. MQTT protocol employing iot based home safety system with ABE encryption. Multimedia Tools

and Applications, 80(2):2931‚Äì2949, 2021.

[34] Hanan Hindy, Christos Tachtatzis, Robert Atkinson, Ethan Bayne, and Xavier Bellekens. MQTT-IOT-IDS2020: Mqtt internet of things intrusion

detection dataset. IEEE Dataport, 2020.

[35] Cheng Meng, Jun Yu, Jingyi Zhang, Ping Ma, and Wenxuan Zhong. Sufficient dimension reduction for classification using principal optimal transport

direction. Advances in Neural Information Processing Systems, 33:4015‚Äì4028, 2020.

[36] C√©dric Villani. Optimal transport: old and new, volume 338. Springer, 2009.
[37] Gabriel Peyr√©, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends¬Æ in Machine

Learning, 11(5-6):355‚Äì607, 2019.

[38] Jingyi Zhang, Wenxuan Zhong, and Ping Ma. A review on modern computational optimal transport methods with applications in biomedical

research. Modern Statistical Methods for Health Research, pages 279‚Äì300, 2021.

[39] Jingyi Zhang, Ping Ma, Wenxuan Zhong, and Cheng Meng. Projection-based techniques for high-dimensional optimal transport problems. Wiley

Interdisciplinary Reviews: Computational Statistics, page e1587, 2022.

[40] Hanan Hindy, Ethan Bayne, Miroslav Bures, Robert Atkinson, Christos Tachtatzis, and Xavier Bellekens. Machine learning based iot intrusion
detection system: An MQTT case study (mqtt-iot-ids2020 dataset). Selected Papers from the 12th International Networking Conference, January
2021(73-84), 2021.

Manuscript submitted to ACM

22

Junyi Liu, Yifu Tang, Haimeng Zhao, Xieheng Wang, Fangyu Li, and Jingyi Zhang

[41] Vaishali Ganganwar. An overview of classification algorithms for imbalanced datasets. International Journal of Emerging Technology and Advanced

Engineering, 2(4):42‚Äì47, 2012.

[42] Bartosz Krawczyk. Learning from imbalanced data: open challenges and future directions. Progress in Artificial Intelligence, 5(4):221‚Äì232, 2016.
[43] Neel Guha, Ameet Talwalkar, and Virginia Smith. One-shot federated learning. arXiv preprint arXiv:1902.11175, 2019.
[44] Saber Salehkaleybar, Arsalan Sharif-Nassab, and S Jamaloddin Golestani. One-shot federated learning: Theoretical limits and algorithms to achieve

them. J. Mach. Learn. Res., 22:189‚Äì1, 2021.

[45] Yanlin Zhou, George Pu, Xiyao Ma, Xiaolin Li, and Dapeng Wu. Distilled one-shot federated learning. arXiv preprint arXiv:2009.07999, 2020.
[46] Anirudh Kasturi, Anish Reddy Ellore, and Chittaranjan Hota. Fusion learning: A one shot federated learning. In International Conference on

Computational Science, pages 424‚Äì436. Springer, 2020.

[47] Jonas Geiping, Hartmut Bauermeister, Hannah Dr√∂ge, and Michael Moeller. Inverting gradients-how easy is it to break privacy in federated learning?

Advances in Neural Information Processing Systems, 33:16937‚Äì16947, 2020.

[48] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
[49] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quantum machine learning. Nature, 549(7671):195‚Äì

202, 2017.

[50] Yann LeCun. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.

Manuscript submitted to ACM

