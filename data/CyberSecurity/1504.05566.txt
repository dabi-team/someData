Secure State Estimation: Optimal Guarantees against
Sensor Attacks in the Presence of Noise

Shaunak Mishra∗, Yasser Shoukry∗, Nikhil Karamchandani∗, Suhas Diggavi∗ and Paulo Tabuada∗
∗Electrical Engineering Department, University of California, Los Angeles

5
1
0
2

r
p
A
3
2

]

C
O
.
h
t
a
m

[

2
v
6
6
5
5
0
.
4
0
5
1
:
v
i
X
r
a

Abstract—Motivated by the need to secure cyber-physical
systems against attacks, we consider the problem of estimating
the state of a noisy linear dynamical system when a subset of
sensors is arbitrarily corrupted by an adversary. We propose a
secure state estimation algorithm and derive (optimal) bounds
on the achievable state estimation error. In addition, as a result
of independent interest, we give a coding theoretic interpretation
for prior work on secure state estimation against sensor attacks
in a noiseless dynamical system.

I.

INTRODUCTION

Cyber-physical systems (CPS) manage the vast majority of
today’s critical infrastructure and securing such CPS against
malicious attacks is a problem of growing importance [1]. As
a stepping stone towards securing complex CPS deployed in
practice, several recent works have studied security problems
in the context of linear dynamical systems [1], [2], [3], [4], [5],
[6] leading to a fundamental understanding of how the system
dynamics can be leveraged for security guarantees. With this
motivation, in this paper we focus on securely estimating the
state of a linear dynamical system from a set of noisy and
maliciously corrupted sensor measurements. We restrict the
sensor attacks to be sparse in nature, i.e., an adversary can
arbitrarily corrupt a subset of sensors in the system.

Prior work related to secure state estimation against sensor
attacks in linear dynamical systems can be broadly categorized
into three classes depending on the noise model for sensor
measurements: 1) noiseless 2) bounded non-stochastic noise,
and 3) Gaussian noise. For the noiseless setting, the work
reported in [1], [2], [3] shows that, under a strong notion
of observability, sensor attacks (modeled as a sparse attack
vector) can always be detected and isolated, and hence the
state of the system can be exactly estimated. In contrast, when
the sensor measurements are affected by noise as well as
maliciously corrupted, the problem of distinguishing between
noise and attack vector arises. Results reported in [5], [6], [7]
are representative of the second class: bounded non-stochastic
noise. They provide sufﬁcient conditions for distinguishing the
sparse attack vector from bounded noise but do not guarantee
the optimality of their estimation algorithm. The work reported
in this paper falls in the third class: Gaussian noise. Prior
work in this class includes [8], [9], [10], [11]. In [8], the
analysis is restricted to detecting a class of sensor attacks
called replay attacks (i.e., attacks in which legitimate sensor
outputs are replaced with outputs from previous time instants).
In [9], the authors focus on the performance degradation of
a scalar Kalman ﬁlter (i.e., scalar state and a single sensor)

The work was supported by NSF grant 1136174 and DARPA under

agreement number FA8750-12-2-0247.

when the sensor is under attack. Since they consider a single
sensor setup, attack sparsity across multiple sensors is not
studied, and in addition, they focus on an adversary whose
objective is to degrade the estimation performance and stay
undetected at the same time (thereby restricting the class of
sensor attacks). In [10] and [11], robustiﬁcation approaches for
state estimation against sparse sensor attacks are proposed, but
they lack optimality guarantees against arbitrary sensor attacks.

In contrast to prior work in the Gaussian noise setup,
we consider a general
linear dynamical system and give
(optimal) guarantees on the achievable state estimation error
against arbitrary sensor attacks. The following toy example is
illustrative of the nature of the problem addressed in this paper
and some of the ideas behind our solution.

Example 1: Consider a linear dynamical system with a
scalar state x(t) such that x(t + 1) = x(t) + w(t), where w(t)
is the process noise following a Gaussian distribution with
zero mean and is instantiated i.i.d. over time. The system has
three sensors (indexed by d) with outputs yd(t) = x(t) + vd(t),
where vd(t) is the sensor noise at sensor d. Similarly to the
process noise, vd(t) is Gaussian distributed with zero mean
and is instantiated i.i.d. over time. The sensor noise is also
independent across sensors. Now, consider an adversary which
can attack any one of the sensors and arbitrarily change its
output. In the absence of sensor noise, it is trivial to detect
such an attack since the two good sensors (not attacked by the
adversary) will have the same output. Hence, a majority based
rule on the outputs leads to the exact state. However, in the
presence of sensor noise, even the good sensors may not have
the same output and a simple majority based rule cannot be
used for estimation. In this paper, we build on the intuition that
we may still be able to identify sensors whose outputs can lead
to a good state estimate by leveraging the noise statistics over
a large enough time window. In particular, our approach for
this example would be to hypothesize a subset of two sensors
as good, and then check whether the outputs from the two
sensors are consistent with the Kalman state estimate based
on outputs from the same subset of sensors. Furthermore, we
show in this paper that such an approach leads to the optimal
state estimation error for the given adversarial setup.

In this paper, we generalize the Kalman ﬁlter based ap-
proach in the above example to a general linear dynamical
system with sensor and process noise. Our main contributions
can be listed as follows:

•

We give optimal guarantees on the achievable state
estimation error against arbitrary sensor attacks and
propose an algorithm to achieve the same guarantees;

 
 
 
 
 
 
•

As a result of independent interest, we give a cod-
ing theoretic interpretation (alternate proof) for the
necessary and sufﬁcient conditions for secure state
estimation in the absence of noise [2], [3], [6] (known
as the sparse observability condition).

The remainder of this paper is organized as follows.
Section II deals with the setup. The main results are stated
in Section III. Section IV considers the simpler setting of a
scalar state and illustrates the main ideas behind our estimation
algorithm and Section V considers its generalization to a vector
state. Finally, we discuss the coding theoretic view of the
sparse observability condition [3] in Section VI.

II. SETUP

A. System model

We consider a linear dynamical system with sensor attacks

as shown below:

x (t + 1) = Ax(t) + w(t),

y(t) = Cx(t) + v(t) +φφφ (t),

(1)

∈

∈

Rn denotes the state of the plant at time t

Rn denotes the process noise at time t, y(t)

N,
where x(t)
∈
Rp de-
w(t)
∈
Rp denotes the
notes the output of the plant at time t and v(t)
(cid:1),
N (cid:0)0, σ 2
∈
sensor noise at time t. The process noise w(t)
∼
i.e., w(t) is Gaussian distributed with zero mean and covari-
ance matrix σ 2
wIn, where In is the identity matrix of dimension
(cid:1).
R. Similarly, sensor noise v(t)
n and σw
∈
Both v(t) and w(t) are instantiated i.i.d. over time, and v(t) is
independent of w(t).

N (cid:0)0, σ 2

v Ip

wIn

∼

∈

The sensor attack vector φφφ (t)

Rp in (1) is introduced
by a k-adversary deﬁned as follows. A k-adversary has access
to any k out of the p sensors in the system. Speciﬁcally, let
κκκ
=
k). The k-adversary can observe the actual outputs in the k
attacked sensors and change them arbitrarily. Speciﬁcally, the
output of an attacked sensor j

denote the set of attacked sensors (with

κκκ can be expressed as

1, 2, . . . p
}

⊆ {

κκκ
|

|

∈

y j(t) = cT

j x(t) + v j(t) + φ j(t),

(2)

where T denotes the matrix transpose operation, cT
is the
j
j and φ j(t) is
jth row of C, v j(t) is the noise at sensor
the adversarial corruption introduced at sensor j. For j /
κκκ,
∈
φ j(t) = 0. The adversary’s choice of κκκ is unknown but is
assumed to be constant over time (static adversary). The
adversary is assumed to have unbounded computational power,
and knows the system parameters (e.g., A and C) and noise
statistics (e.g., σ 2
v ). However, the adversary is limited
to have only causal knowledge of the process noise and the
sensor noise in good sensors (not attacked by the adversary).
We discuss this assumption in more detail in Section II-C.

w and σ 2

B. State estimation: prediction and ﬁltering

In this paper, we address two state estimation problems:

(1) state prediction and (2) state ﬁltering.

In the state prediction problem, the goal is to estimate
the state at time t based on outputs till time t
1. In the
absence of sensor attacks, using a Kalman ﬁlter for predicting
the state in (1) leads to the optimal (MMSE) error covariance

−

asymptotically [12]. In particular, the Kalman ﬁlter update rule
can be written as:

ˆx(t + 1) = Aˆx(t) + L(t) (y(t)

Cˆx(t)) ,

(3)

−

where ˆx(t + 1) is the state estimate at time t + 1 and L(t) is
the Kalman ﬁlter gain. For a Kalman ﬁlter in steady state [12],
the steady state gain satisﬁes L(t) = L. Also, we use Popt,s to
denote the trace of steady state (prediction) error covariance
matrix [12] obtained by using a Kalman ﬁlter on a sensor
subset s

.

1, 2, . . . p
}

⊆ {

In contrast to the prediction problem, the goal in the state
ﬁltering problem is to estimate the state at time t based on
outputs till time t. In the absence of sensor attacks, a Kalman
ﬁlter update rule similar to (3) can be used for the ﬁltering
problem [12] (see Appendix C for details) and we use Fopt,s
to denote the trace of steady state (ﬁltering) error covariance
matrix obtained by using a Kalman ﬁlter on a sensor subset s.

C. Causal knowledge assumptions

At time t, the attack vector φφφ (t) in (1) depends on the
knowledge of the adversary at time t, and in this context, we
limit the adversary’s knowledge of the process and sensor noise
along the lines of causality. In particular, for the prediction
problem we assume the following for a k-adversary:

(A1)

(A2)

The adversary’s knowledge at time t is statistically
independent of w(t(cid:48)) for t(cid:48) > t, i.e., φφφ (t) is statistically
independent of

}t(cid:48)>t ;
1, 2, . . . p
For a good sensor d
κκκ, the adversary’s
∈ {
knowledge at time t (and hence φφφ (t)) is statistically
vd(t(cid:48))
}t(cid:48)>t .
independent of
{

w(t(cid:48))
{

} −

Intuitively, assumptions (A1) and (A2) limit the adversary
to have only causal knowledge of the process noise and the
sensor noise in good sensors (not attacked by the adversary).
Note that, apart from (A1) and (A2), we do not
impose
any restrictions on the statistical properties, boundedness and
the time evolution of the corruptions introduced by the k-
adversary. In the ﬁltering problem, we replace assumptions
(A1) and (A2) with (A3) and (A4) as described below:

(A3)

(A4)

The adversary’s knowledge at time t is statistically
t, i.e., φφφ (t) is statistically
independent of w(t(cid:48)) for t(cid:48) ≥
w(t(cid:48))
t ;
independent of
}t(cid:48)≥
{
For a good sensor d
1, 2, . . . p
κκκ, the adversary’s
knowledge at time t (and hence φφφ (t)) is statistically
t .
independent of
}t(cid:48)≥
Clearly, (A3) is a stronger version of (A1), requiring φφφ (t) to
be independent of w(t). Similarly, (A4) is a stronger version
of (A2).

∈ {
vd(t(cid:48))
{

} −

D. Sparse observability condition

For the matrix pair (A, C), the observability matrix O with

observability index µ is deﬁned as shown below:

O =







C
CA
...
CAµ

1
−







.

(4)

In this context, a linear dynamical system, characterized by the
pair (A, C), is said to be observable if there exists a positive
integer µ such that O has full column rank. In the absence
of sensor and process noise, the conditions under which state
estimation can be done despite sensor attacks have been
studied in [2], [3], [6]. In particular, a linear dynamical system
as shown in (1) is called θ -sparse observable if for every subset
of size θ , the pair (A, Cs) is observable (where Cs
s
is formed by the rows of C corresponding to sensors indexed
by the elements of s). Also, θ is the smallest positive integer
to satisfy the above observability property. The condition:

1, . . . p
}

⊆ {

θ

p

−

≤

2k,

(5)

is necessary and sufﬁcient for exact state estimation against a
k-adversary in the absence of process and sensor noise [3]; we
will refer to this condition as the sparse observability condition.
We provide a coding theoretic interpretation for the same in
Section VI.

III. MAIN RESULTS

We ﬁrst state our achievability result followed by an

impossibility result.

Theorem 1 (Achievability): Consider the linear dynamical
system deﬁned in (1) satisfying the sparse observability con-
dition (5) against a k-adversary. Assuming (A1) and (A2),
and a time window G =
for the state
prediction problem, the following bound on the prediction error
is achievable against a k-adversary. For any ε > 0 and δ > 0,
there exists a large enough N such that:

t1,t1 + 1, . . .t1 + N
{

1
}

−

(cid:32)

P

1
N ∑

t

G

∈

eT (t)e(t)

max
=p
1,2,...p
,
s
|
|
}

k
−

≤

s

⊂{

(Popt,s) + ε

(cid:33)

δ ,

1

−

≥

(6)

where e(t) = x(t)
ˆx(t) is the estimation error for the state
estimate ˆx(t). In other words, with high probability (w.h.p.),
(Popt,s) is

the bound lim sup

eT (t)e(t)

≤

s

⊂{

max
=p
1,2,...p
,
s
|
|
}

k
−

−
1
N ∑

t

G

∈

N

∞

→

achievable. Similarly, for the state ﬁltering problem, assuming
(A3) and (A4) against a k-adversary, the following bound on
the corresponding ﬁltering error e(t) is achievable w.h.p.:

lim sup

N

∞

→

1
N ∑

t

G

∈

eT (t)e(t)

max
=p
1,2,...p
,
s
|
|
}

k
−

≤

s

⊂{

(Fopt,s) .

(7)

The achievability in Theorem 1 is through our proposed
algorithms, which we discuss in the following sections. The
impossibility result can be stated as follows.

Theorem 2 (Impossibility): Consider the linear dynamical
system deﬁned in (1) and an oracle MMSE estimator that
has knowledge of κκκ, i.e., the set of sensors attacked by a
k-adversary. Then, there exists an attack sequence φφφ (t) such
that the trace of the prediction error covariance of the oracle
estimator is bounded from below as follows:

tr (cid:0)E (cid:0)e(t)eT (t)(cid:1)(cid:1)

Popt,s,

≥

(8)

where e(t) above is the oracle estimator’s prediction error and
s =

κκκ. Similarly, for the ﬁltering problem,
tr (cid:0)E (cid:0)e(t)eT (t)(cid:1)(cid:1)

Fopt,s.

1, 2, . . . p
{

} −

(9)

≥

Proof: Consider the attack scenario where the outputs
from all attacked sensors are equal to zero, i.e., the corruption
φ j(t) =
κκκ. Hence, the information col-
lected from the attacked sensors cannot enhance the estimation
performance. Accordingly, the estimation performance from
the remaining sensors is the best one can expect to achieve.

cT
j x(t)

v j(t),

−

−

∈

∀

j

Clearly, for the adversary’s best choice of κκκ, the guarantees
given in our achievability match the impossibility bound (in
an empirical average sense), and hence, we consider our guar-
antees optimal. We measure the performance of our proposed
algorithms in terms of empirical average (and not expectation)
since the resultant error in the presence of attacks may not be
ergodic.

IV. SECURE STATE ESTIMATION: SCALAR STATE

In this section, we illustrate the main ideas behind our
general scheme in the simpler setting of estimating a scalar
state variable against a k-adversary. In particular, we focus on
the state prediction problem for the system in (1) when the
2k + 1 sensors (i.e., 1-sparse
state is a scalar and there are p
observability condition against k-adversary). For clarifying the
presence of scalar terms in our analysis, we use the scalar
version (regular instead of bold face) of the notation developed
in Section II, i.e., x(t) for the plant’s state, ˆx(t) for the estimate,
and yd(t) = cdx(t) + vd(t) for the output of a good sensor
d
κκκ. We ﬁrst describe our proposed algorithm
} −
for a time window G =
of size N, and
then analyze its performance.

t1,t1 + 1, . . .t1 + N
{

1, 2, . . . p

1
}

∈ {

≥

−

−

−

Secure scalar state prediction algorithm: Considering a
time window G, Algorithm 1 shows the secure state prediction
algorithm for the case when the state is a scalar. The algorithm
(cid:1) Kalman ﬁlters in parallel; one Kalman
runs a bank of (cid:0) p
k
p
−
k sensors. For
ﬁlter associated with each distinct set of p
each distinct set s of p
k sensors, the corresponding Kalman
ﬁlter fuses all the measurements from these sensors in order
to calculate (prediction) estimate ˆxs(t). Using the calculated
estimate ˆxs(t), we calculate the individual residues for each
sensor as shown in (10). The algorithm, then, exhaustively
searches for the set s of p
k sensors which satisfy the
−
residue test shown in (11). If a set s(cid:63) satisﬁes the residue test,
it is declared good and the corresponding Kalman estimate
ˆxs(cid:63)(t) is used as the state estimate for the given time window.
Intuitively, the residue test checks if the outputs from a given
sensor set s are consistent with the corresponding Kalman
estimate over the time window G.

∈

−

d
∀

d(t)(cid:1) = c2

dPopt,s +σ 2
v ,

Performance analysis: Consider the set s of p

k
sensors which are not attacked by the k-adversary. Assuming
that the Kalman ﬁlter corresponding to set s is in steady state, it
can be shown that E (cid:0)r2
s [12] (where
residue rd(t) is as deﬁned in (10)). For large enough N, due to
the (strong) law of large numbers (LLN), the residue test will
be satisﬁed w.h.p. for at least this set of good sensors. This
ensures that w.h.p., the algorithm will not return an empty set.
Also, the estimate ˆxs(t) from this set of good sensors trivially
achieves the error bound (6). But, since the algorithm can
k which satisﬁes the residue test,
return any set of size p
it may be possible that some of the sensors in the returned set
are corrupt. In the remainder of our analysis, we show that for
any set returned by the algorithm, the corresponding Kalman
estimate achieves (6).

−

Algorithm 1 SECURE STATE PREDICTION - SCALAR CASE

1: Enumerate all sets s

S such that:
1, 2, . . . p
s
{
|
|
}
S, run a Kalman ﬁlter that uses all sensors

∈
⊂ {
indexed by s and returns estimate ˆxs(t)

2: For each s

}
R.

s
s
|

= p

S =

−

∈

k

,

.

3: For each s

∈
over a time window G =

S, calculate the residues for all sensors d
as:

∈

t1,t1 + 1, . . .t1 + N
{
cd ˆxs(t)

s,

−

1
}
G.

d
∀

t
∀

−
S which satisﬁes the following residue

∈

∈

(10)

rd(t) = yd(t)

4: Pick the set s(cid:63)

test:

∈

where (a) follows from the independence of e(t) from vd(t)
(due to assumption (A2), ˆxs(t) is independent of good sensor
noise vd(t) despite sensor attacks). Also, using (15) and taking
the expectation in (13):

s

∈

(cid:32)

E

(cid:33)

e2(t)

Popt,s +

≤

2ε
c2
d

.

(16)

1
N ∑

t

G

∈

As the ﬁnal step in our analysis, we will now show that the
variance of cross term 2cd
G e(t)vd(t) is vanishingly small
∈
as N
∞. For any ε1 > 0, there exists a large enough N such
that:


N ∑t

→

s(cid:63),

d
∀

∈

(11)

0 is a design parameter and can be made

t

G

≤

r2
d(t)

dPopt,s(cid:63) + σ 2
c2

1
N ∑
∈
where ε
arbitrarily small for large enough N.
G.

5: Return s(cid:63) and ˆx(t) := ˆxs(cid:63)(t)

v + ε

≥

t
∀

∈

Suppose the algorithm returns a set s of p

k sensors.
There is deﬁnitely one good sensor (say sensor d) in this set
because there can be at most k attacked sensors and p
k > k.
Since the residue test is satisﬁed for this sensor, we have the
following constraint:

−

−

(cid:33)2


(cid:32)

E



e(t)vd(t)

t

G

1
N ∑
G E (cid:0)e2(t)v2
N2

∈

d(t)(cid:1)

∑t

∈

E (cid:0)e2(t)(cid:1) E (cid:0)v2

+

∈

G

1
N2 ∑
t
2
N2 ∑
t, t(cid:48)∈
(cid:18) ∑t
σ 2
v
N

E

G, t<t(cid:48)
G e2(t)
∈
N

=

(a)
=

=

E (cid:0)e(t)vd(t)e(t(cid:48))vd(t(cid:48))(cid:1)

+

2
N2 ∑
t, t(cid:48)∈
d(t)(cid:1)

G, t<t(cid:48)

E (cid:0)e(t)vd(t)e(t(cid:48))(cid:1) E (cid:0)vd(t(cid:48))(cid:1)

(cid:19) (b)

≤

ε1,

(17)

t

∈

1
N ∑
1
N ∑
c2
d
N ∑

G

∈

t

t

G

∈

1
N ∑

t

G

∈

r2
d(t)

(a)
=

=

=

(b)

≤

cd ˆxs(t))2

(cdx(t) + vd(t)

G
(cde(t) + vd(t))2

−

e2(t) +

1
N ∑

t

G

v2
d(t) +

2cd
N ∑

t

G

∈

dPopt,s + σ 2
c2

∈
v + ε,

where (a) follows from the independence of e(t) from vd(t)
and the independence of vd(t(cid:48)) from e(t)vd(t)e(t(cid:48)) (for t(cid:48) > t),
(b) follows from (16). The above result implies that the cross
term 2cd
G e(t)vd(t) (with zero mean) has vanishingly small
∈
variance as N
∞. As a result, using Chebyshev’s inequality
and (14), we have the error bound (6).

N ∑t

→

e(t)vd(t)

(12)

V. SECURE STATE ESTIMATION: VECTOR STATE

where (a) follows from yd(t) = cdx(t)+vd(t) for a good sensor
d and (b) follows from the residue test. The error e(t) above is
the state estimation (prediction) error at time t (in the presence
of a k-adversary) when ˆxs(t) is used as the state estimate. Using
LLN, we can make an additional simpliﬁcation as follows. For
any ε > 0, there exists a large enough N such that:

e(t)vd(t)

c2
d
N ∑

t

G

e2(t) +

∈
c2
dPopt,s +

(a)

≤

t

G

2cd
N ∑
1
N ∑

(cid:12)
(cid:12)
σ 2
(cid:12)
v −
(cid:12)
(cid:12)

∈

t

G

∈

(13)

(cid:12)
(cid:12)
v2
(cid:12)
d(t)
(cid:12)
(cid:12)

+ ε

(b)

≤

c2
dPopt,s + 2ε,

(14)

G e(t)vd(t) in (13) is vanishingly small w.h.p. as N
∈

where (a) follows from (12), and (b) follows w.h.p. due to
the cross term
LLN. Our next step will be to show that
2cd
∞;
N ∑t
this leads to the required bound on 1
G e2(t) using (14). We
∈
do so in two steps: ﬁrst we show that the mean of the cross
term 2cd
G e(t)vd(t) is zero and then show that its variance
∈
is vanishingly small as N

N ∑t

N ∑t

→

∞.

The mean of the cross term 2cd

computed as shown below:

N ∑t

G e(t)vd(t) can be
∈

→

In this section, we consider the state estimation problem
(against a k-adversary) for the general linear dynamical system
described in (1), when the state is a vector. We focus on
the prediction problem in this section; the ﬁltering problem
is studied in Appendix C. We assume that
the system is
θ -sparse observable such that it satisﬁes the sparse observ-
ability condition (5) against a k-adversary. We ﬁrst introduce
some additional notation required for our proposed algorithm.

−

Additional notation: Consider a set s of p

k sensors.
Such a set has (cid:0)p
(cid:1) sensor subsets of size θ , and we index
k
−
θ
these subsets of s by i. Due to the θ -sparse observability
i forms an observable pair (A, Ci)
condition, each subset
with observability matrix Oi and observability index µi; Ci
is formed by rows of C corresponding to subset i of s. We
deﬁne matrices Ji and Mµi as shown below:
0
0
Ci
...
2 CiAµi
−

. . .
0
. . .
0
. . .
0
...
. . .
. . . Ci

0
Ci
CiA
...
CiAµi

, Mµi = σ 2

i + σ 2

wJiJT

v Iµi.













Ji =

3
−





(18)

(cid:32)

E

2cd
N ∑

t

G

∈

(cid:33)

e(t)vd(t)

(a)
=

2cd
N ∑

t

G

∈

E (e(t)) E (vd(t)) = 0,

(15)

The pseudo-inverse of Oi is denoted by O†
i . The output from
Rθ .
sensor subset i (of size θ ) at time t is denoted by yi(t)
We consider the state estimation problem for a time window G

∈

Algorithm 2 SECURE STATE PREDICTION - VECTOR CASE

1: Enumerate all sets s

S such that:
1, 2, . . . p
s
{
|
|
}
S, run a Kalman ﬁlter that uses all sensors

∈
⊂ {
indexed by s and returns estimate ˆxs(t)

2: For each s

}
Rn.

s
s
|

= p

S =

−

∈

k

,

.

3: For each set s

S, enumerate all subsets of size θ
and index them by i. Let µi be the observability index
associated with sensor subset i. For each subset i of s
(subset of size θ ), calculate the block residue:

∈

∈

ri(t) =







yi(t)
yi(t + 1)
...
yi(t + µi





 −

1)

Oi ˆxs(t)

G.

t
∀

∈

−

4: Pick the set s(cid:63)

∈

S which satisﬁes the following block
residue test for each subset i of s(cid:63) (subset of size θ ).
1 of size NB such
Partition G into µi groups G0, G1, . . . Gµi
that Gl =
and check that for each
Gl:

t1) mod µi) = l

t
{

((t

−

}

−

|

i ri(t)rT

i (t)O†T
i

(cid:17)

(cid:16)
O†

t

tr

∑
Gl
∈
Popt,s(cid:63) + tr

1
NB

≤

(cid:16)
O†

i MµiO†T

i

(cid:17)

+ ε,

(19)

0 is a design parameter which can be made

where ε
arbitrarily small for large enough NB.
G.

5: Return s(cid:63) and ˆx(t) := ˆxs(cid:63)(t)

≥

t
∀

∈

of size N and assume without loss of generality that µi divides
N such that µiNB = N.

−

Secure state prediction algorithm: Similar to the scalar
(cid:1) Kalman ﬁlters
setting, Algorithm 2 runs a bank of (cid:0) p
k
p
−
in parallel. For each distinct set s of p
k sensors,
the
corresponding Kalman ﬁlter fuses all the measurements from
these sensors in order to calculate an estimate ˆxs(t). For a
k to satisfy the block residue test,
sensor set s of size p
each of its (cid:0)p
(cid:1) subsets should satisfy (19) for each group
k
−
θ
Gl. If a set s(cid:63) satisﬁes the residue test, it is declared good and
the corresponding Kalman estimate ˆxs(cid:63)(t) is used as the state
estimate for the given time window. Intuitively, the residue
test checks if the outputs from every observable sensor subset
of size θ within set s are consistent with the corresponding
Kalman estimate over the time window G. We analyze the
performance of Algorithm 2 in Appendix A.

−

VI. SPARSE OBSERVABILITY: CODING THEORETIC VIEW

In this section, we revisit the sparse observability condition
(5) against a k-adversary and give a coding theoretic interpre-
tation for the same. We ﬁrst describe our interpretation for a
linear system, and then discuss how it can be generalized for
non-linear systems.

Consider the linear dynamical system in (1) without the
process and sensor noise (i.e., x (t + 1) = Ax(t), y(t) = Cx(t)+
Rn and the system
φφφ (t)). If the system’s initial state is x(0)
is θ -sparse observable, then clearly in the absence of sensor
attacks, by observing the outputs from any θ out of p sensors
for n time instants (t = 0, 1, . . . n
1) we can exactly recover

∈

−

Y

Y

Y
1

∈ {

x(0) and hence, exactly estimate the state of the plant. A coding
theoretic view of this can be given as follows. Consider the
for n time instants as a
1, 2, . . . p
outputs from sensor d
}
Rn. Thus, in the (symbol) observation vector
symbol
d
∈
2 . . .
p], due to θ -sparse observability, any θ
= [
Y
Y
symbols are sufﬁcient (in the absence of attacks) to recover
the initial state x(0). Now, let us consider the case of a k-
adversary which can arbitrarily corrupt any k sensors. In the
coding theoretic view, this corresponds to arbitrarily corrupting
any k (out of p) symbols in the observation vector. Intuitively,
based on the relationship between error correcting codes and
the Hamming distance between codewords in classical coding
theory [13], one can expect the recovery of the initial state
despite such corruptions to depend on the (symbol) Ham-
ming distance between the observation vectors corresponding
initial states (say x(1)(0) and x(2)(0) with
to two distinct
x(1)(0)
= x(2)(0)). In this context, the following lemma relates
θ -sparse observability to the minimum Hamming distance
between observation vectors in the absence of attacks; this
leads to a (tight) bound on the number of attacked sensors
that can be tolerated for state estimation.

Lemma 1: For a θ -sparse observable system with p sen-
sors, the minimum (symbol) Hamming distance between ob-
servation vectors corresponding to distinct
initial states is
p

θ + 1.

−

Y

Y

Y

−

(1) and

(1) and

(2) can be identical;

Y
1 symbols in

Proof: Consider observation vectors

this would imply x(1)(0) = x(2)(0). Hence,

(2) cor-
responding to distinct initial states x(1)(0) and x(2)(0). Due
(1)
to θ -sparse observability, at most θ
if any θ of the symbols are
and
identical,
the
(symbol) Hamming distance between the observation vectors
(2) (corresponding to x(1)(0) and x(2)(0)) is at
θ + 1 symbols. Furthermore, there
, such that the
(2) are identical
corresponding observation vectors
in exactly θ
θ + 1
symbols. Hence, the minimum (symbol) Hamming distance
between the observation vectors is p

Y
least p
−
exists a pair of initial states

Y
1 symbols1 and differ in the rest p

(cid:17)
x(1)(0), x(2)(0)

(1) and

1) = p

θ + 1.

Y
(θ

−

−

−

−

Y

(cid:16)

−

−

The above lemma connects the problem of state estimation
with sensor attacks in a dynamical system to error correction in
classical coding theory. Since the minimum Hamming distance
between the observation vectors corresponding to distinct
θ + 1, we can correct up to k < p
θ +1
initial states is p
−
2
sensor corruptions; this is equivalent to the condition θ
≤
2k, which is precisely the sparse observability condition
p
required against a k-adversary2. It should be noted that a k-
adversary can attack any set of k (out of p) sensors, and
the condition k < p
is both necessary and sufﬁcient for
−
θ +1
exact state estimation despite such attacks. When k
,
2
it is straightforward to show a scenario where the observation
vector (after attacks) can be explained by multiple initial states,
and hence exact state estimation is not possible. The following

θ +1
2

≥

−

−

p

1If there is no such pair of initial states, the initial state can be recovered by
observing any θ
1 sensors. By deﬁnition, in a θ -sparse observable system,
θ is the smallest positive integer, such that the initial state can be recovered
by observing any θ sensors.

−

2In addition, since the minimum Hamming distance is p

θ + 1, we can

detect attacks up to (p

θ + 1)

1 = p

−

−

−

−
θ sensor corruptions.

(cid:54)
example illustrates such an attack scenario in view of the
coding theoretic interpretation discussed above.

θ +1
2

Example 2: Consider a θ -sparse observable system with
θ = 2, number of sensors p = 5, and a k-adversary with k =
2. Clearly, the condition k < p
is not satisﬁed in this
−
example. Let x(1)(0) and x(2)(0) be distinct initial states, such
(2) have
(1) and
that the corresponding observation vectors
Y
Y
(minimum) Hamming distance p
θ +1 = 4 symbols. Figure 1
(2), and for the sake
depicts the observation vectors
Y
of this example, we assume that the observation vectors have
(1)
the same ﬁrst symbol (i.e.,
1) and differ in
1 =
Y
the rest 4 symbols (hence, a Hamming distance of 4). Now, as

−
(1) and

(2)
1 =

Y

Y

Y

(1) and

Fig. 1. Example with θ = 2, p = 5 and k = 2. For distinct initial states x(1)(0)
(2). Both
and x(2)(0), the corresponding observation vectors are
Y
(2) have the same ﬁrst symbol, but differ in the rest four symbols.
(cid:105)
Y
, there are
Given (attacked) observation vector
=
two possibilities for the initial state: (a) x(1)(0) with attacks on sensors 4 and
5, or (b) x(2)(0) with attacks on sensors 2 and 3.

(1)
3 Y

(2)
4 Y

(1)
2 Y

(1) and

(2)
5

Y

Y

Y

Y

Y

(cid:104)

1

1

(cid:104)

Y

Y

Y

=

(2)
5

(1)
3 Y

(1)
2 Y

shown in Figure 1, suppose the observation vector after attacks
(cid:105)
(2)
was
. Clearly, there are two
4 Y
possible explanations for this (attacked) observation vector: (a)
the initial state was x(1)(0) and sensors 4 and 5 were attacked,
or (b) the initial state was x(2)(0) and sensors 2 and 3 were
attacked. Since there are two possibilities, we cannot estimate
the initial state exactly given the attacked observation vector.
This example can be easily generalized to show the necessity
of the condition k < p
−

.

θ +1
2

For (noiseless) non-linear systems, by analogously deﬁning
θ -sparse observability, the same coding theoretic interpretation
holds. Hence, this leads to an alternative proof for the neces-
sary and sufﬁcient conditions for secure state estimation in any
noiseless dynamical system.

REFERENCES

[1] F. Pasqualetti, F. Dorﬂer, and F. Bullo, “Control-theoretic methods
for cyber-physical security,” IEEE Control Systems Magazine, Aug.
2014,
to appear. [Online]. Available: http://motion.me.ucsb.edu/pdf/
2013u-pdb.pdf

[2] H. Fawzi, P. Tabuada, and S. Diggavi, “Secure estimation and control for
cyber-physical systems under adversarial attacks,” IEEE Transactions
on Automatic Control, vol. 59, no. 6, pp. 1454–1467, June 2014.

[3] Y. Shoukry and P. Tabuada, “Event-triggered state observers for sparse
sensor noise/attacks,” arXiv pre-print, Sep. 2013. [Online]. Available:
http://arxiv.org/abs/1309.3511

[4] S. Mishra, N. Karamchandani, P. Tabuada, and S. Diggavi, “Secure state
estimation and control using multiple (insecure) observers,” in IEEE
Conference on Decision and Control (CDC), 2014.

[5] Y. Shoukry, P. Nuzzo, A. Puggelli, A. L. Sangiovanni-Vincentelli, S. A.
Seshia, and P. Tabuada, “Secure state estimation for cyber physical
systems under sensor attacks: a satisﬁability modulo theory approach,”
arXiv pre-print, Dec. 2014.

[6] M. S. Chong, M. Wakaiki, and J. P. Hespanha, “Observability of linear
systems under adversarial attacks,” in American Control Conference
(ACC), 2015.

[7] M. Pajic, J. Weimer, N. Bezzo, P. Tabuada, O. Sokolsky, I. Lee,
and G. Pappas, “Robustness of attack-resilient state estimators,” in
ACM/IEEE International Conference on Cyber-Physical Systems (IC-
CPS), 2014.

[8] Y. Mo and B. Sinopoli, “Secure control against replay attacks,” in
Allerton Conference on Communication, Control, and Computing, 2009.

[9] C.-Z. Bai and V. Gupta, “On kalman ﬁltering in the presence of a
compromised sensor: fundamental performance bounds,” in American
Control Conference (ACC), 2014.

[10]

J. Mattingley and S. Boyd, “Real-time convex optimization in signal
processing,” IEEE Signal Processing Magazine, vol. 27, no. 3, pp. 50–
61, May 2010.

[11] S. Farahmand, G. B. Giannakis, and D. Angelosante, “Doubly robust
smoothing of dynamical processes via outlier sparsity constraints,” IEEE
Trans. on Signal Processing, vol. 59, no. 10, pp. 4529–4543, Oct. 2011.
[12] T. Kailath, A. Sayed, and B. Hassibi, Linear Estimation. Prentice Hall,

2000.

[13] R. Blahut, Algebraic Codes for Data Transmission.

Cambridge

University Press, 2003.

[14] S.-D. Wang, T.-S. Kuo, and C.-F. Hsu, “Trace bounds on the solution
of the algebraic matrix Riccati and Lyapunov equation,” IEEE Trans-
actions on Automatic Control, vol. 31, no. 7, pp. 654–656, Jul 1986.

APPENDIX

A. Algorithm 2: performance analysis

In this section, we analyze the performance of Algorithm 2.
Similar to the analysis done for the scalar setting in Section IV,
we ﬁrst derive a bound using LLN, and then analyze the
cross term in the bound to obtain ﬁnal guarantees on the state
estimation error in the presence of attacks. The details of the
analysis are described below.

Consider the set s of p

k sensors which are not attacked
by the k-adversary. For such a set s, the block residue ri(t) for
a subset i of s (subset of size θ ) can be expressed as shown
below:

−

ri(t) =






yi(t)
...
yi(t + µi




 −

Oi ˆxs(t)

1)

−


w(t)
w(t + 1)
...
w(t + µi





−







+

2)







vi(t)
vi(t + 1)
...
vi(t + µi







1)

−

ˆxs(t)) + (cid:0)Jiwt:t+µi
2 + vi,t:t+µi
−
ˆxs(t)) + zi,t:t+µi
1,

−

(cid:1)

1
−

(20)
(21)

= Oix(t) + Ji

Oi ˆxs(t)

−

= Oi (x(t)
= Oi (x(t)

−
−

Y1Y(1)2Y(1)3Y(1)4Y(1)5Y1Y(2)2Y(2)3Y(2)4Y(2)5Y(1)Y(2)Y1Y(1)2Y(1)3Y(2)4Y(2)5Yx(1)(0)x(2)(0)(cid:16)
O†

i zi,t:t+µi

tr

1zT
−

i,t:t+µi

1O†T
i
−

(cid:17)

(cid:32)

(cid:32)

E

−

1
NB

and assuming that the Kalman ﬁlter corresponding to sensor
set s is in steady state:

(cid:16)

tr

E

(cid:16)
O†

i ri(t)rT

i (t)O†T
i

(cid:17)(cid:17) (a)

= Popt,s + tr

(cid:16)
O†

i MµiO†T

i

(cid:17)

,

(cid:16)

(cid:17)

1
−

i,t:t+µi

zi,t:t+µi

1zT
−

i +σ 2

follows from Mµi = E
where (a)
=
wJiJT
σ 2
v Iµi. Hence, due to LLN, the block residue test (19)
will be satisﬁed w.h.p. for at least this set of good sensors and
w.h.p. the algorithm will not return an empty set. Also, the
estimate ˆxs(t) from this set of good sensors trivially satisﬁes
the error bound (6). But, since the algorithm can return any
set of size p
k which satisﬁes the block residue test, it may
be possible that some of the sensors in the returned set are
corrupt. In the remainder of our analysis, we show that for
any set returned by the algorithm, the corresponding Kalman
estimate achieves the error bound (6).

−

p

Suppose the algorithm returns a set s of p

k sensors.
2k (sparse observability condition), there exists
Since θ
a subset of θ good sensors in s. The following can be inferred
when the block residue test (19) is satisﬁed for such a subset
i (of size θ ):

−

≤

−

(cid:32)

tr

(a)
=

1
NB
1
NB
t
2
NB

t

∑
Gl
∈
∑
Gl
∈
∑
Gl
∈

t

+

(cid:33)

O†

i ri(t)rT

i (t)O†T
i

eT (t)e(t) +

eT (t)O†

1
NB

∑
Gl
∈
i zi,t:t+µi

t

1
−

Popt,s + tr

(cid:16)
O†

i MµiO†T

i

(cid:17)

+ ε,

(22)

(b)

≤

where e(t) in (a) is the state estimation error at time t (in
the presence of a k-adversary) when ˆxs(t) is used as the state
estimate, and (b) follows from the block residue test (19).
Using (a) and (b) above, for any ε > 0 there exists a large
enough NB such that:

2
NB

(cid:17)

−

t

∑
Gl
∈
1
NB

t

eT (t)O†

i zi,t:t+µi

1
−

(23)

(cid:16)
O†

i zi,t:t+µi

1zT
−

i,t:t+µi

1O†T
i
−

tr

∑
Gl
∈

(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)

eT (t)e(t) +

1
NB

≤

(c)

∑
Gl
∈

(cid:16)
O†

t
(cid:12)
(cid:12)
(cid:12)
tr
(cid:12)
(cid:12)
+ Popt,s + ε

Popt,s + 2ε,

i MµiO†T

i

(24)

i

(cid:17)

i,t:t+µi

1O†T

(cid:16)
O†

1zT
−

i zi,t:t+µi

≤
where (c) follows w.h.p. from LLN; for different time indices
in Gl, tr
corresponds to i.i.d. re-
−
alizations of the same random variable. Along the lines of the
analysis done in the scalar setting in Section IV, we can show
Gl eT (t)O†
that the cross term 2
1 in (23) has zero
NB ∑t
−
mean and vanishingly small variance as NB
∞; this leads
→
to the required bound on 1
G eT (t)e(t). To complete our
∈
analysis we calculate the mean and the variance of the cross
term 2

1 as shown below.

i zi,t:t+µi

N ∑t

Gl eT (t)O†

i zi,t:t+µi

∈

NB ∑t

∈

−

The mean of

2
NB ∑t

Gl eT (t)O†

i zi,t:t+µi

∈

1 can be computed

−

as shown below:

(cid:32)

E

(a)
=

2
NB
2
NB

(cid:33)

eT (t)O†

i zi,t:t+µi

1
−

E (cid:0)eT (t)(cid:1) E

(cid:16)
O†

i zi,t:t+µi

(cid:17)

1
−

= 0,

(25)

t

∑
Gl
∈
∑
Gl
∈

t

follows from the independence of e(t) from
where (a)
1. This is true since both x(t) and ˆxs(t) are inde-
zi,t:t+µi
pendent3 of w(t) and vi(t). Also, using (25) and taking the
expectation in (23):

−

(cid:32)

E

1
NB

∑
Gl
∈

t

(cid:33)

eT (t)e(t)

Popt,s + 2ε.

≤

(26)

Now, we will show that the variance of the cross term
∞. For

2
1 is vanishingly small as NB
NB ∑t
any ε1 > 0, there exists a large enough NB such that:

Gl eT (t)O†

i zi,t:t+µi

→

−

∈

Var

(cid:32)

1
NB

(cid:32)

= E



t

∑
Gl
∈
1
NB

t

(a)
= E



(cid:32)



1
NB

(cid:32)

= E

+ E

(cid:32)

(b)
= E

1
N2
B
(cid:32)

t

∑
Gl
∈
2
N2
B

1
N2
B

t,t(cid:48)∈
∑
Gl
∈
∑
Gl , t<t(cid:48)

t

2
N2
B
t,t(cid:48)∈
(cid:32)
1
N2
B

(c)
= E

(cid:32)

= E

1
N2
B

t

(d)
= E

(cid:32)

1
N2
B

(cid:32)

= E

1
N2
B

t

t

∑
Gl
∈
∑
Gl
∈
∑
Gl
∈
∑
Gl
∈

t

(cid:33)

eT (t)O†

i zi,t:t+µi

1
−

(cid:33)2


1
−

(cid:33)(cid:33)2

1
−
(cid:33)2


eT (t)O†

i zi,t:t+µi

∑
Gl
∈

eT (t)O†

i zi,t:t+µi

∑
Gl
∈

t

eT (t)O†

i zi,t:t+µi

∑
Gl
∈

t

1
−

eT (t)O†

i zi,t:t+µi

1eT (t)O†
−

i zi,t:t+µi

1
−

(cid:33)

eT (t)O†

i zi,t:t+µi

1eT (t(cid:48))O†
−

i zi,t(cid:48):t(cid:48)+µi

1
−

(cid:33)

∑
Gl , t<t(cid:48)

eT (t)O†

i zi,t:t+µi

1eT (t)O†
−

i zi,t:t+µi

1
−

(cid:33)

+

E

(cid:16)
eT (t)O†

i zi,t:t+µi

(cid:17)

1eT (t(cid:48))O†
i
−

E (cid:0)zi,t(cid:48):t(cid:48)+µi
(cid:33)

(cid:1)

1
−

eT (t)O†

i zi,t:t+µi

1eT (t)O†
−

i zi,t:t+µi

1
−

eT (t)O†

i zi,t:t+µi

(cid:16)
O†

i zi,t:t+µi

(cid:17)T

1
−

1
−

(cid:33)

e(t)

(cid:18)

eT (t)O†

i zi,t:t+µi

tr

(cid:16)
O†

i zi,t:t+µi

(cid:17)T

1
−

1
−

(cid:19)(cid:33)

e(t)

(cid:18)

O†

i zi,t:t+µi

(cid:16)
O†

i zi,t:t+µi

(cid:17)T

1
−

1
−

(cid:19)(cid:33)

e(t)eT (t)

tr

3The adversary’s corruptions till time t

1 can inﬂuence ˆxs(t) which is
based on outputs till time t
1. Due to assumption (A1), the adversary’s
1 are independent of w(t) and hence ˆxs(t) is
corruptions till
independent of w(t). Also, x(t) is independent of w(t). Due to assumption
(A2), ˆxs(t) is independent of vi(t).

time t

−

−

−

(cid:16)

E

tr

(cid:16)
O†

i zi,t:t+µi

1zT
−

i,t:t+µi

1O†T
i
−

(cid:17)

E (cid:0)e(t)eT (t)(cid:1)(cid:17)

λ ∗tr (cid:0)E (cid:0)e(t)eT (t)(cid:1)(cid:1)

(e)
=

1
N2
B

t

( f )

≤

1
N2
B

∑
Gl
∈
∑
Gl
∈
(cid:32)

t

E

E

λ ∗
NB

λ ∗
NB

1
NB

(cid:32)

1
NB

(cid:33)

tr (cid:0)e(t)eT (t)(cid:1)

(cid:33)

eT (t)e(t)

∑
Gl
∈
∑
Gl
∈

t

t

=

=

(g)

ε1,

zi,t:t+µi
(cid:18)

(cid:18)

E

(a)

follows

from (25),

≤
from the
follows
where
1eT (t(cid:48))O†
1 from eT (t)O†
i zi,t:t+µi
independence of zi,t(cid:48):t(cid:48)+µi
i
(cid:1) = 0, (d) follows
for t(cid:48) > t, (c) follows from E (cid:0)zi,t(cid:48):t(cid:48)+µi
−
−
1
−
from eT (t)O†
follows from
(e)
1 being a scalar,
the
follows
from Lemma 2 (discussed in Appendix B) with eigen

−
independence of

from e(t),

i zi,t:t+µi

1
−

(b)

(f)

O†

i zi,t:t+µi

1
−

(cid:16)
O†

i zi,t:t+µi

1
−

(cid:17)T (cid:19)(cid:19)

=

λ ∗ = λmax
(cid:17)
i MµiO†T

value
(cid:16)
O†
i MµiO†T

i

i
). Finally,

(i.e., λ ∗ is the maximum eigen value
λmax
of O†
from (26). This
(g)
completes the variance analysis and clearly the cross term
Gl eT (t)O†
2
1 has vanishingly small variance as
NB ∑t
∈
NB
∞. As a result, using Chebyshev’s inequality and (24),
→
we have the following bound: for any ε2 > 0 and δ > 0, there
exists a large enough NB such that:

i zi,t:t+µi

follows

−

(cid:33)

(cid:32)

1
NB

t

P

≤

eT (t)e(t)

Popt,s + ε2

∑
Gl
∈
1
Gl eT (t)e(t)
Popt,s + ε2
1
NB ∑t
}
∈ {
∈
G eT (t)e(t)
Popt,s + ε2, we have the required
N ∑t
∈
G eT (t)e(t) from (28) as follows. For any ε2 > 0
N ∑t
∈

Since
implies 1
bound on 1
and δ > 0, there exists a large enough N such that:

0, 1, . . . µi

≤
≤

(28)

l
∀

δ .

−

≥

−

1

(cid:32)

P

1
N ∑

t

G

∈

(cid:33)

eT (t)e(t)

Popt,s + ε2

≤

δ .

1

−

≥

(29)

This completes our performance analysis.

B. Bounds on the trace of product of symmetric matrices

A useful lemma from [14] providing bounds on the trace

of product of symmetric matrices is as follows.

Lemma 2: If A and B are two symmetric matrices in Rn
n,
×
0), then the following

and B is positive semi-deﬁnite (i.e., B
inequality holds:

(cid:23)

λmin (A)tr (B)

tr (AB)

λmax (A)tr (B) .

(30)

≤

≤

where λmin (A) and λmax (A) denote the minimum and maxi-
mum eigen values of matrix A.

C. Secure state ﬁltering

In this section, for the general linear dynamical system
deﬁned in (1), we study the ﬁltering problem where the goal
is to estimate the state at time t based on outputs till time t
(in contrast to using outputs till time t
1 in the prediction

−

Algorithm 3 SECURE STATE FILTERING - VECTOR CASE

2: For each s

1: Enumerate all sets s

k

∈

,
}

S =

= p

s
s
|
{

∈
⊂ {

S such that:
1, 2, . . . p
s
|
|
S, run a Kalman ﬁlter that uses all sensors in-
dexed by s. The corresponding Kalman (ﬁltering) estimate
is denoted by ˆxs(t)

∈
S, enumerate all subsets of size θ
and index them by i. Let µi be the observability index
associated with sensor subset i. For each subset i of s
(subset of size θ ), calculate the block residue:

Rn.

.
}

−

∈

3: For each set s

(27)

ri(t) =







yi(t)
yi(t + 1)
...
yi(t + µi





 −

1)

Oi ˆxs(t)

G.

t
∀

∈

−

4: Pick the set s(cid:63)

∈

S which satisﬁes the following block
residue test for each subset i of s(cid:63) (subset of size θ ).
Partition G into µi groups G0, G1, . . . Gµi
1 of size NB such
that Gl =
and check that for each
Gl:

t1) mod µi) = l

t
{

((t

−

}

−

|

1
NB

≤

(cid:16)
O†

t

tr

∑
Gl
∈
Fopt,s + tr
(cid:16)
i (t)LT
vT

(cid:16)
O†

2E

−

i ri(t)rT

i (t)O†T
i

(cid:17)

i

i MµiO†T
i O†

i vi,t:t+µi

(cid:17)

(cid:17)

+ ε,

1
−

(32)

0 is a design parameter which can be made

where ε
arbitrarily small for large enough NB.
G.

5: Return s(cid:63) and ˆx(t) := ˆxs(cid:63)(t)

≥

t
∀

∈

problem). In the absence of sensor attacks, using a Kalman
ﬁlter for state ﬁltering in (1) leads to the optimal (MMSE)
error covariance asymptotically [12]. The Kalman ﬁlter update
rule (in steady state) for the ﬁltering problem (without sensor
attacks) is as shown below:

ˆx(t) = ˆx(P)(t) + L

(cid:16)

y(t)

−

(cid:17)
Cˆx(P)(t)

,

ˆx(P)(t + 1) = Aˆx(t),
(31)

where ˆx(t) is the state (ﬁltering) estimate (see [12] for further
ˆx(t), and
details). The ﬁltering error is deﬁned as e(t) = x(t)
as shown in (31), the state estimate ˆx(t) at time t depends on
the outputs at time t. Also, in the absence of sensor attacks,
Fopt,s is the trace of steady state (ﬁltering) error covariance
matrix obtained by using the Kalman ﬁlter on a sensor subset
s

1, 2, . . . p

−

⊆ {

.
}

For the secure state ﬁltering problem, we assume that
sparse observability condition (5), and assumptions (A3) and
(A4) hold against a k-adversary. In addition to the notation
developed in Section V for the prediction problem, we will
θ denotes the matrix
require the following deﬁnition: Li
formed by columns of L corresponding to sensor subset i of set
s (subset i is of size θ ). The algorithm for secure state ﬁltering
(and its analysis) is similar to that for the prediction setting.
In the remainder of this section, we ﬁrst describe the secure
state ﬁltering algorithm and then analyze its performance.

Rn

∈

×

Secure state ﬁltering algorithm: Algorithm 3 shows
the secure state ﬁltering algorithm against a k-adversary. It is

same as Algorithm 2 except for the usage of Kalman (ﬁltering)
estimate and the bound used for the block residue test (32);
is used instead of Popt,s and there is an extra term
Fopt,s
2E
vT
i (t)LT
1 is as deﬁned in
−
(20)).

(where vi,t:t+µi

i vi,t:t+µi

i O†

1
−

(cid:16)

(cid:17)

−

Performance analysis: The performance analysis is
similar to the analysis done for the prediction problem in
Appendix A and we describe the details below.

Consider the set s of p

k sensors which are not attacked
by the k-adversary. For such a set s, the block residue ri(t) for
a subset i of s (subset of size θ ) can be expressed as shown
below:

−

ri(t) = Oi (x(t)

ˆxs(t)) + zi,t:t+µi

1,

−

−

(33)

and assuming that the Kalman ﬁlter corresponding to sensor
set s is in steady state, it can be shown that:

(cid:16)

tr

E

(cid:16)
O†

(cid:17)(cid:17)

i (t)O†T
i

(a)
= Fopt,s + tr

i MµiO†T

i

i ri(t)rT
(cid:16)
O†

(cid:17)

2E

−

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

1
−

(cid:17)

.
(34)

Hence, due to LLN, the block residue test (32) will be satisﬁed
w.h.p. for at least this set of good sensors and w.h.p. the
algorithm will not return an empty set. Also, the estimate ˆxs(t)
from this set of good sensors trivially satisﬁes the error bound
(7). But, since the algorithm can return any set of size p
k
which satisﬁes the block residue test, it may be possible that
some of the sensors in the returned set are corrupt. In the
remainder of our analysis, we show that for any set returned
by the algorithm, the corresponding Kalman estimate achieves
the error bound (7).

−

p

−

≤

Suppose the algorithm returns a set s of p

k sensors.
2k (sparse observability condition), there exists
Since θ
a subset of θ good sensors in s. The following can be inferred
when the block residue test is satisﬁed for such a subset i (of
size θ ):
(cid:32)

(cid:33)

−

tr

(a)
=

1
NB
1
NB
t
2
NB

t

∑
Gl
∈
∑
Gl
∈
∑
Gl
∈

t

+

O†

i ri(t)rT

i (t)O†T
i

eT (t)e(t) +

eT (t)O†

1
NB

∑
Gl
∈
i zi,t:t+µi

t

1
−

(b)

≤

(cid:16)
O†

Fopt,s + tr
(cid:16)
i (t)LT
vT

2E

i

i MµiO†T
i O†

i vi,t:t+µi

(cid:17)

−

(cid:17)

+ ε,

1
−

(35)

where e(t) in (a) is the state estimation error at time t (in
the presence of a k-adversary) when ˆxs(t) is used as the state
estimate, and (b) follows from the block residue test (32).
Using (a) and (b) above, for any ε > 0 there exists a large
enough NB such that:
1
2
NB
NB

eT (t)e(t) +

eT (t)O†

i zi,t:t+µi

(36)

1
−

t

∑
Gl
∈
Fopt,s

≤

2E

−

∑
t
Gl
∈
(cid:16)
i O†
i (t)LT
vT

i vi,t:t+µi

(cid:17)

+ ε

1
−

+

(c)

(cid:12)
(cid:12)
(cid:12)
tr
(cid:12)
(cid:12)

(cid:16)
O†

i MµiO†T

i

(cid:17)

1
NB

−

tr

∑
Gl
∈

t

(cid:16)
O†

i zi,t:t+µi

1zT
−

i,t:t+µi

1O†T
i
−

(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)

(37)

Fopt,s

2E

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

1
−

(cid:17)

+ 2ε,

(cid:17)

tr

−

(cid:16)
O†

in Gl,

≤
where (c) follows w.h.p. from LLN as for different
time
1zT
i zi,t:t+µi
corresponds
indices
−
to i.i.d. realizations of the same random variable. Simi-
lar to the prediction problem,
the
i zi,t:t+µi
cross
in (36) has mean
NB ∑t
i O†
2E
and has vanishingly small vari-
∞. This leads to the claimed bound (7) on state

−
ance as NB
estimation error and we describe the details below.

term 2
vT
i (t)LT

it can be shown that

Gl eT (t)O†
(cid:17)

∈
i vi,t:t+µi

1O†T

i,t:t+µi

1
−

1
−

→

(cid:16)

−

i

For simplifying our calculations, we introduce the term
˜e(t) = e(t) + Livi(t). Due to assumptions (A3) and (A4),
˜e(t) is independent from w(t) and vi(t), and hence inde-
pendent from zi,t:t+µi
1. Now, the mean of the cross term
Gl eT (t)O†
2
NB ∑t
(cid:16)
2
eT (t)O†
NB

1 can be computed as follows:

−
i zi,t:t+µi

i zi,t:t+µi

1
−

E

(cid:17)

−

∈
∑
t
Gl
∈
2
NB

(cid:16)

(cid:16)
˜eT (t)O†

i zi,t:t+µi

i O†

i zi,t:t+µi

1
−

=

(a)
=

E

t

∑
Gl
∈
(cid:16)
i (t)LT
vT
2E

−
where (a)
zi,t:t+µi

−

follows from the independence of

˜e(t) from
1. Also, using (38) and taking the expectation in (36):
(cid:33)

(cid:32)

E

1
NB

eT (t)e(t)

Fopt,s + 2ε.

≤

∑
Gl
∈

t

We
useful
2
NB ∑t

∈

now state

the
in our variance calculation for
Gl eT (t)O†
1.

i zi,t:t+µi

following

−

claim which
the cross

Claim 1: Consider a subset

i of s (subset of size θ )
which satisﬁes the residue test (32) in Algorithm 3. With

(39)

is
term

(cid:17)

E

−

1
−

(cid:17)

,

(cid:16)
i (t)LT
vT

i O†

i zi,t:t+µi

(cid:17)(cid:17)

1
−

(38)

E (cid:0)˜eT (t)˜e(t)(cid:1) < η1,

(40)

1
NB

∑
Gl
∈

t

where η1
1
NB ∑t

Gl

d
is a constant. Furthermore,
∀
E ( ˜ed(t)) < η2 where η2 is a constant.

∈
Proof: See Appendix D.

1, 2, . . . n
}

,

∈ {

Now, we will
Gl eT (t)O†
i zi,t:t+µi

1
NB ∑t
→
For any ε1 > 0, there exists a large enough NB such that:

variance
show that
1 is vanishingly small as NB

the

−

∈

of
∞.



(cid:32)

E



1
NB

eT (t)O†

i zi,t:t+µi

∑
Gl
∈

t

(cid:33)2


1
−

(cid:32)

= E

1
N2
B

∑
Gl
∈

t

eT (t)O†

i zi,t:t+µi

1eT (t)O†
−

i zi,t:t+µi

1
−

(cid:33)

(cid:16)
O†

i zi,t:t+µi

tr

1zT
−

i,t:t+µi

1O†T
i
−

(cid:17)

˜e(t) =


 = e(t) + Livi(t), the following holds:








˜e1(t)
...
˜en(t)

∑
Gl , t<t(cid:48)

eT (t)O†

i zi,t:t+µi

1eT (t(cid:48))O†
−

i zi,t(cid:48):t(cid:48)+µi

1
−

eT (t)O†

i zi,t:t+µi

(cid:33)

1eT (t)O†
−

i zi,t:t+µi

1
−

(cid:33)

(cid:33)

= E

+

= E

(cid:32)

1
∑
N2
B
Gl
t
∈
NB(NB
−
N2
B

(cid:32)

(cid:16)
˜eT (t)O†

i zi,t:t+µi

1
−

−

i (t)LT
vT

i O†

i zi,t:t+µi

(cid:33)

(cid:17)2

1
−

1)

(cid:16)

E

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

(cid:17)(cid:17)2

1
−

(cid:33)

˜eT (t)O†

i zi,t:t+µi

1 ˜eT (t)O†
−

i zi,t:t+µi

1
−

eT (t)O†

i zi,t:t+µi

1 ˜eT (t(cid:48))O†
−

i zi,t(cid:48):t(cid:48)+µi

1
−

eT (t)O†

i zi,t:t+µi

1vT
−

i (t(cid:48))LT

i O†

i zi,t(cid:48):t(cid:48)+µi

(cid:33)

1
−

+ E

eT (t)O†

i zi,t:t+µi

(cid:33)

1eT (t)O†
−

i zi,t:t+µi

1
−

E

(cid:16)
eT (t)O†

i zi,t:t+µi

(cid:17)
1 ˜eT (t(cid:48))
−

×

E

(cid:16)
O†

i zi,t(cid:48):t(cid:48)+µi

(cid:17)

1
−

∑
Gl , t<t(cid:48)

eT (t)O†

i zi,t:t+µi

1vT
−

i (t(cid:48))LT

i O†

i zi,t(cid:48):t(cid:48)+µi

eT (t)O†

i zi,t:t+µi

(cid:33)

1eT (t)O†
−

i zi,t:t+µi

1
−

(cid:33)

1
−

(cid:33)

1
−

∑
Gl , t<t(cid:48)

eT (t)O†

i zi,t:t+µi

1vT
−

i (t(cid:48))LT

i O†

i zi,t(cid:48):t(cid:48)+µi

eT (t)O†

i zi,t:t+µi

(cid:33)

=

1eT (t)O†
−

i zi,t:t+µi

1
−

+ E

(cid:32)

= E

(cid:32)

1
N2
B
(cid:32)

2
N2
B

t,t(cid:48)∈
∑
Gl
∈
2
N2
B

t

t,t(cid:48)∈

+ E

(cid:32)

E

−

(cid:32)

(a)
= E

2
N2
B

1
N2
B

t

+

2
N2
B

t,t(cid:48)∈

t,t(cid:48)∈
∑
Gl
∈
∑
Gl , t<t(cid:48)

∑
Gl , t<t(cid:48)

∑
Gl , t<t(cid:48)

(cid:32)

2
N2
B

1
N2
B
(cid:32)

t,t(cid:48)∈
∑
Gl
∈
2
N2
B

t

−

E

(cid:32)

= E

E

−

(cid:32)

1
N2
B
2
N2
B

t,t(cid:48)∈
∑
Gl
∈
∑
Gl , t<t(cid:48)

t

t,t(cid:48)∈

(cid:32)

1
N2
B
2
N2
B

t

∑
Gl
∈
∑
Gl , t<t(cid:48)

t,t(cid:48)∈

= E

−

= E

+

= E

+

= E

+

= E

+

E

(cid:16)
eT (t)O†

i zi,t:t+µi

(cid:17)

1
−

×

E

(cid:16)
i (t(cid:48))LT
vT

i O†

i zi,t(cid:48):t(cid:48)+µi

(cid:17)

1
−

(cid:33)

eT (t)O†

i zi,t:t+µi

1eT (t)O†
−

i zi,t:t+µi

1
−

E

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

(cid:17)

1
−

E

(cid:16)
i (t(cid:48))LT
vT

i O†

i vi,t(cid:48):t(cid:48)+µi

×
(cid:17)

1
−

(cid:33)

eT (t)O†

i zi,t:t+µi

1eT (t)O†
−

i zi,t:t+µi

1
−

1)

(cid:16)

E

(cid:16)
i (t)LT
vT

i O†

(cid:16)
eT (t)O†

i zi,t:t+µi

1
−

i vi,t:t+µi
(cid:33)

(cid:17)2

1)

(cid:16)

E

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

(cid:17)(cid:17)2

1
−

(cid:17)(cid:17)2

1
−

(cid:16)(cid:0)˜eT (t)

−

i (t)LT
vT
i

(cid:1) O†

i zi,t:t+µi

1
−

(cid:33)

(cid:17)2

1)

(cid:16)

E

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

(cid:17)(cid:17)2

1
−

(cid:32)

1
∑
N2
B
Gl
t
∈
NB(NB
−
N2
B

(cid:32)

1
∑
N2
B
Gl
t
∈
NB(NB
−
N2
B

1
∑
N2
B
Gl
t
∈
NB(NB
−
N2
B

(cid:32)

i (t)LT
vT

i O†

i zi,t:t+µi

1vT
−

i (t)LT

i O†

i zi,t:t+µi

(cid:33)

1
−
(cid:33)

˜eT (t)O†

i zi,t:t+µi

1vT
−

i (t)LT

i O†

i zi,t:t+µi

(cid:16)

E

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

(cid:17)(cid:17)2

1
−

(cid:33)

˜eT (t)O†

i zi,t:t+µi

1 ˜eT (t)O†
−

i zi,t:t+µi

1
−

i (t)LT
vT

i O†

i zi,t:t+µi

(cid:17)2(cid:19)

1
−

˜eT (t)O†

i zi,t:t+µi

1vT
−

i (t)LT

i O†

i zi,t:t+µi

(cid:16)

E

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

(cid:17)(cid:17)2

1
−

(cid:16)
˜eT (t)O†

i zi,t:t+µi

1 ˜eT (t)O†
−

i zi,t:t+µi

1
−

(cid:17)

(cid:18)(cid:16)

i (t)LT
vT

i O†

i zi,t:t+µi

(cid:17)2(cid:19)

1
−

˜eT (t)O†

i zi,t:t+µi

1vT
−

i (t)LT

i O†

i zi,t:t+µi

(cid:16)

E

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

(cid:17)(cid:17)2

1
−

1
−

(cid:33)

1
−

(cid:33)

1
−

−

+

= E

+

−

+

1
N2
B
(cid:32)

t

∑
Gl
∈
1
N2
B
(cid:32)

t

∑
Gl
∈

2E

1
N2
B

t

∑
Gl
∈
1)

NB(NB
−
N2
B

(cid:32)

1
N2
B

∑
t
Gl
∈
(cid:18)(cid:16)

NB
N2
B

E

(cid:32)

2E

NB(NB
−
N2
B
E

1
N2
B

t

∑
Gl
∈
1)

1
∑
N2
B
Gl
t
∈
NB
E
N2
B

+

(cid:32)

2E

1
N2
B

t

∑
Gl
∈
1)

NB(NB
−
N2
B
E

1
∑
N2
B
Gl
t
∈
(cid:18)(cid:16)
NB
E
N2
B
(cid:18)

2

∑t

Gl

∈

−

+

+

−

+

NB
1)

NB(NB
−
N2
B

(b)
=

(cid:16)
˜eT (t)O†

i zi,t:t+µi

1 ˜eT (t)O†
−

i zi,t:t+µi

1
−

(cid:17)

(cid:17)2(cid:19)

1
−

i (t)LT
vT

i zi,t:t+µi

i O†
(cid:19)

E(˜eT (t))
NB

E

(cid:16)
O†

i zi,t:t+µi

(cid:16)

E

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

1vT
−

i (t)LT
(cid:17)(cid:17)2

1
−

i O†

i zi,t:t+µi

(cid:17)

1
−

(cid:18)

(cid:18)

E

tr

˜eT (t)O†

i zi,t:t+µi

(cid:16)
O†

i zi,t:t+µi

(cid:19)(cid:19)

˜e(t)

(cid:17)T

1
−

=

1
∑
N2
B
t
Gl
∈
NB
E
N2
B
(cid:18)

+

1
−
(cid:17)2(cid:19)

(cid:18)(cid:16)

i (t)LT
vT

i zi,t:t+µi

1
−

i O†
(cid:19)

2

∑t

Gl

∈

E(˜eT (t))
NB

−

+

NB
1)

NB(NB
−
N2
B

(cid:16)

E

E

(cid:16)
O†

i zi,t:t+µi

i O†

i zi,t:t+µi

(cid:17)

1
−

1vT
−

i (t)LT
(cid:17)(cid:17)2

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

1
−

This completes our performance analysis.

D. Proof of Claim 1

Using (39):

E (cid:0)˜eT (t)˜e(t)(cid:1) + E (cid:0)vT

i (t)LT

i (t)Livi(t)(cid:1)

1
NB

≤

∑
Gl
t
∈
Fopt,s + 2ε.

The above result implies that 1
by a constant. This also implies that
bounded by a constant

NB ∑t

Gl

∈

d
∀

1, 2, . . . n
}

∈ {

(44)
E (cid:0)˜eT (t)˜e(t)(cid:1) is bounded
E ( ˜ed(t)) is

1
NB ∑t
Gl
as shown below:

∈

E ( ˜ed(t))
|

+

1
NB

t

Gl ,

∈

∑
E( ˜ed (t))
|

1 |

|≥

E ( ˜ed(t))
|

1
NB

≤

≤

≤

≤

(a)

≤

≤

≤

∑
Gl
t
∈
1
NB
1
NB

t

t

1 +

1 +

1 +

1 +

1 +

E ( ˜ed(t))

E ( ˜ed(t))
|
|

∑
Gl
∈

∑
<1 |
E( ˜ed (t))
|
|
∑
E( ˜ed (t))
|

Gl ,

∈

Gl ,
∈
1
NB

t

E ( ˜ed(t))
|

1 |

|≥

∑
E( ˜ed (t))
|

1 |

|≥

2
E ( ˜ed(t))
|

t

Gl ,

∈

E (cid:0) ˜e2

d(t)(cid:1)

∑
E( ˜ed (t))
|
d(t)(cid:1)
E (cid:0) ˜e2

1
|≥

1
NB

1
NB

1
NB
1
NB

t

Gl ,

∈
∑
Gl
∈
∑
Gl
∈

t

t

E (cid:0)˜eT (t)˜e(t)(cid:1) ,

(45)

where (a) follows from Jensen’s inequality. This completes the
proof of Claim 1.

(cid:18)

E

(cid:18)

O†

i zi,t:t+µi

(cid:16)
O†

i zi,t:t+µi

1
−

1
−

(cid:17)T (cid:19)

E (cid:0)˜e(t)˜eT (t)(cid:1)

(cid:19)

(c)
=

tr

1
N2
B ×
∑
Gl
∈
NB
N2
B
(cid:18)

t

+

−

+

−

+

(d)

≤

1
∑
N2
B
Gl
t
∈
(cid:18)(cid:16)
NB
E
N2
B
(cid:18)

+

2

∑t

Gl

∈

(cid:18)(cid:16)

E

i (t)LT
vT

i zi,t:t+µi

i O†
(cid:19)

(cid:17)2(cid:19)

1
−

E

(cid:16)
O†

i zi,t:t+µi

2

∑t

Gl

∈

E(˜eT (t))
NB

NB
1)

(cid:16)

E

(cid:16)
i (t)LT
vT

NB(NB
−
N2
B
λ ∗tr (cid:0)E (cid:0)˜e(t)˜eT (t)(cid:1)(cid:1)

i O†

i vi,t:t+µi

1
−

(cid:17)2(cid:19)

1
−

i (t)LT
vT

i zi,t:t+µi

i O†
(cid:19)

E(˜eT (t))
NB

E

(cid:16)
O†

i zi,t:t+µi

i O†

i zi,t:t+µi

1vT
−

i (t)LT
(cid:17)(cid:17)2

(cid:17)

1
−

(cid:17)

1
−

NB
1)

(cid:16)

E

(cid:16)
i (t)LT
vT

NB(NB
−
N2
B
λ ∗E (cid:0)˜eT (t)˜e(t)(cid:1) +

i O†

i zi,t:t+µi

1vT
−

i (t)LT
(cid:17)(cid:17)2

1
−

i vi,t:t+µi
(cid:18)(cid:16)

E

i O†

NB
N2
B

i (t)LT
vT

i O†

i zi,t:t+µi

(cid:17)2(cid:19)

1
−

1
N2
B

∑
t
Gl
∈
(cid:18)

(cid:19)

E(˜eT (t))
NB

2

∑t

Gl

∈

−

+

NB
1)

NB(NB
−
N2
B

(cid:16)

E

=

(e)

E

(cid:16)
O†

i zi,t:t+µi

i O†

i zi,t:t+µi

(cid:17)

1
−

1vT
−

i (t)LT
(cid:17)(cid:17)2

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

1
−

(cid:16)

E

(cid:16)
i (t)LT
vT

i O†

i vi,t:t+µi

(cid:17)(cid:17)2

+ ε1,

(a)
i zi,t:t+µi

1
−
follows
from the
1 ˜eT (t(cid:48)) from zi,t(cid:48):t(cid:48)+µi
−
i zi,t:t+µi

≤
where
independence
of
eT (t)O†
1, (b) follows from the
−
i O†
independence of ˜e(t) from O†
1vT
1,
−
(c) follows from the independence of ˜e(t) from O†
1,
from Lemma 2 (see Appendix B) with
(d)
E

i zi,t:t+µi
i zi,t:t+µi

i (t)LT

follows

(cid:18)

(cid:18)

−

−

(cid:17)T (cid:19)(cid:19)
,

(cid:16)
O†

and

O†

(e)

(41)

λ ∗ = λmax
follows from Claim 1.

i zi,t:t+µi

1
−

i zi,t:t+µi

1
−

∈

i zi,t:t+µi

Gl eT (t)O†

The above result implies that the variance of the cross term
2
∞. As
NB ∑t
a result, using Chebyshev’s inequality and (37), we have the
following bound: for any ε2 > 0 and δ > 0, there exists a large
enough NB such that:
(cid:32)

1 is vanishingly small as NB

→

(cid:33)

−

t

P

≤

eT (t)e(t)

Fopt,s + ε2

∑
Gl
∈
1
Gl eT (t)e(t)
Fopt,s + ε2
1
NB ∑t
}
∈ {
∈
G eT (t)e(t)
Fopt,s + ε2, we have the required
N ∑t
∈
G eT (t)e(t) from (42) as follows. For any ε2 > 0
N ∑t
∈

Since
implies 1
bound on 1
and δ > 0, there exists a large enough N such that:

0, 1, . . . µi

≤
≤

(42)

l
∀

δ .

−

≥

−

1

1
NB

(cid:32)

P

1
N ∑

t

G

∈

(cid:33)

eT (t)e(t)

Fopt,s + ε2

≤

δ .

1

−

≥

(43)

