Using Cyber Terrain in Reinforcement Learning for
Penetration Testing

Rohit Gangupantulua,†, Tyler Codyb,†,∗, Paul Parka, Abdul Rahmanc,
Logan Eisenbeiserb, Dan Radkec, Ryan Clarkc, Christopher Redinoc
aDeloitte Consulting LLC
bNational Security Institute, Virginia Tech cDeloitte & Touche LLP
†Co-First Authors
∗Corresponding Author: Tyler Cody; tcody@vt.edu

2
2
0
2

g
u
A
4

]

G
L
.
s
c
[

2
v
4
2
1
7
0
.
8
0
1
2
:
v
i
X
r
a

Abstract—Reinforcement learning (RL) has been applied to
attack graphs for penetration testing, however, trained agents
do not reﬂect reality because the attack graphs lack operational
nuances typically captured within the intelligence preparation of
the battleﬁeld (IPB) that include notions of (cyber) terrain. In
particular, current practice constructs attack graphs exclusively
using the Common Vulnerability Scoring System (CVSS) and
its components. We present methods for constructing attack
graphs using notions from IPB on cyber terrain. We consider a
motivating example where ﬁrewalls are treated as obstacles and
represented in (1) the reward space and (2) the state dynamics.
We show that terrain analysis can be used to bring realism to
attack graphs for RL. We use an attack graph with roughly 1000
vertices and 2300 edges and deep Q reinforcement learning with
experience replay to demonstrate the method.

Index Terms—attack graphs, reinforcement learning, cyber

terrain

I. INTRODUCTION

Prediction of vulnerabilities and exploits to support cyber
defense (i.e., the blue picture) typically involves analyzing
ingested data acquired from sensors and agents in various
networks. Learning behaviors based on what has been seen,
i.e., observed on the network, involves meticulous curation
and processing of this data to support model development,
training, testing, and validation. While this has provided some
degree of results in the past, this work intends to explore
an alternative approach toward identifying weaknesses within
networks. The goal is to leverage the attack graph construct
[1] and train machine learning models over them to predict
weaknesses within network topologies.

Under this approach, instead of observing a static, curated
data set, machine learning algorithms can learn by interact-
ing with attack graphs directly. Reinforcement learning (RL)
for penetration testing has shown this to be feasible given
constraints on attack graph representation such as scale and
observability. However, existing literature constructs attack
graphs either with no vulnerability information [2]–[5] or
entirely with vulnerability information [6]–[8].

Youseﬁ et al., Chowdary et al., and Hu et al. use the
Common Vulnerability Scoring System (CVSS) and its com-
ponents to construct attack graphs [6]–[8], similar to Gallon

978-1-6654-8356-8/22/$31.00 ©2022 IEEE

and Bascou [9]. CVSS scores are an open, industry-standard
means of scoring the severity of cybersecurity vulnerabilities.
They provide an empirical and automatic means of con-
structing attack graphs for RL. However, they do not always
correlate to a useful contextual picture for cyber operators.
By relying totally on its abstractions, network representations
unfortunately can be biased totally towards vulnerabilities and
not on a realistic view of how an adversary plans or executes
an attack campaign. As a result, this leads to RL methods
converging to unrealistic attack campaigns.

While CVSS scores provide a strong foundation for attack
graphs, we posit that notions of cyber terrain [10] should be
built into attack graph representations to enable RL agents to
construct more realistic attack campaigns during penetration
testing. In particular, we suggest a focus on the OAKOC
terrain analysis framework that consists of obstacles, avenues
of approach, key terrain, observation and ﬁelds of ﬁre, and
cover and concealment [10]. This work makes the following
contributions:

• We contribute methodology for building OAKOC cyber
terrain into Markov decision process (MDP) models of
attack graphs.

• We apply our methodology to RL for penetration testing
by treating ﬁrewalls as cyber terrain obstacles in an
example network that is at least an order of magnitude
larger than the networks used by previous authors [2]–[8].
In doing so we extend the literature on using CVSS scores to
construct attack graphs and MDPs as well as the literature on
RL for penetration testing.

The paper is structured as follows. First, background is
given on terrain analysis and cyber terrain, reinforcement
learning, and penetration testing. Second, our methods for
constructing terrain-based attack graphs are presented. Then,
results are presented before concluding with remarks on future
steps.

II. BACKGROUND

A. Terrain Analysis and Cyber Terrain

Intelligence preparation of the battleﬁeld (IPB) considers
terrain a fundamental concept [11]. In the physical domain,
terrain refers to land and its features. Conti and Raymond

 
 
 
 
 
 
Fig. 1: Figure 1A shows a reinforcement learning agent L taking actions a in environment E and receiving state s and reward
r. Figure 1B shows a supervised learning agent L learning from example-label pairs (x, y) provided by an oracle. Figure 1C
shows an environment E under the attack tree model. Figure 1D shows an environment E under the attack graph model.

deﬁne cyber terrain as, “the systems, devices, protocols, data,
software, processes, cyber personas, and other network entities
that comprise, supervise, and control cyberspace [10].”

They note that cyber terrain exists at strategic, operational,
and tactical levels, including, e.g., transatlantic cables and
satellite constellations,
telecommunications ofﬁces and re-
gional data centers, and wireless spectrum and Local Area
Network protocols, respectively. In this paper, we consider
what Conti and Raymond refer to as the logical plane of
cyber terrain, which consists of data link, network, network
transport, session, presentation, and application, i.e., layers 2-
7 of the Open Systems Interconnection model [12].

Terrain analysis typically follows the OAKOC framework,
consisting of observation and ﬁelds of ﬁre (O), avenues of
approach (A), key terrain (K), obstacles and movement (O),
and cover and concealment (C). These notions from traditional
terrain analysis can be applied to cyber terrain [13]. For ex-
ample, ﬁelds of ﬁre may concern all that is network reachable
(i.e., line of sight) and avenues of approach may consider
network paths inclusive of available bandwidth [10]. In this
paper, we use obstacles to demonstrate how our methodology
can be used to bring the ﬁrst part of the OAKOC framework
to attack graph construction for reinforcement learning.

B. Reinforcement Learning

Reinforcement learning is concerned with settings where
agents learn from taking actions in and receiving rewards
from an environment [14]. It can be contrasted with supervised
learning, where agents learn from example-label pairs given
by an oracle or labeling function. This contrast is depicted in
Figures 1A and 1B. Naturally, reinforcement learning solution
methods take a more dynamic formulation.

An agent is considered to interact with an environment E
over a discrete number of time-steps by selecting an action
at at time-step t from the set of actions A. In return, the
environment E returns to the agent a new state st+1 and reward
rt+1. Thus, the interaction between the agent and environment
E can be seen as a sequence s1, a1, s2, a2, ..., at−1, st. When
the agent reaches a terminal state, the process stops.

Here we consider a case when E is a ﬁnite MDP. A ﬁnite
MDP is a tuple (cid:104)S, A, Φ, P, R(cid:105), where S is a set of states, A is

a set of actions, Φ ⊂ S ×A is the set of admissible state-action
pairs, P : Φ × S → [0, 1] is the transition probability function,
and R : Φ → R is the expected reward function where R
is the set of real numbers. P (s, a, s(cid:48)) denotes the transition
probability from state s to state s(cid:48) under action a, and R(s, a)
denotes the expected reward from taking action a in state s.
The goal of learning is to maximize future rewards. Using
a discount factor γ ∈ (0, 1],
the expected value of the
discounted sum of future rewards at time t is deﬁned as
Rt = (cid:80)∞
k=0 γkrt+k, that is, the sum of discounted rewards
from time t onward. The action value function Qπ(s, a) =
E[Rt|st = s, a] is the expected return after taking action a in
state s and then following policy π, where π maps (s, a) ∈ Φ
to the probability of picking action a in state s. The optimal
action-value function Q∗(s, a) = maxπ Qπ(s, a).

The action value function Q can be represented by a
function approximator. Herein, we use deep Q-learning (DQN)
to approximate Q∗ with a neural network Q(s, a; θ), where
θ are parameters of the neural network [15], [16]. DQN has
seen broad success and is the basis for many deep RL variants
[17]–[19].

The parameters are learned iteratively by minimizing a

sequence of loss functions Li(θi),

Li(θi) = E(r + γ max
a(cid:48)

Q(s(cid:48), a(cid:48); θi−1) − Q(s, a; θi))2.

This speciﬁc formulation is termed one-step Q-learning, be-
cause s(cid:48) is the state that succeeds s, but it can be relaxed to
n-step Q-learning by considering rewards over a sequence of n
steps. Alternative solution methods to DQN include proximal
policy optimization [20] and asynchronous advantage actor-
critic A3C [21], both of which learn the policy π directly.

C. Penetration Testing

Penetration testing is deﬁned by Denis et al. as, “a simu-
lation of an attack to verify the security of a system or envi-
ronment to be analyzed . . . through physical means utilizing
hardware, or through social engineering [22].” They continue
by emphasizing that penetration testing is not the same as
port scanning. Speciﬁcally, if port scanning is looking through
binoculars at a house to identify entry points, penetration
testing is having someone actually break into the house.

Paper

Network Description(s)

Ghanem and Chen [2]
Schwartz and Kurniawati [3]
Ghanem and Chen [4]
Chaudhary et al. [5]
Youseﬁ et al. [6]
Chowdary et al. [7]
Hu et al. [8]
Our network

100 machine local area network
50 machines with unknown services and 18 machines with 50 services
100 machine local area network
Not reported
Attack graph with 44 vertices and 43 edges
Attack graph with 109 vertices, edges unknown, and a 300 host ﬂat network
Attack graph with 44 vertices and 52 edges
Attack graph with 955 vertices and 2350 edges

TABLE I: Network sizes in the literature.

Penetration testing is part of broader vulnerability detection
and analysis, which typically combines penetration testing
with static analysis [23]–[25]. Penetration testing models have
historically taken the form of either the ﬂaw hypothesis model
[26], [27], the attack tree model [28], [29], or the attack graph
model [1], [30], [31].

The ﬂaw hypothesis model describes the general process
of gathering information about a system, determining a list
of hypothetical ﬂaws, e.g., via domain expert brain-storming,
sorting that list by priority, testing hypothesized ﬂaws in order,
and ﬁxing those that are discovered. As McDermott notes, this
model is general enough to describe almost all penetration
testing [1]. The attack tree model adds a tree structure to
the process of gathering information, generating hypotheses,
etc., which allows for a standardization of manual penetration
testing, and also gives a basis for automated penetration testing
methods. The attack graph model adds a network structure,
differing from the attack tree model in regard to the richness
of the topology and, accordingly, the amount of information
needed to specify the model.

Automated penetration testing has become a part of practice
[32], with the attack tree and attack graph models as its
basis. In reinforcement learning, these models serve as the
environment E. They are depicted in Figure 1C and 1D,
respectively. Both modeling approaches involve constructing
topologies of networks by treating machines (i.e., servers and
network devices) as vertices and links between machines as
edges between vertices. Variants involve integrating additional
detail regarding sub-networks and services. In the case of
attack trees, probabilities must be assigned to the branches
between parent and child nodes, and in the case of attack
graphs, transition probabilities between states must be assigned
to each edge. While many of the favorable properties of attack
trees persist in attack graphs, it is unclear whether attack
graphs can outperform attack trees in largely undocumented
systems, i.e., systems with partial observability [1], [33].

D. Reinforcement Learning for Penetration Testing

Reinforcement learning in penetration testing is promising
because it addresses many challenges. A single penetration
testing tool has never been enough [34]. Yet, RL can be the
basis for many tools, such as analysis, bypassing security, and

penetration, and can by applied to the various types of penetra-
tion testing, i.e., external testing, internal testing, blind testing,
and double-blind testing [35]. The automation and generality
of RL means it can be deployed quickly, in the form of many
variants with different policies, at many points in a network.
And, as Chen et al. note [36], as future networks scale in
the Internet of Things age, intelligent payload mutation and
intelligent entry-point crawling, the kinds of tasks RL is well-
suited for, will be necessary in penetration testing.

Reinforcement

learning for penetration testing uses the
attack graph model [2]–[8]. The environment E is treated as
either a MDP, mirroring classical planning, where actions are
deterministic and the network structure and conﬁguration are
known, or as a Partially Observable Markov Decision Process
(POMDP), where the outcomes of actions are stochastic and
network structure and conﬁguration are uncertain.

While POMDPs are more realistic,

they have not been
shown to scale to large networks and require modeling many
prior probability distributions [33]. Since full observability
leads MDPs to underestimate attack cost, its main ﬂaw is
in ﬁnding vulnerabilities which are unlikely to be found or
exploited. As such, penetration testing on MDPs gives a worst
case analysis, making it the risk averse option in the sense
that it tends towards false alarms. We use MDPs for our attack
graph model because it can scale and because our methodology
for adding cyber terrain to MDP attack graphs can later be
extended to POMDPs.

Unlike most previous work in RL for penetration testing
[2]–[5], but similar to Youseﬁ et al., Hu et al., and Chowdary
et al. [6]–[8], we use vulnerability information to construct the
MDP. In particular, we use the Common Vulnerability Scoring
System (CVSS) [9]. Unlike those previous authors, however,
we extend beyond vulnerability information by folding in
notions of cyber terrain. Following the literature, we use DQN
as the RL solution method [3], [7], [8]. However, we use a
larger network than those in the literature, as reported in Table
I.

III. METHODS

RL-based penetration testing involves a three-step procedure
of (1) extracting the network structure into an attack graph,
(2) specifying an MDP (or POMDP) over the extracted attack

Fig. 2: The network is extracted into an attack graph using MulVal [37]. Terrain can be added via state or reward. To add
via state, the attack graph is ﬁrst modiﬁed to include more state information related to OAKOC [10]. The CVSS MDP is
constructed as usual with the transition terrain-adjusted probabilities. To add via reward, the CVSS MDP is constructed as usual
followed by including terrain-adjusted rewards. Each method leads to a terrain-adjusted CVSS MDP. Note, attack complexity
is a component of CVSS. The values inside the nodes correspond to reward values and the values on the edges are transition
probabilities.

graph, and (3) deploying RL on the MDP. The outcome of
deploying RL can then be studied in various ways to express
the penetration testing results.

The attack graph is extracted using MulVal, a framework
that conducts multihost, multistage vulnerability analysis on
a network representation using a reasoning engine [37]. The
states S of the MDP are given by the vertices of the attack
graph, which can be components of the network, e.g., entries
into a speciﬁc subnet or an intermediary ﬁle server, or can be
means of traversal, e.g., the interaction rules between network
components. That is, not all states are locations in the network.
The actions A of the MDP that are available in a particular
state are given by the outbound edges from that state.

The transition probabilities P (s, a, s(cid:48)) and the reward R of
the MDP are constructed using CVSS. The transition prob-
abilities are assigned using the attack complexity associated
with s(cid:48), which CVSS ranks as either low, medium, or high,
and which we translate into transition probabilities of 0.9, 0.6,
and 0.3, respectively, in following with Hu et al. [8]. The agent
remains in s if the action fails. The reward for arriving at s(cid:48)
is given by

Base Score +

Exploitability Score
10

.

Then, a target node in the network is deemed the terminal
state and given a reward of 100. An initial state is deﬁned and
given a reward of 0.01, and, using a depth ﬁrst search, reward
is linearly scaled from the initial state to the terminal state.

Lastly, −1 reward is assigned to actions which bring the agent
to a state from which the terminal state is inaccessible without
backtracking, or otherwise lead to entering a sub-network from
which the terminal state is not reachable.

We term this particular MDP the CVSS MDP. The RL
agent is trained using DQN in an episodic fashion. Episodes
terminate when the terminal state is reached or after taking a
number of hops, i.e., actions, in the network. This formulation
is similar to those of Youseﬁ et al., Hu et al., and Chowdary
et al. [6]–[8]. This terrain-blind approach ignores the typical
perspectives of attackers when traversing and navigating en-
terprise networks.

Our methodology for adding cyber terrain builds on this
formulation. We propose to add terrain via state and reward to
resolve its short-comings in realism. To add terrain information
via state is to do so by modifying S and P (s, a, s(cid:48)). First,
additional information must be included from MulVal and
other sources into the attack graph originally generated by
MulVal. Then, this additional state information can be used to
modify S or P (s, a, s(cid:48)). By using state, we represent terrain as
an effect on the dynamics of the MDP. As such, it adds terrain
by creating a more realistic model of the environment E. To
add terrain information via reward is to do so by modifying R,
i.e., by reward engineering. Depending on the OAKOC phe-
nomena, this means incrementing or decrementing the reward.
By using reward, we introduce terrain not by directly bringing
realism to E, but rather by incentivizing the agent to behave in

a more realistic manner. These two processes are depicted in
Figure 2. We term these terrain-adjusted CVSS MDPs. The
way this is done within our experiment is by assessing the
network structure of the attack graph gathered from MulVAL
initially. Then, ﬁrewalls, subnets and allowed/blocked services
are parsed accordingly from the MulVAL output. Afterwards,
this information is populated into a YAML ﬁle that captures,
in hash map format, the relations between subnets and services
that are allowed or blocked with ﬁrewalls. This YAML ﬁle is
used to manipulate state and reward using scripts that adjust
the rewards and transition probabilities based on the presence
of a ﬁrewall and type of service blocked. This process of
parsing out aspects of network structure then using them to
create terrain-adjusted CVSS MDPs can be applied to terrain
generally. The following subsections elaborate for this case of
treating ﬁrewalls as obstacles.

The training process utilized a deep Q-network reinforce-
ment learning agent using experience replay and a target Q-
Network. This leveraged a ﬂat action space, with transformed
layers with a ReLu activation function during a forward pass.
An Adam optimizer was used, and smooth L1 loss was also
used to ensure training stability. There were 850,000 training
steps, among which 300,000 were exploration-based, with a
replay buffer size of 50,000. The learning rate was 0.001, with
a batch size of 32, and hidden layer sizes of 64 neurons. An
initial epsilon of 1.0, with a ﬁnal epsilon of 0.05 was also used
to assist in training. Our next step will be to explain how the
rewards or states were modiﬁed with the presence of ﬁrewalls,
speciﬁcally based on the type of protocol.

A. Firewalls as Obstacles

We now consider ﬁrewall as a cyber terrain obstacles. Conti
and Raymond categorize obstacles as physical or virtual ca-
pabilities that ﬁlter, disrupt, or block trafﬁc between networks
using different methods [10]. For the purposes of this work, we
consider ﬁrewalls as blocking obstacles and use the presented
methodology to incorporate cyber terrain into a CVSS-based
attack graph.

1) Adding via Reward: We engineer the reward to incen-
tivize realistic attack campaigns using a term k such that the
reward in state s after taking action a becomes

R(s, a) = R(s, a) + k(s).

The term k decrements the reward to incentivize avoiding
ﬁrewalls. The value of k is dependent on the protocol, i.e.,

k(s) =






if no ﬁrewall

0
0.8w if FTP
0.6w if SMTP
0.4w if HTTP
0.2w if SSH

where w ≤ 0 is a parameter for tuning the strength of
incentivization. That is, we vary the change in reward based
on the security of the communication protocol. Note, when
multiple protocols are blocked, their k values are averaged

together. The various k values, dependent on protocol, are
set in our paradigm using the criticality of each service to
a ﬁrewall in a red-teaming exercise. For example, FTP is
given a k multiplier value of 0.8, whereas SSH is given a
k multiplier value of 0.2. Penetrating a host containing an
FTP-based ﬁrewall along its path projections is likely to be
harder than that of SSH, so we incentivize avoiding a ﬁrewall
containing FTP more than that of SSH.

Consider the ﬂow from the left to the bottom right of Figure
2. To create the reward-adjusted MDP given an attack graph,
ﬁrst the CVSS MDP is constructed. Reward is assigned to the
nodes using vulnerability scores and transition probabilities are
assigned to the edges using exploitability scores as either 0.9,
0.6, or 0.3. This is depicted by the ﬁrst step from the attack
graph to the lower right steps in Figure 2. Then, this CVSS
MDP has its rewards modiﬁed corresponding to the presence
of ﬁrewalls as depicted by the different reward values in the
nodes in the ﬁnal step of the lower right of Figure 2, resulting
in the reward-adjusted MDP.

2) Adding via State: Alternatively, we introduce realism by
engineering the state transition probabilities. We use two terms
k1(s) and k2(s) such that the state transition probabilities
become

P (s, a, s(cid:48)) = P (s, a, s(cid:48)) ∗ k1(s(cid:48)) ∗ k2(s(cid:48)).

The term k1 corresponds to ﬁrewall presence and k2 to the
importance of the ﬁrewall. They are deﬁned as follows.

k1(s) =

(cid:40)

0.01 if ﬁrewall
1.0
else

k2(s) =






1.0
0.2
0.4
0.6
0.8

if no ﬁrewall
if FTP
if SMTP
if HTTP
if SSH

Recall, P (s, a, s(cid:48)) is initialized using the low, medium, and
high CVSS attack complexity classes. Note that, k1 introduces
an emphasis on avoiding ﬁrewalls and k2 counterbalances that
emphasis for high-value targets. Note, when multiple protocols
are blocked, their k2 values are averaged together.

Consider the ﬂow from the left to the upper right of Figure
2. To create the state-adjusted MDP given an attack graph,
ﬁrst the transition probabilities given by exploitability scores
are modiﬁed corresponding to the presence of ﬁrewalls. This
is depicted by the ﬁrst step from the attack graph to the upper
right steps in Figure 2 where the attack graph has transition
probabilities on its edges but no reward. Then, reward is
assigned using the vulnerability score, resulting in the state-
adjusted MDP, shown in the ﬁnal step of the upper right of
Figure 2.

IV. RESULTS

We now compare the performance of DQN across (1)
the vanilla, terrain-blind CVSS MDP, (2) the reward-adjusted
MDP, enhanced via R, and (3) the state-adjusted MDP, en-
hanced via state transition probabilities P (s, a, s(cid:48)). We use a
122 host network whose attack graph has 955 vertices and

MDP

“Vanilla”

via R via P (s, a, s(cid:48))

V. REMARKS

Total Number of Hops
Total Reward

62
221

91
179

85
237

TABLE II: Total number of hops and total reward with all
protocols available.

2350 edges. All presented results use w = −2. The top-
line results are shown in Table II. The introduction of terrain
increases the number of hops, as agents must now navigate
around ﬁrewalls.

We can compare the reward as well. Note the reward
functions are identical between the vanilla MDP and the state-
adjusted MDP, but are different between the vanilla MDP
and reward-adjusted MDP. The w parameter for adjusting R
decreases reward, and so we expect to see a lower reward.
Notably, we see a decrease in reward despite taking almost 30
more hops. Again, this simply conﬁrms the reward has been
decremented.

Similar comparisons between the vanilla MDP and state-
adjusted MDP, we see a greater reward, as expected due to the
larger number of hops. Whereas the agent averages 3.6 units
of reward per hop on the vanilla MDP, the agent averages 2.8
units of reward per hop on the state-adjusted MDP. Recalling
that reward for approaching the terminal state is linearly scaled
using a depth ﬁrst search from the initial state to terminal
state, the maintenance of a high average reward suggests the
RL agent can still make steady progress to the terminal state
while accounting for obstacles.

A closer look at the results is shown in Figure 3. The
plots show the average reward achieved by the DQN agent
against the number of training episodes. The total reward was
evaluated every 4 episodes and each episode had a maximum
length of 2500 steps. The high average reward values achieved
after 80 episodes signify that the agents spend a majority
of their time close to the terminal state. The vanilla MDP
is protocol agnostic. While Table II shows state-adjusted and
reward-adjusted total reward when agents can choose between
protocols, in Figure 3, the state-adjusted and reward-adjusted
plots show average reward when the agent is restricted to a
single choice of protocol. The plots show our method was able
to represent that FTP is a more signiﬁcant cyber obstacle than
SSH.

Lastly, Figure 4 shows the paths derived from the approx-
imated policies. The top ﬁgure shows the vanilla path, the
middle ﬁgure show the state-adjusted path, and the bottom
ﬁgure shows the reward-adjusted paths. The paths have been
greatly reduced by focusing on key nodes along the path.
The red edges highlight differences in the path taken from
the initial to terminal state. At node 681, a ﬁrewall existed
that led to the agents using state-adjusted and reward-adjusted
MDPs to seek an alternate path. Their paths differentiate after
node 136.

The presented reward- and state-adjusted MDP construction
methods build on the existing practice of constructing MDPs
using the CVSS. Importantly,
the presented methodology
maintains the scalability of CVSS MDPs while folding in
richer aspects of network and path structures. Historically, con-
structing attack graphs was largely a manual process [1]. Cyber
terrain was included by operators manually in these detailed,
tedious, and unscalable MDPs. This tradition has continued in
the construction of MDPs for RL for penetration testing [2]–
[5]. The use of MulVal to construct attack graphs marked a
broader change in philosophy, however, where scalability of
attack graph construction was valued over realism [9], [37].
This has been mirrored by the use of the CVSS to scale MDP
construction alongside attack graph construction [6]–[8]. The
use of CVSS to scale MDP construction, like the broader
shift in philosophy marked by MulVal, sacriﬁces realism. The
presented methodology offers scalable approaches for building
realism into scalable approaches to the construction of MDPs
over attack graphs by drawing on notions of cyber terrain from
intelligence preperation of the battleﬁeld.

VI. CONCLUSION

In this paper, we present methods for enhancing “vanilla”,
CVSS-based attack graphs by using concepts of cyber terrain
within intelligence preparation of the battleﬁeld. Our method
introduces cyber terrain by modifying the state transition
probabilities P (s, a, s(cid:48)) and reward function R. Using an
example attack graph with nearly 1000 nodes, we showed
how our approach can be used to introduce cyber obstacles,
particularly ﬁrewalls. We evaluated using DQN, and showed
notable differences in total reward, number of hops, average
reward, and attack campaigns.

The shift from manually constructed MDPs [2]–[5] to
CVSS-based MDPs [6]–[8] marks an emphasis on scaling the
construction of attack-graph-based MDPs. Our methodology
maintains an automated, scale-oriented approach to construct-
ing MDPs, while introducing notions of cyber terrain that help
ground RL agent behavior to reality.

Future work should consider how more elements of cyber
terrain can be folded into MDP construction. In doing so,
a primary consideration should be to continue to scale the
size of attack graphs, using more hosts at an enterprise scale.
This would help further validate the use of cyber-terrain IPB
principles in creating realistic contexts for penetration testing.
Also, methods should be developed that use multiple initial
and terminal states to assist in attack surface cartography. In
addition, the current literature considers RL agents that are
trained and deployed on the same network. Notions of transfer
learning, meta-learning, and lifelong learning are promising
paths for generalizing penetration testing agents.

REFERENCES

[1] J. P. McDermott, “Attack net penetration testing,” in Proceedings of the

2000 workshop on New security paradigms, 2001, pp. 15–21.

Fig. 3: Average reward plotted against training episode for each MDP. The middle and right plots show special cases where
the state-adjusted and reward-adjusted MDPs were restricted to a single communication protocol.

[9] L. Gallon and J. J. Bascou, “Using cvss in attack graphs,” in 2011
Sixth International Conference on Availability, Reliability and Security.
IEEE, 2011, pp. 59–66.

[10] G. Conti and D. Raymond, On cyber: towards an operational art for

cyber conﬂict. Kopidion Press, 2018.

[11] T. C. Purcell, “Operational level intelligence: Intelligence preparation
of the battleﬁeld,” ARMY WAR COLL CARLISLE BARRACKS PA,
Tech. Rep., 1989.

[12] H. Zimmermann, “Osi reference model-the iso model of architecture for
open systems interconnection,” IEEE Transactions on communications,
vol. 28, no. 4, pp. 425–432, 1980.

[13] S. D. Applegate, C. L. Carpenter, and D. C. West, “Searching for digital
hilltops,” Joint Force Quarterly, vol. 84, no. 1, pp. 18–23, 2017.
[14] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[15] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing atari with deep reinforcement learn-
ing,” arXiv preprint arXiv:1312.5602, 2013.

[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., “Human-level control
learning,”
nature, vol. 518, no. 7540, pp. 529–533, 2015.

through deep reinforcement

[17] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep q-
learning with model-based acceleration,” in International Conference
on Machine Learning. PMLR, 2016, pp. 2829–2838.

[18] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double q-learning,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, vol. 30, no. 1, 2016.

[19] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas,
“Dueling network architectures for deep reinforcement learning,” in
International conference on machine learning. PMLR, 2016, pp. 1995–
2003.

[20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
2017.

[21] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-
forcement learning,” in International conference on machine learning.
PMLR, 2016, pp. 1928–1937.

[22] M. Denis, C. Zena, and T. Hayajneh, “Penetration testing: Concepts,
attack methods, and defense strategies,” in 2016 IEEE Long Island
Systems, Applications and Technology Conference (LISAT). IEEE, 2016,
pp. 1–6.

[23] A. G. Bacudio, X. Yuan, B.-T. B. Chu, and M. Jones, “An overview
of penetration testing,” International Journal of Network Security & Its
Applications, vol. 3, no. 6, p. 19, 2011.

[24] S. Shah and B. M. Mehtre, “An overview of vulnerability assessment
and penetration testing techniques,” Journal of Computer Virology and
Hacking Techniques, vol. 11, no. 1, pp. 27–49, 2015.

[25] B. Chess and G. McGraw, “Static analysis for security,” IEEE security

& privacy, vol. 2, no. 6, pp. 76–79, 2004.

Fig. 4: Visualization of attack campaigns.

[2] M. C. Ghanem and T. M. Chen, “Reinforcement learning for intelligent
penetration testing,” in 2018 Second World Conference on Smart Trends
in Systems, Security and Sustainability (WorldS4).
IEEE, 2018, pp.
185–192.

[3] J. Schwartz and H. Kurniawati, “Autonomous penetration testing using
reinforcement learning,” arXiv preprint arXiv:1905.05965, 2019.
[4] M. C. Ghanem and T. M. Chen, “Reinforcement learning for efﬁcient
network penetration testing,” Information, vol. 11, no. 1, p. 6, 2020.
[5] S. Chaudhary, A. O’Brien, and S. Xu, “Automated post-breach penetra-
tion testing through reinforcement learning,” in 2020 IEEE Conference
on Communications and Network Security (CNS).
IEEE, 2020, pp. 1–2.
[6] M. Youseﬁ, N. Mtetwa, Y. Zhang, and H. Tianﬁeld, “A reinforcement
learning approach for attack graph analysis,” in 2018 17th IEEE In-
ternational Conference On Trust, Security And Privacy In Computing
And Communications/12th IEEE International Conference On Big Data
Science And Engineering (TrustCom/BigDataSE). IEEE, 2018, pp. 212–
217.

[7] A. Chowdary, D. Huang, J. S. Mahendran, D. Romo, Y. Deng, and
A. Sabur, “Autonomous security analysis and penetration testing,” 2020.
[8] Z. Hu, R. Beuran, and Y. Tan, “Automated penetration testing using
deep reinforcement learning,” in 2020 IEEE European Symposium on
Security and Privacy Workshops (EuroS&PW).
IEEE, 2020, pp. 2–10.

[26] C. P. Pﬂeeger, S. L. Pﬂeeger, and M. F. Theofanos, “A methodology for
penetration testing,” Computers & Security, vol. 8, no. 7, pp. 613–620,
1989.

[27] C. Weissman, “Penetration testing,” Information security: An integrated

collection of essays, vol. 6, pp. 269–296, 1995.

[28] C. Salter, O. S. Saydjari, B. Schneier, and J. Wallner, “Toward a secure
system engineering methodolgy,” in Proceedings of the 1998 workshop
on New security paradigms, 1998, pp. 2–10.

[29] B. Schneier, “Attack trees,” Dr. Dobb’s journal, vol. 24, no. 12, pp.

21–29, 1999.

[30] B. Duan, Y. Zhang, and D. Gu, “An easy-to-deploy penetration testing
platform,” in 2008 The 9th International Conference for Young Computer
Scientists.

IEEE, 2008, pp. 2314–2318.

[31] H. Polad, R. Puzis, and B. Shapira, “Attack graph obfuscation,” in
International Conference on Cyber Security Cryptography and Machine
Learning. Springer, 2017, pp. 269–287.

[32] Y. Steﬁnko, A. Piskozub, and R. Banakh, “Manual and automated pene-
tration testing. beneﬁts and drawbacks. modern tendency,” in 2016 13th
International Conference on Modern Problems of Radio Engineering,
Telecommunications and Computer Science (TCSET).
IEEE, 2016, pp.
488–491.

[33] D. Shmaryahu, G. Shani, J. Hoffmann, and M. Steinmetz, “Constructing
plan trees for simulated penetration testing,” in The 26th international
conference on automated planning and scheduling, vol. 121, 2016.
[34] A. Austin and L. Williams, “One technique is not enough: A comparison
of vulnerability discovery techniques,” in 2011 International Symposium
on Empirical Software Engineering and Measurement.
IEEE, 2011, pp.
97–106.

[35] C. Weissman, “Security penetration testing guideline,” Naval Research

Laboratory, Unisys Government Systems, vol. 12010, 1995.

[36] C.-K. Chen, Z.-K. Zhang, S.-H. Lee, and S. Shieh, “Penetration testing

in the iot age,” computer, vol. 51, no. 4, pp. 82–85, 2018.

[37] X. Ou, S. Govindavajhala, A. W. Appel et al., “Mulval: A logic-based
network security analyzer.” in USENIX security symposium, vol. 8.
Baltimore, MD, 2005, pp. 113–128.

