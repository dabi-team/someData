Chapter 2  
Game-Theoretic Analysis of Cyber Deception: Evidence-
Based Strategies and Dynamic Risk Mitigation 
Tao Zhang1, Linan Huang1, Jeffrey Pawlick1, and Quanyan Zhu1 

1New York University, 2 Metrotech Center, Brooklyn, 11201, USA. 

2.1  Abstract 

Deception  is  a  technique  to  mislead  human  or  computer  systems  by  manipulating  beliefs  and 
information. For the applications of cyber deception, non-cooperative games become a natural 
choice of models to capture the adversarial interactions between the players, and quantitatively 
characterizes the conflicting incentives and strategic responses. In this chapter, we provide an 
overview of deception games in three different environments and extend the baseline signaling 
game  models  to  include  evidence  through  side-channel  knowledge  acquisition  to  capture  the 
information  asymmetry,  dynamics,  and  strategic  behaviors  of  deception.  We  analyze  the 
deception in binary information space based on signaling game framework with a detector that 
gives off probabilistic evidence of the deception when the sender acts deceptively. We then focus 
on a class of continuous one-dimensional information space and take into account the cost of 
deception  in  the  signaling  game.  We  finally  explore  the  multi-stage  incomplete-information 
Bayesian game model for defensive deception for advanced persistent threats (APTs).  We use 
the perfect Bayesian Nash equilibrium (PBNE) as the solution concept for the deception games 
and analyze the strategic equilibrium behaviors for both the deceivers and the deceivees.  

2.2 Introduction 

Deception is a technique used to cause animals [9], human [13, 35] or computer systems [3] to have 
false beliefs. The purpose of deception is to mislead the deceivees to behave against their interests 
but  favorably  to  the  deceiver.  It  is  a  fundamental  type  of  interactions  that  can  be  found  in 
applications ranging from biology [9] to criminology [35] and from economics [13] to the Internet 
of Things (IoT) [31]. Cyberspace creates particular opportunities for deception, since information 
lacks  permanence,  imputing  responsibility  is  difficult  [20],  and  some  agents  lack  repeated 
interactions [24]. For instance, online interactions are  vulnerable to identify theft [14] and spear 
phishing  [1],  and  authentication  in  the  IoT  suffers  from  a  lack  of  infrastructure  and  local 
computational resources [2]. 

 
 
 
 
 
 
 
Deception can be used as an approach for attacks. For example, phishing is a  typical  deception-
based attack that is one of the top threat vectors for cyberattacks [32]. Phishing can be email-based 
in which a phisher manipulates the email to appear as a legitimate request for sensitive information 
[4,  5].  It  can  also  be  website-based  in  which  the  deceiver  uses  genuine  looking  content  to 
camouflage a legitimate website to attract target deceivees to reveal their personal data such as 
credit  card  information  and  social  security  number.  Defenders  can  also  implement  deception. 
Defenders in the security and privacy domains have proposed, e.g., honeynets [7], moving target 
defense  [39],  obfuscation  [30],  and  mix  networks  [36].  Using  these  techniques,  defenders  can 
obscure valuable data such as personally identifiable information or the configuration of a network. 
Using these approaches, defenders can send false information to attackers to waste their resources or 
distract them from critical assets. They can also obscure valuable data such as sensitive information 
or the configuration of a network to avoid direct accesses from the attackers. Both malicious and 
defensive  deception  have  innumerable  implications  for  cybersecurity.  Successful  deception 
fundamentally depends on the information asymmetry  between  the  deceiver  and  the  deceivee. 
Deceivees make indirect observations of the true state and then make decisions. Deceivers can 
take advantage of this by pretending to be a trustworthy information provider. It is possible to fool, 
mislead, or confuse the deceivees. But the deceivers need to plan their strategies and take actions 
that may be costly. Therefore, successful deception also requires the deceivers to have the ability 
to acquire information, accurately understand the goals of the deceivees, and make the induced 
actions predictable. 

The deceivers strategically manipulate the private information to suit their own self-interests. The 
manipulated information is then revealed to the deceivees, who, on the other hand, make decisions 
based on the information received. It is important for the deceivee to form correct beliefs based on 
past observations, take into account the potential damage caused by deception, and strategically use 
the observed information for decision-making. If deception is necessary to achieve the deceivers’ 
goal  that  would  cause  damages  to  the  deceivees,  the  deceivees  can  then  be  prepared  to  invest 
resources in detecting and denying the deceptions as well as recovering the damage.  

Modeling  deceptive  interactions  online  and  in  the  IoT  would  allow  government  policymakers, 
technological  entrepreneurs,  and  vendors  of  cyber-insurance  to  predict  changes  in  these 
interactions  for  the  purpose  of  legislation,  development  of  new  technology,  or  risk  mitigation. 
Game-theoretic  models  are  natural  frameworks  to  capture  the  adversarial  and  defensive 
interactions between players [11, 16, 22, 23, 37, 42, 50, 51, 52, 53]. It can provide a quantitative 
measure of the quality of protection with the concept of Nash equilibrium where both defender 
and an attacker seek optimal strategies, and no one has an incentive to deviate unilaterally from 
their equilibrium strategies despite their conflict for security objectives. The equilibrium concept 
also provides a quantitative prediction of the security outcomes of the scenario the game model 
captures.  With  the  quantitative  measures  of  security,  game  theory  makes  security  manageable 

 
 
 
beyond  the  strong  qualitative  assurances  of  cryptographic  protections.  Recently,  we  have  seen 
game-theoretic methods applied to deal with problems in cross-layer cyber-physical security [23, 
31, 40, 52, 54, 55], cyber deception [16, 27, 28, 29, 37, 53], moving target defense [39, 56, 57], 
critical infrastructure protection [19, 50, 51, 58, 59, 60, 61], adversarial machine learning [30, 62, 
63, 64, 65], insider threats [66, 67], and cyber risk management [68, 69, 70, 71]. 

This chapter shows a class of modeling of deception based on signaling games to provide a generic, 
quantitative,  and  systematic  understanding  of  deceptions  in  the  cyber-domain.  We  show  three 
variants of the model to illustrate the applications in different situations. We consider the cases when 
the deceivee is allowed to acquire knowledge through investigations or by deploying detectors. The 
baseline  signaling  game  model  is  extended  to  include  evidence  through  side-channel  knowledge 
acquisition. We also show a multi-stage Bayesian game with two-sided incomplete information and 
present a dynamic belief update and an iterative decision process that are used to develop long-
term optimal defensive policies to deter the deceivers and mitigate the loss. 

2.2.1  Related Work 

Deception game is related to a class of security games of incomplete information.  For  example, 
Powell in [34] has considered a game between an attacker and a defender, where the defender has 
private  information  about  the  vulnerability  of  their  targets  under  protection.  Powell models  the 
information  asymmetric  interactions between players by a signaling game, and finds a pooling 
equilibrium where the defender chooses to pool, i.e., allocate resources in the same way for all 
targets of different vulnerabilities, and the attacker cannot know the true level of vulnerability of 
all targets. Brown et al. [6] have studied a zero-sum game between an attacker and a defender in 
the scenario of ballistic missile positioning. They have introduced the incomplete information to 
investigate the value of secrecy by restricting the players’ access to information. 

Previous  literature  has  also  considered  deception  in  a  variety  of  scenarios  including proactive 
defense against advanced persistent threats [11,16,17,19,37], moving target defense [8, 39, 41], and 
social  engineering  [28,  29, 42].  Horák  et  al.  [16]  have  considered  a  class  of  cyber  deception 
techniques in the field of network security and studied the impact of the deception on attacker’s 
beliefs using the quantitative framework of the game theory by taking into account the sequential 
nature of the attack and investigating how attacker’s belief evolves and influences the actions of 
the players. Zhang et al., [37] have proposed an equilibrium approach to analyze the GPS spoofing 
in a model of signaling game with continuous type space. They have found a PBNE with pooling 
in low types and separating in high types, and provided an equilibrium analysis of spoofing. The 
hypothesis testing game framework in [43] has studied the influence of deceptive information on 
the decision making and analyzed the worst-case scenario by constructing equilibrium strategies.  
The model proposed by Ettinger et al. [10] has used an equilibrium approach to belief deception 

 
 
 
 
 
in  bargaining  problems  when  the  agents  only  have  coarse  information  about  their  opponent’s 
strategy.  

2.2.2  Organization of the Chapter 

The chapter is organized as follows. In Section 2.2, we briefly describe common game-theoretic 
approaches  for  security  models  and  introduce  the  basic  signaling  game  model  and  define  the 
perfect Bayesian Nash equilibrium (PBNE). In Section 2.3 we formulate the deception using a 
signaling  game  model  with  a  detector  over  a  binary  information  space,  and  describe  the 
equilibrium result. In Section 2.4, we present a signaling-game-based framework of a deception 
game to model the strategic behaviors over a continuous one-dimensional information space. We 
also consider the knowledge acquisition for the receiver through investigations. In Section 2.5, we 
show  a  multi-stage  Bayesian  game  framework  to  model  the  deception  in  advanced  persistent 
threats. Section 2.6 discusses the results and provides concluding remarks.  

2.3 Game Theory in Security 

Game theory is the systems science that studies interactions between rational and strategic players 
(or agents) that are coupled in their decision makings. Players are rational in the sense that they 
choose  actions  to  optimize  their  own  objectives  (e.g.,  utility  or  cost),  which  capture  varying 
interaction contexts. Being strategic in game theory refers to that players choose their own actions 
by anticipating the actions of the other agents. Their decision makings are coupled because their 
objective  functions  depend  both  on  their  own  actions,  and  on  the  actions  of  the  other  players. 
Among  the  game-theoretic  cybersecurity  models,  Stackelberg  game,  Nash  game,  and  signaling 
game account for the most commonly used approaches [28].  

Stackelberg games consist of a leader (L) and a follower (F). L has actions 𝑎"Î	𝐴" and receives 
utility 𝑈", and F has actions 𝑎&Î	𝐴& and receives utility 𝑈&. Once both players have taken actions, 
L  receives  utility 𝑈"(𝑎", 𝑎&) and 𝑈&(𝑎", 𝑎&).  In  Stackelberg  games,  F  acts  after  knowing  L’s 
action. Therefore, defensive cybersecurity models often take the defender as the leader and the 
attacker as the follower by considering the worst-case scenario that the attacker will observe and 
react to defensive strategies. Let 𝒫(𝐴) denote the power set of the set 𝐴. Let 𝐵𝑅&: 𝐴" → 𝒫(𝐴&) 
denote the best response of F to L’s action such that 𝐵𝑅&(𝑎") gives the optimal 𝑎& to respond to 
𝑎". Best response can be one single action or a set of equally optimal actions. The function 𝐵𝑅& is 
defined as 

𝐵𝑅& 𝑎" = 𝑎𝑟𝑔𝑚𝑎𝑥

45∈75

	𝑈& 𝑎", 𝑎& . 

 
 
 
 
 
 
 
 
By anticipating 𝐵𝑅& 𝑎" , L chooses optimal action 𝑎"

∗ which satisfies  

∗ ∈ 𝑎𝑟𝑔𝑚𝑎𝑥

𝑎"

45∈75

	𝑈" 𝑎", 𝐵𝑅& 𝑎"

. 

The  action  profile (𝑎"
𝐵𝑅& 𝑎"

∗ . 

∗, 𝑎&

∗ )  characterizes  the  equilibrium  of  the  Stackelberg  game,  where 𝑎&

∗ ∈

In  Nash  games,  on  the  other  hand,  players  commit  to  his  or  her  own  strategy  and  move 
simultaneously or before knowing the other player’s action [28]. Let H and T denote two players 
in a Nash game with actions 𝑎: 	 ∈ 	 𝐴: and 𝑎; 	 ∈ 	 𝐴;, respectively. Define 𝐵𝑅: 𝑎;  as the best 
response for H that optimally respond to T’s action 𝑎;. Similarly, let 𝐵𝑅; 𝑎:  be the best response 
for  T.  Nash  equilibrium  is  the  solution  concept  of  such  games,  which  is  defined  by  a  strategy 
profile (𝑎:

∗ ), where 

∗ , 𝑎;

∗ ∈ 	 𝐵𝑅: 𝑎; , 
𝑎:

∗ ∈ 	 𝐵𝑅; 𝑎: . 
𝑎;

In other words, Nash equilibrium requires each player to simultaneously choose a strategy that is 
optimal given the other player’s optimal strategy. In a pure-strategy Nash equilibrium, each player 
chooses one specific strategy (i.e., one pure strategy), while in a mixed-strategy equilibrium, at 
least on player randomizes over some or all pure strategies.  

A  signaling  game  is  a  two-player  dynamic  game  of  incomplete  information.  Signaling  game 
typically  names  two  players  as  sender  (S,  she)  and  receiver  (R,  he)  [12].  Signaling  game  is 
information asymmetric because the sender privately possesses some information that is unknown 
to the receiver. The private information is usually referred to as state (or type) of the world. The 
sender communicates the receiver by sending a message, and the receiver only learns about the state 
through the message. Generally, the degree of conflict of interest between S and R may range from 
perfectly aligned (e.g., Lewis signaling game [21]) to completely opposite (e.g., zero-sum game 
[33]). The timing of the game is described as follows: 

1.  Nature  randomly  draws  a  state  with  a  prior  common  to  both  players,  and  the  sender 

privately observes the state. 

2.  The sender sends a message to the receiver. 
3.  The receiver takes an action upon observing the message.  

One key feature of cyber deception is the multi-stage execution of the attack.  A deceiver has to 
engage the deceivee in multiple rounds of interactions to gain the trust. The dynamic interactions 
have been observed in APT threats and the operation of honey devices [19]. Hence signaling games 

 
 
 
 
 
 
 
 
provide a suitable description of essential features of the deception, and the basic signaling game 
models will be elaborated in the following section. 

2.3.1  Signaling Game Model 

Let 𝜃 ∈ 𝛩 denote the state privately possessed by 𝑆 that is unknown to 𝑅. The state space 𝛩 can 
be  discrete  (e.g., 𝛩? ≡ {𝜃B, 𝜃C})  or  continuous  (e.g., 𝛩E ≡ [𝜃, 𝜃]).  For  example,  the  state  could 
represent, whether the sender is a malicious or benign actor, whether she has one set of preferences 
over another, samples of data stream, and location coordinate. For simplicity but without loss of 
generality, we focus on discrete state space in this introductory description of the signaling game. 
The  state  𝜃  is  drawn  according  to  a  prior  distribution  common  to  both  players.  Harsanyi 
conceptualized the state selection as a randomized move by a non-strategic player called nature. 
Let 𝑝 denote the probability mass function of the state, where 
(𝜃) = 1. All aspects of the 
game except the value of the true state 𝜃 are common knowledge. 

I∈JK

𝑝

After  privately  observing  the  state  𝜃 ,  𝑆  chooses  a  message  𝑚 ∈ 𝑀 .  Let  𝜎 O ∈ 𝛤 O  denote  the 
behavioral  strategy  of 𝑆 to  choose 𝑚 based  on 𝜃.  She  could  use  pure  strategy 𝜎 O(𝜃) as  well  as 
mixed  strategy  𝜎 O(𝑚|𝜃) ,  such  that  𝜎 O(𝜃)  chooses  message  𝑚  given  𝜃  and  𝜎 O(𝑚|𝜃)  gives 
probability with which 𝑆 sends message 𝑚 given the state 𝜃. We assume the pure strategy 𝜎 O(𝜃) 
induces a conditional probability 𝑞O(𝑚|𝜃) ∈ [0,1]. 

After  receiving 𝑚, 𝑅 forms  a  posterior  belief µU of  the  true  state  such  that µU(θ|𝑚): Θ → [0,1] 
gives the likelihood with which R believes that the true state is θ given the message 𝑚. Based on 
the belief µU, 𝑅 then chooses an action a ∈ A according to a strategy σU ∈ ΓU. Similarly, 𝑅 may 
employ  pure  strategy σU(𝑚) or  mixed  strategy σU(𝑎|𝑚),  where σU(𝑚) yields  the  action 𝑅	acts 
upon  the  message m, and σU(𝑎|𝑚) produces  the  probability  with  which 𝑅 takes  action 𝑎 given 
message m. The action a is the final decision of 𝑅 that represents the inference about the true state. 
Let 𝑈O:  Θ×M×A → ℝ denote a utility function for 𝑆 such that 𝑈O θ, 𝑚, 𝑎  yields the utility of the 
player when her type is θ, she sends message 𝑚, and 𝑅 plays action 𝑎. Similarly, let 𝑈b:  Θ×M×A 
denote R’s utility function so that 𝑈b θ, 𝑚, 𝑎  gives his payoff under the same scenario. 

2.3.2  Perfect Bayesian Nash Equilibrium 

In  two  player  games,  Nash  equilibrium  defines  a  strategy  profile  in  which  each  player  best 
responds to the optimal strategies of the other player. Signaling games motivate the extension of 
Nash equilibrium in two ways. First, information asymmetry requires 𝑅 to maximize his expected 
utility over the possible types of 𝑆. An equilibrium in which 𝑆 and 𝑅 best respond to each other’s 

 
 
 
 
 
 
 
 
strategies given some belief 𝜇b is called a Bayesian Nash equilibrium. Furthermore, 𝑅 is required 
to update 𝜇b rationally. Perfect Bayesian Nash equilibrium (PBNE) captures this constraint and is 
described at Definition 1. 

Definition 2.1: (Perfect Bayesian Nash Equilibrium) A perfect Bayesian Nash equilibrium of a 
signaling profile (𝜎 O∗, 𝜎b∗) and a posterior belief system 𝜇b such that: 

1.  (𝐶C): ∀𝜃, 𝜎 O∗ ∈ 𝑎𝑟𝑔𝑚𝑎𝑥fg𝑈O(𝜃, 𝜎 O, 𝜎b∗),  

2.  (𝐶h): ∀𝑚 ∈ 𝑀, 𝜎b∗ ∈ 𝑎𝑟𝑔𝑚𝑎𝑥fi

I

𝜇b

(𝜃|𝑚)𝑈b(𝑚, 𝜎b, 𝜃), 

and 

3.  ( 𝐶j ):  𝜇b(𝜃|𝑚) =

k(I)fg∗(l|I)
km′ (In)fg∗(l|In)

,  if 

𝑝In

(𝜃o)𝜎 O∗(𝑚|𝜃o) > 0 ;  and  𝜇b(𝜃|𝑚)  is  any 

probability distribution on 𝛩 if 

𝑝In

(𝜃o)𝜎 O∗(𝑚|𝜃o) = 0. 

In  Definition  2.1,  𝐶C  and  𝐶h  are  the  perfection  conditions  for  𝑆  and  𝑅 ,  respectively,  that 
characterizes the sequential rationality of both players. Specifically, 𝐶C captures that 𝑆 optimally 
determines 𝜎 O∗ by taking into account the effect of 𝜎 O on 𝜎b. 𝐶h says that 𝑅 responds rationally 
to 𝑆’s strategy given his posterior belief about the state 𝜃. 𝐶j states that the posterior belief is 
updated based on Bayes’ rule. If the observation is a probability-0 event, then Bayes’ rule is not 
applicable. In this case, any posterior beliefs over the state space is admissible. 𝐶j also implies 
that the consistency between the posterior beliefs and strategies: belief is updated depending on 
the strategy that is optimal given the belief. There are three categories of strategies: separating, 
pooling,  and  partially-pooling  equilibria,  which  are  defined  based  on  the  strategy  of  𝑆 .  In 
separating PBNE (S-PBNE), 𝑆 chooses different strategies for different states. In this case, 𝑅 is 
able  to  identify  each  state  with  certainty.  In  pooling  PBNE  (P-PBNE), 𝑆 chooses  the  same 
strategy for different states. This strategy makes the corresponding message 𝑚 uninformative to 
𝑅. In partially-pooling PBNE (PP-PBNE), however, 𝑆 chooses messages with different, but not 
completely  distinguishable,  strategies  for  different  states.  This  makes  the  belief  of 𝑅 remain 
uncertain. 

2.4 Binary State Space: Leaky Deception using Signaling Game with Evidence 

In [27], Pawlick  et  al.  have  modeled  the  strategic  interactions  between  the  deceiver 𝑆 and  the 
deceivee 𝑅 over a binary information space by extending signaling games by including a detector 
that gives off probabilistic warnings called evidence when 𝑆 acts deceptively. 

 
 
 
 
 
 
 
S: 𝜎 𝑆 𝑚 𝜃)

type 𝜃
∈ {0,1}

𝜆 𝑒 𝜃, 𝑚)

message 𝑚
∈ {0,1}

evidence 𝑒
∈ {0,1}

R: 𝜎𝑅 𝑎 𝑚, 𝑒 )

action 𝑎
∈ {0,1}

Fig. 2.1: Signaling games with evidence add the red detector block to the S and 
R blocks. The probability λ e θ, 𝑚) of emitting evidence e depends on S’s type θ 
and the message m that she transmits  [27]. 

2.4.1  Game-Theoretic Model 

With reference to Fig. 2.1, the detector emits evidences based on whether the message m is equal 
to the state θ. The detector emits 𝑒 ∈ 𝐸 ≡ {0,1} by the probability λ e | θ, 𝑚 . Let e = 1 denote an 
alarm  and  e = 0  no  alarm.  The  evidence  e  is  assumed  to  be  emitted  with  an  exogeneous 
probability that neither 𝑅 nor 𝑆 can control. In this respect, the detector can be seen as a second 
move by nature. Let β ∈ [0,1] be the true-positive rate of the detector. For simplicity, both true-
positive rates are set to be equal: β = λ(1 | 0,1) = λ(1 | 1,0). Similarly, let α ∈ [0,1] denote the 
false-positive  rate  of  the  detector  with α = λ(1 | 0,0) = λ(1 | 1,1). A  valid  detector  has β ≥ α. 
This is without loss of generality, because otherwise α and β can be relabeled. The timing of the 
game becomes: 

1.  Nature randomly draws state θ ∈ {0,1} according to p(θ).  

2.   𝑆 privately observes θ and then chooses a message m based on strategy σy(𝑚|θ).  

3.  The detector emits evidence e ∈ {0,1} with λ(e|θ, 𝑚).  

4.  After  receiving  both 𝑚 and 𝑒, 𝑅 forms  a  belief  system 𝜇b(𝜃|𝑚, 𝑒) and  then  chooses  an 

action 𝑎 ∈ {0,1} according to strategy 𝜎b(𝑎|𝑚, 𝑒). 

The following assumptions characterizes a cheap-talk signaling game with evidence. The message 
𝑚 is payoff-irrelevant in a cheap-talk signaling game. 

Assumption 2.1: The utilities 𝑈O and 𝑈b satisfy the following assumptions: 

1.  𝑈O and 𝑈b do not depend exogenously on 𝑚.  

 
 
 
 
 
 
2.  ∀𝑚, 𝑚 ∈ 𝑀, 𝑈b 0, 𝑚, 0 > 𝑈b 0, 𝑚 , 1 .  

3.  ∀𝑚, 𝑚 ∈ 𝑀, 𝑈b 1, 𝑚, 0 < 𝑈b 1, 𝑚 , 1 .  

4.  ∀𝑚, 𝑚 ∈ 𝑀, 𝑈O 0, 𝑚, 0 < 𝑈O 0, 𝑚 , 1 .  

5.  ∀𝑚, 𝑚 ∈ 𝑀, 𝑢O 1, 𝑚, 0 > 𝑢O 1, 𝑚 , 1 . 

Assumption 2.1-1 implies that the interaction is a cheap-talk signaling game. Assumption 2.1-2 
and 2.1-3 state that 𝑅 receives higher utility if he correctly chooses 𝑎 = 𝜃 than if he chooses 𝑎 ≠
𝜃. Finally, Assumption 2.1-4 and 2.1-5 say that 𝑆 receives higher utility if 𝑅 chooses 𝑎 ≠ 𝜃 than 
if he chooses 𝑎 = 𝜃. 

: 𝛤×𝛤b → ℝ such  that 𝑈(𝜎 O, 𝜎b|𝜃) gives  the 
Utilities.  Define  an  expected  utility  function 𝑈
expected utility to 𝑆 when she plays strategy 𝜎 O given that the state is 𝜃. This expected utility is 
given by 

O

O

𝑈

𝜎 O, 𝜎b | 𝜃 =

𝜎b 𝑎 | 𝑚, 𝑒 𝜆 𝑒 | 𝜃, 𝑚 𝜎 O 𝑚 | 𝜃 𝑈O 𝜃, 𝑚, 𝑎

.														(2.1)

4∈7

(cid:127)∈(cid:128)

l∈~

The involvement of evidence 𝑒 in Eq. (2.1) is due to the dependence of 𝑅’s strategy 𝜎b(𝑎|𝑚, 𝑒). 
𝑆 must  anticipate  the  probability  of  leaking  evidence 𝑒 by  using 𝜆(𝑒|𝜃, 𝑚).  Similarly,  define 

b

b

: 𝛤 → ℝ such  that 𝑈

𝑈
given message 𝑚, evidence 𝑒, and state 𝜃. The expected utility function is given by 

(𝜎b|𝜃, 𝑚, 𝑒) gives  the  expected  utility  to 𝑅 when  he  plays  strategy 𝜎b 

b

𝑈

(𝜎b | 𝜃, 𝑚, 𝑒) =

𝜎b

𝑎 | 𝑚, 𝑒 𝑈b 𝜃, 𝑚, 𝑎 . 

4∈7

2.4.2  Equilibrium Concept 

The involvement of the evidence extends the PBNE in Definition 2.2 as follows.  

Definition 2.2: (PBNE with Evidence) A PBNE of a cheap-talk signaling game with evidence is 
a strategy profile (𝜎 O∗, 𝜎b∗) and posterior beliefs 𝜇b(𝜃 | 𝑚, 𝑒) such that 

∀𝜃 ∈ 𝛩,	 𝜎 O∗ ∈ 𝑎𝑟𝑔𝑚𝑎𝑥

 𝑈

O

𝜎 O, 𝜎b∗ | 𝜃 ,																		(2.2) 

fg∈(cid:130)g

∀𝑚 ∈ 𝑀,	 ∀𝑒 ∈ 𝐸, 

 
 
 
 
 
𝜎b∗ ∈ 𝑎𝑟𝑔𝑚𝑎𝑥

fi∈(cid:130)i

I∈J

𝜇b

𝜃 | 𝑚, 𝑒 𝑈

b

𝜎b   𝜃, 𝑚, 𝑒 ,			(2.3) 

and if 

I∈J (𝑒 | 𝜃 , 𝑚)𝜎 O(𝑚 | 𝜃)𝑝(𝜃) > 0, then 

𝜆

𝜇b 𝜃 | 𝑚, 𝑒 =

𝜆 𝑒 | 𝜃, 𝑚 𝜇b 𝜃 | 𝑚

𝜆

𝑒 | 𝜃 , 𝑚 𝜇b 𝜃  | 𝑚

I∈J

,												(2.4) 

where 

𝜇b 𝜃 | 𝑚 =

𝜎 O 𝑚 | 𝜃 𝑝 𝜃

.																											(2.5) 

𝑚 | 𝜃 𝑝 𝜃
I∈J (𝑒 | 𝜃 , 𝑚)𝜎 O(𝑚 | 𝜃)𝑝(𝜃) = 0,  then  𝜇b 𝜃 | 𝑚, 𝑒  may  be  set  to  any  probability 

I∈J

𝜎O

If 
𝜆
distribution over 𝛩. 

Eq. (2.4)-(2.5) extend 𝐶j in Definition 2.2. First, 𝑅 updates her belief according to 𝑚 using Eq. 
(2.5); then 𝑅 updates her belief according to 𝑒 using Eq. (2.4). When 𝑆 plays pooling strategy, i.e., 
∀𝑚 ∈ 𝑀, 𝜎 O(𝑚|0) = 𝜎 O(𝑚|1). In this case, the message 𝑚 is uninformative, and 𝑅 updates his 
belief only depends on the evidence 𝑒, i.e., 

𝜇b 𝜃 | 𝑚, 𝑒 =

𝜆 𝑒 | 𝜃, 𝑚 𝑝 𝜃

𝜆

𝑒 | 𝜃 , 𝑚 𝑝 𝜃

I∈J

.																						(2.6) 

2.4.3  Equilibrium Results 

Under  Assumption  2.1-1  to  2.1-5,  the  cheap  talk  signaling  game  with  evidence  admits  no 
separating PBNE. This results from the opposing utility functions of S and R. S wants to deceive 
R, and R wants to correctly guess the type. It is not incentive-compatible for S to fully reveal the 
type by choosing a separating strategy. For brevity, define the following notations: 

U ≜ uU θ = 0, 𝑚, 𝑎 = 0 − uU θ = 0, 𝑚, 𝑎 = 1 , 
ΔB

U ≜ uU θ = 1, 𝑚, 𝑎 = 1 − uU θ = 1, 𝑚, 𝑎 = 0 . 
ΔC

U gives the benefit to R for correctly guessing the type when θ = 0, and ΔC
U gives the benefit to 
ΔB
R for correctly guessing the type when θ = 1. Lemmas 2.1-2.2 solve for σU∗ within five regimes 
of the prior probability p(θ) of each type θ ∈ {0,1}. 

 
 
 
 
 
𝛽 < 1 − 𝛼
(Conservative)

𝑅
𝛼Δ0
𝑅
𝑅 + 𝛽Δ1

𝛼Δ0

𝑅
(1 − 𝛽)Δ0

𝑅
(1 − 𝛼)Δ0

(1 − 𝛽)Δ0

𝑅
𝑅 + 1 − 𝛼 Δ1

(1 − 𝛼)Δ0

𝑅
𝑅 + (1 − 𝛽)Δ1

𝑅
𝛽Δ0
𝑅
𝑅 + 𝛼Δ1

𝛽Δ0

Zero-Dominant

Zero-Heavy

Middle

One-Heavy

One-Dominant

𝑝(1) = 0

𝛽 > 1 − 𝛼
(Aggressive)

𝑅
(1 − 𝛽)Δ0

1 − 𝛽 Δ0

𝑅
𝑅 + 1 − 𝛼 Δ1

𝑅
𝛼Δ0
𝑅
𝑅 + 𝛽Δ1

𝛼Δ0

𝑅
𝛽Δ0
𝑅
𝑅 + 𝛼Δ1

𝛽Δ0

𝑅
(1 − 𝛼)Δ0

1 − 𝛼 Δ0

𝑅
𝑅 + 1 − 𝛽 Δ1

𝑝(1) = 1

Fig.  2.2:  PBNE  differ  within  five  prior  probability  regimes.  In  the  Zero-

Dominant  regime,  p θ = 1 ≈ 0  i.e.,  type  0  dominates.  In  the  Zero-Heavy 
regime, p(θ = 1) is slightly higher, but still low. In the Middle regime, the types 
are mixed almost evenly. The One- Heavy regime has a higher p θ = 1 , and 

the  One-Dominant  regime  has  p θ = 1 ≈ 1.  The  definitions  of  the  regime 
boundaries depend on whether the detector is conservative or aggressive. 

Lemma 2.1: For pooling PBNE, 𝑅’s optimal actions 𝜎b∗ for evidence 𝑒 and messages 𝑚 on the 
equilibrium path1 vary within five regimes of 𝑝(𝜃). The top half of Fig. 2.2 lists the boundaries 
of  these  regimes  for  detectors  in  which 𝛽 < 1 − 𝛼, and  the  bottom  half  of  Fig.  2.2  lists  the 
boundaries of these regimes for detectors in which 𝛽 > 1 − 𝛼. 

The  regimes  in  Fig.  2.2  shift  towards  the  right  as ΔB
necessary to balance out the benefit to R for correctly identifying a type θ = 0 as ΔB
The regimes shift towards the left as ΔC

U  increases.  Intuitively,  a  higher p(1) is 
U increases. 

U increases for the opposite reason. 

Lemma 2.2 gives the optimal strategies of R in response to pooling behavior within each of the 
five parameter regimes. 

Lemma 2.2: For each regime, 𝜎b∗ on the equilibrium path is listed in Table 2.1 if 𝛽 < 1 − 𝛼 
and in Table 2.2 if 𝛽 > 1 − 𝛼. The row labels correspond to the Zero-Dominant (O-D), Zero-
Heavy (0-H), Middle, One-Heavy (1-H), and One-Dominant (1-D) regimes. 

Table 2.1:  𝜎b∗(1 |𝑚, 𝑒) in Pooling PBNE with 𝛽 > 1 − 𝛼. 

𝜎b∗(1 | 0,0)  𝜎b∗(1 | 0,1)  𝜎b∗(1 | 1,0)  𝜎b∗(1 | 1,1) 

0-D 

0 

0 

0 

0 

1 In pooling PBNE, the message “on the equilibrium path” is the one that is sent by both types of 
𝑆. Messages “off the equilibrium path” are never sent in equilibrium, although determining the 
actions that 𝑅 would play if 𝑆 were to transmit a message off the path is necessary in order to 
determine the existence of equilibria. 

 
 
 
 
 
 
 
 
 
																																																								
0-H 

Middle 

1-H 

1-D 

0 

0 

1 

1 

1 

1 

1 

1 

0 

1 

1 

1 

0 

0 

0 

1 

Table 2.2: 𝜎b∗(1 |𝑚, 𝑒) in Pooling PBNE with 𝛽 > 1 − 𝛼. 

𝜎b∗(1 | 0,0)  𝜎b∗(1 | 0,1) 

𝜎b∗(1 | 1,0)  𝜎b∗(1 | 1,1) 

0-D 

0-H 

Middle 

1-H 

1-D 

0 

0 

0 

0 

1 

0 

0 

1 

1 

1 

0 

1 

1 

1 

1 

0 

0 

0 

1 

1 

Lemmas 2.3-2.4 give conditions under which the beliefs 𝜇b exist such that each pooling strategy 
is optimal for both types of 𝑆. 

Lemma  2.3:  Let 𝑚 be  the  message  on  the  equilibrium  path.  If 𝜎b∗(1 | 𝑚, 0) = 𝜎b∗(1 | 𝑚, 1), 
then there exists a 𝜇b such that pooling on message 𝑚 is optimal for both types of 𝑆. For brevity, 
let 𝑎∗ ≜ 𝜎b∗(1 | 𝑚, 0) = 𝜎b∗(1 | 𝑚, 1). Then 𝜇b is given by, 

∀𝑒 ∈ 𝐸,	 𝜇b 𝜃 = 𝑎∗ | 1 − 𝑚, 𝑒 ≥

b
𝛥C(cid:144)4∗

b
𝛥C(cid:144)4∗

b . 
+ 𝛥4∗

Lemma 2.4: If 𝜎b∗(1 | 𝑚, 0) = 1 − 𝜎b∗(1 | 𝑚, 1) and 𝛽 ≠ 1 − 𝛼, then there does not exist a 𝜇b 
such that pooling on message 𝑚 is optimal for both types of 𝑆. 

𝛽 < 1 − 𝛼
(Conservative)

S Pool on 0 (cid:198) R Plays 0
S Pool on 1 (cid:198) R Plays 0

–
S Pool on 1 (cid:198) R Plays 0

Zero-Dominant

Zero-Heavy

𝑝(1) = 0

𝛽 > 1 − 𝛼
(Aggressive)

S Pool on 0 (cid:198) R Plays 0
S Pool on 1 (cid:198) R Plays 0

S Pool on 0 (cid:198) R Plays 0
–

–
–

Middle

–
–

S Pool on 0 (cid:198) R Plays 1
–

S Pool on 0 (cid:198) R Plays 1
S Pool on 1 (cid:198) R Plays 1

One-Heavy

One-Dominant

–
S Pool on 1 (cid:198) R Plays 1

S Pool on 0 (cid:198) R Plays 1
S Pool on 1 (cid:198) R Plays 1

𝑝(1) = 1

Fig. 2.3: PBNE  in  each  of  the  parameter  regions  defined  in  Fig. 2.2.  For m ∈

{0,1},  S  Pool  on m”Pool  on  ach	σ

O∗

𝑚 0 =	 σ

O∗

plays  a”plays  n  ach σ

b∗

𝑎 0,0 =	 σ

b∗

𝑎 0,1 = σ

𝑚 1 = 1.  For 	𝑎 ∈ {0,1},  R 
b∗
b∗

𝑎 1,0 =	 σ

𝑎 1,1 = 1. 

 
 
 
 
 
 
 
 
b∗

. The Dominant regimes support pooling PBNE on both 
Lemma 2.3 gives μ
messages. The Heavy regimes support pooling PBNE on only one message. The 
Middle regime does not support any pooling PBNE. 

The pooling PBNE of the cheap-talk signaling game with evidence are summarized by Fig. 2.3. 
For β ≠ 1 − α, the Middle regime does not admit any pooling PBNE. This result is not found in 
conventional  signaling  games  for  deception,  in  which  all  regimes  support  pooling  PBNE.  It 
occurs  because R’s  responses  to  message m depends  on e, i.e., σU∗(1 | 0,0) = 1 − σU∗(1 | 0,1) 
and σU∗(1 | 1,0) = 1 − σU∗(1 | 1,1). One of the types of S prefers to deviate to the message off 
the equilibrium path. Intuitively, for a conservative detector, S with type θ = 𝑚 prefers to deviate 
to message 1 − m, because his deception is unlikely to be detected. On the other hand, for an 
aggressive  detector, S with  type θ = 1 − 𝑚 prefers  to  deviate  to  message 1 − 𝑚, because  his 
honesty is likely to produce a false-positive alarm, which will lead R to guess 𝑎 = 𝑚. 

For 𝛽 ≠ 1 − 𝛼, since the Middle regime does not support pooling PBNE, we search for partially-
separating PBNE. In these PBNE, 𝑆 and 𝑅 play mixed strategies. In mixed-strategy equilibria in 
general, each player chooses a mixed strategy that makes the other players indifferent between 
the actions that they play with positive probability. Theorems 2.1-2.2 give the results. 

Theorem 2.1: (Partially-Separating PBNE for Conservative Detectors) For 𝛽 < 1 − 𝛼, within 
the Middle Regime, there exists an equilibrium in which the sender strategies are 

𝜎 O∗ 𝑚 = 1 | 𝜃 = 0 =

𝜎 O∗ 𝑚 = 1 | 𝜃 = 1 =

𝛽h
𝛽h − 𝛼h −
b
𝛼𝛽𝛥B
b
𝛽h − 𝛼h 𝛥C

b
𝛼𝛽𝛥C
b
𝛽h − 𝛼h 𝛥B
1 − 𝑝(1)
𝑝(1)

𝑝(1)
1 − 𝑝(1)

,

−

𝛼h
𝛽h − 𝛼h ,

the receiver strategies are 

𝜎b∗(𝑎 = 1 | 𝑚 = 0, 𝑒 = 0) = 

1 − 𝛼 − 𝛽
2 − 𝛼 − 𝛽

,

𝜎b∗(𝑎 = 1 | 𝑚 = 0, 𝑒 = 1) =  1,

𝜎b∗(𝑎 = 1 | 𝑚 = 1, 𝑒 = 0) = 

1
2 − 𝑎 − 𝑏

,

𝜎b∗(𝑎 = 1 | 𝑚 = 1, 𝑒 = 1) =  0,

and the beliefs are computed by Bayes’ Law in all cases. 

Theorem 2.2: (Partially-Separating PBNE for Aggressive Detectors) For any 𝑔 ∈ [0,1], let 𝑔 ≜

 
 
 
 
 
 
 
 
1 − 𝑔. For 𝛽 > 1 − 𝛼, within the Middle Regime, there exists an equilibrium in which the sender 
strategies are 

𝜎 O∗ 𝑚 = 1 | 𝜃 = 0 =

𝜎 O∗ 𝑚 = 1 | 𝜃 = 1 =

b
𝛼𝛽𝛥C
h
𝛼h − 𝛽
𝛼h
𝛼h − 𝛽

h −

𝑝(1)
1 − 𝑝(1)

b
𝛥B

b
𝛼𝛽𝛥B
h
𝛼h − 𝛽

b
𝛥C

−

h

𝛽
𝛼h − 𝛽

h ,

1 − 𝑝(1)
𝑝(1)

,

the receiver strategies are 

𝜎b∗(𝑎 = 1 | 𝑚 = 0, 𝑒 = 0) =  0,

𝜎b∗(𝑎 = 1 | 𝑚 = 0, 𝑒 = 1) = 

1
𝛼 + 𝛽

,

𝜎b∗(𝑎 = 1 | 𝑚 = 1, 𝑒 = 0) =  1,

𝜎b∗(𝑎 = 1 | 𝑚 = 1, 𝑒 = 1) = 

𝛼 + 𝛽 − 1
𝛼 + 𝛽

,

and the beliefs are computed by Bayes’ Law in all cases. 

In  Theorem  2.1, 𝑆 chooses  the 𝜎 O∗ that  makes 𝑅 indifferent  between 𝑎 = 0 and 𝑎 = 1 when  he 
observes the pairs (𝑚 = 0, 𝑒 = 0) and (𝑚 = 1, 𝑒 = 0). This allows 𝑅 to choose mixed strategies 
for 𝜎b∗(1 | 0,0) and 𝜎b∗(1 | 1,0). Similarly, 𝑅  chooses 𝜎b∗(1 | 0,0) and 𝜎b∗(1 | 1,0) that  make 
both  types  of 𝑆 indifferent  between  sending 𝑚 = 0 and 𝑚 = 1. This  allows 𝑆 to  choose  mixed 
strategies. A similar pattern follows in Theorem 2.2 for 𝜎 O∗, 𝜎b∗(1 | 0,1), and 𝜎b∗(1 | 1,1). 

2.5 Continuous State Space: Knowledge Acquisition and Fundamental Limits of Deception 

In [38], Zhang et al. proposes a game-theoretic framework of a deception game to model the strategic 
behaviors of the deceiver S and the deceivee R and construct strategies for both attacks and defenses 
over a continuous one-dimensional state space. R is allowed to acquire probabilistic evidence about 
the deception through investigations, and misrepresenting the state is costly for S. The deceivability 
of the deception game is analyzed by characterizing the PBNE. 

2.5.1  Game-Theoretic Model 

We assume that the state θ is continuously distributed over Θ ≡ [𝜃, θ] according to a differentiable 
probability distribution F(θ), with strictly positive density f(θ) for all θ ∈ Θ. Again, all aspects of 
the game except the value of the true state θ are common knowledge. 

 
 
 
 
 
 
 
Message  and  Report.  In  this  game  model,  we  use  the  message  to  describe  the  format  of 
information about the state S communicates to R. We introduce a notion report to represent the 
value of state carried by the message. After privately observing the state θ, S first determines a 
report r ∈ Θ for the true state θ, and then sends R a message 𝑚 ∈ M, where M is a Borel space of 
messages. Let Ω: M → Θ denote the report interpretation function such that Ω(𝑚) gives the report 
r carried in 𝑚. Given the true state θ, we say m tells the truth if Ω(𝑚) = θ. We assume that for 
each state θ ∈ Θ, there is a sufficiently large number of messages that yields the same report, and 
each 𝑚 ∈ M has  a  unique  value  of  report Ω(𝑚).  In  other  words,  the  message  space  can  be 
partitioned  as M =∪(cid:152) M(cid:152) ,  with |M(cid:152)| → ∞ for  all r and M(cid:152) ∩ M(cid:152)o = ∅ if r ≠ r′,  and ∀𝑚 ∈ M(cid:152) , 
Ω(𝑚) = r. This assumption can capture the feature of rich language in practical deceptions. We 
further assume that message 𝑚 is formed by “common language” that can be understood precisely 
by both S and R. In other words, function Ω is commonly known by both players. 

Strategies  and  actions.  Let σy: Θ → Θ be  the  strategy  of S such  that r = σy(θ) determines  the 
report r of the true state θ. Let ηy: Θ×Θ → M be the message strategy of S associated with σy such 
that 𝑚 = ηy(r) selects  the  message 𝑚 from M(cid:152) when  the  strategy σy(θ) determines  the  report r 
and the true state is θ. Given θ, the strategy σy(θ) determines the set of messages M(cid:158)(cid:159)((cid:160)) for ηy to 
choose from, and ηy determines which specific message 𝑚 ∈ M(cid:158)(cid:159)((cid:160)) to send. We assume σy(θ) 
associated  with ηy induces  a  conditional  probability qy(𝑚|θ).  After  receiving m, R chooses  an 
action  a ∈ A ≡ Θ  according  to  a  strategy  σU: Θ×M → A  using  r = Ω(𝑚) .  σU(r, 𝑚)  gives  the 
action R acts upon the message m (and thus r = Ω(𝑚)). The action a is the final decision of R that 
represents the inference about the true state. 

Utilities.  The  utility  functions  of  S  is  given  by  Uy(𝑎, θ, r) ≡ U£(a, θ) − kU¥(r, θ)  where 
U£(𝑎, θ) ≡ −(𝑎 − (θ + b))h is  the  utility  depending  on  the  induced  action a in R, U¥ ≡ −(r −
θ)h is the utility related to the misrepresentation of the true state, and k ≥ 0 quantifies the intensity 
of U¥. On the deceivee’s side, his utility is given by UU ≡ −(𝑎 − θ)h, which takes into account 
the  risk  induced  by R’s  misinference  of  the  true  state θ via  his  action 𝑎.  Define,  for  all θ ∈ Θ, 
αy(θ) ≡ argmin4Cy(𝑎, θ, r),  and  αU(θ) ≡ argmin4CU(𝑎, θ);  i.e.,  αU(θ)  and  αy(θ)  are  two 
actions taken by R as functions of θ that are the most preferred by R and S, respectively. 

Beliefs.  Based  on m (and  thus r = Ω(𝑚))  and  his  prior  belief f(θ), R forms  a  posterior  belief 
µU: Θ → [0,1] of  the  true  state θ ∈ Θ.  The  posterior  belief µU(θ|𝑚) gives  the  likelihood  with 
which R believes  that  the  true  state  is θ based  on m. R then  determines  which  action  to  choose 
based on his belief µU. 

2.5.2  Deceivability 

 
 
 
 
 
We  restrict  attention  to  a  class  of  monotone  inflated  deception,  in  which  the  strategy  profile 
(σy, σU) satisfies conditions in the following definition. 

Definition 2.3: A deception with 𝑆’s strategy 𝜎 O and 𝑅’s belief 𝜇b is monotone if 

1.  𝜎 O(𝜃) is a non-decreasing function of 𝜃; 

2.  𝜎b(𝑟, 𝑚) is a non-decreasing function of 𝑟. 

The deceivability can be quantified as follows. 

Definition  2.4:  Given  the  state  𝜃 ∈ [𝜃‹, 𝜃o] ,  𝑆 ’s  strategy  𝜎 O(𝜃) = 𝑟 ,  and  message  strategy 
𝜂O(𝑟) = 𝑚, 

–  𝑅  is  undeceivable  over  [𝜃‹, 𝜃o]  if  𝜎b(𝑟, 𝑚) = 𝛼b(𝜃) ≡ 𝜃 ,  for  all  𝜃 ∈ [𝜃‹, 𝜃,] .  Here, 
𝜎 O(𝜃) ≠ 𝜎 O(𝜃′)  for  all  𝜃 ≠ 𝜃′∈ [𝜃‹, 𝜃o] ,  and  𝜂O(𝑟) ∈ 𝑀ﬁ .  The  corresponding  𝜇b  is 
informative. The interval [𝜃‹, 𝜃o] is called undeceivable region (UR). 

–  𝑅 is  deceivable  over [𝜃‹, 𝜃o] if  the  only  knowledge  she  has  is  that 𝜃 lies  in [𝜃‹, 𝜃o]. 𝑅 

chooses 𝜎b(𝑟, 𝑚) = 𝑎

b

(𝜃‹, 𝜃o), by maximize the expected utility over [𝜃‹, 𝜃o], i.e., 

b

𝑎

(𝜃‹, 𝜃o) ∈ argmax
fi∈7

I(cid:176)

In

𝑈b

(𝜎b, 𝜃)𝑓(𝜃)𝑑𝜃. 

Here, 𝜎 O(𝜃) and 𝜂O(𝑟), respectively, choose the same report 𝑟 and the same message 𝑚, 
for all 𝜃 ∈ [𝜃‹, 𝜃o]. Thus, given 𝑚, 𝑞O(𝑚|𝜃) is the same for all 𝜃 ∈ [𝜃‹, 𝜃o], where 𝑞 ∈
(0,1). The corresponding 𝜇b is uninformative. The interval [𝜃‹, 𝜃o] is called deceivable 
region (DR). 

2.5.3  Knowledge Acquisition: Evidence 

We  allow 𝑅 to  acquire  additional  knowledge  through  investigations  when  the  state  is  in  a  DR, 
[𝜃‹, 𝜃o],  by  partitioning  it  into  multiple  intervals,  denoted  by  a  strictly  increasing  sequence, <
𝜃B = 𝜃‹, 𝜃C, … , 𝜃· = 𝜃o >.  Then 𝑅 conducts  investigations  for  each  interval.  Here,  we  consider 
the case when there are two investigation intervals to simplify the analysis. Let 𝜎(cid:181) ∈ (𝜃‹, 𝜃o) be 
the  investigation  partition  state  such  that  [𝜃‹, 𝜃o]  is  partitioned  into  two  non-overlapping 
investigation  regions 𝛩B = [𝜃‹, 𝜃(cid:181)] and 𝛩C = [𝜃(cid:181), 𝜃o].  Let 𝛹 ∈ 𝛤 = {𝛹B, 𝛹C},  where 𝛹• denote 

the event {𝜃 ∈ 𝛩•}, for 𝑖 = 0, 1, with the probability 𝑃(𝛹•) =

”

(I)‰I

»…

[m(cid:176),mn] (I)‰I
”

. The investigation for 

𝛩B  and 𝛩C  generates  noisy  evidence 𝑒 ∈ 𝐸 = {0,1},  where 𝑒 = 𝑖  represents 𝛹• ,  for 𝑖 = 0, 1. 

 
 
 
 
 
 
Suppose  that  the  investigation  emits  evidence  by  the  probability  𝛾(𝑒|𝛹, 𝑚) .  Let  𝑥 = 𝛾(𝑒 =
0|𝛹B, 𝑚) and 𝑦 = 𝛾(𝑒 = 1|𝛹C, 𝑚) be the two true positive rates, which are private information 
of 𝑅. 

With a slight abuse of notation, let 𝜎b(𝛹, 𝑚, 𝑒): 𝛤×𝑀×𝐸 → 𝐴 be the strategy of 𝑅 with evidence 
𝑒. Fig. 2.4 depicts the signaling game model for the deception with knowledge acquisition through 
investigation. 

Fig. 2.4: Signaling games with evidence acquisition by investigation. The probability 
𝛾(𝑒|𝛹, 𝑚) of emitting evidence 𝑒 depends on the event 𝛹 and the message 𝑚 sent by 
𝑆. If the belief 𝜇b is informative, 𝜇b is used; if 𝜇b is uninformative, 𝛽b is used as the 
posterior. 

2.5.4  Equilibrium Concept 

The knowledge acquisition of the signaling game over continuous state space extends the PBNE 
in Definition 1 to the following. 

Definition 2.5: (Perfect Bayesian Nash Equilibrium) A PBNE of the game is a strategy profile 
(𝜎 O, 𝜎b) and a posterior belief system (𝜇b, 𝛽b) that satisfy the following conditions: 
– 

(Deceiver’s  Sequential  Rationality) 𝑆 maximizes  her  expected  utility  given  the  deceivee’s 
strategy 𝜎b and the distribution of the evidence 𝑒: for each 𝜃 ∈ 𝛩, 

𝜎 O∗(𝜃) ∈ argmax
fg

𝑈O(𝜎b∗, 𝜃, 𝜎 O). 

– 

(Deceivee’s Sequential Rationality) 𝑅 maximizes his expected utility given 𝑆’s strategy 𝜎 O∗ 
and his posterior belief 𝜇b(𝜃|𝑚): for any 𝑚 ∈ 𝑀, 

• 

if 𝜇b(𝜃|𝑚) is informative, 𝜎b∗(𝑟, 𝑚) ∈ argmax
fi∈7

I∈J

𝑈b

(𝜎b, 𝜃)𝜇b(𝜃|𝑚)𝑑𝜃;  

 
 
 
 
 
 
• 

if  𝜇b(𝜃|𝑚)

argmax

…

4

C
•ˆB

 is  uninformative  over  𝛩(cid:192) ≡ [𝜃‹, 𝜃o] ⊆ 𝛩 ,  𝜎b∗(𝛹, 𝑚, 𝑒) ∈
𝛽J´

(𝛹•|𝑚, 𝑒)𝑈b(𝑎

, 𝜃)𝑓(𝜃)𝑑 𝜃,  

•

…

where 𝜃(cid:192)

B ≡ [𝜃‹, 𝜃(cid:181)], 𝜃(cid:192)

C ≡ [𝜃‹, 𝜃(cid:181)], and 𝑎

•

≡ argmax

𝑈b

(𝑎, 𝜃)𝑑 𝜃. 

…
I´

– 

– 

(Consistent  Belief)  The  posterior  belief  of  𝑅  is  updated  according  to  Bayes’  rule  (i.e., 
𝜇b 𝜃 𝑚  is informative), as 

𝜇b 𝜃 𝑚 =

𝑓 𝜃 𝑞O 𝑚 𝜃

𝑓J

𝜃 𝑞O 𝑚 𝜃 𝑑 𝜃

	.			 

(𝜃)𝑞O(𝑚| 𝜃)𝑑 𝜃 = 0, 𝜇b(𝜃|𝑚) may be set to any probability distribution over 𝛩. If 

If  𝑓J
𝜇b is uninformative, i.e., 

𝜇b =

𝑓(𝜃)

𝑓

(𝜃)𝑑 𝜃

[I(cid:176),In]

	.			 

– 

𝑅 acquires evidence through investigation, and updates belief using evidence as, 

𝛽b(𝛹|𝑒, 𝑚) =

𝛾(𝑒|𝛹, 𝑚)𝑃(𝛹)

𝛾C
˜ˆB

(𝑒|𝛹˜, 𝑚)𝑃(𝛹˜)

, 

(𝑒|𝛹˜, 𝑚)𝑃(𝛹˜) = 0, 𝛽b(𝛹|𝑒, 𝑚) may  be  set  to  any  probability  distribution 

– 

𝛾C
˜ˆB

and  if 
over 𝛩. 

In separating equilibrium, the deceiver sends message 𝑚 with different values of report 𝛺(𝑚) for 
different  states.  Separating  equilibria  are  also  called  revealing  equilibria  because  the  strategic 
deceivee can infer the true state even if 𝛺(𝑚) does not tell the truth. In pooling equilibrium, the 
deceiver sends message 𝑚 ∈ 𝑀ﬁ with the same value of report 𝛺(𝑚) = 𝑟 for all states. In partial-
pooling equilibrium, however, the deceiver sends the message with the same report for some states 
and different reports for other states. Clearly, the PBNE strategy 𝜎 O∗ associated with a DR (resp. 
UR) is pooling (resp. separating) strategy. 

2.5.5  Equilibrium Results 

From the definition of UR, the equilibrium strategy of 𝑅 gives the most preferred action, 𝛼b(𝜃) ≡
𝜃, for all 𝜃 in the UR. Therefore, in any differentiable S-PBNE, the utility 𝑈O and the strategy 𝜎 O 
have to satisfy the following first-order optimality condition given 𝜎b∗(𝜃) = 𝛼b(𝜃) according to 

the sequential rationality: 𝑈C

O(𝛼b(𝜃), 𝜃, 𝜎 O(𝜃))

‰˘i(I))
‰I

+ 𝑈j

O(𝜃, 𝜃, 𝜎 O(𝜃))

‰fg(I)
‰I

= 0. 

Lemma 2.5 summarizes the property of the strategy 𝜎 O∗ in any UR. 

  
 
Lemma  2.5:  If [𝜃˙, 𝜃¨] is  an  undeceivable  region,  then  for  each 𝜃 ∈ [𝜃˙, 𝜃¨],  the  equilibrium 
strategy 𝜎 O∗(𝜃) > 𝜃 and it is a unique solution of the differential equation 

𝑑𝜎 O(𝜃)
𝑑𝜃

=

𝑏
𝑘(𝜎O(𝜃) − 𝜃)

, 

with initial condition 𝜎 O∗(𝜃˙) = 𝜃˙.  

Lemma 2.5 underlies the following proposition. 

Proposition 2.1: With initial condition 𝜎 O∗(𝜃) = 𝜃, there exists a cut-off state 𝜃 < 𝜃 such that a 

unique  solution 𝜎 O∗  to  the  differential  equation  in  Lemma  2.5  is  well-defined  on [𝜃, 𝜃] with 

𝜎 O∗(𝜃) = 𝜃, and there is no solution on (𝜃 , 𝜃].  

Proposition 2.1 notes that in S-PBNE, the optimal strategy 𝜎 O∗ of 𝑆 has to choose a report 𝑟 that 
is  strictly  larger  than  the  true  state 𝜃 ,  but  eventually 𝜎 O∗  runs  out  of  such  report  for 𝜃 > 𝜃 . 

Proposition 2.1 implies that there is no S-PBNE strategy of 𝑆 for all 𝜃 > 𝜃, because there are not 

enough states to support the monotone S-PBNE strategy of 𝑆 for the state in (𝜃 , 𝜃]. This suggests 
a class of PP-PBNE for the state space 𝛩, which is separating in low states and pooling in higher 
states. For convention, let 𝜎 O,k: 𝛩 → 𝛩 and 𝜂O,k, respectively, denote the P-PBNE strategy and the 
associated message strategy of 𝑆. We define this class of PP-PBNE by introducing a boundary 
state in the following precise sense. 

Definition 2.6: We say that the strategy 𝜎 O is a SLAPH (Separating in Low states And Pooling 

in High states) strategy if there exists a boundary state 𝜃˚ ∈ [𝜃, 𝜃] such that 
1.  (S-PBNE) 𝜎 O∗(𝜃) = 𝑟 with 𝜂O∗(𝑟) ∈ 𝑀ﬁ ,  for  all 𝜃 ∈ [𝜃, 𝜃˚),  and 𝜎 O∗(𝜃) ≠ 𝜎 O∗(𝜃′) for  all 

𝜃 ≠ 𝜃′ ∈ [𝜃, 𝜃˚); 

2. 

(P-PBNE) 𝜎 O∗,k(𝜃) = 𝜃 with 𝜂O∗,k(𝜃) ∈ 𝑀I, for all 𝜃 ∈ [𝜃˚, 𝜃]. 

In  any  SLAPH  equilibrium,  both  players  have  no  incentive  to  deviate  from  the  equilibrium 
strategies. This requires the boundary state 𝜃˚ to be consistent in the sense that the equilibrium 
at  𝜃˚  is  well-defined.  Specifically,  the  utility  of  𝑆  has  to  satisfy  the  following  boundary 
consistency (BC) condition at 𝜃˚: 

𝑈O(𝜎b∗(𝜎 O,k(𝜃˚), 𝑚k), 𝜃˚, 𝑚k) = 𝑈O(𝛼b(𝜃˚), 𝜃˚, 𝑚˙), 

 
 
 
 
 
 
where  𝑚k ∈ 𝑀I  and  𝑚˙ ∈ 𝑀fg∗(I¸) .  The  BC  condition  implies  that  𝑆  is  indifferent  between 
sending 𝑚k ∈ 𝑀I with 𝑎∗ = 𝜎b∗(𝜎 O∗(𝜃˚), 𝑚k) and sending 𝑚˙ ∈ 𝑀fg∗(I¸) with action 𝑎∗ = 𝜃˚. 

The conflict of interest, b, is a utility-relevant parameter for S that can induce incentives for S to 
reveal partial information about any state θ ∈ [θ(cid:204), θ] to R while her utility-maximizing P-PBNE 
strategy σy∗ is maintained. This can be achieved based on the assumption |M(cid:160)| → ∞ and the fact 
that C¥ is equally expensive for all the messages chosen for all state θ ∈ [θ(cid:204), θ]. Specifically, the 
P-PBNE  region [θ(cid:204), θ] can  be  further  partitioned  into  multiple  pools.  First,  some  notations  for 
describing the multiple pools are needed. Let Θ˝ ≡ (θB, θC, … , θ˛(cid:144)C, θ˛) be a partition of [θ(cid:204), θ], 
with θB = θ(cid:204) < θC < ⋯ < θ˛ = θ.  We  call  each  interval Θ—,—(cid:209)C = [θ—, θ—(cid:209)C] is  a  pool.  With  an 
abuse  of  notation,  let ηy∗,(cid:210)(σy∗(θ), θ) denote  the  message  strategy  that  chooses  a  message 𝑚 ∈
M(cid:158)(cid:159)∗((cid:160)) for  a  state θ.  In  each  pool Θ—,—(cid:209)C, ηy∗,(cid:210)(θ, θ) chooses  the  same  message 𝑚 ∈ M(cid:160),  for  all 
θ ∈ Θ—,—(cid:209)C, j = 0, … , K − 1.  After R determines  a  pool Θ—,—(cid:209)C,  she  acquires  evidence e ∈ {eB, eC} 
C ≡
through  investigations  by  dividing Θ—,—(cid:209)C into  two  sub-intervals Θ—,—(cid:209)C
(cid:215)
(cid:213)
[θ—,—(cid:209)C

] and Θ—,—(cid:209)C
 represents the event {θ ∈ Θ—,—(cid:209)C

B ≡ [θ—, θ—,—(cid:209)C

} such that Ψ—,—(cid:209)C

C
, Ψ—,—(cid:209)C

}, 

B

(cid:213)

(cid:215)

, θ—(cid:209)C]. Let Ψ—,—(cid:209)C ∈ Γ—,—(cid:209)C ≡ {Ψ—,—(cid:209)C
((cid:160))(cid:223)(cid:160)

(cid:217)

with  probability  P(Ψ—,—(cid:209)C

(cid:215)

) =

,  for  i = 0 ,  1 .  On  the  equilibrium  path,  R  must  play 

(cid:222)
(cid:218)(cid:219),(cid:219)(cid:220)(cid:221)

(cid:217)

((cid:160))(cid:223)(cid:160)

(cid:218)(cid:219),(cid:219)(cid:220)(cid:221)

σU∗(Ψ—,—(cid:209)C, 𝑚—, e) as  defined  in  Definition  2.5  for  any m— such  that ηy∗,(cid:210)(θ, θ) = 𝑚— for  all θ ∈
Θ—,—(cid:209)C. Define 

•

𝑎

(𝜃˜, 𝜃˜(cid:209)C) ≡ 𝑎𝑟𝑔𝑚𝑎𝑥

4

…
J(cid:224),(cid:224)(cid:220)(cid:221)

𝑈b

(𝑎, 𝜃)𝑓(𝜃)𝑑𝜃, 

for 𝑖 = 0, 1. 

The necessary and sufficient conditions for the existence of SLAPH equilibrium are summarized 
in the following theorem. 

Theorem 2.3: (Necessary condition.) In any SLAPH equilibrium, there exists a boundary state 
𝜃˚  such  that  the  pooling  interval [𝜃˚, 𝜃] can  be  partitioned  into  multiple  pools  denoted  by  a 
strictly increasing sequence (𝜃B, 𝜃C, … , 𝜃Æ(cid:144)C, 𝜃Æ) with 𝜃B = 𝜃˚ and 𝜃Æ = 𝜃, such that, for all 𝑗 =
0, … , 𝐾 − 1,  

𝑈7 𝑎 𝜃˜, 𝜃˜(cid:209)C , 𝜃˜(cid:209)C = 𝑈7 𝑎 𝜃˜(cid:209)C, 𝜃˜(cid:209)h , 𝜃˜(cid:209)C ,							(2.7) 

𝑈O 𝑎 𝜃B, 𝜃C , 𝜃˚, 𝜃 = 𝑈O 𝜃˚, 𝜃˚, 𝜎 O∗ 𝜃˚ ,   	if	𝜃˚ > 𝜃,			(2.8) 

where 𝑎(𝜃˜, 𝜃˜(cid:209)C) =
Given the multiple-pool PBNE characterized by Eq. (2.7)-(2.8), and if 𝜃˚ = 𝜃

(𝜃˜, 𝜃˜(cid:209)C),  for  all 𝑗 = 0, … , 𝐾 − 1.  (Sufficient  Condition.) 

 and 

𝑃C
•ˆB

(𝛹•) 𝑎

•

 
 
̲
𝑈O 𝑎 𝜃B, 𝜃C , 𝜃, 𝜃 ≤ 𝑈O 𝛼b 𝜃 , 𝜃, 𝜎 O∗ 𝜃 ,						(2.9) 

there exists an SLAPH equilibrium. 

Note  that 𝑈? is  equal  for  all 𝜃 ∈ [𝜃˚, 𝜃].  Eq.  (2.7)  says  that  at  each  link  state 𝜃˜(cid:209)C connecting 
𝛩˜,˜(cid:209)C and 𝛩˜(cid:209)C,˜(cid:209)h,  the  utilities  induced  by 𝑎(𝜃˜, 𝜃˜(cid:209)C) and 𝑎(𝜃˜(cid:209)C, 𝜃˜(cid:209)h),  respectively,  should 
keep the same. Otherwise, 𝑆 has incentives to deviate from the current partition to combine these 
two consecutive pools by sending the message that induces more profitable 𝑈7 but the same 𝑈?. 
This is not ideal for 𝑅 because larger pools make the posterior less informative that could decrease 
the utilities for 𝑅. Similarly, Eq. (2.8) says that at the boundary state 𝜃˚, 𝑆 should be indifferent 
between  playing  S-PBNE  strategy  and  inducing  action 𝜎b∗(𝜃˚) versus  playing  P-PBNE  and 
introducing  action 𝑎(𝜃B, 𝜃C).  Inequality  (2.9)  notes  that  if  the  boundary  state 𝜃˚ = 𝜃,  then 𝑆 is 
indifferent  between  pooling  with  [𝜃, 𝜃C]  and  reporting  𝜃  for  𝜃  versus  separating  at  𝜃 .  The 
existence of SLAPH requires (2.7)-(2.9) to be jointly satisfied. 

2.6 Adaptive Strategic Cyber Defense for APT in Critical Infrastructure Network 

2.6.1  Multi-Stage Dynamic Bayesian Game Model 

In [18] and [19], Huang et al. have proposed a multi-stage Bayesian game framework to capture the 
stealthy, dynamic, and adaptive natures of the advanced persistent threats (APTs) as shown in Fig. 
2 . 5. 

Fig. 2.5: The APTs’ life cycle includes a sequence of phases and stages such as the initial 
entry, privilege  escalations,  and  lateral  movements.  APTs use  each  stage  as  a  stepping 
stone for the next and aim to cause physical damages or collect confidential data. 

Continuous Type: Due to the cyber deception, the system defender 𝑅 cannot directly determine 
whether a user is legitimate or not even he can observe the user’s apparent behaviors. Thus, user 

 
 
 
 
 
 
 
𝑆  has  a  type  𝜃  which  is  a  random  variable  and  the  realization  𝜃 ∈ 𝛩: = [0,1]  is  private 
information of 𝑆. The value of the type indicates the strength of the user in terms of damages that 
she can inflict on the system. A user with a larger type value indicates a higher threat level. At 
each stage 𝑘 ∈ {0,1, ⋯ , 𝐾}, 𝑆 chooses an action 𝑚Œ ∈ 𝑀Œ and 𝑅 chooses an action 𝑎Œ ∈ 𝐴Œ. The 
user’s actions represent the apparent behaviors and observable activities from log files such as a 
privilege escalation request and sensor access. A defender cannot identify the user’s type from 
observing his actions. The defender’s action includes prevention and proactive behaviors such as 
restricting  the  escalation  request  or  monitoring  the  sensor  access.  The  action  pair (𝑚Œ, 𝑎Œ) is 
known to both players after stage 𝑘 and forms a history ℎŒ: = {𝑚B, ⋯ , 𝑚Œ(cid:144)C, 𝑎B, ⋯ , 𝑎Œ(cid:144)C}. The 
state 𝑥Œ ∈ 𝒳Œ shows the system status at each stage 𝑘 such as the location of the APTs. Since the 
initial state 𝑥B and history ℎŒ uniquely determine the state, 𝑥Œ contains information of history up 
to 𝑘  and  has  the  transition  kernel  described  by 𝑥Œ(cid:209)C = 𝑔Œ(𝑥Œ, 𝑚Œ, 𝑎Œ).  The  behavior  mixed 
strategies 𝜎Œ

O: 𝒳Œ×𝛩 →△ 𝑀Œ and 𝜎Œ

b: 𝒳Œ →△ 𝐴Œ.  

b ∈ 𝛤Œ

O ∈ 𝛤Œ

b: 𝒳Œ ↦
Believe Update: To strategically gauge the user’s type, the defender specifies a belief 𝜇Œ
△ 𝛩 as a distribution over the type space according to the information available at stage 𝑘. The 
prior distribution of the user’s type is known to be 𝑓 and the belief of the type updates according 
to the Bayesian rule. 

b
𝜇Œ(cid:209)C

(𝜃|𝑥Œ(cid:209)C) =

b(𝜃|𝑥Œ)𝜎Œ
𝜇Œ
bC
(𝜃 |𝑥Œ)𝜎Œ
𝜇Œ
B

b(𝑎Œ|𝑥Œ, 𝜃)
b(𝑎Œ|𝑥Œ, 𝜃)𝑑 𝜃

. 

Utility Function: The user’s type influences 𝑃•’s immediate payoff received at each stage 𝑘, i.e., 
O: 𝒳Œ×𝑀Œ×𝐴Œ×𝛩 ↦ ℝ. For example, a legitimate user’s access to the sensor benefits the system 
𝑈Œ
O}ŒˆŒo,⋯,Æ ∈
while a pernicious user’s access can incur a considerable loss. Define 𝜎Œo:Æ
𝛤Œo:Æ

O  as a sequence of policies from 𝑘′ to 𝐾. 

: = {𝜎Œ

O ∈ 𝛤Œ

O

The defender has the objective to maximize the cumulative expected utility: 

b
𝑈Œo:Æ

O
(𝜎Œo:Æ

, 𝜎Œo:Æ

b , 𝑥Œo): =

and the user’s objective function is 

Æ

ŒˆŒo

Æ

𝐸I∼(cid:240)æ

i,læ∼(cid:130)æ

g,4æ∼(cid:130)æ
i

𝑈Œ

b(𝑥Œ, 𝑚Œ, 𝑎Œ, 𝜃), 

O
𝑈Œo:Æ

O
(𝜎Œo:Æ

, 𝜎Œo:Æ

b , 𝑥Œo, 𝜃) =

b
𝜎Œ

(𝑎Œ|𝑥Œ)

O
𝜎Œ

(𝑚Œ|𝑥Œ, 𝜃)𝑈Œ

O. 

ŒˆŒo

4æ∈7æ

læ∈~æ

2.6.2  Equilibrium Concept 

 
 
 
The insider threats of APTs lead to the following definition of perfect Bayesian Nash equilibrium 
(PBNE) where the defender chooses the most rewarding policy to confront the attacker’s best-
response policies. 

Definition 2.7: In the two-person multi-stage game with a sequence of beliefs 𝜇Œ

satisfying the Bayesian update in and the cumulative utility function 𝑈Œo:Æ
O
b ∈ 𝛤Œo:Æ

O : 𝑈Œo:Æ

b , 𝜎Œo:Æ

b , 𝛾, 𝑥Œo, 𝜃) ≥ 𝑈Œo:Æ
(𝜎Œo:Æ
{𝛾 ∈ 𝛤Œo:Æ
𝑆’s best-response set to 𝑅’s policy 𝜎Œo:Æ

, 𝑥Œo, 𝜃), ∀𝛤Œo:Æ
b  under state 𝑥Œo and type 𝜃. 

(𝜎Œo:Æ
b ∈ 𝛤Œo:Æ

O

O

O

b, 𝑘 ∈ {𝑘′, ⋯ , 𝐾} 
I,(cid:242)æn(𝜎Œo:Æ

, the set 𝑅h

b ): =
b , ∀𝑥Œo ∈ 𝒳Œo, 𝜃 ∈ 𝛩}  is 

Definition  2.8:  In  the  two-person  multi-stage  Bayesian  game  with  𝑅  as  the  principal,  the 

cumulative utility function 𝑈Œo:Æ
of beliefs in , a sequence of strategies 𝜎Œo:Æ
(PBNE) for the principal, if 

O

b
, 𝑈Œo:Æ

, the initial state 𝑥Œo ∈ 𝒳Œo, the type 𝜃 ∈ 𝛩, and a sequence 
b  is called a perfect Bayesian Nash equilibrium 
b∗ ∈ 𝛤Œo:Æ

b∗
𝑈Œo:Æ

(𝑥Œo): =

inf
m,ıæn(fæn:(cid:243)
i∗ )

g ∈b(cid:244)
fæn:(cid:243)

b∗
𝑈Œo:Æ

(𝜎Œo:Æ

b∗ , 𝜎Œo:Æ

O

, 𝑥Œo). 

A strategy 𝜎Œo:Æ

O∗ ∈ 𝑎𝑟𝑔maxfæn:(cid:243)

g ∈(cid:130)æn:(cid:243)

O
g 𝑈Œo:Æ

(𝜎Œo:Æ

b∗ , 𝜎Œo:Æ

O

, 𝑥Œo, 𝜃) is a PBNE for the agent 𝑆. 

A conjugate-prior method allows online computation of the belief and reduces Bayesian update 
into  an  iterative  parameter  update.  The  forwardly  updated  parameters  are  assimilated  into  the 
backward  dynamic  programming  computation  to  characterize  a  computationally  tractable  and 
time-consistent equilibrium solution based on the expanded state space. 

2.6.3  Case Study 

We  consider  a  four-stage  transition  with  the  first  three  stages  related  to  cyber  transition  of  the 
APTs and the last stage related to the benchmark Tennessee Eastman (TE) process as the targeted 
physical  plant.  The  TE  process  involves  two  irreversible  reactions  to  produce  two  liquid  (liq) 
products. The process shuts down when the safety constraints are violated such as a high reactor 
pressure,  a  high/low  separator/stripper  liquid  level.  The  attacker  can  revise  the  sensor  reading, 
trigger  an  undesired  feedback-control,  and  cause  a  loss.  The  state 𝑥Œ  has  five  possible  values 
representing the working status of the system where the state 1 is most desirable and state 5 is the 
least desirable. We can obtain the normal operation reward and the reward of attacks from the 
simulation. The belief state {𝛼Œ, 𝛽Œ} uniquely determines the belief distribution which is assumed 
to take the form of the beta distribution. The larger 𝛼Œ means that the user is more likely to have a 
smaller type value and have lower threats to the system. 

 
 
 
 
 
 
b∗
As shown in Fig. 2.6, a high value 𝑈Œo:Æ
(𝑥Œo) for the defender is the result of a healthy system 
state 𝑥Œ as well as a belief of a low-threat user. At the most desirable system state 𝑥Œ = 1, attackers 
will  not  attack  because  the  reward  incurred  is  insufficient.  Then  the  defender  does  not  need  to 
defend and obtains the maximum utility. 

b∗
Fig. 2.6:  Value function 𝑈Œ′:Æ

(𝑥Œ′) under different expanded states {𝑥Œ, 𝛼Œ, 𝛽Œ}. 

To investigate the effect of the defender’s belief, we change the belief state (𝛼Œ, 𝛽Œ) from (9,1) 
to (1,9), which means that the defender grows optimistically that the user is of a low threat level 
with a high probability. Since players’ value functions are of different scales in terms of the attack 
threshold and the probability, we normalize the value functions with respect to their maximum 
values to illustrate their trends and make them comparable to the threshold and the probability as 
shown  in  Fig.  2.7.  When 𝛼Œ is  large,  the  defender  chooses  to  protect  the  system  with  a  high 
probability, which completely deters attackers with any type values because the probability to 
attack is 0. 

As the defender trusts more about the user’s legitimacy, the defending probability decreases to 0 
when 𝛼Œ = 1. Since the defender is less likely to defend, the attacker bears a smaller threshold 
to  launch  the  attack.  The  resulted  defending  policy  captures  a  tradeoff  between  security  and 
economy and guarantees a high value for defenders at most of the belief states. 

 
 
 
 
Fig. 2.7: The effect of the defender’s belief. 

The  central  insight  from  the  multi-stage  analysis  is  the  adversary’s  tradeoff  between  the 
instantaneous reward and the hiding to arrive at a more favorable state in the future stages. The 
higher  the  belief  of  the  defender  in  𝑆  as  a  legitimate  user,  the  less  probability  he  will  act 
defensively and thus the attacker has a smaller threshold to launch the attack. 

2.7 Conclusion 

Deception is a technique that can be viewed as an advanced approach to secure the devices or 
attacks. Understanding deception quantitatively is pivotal to provide rigor, predictability, and 
design principles. In this chapter, we have formulated signaling-game theoretic models of 
deceptions over discrete and continuous information spaces. We have studied leaky deception 
models and extended the baseline perfect Bayesian Nash equilibrium (PBNE) to versions 
involving knowledge acquisitions characterized by evidences. We have analyzed the impacts of 
evidence on the belief updates and strategy constructions on the equilibrium path. 

In the binary state space, the leaky deception game with evidence admits an equilibrium that 
includes a regime in which the deceivee should choose whether to trust the deceiver based on the 
evidence, and regimes in which the deceivee should ignore the message and evidence and merely 
guess the private information based only on the prior probabilities. For the deceiver, the 
equilibrium results imply that it is optimal to partially reveal the private information in the 
former regime. 

We have also studied leaky deception games over a continuous one-dimensional information 

 
 
 
 
 
 
 
 
space. We have studied the PBNE as the solution concept to analyze the outcome of the 
deception game and characterize the deceivability of the game. The proposed deception game 
admits a class of PBNE called SLAPH (Separating in Low states And Pooling in High states). 
The necessary and sufficient conditions for the existence of such PBNE are given. However, a 
full undeceivable region does not exist and there exists a deceivable region. We have also shown 
that the deceivable region can be partitioned into multiple sub-deceivable regions without 
decreasing total utilities for the deceiver when the conflict of interest is insignificant. 

Furthermore, we have explored a multi-stage incomplete information Bayesian game model for 
defensive deception frameworks for critical infrastructure networks with the presence of 
advanced persistent threats (APT). With the multi-stage and multi-phase structure of APTs, the 
belief of the defender is formed dynamically using observable footprints. The conjugate priors 
are used to reduce Bayesian updates into parameter updates, which leads to a computationally 
tractable extended-state dynamic programming that admits an equilibrium solution consistent 
with the forward belief update and backward induction. Tennessee Eastman process has been 
used as a case study to demonstrate the multi-stage deception game. The numerical simulations 
have shown that the game-theoretic defense strategies have significantly improved the security of 
the critical infrastructures. 

References  

1.  Symantec  Corporation,  “Internet  security  threat  report.  Tech.  rep.,”  Symantec 
Corporation,  2016.  [Online].  Available:  https://www.symantec.com/.  [Accessed 
Nov. 10, 2018]. 

2.  L.  Atzori,  A.  Iera,  G.  Morabito,  “The  Internet  of  Things:  A  survey,”  Computer 

Networks, Volume 54, Issue 15, 2010, Pages 2787-2805 

3.  S. Bodmer, D.M. Kilger, G. Carpenter, et al. Reverse deception: organized cyber 

threat counter-exploitation. McGraw-Hill New York, 2012. 

4.  I.  Bose,  A.C.M.  Leung,  “Unveiling  the  Mask  of  Phishing:  Threats,  Preventive 
Measures,  and  Responsibilities,”  Communications of the Association for Information 
Systems, vol 19, no. 1, pp. 24, 2007. 

5.  I. Bose, A.C.M. Leung, “Assessing anti-phishing preparedness: A study of online 
banks in Hong Kong,” Decision Support Systems vol. 45, no. 4, pp. 897–912, 2008. 
6.  G.  Brown,  M.  Carlyle,  D.  Diehl,  et  al.,  “A  two-sided  optimization  for  theater 
ballistic missile defense,” Operations research vol 53, no. 5, pp.745–763,   2005. 
7.  T.E.  Carroll,  D.  Grosu,  “A  game  theoretic  investigation  of  deception  in  network 

security,” Security and Commun. Nets. Vol 4, no. 10, pp. 1162–1172, 2011. 

8.  A. Clark, Q.  Zhu, R. Poovendran, et al., “Deceptive routing in relay networks,” In 
Proc. International Conference on Decision and Game Theory for Security, 2012,  
pp. 171–185.  

 
 
 
 
9.  H. Cott, Adaptive Coloration in Animals. Methuen,   1940. 
10.  D.  Ettinger,  P.  Jehiel,  “A  theory  of  deception,”  American  Economic  Journal: 

Microeconomics vol 2, no. 1, pp. 1–20,   2010. 

11.  S. Farhang, M.H. Manshaei, M.N. Esfahani, et al., “A dynamic Bayesian security 
game framework for strategic defense mechanism design,” In Proc. Decision and 
Game Theory for Security,2014,  pp. 319–328. 

12.  D. Fudenberg, J. Tirole, Game Theory. The MIT Press, 1991. 
13.  U. Gneezy, “Deception: The role of consequences,” American Econ. Review, vol. 95, 

no. 1, pp. 384–394, 2005. 

14.  E. Harrell, “Victims of identity theft, 2014,” US Dept. of Justice Bureau of Justice 

Stat. Bulletin, September,  2015. 

15.  J.C. Harsanyi, “Games with incomplete information played by “Bayesian” players,” 

Manage. Sci. vol. 50, no. 12, pp.1804–1817, 1967. 

16.  K.  Horák,   Q. Zhu,   B. Bošansky`,   “Manipulating   adversaryâs   belief:   A   dynamic 
game  approach  to  deception  by  design  for  proactive  network  security,”  In  Proc. 
International Conference on Decision and Game Theory for Security, 2017,  pp. 273–
294. 

17.  L. Huang, Q. Zhu, “Analysis and computation of adaptive defense strategies 

against  advanced  persistent 
threats  for  cyber-physical  systems,”  In  Proc.  
International  Conference  on  Decision  and  Game  Theory  for  Security,  2018,  pp. 
205-226. 

18.  L.  Huang,  J.  Chen,  Q.  Zhu,  “Factored  Markov  game  theory  for  secure 
interdependent  infrastructure  networks,”  Game  Theory  for  Security  and  Risk 
Management, pp. 99–126. Springer, 2018. 

19.  L. Huang, Q. Zhu, “Adaptive strategic cyber defense for advanced persistent threats 
in critical infrastructure networks,” ACM SIGMETRICS Performance Evaluation 
Review, 2019, 46(2): 52-56.  

20.  L.J. Janczewski, A.M. Colarik, “Cyber Warfare and Cyber Terrorism,” Inform. Sci. 

Reference, New Yor,  2008. 

21.  D. Lewis, Convention cambridge. Mass.: Harvard UP, 1969. 
22.  M.H. Manshaei, Q. Zhu, T. Alpcan, T., et al., “Game theory  meets network security 

and privacy,” ACM Computing Surveys (CSUR) vol. 45, no. 3, pp. 25, 2013. 

23.  F. Miao,  Q .   Zhu,  M .   Pajic,  et al.,  “ A  hybrid  stochastic  game  for  secure 
control of cyber-physical systems,” Automatica vol. 93, pp. 55–63, 2018. 

24.  U.F., Minhas, J. Zhang, T. Tran, et al., “A multifaceted approach to modeling agent 
trust  for  effective  communication  in  the  application  of  mobile  ad  hoc  vehicular 
networks,” IEEE Trans. Systems, Man, and Cybernetics, Part C (Applications and 
Reviews) vol. 41, no. 3, pp. 407–420, 2011. 

25.  A. Monga,  Q. Zhu,  “ On  solving  large-scale  low-rank  zero-sum  security  games  of 

incomplete information,” In Proc. Information Forensics and Security (WIFS), 2016 
IEEE International Workshop on, pp. 1–6, IEEE,   2016. 

26.  J.F., Nash, “Equilibrium points in n-person games,” In Proc. Nat. Acad. Sci. USA 

vol. 36, no. 1, pp. 48–49, 1950. 

27.  J. Pawlick, E. Colbert, Q. Zhu, “Analysis of leaky deception for network security 

using  signaling  games  with  evidence,”  In  Proc.  17th  Annual  Workshop  on  the 
Economics of Information Security (WEIS), Innsbruck, Austria, June 18-19, 2018. 
28.  J. Pawlick,  E. Colbert,  Q .   Zhu,  Q,  “ A  game-theoretic  taxonomy  and  survey  of 
preprint 

cybersecurity 

deception 

privacy,” 

arXiv 

and 

for 

defensive 
arXiv:1712.05441, 2017. 

29.  J.  Pawlick,  Q.  Zhu,  “Deception  by  design:  Evidence-based  signaling  games  for 
network defense,” In Proc. Workshop on the Econ. of Inform. Security. Delft, The 
Netherlands, 2015. 

30.  J. Pawlick, Q. Zhu, “A Stackelberg game perspective on the conflict between ma- 
chine  learning  and  data  obfuscation,”  In  Proc.  IEEE  Intl.  Workshop  on  Inform. 
Forensics and Security,   2016. 

31.  J. Pawlick, Q. Zhu, “Strategic trust in cloud-enabled cyber-physical systems with 
an application to glucose control,” IEEE Trans. Inform. Forensics and Security, vol. 
12, no. 12, 2017. 

32.  Phishlabs,  “2018  Phishing  trends  intelligence  report,”  Phishlabs,  2018.  [online]. 

Available: https://info.phishlabs.com/. [Accessed Nov. 10, 2018]. 

33.  J.P. Ponssard, S. Zamir, “Zero-sum sequential games with incomplete information," 

International Journal of Game Theory vol.2 , no. 1, pp. 99–107, 1973. 

34.   R.  Powell,  “Allocating  defensive  resources  with  private  information  about 
vulnerability,”  American  Political  Science  Review  vol.  101,  no.  4,  pp.  799–809,  
2007. 

35.  A.  Vrij,  S.A.  Mann,  R.P.  Fisher,  et  al.,  “Increasing  cognitive  load  to  facilitate  lie 
detection:  The  benefit  of  recalling  an  event  in  reverse  order,”  Law  and  Human 
Behavior vol. 32, no. 3, pp. 253–265, 2008. 

36.  N. Zhang, W. Yu, X. Fu, et al., “gPath: A game-theoretic path selection algorithm 
to  protect  tor’s  anonymity,”  In  Proc.  International  Conference  on  Decision  and 
Game Theory for Security, 2010, pp. 58–71. 

37.  T. Zhang, Q. Zhu, Q, “Strategic defense against deceptive civilian GPS spoofing of 
unmanned aerial vehicles,” In Proc. International Conference on Decision and Game 
Theory for Security, 2017, pp. 213–233. 

38.  T.  Zhang,  Q.  Zhu,  “A  game-theoretic  foundation  of  deception:  Knowledge 
p r e p r i n t  

fundamental 

a r X i v  

limits,” 

and 
acquisition 
a r X i v : 1 8 1 0 . 0 0 7 5 2 ,   2 0 1 8 .  

39.  Q. Zhu, T. Başar, “Game -theoretic approach to feedback-driven multi-stage moving 
target defense,” In Proc. International Conference on Decision and Game Theory for 
Security, 2013, pp. 246–263. 

40.  Q. Zhu, T. Basar, “Game-theoretic methods for robustness, security, and resilience 
of cyberphysical control systems: games-in-games principle for optimal cross-layer 
resilient control systems,” IEEE control systems vol. 35, no. 1, pp. 46–65, 2015. 
41.  Q. Zhu, A. Clark, R. Poovendran, et al., “Deceptive routing games,” In Proc. IEEE 
51st Annual Conference on Decision and Control (CDC), 2012, pp. 2704–2711. 
42.  Q. Zhu, A. Clark, R. Poovendran, et al., “Deployment and exploitation of deceptive 
honeybots in social networks,” In Proc. IEEE 52nd Annual Conference  on Decision 
and Control (CDC), 2013, pp. 212–219. 

43.  T. Zhang, Q. Zhu, “Hypothesis Testing Game for Cyber Deception,” In Proc. International 

Conference on Decision and Game Theory for Security, 2018, pp. 540-555.   

44.  S.  Roy,  C.  Ellis,  S.  Shiva,  et  al.,  “A  Survey  of  Game  Theory  as  Applied  to  Network 
Security,”  In Proc. 43rd Hawaii International Conference on System Sciences, 2010, pp. 1-
10. 

45.  R. Zhang R, Q. Zhu, “Secure and resilient distributed machine learning under adversarial 
environments,”  In  Proc.  18th  International  Conference  on  Information  Fusion  (Fusion), 
2015, pp. 644-651. 

46.  T.  Alpcan,  T.  Basar,  “A  game  theoretic  approach  to  decision  and  analysis  in  network 
intrusion detection,” In Proc. 42nd IEEE International Conference on Decision and Control, 
2003, pp. 2595-2600 Vol.3. 

47.  T. Basar, “The Gaussian test channel with an intelligent jammer,” IEEE Transactions on 

Information Theory, vol. 29, no. 1, pp. 152-157, January 1983. 

48.  J. Pita, M. Jain, J. Marecki, et al. “Deployed ARMOR protection: the application of a game 
theoretic model for security at the Los Angeles International Airport,” In Proc. of the 7th 
international  joint  conference  on  Autonomous  agents  and  multiagent  systems:  industrial 
track, 2008,  pp. 125-132. 

49.  E. Shieh, B. An, R. Yang, et al., “Protect: A deployed game theoretic system to protect the 
ports of the united states,” In Proc. of the 11th International Conference on Autonomous 
Agents and Multiagent Systems, 2012, pp. 13-20. 

50.  S. Farhang, M.H. Manshaei Esfahani, M.N., Zhu, Q.: A dynamic Bayesian security game 
framework  for  strategic  defense  mechanism  design.  In:  Decision  and  Game  Theory  for 
Security, pp. 319–328. Springer (2014) 

51.  S. Rass, A. Alshawish, M.A. Abid, et al, “Physical Intrusion Games—Optimizing 
Surveillance by Simulation and Game Theory,” IEEE Access, 2017, vol. 5, pp. 8394-
8407. 

52.  Q. Zhu Q, S. Rass, “On Multi-Phase and Multi-Stage Game-Theoretic Modeling of 

Advanced Persistent Threats,” IEEE Access, 2018, vol. 6, pp. 13958-13971. 

53.  J. Zhuang, V.M. Bier, O. Alagoz, “Modeling secrecy and deception in a multiple-
period  attacker–defender  signaling  game,”  European  Journal  of  Operational 
Research, 2010, vol. 203, no. 2, pp. 409-418. 

54.  J. Chen, Q. Zhu, “Security as a service for cloud-enabled internet of controlled things 
under advanced persistent threats: a contract design approach,” IEEE Transactions 
on Information Forensics and Security, 2017, vol. 12, no. 11, pp. 2736-2750. 

55.  Z.  Xu,  Q.  Zhu,  “Secure  and  practical  output  feedback  control  for  cloud-enabled 
cyber-physical  systems,”  In  Proc.  IEEE  Conference  on  Communications  and 
Network Security (CNS), pp. 416-420. 

56.  S. Jajodia, A.K. Ghosh, V. Swarup, C. Wang, et al. Moving target defense: creating 
asymmetric  uncertainty  for  cyber  threats  (Vol.  54).  Springer  Science  &  Business 
Media. 

57.  H. Maleki, S. Valizadeh, W. Koch, et al, “Markov modeling of moving target defense 
games,”  In  Proc.  of  the  2016  ACM  Workshop  on  Moving  Target  Defense.  ACM, 
2016, pp. 81-92. 

58.  J. Chen, C. Touati, Q. Zhu, “A Dynamic Game Analysis and Design of Infrastructure 

Network Protection and Recovery,” ACM SIGMETRICS Performance Evaluation 
Review, 2017, vol. 45, no.2 pp. 128. 

59.  J. Chen, Q. Zhu, “Interdependent network formation games with an application to 
critical  infrastructures,”  In  Proc.  American  Control  Conference  (ACC),  2016,  pp. 
2870-2875. 

60.  Y.  Hayel,  Q.  Zhu,  “Resilient  and  secure  network  design  for  cyber  attack-induced 
cascading  link  failures  in  critical  infrastructures,”  In  Proc.  49th  IEEE  Annual 
Conference on Information Sciences and Systems (CISS), 2015, pp. 1-3. 

61.  J. Pawlick, Q. Zhu, “Proactive Defense Against Physical Denial of Service Attacks 
Using Poisson Signaling Games,” In Proc. International Conference on Decision and 
Game Theory for Security, Springer, 2017, pp. 336-356. 

62.  J.  Pawlick,  Q.  Zhu,  “A  mean-field  stackelberg  game  approach  for  obfuscation 
adoption in empirical risk minimization,” In Proc. IEEE Global Conference on Signal 
and Information Processing (GlobalSIP), 2017, pp. 518-522. 

63.  W. Wang, Q. Zhu, “On the Detection of Adversarial Attacks against Deep Neural 
Networks,”  In  Proc.  ACM  Workshop  on  Automated  Decision  Making  for  Active 
Cyber Defense, 2017, pp. 27-30. 

64.  R.  Zhang,  Q.  Zhu,  “A  game-theoretic  defense  against  data  poisoning  attacks  in 
distributed  support  vector  machines,”  In  Proc.  IEEE  56th  Annual  Conference  on  
Decision and Control (CDC), 2017, pp. 4582-4587. 

65.  R.  Zhang,  Q.  Zhu,  “A  Game-Theoretic  Approach  to  Design  Secure  and  Resilient 
Distributed Support Vector Machines,” IEEE Transactions on Neural Networks and 
Learning Systems, 2018. 

66.  W.  Casey,  J.A.  Morales,  E.  Wright,  et  al.,  “Compliance  signaling  games:  toward 
modeling  the  deterrence  of  insider  threats,”  Computational  and  Mathematical 
Organization Theory, 2016, vol. 22, no. 3, pp. 318-349. 

67.  W.A.  Casey,  Q.  Zhu,  J.A.  Morales,  et  al.,  “Compliance  control:  managed 
vulnerability surface in social-technological systems via signaling games,” In Proc. 
7th ACM CCS International Workshop on Managing Insider Security Threats, 2015, 
pp. 53-62. 

68.  J. Chen, Q. Zhu, “Security investment under cognitive constraints: A Gestalt Nash 
equilibrium approach,” In Proc. IEEE Annual Conference on Information Sciences 
and Systems (CISS), 2018, pp. 1-6. 

69.  C.J.  Fung,  Q.  Zhu,  “FACID:  A  trust-based  collaborative  decision  framework  for 

intrusion detection networks,” Ad Hoc Networks, 2016, vol. 53, pp. 17-31. 

70.  Y.  Hayel,  Q.  Zhu,  “Epidemic  protection  over  heterogeneous  networks  using 
evolutionary  Poisson  games,”  IEEE  Transactions  on  Information  Forensics  and 
Security, 2017, vol. 12, no. 8, pp. 1786-1800. 

71.  R.  Zhang,  Q.  Zhu,  Y.  Hayel,  “A  bi-level  game  approach  to  attack-aware  cyber 
in 

IEEE  Journal  on  Selected  Areas 

insurance  of  computer  networks,” 
Communications, 2017, vol. 35, no. 3, pp. 779-794. 

 
 
