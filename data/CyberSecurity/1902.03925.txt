Chapter 2  
Game-Theoretic Analysis of Cyber Deception: Evidence-
Based Strategies and Dynamic Risk Mitigation 
Tao Zhang1, Linan Huang1, Jeffrey Pawlick1, and Quanyan Zhu1 

1New York University, 2 Metrotech Center, Brooklyn, 11201, USA. 

2.1  Abstract 

Deception  is  a  technique  to  mislead  human  or  computer  systems  by  manipulating  beliefs  and 
information. For the applications of cyber deception, non-cooperative games become a natural 
choice of models to capture the adversarial interactions between the players, and quantitatively 
characterizes the conflicting incentives and strategic responses. In this chapter, we provide an 
overview of deception games in three different environments and extend the baseline signaling 
game  models  to  include  evidence  through  side-channel  knowledge  acquisition  to  capture  the 
information  asymmetry,  dynamics,  and  strategic  behaviors  of  deception.  We  analyze  the 
deception in binary information space based on signaling game framework with a detector that 
gives off probabilistic evidence of the deception when the sender acts deceptively. We then focus 
on a class of continuous one-dimensional information space and take into account the cost of 
deception  in  the  signaling  game.  We  finally  explore  the  multi-stage  incomplete-information 
Bayesian game model for defensive deception for advanced persistent threats (APTs).  We use 
the perfect Bayesian Nash equilibrium (PBNE) as the solution concept for the deception games 
and analyze the strategic equilibrium behaviors for both the deceivers and the deceivees.  

2.2 Introduction 

Deception is a technique used to cause animals [9], human [13, 35] or computer systems [3] to have 
false beliefs. The purpose of deception is to mislead the deceivees to behave against their interests 
but  favorably  to  the  deceiver.  It  is  a  fundamental  type  of  interactions  that  can  be  found  in 
applications ranging from biology [9] to criminology [35] and from economics [13] to the Internet 
of Things (IoT) [31]. Cyberspace creates particular opportunities for deception, since information 
lacks  permanence,  imputing  responsibility  is  difficult  [20],  and  some  agents  lack  repeated 
interactions [24]. For instance, online interactions are  vulnerable to identify theft [14] and spear 
phishing  [1],  and  authentication  in  the  IoT  suffers  from  a  lack  of  infrastructure  and  local 
computational resources [2]. 

 
 
 
 
 
 
 
Deception can be used as an approach for attacks. For example, phishing is a  typical  deception-
based attack that is one of the top threat vectors for cyberattacks [32]. Phishing can be email-based 
in which a phisher manipulates the email to appear as a legitimate request for sensitive information 
[4,  5].  It  can  also  be  website-based  in  which  the  deceiver  uses  genuine  looking  content  to 
camouflage a legitimate website to attract target deceivees to reveal their personal data such as 
credit  card  information  and  social  security  number.  Defenders  can  also  implement  deception. 
Defenders in the security and privacy domains have proposed, e.g., honeynets [7], moving target 
defense  [39],  obfuscation  [30],  and  mix  networks  [36].  Using  these  techniques,  defenders  can 
obscure valuable data such as personally identifiable information or the configuration of a network. 
Using these approaches, defenders can send false information to attackers to waste their resources or 
distract them from critical assets. They can also obscure valuable data such as sensitive information 
or the configuration of a network to avoid direct accesses from the attackers. Both malicious and 
defensive  deception  have  innumerable  implications  for  cybersecurity.  Successful  deception 
fundamentally depends on the information asymmetry  between  the  deceiver  and  the  deceivee. 
Deceivees make indirect observations of the true state and then make decisions. Deceivers can 
take advantage of this by pretending to be a trustworthy information provider. It is possible to fool, 
mislead, or confuse the deceivees. But the deceivers need to plan their strategies and take actions 
that may be costly. Therefore, successful deception also requires the deceivers to have the ability 
to acquire information, accurately understand the goals of the deceivees, and make the induced 
actions predictable. 

The deceivers strategically manipulate the private information to suit their own self-interests. The 
manipulated information is then revealed to the deceivees, who, on the other hand, make decisions 
based on the information received. It is important for the deceivee to form correct beliefs based on 
past observations, take into account the potential damage caused by deception, and strategically use 
the observed information for decision-making. If deception is necessary to achieve the deceiversâ€™ 
goal  that  would  cause  damages  to  the  deceivees,  the  deceivees  can  then  be  prepared  to  invest 
resources in detecting and denying the deceptions as well as recovering the damage.  

Modeling  deceptive  interactions  online  and  in  the  IoT  would  allow  government  policymakers, 
technological  entrepreneurs,  and  vendors  of  cyber-insurance  to  predict  changes  in  these 
interactions  for  the  purpose  of  legislation,  development  of  new  technology,  or  risk  mitigation. 
Game-theoretic  models  are  natural  frameworks  to  capture  the  adversarial  and  defensive 
interactions between players [11, 16, 22, 23, 37, 42, 50, 51, 52, 53]. It can provide a quantitative 
measure of the quality of protection with the concept of Nash equilibrium where both defender 
and an attacker seek optimal strategies, and no one has an incentive to deviate unilaterally from 
their equilibrium strategies despite their conflict for security objectives. The equilibrium concept 
also provides a quantitative prediction of the security outcomes of the scenario the game model 
captures.  With  the  quantitative  measures  of  security,  game  theory  makes  security  manageable 

 
 
 
beyond  the  strong  qualitative  assurances  of  cryptographic  protections.  Recently,  we  have  seen 
game-theoretic methods applied to deal with problems in cross-layer cyber-physical security [23, 
31, 40, 52, 54, 55], cyber deception [16, 27, 28, 29, 37, 53], moving target defense [39, 56, 57], 
critical infrastructure protection [19, 50, 51, 58, 59, 60, 61], adversarial machine learning [30, 62, 
63, 64, 65], insider threats [66, 67], and cyber risk management [68, 69, 70, 71]. 

This chapter shows a class of modeling of deception based on signaling games to provide a generic, 
quantitative,  and  systematic  understanding  of  deceptions  in  the  cyber-domain.  We  show  three 
variants of the model to illustrate the applications in different situations. We consider the cases when 
the deceivee is allowed to acquire knowledge through investigations or by deploying detectors. The 
baseline  signaling  game  model  is  extended  to  include  evidence  through  side-channel  knowledge 
acquisition. We also show a multi-stage Bayesian game with two-sided incomplete information and 
present a dynamic belief update and an iterative decision process that are used to develop long-
term optimal defensive policies to deter the deceivers and mitigate the loss. 

2.2.1  Related Work 

Deception game is related to a class of security games of incomplete information.  For  example, 
Powell in [34] has considered a game between an attacker and a defender, where the defender has 
private  information  about  the  vulnerability  of  their  targets  under  protection.  Powell models  the 
information  asymmetric  interactions between players by a signaling game, and finds a pooling 
equilibrium where the defender chooses to pool, i.e., allocate resources in the same way for all 
targets of different vulnerabilities, and the attacker cannot know the true level of vulnerability of 
all targets. Brown et al. [6] have studied a zero-sum game between an attacker and a defender in 
the scenario of ballistic missile positioning. They have introduced the incomplete information to 
investigate the value of secrecy by restricting the playersâ€™ access to information. 

Previous  literature  has  also  considered  deception  in  a  variety  of  scenarios  including proactive 
defense against advanced persistent threats [11,16,17,19,37], moving target defense [8, 39, 41], and 
social  engineering  [28,  29, 42].  HorÃ¡k  et  al.  [16]  have  considered  a  class  of  cyber  deception 
techniques in the field of network security and studied the impact of the deception on attackerâ€™s 
beliefs using the quantitative framework of the game theory by taking into account the sequential 
nature of the attack and investigating how attackerâ€™s belief evolves and influences the actions of 
the players. Zhang et al., [37] have proposed an equilibrium approach to analyze the GPS spoofing 
in a model of signaling game with continuous type space. They have found a PBNE with pooling 
in low types and separating in high types, and provided an equilibrium analysis of spoofing. The 
hypothesis testing game framework in [43] has studied the influence of deceptive information on 
the decision making and analyzed the worst-case scenario by constructing equilibrium strategies.  
The model proposed by Ettinger et al. [10] has used an equilibrium approach to belief deception 

 
 
 
 
 
in  bargaining  problems  when  the  agents  only  have  coarse  information  about  their  opponentâ€™s 
strategy.  

2.2.2  Organization of the Chapter 

The chapter is organized as follows. In Section 2.2, we briefly describe common game-theoretic 
approaches  for  security  models  and  introduce  the  basic  signaling  game  model  and  define  the 
perfect Bayesian Nash equilibrium (PBNE). In Section 2.3 we formulate the deception using a 
signaling  game  model  with  a  detector  over  a  binary  information  space,  and  describe  the 
equilibrium result. In Section 2.4, we present a signaling-game-based framework of a deception 
game to model the strategic behaviors over a continuous one-dimensional information space. We 
also consider the knowledge acquisition for the receiver through investigations. In Section 2.5, we 
show  a  multi-stage  Bayesian  game  framework  to  model  the  deception  in  advanced  persistent 
threats. Section 2.6 discusses the results and provides concluding remarks.  

2.3 Game Theory in Security 

Game theory is the systems science that studies interactions between rational and strategic players 
(or agents) that are coupled in their decision makings. Players are rational in the sense that they 
choose  actions  to  optimize  their  own  objectives  (e.g.,  utility  or  cost),  which  capture  varying 
interaction contexts. Being strategic in game theory refers to that players choose their own actions 
by anticipating the actions of the other agents. Their decision makings are coupled because their 
objective  functions  depend  both  on  their  own  actions,  and  on  the  actions  of  the  other  players. 
Among  the  game-theoretic  cybersecurity  models,  Stackelberg  game,  Nash  game,  and  signaling 
game account for the most commonly used approaches [28].  

Stackelberg games consist of a leader (L) and a follower (F). L has actions ğ‘"Ã	ğ´" and receives 
utility ğ‘ˆ", and F has actions ğ‘&Ã	ğ´& and receives utility ğ‘ˆ&. Once both players have taken actions, 
L  receives  utility ğ‘ˆ"(ğ‘", ğ‘&) and ğ‘ˆ&(ğ‘", ğ‘&).  In  Stackelberg  games,  F  acts  after  knowing  Lâ€™s 
action. Therefore, defensive cybersecurity models often take the defender as the leader and the 
attacker as the follower by considering the worst-case scenario that the attacker will observe and 
react to defensive strategies. Let ğ’«(ğ´) denote the power set of the set ğ´. Let ğµğ‘…&: ğ´" â†’ ğ’«(ğ´&) 
denote the best response of F to Lâ€™s action such that ğµğ‘…&(ğ‘") gives the optimal ğ‘& to respond to 
ğ‘". Best response can be one single action or a set of equally optimal actions. The function ğµğ‘…& is 
defined as 

ğµğ‘…& ğ‘" = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥

45âˆˆ75

	ğ‘ˆ& ğ‘", ğ‘& . 

 
 
 
 
 
 
 
 
By anticipating ğµğ‘…& ğ‘" , L chooses optimal action ğ‘"

âˆ— which satisfies  

âˆ— âˆˆ ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥

ğ‘"

45âˆˆ75

	ğ‘ˆ" ğ‘", ğµğ‘…& ğ‘"

. 

The  action  profile (ğ‘"
ğµğ‘…& ğ‘"

âˆ— . 

âˆ—, ğ‘&

âˆ— )  characterizes  the  equilibrium  of  the  Stackelberg  game,  where ğ‘&

âˆ— âˆˆ

In  Nash  games,  on  the  other  hand,  players  commit  to  his  or  her  own  strategy  and  move 
simultaneously or before knowing the other playerâ€™s action [28]. Let H and T denote two players 
in a Nash game with actions ğ‘: 	 âˆˆ 	 ğ´: and ğ‘; 	 âˆˆ 	 ğ´;, respectively. Define ğµğ‘…: ğ‘;  as the best 
response for H that optimally respond to Tâ€™s action ğ‘;. Similarly, let ğµğ‘…; ğ‘:  be the best response 
for  T.  Nash  equilibrium  is  the  solution  concept  of  such  games,  which  is  defined  by  a  strategy 
profile (ğ‘:

âˆ— ), where 

âˆ— , ğ‘;

âˆ— âˆˆ 	 ğµğ‘…: ğ‘; , 
ğ‘:

âˆ— âˆˆ 	 ğµğ‘…; ğ‘: . 
ğ‘;

In other words, Nash equilibrium requires each player to simultaneously choose a strategy that is 
optimal given the other playerâ€™s optimal strategy. In a pure-strategy Nash equilibrium, each player 
chooses one specific strategy (i.e., one pure strategy), while in a mixed-strategy equilibrium, at 
least on player randomizes over some or all pure strategies.  

A  signaling  game  is  a  two-player  dynamic  game  of  incomplete  information.  Signaling  game 
typically  names  two  players  as  sender  (S,  she)  and  receiver  (R,  he)  [12].  Signaling  game  is 
information asymmetric because the sender privately possesses some information that is unknown 
to the receiver. The private information is usually referred to as state (or type) of the world. The 
sender communicates the receiver by sending a message, and the receiver only learns about the state 
through the message. Generally, the degree of conflict of interest between S and R may range from 
perfectly aligned (e.g., Lewis signaling game [21]) to completely opposite (e.g., zero-sum game 
[33]). The timing of the game is described as follows: 

1.  Nature  randomly  draws  a  state  with  a  prior  common  to  both  players,  and  the  sender 

privately observes the state. 

2.  The sender sends a message to the receiver. 
3.  The receiver takes an action upon observing the message.  

One key feature of cyber deception is the multi-stage execution of the attack.  A deceiver has to 
engage the deceivee in multiple rounds of interactions to gain the trust. The dynamic interactions 
have been observed in APT threats and the operation of honey devices [19]. Hence signaling games 

 
 
 
 
 
 
 
 
provide a suitable description of essential features of the deception, and the basic signaling game 
models will be elaborated in the following section. 

2.3.1  Signaling Game Model 

Let ğœƒ âˆˆ ğ›© denote the state privately possessed by ğ‘† that is unknown to ğ‘…. The state space ğ›© can 
be  discrete  (e.g., ğ›©? â‰¡ {ğœƒB, ğœƒC})  or  continuous  (e.g., ğ›©E â‰¡ [ğœƒ, ğœƒ]).  For  example,  the  state  could 
represent, whether the sender is a malicious or benign actor, whether she has one set of preferences 
over another, samples of data stream, and location coordinate. For simplicity but without loss of 
generality, we focus on discrete state space in this introductory description of the signaling game. 
The  state  ğœƒ  is  drawn  according  to  a  prior  distribution  common  to  both  players.  Harsanyi 
conceptualized the state selection as a randomized move by a non-strategic player called nature. 
Let ğ‘ denote the probability mass function of the state, where 
(ğœƒ) = 1. All aspects of the 
game except the value of the true state ğœƒ are common knowledge. 

IâˆˆJK

ğ‘

After  privately  observing  the  state  ğœƒ ,  ğ‘†  chooses  a  message  ğ‘š âˆˆ ğ‘€ .  Let  ğœ O âˆˆ ğ›¤ O  denote  the 
behavioral  strategy  of ğ‘† to  choose ğ‘š based  on ğœƒ.  She  could  use  pure  strategy ğœ O(ğœƒ) as  well  as 
mixed  strategy  ğœ O(ğ‘š|ğœƒ) ,  such  that  ğœ O(ğœƒ)  chooses  message  ğ‘š  given  ğœƒ  and  ğœ O(ğ‘š|ğœƒ)  gives 
probability with which ğ‘† sends message ğ‘š given the state ğœƒ. We assume the pure strategy ğœ O(ğœƒ) 
induces a conditional probability ğ‘O(ğ‘š|ğœƒ) âˆˆ [0,1]. 

After  receiving ğ‘š, ğ‘… forms  a  posterior  belief ÂµU of  the  true  state  such  that ÂµU(Î¸|ğ‘š): Î˜ â†’ [0,1] 
gives the likelihood with which R believes that the true state is Î¸ given the message ğ‘š. Based on 
the belief ÂµU, ğ‘… then chooses an action a âˆˆ A according to a strategy ÏƒU âˆˆ Î“U. Similarly, ğ‘… may 
employ  pure  strategy ÏƒU(ğ‘š) or  mixed  strategy ÏƒU(ğ‘|ğ‘š),  where ÏƒU(ğ‘š) yields  the  action ğ‘…	acts 
upon  the  message m, and ÏƒU(ğ‘|ğ‘š) produces  the  probability  with  which ğ‘… takes  action ğ‘ given 
message m. The action a is the final decision of ğ‘… that represents the inference about the true state. 
Let ğ‘ˆO: â€‰Î˜Ã—MÃ—A â†’ â„ denote a utility function for ğ‘† such that ğ‘ˆO Î¸, ğ‘š, ğ‘  yields the utility of the 
player when her type is Î¸, she sends message ğ‘š, and ğ‘… plays action ğ‘. Similarly, let ğ‘ˆb: â€‰Î˜Ã—MÃ—A 
denote Râ€™s utility function so that ğ‘ˆb Î¸, ğ‘š, ğ‘  gives his payoff under the same scenario. 

2.3.2  Perfect Bayesian Nash Equilibrium 

In  two  player  games,  Nash  equilibrium  defines  a  strategy  profile  in  which  each  player  best 
responds to the optimal strategies of the other player. Signaling games motivate the extension of 
Nash equilibrium in two ways. First, information asymmetry requires ğ‘… to maximize his expected 
utility over the possible types of ğ‘†. An equilibrium in which ğ‘† and ğ‘… best respond to each otherâ€™s 

 
 
 
 
 
 
 
 
strategies given some belief ğœ‡b is called a Bayesian Nash equilibrium. Furthermore, ğ‘… is required 
to update ğœ‡b rationally. Perfect Bayesian Nash equilibrium (PBNE) captures this constraint and is 
described at Definition 1. 

Definition 2.1: (Perfect Bayesian Nash Equilibrium) A perfect Bayesian Nash equilibrium of a 
signaling profile (ğœ Oâˆ—, ğœbâˆ—) and a posterior belief system ğœ‡b such that: 

1.  (ğ¶C): âˆ€ğœƒ, ğœ Oâˆ— âˆˆ ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥fgğ‘ˆO(ğœƒ, ğœ O, ğœbâˆ—),  

2.  (ğ¶h): âˆ€ğ‘š âˆˆ ğ‘€, ğœbâˆ— âˆˆ ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥fi

I

ğœ‡b

(ğœƒ|ğ‘š)ğ‘ˆb(ğ‘š, ğœb, ğœƒ), 

and 

3.  ( ğ¶j ):  ğœ‡b(ğœƒ|ğ‘š) =

k(I)fgâˆ—(l|I)
kmâ€² (In)fgâˆ—(l|In)

,  if 

ğ‘In

(ğœƒo)ğœ Oâˆ—(ğ‘š|ğœƒo) > 0 ;  and  ğœ‡b(ğœƒ|ğ‘š)  is  any 

probability distribution on ğ›© if 

ğ‘In

(ğœƒo)ğœ Oâˆ—(ğ‘š|ğœƒo) = 0. 

In  Definition  2.1,  ğ¶C  and  ğ¶h  are  the  perfection  conditions  for  ğ‘†  and  ğ‘… ,  respectively,  that 
characterizes the sequential rationality of both players. Specifically, ğ¶C captures that ğ‘† optimally 
determines ğœ Oâˆ— by taking into account the effect of ğœ O on ğœb. ğ¶h says that ğ‘… responds rationally 
to ğ‘†â€™s strategy given his posterior belief about the state ğœƒ. ğ¶j states that the posterior belief is 
updated based on Bayesâ€™ rule. If the observation is a probability-0 event, then Bayesâ€™ rule is not 
applicable. In this case, any posterior beliefs over the state space is admissible. ğ¶j also implies 
that the consistency between the posterior beliefs and strategies: belief is updated depending on 
the strategy that is optimal given the belief. There are three categories of strategies: separating, 
pooling,  and  partially-pooling  equilibria,  which  are  defined  based  on  the  strategy  of  ğ‘† .  In 
separating PBNE (S-PBNE), ğ‘† chooses different strategies for different states. In this case, ğ‘… is 
able  to  identify  each  state  with  certainty.  In  pooling  PBNE  (P-PBNE), ğ‘† chooses  the  same 
strategy for different states. This strategy makes the corresponding message ğ‘š uninformative to 
ğ‘…. In partially-pooling PBNE (PP-PBNE), however, ğ‘† chooses messages with different, but not 
completely  distinguishable,  strategies  for  different  states.  This  makes  the  belief  of ğ‘… remain 
uncertain. 

2.4 Binary State Space: Leaky Deception using Signaling Game with Evidence 

In [27], Pawlick  et  al.  have  modeled  the  strategic  interactions  between  the  deceiver ğ‘† and  the 
deceivee ğ‘… over a binary information space by extending signaling games by including a detector 
that gives off probabilistic warnings called evidence when ğ‘† acts deceptively. 

 
 
 
 
 
 
 
S: ğœ ğ‘† ğ‘š ğœƒ)

type ğœƒ
âˆˆ {0,1}

ğœ† ğ‘’ ğœƒ, ğ‘š)

message ğ‘š
âˆˆ {0,1}

evidence ğ‘’
âˆˆ {0,1}

R: ğœğ‘… ğ‘ ğ‘š, ğ‘’ )

action ğ‘
âˆˆ {0,1}

Fig. 2.1: Signaling games with evidence add the red detector block to the S and 
R blocks. The probability Î» e Î¸, ğ‘š) of emitting evidence e depends on Sâ€™s type Î¸ 
and the message m that she transmits  [27]. 

2.4.1  Game-Theoretic Model 

With reference to Fig. 2.1, the detector emits evidences based on whether the message m is equal 
to the state Î¸. The detector emits ğ‘’ âˆˆ ğ¸ â‰¡ {0,1} by the probability Î» eâ€‰|â€‰Î¸, ğ‘š . Let e = 1 denote an 
alarm  and  e = 0  no  alarm.  The  evidence  e  is  assumed  to  be  emitted  with  an  exogeneous 
probability that neither ğ‘… nor ğ‘† can control. In this respect, the detector can be seen as a second 
move by nature. Let Î² âˆˆ [0,1] be the true-positive rate of the detector. For simplicity, both true-
positive rates are set to be equal: Î² = Î»(1â€‰|â€‰0,1) = Î»(1â€‰|â€‰1,0). Similarly, let Î± âˆˆ [0,1] denote the 
false-positive  rate  of  the  detector  with Î± = Î»(1â€‰|â€‰0,0) = Î»(1â€‰|â€‰1,1). A  valid  detector  has Î² â‰¥ Î±. 
This is without loss of generality, because otherwise Î± and Î² can be relabeled. The timing of the 
game becomes: 

1.  Nature randomly draws state Î¸ âˆˆ {0,1} according to p(Î¸).  

2.   ğ‘† privately observes Î¸ and then chooses a message m based on strategy Ïƒy(ğ‘š|Î¸).  

3.  The detector emits evidence e âˆˆ {0,1} with Î»(e|Î¸, ğ‘š).  

4.  After  receiving  both ğ‘š and ğ‘’, ğ‘… forms  a  belief  system ğœ‡b(ğœƒ|ğ‘š, ğ‘’) and  then  chooses  an 

action ğ‘ âˆˆ {0,1} according to strategy ğœb(ğ‘|ğ‘š, ğ‘’). 

The following assumptions characterizes a cheap-talk signaling game with evidence. The message 
ğ‘š is payoff-irrelevant in a cheap-talk signaling game. 

Assumption 2.1: The utilities ğ‘ˆO and ğ‘ˆb satisfy the following assumptions: 

1.  ğ‘ˆO and ğ‘ˆb do not depend exogenously on ğ‘š.  

 
 
 
 
 
 
2.  âˆ€ğ‘š, ğ‘š âˆˆ ğ‘€, ğ‘ˆb 0, ğ‘š, 0 > ğ‘ˆb 0, ğ‘š , 1 .  

3.  âˆ€ğ‘š, ğ‘š âˆˆ ğ‘€, ğ‘ˆb 1, ğ‘š, 0 < ğ‘ˆb 1, ğ‘š , 1 .  

4.  âˆ€ğ‘š, ğ‘š âˆˆ ğ‘€, ğ‘ˆO 0, ğ‘š, 0 < ğ‘ˆO 0, ğ‘š , 1 .  

5.  âˆ€ğ‘š, ğ‘š âˆˆ ğ‘€, ğ‘¢O 1, ğ‘š, 0 > ğ‘¢O 1, ğ‘š , 1 . 

Assumption 2.1-1 implies that the interaction is a cheap-talk signaling game. Assumption 2.1-2 
and 2.1-3 state that ğ‘… receives higher utility if he correctly chooses ğ‘ = ğœƒ than if he chooses ğ‘ â‰ 
ğœƒ. Finally, Assumption 2.1-4 and 2.1-5 say that ğ‘† receives higher utility if ğ‘… chooses ğ‘ â‰  ğœƒ than 
if he chooses ğ‘ = ğœƒ. 

: ğ›¤Ã—ğ›¤b â†’ â„ such  that ğ‘ˆ(ğœ O, ğœb|ğœƒ) gives  the 
Utilities.  Define  an  expected  utility  function ğ‘ˆ
expected utility to ğ‘† when she plays strategy ğœ O given that the state is ğœƒ. This expected utility is 
given by 

O

O

ğ‘ˆ

ğœ O, ğœbâ€‰|â€‰ğœƒ =

ğœb ğ‘â€‰|â€‰ğ‘š, ğ‘’ ğœ† ğ‘’â€‰|â€‰ğœƒ, ğ‘š ğœ O ğ‘šâ€‰|â€‰ğœƒ ğ‘ˆO ğœƒ, ğ‘š, ğ‘

.														(2.1)

4âˆˆ7

(cid:127)âˆˆ(cid:128)

lâˆˆ~

The involvement of evidence ğ‘’ in Eq. (2.1) is due to the dependence of ğ‘…â€™s strategy ğœb(ğ‘|ğ‘š, ğ‘’). 
ğ‘† must  anticipate  the  probability  of  leaking  evidence ğ‘’ by  using ğœ†(ğ‘’|ğœƒ, ğ‘š).  Similarly,  define 

b

b

: ğ›¤ â†’ â„ such  that ğ‘ˆ

ğ‘ˆ
given message ğ‘š, evidence ğ‘’, and state ğœƒ. The expected utility function is given by 

(ğœb|ğœƒ, ğ‘š, ğ‘’) gives  the  expected  utility  to ğ‘… when  he  plays  strategy ğœb 

b

ğ‘ˆ

(ğœbâ€‰|â€‰ğœƒ, ğ‘š, ğ‘’) =

ğœb

ğ‘â€‰|â€‰ğ‘š, ğ‘’ ğ‘ˆb ğœƒ, ğ‘š, ğ‘ . 

4âˆˆ7

2.4.2  Equilibrium Concept 

The involvement of the evidence extends the PBNE in Definition 2.2 as follows.  

Definition 2.2: (PBNE with Evidence) A PBNE of a cheap-talk signaling game with evidence is 
a strategy profile (ğœ Oâˆ—, ğœbâˆ—) and posterior beliefs ğœ‡b(ğœƒâ€‰|â€‰ğ‘š, ğ‘’) such that 

âˆ€ğœƒ âˆˆ ğ›©,	 ğœ Oâˆ— âˆˆ ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥

â€‰ğ‘ˆ

O

ğœ O, ğœbâˆ—â€‰|â€‰ğœƒ ,																		(2.2) 

fgâˆˆ(cid:130)g

âˆ€ğ‘š âˆˆ ğ‘€,	 âˆ€ğ‘’ âˆˆ ğ¸, 

 
 
 
 
 
ğœbâˆ— âˆˆ ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥

fiâˆˆ(cid:130)i

IâˆˆJ

ğœ‡b

ğœƒâ€‰|â€‰ğ‘š, ğ‘’ ğ‘ˆ

b

ğœbâ€‰ â€‰ğœƒ, ğ‘š, ğ‘’ ,			(2.3) 

and if 

IâˆˆJ (ğ‘’â€‰|â€‰ğœƒ , ğ‘š)ğœ O(ğ‘šâ€‰|â€‰ğœƒ)ğ‘(ğœƒ) > 0, then 

ğœ†

ğœ‡b ğœƒâ€‰|â€‰ğ‘š, ğ‘’ =

ğœ† ğ‘’â€‰|â€‰ğœƒ, ğ‘š ğœ‡b ğœƒâ€‰|â€‰ğ‘š

ğœ†

ğ‘’â€‰|â€‰ğœƒ , ğ‘š ğœ‡b ğœƒ â€‰|â€‰ğ‘š

IâˆˆJ

,												(2.4) 

where 

ğœ‡b ğœƒâ€‰|â€‰ğ‘š =

ğœ O ğ‘šâ€‰|â€‰ğœƒ ğ‘ ğœƒ

.																											(2.5) 

ğ‘šâ€‰|â€‰ğœƒ ğ‘ ğœƒ
IâˆˆJ (ğ‘’â€‰|â€‰ğœƒ , ğ‘š)ğœ O(ğ‘šâ€‰|â€‰ğœƒ)ğ‘(ğœƒ) = 0,  then  ğœ‡b ğœƒâ€‰|â€‰ğ‘š, ğ‘’  may  be  set  to  any  probability 

IâˆˆJ

ğœO

If 
ğœ†
distribution over ğ›©. 

Eq. (2.4)-(2.5) extend ğ¶j in Definition 2.2. First, ğ‘… updates her belief according to ğ‘š using Eq. 
(2.5); then ğ‘… updates her belief according to ğ‘’ using Eq. (2.4). When ğ‘† plays pooling strategy, i.e., 
âˆ€ğ‘š âˆˆ ğ‘€, ğœ O(ğ‘š|0) = ğœ O(ğ‘š|1). In this case, the message ğ‘š is uninformative, and ğ‘… updates his 
belief only depends on the evidence ğ‘’, i.e., 

ğœ‡b ğœƒâ€‰|â€‰ğ‘š, ğ‘’ =

ğœ† ğ‘’â€‰|â€‰ğœƒ, ğ‘š ğ‘ ğœƒ

ğœ†

ğ‘’â€‰|â€‰ğœƒ , ğ‘š ğ‘ ğœƒ

IâˆˆJ

.																						(2.6) 

2.4.3  Equilibrium Results 

Under  Assumption  2.1-1  to  2.1-5,  the  cheap  talk  signaling  game  with  evidence  admits  no 
separating PBNE. This results from the opposing utility functions of S and R. S wants to deceive 
R, and R wants to correctly guess the type. It is not incentive-compatible for S to fully reveal the 
type by choosing a separating strategy. For brevity, define the following notations: 

U â‰œ uU Î¸ = 0, ğ‘š, ğ‘ = 0 âˆ’ uU Î¸ = 0, ğ‘š, ğ‘ = 1 , 
Î”B

U â‰œ uU Î¸ = 1, ğ‘š, ğ‘ = 1 âˆ’ uU Î¸ = 1, ğ‘š, ğ‘ = 0 . 
Î”C

U gives the benefit to R for correctly guessing the type when Î¸ = 0, and Î”C
U gives the benefit to 
Î”B
R for correctly guessing the type when Î¸ = 1. Lemmas 2.1-2.2 solve for ÏƒUâˆ— within five regimes 
of the prior probability p(Î¸) of each type Î¸ âˆˆ {0,1}. 

â€‰
 
 
 
 
ğ›½ < 1 âˆ’ ğ›¼
(Conservative)

ğ‘…
ğ›¼Î”0
ğ‘…
ğ‘… + ğ›½Î”1

ğ›¼Î”0

ğ‘…
(1 âˆ’ ğ›½)Î”0

ğ‘…
(1 âˆ’ ğ›¼)Î”0

(1 âˆ’ ğ›½)Î”0

ğ‘…
ğ‘… + 1 âˆ’ ğ›¼ Î”1

(1 âˆ’ ğ›¼)Î”0

ğ‘…
ğ‘… + (1 âˆ’ ğ›½)Î”1

ğ‘…
ğ›½Î”0
ğ‘…
ğ‘… + ğ›¼Î”1

ğ›½Î”0

Zero-Dominant

Zero-Heavy

Middle

One-Heavy

One-Dominant

ğ‘(1) = 0

ğ›½ > 1 âˆ’ ğ›¼
(Aggressive)

ğ‘…
(1 âˆ’ ğ›½)Î”0

1 âˆ’ ğ›½ Î”0

ğ‘…
ğ‘… + 1 âˆ’ ğ›¼ Î”1

ğ‘…
ğ›¼Î”0
ğ‘…
ğ‘… + ğ›½Î”1

ğ›¼Î”0

ğ‘…
ğ›½Î”0
ğ‘…
ğ‘… + ğ›¼Î”1

ğ›½Î”0

ğ‘…
(1 âˆ’ ğ›¼)Î”0

1 âˆ’ ğ›¼ Î”0

ğ‘…
ğ‘… + 1 âˆ’ ğ›½ Î”1

ğ‘(1) = 1

Fig.  2.2:  PBNE  differ  within  five  prior  probability  regimes.  In  the  Zero-

Dominant  regime,  p Î¸ = 1 â‰ˆ 0  i.e.,  type  0  dominates.  In  the  Zero-Heavy 
regime, p(Î¸ = 1) is slightly higher, but still low. In the Middle regime, the types 
are mixed almost evenly. The One- Heavy regime has a higher p Î¸ = 1 , and 

the  One-Dominant  regime  has  p Î¸ = 1 â‰ˆ 1.  The  definitions  of  the  regime 
boundaries depend on whether the detector is conservative or aggressive. 

Lemma 2.1: For pooling PBNE, ğ‘…â€™s optimal actions ğœbâˆ— for evidence ğ‘’ and messages ğ‘š on the 
equilibrium path1 vary within five regimes of ğ‘(ğœƒ). The top half of Fig. 2.2 lists the boundaries 
of  these  regimes  for  detectors  in  which ğ›½ < 1 âˆ’ ğ›¼, and  the  bottom  half  of  Fig.  2.2  lists  the 
boundaries of these regimes for detectors in which ğ›½ > 1 âˆ’ ğ›¼. 

The  regimes  in  Fig.  2.2  shift  towards  the  right  as Î”B
necessary to balance out the benefit to R for correctly identifying a type Î¸ = 0 as Î”B
The regimes shift towards the left as Î”C

U  increases.  Intuitively,  a  higher p(1) is 
U increases. 

U increases for the opposite reason. 

Lemma 2.2 gives the optimal strategies of R in response to pooling behavior within each of the 
five parameter regimes. 

Lemma 2.2: For each regime, ğœbâˆ— on the equilibrium path is listed in Table 2.1 if ğ›½ < 1 âˆ’ ğ›¼ 
and in Table 2.2 if ğ›½ > 1 âˆ’ ğ›¼. The row labels correspond to the Zero-Dominant (O-D), Zero-
Heavy (0-H), Middle, One-Heavy (1-H), and One-Dominant (1-D) regimes. 

Table 2.1:  ğœbâˆ—(1â€‰|ğ‘š, ğ‘’) in Pooling PBNE with ğ›½ > 1 âˆ’ ğ›¼. 

ğœbâˆ—(1â€‰|â€‰0,0)  ğœbâˆ—(1â€‰|â€‰0,1)  ğœbâˆ—(1â€‰|â€‰1,0)  ğœbâˆ—(1â€‰|â€‰1,1) 

0-D 

0 

0 

0 

0 

1 In pooling PBNE, the message â€œon the equilibrium pathâ€ is the one that is sent by both types of 
ğ‘†. Messages â€œoff the equilibrium pathâ€ are never sent in equilibrium, although determining the 
actions that ğ‘… would play if ğ‘† were to transmit a message off the path is necessary in order to 
determine the existence of equilibria. 

 
 
 
 
 
 
 
 
 
																																																								
0-H 

Middle 

1-H 

1-D 

0 

0 

1 

1 

1 

1 

1 

1 

0 

1 

1 

1 

0 

0 

0 

1 

Table 2.2: ğœbâˆ—(1â€‰|ğ‘š, ğ‘’) in Pooling PBNE with ğ›½ > 1 âˆ’ ğ›¼. 

ğœbâˆ—(1â€‰|â€‰0,0)  ğœbâˆ—(1â€‰|â€‰0,1) 

ğœbâˆ—(1â€‰|â€‰1,0)  ğœbâˆ—(1â€‰|â€‰1,1) 

0-D 

0-H 

Middle 

1-H 

1-D 

0 

0 

0 

0 

1 

0 

0 

1 

1 

1 

0 

1 

1 

1 

1 

0 

0 

0 

1 

1 

Lemmas 2.3-2.4 give conditions under which the beliefs ğœ‡b exist such that each pooling strategy 
is optimal for both types of ğ‘†. 

Lemma  2.3:  Let ğ‘š be  the  message  on  the  equilibrium  path.  If ğœbâˆ—(1â€‰|â€‰ğ‘š, 0) = ğœbâˆ—(1â€‰|â€‰ğ‘š, 1), 
then there exists a ğœ‡b such that pooling on message ğ‘š is optimal for both types of ğ‘†. For brevity, 
let ğ‘âˆ— â‰œ ğœbâˆ—(1â€‰|â€‰ğ‘š, 0) = ğœbâˆ—(1â€‰|â€‰ğ‘š, 1). Then ğœ‡b is given by, 

âˆ€ğ‘’ âˆˆ ğ¸,	 ğœ‡b ğœƒ = ğ‘âˆ—â€‰|â€‰1 âˆ’ ğ‘š, ğ‘’ â‰¥

b
ğ›¥C(cid:144)4âˆ—

b
ğ›¥C(cid:144)4âˆ—

b . 
+ ğ›¥4âˆ—

Lemma 2.4: If ğœbâˆ—(1â€‰|â€‰ğ‘š, 0) = 1 âˆ’ ğœbâˆ—(1â€‰|â€‰ğ‘š, 1) and ğ›½ â‰  1 âˆ’ ğ›¼, then there does not exist a ğœ‡b 
such that pooling on message ğ‘š is optimal for both types of ğ‘†. 

ğ›½ < 1 âˆ’ ğ›¼
(Conservative)

S Pool on 0 (cid:198) R Plays 0
S Pool on 1 (cid:198) R Plays 0

â€“
S Pool on 1 (cid:198) R Plays 0

Zero-Dominant

Zero-Heavy

ğ‘(1) = 0

ğ›½ > 1 âˆ’ ğ›¼
(Aggressive)

S Pool on 0 (cid:198) R Plays 0
S Pool on 1 (cid:198) R Plays 0

S Pool on 0 (cid:198) R Plays 0
â€“

â€“
â€“

Middle

â€“
â€“

S Pool on 0 (cid:198) R Plays 1
â€“

S Pool on 0 (cid:198) R Plays 1
S Pool on 1 (cid:198) R Plays 1

One-Heavy

One-Dominant

â€“
S Pool on 1 (cid:198) R Plays 1

S Pool on 0 (cid:198) R Plays 1
S Pool on 1 (cid:198) R Plays 1

ğ‘(1) = 1

Fig. 2.3: PBNE  in  each  of  the  parameter  regions  defined  in  Fig. 2.2.  For m âˆˆ

{0,1},  S  Pool  on mâ€Pool  on  ach	Ïƒ

Oâˆ—

ğ‘š 0 =	 Ïƒ

Oâˆ—

plays  aâ€plays  n  ach Ïƒ

bâˆ—

ğ‘ 0,0 =	 Ïƒ

bâˆ—

ğ‘ 0,1 = Ïƒ

ğ‘š 1 = 1.  For 	ğ‘ âˆˆ {0,1},  R 
bâˆ—
bâˆ—

ğ‘ 1,0 =	 Ïƒ

ğ‘ 1,1 = 1. 

 
 
 
 
 
 
 
 
bâˆ—

. The Dominant regimes support pooling PBNE on both 
Lemma 2.3 gives Î¼
messages. The Heavy regimes support pooling PBNE on only one message. The 
Middle regime does not support any pooling PBNE. 

The pooling PBNE of the cheap-talk signaling game with evidence are summarized by Fig. 2.3. 
For Î² â‰  1 âˆ’ Î±, the Middle regime does not admit any pooling PBNE. This result is not found in 
conventional  signaling  games  for  deception,  in  which  all  regimes  support  pooling  PBNE.  It 
occurs  because Râ€™s  responses  to  message m depends  on e, i.e., ÏƒUâˆ—(1â€‰|â€‰0,0) = 1 âˆ’ ÏƒUâˆ—(1â€‰|â€‰0,1) 
and ÏƒUâˆ—(1â€‰|â€‰1,0) = 1 âˆ’ ÏƒUâˆ—(1â€‰|â€‰1,1). One of the types of S prefers to deviate to the message off 
the equilibrium path. Intuitively, for a conservative detector, S with type Î¸ = ğ‘š prefers to deviate 
to message 1 âˆ’ m, because his deception is unlikely to be detected. On the other hand, for an 
aggressive  detector, S with  type Î¸ = 1 âˆ’ ğ‘š prefers  to  deviate  to  message 1 âˆ’ ğ‘š, because  his 
honesty is likely to produce a false-positive alarm, which will lead R to guess ğ‘ = ğ‘š. 

For ğ›½ â‰  1 âˆ’ ğ›¼, since the Middle regime does not support pooling PBNE, we search for partially-
separating PBNE. In these PBNE, ğ‘† and ğ‘… play mixed strategies. In mixed-strategy equilibria in 
general, each player chooses a mixed strategy that makes the other players indifferent between 
the actions that they play with positive probability. Theorems 2.1-2.2 give the results. 

Theorem 2.1: (Partially-Separating PBNE for Conservative Detectors) For ğ›½ < 1 âˆ’ ğ›¼, within 
the Middle Regime, there exists an equilibrium in which the sender strategies are 

ğœ Oâˆ— ğ‘š = 1â€‰|â€‰ğœƒ = 0 =

ğœ Oâˆ— ğ‘š = 1â€‰|â€‰ğœƒ = 1 =

ğ›½h
ğ›½h âˆ’ ğ›¼h âˆ’
b
ğ›¼ğ›½ğ›¥B
b
ğ›½h âˆ’ ğ›¼h ğ›¥C

b
ğ›¼ğ›½ğ›¥C
b
ğ›½h âˆ’ ğ›¼h ğ›¥B
1 âˆ’ ğ‘(1)
ğ‘(1)

ğ‘(1)
1 âˆ’ ğ‘(1)

,

âˆ’

ğ›¼h
ğ›½h âˆ’ ğ›¼h ,

the receiver strategies are 

ğœbâˆ—(ğ‘ = 1â€‰|â€‰ğ‘š = 0, ğ‘’ = 0) =â€‰

1 âˆ’ ğ›¼ âˆ’ ğ›½
2 âˆ’ ğ›¼ âˆ’ ğ›½

,

ğœbâˆ—(ğ‘ = 1â€‰|â€‰ğ‘š = 0, ğ‘’ = 1) =â€‰ 1,

ğœbâˆ—(ğ‘ = 1â€‰|â€‰ğ‘š = 1, ğ‘’ = 0) =â€‰

1
2 âˆ’ ğ‘ âˆ’ ğ‘

,

ğœbâˆ—(ğ‘ = 1â€‰|â€‰ğ‘š = 1, ğ‘’ = 1) =â€‰ 0,

and the beliefs are computed by Bayesâ€™ Law in all cases. 

Theorem 2.2: (Partially-Separating PBNE for Aggressive Detectors) For any ğ‘” âˆˆ [0,1], let ğ‘” â‰œ

 
 
 
 
 
 
 
 
1 âˆ’ ğ‘”. For ğ›½ > 1 âˆ’ ğ›¼, within the Middle Regime, there exists an equilibrium in which the sender 
strategies are 

ğœ Oâˆ— ğ‘š = 1â€‰|â€‰ğœƒ = 0 =

ğœ Oâˆ— ğ‘š = 1â€‰|â€‰ğœƒ = 1 =

b
ğ›¼ğ›½ğ›¥C
h
ğ›¼h âˆ’ ğ›½
ğ›¼h
ğ›¼h âˆ’ ğ›½

h âˆ’

ğ‘(1)
1 âˆ’ ğ‘(1)

b
ğ›¥B

b
ğ›¼ğ›½ğ›¥B
h
ğ›¼h âˆ’ ğ›½

b
ğ›¥C

âˆ’

h

ğ›½
ğ›¼h âˆ’ ğ›½

h ,

1 âˆ’ ğ‘(1)
ğ‘(1)

,

the receiver strategies are 

ğœbâˆ—(ğ‘ = 1â€‰|â€‰ğ‘š = 0, ğ‘’ = 0) =â€‰ 0,

ğœbâˆ—(ğ‘ = 1â€‰|â€‰ğ‘š = 0, ğ‘’ = 1) =â€‰

1
ğ›¼ + ğ›½

,

ğœbâˆ—(ğ‘ = 1â€‰|â€‰ğ‘š = 1, ğ‘’ = 0) =â€‰ 1,

ğœbâˆ—(ğ‘ = 1â€‰|â€‰ğ‘š = 1, ğ‘’ = 1) =â€‰

ğ›¼ + ğ›½ âˆ’ 1
ğ›¼ + ğ›½

,

and the beliefs are computed by Bayesâ€™ Law in all cases. 

In  Theorem  2.1, ğ‘† chooses  the ğœ Oâˆ— that  makes ğ‘… indifferent  between ğ‘ = 0 and ğ‘ = 1 when  he 
observes the pairs (ğ‘š = 0, ğ‘’ = 0) and (ğ‘š = 1, ğ‘’ = 0). This allows ğ‘… to choose mixed strategies 
for ğœbâˆ—(1â€‰|â€‰0,0) and ğœbâˆ—(1â€‰|â€‰1,0). Similarly, ğ‘…  chooses ğœbâˆ—(1â€‰|â€‰0,0) and ğœbâˆ—(1â€‰|â€‰1,0) that  make 
both  types  of ğ‘† indifferent  between  sending ğ‘š = 0 and ğ‘š = 1. This  allows ğ‘† to  choose  mixed 
strategies. A similar pattern follows in Theorem 2.2 for ğœ Oâˆ—, ğœbâˆ—(1â€‰|â€‰0,1), and ğœbâˆ—(1â€‰|â€‰1,1). 

2.5 Continuous State Space: Knowledge Acquisition and Fundamental Limits of Deception 

In [38], Zhang et al. proposes a game-theoretic framework of a deception game to model the strategic 
behaviors of the deceiver S and the deceivee R and construct strategies for both attacks and defenses 
over a continuous one-dimensional state space. R is allowed to acquire probabilistic evidence about 
the deception through investigations, and misrepresenting the state is costly for S. The deceivability 
of the deception game is analyzed by characterizing the PBNE. 

2.5.1  Game-Theoretic Model 

We assume that the state Î¸ is continuously distributed over Î˜ â‰¡ [ğœƒ, Î¸] according to a differentiable 
probability distribution F(Î¸), with strictly positive density f(Î¸) for all Î¸ âˆˆ Î˜. Again, all aspects of 
the game except the value of the true state Î¸ are common knowledge. 

 
 
 
 
 
 
 
Message  and  Report.  In  this  game  model,  we  use  the  message  to  describe  the  format  of 
information about the state S communicates to R. We introduce a notion report to represent the 
value of state carried by the message. After privately observing the state Î¸, S first determines a 
report r âˆˆ Î˜ for the true state Î¸, and then sends R a message ğ‘š âˆˆ M, where M is a Borel space of 
messages. Let Î©: M â†’ Î˜ denote the report interpretation function such that Î©(ğ‘š) gives the report 
r carried in ğ‘š. Given the true state Î¸, we say m tells the truth if Î©(ğ‘š) = Î¸. We assume that for 
each state Î¸ âˆˆ Î˜, there is a sufficiently large number of messages that yields the same report, and 
each ğ‘š âˆˆ M has  a  unique  value  of  report Î©(ğ‘š).  In  other  words,  the  message  space  can  be 
partitioned  as M =âˆª(cid:152) M(cid:152) ,  with |M(cid:152)| â†’ âˆ for  all r and M(cid:152) âˆ© M(cid:152)o = âˆ… if r â‰  râ€²,  and âˆ€ğ‘š âˆˆ M(cid:152) , 
Î©(ğ‘š) = r. This assumption can capture the feature of rich language in practical deceptions. We 
further assume that message ğ‘š is formed by â€œcommon languageâ€ that can be understood precisely 
by both S and R. In other words, function Î© is commonly known by both players. 

Strategies  and  actions.  Let Ïƒy: Î˜ â†’ Î˜ be  the  strategy  of S such  that r = Ïƒy(Î¸) determines  the 
report r of the true state Î¸. Let Î·y: Î˜Ã—Î˜ â†’ M be the message strategy of S associated with Ïƒy such 
that ğ‘š = Î·y(r) selects  the  message ğ‘š from M(cid:152) when  the  strategy Ïƒy(Î¸) determines  the  report r 
and the true state is Î¸. Given Î¸, the strategy Ïƒy(Î¸) determines the set of messages M(cid:158)(cid:159)((cid:160)) for Î·y to 
choose from, and Î·y determines which specific message ğ‘š âˆˆ M(cid:158)(cid:159)((cid:160)) to send. We assume Ïƒy(Î¸) 
associated  with Î·y induces  a  conditional  probability qy(ğ‘š|Î¸).  After  receiving m, R chooses  an 
action  a âˆˆ A â‰¡ Î˜  according  to  a  strategy  ÏƒU: Î˜Ã—M â†’ A  using  r = Î©(ğ‘š) .  ÏƒU(r, ğ‘š)  gives  the 
action R acts upon the message m (and thus r = Î©(ğ‘š)). The action a is the final decision of R that 
represents the inference about the true state. 

Utilities.  The  utility  functions  of  S  is  given  by  Uy(ğ‘, Î¸, r) â‰¡ UÂ£(a, Î¸) âˆ’ kUÂ¥(r, Î¸)  where 
UÂ£(ğ‘, Î¸) â‰¡ âˆ’(ğ‘ âˆ’ (Î¸ + b))h is  the  utility  depending  on  the  induced  action a in R, UÂ¥ â‰¡ âˆ’(r âˆ’
Î¸)h is the utility related to the misrepresentation of the true state, and k â‰¥ 0 quantifies the intensity 
of UÂ¥. On the deceiveeâ€™s side, his utility is given by UU â‰¡ âˆ’(ğ‘ âˆ’ Î¸)h, which takes into account 
the  risk  induced  by Râ€™s  misinference  of  the  true  state Î¸ via  his  action ğ‘.  Define,  for  all Î¸ âˆˆ Î˜, 
Î±y(Î¸) â‰¡ argmin4Cy(ğ‘, Î¸, r),  and  Î±U(Î¸) â‰¡ argmin4CU(ğ‘, Î¸);  i.e.,  Î±U(Î¸)  and  Î±y(Î¸)  are  two 
actions taken by R as functions of Î¸ that are the most preferred by R and S, respectively. 

Beliefs.  Based  on m (and  thus r = Î©(ğ‘š))  and  his  prior  belief f(Î¸), R forms  a  posterior  belief 
ÂµU: Î˜ â†’ [0,1] of  the  true  state Î¸ âˆˆ Î˜.  The  posterior  belief ÂµU(Î¸|ğ‘š) gives  the  likelihood  with 
which R believes  that  the  true  state  is Î¸ based  on m. R then  determines  which  action  to  choose 
based on his belief ÂµU. 

2.5.2  Deceivability 

 
 
 
 
 
We  restrict  attention  to  a  class  of  monotone  inflated  deception,  in  which  the  strategy  profile 
(Ïƒy, ÏƒU) satisfies conditions in the following definition. 

Definition 2.3: A deception with ğ‘†â€™s strategy ğœ O and ğ‘…â€™s belief ğœ‡b is monotone if 

1.  ğœ O(ğœƒ) is a non-decreasing function of ğœƒ; 

2.  ğœb(ğ‘Ÿ, ğ‘š) is a non-decreasing function of ğ‘Ÿ. 

The deceivability can be quantified as follows. 

Definition  2.4:  Given  the  state  ğœƒ âˆˆ [ğœƒâ€¹, ğœƒo] ,  ğ‘† â€™s  strategy  ğœ O(ğœƒ) = ğ‘Ÿ ,  and  message  strategy 
ğœ‚O(ğ‘Ÿ) = ğ‘š, 

â€“  ğ‘…  is  undeceivable  over  [ğœƒâ€¹, ğœƒo]  if  ğœb(ğ‘Ÿ, ğ‘š) = ğ›¼b(ğœƒ) â‰¡ ğœƒ ,  for  all  ğœƒ âˆˆ [ğœƒâ€¹, ğœƒ,] .  Here, 
ğœ O(ğœƒ) â‰  ğœ O(ğœƒâ€²)  for  all  ğœƒ â‰  ğœƒâ€²âˆˆ [ğœƒâ€¹, ğœƒo] ,  and  ğœ‚O(ğ‘Ÿ) âˆˆ ğ‘€ï¬ .  The  corresponding  ğœ‡b  is 
informative. The interval [ğœƒâ€¹, ğœƒo] is called undeceivable region (UR). 

â€“  ğ‘… is  deceivable  over [ğœƒâ€¹, ğœƒo] if  the  only  knowledge  she  has  is  that ğœƒ lies  in [ğœƒâ€¹, ğœƒo]. ğ‘… 

chooses ğœb(ğ‘Ÿ, ğ‘š) = ğ‘

b

(ğœƒâ€¹, ğœƒo), by maximize the expected utility over [ğœƒâ€¹, ğœƒo], i.e., 

b

ğ‘

(ğœƒâ€¹, ğœƒo) âˆˆ argmax
fiâˆˆ7

I(cid:176)

In

ğ‘ˆb

(ğœb, ğœƒ)ğ‘“(ğœƒ)ğ‘‘ğœƒ. 

Here, ğœ O(ğœƒ) and ğœ‚O(ğ‘Ÿ), respectively, choose the same report ğ‘Ÿ and the same message ğ‘š, 
for all ğœƒ âˆˆ [ğœƒâ€¹, ğœƒo]. Thus, given ğ‘š, ğ‘O(ğ‘š|ğœƒ) is the same for all ğœƒ âˆˆ [ğœƒâ€¹, ğœƒo], where ğ‘ âˆˆ
(0,1). The corresponding ğœ‡b is uninformative. The interval [ğœƒâ€¹, ğœƒo] is called deceivable 
region (DR). 

2.5.3  Knowledge Acquisition: Evidence 

We  allow ğ‘… to  acquire  additional  knowledge  through  investigations  when  the  state  is  in  a  DR, 
[ğœƒâ€¹, ğœƒo],  by  partitioning  it  into  multiple  intervals,  denoted  by  a  strictly  increasing  sequence, <
ğœƒB = ğœƒâ€¹, ğœƒC, â€¦ , ğœƒÂ· = ğœƒo >.  Then ğ‘… conducts  investigations  for  each  interval.  Here,  we  consider 
the case when there are two investigation intervals to simplify the analysis. Let ğœ(cid:181) âˆˆ (ğœƒâ€¹, ğœƒo) be 
the  investigation  partition  state  such  that  [ğœƒâ€¹, ğœƒo]  is  partitioned  into  two  non-overlapping 
investigation  regions ğ›©B = [ğœƒâ€¹, ğœƒ(cid:181)] and ğ›©C = [ğœƒ(cid:181), ğœƒo].  Let ğ›¹ âˆˆ ğ›¤ = {ğ›¹B, ğ›¹C},  where ğ›¹â€¢ denote 

the event {ğœƒ âˆˆ ğ›©â€¢}, for ğ‘– = 0, 1, with the probability ğ‘ƒ(ğ›¹â€¢) =

â€

(I)â€°I

Â»â€¦

[m(cid:176),mn] (I)â€°I
â€

. The investigation for 

ğ›©B  and ğ›©C  generates  noisy  evidence ğ‘’ âˆˆ ğ¸ = {0,1},  where ğ‘’ = ğ‘–  represents ğ›¹â€¢ ,  for ğ‘– = 0, 1. 

 
 
 
 
 
 
Suppose  that  the  investigation  emits  evidence  by  the  probability  ğ›¾(ğ‘’|ğ›¹, ğ‘š) .  Let  ğ‘¥ = ğ›¾(ğ‘’ =
0|ğ›¹B, ğ‘š) and ğ‘¦ = ğ›¾(ğ‘’ = 1|ğ›¹C, ğ‘š) be the two true positive rates, which are private information 
of ğ‘…. 

With a slight abuse of notation, let ğœb(ğ›¹, ğ‘š, ğ‘’): ğ›¤Ã—ğ‘€Ã—ğ¸ â†’ ğ´ be the strategy of ğ‘… with evidence 
ğ‘’. Fig. 2.4 depicts the signaling game model for the deception with knowledge acquisition through 
investigation. 

Fig. 2.4: Signaling games with evidence acquisition by investigation. The probability 
ğ›¾(ğ‘’|ğ›¹, ğ‘š) of emitting evidence ğ‘’ depends on the event ğ›¹ and the message ğ‘š sent by 
ğ‘†. If the belief ğœ‡b is informative, ğœ‡b is used; if ğœ‡b is uninformative, ğ›½b is used as the 
posterior. 

2.5.4  Equilibrium Concept 

The knowledge acquisition of the signaling game over continuous state space extends the PBNE 
in Definition 1 to the following. 

Definition 2.5: (Perfect Bayesian Nash Equilibrium) A PBNE of the game is a strategy profile 
(ğœ O, ğœb) and a posterior belief system (ğœ‡b, ğ›½b) that satisfy the following conditions: 
â€“ 

(Deceiverâ€™s  Sequential  Rationality) ğ‘† maximizes  her  expected  utility  given  the  deceiveeâ€™s 
strategy ğœb and the distribution of the evidence ğ‘’: for each ğœƒ âˆˆ ğ›©, 

ğœ Oâˆ—(ğœƒ) âˆˆ argmax
fg

ğ‘ˆO(ğœbâˆ—, ğœƒ, ğœ O). 

â€“ 

(Deceiveeâ€™s Sequential Rationality) ğ‘… maximizes his expected utility given ğ‘†â€™s strategy ğœ Oâˆ— 
and his posterior belief ğœ‡b(ğœƒ|ğ‘š): for any ğ‘š âˆˆ ğ‘€, 

â€¢ 

if ğœ‡b(ğœƒ|ğ‘š) is informative, ğœbâˆ—(ğ‘Ÿ, ğ‘š) âˆˆ argmax
fiâˆˆ7

IâˆˆJ

ğ‘ˆb

(ğœb, ğœƒ)ğœ‡b(ğœƒ|ğ‘š)ğ‘‘ğœƒ;  

 
 
 
 
 
 
â€¢ 

if  ğœ‡b(ğœƒ|ğ‘š)

argmax

â€¦

4

C
â€¢Ë†B

 is  uninformative  over  ğ›©(cid:192) â‰¡ [ğœƒâ€¹, ğœƒo] âŠ† ğ›© ,  ğœbâˆ—(ğ›¹, ğ‘š, ğ‘’) âˆˆ
ğ›½JÂ´

(ğ›¹â€¢|ğ‘š, ğ‘’)ğ‘ˆb(ğ‘

, ğœƒ)ğ‘“(ğœƒ)ğ‘‘ ğœƒ,  

â€¢

â€¦

where ğœƒ(cid:192)

B â‰¡ [ğœƒâ€¹, ğœƒ(cid:181)], ğœƒ(cid:192)

C â‰¡ [ğœƒâ€¹, ğœƒ(cid:181)], and ğ‘

â€¢

â‰¡ argmax

ğ‘ˆb

(ğ‘, ğœƒ)ğ‘‘ ğœƒ. 

â€¦
IÂ´

â€“ 

â€“ 

(Consistent  Belief)  The  posterior  belief  of  ğ‘…  is  updated  according  to  Bayesâ€™  rule  (i.e., 
ğœ‡b ğœƒ ğ‘š  is informative), as 

ğœ‡b ğœƒ ğ‘š =

ğ‘“ ğœƒ ğ‘O ğ‘š ğœƒ

ğ‘“J

ğœƒ ğ‘O ğ‘š ğœƒ ğ‘‘ ğœƒ

	.			 

(ğœƒ)ğ‘O(ğ‘š| ğœƒ)ğ‘‘ ğœƒ = 0, ğœ‡b(ğœƒ|ğ‘š) may be set to any probability distribution over ğ›©. If 

If  ğ‘“J
ğœ‡b is uninformative, i.e., 

ğœ‡b =

ğ‘“(ğœƒ)

ğ‘“

(ğœƒ)ğ‘‘ ğœƒ

[I(cid:176),In]

	.			 

â€“ 

ğ‘… acquires evidence through investigation, and updates belief using evidence as, 

ğ›½b(ğ›¹|ğ‘’, ğ‘š) =

ğ›¾(ğ‘’|ğ›¹, ğ‘š)ğ‘ƒ(ğ›¹)

ğ›¾C
ËœË†B

(ğ‘’|ğ›¹Ëœ, ğ‘š)ğ‘ƒ(ğ›¹Ëœ)

, 

(ğ‘’|ğ›¹Ëœ, ğ‘š)ğ‘ƒ(ğ›¹Ëœ) = 0, ğ›½b(ğ›¹|ğ‘’, ğ‘š) may  be  set  to  any  probability  distribution 

â€“ 

ğ›¾C
ËœË†B

and  if 
over ğ›©. 

In separating equilibrium, the deceiver sends message ğ‘š with different values of report ğ›º(ğ‘š) for 
different  states.  Separating  equilibria  are  also  called  revealing  equilibria  because  the  strategic 
deceivee can infer the true state even if ğ›º(ğ‘š) does not tell the truth. In pooling equilibrium, the 
deceiver sends message ğ‘š âˆˆ ğ‘€ï¬ with the same value of report ğ›º(ğ‘š) = ğ‘Ÿ for all states. In partial-
pooling equilibrium, however, the deceiver sends the message with the same report for some states 
and different reports for other states. Clearly, the PBNE strategy ğœ Oâˆ— associated with a DR (resp. 
UR) is pooling (resp. separating) strategy. 

2.5.5  Equilibrium Results 

From the definition of UR, the equilibrium strategy of ğ‘… gives the most preferred action, ğ›¼b(ğœƒ) â‰¡
ğœƒ, for all ğœƒ in the UR. Therefore, in any differentiable S-PBNE, the utility ğ‘ˆO and the strategy ğœ O 
have to satisfy the following first-order optimality condition given ğœbâˆ—(ğœƒ) = ğ›¼b(ğœƒ) according to 

the sequential rationality: ğ‘ˆC

O(ğ›¼b(ğœƒ), ğœƒ, ğœ O(ğœƒ))

â€°Ë˜i(I))
â€°I

+ ğ‘ˆj

O(ğœƒ, ğœƒ, ğœ O(ğœƒ))

â€°fg(I)
â€°I

= 0. 

Lemma 2.5 summarizes the property of the strategy ğœ Oâˆ— in any UR. 

  
 
Lemma  2.5:  If [ğœƒË™, ğœƒÂ¨] is  an  undeceivable  region,  then  for  each ğœƒ âˆˆ [ğœƒË™, ğœƒÂ¨],  the  equilibrium 
strategy ğœ Oâˆ—(ğœƒ) > ğœƒ and it is a unique solution of the differential equation 

ğ‘‘ğœ O(ğœƒ)
ğ‘‘ğœƒ

=

ğ‘
ğ‘˜(ğœO(ğœƒ) âˆ’ ğœƒ)

, 

with initial condition ğœ Oâˆ—(ğœƒË™) = ğœƒË™.  

Lemma 2.5 underlies the following proposition. 

Proposition 2.1: With initial condition ğœ Oâˆ—(ğœƒ) = ğœƒ, there exists a cut-off state ğœƒ < ğœƒ such that a 

unique  solution ğœ Oâˆ—  to  the  differential  equation  in  Lemma  2.5  is  well-defined  on [ğœƒ, ğœƒ] with 

ğœ Oâˆ—(ğœƒ) = ğœƒ, and there is no solution on (ğœƒ , ğœƒ].  

Proposition 2.1 notes that in S-PBNE, the optimal strategy ğœ Oâˆ— of ğ‘† has to choose a report ğ‘Ÿ that 
is  strictly  larger  than  the  true  state ğœƒ ,  but  eventually ğœ Oâˆ—  runs  out  of  such  report  for ğœƒ > ğœƒ . 

Proposition 2.1 implies that there is no S-PBNE strategy of ğ‘† for all ğœƒ > ğœƒ, because there are not 

enough states to support the monotone S-PBNE strategy of ğ‘† for the state in (ğœƒ , ğœƒ]. This suggests 
a class of PP-PBNE for the state space ğ›©, which is separating in low states and pooling in higher 
states. For convention, let ğœ O,k: ğ›© â†’ ğ›© and ğœ‚O,k, respectively, denote the P-PBNE strategy and the 
associated message strategy of ğ‘†. We define this class of PP-PBNE by introducing a boundary 
state in the following precise sense. 

Definition 2.6: We say that the strategy ğœ O is a SLAPH (Separating in Low states And Pooling 

in High states) strategy if there exists a boundary state ğœƒËš âˆˆ [ğœƒ, ğœƒ] such that 
1.  (S-PBNE) ğœ Oâˆ—(ğœƒ) = ğ‘Ÿ with ğœ‚Oâˆ—(ğ‘Ÿ) âˆˆ ğ‘€ï¬ ,  for  all ğœƒ âˆˆ [ğœƒ, ğœƒËš),  and ğœ Oâˆ—(ğœƒ) â‰  ğœ Oâˆ—(ğœƒâ€²) for  all 

ğœƒ â‰  ğœƒâ€² âˆˆ [ğœƒ, ğœƒËš); 

2. 

(P-PBNE) ğœ Oâˆ—,k(ğœƒ) = ğœƒ with ğœ‚Oâˆ—,k(ğœƒ) âˆˆ ğ‘€I, for all ğœƒ âˆˆ [ğœƒËš, ğœƒ]. 

In  any  SLAPH  equilibrium,  both  players  have  no  incentive  to  deviate  from  the  equilibrium 
strategies. This requires the boundary state ğœƒËš to be consistent in the sense that the equilibrium 
at  ğœƒËš  is  well-defined.  Specifically,  the  utility  of  ğ‘†  has  to  satisfy  the  following  boundary 
consistency (BC) condition at ğœƒËš: 

ğ‘ˆO(ğœbâˆ—(ğœ O,k(ğœƒËš), ğ‘šk), ğœƒËš, ğ‘šk) = ğ‘ˆO(ğ›¼b(ğœƒËš), ğœƒËš, ğ‘šË™), 

 
 
 
 
 
 
where  ğ‘šk âˆˆ ğ‘€I  and  ğ‘šË™ âˆˆ ğ‘€fgâˆ—(IÂ¸) .  The  BC  condition  implies  that  ğ‘†  is  indifferent  between 
sending ğ‘šk âˆˆ ğ‘€I with ğ‘âˆ— = ğœbâˆ—(ğœ Oâˆ—(ğœƒËš), ğ‘šk) and sending ğ‘šË™ âˆˆ ğ‘€fgâˆ—(IÂ¸) with action ğ‘âˆ— = ğœƒËš. 

The conflict of interest, b, is a utility-relevant parameter for S that can induce incentives for S to 
reveal partial information about any state Î¸ âˆˆ [Î¸(cid:204), Î¸] to R while her utility-maximizing P-PBNE 
strategy Ïƒyâˆ— is maintained. This can be achieved based on the assumption |M(cid:160)| â†’ âˆ and the fact 
that CÂ¥ is equally expensive for all the messages chosen for all state Î¸ âˆˆ [Î¸(cid:204), Î¸]. Specifically, the 
P-PBNE  region [Î¸(cid:204), Î¸] can  be  further  partitioned  into  multiple  pools.  First,  some  notations  for 
describing the multiple pools are needed. Let Î˜Ë â‰¡ (Î¸B, Î¸C, â€¦ , Î¸Ë›(cid:144)C, Î¸Ë›) be a partition of [Î¸(cid:204), Î¸], 
with Î¸B = Î¸(cid:204) < Î¸C < â‹¯ < Î¸Ë› = Î¸.  We  call  each  interval Î˜â€”,â€”(cid:209)C = [Î¸â€”, Î¸â€”(cid:209)C] is  a  pool.  With  an 
abuse  of  notation,  let Î·yâˆ—,(cid:210)(Ïƒyâˆ—(Î¸), Î¸) denote  the  message  strategy  that  chooses  a  message ğ‘š âˆˆ
M(cid:158)(cid:159)âˆ—((cid:160)) for  a  state Î¸.  In  each  pool Î˜â€”,â€”(cid:209)C, Î·yâˆ—,(cid:210)(Î¸, Î¸) chooses  the  same  message ğ‘š âˆˆ M(cid:160),  for  all 
Î¸ âˆˆ Î˜â€”,â€”(cid:209)C, j = 0, â€¦ , K âˆ’ 1.  After R determines  a  pool Î˜â€”,â€”(cid:209)C,  she  acquires  evidence e âˆˆ {eB, eC} 
C â‰¡
through  investigations  by  dividing Î˜â€”,â€”(cid:209)C into  two  sub-intervals Î˜â€”,â€”(cid:209)C
(cid:215)
(cid:213)
[Î¸â€”,â€”(cid:209)C

] and Î˜â€”,â€”(cid:209)C
 represents the event {Î¸ âˆˆ Î˜â€”,â€”(cid:209)C

B â‰¡ [Î¸â€”, Î¸â€”,â€”(cid:209)C

} such that Î¨â€”,â€”(cid:209)C

C
, Î¨â€”,â€”(cid:209)C

}, 

B

(cid:213)

(cid:215)

, Î¸â€”(cid:209)C]. Let Î¨â€”,â€”(cid:209)C âˆˆ Î“â€”,â€”(cid:209)C â‰¡ {Î¨â€”,â€”(cid:209)C
((cid:160))(cid:223)(cid:160)

(cid:217)

with  probability  P(Î¨â€”,â€”(cid:209)C

(cid:215)

) =

,  for  i = 0 ,  1 .  On  the  equilibrium  path,  R  must  play 

(cid:222)
(cid:218)(cid:219),(cid:219)(cid:220)(cid:221)

(cid:217)

((cid:160))(cid:223)(cid:160)

(cid:218)(cid:219),(cid:219)(cid:220)(cid:221)

ÏƒUâˆ—(Î¨â€”,â€”(cid:209)C, ğ‘šâ€”, e) as  defined  in  Definition  2.5  for  any mâ€” such  that Î·yâˆ—,(cid:210)(Î¸, Î¸) = ğ‘šâ€” for  all Î¸ âˆˆ
Î˜â€”,â€”(cid:209)C. Define 

â€¢

ğ‘

(ğœƒËœ, ğœƒËœ(cid:209)C) â‰¡ ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥

4

â€¦
J(cid:224),(cid:224)(cid:220)(cid:221)

ğ‘ˆb

(ğ‘, ğœƒ)ğ‘“(ğœƒ)ğ‘‘ğœƒ, 

for ğ‘– = 0, 1. 

The necessary and sufficient conditions for the existence of SLAPH equilibrium are summarized 
in the following theorem. 

Theorem 2.3: (Necessary condition.) In any SLAPH equilibrium, there exists a boundary state 
ğœƒËš  such  that  the  pooling  interval [ğœƒËš, ğœƒ] can  be  partitioned  into  multiple  pools  denoted  by  a 
strictly increasing sequence (ğœƒB, ğœƒC, â€¦ , ğœƒÃ†(cid:144)C, ğœƒÃ†) with ğœƒB = ğœƒËš and ğœƒÃ† = ğœƒ, such that, for all ğ‘— =
0, â€¦ , ğ¾ âˆ’ 1,  

ğ‘ˆ7 ğ‘ ğœƒËœ, ğœƒËœ(cid:209)C , ğœƒËœ(cid:209)C = ğ‘ˆ7 ğ‘ ğœƒËœ(cid:209)C, ğœƒËœ(cid:209)h , ğœƒËœ(cid:209)C ,							(2.7) 

ğ‘ˆO ğ‘ ğœƒB, ğœƒC , ğœƒËš, ğœƒ = ğ‘ˆO ğœƒËš, ğœƒËš, ğœ Oâˆ— ğœƒËš , â€„â€„	if	ğœƒËš > ğœƒ,			(2.8) 

where ğ‘(ğœƒËœ, ğœƒËœ(cid:209)C) =
Given the multiple-pool PBNE characterized by Eq. (2.7)-(2.8), and if ğœƒËš = ğœƒ

(ğœƒËœ, ğœƒËœ(cid:209)C),  for  all ğ‘— = 0, â€¦ , ğ¾ âˆ’ 1.  (Sufficient  Condition.) 

 and 

ğ‘ƒC
â€¢Ë†B

(ğ›¹â€¢) ğ‘

â€¢

 
 
Ì²
ğ‘ˆO ğ‘ ğœƒB, ğœƒC , ğœƒ, ğœƒ â‰¤ ğ‘ˆO ğ›¼b ğœƒ , ğœƒ, ğœ Oâˆ— ğœƒ ,						(2.9) 

there exists an SLAPH equilibrium. 

Note  that ğ‘ˆ? is  equal  for  all ğœƒ âˆˆ [ğœƒËš, ğœƒ].  Eq.  (2.7)  says  that  at  each  link  state ğœƒËœ(cid:209)C connecting 
ğ›©Ëœ,Ëœ(cid:209)C and ğ›©Ëœ(cid:209)C,Ëœ(cid:209)h,  the  utilities  induced  by ğ‘(ğœƒËœ, ğœƒËœ(cid:209)C) and ğ‘(ğœƒËœ(cid:209)C, ğœƒËœ(cid:209)h),  respectively,  should 
keep the same. Otherwise, ğ‘† has incentives to deviate from the current partition to combine these 
two consecutive pools by sending the message that induces more profitable ğ‘ˆ7 but the same ğ‘ˆ?. 
This is not ideal for ğ‘… because larger pools make the posterior less informative that could decrease 
the utilities for ğ‘…. Similarly, Eq. (2.8) says that at the boundary state ğœƒËš, ğ‘† should be indifferent 
between  playing  S-PBNE  strategy  and  inducing  action ğœbâˆ—(ğœƒËš) versus  playing  P-PBNE  and 
introducing  action ğ‘(ğœƒB, ğœƒC).  Inequality  (2.9)  notes  that  if  the  boundary  state ğœƒËš = ğœƒ,  then ğ‘† is 
indifferent  between  pooling  with  [ğœƒ, ğœƒC]  and  reporting  ğœƒ  for  ğœƒ  versus  separating  at  ğœƒ .  The 
existence of SLAPH requires (2.7)-(2.9) to be jointly satisfied. 

2.6 Adaptive Strategic Cyber Defense for APT in Critical Infrastructure Network 

2.6.1  Multi-Stage Dynamic Bayesian Game Model 

In [18] and [19], Huang et al. have proposed a multi-stage Bayesian game framework to capture the 
stealthy, dynamic, and adaptive natures of the advanced persistent threats (APTs) as shown in Fig. 
2 . 5. 

Fig. 2.5: The APTsâ€™ life cycle includes a sequence of phases and stages such as the initial 
entry, privilege  escalations,  and  lateral  movements.  APTs use  each  stage  as  a  stepping 
stone for the next and aim to cause physical damages or collect confidential data. 

Continuous Type: Due to the cyber deception, the system defender ğ‘… cannot directly determine 
whether a user is legitimate or not even he can observe the userâ€™s apparent behaviors. Thus, user 

 
 
 
 
 
 
 
ğ‘†  has  a  type  ğœƒ  which  is  a  random  variable  and  the  realization  ğœƒ âˆˆ ğ›©: = [0,1]  is  private 
information of ğ‘†. The value of the type indicates the strength of the user in terms of damages that 
she can inflict on the system. A user with a larger type value indicates a higher threat level. At 
each stage ğ‘˜ âˆˆ {0,1, â‹¯ , ğ¾}, ğ‘† chooses an action ğ‘šÅ’ âˆˆ ğ‘€Å’ and ğ‘… chooses an action ğ‘Å’ âˆˆ ğ´Å’. The 
userâ€™s actions represent the apparent behaviors and observable activities from log files such as a 
privilege escalation request and sensor access. A defender cannot identify the userâ€™s type from 
observing his actions. The defenderâ€™s action includes prevention and proactive behaviors such as 
restricting  the  escalation  request  or  monitoring  the  sensor  access.  The  action  pair (ğ‘šÅ’, ğ‘Å’) is 
known to both players after stage ğ‘˜ and forms a history â„Å’: = {ğ‘šB, â‹¯ , ğ‘šÅ’(cid:144)C, ğ‘B, â‹¯ , ğ‘Å’(cid:144)C}. The 
state ğ‘¥Å’ âˆˆ ğ’³Å’ shows the system status at each stage ğ‘˜ such as the location of the APTs. Since the 
initial state ğ‘¥B and history â„Å’ uniquely determine the state, ğ‘¥Å’ contains information of history up 
to ğ‘˜  and  has  the  transition  kernel  described  by ğ‘¥Å’(cid:209)C = ğ‘”Å’(ğ‘¥Å’, ğ‘šÅ’, ğ‘Å’).  The  behavior  mixed 
strategies ğœÅ’

O: ğ’³Å’Ã—ğ›© â†’â–³ ğ‘€Å’ and ğœÅ’

b: ğ’³Å’ â†’â–³ ğ´Å’.  

b âˆˆ ğ›¤Å’

O âˆˆ ğ›¤Å’

b: ğ’³Å’ â†¦
Believe Update: To strategically gauge the userâ€™s type, the defender specifies a belief ğœ‡Å’
â–³ ğ›© as a distribution over the type space according to the information available at stage ğ‘˜. The 
prior distribution of the userâ€™s type is known to be ğ‘“ and the belief of the type updates according 
to the Bayesian rule. 

b
ğœ‡Å’(cid:209)C

(ğœƒ|ğ‘¥Å’(cid:209)C) =

b(ğœƒ|ğ‘¥Å’)ğœÅ’
ğœ‡Å’
bC
(ğœƒ |ğ‘¥Å’)ğœÅ’
ğœ‡Å’
B

b(ğ‘Å’|ğ‘¥Å’, ğœƒ)
b(ğ‘Å’|ğ‘¥Å’, ğœƒ)ğ‘‘ ğœƒ

. 

Utility Function: The userâ€™s type influences ğ‘ƒâ€¢â€™s immediate payoff received at each stage ğ‘˜, i.e., 
O: ğ’³Å’Ã—ğ‘€Å’Ã—ğ´Å’Ã—ğ›© â†¦ â„. For example, a legitimate userâ€™s access to the sensor benefits the system 
ğ‘ˆÅ’
O}Å’Ë†Å’o,â‹¯,Ã† âˆˆ
while a pernicious userâ€™s access can incur a considerable loss. Define ğœÅ’o:Ã†
ğ›¤Å’o:Ã†

O  as a sequence of policies from ğ‘˜â€² to ğ¾. 

: = {ğœÅ’

O âˆˆ ğ›¤Å’

O

The defender has the objective to maximize the cumulative expected utility: 

b
ğ‘ˆÅ’o:Ã†

O
(ğœÅ’o:Ã†

, ğœÅ’o:Ã†

b , ğ‘¥Å’o): =

and the userâ€™s objective function is 

Ã†

Å’Ë†Å’o

Ã†

ğ¸Iâˆ¼(cid:240)Ã¦

i,lÃ¦âˆ¼(cid:130)Ã¦

g,4Ã¦âˆ¼(cid:130)Ã¦
i

ğ‘ˆÅ’

b(ğ‘¥Å’, ğ‘šÅ’, ğ‘Å’, ğœƒ), 

O
ğ‘ˆÅ’o:Ã†

O
(ğœÅ’o:Ã†

, ğœÅ’o:Ã†

b , ğ‘¥Å’o, ğœƒ) =

b
ğœÅ’

(ğ‘Å’|ğ‘¥Å’)

O
ğœÅ’

(ğ‘šÅ’|ğ‘¥Å’, ğœƒ)ğ‘ˆÅ’

O. 

Å’Ë†Å’o

4Ã¦âˆˆ7Ã¦

lÃ¦âˆˆ~Ã¦

2.6.2  Equilibrium Concept 

 
 
 
The insider threats of APTs lead to the following definition of perfect Bayesian Nash equilibrium 
(PBNE) where the defender chooses the most rewarding policy to confront the attackerâ€™s best-
response policies. 

Definition 2.7: In the two-person multi-stage game with a sequence of beliefs ğœ‡Å’

satisfying the Bayesian update in and the cumulative utility function ğ‘ˆÅ’o:Ã†
O
b âˆˆ ğ›¤Å’o:Ã†

O : ğ‘ˆÅ’o:Ã†

b , ğœÅ’o:Ã†

b , ğ›¾, ğ‘¥Å’o, ğœƒ) â‰¥ ğ‘ˆÅ’o:Ã†
(ğœÅ’o:Ã†
{ğ›¾ âˆˆ ğ›¤Å’o:Ã†
ğ‘†â€™s best-response set to ğ‘…â€™s policy ğœÅ’o:Ã†

, ğ‘¥Å’o, ğœƒ), âˆ€ğ›¤Å’o:Ã†
b  under state ğ‘¥Å’o and type ğœƒ. 

(ğœÅ’o:Ã†
b âˆˆ ğ›¤Å’o:Ã†

O

O

O

b, ğ‘˜ âˆˆ {ğ‘˜â€², â‹¯ , ğ¾} 
I,(cid:242)Ã¦n(ğœÅ’o:Ã†

, the set ğ‘…h

b ): =
b , âˆ€ğ‘¥Å’o âˆˆ ğ’³Å’o, ğœƒ âˆˆ ğ›©}  is 

Definition  2.8:  In  the  two-person  multi-stage  Bayesian  game  with  ğ‘…  as  the  principal,  the 

cumulative utility function ğ‘ˆÅ’o:Ã†
of beliefs in , a sequence of strategies ğœÅ’o:Ã†
(PBNE) for the principal, if 

O

b
, ğ‘ˆÅ’o:Ã†

, the initial state ğ‘¥Å’o âˆˆ ğ’³Å’o, the type ğœƒ âˆˆ ğ›©, and a sequence 
b  is called a perfect Bayesian Nash equilibrium 
bâˆ— âˆˆ ğ›¤Å’o:Ã†

bâˆ—
ğ‘ˆÅ’o:Ã†

(ğ‘¥Å’o): =

inf
m,Ä±Ã¦n(fÃ¦n:(cid:243)
iâˆ— )

g âˆˆb(cid:244)
fÃ¦n:(cid:243)

bâˆ—
ğ‘ˆÅ’o:Ã†

(ğœÅ’o:Ã†

bâˆ— , ğœÅ’o:Ã†

O

, ğ‘¥Å’o). 

A strategy ğœÅ’o:Ã†

Oâˆ— âˆˆ ğ‘ğ‘Ÿğ‘”maxfÃ¦n:(cid:243)

g âˆˆ(cid:130)Ã¦n:(cid:243)

O
g ğ‘ˆÅ’o:Ã†

(ğœÅ’o:Ã†

bâˆ— , ğœÅ’o:Ã†

O

, ğ‘¥Å’o, ğœƒ) is a PBNE for the agent ğ‘†. 

A conjugate-prior method allows online computation of the belief and reduces Bayesian update 
into  an  iterative  parameter  update.  The  forwardly  updated  parameters  are  assimilated  into  the 
backward  dynamic  programming  computation  to  characterize  a  computationally  tractable  and 
time-consistent equilibrium solution based on the expanded state space. 

2.6.3  Case Study 

We  consider  a  four-stage  transition  with  the  first  three  stages  related  to  cyber  transition  of  the 
APTs and the last stage related to the benchmark Tennessee Eastman (TE) process as the targeted 
physical  plant.  The  TE  process  involves  two  irreversible  reactions  to  produce  two  liquid  (liq) 
products. The process shuts down when the safety constraints are violated such as a high reactor 
pressure,  a  high/low  separator/stripper  liquid  level.  The  attacker  can  revise  the  sensor  reading, 
trigger  an  undesired  feedback-control,  and  cause  a  loss.  The  state ğ‘¥Å’  has  five  possible  values 
representing the working status of the system where the state 1 is most desirable and state 5 is the 
least desirable. We can obtain the normal operation reward and the reward of attacks from the 
simulation. The belief state {ğ›¼Å’, ğ›½Å’} uniquely determines the belief distribution which is assumed 
to take the form of the beta distribution. The larger ğ›¼Å’ means that the user is more likely to have a 
smaller type value and have lower threats to the system. 

 
 
 
 
 
 
bâˆ—
As shown in Fig. 2.6, a high value ğ‘ˆÅ’o:Ã†
(ğ‘¥Å’o) for the defender is the result of a healthy system 
state ğ‘¥Å’ as well as a belief of a low-threat user. At the most desirable system state ğ‘¥Å’ = 1, attackers 
will  not  attack  because  the  reward  incurred  is  insufficient.  Then  the  defender  does  not  need  to 
defend and obtains the maximum utility. 

bâˆ—
Fig. 2.6:  Value function ğ‘ˆÅ’â€²:Ã†

(ğ‘¥Å’â€²) under different expanded states {ğ‘¥Å’, ğ›¼Å’, ğ›½Å’}. 

To investigate the effect of the defenderâ€™s belief, we change the belief state (ğ›¼Å’, ğ›½Å’) from (9,1) 
to (1,9), which means that the defender grows optimistically that the user is of a low threat level 
with a high probability. Since playersâ€™ value functions are of different scales in terms of the attack 
threshold and the probability, we normalize the value functions with respect to their maximum 
values to illustrate their trends and make them comparable to the threshold and the probability as 
shown  in  Fig.  2.7.  When ğ›¼Å’ is  large,  the  defender  chooses  to  protect  the  system  with  a  high 
probability, which completely deters attackers with any type values because the probability to 
attack is 0. 

As the defender trusts more about the userâ€™s legitimacy, the defending probability decreases to 0 
when ğ›¼Å’ = 1. Since the defender is less likely to defend, the attacker bears a smaller threshold 
to  launch  the  attack.  The  resulted  defending  policy  captures  a  tradeoff  between  security  and 
economy and guarantees a high value for defenders at most of the belief states. 

 
 
 
 
Fig. 2.7: The effect of the defenderâ€™s belief. 

The  central  insight  from  the  multi-stage  analysis  is  the  adversaryâ€™s  tradeoff  between  the 
instantaneous reward and the hiding to arrive at a more favorable state in the future stages. The 
higher  the  belief  of  the  defender  in  ğ‘†  as  a  legitimate  user,  the  less  probability  he  will  act 
defensively and thus the attacker has a smaller threshold to launch the attack. 

2.7 Conclusion 

Deception is a technique that can be viewed as an advanced approach to secure the devices or 
attacks. Understanding deception quantitatively is pivotal to provide rigor, predictability, and 
design principles. In this chapter, we have formulated signaling-game theoretic models of 
deceptions over discrete and continuous information spaces. We have studied leaky deception 
models and extended the baseline perfect Bayesian Nash equilibrium (PBNE) to versions 
involving knowledge acquisitions characterized by evidences. We have analyzed the impacts of 
evidence on the belief updates and strategy constructions on the equilibrium path. 

In the binary state space, the leaky deception game with evidence admits an equilibrium that 
includes a regime in which the deceivee should choose whether to trust the deceiver based on the 
evidence, and regimes in which the deceivee should ignore the message and evidence and merely 
guess the private information based only on the prior probabilities. For the deceiver, the 
equilibrium results imply that it is optimal to partially reveal the private information in the 
former regime. 

We have also studied leaky deception games over a continuous one-dimensional information 

 
 
 
 
 
 
 
 
space. We have studied the PBNE as the solution concept to analyze the outcome of the 
deception game and characterize the deceivability of the game. The proposed deception game 
admits a class of PBNE called SLAPH (Separating in Low states And Pooling in High states). 
The necessary and sufficient conditions for the existence of such PBNE are given. However, a 
full undeceivable region does not exist and there exists a deceivable region. We have also shown 
that the deceivable region can be partitioned into multiple sub-deceivable regions without 
decreasing total utilities for the deceiver when the conflict of interest is insignificant. 

Furthermore, we have explored a multi-stage incomplete information Bayesian game model for 
defensive deception frameworks for critical infrastructure networks with the presence of 
advanced persistent threats (APT). With the multi-stage and multi-phase structure of APTs, the 
belief of the defender is formed dynamically using observable footprints. The conjugate priors 
are used to reduce Bayesian updates into parameter updates, which leads to a computationally 
tractable extended-state dynamic programming that admits an equilibrium solution consistent 
with the forward belief update and backward induction. Tennessee Eastman process has been 
used as a case study to demonstrate the multi-stage deception game. The numerical simulations 
have shown that the game-theoretic defense strategies have significantly improved the security of 
the critical infrastructures. 

References  

1.  Symantec  Corporation,  â€œInternet  security  threat  report.  Tech.  rep.,â€  Symantec 
Corporation,  2016.  [Online].  Available:  https://www.symantec.com/.  [Accessed 
Nov. 10, 2018]. 

2.  L.  Atzori,  A.  Iera,  G.  Morabito,  â€œThe  Internet  of  Things:  A  survey,â€  Computer 

Networks, Volume 54, Issue 15, 2010, Pages 2787-2805 

3.  S. Bodmer, D.M. Kilger, G. Carpenter, et al. Reverse deception: organized cyber 

threat counter-exploitation. McGraw-Hill New York, 2012. 

4.  I.  Bose,  A.C.M.  Leung,  â€œUnveiling  the  Mask  of  Phishing:  Threats,  Preventive 
Measures,  and  Responsibilities,â€  Communications of the Association for Information 
Systems, vol 19, no. 1, pp. 24, 2007. 

5.  I. Bose, A.C.M. Leung, â€œAssessing anti-phishing preparedness: A study of online 
banks in Hong Kong,â€ Decision Support Systems vol. 45, no. 4, pp. 897â€“912, 2008. 
6.  G.  Brown,  M.  Carlyle,  D.  Diehl,  et  al.,  â€œA  two-sided  optimization  for  theater 
ballistic missile defense,â€ Operations research vol 53, no. 5, pp.745â€“763,   2005. 
7.  T.E.  Carroll,  D.  Grosu,  â€œA  game  theoretic  investigation  of  deception  in  network 

security,â€ Security and Commun. Nets. Vol 4, no. 10, pp. 1162â€“1172, 2011. 

8.  A. Clark, Q.  Zhu, R. Poovendran, et al., â€œDeceptive routing in relay networks,â€ In 
Proc. International Conference on Decision and Game Theory for Security, 2012,  
pp. 171â€“185.  

 
 
 
 
9.  H. Cott, Adaptive Coloration in Animals. Methuen,   1940. 
10.  D.  Ettinger,  P.  Jehiel,  â€œA  theory  of  deception,â€  American  Economic  Journal: 

Microeconomics vol 2, no. 1, pp. 1â€“20,   2010. 

11.  S. Farhang, M.H. Manshaei, M.N. Esfahani, et al., â€œA dynamic Bayesian security 
game framework for strategic defense mechanism design,â€ In Proc. Decision and 
Game Theory for Security,2014,  pp. 319â€“328. 

12.  D. Fudenberg, J. Tirole, Game Theory. The MIT Press, 1991. 
13.  U. Gneezy, â€œDeception: The role of consequences,â€ American Econ. Review, vol. 95, 

no. 1, pp. 384â€“394, 2005. 

14.  E. Harrell, â€œVictims of identity theft, 2014,â€ US Dept. of Justice Bureau of Justice 

Stat. Bulletin, September,  2015. 

15.  J.C. Harsanyi, â€œGames with incomplete information played by â€œBayesianâ€ players,â€ 

Manage. Sci. vol. 50, no. 12, pp.1804â€“1817, 1967. 

16.  K.  HorÃ¡k,   Q. Zhu,   B. BoÅ¡ansky`,   â€œManipulating   adversaryÃ¢s   belief:   A   dynamic 
game  approach  to  deception  by  design  for  proactive  network  security,â€  In  Proc. 
International Conference on Decision and Game Theory for Security, 2017,  pp. 273â€“
294. 

17.  L. Huang, Q. Zhu, â€œAnalysis and computation of adaptive defense strategies 

against  advanced  persistent 
threats  for  cyber-physical  systems,â€  In  Proc.  
International  Conference  on  Decision  and  Game  Theory  for  Security,  2018,  pp. 
205-226. 

18.  L.  Huang,  J.  Chen,  Q.  Zhu,  â€œFactored  Markov  game  theory  for  secure 
interdependent  infrastructure  networks,â€  Game  Theory  for  Security  and  Risk 
Management, pp. 99â€“126. Springer, 2018. 

19.  L. Huang, Q. Zhu, â€œAdaptive strategic cyber defense for advanced persistent threats 
in critical infrastructure networks,â€ ACM SIGMETRICS Performance Evaluation 
Review, 2019, 46(2): 52-56.  

20.  L.J. Janczewski, A.M. Colarik, â€œCyber Warfare and Cyber Terrorism,â€ Inform. Sci. 

Reference, New Yor,  2008. 

21.  D. Lewis, Convention cambridge. Mass.: Harvard UP, 1969. 
22.  M.H. Manshaei, Q. Zhu, T. Alpcan, T., et al., â€œGame theory  meets network security 

and privacy,â€ ACM Computing Surveys (CSUR) vol. 45, no. 3, pp. 25, 2013. 

23.  F. Miao,  Q .   Zhu,  M .   Pajic,  et al.,  â€œ A  hybrid  stochastic  game  for  secure 
control of cyber-physical systems,â€ Automatica vol. 93, pp. 55â€“63, 2018. 

24.  U.F., Minhas, J. Zhang, T. Tran, et al., â€œA multifaceted approach to modeling agent 
trust  for  effective  communication  in  the  application  of  mobile  ad  hoc  vehicular 
networks,â€ IEEE Trans. Systems, Man, and Cybernetics, Part C (Applications and 
Reviews) vol. 41, no. 3, pp. 407â€“420, 2011. 

25.  A. Monga,  Q. Zhu,  â€œ On  solving  large-scale  low-rank  zero-sum  security  games  of 

incomplete information,â€ In Proc. Information Forensics and Security (WIFS), 2016 
IEEE International Workshop on, pp. 1â€“6, IEEE,   2016. 

26.  J.F., Nash, â€œEquilibrium points in n-person games,â€ In Proc. Nat. Acad. Sci. USA 

vol. 36, no. 1, pp. 48â€“49, 1950. 

27.  J. Pawlick, E. Colbert, Q. Zhu, â€œAnalysis of leaky deception for network security 

using  signaling  games  with  evidence,â€  In  Proc.  17th  Annual  Workshop  on  the 
Economics of Information Security (WEIS), Innsbruck, Austria, June 18-19, 2018. 
28.  J. Pawlick,  E. Colbert,  Q .   Zhu,  Q,  â€œ A  game-theoretic  taxonomy  and  survey  of 
preprint 

cybersecurity 

deception 

privacy,â€ 

arXiv 

and 

for 

defensive 
arXiv:1712.05441, 2017. 

29.  J.  Pawlick,  Q.  Zhu,  â€œDeception  by  design:  Evidence-based  signaling  games  for 
network defense,â€ In Proc. Workshop on the Econ. of Inform. Security. Delft, The 
Netherlands, 2015. 

30.  J. Pawlick, Q. Zhu, â€œA Stackelberg game perspective on the conflict between ma- 
chine  learning  and  data  obfuscation,â€  In  Proc.  IEEE  Intl.  Workshop  on  Inform. 
Forensics and Security,   2016. 

31.  J. Pawlick, Q. Zhu, â€œStrategic trust in cloud-enabled cyber-physical systems with 
an application to glucose control,â€ IEEE Trans. Inform. Forensics and Security, vol. 
12, no. 12, 2017. 

32.  Phishlabs,  â€œ2018  Phishing  trends  intelligence  report,â€  Phishlabs,  2018.  [online]. 

Available: https://info.phishlabs.com/. [Accessed Nov. 10, 2018]. 

33.  J.P. Ponssard, S. Zamir, â€œZero-sum sequential games with incomplete information," 

International Journal of Game Theory vol.2 , no. 1, pp. 99â€“107, 1973. 

34.   R.  Powell,  â€œAllocating  defensive  resources  with  private  information  about 
vulnerability,â€  American  Political  Science  Review  vol.  101,  no.  4,  pp.  799â€“809,  
2007. 

35.  A.  Vrij,  S.A.  Mann,  R.P.  Fisher,  et  al.,  â€œIncreasing  cognitive  load  to  facilitate  lie 
detection:  The  benefit  of  recalling  an  event  in  reverse  order,â€  Law  and  Human 
Behavior vol. 32, no. 3, pp. 253â€“265, 2008. 

36.  N. Zhang, W. Yu, X. Fu, et al., â€œgPath: A game-theoretic path selection algorithm 
to  protect  torâ€™s  anonymity,â€  In  Proc.  International  Conference  on  Decision  and 
Game Theory for Security, 2010, pp. 58â€“71. 

37.  T. Zhang, Q. Zhu, Q, â€œStrategic defense against deceptive civilian GPS spoofing of 
unmanned aerial vehicles,â€ In Proc. International Conference on Decision and Game 
Theory for Security, 2017, pp. 213â€“233. 

38.  T.  Zhang,  Q.  Zhu,  â€œA  game-theoretic  foundation  of  deception:  Knowledge 
p r e p r i n t  

fundamental 

a r X i v  

limits,â€ 

and 
acquisition 
a r X i v : 1 8 1 0 . 0 0 7 5 2 ,   2 0 1 8 .  

39.  Q. Zhu, T. BaÅŸar, â€œGame -theoretic approach to feedback-driven multi-stage moving 
target defense,â€ In Proc. International Conference on Decision and Game Theory for 
Security, 2013, pp. 246â€“263. 

40.  Q. Zhu, T. Basar, â€œGame-theoretic methods for robustness, security, and resilience 
of cyberphysical control systems: games-in-games principle for optimal cross-layer 
resilient control systems,â€ IEEE control systems vol. 35, no. 1, pp. 46â€“65, 2015. 
41.  Q. Zhu, A. Clark, R. Poovendran, et al., â€œDeceptive routing games,â€ In Proc. IEEE 
51st Annual Conference on Decision and Control (CDC), 2012, pp. 2704â€“2711. 
42.  Q. Zhu, A. Clark, R. Poovendran, et al., â€œDeployment and exploitation of deceptive 
honeybots in social networks,â€ In Proc. IEEE 52nd Annual Conference  on Decision 
and Control (CDC), 2013, pp. 212â€“219. 

43.  T. Zhang, Q. Zhu, â€œHypothesis Testing Game for Cyber Deception,â€ In Proc. International 

Conference on Decision and Game Theory for Security, 2018, pp. 540-555.   

44.  S.  Roy,  C.  Ellis,  S.  Shiva,  et  al.,  â€œA  Survey  of  Game  Theory  as  Applied  to  Network 
Security,â€  In Proc. 43rd Hawaii International Conference on System Sciences, 2010, pp. 1-
10. 

45.  R. Zhang R, Q. Zhu, â€œSecure and resilient distributed machine learning under adversarial 
environments,â€  In  Proc.  18th  International  Conference  on  Information  Fusion  (Fusion), 
2015, pp. 644-651. 

46.  T.  Alpcan,  T.  Basar,  â€œA  game  theoretic  approach  to  decision  and  analysis  in  network 
intrusion detection,â€ In Proc. 42nd IEEE International Conference on Decision and Control, 
2003, pp. 2595-2600 Vol.3. 

47.  T. Basar, â€œThe Gaussian test channel with an intelligent jammer,â€ IEEE Transactions on 

Information Theory, vol. 29, no. 1, pp. 152-157, January 1983. 

48.  J. Pita, M. Jain, J. Marecki, et al. â€œDeployed ARMOR protection: the application of a game 
theoretic model for security at the Los Angeles International Airport,â€ In Proc. of the 7th 
international  joint  conference  on  Autonomous  agents  and  multiagent  systems:  industrial 
track, 2008,  pp. 125-132. 

49.  E. Shieh, B. An, R. Yang, et al., â€œProtect: A deployed game theoretic system to protect the 
ports of the united states,â€ In Proc. of the 11th International Conference on Autonomous 
Agents and Multiagent Systems, 2012, pp. 13-20. 

50.  S. Farhang, M.H. Manshaei Esfahani, M.N., Zhu, Q.: A dynamic Bayesian security game 
framework  for  strategic  defense  mechanism  design.  In:  Decision  and  Game  Theory  for 
Security, pp. 319â€“328. Springer (2014) 

51.  S. Rass, A. Alshawish, M.A. Abid, et al, â€œPhysical Intrusion Gamesâ€”Optimizing 
Surveillance by Simulation and Game Theory,â€ IEEE Access, 2017, vol. 5, pp. 8394-
8407. 

52.  Q. Zhu Q, S. Rass, â€œOn Multi-Phase and Multi-Stage Game-Theoretic Modeling of 

Advanced Persistent Threats,â€ IEEE Access, 2018, vol. 6, pp. 13958-13971. 

53.  J. Zhuang, V.M. Bier, O. Alagoz, â€œModeling secrecy and deception in a multiple-
period  attackerâ€“defender  signaling  game,â€  European  Journal  of  Operational 
Research, 2010, vol. 203, no. 2, pp. 409-418. 

54.  J. Chen, Q. Zhu, â€œSecurity as a service for cloud-enabled internet of controlled things 
under advanced persistent threats: a contract design approach,â€ IEEE Transactions 
on Information Forensics and Security, 2017, vol. 12, no. 11, pp. 2736-2750. 

55.  Z.  Xu,  Q.  Zhu,  â€œSecure  and  practical  output  feedback  control  for  cloud-enabled 
cyber-physical  systems,â€  In  Proc.  IEEE  Conference  on  Communications  and 
Network Security (CNS), pp. 416-420. 

56.  S. Jajodia, A.K. Ghosh, V. Swarup, C. Wang, et al. Moving target defense: creating 
asymmetric  uncertainty  for  cyber  threats  (Vol.  54).  Springer  Science  &  Business 
Media. 

57.  H. Maleki, S. Valizadeh, W. Koch, et al, â€œMarkov modeling of moving target defense 
games,â€  In  Proc.  of  the  2016  ACM  Workshop  on  Moving  Target  Defense.  ACM, 
2016, pp. 81-92. 

58.  J. Chen, C. Touati, Q. Zhu, â€œA Dynamic Game Analysis and Design of Infrastructure 

Network Protection and Recovery,â€ ACM SIGMETRICS Performance Evaluation 
Review, 2017, vol. 45, no.2 pp. 128. 

59.  J. Chen, Q. Zhu, â€œInterdependent network formation games with an application to 
critical  infrastructures,â€  In  Proc.  American  Control  Conference  (ACC),  2016,  pp. 
2870-2875. 

60.  Y.  Hayel,  Q.  Zhu,  â€œResilient  and  secure  network  design  for  cyber  attack-induced 
cascading  link  failures  in  critical  infrastructures,â€  In  Proc.  49th  IEEE  Annual 
Conference on Information Sciences and Systems (CISS), 2015, pp. 1-3. 

61.  J. Pawlick, Q. Zhu, â€œProactive Defense Against Physical Denial of Service Attacks 
Using Poisson Signaling Games,â€ In Proc. International Conference on Decision and 
Game Theory for Security, Springer, 2017, pp. 336-356. 

62.  J.  Pawlick,  Q.  Zhu,  â€œA  mean-field  stackelberg  game  approach  for  obfuscation 
adoption in empirical risk minimization,â€ In Proc. IEEE Global Conference on Signal 
and Information Processing (GlobalSIP), 2017, pp. 518-522. 

63.  W. Wang, Q. Zhu, â€œOn the Detection of Adversarial Attacks against Deep Neural 
Networks,â€  In  Proc.  ACM  Workshop  on  Automated  Decision  Making  for  Active 
Cyber Defense, 2017, pp. 27-30. 

64.  R.  Zhang,  Q.  Zhu,  â€œA  game-theoretic  defense  against  data  poisoning  attacks  in 
distributed  support  vector  machines,â€  In  Proc.  IEEE  56th  Annual  Conference  on  
Decision and Control (CDC), 2017, pp. 4582-4587. 

65.  R.  Zhang,  Q.  Zhu,  â€œA  Game-Theoretic  Approach  to  Design  Secure  and  Resilient 
Distributed Support Vector Machines,â€ IEEE Transactions on Neural Networks and 
Learning Systems, 2018. 

66.  W.  Casey,  J.A.  Morales,  E.  Wright,  et  al.,  â€œCompliance  signaling  games:  toward 
modeling  the  deterrence  of  insider  threats,â€  Computational  and  Mathematical 
Organization Theory, 2016, vol. 22, no. 3, pp. 318-349. 

67.  W.A.  Casey,  Q.  Zhu,  J.A.  Morales,  et  al.,  â€œCompliance  control:  managed 
vulnerability surface in social-technological systems via signaling games,â€ In Proc. 
7th ACM CCS International Workshop on Managing Insider Security Threats, 2015, 
pp. 53-62. 

68.  J. Chen, Q. Zhu, â€œSecurity investment under cognitive constraints: A Gestalt Nash 
equilibrium approach,â€ In Proc. IEEE Annual Conference on Information Sciences 
and Systems (CISS), 2018, pp. 1-6. 

69.  C.J.  Fung,  Q.  Zhu,  â€œFACID:  A  trust-based  collaborative  decision  framework  for 

intrusion detection networks,â€ Ad Hoc Networks, 2016, vol. 53, pp. 17-31. 

70.  Y.  Hayel,  Q.  Zhu,  â€œEpidemic  protection  over  heterogeneous  networks  using 
evolutionary  Poisson  games,â€  IEEE  Transactions  on  Information  Forensics  and 
Security, 2017, vol. 12, no. 8, pp. 1786-1800. 

71.  R.  Zhang,  Q.  Zhu,  Y.  Hayel,  â€œA  bi-level  game  approach  to  attack-aware  cyber 
in 

IEEE  Journal  on  Selected  Areas 

insurance  of  computer  networks,â€ 
Communications, 2017, vol. 35, no. 3, pp. 779-794. 

 
 
