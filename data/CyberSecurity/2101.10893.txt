1
2
0
2

r
p
A
2
2

]

R
C
.
s
c
[

2
v
3
9
8
0
1
.
1
0
1
2
:
v
i
X
r
a

DYNAMIC CYBER RISK ESTIMATION WITH COMPETITIVE
QUANTILE AUTOREGRESSION

A PREPRINT

Raisa Dzhamtyrova1 and Carsten Maple 1, 2

1The Alan Turing Institute
2Secure Cyber Systems Research Group, WMG, University of Warwick

rdzhamtyrova@turing.ac.uk

cm@warwick.ac.uk

April 23, 2021

ABSTRACT

The increasing value of data held in enterprises makes it an attractive target to attackers. The
increasing likelihood and impact of a cyber attack have highlighted the importance of effective cyber
risk estimation. We propose two methods for modelling Value-at-Risk (VaR) which can be used
for any time-series data. The ﬁrst approach is based on Quantile Autoregression (QAR), which can
estimate VaR for different quantiles, i. e. conﬁdence levels. The second method, we term Competitive
Quantile Autoregression (CQAR), dynamically re-estimates cyber risk as soon as new data becomes
available. This method provides a theoretical guarantee that it asymptotically performs as well as any
QAR at any time point in the future. We show that these methods can predict the size and inter-arrival
time of cyber hacking breaches by running coverage tests. The proposed approaches allow to model a
separate stochastic process for each signiﬁcance level and therefore provide more ﬂexibility compared
to previously proposed techniques. We provide a fully reproducible code used for conducting the
experiments.

Keywords cyber risk · dynamic risk estimation · time-series · Quantile Autoregression · competitive prediction · cyber
breach modelling

1

Introduction

The prevalence and impact of cyber attacks on organisations are increasing at an alarming rate. Risk estimation is
an important task for any company or institution, allowing them to predict and assess adverse events which can lead
to ﬁnancial and reputation losses, enabling them to plan for and mitigate against these threats through effective risk
management.

Kaplan deﬁnes risk to be a set of triplets, which consist of a risk scenario description, the probability of that scenario,
and the consequence or evaluation measure of that scenario, i.e., a measure of damage [1]. Another deﬁnition of
risk is provided by Holton, in which the risk comprises two components: uncertainty and exposure [2]. Indeed all
deﬁnitions of risk require some form of assessment of the likelihood of adverse events and their severity. A recent
report by the Department of Homeland Security [3] provides a survey of risk metric frameworks and risk models. One
of the quantitative risk metrics described in the report is Cyber Value-at-Risk (VaR), an adaptation of the ﬁnancial
VaR, for the quantiﬁcation of cyber security risk. VaR is one of the most important risk measurements in ﬁnance
and involves measuring the maximum loss over a preset horizon with a pre-deﬁned conﬁdence level [4]. VaR has
now found various applications in cyber security areas. For example, Factor Analysis of Information Risk (FAIR),
considered “an international standard information risk management model”, is based on VaR. FAIR is deﬁned as “a
standard Value-at-Risk model for information and operational risk that helps information risk, cyber security and
business executives measure, manage, and communicate on information risk in a language that the business understands,
dollars and cents” [3]. In [5] VaR is used to estimate the probability of extreme cyber attacks over a pre-deﬁned period

 
 
 
 
 
 
A PREPRINT - APRIL 23, 2021

of time. The paper [6] proposes a model to quantify the monetary VaR due to cyber threats based on the Bayesian
networks. The detailed model describes the example attack graph of unauthorized access to intellectual property. In this
paper, we propose a new methodology of estimation of VaR for cyber events.

In this paper, we aim to provide a framework that can model risks dynamically and re-estimate cyber risk when new data
becomes available. Many current risk methods are based on manual risk analysis during the system’s design process.
Some of the examples of traditional qualitative methods include scenario analysis and questionnaires, which are heavily
dependent on experts’ subjective opinions. On the other hand, quantitative risk methods are usually based on unreliable
data, and therefore their precision is prone to errors [7]. As a result, there is a lack of current research on dynamic cyber
risk estimation. Of the work that has been proposed for dynamic risk modelling, a number of approaches are based on
Hidden Markov Models (HMM). The paper [8] proposes a real-time risk estimation method, which aggregates data
from several intrusion detection systems allowing dynamic estimation of systemic risk using HMM. The paper [9]
developed a method to dynamically model the risks of users’ activity patterns in social networks. The approach is based
on HMM and Bayesian Risk Graph model. Unlike the previous approaches, we do not model the dynamics of the
system states with HMM. Instead, we focus on time-series data and propose a new method for dynamic estimation
of VaR. System monitoring is essential to effective risk governance. The monitored data is usually a different kind of
time-series, such as various sensor data, login data, and intrusion and hacking attempts. From a risk perspective, it is
critical to estimate the probability of extreme events. For example, we do not want to predict the mean or the median of
hacking attempts over a pre-deﬁned period. Instead, we aim to assess the maximum number of hacking attempts with
the desired conﬁdence. For this purpose, we suggest to model VaR as a quantile of time-series, where each quantile
corresponds to the desired conﬁdence level. These values of VaR can also be translated into the monetary equivalent.
For example, if we assume that each cyber hacking attempt costs one pound for a company, we can estimate the budget
allocation which should be devoted to security defence. The proposed method builds upon the Weak Aggregating
Algorithm for Quantile Regression (WAAQR), proposed in [10] and is adapted to the case of time-series forecasting.

We apply the proposed approach to the prediction of cyber hacking breaches. The data is taken from the report of
the Privacy Rights Clearinghouse (PRC), which contains the chronology of the reported data breaches since January
2005 1. This benchmark dataset has been used by a number of other researchers in establishing the efﬁcacy of their
work [11,12]. The analysis of data breaches is attracting research activity lately, given the importance of the topic. Some
of this work suggests that data breaches can be modelled using a variety of distributions. In [13] the authors suggest
using the beta distribution for estimating the probability of data breaches based on industry data. After estimating
the probability of data breaches, the VAR is modelled with the Monte-Carlo simulation, which gives a forecast of the
possible losses. The paper [11] investigates the PRC data set from the period between January 2005 and September 2015.
The study examined over 20 different distributions, such as log-normal, power-law, generalised Pareto to determine
which provided the best ﬁt for the size of the data breach. To model the breach frequencies, the authors investigated a
number of discrete distributions, such as Poisson, binomial, and negative binomial. The results suggest that neither
the size nor the frequency of data breaches has increased over the period under consideration. Furthermore, the study
proposes to model the daily frequency of breaches using the negative binomial distribution, whereas breach sizes are
best described by the log-normal family of distributions. It is, of course, possible that the nature of data breaches has
changed signiﬁcantly in the era of increasing data connectivity. The paper [12] analyses the PRC data set with a focus
on hacking breach incidents. Their analysis shows that both the inter-arrival time and the size of hacking breaches reveal
signiﬁcant auto-correlation and partial auto-correlation, suggesting that the breaches can be modelled with stochastic
processes. The paper estimates the inter-arrival times with the autoregressive conditional mean model, whereas the
breach sizes are estimated with ARMA(1, 1)-GARCH(1, 1). The authors also show that there is a positive correlation
between inter-arrival times and sizes of cyber incidents, and describe this dependence by a particular copula.

In this paper, we propose a new framework for dynamic estimation of VaR. Though the proposed methods can be used
to predict any types of time-series, we perform our experiments on the PRC data report. The reasons are that this dataset
contains one of the largest cyber events data available online, it is regularly updated, and it was studied before in the
literature. Our analysis closely resembles the analysis in [12], however, we propose different modelling approaches of
hacking breaches. First, our methods can be applied to any kind of time-series data. Second, the analysis in [12] models
the mean of inter-arrival times and sizes, and then VaR is found by simulating 10000 samples based on the estimated
copula. Instead, we suggest that each quantile of inter-arrival times and sizes of cyber incidents can be modelled with
separate stochastic processes. Though we do not investigate the relationship between inter-arrival times and sizes of
breaches, we argue that the proposed methods are more ﬂexible in comparison to previous research as they make fewer
assumptions on the nature of the data, since each quantile of breach size or inter-arrival time can be modelled with a
separate stochastic process. In our experiments, we ﬁrst show that we can apply Quantile Autoregression (QAR) [14]
to estimate VaR of hacking breaches. The Basil Committee recommends assessing the quality of the VaR models by
running some form of backtesting. Standard backtesting methods include the Kupiec unconditional coverage test [15]

1 https://privacyrights.org/data-breaches

2

A PREPRINT - APRIL 23, 2021

and the Christoffersen conditional coverage test [16]. We apply both tests to assess the performance of QAR. The results
show that for breach size QAR ﬁts well, and for an inter-arrival time, it rejects the null hypothesis of the conditional
coverage test of violation occurrence for one considered quantile. We then propose a new framework, Competitive
Quantile Autoregression (CQAR), which improves the prediction of hacking breach inter-arrival times.

The proposed method CQAR is based on the competitive prediction approach, where one algorithm ‘competes’ with
other predictive algorithms. The goal is to provide a strategy that can guarantee a performance close to the best
predictive models. To solve the problem of competitive prediction, the Aggregating Algorithm (AA) was proposed
in [17]. The AA mixes the predictions of a number of models in a similar manner to the Bayesian method, where the
prediction is calculated based on the model’s prior distribution and the data likelihood. Furthermore, the AA guarantees
that the loss of the resulting mixing strategy is as small as the best model’s plus a constant for any time point in the
future. The Weak Aggregating Algorithm (WAA) [18] was proposed as an alternative for the AA, which provides better
theoretical guarantees for some loss functions, such as the pinball loss, which we consider in this paper. In the general
case, both the AA and the WAA mix and compete with a ﬁnite number of algorithms.

It is possible to construct strategies that combine inﬁnite classes of functions and provide theoretical guarantees
compared to these classes. The Aggregating Algorithm for Regression chooses the competitor strategies to be all linear
functions [19]. The resulting strategy asymptotically performs as well as any linear regression in terms of the cumulative
square loss. A similar approach is undertaken in [10], where the authors propose the Weak Aggregating Algorithm for
Quantile Regression [10]. The strategy is a Bayesian mixture, which combines an inﬁnite pool of quantile regressions,
and asymptotically predicts as well as any of them in terms of the cumulative pinball loss. The algorithm was previously
applied to probabilistic forecasting of renewable energy and showed a good performance on real data. The proposed
algorithm CQAR is built on the WAAQR algorithm and is adapted to time-series forecasting. Instead of mixing a class
of quantile regressions, we suggest combining a class of QAR. It also has the property that it asymptotically predicts as
well as any QAR. We provide the pseudo-code of CQAR, which uses Metropolis-Hastlings sampling [20] to calculate
its predictions, however, it can be substituted with any other sampling algorithm. We show that CQAR produces better
results in comparison to QAR for estimating VaR of hacking breach inter-arrival times. Another advantage of CQAR is
that it re-estimates cyber risks dynamically after new observations become available. We also plot the average regret
between CQAR and the best QAR depending on time and show that it goes to zero as time increases. This empirically
conﬁrms the theoretical guarantees of the method and shows that CQAR is asymptotically as good as the best QAR
which was trained on the training data set.

2 Contributions

Our ﬁrst contribution is a new analysis and adaptation of QAR for calculating cyber VaR. To the best of our knowledge,
it was not done before. The method can be applied to any time-series data. It is common to predict the mean or median
values of time-series. Some research also focuses on the prediction of extreme values. This analysis provides a new
way to model extreme values that also comes with the desired conﬁdence level. QAR allows to model VaR for each
conﬁdence level with a separate stochastic process, and hence allows more ﬂexibility compared to previously proposed
approaches in the literature.

The second contribution is a new dynamic risk estimation method, Competitive Quantile Autoregression (CQAR).
There is a lack of research on dynamic cyber risk estimation. CQAR allows to re-estimate cyber risk at each time step
when new data becomes available and works for any time-series data. An important property of this approach is its
theoretical guarantee that it asymptotically predicts as well as the best QAR. The theoretical performance guarantees
provide conﬁdence in the prediction as they will hold for any new unseen data, while at the same time the method
allows adapting to a changing environment. As with QAR, CQAR is also more ﬂexible as it models each quantile with
a separate stochastic process.

The third contribution is the modelling of cyber hacking breaches with the proposed methods. We show that both
QAR and CQAR can be used to estimate VaR of cyber hacking breaches’ sizes and inter-arrival times. The coverage
tests show a good ﬁt of both approaches. We show that CQAR provides better results for modelling hacking breaches’
inter-arrival times compared to QAR. We also illustrate the behaviour of the average regret between CQAR and QAR
during the time and show that it conforms to the theoretical bounds of CQAR. The fully reproducible open-source code
of our implementation is available at GitHub 2.

2 https://github.com/alan-turing-institute/dynamic_cyber_risk

3

A PREPRINT - APRIL 23, 2021

3 Risk estimation with Quantile Autoregression

VaR is a widely used risk measurement in ﬁnance. VaRα is deﬁned as the loss corresponding to the α-quantile of
the distribution of the gain in the value of the portfolio over the next N days (Chapter 21.1 in [4]). In ﬁnance, VaR
provides an estimate of the maximum loss for a certain conﬁdence level and is important for budget allocation and
ﬁnancial reserves. Analogously, in cyber security, we want to estimate possible losses of extreme cyber events, such as
cyber attacks and subsequent data losses. Accurate forecasting of these adverse events can allow an adaptation of risk
mitigation strategies and better ﬁnancial planning.

Let the outcomes have a cumulative distribution FY (z), then we deﬁne

VaRα = inf{z : FY (z) ≥ α}

(1)

as the α-quantile of Y . Then we can estimate VaRα as α-quantile of outcomes.

QAR, proposed in [14], allows to model each quantile of outcomes with a separate autoregressive process. Let
time-series yt to be the p-order autoregressive process:

yt = θ0(Ut) + θ1(Ut)yt−1 + · · · + θp(Ut)yt−p,

(2)

where {Ut} is a sequence of i.i.d. standard uniform random variables. We want to estimate the coefﬁcients θj, which
are unknown functions [0, 1] → R. The αth conditional quantile of yt is:

Qyt(α|yt−1, yt−2, . . . , yt−p) = θ0(α) + θ1(α)yt−1 + · · · + θp(α)yt−p.

Equation (3) can be rewritten in analogous to the deﬁnition of quantile regression in [21]:

Qyt (α|Ft−1) = x(cid:48)

tθ(α),

where xt = (1, yt−1, . . . , yt−p)(cid:48), θ = (θ0, θ1, . . . , θt−p)(cid:48), and Ft−1 is the σ-ﬁeld generated by {ys, s ≤ t}.
The coefﬁcients θ(α) in (4) are found by minimising the following expression:

min
θ∈Rp+1

(cid:88)

t

λ(yt, x(cid:48)

tθ),

where λ(y, γ) is the pinball loss function:

λ(y, γ) =

(cid:26)α(y − γ),

(1 − α)(γ − y),

if y ≥ γ
if y < γ

.

4 Framework of competitive prediction

(3)

(4)

(5)

(6)

In this section, we describe the framework of competitive prediction. In this framework, a learner plays a game G
against other prediction strategies and a nature, which reveals the true outcomes. A game G = (cid:104)Ω, Γ, λ(cid:105) is a tuple with
the space of outcomes Ω, decision space Γ, and a loss function λ. In this paper, we consider Ω = Γ = R, and λ to be
the pinball loss, deﬁned in (6) for α ∈ (0, 1).

The learner works according to the following protocol:
Protocol 1.

for t = 1, 2, . . .

nature announces signal xt ⊆ Rp+1
learner outputs prediction γt ∈ Γ
nature announces outcome yt ∈ Ω
learner suffers loss λ(yt, γt)

end for

Before seeing the true outcome yt ∈ Ω, the learner needs to make a prediction γt ∈ Γ, based on a signal xt, which is
announced by nature. After seeing the true outcome yt, the learner’s loss λ(yt, γt) can be calculated.

In this paper, we assume that the outcomes follow the p-order autoregressive process deﬁned in (2). The learner makes
a prediction γt based on the signal xt = (1, yt−1, . . . , yt−p) ∈ Rp+1. For ease of notation, we replace θ(α) with θ. Let
us denote ξt(θ) to be the prediction (4) of QAR(p):

ξt(θ) = x(cid:48)

tθ.

4

(7)

A PREPRINT - APRIL 23, 2021

We denote the cumulative loss of the learner at step T as:

LT :=

T
(cid:88)

t=1

λ(yt, γt) =

(cid:88)

t=1,...,T :
yt>γt

α|yt − γt| +

(cid:88)

(1 − α)|yt − γt|.

t=1,...,T :
yt<γt

The cumulative loss of the prediction strategy θ, which at step T outputs ξt(θ):

Lθ

T :=

T
(cid:88)

t=1

λ(yt, ξt(θ)) =

(cid:88)

t=1,...,T :
yt>ξt(θ)

α|yt − ξt(θ)| +

(cid:88)

t=1,...,T :
yt<ξt(θ)

(1 − α)|yt − ξt(θ)|.

Our goal is to ﬁnd a strategy which at time t can compete with any prediction strategy ξt(θ) in terms of cumulative
losses.

We denote the regret at time T to be the difference between the cumulative losses of the learner and the prediction
strategy θ:

and the average regret at time T to be:

RT = LT − Lθ
T ,

ˆRT = (cid:0)LT − Lθ

T

(cid:1) /T.

5 Competitive Quantile Autoregression

(8)

(9)

In this section, we describe CQAR, which is built on WAAQR, proposed in [10], and adapted to time-series forecasting.
The algorithm works according to Protocol 1, which is different from the traditional machine learning approach, where
one needs a data set for the algorithm’s training. CQAR makes its prediction based on the signal, which is announces
by the nature. We assume that the outcomes follow p-order autoregressive process (2). At the time step T we observe
signal xT = (1, yT −1, . . . , yT −p), which contains p previous outcomes. Based on this signal, we need to output the
prediction γT before seeing the true outcome yT . In contrast to QAR, CQAR does not try to ﬁnd the optimal parameters
θ by minimising the pinball loss function (5). Instead, CQAR combines the predictions of a large pool of QAR in a way,
which is similar to a Bayesian mixture:

γT =

ξT (θ)q∗

T −1(θ)dθ,

(cid:90)

Θ

where

q∗
T (θ) = ZqT (θ) = Z exp

(cid:16)

−

1
√
T

(cid:16) (cid:88)

(1 − α)|yt − ξt(θ)| +

(cid:88)

α|yt − ξt(θ)|

(cid:17)

− a(cid:107)θ(cid:107)1

(cid:17)

,

t=1,...,T :
yt<ξt(θ)
where a is a regularisation parameter and Z is the normalising constant ensuring that (cid:82)
T (θ)dθ = 1, and (cid:107)θ(cid:107)1
denotes L1-norm of parameter θ. Function q∗
T (θ) has a meaning of the likelihood of the parameters θ at time step T .
The pseudo-code of CQAR uses the Metropolis-Hastings algorithm, which is a Markov chain Monte Carlo (MCMC)
method [20], though any other sampling algorithm could be used instead to approximate the integral (10). We start with
some initial parameter θ0 and at each step m we update:

t=1,...,T :
yt>ξt(θ)

Θ q∗

θm = θm−1 + N (0, σ2), m = 1, . . . , M,

where N (0, σ2) is the Gaussian proposal distribution with standard deviation σ, and M is the total number of MCMC
iterations. The Metropolis-Hastings randomly walks through the parameter space Θ, and either accepts or rejects
new parameters θ. If the likelihood of the new parameters (11) is higher than the old parameters’ likelihood, the new
parameters are always accepted. Otherwise, the new parameters can be either accepted or rejected. By moving this way,
the algorithm mostly samples parameters θ from the high-density regions of (11), only sometimes visiting the area of
low-density of the parameters’ likelihood. This procedure allows giving an accurate approximation of the integral (10).

We provide the pseudo-code of CQAR below. The algorithm has four input parameters: the number of MCMC iterations
M , the ‘burn-in period’ M0, the regularisation parameter a, and the standard deviation σ. The burn-in period M0 means
that we sample M0 values of the parameters, but they are not used in the integral approximation. It is useful as we
probably did not yet reach the area of high density of the parameters’ likelihood.

5

(10)

(11)

A PREPRINT - APRIL 23, 2021

CQAR

Parameters: number M > 0 of MCMC iterations,

burn-in period M0 > 0,

standard deviation σ > 0,

regularisation parameter a > 0

initialize θM

0 := 0 ∈ Θ

deﬁne q0(θ) := exp(−a(cid:107)θ(cid:107)1)

for t = 1, 2, . . . do

γt := 0

deﬁne qt−1(θ) by (11) if t > 1
read xt ∈ Rn
initialize θ0

t = θM
t−1

for m = 1, 2, . . . , M do

θ∗ := θm−1

t

+ N (0, σ2I)

ﬂip coin with success probability

min (cid:0)1, qt−1(θ∗)/qt−1(θm−1

t

)(cid:1)

if success then

θm
t

:= θ∗

else

θm
t
end if

:= θm

t−1

if m > M0 then

γt := γt + ξt(θm
t )

end for

output predictions γt = γt/(M − M0)

end for

An important property of CQAR is that it asymptotically predicts as well as the best QAR. The following theorem
provides the upper bound for the average regret between CQAR and the best QAR.

Lemma 1. (Theorem 1 in [10]) Let a > 0, A ≤ yt ≤ B for any t = 1, 2, . . . , T − 1, where T is a positive integer. For
every sequence of outcomes of length T , and every θ ∈ Rp+1 the average regret ˆRT between CQAR and QAR satisﬁes

ˆRT ≤

1
√
T

a(cid:107)θ(cid:107)1 +

1
√
T

(cid:32)

(cid:32)

(p + 1) ln

1 +

√

T
a

(cid:33)

max(1, B)

+ (B − A)2

(cid:33)
.

The theorem states that CQAR asymptotically predicts as well as the best QAR as the average regret ˆRT → 0, for
T → +∞. Although the bound contains the information about the minimum and maximum values of the outcomes at
the previous steps, it does not affect the asymptotic behaviour of the bound. The choice of the regularisation parameter
a affects the behaviour of the theoretical bound. As a result, it is important to pick the parameter which minimizes the
regret’s bound. However, in most cases, the optimal choice of the regularisation parameter cannot be found in advance
as the number of steps T is usually not known from the start. We discuss the choice of the parameters of CQAR in
detail in the experimental part of the article.

6

A PREPRINT - APRIL 23, 2021

6 Experiments

The open-source code of our implementation is fully reproducible and available at GitHub 2. Data is downloaded
from the PRC report 1, which contains the chronology of various types of data breaches such as card fraud, insider
incidents, paper, and computer physical losses, and unintended information disclosure. The companies which suffer
the incidents are classiﬁed into seven types of businesses: BSF (Financial and Insurance Services Businesses), BSR
(Retail/Merchant including Online Retail Businesses), BSO (Other Businesses), EDU (Educational Institutions), GOV
(Government and Military), MED (Medical and Healthcare), and NGO (Nonproﬁts). The report contains 9015 data
breaches between January 2005 and September 2019. Analogous to [12], we focus only on hacking breaches. The total
number of observations after removing all incomplete, unknown, and missing breaches are 1602. The data is divided
into training and test data sets in the proportion of 60% to 40%: the size of the training set is 956, whereas that of the
test set is 636.

6.1 Data exploration

We start with data pre-processing. Most days have only one incident per day, 232 days have two incidents, 52 days have
three, and 35 days are with more than three incidents. Similarly to [12], if several events occur in one day, they are
analysed as separate incidents. For these events, we generate a random number from zero to one, which corresponds to
some time during the day. After that, these events are sorted by these randomly generated numbers.

Figure 1 visualises inter-arrival times and the logarithm of breach size, where size is the total number of accounts
affected by the breach. We visualise breach sizes on a logarithmic scale because some of the incidents exhibit particularly
extreme values. Table 1 describes the summary statistics of breach sizes, where sd denotes the standard deviation.
The analysis in [12] describes the period between January 2005 and April 2017 and contains 600 hacking breaches.
We observe that more than 1000 incidents have been added to the report in the last two years. It indicates that either
hacking incidents become more frequent or the companies become more transparent about reporting their data breaches.
The largest number of incidents are reported in the medical and healthcare sector. The largest incident was reported
by Yahoo on the 14th of December 2016, which compromised users’ data from three billion accounts. Table 2 shows
the same statistics for inter-arrival times. We observe that the mean values of inter-arrival times are less than the
standard deviations for each category. It provides evidence that inter-arrival times cannot be modelled with the Poisson
distribution. A similar conclusion can be drawn for the breach sizes.

(a) Logarithm of breach sizes

(b) Inter-arrival times

Figure 1: Visualisation of breach sizes and inter-arrival times

Analogously to [12], we check auto-correlation (ACF) and partial auto-correlation functions (PACF) of the logarithm of
breach size and logarithm of inter-arrival time. ACF measures the linear dependence between the lags of time-series,
whereas PACF is the correlation between lags adjusted for the contributions of observations in between [22, 23]. These
measures are used to ﬁnd if observations exhibit a correlation between each other and can be modelled with a stochastic
process. Figure 2 shows that both breach sizes and inter-arrival times exhibit signiﬁcant auto-correlations above the
threshold values depicted with dotted lines. It indicates that they can be modelled with stochastic processes.

6.2 Quantile Autoregression

In this section, we model VaRα of the logarithm of breach sizes and the logarithm of inter-arrival times with QAR.
First, we need to pick the optimal lag of QAR. Analogous to the problem of choosing the optimal degree of polynomial
regression, the optimal order of the autoregressive process (2) can be chosen by some information criterion. We use the

7

A PREPRINT - APRIL 23, 2021

type of
organi-
sation
BSF
BSO
BSR
EDU
GOV
MED
NGO
Total

Table 1: Summary statistics of breach sizes

min

median
(×103)

mean
(×106)

sd
(×106)

max
(×106)

6
2
1
12
8
1
13
1

1.7
10.4
2.1
8.5
6.0
4.0
4.0
4.6

4.8
26.4
6.7
222.5
457.7
200.1
142.1
4.5

21.3
214.8
33.3
2.7
2.4
2.9
0.6
78.6

145.5
3000.0
327.0
40.0
21.5
78.8
3.0
3000

number
of obser-
vations
111
208
138
223
93
805
24
1602

Table 2: Summary statistics of breach inter-arrival times

type of
organi-
sation
BSF
BSO
BSR
EDU
GOV
MED
NGO
Total

min

median mean

sd

max

0.0111
0.0480
0.0233
0.0134
0.0842
0.0019
0.0131
0.0019

2.00
1.00
2.00
3.00
2.00
1.00
1.00
2.00

4.16
3.08
3.52
5.86
3.66
2.85
2.70
3.49

5.78
4.18
5.09
8.12
5.06
4.10
3.56
5.20

36
38
33
59
28
37
13
59

number
of obser-
vations
111
208
138
223
93
805
24
1602

Bayesian Information Criterion (BIC) [24] to pick the optimal lag of QAR. BIC is deﬁned as follows:

BIC = −2 ln L + p ln N,

where L is the maximum of the model’s likelihood, p is the number of parameters, and N is the sample size. BIC
penalises complex models with large lag number p, and smaller values of the criterion are favourable. Figure 3a
shows BIC values for a different number of lags of QAR, which is built on the training data for quantiles equal to 0.5,
i.e. median values. The smallest values of BIC correspond to the optimal choice of the lag and are depicted with the
red dots. We observe that the optimal values of lag are equal to six in the case of the breach size, and the optimal lag for
the inter-arrival time is ﬁve.

We then build QAR for the optimal lags on the training data set. These models are used for making predictions of VaRα
on the test data set. We pick the signiﬁcance levels to be α = 0.9, 0.92, 0.95. From the risk perspective, it is important
to estimate how large the potential losses might be in order to prevent or hedge these losses. Therefore, α values should
be large. Figures 4a, 4b illustrate the predictions of QAR for breach sizes and inter-arrival times respectively, on the test
data.

If the observed value exceeds the predicted VaRα , we call it violation. The Kupiec unconditional coverage test [15]
measures whether the number of violations is consistent with the conﬁdence level. For example, if α = 0.9, then the
percent of observation, which exceeds the predicted VaR0.9, should be close to 0.1. The null hypothesis H0 is that
the observed violation rate is equal to 1 − α. The Kupiec unconditional coverage test focuses only on the number of
violations. However, we would like to test whether these exceptions are evenly spread over time. The null hypothesis
H0 for the Christoffersen conditional coverage test [16] is that the probability of observing a violation at some time
point does not depend on whether a violation occurred. Table 3 illustrates the results of backtesting of both coverage
tests for breach size and inter-arrival time respectively, on the test data. The table shows the expected number of
violations of the considered conﬁdence level and the actual number of violations of the considered method. We use the
following notations: exp (expected number of violations), act (actual number of violations), the unconditional coverage
test p-value (uc.LRp), the conditional coverage test p-value (cc.LRp), the unconditional coverage test decision (uc.D),
and the conditional coverage test decision (cc.D), fail to reject the null hypothesis H0 (FR), reject the null hypothesis H0
(R). We can see that QAR(6) fails to reject the null hypothesis H0 for both unconditional and conditional coverage tests,
which means that the models ﬁt well and describe the quantiles of breach size correctly. In the case of inter-arrival time,
QAR(5) ﬁts well for 0.9 and 0.95 quantiles, however, for 0.92 the conditional coverage test rejects the null hypothesis.
In the next section, we show how we can improve the prediction of the breach inter-arrival times by applying CQAR.

8

A PREPRINT - APRIL 23, 2021

(a) Logarithm of breach
sizes

(b) Logarithm of inter-
arrival times

(c) Logarithm of breach
sizes

(d) Logarithm of inter-
arrival times

Figure 2: ACF and PACF

(a) Logarithm of breach sizes

(b) Logarithm of inter-arrival times

Figure 3: BIC for different lags

6.3 Competitive Quantile Autoregression

In this section, we estimate the hacking breaches’ inter-arrival times with CQAR. In contrast to QAR, CQAR does not
need a training data set. The algorithm starts its training when it gets the ﬁrst observation of the test data set. However,
as we have the training data set available, we pick the regularisation parameter a and the standard deviation σ from
the training data. Table 5 illustrates the acceptance ratio and the total pinball loss of CQAR on the training data set
for different parameters a and σ. The lowest pinball loss on the training data is achieved with a = 1 and σ = 0.7,
which is depicted in bold. The corresponding acceptance ratio for these parameters is 0.27. It is important to ‘track’
the acceptance ratio of CQAR. A very high acceptance ratio might indicate that the algorithm moves too slowly to the
optimal parameter θ. Therefore, the total number of iterations and the burn-in period should be increased. Another

9

A PREPRINT - APRIL 23, 2021

(a) Logarithm of breach sizes

(b) Logarithm of inter-arrival times

Figure 4: Predictions of QAR

Table 3: Coverage tests for QAR for breach sizes at test data

method
QAR(6)
QAR(6)
QAR(6)

quantile
0.90
0.92
0.95

exp
63
50
31

act
55
44
29

uc.LRp
0.2509
0.3095
0.6116

cc.LRp
0.4784
0.5103
0.7456

uc.D cc.D
FR
FR
FR
FR
FR
FR

option is to increase the standard deviation σ. Table 5 shows that increasing σ leads to decreasing of the acceptance
ratio.

Table 6 shows the results of the backtesting for CQAR(5) on the test data set. Note that even though we pick the
parameters of the CQAR using the prior knowledge, the algorithm starts with zero parameters θ and trains using only
the test data set. We can see from the table that both unconditional and conditional coverage tests for CQAR(5) fail to
reject the null hypothesis. Therefore, CQAR(5) produces better results for predicting breach inter-arrival times than
QAR(5). The p-values of CQAR are also higher than p-values of QAR, apart from the cc.LRp for 0.90 quantile.

The important property of CQAR is that it asymptotically predicts as well as any QAR. Figure 5a illustrates the
predictions of CQAR for α = 0.9, 0.92, 0.95 on the test data. Figure 5b shows the average regret between CQAR(5)
and QAR(5). As we discussed, CQAR starts with zero parameters θ at the beginning of its training, and as a result, the
average regret is high at the start. However, it becomes close to zero for all considered quantiles as time increases. The
resulting graph conﬁrms the theoretical behaviour of the average regret described in Lemma 1.

(a) Predictions of CQAR

(b) Average regret between CQAR and QAR

Figure 5: CQAR

7 Conclusions

In this paper, we have presented two approaches to cyber VaR estimation of time-series. VaR gives a prediction of
extreme values with the desired conﬁdence level for a different kind of time-series. These estimates can sequentially be
translated into the monetary VaR, which is essential for budget planning and allocation. The ﬁrst approach to estimate
VaR is based on QAR, which provides a new way to model extreme values with the desired conﬁdence level. QAR is

10

A PREPRINT - APRIL 23, 2021

Table 4: Coverage tests for QAR for inter-arrival times at test data
uc.D cc.D
method
FR
FR
QAR(5)
R
FR
QAR(5)
FR
FR
QAR(5)

quantile
0.90
0.92
0.95

uc.LRp
0.3062
0.1360
0.2765

cc.LRp
0.3539
0.0146
0.1463

exp
63
50
31

act
56
41
26

Table 5: Parameters of CQAR on training

(a) Acceptance ratio

a \ σ
0.1
0.5
1

0.5
0.69
0.61
0.53

0.7
0.47
0.36
0.27

1
0.22
0.12
0.06

(b) Pinball losses

a \ σ
0.1
0.5
1

0.5
281.69
177.52
137.20

0.7
281.74
171.76
135.42

1
268.00
172.50
138.21

more ﬂexible compared to the previously proposed approaches as it allows to model VaR for each conﬁdence level with
a separate stochastic process, and hence relies on fewer assumptions on the nature of the data.

The second proposed approach, called CQAR, provides a new framework for dynamic cyber risk estimation. The
method re-estimates VaR at each step as soon as new data becomes available. A signiﬁcant property of this approach is
the theoretical guarantee that it asymptotically performs as well as the best QAR found retrospectively. This important
property provides conﬁdence in the prediction as it will hold for any new unseen data, while at the same time the
method allows adapting to a changing environment.

Finally, we demonstrate that both methods provide a good ﬁt for predicting the size and inter-arrival times of cyber
hacking breaches by running coverage tests. We illustrate the behaviour of the average regret between which conforms
to the theoretical bounds of CQAR. In addition, we provide a fully-reproducible code of our experiments.

Acknowledgments

This work was supported, in whole or in part, by the Bill & Melinda Gates Foundation [INV-001309]. Under the grant
conditions of the Foundation, a Creative Commons Attribution 4.0 Generic License has already been assigned to the
Author Accepted Manuscript version that might arise from this submission.

References

[1] S. Kaplan and J. B. Garrick. On the quantitative deﬁnition of risk. Risk Analysis, 1:1, 1981. https://doi.org/

10.1111/j.1539-6924.1981.tb01350.x.

[2] G. A. Holton. Deﬁning risk. Financial Analysts Journal, 60:6:19–25, 2004. https://doi.org/10.2469/faj.

v60.n6.2669.

[3] N. Jones and B. Tivnan. Cyber risk metrics survey, assessment, and implementation plan. Case Number 18-1246,

The Homeland Security Systems Engineering and Development Institute, 2018.
[4] J. C. Hull. Options, Futures, and Other Derivatives. Prentice Hall, 6th edition, 2006.
[5] C. Peng, M. Xu, S. Xu, and T. Hu. Modeling and predicting extreme cyber attack rates via marked point processes.

Journal of Applied Statistics, 2016. https://doi.org/10.1080/02664763.2016.1257590.

[6] M. Raugas, J. Ulrich, R. Faux, S. Finkelstein, and C. Cabot. Cyberv@r. 2013.

[7] S. Taubenberger, J. Jürjens, Y. Yu, and B. Nuseibeh. Problem analysis of traditional IT-security risk assessment

methods. International Information Security Conference, pages 259–270, 2011.

[8] Arnes A., K. Sallhammar, K. Haslum, T. Brekne, M. E. G. Moe, and S. J. Knapskog. Real-time risk assessment
with network sensors and intrusion detection systems. In Computational Intelligence and Security, pages 388–397,
2005. https://doi.org/10.1007/11596981_57.

[9] S. Li, S. Zhao, Y. Yuan, Q. Sun, and K. Zhang. Dynamic security risk evaluation via hybrid bayesian risk graph
in cyber-physical social systems. IEEE Transactions on Computational Social Systems, 5:4:1133–1141, 2018.
https://doi.org/10.1109/TCSS.2018.2858440.

11

A PREPRINT - APRIL 23, 2021

Table 6: Coverage tests for CQAR for inter-arrival times at test data
uc.D cc.D
method
FR
FR
CQAR(5)
FR
FR
CQAR(5)
FR
FR
CQAR(5)

quantile
0.90
0.92
0.95

uc.LRp
0.4808
0.6514
0.3705

cc.LRp
0.0785
0.1025
0.4844

exp
63
50
31

act
69
54
27

[10] R. Dzhamtyrova and Y. Kalnishkan. Competitive online quantile regression. In International Conference on

Information Processing and Management of Uncertainty in Knowledge-Based Systems, pages 499–512, 2020.

[11] B. Edwards, S. Hofmeyr, and S. Forrest. Hype and heavy tails: A closer look at data breaches. Journal of

Cybersecurity, 2:1:3–14, 2016.

[12] M. Xu, K. M. Schweitzer, R. M. Bateman, and S. Xu. Modeling and predicting cyber hacking breaches. IEEE

Transactions on Information Forensics and Security, 13:10, 2018.

[13] Douglas W. Hubbard and Richard Seiersen. How to Measure Everything in Cybersecurity Risk. Wiley, 2016.

[14] R. Koenker and Z. Xiao. Quantile autoregression. Journal of the American Statistical Association, 2006.

[15] P. Kupiec. Techniques for verifying the accuracy of risk measurement models. Journal of Derivatives, pages

3(2):73–84, 1995.

[16] P. Christoffersen. Evaluating interval forecasts. International Economic Review, page 39(4):841, 1998.

[17] V. Vovk. Aggregating strategies. In Proceedings of the 3rd Annual Workshop on Computational Learning Theory,

pages 371–383, San Mateo, CA, 1990. Morgan Kaufmann.

[18] Y. Kalnishkan and M. Vyugin. The weak aggregating algorithm and weak mixability. Journal of Computer and

System Sciences, pages 74: 1228–1244, 2008.

[19] V. Vovk. Competitive on-line statistics. International Statistical Review, 69(2):213–248, 2001.

[20] C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan. An introduction to MCMC for machine learning. Machine

Learning Journal, page 50:5–43, 2003.

[21] R. Koenker and G. Bassett. Regression quantiles. Econometrica, pages 46: 33–50, 1978.

[22] R. J. Hyndman and G. Athanasopoulos. Forecasting: Principles and Practice. 2018.

[23] R. H. Shumway and D. S. Stoffer. Time Series Analysis and Its Application With R Examples. Springer, Fourth

Edition, 2016.

[24] G. Schwarz. Estimating the Dimension of a Model. Annals of Statistics, Volume 6, Number 2, 1978. https:

//doi.org/10.1214/aos/1176344136.

[25] T. Levina, Y. Levin, J. McGill, M. Nediak, and V. Vovk. Weak aggregating algorithm for the distribution-free

perishable inventory problem. Operations Research Letters, pages 38: 516–521, 2010.

Appendix

Lemma 2. (Lemma 2 in [25]) Let λ(y, γ) ≤ L for all y ∈ Ω and γ ∈ Γ. The Weak Aggregating Algorithm guarantees
that, for all T

√

LT ≤

Proof of Lemma 1

(cid:18)

T

− ln

exp

−

(cid:90)

Θ

(cid:18)

(cid:19)

Lθ
T√
T

(cid:19)

P0(dθ) + L2

.

Proof. The proof is the same as the proof of Theorem 1 from [10] adjusted for the time-series data. We provide the
proof here for completeness.
Recall, that the learner makes a prediction γt based on the signal xt = (1, yt−1, . . . , yt−p) ∈ Rp+1, and outcomes
come from the interval [A, B]. We choose an initial distribution of parameters

P0(dθ) =

(cid:17)p+1

(cid:16) a
2

e−a(cid:107)θ(cid:107)1dθ,

12

(12)

for some a > 0, and θ ∈ Rp+1. Let us deﬁne the truncated expert ˜Eθ which at step t outputs:

A PREPRINT - APRIL 23, 2021

if x(cid:48)
if A ≤ x(cid:48)
if x(cid:48)
T the cumulative loss of expert ˜Eθ at the step T :

A,
x(cid:48)
tθ,
B,

˜ξt(θ) =



tθ < A

tθ > B




tθ ≤ B

Let us denote ˜Lθ

˜Lθ

T :=

T
(cid:88)

t=1

λ(yt, ˜ξt(θ)).

.

(13)

(14)

We apply the algorithm’s prediction (10) with truncated experts ˜Eθ. As experts output predictions inside the interval
[A, B], and the algorithm’s prediction is a weighted average of experts’ predictions, then γt lies in the interval [A, B],
∀t.

We can bound the maximum loss at each time step:

L :=

max
y∈[A,B], γ∈[A,B]

λ(y, γ) ≤ (B − A) max(α, 1 − α) ≤ B − A.

(15)

Lemma 2 provides the theoretical guarantees for the strategy that follows WAA (10) used by our algorithm. Applying
Lemma 2 for initial distribution (12) and putting the maximum loss (15) we obtain:

√

(cid:18)

T

− ln

LT ≤

(cid:18)(cid:16) a
2

(cid:17)p+1 (cid:90)

(cid:19)

e− ˜J(θ)dθ

+ (B − A)2

(cid:19)

,

Rp+1

where

˜J(θ) :=

˜Lθ
T√
T

+ a(cid:107)θ(cid:107)1.

(16)

(17)

For all θ, θ0 ∈ Rp+1 we have:
(cid:88)

(cid:88)

|x(cid:48)

tθ − yt| ≤

|x(cid:48)

tθ0 − yt| +

(cid:88)

|x(cid:48)

tθ − x(cid:48)

tθ0|

t=1,...,T :
yt<x(cid:48)
tθ

t=1,...,T :
yt<x(cid:48)
tθ

t=1,...,T :
yt<x(cid:48)
tθ

(cid:88)

≤

|x(cid:48)

tθ0 − yt| +

(cid:88)

t=1,...,T :
yt<x(cid:48)
tθ

t=1,...,T :
yt<x(cid:48)
tθ

max
t=1,...,T

(cid:107)xt(cid:107)∞(cid:107)θ − θ0(cid:107)1

|x(cid:48)

tθ0 − yt| + T max(1, B)(cid:107)θ − θ0(cid:107)1.

(18)

(cid:88)

≤

t=1,...,T :
yt<x(cid:48)
tθ

Analogously, we have:

(cid:88)

|x(cid:48)

tθ − yt| ≤

(cid:88)

|x(cid:48)

tθ0 − yt| + T max(1, B)(cid:107)θ − θ0(cid:107)1.

t=1,...,T :
yt>x(cid:48)
tθ

t=1,...,T :
yt>x(cid:48)
tθ

By multiplying inequality (18) by (1 − α), inequality (19) by α and summing them, we have:
T ≤ Lθ0
Lθ

T + T max(1, B)(cid:107)θ − θ0(cid:107)1.

(19)

(20)

The cumulative loss of truncated expert ˜Eθ cannot exceed the cumulative loss of non-truncated expert Eθ for all
θ ∈ Rp+1:

By dividing (20) by

√

T and adding a(cid:107)θ(cid:107)1 to both parts, we have:

˜JT (θ) ≤ JT (θ) ≤ JT (θ0) +

√

T max(1, B)(cid:107)θ − θ0(cid:107)1 + a ((cid:107)θ(cid:107)1 − (cid:107)θ0(cid:107)1)

˜Lθ

T ≤ Lθ
T .

(21)

T max(1, B) + a

(cid:17)

(cid:107)θ − θ0(cid:107)1,

(22)

(cid:16)√

≤ JT (θ0) +

13

A PREPRINT - APRIL 23, 2021

where

JT (θ) :=

Lθ
T√
T

+ a(cid:107)θ(cid:107)1.

(23)

Let us denote bT =

√

T max(1, B) + a. We evaluate the integral:

(cid:90)

Rp+1

e− ˜JT (θ)dθ ≥

(cid:90)

e−(JT (θ0)+bT (cid:107)θ−θ0(cid:107)1)dθ = e−JT (θ0)

(cid:90)

(cid:90)

. . .

R

R

e−bT

(cid:80)p+1

i=1 |θi−θi,0|dθi

= e−JT (θ0)

Rp+1
(cid:90)

. . .

R

(cid:90)

p+1
(cid:89)

R

i=1

e−bT |θi−θi,0|dθi = e−JT (θ0)

p+1
(cid:89)

(cid:90)

R

i=1

e−bT |θi−θi,0|dθi = e−JT (θ0)

(cid:19)p+1

.

(cid:18) 2
bT

By putting this expression in (16) we obtain:

LT ≤ Lθ

T +

√

T a(cid:107)θ(cid:107)1 +

√

(cid:32)

(cid:32)

T

(p + 1) ln

1 +

√

T
a

(cid:33)

max(1, B)

+ (B − A)2

(cid:33)
.

By putting this expression in formula for the average regret (9) we obtain the theoretical bound from Lemma 1.

14

