Automated Malware Design for Cyber Physical
Systems

Ashraf Tantawy
Department of Electrical and Computer Engineering
Virginia Commonwealth University
Richmond, VA, USA
ORCID: 0000-0003-0958-1633

1
2
0
2

l
u
J

6

]

R
C
.
s
c
[

1
v
8
3
5
2
0
.
7
0
1
2
:
v
i
X
r
a

Abstract—The design of attacks for cyber physical systems is
critical to assess CPS resilience at design time and run-time,
and to generate rich datasets from testbeds for research. Attacks
against cyber physical systems distinguish themselves from IT
attacks in that the main objective is to harm the physical system.
Therefore, both cyber and physical system knowledge are needed
to design such attacks. The current practice to generate attacks
either focuses on the cyber part of the system using IT cyber
security existing body of knowledge, or uses heuristics to inject
attacks that could potentially harm the physical process. In
this paper, we present a systematic approach to automatically
generate integrity attacks from the CPS safety and control
speciﬁcations, without knowledge of the physical system or its
dynamics. The generated attacks violate the system operational
and safety requirements, hence present a genuine test for system
resilience. We present an algorithm to automate the malware
payload development. Several examples are given throughout the
paper to illustrate the proposed approach.

Index Terms—Cyber Physical System, CPS, Attack, Malware,
Safety, Security, Control System, SCADA, Formal Speciﬁcation,
Safety Property.

I. INTRODUCTION AND RELATED WORK

A Cyber-Physical System (CPS) is an integration of compu-
tation, networking, and physical processes to monitor, control,
and safeguard the physical system [1]. Most cyber physical
systems are mission-critical, such as process control plants,
energy grid, autonomous vehicles, and pacemakers, just to
name a few [2]. These systems progressively replace closed-
source hardware and software components with open source
embedded system designs, operating systems, and standard
communication protocols, mainly to simplify integration ef-
forts and to reduce total cost of ownership. This open-source
paradigm has exposed, and continues to expose, these mission
critical systems to a myriad of security threats [3]. When such
threats materialize into a successful attack, the outcome could
be catastrophic in terms of human casualty, environmental
damage, and ﬁnancial loss.

To protect against CPS cyber threats, system designers need
to be at least one step ahead of attackers. This requires a
rigorous security risk assessment process at different stages
of the design process [4]. A critical component of security
risk assessment is penetration testing, where a set of attacks
are launched to assess system vulnerabilities and consequence

severity. For penetration tests to be effective, attacks should
be comprehensive and non-generic, i.e., closely-related to the
system under study. The current penetration testing practice
relies on the pen tester expertise as well as IT security type
attacks. Little work has been done on how to design attacks
for a given cyber physical system.

In this work, we make an initial step towards automating
malware design for cyber physical systems. The idea is to
extract system knowledge embedded in control and safety
software programs used to safeguard the system. By simple
reversal of the system operating logic, we guarantee the
generation of a relevant system attack with a consequence
varying from operation disruption to a system hazard. Little
pre-requisite knowledge is required to design such attacks.
Therefore, they could be utilized by penetration testers to study
the impact of viable attack scenarios against the CPS under
consideration.

Research on attack design for cyber physical systems is
a relatively new ﬁeld. Most of the work so far has focused
on modeling the attacker behavior and objectives, collectively
referred to as attacker proﬁle [5]–[8]. This work attempts to
couple the attacker objectives to the cyber physical system
in an abstract way that
is applicable to any given CPS.
Translation of abstract attacker models into actual attacks that
are relevant to the given CPS is not yet explored. Another
line of research is the development of specialized attacks for
CPS protocols, such as Modbus and DNP3 protocols [9]–[15].
This work is important but still lacks a systematic way to
include physical system knowledge into the attacks. A related
emerging research is the integration of security and safety
speciﬁcations in the design process rather than treating security
as a post-design issue [16], [17]. The automated generation of
cyber attacks from the model-based engineering design process
is still unexplored.

The work in this paper distinguishes itself in that it uti-
lizes the physical system knowledge in a systematic way
to design integrity attacks customized to the given cyber
physical system, an activity that is currently done in an ad-hoc
manner. We utilize system speciﬁcations expressed in formal
logic, which is a well-established domain [2]. We summarize
the key contributions of the paper as follows: (1) A formal
method to design a malware for a given cyber physical system,
(2) Automated generation of malware payload given a CPS

 
 
 
 
 
 
Fig. 1. Cyber Physical System Architecture

safety program and optionally a control program, (3) Inference
of system dynamic relationships from the control algorithms
without knowledge about system model, and (4) A systematic
process for attack injection.

Figure 1 shows the classical cyber physical system architec-
ture that is utilized in the paper. The control system typically
controls the normal operation of the system and keeps the
state variables within the pre-designed operating envelope.
The safety system is an independent embedded system, with
its own sensors and actuators. The safety system is normally
dormant and intervenes only when the the control system is
incapable of keeping the state variables inside the operating
envelope. The intervention is mostly in the form of a system
emergency shutdown to prevent any existing hazard from
developing into a catastrophic event.

The paper is organized as follows: Section II is a brief
introduction to safety system requirements and their imple-
mentation in software. Section III describes the attack design
methodology, including how to infer plant dynamics from
the control software. Section IV brieﬂy explains the attack
injection process and how the presented work ﬁts into the
big picture of penetration testing. The work is concluded in
Section VI. Instead of presenting a case study at the end of
the paper, we utilize running examples throughout the paper
to avoid abstract discussion and to ensure effortless reading.

II. SAFETY REQUIREMENTS SPECIFICATION
A safety property for a cyber physical system is generally
formulated as a temporal
logic formula [18]. One of the
simplest and most common forms of temporal logic formulae
is the invariant; a property that holds true at all times during
system operation [2]. The invariant
is often expressed in
natural language and translated into a logical expression for
software or hardware implementation. An example invariant
for a trafﬁc control system is ”Pedestrian crossing light must
be off when the trafﬁc light is green”. Another example invari-
ant for a process control plant could be ”The pump should be
stopped when the tank level is low”. System properties for a
CPS could be deﬁned for both the physical and cyber system.
An example invariant for a multi-threaded C language code
running on a controller is ”The program should never get into
a deadlock”. We focus in this work on safety properties of the
physical system, since this is what distinguishes CPS security
from IT security domain.

In practice, an invariant is ensured by implementing a logic
formula that relates a subset of system state variables to a

Fig. 2. A tank-pump system with pump safety deﬁned by tank level and
discharge pressure.

system output (actuator). The state variables could represent
sensor measurements, human input, or environmental state.
The invariant property could be expressed by the implication
(P (x) =⇒ u), where P is a propositional function, x is
a subset of the state variables, and u is an actuator output.
Since this is an invariant, at any time of system operation
and at any state, whenever P (x) is true, u must be true. The
invariant could be implemented in software using an IF THEN
construct: IF P (x) THEN u = 1.

Example 1. Consider the simple tank-pump system in Figure
2. The pump safety property: ”The pump should stop if the
tank level is below 10% or the pump discharge pressure is
above 5 bars”. Let x1 denote the tank level, x2 denote the
pump discharge pressure, and u denote the pump output, where
u = 1 when the pump is running. The invariant property is
then expressed as:

(x1 < 10%) ∨ (x2 > 5)
(cid:124)
(cid:125)
(cid:123)(cid:122)
P (x)

=⇒ ¬u

and the software implementation will be:

IF (x1 < 10%) ∨ (x2 > 5) THEN u = 0

(1)

(cid:4)

III. ATTACK DESIGN
The invariant expression (P (x) =⇒ u) deﬁnes the logic for
u = 1, but assumes the logic for u = 0 is deﬁned elsewhere
by another logic. For attack design purposes, we utilize the bi-
conditional invariant deﬁnition (P (x) ⇐⇒ u), which clears
the ambiguity for u value when P (x) is false. This could be
implemented in software using an IF THEN ELSE construct
on the form: IF P (x) THEN u = 1 ELSE u = 0.

A simple approach to design an attack out of safety require-

ments is to negate the invariant:

¬ (P (x) ⇐⇒ u) ≡ (P (x) ⇐⇒ ¬u)

(2)

which has the software implementation:

IF P (x) THEN u = 0 ELSE u = 1

This is intuitive, because it implements the exact opposite of
the safety property, which should be capable of producing a
safety hazard in the system. We note that the right hand side
in (2) has the logical equivalence:

knowledge, which is typically not available for the attacker.
Regardless, the disruption attack is still a viable scenario for
the penetration tester to assess CPS resilience against a highly-
probable attack.

(P (x) ⇐⇒ ¬u) ≡ (¬P (x) ∧ u)
(cid:125)

(cid:124)

(cid:123)(cid:122)
Disruption

∨ (P (x) ∧ ¬u)
(cid:123)(cid:122)
(cid:125)
Hazard

(cid:124)

(3)

B. Hazard Attack

which could be interpreted as two ways to inject the attack.
The ﬁrst way, represented by the ﬁrst term on the right hand
side in (3), enforces the actuator value without the cause
P (x) being present. This results in a system disruption, but
no hazard is involved since the safe action has already been
taken by enforcing the safe actuator value. The second way,
represented by the second term on the right hand side in
(3), prevents the safe actuator action from being taken while
the cause P (x) being present. This results in a hazard as it
violates the safety property. Therefore, the utilization of a bi-
conditional invariant deﬁnition results in two possible attack
scenarios.

Example 2. Consider again the pump safety property in Ex-
ample 1. Applying the attack design methodology by negating
the invariant results in the software implementation:

IF (x1 < 10%) ∨ (x2 > 5) THEN u = 1
ELSE u = 0 (Disruption)

(Hazard)

This code results in two effects: (1) System hazard: resulting
from unsafe pump operation. When the tank level is low, the
pump will turn on, potentially resulting in pump cavitation
damage. In addition, if the pump discharge pressure is high,
possibly due to a blockage in the discharge pipe, the pump will
turn on, causing a mounting pressure and potential rupture in
the piping system. (2) System service disruption: when both
tank level and discharge pressure are normal, the code stops
the pump, although the pump is supposed to operate to transfer
(cid:4)
the product in the tank.

A. Disruption Attack

The prerequisite system state for the disruption attack,
¬P (x), is essentially the absence of a state condition that
it represents the
requires a safety action. In other words,
normal state of the system. Therefore, this prerequisite system
state will almost always be available for the attacker. The
attacker does not need to enforce a speciﬁc system state in
order to launch the attack, as in the case of hazard attacks
discussed below. Rather, the attacker will just reverse the
actuator output
to cause system disruption. Although this
seems as a soft target for the attacker, nothing comes for
free. The consequence of the attack will just be a process
disruption, and its associated ﬁnancial loss. However, no safety
hazard will be expected from such an attack. Moreover, it
is hard to conceal a disruption attack for a long time from
system operators, as the disruption of one system component
propagates quickly to other connected components, and the
attacker’s task to conceal the attack would require faking many
state variables simultaneously. This requires a global system

The hazard attack requires the speciﬁc system state con-
dition as per the safety speciﬁcation, P (x). Therefore, the
attacker should drive the system to the required state to launch
the attack. This is harder than the disruption attack because
it requires some system knowledge. Of course, the attacker
could design the malware as dormant waiting for this speciﬁc
system state to be reached to launch the attack. This speciﬁc
system state may take years to be reached. Therefore, in the
rest of the paper, we will discuss the proactive attack approach,
where the attacker tries to drive the system to the desired state.
We classify system states into physical states, environmental
states, and operator action states, i.e., x = [xp xe xo].
The physical state xp represents CPS measurements, such as
car velocity, process tank level, or pump on/off status. The
environmental state xe typically represents disturbances affect-
ing the CPS performance such as wind and temperature. The
operator actions state xo represents inputs from the operator
to manually control the system. Among these three categories,
the operator action is the easiest because the attacker can
simply enforce the operator action variables to the desired
values. The environmental states are out of control of both
the system operator and the attacker. Therefore, the attacker
can do nothing about it if the system state condition includes
environmental states. For the physical state xp, if it is a system
component state, it could be enforced by directly injecting
an actuator attack. However, if the state represents system
physical measurements that need to be enforced, then a sensor
injection attack will not work, the physical system itself has
to be driven to generate the desired state. This needs a more
in-depth discussion.

In cyber physical systems, the system state could be changed
by manipulating system inputs u, also known as actuator
values. Knowledge about system dynamics are needed to force
the states to speciﬁc values. This knowledge is typically not
available to the attacker. However, the relationship between
system inputs u and states xp could be inferred from the
control algorithms implemented on the system controllers.
In this paper, we give an example for Proportional Integral
Derivative (PID) controllers that are pervasively used in cyber
physical systems. However, the discussion could be equally
adopted to other control algorithms.

The PID controller is a single-input single-output controller
that works on the error between a setpoint and system state
measurement. The error is used to calculate the proportional,
integral, and derivative terms of the controller, which are
fused together to calculate the controller output that will drive
the system input actuator to bring the system state as close
as possible to the required setpoint. Figure 2 illustrates an
example PID controller feedback control system to regulate

the tank level. The PID controller algorithm is the simple, yet
powerful, equation:

u(t) = Kpe(t) + Ki

(cid:90) t

0

e(t)dt + Kd

de(t)
dt

e(t) = SP(t) − PV(t)

(4)

(5)

where SP is the setpoint, PV is the process variable, and
Kp, Ki, Kd are the controller parameters, which are tuned
according to the controlled system. In order for the PID
controller to work properly, it has to form a negative feedback
loop with the system. In other words, as the error increases, the
controller output should select the right direction of change,
either increase or decrease, in order to reduce the error. This
is called in industry ”Controller Action”, and depends on how
the system dynamics work and on the actuator. If the controller
output increases as the error increases, it is called ”Direct
Acting” controller. On the other hand, a ”Reverse Acting”
controller decreases its output as the error increases. This is
best illustrated by an example before explaining how to exploit
this knowledge to launch an attack.

Example 3. Consider the Tank-Valve system in the left hand
side of Figure 2. As the tank level deviates from the setpoint,
the valve opens or closes to bring the level back to its desired
setpoint according to the PID controller algorithm. Suppose
initially that when the PID control signal is 0%, the valve
is fully-closed, and when it is 100%, the valve is fully-open
(called fail-closed valve in industrial literature). Now, as the
level increases above the setpoint, the error decreases, and we
need to close the valve further to reduce the tank level, i.e.,
we need to decrease the PID control signal. Therefore, the
controller has to decrease the output when the error increases
and vice versa, i.e., it should be set as ”Direct Acting”.

Now suppose for safety reasons,

the valve is designed
such that when the PID controller output is 0%, the valve
is fully-open, and when the output is 100%, the valve is fully-
closed (called fail-open valve in industrial literature). With this
system conﬁguration, when the tank level increases, the error
decreases, and we need to close the valve further, i.e., increase
the PID control output. In such case, the PID controller should
increase its output when the error decreases and vice-versa,
(cid:4)
i.e., it should be set as ”Reverse Acting”.

In practice, changing direct and reverse acting setting is
done via swapping the setpoint and process variable terms in
the error deﬁnition. Notice that in some industrial literature,
the error is deﬁned as e = PV − SP, and therefore, the logic
of the above discussion will be reversed. However, the core
idea remains the same.

The PID controller setting enables us to infer the system
dynamics without knowing any information about the system.
If the PID is Direct Acting, we know that as the error increases
(equivalently the measurement decreases), the controller out-
put increases. Therefore, if the attacker wants to increase this
speciﬁc system measurement, the associated controller output
needs to be decreased, and vice versa. This gives the attacker
a systematic approach to drive the system states to the desired

Algorithm 1: Malware Payload Development for haz-
ard attacks. Direct Acting DA(.) function returns True
if the controller is direct acting, and similarly for
Reverse Acting RA(.)
: Fs, Fc
Input
Output: Fms, Fmc
P (x), u, IF-Body ← Search(Fs, ”IF ∨ IF*ELSE”)
Mal-Text1 ← Replace(IF-Body, ∗ = 1, ∗ = 0)
Mal-Text2 ← Replace(IF-Body, ∗ = 0, ∗ = 1)
Write(Fms, Mal-Text1, Mal-Text2)
for P ∈ P (x) do
for x ∈ x do

(Safety and Control code ﬁles)

(Malware payload code ﬁles)

Cx, ux ← Search(Fc, x)
if Condition(x, P (x) == ”>”) then

(Controller ID, output)

if DA(Cx) then

Append(Fmc, ”ux = min(ux)”)

else

end

else

Append(Fmc, ”ux = max(ux)”)

if RA(Cx) then

Append(Fmc, ”ux = max(ux)”)

else

end

Append(Fmc, ”ux = min(ux)”)

end

end

end
return Fms, Fmc

state condition expressed by P (x) in order to launch a hazard
attack. This knowledge could be obtained by searching through
the control program.

Example 4. Consider an expanded version of the tank-pump
system in Example 1, now including the inlet pipe and valve
and a feedback PID control loop that regulates the inlet valve
opening to control the tank level, as depicted in Figure 2 at the
tank inlet. Assuming the valve opens with increasing controller
signal, as the level decreases below the setpoint, the error
increases, and the valve needs to open to restore the level,
i.e., controller output needs to increase. Therefore, we need a
direct acting PID controller. Therefore, by inspecting the PID
conﬁguration, the attacker knows that in order to decrease the
level, the valve needs to be closed. Recalling that the safety
property in Example 1 includes x1 < 10%, then the attacker’s
objective is to reduce the level, hence fully closes the valve
(cid:4)
by setting the PID controller output to 0.

Algorithm 1 is a pseudo code that automates the malware
payload development for a hazard attack, where safety prop-
erties are encoded using IF...ELSE constructs in structured
text programming language, and assuming PID control algo-
rithms. It takes as input the safety and control program ﬁles,
searches for the invariants and control information, and returns
malware payload source code ﬁles. This payload could then
be encapsulated in a malware ﬁle that uses any of the common
techniques to exploit a controller vulnerability to execute the
payload, e.g., buffer overﬂow.

Fig. 3. Attack injection process

IV. ATTACK INJECTION

The attack injection process is illustrated in Figure 3. The
process starts with searching and locating the safety and
control programs, followed by the development of a malware
as presented in this paper. The implementation controller is
then located and compromised to gain access control, which
typically involves privilege escalation. Finally, the running
safety and control codes are disabled and replaced by the
malicious codes, optionally launching the attack in stealthy
mode by generating fake normal data to the human display
and logging nodes. In the following, we brieﬂy explain each
attack step.

A. Safety and Control Programs Acquisition

One of the key questions is how to acquire the safety and
control programs. After all, the technique presented in this pa-
per relies mainly on the availability of these programs. It may
be tempting to believe that a simple information security policy
could prevent the leakage of these critical software programs.
Although this is partially true, the reality is far less ideal.
There are multiple stakeholders for these software programs.
Examples are plant operators and engineers, automation ven-
dor engineers, consultancy ﬁrms that originally designed the
system, and the construction company. For operability reasons,
stakeholders must keep the latest version of the software. Even
worse, the turn around time for stakeholder engineers is so
high that an engineer working today for company X may move
to company Y few months later, with a copy of the program
on his laptop or ﬂash memory. Controlling the distribution
of the program beyond the system owner’s perimeter is not
that straightforward, and a trade-off has to be made between
security and operation convenience as well as fast incident
resolution.

One of the typical problems in embedded systems pro-
gramming is maintaining the source code of the deployed
program on the controller. To solve this issue, some of the
vendors introduced a source code program storage capability
on the controller itself, i.e., when the code is compiled and
downloaded to the controller, the source code is downloaded
as well and permanently stored on the controller. This gives
an additional opportunity for the attacker to obtain the source
code once the controller is compromised.

B. Malware Development

The development of malware for a given CPS is described

in Section III and summarized in Algorithm 1.

C. Locating the Controller

After obtaining the software, it is required to identify which
controller executes the code. Different software tools have
different methods to reference the controller in the source

Fig. 4. Malware injection into a CPS. Simultaneous injection into the control
and safety systems may lead to a hazard attack, while safety system injection
may result in a disruption attack.

code. Typically, the controller IP address or name tag will
be referenced in the source code.

D. Controller Compromise

This is a pre-requisite attack step before malware implan-
tation. The controller has to be compromised with escalated
privileges. The compromise of a CPS controller in an operat-
ing environment is an active area of research and depends
mainly on the CPS domain and cyber system architecture.
The interested reader may refer to an example in [4] for a
process control plant compromise or consult the literature for
the speciﬁc CPS domain of interest.

E. Malware Implantation

This process is illustrated in Figure 4. The existing code
has to be disabled, and the malware has to be loaded in
the controller memory. An alternative approach is to launch
the malware only when the desired system state is reached,
i.e., the dormant attack discussed in the paper. In both cases,
the attacker may attempt to generate fake data to the system
observing nodes to cover the attack, although this is more
challenging.

V. DISCUSSION

The paper proposes the idea that a CPS attacker does not
need to understand the system dynamics or behavior in order
to launch a destructive attack. This is because the required
information has already been extracted by system designers
in the control and safety
and encoded, partially or fully,
algorithms. The likelihood of an attack will depend on the
availability of the required data in the control code as well
as the difﬁculty of decoding such information into system
knowledge that could be used by the attacker. The method
presented in Algorithm 1 is for structured text programs that
encode the safety properties using IF...ELSE constructs.
Although quite common in industry,
the only
method to encode and program safety properties. A more
general approach is clearly needed to extract safety-critical
attacks from the diverse representations and programming
languages.

this is not

One important question is how we can utilize the fact that
attackers can reverse-engineer our system to better secure it?
There are several ways: (1) we can carry out a more informed
risk assessment process, focusing on the most relevant attacks,
and design the most suitable security mechanisms. This is
enabled by the fact that we know what attacks are harmful,
how much damage they could cause, and their likelihood that
is proportional to the difﬁculty of decoding the relevant infor-
mation, (2) better design of penetration testing by including
more relevant safety-critical attacks to measure the system
robustness and identify any system vulnerabilities, (3) the
information about safety-critical attacks could be used during
design time to develop more-resilient system architectures. As
an example, if we know that an equipment is critical for the
overall system safety, we can try to separate the hardware and
software components that monitor and control such equipment
from the rest of the control and safety systems, and put more
security protection mechanisms for the isolated hardware and
software. Even without an isolated hardware, we can still
isolate the speciﬁc software task scheduled by the controller
operating system by applying more strict access control rights.
Both techniques are obviously more cost-effective and less
human-demanding than trying to invest in securing the whole
system. Such techniques could also be considered examples
of the secure by design principle.

The proposed approach results in extreme attack conse-
quences, i.e., a service disruption or system hazard. These
consequences will be eventually noticed, either by system
operators or by automated response systems. The impact of
such an attack will vary signiﬁcantly depending on other non-
cyber protection and mitigation measures. On the other hand,
incipient attacks intend to gradually damage the system, and
therefore are mostly launched in a stealthy mode. Incipient
attacks are harder to design, because they require more knowl-
edge about how the system works. The extraction of such
information from available control and safety programs is
currently under investigation.

VI. CONCLUSION

We presented a systematic method to automatically develop
a CPS malware payload out of safety and control programs,
without knowledge about the CPS. The two types of attacks
presented are disruption attacks and hazard attacks. Disruption
attacks are easier to implement, hence have more likelihood,
but have less consequence severity. Hazard attacks are harder
to implement, therefore are less likely, but have higher conse-
quence severity. Several extensions could be identiﬁed out of
this work. A more general algorithm that automates malware
payload development for diverse representations is needed.
Experimental evaluation of the approach on real control code
and controllers of a CPS testbed is important to estimate the
difﬁculty in reverse engineering and malware injection. In
contrast with immediate effect attacks presented, automated
design of incipient attacks by reverse engineering is important
during both design and operation times. Finally,
learning
system dynamics from sensor and actuator communication by

network eavesdropping in order to craft an attack is important
when control programs cannot be obtained.

REFERENCES

[1] R. Rajkumar, I. Lee, L. Sha, and J. Stankovic, “Cyber-physical systems:
The next computing revolution,” in Proceedings - Design Automation
Conference, pp. 731–736, 2010.

[2] E. Lee and S. Seshia, Introduction to Embedded Systems: A Cyber-
Physical Systems Approach. London, England: The MIT Press, second
edi ed., 2017.

[3] R. Langner, “Stuxnet: Dissecting a Cyberwarfare Weapon,” IEEE Secu-

rity Privacy, vol. 9, no. 3, pp. 49–51, 2011.

[4] A. Tantawy, S. Abdelwahed, A. Erradi, and K. Shaban, “Model-Based
Risk Assessment for Cyber Physical Systems Security,” Computers &
Security, 5 2020.

[5] S. Adepu and A. Mathur, “Generalized Attacker and Attack Models
for Cyber Physical Systems,” in Proceedings - International Computer
Software and Applications Conference, vol. 1, pp. 283–292, IEEE
Computer Society, 8 2016.

[6] M. Rocchetto and N. O. Tippenhauer, “On attacker models and proﬁles
for cyber-physical systems,” in Lecture Notes in Computer Science
(including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture
Notes in Bioinformatics), vol. 9879 LNCS, pp. 427–449, Springer
Verlag, 2016.

[7] H. Orojloo and M. Abdollahi Azgomi, “Predicting the behavior of at-
tackers and the consequences of attacks against cyber-physical systems,”
Security and Communication Networks, vol. 9, pp. 6111–6136, 12 2016.
[8] C. Deloglos, C. Elks, and A. Tantawy, “An Attacker Modeling Frame-
work for the Assessment of Cyber-Physical Systems Security,” in
Lecture Notes in Computer Science (including subseries Lecture Notes in
Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), vol. 12234
LNCS, pp. 150–163, Springer, 9 2020.

[9] B. Chen, N. Pattanaik, A. Goulart, K. L. Butler-Purry, and D. Kundur,
“Implementing attacks for modbus/TCP protocol in a real-time cyber
physical system test bed,” in Proceedings - CQR 2015: 2015 IEEE In-
ternational Workshop Technical Committee on Communications Quality
and Reliability, 2015.

[10] S. Bhatia, N. Kush, C. Djamaludin, J. Akande, and E. Foo, “Practical
Modbus ﬂooding attack and detection,” in Conferences in Research and
Practice in Information Technology Series, vol. 149, pp. 57–65, ACSW-
AISC, 2014.

[11] S. East, J. Butts, M. Papa, and S. Shenoi, “A taxonomy of attacks on the
DNP3 protocol,” in IFIP Advances in Information and Communication
Technology, 2009.

[12] D. Jin, D. M. Nicol, and G. Yan, “An event buffer ﬂooding attack in
DNP3 controlled SCADA systems,” in Proceedings - Winter Simulation
Conference, pp. 2614–2626, 2011.

[13] I. Darwish, O. Igbe, and T. Saadawi, “Experimental and theoretical
modeling of DNP3 attacks in smart grids,” in 2015 36th IEEE Sarnoff
Symposium, pp. 155–160, Institute of Electrical and Electronics Engi-
neers Inc., 11 2015.

[14] P. Huitsing, R. Chandia, M. Papa, and S. Shenoi, “Attack taxonomies for
the Modbus protocols,” International Journal of Critical Infrastructure
Protection, 2008.

[15] M. Bashendy, S. Eltanbouly, A. Tantawy, and A. Erradi, “Design and
Implementation of Cyber-Physical Attacks on Modbus/TCP Protocol,” in
World Congress on Industrial Control Systems Security (WCICSS-2020),
2020.

[16] S. Kriaa, L. Pietre-Cambacedes, M. Bouissou, and Y. Halgand, “A
survey of approaches combining safety and security for industrial control
systems,” Reliability Engineering and System Safety, vol. 139, pp. 156–
178, 2015.

[17] X. Lyu, Y. Ding, and S. H. Yang, “Safety and security risk assessment
in cyber-physical systems,” IET Cyber-Physical Systems: Theory and
Applications, vol. 4, no. 3, pp. 221–232, 2019.

[18] D. Peled, “Temporal Logic: Mathematical Foundations and Compu-
tational Aspects, Volume 1,” The Computer Journal, vol. 38, no. 3,
pp. 260–261, 1995.

