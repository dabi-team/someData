Node-wise Hardware Trojan Detection
Based on Graph Learning

Kento Hasegawa∗§, Kazuki Yamashita†§, Seira Hidano∗, Kazuhide Fukushima∗
Kazuo Hashimoto†, and Nozomu Togawa†
∗KDDI Research, Inc., †Waseda University

2
2
0
2

r
a

M
6
1

]

R
C
.
s
c
[

2
v
3
1
2
2
0
.
2
1
1
2
:
v
i
X
r
a

Abstract—In the fourth industrial revolution, securing the pro-
tection of the supply chain has become an ever-growing concern.
One such cyber threat is a hardware Trojan (HT), a malicious
modiﬁcation to an IC. HTs are often identiﬁed in the hardware
manufacturing process, but should be removed earlier, when the
design is being speciﬁed. Machine learning-based HT detection in
gate-level netlists is an efﬁcient approach to identify HTs at the
early stage. However, feature-based modeling has limitations in
discovering an appropriate set of HT features. We thus propose
NHTD-GL in this paper, a novel node-wise HT detection method
based on graph learning (GL). Given the formal analysis of HT
features obtained from domain knowledge, NHTD-GL bridges the
gap between graph representation learning and feature-based HT
detection. The experimental results demonstrate that NHTD-GL
achieves 0.998 detection accuracy and outperforms state-of-the-
art node-wise HT detection methods. NHTD-GL extracts HT
features without heuristic feature engineering.

Index Terms—hardware Trojan, detection, gate-level netlist,

graph learning, node-wise

I. INTRODUCTION

The demand for high-performance, low-cost, and power-
saving ICs has been increasing, which makes supply chain
protection a serious concern in the reality of the fourth
industrial revolution. To meet demand, the IC design process
must be correspondingly secure. Primary vendors often use
third-party intellectual properties (3PIP) and outsource parts of
their products to third-party hardware design houses. Utilizing
3PIP and outsourcing to the third-party vendors lead to the
globalization and complexity of the supply chain, associated
with the risk of unintended third parties’ participation.

A hardware Trojan (HT) is emphasized as a threat in the
supply chain [1]. An HT consists of two core components:
trigger and payload and is often implemented as minute hard-
ware with its trigger deactivated to evade inspections. With
the trigger deactivated and thus leaving its payload disabled,
it acts as an HT-free IC. When the HT’s trigger is eventually
activated, it may leak conﬁdential information, tamper with
functionality, and suspend devices. HT detection at the design
phase has been widely researched [2]. In particular, gate-level
netlists (hereinafter referred to as netlists) are focused. A struc-
tural feature-based HT detection method was proposed to show
optimal performance [3], its merit being that it requires no
simulation. It also realizes the comprehensive and ﬁne analysis
of the target IC design. The methods [4], [5] realize node-
wise HT detection in netlists using machine learning (ML) and

§Equal contribution

show high detection performance. However, feature-based ML
methods have limitations in discovering an appropriate set of
features. Previous studies have adopted heuristic approaches to
ﬁnd structural features for HT detection. The selected features
are valid for known HTs, but skilled attackers can evade them.
It is a tremendous task to put upon structural feature-based
HT detection to continuously extract effective HT features
from the IC design when a new HT is found. Thus, simply
employing a structural feature-based approach is unfeasible for
real world circuits.

To overcome these limitations of structural feature-based
HT detection, a graph learning (GL) method is introduced.
A circuit can be represented as a graph, such as Boolean
networks [6]. Likewise, a netlist is represented as a graph
structure. Its node shows an element of a circuit and its edge,
a wire. Graph structure is becoming an active research area
in recent ML. It is expected that GL extracts generalized
features from netlists, an impossibility via manual feature en-
gineering. Considering that HTs are becoming more technical
and sophisticated, GL is a promising approach. GL-based HT
detection [7]–[9] distinguishes between normal circuits and
HTs effectively. The impracticalities of the existing methods
consist of problem settings, such as circuit-wise classiﬁcation
and trigger-focused detection, and the fact that the features GL
grasps is unknown.

In this paper, we propose NHTD-GL, a novel node-wise HT
detection method based on GL. The contributions of this paper
can be summarized as follows:

• The practical settings for HT detection are clariﬁed by
providing realistic scenarios in the hardware supply chain
based on preliminary experiences of HT detection in
netlists (Section IV).

• This paper bridges the gap between the known structural
features of HTs and the representation capability of
GNNs by clarifying what features a GNN model captures
for HT detection (Section V).

• NHTD-GL,

the hardware Trojan detection method in
netlist using GL is proposed (Section VI), the theoretical
background of which is described in Section V.

• NHTD-GL is evaluated through experiments. The exper-
imental results demonstrate that NHTD-GL outperforms
state-of-the-art HT detection methods. Additionally, by
comparing a GNN-based HT detection method with
simple node features, it is shown that the GNN model
effectively extracts the features characterizing HTs from

 
 
 
 
 
 
SVM and ANN to learn HT features. Since then, new features
and models have been actively investigated [16]–[18]. In the
past ﬁve years, HT detection methods using ML have achieved
90% or more accuracy. The weakness of the feature-based
method is that it requires continuous feature updating. It has
been reported that a powerful feature-based method, COTD
detection method [26], would not be adequate in a certain
application [19]. It suggests that HT-speciﬁc features should
be searched when a new HT is discovered. It is necessary to
automate feature discovery process to real world problem.
GL-based HT detection. Graph learning is now becoming
an active research area in ML [20]. Since a netlist can be
represented as a graph structure by translating an element
of a circuit into a node and a wire into an edge, GL-based
HT detection is a promising approach to break through the
impasse of endless feature engineering. Ref. [9] effectively
detects HTs from netlists using GL. GL-based HT detection no
longer requires feature engineering. Features are automatically
extracted in the learning process. As reported in [7], simple
GL-based HT detection methods bear a weakness that they
do not point out where HTs exist, but only identify whether
the design is compromised or not. For practical use, detected
HT should be presented with evidence so that the user can
trust the detection result. The only solution to this problem
would be explaining the GL model to those who are not well-
versed in HTs. In [9], only the trigger part is detected, leaving
its critical payload part unnoticed. Also, their proposed INF
contains several normal gates, which may hurt generalization
performance. Therefore, different embedding approaches, such
as optimizing node features based on the representation capa-
bility of the GL model, should be considered.

III. PRELIMINARIES
General model. The general model of NHTD-GL is the Graph
Neural Network (GNN). The concept of GNN is introduced
in [21] and is now employed in a variety of ﬁelds [20].

The MPNN model [22] has proposed a uniﬁed framework
for graph convolution operation, and many models can be
considered with this framework. Let G = (V, E) be a graph
with a set V of nodes and a set E of edges. A feature
vector xv is assigned for v ∈ V , which is referred to as an
initial feature vector that represents the property of a node v.
Also, a label yv ∈ R represents the class of the node v. In
general, GNN employs the neighborhood aggregation (a.k.a.
message passing) mechanism, in which node feature vectors
are exchanged between nodes, and updates (or combines) them
using an updating function. This mechanism is expressed as
follows [23]:

m(l)

v = AGGREGATE(l) (cid:16)(cid:110)
= COMBINE(l) (cid:16)
v , m(l)
h(l)
v

h(l)

h(l+1)
v

(cid:17)

,

u : ∀u ∈ N (v)

(cid:111)(cid:17)

(1)

(2)

where N (v) represents a set of nodes adjacent to a node
v, and AGGREGATE(l)(·) (resp. COMBINE(l)(·)) are the
the l-th
message function (resp.
layer. Such a layer is referred to as a GNN layer in this

the update function) at

Fig. 1: Typical IC design process. Malicious 3PIP cores as well
as third-party EDA tools might be involved in the process.

a given training dataset (Section VII).

II. RELATED WORKS

HT detection. HT detection methods were reviewed in [1].
The typical IC design process is illustrated in Fig. 1. In
the design phase, speciﬁcation is sequentially broken down
into behavior level, gate level, and layout level. 3PIP core
and third-party EDA tools have the opportunity to participate
in the design phase. Henceforth, malicious attackers may
take advantage. IC design is written in hardware description
language (HDL) and stored in an electrical design interchange
format (EDIF). Skillful attackers who know the language
and format can hide HTs to contaminate the IC design or
modify the design information. On the other hand, the attack
in the manufacturing phase is difﬁcult since the manufacturing
system is working in real time and being controlled by a
vendor-speciﬁc management process. Therefore, attacking the
design phase is a more realistic scenario than attacking the
manufacturing phase. This paper focuses on HTs inserted in
the design phase, particularly in netlists rather than the more
abstract level design such as register-transfer level (RTL), as
the IC design described in RTL is ultimately translated into a
netlist.
HT detection by logic testing. HTs are likely to be acti-
vated to evade from logic testing. Focusing on rare activation
conditions, early studies have proposed analytical detection
methods based on truth tables or simulations [10], [11]. The
weakness of hardware veriﬁcation by logic testing is that it
is time consuming to apply for large scale circuits, and that
there exists a technique to make HTs stealthy to logic testing
as proposed by DeTrust [12].
Feature-based HT detection. Another approach is the feature-
based method proposed in [3], which requires no simulation
and realizes comprehensive analysis with less time compared
to HT detection by logic testing. The method successfully
detects HTs with a high accuracy rate using the structural
features manually extracted from the Trust-HUB benchmark
netlists [13], [14], which suggests that structural features exist
which would speciﬁcally distinguish HTs from normal circuits.
Following the suggestion, several ML-based approaches have
been proposed [4], [5]. According to [5], one of the ﬁrst ML-
based HT detection methods was proposed in [15], which
achieved high recall but insufﬁcient accuracy by employing

SpecificationRTL Design(Behavior Level)Netlist(Gate Level)Physical Design(Layout Level)Fabrication /PackagingMarket3PIP core3PIP coreThird-partyEDA toolDesign PhaseManufacturing PhaseOff-the-shorefabsv

v = xv, and m(l)
v

paper. h(l)
is a hidden feature vector of v initialized by
v
the initial feature vector as h(0)
denotes
the message exchanged between nodes. In (1), the message
function AGGREGATE(l)(·) aggregates the feature vectors of
the nodes adjacent to v and generates the message vector m(l)
v .
Then, in (2), the update function COMBINE(l)(·), which is a
learnable function, combines the node feature vector h(l)
v and
the message vector m(l+1)
. Finally, the node feature vector
h(l)
for each node at the l-th GNN layer is updated. The
v
functions AGGREGATE(l)(·) and COMBINE(l)(·) are different
to model, which results in the capability of
from model
representation of a node.

Let zv = h(L)

be a ﬁnal output for the node feature vector
of v, where L is the number of GNN layers. In task-speciﬁc
processing, the ML model for graph can be divided into a
graph encoder part and prediction part. Let fenc and fpred be a
graph encoder model and prediction model, respectively. The
graph encoder model can be expressed as fenc(xv) = zv that
implies (1) and (2). The prediction model can be expressed
as fpred(zv) = yv. For example, the optimization problem for
binary classiﬁcation is expressed as follows:

v

min L (yv, fpred(zv)) ,

(3)

where L is a loss function, and fpred is a classiﬁcation model,
such as a fully-connected neural network.

IV. MOTIVATIONS

Threat model. As illustrated in Fig. 1,
there are many
opportunities for attackers to be involved in the hardware
supply chain, such as providing malicious 3PIP cores or
invading as untrusted off-the-shore fabs. In particular, there are
more opportunities for attackers to insert HTs in the design
phase as addressed by our threat model. Speciﬁcally, attackers
may insert HTs into the 3PIP cores and provide them to the
primary vendor. Attackers may also invade the design house
and directly insert HTs with malicious intent. This scenario
housed in the recent hardware supply chain involves many
employees, partners, 3PIPs, and off-the-shore design houses.
Henceforth, the supply chain becomes unsecure, providing
loopholes for malicious attackers to gain entry.

Our motivations. There are three motivations in this paper,
and this section clariﬁes them from the following perspectives:
• Motivation 1 describes why we target netlists and the

node-wise HT detection (Section IV-A).

• Motivation 2 describes why we employ graph learning

for HT detection (Section IV-B).

• Motivation 3 describes why we introduce domain knowl-

edge for graph learning (Section IV-C).

A. Motivation 1: Gate-Level Netlists and Node-Wise HT De-
tection

HT detection in netlists. As illustrated in Fig. 1, the design
phase is roughly broken down into four levels: speciﬁcation,
behavior level, gate level, and layout level. Any IC design must
pass through the gate level. Even if some 3PIPs are provided

as the behavior-level description, they are synthesized to the
gate-level description. This can also be done of the layout
level. However, it includes location information, which is not
needed for behavior analysis. Netlists at the gate level are
common and useful in the IC design phase. In fact, an HT
detection service commercially available is detailed in [24],
which targets netlists. Therefore, this paper focuses on HT
detection in netlists.
Node-wise detection. There are two approaches in HT de-
tection in netlists: circuit-wise and node-wise detection. The
circuit-wise detection identiﬁes whether the IC design includes
an HT or not. Alternatively, node-wise detection identiﬁes
which node is a part of an HT. If a circuit-wise HT detection
system ﬁnds that an HT may exist in the IC design, the user
may doubt such an alert. The user must determine whether
or not the product should be re-designed based on the result
of the detection system. Also, the cause of the HT insertion
should be carefully analyzed if an HT actually exists. Node-
wise detection solves the problem. The results show which
node could be a part of an HT. If a node-wise detection method
achieves an acceptable rate in detection accuracy, its result
must support further analysis and decision-making. Therefore,
node-wise detection is more helpful for practical use.

B. Motivation 2: Graph Learning

As mentioned in Section III, GNNs are expected to capture
the generalized features of graph-structured data. Since many
engineers are now joining the hardware design community
because of the spread of open-source projects such as RISC-
V [25], HT detection that does not require special knowledge
of HTs is needed. Additionally, HT structure can be easily
transformed. For example, the transduction method [26] trans-
forms a logic design into a different structure while keeping
the original functionality. If we can represent a circuit with
a generalized form, the circuits with the same functionality
would be embedded into similar latent spaces; effect of the
automatic feature extraction is signiﬁcant.

To study the features of HTs, we can collect many HTs
by using automatic HT generation tools [27], [28] proposed
very recently. However, even if many types of HTs could be
collected, it is too difﬁcult to extract a set of comprehensive
HT features. GL-based HT detection is expected to extract the
features included in the training dataset automatically.

C. Motivation 3: Domain Knowledge

To make the HT detection system reliable, evidence and
theoretical background information should be made clear.
Although feature engineering may not be required in GL,
knowledge of HT features is vital. Existing studies have
demonstrated that their proposed features are useful for HT
detection, and work effectively for a certain set of HTs.
Therefore, knowledge of HTs still comes of aid for GL-based
HT detection.

Introducing the domain knowledge is different from au-
tomatic feature extraction. The automatic feature extraction,
which can be performed by GL, is a process to choose a set

of efﬁcient features from a set of initial features of nodes
in a given training dataset. If sufﬁcient initial features are
the
not provided to the initial feature vector of a graph,
performance of GL may saturate at an insufﬁcient level. The
role of domain knowledge is the selection of a set of sufﬁcient
initial features that represent the nodes in a netlist. By selecting
common features that can be observed in a wide variety of
HTs as initial features based on domain knowledge, feature
extraction by GL is expected to work effectively.

Optimizing initial feature vectors based on domain knowl-
edge is helpful. As described in Section III, the hidden feature
vector for a node v initialized as h(0)
v = xv. Even though
GL can automatically learn the representation of nodes in
a graph, the node features given as an initial feature vector
signiﬁcantly affect the performance of the subsequent task.
In [29], it has been demonstrated through experiments that
GNNs work well if there is a strong correlation between node
feature vectors and node labels. Thus, the node features are
introduced to sufﬁciently represent known HT features and
HT-related circuit features. This paper aims to ﬁnd efﬁcient
features representing the characteristics of a node for GL-
based HT detection.

V. HT FEATURES

A. HT Features at Gate-Level Netlists

In this subsection, we refer to several structural feature-
based HT detection methods and categorize what they learn
from the perspective of graph-structured data. According to
[2], there are several approaches for HT detection in netlists
using a variety of features. Hereafter, we refer to the three
references [30], [31], and [32] to introduce the representative
features based on the studies above.

In [30], 36 structural features are employed for HT detection
in total. Some of the features are introduced at one of the
earliest studies [15], [16] that are inspired by [3]. They extract
feature values from each net in a netlist from the viewpoint of
fan-ins, neighbor circuit elements, and the minimum distance
to the speciﬁc circuit elements. Ref. [30] further introduces
the pyramidal structure-based features called fan in uxdy, and
they improve detection performance. Table I shows the 36
features presented in [30]. The ‘category’ column is introduced
later. The features mainly focus on the structural features of a
netlist, that is, the topological features of a graph.

In [31], 15 features are employed, which is shown in
Table II. Different from [30], the features mainly focus on
the functional behavior such as static probability and signal
rate (No. 6–15 in Table II). The functional behavior-based
features reﬂect the functionality of a netlist, such as rare or
frequent transition of a signal. Although the structural features
can capture the structure of a set of nodes, it does not explicitly
capture the behavior of the circuit. Functional behavior-based
features solve the problem, and they capture the behavior of
the circuit explicitly.

In [32], six testability metrics-based features are employed.
Table III shows the six features presented in [32]. These
features are known as SCOAP values and utilized inherently

TABLE I: HT features employed in [30].

No.

1–2

3

4–5

6–7

8
9
10

11

12–36

Feature

No. of fan-ins up to 4 and 5-level
away from the input side
No. of ﬂip-ﬂops up to 4-level away
from the input side
No. of ﬂip-ﬂops up to 3 and 4-level
away from the output side
No. of loops up to 4 and 5-level on
the input side
Min. level to any primary input
Min. level to any primary output
Min. level to any ﬂip-ﬂops from the
output side
Min. level to any multiplexer from
the output side
Pyramidal structure-based feature
within 4 levels

Category

1 Degree

(cid:32)
2 Neighboring node

(cid:32)
2 Neighboring node

(cid:32)
3 Relative position

(cid:32)
3 Relative position
3 Relative position
(cid:32)
3 Relative position
(cid:32)
(cid:32)
3 Relative position

(cid:32)
4 Surrounding structure

(cid:32)

TABLE II: HT features employed in [31].

No.

1–2

3
4–5
6
7
8
9
10
11–15

Feature

No. of immediate fan-in and fan-
out
Cell type driving the net
Min. distances from PI and PO
Static probability
Signal rate
Toggle rate
Min. toggle rate of the fan-outs
Entropy of the driver function
Lowest, highest, average, std. and
dev. of controllability

Category

1 Degrees

(cid:32)
2 Neighboring node
3 Relative position
(cid:32)
5 Functional behavior
(cid:32)
5 Functional behavior
(cid:32)
5 Functional behavior
(cid:32)
5 Functional behavior
(cid:32)
5 Functional behavior
(cid:32)
5 Functional behavior
(cid:32)
(cid:32)

for evaluating the testability of a circuit [33]. The recent
study [17] has ﬁrst introduced them to HT detection, and they
are often employed for HT detection. Since an HT is hard
to observe and easy to control from outside the circuit, the
SCOAP values are reasonable for HT detection.

Toward GL. Based on the observation of several existing
HT detection methods, we analyze the features and categorize
them from the viewpoint of GL.

•

•

•

•

•

1 Degree shows the degree of a node in a graph, which
is related to the number of edges connected to the node.
(cid:32)
2 Neighboring node shows the types of nodes neigh-
boring a target node.
(cid:32)
3 Relative position shows the relative position to the
speciﬁc sets of nodes.
(cid:32)
4 Surrounding structure shows the surrounding struc-
ture from a target node.
(cid:32)
5 Functional behavior shows the functional behavior
of a node i.e. characterizes the behavior of a set of
(cid:32)
nodes. An example is static probability. Typically, the
trigger circuit of an HT rarely activates the trigger signal.
The node outputting trigger signal is rarely activated.
Such behavior cannot be observed by simply analyzing
structural features.

The following section analyzes the representation capability
of graph encoding models from the perspective of the cate-
gories

1 –
(cid:32)

5 .
(cid:32)

TABLE III: HT features employed in [32].

TABLE IV: GL-based methods for netlists.

No.

1–2
3
4–5

6

Feature

Category

Controllability (CC0 and CC1)
Observability (CO)
Sequential controllability (SC0 and
SC1)
Sequential observability (SO)

5 Functional behavior
5 Functional behavior
(cid:32)
5 Functional behavior
(cid:32)
(cid:32)
5 Functional behavior

(cid:32)

B. Representing HT Features with GL

Here we bridge the gap between the known HT features
and GL. As described in the previous section, the HT features
are classiﬁed into ﬁve categories. We demonstrate how GL
captures these features in a formal manner.

1 Degree: Fan-ins and fan-outs of a node are useful informa-
tion for HT detection. For example, a trigger circuit composed
(cid:32)
of a combinatorial circuit often has many fan-ins at two or
three levels from the trigger signal wire to implement rare
conditions. The node degrees can be directly assigned to the
initial feature vector to clearly make the graph encoder model
identify node degrees.

2 Neighboring node: The information about the types of
nodes neighboring a target node is helpful for HT detection.
(cid:32)
To utilize the information by GL, a node type is assigned to
the initial feature vector.

The node types are the most fundamental features for
representing the characteristics of nodes in a netlist, as used
in [8] and [9]. In this paper, the node types are used as the
features of a baseline, the details of which is presented later.

2

1

(cid:32)

and/or
identify the subgraphs. According to (2),

3 Relative position: When the surrounding structures of
two subgraphs are identical, the graph encoder model that
(cid:32)
can-
considers only the feature categories
not
the ﬁnal
(cid:32)
node feature vector of v is expressed as zv = h(L) =
COMBINE(L) (cid:16)
= h(L−1)
u
and m(L−1)
for two nodes v and u, the node
v
feature vectors of the two nodes are identical, i.e., zv = zu.
In a netlist, such a case can appears in a ring oscillator that is
composed of multiple inverter gates forming a loop structure or
a multi-bit counter that is composed of several 1-bit counters.
These circuits can be used as a payload circuit or a sequential
trigger circuit of an HT.

h(L−1)
v
= m(L−1)
u

. When h(L−1)

, m(L−1)
v

(cid:17)

v

To overcome the limitation of GNN, a position-aware GNN
model has been proposed in [34]. In the method, a random
subset of k nodes are chosen as anchor-sets and their node
feature vectors are included in the target node feature vector.
Inspired by [34], we state the following proposition:

Proposition 1. Let Ganchor be a graph where an initial feature
vector xv that includes information on anchor-sets is assigned
to each node v. Let φ : Rd → Rd be a GNN layer, and let
Fenc denote a set of GNN models that consist of more than
one layer φ and are injective. There exists a graph encoder
model fenc ∈ Fenc that takes Ganchor as an input and identiﬁes
the difference of two nodes in Ganchor in terms of position in
a graph.

Graph
Undirected graph
Directed graph

Methods
GNN-RE [8], GATE-Net [9]
[35], GNN4TJ [7]

Fig. 2: Example of graph-related features for a netlist.

The proof of Proposition 1 is shown in Appendix A.
According to [34], choosing a set of anchor nodes is a difﬁcult
problem for an inductive learning task. From the viewpoint of
netlists, primary inputs and outputs are reasonable candidates
for anchor-sets making them useful for HT detection. There-
fore, a graph encoder model can best identify the positional
feature of a node by employing minimum distances to any
primary inputs and outputs as initial feature values.

4 Surrounding structure: How a netlist is represented as
a graph structure is one of the most problematic issues in
(cid:32)
GL. There are two forms of representing a netlist: undirected
graph and directed graph. The edge between two nodes in
a graph is directional in a directed graph, whereas it is not
in an undirected graph. The signal wires in a netlist are
inherently directional, and thus directed graph seems to be
suitable to represent a netlist. However, a directed graph cannot
sufﬁciently represent a netlist for graph encoder models. With
a directed graph, the aggregation function gathers only one
direction of each edge, and thus the nodes connected to
the opposite side are ignored. Table IV shows the graph
models employed in recent processing methods using GNNs.
According to [8], which aims to represent netlists for multiple
downstream tasks, representing a netlist as an undirected graph
is more efﬁcient than as a directed graph. It should be noted
that GNN4TJ [7], which employs directed graphs, mainly
focuses on HTs written in RTL. Therefore, a directed graph
may be inefﬁcient for netlists.

The representation capability of the directed and undirected
graphs is broken down, and a new model for representing a
netlist is proposed. Four points of the graph-related features
for a netlist are as follows:

•

•

•

a Input side: The structure of the input side from target
node v.
(cid:35)
b Output side: The structure of the output side from
target node v.
(cid:35)
c Edge direction: The direction of an edge. Namely,
the direction of a wire in a netlist.
(cid:35)

Targetd  Neighbor levela  Input sidec  Edge directionb OutputsideTABLE V: Graph structure and its representation capability in
a graph encoder model.

Graph type

Directed
Undirected
Undirected
(with directional edge attribute)

a
Input
(cid:35)
side
(cid:88)
(cid:88)

(cid:88)

b
Output
(cid:35)
side

(cid:88)

(cid:88)

c
Edge
(cid:35)
direction
(cid:88)

(cid:88)

d
Neighbor
(cid:35)
level

(cid:88)

(cid:88)

•

d Neighbor level: The structure of the neighbor level
from target node v.
(cid:35)

Fig. 2 illustrates an example of the graph-related features using
a circuit. In this ﬁgure, we focus on the ‘target’ node colored in
a Input side corresponds to the area shaded in light blue.
red.
The nodes in this area are connected to the input side of the
(cid:35)
b Output side corresponds to the area shaded in
target gate.
light green. The nodes in this area are connected to the output
(cid:35)
c Edge direction shows whether
side of the target gate.
the edge direction is preserved or not in the modeled graph.
(cid:35)
d Neighbor level corresponds to the area shaded in orange.
The neighbor level area can be reached by traversing some
(cid:35)
hops toward the output side and then some hops backward the
input side or vice-versa.

Next, we summarize the representation capability of graph
models. Table V shows the graph structures and their rep-
resentation capability in a graph encoder model. A directed
graph can represent the input side and the edge direction of
a netlist when a graph encoder model aggregates the nodes
of the input side. However, the nodes of the output side are
not aggregated in this situation. Similarly, the neighbor levels
are not considered unless there is a loop structure. On the
other hand, an undirected graph can represent both the input
and output sides. Due to the both-side aggregation, it can
also represent the nodes in neighbor levels. However, it lacks
directional information.

Then, we propose the directional edge attribute to an undi-
rected graph, called edge-attributed undirected graph (EAUG).

Deﬁnition 1 (Edge-attributed undirected graph). An edge-
attributed undirected graph (EAUG) is a graph GEAUG =
(V, E, X, D) that represents a netlist with a set of initial node
vectors X = {xv : v ∈ V } and a set of edge attribute vectors
D = {du→v : u, v ∈ V }.

The EAUG overcomes the shortcoming of the undirected
graph. Let eu→v denote an edge from a node u to another
node v. du→v ∈ Rd is an edge attribute vector assigned
to eu→v. In a typical undirected graph, the edges eu→v and
ev→u are not distinguished. Furthermore, when feature vectors
are assigned to the edges of an undirected graph, du→v and
dv→u are usually the same. In the EAUG, we distinguish two
edges eu→v and ev→u by assigning different edge attributes as
du→v (cid:54)= dv→u Then, we can state the following proposition.

Proposition 2. There exists a graph encoder model fenc ∈
Fenc that takes an edge-attributed undirected graph GEAUG as
d .
an input and represents all the graph-related features

a –

(cid:35)

(cid:35)

Fig. 3: Example of a node behavior and a functional behavior.

The proof of Proposition 2 is shown in Appendix B. For
example, we assign a vector (1, 0) to a forward direction edge
and (0, 1) to a backward direction edge of a netlist. Then, we
can construct an EAUG for a given netlist.
5 Functional behavior: In this section, we observe the global
behavior of a circuit. At ﬁrst, we consider the behavior of
(cid:32)
each node. We call a function assigned to each node node
behavior, which is identiﬁed by only the node. However, we
can only observe the local behavior of each node by simply
considering the node behavior. To observe the global behavior
of a circuit, we need a new feature that takes into account the
node behavior of the neighboring nodes. Then, we introduce
the functional behavior, which characterizes the behavior of
a set of nodes. The functional behavior of a target node v
refers to a feature computed using an arbitrary function that
takes node behaviors of v and its neighboring nodes Nk(v),
where Nk(·) is a set of neighboring nodes within k hops from
a node. In a netlist, there are several metrics that characterize
the behavior of a set of nodes by assigning a function to each
node, such as static probability and switching probability. We
consider such metrics by the functional behavior.

Speciﬁcally, static probability is an example of the func-
tional behavior, which shows the probability that a signal
holds a logic value of 1 during a period. Fig. 3 illustrates the
example of an HT trigger circuit that consists of seven two-
input AND gates. First, we refer to the truth table to consider
the local behavior of a two-input AND gate. Here, we regard
the probability of outputting 1 as the node behavior. In this
case, the node behavior of a two-input AND gate is 0.25, as
shown in Fig. 3(a). Then, we consider the HT trigger circuit
depicted in Fig. 3(b). Starting from the node behavior values,
we can calculate static probabilities of the target node. We
regard the static probability as the functional behavior, which
characterizes the functionality of a set of neighboring nodes
within two hops from the target node. As exempliﬁed above,
a functional behavior also characterizes the behavior of HTs
well, especially HT triggers as shown in Fig. 3(b).

To formulate the functional behavior, let type(v) be a node
type and nb(v) be the node behavior of node v. The functional
behavior, fb(v) of a node v can be calculated as follows:

fb(v) ← Γtype(v) (nb(v), {nb(u) : ∀u ∈ Nk(v)}) ,

(4)

where Γtype(v)

is an arbitrary function parameterized by

(a) Truthtable of a two-input AND gate.IN1IN2OUT000010100111Τ14=0.25Node behavior0.250.250.250.250.06250.06250.0039…(=0.254)(b) HT trigger circuit (Subgraph of 𝐺).Node behaviorFunctional behaviorTarget nodetype(v). The following proposition states that a graph encoder
model that represents such a functional behavior exists.

Proposition 3. Let G(cid:48) be a graph constructed as a GEAUG
where the node type and a feature value related to a functional
behavior is assigned to each node v. There exists a graph
encoder model fenc ∈ Fenc that takes G(cid:48) as an input and
identiﬁes the functional behavior k-hop away from each node
v for a given k.

The proof of Proposition 3 is given in Appendix C.
We consider the feature extraction model that extracts the
graph structure,
initial feature vectors, and edge attribute
vectors for a given netlist. Let Λ be a feature extraction model
that takes a netlist, which is denoted by ϑ, and outputs a tuple
(V, E, X, D) of a set of nodes, set of edges, set of node feature
vectors, and set of edge attribute vectors. In some cases, X or
D can be an empty set. Let fenc;Λ(ϑ)(v) be a graph encoder
model with respect to Λ(ϑ) = (V, E, X, D) for obtaining
the node feature vector of the node v during the calculation.
fenc(v) implicitly refers to V , E, X, and D of a netlist ϑ.
If ∃u, v ∈ V, u (cid:54)= v, fenc;Λ(ϑ)(u) (cid:54)= fenc;Λ(ϑ)(v), it means
that the combination of the models Λ and fenc can distinguish
the two nodes u and v. Pϑ(Λ, fenc) denotes the representation
capability for the nodes in a given netlist ϑ when using Λ as
a feature extraction model and fenc as a graph encoder model.
Here, the representation capability for a netlist is deﬁned.

Deﬁnition 2 (Representation capability for a netlist). Let
Λ1 and Λ2 be feature extraction models, and let f (1)
enc and
f (2)
enc denote graph encoder models. The representation capa-
enc ) is greater than Pϑ(Λ2, f (2)
bility Pϑ(Λ1, f (1)
enc ) if and only
if ∀u, v ∈ V, u (cid:54)= v, f (2)
enc;Λ2(ϑ)(v) =⇒
f (1)
enc;Λ1(ϑ)(u) (cid:54)= f (1)
enc;Λ1(ϑ)(v), and the relationship of the two
representation capabilities is denoted as follows:

enc;Λ2(ϑ)(u) (cid:54)= f (2)

P(ϑ)(Λ1, f (1)

enc ) (cid:23) P(ϑ)(Λ2, f (2)

enc ).

(5)

is

In particular, Pϑ(Λ1, f (1)
enc )

than
strictly greater
enc ) (cid:23) P(ϑ)(Λ2, f (2)
enc )
enc;Λ1(ϑ)(v) but

enc;Λ1(ϑ)(u) (cid:54)= f (1)

enc ) if and only if P(ϑ)(Λ1, f (1)

Pϑ(Λ2, f (2)
and ∃u, v ∈ V, u (cid:54)= v, f (1)
enc;Λ2(ϑ)(u) = f (2)
f (2)
enc;Λ2(ϑ)(v).
For comparison of the representation capability of graph
encoder models, we introduce two feature extraction models
for a netlist: baseline and netlist feature extraction models.

Deﬁnition 3 (Baseline feature extraction model). A baseline
feature extraction model Λbaseline(ϑ) = (V, E, X, D) extracts
a set of node initial feature vectors X that indicate the node
types of each node v ∈ V and does not extracts any edge
attribute, i.e., D = ∅.

The model that extracts the features belong to the feature

category

2 is a baseline feature model.
(cid:32)

Deﬁnition 4 (Netlist feature extraction model). A netlist
feature extraction model Λnetlist(ϑ) = (V, E, X, D) extracts
a set X of node initial feature vectors that indicate the node

degrees, node types, the features of relative position to anchor-
sets, and the features of functional behavior of each node
v ∈ V and a set D of edge attribute vectors that indicate
the edge direction of nodes u, v ∈ V .

The model that extracts the node features and edge attributes
is a netlist feature

5

belong to the feature categories
model.

1 –
(cid:32)

(cid:32)

Now, we can state the following theorem:

Theorem 1. The representation capability of a netlist feature
extraction model is strictly greater than the representation
capability of a baseline extraction model.

and f (2)

Proof: Let f (1)
enc

enc be graph encoder mod-
els that are assumed to be injective. Since the extracted
features from the netlist feature extraction model contain
the features from the baseline feature extraction model,
∀u, v ∈ V, u (cid:54)= v, fg2;Λbaseline(ϑ)(u) (cid:54)= fg2;Λbaseline(ϑ)(v) =⇒
fg1;Λnetlist(ϑ)(u) (cid:54)= fg1;Λnetlist(ϑ)(v). Thus, P(ϑ)(Λnetlist, f (1)
enc ) (cid:23)
P(ϑ)(Λbaseline, f (2)
enc ). Furthermore, by Propositions 1, 2, and 3,
there exists a set of nodes that the netlist feature extraction
model can distinguish while the baseline feature extraction
enc ) (cid:31) P(ϑ)(Λbaseline, f (2)
model cannot. Thus, P(ϑ)(Λnetlist, f (1)
enc ).
Therefore, Theorem 1 holds.

By Theorem 1, such a feature extraction model represents

the feature categories

1 –
(cid:32)

5 .
(cid:32)

It should be noted that the assumption that graph encoder
models are injective is a realistic modeling approach for theo-
retical analysis. For simplicity, we consider a neural network
model as a matrix product. In general, the weights of the neural
network model are initialized with Gaussian random values.
Since the weight matrices are full-rank in most cases, the
matrix product becomes injective. However, a precise analysis
is remained to be a future work.

C. Summary of HT Features

This section presents the ﬁve feature categories for GL as
a way to
5 and bridges the gap between HT features and
GL. As a result, this section clariﬁes what HT features a graph
(cid:32)
encoder model captures by theoretical analysis.

1 –
(cid:32)

In the existing HT detection methods such as [30] and
[32], we must discover effective features for HT detection.
To consider the topology around each node in a netlist, we
should carefully analyze the relationship between a node to
another node. Therefore, the search space for feature engi-
neering reaches O(|V | × |V |). Alternatively,
the proposed
method automatically extracts the structural features by graph
extraction models. It is enough to deﬁne initial feature vectors
that represent the characteristics of nodes well. To explore
such features, we analyze all nodes and extract representative
features. Therefore, the search space for feature engineering
becomes O(|V |). We will demonstrate through experiments in
Section VII that a graph encoder model automatically extracts
HT features.

TABLE VI: Initial feature vector used in the proposed model a.

Feature

Category

No.

1
2
3–42
43
44
45
46

In-degree
Out-degree
Node types
Min. distance to any primary input
Min. distance to any primary output
Static probability (0) of logic gates
Static probability (1) of logic gates

a We construct an EAUG, which is categorized as

1
1
(cid:32)
2
(cid:32)
3
(cid:32)
3
(cid:32)
5
(cid:32)
5
(cid:32)
4 .
(cid:32)
(cid:32)

VI. PROPOSED METHOD

A. Overview

Fig. 4 illustrates an overview of the proposed method, called
NHTD-GL. As shown in Fig. 4, an HDL design is converted
to a graph that represents the netlist, constructing an EAUG,
GEAUG. Training and/or testing of datasets are based on the
EAUGs.

In the preprocessing phase (see Section VI-B in detail), we
prepare netlists to be trained with the GNN model. First, we
convert the netlists to EAUG where each element of circuit is
assigned to a node, and each wire is assigned to an edge (
4 ).
Then, we assign the initial feature value that covers the feature
(cid:32)
categories
5 to each node in the graph. At the same
time, we assign the edge attribute that shows the direction of
(cid:32)
the edge. Finally, we construct the dataset including multiple
EAUGs.

3 and
(cid:32)

1 –
(cid:32)

In the training phase (see Section VI-C and Section VI-D in
detail), we start from the Trojan sampling on the given EAUGs
to balance the normal and Trojan nodes in each mini-batch.
For the training of HTs, we have to deal with a problem that
HTs are quite tiny. Because of the stealth of HTs, they are
often constructed with a tiny scale. Therefore, the numbers
of genuine nodes and Trojan nodes are imbalanced in the
training dataset. To accurately perform node-wise classiﬁca-
tion for HT detection, we should adequately deal with the
imbalanced dataset. To address the problem, we propose the
Trojan sampling method to train the imbalanced HT dataset
effectively. After that, we train the mini-batches to classify
each node in a graph as Trojan or normal.

In the testing phase, we just classify each node in an EAUG

as either normal or Trojan using the trained model.

B. Initial Feature Vector of a Node

Based on the discussion in Section V, we design the initial

feature vector to be assigned to each node.

Table VI shows the initial feature vector assigned to each
node v in the proposed model. Features 1 and 2, which are
categorized as
1 , correspond to the in-degree and out-degree
of a node v, respectively. To effectively train the features,
(cid:32)
the feature values are standardized. Features 3–42, which
2 , show the types of each node v, e.g.,
are categorize as
a two-input AND gate or a ﬂip-ﬂop. Features 43 and 44
(cid:32)
show the minimum distance to any primary input and output,
respectively. Primary inputs and outputs are characteristic

nodes for HT detection. We use them as anchor-sets, and thus
the features are categorized as
3 . The features 45 and 46 show
the probability that a logic gate outputs 0 and 1, respectively.
(cid:32)
We give the standardized feature values to the initial vector.
They are helpful for calculating the functional behavior of
a circuit and can be categorized as
the
5 described
features cover the feature categories
in Section V.
(cid:32)

5 . In summary,
3 and
1 –
(cid:32)
(cid:32)
(cid:32)

C. Trojan Sampling

Since an HT is tiny compared to a normal circuit, the
numbers of normal nodes and Trojan nodes are signiﬁcantly
imbalanced. We should adequately address the issue to per-
form HT detection stably. In the conventional ML models (not
GL models), over-sampling (or under-sampling) approaches
can be adopted to enhance the minority classes. SMOTE [36]
is a well-known method for over-sampling, which syntheti-
cally generates minority class samples and enhances minority
classes. However, one cannot directly adopt the approach to
graph-structured data. A conventional over-sampling method
cannot consider the adjacent matrix. Although the graph-
version SMOTE method has been proposed very recently [37],
it is difﬁcult to construct an accurate decoder model. There-
fore, in NHTD-GL, we ﬁrst split normal nodes into several
subgraphs. Then, we construct mini-batches by combining a
Trojan subgraph with different normal subgraphs.

Algorithm 1 shows the Trojan sampling algorithm. Let
Vt and Vn be a set of Trojan nodes and a set of normal
nodes, respectively. Given the number of mini-batches m,
the set of normal nodes into m subsets so
we ﬁrst split
that Vn = {V (1)
}. Note that |V (i)
| ≤ (cid:100)|Vn|/m(cid:101),
where | · | shows the cardinality of a given set. Then, we
construct m mini-batches B = {B(1), · · · , B(m)}, where
B(i) = {V (i)
, Vt}, i ∈ [m]. We train mini-batches B in each
iteration.

, · · · , V (m)

n

n

n

n

D. GNN Model

As described in Section V, EAUG corresponds the feature
category
4 . According to (7), we aggregate the edge attribute
as well as the feature vectors of nodes adjacent to the node v.
(cid:32)
We concatenate edge attributes and node feature vectors of u
for each v in our implementation.

In order to classify each node v as normal or Trojan, we
assign a one-hot vector yv ∈ R2, where (1, 0) shows normal
and (0, 1) shows Trojan. Then, we can set the optimization
problem using the loss function L as follows:

min

(cid:88)

∀v∈V

L (yv, f (xv)) ,

(6)

where f (xv) = fpred(fenc(xv)). We repeat the training for
each graph G in a training dataset.

Algorithm 2 describes the training algorithm. Let GEAUG
be a set of EAUGs that corresponds to a set of netlists to
be trained. Y is a set of labels yv assigned to each node
v. We draw mini-batches B from each EAUG, GEAUG and
calculate the loss of the model f like (6) using the nodes in

Fig. 4: Overview of NHTD-GL.

Algorithm 1 Trojan Sampling Algorithm

Input: Sets of Trojan and normal nodes Vt ∪ Vn = V , the number of

mini-batches m

Output: Set of mini-batches B

1: B ← ∅
2: Split Vn into m subsets V (i)

n

(cid:100)|Vn|/m(cid:101).

randomly, where i ∈ [m] and |V (i)

n

| ≤

3: for i = 1 to m do
4: B(i) ← {V (i)
5:
6: end for
7: return B

B ← B ∪ {B(i)}

n

, Vt}

Algorithm 2 NHTD-GL: Training Algorithm

Input: Set of EAUGs GEAUG, set of labels Y , the number of mini-

batches m

Output: Trained model f
1: Initialize the model f .
2: repeat
3:

for all GEAUG = (V, E, X, D) ∈ GEAUG do
B ← TrojanSamplingAlgorithm(V, m)
for all B ∈ B do

// Repeat one-epoch process

Calculate the loss of the model f with xv, dN (v)→v, and
yv for all v ∈ B.
Update the weight of the model f .

end for

end for

8:
9:
10: until Training converges.
11: return f

a mini-batch. Then, we update the weight of the model f so
as to minimize the loss between the model’s outputs and the
labels yv, v ∈ B. We repeat the one-epoch process (ll. 3–9
in Algorithm 2) several times until the training converges. In
other words, we end the training process when the number of
processed epochs reaches the limit, or the loss of the model

4:
5:
6:

7:

f no longer decreases.

VII. EVALUATION

The evaluation in this section aims to answer the following

research questions through the experiments:

• RQ1: Does the domain knowledge on HTs enhance the

detection performance? (Section VII-B-a)

• RQ2: Does the proposed method enhance the detection
performance for node-wise HT detection in netlists?
(Section VII-B-b)

• RQ3: Does the GL automatically extract effective fea-

tures? (Section VII-C)

Each research question corresponds to the motivations de-
scribed in Section IV: RQ1 corresponds to Motivation 3,
RQ2 corresponds to Motivation 1, and RQ3 corresponds to
Motivation 2.

A. Setup

Datasets. For the dataset, we use 24 netlists from Trust-
HUB [13], [14] (the detail is presented in Appendix D). In
the Trust-HUB benchmarks, HTs with various structures are
embedded into several
types of netlists, and the insertion
points are indicated by comments in the source code. The total
number of nodes in the dataset is 621140, of which the number
of Trojan nodes is 1262. This means that Trojan nodes are
only 0.2% of the total data. The Trojan Sampling Algorithm
described in Section VI-C is applied to better learn imbalanced
training data.

Furthermore, randomly generated samples are employed in

Section VII-C. The details are presented in the section.
Models. We train and identify the dataset with the graph
structure described in the Datasets section. We use GAT [38],
MPNN [22], and GIN [23] as the GNN models for the eval-
uation. The graph encoder models for EAUGs are described
in Appendix E. We set the following parameters for all the
models. The maximum number of epochs is 1000, and early

Input: 𝐺EAUGHDL CodeGate-LevelCircuit StructurePreprocessing (Section 6.2)ParseConvertNormal nodesTrojan nodesTrojan Sampling (Section 6.3)1stmini-batch𝑚-thmini-batch1stGNNLayer𝑛-thGNNLayerTarget nodesAggregated nodesCalculate loss /Update weights(for each mini-batch)GNN Model Training (Section 6.4)Trojan / Normal(For each node)Trojan / Normal(For each node)Trained modelTestTestingTrainTrainingAlgorithm 1Algorithm 2TABLE VII: Best parameters for each model.

Method

Proposed

Baseline

Model
GAT
MPNN
GIN
GAT
MPNN
GIN

#batches
20
15
20
5
15
20

#layers
3
2
2
3
3
2

#units
16
32
16
32
32
32

stopping [39] is applied, which ﬁnishes the training process
when the loss does not decrease for 50 epochs. The learning
rate is 0.1, the optimization algorithm is Adam, the activation
function is an exponential linear unit (ELU) function, and
binary cross-entropy is used for the loss function.

for

Since the best parameters

the number of mini-
batches (#batches), the number of GNN layers (#layers), and
the number of dimensions of the feature vectors after the
graph convolution (#units) depend on the GNN model, we
search for them using grid search. We explain the details in
Appendix F. Table VII shows the best parameters in each
model for the proposed method and the baseline method
described in Section VII-B.
Evaluation metrics. To evaluate the performance, we perform
a leave-one-out cross-validation. For each of the 24 netlists
described in the Datasets section, we use one as a test sample
and the remaining 23 as training samples. We perform this
validation on all the netlists and evaluate the average of the
24 classiﬁcation results. The evaluation metrics are recall,
precision, F1-score, and accuracy, in which a Trojan net is
regarded as a positive sample.

B. Performance Evaluation for Trust-HUB Benchmarks Com-
paring with Baseline and State-of-the-Art Methods

1) Comparison with the baseline method (RQ1): In this
experiment, the performance with and without domain knowl-
edge in the features is evaluated. As mentioned in Sec-
tion IV-C, feature selection based on domain knowledge is
effective in GNNs. To conﬁrm this assumption, we compare
the features of the proposed method as mentioned in Table VI
with the features consisting only of node types. In general,
the node type is the most primitive feature when representing
the information in a graph format [9]. Therefore, we pick
up features 3–42, which are categorized as
2 as shown in
Table VI. We deﬁne them as baseline node features that do
(cid:32)
not contain domain knowledge and GL based on them is
considered to be the baseline method.

To compare the results fairly, we search for the best param-
eters of the baseline method by grid search as in the proposed
method. The details are shown in Appendix F.
Detection results. Table VIII shows the detection results of
this experiment (the detailed results are shown in Appendix G).
The boldfaced font indicates the highest rate in each method.
As shown in Table VIII,
the proposed method performed
as well or better than the baseline method in all evaluation
metrics. In particular, the GAT model outperforms the other

TABLE VIII: Detection results of Trust-HUB for the proposed
method and the baseline method.

Method

Proposed

Baseline

Model
GAT
MPNN
GIN
GAT
MPNN
GIN

Recall
0.890
0.865
0.585
0.880
0.879
0.237

Precision
0.978
0.933
0.616
0.976
0.900
0.392

F1-score
0.921
0.887
0.498
0.915
0.871
0.240

Accuracy
0.998
0.998
0.988
0.998
0.998
0.986

models in all metrics. Therefore, GAT is the best GNN model
for HT detection in Trust-HUB.

1 ,
(cid:32)

This means that we can say, “YES” to RQ1 corresponding
to Motivation 3. By eliminating the feature categories
3 ,
5 , which are based on the domain knowledge of
4 , and
(cid:32)
the HTs, the recall, precision and F1-score are decreased.
(cid:32)
(cid:32)
The accuracy is the same as the baseline method because
the Trojan node is only 0.2% of the whole training data, and
the accuracy score is greatly affected by true negatives. For
example, if the classiﬁer determines all nodes as normal nets
(i.e., negative), the accuracy would be 99.8%. Therefore, there
is no difference in accuracy among the methods, but it is
clear that the proposed method can identify the Trojan nodes
better than the baseline method because the recall, precision,
and F1-score are higher. This conﬁrms that the features based
on knowledge of HT features are essential for node-wise HT
detection.

2) Comparison with state-of-the-art methods (RQ2): In this
experiment, we compare the proposed method with the two
state-of-the-art methods [30], [32] mentioned in Section V-A.
The method of [30] extracts 36 features that represent the
structure of HT well, and identiﬁes HT wires by the random
forest (RF). Since the number of false positives is very small,
this method is less likely to misidentify normal circuits. The
method of [32] is based on testability measures, which are
effective features for HT detection, and employs ADASYN to
learn imbalanced training data better. They validated it with
four supervised algorithms, with the highest metrics when
using the bagged trees (BT). Both methods [30], [32] have
been reported to perform well on the Trust-HUB dataset based
on the effective feature engineering.
Detection results. Table IX shows the comparison results
with existing methods. We adopt
the GAT model, which
shows the highest classiﬁcation performance as shown in
Table VIII, as the proposed method for the comparison. As
shown in Table IX, the proposed method outperforms the two
methods [30], [32] in all evaluation metrics.

This means that we can say, “YES” to RQ2 corresponding
to Motivation 1. As far as we know, there is no other gate-
level and node-wise HT detection method that achieves 0.890
recall and 0.978 precision. Since we can accurately identify
the insertion point of the HT, we can obtain the evidence for
further analysis. Moreover, because of the high accuracy, we
can carefully analyze or reﬁne the suspicious circuit using
other veriﬁcation techniques based on the classiﬁcation results
the proposed method
of the proposed method. Therefore,

TABLE IX: Comparison with existing methods.

TABLE X: Detection results of unknown HTs.

Method
Proposed
[30]
[32]

Model
GAT
RF
BT

Recall
0.890
0.636
0.825

Precision
0.978
0.957
0.866

F1-score
0.921
0.667
0.827

Accuracy
0.998
0.994
0.983

solves the problem in practical use.

C. Performance Evaluation for HTs with Unknown Features
(RQ3)

In this experiment, we evaluate the proposed method for
HTs with unknown features using randomly generated sam-
ples. As mentioned in Section IV-B, GNN models are ex-
pected to automatically extract
the features that represent
HTs well. Although conventional ML-based methods require
feature engineering to effectively perform HT detection, GNNs
will overcome it. To conﬁrm this assumption, we randomly
generate HT-infested samples and attempt to detect HTs from
the generated samples without feature engineering of their
HTs. For comparison, we adopt the baseline method and the
method of [30] in this experiment.
Random HT-infested circuit generation.
Inspired by
methodology in [27], HT-infested circuits for the evaluation
were randomly generated. How to randomly generate HT-
infested circuits is described in Appendix H.

We randomly generate 20 training samples, including 79155
normal nodes and 2227 Trojan node, and 100 test samples,
including 396103 normal nodes and 9961 Trojan nodes. In
the generated HTs, most Trojan nodes are less than 5% of the
total number of nodes. The generated HTs are relatively small
compared to the normal circuits. For evaluation, we train the
20 training samples and construct a trained model. Then, we
classify 100 test samples and calculate the average scores of
all of the samples in terms of recall, precision, F1-score, and
accuracy.

1 –
(cid:32)

In this experiment, we use the same model as in the previous
section. Speciﬁcally, we use the same parameters as in the
previous section for the evaluation of the proposed method
and the baseline method. We implement the method in [30]
for comparison with a state-of-the-art HT detection method,
which considers the feature categories
4 and is designed
based on the features that appear in the Trust-HUB benchmark
(cid:32)
netlists.
Detection results. Table X shows the detection results of this
experiment. The boldfaced font indicates the highest rate in
each method. As shown in Table X, the GNN-based methods
(the proposed method and the baseline method) outperform
the method in [30] in any evaluation metrics. Although the
precision of [30] is higher than the proposed method in
Table IX, it was not as high in this experiment. Instead, all the
metrics are quite low compared to the results of GNN models.
This is because the features in [30] are designed for only the
Trust-HUB benchmark netlists, and thus this method fails to
capture the features of randomly generated HTs (i.e., the HTs
with unknown features). On the other hand, the GNN-based

Method

Proposed

Baseline

[30]

Model
GAT
MPNN
GIN
GAT
MPNN
GIN
RF

Recall
0.773
0.868
0.783
0.756
0.854
0.863
0.681

Precision
0.843
0.993
0.847
0.889
0.936
0.965
0.749

F1-score
0.788
0.905
0.800
0.781
0.881
0.897
0.706

Accuracy
0.995
0.997
0.995
0.995
0.996
0.997
0.996

methods successfully capture the node features of HTs and
achieve higher detection performance. From the results, we
can state that GNN-based HT detection successfully extracts
HT features without tedious feature engineering, even when
the datasets are not involved in a speciﬁc benchmark suite.
This means that we can say, “YES” to RQ3 corresponding to
Motivation 2.

VIII. CONCLUSION

In this paper, a novel HT detection method in netlists using
GL called NHTD-GL was proposed. NHTD-GL applies node-
wise detection in netlists, GL, and domain knowledge of HTs
for practical use. Thus, this paper theoretically supports the
relationship between GL and HT detection and clariﬁes what
HT features GL captures. Based on the theoretical analysis
described in Section V, it is established that NHTD-GL ef-
fectively captures the HT features. The experimental results
demonstrate that NHTD-GL successfully outperforms the ex-
isting HT detection methods and extracts HT features without
tedious feature engineering.

REFERENCES

[1] K. Xiao, D. Forte, Y. Jin, R. Karri, S. Bhunia, and M. Tehranipoor,
“Hardware trojans: Lessons learned after one decade of research,” ACM
Trans. Des. Autom. Electron. Syst., vol. 22, 2016.

[2] Y. Yang, J. Ye, Y. Cao, J. Zhang, X. Li, H. Li, and Y. Hu, “Survey:
Hardware trojan detection for netlist,” in 2020 IEEE 29th Asian Test
Symposium (ATS), 2020, pp. 1–6.

[3] M. Oya, Y. Shi, M. Yanagisawa, and N. Togawa, “A score-based classiﬁ-
cation method for identifying hardware-trojans at gate-level netlists,” in
2015 Design, Automation Test in Europe Conference Exhibition (DATE),
2015, pp. 465–470.

[4] Z. Huang, Q. Wang, Y. Chen, and X. Jiang, “A survey on machine learn-
ing against hardware trojan attacks: Recent advances and challenges,”
IEEE Access, vol. 8, pp. 10 796–10 826, 2020.

[5] S. Kundu, X. Meng, and K. Basu, “Application of machine learning in
hardware trojan detection,” in 2021 22nd International Symposium on
Quality Electronic Design (ISQED), 2021, pp. 414–419.

[6] W. Kunz and D. Stoffel, Reasoning in Boolean Networks: Logic Syn-
thesis and Veriﬁcation Using Testing Techniques. Springer, 1997.
[7] R. Yasaei, S.-Y. Yu, and M. A. A. Faruque, “Gnn4tj: Graph neural
networks for hardware trojan detection at register transfer level,” in 2021
Design, Automation Test in Europe Conference Exhibition (DATE), 2021,
pp. 1504–1509.

[8] L. Alrahis, A. Sengupta, J. Knechtel, S. Patnaik, H. Saleh, B. Moham-
mad, M. Al-Qutayri, and O. Sinanoglu, “Gnn-re: Graph neural networks
for reverse engineering of gate-level netlists,” IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, pp. 1–14,
2021.

[9] N. Muralidhar, A.

Zubair, N. Weidler, R. Gerdes,

and
N. Ramakrishnan, “Contrastive graph convolutional networks for
hardware trojan detection in third party ip cores,” 2021. [Online].
Available: https://people.cs.vt.edu/∼ramakris/papers/Hardware Trojan
Trigger Detection HOST2021.pdf

[10] A. Waksman, M. Suozzo, and S. Sethumadhavan, “Fanci: Identiﬁca-
tion of stealthy malicious logic using boolean functional analysis,” in
Proceedings of the 2013 ACM SIGSAC Conference on Computer &
Communications Security, 2013, p. 697–708.

[11] J. Zhang, F. Yuan, L. Wei, Y. Liu, and Q. Xu, “Veritrust: Veriﬁcation
for hardware trust,” IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems, vol. 34, pp. 1148–1161, 2015.
[12] J. Zhang, F. Yuan, and Q. Xu, “Detrust: Defeating hardware trust
veriﬁcation with stealthy implicitly-triggered hardware trojans,” in Pro-
the 2014 ACM SIGSAC Conference on Computer and
ceedings of
Communications Security, 2014, p. 153–166.

[13] B. Shakya, T. He, H. Salmani, D. Forte, S. Bhunia, and M. Tehranipoor,
“Benchmarking of hardware trojans and maliciously affected circuits,”
Journal of Hardware and Systems Security, vol. 1, no. 1, pp. 85–102,
2017.

[14] H. Salmani, M. Tehranipoor, and R. Karri, “On design vulnerability
analysis and trust benchmarks development,” in 2013 IEEE 31st Inter-
national Conference on Computer Design (ICCD), 2013, pp. 471–474.
[15] K. Hasegawa, M. Oya, M. Yanagisawa, and N. Togawa, “Hardware
trojans classiﬁcation for gate-level netlists based on machine learning,”
in Proc. IEEE International Symposium on On-Line Testing and Robust
System Design (IOLTS), 2016, pp. 203–206.

[16] K. Hasegawa, M. Yanagisawa, and N. Togawa, “Trojan-feature extrac-
tion at gate-level netlists and its application to hardware-trojan detection
using random forest classiﬁer,” in Proc. IEEE International Symposium
on Circuits and Systems (ISCAS), 2017, pp. 1–4.

[17] H. Salmani, “Cotd: Reference-free hardware trojan detection and re-
covery based on controllability and observability in gate-level netlist,”
IEEE Transactions on Information Forensics and Security, vol. 12, pp.
338–350, 2017.

[18] S. Li, Y. Zhang, X. Chen, M. Ge, Z. Mao, and J. Yao, “A xgboost based
hybrid detection scheme for gate-level hardware trojan,” in Proc. IEEE
Joint International Information Technology and Artiﬁcial Intelligence
Conference (ITAIC), vol. 9, 2020, pp. 41–47.

[19] A. Ito, R. Ueno, and N. Homma, “A formal approach to identifying
hardware trojans in cryptographic hardware,” in ISMVL, 2021, pp. 154–
159.

[20] Z. Zhang, P. Cui, and W. Zhu, “Deep learning on graphs: A survey,”
IEEE Transactions on Knowledge and Data Engineering, pp. 1–1, 2020.
[21] M. Gori, G. Monfardini, and F. Scarselli, “A new model for learning
in graph domains,” in Proc. IEEE International Joint Conference on
Neural Networks, vol. 2, 2005, pp. 729–734.

[22] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
“Neural message passing for quantum chemistry,” in Proceedings of the
34th International Conference on Machine Learning - Volume 70, 2017,
p. 1263–1272.

[23] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph
neural networks?” in International Conference on Learning Represen-
tations, 2019.

[24] Toshiba Information Systems (Japan) Corp., “Hardware detection tool
‘HTﬁnder’,” (in Japanese). [Online]. Available: https://www.tjsys.co.jp/
lsi/htﬁnder/index j.htm

[25] RISC-V International, “RISC-V International.” [Online]. Available:

https://riscv.org/

[26] S. Muroga, Y. Kambayashi, H. Lai, and J. Culliney, “The transduction
method-design of logic networks based on permissible functions,” IEEE
Transactions on Computers, vol. 38, pp. 1404–1424, 1989.

[27] J. Cruz, Y. Huang, P. Mishra, and S. Bhunia, “An automated conﬁgurable
trojan insertion framework for dynamic trust benchmarks,” in Proc.
Design, Automation & Test in Europe Conference & Exhibition (DATE),
2018, pp. 1598–1603.

[28] S. Yu, W. Liu, and M. O’Neill, “An improved automatic hardware trojan
generation platform,” in IEEE Computer Society Annual Symposium on
VLSI, ISVLSI, 2019, pp. 302–307.

[29] C. T. Duong, T. D. Hoang, H. T. H. Dang, Q. V. H. Nguyen, and
K. Aberer, “On node features for graph neural networks,” in Graph
Representation Learning in NeurIPS 2019 Workshop, 2019.

[30] T. Kurihara and N. Togawa, “Hardware-trojan classiﬁcation based on
the structure of trigger circuits utilizing random forests,” in IEEE
International Symposium on On-Line Testing and Robust System Design,
IOLTS, 2021, pp. 1–4.

[31] T. Hoque, J. Cruz, P. Chakraborty, and S. Bhunia, “Hardware ip
trust validation: Learn (the untrustworthy), and verify,” in 2018 IEEE
International Test Conference (ITC), 2018, pp. 1–10.

[32] C. H. Kok, C. Y. Ooi, M. Moghbel, N. Ismail, H. S. Choo, and
M. Inoue, “Classiﬁcation of trojan nets based on scoap values using
supervised learning,” in 2019 IEEE International Symposium on Circuits
and Systems (ISCAS), 2019, pp. 1–5.
and E. Thigpen,

[33] L. Goldstein

“Scoap: Sandia
controllabil-
in 17th Design Automation

ity/observability analysis program,”
Conference, 1980, pp. 190–196.

[34] J. You, R. Ying, and J. Leskovec, “Position-aware graph neural net-
works,” in Proc. International Conference on Machine Learning, ICML,
2019, pp. 7134–7143.

[35] A. Balakrishnan, Alex, D. rescu, M. Jenihhin, T. Lange, and M. Glo-
rieux, “Gate-level graph representation learning: A step towards the im-
proved stuck-at faults analysis,” in 2021 22nd International Symposium
on Quality Electronic Design (ISQED), 2021, pp. 24–30.

[36] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “Smote:
Synthetic minority over-sampling technique,” Journal of Artiﬁcial Intel-
ligence Research, vol. 16, pp. 321–357, 2002.

[37] T. Zhao, X. Zhang, and S. Wang, “Graphsmote: Imbalanced node
classiﬁcation on graphs with graph neural networks,” in Proc. ACM
International Conference on Web Search and Data Mining, 2021, pp.
833–841.

[38] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and
Y. Bengio, “Graph attention networks,” in International Conference on
Learning Representations, ICLR, 2018.

[39] Y. Yao, L. Rosasco, and A. Caponnetto, “On early stopping in gradient
descent learning,” Constructive Approximation, vol. 26, no. 2, pp. 289–
315, 2007.

[40] M. Fey and J. E. Lenssen, “Fast graph representation learning with
PyTorch Geometric,” in ICLR Workshop on Representation Learning
on Graphs and Manifolds, 2019.

[41] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. S. Pande, and
J. Leskovec, “Strategies for pre-training graph neural networks,” in
International Conference on Learning Representations, ICLR, 2020.

APPENDIX A
PROOF OF PROPOSITION 1

Proof: Let ξ(v, A) be a function that calculates the
minimum distance from a node v to any node in an anchor-
set A. The node feature vector of a node v at the l-th layer
is represented as {h(l)
v , ξ(v, A)}. Then, according to (1), the
feature vectors of v and u at the ﬁrst layer are expressed as
follows:

m(1)
m(1)

v = AGGREGATE(1) ({{xv(cid:48), ξ(v(cid:48), A)} : ∀v(cid:48) ∈ N (v)}) ,
u = AGGREGATE(1) ({{xu(cid:48), ξ(u(cid:48), A)} : ∀u(cid:48) ∈ N (u)}) .

If AGGREGATE(l), ∀l ∈ [L]
(cid:54)=
ξ(u, A) =⇒ m(l)
u . By recursively processing (1) and
v
(2), zv (cid:54)= zu. Thus, the nodes u and v are distinguishable, and
Proposition 1 holds.

injective, ξ(v, A)

(cid:54)= m(l)

is

APPENDIX B
PROOF OF PROPOSITION 2

Proof: We assign the edge attributes as du→v (cid:54)= dv→u :
∀v ∈ V, ∀u ∈ N (v), u (cid:54)= v. Then,
c can be represented
obviously. Since an EAUG inherits the properties of an undi-
(cid:35)
d can also be represented. Here we modify (1)
rected graph,
to involve edge attributions.
(cid:35)

m(l+1)
v

= AGGREGATE(l) (cid:16)(cid:110)

h(l)

u , du→v : ∀u ∈ N (v)

(cid:111)(cid:17)

(7)
If the aggregate function AGGREGATE(l)(·) distinguishes
the edge attributes, the aggregated message m(l+1)
implic-
itly involves both input and output side structures. Since

v

AGGREGATE(l)(·) is injective in the well-trained graph en-
coder model, the input and output sides can be distinguished.
b .
As a result, the downstream task can distinguish
Therefore, the proposition holds.
(cid:35)

a and
(cid:35)

APPENDIX C
PROOF OF PROPOSITION 3

Proof: Let zθ be a learnable function parameterized by
θ. Here, we express the function to update the hidden feature
vector of a node v at the l-th layer of a graph encoder model
fenc as follows:

h(l+1)
v

= zθ(l)

(cid:16)

v , ˜h(l)
h(l)

v

(cid:17)

,

(8)

where θ(l) is a parameter of the function z to be an injective
function, and ˜h(l)
is an aggregated feature vector of adjacent
v
nodes that is calculated as follows:
v = AGGREGATE(l) (cid:16)(cid:110)
˜h(l)

u : ∀u ∈ N (v)

h(l)

(cid:111)(cid:17)

(9)

Now we assign a node type type(v) and a feature value of a
functional behavior nb(v) to a node v. Then, (8) can express
(4).

If let zθ(l) (·, ·) be COMBINE(l) and ˜h(l)

the
equations (8) and (9) correspond to the equations (2) and (1),
respectively. Thus, there exists a graph encoder model with the
function zθ(j) that identiﬁes the functional behavior within k
hops from each node v for a given k.

be m(l)
v ,

v

In terms of the direction of calculating functional behavior,
it is shown that an EAUG identiﬁes edge directions by Propo-
sition 2. This means that a graph encoder model exists that
identiﬁes functional behaviors with distinguishing the input
and output directions.

Hence, the proposition holds.

APPENDIX D
BENCHMARKS
Table XI shows the benchmark netlists used in the experi-

ments in Section VII-B.

APPENDIX E
GNN MODELS FOR EAUGS
We show the models used in this paper as well as the layer

deﬁnition in the PyTorch Geometric library [40].
GAT : The original GAT convolution layer is deﬁned as
GATConv in PyTorch Geometric. Based on the GATConv
layer, we extend the methods to adopt to an EAUG. The GAT
model used in this paper is expressed as follows:

(cid:88)

m(l)

v =

h(l+1)
v

= α(l)

u∈N (v)
v,vW (l)h(l)

u,vW (l) · [h(l)
α(l)
v + m(l)
v ,

v (cid:107) du→v]

TABLE XI: Benchmarks.

Netlist

RS232-T1000
RS232-T1100
RS232-T1200
RS232-T1300
RS232-T1400
RS232-T1500
RS232-T1600
s15850-T100
s35932-T100
s35932-T200
s35932-T300
s38417-T100
s38417-T200
s38417-T300
s38584-T100
s38584-T200
s38584-T300
EthernetMAC10GE-T700
EthernetMAC10GE-T710
EthernetMAC10GE-T720
EthernetMAC10GE-T730
B19-T100
B19-T200
wb conmax-T100
Total

Normal
nodes
289
293
296
290
290
291
290
2397
5967
5962
5975
5656
5656
5688
7064
7064
7064
102453
102453
102453
102453
63170
63170
23194
619878

Trojan
nodes
13
11
10
9
12
13
13
27
15
15
26
12
15
15
9
83
731
13
13
13
13
83
83
15
1262

where H(·, ·) is a learnable function, and ψ is an activation
function. We use LeakyReLU as the activation function ψ.
MPNN : This convolution layer is deﬁned as NNConv in
PyTorch Geometric. The MPNN model used in this paper is
expressed as follows:

(cid:88)

m(l)

v =

u∈N (v)
= W (l)h(l)

h(l)

u · MLPθ (du→v)

(12)

h(l+1)
v

v + m(l)
v ,
where MLPθ is an MLP parameterized by θ and W (l) is a
weight matrix.
GIN : To adopt to the edge attributes in an EAUG, we use the
model leveraged in [41]. This convolution layer is deﬁned as
GINEConv in PyTorch Geometric. The GIN model used in
this paper is expressed as follows:
(cid:16)
(cid:88)

(cid:17)

m(l)

v =

u∈N (v)
(cid:16)(cid:16)

ReLU
1 + (cid:15)(l)(cid:17)

u + W (l)du→v
h(l)

· h(l)

v + m(l)
v

(cid:17)

,

h(l+1)
v

= MLPθ

(13)

where W (l)
parameterized by θ.

is a weight matrix and MLPθ is an MLP

(10)

APPENDIX F
GNN PARAMETERS

where [· (cid:107) ·] concatenates given vectors and W (l) is the param-
eter of the shared linear transformation. α(l)
u,v is an attention
to an edge from a node u to another node v at the l-th layer:

α(l)

u,v =

(cid:16)

(cid:16)

ψ

exp

(cid:80)

w∈N (v) exp

H
(cid:16)

(cid:16)

W (l)h(l)
(cid:16)
(cid:16)

ψ

H

W (l)h(l)

v , W (l)h(l)
w

v , W (l)h(l)
u

(cid:17)(cid:17)(cid:17)

(cid:17)(cid:17)(cid:17) ,

(11)

For the experiment in Section VII-B, we conduct a grid
search to ﬁnd the best parameters for each model, GAT,
MPNN, and GIN. We vary the number of mini-batches, the
number of GNN layers, and the dimensions of the feature vec-
tors updated by the aggregation. We summarize the parameters
in Table XII. We compare the recall, precision, and F1-score of
the classiﬁcation results, and select the model that shows the
highest value in two or more evaluation metrics as the best

TABLE XII: Parameters of the experiment.

#batches
1, 5, 10, 15, 20, 25, 30

#layers
2, 3

#units
16, 32

TABLE XIII: Detection results for the GAT model of the
proposed method with the best parameter.

Netlist
RS232-T1000
RS232-T1100
RS232-T1200
RS232-T1300
RS232-T1400
RS232-T1500
RS232-T1600
s15850-T100
s35932-T100
s35932-T200
s35932-T300
s38417-T100
s38417-T200
s38417-T300
s38584-T100
s38584-T200
s38584-T300
EthernetMAC10GE-T700
EthernetMAC10GE-T710
EthernetMAC10GE-T720
EthernetMAC10GE-T730
B19-T100
B19-T200
wb conmax-T100
Average

TN
289
292
295
290
289
291
290
2397
5967
5962
5972
5656
5656
5687
7063
7064
7064
102453
102452
102453
102453
63170
63161
23194
-

FP
0
1
1
0
1
0
0
0
0
0
3
0
0
1
0
0
0
0
0
0
0
0
9
0
-

FN
0
0
0
0
0
0
4
0
3
4
5
4
0
1
6
1
177
0
0
1
0
0
0
4
-

TP
13
11
10
9
12
13
9
27
12
11
21
8
15
14
3
82
554
13
13
12
13
83
83
11
-

Recall
1.000
1.000
1.000
1.000
1.000
1.000
0.692
1.000
0.800
0.733
0.808
0.667
1.000
0.933
0.333
0.988
0.758
1.000
1.000
0.923
1.000
1.000
1.000
0.733
0.890

Precision
1.000
0.917
0.909
1.000
0.923
1.000
1.000
1.000
1.000
1.000
0.875
1.000
1.000
0.933
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
0.902
1.000
0.978

F1-score
1.000
0.957
0.952
1.000
0.960
1.000
0.818
1.000
0.889
0.846
0.840
0.800
1.000
0.933
0.500
0.994
0.862
1.000
1.000
0.960
1.000
1.000
0.949
0.846
0.921

Accuracy
1.000
0.997
0.997
1.000
0.997
1.000
0.987
1.000
0.999
0.999
0.999
0.999
1.000
1.000
0.999
1.000
0.977
1.000
1.000
1.000
1.000
1.000
1.000
1.000
0.998

parameter. When several models show similar performance,
we focus on F1-score to make a decision because we aim to
maximize both recall and precision as much as possible.

Figs. 5 and 6 show all the evaluation results for searching for
the best parameters of the proposed method and the baseline
method, respectively. In Figs. 5 and 6, we show the recall,
precision, and F1-score results. The x-axis shows the number
of batches, and the y-axis shows the score of an evaluation
metric in each plot. The large-sized plots show the models
that we have selected as the best ones.

APPENDIX G
DETAILS OF THE DETECTION RESULTS

Tables XIII–XVIII show the detection results for each
benchmark netlist using the GNN models with the best
parameters obtained in Appendix F. Tables XIII–XV (resp.
Tables XVI–XVIII) are the results of the proposed method
(resp.
the baseline method). We regard the class of Tro-
jan nodes as ‘positive’ and obtain the true positive (TP),
true negative (TN),
false positive (FP), and false nega-
tive (FN) based on the detection results from the GNN
models. The evaluation metrics are recall, precision, F1-
score, and accuracy, which are expressed as
follows:
Recall = TP/(TP + FN), Precision = TP/(TP + FP),
F1-score = (2 · Recall · Precision)/(Recall + Precision),
and Accuracy = (TP + TN)/(TP + FN + FP + TN). As
shown in the tables, there are few FPs and FNs compared
to the number of all the nodes in a netlist. In particular, as
shown in Table XIII, there are almost no FPs and few FNs for
the GAT model, which is the best model among the three GNN
models of the proposed method for the Trust-HUB benchmark
netlists.

TABLE XIV: Detection results for the MPNN model of the
proposed method with the best parameter.

Netlist
RS232-T1000
RS232-T1100
RS232-T1200
RS232-T1300
RS232-T1400
RS232-T1500
RS232-T1600
s15850-T100
s35932-T100
s35932-T200
s35932-T300
s38417-T100
s38417-T200
s38417-T300
s38584-T100
s38584-T200
s38584-T300
EthernetMAC10GE-T700
EthernetMAC10GE-T710
EthernetMAC10GE-T720
EthernetMAC10GE-T730
B19-T100
B19-T200
wb conmax-T100
Average

TN
289
293
296
288
288
290
290
2387
5967
5961
5973
5656
5656
5687
7063
7060
7044
102453
102452
102453
102453
63170
63170
23180
-

FP
0
0
0
2
2
1
0
10
0
1
2
0
0
1
0
4
20
0
0
0
0
0
0
14
-

FN
0
0
1
0
0
0
3
2
3
2
9
6
1
4
3
0
21
0
1
1
6
0
0
5
-

TP
13
11
9
9
12
13
10
25
12
13
17
6
14
11
6
83
710
13
12
12
7
83
83
10
-

Recall
1.000
1.000
0.900
1.000
1.000
1.000
0.769
0.926
0.800
0.867
0.654
0.500
0.933
0.733
0.667
1.000
0.971
1.000
0.923
0.923
0.538
1.000
1.000
0.667
0.865

Precision
1.000
1.000
1.000
0.818
0.857
0.929
1.000
0.714
1.000
0.929
0.895
1.000
1.000
0.917
1.000
0.954
0.973
1.000
1.000
1.000
1.000
1.000
1.000
0.417
0.933

F1-score
1.000
1.000
0.947
0.900
0.923
0.963
0.870
0.806
0.889
0.897
0.756
0.667
0.966
0.815
0.800
0.976
0.972
1.000
0.960
0.960
0.700
1.000
1.000
0.513
0.887

Accuracy
1.000
1.000
0.997
0.993
0.993
0.997
0.990
0.995
0.999
0.999
0.998
0.999
1.000
0.999
1.000
0.999
0.995
1.000
1.000
1.000
1.000
1.000
1.000
0.999
0.998

TABLE XV: Detection results for the GIN model of the
proposed method with the best parameter.

Netlist
RS232-T1000
RS232-T1100
RS232-T1200
RS232-T1300
RS232-T1400
RS232-T1500
RS232-T1600
s15850-T100
s35932-T100
s35932-T200
s35932-T300
s38417-T100
s38417-T200
s38417-T300
s38584-T100
s38584-T200
s38584-T300
EthernetMAC10GE-T700
EthernetMAC10GE-T710
EthernetMAC10GE-T720
EthernetMAC10GE-T730
B19-T100
B19-T200
wb conmax-T100
Average

TN
285
289
294
287
280
289
290
2389
5957
5961
5950
5656
5655
5663
7027
7063
7047
102453
102452
102453
102443
62891
60742
23194
-

FP
4
4
2
3
10
2
0
8
10
1
25
0
1
25
36
1
17
0
0
0
10
279
2428
0
-

FN
6
6
4
9
1
7
9
1
1
13
0
5
0
2
8
1
1
13
12
9
1
14
1
14
-

TP
7
5
6
0
11
6
4
26
14
2
26
7
15
13
1
82
730
0
1
4
12
69
82
1
-

Recall
0.538
0.455
0.600
0.000
0.917
0.462
0.308
0.963
0.933
0.133
1.000
0.583
1.000
0.867
0.111
0.988
0.999
0.000
0.077
0.308
0.923
0.831
0.988
0.067
0.585

Precision
0.636
0.556
0.750
0.000
0.524
0.750
1.000
0.765
0.583
0.667
0.510
1.000
0.938
0.342
0.027
0.988
0.977
0.000
1.000
1.000
0.545
0.198
0.033
1.000
0.616

F1-score
0.583
0.500
0.667
0.000
0.667
0.571
0.471
0.852
0.718
0.222
0.675
0.737
0.968
0.491
0.043
0.988
0.988
0.000
0.143
0.471
0.686
0.320
0.063
0.125
0.498

Accuracy
0.967
0.967
0.980
0.960
0.964
0.970
0.970
0.996
0.998
0.998
0.996
0.999
1.000
0.995
0.994
1.000
0.998
1.000
1.000
1.000
1.000
0.995
0.962
0.999
0.988

APPENDIX H
RANDOM HT GENERATION

For the experiment

in Section VII-C, we implement a
method to randomly generate HT-infested circuits inspired by
[27]. In our implementation, we consider that the HT circuit
consists of a trigger part and payload part. We prepare several
templates of the parts, as shown in Table XIX, as candidates
of HT circuits.

In the experiments, we select ﬁve normal circuits from the
Trust-HUB benchmark, and generate 20 samples for each nor-
mal circuit. The template is written in RTL, and we synthesize
it with Synopsis Design Compiler using the Synopsys SAED
90nm standard cell library. When generating a sample, the
templates in Table XIX are randomly chosen as a trigger
and payload part with the internal parameters of the parts
randomly conﬁgured. A sample is composed by connecting
the conﬁgured parts and a normal circuit.

(a) Recall of GAT.

(b) Precision of GAT.

(c) F1-score of GAT.

(d) Recall of MPNN.

(e) Precision of MPNN.

(f) F1-score of MPNN.

(g) Recall of GIN.

(h) Precision of GIN.

(i) F1-score of GIN.

Fig. 5: Evaluation for the best parameters of the proposed method.

TABLE XVI: Detection results for the GAT model of the
baseline method with the best parameter.

TABLE XVII: Detection results for the MPNN model of the
baseline method with the best parameter.

Netlist
RS232-T1000
RS232-T1100
RS232-T1200
RS232-T1300
RS232-T1400
RS232-T1500
RS232-T1600
s15850-T100
s35932-T100
s35932-T200
s35932-T300
s38417-T100
s38417-T200
s38417-T300
s38584-T100
s38584-T200
s38584-T300
EthernetMAC10GE-T700
EthernetMAC10GE-T710
EthernetMAC10GE-T720
EthernetMAC10GE-T730
B19-T100
B19-T200
wb conmax-T100
Average

TN
289
293
296
290
290
289
290
2388
5967
5962
5974
5656
5656
5688
7063
7062
7061
102453
102452
102453
102453
63170
63170
23193
-

FP
0
0
0
0
0
2
0
9
0
0
1
0
0
0
0
2
3
0
0
0
0
0
0
1
-

FN
0
0
0
0
0
0
4
2
8
5
15
1
1
2
3
1
82
0
1
1
1
0
1
1
-

TP
13
11
10
9
12
13
9
25
7
10
11
11
14
13
6
82
649
13
12
12
12
83
82
14
-

Recall
1.000
1.000
1.000
1.000
1.000
1.000
0.692
0.926
0.467
0.667
0.423
0.917
0.933
0.867
0.667
0.988
0.888
1.000
0.923
0.923
0.923
1.000
0.988
0.933
0.880

Precision
1.000
1.000
1.000
1.000
1.000
0.867
1.000
0.735
1.000
1.000
0.917
1.000
1.000
1.000
1.000
0.976
0.995
1.000
1.000
1.000
1.000
1.000
1.000
0.933
0.976

F1-score
1.000
1.000
1.000
1.000
1.000
0.929
0.818
0.820
0.636
0.800
0.579
0.957
0.966
0.929
0.800
0.982
0.939
1.000
0.960
0.960
0.960
1.000
0.994
0.933
0.915

Accuracy
1.000
1.000
1.000
1.000
1.000
0.993
0.987
0.995
0.999
0.999
0.997
1.000
1.000
1.000
1.000
1.000
0.989
1.000
1.000
1.000
1.000
1.000
1.000
1.000
0.998

Netlist
RS232-T1000
RS232-T1100
RS232-T1200
RS232-T1300
RS232-T1400
RS232-T1500
RS232-T1600
s15850-T100
s35932-T100
s35932-T200
s35932-T300
s38417-T100
s38417-T200
s38417-T300
s38584-T100
s38584-T200
s38584-T300
EthernetMAC10GE-T700
EthernetMAC10GE-T710
EthernetMAC10GE-T720
EthernetMAC10GE-T730
B19-T100
B19-T200
wb conmax-T100
Average

TN
289
293
296
290
290
291
290
2368
5967
5962
5974
5656
5656
5662
7062
7059
7059
102451
102452
102451
102453
63170
63166
23178
-

FP
0
0
0
0
0
0
0
29
0
0
1
0
0
26
1
5
5
2
0
2
0
0
4
16
-

FN
0
0
0
0
0
0
4
1
5
7
12
2
1
2
1
1
118
2
1
1
0
0
0
5
-

TP
13
11
10
9
12
13
9
26
10
8
14
10
14
13
8
82
613
11
12
12
13
83
83
10
-

Recall
1.000
1.000
1.000
1.000
1.000
1.000
0.692
0.963
0.667
0.533
0.538
0.833
0.933
0.867
0.889
0.988
0.839
0.846
0.923
0.923
1.000
1.000
1.000
0.667
0.879

Precision
1.000
1.000
1.000
1.000
1.000
1.000
1.000
0.473
1.000
1.000
0.933
1.000
1.000
0.333
0.889
0.943
0.992
0.846
1.000
0.857
1.000
1.000
0.954
0.385
0.900

F1-score
1.000
1.000
1.000
1.000
1.000
1.000
0.818
0.634
0.800
0.696
0.683
0.909
0.966
0.481
0.889
0.965
0.909
0.846
0.960
0.889
1.000
1.000
0.976
0.488
0.871

Accuracy
1.000
1.000
1.000
1.000
1.000
1.000
0.987
0.988
0.999
0.999
0.998
1.000
1.000
0.995
1.000
0.999
0.984
1.000
1.000
1.000
1.000
1.000
1.000
0.999
0.998

151015202530Thenumberofmini-batches0.00.20.40.60.81.0Recall#layer=2,#unit=16#layer=2,#unit=32#layer=3,#unit=16#layer=3,#unit=32151015202530Thenumberofmini-batches0.00.20.40.60.81.0Precision151015202530Thenumberofmini-batches0.00.20.40.60.81.0F1-score151015202530Thenumberofmini-batches0.00.20.40.60.81.0Recall151015202530Thenumberofmini-batches0.00.20.40.60.81.0Precision151015202530Thenumberofmini-batches0.00.20.40.60.81.0F1-score151015202530Thenumberofmini-batches0.00.20.40.60.81.0Recall151015202530Thenumberofmini-batches0.00.20.40.60.81.0Precision151015202530Thenumberofmini-batches0.00.20.40.60.81.0F1-score(a) Recall of GAT.

(b) Precision of GAT.

(c) F1-score of GAT.

(d) Recall of MPNN.

(e) Precision of MPNN.

(f) F1-score of MPNN.

(g) Recall of GIN.

(h) Precision of GIN.

(i) F1-score of GIN.

Fig. 6: Evaluation of the best parameters of the baseline method

TABLE XVIII: Detection results for the GIN model of the
baseline method with the best parameter.

Netlist
RS232-T1000
RS232-T1100
RS232-T1200
RS232-T1300
RS232-T1400
RS232-T1500
RS232-T1600
s15850-T100
s35932-T100
s35932-T200
s35932-T300
s38417-T100
s38417-T200
s38417-T300
s38584-T100
s38584-T200
s38584-T300
EthernetMAC10GE-T700
EthernetMAC10GE-T710
EthernetMAC10GE-T720
EthernetMAC10GE-T730
B19-T100
B19-T200
wb conmax-T100
Average

TN
287
292
296
290
289
291
290
2397
5967
5962
5972
5656
5652
5678
7058
7028
7023
102429
102431
102428
102451
63170
62777
23194
-

FP
2
1
0
0
1
0
0
0
0
0
3
0
4
10
5
36
41
24
21
25
2
0
393
0
-

FN
1
11
10
9
9
13
12
27
4
13
23
12
14
14
5
7
386
8
11
8
11
83
54
14
-

TP
12
0
0
0
3
0
1
0
11
2
3
0
1
1
4
76
345
5
2
5
2
0
29
1
-

Recall
0.923
0.000
0.000
0.000
0.250
0.000
0.077
0.000
0.733
0.133
0.115
0.000
0.067
0.067
0.444
0.916
0.472
0.385
0.154
0.385
0.154
0.000
0.349
0.067
0.237

Precision
0.857
0.000
0.000
0.000
0.750
0.000
1.000
0.000
1.000
1.000
0.500
0.000
0.200
0.091
0.444
0.679
0.894
0.172
0.087
0.167
0.500
0.000
0.069
1.000
0.392

F1-score
0.889
0.000
0.000
0.000
0.375
0.000
0.143
0.000
0.846
0.235
0.188
0.000
0.100
0.077
0.444
0.779
0.618
0.238
0.111
0.233
0.235
0.000
0.115
0.125
0.240

Accuracy
0.990
0.961
0.967
0.970
0.967
0.957
0.960
0.989
0.999
0.998
0.996
0.998
0.997
0.996
0.999
0.994
0.945
1.000
1.000
1.000
1.000
0.999
0.993
0.999
0.986

TABLE XIX: Templates used in random HT generation

Part
Trigger
Payload

Template
Combinatorial, Sequential
Denial of service, Information leakage, Power consuming

151015202530Thenumberofmini-batches0.00.20.40.60.81.0Recall#layer=2,#unit=16#layer=2,#unit=32#layer=3,#unit=16#layer=3,#unit=32151015202530Thenumberofmini-batches0.00.20.40.60.81.0Precision151015202530Thenumberofmini-batches0.00.20.40.60.81.0F1-score151015202530Thenumberofmini-batches0.00.20.40.60.81.0Recall151015202530Thenumberofmini-batches0.00.20.40.60.81.0Precision151015202530Thenumberofmini-batches0.00.20.40.60.81.0F1-score151015202530Thenumberofmini-batches0.00.20.40.60.81.0Recall151015202530Thenumberofmini-batches0.00.20.40.60.81.0Precision151015202530Thenumberofmini-batches0.00.20.40.60.81.0F1-score