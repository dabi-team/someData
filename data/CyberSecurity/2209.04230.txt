Springer Nature 2021 LATEX template

Survey on Deep Fuzzy Systems in regression applications: a
view on interpretability

Jorge S. S. J´unior1*, J´erˆome Mendes1, Francisco Souza2 and Cristiano Premebida1

1*Department of Electrical and Computer Engineering, P´olo II, University of Coimbra,
Institute of Systems and Robotics, Coimbra, 3030-290, Portugal.
2Dept. Analytical Chemistry & Chemometrics, Radboud University, Nijmegen, 6525 AJ,
the Netherlands.

*Corresponding author(s). E-mail(s): jorge.silveira@isr.uc.pt;
Contributing authors: jermendes@isr.uc.pt; f.souza@science.ru.nl; cpremebida@isr.uc.pt;

Abstract

Regression problems have been more and more embraced by deep learning (DL) techniques. The
increasing number of papers recently published in this domain,
including surveys and reviews,
shows that deep regression has captured the attention of the community due to eﬃciency and
good accuracy in systems with high-dimensional data. However, many DL methodologies have
complex structures that are not readily transparent to human users. Accessing the interpretabil-
ity of these models is an essential
factor for addressing problems in sensitive areas such as
cyber-security systems, medical, ﬁnancial surveillance, and industrial processes. Fuzzy logic sys-
tems (FLS) are inherently interpretable models, well known in the literature, capable of using
nonlinear representations for complex systems through linguistic terms with membership degrees
mimicking human thought. Within an atmosphere of explainable artiﬁcial intelligence, it is neces-
sary to consider a trade-oﬀ between accuracy and interpretability for developing intelligent models.
This paper aims to investigate the state-of-the-art on existing methodologies that combine DL
and FLS, namely deep fuzzy systems, to address regression problems, conﬁguring a topic that
is currently not suﬃciently explored in the literature and thus deserves a comprehensive survey.

Keywords: Deep Regression, Explainable Artiﬁcial Intelligence, Interpretability, Deep Fuzzy Systems

2
2
0
2

p
e
S
9

]

G
L
.
s
c
[

1
v
0
3
2
4
0
.
9
0
2
2
:
v
i
X
r
a

1 Introduction

The goal in regression is to predict one or more
variables y(k) = (y1(k), . . . , ym(k))T from the
information provided by measurements x(k) =
(x1(k), . . . , xp(k))T for a given sample k. Cus-
tomary, y(k) are refereed as targets, outputs, or
dependent variables, while x(k) are commonly
refereed as predictors, inputs, covariates, regres-
sors, or independent variables. Regression mod-
els covers several application areas, as economic

growth problems [1, 2], air quality prediction
[3, 4], medicine [5, 6], chemical industries [7, 8],
and industrial processes [9, 10]. Recent studies
show that regression models have become predom-
inant in increasingly complex real-world systems
due to the large availability of data, inclusion of
nonlinear parameters, and other aspects intrin-
sic to the application area. For complex systems,
traditional machine learning techniques (i.e., non-
deep/shallow techniques) may become limited as

1

 
 
 
 
 
 
2

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

they face two main characteristics in current sys-
tems: high-dimensionality (with a high amount
of observations and inputs) and complexity (due
to the variety of dynamic and nonlinear features,
hyperparameters, and uncertainty). Deep learn-
ing (DL) techniques have gained prominence in
recent years due to their ability to represent sys-
tems in complex structures with multiple levels
of abstraction and high-level features. However,
deep learning techniques may have limitations
such as the need for a suﬃciently large dataset
for model training, sensitivity to hyperparame-
ter selection, and interpretability issues [11, 12].
In this sense, deep fuzzy systems have emerged
as a viable methodology to balance accuracy and
interpretability in complex real-world systems.

Regression models can be categorized as [13]:
(i) “white-box” when the input-output mapping
is built upon ﬁrst-principle equations, (ii) “black-
box” when the mapping is derived from the data
(also referred to as data-driven modeling), or (iii)
“grey-box” when the knowledge about the input-
output mapping is known beforehand and inte-
grated along with data-driven modeling. White-
box models are advantageous for promoting inter-
pretations of the internal mechanisms associated
with input-output mapping. On the other hand,
black-box models can address complex systems
with predictive analysis without prior knowledge
of the system. Grey-box models can combine the
interpretability presented by “white-box” and the
ability to learn from data given by “black-box”
models (e.g.,
fuzzy systems). Regarding data-
driven modeling, the dependency between input
and output can be built by linear or nonlinear
models. A regression model is linear when the rate
of change between input-output is constant due
to the linear combination of the inputs. Examples
include the multivariate linear regression models.
Data-driven nonlinear regression is adopted when
the input-output dependence is nonlinear and can
not be covered by linear modeling. There is a
plethora of methods for nonlinear regression, and
its applicability is problem-dependent. Examples
include fuzzy systems, support vector regression,
artiﬁcial neural networks (e.g., non-deep/shallow
and deep networks), and rule-based regression
(e.g., decision trees and random forest).

Recent surveys and reviews have showcased
the application of DL to regression problems.

The work of Han et al. [14] reviews deep mod-
els for time-series forecasting, where the models
are categorized as: (i) discriminative, where the
learning stage is based on the conditional proba-
bility of the output/target given an observation,
(ii) generative, which learn the joint probabil-
ity of both output and observation, with the
generation of random instances), or (iii) hybrid,
a combination of diﬀerent deep methods. With
the implementation in benchmark systems and
a real-world use case related to the steel indus-
try, the authors showed that deep models are
eﬃcient in discriminating complex patterns in
time series with high-dimensional data. Sun et al.
[11] discuss the use of DL for soft sensor appli-
cations, showing the trends and applications in
industrial processes, in addition to best practices
for model development. The authors established
some directions for future research, such as solu-
tions to address the lack of labeled samples (e.g.,
semi-supervised methods), hyperparameter opti-
mization, solutions to improve model reliability
(e.g., model visualization), and the development
of DL methods with distributed and parallel mod-
eling. Torres et al. [15] explore deep learning for
time-series forecasting, together with time-series
deﬁnition and processing using deep architectures
commonly described in the literature. In addition,
the authors demonstrated some practical aspects
of using DL methods to solve complex problems
with big data, such as a variety of libraries and
techniques compatible with deep structures for
automatic optimization of hyperparameters, hard-
ware infrastructure to beneﬁt DL implementation
with optimizable complexity and processing time,
and ﬂexibility to address real-world applications.
Pang et al. [16] and Chalapathy et al. [17] present
an overview of DL studies for anomaly detection;
they also discuss the complexities and diﬀerent
types of models (e.g., classiﬁcation, autoregres-
sive, unsupervised, and semi-supervised) applied
in various intelligent systems, such as cyber-
security systems, medical monitoring, ﬁnancial
surveillance, and industrial processes. Other works
that review the DL literature on regression are
[18–20]. It is noted from these works that DL
techniques have some advantages over non-deep
methods, such as the ability to learn complex rep-
resentations with automatic feature engineering,
not requiring prior experience or knowledge, good
performance by increasing the dimensionality of

Survey on Deep Fuzzy Systems

3

Springer Nature 2021 LATEX template

the data, among other speciﬁc advantages depend-
ing on the type of framework and application
[21].

Despite the advantages portrayed in the liter-
ature, deep learning has limitations, such as the
need for a suﬃciently large dataset for model
training, sensitivity to hyperparameter selection,
and lack of interpretability [11, 12]. Also, there
is a lack of proper explanations of the internal
structure of deep model structures, which raises
concern in applications that directly and indi-
rectly impact human life as well for operational
decisions [22, 23]. Having this concern in mind,
recent works address the interpretability issue of
DL from the following principles of “eXplainable
Artiﬁcial Intelligence” (XAI) systems [24]: the
existence of an appropriate explanation for each
decision made; each explanation must be mean-
ingful to the user; the process must accurately
consider what happens in the system; identify the
situations in which the system may or may not
function properly (knowledge limits).

Fuzzy logic systems (FLS), composed of IF-
THEN rules with linguistic terms mimicking
human thought [25], is one of the research areas
that contemplates the XAI principles. FLS has a
wide range of approaches to nonlinear systems,
primarily in terms of
interpretability, whether
related to the complexity and semantics of fuzzy
rules, notation readability, coverage of input data
space (operation regions), and so on [26, 27].
Moral et al.
[28] show the main beneﬁts of
adopting FLS for the development of explainable
methodologies, with a discussion of fundamen-
tal concepts and deﬁnitions associated with XAI
and FLS to how to design increasingly inter-
pretable models. Therefore, DL techniques can be
complemented with FLS, developing deep fuzzy
systems (DFS) and then providing an easy-to-
understand and easy-to-implement interface to
eﬃciently address the main drawbacks of DL
and FLS, thus ensuring good accuracy and good
interpretability. The surveys of [29] and [30] inves-
tigated some recent trends in DFS models and
real-world applications (e.g., time series forecast-
ing, natural language processing, traﬃc control,
and automatic control). However, despite the ben-
eﬁt of adopting FLS principles to DL systems,
no comprehensive survey or review has been con-
ducted focusing exclusively on deep fuzzy for
regression problems.

real inputs

Fuzzifier

fuzzy inputs

Fuzzy
Rule Base

real outputs

Defuzzifier

Fuzzy
Inference Model

fuzzy outputs

Fig. 1: Basic conﬁguration of fuzzy logic systems.

This paper surveys and discusses the state-of-
the-art on deep fuzzy techniques developed to deal
with a diverse range of regression applications.
Initially, Section 2 presents fundamental concepts
about FLS and XAI. Then, an overview of DL
techniques commonly used for regression will be
presented in Section 3, namely Convolutional Neu-
ral Networks, Deep Belief Networks, Multilayer
Autoencoders, and Recurrent Neural Networks.
Next, Section 4 shows the literature on deep fuzzy
systems in two ways: (i) standard deep fuzzy sys-
tems, based on fundamental FLS; (ii) hybrid deep
fuzzy systems, with the combination of FLS and
the conventional deep models discussed in Section
3. Finally, Section 5 presents general discussions
based on the state-of-the-art surveyed.

2 Background

2.1 Fuzzy logic systems

Developed initially by Lotﬁ A. Zadeh [31], FLS
are rule-based systems composed mainly of an
antecedent part, characterized by an “IF” state-
ment, and a consequent part, characterized by a
“THEN” statement, allowing the transformation
of a human knowledge base into mathematical
formulations, thus introducing the concept of lin-
guistic terms related to the membership degree.

The basic conﬁguration of a fuzzy logic system,
shown in Figure 1, depends on an interface that
transforms the real input variables into fuzzy sets
(fuzziﬁer), which are interpreted by a fuzzy infer-
ence model to perform an input-output mapping
based on fuzzy rules. Thus, the mapped fuzzy out-
puts go through an interface that transforms them
into real output variables (defuzziﬁer) [32, 33].
Some well-known fuzzy systems are Mamdani
fuzzy systems [34], Takagi-Sugeno (T-S) fuzzy
systems [35], and Angelov-Yager’s (AnYa) fuzzy
rule-based systems using data clouds [36]. Among

4

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

these, the T-S fuzzy models stand out for their
ability to decompose nonlinear systems into a set
of linear local models smoothly connected by fuzzy
membership functions [37]. T-S fuzzy models are
universal approximators capable of approximat-
ing any continuous nonlinear system, that can be
described by the following fuzzy rules [38]:

Ri : IF x1(k) is F i

1 and . . . and xp(k) is F i
p

THEN yi(k) = f i(x1(k), . . . , xp(k)), (1)

where Ri (i = 1, . . . , N ) represents the i-th fuzzy
rule, N is the number of rules, x1(k), . . . , xp(k) are
the input variables of the T-S fuzzy system, F i
j are
the linguistic terms characterized by fuzzy mem-
bership functions µi
, and f (x1(k), . . . , xp(k))
represents the function model of the system of the
i-th fuzzy rule [39].

F i
j

2.2 Explainable artiﬁcial intelligence

concept

Although the explainable artiﬁcial
intelligence
is often associated with a
(XAI)
homonymous program formulated by a group of
researchers from the Defense Advanced Research
Projects Agency (DARPA) [40], the principles
related to explainability gained strength from the
1970s onwards. The earliest works presented rule-
based structures and decision trees with human-
oriented explanations, such as the MYCIN system
proposed in [41] developed for infectious dis-
ease therapy consultation, the tutoring program
GUIDON proposed in [42] based on natural lan-
guage studies, among numerous other systems
[43–46]. Although many authors commonly use
the terms “explainability” and “interpretability”
as synonyms, Rudin [47] discusses the problem of
using purely explainable methods to only provide
explanations from the results obtained in black-
box models (post-hoc analysis), demystifying the
importance of developing inherently interpretable
methodologies with causality relationships that
are understandable to human users.

3 Deep Regression Overview

Deep neural networks (DNN) have emerged due
to their architecture with multiple levels of repre-
sentation and their remarkable performance in a
variety of tasks [48]. This section aims to discuss

DL techniques commonly employed in regression
problems, providing a better understanding and
context for deep fuzzy regression in Section 4.

3.1 Convolutional Neural Networks

Convolutional neural networks (CNNs or Con-
vNets) are feedforward neural networks with a
grid-like topology that are used for applications
such as time-series data processing in 1-D grids
and image data processing in 2-D pixel grids [49].
Figure 2 presents a CNN architecture for process-
ing time-series data in 1-D grids. The “neocogni-
tron” model proposed in [50] is frequently referred
to as the inspiration model for what is currently
known about CNNs. First proposed in [51], the
neocognitron aimed to represent simple and com-
plex cells from the visual cortex of animals which
present a mechanism capable of detecting light in
receptive ﬁelds [52].

CNNs use a math operation on two func-
tions called “convolution”, with the ﬁrst function
referred to as input, the second function as ker-
nel, and the convolution’s output as the feature
map [49]. Outputs from the convolution layer go
through a pooling layer (downsampling), which
performs an overall statistic of the adjacent out-
puts by reducing the size of the data and associ-
ated parameters via weight sharing [11, 53]. After
the data is processed along with the layers that
alternate between convolution and pooling, the
ﬁnal feature maps go through a fully connected
(dense) layer to extract high-level features. For
regression problems, the extracted features can
be combined in a prediction mechanism with an
activation function or a supervised learning model
(e.g., support vector regression) to estimate the
ﬁnal output [54, 55].

The application of CNNs for regression has
been explored for traﬃc ﬂow forecasting [56, 57],
prediction of natural environmental factors [58–
61], industrial process optimization [62–64], elec-
trical/power systems applications [65–69], and
chemical process analysis [70, 71]. Despite showing
good performance with the extraction of spatially
organized features, essentially for pre-processing,
CNN’s performance depends on a large amount of
data and the correct choice of hyperparameters,
being computationally intensive [21, 72].

Survey on Deep Fuzzy Systems

5

Springer Nature 2021 LATEX template

Input
data

Data pre-
processing

Prediction

Output
data

Convolutional
layer

Pooling
layer

Fully connected
layer

Fig. 2: Convolutional neural network, with 1-D architecture.

Output layer y 

RBM(cid:4) 

RBM2 

h(cid:4) 

h(cid:4)− 1 

h2 

h1 

Unsupervised
learning

RBM1 

Input layer x 

w(cid:1)(cid:2)(cid:3)  

w(cid:4) 

w2 

w1 

Supervised
learning

Fig. 3: Deep Belief Networks architecture.

3.2 Deep Belief Networks

Deep Belief Networks (DBNs) are probabilistic
generative models proposed by Hinton et al. [73],
which have a hierarchical structure as illustrated
in Figure 3. The DBNs are composed of multiple
layers of latent variables (hidden units) of binary
values, organized into multiple learning modules
called restricted Boltzmann machines (RBMs).

Each RBM comprises a layer of visible units
for data representation and a layer of hidden units
for feature representation, learned by capturing
higher-order correlations from the data. The two
RBM layers, with no connections within layers, are
connected by a matrix of symmetrically weighted
connections, with a total of L weight matrices
W = {w1, w2, . . . , wL}, considering a DBN of L
hidden layers. All units in each layer have a bidi-
rectional connection with all units in neighboring
layers, except for the last two layers, L and output,
that have a unidirectional connection [49]. In the

DBN architecture of Figure 3, the layer of visible
units in the ﬁrst RBM represents the input vari-
ables x and the subsequent layers represent hidden
units h, progressing hierarchically until reaching
the estimated output, with the output weights
wout.

Recent studies with DBNs in the context of
regression were mainly applied in industrial pro-
cesses for process monitoring [74–79], soft sensing
[80, 81], and prognostics [82, 83]. Other applica-
tions include time-series forecasting [84–88] and
benchmark systems modeling [89, 90]. Studies
with DBNs have diﬃculties in investigating the
inﬂuence of hidden units on system dynamics,
leading to interpretability issues. As Figure 3
shows, a DBN needs to undergo unsupervised
and supervised learning, which allows the training
process to become slower as the DBN structure
increases. In this way, the DBN becomes sensi-
tive to noisy inputs by not correctly readjusting
its low-level parameters [91].

3.3 Multilayer Autoencoders

Autoencoders (AEs) are feedforward neural net-
works used for dimensionality reduction and rep-
resentation learning, whose training is aimed at
mimicking inputs to outputs [49]. A historical
overview of deep learning in [92] presented some
early works of AEs in the literature, such as the
work in [93] that proposes unsupervised architec-
tures to reconstruct the inputs through internal
representations. Another early work was published
in [94], where the authors explore the eﬀect of
hidden units in simple two-layer associative net-
works, in which they want to map input patterns
to a set of output patterns. As presented in Figure
4, a single AE essentially has a structure com-
posed of a layer with input variables x, a layer
with hidden units h (which performs an encoding
used to represent the input), and an output layer

6

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

AE1 

AE2 

AE(cid:1) 

x 

h1 

x ^

h1 

h2 

^
h1 

h(cid:1)− 1 

^

h(cid:1)  h(cid:1)− 1 

Prediction

y 

x 

h1 

h2 

h(cid:1) 
SAE
Fig. 4: Modiﬁed Stacked Autoencoder architec-
ture for regression.

with the reconstructed inputs represented with a
“hat” symbol (e.g., ˆx). These layers are intercon-
nected by an encoder function (between input and
hidden) and by a function decoder (between hid-
den and output) [49]. As an evident constraint,
the number of neurons in the input layer must be
the same number of neurons in the output layer,
setting AE to unsupervised pre-training or feature
extraction [95].

A viable alternative to deal with increasingly
complex data is to increase the number of hidden
layers in standard AEs, enabling the development
of deep network architectures. A well-known con-
ﬁguration of multilayer autoencoders found in the
literature is stacked autoencoder (SAE). As illus-
trated in Figure 4, an SAE is developed from the
grouping of L AEs, where the hidden layers of the
autoencoders are stacked hierarchically, perform-
ing an unsupervised layerwise learning algorithm.
Thus, the reconstruction of inputs after the L-
th hidden layer can be disregarded to address
regression problems. As for CNNs, a prediction
mechanism or a supervised learning model can
be included after the L-th hidden layer to esti-
mate the system output. Related parameters, such
as weights W = {w1, w2, . . . , wl} between lay-
ers, are ﬁne-tuned by a supervised method (e.g.,
backpropagation algorithm) [96].

Recent studies involving types of AEs with
deep architecture for regression applied to var-
ious cases of industrial processes, such as soft
sensing [97–99], hydrocracking process [100–102],
CNC turning machine [103, 104], end-point qual-
ity prediction [105, 106], and prognostics [107–

y^

W(cid:1)(cid:2)(cid:3)  

Wh 

W(cid:4)(cid:5)  

x

h

unfold

^
y(1) 

y((cid:6)) 
^

W(cid:1)(cid:2)(cid:3)  

W(cid:1)(cid:2)(cid:3)  

h(0) 

h(1) 

h((cid:6)-1) 

h((cid:6)) 

Wh 

W(cid:4)(cid:5)  

Wh 

W(cid:4)(cid:5)  

x(1) 

x((cid:6)) 

Fig. 5: Recurrent Neural Network architecture.

109]. Other works were developed for time-series
forecasting [110–114]. Some of the limitations of
multilayer autoencoders include the sensitivities
to errors or loss of information from the ﬁrst layer,
impairing learning as it progresses through the
hidden layers. With this, the nature of encoding
and decoding by hidden layers can cause a loss of
interpretability and an increase in computational
cost [21].

3.4 Recurrent Neural Networks

Recurrent Neural Networks (RNNs) are artiﬁcial
neural networks with an internal state that allow
the use of feedback signals between neurons [115].
One of the early works that culminated in the pop-
ularization of RNNs was proposed in [116], with
the development of content-addressable memory
systems called Hopﬁeld networks whose dynam-
ics have a Lyapunov function (or energy function)
to direct to a local minimum of “energy” associ-
ated with the system states [117]. Another early
work, published in [118], presented a new learn-
ing procedure, backpropagation, which adjusts
the weights of connections of recurrent networks
with “internal hidden units”. Due to the presence
of an “internal memory”, RNNs are often used
to process data in the time domain (sequential
information), with weight sharing through hidden
states [119]. Figure 5 shows the architecture of an
RNN, whose hidden states h represent the “inter-
nal memory” of the system, and as the inputs are
sequentially observed, the corresponding outputs
are estimated. The weights Win, Wh and Wout
are auxiliary parameters that are shared across
time [120]. The states are updated at every tempo-
ral instant until completing all the input sequences
of length K [121, 122]. The RNN learning process
depends on a “backpropagation through time”

Survey on Deep Fuzzy Systems

7

Springer Nature 2021 LATEX template

Output y((cid:1))

s((cid:1)-1) 

h((cid:1)-1) 

tanh 

f

i

(cid:2)

(cid:0)

s 
~
tanh 

o 

(cid:1)

s((cid:1)) 

h((cid:1)) 

Input x((cid:1))  

Fig. 6: The Long Short-Term Memory cell.

algorithm, which is a gradient-based technique
that updates parameters recursively starting from
the last temporal instant and going backward in
time [123].

In deep learning, there are many variations of
standard RNNs, such as Long Short-Term Mem-
ory networks (LSTMs), Gated Recurrent Units
(GRUs), and Echo State Network (ESNs) [49].
Some of these variations were proposed to cope
with some limitations present in standard RNNs,
such as exploding and vanishing gradients (insta-
bility in networks caused by a large variation in
model parameters), overﬁtting, and diﬃculty to
store low-level features in long data sequences
[124]. In this document, only LSTMs and ESNs
employed for regression problems will be dis-
cussed.

3.4.1 Long Short-Term Memory

Long Short-Term Memory networks (LSTMs)
were initially developed in [125] to overcome the
issues of RNNs associated with vanishing/explod-
ing gradients. These issues can occur during the
training of long temporal sequences with the back-
propagation through time algorithm. In this sense,
during successive operations in compound func-
tions with the weight matrices, gradients can
exponentially reach very low values close to zero
(vanishing) or very high values (exploding) [49].
Figure 6 illustrates the architecture of an LSTM.
LSTMs introduced “gates”, nonlinear elements
that control memory cells using sigmoidal func-
tions σ, hyperbolic tangent functions, current
observation x(k) and hidden units h(k − 1) from
the previous time instant [122]. Each of these
memory cells has input, output and forget gates

(i, o and f , respectively), that protect the infor-
mation from perturbations caused by irrelevant
inputs and irrelevant memory contents [72]. The
information stored in the cells represents the
states s(k) obtained from the data processed in
the time domain. Despite having the same inputs
and outputs as a standard RNN, an LSTM cell
has an internal recurrence (self-loop) to propagate
the information ﬂow through a long sequence and,
therefore, more parameters to be adjusted [49].

Due to the ability to process data sequen-
tially, LSTMs have been used mainly in appli-
cations involving time-series forecasting, such as
traﬃc ﬂow [126, 127] and natural environment
factors [128–133]. Other recent LSTM methodolo-
gies for regression have been applied in industrial
processes for soft sensing [134–136], process mon-
itoring [137–139] and prognostics [140, 141], as
well as applications involving electrical/power sys-
tems [142, 143]. Despite the practicality of LSTMs
in regression problems, they can still suﬀer from
vanishing due to the possibility of saturation of
cell states, which must be reset occasionally, in
addition to requiring a high memory bandwidth
depending on the execution of functions inside the
cell in complex systems (computational cost issue)
[144].

3.4.2 Echo State Network

Echo State Networks (ESNs) are variations of
RNNs that were developed in [145] and share the
basic ideas of reservoir computing of Liquid State
Machines from [146]. The term “reservoir com-
puting” stands for a homonymous research stream
that introduced the concept of a dynamic reser-
voir,
in place of the hidden layer, with many
sparsely connected neurons [147]. In addition, this
reservoir must have a stability condition known
as “echo state property”, which allows for a grad-
ual reduction in the eﬀect of previous states and
inputs on future states over time [148]. Figure 7
illustrates the architecture of an ESN.

An ESN induces nonlinear response signals
from the input signals x(k), whose resulting state
h(k) echoes the input information, estimating a
desired output signal y(k) [145]. In addition to
the input weight Win and output weight Wout
that are present in a standard RNN, the ESN fea-
tures the reservoir weights Wr and, optionally, the
output-to-reservoir feedback weights Wback. The

 
 
 
8

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

(cid:11)1((cid:7)) 

(cid:11)2((cid:7)) 

(cid:11)(cid:12)((cid:7)) 

Input
layer

Dynamic
Reservoir

Output
layer

W(cid:1)(cid:2)  

W(cid:3) 

W(cid:4)(cid:5)(cid:6)  

y((cid:7)) 

h((cid:7)) 

W(cid:8)(cid:9)(cid:10)(cid:7)  

Fig. 7: The Echo State Network.

fundamental idea for the functioning of an ESN is
to adjust only Wout during training, while the rest
are randomly assigned and ﬁxed before learning
[149].

A recurring application of ESN is the time-
series prediction in chaotic systems [150–154],
wind power systems [155–159], solar energy sys-
tems [160–163], and benchmark systems [164–
166]. Other applications include industrial pro-
cesses [167–169], electrical/power systems [170–
172], and turbofan engine time-series prediction
[173, 174]. It is observed in these studies that the
random connectivity within the dynamic reservoir
of an ESN can lead to interpretability issues. In
addition, ESN hyperparameters, such as the num-
ber of units in the reservoir and input scaling, have
a short operating space that ensures maximum
model performance [175].

4 Deep Fuzzy Regression

Deep fuzzy systems (DFSs) are models built on
top of DL structures with fuzzy logic systems
(FLSs). DFS aims to overcome the lack of inter-
pretability of DL systems and the limitations of
FLS when dealing with high-dimensional data.
DFS is often referred to in the literature as an
explainable model due to the incorporation of
fuzzy logic into its core. From the deﬁnition from
[176], an explainable AI (XAI) system should
provide capabilities accessible to human under-
standing, reﬂecting positively on the health of the
system’s processes and being able to operate even
in unforeseen situations. Although DFS is pro-
moted as an XAI system by deﬁnition, this is not
the case, as evidenced by works in the literature.
This section surveys recent applications of DFS
for regression, with the main focus on its structure
and if it adheres to the XAI principles.

4.1 Survey on deep fuzzy systems

This survey will follow two stages to review the
DFS for regression applications. First, the DFS
will be categorized according to its structure. Sec-
ondly, the models will be categorized according to
whether they follow the XAI principles. The struc-
tures of deep fuzzy models can be represented in
several forms, such as those illustrated in Figure 8.
Here, deep fuzzy structures are summarized into
two categories: (i) Standard DFS and (ii) Hybrid
DFS. A model belongs to the ﬁrst category when
the blocks of fuzzy systems are stacked in series,
in parallel, or hierarchically (see Fig. 8a). Also,
there are cases where the architecture of such sys-
tems resembles neural network architecture, such
as the dense DNN architecture (see Fig. 8d). The
second category includes hybrid methodologies,
where conventional DL models are combined with
FLS. The combination of DL and FLS is com-
monly in an ensemble form (see Fig. 8b,c) or mixed
form (see Fig. 8d).

Aside from the deep fuzzy structures, the dis-
cussed works will be investigated whether they
are in synergy with the XAI principles. If so,
these works will be classiﬁed following the cat-
egories deﬁned in [22]: understanding and scope
of the explainability. In terms of understanding,
the models will be classiﬁed as (i) transparent
or (ii) opaque. Transparent models are those in
which the decisions, predictions, or inner func-
tioning are perceptible or are visible; they are
considered opaque otherwise. The scope is related
to accessing the model’s interpretability through
post-hoc explanations, classiﬁed as (i) local, (ii)
global, and (iii) visual. Local explanations facili-
tate comprehension of small regions of interest in
the input space for a given decision/prediction.
Global explanations when such desired under-
standing considers the entire sample space. Visual
explanations are required when visual interfaces
are needed to demonstrate the inﬂuence of fea-
tures on decisions.

The following sections will discuss the methods
containing DFS for regression problems. Section
4.1.1 will discuss Standard DFS, while Section
4.1.2 will discuss Hybrid DFS.

Survey on Deep Fuzzy Systems

9

Springer Nature 2021 LATEX template

Input

Layer 1 

Layer 2 

Layer (cid:1)

Output

Input

DL

(cid:23)(cid:24)(cid:25)

Output

F(cid:3)(cid:4)

(cid:5)(cid:6)(cid:7)

(cid:8)(cid:9)(cid:10)

(cid:20)(cid:21)(cid:22)

(cid:14)(cid:15)(cid:16)

(cid:17)(cid:18)(cid:19)

Input

(cid:26)(cid:27)(cid:28)

DL

Output

((cid:29)(cid:30)

DL

Input

M"#$%

Output

(cid:11)(cid:12)(cid:13)

(cid:31) !

(a)

(c)

Input

Layer 1 

Layer (cid:3) 

Layer (cid:1)?@

&’)*be+,-./0

12345678

9:;<=>

Layer (cid:1)
ABCDEGHIJKt)

Output

∏ 

∏ 

∏ 

∏ 

(cid:2) 

(cid:2) 

(cid:2) 

(cid:2) 

∑

Fig. 8: Examples of deep fuzzy system frameworks: (a) multiple FLSs organized hierarchically; (b)
sequential ensemble models; (c) parallel ensemble models; (d) fuzzy neural network fully integrated into
a deep architecture or mixed with parts of a conventional deep model.

LNO

4.1.1 Standard deep fuzzy systems

The methods discussed in this section have a DFS
structure and were used in any regression prob-
lem domain while adhering to the Standard DFS
structure. They are discussed in order of struc-
tural similarity, with structurally similar methods
following each other.

In [177], a model called Randomly Locally
Optimized Deep Fuzzy System (RLODFS) is pro-
posed. It is composed of a hierarchical structure
of a bottom-up layer-by-layer type with FLS, sim-
ilar to a fully connected DNN; the structure of
RLODFS is shown in Figure 9. In RLODFS,
the input variables are divided into several fuzzy
subsystems, allowing the decrease of the size of
the fuzzy rule’s antecedent, thus reducing the
learning complexity when compared to learning
with all input variables at once. The structure

of RLODFS shows several groupings with shar-
ing of input variables performed randomly for the
fuzzy subsystems of level 1, whose outputs are
used as inputs of the fuzzy subsystems of the sub-
sequent levels until reaching the last level fuzzy
system (used to estimate the model output). The
training of each fuzzy subsystem was followed by
the Wang-Mendel algorithm [178], which performs
the construction of fuzzy rules from a small-scale
observation set (data pairs) with input-output
mapping. Finally, a random local loop optimiza-
tion strategy is performed to remove feature com-
binations and corresponding subsystems with low
correlation to achieve fast convergence [177]. The
performance of the RLODFS method was com-
pared with DBN, LSTM, generalized regression
neural network (GRNN), among other prediction
methods. From the authors’ perspective, RLODFS

10

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

1
cd1

1
fg2

1

j

hi

1

kl

m

1
x1

1
x2

1

x

q

1

x

p

2
rs1

1
no(cid:1)1

2
(cid:131)(cid:132)(cid:1)1

1

x(cid:1)1

V

x1

W

x2

Y

x

X

[

x

Z

]

x

\

_

x

^

a

x

‘

U

x(cid:1) S T

R

x(cid:1) Q 2

0

x(cid:1) -1

P

x(cid:1)

(cid:2)(cid:127)(cid:128)
x1

(cid:2)}~
x2

(cid:2){|

x

z

(cid:2)xy

x

w

(cid:2)uv

x(cid:1) (cid:2)t 1

(cid:2)
(cid:129)(cid:130)1

y

Layer (cid:2) Output
Input Layer 1 
Fig. 9: The structure of RLODFS based on
grouping and input sharing, adapted from [177].

Layer 2 

has good interpretability due to its structure, clear
physical meaning of its parameters, and the ease of
locating fuzzy rules that may fail for future opti-
mizations. However, there is a lack of transparency
in using input sharing strategies, which increases
the method complexity. Furthermore, the selec-
tion of the number of features per fuzzy subsystem
needs to be carefully done, whose manual/arbi-
trary choice may not reﬂect the real needs of the
case study under analysis.

The study in [179] proposes a stacked struc-
ture composed of double-input rule modules and
type-2 fuzzy models, abbreviated as
interval
IT2DIRM-DFM. The proposed model
is illus-
trated in Figure 10, where four layers are pre-
sented: the input layer, which deals directly with
the original input data, grouped two-by-two in
each rule module; the stacked layer, where the
signals coming from the input layer become the
inputs of the ﬁrst layer, whose output becomes
the input of the second layer, and so on; the
dimension reduction layer, where the width of
the IT2DIRM-DFM is hierarchically reduced until

it becomes two; and the output layer, where
the latest IT2DIRM-DFM model produces the
ﬁnal forecasting results. Still, the authors discuss
the interpretability of the model showing lay-
ered learning and fuzzy rules composed of only
two variables in the antecedent, partitioned into
interval type-2 second-order rule partitions. The
resulting model was evaluated in two real-world
applications, subway passenger and traﬃc ﬂow,
whose results allowed to verify the interpretabil-
ity from consistent readability of which partitions
and bounds of the inputs are used in each ﬁred
rule from each rule module. However, the proposed
method is only interpretable locally in each rule
module and not globally, whose structure does not
reﬂect the depth or number of layers needed to
address the experiments. Furthermore, it is not
clear the motivations for considering a function,
denoted as f, in each layer related to the worst-
performing module (this is shown at the bottom
of Figure 10).

Another work that explores double-input rule
modules within a stacked deep fuzzy model (in
a hierarchical way) was proposed in [180]. The
authors investigate the interpretability of the
resulting model, called DIRM-DFM, with con-
clusions similar to [179], mainly regarding the
composition of fuzzy rules. However, in DIRM-
DFM, they promote more transparency and sim-
plicity. Some other studies deal with interval
type-2 fuzzy models for deep learning in regression
problems. In [181], a novel dynamic fractional-
order deep learned type-2 FLS was proposed
and constructed using singular value decomposi-
tion and uncertainty bounds type-reduction. In
addition to determining the limit values of the
input data (upper and lower singular values), the
authors used stability criteria of fractional-order
systems, allowing to reduce the necessary num-
ber of fuzzy rules and reduce the complexity of
nonlinear systems. An evolving recurrent interval
type-2 intuitionistic fuzzy neural network (FNN)
was proposed in [182] for time-series prediction.
Intuitionistic evaluation, ﬁre strength of member-
ship degree and strategies for adding and removing
fuzzy rules were considered to improve uncertainty
modeling. Both studies in [181] and [182] did not
present an analysis of the interpretability of their
models.

 
 
Survey on Deep Fuzzy Systems

11

Springer Nature 2021 LATEX template

Data
Sets

Input
Layer

Stacked
Layer

Dimension
Reduction Layer

Output
Layer

1 2 3 4 5

L-2

L-1

L

1

I
T
2
D
R
M

I

2

3

p

f

f

f

Fig. 10: The structure of the double-input rule modules stacked deep interval type-2 fuzzy model
(IT2DIRM-DFM), adapted from [179].

Methods that use multiple neuro-fuzzy sys-
tems in a deep hierarchical structure were pro-
posed in [183] and [184]. The work in [183] pro-
posed a deep model that cascades multiple neuro-
fuzzy systems modiﬁed as multivariable general-
ized additive models, with application in real eco-
logical time series. The resulting model manages
to locally detail the mechanisms of each neuro-
fuzzy system in each layer. However, it presents
incomplete discussions and results regarding the
increase in the network depth and the inﬂuence of
the inputs and partial outputs of the layers on the
ﬁnal prediction. In [184], a hybrid cascade neuro-
fuzzy network was proposed, which is composed of
multiple extended neo-fuzzy neurons with adap-
tive training designated for online non-stationary
data stream handling. Each layer has a gener-
alization node that performs a weighted linear
combination to obtain an optimal output signal.
The experimental results in electrical loads predic-
tion show the authors’ search for better accuracy,
although at the cost of increasing membership
functions to cover the input space and increasing
adjusted parameters (weights).

The work in [185] proposed a deep learned
recurrent type-3 fuzzy system applied for model-
ing renewable energies (solar panels and wind tur-
bines). The proposed methodology showed good
performance compared to other methods, such as
multilayer perceptron, type-1 FLS, type-2 FLS,
and interval type-3 FLS, despite the lack of a more
elaborate discussion of the presented results. Fur-
thermore, transparency is not guaranteed regard-
ing the modeling steps and the inﬂuence of var-
ious parameters optimized during learning. The
authors in [186] proposed a self-organizing FNN
with incremental deep pre-training, abbreviated
as IDPT-SOFNN, to promote eﬃcient feature
extraction and dynamic adaptation in the struc-
ture according to error-reduction rate. In [187], a
deep fuzzy cognitive map was proposed for mul-
tivariable time-series forecasting, with an analysis
of the model’s interpretability based on nonlinear
and nonmonotonic inﬂuences of unknown exoge-
nous factors. The work in [188] proposed a deep
FNN composed of an input layer, four hidden
layers (membership functions, T-norm operation,

12

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

linear regression, and aggregation), and an out-
put layer, designed exclusively for intra- and
inter-fractional variational prediction for multiple
patients’ breathing motion.

Table 1 summarizes the literature on Standard
DFS. They are categorized according to the appli-
cation domain and regarding its interpretability
categories. The survey shows that the application
domain is vast, with applications in the domain of
industrial systems, power systems, traﬃc systems,
and multivariable benchmark systems. Table 1
shows that only four out of eleven works discuss
interpretability, despite most of their proposed
methods being transparent. Also, not all meth-
ods provide post-hoc explanations, and of these,
the local scope is more frequent. The Standard
DFS architecture is easy to implement, with ﬂex-
ibility in the construction of fuzzy rules, having a
similar structure to feedforward neural networks.
However, these methods may suﬀer from loss of
interpretability when using a non-intuitive hier-
archical structure, the lack of
investigation of
interconnections between the variables involved,
and the limitation of fuzzy rules that can dis-
turb the estimation by not covering all operating
regions (coverage of input data space) [189].

4.1.2 Hybrid deep fuzzy systems

The Hybrid DFSs are discussed in this section.
The selection and order of works follow the same
principles used for Standard DFS. The common
DL architectures used in combination with fuzzy
systems are the Deep Belief Networks, Autoen-
coders, Long Short-Term Memory networks, and
Echo State Networks.

In [190], it is proposed a sparse Deep Belief
Network (SDBN) with FNN for nonlinear system
modeling. The SDBN is considered for unsu-
pervised learning and pre-training to perform
fast weight-initialization and improve modeling
robustness. The FNN is used as supervised learn-
ing to reduce layer-by-layer complexity. As shown
in Figure 11, the structure of the DBN resembles
the structure shown in Figure 3, except for con-
sidering additional constraints (sparsity) used to
penalize ﬂuctuations of values along the hidden
neurons. The proposed method performed better
compared to other similar methods, such as trans-
fer learning-based growing DBN [191], DBN-based

echo-state network [163], and self-organizing cas-
cade neural network [192]. However, the authors
noticed various ﬂuctuations in the assignment of
hyperparameters, which can compromise the sta-
bility of the proposed model, making it necessary
to dynamically and robustly improve its structure
to these ﬂuctuations. In terms of interpretability,
the proposed model guarantees a moderate num-
ber of membership functions and rules, allowing
good consistency and readability of what happens
within the FNN structure. The same cannot be
said for the DBN structure, which is not intuitive
in sparse representation to decide which features
are more valuable than others.

The authors in [193] proposed a novel robust
deep neural network (RDNN) for regression prob-
lems involving nonlinear systems, with the imple-
mentation of three strategies: a fuzzy denoising
autoencoder (FDA) as a base-building unit for
RDNN, improving the ability to represent uncer-
tainties; a compact parameter strategy (CPS),
designed to reconstruct the parameters of the
FDA, reducing unnecessary learning parameters;
and an adaptive backpropagation (ABP) algo-
rithm to update RDNN parameters with fast
convergence. As shown in Figure 11, FDA has an
input layer, a fuzzy layer (built by Gaussian mem-
bership functions), and an output layer, which can
be divided into an encoder (“input” to “fuzzy”)
and a decoder (“fuzzy” to “output”). Further-
more, at each FDA, due to the nature of this
autoencoder, the data is partially corrupted by
noises (represented with a “tilde” symbol) and
is reconstructed using CPS (represented with a
“hat” symbol) with associated parameters (e.g.,
encoder/decoder weights and biases) are adjusted
via ABP. Regarding the interpretability of the pro-
posed model, there was no appropriate discussion
by the authors, as they focused mainly on model
performance/accuracy in the presence of uncer-
tainties. In addition to a fuzzy rule base not being
deﬁned, the number of fuzzy neurons in each FDA
is manually initialized and adapts through redun-
dancies during the reconstruction of parameters
via CPS. Finally, the architecture of the proposed
model is not intuitive based on the mechanisms
related to the representation ability of each FDA,
which would be crucial to determine the inﬂuence
of input variables and learning parameters on the
outputs.

Survey on Deep Fuzzy Systems

13

Springer Nature 2021 LATEX template

Table 1: State-of-the-art on methods with standard deep fuzzy systems for regression problems. XAI:
explainable artiﬁcial intelligence; Disc.: discussion by the authors (Yes/No); Und.: how understandable is
the model, whether it is transparent (T) or opaque (O); Scope: in post-hoc scope, if the model promotes
local explanations (L), global explanations (G) or visual explanations (V).

Reference Approach

Problem

XAI

Disc. Und.

Scope

Prediction of 12 real-world datasets Yes

T

[177]

[179]

[180]

[181]

[182]

[183]

[184]

[185]

[186]
[187]

[188]

deep

fractional-order

Hierarchical Randomly Locally
Optimized Deep Fuzzy System
with input sharing
Double-input rule modules stacked
deep interval type-2 fuzzy model
Double-input-rule-modules stacked
deep fuzzy model
Dynamic
learned type-2 fuzzy logic system
Evolving recurrent interval type-2
intuitionistic FNN
Deep Stacking Convex Neuro-
Fuzzy System
Hybrid cascade neuro-fuzzy scheme
(ensemble of extended neo-fuzzy
neurons)
Deep learned recurrent
fuzzy system
Self-Organizing Deep FNN
Deep Fuzzy cognition map model
(with an alternate function gradi-
ent descent algorithm)

type-3

Time-series forecasting

short-term photo-

Prediction of
voltaic power generation
Benchmark system prediction

Time-series forecasting

Real ecological time-series forecast-
ing
Time-series forecasting (electrical
loads)

Yes

Yes

No

No

No

No

Renewable energy modeling (solar
panels and wind turbines)
Nonlinear system modeling
No
Multivariate time-series forecasting Yes

No

L

L

L,G

-

-

L

-

-

V
V

L,V

T

T

O

T

T

T

O

T
T

T

Fuzzy Deep Learning architecture
(with four hidden layers)

Intra- and Inter-fractional varia-
tion prediction of Lung Tumors

No

Some hybrid methods using FLS and con-
ventional deep models have been applied in the
literature for traﬃc ﬂow prediction. In [194], the
authors developed an algorithm based on Dolphin
Echolocation optimization [195], where the input
features are fuzziﬁed into membership functions to
obtain chronological data, whose integration goes
into the weight update process of the proposed
algorithm converging to a globally optimal solu-
tion with a Deep Belief Network. The study in
[196] combined fuzzy information granulation and
a deep neural network to represent the temporal-
spatial correlation of mass traﬃc data and be able
to adapt to noisy data. A Stacked Autoencoder
is used to obtain the prediction results based on
processed granules that have a good capacity for
interpretation, which have not been discussed by
the authors.

Other methods were implemented for energy
forecasting, with the usual application of LSTM
networks. A novel fuzzy seasonal LSTM was pro-
posed in [197], where the fuzzy seasonality index
[198] and a decomposition method were employed
to solve the seasonal time-series problem in a
monthly wind power output dataset. In [199], the
authors used an LSTM network with rough set
theory [200] and interval type-2 fuzzy sets for
short-term wind speed forecasting, with the aid of
mutual information approach for eﬃcient variable
input selection. A novel ultra-short-term photo-
voltaic power forecasting method was proposed in
[201], where a T-S fuzzy model is developed using
a fuzzy c-means clustering algorithm and deep
belief networks. The studies in [197], [199] and
[201] did not discuss the interpretability of their
models, which have a structure with an ensemble
characteristic whose fuzzy part has an aﬃnity for

14

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

FNN learning
model

y

Output layer ∑

Membership
function
layer

Rule layer

h(cid:1) 

ƒ§¤'“«‹›ﬁﬂ(cid:176)–

of DBN

h(cid:1)− 1 

w(cid:1) 

Output layer

FDA(cid:1) 

(cid:136)(cid:137)(cid:138)(cid:139)(cid:140)(cid:141)(cid:142)(cid:143)(cid:144)(cid:145)(cid:146)(cid:147)

(cid:148)(cid:149)(cid:150)(cid:151)(cid:152)(cid:153)(cid:154)(cid:155)(cid:156)(cid:157)(cid:158)

FDA1 

h1 

(cid:159)(cid:160)¡¢£ ⁄¥yer

w1 

x

(a)

Input layer

(cid:133)(cid:134)(cid:135)

Fig. 11: Examples of FLS application with conventional deep models: (a) with deep belief networks,
adapted from [190]; and (b) with denoising autoencoders, adapted from [193].

good coverage of the input space and good capture
of data uncertainty.

The work in [202] proposed a deep type-2
FLS (D2FLS) architecture with greedy layer-
input data.
wise training for high dimensional
The authors showed how to extract interpretable
explanations related to the contribution of fuzzy
rules to the ﬁnal prediction developed for a two-
layer D2FLS. However, the authors opted for a
large number of fuzzy rules (100, in this case) that
impair interpretability by increasing the complex-
ity of the model. The authors in [203] proposed
an ensemble model composed of an Echo State
Network (ESN), a T-S fuzzy model, and diﬀeren-
tial evolution for time-series forecasting problems.

The diﬀerential evolution method, used to opti-
mize the weight coeﬃcients of the model, managed
to reduce the number of fuzzy rules. The inter-
pretability of the model can be impaired due to the
structure with an ensemble characteristic, which
does not provide enough transparency for learn-
ing. A method based on the T-S fuzzy model and
ESN for the identiﬁcation of nonlinear systems
was developed in [204]. The authors chose to bal-
ance model complexity and performance based on
the parameters involved for learning (e.g., number
of fuzzy rules and reservoir size), which resulted
in better results in comparison with other meth-
ods (e.g., traditional ESN and hybrid fuzzy ESN),
despite the lack of discussion on interpretability.

Survey on Deep Fuzzy Systems

15

Springer Nature 2021 LATEX template

In [205], a framework based on a sparse autoen-
coder (SpAE) and a high-order fuzzy cognitive
map (HFCM) is proposed for time-series forecast-
ing problems. SpAE is used to extract features
from the original data, and these features are via
HFCM. Another study that uses SpAE with FLS
was proposed in [206] and implemented in time-
series forecasting and classiﬁcation problems. The
proposed method uses a method to reduce fuzzy
rules by reducing the data dimensionality with
SpAE. Both studies in [205] and [206] do not
consider addressing the interpretability of the pro-
posed models, which present good directions in
data partitioning and construction of fuzzy rules
but are not intuitive in feature extraction with
sparse representation.

Table 2 summarizes the works presented in
this section, in addition to evaluating them within
intelligence
the context of explainable artiﬁcial
(XAI) systems. Only two works discuss the inter-
pretability in Hybrid DFS, half of the discussed
works are opaque, and only one presents post-hoc
explanations. This fact occurs since the combina-
tion with DNN brings a new layer of black-box
to the system. However, the Hybrid DFS meth-
ods categorized as transparent show that the
fuzzy component can promote good interpretabil-
ity with eﬃcient input-output mappings and the
construction of rules to cover the universe of dis-
course for a given system. Inherently interpretable
methodologies are diﬃcult to achieve only with an
ensemble of multiple methods, as seen in recent
studies, in addition to the challenges of reducing
the time complexity of systems, which is slightly
mitigated by the reduction of fuzzy rules [207].

5 Conclusion

This study surveyed the literature on deep fuzzy
systems (DFSs) for regression applications within
the context of explainable artiﬁcial
intelligence
(XAI) systems (with an emphasis on interpretabil-
ity). For the survey, the DFSs were categorized as
(i) Standard DFS and (ii) Hybrid DFS. Regarding
the interpretability, each method was categorized
as to whether it is transparent or not and whether
it has post-hoc explanations (under the deﬁni-
tion of [22]). Standard DFSs have been shown
to promote more interpretability of their mod-
els when compared to Hybrid DFSs, according to
the survey. Indeed, Standard DFSs are based on

fundamental fuzzy logic systems, which are inher-
ently interpretable, whereas Hybrid DFSs include
conventional deep learning (DL) methods, which
lack ﬂexibility in promoting interpretability. Fur-
thermore, the DFS is frequently referred to as
interpretable by default, but only 5 of the 23 works
surveyed here actually addressed this issue. The
remaining works had a common goal: to improve
prediction accuracy using their proposed meth-
ods. However, this survey presented the potential
of using Standard DFS as a base for developing
accurate models while promoting interpretability
since hybrid models are not straightforward.

Acknowledgments. This work has been sup-
ported by Funda¸c˜ao para a Ciˆencia e a Tecnolo-
gia (FCT) under the project UIDB/00048/2020.
Jorge S. S. J´unior is supported by Funda¸c˜ao para
a Ciˆencia e a Tecnologia (FCT) under the grant
ref. 2021.04917.BD.

Competing Interests

The authors declare no competing Interests.

References

[1] Busu, M. & Trica, C. L.

Sustainabil-
ity of Circular Economy Indicators and
Their Impact on Economic Growth of the
Sustainability 11 (19)
European Union.
(2019). https://doi.org/https://doi.org/10.
3390/su11195481 .

[2] Botev, J., ´Egert, B. & Jawadi, F. The
relationship between economic
nonlinear
growth and ﬁnancial development: Evidence
from developing, emerging and advanced
International Economics 160,
economies.
3–13 (2019).
https://doi.org/https://doi.
org/10.1016/j.inteco.2019.06.004 .

[3] Liu, B., Zhao, Q., Jin, Y., Shen, J. & Li,
C. Application of combined model of step-
wise regression analysis and artiﬁcial neural
network in data calibration of miniature air
quality detector. Scientiﬁc Reports 11 (1),
https://doi.org/https://doi.
1–12 (2021).
org/10.1038/s41598-021-82871-4 .

[4] Gu, K., Qiao, J. & Lin, W. Recurrent Air
Quality Predictor Based on Meteorology-

16

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

Table 2: State-of-the-art on hybrid methods using fuzzy logic systems and conventional deep models for
regression problems. XAI: explainable artiﬁcial intelligence; Disc.: discussion by the authors (Yes/No);
Und.: how understandable is the model, whether it is transparent (T) or opaque (O); Scope: in post-hoc
scope, if the model promotes local explanations (L), global explanations (G) or visual explanations (V).

Reference Approach

Problem

XAI

Disc. Und.

Scope

[190]

[193]

[194]

[196]

[197]

[199]

[201]

[202]

[203]

[204]

[205]

[206]

Dolphin
and Deep

Sparse Deep Belief Network with
FNN
Fuzzy denoising autoencoder with
a compact parameter strategy and
an adaptive backpropagation algo-
rithm
Chronological
Echolocation-Fuzzy
Belief Network
Fuzzy
with Stacked Autoencoder
Fuzzy seasonal
memory network
Long Short-Term Memory model
hybridized with rough and fuzzy
set theory
T-S fuzzy model with fuzzy c-
means and Deep Belief Network

long short-term

information

granulation

Deep Type-2 Fuzzy Logic System,
trained as Stacked Autoencoder
Diﬀerential Evolution based Fuzzy
Echo State Network
T-S fuzzy model with Echo State
Network
Fuzzy Cognitive Maps with Sparse
Autoencoders
Fuzzy rule reduction with Stacked
Sparse Autoencoders

Nonlinear system modeling

Prediction of multi-input single-
output systems

Traﬃc ﬂow prediction in intelligent
transportation system

Traﬃc ﬂow prediction

Wind power forecasting

Wind speed forecasting

Forecasting of photovoltaic power
generation

Benchmark system prediction and
classiﬁcation
Time-series forecasting

Benchmark system prediction

Time-series forecasting

Time-series prediction and classiﬁ-
cation

No

No

No

No

No

No

No

Yes

No

No

No

No

T

O

O

T

O

O

T

T

O

T

T

O

-

-

-

-

-

-

-

G,V

-

-

-

-

and Pollution-Related Factors. IEEE Trans-
actions on Industrial Informatics 14 (9),
3946–3955 (2018). https://doi.org/https://
doi.org/10.1109/TII.2018.2793950 .

[5] Zhu, J. et al. Prevalence and inﬂuencing
factors of anxiety and depression symptoms
in the ﬁrst-line medical staﬀ ﬁghting against
COVID-19 in Gansu. Frontiers in psychiatry
11, 386 (2020). https://doi.org/https://doi.
org/10.3389/fpsyt.2020.00386 .

[6] Shi, B. et al. Nonlinear heart rate variabil-
ity biomarkers for gastric cancer severity:
A pilot study. Scientiﬁc reports 9 (1), 1–9
(2019). https://doi.org/https://doi.org/10.

1038/s41598-019-50358-y .

[7] Orlandi, M., Escudero-Casao, M. & Licini,
G. Nucleophilicity Prediction via Multivari-
ate Linear Regression Analysis. The Journal
of Organic Chemistry 86 (4), 3555–3564
(2021). https://doi.org/https://doi.org/10.
1021/acs.joc.0c02952 .

[8] Yang, Y. et al. A support vector regres-
sion model
to predict nitrate-nitrogen
isotopic composition using hydro-chemical
variables. Journal of Environmental Man-
agement 290, 112674 (2021).
https://
doi.org/https://doi.org/10.1016/j.jenvman.
2021.112674 .

Survey on Deep Fuzzy Systems

17

Springer Nature 2021 LATEX template

[9] Souza, F., Mendes, J. & Ara´ujo, R. A
Regularized Mixture of Linear Experts for
Quality Prediction in Multimode and Multi-
phase Industrial Processes. Applied Sciences
11 (5) (2021). https://doi.org/https://doi.
org/10.3390/app11052040 .

[10] Liu, H., Yang, C., Carlsson, B., Qin, S. J.
& Yoo, C. Dynamic Nonlinear Partial
Least Squares Modeling Using Gaussian
Process Regression. Industrial & Engineer-
ing Chemistry Research 58 (36), 16676–
16686 (2019). https://doi.org/https://doi.
org/10.1021/acs.iecr.9b00701 .

[11] Sun, Q. & Ge, Z. A Survey on Deep Learning
for Data-Driven Soft Sensors. IEEE Trans-
actions on Industrial Informatics 17 (9),
5853–5866 (2021). https://doi.org/https://
doi.org/10.1109/TII.2021.3053128 .

[12] Angelov, P. & Soares, E. Towards explain-
able deep neural networks (xDNN). Neural
Networks 130, 185–194 (2020). https://doi.
org/https://doi.org/10.1016/j.neunet.2020.
07.010 .

[13] Sj¨oberg, J. et al.

Nonlinear black-box
modeling in system identiﬁcation: a uniﬁed
overview. Automatica 31 (12), 1691–1724
(1995). https://doi.org/https://doi.org/10.
1016/0005-1098(95)00120-8 .

[14] Han, Z., Zhao, J., Leung, H., Ma, K. F.
& Wang, W. A Review of Deep Learn-
ing Models for Time Series Prediction.
IEEE Sensors Journal 21 (6), 7833–7848
(2021). https://doi.org/https://doi.org/10.
1109/JSEN.2019.2923982 .

[15] Torres, J. F., Hadjout, D., Sebaa, A.,
Mart´ınez- ´Alvarez, F. & Troncoso, A. Deep
Learning for Time Series Forecasting:
Big Data 9 (1), 3–21
A Survey.
(2021). https://doi.org/https://doi.org/10.
1089/big.2020.0159 .

[17] Chalapathy, R. & Chawla, S.

Deep
learning for anomaly detection: A sur-
vey.
arXiv preprint arXiv:1901.03407
(2019). https://doi.org/https://doi.org/10.
48550/arXiv.1901.03407 .

[18] Dong, S., Wang, P. & Abbas, K.

A
survey on deep learning and its applica-
tions. Computer Science Review 40, 100379
(2021). https://doi.org/https://doi.org/10.
1016/j.cosrev.2021.100379 .

[19] Sengupta, S. et al.

A review of deep
learning with special emphasis on archi-
tectures, applications and recent trends.
Knowledge-Based Systems 194, 105596
(2020). https://doi.org/https://doi.org/10.
1016/j.knosys.2020.105596 .

[20] Pouyanfar, S.

et al.

A Survey on
Deep Learning: Algorithms, Techniques,
and Applications. ACM Computing Surveys
51 (5) (2018). https://doi.org/https://doi.
org/10.1145/3234150 .

[21] Navamani, T.

in Chapter 7 - Eﬃcient
Deep Learning Approaches for Health Infor-
matics (ed.Sangaiah, A. K.) Deep Learning
and Parallel Computing Environment for
Bioengineering Systems 123–137 (Academic
Press, 2019).

[22] Angelov, P. P., Soares, E. A., Jiang, R.,
Arnold, N. I. & Atkinson, P. M. Explainable
artiﬁcial intelligence: an analytical review.
WIREs Data Mining and Knowledge Dis-
covery (2021). https://doi.org/https://doi.
org/10.1002/widm.1424 .

[23] Confalonieri, R., Coba, L., Wagner, B.
& Besold, T. R. A historical perspec-
tive of explainable Artiﬁcial Intelligence.
Wiley Interdisciplinary Reviews: Data Min-
ing and Knowledge Discovery 11 (1)
(2021). https://doi.org/https://doi.org/10.
1002/widm.1391 .

[16] Pang, G., Shen, C., Cao, L. & Hengel, A.
V. D. Deep Learning for Anomaly Detec-
tion: A Review. ACM Computing Surveys
54 (2) (2021). https://doi.org/https://doi.
org/10.1145/3439950 .

[24] Phillips, P. J. et al. Four Principles of
Explainable Artiﬁcial Intelligence (2021).

[25] Chimatapu, R., Hagras, H., Starkey, A. &
Owusu, G. Explainable AI and Fuzzy Logic

18

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

Systems, 3–20 (2018).

[26]  Lapa, K., Cpa lka, K. & Rutkowski, L. New
Aspects of Interpretability of Fuzzy Systems
for Nonlinear Modeling, 225–264 (2017).

[27] Mendes, J., Maia, R., Ara´ujo, R. & Souza,
F. A. A.
Self-Evolving Fuzzy Controller
Composed of Univariate Fuzzy Control
Rules. Applied Sciences 10 (17), 5836
(2020). https://doi.org/https://doi.org/10.
3390/app10175836 .

[28] Moral, J. M. A., Castiello, C., Magdalena,
L. & Mencar, C. Explainable Fuzzy Systems
(Springer International Publishing, 2021).

[29] Das, R., Sen, S. & Maulik, U. A Survey on
Fuzzy Deep Neural Networks. ACM Com-
puting Surveys 53 (3) (2020). https://doi.
org/https://doi.org/10.1145/3369798 .

[30] Zheng, Y., Xu, Z. & Wang, X. The fusion
of deep learning and fuzzy systems: A
state-of-the-art survey. IEEE Transactions
on Fuzzy Systems 1–1 (2021). https://doi.
org/https://doi.org/10.1109/TFUZZ.2021.
3062899 .

[31] Zadeh, L. A.

Fuzzy sets.

Infor-
mation and Control 8 (3), 338 – 353
(1965). https://doi.org/https://doi.org/10.
1016/s0019-9958(65)90241-x .

[32] Wang, L.-X. A Course in Fuzzy Systems and

Control (Prentice-Hall, Inc., 1997).

[33] Mendes, J., Ara´ujo, R., Sousa, P., Ap´ostolo,
F. & Alves, L. An architecture for adaptive
fuzzy control
in industrial environments.
Computers in Industry 62 (3), 364–373
(2011). https://doi.org/https://doi.org/10.
1016/j.compind.2010.11.001 .

[34] Mamdani, E. & Assilian, S. An experi-
ment in linguistic synthesis with a fuzzy
logic controller.
International Journal
of Man-Machine Studies 7 (1), 1–13
(1975). https://doi.org/https://doi.org/10.
1016/S0020-7373(75)80002-2 .

[35] Takagi, T. & Sugeno, M. Fuzzy identi-
ﬁcation of systems and its applications
to modeling and control.
IEEE Transac-
tions on Systems, Man, and Cybernetics
SMC-15 (1), 116–132 (1985). https://doi.
org/https://doi.org/10.1109/TSMC.1985.
6313399 .

[36] Angelov, P. & Yager, R.

(ed.) Sim-
pliﬁed fuzzy rule-based systems using non-
parametric antecedents and relative data
density.
(ed.) 2011 IEEE Workshop on
Evolving and Adaptive Intelligent Systems
(EAIS), 62–69 (2011).

[37] Qiu, J., Gao, H. & Ding, S. X. Recent
Advances on Fuzzy-Model-Based Nonlin-
ear Networked Control Systems: A Survey.
IEEE Transactions on Industrial Electron-
ics 63 (2), 1207–1217 (2016).
https://
doi.org/https://doi.org/10.1109/TIE.2015.
2504351 .

[38] Ying, H.

(ed.) General MISO Takagi-
Sugeno Fuzzy Systems with Simpliﬁed Lin-
ear Rule Consequent as Universal Approx-
imators for Control and Modeling Applica-
tions. (ed.) IEEE International Conference
on Systems, Man, and Cybernetics. Compu-
tational Cybernetics and Simulation, Vol. 2,
1335–1340 (1997).

[39] J´unior, J. S. S., Mendes, J., Ara´ujo, R.,
Paulo, J. R. & Premebida, C.
(ed.)
Novelty Detection for Iterative Learning of
(ed.) 2021 IEEE
MIMO Fuzzy Systems.
19th International Conference on Industrial
Informatics (INDIN), 1–7 (2021).

[40] Hall, P. & Gill, N. An introduction to
machine learning interpretability (O’Reilly
Media, Incorporated, 2019).

[41] Shortliﬀe, E. H., Axline, S. G., Buchanan,
B. G., Merigan, T. C. & Cohen, S. N.
An Artiﬁcial Intelligence program to advise
physicians regarding antimicrobial therapy.
Computers and Biomedical Research 6 (6),
544–560 (1973).
https://doi.org/https://
doi.org/10.1016/0010-4809(73)90029-3 .

Survey on Deep Fuzzy Systems

19

Springer Nature 2021 LATEX template

[42] Clancey, W. J. Tutoring rules for guiding a
case method dialogue. International Jour-
nal of Man-Machine Studies 11 (1), 25–49
(1979). https://doi.org/https://doi.org/10.
1016/S0020-7373(79)80004-8 .

[43] Weiss, S. M., Kulikowski, C. A., Amarel,
S. & Saﬁr, A. A model-based method for
computer-aided medical decision-making.
Intelligence 11 (1), 145–172
Artiﬁcial
(1978). https://doi.org/https://doi.org/10.
1016/0004-3702(78)90015-2 .

[44] Suwa, M., Scott, A. C. & Shortliﬀe,
An approach to verifying com-
E. H.
pleteness and consistency in a rule-based
expert system. Ai Magazine 3 (4), 16–16
(1982). https://doi.org/https://doi.org/10.
1609/aimag.v3i4.377 .

[45] Swartout, W. R. XPLAIN: a system for
creating and explaining expert consulting
programs. Artiﬁcial Intelligence 21 (3),
285–325 (1983).
https://doi.org/https://
doi.org/10.1016/S0004-3702(83)80014-9 .

[46] Swartout, W. R. Explaining and Justify-
ing Expert Consulting Programs, 254–271
(1985).

[47] Rudin, C.

Stop explaining black box
machine learning models for high stakes
decisions and use
interpretable models
instead. Nature Machine Intelligence 1 (5),
206–215 (2019).
https://doi.org/https://
doi.org/10.1038/s42256-019-0048-x .

[48] Bengio, Y., Courville, A. & Vincent, P. Rep-
resentation Learning: A Review and New
Perspectives. IEEE Transactions on Pattern
Analysis and Machine Intelligence 35 (8),
1798–1828 (2013). https://doi.org/https://
doi.org/10.1109/TPAMI.2013.50 .

[49] Goodfellow, I., Bengio, Y., Courville, A.
& Bengio, Y. Deep learning (MIT press
Cambridge, 2016).

[50] Fukushima, K. Neocognitron: A Self Orga-
nizing Neural Network Model for a Mecha-
nism of Pattern Recognition Unaﬀected by
Shift in Position. Biological Cybernetics

36 (4), 193–202 (1980). https://doi.org/
https://doi.org/10.1007/bf00344251 .

[51] Hubel, D. H. & Wiesel, T. N. Receptive
ﬁelds and functional architecture of monkey
striate cortex. The Journal of Physiol-
ogy 195 (1), 215–243 (1968). https://doi.
org/https://doi.org/10.1113/jphysiol.1968.
sp008455 .

[52] Gu, J. et al. Recent advances in convolu-
tional neural networks. Pattern Recognition
77, 354–377 (2018).
https://doi.org/
https://doi.org/10.1016/j.patcog.2017.10.
013 .

[53] Li, Z., Liu, F., Yang, W., Peng, S. &
Zhou, J. A Survey of Convolutional Neu-
ral Networks: Analysis, Applications, and
Prospects.
IEEE Transactions on Neu-
ral Networks and Learning Systems 1–21
(2021). https://doi.org/https://doi.org/10.
1109/TNNLS.2021.3084827 .

[54] Ketkar, N. Convolutional Neural Networks,

63–78 (2017).

[55] Mallat, S. Understanding deep convolu-
tional networks. Philosophical Transactions
of the Royal Society A: Mathematical, Phys-
ical and Engineering Sciences 374 (2065),
20150203 (2016). https://doi.org/https://
doi.org/10.1098/rsta.2015.0203 .

[56] Wu, S.

Spatiotemporal Dynamic Fore-
casting and Analysis of Regional Traf-
ﬁc Flow in Urban Road Networks Using
Deep Learning Convolutional Neural Net-
work.
IEEE Transactions on Intelligent
Transportation Systems 23 (2), 1607–1615
(2022). https://doi.org/https://doi.org/10.
1109/TITS.2021.3098461 .

[57] Zhang, Y., Zhou, Y., Lu, H. & Fujita, H.
Traﬃc Network Flow Prediction Using Par-
allel Training for Deep Convolutional Neural
Networks on Spark Cloud.
IEEE Trans-
actions on Industrial Informatics 16 (12),
7369–7380 (2020). https://doi.org/https://
doi.org/10.1109/TII.2020.2976053 .

20

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

[58] Mukhtar, M. et al.

Development and
Comparison of Two Novel Hybrid Neural
Network Models for Hourly Solar Radia-
tion Prediction. Applied Sciences 12 (3)
(2022). https://doi.org/https://doi.org/10.
3390/app12031435 .

[59] Heo, J., Song, K., Han, S. & Lee, D.-
E.
Multi-channel convolutional neural
network for integration of meteorological
and geographical features in solar power
forecasting. Applied Energy 295, 117083
(2021). https://doi.org/https://doi.org/10.
1016/j.apenergy.2021.117083 .

[60] Liu, T. et al.

Enhancing Wind Tur-
bine Power Forecast via Convolutional
Electronics 10 (3)
Neural Network.
(2021). https://doi.org/https://doi.org/10.
3390/electronics10030261 .

[61] Wan, R., Mei, S., Wang, J., Liu, M.
& Yang, F. Multivariate temporal con-
volutional network: A deep neural net-
approach for multivariate
works
time
Electronics 8 (8)
series
forecasting.
(2019). https://doi.org/https://doi.org/10.
3390/electronics8080876 .

[62] Gao, Z. et al. Multitask-Based Temporal-
Channelwise CNN for Parameter Prediction
IEEE Transactions
of Two-Phase Flows.
on Industrial Informatics 17 (9), 6329–6336
(2021). https://doi.org/https://doi.org/10.
1109/TII.2020.2978944 .

[63] FAN, W. & ZHANG, Z. (ed.) A CNN-SVR
Hybrid Prediction Model
for Wastewater
Index Measurement. (ed.) 2020 2nd Inter-
national Conference on Advances in Com-
puter Technology, Information Science and
Communications (CTISC), 90–94 (2020).

[64] Yuan, X. et al.

Soft sensor model

for
dynamic processes based on multichannel
convolutional neural network. Chemomet-
rics and Intelligent Laboratory Systems 203,
104050 (2020). https://doi.org/https://doi.
org/10.1016/j.chemolab.2020.104050 .

for

Model
Intelligent Load Forecasting.
IEEE Transactions on Industrial Informat-
ics 17 (12), 8243–8253 (2021).
https://
doi.org/https://doi.org/10.1109/TII.2021.
3065718 .

Imani, M. & Moghad-
[66] Eskandari, H.,
Convolutional and recur-
dam, M. P.
for
rent neural network based model
short-term load forecasting.
Electric
Power Systems Research 195, 107173
(2021). https://doi.org/https://doi.org/10.
1016/j.epsr.2021.107173 .

[67] Zahid, M. et al.

Electricity price and
load forecasting using enhanced convolu-
tional neural network and enhanced support
vector regression in smart grids. Electron-
ics 8 (2) (2019). https://doi.org/https://
doi.org/10.3390/electronics8020122 .

[68] Koprinska, I., Wu, D. & Wang, Z.

(ed.)
Convolutional Neural Networks for Energy
Time Series Forecasting. (ed.) 2018 Interna-
tional Joint Conference on Neural Networks
(IJCNN), 1–8 (2018).

[69] Tian, C., Ma, J., Zhang, C. & Zhan, P. A
deep neural network model for short-term
load forecast based on long short-term mem-
ory network and convolutional neural net-
work. Energies 11 (12) (2018). https://doi.
org/https://doi.org/10.3390/en11123493 .

[70] Gao, P., Zhang, J., Sun, Y. & Yu, J.
Accurate predictions of aqueous solubil-
ity of drug molecules via the multilevel
graph convolutional network (MGCN) and
SchNet architectures. Physical Chemistry
Chemical Physics 22 (41), 23766–23772
(2020). https://doi.org/https://doi.org/10.
1039/D0CP03596C .

[71] Wu, K. & Wei, G.-W. Comparison of multi-
task convolutional neural network (MT-
CNN) and a few other methods for toxicity
prediction. arXiv preprint arXiv:1703.10951
(2017). https://doi.org/https://doi.org/10.
48550/arxiv.1703.10951 .

[65] Jalali, S. M. J. et al. A Novel Evolutionary-
Based Deep Convolutional Neural Network

[72] Witten, I. H., Frank, E., Hall, M. A. & Pal,
in Chapter 10 - Deep learning (eds

C. J.

Survey on Deep Fuzzy Systems

21

Springer Nature 2021 LATEX template

Witten, I. H., Frank, E., Hall, M. A. & Pal,
C. J.) Data Mining (Fourth Edition) 417–
466 (Morgan Kaufmann, 2017).

[73] Hinton, G. E., Osindero, S. & Teh, Y.-W.
A fast learning algorithm for deep belief
nets. Neural Computation 18 (7), 1527–1554
(2006). https://doi.org/https://doi.org/10.
1162/neco.2006.18.7.1527 .

1016/j.measurement.2018.10.020 .

[80] Wang, G., Jia, Q.-S., Zhou, M., Bi, J. &
Qiao, J. Soft-sensing of Wastewater Treat-
ment Process via Deep Belief Network with
Event-triggered Learning. Neurocomput-
ing 436, 103–113 (2021). https://doi.org/
https://doi.org/10.1016/j.neucom.2020.12.
108 .

[74] Wang, Y. et al.

An Ensemble Deep
Belief Network Model Based on Random
Subspace for NOx Concentration Predic-
tion. ACS Omega 6 (11), 7655—-7668
(2021). https://doi.org/https://doi.org/10.
1021/acsomega.0c06317 .

[81] Lian, P., Liu, H., Wang, X. & Guo, R. Soft
sensor based on DBN-IPSO-SVR approach
for rotor thermal deformation prediction of
rotary air-preheater. Measurement 165,
108109 (2020). https://doi.org/https://doi.
org/10.1016/j.measurement.2020.108109 .

[75] Hao, X. et al.

Prediction of nitrogen
oxide emission concentration in cement
production process: a method of deep
belief network with clustering and time
series.
Environmental Science and Pol-
lution Research 28 (24), 31689—-31703
(2021). https://doi.org/https://doi.org/10.
1007/s11356-021-12834-9 .

[76] Yuan, X., Gu, Y. & Wang, Y. Supervised
Deep Belief Network for Quality Prediction
in Industrial Processes. IEEE Transactions
on Instrumentation and Measurement 70,
1–11 (2021).
https://doi.org/https://doi.
org/10.1109/TIM.2020.3035464 .

[77] Yuan, X. et al.

FeO Content Predic-
tion for an Industrial Sintering Process
based on Supervised Deep Belief Network.
IFAC-PapersOnLine 53 (2), 11883–11888
(2020). https://doi.org/https://doi.org/10.
1016/j.ifacol.2020.12.703 .

[78] Hao, X. et al.

Energy consumption
prediction in cement calcination process:
A method of deep belief network with
Energy 207, 118256
sliding window.
(2020). https://doi.org/https://doi.org/10.
1016/j.energy.2020.118256 .

[79] Zhu, S.-B., Li, Z.-L., Zhang, S.-M., Ying-
Yu & Zhang, H.-F. Deep belief network-
based internal valve leakage rate predic-
tion approach. Measurement 133, 182–192
(2019). https://doi.org/https://doi.org/10.

[82] Tian, W., Liu, Z., Li, L., Zhang, S. & Li,
C. Identiﬁcation of abnormal conditions in
high-dimensional chemical process based on
feature selection and deep learning. Chinese
Journal of Chemical Engineering 28 (7),
1875–1883 (2020). https://doi.org/https://
doi.org/10.1016/j.cjche.2020.05.003 .

[83] Shao, H., Jiang, H., Li, X. & Liang, T.
Rolling bearing fault detection using contin-
uous deep belief network with locally linear
embedding. Computers in Industry 96, 27–
39 (2018). https://doi.org/https://doi.org/
10.1016/j.compind.2018.01.005 .

[84] Tian, J., Liu, Y., Zheng, W. & Yin,
Smog prediction based on the
L.
deep belief
- BP neural network model
(DBN-BP). Urban Climate 41, 101078
(2022). https://doi.org/https://doi.org/10.
1016/j.uclim.2021.101078 .

[85] Xu, H. & Jiang, C. Deep belief network-
based support vector regression method for
traﬃc ﬂow forecasting. Neural Comput-
ing and Applications 32 (7), 2027—-2036
(2019). https://doi.org/https://doi.org/10.
1007/s00521-019-04339-x .

[86] Xie, T., Zhang, G., Liu, H., Liu, F. & Du,
P. A Hybrid Forecasting Method for Solar
Output Power Based on Variational Mode
Decomposition, Deep Belief Networks and
Auto-Regressive Moving Average. Applied
Sciences 8 (10) (2018).
https://doi.org/

22

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

https://doi.org/10.3390/app8101901 .

[87] Li, X., Yang, L., Xue, F. & Zhou, H.

(ed.)
Time series prediction of stock price using
deep belief networks with intrinsic plastic-
(ed.) 2017 29th Chinese Control And
ity.
Decision Conference (CCDC), 1237–1242
(2017).

[88] Jia, Y., Wu, J. & Du, Y.

(ed.) Traﬃc
speed prediction using deep learning method.
(ed.) 2016 IEEE 19th International Confer-
ence on Intelligent Transportation Systems
(ITSC), 1217–1222 (2016).

[89] Qiao, J., Wang, G., Li, W. & Li, X. A deep
belief network with PLSR for nonlinear sys-
tem modeling. Neural Networks 104, 68–79
(2018). https://doi.org/https://doi.org/10.
1016/j.neunet.2017.10.006 .

[90] Qiu, X., Zhang, L., Ren, Y., Suganthan,
P. N. & Amaratunga, G.
(ed.) Ensemble
deep learning for regression and time series
forecasting.
(ed.) 2014 IEEE Symposium
on Computational Intelligence in Ensemble
Learning (CIEL), 1–6 (2014).

[91] Salakhutdinov, R. & Larochelle, H.
learning of deep Boltz-
(ed.) Eﬃcient
mann machines.
(ed.) Proceedings of
the Thirteenth International Conference
on Artiﬁcial
Intelligence and Statistics,
693–700 (2010).

[92] Schmidhuber, J. Deep learning in neural
networks: An overview. Neural Networks
61, 85–117 (2015). https://doi.org/https://
doi.org/10.1016/j.neunet.2014.09.003 .

[93] Ballard, D. H.

(ed.) Modular learning
in neural networks.
(ed.) Proceedings of
the Sixth National Conference on Artiﬁcial
Intelligence - Volume 1, Vol. 647, 279–284
(1987).

[94] Rumelhart, D. E., Hinton, G. E. & Williams,
R. J. Learning Internal Representations
by Error Propagation. Tech. Rep., Cali-
fornia Univ., San Diego, La Jolla. Inst. for
Cognitive Science (1985).

[95] Baldi, P.

(ed.) Autoencoders, Unsupervised
(ed.)
Learning, and Deep Architectures.
Proceedings of ICML Workshop on Unsuper-
vised and Transfer Learning, Vol. 27, 37–49
(2012).

[96] Liu, G., Bao, H. & Han, B.

A
Stacked Autoencoder-Based Deep Neural
Network for Achieving Gearbox Fault Diag-
nosis. Mathematical Problems in Engineer-
ing 2018, 1–10 (2018).
https://doi.org/
https://doi.org/10.1155/2018/5105709 .

[97] Sun, Q. & Ge, Z. Deep Learning for
Industrial KPI Prediction: When Ensem-
ble Learning Meets Semi-Supervised Data.
IEEE Transactions on Industrial Informat-
ics 17 (1), 260–269 (2021). https://doi.org/
https://doi.org/10.1109/TII.2020.2969709 .

[98] Sun, Q. & Ge, Z. Gated Stacked Target-
Related Autoencoder: A Novel Deep Fea-
ture Extraction and Layerwise Ensemble
Method for Industrial Soft Sensor Appli-
cation. IEEE Transactions on Cybernetics
1–12 (2020).
https://doi.org/https://doi.
org/10.1109/TCYB.2020.3010331 .

[99] Wang, X. & Liu, H. Soft sensor based on
stacked auto-encoder deep neural network
for air preheater rotor deformation predic-
tion. Advanced Engineering Informatics 36,
112–119 (2018).
https://doi.org/https://
doi.org/10.1016/j.aei.2018.03.003 .

[100] Liu, C., Wang, Y., Wang, K. & Yuan, X.
Deep learning with nonlocal and local struc-
ture preserving stacked autoencoder for soft
sensor in industrial processes. Engineering
Applications of Artiﬁcial Intelligence 104,
104341 (2021). https://doi.org/https://doi.
org/10.1016/j.engappai.2021.104341 .

[101] Yuan, X., Ou, C., Wang, Y., Yang, C. & Gui,
W. A novel semi-supervised pre-training
strategy for deep networks and its applica-
tion for quality variable prediction in indus-
trial processes. Chemical Engineering Sci-
ence 217, 115509 (2020). https://doi.org/
https://doi.org/10.1016/j.ces.2020.115509 .

Survey on Deep Fuzzy Systems

23

Springer Nature 2021 LATEX template

[102] Wang, Y., Liu, C. & Yuan, X.

Stacked
locality preserving autoencoder for feature
extraction and its application for industrial
process data modeling. Chemometrics and
Intelligent Laboratory Systems 203, 104086
(2020). https://doi.org/https://doi.org/10.
1016/j.chemolab.2020.104086 .

[103] Shi, C. et al.

Using Multiple-Feature-
Spaces-Based Deep Learning for Tool
Condition Monitoring in Ultraprecision
Manufacturing.
IEEE Transactions on
Industrial Electronics 66 (5), 3794–3803
(2019). https://doi.org/https://doi.org/10.
1109/TIE.2018.2856193 .

[104] Bose, T., Majumdar, A. & Chattopad-
hyay, T.
(ed.) Machine Load Estima-
tion Via Stacked Autoencoder Regression.
(ed.) 2018 IEEE International Conference
on Acoustics, Speech and Signal Processing
(ICASSP), 2126–2130 (2018).

[105] Zhang, X., Zou, Y. & Li, S. Enhancing
incremental deep learning for FCCU end-
point quality prediction.
Information Sci-
ences 530, 95–107 (2020). https://doi.org/
https://doi.org/10.1016/j.ins.2020.04.013 .

[106] Liu, C., Tang, L. & Liu, J. A Stacked
Autoencoder With Sparse Bayesian Regres-
sion for End-Point Prediction Problems in
Steelmaking Process.
IEEE Transactions
on Automation Science and Engineer-
ing 17 (2), 550–561 (2020). https://doi.
org/https://doi.org/10.1109/TASE.2019.
2935314 .

[107] Wei, M., Ye, M., Wang, Q., Xinxin-Xu &
Twajamahoro, J. P. Remaining useful life
prediction of lithium-ion batteries based on
stacked autoencoder and gaussian mixture
regression. Journal of Energy Storage 47,
103558 (2022). https://doi.org/https://doi.
org/10.1016/j.est.2021.103558 .

[108] Li, Z., Li, J., Wang, Y. & Wang, K.
A deep learning approach for anomaly
detection based on SAE and LSTM
in mechanical equipment.
The Inter-
national Journal of Advanced Manufac-
turing Technology 103 (1—4), 499––510

(2019). https://doi.org/https://doi.org/10.
1007/s00170-019-03557-w .

[109] Ren, L., Sun, Y., Cui, J. & Zhang, L. Bear-
ing remaining useful life prediction based on
deep autoencoder and deep neural networks.
Journal of Manufacturing Systems 48, 71–
77 (2018). https://doi.org/https://doi.org/
10.1016/j.jmsy.2018.04.008 .

[110] Li, M., Xie, X. & Zhang, D. Improved Deep
Learning Model Based on Self-Paced Learn-
ing for Multiscale Short-Term Electricity
Sustainability 14 (1)
Load Forecasting.
(2022). https://doi.org/https://doi.org/10.
3390/su14010188 .

[111] Jin, X.-B., Gong, W.-T., Kong, J.-L., Bai,
Y.-T. & Su, T.-L. PFVAE: A Planar Flow-
Based Variational Auto-Encoder Prediction
Model for Time Series Data. Mathematics
10 (4) (2022). https://doi.org/https://doi.
org/10.3390/math10040610 .

[112] Xiao, X. et al. SSAE-MLP: Stacked sparse
autoencoders-based multi-layer perceptron
for main bearing temperature prediction
of large-scale wind turbines. Concurrency
and Computation: Practice and Experience
33 (17) (2021). https://doi.org/https://doi.
org/10.1002/cpe.6315 .

[113] Lv, S.-X., Peng, L. & Wang, L. Stacked
autoencoder with echo-state regression for
tourism demand forecasting using search
query data. Applied Soft Computing 73,
https://doi.org/https://
119–133 (2018).
doi.org/10.1016/j.asoc.2018.08.024 .

[114] Jiao, R., Huang, X., Ma, X., Han, L. &
Tian, W. A Model Combining Stacked
Auto Encoder and Back Propagation Algo-
rithm for Short-Term Wind Power Fore-
IEEE Access 6, 17851–17858
casting.
(2018). https://doi.org/https://doi.org/10.
1109/ACCESS.2018.2818108 .

[115] Grossberg, S. Recurrent neural networks.
Scholarpedia 8 (2), 1888 (2013). https://doi.
org/https://doi.org/10.4249/scholarpedia.
1888 .

24

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

[116] Hopﬁeld, J. J. Neural networks and physical
systems with emergent collective computa-
tional abilities. Proceedings of the National
Academy of Sciences 79 (8), 2554–2558
(1982). https://doi.org/https://doi.org/10.
1073/pnas.79.8.2554 .

[117] Hopﬁeld, J. J. Hopﬁeld network. Scholar-
pedia 2 (5), 1977 (2007). https://doi.org/
https://doi.org/10.4249/scholarpedia.1977 .

[118] Rumelhart, D. E., Hinton, G. E. & Williams,
R. J. Learning representations by back-
propagating errors. Nature 323 (6088),
533––536 (1986). https://doi.org/https://
doi.org/10.1038/323533a0 .

[119] Zhang, S., Bamakan, S. M. H., Qu, Q.
Learning for Personalized
& Li, S.
Medicine: A Comprehensive Review From a
Deep Learning Perspective. IEEE Reviews
in Biomedical Engineering 12, 194–208
(2019). https://doi.org/https://doi.org/10.
1109/RBME.2018.2864254 .

[120] Albertini, F. & Pra, P. D.

in Recurrent neu-
ral networks: Identiﬁcation and other system
theoretic properties (ed.) Neural Network
Systems Techniques and Applications, Vol. 3
1–49 (1998).

[121] Pascanu, R., Gulcehre, C., Cho, K. &
Bengio, Y. How to Construct Deep Recur-
arXiv preprint
rent Neural Networks.
arXiv:1312.6026 (2013).
https://doi.org/
https://doi.org/10.48550/ARXIV.1312.
6026 .

[122] Theodoridis, S.

in Chapter 18 - Neural
Networks and Deep Learning (ed.) Machine
Learning (Second Edition) 901–1038 (2020).

[123] Werbos, P. Backpropagation through time:
what it does and how to do it.
Pro-
ceedings of the IEEE 78 (10), 1550–1560
(1990). https://doi.org/https://doi.org/10.
1109/5.58337 .

[124] Lalapura, V. S., Amudha, J. & Satheesh,
H. S. Recurrent Neural Networks for Edge
Intelligence: A Survey. ACM Computing
Surveys 54 (4), 1–38 (2021). https://doi.

org/https://doi.org/10.1145/3448974 .

[125] Hochreiter, S. & Schmidhuber, J. Long
Short-Term Memory. Neural Computation
9 (8), 1735–1780 (1997). https://doi.org/
https://doi.org/10.1162/neco.1997.9.8.1735
.

[126] Lu, S., Zhang, Q., Chen, G. & Seng, D.
A combined method for short-term traﬃc
ﬂow prediction based on recurrent neural
network. Alexandria Engineering Journal
60 (1), 87–94 (2021).
https://doi.org/
https://doi.org/10.1016/j.aej.2020.06.008 .

[127] Yang, B., Sun, S., Li, J., Lin, X. & Tian,
Y. Traﬃc ﬂow prediction using LSTM with
feature enhancement. Neurocomputing 332,
320–327 (2019).
https://doi.org/https://
doi.org/10.1016/j.neucom.2018.12.016 .

[128] Roy, D. K. et al. Daily Prediction and
Multi-Step Forward Forecasting of Refer-
ence Evapotranspiration Using LSTM and
Agronomy 12 (3)
Bi-LSTM Models.
(2022). https://doi.org/https://doi.org/10.
3390/agronomy12030594 .

[129] Kumari, P. & Toshniwal, D. Long short
term memory–convolutional neural network
based deep hybrid approach for solar irra-
diance forecasting. Applied Energy 295,
117061 (2021). https://doi.org/https://doi.
org/10.1016/j.apenergy.2021.117061 .

[130] Altan, A., Karasu, S. & Zio, E. A new hybrid
model for wind speed forecasting combin-
ing long short-term memory neural network,
decomposition methods and grey wolf opti-
mizer. Applied Soft Computing 100, 106996
(2021). https://doi.org/https://doi.org/10.
1016/j.asoc.2020.106996 .

[131] Ma, J. et al. Air quality prediction at
new stations using spatially transferred
bi-directional
short-term memory
network.
Science of The Total Environ-
ment 705, 135771 (2020). https://doi.org/
https://doi.org/10.1016/j.scitotenv.2019.
135771 .

long

Survey on Deep Fuzzy Systems

25

Springer Nature 2021 LATEX template

[132] Shen, Z., Zhang, Y., Lu, J., Xu, J. & Xiao,
G. A novel time series forecasting model
with deep learning. Neurocomputing 396,
302–313 (2020).
https://doi.org/https://
doi.org/10.1016/j.neucom.2018.12.084 .

[133] Zhang, Q., Wang, H., Dong, J., Zhong,
G. & Sun, X.
Prediction of Sea
Surface Temperature Using Long Short-
Term Memory.
IEEE Geoscience and
Remote Sensing Letters 14 (10), 1745–1749
(2017). https://doi.org/https://doi.org/10.
1109/LGRS.2017.2733548 .

[134] Zhu, X., Hao, K., Xie, R. & Huang,
B.
Soft sensor based on extreme gra-
dient boosting and bidirectional converted
gates long short-term memory self-attention
network. Neurocomputing 434, 126–136
(2021). https://doi.org/https://doi.org/10.
1016/j.neucom.2020.12.028 .

[135] Yuan, X., Li, L., Shardt, Y. A. W.,
Wang, Y. & Yang, C.
Deep Learn-
ing With Spatiotemporal Attention-Based
LSTM for Industrial Soft Sensor Model
Development.
IEEE Transactions on
Industrial Electronics 68 (5), 4404–4414
(2021). https://doi.org/https://doi.org/10.
1109/TIE.2020.2984443 .

[136] Yuan, X., Li, L. & Wang, Y.

Non-
linear Dynamic Soft Sensor Modeling
With Supervised Long Short-Term Mem-
ory Network.
IEEE Transactions on
Industrial Informatics 16 (5), 3168–3176
(2020). https://doi.org/https://doi.org/10.
1109/TII.2019.2902129 .

[137] Cai, W., Zhang, W., Hu, X. & Liu, Y.
A hybrid information model based on
long short-term memory network for tool
condition monitoring.
Journal of Intel-
ligent Manufacturing 31 (6), 1497––1510
(2020). https://doi.org/https://doi.org/10.
1007/s10845-019-01526-4 .

[138] Cheng, Y., Zhu, H., Wu, J. & Shao,
X. Machine Health Monitoring Using
Adaptive Kernel Spectral Clustering and
Deep Long Short-Term Memory Recur-
rent Neural Networks. IEEE Transactions

on Industrial Informatics 15 (2), 987–997
(2019). https://doi.org/https://doi.org/10.
1109/TII.2018.2866549 .

[139] Li, S., Fang, H. & Shi, B.

(ed.) Multi-
Step-Ahead Prediction with Long Short
Term Memory Networks and Support Vector
Regression. (ed.) 2018 37th Chinese Control
Conference (CCC), 8104–8109 (2018).

[140] Cheng, Y., Wang, C., Wu, J., Zhu, H. &
Lee, C. Multi-dimensional recurrent neural
network for remaining useful life prediction
under variable operating conditions and
multiple fault modes. Applied Soft Comput-
ing 118, 108507 (2022). https://doi.org/
https://doi.org/10.1016/j.asoc.2022.108507
.

[141] Guo, J., Lao, Z., Hou, M., Li, C. &
Zhang, S. Mechanical
fault time series
prediction by using EFMSAE-LSTM neu-
ral network. Measurement 173, 108566
(2021). https://doi.org/https://doi.org/10.
1016/j.measurement.2020.108566 .

[142] Moradzadeh, A., Zakeri, S., Shoaran, M.,
Mohammadi-Ivatloo, B. & Mohammadi, F.
Short-Term Load Forecasting of Microgrid
via Hybrid Support Vector Regression and
Long Short-Term Memory Algorithms. Sus-
tainability 12 (17) (2020). https://doi.org/
https://doi.org/10.3390/su12177076 .

[143] Narayan, A. & Hipel, K. W.

(ed.) Long
short term memory networks for short-term
electric load forecasting.
(ed.) 2017 IEEE
International Conference on Systems, Man,
and Cybernetics (SMC), 2573–2578 (2017).

[144] Gers, F. Long short-term memory in recur-
rent neural networks. Ph.D. thesis, ´Ecole
Polytechnique F´ed´erale de Lausanne (2001).

[145] Jaeger, H.

The “echo state” approach
to analysing and training recurrent neu-
ral networks-with an erratum note. Tech.
Rep. 34, Bonn, Germany: German National
Research Center for Information Technology
GMD Technical Report (2001).

26

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

[146] Maass, W., Natschl¨ager, T. & Markram,
H. Real-Time Computing Without Sta-
ble States: A New Framework for Neu-
ral Computation Based on Perturbations.
Neural Computation 14 (11), 2531–2560
(2002). https://doi.org/https://doi.org/10.
1162/089976602760407955 .

[147] Schrauwen, B., Verstraeten, D. & Campen-
hout, J. V.
(ed.) An overview of reservoir
computing: theory, applications and imple-
mentations.
(ed.) Proceedings of the 15th
European Symposium on Artiﬁcial Neural
Networks, 471–482 (2007).

[148] Lukoˇseviˇcius, M. & Jaeger, H. Reservoir
computing approaches to recurrent neural
network training. Computer Science Review
3 (3), 127–149 (2009).
https://doi.org/
https://doi.org/10.1016/j.cosrev.2009.03.
005 .

[149] Jaeger, H. Echo state network. Scholarpedia
2 (9), 2330 (2007). https://doi.org/https://
doi.org/10.4249/scholarpedia.2330 .

[150] Tang, Y., Deng, J., Zang, C. & Wu,
Q. Chaotic Modeling of Stream Nitrate
Concentration and Transportation via
IFPA-ESN and Turning Point Analyses.
in Environmental Science 10
Frontiers
(2022). https://doi.org/https://doi.org/10.
3389/fenvs.2022.855694 .

[151] Na, X., Ren, W. & Xu, X. Hierarchi-
cal delay-memory echo state network: A
model designed for multi-step chaotic time
series prediction.
Engineering Applica-
tions of Artiﬁcial Intelligence 102, 104229
(2021). https://doi.org/https://doi.org/10.
1016/j.engappai.2021.104229 .

[152] Yao, X., Wang, Z. & Zhang, H. Prediction
and identiﬁcation of discrete-time dynamic
nonlinear systems based on adaptive echo
state network. Neural Networks 113, 11–19
(2019). https://doi.org/https://doi.org/10.
1016/j.neunet.2019.01.003 .

[153] Xu, M., Han, M., Qiu, T. & Lin, H. Hybrid
Regularized Echo State Network for Mul-
tivariate Chaotic Time Series Prediction.

IEEE Transactions on Cybernetics 49 (6),
2305–2315 (2019). https://doi.org/https://
doi.org/10.1109/TCYB.2018.2825253 .

[154] McDermott, P. L. & Wikle, C. K. Deep echo
state networks with uncertainty quantiﬁca-
tion for spatio-temporal forecasting. Envi-
ronmetrics 30 (3) (2018). https://doi.org/
https://doi.org/10.1002/env.2553 .

[155] Lian, L. Wind speed prediction based on
CEEMD-SE and multiple echo state net-
work with Gauss–Markov fusion. Review
of Scientiﬁc Instruments 93 (1), 015105
(2022). https://doi.org/https://doi.org/10.
1063/5.0081086 .

[156] Bai, Y., Liu, M.-D., Ding, L. & Ma,
Y.-J. Double-layer staged training echo-
state networks
for wind speed predic-
tion using variational mode decompo-
Applied Energy 301, 117461
sition.
(2021). https://doi.org/https://doi.org/10.
1016/j.apenergy.2021.117461 .

[157] Hu, H., Wang, L. & Tao, R. Wind
speed forecasting based on variational mode
decomposition and improved echo state net-
Renewable Energy 164, 729–751
work.
(2021). https://doi.org/https://doi.org/10.
1016/j.renene.2020.09.109 .

[158] Tian, Z. Approach for short-term wind
power prediction via kernel principal com-
ponent analysis and echo state network opti-
mized by improved particle swarm optimiza-
tion algorithm. Transactions of the Institute
of Measurement and Control 43 (16), 3647–
https://doi.org/https://doi.
3662 (2021).
org/10.1177/01423312211046421 .

[159] L´opez, E., Valle, C., Allende, H., Gil, E.
& Madsen, H. Wind Power Forecast-
ing Based on Echo State Networks and
Long Short-Term Memory. Energies 11 (3)
(2018). https://doi.org/https://doi.org/10.
3390/en11030526 .

[160] Zhang, H. et al. Self-organizing deep belief
modular echo state network for time series
prediction. Knowledge-Based Systems 222,
107007 (2021). https://doi.org/https://doi.

Survey on Deep Fuzzy Systems

27

Springer Nature 2021 LATEX template

org/10.1016/j.knosys.2021.107007 .

[161] Li, Q., Wu, Z. & Zhang, H. Spatio-temporal
modeling with enhanced ﬂexibility and
robustness of solar irradiance prediction:
state network
A chain-structure
Journal of Cleaner Produc-
approach.
tion 261, 121151 (2020).
https://doi.
org/https://doi.org/10.1016/j.jclepro.2020.
121151 .

echo

[162] Xu, M., Han, M. & Lin, H. Wavelet-
denoising multiple echo state networks for
multivariate time series prediction.
Infor-
mation Sciences 465, 439–458 (2018).
https://doi.org/https://doi.org/10.1016/j.
ins.2018.07.015 .

[163] Sun, X., Li, T., Li, Q., Huang, Y. &
Li, Y.
Deep belief echo-state network
and its application to time series predic-
tion. Knowledge-Based Systems 130, 17–29
(2017). https://doi.org/https://doi.org/10.
1016/j.knosys.2017.05.022 .

[164] Li, Z. & Tanaka, G.

for nonlinear

Multi-reservoir
echo state networks with sequence resam-
time-series predic-
pling
Neurocomputing 467, 115–129
tion.
(2022). https://doi.org/https://doi.org/10.
1016/j.neucom.2021.08.122 .

[165] Wang, L., Su, Z., Qiao, J. & Deng, F. A
pseudo-inverse decomposition-based self-
organizing modular echo state network for
time series prediction. Applied Soft Com-
puting 116, 108317 (2022). https://doi.org/
https://doi.org/10.1016/j.asoc.2021.108317
.

[166] Gao, R., Du, L., Duru, O. & Yuen, K. F.
Time series forecasting based on echo state
network and empirical wavelet transforma-
tion. Applied Soft Computing 102, 107111
(2021). https://doi.org/https://doi.org/10.
1016/j.asoc.2021.107111 .

[167] Schwedersky, B. B., Flesch, R. C. C. &
Dangui, H. A. S. Nonlinear MIMO System
Identiﬁcation with Echo-State Networks.
Journal of Control, Automation and Elec-
https://doi.
trical Systems 1–12 (2022).

org/https://doi.org/10.1007/s40313-021-
00874-y .

[168] He, Y.-L., Tian, Y., Xu, Y. & Zhu, Q.-
X. Novel soft sensor development using
echo state network integrated with singular
value decomposition: Application to com-
plex chemical processes. Chemometrics and
Intelligent Laboratory Systems 200, 103981
(2020). https://doi.org/https://doi.org/10.
1016/j.chemolab.2020.103981 .

[169] Huang, R., Li, Z. & Cao, B. A Soft Sensor
Approach Based on an Echo State Network
Optimized by Improved Genetic Algorithm.
Sensors 20 (17) (2020). https://doi.org/
https://doi.org/10.3390/s20175000 .

[170] Mansoor, M., Grimaccia, F., Leva, S. &
Mussetta, M. Comparison of echo state
network and feed-forward neural networks
load forecasting for demand
in electrical
Mathematics and
response programs.
Computers in Simulation 184, 282–293
(2021). https://doi.org/https://doi.org/10.
1016/j.matcom.2020.07.011 .

[171] Yao, X., Wang, Z. & Zhang, H. A novel pho-
tovoltaic power forecasting model based on
echo state network. Neurocomputing 325,
182–189 (2019).
https://doi.org/https://
doi.org/10.1016/j.neucom.2018.10.022 .

[172] Wen, S. et al. Memristor-Based Echo State
Network With Online Least Mean Square.
IEEE Transactions on Systems, Man, and
Cybernetics: Systems 49 (9), 1787–1796
(2019). https://doi.org/https://doi.org/10.
1109/TSMC.2018.2825021 .

[173] Bala, A., Ismail, I., Ibrahim, R., Sait, S. M.
& Oliva, D. An Improved Grasshopper
Optimization Algorithm Based Echo State
Network for Predicting Faults in Airplane
IEEE Access 8, 159773–159789
Engines.
(2020). https://doi.org/https://doi.org/10.
1109/ACCESS.2020.3020356 .

[174] Zhong, S., Xie, X., Lin, L. & Wang,
F.
algorithm optimized
double-reservoir echo state network for

Genetic

28

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

multi-regime time series prediction. Neuro-
computing 238, 191–204 (2017). https://
doi.org/https://doi.org/10.1016/j.neucom.
2017.01.053 .

[175] Xu, Y. A Review of Machine Learning With
Echo State Networks. Project Report (2020)
.

[176] Gunning, D. et al. XAI—Explainable arti-
ﬁcial intelligence. Science Robotics 4 (37)
(2019). https://doi.org/https://doi.org/10.
1126/scirobotics.aay7120 .

[177] Huang, Y., Chen, D., Zhao, W. & Mo,
H. Deep Fuzzy System Algorithms Based
on Deep Learning and Input Sharing for
Regression Application. International Jour-
nal of Fuzzy Systems 23 (3), 727–742
(2021). https://doi.org/https://doi.org/10.
1007/s40815-020-00998-4 .

[178] Wang, L.-X. & Mendel, J.

Generat-
ing fuzzy rules by learning from exam-
ples.
IEEE Transactions on Systems,
Man, and Cybernetics 22 (6), 1414–1427
(1992). https://doi.org/https://doi.org/10.
1109/21.199466 .

[179] Peng, W., Zhou, C., Li, C., Deng,
X. & Zhang, G.
Double-Input Rule
Modules Stacked Deep Interval Type-2
Fuzzy Model with Application to Time
Series Forecasting.
International Jour-
nal of Fuzzy Systems 23 (5), 1326–1346
(2021). https://doi.org/https://doi.org/10.
1007/s40815-021-01087-w .

[180] Li, C., Zhou, C., Peng, W., Lv, Y. &
Luo, X.
Accurate prediction of short-
term photovoltaic power generation via
a novel double-input-rule-modules stacked
deep fuzzy method. Energy 212, 118700
(2020). https://doi.org/https://doi.org/10.
1016/j.energy.2020.118700 .

[182] Luo, C., Tan, C., Wang, X. & Zheng,
Y. An evolving recurrent interval type-
2 intuitionistic fuzzy neural network for
online learning and time series predic-
tion. Applied Soft Computing 78, 150–163
(2019). https://doi.org/https://doi.org/10.
1016/j.asoc.2019.02.032 .

[183] Bodyanskiy, Y., Vynokurova, O., Pliss, I.,
Peleshko, D. & Rashkevych, Y.
(ed.)
Deep Stacking Convex Neuro-Fuzzy System
and Its On-line Learning.
(ed.) DepCoS-
RELCOMEX 2017: Advances in Depend-
ability Engineering of Complex Systems, 49–
–59 (2018).

[184] Bodyanskiy, Y. V. & Tyshchenko, O. K. A
Hybrid Cascade Neuro–Fuzzy Network with
Pools of Extended Neo–Fuzzy Neurons and
its Deep Learning. International Journal of
Applied Mathematics and Computer Science
29 (3), 477––488 (2019). https://doi.org/
https://doi.org/10.2478/amcs-2019-0035 .

[185] Cao, Y. et al. Deep learned recurrent
type-3 fuzzy system: Application for renew-
able energy modeling/prediction. Energy
Reports 7, 8115–8127 (2021). https://doi.
org/https://doi.org/10.1016/j.egyr.2021.07.
004 .

[186] Wang, G. & Qiao, J.

An Eﬃcient
Self-Organizing Deep Fuzzy Neural Net-
work for Nonlinear System Modeling.
IEEE Transactions on Fuzzy Systems 1–1
(2021). https://doi.org/https://doi.org/10.
1109/TFUZZ.2021.3077396 .

J.

[187] Wang, J., Peng, Z., Wang, X., Li, C.
& Wu,
Deep Fuzzy Cognitive
Maps for Interpretable Multivariate Time
Series Prediction.
IEEE Transactions
on Fuzzy Systems 29 (9), 2647–2660
(2021). https://doi.org/https://doi.org/10.
1109/TFUZZ.2020.3005293 .

[181] Qasem, S. N. & Mohammadzadeh, A. A
deep learned type-2 fuzzy neural network:
Singular value decomposition approach.
Applied Soft Computing
107244
(2021). https://doi.org/https://doi.org/10.
1016/j.asoc.2021.107244 .

105,

[188] Park, S., Lee, S. J., Weiss, E. & Motai, Y.
Intra- and Inter-Fractional Variation Pre-
diction of Lung Tumors Using Fuzzy Deep
Learning.
IEEE Journal of Translational
Engineering in Health and Medicine 4, 1–12
(2016). https://doi.org/https://doi.org/10.

Survey on Deep Fuzzy Systems

29

Springer Nature 2021 LATEX template

1109/JTEHM.2016.2516005 .

[189] Magdalena, L. Semantic interpretability in
hierarchical fuzzy systems: Creating seman-
tically decouplable hierarchies. Information
Sciences 496, 109–123 (2019). https://doi.
org/https://doi.org/10.1016/j.ins.2019.05.
016 .

[190] Wang, G., Jia, Q.-S., Qiao, J., Bi, J.
A sparse deep belief net-
& Liu, C.
work with eﬃcient fuzzy learning frame-
Neural Networks 121, 430–440
work.
(2020). https://doi.org/https://doi.org/10.
1016/j.neunet.2019.09.035 .

[191] Wang, G., Qiao, J., Bi, J., Li, W. &
Zhou, M.
TL-GDBN: Growing Deep
Belief Network With Transfer Learning.
IEEE Transactions on Automation Sci-
ence and Engineering 16 (2), 874–885
(2019). https://doi.org/https://doi.org/10.
1109/TASE.2018.2865663 .

[192] Li, F., Qiao, J., Han, H. & Yang, C. A
self-organizing cascade neural network with
random weights for nonlinear system mod-
eling. Applied Soft Computing 42, 184–193
(2016). https://doi.org/https://doi.org/10.
1016/j.asoc.2016.01.028 .

[193] Han, H.-G., Zhang, H.-J. & Qiao, J.-F.
Robust Deep Neural Network Using Fuzzy
Denoising Autoencoder. International Jour-
nal of Fuzzy Systems 22 (4), 1356–1375
(2020). https://doi.org/https://doi.org/10.
1007/s40815-020-00845-6 .

[194] George, S. & Santra, A. K.

Fuzzy
Inspired Deep Belief Network for the Traf-
ﬁc Flow Prediction in Intelligent Trans-
portation System Using Flow Strength
Big Data 8 (4), 291—-307
Indicators.
(2020). https://doi.org/https://doi.org/10.
1089/big.2019.0007 .

[195] Borkar, G. M. & Mahajan, A. R. A secure
and trust based on-demand multipath rout-
ing scheme for self-organized mobile ad-hoc
networks. Wireless Networks 23 (8), 2455—
-2472 (2016). https://doi.org/https://doi.
org/10.1007/s11276-016-1287-y .

[196] Chen, J., Yuan, W., Cao, J. & Lv,
Traﬃc-ﬂow prediction via granu-
H.
lar computing and stacked autoencoder.
Granular Computing 5 (4), 449—-459
(2019). https://doi.org/https://doi.org/10.
1007/s41066-019-00167-5 .

[197] Liao, C.-W., Wang, I.-C., Lin, K.-P. & Lin,
Y.-J. A Fuzzy Seasonal Long Short-Term
Memory Network for Wind Power Forecast-
ing. Mathematics 9 (11) (2021). https://doi.
org/https://doi.org/10.3390/math9111178 .

[198] Chang, P.-T. Fuzzy seasonality forecast-
ing. Fuzzy Sets and Systems 90 (1), 1–10
(1997). https://doi.org/https://doi.org/10.
1016/S0165-0114(96)00138-8 .

[199] Imani, M. et al. Application of Rough
and Fuzzy Set Theory for Prediction of
Stochastic Wind Speed Data Using Long
Short-Term Memory. Atmosphere 12 (7)
(2021). https://doi.org/https://doi.org/10.
3390/atmos12070924 .

[200] Pawlak, Z. Rough sets. International Jour-
nal of Computer & Information Sciences
11 (5), 341––356 (1982). https://doi.org/
https://doi.org/10.1007/bf01001956 .

[201] Liu, L., Liu, F. & Zheng, Y. A Novel
Ultra-Short-Term PV Power Forecasting
Method Based on DBN-Based Takagi-
Sugeno Fuzzy Model. Energies 14 (20)
(2021). https://doi.org/https://doi.org/10.
3390/en14206447 .

[202] Chimatapu, R., Hagras, H., Kern, M. &
Owusu, G.
(ed.) Hybrid Deep Learning
Type-2 Fuzzy Logic Systems For Explainable
AI. (ed.) 2020 IEEE International Confer-
ence on Fuzzy Systems (FUZZ-IEEE), 1–6
(2020).

[203] S.N., D., S., G. & T.S., A.

(ed.) Fuzzy
Echo State Neural Network with Diﬀeren-
tial Evolution Framework for Time Series
Forecasting. (ed.) 2018 17th IEEE Interna-
tional Conference on Machine Learning and
Applications (ICMLA), 1322–1327 (2018).

30

Survey on Deep Fuzzy Systems

Springer Nature 2021 LATEX template

[204] Mahmoud, T. A. & Elshenawy, L. M. TSK
fuzzy echo state neural network: a hybrid
structure for black-box nonlinear systems
identiﬁcation.
Neural Computing and
Applications 1–19 (2022). https://doi.org/
https://doi.org/10.1007/s00521-021-06838-
2 .

[205] Wu, K., Liu, J., Liu, P. & Yang, S. Time
Series Prediction Using Sparse Autoen-
coder and High-Order Fuzzy Cognitive
Maps.
IEEE Transactions on Fuzzy Sys-
tems 28 (12), 3110–3121 (2020). https://
doi.org/https://doi.org/10.1109/TFUZZ.
2019.2956904 .

[206] Sevakula, R. K. & Verma, N. K. (ed.) Fuzzy
Rule Reduction using Sparse Auto-Encoders.
(ed.) 2015 IEEE International Confer-
ence on Fuzzy Systems (FUZZ-IEEE), 1–7
(2015).

[207] Ojha, V., Abraham, A. & Sn´aˇsel, V.
Heuristic design of fuzzy inference systems:
A review of three decades of research.
Engineering Applications of Artiﬁcial Intel-
ligence 85, 845–864 (2019).
https://doi.
org/https://doi.org/10.1016/j.engappai.
2019.08.010 .

