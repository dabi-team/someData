0
2
0
2

n
a
J

6

]
T
G
.
s
c
[

2
v
0
2
2
4
0
.
1
1
9
1
:
v
i
X
r
a

Non-Cooperative Inverse Reinforcement Learning

Xiangyuan Zhang

Kaiqing Zhang

Erik Miehling

Tamer Bas¸ar

Coordinated Science Laboratory
University of Illinois at Urbana-Champaign
{xz7,kzhang66,miehling,basar1}@illinois.edu

Abstract

Making decisions in the presence of a strategic opponent requires one to take into
account the opponent’s ability to actively mask its intended objective. To describe
such strategic situations, we introduce the non-cooperative inverse reinforcement
learning (N-CIRL) formalism. The N-CIRL formalism consists of two agents
with completely misaligned objectives, where only one of the agents knows the
true objective function. Formally, we model the N-CIRL formalism as a zero-sum
Markov game with one-sided incomplete information. Through interacting with
the more informed player, the less informed player attempts to both infer, and act
according to, the true objective function. As a result of the one-sided incomplete
information, the multi-stage game can be decomposed into a sequence of single-
stage games expressed by a recursive formula. Solving this recursive formula
yields the value of the N-CIRL game and the more informed player’s equilibrium
strategy. Another recursive formula, constructed by forming an auxiliary game,
termed the dual game, yields the less informed player’s strategy. Building upon
these two recursive formulas, we develop a computationally tractable algorithm
to approximately solve for the equilibrium strategies. Finally, we demonstrate the
beneﬁts of our N-CIRL formalism over the existing multi-agent IRL formalism
via extensive numerical simulation in a novel cyber security setting.

1

Introduction

In any decision-making problem, the decision-maker’s goal is characterized by some underlying,
potentially unknown, objective function. In machine learning, ensuring that the learning agent does
what the human intends it to do requires speciﬁcation of a correct objective function, a problem
known as value alignment [42, 37]. Solving this problem is nontrivial even in the simplest (single-
agent) reinforcement learning (RL) settings [3] and becomes even more challenging in multi-agent
environments [19, 45, 44]. Failing to specify a correct objective function can lead to unexpected and
potentially dangerous behavior [38].

Instead of specifying the correct objective function, a growing body of research has taken an alter-
native perspective of letting the agent learn the intended task from observed behavior of other agents
and/or human experts, a concept known as inverse reinforcement learning (IRL) [31]. IRL describes
a setting where one agent is attempting to learn the true reward function by observing trajectories of
sample behavior of another agent, termed demonstrations, under the assumption that the observed
agent is acting (optimally) according to the true reward function. More recently, the concept of co-
operative inverse reinforcement learning (CIRL) was introduced in [9]. Instead of passively learning
from demonstrations (as is the case in IRL), CIRL allows for agents to interact during the learning
process, which results in improved performance over IRL. CIRL inherently assumes that the two
agents are on the same team, that is, the expert is actively trying to help the agent learn and achieve
some common goal. However, in many practical multi-agent decision-making problems, the objec-

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
tives of the agents may be misaligned, and in some settings completely opposed [22, 21, 40, 43]
(e.g., zero-sum interactions in cyber security [2, 27]). In such settings, the expert is still trying to
achieve its objective, but may act in a way so as to make the agent think it has a different objective.
Development of a non-cooperative analogue of CIRL to describe learning in these settings has not
yet been investigated.

In this paper, we introduce the non-cooperative inverse reinforcement learning (N-CIRL) formal-
ism. The N-CIRL formalism consists of two agents with completely misaligned objectives, where
only one agent knows the true reward function. The problem is modeled as a zero-sum Markov
game with one-sided incomplete information. In particular, at any stage of the game the informa-
tion available to the player that does not know the true reward function, termed the less informed
player, is contained within the information available to the player that knows the reward function,
termed the more informed player. This one-sided information structure allows for a simpliﬁcation in
the form of the beliefs (compared to the fully general asymmetric information case [10]) and, more
importantly, allows one to deﬁne strategies as the stage-wise solutions to two recursive equations
[7, 35]. By taking advantage of the structure of these recursive equations, and their correspond-
ing solutions (ﬁxed points), computationally tractable algorithms for approximately computing the
players’ strategies are developed.

Our primary motivation for developing the N-CIRL formalism is cyber security. In particular, we
are interested in settings where the attacker has some intent that is unknown to the defender. In
reality, the motivation of attackers can vary signiﬁcantly. For example, if the attacker is ﬁnancially
motivated, its goal may be to ﬁnd personal payment details, whereas if the attacker is aiming to
disrupt the normal operation of a system, reaching a computer responsible for controlling physical
components may be of more interest. This variability in what the attacker values is captured by the
N-CIRL formalism through an intent parameter that serves to parameterize the attacker’s true reward
function. The defender, who does not know the true intent, then faces the problem of learning the
attacker’s intent while simultaneously defending the network. The attacker, knowing this, aims to
reach its goal but may behave in a way that makes its true intent as unclear as possible.1

Throughout the remainder of the paper, we will refer to the more informed player as the attacker
and the less informed player as the defender. While we use the cyber security example throughout
the paper, this is primarily for ease of exposition. The results presented here apply to any zero-sum
Markov game setting where one player does not know the true reward.

Limitations of Existing IRL Approaches. The application of N-CIRL to cyber security is espe-
cially ﬁtting due to the challenges associated with collecting useful attack data. Obtaining accurate
attack logs is a computationally formidable task [16]. Furthermore, learning from sample equilib-
rium behavior, as is done in the multi-agent inverse reinforcement learning (MA-IRL) settings of
[21, 41, 20], is only useful if the goal(s) do not change between learning and execution/deployment.
Such an assumption is not appropriate in cyber security settings – the attacker’s goal as well as the
overall system structure, may frequently change. This non-stationary behavior necessitates the abil-
ity to intertwine learning and execution. N-CIRL provides a formalism for specifying actions that
adapt to the information revealed during the game. We illustrate this adaptivity via an illustrative
example in Sec. A and extensive numerical results in Sec. 5.

Contribution. The contribution of the present work is three-fold: 1) We propose a new formalism
for IRL, termed N-CIRL, that describes how two competing agents make strategic decisions when
only one of the agents possesses knowledge of the true reward; 2) By recognizing that N-CIRL is a
zero-sum Markov game with one-sided incomplete information, we leverage the recursive structure
of the game to develop a computationally tractable algorithm, termed non-cooperative point-based
value iteration (NC-PBVI), for computing both players’ strategies; 3) We demonstrate in a novel cy-
ber security model that the adaptive strategies obtained from N-CIRL outperform strategies obtained
from existing multi-agent IRL techniques.

2 Related Work

Decision-making when the agents are uncertain of the true objective function(s) has been extensively
studied within the ﬁelds of both RL and game theory. One standard and popular way to infer the

1An interesting real-world example of such behavior was Operation Fortitude in World War II [12].

2

actual reward function is via inverse RL, the idea of which was ﬁrst introduced by [17] under the title
of inverse optimal control. Later, [31] introduced the notion of IRL with the goal of inferring the
reward function being optimized by observing the behavior of an actor, termed an expert, over time
[31, 1, 33, 8]. Fundamental to the IRL setting is the assumption that the agent inferring the reward
passively observes the expert’s behavior, while the expert behaves optimally in its own interest
without knowing that the agent will later use the observed behavior to learn.

As pointed out in [9], such an assumption is not valid in certain cooperative settings where the agent
and the expert are able to interact in order to achieve some common objective. In fact, IRL-type
solutions were shown to be suboptimal and generally less effective at instilling the knowledge of
the expert to the agent [9]. As argued by [9], the value alignment problem with cooperative agents
is more appropriately viewed as an interactive decision-making process. The proposed formalism,
termed CIRL, is formulated as a two-player game of partial information with a common reward
function2. Due to the special structure of CIRL, the problem can be transformed into a partially
observable Markov decision process (POMDP), see [30, 9], allowing for single-agent RL algorithms
to be applied. Further improvements in computational efﬁciency can be achieved by exploiting the
fact that the expert expects the agent to respond optimally [23].

Inverse RL under a non-cooperative setting has not received as much attention as its cooperative
counterpart. A recent collection of work on multi-agent IRL (MA-IRL) [21, 41, 20] addresses the
problem of IRL in stochastic games with multiple (usually more than two) agents. Distinct from our
N-CIRL setting, MA-IRL aims to recover the reward function of multiple agents under the assump-
tion that the demonstrations are generated from the Nash equilibrium strategies. Moreover, in the
MA-IRL formalism, agents determine their strategies based on the inferred reward function, i.e., re-
garding the inferred reward as some ﬁxed ground truth. In contrast, under our N-CIRL setting, only
one agent is unaware of the true reward function. Furthermore, the goal of the less informed player
in N-CIRL goes beyond just inferring the true reward function; its ultimate goal is to determine an
optimal strategy against a worst-case opponent who possesses a private reward.

From a game theoretic perspective, the N-CIRL formalism can be viewed as a stochastic dynamic
game with asymmetric information, see [6, 28, 29, 4] and references therein. In particular, N-CIRL
lies within the class of games with one-sided incomplete information [35, 39, 34, 14, 13]. This type
of game allows for a simpliﬁed belief and allows the game to be decomposed into a sequence of
single-stage games [35, 13]. In particular, our N-CIRL formalism can be recognized as one of the
game settings discussed in [35], in which a dual game was formulated to solve for the less informed
player’s strategy. Our algorithm for computing the defender’s strategy is built upon this formulation,
and can be viewed as one way to approximately solve the dual game.

3 Non-Cooperative Inverse Reinforcement Learning

In this section, we introduce the N-CIRL formalism and describe its information structure. As will
be shown, the information structure of N-CIRL admits compact information states for each player.

3.1 N-CIRL Formulation

The N-CIRL formalism is modeled as a two-player zero-sum Markov game with one-sided incom-
plete information. In particular, the attacker knows the true reward function, while the defender does
not. In the context of the cyber security setting of this paper, the reward function is assumed to be
parameterized by an intent parameter that is only known to the attacker. The N-CIRL formalism is
described by the tuple (cid:104)S, {A, D}, T (· | ·, ·, ·), {Θ, R(·, ·, ·, ·; ·)}, P0(·, ·), γ(cid:105), where

• S is the ﬁnite set of states; s ∈ S.
• A is the ﬁnite set of actions for the attacker A; a ∈ A.
• D is the ﬁnite set of actions for the defender D; d ∈ D.
• T (s(cid:48) | s, a, d) is the conditional distribution of the next state s(cid:48) given current state s and

actions a, d.

• Θ is the ﬁnite set of intent parameters that parameterize the reward function; the true intent

parameter θ ∈ Θ is only observed by the attacker.

2Also known as a team problem [24].

3

• R(s, a, d, s(cid:48); θ) is the parameterized reward function that maps the current state s ∈ S,
actions (a, d) ∈ A × D, next state s(cid:48) ∈ S, and parameter θ ∈ Θ to a reward for the attacker.
• P0 is the distribution over the initial state s0 and the true reward parameter θ, assumed to

be common knowledge between A and D.

• γ ∈ [0, 1) is the discount factor.

The game proceeds as follows. Initially, a state-parameter pair (s0, θ) is sampled from the prior
distribution P0. The state s0 is publicly observed by both players, whereas the intent parameter θ
is only observed by the attacker.3 For each stage, the attacker and the defender act simultaneously,
choosing actions a ∈ A and d ∈ D. Note that the action sets may be state-dependent, i.e., a ∈ A(s),
d ∈ D(s). Given both actions, the current state s transitions to a successor state s(cid:48) according
to the transition model T (s(cid:48) | s, a, d). The attacker receives a bounded reward R(s, a, d, s(cid:48); θ);
the defender receives the reward −R(s, a, d, s(cid:48); θ) (incurs a cost R(s, a, d, s(cid:48); θ)). Neither player
observes rewards during the game. Before each subsequent stage, both players are informed of the
successor state s(cid:48) and the actions from the previous stage. While both players are aware of the
current state, only the attacker is aware of the true intent parameter θ ∈ Θ. This results in the
defender possessing incomplete information, requiring it to maintain a belief over the true intent
parameter. The goals of the attacker and the defender are to maximize and minimize the expected
γ-discounted accumulated reward induced by R, respectively.

3.2 The Information Structure of N-CIRL

The N-CIRL formalism falls within the class of partially observable stochastic games [10]. In such
games, perfect recall ensures that behavioral strategies, i.e., strategies that mix over actions, are
outcome equivalent to mixed strategies, i.e., strategies that mix over pure strategies [18]. As a
result, players in N-CIRL can restrict attention to behavioral strategies, deﬁned for each stage t as
πA
t ) represents the space of information
t
available to the attacker (resp. defender) at stage t and ∆(A), ∆(D) represent distributions over
actions. Given any realized information sequences I A

t → ∆(D), where I A

t → ∆(A) and πD
t

t (resp. I D

: I D

: I A

t = (cid:0)s0, θ, a0, d0, . . . , at−1, dt−1, st
I A

(cid:1),

t and I D

t ∈ I D
t ∈ I A
t , represented as
(cid:1)
t = (cid:0)s0, a0, d0, . . . , at−1, dt−1, st
I D

(cid:1) ∈ It = I A
t .

the defender’s information is always contained within the attacker’s information for any stage t,
i.e., there is one-sided incomplete information. Furthermore, note that the attacker has complete
information (it knows everything that has happened in the game); its information at stage t is the full
history of the game at t, denoted by It = (cid:0)s0, θ, a0, d0, . . . , at−1, dt−1, st
Information States. In general, games of incomplete information require players to reason over the
entire belief hierarchy, that is, players’ decisions not only depend on their beliefs on the state of na-
ture, but also on the beliefs on others’ beliefs on the state and nature, and so on [25, 10]. Fortunately,
players do not need to resort to this inﬁnite regress in games of one-sided incomplete information.
Instead, each player is able to maintain a compact state of knowledge, termed an information state,
that is sufﬁcient for making optimal decisions. The more informed player maintains a pair con-
sisting of the observable state and a distribution over the private state [35, 39]. The less informed
player, through construction of a dual game (discussed in Sec. 4.2), maintains a pair consisting of
the observable state and a vector (in Euclidean space) of size equal to the number of private states
[7, 35]. In the context of N-CIRL, the attacker’s information state at each stage is a pair, denoted
by (s, b), in the space S × ∆(Θ) whereas the defender’s information state at each stage (of the dual
game) is a pair, denoted by (s, ζ), in the space S × R|Θ|.

4 Solving N-CIRL

The theoretical results used to solve the CIRL problem [30, 9] do not extend to the N-CIRL setting.
As outlined in [9], the form of CIRL allows one to convert the problem into a centralized control
problem [30].The problem can then be solved using existing techniques from reinforcement learning.

In N-CIRL, such a conversion to a centralized control problem is not possible; one is instead faced
with a dynamic game. As we show in this section, the one-sided incomplete information allows

3The intent parameter θ is further assumed to be ﬁxed throughout the problem.

4

one to recursively deﬁne both the value of the game and the attacker’s strategy. One can further
recursively deﬁne the defender’s strategy via the construction and sequential decomposition of a
dual game. The two recursive formulas permit the development of a computational procedure, based
on linear programming, for approximating both players’ strategies.

4.1 Sequential Decomposition

Solving a game involves ﬁnding strategies for all players, termed a strategy proﬁle, such that the
resulting interaction is in (Nash) equilibrium. A strategy proﬁle for N-CIRL is deﬁned as follows.
Deﬁnition 4.1 (Strategy Proﬁle). A strategy proﬁle, denoted by (σA, σD), is a pair of strategies
σA = (πA
t , . . .), where πA
t are behavioral strategies as
deﬁned in Sec. 3.2.

1 , . . .) and σD = (πD

t and πD

0 , πD

0 , πA

A simpliﬁcation of behavioral strategies are strategies that only depend on the most recent informa-
tion rather than the entire history. These strategies, termed one-stage strategies, are deﬁned below.
Deﬁnition 4.2 (One-Stage Strategies). The one-stage strategies of the attacker and defender are
denoted by ¯πA : S × Θ → ∆(A) and ¯πD : S → ∆(D), respectively. The pair (¯πA, ¯πD) is termed
a one-stage strategy proﬁle.

Due to the information structure of N-CIRL, the attacker’s information state (s, b) can be updated
using one-stage strategies instead of the full strategy proﬁle. In fact, as illustrated by Lemma 1, the
update of the attacker’s information state only depends on the attacker’s one-stage strategy ¯πA, the
attacker’s action a, and the successor state s(cid:48).
Lemma 1 (Information State Update). Given the attacker’s one-stage strategy proﬁle ¯πA, the cur-
rent attacker’s information state (s, b) ∈ S × ∆(Θ), the attacker’s action a ∈ A, and the successor
state s(cid:48) ∈ S, the attacker’s updated information state is (s(cid:48), b(cid:48)) ∈ S × ∆(Θ) where the posterior b(cid:48)
is computed via the function τ : S × ∆(Θ) × A → ∆(Θ), deﬁned elementwise as

b(cid:48)(ϑ) = τϑ(s, b, a) =

¯πA(a | s, ϑ)b(ϑ)

(cid:80)
ϑ(cid:48)∈Θ

¯πA(a | s, ϑ(cid:48))b(ϑ(cid:48))

.

(1)

The attacker’s information state (s, b) also leads to the following deﬁnition of the value function
v : S × ∆(Θ) → R of the game

v(s, b) = max
σA

min
σD

(cid:20) (cid:88)
E

t≥0

γtR(st, at, dt, st+1; θ)

(cid:21)
(cid:12)
(cid:12)
(cid:12) s0 = s, θ ∼ b(·)

,

which denotes the minimax accumulated reward if the initial state is s0 = s, and the belief over Θ is
b. Note that the value exists as all spaces in the game are ﬁnite and the discount factor lies in [0, 1)
[35]. The value function v can be computed recursively via a sequential decomposition. In fact, it is
given by the ﬁxed point of a value backup operator [Gv](s, b), as illustrated in Proposition 1 below.
To differentiate from the dual game to be introduced in Sec. 4.2, we refer to the original N-CIRL
game as the primal game and [Gv](s, b) as the primal backup operator.
Proposition 1 (Sequential Decomposition of Primal Game). The primal game can be sequentially
decomposed into a sequence of single-stage games. Speciﬁcally, the primal value function v satisﬁes
the following recursive formula

v(s, b) = [Gv](s, b) = max
¯πA

min
¯πD

(cid:8)g¯πA,¯πD (s, b) + γV¯πA,¯πD (v; s, b)(cid:9)

(2)

where [Gv](s, b) is referred to as the primal value backup operator, and g¯πA,¯πD (s, b),
V¯πA,¯πD (v; s, b) correspond to the instantaneous reward and the expected value of the continuation
game, respectively, deﬁned as

g¯πA,¯πD (s, b) =

V¯πA,¯πD (v; s, b) =

(cid:88)

a,d,s(cid:48),ϑ
(cid:88)

a,d,s(cid:48),ϑ

b(ϑ)¯πA(a | s, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)R(s, a, d, s(cid:48); ϑ)

b(ϑ)¯πA(a | s, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)v(s(cid:48), b(cid:48))

(3)

(4)

where b(cid:48) represents the posterior distribution on Θ as computed by Eq. (1).

5

For purposes of constructing an algorithm, we need to establish some properties of the backup
operator deﬁned in Proposition 1. The following lemma ensures that each application of the primal
value backup yields a closer approximation of the value of the game.
Lemma 2 (Contraction of Primal Backup Operator). The primal value backup operator [Gv](s, b),
deﬁned in Eq. (2), is a contraction mapping. As a result, iterating the operator converges to the
value of the primal game that solves the ﬁxed point equation (2).

Though conceptually correct, iterating the backup operator [Gv](s, b) exactly does not lead to a com-
putationally tractable algorithm, as the belief b lies in a continuous space with an inﬁnite cardinality.
Thus, an approximate value iteration algorithm is required for solving the ﬁxed point equation (2).
We will address this computational challenge in Sec. 4.3.

(2), cannot
Another challenge in solving N-CIRL is that the ﬁxed point problem, given by Eq.
be solved by the defender.
In fact, as pointed out in [35, Sec. 1.2], if the defender is unaware
of the attacker’s strategy, it cannot form the posterior on Θ. The following section discusses the
formulation of an auxiliary game to address this challenge.

4.2 The Defender’s Strategy

0 (s) = (cid:80)

As shown by [7, 35], the defender’s equilibrium strategy can be determined by construction of a dual
game, characterized by a tuple (cid:104)S, {A, D}, T (· | ·, ·, ·), {Θ, R(·, ·, ·, ·; ·)}, ζ0, P S
0 (·), γ(cid:105). Note that
the sets S, A, D, Θ, the reward function R(·, ·, ·, ·; ·), the discount factor γ, and the state transition
distribution T are identical to those in the primal game. The quantity ζ0 ∈ R|Θ| is the parameter
of the dual game, P S
0 (·) ∈ ∆(S) is the initial distribution of the state s0, which is obtained by
marginalizing P0(·, ·) over θ, i.e., P S
θ∈Θ P0(s, θ). The dual game proceeds as follows:
at the initial stage, s0 is sampled from P S
0 and revealed to both players, the attacker chooses some
θ ∈ Θ; then the game is played identically as the primal one, namely, both players choose actions
a ∈ A and d ∈ D simultaneously, and the state transitions from s to s(cid:48) following T (· | s, a, d). Both
players are then informed of the chosen actions and the successor state s(cid:48). Furthermore, a reward
of R(s, a, d, s(cid:48); θ) + ζ0(θ) is received by the attacker (thus −R(s, a, d, s(cid:48); θ) − ζ0(θ) is incurred by
the defender). Note that the value of θ is decided and only known by the attacker, instead of being
drawn from some probability distribution. This is one of the key differences from the primal game.
The value function of the dual game, denoted by w : S × R|Θ| → R, is deﬁned as the maximin
γ-discounted accumulated reward received by the attacker, if the state starts from some s0 = s ∈ S
and the game parameter is ζ0 = ζ ∈ R|Θ|. The value w(s, ζ) exists since the dual game is ﬁnite
[35]. Similarly as in Proposition 1, the dual game value function w also satisﬁes a recursive formula,
as formally stated below.
Proposition 2 (Sequential Decomposition of Dual Game). The dual game can be decomposed into
a sequence of single-stage games. Speciﬁcally, the dual value function w satisﬁes the following
recursive formula

w(s, ζ) = [Hw](s, ζ) = min
¯πD, ξ

max
µ

(cid:8)h¯πD, µ(s, ζ) + γW¯πD, µ(w, ξ; s)(cid:9)

(5)

where [Hw](s, ζ) is referred to as the dual value backup operator, ¯πD(·|s) ∈ ∆(Θ), ξ ∈ RS×A×Θ
are decision variables with ξa,s ∈ R|Θ| the (a, s)th vector of ξ, µ ∈ ∆(A × Θ). Moreover,
h¯πD, µ(s, ζ) and W¯πD, µ(w, ξ; s) are deﬁned as

h¯πD, µ(s, ζ) :=

(cid:88)

(cid:16)

µ(a, ϑ)

ζ(ϑ) +

¯πD(d | s)T (s(cid:48) | s, a, d)R(s, a, d, s(cid:48); ϑ)

(cid:17)

,

(6)

(cid:88)

d,s(cid:48)

µ(a, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)(cid:0)w(s(cid:48), ξa,s(cid:48)) − ξa,s(cid:48)(ϑ)(cid:1).

(7)

W¯πD, µ(w, ξ; s) :=

a,ϑ
(cid:88)

a,d,s(cid:48),ϑ

The recursive formula above allows for a stage-wise calculation of the defender’s strategy ¯πD. In
particular, by [35], the defender’s equilibrium strategy obtained from the dual game is indeed its
equilibrium strategy for the primal game. Moreover, the pair (s, ζ) is indeed the information state of
the defender in the dual game. More importantly, the formula of Eq.(5) does not involve the update
of the belief b, as opposed to Eq. (2). Instead, the vector ξ plays the similar role as the updated belief

6

b(cid:48), which can be calculated by the defender as a decision variable. This way, as the defender plays
the N-CIRL game, it can calculate its equilibrium strategy by only observing the attacker’s action
a and the successive state s(cid:48), with no need to know the attacker’s strategy. Besides, the defender’s
strategy ¯πD, the decision variable µ ∈ ∆(A × Θ) in Eq. (5), is essentially the attacker’s strategy in
the dual game, i.e., given ζ, the attacker has the ﬂexibility to choose both θ ∈ Θ and action a ∈ A.

The remaining issue is to solve the dual game by solving the ﬁxed point equation (5). As with the
primal counterpart, the dual value backup operator is also a contraction mapping, as shown below.
Lemma 3 (Contraction of Dual Backup Operator). The value backup operator [Hw](s, ζ), deﬁned
in Eq. (5), is a contraction mapping. As a result, iterating the operator converges to the dual game
value that solves the ﬁxed point equation (5).

By Lemma 3, the defender’s strategy can be obtained by iterating the backup operator. However, as
ζ and ξ both lie in continuous spaces, such an iterative approach is not computationally tractable.
This motivates the approximate value iteration algorithm to be introduced next.

4.3 Computational Procedure

One standard algorithm for computing strategies in single-agent partially observable settings is the
point-based value iteration (PBVI) algorithm [32]. The standard PBVI algorithm approximates the
value function, which is convex in the beliefs, using a set of α-vectors. While the value functions v
and w in N-CIRL also have desirable structure (as shown in [35], v : S × ∆(Θ) → R is concave on
∆(Θ) and w : S × R|Θ| → R is convex on R|Θ| for each s ∈ S), the standard PBVI algorithm does
not directly apply to N-CIRL. The challenge arises from the update step of the α-vector set in PBVI,
which requires carrying out one step of the Bellman update [32], for every action-observation pair
of the agent. The corresponding Bellman update in N-CIRL is the primal backup operator in Eq.
(2), which requires knowledge of the defender’s strategy, something that is unknown to the attacker.

To address this challenge, we develop a modiﬁed version of PBVI, termed non-cooperative PBVI
(NC-PBVI), in which the value functions v and w are approximated by a set of information state-
value pairs, i.e., ((s, b), v) and ((s, ζ), w), instead of α-vectors.
Importantly, updating the sets
only requires evaluations at individual information states, avoiding the need to know the opponent’s
strategy.

The evaluations can be approximated using linear programming. Speciﬁcally, to approximate the
value function of the primal game, NC-PBVI updates the value at a given attacker’s information state
(s, b) by solving the primal backup operator of Eq. (2). Using a standard reformulation, the minimax
operation in the primal backup operator can be approximately solved via a linear program, denoted
by PA(s, b). Similarly, one can approximately solve the dual game’s ﬁxed point equation, Eq. (5), at
a given defender’s information state (s, ζ) via another linear program, denoted by PD(s, ζ). For no-
tational convenience, deﬁne Tsad(s(cid:48)) = T (s(cid:48) | s, a, d), P ϑ
s(cid:48) T (s(cid:48) | s, a, d)R(s, a, d, s(cid:48); ϑ),
Asϑ(a) = ¯πA(a | s, ϑ)b(ϑ), and Ds(d) = ¯πD(d | s). The two linear programs, PA(s, b) and
PD(s, ζ), are given below.

sad = (cid:80)

max
[Asϑ(a)]≥0,[Vds(cid:48) ],
[bds(cid:48) (ϑ)]≥0,V
(cid:88)

s.t. V ≤

V

PA(s, b)

min
[Ds(d)]≥0,[Was(cid:48) ],
[λas(cid:48) (ϑ)]≥0,W

W

PD(s, ζ)

Asϑ(a)P ϑ

sad + γ

(cid:88)

Vds(cid:48) ∀ d

s.t. W ≥ ζ(ϑ) +

Ds(d)P ϑ
sad

(cid:88)

d

a,ϑ

bds(cid:48)(ϑ) =

(cid:88)

a

s(cid:48)
Asϑ(a)Tsad(s(cid:48)) ∀ d, s(cid:48), ϑ

(cid:0)Y A

s , W A

s , bds(cid:48)

(cid:1) ∀ d, s(cid:48)

Vds(cid:48) ≤ Υv
(cid:88)

Asϑ(a) = b(ϑ) ∀ ϑ

(cid:88)

+ γ

(cid:0)Was(cid:48) − λas(cid:48)(ϑ)(cid:1) ∀ a, ϑ

s(cid:48)

(cid:0)Y D

s , W D

s , λas(cid:48)

(cid:1) ∀ a, s(cid:48)

Was(cid:48) ≥ Υw
(cid:88)

Ds(d) = 1

a

d

The objective functions of the two linear programs estimate the values of the primal and dual
games. The decision variables [Asϑ], [Vds(cid:48)], [bds(cid:48)(ϑ)] in PA(s, b) are used to ﬁnd the attacker’s
strategy, the continuation value of the (primal) game, and the updated belief, respectively. Similarly,

7

[Ds(d)], [Was(cid:48)], [λas(cid:48)(ϑ)] in PD(s, ζ) are used to ﬁnd the defender’s strategy, the continuation value
of the dual game, and the updated parameter ζ for the dual game, respectively. The ﬁrst constraint
in PA(s, b) encodes the defender’s best response, replacing the minimization over ¯πD in Eq. (2).
Similarly, the ﬁrst constraint in PD(s, ζ) replaces the maximization over µ in Eq. (5). The second
and last constraints in PA(s, b) and the last constraint in PD(s, ζ) enforce basic rules of probabil-
ity. The third constraint in PA(s, b) and the second constraint in PD(s, ζ) provide the information
state-value approximations of the continuation value estimates Vds(cid:48) and Was(cid:48), respectively.

Due to concavity (convexity) of the value function v (resp. w), we use the sawtooth function [11]
In particular, a lower bound on the primal game’s
as an information state-value approximation.
(cid:1) whereas an upper bound of the dual game’s
value function v(s, b) is given by Υv
s , bds(cid:48)
(cid:1).4 The set Y A
value function w(s, ζ) is given by Υw
s , λas(cid:48)
s contains the belief-value pairs
associated with the beliefs that are non-corner points of the simplex over Θ for given s, whereas the
set W A
s contains the belief-value pairs associated with the corner points of the simplex. Analogously,
s represent subsets of R|Θ| that contain the vectors with only one, and more than one, non-
Y D
zero element, respectively. Details of both Υv and Υw using sawtooth functions can be found in the
pseudocode in Sec. C in the Appendix.

s , W A
s , W D

(cid:0)Y A
(cid:0)Y D

s , W D

Lemma 4 ensures that the sawtooth constraints are linear in the decision variables of the respec-
tive problem, which veriﬁes the computational tractability of PA(s, b) and PD(s, ζ). The proof of
Lemma 4 can be found in Sec. B in the appendix.

Lemma 4. By deﬁnitions of SAWTOOTH-A and SAWTOOTH-D in Algorithm 1, given
(cid:1) are
Y A
s , W A
both linear in the decision variables Vds(cid:48), bds(cid:48) and Was(cid:48), λas(cid:48), respectively.

s , constraints Vds(cid:48) ≤ Υv

(cid:1) and Was(cid:48) ≥ Υw

s , W D

s , W D

s , W A

s , λas(cid:48)

s , Y D

s , bds(cid:48)

(cid:0)Y D

(cid:0)Y A

Next, we numerically analyze the proposed algorithm in a cyber security environment.

5 Experiment: Intrusion Response

A recent trend in the security literature concerns the development of automated defense systems,
termed state-based intrusion response systems, that automatically prescribe defense actions in re-
sponse to intrusion alerts [26, 15, 27]. Core to these systems is the construction of a model that
describes the possible ways an attacker can inﬁltrate the system, termed a threat model. Deriving
a correct threat model is a challenging task and has a signiﬁcant impact on the effectiveness of
the intrusion response system. N-CIRL addresses one of the main challenges in this domain: the
defender’s uncertainty of the attacker’s true intent.

The threat model in our experiment is based on an attack graph, a common graphical model in the
security literature [2]. An attack graph is represented by a directed acyclic graph G = (N , E) where
each node n ∈ N represents a system condition and each edge eij ∈ N × N represents an exploit.
Each exploit eij relates a precondition i, the condition needed for the attack to be attempted, to a
postcondition j, the condition satisﬁed if the attack succeeds. Each exploit eij is associated with a
probability of success, βij, describing the likelihood of the exploit succeeding (if attempted). The
state space S is the set of currently satisﬁed conditions (enabled nodes). For a given state s ∈ S,
the attacker chooses among exploits that have enabled preconditions and at least one not yet enabled
postcondition. The defender simultaneously chooses which exploits to block for the current stage;
blocked edges have a probability of success of zero for the stage in which they are blocked. The
attacker’s reward is R(s, a, d, s(cid:48); θ) = re(s, s(cid:48); θ) − cA(a) + cD(d), where s(cid:48) is the updated state,
re(s, s(cid:48); θ) is the attacker’s reward for any newly enabled conditions, and cA(a) and cD(d) are costs
for attack and defense actions, respectively. The experiments are run on random instances of attack
graphs; see some instances in Figure 1. See Sec. C for more details of the experimental setup.

As seen in Figure 1, the strategies obtained from N-CIRL yield a lower attacker reward than the
strategies obtained from MA-IRL. Empirically, this implies that the defender beneﬁts more from in-
terleaving learning and execution than the attacker. Even though the interleaved setting may provide
more ground for the attacker to exercise deceptive tactics, [36] states that in games of incomplete
information, the more informed player “cannot exploit its private information without revealing it,
at least to some extent.” In the context of our example, we believe that the performance gain of

4Note that bds(cid:48) is the vector consisting of bds(cid:48) (ϑ) over all ϑ ∈ Θ (similarly for λas(cid:48) ).

8

N-CIRL arises from the fact that the attacker can only deceive for so long; eventually it must fulﬁll
its true objective and, in turn, reveal its intent.

Figure 1: Top: Instances of randomly generated graphs of sizes n = 6 to n = 10, with state
cardinalities |S| = {8, 24, 32, 48, 64} and action cardinalities |A| = |D| = {54, 88, 256, 368, 676}.
Bottom-left: Attacker’s average reward for each graph size n; ﬁlled/outlined markers represent the
attacker’s accumulated reward versus defense strategies obtained from MA-IRL/N-CIRL. Bottom-
middle: The average relative reduction of attacker reward in N-CIRL compared to MA-IRL as a
function of n. Bottom-right: Average runtime of NC-PBVI (in seconds) as a function of n.

6 Concluding Remarks

The goal of our paper was to introduce the N-CIRL formalism and provide some theoretical results
for the design of learning algorithms in the presence of strategic opponents. The primary motiva-
tion for this work was cyber security, speciﬁcally, problems where the defender is actively trying to
defend a network when it is uncertain of the attacker’s true intent. Learning from past attack traces
(i.e., equilibrium behavior/demonstrations) can lead to poor defense strategies, demonstrating that
such approaches (e.g., MA-IRL) are not directly applicable for settings where rewards may change
between demonstrations. Empirical studies illustrate that the defender can beneﬁt by interleaving
the learning and execution phases compared to just learning from equilibrium behavior (this is an
analogous conclusion to the one found in CIRL that an interactive scenario can lead to better per-
formance). The reason for this is that defense strategies computed using N-CIRL learn the intent
adaptively through interaction with the attacker.

As shown in [9], the value alignment problem is more appropriately addressed in a dynamic and
cooperative setting. The cooperative reformulation converts the IRL problem into a decentralized
stochastic control problem. In our paper, we have shown that the non-cooperative analog of CIRL,
i.e., when agents possess goals that are misaligned, becomes a zero-sum Markov game with one-
sided incomplete information. Such games are conceptually challenging due to the ability of agents
to inﬂuence others’ beliefs of their private information through their actions, termed signaling in
game theoretic parlance. We hope that the N-CIRL setting can provide a foundation for an algorith-
mic perspective of these games and deeper investigation into signaling effects in general stochastic
games of asymmetric information.

Acknowledgements

This work was supported in part by the US Army Research Laboratory (ARL) Cooperative Agree-
ment W911NF-17-2-0196, and in part by the Ofﬁce of Naval Research (ONR) MURI Grant N00014-
16-1-2710.

9

References

[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Inter-

national Conference on Machine Learning, 2004.

[2] P. Ammann, D. Wijesekera, and S. Kaushik. Scalable, graph-based network vulnerability

analysis. In ACM Conference on Computer and Communications Security, 2002.

[3] T. Arnold, D. Kasenberg, and M. Scheutz. Value alignment or misalignment – what will keep

systems accountable? In AAAI Conference on Artiﬁcial Intelligence, 2017.

[4] T. Bas¸ar. Stochastic differential games and intricacy of information structures. In Dynamic

Games in Economics, pages 23–49. Springer, 2014.

[5] D. Blackwell. Discounted dynamic programming. The Annals of Mathematical Statistics,

36(1):226–235, 1965.

[6] P. Cardaliaguet and C. Rainer. Stochastic differential games with asymmetric information.

Applied Mathematics and Optimization, 59(1):1–36, 2009.

[7] B. De Meyer. Repeated games, duality and the central limit theorem. Mathematics of Opera-

tions Research, 21(1):237–251, 1996.

[8] C. Finn, S. Levine, and P. Abbeel. Guided cost learning: Deep inverse optimal control via

policy optimization. In International Conference on Machine Learning, 2016.

[9] D. Hadﬁeld-Menell, S. J. Russell, P. Abbeel, and A. Dragan. Cooperative inverse reinforcement

learning. In Advances in Neural Information Processing Systems, 2016.

[10] E. A. Hansen, D. S. Bernstein, and S. Zilberstein. Dynamic programming for partially observ-

able stochastic games. In AAAI Conference on Artiﬁcial Intelligence, 2004.

[11] M. Hauskrecht. Value-function approximations for partially observable Markov decision pro-

cesses. Journal of Artiﬁcial Intelligence Research, 13:33–94, 2000.

[12] K. Hendricks and R. P. McAfee. Feints. Journal of Economics & Management Strategy,

15(2):431–456, 2006.

[13] K. Hor´ak, B. Bosansk`y, and M. Pechoucek. Heuristic search value iteration for one-sided
partially observable stochastic games. In AAAI Conference on Artiﬁcial Intelligence, 2017.
[14] J. H¨orner, D. Rosenberg, E. Solan, and N. Vieille. On a Markov game with one-sided informa-

tion. Operations Research, 58(4-part-2):1107–1115, 2010.

[15] S. Iannucci and S. Abdelwahed. A probabilistic approach to autonomic security management.

In IEEE International Conference on Autonomic Computing, 2016.

[16] Y. Ji, S. Lee, E. Downing, W. Wang, M. Fazzini, T. Kim, A. Orso, and W. Lee. RAIN: Reﬁnable
attack investigation with on-demand inter-process information ﬂow tracking. In ACM SIGSAC
Conference on Computer and Communications Security, 2017.

[17] R. E. Kalman. When is a linear control system optimal?

Journal of Basic Engineering,

86(1):51–60, 1964.

[18] H. W. Kuhn. Extensive Games and the Problem of Information, volume 2. Princeton University

Press, 1953.

[19] M. Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. P´erolat, D. Silver, and T. Grae-
pel. A uniﬁed game-theoretic approach to multiagent reinforcement learning. In Advances in
Neural Information Processing Systems, 2017.

[20] X. Lin, S. C. Adams, and P. A. Beling. Multi-agent inverse reinforcement learning for certain
general-sum stochastic games. Journal of Artiﬁcial Intelligence Research, 66:473–502, 2019.
[21] X. Lin, P. A. Beling, and R. Cogill. Multiagent inverse reinforcement learning for two-person

zero-sum games. IEEE Transactions on Games, 10(1):56–68, 2018.

[22] M. L. Littman. Markov games as a framework for multi-agent reinforcement learning.

In

Machine Learning Proceedings, pages 157–163. Elsevier, 1994.

[23] D. Malik, M. Palaniappan, J. Fisac, D. Hadﬁeld-Mennell, S. Russell, and A. Dragan. An
efﬁcient, generalized Bellman update for cooperative inverse reinforcement learning. In Inter-
national Conference on Machine Learning, 2018.

[24] J. Marschak and R. Radner. Economic Theory of Teams. Yale University Press, 1972.

10

[25] J.-F. Mertens and S. Zamir. Formulation of bayesian analysis for games with incomplete infor-

mation. International Journal of Game Theory, 14(1):1–29, 1985.

[26] E. Miehling, M. Rasouli, and D. Teneketzis. Optimal defense policies for partially observable
spreading processes on Bayesian attack graphs. In Second ACM Workshop on Moving Target
Defense, 2015.

[27] E. Miehling, M. Rasouli, and D. Teneketzis. A POMDP approach to the dynamic defense
IEEE Transactions on Information Forensics and Security,

of large-scale cyber networks.
13(10):2490–2505, 2018.

[28] A. Nayyar and T. Bas¸ar. Dynamic stochastic games with asymmetric information. In IEEE

Conference on Decision and Control, 2012.

[29] A. Nayyar, A. Gupta, C. Langbort, and T. Bas¸ar. Common information based Markov perfect
equilibria for stochastic games with asymmetric information: Finite games. IEEE Transactions
on Automatic Control, 59(3):555–570, 2014.

[30] A. Nayyar, A. Mahajan, and D. Teneketzis. Decentralized stochastic control with partial his-
IEEE Transactions on Automatic Control,

tory sharing: A common information approach.
58(7):1644–1658, 2013.

[31] A. Y. Ng and S. J. Russell. Algorithms for inverse reinforcement learning. In International

Conference on Machine Learning, 2000.

[32] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for

POMDPs. In International Joint Conferences on Artiﬁcial Intelligence, 2003.

[33] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich. Maximum margin planning. In International

Conference on Machine Learning, 2006.

[34] J. Renault. The value of Markov chain games with lack of information on one side. Mathe-

matics of Operations Research, 31(3):490–512, 2006.

[35] D. Rosenberg. Duality and Markovian strategies.

International Journal of Game Theory,

27(4):577–597, 1998.

[36] D. Rosenberg and N. Vieille. The maxmin of recursive games with incomplete information on

one side. Mathematics of Operations Research, 25(1):23–35, 2000.

[37] S. Russell, D. Dewey, and M. Tegmark. Research priorities for robust and beneﬁcial artiﬁcial

intelligence. AI Magazine, 36(4):105–114, 2015.

[38] S. J. Russell and P. Norvig. Artiﬁcial Intelligence: A Modern Approach. Malaysia; Pearson

Education Limited, 2016.

[39] S. Sorin. Stochastic games with incomplete information. In Stochastic Games and Applica-

tions, pages 375–395. Springer, 2003.

[40] S. Srinivasan, M. Lanctot, V. Zambaldi, J. P´erolat, K. Tuyls, R. Munos, and M. Bowling.
Actor-critic policy optimization in partially observable multiagent environments. In Advances
in Neural Information Processing Systems, 2018.

[41] X. Wang and D. Klabjan. Competitive multi-agent inverse reinforcement learning with sub-

optimal demonstrations. In International Conference on Machine Learning, 2018.

[42] N. Wiener. Some moral and technical consequences of automation. Science, 131(3410):1355–

1358, 1960.

[43] K. Zhang, Z. Yang, and T. Bas¸ar. Policy optimization provably converges to Nash equilibria in

zero-sum linear quadratic games. arXiv preprint arXiv:1906.00729, 2019.

[44] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Bas¸ar. Finite-sample analyses for fully decentral-

ized multi-agent reinforcement learning. arXiv preprint arXiv:1812.02783, 2018.

[45] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Bas¸ar. Fully decentralized multi-agent rein-
forcement learning with networked agents. In International Conference on Machine Learning,
2018.

11

Supplementary Materials for “Non-Cooperative Inverse
Reinforcement Learning”

A Illustrative Example

Consider a simple zero-sum patrolling game where a thief A aims to steal valuables from a museum
m and a gallery g that a security guard D is watching over. The state space is deﬁned as the set of
possible locations of the players (A, D), deﬁned as S = {(m, m), (m, g), (g, m), (g, g)}. The initial
state is uniformly sampled from S. The game is played over multiple stages, where at each stage
of the game, A and D observe the current state s ∈ S and (simultaneously) choose to either stay at
their respective locations or switch to the other location, i.e., A = D = {stay, switch}. The state
deterministically transitions from s to s(cid:48) based on (a, d). If the state transitions to either s(cid:48) = (m, g)
or s(cid:48) = (g, m), A will successfully steal an item with probability 1 and gain a reward of either θ if
s(cid:48) = (m, g), or 1 − θ if s(cid:48) = (g, m), where θ ∈ [0, 1] reﬂects A’s private preference that is unknown
to D. If the state transitions to either s(cid:48) = (m, m) or s(cid:48) = (g, g), the presence of D will lower A’s
probability of success to 1/2. Thus, A’s (expected) reward at each stage is

R(s; θ) =






θ/2
θ
1 − θ
(1 − θ)/2

if s = (m, m)
if s = (m, g)
if s = (g, m)
if s = (g, g)

(8)

Under this setting, we compare the strategies obtained under the MA-IRL formalism, e.g., [21, 41,
20], to the strategies where D can learn the intent adaptively under the N-CIRL formalism. In MA-
IRL, D has access to an attack log, which reﬂects that A prefers the gallery twice as much as the
museum, i.e., θ = 1/3. In an initial learning phase, D learns from previous equilibrium behavior5,
and in the subsequent execution phase, A and D play the game described above. In contrast, D
under N-CIRL has no prior knowledge of A’s intent and learns it from scratch through information
that is revealed as the game unfolds. For purposes of this example, A’s preferences are assumed to
ﬂip in the execution phase, i.e., θ = 2/3. We now compare two strategies in a two-stage instance of
the above game.

MA-IRL Strategies. For θ = 1/3, D has a unique pure Nash equilibrium strategy that generates its
next location to be g, independent of the initial state. However, since A’s true intent in the execution
phase when D actually participates is θ = 2/3, A’s pure Nash equilibrium strategies is to go to m.
Such a combination of A and D’s equilibrium strategies yields a next state of (m, g), which results
in a reward of 2/3 for A. Hence, over the two-stage game (execution phase), the total reward for A
against D under MA-IRL is 2 · 2/3 = 4/3.

N-CIRL Strategies. D has a uniform patrol strategy in the ﬁrst stage (toss a fair coin to decide
where to patrol). Since A has intent parameter θ = 2/3 in the execution phase, it will go to the
museum according to R(s; θ = 2/3), with state (m, m) arising from the Nash equilibrium strategies.
Hence, A’s expected reward in the ﬁrst stage is 1/2 · 1/2 · 2/3 + 1/2 · 2/3 = 1/2. At the second
stage, D receives the observation that A went to m, thus infers θ > 1/2. With the new estimation
of A’s intent, D prefers to defend m, which results in a cost of −1/2 · θ that is smaller than the
cost of defending g, which is −θ. Therefore, the expected reward of A at the second stage becomes
1/2 · 2/3 = 1/3, and the total reward in the two-stage game is 1/2 + 1/3 = 5/6 < 4/3.

The above example illustrates that there exist settings where the defender can incur a lower cost by
interleaving learning and execution.

5D under MA-IRL is (generously) assumed to be able to perfectly recover the parameter θ = 1/3 during
the learning phase. This is often not the case as D’s inference using historical data can not be exactly accurate
in practice. We make this assumption to favor MA-IRL as much as possible.

12

B Proofs of Main Results

B.1 Proof of Lemma 1

Proof. Given a current state s and distribution b on Θ, the posterior distribution is computed by
conditioning on the new information (consisting of the actions (a, d) and updated state s(cid:48)) as

P (θ = ϑ | s, b, a, d, s(cid:48)) =

P (s, b, a, d, s(cid:48) | ϑ)P (θ = ϑ)
ϑ(cid:48) P (s, b, a, d, s(cid:48) | ϑ(cid:48))P (θ = ϑ(cid:48))

(cid:80)

=

=

=

=

=

P (s | ϑ)P (a, d | s, ϑ)P (b, s(cid:48) | s, a, d, ϑ)P (θ = ϑ)
ϑ(cid:48) P (s, b, a, d, s(cid:48) | ϑ(cid:48))P (θ = ϑ(cid:48))

(cid:80)

(cid:80)

P (s | ϑ)P (a | s, ϑ)P (d | s)P (s(cid:48) | s, a, d)P (b | s, ϑ)P (θ = ϑ)
ϑ(cid:48) P (s, b, a, d, s(cid:48) | ϑ(cid:48))P (θ = ϑ(cid:48))
P (s | ϑ)P (a | s, ϑ)P (d | s)P (s(cid:48) | s, a, d)P (b | s, ϑ)b(ϑ)
ϑ(cid:48) P (s | ϑ(cid:48))P (a | s, ϑ(cid:48))P (d | s)P (s(cid:48) | s, a, d)P (b | s, ϑ(cid:48))b(ϑ(cid:48))
P (s(cid:48) | s, a, d)P (d | s)P (a | s, ϑ)b(ϑ)

(cid:80)

P (s(cid:48) | s, a, d)P (d | s) (cid:80)
P (a | s, ϑ)b(ϑ)
ϑ(cid:48) P (a | s, ϑ(cid:48))b(ϑ(cid:48))

(cid:80)

,

ϑ(cid:48) P (a | s, ϑ(cid:48))b(ϑ(cid:48))

(9)

(10)

(11)

(12)

(13)

(14)

where Eq. (9) follows Bayes rule, Eq. (10) uses deﬁnition of conditional probability, Eq. (11)
uses the conditional independence of a and d given s, ϑ, and that of b and s(cid:48) given s, a, d, ϑ, Eq.
(12) expands the denominator as in Eq. (11) and uses the fact that P (θ = ϑ(cid:48)) = b(ϑ(cid:48)), Eq. (13)
uses that P (s | ϑ) = P (b | s, ϑ) = 1, and Eq. (14) follows by cancelling out P (s(cid:48) | s, a, d).
Note that the probabilities P (s(cid:48) | s, a, d), P (d | s), P (a | s, ϑ) are all nonzero, as the calculation
(9)-(14) is only for those tuples of (a, d, s(cid:48)) that have been realized. Recognizing that
in Eqs.
P (a | s, ϑ) = ¯πA(a | s, ϑ) yields the result.

B.2 Proofs of Propositions 1 and 2

Proof. The proofs of Propositions 1 and 2 are similar and are built upon the results of [35]. In the
language of [35], player 1 is the more informed player and player 2 is the less informed player.
Let I and J denote the action sets of player 1 and 2, respectively, K denotes the set of the states
of the world, X denotes the set of the stochastic states, and λ ∈ (0, 1) is the discount factor. Let
s ∈ ∆(I)K denote the strategy6 of player 1 and t ∈ ∆(J) denote the strategy of player 2. To prove
Proposition 1, consider the following sequential decomposition from [35, Proposition 6],

vλ(p, x) = max

s∈∆(I)K

min
t∈∆(J)

(cid:20)

λ

(cid:88)

(cid:88)

k∈K

(i,j)∈I×J

pksk(i)Akx

ij t(j)

+ (1 − λ)

(cid:88)

(cid:88)

(cid:88)

(cid:21)
pksk(i)t(j)q(x, i, j, y)vλ(pi, y)

,

(15)

k∈K

(i,j)∈I×J

y∈X

(cid:80)

where superscripts denote indexing and pi denotes the Bayesian update deﬁned elementwise by
i = pksk(i)
pk
l∈K plsl(i) . There are two modiﬁcations that need to be made to the recursive formula. First,
compared to the reward function A in [35], the reward function in N-CIRL additionally depends on
the successor state s(cid:48). This dependence requires that the ﬁrst term in the summation of Eq. (15) also
needs to take an expectation over s(cid:48). Second, the form of the payoff differs in N-CIRL; dividing
Eq. (15) by λ yields an expression for vλ/λ which corresponds to value of the primal game v in
Proposition 1. Denoting s, t, p, A, q, 1 − λ therein by ¯πA, ¯πD, b, R, T , γ in our notation system,
respectively, yields the sequential decomposition of (2) in Proposition 1.

6The notation ∆(I)K means all functions from K to ∆(I).

13

To prove Proposition 2, consider the sequential decomposition from [35, Proposition 7],
(cid:19)

(cid:18)

wλ(α, x) = inf

t∈∆(J)

inf
β∈RK×I×X

sup
π∈∆(I×K)

(cid:88)

(cid:88)

i∈I

k∈K

π(i, k)

λt(j)Akx

ij + αk

−

+

(cid:88)

(cid:88)

(cid:88)

(1 − λ)π(i, k)βk(i, y)t(j)q(x, i, j, y)

k∈K

(i,j)∈I×J

y∈X

(cid:88)

(cid:88)

(cid:18) (cid:88)

(1 − λ)

i∈I

y∈X

j∈J

(cid:19)

t(j)q(x, i, j, y)

π(i)wλ(β(i, y), y)

(16)

where π(i) = (cid:80)
k∈K π(i, k) and α ∈ RK. First, note that the dual game in our case is ﬁnite
(I, J, K, X are ﬁnite in our problem) and hence has a value. As a result, the inf and sup in [35,
Propositions 7] can be replaced by min and max. Next, as in the proof of Proposition 1, divide Eq.
(16) by λ and denote t, α/λ, β/λ, π, ωλ/λ, 1−λ therein by ¯πD, ζ, ξ, µ, w, γ, respectively. Grouping
terms, we arrive at the sequential decomposition of (5) in Proposition 2.

B.3 Proof of Lemma 2

Proof. We prove this lemma by showing that the value backup operator G for a given one-stage
strategy proﬁle (¯πA, ¯πD) is a contraction mapping with γ ∈ [0, 1). Let η := (s, b) and v, v(cid:48) be
value functions. By Blackwell’s sufﬁciency theorem [5], [Gv](η) is a contraction mapping if it
[Gv](η) ≥ [Gv(cid:48)](η), if v(η) ≥ v(cid:48)(η) for any η, and ii) discounting:
satisﬁes i) monotonicity:
[Gv](η) = [Gv(cid:48)](η) + γε, ∀v(η) = v(cid:48)(η) + ε. To show monotonicity, assume v(η) ≥ v(cid:48)(η) for all
η; then we have

(cid:88)

a,d,s(cid:48),ϑ

b(ϑ)¯πA(a | s, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)(v(η(cid:48)) − v(cid:48)(η(cid:48))) ≥ 0 ∀η

where η(cid:48) represents the updated attacker information state. Note that the instantaneous reward does
not depend on v, v(cid:48), thus
[Gv](η) − [Gv(cid:48)](η) = γ max
¯πA

(cid:8)V¯πA,¯πD (v; η) − V¯πA,¯πD (v(cid:48); η)(cid:9)

b(ϑ)¯πA(a | s, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)(v(η(cid:48)) − v(cid:48)(η(cid:48)))

(cid:111)

min
¯πD
(cid:110) (cid:88)

a,d,s(cid:48),ϑ

= γ max
¯πA

min
¯πD

≥ 0.

To show discounting, let v(η) = v(cid:48)(η) + ε. Then

[Gv](η) = max
¯πA

min
¯πD

(cid:26) (cid:88)

a,d,s(cid:48),ϑ

b(ϑ)¯πA(a | s, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)R(s, a, d, s(cid:48); ϑ)

b(ϑ)¯πA(a | s, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)(v(cid:48)(η(cid:48)) + ε)

(cid:27)

(cid:88)

+ γ

a,d,s(cid:48),ϑ

= max
¯πA

min
¯πD

(cid:26) (cid:88)

a,d,s(cid:48),ϑ

b(ϑ)¯πA(a | s, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)R(s, a, d, s(cid:48); ϑ)

b(ϑ)¯πA(a | s, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)(v(cid:48)(η(cid:48)))

(cid:27)

+ γε

(cid:88)

+ γ

a,d,s(cid:48),ϑ

= [Gv(cid:48)](η) + γε.

Therefore, the one-stage value backup operator is a contraction mapping for a given one-stage strat-
egy proﬁle (¯πA, ¯πD).

B.4 Proof of Lemma 3

Proof. Let w, w(cid:48) be value functions. As in the proof of Lemma 2, we show the monotonicity and dis-
counting properties, and then invoke Blackwell’s sufﬁciency theorem. Assume w(ξ, s) ≥ w(cid:48)(ξ, s)

14

for all s, ξ; then
(cid:88)

a,d,s(cid:48),ϑ

µ(a, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)(w(ξa,s(cid:48), s(cid:48)) − w(cid:48)(ξa,s(cid:48), s(cid:48))) ≥ 0 ∀s, ξ

Note that the instantaneous reward does not depend on w, w(cid:48), and thus

[Hw](s, ζ) − [Hw(cid:48)](s, ζ)

= γ min
¯πD,ξ

max
µ

= γ min
¯πD,ξ

max
µ

≥ 0.

(cid:8)W¯πD, µ(w, ξ; s) − W¯πD, µ(w(cid:48), ξ; s)(cid:9)
(cid:8) (cid:88)

µ(a, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)(w(ξa,s(cid:48), s(cid:48)) − w(cid:48)(ξa,s(cid:48), s(cid:48)))(cid:9)

a,d,s(cid:48),ϑ

To show discounting, let w(ξ, s) = w(cid:48)(ξ, s) + ε for all s, ξ. Then

[Hw](s, ζ) = min
¯πD,ξ

max
µ

= min
¯πD,ξ

max
µ

+ γ

(cid:26) (cid:88)

a,ϑ

+ γ

(cid:26) (cid:88)

a,ϑ

(cid:88)

(cid:18)

µ(a, ϑ)

ζ(ϑ) +

(cid:88)

d,s(cid:48)

¯πD(d | s)T (s(cid:48) | s, a, d)R(s, a, d, s(cid:48); ϑ)

(cid:19)

µ(a, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)(cid:0)w(ξa,s(cid:48), s(cid:48)) − ξa,s(cid:48)(ϑ)(cid:1)

(cid:27)

(cid:88)

a,d,s(cid:48),ϑ

(cid:18)

µ(a, ϑ)

ζ(ϑ) +

(cid:88)

d,s(cid:48)

¯πD(d | s)T (s(cid:48) | s, a, d)R(s, a, d, s(cid:48); ϑ)

(cid:19)

µ(a, ϑ)¯πD(d | s)T (s(cid:48) | s, a, d)(cid:0)w(cid:48)(ξa,s(cid:48), s(cid:48)) − ξa,s(cid:48)(ϑ)(cid:1)

(cid:27)

+ γε

a,d,s(cid:48),ϑ

= [Hw(cid:48)](s, ζ) + γε.

Therefore, the one-stage value backup operator is a contraction mapping for a given (¯πD, ξ, µ).

B.5 Proof of Lemma 4

Proof. By deﬁnition of the subroutine SAWTOOTH-A in Algorithm 1, for given Ys, Ws, the func-
(cid:0)Ys, Ws, ·(cid:1) returns the xj for the j that makes vj − cT bj > 0. Denote this j by j∗. Let
tion Υv
ej,ϑ = (cid:0)0, · · · , 1/bj(ϑ), · · · , 0(cid:1)T
∈ R|Θ| be an all-zero vector except that the ϑ-th element is
1/bj(ϑ). Thus, the following equivalence relationship holds, i.e., for any V

V ≤ Υv

(cid:0)Ys, Ws, b(cid:1) ⇐⇒ V ≤ cT b + min

ϑ∈Θ

(cid:8)eT

j∗,ϑb|bj∗ (ϑ) > 0(cid:9) · (vj∗ − cT bj∗ ).

(17)

The positivity of vj∗ − cT bj∗ implies that (17) can then equivalently be written as

V ≤ Υv

(cid:0)Ys, Ws, b(cid:1) ⇐⇒ V ≤ cT b + eT

j∗,ϑb · (vj∗ − cT bj∗ ),

∀ϑ ∈ Θ,

(18)

which essentially describes |Θ| constraints that are linear in b. Note that the dependences of the
constraints (18) on Ws and Ys are implicitly embedded in ﬁnding c and (bj∗ , vj∗ ), respectively.
Similarly, SAWTOOTH-D in Algorithm 1 returns the yj that makes wj − cT ζj < 0. Denoting this
j by j∗, we have the following equivalent conditions

V ≥ Υw

(cid:0)Ys, Ws, ζ(cid:1)

⇐⇒

V ≥ cT ζ + dT

j∗,ϑb · (wj − cT ζj),

∀ϑ ∈ Θ,

(19)

where dj∗,ϑ = (cid:0)0, · · · , 1/ζj(ϑ), · · · , 0(cid:1)T
is 1/ζj(ϑ). Note that (19) describes |Θ| constraints that are linear in ζ. This completes the proof.

∈ R|Θ| be an all-zero vector except that the ϑ-th element

15

C Details of the NC-PBVI Algorithm

The experimental setup is as follows. We randomly generate attack graphs with sizes ranging from
6 to 10 nodes. Root nodes are assumed to be enabled initially. Furthermore, we limit the in-degree
and out-degree of nodes to be at most 3. For each graph of size n, we run an experiment on a ﬁnite
horizon of length n. The intent parameter is uniformly chosen from a set of random intent parameters
of size |Θ| = 10. The attacker’s accumulated reward is collected at each stage and normalized by
the total reward across all nodes. To compare the average performance of N-CIRL and MA-IRL, we
run 20 graph instances for each size and plot the attackers’ average reward, see Figure 1. Note that in
MA-IRL, both players are playing a complete information game. The difference is that the defender
is playing Nash equilibrium strategies based on an intent parameter that is inferred from existing
attack data. The attacker, on the other hand, knows its true intent (which is in general different from
the defender’s inferred intent) and plays its corresponding Nash equilibrium strategies.

All the experiments were run on a machine with an AMD Ryzen 1950X Processor and 32GB of
RAM. We used GUROBI 8.1.1 to solve the LPs used in our algorithm and the probability of suc-
cess, βiy, is assumed to be 0.8. The detailed pseudocode of the proposed NC-PBVI algorithm is
summarized in Algorithm 1.

16

Algorithm 1 Non-Cooperative Point-Based Value Iteration (NC-PBVI)

function EXPAND-D (Y, W)

for s ∈ S do

for (ζ, w) ∈ Ys ∪ Ws do
ζ (cid:48) ← solve PD(s, ζ)
if (ζ (cid:48), ·) /∈ Ys ∪ Ws then

Ws,ζ(cid:48) ← solve PD(s, ζ (cid:48))
Ys ← Ys ∪ (ζ (cid:48), Ws,ζ(cid:48) )

end if

end for

end for
return Y, W

end function

function SAWTOOTH-A (Ys, Ws, b)

for (bi, vi) ∈ Ws do

ci ← vi

end for
xj ← cT b
for (bj, vj) ∈ Ys do

if vj − cT bj > 0 then

φ ← min
ϑ∈Θ

{b(ϑ)/bj(ϑ)|bj(ϑ) > 0}

xj ← xj + φ(vj − cT bj)
break

end if

end for
return xj
end function

function SAWTOOTH-D (Ys, Ws, ζ)

for (ζi, vi) ∈ Ws do

ci ← vi

end for
yj ← cT ζ
for (ζj, wj) ∈ Ys do

if wj − cT ζj < 0 then

ψ ← min
ϑ∈Θ

{ζ(ϑ)/ζj(ϑ)|ζj(ϑ) > 0}

yj ← yj + ψ(wj − cT ζj)
break

end if

end for
return yj
end function

function NC-PBVI (b0, ζ0, N, T )

for p in {A, D} do
for s ∈ S do

Initialize Y p

s , W p
s

end for
for N expansions do

for T iterations do

Y p, W p ← UPDATE-P(Y p, W p)

end for
Y p, W p ← EXPAND-P (Y p, W p)

end for

end for
Compute πA, πD by solving PA(s, b) and
PD(s, ζ) for all s and ﬁnite sets of b (resp. ζ).

end function

function UPDATE-A (Y, W)

for s ∈ S do

for (b, v) ∈ Ys ∪ Ws do

update v by solving PA(s, b)

end for

end for
return Y, W

end function

function UPDATE-D (Y, W)

for s ∈ S do

for (ζ, w) ∈ Ys ∪ Ws do

update w by solving PD(s, ζ)

end for

end for
return Y, W

end function

function EXPAND-A (Y, W)

for s ∈ S do

for (b, v) ∈ Ys ∪ Ws do

Ω ← ∅
¯πA ← solve PA(s, b)
for a ∈ A(s) do

ba ← τ (s, b, a)
Ω ← Ω ∪ ba

|ba(ϑ) − b(ϑ)|

end for
b(cid:48) ← arg max

ba∈Ω

(cid:80)
ϑ∈Θ

if (b(cid:48), ·) /∈ Ys ∪ Ws then

Vs,b(cid:48) ← solve PA(s, b(cid:48))
Ys ← Ys ∪ (b(cid:48), Vs,b(cid:48) )

end if

end for

end for
return Y, W

end function

17

