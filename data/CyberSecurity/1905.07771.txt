Noname manuscript No.
(will be inserted by the editor)

Estimating variances in time series
linear regression models using empirical BLUPs
and convex optimization

Martina Hanˇcov´a · Gabriela Voz´arikov´a ·
Andrej Gajdoˇs · Jozef Hanˇc

9
1
0
2

y
a
M
9
1

]
E
M

.
t
a
t
s
[

1
v
1
7
7
7
0
.
5
0
9
1
:
v
i
X
r
a

Received: date / Accepted: date

Abstract
We propose a two-stage estimation method of variance components in time se-
ries models known as FDSLRMs, whose observations can be described by a linear
mixed model (LMM). We based estimating variances, fundamental quantities in a
time series forecasting approach called kriging, on the empirical (plug-in) best lin-
ear unbiased predictions of unobservable random components in FDSLRM. The
method, providing invariant non-negative quadratic estimators, can be used for
any absolutely continuous probability distribution of time series data. As a result
of applying the convex optimization and the LMM methodology, we resolved two
problems — theoretical existence and equivalence between least squares estima-
tors, non-negative (M)DOOLSE, and maximum likelihood estimators, (RE)MLE,
as possible starting points of our method and a practical lack of computational im-
plementation for FDSLRM. As for computing (RE)MLE in the case of n observed
time series values, we also discovered a new algorithm of order O(n), which at
the default precision is 107 times more accurate and n2 times faster than the best
current Python(or R)-based computational packages, namely CVXPY, CVXR,
nlme, sommer and mixed. We illustrate our results on three real data sets —
electricity consumption, tourism and cyber security — which are easily available,
reproducible, sharable and modiﬁable in the form of interactive Jupyter notebooks.

Keywords ﬁnite discrete spectrum linear regression model · linear mixed model ·
double ordinary least squares estimators · maximum likelihood estimators ·
eﬃcient computational algorithms

Mathematics Subject Classiﬁcation (2010) 62M10 · 62J12 · 91B84 · 90C25

This work was supported by the Slovak Research and Development Agency under the contract
No. APVV-17-0568, the Scientiﬁc Grant Agency of the Slovak Republic (VEGA), VEGA grant
No.1/0311/18 and the Internal Research Grant System of Faculty of Science, P. J. ˇSaf´arik
University in Koˇsice (VVGS PF UPJˇS) — project VVGS-PF-2018-792.

M. Hanˇcov´a · G. Voz´arikov´a · A. Gajdoˇs
Institute of Mathematics, Pavol Jozef ˇSaf´arik University, Koˇsice, Slovakia
Tel.: +421-55-2342213, Fax: +421-55-6222124
E-mail: martina.hancova@upjs.sk

J. Hanˇc
Institute of Physics, Pavol Jozef ˇSaf´arik University, Koˇsice, Slovakia

 
 
 
 
 
 
2

1 Introduction

Martina Hanˇcov´a et al.

The need to obtain suﬃciently accurate predictions for facilitating and improving
decision making becomes an integral part not only of science, industry or economy
but also in many other human activities. Our recent article [21] summarizes the
guiding methodology and corresponding references dealing with kriging for time
series econometric forecasting as one of the advanced alternative approaches to
the most popular Box-Jenkins methodology ([5], [52]).

The key idea of the time series kriging is to model the given time series data in
an appropriate general class of linear regression models (LRMs) and subsequently
ﬁnding the best linear unbiased predictor — the BLUP which minimizes the mean
squared error (MSE) of prediction among all linear unbiased predictors, see e.g.
[57], [32], [11], [10].

In the frame of kriging, we investigate theoretical features and econometric
applications of a class of time series models called ﬁnite discrete spectrum linear
regression models or shortly FDSLRMs. The FDSLRM class was introduced in
2002-2003 by ˇStulajter ([57], [58]) as a direct extension of classical (ordinary)
regression models (see e.g. [11], [30]).

The FDSLRM has mean values (trend) given by linear regression and random
components (error terms) are represented as a sum of a linear combination of
uncorrelated zero-mean random variables and white noise which can be interpreted
in terms of the ﬁnite discrete spectrum [43].

Formally the FDSLRM can be presented as

X(t) =

k
(cid:88)

i=1

βifi(t) +

l
(cid:88)

j=1

Yjvj(t) + w(t); t ∈ T,

(1.1)

where

T representing the time domain is a countable subset of the real line R,
k and l are some ﬁxed non-negative integers, i.e. k, l ∈ N0,
β = (β1, β2, . . . , βk)(cid:48) ∈ Rk is a vector of regression parameters,
Y = (Y1, Y2, . . . , Yl)(cid:48) is an unobservable l×1 random vector with zero mean vector
(cid:9),
E {Y } = 0l, and with an l×l diagonal covariance matrix Cov{Y } = diag (cid:8)σ2
where σ2

j ∈ R+ are non-negative real numbers,

j

fi(.); i = 1, 2, . . . , k and vj(.); j = 1, 2, . . . , l are real functions deﬁned on R,
w(.) stands for white noise uncorrelated with Y and having a positive dispersion

D {w(t)} = σ2

0 ∈ R++.

Typically, due to the nature of time series data collection, the most frequently
considered time domain T is the set of natural numbers N = {1, 2, . . .}. FDSLRM
variance parameters, which are fundamental quantities in kriging, are commonly
l )(cid:48), an element of the
described by one vector ν ≡ (ν0, ν1, . . . , νl)(cid:48) = (σ2
parametric space Υ = (0, ∞) × [0, ∞)l or Υ = R++ × Rl

+ for short.

1, . . . , σ2

0, σ2

The principal goal of our paper is to present an alternative estimation method
for FDSLRM variances, one of the time series kriging steps [21]. Our approach
represents an improved two-stage modiﬁcation of the previously developed method
of natural estimators (NE) [26] which is now based on the idea of empirical (plug-
in) BLUPs. We will refer to our new method as EBLUP-NE for short.

Estimating variances in FDSLRMs via EBLUPs and convex optimization

3

The structure of the paper is organized as follows. Section 2 includes several
important notes about theoretical methods and computational tools for FDSLRM
kriging oﬀered by closely connected mathematical branches and computational
research. This background became the strong basis for our research.

Section 3 describes the ﬁrst stage of our EBLUP-NE method. It develops the
deﬁnition and computational form of EBLUP-NE at known variance parameters
ν using two special matrices known in linear algebra as the Schur complement and
Gram matrix [65]. The method requires very minimal distributional assumptions
on a ﬁnite FDLSRM observation X = (X(1), . . . , X(n))(cid:48), n ∈ N. It must have
only an absolutely continuous probability distribution with respect to some σ-ﬁnite
measure. In section 3 we also derive the basic statistical properties of EBLUP-NE
at known ν.

Section 4 is devoted to the second stage of EBLUP-NE method which consists
in replacing true, in practice unknown variance parameters ν by another, appro-
priate FDSLRM estimates of ν obtained by maximum likelihood or least squares
[21]. Since the theory and computational aspects of these estimation procedures in
FDSLRM ﬁtting based on the projection theory in Hilbert spaces are more than
10 years old ([57], [59], [26]), we revisit and update them in the light of recent
advances in closely related linear mixed modeling and convex optimization.

In the following ﬁfth section, we illustrate theoretical results of the paper and
the performance of EBLUP-NE on three real time series data sets — electricity
consumption, tourism and cyber security. Section 6 presents the conclusions of
the paper. For the sake of paper readability, we moved very technical details and
proofs to the Appendix. In the Appendix we also report a list of acronyms and
abbreviations used in the paper (tab. 5).

Since the paper is aimed at statisticians, analysts, econometricians and data
scientists applying time series and forecasting methods, our notation is standard
for time series analysis and prediction using linear regression models ([57], [32],
[9]). Sets like R, R+, R++ are labeled as it is common in convex optimization [6].

2 Theoretical and computational bases for FDSLRM kriging

Let us recall some important notes about time series FDSLRM kriging and its
connection to other math branches and current computational technology.

From a modeling point of view, if we consider any time series model X(.) in
the additive form X(t) = m(t) + ε(t); t ∈ T whose mean value function m(.)
is a real function on R expressible by a functional series (e.g. Taylor or Fourier
series) and error term ε(.) is a mean-zero stationary process, then according to the
spectral representation theory of time series ([43],[40]) X(.) can be approximated
arbitrarily closely by FDSLRM. Therefore, from a practice perspective, FDSLRMs
can be potentially applied in many practical situations.

In econometric applications of FDSLRM kriging, we almost always have only
one realization of time series X(.) and we do not know the mean-value parameters
β ∈ Rk nor the variance parameters ν ∈ Υ. From the view of time series analysis,
the problem of estimating variances ν belongs to the class of problems concerning
covariance matrix estimation with one realization [63].

4

Martina Hanˇcov´a et al.

In addition, any ﬁnite FDSLRM observation X satisﬁes a special type of linear

mixed model (LMM) of the form

X = Fβ + VY + w with E {w} = 0n, Cov{w} = σ2
0In + VDV(cid:48),

Cov{Y , w} = 0l×n, Cov{X} ≡ Σν = σ2

0In,

(2.1)

where design matrices F = {Fti} = {fi(t)}, V = {Vtj} = {vj(t)} for t = 1, 2, . . . , n;
i = 1, 2, . . . , k; j = 1, 2 . . . , l and random vector w = (w(1), w(2), . . . , w(n))(cid:48) is a
ﬁnite n-dimensional white noise observation.

The fundamental property (2.1) of FDSLRM allows us to apply convenient
LMM mathematical techniques for FDSLRM ﬁtting and forecasting ([41], [11],
[61], [15], [48], [13], [53]) together with up-to-date LMM software packages — nlme,
sommer written in R ([42], [14]), MATLAB function mixed [62] or SAS package
Proc MIXED [49]. As the ﬁrst practical consequence of the LMM theory, to have
identiﬁable FDSRLMs under assumptions of multivariate normal distribution [15,
sec. 3.2, thm 11], in the paper, we will assume for the block matrix (F V) and
for the number of data n in x, a realization of X in real situations, the following
suﬃcient condition

identiﬁability : the full column rank r(F V) = k + l and n > k + l.

(2.2)

On the other hand, the standard maximum likelihood or least squares esti-
mates of ν in FDSLRM are optimization problems. Our motivation to explore
estimating FDSLRM variances in the frame of convex optimization lays on the
fact that its mathematical tools and eﬃcient, very reliable computational interior-
point methods became important or fundamental tools in many other branches of
mathematics [6] like the design of experiments, high-dimensional data, machine
learning or data mining.

Inspired by essential and recent works on convex optimization ([6], [4], [34], [12],
[1]) we prove new theoretical relations among existing FDSLRM estimators in the
extended parametric space Υ = Rl+1
+ for the orthogonal version of FDSLRM, most
used in real time series applications. Moreover, we show how to apply the latest
convex optimization packages CVXPY and CVXR based on disciplined convex
programming, written in Python [16] and R [17], for estimating ν. Finally, we also
focus on the development of a new fast and accurate computational optimization
algorithm for estimating variances in FDSLRM.

As for computational technology, our time series calculations are carried out
using free open source software based on the R statistical language and packages
([46], [38]), the Scientiﬁc Python with the SciPy ecosystem ([31], [39]) and free
Python-based mathematics software SageMath ([55], [3], [66]), an open source
alternative to the well-known commercial computer algebra systems Mathematica
or Maple. The simultaneous use of R, SciPy and SageMath provides us valuable
cross-checking of our computational results.

All our computational algorithms and results are easily readable, sharable, re-
producible, and modiﬁable thanks to open-source Jupyter technology [33]. In par-
ticular, we present our results in the form of Jupyter notebooks, dynamic HTLM
documents integrating prose, code and results similarly as Mathematica notebooks,
stored in our free available GitHub repository ([19], [37]).

The Jupyter notebooks, detailed records of our computing with explaining
narratives, can be seen or studied as static HTML pages via Jupyter nbviewer

Estimating variances in FDSLRMs via EBLUPs and convex optimization

5

(https://nbviewer.jupyter.org/, [33]) or interactively as live HTML documents us-
ing Binder (https://mybinder.org/, [44]) where the code is executable. The way
and presentation of our computing with real data are inspired by works [8], [60].

3 The BLUP estimation method for ν

3.1 Deﬁnition and computational form of estimators

In the paper [26], we proposed the method of always non-negative natural esti-
mators (NE) of FDSLRM variances ν. The main idea behind NE came from the
j = Cov{Yj} = E (cid:8)Y 2
(cid:9) ; j = 1, . . . , l. Therefore, if random vector Y in
fact that σ2
(1.1) was known, the natural estimate of σ2
j . This initial consid-
eration was identical with Rao’s estimates known as MINQUE ([47, sec. 5.1], [11,
sec. 12.7]).

j would be just Y 2

j

To get suﬃciently simple, explicit analytic expressions available for further
theoretical study, we predicted the random vector Y by the ordinary least squares
method leading to the following linear predictor of Y based on X

˘Y = W−1V(cid:48)MFX; W = V(cid:48)MFV ∈ Rl×l.

Matrix MF = (In−F(F(cid:48)F)−1F(cid:48)) ∈ Rn×n represents ([26]) the orthogonal projector
onto the orthogonal complement of the column space of F and matrix W (cid:31) 0 ((cid:31) 0
means positive deﬁniteness) is known in linear algebra and statistics [65, chap. 6]
as the Schur complement of F(cid:48)F in the block matrix (cid:0) F(cid:48)F F(cid:48)V
V(cid:48)F V(cid:48)V

However, from perspective of the general theory of the best linear unbiased
prediction in LMM (see e.g. [11, chap. 12]), we could consider natural estimators of
ν in FDSLRM based on the BLUP Y ∗
ν of Y . Then intuitively, it seems reasonable
to assume that the BLUP as the unbiased linear predictor with minimal MSE could
lead to better estimators of ν. This intuition will be conﬁrmed by our following
derivations.

(cid:1).

The previous heuristic consideration motivates the following deﬁnition. Let us

consider FDSLRM (1.1) and its observation (2.1). Then estimators in the form

j (X) = (Y ∗
˚σ2

ν )2

j ; Y ∗

ν is the BLUP of Y based on X

(3.1)

are called natural estimators of ν based on the BLUP of Y or BLUP-NE for short.

Remark 1
Following the analogy with original NE in [26], for the ﬁrst component ν0 = σ2
0 of ν we
could take as a BLUP based natural estimator the sum of squares of white noise residuals
w∗

ν divided by the number of degrees of freedom in FDSLRM

ν = X − Fβ∗

ν − VY ∗

˚σ2

0(X) =

1
n − k − l

[X − Fβ∗

ν − VY ∗

ν ](cid:48)[X − Fβ∗

ν − VY ∗
ν ],

where β∗
white noise residuals w∗

ν is the best linear unbiased estimator (BLUE) of β. In the LMM framework, FDSLRM

ν are also called conditional residuals [53].

But now our intuition fails. The computational form and properties of Y ∗
ν depend on all
variance components ν. That dependency also appears in the proposed estimator ˚σ2
0(X) which
causes its biasedness even in a simpler special case of FDSLRM (orthogonality condition, see
(3.9)). Therefore, further we do not pay any special attention to its form and properties. As
a natural estimator of σ2
0(X) of
σ2
0, eq. (2.2) in [26].

0 we will keep the original unbiased invariant quadratic NE ˘σ2

6

Martina Hanˇcov´a et al.

The BLUP Y ∗

ν of Y together with the BLUE β∗

ν of β can be obtained from
celebrated Henderson’s mixed model equations, MME for short (for MME in the
current framework of LMM see e.g. [11, sec. 12.3] or [61, sec. 2]). MME have the
following form in the case of FDSLRM

(cid:19)

(cid:18)F(cid:48)F F(cid:48)V
V(cid:48)F GV

(cid:18)β∗
ν
Y ∗
ν

·

(cid:19)

(cid:18)F(cid:48)X
V(cid:48)X

(cid:19)

,

=

(3.2)

where GV = GV + σ2
matrix ([7, sec. 10.1]) for columns of design matrix V.

0D−1 = V(cid:48)V + σ2

0D−1 and GV = V(cid:48)V ∈ Rl×l is the Gram

Under the assumption of the full column rank r = k + l of matrix (F V), when
FDSRLM is identiﬁable, and using the so-called Banachiewicz inversion formula
for the inverse of a 2 × 2 partitioned (block) matrix (see e.g. [65, sec. 6.0.2] or [26,
sec. 2.1]), we can easily prove the existence and the following form of the inverse
to the block matrix in (3.2)

(cid:19)−1

(cid:18)F(cid:48)F F(cid:48)V
V(cid:48)F GV

=

(cid:18)(F(cid:48)F)−1 0k×l
0l×l

0l×k

(cid:19)

+

(cid:18)−(F(cid:48)F)−1F(cid:48)V
Il

(cid:19)

W−1 (cid:16)

−V(cid:48)F(F(cid:48)F)−1 Il

(cid:17)

,

0D−1 = V(cid:48)MFV + σ2

where W = W + σ2
F(cid:48)F but now in the block matrix
as the extended Schur complement determined by variances ν.

0D−1 (cid:31) 0 is again the Schur complement of
. We also call matrix W more speciﬁcally

(cid:16) F(cid:48)F F(cid:48)V
V(cid:48)F GV

(cid:17)

Substituting the last result for the inverse into MME (3.2) and rearranging,

we get for Y ∗

ν (symbols • denote blocks not needed in deriving)

(cid:19)

(cid:18)β∗
ν
Y ∗
ν

=

(cid:18)F(cid:48)F F(cid:48)V
V(cid:48)F GV

(cid:19)−1

(cid:18)F(cid:48)X
V(cid:48)X

·

(cid:19)

(cid:18)

=

•

(cid:19)

•

−W−1V(cid:48)F(F(cid:48)F)−1 W−1

(cid:19)

(cid:18)F(cid:48)X
V(cid:48)X

·

ν = W−1V(cid:48)(In − F(F(cid:48)F)−1F(cid:48))X = W−1V(cid:48)MFX
Y ∗

(3.3)

Denoting matrix W−1V(cid:48)MF ∈ Rl×n as T, we just derived the following computa-
tional form of BLUP-NE estimators.

Proposition 1 (the computational form of BLUP-NE)
Let us consider the following LMM model for FDSLRM observation X

X = Fβ + VY + w, E {w} = 0n, Cov{w} = σ2
(cid:111)

(cid:110)

0In,

Cov{Y } = diag

, Cov{Y , w} = 0l×n.

σ2
j

Then BLUP-NE estimators ˚σ2

1, . . . ,˚σ2

l of parameters σ2

1, . . . , σ2

l are given by

j (X) = (Y ∗)2
˚σ2

j = (TX)(cid:48)

j(TX)j = X (cid:48)tjt(cid:48)

jX; j = 1, . . . , l,

(3.4)

where tj = (Tj1, Tj2, . . . , Tjn)(cid:48) are rows of matrix T and T = W−1V(cid:48)MF with
W = diag (cid:8)σ2

(cid:9) + V(cid:48)MFV.

0/σ2
j

Estimating variances in FDSLRMs via EBLUPs and convex optimization

7

Remark 2
It is straightforward to extend our computational form to the case when some variances σ2
j
are zero or in other words when matrix D is singular. For a singular D, MME (3.2) have the
alternative form [61]

(cid:18)β∗
ν
Z∗
ν
0Il and Y ∗ = DZ∗.
Applying the same argument with the Banachiewicz inversion formula, we get the second

(cid:18)F(cid:48)F F(cid:48)VD
V(cid:48)F HV

0Il = V(cid:48)VD + σ2

(cid:18)F(cid:48)X
V(cid:48)X

where HV = GVD + σ2

(3.5)

=

(cid:19)

(cid:19)

(cid:19)

·

,

version of the computational form of T determining Y ∗
ν

T = DU−1V(cid:48)MF with U = WD + σ2

0Il = V(cid:48)MFVD + σ2

0Il.

(3.6)

As opposed to W, GV, matrices U, HV always exist under identiﬁability assumptions of our
FDSLRM. Simultaneously both of U, HV are also always invertible, since both of det U, det HV
are nonzero for any D in our FDSLRM as it is shown in the Appendix. In the case of a
nonsingular D the mentioned matrices are connected via following relationships

HV = GVD, G−1

V = DH−1

V , U = WD, W−1 = DU−1.

(3.7)

The version (3.5) of MME is also preferred in numerical calculations [61], since it can handle
not only a singular D but also a very ill-conditioned D appearing when ν has very small positive
components σ2
j .

It is worth to mention that originally MME were derived under the normality assumptions
(see the original work [29] or [61]), but from the viewpoint of least squares both versions (3.2),
(3.5) of MME describe the BLUP Y ∗
ν ([11, sec. 12.3]) with no need to restrict
distributions of Y and w to be normal.

ν and BLUE β∗

3.2 Statistical properties at known variance parameters

The derivation of theoretical properties of BLUP-NE estimates under the assump-
tion of known ν, regarding the ﬁrst and second order moment characteristics, can
be signiﬁcantly facilitated by the following lemma describing the properties of
matrix T determining the BLUP Y ∗
ν in (3.3).

Its proof can be accomplished in a similar way as the proof of Lemma 3.1 in [26]
by a direct routine computation employing formulas (3.7), T = W−1V(cid:48)MF, W =
V(cid:48)MFV+σ2
0D−1, properties of orthogonal projectors (orthogonality, idempotency,
symmetry) and Schur complements (symmetry, positive deﬁniteness).

Lemma 1 (basic properties of T)

(1) TT(cid:48) = W−1(Il − σ2
(2) TF = 0l×n and TV = Il − σ2
(3) TΣνT(cid:48) = D − σ2

0W−1 = D(Il − σ2

0U−1)

0D−1W−1) = DU−1(Il − σ2

0U−1)

0D−1W−1 = Il − σ2

0U−1

The computational forms (3.4) of BLUP-NE are also quadratic forms of X. In
addition, result (2) TF = 0l×n implies that tjF = 0. Such a condition leads to
the conclusion that BLUP natural estimators ˚σ2
j (X) are translation invariant or
shortly invariant quadratic estimators [57, sec. 1.5] or [26, sec. 3.1]. The following
theorem summarizes theoretical properties of BLUP-NE.

Theorem 1 (statistical properties of ˚σ2
Natural estimators ˚σ2
invariant quadratic estimators having the following properties

j (X))
j (X); j = 1, 2, . . . , l of ν based on the BLUP Y ∗

ν of Y are

8

Martina Hanˇcov´a et al.

0(W−1)jj; j = 1, 2, . . . , l,

(cid:8)˚σ2

(1) Eν

j − σ2

j (X)(cid:9) = σ2
If X ∼ Nn(Fβ, Σν), then
j − σ2
j (X)} = 2(σ2
(2) Dν{˚σ2
j (X)(cid:9) = 2(σ2
(cid:8)˚σ2
i (X),˚σ2
(3) Covν
j − σ2
j (X)} = 2(σ2
(4) M SEν{˚σ2

0(W−1)jj)2; j = 1, 2, . . . , l,

0(W−1)ij)2; i, j = 1, 2, . . . , l, i (cid:54)= j,
0(W−1)jj)2 + σ4
0(W−1)2

jj; j = 1, 2, . . . , l.

Proof See the Appendix.

Remark 3
If some components in ν are zero, there is a need to use expression DU−1 instead of W−1.
The ﬁrst property, biasedness of ˚σ2
j (X), j = 1, 2, . . . , l, is in full accordance with the Ghosh
theorem ([23], [21]) about the incompatibility between simultaneous non-negativity and unbi-
asedness of estimators for variances of random components Y in LMM. As for bias, deﬁned
as ∆ν
j , we have the following expression given by the extended
= Eν
j (X)
Schur complement W

j (X)

− σ2

˚σ2

˚σ2

(cid:111)

(cid:111)

(cid:110)

(cid:110)

∆ν

(cid:8)˚σ2

j (X)(cid:9) = −σ2

0(W−1)jj

(3.8)

Now we can theoretically compare quality of the BLUP natural estimators with
original natural estimators (NE) proposed in our previous paper [26]. As we can
see from summarizing tab. 1, their quality is determined by Schur complements
W and W where both of them are symmetric and positive deﬁnite matrices.

Table 1: First and second order moment characteristics for BLUP-NE and NE.

characteristic

∆ν
Dν
M SEν

NE estimators ˘σ2

j (X)
0(W−1)jj
σ2

BLUP-NE estimators ˚σ2

j (X)
0(W−1)jj

−σ2

2(σ2
2(σ2

j + σ2
j + σ2

0(W−1)jj )2
0(W−1)jj )2 + σ4

0(W−1)2
jj

2(σ2
2(σ2

j − σ2
j − σ2

0(W−1)jj )2
0(W−1)jj )2 + σ4

0(W−1)2
jj

Using elementary properties of the L¨owner partial ordering [45, chap. 24], we
have immediately relations W > W, W−1 > W−1, (W)jj > (W−1)jj which imply a
smaller absolute value of bias, dispersion and MSE for BLUP-NE ˚σ2
j (X) in com-
parison with original NE ˘σ2
j (X). This conclusion directly supports our heuristic
idea leading to the BLUP-NE deﬁnition (3.1).

Orthogonal FDSLRM

In real time series analysis using FDSLRMs, the fundamental modeling procedure
is based on spectral analysis of time series ([43], [9]). We identify the signiﬁcant
Fourier frequencies by periodogram ([59], [21]) which restrict the form of design
matrices F an V leading to the orthogonality condition for FDSLRM [59]

F(cid:48)V = 0 and GV = V(cid:48)V = diag

(cid:110)

(cid:107)vj(cid:107)2(cid:111)

.

(3.9)

Estimating variances in FDSLRMs via EBLUPs and convex optimization

9

where vj, j = 1, 2 . . . , l is j -th column of V. Such a FDSLRM, satisfying condition
(3.9), is called orthogonal [59]. So in practice, we mainly work with the orthogonal
version of FDSLRM.

Under the condition of orthogonality (3.9), we can write for matrices MF, U,

W, T and BLUP-NE ˚σ2

j (X)

MFV = V, U = GVD + σ2

W−1 = DU−1 = diag

(cid:110)

j (X) = ρ2
˚σ2

j X (cid:48)vjv(cid:48)

(cid:110)

0 + σ2
σ2

0Il = diag
ρj/ (cid:107)vj(cid:107)2(cid:111)
jX/ (cid:107)vj(cid:107)4 , j = 1, 2, . . . , l,

, tj = ρjv(cid:48)

j (cid:107)vj(cid:107)2(cid:111)
j/ (cid:107)vj(cid:107)2 ,

,

where we introduced ρj ≡

j (cid:107)vj(cid:107)2
σ2
j (cid:107)vj(cid:107)2 ∈ R, 0 (cid:53) ρj < 1, j = 1, 2, . . . , l.
0 +σ2

σ2

Applying these results, we get the following direct corollary of Theorem 1 for

any orthogonal FDSLRM.

Corollary 1 (properties of ˚σ2
In an orthogonal FDSLRM natural estimators ˚σ2
following properties

j (X) in orthogonal FDSLRM)

j based on the BLUP Y ∗

ν have the

(1) Eν

(cid:8)˚σ2

j (X)(cid:9) = ρjσ2

j ; j = 1, 2, . . . , l, where ρj =

j (cid:107)vj(cid:107)2
σ2
0 + σ2
σ2

j (cid:107)vj(cid:107)2 ∈ R+.

If X ∼ Nn(Fβ, Σν), then
(2) Dν{˚σ2
j (X)} = 2ρ2
(cid:8)˚σ2
i (X),˚σ2
(3) Covν
j (X)} = [2ρ2
(4) M SEν{˚σ2

j ; j = 1, 2, . . . , l,

j σ4
j (X)(cid:9) = 0; i, j = 1, 2, . . . , l, i (cid:54)= j,

j + (1 − ρj)2]σ4

j ; j = 1, 2, . . . , l.

Finally, if we look at original natural estimators ˘σ2
j (X) from our paper [26],
then in orthogonal FDSLRM it can be easily shown that ˚σ2
j (X), j =
1, 2, . . . , l. If we introduce ρ0 = 1, then we obtain the complete orthogonal version
of (3.4) for computing BLUP-NE ˚σ2

j (X) = ρ2

j ˘σ2

j (X):

j (X) = ρ2
˚σ2

j ˘σ2

j (X), j = 0, 1, . . . , l.

(3.10)

4 Mathematical and computational tools of convex optimization

4.1 Empirical BLUPs in the BLUP-NE estimation method

The computational form of our BLUP natural estimators, their ﬁrst and second
order moment characteristics were derived at known variances ν. But if we really
knew variances ν, we would not need to estimate them. The practical estimation
procedure for ν cannot depend on the parameters ν which are to be estimated.

However, such situation is not rare at all in LMMs, also in the FDSLRM the-
ory, when we consider e.g. double weighted least squares estimators (DOWELSE)
or maximum likelihood estimators (MLE) for ν. In FDSRLM ﬁtting these estima-
tors are computed by iterative numerical procedures using projections in Hilbert
spaces ([57, sec. 3.4] or [59]). Generally, in the LMM framework, similar iterative
numerical estimation procedures are described in [50, chap. 8], [61].

10

Martina Hanˇcov´a et al.

In our case, the simplest reasonable solution of the mentioned situation is to
use an empirical version of BLUP (EBLUP) based on the computational form of
BLUP-NE (3.4), or (3.10) in an orthogonal case, as it is usual in the general theory
of empirical BLUPs in LMMs ([57, chap. 5], [61], [48, chap. 5]). In particular, this
step means replacing unknown true parameters ν with other ”initial” values or
estimates of ν in FDSLRM. In the light of iterative approaches, our EBLUP-NE
can be viewed as a two-stage iterative method with one step in the iteration.

If we think again heuristically as during the formulation of the BLUP-NE
deﬁnition (3.1), it seems reasonable to expect as good starting values of ν any, in
some sense optimal estimates of ν, e.g. estimates obtained by maximum likelihood
or least squares in FDSLRM.

On the top of that, from the theoretical perspective we should prefer estimates
with the simplest explicit form and under less restrictive assumptions on the struc-
ture or distribution of FDSLRM. On the other hand, from the practical point of
view, it will be suﬃcient to have at least initial estimates which can be obtained
by reliable and time eﬃcient computational methods.

Therefore, in the following sections we investigate, theoretically and practically
with real data sets, ﬁve estimation methods providing initial estimates for ν under
diﬀerent assumptions on the structure and distribution of FDSLRM observation
X. Our candidates for the second stage of EBLUP-NE are summarized in tab. 2.

Table 2: Estimation methods chosen for the second stage in EBLUP-NE.

least squares method in FDSLRM maximum likelihood method in FDSLRM

double ordinary least squares estimators
(DOOLSE) ref. [59], [57]

maximum likelihood estimators
(MLE) ref. [59], [57]

modiﬁed (unbiased) DOOLSE
(MDOOLSE) ref. [59]

natural estimators (NE) ref. [26]

restricted (residual) MLE
(REMLE) ref. [59]

Since theoretical and computational aspects of these estimation procedures in
FDSLRM ﬁtting based on the projection theory in Hilbert spaces are more than
10 years old ([57], [59], [26]), we revisit and update them in the light of the current
theoretical and computational tools of convex optimization.

According to [1], [6], to formulate any mathematical problem as an optimization
problem, we need to identify three attributes of the problem: optimization variable,
constraints that the variable must satisfy and the objective function depending on
the variable whose optimal value we want to achieve.

In all our estimation methods, optimization variable is ν ∈ Rl+1 satisfying
constraints Υ = R++ × Rl
+. In order to avoid any incompatibility problems in
the convex optimization theoretical or computational framework, we extend con-
straints Υ into the form of standard nonnegativity constraints Υ∗ = [0, ∞]l+1 or
ν (cid:23) 0 using generalized inequality.

Finally, we also consider using present theoretical and computational tools used
in linear mixed modeling, since in the LMM framework, the problem of estimating
variances has a long and rich history with many essential reference works (e.g.
[47], [50], [11], [15], [48], see also a review paper [61]).

Estimating variances in FDSLRMs via EBLUPs and convex optimization

11

4.2 General case of the orthogonal FDSLRM

First of all, we formulate the basic assumptions required in our investigations on
the structure and distribution of FDSLRM. As we mentioned in subsection 3.2,
in real (practical) FDSLRM analysis we mainly work with orthogonal FDSLRMs
(3.9). Therefore, in the rest of the paper we will focus primarily on this type of
FDSLRMs.

The second assumption deals with a distribution of the FDSLRM observation
X = (X(1), . . . , X(n))(cid:48), n ∈ N. For now, we assume X satisfying (2.1) and having
any absolutely continuous probability distribution with respect to some σ-ﬁnite
measure.

Under these two assumptions, we can apply in the second stage of EBLUP-
NE three estimation methods: natural estimators (NE), double ordinary least
squares estimators (DOOLSE) and modiﬁed double ordinary least squares esti-
mators (MDOOLSE).

Original natural estimators – NE

In the case of NE, we know the analytic solution of the estimation problem [26],
which gives us always required non-negative estimates of FDSLRM variances. Em-
ploying results of [26], orthogonality condition (3.9) leads to the following form of
NE

(cid:32)

˘ν(e) =













1

n−k−l e(cid:48) MV e
(e(cid:48)v1)2/ (cid:107)v1(cid:107)4
(e(cid:48)v2)2/ (cid:107)v2(cid:107)4
...
(e(cid:48)vl)2/ (cid:107)vl(cid:107)4

1
n−k−l













=
















e(cid:48)e −

l
(cid:80)
j=1
(e(cid:48)v1)2/ (cid:107)v1(cid:107)4
(e(cid:48)v2)2/ (cid:107)v2(cid:107)4
...
(e(cid:48)vl)2/ (cid:107)vl(cid:107)4

(e(cid:48)vj)2/ (cid:107)vj(cid:107)2

(cid:33)
















,

(4.1)

where e = x − Fβ∗ = MFx is nothing else than the vector of ordinary least
squares (OLS) residuals in FDSLRM, MV = In − V(V(cid:48)V)−1V(cid:48) is the orthogo-
nal projector onto the orthogonal element of the column space of V and x is an
arbitrary realization of the FDSLRM observation X. Vectors vj, j = 1, 2, . . . , l
are columns of design matrix V. Moreover, in orthogonal FDSLRM BLUE β∗
ν =
(F(cid:48)Σ−1
ν x from MME (3.2) is identical with ordinary least squares esti-
mate β∗ = (F(cid:48)F)−1F(cid:48)x not depending on ν [59].

ν F)−1F(cid:48)Σ−1

As for computational complexity of NE, using elementary theory of complexity
[7], we get the complexity 2kn operations for the residual vector e and 4ln for NE
from (4.1). Therefore, computing NE represents an algorithm with the complexity
having order O(n) with respect to the realization length n.

Remark 4
For the purposes of software cross-checking of results and evaluating numerical precision of
convex optimization algorithms, it is worth to formulate the calculation of NE as a convex
optimization problem. Since NE employ the least-squares method, it is not complicated to
show that in orthogonal FDSLRMs, NE can be obtained as a unique, always existing, non-
negative solution of the following convex optimization problem

12

Martina Hanˇcov´a et al.

NE

minimize

f0(ν) = ||ee(cid:48) −

l
(cid:80)
j=1

subject to

ν = (ν0, . . . , νl)(cid:48) ∈ [0, ∞)l+1

νj Vj ||2 + (cid:107)MVee(cid:48)MV − ν0V0(cid:107)2

where matrices in the objective function f0(ν) are deﬁned by expressions

V0 = MFMV and Vj = vj v(cid:48)

j , j = 1, . . . , l.

(4.2)

(4.3)

In convex optimization, this kind of optimization problems belongs to the convex quadratic
optimization problems or the norm approximation problems [6, sec. 4.4, sec. 6.1].

Double least squares estimators – DOOLSE, MDOOLSE

From the optimization viewpoint, assuming general (not necessarily orthogonal)
FDSLRM observation X (2.1), DOOLSE and MDOOLSE ([57], [59]) can be viewed
as the following optimization problems for ν at given OLS residuals e ([1], [6])

DOOLSE

minimize

subject to

MDOOLSE

minimize

subject to

f0(ν) = (cid:13)
(cid:13)ee(cid:48) − Σν
ν = (ν0, . . . , νl)(cid:48) ∈ [0, ∞)l+1

(cid:13)
2
(cid:13)

(cid:13)ee(cid:48) − MFΣνMF

f0(ν) = (cid:13)
ν = (ν0, . . . , νl)(cid:48) ∈ [0, ∞)l+1

(cid:13)
2
(cid:13)

(4.4)

(4.5)

0In + VDV(cid:48) is
where e are again OLS residuals as in the case of NE and Σν = σ2
the covariance matrix of X. We call matrix Se ≡ ee(cid:48) matrix of residual products
or more compactly the residual products matrix.

Remark 5
In the next text, some theoretical results can be written in one form for both of DOOLSE and
MDOOLSE problems. To emphasize this fact, we will use one abbreviation with parenthesis
( ), e.g (M)DOOLSE, assuming that given results or considerations hold for both problems.

Using the geometrical language of Hilbert spaces ([9], [57]), (M)DOOLSE de-
termine a covariance matrix with the parametric structure as in Σν but having
the smallest Euclidean distance to the matrix Se. In such case, (M)DOOLSE for
orthogonal FDSLRMs could be computed geometrically as the orthogonal pro-
jection of Se onto the linear span L (V0, V1, . . . , Vl) using the Gram matrix G
of {V0, V1, . . . , Vl} generated by the inner product (•, •) = tr(• · •). Matrices
Vj, j = 1, . . . , l are given by (4.3), for DOOLSE: V0 = In and for MDOOLSE:
V0 = MF.

According to [59], [21], the mentioned projection is given by

˜ν(e) = G−1q,

(4.6)

Estimating variances in FDSLRMs via EBLUPs and convex optimization

13

where G =










n∗

(cid:107)v1(cid:107)2 (cid:107)v2(cid:107)2 . . . (cid:107)vl(cid:107)2

(cid:107)v1(cid:107)2 (cid:107)v1(cid:107)4
(cid:107)v2(cid:107)2
...
(cid:107)vl(cid:107)2

0
...
0

0

. . .
(cid:107)v2(cid:107)4 . . .
...
0

0
0
...
. . .
. . . (cid:107)vl(cid:107)4










(cid:31) 0,

q =










e(cid:48)e
(e(cid:48)v1)2
(e(cid:48)v2)2
...
(e(cid:48)vl)2










,

where for DOOLSE: n∗ ≡ n, for MDOOLSE: n∗ ≡ n−k, e are again OLS residuals,
vj are columns of V.

However, the projection method can produce estimates out of the parametric
space Υ at a relatively high probability [21], i.e. in many cases it produces negative
estimates. The proposed method (4.6) is not able to handle constraints given by
Υ reliably and in some simple way.

To distinguish the projection estimates from real DOOLSE (4.4), MDOOLSE
(4.5) which are always non-negative, we suggest to be more accurate and use full
names for real DOOLSE and MDOOLSE: non-negative DOOLSE (NN-DOOLSE)
and non-negative MDOOLSE (NN-MDOOLSE) This speciﬁc way is similarly used
e.g. in the case of Rao’s MINQUE [47, chap. 5]. For the projection-based DOOLSE
and MDOOLSE, without considering nonnegativity constraints, we leave the orig-
inal acronyms DOOLSE, MDOOLSE.

Applying the basic convex quadratic optimization theory ([12], [4]) in orthogo-
nal FDSLRM, we can rewrite NN-(M)DOOLSE as strictly convex quadratic prob-
lems, whose solutions ˜ν always exist and are unique global minimizers on Υ∗ (see
Propositions 2 and 4 in the Appendix)

NN-(M)DOOLSE

minimize

subject to

f0(ν) = ν(cid:48)Gν − 2q(cid:48)ν

−Il+1ν (cid:22) 0l+1

(4.7)

The most important result of convex optimization for NN-(M)DOOLSE prob-
lems are fundamental Karush-Kuhn-Tucker (KKT) optimality conditions, which
in orthogonal FDSLRM provide a necessary and suﬃcient condition for optimality
of solutions (see Proposition 3 in the Appendix). In practice, ﬁnding solutions of
convex optimization problems is nothing else than solving the corresponding KKT
conditions analytically or in general numerically.

In our case of NN-(M)DOOLSE, the KKT conditions can be represented by a
set of 2l nonsingular linear systems of equations, where each of them is given by
matrix K derived from G and by vector q from (4.6). Our theoretical results in
the Appendix show that only one of the linear systems always gives us all required
non-negative elements of ˜ν.

On this basis, we can establish a simple KKT optimization algorithm which run
through given linear systems, compute their analytical solution and stops when
the solution appears non-negative. In other words, the proposed KKT algorithm
always founds the required NN-(M)DOOLSE ˜ν+ in at most 2l steps. The algorithm
is summarized in tab. 3, whereas the proof and details are in the Appendix.

As for computational complexity of the KKT optimization algorithm, all calcu-
lations of input have complexity O(n), whereas the body of the algorithm (steps 1-
3) is O(l2·2l). Since ﬁxed number l of variance parameters in matrix Dν = Cov{Y }

14

Martina Hanˇcov´a et al.

is usually much smaller than n (typically l2 · 2l is of O(n)), then the complete al-
gorithm has the leading order O(n) with respect to n.

Table 3: Scheme of the KKT algorithm for NN-(M)DOOLSE ˜ν+ in orthogonal FDSLRM.

Input: Form the matrix G, the vector q from vectors e and vj ; j = 1, . . . , l.

For each auxiliary vector b = (b1, b2, . . . , bl)(cid:48) ∈ {0, 1}l do:
1. Set the KKT-conditions matrix K:

K ← G;
For j = 1, 2, . . . , l do: If bj = 0 then K0j ← 0, Kjj ← −1.

2. Calculate the auxiliary vector γ:

γ ← K−1q.

3. Test non-negativity of γ:
If γ ≥ 0 then quit.

Output: Use the last b, γ to form NN-(M)DOOLSE ˜ν+ of ν:

˜ν+ ← γ;
For j = 1, 2, . . . , l do: If bj = 0 then ˜ν+j ← 0.

It is the same as in the case of NE and generally typical only for computationally
fastest analytical solutions of the KKT conditions.

Computational tools for orthogonal FDSLRM

In nonlinear optimization, there is a variety of highly eﬃcient, fast and reliable
open-source and commercial software packages [12, chap. 20]. We have chosen one
of the most well-known open source libraries for solving convex optimization tasks
– CVXPY [16] and its R version CVXR [17].

CVXPY is a scientiﬁc Python library but also a language with very sim-
ple, readable syntax not requiring any expertise in convex optimization and its
PC implementation. CVXPY allows the user to specify the mathematical opti-
mization problem naturally following normal mathematical notation as we can
see in computing NN-DOOLSE (ﬁg. 1) where a code easily mimics the non-
negative DOOLSE mathematical formulation (4.4) with Σν = σ2
0In + VDV(cid:48) =
ν0In + Vdiag {νj} V(cid:48).

CVXPY, implements not only convex optimization solvers using interior-point
numerical methods which are extremely reliable and fast, but also veriﬁes convex-
ity of the given problem using rules of disciplined convex programming [25]. In
FDSLRM, interior-points numerical methods have complexity n3 for one iteration
or log(1/ε) times bigger for the complete computation with a precision ε of the
required optimal solution [6].

Remark 6
It is worth to mention that both CVXPY and CVXR are able to solve non-negative DOOLSE
and MDOOLSE in a general FDSLRM without the orthogonality condition. CVXPY was
inspired by MATLAB optimization package CVX [24] still used in many references, e.g. [12].
However, in CVXPY (CVXR) the user can easily combine convex optimization and simplicity
of Python (R) language together with its high-level features such as object-oriented design or
parallelism.

Estimating variances in FDSLRMs via EBLUPs and convex optimization

15

Fig. 1: CVXPY code for computing DOOLSE in Jupyter environment.

4.3 Gaussian orthogonal FDSLRM

In the LMM framework, the most usual X distribution used in practice is repre-
sented by the multivariate normal (Gaussian) distribution. Therefore, in addition
to the orthogonality of FDSLRM, we will require in this section the distributional
assumption X ∼ Nn(Fβ, Σν). We will refer to FDSLRM under these assumptions
as to Gaussian orthogonal FDSLRM.

In the case of Gaussian orthogonal FDSLRM, we can add to our investigation
last two estimation methods from tab. 2: maximum likelihood estimators (MLE)
and residual maximum likelihood estimators (REMLE).

Maximum likelihood estimators – MLE, REMLE

Both MLE and REMLE of variances ν provide estimates maximizing ML and
REML loglikelihood functions (logarithms of likelihoods). Using simple and clear
arguments from Lamotte’s paper [36], we can easily form the ML loglikelihood
function lm(ν, x) and REML loglikelihood function lr(ν, x), assuming general (not
necessarily orthogonal) FDSLRM observation X (2.1), as

lm(ν, x) = 1
2

(cid:16)
ln det(Σ−1

ν ) − (cid:13)

lr(ν, x) = 1
2

(cid:16)
ln det(Σ−1

ν ) − ln det(F(cid:48)Σ−1

(cid:17)

(cid:13)x − Fβ∗
ν
ν F) − (cid:13)

(cid:13)
2
(cid:13)
Σ−1
ν
(cid:13)x − Fβ∗
ν

(4.8)

(4.9)

(cid:17)

(cid:13)
2
(cid:13)
Σ−1
ν

where eν = x − Fβ∗
ν are FDSLRM residuals, known as marginal residuals in
LMM [53], x is an arbitrary realization of the FDSLRM observation X with co-
variance matrix Σν, β∗
ν (•) is
a generalized vector norm given by Σ−1
ν .

ν is BLUE of β in MME (3.2) and (cid:107)•(cid:107)2

= (•)(cid:48)Σ−1

Σ−1
ν

16

Martina Hanˇcov´a et al.

Remark 7
In Lammote’s paper [36], which presents a direct derivation of REML likelihood function
in LMM using familiar linear algebra operations, we can ﬁnd that the formulation of ML
and REML likelihood in LMM under the assumption of normality was originally done in
Harville’s works [27], [28]. However, LMM references up to 2007 show the explicit form of
REML likelihood for LMM rarely. In ˇStulajter’s FDSLRM reference works dealing with MLE
[57], [59] REML likelihood for FDSLRM is also absent. According to Lammote [36], the reason
why seems to be Harville’s too sophisticated, diﬃcult and indirect derivation.

Maximum likelihood and residual maximum likelihood estimation of variance
components in LMMs produce, in general, no analytical expressions for the esti-
mators [50, chap. 6]. They exist only for ν0 > 0. According to [15, sec. 2.5, thm 4]
MLE and REMLE estimates do not exist in general FDSLRM with LMM obser-
vation X (2.1) if and only if a realization x of X belongs to L (F, V), linear span
of columns F and V. This condition is also equivalent with e ∈ L (V).

In orthogonal FDSLRM, the MLE and REMLE can be rewritten as the op-
timization problems for ν ([1], [6]) at given OLS residuals e (for orthogonal
FDSLRM e = eν, [59])

MLE

maximize

subject to

REMLE

maximize

subject to

ν ) − (cid:107)e(cid:107)2
f0(ν) = ln det(Σ−1
Σ−1
ν
ν = (ν0, . . . , νl)(cid:48) ∈ [0, ∞)l+1

f0(ν) = ln det(Σ−1
ν = (ν0, . . . , νl)(cid:48) ∈ [0, ∞)l+1

ν ) − ln det(F(cid:48)Σ−1

ν F) − (cid:107)e(cid:107)2

Σ−1
ν

(4.10)

(4.11)

where (cid:107)e(cid:107)2
in deﬁnitions (4.4), (4.5) of (M)DOOLSE.

ν e = tr (cid:0)SeΣ−1

= e(cid:48)Σ−1

Σ−1
ν

ν

(cid:1) and Se is the residual products matrix used

Concerning convex optimization, (RE)MLE problems (4.11), (4.10) are gen-
erally not convex problems. However in orthogonal FDSLRM, according to [6,
sec. 4.1.3], using an appropriate bijective transformation, we can reformulate
(RE)MLE as equivalent convex optimization problems (see Proposition 5).

Employing the theory of convex optimization, we can prove that equivalent
(RE)MLE convex problems are strictly convex, for e (cid:54)∈ L (V) have always a unique
solution, unique global minimizer satisfying corresponding KKT conditions (see
Propositions 7, 6 and their proofs in the Appendix). These conditions are again
necessary and suﬃcient for optimal solutions.

Combining all our theoretical results, we arrive to the main theoretical result
of the section. It can be shown that for ν0 (cid:54)= 0 (or e (cid:54)∈ L (V)) the KKT conditions
for NN-(M)DOOLSE are equivalent with the KKT conditions for (RE)MLE and
the bijective transformation. Or in other words, we can formulate the following
theorem.

Theorem 2 (equivalence between NN-(M)DOOLSE and (RE)MLE)
In a Gaussian orthogonal FDSLRM non-negative (M)DOOLSE are almost sure
equal to (RE)MLE, i.e.
(cid:18)non-negative
DOOLSE

(cid:18)non-negative
MDOOLSE

= REMLE

= MLE

= 1.

= P

(cid:19)

(cid:19)

P

Proof See the Appendix.

Estimating variances in FDSLRMs via EBLUPs and convex optimization

17

Computational tools for Gaussian orthogonal FDSLRM

The most important and useful consequence of Theorem 2 is that we can compute
MLE or REMLE in orthogonal FDSLRM as quadratic NN-(M)DOOLSE. There-
fore, we can use our KKT algorithm of order O(n) or computational tools CVXPY
(CVXR) with O(n3) described in the previous section. Moreover, for e ∈ L (V)
NN-(M)DOOLSE do not fail as (RE)MLE, but naturally extend them.

Since a Gaussian orthogonal FDSLRM is also Gaussian LMM, we can also
apply current LMM packages. In our previous paper [21], we stated that no current
package in R is directly and eﬀectively suitable for FDSLRM. Thanks to a detailed
study of [15], [22] we were successfully directed and instructed how to implement
the FDSLRM variance structure into one of the best R packages for LMM known
as nlme ([41], [42]). After that, inspired by nlme, we found another R package
called sommer , also suitable for FDSLRM ﬁtting ([13], [12]).

Simultaneously, thanks to online computing environment CoCalc for SageMath
[55] with the possibility to run computations and codes for free in many other
programming languages and open softwares, we are also able to run and test
MATLAB function mixed ([62], [61]) primarily intended for estimating variances
in LMMs. On the basis of successful tests, we wrote its R version called MMEinR,
which is also available in our GitHub Repository [18].

Remark 8
The mentioned LMM packages can also handle (RE)MLE in the general FDSLRM without
orthogonality restriction. The packages use iterative methods based on EM algorithm or Hen-
derson MME together with some version of the Newton-Raphson method whose complexity is
generally at least n3.

We are fully aware that another eﬃcient implementation for estimating variances in LMMs
provides lme4 package [2] or SAS package PROC MIXED [56], [49]. As for lme4, which is much
faster than nlme, we found that the package does not allow implementing FDSLRM using lme4
standard input procedure.

Finally, as SAS laymans, we also tried a university edition of SAS, free from 2014, but
we left it after running into initial problems with Windows installation process in a virtual
machine environment and subsequently with a more sophisticated programming language than
Python or R. Therefore our knowledge with SAS remained only at the theoretical level.

5 Application in real data examples

5.1 Three real data sets and their FDSLRMs

We illustrate the obtained theoretical results and the performance of the proposed
EBLUP-NE method of variances ν in the FDSLRM (1.1) using three real data sets:
electricity consumption, tourism and cyber security. For all data sets, we identiﬁed
the most parsimonious structure of the FDSLRM using an iterative process of
the model building and selection based on exploratory tools of spectral analysis
and LMM theory. Details of our analysis and modeling can be found in our easy
reproducible Jupyter notebooks freely available at our GitHub repository [19].

Electricity consumption

As the ﬁrst real data example, we have the econometric time series data set,
representing hourly observations of the consumption of the electric energy (in
kWh) in an department store. The number of time series observations is n = 24.

18

Martina Hanˇcov´a et al.

The data was adapted from [59]. The consumption data can be ﬁtted by the
following Gaussian orthogonal FDSLRM [59]:

X(t) =β1 + β2 cos(ω1t) + β3 sin(ω1t)+

+ Y1 cos(ω2t) + Y2 sin(ω2t)+
+ Y3 cos(ω3t) + Y4 sin(ω3t) + w(t), t ∈ N

(5.12)

1, σ2

0, σ2

3, σ2

2, σ2

4)(cid:48) ∈ R5

with k = 3, l = 4, Y = (Y1, Y2, Y3, Y4)(cid:48) ∼ N4(04, Dν), w(t) ∼ iid N (0, σ2
0)
and ν = (σ2
+. Frequencies ω1, ω2, ω3 are suitable Fourier
frequencies from the time series periodogram in spectral analysis ([43], [59], [21]).
Since the time series data set contains only 24 observations and we have to esti-
mate three regression and ﬁve variance parameters of the FDSLRM, this real data
FDSLRM example should be considered only as a toy example. Taking two sets of
Fourier frequencies (2π/24, 2π · 3/24, 2π · 4/24) or (2π/24, 2π · 2/24, 2π · 3/24), we
get two toy models previously introduced in [21] and [59]. These models give us the
opportunity to check our numerical results and to demonstrate how in principle
FDSLRM estimation methods and computational tools work.

Tourism

In this econometric FDSLRM application, we consider the time series data set,
called visnights, representing total quarterly visitor nights (in millions) from 1998-
2016 in one of the regions of Australia – inner zone of Victoria state. The number
of time series observations is n = 76. The data was adapted from [30].

The Gaussian orthogonal FDSLRM ﬁtting the tourism data has the following

form (for details see our Jupyter notebook tourism.ipynb):

X(t) =β1 + β2 cos (cid:0) 2πt
+ Y1 cos (cid:0) 2πt·19

76

76

(cid:1) + β3 sin (cid:0) 2πt·2
(cid:1) + Y2 sin (cid:0) 2πt·19

(cid:1) +
(cid:1) + Y3 cos (cid:0) 2πt·38

76

76

76

(5.13)

(cid:1) + w(t), t ∈ N

with k = 3, l = 3, Y = (Y1, Y2, Y3)(cid:48) ∼ N3(03, Dν) and ν = (σ2

0, σ2

1, σ2

2, σ2

3)(cid:48) ∈ R4
+.

Cyber attacks

Our ﬁnal FDSLRM application describes the real time series data set representing
the total weekly number of cyber attacks against a honeynet – an unconventional
tool which mimics real systems connected to Internet, like business or school com-
puters intranets, to study methods, tools and goals of cyber attackers. Data, taken
from [54], were collected from November 2014 to May 2016 in CZ.NIC honeynet
consisting of Kippo honeypots in medium-interaction mode. The number of time
series observations is n = 72.

The suitable FDSLRM, after a preliminary logarithmic transformation of data
Z(t) = log X(t), is again Gaussian orthogonal (for details see our Jupyter notebook
cyberattacks.ipynb) and in comparison with previous models (5.12), (5.13) has
the simplest structure:

Z(t) =β1 + β2 cos (cid:0) 2πt
+ Y1 cos (cid:0) 2π·14t

72

72

(cid:1) + β3 sin (cid:0) 2π·3t
(cid:1) + Y2 sin (cid:0) 2π·14t

(cid:1) +
(cid:1) + w(t), t ∈ N,

72

72

(5.14)

with k = 3, l = 2, Y = (Y1, Y2)(cid:48) ∼ N2(02, Dν) and ν = (σ2

0, σ2

1, σ2

2)(cid:48) ∈ R3
+.

Estimating variances in FDSLRMs via EBLUPs and convex optimization

19

5.2 Numerical results of estimating ν

For cross-checking purposes, we realized our numerical computations in both Python
and R based software tools (or packages). As we described in previous sections, we
implemented own algorithms and methods in SciPy, SageMath and R, particularly
analytical expressions (4.1) for NE and the KKT algorithm for NN-(M)DOOLSE
(tab. 3) in orthogonal FDSLRM.

Simultaneously, we conﬁrmed the same estimations using CVXPY (or CVXR)
package based on convex optimization and analogically using up-to-date standard
LMM R packages nlme, MMEinR and sommer.

Detailed computational results for all three data sets using all corresponding
relevant tools can be found and reproduced in our collection of Jupyter note-
books (asterisk * represents a name of data and speciﬁc computational tool):
PY-estimation-*.ipynb and R-estimation-*.ipynb.

Table 4 summarizes all types of considered initial estimates and their corre-
sponding EBLUP-NE in all four models. The computations also conﬁrmed Theo-
rem 2 therefore we wrote results for MLE and REMLE only.

Table 4: Results for initial estimates and corresponding EBLUP-NE.

Results for (RE)MLE are identical with NN-(M)DOOLSE.

electricity consumption - toy model 1

method

NE
MLE
REMLE

method

NE
MLE
REMLE

estimate ˜ν

(3.53, 0.37, 1.86, 0.00, 1.26)(cid:48)
(2.86, 0.13, 1.62, 0.00, 1.03)(cid:48)
(3.34, 0.09, 1.59, 0.00, 0.99)(cid:48)

(cid:107)˜ν(cid:107)

4.20
3.45
3.83

EBLUP-NE ˚ν

(3.53, 0.12, 1.39, 0.00, 0.83)(cid:48)
(3.53, 0.05, 1.42, 0.00, 0.84)(cid:48)
(3.53, 0.02, 1.35, 0.00, 0.77)(cid:48)

electricity consumption - toy model 2

estimate ˜ν

(1.09, 2.97, 1.76, 0.37, 1.86)(cid:48)
(0.93, 2.89, 1.68, 0.29, 1.79)(cid:48)
(1.09, 2.87, 1.67, 0.28, 1.77)(cid:48)

(cid:107)˜ν(cid:107)

4.09
3.91
3.93

tourism

EBLUP-NE ˚ν

(1.09, 2.79, 1.59, 0.24, 1.69)(cid:48)
(1.09, 2.81, 1.61, 0.23, 1.71)(cid:48)
(1.09, 2.79, 1.58, 0.21, 1.69)(cid:48)

(cid:107)˚ν(cid:107)

3.89
3.90
3.86

(cid:107)˚ν(cid:107)

3.80
3.83
3.79

NE
MLE
REMLE

(0.108, 0.004, 0.230, 0.022)(cid:48)
(0.103, 0.001, 0.228, 0.021)(cid:48)
(0.108, 0.001, 0.227, 0.021)(cid:48)

0.255
0.251
0.253

(0.108, 0.001, 0.225, 0.020)(cid:48)
(0.108, 0.000, 0.225, 0.020)(cid:48)
(0.108, 0.000, 0.225, 0.020)(cid:48)

0.250
0.250
0.250

cyber attacks

NE
MLE
REMLE

(0.0593, 0.0255, 0.0155)(cid:48)
(0.0560, 0.0239, 0.0139)(cid:48)
(0.0593, 0.0238, 0.0138)(cid:48)

0.0664
0.0624
0.0654

(0.0593, 0.0225, 0.0127)(cid:48)
(0.0593, 0.0225, 0.0125)(cid:48)
(0.0593, 0.0223, 0.0124)(cid:48)

0.0647
0.0647
0.0646

Finally, we point out that thanks to SageMath and our very fast KKT algo-
rithm, we were able to compute (in real time) NN-(M)-DOOLSE in toy examples
with inﬁnite precision – as the exact closed-form (algebraic) numbers. To get an
explicit idea, e.g. for our ﬁrst toy model, we got these results for NN-MDOOLSE
˜ν+ identical with REMLE

20

Martina Hanˇcov´a et al.

˜ν+ =













=

























σ2
0
σ2
1
σ2
2
σ2
3
σ2
4

− 6569
4320
6569
51840
6569
51840

√

√

3
√

√

√

√

3

3

2 − 46513
21600
2 + 46513
259200
2 + 46513
259200

√

√

√

√

3 − 7511
2400
3 + 11291
28800
3 + 2803
3200

2 + 328739
21600
√
2 − 56089
51840
2 − 71213
259200

√

√

3

6569
51840

√

2 + 46513
259200

0
√

3 + 7511
28800

√

2 − 203

259200













≈

























3.339

0.094

0

0.989

1.586

.

It means that we can compute real errors in results for all used computational tools
to explore their quality from the viewpoint of numerical precision. At the same
time, we also watched a run time for particular computational tools using Jupyter
notebook extension ExecuteTime, which provided a preliminary comparison of real
execution times.

6 Conclusions

We suggested and investigated an alternative, new method EBLUP-NE based on
empirical BLUPs for estimating variances in time series modeled by FDSLRM.
The estimation method can be also viewed, analogously like EBLUP ([48], [61]),
as a two-stage iterative method with one step in the iteration.

EBLUP-NE are invariant quadratic, non-negative estimators whose simple
computational form and subsequent ﬁrst and second-moment statistical properties
are given by two special matrices – the Schur complement and Gram matrix [65].
The method can be used not only in the case of normally distributed time series
data, but for any absolutely continuous probability distribution of time series data.
As initial starting estimates for EBLUP-NE, we can principally employ any
of the previously used methods based on least squares (NE, NN-DOOLSE, NN-
MDOOLSE) or maximum likelihood (MLE, REMLE). Applications of FDSLRM
with the EBLUP-NE on three real data sets (electricity consumption, tourism, cy-
ber attacks) providing two toy and two real models indicate that the method com-
putationally gives at least comparable results with REMLE or NN-MDOOLSE,
but in faster run time (approximately 10-1000 times on the standard PC).

Due to lack of computational implementation for FDSLRM modeling, which
would be generally available and readily applicable, and the fact that FDSLRM
least squares and maximum likelihoods estimation procedures are more than 10
years old, we revisited and updated theoretical and computational knowledge deal-
ing with these methods. Speciﬁcally, applying the convex optimization theory, we
reformulated all estimation methods as convex optimization problems in the so-
called orthogonal FDSLRM, the most usual form of FDSLRM used in practice.

We formulated the KKT optimality conditions, which, unlike likelihood equa-
tions, are necessary and suﬃcient conditions for optimal solutions of (RE)MLE or
NN-(M)DOOLSE on extended parametric space Υ = [0, ∞)l+1. KKT optimality
conditions dictate not only the exact existence conditions of estimates but they
also solve the well-known problem dealing with standardly used likelihood equa-
tions [11, chap. 12], [50, chap. 6] in LMM where their solutions for (RE)MLE
or NN-(M)DOOLSE may not be required estimates — they may be out of the
parameter space or they may be other than the maximum or minimum.

Estimating variances in FDSLRMs via EBLUPs and convex optimization

21

Moreover, using KKT optimality conditions in orthogonal FDSLRM, we proved
the equivalence of NN-(M)DOOLSE and (RE)MLE with propability 1. This most
important theoretical result of the paper is the stronger and more general result
than in [59] proved only for interior points of Υ.

Simultaneously, the convex optimization theory brought us to the new KKT
algorithm for computing NN-(M)DOOLSE, equivalent to (RE)MLE, with double
ﬂoating-point precision (cid:15) < 10−15 as the default precision of outputs and with com-
putational complexity O(n). Such an algorithm which we implemented in SciPy,
SageMath and R, which at the default precision level is 107 times more accurate
and approximately n2 faster than the best current Python- or R-based standard
computational tools, can be used in eﬀective computational time series research
to study properties of FDSLRM (Monte Carlo and bootstrap methods, [35]).

Regarding computational aspects, we were also successful in the identiﬁca-
tion and demonstration of consistent results for the real data applications of
FDSLRM in several free, open-source current standard computational tools —
namely CVXPY, CVXR (R version of CVXPY) packages for convex optimization
and LMM R packages nlme, sommer and MMEinR (our R version of Witkovsky’s
MATLAB mixed function). These results and procedures can be freely viewed in
our 16 Jupyter notebooks which are easily readable, sharable, reproducible and
modiﬁable directly in our GitHub repository ([19]). Open-source Jupyter technol-
ogy with Python and R packages also solved our problem stated in [21] that no
current package in R was directly and eﬀectively suitable for FDSLRM.

Finally, our investigation has also brought new questions for further research.
There is deﬁnitely a need for more exact and detailed analysis based on a simu-
lation study and general EBLUP theory ([64], [61], [48]) focusing on EBLUP-NE
quality with respect to previously used estimation methods and the performance of
the EBLUP-NE method itself with diﬀerent initial starting points, using diﬀerent
computational tools in various probability distributions. These research questions
are under our investigation and will be published in the near future.

In connection with our current computational research in FDSLRM, but also
for real time series data analysis and forecasting, we started to build our own R
package (see a preliminary, fully functional version at [20]) on mentioned LMM R
packages to manipulate readily with FDSLRM concepts and procedures.

Finally, our results in the paper can be seen reciprocally as contributions to
convex optimization and LMM methodology. Particularly, our convex optimiza-
tion application in the context of time series modeling has become another one
from a wide variety of application areas of convex optimization [6]. Since FDSLRM
describing n observed time series values is also a special type of LMM, our EBLUP-
NE and the very fast, accurate KKT algorithm to compute (RE)MLE may have po-
tential to be used in computational research and applications dealing with LMMs.

Acknowledgements
We are very grateful especially to Frantiˇsek ˇStulajter (Comenius University in Bratislava,
SK) for his guidance during our early contact and study of FDSLRM concepts and to Viktor
Witkovsk´y (Slovak Academy of Science, SK) for his recommendations and deep insights dealing
with the LMM methodology and its computational tools connected to FDSLRM. We would
also like to thank Ivan ˇZezula and Daniel Klein (both from P. J. ˇSaf´arik University in Koˇsice,
SK) for a valuable feedback dealing with considered estimation methods.

Concerning applied computational tools, we would also like to acknowledge involvement
and recommendations of Giovanny Covarrubias-Pazaran (University of Wisconsin, US) in using
sommer package, Steve Diamond and Stephen Boyd (Stanford University, US) in using CVXPY

22

Martina Hanˇcov´a et al.

and CVXR. Finally, in connection with using SageMath, Jupyter and GitHub, our specials
thanks go namely to William Stein (University of Washington, US), Harald Schilly (Universit¨at
Wien, AT), Erik Bray (Universit´e Paris-Sud, FR), Luca de Feo (Universit´e de Versailles Saint-
Quentin-en-Yvelines, FR) and Charles J. Weiss (Augustana University, US).

Appendix

Acronyms and abbreviations

Table 5: List of acronyms used in the paper.

acronym

explanation

speciﬁcation

BLUE
BLUP
BLUP-NE
CVXPY
CVXR
DOOLSE

EBLUP
EBLUP-NE
FDSLRM

KKT
LMM
LRM
MDOOLSE

(M)DOOLSE
MLE
MME
MMEinR
MSE
NE
nlme

NN-DOOLSE
NN-MDOOLSE
NN-(M)DOOLSE

OLS
REMLE

(RE)MLE
SageMath
sommer

SciPy

best linear unbiased estimator
best linear unbiased predictor
natural estimators based on BLUP
Python library for convex optimization
R version of CVXPY
double ordinary least squares estimator
without non-negativity constraints
empirical (plug-in) BLUP
natural estimators based on EBLUP
ﬁnite discrete spectrum
linear regression model
Karush-Kuhn-Tucker
linear mixed model
linear regression model
modiﬁed (unbiased) DOOLSE
without non-negativity constraints
considering both DOOLSE and MDOOLSE
maximum likelihood estimators
Henderson’s mixed model equations
R version of MATLAB function mixed
mean squared error
natural estimators
R package for (non) linear
mixed(-eﬀects) models
non-negative DOOLSE
non-negative modiﬁed DOOLSE
considering both NN-DOOLSE
and NN-MDOOLSE
ordinary least squares
residual (restricted)
maximum likelihood estimator
considering both MLE and REMLE
free Python-based mathematics software
R package for multivariate LMMs
solving MME
Scientiﬁc Python,
Python-based ecosystem of open software

(3.2), p. 6
(3.2), p. 6
(3.1), p. 5
p. 14
p. 14
(4.6), p. 12

p. 10
p. 10
(1.1), p. 2

p. 13
p. 4
p. 2
(4.6), p. 12

Remark 5, p. 12
(4.10), p. 16
(3.2), p. 6
p. 17
p. 2
p. 5, (4.1), p. 11
p. 17

(4.4), p. 12
(4.5), p. 12
Remark 5, p. 12

p. 11
(4.11), p. 16

Remark 5, p. 12
p. 4
p. 17

p. 4

Estimating variances in FDSLRMs via EBLUPs and convex optimization

23

The BLUP-NE method

Proof (Theorem 1)
Employing Lemma 1 and the well-known standard expressions for mean values and covariances
of invariant quadratic estimators (see e.g. [11], [45])) Eν {X (cid:48)AX} = tr(AΣν) and if X ∼
Nn(Fβ, Σν) then Covν {X (cid:48)AX, X (cid:48)BX} = 2 tr(AΣνBΣν), we have

(cid:110)

(cid:111)

(1) Eν

˚σ2
j

= E(Xtj t(cid:48)

j Σνtj ) = (TΣνT(cid:48))jj .
According to (3) of Lema 1 (TΣνT(cid:48))jj = Djj − σ2(W−1)jj = σ2
j Σν) = 2tr(t(cid:48)

j tj Σνtj ) = 2(TΣνT(cid:48))jj

j X) = tr(tj t(cid:48)

j } = 2tr(tj t(cid:48)

j Σν) = tr(t(cid:48)

j Σνtj t(cid:48)

(2) Dν{˚σ2

j − σ2(W−1)jj .

j Σνt(cid:48)
j − σ2(W−1)jj )2.

As a consequence of (1) Dν{˚σ2

j } = 2(σ2

(cid:111)

(cid:110)

iΣνtj t(cid:48)

(3) In a similar way as in (2)
= 2tr(tit(cid:48)
i , ˚σ2
˚σ2
Covν
j
ij = 2(0 − σ2(W−1)ij )2 = 2(σ2(W−1)ij )2.
= 2(TΣνT(cid:48))2
2
− σ2
˚σ2
= (Eν
j )
j
j − σ2(W−1)jj )2 =

(4) M SEν{˚σ2
= (σ2
= σ4(W−1)2

(˚σ2
j } = Eν
j − σ2(W−1)jj − σ2
jj + 2(σ2

j )2(cid:111)
j − σ2
j )2 + 2(σ2
j − σ2(W−1)jj )2.

j Σν) = 2tr(t(cid:48)

iΣνtj t(cid:48)

(cid:110)

(cid:111)

(cid:110)

j Σνti) = 2(t(cid:48)

iΣνtj )2 =

+ Dν{˚σ2

j } =

Existence of inversions for U, HV

To prove nonsingularity of U, HV, it is suﬃcient to show that both matrices have non-zero
determinants. Using idempotence of MF and expression [51, sec. 6.8]

det(λIn − AB) = λn−l det(λIl − BA) for λ (cid:54)= 0, A ∈ Rn×l, B ∈ Rl×n,

(6.15)

we can write for determinants of U, HV

det HV = det(σ2

0Il + V(cid:48)VD) = (−σ2
0Il + V(cid:48)MFMFVD) = (−σ2

0)l−n det(σ2
0)l−n det(σ2

0Il + VDV(cid:48))
0Il + MFVDV(cid:48)MF)

det U = det(σ2

(6.16)

Now we can see that the sum of positive deﬁnite matrix σ2
0Il (σ0 > 0) and each of positive
semideﬁnite matrices VDV(cid:48), MFVDV(cid:48)MF is a positive deﬁnite matrix, whose determinant is
always positive.

Double least squares estimators NN-(M)DOOLSE

Using the standard inner product of matrices deﬁned as (•, •) = tr(•, •), generating the Eu-
clidean norm (cid:107)•(cid:107) of a matrix, and basic properties of the trace function, we can easily rewrite
expressions (4.4), (4.5) for the objective functions as quadratic forms. The particular form of
these quadratic forms in optimization problems (M)DOOLSE is described by the following
proposition.

Proposition 2 (NN-(M)DOOLSE as quadratic optimization problems)
In an orthogonal FDSLRM the NN-(M)DOOLSE problems are convex quadratic optimization
problems in the form

minimize f0(ν) = ν(cid:48)Gν − 2q(cid:48)ν

subject to −Il+1ν (cid:22) 0l+1

(6.17)

q =










e(cid:48)e
(e(cid:48)v1)2
(e(cid:48)v2)2
...
(e(cid:48)vl)2




















, G =

n∗

(cid:107)v1(cid:107)2 (cid:107)v2(cid:107)2 . . . (cid:107)vl(cid:107)2

(cid:107)v1(cid:107)2 (cid:107)v1(cid:107)4
(cid:107)v2(cid:107)2
...
(cid:107)vl(cid:107)2

0
...
0

0

. . .
(cid:107)v2(cid:107)4 . . .
...
0

0
0
...
. . .
. . . (cid:107)vl(cid:107)4











(cid:31) 0

where for NN-DOOLSE: n∗ ≡ n, NN-MDOOLSE: n∗ ≡ n − k,

24

Martina Hanˇcov´a et al.

According to the fundamental KKT theorem of convex optimization [6, chap.5] which
handles convex optimization problems with constraints in the form of inequalities giving a list of
the so-called Karush-Kuhn-Tucker (KKT) optimality conditions, we consider our optimization
problem NN-(M)DOOLSE in the Lagrange multiplier form with Lagrangian L as a sum of the
objective function f0(ν) and a linear combination of multipliers λ = (λ0, λ1, . . . , λl)(cid:48) (cid:23) 0l+1
and constraints −ν = (ν0, ν1, . . . , νl)(cid:48) (cid:22) 0l+1.

Then KKT optimality conditions for (6.17) become necessary and suﬃcient conditions of
optimality, satisﬁed at any local optimal solution, being represented by three sets of conditions:
(1) primal and dual feasibility: ν (cid:23) 0l+1, λ (cid:23) 0l+1, (2) stationarity of the Lagrangian:
∇νL(ν, λ) = 0l+1 and (3) complementary slackness: νj λj = 0, j ∈ {0, 1, . . . , l}. Computing
the gradient of the Lagrangian, we arrive at the following proposition.

Proposition 3 (KKT optimality conditions for NN-(M)DOOLSE)
In an orthogonal FDSLRM consider the non-negative NN-(M)DOOLSE problems in the La-
grange multiplier form with the Lagrangian

L(ν, λ) = f0(ν) −

l
(cid:88)

j=0

λj νj .

Then, a necessary and suﬃcient condition for ˜ν, ˜λ to be problems’ optimal solution is
(1) ν (cid:23) 0l+1,
(cid:1)
(2) (cid:0)G −Il+1
(3) ν ◦ λ = 0l+1

λ (cid:23) 0l+1
(cid:18)ν
λ

(⇔ νj λj = 0, j ∈ {0, 1, . . . , l}).

= q

(cid:19)

As for the existence of optimal solutions, we can apply the well-known basic results for
minimizing quadratic forms [4, chap. 3]. Since the Hessian of (6.17) equal to ∇2
νf0(ν) = 2G
is a positive deﬁnite matrix, like G, then f0(ν) is strictly convex and also coercive proper
function. These two properties are suﬃcient conditions for NN-(M)DOOLSE [4, Weierstrass’
theorem, p. 119] to have always a unique global optimal solution ˜ν.

Employing the familiar Bessel’s inequality

l
(cid:80)
j=1

(e(cid:48)vj / (cid:107)vj (cid:107))2 ≤ (cid:107)e(cid:107)2, where equality holds

if and only if e ∈ L (V), it is also not diﬃcult to see that KKT conditions (1-3) rewritten as
the following system

l
(cid:88)

n∗ν0 +

νj (cid:107)vj (cid:107)2 − λ0 = (cid:107)e(cid:107)2

j=1

ν0 (cid:107)vj (cid:107)2 + νj (cid:107)vj (cid:107)4 − λj = (e(cid:48)vj )2; j = 1, . . . , l

νj λj = 0, λj ≥ 0, νj ≥ 0; j = 0, 1, . . . , l

(6.18)

(6.19)

(6.20)

imply an optimal solution ˜ν with ˜ν0 = 0 if and only if the vector of OLS residuals e belongs
to the column space of V. Since probability of e ∈ L (V) is zero, our existence conclusions can
be summarized in the next proposition.

Proposition 4 (Existence of NN-(M)DOOLSE)
In an orthogonal FDSLRM the following holds

(1) NN-(M)DOOLSE problems are strictly quadratic optimization problems.
(2) Their solutions ˜ν always exist and they are unique global minimizers.
(3) ˜ν0 = 0 ⇔ e ∈ L (V) ⇔ x ∈ L (F, V).
(4) P (˜ν0 = 0) = P (e ∈ L (V)) = P (x ∈ L (F, V)) = 0.

KKT algorithm for NN-(M)DOOLSE

If we consider ν0 = 0 which occurs if and only if e =

l
(cid:80)
j=1

αj vj in KKT conditions (6.18)-(6.20)

then the optimal solution ˜ν+ for NN-(M)DOOLSE is trivial: ν+0 = 0, ν+j = α2

j , j = 1, . . . , l.
For ν0 (cid:54)= 0 implying λ0 = 0, νj and λj cannot be simultaneous zero otherwise it would lead
to contradiction between (6.18) and (6.19). Therefore, in the case ν0 (cid:54)= 0, the complementary

Estimating variances in FDSLRMs via EBLUPs and convex optimization

25

slackness condition (3) νj λj = 0, j ∈ {1, . . . , l} can be rewritten in the form bj νj (1−bj )λj = 0,
where bj is an auxiliary indicator, which is zero if νj = 0, λj (cid:54)= 0 and one if νj (cid:54)= 0, λj = 0.

Using vector b = (b1, b2, . . . , bl)(cid:48) ∈ {0, 1}l, the derived KKT conditions (2) in Proposition
3 can be described by a (l + 1) × (l + 1) matrix function K(b) = {Kij (b)} and a vector function
γ(b) = (γ0(b), γ1(b), . . . , γl(b))(cid:48) as

K(b)γ(b) = q,

(6.21)

where

Kij (b) =






if i = 0, bj = 0,
if i = j (cid:54)= 0, bj = 0,

0,
−1,
Gij , otherwise.

(cid:26)

γj (b) =

ν0,

if j = 0

bj νj + (1 − bj )λj , otherwise.

Applying the Banachiewicz formula, we can write for the inverse of K(b) the following analytic
expression

K(b)−1 = φ−1

(cid:18)

1
−D−1

b GVjl

−b(cid:48)GVD−1

b

φD−1

b + D−1

b GVjlb(cid:48)GVD−1

b

(cid:19)

where

(cid:110)

bj (cid:107)vj (cid:107)4 + bj − 1
Db = diag
, GV = diag
jl = (1, 1, . . . , 1)(cid:48) ∈ Rl, φ = n∗ − b(cid:48)GVD−1

b GVjl.

(cid:107)vj (cid:107)2(cid:111)
(cid:110)

,

(cid:111)

Finally, Proposition 4 guarantees the existence of unique auxiliary vectors ˜γ, ˜b:

˜γ = {γ; γ = K(b)−1q (cid:61) 0, b ∈ {0, 1}l}

˜b = {b; ˜γ = K(b)−1q, b ∈ {0, 1}l}

Based on vectors ˜γ, ˜b, the NN-(M)DOOLSE ˜ν+ = (˜ν+0, ˜ν+1, . . . , ˜ν+l)(cid:48) of ν as a solution of
KKT conditions has the ﬁnal form

˜ν+j =

(cid:26) 0,

if j (cid:54)= 0, bj = 0,

˜γj , otherwise.

Thanks to Proposition 4, it is worth to mention that the matrix system (6.21) includes also
solutions with ν0 = 0.

Maximum likelihood estimators (RE)MLE

Generally, (RE)MLE in FDSLRM are not convex optimization problems with respect to ν and
they do not exist for ν0 = 0 which occurs if and only if e ∈ L (V ). If we apply the following
bijective transformation deﬁned on Υ = (0, ∞) × [0, ∞)l

d0 = 1
ν0

, dj =

νj
ν0(ν0+(cid:107)vj(cid:107)2νj )

, j ∈ {1, . . . , l},

(6.22)

we can convert the (RE)MLE problems in the form of equivalent convex problems whose
solutions can be readily converted to a (RE)MLE solutions by the inverse transformation to
(6.22). Here are more detailed steps of the conversion.
(cid:107)vj (cid:107)2(cid:111)
(cid:110)
In orthogonal FDSLRM (F(cid:48)V = 0, V(cid:48)V = diag

ν [58, lem 2.1] and e(cid:48)Σ−1
ν e

), Σ−1

are equal to

Σ−1

ν = 1
ν0

In −

l
(cid:88)

j=1

νj
ν0(ν0+(cid:107)vj(cid:107)2νj )

vj v(cid:48)

j = d0In − Vdiag {dj } V(cid:48),

(6.23)

(cid:107)e(cid:107)2

Σ−1
ν

= e(cid:48)Σ−1

ν e = d0e(cid:48)e − e(cid:48)V diag {dj } V(cid:48)e.

Using orthogonality conditions (3.9) and expression (6.15), we get for determinants in

(RE)ML loglikelihoods (4.10), (4.11)

det Σ−1

ν = dn−l

0

det(d0Il − V(cid:48)Vdiag {dj }) = dn−l

0

(d0 − dj (cid:107)vj (cid:107)2).

(6.24)

det(F(cid:48)Σ−1

ν F) = det(d0F(cid:48)F) = dk

0 det(F(cid:48)F),
l
(cid:89)

j=1

26

Martina Hanˇcov´a et al.

Since the chosen bijective transformation (6.22) also transforms convex constraints for ν
to convex constraints for d, substituting (6.23), (6.24) into objective functions (4.10), (4.11)
of (RE)MLE, we can formulate the following proposition.

Proposition 5 (Equivalent (RE)MLE convex problems)
Let assume e (cid:54)∈ L (V) and consider a bijective transformation in the following form:

d0 =

1
ν0

, dj =

νj
ν0(ν0 + (cid:107)vj (cid:107)2 νj )

, j ∈ {1, . . . , l}.

Then, in a Gaussian orthogonal FDSLRM the (RE)MLE problems are equivalent to convex
problems:

minimize

f0(d) = −(n∗ − l) ln d0 −

l
(cid:88)

j=1

ln(d0 − dj (cid:107)vj (cid:107)2) + d0e(cid:48)e − e(cid:48)V diag {dj } V(cid:48)e

subject to

d0 > max{dj (cid:107)vj (cid:107)2 , j = 1, . . . , l}
dj ≥ 0, j = 1, . . . l

where for MLE: n∗ ≡ n, and for REMLE: n∗ ≡ n − k.

Once again, we can write the equivalent (RE)MLE problem with 2l constraints in the
corresponding Lagrangian multiplier form based on the following Lagrangian with 2l multipliers
(λ1, . . . , λl, µ1, . . . , µl)(cid:48) = (λ(cid:48), µ(cid:48))(cid:48) ∈ R2l
+

L(d, λ, µ) = f0(d) −

l
(cid:88)

j=1

(cid:2)λj dj + µj (d0 − dj ||vj ||2)(cid:3) .

Then by the direct computation, we obtain KKT conditions for equivalent (RE)MLE de-
scribing (1) primal and dual feasibility for d and (λ(cid:48), µ(cid:48))(cid:48), (2) stationarity of the Lagrangian
(∇dL(d, λ, µ) = 0) and (3) complementary slackness, all described by the next proposition.

Proposition 6 (KKT optimality conditions for equivalent (RE)MLE)
Consider equivalent convex optimization problems to (RE)MLE in the Lagrangian multiplier
form

L(d, λ, µ) = f0(d) −

(cid:2)λj dj + µj (d0 − dj ||vj ||2)(cid:3) .

l
(cid:88)

Then, a necessary and suﬃcient condition for ˆd, ˆλ, ˆµ to be problems’ optimal solution is

j=1

(1) d0 − dj (cid:107)vj (cid:107)2 > 0, dj ≥ 0, λj ≥ 0, µj ≥ 0, j ∈ {1, . . . , l}

(2) ||e||2 −

n∗ − l
d0

−

l
(cid:88)

j=1

(cid:18)

µj +

1
d0 − dj ||vj ||2

(cid:19)

= 0,

||vj ||2
d0 − dj ||vj ||2

− (e(cid:48)vj )2 − λj + µj ||vj ||2 = 0, j ∈ {1, . . . , l}

(3) −dj λj = 0, −(d0 − dj ||vj ||2)µj = 0, j ∈ {1, . . . , l}.

Similarly as in the case of NN-(M)-DOOLSE, the Hessian H = ∇2

df0(d) equal to

n∗−l
d2
0



















+

−

−

−

l
(cid:80)
j=1

1
(dj(cid:107)vj(cid:107)2−d0)
(cid:107)v1(cid:107)2
(d1(cid:107)v1(cid:107)2−d0)2
(cid:107)v2(cid:107)2
(d2(cid:107)v2(cid:107)2−d0)2
...
(cid:107)vl(cid:107)2
(dl(cid:107)vl(cid:107)2−d0)2

2 −

(cid:107)v1(cid:107)2
(d1(cid:107)v1(cid:107)2−d0)2 −

(cid:107)v2(cid:107)2

(d2(cid:107)v2(cid:107)2−d0)2 . . . −

(cid:107)vl(cid:107)2
(dl(cid:107)vl(cid:107)2−d0)2

(cid:107)v1(cid:107)4
(d1(cid:107)v1(cid:107)2−d0)2

0

0
...

0

(cid:107)v2(cid:107)4
(d2(cid:107)v2(cid:107)2−d0)2
...

0

. . .

. . .

· · ·

. . .

0

0
...
(cid:107)vl(cid:107)4
(dl(cid:107)vl(cid:107)2−d0)2



















(cid:31) 0

Estimating variances in FDSLRMs via EBLUPs and convex optimization

27

leads to strict convexity of the problem. Since f0(d) is also coercive, we can summarize the
existence conditions of optimal solutions in the ﬁnal proposition as we did for NN-(M)DOOLSE
problems.

Proposition 7 (Existence of equivalent (RE)MLE)
Let assume e (cid:54)∈ L (V). Then in a Gaussian orthogonal FDSLRM the following holds
(1) Equivalent (RE)MLE problems are strictly convex optimization problems.
(2) The objective function f0(d) is coercive with respect to constraints.
(3) Their solutions ˆd always exist and they are unique global minimizers.

Proof (Theorem 2)
The bijection (6.22) implies d0 − dj (cid:107)vj (cid:107)2 = (ν0 + νj (cid:107)vj (cid:107)2)−1 (cid:54)= 0 which dictates µj = 0 to
satisfy the complementary slackness conditions in KKT (3) and simultaneously forms the new
version of KKT (1-3) of Proposition 6

||e||2 − (n∗ − l)ν0 −

l
(cid:88)

(ν0 + νj (cid:107)vj (cid:107)2) = 0,

j=1

||vj ||2(ν0 + νj (cid:107)vj (cid:107)2) − (e(cid:48)vj )2 − λj = 0, j ∈ {1, . . . , l}

ν0 > 0, −νj λj = 0, νj ≥ 0, λj ≥ 0, j ∈ {1, . . . , l}

(6.25)

(6.26)

(6.27)

But for e (cid:54)∈ L (V), which occurs with probability 1, the system (6.25)-(6.27) is the same as
the system (6.18)-(6.20), which is what we set out to prove.

References

1. Agrawal, A., Verschueren, R., Diamond, S., Boyd, S.: A rewriting system for convex opti-

mization problems. Journal of Control and Decision 5(1), 42–60 (2018)

2. Bates, D., M¨achler, M., Bolker, B., Walker, S.: Fitting Linear Mixed-Eﬀects Models Using

lme4. Journal of Statistical Software 67(1), 1–48 (2015)

3. Beezer, R.A., Bradshaw, R., Grout, J., Stein, W.A.: Sage. In: L. Hogben (ed.) Handbook
of Linear Algebra, 2nd edn., pp. 91–1–91–26. Chapman and Hall/CRC, Boca Raton (2013)
4. Bertsekas, D.P.: Convex Optimization Theory, 1st edn. Athena Scientiﬁc, Belmont (2009)
5. Box, G.E.P., Jenkins, G.M., Reinsel, G.C., Ljung, G.M.: Time Series Analysis: Forecasting

and Control, 5th edn. Wiley, Hoboken (2015)

6. Boyd, S., Vandenberghe, L.: Convex Optimization. 7th printing. Cambridge University

Press, Cambridge (2009)

7. Boyd, S., Vandenberghe, L.: Introduction to Applied Linear Algebra: Vectors, Matrices,

and Least Squares. Cambridge University Press, Cambridge (2018)

8. Brieulle, L., De Feo, L., Doliskani, J., Flori, J.P., Schost, ´E.: Computing isomorphisms and

embeddings of ﬁnite ﬁelds. Math. Comp. 88(317), 1391–1426 (2019)

9. Brockwell, P.J., Davis, R.A.: Time Series: Theory and Methods. Springer, New York

(2009)

10. Brockwell, P.J., Davis, R.A.: Introduction to Time Series and Forecasting, 3rd edn.

Springer, New York (2016)

11. Christensen, R.: Plane Answers to Complex Questions: The Theory of Linear Models, 4th

edn. Springer, New York (2011)

12. Cornu´ejols, G., Pe˜na, J., T¨ut¨unc¨u, R.: Optimization Methods in Finance. Cambridge

University Press, Cambridge (2018)

13. Covarrubias-Pazaran, G.: Genome-Assisted Prediction of Quantitative Traits Using the R

Package sommer. PLOS ONE 11(6), 1–15 (2016)

14. Covarrubias-Pazaran, G.: sommer: Solving Mixed Model Equations in R (2019). https:

//CRAN.R-project.org/package=sommer

15. Demidenko, E.: Mixed Models: Theory and Applications with R, 2nd edn. Wiley, Hoboken,

New Jersey (2013)

16. Diamond, S., Boyd, S.: CVXPY: A Python-Embedded Modeling Language for Convex
Optimization. Journal of Machine Learning Research 17(83), 1–5 (2016). http://www.
cvxpy.org

28

Martina Hanˇcov´a et al.

17. Fu, A., Narasimhan, B., Diamond, S., Miller, J., Boyd, S., Rosenﬁeld, P.K.: CVXR: Dis-
ciplined Convex Optimization (2019). https://CRAN.R-project.org/package=CVXR
18. Gajdoˇs, A.: MMEinR: R-version of Witkovsk´y’s MATLAB implementation of itera-
tive solving of Henderson’s mixed model equations (2019). https://github.com/fdslrm/
MMEinR

19. Gajdoˇs, A., Hanˇc, J., Hanˇcov´a, M.: fdslrm EBLUP-NE (2019). https://github.com/

fdslrm/EBLUP-NE

20. Gajdoˇs, A., Hanˇc, J., Hanˇcov´a, M.: fdslrm R package (2019). https://github.com/fdslrm/

R-package

21. Gajdoˇs, A., Hanˇcov´a, M., Hanˇc, J.: Kriging Methodology and Its Development in Fore-
casting Econometric Time Series. Statistika : Statistics and Economy Journal 97(1), 59–73
(2017)

22. Galecki, A., Burzykowski, T.: Linear Mixed-Eﬀects Models Using R: A Step-by-Step Ap-

proach. Springer, New York (2013)

23. Ghosh, M.: On the Nonexistence of Nonnegative Unbiased Estimators of Variance Com-
ponents. Sankhy¯a: The Indian Journal of Statistics, Series B (1960-2002) 58(3), 360–362
(1996)

24. Grant, M., Boyd, S.: CVX: Matlab Software for Disciplined Convex Programming (2018).

http://cvxr.com/cvx/

25. Grant, M., Boyd, S., Ye, Y.: Disciplined Convex Programming. In: L. Liberti, N. Maculan
(eds.) Global Optimization: From Theory to Implementation. Springer Science & Business
Media, New York (2006)

26. Hanˇcov´a, M.: Natural estimation of variances in a general ﬁnite discrete spectrum linear

regression model. Metrika 67(3), 265–276 (2008)

27. Harville, D.A.: Bayesian Inference for Variance Components Using Only Error Contrasts.

Biometrika 61(2), 383–385 (1974)

28. Harville, D.A.: Maximum Likelihood Approaches to Variance Component Estimation and
to Related Problems. Journal of the American Statistical Association 72(358), 320–338
(1977)

29. Henderson, C.R., Kempthorne, O., Searle, S.R., von Krosigk, C.M.: The Estimation of
Environmental and Genetic Trends from Records Subject to Culling. Biometrics 15(2),
192–218 (1959)

30. Hyndman, R.J., Athanasopoulos, G.: Forecasting: principles and practice, 2nd edn.

OTexts, Monash University (2018)

31. Jones, E., Oliphant, T., Peterson, P., others: SciPy: Open source scientiﬁc tools for Python

(2001-). http://www.scipy.org/

32. Kedem, B., Fokianos, K.: Regression Models for Time Series Analysis. John Wiley & Sons,

New York (2005)

33. Kluyver, T., Ragan-Kelley, B., Perez, F., Granger, B., Bussonnier, M., Frederic, J., Kel-
ley, K., Hamrick, J., Grout, J., Corlay, S., Ivanov, P., Avila, D., Abdalla, S., Willing,
C.: Jupyter Notebooks – a publishing format for reproducible computational workﬂows.
In: F. Loizides, B. Schmidt (eds.) Positioning and Power in Academic Publishing: Play-
ers, Agents and Agendas. Proceedings of the 20th International Conference on Electronic
Publishing, pp. 87 – 90. Ios Press, Amsterdam (2016)

34. Koenker, R., Mizera, I.: Convex optimization in R. Journal of Statistical Software 60(5),

1–23 (2014)

35. Kreiss, J.P., Lahiri, S.N.: Bootstrap Methods for Time Series.

In: T.S. Rao, S.S. Rao,
C.R. Rao (eds.) Handbook of Statistics, Time Series Analysis: Methods and Applications,
vol. 30, pp. 3–26. North-Holland, Elsevier, Amsterdam (2012)

36. LaMotte, L.R.: A direct derivation of the REML likelihood function. Statist. Papers 48(2),

321–327 (2007)

37. Lima, A., Rossi, L., Musolesi, M.: Coding Together at Scale: GitHub as a Collaborative
Social Network.
In: E. Adar, P. Resnick, M.D. Choudhury, B. Hogan, A.H. Oh (eds.)
Proceedings of the Eighth International Conference on Weblogs and Social Media, ICWSM
2014, Ann Arbor, Michigan, USA, June 1-4, 2014. The AAAI Press, Palo Alto (2014)
38. McLeod, A.I., Yu, H., Mahdi, E.: Time Series with R. In: T.S. Rao, S.S. Rao, C.R. Rao
(eds.) Time Series Analysis: Methods and Applications, vol. 30, pp. 661–712. Elsevier,
Amsterdam (2012)

39. Oliphant, T.E.: Python for Scientiﬁc Computing. Comput Sci Eng 9(3), 10–20 (2007)
40. Percival, D.B., Walden, A.T.: Spectral Analysis for Physical Applications: Multitaper and
Conventional Univariate Techniques. Cambridge University Press, Cambridge (2009)

Estimating variances in FDSLRMs via EBLUPs and convex optimization

29

41. Pinheiro, J., Bates, D.: Mixed-Eﬀects Models in S and S-PLUS. Springer, New York (2009)
42. Pinheiro, J.C., Bates, D., DebRoy, S., Sarkar, D., EISPACK authors, Heiserkamp, S.,
Van Willingen, B., R-core: nlme: Linear and Nonlinear Mixed Eﬀects Models (2018). https:
//CRAN.R-project.org/package=nlme

43. Priestley, M.: Spectral Analysis and Time Series. Elsevier Acad. Press, Amsterdam (2004)
44. Project Jupyter, Bussonnier, M., Forde, J., Freeman, J., Granger, B., Head, T., Holdgraf,
C., Kelley, K., Nalvarte, G., Osheroﬀ, A., Pacer, M., Panda, Y., Perez, F., Ragan-Kelley,
B., Willing, C.: Binder 2.0 – Reproducible, interactive, sharable environments for science
at scale. Proceedings of the 17th Python in Science Conference pp. 113–120 (2018)
45. Puntanen, S., Styan, G.P.H., Isotalo, J.: Formulas Useful for Linear Regression Analysis

and Related Matrix Theory. SpringerBriefs in Statistics. Springer, Berlin (2013)

46. R Development Core Team: R: A language and environment for statistical computing

(2019). http://www.r-project.org/

47. Rao, C.R., Kleﬀe, J.: Estimation of variance components and applications. North-Holland,

Amsterdam (1988)

48. Rao, J.N.K., Molina, I.: Small Area Estimation, 2nd edn. Wiley, Hoboken, New Jersey

(2015)

49. SAS Institute Inc.: SAS/STAT 15.1 User’s Guide: The MIXED procedure. SAS Institute

Inc., Cary, NC (2018)

50. Searle, S.R., Casella, G., McCulloch, C.E.: Variance components. John Wiley & Sons,

Hoboken (2009)

51. Searle, S.R., Khuri, A.I.: Matrix Algebra Useful for Statistics, 2nd edn. Wiley, Hoboken

(2017)

52. Shumway, R.H., Stoﬀer, D.S.: Time Series Analysis and Its Applications: With R Exam-
ples, 4th edn. Springer Texts in Statistics. Springer International Publishing, New York
(2017)

53. Singer, J.M., Rocha, F.M.M., Nobre, J.S.: Graphical Tools for Detecting Departures from
Int Stat Rev. 85(2),

Linear Mixed Model Assumptions and Some Remedial Measures.
290–324 (2017)

54. Sokol, P., Gajdoˇs, A.: Prediction of Attacks Against Honeynet Based on Time Series Mod-
eling. In: R. Silhavy, P. Silhavy, Z. Prokopova (eds.) Applied Computational Intelligence
and Mathematical Methods, Advances in Intelligent Systems and Computing, pp. 360–371.
Springer International Publishing (2018)

55. Stein, W.A., The Sage Developers: Sagemath, the Sage Mathematics Software System

(Version 8.3) (2018). http://www.sagemath.org, https://cocalc.com/

56. Stroup, W.W., Milliken, G.A., Claassen, E.A., Wolﬁnger, R.D.: SAS for Mixed Models:

Introduction and Basic Applications. SAS Institute, Cary (2018)

57. ˇStulajter, F.: Predictions in Time Series Using Regression Models. Springer, New York

(2002)

58. ˇStulajter, F.: The MSE of the BLUP in a ﬁnite discrete spectrum LRM. Tatra Mountains

Mathematical Publications 26(1), 125–131 (2003)

59. ˇStulajter, F., Witkovsk´y, V.: Estimation of variances in orthogonal ﬁnite discrete spectrum

linear regression models. Metrika 60(2), 105–118 (2004)

60. Weiss, C.J.: Scientiﬁc Computing for Chemists: An Undergraduate Course in Simulations,

Data Processing, and Visualization. J. Chem. Educ. 94(5), 592–597 (2017)

61. Witkovsk´y, V.: Estimation, Testing, and Prediction Regions of the Fixed and Random
Eﬀects by Solving the Henderson’s Mixed Model Equations. Measurement Science Review
12(6), 234–248 (2012)

62. Witkovsk´y, V.: mixed – File Exchange – MATLAB Central (2018).

https://www.

mathworks.com/matlabcentral/ﬁleexchange/200

63. Wu, W.B., Xiao, H.: Covariance Matrix Estimation in Time Series. In: T.S. Rao, S.S. Rao,
C.R. Rao (eds.) Handbook of Statistics, Time Series Analysis: Methods and Applications,
vol. 30, pp. 187–209. North-Holland, Elsevier, Amsterdam (2012)
˙Z¸ad(cid:32)lo, T.: On MSE of EBLUP. Statist. Papers 50(1), 101–118 (2009)

64.
65. Zhang, F.: The Schur Complement and Its Applications. Springer, New York (2005)
66. Zimmermann, P., Casamayou, A., Cohen, N., Connan, G., Dumont, T., Fousse, L., Maltey,
F., Meulien, M., Mezzarobba, M., Pernet, C., Thi´ery, N.M., Bray, E., Cremona, J., Forets,
M., Ghitza, A., Thomas, H.: Computational Mathematics with SageMath. SIAM, Philadel-
phia (2018)

