Modelling Direct Messaging Networks with
Multiple Recipients for Cyber Deception

Kristen Moore∗†, Cody James Christopher∗†, David Liebowitz‡§, Surya Nepal∗†, Renee Selvey¶
∗Data61, CSIRO
Australia
{ﬁrst.last}@data61.csiro.au

§UNSW
Sydney, Australia
{ﬁrst.last}@unsw.edu.au

‡Penten Pty Ltd
Canberra, Australia
{ﬁrst.last}@penten.com

†Cyber Security
Cooperative Research
Centre Australia

¶ANU
Canberra
Australia

1
2
0
2

v
o
N
1
2

]

R
C
.
s
c
[

1
v
2
3
9
1
1
.
1
1
1
2
:
v
i
X
r
a

Abstract—Cyber deception is emerging as a promising ap-
proach to defending networks and systems against attackers and
data thieves. However, despite being relatively cheap to deploy [1],
the generation of realistic content at scale is very costly, due to
the fact that rich, interactive deceptive technologies are largely
hand-crafted. With recent improvements in Machine Learning,
we now have the opportunity to bring scale and automation
to the creation of realistic and enticing simulated content. In
this work, we propose a framework to automate the generation
of email and instant messaging-style group communications at
scale. Such messaging platforms within organisations contain
a lot of valuable information inside private communications
and document attachments, making them an enticing target for
an adversary. We address two key aspects of simulating this
type of system: modelling when and with whom participants
communicate, and generating topical, multi-party text to populate
simulated conversation threads. We present the LogNormMix-
Net Temporal Point Process as an approach to the ﬁrst of these,
building upon the intensity-free modeling approach of Shchur
et al. [2] to create a generative model for unicast and multi-
cast communications. We demonstrate the use of ﬁne-tuned,
pre-trained language models to generate convincing multi-party
conversation threads. A live email server is simulated by uniting
our LogNormMix-Net TPP (to generate the communication
timestamp, sender and recipients) with the language model,
which generates the contents of the multi-party email threads.
We evaluate the generated content with respect to a number
of realism-based properties, that encourage a model to learn to
generate content that will engage the attention of an adversary
to achieve a deception outcome. Our simulations run in real
time, making them suitable for deployment in cyber deception
as a honeypot in its own right, or as part of a larger deception
environment.

I. INTRODUCTION

This work is motivated by its application in cyber deception,
an emerging approach to defending networks and systems
against attack. The predominant tool of cyber deception is the
honeypot [3], which is typically a fake system or digital media
artefact which alerts defenders to breaches when attackers
probe or interact with it in some other way. Honeypots can
also provide information about the intent, and tactics, tools and
procedures (TTPs) of attackers if they are high interaction [4],
with realistic detail and plausible behaviours. In particular, the
content and behaviour of deceptive components, devices and
data must all be similar enough to their real counterparts, so
that they cannot be easily identiﬁed by attackers [1].

A famous example of the use of simulated communications
is in Cliff Stoll’s creation of a deceptive research lab group

in the Lawrence Berkeley National Laboratory in the late
1980s. Stoll created a fake communication network associated
with the lab, including bureaucrats and military personnel, and
then created fake correspondences between them. In addition
to the communications, deceptive documents and forms were
deployed, which were all created by hand to trap an intruder on
the lab’s network. Stoll had been observing the intruder, and so
was able to anticipate aspects of their goals and perceptions,
and accordingly write communications and honeyﬁles with
content to match their interests, and entice the interactions
necessary to trace their location.

Despite the appetite for and demonstrated success of con-
vincing simulations in cyber deception, their uptake has been
inhibited by the fact that realistic generation at scale is very
costly. Current deception technologies and tools exist on a
continuum of sophistication, with simple but automated tools
at one end and detailed, realistic and largely hand-crafted traps
at the other. With recent improvements in Machine Learning
we now have the opportunity to bring scale and automation to
the creation of realistic and enticing simulated content.

This work investigates a model of what we will call a direct
messaging network or DMN, encompassing both email and
Instant Messaging (IM). IM apps now have more users than so-
cial network platforms [5], and though IM may have taken over
e-mail for social communications, within organisations e-mail
has evolved to become much more than just communications.
Many E-mail platforms are integrated with task management
tools, and content management systems like Sharepoint, which
encourage the sharing of important documents in the form
of e-mail attachments. This also results in users using e-
mail servers for personal information management, leaving
information items as attachments in order to retrieve them
later. For these reasons, simulated DMNs are a valuable
addition to cyber deception systems. From the viewpoint of
an adversary, they make an enticing target for discovering a
company’s secrets, and accessing important documents that are
shared between employees as attachments.

IM and email are very similar from a modelling perspective.
Communications in both originate with a message from a
member of a network, addressed to a group of one or more
members, often with a sequence of responses conﬁned to that
group. In this work, we model two separate but related aspects
of a DMN:

1) when and with whom participants communicate, and

 
 
 
 
 
 
2) the content of sequences of messages (threads or chan-

nels).

We can simulate a DMN with the artefacts generated by these
models, populating a messaging server with messages sent by
agents on a network.

Existing ML approaches [6] can model these two tasks, but
there are limitations. Since DMNs model multi-cast commu-
nications, the natural setting to address the ﬁrst of the DMN
modelling tasks is to represent Direct Messages (DMs) as
timestamped, directed hyperedges on the social network graph.
Here the hyperedges are directed from one sender to one or
more recipients. Accurate modeling of DMs therefore requires
careful treatment of these hyperedges. Existing approaches to
modeling DMs in the Temporal Point Process (TPP) literat-
ure [6], [7] model hyperedges as collections of independent
edges. Formally, this is done by modeling edge weights for
all pairs of nodes in the network, and then using binary
classiﬁcation independently on each edge incident
to the
sender node to decide whether or not to include the prospective
recipient node in the recipient set. This dyadic network repres-
entation is a simpliﬁcation with analytical beneﬁts, but higher-
order dependencies seen in polyadic interactions like multi-
cast DM communications can not be adequately expressed
by these pairwise models [8]. In particular, assembling the
recipient set through independent selection can lead to over or
underestimation of event rates for DMs with more than one
recipient, and can also result in the construction of invalid
participant sets that were not seen in the training data. To
overcome this limitation, and increase the realism of generated
deceptive communications, we propose the LogNormMix-Net
Temporal Point Process, extending the intensity-free model
developed by Shchur et al. [2]. Our LogNormMix-Net learns
edge weights for each hyperedge, as opposed to each pairwise
edge as in prior works.

Our approach to generating the content of communication
threads leverages recent advances in deep learning based
language models. There are a number of large pre-trained
language generation models in the literature that are capable of
generating coherent and grammatically correct textual content
[9]–[13]. Open AI’s GPT-3 model [12], which was released
in 2020, has been found to be so good that without training,
evaluators distinguished between GPT-3 and human-authored
text at random chance level [14]. By ﬁne-tuning such a
pre-trained language model on an organisation’s email or
messaging dataset, communications can be generated that, in
isolation, sound appropriate to the organisation. However this
approach alone is not enough to simulate an ofﬁce network
where each individual will have consistent topics and themes
to their communications, that are appropriate to their role
in the organisation, and where communication threads are
coherent and stay on topic. Such details are important to
capture in order to fool increasingly sophisticated, AI-enabled
adversaries. In order to achieve this, we propose a pipeline
for ﬁne-tuning large generative language models to generate
coherent, topical, multi-party conversation threads.
is

then created by uniting the
LogNormMix-Net TPP model with our multi-party conver-

Our DMN model

sation generation model. We validate our framework on real
life data, and simulate an organisation’s email server, where
each event generated with this system consists of a timestamp,
sender, recipient set, email
thread id, message type (new
thread, reply, or forward), and the text of the communication.
There is limited work in the literature on evaluating the real-
ism of deceptive content and understanding how adversaries
interact with deceptions. We propose a number of properties
to evaluate the realism of the generated TPP content from
our e-mail server simulator, including preserving the distri-
bution of inter-event times, the seasonality of communica-
tion interactions, and the proportion of events from/to each
node/hyperedge in the network.

Our main contribution is a framework for the end-to-end
simulation of DMNs that can generate events in real time,
making it suitable for deployment as a honeypot in its own
right, or in larger deceptive environments. To the best of
our knowledge, our proposed LogNormMix-Net TPP is
the ﬁrst neural TPP that models network communication
events. It is also the ﬁrst of any TPP approach (neural or
parametric) where the recipients for multi-cast events are not
independently sampled.

II. BACKGROUND

There are two distinct components to our solution for
simulating deceptive DMNs. In the ﬁrst part we propose
a neural TPP approach for modelling DMs within a social
network. To develop our neural TPP, we have drawn upon
some foundational concepts from state-of-the-art methods re-
ported in the literature. This includes modeling communic-
ation network event streams with TPPs, and intensity-free
approaches to learning TPP models. Our second task is to
generate text content, for which we propose a pipeline to ﬁne-
tune large generative language models to produce coherent,
topical, multi-party conversation threads. This section provides
a brief overview of the background for both tasks.

A. Content generation for cyber deception

As outlined prior, content generation at scale for deception
remains appealing yet under-investigated. With the increasing
sophistication of attackers, there is a growing body of works
introducing specialised generation models and attempting to
measure their efﬁcacy. As manual generation is expensive and
does not scale, work has focused increasingly on automatic
generation. Numerous algorithmic approaches exist that rely
on existing ﬁles and artefacts [15]–[21], however these suffer
from a variety of drawbacks, including but not limited to
decoherence issues and sensitive information leakage. Recent
works also include models designed to generate coherent and
cohesive long-form texts [22], and realistic software reposit-
ories [23]. Contemporary language models, such as those in
the GPT family [9], [12], [24], enable the generation of novel
and realistic text with appropriate ﬁne-tuning and seeding, in
addition to recent adaption for non-language domains [25]. To
the best of our knowledge, however, no work has been done
in generating multi-party conversation threads.

B. TPP theory and the LogNormMix model

C. TPPs for modeling communication networks

Temporal point processes (TPP) are probabilistic models for
event data with timestamps. Speciﬁcally, a TPP is a random
process whose realization consists of the times, or equivalently
the inter-event arrival times {τi = ti−ti−1}, of isolated events.
The simplest model of such event streams is the Poisson
Process [26], which assumes events occur independently of
each other, with exponentially distributed inter-arrival times.
Many extensions of the Poisson Process have been proposed
to try to capture more complex relationships between event
arrivals. Popular examples include the Cox Process [27],
which consists of a doubly stochastic Poisson process, and the
Hawkes Process [28], which consists of a self-exciting process
that posits that past events temporarily increase the chance of
future events.

A TPP is uniquely deﬁned by its conditional

intensity
function λ∗(t) := λ(t | H(t)), which deﬁnes the rate of event
occurrence given the event history, H(t) = {ti}. (Here the
star is a shorthand notation that denotes the dependency on
past events.) Formally, it’s the probability an event occurs in
the interval [t, t + dt) but not before t

λ∗(t) =

P r(cid:0)event occurs in interval [t, t + dt) | H(t)(cid:1)
dt

Conditional intensity functions provide a convenient way
to specify point processes with a simple predeﬁned behavior.
The main challenge of intensity-based approaches lies in
choosing a good form for the intensity function. Simpler
intensity functions with closed-form log-likelihood tend to
have limited expressiveness. More sophisticated intensity
functions can better capture the dynamics of the system, but
require approximation of the log-likelihood function using
Monte Carlo or or numerical quadrature methods [6], [29],
[30], which results in noisy approximation of gradients when
training the model.

Shchur et al. [2] show that the drawbacks of the intensity-
based TPP modeling approaches can be remedied by directly
learning the conditional probability density function p∗(τ )
of the time τi until
instead of modeling
the next event,
the intensity function. The conditional intensity function can
be expressed in terms of the probability density function
p∗(t) := p(t|H(t)) and its cumulative density function F ∗
as follows

λ∗(t) =

p∗(t)
1 − F ∗(t)

.

The intensity-free learning approach for conditional density
estimation of Shchur et al. [2] utilises normalizing ﬂows
to design ﬂexible and efﬁcient TPP models. The idea of
normalizing ﬂows is to deﬁne a ﬂexible probability distri-
bution by transforming a simple one. They propose using
the LogNormMix model, a log-normal mixture model which
estimates the density p∗(τ ) using mixture distributions. Their
LogNormMix model is implemented using the normalizing
ﬂows framework, but has the additional beneﬁt of closed form
sampling and moment computation.

In addition modelling event times, TPPs can be extended to
model additional event details such as types and/or locations.
When there are two or more event types, each event in the TPP
is described by a tuple (τi, mi), where τi denotes the inter-
arrival time and the event mark mi denotes the event type.
TPPs have been used to model the timestamp and sender of
social network communications, with the mark mi represent-
ing the sender [2], [29], [31]. DMN events are described by a
timestamp, a sender, and one or more recipients, so in order
to model DMN communications this standard TPP framework
needs to be extended to include a recipient selection module.

Accurate modeling of DMNs by adding recipient modeling
treatment
to the classical TPP framework requires careful
of the hyperedges representing multi-cast events. Perry and
Wolfe [7] motivate their DMN model with a TPP intensity
function that can model multi-cast event hyperedges. However
for computational tractability reasons, they break multi-cast
events into separate edges/events using duplication. Further-
more, they do not provide an algorithm for sampling multi-cast
events using their approach.

To improve upon the duplication approach to dealing with
multi-cast events, Kim et al. [6] proposed the hyperedge event
model (HEM). HEM uses a dyadic modeling approach for
recipient selection, placing recipient intensity models on the
network edges. Hyperedges are then assembled as a function of
these dyadic event participant functions. Namely, during event
generation, the inclusion or exclusion of each edge is determ-
ined by drawing from an edge-speciﬁc Bernoulli distribution.
In the typical binary relevance formulation used by HEM, each
single classiﬁer operates independently, without any regard to
other labels. This independent Bernoulli sampling is not ideal
for modeling multi-cast events, as it is likely to result in the
over or underestimation of true event rates, and even allows
for recipient sets that have not been seen in the data to be
sampled. To the best of our knowledge, the HEM of Kim [6]
is the only work in the literature that includes a generative
model for multi-cast events.

All existing works in the literature have used parametric
TPPs to model network communications [6], [7], [31]–[34].
Despite the elegance and simplicity of parametric TPP models,
they assume that all samples obey a single parametric form,
which is too restrictive and idealistic for real-world data. As
a result, neural network approaches have become popular in
the TPP literature in recent years because of their universal
approximation power, which enables them to (at least theor-
etically) generate a good approximation to the ground truth.
To the best of our knowledge, no existing neural TPP work in
the literature has been used to model network communications
including recipients. There are no existing approaches that can
model or generate communication network events involving
both a sender and recipient(s), since they all lack a recipient
selection module. In this work we extend the LogNormMix
model of Shchur [2] (described below) to model DM commu-
nications by adding a recipient module.

D. Conversation generation using language models

F. Related Work

Controlled generation of textual content is a hot topic, and
accordingly there is a multitude of work in this direction in
the literature. One approach to generating conversation text is
using open domain chatbots, such as Google’s Meena [10],
or Facebook’s BlenderBot [11]. They are typically trained on
sequence-to-sequence (Seq2Seq) tasks using encoder-decoder
ML architectures, making them suited to chit-chat or Q&A
tasks. Datasets in this Seq2Seq format tend to have short re-
sponses, and may not be suitable for more general applications.
Another option is to use one of the large pre-trained Trans-
formers, of which OpenAI’s Generative Pre-training (GPT)
models [9], [12], [24] are the most well known. These models
can be used out of the box for language generation. Input
text is given to the GPT model to seed the generation, and
text is generated recursively. Fine tuning can be used to bias
generation to topics of interest.

Plug and Play approaches [35] [36] [37] aim to control
the language generated from large, pre-trained language mod-
els. Dathathri et al. introduced the Plug and Play Language
Model (PPLM) [35] that employs small, custom attribute-
based models to help control the language generated from
general pre-trained language models. This results in cheap,
but still powerful, conditional generative models. The control
extends to the topic and sentiment of text.

E. Simulating communication networks.

DMNs can be modeled by uniting a TPP (to generate the
communication timestamp, sender and recipients) with a lan-
guage model (to generate the contents of the communication).
There are a number of works in the literature that combine a
TPP model with a topic model to model broadcast communic-
ations [38]–[40], however they do not include communication
recipients in the modeling framework. To the best of our know-
ledge, the only work in the literature that models DMN events
and content is that of Kim [6], who introduced the interaction-
partitioned topic model (IPTM). IPTM integrates the HEM
TPP model with the Latent Dirichlet allocation (LDA) topic
model to identify the temporal and textual patterns in the
data. This approach however has a number of limitations. The
shortcomings of HEM’s independent Bernoulli sampling of
recipients is discussed above, and the topic model approach
used by IPTM does not generate human readable conversation
text. They instead use topic modelling to generate a set of word
counts for each word in the vocabulary under consideration.
This is a limitation that makes IPTM inappropriate for use
in cyber deception, as an adversary that inspects a generated
communication will immediately identify the deception.

We deliver a framework for the end-to-end simulation of
DMN communications. Each event generated consists of a
timestamp, sender, recipient set, conversation thread id, and
the text of the communication. Unlike existing work, our
TPP framework directly models hyperedge events, and our
language model generates coherent conversation threads with
topics personalised to the sender initiating the conversation.

a) TPP models of social networks: There are TPP ap-
proaches in the literature designed to model public com-
munications in social networks like Twitter or Sina Weibo
(a Twitter-type platform in mainland China). Since they are
modeling public, broadcast communications, recipients are not
considered. A recent example is the Network Group Hawkes
Process Model (NGH) of Xu et al. [41], which models network
users by embedding network information into the Hawkes
Process modeling framework. NGH captures the inﬂuence of
a user’s connected friends by adding an additional component
to the intensity function, the network intensity, on top of
the usual baseline and triggering kernel terms of the Hawkes
Process. It is assumed that each user in the network comes
from one of G latent groups, according to their dynamic
behavior patterns. Users within a group are assumed to share
the same set of group-level parameters, including the baseline
intensity. Wu et al. [42] proposed the Graph Biased TPP
(GBTPP) for modeling sequential event propagation on a
network graph, such as retweeting by social network users,
or news transmission between websites.

b) Network TPP models that include recipients: Network
TPP approaches model communications between senders and
recipients. An early example of this is by Perry and Wolfe
[7], who used the multivariate Cox process to model email
communications networks. The point processes are placed on
the directed edges of the network to measure the rate of
sending or receiving e-mails between pairs of email users. To
extend this to cover multi-cast interactions between a sender
and receiver set, they propose a multivariate intensity function.
However,
its log-partial-likelihood is complicated, so they
instead use duplication to obtain pairwise interactions from
the multi-cast events, and then evaluate the corresponding log-
partial-likelihood of the pairwise intensity model as a way of
performing approximate inference under the multi-cast model.
Fox et al. [31] presented a parametric Hawkes Process
approach to model e-mail network communications, where the
parameters of the intensity function characterise important e-
mail communication behaviours such as the baseline sending
rates, average reply rates, and average response times. In their
approach, the point processes are placed on the nodes of the
network, and the triggering kernel of the Hawkes Process term
sums over only those messages received by the user, since
the user cannot be inﬂuenced by emails they don’t receive.
Recipients aren’t modeled in their Hawkes framework, and
in the event generation algorithm for the model are sampled
from a categorical distribution determined by counts from the
training dataset. They propose to generate from their Hawks
Process in layers, making it unsuitable for deployment in a
system that requires real time event generation.

The Markov modulated Hawkes Process (MMHP) of Wu
et al. [32] combines Markov modulation with the Hawkes
process in an attempt to address the limitation of the Hawkes
process in capturing “silent periods” and isolated events,
whilst extending the ﬂexibility of the Markov Modulated
Poisson Process (MMPP) of Fischer and Meier-Hellstern [43]
to enable it to capture “bursty periods” of activity. Like in

the multivariate Cox model approach of Perry and Wolfe [7],
pairwise intensity functions are placed on the directed edges
of the networks. The model is motivated in terms of pairwise
interactions in email networks, as well as ﬁghts between pairs
of mice in a network of 12 mice living in a vivarium, and
multi-cast events are not considered.

Ward et al. [33] propose the Cohort Markov-modulated
Hawkes Process (C-MMHP) to improve upon the MMHP for
modeling ﬁghts between pairs of mice. C-MMHP models the
winner effect, where an animal that has experienced previous
wins will continue to win future ﬁghts. This is captured in
their model via a speciﬁc form to the coefﬁcients of the
triggering kernel term in the Hawkes Process component. C-
MMHP also utilises degree correction in the pairwise baseline
intensity to allow the model to better capture individual level
heterogeneity. This work only models pairwise events, and
multi-cast events are not considered.

III. KEY INSIGHTS: MULTI-CAST EVENT MODELING

Temporal Point Processes (TPPs) have been very popular
for modeling various aspects of public, broadcast interactions
on social network platforms like Twitter, Reddit and Yelp [2],
[29], [40]. In this work, we use TPPs to model and simu-
late the private communications within organisations, which
take place on e-mail servers, messaging apps, and business
communications platforms like Teams and Slack. The key
aspect differentiating these two types of communication is the
recipient set. Recipients are not considered when modeling
broadcast, newsfeed-style messages, as everyone following
the person posting the message is considered a recipient by
default. On the other hand, DMs are multi-cast events, where
the sender must select a set of recipients from their social
network every time a message is sent. Accurate modeling of
recipient selection in multi-cast communications is therefore
critical for the simulation of realistic DMN communications
for cyber deception.

Since DM platforms allow for multi-cast communica-
tions, the natural setting for DMs is to represent them as
timestamped, directed hyperedges on the DMN graph. As
discussed previously, the weakness of existing DMN TPP
models [6], [7] is that they model hyperedges as collections
of independent edges. To better understand the implications
of this modelling approach, we consider a toy example of
a ﬁcticious organisation E Corp. Logically, the CEO of E
Corp sends many messages to the Chairman of the Board
and also to their team of C-level executives (including the
chief operating ofﬁcer (COO), chief ﬁnancial ofﬁcer (CFO)
and chief marketing ofﬁcer (CMO)), but they do not typically
send the same messages to both the Chairman and the C-level
executives. In this section we motivate why such higher-order
dependencies can not be adequately expressed by existing
pairwise models, and how our hyperedge modelling approach
can address this limitation.

Figure 1(a) depicts the edge weights learned via the pairwise
approach of existing works, which independently assemble re-
cipient sets using binary classiﬁcation. Figure 1(a) shows that
this approach can capture the fact that the CEO communicates

Figure 1: Graphical representation of two different approaches
to modelling DMs at the ﬁctitious company E Corp. Figure (a)
shows the binary classiﬁcation approach to recipient modeling
is able to capture the pairwise probability of a communication
being sent between 2 nodes. Figure (b) shows the multi-class
classiﬁcation approach is able to capture the probability of a
communication between a sender and an entire recipient set.

with the Chairman and each of the C-level executives roughly
the same amount, illustrated by the fact that the outgoing edges
from v2 are the same width. It is however unable to capture
the fact that the CEO and the Chairman of the Board interact
solely via unicast communications, whereas communications
sent by the CEO to the C-level executives are all multi-cast.
This communication behaviour is clearly evident in Figure
1(b), which illustrates the hyperedge modelling approach
we propose in our LogNormMix-Net model. To address the
limitation of previous works, our model learns edge weights
for each hyperedge. This is achieved by modeling recipient
selection as a multi-class classiﬁcation problem, where the
classes are composed of the distinct recipient sets seen in
training.

To understand how recipients are chosen during event simu-
lation in both approaches, we consider the case where the CEO
at node v2 is sending an email. Under the binary classiﬁcation
approach of Figure 1(a), the model will estimate a pairwise
probability that each of v1, v3, v4 and v5, will be included as a
recipient. An independent Bernoulli sample will then be drawn
from each of these 4 pairwise probabilities to decide whether
or not the respective person/node is included as a recipient.
The problem with this is that each sample is unaware of the
outcome of the other 3 samples, whereas Figure 1(b) shows
that if the Chairman at node v1 is selected as a recipient, then
none of the C-level executives should be chosen. On the other
hand, if one of the C-level executives are chosen, then they
all should be chosen, since the CEO’s only communications
to nodes v3, v4 and v5 are multi-cast messages sent to all of
them together. This example highlights how this multi-label
binary classiﬁcation approach can result in over- or under-
estimation of true event rates for hyperedges involving more
than one recipient, and can even allow for invalid recipient
sets to be selected. Since such higher-order dependencies can
not be expressed by pairwise intensity models, this motivates
our alternative approach to modelling hyperedge events.

Our LogNormMix-Net approach frames recipient selection

as multi-class classiﬁcation problem, conditioned on the sender
of the communication. This enables the model to learn the
probability distribution of the recipient sets, given the sender
- which is akin to learning hyperedge weights for directed
communications between participant sets. A further beneﬁt of
this approach is that only those recipient sets seen in training
are available for the model to select, thereby preventing invalid
recipient sets from being constructed.

IV. LOGNORMMIX-NET FOR MODELING DMNS

In this section, we present the technical details of how
we extend upon the LogNormMix model [2] to create our
LogNormMix-Net network communication TPP model to gen-
erate deceptive communications at scale for cyber defence.

A. Why choose LogNormMix?

Before discussing our model architecture, it is pertinent to
justify our choice to build upon the LogNormMix model [2].
There are 3 properties that are desirable when designing any
TPP model [44]:

(i) Flexibility: a TPP should have the ability to approximate
any probability density on R arbitrarily well, including
multi-modal ones.

(ii) Closed form likelihood: in cases where a closed form of
the likelihood function cannot be computed, approxim-
ation via Monte Carlo or numerical quadrature must be
performed, which is slower and less accurate.

(iii) Closed form sampling: closed form sampling is especially
important for this work. In particular, it is ideal to be
able to draw samples analytically via inversion sampling
[45]. Sampling from a point process where a closed-
form expression of the inter-event time distribution is not
available is difﬁcult to do. A popular approach in this
case is to use the thinning algorithm [46], which involves
generating a sequence of events from a homogeneous
Poisson process with intensity given by the upper bound
of the intensity function you wish to sample from, then
rejecting some of the events so that the sequence follows
the desired point process. If the upper bound on the
intensity function is not tight, a large fraction of samples
will be rejected. This approach is less accurate and slower
than inversion sampling, and is not suitable for parallel
hardware like GPUs.

There are two existing approaches that satisfy all three of
these properties. The ﬁrst is specifying the PDF p∗(t) with
a mixture distribution [2], and the second is with invertible
splines [47]. In this work we choose to build upon the mixture
distribution approach, as it is naturally extendable to network
communication modeling whilst preserving all 3 of these key
properties, whereas the existing invertible splines approach
does not model marked events.

B. Developing a network event TPP model

To model DM communication events in a social network
of size n, we represent an event by the tuple (τi, si, ri),
where τi denotes the inter-arrival time, si denotes the sender
(represented by an integer between 1 and n), and ri denotes

the recipient list, represented by an n-dimensional multi-hot
vector with unit entries denoting the recipients for event i.

We frame network event modeling as a multitask machine
learning problem with 3 tasks: inter-arrival time prediction,
sender prediction, and recipient prediction. Each of these tasks
has its own prediction module, and associated negative log
likelihood loss function:

Ltotal =

(cid:88)

i

lτi + lsi + lri

(1)

The event history, H = {τi, si, ri} is embedded with an RNN
into a ﬁxed-dimensional vector hi ∈ RH , and then input into
each of the 3 modules, as shown in Figure 2.

Figure 2: High level overview of the LogNormMix-Net archi-
tecture for event simulation.

As in the original LogNormMix implementation, inter-event
times are conditionally independent of the event participants,
given the history. When extending the LogNormMix to the
network communication setting, we frame event participant
selection as a sender-driven process, i.e. the sender is chosen
ﬁrst using the sender module. Then given the sender, recipient
selection is formulated as:

1) a multi-class classiﬁcation problem
2) conditioned on the sender

Detailed motivation for this formulation for the recipient
selection module is given in Section III.

C. LogNormMix-Net model architecture

I this section we provide the technical details of each of the

modules in the LogNormMix-Net.

a) Temporal module: As in the LogNormMix model, the
inter-arrival time probability distribution p∗(τ ) is modeled by
a log-normal mixture model

p(τ |ω, µ, σ) =

K
(cid:88)

k=1

ωk

1
√
τ σk

2π

exp −

(log τ − µk)2
2σ2
k

(2)

where K is the number of mixture components, ω denotes
the mixtures weights, and µ and σ are the mixture means and
standard deviations, respectively. In order to help the model
accurately learn the seasonality of DM sending in the training
data, metadata mi is concatenated with the history embedding
vector hi to serve as additional input to the model. The choice
of metadata is dataset speciﬁc - for instance, the distribution
of sending times of emails on a corporate network depends
on the day of the week and whether or not the current time is
within ofﬁce hours.

b) Sender module: The sender module consists of a
feedforward network plus softmax, with the output
logits
deﬁning a categorical distribution across all the nodes in the
network. For prediction tasks, the next sender si+1 is predicted
to be the class with the highest probability. In our experiments
the feedforward network consists of two fully connected layers
separated by a tanh activation layer.

c) Recipient module: The recipient selection module is
an additional module that we have added to the original
LogNormMix model. We formulate recipient selection as a
multi-class classiﬁcation problem by creating an ID for each
of the recipient set combinations seen in the training set.
Then similar to the sender module, a fully connected layer
plus softmax is used to learn a categorical distribution across
all the recipient IDs in the dataset. In order to condition the
recipient selection probability distribution on the sender, we
concatenate the history context embedding with the sender
embedding (ie. the embedding of the ground truth sender),
and pass that as input into the recipient selection module. As
with the sender module, for prediction tasks, the next recipient
ri+1 is predicted to be the class with the highest probability.

generation was used. In particular, our we train 2 generative
langauge models:

1) Email Subject generator model: In cases where the sender
is starting a new e-mail thread, this language model is
used to generate the subject line. For reply or forwarding
e-mails, the subject is inherited from the existing e-mail
thread, and is therefore not generated.

2) Email body generator model: This language model gen-

erates message for the body text for each email.

The process to train and utilize the subject and body

generation models is outlined below.

a) Email Subject generator model:

– Training: the model is trained by ﬁne-tuning a pre-trained
language model on the email subject lines from the Enron
corpus. A set of topic/keywords are also extracted for
each user, that represent topics of interest to them.

– Generation: In order to personalise email subject gener-
ation, the topic of conversation threads is controlled by
seeding the subject generation model with a topic word
that is sampled from the sender’s keywords.

D. Event generation with LogNormMix-Net

b) Email body generator model::

The LogNormMix-Net architecture makes it simple to
sample event streams. The next inter-event time τ is sampled
from the lognormal mixture in equation 2 using the standard
mixture model sampling approach:

1) Sample the mixture component: z ∼ Categorical(ω)

where z is a one-hot vector of size K.
2) Sample from the unit Normal distribution:

ε ∼ N ormal(0, 1).

3) Transform the unit normal sample ε to the lognormal

distribution for mixture component z:
τ = exp(sTz · ε + µTz).

The sender and recipient(s) of the next e-mail is drawn
from their respective categorical distribution, with the sampled
sender being fed as input into the recipient selection module.
A detailed diagram of the use of our LogNormMix-Net model
for event sampling is shown in Figure 11 in Appendix A-A.

V. MULTI-PARTY CONVERSATION GENERATION

The other key task of our DMN simulation solution is
to generate multi-party conversation threads. To do this we
leverage generative pre-trained language models, though our
approach does not depend on any particular model. Due to
the private nature of their contents, limited DM datasets are
available, so we validate our approach by simulating e-mail
threads using the Enron email corpus. Our implementation
requires ﬁne-tuning the pre-trained generative language model
on the Enron email dataset, however this dataset need not be
labelled.

A. E-Mail content generation

A difference between e-mail and other DMN platforms like
Whatsapp, Teams or Slack, is that e-mails include a subject
line. When simulating an e-mail server, our best results were
obtained when an additional, separate model for e-mail subject

– Training: the body generation model is trained by ﬁne-
tuning a pre-trained language model on the email bodies
from the Enron corpus.

– Generation: the email subject and existing email thread
(in cases of reply/fwd emails) are used to seed the
generation of the email body.

B. Assembling e-mail threads

a) Sampling the communication type:

In order to de-
velop realistic e-mail threads, 3 types of communication are
modeled:

(i) "new thread": starting a new conversation thread,
(ii) "reply": replying in an existing conversation thread,
(iii) "fwd": forwarding the existing communication to a new

recipient.

For each sender, we try to ensure the proportions of each
communication type (new-thread, reply, fwd) in the rolling
simulated content resembles the proportions of that sender
from the training data. The pseudocode for email type se-
lection is provided in Appendix B-A.

b) E-mail thread generation: The detailed steps required
to generate an event, consisting of (e-mail ID, communication
type,
thread ID, e-mail subject, e-mail body), assuming
the e-mail sender and recipients are known, are detailed in
Appendix B-B. E-mails with the same thread ID constitute
a communication thread, with the message order within the
tread implied by the ordering of the e-mail ID. We make use
of canned text where appropriate, by sampling from canned
lists for greetings, salutations and for the message text for
forwarded emails. Figure 3 gives a high level overview of the
main components of the framework.

C. Additional implementation options

VI. EXPERIMENTS

a) Increasing the realism of generated content: There
are various strategies that one may take when preparing the
training dataset. To increase the realism of generated text to be
used in cyber deception, it would be ideal to ﬁne-tune the pre-
trained generative language model on on real, but non-sensitive
communications from the organisation being simulated. If real
data is not available, it is possible to use reg-ex and NLP
approaches to create a training data pre-processing script that
can replace Enron-speciﬁc terms like names of entities and
locations, etc., with those relevant to the organisation wishing
to deploy the generated content.

To try to improve the cohesion of communication threads,
a training dataset was created that captures the existing con-
versation thread for each training sample, rather than treating
email bodies independently of any existing communications.
b) Increasing the enticement of generated content: An-
other option is to employ "Plug and Play" language mod-
els. We investigated the Plug and Play Language Model
(PPLM) [35] as an approach to give better control over the
topic of the generated e-mails, with a view to increasing their
enticement. This would be especially useful in cyber deception
when a defender can anticipate a key piece of information
that an adversary would be interested in, or some other
aspect of their goals and perceptions [21]. PPLM enables that
information to be leveraged, by adding relevant words to the
PPLM model’s bag of words (BOW), which then encourages
the model to use those words during generation. An intruder
navigating the ﬁle system, searching for ﬁles worth stealing,
would then surface the simulated decoys that had the keywords
in them [48].

D. End-to-end simulation of social network communications

Our framework for the end-to-end, real-time simulation of
DMNs now follows simply by uniting our LogNormMix-Net
TPP from section IV with the language models from section
V. Figure 3 gives a high level overview of the approach.

Figure 3: Overview of our e-mail generation framework.

To summarise the key steps required to generate an event,
consisting of (timestamp, sender, recipient set, e-mail ID,
communication type, thread ID, e-mail subject, e-mail body);
(a) Generate the timestamp, sender and recipients using the
LogNormMix-Net TPP model, and (b) Generate the e-mail
ID, communication type, thread ID, e-mail subject and e-mail
body as detailed in §V-B.

Our experimental aims are to validate:
1) our choice to use the multi-class classiﬁcation recipient
selection approach, over multi-label binary classiﬁcation,
2) that our end-to-end e-mail simulation framework can
generate convincing content, appropriate for application
in cyber deception.

We present a combination of statistical goodness of ﬁt tests
from the literature, together with realism tests speciﬁc to our
application of generating realistic deceptive content.

There is limited work in the literature on understanding how
adversaries perceive and interact with deceptions, and how the
realism and enticement of deceptive content can be quantiﬁed
[18], [19], [21]. We posit that an adversary who is being
careful not to betray their presence on a system does not have
the luxury of being able to thoroughly inspect every aspect of
a network environment for signs of imperfection which may
be indicative of deceptive content. For this reason, generated
deceptive content need not be perfect, but rather we aim to
generate content that will stand up to moderate scrutiny. In
particular, we want simulated content from our TPP to appear
to be generated from the same distribution as the training data.
We train TPP models on 2 real-world e-mail datasets. Each
dataset consists of multiple sequences of events, and we use
60% of the sequences for train, 20% for dev and 20% for test.
For the generation of email text, we train our model on the
Enron email corpus.

A. Experimental Setup and Metrics

1) LogNormMix-Net evaluation: When training our TPP
models, we minimize the combined negative log-likelihood
(NLL) loss of the inter-event times, the next-sender prediction
and the next-recipient prediction in the training set. We then
evaluate our trained TPP model on 2 tasks: event prediction
and generation.

a) Event Prediction: Event prediction is a classic good-
ness of ﬁt test for TPP models. Though this task is not directly
related to our cyber deception application, we include it in
order to compare the ability of the two different recipient
classiﬁcation approaches to learn the relationship between
senders and the recipient groups they communicate with. In
particular, the recipient prediction task asks the model which
recipient set is likely to be chosen next, given the sender
of the next communication, and the history of all previous
communications.

As is customary in the neural TPP literature, we report
the NLL loss for event time prediction, sender prediction and
recipient prediction on the test set. In addition to this, for the
time prediction task we report the RMSE and MAE, and for
sender and recipient prediction we report the top-1 and top-3
accuracy.

b) Event generation: For this task, we perform 100 sim-
ulations for each dataset for both the LogNorMix-Net and the
alternative binary recipient classiﬁcation model LogNormMix-
BC. We then compare the generated event sequences with the
observed data. To evaluate the temporal realism, we calculate

the Q-Q plot and Earth Mover’s Distance (EMD) of the
inter-arrival time distribution. In addition to this, we perform
seasonality checks by plotting histograms and computing the
EMD of the proportion of emails sent in each of the 24 hours
of the day, and 7 days of week.

To evaluate the realism of the participant selection, we
compute the EMD of the proportion of e-mail sent by each
sender and the proportion of e-mails received by each recipient
group. We also compare the hyperedge sizes of each sender.
c) E-mail content evaluation: The e-mail subject and
body generation models were trained using the Huggingface
implementation of GPT-21. We then created a corpus of e-
mail subjects. For each person in the network, we took the 10
most frequently used words from their keyword list, and then
used each one as a prompt to generate roughly 45 subjects.
This resulted in a set of at least 450 subjects generated per
user. This is used to investigate whether using keywords as
prompts for the subject generation can differentiate sender’s
emails from one another, and give an indication of their role
in the network.

A corpus of e-mail threads was generated following the
Algorithm in §V-B. There are a number of papers in the liter-
ature that have already evaluated the quality of generated text
from large pre-trained language models using user studies [14],
[35]. Our aim is to investigate the coherence of generated
e-mail threads. To do this, we use an approach similar to
that proposed by Karuna et al. [18]. Namely, we compare the
coherence of e-mails with their replies in conversation threads.

B. Datasets

For each of the datasets we extract the timestamp, sender,
recipients, and metadata for each event, where the metadata is
a categorical input we compute from the date-times, that de-
notes whether the event happened during typical ofﬁce hours,
shoulder period (weekday morning and evenings ouside of
regular ofﬁce hours), or during non-working hours (weekends
plus weekday nights).

The EU Email dataset2 consists of the email network
from a large European research institution, divided into 4
departments. To create a smaller network from the department
4 dataset, we selected the 52 members who sent at least
100 emails over the duration of the dataset, and then created
a closed network of the emails sent between them. This
comprised 21,260 emails sent between the 52 employees over
a period of approximately 75 weeks beginning in October
2003. From this we discarded 907 emails that were sent to
recipient groups that received fewer than 10 emails in the
entire dataset. This resulted in a training set of 20,353 emails
sent between 52 employees and 130 different recipient sets.
Of these emails, 68.4% were multi-cast, ie. having 2 or more
recipients. From the 75 weeks of data, 6 weeks of low activity
(less than 80 emails) were discarded, leaving 69 weeks of data
comprising 20,098 emails.

The Enron Email dataset. The Enron dataset was originally
made public, and posted to the web, by the Federal Energy

Regulatory Commission during its investigation into the Enron
corporation. The CMU version of the dataset was used in this
work, which does not include attachments, and has had some
messages deleted "as part of a redaction effort due to requests
from affected employees"3. For our TPP work, we consider the
sender, recipient, and timestamp of each message in a closed
version of the Enron e-mail network containing messages sent
between 148 users. Once duplicates and messages individuals
sent to themselves are removed, the corpus is reduced to
30,021 unique messages. From this, a smaller network was
created by selecting the 54 users who each sent at least 100
emails, and then forming a closed network from those between
164 different recipient groups. This resulted in a dataset of
13,726 unique messages, spanning 144 weeks.

a) Ethics: Due to the sensitivity of the Enron e-mail
dataset, we obtained an ethical clearance for use of the Enron
dataset in our work. Since the other 2 datasets consist only
of timestamps and anonymous participant ID numbers, we
consider them to be fully anonymised.

C. Baseline/comparison models:

a) LogNormMix: We report the performance of the ori-
ginal LogNormMix model for inter-arrival times and sender
prediction on our DM event datasets. This allows us to set
a baseline for temporal and sender prediction performance in
order to demonstrate that adding the recipient prediction task
doesn’t degrade performance on these original tasks too much.
b) Transformer Hawkes: The Transformer Hawkes Pro-
cess (THP) [29] is another SOTA neural TPP model. We report
the RMSE and sender accuracy of this model to serve as a
comparison with the baseline LogNormMix.

c) LogNormMix with binary classiﬁcation: LNM-BC:

In order to compare the performance of the multi-class and
multi-label binary classiﬁcation approaches for the recipient
module, we include LNM-BC, which is the LogNormMix
with a recipient module that consists of the typical multi-label
binary classiﬁcation approach (shown in red in Figure 11).

A. LogNormMix-Net

VII. RESULTS

In section VII-A1 we address our ﬁrst experimental aim,
using the event prediction task to validate the superior ability
of the LogNormMix-Net multi-class recipient classiﬁcation
approach over the alternative multi-label classiﬁcation ap-
proach. Then we address our second experimental aim in
section VII-A2, by evaluating the realism of the LogNormMix-
Net generated content.

1) Model goodness of ﬁt and predictive ability: Since
the temporal and sender modules are identical across both
is
the LogNormMix-Net and LogNormMix-BC models,
reasonable to expect
them to be able to achieve similar
performance for inter-arrival time and sender prediction. We
include results for these modules to validate that adding
the recipient prediction task doesn’t unreasonably degrade
performance on those two original tasks. The main goal of this

it

1https://huggingface.co/transformers/model_doc/gpt2.html
2http://snap.stanford.edu/data/email-Eu-core-temporal.html

3https://www.cs.cmu.edu/~./enron/

Model
THP
LogNormMix
LNM-Net
LNM+BC

Enron

Time RMSE
(hours)
11.5
5.9
6.1
5.9

Time MAE
(hours)
-
3.0
5.3
3.6

EU

Time RMSE
(hours)

3.63
2.96
7.72

Time MAE
(hours)
-
2.41
0.93
6.38

Table I: Event Time prediction on the test set across datasets.

Model
THP
LogNormMix
LNM-Net
LNM+BC

Enron
Recip
Top-1
-
-

Sender
Top-1

Sender
Top-1
9.7%
17.5%
19.4% 33.6% 61.1% 22.4% 40.8% 71.3%
17.7%

Recip
Top-3
-
-

Recip
Top-3
-
-

13.6%

0.00%

0.05%

7.16%

EU
Recip
Top-1
-
-

Table II: Participant prediction accuracy on the test set across
both datasets.

section, though, is to demonstrate that the LogNormMix-Net
can learn the relationship between senders and the recipient
sets they communicate with.

Results for the event prediction NLL are included in Table
VI in Appendix A since they don’t give the reader much
intuition into the performance of the models. The results
for time prediction and recipient prediction on the test set
are summarized in Tables I and II respectively. We see that
the multi-label classiﬁcation approach, LNM-BC, performs
similarly to the original LogNormMix model on time and
sender prediction, but is completely unable to predict the next
recipient set correctly given the sender. On the other hand, the
LogNormMix-Net is able to correctly predict the next recipient
set given the sender with an accuracy of 33.6% and 40.8%
on the Enron and EU datasets respectively, and with top-3
accuracy of 61% and 71%.

2) Evaluating generated content: Table III shows that the
LogNormMix-BC model with multi-label recipient classiﬁca-
tion does a bit better at reproducing sender proportions in its
generated content, but as with the prior recipient prediction
task, it is unable to reproduce the recipient set proportions
when generating event streams. Digging deeper into the gen-
erated content, it was found that the multi-cast recipient sets
were invalid more than 99% of the time. The LogNormMix-
Net, on the other hand, was able to reproduce the proportion
of e-mails received by each recipient set in its generated event
streams, whilst maintaining strong performance in time-delta
and sender proportions.

Providing a visual representation of the results in Table III
gives the reader a better feeling for whether these results pro-
duce realistic enough trafﬁc for the purpose of cyber deception.
The Q-Q plot in Figure 4 indicates that the LogNorMix-Net
tends to under-generate, in particular, it does a good job of
reproducing the distribution shorter time-deltas, but the longer
time-deltas are too big. Figure 5 shows that the LogNormMix-
Net is able to capture some of the seasonality effect, though
it is somewhat smoothed/dampened. Figures 6 and 7 show
that the LogNormMix-Net is able to reproduce the sender
and recipient set distributions from the EU corpus training
data respectively, with the true sender proportion overlapping

Figure 4: Q-Q plot comparison of inter-arrival times generated
by LogNormMix-Net Vs the Enron training data corpus.

Figure 5: Proportion of e-mails sent in each hour of the day
in LogNormMix-Net generated data Vs Enron email training
data.

the boxplot IQR of generated content for the majority of
sender/recipient IDs.

B. Multi-party e-mail thread generation

of

goal

1) Subject

personalisation: The

extracting
keywords for each user
is to personalise the topics of
generated threads. Examples from the generated subjects of
user IDs 4 and 15 are shown in Table V. User ID 4’s e-mails
have a theme of communications and reporting, with the
word "update" being used in 22% of the generated subjects,
including various versions of recurring "Weekly Update"
themed subjects, and other recurring updates on high proﬁle
portfolios like the "Pipeline" project, while ID 15’s e-mails
have a theme of scheduling meetings, interviews, and other

Figure 6: Proportion of e-mails sent per user in LogNormMix-
Net generated data Vs EU email training data.

Figure 7: Proportion of emails received per hyperedge ID in
LogNormMix-Net generated Vs EU email training data.

Model

LogNormMix-Net
LogNormMix-BC

Time
deltas
0.65 (0.07)
0.42 (0.05)

Hour
of day
2.28 (0.09)
1.81 (0.12)

Day
of week
0.95 (0.08)
1.26(0.08)

Sender
outdegree
5.02 (0.49)
5.33 (0.22)

Recip set
indegree
7.40 (0.98)
669.14 (49.15)

Table III: EMD between real and simulated data on the Enron email corpus. The values in the columns are the means of the
speciﬁed variables over the 100 trials, with corresponding standard deviations given in parenthesis.
Time
d eltas
0.24 (0.09)
0.27 (0.22)

Model
LogNormMix-Net-Sender-Embed
LogNormMix+BC

Recip set
indegree
3.81 (0.82)
2186.59 (943.94)

Sender
outdegree
0.64 (0.14)
3.06 (0.20)

Hour
of day
2.35 (0.17)
2.57 (0.23)

Day
of week
0.49 (0.12)
0.40 (0.12)

Table IV: EMD comparison between real and simulated data on EU email corpus. The values in the columns are the means
of the speciﬁed variables over the 100 trials, with corresponding standard deviations given in parenthesis.

Figure 8: An example of a generated e-mail and subsequent
reply with a similarity score of 0.95.

recruitment tasks. An additional example of a user who may
be in sales or legal is shown in Figure 12 in Appendix B-C.
These results support our claim that this keyword generation
approach can be applied to differentiate sender’s email topics
from one another, and give an indication of their role in a
network.

Repetition was not a problem in the generated subjects, even
among subjects where the ﬁrst couple of words were identical.
For example, user ID 15 had many subjects being generated
about meetings and conference calls, and it was observed that
the subject generation model generated different dates and/or
descriptors for each subject, as shown in Figure 11.

2) E-mail thread generation: Example e-mail thread gen-
erated using a GPT-2-based e-mail body generation model are
shown in Figures 8 and 9. Inspired by the approaches in [18]
and [21], we evaluate the coherence of threads by computing
the similarity score between an e-mail and its reply. The e-
mails in Figure 8 have a similarity score of 0.95, and for Figure
9, the ﬁrst e-mail and it’s reply have a similarity score of 0.98,
with the second and third having a lower score of 0.87 as the
third e-mail is forwarding the thread to an new recipient to
elicit their help.

The above 2 examples were generated using a GPT-2 e-
mail body generation model whose training dataset considered
each e-mail body independently of any previous emails in
the thread. Figure 10 shows an example e-mail and reply
generated with a different GPT-2 model where the training data
included e-mail thread information. The e-mail pair in Figure
10 has a similarity score of 0.97. Overall, the authors felt

Figure 9: An example generated thread containing the original
e-mail, a reply and a fwd e-mail.

Figure 10: An example generated e-mail from that model
whose training dataset included conversation thread details.

the replies generated using this approach typically did seem
directly relevant and arguably created a slightly more coherent
thread than the previous model, though both models produce
high similarity scores.

VIII. DISCUSSION

a) Multi-class Vs multi-label recipient classiﬁcation:

In this work we have demonstrated the superior ability of
the multi-class classiﬁcation approach to model recipient set
selection in DM communications. A further and signiﬁcant
the LogNormMix-Net presents a single
advantage is that
neural TPP architecture to simulate both unicast data, seen
for example in WiFi network trafﬁc, and multi-cast data, seen
in DMs modelled in this work. Multi-class recipient selection

ID 4
Keywords: Update, Meeting, Bullets, Weekly, Capacity,
Pipeline, Storage, Project, Revised, List

Keywords: Meeting, Conference, Risk, Resume,
Visit, Energy, Research, Power, Model, Summer

Generated Subjects

ID 15

List of Top 10 Things You Wish Your ISP Would Notice.

Risk Management Simulation-Please review..

Pipeline News: August 26, 2001.
Pipeline Summary for October 11, 2001.
Meeting to discuss Team Selection -Reply.
Update on California Electricity Market.
Bullets 09/02/01.
Capacity Matrix Update.
Capacity Report.

Summer and Fall Schedule, November.
Summer Intern Information.
Summer Associate Candidate - Angela Davis.
Resume for Jeff Skilling.
Resume : Your Input Required.
Power Point Presentation on Credit Risk.
Energy Analysis - New Issue.

Weekly Update from the Ofﬁce of the Chairman.
Weekly Updates: Energy, Environment, and Weather.
Weekly Update on Power Markets & Energy Market.
Weekly Update - RTO Week – Summary of Comments.

Risk Systems Update for December 11th.
Visit to Portland - July 18.
Visit to Weather Desk.
Model Review Meeting - June 9, 2001.

Subjects starting "Conference Call"

.. Monday, April 19, 2001.
.. Scheduled for 6pm on Tuesday,

June 9th.

.. on Tuesday, November 11th.
.. re: Teams.
.. .
..: Trading Floor
.. Cancelled.
.. to discuss the P&D Program.
.. to Discuss California/West
Wholesale Activities.

Table V: Keywords and resulting generated subjects for user IDs 4 and 15.

is the natural choice for modeling unicast communications,
whereas multi-label classiﬁcation recipient selection does not
make sense, since only one recipient is involved for each event.
A drawback of the multi-class classiﬁcation approach is that
it doesn’t scale, and therefore requires infrequent recipient sets
to be discarded from training data. This is not necessarily a
problem for cyber deception, but the multi-label classiﬁcation
approach may be preferred for simulating very large DMNs,
or when there is a preference to use the entire original training
set without needing to reﬁne it.

Using classiﬁer chains is a potential way to get around the
limitations of the multi-label binary classiﬁcation approach.
It is an extension of the typical binary relevant multi-label
classiﬁcation which instead builds a chain of classiﬁers, where
label information is passed between the classiﬁers.

b) Network evolution over time: Over time it is natural
for an organisation’s members to change, and one may wish to
reﬂect this in their e-mail simulation model. In particular, if a
company is growing in size it would be desirable to add users
to the network simulation. If the original trained TPP model
simulates a network of size n, creating a larger network of
size o > n requires the addition of new marks to the mark
embedding space, as well as an increase in the overall tempo
of network generation, to account for the larger network size. It
would be possible to extend the code in future work to enable
this to be achieved by ﬁne-tuning the existing pre-trained
model on training data of the target network size. Speciﬁcally,
this would involve loading the model and removing the last
layer of the sender and recipient modules, then changing both
those layers to the correct number of neurons for the updated
classiﬁer sizes. Fine-tuning with the new data then amounts
to training the weights of these classiﬁcation ﬁnal layers from
scratch. For this reason we believe it is more practical to
periodically train the entire model again from scratch. For
the alternative case of removing a user in an existing trained
TPP, this can be achieved by rejecting any sampled events that
include them, or re-training the model again from scratch.

c) Applying pre-trained TPPs in cyber deception for
ﬂexible and controllable simulation of social networks: When
using a trained TPP model to simulate a social network, the

generated events will mimic the underlying network dynamics
of the dataset the model was trained on. The lack of publicly
available datasets for private communications like e-mail,
SMS and instant messaging presents a barrier to developing
a diverse range of social network event generation models.
Pre-trained language models like GPT have proven to be
ﬂexible and versatile tools for language generation, that can
be ﬁne-tuned and used for various language applications like
sentiment analysis, document categorization and entity extrac-
tion4, and even for use with different data types (ImageGPT).
Controlling the properties of generated images is another
popular research topic [49]–[52]. With this in mind, we ask if
a pre-trained TPP model has the ﬂexibility to allow the user
deploying the model to control aspects of the simulation such
as the number of people in the network, the habits of certain
people in the network and the overall tempo and periodicity
of communications. This is a problem we have not seen in the
literature before.

The classical parametric TPP framework lends itself natur-
ally to this problem, as adjusting parameters in an individual’s
intensity function leads to prescribed and intuitive changes in
the temporal dynamics of their interactions. Furthermore, the
addition or removal of users can be achieved by increasing
or decreasing the size of the parameter vectors. For neural
network TPP architectures, this is not as straightforward to
achieve. There are tens of thousands of parameters in the
model and no obvious interpretation of how each individual
one affects the network dynamics. An opportunity for future
research is to investigate methods for disengangling the latent
space of neural TPP models for the puprose of controlled
network simulation.

d) Application of this work to cyber range simulation:

Cyber range environments are another rapidly growing ap-
plication domain for the use of realistic simulations. Cyber
ranges have a host of important applications, including security
testing and research, security education and capability devel-
opment [53], all of which rely on content generation to make
each environment distinct and convincing. An example within
this evolving ﬁeld is that of Autonomous Cyber Operations

4https://beta.openai.com/docs/guides/ﬁne-tuning

(ACO). As of today, there are a number of network envir-
onment frameworks, much like cyber ranges, for the training
of autonomous cyberdefence and pentesting agents [54]–[57].
Making ACO environments as realistic as possible is vital
to reducing the ’reality gap’ between an agent’s performance
on the simulation environments they’re trained on compared
to a real network environment, since simulated environments
abstract away information that may be critical to an agent’s
effectiveness [54].

e) Task prioritization in multi-task

learning: Our
LogNormMix-Net can be thought of as a heterogeneous multi-
task learning framework. We found that the temporal module
trains a lot faster than the 2 classiﬁcation modules, and training
until the combined NLL loss reaches a minimum resulted in
over-ﬁtting of the temporal module. This called for a training
strategy to be implemented. How to balance the learning of
multiple tasks to optimise performance is an ongoing research
problem in multi-task learning. In our case, the most basic
approach of weighting the 3 loss terms was not sufﬁcient.
Instead we trained the 3 tasks in stages, and tracked the mean
absolute error (MAE) and the root mean squared error (RMSE)
of the inter-event time prediction during training to help inform
this process.

1) Stage 1: we let all tasks train for a number of epochs
until the RMSE of time prediction on the validation set
begins to degrade.

2) Stage 2: we freeze the parameters associated with the
temporal module (ie. the Lognormal mixture linear layer
and the metadata embedding) and the RNN that embeds
the history. We then continue to train the sender and
recipient prediction modules.

3) Stage 3: as an optional third step, we additionally freeze
the sender prediction module and continue to train the
recipient prediction module.
f) Independence assumption between marks and inter-
arrival times: Our TPP approach models the event time and
participants as conditionally independent given the history,
since this independence assumption leads to signiﬁcant be-
neﬁts for the speed of event generation at deployment time. If
instead the time was modeled to be conditionally dependent
on the participants, then the observed sender, receivers, and
timestamp of each event would be generated by selecting the
sender–receiver-set pair with the smallest time delta [58]. This
approach requires a candidate recipient set and time delta to
be generated for every sender in the network for every event,
though, which increases the event generation time.

Possible approaches suggested by the author to jointly
model time and marks include mimicking the Neural Hawkes
[59] architecture and directly modeling the distribution of n
inter-event times separately, using a shared RNN to process
the history. However,
this requires each sender to have a
sufﬁcient number of events to learn an individual temporal
mixture model.

g) PPLM for multi-person conversation generation: We
also experimented with using PPLM, though did not ﬁnd it
to outperform the standard GPT-2 model. Degeneration is a
recognised issue with the PPLM, and discussed in the paper

[35], and we found it to be particularly noticeable when the
step size is set too high or top-k is set too low. Figure 13 in
Appendix B-C shows an example of the difference observed
in output when the same prompt is used but different step size.
An advantage of using PPLM over generating emails
without is that if there was a key piece of information that an
intruder would be interested in, relevant words could be added
to the BOW. So, if the intruder conducted a search for some
of these relevant words the fake emails would be of interest
to them. PPLM also gives some control as to the topic of the
emails, which is important for the goal of enticing intruders.
On the other hand, if the intruder were to do a quick scan
of an email to determine its legitimacy, the standard GPT-2
model without PPLM tends to sound more natural. Despite
the overall improvement of degeneration of PPLM with the
tweaking of parameters, there were still some cases where
this was an issue. It can particularly be seen when the model
produces dates and times – it often degenerates to a list of
dates and times regardless of the previous context.

Another part of our investigations were how to select the
"best" sample when generating e-mail bodies. We typically
asked the model to generate a number of outputs, and then
selected one of them to use. A natural metric for this task
is perplexity, with the default choice being the sample with
the lowest perplexity. When using the PPLM model however,
we found that generated content with degeneration was scored
disproportionally well, above arguably more coherent output.
This resulted in many simulations that contained not very
coherent emails. Generation was also found to be much slower
using PPLM than standard GPT-2.

IX. CONCLUSION

In this work we present a framework to automate the
generation of email and instant messaging-style group com-
munications at scale, for use in cyber deception. We introduce
the LogNormMix-Net TPP which is a single neural TPP
architecture capable of simulating both unicast and multi-
cast data that
is ﬂexible, and beneﬁts from closed form
likelihood and sampling. We demonstrate the superior ability
to model recipient set selection
of the LogNormMix-Net
over existing recipient modeling approaches, and validate that
generated content captures the underlying dynamics of real
world e-mail network trafﬁc datasets, essential for applications
in deception. We demonstrate the ability of ﬁne-tuned, pre-
trained language models to generate coherent, topical, multi-
party conversation threads, and then propose a framework for
uniting our generated TPP and language content for real-time
simulation of an e-mail server. Such content could be used as
a honeypot or to add realism to larger deceptive environments.

ACKNOWLEDGEMENTS

The work has been supported by the Cyber Security Re-
search Centre Limited whose activities are partially funded
by the Australian Government’s Cooperative Research Centres
Programme.

REFERENCES

[1] M. Zhu, A. H. Anwar, Z. Wan, J.-H. Cho, C. Kamhoua, and M. P. Singh,
“A survey of defensive deception: Approaches using game theory and
machine learning,” IEEE Communications Surveys & Tutorials, 2021.

[2] O. Shchur, M. Biloš, and S. Günnemann, “Intensity-free learning of
temporal point processes,” in International Conference on Learning
Representations, 2019.

[3] L. Spitzner, “Honeypots: Catching the insider threat,” in 19th Annual
IEEE,

Computer Security Applications Conference, 2003. Proceedings.
2003, pp. 170–179.

[4] I. Mokube and M. Adams, “Honeypots: concepts, approaches, and
the 45th annual southeast regional

challenges,” in Proceedings of
conference, 2007, pp. 321–326.
Schaefer. What

[5] M. W.
about
what-marketers-need-to-know-about-chat-apps

marketers
[Online]. Available:

apps.

chap

need

know
to
https://hbr.org/2016/06/

[6] B. Kim, “Latent modeling of dynamic social networks,” Ph.D. disserta-

tion, The Pennsylvania State University, 8 2018.

[7] P. O. Perry and P. J. Wolfe, “Point process modelling for directed
interaction networks,” Journal of the Royal Statistical Society Series
B, vol. 75, no. 5, pp. 821–849, November 2013. [Online]. Available:
https://ideas.repec.org/a/bla/jorssb/v75y2013i5p821-849.html

[8] P. S. Chodrow, “Conﬁguration models of random hypergraphs,” Journal

of Complex Networks, vol. 8, no. 3, p. cnaa018, 2020.

[9] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,
“Language models are unsupervised multitask learners,” OpenAI blog,
vol. 1, no. 8, p. 9, 2019.

[10] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan,
Z. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, and Q. V. Le, “Towards
a human-like open-domain chatbot,” 2020.

[11] S. Roller, E. Dinan, N. Goyal, D. Ju, M. Williamson, Y. Liu, J. Xu,
M. Ott, K. Shuster, E. M. Smith, Y.-L. Boureau, and J. Weston, “Recipes
for building an open-domain chatbot,” 2020.

[12] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models
are few-shot learners,” arXiv preprint arXiv:2005.14165, 2020.

[13] J. Li, T. Tang, W. X. Zhao, and J.-R. Wen, “Pretrained language models
for text generation: A survey,” arXiv preprint arXiv:2105.10311, 2021.
[14] E. Clark, T. August, S. Serrano, N. Haduong, S. Gururangan, and N. A.
Smith, “All that’s’ human’is not gold: Evaluating human evaluation of
generated text,” arXiv preprint arXiv:2107.00061, 2021.

[15] J. Voris, N. Boggs, and S. J. Stolfo, “Lost in translation: Improving
decoy documents via automated translation,” in 2012 IEEE Symposium
on Security and Privacy Workshops.

IEEE, 2012, pp. 129–133.

[16] B. M. Bowen, S. Hershkop, A. D. Keromytis, and S. J. Stolfo, “Baiting
inside attackers using decoy documents.” in SecureComm, vol. 19.
Springer, 2009, pp. 51–70.

[17] B. Whitham, “Automating the generation of enticing text content for
high-interaction honeyﬁles,” in Proceedings of the 50th Hawaii Interna-
tional Conference on System Sciences, 2017.

[18] P. Karuna, H. Purohit, Ö. Uzuner, S. Jajodia, and R. Ganesan, “En-
hancing cohesion and coherence of fake text to improve believability
for deceiving cyber attackers,” in Proceedings of the First International
Workshop on Language Cognition and Computational Models. Santa
Fe, New Mexico, USA: Association for Computational Linguistics, Aug.
2018.

[19] P. Karuna, H. Purohit, R. Ganesan, and S. Jajodia, “Generating hard
to comprehend fake documents for defensive cyber deception,” IEEE
Intelligent Systems, vol. 33, no. 5, pp. 16–25, 2018.

[20] H. Li, Y. Guo, S. Huo, and Y. Ding, “Edge: An enticing deceptive-
content generator as defensive deception,” KSII TRANSACTIONS ON
INTERNET AND INFORMATION SYSTEMS, vol. 15, no. 5, pp. 1891–
1908, 2021.

[21] R. Timmer, D. Liebowitz, S. Nepal, and S. Kanhere, “Tsm: Measuring
language processing,” in
the enticement of honeyﬁles with natural
Proceedings of the 55th Hawaii International Conference on System
Sciences, 2022.

[22] W. S. Cho, P. Zhang, Y. Zhang, X. Li, M. Galley, C. Brockett, M. Wang,
and J. Gao, “Towards coherent and cohesive long-form text generation,”
arXiv preprint arXiv:1811.00511, 2018.

[23] D. Nguyen, D. Liebowitz, S. Nepal, and S. Kanhere, “Honeycode: Auto-
mating deceptive software repositories with deep generative models,” in
Proceedings of the 54th Hawaii International Conference on System
Sciences, 2021, p. 6945.

[24] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. (2018)

Improving language understanding by generative pre-training.

[25] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever,
“Generative pretraining from pixels,” in International Conference on
Machine Learning. PMLR, 2020, pp. 1691–1703.

[26] C. Palm, Intensitätsschwankungen im Fernsprechverkehr, ser. Ericsson
technics. L. M. Ericcson, 1943. [Online]. Available: https://books.
google.com.au/books?id=5cy2NQAACAAJ

[27] D. R. Cox, “Some statistical methods connected with series of events,”
Journal of the Royal Statistical Society: Series B (Methodological),
vol. 17, no. 2, pp. 129–157, 1955.

[28] A. G. Hawkes, “Spectra of some self-exciting and mutually exciting

point processes,” Biometrika, vol. 58, 1971.

[29] S. Zuo, H. Jiang, Z. Li, T. Zhao, and H. Zha, “Transformer hawkes
PMLR,

process,” in International Conference on Machine Learning.
2020, pp. 11 692–11 702.

[30] Q. Zhang, A. Lipani, O. Kirnap, and E. Yilmaz, “Self-attentive hawkes
PMLR,

process,” in International Conference on Machine Learning.
2020, pp. 11 183–11 193.

[31] E. W. Fox, M. B. Short, F. P. Schoenberg, K. D. Coronges, and
A. L. Bertozzi, “Modeling e-mail networks and inferring leadership
using self-exciting point processes,” Journal of the American Statistical
Association, vol. 111, no. 514, pp. 564–584, 2016. [Online]. Available:
https://doi.org/10.1080/01621459.2015.1135802

[32] J. Wu, O. Ward, J. Curley, and T. Zheng, “Markov-modulated hawkes

processes for sporadic and bursty event occurrences,” 2019.

[33] O. G. Ward, J. Wu, T. Zheng, A. L. Smith, and J. P. Curley, “Network
hawkes process models for exploring latent hierarchy in social animal
interactions,” 2020.

[34] M. Price-Williams and N. A. Heard, “Nonparametric self-exciting mod-
els for computer network trafﬁc,” Statistics and Computing, pp. 1–12,
2019.

[36] Y. Duan, C. Xu,

[35] S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yos-
inski, and R. Liu, “Plug and play language models: A simple approach
to controlled text generation,” arXiv preprint arXiv:1912.02164, 2019.
J. Han, and C. Li, “Pre-train and
J. Pei,
text generation with variational auto-
plug-in: Flexible conditional
encoders,” in Proceedings of
the
the 58th Annual Meeting of
Association for Computational Linguistics. Online: Association for
Computational Linguistics, Jul. 2020, pp. 253–262. [Online]. Available:
https://www.aclweb.org/anthology/2020.acl-main.23

[37] F. Mai, N. Pappas, I. Montero, N. A. Smith, and J. Henderson,
text generation,” in
“Plug and play autoencoders for conditional
Proceedings of
in
Natural Language Processing (EMNLP). Online: Association for
Computational Linguistics, Nov. 2020, pp. 6076–6092.
[Online].
Available: https://www.aclweb.org/anthology/2020.emnlp-main.491

the 2020 Conference on Empirical Methods

[38] X. He, T. Rekatsinas,

J. R. Foulds, L. Getoor, and Y. Liu,
“Hawkestopic: A joint model for network inference and topic modeling
from text-based cascades,” in ICML, 2015, pp. 871–880. [Online].
Available: http://proceedings.mlr.press/v37/he15.html

[39] J. Choudhari, A. Dasgupta, I. Bhattacharya, and S. Bedathur, “Discov-
ering topical interactions in text-based cascades using hidden markov
hawkes processes,” in 2018 IEEE International Conference on Data
Mining (ICDM), 2018, pp. 923–928.

[40] S. Bedathur, I. Bhattacharya, J. Choudhari, and A. Dasgupta, “Analyzing
topic transitions in text-based social cascades using dual-network hawkes
process,” PAKDD-2021, 2021.

[41] H. Xu, G. Fang, and X. Zhu, “Network group hawkes process model,”

2020.

[42] W. Wu, H. Liu, X. Zhang, Y. Liu, and H. Zha, “Modeling event propaga-
tion via graph biased temporal point process,” IEEE Transactions on
Neural Networks and Learning Systems, 2020.

[43] W. Fischer and K. Meier-Hellstern, “The markov-modulated poisson
process (mmpp) cookbook,” Perform. Eval., p. 149–171, Sep. 1993.

[44] O. Shchur, A. C. Türkmen, T.

and S. Gün-
nemann, “Neural temporal point processes: A review,” arXiv preprint
arXiv:2104.03528, 2021.

Januschowski,

[45] J. G. Rasmussen, “Temporal point processes: the conditional intensity

function,” Lecture Notes, Jan, 2011.

[46] P. Lewis and G. Shedler, “Simulation of nonhomogeneous poisson
processes by thinning,” 1978. [Online]. Available: https://calhoun.nps.
edu/handle/10945/63247

[47] O. Shchur, N. Gao, M. Biloš, and S. Günnemann, “Fast and ﬂex-
ible temporal point processes with triangular maps,” arXiv preprint
arXiv:2006.12631, 2020.

[48] M. B. Salem and S. J. Stolfo, “Decoy document deployment for effective
masquerade attack detection,” in International Conference on Detection
of Intrusions and Malware, and Vulnerability Assessment.
Springer,
2011, pp. 35–54.

[49] D. P. Kingma and P. Dhariwal, “Glow: generative ﬂow with invertible
1× 1 convolutions,” in Proceedings of the 32nd International Conference
on Neural Information Processing Systems, 2018, pp. 10 236–10 245.

[50] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture
for generative adversarial networks,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019, pp.
4401–4410.

[51] X. Zhu, C. Xu, and D. Tao, “Where and what? examining interpretable
disentangled representations,” in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, 2021, pp. 5861–5870.
[52] A. Shoshan, N. Bhonker, I. Kviatkovsky, and G. Medioni, “Gan-control:

Explicitly controllable gans,” arXiv preprint arXiv:2101.02477, 2021.

[53] Understanding

real-
[Online]. Available: https://www.ecs-org.eu/documents/uploads/

ity.
understanding-cyber-ranges-from-hype-to-reality.pdf

ranges:

cyber

From

hype

to

[54] M. Standen, M. Lucas, D. Bowman, T. J. Richer, J. Kim, and D. Marriott,
“Cyborg: A gym for the development of autonomous cyber agents,”
arXiv preprint arXiv:2108.09118, 2021.

[55] A. Molina-Markham, C. Miniter, B. Powell, and A. Ridley, “Net-
work environment design for autonomous cyberdefense,” arXiv preprint
arXiv:2103.07583, 2021.

[56] M. D. R. Team et al., “Cyberbattlesim,” 2021.
[57] J. Schwartz and H. Kurniawati, “Autonomous penetration testing using
reinforcement learning,” arXiv preprint arXiv:1905.05965, 2019.
[58] T. A. Snijders, “Stochastic actor-oriented models for network change,”

The Journal of Mathematical Sociology, 1996.

[59] H. Mei and J. M. Eisner, “The neural hawkes process: A neurally
self-modulating multivariate point process,” in Advances in Neural
Information Processing Systems, 2017, pp. 6754–6764.

APPENDIX A
FURTHER DEATILS ON THE LOGNORMMIX-NET AND
TRAFFIC GENERATION

A. LogNormMix-Net architecture

Figure 11 shows the LogNormMix-Net architecture over-

view.

sender from the training data. The pseudocode for email type
selection is provided below.

Pseudocode for selecting E-mail type given the sender and

recipients.
if there’s an existing active e-mail
thread between those
participants, and the proportion of "reply" type emails sent
by the sender in the past "recent" time period (eg. 2 months)
of simulated events is less than 1.1 times the training dataset
proportion:

email type = "reply"

else if
there’s an existing active thread between a subset of
those participants, and the proportion of fwd messages sent
by that sender in the past "recent" time period (eg. 2 months)
of simulated events is less than 1.1 times the training dataset
proportion:

email type = "fwd"

else:

email type = "new thread".

B. E-mail thread generation

Figure 11: LogNormMix-Net conditional architecture.

given the sender and recipients.
E-mail thread generation

Here we describe the steps to generate an e-mail thread

B. NLL results for event prediction experiments

Table VI shows the NLL results for our event prediction
experiments from Section VII. We see that the LogNormMix-
Net (denoted LMN-Net) gets the lowest NLL for the sender
and recipient prediction task, with the original LogNormMix
getting the lowest for time prediction (though it has no
recipient module and thus is only solving 2 tasks instead of
3).

Model

LogNormMix
LMN-Net
LMN-BC

Time
NLL
74.4
74.7
78.1

Enron
Sender
NLL
361.0
340.5
347.6

Recip
NLL
-
447.8
625.0

Time
NLL
-68.2
-63.1
-2.1

EU
Sender
NLL
980.3
857.4
1002.8

Recip
NLL
-
1004.2
6851.2

Table VI: NLL by task and dataset

APPENDIX B
FURTHER DETAILS ON E-MAIL CONTENT GENERATION

A. Selecting email communication type

We consider 3 types of communications:

i) Starting a new conversation thread
ii) "reply": Replying in an existing conversation thread
iii) "fwd": Forwarding the existing communication to a
(note: although unrestricted forwarding of
new recipient
it
communications isn’t possible in platforms like Slack,
does have the functionality to share a message from a public
channel or a channel the recipient is a member of, to a private
message to said recipient).

For each sender, we aim to ensure the proportions of
each communication type (new-thread, reply, fwd) in the
rolling simulated content resembles the proportions of that

a: Sample the communication type (new thread, reply, fwd)
based on that sender’s proportions from the training data,
according to Algorithm B-A in Appendix B-A.

b: Generate the email Subject

to begin a new thread or
sample an existing thread to respond to (depending on
the communication type sampled in step a):

if "reply"/"fwd" communication:

i) sample a conversation thread to respond to, that is:

- recently active (eg. in the last week)
- appropriate for the recipients selected in step a,

and communication type.

ii) inherit the thread ID from the selected thread.

else if "new thread":

i) generate email Subject:

- sample from the sender’s topic keywords
- feed that topic into the subject generation model

as the prompt for the generation.

ii) generate a new ID for this new e-mail thread.

c: Generate e-mail body text:

i) Generate a greeting: randomly sample a greeting

from a canned list, and append the recipient name(s)
to complete the greeting.

ii) Generate the body message text:

if "fwd" communication:

- randomly sample the body text message from a

canned list of responses for "forward"-type emails,

else:

use the e-mail subject and existing email thread
(if any) to seed the e-mail body generation model.

iii) Generate the salutation: randomly sample a salutation
for the body text from a canned list, and append the
sender name to complete the salutation.

Figure 12: Keywords and generated subjects for ID 5.

Figure 13: E-mail generation using PPLM.

iv) Assemble the message: combine the greeting, message
and salutation from steps i), ii) and iii) to form the ﬁnal
e-mail message.
v) Assign an email ID to the generated e-mail.

C. Additional generated textual content

a) Subject generation: Another example employee was
ID 5, who appears to be a person that works in sales or perhaps
the legal department. Figure 12 shows their keywords and
generated subjects.

b) PPLM content generation: Figure 13 shows the differ-
ence in PPLM generated content when the step size is changed.
While the smaller step size results in content of reasonable
quality, the larger step size leads to degeneration.

