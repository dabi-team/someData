Testing SOAR Tools in Use

2
2
0
2

g
u
A
2
1

]

R
C
.
s
c
[

1
v
5
7
0
6
0
.
8
0
2
2
:
v
i
X
r
a

ROBERT A. BRIDGES, ASHLEY E. RICE, SEAN OESCH, JEFF A. NICHOLS, CORY WATSON,
KEVIN SPAKES, SAVANNAH NOREM, MIKE HUETTEL, BRIAN JEWELL, BRIAN WEBER,
CONNOR GANNON, OLIVIA BIZOVI, and SAMUEL C HOLLIFIELD, Oak Ridge National
Laboratory, USA
SAMANTHA ERWIN, Pacific Northwest National Laboratory, USA

Modern security operation centers (SOCs) rely on human operators and a tapestry of diverse logging and
alerting tools with large-scale collection and query abilities. Yet, SOC investigations are tedious as they rely
on manual efforts to query diverse data sources, overlay related logs, and correlate the data into information
and then document results in a ticketing system. Security orchestration, automation, and response (SOAR)
tools are a relatively new technology that promise, with appropriate configuration, to collect, filter, and
display needed diverse information; automate many of the common tasks that unnecessarily require SOC
analysts’ time; facilitate SOC collaboration; and, in doing so, improve both efficiency and consistency of SOCs.
SOAR tools have never been tested in practice to evaluate their effect and understand them in use. In this
paper, we design and administer the first hands-on user study of SOAR tools, involving 24 participants and
6 commercial SOAR tools. Our contributions include the experimental design, itemizing six characteristics
of SOAR tools, and a methodology for testing them. We describe configuration of a cyber range to the test
environment, including network, user, and threat emulation; a full SOC tool suite; and creation of artifacts
allowing multiple representative investigation scenarios to permit testing. We present the first research results
on SOAR tools. We found that SOAR configuration is critical for success, as it involves creative design for
data display and automation and should involve iteration with users and vendors. We also found that SOAR
tools increased efficiency and reduced context switching during investigations, although ticket accuracy and
completeness (potentially indicating investigation quality) decreased with SOAR use. Our findings indicated
that user preferences from usability studies are slightly negatively correlated with their performance with
the tool; overautomation was a concern of senior analysts, and SOAR tools that balanced automation with
assisting a user to make decisions were preferred. We also found that SOAR dependence on constant internet
varies widely with the tool. Finally, we deliver a public user- and tool-anonymized and -obfuscated version of
the data from the study to assist future research.

Data & code from this work available at https://github.com/bridgesra/soar_experiment_data_code.
This manuscript has been co-authored by UT-Battelle LLC under contract DE-AC05-00OR22725 with the US Department of
Energy (DOE). The US government retains and the publisher, by accepting the article for publication, acknowledges that
the US government retains a nonexclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published
form of this manuscript, or allow others to do so, for US government purposes. DOE will provide public access to these
results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-
public-access-plan).
Authors’ addresses: Robert A. Bridges, bridgesra@ornl.gov; Ashley E. Rice, riceae@ornl.gov; Sean Oesch, oeschts@ornl.gov;
Jeff A. Nichols, nicholsja2@ornl.gov; Cory Watson, watsoncl1@ornl.gov; Kevin Spakes, spakeskd@ornl.gov; Savannah
Norem, savannah.norem@gmail.com; Mike Huettel, huettelmr@ornl.gov; Brian Jewell, jewellbc@ornl.gov; Brian Weber,
weberb@ornl.gov; Connor Gannon, gannoncm@ornl.gov; Olivia Bizovi, bizovio@ornl.gov; Samuel C Hollifield, hollifieldsc@
ornl.gov, Oak Ridge National Laboratory, TN, USA; Samantha Erwin, samantha.erwin@pnnl.gov, Pacific Northwest National
Laboratory, WA, USA.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2022 Association for Computing Machinery.
XXXX-XXXX/2022/8-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

, Vol. 1, No. 1, Article . Publication date: August 2022.

 
 
 
 
 
 
2

Bridges, et al.

ACM Reference Format:
Robert A. Bridges, Ashley E. Rice, Sean Oesch, Jeff A. Nichols, Cory Watson, Kevin Spakes, Savannah Norem,
Mike Huettel, Brian Jewell, Brian Weber, Connor Gannon, Olivia Bizovi, Samuel C Hollifield, and Samantha
Erwin. 2022. Testing SOAR Tools in Use. 1, 1 (August 2022), 37 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Security operation centers (SOCs) are the portion of an enterprise’s information technology (IT)
team responsible for protecting the organization from cyber threats. Modern SOCs leverage a wide
variety of sensors that generate an enormous quantity of audit logs (e.g., operating system logs
from workstations and servers, network flows from switches); intelligence feeds (e.g., vulnerability
scanning reports, malware information); and alerts (e.g., from endpoint antivirus (AV) and network
intrusion detection systems [NIDS]). Most SOCs are equipped with a Security Information and Event
Management (SIEM) system, which aggregates these diverse data feeds into a centralized platform
and provides configurable dashboards and query interfaces to monitor data and sift, sort, and find
artifacts from the network. However, even with the centralization that the SIEM can provide, the
burden of sorting through vast amounts of information across many sources, identifying which
logs require further attention, understanding which alerts are related, and executing a solution still
falls on operators [1, 2] and requires substantial manual effort [2–6].

Security orchestration, automation, and response (SOAR) tools represent a relatively new tech-
nology that attempts to automate many of the common manual tasks with the goal of improving
both efficiency and consistency, and as a result security. According to Gartner’s report on SOAR
[7], a SOAR tool enables “organizations to take inputs from a variety of sources and apply work-
flows aligned to processes and procedures ... improving efficiency and consistency of people and
processes.” The main difference between a SOAR tool and a SIEM tool is that SOAR tools have
configurable workflows or playbooks that guide analysts and automate many investigation and
incident response actions, whereas SIEM tools primary function is widespread data collection
and query. There is overlap, e.g., both require and facilitate diverse data ingestion and provide
customizable data visualizations, in particular real-time dashboards. SOAR tools aim to make
security operations (1) more efficient by automating common tasks, prioritizing, correlating, and
augmenting incoming alerts (e.g., with threat intelligence), and facilitating communication and
collaboration between and within SOCs, and (2) more consistent by providing a unified, usable
interface where analysts interact with augmented intelligence and alerts and by using playbooks to
standardize procedures.

The promised benefits of SOAR tools are extensive, particularly for expediting and adding
consistency to the work of low-tier analysts who repeatedly perform investigation tasks. Because
of their integration capabilities, SOAR tools can relieve analysts from manually navigating multiple
heterogeneous data sources to piece together alert context by automatically gathering and displaying
all—and ideally only—the data needed for investigation into an usable interface. Expediting and
enhancing quality of low-tier operators is particularly important for large (i.e., dozens to hundreds
of analysts) organizations that are expected to perform and document investigations according to
strict processes and with similar quality, as well as for organizations that experience high turnover
of junior analysts (e.g., military). Furthermore, incident handling often requires several analysts,
all of whom may not necessarily be located at the same SOC. SOAR tools address this specific
need by offering collaboration capabilities that allow analysts, regardless of geographic location, to
coordinate synchronously or asynchronously on a single incident.

While SOAR tools offer potentially significant efficiency gains, they also entail nontrivial costs.
SOAR tools are expensive products, both financially and in terms of the effort required to ensure
a stable installation with proper configuration; further, as a centerpiece technology for a SOC,

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

3

SOAR adoption entails a systemic shift for SOC processes including training, daily operations,
and technical debt. A priori, SOAR tools entail risk. We conjecture that ease of integration and
customization is important for adoption, as is the usability of the configuration of the SOAR tool in
the eyes of the user.

Consequently, it is natural for SOCs to want to understand the benefit of SOAR tools in practice,

and our work attempts to provide insight. For example, many research questions arise:

• How much do SOAR tools enhance efficiency, especially for repeated investigations suitable
for lower-tier analysts? In particular, can a SOAR tool be used to effectively “automate-out”
the role of a Tier 1 (junior) analyst, at least for prespecified, well-defined investigation
procedures?

• Do SOAR tools reduce context switching for analysts?
• Do SOC operators believe current SOAR tools are usable, and would they prefer using them?
• How easily do SOAR tools integrate and permit customization to SOC-specific procedures?
• Do SOAR tools perform in internet-degraded environments?
• Can junior analysts perform investigations with the quality of a more senior analyst with

SOAR tools?

• Do SOAR tools improve collaboration?

Despite their apparent usefulness, SOAR tools in practice have not been studied to any great
degree (see Related Works, Section 2). In part, SOAR tools are simply too new to admit ample
research; in fact, the term “SOAR” was coined by Gartner only in 2017, and SOAR deployments
are still in their infancy. Because SOAR tools must integrate with a variety of SOC tools and are
designed to assist humans though interactive use, studying SOAR tools imposes significant barriers.
For example, an interested research team can seek a cooperative SOC with a SOAR deployment
to administer qualitative research of that SOAR tool in situ or, as this paper describes, can build
a test environment that emulates a network, users, threats, and the full SOC tool suite allowing
deployment and use of the SOAR tool in a controlled environment. Both are difficult, because of
SOC operators’ time constraints, SOCs’ privacy requirements, and a wide variety of logistic issues
imposed by such studies. As such, SOAR tools remain full of promise but with with no empirical
verification of their benefits and pitfalls.

1.1 Contributions
To this end, we pioneer first-of-their-kind methods for evaluating a SOAR tool by administering a
user study of real SOC operators working investigations with and without six commercial SOAR
tools, in a controlled test environment. In doing so, we seek answers to the pressing questions
above, share results, and discuss key takeaways. We make the following contributions:

SOAR Testing Methodology: We explain the design and instantiation of our cyber range (i.e., a
data center dedicated to network, user, threat emulation for cyber experimentation) to create an
environment in which the SOAR tools can be deployed, configured, and used by analysts. We itemize
particular challenges and solutions for this experimental setup to assist future researchers. We
provide our experimental design, including SOAR evaluation criteria—installation and configuration,
resilience to degraded internet, usability, efficiency gains, reduction in context switching, and
investigation quality—and their corresponding testing methodologies. Lessons learned are given in
our Discussion (Section 6).

Dataset: Overall, the experiment reported here involved 24 participants testing six commercial
SOAR tools with over 200 total hours spent working investigations while under study. The ex-
periment produced 75 raw measurements per user-tool test and 68 user-tool tests. In the spirit of

, Vol. 1, No. 1, Article . Publication date: August 2022.

4

Bridges, et al.

reproducibility and to assist further investigations, we have made our experimental data public in a
user- and tool-anonymized and -obfuscated table (see Section 5.5.2).

SOAR Takeaways: We gather findings from our data including coded results (i.e., categorized
themes from analyzing free-response data) from the user study to illuminate themes learned from
users, ratings from the research team on SOAR tool installations and resilience to degraded internet
situations, and data analytics results investigating the questions outlined above.

1.2 AI ATAC Challenge 3: Efficiency and Effectiveness Afforded by SOAR Capabilities
This work details the first scientific evaluation and user study of many commercial SOAR tools as
part of the third Artificial Intelligence Applications to Autonomous Cybersecurity (AI ATAC 3)
Challenge funded by the US Navy [8]. AI ATAC 3 was administered to entice the best SOAR tool
creators to submit their technology to the competition with the goal of evaluating their benefits and
informing potential acquisition decisions for adoption into Navy SOCs. The AI ATAC 3 Challenge
was a multistep project consisting of interdisciplinary efforts from a large team of researchers. In the
first phase, Navy security professionals rated and ranked the eligible submissions based on video
overviews of the SOAR tools. All but the lowest-rated tools were advanced to the hands-on testing
(i.e., final) round. This down-selection subexperiment is described in detail in our previous work
by Norem et al. [9] and an overview resides in Appendix A. This paper describes the evaluation
experiment, including a hands-on user study, performed in the second phase, and its results. As
required by the AI ATAC Challenges, SOAR tool contenders involved in this evaluation must be
held anonymous.

1.3 How to Navigate this Paper
Section 2 provides previous papers contributing to SOC user studies and SOAR tools. Figure 1
provides a workflow infographic of the whole experiment to be used as a reference throughout
the paper. In Section 3, we articulate a definition of SOAR tools, then list the characteristics and
capabilities of SOAR tools we will test and our methods. A concise overview of the testing workflow
concludes the section. For those interested in how the experimental environment was designed and
instantiated, Section 4 describes the components and their instantiation. In particular, descriptions
of the investigation scenarios used for the test concludes the section. Section 5 presents the data,
processing methods, and results of our investigation. We provide a discussion of limitations to this
study in Section 6. Those interested only in our takeaways may choose to focus on the italicized
“Observation” statements throughout Section 5 and the summarized takeaways in Section 6.

2 RELATED WORK
In this section, we present a survey of research that investigates the need for and use of SOAR tools
in SOCs, as well as present works detailing known issues in SOCs. Our literature survey reveals
that SOAR tools are an exciting technology holding much promise for the cyber industry but that
there is lack of research evaluating the actual effectiveness of SOAR solutions. In fact, there is no
academic literature focused on evaluating SOAR tools (save our preliminary work of Norem et al.
[9] discussed in detail in Section A); further, there is no academic literature that proposes and tests
experimental frameworks for SOAR tool comparison. Our work, therefore, fills a gap in existing
literature by presenting the first experimental framework to evaluate and compare SOAR solutions.

2.0.1 The Importance of SOAR. Many articles discuss orchestration and automation for security,
and some even argue that SOAR tools are critical for the modern SOC (e.g., to reduce alert fatigue,
unify data feeds into a common user interface (UI), and automate common tasks) [10–16]. Islam
et al. attempt to define SOAR in their survey paper [1], in which they conclude that unification,

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

5

orchestration, and automation are central to SOAR platforms. They also provide a conceptual
diagram of SOAR in a related paper [17], which argues that such a diagram helps SOAR platform
architects build better platforms and that an architectural approach allows SOC staff to better
compare potential SOAR solutions for their environment. Literature related to SOC effectiveness
and the future of SOCs [18–21] provides helpful context for our study, though it did not directly
inform it. Work proposing SOAR solutions or machine learning methods for detecting anomalous
activity in the SOC is out of the scope of the present work.

SOAR Use Cases. Even though SOAR tools themselves are still a new concept, specialized
2.0.2
use cases are already being proposed and developed in the research literature. Mavroeidis et
al. [22] develop a framework for sharing playbooks. Configuring a SOAR tool is important for
adoption and involves manually identifying the proper application programming interface (API)
for many different tools for integration with SOAR tools. Sworna et al. [23] introduce APIRO (API
Recommendation for security Orchestration, automation, and response), a system that aids SOAR
configuration through the use of a convolutional neural network. Bearicade is a SOAR system
integrated with artificial intelligence technology that is specifically designed to meet the security
needs of high-performance computing (HPC) systems [24]. Another specialized case for SOAR
is use in organizations that may lack a large budget for cybersecurity solutions but would still
benefit from the offerings of a SOAR tool. As such, Gibadullin et al. [25] propose a solution in which
an automated incident management system was constructed using only open-source software.
Additionally, Vast et al. [26] propose an AI-informed SOAR tool that implements deep learning to
assign a risk score to a detected threat, along with natural language processing–driven translators to
translate the threat intelligence necessary for sufficient alert context, if it is in a language different
from computer settings. Applications of SOAR are being developed to aid autonomous computing,
specifically by managing fragmentation issues, in which the attacker identifies weak areas in an
organization’s cyber kill chain and exploits these vulnerabilities to gain access to and/or control of
a system [27]. SOAR tools can also offer a cybersecurity solution for energy microgrids—cyber-
physical systems designed to deliver power—by providing a centralized dashboard with information
from the various data sources in the energy microgrid with which it is integrated. Early detection
of security anomalies in microgrid devices, followed by at least a partially automated response, can
help mitigate the effects of an attack, if not prevent it entirely [28].

Issues in the SOC. Common problems within SOCs largely revolve around consistency among
2.0.3
operators, along with operator fatigue associated with navigating excessive amounts of data from
disparate sources and completing many repetitive tasks during their shift. Many research works
seek to define and develop methods for orchestration and automation [10–13]. Orchestration refers
to the integration of separate tools into a central platform, and automation refers to the tool’s
ability to complete certain tasks without necessary aid from an operator. In general, orchestration is
often considered the driver of automation, and the two terms are frequently used interchangeably
[1, 14, 15]. Notably, none focus on evaluating SOAR tools, nor their automation. Automation is
instrumental in protecting operators from becoming overwhelmed by great swaths of data when
implemented alongside a filtering mechanism such that the alerts that reach an operator are
almost guaranteed to be legitimate threats that need attention [10, 13]. However, it is worth noting
that, while automation of specific tasks can significantly reduce operator burden, it should be
implemented with caution. The cybersecurity domain requires some level of human oversight
due to the inherent uncertainty. As such, automation should be employed as an aid to human
effort, rather than as a replacement [10, 29]. Nonetheless, these definitions of orchestration and
automation were critical in constructing the submission criteria for this competition.

, Vol. 1, No. 1, Article . Publication date: August 2022.

6

Bridges, et al.

Fig. 1. SOAR experiment workflow depicting (left to right): study design, test network, and investigation
scenarios; SOAR vendor installations and tests administered, including DDIL (denied, degraded, intermittent,
or limited bandwidth) resilience testing by the research team and user study tests involving naval SOC
operators who worked investigations in the test network with and without SOAR tools; artifacts and data
collected from install and tests; and SOAR capabilities under study.

SOC User Studies. In short, existing user studies of SOCs have the illuminated the fact
2.0.4
that (1) the sheer amount of data at the SOC operators’ fingertips is enormous and increasing,
yet converting disparate data sources into actionable information requires tacit network-specific
knowledge, (2) manual effort is unable to keep pace with SOC needs, and (3) mental correlation of
diverse data source remains a challenge. Further, investigations that require multiple analysts need
tools that facilitate collaborations. [3, 4, 6, 30–33] Most directly relevant to SOAR tools is the work
of Nyre-Yu et al. [34], who perform a SOC user study followed by a market analysis of security
tools, concluding that SOAR tools aim to address needs of incident response teams.

Goodall et al. [3] note that SOCs generally organize operator into Tiers 1–3, from junior to experi-
enced members. The user studies of SOCs drive home the fact that network-specific experience (e.g.,
as common with Tier 3 analysts) provides invaluable knowledge when performing investigations
[3, 6, 32]. Operators must be consistently diligent in monitoring incoming information and must
have strong pattern recognition skills to be able to construct a comprehensive overview of a security
threat from many different sources [4]. As such, operator fatigue is common, and recent user studies
of SOCs are finding that fatigue is exacerbated by incommensurate demands (e.g., satisfying goals
from management that do not align with SOC needs [33]). Further, there is evidence from Werlinger
et al.’s 2009 study [31] that SOC tools are not conducive to collaboration.

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

7

3 EXPERIMENTAL DESIGN
To design a framework that can effectively evaluate SOAR tools, we needed to define what is/is not
a SOAR tool, identify the qualities and benefits of SOAR tools warranting investigation, and create
an experimental workflow to systematically test those properties. Figure 1 provides a workflow
diagram of the experiment.

3.1 Defining Aspects of SOAR
Our first task was to articulate the defining aspects of a SOAR tool to clearly delineate what is
eligible for consideration and identify the capabilities of these tools that could be studied. Starting
with the discussion of SOAR tools above, and after consulting the related work on SOAR (see
Section 2), we developed the below-listed defining aspects of a SOAR tool, which comprised the
necessary and sufficient components for eligibility as a SOAR tool in our study:

• Ingest data: Ingest logs and alerts from a wide variety of security tools in a SOC; ingest or

provide threat intelligence information from both internal and external sources.

• Correlate and prioritize data: Combine, coordinate, and enrich logging, alert, and threat
data in the tool’s UI; identify common attack patterns and highlight them in the tool’s UI.
• Automate processes: Automate and ultimately simplify the alert triage and incident re-
sponse processes via preset and configurable workflows or playbooks; automate and expedite
documentation of triage, incident response, and forensics (e.g., via ticketing); reduce context
switching by automatically obtaining and presenting all the necessary information in a single
interface.

• Facilitate collaboration: Facilitate investigation collaborations by different, potentially
geographically disparate, operators; support both synchronous (i.e., working together) and
asynchronous (i.e., sharing or handing off cases) teamwork.

3.2 What to Test and How?
The capabilities we targeted were driven by the goals of a SOAR tool—to enhance efficiency and
quality of investigations by providing a more usable interface that integrates multiple data systems
and promotes collaboration. We targeted the following capabilities of SOAR tools in our evaluation,
along with methods for evaluation.

3.2.1 Ease of Install, Integration/Ingestion Abilities, and Configurability. SOAR tool installation
requires integration with a SOC-specific tapestry of tools and data feeds, followed by configuration
of dashboards and automation playbooks unique to the particular SOC’s operating procedures. As
evidenced from a recent study of a SOC that adopted a tool by Sundaramurthy et al. [33], improper
or outdated configuration of promising SOC tools can quickly lead the organization to abandon the
tool’s use; hence, installation and configurability of the SOAR tool are worthwhile characteristics to
study. To test this, SOAR vendors were given the standard operating procedures (SOPs) defining the
investigations used in our test and asked to install their tool in our test environment. This involved
designing and configuring playbooks or automation workflows custom to the SOPs and SOC tool
suite. To evaluate ease of install, ability to ingest and integrate with diverse data sources/systems,
and configurability, we assigned a two-person research team to interface with each SOAR vendor
during installation in our test environment. Research team members completed a survey evaluating
the quality of the installation and integration processes and highlighting any issues. Appendix F
provides the details for the vendor installation process and evaluation.

3.2.2 Resilience to a DDIL (Denied, Disrupted, Intermittent, or Limited bandwidth) environment.
First (denied), with the SOAR tool inside a fully functioning network, the internet connection

, Vol. 1, No. 1, Article . Publication date: August 2022.

8

Bridges, et al.

was severed by removing the connection on the WAN side of the network’s border firewall. An
investigation scenario (Section 4.6) was then executed within the network to start the SOAR tool’s
playbook for that scenario. The tools were evaluated on how gracefully they handled not having
access to outside data sources and tools. Next (disrupted or intermittent), network access to the
SOAR tool was disabled while malware was injected into the environment, initiating a malware
alert scenario, and then network access was restored. The evaluation looked at how well the SOAR
tool could recover events that occur during and after impairment. Finally (limited), we simply took
measurements to quantify the “steady-state” bandwidth used by the SOAR tool during normal
operations and the amount of data consumed while working an incident with the SOAR tool. Then,
we quantified “working” bandwidth as the minimum bandwidth required to completely work a
given scenario.

3.2.3 Usability, Including Information Quality, Interface Quality, Collaboration Capabilities, and
Other Features. During the user study, Likert scale surveys, pertaining to the overall usability of the
tools, with questions specifically targeting information quality, interface quality, and collaboration
capacity, among other features of the tools, were administered. Users also completed a free-response
questionnaire that solicited feedback on each tool, and these responses were mapped to a 1–5
Likert scale with sentiment analysis. Tier 3 (i.e., senior) participants were given access to all tools
and encouraged to conduct a more open-ended exploration to gain detailed understanding of the
tools’ functionalities. These senior analysts completed the questionnaires and participated in a
roundtable discussion led by the research team to explain their experiences and preferences with
regard to each tool.

Investigation Completion Time. Time to complete the investigation was recorded for each
3.2.4
scenario using a background script the operator started once the investigation was initiated and
terminated once it was completed. This script recorded the time in seconds of each investigation
and provided a helpful metric for evaluating the ability of the tool to improve response time when
compared to a baseline environment with no tool, as well as how the tool compared to other tools
that were being evaluated.

3.2.5 Context Switching. Context switching is quantified by monitoring the number of times an
operator has to switch between windows during an investigation. The number of window switches
was recorded using the same background script that measured time spent on an investigation.

Investigation Quality Evaluation. Because users followed an SOP document for each investi-
3.2.6
gation, we were able to create a rubric for the “gold standard” ticket based on our SOP. The rubric
included fields that were based on vendor-submitted SOPs, wherein they detailed any additional
information for their tool that should be included in the ticket. All tickets generated by each tool
were aggregated and assessed for accuracy and completeness according to the rubric for that
scenario.

3.3 Study Workflow
To accommodate studying the SOAR capabilities/characteristics above, we designed our test frame-
work to encompass the entire process of acquiring and utilizing a SOAR tool, from vendor installation
into our test environment, to SOC participants working in realistic scenarios with the tool. SOAR
vendors were asked to treat installation of their tool into the test environment as a fresh install.
They were given all information needed for integrating with SOC tools and data feeds, as well as
the SOPs describing investigation steps so they could create automated playbooks. The vendors
worked with the research team to install the tool, and the research team documented the process as
described in Section 3.2.1. Once the SOAR tool was established in the test environment, it was tested

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

9

by our research team for DDIL resilience as described in Section 3.2.2. Separately, a user study
was administered with US Navy SOC operators, which corresponds with evaluations described in
Sections 3.2.3–3.2.6.

The workflow for a US Navy participant was to remotely log into a SOC workstation in our
environment and share screen with two research team members who administered the test. At
the initial log-in, a hands-on tutorial was performed by the participant to learn the tools in the
environment (e.g., how to search in the SIEM, use the malware sandbox tool) and to learn how to do
a handful of investigation scenarios. Section 4.6 provides descriptions of the investigation scenario
types. Similarly, during a test with a SOAR tool, a vendor-provided tutorial on how to use the
SOAR tool was completed before testing. SOPs, which served as step-by-step guides for each type
of investigation scenario, were provided by the research team and followed by the participants. In
addition, the SOPs served as a “blueprint” for the SOAR vendors to design playbooks and automated
workflows. Each participant completed multiple scenarios for each tool they tested. These scenarios
consisted of two NIDS scenarios, two Malware Triage scenarios, a Malware Investigation scenario,
an IP Report scenario, and an Across-SOC Sharing scenario. During the investigations, participants
shared and ran a script that recorded their active window each second (allowing completion time
and window-swapping counts to be computed). All investigations require documentation in a ticket
or report, allowing a data source to be collected for quality and accuracy of the conclusions. At
stages throughout the test, we administered usability questionnaires (Appendix D) and conducted
a semistructured interview (Appendix E) to better understand the participant’s experience of the
tool, in addition to recording their usage during testing.

4 BUILDING THE TEST ENVIRONMENT
To perform the tests, a test environment was designed to meet many constraints and challenges.
Overall, the test environment must provide many SOC workstations and allow many representative
security investigations to be completed with and without a SOAR tool while measurements are
taken. Here, we itemize these challenges and our approach. Note that the SOC operators for our
test were US Navy security professionals in diverse geographic locations; hence, remote access
from a secure SOC into the environment is a constraint to our particular application.

4.1 Establish a Uniform Baseline Environment for Operator Testing and SOAR
Installations that Is Reproducible, Easily Learned, and Representative

To accommodate a user study of SOC analysts, we established a baseline environment with a
representative SOC tool suite. Our goal was to establish a SOC work environment in which the
analysts could, with minimal training, become equally familiar (in comparison to each other) prior
to engaging with the SOAR tools so that our focus could mainly be on the investigation.

To solve this challenge, we built a test network that simulated a small enterprise composed of
approximately 50 virtual machines complete with servers, workstations, and networking with a full
security software stack (see Figure 2). The IP addresses for each of the core components were fixed,
which enabled consistency across tools and assisted the analysts in developing “muscle memory”
in the environment. Participants learned the environment by following an detailed tutorial of the
baseline environment prepared by the research team. Multiple instances of the baseline are needed
to facilitate concurrent tests and ensure the same starting conditions for each SOAR tool. Once
established, this baseline environment was duplicated onto separate nodes in our environment
such that each vendor’s tool installation began from a consistent state.

The baseline environment consisted of a Windows Server 2019–based Active Directory domain
with a DMZ (demilitarized zone), internal enclaves, and cloud-provided services. We used Elas-
ticsearch with a Kibana UI as the SIEM, Suricata as the NIDS tool, and Endgame as the Endpoint

, Vol. 1, No. 1, Article . Publication date: August 2022.

10

Bridges, et al.

Fig. 2. Diagram of the baseline experiment environment depicted.

Detection and Response (EDR) tool. In the cloud, we used Jira instance for ticketing, ThreatGrid
for malware investigations, and a dedicated email system with an associated, registered domain.
Several Linux systems provided common server functions including database, web, and file transfer,
as well as entry points for compromising the environment in detectable ways, which was needed
to produce the scenarios. We used Windows 10 desktops as user workstations and SOC operator
workstations. Agents for endpoint management and protection were installed on the workstations,
and user activity was simulated using the user emulation technology D2U [35]. Full network
vulnerability scans and their associated logs were made by Nessus. This activity provided benign
network security events for monitoring by the SOC. All logs and events were consolidated by the
SIEM at approximately 100K records per hour.

4.2 Train the Analysts to Perform Scenarios Using Unfamiliar Tools and Procedures
The same training materials developed for the analysts were provided to a vendor along with a set
of slides describing the expectations for the vendor. In addition, the vendors were provided with
the original training documents, so they could be modified for analyst training for their SOAR tool.
The vendors were responsible for the training materials and SOPs provided to the analysts for their
own tools. This method served as a proxy for training provided by a vendor.

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

11

4.3 Enable Remote, Secure connections to the Test Environment for Remote Analysts
For our particular sponsor, we needed to enable simple, fast, secure, and flexible remote access
to our experimental environment for both vendors and SOC analysts. Analysts participated, for
example, across 18 hours of time zones with various bandwidth, times, and locations affecting
their Internet access. We also needed to prevent unfettered access to the environments to ensure
the integrity of our experiment. Additionally, each time the analysts participated, they needed to
connect to a new test environment.

As analysts were onboarded, a single account, long passphrase, and unique port number was
assigned to each of them. They used this single set of credentials to establish an SSH tunnel to
a single, pass-through SSH server located in the cloud, which was the only system with access
into our firewalls. We controlled when the accounts were enabled and to which SOC workstation
destination the tunnels were connected. The analyst was unaware of the complexity and would
simply establish the tunnel and connect their RDP client to a consistent port on their localhost.

4.4 Test Multiple Analysts at the Same Time on the Same Tool
Logistics around SOC operator availability meant that we need to host multiple operators testing
different tools simultaneously. The assigned user accounts were enabled on the Windows Active
Directory in each environment with the same passwords, so the individual analysts used their user
account in each vendor’s environment, which greatly simplified collecting and recording results.
With multiple, identical SOC workstations in each environment, we could test multiple analysts at
the same time.

4.5 Sequentially Evaluate Vendors While Preventing the Later Vendors from Gaining

Advantage

There was a risk that tools scheduled to be tested later in the experiment would experience an
advantage due to increased familiarity of the research team and of the participants with the
functionality of the network environment and the testing process. To help mitigate these effects, we
randomly assigned a pair of research team members to one or two tools such that our internal test
leads for each tool were all roughly at the same level of experience. In addition, we also limited the
exact, repetitive investigations that operators performed across the tools to prevent memorization
from test to test.

4.6 Design and Build Investigation Scenarios
To test a SOAR tool, representative investigations needed to be possible for the analysts to work.
Our initial efforts including meeting with many SOC operators to learn about scenarios that were
often repeated, as this informed us of the SOC operators’ target actions that SOAR tools could assist.
Ultimately, the goal was to create multiple types of scenarios, then have a method of reproducing
the scenarios (e.g., by curating all the necessary artifacts in the SIEM to allow a participant to
perform the investigation), but with slight variations. It was important to produce similar but not
identical versions of each scenario so, as the same participant tested different tools, they were not
working identical events and simply remembering details. On the other hand, similarity across
variants is needed, so difficulty of tests did not vary.

Five types of scenarios were completed by participants. They are listed below, with more detail

provided in the subsections.

• Network Intrusion Detection: Operators were tasked with determining the nature of a

detected intrusion event and identifying hosts on which it was present.

, Vol. 1, No. 1, Article . Publication date: August 2022.

12

Bridges, et al.

• Malware Triage: Operators performed this scenario twice with two malware samples. For
each sample, they were instructed to obtain the VirusTotal score and record a decision of
benign, suspicious, or malicious for the samples. Of the two samples, one was benign, and
the other was determined to be either suspicious or malicious.

• Malware Investigation: The nature of the malware sample that was determined to be

suspicious or malicious was further investigated.

• IP Reporting: Given a time range and a description of suspicious activity for an internal IP

address, analysts generated a report after analyzing packet capture (PCAP) files.

• Across-SOC Sharing Add-on: After completion of a scenario, the report was shared using

the SOAR tool’s sharing mechanisms.

4.6.1 NIDS Scenario. NIDS scenarios and the Malware Triage Scenario were designed to be rep-
resentative of common tasks for a Tier 1 (i.e., junior) operator. For these scenarios, the analyst
investigated an alert generated by Suricata, the NIDS component of the test environment. The
analyst had to understand the given NIDS alert and determine if it was a true or false indicator
of compromise on the network. If a true positive, the list of affected machines was determined
using the SIEM to gather evidence. Some of the NIDS tests contained information such as common
vulnerability and exposure (CVE) numbers that could be easily enriched, while others were more
vague. With these types of scenarios, the SOAR tools could assist the analysts by enriching the
content of the alert and performing common data-gathering tasks. In particular, these scenarios
exercised the SOAR tools’ ability to interact with the SIEM and to follow complex playbooks.
Unassisted, these scenarios took the analysts about an hour to complete.

All of our scenarios had to be real so the SOAR tools could apply their resources to enrich the
events for the analyst. To develop the NIDS scenarios, we surveyed recent CVEs, selecting 20 that
we could implement in our baseline environment. Often this involved adding misconfigured servers
or software. Once created, inside the baseline test environment, we triggered the compromise and
worked out the complete scenario through all cybersecurity systems. These were then turned into
events that were triggered for the analysts to investigate.

4.6.2 Malware Triage Scenario. Malware Triage scenarios were the second of the common tasks.
For these scenarios, a potential malware sample was copied to a Windows system, causing the
EDR tool, Endgame, to alert and quarantine the sample. Unassisted, the analyst was expected to
retrieve the sample from Endgame, submit it to the VirusTotal website to obtain a consensus score,
and mark the sample as benign, suspicious, or malicious based on the score. Depending on this
determination, the analyst directed Endgame to take certain actions with the samples. If the sample
was determined to be suspicious, for example, a ticket was generated in Jira to elevate it for further
investigation. These scenarios exercised the abilities of the SOAR tools to interact with Endgame’s
API to carry out automated responses. Unassisted, these scenarios typically took analysts about 15
minutes. Assisted by SOAR tools, these became almost instantaneous because the complete chain
could be automated.

To build these scenarios, benign, suspicious, and malicious software samples were selected from
a repository we built for previous challenges. The first qualification for a sample was that Endgame
alerted on it; finding benign samples that would elicit a reaction from Endgame required extra
effort. This pool of samples was each submitted to VirusTotal for its ratings. As expected, known
malicious samples received high scores, and benign samples received low scores; finding samples
that received scores in between, again, required extra effort. In the end, 20 samples were selected
and developed into scenarios for testing by the analysts. The same samples were used on all test
environments.

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

13

4.6.3 Malware Investigation Scenario. In this scenario, samples triaged as suspicious were submitted
for further analysis using ThreatGrid, a cloud-based malware sandbox testing environment. These
scenarios evaluated the SOAR tools’ ability to interact with a foreign tool (i.e., a tool for which no
native integration existed for these SOAR tools), enrich the analysis of suspicious samples beyond
VirusTotal, and to interface this information to the analyst. After evaluation, the analyst would
report a final determination on the associated ticket.

IP Reputation Reporting Scenario. This scenario was derived from reporting tasks, which
4.6.4
currently require ample manual effort by senior analysts to dig through PCAPs to investigate a
questionable IP and then report findings in a precisely formatted report. Our analogous testing
scenarios involved detected, suspicious activity by a machine at a given IP address, warranting
the investigation and report. To build these scenarios, a set of PCAPs were captured containing
various suspicious interactions by machines at predetermined IP addresses. We created suspicious
activity by including interactions with foreign websites and various data infiltration and exfiltration.
A formatted inquiry request was created for each scenario, and a required report template was
provided. Unassisted, these scenarios were tedious and took at least an hour to complete.

4.6.5 Across-SOC Sharing (Add-On). The final scenario type was created to allow SOAR tool
vendors to demonstrate sharing and interaction between their software at different SOC locations.
Upon completion of one of the above scenarios, the analyst went through a set of steps to share
the report and/or extend an invitation to a colleague located at another SOC to collaborate on
an incident. The analyst would then log in as the remote analyst and retrieve the shared report.
By definition, the baseline environment did not have any sharing capabilities whereas the SOAR
tools all purported to provide an extensive variety of sharing and interaction abilities. The vendors
determined which sharing abilities were demonstrated. The analysts answered questions regarding
the ease, feasibility, and security of the sharing mechanisms.

5 DATA & RESULTS
We used a combination of qualitative and quantitative measures to assess the benefits of SOAR
tools. This section presents the results of our evaluation.

Table 1. Counts of configuration ratings per category across all six
tools.

5.1 Vendor Installation and Ingestion
Two research team members were assigned to assist each tool vendor with their installation.
Upon completion of the installation period, the team members completed a survey document-
ing the process for installing each tool. For analytical purposes, a separate team member read
these documents and discussed
the tool installation and config-
uration process and collected
information, distilling results
into three categories: data in-
gestion, ticketing/Jira integra-
tion, and playbook setups. Ques-
tions relating to data ingestion
addressed how easily the tool
could integrate with multiple
data sources. Similarly, ease of
integration with our ticketing system, Jira, was also evaluated. Questions related to playbook setup
addressed playbook flexibility, vendor support, documentation, and ease and speed of deployment.
In each of these categories, the tools received a score of -1, 0, or 1 indicating poor, neutral, and

Well (1)
Neutral (0)
Poorly (-1)

Configuration
rating

Ticketing/Jira
integration

Playbook
setup

Data
ingestion

1
2
3

1
3
2

2
3
1

, Vol. 1, No. 1, Article . Publication date: August 2022.

14

Bridges, et al.

good performance, respectively. Overall configuration was defined as the average of the scores
in each category (Table 1). To respect tool anonymization requirements, we are not permitted to
release the per-tool configuration scores.

Observation: Our observation is that configuration of SOAR tools involves creative design by the
installation team, especially with regard to displaying information, automating tasks, and facilitating
effective user interaction with the SOAR tool. Consequently, making this an iterative process with
users is critical to avoid pitfalls such as inconsistent data placement across playbooks that leads to
confusion. It is important to have clearly defined procedures for playbook/workflow creation and
SOAR automation, especially to determine what is “OK” to automate and what requires the analysts’
attention.

5.2 DDIL Results
This section presents the results of the tools’ abilities to navigate a DDIL environment (discussed
in Sec. 3.2.2).

5.2.1 Denied Internet. For this test, a Malware Investigation scenario was performed. Of the six
SOAR tools tested, one tool ran its playbook and completed a ticket, along with providing warnings
of queries that were unsuccessful. Three tools executed their playbooks in some capacity but
eventually stalled and were unable to complete. Of these, two displayed an error message, and one
did not. For the remaining two tools, one ran its playbook through to completion but produced an
incorrect result. The other, a cloud-based tool, was unable to initiate its playbook at all.

Observation: The results of Denied Internet testing varied widely. Most tools (excluding the cloud-
based tool) were able to at least initiate their playbook, investigation completeness and accuracy were
inconsistent.

5.2.2 Disrupted/Intermittent Internet. For this test, we investigated a tool’s ability to identify
malicious network activity that was generated during a lapse in internet service. Only one tool did
not recover network events once its internet connection was restored. The other five tools were
able to do so, with one tool given special attention for alerting with an on-screen notification that
there had been a lapse in internet service.

Observation: All but one tool were able to recover and report network events that were generated

during a lapse in internet connection.

Steady-State Limited Bandwidth. For this test, we investigated each SOAR tool’s average
5.2.3
load on a network (i.e., the amount of bandwidth it used to monitor events). Values were reported
in the format of mean ± standard deviation. Per 1,000 events, tools required 0.373 MB, with a
minimum of 0.016 MB and a maximum of 1.75 MB.

Observation: Total network traffic was measured in MB for each SOAR tool during a period of rest
while no actions were being taken on the network by users; the SOAR tool was simply monitoring the
network. The amount of required bandwidth to monitor the network varied widely across tools.

5.2.4 Working-Test Limited Bandwidth. The final DDIL test case used a NIDS scenario and records
the bandwidth in MB required to conduct the investigation start to finish. On average, the tools
required 22.80 ± 15.73 MB, with a minimum of 6.5 MB and a maximum of 46 MB.

5.3 Recruitment, Ethics, & Participant Demographics
We conducted this study in order to evaluate SOAR tools for our Navy sponsor with Institutional
Review Board (IRB) approval. We provided participants with an information sheet detailing the
nature of the research and their rights prior to their agreement to join the study. In total we had 24
participants (Table 2) in the task-based evaluations, with 21 Navy analysts and 3 ORNL personnel.

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

15

On average across the participants, operators had 5.14 ± 5.82 years of experience, with a minimum
of 5 months and a maximum of 21 years. Two participants did not fill out the demographic survey.

Table 2. Demographics of participants: job role & SOAR familiarity

Count

Count

Job role

3
11
10

SOAR familiarity

Network operator
Security operator
Other

Expert user
Never used
Somewhat familiar

5.4 Qualitative Evaluation
For each tool an analyst tested,
we conducted a semistructured
interview (see Appendix E for
the interview guide) after test-
ing with that analyst to better
understand the analyst’s experience with and perspectives on the tool. A research team member
then coded the data to better understand the key themes present in the data. Figure 3 summarizes
the prevalence of codes expressing positive sentiment, negative sentiment, and reasons for concern.
This qualitative evaluation provides insight into how analysts perceived the tools, the benefit the
tools might provide, and the potential downsides to SOAR tool adoption. Our evaluation showed
that, even if an analyst clearly preferred a specific tool, they might not perform best with that tool.
Quantitatively, in terms of measures such as time to complete tasks and ticket completeness, they
may have performed better with a tool different from the one they preferred. This disparity can be
expected given that analysts’ preferences may be based on many factors besides performance, such
as the usability or attractiveness of the tool’s UI or a specific feature of the tool.

1
9
12

Notably, several analysts recognized that, given more time to optimize each tool for the environ-
ment, several of the tools have more potential than was demonstrated in this evaluation—see a
quote from participant 5 (P5) below. Given that we did not have time or resources to fully configure
each tool, it was an inherent limitation of our approach that the full capabilities of each tool would
not be exercised.

Tool has more potential – P5 – ‘While there are some concerns with how to manage cases
and collaborate amongst analysts, the capabilities of the tool would certainly increase
our SOC’s efficiency. Many of my concerns regarding triage and data visualization e.g.
dashboard elements, are likely a result of how the system is configured to support the
challenge and not a limitation of the technology itself. We could likely tailor our own
dashboards to better get after APT and effective triage.’

5.4.1 Appropriate Role of Automation. While SOAR tools are intended to take analysts out of the
loop to increase efficiency, there are voices of caution related to automation. Schneier [29] reminds
us that we cannot automate what we do not know and, therefore, can never completely remove
humans from the loop because uncertainty is a given in cybersecurity; similarly, Baxter [36] echoes
Schneier’s assertion, that the more we depend on technology the more we also depend on highly
skilled individuals to ensure that our technology is both resilient and properly configured. Several
participants in our study, as demonstrated by the quote below from P12, also expressed concerns
that too much automation could have unintended negative consequences. It is interesting to note
that while our initial survey of analysts, conducted prior to testing, showed that automation was the
most-sought-after feature in a SOAR tool, once analysts actually utilized the tools, they expressed
concerns regarding too much automation. Determining the appropriate place for automation in
SOCs is an open area of research ripe for further study.

Concerned Too Much is Automated – P12 – ‘There should be a double check option, in
case there was an automation error. We need to review tickets before closing it out.’

, Vol. 1, No. 1, Article . Publication date: August 2022.

16

Bridges, et al.

Fig. 3. Participant responses coded to the semistructured interviews for Tools 1–6. This figure provides a
heatmap with the count of each code for each tool, where the count represents the number of participants
who had at least one statement with a given code.

5.4.2 Usefulness Dependent on Integration and Training. Across solutions, multiple participants
expressed concern that these tools would be difficult to integrate into their environment and require
highly trained developers/analysts, as demonstrated below by quotes from P14 and P5. It should be
noted that these concerns match the experiences of companies that currently utilize SOAR solutions.
The complexities of integrating SOAR tools are such that Gartner recommends beginning with
several clear use cases to ensure that integration has a clear objective to help prioritize data sources
for effective ingestion and quick impact [7]. Not all organizations have the necessary personnel
or existing infrastructure to utilize SOAR solutions, which is why defining clear use cases is so
important.

Usefulness Dependent – on Integration – P14 – ‘Highly dependent on Automation
functionality and integration into our current architecture. Current toolset is already
baked in and analysts are well trained.’
Only as Good as Developers – P5 – ‘The tool would certainly enhance our SOC by
reducing the amount of time taken to conduct analytic functions. I do however feel that we
would have to spend a considerable amount of time configuring, updating, maintaining
the playbooks to ensure that it was working as expected.’

5.5 Data & Preprocessing
To enable data analysis, data was parsed from various raw formats into three tables (not presented in
raw form but discussed). The user table provides columns for each participant’s unique ID number

, Vol. 1, No. 1, Article . Publication date: August 2022.

Good IntegrationsGood TicketingHelpful CollaborationNice InterfaceInformative InterfaceGood Insight / CorrelationGreat AutomationPositive CodesMissing / Incomplete IntegrationsBad TicketingUnhelpful CollaborationNot Nice InterfaceLack Information in InterfaceToo Much InformationPoor Insight / CorrelationFailed to Realize its PotentialLittle Benefit Over Existing SolutionNegative Codes123456Usefulness Dependent IntegrationOnly as Good as DevelopersConcerned Too Much is AutomatedCodes Expressing Concerns0.02.55.07.510.002460.02.5Testing SOAR Tools in Use

17

and their responses to the demographic survey. Section B of the appendix lists demographics
questionnaire items, and Section 5.3 gives an overview of the participants’ demographics. Similarly,
the tool table provides fields for the tool’s index number and their scores for the four configuration
categories: data integration, ticketing/Jira integration, playbook setup, and overall configuration
(i.e., an average of previous three). Section C of the appendix lists installation questionnaire items
completed by our research team upon SOAR install teams, and summary results are given in Section
5.1. Finally, the results table houses all testing data, with each row indexed by the combination of
user and tools columns and the remaining 75 columns for the measurement data (for that user’s
test of that tool).

5.5.1 Measurement Categories and Subcategories. The 75 measurements used in this evaluation
are broken naturally into five categories and, in turn, multiple subcategories; preprocessing varies
per category.

• Likert ([1–5]) responses data to the 32 “usability” survey questions was normalized to
prevent user bias by subtracting the user’s mean (i.e., the mean of all Likert responses for all
tools by that user), as some users are overall more critical/charitable in their responses. All
data was scaled and translated into the interval [0,1]. As shown in Section D of the appendix,
these questions naturally fall into seven subcategories. We considered averages of responses
to questions pertaining to the following subcategories: training quality, ability to automate
recurring (“1,000 time per day”) tasks, collaboration capabilities, ability to handle advanced
persistent threats (APTs), usability, information quality, and interface quality.

• Free responses were gathered from the semistructured interviews and comprise 20 columns
(20 different response values) in total. We applied sentiment analysis using TweetEval1, a
roBERTa-based sentiment analyzer [37], mapping the raw text to a value in the interval [-1,
1], indicating negative (-1) to neutral (0) to positive (1) sentiment. As before, we normalized
the data by subtracting the users’ mean to account for user bias and then mapped the values
linearly to [0,1]. As shown in Section E of the appendix, these questions naturally fall into five
subcategories. We will consider averages of responses to questions pertaining to the following
subcategories: ability to automate recurring tasks, collaboration capabilities, information
quality, attack scenarios, information quality, and a general category comprised of questions
about the tool.

• Ticket completion ratios were collected by scoring the tickets produced by investigations
for accuracy and completeness based on a rubric we created from the SOP for each attack
scenario. In total, the ticketing data accounts for seven columns. Because scenarios vary widely
in difficulty, ticket completion scores are not comparable across scenarios. Consequently,
we normalize per scenario by subtracting the scenario’s mean ticket completeness across
all users and then linearly map back to [0,1]. (One may wish to normalize by each user’s
mean per scenario, but, as many users only worked tests with one to a few tools, this was
not viable.) We considered subcategories averages (for each user and tool, we average the
responses for each subcategory) of each scenario type.

• Window swapping counts were parsed from a custom Python script that programmatically
recorded the user’s active window during each investigation. In total, eight columns were
attributed to window-swapping data. Similar to the ticket completion scores, the window swap
counts varied based on scenario, and we normalized by subtracting the per-scenario means.
The normalized counts were scaled and shifted to [0,1] and then inverted via 𝑥 ↦→ (1 − 𝑥),
maintaining the convention that larger scores indicate better performance.

1Code for this sentiment analyzer is available: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment.

, Vol. 1, No. 1, Article . Publication date: August 2022.

18

Bridges, et al.

• Investigation completion time, as with window swapping counts, was computed from
the Python script, normalized per scenario, mapped linearly to [0,1], and inverted via 𝑥 ↦→
(1 − 𝑥). The same eight columns were reported for the investigation time data. We considered
subcategories averages for results of each scenario type.

Once processed, we had a results table with 68 rows (user-tool tests), and 75 measurements
columns. All measurement values reside in [0,1], with smaller/larger values indicating worse/better
performance in that category. Each column attains the minimum (0) and maximum (1). There are
3,517 filled of 5,100 cells in this table (69%). Missing data can be explained by the fact that many Likert
and free response category questions were not applicable to baseline (i.e., no SOAR tool) tests; each
test involved different, specific scenarios leading to different filled/missing columns of ticket comple-
duration;
counts,
tion,
three Tier 3 (i.e., senior) analysts worked in-
vestigations with every tool, but (because of
logistical constraints) duration and windows-
swapping data was not collected for these tests;
and some participants neglected to answer
some questions. See Figure 4.

window

swap

time

and

Broadly, our data falls into five overarch-
ing categories: Likert survey responses, free
response survey responses, ticket completion
fraction, window swap counts, and investiga-
tion time.

5.5.2 Public Data. To make this dataset public,
we combined the normalized results table with
the user demographic table (see Section B of the
appendix for descriptions of the questions that
led to the data in this table), which adds four
columns providing info on the user in the test
to the 75 columns of user-tool tests, allowing
investigations of correlations across user de-
mographics and testing results. To protect the
users’ and vendors’ privacy, we excluded user
and tool identifiers and coarsened the added de-
mographic columns to binary values and then
applied a bit of noise to all of the data. This user-
and tool-anonymized and -obfuscated table has
been made public (link on first page in the title
notes), with greater explanation, and we have provided code to reproduce some of the correlation
results, showing that data utility was not lost in the obfuscation process.

Fig. 4. Binary heatmap of normalized data table (68
rows for tests, each indexed by user × tool, by 75 mea-
surements, not all measurements applicable to each
test) shows distribution of missing data. Top six rows
are baseline tests for which many questions are not
applicable; missing data on right half of matrix is attrib-
uted to different scenario versions being worked per
test and a few operators working investigations with
tools for which monitoring scripts for data collection
were not possible.

5.6 Results Summary per Tool
Figure 5 summarizes all collected data including per-category and per-subcategory averages for
each tool. Notably, Figure 5 shows a discrepancy between some of the qualitative and quantitative
measurements. For example, Tool 5 scored very well in the Likert response and sentiment data but
lacked efficiency, as it required more context switching and on average longer times to complete
tasks. In contrast, Tool 2 was the most efficient in terms of context switching and investigation
time, but was not a preferred tool based on Likert response and sentiment data. Ticket completion

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

19

Fig. 5. Tool performance for each type of data collected. Dark colors imply strong performance; light colors
imply weak performance. Top: Broad categories of data for each tool. Bottom: Subcategory details indicate
the contribution of each to the overall score for the tool in the category. For instance, Tool 5 performed the
best in the Likert Response category, and it is largely driven by its high scores in the collaboration, usability,
and interface quality subcategories. Similarly, Tool 2 performed well in the investigation time category, with
that mostly attributed to the IP Reporting (IPREP), Malware Analysis, and Across-SOC (XSOC) scenarios.

fractions are inconsistent across tools for a number of reasons. Baseline (i.e., test investigations
performed with no SOAR tool) had the most complete tickets overall, but it is important to recognize
that this was accomplished in the absence of automation and the operators had to manually complete
every ticket. This is reflected in the time data for baseline. Furthermore, auto-population of tickets
occasionally exhibited great inconsistency for the same tool: some fields were populated for some
investigations but not for others.

Observation: Two SOAR tools on average received poorer Likert results than the baseline, and all
SOAR tools ticket accuracy scores were worse than the baseline. Four SOAR tools decreased the number
of window swaps required for investigations, and all SOAR tools improved or did not exhibit degraded
investigation times and received better sentiment responses compared to the baseline.

Observation: Areas of strengths and weaknesses varied greatly across these six SOAR tools. Many
of the problems and limitations discovered in the test were seemingly fixable with iterative tuning
(e.g., insufficient content in automatic ticket population, bugs in integrations, confusing layout of

, Vol. 1, No. 1, Article . Publication date: August 2022.

20

Bridges, et al.

Fig. 6. The 75 normalized measurements are coarsened into 25 subcategories by averaging measurements
that fall in the same subcategory (e.g., multiple Likert responses pertaining to usability are averaged to obtain
the subcategory ‘likert-usability-ave’) for each test (user, tool). Correlation of these subcategory scores across
all tests (user, tool) is depicted. Multiple imputation is used to prevent artificial inflation of correlation caused
by missing data. Missing data is sampled from kernel densities estimated from the missing values’ peers,
and the average pairwise correlation across 𝑚 = 1, 000 samples is depicted. On the left of the heatmap is
a dendrogram showing hierarchical clustering of all subcategories based on their the mutual correlations.
Notably, the data breaks into two main clusters. The first cluster (from bottom) included every Likert and
free response subcategory, showing user sentiment is on average consistent throughout the questionnaires.
Meanwhile, the second cluster included every subcategory of the Ticket Quality Ratios, Window Swapping
Counts, and Time per Investigation data, indicating that performance with the tools trend together. Finally,
we note that, broadly, the correlations of these two clusters (top right/bottom left of heatmap) are light
blue, indicating a slightly negative correlation. While users may express favorable/unfavorable sentiment
in questions about a SOAR tool, their performance with the tool may trend in the opposite direction. This
accentuates the importance of using objective measurements of performance in addition to the qualitative
data collected.

configurable displays); hence, we suggest iterative operator use, feedback, and reconfiguration to be
expected upon deployment.

5.7 Correlations in the Test Data
Next, we considered correlations in the data. Rather than looking at data per tool as in Figure 5, in
this step, we explored correlations across all tools. For each row in the normalized data table, which

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

21

corresponds to a 75-length vector of user–tool test results, we shortened the vector by averaging
those columns from the same subcategory. For example, multiple questions about how well the tool
assists recurring SOC tasks are given to each user in each tool test, allowing us in this analysis to
average these few questions’ response scores to get that users’ average “likert-recurring-task-ave”
score. Similarly, as two different NIDS and Malware Triage scenarios were completed in each test,
we averaged the corresponding measurements per scenario. This method allowed for aggregating
multiple measurements in a single category, reduced the number of features, and improved the
ratio of missing data (e.g., if a user declined to answer one question, we could still compute an
average from their other responses in that subcategory).

The subsections details our correlation data analytics approach in the subsection below, while
this introduction summarizes takeaways. Figure 6 shows a heatmap correlating subcategory scores
across all tests. The subcategory averages form two main clusters. The first cluster includes every
Likert and free response subcategory, showing user sentiment is on average consistent throughout
the questionnaires. The second cluster included every subcategory of the Ticket Quality Ratios,
Window Swapping Counts, and Time per Investigation data, indicating that performance with the
tools trend together. Additionally, we noted that, broadly, the correlations of these two clusters (top
right/bottom left of heatmap in Figure 6) is light blue, indicating a slightly negative correlation.
While users may express favorable/unfavorable sentiment in questions about a SOAR tool, their
performance with the tool may trend in the opposite direction. This accentuates the importance of
using objective measurements of performance in addition to the qualitative data collected.

Observation: The users’ sentiment and Likert responses for the tools were positively correlated with
each other. Similarly, the performance measurements with the tools were positively correlated. Yet,
these two groups of measurements were slightly negatively correlated, indicating that user prefer-
ences about the tools do not match how well they performed with the tools. Our data implies that
SOAR tool selection will require a balance between performance (e.g., investigation quality, efficiency,
and context switching) and user preferences.

The next two subsections describe the analytic methods

used to obtain insights from the data.

5.7.1 Handling Missing Data, Multiple Imputation (MI). Di-
rect computation of correlation of two vectors with missing
data (i.e., by simply ignoring components with missing values)
overweights the contributions of the present components. Con-
sequently, we employed multiple imputation (MI), a standard
technique for statistical analysis with missing data [38, 39],
and compared the MI results to the naive statistics computed
with missing values when drawing conclusions. MI estimates
the desired statistic (in our case, correlation) by averaging
samples from a Monte Carlo simulation of the missing data.
Given two vectors 𝑥, 𝑦 (in our case, columns of a results table)
with missing data, we leveraged the fact that all measurements
lie in [0, 1] and impute (i.e., simulate) the missing components by sampling each 𝑚 = 1, 000 times.
Furnished with now-complete, partially imputed versions of our data {(𝑥𝑖, 𝑦𝑖 }𝑚
𝑖=1, the average
correlation (1/𝑚) (cid:205)𝑚
𝑖=1 corr(𝑥𝑖, 𝑦𝑖 ) was used, and we leveraged the imputed sample statistics to
quantify confidence of findings with hypothesis tests and confidence intervals. We used 𝑚 = 1, 000
by investigating convergence empirically—plotting and seeking negligible average ℓ1 change of the
average correlation upon adding more samples.

Fig. 7. Kernel densities estimated from
leaf nodes of a decision tree regressor,
depicted for three subcategories. Sam-
pling from these estimations is used
for imputation of missing data.

, Vol. 1, No. 1, Article . Publication date: August 2022.

22

Bridges, et al.

We considered results from two approaches. The first approach involved sampling uniformly at
random (𝑝 (𝑥) = 1 for all 𝑥 ∈ [0, 1]), which quantifies results from the standpoint that all possible
values of a missing data point are equally likely. The second sampling method for a missing target
value leveraged all data for that user–tool test (row)—the user demographics data, tool configuration
data, and all result columns except the target column—to estimate a probability distribution that
is more precise. A decision tree was trained
to predict the target column from all other
columns (using the known data), and kernel
density estimates (KDEs) were used to produce
a probability distribution from the training sam-
ples residing in each leaf of the tree. Imputed
values were then sampled from the KDE corre-
sponding to its leaf node, that is, from the KDE
informed only by the known data that had been
learned to best predict the value of the target
column for those with similar other columns.
See Figure 7.

Full details of the approach are provided in

Section G of the appendix.

5.7.2 Correlations in Subcategory Averages.
Figure 6 displays the correlations between
all pairs of the 25 subcategory measurements
alongside hierarchical clustering of each mea-
surement based on their mutual correlations.
The takeaway from this analysis is immediate
from the plot: all questionnaire subcategories
cluster together with high correlation, as do
the performance measurement subcategories
(Ticket Completion Fraction, Window Swap-
ping Counts, and Investigation Time), but the
between-group correlations are slightly nega-
tive. This indicates that, for a given test, the
participant expressed broadly consistent Lik-
ert responses and sentiments when respond-
ing to the questionnaires, but their feedback
was on average slightly in opposition to their
performance with the tool, as indicated by our
measurements.

Fig. 8. Images depicting 95% confidence interval for
correlation of subcategory average values based on
𝑚 = 1, 000 imputations of missing data cells sampled
from KDEs fit to missing values peers. Due to space
constraints, out of the 300 pairs of Subcategory Average
columns, we randomly selected two subcategories from
each category and displayed the confidence intervals
for 45 pairs’, a subset of which are labeled on the x-
axis. Notably, all the pairs of subcategories from Likert
and free response and, separately, pairs of the objec-
tive measurement subcategories have 95% confidence
intervals above or nearly above 0, indicating they are
almost certainly mutually correlated. Confidence inter-
vals for subcategory pairs crossing these two groups
lie mostly and sometimes fully below 0. These results
provide evidence that the two main clusters of Figure
6 indeed have positive within-group and small or even
negative between-group correlations.

To further investigate this finding, we com-
puted 95% confidence intervals for our imputed
correlation samples, following Schafer [38] (in our case, with 𝑚 = 1, 000, this is a 𝑧−test for a
normal distribution). See Figure 8, We provide more details and data for this correlation analysis in
Section G of the appendix.

Considering the coded responses in Figure 3 and Figure 5, we can find specific evidences of the
subcategory correlation results. Consider Tool 2, which according to the coded responses (Figure
3) was praised by many operators for “great automation" who also reported concerns regarding
overautomation. The average responses (Figure 5) for Tool 2 show that this tool was, on average,

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

23

best at reducing context switching (as evidenced by the lowest number of window swaps) and the
quickest (as evidenced by the lowest investigation times), but received middle-of-the-road scores in
the Likert and free response categories. Furthermore, the average Likert and free response results for
Tool 5 show that it was overwhelmingly preferred by users, but it scored by far the worse in ticket
completion and delivered mediocre performance in window swap count and investigation time.
Hence, even though operators provided strong feedback, ticket quality suffered, and reduction in
context switching and investigation time were mediocre. These results underscore the importance
of using quantitative measurements of the efficiency, quality, and context switching enabled by the
SOAR tools.

5.8 Correlation of Test Data with User Demographics and Tool Configuration
We also investigated correlations among user demographics data, tool configuration data, and test
results data. See Section 5.5 for an overview of the data tables. Results are visualized in Figure 9.
Recall ”Job Role” question was encoded as a 1 for the response “SOC operator” and as a 0 for
“NOC (network operation center) Operator” or “Other.” The “config-ave” column is the average
of the three different configuration ratings given upon installation of each tool (i.e., the overall
configuration). ‘Data-config’ references ‘Data Integration,’ ‘playbooks-config’ references ’Playbook
Setup,’ and ‘ticket-config’ references ’Jira/Ticket Integration,’ all of which are described in Section
5.1.

Observation: Correlations of test results with user demographics are negligible, except for “Job Role,"
which negatively correlated with questions intended to assess how well trained to use the SOAR tool
they believed they were. This suggests that non-SOC users (e.g, those trained for network operations or
other related IT roles) will need more training to feel equally comfortable (as SOC operators) with a
SOAR tool.

Observation: Configuration ratings (recorded at time of installation) for a tool’s ability to integrate
with diverse data sources and assist in ticketing were highly correlated with efficiency metrics whereas,
playbook configuration ratings were negatively correlated. This suggests that the appearance at time of
installation of well-configured playbooks may not be representative of their functionality in practice.

5.8.1 Do SOAR Tools Improve Efficiency in an SOC?. Figure 5 (top, right column) shows that,
compared to the baseline, four of the six tested SOAR tools clearly decreased investigation time
(top, second to right column) and the number of window swaps on average, and all tools were
as good as the baseline. Given that our results indicated most SOAR tools help, and none hurt,
efficiency metrics, we overlaid configuration data to check for correlations. Referring still to results
in Figure 5, of the two tools that, on average, showed no improvement to the baseline in these
categories (i.e., Tools 1 and 3), one was rated poorly in configuration scores (-1 for ticketing and
data integration, 0 for playbooks), while the other was given configuration scores (0 for ticketing,
-1 for data integration, and 1 for playbook configuration).

To investigate efficiency metrics correlation with configuration, we set up the following di-
chotomy: tools that received an average configuration score less than 0 were termed “not well-
configured whereas tools with an average configuration score at least 0 were considered “wellcon-
figured.”

Figure 10 compares window swaps vs. investigation time for well-configured SOAR tools, poorly
configured SOAR tools, and our no-SOAR-tool baseline environment. The primary takeaway of
the left plot is that, when comparing investigations with similar amounts of required window
swapping, SOAR tools took less time than the baseline, on average, regardless of configuration
rating.

, Vol. 1, No. 1, Article . Publication date: August 2022.

24

Bridges, et al.

Fig. 9. Heatmap detailing the impacts of user demographics and tool configuration on SOAR tool performance
in each of the data subcategories. The strongest correlation of a user demographic comes from "Job Role,"
where users functioning as an SOC operator felt less trained following vendor-provided training than did
those functioning as an NOC operator or in some other role. In terms of the quantitative metrics, (i.e., ticket
completion, window swapping, and investigation time), the overall tool configuration (config-ave) plays a
bigger role than for the qualitative Likert and sentiment scores. At a finer level, playbook configuration has a
slight positive correlation with some of the Likert questions, though the effects of this are neutralized in the
overall configuration due to the weak and opposite correlations of data configuration and ticket configuration.

Observation: For an investigation with a fixed number of window swaps, all SOAR tools provided an
efficiency gain in our data, with greater gains for well-configured tools. Our data implies that average
configuration is correlated with a SOAR tool’s impact on efficiency.

5.8.2 Do SOAR Tools Improve Investigation Quality? In addition to efficiency gains, we evaluated
the claim that SOAR tools improve investigation quality. The tickets created and populated by
SOAR tools provide documentation allowing us to examine investigation quality and to determine
whether increased efficiencies derived from using a SOAR tool might come at the expense of
investigation quality (defined as a thorough investigation).

As noted in the top center column of Figure 5, our baseline environment produced the most
complete tickets for the investigation. However, this result was expected given that tickets were
completed manually for the baseline. In the case of SOAR tools, there were some instances in
which the tools were inconsistent with the fields they populated in the ticket, leading to less-than-
thorough documentation of an investigation when compared to the manual recording with the
baseline environment.

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

25

Fig. 10. Left: Plot comparing efficiencies of SOAR tools and baseline environment. Both window swaps and
investigation time have been inverted to reflect favorable investigation conditions with fewer swaps and
shorter investigation times. We note that, regardless of configuration, SOAR tools offer an edge over an
environment with no SOAR tool in terms of efficiency. This is observed in the greater slope for the SOAR
tools as opposed to the baseline environment, in which more context switching (denoted by window swaps)
occurs in a shorter amount of time. Right: Subcategory averages from Figure 6, where the black dotted lines
highlight notable correlations of windows swaps and investigation time data with user demographics and
tool configurations. We observed distinct correlations in the window swaps data for the IPREP and Malware
scenario with the overall configuration at a coarse level, but then we note that is heavily influenced by the
playbook configuration and the ticket configuration, or Jira integration. For the investigation time data,
we note overall stronger correlations with the overall configuration of the SOAR tool, with IP Reporting,
Malware Analysis, and NIDS scenario times being influenced by playbook configuration and Jira integration.
In terms of user demographics, the Malware Analysis scenario may have a slight correlation with job role,
but otherwise there appears to be no real influence of user demographics on investigation efficiency.

Similar to our examination of configuration effects on efficiency gains, we probed the effect of
SOAR tool use on ticket completion, along with accounting for analysts’ years of experience to
determine whether more experienced operators completed more thorough investigations. Figure
11 shows that configuration status does not seem to impact ticket completeness, with both configu-
rations of SOAR tools performing similarly. Furthermore, no strong relationship appears to exist
between operator experience and ticket completeness.

Observation: When following an SOP step by step with no SOAR tool (i.e., in the baseline test),
operators produced far better investigation tickets (graded on correctness and completeness) than with
SOAR tools. Inconsistencies in SOAR tools’ ticketing automation were problematic for ticketing quality.

6 DISCUSSION
This section presents limitations, takeaways learned about SOAR tools and methods for testing
them.

6.1 Limitations
Due to the length of the testing and time restrictions, analysts were not able to test every tool.
Previous user studies (e.g., involving SOC operators) also report that SOC operator time constraints

, Vol. 1, No. 1, Article . Publication date: August 2022.

26

Bridges, et al.

Fig. 11. Left: Ticket completion fraction vs. operator years of experience. There appears to be no strong
correlation between ticket completion and experience for any of the configuration cases. On average, baseline
tickets were completed at a higher fraction than tickets completed using SOAR tools. This is largely due
to some inconsistencies that plagued SOAR tool ticket population, as is reflected in the NIDS scenario’s
correlation with ticket and playbook configuration status. Right: There are possible slight negative correlations
with some aspects of user demographics, such as number of SOCs they worked in and familiarity, indicated
by the blue boxes highlighted with black dotted lines.

limited their ability to assist in such studies (e.g., [3, 4, 31]). All tools had at least eight users, with
most users testing two or three tools. To offset this disparity, we had three Tier 3 analysts explore
all the tools in a hands-on manner in our environment.

Additionally, due to analyst availability and time restrictions leading to different users testing
each tool, we took care to compensate for analysts who were “tough” vs. “easy” in terms of their
feedback by normalizing the scores (by subtracting the user’s average). This same normalization
methods was also employed to compensate for the different levels of difficulty represented by the
test scenarios. Most analysts (21 of 24) were recruited from US Navy SOCs, with the remaining 3
coming from the research organization; this makes it difficult to generalize results beyond a specific
population. For best results, our experimental framework should be used to conduct additional
tests in the target environment.

One question of interest that remains is: Do SOAR tools allow junior operators to perform at
a higher level? Our testing method was unable to provide any answers to this question for two
reasons. First, ticketing quality was the artifact allowing quantification of investigation quality,
yet all SOAR tools tested were worse in ticket quality than the baseline. This leads us to believe
that analyst quality may be obscured by trusting ticket auto-population that was not configured to
perform to the same level of thoroughness as the analysts alone. Second, as seen in Figure 11, there
was no correlation of ticket completeness (our measure of investigation quality) with experience
level.

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

27

6.2 SOAR Takeaways
Configuration of SOAR tools is critical, and it requires creative design by the installation team as
well as iteration with the SOC to ensure the tool is usable and the automation satisfies SOC needs.
The SOC should have well-defined procedures for automation. One of the main lessons learned
shared by the senior analysts involved in the study is that SOAR tools can indeed overautomate
pertinent processes, so, SOCs should help curate a semiautomated process that expedites analyst
activities while prompting the user to complete investigation steps that require human attention
(and ideally the SOAR tool will present the needed correlated information so the analyst can focus
on the investigation).

Our average results indicated that analysts preferred SOAR tools to the baseline. Importantly,
our results also confirm that SOAR tools increase efficiency while also decreasing the number of
window swaps. Interestingly, even though the number of window swaps per investigation is lower
with SOAR tools than without, for the same number of window swaps, SOAR tools enable analysts
to work faster (Figure 10). The only “sore spot” was ticket quality, which likely can be addressed
with SOAR configuration and analyst training, given we are now aware that these influential
preliminaries can suffer.

Interestingly, our results revealed negative correlations between analyst-preferred tools and
performance. This was in part driven by overautomation concerns and poor ticketing of a highly
preferred tool, but shows that analysts preferred SOAR tools with which they performed worse
more than those with which they performed better.

Finally, if a SOAR tool will be used in a DDIL bandwidth environment, it should be tested for
how well it performs when an internet connection is unavailable or limited, as some tools failed
under this condition while others exhibited graceful declines in abilities.

6.3 Testing Takeaways
In terms of testing SOAR tools, we recommend designing investigation scenarios at the beginning of
the process, in conjunction with SOC operators and then building the environment to establish all
of the necessary tools an SOC analyst will need to work investigations, populating with necessary
logs and alerts, and so on. As many baseline environments are needed, and (separately) many similar
but different versions of each investigation scenario are needed, establishing these components
and then automating these recurring processes is a necessary and worthwhile milestone.

Investigations for testing SOAR tools arguably must require querying external intelligence feeds
(e.g., we leveraged VirusTotal, CVE, NVD), yet some of the external information required for a
particular scenario may change throughout the testing period. We recommend taking note of this
need as tests are run and accuracy measurements are being gathered.

We allowed vendors 13 days to review materials and prepare for installation, and the research
team provided a week of hands-on support. We suggest planning a longer setup/training period
per tool, as many of our takeaways and results reflect need for more configuration effort.

Much of the data collection was manual (e.g., recording Likert scores and free responses by hand).
Establishing very consistent practices across team members is necessary for both uniformity in
testing and manipulating the raw data into machine-readable data.

Our results also showed that, once established, the test environment can be used for training
analysts and red team members, testing other SOC tools in conjunction with a full tool suite, and
creating simulated SIEM datasets.

We hope this study provides worthwhile lessons for future researchers.

, Vol. 1, No. 1, Article . Publication date: August 2022.

28

Bridges, et al.

ACKNOWLEDGMENT
The authors thank: Mike Karlbom for ongoing support and leadership through the AI ATAC
Challenge series; Jonathan Hodapp for the bureaucracy; Jessica Briesacker for the counseling (legal
and otherwise); Jaimee Janiga and Laurie Varma for assistance with the infographic; Laurie Varma
for editoral review; Mingyan Li for technical review. The research is based upon work supported by
the US Department of Defense (DOD), Naval Information Warfare Systems Command (NAVWAR),
via the US Department of Energy (DOE) under contract DE-AC05-00OR22725. The views and
conclusions contained herein are those of the authors and should not be interpreted as representing
the official policies or endorsements, either expressed or implied, of the DOD, DOE, NAVWAR, or
the US Government. The US Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright annotation thereon.

REFERENCES

[1] C. Islam and M. A. Babar, “A Multi-Vocal Review of Security Orchestration,” ACM Computing Surveys, vol. 52, no. 2,

p. 45.

[2] G. A. Fink, V. Duggirala, R. Correa, and C. North, “Bridging the host-network divide: Survey, taxonomy, and solution.”

in LISA, 2006, pp. 247–262.

[3] J. Goodall, W. Lutters, and A. Komlodi, “The work of intrusion detection: rethinking the role of security analysts,”

AMCIS 2004 Proceedings, p. 179, 2004.

[4] D. Botta, R. Werlinger, A. Gagné, K. Beznosov, L. Iverson, S. Fels, and B. Fisher, “Towards understanding it security
professionals and their tools,” in Proceedings of the 3rd symposium on Usable privacy and security. ACM, 2007, pp.
100–111.

[5] F. B. Kokulu, A. Soneji, T. Bao, Y. Shoshitaishvili, Z. Zhao, A. Doupé, and G.-J. Ahn, “Matched and mismatched SOCs:
A qualitative study on security operations center issues,” in Proceedings of the 2019 ACM SIGSAC Conference on
Computer and Communications Security, 2019, pp. 1955–1970.

[6] R. A. Bridges, M. D. Iannacone, J. R. Goodall, and J. M. Beaver, “How do information security workers use host data? a

summary of interviews with security analysts,” arXiv preprint arXiv:1812.02867, 2018.

[7] Gartner, “Market guide for security orchestration, automation and response solutions.” [Online]. Available:

https://www.gartner.com/en/documents/3942064/market-guide-for-security-orchestration-automation-and-r

[8] ORNL, “AI ATAC 3 challenge description.” [Online]. Available: https://www.challenge.gov/challenge/AI-ATAC-3-

challenge/

[9] S. Norem, A. E. Rice, S. Erwin, R. A. Bridges, S. Oesch, and B. Weber, “A mathematical framework for evaluation of
SOAR tools with limited survey data,” in Computer Security. ESORICS 2021 International Workshops. Springer, 2022.
[10] S. Young, “Automated systems only: why CISOs should switch off their dumb machines,” Network Security, vol. 2019,

no. 9, pp. 6–8, Sep. 2019. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/S1353485819301060

[11] R. Brewer, “Could SOAR save skills-short SOCs?” Computer Fraud & Security, vol. 2019, no. 10, pp. 8–11, Oct. 2019.

[Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/S136137231930106X

[12] C. Tankard, “Pandemic underpins need for SOAR,” Network Security, vol. 2020, no. 5, p. 20, May 2020. [Online].

Available: https://linkinghub.elsevier.com/retrieve/pii/S135348582030057X

[13] F. Fransen, R. Wolthuis, and N. el Ouajdi, Security at machine speed. Groningen: TNO, 2019.
[14] J. Trull, “Top 5 best practices to automate security operations,” 2017. [Online]. Available: https://www.microsoft.com/

security/blog/2017/08/03/top-5-best-practices-to-automate-security-operations/

[15] Rohan, “Security orchestration market with 1682.4 million usd by 2021,” 2018. [Online]. Available: https:

//www.marketsandmarkets.com/PressReleases/security-orchestration.asp

[16] M. Nyre-Yu, “Identifying Expertise Gaps in Cyber Incident Response: Cyber Defender Needs vs. Technological

Development.”

[17] C. Islam, M. Ali Babar, and S. Nepal, Architecture-centric Support for Integrating Security Tools in a Security

Orchestration Platform, Sep. 2020.

[18] E. S. Erdur,

“Continuous

security operation centers,”
improvement on maturity and capability of
Masters Thesis, The Middle East Technical University, p. 61. [Online]. Available: https://open.metu.edu.tr/handle/
11511/45040

[19] G. Kaur and A. H. Lashkari, “An Introduction to Security Operations,” in Advances in Cybersecurity Management,
K. Daimi and C. Peoples, Eds. Cham: Springer International Publishing, 2021, pp. 463–481. [Online]. Available:
https://doi.org/10.1007/978-3-030-71381-2_21

[20] K. Singh, “Application of SIEM/UEBA/SOAR/SOC (Cyber SUSS) Concepts on MSCS 6560 Computer Lab,” p. 82.

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

29

[21] A. Perera, S. Rathnayaka, N. D. Perera, W. Madushanka, and A. N. Senarathne, “The Next Gen Security Operation

Center,” in 2021 6th International Conference for Convergence in Technology (I2CT), Apr. 2021, pp. 1–9.

[22] V. Mavroeidis and J. Brule, “A nonproprietary language for the command and control of cyber defenses—OpenC2,”

Computers & Security, vol. 97, p. 101999, 2020.

[23] T. Z. Sworna, I. Chandi, and A. B. Muhammad, “APIRO: A framework for automated security tools API recommendation,”

submitted to Association for Computing Machinery, 2021.

[24] T. Al-Jody, “Bearicade: A novel high-performance computing user and security management system augmented with

machine learning technology,” p. 183, 2021.

[25] R. F. Gibadullin and V. V. Nikonorov, “Development of the system for automated incidient management based on

open-source software,” International Russian Automation Conference, 2021.

[26] R. Vast, S. Sawant, A. Thorbole, and V. Badgujar, “Artificial Intelligence based Security Orchestration, Automation and

Response System,” in 2021 6th International Conference for Convergence in Technology (I2CT), Apr. 2021, pp. 1–5.

[27] M. Donevski and T. Zia, “A Survey of Anomaly and Automation from a Cybersecurity Perspective,” in 2018 IEEE

Globecom Workshops (GC Wkshps), Dec. 2018, pp. 1–6.

[28] S. D. Dutta and R. Prasad, “Cybersecurity for Microgrid,” in 2020 23rd International Symposium on Wireless Personal

Multimedia Communications (WPMC), Oct. 2020, pp. 1–5, iSSN: 1882-5621.

[29] “Security Orchestration and Incident Response - Schneier on Security.” [Online]. Available: https://www.schneier.com/

blog/archives/2017/03/security_orches.html

[30] R. Werlinger, K. Hawkey, and K. Beznosov, “An integrated view of human, organizational, and technological challenges

of it security management,” Information Management & Computer Security, vol. 17, no. 1, pp. 4–19, 2009.

[31] R. Werlinger, K. Hawkey, D. Botta, and K. Beznosov, “Security practitioners in context: Their activities and interactions
with other stakeholders within organizations,” International Journal of Human-Computer Studies, vol. 67, no. 7, pp.
584–606, 2009.

[32] R. Werlinger, K. Muldner, K. Hawkey, and K. Beznosov, “Preparation, detection, and analysis: the diagnostic work of it

security incident response,” Information Management & Computer Security, vol. 18, no. 1, pp. 26–42, 2010.

[33] S. C. Sundaramurthy, J. McHugh, X. Ou, M. Wesch, A. G. Bardas, and S. R. Rajagopalan, “Turning contradictions into
innovations or: How we learned to stop whining and improve security operations,” in Proc. 12th Symp. Usable Privacy
and Security, 2016.

[34] M. Nyre-Yu, “Identifying expertise gaps in cyber incident response: Cyber defender needs vs. technological development,”

in Proceedings of the 54th Hawaii International Conference on System Sciences, 2021, p. 1978.

[35] S. Oesch, R. A. Bridges, M. Verma, B. Weber, and O. Diallo, “D2U: Data driven user emulation for the enhancement
of cyber testing, training, and data set generation,” in Cyber Security Experimentation and Test Workshop, 2021, pp.
17–26.

[36] G. Baxter, J. Rooksby, Y. Wang, and A. Khajeh-Hosseini, “The ironies of automation: still going strong at 30?” in
Proceedings of the 30th European Conference on Cognitive Ergonomics - ECCE ’12. Edinburgh, United Kingdom:
ACM Press, 2012, p. 65. [Online]. Available: http://dl.acm.org/citation.cfm?doid=2448136.2448149

[37] F. Barbieri, J. Camacho-Collados, L. E. Anke, and L. Neves, “Tweeteval: Unified benchmark and comparative evaluation
for tweet classification,” in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:
Findings, 2020, pp. 1644–1650.

[38] J. L. Schafer, “Multiple imputation: a primer,” Statistical methods in medical research, vol. 8, no. 1, pp. 3–15, 1999.
[39] P. C. Austin, I. R. White, D. S. Lee, and S. van Buuren, “Missing data in clinical research: a tutorial on multiple

imputation,” Canadian Journal of Cardiology, vol. 37, no. 9, pp. 1322–1331, 2021.

[40] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg et al., “Scikit-learn: Machine learning in python,” Journal of machine learning research, vol. 12, no. Oct,
pp. 2825–2830, 2011.

[41] A. G. Asuero, A. Sayago, and A. Gonzalez, “The correlation coefficient: An overview,” Critical reviews in analytical

chemistry, vol. 36, no. 1, pp. 41–59, 2006.

, Vol. 1, No. 1, Article . Publication date: August 2022.

30

Bridges, et al.

A APPENDIX: SOAR TOOL DOWN-SELECTION OVERVIEW (AI ATAC 3 PHASE 1)
This section of the appendix provides an overview of the main components of the Phase 1 experi-
mental design. It describes the pipeline we have developed to carefully select the top contenders
qualifying for the hands-on evaluation (i.e., Phase 2) of the challenge. Norem et al. [9] focuses
exclusively on the Phase 1 study, providing full details of our Phase 1 methodology and findings.
Phase 1 began with the analysis of two videos, one “overview” and one “demonstration,” submitted
by each of 11 SOAR tool vendors. These videos were viewed by US Navy SOC personnel and were
meant to provide the necessary context of each tool and demonstrate its functionality. After
watching the videos, operators reviewed each tool according to a questionnaire. Prior to evaluating
the tools based on the videos, operators were asked to rank six features of a SOAR tool in order
of importance: ticketing, collaboration, automation, data ingestion, playbooks, and alert ranking.
Tools were evaluated with a Likert-scale survey based on their capabilities in these areas. Operators
were also asked to provide an overall score, also using a Likert scale, and a ranking of the tools
that they reviewed. Free response answers on the questionnaire were converted to Likert values
with sentiment analysis. Because of the constraints on operator time, it was not feasible for every
operator to review all 11 tools. Therefore, we requested that operators review a minimum of eight
tools.

Due to the sensitivity of statistical analyses on population size, we first identified the number of
operators needed to ensure statistical validity of the results. To do this, we designed and implemented
a simple set of simulations consisting of just two tools and assuming only one rating per tool per
operator. We performed a sensitivity analysis to quantify how accurately we could use a hypothesis
test with p-value 5% defining statistical significance and to determine when one tool is preferred
over another and when both tools are preferred equally, as the number of ratings varies.

Multiple hypothesis tests (t-, z-, 𝜒 2−, and Binomial) were used in a Monte Carlo simulation, so
we could first quantify which tests provide the lowest Type I and Type II errors, simultaneously.
The two-sided t-test was determined to be most accurate. We plotted the number of reviews
per pair of tools against the likelihood of correctly concluding user preferences in Figure 12.
Secondly, we used these results
to answer questions about the
number of reviewers needed, as
follows. Our study examined [11
choose 2] = 55 pairs of tools, and
each reviewer rated 8 tools, pro-
viding 28 = [8 choose 2] pair-
wise tool reviews. Hence, we re-
ceived 𝑛 = 𝑘 × 28/55 or approxi-
mately 𝑛 = 𝑘/2 reviews per pair
of tools.

Fig. 12. Results of simulation to quantify our ability to accurately deter-
mine tool preference using a two-sided 𝑡-test as the number of reviews
changes.
only 18 total participants including one non–US Navy SOC operator participated in Phase 1.

Using Figure 12, and follow-
ing discussions with the sponsor,
we targeted at least 𝑛 = 25 re-
views per pair of tool, requiring
𝑘 = 50 or more reviewers. De-
spite a widespread effort by our
sponsor to recruit participants,

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

31

As mentioned above, operators reviewed only a subset of the submitted tools, which presents a
challenge regarding assigning tools to ensure evenly distributed reviews. When a single operator
rated two tools, we obtained a “direct comparison”; hence, we sought to implement an assignment
method that maximized the number of direct comparisons of all [11 choose 2] = 55 tool pairs. To
do this, a greedy optimization procedure was implemented to approximate the optimal solution, or
the case in which the direct comparisons are uniformly evaluated across the set of operators. The
algorithm we used for this research was equipped with an objective function 𝑓 that analytically
measures how far the current set of assignments is from the optimal set, and at each step it adds
an assignment that minimizes 𝑓 . This approach was sufficiently fast as well as flexible in terms of
assigning tools to any number of operators. The assignment set generated was very close to optimal;
specifically, each pair of tools received within one review of an optimal assignments number of
reviews per pair. In practice, we ran the algorithm 100 times with more assignments than needed
and selected the most optimal of all 100 sets.

Because operators did not review all 11 tools, the collected data contained missing values in
some areas where no survey responses were obtained from any operator for a specific tool. A
collaborative filtering algorithm was implemented and customized to predict unknown aspect
ratings. From the complete set of aspect ratings, the unknown overall ratings were predicted with
the best-performing supervised machine learning algorithm from the several tested. To account for
all the collected data and ensure that our methods for filling in missing data were not interfering
with the results, four methods for producing an overall ranking were used: (1) per-user raw overall
ratings; (2) per-user raw rankings represented as a directed graph, with a diffusion algorithm
used to determine the overall tool rankings; (3) per-user raw and predicted overall ratings; and (4)
per-user predicted rankings (which are complete, as in ranking all tools) were used with the graph
diffusion process.

Notably, there was a clear dichotomy in the ranking from the four methods, providing confidence
in the tools promoted to Phase 2 and those eliminated. Tools that were consistently low-scoring
were eliminated from Phase 2. Statistical analysis showed that results were not correlated with
operators’ reported years of experience nor occupation but were correlated with familiarity with
SOAR tools in general and with the quality of SOAR tools’ videos.

B APPENDIX: DEMOGRAPHIC SURVEY
Q1. How familiar are you with SOAR tools?
◦ Never used ◦ Somewhat familiar ◦ Expert user

Q2. Which of these best fits your role?
◦ Security operator ◦ Network operator ◦ Systems administrator ◦ Other

Q3. How many years have you been in that role?

Q4. How many different SOCs have you worked in?

C EASE OF INSTALLATION SURVEY
Each two-person research team assisting a SOAR vendor during their installation in our environment
completed the following survey. They also noted any observations they felt would be relevant
during analysis.

, Vol. 1, No. 1, Article . Publication date: August 2022.

32

Bridges, et al.

Q1. Was the tool able to ingest all new data sources? Y / N. If no, provide details.

Q2. Approximately how long did it take to configure the tool to ingest the new data source?

Q3. Please rate the following statements on a scale of 1 to 5, where 1 is ”very dissatisfied” and 5 is
”very Satisfied”:
◦ Overall, I am satisfied with the ease of ingesting new data sources with this tool. ◦ Overall, I am
satisfied with the amount of time it took to ingest new data sources with this tool. ◦ Overall, I am
satisfied with the support information (on-line help, messages, documentation) when ingesting new
data sources with this tool. ◦ Overall, I am satisfied with the support provided by the vendor when
ingesting new data sources with this tool.

D USABILITY AND RANKING SHEET
Analysts completed the following for each tool they tested and then ranked them by order of
preference.

Please rate the following statements on a scale of 1 to 5, where 1 is ”not at all” and 5 is ”very

much so”:
Section 1. Training
◦ I felt prepared for testing after the training period. ◦ The training videos covered what I needed to
know to use the tool.

Section 2. 1,000x per day
◦ It was easy to triage common events with this tool. ◦ This tool helped me find the information I
needed to triage an event. ◦ This tool helped me understand the importance and meaning of alerts. ◦ It
was easy to create, update, and close tickets with this tool.

Section 3. Collaboration
◦ It was easy to collaborate with another analyst with this tool. ◦ This tool helped me share my findings
with my collaborator. ◦ This tool made it easy to communicate nonverbally (via chat, for example)
with my collaborator. ◦ It was easy collaborate on tickets with this tool.

Section 4. Advanced persistent threats
◦ It was easy to triage an APT with this tool. ◦ I was easy to initially create and populate a ticket
with this tool. ◦ It was easy to update/pass off a ticket with this tool. ◦ This tool would significantly
decrease the amount of time I spend triaging an APT. ◦ This tool helped me find the information I
needed to trace an attacker’s entry, movement within, and objectives in my network. ◦ This tool helped
me quickly understand the importance and meaning of alerts. ◦ This tool helped me prioritize alerts
and events so that I could focus on the most important threats.

Section 5. System usefulness
◦ Overall, I am satisfied with how easy it is to use this tool. ◦ It was simple to use this tool. ◦ I was able
to complete the tasks and scenarios quickly using this tool. ◦ I felt comfortable using this tool. ◦ It was
easy to learn to use this tool.

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

33

Section 6. Information quality
◦ The tool gave error messages that clearly told me how to fix problems. (“n/a” if did not experience
error messages) ◦ Whenever I made a mistake using the tool, I could recover easily and quickly. ◦ The
information (such as online help, onscreen messages, and other documentation) provided with this tool
was clear. ◦ It was easy to find the information I needed using this tool. ◦ The information available in
the tool was effective in helping me complete the tasks and scenarios. ◦ The organization of information
in the tool windows was clear.

Section 7. Interface quality
◦ The interface of this tool was pleasant. ◦ I liked using the interface of this tool. ◦ This tool has all the
functions and capabilities I expect it to have. ◦ Overall, I am satisfied with this tool.

Section 8. Ranking
◦ Rank tools in order (1 = best) based on which tool you want to see in your SOC.

E SEMISTRUCTURED INTERVIEW GUIDE
We used the following questions for a semistructured interview with each analyst for each tool
they tested and then coded their answers using open coding, as well as conducting a sentiment
analysis on the results.
Section 1. 1,000x per day
◦ What could be improved about this tool to help you triage common events? ◦ What do you feel were
the greatest strengths of this tool? ◦ What do you feel were the greatest weaknesses of this tool? ◦ Do
you think that this tool would improve your SOC? How/in what ways?

Section 2. Collaboration
◦ Which parts of this tool helped you collaborate with other analysts the most? ◦ What could be
improved about this tool to help you collaborate with other analysts? ◦ How did collaborating with
this tool differ from the way that you normally collaborate in your SOC?

Section 3. Attack scenarios
◦ What do you feel was the greatest benefit this tool provided over your existing tool suite when triaging
an APT? ◦ What could be improved about this tool to help you triage APTs? ◦ What could be improved
about this tool to help with ticket management (create, update, pass off)? ◦ Did this tool help you
connect the dots to understand the actions and goals of an attacker?

Section 4. Usability and wrap-up
◦ Did the SOAR tool prioritize information in the way it was displayed so that it was useful? ◦ Did
the SOAR tool display information in a way that separates noise (too much data) from signal (needed
information)? If so, does it throw away or occlude/exclude too much? ◦ Did the SOAR tool group data
so that multiple related events could easily be understood? ◦ Does the tool promote standardization
of processes (e.g., via playbooks)? ◦ Would you want this tool in your SOC? Why/why not? ◦ Is there
anything else about this tool you think would be helpful for us to know?

For data analysis, the free response questions listed below were reorganized thematic subcate-

gories with large overlap, but slight differences compared with the sections above.

, Vol. 1, No. 1, Article . Publication date: August 2022.

34

Bridges, et al.

Subcategory 1. Recurring tasks
◦ What could be improved about this tool to help you triage common events? ◦ What could be improved
about this tool to help with ticket management (create, update, pass off)? ◦ Does the tool promote
standardization of processes (e.g., via playbooks)?

Subcategory 2. Collaboration
◦ Which parts of this tool helped you collaborate with other analysts the most? ◦ What could be
improved about this tool to help you collaborate with other analysts? ◦ How did collaborating with
this tool differ from the way that you normally collaborate in your SOC?

Subcategory 3. Attack scenarios
◦ What do you feel was the greatest benefit this tool provided over your existing tool suite when triaging
an APT? ◦ what could be improved about this tool to help you triage APTs? ◦ Did this tool help you
connect the dots to understand the actions and goals of an attacker?

Subcategory 4. Information quality
◦ Did the SOAR tool prioritize information in the way it was displayed so that it was useful? ◦ Did
the SOAR tool display information in a way that separates noise (too much data) from signal (needed
information)? If so, does it throw away or occlude/exclude too much? ◦ Did the SOAR tool group data
so that multiple related events could easily be understood?

Subcategory 5. General
◦ (Prompt to participant to discuss anything they wish about the tool between Likert response questions
and free response questions.) ◦ What do you feel were the greatest strengths of this tool? ◦ What do
you feel were the greatest weaknesses of this tool? ◦ Do you think that this tool would improve your
SOC? How/in what ways? ◦ Would you want this tool in your SOC? Why/why not? ◦ Is there anything
else about this tool you think would be helpful for us to know? ◦ What tools are missing from this tool
suite that you would normally use? ◦ How does the process used in this exercise differ from the way
that you normally triage common events?

F APPENDIX: VENDOR INSTALLATION AND CONFIGURATION
Each vendor was assigned a 13-business-day period for familiarization, configuration, installation,
and training development. The vendors were hands-on in the environment for only the final
7 days of this period. At the beginning of the 13-day period, vendors received an information
packet complete with descriptions of scenario their tool would need to perform during evaluation.
The packet also included information about the platforms in our test environment with which
their tool would need to integrate. The vendors appointed an installation team on their end who
worked together for the first week on preparation of their tool. The following week, the vendor’s
installation team met with our assigned research team, who assisted them with configuring their
tool in our environment and performing final checks. At the end of the installation week, the
vendor submitted training videos demonstrating each of the scenarios with their tools that would
be used to familiarize the operators prior to performing the evaluation. The training scenarios were
similar, but not identical, to the evaluation scenarios.

This design served two purposes: (1) standardizing and limiting the length of time a vendor had
to accomplish everything, thus informing us about ease of configuration; and (2) enabling us to
focus our limited resources on each vendor in linear fashion, so they could each be satisfied they
had received full attention and had been able to present their tools to the analysts as thoroughly as

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

35

possible. Because the vendors did their own configuration, installation, and training, they were
entirely responsible for the quality of the presentation of their tool. We included questions in the
user surveys about the presentation and training quality. The research team who assisted each
vendor during their installation week completed a survey (Section C of the appendix) on the ease
of installation and integration in our environment. This information was used to evaluate the ease
of installation for the tool.

G MULTIPLE IMPUTATION FOR CORRELATION WITH MISSING DATA
We employed multiple imputation (MI) of missing data when computing correlations of two vectors.
We let (cid:174)𝑥, (cid:174)𝑦 be two vectors of length 𝑁 with missing data. Note that direct computation of correlation
of (cid:174)𝑥, (cid:174)𝑦, denoted corr( (cid:174)𝑥, (cid:174)𝑦) by ignoring missing values, would have overweighted the contributions
of the data. MI seeks to identify the expected correlation given the observed data through a Monte
Carlo simulation. Our treatment followed methods described in Schafer [38] and Austin et al. [39],
involving sampling each missing data value 𝑚 times and computing imputed correlation values
𝑖=1, where (cid:174)𝑥𝑖, (cid:174)𝑦𝑖 denoted the now-filled vectors (cid:174)𝑥, (cid:174)𝑦 with the 𝑖-th simulated missing
{corr( (cid:174)𝑥𝑖, (cid:174)𝑦𝑖 )}𝑚
values. The resulting estimate was simply the sample mean, (1/𝑚) (cid:205)𝑖 corr( (cid:174)𝑥𝑖, (cid:174)𝑦𝑖 ), and this Monte
Carlo simulation allowed us to investigate statistical power of the estimates through hypothesis
tests and confidence intervals. We chose 𝑚 = 1, 000 by observing the average ℓ1 change of the
resulting correlation matrix from the first 𝑖 samples to the 𝑖 + 1.

G.1 Which Density Function? Uniform Random or Kernel Density from Peers
In our case, all data was normalized to [0, 1]. Generating samples for the missing components
of (cid:174)𝑥, (cid:174)𝑦 required a distribution on [0, 1]. We considered two approaches. In the first approach, we
sampled all missing data uniformly at random (probability density function 𝑝 (𝑥) = 1 for 𝑥 ∈ [0, 1]).
As it may seem unbelievable that all possible values for a
missing data point are equally likely, we considered a second
approach in which a distribution for each missing cell in our
data table is learned from its peers’ values. We first joined our
user demographics and tool configuration data to the subcate-
gory averages table so each row (comprising the subcategory
average measurements from a user testing a tool) includes in-
formation for a specific user and a specific tool, which may be
pertinent to predicting missing values. For each subcategory
(i.e., column in the original table), we trained a decision tree
regressor to predict that target subcategory from the values
in the other columns. Each leaf node of the fit decision tree
was then considered a “peer” group as the rows of the data
lying in that leaf are the subset of the table that has been
learned to best predict the value of the target column for those
with similar other columns. Armed with a decision tree’s fit
to predict a target column, we created a kernel density esti-
mate (KDE) from the target column values in each leaf; that is,
probability density function 𝑝 (𝑥) = (cid:205)𝑥𝑖 ∈leaf(𝑥) 𝜙 (𝑥)/|leaf(𝑥)|
with 𝜙 (𝑥) the “tophat” (square wave) kernel with bandwidth
𝑏 = min(.05, dist(leaf(𝑥), {0, 1})) and shifted to keep positive
values inside [0, 1]. We leveraged Scikit’s [40] Decision Tree and passed the constraint that each
leaf node (i.e., peer group) must have at least five training data points. Figure 7 depicts three such
KDEs.

Fig. 13. Log plot of average ℓ1 change
in the imputed estimate (average cor-
relation matrix) from the first 𝑖 im-
puted samples vs. 𝑖+1-st samples using
the KDE for sampling. Similar conver-
gence is observed for uniform random
sampling.

, Vol. 1, No. 1, Article . Publication date: August 2022.

36

Bridges, et al.

Fig. 14. Heatmap and hierarchical clustering of subcategory average’s correlations computed (left) with
missing data values and (right) missing values imputed uniformly at random. Compare with Figure 6, which
provides the same plot using kernel densities for more precise sampling. Viewing side-by-side shows the
dampening effect imputation has to counteract overweighting caused by missing data. Importantly, while
the groupings are slightly different, the main trend holds: the two primary clusters (Likert and Sentiment
vs. all other subcategories) are consistent, with positive within-cluster correlation but slightly negative
between-cluster correlations on average.

Fig. 15. Plots show difference of imputed correlation from correlation with missing values (delta) vs. correlation
with missing values. Linear models are displayed for different numbers of missing components. Plot supports
the hypothesis uniform random sampling scales correlations by 𝑘/𝑁 with 𝑘 the missing number of components
and 𝑁 the length of the vector, while KDE imputation scales by less.

(More) Subcategory Averages’ Correlation Results. In our application to subcategory average
G.1.1
correlations, we included the naive (i.e., no imputation, simply ignore missing values) correlation
heatmap and clustering dendrogram alongside the uniform random sampling imputation in Figure
14, and both can be compared with the KDE-imputation version discussed in the paper (Figure 6).

, Vol. 1, No. 1, Article . Publication date: August 2022.

Testing SOAR Tools in Use

37

G.1.2 How MI Dampens Correlations. We found that the uniform random imputations roughly
mapped corr( (cid:174)𝑥, (cid:174)𝑦) ↦→ (−𝑘/𝑁 )corr( (cid:174)𝑥, (cid:174)𝑦), where 𝑘 is the number of missing components in 𝑥 or 𝑦,
and 𝑁 = 68 is the length of our vectors while, using the KDE sampling, the scale factor was roughly
half as much. See Figure 15.

G.1.3 Confidence Intervals. To compute confidence intervals with the imputed statistics, we
converted the correlations via Fisher’s transform (𝑓 (𝑟 ) := (1/2) log (1 + 𝑟 )/(1 − 𝑟 ) to an ap-
proximately normal random variable with known variance 𝑈 := 1/(𝑁 − 3), where 𝑁 = 68
:= 𝑓 (corr( (cid:174)𝑥𝑖, (cid:174)𝑦𝑖 ), ¯𝑄 = (1/𝑚) (cid:205)𝑖 𝑄𝑖 (our sample
is the length of our vectors [41]. We let 𝑄𝑖
mean), 𝑈 𝑖 = 𝑈 = 1/(𝑁 − 3) = 1/65 the within-sample variance (i.e., the variance of 𝑄𝑖 ), and
𝐵 = (cid:205)(𝑄𝑖 − ¯𝑄)2/(𝑚 − 1) our between-sample variance (i.e., the sample variance of observations
caused by imputation). Our total variance, following Schafer [38], was 𝑈 + 𝐵 + 𝐵/𝑚, although
the 𝐵/𝑚 term was negligible for our data with 𝑚 = 1, 000. Further, the literature sets up a t-test
for our statistic, although the degrees of freedom were so large in our case (>1,000) that the t-
distribution became a normal distribution (again because 𝑚 = 1, 000). Hence, we simply computed
our confidence interval as ¯𝑄 ± 1.96
𝑈 + 𝐵 (1.96 corresponding to two-sided 95% confidence from a
standard normal distribution) and pulled these intervals back into [0, 1] via the 𝑓 −1. See Figure 8
for confidence intervals from the KDE imputations. Note that when using the uniform distribution
to sample, the confidence intervals were much larger, and nearly all crossed 0.

√

, Vol. 1, No. 1, Article . Publication date: August 2022.

