2
1
0
2

c
e
D
2

]

C
O
.
h
t
a
m

[

1
v
6
2
2
0
.
2
1
2
1
:
v
i
X
r
a

A Secure Control Framework for Resource-Limited
Adversaries

André Teixeira⋆, Iman Shames†, Henrik Sandberg⋆, Karl H. Johansson⋆

⋆ACCESS Linnaeus Centre, KTH Royal Institute of Technology, Electrical Engineering, Stockholm, Sweden

†Department of Electrical and Electronic Engineering, University of Melbourne, Australia

Abstract

Cyber-secure networked control is modeled, analyzed, and experimentally illustrated in this paper. An attack space deﬁned
by the adversary’s system knowledge, disclosure, and disruption resources is introduced. Adversaries constrained by these
resources are modeled for a networked control system architecture. It is shown that attack scenarios corresponding to denial-
of-service, replay, zero-dynamics, and bias injection attacks can be analyzed using this framework. Furthermore, the attack
policy for each scenario is described and the attack’s impact is characterized using the concept of safe sets. An experimental
setup based on a quadruple-tank process controlled over a wireless network is used to illustrate the attack scenarios, their
consequences, and potential counter-measures.

Key words: Cyber-physical systems, security, attack space, secure control systems.

1 Introduction

Safe and reliable operation of infrastructures is of major societal importance. These systems need to be engineered
in such a way so that they can be continuously monitored, coordinated, and controlled despite a variety of poten-
tial system disturbances. Given the strict operating requirements and system complexity, such systems are operated
through IT infrastructures enabling the timely data ﬂow between digital controllers, sensors, and actuators. However,
the use of communication networks and heterogeneous IT components has made these cyber-physical systems vul-
nerable to cyber threats. One such example are the industrial systems and critical infrastructures operated through
Supervisory Control and Data Acquisition (SCADA) systems. The measurement and control data in these systems
are commonly transmitted through unprotected communication channels, leaving the system vulnerable to several
threats [10]. As illustrative examples, we mention the cyber attacks on power transmission networks operated by
SCADA systems reported in the public media [11], and the Stuxnet malware that supposedly infected an industrial
control system and disrupted its operation [9, 21].

There exists a vast literature on computer security focusing on three main properties of data and IT services, namely
conﬁdentiality, integrity, and availability [3]. Conﬁdentiality relates to the non-disclosure of data by unauthorized
parties. Integrity on the other hand concerns the trustworthiness of data, meaning there is no unauthorized change
of the data contents or properties, while availability means that timely access to the data or system functionalities
is ensured. Unlike other IT systems where cyber-security mainly involves the protection of data, cyber attacks on
networked control systems may inﬂuence physical processes through feedback actuation. Therefore networked control
system security needs to consider threats at both the cyber and physical layers. Furthermore, it is of the utmost

⋆

This paper was not presented at any IFAC meeting. Corresponding author André Teixeira. Tel. +46-73-429 78 31. Fax

+46-8-790 73 29.

⋆
Email addresses: andretei@kth.se (André Teixeira

), iman.shames@unimelb.edu.au (Iman Shames†), hsan@kth.se

⋆
(Henrik Sandberg

), kallej@kth.se (Karl H. Johansson

).

⋆

Preprint submitted to Automatica

December 4, 2012

 
 
 
 
 
 
importance in the study of cyber attacks on control systems to capture the adversary’s resources and knowledge.
Cyber threats can be captured in the attack space illustrated in Figure 1, which depicts several attack scenarios as
points. For instance, the eavesdropping attack and the denial-of-service (DoS) attack are indicated in the ﬁgure.

System knowledge

s

a m i c

n

y

k

d
c

o

t

a

r
t

Z

e

a

n

o

i

t

c

e

j

n
k

i

c

k

c

a

t

t

a

s

t

a

t

a

B i
a

S

D o

Covert attack

Eavesdropping
attack

Disclosure resources

Replay attack

Disruption resources

Figure 1. The cyber-physical attack space.

We propose three dimensions for the attack space: the adversary’s a priori system model knowledge and his disclosure
and disruption resources. The a priori system knowledge can be used by the adversary to construct more complex
attacks, possibly harder to detect and with more severe consequences. Similarly, the disclosure resources enable the
adversary to obtain sensitive information about the system during the attack by violating data conﬁdentiality. Note
that disclosure resources alone cannot disrupt the system operation. An example of an attack using only disclosure
resources is the eavesdropping attack illustrated in Figure 1. On the other hand, disruption resources can be used to
aﬀect the system operation, which happens for instance when data integrity or availability properties are violated.
One such example is the DoS attack, where the data required for correctly operating the system are made unavailable.
In particular this characterization ﬁts the Stuxnet malware, which had resources to record and manipulate data in
the SCADA network [9]. Moreover, the complexity and operation of Stuxnet also indicate that its developers had
access to a reasonable amount of knowledge of both physical and cyber components of the target control system.

1.1 Related Work

Control theory has contributed with frameworks to handle model uncertainties and disturbances as well as fault
diagnosis and mitigation, see, for example, [32] and [7, 14], respectively. These tools can be used to detect and
attenuate the consequences of cyber attacks on networked control systems, as has recently been done in the literature.

Cyber attacks on control systems compromising measurement and actuator data integrity and availability have been
considered in [6], where the authors modeled the attack eﬀects on the physical dynamics. Several attack scenarios
have been simulated and evaluated on the Tennessee-Eastman process control system [5] to study the attack impact
and detectability. The attack scenarios in [5] are related to the ones considered in this paper, but we quantify the
attack resources and policies in a systematic way.

Availability attacks have been analyzed in [1, 12] for resource constrained adversaries with full-state information.
Particularly, the authors considered DoS attacks in which the adversary could tamper with the communication
channels and prevent measurement and actuator data from reaching their destination, rendering the data unavailable.
A particular instance of the DoS attack in which the adversary does not have any a priori system knowledge, as the
attack in [1], is represented in the attack space in Figure 1.

Deception attacks compromising integrity have recently received attention. Replay attacks on the sensor measure-
ments, which is a particular kind of deception attack, have been analyzed in [19]. The authors considered the case
where all the existing sensors were attacked and suitable counter-measures to detect the attack were proposed. In
this attack scenario the adversary does not have any system knowledge but is able to access and corrupt the sensor
data, thus having disclosure and disruptive resources, as depicted in Figure 1.

Another class of deception attacks, false-data injection attacks, has been studied in recent work. For instance, in the
case of power networks, an adversary with perfect model knowledge has been considered in [18]. The work in [17]

2

considered stealthy attacks with limited resources and proposed improved detection methods, while [22] analyzed
the minimum number of sensors required for stealthy attacks. A corresponding measurement security metric for
studying sets of vulnerable sensors was proposed in [22]. The consequences of these attacks have also been analyzed
in [26,28,30]. In particular, in [26] the authors analyzed attack policies with limited model knowledge and performed
experiments on a power system control software, showing that such attacks are stealthy and can induce the erroneous
belief that the system is at an unsafe state. The models used in the previous work are static, hence these attack
scenarios are closest to the bias injection attack shown in Figure 1.

Data injection attacks on dynamic control systems were also considered. In [25] the author characterizes the set of
attack policies for covert (undetectable) false-data injection attacks with detailed model knowledge and full access
to all sensor and actuator channels, while [20] described the set of undetectable false-data injection attacks for
omniscient adversaries with full-state information, but possibly compromising only a subset of the existing sensors
and actuators. In the context of multi-agent systems, optimal adversary policies for data injection using full model
knowledge and state information were derived in [16]. In these attack scenarios conﬁdentiality was violated, as the
adversary had access to either measurement and actuator data or full-state information. These attacks are therefore
placed close to the covert attack in Figure 1.

Most of the recent work on cyber-security of control systems has considered scenarios where the adversary has access
to a large set of resources and knowledge, thus being placed far from the origin of the attack space in Figure 1.
A large part of the attack space has not been addressed. In particular, the class of detectable attacks that do not
trigger conventional alarms has yet to be covered in depth.

1.2 Contributions and Outline

In this paper we consider a typical networked control architecture under both cyber and physical attacks. A generic
adversary model applicable to several attack scenarios is discussed and the attack resources are mapped to the
corresponding dimensions of the attack space. To illustrate the proposed framework, we consider several attack
scenarios where the adversary’s goal is to drive the system to an unsafe state while remaining stealthy. For each
scenario we formulate the corresponding stealthy attack policy, comment on the attack’s performance, and describe
the adversary’s capabilities along each dimension of the attack space in Figure 1, namely the disclosure resources,
disruption resources, and system knowledge. Some of the attack scenarios analyzed in the paper have been staged
on a wireless quadruple tank testbed for security of control systems. The testbed architecture and results from the
staged attacks are presented and discussed.

One of the attack scenarios analyzed corresponds to a novel type of detectable attack, the bias injection attack.
Although this attack may be detected, it can drive the system to an unsafe region and it only requires limited model
knowledge and no information about the system state. Stealthiness conditions for this attack are provided, as well
as a methodology to assess the attack impact on the physical state of the system.

The material in this paper is an extension of the authors’ preliminary work, see [27]. Particularly, in the current work
the attack goals are formalized using the notion of safe regions of the state space and two additional attack scenarios
are described and analyzed. Furthermore, the attack performance of each scenario is analyzed in more detail and
additional results for the zero-dynamics and bias injection attacks are presented.

The outline of the paper is as follows. The system architecture and model are described in Section 2, while Section 3
contains the adversary model and a detailed description of the attack resources on each dimension of the attack
space. The framework introduced in the previous sections is then illustrated for ﬁve particular attack scenarios in
Section 4, supposing that the adversary aims at driving the system to an unsafe state while remaining stealthy. The
attack policy, attack performance, and required system knowledge, disclosure, and disruption resources are described
in detail for each attack scenario. The results of the experiments for four of the attack scenarios in a secure control
systems testbed are presented and discussed in Section 5, followed by conclusions in Section 6.

2 Networked Control System

In this section we describe the networked control system structure, where we consider three main components: the
physical plant and communication network, the feedback controller, and the anomaly detector.

3

2.1 Physical Plant and Communication Network

The physical plant is modeled in a discrete-time state-space form

xk+1 = Axk + B ˜uk + Gwk + F fk

:

P

yk = Cxk + vk

,

(1)

(cid:26)
Rq the control actions applied to the process, yk ∈
Rn is the state variable, ˜uk ∈

Rp the measurements
where xk ∈
Rd is the unknown signal representing the eﬀects of
from the sensors at the sampling instant k
anomalies, usually denoted as fault signal in the fault diagnosis literature [8]. The process and measurement noise,
Rp, represent the discrepancies between the model and the real process, due to unmodeled
wk ∈
dynamics or disturbances, for instance, and we assume their means are respectively bounded by δw and δv, i.e.
¯w =

Z, and fk ∈

δw and ¯v =

δv.

∈

Rn and vk ∈
E
wk}k ≤
{

k

E
{

k

vk}k ≤

The physical plant operation is supported by a communication network through which the sensor measurements and
actuator data are transmitted, which at the plant side correspond to yk and ˜uk, respectively. At the controller side
Rq, respectively. Since the communication network may
we denote the sensor and actuator data by ˜yk ∈
be unreliable, the data exchanged between the plant and the controller may be altered, resulting in discrepancies in
the data at the plant and controller ends. In this paper we do not consider the usual communication network eﬀects
such as packet losses and delays. Instead we focus on data corruption due to malicious cyber attacks, as described
in Section 3. Therefore the communication network per se is supposed to be reliable, not aﬀecting the data ﬂowing
through it.

Rp and uk ∈

Given the physical plant model (1) and assuming an ideal communication network, the networked control system is
said to have a nominal behavior if fk = 0, ˜uk = uk, and ˜yk = yk. The absence of either one of these condition results
in an abnormal behavior of the system.

2.2 Feedback Controller

In order to comply with performance requirements in the presence of the unknown process and measurement noises,
we consider that the physical plant is controlled by an appropriate linear time-invariant feedback controller [32]. The
output feedback controller can be written in a state-space form as

zk+1 = Aczk + Bc ˜yk
uk = Cczk + Dc ˜yk

:

F

(cid:26)

(2)

Rm, may include the process state and tracking error estimates. Given the
where the states of the controller, zk ∈
plant and communication network models, the controller is supposed to be designed so that acceptable performance
is achieved under nominal behavior.

2.3 Anomaly Detector

In this section we consider the anomaly detector that monitors the system to detect possible anomalies, i.e. deviations
from the nominal behavior. The anomaly detector is supposed to be collocated with the controller, therefore it only
has access to ˜yk and uk to evaluate the behavior of the plant.

Several approaches to detecting malfunctions in control systems are available in the fault diagnosis literature [8, 14].
Here we consider the following observer-based Fault Detection Filter

:

D

ˆxk|k = Aˆxk−1|k−1 + Buk−1 + K(˜yk −
rk = V (˜yk −
ˆyk|k)
Rp are the state and output estimates given measurements up until time k,

ˆyk|k−1)

(3)

,

(cid:26)
Rn and ˆyk|k = C ˆxk|k ∈

where ˆxk|k ∈
respectively, and rk ∈
The anomaly detector is designed by choosing K and V such that

Rpd the residue evaluated to detect and locate existing anomalies.

4

System Knowledge

=

ˆ
P

, ˆ
F

, ˆ
D}

{

K

Disruption
Resources

B

Ik)
Attack Policy
Figure 2. Adversary model for a point in the attack space in Figure 1.

ak = g(

K

,

Disclosure
Resources
uk
yk

Υu
Υy

(1) under nominal behavior of the system (i.e., fk = 0, uk = ˜uk, yk = ˜yk), the expected value of the residue

converges asymptotically to a neighborhood of zero, i.e., limk→∞ k

(2) the residue is sensitive to the anomalies (fk 6≡
An alarm is triggered if the residue meets

0).

E
{

rk}k ≤

δr, with δr ∈

R+;

where δα ∈

k
R+ is chosen so that the false alarm rate does not exceed a given threshold α

rkk ≥

δr + δα,

(4)

[0, 1].

∈

3 Adversary Models

The adversary model considered in this paper is illustrated in Figure 2 and is composed of an attack policy and
the adversary resources i.e., the system model knowledge, the disclosure resources, and the disruption resources.
, ˆ
Each of the adversary resources can be mapped to a speciﬁc axis of the attack space in Figure 1:
D}
is the a priori system knowledge possessed by the adversary;
Ik corresponds to the set of sensor and actuator data
available to the adversary at time k as illustrated in (8), thus being mapped to the disclosure resources; ak is the
attack vector at time k that may aﬀect the system behavior using the disruption resources captured by B, as deﬁned
in the current section. The attack policy mapping

, ˆ
F

and

ˆ
P

=

K

{

K
ak = g(

Ik to ak at time k is denoted as
,

Ik).

K

(5)

Examples of attacks policies for diﬀerent attack scenarios are given in Section 4.

In this section we describe the networked control system under attack with respect to the attack vector ak. Then
we detail the adversary’s system knowledge, the disclosure resources, and the disruption resources. Models of the
attack vector ak for particular disruption resources are also given.

3.1 Networked Control System under Attack

The system components under attack are now characterized for the attack vector ak, which also includes the fault
k ]⊤, the dynamics of the
z⊤
signal fk. Considering the plant and controller states to be augmented as ηk = [x⊤
k
closed-loop system composed by

under the eﬀect of ak can be written as

and

P

F

where the system matrices are

ηk+1 = Aηk + Bak + G

˜yk = Cηk + Dak + H

wk
vk #

"

wk
vk #

"

,

A =

A + BDcC BCc

"

BcC

Ac #

, G =

G BDc
0 Bc #

"

,

C =

,

C 0
h

i

H =

,

0 I
h

i

5

(6)

and B and D capture the way in which the attack vector ak aﬀects the plant and controller. These matrices are
characterized for some attack scenarios in Section 3.4.

Similarly, using
described by

P

and

D

as in (1) and (3), respectively, the anomaly detector error dynamics under attack are

ξk|k = Aeξk−1|k−1 + Beak−1 + Ge

rk = Ceξk−1|k−1 + Deak−1 + He

wk−1

"

vk #

wk−1

"

vk #

,

(7)

where ξk|k ∈

Rn is the estimation error and

Ae = (I

−
Ce = V C(I

KC)A,

Ge =

KC)A, He =

−

KC)G

K

,

−
KC)G V (I

i

−

(I
−
h
V C(I
h

−

.

CK)
i

The matrices Be and De are speciﬁc to the available disruptive resources and are characterized in Section 3.4.

3.2 System Knowledge

The amount of a priori knowledge regarding the control system is a core component of the adversary model, as
it may be used, for instance, to render the attack undetectable. In general, we may consider that the adversary
approximately knows the model of the plant ( ˆ
) and the
P
anomaly detector ( ˆ
. Figure 1 illustrates several types
D
K
of attack scenarios with diﬀerent amounts of required system knowledge. In particular, note that the replay attacks
do not need any knowledge of the system components, thus having
, while the covert attack requires full
knowledge about the system, hence

) and the algorithms used in the feedback controller ( ˆ
F
, ˆ
D}

), thus denoting the adversary knowledge by

ˆ
P
=

, ˆ
F

=

=

K

{

∅

,

,

.

K

{P

F

D}

3.3 Disclosure Resources

The disclosure resources enable the adversary to gather sequences of data from the calculated control actions uk and
the real measurements yk through disclosure attacks. Denote
as the disclosure
resources, i.e. set of actuator and sensor channels that can be accessed during disclosure attacks, and let
Ik be the
control and measurement data sequence gathered by the adversary from time k0 to k. The disclosure attacks can
then be modeled as

1, . . . , p

1, . . . , q

u
R

y
R

and

⊆ {

⊆ {

}

}

Ik :=

Ik−1 ∪ ("

Υu 0
0 Υy# "

uk
yk #)

,

(8)

where Υu
sponding data gathered by the adversary and

B|Ru|×q and Υy

∈

∈

Ik0 =

.

∅

B|Ry|×p are the binary incidence matrices mapping the data channels to the corre-

As seen in the above description of disclosure attacks, the physical dynamics of the system are not aﬀected by these
type of attacks. Instead, these attacks gather intelligence that may enable more complex attacks, such as the replay
attacks depicted in Figure 1.

3.4 Disruption Resources

As seen in the system dynamics under attack, (6) and (7), disruption resources are related to the attack vector ak
and may be used to aﬀect the several components of the system. The way a particular attack disturbs the system
operation depends not only on the respective resources, but also on the nature of the attack. For instance, a physical
attack directly perturbs the system dynamics, whereas a cyber attack disturbs the system through the cyber-physical
couplings. To better illustrate this discussion we now consider physical and data deception attacks.

6

3.4.1 Physical Resources

Physical attacks may occur in control systems, often in conjunction with cyber attacks. For instance, in [2] water was
pumped out of an irrigation system while the water level measurements were corrupted so that the attack remained
stealthy. Since physical attacks are similar to the fault signals fk in (1), in the following sections we consider fk to
be the physical attack modifying the plant dynamics as

xk+1 = Axk + B ˜uk + Gwk + F fk

yk = Cxk.

Considering ak = fk, the resulting system dynamics are described by (6) and (7) with

B =

F

"

0 #

, D = 0, Be = (I

KC)F, De = V C(I

KC)F.

−

−

Note that the disruption resources in this attack are captured in the matrix F .

3.4.2 Data Deception Resources

The deception attacks modify the control actions uk and sensor measurements yk from their calculated or real values
to the corrupted signals ˜uk and ˜yk, respectively. Denoting
as the deception
resources, i.e. set of actuator and sensor channels that can be aﬀected, the deception attacks are modeled as

y
I ⊆ {
R

u
I ⊆ {
R

1, . . . , p

1, . . . , q

and

}

}

˜uk := uk + Γubu
k,

˜yk := yk + Γyby
k,

(9)

Bp×|Ry
I |
I | represent the data corruption and Γu
where the signals bu
(B :=
0, 1
) are the binary incidence matrices mapping the data corruption to the respective data channels. The
}
matrices Γu and Γy indicate which data channels can be accessed by the adversary and are therefore directly related
to the adversary resources in deception attacks.

I | and by

I | and Γy

Bq×|Ru

R|Ry

R|Ru

k ∈

k ∈

∈

∈

{

Deﬁning ak = [bu⊤

k

by⊤
k+1

by⊤
k ]⊤, the system dynamics are given by (6) and (7) with

B =

BΓu 0 BDcΓy

"

0

0 BcΓy #

, D =

0 0 Γy

, Be =

h

i

KC)BΓu

(I
h

−

, De =

−

KΓy 0
i

V C(I
h

−

KC)BΓu V (I

−

CK)Γy 0

.

i

Note that deception attacks do not possess any disclosure capabilities, as depicted in Figure 1 for examples of
deception attacks such as the bias injection attack.

4 Attack Scenarios

In this section we discuss the general goal of an adversary and likely choices of the attack policy g(
,
). In particular,
·
·
using the framework introduced in the previous sections, we consider several attack scenarios where the adversary’s
goal is to drive the system to an unsafe state while remaining stealthy. For each scenario we formulate the cor-
responding stealthy attack policy, comment on the attack’s performance, and describe the adversary’s capabilities
along each dimension of the attack space in Figure 1, namely the disclosure resources, disruption resources, and
system knowledge. A set of these scenarios is illustrated by experiments on a process control testbed in Section 5.

4.1 Attack Goals and Constraints

In addition to the attack resources, the attack scenarios need to also include the intent of the adversary, namely the
attack goals and constraints shaping the attack policy. The attack goals can be stated in terms of the attack impact
on the system operation, while the constraints may be related to the attack detectability.

7

Several physical systems have tight operating constraints which if not satisﬁed might result in physical damage to
the system. In this work we use the concept of safe regions to characterize the safety constraints.

Deﬁnition 1 At a given time instant k, the system is said to be safe if xk ∈ Sx, where
set with non-empty interior.

Sx is a closed and compact

Assumption 2 The system is in a safe state at the beginning of the attack, i.e. xk0 ∈ Sx.
The physical impact of an attack can be evaluated by assessing whether or not the state of the system remained in
the safe set during and after the attack. The attack is considered successful if the state is driven out of the safe set.

Regarding the attack constraints, we consider that attacks are constrained to remain stealthy. Furthermore, we
consider the disruptive attack component consists of only physical and data deception attacks, and thus we have
by⊤
k ]⊤. Given the anomaly detector described in Section 2 and denoting
the attack vector ak = [f ⊤
k
ak0, . . . , akf }

as the attack signal, the set of stealthy attacks are deﬁned as follows.

kf
k0 =

by⊤
k+1

bu⊤
k

A

{

Deﬁnition 3 The attack signal

kf
k0 is stealthy if

A

rkk

k

< δr + δα,

k

∀

≥

k0.

Note that the above deﬁnition is dependent on the initial state of the system at k0, as well as the noise terms wk
and vk.

Since the closed-loop system (6) and the anomaly detector (7) under linear attack policies are linear systems, each
k and the following
of these systems can be separated into two components, the nominal component with ak = 0
systems

∀

k+1 = Aηa
ηa
k = Cηa
˜ya

k + Bak
k + Dak

and

k|k = Aeξa
ξa
ra
k = Ceξa

k−1|k−1 + Beak−1
k−1|k−1 + Deak−1,

with ηa

0 = ξa

0|0 = 0.

(10)

(11)

Assuming the system is behaving nominally before the attack, using the triangle inequality and linearity of (7) we
have

δr + δα, leading to the following deﬁnition:

ra
k|| ≤
||

δα ⇒ ||
Deﬁnition 4 The attack signal

rk|| ≤

kf
k0 is α

A

stealthy with respect to

if

ra
k||

||

D

< δα,

k

∀

≥

k0.

−

kf
k0 . Similarly, the
Albeit more conservative than Deﬁnition 3, this deﬁnition only depends on the attack signals
impact of attacks on the closed-loop system can also be analyzed by looking at the linear system (10), as illustrated
in Section 4.6 for the bias injection attack.

A

4.2 Denial-of-Service Attack

The DoS attacks prevent the actuator and sensor data from reaching their respective destinations and should therefore
be modeled as the absence of data, for instance uk =
if all the actuator data is unavailable. However such a model
would not ﬁt the framework in (6) and (7) where ak is assumed to be a real valued vector. Hence we consider instead
one of the typical mechanisms used by digital controllers to deal with the absence of data [23], in which the absent
1, . . . , p
data is replaced with the last received data, uτu and yτy respectively. Denoting
}
as the set of actuator and sensor channels that can be made unavailable, we can model DoS attacks as deception
attacks in (9) with

y
A ⊆ {
R

u
A ⊆ {
R

1, . . . , q

and

}

∅

bu
k :=
by
k :=

Su
k Γu⊤(uk −
Sy
k Γy⊤(yk −

−

−

uτu)
yτy )

(12)

B|Ru

A|×|Ru

k ∈

where Su
whether a DoS attack is performed ([S(·)
attacks on the data are a type of disruptive attacks, as depicted in Figure 1.

k ]ii = 1) or not ([S(·)

A| are boolean diagonal matrices where the i

th diagonal entry indicates
−
k ]ii = 0) on the corresponding channel. Therefore DoS

k ∈

A| and Sy

B|Ry

A|×|Ry

8

Attack policy: The attack scenario analyzed in this paper considers a Bernoulli adversary [1] on the sensor channels
following the random policy

where p is the probability of blocking the data packet at any given time.

P([Sy
P([Sy

k ]ii = 1) = 0,
k ]ii = 1) = p,

i = 1, . . . ,
i = 1, . . . ,

∀
∀

u
,
A|
|R
u
,
A|
|R

k < k0
k0
k

≥

Attack performance: Although the absence of data packets is not stealthy since it is trivially detectable, DoS
attacks may be misdiagnosed as a poor network condition. As for the impact on the closed-loop system, the results
available for Bernoulli packet losses readily apply to the current attack scenario [23, 24, 31]. In particular, we recall
a result for the case where a hold scheme (12) is used in the absence of data.

Proposition 5 (Theorem 8 in [31]) Assume the closed-loop system with no DoS attack is stable. Then the closed-
loop system with Bernoulli DoS attacks is exponentially stable for p

[0, 1) if the open-loop system

∈

is marginally stable.

ηk+1 =

A BCc
0 Ac #

"

ηk

Disclosure resources: Although the proposed model of DoS attacks in (12) contains the control and output
signals, note that no disclosure resources are needed in the actual implementation of the attack. Thus we have

u =

y =

R

R

.

∅

Disruption resources: The disruption capabilities correspond to the data channels that the adversary is able to
make unavailable,

u
A and
R

y
A.
R

System knowledge: For the Bernoulli attack policy, no a priori knowledge of the system model is needed.

4.3 Replay Attack

In replay attacks the adversary ﬁrst performs a disclosure attack from k = k0 until kr, gathering sequences of data
Ikr , and then begins replaying the recorded data at time k = kr + 1 until the end of the attack at k = kf > kr, as
illustrated in Figure 3. In the scenario considered here the adversary is also able to perform a physical attack while
replaying the recorded data, which covers the experiment on a water management SCADA system reported in [2]
and one of Stuxnet’s operation mode [9].

˜uk

yk

˜uk

yk

Network

Υuuk

uk

P

Ik

F

D

P

fk

Υyyk

Network

Network

bu
k

,

g(
∅

Ikr )

by
k

Network

˜yk

uk

˜yk

F

D

(a) Phase I of the replay attack (13).

(b) Phase II of the replay attack (14).

Figure 3. Schematic of the replay attack.

Attack policy: Similar to the work in [19], assuming

i.e., the adversary can corrupt the digital channels

(·) =

R

(·)
I
R

9

(13)

(14)

from which the data sequences are gathered, the replay attack policy can be described as

ak = 0

Phase I: 


Ik =

Ik−1 ∪ ("

Υu 0
0 Υy# "

uk
yk #)

,

with k0 ≤

k

≤

kr and

Ik0 =

∅

and



Phase II:

gf (
,
Ikr )
K
Υu(uk−T −

Υy(yk+1−T −



Υy(yk−T −



Ik−1,

uk)
yk+1)
yk)










ak =

Ik =






kf . An interesting instance of this attack scenario consists of applying a
where T = kr −
≤
pre-deﬁned physical attack to the plant, while using replay attacks to render the attack stealthy. In this case the
physical attack signal fk corresponds to an open-loop signal, fk = gf (k).

1 + k0 and kr + 1

≤

k

Attack performance: The work in [19] provided conditions under which replay attacks with access to all measure-
ment data channels are stealthy. However, these attacks are not guaranteed to be stealthy when only a subset of the
data channels is attacked. In this case, the stealthiness constraint may require additional knowledge of the system
model. For instance, the experiment presented in Section 5 requires knowledge of the physical system structure,
so that fk only excites the attacked measurements. Hence fk can be seen as a zero-dynamics attack with respect
to the healthy measurements, which is characterized in the section below. Since the impact of the replay attack is
dependent only on fk, we refer the reader to Section 4.4 for a characterization of the replay attack’s impact.

Disclosure resources: The disclosure capabilities required to stage this attack correspond to the data channels
that can be eavesdropped by the attacks, namely

u and

y.

R

R

Disruption resources: In this case the deception capabilities correspond to the data channels that the adversary
y
I . In particular, for replay attacks the adversary can only tamper with the data channels
can tamper with,
R
y
from which data has been previously recorded, i.e.
I ⊆ R
R

u
I ⊆ R
R

u
I and
R

u and

y.

Direct disruption of the physical system through the signal fk depends on having direct access to the physical system,
modeled by the matrix F in (1).

System knowledge: Note that no a priori knowledge
on the system model is needed for the cyber component
K
of the attack, namely the data disclosure and deception attack, as seen in the attack policy (13) and (14). As for
the physical attack, fk, the required knowledge is scenario dependent. In the scenario considered in the experiments
described in Section 5, this component was modeled as an open-loop signal, fk = gf (k).

4.4 Zero-Dynamics Attack

Recalling that for linear attack policies the plant and the anomaly detector are linear systems, (10) and (11)
k = 0, k = k0, . . . , kf . The idea of
respectively, Deﬁnition 4 states that this type of attacks are 0
kf
k0 so that the residue rk does not
0
change due to the attack.

stealthy if ra
stealthy attacks then consists of designing an attack policy and attack signal

A

−

−

A particular subset of 0

−

stealthy attacks are characterized in the following lemma:

Lemma 6 The attack signal

kf
k0 is 0

A

−

stealthy with respect to any

if ˜ya

k = 0,

k

∀

≥

k0.

D

10

PROOF. Consider the attacked components of the controller and the anomaly detector in (10) and (11) with
ˆxa
0 = ξa
k0, as
k
∀
the input to the controller (˜ya
k0, meaning that the detector’s inputs
are zero, we then conclude ra

0|0 = 0. From the controller dynamics it directly follows that ˜ya
k) is zero. Since ˆxa
k = 0,
k = 0,
≥

k0 results in ua

0 = 0 and ˜ya

k = 0,
k

k = ua

k = 0,

k0.

≥

≥

≥

∀

∀

∀

k

k

−

stealthy attacks and Lemma 6 indicate that these attacks are decoupled from the outputs of
Both the deﬁnition of 0
linear systems, rk and yk respectively. Hence ﬁnding 0
stealthy attack signals relates to the output zeroing problem
or zero-dynamics studied in the control theory literature [32]. Note that such an attack requires the perfect knowledge
of the plant dynamics P and the attack signal is then based on the open-loop prediction of the output changes due
Kz denote the zero-dynamics and there is no disclosure of sensor
to the attack. This is illustrated in Figure 4 where
or actuator data.

−

˜uk

yk

P

Network

bu
k

uk

g(

Kz,

)

∅

Network

˜yk

F

D

Figure 4. Schematic of the zero-dynamics attack.

Attack policy: The attack policy then corresponds to the input sequence (ak) that makes the outputs of the
process (˜ya
k) identically zero for all k and is illustrated in Figure 4. It can be shown [32] that the solution to this
problem is given by the sequence

ak = gνk,

(15)

parameterized by the input-zero direction g and the system zero ν.

For sake of simplicity we consider a particular instance of this attack, where only the actuator data is corrupted. In
this case the zero attack policy corresponds to the transmission zero-dynamics of the plant. The plant dynamics due
to an attack on the actuator data are described by

k+1 = Axa
xa
k + Bak
k = Cxa
˜ya
k

(16)

with ak = bu
calculated as the values ν

∈

k. Given the discrete-time system (16) with B having full column rank, the transmission zeros can be

C that cause the matrix P (ν) to lose rank, where

P (ν) =

νI

A

B

−
0 #

.

−
C

"

Those values are called minimum phase or non-minimum phase zeros depending on whether they are stable or
unstable zeros, respectively. In discrete-time systems a zero is stable if

< 1 and unstable otherwise.

ν
|

|

The input zero direction can be obtained by solving the following equation

νI

"

A

−
C

B

−

0 # "

x0
g #

=

0

"

0#

,

(17)

where x0 is the initial state of the system for which the input sequence (15) results in an identically zero output,
˜ya
k = 0

k.

∀

11

Lemma 7 Let x0 be the initial state of the system, where x0 satisﬁes (17). The state trajectories generated by the
zero-dynamics attack are contained in span(x0) i.e., xa

span(x0)

0.

k

k ∈

∀

≥

PROOF. Consider the zero-dynamics attack parameterized by x0 and g and denote L as a map for which Lx0 = g.
(A + BL)) x0 = 0 and conclude that x0 is an eigenvector of A+ BL associated with its
Then from (17) we have (νI
eigenvalue ν. Now consider the state evolution under attack, xa
0 = x0. The proof is completed
by noting that xa

1 = Ax0 + Bg = (A + BL)x0 = νx0 and applying an induction argument.

k + Bg with xa

k+1 = Axa

−

Attack performance: Note that the zero-dynamics attack is 0
of the system under attack xa
attack may be violated for large diﬀerences between xa
of the eﬀects of zero initial conditions on zero-dynamics attacks.

0 = x0. However the initial state
0 is deﬁned to be zero at the beginning of the attack. Therefore stealthiness of the
0 = 0 and x0. We refer the reader to [29] for a detailed analysis

stealthy only if xa

−

< 1, the attack will asymptotically decay to zero, thus having little eﬀect on the
If the zero is stable, that is
|
plant. However, in the case of unstable zeros the attack grows geometrically, which could cause a great damage to
the process. This statement is captured in the following result.

ν
|

Theorem 8 A zero-dynamics attack with
contained in

Sx.

ν
|

|

> 1 leads the system to an unsafe state if and only if span(x0) is not

PROOF. Follows directly from Lemma 7 and from the fact that the zero-attack with
state trajectory moving away from the origin along span(x0).

ν
|

|

> 1 generates an unstable

Disclosure resources: This attack scenario considers an open-loop attack policy and so no disclosure capabilities
are required, resulting in

u =

y =

and

k.

R

R

∅

u
k =
I

y
k =
I

∅ ∀

Disruption resources: The disruption capabilities in this attack scenario correspond to the ability of performing
deception attacks on the actuator data channels. Therefore the required resources are
, and
F = 0

y
I =
R

u
I =
R

1, . . . , q

{

}

∅

,

System knowledge: The ability to compute the open-loop attack policy requires the perfect knowledge zero-
dynamics, which we denote as
Kz. Note that computing the zero-dynamics requires perfect knowledge of the plant
dynamics, namely A, B, and C. No knowledge of the feedback controller or anomaly detector is assumed in this
scenario.

4.5 Local Zero-Dynamics Attack

In the previous scenario the zero-dynamics attack was characterized in terms of the entire system. Here we further
restrict the adversary resources by considering that the adversary has disruption resources and knows the model of
only a subset of the system. In particular, we rewrite the plant dynamics (16) as

x1
x2

k+1
k+1#

"

=

A11 A12
A21 A22# "

"

x1
k
x2
k#

+

B1

"

0 #

ak

˜ya
k =

C1 C2
h

i

x1
k
x2
k#

"

(18)

and assume the adversary has access to only A11, A21, B1, and C1. From the adversary’s view, this local system is
characterized by

k + B1ak + A12x2
k

k+1 = A11x1
x1
C1
A21#

yl
k =

"

x1
k,

12

where yl
subsystem and the remaining subsystems, A21x1
k.

k encodes the measurements depending on the local state, C1x1

k, and the interaction between the local

Attack policy: Similar to the zero-dynamics attack, the attack policy is given by the sequence

ak = gνk,

where g is the input zero direction for the chosen zero ν. The input zero direction can be obtained by solving the
following equation

B1

A11 −
0

0

νI







−
C1
A21

x1
0
g1#

"







0

= 

0


.





0




Note that the zero-dynamics parameterized by g1 and ν correspond to local zero-dynamics of the global system.

Attack performance: A similar discussion as for the global zero-dynamics attack applies to this scenario. In
particular, the stealthiness of the local zero-dynamics attack may be violated for large diﬀerences between x1
0 and
> 1 are more dangerous
0. Additionally, as stated in Theorem 8, attacks associated with unstable zeros yielding
and may lead the system to an unsafe state.

ν
|

|

R

R

and

y =

u =

Disclosure resources: This attack scenario considers an open-loop attack policy and so no disclosure capabilities
are required, resulting in

∅ ∀
Disruption resources: The disruption capabilities in this attack scenario correspond to the ability of performing
u
deception attacks on the actuator data channels of the local subsystem. Therefore the required resources are
I =
R
1, . . . , q1}
{
System knowledge: The open-loop attack policy requires the perfect knowledge of the local zero-dynamics,
denoted as ˜

, and F = 0.

y
I =
R

y
k =
I

u
k =
I

k.

∅

∅

,

Kz and obtained from A11, B1, C1, and A21.

4.6 Bias Injection Attack

Here a particular scenario of false-data injection is considered, where the adversary’s goal is to inject a constant
bias in the system without being detected. For this scenario, the class of α
stealthy attacks is characterized at
steady-state and a method to evaluate the corresponding impact is proposed. Furthermore, we derive the policy
yielding the largest impact on the system.

−

Attack policy: The bias injection attack is illustrated in Figure 5. The attack policy is composed of a steady-state
component, the desired bias denoted as a∞, and a transient component. For the transient, we consider that the
adversary uses a linear low-pass ﬁlter so that the data corruptions are slowly converging to the steady-state values.
As an example, for a set of identical ﬁrst-order ﬁlters the open-loop attack sequence is described by

ak+1 = βak + (1

β)a∗

∞,

−

(19)

where a0 = 0 and 0 < β < 1 can be chosen using the results from Theorem 15. The steady-state attack policy yielding
the maximum impact on the physical system is described below, where the computation of a∞ is summarized in
Theorem 12 and Theorem 14.

Attack performance: First the steady-state policy is considered. Denote a∞ as the bias to be injected and recall
the anomaly detector dynamics under attack (7). The steady-state detectability of the attack is then dependent on
the steady-state value of the residual

The largest α

−

stealthy attacks are then characterized by

(cid:0)

(cid:1)

ra
∞ =

Ce(I

−

Ae)−1Be + De

a∞ =: Graa∞.

Graa∞k2 = δα.
k

13

(20)

˜uk

yk

P

Network

bu
k

by
k

Network

g(

K0,

)

∅

uk

˜yk

F

D

Figure 5. Schematic of the bias injection attack.

Although attacks satisfying (20) could be detected during the transient, incipient attack signals slowly converging
to a∞ may go undetected, as it is stated in Theorem 15 and shown in the experiments in Section 5.

The impact of such attacks can be evaluated using the closed-loop dynamics under attack given by (6). Recalling
that ηa

k ]⊤, the steady-state impact on the state is given by
za

k = [xa
k

⊤

⊤

xa
∞ = [I

0] (I

−

A)−1 Ba∞ =: Gxaa∞.

Consider the following safe set deﬁned in terms of xa
k.

Deﬁnition 9 The 2

norm safe set

−

2
xa is deﬁned as
S

2
xa =

S

x

∈

Rn :

x
k

2
2 ≤

k

1

,

and the system is said to be in a safe state if xa

(cid:8)
k ∈ S

2
xa.

(cid:9)

For the 2
norm safe set
the largest bias in the 2

−

2
xa , the most dangerous bias injection attack corresponds to the α
S
norm sense, which can be computed by solving
−

−

stealthy attack yielding

max
a∞ k

Gxaa∞k

2
2

s.t.

Graa∞k

2
2 ≤

k

δ2
α.

(21)

Lemma 10 The optimization problem (21) is bounded if and only if ker(Gra)

ker(Gxa).

⊆

PROOF. Suppose that ker(Gra)
=
∅
of solutions, the optimization problem then becomes maxa∞∈ker(Gra) k
a maximization of a convex function, its solution is unbounded unless Gxaa∞ = 0 for all a∞ ∈
ker(Gra)
ker(Gra) concludes the proof.

ker(Gra). For this subset
2
2. Since the latter corresponds to
ker(Gra) i.e.,
ker(Gxa). Noting that the feasible set and the objective function are bounded for all solutions a∞ 6∈

and consider the subset of solutions where a∞ ∈

Gxaa∞k

⊆

Given Lemma 10, below we consider the non-trivial case for which it holds that ker(Gra)
ker(Gxa). The above
optimization problem can be transformed into a generalized eigenvalue problem and the corresponding optimal
solution characterized in terms of generalized eigenvalues and eigenvectors. Before formalizing this statement, we
introduce the following result.

⊆

Lemma 11 Let Q
ker(P ). Denote
λ∗ as the largest generalized eigenvalue of the matrix pencil (P, Q) and v∗ as the corresponding eigenvector. Then

Rn×n be positive semi-deﬁnite matrices satisfying ker(Q)

Rn×n and P

⊆

∈

∈

14

6
the matrix P
λ∗

0 and x⊤(P

−

≥

−

λ∗Q)x = 0 with Qx

= 0 if and only if x

∈

span(v∗).

λQ is negative semi-deﬁnite for a generalized eigenvalue λ if and only if λ = λ∗. Moreover, we have

PROOF. The proof can be found in Appendix A.

The optimal bias injection attack in the sense of (21) is characterized by the following result.

Theorem 12 Consider the 2
stealthy bias injection attack
−
parameterized by the optimization problem (21), which is assumed to be bounded. Denote λ∗ and v∗ as the largest
generalized eigenvalue and corresponding unit-norm eigenvector of the matrix pencil (G⊤
xaGxa, G⊤
raGra). The optimal
bias injection attack is given by

2
xa and the corresponding optimal α
S

norm safe set

−

and the corresponding optimal value is
and only if λ∗δ2
1.

α ≤

Gxaa∞k

k

a∗
∞ =

δα
Grav∗
±
k
2 = λ∗δ2
2
α. Moreover, at steady-state the system is in a safe state if

(22)

v∗,

k2

PROOF. Let P, Q
generalized eigenvalue of (P, Q) if rank(P
νQ for almost all values of ν
of P
−
which (P
λQ)v = 0 with v
are given by [13]

−

6∈

∈

Rn×n be positive semi-deﬁnite matrices such that ker(Q)

ker(P ). Recall that λ is a
λQ) < normalrank(P, Q), where normalrank(P, Q) is deﬁned as the rank
C. Furthermore, denote v as the generalized eigenvector associated with λ for
ker(Q). The necessary and suﬃcient conditions for the optimization problem (21)

⊆

−

∈

∞,

λ∗G⊤

0 = (G⊤
0 = a∗⊤
0

xaGxa −
∞ G⊤
y⊤(G⊤

raGra)a∗
δ2
raGraa∗
α,
λ∗G⊤
xaGxa −
raGra)y, for y
xaGxa, G⊤
Suppose λ∗ is the largest generalized eigenvalue of (G⊤
2
2 = δ2
Scaling v∗ by κ so that a∗
conditions are satisﬁed. As for the third condition, note that G⊤
Lemma 11, given that λ∗ is the largest generalized eigenvalue, G⊤
and the assumption that ker(Gra)
raGraa∗
∞ = λ∗a∗⊤
∞ G⊤
a∗⊤

∞ = κv∗ satisﬁes

⊆
∞ = λ∗δ2

xaGxaa∗

∞ G⊤

Graa∗

∞ −

α =

≥

raGra) and let v∗ be the corresponding eigenvector.
α leads to κ =
kGrav∗k2 , and the ﬁrst and second
raGra is negative semi-deﬁnite by
xaGxa −
xaGxa and G⊤
raGra are positive semi-deﬁnite,
ker(Gxa). To conclude our proof, observe that the optimal value is given by

±
λ∗G⊤

∞k

δα

k

xa
∞k

k

2
2 and thus, by deﬁnition, xa

2
xa if and only if λ∗δ2

∞ ∈ S

1.

α ≤

= 0.

More generally, the optimal bias injection attacks for ellipsoidal safe sets of the form

with P positive deﬁnite, can be found by replacing the objective function in (21) by

Sxa =
k

xa
∈
P 1/2Gxaa∞k

Rn : xa
2
2.

n

⊤

P xa

≤

1

,

o

Similarly, consider the safe set as deﬁned below.

Deﬁnition 13 The inﬁnity-norm safe set

∞
xa is deﬁned as
S

and the system is said to be in a safe state if xa

∞
xa =

S

x

{

Rn :

x
k∞ ≤

k

,

1

}

∈
∞
xa.

k ∈ S

Given the inﬁnity-norm safe set
stealthy
attack yielding the largest bias in the inﬁnity-norm sense. This attack can be obtained by solving the following
optimization problem

∞
xa , the bias injection attack with the largest impact corresponds to the α
S

−

max
a∞ k

Gxaa∞k∞
Graa∞k2 ≤

s.t.

k

δα.

(23)

A possible method to solve this problem is to observe that

Gxaa∞k∞ = max

i

k

e⊤
i Gxaa∞k2,

k

15

6
6
where the vector ei is i
into a set of problems with the same structure as (21), obtaining

−

th column of the identity matrix. Thus one can transform the optimization problem (23)

max
i

max
ai

∞

i Gxaai
e⊤
∞

2

s.t.

(cid:13)
Graai
(cid:13)
∞

(cid:13)
δα.
(cid:13)

2 ≤

(24)

Theorem 14 Consider the inﬁnity-norm safe set
parameterized by the optimization problem (23), which is assumed to be bounded. Let ei be the i
identity matrix and denote λ∗
the matrix pencil G⊤
the optimal bias attack is given by

stealthy bias injection attack
th column of the
i as the largest generalized eigenvalue and corresponding unit-norm eigenvector of
i , with v∗ as the corresponding generalized eigenvector,
raGra. Letting λ∗ = maxi λ∗

(cid:13)
∞
xa and the corresponding optimal α
(cid:13)
S

i and v∗
λG⊤

i Gxa−

xaeie⊤

(cid:13)
(cid:13)

−

−

and the corresponding optimal value is
if and only if λ∗δ2
α ≤

1.

k

Gxaa∞k∞ = √λ∗δα. Moreover, at steady-state the system is in a safe state

a∗
∞ =

±

δα
Grav∗

k

k2

v∗,

(25)

PROOF. The proof follows directly from considering the set of optimization problems in (24) and applying Theo-
rem 12.

Note that the steady-state value of the data corruption a∗
stealthy, since the
transients are disregarded. In practice, however, it has been observed in the fault diagnosis literature that incipient
faults with slow dynamics are hard to detect [7]. Therefore the low-pass ﬁlter dynamics in the attack policy (19)
could be designed suﬃciently slow as to diﬃcult detection. Below we provide a method to verify whether a given
ﬁlter parameter β renders the bias attack α

∞ is not suﬃcient for the attack to be α

stealthy.

−

−

Theorem 15 Consider the attack policy ak+1 = βak + (1
as the output of the autonomous system

β)a∗

∞ with β

(0, 1). The residual ra

k is characterized

∈

with

Ae Be

¯A = 

0 βI (1

0

−
I

, ψa

0 = 

0

0
a∗
∞

,











β)I







¯C =

0

0




Ce De 0
h

.

i

Moreover, the attack policy is α

−

stealthy for a given β if the following optimization problem admits a solution

min
γ,P
γ

s.t.

γ

≤

δ2
α,
0,

⊤

≻
0 ≤

P
ψa
0 P ψa
P ¯C⊤
¯C γI # (cid:23)
P

−

≺

"
¯A⊤P ¯A

1,

0,

0.

(26)

(27)

−
k+1 = ¯Aψa
ψa
k = ¯Cψa
ra

k

k

PROOF. The autonomous system is directly obtained by considering the augmented state ψa = [ξa
k ]⊤, where
lk is the state of the low-pass ﬁlter bank and sk the integral state initialized at s0 = a∞. Given this autonomous

k|k l⊤

k s⊤

⊤

16

system, one observes that the attack is α
by δ2
from the results in [4] regarding output-peak bounds for autonomous systems.

−
0, given the initial condition parameterized by α∗

α for all k

≥

stealthy if and only if the corresponding output-peak

2
2 is bounded
∞. The remainder of the proof follows directly

ra
kk

k

However, the output-peak bounds are in general conservative and thus the conditions in the previous theorem are
only suﬃcient.

Disclosure resources: Similarly to the zero attack, no disclosure capabilities are required for this attack, since
the attack policy is open-loop. Therefore we have

u =

y =

and

k.

R

R

∅

u
k =
I

u
k =
I

∅ ∀

Disruption resources: The biases may be added to both the actuator and sensor data, hence the required resources
are

. Since no physical attack is performed, we have F = 0.

1, . . . , p

1, . . . , q

,

u
I ⊆ {
R

y
I ⊆ {
R

}

}

System knowledge: As seen in (21), the open-loop attack policy (19) requires the knowledge of the closed-loop
system and anomaly detector steady-state gains Gra and Gxa, which we denoted as

K0 as shown in Figure 5.

5 Experiments

In this section we present our testbed and report experiments on staged cyber attacks following the diﬀerent scenarios
described in the previous section.

5.1 Quadruple-Tank Process

Our testbed consists of a Quadruple-Tank Process (QTP) [15] controlled through a wireless communication network,
as shown in Figure 6.

Actuator 

Attacker 

(cid:1873)(cid:3556)

(cid:1873)

Centralized controller 

(cid:1877)(cid:3556)

Sensor 

(cid:1877)

Figure 6. Schematic diagram of the testbed with the Quadruple-Tank Process and a multi-hop communication network.

The plant model can be found in [15]

˙h1 =

˙h2 =

˙h3 =

˙h4 =

a1
A1
a2
A2
a3
A3
a4
A4

−

−

−

−

2gh1 +

p

2gh2 +

p

2gh3 +

a3
A1
a4
A2
(1

p

2gh4 +

(1

p

2gh3 +

p

2gh4 +

γ1k1
A1
γ2k2
A2

u1,

u2,

p
γ2)k2
−
A3
γ1)k1
−
A4

u2,

u1,

(28)

[0, 30] are the heights of water in each tank, Ai the cross-section area of the tanks, ai the cross-section
where hi ∈
area of the outlet hole, ki the pump constants, γi the ﬂow ratios and g the gravity acceleration. The nonlinear plant

17

 
 
 
 
 
h1

h2

h3

h4

50

100

150

200

250

300

350

˜y1
˜y2

]

m
c
[

r
e
t
a
w

f
o

l
e
v
e
L

15

10

5

0
0

15

a
t
a
D
r
o
s
n
e
S

10

5

0
0
0.8

0.6

e
u
d
i
s
e
R

0.4

0.2

0
0

50

100

150

200

250

300

350

krkk2
δr + δα

50

100

150

200

250

300

350

Time [s]

Figure 7. Results for the DoS attack performed against both sensors since t ≈ 100 s.

model is linearized for a given operating point. Moreover, given the range of the water levels, the following safe set
is considered

Rn is a vector with all entries set to 1.

15, σ = 15

, where 1

Rn :

σ1

x

x

Sx =

{

∈

k

−

k∞ ≤

}

∈

The QTP is controlled using a centralized LQG controller with integral action running in a remote computer and
a wireless network is used for the communications. A Kalman-ﬁlter-based anomaly detector is also running in the
remote computer and alarms are triggered according to (4), for which we computed δr = 0.15 and chose δα = 0.25
for illustration purposes. The communication network is multi-hop, having one additional wireless device relaying
the data, as illustrated in Figure 6.

5.2 Denial-of-Service Attack

Here we consider the case where the QTP suﬀers a DoS attack on both sensors, while operating at a constant
set-point. The state and residual trajectories from this experiment are presented in Figure 7. The DoS attack follows
a Bernoulli model [1] with p = 0.9 as the probability of packet loss and the last received data is used in the absence
of data. From Proposition 5, we have that the closed-loop system under such DoS attack is exponentially stable.

The DoS attack initiates at t
100 s, leading to an increase in the residual due to successive packet losses. However
the residual remained below the threshold during the attack and there were no signiﬁcant changes in the system’s
state.

≈

5.3 Replay Attack

In this scenario, the QTP is operating at a constant set-point while a hacker desires to steal water from tank 4, the
upper tank on the right side. An example of this attack is presented in Figure 8, where the replay attack policy is the
90 s and then begins stealing
one described in Section 4.3. The adversary starts by replaying past data from y2 at t
water from tank 4 at t
180 s.
≈
To ensure stealthiness, the replay attack continues until the system recovered its original setpoint at t
280 s. As
we can see, the residue stays below the alarm threshold and therefore the attack is not detected.

100 s. Tank 4 is successfully emptied and the attacks stops removing water at t

≈

≈

≈

18

 
 
 
 
 
 
12

10

8

6

4

2

]

m
c
[

r
e
t
a
w

f
o

l
e
v
e
L

0
0
0.08

e
u
d
i
s
e
R

0.07

0.06
0

h2
h4
˜y2

50

100

150

200

250

300

350

krkk2

50

100

150

200

Time [s]

250

300

350

Figure 8. Results for the replay attack performed against sensor 2 from t ≈ 90 s to t ≈ 280 s. Additionally, the adversary
opens the tap of tank 4 at t ≈ 100 s and closes it at t ≈ 180 s.

5.4 Zero-Dynamics Attack

The QTP has a non-minimum phase conﬁguration in which the plant possesses an unstable zero. In this case, as
discussed in Section 4.4, an adversary able to corrupt all the actuator channels may launch a false-data injection
Sx =
attack where the false-data follows the zero-dynamics. Moreover, since the safe region is described by the set
, from Theorem 8 we expect that the zero-dynamics attack associated with the

15, σ = 15

Rn :

{
unstable zero can drive the system to an unsafe region. This scenario is illustrated in Figure 9.

k∞ ≤

σ1

−

∈

x

x

}

k

The adversary’s goal is to either empty or overﬂow at least one of the tanks, considered as an unsafe state. The
attack on both actuators begins at t
30 s, causing a slight increase in the residual. Tank 3 becomes empty at
t
55 s and shortly after actuator 2 saturates, producing a steep increase in the residual which then crosses the
threshold. However, note that the residual was below the threshold when the unsafe state was reached.

≈

≈

After saturation of the water level and the actuators, the system dynamics change and therefore the attack signal no
longer corresponds to the zero-dynamics and is detected, although it has already damaged the system. Thus these
attacks are particularly dangerous in processes that have unstable zero-dynamics and in which the actuators are
over-dimensioned, allowing the adversary to perform longer attacks before saturating.

5.5 Bias Injection Attack

The results for the case where u1 and y1 are respectively corrupted with bu
∞ and by
∞ are presented in the Figure 10. In
Sx while remaining stealthy for δα = 0.25.
this scenario, the adversary aimed at driving the system out of the safe set
The bias was slowly injected using a ﬁrst-order low-pass ﬁlter with β = 0.95 and the following steady-state value,
computed using Theorem 14, a∞ = [bu

9.42]⊤.

∞]⊤ = [2.15

∞ by

−

The bias injection began at t
70 s and led to an overﬂow in tank 4 at t
≈
started removing the bias and the system recovered the original setpoint at t
≈
the allowable bounds throughout the attack, thus the attack was not detected.

≈

225 s. At that point, the adversary
350 s. The residual remained within

19

 
 
 
 
h1
h2
h3
h4

20

40

60

80

100

˜u1
˜u2

20

40

60

80

100

krkk2
δr + δα

]

m
c
[

l
e
v
e
l

r
e
t
a

W

30

25

20

15

10

5

0
0
15

]

V

[

n
o
i
t
c
a

l
o
r
t
n
o
C

10

5

0
0
0.8

0.6

e
u
d
i
s
e
R

0.4

0.2

0
0

20

40

60

Time [s]

80

100

Figure 9. Results for the zero-dynamics attack starting at t ≈ 30 s. Tank 3 is emptied at t ≈ 55 s, resulting in a steep increase
in the residual since the linearized model is no longer valid.

6 Conclusions

In this paper we have analyzed the security of networked control systems. A novel attack space based on the
adversary’s system knowledge, disclosure, and disruption resources was proposed and the corresponding adversary
model described. Attack scenarios corresponding to replay, zero-dynamics, and bias injection attacks were analyzed
using this framework. In particular the maximum impact of stealthy bias injection attacks was derived and it
was shown that the corresponding policy does not require perfect model knowledge. These attack scenarios were
illustrated using an experimental setup based on a quadruple-tank process controlled over a wireless network.

7 Acknowledgments

This work was supported in part by the European Commission through the HYCON2 project, the EIT-ICT Labs
through the project SESSec-EU, the Swedish Research Council under Grants 2007-6350 and 2009-4565, and the
Knut and Alice Wallenberg Foundation.

References

[1] S. Amin, A.A. Cárdenas, and S.S. Sastry. Safe and secure networked control systems under denial-of-service attacks. In Hybrid
Systems: Computation and Control, pages 31–45. Lecture Notes in Computer Science. Springer Berlin / Heidelberg, April 2009.

[2] S. Amin, X. Litrico, S. S. Sastry, and A. M. Bayen. Stealthy deception attacks on water scada systems. In Proc. of the 13th ACM

Int. Conf. on Hybrid systems: computation and control, HSCC ’10, New York, NY, USA, 2010. ACM.

20

 
 
 
 
 
 
30

20

10

]

m
c
[

l
e
v
e
l

r
e
t
a

W

0
0
15

]

V

[

n
o
i
t
c
a

l
o
r
t
n
o
C

10

5

0
0
0.8

0.6

e
u
d
i
s
e
R

0.4

0.2

0
0

h1
h2
h3
˜y1
h4

100

200

300

400

500

u1
u2
˜u1

100

200

300

400

500

krkk2
δr + δα

100

200

300

400

500

Time [s]

Figure 10. Results for the bias attack against the actuator 1 and sensor 1 in the minimum phase QTP. The attack is launched
using a low-pass ﬁlter in the instant t ≈ 70 s and stopped at t ≈ 230 s.

[3] M. Bishop. Computer Security: Art and Science. Addison-Wesley Professional, 2002.

[4] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan. Linear Matrix Inequalities in System and Control Theory, volume 15 of

Studies in Applied Mathematics. SIAM, Philadelphia, PA, June 1994.

[5] A. Cárdenas, S. Amin, Z. Lin, Y. Huang, C. Huang, and S. Sastry. Attacks against process control systems: risk assessment, detection,
and response. In Proceedings of the 6th ACM Symposium on Information, Computer and Communications Security, ASIACCS ’11,
pages 355–366, New York, NY, USA, 2011. ACM.

[6] A.A. Cárdenas, S. Amin, and S.S. Sastry. Research challenges for the security of control systems. In Proc. 3rd USENIX Workshop

on Hot topics in security, San Jose, CA, USA, July 2008.

[7] J. Chen and R. J. Patton. Robust Model-Based Fault Diagnosis for Dynamic Systems. Kluwer Academic Publishers, 1999.

[8] S. X. Ding. Model-based Fault Diagnosis Techniques: Design Schemes. Springer Verlag, 2008.

[9] N. Falliere, L. Murchu, and E. Chien. W32.Stuxnet dossier, February 2011.

[10] A. Giani, S. Sastry, K. H. Johansson, and H. Sandberg. The VIKING project: an initiative on resilient control of power networks.

In Proc. 2nd Int. Symp. on Resilient Control Systems, Idaho Falls, ID, USA, August 2009.

[11] S. Gorman. Electricity grid in U.S. penetrated by spies. The Wall Street Journal, page A1, April 8th 2009.

[12] A. Gupta, C. Langbort, and T. Başar. Optimal control in the presence of an intelligent jammer with limited actions. In Proc. of

the 49th IEEE Conf. on Decision and Control, Atlanta, GA, USA, December 2010.

[13] Jean-Baptiste Hiriart-Urruty. Global optimality conditions in maximizing a convex quadratic function under convex quadratic

constraints. Journal of Global Optimization, 21(4):443–453, December 2001.

[14] I. Hwang, S. Kim, Y. Kim, and C. E. Seah. A survey of fault detection, isolation, and reconﬁguration methods. IEEE Transactions

on Control Systems Technology, 18(3):636–653, May 2010.

[15] K.H. Johansson. The quadruple-tank process: a multivariable laboratory process with an adjustable zero. IEEE Transactions on

Control Systems Technology, 8(3):456–465, May 2000.

21

 
 
 
 
 
 
[16] A. Khanafer, B. Touri, and T. Başar. Consensus in the presence of an adversary. In Proc. 3rd IFAC Workshop on Estimation and

Control of Networked Systems (NecSys’12), Santa Barbara, CA, USA, September 2012. To appear.

[17] O. Kosut, L. Jia, R. Thomas, and L. Tong. Malicious data attacks on smart grid state estimation: Attack strategies and
countermeasures. In Proceedings of the First IEEE International Conference on Smart Grid Communications, Gaithersburg, MD,
USA, October 2010.

[18] Y. Liu, M. K. Reiter, and P. Ning. False data injection attacks against state estimation in electric power grids. In Proc. 16th ACM

Conf. on Computer and Communications Security, Chicago, IL, USA, November 2009.

[19] Y. Mo and B. Sinopoli.

Secure control against replay attack.

In Proceedings of the 47th Annual Allerton Conference on

Communication, Control, and Computing, Allerton, IL, USA, October 2009.

[20] F. Pasqualetti, F. Dorﬂer, and F. Bullo. Cyber-physical attacks in power networks: Models, fundamental limitations and monitor
design. In Proc. of the 50th IEEE Conf. on Decision and Control and European Control Conference, Orlando, FL, USA, December
2011.

[21] T. Rid. Cyber war will not take place. Journal of Strategic Studies, 35(1):5–32, 2011.

[22] H. Sandberg, A. Teixeira, and K. H. Johansson. On security indices for state estimators in power networks. In Preprints of the First

Workshop on Secure Control Systems, CPSWEEK 2010, Stockholm, Sweden, April 2010.

[23] L. Schenato. To zero or to hold control inputs with lossy links? Automatic Control, IEEE Transactions on, 54(5):1093–1099, May

2009.

[24] L. Schenato, B. Sinopoli, M. Franceschetti, K. Poolla, and S. Sastry. Foundations of control and estimation over lossy networks.

Proceedings of the IEEE, 95(1):163–187, January 2007.

[25] R. Smith. A decoupled feedback structure for covertly appropriating networked control systems. In Proc. of the 18th IFAC World

Congress, Milano, Italy, August-September 2011.

[26] A. Teixeira, G. Dán, H. Sandberg, and K. H. Johansson. Cyber security study of a scada energy management system: stealthy

deception attacks on the state estimator. In Proc. of the 18th IFAC World Congress, Milano, Italy, August-September 2011.

[27] A. Teixeira, D. Pérez, H. Sandberg, and K. H. Johansson. Attack models and scenarios for networked control systems. In Proc. 1st

International Conference on High Conﬁdence Networked Systems, CPSWeek 2012, Beijing, China, 2012.

[28] A. Teixeira, H. Sandberg, G. Dán, and K. H. Johansson. Optimal power ﬂow: Closing the loop over corrupted data.

In Proc.

American Control Conference, Montreal, Canada, June 2012.

[29] A. Teixeira, I. Shames, H. Sandberg, and K. H. Johansson. Revealing stealthy attacks in control systems. In Proceedings of the 50th

Annual Allerton Conference on Communication, Control, and Computing, Allerton, IL, USA, October 2012.

[30] L. Xie, Y. Mo, and B. Sinopoli. False data injection attacks in electricity markets. In Proceedings of the First IEEE International

Conference on Smart Grid Communications, Gaithersburg, MD, USA, October 2010.

[31] W. Zhang, M. S. Branicky, and S. M. Phillips. Stability of networked control systems. IEEE Control Systems Magazine, 21:84–99,

2001.

[32] K. Zhou, J. C. Doyle, and K. Glover. Robust and Optimal Control. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1996.

A Proof of Lemma 11

Recall that λ is a generalized eigenvalue of (P, Q) if rank(P
deﬁned as the rank of P
associated with λ for which (P

νQ for almost all values of ν
λQ)v = 0 with v

∈
ker(Q).

−

6∈

λQ) < normalrank(P, Q), where normalrank(P, Q) is
−
C. Furthermore, denote v as the generalized eigenvector

Deﬁne T = [V ¯N VN ]
nonsingular. Given that ker(Q)

∈

Rn×n where the columns of VN are a basis for ker(Q) and V ¯N is chosen such that T is

ker(P ), the coordinate transformation induced by T leads to

−

⊆

T (P

−

λQ)T −1 =

˜P

"

−
0

λ ˜Q 0

0#

,

0 and ˜P

where ˜Q
0 if and only if ˜P
−
all the non-zero generalized eigenvalues of (P, Q) need to reduce the rank of ˜P
Hence we have proved that all generalized eigenvalues are non-negative and that λ∗

0 and we conclude that P

λQ

(cid:22)

−

−

(cid:23)

≻

λ ˜Q
0. Additionally, we see that
λ ˜Q and thus need to be positive.

(cid:22)

0.

≥

Now we show that ˜P
eigenvalue of ( ˜P , ˜Q) with the associated eigenvector ¯v. Then ¯v⊤( ˜P
positive or negative for all generalized eigenvalues λ

λ ˜Q is indeﬁnite for all generalized eigenvalues 0 < λ < λ∗. Let ¯λ > 0 be a generalized
λ)¯v⊤ ˜Q¯v, which can be made

(0, λ∗) and thus our assertion is proved.

λ ˜Q)¯v = (¯λ

−

−

−

∈

As the next step, we show that ˜P
to the eigenvalues of the positive semi-deﬁnite matrix M ˜P M with M = ˜Q−1/2. Furthermore note that ˜P

0. Since ˜Q is invertible, the generalized eigenvalues of ( ˜P , ˜Q) correspond
0

λ∗ ˜Q

λ∗ ˜Q

−

(cid:22)

−

(cid:22)

22

is equivalent to having M ˜P M
eigenvalue.

−

λ∗I

(cid:22)

0, which holds since M ˜P M is positive semi-deﬁnite with λ∗ as the largest

−
= 0, it is enough to verify that x⊤( ˜P

All that is left to show now is that x⊤(P
Qx
generalized eigenvector of ( ˜P , ˜Q) associated with λ∗. The proof is concluded by recalling that ˜P
x⊤( ˜P

λ∗ ˜Q)x = 0 if and only if x belongs to the subspace spanned by the eigenvectors associated with λ∗.

span(v∗). Given the condition
span(˜v∗), where ˜v∗ is the
λ∗ ˜Q
0, hence

λ∗Q)x = 0 with Qx
λ∗ ˜Q)x = 0 for x

∈
= 0 if and only if x

= 0 if and only if x

−

(cid:22)

−

∈

−

23

6
6
6
