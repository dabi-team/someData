8
1
0
2

p
e
S
5
1

]

G
L
.
s
c
[

1
v
4
8
7
6
0
.
9
0
8
1
:
v
i
X
r
a

Adversarial Reinforcement Learning for Observer Design in
Autonomous Systems under Cyber Attacks

Abhishek Gupta
Electrical and Computer Engineering
The Ohio State University
Columbus, OH 43210
gupta.706@osu.edu

Zhaoyuan Yang
Electrical and Computer Engineering
The Ohio State University
Columbus, OH 43210
yang.2507@buckeyemail.osu.edu

September 19, 2018

Abstract

Complex autonomous control systems are subjected to sensor failures, cyber-attacks, sensor
noise, communication channel failures, etc. that introduce errors in the measurements. The
corrupted information, if used for making decisions, can lead to degraded performance. We
develop a framework for using adversarial deep reinforcement learning to design observer strategies
that are robust to adversarial errors in information channels. We further show through simulation
studies that the learned observation strategies perform remarkably well when the adversary’s
injected errors are bounded in some sense. We use neural network as function approximator in
our studies with the understanding that any other suitable function approximating class can be
used within our framework.

1

Introduction

In 2009, Air France Flight 447 crashed in Atlantic Ocean, killing everyone on board, due to
inconsistency in airspeed readings from multiple sensors. The inconsistency in reading was attributed
to ice crystal growth in pitot tubes, which corrupted the measurement of the airspeed. Based on
corrupted measurements, the pilots took actions that forced the airplane into deep stall, after which
the pilots were unable to regain control over the airplane and it crashed. It should be noted that the
ﬂight was on autopilot mode before the pilots were alerted to take control of the aircraft after the on
board computer detected inconsistent measurements.

Such incidents in future autonomous systems cannot be avoided. One of the key issue in
widespread deployment of future autonomous systems are that some of the sensors may fail to
provide the correct information or the sensor measurement is tampered with by a strategic adversary1.
Such attacks, called deception attack, may induce the controller to take an action that is detrimental
to the system performance. In order to keep the system safe from such attacks, one needs to design
ﬁltering scheme (also called observer design in control theory literature) to automatically identify bad
data/measurements (which may be based on the system model), and reverse the eﬀects of the attack
(that is, compute the correct state of the system from the corrupted measurements). One of the key

1We freely assume in this paper that nature could be an adversarial agent.

1

 
 
 
 
 
 
challenge in this situation is that the observer may not know a priori if the data/measurements are
corrupted or not.

As a result, many authors have studied methods to detect a deception attack on the control system,
and have devised dynamic ﬁltering schemes to obtain the true state from corrupted measurements.
Kalman ﬁltering and particle methods are some of the techniques. If the measured information lie on
a hyperplane, then principal component analysis can also be used to detect bad data [27]. However,
these ﬁltering schemes depend strongly on the model of the control system, that is, the system’s
accurate model is explicitly needed to design the ﬁltering scheme. However, for very complex systems
like humanoid robots or autonomous cars, it may be diﬃcult to derive accurate system model from
basic physical principles. Consequently, one may be interested in designing a model-free ﬁltering
scheme that detect corrupted measurements and automatically corrects the errors introduced by the
adversary. We view our paper as a contribution to this important problem.

In our study, we assume that the state measurements have higher dimensions than the actual
state of the system. Thus, there is inherent redundancy in the measurement space, allowing the
observer to detect low dimensional adversarial attacks. Since the system follows rules of the physics,
the measurements lie in a lower dimensional (potentially nonlinear) manifold in the measurement
space. Thus, the goal of the observer is to learn a function that projects corrupted measurements
(which could lie anywhere in the measurement space) to that lower dimensional manifold.

Figure 1: The manifold on which measurements of an inverted pendulum lie. The point in space
away from the manifold is the observed measurements corrupted due to adversarial noise.

1 + y2

As a concrete example, consider the classical inverted pendulum on a cart problem. Suppose that
the inverted pendulum makes an angle θ with the vertical axis, and we have two measurements y1 =
cos(θ) and y2 = sin(θ). Naturally, the measurement space is [−1, 1]2, but the actual measurements
lie in the set y2
2 = 1. Thus, if the adversary perturbs the measurements slightly, then it is easy
to obtain an estimate of the state that is close to the true state. Indeed, if we do not know the
system model and the measurement functions accurately, then we need an algorithmic approach
to identify the lower dimensional subspace in which the true measurements of the states would lie.
We use reinforcement learning, with neural network used as function approximator, to identify such
a manifold in this paper. In Figure 1, we show such a manifold for the classical single inverted
pendulum in which we plot the manifold in which the position, speed, and angular velocity lie. Our
work in intimately connected to several strands of work in the literature as explicated below.

2

1.1 Deep Reinforcement Learning

Advances in deep learning has allowed researchers to use neural networks as function approximators
for reinforcement learning in systems with extremely large state and action spaces[13, 24, 15, 16, 7].
In these situations, neural network is used to store the value function, learned policy, and/or the
Q function. If the action space is continuous, then the reinforcement learning algorithm generally
suﬀers from low data eﬃciency and unstable performance during training. In benchmarking paper
[4], result shows that among the algorithms being implemented, Truncated Natural Policy Gradient
(TNPG), Trust Region Policy Optimization (TRPO) [22], and Deep Deterministic Policy Gradient
(DDPG) [13] are eﬀective methods for training deep neural network policies for continuous control
problems. In our study, we are using neural network as observer, which maps continuous corrupted
observations to continuous ﬁltered observations.

We use TRPO for implementing the observer design. Through solving constrained optimization
with KL divergence as constraints, the algorithm tends to give monotonic improvement with only
little changes in the hyperparameters [22].

1.2 Adversarial Example and Reinforcement Learning

Neural network are vulnerable to adversarial attacks [25, 5]. Fast gradient sign method [6] can be
used to generate adversarial image noises for convolutional neural network, and authors explain that
cause of adversarial example is linear nature of neural network. In the context of reinforcement
learning featuring attacks on neural network policies, authors in [11] use fast gradient sign method
[6] to generate adversarial noise to image input, and the corrupted image is used for the control
task. It turns out regardless of which environment the policy is trained for or how it is trained, it is
possible to signiﬁcantly decrease the performance of policy [11]. Besides adding adversarial noise on
inputs, adversarial noise can also be added by applying adversarial force on physical systems [20] or
adding noise to corrupt actions made by controller [19]. It turns out adding adversarial noise while
training will make reinforcement learning more robust to environmental noise. Imitation learning
can also beneﬁt from adding adversarial noise while supervisors making demonstrations [12].

1.3 Neural Network as State Estimator/Observer

In continuous control problems, neural network is typically used to store policy or value function, but
they can also be used as an observer. Neural network observers can be used to estimate unavailable
system states, as has been investigated in [14] and [9]. Compared with Kalman ﬁlters for sensor
failure detection, identiﬁcation, and accommodation, neural networks’ performance for sensor failure
detection, identiﬁcation, and accommodation are better when applied to any dynamic system where
system model and ﬁlter’s model mismatch [18]. Reinforcement learning can also be used as state
estimator to estimate hidden variables and parameters of nonlinear dynamic systems[17].

1.4 Deception Attack of Autonomous System

Deception attacks are cyber attacks on autonomous systems in which a strategic adversary injects
noise in the communication channel between the sensor and the command center. Such attacks
on autonomous systems have been investigated in [1, 8, 2, 26]. In complex cyberphysical systems,
the nominal plant model and observation model may not capture all the nonlinearities in the

3

measurements. A natural question to ask would be: Can we use the measurement data of the plant
to identify the manifold in which the measurements lie?

Motivated by disparate strands of research on deep reinforcement learning and cyber attacks
on autonomous systems, we investigate in this paper if deep reinforcement learning can be used
to reliably design an observer for autonomous systems in a data-driven manner that can ﬁlter the
eﬀects of adversarial attacks. We ﬁx the control policy, and use system’s performance to train
the observer. The adversary is simultaneously trained (as in ﬁctitious play method used in game
theory) to minimize the system’s performance. In our simulation, we use neural networks as function
approximators for both observer map and adversary’s policy. However, some prior domain knowledge
can be used for coming up with better/reliable function approximators depending on the application.

2 Problem Formulation

We consider a Markov decision problem in which the state, action, and actuation noise at time t is
denoted by xt, ut, and wt, respectively. We use X to denote the state space, U to denote the action
space, and W to denote the actuation noise space. The state evolution equation is given by

xt+1 = f (xt, ut, wt), t = 1, 2, . . . .

We assume that x0 is the initial state and the noise distribution is known. Further, we assume that
the optimal control policy γ∗ : X → U is known and is Lipschitz continuous with Lipschitz constant
Lγ∗.

The state is measured through sensors and sent to the command center via a communication
channel. The measurement and communication process opens up the information to sensor noise,
sensor failures, packet errors, and cyber-attacks, which corrupt the information being sent to the
command center. If the command center uses the corrupted information to make decisions, then the
reward to the command center reduces. Thus, there is a need to robustly ﬁlter the errors in the
information introduced due to exogenous sources to make optimal decisions that maximizes long
term discounted reward.

To model this situation, we consider an adversary that takes as input the true state of the
system and outputs a corrupted version of the state. Let vt and zt be uniformly distributed random
variables in the set [0, 1] and are independent of each other. Let g : X × [0, 1] → X be the
adversary’s (behavioral) strategy and G be the set of all possible adversary’s (behavioral) strategies.
The command center takes as input the corrupted state yt = g(xt, vt) and generates an estimate
ˆxt = h(yt, zt) using estimation rule h : X × [0, 1] → X . We use H to denote the set of all possible
estimation rules. We note here that the observer can potentially use randomized estimation rules as
well (in many games, it is beneﬁcial to act according to randomized rules to introduce robustness to
other player’s biases)

The reward of the command center is given by

J(g, h) := Jx0,γ∗(g, h) = E

(cid:34) ∞
(cid:88)

t=0

(cid:35)
(cid:12)
(cid:12)
ut = γ∗(h(g(xt, vt), zt))
αtr(xt, ut)
(cid:12)
(cid:12)

.

The goal of the command center is to maximize the total discounted reward by picking an
appropriate estimation map h, whereas the goal of the adversary is to minimize the total discounted

4

yt = g(xt, vt)

Observer

ˆxt = h(yt, zt)

Controller

Adversary

xt

Environment

ut

Figure 2: Observer to recover true state. Objective of observer is to estimate true state returned
from environment such that controller can maintain its good performance.

reward. Thus, we arrive at a zero-sum game, where we are interested in the saddle-point equilibrium
(g∗, h∗) such that

J(g, h∗) ≤ J(g∗, h∗) ≤ J(g∗, h).

In this paper, we refer to the command center and the adversary to be the players of this zero-sum
game.

3 Fictitious Play and A Function Approximation Approach

Fictitious play is a iterative best response method in game theory in which at each time, each player
forms an empirical belief on the actions taken by the other player, and plays the best response to
that belief. It has been shown to converge in zero-sum game with ﬁnite strategy spaces in [3].

However, in the problem at hand, the strategy spaces of the adversary and the command center,
G and H, are uncountable spaces, even if the state and action spaces of the command center are
ﬁnite. Moreover, if the state and action spaces are uncountable (as in inverted pendulum), then
the strategy spaces are inﬁnite dimensional. Thus, we need to resort to function approximation,
wherein we pick a suitable choice of parametrized function spaces to restrict the strategy spaces of
the players. We let Θ ⊂ Rn denote the parameter space of the adversary’s parametrized strategy
set G(Θ) := {gθ : X × [0, 1] → X }. Similarly, we let Ξ ⊂ Rm denote the parameter space of the
command center’s parametrized estimator set H(Ξ) := {hξ : X × [0, 1] → X }.

3.1 Existence of Approximate Nash Equilibrium

Since we are using function approximation using neural network in this paper, we need to understand
if ﬁctitious play can lead to an approximate saddle-point equilibrium in the game formulated above.
We have the following result on this issue.

Theorem 1 Fix (cid:15) > 0. Suppose that

1. For every h ∈ H, there exists hξ ∈ H(Ξ) such that

|J(g, h) − J(g, hξ)| ≤ (cid:15)

for all g ∈ G.

2. For every g ∈ G, there exists gθ ∈ G(Θ) such that

|J(gθ, h) − J(g, h)| ≤ (cid:15)

for all h ∈ H.

5

Then, for every saddle-point equilibrium (g∗, h∗) of the game, there exists a pair of functions (gθ∗, hξ∗)
that is 3(cid:15)-Nash equilibrium of the game.

Proof:
need to prove that for any (g, h) ∈ G × H, we have

First, construct a pair of functions (gθ∗, hξ∗) that is at most (cid:15) away from (g∗, h∗). We

J(gθ∗, hξ∗) ≤ J(gθ∗, h) + 3(cid:15),
J(gθ∗, hξ∗) ≥ J(g, hξ∗) − 3(cid:15).

(1)

(2)

The following two set of inequalities follows from the assumptions:

J(g∗, h∗) ≤ J(g∗, h) ≤ J(gθ∗, h) + (cid:15),
J(g∗, h∗) ≥ J(gθ∗, h∗) − (cid:15) ≥ J(gθ∗, hξ∗) − 2(cid:15).

Collecting the inequalities above, we get (1). The inequalities in (2) can also be obtained in a similar
fashion.

Thus, by picking an appropriate function approximator class for the adversary and the observer, we
can reach as close to the saddle-point equilibrium as possible.

3.2 Sequential and Simultaneous Policy Update Rules in Games
We are now in a position to deﬁne ﬁctitious play for our observer design. Let us deﬁne ˜J(θ, ξ) :=
J(gθ, hξ). The observer map and adversary’s best response can be trained through sequential update
scheme or simultaneous update scheme. First, we can pick θ0 and ξ0 arbitrarily. The traditional
simultaneous and sequential update schemes in zero-sum games are deﬁned as follows:

Sequential Updates:

θk+1 =

(cid:40)

θk + η∇θ ˜J(θk, ξk)
θk

k is odd
k is even

,

ξk+1 =

(cid:40)

ξk − η∇ξ ˜J(θk, ξk)
ξk

k is even
k is odd

.

Simultaneous Updates:

θk+1 = θk + η∇θ ˜J(θk, ξk),

ξk+1 = ξk − η∇ξ ˜J(θk, ξk).

Typically, if the function ˜J is strictly convex in ξ and strictly concave in θ, then the update
schemes are known to converge [21].
In this paper, since we use neural networks as function
approximators, the map ˜J does not satisfy the strict concave/convex conditions. As a result, the
ﬁctitious play may not converge.

In the problem at hand, computing ˜J for every value of θ ∈ Θ and ξ ∈ Ξ is not possible. Thus,
we use TRPO with generalized advantage estimate to compute the next iterate [23]. In this method,
we compute θk+1 by solving

θk+1 = argmax

θ∈Θ

1
N

(cid:34)

N
(cid:88)

n=1

gθ(yn|xn)
gθk (yn|xn)

(cid:35)
Agθk (xn, yn)

subject to

1
N

N
(cid:88)

(cid:104)

n=1

DKL

(cid:0)gθk (·|xn)(cid:13)

(cid:13)gθ(·|xn)(cid:1)(cid:105)

≤ δ,

6

where x1 is initialized randomly, and the generalized advantage estimate Agθk is given by

Agθk (xn, yn) = (1 − λ)

(λα)l(cid:16)

∞
(cid:88)

l=0

r(xn+l, un+l) + αVζk (xn+l+1) − Vζk (xn+l)

(cid:17)

,

with Vn =

∞
(cid:88)

t=0

(cid:16)
αtr(xn+t, un+t) and Vζk = min
ζ

Vn(xn) − Vζ(xn)

(cid:17)2

.

In the computation of the advantage estimate for computing θk+1, we ﬁx ξk. A similar approach

is adopted for computing the next iterate ξk+1 from ﬁxed ξk and θk.

4 Simulation Results on Inverted Pendulum and Key Observations

We now demonstrate performance of the reinforcement learning for learning adversary’s policy and
observer’s map in three diﬀerent settings: (a) performance of adversary without an observer; (b)
performance of adversary and observer in sequential update setting; (c) performance of adversary
and observer in simultaneous update setting. In following sections, Trust Region Policy Optimization
from OpenAI Baselines [10] is used to optimize the players’ policies.

θ

ut

x

Figure 3: An inverted pendulum, where x is the displacement from the origin, θ is angle from the
vertical, and ut is the force on the cart.

4.1 Environment

We use inverted pendulum from Roboschool [24] as simulation environment. Environment is shown
in Figure 3. The inverted pendulum’s state space has three dimensions – location x, velocity vx, and
the angle from the vertical θ, and the rotational speed ˙θ. The measurement space has 5 dimensions –
location x, speed vx, cos θ, sin θ and angular velocity ˙θ. Reward for the system is r(θ) = 0.08 − | sin θ|.
Since θ is the only payoﬀ relevant state, we assume that the adversary attacks the measurement
sin θ. The control action on the pendulum is the force F . Roboschool provides pre-trained model for
diﬀerent environments, and we use pre-trained model of inverted pendulum from OpenAI to analyze
performance of adversary agent.

In what follows, each episode ends at either 1000 time steps or when |θ| > 0.2, whichever is
earlier. Thus, when the adversary learns the policy that can push |θ| > 0.2 in 200 time steps, then
the episode ends at 200.

7

4.2 Performance of Controller with Adversary and without Observer

Among all measurements, sin θ is the most crucial measurement, since it relays information about
the angle of the pendulum. Thus, we let the adversarial agent to corrupt sin θ through an adversarial
noise. Reward for adversarial agent is deﬁned as | sin θ| − 0.08. We assume that the adversarial noise
is bounded as follows. We compute the standard deviation σ of sin θ in the noiseless environment in
1000 trials, and we bound the adversarial noise magnitude to be between [−2σ, 2σ]. We empirically
found σ = 0.00789366.

Without observer, adversary learns a policy that reduces the performance of controller through
generating adversarial noise for sin θ. Performance of adversary in each episode is shown in Figure 4.
Each episode has at most 1000 time steps. Also note that since we have 1000 time steps in each
episode, if θ is small all the time (which implies the inverted pendulum is perfectly balanced by
the controller), then the total accumulated reward by the controller at the end of the episode is
0.08 ∗ 1000 = 80. Naturally, the minimum possible reward the adversary can earn in each episode
is also -80. The maximum reward adversary can earn is 0, which it can secure if it induces the
pendulum to fall in the ﬁrst time step itself (if pendulum falls in ﬁrst step, then the reward for
adversary will be zero since episode has 0 time steps).

Figure 4: The left ﬁgure depicts the evolution of reward of the adversary while learning, and the
right ﬁgure depicts the number of time steps in each episode.

We observe in the Figure 4 that in the absence of the observer (or the observer is ﬁxed to be an
identity map), the adversary was able to learn the worst case adversarial noise that leads to the
worst performance. In the initial learning phase (episodes 1-1900), the adversary’s noise did not
aﬀect the ﬁnal control action very much and its accrued reward in each episode was roughly equal to
−1000 × 0.08. After about 1900 episodes, the adversary’s performance improves dramatically as it
learns how to sway the observation in a manner that reduces the controller’s performance.

4.3 Fictitious Play with Adversary and Observer

We count 15000 time steps as one iteration in the ﬁctitious play and perform both sequential updates
and simultaneous updates for 5 trials. Bounds for adversarial noise are set to 2σ, 4σ, and 8σ in the
following analysis. The results are as follows:

Sequential Update: We run 5 trials for each bound on adversarial noise magnitude. Since
adversary and observer learn alternatively during the simulation, each player updates its neural
network parameters for 15000 steps against the ﬁxed policy of the other player, and then the other
player starts learning. Results of all 5 trials are similar, so we only show one of the results as a
representative learning behavior for varying bounds on adversarial noise in Figure 5.

8

Figure 5: The plot of cumulative rewards vs. episodes for the adversary and the observer. A sample
trial of sequential update with adversarial noise in [−2σ, 2σ] in the top plot, [−4σ, 4σ] in the middle
plot, and [−8σ, 8σ] in the bottom plot.

Simultaneous Update: Similar to sequential update, we ran 5 trials, and since the results were
similar, we present the representative plots below in Figure 6 with diﬀerent bounds on adversarial
noise.

Figure 6: The plot of cumulative rewards vs. episodes for the adversary and the observer. A sample
trial of simultaneous update with adversarial noise in [−2σ, 2σ] in the top plot, [−4σ, 4σ] in the
middle plot, and [−8σ, 8σ] in the bottom plot.

9

An interesting observation in the plots above is that for small values of adversarial noise, the
observer is in most cases able to reverse the corruption introduced by adversary. Consequently, the
payoﬀ to the system in each episode is close to the maximum attainable payoﬀ (about 80) after
the observer is trained. We note here that the observer need not compute the uncorrupted state
exactly; as far as the observer is able to compute the estimate that, when fed into the controller,
takes the same action as in the uncorrupted state case, the ﬁnal payoﬀs are the same. The abrupt
jumps or drops in the payoﬀs of the players in certain episodes during the learning phase are due
to the other player learning the best response strategy very well in the corresponding iteration of
the ﬁctitious play. If the adversary is strong enough to corrupt the measurements by large quantity,
then the data-driven observer design using our method is not successful. We intend to address this
very interesting problem in our future research.

5 Conclusion

Autonomous systems must be resilient to cyber attacks and sensor failures. In modern systems, this
is typically attained through introducing redundancy in measurements and designing an observer,
which takes into account the physical model of the plant and determines an estimate of the state
based on the measurements. Kalman ﬁltering scheme and its variants presents a method to do it if
the model of the system is known. In our work, we studied the problem of observer design through
measurement data and in the presence of an adversary for a simple autonomous system–an inverted
pendulum. We believe that the methods developed in this paper can be extended to more complex
systems, which we will investigate in our future work.

References

[1] Saurabh Amin, Xavier Litrico, S Shankar Sastry, and Alexandre M Bayen. Stealthy deception
attacks on water scada systems. In Proceedings of the 13th ACM International Conference on
Hybrid systems: Computation and Control, pages 161–170. ACM, 2010.

[2] Saurabh Amin, Xavier Litrico, Shankar Sastry, and Alexandre M Bayen. Cyber security of
water scada systems—part i: Analysis and experimentation of stealthy deception attacks. IEEE
Transactions on Control Systems Technology, 21(5):1963–1970, 2013.

[3] George W Brown. Iterative solution of games by ﬁctitious play. Activity analysis of production

and allocation, 13(1):374–376, 1951.

[4] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learning,
pages 1329–1338, 2016.

[5] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning
visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1625–1634, 2018.

[6] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. arXiv preprint arXiv:1412.6572, 2014.

10

[7] Xiaoxiao Guo, Satinder Singh, Honglak Lee, Richard L Lewis, and Xiaoshi Wang. Deep learning
for real-time atari game play using oﬄine monte-carlo tree search planning. In Advances in
neural information processing systems, pages 3338–3346, 2014.

[8] Abhishek Gupta, Cédric Langbort, and Tamer Başar. One-stage control over an adversarial
channel with ﬁnite codelength. In Decision and Control and European Control Conference
(CDC-ECC), 2011 50th IEEE Conference on, pages 4072–4077. IEEE, 2011.

[9] Pingan He and Sarangapani Jagannathan. Reinforcement learning-based output feedback
control of nonlinear systems with input constraints. IEEE Transactions on Systems, Man, and
Cybernetics, Part B (Cybernetics), 35(1):150–154, 2005.

[10] Christopher Hesse, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and

Yuhuai Wu. OpenAI baselines, 2017.

[11] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial

attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017.

[12] Michael Laskey, Jonathan Lee, Roy Fox, Anca Dragan, and Ken Goldberg. Dart: Noise injection

for robust imitation learning. In Conference on Robot Learning, pages 143–156, 2017.

[13] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval
Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning.
arXiv preprint arXiv:1509.02971, 2015.

[14] Yan-Jun Liu, Mingzhe Gong, Shaocheng Tong, CL Philip Chen, and Dong-Juan Li. Adaptive
fuzzy output feedback control for a class of nonlinear systems with full state constraints. IEEE
Transactions on Fuzzy Systems, 2018.

[15] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pages 1928–1937, 2016.

[16] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, and George Ostrovski.
Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.

[17] Jun Morimoto and Kenji Doya. Reinforcement learning state estimator. Neural computation,

19(3):730–756, 2007.

[18] Marcello R Napolitano, Dale A Windon, Jose L Casanova, Mario Innocenti, and Giovanni
Silvestri. Kalman ﬁlters and neural-network schemes for sensor validation in ﬂight control
systems. IEEE transactions on control systems technology, 6(5):596–611, 1998.

[19] Olalekan Ogunmolu, Nicholas Gans, and Tyler Summers. Robust zero-sum deep reinforcement

learning. arXiv preprint arXiv:1710.00491, 2017.

[20] Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial

reinforcement learning. arXiv preprint arXiv:1703.02702, 2017.

11

[21] J Ben Rosen. Existence and uniqueness of equilibrium points for concave n-person games.

Econometrica: Journal of the Econometric Society, pages 520–534, 1965.

[22] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pages 1889–1897, 2015.

[23] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
arXiv preprint

dimensional continuous control using generalized advantage estimation.
arXiv:1506.02438, 2015.

[24] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

[25] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199,
2013.

[26] André Teixeira, Iman Shames, Henrik Sandberg, and Karl H Johansson. Revealing stealthy
attacks in control systems. In Communication, Control, and Computing (Allerton), 2012 50th
Annual Allerton Conference on, pages 1806–1813. IEEE, 2012.

[27] Le Xie, Yang Chen, and P Roshan Kumar. Dimensionality reduction of synchrophasor data for
early event detection: Linearized analysis. IEEE Transactions on Power Systems, 29(6):2784–
2794, 2014.

12

