Building an Emulation Environment for Cyber Security
Analyses of Complex Networked Systems

Florin Dragos Tanasache1, Mara Sorella1, Silvia Bonomi1,2,
Raniero Rapone, Davide Meacci
1DIAG - Sapienza University of Rome, Via Ariosto 25, 00185, Rome, Italy
2CINI Cyber Security National Laboratory, Via Ariosto 25, 00185, Rome, Italy
tanasache.1524243@studenti.uniroma1.it,
{sorella,bonomi}@diag.uniroma1.it,
raniero.rapone@outlook.com, davide.meacci@gmail.com

Abstract

Computer networks are undergoing a phenomenal growth, driven by the rapidly
increasing number of nodes constituting the networks. At the same time, the number of
security threats on Internet and intranet networks is constantly growing, and the testing
and experimentation of cyber defense solutions requires the availability of separate, test
environments that best emulate the complexity of a real system. Such environments
support the deployment and monitoring of complex mission-driven network scenarios,
thus enabling the study of cyber defense strategies under real and controllable traﬃc
and attack scenarios.
In this paper, we propose a methodology that makes use of a combination of techniques
of network and security assessment, and the use of cloud technologies to build an em-
ulation environment with adjustable degree of aﬃnity with respect to actual reference
networks or planned systems. As a byproduct, starting from a speciﬁc study case,
we collected a dataset consisting of complete network traces comprising benign and
malicious traﬃc, which is feature-rich and publicly available.

Keywords: Emulation Environment, Private Cloud, Cyber Security

8
1
0
2

t
c
O
3
2

]

R
C
.
s
c
[

1
v
2
5
7
9
0
.
0
1
8
1
:
v
i
X
r
a

1

 
 
 
 
 
 
1

Introduction

Context and Motivation. Starting from the past decade, cyber attacks have become
increasingly sophisticated, stealthy, targeted and multi-faceted, featuring zero-day exploits
and highly creative interdisciplinary attack methods. Furthermore, it is most likely an
upward trend, due to two main factors: (i) increasing complexity of ICT networks and (ii)
increasing capabilities of cyber attackers. As a consequence, cyber security is becoming of
primary importance for every organization, including small and medium enterprises and large
companies, as well as critical infrastructures on which our society is becoming increasingly
dependent.

One of the countermeasures that can be taken to face attackers is to try to play the role
of the attacker and stress the environment that is aimed to protect. However, in order to
enable this possibility, a separate, dedicated emulation/simulation environment must be set
in place. Such environment should be able to: (i) represents realistic cyber environments
that ﬁt the testing objectives, (ii) oﬀer tools for producing both benign and malicious system
events and (iii) support the deﬁnition and creation of new scenarios and cyber threats in a
cost and time-eﬀective manner [3, 13].

Combining network and security assessment techniques with the use of cloud technolo-
gies allows for the deployment of emulation environments. The objective of an emulation
environment is enabling the deployment of fully virtualized instances of computer networks,
with adjustable degree of aﬃnity with respect to an actual reference networks or planned
systems [14], allowing for arbitrary speculative analyses, notably for what concerns cyber
security.

Related Work. To the best of our knowledge, currently, very few complete solutions exist
to support the design, development and deployment of an emulation environment to support
testing and training. In [3] the authors proposed a Virtual Infrastructure for Network Emu-
lation (VINE), pointing out for a set of tools that helps the construction of virtual topologies
allowing for diﬀerent type of experiments for cyber security researchers. Furthermore, VINE
supports traﬃc behavior generation using monitored agents or the traditional packet capture
for a low-level analysis of the state of the hosts. Operators can exploit a web-based interface,
to manually create, modify, and monitor experiment testbeds. The main drawback of this
paper is that the platform is overviewed at a rather general viewpoint, without allowing
for a deep comparison. In the speciﬁc context of cybersecurity, in [13], the Cyber Range
Instantiation System (CyRIS) system is presented, as a solution, based on OpenStack, for
reliably and repeatably deploying cyber ranges, i. e., , technology development environments
for cybersecurity training courses. It supports basic automation for systems and network
deployment, and a certain degree of automation for sophisticated security activities (such as
modifying ﬁrewall rules or planting malware). Another, close-sourced example is [23]. While
these works provide valid tools for deploying testbeds for cyber experimentation, none of
them tackles the task of documenting a methodological framework for creating emulation
environments (along with the multiple existing technical tradeoﬀs). Our paper, instead,
presents a complete end-to-end methodology to build a virtual environment and deploying

2

a cyber security testbed, mapped from an existing network. Furthermore, our work directly
shows the beneﬁt of the approach by providing a realistic dataset which can beneﬁt multiple
sides of the cyber-security scientiﬁc community.

Contributions. This paper provides three main contributions: (i) it proposes a methodol-
ogy for designing and deploying a cloud-based emulation environment, (ii) it discusses the
application of the proposed methodology to a case study and (iii) it shows the ﬂexibility and
power of the proposed approach to build an environment that is able to generate a realistic
dataset, comprising of benign and malicious traﬃc. Furthermore, the dataset has been pro-
cessed to obtain feature-rich labeled attack ﬂows, useful for cyber security analyses such as
training IDS and IPS classiﬁers and other machine-learning tasks, as well as for deep packet
inspection investigation and related activities. In particular, we exploit our practical study
case to highlight, for each step of the methodology, how the relevant technologies and tools
can be combined to achieve speciﬁc design requirements in an eﬀective way.

2 Methodology

In this section, we present a methodology for the design and deployment of a cloud-based
emulation environment to support cyber-security testing, analysis and training activities.

The main goal of our methodology is to drive designers and system analysts in the creation
of a virtualized environment that is able to reproduce a target networked physical environ-
ment, which will be used as a testbed. The virtual environment will be deployed in a private
cloud (leveraging IaaS frameworks) built on dedicated hardware. Our approach is articu-
lated in three main activities: (i) Virtual Environment Infrastructure setup (i. e., how to set
up a private cloud based on the physical infrastructure), (ii) Virtual Network Design and
(iii) Data Collection from a deployed testbed. We will use the term virtual and emulation
environments interchangeably to refer to the enabling infrastructure and emulation/virtual
testbed to mean the reproduced network within it.

Virtual Environment Infrastructure Setup. This activity takes care of the design and
deployment of the virtual environment on dedicated hardware. It is articulated along the
following tasks: (a) select a cloud management platform to manage the storage and compute
resources provided by the physical infrastructure, (b) deﬁne storage management policies for
the virtual machines templates repository, in a way to foster data locality at instantiation
time and fault tolerance and ﬁnally (c) design and deployment of the virtual communication
infrastructure. Section 2.1 discusses this activity, suggesting technological options to support
each task.

Virtual Network Design This activity has the objective, starting from a reference network
speciﬁcation, to create a testbed conﬁguration, i. e., all the metadata needed to deploy its
virtual counterpart. This can be done main in two ways: (i) by manually deﬁning the char-

3

Figure 1: Overview of the Emulation Environment infrastructure. An example conﬁgura-
tion of the communication layer is reported, making use of three OVS bridges, namely br1
(internal lans), br2 (DMZ service lans) and br3 (external lans).

acteristics of the required network or (ii) by gathering information from an existing network
in order to clone it or use it as starting point. In the following, we will consider the second
and most challenging case. To this aim, we will discuss the following necessary tasks: (i) net-
work topology identiﬁcation,(ii) OS detection, and (iii) active service discovery in Section 2.2.

Data Collection This activity is responsible of ensuring proper handling of the experimental
data (in the form of traﬃc captures) generated by the emulated testbed, and is reported in
Section 2.3.

2.1 Virtual Environment Infrastructure

In the following, we provide an overview of the main steps required to build up an em-
ulation environment stack that ﬁts our purposes of network emulation and cyber security
experimentation.

2.1.1 Private cloud Setup

Many IaaS platforms exist, and each one has its speciﬁc characteristics, furthermore, some
platforms are constantly developing. The two major open-source ones by market penetration
are OpenStack and OpenNebula, currently widely used by many people around the world [20,
In order to set up our private cloud environment, we compared them taking into
19].

4

oned	–NodeNoned	–node2Virtual	Routeroned	–node1+	SunstoneInternalAreabr1:	10.2.0.0/16DMZ	Areabr2:	10.2.0.0/16ExternalAreabr3:	10.3.0.0/16...br3br1br2ImagesExt.	FirewallIDSFirewallVIRTUAL	ENVIRONMENT	PHYSICAL	INFRASTRUCTURESwitch	1Switch	2cablesaccount the following features, namely Internal Organization, Software Deployment, Storage,
Networking, Hypervisors and Governance. Due to lack of space, the detailed comparison
with respect to the selected features is reported in Appendix A. After the analysis and
experimentation with both, we found OpenNebula’s solution both enterprise-ready and easy
to use and deploy, thus being a good ﬁt for any organization. We chose OpenNebula as our
IaaS for our case study (Section 3).

2.1.2 Storage and Distributed File System

In a typical real environment, machines run various operating systems. On all cloud plat-
forms, VM types have a corresponding notion of template 1: a collection of data (i. e., a set of
disk images) and metadata (i. e., no. of associated cores, RAM, network interfaces, booting
order, context information, etc).

Thus, typically when setting up an emulated environment, a proper template repository
must be maintained, i. e., a physical area where templates are stored, so that they can be
easily browsed and recalled at VM instantiation time. However, VMs can be instantiated
on all physical nodes in the cluster, so a centralized repository would result in heavy load
on the cluster (especially as such templates can be large, typically in the order of 20-100
Gbytes). Therefore, a distribution or replication strategy could be implied.

GlusterFS (later, Gluster), is a scalable, distributed ﬁle system that aggregates disk
storage resources from a pool of trusted storage servers into a single global namespace. It
is used in production at thousands of enterprises spanning media, healthcare, government,
education, web 2.0, and ﬁnancial services [18]. A gluster volume is a logical collection of
export directories (called bricks) located on the various servers of the pool. Various types of
volumes are supported, and can be chosen depending on the distribution criteria that ﬁts a
speciﬁc use-case. To exploit data-locality at VM instantiation time, a good choice is setting
up a Replicated GlusterFS Volume

Apart for the inherent data redundancy, this solution also oﬀers reliability as in case
of a brick failure, the data can still be recovered from its replicas. Gluster supports LVM
Volumes as mount points, so that multiple disks can be used for each server and regarded
as a single volume.

Other options include Lustre and Ceph. OpenNebula storage is structured around the
Datastore concept, i. e., any storage medium to store disk images. It makes use of a Datastore
called Images, that points to the GlusterFS mount point, plus, other two (non replicated)
datastores: System, containing the instantiated machines data and Files & Kernels to store
plain text ﬁles such as scripts.

2.1.3 Communication layer

In virtual environments, VMs are connected to virtual networks (VLANs) via virtual network
interfaces.

1Templates on OpenNebula documentation:

http://docs.opennebula.org/5.4/operation/vm_

management/vm_templates.html

5

IntraLAN communication refers to interactions among VMs in the same VLAN, while in-
terLAN refers to cross VLAN communication. The VLAN deployment and communications
are managed in ﬁrst instance by the IaaS platform. The Layer 3 communication between
diﬀerent VLANs is made possible by Virtual routers (VR), another type of VM that works
just like a physical Layer 3 router, thus allowing for the deﬁnition of speciﬁc routing tables,
gateways, etc.. A VR has one network interface in each VLAN that it manages. However
VMs might be deployed on diﬀerent physical servers, therefore VLANs can span diﬀerent
nodes, that are in turn connected via physical switches. Virtual switches are a key compo-
nent in virtual environments, as they provide Hypervisors with connectivity between virtual
interfaces of the VMs and the physical interfaces of the various nodes. They are software
modules running on all nodes that maintain a MAC addresses database, to keep track of the
VM addresses in the various LANs, process incoming input frames from physical interfaces
and forward them based on the database.

For the intraLAN and interLAN communication the two most commonly used virtual

switches are Linux Ethernet Bridge and Open vSwitch.

Linux Bridge is a layer 2 virtual device, integrated in Linux OS. It mainly consists of
four major components: i) a set of managed physical interfaces, ii) a control plane managing
the Spanning Tree Protocol (STP) (for network loops prevention), iii) a forwarding plane
used to directs traﬃc to the appropriate interfaces based on mac address.

Open vSwitch (OVS) has been around since 2009 but hasn’t been a contender to Linux
Bridge before 2014. OVS provides the same functionality, plus other Layer3 functions.
Licensed under the open Source Apache 2.0, it implements a virtual multilayer network
switch, designed to enable eﬀective network automation through programmatic extensions,
while supporting standard management interfaces and protocols such as NetFlow, sFlow,
SPAN, RSPAN, LACP and VLAN tagging. OVS is used in multiple products and testing
environments.
Summing up the diﬀerences between the virtual switches, OVS is targeted at large multi-
server virtualization environments, so it is focused on logical abstraction and management.
The Linux bridge is fast and reliable, but lacks many control features. IaaS like OpenStack
and OpenNebula have started using OVS as default conﬁguration in their networking frame-
work. We also recommend the choice of OVS and assume this as a conﬁguration choice in
the following.

Multiple virtual switches (that in OVS are referred as bridges) can be deployed to separate
groups of VLANs logically, e. g., internal lans versus others providing external services. Each
VLAN is conﬁgured using the IaaS to sit on a single, speciﬁc bridge, as VMs receive their
network address through the corresponding bridges (that, instead, have no address).

2.1.4 Physical Infrastructure Conﬁguration

Typically, the required physical infrastructure for the deployment of a Virtual Environment
infrastructure consists of a number of physical servers (or nodes), each one with its own

6

speciﬁcations, and a number of physical switches, used to connect them (Figure 1). The
servers run the IaaS software: in the particular case of OpenNebula, a “master” node must
be conﬁgured to host the OpenNebula daemon, oned, and the Sunstone Frontend, that
communicates with oned, via XML-RPC APIs. Other nodes just need to have the Hypervisor
installed (package opennebula-kvm on Debian based systems).

If OpenNebula’s Image Datastore is managed via GlusterFS, the servers must be con-
nected through a switch (we will call this the “backbone” network): when new templates
containing VM datastores are added to the master node, the GlusterFS daemon will start
ﬁle transfers to keep replicas updated. To speed the sync between the replicas the band-
width must be maximized. This translates to using high-end network interfaces on both the
servers and the switch (i.e., use at least 10Gb ports). If this is not possible, a bandwidth
improvement can be made via interface bonding.

Linux bonding driver provides a method for aggregating multiple network interfaces into
a single logical ”bonded” interface. In fact, the bonding allows combines several network
interfaces into a single link, providing either high-availability, load-balancing, maximum
throughput, or a combination of these. There are several bonding modes. One of them is
mode 0 (balance-rr). In this mode, packets are transmitted in sequential order on the bond
interfaces.
Furthermore, to enable VM communication via OVS bridges, the servers must be connected
to a switch using one physical port per bridge. It is a good practice to use a separate switch
for the backbone network and for OVS bridges, in order to physically separate business traﬃc
pertaining to the emulated environment, and communication between the replicas.
All the logical pieces comprising the Emulation environment are resumed in Figure 1.

2.2 Virtual Network Design

As previously stated, one of the objectives of an emulation environment is obtaining a sep-
arate, virtual and arbitrarily modiﬁable copy of an existing network for conducting analysis
and tests. In order to do that, fundamental inputs are the Network and Vulnerability In-
ventories and communication rules. Such inputs are commonly available in large enterprise
networks as they are outputs of a continuous monitoring process and can be used to trigger
cyber attacks detection and response processes. It is important to note that the eﬀectiveness
of the monitoring, detection and response activities depends on the accuracy of the collected
inventories.

Inventory collection tools, such as GFI LANGUARD Network Scanner, typically require
an intrusive approach, where monitoring agents are preinstalled on each host system. Fur-
thermore, such as for the case of LANGUARD, they usually require expensive licenses. For
these reasons in the following we will direct our choices towards software tools that are free
and allow to perform this phase in a non intrusive way.

Instead, when this information is not available, such inputs can, in principle, be collected

7

manually by interviewing a network administrator that lists all the requirements and details
of the desired virtual network or by collecting them starting from a real physical network.

2.2.1 Network topology identiﬁcation

This task allows to obtain a logic schema of the network infrastructure of the reference
network in terms of ISO/OSI layers 2 and 3, i.e., of how many subnetworks it is composed,
how they are separated using network devices (e.g., router, hubs, switches), as well as the
presence of ﬁrewalls and their relative conﬁguration rules.

Common approaches for network discovery rely on distributed traceroute monitors [2]:
however, the accuracy of this approach strongly depends on the observability of the network.
Overall, this phase is the hardest to automate, as in order to be accurate it relies heavily
on pre-existing knowledge about the reference network, such as the one coming from Network
Administrators and Network schemes.

In the following, we make the assumption that the information about the number and
composition of network LANS, as well as the network devices, is known a priori or can be
determined by querying the network administrator.

2.2.2 Host and OS Detection

Once that the network topology is known, an important step in the reference network map-
ping concerns identifying the computer hosts deployed in the network, to be able to replicate
their presence in the virtual testbed. To this end, the primary information required is dis-
covering active hosts on the network and their OS.

OS detection is the process of determining the OS of a remote computer [11]. When this
is accomplished in a non intrusive way (i. e., , without deploying agents), it is achieved by
observing the external behavior of the remote host, mostly in terms of network traﬃc, in
a way to derive what is called an OS ﬁngerprint 2, adopting either an active or an passive
approach.

Active OS ﬁngerprinting requires sending specially crafted packet probes to the system
in question, aiming at eliciting a distinguishable responses from diﬀerent OS’es, derived from
the implementation dissimilarities of their communication protocols [8]. System responses
from this active probing are then used to generate the ﬁngerprint and fed into a classiﬁcation
model. Passive OS ﬁngerprinting involves sniﬃng network traﬃc inside a network segment
and matching known patterns to a table of pre-established OS identities. In general, active
scanning allows for higher accuracy [15], and passive is chosen only when a given degree
of stealth or anonymity of capture is of concern(e.g., when aiming at performing malicious
activities).

Tools The most popular active network scanner is Nmap [7] , an open source tool designed
for network exploration and security auditing. Nmap uses raw IP packets in novel ways to

2a 67-bit signature describing the host behavior.

8

determine what hosts are available on the network, what services (application name and
version) those hosts are oﬀering, what OS version they are running, what type of packet
ﬁlters/ﬁrewalls are in use, and dozens of other characteristics. Its output is composed of a
list of scanned targets together with supplemental information on each, depending on the
options used. Another tool oﬀering OS detection capability is OpenVAS, most known for
its value as a vulnerability assessment tool (see Section 2.2.3). Let us note that each of
these tools has its own characteristics and main target applications. As a consequence, the
produced output will satisfy diﬀerent level of quality depending on the considered attributes.
To improve the overall quality (in terms of accuracy of the scan operation) it is mandatory
to integrate several scanners combining their output. Due to the lack of space, we postpone
to Appendix B our proposal for combining results coming from Nmap and OpenVAS.

2.2.3 Active service discovery and vulnerability surface identiﬁcation

Determining what services are running on the network hosts is of primary importance to
allow for an adequate mapping of the reference network. In particular, for cyber security
purposes, what is of interest is understanding the hosts’ exposed vulnerability surface, i.e.
identifying which running services have an associated open port, and are thus, potentially
directly exploitable.
A typical example is identifying DNS and HTTP servers are running and their respective
versions. An accurate version number is key in determining which exploits a host is vulner-
able to.
Vulnerability scanning tools usually produce a detailed report with the severity level of every
vulnerability detected.

Tools OpenVAS is the open-source spin-oﬀ of the popular security scanner Nessus, and
provides a modular architecture composed of a scanning agent and a manager with a pow-
erful interface. The security scanner, is accompanied with a daily updated feed of Network
Vulnerability Tests (NVT). Being one of the most powerful free security scanners, supports
thousands of vulnerabilities and oﬀers false positive management of scanning results. How-
ever, being a free tool it suﬀers from limitations with respect to its paid counterparts, such
as Nessus, Nexpose or LANGUARD, such as limited DB, and cumbersome conﬁguration.
Nmap is also capable of discovering open ports and associate services.

2.2.4 Testbed Conﬁguration

In this section we summarize the metadata needed in order to deploy our testbed, derived
from the phase of Virtual Network Design. It is composed of Hosts conﬁguration and Net-
work conﬁguration, consolidated in the Testbed Conﬁguration (Figure 2).

Hosts Conﬁguration Each host identiﬁed from the reference network is associated to
an OS template in the Images Datastore, a list of installed services CPE (with optional
version) and a list of network interfaces. Optionally, a host-speciﬁc synthetic behavior can

9

be embedded by deploying appropriate software code (see for example Section 4.1).

Network Conﬁguration All network LANS will have a corresponding VLAN conﬁguration,
assigned to OVS Bridges also based on the data collection requirements (see Section 2.3). All
layer 3 network devices will have a corresponding Virtual Router conﬁguration, with their
corresponding interfaces (see Section 2.1.3), physical Firewalls will correspond to Virtual
Firewalls.

This information may be stored in a database or other means, if automation of the
deployment is of concern. Most IaaS platforms oﬀer their APIs to allow for automation of
deployment to various extents. OpenNebula oﬀers a centralized XML-RPC interface that
can be used to entirely manage the emulated testbed in an automatic way, such as creating
VLANs, instantiating VMs, etc.
In Section 3 can be found an example on how to fully
automate the deployment of a test environment using OpenNebula API.

2.3 Data Collection

To wrap up all information concerning the communication layer from Section 2.1.3, a testbed
contains a number of virtual machines whose network interfaces belong to a number of
VLANs sitting on a number of OVS bridges. Layer3 routing between VLANs is handled
by Virtual routers.In this section, we elaborate on diﬀerent techniques and solutions for
achieving network traﬃc data capture in an emulated testbed deployed in an emulation en-
vironment.

The problem of capturing and decoding traﬃc in high-speed network, be it physical or
In the context of a testbed for
virtual has been deeply studied (see for example [1, 4]).
cyber security experimentation, the choice of a capturing solution depends on several crite-
ria: reaction behavior, impact on network performance (delay, throughput), traﬃc rate and
architectural considerations.

Typically, inline capture mode is preferred when the probe needs to react to intrusions
(IPS) as it can alter or delete packets [5]. In particular, network taps can be implied, i. e.,
systems that are installed in network segments to monitor network devices such as routers,
ﬁrewalls, servers and hosts by means of network, application, or security analyzers, achieving
high-speed full duplex capture with no data loss. Inline capture mode might alter network
performances due to packet processing delays, yet the analysis capacity of Next-Generation
Intrusion Systems (NGIPS) in inline transparent mode has now reached speeds higher than
the physical link, allowing for extremely low latencies [9]. However, inline taps are only
suited to monitor a point-to-point link, thus not being a viable choice when the full data
capture is of concern. Moreover, an inline probe failure can induce network failures.

On the other hand, passive mode is stealthier and does not alter the performances or the
availability of the monitored network. A cheap option is capturing on the hardware switches,
yet VM-to-VM traﬃc that does not pass over a physical wire (e.g., if they are on the same
VLAN on the same server) would not be captured.

10

Depending on the capturing requirements, thanks to its high ﬂexibility and capabilities

OVS can come to rescue.

As mentioned in Section 2.1.3, OVS bridges can help distributing the VLANs in logic
compartments; this choice can also be made with data collection purposes in mind: indeed
deploying the VLANs on diﬀerent bridges allows the sniﬃng in speciﬁc parts of the envi-
ronment. However, even if the physical interface associated to each OVS bridge could be
monitored, the issues related on same-node traﬃc is still present.

When internal traﬃc of the VLANs is of concern (as, for example, if full capture is de-
sired), OVS supports port mirroring capabilities (with the SPAN and RSPAN protocol) [12].
For each network to be monitored, SPAN allows to mirror the traﬃc from all network inter-
faces toward a speciﬁc output port (the SPAN port). To achieve full capture the best option
is to have a SPAN port for each bridge, for each physical node. Span output ports can be
for example a network interface of a “Sniﬀer” VM, where the traﬃc is then collected to be
later analyzed.

However, this solution can potentially suﬀer from bandwidth limitation and data loss, as

the full traﬃc is replicated and sent to the sniﬀer.

3 A Case Study: DIAG Department

As a candidate for a case study, we chose the network of the Department of Computer,
Control, and Management Engineering Antonio Ruberti (DIAG). In particular, we followed
the methodology described in Section 2 and set up an emulation environment based on
OpenNebula, using OVS for the connection layer, and GlusterFS for mantaining a replicated
Images Datastore, and deployed an emulated testbed.

The network topology of the department consists of seven subnets, that we will call LAN1,
LAN2, LAN3, LAN4, CED, DMZ INT and DMZ EXT. The ﬁrst three contain personal
workstations (with a few exceptions of servers) of research staﬀ belonging to various research
areas, like Computer Science, Automation and Control and Management Engineering and
the fourth pertains to administrative staﬀ. Furthermore, CED contains servers and compute
nodes used by researchers to perform their work, while DMZ INT hosts internal services,
such as SMB or SFTP and ﬁnally DMZ EXT hosts servers that expose services towards
Internet (such as a WWW server and an SVN repository).

The following sections discuss each implementation step of the methodology in the context

of this case study.

3.1 OS Detection and Vulnerability Discovery

We performed OS detection following the hybrid approach of Appendix B after executing
Nmap and OpenVAS scans of each LAN. Figure 3 shows the distribution of the diﬀerent OS
Families for each LAN, fot a total of 215 hosts.

11

Figure 2: Overview of the emulation environment testbed deployment and experimentation
phases.

12

Network Traﬃc CollectionEmulation environmentBridgesNetwork layoutVLANS, RoutersHost-speciﬁc Init scriptsSunstone Env Automatic DeployXML-RPC NetworkHostsBehavior 1Testbed ConﬁgurationInstalled applicationsInstalledOSOS detectionService & Vulnerabilities IdentiﬁcationReference network`Figure 4: Top 5 installed services by percentage of installations in the various LAN of the
reference network.

Figure 3: OS Detection results.

Since we are interested in mapping each host to a VM Template, we need accuracy
at the OS Generation level. Excluding all entries belonging to the Other Family (mostly
network devices, ﬁrewalls, access points, etc.), we could unambiguously determine the OS
Generation for about 60% of the host virtual machines, while relying solely on Nmap we
could map only about 25% (mostly corresponding to Windows machines, for the reasons
expressed in Section 2.2.2, while with OpenVAS alone we reached 45% overall. Unfortunately,
due to problems with the KVM Hypervisor, we couldn’t manage to emulate correctly the
OSX Machines, and resorted to only keeping Windows and Linux VMs (the speciﬁc OS
Generations will be detailed in the following).

OpenVAS and Nmap were also implied in the discovery of running services as well the
known vulnerabilities for each host. Figure 4 reports the top occurring services found in the
various LANs.

3.2 Physical infrastructure

Table 1 reports hardware conﬁguration of our Virtual Environment infrastructure. The

13

44% 14% 8% 6% 5% opensshapachesambapythonmysqlLAN115% 8% 8% 4% 4% opensshapacheemwebcupsgsoapLAN211% 7% 4% 4% 4% iisopensshwebadmineclipsemysqlLAN367% 44% 22% 22% 11% opensshtightvnccupsmysqlapacheDMZ_INT13% 10% 7% 3% 3% afp_serverkerberosopensshapacheemwebLAN465% 19% 19% 15% 8% opensshsfcbtightvncapachecupsCED91% 64% 32% 32% 27% opensshapachetightvncmysqlwebminDMZ_EXT10724231618980%20%40%60%80%100%LAN1LAN2LAN3LAN4CEDDMZ_INTDMZ_EXTOS DetectionLinuxWindowsMac OS XOtherTable 1: Hardware Speciﬁcations

Type

Name

Server

2× Cisco UCS C240
M4S

Switch

Cisco Catalyst 2960XR

Cisco Catalyst WS-C3850

Speciﬁcations
- 2× Intel Xeon E5-2620v3
(24 total threads)
- 64 GB RAM
- 8 x HDD 1.2 TB
- APM86392/512 MB RAM
- 48 x 1GB Ports
- MIPS/4 GB RAM
- 24 x 10GB Ports

two servers, connected via the L2 switch Cisco Catalyst 2960 form the backbone network
for GlusterFS traﬃc, while the second L3 Cisco Catalyst WS-C3850 switch is used for the
emulation environment (OVS bridges)As detailed in the following, we will make use of 3
OVS bridges for networking. Each bridge uses a physical network interface on each server.
The basic templates for the VMs and the ISO Images for the various operating systems
are saved in the Image Datastore. Since we couldn’t count on high performance Ethernet
interfaces on the nodes, we adopted the Linux bonding strategy to maximize the bandwidth
on the backbone network. In particular, we used Linux kernel bonds with bond-mode 0, and
6 1GBps interfaces per node, achieving in this way a maximum bandwidth equal to 6 Gbps.

3.3 Testbed Architecture

Due to lack of resources, and also considering other VMs necessary for networking, ﬁrewalling
and generation of benign and malicious background traﬃc by external hosts, we could only
aﬀord to instantiate about half of the identiﬁed host machines. We deployed all the VLANs
necessary for replicating the reference network, using three OVS virtual bridges, namely br1,
br2 and br3. The ﬁrst bridge will be devoted to internal, business LANs, the second to
service-related VLANs, and network devices (routers and a ﬁrewall), while the third bridge
provides Internet connection to the whole testbed hosting the interfaces of all such machines
that need direct internet connection.

For VLAN Routing at Layer3, we made use of three Virtual Routers, namely VR0, VR1
and VR2. Furthermore, as per the reference network, we use a Virtual Firewall, FW. Virtual
Routers run ZeroShell v3.8.2, a lightweight Linux Distribution suited for router emulation.
For the ﬁrewall, we used PfSense 2.4.3.

More speciﬁcally, the architecture is divided in the following parts:
Internal Area: This area is composed by LAN1, LAN2, LAN3 and LAN4. The virtual
networks are available to the virtual machines through the corresponding bridge br1.
DMZ Area: CED and DMZ INT provide internal services to authorized hosts in the Inter-
nal Area through VR1. DMZ EXT contains servers exposing services to the Internet. Note
that access to services in the DMZ area is allowed by well deﬁned ﬁrewall rules speciﬁed on

14

Figure 5: Testbed network architecture: The VLANs arrangement in L2 OVS bridges is
highlighted using blue boxes. Installed services on service LANs are also shown. Reported
package versions were automatically installed in the testbed, original ones from the reference
network are in parentheses.

FW.
External Hosts: This VLAN, as well as Attackers, is used to simulate a portion of the
Internet, made by hosts that will connect to services from the DMZ EXT VLAN.
Attackers: The Attackers network, hosts malicious external attackers that want to exploit
or attack hosted services (Section 4.2).

DMZ Area, Attackers and External Hosts VLANs are deployed on bridge br2. Table 2
summarizes the list of VLANs, networks addresses and number of hosts, with installed
operating systems and related public and private IPs and services installed in the service
LANs. Note that both External Hosts, Attackers and servers in the DMZ EXT have ranges
indeed we replicated a small portion of the internet inside our
in the public IPv4 space:
testbed.

In order for the communication from such networks to correctly reach the DMZ EXT
(instead of ﬂowing towards the Externet VLAN), a static route was conﬁgured on VR2 to
direct traﬃc with destination inside DMZ EXT through VR2’s interface on FWVR2.

Thus, traﬃc from such external hosts directed to DMZ EXT will be routed by VR2, us-
ing the static route, and all the corresponding responses will return back to the originating
hosts, since VR2 knows both External Hosts and Attackers through direct interfaces.

Testbed deployment The deployment of all VLANS, VMs, Virtual Routers and Firewall com-
ponents was completely automated using a Python based module via the Python bindings
of the XML-RPC APIs exposed by OpenNebula.

In order to provide each machine with the appropriate services (with version) determined
in the vulnerability surface identiﬁcation phase, we created startup scripts according to each
operating system family, that we feed with the services to be installed, so that they are

15

LAN2 : 10.1.11.0/24VM1VM5…LAN1 : 10.1.10.0/24VM1VM20…LAN3 : 10.1.8.0/24VM1VM8…LAN4 : 10.1.12.0/24VM1VM4…VR0.1.1.1.1OVS bridge:br1CED: 10.2.20.0/24VM1VM4…VM1VM4…WWWSVNDMZ_EXT: 200.0.0.0/26DMZ_INT: 10.2.9.0/24VR1.1.2FWVR0: 10.2.112.0/30FWVR2: 10.2.114.0/30.2.1.1FWVR1: 10.2.113.0/30.1.2.1.1OVS bridge:br2VR2VM1…Attackers: 1.2.3.0/27VM1VM2VM3.1Ext. Hosts: 80.0.0.0/8Externet:192.168.60.0/24OVS bridge:br3.1LANIPNameServices10.2.20.2ced1openssh:7.2p210.2.20.3ced2openssh:7.2p210.2.20.4ced3openssh:7.2p210.2.20.5ced4openssh:7.2p2apache2:2.2.14DMZ_INT10.2.9.2lsrvopenssh:6.6.1p1tightvnc:1.3.9	(1.2.9)samba:3.6.310.2.9.3int1openssh:6.6.1p1	(5.1)tightvnc:1.3.9	(1.2.9)vsftpd:2.0.710.2.9.4int3openssh:6.6.1p1	(4.2)tightvnc:1.3.9	(1.2.9)xorg:1.6.510.2.9.5int2openssh:6.6.1p1	(5.1)tightvnc:1.3.9	(1.2.9)DMZ_EXT200.100.0.3wwwdrupal:7.1mysql:5.5.54	(5.7.22)openssh:7.2p2	(6.6.1)apache2:2.4.18	(2.2)php:5.6.36	(5.3.17)200.100.0.8svnapache2:2.4.7(2.2.10)openssh:6.6.1p1	(5.1)mysql:5.5.54	(5.7.22)CEDLAN

Network

Table 2: LANs Operating Systems and IPs
OS
Ubuntu 12.04
Ubuntu 14.04

LAN1

10.1.10.0/24
20 hosts

LAN2

10.1.11.0/24
5 hosts

LAN3

10.1.8.0/24
8 hosts

LAN4

CED

DMZ INT

DMZ EXT

Ext. Hosts

Attackers

10.1.12.0/24
4 hosts

10.2.20.0/24
4 hosts
10.2.9.0/24
4 hosts
200.100.0.0/26
2 hosts

80.0.0.0/8
3 hosts

1.2.3.0/27
2 hosts

IPs
16
4, 22
6-10, 13
15, 17, 19
18
12
11, 14
21
20
.3
6
5
2, 4
.6
2, 4
5
3, 7, 9
8
2
3
5
4
5
2-4

Ubuntu 16.04

Ubuntu 16.10
Debian 7
Windows 10 (1803)
Win. Server 2008 SP1
Win. Server 2012 R2
Ubuntu 16.04
Windows 7
Windows 10 (1709)
Windows 10 (1803)
Ubuntu 14.04
Windows 7 SP1
Windows 10 (1709)
Windows 10 (1803)
Win. Server 2008 SP1
Windows 7
Windows 7 SP1
Windows 10 (1709)
Windows 10 (1803)
Ubuntu 10.04
Ubuntu 16.04

OpenSUSE 11.4

2-5

Ubuntu 14.04
Ubuntu 16.04

Ubuntu 16.10

8
3
20.40.2,
100.3.50,
20.55.21

Kali Linux 4.15.0

2, 16

16

fetched and installed automatically at ﬁrst machine boot.

A particular eﬀort must be devoted for the installation of the services, starting from the
CPEs. Currently, there is no available data source that allows to match application CPEs
to actual packages. For this reason the mapping must resort to either a manual association,
or a tentative automatic association based on similarity.

In particular, we resorted to manually mapping application CPE to software packages,
that then were installed automatically using the closest version available in the repositories.

4 Dataset

As part of our work, we gathered a realistic dataset, comprising of benign and malicious
traﬃc, that has been processed to obtain feature-rich labeled attack ﬂows, useful for cyber
security analyses such as training IDS and IPS classiﬁers and other machine-learning tasks,
as well as for deep packet inspection investigation and related activities. In particular, the
following sections elaborate on our benign proﬁle agents and the a number attack scenarios
that we considered.

4.1 Benign Traﬃc Agents

To form the main part of our dataset, out aim was having realistic background traﬃc that
comprises a number of widely used protocols, namely HTTP, HTTPS, SSH, SMB and SFTP,
to act as background noise with respect to a number of traﬃc ﬂows corresponding to spe-
ciﬁc instantiated attack patterns. To this end, we used a number of software agents written
in Python to provide close-to-realistic user behavior for the VLANS from LAN1, through
LAN4.
The agents’ software consists in a number of traﬃc simulation jobs executed by worker
threads managed by a scheduler, which feeds the threads following a speciﬁc traﬃc proﬁle
shown in Table 3), and assigned to each machine. In particular, on average, each job will be
executed during the interval (Start, End) deﬁned in the proﬁle, a number of times depending
on the Frequency proﬁle. The scheduler will randomize the time between the execution of
each job to prevent the traﬃc from looking too synthetic. Furthermore, all jobs have an
initial setup phase that always entails a degree of randomness in all conﬁgurable parameters,
as well as multipliers for randomly delaying each request.

In the following, we summarize the behavior of the simulation jobs for the diﬀerent pro-

tocols supported by our agents.

HTTP/HTTPS generator: An HTTP job, “browses” the internet starting at pre-deﬁned
root urls and simulates clicks by randomly following links on pages until a pre-deﬁned click
depth of 7 is met. Wait time between HTTP requests is chosen at random uniformly be-
tween 5 and 10 seconds. The number of total http requests is given by a random integer
generated uniformly between 1 and of 20. The value of the parameters has been chosen

17

Hosts
All

Type Start End Freq
15
WEB
60 All but h2
SMB1
180 All but h2
SMB2
60
SSH3
240
SSH4
60
SSH5
60
SSH6
60
SSH7
240 All but h2
SFTP

9:00
9:00
14:00
9:00
9:00
9:00
9:00
9:00
9:00

18:30
18:00
18:00
18:00
18:00
18:00
18:00
18:00
18:00

h1
h2
h3
h4
h5

Servers
*
lsrv
lsrv
ced2
lsrv, ced1-3, int1-3
ced3
int1
int2
int3

Table 3: Traﬃc generator proﬁles for the various supported protocols. All, refers generically
to all hosts in the four business VLAN1-4. The reported custom hosts correspond to: h1
(10.1.10.9), h2 (10.1.10.8), h3 (10.1.10.17), h4 (10.1.10.13), h5 (10.1.11.3).

following previous research [21, 22], that found that only about 5% of sessions are longer
than 20 requests and the average session length is about 7. Note that to generate request
that are compatible with the associated OS, each agent will perform requests with a speciﬁc
User-Agent that is automatically identiﬁed by the software based on the machine OS.

SSH generator: An SSH job connects to a host with the provided credentials using SSH
and start sending commands selected at random from a list of supplied commands, keeping
the connection open until a given period of time generated at random. The commands list
is generated at startup time from a list of common commands such as ‘ls’, ‘cd’, ‘cat
/var/log/messages’, etc.

SMB generator: A SMB job connects to a SMB share with the provided credentials
and uploads and downloads a number of ﬁles or directories. The ﬁles are randomly gener-
ated and their size are controlled by an optional size multilier of the default size of 8192 bytes.

SFTP generator: An SFTP generator connects to a host with the provided credentials
using the SSH subsystem SFTP. It then starts uploading and downloading ﬁles provided
with the same parameters of the SMB generator.

The agents’ code has been deployed at machine instantiation time using init scripts, and
the resulting processes are conﬁgured as Systemd (Ubuntu 14.04+, Debian) or Upstart Jobs
(Ubuntu until 14.04), and using Task Scheduler on all Windows OS, conﬁgured to run as
automatic startup jobs and to be automatically restarted in case of failures.

4.2 Attack proﬁles and scenarios

To give a practical demonstration of the power and ﬂexibility of our emulated environment,
we performed various cyber attacks in the testbed, covering a diverse set of attack scenarios.
In particular, starting from common attack taxonomies, 4 attack proﬁles were selected, and

18

executed by using related tools and codes.

Brute Force: A brute force attack can target diﬀerent types of services. The main goal of
this attack is obtaining private information, i. e., a password, or private key, by testing many
input combinations through a trial and error process, until the private information is ﬁnally
gathered. This attack is one of the easiest to perform and detect, given the huge amount
of traﬃc generated by the probing process. In particular, we performed a Dictionary-based
brute-force attack to an OpenSSH daemon running on a Linux Machine.

Heartbleed: Heartbleed is a vulnerability of the OpenSSL implementation of the SSL
protocol. The name comes from a speciﬁc function introduced in version 1.0.1, called Heart-
Beat: this extension allows the clients to test and keep a connection alive without the need
of re-negotiating it from scratch. Thus, suﬀering from a buﬀer-over-read vulnerability, it
allows the clients to read more data than expected, possibly leaking sensitive information.
To perform this attack, an SSL connection must be instantiated with a vulnerable server,
sending a malformed Heartbeat request.

Web Application Attack: Attacks on web applications may target diﬀerent services. The
OWASP Top 10 3 shows the most common pattern for this speciﬁc attack surface, where
the targets span from simple websites to complex CMS. As a practical case, we targeted
the Drupal CMS version 7.31, vulnerable to a Remote Command Execution (RCE) exploit
called drupalgeddon2 (CVE 2018-7600). With this attack, it is possible to obtain control of
the attacked server by exploiting a sanitization vulnerability in the AJAX requests allowing
the attacker to force the vulnerable server to execute arbitrary malicious code.

LAN Attack (+ Ransomware Deployment): This attack pattern was introduced to
represent an insider threat, i. e., a malicious user who has access to the organization’s internal
network. We conducted a lateral spreading attack by exploiting an SMB vulnerability (CVE-
2017-0144) aﬀecting various Microsoft Windows versions, as made by the Eternalblue exploit
used in 2017 for spreading the DoublePulsar backdoor. In this case, the attack started from
an intruder Kali Linux, connected to the LAN1. The attack begins with a series of Nmap
scans (alive, services) towards LAN4, where a victim is found having a vulnerable Windows
7 system. The attacker installs a reverse TCP shell via Eternalblue and using a local HTTP
server installed on the attacker machine downloads and runs the famous WannaCry software
on the victim machine.

All the speciﬁc attack steps as well as the corresponding timeline will be publicly released

along with the full capture dataset.

3https://www.owasp.org/index.php/Top_10-2017_Top_10

19

Name
Heartbleed
Bruteforce
Web
LAN
PortScan

Attacker
1.2.3.3
1.2.3.3
1.2.3.16
10.1.10.2
10.1.10.2

Victim
200.100.0.8
200.100.0.3
200.100.0.3
10.1.12.2
LAN4

Day
25
25
27
27
27

Start
15:01:51
16:03:37
10:39:05
12:21:30
12:07:32

End
15:06:08
17:59:13
11:06:50
12:40:57
12:21:30

Time (s) # pkts Avg. pps Avg. size (B)
720
137
581
1464
61

144
504641
554
2816
24388

256.3
6935.5
1665.2
1167.5
838

0.6
72.76
0.33
2.41
29.10

Table 4: Time and statistics for the attack instances deployed in the testbed. Times are in
CEST timezone

4.3 Data Collection

Our data collection objective was gathering full traﬃc capture for all VLANs pertaining to
the department network infrastructure, namely LAN1 through LAN4, FWVR0, FWVR1, as
well as the service LANs CED, DMZ INT and DMZ EXT.

Following the directions discussed in Section 2.3, to perform the full traﬃc capture, we
deployed a sniﬀer VM for each pair (n, b) (where n is a node and b is an OVS bridge) having
one network interface on each VLAN v belonging to bridge b. Each of such port has been
conﬁgured to be the output port of a SPAN mirror, that receives traﬃc from all interfaces
of VLAN v, belonging to machines deployed on node n. We thus captured traﬃc from all
sniﬀers network interfaces using the Linux network traﬃc capturing tool tcpdump.

The data capturing period started at 14:30 on 25 July 2018 and continuously ran for an

exact duration of 4 days, ending at 14:30 on 29 July 2018.
As described in the previous section, it contains both benign traﬃc generated with our agents
as well as malicious traﬃc ﬂows. Concerning the latter, we deployed some instances of the
attack proﬁles reported in Section 4.2 and extracted the corresponding network traﬃc from
the captures to obtain a list of attack-traﬃc subcaptures. Statistics for each attack are shown
in Table 4, i. e., involved hosts, attack timespans (start, end and duration), total number
of packets, average packets per second and average packet size in bytes. The PortScan row
refers to the Nmap scanning phase of the LAN Attack scenario. Furthermore, we processed
all attack-traﬃc captures with a ﬂow-based feature extractor [6, 17], in a way to extract 80
features for all included ﬂows, where each ﬂow corresponds to a (SrcIP, SrcPort, DstIP,
DstPort,Protocol) tuple. The extracted features are useful for training Intrusion Detection
and Prevention Systems (IDS, IPS) classiﬁers and other machine-learning tasks for cyber se-
curity analyses [17].
To summarize, the dataset 4 contains the following information:

• Full daily captures: One PCAP ﬁle per day with the full capture for each VLAN.

• Attacks: A separated PCAP ﬁles for each attack, as per Table 4, a well as a description

of the necessary steps to replicate it.

4The full dataset is available at https://www.cis.uniroma1.it/dataset_ICDCN2019, released under

the CC-BY license.

20

• Benign: A separate PCAP ﬁle with all traﬃc from LAN1-LAN2 for 26/07, containing

only benign (agent-related) ﬂows, as per Table 3).

• Features: for each PCAP ﬁle, a CSV ﬁles containing, for each extracted ﬂow, the

values for 80 traﬃc related features.

5 Conclusions and Future Work

This paper proposes a methodology for the design, development and deployment of cloud-
based emulation environments. Based on our practical experience, we provided some rec-
ommendation about technological choices to support the creation of virtual environments.
We also show the eﬀectiveness of our approach by applying our methodology to build an
emulation environment for the study case of our Department network. Finally, we show the
ﬂexibility of the deployed environment to simulate actual cyber attacks and benign traﬃc,
that we collected in a publicly released dataset containing complete network traces, enriched
with labeled features. A limitation of our work is the lack of an evaluation of the accuracy of
the information acquired from the reference network mapping phase, due to lack of precise
ground truth, which would require an intrusive approach. As a future step in this direction,
we plan to work on the deﬁnition of a strategy to measure the aﬃnity of the ﬁnal emulated
environment and the original reference network. The next step in our investigation will be
toward the deﬁnition of novel approaches to develop user behavior proﬁlers to improve the
quality of benign data generators in terms of closeness to real human behavior. Addition-
ally, we plan to investigate approaches to fully automate the installation of services, which
is currently an open issue.

Acknowledgements

This work has been partially supported by CINI Cybersecurity National Laboratory within
the project FilieraSicura: Securing the Supply Chain of Domestic Critical Infrastructures
from Cyber Attacks (www.filierasicura.it) funded by CISCO Systems Inc., Leonardo
SpA and by the INOCS Sapienza Ateneo 2017 Project (protocol number RM11715C816CE4CB).
The authors are deeply grateful to Tiziana Toni for her precious support and guidance, and
Prof. Antonio Cianfrani for useful discussions.

References

[1] L. Braun, A. Didebulidze, N. Kammenhuber, and G. Carle. Comparing and improving
current packet capturing solutions based on commodity hardware. In Proceedings of the
10th ACM SIGCOMM conference on Internet measurement. ACM, 2010.

21

[2] B. Donnet, T. Friedman, and M. Crovella. Improved algorithms for network topology
In International Workshop on Passive and Active Network Measurement.

discovery.
Springer, 2005.

[3] T. C. Eskridge, M. M. Carvalho, E. Stoner, T. Toggweiler, and A. Granados. Vine: a
cyber emulation environment for mtd experimentation. In Proceedings of the Second
ACM Workshop on Moving Target Defense. ACM, 2015.

[4] F. Fusco and L. Deri. High speed network traﬃc analysis with commodity multi-core
systems. In Proceedings of the 10th ACM SIGCOMM conference on Internet measure-
ment. ACM, 2010.

[5] C. Kreibich, M. Handley, and V. Paxson. Network intrusion detection: Evasion, traﬃc
normalization, and end-to-end protocol semantics. In Proc. USENIX Security Sympo-
sium, volume 2001, 2001.

[6] A. H. Lashkari, G. Draper-Gil, M. S. I. Mamun, and A. A. Ghorbani. Characterization

of tor traﬃc using time based features. In ICISSP, 2017.

[7] G. F. Lyon. Nmap network scanning: The oﬃcial Nmap project guide to network

discovery and security scanning. Insecure, 2009.

[8] J. P. S. Medeiros, A. M. Brito, and P. S. M. Pires. A data mining based analysis of
nmap operating system ﬁngerprint database. In Computational Intelligence in Security
for Information Systems. Springer, 2009.

[9] R. K. Meena, H. Kaur, K. Sharma, S. Kaur, and S. Sharma. Integrated next-generation

network security model. In Next-Generation Networks. Springer, 2018.

[10] D. Milojiˇci´c, I. M. Llorente, and R. S. Montero. Opennebula: A cloud management

tool. IEEE Internet Computing, 15(2), 2011.

[11] A. Orebaugh and B. Pinkard. Nmap in the enterprise: your guide to network scanning.

Elsevier, 2011.

[12] J. Pettit, J. Gross, B. Pfaﬀ, M. Casado, and S. Crosby. Virtual switching in an era of

advanced edges, 2010.

[13] C. Pham, D. Tang, K.-i. Chinen, and R. Beuran. Cyris: A cyber range instantiation
system for facilitating security training. In Proceedings of the Seventh Symposium on
Information and Communication Technology. ACM, 2016.

[14] R. Rapone, S. Alessandroni, and A. Iacomini. Cyber security software development lim-
itation: an approach to estimate the aﬃnity between an emulated and real environment
during development of software for cyber security, 04 2018.

22

[15] D. W. Richardson, S. D. Gribble, and T. Kohno. The limits of automatic os ﬁngerprint
In Proceedings of the 3rd ACM workshop on Artiﬁcial intelligence and

generation.
security. ACM, 2010.

[16] O. Sefraoui, M. Aissaoui, and M. Eleuldj. Openstack: toward an open-source solution
for cloud computing. International Journal of Computer Applications, 55(3), 2012.

[17] I. Sharafaldin, A. H. Lashkari, and A. A. Ghorbani. Toward generating a new intrusion

detection dataset and intrusion traﬃc characterization. In ICISSP, 2018.

[18] S. Shirinbab, L. Lundberg, and D. Erman. Performance evaluation of distributed storage

systems for cloud computing. IJ Comput. Appl., 20(4), 2013.

[19] A. Vogel, D. Griebler, C. A. Maron, C. Schepke, and L. G. Fernandes. Private iaas
clouds: a comparative analysis of opennebula, cloudstack and openstack. In Parallel,
Distributed, and Network-Based Processing (PDP), 2016 24th Euromicro International
Conference on. IEEE, 2016.

[20] G. Von Laszewski, J. Diaz, F. Wang, and G. C. Fox. Comparison of multiple cloud
frameworks. In Cloud Computing (CLOUD), 2012 IEEE 5th International Conference
on. IEEE, 2012.

[21] Q. Wang, D. J. Makaroﬀ, and H. K. Edwards. Characterizing customer groups for an
e-commerce website. In Proceedings of the 5th ACM conference on Electronic commerce.
ACM, 2004.

Investigating the querying and browsing behavior of
[22] R. W. White and D. Morris.
advanced search engine users.
In Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in information retrieval. ACM, 2007.

[23] S. Yasuda, R. Miura, S. Ohta, Y. Takano, and T. Miyachi. Alfons: A mimetic network
environment construction system. In International Conference on Testbeds and Research
Infrastructures. Springer, 2016.

23

A IaaS Comparison: OpenStack vs OpenNebula

Started as a research project back in in 2005, OpenNebula [10] provides a simple-to use
but feature-rich and ﬂexible solution for the comprehensive management of virtualized data
centers to enable private, public and hybrid IaaS clouds [20]. Storage, virtual machine, data
transfer, network management, and job scheduler are conﬁgured by OpenNebula services
managementIt allows integration with diﬀerent storage and network infrastructure conﬁgu-
rations, and hypervisor technologies.

OpenStack [16] ﬁrst appeared in July 2010, and consists in a set of software tools for
building and managing cloud computing platforms for public and private clouds. It builds
on API stacks that communicate using a queuing service. It is composed of around forty
individual components available. The storage may be deployed in diﬀerent ways and stored
locally or distributed.

Next, we provide a short discussion about the major diﬀerences between them.

Internal Organization. While OpenStack comprises many diﬀerent subprojects, among
which: Orchestrator, Ceilometer (metering), swift (object storage support), Neutron (net-
working), Keystone and hybrid support aimed at building the diﬀerent subsystems in a cloud
infrastructure (each of them with its own API interface, and separate conﬁguration eﬀort),
while OpenNebula oﬀers a single integrated, comprehensive management platform for all
cloud subsystems, with a single XML-RPC API interface..

Software Deployment. An important feature is the easiness of installation. From our
own experience OpenNebula was found easier to deploy than OpenStack because only a
machine must be conﬁgured as front-end, which will host the cloud manage, and all the
other machines will be slave nodes, thus only requiring the hypervisor daemon. OpenStack
is more diﬃcult because it requires the conﬁguration of its multiple components according
to a speciﬁc conﬁguration [20].

Storage. Storage is very important in cloud computing since all the images have to be
managed and available for users anytime. OpenStack provides a sophisticated storage system
called Swift and the images are transferred using ssh or http/s. OpenNebula does not
provide a cloud storage product, and natively supportsfrom non-shared ﬁle systems with
image transferring via SSH to shared ﬁle systems (NFS, GlusterFS, Lustre) or LVM with
CoW (copy-on-write), and any storage server, from using commodity hardware to enterprise-
grade solutions. By default it uses a shared ﬁle system, typically NFS or GlusterFS to store
all ﬁles (see also Section 2.1.2).

Networking. Both frameworks provide diﬀerent options to manage the network. Open-
Stack manages the networks in two modes: ﬂat networking and VLAN networking where
the ﬁrst one uses Ethernet bridging to connect multiple compute hosts together. In Open-

24

Nebula the networks can be deﬁned to support VLAN tagging that requires support from
the hardware switches and Open vSwitch to restrict network access with Open vSwitch.

Hypervisors. Both of them support KVM, VMWare and Xen, which are the most popu-
lar open source hypervisors. Additionally, OpenStack also supports LXC, HyperV and Xen
Server.

Open Source model and Governance Both projects release code under the liberal
Apache 2.0 license, follow a transparent development process with a public roadmap, and
have the same license agreement for new contributions. While OpenStack is controlled by a
Foundation 5 driven by vendors that also sell their enterprise-grade proprietary implementa-
tions of its subcomponents, OpenNebula instead only release a single, free, community level
dstribution, and is managed by a single organization backed by a community of developers.
We refer the interested reader to [19] where the authors perform a deep comparison of
the two projects, from the point of view of ﬂexibility, resiliency and performance. Overall,
enterprise networks can beneﬁt from OpenStack’s proprietary implementations by HP, Red
Hat and IBM that provide extended features, custom enhancements and integrations, as well
as paid customer support. However, this can also translate in lock-in phenomena that erode
compatibility and interoperability.

B OS Detection: a Hybrid Approach

Nmap’s algorithm for detecting matches is relatively simple. It takes a subject ﬁngerprint
and tests it against every single reference ﬁngerprint in Nmap-os-db, and produces a list of
OS matches (see Figure 6). When there are no perfect matches, Nmap adds an accuracy
percentage.
The same OS release may ﬁngerprint diﬀerently based on what network drivers are in use,
user-conﬁgurable options, patch levels, processor architecture, amount of RAM available,
ﬁrewall settings, and more. Sometimes the ﬁngerprints diﬀer for no discernible reason [15].
The probes and response matches are located in the Nmap-os-db ﬁle. Nmap will attempt to
identify the following parameters:

• Vendor Name The vendor of the OS such as Microsoft or Sun.

• Operating System The underlying OS such as Windows, Mac OS X, Solaris.

• OS Generation The version of the OS such as Vista, XP, 2003, 10.5, or 10.

• Device Type The type of device such as general purpose, print server, media, router,

WAP, or power device.

5https://www.openstack.org/foundation/companies/

25

OS detection is enabled by providing the -O parameter. Additionally, one can increase the
verbosity level with -v for even more OS-related details. Nmap includes several command-
line options to conﬁgure the OS detection engine.
If Nmap can’t make a perfect match
for an OS it will guess something that is close, but not exact. To make Nmap guess more
aggressively, it can be used the –osscan-guess command-line option.
The main problem with Nmap OS Detection concerns linux operating systems: given that
the os ﬁngerprinting technique mostly relies on network traﬃc features, the only observable
diﬀerences are determined by diﬀerent kernel levels, hence the impossibility of Nmap to
discriminate between diﬀerent Linux distributions. For our mapping purposes (OS and
services), we exploited a combination of Nmap and OpenVAS, which we describe next.

Most os ﬁngerprints, also have a Common Platform Enumeration (CPE) representation,
like cpe:/o:linux:linux kernel:2.6. Common Platform Enumeration (CPE) is a structured
naming scheme for information technology systems, software, and packages. Based upon the
generic syntax for Uniform Resource Identiﬁers (URI), CPE includes a formal name format,
a method for checking names against a system, and a description format for binding text
and tests to a name.

Figure 6: OS features

Sometimes Nmap gives more than one CPE for a speciﬁc osmatch, including the name,
the type, the OS family and the accuracy. A good strategy is to take all those that have
the highest accuracy for each IP. Thus, in some cases, we can have one or more CPE for an
osmatch.

However, for the emulation environment the results of Nmap are not suﬃcient, since
speciﬁcally we need the correct OS generation for each host and not only the kernel linux
version. OpenVAS tool uses diﬀerent scanning tools and it provides among other func-
tionalities, a good OS Detection. In particular, OS Detection Consolidation and Reporting
consolidates the OS information detected by several NVTs and tries to ﬁnd the best matching
OS.

For these reasons, for our OS detection task, we decided for a hybrid strategy that merges
information from these two tools. Summarized in Figure 6. The algorithm takes as input the
output from Nmap and OpenVAS and for each entry, the network host, identiﬁes the OS,
Vendor, Family and Gen like Linux, Linux, Ubuntu, 16.04 for Linux hosts and Windows,
Microsoft, 7, SP1 for Windows hosts. In this part was not considered the Microsoft Windows
XP as operating system since it is a largely not used anymore and probably it can be a false
positive. For best results, the algorithm starts with considering the output from OpenVAS
since it is more accurate with the Linux distributions. In particular, when it matches a Linux
family it can provide also the name of the distribution and its version. In the case tools have

26

Figure 7: OS Matching Flow

27

OpenVAShas CPE ?Nmap has CPE ? Use OpenVAS with versionif #cpe == 1:Fill OS, split cpe, ﬁll Vendor, Family, Genif #cpe > 1:Fill OS, split cpe, ﬁll Vendor/FamilyYesOpenVAS hasVersion ?YesNmap has CPE ?NoNmap has Vendor,Family?NoUse NmapFill OS Fill Vendor/Family YesOpenVAS is Generic ?YesYesUse Nmap and OpenVASFill OS with OpenVASif #cpe Nmap > 1: Vendor, Familyif #cpe Nmap == 1:Vendor, Family, GenNoGenericLinuxWindowsMac OS XNetBSDFreeBSDlinux:kernelmicrosoft:windowsfreebsd:mac_os_x apple:mac_os_xnetbsd:netbsdfreebsd:freebsdNoUse OpenVASFill OSFill Vendor/FamilyNoUse Nmap with version Fill OSif #cpe >1: Vendor, Familyif #cpe == 1:Vendor, Family, GenYesNmap has Vendor,Family?NoYesFAILNomore than one CPE for a speciﬁc host the algorithm will not provide the OS generation.
The following are all the possible outputs:

• Use OpenVAS with version: This output takes in consideration only the output
from OpenVAS. In particular, for each entry it checks if the CPE is present. If yes,
the OpenVAS output will be considered.

• Use Nmap with version: If an entry has not a CPE from OpenVAS the algorithm
considers the Nmap output if the last one has a CPE and if the CPE of OpenVAS is
generic, that is belongs from too general categories.

• Use OpenVAS and OpenVAS: In this part the algorithm use the OpenVAS output
for the OS, since it is not general, and Nmap output for the OS family and generation.

• Use OpenVAS: This kind of output and the last one are less accurate than previous
ones since the algorithm reaches these states when neither Nmap nor OpenVAS return
a valid CPE for an entry. The output brings only information about the OS, Family
and Vendor but nothing about the generation.

• Use Nmap: This part produces the same output as the precedent one using only the

information from Nmap.

• Fail: Sometimes, for some entry like routers or ﬁrewall there are not CPE and not even
information for the OS family and vendor. In these rare cases, the algorithm conclude
with a fail status. To overcome this case, a manual checking could be necessary.

28

