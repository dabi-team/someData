1

Determining Training Needs for Cloud
Infrastructure Investigations using I-STRIDE

Joshua I. James1, Ahmed F. Shosha2, and Pavel Gladyhsev2

1 Digital Forensic Investigation Research Group: ASP Region

Soon Chun Hyang University
Shinchang-myeon, Asan-si, South Korea

2 Digital Forensic Investigation Research Group: Europe

University College Dublin
Belﬁeld, Dublin 4, IE

Summary. As more businesses and users adopt cloud computing services, security
vulnerabilities will be increasingly found and exploited. There are many technolog-
ical and political challenges where investigation of potentially criminal incidents in
the cloud are concerned. Security experts, however, must still be able to acquire
and analyze data in a methodical, rigorous and forensically sound manner. This
work applies the STRIDE asset-based risk assessment method to cloud computing
infrastructure for the purpose of identifying and assessing an organization’s ability
to respond to and investigate breaches in cloud computing environments. An exten-
sion to the STRIDE risk assessment model is proposed to help organizations quickly
respond to incidents while ensuring acquisition and integrity of the largest amount
of digital evidence possible. Further, the proposed model allows organizations to
assess the needs and capacity of their incident responders before an incident occurs.

Key words: Digital Forensic Investigation; Incident Response; Capability Assess-
ment; Cloud Forensics; I-STRIDE; Asset-based Risk Assessment; Security Policy

1.1 Introduction

New concepts in cloud computing have created new challenges for security
teams and researchers alike [27]. Cloud computing service and deployment
models have a number of potential beneﬁts for businesses and customers,
but security and investigation challenges – some inherited from ‘traditional’
computing, and some unique to cloud computing – create uncertainty and
potential for abuse as cloud technologies proliferate.

According to a survey from Ponemon Institute [19], only 35% of IT respon-
dents and 42% of compliance respondents believe their organizations have
adequate technologies to secure their Infrastructure as a Service (IaaS) en-
vironments. The report shows that respondents believe IaaS is less secure

5
1
0
2

b
e
F
8
1

]

Y
C
.
s
c
[

1
v
1
9
1
5
0
.
2
0
5
1
:
v
i
X
r
a

 
 
 
 
 
 
2

Joshua I. James et al.

than their on-premise systems, however, “[m]ore than half (56 percent) of IT
practitioners say that security concerns will not keep their organizations from
adopting cloud services”. A drive towards cloud service oﬀerings is reiterated
by Gartner [10], who forecasts that spending on cloud computing at each ser-
vice model layer will more than double by 2016. At the same time Ernst and
Young [9] found that there is a perceived increase in risk by adopting cloud
and mobile technologies, and many respondents believe that these risks are
not currently being adequately dealt with. However, North Bridge [23] sug-
gests that conﬁdence in cloud computing is increasing, even though maturity
of the technologies remains a concern.

An increased conﬁdence in cloud computing and a drive to improve busi-
ness processes while reducing costs are leading to security of such systems
sometimes being a secondary concern. This attitude has somewhat been car-
ried over from traditional computing, which could possibly result in the same,
or similar, security challenges, such as those presented by the Computer Re-
search Association [6] in the Four Grand Challenges in Trustworthy Comput-
ing. If both security and insecurity from traditional computing are inherited
by cloud computing, both may be augmented with the increased complex-
ity of the cloud model, the way that services are delivered, and on-demand
extreme-scale computing. Each cloud deployment and service model has its
own considerations as far as security and liability are concerned. For example,
in a private, single-tenant cloud where all services may be hosted on-premise,
the risks are similar to on-premise, non-cloud hosting. The organization has
end-to-end control, can implement and target security systems, and can con-
trol critical data ﬂow and storage policies. A challenge with this model is that
the organization must have the capability to be able to create, implement, and
maintain a comprehensive security strategy for increasingly complex systems.
Several works have previously examined some cloud security concerns
[3, 12, 4, 15, 28, 7, 21]. This work, however, is concerned with an organi-
zation’s ability to assess the investigation and response capability of their
investigators considering the organization’s unique needs. Security groups,
such as the National Institute of Standards and Technology, have previously
called for digital forensic readiness to be included in incident response plan-
ning [13]. However, determining the required capabilities of investigators has
not been directly addressed. Prior works, such as Kerrigan [14] proposed a
capability maturity model for digital investigations. Such maturity models
essentially focus on assessing how standardized knowledge and processes are
in a particular organization, and how well these organizations actually con-
form to these standards. While technical capability of digital investigators is
a factor, this model does not include assessment of speciﬁc technical needs of
an organization. Pooe and Labuschange [20] proposed a model for assessing
digital forensic readiness that includes identiﬁcation of more speciﬁc techni-
cal challenges; however, this model too does not help guide an organization
in speciﬁcally deﬁning their internal digital investigation capability needs in
regards to the technical skills.

1 Cloud Investigation Training using I-STRIDE

3

1.1.1 Contribution

This work proposes a method to guide organizations in determining the tech-
nical skills needed for incident response and digital investigations that are
tailored speciﬁcally to the organization. The proposed method uses an exten-
sion of a previously known asset-based risk assessment model to help guide
an organization and prepare for digital investigations, including determina-
tion of technical training that is speciﬁcally required for investigators within
the organization.

The remainder of this paper ﬁrst discusses related prior work for assessing
risk to cloud infrastructure. After, a method is described for assessing inves-
tigation capability based on an organizational risk assessment. A case study
is then given applying the proposed model to cloud infrastructure. In-house
knowledge can be questioned based on the identiﬁcation and prioritization of
risks, and gaps in knowledge may be identiﬁed from which speciﬁed training
areas can be deﬁned. Finally, conclusions are given and potential future work
is discussed.

1.2 Assessing Risk to Cloud Infrastructure

To help in the identiﬁcation of threats, their impact on a system, potential
evidential traces and technical skill needed by investigators, an extension to
the six-category, threat categorization model – ‘Spooﬁng, Tampering, Repu-
diation, Information Disclosure, Denial of Service, and Elevation of Privilege’
(STRIDE) [26] – is proposed. The STRIDE model is a threat categorization
model that can be used to help understand the impact of a speciﬁc threat
being exploited in a system [17]. It helps to determine vectors of attack, the
impact of an attack on data, and the overall impact to the organization due to
the altered - or loss of - data. The STRIDE model has previously been applied
to probabilistic risk assessment in cloud environments [22], threat modeling
using fuzzy logic [24], among others.

James, Shosha et al. [11] previously proposed an extension to the STRIDE
model beyond risk assessment and potential exploitation results, to add the
identiﬁcation of possible investigation-relevant traces produced by the ex-
ploitation, named the “Investigation STRIDE model”, or I-STRIDE.

As shown in Figure 1.1, the I-STRIDE process is conducted by ﬁrst de-
constructing a service into its dependent components. A risk assessment is
conducted per component, and risk mitigation techniques are derived. Each
risk identiﬁed by I-STRIDE has associated investigation-relevant data sources.
When a threat to a component has been identiﬁed, an investigator may de-
termine what data is likely to be eﬀected by the threat. From this subset
of aﬀected data, speciﬁc data sources that may be of evidential value can
be identiﬁed. These potential evidential data sources may then be used for
pre-investigation planning and data targeting purposes.

4

Joshua I. James et al.

Fig. 1.1. The I-STRIDE process model for risk assessment, mitigation and investi-
gation

Determining forensic investigation knowledge required to investigate a par-
ticular threat can be modeled using risk analysis and assessment techniques.
In particular, risk assessment models such as Root Cause Analysis (RCA) [25]
can be used to help identify required training. Broadly speaking, RCA is used
to identify the root cause of an event that causes a phenomena of interest.
Thus, RCA in combination with the proposed I-STRIDE model can be used
as a basis to identify training related to the investigation of identiﬁed threats
(Figure 1.2).

Utilizing methodologies such as RCA beneﬁts not only the process of train-
ing identiﬁcation, or gaps in knowledge, but also training eﬃcacy. When con-
sidering investment in training, organizations attempt to determine whether
a speciﬁc training meets their unique needs. By identifying gaps in knowl-
edge related to prioritized organizational risks, training can be more focused
at areas the organization speciﬁcally needs. As such, training can be audited
according to the identiﬁed training objectives and scope based on the needs
of the organization.

1.3 Incident Response Planning and Capability

Assessment

An important step in Incident Response – if not the most important – is the
readiness phase. In the integrated digital investigation process (IDIP) model,
Carrier and Spaﬀord [5] state that “the goal of the readiness phases is to

1 Cloud Investigation Training using I-STRIDE

5

Fig. 1.2. Root Cause Analysis to guide forensic investigation training Stephen-
son, Peter. ”Modeling of post-incident root cause analysis.” International Journal
of Digital Evidence 2.2 (2003): 1-16.

ensure that the operations and infrastructure are able to fully support an
investigation”. The operations readiness phase involves the on-going training
of personnel, such as ﬁrst responders and lab technicians, and the procurement
and testing of equipment needed for the investigation. However, while general
training may be applicable to each organization, an organization may have
speciﬁc training needs that need to be identiﬁed.

For example, operational readiness in cloud environments should include
education in cloud-related technologies, such as hypervisors, virtual machines
and cloud-based storage, but may speciﬁcally depend on what services the
organization is providing. Personnel should have general knowledge of how to
interact with cloud technologies at the infrastructure, platform and software
layers, and understand the eﬀect their actions have on the environment. They
should understand the methods and tools available to collect investigation-
relevant data in each layer of the cloud. Diﬀerent Cloud Service Providers
(CSP) may have proprietary systems, so training on the use and investigation
of these proprietary systems should be considered. However, determination of
exactly what skills and knowledge are necessary to ensure quality investiga-
tions may be diﬃcult. Further, identifying what technologies should be the
focus of technical training may not be fully known.

The training of personnel, identiﬁcation of potential risks, and identiﬁca-
tion of potential data sources before an incident occurs can greatly help in
eﬃcient incident response, and with the timely and sound acquisition of rele-
vant data. For this reason this work recommends organizations model threats,

6

Joshua I. James et al.

their potential impact, and potential evidential trace data sources before an
incident occurs. This will assist the CSP in preserving potential evidence dur-
ing incident response, and will help law enforcement have a better idea of what
data will be available, and how to handle such data, if a particular incident
occurs.

1.4 Methodology

The proposed knowledge identiﬁcation method is broken into two areas of
assessment: Technology (security) risk and Knowledge (training/education)
risk. Assessment of ‘knowledge risk’ is necessary because simply knowing a
technical vulnerability exists will not aid in incident response or investigation
unless the responder/investigator has knowledge of concepts such as where
relevant evidential data may exist and how to properly acquire such data.
Below are both the Technology risk and knowledge risk assessment processes.

• Technology Risk Assessment (I-STRIDE)

1. Identify Assets
2. Identify Threats to Assets
3. Determine Potential Threat Impact
4. Determine Potential Evidential Data Sources (Pre-Investigation)
5. Prioritize Threats

– Organizational needs
– Common Vulnerability Scoring System (CVSS) [18]

• Knowledge Risk Assessment

1. Identify required investigation knowledge
2. Assess current in-house knowledge

– Knowledge of the collection/analysis of associated evidential data

sources

3. Compare in-house knowledge with risk prioritization

Knowledge risk in this case can be assessed based on Bloom’s Taxonomy
[2]. Using Bloom’s Taxonomy in-house knowledge could be assessed, either
through self-assessment or a more formal process. The Taxonomy would allow
an organization to understand the level of knowledge they possess about the
investigation of breaches caused by the speciﬁc vulnerability.

Bloom’s Taxonomy has 6 ‘verbs’ that correspond to a level of knowledge:
remembering, understanding, applying, analyzing, evaluating, and creating.
Knowledge of a vulnerability or evidential data sources can be assessed based
on these levels of knowledge. As such, a score can be assigned to each level (1
- 6), which can be considered the knowledge risk score. Such a score implies
that higher-level knowledge such as an ability to analyze and evaluate a topic
is preferred over simple remembering.

1 Cloud Investigation Training using I-STRIDE

7

If an organization is also using a threat prioritization metric – such as the
CVSS – then these scores can be combined to create a ‘knowledge prioritiza-
tion’ model. For example, with CVSS a score of 1 indicates a low risk, where
a score of 10 indicates a high risk. In Bloom’s taxonomy a score of 1 indicates
low knowledge, and a score of 6 indicates higher-level knowledge. Assume
CVSS is used to assess the severity of a threat (1 being least severe, 10 being
the most severe). Bloom’s taxonomy measures can be scaled to the same scale
as CVSS, and the knowledge risk can be subtracted from the technology risk
as so:

Ts − ((Ks ÷ Kmax) · Tmax)

where:

• Ts is the technology risk score
• Ks is the knowledge risk score
• Kmax is the maximum knowledge risk score
• Tmax is the maximum technology risk score

In this case, if the technology risk score is high (10 out of 10), and the
knowledge risk score is also high (6 out of 6), then the knowledge priority will
be low (0) in terms of training or education needs. If the technology risk score
is low (2 out of 10), and the knowledge risk score is also low (1 out of 6), then
the overall knowledge priority will still remain low (0.33). The threats with
the highest priority will be high-scoring technology risks that the organization
has little knowledge about. Further, as knowledge is updated (either gained
or lost) knowledge risk can also be updated to reﬂect the current state of
knowledge in the organization.

Again, this prioritization is used to identify areas where investigation ed-
ucation or training is lacking and supplementation may be necessary due to
technology risk, not to imply that a high knowledge of a vulnerability will
reduce an organization’s risk of that vulnerability being exploited.

1.5 Case Studies

To show the applicability of the I-STRIDE model for determining training
needs, assessment of cloud computing infrastructure based on Eucalyptus [8]
and OpenStack will be given as examples.

1.5.1 Case 1: Eucalyptus Cloud

This case will speciﬁcally look at a deployed Eucalyptus Cloud. The compo-
nents of this platform will be explained, and an analysis using the I-STRIDE
model will be conducted against this deployment.

The Eucalyptus architecture is composed of ﬁve high-level components

that are essentially standalone web services. These components include:

8

Joshua I. James et al.

• Cloud Controller (CLC): The cloud controller is the main entry point for
the cloud environment. CLC is responsible for “exposing and managing
the underlying virtualized resources”.

• Cluster Component (CC): CC is responsible for managing the execution

of VM instances.

• Storage Controller (SC): Provides block-level network storage that can be

dynamically attached by VMs instances.

• Node Controller (NC): Executed on every node that is designated for host-

ing and allows management of VM instances.

• Walrus: Allows the storage and management of persistent data.

Figure 1.3 shows the Eucalyptus components and their connection and

communication channels.

Fig. 1.3. Deployed Eucalyptus Architecture

The scope of this case will be limited to asset-centric threat modeling. The
assets in this case will be deﬁned as each of the Eucalyptus components which
can be thought of as the Cloud Service Provider (CSP), and will also include
a cloud client. In this case, threats (Table 1.1) were identiﬁed and exploited
in the deployed Eucalyptus architecture. Per the I-STRIDE model, an anal-
ysis of aﬀected assets was conducted. An investigation was then conducted
to determine potential evidential data sources. Identiﬁed threats, the threat
description, the aﬀected asset, the impact on the asset, and the location of
potential evidential data sources are listed in Table 1.1. Information in this
table may normally be used to assess threats, and, if a threat is exploited,
help to collect potential evidential traces during the incident response phase.

Threat

Description

Asset

1 Cloud Investigation Training using I-STRIDE

9

Threat Im-
pact
Denial of Ser-
vice

Potential Eviden-
tial Sources
XML parser logs at
the cloud controller

Cloud
Con-
troller, Cloud
Client

XML De-
of
nial
Service

Replay At-
tack Flaws

WSDL
Parameter
Tampering

Attacker
crafts XML
message with a large
payload,
recursive con-
tent or with malicious
DTD schema.
Attacker could issue re-
currence overloaded Sim-
ple Object Access Pro-
tocol (SOAP) messages
over HTTP to overwhelm
the CSP and stop the ser-
vice
could embed
Attacker
command
code
into WSDL documents
or
to
command shell
execute the command

line

Schema
Poisoning

Attacker could compro-
mise the XML schema
grammar and manipulate
the data

Cloud
Con-
troller, Cloud
Client

Denial of Ser-
vice

message
SOAP
and
timestamp
message payload to
identify the message
ﬂaws

Cloud
Con-
troller, Cluster
Controller,
Node
Con-
troller, Cloud
client
Cloud
Con-
troller, Cloud
Client

Denial of Ser-
vice

Detailed
investiga-
tion of WSDL ﬁle
could identify pa-
rameter tampering

Denial of Ser-
vice

of
logs at

investi-
Detailed
XML
gation
parser
the
cloud controller may
evidence
contain
of XML
schema
tampering

Table 1.1. Identiﬁed threats, the estimated threat impact and potential evidential
data sources identiﬁed for a Eucalyptus that is the result of the proposed I-STRIDE
model

Using the I-STRIDE model could help CSPs and law enforcement iden-
tify an investigation starting point during Incident Response (IR). This level
of readiness would potentially allow for improved pre-planning, ﬁrst response
and cooperation once an incident occurred. While I-STRIDE may help to de-
termine data sources interesting to an investigation, this information is only
useful if the responders understand how to properly access, acquire and pre-
serve such data. The output of the I-STRIDE model can be considered the
knowledge the organization needs. For example, if the Cluster Component is
compromised then incident responders need knowledge of the Cluster Com-
ponent to be able to make use of the knowledge of associated evidential data
sources. The I-STRIDE model can be used to guide training and education
development plans based on the needs of an assessed organization.

Case 1: Knowledge Prioritization

Consider again Table 1.1. If denial of service was found to be a priority risk
in an organization, then at least 4 threats have an impact deﬁned as denial

10

Joshua I. James et al.

of service. The threat vector, and already known potential evidential sources
can be used to deﬁne the type of training necessary to thoroughly investigate
breaches using such vectors of attack. For example, the ﬁrst priority threat
as shown in Table 1.2 uses XML as an attack vector. XML parser logs were
identiﬁed as potential evidential sources at the cloud controller.

The organization can apply the methodology described in Section 1.4 to
determine knowledge risk. Notice, in table 1.2 the organization did not use
CVSS, but instead chose a low-medium-high technology risk prioritization
scheme. In this case, since each technology risk is a high priority (3 out of
3), the organization can now assess their knowledge about the investigation
of each technology risk. For this case, let’s assume that the organization has
a great understanding of XML attack vector investigations (5 out of 6), and
very little knowledge of SOAP exploit prevention and investigation (1 out of
6). Knowledge prioritization can then be assessed as follows

• XML investigation training priority: 3 − ((5 ÷ 6) · 3) = 0.5
• SOAP investigation training priority: 3 − ((1 ÷ 6) · 3) = 2.5

If an organization can answer questions about in-house knowledge for high
priority technology risks, then the organization can identify associated knowl-
edge risk, and invest in training of personnel more eﬀectively based on their
unique needs. Once these knowledge priority areas have be en identiﬁed, they
can be fed directly into training development models such as the Successive
Approximation Model [1] for rapid training development. This will allow or-
ganizations to quickly target and close gaps in knowledge based on prioritized
organizational risks.

1.5.2 Case 2: OpenStack

The next case concerns the assessment of OpenStack, an open source cloud
infrastructure project. This example will use vulnerabilities identiﬁed in CVE
Details [16], along with the threat’s identiﬁed CVSS score. From CVE, an or-
ganization running OpenStack may assess their speciﬁc technology and knowl-
edge risk in relation to new vulnerabilities.

The knowledge required for each of the technology risks identiﬁed in Table
1.3 can be identiﬁed using the I-STRIDE process, and speciﬁcally by simulat-
ing an incident and determine where data relevant to the investigation may
be found. In this case, the required knowledge has been deﬁned as parts of
the swift architecture.

The CVSS in this case represents the technology risk if the vulnerability
has not been patched yet, or for some reason cannot be. The organization
must conduct an internal knowledge risk assessment based on the identiﬁed
knowledge areas for newly identiﬁed vulnerabilities. In this case, assume an
organization has moderate (3 out of 6) knowledge about swift proxy and
account servers, and little (1 out of 6) knowledge of swift object servers. The
investigation knowledge priority for each threat can be calculated as so:

1 Cloud Investigation Training using I-STRIDE

11

Priority Threat

Asset

High XML De-
of
nial
Service

High Replay At-
tack Flaws

Cloud
Cloud Client

Controller,

Threat
Impact
Denial
Service

Cloud
Cloud Client

Controller,

Denial
Service

High WSDL

Parameter
Tampering

High

Schema
Poisoning

Cloud
Controller,
Cluster Controller,
Node
Controller,
Cloud client
Cloud
Cloud Client

Controller,

Denial
Service

Denial
Service

Knowledge

XML attack
inves-
vector
tigation
SOAP exploit
prevention
and
gation

investi-

WSDL Secu-
rity and In-
vestigation

XML attack
vector
inves-
tigation

of

of

of

of

Potential Eviden-
tial Sources
XML parser logs at
the cloud controller

message
SOAP
timestamp
and
message payload to
identify the message
ﬂaws
Detailed
investiga-
tion of WSDL ﬁle
could identify pa-
rameter tampering
investi-
Detailed
XML
gation
parser
the
cloud controller may
evidence
contain
of XML
schema
tampering

of
logs at

Table 1.2. Threat prioritization (in this case, based on the organization’s subjec-
tive decision) and required investigation knowledge identiﬁcation based on identiﬁed
threats that cause a particular class of threat impact

Threat

Description

Asset

Threat
Impact

Swift Cluster

Denial
Service

of

4.0

CVSS Potential

Knowledge

Evidential
Sources
Tombstone ﬁles Swift Object

Servers

Authenticated
attacker
can ﬁll an object server
with superﬂuous object
tombstones

When an actor claims
to have a given iden-
tity,
the software does
not prove or insuﬃciently
proves that the claim is
correct.
Unchecked user input in
Swift XML responses

Issue
re-
quests with
an old X-
Timestamp
value
Re-auth
deleted
user with
old token

Generate
unparsable
or
arbi-
trary XML
responses

Keystone

Security
Bypass

6.0

Instance
user logs

and

Swift Proxy

Swift
servers

account

Security
Bypass

7.5

Account
logs

server

Account
server

Table 1.3. Identiﬁed threats, the estimated threat impact and potential evidential
data sources identiﬁed for OpenStack that is the result of CVE details and the
I-STRIDE model

12

Joshua I. James et al.

• Object server investigation training priority: 4 − ((1 ÷ 6) · 10) = 2.33
• Proxy server investigation training priority: 6 − ((3 ÷ 6) · 10) = 1
• Account server investigation training priority: 7.5 − ((3 ÷ 6) · 10) = 2.5

In this case, because the account server has the highest technology risk
and the organization only has a moderate level of knowledge about the inves-
tigation of such a risk, it is given the highest priority. It is then followed by
a technology risk that is relatively low, but is a risk which the organization
does not have much knowledge about.

1.6 Conclusions

Cloud computing has a number of beneﬁts, such as high availability, poten-
tially lower cost, and potentially improved security. However, cloud computing
also has a number of associated risks. Some of these risks have been inherited
from traditional computing models, while the cloud business model introduces
others. As more businesses and end users move their data and processing to
cloud environments, these environments will increasingly become the target,
or even the originator, of malicious attacks. By taking an asset-based risk as-
sessment approach, and speciﬁcally using the I-STRIDE model, organizations
can identify and prioritize threats, determine threat impact and potential evi-
dential sources, and ultimately identify gaps in investigator knowledge before a
threat is exploited. By implementing the proposed technology and knowledge
risk assessment metrics, and organization can at least be better positioned
to make training and education investment decisions based on observed deﬁ-
ciencies. I-STRIDE can act as a base for CSPs and law enforcement to more
eﬀectively work together before and during the investigation of incidents in
cloud environments, and not only in the discussion of vulnerabilities but in
the discussion of required knowledge.

While this work proposed a naive model for determining investigator train-
ing needs speciﬁc to the organization, future work will attempt to evaluate the
model with real organizations rather than a researcher-created case study. For
example, the model, as proposed, integrates prevention (security) and inves-
tigation (post incident) to attempt to improve both. However, such a model
takes a considerable amount of eﬀort and pre-planning to implement. In large
organizations, even communication between investigators and security oﬃ-
cers may be diﬃcult. Such real-world case studies are needed to evaluate the
practicality of the proposed training-guidance method.

References

1. Michael W. Allen. Creating successful e-learning: a rapid system for getting it

right the ﬁrst time, every time. Pfeiﬀer & Co., 2006.

1 Cloud Investigation Training using I-STRIDE

13

2. Lorin W. Anderson, David R. Krathwohl, and Benjamin Samuel Bloom. A

taxonomy for learning, teaching, and assessing. Longman, 2005.

3. Michael Armbrust, Armando Fox, Rean Griﬃth, Anthony D Joseph, Randy H
Katz, Andrew Konwinski, Gunho Lee, David A Patterson, Ariel Rabkin, Ion
Stoica, and Matei Zaharia. Above the Clouds: A Berkeley View of Cloud Com-
puting. Science, 53(UCB/EECS-2009-28):07–013, 2009.

4. Marco Balduzzi, Jonas Zaddach, Davide Balzarotti, Engin Kirda, and Sergio
Loureiro. A security analysis of amazon’s elastic compute cloud service, 2012.
5. Brian D Carrier and Eugene H Spaﬀord. Getting physical with the digital
investigation process. International Journal of Digital Evidence, 2(2):1–20, 2003.
6. CRA and Computing Research Association. Four Grand Challenges in Trust-

worthy Computing. Technical report, 2003.

7. Josiah Dykstra and Alan T Sherman. Acquiring forensic evidence from
infrastructure-as-a-service cloud computing: Exploring and evaluating tools,
trust, and techniques. Digital Investigation, 9:S90–S98, August 2012.

8. Eucalyptus. Eucalyptus: The Open Source Cloud Platform, 2013.
9. EY and EYGM Limited. Into the cloud, out of the fog: Ernst & Young’s 2011

Global Information Security Survey. Technical report, 2011.

10. Gartner. Forecast: Public Cloud Services, Worldwide, 2010-2016, 2Q12 Update.

Technical report, 2012.

11. Joshua I James, Ahmed F Shosha, and Pavel Gladyshev. Digital Forensic Inves-
tigation and Cloud Computing. In Keyun Ruan, editor, Cybercrime and Cloud
Forensics: Applications for Investigation Processes, pages 1–41. IGI Global,
2013.

12. W A Jansen. Cloud Hooks: Security and Privacy Issues in Cloud Computing.

pages 1–10. IEEE, 2011.

13. Karen Kent, Suzanne Chaevalier, Tim Grance, and Hung Dang. Guide to Inte-
grating Forensic Techniques into Incident Response. Technical Report SP800-86,
2006.

14. Martin Kerrigan. A capability maturity model for digital investigations. Digital

Investigation, 10(1):19–33, March 2013.

15. Ren Kui, Wang Cong, and Wang Qian. Security Challenges for the Public

Cloud. Internet Computing, IEEE, 16(1):69–73, 2012.

16. MITRE. OpenStack Security Vulnerabilities.
17. MSDN. The STRIDE Threat Model, 2005.
18. NIST. Common Vulnerability Scoring System.
19. Ponemon and L L C Ponemon Institute. The Security of Cloud Infrastructure:
Survey of U.S. IT and Compliance Practitioners. Technical report, 2011.
20. Antonio Pooe and L Labuschagne. A conceptual model for digital forensic
In 2012 Information Security for South Africa, pages 1–8. IEEE,

readiness.
August 2012.

21. Keyun Ruan, Joe Carthy, Tahar Kechadi, and Ibrahim Baggili. Cloud forensics
deﬁnitions and critical criteria for cloud forensic capability: An overview of
survey results. Digital Investigation, 10(1):34–43, June 2013.

22. P Saripalli and B Walters. QUIRC: A Quantitative Impact and Risk Assessment

Framework for Cloud Security. pages 280–288. Ieee, 2010.

23. Michael J Skok. Future of Cloud Computing 2012, 2012.
24. Adesina Simon Sodiya, Saidat Adebukola Onashoga, and Beatrice Oladunjoye.
Threat modeling using fuzzy logic paradigm. Journal of Issues in Informining
Science and Technology, 4(1):53–61, 2007.

14

Joshua I. James et al.

25. Peter Stephenson. Modeling of post-incident root cause analysis. International

Journal of Digital Evidence, 2(2):1–16, 2003.

26. F Swiderski and W Snyder. Threat modeling. Microsoft Press Redmond, WA,

USA, 2004.

27. M A Vouk. Cloud computing-Issues, research and implementations. pages 31–

40. IEEE, 2008.

28. Dimitrios Zissis and Dimitrios Lekkas. Addressing cloud computing security

issues. Future Generation Computer Systems, 28(3):583–592, 2012.

