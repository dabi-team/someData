Automated Adversary Emulation for Cyber-Physical
Systems via Reinforcement Learning

Arnab Bhattacharya∗, Thiagarajan Ramachandran∗, Sandeep Banik‡, Chase P. Dowling†, Shaunak D. Bopardikar‡
∗Optimization and Control Group, †Information Modeling and Analytics Group
Paciﬁc Northwest National Laboratory, Richland, WA, USA
‡Electrical and Computer Engineering, Michigan State University, East Lansing, MI, USA

0
2
0
2

v
o
N
9

]

G
L
.
s
c
[

1
v
5
3
6
4
0
.
1
1
0
2
:
v
i
X
r
a

Abstract—Adversary emulation is an offensive exercise that
provides a comprehensive assessment of a system’s resilience
against cyber attacks. However, adversary emulation is typically
a manual process, making it costly and hard to deploy in cyber-
physical systems (CPS) with complex dynamics, vulnerabilities,
and operational uncertainties. In this paper, we develop an
automated, domain-aware approach to adversary emulation for
CPS. We formulate a Markov Decision Process (MDP) model to
determine an optimal attack sequence over a hybrid attack graph
with cyber (discrete) and physical (continuous) components and
related physical dynamics. We apply model-based and model-
free reinforcement learning (RL) methods to solve the discrete-
continuous MDP in a tractable fashion. As a baseline, we also
develop a greedy attack algorithm and compare it with the RL
procedures. We summarize our ﬁndings through a numerical
study on sensor deception attacks in buildings to compare the
performance and solution quality of the proposed algorithms.

Index Terms—Adversary Emulation, Reinforcement Learning,

Cyber-Physical Security, Hybrid Attack Graph

I. INTRODUCTION

With increasing sophistication of today’s cyber attacks,
there is a critical need for offensive testing to assess the
resilience of cyber systems. Such offensive exercises come
in different ﬂavors. Pre-compromise tests, such as penetration
testing, involve probing a system to identify vulnerabilities
under controlled rules of engagement. By contrast, post-
compromise exercises, such as adversary emulation, require
a team of cybersecurity experts - called a red team - to
emulate end-to-end attacks following a set of realistic tactics,
techniques, and procedures (TTPs). Compared to penetration
testing, adversary emulation provides a complete security
assessment
to identify, contain and mitigate cyber threats.
Moreover, adversary emulation generates attack scenarios that
can be used to verify and improve post-compromise resilience.
However, current adversary emulation requires a highly-skilled
red team to manually draft the attack sequences, which can be
time-consuming, costly, and personnel constrained [1], [2].

Adversary emulation is even more challenging for cyber-
physical systems (CPS), which integrate computing resources,
communication protocols and physical processes. With rapid
inﬁltration of Internet-of-Things (IoT) devices and smart sen-
sors, red teams have to deal with an ever expanding attack
surface. Unlike cyber systems, scant forensic evidence exists
of post-compromise breaches in CPS, which makes it difﬁcult
to plan and execute emulation tests. Moreover, CPS operate

under multiple sources of uncertainty that need to be charac-
terized in emulation exercises. This is challenging for complex
CPS where red teams have limited domain knowledge and the
number of operational scenarios can be prohibitively large [2].
Finally, adversary emulation is risky during online operations
of critical CPS (e.g. hospitals, power grid) as possible equip-
ment damages and service disruptions can cause widespread
economic loss and safety hazards [3]. The aforesaid challenges
create a pressing need for automated emulation tools for CPS.
Attack graphs are traditionally used to develop emulation
tools for cyber systems [4]. An attack graph models vulner-
abilities in networked systems via series of discrete exploits
that lead to a compromised security state [5]. Efﬁcient greedy
algorithms with adversarial performance guarantees exist for
attack graphs with speciﬁc topological constraints [6]. More-
over, game theory has been extensively used to harden cyber
networks via analysis on attack graphs [7]. It is noted that
hybrid attack graphs are more suitable to model the discrete
(cyber) and continuous (physical) components of cross-domain
CPS attacks [8]. Recently, advances in adversarial machine
learning, such as generative adversarial networks, have been
used to design intrusion detection systems that are robust to
adversarial data perturbations in CPS [9].

While optimal attack sequences can be determined for
certain classes of CPS [10], the general problem of adversary
emulation needs a principled application of the philosophy of
“thinking like an attacker”. A key gap in the literature on CPS
security is the assumption of an omniscient attacker, which is
unrealistic for many complex CPS and may lead to extremely
conservative defensive postures [7]. Moreover, it is essential
to consider a cross-layer viewpoint of adversary emulation
due to strong security inter-dependencies between the cyber
and physical layers.Existing studies do not adequately address
the coupling between cross-layer vulnerabilities and physical
dynamics for adversary emulation in CPS.

We present a new domain-aware, reinforcement learning
based approach to automated adversary emulation for CPS.
The key contributions of this work are summarized as follows.
First, we formulate a novel Markov Decision Process (MDP)
model to determine an optimal attack strategy over a hybrid
attack graph with cyber (discrete) and physical (continuous)
components and domain-speciﬁc dynamics. Second, we de-
velop two competing solution procedures, based on model-

 
 
 
 
 
 
learning (RL),

based and model-free reinforcement
to ap-
proximately solve the MDP model with a hybrid state space.
Third, we design a greedy attack algorithm that exploits the
topology of hybrid attack graphs and serves as a baseline for
the RL procedures. Finally, we demonstrate the performance
and solution quality of the proposed algorithms on a use-case
involving sensor deception attacks on buildings.

II. MODEL FORMULATION

A. Hybrid Attack Graph

A hybrid attack graph (HAG) models the security state space
of a CPS, where the nodes represent security attributes (or
capabilities), while the edges denote adversarial exploits (or
actions). The leaf nodes describe entry-point cyber attributes,
edges denote cyber exploits, and root nodes signify target
physical attributes. Each attack action (cyber exploit or physi-
cal attack) has associated success probability, reward and cost
values. We restrict our analysis to directed acyclic HAGs that
enforce the well-known monotonicity assumption [11], which
states that an adversary never willingly relinquishes attributes
once obtained. Next, we formally deﬁne a HAG.

Deﬁnition 1 (HAG). A directed acyclic hybrid attack graph
is a tuple G = (N , E, F, A, R, C, Φ), where

1) N = {1, 2, . . . , N } is the set of attribute nodes. The
set of cyber and physical nodes is denoted by C and P,
respectively, such that N = C ∪ P.

2) E is the set of edges describing unique cyber exploits.
3) F is the set of functions governing the dynamics at the
physical nodes, where fn ∈ F is the dynamics at n ∈ P.
4) For e ∈ E, let ae be the cyber exploit along edge e. Deﬁne
AE := ∪e∈E ae to be the set of all cyber exploits. Let An
be the set of available actions at a physical node n ∈ P.
Denote the act of doing nothing by ∅. Then, the set of
(cid:1),
all attack actions in G is A = {∅} ∪ AE ∪ (cid:0)∪n∈P An
where A is assumed to be ﬁnite.

e (resp. ra

5) R = {re, rn}e∈E,n∈P is the set of real-valued reward
functions, where ra
n) is the reward for selecting
action a along edge e (resp. at physical node n). The cor-
responding set of cost functions is C = {ce, cn}e∈E,n∈P .
6) Φ : A × T → [0, 1] is a probability mass function that
maps an action a at time t to its success probability Φa
t .

Remark 1 (Relationship with pre-conditioning). Deﬁnition 1
ignores the notion of the so-called pre-conditions [12], where
access to nodes with an AND pre-condition require all origin
nodes to be compromised, while an OR pre-condition requires
at least one is compromised. We assume that each node in a
HAG has an OR pre-condition only; this can be easily relaxed
at the expense of notational complexity.

Remark 2 (Attack success probabilities). It is assumed that
the attack-success probabilities are governed by a time-varying
and non-adaptive defender policy, which is independent of the
attack policy (see [13] for a similar setup).

Fig. 1: Example of a hybrid attack graph where C = {1, 2, 3, 4, 5}
and P = {6, 7} are the set of cyber (red) and physical (blue) nodes.

B. Markov Decision Process Model

Here, we describe the main constituents of the MDP model
including the state and action spaces, state transitions, reward
function, and MDP objective function.

1) State Space: Let T = {1, . . . , T } be a ﬁnite attack
horizon, where t ∈ T is the t-th time period. Let si
t ∈ {0, 1}
be the security state of node n ∈ N at time t, where si
t = 1 if
the adversary has compromised node i by time t, and si
t = 0
otherwise. Let P be the total number of physical nodes. The
dynamical state at a physical node n ∈ P at time t is denoted
by xn
t is a random disturbance that affects the
dynamics fn. Then, the (random) system state at time t is
t , w1
where S is a bounded, hybrid (discrete-continuous) state
space. We use s to denote a generic state in S.

t ∈ X , while wn

t , . . . , xP

t , . . . , sN

st = (s1

t ) ∈ S,

t , wP

t , x1

(1)

t = 1, sj

2) Action Space: Deﬁne Et := {(i, j) ∈ E : si

t =
0}, and Pt := {i ∈ P : si
t = 1}, where Et is the set of edges
with available cyber exploits and Pt is the set of compromised
physical nodes at time t, respectively. Let Bt = ∪e∈Etae be
the set of available cyber exploits at time t, where Bt ⊆ AE .
The action space at time t, denoted by At(st), is the set of all
cyber exploits and physical-node actions available at t, i.e.,

At(st) = {∅} ∪ Bt ∪ (cid:0)∪n∈PtAn

(cid:1) ⊆ A.

(2)

Note that E0 = {(i, j) ∈ E : i ∈ C, si
action at time t is denoted by at.

0 = 1} and P0 = ∅. The

t , an

t , Φt),

3) State Transition Function: The attack-success probabil-
t ) govern
t at

ities (Φt) and the random disturbances (w1
the state transitions in the MDP. The dynamical state xn
node n ∈ P evolves according to the dynamics
t+1 = fn(xn
xn
t = λn(xn
un

t , wn
t , Φt),

t , . . . , wP

t , un
t , an
t ∈ U is the control

:= (cid:8)(k, n) ∈ E : sk

where un
input, and λn is a con-
law that can be an outcome of an optimization. To
trol
describe transitions due to cyber exploits, deﬁne the set
t = 0(cid:9) ⊆ Et that contains the
E n
t
available edges to reach an uncompromised node n at time
t; note that E n
t be the set of
cyber exploits related to the edges in E n
(cid:54)= ∅, the
probability that n is compromised, denoted by pn
t , is equal to
the probability that at least one available exploit is successful,

t = 1. Let An

t = ∅ if sn

t . When E n
t

t = 1, sn

(3b)

(3a)

a∈An
t

t = 1 − (cid:81)

(1 − Φa

t = 0 if E n

t ), where pn

i.e., pn
t = ∅.
Here, it is assumed that the exploits fail independently of
each other. In what follows, the acronym “w.p.” stands for
with probability. Then, the dynamics for sn
sn
t ,
1,
0,

t = 1, (w.p. 1),
t = 0, (w.p. pn
t ),
t = 0, (w.p. 1 − pn

t is described by

if sn
if sn
if sn

sn
t+1 =




t ),

(4)



where the ﬁrst condition in (4) follows from the monotonicity
assumption. Let dt := (Φt, wn
t )n∈P . For notational brevity,
we jointly express the dynamics in (3) and (4) as

st+1 = g(st, at, dt),

(5)

where g is a probability transition kernel.

4) Reward Function: Let rt(s, a) be the random net-reward
(reward minus cost) at time t for state s and action a. Then,

rt(s, a) =

(cid:40)

ra − ca,
r∅ − ca,

t ),

(w.p. Φa
(w.p. 1 − Φa

t ),

(6)

e , ra

e and ca = ca

n in (6). The rewards ra

n and ca = ca
n were deﬁned in Deﬁnition 1.

where ra = ra
e if a is a cyber exploit, i.e., a ∈ Bt.
When a is a physical-node action, i.e., a ∈ An for n ∈ Pt,
then ra = ra
n and costs
e , ca
ca
5) Objective Function: Let πt : S → ∆At denote a policy
that maps a state s to a probability distribution over the
action space At(s). Note that this also includes the space of
deterministic policies πd
t : S → At. We seek an attack policy
of the form π = (πt)t∈T , such that a = πt(s) ∈ At(s). Let
Π be the space of all feasible policies. Starting from an initial
state s, the adversary seeks a policy π∗ ∈ Π that maximizes
the total expected ﬁnite-horizon reward, i.e.,

π∗ ∈ arg max

E

π∈Π

(cid:34)

(cid:88)

t∈T

(cid:35)
(cid:12)
(cid:12)
(cid:12)π, s0 = s
rt(st, πt(st))

,

(7)

where the expectation is taken with respect to the transition
kernel in (5). Using the linearity of the expectation operator,
Equation (7) can be written in the following recursive form,
known as the Bellman optimality equation [14]:

V ∗
t (s) = max
a∈At(s)

E(rt(s, a) + V ∗

t+1(st+1|s, a)),

(8)

where V ∗
t
policy π∗ can be extracted from V ∗

is the optimal value function at time t. The optimal

t using

However, an adversary usually has limited knowledge of the
dynamics in (3) and the attack success probabilities. Next, we
discuss two competing reinforcement learning (RL) algorithms
and one greedy baseline procedure to approximately solve (7).

III. SOLUTION APPROACHES

A. Approximate Dynamic Programming

Approximate dynamic programming (ADP) is a model-
based RL approach that overcomes the challenge of a hybrid
MDP state space. Unlike DP methods that use backward
enumeration, ADP steps forward in time and generates sample
state trajectories to learn a low-dimensional parametric ap-
proximation of V ∗
t . Let θ ∈ Θ be a parameter vector, where
Θ has a signiﬁcantly smaller dimension than that of S. The
goal in ADP is to iteratively learn a value θ∗ ∈ Θ such that
t (s)−Jt(s; θ∗)| < (cid:15), where Jt(s; θ) is a parametric
∀s ∈ S, |V ∗
function of θ, and (cid:15) > 0 is a small tolerance value. Let k
be the iteration index, sk
t denote the sampled state at time
t in iteration k, and ˆθ be the current estimate of θ∗ at start
of iteration k. Assuming an attacker has knowledge of the
dynamics in (5), a sample estimate of V ∗
t is

(cid:104)
rt(sk

t , a) + Jt+1(sk

t at state st = sk
(cid:105)
t+1; ˆθ)

.

(10)

ˆvk
t = max
a∈At(sk
t )

E

Note that (10) evaluates a greedy action with respect to (w.r.t.)
the current function approximation evaluated at the sampled
state, without requiring a complete sweep of the state space.
Once ˆvk
is computed, θ is immediately updated using an
t
online stochastic gradient algorithm as follows:

ˆθ ← ˆθ + α∇θ

(cid:16)

Jt(sk

t ; ˆθ) − ˆvk

t

(cid:17)

,

(11)

where α > 0 is a step size that varies over the iterations,
t , ˆθ). The
and ∇θ is the gradient of Jt w.r.t. θ evaluated at (sk
process is repeated until ˆθ converges or a prescribed number
of iterations is completed. The update in (11) is guaranteed to
converge if (cid:80)∞
i < ∞ [15]. The steps
of the ADP algorithm are described in the Appendix. Although
ADP is applicable even if the dynamics are unknown or mis-
speciﬁed, we use the model-based version here to compare it
against pure model-free RL methods, such as the Actor-Critic
algorithm that is discussed next.

i=1 αi = ∞ and (cid:80)∞

i=1 α2

B. Actor-Critic Algorithm

π∗
t (s) ∈ arg max
a∈At(s)

E(rt(s, a) + V ∗

t+1(st+1|s, a)).

(9)

Note that V ∗
t (s) denotes the expected total return when,
starting from state s at time t, an adversary follows π∗ till
the end of the horizon.

C. Computational Challenges

We now emphasize the major challenges in solving the MDP
in (7). First, note that classical dynamic programming (DP)
algorithms, such as value- and policy-iteration [14], are not
amenable for solving (7) as they require multiple sweeps over
the state space to solve the optimality equations in (8), which
is clearly intractable as S is uncountable. Second, DP methods
in (5).
assume perfect knowledge of the transition model

The Actor-Critic (AC) algorithm [14] is an iterative model-
free procedure that concurrently trains two models (called
the actor and the critic) to learn a parametric form of the
optimal policy of (7), without requiring any knowledge of the
transition dynamics in (5). Let πt(a|s; ψ) denote a stochastic
policy at time t parameterized by ψ ∈ Ψ, and let Jt(s; θ) be
the corresponding value function approximation, as deﬁned
in Section III-A. At each time step of a given episode, the
critic updates the value-function parameters θ using sampled
actions and successor states, while the actor updates the
policy parameters ψ in a direction suggested by the critic.
A stochastic gradient scheme (similar to (11)) updates both
ψ and θ. The process is repeated for different episodes and
terminates once a prescribed convergence criterion is met.

The AC algorithm is most suited for problems with con-
tinuous action spaces. To apply it for the MDP in (7) with
discrete actions, an exponential softmax distribution is used
as the parametric form for πt, i.e., for each t ∈ T ,

πt(a|s; ψ) =

eh(s,a,ψ)
b∈At(s) eh(s,b,ψ)

(cid:80)

,

∀a ∈ At(s),

(12)

where e is the Euler constant. The function h(s, a, ψ) in (12)
denotes a real-valued parametric preference deﬁned for each
state-action pair, which can be encoded using tile coding or
deep neural networks. The main steps of the AC algorithm are
described in the Appendix.

C. Greedy Attack Policy

We discuss a greedy attack scheme for hybrid attack graphs
that serves as a baseline for the aforementioned RL procedures.
The proposed greedy policy exploits the topology of a HAG to
identify a reduced set of available actions, which is then used
to execute cross-layer attacks in a myopic fashion. Next, we
describe the main steps of the greedy policy in more detail.

For a given cyber exploit a, let Ra be the set of successive
actions an adversary can use to reach any root node in P. For
example, in Figure 1, the action a = a3,5 along the edge (3, 5)
has an associated Ra = {a3,5, a5,4, a4,6, a5,7}. Starting from
an initial state s at time t = 0, the value in using a cyber
exploit a ∈ B0 to reach any of the root nodes is expressed as
(cid:88)

Q(a, s) =

E[r0(s, ˜a)],

(13)

˜a∈Ra
where the expectation is taken w.r.t to the success probabilities
in (6) but not on the security states. We extend the deﬁnition
in (13) to a set of cyber exploits A ⊆ AE as
E[r0(s, a)].

Q(A, s) =

(14)

(cid:88)

a∈RA
Starting with an empty set A, the greedy policy iteratively adds
the most valuable cyber exploits to A by maximizing Q(A, s)
in (14), i.e.,

a∗ = arg max
a∈B0\A

Q(A ∪ {a}), A ← A ∪ {a∗}.

(15)

The number of actions to be added to A, denoted by (cid:96), is set
apriori. The ﬁnal output A provides an adversary with the (cid:96)
most valuable (initial) exploits to reach the root nodes. For
example, if (cid:96) = 1, the adversary selects an available exploit
that yields the largest reward along a path to a root node.
Starting from an initial state s0 and using the initial exploits
in A, the adversary now executes a myopic policy at each time
t, denoted by πg
t , that maximizes the nominal net-reward, i.e.,
πg
t (st) ∈ arg max
(16)
a∈At(st)∩A
where At(st) ∩ A is the reduced action space at t. The steps
of the Greedy attack policy is provided in the following table.

{ra − ca},

IV. RESULTS AND DISCUSSION

A. Use Case: Sensor Deception Attacks in Buildings

We consider sensor-deception attacks in buildings where an
adversary targets to maximize occupant discomfort. Regular

Algorithm 1: Greedy Attack Policy
Phase 1: Pruning initial cyber exploits
Input: G, (cid:96), s0
Result: Set of valuable cyber exploits A
Initialize A ← empty;
while |A| < (cid:96): do

select an exploit a∗ and update A using (15);

end
Phase 2: Myopic policy execution
Input: A, s0
for t ∈ T : do

select action at = πg
transition to the next state according to (5);

t (st) using (16);

end

building operations involve an air-handling unit (AHU) that
re-conditions ambient air to a speciﬁc supply-air temperature,
which is then forced into the building zones by a supply fan.
The adversary seeks an intelligent way to manipulate temper-
ature measurements (from zone-level sensors) to deceive the
AHU control in sending poorly conditioned air into the zones,
causing large comfort-bound violations over time. However,
to access the temperature sensors, the adversary has to ﬁrst
execute a set of cyber exploits on different components of
a Building Automation System (BAS), including IoT devices
(e.g. IP camera and smart thermostats), building-management
workstations, and programmable logic controllers (PLC). For
demonstrative purposes, we assume that only a single zone,
with a dedicated AHU and temperature sensor, is under attack.
For our use case, we use the HAG in Figure 2 as a proof-of-
concept. Similar attack graphs for BAS were used in [16].

Fig. 2: The proposed HAG for building sensor deception attacks.
The red circles depict the cyber nodes, while the blue rectangle is
the root node representing the building zone under attack.

Next, we describe the cyber and physical layer components of
the proposed HAG using notation similar to Section II.

1) Cyber Layer: The HAG consists of four cyber nodes
and ﬁve cyber exploits. Table I tabulates the different cyber
exploits and associated success probabilities (assumed to be
incurs a cost (ca
e ) of 0.1. A
time-invariant). Each exploit
nominal reward (ra
e ) of 1 is gained if an exploit is successful,
while the reward for doing nothing (r∅

e ) is set to 0.

2) Physical Layer: Let xt ∈ R be the zone temperature (in
◦C) at time t and ut = (zt, mt) denote the vector of AHU
control inputs, where zt ∈ R is the supply-air temperature
(in ◦C) and mt ∈ R+ is the airﬂow rate (in kg/s) at time
t, respectively. The (random) outside-air temperature (in ◦C)

TABLE I: Description of the cyber exploits and associated success
probabilities in the cyber layer.

Edge
(1, 2)
(1, 3)
(2, 3)
(3, 4)
(4, 5)

Type of Cyber Exploit
Initial access to node 2
Initial access to node 3
Lateral movement to node 3
Lateral movement to node 4
Command and control from node 4

Success Probability
0.9
0.7
0.9
0.8
0.5

at time t is denoted by wt ∈ R. Similar to [17], the zone
temperatures evolve according to the nonlinear dynamics

wk

xt+1 = xt + 0.01mt(zt − xt) + 0.1(wt − xt).

(17)
Let xL = 23◦C and xU = 25◦C be the lower and upper
thermal-comfort bounds, respectively, where an occupant is
comfortable if xt ∈ [xL, xU ], and is uncomfortable otherwise.
The outside-air temperature at each time step is sampled from
t ∼ 0.5(xL + xU ) + 4 sin(0.125t + k) + φt,
where φt ∼ U (−1, 1) is a uniform random variable between
[-1,1] and k denotes the phase shift of the sine wave. With
a slight abuse of notation, let at ∈ A5 denote the adversarial
perturbation (in ◦C) at time t. We assume the success proba-
bilities to be independent of the attack actions and decreases
monotonically over time

(18)

Φt = 0.5 − 0.1(cid:98)t/10(cid:99),

(19)

where (cid:98)·(cid:99) is the ﬂoor function. Let bt ∼ B(Φt) be a Bernoulli
random variable with parameter Φt. The perturbed measure-
ment at time t equals yt = xt + atbt, where yt = xt + at if at
is successful (w.p. Φt), and yt = xt otherwise (w.p. 1 − Φt).
The control variables in ut are set according a threshold policy
that depends on yt as follows:



(30, 10 min{xL − yt, 1}),
(15, 10 min{yt − xU , 1})
(zt−1, 0)



if yt < xL,
if yt > xU ,
otherwise.

(zt, mt) =

(20)

Using (20), the dynamics in (17) can be expressed as xt+1 =
f (xt, wt, at). Deﬁne g(u) ≡ max{u, 0} for u ∈ R. Then, the
reward for executing action at, denoted by r(at), equals

r(at) = g (cid:0)xL − f (xt, wt, at)(cid:1) + g (cid:0)f (xt, wt, at) − xU (cid:1) ,
where the ﬁrst (resp. second) term is the thermal discomfort
caused by temperature deviation from the lower (resp. upper)
comfort bound. The cost of executing an action at is set to
0.5a2

t . Therefore, the net-reward at time t equals
r(at) − 0.5a2
t ,
−0.5a2
t ,

(w.p. Φt),
(w.p. 1 − Φt).

rt(st, at) =

(cid:40)

B. Experimental Setup

We used tile coding [14] to construct a sparse feature
representation of the state space. The value functions were
deﬁned as linear-function approximations over the set of tiles.
The number of time steps in the attack horizon was set
to 48. The comfort range [xL, xU ] was kept ﬁxed over the
entire horizon. The attack policies and the corresponding value
functions for the ADP and the AC procedures were trained

over 50,000 episodes. For each episode k, sample trajectories
of the outside-air temperature and success probabilities were
generated using (18) and (19), respectively. Note that the phase
shift in (18) is incremented over the episodes to avoid over-
ﬁtting to a speciﬁc outside temperature trajectory. The initial
zone temperatures were sampled uniformly from the range
[xL, xU ]. Once the training was complete, the performance of
all three policies was compared over 10,000 test episodes.

C. Results and Discussion

Table II compares the average number of time steps (over
the test episodes) that it took to access the physical node under
the ADP, AC, and Greedy policies. Due to their predictive
capabilities, the ADP and AC policies secured quicker access
to the physical node to cause occupant discomfort early in
the attack horizon. By contrast, the Greedy policy prioritizes
short-term gains accrued by executing all of the cyber exploits,
which delays the corresponding access to the physical node.

TABLE II: Average number of time steps to reach the physical node.

Attack policy
Greedy
ADP
AC

Avg. time to reach root node
10.7
8.4
8.7

Figure 3 depicts the performance of the three attack policies
at the physical node for a representative test trajectory. Note
at each time t, the sensor measurement (yt) is bounded by
the actual zone temperature (xt) and the attacker’s intent
(xt + at), depending on whether action at is successful or
not. In general, all three policies seeks to deceive the AHU
controller by maintaining the sensor measurements near the
comfort bounds and keeping the true temperatures outside
the comfort bounds over longer durations. As the success
probabilities decay towards the end of the horizon, the greedy
policy prescribes high-risk, high-reward actions that result in
lower average performance towards the end. This is evident
in Fig 3c where there is a consistent mismatch between the
attacker’s intent and the sensor measurement towards the end
of the horizon. By contrast, both the ADP and AC policies
reduce the frequency and the magnitude of attacks towards
the end of the horizon where the success probabilities are low
(see Figures 3a and 3b). We compare the performance of the
algorithms using a metric ρ that measures the attacker’s overall
return in investment, deﬁned as

(21)

ρ =

(cid:80)
(cid:80)

t∈T ˆr(at)
t∈T 0.5a2
t

,

where ˆr(at) is the reward observed after taking an action at.
For the trajectories shown in Fig 3, the computed return is
13.84 (ADP), 11.3 (AC), and 4.03 (Greedy).

To study the impact of the size of action space on policy per-
formance, we assume at ∈ {−2, −2 + δ, . . . , 2 − δ, 2}, where
different values of δ ∈ {1/s | s ∈ {1, 3, . . . , 9}} produce
different action spaces. Table III highlights the performance of
the different algorithms for different sizes of the action space.
Note that action space becomes larger, both the ADP and the

(a) ADP Policy

(b) Actor-Critic Policy

(c) Greedy Policy

Fig. 3: Illustration of sample trajectories using the learned policies from ADP, AC, and Greedy algorithms.

AC policies exhibit similar performance, while outperforming
the greedy algorithm.

TABLE III: The mean and variance of the net reward for different
action-space dimensions

Number of
actions
5
13
21
29
37

Mean of net reward
Greedy
AC
6.09
6.6
6.0
7.9
6.3
7.9
6.37
8
6.39
7.96

ADP
7.2
7.8
8.03
8.06
8.09

Variance of net reward
Greedy
AC
ADP
0.804
0.75
0.49
1.51
1.61
0.882
1.85
1.56
0.861
1.99
1.49
0.876
2.04
1.84
0.881

V. CONCLUSION AND FUTURE WORK

In this paper, an MDP-based approach is adopted to deter-
mine the optimal attack sequence over a hybrid attack graph
having both discrete and continuous components. Two RL
algorithms were implemented to overcome the computational
challenges of a hybrid state MDP. A greedy attack scheme
was developed as a baseline for the RL procedures. Finally,
a building use-case is used to demonstrate the comparative
performance of the proposed algorithms.

In the future, this methodology will be extended to cases
in which an attacker may have limited information of the
attack graph using partially observable MDP models. Hybrid
model-based and model-free RL algorithms will be developed
to guide the learning process in cases where the CPS dynamics
are complex and accurate system models are unavailable.
Finally, deep RL methods will be explored to solve larger
hybrid attack graphs of complex CPS.

ACKNOWLEDGEMENTS

This research is part of a sponsored project under the Mathe-
matics for Artiﬁcial Reasoning in Science (MARS) initiative
at the Paciﬁc Northwest National Laboratory, USA.

REFERENCES

[1] D. Miller, R. Alford, A. Applebaum, H. Foster, C. Little, and B. Strom,
“Automated adversary emulation: A case for planning and acting with
unknowns,” MITRE Corporation, Technical Paper, 2018.

[2] A. Applebaum, D. Miller, B. Strom, H. Foster, and C. Thomas, “Analysis
of automated adversary emulation techniques,” in Simulation Series,
2017.

[3] M. G. Angle, S. Madnick, J. L. Kirtley, and S. Khan, “Identifying and
anticipating cyberattacks that could cause physical damage to industrial
control systems,” IEEE Power and Energy Technology Systems Journal,
vol. 6, no. 4, pp. 172–182, 2019.

[4] A. Applebaum, D. Miller, B. Strom, C. Korban, and R. Wolf, “Intelli-
gent, automated red team emulation,” in ACM International Conference
Proceeding Series. ACM, 2016, pp. 363–373.

[5] L. Mu˜noz-Gonz´alez, D. Sgandurra, M. Barr`ere, and E. Lupu, IEEE
Transactions on Dependable and Secure Computing, vol. 16, no. 2, pp.
231–244, 2015.

[6] H. T. Nguyen and T. N. Dinh, “Targeted cyber-attacks: Unveiling target
reconnaissance strategy via social networks,” in 2016 IEEE Conference
on Computer Communications Workshops.
IEEE, 2016, pp. 288–293.
[7] S. R. Etesami and T. Bas¸ar, “Dynamic games in cyber-physical security:
An overview,” Dynamic Games and Applications, vol. 9, no. 4, pp. 884–
913, 2019.

[8] M. Ibrahim and A. Alsheikh, “Automatic hybrid attack graph generation
for complex engineering systems,” Processes, vol. 7, no. 11, 2019.
[9] S. R. Chhetri, A. B. Lopez, J. Wan, and M. A. Al Faruque, “GAN-
Sec: Generative adversarial network modeling for the security analysis
of cyber-physical production systems,” in Proceedings of the 2019 IEEE
Design, Automation and Test
in Europe Conference and Exhibition
(DATE).

IEEE, 2019, pp. 770–775.

[10] Y. Chen, S. Kar, and J. M. Moura, “Optimal attack strategies subject to
detection constraints against cyber-physical systems,” IEEE Transactions
on Control of Network Systems, vol. 5, no. 3, pp. 1157–1168, 2017.

[11] E. Miehling, M. Rasouli, and D. Teneketzis, “Optimal defense policies
for partially observable spreading processes on Bayesian attack graphs,”
in Proceedings of the 2nd ACM Workshop on Moving Target Defense.
ACM, 2015, pp. 67–76.

[12] S. Saha, A. K. S. Vullikanti, M. Halappanavar, and S. Chatterjee,
“Identifying vulnerabilities and hardening attack graphs for networked
systems,” in 2016 IEEE Symposium on Technologies for Homeland
Security (HST).

IEEE, 2016, pp. 1–6.

[13] A. R. Hota, A. A. Clements, S. Bagchi, and S. Sundaram, “A game-
theoretic framework for securing interdependent assets in networks,”
in Game Theory for Security and Risk Management: From Theory to
Practice. Springer International, 2018, pp. 157–184.

[14] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[15] W. Powell, “What you should know about approximate dynamic pro-
gramming,” Naval Research Logistics, vol. 56, no. 3, pp. 239–249, 2009.
[16] D. dos Santos, C. Speybrouck, and E. Costante, “Cybersecurity in
Building Automation Systems,” Forescout Technologies, Tech. Rep.,
2019.

[17] J. Dong, T. Ramachandran, P. Im, S. Huang, V. Chandan, D. L.
Vrabie, and T. Kuruganti, “Online learning for commercial buildings,”
in Proceedings of the Tenth ACM International Conference on Future
Energy Systems, 2019, pp. 522–530.

The appendix is available at: https://bit.ly/3odSMce

APPENDIX

