Security of Distributed Machine Learning: A

Game-Theoretic Approach to Design Secure

1

DSVM

Rui Zhang, Quanyan Zhu

Abstract

Distributed machine learning algorithms play a signiﬁcant role in processing massive data sets over

large networks. However, the increasing reliance on machine learning on information and communication

technologies (ICTs) makes it inherently vulnerable to cyber threats. This work aims to develop secure

distributed algorithms to protect the learning from data poisoning and network attacks. We establish

a game-theoretic framework to capture the conﬂicting goals of a learner who uses distributed support

vector machines (SVMs) and an attacker who is capable of modifying training data and labels. We

develop a fully distributed and iterative algorithm to capture real-time reactions of the learner at each

node to adversarial behaviors. The numerical results show that distributed SVM is prone to fail in

different types of attacks, and their impact has a strong dependence on the network structure and attack

capabilities.

Index Terms

Adversarial distributed machine learning Game theory Distributed support vector machines Label-

ﬂipping attack Data-poisoning attack Network-type attacks

I. INTRODUCTION

Recently, parallel and distributed machine learning (ML) algorithms have been developed

to scale up computations in large datasets and networked systems [2], such as distributed

Taken partially from the dissertation submitted to the Faculty of the New York University Tandon School of Engineering in

partial fulﬁllment of the requirements for the degree Doctor of Philosophy, January 2020 [1].

R. Zhang and Q. Zhu are with the Department of Electrical and Computer Engineering, New York University, Brooklyn, NY,

11201 E-mail:{rz885,qz494}@nyu.edu.

0
2
0
2

r
p
A
6
2

]

R
C
.
s
c
[

2
v
5
3
7
4
0
.
3
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

spam ﬁltering [3] and distributed trafﬁc control [4]. However, they are inherently vulnerable to

adversaries who can exploit them. For instance, nodes of distributed spam ﬁlter system can fail

to detect spam email messages after an attacker modiﬁes the training data [5], or disrupts the

network services using denial-of-service attacks [6].

ML algorithms are often open-source tools and security is usually not the primary concerns

of their designers. It is undemanding for an adversary to acquire the complete information of

the algorithm, and exploits its vulnerabilities. Also, the growing reliance of ML algorithms

on off-the-shelf information and communication technologies (ICTs) such as cloud computing

and the wireless networks [7] has made it even easier for adversaries to exploit the existing

known vulnerabilities of ICTs to achieve their goals. Security becomes a more critical issue in

the paradigm of distributed ML, since the learning consists of a large number of nodes that

communicate using ICTs, and its attack surface grows tremendously compared to its centralized

counterpart.

Hence, it is imperative to design secure distributed ML algorithms against cyber threats.

Current research endeavors have focused on two distinct directions. One is to develop robust

algorithms despite uncertainties in the dataset [8]–[10]. The second one is to improve detection

and prevention techniques to defend against cyber threats, e.g., [11]–[13]. These approaches

have mainly focused on centralized ML and computer networks, separately. The investigation of

security issues in the distributed ML over networks is lacking.

The challenges of developing secure distributed machine learning arise from the complexity

of the adversarial behaviors and the network effects of the distributed system. The attacker’s

strategies can be multi-stage. For instance, he can reach his target to modify training data

by launching multiple successful network attacks. Furthermore, the impact of an attack can

propagate over the network. Uncompromised nodes can be affected by the misinformation from

compromised nodes, leading to a cascading effect.

Traditional detection and defense strategies for centralized ML and networked systems are not

sufﬁcient to protect the distributed ML systems from attacks. To bridge this gap, this work aims

to develop a game-theoretic framework to model the interactions of an attacker and a defender to

assess and design security strategies in distributed systems. Game theory has been used to address

the security issues in centralized ML, e.g., [14], [15], and those in computer networks [16]–[18]

and cyber-physical networks [19]–[22]. The proposed game-theoretic model captures the network

structures of the distributed system and leads to fully distributed and iterative algorithms that

3

can be implemented at each node as defense strategies.

In particular, we use game models to study the impact on consensus-based distributed support

vector machines (DSVMs) of an attacker who is capable of modifying training data and labels. In

[23]–[25], we have built a game-theoretic framework to investigate the impacts of data poisoning

attacks to DSVMs, we have further proposed four defense methods and veriﬁed their effectiveness

with numerical experiments in [26]. In [27], we have proposed a game-theoretic framework to

model the interactions between a DSVM learner and an attacker who can modify the training

labels.

In this paper, we extend our previous works by studying a broad range of attack models and

classify them into two types. One is ML attacks, which exploit the vulnerabilities of ML algorithm,

and the other one is Net-attacks, which arise from the vulnerabilities of the communication

network. We then build the game-theoretic minimax problem to capture the competition between a

DSVM learner and an attacker who can modify training data and labels. With alternating direction

method of multipliers (ADMM) [28]–[30], we develop a fully distributed iterative algorithms

where each compromised node operates its own sub-max-problem for the attacker and sub-min-

problem for the learner. The game between a DSVM learner and an attacker can be viewed as a

collection of small sub-games associated with compromised nodes. This unique structure enables

the analysis of per-node-behaviors and the transmission of misleading information under the

game framework.

Numerical results on Spambase data set [31] are provided to illustrate the impact of different

attack models by the attacker. We ﬁnd that network plays a signiﬁcant role in the security of

DSVM, a balanced network with fewer nodes and higher degrees are less prone to attackers who

can control the whole system. We also ﬁnd that nodes with higher degrees are more vulnerable.

The distributed ML systems are found to be prone to network attacks even though the attacker

only adds small noise to information transmitted between neighboring nodes. A summary of

notations in this paper is provided in the following table.

II. PRELIMINARY

Consider a distributed linear support vector machines learner in the network with V = {1, ..,V }
representing the set of nodes. Node v ∈ V only communicates with his neighboring nodes
Bv ⊆ V . Note that without loss of generality, any two nodes in this network are connected by
a path, i.e., there is no isolated node in this network. At every node v, a labelled training set

4

V , v, Bv
wv, bv, rv

xvn, yvn

Xv, Yv

ωvu

θv

δvn

Summary of Notations

Set of Nodes, Node v, Set of Neighboring Nodes of Node v

Decision Variables at Node v

n-th Data and Label at Node v

Data Matrix and Label Matrix at Node v

Consensus Variable between Node v and Node u

Indicator Vector of Flipped Labels at Node v

Vector of Data Poisoning on the n-th Data at Node v

Dv = {(xvn, yvn)}Nv
n=1 of size Nv is available. The goal of the learner is to ﬁnd a maximum-margin
v + bv∗ at every node v ∈ V based on its local training
linear discriminant function gv(x) = xT w∗
set Dv. Consensus constraints w∗
V are used to force all local
decision variables {w∗
v} to agree across neighboring nodes. This approach enables each node
to classify any new input x to one of the two classes {+1, −1} without communicating Dv to
other nodes v(cid:48) (cid:54)= v. The discriminant function gv(x) can be obtained by solving the following

2 = ... = w∗

2 = ... = b∗

1 = w∗

1 = b∗

V , b∗

v, b∗

optimization problem:

Nv
∑
n=1

(cid:2)1 − yvn(wT

v xvn + bv)(cid:3)

+

min
{wv,bv}

1
(cid:107) wv (cid:107)2 +VCl ∑
2 ∑
v∈V
v∈V
s.t. wv = wu, bv = bu, ∀v ∈ V , u ∈ Bv.
In the above problem, the term (cid:2)1 − yvn(wT
v xvn + bv)(cid:3)
v xvn + bv)(cid:3) is the
hinge loss function, Cl is a tunable positive scalar for the learner. To solve Problem (1), we ﬁrst
deﬁne rv := [wT
v , bv]T , the augmented matrix Xv := [(xv1, ..., xvNv)T , 1v], and the diagonal label
matrix Yv := diag([yv1, ..., yvNv]). With these deﬁnitions, it follows readily that wv = (cid:98)Ip×(p+1)rv,
(cid:98)Ip×(p+1) = [Ip×p, 0p×1] is a p × (p + 1) matrix with its ﬁrst p columns being an identity matrix,
and its (p + 1) column being a zero vector. Thus, Problem (1) can be rewritten as

+ := max (cid:2)0, 1 − yvn(wT

(1)

min
{rv,ωvu}

1
2 ∑
v∈V

rT
v Πp+1rv +VCl ∑
v∈V
v ∈ V , u ∈ Bv,

rv = ωvu, ωvn = ru,

1T
v {1v − YvXvrv}+

s.t.

(2)

where the consensus variable ωvu is used to decompose the decision variable rv to its neighbors ru.
Note that Πp+1 = (cid:98)IT
p×(p+1)(cid:98)Ip×(p+1) is a (p + 1) × (p + 1) identity matrix with its (p + 1, p + 1)-st
entry being 0. The term {1v − YvXvrv}+ := max [0v, 1v − YvXvrv], which returns a vector of size
Nv. The algorithm of solving Problem (1) can be shown as the following lemma from Proposition

1 in [32].

5

Fig. 1. DSVM example [27]. At every iteration, each node v ﬁrst computes λv and rv by (3) and (4). Then, each node sends its

rv to its neighboring nodes. After that, each node computes αv using (5). Iterations continue until convergence.

Lemma 1. With arbitrary initialization r(0)

v , λ (0)
v

and α (0)

v = 0(p+1)×1, the iterations per node

are:

λ (t+1)
v

∈ arg max

0≤λv≤VCl1v

− 1

2 λ T

v YvXvU−1

v XT

v Yvλv+(1v + YvXvU−1
v

r(t+1)
v

= U−1
v

α (t+1)
v

= α (t)

v +

v

(cid:16)
v Yvλ (t+1)
XT
(cid:104)
r(t+1)
v

η
2 ∑
u∈Bv

− f(t)
v

(cid:17)
,

− r(t+1)
u

(cid:105)
,

T
f(t)
v )

λv,

(3)

(4)

(5)

where Uv = Πp+1 + 2η|Bv|Ip+1, f(t)

v = 2α (t)

v − η ∑u∈Bv(r(t)

v + r(t)
u ).

Note that ωvu has been solved directly and plugged into each equations. λv and αv are Lagrange

multipliers. The ADMM-DSVM algorithm is illustrated in Figure 1. Note that at any given
iteration t of the algorithm, each node v ∈ V computes its own local discriminant function g(t)
for any vector x as g(t)

v . Since we only need the decision variables rv for the

v (x) = [xT , 1]r(t)

v (x)

discriminant function, we use the DSVMv as a short-hand notation to represent iterations (3)-(5)

at node v:

r(t+1)
v

∈ DSVMv

(cid:16)
Xv, Yv, r(t)

v , {r(t)

u }u∈Bv

(cid:17)

.

(6)

The iterations stated in Lemma 1 are referred to as ADMM-DSVM. It is a fully decentralized

network operation. Each node v only shares decision variables rv to his neighboring nodes
u ∈ Bv. Other DSVM approaches include distributed chunking SVMs [33] and distributed parallel

SVMs [34], where support vectors (SVs) are exchanged between each nodes, and distributed

6

semiparametric SVMs, where the centroids of local SVMs are exchanged among neighboring

nodes [33]. In comparison, ADMM-DSVM has no restrictions on the network topology or the type

of data, and thus, we use it as an example to illustrate the impact of the attacker on distributed

learning algorithms.

III. ATTACK MODELS AND RELATED WORKS

In this section, we summarize and analyze possible attack models from the attacker. We start

by identifying the attacker’s goal and his knowledge. Based on the three critical characteristics

of information, the attacker’s goal can be captured as damageing the conﬁdentiality, integrity

and availability of the systems [35]. Damaging conﬁdentiality indicates that the attacker intends

to acquire private information. Damaging integrity means that the data or the models used by

the learner are modiﬁed or replaced by the attacker, which can not represent real information

anymore. Damaging availability indicates that the attacker, which is an unauthorized user, uses

the information or services provided by the learner. In this paper, we assume that the attacker

intends to damage the integrity of the distributed ML systems by modifying either the data or

the models of the learner.

The impacts of the attacker are affected by the attacker’s knowledge of the learner’s systems.

For example, an attacker may only know the data used by the learner, but he does not know the

algorithm the learner use; or he may only know some nodes of the network. To fully capture

the damages caused by the attacker, in this paper, we assume that the attacker has a complete

knowledge of the learner, i.e., the attacker knows the learner’s data and algorithm and the network

topology.

The attack models on distributed ML learner can be summarized into two main categories.

One is the machine learning type attacks (ML-attacks) [36], the other one is the network type

attacks (Net-attacks) [37]. In the ML-attacks, the attacker can exploit machine learning systems

which produces classiﬁcation or prediction errors. In the Net-attacks, an adversary attacks a

networked system to compromise the security of this system by actions, which leads to the leak

of private information or the failure of operations in this network. In this paper, we further divide

ML-attacks into two sub-categories, training attacks and testing attacks. Note that the attack

models described here are generally applicable to different machine learning algorithms. The

focus of this work is to investigate the impact of these attack models on DSVM, which provides

fundamental insights on the inherent vulnerability of distributed machine learning.

7

A. Training Attacks

In the training attacks, an adversary attacks the learner at the time when the learner solves

Problem (1). In these attacks, communications in the network may lead to unanticipated

results as misleading information from compromised nodes can be spread to and then used by

uncompromised nodes. One challenge of analyzing training attacks is that the consequences of

attacker’s actions may not be directly visible. For example, assuming that the attacker modiﬁes

some training data xvn in node v, the learner may not be able to ﬁnd out which data has been

modiﬁed, and furthermore, in distributed settings, the learner may not even be able to detect

which nodes are under attack. We further divide training attacks into three categories based on

the scope of modiﬁcations made by the attacker.

1) Training Labels Attacks: In this category, the attacker can modify the training labels
{yvn}n=1,...,Nv, where v ∈ Va. After training data with ﬂipped labels, the discriminant functions
will be prone to give wrong labels to the testing data. In early works [38], [39], centralized SVMs

under training label attacks have been studied, and robust SVMs have been brought up to reduce

the effects of such attacks. In this work, we further extend such attack models to distributed

SVM algorithms, and we use game theory to model the interactions between the DSVM learner

and the attacker. We verify the effects of the attacker with numerical experiments.

2) Training Data Attacks: In this category, the attacker modiﬁes the training data {xvn}n=1,...,Nv
on compromised nodes v ∈ Va. Since the training and testing data are assumed to be generated
from the same distribution, the discriminant function found with training data on distribution X
can be used to classify testing data from the same distribution X . However, after modifying
training data xvn into (cid:98)xvn, which belongs to a different distribution (cid:99)X , the discriminant function
with training such crafted data is suitable to classify data of distribution (cid:99)X . Thus, the testing
data x ∈ X are prone to be misclassiﬁed with this discriminant function.

The attacker can delete or craft several features [9], [10], change the training data of one class

[40], add noise to training data [41], change the distributions of training data [14], inject crafted

data [42], and so on. However, these works aim at centralized machine learning algorithms.

In distributed algorithms, the information transmissions between neighboring nodes can make

uncompromised nodes to misclassify testing data after training with information from compromised

nodes. In [24], an attacker aims at reducing a DSVM learner’s classiﬁcation accuracy by changing
training data xvn in node v ∈ Va into (cid:98)xvn := xvn − δvn. This work shows that the performances of

8

DSVM under adversarial environments are highly dependent on network topologies.

3) Training Models Attacks: The DSVM learner aims to ﬁnd the discriminant function with
the lowest risks by solving Problem (1) with local training sets Dv. However, the attacker may

change Problem (1) into a different problem or he can modify parameters in Problem (1). For

example, when the attacker changes Cl in Problem (1) into 0, the learner can only ﬁnd wv = 0,

which does not depend on the distribution of training data, and thus, the learner will misclassify

input testing data in the same distribution.

compromised node v ∈ Va. However, the consensus constraints w∗
... = b∗

With training attacks, the DSVM leaner will ﬁnd wrong decision variables (cid:98)w∗
2 = ... = w∗

v in
2 =
V force all the decision variables to agree on each other. Hence uncompromised nodes with
correct decision variables will be affected by misleading decision variables from compromised

v and (cid:98)b∗
1 = b∗
V , b∗

1 = w∗

nodes. As a result, the training process in the network can be damaged even the attacker only

attacks a small number of nodes.

B. Testing Attacks

In testing attacks, the attacker attacks at the time when the DSVM learner labels new input

x into +1 or −1 with w∗

v and b∗

v from the solved Problem (1). The attacker can conduct three

different operations in testing attacks.

Firstly, the attacker can directly change the given label y of the testing data x into −y, and

thus a wrong label is generated with this operation. Secondly, the attacker can replace the testing

data x with crafted (cid:98)x, or he can modify that into (cid:98)x = x − δ [43]. In such cases, the learner
gives the label of (cid:98)x rather than the label of x, which leads to misclassiﬁcation. Thirdly, the
attacker can modify or replace w∗
v, and thus the compromised discriminant
function (cid:98)gv(x) = xT
(cid:98)w∗
v + (cid:98)b∗
v will yield wrong results. A simple example is that the attacker can
v = −b∗
v and (cid:98)b∗
v = −w∗
set (cid:98)w∗
v, thus (cid:98)gv(x) = −gv(x), which leads to contrary predictions.

v into (cid:98)w∗

v and b∗

v and (cid:98)b∗

Testing attacks can cause disastrous consequences in centralized machine learning algorithms

as there is only one discriminant function. However, Testing attacks are weak in distributed

machine learning algorithms as uncompromised nodes can still give correct predictions, and

compromised nodes can be easily detected as they have higher classiﬁcation risks.

9

(a)

(b)

(c)

(d)

Fig. 2. Network attacks examples. (a) No attack. The learner uses a fully connected network with 3 nodes. (b) Node capture

attacks. The attacker controls Node 1 and make modiﬁcations on it. (c) Sybil attacks. Two red malicious nodes are generated by

the attacker and pretend that they are in the training network. (d) MITM attacks. The attacker creates adversarial nodes (depicted

in red) and pretend to be the other nodes on the connections.

C. Network Attacks

Network attacks can pose a signiﬁcant threat with potentially severe consequences on networked

systems [37]. Since distributed machine learning algorithms have been developed to solve large-

scale problems using networked systems [32], network attacks can also cause damages on

distributed learning systems. Network attacks include node capture attacks, Sybil attacks, and

Man-In-The-Middle attacks, which are illustrated in Figure 2.

In node capture attacks (Figure 2(b)), an attacker gains access into the network, and controls

a set of nodes, then he can alter both software programming and hardware conﬁguration, and

inﬂuence the outcome of network protocols [44]. When the attacker conducts node capture

attacks on a DSVM learner, he can modify either the data in the compromised nodes, or the

algorithms of the learner. Both of the modiﬁcations can lead to misclassiﬁcations in compromised

nodes. Moreover, the attacker can also send misleading information through network connections

between neighboring nodes, thus even uncompromised nodes can be affected by the attacker.

In Sybil attacks (Figure 2(c)), the attacker can create an adversarial node in the middle of a

connection, and talk to both nodes and pretends to be the other node [45]. If such attacks happen

on the network of a DSVM learner, nodes in compromised connections will receive misleading

information, and thus, the classiﬁcations in these nodes will be damaged.

In Man-in-the-Middle (MITM) attacks (Figure 2(d)), the attacker can create an adversarial

node in the middle of a connection, talk to both nodes and pretend to be the other node [46]. If

such attacks happen on the network of a DSVM learner, nodes in compromised connections will

10

receive misleading information, and thus classiﬁcations in these nodes will be damaged.

There are many other network attack models, such as botnet attacks [47] and denial of service

[6], which makes it challenging to analyze and defend attacker’s behaviors. Though distributed

systems improve the efﬁciency of the learning algorithms, however, the systems becomes more

vulnerable to network attacks. Thus, it is important to design secure distributed ML algorithms

against potential adversaries.

With various types of attack models, a distributed machine learning learner can be vulnerable in

a networked system. Thus, it is important to design secure distributed algorithms against potential

adversaries. Since the learner aims to increase the classiﬁcation accuracy, while the attacker seeks

to reduce that accuracy, game theory becomes a useful tool to capture the conﬂicting interests

between the learner and the attacker. The equilibrium of this game allows us to predict the

outcomes of a learner under adversary environments. In the next section, we build a game-theoretic

framework to capture the conﬂicts between a DSVM learner and an attacker who can modify

training labels and data.

IV. A GAME-THEORETIC MODELING OF ATTACKS

In this section, we use training attacks as an example to illustrate the game-theoretic modeling

of the interactions between a DSVM learner and an attacker. We mainly focus on modelling

training labels attacks, and the training data attacks.

A. Training Labels Attacks

In training labels attacks, the attacker controls a set of nodes Va, and aims at breaking the
training process by ﬂipping training labels yvn to (cid:98)yvn =: −yvn. To model the ﬂipped labels at each
node, we ﬁrst deﬁne the matrix of expanded training data (cid:98)Xv := [[xv1, ..., xvNv, xv1, ..., xvNv]T ,(cid:98)1v]
and the diagonal matrix of expanded labels (cid:98)Yv := diag([yv1, ..., yvNv, −yv1, ..., −yvNv]). Note that
the ﬁrst Nv data and the second Nv data are the same in the matrix of expanded training data.
We further introduce corresponding indicator vector θv := [θv1, ..., θv(2Nv)]T , where θvn ∈ {0, 1},
and θvn + θv(n+Nv) = 1, for n = 1, ..., Nv [39]. θv indicates whether the label has been ﬂipped, for
example, if θvn = 0 for n = 1, ..., Nv, i.e., θv(n+Nv) = 1, then the label of data xvn has been ﬂipped.
Note that θv = [1T

v ]T indicates that there is no ﬂipped label.

v , 0T

Since the learner aims to minimize the classiﬁcation errors by minimizing the objective function

in Problem (2), the attacker’s intention to maximize classiﬁcation errors can be captured as

maximizing that objective function. As a result, the interactions of the DSVM learner and the

attacker can be captured as a nonzero-sum game. Furthermore, since they have same objective

functions with opposite intentions, the nonzero-sum game can be reformulated into a zero-sum

game, which takes the minimax or max-min form [27]:

11

rT
v Πp+1rv

+

−VaCa ∑
v∈Va
(7a)

min
{rv,ωvu}

max
{θv}v∈Va

K ({rv, ωvu} , {θv}) := 1
(cid:110)

2 ∑
v∈V
(cid:111)

(cid:98)1v − (cid:98)Yv (cid:98)Xvrv

+VCl ∑
v∈V

θ T
v

s.t.

rv = ωvu, ωvn = ru,

v ]T ;

v , 0T
θv = [1T
qT
v θv ≤ Qv,
[INv, INv] θv = 1v,
θv ⊆ {0, 1}2Nv

v ∈ V , u ∈ Bv;
v ∈ Vl;
v ∈ Va;
v ∈ Va;
v ∈ Va.

v [0T
θ T

v , 1T

v ]T

(7)

(7b)

(7c)

(7d)

(7e)

2Nv

denotes a vector of size 2Nv, Vl denotes the set of uncompromised
In (7), (cid:98)1v = [1, 1, ..., 1]T
nodes. Note that the ﬁrst two terms in the objective function and Constraints (7a) are related
to the min-problem for the learner. When θv = [1T
v ]T , i.e., there are no ﬂipped labels, the
min-problem is equivalent to Problem (2). The last two terms in the objective function and

v , 0T

Constraints (7c)-(7e) are related to the max-problem for the attacker. Note that the last term of the

objection function represents the number of ﬂipped labels in compromised nodes. By minimizing

this term, the attacker aims to create the largest impact by ﬂipping the fewest labels. In Constraints
(7c), qv := [0, .., 0, qv(Nv+1), ..., qv(2Nv)]T , where qv(Nv+n) indicates the cost for ﬂipping the labels
of xvn in node v. This constraint indicates that the capability of the attacker is limited to ﬂip

labels with a boundary Qv at a compromised node v. Constraints (7d) show that the labels in

compromised nodes are either ﬂipped or not ﬂipped.

The Minimax-Problem (7) captures the learner’s intention to minimize the classiﬁcation errors

with attacker’s intention of maximizing that errors by ﬂipping labels. Problem (7) can be also

written into a max-min form, which captures the attacker’s intention to maximize the classiﬁcation

errors while the learner tries to minimize it. By minimax theorem, the minimax form in (7) is

equivalent to its max-min form. Thus, solving Problem (7) can be interpreted as ﬁnding the

saddle-point equilibrium of the zero-sum game between the learner and the attacker.

Deﬁnition 1. Let SL and SA be the action sets for the DSVM learner and the attacker
vu} , {θ ∗
respectively. Then, the strategy pair ({r∗
v }) is a saddle-point equilibrium solution

v, ω ∗

12

of the zero-sum game deﬁned by the triple Gz := (cid:104){L, A} , {SL, SA} , K(cid:105), if K ({r∗
K ({r∗

vu} , {θv}) ≤
v }) , ∀v ∈ Va, where K is the objective function in Problem

v }) ≤ K ({rv, ωvu} , {θ ∗

vu} , {θ ∗

v, ω ∗

v, ω ∗

(7).

To solve Problem (7), we construct the best response dynamics for the max-problem and

min-problem separately. The max-problem and the min-problem can be achieved by ﬁxing
{r∗

v }, respectively. With solving both problems in a distributed way, we achieve

vu} and {θ ∗

v, ω ∗

the fully distributed iterations of solving Problem (7) as
v {(cid:98)1v − (cid:98)Yv (cid:98)Xvr(t)

∈ arg max

θ (t+1)
v

VClθ T

θv

v }+ −VaCaθ T

v [0T

v , 1T

v ]T

s.t.

qT
v θv ≤ Qv,
[INv, INv] θv = 1v,
(cid:98)0v ≤ θv ≤ (cid:98)1v.

(8a)

(8b)

(8c)

r(t+1)
v

∈ DSVMv,L

(cid:16)
(cid:98)Xv, (cid:98)Yv, r(t)

v , {r(t)

u }u∈Bv

(8)

(9)

(cid:17)

(cid:12)
(cid:12)θ (t+1)
v

Problem (8) is a linear programming problem. Note that integer constraint (7e) has been further
relaxed into (8c). It captures the attacker’s actions in compromised nodes v ∈ Va. Note that each

node can achieve their own θv without transmitting information to other nodes. Problem (9)

comes from the min-part of Problem (7), which can be solved using similar method in [32] with

ADMM [28]. Note that DSVMv,L differs from DSVMv only in the feasible set of the Lagrange
multipliers λv, In DSVMv,L, 0 ≤ λv ≤ VClθ (t+1)
been ﬂipped, and it comes from the attacker’s Problem (8).

indicates whether the label has

, where θ (t+1)

v

v

With (8) and (9), the algorithm of solving Problem (7) can be summarized: Each compromised

node computes θv via (8), then compromised and uncompromised nodes compute rv via (6) and

(9), respectively. The iterations will go until convergence.

B. Training Data Attacks

In training data attacks, the attacker has the ability to modify the training data xvn into
(cid:98)xvn := xvn − δvn in compromised node v ∈ Va. Following a similar method in training label attacks,

we can capture the interactions of the DSVM learner and the attacker as a zero-sum game which

13

is shown as follows:

min
{wv,bv}

max
{δvn}

K ({wv, bv} , {δvn}) := 1

(cid:107)wv(cid:107)2

2 ∑
v∈V
v xvn + bv)(cid:3)

+

(cid:2)1 − yvn(wT

+VlCl ∑
v∈Vl

+VaCl ∑
v∈Va

Nv
∑
n=1
Nv
∑
n=1
Nv
∑
n=1

(cid:107)δvn(cid:107)0

−VaCa ∑
v∈Va
wv = wu, bv = bu,
∑Nv
n=1 (cid:107) δvn (cid:107)2≤ Cv,δ ,

s.t.

∀v ∈ V , u ∈ Bv;
∀v ∈ Va.

(10a)

(10b)

(cid:2)1 − yvn(wT

v (xvn − δvn) + bv)(cid:3)

+

(10)

Note that in the last term, l0 norm (cid:107)x(cid:107)0 := |{i : xi (cid:54)= 0}| denotes the number of elements which
are changed by the attacker, and deleting it captures the attacker’s intentions to maximizing the

classiﬁcation errors with changing least number of elements. Constraint (10b) indicates that the

sum of modiﬁactions in node v are bounded by Cv,δ . Following a similar method in training
labels attacks, we can construct the iterations of solving Problem (10) as follows [23]–[25]:
VaClr(t)T

δ (t+1)
v

v (cid:98)IT

p×(p+1)δv − 1T

v sv

∈ arg max
{δv,sv}
VaCaδv ≤ sv,

s.t.

VaCaδv ≥ −sv,
(cid:107) δv (cid:107)2≤ Cv,δ .

(11a)

(11b)

(11c)

r(t+1)
v

∈ DSVMv,D

(cid:16)
Xv, Yv, r(t)

v , {r(t)

u }u∈Bv

(11)

(12)

(cid:17)

(cid:12)
(cid:12)δ (t+1)
v

Note that here δvn has been summed into δv, which captures the modiﬁcations in node v.

Constraints (11ab) and the last term of the objective function is the relaxation of the l0 norm.
Note that comparing to DSVMv, DSVMv,D has f(t+1)
where δ (t+1)
comes from Problem (11).

v − η ∑u∈Uv(r(t)

= VaClδ (t+1)

v + r(t)
u ),

+ 2α (t)

v

v

v

In this section, we have modeled the conﬂicting interests of a DSVM learner and an attacker

using a game-theoretic framework. The interactions of them can be captured as a zero-sum game

where a minimax problem is formulated. The minimization part captures the learner’s intentions

to minimize classiﬁcation errors, while the maximization part captures the attacker’s intentions

to maximize that errors with making less modiﬁcations, i.e., ﬂipping labels and modifying data.

By constructing the min-problem for the learner and max-problem for the attacker separately, the

minimax problem can be solved with the best response dynamics. Furthermore, the min-problem

14

and max-problem can be solved in a distributed way with Va Sub-Max-Problems (8) and (11),

and V Sub-Min-Problems (9) and (12). Combing the iterations of solving these problems, we

have the fully distributed iterations of solving the Minimax-Problems (7) and (10). The nature of

this iterative operations provides real-time mechanisms for each node to reacts to its neighbors

and the attacker. Since each node operates its own sub-max-problem and sub-min-problems, the

game between a DSVM learner and an attacker now can be represented by Va sub-games in

compromised nodes. This structure provides us tools to analyze per-node-behaviors of distributed

algorithms under adversarial environments. The transmissions of misleading information (cid:98)rv can
be also analyzed via the connections between neighboring games.

V. IMPACT OF ATTACKS

In this section, we present numerical experiments on DSVM under adversarial environments.

We will verify the effects of both the training attacks and the network attacks. The performance

of DSVM is measured by both the local and global classiﬁcation risks. The local risk at node v

at step t is deﬁned as follows:

R(t)
v

:=

1
Nv

Nv
∑
n=1

1
2

(cid:12)
(cid:12)yvn − (cid:101)y(t)
(cid:12)

vn

(cid:12)
(cid:12)
(cid:12),

(13)

where yvn is the true label, (cid:101)y(t)
samples in node v. The global risk is deﬁned as follows:

vn is the predicted label, and Nv represents the number of testing

R(t)
G :=

1
∑v∈V Nv

∑
v∈V

Nv
∑
n=1

1
2

(cid:12)
(cid:12)yvn − (cid:101)y(t)
(cid:12)

vn

(cid:12)
(cid:12)
(cid:12),

(14)

A higher global risk shows that there are more testing samples being misclassiﬁed, i.e., a worse

performance of DSVM.

We deﬁne the degree of a node v as the actual number of neighboring nodes |Bv| divided by
the most achievable number of neighbors |V | − 1. The normalized degree of a node is always

larger than 0 and less or equal to 1. A higher degree indicates that the node has more neighbors.

We further deﬁne the degree of the network as the average degrees of all the nodes.

A. DSVM Under Training Attacks

Recall the attacker’s constraints qT

v θv ≤ Qv in training labels attacks, where qv := [0, .., 0, qv(Nv+1), ..., qv(2Nv)]T

indicates the cost for ﬂipping labels in node v. This constraint indicates that the attacker’s

modiﬁcations in node v are bounded by Qv. Without loss of generality, we assume that

15

(a)

(b)

Fig. 3. ADMM-DSVM under training-label-attacks with |Va| = 4, Q = 30, and Ca = 0.01.

(a)

(b)

Fig. 4. ADMM-DSVM under training-label-attacks with |Va| = 1, Q = 30, and Ca = 0.01.

qv := [0, .., 0, 1, ..., 1]T , and thus, the constraint now indicates that the number of ﬂipped labels in

node v are bounded by Qv. We also assume that the attacker has the same Qv = Q and Cv,δ = Cδ
in every compromised node v ∈ Va. Note that we assume that the learner has Cl = 1 and η = 1
in every experiments.

Figures 3-6 show the results when the attacker has different capabilities to ﬂip labels in a

fully connected network with 6 nodes [27]. Each node contains 40 training samples and 500

testing samples from the same global training dataset with labels +1 and −1. The data are

generated from two-dimensional Gaussian distributions with mean vectors [1, 1] and [2, 2], and
same covariance matrix [1, 0; 0, 1]. |Va| indicates the number of compromised nodes. Q indicates

-2-10123456-2-1012345678No attackAttack20406080100120140160180200Iterations0.150.20.250.30.350.40.450.50.550.60.65RiskNo attack, GlobalNo attack, Node 1No attack, Node 5Attack, GlobalAttack, Node 1Attack, Node 5-2-10123456-2-1012345678No attackAttack20406080100120140160180Iterations0.150.20.250.30.350.40.450.50.550.60.65RiskNo attack, GlobalNo attack, Node 1No attack, Node 5Attack, GlobalAttack, Node 1Attack, Node 516

(a)

(b)

Fig. 5. ADMM-DSVM under training-label-attacks with |Va| = 4, Q = 10, and Ca = 0.01.

(a)

(b)

Fig. 6. ADMM-DSVM under training-label-attacks with |Va| = 4, Q = 30, and Ca = 5.

the largest number of training samples that can be ﬂipped in each node. Ca indicates the cost

parameter. Blue ﬁlled circles and red ﬁlled circles are the original samples with class +1 and

−1, respectively. Blue hollow circles and red hollow circles are the ﬂipped samples with class

+1 and −1, i.e, originally −1 and +1 respectively. We can see from the ﬁgures that when the

attacker attacks more nodes, the equilibrium global risk is higher, and the uncompromised nodes

have higher risks (i.e., Figures 3 and 4). When Q is large, the attacker can ﬂip more labels, and

thus the risks are higher (i.e., Figures 3 and 5). When Ca is large, the cost for the attacker to

take an action is too high, and thus there is no impact on the learner (i.e., Figures 3 and 6).

Note that, when the attacker has large capabilities, i.e., larger Q, smaller Ca or larger Va, even

-2-10123456-2-1012345678No attackAttack20406080100120140160180200Iterations0.150.20.250.30.350.40.450.50.550.60.65RiskNo attack, GlobalNo attack, Node 1No attack, Node 5Attack, GlobalAttack, Node 1Attack, Node 5-2-10123456-2-1012345678No attackAttack20406080100120140160180200Iterations0.150.20.250.30.350.40.450.50.550.60.65RiskNo attack, GlobalNo attack, Node 1No attack, Node 5Attack, GlobalAttack, Node 1Attack, Node 5AVERAGE EQUILIBRIUM CLASSIFICATION RISKS (%) OF DSVM USING SPAMBASE DATASET [31] IN NETWORK 1 AND

NETWORK 2.

TABLE I

17

NET

1

1L

1D

2

2L

2D

RISK

11.6

32.3

42.2

10.6

29.4

39.3

STD

1.6

0.6

2.6

0.6

0.3

1.1

AVERAGE EQUILIBRIUM CLASSIFICATION RISKS (%) OF DSVM USING SPAMBASE DATASET [31] IN NETWORK 3 AND

NETWORK 4.

TABLE II

NET

3

3LA

3LB

3DA

3DB

4

4L

4D

RISK

11.7

29.5

26.9

36.4

34.6

13.5

35.0

47.0

STD

1.5

0.6

1.2

0.9

0.8

1.8

0.9

2.5

uncompromised nodes, e.g., Node 5, has higher risks.

Tables I and II show the results when the learner trains on different networks. L and D indicate

training labels attacks and training data attacks, respectively. Networks 1 , 2 and 3 have 6

nodes, where each node contains 40 training samples. Network 1 and 2 are balanced networks

with degree 0.4 and 1, respectively. The normalized degrees of each node in Network 3 are

1, 0.4, 0.4, 0.2, 0.2, 0.2. Network 4 has 12 nodes where each node contains 20 training samples

and has 2 neighbors. In 1L (resp. 1D) and 2L (resp. 2D), the attacker attacks 6 nodes with
Q = 20 (resp. Cδ = 1010). In 3LA (resp. 3DA) and 3LB (resp. 3DB), the attacker attacks 3 (resp.
2) nodes with higher degrees and lower degrees with Q = 20 (resp. Cδ = 1010). In 4L (resp. 4D),
the attacker attacks 12 nodes with Q = 10 (resp. Cδ = 0.5 × 1010). Comparing 1L (resp. 1D) with
2L (resp. 2D), we can see that network with higher degree has a lower risk when there is an

attacker. From 3LA (resp. 3DA) and 3LB (resp. 3DB), we can tell that the risks are higher when

nodes with higher degrees are under attack. From 1L (resp. 1D) and 4L (resp. 4D), we can see

that network with more nodes has a higher risk when it is under attack. Thus, from Tables I and

II, we can see that network topology plays an important role in the security of the ML learner.

18

(a) Network 1

(b) Network 2

Fig. 7. ADMM-DSVM under network attacks.

B. DSVM Under Network Attacks

In this subsection, we use numerical experiments to illustrate the impact of network attacks.
In node capture attacks, we assume that the attacker controls a set of nodes Va, and he has the

ability to add noise to the decision variables rv. In Sybil attacks, the attacker can obtain access
to compromised node v ∈ Va, then he generates another malicious node to exchange information

with the compromised node. Instead of sending random information, which can be easily detected,

we assume that he sends a perturbed rv to make compromised node believe that this is a valid

information, where rv comes from the compromised node v. In MITM attacks, we assume that

the attacker creates adversarial nodes in the middle of a connection, and he receives rv from

both sides, but he sends a perturbed rv to the other sides.

In the experiment in Figure 7, we assume that the elements of the noise vector are generated by

a uniform distribution in [0, R], where R indicates the size of the noise. Network 1 and 2 are fully

connected networks with 4 nodes and 8 nodes, respectively. In node capture attacks and Sybil

attacks, the x-axis indicates the number of compromised nodes. The attacker has R = 1, 0.01 for

node capture attacks and Sybil attacks, respectively. In MITM attacks, the the x-axis represents

the number of compromised connections, especially, 1 indicates only 1 connection has been

broken, 4 and 8 for network 1 and 2 indicates that all the connections, i.e., 6 and 28 connections,

have been attacked. Note that the attacker has R = 0.05, 0.02 for network 1 and 2 in MITM

attacks, respectively . From the ﬁgure, we can see that when the attacker attacks more nodes or

connections, the risk becomes larger. Note that, when more than half of the nodes or connections

11.522.533.5400.20.40.60.81Risk  Node Capture AttacksSybil AttacksMITM Attacks1234567800.20.40.60.81Risk  Node Capture AttacksSybil AttacksMITM Attacks19

are compromised, the DSVM will completely fail, i.e, the classiﬁer is the same as the one that

randomly labels testing data.

VI. DISCUSSIONS AND FUTURE WORK

Distributed machine learning (ML) algorithms are ubiquitous but inherently vulnerable to

adversaries. This paper has investigated the security issue of distributed machine learning in

adversarial environments. Possible attack models have been analyzed and summarized into

machine learning type attacks and network type attacks. Their effects on distributed ML have

been studied using numerical experiments.

One major contribution of this work is the investigation of the security threats in distributed

ML. We have shown that the consequence of both ML-type attacks and Network-type attacks

can exacerbate in distributed ML. We have established a game-theoretic framework to capture

the strategic interactions between a DSVM learner and an attacker who can modify training data

and training labels. By using the technique of ADMM, we have developed a fully distributed

iterative algorithms that each node can respond to his neighbors and the adversary in real-time.

The developed framework is broadly applicable to a broad range of distributed ML algorithms

and attack models.

Our work concludes that network topology plays a signiﬁcant role in the security of distributed

ML. We have recognized that a balanced network with fewer nodes and a higher degree is less

vulnerable to the attacker who controls the whole system. We have shown that nodes with higher

degrees can cause more severe damages to the system when under attack. Also, network-type

attacks can increase the risks of the whole networked systems even though the attacker only add

small noise to a small subset of nodes.

Our work is a starting point to explore the security of distributed ML. In the future, we aim to

extend the game-theoretic framework to capture various attack models and different distributed

ML algorithms. We have proposed four defense methods in [26] to protect DSVM against training

data attacks, and we intend to explore other defense strategies for distributed ML by leveraging

techniques developed in robust centralized algorithms [8], [38] and detection methods [12], [48].

Furthermore, we could use cyber insurance to mitigate the cyber data risks from attackers [49],

[50].

20

REFERENCES

[1] R. Zhang, Strategic Cyber Data Risk Management over Networks: From Proactive Defense to Cyber Insurance. PhD thesis,

New York University Tandon School of Engineering, 2020.

[2] R. Bekkerman, M. Bilenko, and J. Langford, Scaling up machine learning: Parallel and distributed approaches. Cambridge

University Press, 2011.

[3] J. Metzger, M. Schillo, and K. Fischer, “A multiagent-based peer-to-peer network in java for distributed spam ﬁltering,” in

Multi-Agent Systems and Applications III, pp. 616–625, Springer, 2003.

[4] E. Camponogara and W. Kraus Jr, “Distributed learning agents in urban trafﬁc control,” in Progress in artiﬁcial intelligence,

pp. 324–335, Springer, 2003.

[5] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. Rubinstein, U. Saini, C. Sutton, J. Tygar, and K. Xia, “Misleading

learners: Co-opting your spam ﬁlter,” in Machine learning in cyber trust, pp. 17–51, Springer, 2009.

[6] A. D. Wood, J. Stankovic, et al., “Denial of service in sensor networks,” Computer, vol. 35, no. 10, pp. 54–62, 2002.

[7] L. Drevin, H. A. Kruger, and T. Steyn, “Value-focused assessment of ict security awareness in an academic environment,”

Computers & Security, vol. 26, no. 1, pp. 36–43, 2007.

[8] A. Globerson and S. Roweis, “Nightmare at test time: robust learning by feature deletion,” in Proceedings of the 23rd

international conference on Machine learning, pp. 353–360, ACM, 2006.

[9] O. Dekel, O. Shamir, and L. Xiao, “Learning to classify with missing and corrupted features,” Machine learning, vol. 81,

no. 2, pp. 149–178, 2010.

[10] L. Maaten, M. Chen, S. Tyree, and K. Q. Weinberger, “Learning with marginalized corrupted features,” in Proceedings of

the 30th International Conference on Machine Learning (ICML-13), pp. 410–418, 2013.

[11] D. N. Serpanos and R. J. Lipton, “Defense against man-in-the-middle attack in client-server systems,” in Computers and

Communications, 2001. Proceedings. Sixth IEEE Symposium on, pp. 9–14, IEEE, 2001.

[12] B. N. Levine, C. Shields, and N. B. Margolin, “A survey of solutions to the sybil attack,” University of Massachusetts

Amherst, Amherst, MA, 2006.

[13] E. Kohno, T. Ohta, and Y. Kakuda, “Secure decentralized data transfer against node capture attacks for wireless sensor

networks,” in Autonomous Decentralized Systems, 2009. ISADS’09. International Symposium on, pp. 1–6, IEEE, 2009.

[14] W. Liu and S. Chawla, “A game theoretical model for adversarial learning,” in Data Mining Workshops, 2009. ICDMW’09.

IEEE International Conference on, pp. 25–30, IEEE, 2009.

[15] M. Kantarcıo˘glu, B. Xi, and C. Clifton, “Classiﬁer evaluation and attribute selection against active adversaries,” Data

Mining and Knowledge Discovery, vol. 22, no. 1-2, pp. 291–335, 2011.

[16] P. Michiardi and R. Molva, “Game theoretic analysis of security in mobile ad hoc networks,” 2002.

[17] K.-w. Lye and J. M. Wing, “Game strategies in network security,” International Journal of Information Security, vol. 4,

no. 1, pp. 71–86, 2005.

[18] Y. Huang and Q. Zhu, “A differential game approach to decentralized virus-resistant weight adaptation policy over complex

networks,” IEEE Transactions on Control of Network Systems, 2019.

[19] J. Chen, C. Touati, and Q. Zhu, “A dynamic game approach to strategic design of secure and resilient infrastructure network,”

IEEE Transactions on Information Forensics and Security, vol. 15, pp. 462–474, 2019.

[20] J. Chen and Q. Zhu, “Interdependent strategic security risk management with bounded rationality in the internet of things,”

IEEE Transactions on Information Forensics and Security, vol. 14, no. 11, pp. 2958–2971, 2019.

[21] J. Chen, C. Touati, and Q. Zhu, “Optimal secure two-layer iot network design,” IEEE Transactions on Control of Network

Systems, 2019.

21

[22] Y. Nugraha, T. Hayakawa, A. Cetinkaya, H. Ishii, and Q. Zhu, “Subgame perfect equilibrium analysis for jamming attacks

on resilient graphs,” in 2019 American Control Conference (ACC), pp. 2060–2065, IEEE, 2019.

[23] R. Zhang and Q. Zhu, “A game-theoretic approach to design secure and resilient distributed support vector machines,”

IEEE transactions on neural networks and learning systems, vol. 29, no. 11, pp. 5512–5527, 2018.

[24] R. Zhang and Q. Zhu, “Secure and resilient distributed machine learning under adversarial environments,” in Information

Fusion (Fusion), 2015 18th International Conference on, pp. 644–651, IEEE, July 2015.

[25] R. Zhang and Q. Zhu, “Student research highlight: Secure and resilient distributed machine learning under adversarial

environments,” IEEE Aerospace and Electronic Systems Magazine, vol. 31, no. 3, pp. 34–36, Mar. 2016.

[26] R. Zhang and Q. Zhu, “Game-theoretic defense of adversarial distributed support vector machines,” Journal of Advances in

Information Fusion, vol. 14, no. 1, pp. 3–21, 2019.

[27] R. Zhang and Q. Zhu, “A game-theoretic analysis of label ﬂipping attacks on distributed support vector machines,” in

Information Sciences and Systems (CISS), 2017 51st Annual Conference on, pp. 1–6, IEEE, Mar. 2017.

[28] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical learning via the alternating

direction method of multipliers,” Foundations and Trends R(cid:13) in Machine Learning, vol. 3, no. 1, pp. 1–122, 2011.

[29] R. Zhang and Q. Zhu, “Consensus-based transfer linear support vector machines for decentralized multi-task multi-agent

learning,” in 2018 52nd Annual Conference on Information Sciences and Systems (CISS), pp. 1–6, IEEE, 2018.

[30] R. Zhang and Q. Zhu, “Consensus-based distributed discrete optimal transport for decentralized resource matching,” IEEE

Transactions on Signal and Information Processing over Networks, vol. 5, no. 3, pp. 511–524, 2019.

[31] A. Frank and A. Asuncion, “UCI machine learning repository [http://archive. ics. uci. edu/ml]. irvine, ca: University of

california,” School of Information and Computer Science, vol. 213, 2010.

[32] P. A. Forero, A. Cano, and G. B. Giannakis, “Consensus-based distributed support vector machines,” The Journal of

Machine Learning Research, vol. 11, pp. 1663–1707, 2010.

[33] A. Navia-V´azquez and E. Parrado-Hernandez, “Distributed support vector machines,” Neural Networks, IEEE Transactions

on, vol. 17, no. 4, pp. 1091–1097, 2006.

[34] Y. Lu, V. Roychowdhury, and L. Vandenberghe, “Distributed parallel support vector machines in strongly connected

networks,” Neural Networks, IEEE Transactions on, vol. 19, no. 7, pp. 1167–1178, 2008.

[35] J. McCumber, “Information systems security: A comprehensive model,” in Proceedings of the 14th National Computer

Security Conference, 1991.

[36] M. Barreno, B. Nelson, A. D. Joseph, and J. Tygar, “The security of machine learning,” Machine Learning, vol. 81, no. 2,

pp. 121–148, 2010.

[37] S.-D. Chi, J. S. Park, K.-C. Jung, and J.-S. Lee, “Network security modeling and cyber attack simulation methodology,” in

Information Security and Privacy, pp. 320–333, Springer, 2001.

[38] B. Biggio, B. Nelson, and P. Laskov, “Support vector machines under adversarial label noise.,” in ACML, pp. 97–112, 2011.

[39] H. Xiao, H. Xiao, and C. Eckert, “Adversarial label ﬂips attack on support vector machines.,” in ECAI, pp. 870–875, 2012.

[40] N. Dalvi, P. Domingos, S. Sanghai, D. Verma, et al., “Adversarial classiﬁcation,” in Proceedings of the tenth ACM SIGKDD

international conference on Knowledge discovery and data mining, pp. 99–108, ACM, 2004.

[41] B. Biggio, I. Corona, G. Fumera, G. Giacinto, and F. Roli, “Bagging classiﬁers for ﬁghting poisoning attacks in adversarial

classiﬁcation tasks,” in Multiple Classiﬁer Systems, pp. 350–359, Springer, 2011.

[42] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support vector machines,” arXiv preprint arXiv:1206.6389,

2012.

[43] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c, P. Laskov, G. Giacinto, and F. Roli, “Evasion attacks against

machine learning at test time,” in Machine Learning and Knowledge Discovery in Databases, pp. 387–402, Springer, 2013.

22

[44] P. Tague and R. Poovendran, “Modeling node capture attacks in wireless sensor networks,” in Communication, Control,

and Computing, 2008 46th Annual Allerton Conference on, pp. 1221–1224, IEEE, 2008.

[45] J. Newsome, E. Shi, D. Song, and A. Perrig, “The sybil attack in sensor networks: analysis & defenses,” in Proceedings of

the 3rd international symposium on Information processing in sensor networks, pp. 259–268, ACM, 2004.

[46] Y. Desmedt, “Man-in-the-middle attack,” in Encyclopedia of Cryptography and Security, pp. 759–759, Springer, 2011.

[47] L. Zhang, S. Yu, D. Wu, and P. Watters, “A survey on latest botnet attack and defense,” in Trust, Security and Privacy in

Computing and Communications (TrustCom), 2011 IEEE 10th International Conference on, pp. 53–60, IEEE, 2011.

[48] R. Chen, E. K. Lua, J. Crowcroft, W. Guo, L. Tang, and Z. Chen, “Securing peer-to-peer content sharing service from

poisoning attacks,” in Peer-to-Peer Computing, 2008. P2P’08. Eighth International Conference on, pp. 22–29, IEEE, 2008.

[49] R. Zhang and Q. Zhu, “FlipIn: A game-theoretic cyber insurance framework for incentive-compatible cyber risk management

of internet of things,” IEEE Transactions on Information Forensics and Security, 2019. Forthcoming.

[50] R. Zhang, Q. Zhu, and Y. Hayel, “A bi-level game approach to attack-aware cyber insurance of computer networks,” IEEE

Journal on Selected Areas in Communications, vol. 35, no. 3, pp. 779–794, 2017.

