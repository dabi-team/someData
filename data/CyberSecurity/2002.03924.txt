0
2
0
2

b
e
F
0
1

]

G
L
.
s
c
[

1
v
4
2
9
3
0
.
2
0
0
2
:
v
i
X
r
a

Playing to Learn Better: Repeated Games for Adversarial Learning with Multiple
Classiﬁers

Prithviraj Dasgupta, Joseph B. Collins, Michael McCarrick
Distributed Intelligent Systems Section (Code 5583)
Info. Mgmt. & Decision Arch. (IMDA) Branch
Information Technology Division
U. S. Naval Research Laboratory, Washington, D.C.

Abstract

We consider the problem of prediction by a machine learn-
ing algorithm, called learner, within an adversarial learning
setting. The learner’s task is to correctly predict the class of
data passed to it as a query. However, along with queries
containing clean data, the learner could also receive mali-
cious or adversarial queries from an adversary. The objective
of the adversary is to evade the learner’s prediction mecha-
nism by sending adversarial queries that result in erroneous
class prediction by the learner, while the learner’s objec-
tive is to reduce the incorrect prediction of these adversar-
ial queries without degrading the prediction quality of clean
queries. We propose a game theory-based technique called
a Repeated Bayesian Sequential Game where the learner in-
teracts repeatedly with a model of the adversary using self
play to determine the distribution of adversarial versus clean
queries. It then strategically selects a classiﬁer from a set of
pre-trained classiﬁers that balances the likelihood of correct
prediction for the query along with reducing the costs to use
the classiﬁer. We have evaluated our proposed technique us-
ing clean and adversarial text data with deep neural network-
based classiﬁers and shown that the learner can select an ap-
propriate classiﬁer that is commensurate with the query type
(clean or adversarial) while remaining aware of the cost to
use the classiﬁer.

Introduction
machine

learn-
Adversarial
ing (Vorobeychik and Kantarcioglu 2018) is an important
problem in machine learning based prediction systems
such as email spam ﬁlters, online recommender systems,
text classiﬁer and sentiment analysis techniques used on
social media, and, automatic video and image classiﬁers.
The main problem in adversarial learning is to prevent an
adversary from bypassing an ML-based predictive model
such as a classiﬁer by sending engineered, malicious data
instances called adversarial examples. These attacks, called
evasion attacks, could enable a malicious adversary to
subvert the learner’s ML model and possibly get access
to critical resources being protected by the learner. For
instance, in the context of malware detection, an adversary
could try to surreptitiously insert ill-formed PDF objects
into a valid PDF ﬁle to convert it into a malware that could

bypass a ML-based malware detector and subsequently
crash an Internet browser attempting to read the corrupted
PDF ﬁle. Researchers have proposed techniques including
adversarial
training (Yuan et al. 2019) and game theory
based techniques (Dasgupta and Collins 2019) to address
the problem of adversarial
learning. These techniques
employ an approach called classiﬁer hardening on a single
classiﬁer where the decision boundary of the classiﬁer
is reﬁned over time via re-training with adversarial data.
However,
improving the robustness of single classiﬁer
hardening techniques is an open problem and these tech-
niques are still been known to be susceptible to adversarial
attacks (Madry et al. 2017). Moreover, classiﬁer hardening
techniques do not explicitly align costs to harden the
classiﬁer (e.g., costs to acquire adversarial training data,
and, time and costs to harden it with adversarial data)
with the data being classiﬁed. For instance, for classifying
clean data, a classiﬁer hardened over several batches of
adversarial data might be excessive, as a classiﬁer that is not
hardened might achieve similar performance. In this paper,
we posit that the costs of a classiﬁer-based ML model to
adversarial attacks can be improved without deteriorating
classiﬁcation accuracy, if instead of using a single classiﬁer,
we use multiple classiﬁers that are hardened separately
against attacks of different strengths. Our idea is based on
the well-known result of Wolpert’s theorem (Wolpert 2002)
that there is not a single classiﬁer which can be optimal for
all classiﬁcation tasks and multiple, combined classiﬁers
could outperform the best individual classiﬁer. The main
challenge with using multiple classiﬁers is to determine the
appropriate pairing between a query sent to the classiﬁer,
with either clean or adversarial data of different attacks
strengths, and a commensurate classiﬁer from the collection
of classiﬁers to handle the query most effectively, e.g., with
least likelihood of classiﬁcation errors and while aligning
classiﬁer hardening costs with the query’s attack strength.
A further wrinkle to the problem is that the classiﬁer is not
aware whether the query is with clean data from a legitimate
client versus with adversarial data from an attacker. To ad-
dress this problem, we propose a game theoretic framework
called a Repeated Bayesian Sequential Game with self play
between a learner and an adversary. The outcome of the

 
 
 
 
 
 
game strategically selects an appropriate classiﬁer for the
learner. Our proposed formulation enables us to realize
several practical aspects of learner-attacker interactions
including uncertainty of the learner about the strengths of
different attacks, costs to the learner and attacker to train the
classiﬁer and generate adversarial examples respectively,
rewards and penalties to attacker and learner for successes in
their attacks and defenses respectively. Finally, a Bayesian
game based representation enables our approach to handle
asymmetric interactions between the learner and its clients
for both non-competitive (legitimate clients, clean queries)
and competitive (attackers, adversarial queries) settings. To
the best of our knowledge, our work is one of the ﬁrst at-
tempts at using multiple classiﬁers deployed strategically to
tackle the adversarial learning problem. We have validated
our approach within a learner-adversary setting where the
adversary generates queries with both clean and adversarial
text data with different attack strengths while the learner’s
classiﬁers use deep network models for classiﬁcation. Our
results show that the learner can successfully converge to
the distribution of different attacks types of the adversary
and can strategically select different classiﬁers to reduce
the overall classiﬁcation cost without deteriorating the
classiﬁcation accuracy.

Related Work
Early work in adversarial learning modeled the interaction
between the learner and adversary as a competitive, 2-player
game (Dalvi et al. 2004) (Globerson and Roweis 2006). The
game is solved as a constrained optimization problem and
its solution provides an attack strategy for the attacker,
e.g., which subset of features to modify in the data sent
to the learner’s classiﬁer to effect an incorrect predic-
tion by the classiﬁer. While for the learner, a response
strategy determines appropriate parameter values for its
ML model, e.g., weights for regression or for a neural
network model, so that the attack would be unsuccess-
ful. Subsequently, researchers extended the adversarial
learning game using different formalisation including a
sequential game (Br¨uckner, Kanzow, and Scheffer 2012),
the
a Bayesian game
(Grosshans et al. 2013) where
the
information about
has
learner
at-
tacker’s
problem
optimization
strategy,
(Alfeld, Zhu, and Barford 2017),
(Mei and Zhu 2015)
strategic classiﬁcation (Dong et al. 2018) and random-
ization over strategies of
the learner and the adver-
sary (Bul`o et al. 2017). In most of these techniques, the
learner’s strategy at each instance of the game is to adjust its
model parameters. While that might be practical for smaller
models with few parameters, as the model size increases,
e.g., for a deep network with thousands of parameters, the
strategy might become infeasible to realize in practice. In
contrast, in our work, we use the strategy output of the game
to select an appropriate classiﬁer for the learner from an
existing, pre-trained set of classiﬁers.

incomplete

bi-level

a

In adversarial training, the learner’s ML model is trained
with both clean and adversarial data to improve its capabil-
ity to correctly classify adversarial data. Unlike game theory
representations, adversarial
training techniques do not
explicitly model costs, penalties and rewards for learner and
attacker. In (Tram`er et al. 2017), researchers have proposed
an ensemble of ML models to generate adversarial examples
and then use those examples from different models to harden
a classiﬁer. Most of these techniques use adversarial training
to harden a single classiﬁer. In contrast, instead of hardening
one classiﬁer, in our work, the learner maintains multiple
classiﬁers with different degrees of hardening and strate-
gically deploys one of them. Recently, researchers have
proposed using ensembles of classiﬁers for adversarial train-
ing (Bagnall, Bunescu, and Stewart 2017) (Kariyappa and Qureshi 2019) (Li et al. 2018)
that use a diversity measure between classiﬁer ensembles to
determine if an instance is adversarial versus clean. Genera-
tive Adversarial Nets (GANs) (Goodfellow et al. 2014) also
model interaction between an adversary (generator) and
learner (discriminator) as a 2-player game. However, the
objectives of GANs and adversarial learning are different.
GANs enable an adversary to reﬁne its data generation
process, starting from a random distribution, so that the
generated data is indistinguishable from legitimate data.
Adversarial learning, on the other hand, aims to enable
to strategically defend against adversarial
the learner
attacks. Recently, security games (Tambe 2011) and ad-
versarial games for network security have been proposed
in (Schlenker et al. 2018). The proposed technique employs
deception by the learner to misguide the adversary, which
can be considered as a complimentary approach to the
techniques proposed here, for building defenses against
adversarial attacks.

Adversarial Learning as Bayesian Game

We consider a supervised learning setting for binary classi-
ﬁcation where learner, L, receives data instances as queries
from an attacker or adversary, A. We represent this in-
teraction between L and A as a 2-player Bayesian game
for adversarial learning (Grosshans et al. 2013) while adapt-
ing Großhans’ model to multiple classiﬁers, different attack
strengths and repeated interactions between L and A. We
describe the different components of the game below:

Let Xev denote a set of queries. We refer to Xev as the
clean query set. Let X = (x, y), X ∈ Xev denote a query
data instance, where x = {x1, x2, ...} is its set of attributes
or features and y ∈ {0, 1} is its ground truth label.

Adversary. A sends either clean or adversarial data as
queries; the latter is generated by perturbing clean data us-
ing a perturbation function δ : x → x. We assume that
A uses different perturbation functions δi, i = 0, 1, 2, ...,
where i denotes the strength of the perturbation. For exam-
ple, perturbation strength could correspond to the number of
features of x that are modiﬁed to convert it into an adversar-
ial instance (Globerson and Roweis 2006). δi(x) denotes the
adversarial data generated with perturbation strength i and
δi+1 is a stronger perturbation than δi. Perturbing x does not
change its ground truth label, y. For notational convenience,

With the popularity of deep neural networks as ML
models, several adversarial learning techniques based on
adversarial training for deep networks have also been re-
searched (Goodfellow, Shlens, and Szegedy 2014) (Kurakin, Goodfellow, and Bengio 2016).

we refer to clean data, x = δ0(x). An action for A is to se-
lect a δi, use it to convert clean instance x into adversarial
instance δi(x), and send the adversarial instance to L.

Learner. L receives a query data instance ¯x and its task
is to correctly predict its category. L is neither aware of the
perturbation strength i of xδi inside the data, nor is it aware
of ¯y, the ground truth label of ¯x. L uses a set of classiﬁers,
Lj, j = 0, 1, 2... for its prediction task. Lj implements a
: x → {0, 1}, that outputs a category
classiﬁcation, Lj
given the features of the query data. Classiﬁer Lj is adver-
sarially trained using training data Xtr,δj /∈ Xev, where δj
denotes the perturbation strength of the training data. We as-
sume that Lj+1 is a stronger classiﬁer than Lj: for a query
x, Lj+1 has a higher conﬁdence in its output than Lj, or,
mathematcally, P (Lj+1(x) = y) ≥ P (Lj(x) = y). An ac-
tion for L is to select a classiﬁer Lj and use it to classify
the data instance sent by A. We denote the action set of L as
AcL = {L0, L1, L2...}. Let Π(AcL) be the set of probabil-
ity distributions over AcL. sL ∈ Π(AcL) denotes a strategy
for L and sL(Lj)the probability of selecting Lj under strat-
egy sL. Finally, recall that L is not aware of the perturba-
tion δi that has been used by A on the query data instance,
¯X that it receives. To model this uncertainty about its op-
ponent, L uses epistemic types for A (Harsanyi 1967). A’s
type θi denotes that A uses perturbation strength i to create
¯x, i.e., ¯x = xθi = δi(x). ΘA = {θi} is A’s set of types and
p : ΘA → [0, 1]|θA| denotes a probability distribution over
these types. ΘA is known to L, p() is calculated by L. But
θi, the exact realization of A’s type (in other words, the per-
turbation strength used to create ¯x) is not known to L when
it receives ¯x from A.

Utilities. Utilities are numeric values assigned by each
player to the outcomes from the players’ joint actions in a
game. Each player could then preferentially rank its joint
outcomes and select a suitable action such as a utility maxi-
mizing action. For our game, recall that L is not able to ob-
serve A’s type θi (amount of perturbation in ¯x). Therefore,
L calculates an expected utility over A’s possible types, ΘA,
using A’s type distribution p(). L’s expected utility for strat-
egy sL with query data ¯x and ground truth label ¯y is given
by:

EUL(sL, ¯x, θA, p()) =

p(θi)UL(Lj, ¯x, θi),

X
θi∈ΘA

UL(Lj, ¯x, θi) =

X
Lj

sL(Lj)(cid:16)P (Lj((¯x) = ¯y)|θi)vL (Lj, θi)

− cLj (cid:17),

(1)

where P ((Lj(¯x) = ¯y)|θi) is the probability that L
makes a correct prediction given ¯x was generated using θi,
vL(Lj, θi) is the value for L from classifying ¯x using Lj and
cLj is the cost of using classiﬁer Lj.

adversary is

it
aware of

In adversarial settings,

is usually assumed that
the
learner’s prediction
the
model, e.g., model parameters of the learner’s classi-
ﬁer (Alfeld, Zhu, and Barford 2017). In the context of our
game, this can be interpreted as A knowing L’s strategy, sL.

A’s utility for query data ¯x with ground truth label ¯y, for L’s
strategy sL and its own type θi is given by:

UA(sL, ¯x, θi) =

X
Lj

sL(Lj)(cid:16)P (Lj(¯x) 6= ¯y)vA (Lj, θi)

− cθi(cid:17),

(2)

where P (Lj(¯x) 6= ¯y) represents the probability that L
makes a mistake in prediction (in other words, A’s adversar-
ial perturbation of clean data was successful) and vA(Lj, θi)
is the value that A derives from sending the query data xθi
when L’s action is Lj and cθi is A’s cost for generating ad-
versarial data with type (perturbation strength) θi.

Bayesian Sequential Game. Using the above actions and
utility functions, we can represent a Bayesian sequential
game between L and A as Γ = [N, Ac, U, ΘA, p()], where
N = {L, A} is the set of players, Ac = AcL × ΘA is the set
of joint action-types of L and A, U = (EUL, UA) denotes
the utilities received by L and A (given in Eqns. 1 and 2),
ΘA and p() are the set of A’s types and probability distribu-
tion over those types, as deﬁned before.

L and suitable type θ∗

The computational problem facing L and A is to calcu-
late a suitable strategy s∗
i respectively.
To do this calculation using Eqn. 1, L also needs to know
the value of p(), the probability distribution over A’s types.
To address these issues, we propose an approach using a
technique called self play with repeated plays of the above
Bayesian Sequential game called a Repeated Bayesian Se-
quential Game (RBSG), as described below.

Repeated Bayesian Sequential Game and Self-Play
The objective of L is to determine a suitable strategy s∗
L to
play against A that would improve its expected utility by
deploying an appropriate classiﬁer that has been hardened
commensurate to the strength of the perturbation used by
A. To achieve this, L uses self play, where L and A play
the Bayesian Sequential game, Γ, repeatedly. For the sake
of legibility, we continue to use the notation A to denote L’s
self play adversary. The repeated interactions between L and
A can be represented as a game tree with sequential moves
between them. A node in the game tree denotes a player’s
turn to make a move. In a move, a player selects an action
from its action set. L and A make alternate moves with L
moving ﬁrst. A pair of moves by L and A corresponds to
an instance of the Bayesian Sequential game, Γ, realized as
below.

Algorithm 1: game-play()
1 Select s∗
L using current belief of ˆp, and θ∗
i (Eqn. 3 or 4)
2 Calculate utils. recd.: ˆuL and ˆuA with observed values
i resp. (using Eqns. 1 and 2)

L and θ∗

of s∗

3 return (ˆuL, ˆuA)

Game Play. As shown in Algo. 1, L’s moves by select-
ing a strategy s∗
L. A then selects type (perturbation strength)
θ∗
i ∼ p() while observing s∗
i , A then
generates q adversarial queries by perturbing q clean data

L. With the selected θ∗

instances from Xev, and sends each adversarial query, ¯x, to
L. After L processes the queries, both L and A receive utili-
ties given by Eqns. 1 and 2 respectively. The problem facing
L without observing θ∗
L is to calculate s∗
i and p() from A’s
moves. We solve this problem using a modiﬁed Monte Carlo
Tree Search (MCTS) algorithm, as described below.

L. To calculate s∗

Calculating strategy s∗
L, L generates
different paths in the game tree to discover utilities re-
ceived from different sequences of moves. To systemati-
cally explore the game tree, L uses an MCTS-like algo-
rithm (Browne et al. 2012), called TreeTraverse. shown in
Algos. 2 and 3. TreeTraverse works by generating a se-
quences of moves or game plays corresponding to a path in
the game tree up to a ﬁnite cutoff depth h. L and A’s utilities
from their moves are recorded along the path and once the
bottommost level is reached, the utilities are updated along
the path upwards toward the root. In this way, moves that
could lead to high utility can be identiﬁed by each player.

The key aspects of MCTS are to balance exploration and
exploitation while traversing the game tree by using a heuris-
tic function called selectBestChild (Algo. 2, line 4), and, do-
ing an operation called rollout to rapidly traverse unexplored
parts of the game tree by selecting actions for each player up
to the game tree’s cutoff depth h (Algo. 3). In our TreeTra-
verse algorithm, we have used two heuristic functions for
selectBestChild, as described below:

Bayes Nash Equilibrium (BNE). In BNE, each player
its
selects a best
utilities, given the possible strategies of
its oppo-
nent (Harsanyi 1967). The strategies for L and A calculated
using BNE are given by:

reponse strategy that maximizes

s∗
L = arg max

EUL(sL, ¯x, ΘA, p()),

sL∈Π(AcL)
θ∗
i = arg max
θi∈ΘA

UA(s∗

L, ¯x, θi),

(3)

where uA is given by Eqn. 2 and EUL is given by Eqn. 1
with A’s actual type distribution p(θi) replaced by L’s belief
distribution ˆp(θi).

Upper Conﬁdence Bound (UCB). UCB is a bandit-based
technique (Browne et al. 2012) that weighs the expected
utility of a move with the number of times it has been vis-
ited, so that previously unexplored or less-explored actions
at a move are also tried. UCB uses the following equation to
calculate s∗

L and θ∗
i :

s∗
L=arg maxΠ(Lj )

P

p(θi)

θi (cid:18)

θ∗
i =arg maxθi

Lj (cid:18)

P

P

¯x∈ ¯X UL(Lj ,¯x,θi)+C

P
¯x∈ ¯X s∗
L(Lj )UA(Lj ,¯x,θi)+C

2 ln P arvisit

Lj,visit (cid:19)

r

2 ln P arvisit

θi,visit (cid:19)

r

(4)

Here, C is a constant, P arvisit is the number of times the
parent node of the current node was visited and Lj,visit and
θi,visit are the number of times the current node has been
visited for L and A respectively.

Updating belief of A’s type distribution. The Tree-
Traveese algorithm explores a sequence of moves along
aany single path from the root of the game tree up to the
cutoff depth h. We call this a trial for the RBSG. To update
its belief distribution ˆp, L uses multiple trials and, at the end

Algorithm 2: TreeTraverse(v)
Input: v: start node for traversal
Output: vval: value from tree traversal (via

backtracking) starting from v up to depth h

1 if vdepth = h then
2

return

3 else if v is fully expanded then
4

cval ← TreeTraverse(selectBestChild(v)) // go
down game tree along best action (Eqns. 3 or 4)
Update vval ← vval + cval; increment vvisit
return cval

7 else if v is visited but not expanded then
8

¯c ← generatedAllChildren(v) // all actions
c ← select random child (action) from ¯c
cval ← rollout(c)
Update vval ← vval + cval, increment vvisit and
cvisit
return cval

5

6

9

10

11

12

13 else if v is not visited then
vval ← rollout(v)
14
Increment vvisit
return vval

15

16

Algorithm 3: Rollout(v)
Input: v: start node for rollout
Output: vval: value from rollout (via backtracking)

starting from v up to depth h

1 if v is terminal then
2

ˆuL, ˆuA ← game-play()
return (ˆuL, ˆuA)

3
4 else
5

6

7

c ← select child of v prop. to uL (for L’s move) or
prop. to p() (for A’s move)
cval ← rollout(c)
return cval

of each trial, L uses an update strategy to update ˆp(). We
consider two probability update strategies that can be used
by L (Algo. 4, line 4) for updating p ˜ΘA. 1) Fictitious Play
(FP): In ﬁctitious play (Shoham and Leyton-Brown 2009),
the probability of type θi is the fraction of times it was
played following action Lj, as given by the following up-
date rule:

P (θi|Lj) =

No. of times θi selected after Lj
Total no. of times Lj selected

(5)

2) Bayesian Update (BU): Bayesian update of θi calculates
the conditional probability of selecting θi when it followed
Lj using Bayes rule, given by the following equation:

P (θi|Lj) =

P (Lj|θi)P (θi)
P (Lj)

=

P (Lj|θi)P (θi)

P (Lj|θi)P (θi)

,

(6)

Pθi

where P (Lj|θi) is the fraction of times Lj was played fol-
lowing θi, P (Lj) is known to L and the denominator is a

normalization term. The updated probability estimate is then
used by L to calculate the expected utilities in Eqns. 3 and
4 for its actions more accurately against A’s in future trials.

Algorithm 4: Self-Play()
1 for τ = 1...ntrials do
2

3

4

root ← L’s ﬁrst move with randomly sel. action
TreeTraverse(root)
Update ˆp using prob. update strategy (ﬁc. play,
Eqn. 5 or Bayes update, Eqn. 6)

Experimental Results
We have evaluated the performance our proposed RBSG
with self play-based adversarial learning technique for a bi-
nary classiﬁcation task with text data using the Yelp review
polarity data set. (dat ). Each data instance has either of two
labels, 1 (negative) and 2 (positive). The clean training and
test sets have 560, 000 and 38, 000 samples respectively. We
used the Character Convolutional Neural Network (Char-
CNN) (Zhang, Zhao, and LeCun 2015) model that consists
of 5 convolution layers followed by 3 fully connected lay-
ers. It uses convolution layers to identify character level fea-
tures to classify text. For generating adversarial text, we
used the single character gradient based replacement tech-
nique (Liang et al. 2018). Given a data instance in the form
of a text character string as input to an ML model, the
method works by classifying the text using the model and
calculating the gradient of the loss function for each char-
acter in the input text. It then replaces the character with
the most negative gradient (most inﬂuential on the classiﬁer
output) in the text with the character that has the least pos-
itive gradient (least inﬂuential on the classiﬁer output). The
technique can be used iteratively on a data instance to re-
place multiple characters in the text and create adversarial
text with different attack strengths, e.g., two iterations of the
technique yields adversarial text with perturbation strength
2. All experiments were performed on a computer with
20 dual core, 2.3 GHz Intel Xeon CPUs with Nvidia Tesla
K40C GPU. The RBSG self play code was implemented in
Python 2.7; the CharCNN and adversarial text generataion
code used Tensorﬂow 1.11 for building and training their
deep network models. The CharCNN was ﬁrst trained with
clean data, and then hardened separately with two adversar-
ial training data sets with 200, 000 adversarial training sam-
ples of perturbation strengths 1 and 2 respectively. This gave
three classiﬁers for L with increasing hardening levels, de-
noted by L0, L1 and L2. The accuracies of these classiﬁers
were then evaluated with 50, 000 instances of test data of
perturbation strengths 1, 2 and 3 each, as reported in Table 1.
Adversary A generates queries with either clean data or
adversarial data with perturbation strengths 1, 2 and 3, giv-
ing ΘA = {θ0, θ1, θ2, θ3}. L uses three classiﬁers, so,
AcL = {L0, L1, L2}. The different parameters used for our
experiments are: cutoff depth in self play, h = 20; number
of trials in self play, ntrials = 10; batch size for queries
sent by A to L, q = 10; and constant in UCB calculation

Clean
Adv 1
Adv 2
Adv 3

L0
0.9392
0.8684
0.7706
0.6814

L1
0.9426
0.88
0.7922
0.7056

L2
0.94
0.8782
0.8152
0.7502

Table 1: Testing accuracy of individual classiﬁers with dif-
ferent hardening levels (columns) on adversarial test data
with different perturbation strengths (rows).

(Eqn. 4), C = 2.

UCB
Clean
Adv 1
Adv 2
Adv 3
BNE
Clean
Adv 1
Adv 2
Adv 3

L2

L1

L0

Acc.
43.75% 29.46% 26.79% 0.9321
39.65% 24.13% 36.21
0.8716
50.89% 0.8062
25%
24.11%
39.81% 20.37% 39.81% 0.7222
Acc.
57.56% 10.37% 32.07% 0.9302
0.867
33.91% 46.96% 19.13
29.46% 27.68% 42.86% 0.808
31.53% 32.43% 36.04% 0.709

L1

L2

L0

Table 2: Percentage of different classiﬁers used and accu-
racies (columns) obtained for clean and adversarial data of
different perturbation strengths (rows). Data in the top and
bottom tables are with Upper Conﬁdence Bound (UCB) and
Bayes Nash Equilibrium (BNE), respectively, for action se-
lection during self play.

For our ﬁrst set of experiments, we validated if L, using
the self play algorithm, could effectively deploy appropri-
ate classiﬁers for data of different perturbation strengths.
We created four different type distributions for data gen-
erated by A, each distribution having 98% of one of the
four types{θ0, θ1, θ2, θ3}. L used either Upper Conﬁdence
Bound (UCB) or Bayes Nash Equilibrium (Eqn. 3 or Eqn. 4)
to select actions in the game tree during self play. Our re-
sults are shown in Table 2. The results show that both
UCB and BNE metric for action selection perform compara-
bly. The accuracy obtained using our RBSG-based self play
technique on clean and adversarial data perturbed with dif-
ferent perturbation strengths (last column of Table 2 is not
degraded and comparable to the best accuracies obtained
with the most hardened classiﬁer, L2, when used individ-
ually (column 4 of Table 1). The RBSG with self play tech-
nique is also able to align adversarial data of different per-
turbation strengths with the commensurately hardened clas-
siﬁer, as shown by the maximum percentage of each row
in Table 1 corresponding to the classiﬁer hardened with ad-
versarial data of that perturbation strength. Note that with
adversarial data of perturbation strength 3, Adv 3, the clas-
siﬁers are selected almost uniformly. This is because none
of the classiﬁers, L0, L1 or L2 were trained with adversarial
data of perturbation strength 3. L2, which had the highest
individual accuracy for Adv 3 data, is used most frequently,
albeit marginally, for Adv 3 data in Table 2. Our Self-play
technique also strategically also uses L0 and L1 that incur

lower costs to deploy than L2. Consequently, the utility ob-
tained by L with self play is better than its utility while us-
ing individual classiﬁer L2 only. Fig. 1 shows the compar-
ison of the relative utilities obtained by L while using the
proposed RBSG with self play technique versus the utilities
obtained while using the most hardened individual classiﬁer
L2. As illustrated, the RBSG with self play technique is able
to improve utilities as it deploys lower cost classiﬁers L0
and L1 along with L2 while aligning the expected pertur-
bation strength of the query data, estimated via ˆp, with the
commensurately hardened classiﬁer.

Figure 1: Relative utilities obtained by individual classiﬁer
L2, and RBSG with self play-based techniques with UCB
and BNE action selection for data reported in Table 2

.

For our next experiments, we evaluated the convergence
of L’s belief distribution ˆp() to A’s actual type distribu-
tion p() using the ﬁctitious play and Bayesian update proba-
bility update strategies (Eqns. 5 and 6). Results were av-
eraged over 10 runs. For each run, p() was selected as a
random distribution. We report the Kullback-Liebler(KL)
divergence between ˆp() and p(), given by DKL(ˆp||p) =
Pθi∈ΘA ˆp(θi)ln ˆp(θi)
p(θi) . As shown in Fig. 2, with both strate-
gies ˆp is able to converge to within 5% of p() within about
6 trials. Fictitious play converges faster with higher KL di-
vergence values while Bayesian update takes a longer time
to converge owing to its more complex calculations.

Conclusion
We proposed a technique for improving the costs of a
classiﬁer-based ML model against adversarial attacks of dif-
ferent strengths without deteriorating its performance by us-
ing repeated game-like interactions between a learner and
an adversary. There are several important directions that are
worthy of further investigation. First, the assumption in ex-
isting research which assumes that the learner reveals its
classiﬁer to the adversary is rather limiting. A more realistic
situation would be that the adversary is able to reverse en-
gineer the learner’s classiﬁers, but it is not aware of the fre-
quency with which the learner deploys them. The adversary
could then also build a model of the learner via repeated in-
teractions to determine its perturbation strength strategically.
Secondly, although used as a popular solution technique in

Figure 2: KL divergence between A’s actual type distri-
bution and L’s belief distribution using ﬁctitious play and
Bayesian update for ntrials = 10, h = 20. Results are aver-
aged over 10 runs.

games, Nash equilibrium (NE) strategy calculation is known
to have certain shortcomings such as assuming that play-
ers always behave rationally. In reality, an adversary could
behave myopically, select a greedy outcome, or, adopt sub-
optimal, low and slow strategies to misguide the learner. To
handle these situations, a direction we are interested in ex-
ploring is to use recent techniques such as regret-based tech-
niques, safety value and exploitability of opponents, instead
of Bayes Nash equilibrium-based strategy selection. Finally,
integrating reinforcement learning for our adversarial learn-
ing setting promises to be another direction worthy of further
investigation.

References

[Alfeld, Zhu, and Barford 2017] Alfeld, S.; Zhu, X.; and Barford, P.
2017. Explicit defense actions against test-set attacks. In Proceed-
ings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence,
February 4-9, 2017, San Francisco, California, USA., 1274–1280.

[Bagnall, Bunescu, and Stewart 2017] Bagnall, A.; Bunescu, R.;
and Stewart, G. 2017. Training ensembles to detect adversarial
examples. arXiv preprint arXiv:1712.04006.

[Browne et al. 2012] Browne, C. B.; Powley, E.; Whitehouse, D.;
Lucas, S. M.; Cowling, P. I.; Rohlfshagen, P.; Tavener, S.; Perez,
D.; Samothrakis, S.; and Colton, S. 2012. A survey of monte carlo
tree search methods. IEEE Trans. on Comp. Intelligence and AI in
games 4(1):1–43.

[Br¨uckner, Kanzow, and Scheffer 2012] Br¨uckner, M.; Kanzow, C.;
and Scheffer, T. 2012. Static prediction games for adversarial learn-
ing problems. J. Mach. Learn. Res. 13(1):2617–2654.

[Bul`o et al. 2017] Bul`o, S. R.; Biggio, B.; Pillai, I.; Pelillo, M.;
and Roli, F. 2017. Randomized prediction games for adversar-
IEEE Trans. Neural Netw. Learning Syst.
ial machine learning.
28(11):2466–2478.

[Dalvi et al. 2004] Dalvi, N.; Domingos, P.; Sanghai, S.; Verma, D.;
et al. 2004. Adversarial classiﬁcation. In Proc. 10th ACM SIGKDD
Intl. Conf. Knowledge Discovery and Data mining, 99–108. ACM.

[Dasgupta and Collins 2019] Dasgupta, P., and Collins, J. 2019. A
survey of game theoretic approaches for adversarial machine learn-
ing in cybersecurity tasks. AI Magazine 40(2):31–43.

[Vorobeychik and Kantarcioglu 2018] Vorobeychik,

and
Syn-
Kantarcioglu, M.
thesis Lectures on Artiﬁcial Intelligence and Machine Learning
12(3):1–169.

Y.,
2018. Adversarial machine learning.

[Wolpert 2002] Wolpert, D. H. 2002. The supervised learning no-
free-lunch theorems. In Soft computing and industry. Springer. 25–
42.

[Yuan et al. 2019] Yuan, X.; He, P.; Zhu, Q.; and Li, X. 2019.
Adversarial examples: Attacks and defenses for deep learning.
IEEE Transactions on Neural Networks and Learning Systems
30(9):2805–2824.

[Zhang, Zhao, and LeCun 2015] Zhang, X.; Zhao, J.; and LeCun,
Y. 2015. Character-level convolutional networks for text classi-
In Advances in neural information processing systems,
ﬁcation.
649–657.

[dat ] Yelp reviews polarity data set. http://goo.gl/JyCnZq. Ac-

cessed: 2019-07-15.

[Dong et al. 2018] Dong, J.; Roth, A.; Schutzman, Z.; Waggoner,
B.; and Wu, Z. S. 2018. Strategic classiﬁcation from revealed
preferences. In Proceedings of the 2018 ACM Conference on Eco-
nomics and Computation, 55–70. ACM.

[Globerson and Roweis 2006] Globerson, A., and Roweis, S. 2006.
Nightmare at test time: robust learning by feature deletion. In Pro-
ceedings of the 23rd international conference on Machine learning,
353–360. ACM.

[Goodfellow et al. 2014] Goodfellow, I.; Pouget-Abadie, J.; Mirza,
M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; and Ben-
gio, Y. 2014. Generative adversarial nets. In Advances in neural
information processing systems, 2672–2680.

[Goodfellow, Shlens, and Szegedy 2014] Goodfellow, I. J.; Shlens,
J.; and Szegedy, C. 2014. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572.

[Grosshans et al. 2013] Grosshans, M.; Sawade, C.; Bruckner, M.;
and Scheffer, T. 2013. Bayesian games for adversarial regression
In Proceedings of the 30th International Conference
problems.
on International Conference on Machine Learning - Volume 28,
ICML’13, III–55–III–63.

[Harsanyi 1967] Harsanyi, J. C. 1967. Games with incomplete in-
formation played by bayesian players, i–iii part i. the basic model.
Management science 14(3):159–182.

[Kariyappa and Qureshi 2019] Kariyappa, S., and Qureshi, M. K.
2019. Improving adversarial robustness of ensembles with diver-
sity training. arXiv preprint arXiv:1901.09981.

[Kurakin, Goodfellow, and Bengio 2016] Kurakin, A.; Goodfellow,
I.; and Bengio, S. 2016. Adversarial machine learning at scale.
arXiv preprint arXiv:1611.01236.

[Li et al. 2018] Li, D.; Li, Q.; Ye, Y.; and Xu, S. 2018. Enhanc-
ing robustness of deep neural networks against adversarial malware
samples: Principles, framework, and aics’2019 challenge. arXiv
preprint arXiv:1812.08108.

[Liang et al. 2018] Liang, B.; Li, H.; Su, M.; Bian, P.; Li, X.; and
Shi, W. 2018. Deep text classiﬁcation can be fooled. In Proc. 22nd
Intl. Joint Conf on AI, IJCAI, 4208–4215.

[Madry et al. 2017] Madry, A.; Makelov, A.; Schmidt, L.; Tsipras,
D.; and Vladu, A. 2017. Towards deep learning models resistant to
adversarial attacks. arXiv preprint arXiv:1706.06083.

[Mei and Zhu 2015] Mei, S., and Zhu, X. 2015. Using machine
teaching to identify optimal training-set attacks on machine learn-
ers. In Proceedings of the Twenty-Ninth AAAI Conference on Arti-
ﬁcial Intelligence, AAAI’15, 2871–2877. AAAI Press.

[Schlenker et al. 2018] Schlenker, A.; Thakoor, O.; Xu, H.; Fang,
F.; Tambe, M.; Tran-Thanh, L.; Vayanos, P.; and Vorobeychik, Y.
2018. Deceiving cyber adversaries: A game theoretic approach. In
Proceedings of the 17th International Conference on Autonomous
Agents and MultiAgent Systems, 892–900.
International Founda-
tion for Autonomous Agents and Multiagent Systems.

[Shoham and Leyton-Brown 2009] Shoham, Y.,

and Leyton-
2009. Multiagent Systems - Algorithmic, Game-
Cambridge University

Brown, K.
Theoretic, and Logical Foundations.
Press.

[Tambe 2011] Tambe, M. 2011. Security and Game Theory: Algo-
rithms, Deployed Systems, Lessons Learned. New York, NY, USA:
Cambridge University Press, 1st edition.

[Tram`er et al. 2017] Tram`er, F.; Kurakin, A.; Papernot, N.; Good-
fellow, I.; Boneh, D.; and McDaniel, P. 2017. Ensemble adversarial
training: Attacks and defenses. arXiv preprint arXiv:1705.07204.

