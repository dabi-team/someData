Sex Trafﬁcking Detection with Ordinal Regression Neural Networks

Longshaokan Wang1∗, Eric Laber2, Yeng Saanchi3, Sherrie Caltagirone4
1Alexa AI, Amazon, 23Department of Statistics, North Carolina State University, 4Global Emancipation Network
1longsha@amazon.com, 23{eblaber, ysaanch}@ncsu.edu, 4sherrie@globalemancipation.ngo

0
2
0
2

n
a
J

2
1

]

G
L
.
s
c
[

2
v
4
3
4
5
0
.
8
0
9
1
:
v
i
X
r
a

Abstract

Sex trafﬁcking is a global epidemic. Escort websites are a pri-
mary vehicle for selling the services of such trafﬁcking vic-
tims and thus a major driver of trafﬁcker revenue. Many law
enforcement agencies do not have the resources to manually
identify leads from the millions of escort ads posted across
dozens of public websites. We propose an ordinal regression
neural network to identify escort ads that are likely linked to
sex trafﬁcking. Our model uses a modiﬁed cost function to
mitigate inconsistencies in predictions often associated with
nonparametric ordinal regression and leverages recent ad-
vancements in deep learning to improve prediction accuracy.
The proposed method signiﬁcantly improves on the previ-
ous state-of-the-art on Trafﬁcking-10K, an expert-annotated
dataset of escort ads. Additionally, because trafﬁckers use
acronyms, deliberate typographical errors, and emojis to re-
place explicit keywords, we demonstrate how to expand the
lexicon of trafﬁcking ﬂags through word embeddings and t-
SNE.

1

Introduction

Globally, human trafﬁcking is one of the fastest growing
crimes and, with annual proﬁts estimated to be in excess of
150 billion USD, it is also among the most lucrative (Amin
2010). Sex trafﬁcking is a form of human trafﬁcking which
involves sexual exploitation through coercion. Recent esti-
mates suggest that nearly 4 million adults and 1 million chil-
dren are being victimized globally on any given day; further-
more, it is estimated that 99 percent of victims are female
(International Labour Organization, Walk Free Foundation,
and International Organization for Migration 2017). Escort
websites are an increasingly popular vehicle for selling the
services of trafﬁcking victims. According to a recent sur-
vivor survey (THORN and Bouch´e 2018), 38% of underage
trafﬁcking victims who were enslaved prior to 2004 were
advertised online, and that number rose to 75% for those
enslaved after 2004. Prior to its shutdown in April 2018,
the website Backpage was the most frequently used online

∗This work was done when Wang was a PhD student at North

Carolina State University.
Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

advertising platform; other popular websites used for ad-
vertising escort service include Craigslist, Redbook, Sugar-
Daddy, and Facebook (THORN and Bouch´e 2018). Despite
the seizure of Backpage, there were nearly 150,000 new on-
line sex advertisements posted per day in the U.S. alone
in late 2018 (Tarinelli 2018); even with many of these new
ads being re-posts of existing ads and trafﬁckers often post-
ing multiple ads for the same victims (THORN and Bouch´e
2018), this volume is staggering.

Because of their ubiquity and public access, escort web-
sites are a rich resource for anti-trafﬁcking operations. How-
ever, many law enforcement agencies do not have the re-
sources to sift through the volume of escort ads to identify
those coming from potential trafﬁckers. One scalable and ef-
ﬁcient solution is to build a statistical model to predict the
likelihood of an ad coming from a trafﬁcker using a dataset
annotated by anti-trafﬁcking experts. We propose an ordi-
nal regression neural network tailored for text input. This
model comprises three components: (i) a Word2Vec model
(Mikolov et al. 2013b) that maps each word from the text
input to a numeric vector, (ii) a gated-feedback recurrent
neural network (Chung et al. 2015) that sequentially pro-
cesses the word vectors, and (iii) an ordinal regression layer
(Cheng, Wang, and Pollastri 2008) that produces a predicted
ordinal label. We use a modiﬁed cost function to mitigate
inconsistencies in predictions associated with nonparamet-
ric ordinal regression. We also leverage several regulariza-
tion techniques for deep neural networks to further improve
model performance, such as residual connection (He et al.
2016) and batch normalization (Ioffe and Szegedy 2015).
We conduct our experiments on Trafﬁcking-10k (Tong et
al. 2017), a dataset of escort ads for which anti-trafﬁcking
experts assigned each sample one of seven ordered labels
ranging from “1: Very Unlikely (to come from trafﬁckers)”
to “7: Very Likely”. Our proposed model signiﬁcantly out-
performs previously published models (Tong et al. 2017) on
Trafﬁcking-10k as well as a variety of baseline ordinal re-
gression models. In addition, we analyze the emojis used in
escort ads with Word2Vec and t-SNE (van der Maaten and
Hinton 2008), and we show that the lexicon of trafﬁcking-
related emojis can be subsequently expanded.

The main contributions of this paper are summarized as

 
 
 
 
 
 
follows: 1. We propose a neural network architecture for
text data with ordinal labels, which outperforms the previ-
ous state-of-the-art (Tong et al. 2017) on Trafﬁcking-10k.
2. We propose a simple penalty term in the cost function
to mitigate the monotonicity violation and improve the in-
terpretability of the output of the ordinal regression layer,
where the monotonicity violation was previously deemed
too computationally costly to resolve (Niu et al. 2016). 3. We
provide a qualitative analysis on the top escort ads ﬂagged
by our model, which offers patterns that anti-trafﬁcking ex-
perts can potentially conﬁrm or make use of. 4. We provide
an emoji analysis that shows how to use unsupervised learn-
ing techniques on the raw data to generate leads on new traf-
ﬁcking key words. 5. We open source our code base and
trained model to encourage further research on trafﬁcking
detection and to allow the law enforcement to make use of
our research for free.

In Section 2, we discuss related work on human traf-
ﬁcking detection and ordinal regression. In Section 3, we
present our proposed model and detail its components. In
Section 4, we present the experimental results, including
the Trafﬁcking-10K benchmark, a qualitative analysis of the
predictions on raw data, and the emoji analysis. In Section
5, we summarize our ﬁndings and discuss future work.

2 Related Work
Trafﬁcking detection: There have been several software
products designed to aid anti-trafﬁcking efforts. Examples
include Memex1 which focuses on search functionalities in
the dark web; Spotlight2 which ﬂags suspicious ads and
links images appearing in multiple ads; Trafﬁc Jam3 which
seeks to identify patterns that connect multiple ads to the
same trafﬁcking organization; and TrafﬁckCam4 which aims
to construct a crowd-sourced database of hotel room images
to geo-locate victims. These research efforts have largely
been isolated, and few research articles on machine learn-
ing for trafﬁcking detection have been published. Closest to
our work is the Human Trafﬁcking Deep Network (HTDN)
(Tong et al. 2017). HTDN has three main components: a lan-
guage network that uses pretrained word embeddings and a
long short-term memory network (LSTM) to process text in-
put; a vision network that uses a convolutional network to
process image input; and another convolutional network to
combine the output of the previous two networks and pro-
duce a binary classiﬁcation. Compared to the language net-
work in HTDN, our model replaces LSTM with a gated-
feedback recurrent neural network, adopts certain regular-
izations, and uses an ordinal regression layer on top. It sig-
niﬁcantly improves HTDN’s benchmark despite only using
text input. As in the work of Tong et al. (2017), we pre-
train word embeddings using a skip-gram model (Mikolov
et al. 2013b) applied to unlabeled data from escort ads, how-
ever, we go further by analyzing the emojis’ embeddings and
thereby expand the trafﬁcking lexicon.

1darpa.mil/program/memex
2htspotlight.com
3marinusanalytics.com/trafﬁcjam
4trafﬁckcam.com

Ordinal regression: We brieﬂy review ordinal regression
before introducing the proposed methodology. We assume
that the training data are Dtrain = {(Xi, Yi)}n
i=1, where
Xi ∈ X are the features and Yi ∈ Y is the response; Y is the
set of k ordered labels {1, 2, . . . , k} with 1 ≺ 2 . . . ≺ k.
Many ordinal regression methods learn a composite map
η = h ◦ g, where g : X → R and h : R → {1, 2, . . . , k}
have the interpretation that g(X) is a latent “score” which
is subsequently discretized into a category by h. η is often
estimated by empirical risk minimization, i.e., by minimiz-
ing a loss function C{η(X), Y } averaged over the training
data. Standard choices of η and C are reviewed by Rennie
and Srebro (2005).

Another common approach to ordinal regression, which
we adopt in our proposed method, is to transform the la-
bel prediction into a series of k − 1 binary classiﬁcation
sub-problems, wherein the ith sub-problem is to predict
whether the true label exceeds i (Frank and Hall 2001; Li
and Lin 2006). For example, one might use a series of lo-
gistic regression models to estimate the conditional proba-
bilities fi(X) = P (Y > i(cid:12)
(cid:12)X) for each i = 1, . . . , k − 1.
Cheng, Wang, and Pollastri (2008) estimated these prob-
abilities jointly using a neural network; this was later ex-
tended to image data (Niu et al. 2016) as well as text data
(Irsoy and Cardie 2015; Ruder, Ghaffari, and Breslin 2016).
However, as acknowledged by Cheng, Wang, and Pollastri
(2008), the estimated probabilities need not respect the or-
dering fi(X) ≥ fi+1(X) for all i and X. We force our
estimator to respect this ordering through a penalty on its
violation.

3 Method
Our proposed ordinal regression model consists of the fol-
lowing three components: Word embeddings pre-trained by
a Skip-gram model, a gated-feedback recurrent neural net-
work that constructs summary features from sentences, and
a multi-labeled logistic regression layer tailored for ordinal
regression. See Figure 1 for a schematic. The details of its
components and their respective alternatives are discussed
below.

3.1 Word Embeddings
Vector representations of words, also known as word em-
beddings, can be obtained through unsupervised learning on
a large text corpus so that certain linguistic regularities and
patterns are encoded. Compared to Latent Semantic Analy-
sis (Dumais 2004), embedding algorithms using neural net-
works are particularly good at preserving linear regularities
among words in addition to grouping similar words together
(Mikolov et al. 2013a). Such embeddings can in turn help
other algorithms achieve better performances in various nat-
ural language processing tasks (Mikolov et al. 2013b).

Unfortunately, the escort ads contain a plethora of emojis,
acronyms, and (sometimes deliberate) typographical errors
that are not encountered in more standard text data, which
suggests that it is likely better to learn word embeddings
from scratch on a large collection of escort ads instead of
using previously published embeddings (Tong et al. 2017).

3.3 Multi-Labeled Logistic Regression Layer
As noted previously, the ordinal regression problem can
be cast into a series of binary classiﬁcation problems and
thereby utilize the large repository of available classiﬁcation
algorithms (Frank and Hall 2001; Li and Lin 2006; Niu et
al. 2016). One formulation is as follows. Given k total ranks,
the i-th binary classiﬁer is trained to predict the probability
that a sample X has rank larger than i : (cid:98)fi(X) = (cid:98)P(Y >
i|X). Then the predicted rank is

(cid:98)Y = 1 +

k−1
(cid:88)

i=1

Round

(cid:110)

(cid:111)

.

(cid:98)fi(X)

Figure 1: Overview of the ordinal regression neural net-
work for text input. H represents a hidden state in a gated-
feedback recurrent neural network.

We use 168,337 ads scraped from Backpage as our train-
ing corpus and the Skip-gram model with Negative sampling
(Mikolov et al. 2013b) as our model.

3.2 Gated-Feedback Recurrent Neural Network

To process entire sentences and paragraphs after mapping
the words to embeddings, we need a model to handle se-
quential data. Recurrent neural networks (RNNs) have re-
cently seen great success at modeling sequential data, es-
pecially in natural language processing tasks (LeCun, Ben-
gio, and Hinton 2015). On a high level, an RNN is a neural
network that processes a sequence of inputs one at a time,
taking the summary of the sequence seen so far from the
previous time point as an additional input and producing a
summary for the next time point. One of the most widely
used variations of RNNs, a Long short-term memory net-
work (LSTM), uses various gates to control the information
ﬂow and is able to better preserve long-term dependencies in
the running summary compared to a basic RNN (see Good-
fellow, Bengio, and Courville 2016 and references therein).
In our implementation, we use a further reﬁnement of multi-
layed LSTMs, Gated-feedback recurrent neural networks
(GF-RNNs), which tend to capture dependencies across dif-
ferent timescales more easily (Chung et al. 2015).

Regularization techniques for neural networks including
Dropout (Srivastava et al. 2014), Residual connection (He et
al. 2016), and Batch normalization (Ioffe and Szegedy 2015)
are added to GF-RNN for further improvements.

After GF-RNN processes an entire escort ad, the average
of the hidden states of the last layer becomes the input for
the multi-labeled logistic regression layer which we discuss
next.

In a classiﬁcation task, the ﬁnal layer of a deep neu-
ral network is typically a softmax layer with dimension
equal to the number of classes (Goodfellow, Bengio, and
Courville 2016). Using the ordinal-regression-to-binary-
classiﬁcations formulation described above, Cheng, Wang,
and Pollastri (2008) replaced the softmax layer in their neu-
ral network with a (k −1)-dimensional sigmoid layer, where
each neuron serves as a binary classiﬁer (see Figure 2 but
without the order penalty to be discussed later).

With the sigmoid activation function, the output of the ith
neuron can be viewed as the predicted probability that the
sample has rank greater5 than i. Alternatively, the entire sig-
moid layer can be viewed as performing multi-labeled lo-
gistic regression, where the ith label is the indicator of the
sample’s rank being greater than i. The training data are
thus re-formatted accordingly so that response variable for
(cid:124)
k−i)(cid:124). The k − 1
a sample with rank i becomes (1
i−1, 0
binary classiﬁers share the features constructed by the ear-
lier layers of the neural network and can be trained jointly
with mean squared error loss. A key difference between the
multi-labeled logistic regression and the naive classiﬁcation
(ignoring the order and treating all ranks as separate classes)
is that the loss for (cid:98)Y (cid:54)= Y is constant in the naive classiﬁca-
tion but proportional to | (cid:98)Y − Y | in the multi-labeled logistic
regression.

(cid:124)

Cheng, Wang, and Pollastri’s (2008) ﬁnal layer was pre-
ceded by a simple feed-forward network. In our case, word
embeddings and GF-RNN allow us to construct a feature
vector of ﬁxed length from text input, so we can simply at-
tach the multi-labeled logistic regression layer to the output
of GF-RNN to complete an ordinal regression neural net-
work for text input.

The violation of the monotonicity in the estimated prob-
abilities (e.g., (cid:98)fi(X) < (cid:98)fi+1(X) for some X and i) has
remained an open issue since the original ordinal regres-
sion neural network proposal of Cheng, Wang, and Pollastri
(2008). This is perhaps owed in part to the belief that correct-
ing this issue would signiﬁcantly increase training complex-
ity (Niu et al. 2016). We propose an effective and computa-
tionally efﬁcient solution to avoid the conﬂicting predictions

5Actually, in Cheng, Wang, and Pollastri’s original formulation,
the ﬁnal layer is k-dimensional with the i-th neuron predicting the
probability that the sample has rank greater than or equal to i. This
is redundant because the ﬁrst neuron should always be equal to 1.
Hence we make the slight adjustment of using only k − 1 neurons.

an ad; adding white spaces around every emoji so that it can
be tokenized properly; stripping tabs, line breaks, punctua-
tions, and extra white spaces; removing phone numbers; and
converting all letters to lower case. We have ensured that
the raw dataset has no overlap with the labeled dataset to
avoid bias in test accuracy. While it is possible to scrape
more raw data, we did not observe signiﬁcant improvements
in model performances when the size of raw data increased
from ∼70,000 to ∼170,000, hence we assume that the cur-
rent raw dataset is sufﬁciently large.

The labeled dataset is called Trafﬁcking-10k. It consists
of 12,350 ads from Backpage labeled by experts in human
trafﬁcking detection6 (Tong et al. 2017). Each label is one
of seven ordered levels of likelihood that the corresponding
ad comes from a human trafﬁcker. Descriptions and sam-
ple proportions of the labels are in Table 1. The original
Trafﬁcking-10K includes both texts and images, but as men-
tioned in Section 1, only the texts are used in our case. We
apply the same preprocessing to Trafﬁcking-10k as we do to
raw data.

4.2 Comparison with Baselines

We compare our proposed ordinal regression neural net-
work (ORNN) to Immediate-Threshold ordinal logistic re-
gression (IT) (Rennie and Srebro 2005), All-Threshold or-
dinal logistic regression (AT) (Rennie and Srebro 2005),
Least Absolute Deviation (LAD) (Bloomﬁeld and Steiger
1980; Narula and Wellington 1982), and multi-class logis-
tic regression (MC) which ignores the ordering. The pri-
mary evaluation metrics are Mean Absolute Error (MAE)
and macro-averaged Mean Absolute Error (MAEM ) (Bac-
cianella, Esuli, and Sebastiani 2009). To compare our model
with the previous state-of-the-art classiﬁcation model for
escort ads, the Human Trafﬁcking Deep Network (HTDN)
(Tong et al. 2017), we also polarize the true and predicted
labels into two classes, “1-4: Unlikely” and “5-7: Likely”;
then we compute the binary classiﬁcation accuracy (Acc.)
as well as the weighted binary classiﬁcation accuracy (Wt.
Acc.) given by

Wt. Acc. = 1
2

(cid:16) True Positives
Total Positives + True Negatives

Total Negatives

(cid:17)

.

Note that for applications in human trafﬁcking detection,
MAE and Acc. are of primary interest. Whereas for a more
general comparison among the models, the class imbalance
robust metrics, MAEM and Wt. Acc., might be more suit-
able. Bootstrapping or increasing the weight of samples in
smaller classes can improve MAEM and Wt. Acc. at the cost
of MAE and Acc..

The text data need to be vectorized before they can be fed
into the baseline models (whereas vectorization is built into

6 Backpage was seized by FBI in April 2018, but we have ob-
served that escort ads across different websites are often similar,
and a survivor survey shows that trafﬁckers post their ads on multi-
ple websites (THORN and Bouch´e 2018). Thus, we argue that the
training data from Backpage are still useful, which is empirically
supported by our qualitative analysis in Section 4.4.

Figure 2: Ordinal regression layer with order penalty.

as follows: penalize such conﬂicts in the training phase by
adding

P (X; λ) = λ

k−2
(cid:88)

i=1

max

(cid:110)

(cid:98)fi+1(X) − (cid:98)fi(X), 0

(cid:111)

to the loss function for a sample X, where λ is a penalty
parameter (Figure 2). For sufﬁciently large λ the estimated
probabilities will respect the monotonicity condition; re-
specting this condition improves the interpretability of the
predictions, which is vital in applications like the one we
consider here as stakeholders are given the estimated prob-
abilities. We also hypothesize that the order penalty may
serve as a regularizer to improve each binary classiﬁer (see
the ablation test in Section 4.3).

All three components of our model (word embeddings,
GF-RNN, and multi-labeled logistic regression layer) can be
trained jointly, with word embeddings optionally held ﬁxed
or given a smaller learning rate for ﬁne-tuning. The hyperpa-
rameters for all components are given in the Appendix. They
are selected according to either literature or grid-search.

4 Experiments
We ﬁrst describe the datasets we use to train and evaluate our
models. Then we present a detailed comparison of our pro-
posed model with commonly used ordinal regression models
as well as the previous state-of-the-art classiﬁcation model
by Tong et al. (2017). To assess the effect of each component
in our model, we perform an ablation test where the compo-
nents are swapped by their more standard alternatives one at
a time. Next, we perform a qualitative analysis on the model
predictions on the raw data, which are scraped from a dif-
ferent escort website than the one that provides the labeled
training data. Finally, we conduct an emoji analysis using
the word embeddings trained on raw escort ads.

4.1 Datasets
We use raw texts scraped from Backpage and TNABoard
to pre-train the word embeddings, and use the same la-
beled texts Tong et al. (2017) used to conduct model com-
parisons. The raw text dataset consists of 44,105 ads from
TNABoard and 124,220 ads from Backpage. Data clean-
ing/preprocessing includes joining the title and the body of

SigmoidOrder PenaltyLabel

Description

Count

1
Strongly
Unlikely
1,977

2

Unlikely

1,904

3
Slightly
Unlikely
3,619

4

Unsure

796

5
Weakly
Likely
3,515

6

Likely

457

7
Strongly
Likely
82

Table 1: Description and distribution of labels in Trafﬁcking-10K.

ORNN). The standard practice is to tokenize the texts us-
ing n-grams and then create weighted term frequency vec-
tors using the term frequency (TF)-inverse document fre-
quency (IDF) scheme (Beel et al. 2016; Manning, Ragha-
van, and Sch¨utze 2009). The speciﬁc variation we use is
the recommended unigram + sublinear TF + smooth IDF
(Manning, Raghavan, and Sch¨utze 2009; Pedregosa et al.
2011). Dimension reduction techniques such as Latent Se-
mantic Analysis (Dumais 2004) can be optionally applied
to the frequency vectors, but Schuller, Mousa, and Vrynio-
tis (2015) concluded from their experiments that dimension
reduction on frequency vectors actually hurts model perfor-
mance, which our preliminary experiments agree with.

All models are trained and evaluated using the same
(w.r.t. data shufﬂe and split) 10-fold cross-validation (CV)
on Trafﬁcking-10k, except for HTDN, whose result is read
from the original paper (Tong et al. 2017)7. During each
train-test split, 2/9 of the training set is further reserved as
the validation set for tuning hyperparameters such as L2-
penalty in IT, AT and LAD, and learning rate in ORNN. So
the overall train-validation-test ratio is 70%-20%-10%. We
report the mean metrics from the CV in Table 2. As previous
research has pointed out that there is no unbiased estima-
tor of the variance of CV (Bengio and Grandvalet 2004),
we report the naive standard error treating metrics across
CV as independent. Recall that a 95% conﬁdence interval
is roughly the point estimate ± 1.96 × the standard error.

We can see that ORNN has the best MAE, MAEM and
Acc. as well as a close 2nd best Wt. Acc. among all models.
Its Wt. Acc. is a substantial improvement over HTDN de-
spite the fact that the latter use both text and image data. It is
important to note that HTDN is trained using binary labels,
whereas the other models are trained using ordinal labels
and then have their ordinal predictions converted to binary
predictions. This is most likely the reason that even the base-
line models except for LAD can yield better Wt. Acc. than
HTDN, conﬁrming our earlier claim that polarizing the or-
dinal labels during training may lead to information loss.

4.3 Ablation Test
To ensure that we do not unnecessarily complicate our
ORNN model, and to assess the impact of each compo-
nent on the ﬁnal model performance, we perform an ab-
lation test. Using the same CV and evaluation metrics, we
make the following replacements separately and re-evaluate
the model: 1. Replace word embeddings pre-trained from
skip-gram model with randomly initialized word embed-
dings; 2. replace gated-feedback recurrent neural network

7The authors of HTDN used a single train-validation-test split

instead of CV.

with long short-term memory network (LSTM); 3. disable
batch normalization; 4. disable residual connection; 5. re-
place the multi-labeled logistic regression layer with a soft-
max layer (i.e., let the model perform classiﬁcation, treating
the ordinal response variable as a categorical variable with
k classes); 6. replace the multi-labeled logistic regression
layer with a 1-dimensional linear layer (i.e., let the model
perform regression, treating the ordinal response variable as
a continuous variable) and round the prediction to the near-
est integer during testing; 7. set the order penalty to 0. The
results are shown in Table 3.

The proposed ORNN once again has all the best met-
rics except for Wt. Acc. which is the 2nd best. Note that
if we disregard the ordinal labels and perform classiﬁcation
or regression, MAE falls off by a large margin. Setting order
penalty to 0 does not deteriorate the performance by much,
however, the percent of conﬂicting binary predictions (see
Section 3.3) rises from 1.4% to 5.2%. So adding an order
penalty helps produce more interpretable results8.

4.4 Qualitative Analysis of Predictions
To qualitatively evaluate how well our model predicts on raw
data and observe potential patterns in the ﬂagged samples,
we obtain predictions on the 44,105 unlabelled ads from
TNABoard with the ORNN model trained on Trafﬁcking-
10k, then we examine the samples with high predicted like-
lihood to come from trafﬁckers. Below are the top three sam-
ples that the model considers likely:
• “amazing reviewed crystal only here till fri book now
please check our site for the services the girls provide
all updates specials photos rates reviews njfantasygirls
. . . look who s back amazing reviewed model saman-
tha. . . brand new spinner jessica special rate today 250 hr
21 5 4 120 34b total gfe total anything goes no limits. . . ”
• “2 hot toght 18y o spinners 4 amazing providers today

specials. . . ”

• “asian college girl is visiting bellevue service type escort
hair color brown eyes brown age 23 height 5 4 body type
slim cup size c cup ethnicity asian service type escort i
am here for you settle men i am a tiny asian girl who is
waiting for a gentlemen. . . ”

Some interesting patterns in the samples with high predicted
likelihood (here we only showed three) include: mentioning
of multiple names or > 1 providers in a single ad; possibly
intentional typos and abbreviations for the sensitive words
such as “tight” → “toght” and “18 year old” → “18y o”;

8It is possible to increase the order penalty to further reduce
or eliminate conﬂicting predictions, but we ﬁnd that a large order
penalty harms model performance.

MAE

Model
ORNN 0.769 (0.009)
0.807 (0.010)
IT
0.778 (0.009)
AT
0.829 (0.008)
LAD
0.794 (0.012)
MC
-
HTDN

MAEM
1.238 (0.016)
1.244 (0.011)
1.246 (0.012)
1.298 (0.016)
1.286 (0.018)
-

Acc.
0.818 (0.003)
0.801 (0.003)
0.813 (0.003)
0.786 (0.004)
0.804 (0.003)
0.800

Wt. Acc.
0.772 (0.004)
0.781 (0.004)
0.755 (0.004)
0.686 (0.003)
0.767 (0.004)
0.753

Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic
regression (IT), All-Threshold ordinal logistic regression (AT), Least Absolute Deviation (LAD), multi-class logistic regression
(MC), and the Human Trafﬁcking Deep Network (HTDN) in terms of Mean Absolute Error (MAE), macro-averaged Mean
Absolute Error (MAEM ), binary classiﬁcation accuracy (Acc.) and weighted binary classiﬁcation accuracy (Wt. Acc.). The
results are averaged across 10-fold CV on Trafﬁcking-10k with naive standard errors in the parentheses. The best and second
best results are highlighted.

Model
0. Proposed ORNN
1. Random Embeddings
2. LSTM
3. No Batch Norm.
4. No Res. Connect.
5. Classiﬁcation
6. Regression
7. No Order Penalty

MAE
0.769 (0.009)
0.789 (0.007)
0.778 (0.009)
0.780 (0.009)
0.775 (0.008)
0.785 (0.012)
0.850 (0.009)
0.769 (0.009)

MAEM
1.238 (0.016)
1.254 (0.013)
1.261 (0.021)
1.311 (0.013)
1.271 (0.020)
1.253 (0.017)
1.279 (0.016)
1.251 (0.016)

Acc.
0.818 (0.003)
0.810 (0.002)
0.815 (0.003)
0.815 (0.003)
0.816 (0.003)
0.812 (0.004)
0.784 (0.004)
0.818 (0.003)

Wt. Acc.
0.772 (0.004)
0.757 (0.003)
0.764 (0.003)
0.754 (0.004)
0.766 (0.004)
0.780 (0.004)
0.686 (0.006)
0.769 (0.004)

Table 3: Ablation test. Except for models everything is the same as Table 2.

keywords that indicate traveling of the providers such as “till
fri”, “look who s back”, and “visiting”; keywords that hint
on the providers potentially being underage such as “18y
o”, “college girl”, and “tiny”; and switching between third
person and ﬁrst person narratives.

4.5 Emoji Analysis

The ﬁght against human trafﬁckers is adversarial and dy-
namic. Trafﬁckers often avoid using explicit keywords when
advertising victims, but instead use acronyms, intentional ty-
pos, and emojis (Tong et al. 2017). Law enforcement main-
tains a lexicon of trafﬁcking ﬂags mapping certain emojis
to their potential true meanings (e.g., the cherry emoji can
indicate an underaged victim), but compiling such a lexicon
manually is expensive, requires frequent updating, and re-
lies on domain expertise that is hard to obtain (e.g., insider
information from trafﬁckers or their victims). To make mat-
ters worse, trafﬁckers change their dictionaries over time and
regularly switch to new emojis to replace certain keywords
(Tong et al. 2017). In such a dynamic and adversarial envi-
ronment, the need for a data-driven approach in updating the
existing lexicon is evident.

As mentioned in Section 3.1, training a skip-gram model
on a text corpus can map words (including emojis) used in
similar contexts to similar numeric vectors. Besides using
the vectors learned from the raw escort ads to train ORNN,
we can directly visualize the vectors for the emojis to help
identify their relationships, by mapping the vectors to a 2-

dimensional space using t-SNE9 (van der Maaten and Hin-
ton 2008) (Figure 3).

We can ﬁrst empirically assess the quality of the emoji
map by noting that similar emojis do seem clustered to-
gether: the smileys near the coordinate (2, 3), the ﬂowers
near (-6, -1), the heart shapes near (-8, 1), the phones near
(-2, 4) and so on. It is worth emphasizing that the skip-gram
model learns the vectors of these emojis based on their con-
texts in escort ads and not their visual representations, so the
fact that the visually similar emojis are close to one another
in the map suggests that the vectors have been learned as
desired.

The emoji map can assist anti-trafﬁcking experts in ex-
panding the existing lexicon of trafﬁcking ﬂags. For ex-
ample, according to the lexicon we obtained from Global
Emancipation Network10, the cherry emoji and the lollipop
emoji are both ﬂags for underaged victims. Near (-3, -4)
in the map, right next to these two emojis are the porce-
lain dolls emoji, the grapes emoji, the strawberry emoji, the
candy emoji, the ice cream emojis, and maybe the 18-slash
emoji, indicating that they are all used in similar contexts
and perhaps should all be ﬂags for underaged victims in the

9t-SNE is known to produce better 2-dimensional visualizations
than other dimension reduction techniques such as Principal Com-
ponent Analysis, Multi-dimensional Scaling, and Local Linear Em-
bedding (van der Maaten and Hinton 2008).

10Global Emancipation Network is a non-proﬁt organization
dedicated to combating human trafﬁcking. For more information
see https://www.globalemancipation.ngo.

Figure 3: Emoji map produced by applying t-SNE to the emojis’ vectors learned from escort ads using skip-gram model. For
visual clarity, only the emojis that appeared most frequently in the escort ads we scraped are shown out of the total 968 emojis
that appeared.

updated lexicon.

If we re-train the skip-gram model and update the emoji
map periodically on new escort ads, when trafﬁckers switch
to new emojis, the map can link the new emojis to the
old ones, assisting anti-trafﬁcking experts in expanding the
lexicon of trafﬁcking ﬂags. This approach also works for
acronyms and deliberate typos.

5 Discussion
Human trafﬁcking is a form of modern day slavery that vic-
timizes millions of people. It has become the norm for sex
trafﬁckers to use escort websites to openly advertise their
victims. We designed an ordinal regression neural network
(ORNN) to predict the likelihood that an escort ad comes
from a trafﬁcker, which can drastically narrow down the set
of possible leads for law enforcement. Our ORNN achieved
the state-of-the-art performance on Trafﬁcking-10K (Tong
et al. 2017), outperforming all baseline ordinal regression
models as well as improving the classiﬁcation accuracy over
the Human Trafﬁcking Deep Network (Tong et al. 2017).
We also conducted an emoji analysis and showed how to use
word embeddings learned from raw text data to help expand
the lexicon of trafﬁcking ﬂags.

Since our experiments, there have been considerable ad-
vancements in language representation models, such as
BERT (Devlin et al. 2018). The new language representa-

tion models can be combined with our ordinal regression
layer, replacing the skip-gram model and GF-RNN, to po-
tentially further improve our results. However, our contri-
butions of improving the cost function for ordinal regres-
sion neural networks, qualitatively analyzing patterns in the
predicted samples, and expanding the trafﬁcking lexicon
through a data-driven approach are not dependent on a par-
ticular choice of language representation model.

As for future work in trafﬁcking detection, we can design
multi-modal ordinal regression networks that utilize both
image and text data. But given the time and resources re-
quired to label escort ads, we may explore more unsuper-
vised learning or transfer learning algorithms, such as using
object detection (Ren et al. 2015) and matching algorithms
to match hotel rooms in the images.

Acknowledgments

We thank Cara Jones and Marinus Analytics LLC for shar-
ing the Trafﬁcking-10K dataset. We thank Praveen Bod-
igutla for his suggestions on Natural Language Processing
literature.

dimension 2dimension 1Supplemental Materials

Hyperparameters of the Proposed Ordinal
Regression Neural Network
Word Embeddings: speedup method: negative sampling;
number of negative samples: 100; noise distribution: uni-
gram distribution raised to 3/4rd; batch size: 16; window
size: 5; minimum word count: 5; number of epochs: 50; em-
bedding size: 128; pretraining learning rate: 0.2; ﬁne-tuning
learning rate scale: 1.0.
GF-RNN: hidden size: 128; dropout: 0.2; number of layers:
3; gradient clipping norm: 0.25; L2 penalty: 0.00001; learn-
ing rate decay factor: 2.0; learning rate decay patience: 3;
early stop patience: 9; batch size: 200; output layer type:
mean-pooling; minimum word count: 5; maximum input
length: 120.
Multi-labeled Logistic Regression Layer:
scheme: uniform; conﬂict penalty: 0.5.

task weight

Access to the Source Materials
The ﬁght against human trafﬁcking is adversarial, hence the
access to the source materials in anti-trafﬁcking research
is typically not available to the general public by choice,
but granted to researchers and law enforcement individually
upon request.
Source code: https://gitlab.com/BlazingBlade/TrafﬁcKill
Trafﬁcking-10k: cara@marinusanalytics.com
Trafﬁcking lexicon: sherrie@globalemancipation.ngo

References

[Amin 2010] Amin, S.

2010. A step towards modeling
and destabilizing human trafﬁcking networks using machine
learning methods. Conference: Artiﬁcial intelligence for de-
velopment, papers from the 2010 AAAI Spring Symposium,
Techinical Report SS10-01 (pp. 2-7), Stanford.

[Baccianella, Esuli, and Sebastiani 2009] Baccianella,

S.;
Esuli, A.; and Sebastiani, F. 2009. Evaluation measures
9th International Conference on
for ordinal regression.
Intelligent Systems Design and Applications.
[Beel et al. 2016] Beel, J.; Gipp, B.; Langer, S.; and Bre-
itinger, C. 2016. Research-paper recommender systems: a
literature survey. International Journal on Digital Libraries
17(4):305–338.

[Bengio and Grandvalet 2004] Bengio, Y., and Grandvalet, Y.
2004. No unbiased estimator of the variance of k-fold cross-
validation. Journal of Machine Learning Research 5:1089–
1105.

[Bloomﬁeld and Steiger 1980] Bloomﬁeld, P., and Steiger, W.
1980. Least absolute deviations curve-ﬁtting. SIAM Journal
on Scientiﬁc and Statistical Computing 1(2):290–301.
[Cheng, Wang, and Pollastri 2008] Cheng, J.; Wang, Z.; and
Pollastri, G. 2008. A neural network approach to ordinal re-
gression. 2008 IEEE International Joint Conference on Neu-
ral Networks (IEEE World Congress on Computational Intel-
ligence) 1279–1284.
[Chung et al. 2015] Chung, J.; Gulcehre, C.; Cho, K.; and
Bengio, Y. 2015. Gated feedback recurrent neural networks.
ICML-15.

[Devlin et al. 2018] Devlin, J.; Chang, M.-W.; Lee, K.; and
Toutanova, K.
2018. Bert: Pre-training of deep bidi-
rectional transformers for language understanding. CoRR
abs/1810.04805.

[Dumais 2004] Dumais, S.
2004. Latent semantic analy-
sis. Annual Review of Information Science and Technology
38(1):188–230.

[Fan et al. 2008] Fan, R.; Chang, K.; Hsieh, C.; Wang, X.; and
Lin, C. 2008. Liblinear: A library for large linear classiﬁ-
cation. The Journal of Machine Learning Research 9:1871–
1874.

[Frank and Hall 2001] Frank, E., and Hall, M. 2001. A simple
approach to ordinal classiﬁcation. Lecture Notes in Artiﬁcial
Intelligence 145–156.
[Goodfellow, Bengio, and Courville 2016] Goodfellow,
I.;
Bengio, Y.; and Courville, A. 2016. Deep Learning. MIT
Press.

[Graves, Fern´andez, and Schmidhuber 2005] Graves,

A.;
Fern´andez, S.; and Schmidhuber, J.
2005. Bidirectional
lstm networks for improved phoneme classiﬁcation and
recognition. Proc. Int’l Conf. Artiﬁcial Neural Networks
799–804.

[He et al. 2016] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.
Deep residual learning for image recognition. CVPR.
[Ho and Lin 2012] Ho, C., and Lin, C. 2012. Large-scale lin-
ear support vector regression. The Journal of Machine Learn-
ing Research 13(1):3323–3348.
[International Labour Organization, Walk Free Foundation, and International Organization for Migration 2017]

International Labour Organization; Walk Free Foundation;
and International Organization for Migration. 2017. Global
estimates of modern slavery: forced labour and forced mar-
riage. Geneva: International Labour Organization.
ISBN:
978-92-2-130131-8.

[Ioffe and Szegedy 2015] Ioffe, S., and Szegedy, C.

2015.
Batch normalization: Accelerating deep network training by
reducing internal covariate shift. ICML.
[Irsoy and Cardie 2015] Irsoy, O., and Cardie, C. 2015. Mod-
eling compositionality with multiplicative recurrent neural
networks. ICLR.
[Kim 2014] Kim, Y. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP), 1746–1751.
[LeCun, Bengio, and Hinton 2015] LeCun, Y.; Bengio, Y.;
and Hinton, G. 2015. Deep learning. Nature 521:436–444.
[Li and Lin 2006] Li, L., and Lin, H. 2006. Ordinal regression
by extended binary classiﬁcation. NIPS 865–872.
[Manning, Raghavan, and Sch¨utze 2009] Manning,
C.;
Raghavan, P.; and Sch¨utze, H. 2009. An Introduction to
Information Retrieval. Cambridge University Press.
[Mikolov et al. 2013a] Mikolov, T.; Chen, K.; Corrado, G.;
and Dean, J. 2013a. Efﬁcient estimation of word representa-
tions in vector space. ICLR Workshop Papers.
[Mikolov et al. 2013b] Mikolov, T.; Sutskever, I.; Chen, K.;
Corrado, G.; and Dean, J. 2013b. Distributed representa-

multimodal models. Association for Computational Linguis-
tics.
[van der Maaten and Hinton 2008] van der Maaten, L., and
Hinton, G. 2008. Visualizing data using t-sne. Journal of
Machine Learning Research 9:2431–2456.

tions of words and phrases and their compositionality. NIPS
3111–3119.

[Narula and Wellington 1982] Narula, S., and Wellington, J.
1982. The minimum sum of absolute errors regression: A
state of the art survey. International Statistical Review 317–
326.

[Niu et al. 2016] Niu, Z.; Zhou, M.; Wang, L.; Gao, X.; and
Hua, G. 2016. Ordinal regression with multiple output cnn
for age estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 4920–4928.
[Pedregosa et al. 2011] Pedregosa, F.; Varoquaux, G.; Gram-
fort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Pret-
tenhofer, P.; Weiss, R.; Dubourg, V.; Vanderplas, J.; Passos,
A.; Cournapeau, D.; Brucher, M.; Perrot, M.; and Duchesnay,
E. 2011. Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research 12:2825–2830.
[Pedregosa-Izquierdo 2015] Pedregosa-Izquierdo, F.

2015.
Feature extraction and supervised learning on fMRI: from
practice to theory. Ph.D. Dissertation, Universit´e Pierre et
Marie Curie, Paris VI.

[Ren et al. 2015] Ren, S.; He, K.; Girshick, R.; and Sun, J.
2015. Faster r-cnn: Towards real-time object detection with
region proposal networks. NIPS.
[Rennie and Srebro 2005] Rennie, J., and Srebro, N. 2005.
Loss functions for preference levels: Regression with dis-
crete ordered labels. In Proc. Int’l Joint Conf. Artiﬁcial Intel-
ligence Multidisciplinary Workshop Advances in Preference
Handling.
[Rosenthal, Farra, and Nakov 2017] Rosenthal, S.; Farra, N.;
and Nakov, P. 2017. Semeval-2017 task 4: Sentiment analysis
in twitter. In Proceedings of the 11th International Workshop
on Semantic Evaluation, volume 3 of 4, 502–518.
[Ruder, Ghaffari, and Breslin 2016] Ruder, S.; Ghaffari, P.;
and Breslin, J. 2016. Insight-1 at semeval-2016 task 4: Con-
volutional neural networks for sentiment classiﬁcation and
quantiﬁcation. In Proceedings of the 10th International Work-
shop on Semantic Evaluation (SemEval 2016).
[Schuller, Mousa, and Vryniotis 2015] Schuller, B.; Mousa,
A.; and Vryniotis, V. 2015. Sentiment analysis and opin-
ion mining: on optimal parameters and performances. WIREs
Data Mining Knowl. Discov. 5:255–263.
[Socher et al. 2013] Socher, R.; Perelygin, A.; Wu,

J.;
Chuang, J.; Manning, C.; Ng, A.; and Potts, C. 2013. Recur-
sive deep models for semantic compositionality over a senti-
ment treebank. In Proceedings of EMNLP.
[Srivastava et al. 2014] Srivastava,

G.;
I.; and Salakhutdinov, R.
Krizhevsky, A.; Sutskever,
2014. Dropout: A simple way to prevent neural networks
Journal of Machine Learning Research
from overﬁtting.
15:1929–1958.

Hinton,

N.;

[Tarinelli 2018] Tarinelli, R. 2018. Online sex ads rebound,
months after shutdown of backpage. The Associated Press.
[THORN and Bouch´e 2018] THORN, and Bouch´e, V. 2018.
Survivor insights: The role of technology in domestic minor
sex trafﬁcking. THORN.
[Tong et al. 2017] Tong, E.; Zadeh, A.;

Jones, C.; and
Morency, L. 2017. Combating human trafﬁcking with deep

