0
2
0
2

b
e
F
9
2

]

G
L
.
s
c
[

2
v
4
1
3
0
0
.
2
1
9
1
:
v
i
X
r
a

ACE – An Anomaly Contribution Explainer
for Cyber-Security Applications

Xiao Zhang
Department of CS, Purdue University
USA
zhang923@purdue.edu

Manish Marwah
Micro Focus
USA
manish.marwah@microfocus.com

I-ta Lee
Department of CS, Purdue University
USA
lee2226@cs.purdue.edu

Martin Arlitt
Micro Focus
Canada
martin.arlitt@microfocus.com

Dan Goldwasser
Department of CS, Purdue University
USA
dgoldwas@cs.purdue.edu

Abstract—In this paper we introduce Anomaly Contribution
Explainer or ACE, a tool to explain security anomaly detection
models in terms of the model features through a regression
framework, and its variant, ACE-KL, which highlights the
important anomaly contributors. ACE and ACE-KL provide
insights in diagnosing which attributes signiﬁcantly contribute
to an anomaly by building a specialized linear model to locally
approximate the anomaly score that a black-box model generates.
We conducted experiments with these anomaly detection models
to detect security anomalies on both synthetic data and real data.
In particular, we evaluate performance on three public data sets:
CERT insider threat, netﬂow logs, and Android malware. The
experimental results are encouraging: our methods consistently
identify the correct contributing feature in the synthetic data
where ground truth is available; similarly, for real data sets,
our methods point a security analyst in the direction of the
underlying causes of an anomaly, including in one case leading to
the discovery of previously overlooked network scanning activity.
We have made our source code publicly available.

Index Terms—anomaly detection, model explanation, model

interpretability, cyber-security

I. INTRODUCTION
Cyber-security is a key concern for both private and public
organizations, given the high cost of security compromises
and attacks; malicious cyber-activity cost the U.S. economy
between $57 billion and $109 billion in 2016 [1]. As a result,
spending on security research and development, and security
products and services to detect and combat cyber-attacks has
been increasing [2].

Organizations produce large amounts of network, host and
application data that can be used to gain insights into cyber-
security threats, misconﬁgurations, and network operations.
While security domain experts can manually sift through some
amount of data to spot attacks and understand them, it is
virtually impossible to do so at scale, considering that even a
medium sized enterprise can produce terabytes of data in a few
hours. Thus there is a need to automate the process of detecting
security threats and attacks, which can more generally be
referred to as security anomalies.

Work done during an internship at HPE Software (now Micro Focus).

Major approaches to detect such anomalies fall into two
broad categories: human expert driven (mostly rules-based)
and machine learning based (mostly unsupervised) [3]. The
ﬁrst approach involves codifying domain expertise into rules,
for example, if the number of login attempts exceeds a thresh-
old, or more than a threshold number of bytes are transferred
during the night, and so on. While rules formulated by security
experts are useful, they are ineffective against new (zero-day)
and evolving attacks; furthermore, they are brittle and difﬁcult
to maintain. On the other hand, enabled by the vast amounts of
data collected in modern enterprises, machine learning based
approaches have become the preferred choice for detecting
security anomalies.

The machine learning models to detect security anomalies
typically output a severity or anomaly score; this score allows
ranking and prioritization of the anomalies. A security analyst
can then further investigate these anomalies to understand their
root causes, if they are true positives, and if any remedial
action is required. However, anomaly detectors typically do not
provide any assistance in this process. In fact, any direction or
pointers in terms of features, or groups of features, responsible
for a high anomaly score would allow prioritization of causes
to look at ﬁrst and thus save an analyst’s time and effort;
this would help even though the information may not directly
reveal the root cause of an anomaly. For example, based on the
contributions an anomaly detector assigns to features related
to external trafﬁc volume, number of ports active, number
of external hosts, etc, analysts would decide the course of
their investigation into the underlying causes of that particular
anomaly.

However, most anomaly detection models are black-boxes
that output an anomaly score without any associated expla-
nation or reasoning. In fact, there is an inverse relationship
between building complex models that can make accurate
predictions and explaining these predictions in a human-
interpretable way. For example, explaining the predictions of
simpler models, such as linear regression, logistic regression
or decision trees, is considerably easier compared to complex

 
 
 
 
 
 
models such as random forests or deep neural networks, which
build complex non-linear relationships between the input and
the predicted output.

As a result, when models that can explain their output
are needed, as is often the case, for example, in medical
diagnosis (a doctor needs to provide a detailed explanation
of the diagnosis to the patients [4]), or credit card application
(an explanation of why or why not a particular application
is approved is usually required [5]), simpler models are
preferred. However, interpretability comes at a cost since in
most instances complex models tend to have higher accuracy.
Therefore, there is an unavoidable trade-off between model
interpretability and model accuracy. Recently deep learning
models are being successfully used for cyber-security applica-
tions [6]–[9]. In fact, a part of the focus of a recently organized
workshop is the application of deep learning to security [10].
In this paper, we focus on explaining the outputs of complex
models in the cyber-security anomaly detection domain, where
outputs are usually anomaly scores. We propose ACE –
Anomaly Contribution Explainer, to bridge the gap between
the predictions provided by an anomaly detection model and
the interpretation required to support human intervention in
realistic applications. Speciﬁcally, ACE provides explanations,
in terms of the features’ contributions, by building a special-
ized linear model to locally approximate the anomaly score
that a black-box anomaly detection model generates. These
explanations aid a security analyst to quickly diagnose the
reported anomaly. Our source code is publicly available1.

Our key contributions are:

• We design and implement two methods, ACE and ACE-
KL, for explaining scores of individual anomalies de-
tected by black-box models in terms of feature contri-
butions.

• We validate our methods on three data sets: 1) a synthet-
ically generated insider threat data set; 2) a real world
netﬂow data set; and 3) a real world Android malware
data set. In all of these cases, the results are encouraging
and improved upon a recent work [11] as a baseline.

The high-level overview of our approach is shown in
Figure 1, and we focus on the meta-model approximation to
explain the score of a particular anomaly.

Fig. 1: Overview of the anomaly detection and interpretation
workﬂow for the ACE or ACE-KL meta-model.

1Source code available at https://github.com/cosmozhang/ACE-KL

II. RELATED WORK

While model interpretability and explanation have a long
history [12]–[14], the recent success and rise in popularity of
complex machine learning models (such as deep neural net-
works) has led to a surge of interest in model interpretability,
as these complex models offer no explanation of their output
predictions. Given the extensive literature on this subject, we
only discuss work most related to ours; Guidotti et al. [15]
provide a comprehensive survey on explainability.

Methods for generating explanations from complex models
[16]–[20],
fall into two main categories: 1) model-speciﬁc
which exploit a model’s internal structure and as a result
only work for that speciﬁc model type; and 2) model-agnostic
[21]–[23] or black-box methods, which do not depend on
the underlying model type. Our work belongs to the second
category.

Several model-agnostic methods investigate the sensitivity
of output with respect to the inputs to explain the output: An
early attempt called ExplainD used additive models to weight
the importance of features with a graphical explanation [24].
In Strumbelj et al.’s work [25], the authors exploited notions
from coalitional game theory to explain the contribution of
value of different individual features. LIME [11], designed
for classiﬁcation problems, was built to explain a new data
point after a black-box model is trained. MFI [26] is a non-
linear method able to detect features impacting the prediction
through their interaction with other features. More recently,
LORE [27] used a synthetic neighbourhood generated through
a genetic algorithm to explain the feature importance and
SHAP [28] assigns each feature an importance value for a
particular prediction. Our proposed methods belong to this
category, and are closest to LIME [11].

Anomaly detection is widely studied and an important topic
in data mining. However, explanation of the detected anoma-
lies has received relatively little attention from researchers. For
instance, one of the most widely cited surveys on anomaly
detection [29] makes no reference to explainability. Research
on anomaly detection and explainability includes: work on
anomaly localization [30], [31], which refers to the task of
identifying faulty sensors from a population of sensors; feature
selection or importance [32]; estimating model sensitivity [33];
and method speciﬁc techniques [34], [35]. Despite their advan-
tages, these methods are either tailored too closely for speciﬁc
anomaly detection methods, or only consider sensitivity of
inputs, not their entire contribution, and are not suitable for
the security domain where anomalies and methods to detect
evolve rapidly.

III. METHODS

A. Problem Statement

Formally, the model explanation problem in the context of

anomaly detection can be stated as follows:

Given 1) a black-box anomaly detection model f , an arbi-
trary function with an input x having M features: x1, . . . , xM ,
which outputs an anomaly score As, that is, f : x → As,

where As ∈ [0, ∞] is a scalar; and 2) a random data point x(cid:48)
that produces score As(cid:48), the goal is to estimate c1, . . . , cM ,
the normalized contributions of each feature of x(cid:48). Note that
a ci may be zero if it does not contribute to an anomaly.

B. Assumptions and Observations

We assume the output of the anomaly detector is an anomaly
score in the range [0, ∞], with 0 indicating no anomaly, and
the score monotonically increasing with the severity of the
anomaly. Such a score is widely used in anomaly detectors.
Note that if an anomaly detector outputs an anomaly proba-
bility, PA(X), it is easy to convert it to such a score by the
transformation: As = − log(1 − PA(X)).

A careful study of existing techniques such as LIME [11]
reveals their unsuitability for anomaly explanation. These
methods can only explain the importance of a feature locally,
but not the whole contribution of that feature. For example,

value; further, the convexity of this function simpliﬁes the
computation.

We deﬁne As as the anomaly score, calculated by the
blackbox model. Further, to normalize all of the contributions
towards the anomaly score, by denoting the normalized con-
tribution of feature i as ci, we formally deﬁne the normalized
contribution (“contribution” thereafter) of each feature as

ci =

log(1 + exi·wi

)

M
(cid:80)
j=1

log(1 + exi·wi )

.

(1)

To approximate a particular anomaly score As generated by
a black-box model at a point x of interest, we form the loss
function with a modiﬁed linear regression, by sampling the
neighborhood of x to obtain N neighbors and obtaining their
corresponding N anomaly scores:

consider a linear regression scenario, i.e.,

xi · wi = As,

loss =

M
(cid:80)
i=1

1
N

N
(cid:88)

j=1

πx(xj) · (w(cid:124)xj − Asj)2 + α||w||2
2,

where xi is the ith feature of the vector (here we also encode
the bias term b as wM , and the corresponding xM is always
1). Assume a given feature xa is of “small
importance”,
determined by its non-signiﬁcant corresponding weight wa,
(assuming wa is a value close to 0.01). However, if xa is
extremely large in a new example, for instance, 1000, and
As = 50, the multiplication of xa and wa still makes a large
contribution to the predicted value As.

This observation has practical implications in anomaly de-
tection problems, especially in security-related problems. For
example, when a feature tends to appear in some range of
values in training, a trained black-box model will weigh it
accordingly. After the well-trained model is deployed, a new
attack prototype can evolve focusing on speciﬁc attributes,
which were neglected at training time, but now takes high
attribute values. Even if the anomaly may be detected by a well
trained black-box model as it results in high output scores,
the underlying reason might escape the security analysts’
attention.

C. Anomaly Contribution Explainer (ACE)

ACE explains a speciﬁc anomaly by computing its feature
contribution vector obtained through a local linear approxima-
tion of the anomaly score. Using this simple approximation,
the real contribution that ith feature xi makes to form As is
naturally xi · wi. However, it is possible some xi · wi are nega-
tive. These terms correspond to features that negatively impact
an anomaly, and thus cannot be its cause. We want to discard
these terms and focus on the features positively contributing
to an anomaly. Therefore, we use the “softplus” function [36],
which is a “smoothed” relu function to model the contribution
of xi ·wi towards the entire anomaly. The intuition behind this
choice is evident: we calculate the contribution by neglecting
the negative components while considering the positive part
linearly; this function forces all negative components to 0
and retains all the positive components linear to their original

where Asj is the anomaly score generated by a black-box
model for the jth neighbor, α set to be 1 in this study is
the hyper parameter that controls the L2 norm regularizer,
and πx(xj) is the weight calculated by a distance kernel for
the jth neighbor. The parameters are estimated by minimizing
the loss function, using the neighbourhood of the original
example formed through sampling. Based on the fact that
this neighbourhood is close enough to the point of inter-
section between the surface and the tangent plane, we use
this neighbourhood to approximate the tangent plane, which
is the linear regression. We choose the normal distribution
N (x, 0.01I), where I is an identity matrix for continuous
features, as the neighborhood region to ensure the samples
are close enough to the examined point; and a Bern (0.1)
distribution to ﬂip the value for binary features x ∈ {0, 1}M
for the same reason. A distance kernel πx is used to calculate
the distance between the examined point and the neighbors as
such: πx(xj) = exp(−D(x, xj)2/σ2), where D(x, xj) is the
distance between the original point x and the neighbor xj,
which in our study was used as the Euclidean distance.σ is
a pre-deﬁned kernel width; here we use 0.75 ×
M . Thus,
the larger the distance, the smaller the weight of that neighbor
in parameter estimation, and vice versa. The overview of this
approach is shown in Algorithm 1.

√

D. Anomaly Contribution Explainer with KL Regularizer
(ACE-KL)

The ACE-KL model extends the ACE model by adding
an additional regularizer. This regularizer tries to maximize
the KL divergence between a uniform distribution and the
calculated distribution of contributions of all the inspected
features. By adding this regularizer to our loss function,
our anomaly contribution explainer assigns contribution to
inspected features in a more distinguishable way, inducing
more contributions from the dominant ones and reducing
the contributions from those less dominant ones. The KL

5:
6:

7:
8:

Algorithm 1 Anomaly Contribution Explainer (ACE)

Require: black-box model f, Number of neighbors N
Require: The sample x to be examined
Require: Distance kernel πx, Number of feature K

(cid:46) πx
measures the distance between a sample and x, which is
used as the inverse weight

1: Z ← {}
2: for j ∈ {1, 2, 3, . . . , N } do
if x ∈ RM then
3:
4:

xj ← sample from N (x, 0.01)

(cid:46) N is a normal

distribution

else if x ∈ {−1, 1}M then

xj ← f lip(x) with Bern (0.1)

(cid:46) Bern is a

Bernoulli distribution

end if
Asj ← f(xj)
Z ← Z ∪ (cid:104)xj, f(xj), πx(xj)(cid:105)

9:
10: end for
11: w ← min
w
12: Compute and sort wi

loss(Z)

ith feature

j · xi

j for each i (cid:46) i is the index for

13: Pick the top K from the sorted results and calculate the

contribution (Eq. 1)

divergence between a uniform distribution and a particular
distribution takes the following form:

KL(P ||Q) =

M
(cid:88)

i

P (i) log

P (i)
Q(i)

, P (i) ∼ U nif orm,

(2)

where P (i) is the uniform distribution and Q(i) is the

calculated distribution.

Hence, the loss function is formalized as following:

loss =

1
N

N
(cid:88)

πx(xj) · (w(cid:124)xj − Asj)2

j=1
+ α||w||2

2 − βKLj(P ||Q),

where β set
to 50 in this study is the hyper parameter
to control the KL regularizer. This formulation forces the
calculated distribution to be peaky. Therefore, in terms of
contributions, those features that contribute most get better
explained than others. Intuitively, this characteristic yields a
better visualization for security analysts in real applications.
Further, a merit that our ACE-KL model retains is that the
new loss function is still a convex function. We sketch the
proof by taking advantage of the Scalar Composition Theorem
[37]:

Corollary 1. The loss function of ACE-KL model is a convex
function, w.r.t. its model parameters.

Proof. The formulation of the loss function for ACE-KL con-
sists of two parts: a regular ridge regression and an additional
regularizer. It is trivial to show a ridge regression is a convex
function w.r.t. its parameters.

Now we show the additional regularizer is also a convex
function (for each j):

KL(P ||Q) =

=

=

M
(cid:88)

i

M
(cid:88)

i

M
(cid:88)

i

P (i) log

P (i)
Q(i)

P (i) log P (i) − P (i) log Q(i)

C − c log Q(i)

(P (i) ∼ U nif orm , so P (i) is a constant)

C − c log

log(1 + ewj ·xj

)

M
(cid:80)
j=1

log(1 + ewj ·xj )

C − c log log(1 + ewi·xi

) + c log

M
(cid:88)

=

i

M
(cid:88)

=

i

M
(cid:88)

j=1

log(1 + ewj ·xj

),

where log(1 + ewi·xi
) is convex. By the Scalar Composi-
tion Theorem, the KL regularizer is convex. Then a linear
combination of the convex ridge regression part and the KL
regularizer retain the property of convexity.

IV. EXPERIMENTS AND RESULTS

A. Data sets

We validate our methods on three security related data sets.
The ﬁrst data set is the CERT Insider Threat v6.2 (abbreviated
as CERT) [38], [39]. It is a synthetically generated, realistic
data set consisting of application-level system logs, such as
HTTP get/post requests, emails and user login/logout events.
The second data set contains pcap traces from UNB [40],
which we converted to netﬂow logs using nfdump. It
is
partially labelled with port scanning and intrusion events.
Lastly, the third data set–AndroidMalware [41]–is a collection
of about 1,200 malwares observed on Android devices.

B. Feature Extraction

To evaluate our methods, we build anomaly detection mod-
els on these data sets. Note that the models can be supervised
or unsupervised as long as they produce an anomaly score.
Furthermore, while we ensure these models have reasonable
accuracy, building the best possible anomaly detection models
for these data sets is not the focus of this work. We extract
the following features from the data sets.
CERT. Similar to a previous study [6], we extract count
features conditioned on time of day, where a day is uniformly
discretized into four intervals. In our experiments, we use
one day as the smallest time window and each example is
the composite record of day-user. We examine three different
Internet activities: “WWW visit”, “WWW upload” and “WWW
download”. Hence, in this setting, the total number of features
are 3 × 4 = 12, and so is the input dimensionality of the
autoencoder model (one of the baselines).
UNB Netﬂow: We extracted 108 features that can be catego-
rized into three sets: Count, Bitmap, and Top-K. The Count

features count the number of bytes/packets for incoming and
outgoing trafﬁc; the Bitmap features include type of services,
TCP ﬂags, and protocols; the Top-K features encode the IP
addresses with trafﬁc ﬂows ranked in top k over all
the
addresses.
AndroidMalware: 122 binary features are extracted, mainly
related to frequent permission requests from apps.

C. Evaluation Metrics

We consider the contributions to be a distribution over
features. To quantitatively evaluate contributions produced by
a method, we use its Kullback-Leibler (KL) divergence with
respect to ground truth contributions. The KL divergence mea-
sures how one probability distribution diverges from another
probability distribution. Given the distribution of modeled con-
tributions, Q, and the ground truth contributions distribution
of the data point, P, the KL divergence is formulated as:

KL(P ||Q) =

M
(cid:88)

i

P (fi) log

P (fi)
Q(fi)

,

(3)

where fi is the ith feature. The lower the KL divergence,
the closer the modeled contribution is to the real contribution
for that data point. Note that this KL divergence metrics is
different from the regularizer term in ACE-KL, which forces
the formulated distribution away from a uniform distribution.

D. Baseline Methods

We use LIME [11] as our main baseline. We consider it
representative of similar methods since it is recent and well
cited. LIME only works for classiﬁcation problems; however,
most anomaly detection problems require an anomaly score to
express the conﬁdence of detection. We therefore extend LIME
to support regression problems. This extension is straightfor-
ward: in classiﬁcation problems, each feature is mapped onto
a classiﬁcation class by looking at the estimated weights of
each feature to decide the importance of that feature to the
particular class. In a regression problem, we can assume it to
be a one-class classiﬁcation problem. We therefore transform
LIME from multi-class classiﬁcation to a one-class problem,
and examine the importance of each feature to the anomaly
score.

E. Evaluation on CERT

To evaluate ACE and ACE-KL on CERT, ﬁrst we train an
autoencoder as our black-box model, although in principle it
could be any model. Its anomaly score (As) on a data point
is computed as the mean squared error (MSE) between the
input and the output vector. In addition to applying ACE
and ACE-KL, we compute feature contributions from the
autoencoder model using the reconstruction error of each of
the inputs, similar to [6]. Thus, the autoencoder model serves
as an additional baseline. While the CERT data set has some
anomalies, we also artiﬁcially inject some by perturbing the
input features. The data set contains two years of activities.
We use the ﬁrst year of the data set for injected anomalies
detection, as it has no anomaly marked. We also detect
anomalies present in the second year.

1) Evaluation on Injected Anomalies: We perturb individ-
ual features and groups of features. However, due to space
limitations, we only present perturbation of groups of ﬁve
features. The rest of the results on injected anomalies are
described in the appendix.

a) Multiple Feature Perturbation: The synthetic anoma-
lies are created as follows. We ﬁrst calculated the mean values
of each feature based on the non-preprocessed raw data, and
draw from a Poisson distribution based on the mean value of
each feature using P (x) = e−λ λk
k! . This sampling approach
ensures that ﬁrst, all the synthesized features are integers;
second, the original value is around the mean of the raw data.
After we sample from this distribution, we perturbed it by
adding a value λ to the feature x: x(cid:48) = x + λ. This new
value’s expectation is E[x(cid:48)] = 2λ, which exceeds the mean
value of λ by a large magnitude, thus this perturbation can
represent an anomaly from the original data.

We randomly chose ﬁve features to perturb. Each feature
of a data point is standardized to N (0, 1) using the mean and
the standard deviation of that feature of the training set, and
fed into the trained black-box to create an anomaly score.

The results are shown in Figure 2. ACE accurately iden-
tiﬁed the contributions in both anomalies, and performed
signiﬁcantly better than both baselines considered according
to the KL-divergence metric. ACE-KL, while not as accurate
as ACE, highlights the top contributors.

(a) Features 1, 3, 10, 8, 7 were perturbed.

(b) Features 6, 0, 9, 7, 3 were perturbed.

Fig. 2: (left) Feature contributions on two synthetic examples,
with perturbation on ﬁve randomly chosen features. Contri-
bution is the percentage of a feature towards the anomaly
score. (right) KL-divergence of each method with respect to
the ground truth.

2) Evaluation on Real Anomalies: The CERT data set
contains labeled scenarios where insiders behave maliciously.
Figure 3 shows contribution analysis on the days that have the
malicious activities. In Figure 3(a) and 3(c), feature 7 captures
the malicious activities, while in Figure 3(b) feature 8 is the
ground-truth anomalous feature. The experimental results and

Contribution00.20.40.60.81Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence0123MethodsACE-KLACELIMEAutoencoderContribution00.140.280.420.560.7Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence012345MethodsACE-KLACELIMEAutoencoderthe corresponding KL-divergence are shown in Figure 3. ACE
and ACE-KL accurately capture the feature responsible for the
anomalies. Both ACE and ACE-KL have signiﬁcantly lower
KL divergence, outperforming the baselines.

for this feature was related to long-lived (i.e., persistent) TCP
connections. Although benign, this was an unusual activity
relative to other recorded trafﬁc. The other high values corre-
spond to the number of standard source and destination ports.
This was found to be related to a port scanning activity, which
was not previously discovered, i.e., was not labeled. Anomaly
2 is almost exactly similar to Anomaly 1 with a similar port
scanning activity.

(a) WWW download anomaly, feature 7, day 398.

(b) WWW upload anomaly, feature 8, day 404.

(a) Anomaly 1

(c) WWW download anomaly, feature 7, day 409.

Fig. 3: Three real anomalies in the CERT data set. (left)
Feature contributions using ACE, ACE-KL and two base-
lines. (right) KL-divergence between feature contributions
computed by the methods and the ground truth contributions.
ACE and ACE-KL has the most similar contribution as the
ground truth (which is always 1.0).

F. Evaluation on UNB Netﬂow Data Set

This section presents the evaluation of ACE and ACE-KL
on UNB Netﬂow with similar settings as CERT. A separately
trained autoencoder is used as a black-box anomaly detection
model. Due to space limitations, we present results of applying
ACE and ACE-KL to only two anomalies here. Table I pro-
vides a short description of the top 10 features that are useful
to interpret the results. Figure 4 shows the feature contributions
for the anomalies, and Table II provides details on the feature
values and their contributions. Since the annotation is at the
packet level, it is not easy for a person to manually determine
the root cause for the anomaly.

For anomaly 1,

the highest contributing feature is
‘max duration in’, which is the maximum duration of an
incoming ﬂow into this IP address (192.168.1.103). After
examining the netﬂow records, we found that the high value

(b) Anomaly 2

Fig. 4: Contribution analysis on two anomalies in netﬂow data.

Identifying anomalies from netﬂow records is a time con-
suming and laborious (and thus error-prone) task. Since our
method is able to systematically provide a basic explanation
(in terms of features) of why some of the anomalies were
identiﬁed as such, the internal security expert who we consult
is convinced that our method is trustworthy and practical. As
noted earlier, several of the IP addresses exhibited multiple
distinct anomalous behaviors, as well as benign characteristics
such as the persistent TCP connections for certain applications.
As future work the expert recommended investigating how to
systematically discern between multiple anomalies involving
a single IP address, to make it easier for a security analyst to
understand which are malicious and require their attention, and
which are benign and can be ignored. This would accelerate an
analyst’s ability to respond faster to malicious activities, and
therefore improve the security of the analyst’s organizations.

G. Evaluation on Android Malware Data Set

Finally, we evaluate ACE and ACE-KL on the Android
malware data set [41]. This data set captures various features
related to app activities, including their installation methods,
activation mechanisms as well as their susceptibility to carry
malicious payloads. In this data set, each example is a numeric,
binary vector of 122 dimensions, representing features for mal-
ware detection. Peng et al. [42] successfully built probabilistic
generative models for ranking risks of those Android malwares

Contribution00.20.40.60.81Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence00.61.21.82.4MethodsACE-KLACELIMEAutoencoderContribution00.20.40.60.81Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence036912MethodsACE-KLACELIMEAutoencoderContribution00.20.40.60.81Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence00.30.60.91.21.5MethodsACE-KLACELIMEAutoencoderContribution00.070.140.210.280.35Feature Index4303013326337056981242ACE-KLACELimeContribution00.060.120.180.240.3Feature Index030923337010110510410713102ACE-KLACELime# std src ports
avg std src ports per dst ip
protos out 3

top1 out
top3 out

max duration in
# std dst ports
avg std dst ports per src ip
Flags in 3
total duration in

Number of standard source ports
Average number of standard source ports per destination IP
Third bit in the Protocol feature (3 bit feature indicating TCP, UDP,
or Other)
Top 1st outgoing IP address (in terms of bytes)
Top 3rd outgoing IP address (in terms of bytes)

(a) Features for outgoing ﬂows (when IP is source)

Maximum incoming ﬂow duration
Number of standard destination ports
Average number of standard destination ports per source IP
Third bit in the ﬂags ﬁeld
Total duration of the incoming ﬂows

(b) Features for incoming ﬂows (when IP is destination)

TABLE I: Short descriptions of top features in the results.

Index
43
0
30
13
3

26
33

70
56
98
12
42

Feature Name
max duration in
# std src ports
# std dst ports
max duration out
avg std src ports per
dst ip
min n bytes out
avg std dst ports per
src ip
protos out 3
min n bytes in
top1out
total duration out
total duration in

ACE-KL
0.207
0.195
0.174
0.100
0.064

0.062
0.052

0.046
0.041
0.025
0.018
0.018

ACE
0.317
0.030
0.071
0.011
0.177

0.034
0.250

0.021
0.035
0.005
0.008
0.041

value
239.961
158
156
240.085
1

20
1

1
20
192.168.1.101
62154.867
44405.557

Index
0
30
92
3

33

70
101
105
104
107
13
102

Feature Name
# std src ports
# std dst ports
ﬂags in 3
avg std src ports per
dst ip
avg std dst ports per
src ip
protos out 3
top4 out
top3 in
top2 in
top5 in
max duration out
top5 out

ACE-KL
0.275
0.246
0.104
0.090

ACE
0.087
0.175
0.0496
0.097

value
158
156
0
0

0.074

0.156

0

0.065
0.030
0.030
0.023
0.023
0.020
0.020

0.130
0.024
0.029
0.109
0.059
0.005
0.078

1
67.220.214.50

61.112.44.178

125.6.176.113

192.168.5.122
280.53
203.73.24.75

(a) Anomaly1: 192.168.1.103, Sunday

(b) Anomaly2: 192.168.2.110, Sunday

TABLE II: Contributions and feature values for top two anomalies in netﬂow data. The contributions in bold are the top ones.

in a semi-supervised learning setting by using a large amount
of additional unlabeled data. The risk scoring procedure is
a form of anomaly detection, and the risk scores equate to
anomaly scores. Thus, in this evaluation, we used the pre-
built hierarchical mixture of naive Bayes (HMNB) model [42]
as the black-box model to generate an anomaly score, and
applied our approach to explain the anomaly. As the HMNB
model calculates the likelihood of a malware in the population,
we use the negative log-likelihood as the anomaly score.

We inspected the four malwares that obtained the highest
anomaly score by using the pre-trained HMNB model. Before
we analyzed the anomalies using ACE and ACE-KL, all the
0s in the features were replaced by −1s, as a 0 feature will
result in a constant contribution of a feature. The contributions
of each feature is calculated using ACE and ACE-KL. The
ﬁnal results are presented in Fig 5. The feature indices are
sorted by the contributions calculated by ACE, and we only
show the top 10 features. In all four cases, ACE and ACE-KL
produce consistent contributions, although their results differ
from LIME.

To gain a better understanding of the difference between
ACE, ACE-KL and LIME, we show the probability mass
graph of all the features as the contributions for Malware 1
in Figure 6. As stated, both ACE and ACE-KL identiﬁed the

(a) Anomaly 1

(b) Anomaly 2

(c) Anomaly 3

(d) Anomaly 4

Fig. 5: Contribution analysis on four anomalies in Android
malware data. We only show the top 10 features that contribute
most signiﬁcantly to the anomaly score in terms of percentage.

same features that contribute most to the anomaly. Further,
the contribution distribution induced by ACE-KL forms a
more skewed distribution, highlighting those features that

Contribution0.000.020.030.050.06Feature Index475154111118621201217776ACE-KLACELimeContribution0.000.020.040.050.07Feature Index83100438111112099768287ACE-KLACELimeContribution0.000.020.040.050.07Feature Index83100814311112099768249ACE-KLACELimeContribution0.000.020.030.050.06Feature Index1115412064764182878673ACE-KLACELimecontribute most to the anomaly while neglecting those with
small contributions. In contrast, the contribution distribution
calculated by LIME is relatively ﬂat compared to ACE and
ACE-KL.

Fig. 6: Probability mass function for each feature in Mal-
ware 1. This forms the whole contribution distribution to the
anomaly score for this Malware.

Anomaly Remediation: Although the Android Malware
data set is labeled with anomalies, the contributing features to
these anomalies are unknown, making it difﬁcult to validate
our results. To get some degree of validation, we conducted
additional experiments which we call “anomaly remediation”.
Essentially, we change input feature values (ﬂip binary fea-
tures) to repair a particular anomaly, i.e., to see if the anomaly
score reduces signiﬁcantly for a particular example.

In these experiments, we ﬁrst ﬂip the top 10 binary con-
tributing features detected by ACE (or ACE-KL, in all four
cases the top 10 features are identical for ACE and ACE-KL)
for the four anomalies, and the top 10 features selected by
LIME. We also randomly sample 10 features among all the 112
features, and ﬂip them. Our conjecture is as follows: if the true
features causing the Android app to be classiﬁed as malware
correspond to those detected by ACE, then ﬁxing the anomaly
(by ﬂipping the features) should result in much higher drop
in the anomaly score than if the 10 features were randomly
picked. The results of our experiments are summarized in
Figure 7.

As can be seen in Figure 7, by ﬂipping the top 10 features
detected by ACE/ACE-KL, the anomaly scores generated by
the well-trained black-box model signiﬁcantly drop for all four
malwares. If we randomly pick the 10 features, the anomaly
scores increase for all the four malwares. This is expected
since only a small number of features are likely to cause
a particular anomaly, and random sampling is more likely
to select non-contributing features. Surprisingly, remediation
of the top 10 features selected by LIME result in a higher
increase in the score than random selection, which further
shows LIME is not suitable for this problem. We suspect this is
likely because LIME only considers the weight vector of the

Fig. 7: Comparison of original anomaly scores, the scores
after anomaly remediation for ACE/ACE-KL and LIME, and
the scores after random feature selection. Remediation with
ACE/ACE-KL greatly reduces the anomaly score after the
correct features contributing mostly to the anomaly score are
identiﬁed, while LIME and randomly choosing a feature to
remedy increase the anomaly score (by ﬂipping a feature that
does not contributing signiﬁcantly to the anomaly originally).

regression framework, neglecting the importance of whether
the feature is 1 or -1.

V. CONCLUSIONS

In this paper we proposed methods for explaining results
of complex security anomaly detection models in terms of
feature contributions, which we deﬁne as the percentage of a
particular feature contributing to the anomaly score. Based on
our experimental results on synthetic and real data sets, we
demonstrated that ACE consistently outperforms the baseline
approaches for anomaly detection explanation. ACE-KL helps
provide a simpler explanation focusing on the most signiﬁcant
contributors. Both approaches have valuable applications in the
area of anomaly detection explanation in security. In the future,
we plan to further validate our approach in other security
problems and other domains.

REFERENCES

[1] (2018) The cost of malicious cyber activity to the u.s. economy.
[Online]. Available: www.whitehouse.gov/wp-content/uploads/2018/03/
The-Cost-of-Malicious-Cyber-Activity-to-the-U.S.-Economy.pdf
[2] (2018) Global information security spending to exceed $124b in 2019.
[Online]. Available: www.forbes.com/sites/rogeraitken/2018/08/19/
global-information-security-spending-to-exceed-124b-in-2019-privacy-
concerns-driving-demand

[3] K. Veeramachaneni, I. Arnaldo, V. Korrapati, C. Bassias, and K. Li,
“AI2: Training a big data machine to defend,” in BigDataSecurity, HPSC
and IDS, 2016.

[4] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad,
“Intelligible models for healthcare: Predicting pneumonia risk and
hospital 30-day readmission,” in Proc. of the International Conference
on Knowledge Discovery and Data Mining (KDD), 2015.

Contribution0.0000.0150.0300.0450.060Feature IndexACEACE-KLLIMEAnomaly Score04080120160Malware1Malware2Malware3Malware4OriginalACE/ACE-KLLIMERandom[5] Y. Shi, “China’s national personal credit scoring system: A real-life intel-
ligent knowledge application,” in Proc. of the International Conference
on Knowledge Discovery and Data Mining (KDD), 2012.

[6] A. Tuor, S. Kaplan, B. Hutchinson, N. Nichols, and S. Robinson,
“Deep learning for unsupervised insider threat detection in structured
cybersecurity data streams,” in AAAI Workshop on AI for Cybersecurity
Workshop, 2017.

[7] M. Youseﬁ-Azar, V. Varadharajan, L. Hamey, and U. Tupakula,
“Autoencoder-based feature learning for cyber security applications,” in
2017 International joint conference on neural networks (IJCNN).
IEEE,
2017, pp. 3854–3861.

[8] D. S. Berman, A. L. Buczak, J. S. Chavis, and C. L. Corbett, “A survey
of deep learning methods for cyber security,” Information, vol. 10, no. 4,
p. 122, 2019.

[9] Z. Cui, F. Xue, X. Cai, Y. Cao, G.-g. Wang, and J. Chen, “Detection of
malicious code variants based on deep learning,” IEEE Transactions on
Industrial Informatics, vol. 14, no. 7, pp. 3187–3196, 2018.

of the International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS), 2015.

[31] R. Jiang, H. Fei, and J. Huan, “Anomaly localization for network data
streams with graph joint sparse pca,” in Proc. of the International
Conference on Knowledge Discovery and Data Mining (KDD), 2011.

[32] S. Hara, T. Katsuki, H. Yanagisawa, T. Ono, R. Okamoto, and
S. Takeuchi, “Consistent and efﬁcient nonparametric different-feature
selection,” in Proc. of the International Conference on Artiﬁcial Intelli-
gence and Statistics (AISTATS), 2017.

[33] W. H. Woodall, R. Koudelik, K.-L. Tsui, S. B. Kim, Z. G. Stoumbos,
and C. P. C. MD, “A review and analysis of the mahalanobistaguchi
system,” Technometrics, 2003.

[34] S. Hirose, K. Yamanishi, T. Nakata, and R. Fujimaki, “Network anomaly
detection based on eigen equation compression,” in Proc. of the Inter-
national Conference on Knowledge Discovery and Data Mining (KDD),
2009.

[35] T. Id´e, A. C. Lozano, N. Abe, and Y. Liu, “Proximity-based anomaly

[10] (2019) Deep learning and security workshop. [Online]. Available:

detection using sparse structure learning,” in SDM, 2009.

www.ieee-security.org/TC/SPW2019/DLS/

[11] M. T. Ribeiro, S. Singh, and C. Guestrin, “”why should i trust you?”:
Explaining the predictions of any classiﬁer,” in Proc. of the International
Conference on Knowledge Discovery and Data Mining (KDD), 2016.

[12] O. Biran and C. Cotton, “Explanation and justiﬁcation in machine
learning: A survey,” in IJCAI-17 Workshop on Explainable AI (XAI),
2017.

[13] F. Doshi-Velez and B. Kim, “Towards a rigorous science of interpretable

machine learning,” ArXiv e-prints, 2017.

[14] Z. C. Lipton, “The mythos of model interpretability,” Queue, 2018.
[15] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and
D. Pedreschi, “A survey of methods for explaining black box models,”
CSUR, no. 5, Aug. 2018.

[16] H. J. Suermondt, “Explanation in bayesian belief networks,” Ph.D.

dissertation, Stanford University, 1992.

[17] R. F´eraud and F. Cl´erot, “A methodology to explain neural network

classiﬁcation,” Neural Networks, 2002.

[18] M. Robnik- ˇSikonja, A. Likas, C. Constantinopoulos, I. Kononenko,
and E. ˇStrumbelj, “Efﬁciently explaining decisions of probabilistic rbf
classiﬁcation networks,” in International Conference on Adaptive and
Natural Computing Algorithms, 2011.

[19] W. Landecker, M. D. Thomure, L. M. Bettencourt, M. Mitchell, G. T.
Kenyon, and S. P. Brumby, “Interpreting individual classiﬁcations of
hierarchical networks,” in IEEE Symposium on CIDM, 2013.

[20] D. Martens, J. Huysmans, R. Setiono, J. Vanthienen, and B. Baesens,
“Rule extraction from support vector machines: An overview of issues
and application in credit scoring,” in Rule extraction from support vector
machines. Springer, 2008.

[21] M. Robnik- ˇSikonja and I. Kononenko, “Explaining classiﬁcations for
individual instances,” IEEE Transactions on Knowledge and Data En-
gineering, 2008.

[22] I. Kononenko, E.

ˇStrumbelj, Z. Bosni´c, D. Pevec, M. Kukar, and
M. Robnik- ˇSikonja, “Explanation and reliability of individual predic-
tions,” Informatica, 2013.

[23] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen,
and K.-R. M ˜Aˇzller, “How to explain individual classiﬁcation decisions,”
JMLR, 2010.

[24] B. Poulin, D. Eisner, Roman andSzafron, P. Lu, R. Greiner, D. S Wishart,
A. Fyshe, B. Pearcy, C. MacDonell, and J. Anvik, “Visual explanation of
evidence with additive classiﬁers,” in National Conference on Artiﬁcial
Intelligence, 2006.

[25] E. Strumbelj and I. Kononenko, “An efﬁcient explanation of individual

classiﬁcations using game theory,” JMLR, 2010.

[26] M. M. C. Vidovic, N. G¨ornitz, K.-R. M¨uller, and M. Kloft, “Feature
Importance Measure for Non-linear Learning Algorithms,” ArXiv e-
prints, 2016.

[27] R. Guidotti, A. Monreale, S. Ruggieri, D. Pedreschi, F. Turini, and
F. Giannotti, “Local Rule-Based Explanations of Black Box Decision
Systems,” ArXiv e-prints, 2018.

[28] S. M. Lundberg and S.-I. Lee, “A uniﬁed approach to interpreting
model predictions,” in Proc. of the Conference on Advances in Neural
Information Processing Systems (NIPS), 2017.

[29] V. Chandola, A. Banerjee, and V. Kumar, “Anomaly detection: A survey,”

CSUR, 2009.

[30] S. Hara, T. Morimura, T. Takahashi, H. Yanagisawa, and T. Suzuki,
“A consistent method for graph based anomaly localization,” in Proc.

[36] C. Dugas, Y. Bengio, F. B´elisle, C. Nadeau, and R. Garcia, “Incorpo-
rating second-order functional knowledge for better option pricing,” in
Proc. of the Conference on Advances in Neural Information Processing
Systems (NIPS), 2000.

[37] S. Boyd and L. Vandenberghe, Convex Optimization. New York, NY,

USA: Cambridge University Press, 2004.

[38] B. Lindauer, J. Glasser, M. Rosen, K. C. Wallnau, and L. ExactData,
“Generating test data for insider threat detectors.” Journal of Wireless
Mobile Networks, Ubiquitous Computing, and Dependable Applications,
2014.

[39] J. Glasser and B. Lindauer, “Bridging the gap: A pragmatic approach

to generating insider threat data,” in IEEE SPW, 2013.

[40] A. Shiravi, H. Shiravi, M. Tavallaee, and A. A. Ghorbani, “Toward
developing a systematic approach to generate benchmark datasets for
intrusion detection,” Computer Security, 2012.

[41] Y. Zhou and X. Jiang, “Dissecting android malware: Characterization

and evolution,” in IEEE Symposium on SP, 2012.

[42] H. Peng, C. Gates, B. Sarma, N. Li, Y. Qi, R. Potharaju, C. Nita-Rotaru,
and I. Molloy, “Using probabilistic generative models for ranking risks
of android apps,” in CCS, 2012.

VI. APPENDIX: ADDITIONAL EXPERIMENTAL RESULTS

A. Perturb One Feature for CERT

In this section we present the experimental results of ap-
plying different anomaly explanation methods by perturbing
only one of the twelve features in the CERT data set as
an injected anomaly in Figure 8. The left column shows
the contributions of each feature calculated by the anomaly
explanation method, and the right column the KL-divergence
between the distribution of the calculated contributions and the
uniform distribution. As we can see from Figure 8, ACE and
ACE-KL perform well across all six examples consistently,
while Autoencoder and LIME fail to capture the contribution
of the anomaly in some cases even there is only one anomaly
feature.

B. Perturb Two Feature for CERT

In this section, we present

the experimental results of
applying different anomaly explanation methods by perturbing
two of the twelve features in the CERT data set as an injected
anomaly in Figure 9. As previously described, the left column
shows the contributions of each feature calculated by anomaly
explanation method, and the right column the KL-divergence
between the distribution of the calculated contributions and
the true distribution. As seen from Figure 9, ACE and ACE-
KL perform well across all four examples consistently, while
Autoencoder only captures the second anomaly in the third

and the fourth examples, and LIME fails to capture any
of the anomalies in an accurate manner, with higher KL-
divergence compared to the true distribution. These results
further empirically support our claim that LIME is not suitable
for anomaly explanation in the security domain while ACE and
ACE-KL are very powerful tools in this application domain.

(a) Features 0 is perturbed.

(b) Features 2 is perturbed.

(a) Features 0, 1 are perturbed.

(c) Features 4 is perturbed.

(b) Features 0, 2 are perturbed.

(d) Features 6 is perturbed.

(c) Features 0, 3 are perturbed.

(e) Features 8 is perturbed.

(d) Features 0, 4 are perturbed.

Fig. 9: Feature contribution calculated by different methods
on 4 synthetic examples, where each of them has two features
perturbed. The left side are the contributions of each feature
calculated using different method, and the right side are the
KL-divergence for each method.

(f) Features 10 is perturbed.

Fig. 8: Feature contribution calculated by different methods
on 6 synthetic examples, where each of them has one feature
perturbed. The left side are the contributions of each feature
calculated using different method, and the right side are the
KL-divergence for each method.

Contribution00.20.40.60.81Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence03710MethodsACE-KLACELIMEAutoencoderContribution00.140.280.420.560.7Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence02.44.87.29.612MethodsACE-KLACELIMEAutoencoderContribution00.140.280.420.560.7Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence0.00.20.40.50.70.9MethodsACE-KLACELIMEAutoencoderContribution00.140.280.420.560.7Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence0.00.30.61.01.31.6MethodsACE-KLACELIMEAutoencoderContribution00.140.280.420.560.7Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence00.0120.0240.0360.0480.06MethodsACE-KLACELIMEAutoencoderContribution00.140.280.420.560.7Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence00.160.320.480.640.8MethodsACE-KLACELIMEAutoencoderContribution00.140.280.420.560.7Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence0246810MethodsACE-KLACELIMEAutoencoderContribution00.140.280.420.560.7Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence0246810MethodsACE-KLACELIMEAutoencoderContribution00.140.280.420.560.7Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence0246810MethodsACE-KLACELIMEAutoencoderContribution00.140.280.420.560.7Feature Index01234567891011ACE-KLACELimeAutoencoderGround TruthKL Divergence01.22.43.64.86MethodsACE-KLACELIMEAutoencoder