Relational Analysis of Sensor Attacks on
Cyber-Physical Systems

Jian Xiang∗, Nathan Fulton†, Stephen Chong∗
∗SEAS, Harvard University {jxiang, chong}@seas.harvard.edu
†MIT-IBM Watson AI Lab. nathan@ibm.com

Abstract—Cyber-physical systems, such as self-driving cars or
autonomous aircraft, must defend against attacks that target
sensor hardware. Analyzing system design can help engineers
understand how a compromised sensor could impact the system’s
behavior; however, designing security analyses for cyber-physical
systems is difﬁcult due to their combination of discrete dynamics,
continuous dynamics, and nondeterminism.

This paper contributes a framework for modeling and analyzing
sensor attacks on cyber-physical systems, using the formalism
of hybrid programs. We formalize and analyze two relational
properties of a system’s robustness. These relational properties
respectively express (1) whether a system’s safety property can
be inﬂuenced by sensor attacks, and (2) whether a system’s high-
integrity state can be affected by sensor attacks. We characterize
these relational properties by deﬁning an equivalence relation
between a system under attack and the original unattacked
system. That is, the system satisﬁes the robustness properties
if executions of the attacked system are appropriately related to
executions of the unattacked system.

We present two techniques for reasoning about the equivalence
relation and thus proving the relational properties for a system.
One proof
technique decomposes large proof obligations to
smaller proof obligations. The other proof technique adapts
the self-composition technique from the literature on secure
information-ﬂow, allowing us to reduce reasoning about the
equivalence of two systems to reasoning about properties of
a single system. This technique allows us to reuse existing
tools for reasoning about properties of hybrid programs, but
is challenging due to the combination of discrete dynamics,
continuous dynamics, and nondeterminism.

To validate the usefulness of our relational properties and proof
techniques, we present three case studies motivated by real design
ﬂaws in existing cyber-physical systems.

1
2
0
2

n
u
J

3

]

R
C
.
s
c
[

1
v
0
5
8
1
0
.
6
0
1
2
:
v
i
X
r
a

I. INTRODUCTION

Cyber-physical systems, which consist of both physical and
cyber components, are often safety and security critical [1]–
[4]. Designing secure cyber-physical systems is difﬁcult be-
cause adversaries beneﬁt from a broad attack surface that
includes both software controllers and physical components.
Sensor attacks often allow an adversary to directly control
the system under attack. For example, Cao et al. demon-
strate how to manipulate an autonomous vehicle’s distance
measurements by shining a laser into its Light Detection and

This is an extended version of the paper with the same title that appeared in
the 2021 Computer Security Foundations Symposium. This version includes
a proof of Theorem 3 in Appendix D.

Ranging (LiDAR) sensors [5], Humphreys et al. demonstrate
how spooﬁng Global Positioning System (GPS) signals may
allow an attacker to force a yacht autopilot to deviate from
a designated course [6], and Davidson et al. demonstrate a
GPS-based hijacking attack on unmanned aircraft [7]. The
breadth of the cyber-physical attack surface affords adversaries
a range of attack modalities even when hijacking control is
not possible. For example, Son et al. demonstrate how to
crash a quadcopter using a magnetic attack on a quadcopter’s
gyroscopic sensors [8].

Testing-based approaches are insufﬁcient
to guarantee the
safety of a cyber-physical system, even when the system is
not under attack. In a 2016 study on autonomous vehicles,
Kalra et al. conclude that a self-driving ﬂeet would need to
drive hundreds of millions or sometimes hundreds of billions
of miles to provide a purely testing-based reliability case [9].
Driving these miles in a representative set of road conditions
would take tens or hundreds of years depending on the size
of the test ﬂeet. The intractability of testing-based approaches
is also conﬁrmed by incompleteness results [10]. Establishing
security is even more difﬁcult than establishing safety.

The importance and difﬁculty of ensuring the safety of cyber-
physical systems motivate a growing body of work on for-
mal veriﬁcation for embedded and hybrid systems [11]–[15].
However, relatively little work considers formal veriﬁcation of
such systems in the presence of sensor attacks. Some recent
work emphasize timing aspects of sensor-related attacks [16],
[17]; however, the work model the system’s dynamics as a
deterministic discrete time dynamical system, whereas most
cyber-physical systems are best modeled with a nondetermin-
istic combination of discrete and continuous dynamics.

It is important for cyber-physical system designers to under-
stand whether a compromised sensor can result in undesired
behavior, such as violating a safety property or corrupting a
critical state. For example, the designer of an adaptive cruise
control system might want to verify that the car’s minimum
following distance is not affected by a compromised GPS
sensor.

Understanding the impact of compromised sensors requires
us to reason about relational properties [18], that
the
relationship between executions of the original uncompro-
mised system and executions of the system where some of
the sensors have been compromised. Relational properties are

is,

 
 
 
 
 
 
often harder to reason about than functional properties, as they
require reasoning simultaneously about multiple executions.
And there is less tool support for formal veriﬁcation of
relational properties, compared to functional properties.

In this work, we deﬁne and explore two relational properties
that characterize the robustness of cyber-physical systems
under sensor attacks. Our threat model assumes a powerful
attacker that may compromise a subset of sensors and arbi-
trarily manipulate those sensors’ values. We do not model or
discover the mechanisms by which an attacker manipulates
sensor values; we simply assume they are able to do so.

Our ﬁrst relational property is robustness of safety, which
intuitively holds when the compromised sensors are unable
to affect whether a given safety property holds in the attacked
system. Note that this is not the same as requiring that the
attacked system satisﬁes the safety property. Indeed it may be
beyond current veriﬁcation techniques to determine whether
the safety property holds in the uncompromised system, let
alone the compromised system. Nonetheless, even in such
cases it can be possible to verify that compromised sensors do
not affect the safety property. Robustness of safety implies that
if the uncompromised system satisﬁes the safety property then
the compromised system will too. Reasoning about robustness
of safety separates reasoning about the implications of sensor
attacks from reasoning directly about functional properties.

Our second relational property is robustness of high-integrity
state, which requires that high-integrity parts of a system
cannot be inﬂuenced by the attacker. For example, returning
to our autonomous vehicle example, parts of the system
pertaining to steering and braking should be regarded as high
integrity and independent from low-integrity sensors such as
the interior thermometer. Robustness of high-integrity state is
similar to noninterference [19], [20], which requires that low-
integrity inputs can not inﬂuence high-integrity outputs.

We work within the formalism of hybrid programs [10],
[21], [22] and their implementation in the theorem prover
KeYmaera X [23]. Hybrid programs model cyber-physical
systems as hybrid-time dynamical systems, with the discrete
time component of the system modeling software components
and the continuous time component of the system modeling
physical phenomenon.

large programs to reasoning about H-equivalence of their
subprograms. The second technique reduces reasoning about
H-equivalence of two programs A and B to reasoning about
safety properties of a single program that represents both A
and B. This reduction allows us to prove relational properties
using KeYmaera X, an existing theorem prover for hybrid pro-
grams that does not directly support relational reasoning. This
technique is inspired by the self-composition technique [24]
used to prove noninterference in imperative and deterministic
programs. A key challenge we faced in adapting the self-
composition technique for hybrid programs is reasoning about
nondeterminism and physical dynamics, and in particular,
ensuring that certain nondeterministic choices are resolved the
same in both executions.

The main contributions of this paper are the following:

1. We introduce a threat model of sensor attacks in the
context of hybrid programs that model cyber-physical
systems. We show that these sensor attacks can be for-
malized in terms of syntactic manipulations of hybrid
programs. We introduce robustness of safety and robust-
ness of high-integrity state, two relational properties that
express security guarantees in the presence of sensor
attacks. (Section III)

2. We introduce H-equivalence, an equivalence relation over
hybrid programs, and express our relational properties in
terms of H-equivalence. (Section IV)
for

reasoning about

two techniques

present

3. We

H-equivalence and prove their soundness. (Section V)
4. We validate the approach developed throughout the paper
through three case studies of non-trivial cyber-physical
systems: an anti-lock braking system, the Maneuvering
Characteristics Augmentation System (MCAS) of the
Boeing 737-MAX, and an autonomous vehicle with a
shared communication bus. (Section VI)

We introduce some background about hybrid programs in
Section II. Section VII discusses related work.

This is an extended version of the paper with the same title
that appeared in the 2021 Computer Security Foundations
Symposium. The main addition of this paper is the proof of
Theorem 3 in Appendix D.

II. BACKGROUND

To deﬁne our two relational properties, we introduce the
H-equivalence relation over hybrid programs, where H is a set
of variables. Intuitively, two hybrid programs are H-equivalent
if they agree on the values of all variables in H at appropriate
times. In particular, we deﬁne our two relational properties as
H-equivalence between the original system and the compro-
mised system (for suitable sets of variables H).

We introduce two sound and tractable techniques to reason
about H-equivalence (and thus to prove that robustness of
safety and robustness of high-integrity state hold). The ﬁrst
technique decomposes reasoning about H-equivalence of two

Hybrid programs [22] are a formalism for modeling cyber-
physical systems, i.e., systems that have both continuous and
discrete dynamic behaviors. Hybrid programs can express
continuous evolution (as differential equations) as well as
discrete transitions.

Figure 1 gives the syntax for hybrid programs. Variables are
real-valued and can be deterministically assigned (x := θ,
where θ is a real-valued arithmetic term) or nondeterministi-
cally assigned (x := ∗). Hybrid program x′ = θ&φ expresses
the continuous evolution of variables: given the current value
of variable x, the system follows the differential equation

Real-valued terms θ
x
c
θ1 ⊕ θ2
Hybrid Program α, β, P
x := θ

Real-valued program variable
Constant
Computation on terms ⊕ ∈ {+, ×}

x := ∗
x′ = θ&φ

?φ
α; β
α ∪ β
α∗

¬φ
φ ∧ ψ
φ ∨ ψ
φ → ψ
∀x. φ
∃x. φ
[α]φ

Deterministic assignment of real arithmetic term
θ to variable x
Nondeterministic assignment to variable x
Continuous evolution along the differential equa-
tion system x′ = θ for an arbitrary real duration
within the region described by formula φ
Test if formula φ is true at the current state
Sequential composition of α and β
Nondeterministic choice between α and β
Nondeterministic repetition, repeating α zero or
more times

Comparison between real arithmetic terms (∼∈
{<, ≤, =, >, ≥})
Negation
Conjunction
Disjunction
Implication
Universal quantiﬁcation
Existential quantiﬁcation
Program necessity (true if φ is true after each
possible execution of hybrid program α)

Differential Dynamic Logic φ, ψ
θ1 ∼ θ2

Fig. 1: Syntax of hybrid programs and dL

x′ = θ for some (nondeterministically chosen) amount of time
so long as the formula φ, the evolution domain constraint,
holds for all of that time. Note that x can be a vector of
variables and then θ is a vector of terms of the same dimension.

Hybrid programs also include the operations of Kleene alge-
bra with tests [25]: sequential composition, nondeterministic
choice, nondeterministic repetition, and testing whether a
formula holds. Hybrid programs are models of systems and
typically over-approximate the possible behaviors of a system.

Differential dynamic logic (dL) [21], [22], [26] is the dynamic
logic [27] of hybrid programs. Figure 1 also gives the syntax
for dL formulas. In addition to the standard logical connectives
of ﬁrst-order logic, dL includes primitive propositions that
allow comparisons of real-valued terms (which may include
derivatives) and program necessity [α]φ, which holds in a state
if and only if after any possible execution of hybrid program
α, formula φ holds.

The semantics of dL [21], [26] is a Kripke semantics in which
the Kripke model’s worlds are the states of the system. Let
R denote the set of real numbers and V denote the set of
variables. A state is a map ω : V 7→ R assigning a real value
ω(x) to each variable x ∈ V. The set of all states is denoted
by STA. The semantics of hybrid programs and dL are shown
in Figure 2. We write ω |= φ if formula φ is true at state
ω. The real value of term θ at state ω is denoted ωJθK. The
semantics of a hybrid program P is expressed as a transition
relation JP K between states. If (ω, ν) ∈ JP K then there is an

Term semantics

ωJxK = ω(x)
ωJcK = c

ωJθ1 ⊕ θ2K = ωJθ1K ⊕ ωJθ2K for ⊕ ∈ {+, ×}

Program semantics

Jx := θK = {(ω, ν) | ν(x) = ωJθK and for all other

variables z 6= x, ν(z) = ω(z)}

Jx := ∗K = {(ω, ν) | ν(z) = ω(z) for all variables z 6= x}

J?φK = {(ω, ω) | ω |= φ}

= θ &φK = {(ω, ν) | iff exists solution ϕ : [0, r] 7→ STA of

x′ = θ with ϕ(0) = ω and
ϕ(r) = ν, and ϕ(t) |= φ for all t ∈ [0, r]}

Jx′

Jα ∪ βK = JαK ∪ JβK

Jα; βK = {(ω, ν) | ∃µ, (ω, µ) ∈ JαK and (µ, ν) ∈ JβK}
Jα∗
Formula semantics
ω |= θ1 ∼ θ2 iff ωJθ1K ∼ ωJθ2K for ∼∈ {=, ≤, <, ≥, >}

∗ the transitive, reﬂexive closure of JαK

K = JαK

ω |= φ ∧ ψ iff ω |= φ ∧ ω |= ψ, similar for {¬, ∨, →, ↔}
ω |= ∀x.φ iff ν |= φ for all states ν that agree with ω

except for the value of x

ω |= ∃x.φ iff ν |= φ for some state ν that agrees with ω

ω |= [α]φ iff ν |= φ for all state ν with (ω, ν) ∈ JαK

except for the value of x

Fig. 2: Semantics of hybrid programs and dL

execution of P that starts in state ω and ends in state ν.

We are often interested in partial correctness formulas of the
form φ → [α]ψ: if φ is true then ψ holds after any possible
execution of α. The hybrid program α often has the form
(ctrl;plant)∗, where ctrl models atomic actions of the
control system and does not contain continous parts (i.e.,
differential equations); and plant models evolution of the
physical environment and has the form of x′ = θ &φ. That is,
the system is modeled as unbounded repetitions of a controller
action followed by an update to the physical environment.

Consider, as an example, an autonomous vehicle that needs
to stop before hitting an obstacle. For simplicity, we model
the vehicle in just one dimension. Figure 3 shows a HP
model (hybrid program model) of such an autonomous vehicle.
Let d be the vehicle’s distance from the obstacle. The safety
condition that we would like to enforce (φpost) is that d is
positive. Let v be the vehicle’s velocity towards the obstacle in
meters per second (m/s) and let a be the vehicle’s acceleration
(m/s2). Let t be the time elapsed since the controller was
last invoked. The hybrid program plant describes how the
physical environment evolves over time interval ǫ: distance
changes according to −v (i.e., d′ = −v), velocity changes
according to the acceleration (i.e., v′
= a), and time passes at
a constant rate (i.e., t′
= 1). The differential equations evolve
only within the time interval t ≤ ǫ and if v is non-negative
(i.e., v ≥ 0).

The hybrid program ctrl models the vehicle’s controller.

Platzer introduces this autonomous vehicle example [22].
Syntax of hybrid programs used in this paper is similar to the syntax used

in KeYmaera X, but revised for better presentation.

/* cannot change over time */

/* time limit for control */
/* acceleration rate */
/* braking rate */
≡ A ≥ 0 ∧ B ≥ 0 ∧ 2Bd > v2
≡ d > 0
≡ 2Bd > v2 + (A + B)(Aǫ2 + 2vǫ)
≡ ?ψ; a := A
≡ a := −B
≡ ((accel ∪ brake); t := 0)
≡ d′ = −v, v′ = a, t′ = 1 & (v ≥ 0 ∧ t ≤ ǫ)

1 Definitions.
2 R ǫ.
3 R A.
4 R B.
5 B φpre
6 B φpost
7 B ψ
8 HP accel
9 HP brake
10 HP ctrl
11 HP plant
12 ProgramVariables.
13 R t.
14 R d.
15 R v.
16 R a.
17 Problem.
18 φpre → [(ctrl; plant)∗]φpost

/* clock variable */
/* distance to obstacle */
/* vehicle velocity */
/* acceleration of the vehicle */

/* may change over time */

/* dL formula to be proven */

Fig. 3: HP model of an autonomous vehicle

The vehicle can either accelerate at A m/s2 or brake at −B
m/s2. For the purposes of the model, the controller chooses
nondeterministically between these options. Hybrid programs
accel and brake express the controller accelerating or brak-
ing (i.e., setting a to A or −B respectively). The controller
can accelerate only if condition ψ is true, which captures that
the vehicle can accelerate for the next ǫ seconds only if doing
so would still allow it to brake in time to avoid the obstacle.

The formula to be veriﬁed is presented on the last line of
the HP model. Given an appropriate precondition φpre, the
axioms and proof rules dL can be used to prove that the
safety condition φpost holds. The tactic-based theorem prover
KeYmaera X [23] provides tool support for automating the
construction of these proofs.

To present some of our deﬁnitions, we need to refer to the
variables that occur in a hybrid program [22], [26]. The
free variables of hybrid program P , denoted FV(P ), is the
variables that may potentially be read by P . Values of FV(P )
won’t be modiﬁed during executions of program P . The bound
variables of program P , denoted BV(P ), is the set of variables
that may potentially be written to by P . We write VAR(P ) for
the set of all variables of P , and have VAR(P ) = BV(P ) ∪
FV(P ). For example, let P be the hybrid program modeling
an autonomous vehicle with sensors shown in Figure 3, then
FV(P ) = {A, B, ǫ, v, d}, BV(P ) = {t, v, d, a, t′, v′, d′}, and
VAR(P ) = {A, B, ǫ, t, v, d, a, t′, v′, d′}. Formal deﬁnitions of
BV(P ), FV(P ), and VAR(P ) are included in Appendix A.

s + (A + B)(Aǫ2 + 2vsǫ)

1 Definitions.
/* time limit for control */
2 R ǫ.
/* acceleration rate */
3 R A.
/* braking rate */
4 R B.
≡ A ≥ 0 ∧ B ≥ 0 ∧ 2Bdp > v2
5 B φpre
p
6 B φpost
≡ dp > 0
≡ 2Bds > v2
7 B ψ
8 HP accel
≡ ?ψ; a := A
9 HP brake
≡ a := −B
≡ vs := vp; ds := dp; (accel ∪ brake); t := 0
10 HP ctrl
≡ d′
11 HP plant
12 ProgramVariables.
13 R t.
14 R dp.
15 R vp.
16 R ds.
17 R vs.
18 R a.
19 Problem.
20 φpre → [(ctrl; plant)∗]φpost

/* clock variable */
/* distance to obstacle (physical) */
/* vehicle velocity (physical) */
/* distance to obstacle (sensed) */
/* vehicle velocity (sensed) */
/* acceleration of the vehicle */

p = a, t′ = 1 & (vp ≥ 0 ∧ t ≤ ǫ)

p = −vp, v′

Fig. 4: HP model of an autonomous vehicle with sensors

contains a single continuous variable v that represents the
value measured by a sensor; the model does not separate
the model’s representation of the value of v in the physical
model from the software component’s representation of v.
Therefore, our analysis begins with a hybrid program Porig in
which sensor reads are not explicitly modeled. We construct a
program P that is equivalent to Porig but separately represents
sensor reads and requires that variables holding sensor reads
are equal to the underlying sensor’s value. For example, vp
may represent the actual physical velocity of a vehicle and it
changes according to laws of physics, and vs may represent
the variable in the controller into which the sensor’s value is
read. In model P we have the constraint vs = vp. From P
we can derive additional models that allow sensed values to
differ from actual physical values. For example, a model that
represents the compromise of the velocity sensor would be
identical to P except that the constraint vs = vp is removed,
allowing vs to take arbitrary values. Similar modiﬁcations to P
can represent the compromise of other sensors, or of multiple
sensors at the same time.

As an example, Figure 4 shows a HP model of an autonomous
vehicle introduced in Figure 3 whose hybrid program separates
physical and sensed values: vp and dp are physical values of
velocity and distance, while vs and ds are the corresponding
sensed values. Note that the ctrl program sets the sensed
values equal to the physical values (line 10).

III. MODELING SENSOR ATTACKS

B. Threat Model

In this section, we explain how we model the sensor attacks
in hybrid programs. In particular, we introduce how sensor
readings are modeled and describe our threat model.

A. Modeling Sensor Readings

Hybrid programs typically conﬂate the values of variables in
the physical model and the values ultimately perceived by
the sensor. For example, in Figure 3, the hybrid program

We follow the naming convention of related work on hybrid programs by

using the names of free variables and bound variables [26].

We allow attackers to arbitrarily change sensed values. We are
not concerned with the physical mechanisms by which an at-
tacker compromises a sensor. Instead, we model sensor attacks
as assignments to variables that represent sensed values. Let P
be a hybrid program, SA ⊆ BV(P ) be a set of distinguished
variables corresponding to sensors that may be vulnerable to
attacks, the sensor attack on P is deﬁned as follows:

Deﬁnition 1 (SA-sensor attack). For a hybrid program P
the form (ctrl;plant)∗ and a set of variables SA
of
the SA-sensor attack on program P , denoted
⊆ BV(P ),

ATTACKED(P, SA), is the program obtained from P by re-
placing all assignments to variable v ∈ SA with assignment
v := ∗.
For example, let P be the hybrid program (ctrl;plant)∗
modeling an autonomous vehicle with separate physical and
sensed values shown in Figure 4. If the velocity sensor
vs is under attack, program ATTACKED(P, {vs}) would be
(ctrl′;plant)∗ where ctrl′ is the following:

vs := ∗; ds := dp; (accel ∪ brake); t := 0.
Note that with such a threat model, only the ctrl part of
a program (ctrl;plant)∗ is modiﬁed by an attack, i.e.,
ATTACKED((ctrl; plant), SA) = (ATTACKED(ctrl, SA));
plant. Intuitively, it means a sensor attack does not directly
affect the physical dynamics with which the system interacts.

C. Robustness to Sensor Attacks

We explore the impact of an SA-sensor attack by studying
two relational properties that characterize the robustness of the
system to the attack: (1) whether a SA-sensor attack affects
the safety of the system and (2) whether a SA-sensor attack
affects the system’s high-integrity state.

a) Robust Safety: Safety is critical in many cyber-physical
systems, e.g., a vehicle should not collide with obstacles and
pedestrians. We ﬁrst present the deﬁnitions of safety and our
relational property robust safety, and then show an example.

Deﬁnition 2 (Safety). A hybrid program P of
the form
(ctrl;plant)∗ is safe for φpost assuming φpre, denoted
SAFE(P, φpre, φpost), if the formula φpre → [P ]φpost holds.

This deﬁnition says P is safe if for any execution of P whose
starting state satisﬁes φpre, its ending state satisﬁes safety
condition φpost.

A system is robustly safe if compromise of sensors SA does
not affect whether the system is safe. Note that robust safety
does not require that the attacked system is safe; instead it
requires that if the original system is safe, then the attacked
system is also safe. The distinction is important: it allows us
to separate the task of reasoning about safety from the task
of reasoning about sensor attacks. Indeed, as we will see in a
case study in Section VI, it is possible to prove robust safety
even when it is beyond current techniques to prove safety.

the

(Robust
form (ctrl;plant)∗

safety). For a hybrid program
Deﬁnition 3
of
and
of
P
variables SA ⊆ BV(P ), P is robustly safe for φpost
denoted
assuming φpre
ROBUST(P, φpre, φpost, SA), if SAFE(P, φpre, φpost) implies
SAFE(ATTACKED(P, SA), φpre, φpost).

the SA-sensor

attack,

under

set

a

let P be the hybrid program modeling an
For example,
autonomous vehicle with sensors shown in Figure 4. P is
safe for φpost assuming φpre (i.e., SAFE(P, φpre, φpost)).
However, P is not
assuming
φpre under SA-sensor attack where SA is {vs}, since
SAFE(ATTACKED(P, SA), φpre, φpost) doesn’t hold.

robustly safe for φpost

4

5
6 HP ctrl
7 ...

1 ...
2 R T .
3 HP ctrlt
4

5

6
7 HP ctrl

8
9 HP plant

1 ...
2 HP voting ≡ vs1 := vp; vs2 := vp; vs3 := vp;
3

(

(?vs1 = vs2 ; vs:= vs1 )
∪ (?vs1 = vs3 ; vs:= vs1 )
∪ (?vs2 = vs3 ; vs:= vs2 ) )

≡ voting; ds := dp; (accel ∪ brake); t := 0

Fig. 5: HP model of an autonomous vehicle with sensor voting

/* target temperature */
≡ temps:= tempp;

(

(?temps > T ; thermo:= -1)

∪ (?temps < T ; thermo:= 1)
∪ (?temps = T ) )

≡ ctrlt;

≡ d′

vs:= vp; ds:= dp; (accel ∪ brake); t := 0
p = −vp, v′
& (vp ≥ 0 ∧ t ≤ ǫ)

p = thermo; t′ = 1

p = a, temp′

10
11 ProgramVariables.
12 R temps.
13 R tempp.
14 R thermo. /* thermostat command */
15 ...

/* interior temperature (sensed) */
/* interior temperature (physical) */

Fig. 6: HP model of an autonomous vehicle with interior temperature
control

The system can be modiﬁed so that it does satisfy robust safety.
For example, we can modify the system to use three velocity
sensors (perhaps measuring velocity by different mechanisms)
and use a voting scheme to determine the current velocity.
Figure 5 shows a model of such a modiﬁed system. The
physical velocity vp is sensed by three sensors (line 2), and
voting performed to determine the ﬁnal reading vs (lines 3–5).
The contents elided in Figure 5 are the same as Figure 4.

Let P be the hybrid program modeling an autonomous vehicle
with duplicated sensors shown in Figure 5. For any set SA ∈
{{vs1}, {vs2}, {vs3}}, program P is robustly safe under SA-
sensor attack, i.e., P is robustly safe if at most one of the
velocity sensors is compromised. Intuitively, this is because
vs = vp holds after running program voting, even if up to one
of the velocity sensors is compromised. A systematic approach
for proving robustness safety is presented in Section V.

b) Robustness of High-Integrity State: Sensors that may be
compromised are low integrity: the sensed values might be
under the control of the attacker. By contrast, parts of the
system state might be deemed to be high integrity: their values
are critical to the correct and secure operation of the system.
Low-integrity sensor readings should not be able to affect a
system’s high-integrity state. For example, an attacker with
access to a car’s interior temperature sensor should not be
able to affect the control of the car’s velocity.

We can state this requirement as a relational property: we say
the high-integrity state is robust if, for any execution of the
system with its low-integrity sensors compromised, there is an
execution of the non-compromised system that can achieve the
same values on all high-integrity variables. We delay formal
deﬁnition of robustness of high-integrity state to Section IV.

Let’s consider an example. Figure 6 presents a HP model

of an autonomous vehicle with sensors shown in Figure 4
but added with interior temperature control (elided contents
in Figure 6 are the same as Figure 4). The vehicle has
sensor readings of interior temperature (temps). The physical
temperature (tempp) changes according to thermo that is set
by ctrlt after comparing temps with target temperature T
(lines 4–6). In this example, the temperature sensor is low-
integrity and may be compromised.

A system designer may want to understand if such an attack
can interfere with the vehicle’s high-integrity state such as its
velocity. Let P be the model of an autonomous vehicle with
interior temperature control shown in Figure 6. Intuitively, its
velocity (i.e., variable vp) is robust with respect to sensor
temps: for any execution of ATTACKED(P , {temps}), we have
an execution of P that can produce the same values of vp at
every control iteration. The system does satisfy robustness of
high-integrity state, and we will prove it in Section V.

IV. H-EQUIVALENCE
This section introduces H-equivalence, a notion of equivalence
that allows us to reason about our relational properties.

A. Equivalence of Hybrid Programs

Intuitively, H-equivalence of two programs means that for
every execution of one program, there exists an execution of
the other program such that the two executions agree on set H
initially and at the end of every control loop iteration, where
H is a set of high-integrity variables.

The formal deﬁnition of H-equivalence of programs builds on
H-equivalence of program states.

Deﬁnition 4 (H-equivalence of program states). For states
ω1, ω2 ∈ STA and a set of variables H, states ω1 and ω2 are
H-equivalent, denoted ω1 ≈H ω2, if they agree on valuations
of all variables in the set H; i.e., ∀x ∈ H, ω1(x) = ω2(x).

Deﬁnition 5 (H-equivalence of programs). For hybrid pro-
grams P1 = α∗, P2 = β∗, and a set of variables H, P1 and
P2 are H-equivalent, denoted P1 ≈H P2, if they satisfy the
following:

∀n : N
∀ω0, ω1 . . . ωn : STA such that ∀i ∈ 0...(n − 1),
(ωi, ωi+1) ∈ JαK (respectively JβK)
∃ν0, ν1...νn : STA such that ∀j ∈ 0...(n − 1),
(νj, νj+1) ∈ JβK (respectively JαK)
and ∀k ∈ 0...n, ωk ≈H νk

In the deﬁnition, the number n corresponds to an arbitrary
number of loop iterations, and the last line indicates that the
two executions agree on H at the beginning and end of every
loop iteration. The deﬁnition is symmetric.

This deﬁnition can be readily adjusted for loop-free programs.

Deﬁnition 6 (H-equivalence of loop-free programs). For two
loop-free hybrid programs α and β, and a set of variables H,

α and β are H-equivalent, denoted α ≈H β, if they satisfy
the following:
∀ω0, ω1 : STA such that (ω0, ω1) ∈ JαK (respectively JβK)
∃ν0, ν1 : STA such that
(ν0, ν1) ∈ JβK (respectively JαK) ∧ ω0 ≈H ν0 ∧ ω1 ≈H ν1

Note that Deﬁnition 5 is deﬁned in lock-step, i.e., both loops
iterate exactly the same number of times [29]. As pointed
out by previous work [30], a lock-step approach is sometimes
not ﬂexible enough to express and verify some properties,
e.g., properties that may hold for two programs that execute
for different numbers of iterations. However, such a lock-
step deﬁnition is reasonable in our setting. According to the
threat model, we are comparing a system with compromised
sensors and a system with uncompromised sensors and so the
attack should not affect the rate of a system’s control (i.e.,
how frequently the system’s control loop executes). Thus,
the robustness of a system is correctly encoded by a lock-
step deﬁnition, in which states of a system with and without
compromised sensors are consistent after every loop iteration.
An additional beneﬁt of this deﬁnition is that
is more
tractable for veriﬁcation, which we will explore in Section V.

it

B. Reasoning about Robustness using H-equivalence
The H-equivalence relation can be used to reason about our
two relational properties.

a) Reasoning about Robustness of Safety: Robustness of
safety can be established by proving H-equivalence with the
help of the following theorem, which states that if program
P is H-equivalent to ATTACKED(P, SA) where H is the free
variables of formulas φpre and φpost, then P is robustly safe
for φpost assuming φpre under the SA-sensor attack.

Theorem 1 (H-equivalent programs are robustly safe). For
a hybrid program P of the form (ctrl;plant)∗, a set of
variables SA ⊆ BV(P ), and formulas φpre and φpost, if
P ≈FV(φpre∧φpost) ATTACKED(P, SA), then
ROBUST(P, φpre, φpost, SA)

A proof is in Appendix B. Intuitively, the theorem holds
because if there were an execution of attacked program such
that φpre held at the beginning but φpost did not hold at the
end of a loop, then there must be an execution of P where
the same is true, contradicting the assumption that P is safe.

it

Note that the converse of Theorem 1 does not hold, i.e.,
if ROBUST(P, φpre, φpost, SA),
is not always true that
P ≈FV(φpre∧φpost) ATTACKED(P, SA). For example, let P be
the program (b := 1; a := b)∗, formula φpre be a > 0, φpost
be b > 0, and SA be {a}. Then ROBUST(P, φpre, φpost, SA)
holds, but P ≈{a,b} ATTACKED(P, SA) does not hold since
some executions of ATTACKED(P, SA) (i.e., (b := 1; a := ∗)∗)
do not have a matching execution of P .

Theorem 1 reduces proving robustness of safety to proving
H-equivalence, which can be achieved by the techniques
introduced in Section V.

b) Reasoning about Robustness of High-Integrity State:
H-equivalence directly expresses robustness of high-integrity
state by letting H be the set of high-integrity variables.
Therefore, proving robustness of high-integrity state is the
same as proving H-equivalence of the high-integrity state. The
following deﬁnition makes this clear.

Deﬁnition 7 (Robustness of high-integrity state). For program
P of the form (ctrl;plant)∗ and a set of variables SA
⊆ BV(P ), and set of variables H, P satisﬁes robustness of
high-integrity state H under the SA-sensor attack if P ≈H
ATTACKED(P, SA).

V. PROVING H-EQUIVALENCE

We present
H-equivalence.

two sound techniques for

reasoning about

A. Decomposition Approach

Our ﬁrst approach proves H-equivalence of programs by
decomposing the proof obligation into simpler obligations for
components of the programs. This relies on various composi-
tional properties of H-equivalence, stated here and proven in
Appendix B.

Theorem 2. For all loop-free hybrid programs A, B, C, D
and sets H and H′ of variables, the following properties hold:

1. A ≈H A;
2. If H ⊆ H′ and A ≈H′ B, then A ≈H B;
3. If A ≈H B and (VAR(A) ∪ VAR(B)) ∩ H′ = ∅, then

A ≈H∪H′ B;

4. If FV(C) ∪ FV(D) ⊆ H, A ≈H B, and C ≈H D, then

(A; C) ≈H (B; D);

5. If FV(A) ∪ FV(B) ⊆ H and A ≈H B, then A∗ ≈H B∗.

Sequential composition (Property 4) is particularly useful. The
condition FV(C) ∪ FV(D) ⊆ H ensures that H includes
all variables that might affect the evaluation of programs C
and D. We use this property when considering H-equivalence
of ctrl; plant and ATTACKED(ctrl; plant, SA) =
ATTACKED(ctrl, SA); plant. In particular, if H includes
the actuators by which the controller interacts with the physical
environment, then ctrl ≈H ATTACKED(ctrl, SA) ensures
that the physical dynamics (i.e., program plant) can evolve
identically in both the attacked and unattacked systems.

Consider the previously presented model of an autonomous
vehicle with three velocity sensors shown in Figure 5, and
let P be its hybrid program (ctrl; plant)∗ and α be pro-
gram P with voting excluded, i.e., P = (voting; α)∗ and
ATTACKED(P, {vs1 }) = (ATTACKED(voting, {vs1 }); α)∗.
Here, FV(voting) = FV(ATTACKED(voting, {vs1 })) =
{vp}, FV(α) = {vs, vp, dp, A, B, ǫ}, and FV(voting; α) =
FV(ATTACKED(voting, {vs1 }); α) = {vp, dp, A, B, ǫ}.

deﬁnition of ≈H, we

By
ATTACKED(voting, {vs1}). By Property 3, we get

know voting ≈{vs,vp}

voting ≈FV(α) ATTACKED(voting, {vs1 })

Then by Property 2,
(voting; α) ≈FV(voting;α) (ATTACKED(voting, {vs1 }); α)
Since α ≈FV(voting;α) α (Property 1), by Property 4 we know,
voting; α ≈FV(voting;α) ATTACKED(voting, {vs1 }); α

By Property 5, we get
(voting; α)∗ ≈FV(voting;α) (ATTACKED(voting, {vs1}); α)∗
The free variables of φpre ∧ φpost
(shown in Figure 4)
are {vp, dp, A, B, ǫ}, the same as FV(voting; α). Thus, by
Theorem 1, we have ROBUST(P, φpre, φpost, {vs1}).

B. Self-Composition Approach

The second approach toward proving H-equivalence is in-
spired by self-composition [24], [31], a proof technique often
used for proving noninterference [19], [20]. Noninterference
is a well-known strong information security property that,
intuitively, guarantees that conﬁdential inputs do not inﬂuence
observable outputs, or dually guarantees that
low-integrity
inputs of a system do not affect high-integrity outputs. Nonin-
terference is a relational property: it compares two executions
of a program with different low-integrity inputs.

To develop an intuition for how the self-composition tech-
nique is used to prove noninterference, consider the problem
of checking whether low-integrity inputs of a deterministic
program affect high-integrity outputs. Construct two copies
of the program, renaming the program variables so that the
variables in the two copies are disjoint. Set the high-integrity
inputs in both copies to identical values but allow the low-
integrity inputs to take different values. Now, sequentially
compose these two programs together. If the composed pro-
gram can terminate in a state where the corresponding high-
integrity outputs differ, then the original program does not
satisfy noninterference; conversely, if in all executions of the
composed program, the high-integrity outputs are the same,
then the original program satisﬁes noninterference. Intuitively,
the composition of the two copies allows a single program to
represent two executions of the original program, reducing
checking a relational property of the original problem to
checking a safety property of the composed program.

Using the same insights, we develop a self-composition
technique for hybrid programs, allowing us to use existing
veriﬁcation tools such as KeYmaera X (which can reason
about safety properties of hybrid programs) to reason about
H-equivalence of two hybrid programs.

It is non-trivial to adapt the self-composition approach to hy-
brid programs due to the nondeterminism in hybrid programs.
In particular, to show that two executions of the same hybrid
program are in an appropriate relation, it may be necessary
to force (some of) the nondeterminism in the two executions
to resolve in the same way. For example, a nondeterministic
choice in a hybrid program may represent a decision by a
driver to brake or accelerate; the driver’s decision is assumed
to be a high-integrity input, and so the resolution of the
nondeterministic choice should be the same in both executions.

The self-composition must somehow couple the nondeter-
ministic choices to ensure this. Nondeterministic assignment
must be similarly handled, i.e., resolution of high-integrity
nondeterminism must be coupled in the two executions.

An additional source of nondeterminism in hybrid programs
is the duration of physical evolution. The program construct
for physical dynamics, x′ = θ&φ, speciﬁes that the variable(s)
evolve according to the differential equation system x′ = θ for
an arbitrary duration within the region described by formula
φ. The duration is chosen nondeterministically.

Our self-composition technique takes as input a program P
and set of sensor variables SA and creates a program that rep-
resents an execution of each of P and ATTACKED(P, SA). We
ensure that the composed program (1) resolves high-integrity
nondeterministic choices and assignments the same in both
executions; and (2) has the same duration for corresponding
physical evolutions.

To ensure that the two executions are appropriately related,
we produce a formula that encodes that the two executions
have the same values for high-integrity variables; we assume
this formula holds at the beginning of the executions, and
require the formula to hold at the end of every control iteration.
If we can prove that this is the case, then we have proved
that if the two executions (1) have the same values for high-
integrity inputs at the beginning of their executions, (2) follow
the same decisions on high-integrity nondeterminism during
their executions, and (3) evolve for the same duration, then
the two executions have the same values for high-integrity
variables at the end of every control iteration.

Our self-composition approach has some limitations on the
hybrid programs to which it applies. First, it is applicable
only for hybrid programs of the form (ctrl; plant)∗.
Second, it is applicable only for hybrid programs that have
total semantics for low-integrity inputs. Intuitively, it means
if a program has a valid execution for an input state ω (i.e.,
exists a state ν such that (ω, ν) ∈ JP K), then the program
has a valid execution for every input state that differs with ω
only on low-integrity inputs. The reason for this requirement
is that self-composition uses a single program to represent
two executions; this composed program has a valid execution
only if both executions are valid. Since the two executions
differ only on low-integrity inputs, our technique works only
if semantics of the unattacked program is total on low-integrity
inputs. A straightforward syntactic checker can be developed
to check whether a hybrid program meets this requirement.
More discussion about the limitation and the syntactic checker
can be found in Appendix C.

The rest of this section describes in detail our self-composition
approach: how to construct a single program that represents
an execution of P and ATTACKED(P, SA), and then prove it
correct. At a high level, our approach works by (1) converting
program P to a canonical form Pcanon that makes high-
integrity nondeterministic choices and assignments explicit;

1 ...
2 HP choices ≡ c:= ∗
3 HP ctrl

≡ choices; ctrlt; vs:= vp; ds:= dp;

(if (c) then accel else brake); t := 0

4
5 ...
6 ProgramVariables.
7 B c.
8 ...

/* choice variable */

Fig. 7: HP model of an autonomous vehicle with interior temperature
control shown in Figure 6 whose hybrid program is rewritten to
canonical form with a choice variable c

and then (2) composing Pcanon and ATTACKED(Pcanon , SA)
to ensure that the values of high-integrity nondeterministic
choices and assignments, and evolution durations are the same
for both executions.

a) Canonical Form for Hybrid Programs: Given a hybrid
program of the form (ctrl; plant)∗, we rewrite it to a canonical
form (choices; ctrl′; plant)∗ such that (1) each high-integrity
nondeterministic choice α∪β in ctrl is turned into a construct
if c then α else β in ctrl′, and (2) each high-integrity non-
deterministic assignment x := ∗ in ctrl is turned into x := c
in ctrl′, where c is a fresh variable, and choices contains
a nondeterministic assignment c := ∗. The program fragment
choices consists solely of a sequence of these nondeterministic
assignments to these choice variables. Note that (ctrl; plant)∗
is semantically equivalent to (choices; ctrl′; plant)∗.

The goal of the canonical form is to make it easier to share
the same nondeterministic choices and assignments between
the two executions: when we compose the two programs, they
will essentially share the same choices program.

For example, Figure 7 shows the previously presented model
of an autonomous vehicle with interior temperature control
shown in Figure 6 whose hybrid program is rewritten to the
canonical form (elided contents in Figure 7 are the same as
Figure 6). The program has a nondeterministic choice variable
c that represents a decision to brake or accelerate. This choice
is considered high-integrity.

b) Hybrid Program with Renaming: Note that program P and
ATTACKED(P, SA) have the same set of variables. To compare
executions of P and ATTACKED(P, SA) in a composition, we
need to rename bound variables in one of the two programs.
Renaming is needed only for bound variables, since their
values may differ during executions. Other variables are read-
only and their values will be the same for executions of
program P and ATTACKED(P, SA). Thus, these variables can
be shared by both programs, and renaming is not needed.

To help us with renaming, we deﬁne renaming functions that
map all and only the bound variables of a program to fresh
variables.

Deﬁnition 8 (Renaming function). For hybrid program P ,
function ξ : VAR(P ) → V (where V is a set of variables) is
a renaming function for P if:

Construct if φ then α else β is syntactic sugar for (?φ;α)∪(?¬φ;β).

1) ξ is a bijection;
2) For all x ∈ BV(P ), ξ(x) 6∈ VAR(P );
3) For all x ∈ VAR(P ) \ BV(P ), ξ(x) = x.

We write ξ(P ) for the program identical to P but whose
variables have been renamed according to function ξ. We
also apply renaming functions to states and formulas, with
the obvious meaning.

c) Interleaved Composition: We develop an interleaved com-
position that composes two programs so their executions have
the same values for high-integrity nondeterministic choices
and assignments, and last the same evolution duration.

Deﬁnition 9 (Interleaved composition). Given a hybrid pro-
gram P = (choices; ctrl; (x′ = θ & φ))∗ in canonical form, a
renaming function ξ for P , a set of variables SA ⊆ BV(P ), the
interleaved composition of P under SA attack with renaming
function ξ, denoted IC(P, SA, ξ), is the following program:
(choices; ctrl; SUB(choices, ξ); ξ(ATTACKED(ctrl, SA));

(x′ = θ, ξ(x′ = θ) & φ ∧ ξ(φ)))∗

Where function SUB(choices, ξ) replaces ci := ∗ in program
choices with ξ(ci) := ci for all variables ci in BV(choices).

The composition has the following properties: (1) control
components from two programs are executed sequentially (i.e.,
choices; ctrl; SUB(choices, ξ); ξ(ATTACKED(ctrl, SA))); (2)
plants are executed in parallel (i.e., x′ = θ, ξ(x′ = θ)) [32]; (3)
the evolution constraint is a conjunction of the two evolution
constraints (i.e., φ ∧ ξ(φ)), and (4) nondeterministic choices
in choices used by ctrl and their counterparts used by
ξ(ATTACKED(ctrl, SA)) have the same values.

For example, let P be the previously presented hybrid pro-
gram (in canonical form) of an autonomous vehicle with
interior temperature control shown in Figure 7. Figure 8
shows IC(P, {temps}, ξ), where function ξ renames bound
variables in ATTACKED(P, SA) with subscript 1. Program
ctrl′ and Plant′ compose two programs as described in
Deﬁnition 9 (lines 27–30). Line 18 shows the effect of function
SUB(choices, ξ): substituting c = ∗ with c1 = c in choices.
The choice represents a decision to accelerate or brake, which
is high-integrity. The resolution of this choice should be the
same in both executions.

d) Proving H-equivalence with an Interleaved Composition:
Given an interleaved composition IC(P, SA, ξ), to prove that
two programs are H-equivalent on a set H, we need to
ﬁrst identify a set η of high-integrity variables on which the
evaluation of variables in H depend. Then we construct a
formula to express that the two program executions have the
same values for variables in set η, and ﬁnally prove that, for
any execution of the composition, if the formula holds initially,
it would hold at the end of every control loop iteration of the
execution.

Deﬁnition 10 (Equivalence formula). For a set η of variables,
a renaming function ξ such that η ⊆ dom(ξ), the equivalence

1 Definitions.
2 R ǫ.
3 R A.
4 R B.
5 R T .
6 B eqη
7 B ψ
8 HP choices ≡ c:= ∗
9 HP ctrlt
10

(

/* time limit of control */
/* acceleration rate */
/* braking rate */
/* target temperature */

≡ vp = vp1 ∧ dp = dp1
≡ 2Bds > v2

s + (A + B)(Aǫ2 + 2vsǫ)

≡ temps:= tempp;

(?temps > T ; thermo:= -1)

11

12
13 HP accel
14 HP brake
15 HP ctrl

∪ (?temps < T ; thermo:= 1)
∪ (?temps = T ) )

≡ ?ψ; a := A
≡ a := −B
≡ ctrlt; vs:= vp; ds:= dp;

21

22
23 HP accel1
24 HP brake1
25 HP ctrl1
26
27 HP ctrl′
28 HP plant′
29

(if (c) then accel else brake); t := 0

≡ 2Bds1 > v2
s1

+ (A + B)(Aǫ2 + 2vs1 ǫ)

16
17 B ψ1
18 HP choices1 ≡ c1:= c
19 HP ctrlt1
20

(

≡ temps1 := *;

(?temps1 > T ; thermo1:= -1)
∪ (?temps1 < T ; thermo1:= 1)
∪ (?temps1 = T ) )

≡ ?ψ1; a1 := A
≡ a1 := −B
≡ ctrlt1 ; vs1 := vp1 ; ds1 := dp1 ;

(if (c1) then accel1 else brake1); t1 := 0

≡ choices; ctrl; choices1; ctrl1
≡ d′
p = a, temp′
p = −vp, v′
d′
p1
& (vp ≥ 0 ∧ vp1 ≥ 0 ∧ t ≤ ǫ ∧ t1 ≤ ǫ)

= a1, temp′
p1

= −vp1 , v′
p1

p = thermo, t′ = 1

= thermo1, t′

1 = 1

30
31 ProgramVariables.
32 B c, c1.
33 R t, t1.
34 R dp, dp1 .
35 R ds, ds1 .
36 R vp, vp1 .
37 R vs, vs1 .
38 R a, a1.
39 R temps, temps1 .
40 R tempp, tempp1 .
41 R thermo, thermo1. /* rates of change for temperature */
42 Problem.
43 eqη → [(ctrl′; plant′)∗]eqη

/* choice variables */
/* clock variables */
/* distance to obstacle (physical) */
/* distance to obstacle (sensed) */
/* vehicle velocity (physical) */
/* vehicle velocity (sensed) */
/* acceleration of the vehicle */
/* interior temperature (sensed) */
/* interior temperature (physical) */

Fig. 8: Interleaved composition of the hybrid program (in canonical
form) modeling an autonomous vehicle with interior temperature
control shown in Figure 7

formula of η and ξ, denoted eqξ

η, is deﬁned as:
(x = ξ(x))

eqξ

η ≡ ^
x∈η

Then the desired property is, for any execution of the compo-
sition, if the equivalence formula holds at the beginning of an
execution, it holds at the end of every control loop iteration of
the execution. That means, we want to prove the following:

eqξ

η → [IC(P, SA, ξ)]eqξ
η

For example, eqη in Figure 8 (line 6) encodes that the two
executions have the same position (dp = dp1 ) and velocity
(vp = vp1 ). The desired property is shown at line 43.

We have proven this property using Keymaera X. Intuitively,
proving this property means that for any execution of the
autonomous vehicle model, whether or not its temperature
sensor is compromised, if the vehicle starts with the same
position and velocity, makes the same control decisions for

acceleration and brake, and runs for the same duration, it
would end with the same position and velocity.

e) Soundness: The soundness
the self-
composition approach with proving H-equivalence. Proof of
this theorem is based on trace semantics of hybrid pro-
grams [33], [34] and can be found in Appendix D.

theorem links

Theorem 3 (Soundness of the self-composition approach). For
hybrid program P and Pc, a set SA ⊆ BV(P ), a renaming
function ξ of Pc, a set of variables η ⊆ BV(P ), and a set H
⊆ η, if Pc is P in canonical form, SA ∩ η = ∅, and eqξ
η →
[IC(Pc, SA, ξ)]eqξ

η, then P ≈H ATTACKED(P, SA).

Note that the condition SA ∩ η = ∅ indicates that the adversary
cannot compromise high-integrity variables.

f) Applicability: Our self-composition technique applies to a
subset of problems of interest rather than general problems. In
particular, our technique requires that two executions having
the same duration at every control iteration for the plant,
and identical values for high-integrity nondeterministic assign-
ments. Our self-composition technique cannot be applied to
compare two executions that evolve for different durations or
that resolve high-integrity nondeterministic choices and high-
integrity nondeterministic assignments differently. However,
these restrictions arise naturally for many systems. First,
requiring the same duration of evolution for both executions
corresponds to the control system having the same frequency
of operation. That is, the rate of the the system’s control
can’t be inﬂuenced by the attacker. Second, high-integrity
non-deterministic choices and high-integrity non-deterministic
assignments are used to model exactly the nondeterminism
they
that cannot be inﬂuenced by the attacker. As such,
should be resolved the same in both executions. For example,
when considering how a corrupted temperature sensor can
affect a (non-autonomous) vehicle, the driver’s decisions (i.e.,
whether to accelerate or brake) would be modeled with a high-
integrity nondeterministic choice, since we are concerned with
understanding the relationship between two executions where
the driver makes the same decisions but in one execution the
sensor is corrupted. If in the two executions the driver is
making different choices, the two executions might diverge
almost arbitrarily, even if the corrupted sensor has no security
impact. If, on the other hand, we want to use this technique to
determine whether an autonomous vehicle’s driving subsystem
can be inﬂuenced by a corrupted temperature sensor, we
would need a more precise model of the system that does not
use nondeterministic choice between accelerating and braking
to model the driving subsystem’s decisions. That is, high-
integrity nondeterministic choices are by assumption choices
that cannot be inﬂuenced by the attacker.

VI. CASE STUDIES

To demonstrate the feasibility and efﬁcacy of our approach,
we conduct three case studies of non-trivial systems. The
ﬁrst two case studies analyze robustness of safety with the

1 Definitions.
2 HP ctrl ≡ ωs:= ωp; vs:= vp;

vs − ωs ∗ R
vs

; λp:=

λc:=
µp:= C1(1 − e−C2 λp ) − C3λp;
(

(?λc < λref ; BRAKE:= 0; Tb:= 0)

vp − ωp ∗ R
vp

;

3

4

5

6

7

8 HP Plant ≡ v′

∪ (?λc = λref ; ?True)
∪ (?λc > λref ; BRAKE:= 1; Tb:= 1200) ); t := 0
µpFN R − Tb
J

− µpFN
m

, t′ = 1

p =

p =

, ω′

& vp ≥ 0 ∧ ωp ≥ 0 ∧ t ≤ ǫ

/* control interval */

≡ (vp = 100 ∧ ωp ≥ 0)
≡ (vp > 25 → ωp ≥ 1)

/* wheel inertia and wheel radius */
/* normal force and vehicle mass */
/* reference value of wheel slip ratio */

9
10 B φpre
11 B φpost
12 R ǫ.
13 R C1, C2, C3. /* constant for computing µ */
14 R J, R.
15 R FN , m.
16 R λref .
17 ProgramVariables.
18 R BRAKE.
19 R Tb.
20 R ωp, ωs.
21 R vp, vs.
22 R λp, λc.
23 R µp.
24 R t.
25 Problem.

/* brake status */
/* braking torque */
/* wheel speed (physical and sensed ) */
/* vehicle speed (physical and sensed) */
/* wheel slip (physical and calculated) */
/* adhesion coefficient */
/* clock variable */

26

φpre → [(ctrl; plant)∗]φpost

Fig. 9: HP model of an ABS system

decomposition approach, and the third one proves robustness
of high-integrity state with the self-composition approach.

A. Case Study: an Anti-lock Braking System

System designers may wonder if the system is robustly safe
against sensor attacks or if their countermeasures are effective.
This case study demonstrates analyzing robustness of safety
with the decomposition approach in an Anti-lock Braking
System (ABS). An ABS is a safety braking system used on
aircraft and vehicles. It operates by preventing the wheels from
locking up during braking, thereby maintaining tractive contact
with the road surface. ABS monitors the speed of wheels using
the wheel-speed sensors. If the controller sees that one wheel
is decelerating at a rate that couldn’t possibly correspond to
the vehicle’s rate of deceleration, it reduces the brake pressure
applied to that wheel, which allows it to turn faster. Once the
wheel is back up to speed, it applies the brake again [35].

a) Modeling ABS: Figure 9 shows a model of an ABS system
[36], [37]. The model assumes a single wheel and uses a
simple controller that turns on and off maximum braking
torque. Intuitively, ABS systems are designed to achieve the
maximum friction under certain circumstances (e.g., braking
on icy road surface). They achieve this by maintaining an ideal
slip ratio (e.g., λref in Figure 9). Our controller switches the
brake on and off based on the calculated slip ratio (λc) and
reference slip ratio (lines 5–7). The calculated slip ratio is
computed using sensed wheel speed and vehicle speed (lines
2–3). The physical slip ratio (λp) depends on physical wheel
speed (vp) and vehicle speed (wp), which are affected by
braking torque (Tb) and adhesion coefﬁcient (µp) that depends
on the physical slip ratio (line 4).

The initial condition of the ABS system (φpre) is that the
vehicle is moving at a high speed and its wheel speed is
not negative (line 10). The safety condition (φpost) is that the
vehicle’s wheel should not lock if the current vehicle speed is
large (line 11) [38].

b) Modeling Non-invasive Attack on ABS: Previous research
has demonstrated attacks on ABS through physical channels
[39]. By placing a thin electromagnetic actuator near the ABS
wheel-speed sensors, an attacker can inject magnetic ﬁelds to
both cancel the true measured signal and inject a malicious
signal, thus spooﬁng the measured wheel speeds. Such an
attack is a SA-sensor attack, where SA = {ωs}, on the
wheel-speed sensor. Let P be the hybrid program modeling
an ABS system shown in Figure 9. Then ATTACKED(P, {ωs})
is program P with line 2 changed into the following:

ctrl ≡ ωs := ∗; vs := vp

assuming

robustly safe when the sensor ωs
Program P is not
SAFE(P, φpre, φpost)
is
holds,
compromised:
SAFE(ATTACKED(P, {ωs}), φpre, φpost) doesn’t necessarily
hold, since ωs can be an arbitrary value.
c) Designing Robustly Safe ABS System: System designers,
in attempts to make ABS system modeled in Figure 9 safer,
would be conﬁdent in their design if the system with counter-
measures can be proven to be robustly safe against the attack.

Assume that designers deploy three wheel-speed sensors and
a majority voting scheme in the ABS system modeled in
Figure 9. The countermeasure can be modeled by changing
line 2 in Figure 9 into the ctrl ≡ voting;vs := vp;, where
voting is the following:

voting ≡ ωs1 := ωp; ωs2 := ωp; ωs3 := ωp;

if (ωs1 = ωs2 ∨ ωs1 = ωs3 )
then ωs := ωs1 else ωs := ωs2

Using the decomposition approach, we can prove that such an
ABS system is robustly safe if only one wheel-speed sensor
is compromised. The proof can be found in Appendix B.

B. Case study: Boeing 737-MAX

Robustness of safety is a relational property: if the original
system is safe then the attacked system will be safe too.
Importantly, this separates reasoning about the implications
of sensor attacks from reasoning directly about safety prop-
erties. Proving a system’s safety is often labor-intensive and
may even be epistemically problematic. For example, many
systems must be veriﬁed and validated empirically because
their correctness properties are not possible to state in a formal
language. However, when it is not easy or even impossible to
formally verify safety, it is often still possible to prove that
compromised sensors do not affect the safety property.

To demonstrate this advantage of relational reasoning, we
present a case study inspired by the Boeing 737-MAX Ma-
neuvering Characteristics Augmentation System (MCAS) [40].
The 737-MAX’s dynamics are extremely complicated, and
proving properties about similar stabilization systems is an

1 Definitions.
2 B φpre
3 B φpost
4 HP plant.
5 HP MCAS.
6 HP ctrlaoa
7

/* preconditions (abstract) */
/* functional safety property (abstract) */
/* plane’s dynamics (abstract) */
/* MCAS actuation (abstract) */
≡ ( (sL := aoap; sR := ∗)

∪ (sL := ∗; sR := aoap) );
(aoas := sL ∪ aoas := sR)

≡ ctrlaoa; MCAS(aoas)

/* physical AOA */
/* left and right AOA sensor */
/* AOA used by MCAS */

8
9 HP ctrl
10 ProgramVariables.
11 R aoap.
12 R sL, sR.
13 R aoas.
14 Problem.
15 φpre → [(ctrl;plant)∗]φpost

Fig. 10: A Simple Model of Boeing737 Max ﬂawed MCAS.

open challenge in hybrid systems veriﬁcation [41]. Nonethe-
less, we are able to analyze robustness of safety against faults
or attacks on the angle of attack (AOA) sensor used by the
737-MAX MCAS, even without an analysis of the system’s
overall safety property or the MAX’s ﬂight dynamics.

a) Modeling MCAS: The MCAS caused at least two deadly
crashes in 2019 [42]. MCAS was added to compensate for
instability induced by the 737-MAX’s new engines. Adding
new engines to an existing airframe resulting in an aircraft
whose nose tended to pitch upward, risking stalls. The MCAS
adjusts the plane’s horizontal stabilizer in order to push the
nose down when the aircraft is operating in manual ﬂight at an
elevated angle of attack (AOA). In many 737-MAX planes, the
MCAS is activated by inputs from only one of the airplane’s
two angle of attack sensors. In both 2019 crashes, the MCAS
was triggered repeatedly due to a failed AOA sensor. These
false readings caused the MCAS software to repeatedly push
the plane’s nose down, ultimately interacting with manual
inputs in a way that caused violently parabolic ﬂight paths
terminating in lost altitude and an eventual crash.

Figure 10 shows a simpliﬁed model of the original MCAS. The
controller, plane’s ﬂight dynamics, and manual control inputs
are all left abstract: the model focuses only on how values
read by the left and right AOA sensors are used in MCAS.
On each control iteration, one of the two AOA sensors is
randomly chosen (ctrlaoa) and the MCAS is activated using
the value of the chosen sensor (line 9). In this model, we
intentionally omit details about the ﬂight controller, MCAS
system, and ﬂight dynamics. Even with a high-ﬁdelity model
[43], proving correctness for the 737-MAX MCAS requires
advances in state-of-the-art reachability analysis for hybrid
time systems; fortunately, relational reasoning allows us to
nonetheless analyze robustness of the system against faults or
attacks on the AOA sensors.

b) Reasoning for Robustness of Safety: Program ctrlaoa is
not robustly safe if either of the AOA sensors is compromised,
since aoas can have false readings. Therefore, the system is
not robustly safe for attacks on AOA sensors. Boeing’s pro-
posed ﬁx to MCAS includes a requirement that the controller
should compare inputs from both AOA sensors [44], which can

1 Definitions.
2 ...
3 HP accel
4 HP brake
5 HP ctrlv
6
7 HP ctrlt
8

9

≡ ?ψ; busV := A
≡ busV := −B
≡ vs:= vp; ds:= dp;

if (c) then accel else brake

≡ temps:= tempp;

(

(?temps > T ; busV := -1)

∪ (?temps < T ; busV := 1)
∪ (?temps = T ) )

10
11 HP ctrlbus ≡ (
12
13 HP ctrlr
14
15 HP ctrl
16 HP plant

≡ (

(?busV = a; ctrlt; busH:= 1)

∪ busH:= 0 )

(?busH = 0; a:= busV )

∪ (?busH = 1; thermo:= busV ) )
≡ choices; ctrlv; ctrlbus; ctrlr; t:= 0
≡ d′

p = a, temp′ = thermo, t′ = 1

p = −vp, v′

& (vp ≥ 0 ∧ t ≤ ǫ)

17
18 ProgramVariables.
19 R busV .
20 R busH.
21 ...
22 Problem.
23 φpre → [(ctrl; plant)∗]φpost

/* value on the bus */
/* header indicating the type of information */

Fig. 11: HP model (in canonical form) of an autonomous vehicle
with an internal bus

be modeled by adding the following at the end of ctrlaoa:
(?sL = sR) ∪ (?¬(sL = sR); aoas := 0)

We can prove that
safe. Let ctrl′
we know ctrl′
and ctrl′
(ctrl;plant)∗ with ctrl′
position approach. The proof is included in Appendix B.

the system with this ﬁx is robustly
aoa be ctrlaoa with this simple ﬁx. Then
aoa, {sL})
aoa, {sR}). Program
aoa is robustly safe by the decom-

aoa ≈{aoas}
aoa ≈{aoas} ATTACKED(ctrl′

ATTACKED(ctrl′

C. Case Study: An Autonomous Vehicle with an Internal Bus

Figure 8 shows the self-composition approach with a model
of an autonomous vehicle with interior temperature control.
However, the model doesn’t account for any internal commu-
nication mechanisms. In modern vehicles, Electronic Control
Units (ECUs) oversee a broad range of functionality, including
the drivetrain, lighting, and entertainment. They often commu-
nicate through an internal bus [45].

In this case study, we explore how to use the self-composition
approach to analyze robustness of high-integrity state in a
model of an autonomous vehicle with an internal bus that
communicates both low-integrity messages (sensed tempera-
ture) and a high-integrity messages (sensed velocity). We are
interested in whether the high-integrity state (i.e., velocity) is
robust when the temperature sensor is compromised.

a) Modeling a Vehicle with an Internal Bus: Figure 11
shows a model (in canonical form) of such a system (elided
contents are the same as in the model previously presented
in Figure 8). We model the bus using two variables: a value
variable (busV ), which indicates the current value that sits
on the bus, and a header variable (busH), which indicates
the type of information that sits on the bus: busH = 0 for
acceleration, busH = 1 for temperature. Exactly one message
is communicated via the bus at each control loop iteration.
Acceleration messages have higher priority over thermostat

messages. Program (ctrlv) ﬁrst sets the bus value to the next
acceleration value. Program ctrlbus then checks if the value
has changed from the existing acceleration value. If not, it
activates temperature control (ctrlt) to set the bus value to
desired thermostat value (line 11). Otherwise, busH is sent
to 0 to indicate that a new acceleration value has arrived
(line 12). Program ctrlr reads a value off the bus and sets
corresponding values based on the header (lines 13–14).

b) Robust High-Integrity State: We are interested in whether
the vehicle’s high-integrity state—vp,
the velocity of the
vehicle—is robust when its low-integrity sensor (temps) is
compromised. Speciﬁcally, we wonder whether P ≈{vp}
ATTACKED(P, {temps}), where P is the model shown in Fig-
ure 11. We can prove this using the self-composition approach.
Figure 12 shows IC(P, {temps}, ξ), where ξ renames vari-
ables in BV(P ) with a subscript 1 (we elide the descriptions
of program variables introduced in Figure 11). By choosing the
equivalence formula as vp = vp1 ∧ dp = dp1 ∧ a = a1 (line 3),
we are able to prove the desired property at line 40. Proving
this property means for this vehicle, its high-integrity variable
vp, dp, and a are robust when its temperature sensor is
compromised. We have proven the model in Figure 12 using
KeYmaera X.

Note that the decomposition approach and self-composition
approach may work well in different settings. The decompo-
sition approach is easy to apply and works well when the
sensor attack affects a small portion of the system, as in
our ﬁrst two case studies; by constrast, the self-composition
approach can handle cases where the effect of the attack may
be complicated—as in our third case study—but requires more
effort to use. It is possible to combine the two techniques to
prove robustness properties of complicated cases. For example,
if we can identify that only a single component of a large
system is affected by an attack, the self-composition approach
can be used to prove robustness of this component, while the
decomposition approach delivers the robustness proof of the
whole system.

VII. RELATED WORK

Formal analysis of sensor attacks Lanotte et al. [16],
[17] propose formal approaches to model and analyze sensor
attacks with a process calculus. The threat model allows
attacks that manipulate sensor readings or control commands
to compromise state. Their model of physics is discrete and it
focuses on timing aspects of attacks on sensors and actuators.
In comparison, we analyze relational properties in systems
whose dynamics are modeled with differential equations and
we introduce techniques to establish proofs of these properties.

Bernardeschi et al. [46] introduce a framework to analyze
the effects of attacks on sensors and actuators. Controllers
of systems are speciﬁed using the formalism PVS [47]. The
physical parts are assumed to be described by other modeling
tools. Their threat model is similar to ours: the effect of an
attack is a set of assignments to the variables deﬁned in the

1 Definitions.
2 ...
3 B eqη
4 B ψ
5 HP choices
6 HP accel
7 HP brake
8 HP ctrlv
9
10 HP ctrlt
11

12

s + (A + B)(Aǫ2 + 2vsǫ)

≡ vp = vp1 ∧ dp = dp1 ∧ a = a1
≡ 2Bds > v2
≡ c:= ∗
≡ ?ψ; busV := A
≡ busV := −B
≡ vs:= vp; ds:= dp;

if (c) then accel else brake

≡ temps:= tempp;

(

(?temps > T ; busV := -1)

∪ (?temps < T ; busV := 1)
∪ (?temps = T ) )

13
14 HP ctrlbus
15
16 HP ctrlr
17
18 HP ctrl
19 B ψ1
20 HP choices1 ≡ c1:= c
21 HP accel1
22 HP brake1
23 HP ctrlv1
24
25 HP ctrlt1
26

(

27

≡ ( (?busV = a; ctrlt; busH:= 1)

∪ busH:= 0 )

≡ ( (?busH = 0; a:= busV )

∪ (?busH = 1; thermo:= busV ) )

≡ ctrlv; ctrlbus; ctrlr; t:= 0
≡ 2Bds1 > v2
s1

+ (A + B)(Aǫ2 + 2vs1 ǫ)

≡ ?ψ1; busV1 := A
≡ busV1 := −B
≡ vs1 := vp1 ; ds1 := dp1 ;

if (c1) then accel1 else brake1

≡ temps1 := *;

(?temps1 > T ; busV1:= -1)
∪ (?temps1 < T ; busV1:= 1)
∪ (?temps1 = T ) )

≡ ( (?busH1 = 0; a1:= busV1)

28
29 HP ctrlbus1 ≡ ( (?busV1 = a1; ctrlt1 ; busH1:= 1)
∪ busH1:= 0 )
30
31 HP ctrlr1
32
33 HP ctrl1
34 HP ctrl′
35 HP plant′
36

≡ ctrlv1 ; ctrlbus1 ; ctrlr1 ; t1:= 0
≡ choices; ctrl; choices1; ctrl1
≡ d′
p = a, temp′
p = −vp, v′
d′
p1
& (vp ≥ 0 ∧ vp1 ≥ 0 ∧ t ≤ ǫ ∧ t1 ≤ ǫ)

∪ (?busH1 = 1; thermo1:= busV1) )

= a1, temp′
p1

= −vp1 , v′
p1

p = thermo, t′ = 1

37
38 ...
39 Problem.
40 eqη → [(ctrl′; plant′)∗]eqη

= thermo1, t′

1 = 1

Fig. 12: Interleaved composition of the hybrid program (in canonical
form) modeling an autonomous vehicle with an internal bus shown
in Figure 11

controller. Simulation is used to analyze effects of attacks. By
contrast, we focus on formal analysis for the whole system and
propose concrete proof techniques for relational properties.

Analyzing relational properties of cyber-physical systems
Akella et al. [48] use trace-based analysis and apply model
checking to verify information-ﬂow properties for discrete
models based on process algebra. Prabhakar et al. [49] in-
troduce a type system that enforces noninterference for a
hybrid system modeled as a programming language. Nguyen et
al. [50] propose a static analysis that checks noninterference
for hybrid automata. Liu et al. [51] introduce an integrated
architecture to provide provable security and safety assurance
for cyber-physical systems. They focus on integrated co-
development: language-based information-ﬂow control using
Jif [52] and a veriﬁed hardware platform for information-ﬂow
control. Their focus is not on sensor attacks.

Bohrer et al. [53] verify nondeducibility in hybrid programs,
a noninterference-like guarantee. To do this, they introduce a
very expressive modal logic that can explicitly express that
formulas hold in a given world (i.e., state). By contrast, we

use an existing logic (that has good tool support) to express
and reason about a speciﬁc threat model.

Closely related to our work is that of Kolˇcák et al. [54] which
introduces a relational extension of dL. A key contribution
of their work is a new proof rule to combine two dynamics,
allowing existing inference rules of dL to be applied in a
relational setting. Similar to their work, our self-composition
technique expresses relational properties by leveraging a com-
position of two programs whose variables are disjoint. Unlike
their work, our self-composition technique aims to prove
relational properties that require some of the nondeterministic
choices to be resolved in the same way in both executions.
For instance, our example shown in Figure 8 is not directly
expressible in their setting. We believe that the work by Kolˇcák
et al. [54] is orthogonal to ours, and the two can be combined
to express and prove more complicated relational properties.

Security analysis for CPSs Much work have focused on the
security of cyber-physical systems (CPS), but primarily from a
systems security perspective rather than using formal methods.
Various attacks (and mitigations of these attacks) have been
identiﬁed, including false data injection [55], replay attacks
[56], relay attacks [57], spying [58], and hijacking [59]. Our
work focuses on formal methods for CPS security, ruling out
entire classes of attacks.

Mitigating sensor attacks Some work propose attack-resilient
state estimation to defend against adversarial sensor attacks
in cyber-physical systems [60], [61]. These methods model
systems with bounded sensor noises as an optimization prob-
lem to locate potentially malicious sensors. Our work has a
different formal model of sensor attacks and focuses on formal
guarantees of robustness of systems under sensor attacks.

VIII. CONCLUSION

We have introduced a formal framework for modeling and
analyzing sensor attacks on cyber-physical systems. We for-
malize two relational properties that relate executions in the
original system and a system where some sensors have been
compromised. The relational properties express the robustness
of safety properties and the robustness of high-integrity state.

Both relational properties can be expressed in terms of an
equivalence relation between programs, and we presented two
approaches to reason about this equivalence relation, one based
on decomposition and the other based on using a single
program to represent executions of the original system and
the attacked system. We have shown both of these approaches
sound, and used them on three case studies of non-trivial
cyber-physical systems.

This work focuses on sensors, but our approach can also be
used to model and analyze attacks on actuators.

REFERENCES

[1] R. Alur, “Formal veriﬁcation of hybrid systems,” in ACM International

Conference on Embedded Software, 2011, pp. 273–278.

[2] D. Bresolin, L. Geretti, R. Muradore, P. Fiorini, and T. Villa, “Formal
veriﬁcation applied to robotic surgery,” in Coordination Control of
Distributed Systems, 2015, pp. 347–355.

[3] J.-B. Jeannin, K. Ghorbal, Y. Kouskoulas, R. Gardner, A. Schmidt,
E. Zawadzki, and A. Platzer, “A formally veriﬁed hybrid system for the
next-generation airborne collision avoidance system,” in International
Conference on Tools and Algorithms for the Construction and Analysis
of Systems, 2015, pp. 21–36.

[4] S. Mitsch, K. Ghorbal, D. Vogelbacher, and A. Platzer, “Formal ver-
iﬁcation of obstacle avoidance and navigation of ground robots,” The
International Journal of Robotics Research, vol. 36, no. 12, pp. 1312–
1340, 2017.

[5] Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A. Chen,
K. Fu, and Z. M. Mao, “Adversarial sensor attack on LiDAR-based
perception in autonomous driving,” in ACM SIGSAC Conference on
Computer and Communications Security, 2019, pp. 2267–2281.

[6] “‘Spoofers’ use fake GPS signals to knock a yacht off course,” MIT

Technology Review, 2013.

[7] D. Davidson, H. Wu, R. Jellinek, V. Singh, and T. Ristenpart, “Control-
ling UAVs with sensor input spooﬁng attacks,” in USENIX Workshop
on Offensive Technologies, 2016.

[8] Y. Son, H. Shin, D. Kim, Y. Park, J. Noh, K. Choi, J. Choi, and Y. Kim,
“Rocking drones with intentional sound noise on gyroscopic sensors,”
in USENIX Security Symposium, 2015, pp. 881–896.

[9] N. Kalra and S. M. Paddock, “Driving to safety: How many miles of
driving would it take to demonstrate autonomous vehicle reliability?”
Transportation Research Part A: Policy and Practice, vol. 94, pp. 182–
193, 2016.

[10] A. Platzer, “The complete proof theory of hybrid systems,” in IEEE/ACM
Symposium on Logic in Computer Science, 2012, pp. 541–550.

[11] R. Alur, Principles of cyber-physical systems. MIT Press, 2015.
[12] K. G. Larsen, “Veriﬁcation and performance analysis for embedded
systems,” in IEEE International Symposium on Theoretical Aspects of
Software Engineering, 2009, pp. 3–4.

[13] E. A. Lee and S. A. Seshia, Introduction to embedded systems: A cyber-

physical systems approach. MIT press, 2016.

[14] P. Tabuada, Veriﬁcation and control of hybrid systems: a symbolic

approach. Springer, 2009.

[15] A. Tiwari, “Logic in software, dynamical and biological systems,” in
IEEE Symposium on Logic in Computer Science, 2011, pp. 9–10.
[16] R. Lanotte, M. Merro, R. Muradore, and L. Viganò, “A formal approach
to cyber-physical attacks,” in IEEE Computer Security Foundations
Symposium, 2017, pp. 436–450.

[17] R. Lanotte, M. Merro, A. Munteanu, and L. Viganò, “A formal approach
to physics-based attacks in cyber-physical systems,” ACM Transactions
on Privacy and Security, vol. 23, no. 1, pp. 1–41, 2020.

[18] M. R. Clarkson and F. B. Schneider, “Hyperproperties,” Journal of

Computer Security, vol. 18, no. 6, pp. 1157–1210, 2010.

[19] A. Sabelfeld and A. C. Myers, “Language-based information-ﬂow secu-
rity,” IEEE Journal on selected areas in communications, vol. 21, no. 1,
pp. 5–19, 2003.

[20] J. A. Goguen and J. Meseguer, “Security policies and security models,”
in IEEE Symposium on Security and Privacy, 1982, pp. 11–20.
[21] A. Platzer, “Differential dynamic logic for hybrid systems,” Journal of

Automated Reasoning, vol. 41, no. 2, pp. 143–189, 2008.

[22] A. Platzer, Logical foundations of cyber-physical systems.

Springer,

2018, vol. 662.

[23] N. Fulton, S. Mitsch, J.-D. Quesel, M. Völp, and A. Platzer, “KeY-
maera X: An axiomatic tactical theorem prover for hybrid systems,” in
International Conference on Automated Deduction, 2015, pp. 527–538.
[24] G. Barthe, P. R. D’Argenio, and T. Rezk, “Secure information ﬂow
by self-composition,” in Proceedings. 17th IEEE Computer Security
Foundations Workshop, 2004, pp. 100–114.

[25] D. Kozen, “Kleene algebra with tests,” ACM Transactions on Program-
ming Languages and Systems (TOPLAS), vol. 19, no. 3, pp. 427–443,
1997.

[26] A. Platzer, “A complete uniform substitution calculus for differential
dynamic logic,” Journal of Automated Reasoning, vol. 59, no. 2, pp.
219–265, 2017.

[27] D. Harel, D. Kozen, and J. Tiuryn, Dynamic Logic. MIT Press, 2000.

[28] D. Sangiorgi, Introduction to bisimulation and coinduction. Cambridge

University Press, 2011.

[29] L. Pick, G. Fedyukovich, and A. Gupta, “Exploiting synchrony and
symmetry in relational veriﬁcation,” in International Conference on
Computer Aided Veriﬁcation, 2018, pp. 164–182.

[30] R. Shemer, A. Gurﬁnkel, S. Shoham, and Y. Vizel, “Property directed
self composition,” in International Conference on Computer Aided
Veriﬁcation, 2019, pp. 161–179.

[31] T. Terauchi and A. Aiken, “Secure information ﬂow as a safety problem,”
in International Static Analysis Symposium, 2005, pp. 352–367.
[32] A. Müller, S. Mitsch, W. Retschitzegger, W. Schwinger, and A. Platzer,
“A component-based approach to hybrid systems safety veriﬁcation,”
in International Conference on Integrated Formal Methods, 2016, pp.
441–456.

[33] A. Platzer, “A temporal dynamic logic for verifying hybrid system
invariants,” in International Symposium on Logical Foundations of
Computer Science, 2007, pp. 457–471.

[34] J.-B. Jeannin and A. Platzer, “dTL2: differential temporal dynamic logic
with nested temporalities for hybrid systems,” in International Joint
Conference on Automated Reasoning, 2014, pp. 292–306.

[35] U. Kiencke and L. Nielsen, Automotive Control Systems: For Engine,
Berlin, Heidelberg: Springer-Verlag,

Driveline and Vehicle, 1st ed.
2000.

[36] M. Tanelli, A. Astolﬁ, and S. M. Savaresi, “Robust nonlinear output
feedback control for brake by wire control systems,” Automatica, vol. 44,
no. 4, pp. 1078–1087, 2008.

[37] P. Bhivate, “Modelling & development of antilock braking system,”

Ph.D. dissertation, 2011.

[38] S. Solyom, A. Rantzer, and J. Lüdemann, “Synthesis of a model-based
tire slip controller,” Vehicle System Dynamics, vol. 41, no. 6, pp. 475–
499, 2004.

[39] Y. Shoukry, P. Martin, P. Tabuada, and M. Srivastava, “Non-invasive
spooﬁng attacks for anti-lock braking systems,” in International Work-
shop on Cryptographic Hardware and Embedded Systems, 2013, pp.
55–72.

[40] “Maneuvering characteristics

augmentation system - wikipedia,”

https://en.wikipedia.org/wiki/Maneuvering_Characteristics_Augmentation_System,
accessed: 2021-1-10.

[41] P. Heidlauf, A. Collins, M. Bolender, and S. Bak, “Veriﬁcation chal-
lenges in F-16 ground collision avoidance and other automated ma-
neuvers.” in 5th International Workshop on Applied Veriﬁcation for
Continuous and Hybrid Systems, 2018, pp. 208–217.

[42] “Boeing

737

Is
https://www.eetasia.com/automation-and-boeings-b737-max-crash/,
accessed: 2021-1-10.

automation

Max:

to

blame?”

[43] A. Marcos and G. Balas, “Linear parameter varying modeling of the
Boeing 747-100/200 longitudinal motion,” in AIAA Guidance, Naviga-
tion, and Control Conference and Exhibit, 2001, p. 4347.

[44] “Boeing:

The

737 MAX MCAS

software

enhancement,”

https://www.boeing.com/commercial/737max/737-max-software-updates.page,
accessed: 2021-1-10.

[45] S. Checkoway, D. McCoy, B. Kantor, D. Anderson, H. Shacham,
S. Savage, K. Koscher, A. Czeskis, F. Roesner, and T. Kohno, “Com-
prehensive experimental analyses of automotive attack surfaces.” in
USENIX Security Symposium, 2011, pp. 447–462.

[46] C. Bernardeschi, A. Domenici, and M. Palmieri, “Formalization and co-
simulation of attacks on cyber-physical systems,” Journal of Computer
Virology and Hacking Techniques, pp. 1–15, 2020.

[47] S. Owre, J. M. Rushby, and N. Shankar, “PVS: A prototype veriﬁcation
system,” in International Conference on Automated Deduction, 1992,
pp. 748–752.

[48] R. Akella, “Veriﬁcation of information ﬂow security in cyber-physical

systems,” 2013.

[49] P. Prabhakar and B. Köpf, “Verifying information ﬂow properties of
hybrid systems,” in ACM international conference on High conﬁdence
networked systems, 2013, pp. 77–84.

[50] L. V. Nguyen, G. Mohan, J. Weimer, O. Sokolsky, I. Lee, and R. Alur,
“Detecting security leaks in hybrid systems with information ﬂow
analysis,” in ACM-IEEE International Conference on Formal Methods
and Models for System Design, 2019, p. 14.

[51] J. Liu, J. Corbett-Davies, A. Ferraiuolo, A. Ivanov, M. Luo, G. E. Suh,
A. C. Myers, and M. Campbell, “Secure autonomous cyber-physical
systems through veriﬁable information ﬂow control,” in Proceedings of
the 2018 Workshop on Cyber-Physical Systems Security and Privacy,
2018, pp. 48–59.

[52] A. C. Myers, “JFlow: Practical mostly-static information ﬂow control,”
the 26th ACM SIGPLAN-SIGACT symposium on

in Proceedings of
Principles of programming languages, 1999, pp. 228–241.

[53] B. Bohrer and A. Platzer, “A hybrid, dynamic logic for hybrid-dynamic
information ﬂow,” in ACM/IEEE Symposium on Logic in Computer
Science, 2018, pp. 115–124.

[54] J. Kolˇcák, J. Dubut, I. Hasuo, S.-y. Katsumata, D. Sprunger, and
A. Yamada, “Relational differential dynamic logic,” in International
Conference on Tools and Algorithms for the Construction and Analysis
of Systems. Springer, 2020, pp. 191–208.

[55] K. Koscher, A. Czeskis, F. Roesner, S. Patel, T. Kohno, S. Checkoway,
D. McCoy, B. Kantor, D. Anderson, H. Shacham, and S. Savage,
“Experimental security analysis of a modern automobile,” in IEEE
Symposium on Security and Privacy, 2010, pp. 447–462.

[56] C. Li, A. Raghunathan, and N. K. Jha, “Hijacking an insulin pump:
Security attacks and defenses for a diabetes therapy system,” in IEEE
International Conference on e-Health Networking, Applications and
Services, 2011, pp. 150–156.

[57] A. Francillon, B. Danev, and S. Capkun, “Relay attacks on passive
keyless entry and start systems in modern cars,” in Network and
Distributed System Security Symposium, 2011.

[58] S. Checkoway, D. McCoy, B. Kantor, D. Anderson, H. Shacham,
S. Savage, K. Koscher, A. Czeskis, F. Roesner, and T. Kohno, “Com-
prehensive experimental analyses of automotive attack surfaces.” in
USENIX Security Symposium, 2011, pp. 447–462.

[59] R. Langner, “Stuxnet: Dissecting a cyberwarfare weapon,” IEEE Security

& Privacy, vol. 9, no. 3, pp. 49–51, 2011.

[60] M. Pajic, J. Weimer, N. Bezzo, P. Tabuada, O. Sokolsky, I. Lee, and
G. J. Pappas, “Robustness of attack-resilient state estimators,” in 2014
ACM/IEEE International Conference on Cyber-Physical Systems, 2014,
pp. 163–174.

[61] M. Pajic, I. Lee, and G. J. Pappas, “Attack-resilient state estimation for
noisy dynamical systems,” IEEE Transactions on Control of Network
Systems, vol. 4, no. 1, pp. 82–92, 2016.

APPENDIX A
DEFINITIONS

We present the formal deﬁnitions of bound variables, free vari-
ables, and variable sets here. These deﬁnitions are exactly as
given in [22], [26], and included for the reader’s convenience.

Deﬁnition 11 (Bound variables). The set BV(φ) of bound
variables of dL formula φ is deﬁned inductively as:
BV(θ1 ∼ θ2) = ∅

∼∈ {<, ≤, =, >, ≥}

BV(¬φ) = BV(φ)

BV(φ ∨ ψ) = BV(φ ∧ ψ) = BV(φ) ∪ BV(ψ)
BV(φ → ψ) = BV(φ) ∪ BV(ψ)

BV(∀x. φ) = BV(∃x. φ) = {x} ∪ BV(φ)

BV([α]φ) = BV(α) ∪ BV(φ)

The set BV(P ) of bound variables of hybrid program P , i.e.,
those may potentially be written to, is deﬁned inductively as:

BV(x := θ) = BV(x := ∗) = {x}

BV(?φ) = ∅
BV(x′ = θ & φ) = {x, x′}
BV(α; β) = BV(α ∪ β) = BV(α) ∪ BV(β)

BV(α∗) = BV(α)

Deﬁnition 12 (Must-bound variables). The set MBV(P ) ⊆
BV(P ) of most bound variables of hybrid program P , i.e.,
all those that must be written to on all paths of P , is deﬁned
inductively as:

MBV(x := θ) = MBV(x := ∗) = {x}

MBV(?φ) = ∅
MBV(x′ = θ & φ) = {x, x′}

MBV(α ∪ β) = MBV(α) ∩ MBV(β)
MBV(α; β) = MBV(α) ∪ MBV(β)

MBV(α∗) = ∅

Deﬁnition 13 (Free variables). The set FV(θ) of variables of
term θ is deﬁned inductively as:

FV(x) = {x}
FV(c) = ∅

FV(θ1 ⊕ θ2) = FV(θ1) ∪ FV(θ2) ⊕ ∈ {+, ×}
The set FV(φ) of free variables of dL formula φ is deﬁned
inductively as:

FV(θ1 ∼ θ2) = FV(θ1) ∪ FV(θ2)

FV(¬φ) = FV(φ)

FV(φ ∨ ψ) = FV(φ ∧ ψ) = FV(φ) ∪ FV(ψ)
FV(φ → ψ) = FV(φ) ∪ FV(ψ)

FV(∀x. φ) = FV(∃x. φ) = FV(φ) \ {x}

FV([α]φ) = FV(α) ∪ (FV(φ) \ MBV(α))

The set FV(P ) of bound variables of hybrid program P is
deﬁned inductively as:

FV(x := θ) = FV(θ)
FV(x := ∗) = ∅

FV(?φ) = FV(φ)

FV(x′ = θ & φ) = {x} ∪ FV(θ) ∪ FV(φ)
FV(α ∪ β) = FV(α) ∪ FV(β)

FV(α; β) = FV(α) ∪ (FV(β) \ MBV(α))

FV(α∗) = FV(α)

Deﬁnition 14 (Variable sets). The set VAR(P ), variables of
hybrid program P is BV(P ) ∪ FV(P ). The set VAR(φ),
variables of dL formula φ is BV(φ) ∪ FV(φ).

APPENDIX B
PROOFS

Proof of Theorem 1. P ≈FV(φpre∧φpost) ATTACKED(P, SA)
means for any execution σq of ATTACKED(P, SA), there exists
an execution σp of P that agrees on FV(φpre ∧ φpost) at
the starting state and the end of every control iteration. That
means if the starting state of σq satisﬁes φpre, the starting
state of σp satisﬁes φpre (Lemma 3 from [26]). Meanwhile,
since φpre → [P ]φpost, the last state of σp satisﬁes φpost. And
last states of σq and σp agree on free variables used in φpost,
so the last state of σq satisﬁes φpost (Lemma 3 from [26]).
(cid:3)
φpre → [ATTACKED(P, SA)]φpost holds.

Proof of Property 1 to 3 of Theorem 2. By the deﬁnition of
(cid:3)
H-equivalence.

Lemma 1. H-equivalence of states is transitive, reﬂective, and
symmetric.

Proof. By the deﬁnition of ≈H.

(cid:3)

Lemma 2. For program P , state ω, ω′, ν, and set H such
that (ω, ν) ∈ JP K, ω ≈H ω′, and FV(P ) ⊆ H, then there
exists ν′ such that (ω′, ν′) ∈ JP K and ν ≈H ν′.
Proof. By the deﬁnition of ≈H and lemma 4 from [26]. (cid:3)

Proof of robust safety of Boeing 737-MAX model. Let A
be program ctrl′
aoa, B be program ATTACKED(A, {sL}), C
be program MCAS(aoa); plant in Figure 10. Here, FV(A)
= FV(B) = {aoap}, let fv be the set of free variables of
program A; C, then FV(B; C)=fv, and FV(C) would be
{aoas} ∪ fv. We can prove ROBUST(A; C, φpre, φpost, {sL})
with the following steps:

By deﬁnition of ≈H, we prove A ≈{aoas,aoap} B, which
means

A ≈{aoas,aoap}∪fv B

(Property 3)

Proof of Property 4 of Theorem 2. We prove that for
any execution of A; C, there exists an execution of B; D
that agrees with it on H. The other direction can be proven
similarly. For any execution σac of A; C, let ωacf and ωacl
be its ﬁrst and last state respectively. Then there exists a state
ωacm such that (ωacf , ωacm) ∈ JAK and (ωacm, ωacl) ∈ JCK.
Since A ≈H B, there exist state ωbf , ωbl such that (ωbf , ωbl)
∈ JBK, ωbf ≈H ωacf , and ωbl ≈H ωacm. Likewise, since
C ≈H D, there exists state ωdf , ωdl such that (ωdf , ωdl)
∈ JDK, ωdf ≈H ωacm, and ωdl ≈H ωacl. By transitivity
(Lemma 1), we get ωbl ≈H ωdf . Since FV(D) ⊆ H, by
) ∈ JDK
Lemma 2, there exist state ωd′
and ωdl ≈H ωd′
and ωacf ≈H ωbf (by
transitivity), for the execution of A; C from ωacf to ωacl, we
) ∈ JB; DK, ωacf ≈H ωbf , and ωacl ≈H ωd′
have (ωbf , ωd′
.
(cid:3)
A; C ≈H B; D holds.

. Since ωacl ≈H ωd′

such that (ωbl , ωd′

l

l

l

l

l

l

Proof of Property 5 of Theorem 2. By induction on the
number of iterations of α∗ and β∗. Base case is trivial. For
the induction case, assume αk ≈H βk is true, we can prove
αk; α ≈H βk; β using Property 4 by letting A be αk, B be
(cid:3)
βk, C be α, and D be β. Thus, α∗ ≈H β∗ holds.

Proof of robust safety of the ABS model. Let P be the hybrid
program modeling ABS with duplicated sensors. Assume
sensor ω1 is compromised. Let A be the voting program, B
be ATTACKED(A, {ω1}), and C be program P with voting
excluded (i.e., P = (A; C)∗ and ATTACKED(P, {ω1}) =
(B; C)∗). Here, FV(A) = FV(B) = {ωp}, FV(A; C) =
FV(B; C), and FV(C) = {ωs} ∪ FV(A; C).

By the deﬁnition of ≈H, A ≈{ωs,ωp} B holds, which means
A ≈FV(C) B
(Property 3)

With C ≈FV(C) C (Property 1), we get

(A; C) ≈FV(C) (B; C)

(Property 4)

which leads to

(A; C)∗ ≈FV(C) (B; C)∗

(Property 5)
Property 2 also applies to programs with loop, and {ωp, vp}
⊆ FV(C), thus

(A; C)∗ ≈{ωp,vp} (B; C)∗

FV(φpre ∧ φpost)

Since
ROBUST(P, φpre, φpost, {ωs1}) (Theorem 1).

=

{ωp, vp}, we

(Property 2)
have

Similarly, we can prove ROBUST(P, φpre, φpost, {ωs2}) and
(cid:3)
ROBUST(P, φpre, φpost, {ωs3}).

With C ≈{aoas,aoap}∪fv C (Property 1), we know
A; C ≈{aoas,aoap}∪fv B; C

(Property 4)
Since FV(A; C) ∪ FV(B; C) ⊆ {aoas, aoap} ∪ fv, we know
(Property 5)

(A; C)∗ ≈{aoas,aoap}∪fv (B; C)∗

Property 2 applies to programs with loop as well, so

(A; C)∗ ≈fv (B; C)∗

(Property 2)
Since formula φpre and φpost typically refer to free variables in
fv, we get ROBUST(P, φpre, φpost, {sL}) holds. (Theorem 1).
(cid:3)
Similarly, we can prove ROBUST(P, φpre, φpost, {sR}).

APPENDIX C
LIMITATIONS OF THE SELF-COMPOSITION APPROACH

One limitation of our self-composition approach is that it
applies only for hybrid programs that have total semantics
low-integrity inputs. It means if a program has a
for all
valid execution on an input state ω (i.e., exist state ν such
that (ω, ν) ∈ JP K), then for any state ω′ that differs with
ω only in low-integrity inputs, there exists ν′ that (ω, ν) ∈
JATTACKED(P, SA)K.

A program may have partial (not total) semantics on low-
integrity inputs for two reasons: (1) some low-integrity inputs
fail test conditions in all execution paths, for example, if a
is a low-integrity variable, then ?a > 0 is a program whose
semantics are partial on low-integrity inputs; (2) the program’s
evolution constraint depends on low-integrity inputs. For ex-
ample, if a is a low-integrity variable, (x′ = θ&a > 0 ) is a
program whose semantics are partial on low-integrity inputs.

Fortunately, there is a relatively simple way to check that
hybrid programs meet this requirement. First, given a set
of low-integrity sensor variables, a straightforward program
analysis can identify all variables that might depend on a low-
integrity sensor variables; call these the low-integrity variables.
Second, check that all evolution constraints do not include
any low-integrity variables. Third, check that any test ?φi that
includes a low-integrity variable occurs as part of a construct
?φ1; α1 ∪ · · · ∪?φn; αn such that φ1 ∨ · · · ∨ φn is valid (i.e.,
the tests are exhaustive and so at least one of the branches of
the nondeterministic choice will be true).

Well-designed hybrid program models should have total se-
mantics on low-integrity inputs, except in speciﬁc situations
that rarely depend on low-integrity sensor variables. Models

that do not have total semantics on low-integrity inputs typ-
ically do not correspond to actually implementable control
strategies, and are therefore only vacuously safe.

APPENDIX D
SOUNDNESS PROOF OF THE SELF-COMPOSITION
APPROACH

TRACE-PLANT

σa
0

∼∼∼ζ σb

0

SINGLE.FUNCTION
∼∼∼ζ σb
σa
0
0
0 ) ∼∼∼ζ (σb
(σa
0)

m ≥ 1

n ≥ 1

dom(σa
(σa
1 . . . σa
0 . . . σa
(σa
m

0 ) 6= [0, 0]
m) ∼∼∼ζ (σb
∼∼∼ζ (σb

dom(σb
1 . . . σb
n)
0 . . . σb
n)

0) 6= [0, 0]

We use trace semantics of hybrid programs [33], [34] to prove
Theorem 3. The trace semantics of hybrid programs assigns
to each program α a set of traces τ (α). A state is a map from
the set of variables to real numbers. The set of all variables
is denoted V. The set of all states is denoted STA. A separate
state Λ (not in STA) denoting a failure of the system.

A trace is a (non-empty) ﬁnite or inﬁnite sequence σ =
(σ0, σ1, ...) of trace functions σi : [0, ri] → STA with duration
ri ∈ R. A position of σ is a pair (i, ι) with i ∈ N and ι in
the interval [0, ri]; the state of σ at (i, ι) is σι
i . For a state
ω ∈ STA, ˆω: 0 7→ ω is a point ﬂow at ω with duration 0. A
trace terminates if it is a ﬁnite sequence σ = (σ0, σ1, ...σn)
and σn 6= Λ. In that case, the last state is denoted as σn(rn).
The ﬁrst state of σ, denoted FST σ, is σ0(0). The set of all
traces is TRA.

We denote by ω[x 7→ r] the valuation assigning variable x to
d ∈ R and matching with ω on all other variables.

The trace semantics τ (α) of a hybrid program α is deﬁned
inductively [34]:

• τ (x := θ) = {(ˆω, ˆν) | ν = ω[x 7→ ωJθK]};
• τ (x′ = θ&φ) = {(σ) : σ is a state ﬂow of order 1
deﬁned on [0, r] or [0, +∞] solution of x′ = θ, and for
all t in its domain, σ(t) |= φ} ∪ {(ˆω, ˆΛ) : ω 6|= φ};

• τ (?φ) = {(ˆω) | ω |= φ} ∪ {(ˆω, ˆΛ) : ω 6|= φ};
• τ (α ∪ β) = τ (α) ∪ τ (β);
• τ (α; β) = {σ ◦ ρ

: σ ∈ τ (α), ρ ∈ τ (β) when
σ ◦ ρ is deﬁned}; where the composition σ ◦ ρ of σ =
(σ0, ..., σn) and ρ = (ρ0, ..., ρm) is
– σ ◦ ρ = (σ0, ..., σn, ρ0, ..., ρm) if σ terminates and

LST σ = FST ρ;

– σ if σ does not terminate;
– undeﬁned otherwise;

• τ (α∗) = ∪n∈Nτ (αn), where α0 is deﬁned as ?true, α1
is deﬁned as α and αn+1 is deﬁned as αn; α for n ≥ 1;
• τ (x := ∗) = {(ˆω, ˆν) | ν = ω[x 7→ d]} where d is some

real value.

Notice that the trace semantic for τ (x := ∗) is not deﬁned
in [33], [34]. We add it to complete the deﬁnition of trace
semantic needed in this work.

We refer to ﬁnite traces that end with failure state Λ as failure
traces, and other traces as normal traces. We denote τ⊲(P )
the set of normal traces of a program P :

τ⊲(P ) = {σ ∈ τ (P ) | LST σ 6= Λ ∨ σ does not terminate}

Now, we formalize the H-equivalence of states, trace func-
tions, traces, and programs. Compare with Deﬁnition 5, these

TRACE-DISCRETE

(m ≥ 0 ∧ n ≥ 1) ∨ (m ≥ 1 ∧ n ≥ 0)
FST σa ∼∼∼ζ FST σb

dom(σa
dom(σb

p ) 6= [0, 0] ∨ p = m
q) 6= [0, 0] ∨ q = n
∀i(0 ≤ i < p ∧ p < m), dom(σa
i ) = [0, 0]
∀j(0 ≤ j < q ∧ q < n), dom(σb
j ) = [0, 0]
q . . . σb
n)
0 . . . σb
q−1, σb

m) ∼∼∼ζ (σb
m) ∼∼∼ζ (σb
Fig. 13: Deﬁnition of H-equivalence of traces

p . . . σa
p . . . σa

(σa
p−1, σa

(σa

0 . . . σa

q . . . σb
n)

formal deﬁnitions are more general (can be applied on pro-
grams with different variable sets) and uses a mapping function
between variables in two states (instead of using just a set).
Deﬁnition 15 (H-equivalence of states). We deﬁne ωi ∼∼∼ζ ωj,
for states ωi and ωj that agree on corresponding variables
that are related by function ζ, i.e.,

∀x ∈ dom(ζ), ωi(x) = ωj(ζ(x))

Here the domain of ζ corresponds to the H in Deﬁnition 5.
And ζ is often a subset of the renaming function of the
program of concern.

Deﬁnition 16 (H-equivalence of trace functions). We deﬁne
σi ∼∼∼ζ σj, for trace functions σi and σj that have the same
domain and H-equivalent states at all domain values:

dom(σi) = dom(σj ) and ∀p ∈ dom(σi), σi(p) ∼∼∼ζ σj (p)
Deﬁnition 17 (H-equivalence of traces). We deﬁne σa ∼∼∼ζ σb,
for trace σa and σb that agree on (1) the ﬁrst state (2) trace
functions whose domains are not [0, 0], and (3) the last state
if both are ﬁnite traces. Figure 13 shows the formal deﬁnition.
We write ω1 ∼∼∼idH ω2 to mean that ω1 and ω2 are
H-equivalence with respect to an identity function deﬁned
on set H and undeﬁned otherwise (i.e., idH). We write
σa ∼∼∼idH σb to indicate traces σa and σb are equivalent on idH.
We write σa ∼∼∼id σb to mean that σa and σb are equivalent
with all variables, i.e., the two traces use the same set of
variables and they are H-equivalent.

Deﬁnition 18 (H-equivalence of two programs by traces). For
two hybrid programs P1 and P2 of the canonical form, a
function ζ maps variables in P1 to variables in P2, P1 ∼∼∼ζ
P2 is deﬁned as follows:

∀σa ∈ τ⊲(P1), ∃σb ∈ τ⊲(P2) such that σa ∼∼∼ζ σb

To help express the agreement between executions composed
in a self-composition, we introduce the notion of projections
on states, trace functions, and traces.

Deﬁnition 19 (Projection). For state ω and a set V of
variables such that V ⊆ VAR(ω), the V projection of state ω,
denoted ω ⇓ V , is a map {x 7→ ω(x)} for all x ∈ V .

For a trace function σi: [0, ri] → STA and a set V of variables
such that V ⊆ VAR(σi), the V projection of σi, denoted σi ⇓
V , is {ι 7→ (σi(x) ⇓ V )} for all ι ∈ dom(σi).

For a trace σ = (σ0, . . . , σn) and a set V of variables such
that V ⊆ VAR(σ), the V projection of σ, denoted σ ⇓ V , is
computed by pointwise projecting every trace function of σ:
σ ⇓ V = (σ0 ⇓ V, . . . , σn ⇓ V )

For a program P , we write σ ⇓ P , to mean σ ⇓ VAR(P ).
Notation ⇓ P also applies to states and trace functions.

The soundness theorem (Theorem 3) has a list of promises: a
program P and Pc (P in canonical form), a set SA of variables,
a set η of variables such that SA ⊆ BV(P ), η ⊆ BV(P ),
and SA ∩ η = ∅. We assume but elide these promises in the
following deﬁnitions and lemmas.

Deﬁnition 20 (Self-composition preserves equivalence for-
mula). The desired property of a self-composition:

eqξ

η → [IC(P, SA, ξ)]eqξ
η

is formalized as follows:

∀σ ∈ τ⊲(IC(P, SA, ξ)) such that
FST (σ ⇓ P ) ∼∼∼ζ FST (σ ⇓ ξ(P )),
σ ⇓ P ∼∼∼ζ σ ⇓ ξ(P )

Where ζ is {(x, ξ(x)) | x ∈ η}.

Assumption 1 (A program has total semantics on low-integrity
inputs (formalized)).
∀ω1, ω2 : STA such that ω1 ≈η ω2,

∃σa ∈ τ⊲(P ) such that FST σa = ω1
↔ ∃σb ∈ τ⊲(ATTACKED(P, SA)) such that FST σb = ω2

Lemma 3 (Renaming preserves trace). For a hybrid program
P and a renaming function ξ on P :

∀σ ∈ τ⊲(P ), ξ(σ) ∈ τ⊲(ξ(P ))

Where ξ(σ) is σ with variables renamed according to ξ.

Proof. By induction on P .

(cid:3)

Lemma 4 (Renaming preserves trace existence).
∀ω1, ω2 : STA such that ω1 ≈η ω2,
∀σ ∈ τ⊲(P ) such that FST σ = ω1,
∃σ′ ∈ τ⊲(ξ(ATTACKED(P, SA))) such that FST σ′ = ξ(ω2)

Proof. By assumption 1, a trace of ATTACKED(P, SA) exists
with starting state ω2. By lemma 3, we know ξ(σ′) is a normal
(cid:3)
trace of ξ(ATTACKED(P, SA)).

Lemma 5. Trace preserves after adding disjoint variable sets.

∀ω1 ω2 : STA such that

VAR(ω1) = VAR(P ) and VAR(ω1) ∩ VAR(ω2) = ∅
∀σ ∈ τ⊲(P ) such that FST σ = ω1,
∃σ′ ∈ τ⊲(P ) such that
FST σ′ = ω1 ⊕ ω2 and σ′ ⇓ P = σ

Where ⊕ means the join of two non-overlapping states.

Proof. By induction on P .

(cid:3)

Lemma 6 (Projection preserves trace). For program P ,
∀σ ∈ τ⊲(P ), σ ⇓ P ∈ τ⊲(P ) and σ ⇓ P ∼∼∼idVAR(P ) σ

Proof. By the deﬁnition of trace semantics and projection. (cid:3)

Lemma 7 (Projection not affected by programs with disjoint
variables). For program P1 and P2 such that BV(P1) ∩
BV(P2) = ∅,

∀σ ∈ τ⊲(P1), FST σ ⇓ P2 = LST σ ⇓ P2

Proof. By induction on P1 and deﬁnition of ⇓.

(cid:3)

Lemma 8 (Composition preserves trace existence). For pro-
gram α = (ctrl; x′ = θ&φ),
∀ω1, ω2 : STA such that ω1 ∼∼∼ζ ω2,
VAR(ω1) = VAR(α), and VAR(ω2) = VAR(ξ(α)),
∀σ ∈ τ⊲(α) such that FST σ = ω1,
∃σ′ ∈ τ⊲(ctrl; ξ(ATTACKED(ctrl, SA)); (x′ = θ, ξ(x′ = θ))

&(φ ∧ ξ(φ))) such that
σ′ ⇓ α ∼∼∼id σa and (FST σ′) ⇓ ξ(α) = ω2

Where ζ is {(x, ξ(x)) | x ∈ η}.

0 ...σa

0 ...σa

then (σa

Proof. Let σa = (σa
m),
m−1) is a trace
of ctrl, and σm : [0, r1]
7→ STA is a trace function for
x′ = θ&φ. According to Assumption 1, there exists σb ∈
τ⊲(ξ(ATTACKED(α, SA))). We can then prove the part of
ctrl; ξ(ATTACKED(ctrl, SA)) by lemma 5, 7 and the deﬁnition
of ∼∼∼. For the plant part, we know (by Assumption 1) low-
integrity values cannot affect evolution constraints, meaning
input states ω1 and ω2 should be able to last
the same
duration of evolution. Thus, for any duration r1 that trace
σa has, the duration of the other trace σb can match it, i.e.,
r1 = r2. Thus there exist a trace function [0, r1]
7→ STA:
x 7→ σ1(x) ⊕ (σ2(x) ⇓ BV(ξ(α))) for the composed dy-
namic (x′ = θ, ξ(x′ = θ))&(φ ∧ ξ(φ)), whose α projection
is indistinguishable from σa. Combined with the result for
(cid:3)
ctrl; ξ(ATTACKED(ctrl, SA)), this lemma is proven.

Lemma 9 (Assigning the same value to connected variables
preserves equivalence).

∀ω1, ω2 : STA such that ω1 ∼∼∼ζ ω2,
∀x : V, d : R such that x ∈ dom(ζ),
ω1[x 7→ d] ∼∼∼ζ ω2[ξ(x) 7→ d]

q, and IC(P, SA, ξ) = α∗
c ,

p, ξ(ATTACKED(P, SA)) = α∗
α∗
∀σ ∈ τ⊲(αc),
∃σa ∈ τ⊲(αp), σb ∈ τ⊲(αq) such that
(σ ⇓ αp) ∼∼∼id σa and (σ ⇓ αq) ∼∼∼id σb

Proof. By Lemma 13 and 14.

(cid:3)

Lemma 16 (Renaming preserve equivalence).
P ∼∼∼ζ ξ(ATTACKED(P, SA)) ↔ P ≈dom(ζ) ATTACKED(P, SA)
Proof. By induction on the variables in program P [24]. (cid:3)

Proof of Theorem 3. By Deﬁnition 20, Lemma 6, 12, 15,
16, and induction on the number of iterations, we get P ≈η
ATTACKED(P, SA). Since H ⊆ η, P ≈H ATTACKED(P, SA)
(cid:3)
(Property 2).

Lemma 10 (Assigning arbitrary values to non-connected
variables preserves equivalence).

∀ω1, ω2 : STA such that ω1 ∼∼∼ζ ω2,
∀x : V, d1, d2 : R such that x 6∈ dom(ζ)
ω1[x 7→ d1] ∼∼∼ζ ω2[ξ(x) 7→ d2]

Lemma 9 and 10 can be proven by the deﬁnition of ∼∼∼ζ.

Lemma 11 (Choice part of composition preserve equivalence).
For program choices that consists of non-deterministic assign-
ments of choice variables,

∀ω1, ω2 : STA such that ω1

∼∼∼ζ ω2,

∀σ ∈ τ⊲(choices) such that FST σ = ω1,
∃σ′ ∈ τ⊲(choices; SUB(choices, ξ)) such that
σ′ ⇓ choices ∼∼∼id σ and
LST σ′ ⇓ choices ∼∼∼ζ LST σ′ ⇓ ξ(choices)

Proof. Let σ = (σ0 . . . σp) be the trace of program choices,
then there exists a trace σb for ξ(choices) with the same
length as σ, i.e., σb = (σb
p). We can then get a trace for
program SUB(choices, ξ) by altering corresponding variables
in the state. Then by lemma 9 and 10 and induction on the
(cid:3)
number of assignments in choices.

0 . . . σb

Lemma 12 (Completeness of a single iteration). Let program
p and IC(P, SA, ξ) = α∗
P = α∗
c ,

∀ω1, ω2 : STA such that ω1 ∼∼∼ζ ω2,
VAR(ω1) = VAR(P ), and VAR(ω2) = VAR(ξ(P )),
∀σ ∈ τ⊲(αp) such that FST σ = ω1,
∃σ′ ∈ τ⊲(αc) such that σ′ ⇓ P ∼∼∼id σ and
FST σ′ ⇓ ξ(P ) = ω2

Proof. By Lemma 8 and 11.

(cid:3)

Lemma 13 (Projections of a sequence).

∀σ ∈ τ⊲(α; β) such that BV(α) ∩ BV(β) = ∅,
∃σa ∈ τ⊲(α), σb ∈ τ⊲(β) such that
σ ⇓ α ∼∼∼id σa and σ ⇓ β ∼∼∼id σb

Proof. By induction on α, β, and deﬁnition of projection. (cid:3)

14

(Soundness of

Lemma
composed
plant). For program α = (ctrl; x′ = θ&φ) and β =
(ξ(ATTACKED(ctrl, SA)); ξ(x′ = θ)&ξ(φ)),

trace

the

for

∀σ ∈ τ⊲(ctrl; ξ(ATTACKED(ctrl, SA));
(x′ = θ, ξ(x′ = θ)&(φ ∧ ξ(φ)))

∃σa ∈ τ⊲(α), σb ∈ τ⊲(β) such that
σ ⇓ α ∼∼∼id σa and σ ⇓ β ∼∼∼id σb

Proof. By deﬁnition of trace semantics and Lemma 13.

(cid:3)

Lemma 15 (Soundness of single iteration). Let program P =

