An Adaptive Multi-Agent Physical Layer Security
Framework for Cognitive Cyber-Physical Systems

Mehmet Özgün Demir, Ozan Alp Topal, Ali Emre Pusane, Guido Dartmann,

Gerd Ascheid, and Güne¸s Karabulut Kurt

1

1
2
0
2

n
a
J

7

]

Y
S
.
s
s
e
e
[

1
v
6
4
4
2
0
.
1
0
1
2
:
v
i
X
r
a

Abstract—Being capable of sensing and behavioral adapta-
tion in line with their changing environments, cognitive cyber-
physical systems (CCPSs) are the new form of applications in
future wireless networks. With the advancement of the machine
learning algorithms, the transmission scheme providing the best
performance can be utilized to sustain a reliable network of
CCPS agents equipped with self-decision mechanisms, where
the interactions between each agent are modeled in terms of
service quality, security, and cost dimensions. In this work,
ﬁrst, we provide network utility as a reliability metric, which
is a weighted sum of the individual utility values of the CCPS
agents. The individual utilities are calculated by mixing the
quality of service (QoS), security, and cost dimensions with the
proportions determined by the individualized user requirements.
By changing the proportions, the CCPS network can be tuned
for different applications of next-generation wireless networks.
Then, we propose a secure transmission policy selection (STPS)
mechanism that maximizes the network utility by using the
Markov-decision process (MDP). In STPS, the CCPS network
jointly selects the best performing physical layer security policy
and the parameters of the selected secure transmission policy to
adapt to the changing environmental effects. The proposed STPS
is realized by reinforcement learning (RL), considering its real-
time decision mechanism where agents can decide automatically
the best utility providing policy in an altering environment.

Index Terms—Cyber-physical systems, physical layer security,

quality of service, risk-aware control, utility.

I. INTRODUCTION

Cyber-physical systems (CPS) can a be seen as special
form of a distributed system with integrated closed loops
for control task among multiple agents, as shown in Fig. 1.
In this paper, we consider a special type of CPS where all
agents communicate their sates and control information over
wireless channels. In this ﬁgure, an agent is deﬁned as an
individual unit that has a control center, multiple sensors,
and actuators. In real-life deployments, a system consists of
multiple agents, that interact with each other, such as trafﬁc
signs on the road and units inside the vehicles in vehicle-
to-everything (V2X), or industrial networks. Typical CCPS
have the ability to locally (edge) analyze the environment
and make decisions to a limited extent. In this article, we

M. Ö. Demir and A. E. Pusane are with the Bo˘gaziçi University, Istanbul,

Turkey, {ozgun.demir1, ali.pusane}@boun.edu.tr.

O. A. Topal and G. Karabulut Kurt are with the Istanbul Technical

University, Istanbul, Turkey, {ozan.topal,gkurt}@itu.edu.tr.

G. Dartmann is with the University of Applied Sciences Trier, Trier,

Germany, g.dartmann@umwelt-campus.de.

G. Ascheid is with the RWTH Aachen University, Aachen, Germany,

ascheid@ice.rwth-aachen.de.

This work is supported by TUBITAK under Grant 115E827.

Fig. 1: System model of a CCPS that consists of multiple
agents in layered structure with possible attacks.

consider agents that utilize reinforcement-learning (RL), where
they can analyze the environment, take action in real time.
An essential aspect of the environment here is the physical
wireless communication channel, which is also essential for
monitoring and controlling the security of the communication.
On the one hand the communication channel is inﬂuenced by
the interaction of the agents and on the other hand the physical
properties of the communication channel also inﬂuence the
actions and behavior of the agents. Hence, the operation of
each agent and the coordination between agents depend on the
wireless communication infrastructure. However, a wireless
network cannot guarantee data security, since radio signals
can be intercepted or manipulated by malicious devices [1],
which may be active or passive attackers, e.g., eavesdroppers,
as shown in Fig. 1.

In many distributed systems, the environment is also chang-
ing continuously, (e.g., additional agents, updated attacker
models, increasing noise, and interference). Therefore, CPS

User / DeviceActuatorsSensorsExpected to have:- Desired Security- Sufﬁcient QoS- Low CostActuatorsControlCenterControlCenterPLS policyUser Commands:- User Preferences- Requirements- Desired Security LevelAgent #1Agent #2SensorsActive / Passive AttackerSensor Cognition ChannelPhysical Interaction betweensensors/actuators and user/deviceAgents, which consist of controlcenter, actuators, and sensors Physical WorldApp.LayerPHYLayerPassive Cyber AttacksActive Cyber AttacksCognitionUser CommandsPLS Policy information channel PLS policy 
 
 
 
 
 
also have to adapt to changing environmental conditions to
provide continuous security as well as the requirements of
the target application. In the near future, it is expected that
CPS will have cognitive abilities to sense the dynamically
changing environments and make decisions based on the
updated conditions [2]. This critical evolution of CPS would
be plausible if the agents of CCPS can adapt their operational
preferences, e.g., transmission technology, transmit power, and
security policy, to simultaneously provide the desired system
performance and security [3].

Numerous security mechanisms are proposed in the physical
layer security (PLS) literature, where the body of work has
unraveled different security opportunities at the physical layer
considering various set-ups and attack scenarios [4]. The
in this body of work is maximizing the
modus operandi
security performance of a single PLS policy by optimizing
the power-frequency-antenna resources under a time invariant
channel and attack model. Considering a CCPS framework
composed of many agents equipped with various resources,
utilizing a single security mechanism with a constant purpose
would create a bottleneck for the security and the reliability of
the CCPS networks. Instead of focusing on a single speciﬁc
security scenario, as given in our previous work [2], we extend
our perspective, and try to answer two large-picture questions
for the secure CCPS framework: "What is the best available
security policy and conﬁguration for the CCPS agents under
sensed channel conditions?" and "Besides security, how can
we measure the physical layer performance of a CCPS net-
work?"

As an answer to the ﬁrst question, we propose a decision
mechanism, where the best performing PLS policy with best
performing transmission conﬁguration is aimed to be selected
from the available set of security policies, as shown in Fig.
2. More speciﬁcally, we deﬁne a control center that is mainly
collecting channel data and deciding the best strategy for the
agents. The units are designed speciﬁcally for sensing, adapt-
ing, and conﬁguration located in the control center, where they
collect information about ambient risks and environmental
changes from sensors, estimating the risks, and communication
resources. They also conﬁgure the physical layer parameters
of the entire system based on agent preferences. Sensors and
actuators are then informed by the control center to update
transmission parameters. Since the various PLS policies are
already available in the literature, the selective nature of the
algorithm provides the adaptability for the CCPS network.

As an answer to the second question, we deﬁne a user-
centric utility as the main performance metric. The utility
is the weighted average of the security, quality of service
(QoS), and cost dimensions of the CCPS performance. These
dimensions include various system performance metrics after
classifying into three main dimensions, as shown in Fig. 2. The
weights of each dimension are determined by the requirements
imposed by users. Since there are distinct operational require-
ments of the main applications of CPS, e.g., drone swarms,
ultra-reliable low latency communication (URRLC), massive
machine type communication (mMTC), smart grid, vehicle-
to-everything (V2X) communication, and health networks [2],
[5], we can tune the weights to perform best under the

2

Fig. 2: Proposed secure transmission policy selection and
designed utility metric with its sub-dimensions for each agent.

sensed channel conditions and available resources. However,
separately selecting the best performing policy for each agent
in the network may not provide the best performing network
results due to the interference resulted from other agents and
the required operational costs. Instead of using individual
utilities for each agent in the network, in this work, we propose
mixing the utilities of each agent with correct proportions to
obtain a single network utility value that symbolizes the total
performance of the CCPS network. To obtain the network
utility, we consider two different operational structures of
CCPS networks. In the ﬁrst structure, the agents calculate their
utilities and select the best performing PLS policy individually
and locally. In the second structure, agents are connected to a
central control unit, where their network utility is calculated
by averaging their individual utilities. Our contributions in
this work can be listed as below.

• We propose an adaptive PLS policy selection and trans-
mission conﬁguration mechanism, which is called secure
transmission policy selection (STPS), for a multi-agent
physical layer security framework using an Markov deci-
sion process (MDP). The proposed mechanism is inher-
ently adaptable, where it may maximize the individual
rewards or a joint reward respectively considering selﬁsh
and cooperating agents.

• The reward of the individual agents is calculated by a
utility function, which is a weighted sum of the QoS,
security, and cost dimensions. By multiplying the util-
ity function with time-dependent characteristics, such as
degrading battery, we propose a more applicable reward

Agent 1Agent 2Agent IUtilitySecurityQuality of ServiceCost...- Secrecy Capacity- Privacy- SINR- Power Consumption- Delay- Bandwidth- ...- ...- ...Network UtilityAgent 1Agent 2Agent IConﬁgurationPolicy 1Policy N...Secure Transmission Policy SelectionTransmissionConﬁg. 1...TransmissionConﬁg. MTransmissionConﬁg. 1...TransmissionConﬁg. M...function considering the CCPS frameworks.

B. Organization

3

• We simulate the proposed mechanism with an RL-based
setup considering different beyond 5G applications and
environment characteristics. Simulation results of the pro-
posed mechanism are compared with the only security-
based and only QoS-based policy selection mechanisms
to demonstrate the trade-offs between QoS, security, and
cost dimensions.

Overall, we provide an adaptive CCPS framework that is
able to make the best STPS for each agent to maximize its
utility, which is deﬁned as the weighted sum of QoS, secu-
rity, and cost dimensions, based on time-dependent changing
environment and application-based requirements.

A. Related Works

The PLS policies and their performance criteria are provided
in [4]. One way to secure the physical layer transmission
is a resource allocation at the transmitter node [6]. Other
PLS policies are obtained by the different signal processing
methods, such as beamforming and precoding [7], [8]. Since
the message is pre-coded in line with the receiver’s channel,
the performance of any other node at a different position would
decrease [9]. Antenna-selection or node-selection based PLS
policies are other types of physical layer defense mechanism
[10]. The joint utilization of different PLS policies is previ-
ously considered in [10], and [11]. In [10], the authors pro-
pose antenna selection with beamforming in MIMO networks.
However, these works consider utilizing the same PLS policies
in changing conditions. As a difference, we consider that a
set of PLS policies are available for the CCPS framework.
Similar to our previous work [2], this methodology can be
categorized under a novel PLS policy selection umbrella in
the PLS literature.

Another difference from the aforementioned works is that
here, we consider multi-agent security, where the total util-
in
ity is aimed to be maximized. Similar to our study,
[12],
layer
the authors propose a game-theoretic physical
security mechanism, where a model-based predictive control
system is utilized to connect the cyber and physical layers
of the autonomous systems. In [13],
the authors consider
non-orthogonal-multiple-access (NOMA) to establish a secure
wiretap model and optimize the user power allocation co-
efﬁcients to maximize the secrecy. In [14], the security of
CCPS agents is established by the optimal power alloca-
tion and power splitting at the secondary transmitter under
secrecy constraints. In [15], the medium access probability
and transmission power of secondary transmitters are jointly
optimized to maximize the security of all network nodes. In
[16], the authors propose a beamforming design for a two-
way cognitive radio (CR) Internet of Things (IoT) network
aided with the simultaneous wireless information and power
transfer (SWIPT). While these works consider the performance
of a single method under stable channel characteristics, our
work provides an adaptable joint power and PLS policy
selection in changing environmental conditions, addressing the
corresponding gap in the literature.

In the following section, we detail the calculation of utility
dimensions from the physical layer parameters. In Section III,
we provide the MDP-based STPS mechanism and the pro-
posed utility function. In Section IV, we present the simulation
parameters and results of the proposed mechanism. In Section
V, the paper is concluded.

II. CALCULATING THE DIMENSIONS OF UTILITY

i , R2

i , P 2

i , . . . P Ni

i , . . . , RMi

In our system model, we assume that the CCPS network
consists of I agents, where i denotes the index of an agent.
Due to different requirements and environmental conditions,
each agent may have different sets of available PLS policies
and transmission conﬁgurations, which are denoted as Pi and
Ri for the ith agent, respectively. These sets can be stated as
i } and Ri = {R1
Pi = {P 1
i }, where
Ni is the number of available PLS policies and Mi is the num-
ber of available transmission conﬁgurations for the ith agent.
When we consider the transmission time slot of a frame, t is
the time slot index, where t = 1, 2, . . . T . Due to the impact of
increasing time slots, we have to distinguish which STPS is de-
cided at time slot t from the sets of Pi and Ri for each agent,
where i = 1, 2, . . . , I. Under these circumstances, K and L
are the sets of chosen PLS policies and transmission conﬁgu-
rations by agents at t, where K = {P k1
I [t]}
and L = {Rl1
I [t]}. In these statements, ki
and li are indices of the chosen PLS policies and transmission
conﬁgurations by each agent and these terms should satisfy
following conditions

2 [t], . . . , P kI

2 [t], . . . , RlI

1 [t], P k2

1 [t], Rl2

ki ≤ Ni,

li ≤ Mi,

∀i ∈ {1, 2, . . . I}.

(1)

In Fig. 3, the block diagram of the proposed system model
is given. In this paper, we assume that the agents are in-
dependent and each of the agents consists of a transmitter
and a receiver unit. We will denote the transmitter and the
receiver of the ith agent respectively as ai and bi. The
number of antennas of the transmitter node of the ith agent
is denoted as Wi. We assume that each receiver node is
equipped with a single antenna. The eavesdropper is also
assumed to be equipped with a single antenna. The channel
fading coefﬁcient vector between the nodes ci and dj
is
expressed by hcidj = (cid:2)hcidj (1), hcidj (2), · · · , hcidj (Wi)(cid:3)T
,
), where c ∈ {a, b}, d ∈ {a, b},
and hcidj (ω) ∼ CN (0,
i, j ∈ {1, 2, . . . , I} and i (cid:54)= j. pT denotes the transpose of
the vector p. We consider three different PLS methodologies:
subcarrier-based artiﬁcial noise (SC-AN), full-duplex artiﬁcial
interference (FD-AI), and artiﬁcial noise (AN) with supported
beamforming. We also consider the case of only beamforming
(B), hence the agents may utilize four different PLS policies.
The signal models for each of the PLS policies are detailed
in the link1. For fairness, we consider that
the message
transmission power, Rs, from all agents are equal. The signal

1
Dcidj

1The operations to obtain dimensions and utility

is detailed in https://github.com/ozanalpt/
PLS-toolbox-for-CCPS/The_Guidebook_of_the_PLS_
toolbox_for_CCPS.pdf

4

Fig. 3: The block diagram of the utility calculation with the proposed MDP-based STPS.

to interference and noise ratio (SINR) at node bi in the tth
time slot is expressed by

SINRb

i [t] =

i hai,bi|2Rs
|vH
γki
i (Υi[t] + N0i)

,

(2)

where Υi denotes the interference from other agents, N0i
denotes the noise variance at the receiver node of the ithagent,
and γki
i denotes the interference cancellation error coefﬁcient
of the selected PLS policy. The transmit beamforming vector
for the ith agent is denoted by vi = hai,bi. The interference
from other agents can be divided into parts as

Υi[t] =

(cid:88)

(Υs

ji[t] + Υc

ji[t]),

(3)

j(cid:54)=i

where Υc
i denotes the interference resulted from the message
signal transmitted by other agents, and Υa
i denotes the in-
terference resulted from the security signal by other agents.
The interference from the message signal transmission can be
expressed by

Υs

ji[t] = |vH

j haj bi |2Rs.

(4)

The security interference results from the security signal trans-
mitted from other agents, where Υc
ji[t] denotes the interference
from the security signal of jth agent. This term differs based
on the selected PLS policy. Similarly, considering the SC-AN
policy, Υc

ji[t] becomes

Υc,SC−AN

ji

[t] = |haj bi |2Rlj

j [t],

(5)

where Rlj
of the jth agent. Note that, γki
For other policies, γki
becomes

j [t] denotes the selected transmission conﬁguration
i = 2 for the SC-AN policy.
ji[t]

i = 1. Considering FD-AI policy, Υc

Υc,F D−AI

ji

[t] = |hbj bi |2Rlj

j [t].

Considering the AN policy, Υc

ji[t] becomes

Υc,AN
ji

[t] = |αaj bj · haj bi|2Rlj

j [t],

(6)

(7)

where αaj bj =
of the vector ζ. Considering the only beamforming policy,

and N S(ζ) denotes the null-space

N S(haj bj )
||haj bj ||

Υc,B
ji

[t] = 0.

(8)

Similarly, SINR for the link from the transmitter of the ith
agent to the eavesdropper can be expressed by

SINRe

i [t] =

i haie|2Rs
|vH
(Υie[t] + N0e)

.

In this case, Υie[t] becomes

Υie[t] =

Υs

je[t] +

(cid:88)

j(cid:54)=i

I
(cid:88)

i=1

Υc

ie[t],

where

Υs

je[t] = |vjhaj e|2Rs.

(9)

(10)

(11)

In addition to the interference resulting from the security signal
of other agents, the security signal from the considered agent
also decreases the performance of the eavesdropper.

In the following, we detail how physical layer parameters
are converted into the dimensions of the individual utility
function.

A. Security

As a security metric, we utilize secrecy pressure as proposed
in [17]. The main difference between other secrecy metrics
is that the location of the eavesdropper is assumed to be
unknown. In this way, secrecy pressure shows the security
level of the considered medium, instead of focusing on the
exact security levels. As a ﬁrst step,
the ergodic secrecy
capacity of each point (x, y) of the surface S is obtained. This
step provides us with a secrecy map of the surface S for the
corresponding security policy. After that, expectation operation
is applied over the surface S. The location of the eavesdropper
is assumed to be a 2D Gaussian random variable as the surface
might contain different physical measures in different areas.
For example, considering a factory environment, the mean of

Agent 1Agent 2Agent I. . .Calculationof the UtilityDimensionsUser / Applicationsw[t]TPLS Policy N1xN2x...xNITransmissionConﬁguration M1xM2x...xMIPLS Policy 1PLS Policy 2Transmission Conﬁg. 1Transmission Conﬁg. 2Current State(tth Time Slot)Secure Transmission Policy Selection         ...         ...SINR1 ,W1SINR2 ,W2SINRI ,WIs1[t], q1[t], c1[t]s2[t], q2[t], c2[t]sI	[t], qI	[t], cI [t]Updating the utility dimension based on the selectionsfor the next state k1,l1,k2,l2...,kI,lI (t+1)th Time SlotU1[t]U2[t]UI	[t]γ[t]TUpdating weights H, BSensoryInformationthe Gaussian distribution may be assumed to be the blind spot
of the security cameras. The position of the eavesdropper is
unknown; therefore, it will be denoted by (x, y). The secrecy
capacity for the ith agent can be represented by

C i

sec(x, y) = max{0, (C i

B − C i

E(x, y)),

(12)

B and C i

where C i
E(x, y) are respectively the channel capacities
of the link of the ith agent’s transmitter and receiver, and the
link of ith agent’s transmitter and eavesdropper. The capacity
of the ith agent’s channel can be given as

C i

B =

(cid:16)

log

1
2

1 + SINRb
i

(cid:17)

,

(13)

and eavesdropper’s channel capacity is

C i

E(x, y) =

1
2

log (cid:0)1 + SINRi

e(x, y)(cid:1) .

(14)

Note that the capacity of a generic (x, y) point is given since
the location of the eavesdropper is unknown. Since the secrecy
capacity depends on the mutually independent, independent
and identically distributed (i.i.d.) channel fading coefﬁcients,
the ergodic secrecy capacity can be given as
sec(x, y)(cid:3) ,

sec(x, y) = E (cid:2)C i
˜C i

(15)

where E{·} denotes the expectation operator. Note that the
ergodic secrecy capacity is also deﬁned for a generic (x, y)
point. Calculating ergodic secrecy capacity for each point on
the surface S would result in calculating the secrecy map of
the surface.

Considering this perspective, we can weigh the ergodic se-
crecy capacity for each (x, y) generic point by the probability
of the eavesdropper’s existence at that point. Then, the ergodic
secrecy pressure of the surface S of the time slot t can be given
by

Ωi[t] =

γ(x, y) ˜Csec(x, y)dxdy,

(16)

S

where γ(x, y) is the probability density function (pdf) of the
presence of the eavesdropper at a point (x, y) on the surface.
In this case, the security of ith agent can be obtained by

si[t] =

Ωi[t]
maxj∈{1,2,...,I}[Ωj[t]]

.

(17)

B. Quality of Service

QoS indicates the general performance of the communi-
cation link. In the considered setup, we consider the SINR
levels at the receiver nodes of each agent as their QoS metric.
As described in the security calculation,
the SINR levels
with respect to each policy and transmission conﬁguration are
calculated. Then, they are normalized by the maximum SINR
level of the determined STPS. Hence, QoS of the ith agent can
be described by

qi[t] =

SINRb
maxj∈{1,2,...,I}[SINRb

i [t]

j[t]]

.

(18)

(cid:90) (cid:90)

5

C. Cost

Cost is a measure of the limited resources considering the
CCPS framework. The number of active antennas, the selected
level of the transmission power, the selected coding scheme
are the main elements determining the battery life and the used
memory of the CCPS agents. In this work, we consider the
cost of the ith agent as a weighted sum of the utilized number
of antennas and selected transmit power conﬁgurations as

ci[t] =

Wi[t] + Rli
maxj∈{1,2,...,I}[Wj[t] + Rlj

i [t]

j [t]]

.

(19)

As described in the previous section, the weighted sum of
security, QoS, and cost dimensions determine the utility of the
agent. The processes explained in this part are conceptualized
in [2]. Even though this methodology effectively illustrates the
general performance of a single-agent, it becomes inefﬁcient
for the multi-agent systems, where limited resources, such as
frequency bandwidth, battery life, are shared by all agents.
Therefore, in the following section, ﬁrst we obtain a novel
performance metric, named as network utility, which models
the performance of the CCPS framework considering the
joint utilization of the shared resources. Then, we detail
the proposed STPS approach for the considered multi-agent
framework.

III. ADAPTIVE MULTI-LAYER SECURITY FRAMEWORK
FOR MULTI-AGENT SYSTEMS

A. Network Utility Calculation

During the deployment of an adaptive CCPS framework
for multi-agent systems, a proper utility calculation should be
done before each transmission, as shown in Fig. 3. Deﬁning
utility is not a straightforward problem; however, it can be
calculated as a weighted sum of security, QoS, and cost
terms. The calculation of these terms is highly related to the
interactions of the agents inside the network, PLS policies,
and a proper performance metric. The details about these
calculations will be explained in detail in the next section.
In short, there are normalized values of security, QoS, and
cost, which are denoted as si[t], qi[t], ci[t].

The importance of these dimensions is fundamentally based
on the chosen application of CCPS. However, the roles of the
agent may also be distinct from one another; therefore, the
weights of the dimensions for each agent may signiﬁcantly
vary. Another factor on the weights is the impact of the active
communication time on the weights of each dimension. Due
to the rise of power consumption and the reduction of QoS
levels of the agents, we try to avoid retransmissions for an
increasing number of successful transmission of packets in a
frame. To deploy this scheme, we slightly adjust the weights
by increasing the weights of QoS and cost dimensions while
decreasing the weight of the security dimension for after each
successful communication time slot. In these circumstances,
we can state a vector of the weights with the impacts of the
increasing number of time slots as
wi[t] = (cid:2)γs[t]γq[t]γc[t](cid:3)T

◦ (cid:2)wi,s[t]wi,q[t]wi,q[t](cid:3)T

(20)

,

6

Fig. 4: MDP-based PLS policy selection and transmission conﬁguration schemes: (a) individual decision, (b) joint decision

where wi,s[t], wi,q[t], and wi,c[t] are the security, QoS, and
cost-related weights at t for the ith agent, and ◦ is the
Hadamard product operator. These weights should satisfy
wi,s[t] + wi,q[t] + wi,c[t] = 1 for ∀i ≤ I and ∀t ≤ T . In
(20), γs[t], γq[t], and γc[t] are the impacts of the increasing
number of time slots during the transmission of a frame.

We also assume that there are negative effects on system
performance when a different PLS policy is chosen in con-
secutive time slots. In case of different policies are selected
sequentially, the agents may have to turn on/off some of their
hardware. This procedure may result in additional delays, and
cost; therefore, we also consider these impacts by deﬁning
transition values of PLS policies from the policy ki to k(cid:48)
i,
which denotes the next possible values of ki. The transition
value is chosen as δi[t]ki→k(cid:48)
i = 1, where ki = k(cid:48)
i, on the other
hand the condition δi[t]ki→k(cid:48)
i < 1 is satisﬁed, where ki (cid:54)= k(cid:48)
i.
A similar formula can be generated for the transmission
conﬁguration adjustments of the agents, but we omit these
impacts of transmission conﬁguration adjustments to simplify
the model.

When we combine dimensions of the utility, weights of each
dimension, the impacts of increasing time slots, and transitions
to different PLS policies, we can express utility as

U ki,li
i

[t] = δi[t]ki→k(cid:48)
i ·

(cid:32)

wi[t]T ×









si[t]
qi[t]
(1 − ci[t])

(cid:33)
,

(21)

In this utility expression, the cost related term is substracted,
since increased cost leads to a smaller utility. The expression
(21) stands at the center of the CCPS framework, and it will
commonly be utilized in the rest of the paper. It should also
be remembered that the values of utilities are between 0 and 1
due to the normalization of the weights and utility dimensions.

B. MDP-based Secure Transmission Policy Selection

In order to deploy a ﬂexible decision mechanism, an MDP-
based Q-learning algorithm is developed, as shown in Fig.
3. This algorithm is practiced inside control centers of each
agent at each time slot t. In the designed Q-learning algorithm,

the states are the available PLS policies and transmission
conﬁgurations, as shown inside the block of STPS in Fig
3. In the same ﬁgure,
the current state indicates the last
active PLS policy from the time slot t. The actions between
states are considered as the changing or continuing the current
conﬁgurations for each transmission timeslot based on the
calculated reward values with the aid of utility values. It should
be noted that this model is valid for the individual selections of
the agents, and should be updated by combining agent-based
procedures for joint policy and transmission conﬁguration
selection of agents.

As we discussed earlier,

the number of available PLS
policies is Ni and the number of available transmission con-
ﬁgurations is Mi for the ith agent. Therefore, the number of
states is Ni + Mi + 1 after adding the current operational state
during the individual operations. It also results in Ni available
actions for the ﬁrst stage of the algorithm and Mi available
actions for the second stage for individual decisions. In case of
joint decisions by the agents, the number of available actions
on the ﬁrst stage is N1 × N2 × · · · × NI , while the number of
available actions for the second stage is M1 × M2 × · · · × MI .
With this procedure, an agent decide a STPS based on
the calculated utility values by (21) for each possible policy
and transmission conﬁgurations. It also indicates the initial
state of the operation at the beginning. In this procedure,
there is a reward for each transition, which is calculated with
the help of utility values. At the policy selection stage, the
rewards are speciﬁc values of the calculated utilities with a
determined transmission conﬁguration li at time slot t, i.e.,
U 1,li
[t]. The rewards through the second
i
selection stage are calculated using the utility differences
of the possible transmission conﬁgurations, and the chosen
the policy stage, li. The re-
transmission conﬁguration at
wards from the state of the ﬁrst PLS policy to the possible
transmission conﬁgurations of this policy can be written as
U 1,1
[t].
i
i
These values can be positive or negative depending on the
values of the utility values and the calculation of its sub-
dimensions, which is given in the next section.

[t], . . . U Ni,li

[t], . . . U 1,Mi

[t] − U 1,li

[t] − U 1,li

[t] − U 1,li

[t], U 2,li
i

[t], U 1,2
i

i

i

i

i

If we update the MDP for a joint selection of PLS policies,

LastStateSC-AN,SC-ANSC-AN,FD-AISC-AN,BSC-AN,ANFD-AI,SC-ANFD-AI,ANSC-AN,ANB,ANAN,SC-ANAN,AN..................(5dB,5dB),(5dB,5dB)(5dB,5dB),(5dB,10dB)(5dB,5dB),(10dB,5B)(5dB,5dB),(10dB,10dB)(5dB,10dB),(5dB,5dB)(5dB,10dB),(10dB,10dB)(10dB,5dB),(5dB,5dB)(10dB,5dB),(10dB,10dB)(10dB,10dB),(5dB,5dB)(10dB,10dB),(10dB,10dB)ANBFD-AISC-AN10dB10dB10dB5dB5dB10dB5dB5dB--------LastState(a)(b)TABLE I: RL-based simulation parameters and the decision
methodologies for actualizing a STPS

Decision methodology
Individual
Joint

Time slot of the agents
t − 1
Simultaneously

Simulation Parameters

Number of agents, I

Discount rate

Learning rate

(cid:15)

Number of Training Episodes

Number of time slots, T
ki→k(cid:48)
i
i

δ

2
0.75 (for individual decisions)
0.85 (for joint decisions)
0.02 (for individual decisions)
0.035 (for joint decisions)
0.95 (for individual decisions)
0.7 (for joint decisions)
200 (for individual decisions)
300 (for joint decisions)
50
0.95 for ki (cid:54)= k(cid:48)
i
1 for ki = k(cid:48)
i

the available number of policies and transmission conﬁgura-
tions drastically increase. Cooperating agents do not have to
operate with the same STPS. In some cases, they should not
run with the same STPS, since some of the policies are very
disadvantageous and costly if multiple agents operate with
them. As a result, we have to extend the MDP procedure
scheme for the joint operation. In this case, the available
policies at the policy stage will be N1 × N2 × · · · × NI . In
this setup, each state represents PLS policies of each agent,
e.g., a stage indicates two possible PLS policies if we have
a joint decision model for two agents. Similarly, the number
of available conﬁgurations at the transmission conﬁguration
selection stage is M1 × M2 × · · · × MI , in the case, where I
agents jointly make their STPS. In the following section, we
utilize RL-based simulations to implement MDP architecture
with network utility.

IV. REINFORCEMENT LEARNING SIMULATIONS

In order to analyze the performance of the CCPS security
framework for multi-agent systems, an RL-based operation
is studied and simulated in MATLAB environment. RL can
be easily applied for cyclic system models, including CCPS,
thanks to its similar design.

In the given simulations, we study I = 2 agents, which
are able to operate individually or jointly. In the scope of this
paper, the considered PLS policies can be listed as P1 = P2 =
{SC − AN, F D − AI, AN, B} for N1 = N2 = 4. The deter-
mined tuples of transmission conﬁgurations for each agent can
be written as R1 = R2 = {(5, 5), (5, 10), (10, 5), (10, 10)}
dB for M1 = M2 = 4, where (·, ·) indicates the transmission
power and artiﬁcial noise levels of an agent, respectively. A
rectangular surface of 4m × 4m is assumed as the CCPS en-
vironment. The locations of the transmitter and receiver nodes
for the ﬁrst agent are respectively selected as (−1, 1), (1, 1),
while the transmitter and receiver nodes of the second agent
are placed at (−1, −1), (1, −1), respectively. The eavesdrop-
per is located at the center, as (xe, ye) = (0, 0). The pdf of
the eavesdropper’s location is 2D Gaussian distribution with
variance 1. The noise variances at the eavesdropper and agents
are assumed to be equal to 1.

7

(a) STPS of the agents.

(b) Agent-based utility values and average weighted sum values.

Fig. 5: Simulation results observed with individual and joint
decision methodologies for chosen CCPS applications.

In Table I, we present the RL-based training and simulation
parameters, where the future values are taken into account
by choosing a discount rate as one while maximizing utility
during training RL environment. T , which is the number of
time slots that affect the behavior of the agents due to changing
weights of the utility dimensions, is chosen to be 50. There
are two possible (cid:15) values for the designed methodologies of
STPS. These values are chosen for maximizing utilities during
the operations.

In this paper, we consider four main applications of CCPS,
whose weights are given in Table II, and also presented in
[2]. As detailed in the previous section, the utility calculation
consists of several parameters, including the vector of weights
for each dimension wi[t], the transition vector dki
i , impacts of
an increasing number of time slots on each dimension denoted
as γs[t], γq[t], and γc[t], respectively. These weights are arbi-
trarily chosen and can be altered based on usage differences.
As the second parameter, each transition from one PLS policy
to another creates a utility loss with the 0.05 ratio; therefore,
δki→k(cid:48)
= 0.95 when ki (cid:54)= k(cid:48)
i as given in Table I.
1
This value can also be updated based on various operational
environments, which may lead to additional penalties, e.g.,
delays and distortion for changing PLS policies.

= δki→k(cid:48)

2

i

i

In the simulations, the considered decision methodologies
are based on the individual and joint operations of the agents
with different Q-learning tables and MDP-based schemes, as
shown in Fig. 4. When the agents individually determine their
operational PLS policy and transmission conﬁguration, they

1020304050SC-ANFD-AIBANC1Policies1020304050(5,5)(5,10)(10,5)(10,10)(PS, PN) [dB]1020304050SC-ANFD-AIBANPolicies1020304050(5,5)(5,10)(10,5)(10,10)(PS, PN) [dB]1020304050SC-ANFD-AIBANC21020304050(5,5)(5,10)(10,5)(10,10)1020304050SC-ANFD-AIBAN1020304050(5,5)(5,10)(10,5)(10,10)1020304050SC-ANFD-AIBANC31020304050(5,5)(5,10)(10,5)(10,10)1020304050SC-ANFD-AIBAN1020304050(5,5)(5,10)(10,5)(10,10)1020304050TimeSlotsSC-ANFD-AIBANC4Individiual DecisionJoint Decision1020304050TimeSlots(5,5)(5,10)(10,5)(10,10)Individiual DecisionJoint Decision1020304050TimeSlotsSC-ANFD-AIBANIndividiual DecisionJoint Decision1020304050TimeSlots(5,5)(5,10)(10,5)(10,10)Individiual DecisionJoint DecisionAGENT 1AGENT 210203040500.40.6C1Agent 1 Utility10203040500.40.6Agent 2 Utility10203040500.40.50.6Average Weighted Sum10203040500.40.60.8C210203040500.40.610203040500.40.610203040500.40.50.6C310203040500.40.610203040500.40.50.61020304050TimeSlots0.40.50.6C4Individiual DecisionJoint Decision1020304050TimeSlots0.50.60.7Individiual DecisionJoint Decision1020304050TimeSlots0.40.50.6Individual DecisionJoint DecisionTABLE II: Utility weights for chosen CCPS applications for
making a STPS.

Agent

1
2
1
2
1
2
1
2

C1

C2

C3

C4

Utility Weights
wi,q[1]
0.3
0.33
0.5
0.4
0.3
0.1
0.2
0.2

wi,s[1]
0.4
0.33
0.3
0.4
0.2
0.5
0.3
0.2

wi,c[1]
0.3
0.33
0.2
0.2
0.5
0.4
0.5
0.6

Applications

Drone Swarms

URLLC, V2X

mMTC, Smart Grid

Health Networks

TABLE III: Q-table that is logged for the weights of the
scenario C1 for the second agent when t = 50. The actions
are indicated based on the transitions on the Fig. 4(a), where
the condition of Ni = Mi = 4 results in 4 available actions.

Left

States (From)
Last State

SC-AN
0.3783

Actions
Mid-Left Mid-Right
States (To)
B
0.5373

FD-AI
0.4145

Right

AN
0.4336

States (To)

States (From)
SC-AN
FD-AI
B
AN

(5,5) dB
0
0
0
0

(5,10) dB
-0.1648
-0.2121
0.0410
-0.1626

(10,5) dB
0.0011
0.0133
0.0608
0.0586

(10,10) dB
-0.2011
-0.1780
0.0523
-0.0997

make this decision based on other agent’s STPS, as shown in
Table I. On the other hand, the agents simultaneously decide
their conﬁgurations if they choose joint decision methodology
at the same time.

According to the ﬁrst decision methodology, each agent
makes its decisions that maximize their utility without con-
sidering the performance of other agents. The representation
of the individual decision methodology given in Fig. 4a for the
both of the agents, where i = 1, 2. In this ﬁgure, the rewards
of each path are indicated in terms of utility values based
on available PLS policies and transmission conﬁgurations.
the policy selection stage, (5, 5) dB is chosen as the
At
reference transmission conﬁguration, where li = 1. Therefore,
the rewards of the transmission conﬁguration selection stage
are calculated as the difference between the utility of (5, 5)
dB transmission conﬁguration and the utility of an available
transmission conﬁguration for a selected PLS policy. In this
methodology, the rewards, which are calculated with utilities
as expressed in (21), are logged in Q-tables, which show the
action based rewards, and utilized in RL-based simulations.
An examplary Q-table is given in Table III for the weights
of C1 at the end of the assigned frame time, t = 50. These
values signiﬁcantly match with the calculated rewards based
on the MDP-based schemes given in Fig. 4.

In the second scenario, we assume that the agents are able to
make joint decisions that maximize the overall system utility.
Since the agents do not have to operate at the same STPS,
the available number of the PLS policies and the transmission
conﬁgurations are N1 × N2, and M1 × M2, respectively. In
these circumstances, the MDP-based joint decision methodol-
ogy is presented in Fig. 4b with 16 available PLS policies and
transmission conﬁgurations.

8

V. RESULTS AND DISCUSSION

A. Simulation Results

In this section, we present the results of simulations con-
sidering different CCPS applications determining agent be-
haviours and capabilities detailed in the previous section. The
results are illustrated in Fig. 5, Table IV, and Table V, whereas
each presentation is applied for the chosen weights given in
Table II due to the existence of multiple utility weights with
respect to several applications.

In Fig. 5a,

the behavior of two agents is shown for
an increasing number of time slots based on two decision
methodologies, while the operating STPS are given separately.
In these subﬁgures, readers can follow the changes of each
agent, where they make dyna mic decisions for changing
environments. These results show that
the agents tend to
choose beamforming most of the time to satisfy the operational
requirements with individual decision methodology. On the
other hand, there is a cooperation between agents with a joint
decision methodology, since at least one of the agents chooses
(5, 5) dB as transmission conﬁguration in most of the cases.
The impacts on the performance of these selections in terms
of joint utility values are shown in Fig. 5b for each agent
with a comparison of the average weighted sum of agent-based
utility values. The results of the weighted sum of utility values
show that joint decision methodology provides higher utility
values than individual decision methodology most of the time.
This observation is expected since the agents operate more
adequate PLS policies and transmission conﬁgurations with
a joint decision methodology rather than individual decisions.
From our point of view, the amount of ﬂuctuation in individual
decision-based results is considerably high compared to joint
decision-based outcomes. This situation is observed due to
the lack of cooperation between agents. Without cooperation,
agents individually try to obtain their maximum utility values,
but the selection of one agent may not satisfy the utility
maximization conditions for the other agent. This situation is
especially valid for health networks and mMTC applications,
as shown in the same ﬁgure. On the other hand, if there is a
cooperation, the average weighted sum values are more stable.
To observe the secure transmission performance in terms
of security, QoS and cost dimensions individually, we focus
on Table IV and Table V. In these tables, the average relative
indicator results are provided considering the individual and
joint decision methodologies for each application and PLS
policies. The average relative indicator is identiﬁed as the av-
erage percentage-based relative differences between the STPS
for T and each PLS policies that operate with the transmission
conﬁguration (10, 10) dB. In other words, these tables show
the relative differences for utility and its dimensions when
the proposed methodologies are applied instead of operating
with a single PLS policy at maximum transmit power levels.
Since these tables present dimension-based relative indicators,
weights of each dimension given in Table I should also be
considered. In Table IV, readers may observe that the average
relative indicator values are consistent with the assigned
weights for both of the agents due to the independent and
selﬁsh decisions of the agents to satisfy their requirements.

9

TABLE IV: Average Relative Indicator based on individual decision based policy and transmission conﬁguration selection
mechanism with (cid:15) = 0.95.

Drone Swarms
Average relative indicator

Utility
241%
150%
0%
105%

Sec.
56%
16%
1%
4%

QoS
164%
93%
0%
56%

Cost
49%
49%
99%
49%

URLLC
Average relative indicator

Utility
188%
113%
3%
71%

Sec.
73%
29%
9%
6%

QoS
161%
91%
1%
54%

Cost
61%
61%
122%
61%

mMTC
Average relative indicator

Utility
418%
280%
7%
210%

Sec.
22%
8%
22%
24%

QoS
132%
69%
12%
37%

Cost
34%
34%
69%
34%

Health Networks
Average relative indicator

Utility
478%
325%
13%
246%

Sec.
22%
22%
35%
37%

QoS
54%
12%
42%
10%

Cost
23%
23%
46%
23%

SCAN
FD-AI
B
AN

AGENT 1

(a)

(b)

(c)

(d)

Average relative indicator

Average relative indicator

Average relative indicator

Average relative indicator

SCAN
FD-AI
B
AN

Utility
142%
78%
4%
45%

Sec.
58%
16%
1%
4%

QoS
152%
80%
5%
46%

Cost
49%
49%
99%
49%

Utility
104%
49%
19%
22%

Sec.
54%
14%
3%
5%

QoS
107%
49%
21%
20%

Cost
63%
62%
124%
62%

Utility
293%
34%
8%
134%

Sec.
33%
0%
15%
18%

QoS
108%
49%
21%
20%

Cost
37%
37%
74%
37%

Utility
263%
167%
9%
119%

Sec.
18%
11%
25%
27%

QoS
105%
47%
22%
18%

Cost
30%
30%
59%
30%

AGENT 2

(e)

(f)

(g)

(h)

TABLE V: Average Relative Indicator based on joint decision based policy and transmission conﬁguration selection mechanism
with (cid:15) = 0.7.

Drone Swarms
Average relative indicator

Utility
266%
169%
6%
119%

Sec.
39%
3%
12%
14%

QoS
180%
105%
6%
66%

Cost
41%
41%
82%
41%

URLLC
Average relative indicator

Utility
240%
151%
14%
105%

Sec.
56%
16%
2%
4%

QoS
220%
134%
21%
89%

Cost
49%
49%
98%
49%

mMTC
Average relative indicator

Utility
419%
277%
7%
207%

Sec.
7%
31%
41%
43%

QoS
19%
13%
55%
29%

Cost
16%
16%
32%
16%

Health Networks
Average relative indicator

Utility
487%
331%
14%
252%

Sec.
7%
31%
41%
43%

QoS
18%
213%
55%
29%

Cost
16%
16%
32%
16%

SCAN
FD-AI
B
AN

AGENT 1

(a)

(b)

(c)

(d)

Average relative indicator

Average relative indicator

Average relative indicator

Average relative indicator

Utility
226%
136%
9%
92%

Sec.
15%
14%
27%
29%

QoS
83%
32%
30%
7%

Cost
30%
30%
60%
30%

Utility
116%
57%
25%
27%

Sec.
13%
16%
28%
30%

QoS
54%
10%
42%
10%

Cost
30%
30%
59%
29%

Utility
425%
283%
8%
213%

Sec.
8%
32%
42%
43%

QoS
13%
20%
57%
35%

Cost
16%
16%
32%
16%

Utility
608%
413%
15%
317%

Sec.
8%
8%
41%
44%

QoS
13%
12%
57%
35%

Cost
16%
16%
32%
16%

SCAN
FD-AI
B
AN

AGENT 2

(e)

(f)

(g)

(h)

On the other hand, the decisions are delivered to maximize the
system utility with the joint decision methodology. Therefore,
the average relative indicator values of utility in Table V
are signiﬁcantly higher than the same group of outcomes in
Table IV. These observations are expected due to the lack
of dimension-based constraints while maximizing the utility
value. This fact may signiﬁcantly degrade system performance,
and its importance will be discussed in the next section.

B. Discussion and Open Issues

The lack of dimension-based constraints is the reason be-
hind the inconsistencies between observed high utility values
due to the insufﬁcient dimension-based gains. As a result, the
maximized utility may not be sufﬁcient to satisfy the speciﬁc
the most
requirements of the agents. An example is that
important dimension of the agents is the cost with weights
of 0.5 and 0.6, respectively in C4. The results show that
the joint decision methodology provides much higher utility
than individual decision methodology for this agent; however,
individual decision methodology satisﬁes the cost requirement
far better than joint decision methodology. Naturally, this agent
should individually operate, while the second agent tries to
cooperate with the ﬁrst agent due to a high amount of utility
without any dominant weight for each dimension. Here, a

discussion arises on when the average weighted sum of utility
values and the average relative indicator should be utilized
for deciding on real system deployments. In general, we can
state that the agents should jointly decide if their weights are
close and there is no dominant weight in Table II, e.g., drone
swarms, or there is the dominant QoS weight for both of
the agents, e.g., URLLC and V2X applications. On the other
hand, the agents should individually operate to maximize their
dimension-based requirements, e.g., the agents in mMTC and
health network applications. Similar observations can also be
made for other applications of CCPS.

One predictable open issue is the implementation of the
proposed system in a real-time testbed. Since the provided al-
gorithm can be implemented over the software deﬁned radios,
considering real-time problems such as hardware impairments
and channel estimation errors are needed to be considered for
a real-time application.

Another important open issue is that CCPS may consist
of several nodes with various priorities. For example, in a
health network, the security and availability of an implant
sensor would be more important than the data access point.
Therefore, there should be a hierarchical order for the agents.
We believe that our formulation is sufﬁciently ﬂexible to deﬁne
a priority-based deﬁnition, which can be extended by adding

10

a multiplication vector to (21). After this update, we can
also model the relationship between the agents based on their
importance. For instance, some nodes may sacriﬁce from their
security to enable high QoS of a more important agent.

VI. CONCLUSION

In this work, we have proposed a secure transmission
policy selection scheme at physical layer based on the Markov
decision process for multi-agents in a CCPS environment.
The reward of the proposed scheme is termed as the utility,
which consists of security, QoS, and cost dimensions. The
individual utilities of the agents are consubstantiated to form
the joint utility, illustrating the overall performance of the
CCPS framework. Reinforcement learning-based simulations
are conducted considering two different agent behaviors: the
individual policy and transmission conﬁguration selection
scheme, and the joint policy and transmission conﬁguration
selection scheme. By tuning the weights of the utility dimen-
sions, we analyze the performance of the proposed schemes
in different beyond-5G applications.

The superior utility performance of the proposed policy
selection schemes indicates that a single physical layer security
method cannot cover all requirements imposed by different
applications. As future work, prioritization based hierarchical
orders of the agents can be studied. To increase the perfor-
mance of the adaptivity of the framework, dimension-based
constraints should be added to the physical layer security
policy and transmission conﬁguration selections.

REFERENCES

[1] R. Khan, P. Kumar, D. N. K. Jayakody, and M. Liyanage, “A survey
on security and privacy of 5G technologies: Potential solutions, recent
advancements, and future directions,” IEEE Commun. Surveys Tuts.,
vol. 22, no. 1, pp. 196–248, 2020.

[2] O. A. Topal, M. O. Demir, Z. Liang, A. E. Pusane, G. Dartmann,
G. Ascheid, and G. K. Kurt, “A physical layer security framework for
cognitive cyber physical systems,” IEEE Wireless Commun., vol. 27,
no. 4, pp. 32–39, 2020.

[3] N. Wang, P. Wang, A. Alipour-Fanid, L. Jiao, and K. Zeng, “Physical-
layer security of 5G wireless networks for IoT: Challenges and oppor-
tunities,” IEEE Internet Things J., vol. 6, no. 5, pp. 8169–8181, 2019.
[4] D. Wang, B. Bai, W. Zhao, and Z. Han, “A survey of optimization
approaches for wireless physical layer security,” IEEE Commun. Surveys
Tuts., vol. 21, no. 2, pp. 1878–1911, 2019.

[5] M. O. Demir, A. E. Pusane, G. Dartmann, G. Ascheid, and G. K.
Kurt, “A garden of cyber physical systems: Requirements, challenges
and implementation aspects,” IEEE Internet Things Mag., Early access.
[6] D. W. K. Ng, E. S. Lo, and R. Schober, “Secure resource allocation
and scheduling for OFDMA decode-and-forward relay networks,” IEEE
Trans. on Wireless Commun., vol. 10, no. 10, pp. 3528–3540, 2011.
[7] Y. P. Hong, P. Lan, and C. J. Kuo, “Enhancing physical-layer secrecy
in multiantenna wireless systems: An overview of signal processing
approaches,” IEEE Signal Process. Mag., vol. 30, no. 5, pp. 29–40,
2013.

[8] H. Wang and X. Xia, “Enhancing wireless secrecy via cooperation:
signal design and optimization,” IEEE Commun. Mag., vol. 53, no. 12,
pp. 47–53, 2015.

[9] P. Huang, Y. Hao, T. Lv, J. Xing, J. Yang, and P. T. Mathiopoulos,
“Secure beamforming design in relay-assisted Internet of Things,” IEEE
Internet Things J., vol. 6, no. 4, pp. 6453–6464, 2019.

[10] N. Yang, P. Yeoh, M. Elkashlan, R. Schober, and I. B. Collings,
“Transmit antenna selection for security enhancement in MIMO wiretap
channels,” IEEE Trans. on Commun., vol. 61, no. 1, pp. 144–154, 2013.
[11] R. Zi, J. Liu, L. Gu, and X. Ge, “Enabling security and high energy efﬁ-
ciency in the Internet of Things with massive MIMO hybrid precoding,”
IEEE Internet Things J., vol. 6, no. 5, pp. 8615–8625, 2019.

[12] Z. Xu and Q. Zhu, “A cyber-physical game framework for secure and
resilient multi-agent autonomous systems,” in IEEE Confer. on Decision
and Control, 2015, pp. 5156–5161.

[13] T. Liu, S. Han, W. Meng, C. Li, and M. Peng, “Dynamic power
allocation scheme with clustering based on physical layer security,” IET
Commun., vol. 12, no. 20, pp. 2546–2551, 2018.

[14] F. Gabry, A. Zappone, R. Thobaben, E. A. Jorswieck, and M. Skoglund,
“Energy efﬁciency analysis of cooperative jamming in cognitive radio
networks with secrecy constraints,” IEEE Wireless Commun. Lett., vol. 4,
no. 4, pp. 437–440, 2015.

[15] X. Xu, Y. Cai, W. Yang, W. Yang, J. Hu, and T. Yin, “Energy-efﬁcient
optimization for physical layer security in large-scale random crns,” in
Inter. Confer. on Wireless Commun. Signal Process. (WCSP), 2015, pp.
1–6.

[16] Z. Deng, Q. Li, Q. Zhang, L. Yang, and J. Qin, “Beamforming design
for physical layer security in a two-way cognitive radio IoT network
with SWIPT,” IEEE Internet Things J., vol. 6, no. 6, pp. 10 786–10 798,
2019.

[17] L. Mucchi, L. Ronga, X. Zhou, K. Huang, Y. Chen, and R. Wang, “A
new metric for measuring the security of an environment: The secrecy
pressure,” IEEE Trans. on Wireless Commun., vol. 16, no. 5, pp. 3416–
3430, May 2017.

