Noname manuscript No.
(will be inserted by the editor)

Dynamic Bayesian Games for Adversarial and Defensive
Cyber Deception

Linan Huang · Quanyan Zhu

8
1
0
2

p
e
S
8

]
T
G
.
s
c
[

2
v
3
1
0
2
0
.
9
0
8
1
:
v
i
X
r
a

Abstract Security challenges accompany the eﬃciency. The pervasive inte-
gration of information and communications technologies (ICTs) makes cyber-
physical systems vulnerable to targeted attacks that are deceptive, persistent,
adaptive and strategic. Attack instances such as Stuxnet, Dyn, and WannaCry
ransomware have shown the insuﬃciency of oﬀ-the-shelf defensive methods in-
cluding the ﬁrewall and intrusion detection systems. Hence, it is essential to
design up-to-date security mechanisms that can mitigate the risks despite the
successful inﬁltration and the strategic response of sophisticated attackers. In
this chapter, we use game theory to model competitive interactions between
defenders and attackers. First, we use the static Bayesian game to capture the
stealthy and deceptive characteristics of the attacker. A random variable called
the type characterizes users’ essences and objectives, e.g., a legitimate user or
an attacker. The realization of the user’s type is private information due to
the cyber deception. Then, we extend the one-shot simultaneous interaction
into the one-shot interaction with asymmetric information structure, i.e., the
signaling game. Finally, we investigate the multi-stage transition under a case
study of Advanced Persistent Threats (APTs) and Tennessee Eastman (TE)

This is a preliminary version of the paper that will appear in the following edited book as
a book chapter: L. Huang and Q. Zhu, Deception and Counter-deception Bayesian Game:
Adaptive Defense Strategies Against Advanced Persistent Threats for Cyber-physical Sys-
tems, Cyber Deception, E. Al-Shaer, K. Hamlen, J. Wei, and C. Wang (Eds.), Springer,
2018, to appear.

Linan Huang
Department of Electrical and Computer Engineering, New York University 2 MetroTech
Center, Brooklyn, NY, 11201, USA
Tel.: 347-204-2406
E-mail: lh2328@nyu.edu

Quanyan Zhu
Department of Electrical and Computer Engineering, New York University 2 MetroTech
Center, Brooklyn, NY, 11201, USA
Tel.: 646-997-3371
E-mail: qz494@nyu.edu

 
 
 
 
 
 
2

Linan Huang, Quanyan Zhu

process. Two-Sided incomplete information is introduced because the defender
can adopt defensive deception techniques such as honey ﬁles and honeypots
to create suﬃcient amount of uncertainties for the attacker. Throughout this
chapter, the analysis of the Nash equilibrium (NE), Bayesian Nash equilibrium
(BNE), and perfect Bayesian Nash equilibrium (PBNE) enables the policy pre-
diction of the adversary and the design of proactive and strategic defenses to
deter attackers and mitigate losses.

Keywords Bayesian games · Multistage transitions · Advanced Persistent
Threats (APTs) · Cyber deception · Proactive and strategic defense

1 Introduction

The operation of the modern society intensively relies on the Internet services
and information and communications technologies (ICTs). Cybersecurity has
been an increasing concern as a result of the pervasive integration of ICTs
as witnessed in Fig. 1. Every peak of the yellow line corresponds to a cyber
attack1 and both the frequency and the magnitude which represents the scope
of inﬂuence has increased, especially in recent years. For example, the Domain
Name System (DNS) provider Dyn has become the targeted victim of the
multiple distributed denial-of-service (DDoS) attacks in October 2016. The
Mirai malware has turned a large number of IoT devices such as printers and
IP cameras to bots and causes an estimate of 1.2 Tbps network ﬂow. More
recently in May 2017, the WannaCry ransomware has attacked more than
200,000 computers across 150 countries, with total damages up to billions of
dollars.

Fig. 1 The search results of three keywords, i.e., the cybersecurity (in blue), the cyber
deception (in red) and the cyber attack (in yellow) in the United States from Jan. 2004,
to Aug. 2018 via the GoogleTrends. Compared with the blue and yellow line, the cyber
deception which endows attackers an information advantage over the defender requires more
investigations. Numbers on the y-axis represent the search frequency normalized with respect
to the highest point on the chart for the given region and time. A value of 100 is the peak
popularity.

1 https://en.wikipedia.org/wiki/List of cyberattacks

Dynamic Bayesian Games for Adversarial and Defensive Cyber Deception

3

One way to contend with the cyber attacks is for the defenders to set
up ﬁrewalls with pre-deﬁned rules to prevent their internal network from the
untrustworthy network traﬃc. Moreover, defenders can use intrusion detec-
tion systems [2] to detect a suspected malicious activity when an intrusion
penetrates the system. These defensive methods are useful in deterring naive
attacks up to certain degree. However, the unequal status between the attacker
and the defender naturally gives the attacker an advantage in the game. An
attacker succeeds by knowing and exploiting one zero-day vulnerability while a
defender can be successful only when he can defend against all attacks. More-
over, attacks evolve to be increasingly sophisticated and can easily challenge
the traditional defense mechanisms, i.e., intrusion prevention, detection, and
response.

Cyber deception is one way to evade the detection. As deﬁned in [14], the
deception is either the prevention from a true belief or a formulation of a false
belief. In the cybersecurity setting, the ﬁrst type of deception corresponds to
a stealthy attack where the virus can behave to be legitimate apparently and
remain undetected. For example, if a strategic attacker knows the pre-deﬁned
rules of the ﬁrewalls or the rule-based intrusion detection system, they can
adapt their behaviors to avoid triggering the alarm. In the second type, for
example, hackers can launch “sacriﬁcial attacks” to trick the defender into a
false belief that all viruses have been detected and repelled [5]. The adversarial
cyber deception introduces the information asymmetry and poses attackers in
a favorable position. A defender is analogous to a blind person who competes
with a sighted attacker in a well-illuminated room.

To tilt the information asymmetry, the defender can be reactive, i.e., con-
tinuously consummates the intrusion prevention and detection system capable
of stealthy and deceptive attacks. This costly method is analogous to curing the
blindness. Defensive deception, however, provides an alternative to the costly
rectiﬁcations of the system by deliberately and proactively introducing un-
certainties into the system, i.e., private information unknown to the attacker.
This proactive method is analogous to turning oﬀ the light and providing ev-
ery participant, especially the attacker with suﬃcient amount of uncertainties.
For example, a system can include honeypots that contain no information or
resource of value for the attackers. However, the defender can make the hon-
eypot indistinguishable from the real systems by faking communication and
network traﬃc. Since a legitimate user should not access the honeypot, the
activities in the honeypot reveal the existence as well as characteristics of that
attack.

The cyber attacks and defenses are the spear and shield, the existence of
attackers motivates the development of defensive technologies, which in turn
stimulates advanced attacks that are strategic, deceptive, and persistent. In
this chapter, we model these competitive interactions using game theory rang-
ing from complete to incomplete information, static to multi-stage transition,
and symmetric to asymmetric information structures.

4

1.1 Literature

Linan Huang, Quanyan Zhu

Deception and its modeling are emerging areas of research. The survey [18]
provides a taxonomy that deﬁnes six types of defensive deception: perturbation
via external noises, moving target defense (MTD), obfuscation via revealing
useless information, mixing via exchange systems, honey-x, and the attacker
engagement that uses feedback to inﬂuence attackers dynamically. MTD [12]
can limit the eﬀectiveness of the attacker’s reconnaissance by manipulating
the attack surface of the network. The authors in [27] combine information-
and control-theory to design an optimal MTD mechanism based on a feedback
information structure while [15], [13] use the Markov chain to model the MTD
process and discuss the optimal strategy to balance the defensive beneﬁt and
the network service quality.

Game-theoretic models are natural frameworks to capture the adversar-
ial and defensive interactions between players [29, 23, 30,17,6,16,28,26,8,9].
There are two perspectives to deal with the incomplete information under
the game-theoretic setting, i.e., the robust game theory [1] that conservatively
considers the worst case and the Bayesian game model [7] that introduces a
random variable called the type and the concept of Bayesian strategies and
equilibrium. Signaling game, a two-stage game with one-sided incomplete in-
formation has been widely applied to diﬀerent cybersecurity scenarios. For ex-
ample, [30] considers a multiple-period signaling game in the attacker-defender
resource-allocation. The authors in [20] combine the signaling game with an
external detector to provide probabilistic warnings when the sender acts de-
ceptively. The recent work of [11] has proposed a multi-stage Bayesian game
with two-sided incomplete information that well characterizes the composite
attacks that are advanced, persistent, deceptive and adaptive. A dynamic be-
lief update and long-term statistical optimal defensive policies are proposed
to mitigate the loss and deter the adversarial users.

1.2 Notation

In this chapter, the pronoun ‘he’ refers to the user denoted by P2, and ‘she’
refers to the defender as P1. Calligraphic fonts such as A represent a set. For
i ∈ I, notation ‘−i’ means I \ {i}. Take I := {1, 2} as an example, if i = 1,
then −i = 2. If A is a ﬁnite set, then we let (cid:52)A represent the set of probability
distributions over A, i.e., (cid:52)A := {p : A (cid:55)→ R+| (cid:80)

a∈A p(a) = 1}.

2 Static Game with Complete Information for Cybersecurity

Game theory has been applied to cybersecurity problems [22,3,26,16,21,25,19]
to capture quantitatively the interaction between diﬀerent “players” including
the system operator, legitimate users, and malicious hackers. As a baseline
security game, the bi-matrix game focuses on two non-cooperative players,

Dynamic Bayesian Games for Adversarial and Defensive Cyber Deception

5

i.e., an attacker P2 aiming at compromising the system and a defender P1 who
tries to prevent systems from adverse consequences, mitigate the loss under
attacks, and recover quickly and thoroughly to the normal operation after the
virus’ removal.

Each player Pi, i ∈ {1, 2} can choose an action ai from a ﬁnite set Ai and
mi := |Ai| is the number of actions Pi can choose from. The value of the utility
Ji(a1, a2) ∈ Rm1×m2 for each player i depends collectively on both players’
actions as shown in Table 1. As stated in the introduction, targeted attacks
can investigate the system thoroughly, exploit vulnerabilities, and obtain the
information on the security settings including the value of assets and possible
defensive actions. Thus, the baseline game with complete information assumes
that both players are aware of the other player’s existence, action sets, and
payoﬀ matrices. However, each player will not know the other player’s action
before making his/her decision. Example 1 considers a nonzero-sum complete-
information security game where the attacker and the defender have conﬂicting
objectives, i.e., ∃a1 ∈ A1, a2 ∈ A2, J1(a1, a2) + J2(a1, a2) (cid:54)= 0. For scenarios
where the defender does not know the utility of the attacker, she can assume
J2(a1, a2) = −J1(a1, a2), ∀a1 ∈ A1, a2 ∈ A2 and use the zero-sum game to
provide a useful worst-case analysis.

Table 1 Utility bi-matrix (J1, J2) of the static secure game, i.e., J1 = [0, −r1; 0, r3], J2 =
[0, r2; 0, −r4]. P1 is the row player and P2 is the column player. Both players are rational
and aim to maximize their own payoﬀs.

P1 \ P2
Permit
Restrict

NOP
(0, 0)
(0, 0)

Escalate
(−r1, r2)
(r3, −r4)

Example 1 Consider the game in Table 1. Attacker P2 can either choose action
a2 = 1 to escalate his privilege in accessing the system, or choose No Operation
Performed (NOP) a2 = 0. Defender P1 can either choose to restrict a1 = 1 or
allow a1 = 0 a privilege escalation. The value in the brackets (·, ·) represents
the utility for P1, P2 under the corresponding action pair, e.g., if the attacker
escalates his privilege and the defender chooses to allow an escalation, then
P2 obtains a reward of r2 > 0 and P1 receives a loss of r1 > 0. In this ex-
ample, no dominant (pure)-strategies exist for both players to maximize their
utilities, i.e., each player’s optimal action choice depends on the other player’s
choice. For example, P1 prefers to allow an escalation only when P2 chooses
the action NOP; otherwise P1 prefers to restrict an escalation. The above ob-
servation motivates the introduction of the mixed-strategy in Deﬁnition 1 and
the concept of Nash equilibrium in Deﬁnition 2 where any unilateral deviation
(cid:117)(cid:116)
from the equilibrium does not beneﬁt the deviating player.

Deﬁnition 1 A mixed-strategy σi ∈ (cid:52)Ai for Pi is a probability distribution
(cid:117)(cid:116)
on his/her action set Ai.

6

Linan Huang, Quanyan Zhu

Denote σi(ai) as Pi’s probability of taking action ai, then (cid:80)
σi(ai) =
1, ∀i ∈ {1, 2} and σi(ai) ≥ 0, ∀i ∈ {1, 2}, ai ∈ Ai. Once player Pi has deter-
mined strategy σi, the action ai will be a realization of the strategy. Hence,
each player Pi under the mixed-strategy has the objective to maximize the ex-
pected utility (cid:80)
σ1(a1)σ2(a2)J1(a1, a2). Note that the concept
of the mixed strategy includes the pure strategy as a degenerate case.

a1∈A1

a2∈A2

ai∈Ai

(cid:80)

Deﬁnition 2 A pair of mixed-strategy (σ∗
strategy) Nash equilibrium (NE) if for all σ1 ∈ (cid:52)A1, σ2 ∈ (cid:52)A2,

2) is said to constitute a (mixed-

1, σ∗

(cid:88)

(cid:88)

a1∈A1
(cid:88)

a2∈A2
(cid:88)

a1∈A1

a2∈A2

1(a1)σ∗
σ∗

2(a2)J1(a1, a2) ≥

1(a1)σ∗
σ∗

2(a2)J1(a1, a2) ≥

(cid:88)

(cid:88)

a1∈A1
(cid:88)

a2∈A2
(cid:88)

a1∈A1

a2∈A2

σ1(a1)σ∗

2(a2)J1(a1, a2),

σ∗
1(a1)σ2(a2)J1(a1, a2).

(cid:117)(cid:116)

In a ﬁnite static game with complete information, the mixed-strategy Nash
equilibrium always exists. Thus, we can compute the equilibrium which may
not be unique via the following system of equations.

σ∗
1 ∈ arg max
σ1

σ∗
2 ∈ arg max
σ2

(cid:88)

(cid:88)

a1∈A1
(cid:88)

a2∈A2
(cid:88)

a1∈A1

a2∈A2

σ1(a1)σ∗

2(a2)J1(a1, a2),

σ∗
1(a1)σ2(a2)J1(a1, a2).

The static game model and equilibrium analysis are useful in the cybersecurity
setting because of the following reasons. First, the strategic model quantita-
tively captures the competitive interaction between the hacker and the system
defender. Second, the NE provides a prediction of the security outcomes of
the scenario which the game model captures. Third, the probabilistic defenses
suppress the probability of adversarial actions and thus mitigate the expected
economic loss. Finally, the analysis of the equilibrium motivates an optimal
security mechanism design which can shift the equilibrium toward ones that
are favored by the defender via an elaborate design of the game structure.

3 Static Games with Incomplete Information for Cyber Deception

The primary restrictive assumption for the baseline security game is that all
game settings including the action sets and the payoﬀ matrices are of complete
information to the players. However, the deceptive and stealthy nature of
advanced attackers makes it challenging for the defender to identify the nature
of the malware accurately at all time. Even the up-to-date intrusion detection
system has the false alarms and misses that can be fully characterized by a
receiver operating characteristic (ROC) curve plotted with the true positive
rate (TPR) against the false positive rate (FPR). To capture the uncertainty
caused by the cyber deception, we introduce a random variable called the type
to model the possible scenario variations as shown in Example 2.

Dynamic Bayesian Games for Adversarial and Defensive Cyber Deception

7

Table 2 Utility bi-matrix when user P2 is either adversarial θ2 = θb or legitimate θ2 = θg.

θ2 = θb
Permit
Restrict

NOP
(0, 0)
(0, 0)

Escalate
(−r2, r2)
(r0, −r0)

θ2 = θg
Permit
Restrict

NOP
(0, 0)
(0, 0)

Escalate
(r1, r1)
(−r1, −r1)

Example 2 Consider the following static Bayesian game where we use two dis-
crete values of the type θ2 ∈ Θ2 := {θb, θg} to distinguish the user P2 as either
an attacker θ2 = θb or a legitimate user θ2 = θg. The attacker can camouﬂage
to be a legitimate user and possess the same action set A2, e.g., both attacker
and legitimate can request to escalate the privilege a2 = 1. However, since
they are of diﬀerent types, the introduced utilities ¯Ji(a1, a2, θ2), i ∈ {1, 2}
are diﬀerent under the same action pair(a1, a2) as shown in Table 2. For ex-
ample, the privilege escalation has a positive eﬀect on the system when the
user P2 is legitimate, yet will harm the system when P2 is an attacker. Since
the defender does not know the type of the user due to the cyber deception,
we extend the Nash equilibrium analysis of the complete-information game
to Bayesian Nash equilibrium in Deﬁnition 3 to deal with the type uncer-
tainty. Since P2 knows his type value to be either θg or θb, his mixed-strategy
¯σ2 : Θ2 (cid:55)→ (cid:52)A2 should be a function of his type value. Thus, with a slight
abuse of notation, ¯σ2(a2, θ2) ≥ 0, ∀a2 ∈ A2, ∀θ2 ∈ Θ2 is the probability of tak-
ing action a2 under the type value θ2. Clearly, the mixed-strategy is a probabil-
ity measure, i.e., (cid:80)
¯σ2(a2, θ2) = 1, ∀θ2 ∈ Θ2. Suppose that P1 manages
to know the probability distribution of the type b0
1 ∈ (cid:52)Θ2, e.g., defender P1
1(θb)
believes with probability b0
that P2 is of an adversarial type. Similarly, we have (cid:80)
b0
1(θ2) = 1 and
b0
(cid:117)(cid:116)
1(θ2) ≥ 0, ∀θ2 ∈ Θ2.

1(θg) that user P2 is of a legitimate type and b0

a2∈A2

θ2∈Θ2

Deﬁnition 3 A pair of mixed-strategy (σ∗
sided) mixed-strategy Bayesian Nash equilibrium (BNE) if

1, ¯σ∗

2(·)) is said to constitute a (one-

(cid:88)

θ2∈Θ2
(cid:88)

θ2∈Θ2

b0
1(θ2)

b0
1(θ2)

(cid:88)

(cid:88)

a1∈A1
(cid:88)

a2∈A2
(cid:88)

a1∈A1

a2∈A2

σ∗
1(a1)¯σ∗

2(a2, θ2) ¯J1(a1, a2, θ2) ≥

σ1(a1)¯σ∗

2(a2, θ2) ¯J1(a1, a2, θ2), ∀σ1(·).

and

(cid:88)

(cid:88)

a1∈A1
(cid:88)

a2∈A2
(cid:88)

a1∈A1

a2∈A2

1(a1)¯σ∗
σ∗

2(a2, θ2) ¯J2(a1, a2, θ2) ≥

1(a1)¯σ2(a2, θ2) ¯J2(a1, a2, θ2), ∀θ2 ∈ Θ2, ∀¯σ2(·, θ2).
σ∗

Note that the binary type space Θ2 can easily extend to ﬁnitely many ele-
ments to model diﬀerent kinds of legitimate users and hackers who bear diverse

(cid:117)(cid:116)

8

Linan Huang, Quanyan Zhu

type-related payoﬀ functions. Since the type distinguishes diﬀerent users and
characterizes their essential attributes, the type space can also be a continuum
and interpreted as a normalized measure of damages or the threat level to the
system [10]. Moreover, the defender P1 can also have a type θ1 ∈ Θ1, which
forms a static version of the two-sided dynamic Bayesian game as shown in
Section 4.2. Theorem 1 guarantees the existence of BNE regardless of exten-
sions mentioned above.

Theorem 1 A mixed-strategy BNE exists for a static Bayesian game with a
ﬁnite type space. For games with a continuous type space and a continuous
strategy space, if strategy sets and type sets are compact, payoﬀ functions are
continuous and concave in players’ own strategies, then a pure-strategy BNE
exists.

4 Dynamic Bayesian Game for Deception and Counter-Deception

Followed from the above static Bayesian game with one-sided incomplete in-
formation, we investigate two types of dynamic games for cyber deception
and counter-deception. The signaling game is two-stage and only the receiver
has the incomplete information of the sender’s type. The two-sided dynamic
Bayesian game with a multi-stage state transition in Section 4.2 can be viewed
as an extension of the signaling game. The solution concept in this section ex-
tends the BNE to the perfect Bayesian Nash equilibrium (PBNE).

4.1 Signaling Game for Cyber Deception

We illustrate the procedure of the signaling game as follows:

– An external player called the Nature draws a type θ2 from a set Θ2 :=
1 ∈ (cid:52)Θ2

{θ1, θ2, · · · , θI } according to a given probability distribution b0
where b0

1(θi) ≥ 0, ∀i ∈ {1, 2, · · · , I} and (cid:80)I

1(θi) = 1.

i=1 b0

– The user P2 (called the sender) observes the type value θ2 and then chooses
an action a2 (called a message) from a ﬁnite set of message space A2.
– The defender P1 (called the receiver) observes the action a2 and then

chooses her action a1 ∈ A1.

– Payoﬀs ( ¯J1(a1, a2, θ2), ¯J2(a1, a2, θ2)) are given to the sender and receiver,

respectively.

Belief Formulation. Since the receiver P1 has incomplete information about
the sender’s type, she will form a belief b1
1 : A2 (cid:55)→ (cid:52)Θ2 on the type θ2 based
on the observation of the sender’s message a2. As a measure of the conditional
1(θi|a2) ≥ 0, ∀i ∈ {1, 2, · · · , I}, ∀a2 ∈ A2
probability, the belief b1
1(θ2|a2) := (cid:80)I
and (cid:80)
b1

1 satisﬁes b1
i=1 b1

1(θi|a2) = 1, ∀a2 ∈ A2.

θ2∈Θ2

Dynamic Bayesian Games for Adversarial and Defensive Cyber Deception

9

Receiver’s Problem. For every received message a2, receiver P1 aims to opti-
mize her expected payoﬀs under her belief b1

1(·|a2), i.e.,

max
a1∈A1

(cid:88)

θ2∈Θ2

1(θ2|a2) ¯J1(a1, a2, θ2).
b1

(1)

As a result, the receiver’s (pure)-strategy is given by the mapping ˆa1 : A2 (cid:55)→
A1. Thus, the receive P1’s action is the outcome of the mapping, i.e., a1 =
ˆa1(a2).

Sender’s Problem. For every type θ2 ∈ Θ2 that the Nature picks for P2, sender
P2 should pick a message a2 ∈ A2 that maximizes the following utility with
the anticipation of receiver’s action a1 = ˆa1(a2), i.e.,

max
a2∈A2

¯J2(ˆa1(a2), a2, θ2).

(2)

Hence, the sender’s (pure)-strategy is given by the mapping ¯a2 : Θ2 (cid:55)→ A2 and
P2’s action under the type value θ2 is a2 = ¯a2(θ2). The sender’s strategy ¯a2 is
called a pooling strategy if he chooses the same message a2 independent of the
type given by the Nature, and is called a separating strategy if the mapping
¯a2 is injective. For all other feasible mappings, ¯a2 is called a semi-separating
strategy.

Mixed-strategy Receiver and Sender’s Problem. We can extend the pure-strategy
to the mixed-strategy ˆσ1 : A2 (cid:55)→ (cid:52)A1 for receiver P1 and the same ¯σ2 : Θ2 (cid:55)→
(cid:52)A2 deﬁned in Section 3 for sender P2. After observing sender’s message a2 as
a realization of the mix-strategy ¯σ2, receiver P1 assigns probability ˆσ1(a1, a2)
to her action a1 with the feasibility constraint (cid:80)
ˆσ1(a1, a2) = 1, ∀a2 ∈ A2
and ˆσ1(a1, a2) ≥ 0, ∀a1 ∈ A1, a2 ∈ A2. The expected objective functions for
both players under the mixed-strategy are deﬁned as follows.

a1∈A1

max
ˆσ1(·)

max
¯σ2(·)

(cid:88)

θ2∈Θ2
(cid:88)

a1∈A1

b1
1(θ2|a2)

(cid:88)

a1∈A1
(cid:88)

ˆσ1(a1, a2)

ˆσ1(a1, a2) ¯J1(a1, a2, θ2), ∀a2 ∈ A2.

¯σ2(a2, θ2) ¯J2(a1, a2, θ2), ∀θ2 ∈ Θ2.

(3)

a2∈A2

Belief Consistency. Since the message a2 is a function of the type θ2, the
observation of the message should reveal some information of the type. Thus,
the receiver updates the initial belief b0
1(·|a2)
via the Bayesian rule.

1(·) to form the posterior belief b1

b1
1(θ2|a2) =

(cid:80)

b0
1(θ2)¯σ2(a2|θ2)

θ2∈Θ2

b0
1(θ2)¯σ2(a2|θ2)

(cid:88)

, if

θ2∈Θ2

b0
1(θ2)¯σ2(a2|θ2) > 0,

b1
1(θ2|a2) = any proability distritbutions, if

(cid:88)

θ2∈Θ2

b0
1(θ2)¯σ2(a2|θ2) = 0.

(4)

10

Linan Huang, Quanyan Zhu

Serving as a particular case, the receiver and the sender’s problem under the
pure strategy should also satisfy the Bayesian update of the belief. Note that
although P1 can observe the message a2 which is a realization of ¯σ2, she cannot
directly update her belief via (4) if the signaling game is only played once.
However, (4) contributes to the PBNE of the signaling game in Deﬁnition 4,
serving as the belief consistency constraint.

Deﬁnition 4 A pure-strategy perfect Bayesian Nash equilibrium of the sig-
naling game is a pair of strategies (ˆa∗
1, ¯a∗
that satisfy (1) (2)
and (4). A mixed-strategy perfect Bayesian Nash equilibrium of the signaling
game is a pair of strategies (ˆσ∗
that satisfy (3) and (4). (cid:117)(cid:116)

2) and belief b1,∗

2) and belief b1,∗

1, ¯σ∗

1

1

The reader may already realize that we can use signaling game to model the
same cyber deception scenario in Example 2 only with the diﬀerence of the
asymmetric information structure, i.e., the defender P1 has a chance to ob-
serve the behavior of the user P2 before making her decision. The information
asymmetry results in the following changes. First, P1’s mixed-strategy ˆσ1(a2)
is a function of her observation, i.e., P2’s action a2. Second, instead of directly
taking an expectation over the initial belief b0
1, defender P1 obtains a posterior
belief b1
1 that is consistent with the new observation a2. Third, the type of
belief can aﬀect the PBNE even under the cheap-talk setting when utilities
of both players are independent of the message. Finally, if there is only one
type with a known b0
1, which means that the type value becomes common
knowledge, the signaling game becomes a Stackelberg game with leader P2
and follower P1.

4.2 Multi-stage with Two-sided Incomplete Information

The deceptive techniques adopted by the attacker make it challenging for the
defender to correctly identify the type of the user even observing the mani-
fested behavior as shown in Example 2. To tilt the information asymmetry,
we can either continue to develop the intrusion detection system to increase
the TPR with decreased FPR or refer to defensive deception techniques to
create a suﬃcient amount of uncertainties for the attackers. Use defensive and
active deception as a counter-deception technique will disorient and slow down
the adversarial inﬁltration because attackers have to judge the target’s type,
i.e., whether it is a real valuable production system or a well-pretended hon-
eypot. Therefore, we introduce a two-sided incomplete information Bayesian
game model with a multistage state transition for advanced attacks such as
Advanced Persistent Threats (APTs) which inﬁltrate stage by stage.

4.2.1 Two-sided Private Types

This section discusses the scenarios where not only the user P2 has a type, the
defender P1 also has a private type θ1 ∈ Θ1 to distinguish a system’s diﬀerent
levels of sophistication and security awareness. For example, the defender’s

Dynamic Bayesian Games for Adversarial and Defensive Cyber Deception

11

type space can be binary Θ1 := {θH , θL} where θH represents a defender who
is well-trained with a high-security awareness and also supported by advanced
virus detection and analysis systems. Thus, she may refer to the log ﬁle with a
higher frequency and more likely to obtain valuable information through the
behavior analysis. Thus, once the attacker requests for privilege escalation and
P1 restricts and inspects the log ﬁle, a higher reward as well as a higher penalty
are introduced under a high-type defender θH than a low-type defender θL,
i.e., r0 = r3 · 1θL + r4 · 1θH where r4 > r3 > 0 as shown in Table 3.

Table 3 Utility bi-matrix when user P2 is either adversarial θ2 = θb or legitimate θ2 = θg
and defender P1 is either of high type θ1 = θH or of low type θ1 = θL.

θ2 = θb
Permit
Restrict

NOP
(0, 0)
(0, 0)

Escalate
(−r2, r2)
(r0, −r0)

θ2 = θg
Permit
Restrict

NOP
(0, 0)
(0, 0)

Escalate
(r1, r1)
(−r1, −r1)

Two aspects motivate us to introduce a random variable as the defender’s
type, i.e., the user P2 only knows the prior probability distribution over the
type space Θ1 yet not the value/realization of P1’s type. On the one hand, the
modern cyberinfrastructure networks have become increasingly interdependent
and complicated, so it is hard to evaluate the system payoﬀ accurately even
given both players’ actions. On the other hand, the adoption of defensive de-
ception techniques brings uncertainties and diﬃculties for the user, especially
attackers to evaluate the system setting. Therefore, we model the uncertainties
by letting the utility function be a function of the type, which is a random
variable.

4.2.2 A Scenario of Advanced Persistent Threats

A class of stealthy and well-planned sequence of hacking processes called Ad-
vanced Persistent Threats (APTs) motivates the multi-stage transition as well
as two strategic players with two-sided incomplete information [29,11]. Unlike
the non-targeted attacks who spray a large number of phishing emails and
pray for some phools to click on the malicious links and get compromised,
nation-sponsored APTs have suﬃcient amount of resources to initiate a re-
connaissance phase to understand their targeted system thoroughly and tailor
their attack strategies with the target. Multistage movement is an inherent
feature of APTs as shown in Fig. 2. The APTs’ life cycle includes a sequence
of stages such as the initial entry, foothold establishment, privilege escalation,
lateral movement, and the ﬁnal targeted attacks on either conﬁdential data
or the physical infrastructures such as nuclear power stations and automated
factories. APTs use each stage as a stepping stone for the next one. Unlike the
static “smash and grab” attacks who launch direct attacks to obtain one-shot
reward and then get identiﬁed and removed, APTs possess a long-term per-
sistence and stage-by-stage inﬁltration to evade detection. For example, APTs
can stealthily scan the port slowly to avoid hitting the warning threshold of

12

Linan Huang, Quanyan Zhu

the IDS. APTs hide and behave like legitimate users during the escalation
and prorogation phases to deceive the defender until reaching the ﬁnal stage,
launch a ‘critical hit’ on their speciﬁc targets, and cause an enormous loss.

Fig. 2 The multi-stage life cycle of APTs forms a tree structure. During the reconnais-
sance phase, the threat actor probes the system and obtains intelligence from open-source
information or insiders. The infection can be either directly through the web phishing and
the physical access or indirectly through social engineering to manipulate the employees and
then obtain a private key. Then, APTs gain the foothold, escalate privilege, propagate lat-
erally in the cyber network and ﬁnally either cause physical damages or collect conﬁdential
data.

The classical intrusion prevention (IP) techniques such as the cryptogra-
phy and the physical isolation can be ineﬀective for APTs because APTs can
steal full cryptographic keys by techniques such as social engineering. Stuxnet,
as one of the most well-known APTs, has proven to be able to successfully
bridge the air gap between local area networks with the insertion of infected
USB drives. Similarly, the intrusion detection (ID) approach including [4] can
be ineﬀective if APTs acquire the setting of the detection system with the
help of insiders. Moreover, APTs operated by human-expert can analyze sys-
tem responses and learn the detection rule during their inactivity, thus deceive
the system defender and evade detection. Additionally, APTs can encrypt the
data as well as their communication content with their human experts. A
well-encrypted outbound network ﬂow will limit the eﬀectiveness of the data
loss prevention (DLS) system which detects potential data ex-ﬁltration trans-
missions and prevents them by monitoring, detecting, and blocking sensitive
data.

Hence, besides traditional defensive methods, i.e., IP, ID, DLS, it is es-
sential to design strategic security mechanisms to capture the competitive
interaction, the multi-stage multi-phase transition, as well as the adversarial
and defensive deception between the APTs and advanced defenders.

As shown in Table 3, the advanced defender with a private type is deceptive
and increases the attacker’s uncertainty. The defender is also adaptive because
she forms and updates the belief of the user’s type according to the observation
of the user’s actions as shown in (5).

Dynamic Bayesian Games for Adversarial and Defensive Cyber Deception

13

4.2.3 Multi-stage State Transition

from a stage-dependent ﬁnite set Ak

As shown in Fig. 2, the APT attacker moves stage by stage from the initial
infection to the ﬁnal target without jumps of multiple stages in one step. There
are also no incentives for the attacker to go back to stages that he has already
compromised because his ultimate goal is to compromise the speciﬁc target at
the ﬁnal stage. Therefore, we model the APT transition as a multistage game
with a ﬁnite horizon K. Each player i ∈ {1, 2} at each stage k ∈ {0, 1, · · · , K}
can choose an action ak
i because the
i
feasible actions are diﬀerent for each player at diﬀerent stages. The history
hk := {a0
} ∈ Hk contains the actions of both players
up to stage k − 1 and can be obtained by reviewing system activities from the
log ﬁle. Note that user’s actions ak
2 ∈ Ak
2 are the behaviors that are directly
observable such as the privilege escalation request and the sensor access in
the case study of Section 4.2.8. Sine both legitimate and adversarial users
can take these activities, a defender cannot identify the user’s type directly
from observing these actions. On the other hand, the defender’s action ak
1 will
be mitigation or proactive actions such as restricting the escalation request
or monitoring the sensor access. These proactive actions also do not directly
disclose the system type.

1, · · · , ak−1

2, · · · , ak−1

, a0

2

1

1, ak

State xk ∈ X k representing the status of the system at stage t is the
suﬃcient statistic of the history hk because a Markov state transition xk+1 =
f k(xk, ak
2) contains all the information of the history update hk = hk−1 ∪
{ak
1, ak
2}. Unlike the history, the cardinality of the state does not necessarily
grows with the number of stages. The function f k is deterministic because
history is fully observable without uncertainties. The function f k is also stage-
dependent and represents diﬀerent meanings. For example, in Section 4.2.8,
the state at the second last stage represents the current privilege level, while
at the ﬁnal stage, the state indicates which sensors have been compromised.

4.2.4 Behavior Mixed-strategy and Believe Update

According to the information available at stage k, i.e., history hk and his/her
type θi, player i takes a behavioral mixed-strategy σk
i with
the available information as the input of the function. Note that σk
i |hk, θi) ∈
:= {σk
Σk
i |hk, θi) = 1} is the probability of
i
taking action ak

i (ak
σk
i given hk, θi for all stage k ∈ {0, 1, · · · , K}.

i |hk, θi) ≥ 0 : (cid:80)

i : Hk × Θi (cid:55)→ (cid:52)Ak

i (ak

i (ak

i ∈Ak
ak
i

To correspond to the challenge of incomplete information of the other
player’s type, each player i forms a belief bk
i : Hk × Θi (cid:55)→ (cid:52)Θ−i that maps the
available information hk, θi to the distribution over the type space of the other
i (θ−i|hk, θi) at stage k is the conditional probability mass
player. Likewise, bk
function (PMF) of the other player’s type θ−i and (cid:80)
1, ∀k ∈ {0, 1, · · · , K}, ∀hk ∈ Hk, θi ∈ Θi, i ∈ {1, 2}.

i (θ−i|hk, θi)dθ−i =
bk

θ−i∈Θ−i

Assume that each player i knows the prior distribution of the other player’s
i according to the historical data and the statistical analysis. If no

type, i.e., b0

14

Linan Huang, Quanyan Zhu

prior information is available, a uniform distribution is an unbiased estimate.
Since the multi-stage model provides a sequential observation of the other
player’s action ak
−i, player i’s
belief of the other’s type can be updated via the Bayesian rule, i.e.,

−i which is a realization of the mixed-strategy σk

bk+1
i

(θ−i|[hk, ak

i , ak

−i], θi) =

i (θ−i|hk, θi)σk
bk

−i(ak
i (θ−i|hk, θi)σk
bk

−i|hk, θ−i)
−i(ak

−i|hk, θ−i)

θ−i∈Θi

(cid:80)

.

(5)

Note that the one-shot observation of the other player’s action does not
directly disclose the type because of the deception. However, since the utility
function in Section 4.2.5 is type dependent, the action made by the type-
dependent policy will serve as a message that contributes to a better estimate
of the other’s type. The accuracy of the belief will be continuously improved
when more actions are observed.

4.2.5 Utility Function and PBNE

At each stage k, J k
is the utility that depends on the type and the action of
i
both players, the current state xk, and some external random noise wk
i with a
known distribution. We introduce the external noise to model other unknown
factors that could aﬀect the value of the stage utility. The existence of the ex-
ternal noise makes it impossible for each player i to directly acquire the value
of the other’s type θ−i based on the combined observation of input parameters
xk, ak
i . In the case study,
we consider any additive noise with a 0 mean, i.e., J k
2, θi, θ−i, wk
1, ak
i ) =
˜J k
i (xk, ak
i , which leads to an equivalent payoﬀ over the expec-
tation of the external noise Ewk

2, θi plus the output value of the utility function J k
i (xk, ak

2, θi, θ−i) + wk

i , ∀xk, ak

i = ˜J k
J k

2, θi, θ−i.

1, ak

1, ak

1, ak

i

One signiﬁcant improvement from the static game to the dynamic game is
that each player i has a long-term objective to maximize the total expected
payoﬀ U k(cid:48):K
. For example, attackers of APTs may sacriﬁce the immediate
attacking reward to remain stealthy and receive more considerable beneﬁts in
the following stages, e.g., successfully reach the ﬁnal target and complete their
mission. Deﬁne σk(cid:48):K
i |hk, θi)]k=k(cid:48),··· ,K and the cumulative expected
utility U k(cid:48):K
sums the expected stage utilities from stage k(cid:48) to K as follows.

:= [σk

i (ak

i

i

i

, σk(cid:48):K
−i

, hK+1, θi)

(σk(cid:48):K
U k(cid:48):K
i
i
K
(cid:88)

:=

Eθ−i∼bk

i ,σk

i ,σk

−i,wk
i

[J k

i (xk, σk

i , σk

−i, θi, θ−i, wk

i )]

k=k(cid:48)

K
(cid:88)

(cid:88)

i (θ−i|hk, θi)
bk

(cid:88)

i (ak
σk

i |hk, θi)

(6)

θ−i∈Θ−i

=

·

k=k(cid:48)
(cid:88)

−i∈Ak
ak
−i

−i(ak
σk

−i|hk, θ−i) ˜J k

i , ak

−i, θi, θ−i).

ak
i ∈Ak
i
i (xk, ak

Dynamic Bayesian Games for Adversarial and Defensive Cyber Deception

15

Similar to the PBNE of the signaling game, the PBNE of multi-stage
Bayesian game deﬁned in Deﬁnition 5 requires a K-stage belief consistency.
Since the equilibrium may not always exist, an ε-equilibrium is introduced.

Deﬁnition 5 In the two-person K-stage Bayesian game with two-sided in-
complete information and a cumulative payoﬀ function U k(cid:48):K
in (6), a se-
i
quence of strategies σ∗,k(cid:48):K
∈ (cid:81)K
is called the ε perfect Bayesian Nash
equilibrium for player i, if bk
i satisﬁes the consistency constraint (5) for all
k ∈ {0, 1, · · · , K − 1} and for a given ε ≥ 0,

k=k(cid:48) Σk
i

i

U k:K
1

(σ∗,k:K
1

, σ∗,k:K
2

U k:K
2

(σ∗,k:K
1

, σ∗,k:K
2

, hK+1, θ1) ≥ sup
σk:K
1
, hK+1, θ2) ≥ sup
σk:K
2

U k:K
1

(σk:K
1

, σ∗,k:K
2

, hK+1, θ1) − ε.

U k:K
2

(σ∗,k:K
1

, σk:K
2

, hK+1, θ2) − ε.

If ε = 0, we have a perfect Bayesian Nash equilibrium.

4.2.6 Dynamic Programming

Given any feasible belief at every stage, we can use dynamic programming to
ﬁnd the PBNE in a backward fashion because of the tree structure and the
ﬁnite horizon. Deﬁne the value function as the utility-to-go function under the
PBNE strategy pair, i.e.,

V k
i (hk, θi) = U k:K

i

(σ∗,k:K
i

, σ∗,k:K
−i

, hk+1, θi).

Let V K+1
(hK+1, θi) := 0 be the boundary condition of the value function,
i
we have the following recursive system equations to solve the PBNE mixed-
strategies σ∗,k

for all stage k = {0, 1, · · · , K}:

1 , σ∗,k

2






(hk−1, θ1) = supσk−1

V k−1
1
1 ([hk−1, ak−1
[V k
V k−1
2
2 ([hk−1, ak−1
[V k

1

1

1 ∈Σk−1
], θ1) + ˜J k−1

1

1

Eθ2∼bk−1

,σk−1
1
(xk−1, ak−1

1

1

,σ∗,k−1
2
, ak−1
2

2 ∈Σk−1
], θ2) + ˜J k−1

2

2

Eθ1∼bk−1

2

,σ∗,k−1
1

(xk−1, ak−1

1

,σk−1
2
, ak−1
2

, ak−1
2

, ak−1
2

(hk−1, θ2) = supσk−1

, θ1, θ2)];

(7)

, θ1, θ2)].

Under the assumption of a Markov mixed-strategy ˜σt
˜V k
i (xk, θi) becomes the suﬃcient statistics of V k
to ˜σk
i |xk, θi) and V k
namic programming equation:

i (ak
i (hk, θi). By replacing σk
i (xk, θi) in (7), we can obtain a new dy-

i (hk, θi) to ˜V k

i |hk, θi),
i (ak

i |xk, θi) ≡ σk

i (ak

i (ak

i |hk, θi)

˜V k−1
i

(xk−1, θi) = sup
˜σk−1
i
, ak−1
2

i (f k(xk−1, ak−1

1

[ ˜V k

Eθ−i∼bk−1

i

,˜σk−1
i

,˜σ∗,k−1
−i

(8)

), θi) + ˜J k−1

i

(xk−1, ak−1

1

, ak−1
2

, θ1, θ2)].

16

Linan Huang, Quanyan Zhu

4.2.7 PBNE Computation by Bilinear Programming

i

To compute the PBNE, we need to solve a coupled system of the forward
belief update in (5) that depends on the PBNE strategies plus a backward
PBNE computation in (8) that can also be inﬂuenced by the type belief. If
there are no additional structures to explore, we have to use a forward and
backward iteration with the boundary condition of the initial belief b0
i (θ−i)
and ﬁnal stage utility-to-go ˜V K+1
(xK+1, θi) = 0. In particular, we ﬁrst assign
any feasible value to the type belief bk
i , k ∈ {1, 2 · · · , K}, then solve (8) from
stage k = K to k = 0 and use the resulted PBNE strategy pair to update
(5). We iteratively compute (8) and (5) until both the K-stage belief and the
PBNE strategy do not change, which provides a consistent pair of the PBNE
and the belief. If the iteration process does not converge, then the PBNE does
not exist. Deﬁne lmi as the column vector of ones with a dimension of mi, we
propose a bilinear program to solve the PBNE strategy for any given belief
bk
i , k ∈ {1, 2 · · · , K}, which leads to Theorem 2. The type space can be either
discrete or continuous. We refer reader to Section 4.4 in [11] for the proof of
the theorem.
Theorem 2 A strategy pair (˜σ∗,k
2 ) with the feasible state xk ∈ X k and
the consistent belief sequence bk
i at stage k ∈ {0, 1, · · · , K} constitutes a mixed-
strategy PBNE of the multistage Bayesian game in Deﬁnition 5, if, and only
if, there exists a sequence of scalar function pair (s∗,k(θ1), w∗,k(θ2)) such that
˜σ∗,k
1 (·|xk, θ1), ˜σ∗,k
2 (·|xk, θ2), s∗,k(θ1), w∗,k(θ2) are the optimal solutions to the
following bi-linear program for each k ∈ {0, 1, · · · , K}:

1 , ˜σ∗,k

sup
1 ,˜σk
˜σk

2 ,s,w

(cid:88)

(cid:88)

bk
1(θ1)

bk
2(θ2)

(cid:88)

1 (ak
˜σk

1|xk, θ1)

(cid:88)

2 (ak
˜σk

2|xk, θ2)

θ1∈Θ1

θ2∈Θ2

1 ∈Ak
ak
1

2 ∈Ak
ak
2

2
(cid:88)

[ ˜J k

i (xk, ak

1, ak

2, θ1, θ2) + ˜V k+1

i

(f k(xk, ak

1, ak

2), θi)]

i=1

+

(cid:88)

bk
2(θ2)w(θ2) +

(cid:88)

bk
1(θ1)s(θ1)

s.t.

(a).

(cid:88)

θ2∈Θ2
bk
1(θ1)

(cid:88)

θ1∈Θ1
1|xk, θ1)[ ˜J k

1 (ak
˜σk

2 (xk, ak

1, ak

2, θ1, θ2)

θ1∈Θ1
+ ˜V k+1
2
(cid:88)

ak
1 ∈Ak
1
(f k(xk, ak
(cid:88)

bk
2(θ2)

1, ak
2), θ2)] ≤ −w(θ2)lm2, ∀θ2 ∈ Θ2
2 (ak
˜σk

2|xk, θ2)[ ˜J k

1 (xk, ak

2, θ1, θ2)

1, ak

(b).

θ2∈Θ2
+ ˜V k+1
1

ak
2 ∈Ak
2
(f k(xk, ak

1, ak

2), θ1)] ≤ −s(θ1)lm1 , ∀θ1 ∈ Θ1.

(9)

(cid:117)(cid:116)

Note that the solution of (9) at stage k + 1 provides the value of ˜V k+1
˜V K+1
i
any given type belief.

and
= 0 is a known value. Thus, we can solve (9) from k = K to k = 0 for

i

Dynamic Bayesian Games for Adversarial and Defensive Cyber Deception

17

4.2.8 An Illustrative Case Study

We adopt the same binary type space in Section 4.2.1 and consider the fol-
lowing three-stage (K = 2) transition. The proactive defensive actions listed
in the case study should be combined with the reactive methods such as the
ﬁrewall to defend attacks other than APTs.

Initial Stage We consider the web phishing scenario for the initial entry. The
state space X 0 := {0, 1} of the initial stage is binary. Let x0 = 0 represents that
the user sends the email from an external IP domain while x0 = 1 represents
an email from the internal network domain. The attacker can also start from
state x0 = 1 due to the insider threats and the social engineering techniques.
To penalize the adversarial exploitation of the open-source intelligence (OS-
INT) data, the defender can create avatars (fake personal proﬁles) on the so-
cial network or the company website. The user P2 at the initial stage can
send emails to a regular employee a0
2 = 0, a Chief Executive Oﬃcer (CEO)
a0
2 = 1, or the avatar a0
2 = 2. The email can contain a legitimate shortening
Uniform Resource Locator (URL). If the user is legitimate, the URL will lead
to the right resources, yet if the user is malicious, the URL will redirect to
a malicious site and then take control of the client’s computer. As for the
defender, suppose that P1 proactively equips the computer with an anti-virus
system that can run the email in the sandbox and apply penetration test.
However, the limited budget can only support either the employees’ computer
or the CEO’s computer. Thus, the defender also has three possible actions,
i.e., equips the CEO’ computer a0
1 = 1, or
does not equip the anti-virus system a0
0. The
defender of high-security awareness θH will deploy an advanced anti-virus sys-
tem that costs higher installation fee than the regular anti-virus system, i.e.,
1, yet also provides a higher penalty to the attacker, i.e., r0
2 > c0
c0
3. De-
ﬁne c0
0 := c0
21{θ1=θH } as the deployment fee for two types of the
defender and r0
41{θ1=θH } as the penalty for attackers. The
attacker θ2 = θb will receive a faked reward r0
5 > 0 when contacting the avatar,
yet he then arrives at an unfavorable state, thus receives limited rewards in
the future stages. The equivalent utility matrix ˜J 0
2, θi, θ−i) is shown
in Table 4. Although the legitimate user can also take action a0
2 = 2, he should
assign zero probability to that action as the payoﬀ is −∞, i.e., a legitimate
user should not contact a person that does not exist.

1 = 2, the employee’s computer a0

1 = 0 to avoid a deployment fee c0

11{θ1=θL} + c0
0 := r0

31{θ1=θL} + r1

i (x0, a0

4 > r0

1, a0

Suppose that there are three possible states X 1 = {0, 1, 2} as the output of
the initial state transition function f 0, i.e., user P2 can reach the employee’s
computer x1 = 1, the CEO’s computer x1 = 2, or the honey pot x1 = 0.
Assume that the state transition from the initial state x0 = 1 is determined
only by the user’s action, i.e., the defender’s action does not aﬀect the email
delivery from the internal network. On the other hand, the state transition
from the external domain x0 = 0 is represented as follows. If defender chooses
not to apply malware analysis system a0
2 = 0, 1, 2
will lead the initial state x0 = 0 to state x1 = 1, 2, 0, respectively. If defender

1 = 0, then user’s action a0

18

Linan Huang, Quanyan Zhu

Table 4 The utility matrix ( ˜J 0
2 ) for player i = 1, 2 under diﬀerent types. Although
the utility matrix is independent of the current state x0, the action will aﬀect the state
transition f 0 and then the ﬁnal state xK where the utility is state-dependent.

1 , ˜J 0

θ2 = θg
NOP
Employee
CEO
θ2 = θb
NOP
Employee
CEO

Employee
(0, r0
1)
0, r0
(−c0
1)
(−c0
0, r0
1)
Employee
2, r0
(−r0
2)
0, −r0
(−c0
0)
0, r0
(−c0
2)

(−c0
(−c0

CEO
(0, r0
1)
0, r0
1)
0, r0
1)
CEO
2, r0
2)
0, r0
2)
0, −r0
0)

(−r0
(−c0
(−c0

Avatars
(0, −∞)

(−c0
(−c0

0, −∞)
0, −∞)
Avatars
(0, r0
5)
0, r0
5)
1, r0
5)

(−c0
(−c0

chooses a proactive deployment on the employee’s computer a0
1 = 1, then
2 = 0, 2 will drive the initial state x0 = 0 to state x1 = 0 and
user’s action a0
user’s action a0
2 = 1 will drive the initial state x0 = 0 to state x1 = 2. The
mitigation of the attack is at the tradeoﬀ of blocking some emails from the
legitimate user. Likewise, if defender chooses a proactive deployment on the
CEO’s computer a0
1 = 2, then user’s action a0
2 = 1, 2 will lead the initial state
x0 = 0 to state x1 = 0 and user’s action a0
2 = 0 will lead the initial state
x0 = 0 to state x1 = 1.

Intermediate Stage Without loss of generality, we use the privilege escalation
scenario in Table 3 as the intermediate stage k = 1. Although the utility matrix
is independent of the current state x1, the action will inﬂuence the long-term
beneﬁt by aﬀecting the state transition f 1 as follows. The output state space
X K = {0, 1, 2, 3} represents four diﬀerent levels of privilege from low to high.
If the user is at the honeypot x1 = 0, then he will end up at the honeypot
with level-zero privilege xK = 0 whatever actions he takes. For the user that
has arrived at the employee’s computer x1 = 1, if the defender allows privilege
escalation a1
1 = 0, then if the user chooses NOP a1
2 = 0, the user arrives at
level-one privilege xK = 1, else if the user requests escalation a1
2 = 1, he arrives
at level-two privilege xK = 2. If the defender restricts the privilege escalation
a1
1 = 1, then P2 arrives at state xK = 1 regardless of his action. The user
arrives at the CEO’s computer x1 = 2 possesses a higher privilege level. Then,
action pair a1
2 = 1 leads to xK = 3,
1 = 1, a1
and a1

1 = 0, a1
2 = 0/1 leads to xK = 2.

2 = 0 leads to xK = 2, and a1

1 = 0, a1

Table 5 Two players’ utility when the user is either adversarial or legitimate. Deﬁne rK
rK
2 1{θ1=θL

3 1{θ1=θH

1 } + rK

0 :=

1 } as the monitoring reward for two types of systems.
θ2 = θg
NOP
Monitor

Access
(rK
1 , rK
4 − rK
1 )
(rK
0 − cK , −rK
0 )

NOP
(0, 0)
(−cK , 0)

Access
4 , rK
(rK
4 )
(rK
4 − cK , rK
4 )

θ2 = θb
NOP
Monitor

NOP
(0, 0)
(−cK , 0)

Dynamic Bayesian Games for Adversarial and Defensive Cyber Deception

19

2 = 1 or not aK

Final Stage At the ﬁnal stage k = K, we use the Tennessee Eastman (TE)
Challenge Process [24] as an example to illustrate how attackers tend to com-
promise the sensors to cause physical damages (state deviation) of an industrial
plant and monetary losses. The user’s action is to get access to the sensor con-
troller aK
2 = 0, yet a user at diﬀerent levels of privilege xK
determines which sensors he can control in the TE process. If the attacker
changes the sensor reading, the system states such as the pressure and the
temperature may deviate from the desired value, which degrades the product
quality and even causes the shutdown of the entire process if the deviation
exceeds the safety threshold. Thus, the shutdown time, as well as the product
quality, can be used as the operating reward measure. By simulating the TE
process, we can determine the reward under the regular operation of the TE
4 (xK) as well as the reward under the compromised sensor read-
process rK
ings rK
1 are a function of the state xK. Assume the
attacker beneﬁts from the reward reduction under the attacking operation
1 (xK) and the system loss under attacks is higher than the moni-
4 (xK) − rK
rK
1 (xK) > cK > 0, ∀xK ∈ X K. On the other hand, the
toring cost rK
4 (xK) − rK
1 = 1 with a cost cK or
defender chooses to monitor the sensor controller aK
not to monitor aK
2 > cK > 0 because the
3 > rK
high-type system can collect more information from the monitoring data and
the beneﬁt outweighs the monitor cost.

1 = 0. Also, we assume rK

1 (xK). Both rK

4 and rK

5 Conclusion and Future Works

The area of cybersecurity is an uneven battleﬁeld. First, an attacker merely
needs to exploit a few vulnerabilities to compromise a system while a defender
has to eliminate all potential vulnerabilities. Second, the attacker has a plenty
of time to study the targeted system yet it is hard for the defender to predict
possible settings of attacks until they have happened. Third, the attacker can
be strategic and deceptive and the defender has to adapt to variations and
updates of the attacker. In this chapter, we aim to avoid the route of analyzing
every attacks and taking costly countermeasures. However, we endeavor to
tilt the unfavorable situation for the defender by applying a series of game
theory models to capture the strategic interactions, the multi-stage persistence,
as well as the adversarial and defensive cyber deceptions. Future directions
include a combination of the theoretical models with data from the simulated
or real system under attacks. The analysis of the game theory model provides a
theoretic underpinning for our understandings of cybersecurity problems. We
can further leverage the scientiﬁc and quantitative foundation to investigate
mechanism design problems to construct a new battleﬁeld that reverses the
attacker’s advantage and make the scenario in favor of the defender.

20

6 Exercise

Linan Huang, Quanyan Zhu

QA. Equilibrium Computation and Code Realization.

1. Write a bi-linear program to compute the PBNE of multi-stage game with
one-sided incomplete information, i.e., only the user has a type θ1 ∈ Θ1,
the defender does not have a type or P1 knows her type. Can you represent
it in a matrix form? (Hint: Corollary 1 in [11].)

2. Compute the mixed-strategy BNE for the static Bayesian game in Table 2
with unbiased belief b1(θb) = b1(θg) = 0.5. You can program it in Matlab
with the toolbox Yalmip2 and a proper nonlinear solver such as Fminicon3.
(Hint: PBNE degenerates to BNE when we take K = 0.)

QB. The Negative Information Gain in Game Theory. Let us consider a static
Bayesian game with the binary type space Θ = {θ1, θ2} and initial type belief
b1(θ1) = b1(θ2) = 0.5 as shown in Table 6. Player 1 is the row player and P2 is
the column player. Both players are rational and maximize their own utilities.

Table 6 A static Bayesian game under two possible types θ1 and θ2.

θ = θ1
A
B

a
(10,10)
(7,19)

b
(18,4)
(17,17)

θ = θ2
A
B

a
(10,10)
(14,18)

b
(18,18)
(20,20)

1. Compute the BNE strategy and the value of the game, i.e., each player’s
utility under the BNE strategy. (Hint: you should get a pure-strategy BNE
(B,b) and the value is (18.5, 18.5))

2. Suppose the type value is known to both players, determine the NE under

θ1 and θ2, respectively.

3. Compute the BNE with one-sided incomplete information, i.e., only P1
knows the type value, which is common knowledge. The term common
knowledge means that P1 knows the type, P2 knows that P1 knows the
type, and P1 knows that P2 knows that P1 knows the type, etc.

4. Compare the results in question 1-3, does more information always beneﬁt
the player with extra information? Can you give an explanation for this
negative information gain in the game setting?

2 https://yalmip.github.io/
3 https://www.mathworks.com/help/optim/ug/fmincon.html

Dynamic Bayesian Games for Adversarial and Defensive Cyber Deception

21

References

1. Aghassi, M., Bertsimas, D.: Robust game theory. Mathematical Programming 107(1-2),

231–273 (2006)

2. Axelsson, S.: Intrusion detection systems: A survey and taxonomy. Tech. rep., Technical

report (2000)

3. Chen, J., Zhu, Q.: Security investment under cognitive constraints: A gestalt nash equi-
In: Information Sciences and Systems (CISS), 2018 52nd Annual

librium approach.
Conference on, pp. 1–6. IEEE (2018)

4. Coppolino, L., D’Antonio, S., Romano, L., Spagnuolo, G.: An intrusion detection system
for critical information infrastructures using wireless sensor network technologies. In:
Critical Infrastructure (CRIS), 2010 5th International Conference on, pp. 1–8. IEEE
(2010)

5. Corporation, S.: Advanced persistent

threats: A symantec perspective.

URL

https://www.symantec.com/content/en/us/enterprise/white_papers/b-advanced_
persistent_threats_WP_21215957.en-us.pdf

6. Farhang, S., Manshaei, M.H., Esfahani, M.N., Zhu, Q.: A dynamic bayesian security
game framework for strategic defense mechanism design. In: Decision and Game Theory
for Security, pp. 319–328. Springer (2014)

7. Harsanyi, J.C.: Games with incomplete information played by bayesian players, i–iii

part i. the basic model. Management science 14(3), 159–182 (1967)

8. Hor´ak, K., Zhu, Q., Boˇsansk`y, B.: Manipulating adversary?s belief: A dynamic game
approach to deception by design for proactive network security. In: International Con-
ference on Decision and Game Theory for Security, pp. 273–294. Springer (2017)

9. Huang, L., Chen, J., Zhu, Q.: A large-scale markov game approach to dynamic protection
of interdependent infrastructure networks. In: International Conference on Decision and
Game Theory for Security, pp. 357–376. Springer (2017)

10. Huang, L., Zhu, Q.: Adaptive strategic cyber defense for advanced persistent threats
In: ACM SIGMETRICS Performance Evaluation

in critical infrastructure networks.
Review (2018)

11. Huang, L., Zhu, Q.: Analysis and computation of adaptive defense strategies against
advanced persistent threats for cyber-physical systems. In: International Conference on
Decision and Game Theory for Security (2018)

12. Jajodia, S., Ghosh, A.K., Swarup, V., Wang, C., Wang, X.S.: Moving target defense:
creating asymmetric uncertainty for cyber threats, vol. 54. Springer Science & Business
Media (2011)

13. Lei, C., Ma, D.H., Zhang, H.Q.: Optimal strategy selection for moving target defense

based on markov game. IEEE Access 5, 156–169 (2017)

14. Mahon, J.E.: The deﬁnition of lying and deception.

In: E.N. Zalta (ed.) The Stan-
ford Encyclopedia of Philosophy, winter 2016 edn. Metaphysics Research Lab, Stanford
University (2016)

15. Maleki, H., Valizadeh, S., Koch, W., Bestavros, A., van Dijk, M.: Markov modeling of
moving target defense games. In: Proceedings of the 2016 ACM Workshop on Moving
Target Defense, pp. 81–92. ACM (2016)

16. Manshaei, M.H., Zhu, Q., Alpcan, T., Bac¸sar, T., Hubaux, J.P.: Game theory meets
network security and privacy. ACM Computing Surveys (CSUR) 45(3), 25 (2013)
17. Miao, F., Zhu, Q., Pajic, M., Pappas, G.J.: A hybrid stochastic game for secure control

of cyber-physical systems. Automatica 93, 55–63 (2018)

18. Pawlick, J., Colbert, E., Zhu, Q.: A game-theoretic taxonomy and survey of defensive
deception for cybersecurity and privacy. arXiv preprint arXiv:1712.05441 (2017)
19. Pawlick, J., Colbert, E., Zhu, Q.: Modeling and analysis of leaky deception using sig-

naling games with evidence. arXiv preprint arXiv:1804.06831 (2018)

20. Pawlick, J., Zhu, Q.: Deception by design: evidence-based signaling games for network

defense. arXiv preprint arXiv:1503.05458 (2015)

21. Pawlick, J., Zhu, Q.: A Mean-Field Stackelberg Game Approach for Obfuscation Adop-
tion in Empirical Risk Minimization. arXiv preprint arXiv:1706.02693 (2017). URL
https://arxiv.org/abs/1706.02693

22

Linan Huang, Quanyan Zhu

22. Pawlick, J., Zhu, Q.: Proactive defense against physical denial of service attacks using
poisson signaling games. In: International Conference on Decision and Game Theory
for Security, pp. 336–356. Springer (2017)

23. Rass, S., Alshawish, A., Abid, M.A., Schauer, S., Zhu, Q., De Meer, H.: Physical in-
trusion games–optimizing surveillance by simulation and game theory. IEEE Access 5,
8394–8407 (2017)

24. Ricker, N.L.: Tennessee Eastman Challenge Archive. http://depts.washington.edu/

control/LARRY/TE/download.html (2013)

25. Xu, Z., Zhu, Q.: A Game-Theoretic Approach to Secure Control of Communication-
In: Proceedings of the 1st
Based Train Control Systems Under Jamming Attacks.
International Workshop on Safe Control of Connected and Autonomous Vehicles, pp.
27–34. ACM (2017). URL http://dl.acm.org/citation.cfm?id=3055381

26. Zhang, T., Zhu, Q.: Strategic defense against deceptive civilian gps spooﬁng of un-
manned aerial vehicles. In: International Conference on Decision and Game Theory for
Security, pp. 213–233. Springer (2017)

27. Zhu, Q., Ba¸sar, T.: Game-theoretic approach to feedback-driven multi-stage moving
target defense. In: International Conference on Decision and Game Theory for Security,
pp. 246–263. Springer (2013)

28. Zhu, Q., Clark, A., Poovendran, R., Basar, T.: Deployment and exploitation of deceptive
honeybots in social networks. In: Decision and Control (CDC), 2013 IEEE 52nd Annual
Conference on, pp. 212–219. IEEE (2013)

29. Zhu, Q., Rass, S.: On multi-phase and multi-stage game-theoretic modeling of advanced

persistent threats. IEEE Access 6, 13958–13971 (2018)

30. Zhuang, J., Bier, V.M., Alagoz, O.: Modeling secrecy and deception in a multiple-period
attacker–defender signaling game. European Journal of Operational Research 203(2),
409–418 (2010)

