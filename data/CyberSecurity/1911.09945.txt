9
1
0
2

v
o
N
2
2

]

R
C
.
s
c
[

1
v
5
4
9
9
0
.
1
1
9
1
:
v
i
X
r
a

Insider threat modeling: An adversarial risk analysis
approach

Chaitanya Joshi, Jesus Rios and David Rios

Abstract

Insider threats entail major security issues in geopolitics, cyber risk management and busi-
ness organization. The game theoretic models proposed so far do not take into account some
important factors such as the organisational culture and whether the attacker was detected or
not. They also fail to model the defensive mechanisms already put in place by an organisation
to mitigate an insider attack. We propose two new models which incorporate these settings
and hence are more realistic. We use the adversarial risk analysis (ARA) approach to ﬁnd
the solution to our models. ARA does not assume common knowledge and solves the problem
from the point of view of one of the players, taking into account their knowledge and uncer-
tainties regarding the choices available to them, to their adversaries, the possible outcomes,
their utilities and their opponents’ utilities. Our models and the ARA solutions are general
and can be applied to most insider threat scenarios. A data security example illustrates the
discussion.

1 Modeling Insider Threat

Insider threats are encountered in areas such as international security, geo-politics, business, trade
and cyber security. They are widely perceived to be signiﬁcant ([Schulze, 2018], [Ware, 2017]),
and even often considered to be more damaging and likely than outsider attacks ([Schulze, 2018],
[CERT, 2012]). Moreover, it is feared that the impact of insider threats actually known is only
the tip of an iceberg, as many organizations are choosing not to report such incidents unless
required to do so by law ([Wood et al., 2016]): as described in [Hunker and Probst, 2009], it is a
problem area in which little data is available, specially in the cyber security domain. Protection
from insider threats is challenging as the perpetrators might have access to sensitive resources and
privileged system accounts. Solutions to insider threat problems are considered to be complex
([Lee and Rotoloni, 2015]): technical solutions do not suﬃce since these threats are fundamentally
a people issue, as thoroughly discussed in [Sarkar, 2010] and [Greitzer et al., 2012].

In its simplest form, it is natural to view the insider threat problem as a two player game. We
may call the ﬁrst player the defending organization or the defender (which could refer to a single
business or military unit or a similar entity, but also to a whole country or a coalition of entities or
countries) and the second one, the malicious insider or attacker (which could refer to one or more
employees, contractors, or persons who have signiﬁcant access to the organization and have been
trusted with such access). A typical scenario would be as follows: since insider threats are a well-
known phenomena, it will frequently be the case that several measures would have already been
implemented by the organization (at least, if it is suﬃciently mature) to prevent or deter insider
attacks. As an example, [Silowash et al., 2012] provides a catalogue of best practices against
insider threats in the cybersecurity domain. The insider will typically be aware of the measures
in place and plans an attack accordingly. Once the attack has been carried out and detected, the
organization will undertake actions to end the attack and mitigate any damage caused, possibly
based on the resources deployed at the ﬁrst stage. This type of interactions have been named
sequential Defend-Attack-Defend games, e.g. in [Brown et al., 2006].

It is therefore natural that game-theoretic models of insider threats have been explored. For
example, [Liu et al., 2008] model the problem as a two-player, zero-sum dynamic game. At each
discrete time point, both players make decisions resulting in a change of state and opposite (given
the zero-sum property) rewards to them. The authors then look for Nash equilibria (NE). While
they model a stochastic game, they assume that both the defender and the attacker have com-
plete knowledge of each other’s beliefs and preferences, which is hardly the case in applications.
Moreover, this model is oversimpliﬁed in several respects. For example, there could be multiple

1

 
 
 
 
 
 
attackers, the attacker pay-oﬀs might not be immediate to obtain and the game might not be
zero-sum. Also, in most cases, the defender would have already employed measures to prevent an
insider attack and, therefore, the problem should be modeled as a sequential Defend-Attack-Defend
game, rather than as a simultaneous one.

A more realistic approach is described in [Kantzavelou and Katsikas, 2010] who consider an
insider threat problem in cybersecurity, trying to model the continuous interactions between an
intruder and an intrusion detection system (IDS). They assume bounded rationality on the agents,
assess outcomes through utilities and use quantal response equilibria instead of standard NE.
However, their model focuses on a particular application and is not immediately generalizable.
Moreover, the game does not consider multiple players and carries on even after detecting an
attack. This is because they assume that detection causes the attack to be stopped, but does not
eliminate the attacker from the game. [Tang et al., 2011] also model insider threats to IT systems
considering bounded rationality, combining game theory with an information fusion algorithm to
improve upon traditional IDS based methods by being able to consider various types of information.
To combat Advanced Persistent Threats (APT) coupled with insider threats, [Feng et al., 2015]
and [Hu et al., 2015] propose three (defender, APT attacker and insider) player games. They
employ a two layer game and show the existence of NE.

Note that, non-game theoretic approaches have also been used to model insider threats. See, for
example, [Martinez-Moyano et al., 2008] who employ a system dynamic approach, [Brdiczka et al., 2012]
who propose an approach that combines structural anomaly detection and psychological proﬁl-
ing and [Axelrad et al., 2013] who use a Bayesian network approach using a structural equations
model. However, the focus of this paper is on game theoretic modeling and for that reason we do
not consider non-game theoretic approaches in further detail.

In an insider attack, sometimes it may be diﬃcult to detect who the attacker is since they
already have access to the organisation, its premises, IT systems, etc. The malicious insider may
be motivated to remain undetected not only to avoid punishment, loss of reputation, etc., but
also to continue to keep causing harm or gain beneﬁt, whatever the case may be. In the wake of
the attack, an organisation would typically take actions, including improving their processes and
defensive systems so that a similar attack is unlikely to succeed again in the future. However, in
some cases, if the attacker has not been detected, then it may be very diﬃcult to prevent a future
attack regardless of any additional measures put in place. As an example, suppose that a malicious
insider has shared sensitive information with a competitor and while the passing of information
has been detected, the investigation has failed to identify the attacker.
In this case, although
the organisation could try and make it diﬃcult for such information to be shared externally in
the future, it may be quite diﬃcult or impossible to guarantee that such sharing will not actually
happen. Attacker detection is therefore an important consideration in the defender decision making
and should be taken into account in a model. However, none of the game theoretic models proposed
so far have taken this information into account.

The presence of a security culture has been identiﬁed as an important factor in preventing
insider threat ([Choi et al., 2018], [Safa, 2017], [Sarkar, 2010]). Moreover, it is widely accepted
that having an unfavourable organisational culture will lead employees to override security poli-
cies and processes increasing the chances of insider attacks being successful ([Probst et al., 2010],
[Sarkar, 2010], [Colwill, 2009]). Further, the right culture could enhance levels of underlying per-
sonal trust, loyalty and mutual dependency ([Colwill, 2009]) thus reducing the chances of an insider
attack being launched and also of it being successful if launched. Yet the game theoretic models
for insider threat proposed so far do not either take into account the culture in the organisation.
An insider threat problem could be a multi-player game with possibly, multiple attackers and
defenders. This could either be because of a group of attackers working together to harm the
organisation or attackers acting independently and without the knowledge of each other, and,
[Liu et al., 2008] acknowledge that we typically face a multi-player
similarly, for the defenders.
game with multiple attackers and defenders in this domain. However, because that is hard to
solve, they club all attackers as a single player and assume that all defenders are coordinated, thus
simplifying to a 2-player game. This could be a reasonable modeling approach since if the attackers
are working in a team, then they could be considered to eﬀectively conform a single entity and
if they are independent individual attackers, each of those attacks could be modeled separately.
However, an insider threat could also be a multi-player game due to the presence of diﬀerent types
of attackers. It is well known that not all attackers are malicious, many are inadvertent. Some
of the previous research has focused on segmenting the employees. For example, [Liu et al., 2009]

2

provide a segmentation with inadvertent and malicious insiders. However, the exact segmentation
(good employees, inadvertent, malicious and so on) could vary from organisation to organisation.
Also, the same employees are also the defenders and each employee will be diﬀerently eﬀective as
a defender based on the type of the employee and also the role they play inside the organisation.
Finally, the exact mix of employees that a malicious insider might come across and how important a
role each of them play in mitigating a malicious attack will vary in every situation. Therefore, it is
impossible to model every possible scenario using a segmentation approach or indeed a multi-player
approach.

Insider threat is a well known phenomenon and it is reasonable to expect that an organisation
would have certain defensive measures in place to prevent an insider threat. Yet, the models pro-
posed so far do not take into account the existing defensive measures or the eﬀects they may have in
either the prevention or the detection of an attack. For example, [Kantzavelou and Katsikas, 2010]
propose a sequential game in which the attacker performs ﬁrst. They do not acknowledge the
existence of defensive processes in place to prevent insider attacks. However, modeling the ini-
tial defensive measures is important since it is well known ([Moore et al., 2015], [Liu et al., 2009],
[Martinez-Moyano et al., 2008]) that such measures could impact the organisational culture includ-
ing unintended negative consequences. While such measures may improve physical security or the
security culture (as intended), they could also make employees feel not trusted or being intruded
upon. The latter case may lead to a culture of mistrust and may cause employees to not follow the
processes or ﬁnd ways to get around them thus increasing the chances of an insider attack success.
Ours is the ﬁrst paper to model consider the initial defensive mechanism and model the insider
threat as a defence-attack-defence game.

Indeed, this paper makes two main contributions. Firstly, it proposes two novel, more realistic,
defend-attack-defend models for the general insider threat problem: one accounts for whether the
attacker has been detected or not; the second one, that, in addition, also takes into account the
organisation culture, even accommodating the possibility that the attack was prevented altogether.
Secondly, we solve these models using the Adversarial Risk Analysis (ARA) approach thus ensuring
that the models do not make any unrealistic common knowledge assumptions, only take into
consideration the information that is likely to be available to the defender and are practicably
solvable. When modeling the insider threat, our focus is on modeling the agents’ interactions and
not on the (anomaly) detection or any other aspect. Speciﬁcally, we focus on ﬁnding the optimal
initial defensive mechanism that the defender should install to mitigate the insider threat and the
optimal defensive action that the defender should take in the wake of an insider attack given the
initial defensive action, the attack and its outcome.

The structure of the paper is as follows. In Section 2 we provide a brief overview of ARA,
show why a solution almost always exists and illustrate how to ﬁnd Monte Carlo solutions to the
problems. Next, we propose our ﬁrst new model and show how it can be solved using ARA. We
propose our second new model in Section 4 and show how an ARA solution for that problem can
be found. In Section 5, we use a numerical example to illustrate how both of the proposed models
could be used to deal with an insider threat problem. Finally, in Section 6 we summarise and
discuss the challenges in implementing ARA and brieﬂy highlight the scope for further work.

2 Adversarial Risk Analysis

While game theory has been the typical choice to model interactions between two or more strategic
adversaries, limitations of such theory have been pointed out, focusing on the common knowledge
assumption and the conservative nature of its solutions, e.g. [Gintis, 2009], [Camerer, 2003], or
[Raiﬀa et al., 2002]. Bayesian game theory ([Harsanyi, 1967]) can be used to model games with
imperfect information by eliciting prior distributions on diﬀerent types of opponents. However,
this approach requires that the prior distributions elicited by each player be commonly known.
This assumption is unrealistic and while methods have been proposed to implement Bayesian
game theory without making the common prior assumption (for example, [Antos and Pfeﬀer, 2010],
[Sákovics, 2001]), these have not caught on. The main challenge in game theory is that it aims
to ﬁnd solution for all the players and moreover this solution needs to be an equilibrium solution.
This makes it increasingly diﬃcult to ﬁnd solutions as the models get more realistic and complex.
On the other hand, while conventional risk analysis does not assume common knowledge and
solves the problem only for one of the players, it cannot model the strategic thinking of an intelligent

3

adversary. Limitations of conventional risk analysis in security have been pointed out as well,
[Cox, 2009] and [Brown and Cox, 2011].

Adversarial risk analysis (ARA) ([Insua et al., 2009]) was proposed to address the shortcomings
of both the game theory and risk analysis approaches. ARA does not assume common knowledge
and solves the problem from the point of view of just one of the players, taking into account their
knowledge and uncertainties regarding the choices available to them, to their adversaries, the pos-
sible outcomes, their utilities and their opponents’ utilities. ARA takes into account the expected
utilities for the defender as well as the random expected utilities for the opponents, incorporating
uncertainty regarding their strategic reasoning. Since its introduction, it has been used to model
a variety of problems such as network routing for insurgency ([Wang and Banks, 2011]), inter-
national piracy ([Sevillano et al., 2012]), counter-terrorism ([Rios and Insua, 2012]), autonomous
social agents ([Esteban and Insua, 2014], urban security resource allocation ([Gil et al., 2016]), ad-
versarial classiﬁcation ([Naveiro et al., 2019]), counter-terrorist online surveillance ([Gil and Parra-Arnau, 2019]),
cyber-security ([Rios Insua et al., 2019]).

One of the distinguishing aspects of ARA compared to game theory is that, ARA only solves
the problem for one of the players (typically, the defender). Unlike game theory, it does not aim to
ﬁnd an equilibrium solution for all the players. Since ARA aims to ﬁnd a solution that is optimal
only for one of the players, solving ARA is relatively easy. In fact, as we show below, solving ARA
essentially involves taking expectations (using MC integration) and ﬁnding the maximum over a
low-dimensional space (often these spaces are discrete, at least on some of the dimensions) and
therefore, a solution almost always exists.
Further, unlike game theory, ARA only considers the information available to the player and their
uncertainties when solving the problem. It does not have to consider the information available to
each player about the rest of the players. Thus for a basic n- player simultaneous game, ARA
would only require eliciting (n − 1) probability distributions, but (Bayesian) game theory would
require eliciting n × (n − 1) probability distributions ([Liu et al., 2008]).

To illustrate how a basic game can be modeled using ARA, we consider a two player simulta-
neous game between a defender D (she) and an attacker A (he). Figure 1[a] presents the problem
using a bi-agent inﬂuence diagram (BAID) where decisions are represented by square nodes, un-
certainties with circular nodes and utilities with hexagonal nodes. Nodes corresponding solely
to D are not shaded; those corresponding exclusively to A are diagonally shaded; ﬁnally, shared
chance nodes are shaded using horizontal dashed lines. Suppose we are solving the problem for
the defender D, Figure 2[b] represents the BAID from her point of view. The only diﬀerence here
is that A is also a random node now since D is not certain as to what action A would take. Let
D, A and S denote the set of all possible actions for the defender, the set of all possible actions for
the attacker and the set of all possible outcomes, respectively.

Figure 1: [a] BAID for a two player simultaneous game and [b] BAID from the point of view of
just the defender D

The basic idea behind ﬁnding a standard ARA solution is to integrate out the uncertainties at
the random nodes and maximize the expected utility at the decision nodes. In this problem, D
has to consider two random nodes, A and S. ARA can be solved using backward induction: we
start from the ﬁnal (random/decision) node and move backwards. The defender will try to ﬁnd
her best action as follows.

First, the defender needs to ﬁnd her expected utility by taking into account the uncertainty

4

due to the random outcome s as

ψD(d, a) =

(cid:88)

s∈S

uD(d, a, s)pD(s|d, a) for all d ∈ D, a ∈ A.

(1)

Next, she needs to ﬁnd her expected utility by considering the uncertainty in the random action
node A as

ψD(d) =

ψD(d, a)pD(a).

(cid:88)

Finally, she can ﬁnd her optimal action d∗ as

a∈A

d∗ = arg max
d∈D

ψD(d).

(2)

(3)

This requires the defender to elicit the uncertainties in the random nodes using probability distri-
butions pD(a) and pD(s|d, a). Of these, pD(s|d, a) can typically be elicited based on her knowl-
edge/experience and/or her subjective beliefs. She can choose to elicit pD(a) also using her beliefs,
knowledge or past experience. However, she can also choose elicit it by modeling the strategic
thinking of her adversary. For example, she may believe that like her, A is also an expected utility
maximizer, who would choose the action a∗ maximizing his expected utility. She can determine
a∗ by solving Equations (1) - (3) for the attacker using his utility function uA(d, a, s) and his
probabilities pA(d) and pA(s|d, a) instead. However, these will typically be unavailable to her. She
can account for her uncertainty in the utility and the probabilities of the attacker, eliciting random
utility and random probabilities UA(d, a, s), PA(d) and PA(s|d, a). She can then ﬁnd the random
expected utility for the attacker through

ΨA(a) =

(cid:32)

(cid:88)

(cid:88)

d

s

UA(d, a, s)PA(s|d, a)

PA(d) .

(cid:33)

Then, she can ﬁnd his random optimal action A∗ as

A∗ = arg max
a∈A

ΨA(a).

(4)

(5)

Finally, once the defender assesses A∗, her predictive distribution about the attack’s action, she
is able to solve her decision problem. We have assumed in our notation above that the sets of
actions D and A for the defender and the attacker respectively as well as the set of outcomes S
are discrete. If any of these sets were continues we would have integrals instead of summations.

If computing expectations using integrals over probability density functions in Equations (1)
and (2) or maximizing a univariate function in Equation (3) cannot be performed analytically, we
can always approximate them using Monte Carlo methods.

Monte Carlo algorithm to solve the defender’s problem

1. For each d in a grid {d1, . . . , dn} ⊂ D:

(a) For k = 1, . . . , N,

sample ak ∼ pD(a)
sample sk ∼ pD(s|d, ak)
compute ψk
(cid:80)N

D(d) = uD(d, ak, sk)
k=1 ψk

D(d)

(b) ˆψD(d) = 1
N

2. Find d∗ = arg max ˆψD(di), 1 ≤ i ≤ n .

When pD(a) is elicited using Equations (4) and (5), then it can also be approximated using
Monte Carlo methods. For this, one must elicit the probability distributions for the random
probabilities PA(d), PA(s|d, a), and random utility UA(d, a, s).

5

Monte Carlo algorithm to elicit pD(a)

1. For k = 1, . . . N :

(a) Sample attacker’s probabilities and utility

uk
A(d, a, s) ∼ UA(d, a, s)
pk
A(s|d, a) ∼ PA(s|d, a)
pk
A(d) ∼ PA(d)
(b) Calculate ψk
(c) Find a∗

A(a) = (cid:80)
k = arg max ψk

(cid:0)(cid:80)

s uk
d
A(a) ∼ A∗

A(d, a, s)pk

A(s|d, a)(cid:1) pk

A(d) ∼ ΨA(a).

2. If A is discrete

approximate pD(a) = P r(A∗ = a) by 1
N

else, if A is continuous

approximate pD(a) = P r(A∗ ≤ a) by 1
N

(cid:80)N

k=1 1(a∗

k = a), for all a ∈ A ,

(cid:80)N

k=1 1(a∗

k ≤ a), for any give a ∈ A .

We have assumed that the attacker is an expected utility maximizer to assess pD(a). However,
this probability can also be assessed assuming the attacker uses a number of alternative solution
concepts other than expected utility maximization. For example, the defender could solve the
problem by assuming that the attacker is a non-strategic player or that he is level-k thinker or uses
NE to determine his optimal action and so on. An ARA solution can be found for each of these
solution concepts, see [Banks et al., 2015] for details.

3 An ARA Defend-Attack-Defend model

We start with a Defend-Attack-Defend model to deal with the insider threat problem, which
considers a defender D (the organisation, she) and an attacker A (the insider, he). Our model
is based upon the graphical framework described in [Banks et al., 2015]. Figure 2 presents the
problem using a bi-agent inﬂuence diagram (BAID) where decisions are represented by square
nodes, uncertainties using circular nodes and utilities with hexagonal nodes. Nodes corresponding
solely to D are not shaded; those corresponding exclusively to A are diagonally shaded; and, ﬁnally,
shared chance nodes are shaded using horizontal dashed lines. Dashed arrows indicate that the
involved decisions are made with the corresponding agent knowing the values of the preceding
nodes, whereas solid arrows indicate probabilistic or value dependence of the corresponding node
with respect to its predecessors.

Figure 2: BAID for the Defend-Attack-Defend insider threat game

The action and outcome sets are as follows.

Initially, the defender must choose one of the
portfolios of preventive measures d1 in set D1. Having observed the portfolio implemented, the
insider will adopt one of the actions a in A; this set could consist of either ’no attack’ or ’attack’

6

decisions or diﬀerent types/intensities of attacks or other attack options. The set S consists of
the possible outcomes s that can occur as a result of the preventive portfolio d1 and the attack a
undertaken. It is possible that the identity of the attacker remains undetected even after observing
s. Node E represents the event of whether the attacker was detected or not. Once E has been
observed, the organization will choose to carry out one of the actions d2 in D2 to end the attack,
limit any damage and possibly pre-empt future attacks leading to the ﬁnal outcomes of both agents,
respectively evaluated through their utility functions uD and uA. Note that all three sets D1, A
and D2 could contain a do nothing action.

For its solution, the defender must ﬁrst quantify the following:

1. The distribution pD(a|d1) modeling her beliefs about the attack a chosen at node A by the

employee, given the chosen defense d1.

2. The distribution pD(s|d1, a) modeling her beliefs about the outcome s of the attack, given a

and d1.

3. The distribution pD(e|d1, a, s) modeling her beliefs about whether or not the attacker is

detected given d1, a and s.

4. Her utility function uD(d1, s, e, d2) which evaluates the consequences associated with their
ﬁrst (d1) and second (d2) defensive actions as well as the outcome s and whether the attacker
was detected or not. It also includes any future consequences of the action d2 as assessed by
the defender when choosing d2.

Given these assessments, the defender ﬁrst seeks to ﬁnd the action d∗
utility

2(d1, s, e) maximizing her

d∗
2(d1, s, e) = arg max
d2∈D2

uD(d1, s, e, d2),

(6)

leading to the best second defense when the ﬁrst one was d1, the outcome was s and the detection
result, e. She seeks to compute the expected utility ψD(d1, a) for each (d1, a) taking into account
the uncertainty in e and s s is deﬁned through

ψD(d1, a) =

(cid:32)

(cid:88)

(cid:88)

s

e

uD(d1, s, e, d∗

2(d1, s, e)) pD(e|d1, a, s)

pD(s|d1, a).

(7)

(cid:33)

Moving backwards, she computes her expected utility for each d1 ∈ D1 using the predictive distri-
bution pD(a|d1) through

ψD(d1) =

ψD(d1, a)pD(a|d1).

(8)

(cid:88)

a

Finally, the defender ﬁnds her maximum expected utility decision d∗
backward induction shows that the defender’s optimal strategy is to ﬁrst choose d∗
having observed s and e, choose d∗
1, s, e).

1 = arg maxd1∈D1 ψD(d1). This
1 and, then, after

2(d∗

The above analysis requires the defender to elicit pD(a|d1). This can be done using risk anal-
ysis based approaches as in [Ezell et al., 2010] or by modeling the strategic analysis process of
the insider. For this, the defender should model the insider’s strategic analysis by assuming that
he will perform an analysis similar to hers to ﬁnd his optimal attack a∗. To do so, she should
assess his utility function uA(a, s, e, d2) and probability distributions pA(e|d1, a, s), pA(s|d1, a) and
pA(d2|d1, a, s, e). However, since the corresponding judgments will not be available to the defender,
we could model her uncertainty about them through a random utility function UA(a, s, e, d2) and
random probability distributions PA(e|d1, a, s), PA(s|d1, a) and PA(d2|d1, a, s, e). There are multi-
ple ways for the defender to elicit these random utilities and probabilities using expert judgments,
for example, by using the ordinal judgment procedure by [Wang and Bier, 2013] or as outlined
in [Ríos Insua et al., 2019], expect perhaps for PA(d2|d1, a, s, e). The elicitation of such random
probability distribution may require the defender to think about how the attacker analyzes her
decision problem at D2, leading to a next level of recursive thinking. There are also several ways to
deal with this hierarchy of recursive analysis when eliciting such random probability distributions
over strategic decisions as discussed in [Rios and Insua, 2012].

Once these random quantities are elicited, the defender solves the insider’s decision problem
using backward induction. This is done following a process similar to how they solved their own

7

decision problem taking into account the randomness in judgments. First, the defender ﬁnds the
random expected utility for each d2 ∈ D2

ΨA(d1, a, s, e) =

(cid:88)

d2

UA(a, s, e, d2)PA(d2|d1, a, s, e).

(9)

Then, the defender ﬁnds the random expected utility taking into account the uncertainty about
whether the insider is detected or not

ΨA(d1, a, s) =

(cid:88)

e

ΨA(d1, a, s, e)PA(e|d1, a, s).

Next, they ﬁnd the random expected utility for each pair (d1, a)

ΨA(d1, a) =

(cid:88)

s

ΨA(d1, a, s)PA(s|d1, a),

and compute the random optimal attack A∗(d1) given the defense d1

A∗(d1) = arg max
a∈A

ΨA(d1, a).

Finally, once the defender assesses the desired conditional predictive distribution through

pD(a|d1) = P r(A∗(d1) = a),

(10)

(11)

(12)

(13)

she is able to solve her decision problem and obtain d∗
2 by solving Equations (6) to (8).
Similarly, pD(a|d1) can also be approximated in practice using Monte Carlo methods. In Section
5, we illustrate this with a numerical example.

1 and d∗

Note that in the above analysis, we have assumed that all the involved quantities are discrete.
Should some of the other quantities be continuous, the sums would be replaced by corresponding
integrals.

4 An ARA model incorporating Organisation Culture

The sequence of interactions between an organization and its employees could be more com-
plex for various reasons. Firstly, it has been described ([Moore et al., 2015], [Liu et al., 2009],
[Martinez-Moyano et al., 2008]) that the measures in D1 can have unintended negative conse-
quences. If the employees feel that the measures introduced to mitigate insider threats are intru-
sive or micro-managing, or even aggressive, that could lead them to react in unintended ways; this
could include not reporting suspicious activities or misusing reporting processes, either accidentally
or intentionally. At worst, it could even motivate an employee to go rogue. Secondly, although we
have treated the employees as a single entity, in reality, this group could include a large number
of people and, therefore, the organization may be facing multiple actors taking diﬀerent actions
(mostly good, but some inadvertently harmful and some even malicious). Usually, a majority of
employees will not take any action that would harm the organization; in fact, some of them would
actively help prevent insider attacks. For example, one of the possible actions could be to correctly
follow the processes or measures set out by the organization, possibly resulting in the successful
prevention of attacks. Finally, actions by diﬀerent types of employees could be dependent (sequen-
tial) or independent (simultaneous). Since the number of employees and the exact nature of their
actions will vary, it is not possible to provide a general solution to the problem by modelling this
as a multi-player game. Some of the previous research has focused on segmenting the employees.
For example, [Liu et al., 2009] provide a segmentation with inadvertent and malicious insiders. In-
stead, we propose to model the culture in the organisation. For simplicity, we classify the culture
in the organisation C as good or not so good. We deﬁne good culture as the one in which a majority
of the employees correctly and promptly perform their duties including following any procedures
to prevent insider attacks. As a result there is a positive, productive and vigilant culture in the
organisation. Any suspicious activity will be promptly reported and investigated. As a result, it
will be relatively diﬃcult to carry out an insider attack. On the other hand, the not so good culture
refers to an environment in which, at least some of the employees will take either deliberate or
inadvertent actions creating a culture of mis-trust. This, in turn, could lead the employees to not

8

feel safe to report suspicious activities and even potentially motivate some to go rogue and plan
an insider attack. Since the culture will change with space and time, we consider C as a random
event. C may or may not prevent (block) and insider attack from going ahead. We deﬁne this as
a random outcome B, that can take two values: attack prevented or not prevented. If the attack is
prevented then the game ends since there is no attack. On the other hand, if it is not prevented,
then the attacker A will carry out an attack that will result in an outcome from the set S. The
game will conclude by observing events E and D2 as deﬁned in Section 3. The decision tree for
this game is shown in Figure 3 and the BAID (Bi-agent inﬂuence diagram) in Figure 4.

Figure 3: Decision tree for the the game in Section 4.

Figure 4: BAID for the game in Section 4.

To ﬁnd the ARA solution for this game, the defender must ﬁrst quantify the following.

1. Her predictive distribution pD(c|d1) about the culture in the organisation given the imple-

mented defenses d1.

2. Her predictive distribution pD(b|d1, c) about the outcome of such a culture (whether it can

prevent an insider attack or not), given c and d1.

3. Her predictive distribution pD(a|d1, b = not prevented) about the attack that will be chosen

at node A given the outcome b was not prevented and action d1.

4. Her predictive distribution pD(s|a, d1, b = not prevented) about the outcome of the attack,
given the attack a, the defenses d1 implemented and given that the outcome b was not
prevented.

5. Her predictive distribution pD(e|d1, a, s) about the detection of the attacker, given the actions

d1 and a and the outcome s.

6. The utility function uD(d1, b, s, e, d2) given their ﬁrst and second defensive actions, the out-
come b of the organisational culture, the outcome s of the attack and the eventual detection
e of the attacker.

9

Given these, the defender works backwards along the decision tree in Figure 3. First, she seeks to
ﬁnd her optimal second defensive action d∗

2(d1, b, s, e) by maximizing her utility

d∗
2(d1, b, s, e) = arg max
d2∈D2

uD(d1, b, s, e, d2).

(14)

Then, for each (d1, b, a) ∈ D1 × B × A, she seeks to compute her expected utility by taking into
account the uncertainty in E and S, as follows

ψD(d1, b, a) =

(cid:32)

(cid:88)

(cid:88)

s

e

uD(d1, b, s, e, d∗

2)pD(e|d1, a, s)

pD(s|d1, b, a);

(15)

(cid:33)

next she computes her expected utility ψD(d1, b) by considering the uncertainty in A, through

ψD(d1, b) =

(cid:88)

a

ψD(d1, b, a)pD(a|d1, b),

and, then, integrate out the uncertainty in b and c through

ψD(d1) =

(cid:32)

(cid:88)

(cid:88)

c

b

ψD(d1, b)pD(b|d1, c)

pD(c|d1),

(cid:33)

(16)

(17)

to ﬁnd her expected utility ψD(d1) for each d1. Finally, she can ﬁnd her initial decision of maximum
utility through d∗
1 = arg maxd1∈D1 ψD(d1). This backward induction shows that the defender’s
optimal strategy is to ﬁrst choose d∗
1 and then, after having observed b, a, s and e, choose action
2(d∗
d∗

1, b, s, e).
The above analysis requires the defender to elicit pD(a|d1, b), which is not straightforward even
when the attack could only go ahead if b = not prevented. This is due to its strategic nature.
The defender could model the insider’s strategic analysis process by assuming that he will perform
an analysis similar to hers to ﬁnd his optimal attack A∗. To elicit it, the defender must assess
UA(a, s, e, d2), PA(s|d1, b, a), PA(e|d1, a, s) and PA(d2|d1, a, s, e), as discussed in Section 3. Once
elicited, the defender solves the attacker’s decision problem using backward induction, similar to
how they solved their own decision problem. First, the defender integrates out action d2 computing
the random expected utilities

ΨA(d1, a, s, e) =

(cid:88)

d2

UA(a, s, e, d2)PA(d2|d1, a, s, e).

(18)

Then, she ﬁnds her random expected utilities for the attacker taking into account the uncertainty
in e,

ΨA(d1, a, s) =

ΨA(d1, a, s, e)PA(e|d1, a, s).

(cid:88)

Finally, they integrate out s ∈ S to obtain

e

ΨA(d1, a, b) =

(cid:88)

s

ΨA(a, d1, s)PA(s|a, d1, b),

and compute the attacker’s random optimal action

A∗(d1, b) = arg max
a∈A

ΨA(d1, a, b).

This produces the defender’s desired predictive distribution about the attack

pD(a|d1, b) = P r(A∗(d1, b) = a)

(19)

(20)

(21)

(22)

Again, as in Section 3, we have assumed that all quantities are discrete. Should some of the
In practice,
In

quantities be continuous, the corresponding sums would be replaced by integrals.
d∗
1, d∗
Section 5, we illustrate this using a numerical example.

2 and PD(a|d1, b) can be obtained using Monte Carlo methods, as shown in Section 2.

10

5 Example

We consider an insider threat scenario motivated by [Martinez-Moyano et al., 2008] in which the
malicious insider attempts to harm the incumbent organization without getting caught. The
organization focuses on information/data collection and needs to protect itself. It already has its
sites and IT systems protected so that only authorized personnel are able to access them. However,
anticipating attacks, the organization is considering implementing an additional security layer to
defend itself. The defensive actions in D1 under consideration are

1. anomaly detection/data provenance tools;

2. information security measures and employee training; and

3. carrying out random audits.

The malicious insider’s aim could be ﬁnancial fraud, data theft, espionage or whistle blowing.
Regardless of the exact nature of the attack, we assume that the attacker’s options in A refer to its
scale, say small, medium or large. For simplicity, we assume that the attack will either fully succeed
or fail at S. Once the attack has been carried out, the attacker may or may not be detected, an
event represented by E. Upon observing E, the organization can choose to carry out one of the
following defensive actions in D2:

1. major upgrade of defenses;

2. minor upgrade of defenses; or

3. no upgrade.

5.1 Solution using the defend-attack-defend model

We ﬁrst analyze the problem using the model in Section 3. We start by assessing the defender’s
utility function uD(d1, s, e, d2). We assume for simplicity that such function additively aggregates
the marginal utilities associated, respectively, with her initial defense action d1, the outcome of
the attack and whether this is detected or not (s, e), and her recovery action d2 through

uD(d1, s, e, d2) = u(d1) + u(s, e) + u(d2).

(23)

These marginal utilities are given in Tables 1 and 2. They are estimated in a common scale from
-100 to 100. The utility uD can then be computed for each combination; for example, uD(d1 =
random audit, s = not successful, e = detected, d2 = no upgrade) = −50 + 100 − 100 = −50.
Observe that while u(d1) measures the relative utilities of the monetary costs associated with each
initial defensive action (therefore the negative values), u(d2) will, in itself, combine the defender’s
cost and expected beneﬁts associated with her recovery action d2. This amounts to considering
her expected utility for the consequences after her decision at D2. For instance, in this example,
the defender seems to think that Minor upgrade will be the best recovery action d2 considering
the investment cost and the expected beneﬁts. In real life, u(d2) will likely depend on s and e,
that is, it would be u(d2|s, e). Because, u(Major upgrade| Successful, Not detected ) will likely be
diﬀerent from u(Major upgrade| Not Successful, Detected ), for example. However, here, for the
sake of simplicity we do not consider this dependence when eliciting u(d2). In order to implement

Table 1: Marginal utilities associated with defensive actions d1 and d2.

d1
Anom. det. & Data prov.
Info. Sec.& train.
Random audits

d2

u(d1)
-100 Major upgrade
-60 Minor upgrade
-50 No upgrade

u(d2)
0
25
-100

the ARA solution, the defender must ﬁrst identify the action d∗
In this case, d∗
2 turns out to be ‘Minor upgrade’.

2(d1, s, e) maximizing their utility.

Then, she must compute her expected utility ψD(d1, a) using Eq. (7). This requires pD(e|d1, a, s)
and pD(s|d1, a). Suppose that her elicited probabilities pD(e|d1, a, s) are as shown in Table 3. We

11

Table 2: Marginal utility for every (s, e) combination.
e

s
Success Yes
Success No
Yes
Fail
No
Fail

u(s, e)
50
-100
100
0

Table 3: Defender’s elicited probabilities pD(E = Yes|d1, a, s) for every (d1, a, s).

D1
Anom.det. & prov.
Info.sec. & train.
Random audits

S = Success
A =
Small Med. Large
0.7
0.4
0.1

0.8
0.5
0.1

0.6
0.3
0.1

S = Fail
A =
Small Med. Large
0.8
0.5
0.1

0.9
0.6
0.1

0.7
0.4
0.1

Table 4: Defender’s elicited probabilities pD(S = successful |d1, a) for every (d1, a).

D1
Anom. det. & Data prov.
Info. sec. & train.
Random audits

A = small A = med. A = large
0.05
0.2
0.3

0.07
0.25
0.4

0.1
0.3
0.5

Table 5: Computed ψD(d1, a) for every (d1, a) combination.

D1
Anom. det. & Data prov.
Info. sec. & train.
Random audits

A = small A = med. A = large
11.5
8
-43.5

-0.25
-7.5
-53

-13
-23.5
-62.5

Table 6: pD(a|d1) directly elicited by defender without using ARA.

D1
Anom. det. & Data prov.
Info. sec. & train.
Random audits

A = small A = med. A = large
0.05
0.2
0.1

0.15
0.6
0.4

0.8
0.2
0.5

can see that the defender believes that there is a higher probability of detecting the attacker if
d1 = Anomaly detection and data provenance systems and the larger the attack is. Suppose also
that elicited the probabilities pD(s|d1, a) are as shown in Table 4, with probabilities of failed at-
tacks obtained through pD(not successful |d1, a) = 1 − pD(successful |d1, a). The expected utilities
ψD(d1, a) can now be computed, producing the values in Table 5.

Now, to compute the defender’s expected utility associated with each d1 ∈ D1 using Eq. (8), we
need the predictive distribution pD(a|d1) about what the malicious insider may do. Assume ﬁrst
that the defender has elicited pD(a|d1) using her own beliefs as in Table 6. The defender’s expected
utility ψD(d1) for each action d1 is computed; for example, ψD(random audits) = −62.5×0.5−53×
0.4 − 43.5 × 0.1 = −56.8. Similarly, the expected utility for anomaly detection and data provenance
is −9.9, whereas for information security and training is −7.6. This implies that the optimal option
for the organization is to invest in information security and staﬀ training; moreover, if the attack
was to happen, then, irrespective of whether it was successful or not, the optimal follow-up action
would be to carry on minor upgrades.

We now illustrate how pD(a|d1) could be obtained by modeling the attacker’s strategic analysis

12

Table 7: Attacker’s random utilities UA(a, s, e, d2) elicited by the defender.

A = small
D2
Maj.upgr.
Min.upgr.
No upgr.
A = medium
Maj.upgr.
Min.upgr.
No upgr.
A = large
Maj.upgr.
Min.upgr.
No upgr.

E = Yes

E = No

Succ.
N (−85, 3)
N (−55, 7)
100 − Exp(3)

Fail
N (−95, 1)
N (−65, 3)
100 − Exp(3)

Succ.
N (−80, 5)
N (−50, 10)
100 − Exp(5)

Fail
N (−90, 2)
N (−60, 5)
100 − Exp(5)

E = Yes

E = No

N (−85, 3)
N (−50, 5)
100 − Exp(2)

N (−95, 1)
N (−65, 3)
100 − Exp(2)

N (−80, 5)
N (−40, 10)
100 − Exp(3)

N (−90, 2)
N (−60, 5)
100 − Exp(3)

E = Yes

E = No

N (−85, 3)
N (−20, 5)
100 − Exp(1)

N (−95, 1)
N (−65, 3)
100 − Exp(1)

N (−80, 5)
N (−30, 10)
100 − Exp(1)

N (−90, 2)
N (−60, 5)
100 − Exp(1)

Table 8: Attacker’s random probabilities PA(d2|d1, a, s, e) elicited by the defender. Dirichlet
distributions with ﬁrst component corresponding to Major upgrade, second to Minor upgrade and
third to No upgrade.

E = No

E = Yes

Succ.
Dir(1, 3, 6)
Dir(1, 5, 4)
Dir(1, 4, 5)

Succ.
Dir(2, 2, 4)
Dir(2, 6, 2)
Dir(2, 5, 3)

Fail
Dir(1, 9, 90)
Dir(1, 9, 90)
Dir(1, 9, 90)

Fail
Dir(2, 18, 80)
Dir(2, 18, 80)
Dir(2, 18, 80)

A = small
D1
Anom. det. & Data prov.
Info. sec. & train.
Random audits
A = Medium
D1
Anom. det. & Data prov. Dir(2.5, 7, 0.5) Dir(1, 9, 90)
Dir(1.5, 8, 0.5) Dir(1, 9, 90)
Info. sec. & train.
Random audits
Dir(1, 9, 90)
A = Large
D1
Anom. det. & Data prov. Dir(5, 4.9, 0.1) Dir(1, 9, 90) Dir(5.5, 4.4, 0.1) Dir(2, 18, 80)
Dir(5, 4.9, 0.1) Dir(1, 9, 90) Dir(5.5, 4.4, 0.1) Dir(2, 18, 80)
Info. sec. & train.
Dir(5, 4.9, 0.1) Dir(1, 9, 90) Dir(5.5, 4.4, 0.1) Dir(2, 18, 80)
Random audits

Succ.
Dir(3, 6.5, 0.5)
Dir(2, 7.5, 0.5)
Dir(3, 7, 1)

Fail
Dir(2, 18, 80)
Dir(2, 18, 80)
Dir(2, 18, 80)

Dir(2, 6, 2)

E = Yes

E = Yes

E = No

E = No

Succ.

Succ.

Succ.

Fail

Fail

Fail

Table 9: Attacker’s random probabilities PA(E = Yes|d1, a, s) elicited by the defender. Note that
P (E = No|d1, a, s) = 1 − P (E = Yes|d1, a, s)

Fail

Succ.

A = small
D1
Anom. det. & Data prov. Beta(6, 4) Beta(7, 3)
Beta(3, 7) Beta(4, 6)
Info. sec. & train.
Beta(1, 9) Beta(1, 9)
Random audits
A = Medium
D1
Anom. det. & Data prov. Beta(7, 3) Beta(8, 2)
Beta(4, 6) Beta(5, 5)
Info. sec. & train.
Random audits
Beta(1, 9) Beta(1, 9)
A = Large
D1
Anom. det. & Data prov. Beta(8, 2) Beta(9, 1)
Beta(5, 5) Beta(6, 4)
Info. sec. & train.
Beta(1, 9) Beta(1, 9)
Random audits

Succ.

Succ.

Fail

Fail

process using Eqs. (9) to (13). To do this, the defender must elicit the attacker’s random utilities
and probabilities UA(a, s, e, d2), PA(e|d1, a, s), PA(s|a, d1) and PA(d2|d1, a, s, e), using any infor-
mation the defender might have, as well as considering the possible motivations for the attackers
and their skill level. To elicit the distribution of the random utilities, UA(a, s, e, d2), the defender
will try to think from the point of view of the attacker. She may think that the attacker is likely

13

Table 10: Attacker’s random probabilities PA(S = successful |d1, a) elicited by defender for every
(d1, s) combination.

A = small A = medium

D1
Anom. det. & Data prov. Beta(4, 6)
Beta(9, 1)
Inf.sec. & train.
Beta(7, 3)
Random audits

A = large
Beta(2, 8) Beta(0.5, 9.5)
Beta(7, 3)
Beta(8, 2)
Beta(3, 7)
Beta(6, 4)

Table 11: Predictive attack probabilities pD(a|d1) computed by the defender using ARA

D1
Anom.det. & Data prov.
Info.sec. & train.
Random audits

A = small A = med. A = large
0.843
0.312
0.699

0.090
0.189
0.105

0.067
0.499
0.196

to strongly prefer d2 = no upgrade since this means that future attacks will remain equally likely
to succeed as now whereas she thinks that d2 = major upgrade would be perceived by the attacker
as strongly undesirable since that will make any future attacks less likely to succeed. Also, the
attacker is better oﬀ not being detected and succeeding in his attack. Table 7 shows the attacker’s
random utilities UA(a, s, e, d2) elicited by the defender, scaled between −100 and +100.

When eliciting PA(d2|d1, a, s, e), the defender may think that the attacker believes that her
decision is inﬂuenced by the costs involved in establishing the upgrades in D2 as well as the
magnitude and success of the attack. Thus, for example, from her perspective, he may believe
that she is more likely to invest in a Major upgrade in the wake of a large, successful attack, but
more likely to go with minor upgrades or no upgrade in all other circumstances. Table 8 shows the
distributions elicited for PA(d2|d1, a, s, e) by the defender. For each of the Dirichlet distributions,
the ﬁrst component of the triplet corresponds to Major upgrade, the second to Minor upgrade and,
ﬁnally, the third one to No upgrade. For example, since the defender thinks that an upgrade is
considered (by the attacker) to be less likely in the event of a failed attack, she could elicit a
Dir(1, 9, 90) distribution for it. On the other hand, the Dir(5, 4.9, 0.1) indicates that an upgrade
is considered almost certainly likely in the wake of a successful large attack.

The defender then elicits the distributions for PA(e|d1, a, s), as shown in Table 9. She thinks
that the attacker believes that the probability of being detected is highest if D1 = anomaly detection
& data provenance and lowest if D1 = random audits and also is higher in the wake of a failed
attack versus a successful one and higher still the larger the attack was. Finally, Table 10 shows the
distributions elicited for PA(s|d1, a) by the defender. For example, she believes that the attacker
thinks that an attack is much more likely to succeed if D1 is information security and training
compared to the other options. Also, she believes that the attacker thinks that a small attack is
much more likely to succeed than a medium or large attack.

Given these assessments, the defender can compute her estimate of pD(a|d1) by Monte Carlo
simulation. For each combination of (d1, a, s, e), we simulate N = 1000 samples from the attacker’s
random utilities UA(a, s, e, d2) and probabilities PA(d2|d1, a, s, e) to obtain samples of the attacker’s
expected utility ΨA(d1, a, s, e) using Eq. (9). Then, we simulate samples from PA(e|d1, a, s) to
obtain samples of the attacker’s expected utility ΨA(d1, a, s) using Eq. (10). Finally, we simulate
samples from PA(s|d1, a) to obtain samples of ΨA(d1, a) using Eq. (11). Then, for each of the
samples, we ﬁnd the optimal attack a maximizing ΨA(d1, a) and, estimate pD(a|d1) by counting
how many times (out of N ) the optimal attack corresponds to each particular a ∈ A for each given
d1. The estimated probabilities are presented in Table 11.

We can now use these estimates of pD(a|d1) to compute the defender’s expected utility ψD(d1)
using Eq. (8). The expected utility for the anomaly detection and data provenance comes out to
be 8.8; for information security and training, −10.65,; and, ﬁnally, for the random audits, −48.22.
This implies that, in this case, the optimal initial defense for the organization is to invest in
anomaly detection and data provenance and, if the attack was to happen irrespective of whether it
was successful or not, the optimal follow-up action would be to carry on minor upgrades of their
existing defenses. Of course, this is based on the elicited inputs given by the defender. Moreover,
observe that the pD(a|d1) assessed by defender by modeling the attacker’s strategic analysis (Table

14

Table 12: ψD(d1, b, a) for when b = not blocked.

D1
Anom.det. & prov.
Info.sec. & train.
Random audits

Small
−88
−98.5
−137.5

A
Large
Med.
−75.25 −63.5
−67
−82.5
−118.5
−128

11), that is using ARA, turns out to be quite diﬀerent from the one elicited directly by the defender
without using ARA but based only on her ﬁrst uninstructed intuition (Table 6), which in turn leads
to a diﬀerent optimal defense d1.

5.2 Solution using the organisational culture model

We now analyze this problem using the model discussed in Section 4 by incorporating the organi-
sational culture. Again, we start by assessing the defender’s utility function uD(d1, b, s, e, d2). Just
like we did in Section 5.1, we assume that uD adopts the form

uD(d1, b, s, e, d2) = u(d1) + u(b) + u(s, e) + u(d2),

(24)

where u(d1) and u(d2) are as deﬁned in Table 1 and u(s, e) is the same as in Table 2. Suppose
the organisation elicits u(b) by assigning u(b = blocked) = 100 and u(b = not blocked) = −50,
representing, in a −100 to +100 scale, the relative utilities for the organisation associated with
a blocked insider attack (very highly desirable outcome) and with an attack that could not be
blocked (a considerably undesirable outcome), respectively. We calculate uD(d1, b, s, e, d2) using
Tables 1 and 2.

In order to implement the ARA solution to this problem, the defender must use backward
2(d1, b, s, e) maximizing her utility. In this case, again, d∗
induction. She ﬁrst ﬁnds her action d∗
2
turns out to be ‘Minor upgrade’, as it maximizes the term u(d2) in uD. Next, we need to elicit the
probabilities pD(e|d1, a, s). Suppose that these are same elicited in Table 3. Next, to integrate out
the uncertainty in s, we need to elicit the probability pD(s|d1, b, a). Note, however, as illustrated
in Figure 3, that the attack will only proceed if it was not blocked from taking place beforehand.
Therefore, at this stage we only need to elicit pD(s|d1, b = not blocked, a), given that the attack
was not blocked, which we suppose is the same as pD(s|d1, a) in Table 4. We can now compute
ψD(d1, b, a) using Equation (15), see Table 12.

We next seek to compute the expected utility ψD(d1, b) using Equation (16), which requires us
to elicit pD(a|d1, b). This can be elicited either directly using the defender’s domain knowledge or
indirectly by modeling the malicious insider’s strategic analysis process using Equations (19) to
(22). Again, note that the attack could only go ahead if it was not blocked beforehand. Therefore,
what we really need to elicit is pD(a|d1, b = not blocked). Thus, suppose that given that the attack
was not blocked, pD(a|d1, b = not blocked) is the same as pD(a|d1) in Table 6 when eliciting
these probabilities directly and the one in Table 11 when eliciting them using ARA. This gives
us ψD(d1, b = not blocked). For the case where the attack was successfully blocked in advance,
we have ψD(d1, b = blocked) = u(b = blocked) + u(d1). Tables 13 and 14 show the computed
values of ψD(d1, b) using both pD(a|d1, b) expressions obtained using direct elicitation and ARA
respectively.

Table 13: Computed values for ψD(d1, b) using pD(a|d1, b) in Table 6.

D1
Anom.det. & prov.
Info.sec. & train.
Random audits

b = blocked
0
40
50

b = not blocked
−59.86
−57.6
−106.8

Finally, the defender needs to integrate out the uncertainties regarding whether the attack will
be blocked in advance and regarding the culture in the organisation that the insider is likely to come
across using Equation (17). This requires eliciting pD(b|d1, c) and pD(c|d1). Suppose the defender

15

Table 14: Computed values for ψD(d1, b) using pD(a|d1, b) in Table 11.

D1
Anom.det. & prov.
Info.sec. & train.
Random audits

b = blocked
0
40
50

b = not blocked
−36.04
−22.54
−192.46

believes that the chances of the attack being blocked in advance are very high in presence of a
good culture in the organisation. Also that both the information security, as well as the random
audit options, are only really eﬀective at preventing an attack if the culture in the organisation is
good. Table 15 shows these probabilities. Eliciting pD(c|d1) can be relatively straightforward and,
unlike most of the probabilities and utilities displayed so far for this analysis, these can be elicited
by using real data collected through employee surveys, for example. Suppose that the organisation
concludes that a majority of its employees would prefer to be trusted and treated as partners. Also
that the employees are likely to be unhappy if they feel as if they are under surveillance or subject
to random checks. Suppose these elicited probabilities pD(c|d1) are as shown in Table 15.

Table 15: Elicited pD(b = blocked|d1, c) and pD(c = good|d1).

pD(b = blocked|d1, c)

pD(c = good|d1)

D1
Anom.det. & prov.
Info.sec. & train.
Random audits

c = good
0.9
0.8
0.7

c = not so good
0.7
0.1
0.2

0.5
0.7
0.3

Then, using Equation 17 and the probabilties pD(a|d1, b) in Table 6, we get that ψD(Anom.det. & prov.) =

−11.97, ψD(Info.sec. & train.) = −0.02 and ψD(Random audits) = −51.92. Based on these elicited
utilities and probabilities, the optimal defensive action d∗
1 is to invest in Information security and
training of the staﬀ.

Similarly, when we use the ARA probabilities pD(a|d1, b) in Table 11, we have that ψD(Anom.det. & prov.) =

−7.21, ψD(nfo.sec. & train.) = 14.36 and ψD(Random audits) = −107.6. Therefore, based on the
elicited utilities and probabilities, the optimal defensive action d∗
1 is invest in Information security
and training of the staﬀ. Note, however, that in this case the expected utilities ψD are quite
diﬀerent than before.

5.3 Model uncertainty

We have seen how one can ﬁnd the ARA optimal solution for the organization given a speciﬁc
game/model. The expected utility ψD(d1) that we ﬁnd with each of the models is,
in fact,
ψD(d1|M ), where M refers to the game under consideration. In reality though, the exact scenario
will be unknown. A Bayesian approach allows the organization to incorporate model uncertainty
into the analysis and compute the expected utility taking into account model uncertainty concepts
as in the classic [Draper, 1995].

For such purpose, the organization starts by listing the set M of relevant models, which will
contain a subset of or all of the models considered above. Then, they must elicit a prior distribution
pD(M ), ∀M ∈ M. The defender then performs the ARA analysis on each of those models to
obtain their expected utilities ψD(d1|M ), ∀M ∈ M. Their expected utility taking into account
model uncertainty is then given by

ψD(d1) =

(cid:88)

M ∈M

pD(M )ψD(d1|M ).

(25)

Then, their maximum utility decision is d∗

1 = arg maxd1∈D1 ψD(d1).

Suppose now that the defender is not certain if the malicious insider will be able to act on their
own or whether his actions will be aﬀected by other employees. She considers two models: M1,
the model in Section 5.1 and M2, that in Section 5.2; and elicits a prior probability pD(M1) = 0.3.
This implies that pD(M2) = 0.7. She would then perform the ARA analysis within each model

16

and arrive at her expected utilities ψD(d1|M1) = (8.8, −10.65, −48.22), for the three options when
pD(a|d1) is elicited by modeling the attacker’s strategic thinking, as illustrated in Section 5.1.
Similarly, she arrives at her expected utilities ψD(d1|M2) = (−7.21, 14.36, −107.6), as illustrated
in Section 5.2. Then, using Eq. (25), she can compute her expected utilities ψD(d1) taking into
account her model uncertainty ψD(d1 = (−2.4, 6.86, −89.78), resulting in investing in Information
security and training of the staﬀ being the optimal strategy for the defender when taking into
account model uncertainty.

6 Discussion and Further Work

Insider threats constitute a major security problem worldwide. We have develped ARA based
models to determine optimal strategies to counter insider threats and illustrated them using a
data security application. The olutions proposed in this paper are desirable for the following
reasons.

• They are based on defend-attack-defend models which take into account the existing defensive

mechanisms already in place.

• They take into account whether the malicious insider has been detected or not since this has

consequences for the persistence of attacks.

• In addition, the model in Section 4 also models the culture in the organisation that can

account for:

– the security culture in the organisation and its ability to prevent an attack,

– the security culture to prevent/enable inadvertent attacks, and

– the inadvertent negative consequences of a not so good culture.

• They only need the information that a defender is likely to have and do not make unrealistic

assumptions such as that of common knowledge or common priors.

• They are practicably easy to solve and are applicable to any general insider threat problem

and not speciﬁc to a single problem.

In general, as in with almost any security application, interactions between the defender and the
attacker will expand over several time periods and they will respectively evolve their defenses and
attacks so as to eﬀectively counter their adversarial actions. This can be modeled using a Markov
decision process (MDP). However, a general ARA solution to MDPs has not been developed yet,
thus being a promising area for further research. We could then provide a speciﬁc MDP solution
to the insider threat problem. This approach could also provide an ARA solution to support the
advanced persistent threat (APT) problem, being a persistent and long term threat.

Insider threats come in many diﬀerent forms. The problem considered here is probably the most
obvious, two player version where the malicious insider seeks to harm the organization directly.
However, there are more complicated three player versions. These could consist of two attackers
and one defender or the other way around or even an attacker, a defender and a victim (the
victim being a third party). For example, a three player case consisting of a malicious insider,
the APT and the organization consists of two attackers and a defender. But the malicious insider
could be also be someone who uses their privileges to exploit, abuse or harm a third party. This
third party could be clients, customers, students, patients, etc. Therefore, an important extension
would be to develop ARA solutions to such complex three player games. This could provide a
much more realistic alternative to the game theoretic models proposed in [Feng et al., 2015] and
[Hu et al., 2015].

Players are not always entirely rational and hence incorporating bounded rationality may make
the model more realistic. ARA is naturally equipped to incorporate attackers with diﬀerent reason-
ings, such as non-strategic thinking, Nash equilibrium, level-k thinking and the mirror equilibrium
([Banks et al., 2015]). However, a general ARA solution using the bounded rationality has not yet
been developed. Developing such a solution will enable a bounded rationality ARA solution to the
insider threat problem.

ARA relies on the elicitation of the adversary’s utilities and probabilities. Robustness analysis
[Ríos Insua et al., 2016]

of ARA to these elicitations is necessary, but has yet to be developed.

17

highlight the need and illustrate how a robustness analysis can be performed in principle for ARA.
It is important to be able to investigate the sensitivity of the ARA outcome, the optimal strategy,
to any errors or mis-speciﬁcations in the utilities and the probabilities elicited for the analysis.

Acknowledgment

The work of CJ was supported by the Strategic Investment funding provided by the Univer-
sity of Waikato. The work of DRI is supported by the AXA-ICMAT Chair on Adversarial Risk
Analysis, the Spanish Ministry of Economy and Innovation program MTM2017-86875-C3-1-R and
project MTM2015-72907-EXP. Work supported by the EU’s Horizon 2020 project 740920 CY-
BECO (Supporting Cyberinsurance from a Behavioural Choice Perspective) as well as as well as
the US National Science Foundation through grant DMS-163851.

References

[Antos and Pfeﬀer, 2010] Antos, D. and Pfeﬀer, A. (2010). Representing bayesian games without
a common prior. In Proceedings of the 9th International Conference on Autonomous Agents and
Multiagent Systems: Volume 1 - Volume 1, AAMAS ’10, pages 1457–1458.

[Axelrad et al., 2013] Axelrad, E. T., Sticha, P. J., Brdiczka, O., and Shen, J. (2013). A bayesian
network model for predicting insider threats. In 2013 IEEE Security and Privacy Workshops,
pages 82–89.

[Banks et al., 2015] Banks, D., Rios, J., and Insua, D. R. (2015). Adversarial Risk Analysis. CRC

Press, ﬁrst edition.

[Brdiczka et al., 2012] Brdiczka, O., Liu, J., Shen, J., Patil, A., Chow, R., Bart, E., and Duche-
neaut, N. (2012). Proactive insider threat detection through graph learning and psychological
context. IEEE CS Security and Privacy Workshop.

[Brown et al., 2006] Brown, G., Carlyle, M., Salmeron, J., and Wood, R. (2006). Defending critical

infrastructure. Interfaces, 36:530–544.

[Brown and Cox, 2011] Brown, G. G. and Cox, Jr., L. A. (2011). Making terrorism risk analysis

less harmful and more useful: Another try. Risk Analysis, 31(2):193–195.

[Camerer, 2003] Camerer, C. (2003). Behavioural Game Theory. Princeton University Press.

[CERT, 2012] CERT (2012). 2012 Cyber Security Watch Survey. How Bad is the Insider Threat?

Software Engineering Institute, Carnegie Mellon.

[Choi et al., 2018] Choi, S., Martins, J. T., and Bernik, I. (2018). Information security: Listening
to the perspective of organisational insiders. Journal of Information Science, 44(6):752–767.

[Colwill, 2009] Colwill, C. (2009). Human factors in information security: The insider threat –

who can you trust these days? Information Security Technical Report, 14(4):186 – 196.

[Cox, 2009] Cox, Jr., L. A. (2009). Game theory and risk analysis. Risk Analysis, 29(8):1062–1068.

[Draper, 1995] Draper, D. (1995). Assessment and propagation of model uncertainty. Journal

Royal Statistical Society, 57(1):45 – 97.

[Esteban and Insua, 2014] Esteban, P. G. and Insua, D. R. (2014). Supporting an autonomous

social agent within a competitive environment. Cybernetics and Systems, 45(3):241–253.

[Ezell et al., 2010] Ezell, B., Bennett, S., Winterfeldt, D., Sokolowski, J., and Collins, A. (2010).

Probabilistic risk analysis and terrorism risk. Risk Analysis, 30(4).

[Feng et al., 2015] Feng, X., Zheng, Z., Hu, P., Cansever, D., and Mohapatra, P. (2015). Stealthy
In MILCOM 2015 - 2015 IEEE

attacks meets insider threats: A three-player game model.
Military Communications Conference, pages 25–30.

18

[Gil and Parra-Arnau, 2019] Gil, C. and Parra-Arnau, J. (2019). An adversarial-risk-analysis ap-

proach to counterterrorist online surveillance. Sensors, 19(3).

[Gil et al., 2016] Gil, C., Rios Insua, D., and Rios, J. (2016). Adversarial risk analysis for urban

security resource allocation. Risk Analysis, 36(4):727–741.

[Gintis, 2009] Gintis, H. (2009). The Bounds of Reason: Game Theory and the Uniﬁcation of

Behavioural Sciences. Princeton University Press.

[Greitzer et al., 2012] Greitzer, F., Dalton, A., Kangas, L., Noonan, C., and Hohimer, R. (2012).
Identifying at-risk employees: Modeling psychosocial precursors of potential insider threats.
Proc. 25th HICSS.

[Harsanyi, 1967] Harsanyi, J. (1967). Games with incomplete information played by "bayesian"

players, i-iii. Management Science, 14(3).

[Hu et al., 2015] Hu, P., Li, H., Fu, H., Cansever, D., and Mohapatra, P. (2015). Dynamic defense
strategy against advanced persistent threat with insiders. In 2015 IEEE Conference on Computer
Communications (INFOCOM), pages 747–755.

[Hunker and Probst, 2009] Hunker, J. and Probst, C. (2009).

Insiders and insider threats: An
overview of deﬁnitions and mitigation techniques. Journal Wireless Mobile Networks, Ubiquitous
Computing, and Dependable Applications, 2(1):4–27.

[Insua et al., 2009] Insua, I. R., Rios, J., and Banks, D. (2009). Adversarial risk analysis. Journal

of the American Statistical Association, 104(486):841–854.

[Kantzavelou and Katsikas, 2010] Kantzavelou, I. and Katsikas, S. (2010). A game-based intrusion
detection mechanism to confront internal attackers. Computers & Security, 29(8):859 – 874.

[Lee and Rotoloni, 2015] Lee, W. and Rotoloni, B. (2015). Emerging Cyber Threat Report 2015.

Georgia Tech Information Security Centre and Georgia Tech Research Institute.

[Liu et al., 2008] Liu, D., Wang, X., and Camp, J. (2008). Game-theoretic modeling and analysis

of insider threats. International Journal of Critical Infrastructure Protection, I:75 – 80.

[Liu et al., 2009] Liu, D., Wang, X., and Camp, J. (2009). Mitigating inadvertent insider threats

with incentives. pages 1–16.

[Martinez-Moyano et al., 2008] Martinez-Moyano, I., Rich, E., Conrad, S., Andersen, D., and
Stewart, T. (2008). A behavioral theory of insider-threat risks: a system dynamic approach.
ACM Transactions on Modeling and Computer Simulation, 18(2).

[Moore et al., 2015] Moore, A., Novak, W., Collins, M., Trzeciak, R., and Theis, M. (2015). Ef-
fective Insider Threat Programs: Understanding and Avoiding Potential Pitfalls. White Paper.

[Naveiro et al., 2019] Naveiro, R., Redondo, A., Insua, D. R., and Ruggeri, F. (2019). Adversarial
International Journal of Approximate

classiﬁcation: An adversarial risk analysis approach.
Reasoning, 113:133 – 148.

[Probst et al., 2010] Probst, C., Hunker, J., Bishop, M., and Gollman, D. (2010). Insider Threats

in Cyber Security. Springer US.

[Raiﬀa et al., 2002] Raiﬀa, H., Richardson, J., and Metcalfe, D. (2002). Negotiation Analysis.

Havard University Press.

[Rios and Insua, 2012] Rios, J. and Insua, D. R. (2012). Adversarial risk analysis for counterter-

rorism modeling. Risk Analysis, 32(5):894–915.

[Ríos Insua et al., 2019] Ríos Insua, D., Banks, D., Ríos, J., and Ortega, J. (2019). Adversarial
Risk Analysis as an Expert Judgement Methodology, pages –. Springer International Publishing.

[Rios Insua et al., 2019] Rios Insua, D., Couce-Vieira, A., Rubio, J. A., Pieters, W., Labunets,
K., and G. Rasines, D. (2019). An adversarial risk analysis framework for cybersecurity. Risk
Analysis, n/a(n/a).

19

[Ríos Insua et al., 2016] Ríos Insua, D., Ruggeri, F., Alfaro, C., and Gomez, J. (2016). Robustness

for Adversarial Risk Analysis, pages 39–58. Springer International Publishing.

[Safa, 2017] Safa, N. (2017). The information security landscape in the supply chain. Computer

Fraud and Security, 2017(6):16–20.

[Sákovics, 2001] Sákovics, J. (2001). Games of incomplete information without common knowledge

priors. Theory and Decision, 50(4):347–366.

[Sarkar, 2010] Sarkar, K. (2010). Assessing insider threats to information security using technical,

behavioural and organisational measures. Info. Sec. Tech. Rep., 15:112–133.

[Schulze, 2018] Schulze, H. (2018). Insider Threat, 2018 report. ca Technologies.

[Sevillano et al., 2012] Sevillano, J. C., Insua, D. R., and Rios, J. (2012). Adversarial Risk Anal-

ysis: The Somali Pirates Case. Decision Analysis, 9(2):86–95.

[Silowash et al., 2012] Silowash, G., Cappelli, D., Moore, A., Trzeciak, R., Shimeall, T., and Flynn,
L. (2012). Common sense guide to mitigating insider threats. Def. Tech. Inf. Center Tech. Report.

[Tang et al., 2011] Tang, K., Zhao, M., and Zhou, M. (2011). Cyber insider threats situation
awareness using game theory and information fusion-based user behavior predicting algorithm.
Journal of Information & Computational Science, 8(3):529 – 545.

[Wang and Bier, 2013] Wang, C. and Bier, V. (2013). Expert elicitation of adversary preferences

using ordinal judgments. Operations Research, 61(2):372–385.

[Wang and Banks, 2011] Wang, S. and Banks, D. (2011). Network routing for insurgency: An

adversarial risk analysis framework. Naval Research Logistics (NRL), 58(6):595–607.

[Ware, 2017] Ware, B. (2017). Insider Attacks, 2017 insider threat study. Haystax.

[Wood et al., 2016] Wood, P., Nahorney, B., Chandrasekar, K., Wallace, S., and Haley, K. (2016).

Internet Security Threat Report, volume 21. Symantec.

20

