Active Deep Learning Attacks under Strict Rate
Limitations for Online API Calls

Yi Shi, Yalin E. Sagduyu, Kemal Davaslioglu, and Jason H. Li
Intelligent Automation, Inc.
Rockville, MD 20855, USA
Email:{yshi, ysagduyu, kdavaslioglu, jli}@i-a-i.com

8
1
0
2

v
o
N
5

]

G
L
.
s
c
[

1
v
1
1
8
1
0
.
1
1
8
1
:
v
i
X
r
a

Abstractâ€”Machine learning has been applied to a broad
range of applications and some of them are available online as
application programming interfaces (APIs) with either free (trial)
or paid subscriptions. In this paper, we study adversarial machine
learning in the form of back-box attacks on online classiï¬er
APIs. We start with a deep learning based exploratory (inference)
attack, which aims to build a classiï¬er that can provide similar
classiï¬cation results (labels) as the target classiï¬er. To minimize
the difference between the labels returned by the inferred
classiï¬er and the target classiï¬er, we show that the deep learning
based exploratory attack requires a large number of labeled
training data samples. These labels can be collected by calling
the online API, but usually there is some strict rate limitation
on the number of allowed API calls. To mitigate the impact of
limited training data, we develop an active learning approach
that ï¬rst builds a classiï¬er based on a small number of API
calls and uses this classiï¬er to select samples to further collect
their labels. Then, a new classiï¬er is built using more training
data samples. This updating process can be repeated multiple
times. We show that this active learning approach can build an
adversarial classiï¬er with a small statistical difference from the
target classiï¬er using only a limited number of training data
samples. We further consider evasion and causative (poisoning)
attacks based on the inferred classiï¬er that is built by the
exploratory attack. Evasion attack determines samples that the
target classiï¬er is likely to misclassify, whereas causative attack
provides erroneous training data samples to reduce the reliability
of the re-trained classiï¬er. The success of these attacks show that
adversarial machine learning emerges as a feasible threat in the
realistic case with limited training data.

Index Termsâ€”Adversarial machine learning, deep learning,
active learning, exploratory attack, evasion attack, causative
attack.

I. INTRODUCTION

As a subï¬eld of artiï¬cial intelligence (AI), machine learn-
ing has been used in many applications including cyber
security, intelligence analysis, Internet of Things (IoT), cyber-
physical systems, autonomous driving, and traditional or com-
puter games. Machine learning provides the automated means
to learn with data. Recently, signiï¬cant progress has been
made with deep neural networks (deep learning) supported by
advances in hardware and computational capabilities, making
machine learning much more effective than ever before. Some
of the machine learning systems are made available to public
or paid subscribers via an application programming interface
(API), e.g., Googleâ€™s Vision API provides image content
analysis via REST API [1]. A user can call such an API for

the speciï¬ed data and obtain the returned results, which may
be further analyzed for different subsequent tasks.

The online service paradigm, although convenient, makes
machine learning vulnerable to various attacks. Machine learn-
ing applications may involve sensitive and/or proprietary infor-
mation that includes training data, machine learning algorithm,
hyperparameters, and functionality of underlying tasks. By
making machine learning applications online, in addition to
traditional attacks (e.g., DoS attack), these applications are
also becoming subject to various new exploits and attacks
(such as stealing the application functionality, trained model,
and training data).

The ï¬eld of adversarial machine learning has emerged to
understand the machine learning behavior in the presence
of an adversary. One example is from the image recogni-
tion application, where the adversary generates an image by
slightly perturbing real images to fool a state-of-the-art image
classiï¬er [2]. As a consequence, a perturbed panda image is
recognized as a gibbon by machine learning, while the original
image is recognized as a panda by a human. Adversarial
machine learning is still a new research ï¬eld and the security
limitations and vulnerabilities of machine learning are not
well understood yet. With the increasing use of machine
learning in critical commercial and government applications,
the security implications ranging from intellectual property
protection to national security raise the need to identify effects
of adversarial machine learning and provide the foundation for
attack mitigation techniques.

There have been efforts to develop various attacks based
on adversarial machine learning. Exploratory (or inference)
attacks [3]â€“[12] aim to understand how the underlying ma-
chine learning algorithm works for an application, and conse-
quently infer any sensitive and/or proprietary information. An
exploratory attack can be launched under different assumptions
made on the attackerâ€™s knowledge. For instance, a black-
box attack can be launched without any prior knowledge on
the machine learning algorithm and the training data [4]â€“
[7]. As shown in [4]â€“[7], the adversary can call the target
classiï¬er T with a large number of samples, collect the labels
on these samples, and then use this data to train a deep
learning classiï¬er Ë†T as an estimate of the target classiï¬er
T . This attack implicitly steals the underlying training data,
the inner-workings of the machine learning algorithm, and its
hyperparameter selection that constitute the core of intellectual

 
 
 
 
 
 
property (which has been often built with long-time research
and development investment). Moreover, once Ë†T is inferred,
further attacks such as evasion attacks or causative (poisoning)
attacks can be launched [13].

training data,

One major assumption of exploratory attacks is that an
adversary can collect training data from the target machine
learning application. Supervised machine learning relies on
the quality and the quantity of training data samples. Without
sufï¬cient
it may not be possible to train a
successful machine learning algorithm [14]. However, machine
learning APIs, in general, have strict rate limitations on how
many API calls can be made in a certain period of time
(ranging from per second to per day). In addition, a user who
makes too frequent API calls may be identiï¬ed as a potential
adversary and may be blocked later. Hence, we consider the
practical case that imposes strict rate limitations on adversarial
machine learning. In this paper, we select a real online machine
learning API for text analysis [15] as the target classiï¬er T .
The adversary treats T as a black box, since the underlying
machine learning algorithm, parameters, and training data are
all unknown.

First, we consider exploratory attacks. We show that when
the number of API calls is limited, the adversary cannot infer
the target classiï¬er T with acceptable accuracy, even when
it launches a deep learning based exploratory attack. Thus,
we design an approach based on active learning [16], [17]
to enhance the training process of the exploratory attack. The
adversary ï¬rst builds a classiï¬er Ë†T based on a small number
of samples and collects their labels. Using active learning,
it further selects additional samples using Ë†T and sends only
samples with the low classiï¬cation conï¬dence by Ë†T to the
target classiï¬er T to be labeled. These samples are used
as additional training data to improve the inferred classiï¬er
Ë†T . This updating process can be repeated multiple times.
The system model is shown in Figure 1. We show that the
proposed method signiï¬cantly improves the performance of
the inferred classiï¬er Ë†T that is measured as the statistical
difference of classiï¬cation results,
labels returned by
the original classiï¬er T and the inferred classiï¬er Ë†T . This
approach provides insights into situational awareness, attack
modeling, detection, and mitigation in different applications
of cyberspace [18]â€“[20].

i.e.,

Once the exploratory attack is successful, the adversary can
launch further attacks such as evasion and causative attacks
by using the inferred classiï¬er Ë†T . The evasion attack [5],
[6], [21]â€“[23] aims to provide the target classiï¬er with some
test data that will likely result in incorrect labels (e.g., a
spam email ï¬lter is fooled into accepting spam emails as
legitimate). The adversary runs some samples through the
inferred classiï¬er Ë†T based on a deep neural network and
further examines their classiï¬cation scores provided by the
classiï¬er. If these scores (likelihoods of labels returned by the
deep neural network) are within the decision region for label
j and are close to the decision region for another label i,
these sample are selected such that the target classiï¬er T is
very likely to misclassify data samples from label i as label

Fig. 1. Adversarial deep learning with limited training.

j. Such samples can be used to select the test data samples in
the form of evasion attacks.

On the other hand, the causative attack [16], [24]â€“[27] aims
to provide the target classiï¬er with incorrect information as the
additional training data samples to reduce the reliability of the
re-trained classiï¬er. For this purpose, the adversary sends some
data samples to its inferred classiï¬er Ë†T . If the deep learning
scores are far away from the decision boundary, the adversary
changes their labels and sends these mislabeled samples as
additional training data to the target classiï¬er T . The target
classiï¬er that is re-trained this way is expected to perform
signiï¬cantly worse than before.

The problem of limited training data in adversarial machine
learning has been studied in [6], where the adversary ï¬rst
infers the target classiï¬er with limited real training data, and
then generates synthetic data samples from real data in an
evasion attack by adding adversarial perturbations that are
gradually improved by the derivative and checking labels
queried from the target classiï¬er. The difference is that we
do not use synthetic data in the evasion attack and our goal is
to improve the inferred classiï¬er (not the adversarial samples)
that can be used later to select an arbitrary number of samples
for evasion or causative attacks without further querying the
target classiï¬er.

The rest of the paper is organized as follows. Section II
studies the exploratory attack assuming that a large number of
API calls is allowed and then presents an active learning based
approach to launch the exploratory attack under limitations on
API calls. Section III investigates the subsequent evasion and
causative attacks. Section IV concludes the paper.

II. EXPLORATORY ATTACK AND TRAINING DATA
REFINEMENT WITH ACTIVE LEARNING

A. Exploratory Attack on an Online Classiï¬er

Many machine learning services are made available online
as APIs. Users can use these services by making API calls and
integrate their API outputs with their own applications. Such
applications include, but not limited to, topic classiï¬cation,
sentiment analysis, subjectivity analysis, keyword extraction,
language detection, and spam detection. APIs usually provide
free subscriptions that allow a user to call them only few times

Machine Learning Classifier under AttackLabelsBuild Training DatasetTest DataUntil producing labels similar to the classifier under attack Train a Deep Learning ClassifierActive LearningFunctionally Equivalent ClassifierAdding totest datain a certain time frame. For example, DatumBox provides a
number of machine learning APIs for text analysis [15]. A
user can specify the input text data, call a DatumBox API to
analyze this text, and obtain results such as classiï¬cation of
the input text as subjective or objective.

This paradigm raises potential security issues for online
services, since an adversary can also access the input and
output of these services. In particular, the adversary can per-
form a black-box exploratory attack, where the adversary does
not have any knowledge on the target classiï¬er T , including
the training data, training algorithm, and machine learning
parameters. Such an attack can be launched as follows:

1) The adversary calls the online API of the target classiï¬er
T multiple times using a set S of samples and for each
sample s âˆˆ S, the adversary collects the label T (s)
returned by the online API.

2) By using S and the collected labels, the adversary trains
a deep neural network to build a classiï¬er Ë†T , as an
estimation of T .

In this paper, we consider a black-box exploratory attack,
i.e., the adversary only has a set S of samples to call the online
API and the returned labels. The target classiï¬er T has been
trained before. The adversary does not know the training data
or the algorithm (e.g., Naive Bayes, Support Vector Machine
(SVM), or a more sophisticated neural network algorithm)
used to build this classiï¬er. The data is divided into training
data and test data. Using the training data,
the adversary
applies deep learning (based on a deep neural network) to
infer T . Deep learning refers using training data to train a
deep neural network (namely, ï¬nding the weights in the neural
network) for computing labels. In particular, we consider a
feedforward neural network (FNN) shown in Figure. 2. We
use the backpropagation algorithm to train the deep neural
network. A neural network consists of simple elements called
neurons and weighted connections, a.k.a. synapses, among
these neurons. A neuron j performs a basic computation over
its input synapses wji from each neuron i that is connected
to j, and outputs a single scalar value yj, which can be inter-
preted as its activation or ï¬ring rate. In a feedforward neural
network (FNN) architecture, neurons are arranged in layers
and synapses are connecting neurons in one layer to neurons
in the next layer. The activations of neurons in the input layer
are set externally while the activations of the hidden layer
neurons and output layer neurons are computed as speciï¬ed
above. In particular, the activations of the output layer neurons
represent the result of the networkâ€™s computation.

We consider a real online classiï¬er API, namely the subjec-
tivity analysis API of DatumBox [15], as the target classiï¬er T .
The adversary calls this API with a number of text samples and
collects the returned labels, which may be label 1 (subjective)
or label 2 (objective). The free subscription allows 1000 calls
per day, i.e., labels for 1000 samples can be collected per day.
For the numerical results presented in this paper, the adver-
sary collects labels for 10000 data samples over time. Each
data sample is the text of a tweet that has been collected from
the public Twitter API. In this section, the adversary uses half

Input
layer

...

Hidden
layer 1

. . .

Hidden
layer N

Output
layer

wji

j

...

i

...

...

Fig. 2. Feedforward neural network.

of the data samples as training data to train a classiï¬er Ë†T and
the other half as test data to evaluate its performance d( Ë†T , T )
of Ë†T .

Deep learning requires representing each sample as a set of
features. We use top word distribution to build such a set of
features as follows. The adversary ï¬rst obtains a list of words
W = (w1, w2, Â· Â· Â· ) sorted by their frequencies of occurrence
in text, i.e., p1 â‰¥ p2 â‰¥ Â· Â· Â· where pi is the frequency of
occurrence for word wi. Then the features for a sample of
text is the set of numbers of occurrence oi for each word wi.
Thus, 2000 features are obtained by considering distributions
of top 2000 words. The adversary uses these features to train
the deep learning classiï¬er Ë†T .

The adversary builds a deep neural network (an FNN) as
the classiï¬er Ë†T . To optimize the hyperparameters of the deep
neural network, the following two measures are computed.
The difference d1( Ë†T , T ) between Ë†T and T on label 1 is the
number of samples with Ë†T (s) = 2 and T (s) = 1 divided by
the number of samples with label 1 in test data. We can deï¬ne
d2( Ë†T , T ) similarly. Then, we aim to minimize dmax( Ë†T , T ) =
max{d1( Ë†T , T ), d2( Ë†T , T )} for test data to balance the effect
on both labels.

We implement this process by using the Microsoft Cognitive
Toolkit (CNTK) [28] in a Python code to train the FNN with
the optimal hyperparameters. The hyperparatemers of the deep
neural network are optimized as follows.

â€¢ The number of hidden layers is 2.
â€¢ The number of neurons per layer is 60.
â€¢ The loss function (that is used to measure the difference
of the labels returned by the deep neural network from
the ground truth labels) is cross entropy.

â€¢ The activation function in hidden layers is sigmoid.
â€¢ The activation function in output layer is softmax.
â€¢ Weights and biases are not initially scaled.
â€¢ Input values are unit normalized in the ï¬rst training pass.
â€¢ The minibatch size is 5.
â€¢ The momentum coefï¬cient to update the gradient is 0.9.
â€¢ The number of epochs per time slot is 8.

The difference between labels returned by T and Ë†T for test

data is found as

d1( Ë†T , T ) = 12.82%, d2( Ë†T , T ) = 12.88%.

Thus, we obtain

dmax( Ë†T , T ) = 12.88%.

However, the above attack requires to collect labels for
many samples, which needs to be done over multiple days
(due to the rate limit of 1000 samples per day). Next, we
will consider the case when the adversary can perform the
exploratory attack with a small number of calls.

B. Active Learning with Limited Training Data

Our approach to mitigate the effect of limited training on the
adversary is based on active learning, which can jointly and
iteratively build the inferred classiï¬er Ë†T using limited training
data. Suppose that
the adversary starts with 200 samples
and their labels obtained from the target classiï¬er T (see
Figure 3(a)). Data is split in 100 training samples (to build
the classiï¬er Ë†T ) and 100 test samples.

Again, the adversary builds the optimal Ë†T by selecting deep
learning hyperparameters to minimize the difference between
Ë†T and T . These hyperparameters are given by:

â€¢ The number of hidden layers is 2.
â€¢ The number of neurons per layer is 10.
â€¢ The loss function is squared error.
â€¢ The activation function in hidden layers is sigmoid.
â€¢ The activation function in output layer is softmax.
â€¢ All weights and biases are initially scaled by 0.5.
â€¢ Input values are unit normalized in the ï¬rst training pass.
â€¢ The minibatch size is 25.
â€¢ The momentum coefï¬cient to update the gradient is 0.9.
â€¢ The number of epochs per time slot is 10.
Note that

this deep neural network is smaller than the
previous one because fewer training data samples are used
to build this second neural network. The difference between
labels returned by T and Ë†T is found as

d1( Ë†T , T ) = 33.78%,

d2( Ë†T , T ) = 30.77%.

Thus, we obtain

dmax( Ë†T , T ) = 33.78%.

The adversary then collects more labels from T to improve
Ë†T . In the benchmark approach, randomly selected samples are
sent to T and the returned labels are collected and added to
the training data in order to improve Ë†T .

We ï¬rst take a closer look at the inferred classiï¬er Ë†T . For
each sample s, Ë†T ï¬rst determines a classiï¬cation likelihood
score Ë†S(s) and then compares it with a threshold S Ë†T , which
is optimized to minimize dmax( Ë†T , T ). The optimal S Ë†T is
found as 0.17. If Ë†S(s) < S Ë†T , sample s is classiï¬ed as
label 1, otherwise sample s is classiï¬ed as label 2. The
difference between score Ë†S(s) and S Ë†T captures the conï¬dence
of classiï¬cation. If Ë†S(s) is close to S Ë†T , then its is likely that

(a) Step 1.

(b) Step 2.

Fig. 3. Exploratory attack with active learning.

such a classiï¬cation is wrong. On the other hand, if Ë†S(s) is
far away from S Ë†T , then it is unlikely that such a classiï¬cation
is wrong.

Based on this understanding of Ë†S(s), an active learning

approach is designed as follows (see Figure 3(b)).

1) The adversary sends randomly selected text samples
s to its inferred classiï¬er Ë†T and obtains classiï¬cation
likelihood scores Ë†S(s) from the inferred classiï¬er.
2) If classiï¬cation likelihood score Ë†S(s) is close to thresh-
old S Ë†T , the adversary sends this text sample to classiï¬er
T and obtain its label T (s). The adversary adds text
sample s and its label T (s) to the training data.

3) If additional training data is sufï¬cient, the adversary

updates Ë†T . Else, the adversary goes to Step 1.

Using the above steps, the adversary generates 500 (1000, or
2000) additional text queries in Step 1 of the training process
and obtains the scores of the corresponding samples from the
inferred classiï¬er Ë†T in Step 2. Based on these scores, 224
(579, or 1007) text samples close to S Ë†T are selected by active
learning to call classiï¬er T . We refer to these samples as
the actively learned samples. Using the scores returned by
T , the inferred Ë†T is updated. In the benchmark scheme, we
consider that only randomly selected text sample is sent to
the original classiï¬er. Active learning performs better than
the benchmark because it calls T with data samples that are
uncertain with respect to the original inferred classiï¬er Ë†T ,
while the benchmark is not able to adapt its decision boundary
to the uncertainty of such samples. In both active learning and
benchmark scheme, the adversary uses the original 100 test
samples to evaluate the performance.

The results are presented in Table I. The total number
of training data samples is set the same for active learn-
ing and benchmark scheme. With 224 additional (actively
learned or randomly selected) training data samples, active
learning performs better than the benchmark scheme and

Inferred Classifier à·¡ğ‘»Target  Classifier ğ‘»SamplesLabels^^Inferred Classifier à·¡ğ‘»New SamplesAdditional selected samplesUpdated Inferred à·¡ğ‘»Target  Classifier ğ‘»ScoresLabelsPrevious samples and labelsTABLE I
PERFORMANCE (DIFFERENCE BETWEEN LABELS RETURNED BY T AND Ë†T ) WITH ACTIVE LEARNING.

Training data size

Initial Samples Additional Samples

100
100
100

224
579
1007

Total Samples
324
679
1107

Active learning

Benchmark

d1( Ë†T , T )
28.38%
24.32%
18.92%

d2( Ë†T , T )
26.92%
23.08%
19.23%

d1( Ë†T , T )
31.08%
29.73%
31.08%

d2( Ë†T , T )
30.77%
30.77%
30.77%

achieves dmax( Ë†T , T ) = 28.38%, whereas the benchmark
scheme achieves dmax( Ë†T , T ) = 31.08%. This performance
gain of active learning further increases with more (579 or
1007) additional training data samples. For example, with
1107 total samples, active learning reduces dmax( Ë†T , T ) to
19.23% (which corresponds to 38.13% improvement over the
benchmark scheme). Thus, active learning always achieves
better performance than the benchmark scheme. Moreover,
with more additional training data samples, the advantage
of active learning grows, which shows the importance of
identifying and selecting uncertain samples in performance
improvement.

III. EVASION ATTACKS AND CAUSATIVE ATTACKS

When the adversary completes a successful exploratory
attack and infers a classiï¬er Ë†T that is similar to the target
classiï¬er T , it can then try to learn the behavior of T by
using Ë†T and launch subsequent attacks. In this section, we
demonstrate two such attacks, namely, the evasion attack and
causative attack.

A. Evasion Attack

The evasion attack aims to ï¬nd test samples for which labels
returned by T are likely wrong. Although T is unknown by
the adversary, the inferred classiï¬er Ë†T can used to predict
which samples may be incorrectly classiï¬ed. There may be
different attack objectives and approaches. In one example,
the objective is to maximize the average error. To achieve
this objective, the adversary selects samples with classiï¬cation
scores that are close to the threshold (i.e., samples with low
conï¬dence on classiï¬cation). In another example, the objective
is to maximize the error of misclassifying a label 1 sample
as label 2. To achieve this objective, the adversary selects
samples classiï¬ed with label 2 and with score that is close
to the threshold. The objective of maximizing the error of
misclassifying a label 2 sample as label 1 can be achieved
similarly.

We need the ground truth (i.e., the correct label for samples
in test data) to evaluate the effectiveness of such attacks. Since
it is assumed that the adversary has no knowledge of ground
truth, we omit the performance results for evasion attack.

B. Causative Attack

To reduce classiï¬cation errors, some classiï¬ers are updated
(re-trained) by user feedback. That
is, a user can review
the returned labels by T and manually ï¬x labels on some
samples or agree on the returned labels. If such ï¬‚ags are

received, the sample with user deï¬ned labels can be used
as additional training data to re-train the classiï¬er. However,
such a process also introduces a new type of attack, namely
causative attack, where the adversary provides wrong labels
such that the updated classiï¬er ËœT becomes less reliable in
terms of classiï¬cation accuracy.

There are different strategies to provide wrong labels. In the
extreme case, the adversary may provide wrong labels for all
data samples to maximize the impact of its attack. However,
it is easy to detect such an attack and block the adversary,
i.e., its feedback will not be considered in the re-training
process. Thus, the adversary aims to achieve a signiï¬cant
impact of its attack by using a small number of wrong labels.
Suppose that the adversary can change labels for only p% of
all samples, where p is a small number. Then, the problem is
to determine how to select the best set of data samples and
provide wrong labels on this set of data samples. Clearly, this
selection requires the knowledge of T , which can be obtained
by analyzing Ë†T after a successful exploratory attack.

We again consider the classiï¬cation scores provided by the
deep learning based classiï¬er Ë†T . If the adversary changes the
label for a sample with a score that is far away from the
threshold (i.e., with high classiï¬cation conï¬dence), this new
label will be very likely a wrong label and will make the
updated classiï¬er ËœT worse. On the other hand, if the adversary
changes the label for a sample with a score that is close to the
threshold (i.e., with low classiï¬cation conï¬dence), this new
label may not be a wrong label and will not make the updated
classiï¬er ËœT worse. Thus, the adversary performs a causative
attack as follows.

1) The adversary sends some samples to Ë†T and receives
their scores and labels. Note that Ë†T is the classiï¬er
inferred by the adversary and thus there is no rate
limitation, i.e., the adversary can collect a large number
of samples with scores and labels.

2) The adversary selects samples with top p

2 % scores and
2 % scores. These scores are far

samples with bottom p
away from the threshold.

3) The adversary switches labels of selected samples and

sends these labels as user feedback.

To measure the impact of a causative attack, we compare the
outputs of the original classiï¬er T and the updated classiï¬er ËœT .
For samples with label 1 (by class T ), suppose that the number
of these sample is n1, while T and ËœT provide different labels
on m1 samples. Then, we deï¬ne the difference on samples
with label 1 as d1(T, ËœT ) = m1
. Similarly, deï¬ne the difference
n1

on samples with label 2 as d2(T, ËœT ) = m2
and the average of
n2
the difference on all samples as d(T, ËœT ) = m1+m2
, where n2
n1+n2
is the number of samples with label 2 and m2 is the number
of different labels by T and ËœT on these n2 samples.

We use the classiï¬er Ë†T built in Section II with 1107 training
data samples, we apply it on another set of 1000 samples and
perform a causative attack with p = 10. Then, we obtain

d1(T, ËœT ) = 39.92%, d2(T, ËœT ) = 55.64%.

Thus, the average difference on all samples is

d(T, ËœT ) = 48.00%.

As a result, the causative attack built upon the exploratory
attack reduces the reliability of the updated classiï¬er ËœT sig-
niï¬cantly.

IV. CONCLUSION

We studied the exploratory attack to infer a target online
classiï¬er, when the number of calls to the online API is
limited. Typically, adversarial deep learning requires a large
number of training samples. Under strict rate limitations on
API calls, we showed that there is a signiï¬cant difference
between the labels returned by the inferred classiï¬er and the
target classiï¬er. To mitigate this effect, we designed an active
learning process for adversarial deep learning, which ï¬rst
builds a classiï¬er based on limited training data and then uses
this classiï¬er to selectively make more API calls. We showed
that this attack can reliably infer the target classiï¬er even with
a limited number of API calls. By analyzing the classiï¬er
inferred by a successful exploratory attack, the adversary can
launch further attacks. In particular, we designed evasion and
causative attacks using the inferred classiï¬er to select test
and training data samples, respectively. The evasion attack
identiï¬es test data that the target classiï¬er cannot reliably
classify, while the causative attack effectively poisons the re-
training process such that the reliability of updated classiï¬er
degraded. We showed that
these attacks are feasible with
limited training data and adversarial machine learning emerges
as a practical threat to online classiï¬er APIs.

REFERENCES

[1] Cloud Vision API, available at https://cloud.google.com/vision.
[2] I. J. Goodfellow, J. Shlens, and C. Szegedy, â€œExplaining and harnessing

adversarial examples,â€ arXiv preprint arXiv:1412.6572. 2014.

[3] M. Barreno, B. Nelson, R. Sears, A. Joseph, and J. Tygar, â€œCan Machine
Learning be Secure?â€ ACM Symposium on Information, Computer and
Communications Security, Taipei, Taiwan, Mar. 2006.

[4] F. Tramer, F. Zhang, A. Juels, M. Reiter, and T. Ristenpart, â€œStealing
Machine Learning Models via Prediction APIs,â€ USENIX Security
Symposium, Austin, TX, Aug. 2016.

[5] Y. Shi, Y. E. Sagduyu, K. Davaslioglu, and R. Levy, â€œVulnerability
Detection and Analysis in Adversarial Deep Learning,â€ in Guide to
Vulnerability Analysis for Computer Networks and Systems: An Arti-
ï¬cial Intelligence Approach, S. Parkinson, A. Crampton, R. Hill, Eds.
Springer, 2018, pp. 235â€“258.

[6] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Celik, and A.
Swami, â€œPractical Black-Box Attacks Against Deep Learning Systems
Using Adversarial Examples,â€ ACM Conference on Computer and
Communications Security (CCS), Dallas, TX, Oct.-Nov. 2017.

[7] Y. Shi, Y. E. Sagduyu, and A. Grushin, â€œHow to Steal a Machine Learn-
ing Classiï¬er with Deep Learning,â€ IEEE Symposium on Technologies
for Homeland Security (HST), Boston, MA, Apr. 2017.

[8] G. Ateniese, L. Mancini, A. Spognardi, A. Villani, D. Vitali, and G.
Felici, â€œHacking Smart Machines with Smarter Ones: How to Extract
Meaningful Data from Machine Learning Classiï¬ers,â€ International
Journal of Security and Networks, 10(3):137-150, 2015.

[9] M. Fredrikson, S. Jha, and T. Ristenpart, â€œModel Inversion Attacks
that Exploit Conï¬dence Information and Basic Countermeasures,â€ ACM
SIGSAC Conference on Computer and Communications Security, Den-
ver, CO, Oct. 2015.

[10] F. Tramer, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel,
â€œThe Space of Transferable Adversarial Examples,â€ ArXiv e-prints
arXiv:1704.03453, 2017.

[11] Y. Shi, Y. E Sagduyu, T. Erpek, K. Davaslioglu, Z. Lu, and J. Li, â€œAd-
versarial deep learning for cognitive radio security: Jamming attack and
defense strategies,â€ IEEE International Conference on Communications
(ICC) Workshop on Promises and Challenges of Machine Learning in
Communication Networks, Kansas City, MO, May 2018.

[12] T. Erpek, Y. E. Sagduyu, and Y. Shi, â€œDeep learning for launching and
mitigating wireless jamming attacks,â€ arXiv preprint arXiv:1807.02567,
2018.

[13] Y. Shi and Y. E Sagduyu, â€œEvasion and Causative Attacks with Ad-
versarial Deep Learning,â€ IEEE Military Communications Conference
(MILCOM), Baltimore, MD, Oct. 2017.

[14] K. Davaslioglu and Y. E. Sagduyu, â€œGenerative adversarial learning for
spectrum sensing,â€ IEEE International Conference on Communications
(ICC), Kansas City, MO, May 2018.

[15] DatumBox Machine Learning API, available at http://www.datumbox.

com/machine-learning-api/

[16] L. Pi, Z. Lu, Y. Sagduyu, and S. Chen, â€œDefending Active Learning
against Adversarial Inputs in Automated Document Classiï¬cation,â€
IEEE Global Conference on Signal and Information Processing (Glob-
alSIP), Washington, D.C, Dec. 2016.

[17] B. Settles, â€œActive Learning,â€ in Synthesis Lectures on Artiï¬cial Intel-
ligence and Machine Learning, Morgan & Claypool Publishers, 2012,
vol. 6, pp. 1-114.

[18] O. Savas, Y. E. Sagduyu, J. Deng, and J. Li, â€œTactical Big Data
Analytics: Challenges, Use Cases, and Solutions,â€ ACM SIGMETRICS
Performance Evaluation Review, vol. 41, no. 4, pp. 86â€“89, Apt. 2014.
[19] Z. El Jamous, S. Soltani, Y. E. Sagduyu, and J. H. Li, â€œRADAR:
An Automated System for Near Real-Time Detection and Diversion
of Malicious Network Trafï¬c,â€ IEEE Symposium on Technologies for
Homeland Security (HST), Bosto, MA, May 2016.

[20] Y. Cheng, Y. E. Sagduyu, J. Deng, J. Li, and P. Liu, â€œIntegrated Situa-
tional Awareness for Cyber-attack Detection, Analysis, and Mitigation,â€
SPIE Defense, Security and Sensing Conference, Baltimore, MD, Apr.
2012.

[21] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Srndic, P. Laskov, G.
Giacinto, and F. Roli, â€œEvasion Attacks Against Machine Learning at
Test Time,â€ European Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases (ECML-PKDD),
Prague, Czech Republic, Sept. 2013.

[22] A. Kurakin, I. Goodfellow, and S. Bengio, â€œAdversarial Examples in the

Physical World,â€ arXiv preprint arXiv:1607.02533, 2016.

[23] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. Celik, and A.
Swami, â€œThe Limitations of Deep Learning in Adversarial Settings,â€
IEEE European Symposium on Security and Privacy, Saarbrucken,
Germany, Mar. 2016.

[24] M. Mozaffari-Kermani, S. Sur-Kolay, A. Raghunathan and N. K. Jha,
â€œSystematic Poisoning Attacks on and Defenses for Machine Learning
in Healthcare,â€ IEEE Journal of Biomedical and Health Informatics, vol.
19, no. 6, pp. 1893â€“1905, Nov. 2015.

[25] B. Biggio, B. Nelson and P. Laskov, â€Poisoning Attacks against Support
Vector Machines,â€ in International Conference on Machine Learning
(ICML), Edinburgh, Scotland, June-July 2012.

[26] S. Alfeld, X. Zhu and P. Barford, â€œData Poisoning Attacks against
Autoregressive Models,â€ in AAAI Conference on Artiï¬cial Intelligence,
Phoenix, AZ, Feb. 2016.

[27] Y. Shi, T. Erpek, Y. E. Sagduyu, and J. Li, â€Spectrum Data Poison-
ing with Adversarial Deep Learning,â€ IEEE Military Communications
Conference (MILCOM), Los Angeles, CA, Oct. 2018.

[28] Microsoft Cognitive Toolkit (CNTK), available at https://docs.microsoft.

com/en-us/cognitive-toolkit.

