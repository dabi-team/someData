WEMAC: WOMEN AND EMOTION MULTI-MODAL AFFECTIVE COMPUTING DATASET

1

WEMAC: Women and Emotion
Multi-modal Affective Computing dataset
Jose A. Miranda∗1, Esther Rituerto-González∗2, Laura Gutiérrez-Martín1,
Clara Luis-Mingueza2, Manuel F. Canabal1, Alberto Ramírez Bárcenas2,
Jose M. Lanza-Gutiérrez3, Carmen Peláez-Moreno2, Celia López-Ongil1
1Department of Electronics, University Carlos III of Madrid (UC3M), Spain
2 Department of Signal Theory and Communications, University Carlos III of Madrid (UC3M), Spain
3Department of Computer Science, Universidad de Alcalá (UAH), Spain

Abstract

Among the seventeen Sustainable Development Goals (SDGs) proposed within the 2030 Agenda and
adopted by all the United Nations member states, the Fifth SDG is a call for action to turn Gender
Equality into a fundamental human right and an essential foundation for a better world. It includes the
eradication of all types of violence against women. Within this context, the UC3M4Safety research team
aims to develop Bindi. This is a cyber-physical system which includes embedded Artiﬁcial Intelligence
algorithms, for user real-time monitoring towards the detection of affective states, with the ultimate goal of
achieving the early detection of risk situations for women. On this basis, we make use of wearable affective
computing including smart sensors, data encryption for secure and accurate collection of presumed crime
evidence, as well as the remote connection to protecting agents. Towards the development of such system,
the recordings of different laboratory and in-the-wild datasets are in process. These are contained within
the UC3M4Safety Database. Thus, this paper presents and details the ﬁrst release of WEMAC, a novel
multi-modal dataset, which comprises a laboratory-based experiment for 47 women volunteers that were
exposed to validated audio-visual stimuli to induce real emotions by using a virtual reality headset while
physiological, speech signals and self-reports were acquired and collected. We believe this dataset will
serve and assist research on multi-modal affective computing using physiological and speech information.

Emotion Recognition, multi-modal affective computing, Smart Sensors, Physiological Signals, Speech

Signal, Gender-based Violence, dataset collection

Index Terms

I. INTRODUCTION

G Ender-based Violence (GBV) constitutes a violation of human rights and fundamental freedoms

recognised by the 1993 United Nations Declaration on the Elimination of Violence against Women
[1]. This declaration provides a clear and complete deﬁnition of what this type of violence means,
which is stated in its ﬁrst article by considering any act of violence, whether it is physical, sexual, or
psychological, directed toward the female gender. The Council of Europe Convention on Preventing and
Combating Violence against Women and Domestic Violence, also known as the Istanbul Convention,
proposes in its article 1: “a) to protect women against all forms of violence, and prevent, prosecute
and eliminate violence against women and domestic violence” and “design a comprehensive framework,
policies and measures for the protection of and assistance to all victims of violence against women and
domestic violence”, among other objectives. Furthermore, in 2020 the European Commission expanded
such deﬁnition and stated that this violence includes the one against women, men, and children [2]. In
fact, from 2000 to 2018, more than one in four (27%) ever-partnered women aged between 15 and 49
years had experienced physical or sexual, or both, intimate partner violence since the age of 15 years
[3].

Based on the above studies, GBV is an urgent problem that should make society, researchers and policy
makers ﬁght against it by using different perspectives and adopting a multidisciplinary approach. For
instance, from a sociological point of view, education and information awareness regarding the prevention
and combat of violence against women is essential. Moreover, the technological perspective is also a
fundamental aspect related to new emerging technologies that ease the creation of systems for preventing
and responding to GBV [4]. Within this context, digital technology growth has beneﬁted the development
of novel web and smartphone applications targeting the generation of new prevention and combating tools
against GBV. They range from mapping sexual violence exposure within a city to a trusted and direct

∗Corresponding authors: J.A. Miranda, e-mail: jmiranda@ing.uc3m.es, E. Rituerto-González, e-mail: erituert@ing.uc3m.es

2
2
0
2

n
u
J

8

]

C
H
.
s
c
[

2
v
6
5
4
0
0
.
3
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
WEMAC: WOMEN AND EMOTION MULTI-MODAL AFFECTIVE COMPUTING DATASET

2

connection to Law Enforcement Agencies (LEAs) [5, 6]. UC3M4Safety1 multidisciplinary research team
was created in 2017 to make reasearch and propose innovative solutions to prevent and combat GBV. One
of these solutions is called Bindi and is conceived as an end-to-end, smart, inconspicuous, edge-computing,
and wearable solution targeting the automatic detection of GBV situations [7, 8, 9, 10, 11].

It should be highlighted that the ultimate goal of UC3M4Safety is the protection of vulnerable people,
GBV Victims (GBVV) among other groups, with the prevention of violent situations. To achieve this, the
study of the activation mechanisms of GBVV under violent situations, the fear-like emotion detection on
different and non-speciﬁc population proﬁles2, the protection of data for further legal evidence use, and
the automatic protection in risk situations are being accomplished by the team. In this sense, realistic
multi-modal data is required to make the best smart system to detect risky situations through acquired
user-related information in a real-life context, so as people can use it in an inconspicuous way. Although
there exist multiple open available datasets [12, 13, 14, 15], they are based on a general emotional
framework [16]. Additionally, no difference is included between men and women for their proposed
emotional recognition smart systems. This fact is crucial given that stimuli interpretation is strongly
affected by gender [17]. Therefore, one of the main shortcomings of generating fear-related detection
systems is the lack of adequate datasets [18]. Those should provide well-balanced labels (positive vs.
negative emotions) with a sufﬁcient number of volunteers, real emotions, a gender perspective, and
considering the target group of people.

On this basis, we introduce the WEMAC dataset. This is a collection of experiments captured in
laboratory conditions, with both GBVV and non-GBVV participants. A set of audiovisual stimuli
are employed to elicit realistic emotions using virtual reality and acquiring volunteers’ physiological
and speech information. Additionally, self-reported emotional annotations on dimensional and discrete
emotional scales are also collected. The objectives and contributions of this novel multimodal dataset are
multiple, as brieﬂy shown below:

• The integration of immersive technology to elicit emotions. Virtual reality is employed as it offers
the closest resemblance to real world scenarios, offering a high degree of correlation between the
research conditions and the emotional phenomenon under study, i.e. with ecological validity.

• The consideration of a high number of volunteers. The ﬁrst experiment accounted for a total of 104
non-GBV women volunteers. To the best of the authors knowledge, there is no public dataset of
this type accounting for such a high number of volunteers.

• The application of a properly balanced stimuli distribution regarding the target emotions. Prior to
the generation of WEMAC, a mixed methodology [19] with expert judges and general public was
applied to select the best audio-visual stimuli for provoking emotional reactions. A public pool was
run with 1, 332 participants for labelling the pre-selected emotion-related stimuli [20].

• The modiﬁcation of the labelling methodology to consider the gender perspective. This problem was
addressed by changing the original Self-Assessment Manikins [21] to the ones presented in Figure
2. These were used for collecting emotional dimensional annotations of the volunteers to eliminate
gender biases that make labels not appropriate for training machine learning algorithms [22].

• The implementation of an active recovery process regarding the physiological stabilization between
stimuli. To the best of our knowledge, there is no public dataset that implemented an online
stabilisation evaluation by means of physiological feedback assessment during the experiments.

• The usage of different wireless sensory systems to provide an heterogeneous set-up approach.

This paper presents and details the ﬁrst release of the ﬁrst experiment of WEMAC, accounting for
47 out of the 104 total non-GBVV volunteers. Note that the different experiments from WEMAC are
intended to be publicly available throughout different releases. Moreover, the very ﬁrst baseline results
for mono-modal and multi-modal emotion classiﬁcation using WEMAC has been already presented in
[11]3.

The rest of this paper is organised as follows. Section II outlines the state-of-the-art regarding
multi-modal realistic emotions’ datasets. In Section III, we introduce our dataset and data collection
process. Section IV describes the data processing, as well as feature and embeddings extraction. Section V
contains the ﬁnal conclusions and on-going work description.

1https://portal.uc3m.es/portal/page/portal/inst_estudios_genero/proyectos/UC3M4Safety
2Note that non-speciﬁc population proﬁles are referred to as those that have not suffered gender-based violence nor are under

post-traumatic stress conditions.

3This paper is under review for IEEE Internet of Things Journal.

WEMAC: WOMEN AND EMOTION MULTI-MODAL AFFECTIVE COMPUTING DATASET

3

II. RELATED WORK

Affective computing [23] is a multidisciplinary research ﬁeld aimed at recognizing human emotions
to provide better working conditions, entertainment, or services to people. It does not only rely on smart
sensors and digital signal processing but also on artiﬁcial intelligence. This latter technology allows
learning the connections between emotional states and the signals collected from the monitored person or
the environment. For instance, multidisciplinary collaborative research including psychology, computer
science, smart sensors, and cognitive science [24] allows detecting different emotional states through
physiological and physical signal monitoring. Some examples of physical variables include audio, speech,
image or video backgrounds of the scene, or the participant’s body parts –such as face or eyes– tracking.
Some examples of physiological variables include Heart Rate (HR), Blood Volume Pulse (BVP), Galvanic
Skin Response (GSR), Skin Temperature (SKT), Electromyogram (EMG), and Electroencephalogram
(EEG).

Within this context, research on negative emotions detection in GBV situations could help prevent
and combat this problem. Note that potentially threatening situations for the participant cause speciﬁc
negative emotions, such as fear, panic or stress. In this sense, UC3M4Safety claims that the identiﬁcation
of violence-related emotions is of paramount importance when trying to protect women’s lives, not only
from sexual aggressions but also for detecting violence on vulnerable social groups [20]. Although there
is a signiﬁcant activity in the literature regarding emotion recognition through physical and physiological
signals for different purposes and considering different setups [25, 26, 27], none of the solutions focus
on the GBV use case. Moreover, work on the integration of physiological and physical information is
rare in the literature [28, 29].

The labeling of emotions is a crucial yet challenging step for training machine learning models towards
fear recognition. Since the 19th century, different emotion theories and models have been proposed to
understand the human response to external stimuli [30, 31, 32]. However, there are two main theories about
emotion classiﬁcation, which are usually considered for labeling. These are the categorical or discrete and
dimensional models. The former identiﬁes various sets of discrete emotion categories common in different
cultures and distinguishes those positive from negative [33]. The dimensional classiﬁcation deﬁnes an
affective space with two or more dimensions, –such as pleasure, arousal and dominance–, and familiarity
[34]. This dimensional method allows a better differentiation among complex speciﬁc emotions, such as
fear and anger related [35].

From a physiological perspective, approaches on the detection of fear among other emotions is not
scarce [36]. However, to the best of the authors’ knowledge, there are only two fear recognition systems
based solely on physiological information and self-reported labels. On the one hand, the authors in [37]
used all the signals available from the DEAP database [12] to provide a specialized fear recognition
system. They achieved a fear accuracy detection slightly below 90%, thereby considering EEG, which
is not currently feasible as an inconspicuous wearable device. On the other hand, in our own previous
research [18], only three physiological variables available from the publicly available MAHNOB [13]
dataset were used, obtaining a fear recognition accuracy rate of up to 76.67% for a subject-independent
Leave-One-Subject-Out (LOSO) approach using data from 12 women volunteers. Other works presented
in the literature are based on valence and arousal quadrant classiﬁcation rather than binary fear
classiﬁcation. For instance, Zao et al. [38] developed a valence and arousal classiﬁcation system and
obtained a 75.56% accuracy for a subject-dependent approach. In [39], Hassan et al. proposed a deep
learning emotion recognition-based method and obtained up to 89.53% accuracy for a subject-independent
model considering ﬁve discrete emotions (happy, relaxed, disgusted, sad, and neutral).

Regarding the use of speech signals, emotion detection is widely reported in the literature [40, 41].
The lack of existing speech corpora with strong elicited fear in real situations is a problem in speech
emotion research. However, a few studies have managed to achieve results in this regard. For instance,
Clavel et al. [42] developed an audio-based abnormal situations detection system for movie clips. Their
results achieved up to 70.3% accuracy for fear detection in a Leave-One-Trial-Out (LOTO) strategy for 30
movies. In [43], they performed emotion detection with para-linguistic cues in a dialog corpus containing
real agent-client recordings obtained from a medical emergency call center. As a result, they achieved a
recognition rate of up to 64% accuracy for fear.

Based on the previous related work on fear detection for physiological and speech modalities, it should
be noted that most of the emotion recognition systems are neither considering vulnerable groups such as
GBVV nor the inﬂuence on the system robustness caused by real life variations. In essence, this work
afﬁrms that to perform emotion classiﬁcation in real life through physiological and auditory information,
it is necessary not only to rely on the literature generated in recent years [25, 26, 27] but also to provide
novel multi-modal datasets and fusion architectures for the physical and physiological variables. These

WEMAC: WOMEN AND EMOTION MULTI-MODAL AFFECTIVE COMPUTING DATASET

4

architectures should be trained on real (non-acted) fear emotion data, considering immersive technologies
to improve emotion elicitation, such as Virtual Reality (VR). There is still a need for research on these
topics, which this work aims to deepen also enouraging other research groups by making WEMAC
publicly available.

III. PROTOCOL DESIGN FOR THE DATASET GENERATION

As a consequence of the limitations found in the literature discussed previously, the UC3M4Safety

team is releasing different datasets, as part of the UC3M4Safety Database and detailed in Table I.

The ﬁrst two are the audio-visual stimuli datasets, composed of videos clips [44] and Emotional Ratings
[45], respectively. They comprise a list of 79 audio-visual stimuli for emotion elicitation, which were
labelled and selected among 160 clips with: 1) quality criteria, 2) balancing over different emotions, and
3) agreement among viewers [20]. The 160 videos were selected by GBV expert judges and the research
team to achieve a good balance between fear and the rest of the emotions to be elicited. This stimuli
selection was assessed by 1, 332 independent participants (811 females and 521 males) [46]. The other
four datasets belonging to WEMAC, include initial questionnaires answers, physiological and speech
data, and self-reported emotional labels. WEMAC datasets has been captured in laboratory conditions
from women volunteers.

Database

Datasets

Conditions

Participants

Audiovisual Stimuli: Videos [44]

Audiovisual Stimuli: Emotional Ratings [45]

Crowdsourcing

General public and

expert judges

UC3M4Safety

WEMAC: Biopsychosocial Questionnaire [47]

Database [46]

WEMAC: Physiological Signals [48]

WEMAC: Audio Features [49]

WEMAC: Self-reported Emotional Annotations [50]

Laboratory

GBVV and Non-GBVV

Table I: Hierarchy and subdivisions of the UC3M4Safety Database datasets

As stated in Section I, the participants for the ﬁrst WEMAC experiment are women volunteers that
never suffered from GBV. They belong to several age groups deﬁned by 10-year intervals: G1 (18-24), G2
(25-34), G3 (35-44), G4 (45-54), and G5 (55 on-wards). In this ﬁrst release presented and detailed in this
paper, we provide the data from 47 volunteers from a total of 104. The processing of the data for the 57
remaining volunteers is currently work in progress. Note that further releases will update this document
accordingly. Besides, the other WEMAC experiment is currently being recorded and consists, of the
same type of data captured from GBVV in laboratory conditions under the supervision of appropriate
medical personnel. A more visual outline and where to ﬁnd each data from each experiment is detailed
in Table II.

Dataset

Experiment

Release

# Participants

Physiological

Speech

WEMAC

Non-GBVV

GBVV

1
2
N/A

47
104
N/A

(cid:88)

(cid:88)

Initial
Questionnaire
(cid:88)

Self-reported
Annotations
(cid:88)

Work in Progress

Table II: WEMAC experiments and releases.

With regards to the basic emotions considered in UC3M4Safety Database, analysis on the research
of human emotion theories has been done [20], merging different approaches to provide a balanced
(positive/negative) set of basic emotions, equally distributed in a PAD affective space. The adopted list
of basic emotions is listed in Table III.

Positive Emotions
Joy
Surprise
Hope
Attraction
Tenderness
Calm

Negative Emotions
Sadness
Contempt
Fear
Disgust
Anger
Tedium

Table III: Classiﬁcation of discrete emotions in UC3M4Safety Database [20].

WEMAC: WOMEN AND EMOTION MULTI-MODAL AFFECTIVE COMPUTING DATASET

5

In WEMAC, we use virtual reality headsets to present immersive audio-visual stimuli, i.e., videos clips,
to elicit realistic emotion reactions. Simultaneously as the videos are being played, a set of physiological
signals (SKT, GSR, BVP, trapezius EMG, and respiratory signal–RESP–) are being recorded from the
participants, some both with a Bindi prototype and by a standardized equipment. After visualizing every
emotion related video clip, the volunteers ﬁnd two questions on screen that must be answered aloud.
Their answers are captured by a microphone embedded in the virtual reality headset. The participants
are then presented with an interactive screen in which they must select, using a joystick, the labels
for the emotions they have felt while watching the videos, in both dimensional and discrete scales.
Before every emotion-related video clip, a neutral video clip is presented to return the participant to a
neutral emotional state. Figure 1 shows a simpliﬁed diagram of the speciﬁc methodology followed during
the experimentation for every volunteer and stimulus. The details for every experimentation stage are
presented in the following subsections.

Figure 1: Experimental methodology followed during the development of the WEMAC dataset. Prior and
during the experimentation.

A. Initial Steps: participant training and general data collection

Prior to the experiment, all the different phases to be followed are explained to the recruited volunteers
including a set of documents, such as an informed consent and an initial generic questionnaire. The former
is necessary for personal data processing and protection regulation. The latter collects information such
as personality traits, gender, age group, recent physical activity or medication –which could alter the
physiological response of the participant–, self-identiﬁed emotional burdens due to work, economic and
personal situations, mood bias (fears, phobias, traumatic experiences), among others. This information
could be relevant and informative of the participants emotional reactions during the experiment, affecting
their cognition, appraisal and attention. The last step in the experiment preparation is an assessment
demonstration where the volunteers get used to the virtual reality environment –headset and joystick–
and get familiar with the different labelling categories and particularities. Note that the documentation
reading, equipment set-up, self-assessment demo, together with the visualization of the videos, can take
from 1 to 1.5 hours per participant.

B. Audio-visual Stimuli Visualization

We have used an Oculus Rift-S Headset4 to present the audio-visual stimuli. Virtual reality is used
to maximize the immersive experience and consequently, achieve a better emotion elicitation. During
the recording experiment, every volunteer visualises a total of 14 audio-visual emotion-related stimuli,
some of them with a 360◦ experience. These stimuli were selected from a 28 audio-visual stimuli pool
resulting in two batches of videos, as shown in Table IV. Thus, in this ﬁrst release, the videos in the
ﬁrst and second batches are visualized and annotated in the same order by 32 and 15 people volunteers,
respectively. The 28 audio-visual stimuli pool were extracted from the 79 audio-visual stimuli pool in

4https://www.oculus.com/rift-s/

Neutral video recording23 to 120 –seconds VideoInteractive Self-AssessmentResearcher triggers next trialRecovery stabilizationDuring the recordingBefore the recordingDocumentationReading:•Informed consent•Personal data processing•General questionnaireEquipment Set-UpSelf-Assessment DemoWEMAC: WOMEN AND EMOTION MULTI-MODAL AFFECTIVE COMPUTING DATASET

6

[44], mentioned above. The 28 audio-visual stimuli were selected based on three main premises: the
highest emotional discrete labelling agreement observed in women during the pre-labelling experiment
[20], targeting for an adequate laboratory experiment duration, and a balanced distribution of fear vs.
no-fear categories and within the four quadrants in the arousal-valence space.

Analyzing the two batches in the 28 audio-visual stimuli pool, it can be found that the stimuli average
time length of both sets are similar, with 1’32”±46” and 1’46”±44” for the ﬁrst and second batches,
respectively. Moreover, it can be observed that both batches have 8 stimuli belonging to the second
arousal-valence model of quadrants to maintain a proper balance between fear-like and non-fear-like
emotions. Note that the balance premise is assessed considering the valence-arousal –or pleasure-arousal,
PA– model, rather than the pleasure-arousal-dominance (PAD) space, for simplicity. Due to this fact,
stimulus pre-labelled as anger or fear are considered within the second quadrant, then being within the
positive class for the dimensional ground truth labelling.

Stimulus ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

Emotion Label
Joy
Fear
Sadness
Anger
Fear
Calm
Anger
Fear
Disgust
Fear
Joy
Fear
Gratitude
Fear
Fear
Joy
Fear
Sadness
Fear
Calm
Anger
Fear
Disgust
Fear
Surprise
Fear
Gratitude
Fear

A-V Space Quadrant
1
2
3
2
2
4
2
2
3
2
1
2
4
2
2
1
2
3
2
4
2
2
3
2
1
2
4
2

Duration
1’26”
1’20"
1’59"
1’03"
1’35"
1’
1’
23"
40"
2’
1’41”
1’20"
1’40"
1’27"
1’52"
1’28”
46”
45"
1’33”
1’
1’59”
1’14"
1’36"
2’
1’41”
1’06"
1’30"
1’59”

Format
2D
3D
2D
3D
2D
3D
2D
2D
2D
3D
2D
2D
2D
2D
2D
2D
2D
2D
3D
2D
2D
2D
2D
3D
2D
2D
2D
3D

Batch
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2

Table IV: List of selected audio-visual stimuli used within the WEMAC Dataset.

C. Physiological Sensors

During the audio-visual stimuli presentation, the physiological signals of the participants are captured.

The equipment used for this purpose includes the following devices and sensors:

• The BioSignalPlux5

research toolkit system, which is commonly used to acquire different
physiological signals, particularly: ﬁnger BVP, ventral wrist GSR, forearm SKT, trapezoidal EMG,
chest RESP, and wrist inertial movement through an accelerometer.

• The Bindi bracelet. It measures dorsal wrist BVP, ventral wrist GSR, and SKT. The hardware and

software particularities of this element are detailed in previous publications [8, 9, 51].

• An additional GSR sensor to be integrated in the next version of the Bindi bracelet. Its hardware

and software particularities are detailed in previous publications [52].

Note that the BioSignalPlux toolkit is employed to provide golden standard measures to be compared
with the sensors included in the Bindi bracelet. In fact, BVP and GSR signals obtained from BioSignalPlux
and Bindi were successfully compared and correlated in [8] and [51]. The acquisition synchronisation of
all sensors together with the stages of the experiment runs on a laptop through a program based on the
Unity framework6. In this sense, the sampling frequency of the devices sensing physiological information
is 200 Hz.

5https://biosignalsplux.com/products/kits/researcher.html
6https://unity.com/es

WEMAC: WOMEN AND EMOTION MULTI-MODAL AFFECTIVE COMPUTING DATASET

7

D. Self-reported Speech and Annotations

After the emotion-related video clip visualization, volunteers ﬁnd a set of interactive screens within
the virtual reality environment, developed with Unity software [53]. On these screens, volunteers label
their emotional reactions. The annotation is done in the following order:

1) An audio signal containing speech is captured by the Oculus Rift S Headset embedded microphone.
To this end, two questions are asked to the volunteers regarding the video stimuli right after its
visualisation. These questions were designed to make the volunteers relive the emotions felt during
the video visualization. Thus, they intend to capture the last traces of emotion in their voice. (e.g.:
"What did you feel during the visualization of the video?", and "Could you describe what happened
in the video using your own words?"7).

2) Modiﬁed Self-Assessment Manikins (SAM) [54] are used to annotate the values of Valence/Pleasure,
Arousal, and Dominance by a 9-point Likert scale. Such modiﬁed SAMs appear in Figure 2, and
the process of redesign and assessment is detailed in [22].

3) Familiarity with the emotion felt and the situation displayed in the video-clip is also annotated. Both

are answered using the same 9-point Likert scale as for the SAMs.
4) Liking of the video is annotated through a binary yes-no question.
5) Selection of one discrete emotion out of a total of 12, already described in Table III [20].

Figure 2: Modiﬁed SAM by the UC3M4Safety team.

E. Emotional Recovery / Physiological Stabilisation

Before the presentation of each of the stimuli, a neutral video clip is displayed to set the participant
in a neutral emotional state. These neutral video clips have been selected from the large pool provided
by the Stanford Psycho-Physiology Laboratory [55]. Similarly, 3D recovery scenes are also shown to the
volunteers after the interactive emotion labelling process. These 3D scenes were selected by unanimous
consensus of the research team. The main difference between the neutral and recovery clips is that
while during the display of the former no action is taken, i.e., there is no recovery monitoring, for the
latter a physiological monitoring through Bindi bracelet is done to ensure the volunteer’s physiological
stabilisation.

Speciﬁcally, an online stabilisation checking is performed during the experiments for the three different
physiological signals being acquired by Bindi’s bracelet. This online process is operated every ten
seconds performing an online basic ﬁltering of all signals, extracting the heart-rate from the BVP signal,
and ﬁnally assessing the beats-per-minute, GSR, and SKT stabilisation for more than four consecutive
processing windows. The latter process is done by using a hard-threshold adjustment following a 90%
level conﬁdence interval with respect to the level of the signal during the previous temporal window.

7All the questions and answers are in Spanish.

WEMAC: WOMEN AND EMOTION MULTI-MODAL AFFECTIVE COMPUTING DATASET

8

Once the stabilisation is achieved at least by two out of the three variables, the bracelet notiﬁes via
Bluetooth®to the host computer running the virtual reality framework that the stabilisation is completed
and the next video clip can be played.

IV. DATA PRE-PROCESSING

In this section we present the data released in more detail and specify the processing done for each
modality. In this regard, Table Table V reports the time length of the data captured for the physiological
and audio modalities. Note that reference training and testing partition information may be provided in
future releases.

Modality

Partition HH:MM:SS

Physiological Signals

Audio Signals

N/A

N/A

16:37:23

05:40:05

Table V: Total duration of signals data of the ﬁrst release of the ﬁrst experiment of WEMAC

A. Physiological Signals

For this type of information, the signals being released are the ones acquired by the BioSignalPlux
research toolkit. Speciﬁcally, the raw and ﬁltered BVP, GSR, and SKT signals are available. The rest of
the signals from this tooklkit, as well as the ones acquired by the rest of the sensor modules, will be
available in future releases of the dataset.

Regarding the ﬁltering processes, these are designed following the physiological behaviour of every
signal. For instance, to tackle the different noise problems observed in BVP, two main ﬁlters are applied.
On the one hand, the high-frequency noise is ﬁltered out by a direct-form low-pass Finite Impulse
Response (FIR) ﬁlter with -6 dB at 3.5 Hz. Note that a Hamming window is used during the design
process to properly minimise the ﬁrst side lobe. On the other hand, the residual baseline wander or low
frequency drift effect presented in the signal is removed using a forward-backwards low-pass Butterworth
Inﬁnite Impulse Response (IIR) ﬁltering stage. Speciﬁcally, the forward-backwards technique handles the
non-linear phase of such ﬁlters. For the GSR and SKT signals, a basic FIR ﬁltering with 2Hz cut-off
frequency is applied. After that, this ﬁltered output is downsampled to 10Hz and also processed with
both a moving average and a moving median ﬁlters. The former used a 1-second window and helped
reducing the high noise residual after the initial FIR, while the latter employed a 0.5-second window and
dealt with the rapid-transients.

B. Speech Signals

As stated in Section III-D, we capture the audio signal that contains the speech answer to two questions
presented to the participants in the interactive VR screen right after every video viewing. The audio signals
recorded have a duration ranging 20 to 40 seconds and mostly contain speech. They were recorded using
the microphone embedded in the Oculus Rift® S VR device at 48kHz mono and a depth of 16 bits.

The WEMAC signals are processed starting with a low pass ﬁlter at 8kHz where most of the energy is
concentrated. We also apply a high pass ﬁlter with a cut-off frequency of 50Hz to remove the electrical
power line interference. Afterwards, the audio is normalized in amplitude per session to ﬁt the range
[−1, 1]. A downsampling at 16kHz is performed with the librosa Python library [56] to facilitate the
handling of the signals. Finally, the signals are padded with zeros to ﬁll in the incomplete last second.
Also by means of the librosa toolkit, we use a Voice Activity Detection (VAD) module to determine
whether there is speech or silence for every 1 second sample, with a threshold of 0.1.

Since we cannot release the raw speech signals due to ethics and privacy issues, we have processed
the speech signals and extracted low- and high-level features so that the research community can analyze
and work with them8. These features are released within WEMAC and are described next in Section
IV-C.

8The code used for the processing of the physiological and speech signals in this report can be found in https://github.com/

BINDI-UC3M/wemac_dataset_signal_processing

WEMAC: WOMEN AND EMOTION MULTI-MODAL AFFECTIVE COMPUTING DATASET

9

C. Audio Features and Embeddings Extraction

We use different Python toolkits to extract information at a window size of 1 second and a hop size
of 1 second per audio ﬁle. We follow a similar approach to the one followed in the MuSe Challenge
2021 [57] for the feature and embeddings extraction of the audio signals. That is:

1) librosa [58]: we extract the mean and the standard deviation of a collection of features computed at
a window size of 20ms and a hop size of 10ms through the librosa toolkit. The 38 features extracted
are 13 MFCC, RMS or Energy, Zero Crossing Rate, Spectral Centroid, Spectral Roll-off, Spectral
Flatness, and Pitch.

2) eGeMAPS [59]: we compute 88 features related to speech and audio through the openSMILE Python
toolkit [60] on its default conﬁguration, i.e., a window size of 25ms and a hop size of 10ms.
3) ComParE: we extract the 6373 features used in the ComParE 2016 challenge [61] by using the

openSMILE Python toolkit.

4) DeepSpectrum [62]: we extract 6144-dimensional embeddings by this toolkit for the extraction of
audio embeddings based on different deep neural network architectures trained with ImageNet [63].
Speciﬁcally, two different conﬁgurations were considered, ResNet50 network and the output of the
last Average Pooling layer (avg_pool), resulting in 2048-dimensional embeddings, and VGG-19 net
and the last Fully Connected layer (fc2), resulting in 4096-dimensional embeddings.

5) VGGish: we extract 128-dimensional embeddings from the output layer of the VGG-19 network

trained for AudioSet [64].

V. CONCLUSIONS AND FURTHER WORK

In this paper, we introduced the ﬁrst release from the ﬁrst experiment of WEMAC. It includes
physiological and physical signals, and self-reported emotional labelling from realistic elicited emotions.
Purposely, we decided to use a standard equipment along with Bindi to capture a wide range of
physiological and auditory signals and to get the highest possible robustness and realism. Besides these
signals, we also publicly share the extracted features and embeddings, together with the developed code,
for the speech data. The ofﬁcial baseline results for this ﬁrst release are detailed in [11], achieving a
67.59 F1-Score for the binary classiﬁcation of fear and no-fear emotions at audio-visual stimulus level
using a late fusion strategy.

As part of WEMAC, we are currently working on the data processing and curation of a set of 57
Non-GBVV volunteers to be released shortly –making up a total of 104 participants–, under the same
conditions as this ﬁrst release described in this report. Besides, the same experiment conditions have
been replicated for women GBVV volunteers and the collection is on-going work. Ultimately, after the
experience and insight gathered of the completion of these experiments in laboratory conditions and
with the objective of achieving ecological validity, we aim to use Bindi for characterizing the activation
mechanisms of women under everyday-life conditions.

We release this database with the aim of sharing it with the research community, encouraging the
improvement of the baseline results –through the use of fusion methods, attention models, transfer or
federation learning, semi-supervised or self-learning strategies or any other that the research community
ﬁnds adequate – and advancing in the research of multi-modal emotion analysis in general and, in
gender-based equality, in particular.

Please note that this report is subject to updates, and so is the GitHub repository. Contact the

corresponding authors for any further request for audio features or embeddings extraction.

ACKNOWLEDGEMENTS

This work has been supported by the Dept. of Research and Innovation of Madrid Regional Authority,
in the EMPATIA-CM research project (reference Y2018/TCS-5046) and the Spanish Ministry of Science,
Innovation and Universities with the FPU grant FPU19/00448. The authors thank all the members of the
UC3M4Safety for their contribution and support of the present work.

REFERENCES

[1] U. Nations, “Declaration on the elimination of violence against women,” 1993.
[2] E. Commission.

(2020) Gender violence deﬁnition and forms.

[Online]. Available: https://ec.europa.eu/info/policies/

justice-and-fundamental-rights/gender-equality/gender-based-violence/what-gender-based-violence

[3] L. Sardinha et al., “Global, regional, and national prevalence estimates of physical or sexual, or both, intimate partner
violence against women in 2018,” The Lancet, 2 2022. [Online]. Available: https://doi.org/10.1016/S0140-6736(21)02664-7
[4] M. Segrave and L. Vitis, Eds., Gender, Technology and Violence, 1st ed., ser. Routledge Studies in Crime and Society. United

Kingdom: Routledge, 2017.

[5] R. Jewkes and E. Dartnall, “More research is needed on digital technologies in violence against women,” The Lancet Public

Health, vol. 4, no. 6, pp. e270–e271, 2019.

WEMAC: WOMEN AND EMOTION MULTI-MODAL AFFECTIVE COMPUTING DATASET

10

[6] Spanish Ministry of the Interior and Public Security, “alertcops.ses.mir.es,” https://alertcops.ses.mir.es/mialertcops/en/index.

html, (Accessed on 04/04/2021).

[7] J. A. Miranda Calero, R. Marino, J. M. Lanza-Gutierrez, T. Riesgo, M. Garcia-Valderas, and C. Lopez-Ongil, “Embedded
emotion recognition within cyber-physical systems using physiological signals,” in 2018 Conference on Design of Circuits
and Integrated Systems (DCIS), 2018, pp. 1–6.

[8] J. A. Miranda, M. F. Canabal, L. Gutiérrez-Martín, J. M. Lanza-Gutiérrez, and C. López-Ongil, “A design space exploration
for heart rate variability in a wearable smart device,” in 2020 XXXV Conference on Design of Circuits and Integrated Systems
(DCIS), 2020, pp. 1–6.

[9] J. A. Miranda, M. F. Canabal, M. Portela García, and C. Lopez-Ongil, “Embedded emotion recognition: Autonomous
multimodal affective internet of things,” in Proceedings of the cyber-physical systems workshop, vol. 2208, 2018, pp. 22–29.
[10] E. Rituerto-González, J. A. Miranda, M. F. Canabal, J. M. Lanza-Gutiérrez, C. Peláez-Moreno, and C. López-Ongil, “A hybrid
data fusion architecture for bindi: A wearable solution to combat gender-based violence,” in Multimedia Communications,
Services and Security, A. Dziech, W. Mees, and A. Czy˙zewski, Eds. Cham: Springer International Publishing, 2020, pp.
223–237.

[11] J. A. Miranda, E. Rituerto-González, M. F. Canabal, A. R. Bárcenas, J. M. Lanza-Gutiérrez, C. Pelaez-Moreno, and
C. López-Ongil, “Bindi: Affective internet of things to combat gender-based violence,” 2021, manuscript submitted for
publication.

[12] S. Koelstra, C. Mühl, M. Soleymani, J. S. Lee, A. Yazdani, T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras, “Deap: A database
for emotion analysis ;using physiological signals,” IEEE Transactions on Affective Computing, vol. 3, pp. 18–31, 2012.
[13] M. Soleymani, J. Lichtenauer, T. Pun, and M. Pantic, “A multimodal database for affect recognition and implicit tagging,”

IEEE Transactions on Affective Computing, vol. 3, no. 1, pp. 42–55, 2012.

[14] J. A. M. Correa, M. K. Abadi, N. Sebe, and I. Patras, “Amigos: A dataset for affect, personality and mood research on

individuals and groups,” IEEE Transactions on Affective Computing, 2018.

[15] P. Schmidt, A. Reiss, R. Duerichen, C. Marberger, and K. Van Laerhoven, “Introducing wesad, a multimodal dataset for
wearable stress and affect detection,” in Proceedings of the 20th ACM International Conference on Multimodal Interaction,
ser. ICMI ’18. New York, NY, USA: Association for Computing Machinery, 2018, p. 400–408.

[16] J. A. Miranda, M. F. Canabal, J. M. Lanza-Gutiérrez, M. P. García, and C. López-Ongil, “Toward fear detection using affect

recognition,” in Conf. on Design of Circuits and Integrated sys. (DCIS), 2019, pp. 1–4.

[17] D. L. Robinson, “Brain function, emotional experience and personality,” Netherlands Journal of Psychology, vol. 64, no. 4,

pp. 152–168, 2008.

[18] J. A. Miranda, M. F. Canabal, L. Gutiérrez-Martín, J. M. Lanza-Gutierrez, M. Portela-García, and C. López-Ongil, “Fear
recognition for women using a reduced set of physiological signals,” Sensors, vol. 21, no. 5, 2021. [Online]. Available:
https://www.mdpi.com/1424-8220/21/5/1587

[19] C. S. de Baranda Andújar, M. B. Ruiz, J. Á. M. Calero, L. G. Martín, M. F. C. Benito, R. San Segundo, and C. L. Ongil,
“Perspectiva de género y social en las stem: La construcción de sistemas inteligentes para detección de emociones,” Sociología
y tecnociencia: Revista digital de sociología del sistema tecnocientíﬁco, vol. 11, no. 1, pp. 83–115, 2021.

[20] M. Blanco-Ruiz, C. Sainz-de Baranda, L. Gutiérrez-Martín, E. Romero-Perales, and C. López-Ongil, “Emotion
the gender perspective?”
[Online]. Available:

elicitation under
International Journal of Environmental Research and Public Health, vol. 17, no. 22, 2020.
https://www.mdpi.com/1660-4601/17/22/8534

reception: Should artiﬁcial

intelligence

audiovisual

consider

stimuli

[21] M. M. Bradley and P. J. Lang, “Measuring emotion: The self-assessment manikin and the semantic differential,” Journal of

Behavior Therapy and Experimental Psychiatry, vol. 25, no. 1, pp. 49–59, 1994.

[22] C. Sainz de Baranda et al., “Gender biases in technology: Re-design and validation of the self-assessment manikin (sam) in

measuring emotions,” 2022, manuscript submitted for publication.

[23] R. W. Picard, “Affective computing for hci.” in HCI (1). Citeseer, 1999, pp. 829–833.
[24] J. Tao and T. Tan, “Affective computing: A review,” in Affective Computing and Intelligent Interaction, J. Tao, T. Tan, and

R. W. Picard, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, pp. 981–995.

[25] S. D. Kreibig, “Autonomic nervous system activity in emotion: A review,” Biological psychology, vol. 84, no. 3, pp. 394–421,

2010.

[26] P. Schmidt, A. Reiss, R. Dürichen, and K. V. Laerhoven, “Wearable-based affect recognition—a review,” Sensors, vol. 19,

no. 19, 2019. [Online]. Available: https://www.mdpi.com/1424-8220/19/19/4079

[27] S. G. Koolagudi and K. S. Rao, “Emotion recognition from speech: a review,” International journal of speech technology,

vol. 15, no. 2, pp. 99–117, 2012.

[28] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition using multi-modal data and machine learning
[Online]. Available: https:

techniques: A tutorial and review,” Information Fusion, vol. 59, pp. 103–126, 2020.
//www.sciencedirect.com/science/article/pii/S1566253519302532

[29] F. Larradet, R. Niewiadomski, G. Barresi, D. G. Caldwell, and L. S. Mattos, “Toward emotion recognition from physiological
signals in the wild: approaching the methodological issues in real-life data collection,” Frontiers in psychology, p. 1111, 2020.

[30] R. Plutchik, “A psychoevolutionary theory of emotions,” Social Science Information, vol. 21, no. 4-5, pp. 529–553, 1982.
[31] D. L. Robinson, “Brain function, emotional experience and personality,” Netherlands Journal of Psychology, vol. 64, no. 4,

pp. 152–168, Dec 2008.

[32] W. Wundt, “Vorselung über die menschen – und tierseele,” Voss Verlag: Leipzig, Germany, pp. 145–172, 1863.
[33] P. Ekman, “Are there basic emotions?” Psycological Rev., 1992.
[34] J. R. Fontaine, K. R. Scherer, E. B. Roesch, and P. C. Ellsworth, “The world of emotions is not two-dimensional,” Psychological

Science, vol. 18, no. 12, pp. 1050–1057, 2007, pMID: 18031411.

[35] H. A. Demaree, D. E. Everhart, E. A. Youngstrom, and D. W. Harrison, “Brain lateralization of emotional processing:
Historical roots and a future incorporating “dominance”,” Behavioral and Cognitive Neuroscience Reviews, vol. 4, no. 1, pp.
3–20, 2005, pMID: 15886400.

[36] A. F. Ax, “The physiological differentiation between fear and anger in humans,” Psychosomatic medicine, vol. 15, no. 5, pp.

433–442, 1953.

[37] O. B˘alan, G. Moise, A. Moldoveanu, M. Leordeanu, and F. Moldoveanu, “Fear level classiﬁcation based on emotional

dimensions and machine learning techniques,” Sensors, vol. 19, no. 7, 2019.

[38] B. Zhao, Z. Wang, Z. Yu, and B. Guo, “Emotionsense: Emotion recognition based on wearable wristband,” in 2018 IEEE
SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computing, Scalable Computing Communications, Cloud
Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI),
2018, pp. 346–355.

[39] M. M. Hassan, M. G. R. Alam, M. Z. Uddin, S. Huda, A. Almogren, and G. Fortino, “Human emotion recognition using

WEMAC: WOMEN AND EMOTION MULTI-MODAL AFFECTIVE COMPUTING DATASET

11

deep belief network architecture,” Information Fusion, vol. 51, pp. 10 – 18, 2019.

[40] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech emotion recognition: Features, classiﬁcation
[Online]. Available: https:

schemes, and databases,” Pattern Recognition, vol. 44, no. 3, pp. 572–587, 2011.
//www.sciencedirect.com/science/article/pii/S0031320310004619

[41] C. Busso, Z. Deng, S. Yildirim, M. Bulut, C. M. Lee, A. Kazemzadeh, S. Lee, U. Neumann, and S. Narayanan, “Analysis of
emotion recognition using facial expressions, speech and multimodal information,” in Proceedings of the 6th International
Conference on Multimodal Interfaces, ser. ICMI ’04. New York, NY, USA: Association for Computing Machinery, 2004,
p. 205–211. [Online]. Available: https://doi.org/10.1145/1027933.1027968

[42] C. Clavel,

I. Vasilescu, L. Devillers, G. Richard, and T. Ehrette, “Fear-type emotion recognition for

future
audio-based surveillance systems,” Speech Communication, vol. 50, no. 6, pp. 487–503, 2008. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S016763930800037X

[43] L. Devillers and L. Vidrascu, “Real-life emotions detection with lexical and paralinguistic cues on human-human call center

dialogs,” in INTERSPEECH, 2006.

[44] M. Blanco Ruiz, L. Gutiérrez Martín, J. A. Miranda Calero, M. F. Canabal Benito, E. Romero Perales, C. Sainz de
Baranda Andújar, R. San Segundo Manuel, D. Larrabeiti López, C. Peláez Moreno, and C. López Ongil, “UC3M4Safety
Database - List of Audiovisual Stimuli (Video),” 2021. [Online]. Available: https://doi.org/10.21950/LUO1IZ

[45] ——, “UC3M4Safety Database - List of Audiovisual Stimuli,” 2021. [Online]. Available: https://doi.org/10.21950/CXAAHR
[46] M. Blanco Ruiz, L. Gutiérrez Martín, J. A. Miranda Calero, M. F. Canabal Benito, E. Rituerto-González, C. Luis Mingueza,
J. C. Robredo García, B. Morán González, A. Páez Montoro, A. Ramírez Bárcenas, E. Martínez Rubio, E. Romero Perales,
C. Sainz de Baranda Andújar, R. San Segundo Manuel, D. Larrabeiti López, C. Peláez Moreno, and C. López Ongil,
“UC3M4Safety Database description,” http://hdl.handle.net/10016/32481, 2021.

[47] J. A. Miranda Calero, L. Gutiérrez Martín, E. Martínez Rubio, M. Blanco Ruiz, C. Sainz de Baranda Andújar,
E. Romero Perales, R. San Segundo Manuel, and C. López Ongil, “UC3M4Safety Database - WEMAC: Biopsychosocial
questionnaire and informed consent,” 2022. [Online]. Available: https://doi.org/10.21950/U5DXJR

[48] J. A. Miranda Calero, L. Gutiérrez Martín, M. F. Canabal Benito, A. Paez Montoro, A. Ramírez Bárcenas, J. M.
Lanza Gutiérrez, E. Romero Perales, and C. López Ongil, “UC3M4Safety Database - WEMAC: Physiological signals,”
2022. [Online]. Available: https://doi.org/10.21950/FNUHKE

[49] E. Rituerto González, J. A. Miranda Calero, C. Luis Mingueza, L. Gutiérrez Martín, M. F. Canabal Benito, J. M.
Lanza Gutiérrez, C. Peláez Moreno, and C. López Ongil, “UC3M4Safety Database - WEMAC: Audio features,” 2022.
[Online]. Available: https://doi.org/10.21950/XKHCCW

[50] J. A. Miranda Calero, L. Gutiérrez Martín, E. Martínez Rubio, M. Blanco Ruiz, C. Sainz de Baranda Andújar,
E. Romero Perales, B. Alboreca Fernández-Barredo, R. San Segundo Manuel, and C. López Ongil, “UC3M4Safety Database
- WEMAC: Emotional labelling,” 2022. [Online]. Available: https://doi.org/10.21950/RYUCLV

[51] M. F. Canabal, J. A. Miranda, J. M. Lanza-Gutiérrez, A. I. Pérez Garcilópez, and C. López-Ongil, “Electrodermal activity
smart sensor integration in a wearable affective computing system,” in 2020 XXXV Conference on Design of Circuits and
Integrated Systems (DCIS), 2020, pp. 1–6.

[52] M. F. Canabal, J. A. Miranda, A. P. Montoro, I. P. Garcilópez, S. P. Álvarez, E. G. Ares, and C. López-Ongil, “Design and
validation of an efﬁcient and adjustable gsr sensor for emotion monitoring,” 2022, manuscript submitted for publication.
[53] L. Gutiérrez Martín, “Entorno de entrenamiento para detección de emociones en víctimas de Violencia de Género mediante

realidad virtual,” in Bachelor Thesis. University Carlos III de Madrid, 2019.

[54] C. S. de Baranda-Andújar, L. G. Martín, J. A. Miranda, M. Blanco-Ruiz, and C. López-Ongil, “Gender biases in technology:
Re-design and validation of the self-assessment manikin (sam) in measuring emotions,” 2022, manuscript in progress.
[55] J. Rottenberg, R. Ray, and J. Gross, “Emotion elicitation using ﬁlms in: Coan ja, allen jjb, editors. the handbook of emotion

elicitation and assessment,” 2007.

[56] B. McFee, C. Raffel, D. Liang, D. Ellis, M. Mcvicar, E. Battenberg, and O. Nieto, “librosa, audio and music signal analysis

in python,” in Proceedings of the 14th python in science conference, 01 2015, pp. 18–24.

[57] L. Stappen, A. Baird, L. Christ, L. Schumann, B. Sertolli, E.-M. Messner, E. Cambria, G. Zhao, and B. W. Schuller, “The

muse 2021 multimodal sentiment analysis challenge: Sentiment, emotion, physiological-emotion, and stress,” 2021.

[58] B. McFee, A. Metsai, M. McVicar, S. Balke, C. Thomé, C. Raffel, F. Zalkow, A. Malek, Dana, K. Lee, O. Nieto, D. Ellis,
J. Mason, E. Battenberg, S. Seyfarth, R. Yamamoto, viktorandreevichmorozov, K. Choi, J. Moore, R. Bittner, S. Hidaka,
Z. Wei, nullmightybofo, A. Weiss, D. Hereñú, F.-R. Stöter, P. Friesch, M. Vollrath, T. Kim, and Thassilo, “librosa/librosa:
0.9.1,” Feb. 2022. [Online]. Available: https://doi.org/10.5281/zenodo.6097378

[59] F. Eyben, K. Scherer, B. Schuller, J. Sundberg, E. Andre, C. Busso, L. Devillers, J. Epps, P. Laukka, S. Narayanan, and
K. Truong, “The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing,” IEEE
Transactions on Affective Computing, vol. 7, pp. 1–1, 01 2015.

[60] F. Eyben, M. Wöllmer, and B. Schuller, “opensmile – the munich versatile and fast open-source audio feature extractor,” in

Proceedings of the 18th ACM international conference on Multimedia, 01 2010, pp. 1459–1462.

[61] B. Schuller, S. Steidl, A. Batliner, J. Hirschberg, J. Burgoon, A. Baird, A. Elkins, Y. Zhang, E. Coutinho, and K. Evanini,
“The interspeech 2016 computational paralinguistics challenge: Deception, sincerity & native language,” Proceedings of the
Annual Conference of the International Speech Communication Association, INTERSPEECH, vol. 08-12-September-2016,
pp. 2001–2005, 2016, publisher Copyright: Copyright © 2016 ISCA.; 17th Annual Conference of the International Speech
Communication Association, INTERSPEECH 2016 ; Conference date: 08-09-2016 Through 16-09-2016.

[62] S. Amiriparian, M. Gerczuk, S. Ottl, N. Cummins, M. Freitag, S. Pugachevskiy, A. Baird, and B. Schuller, “Snore sound

classiﬁcation using image-based deep spectrum features,” in Interspeech 2017.

ISCA, Aug. 2017, pp. 3512–3516.

[63] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg,
and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,” International Journal of Computer Vision (IJCV), vol.
115, no. 3, pp. 211–252, 2015.

[64] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio set:

An ontology and human-labeled dataset for audio events,” in Proc. IEEE ICASSP 2017, New Orleans, LA, 2017.

