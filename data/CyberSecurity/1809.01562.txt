IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

1

Probabilistic Modeling and Inference for
Obfuscated Cyber Attack Sequences

Haitao Du, Member, IEEE and Shanchieh Jay Yang, Senior Member, IEEE,

8
1
0
2

p
e
S
5

]

R
C
.
s
c
[

1
v
2
6
5
1
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—A key element in defending computer networks is to recognize the types of cyber attacks based on the observed malicious
activities. Obfuscation onto what could have been observed of an attack sequence may lead to mis-interpretation of its effect and
intent, leading to ineffective defense or recovery deployments. This work develops probabilistic graphical models to generalize a few
obfuscation techniques and to enable analyses of the Expected Classiﬁcation Accuracy (ECA) as a result of these different obfuscation
on various attack models. Determining the ECA is a NP-Hard problem due to the combinatorial number of possibilities. This paper
presents several polynomial-time algorithms to ﬁnd the theoretically bounded approximation of ECA under different attack obfuscation
models. Comprehensive simulation shows the impact on ECA due to alteration, insertion and removal of attack action sequence, with
increasing observation length, level of obfuscation and model complexity.

Index Terms—Network Security, Probabilistic Graphical Model, Attack Obfuscation

(cid:70)

1 INTRODUCTION

T HE increasing vulnerabilities and freely distributed cy-

ber attack tools have led to signiﬁcant volume of ma-
licious activities from the Internet to penetrate enterprise
networks. Ptacek [1] pointed out that attack obfuscation is
inevitable because the inherent limitations of Network In-
trusion Detection Systems (NIDS) which monitor and match
the target system response. It is impossible to represent the
exact responses of the ever-increasing operating systems,
applications, and communication protocols, so some ma-
licious actions will not be observed. The signature-based
nature of common NIDS requires pattern matching of ob-
served actions to known malicious signatures, and, hence, is
susceptible to attack obfuscation that can present alternative
pattern. The uses of source IP spooﬁng [2], which hides the
real identity of the attacker, and compromised machines as
stepping stones [3], allows the attacker to either hide crucial
actions and/or inject irrelevant actions to distract analysts
with a large number of actions from various origins.

The possibilities to remove, insert, and alter observables
of malicious actions present signiﬁcant challenges for cyber
defense that aims at analyzing and determining the effect
and intent of a cyber attack based on its manifestation.
Research works beyond intrusion detection have presented
novel methods to extract and correlate observables of cy-
ber attacks to determine their behavior or impact. Exam-
ples in this area include those utilizing Dynamic Bayesian
Networks [4], Variable Length Markov Models [5], Attack
Graphs [6], and Attack Social Graphs [7]. A detailed sum-
mary of how some of these methods can be used to project
cyber attacks is given by Yang et al. [8]. These methods
might not work as well when facing aforementioned and
other obfuscation techniques. It is unclear what level of
effect each type of obfuscation will have on these attack
modeling methods. This work, thus, aims at analyzing for-

mally and generally how obfuscation affects the accuracy of
attack modeling; evaluating which obfuscation techniques
will have a higher impact; and discovering the factors may
change their impact on attack modeling.

A general set of probabilistic graphical models is de-
veloped to represent how cyber attacks may transpire with
and without obfuscation action removal, insertion, and al-
teration. These models enable formal analyses of the effect
of obfuscation to correctly classify an observed sequence
of attack actions to the attack model as if there were no
obfuscation. A formal metric is deﬁned to assess the Ex-
pected Classiﬁcation Accuracy (ECA) based on the concept
of Bayes error. In order to compute the expected value with
exponentially large number of scenarios, this work develops
efﬁcient algorithms based on dynamic programming and
Monte-Carlo sampling to perform approximate inference.
Comprehensive simulation on various combinations of at-
tack models and obfuscation techniques provides insights
of how each obfuscation affects ECA under different obser-
vation sequence length, obfuscation level, and attack model
complexity.

The rest of the paper is organized as follows. Section 2
gives a formal deﬁnition of attack models with and without
the three obfuscation techniques. Section 3 presents the
problem formulation of calculating ECA, and the efﬁcient
algorithms that ﬁnd the theoretically bounded approxima-
tions of ECA given the attack and obfuscation models.
Section 4 illustrates the design of experiment and the simu-
lation results. Finally, Section 5 summarizes and concludes
the work presented in this paper.

2 OBFUSCATION AND FORMULATION

2.1 Attack Obfuscation Techniques

• Department of Computer Engineering, Rochester Institute of Technology,

Rochester, New York 14623.
E-mail: jay.yang@rit.edu

Going beyond manipulating individual events, this paper
focuses on how the attackers can use basic obfuscation
actions together to achieve attack strategy level deception.

 
 
 
 
 
 
IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

2

This work considers three categories of obfuscation strate-
gies: action alteration, action insertion and action removal.
These general categories are based upon experiences in
working with security experts during the DARPA cyber
insider threat project [9]. The term noise attack sequence is
a general term and can be used to describe an observed
alert sequence with intentional (attacker’s obfuscation) or
unintentional noise (such as IDS sensor failure). This work
only considers the case of attacker’s intentional obfusca-
tion. The term noise and obfuscation will be interchangeably
used. The term clean sequence will be used to represent the
original attack on selected target without using obfuscation
techniques.

2.1.1 Action alteration

For signature-based detection engine, modifying the pay-
load and craft a signature for intended alerts can be easily
done. The attacker can alter alerts to hide the true origi-
nation and attack characteristic. Furthermore, sometimes,
to achieve the same reconnaissance or intrusion objective,
many actions can be interchangeable to be played. Changing
the order of attacking actions can create equivalent sequence
which can make the whole sequence more versatile and
avoid being detected by matching to the classical intrusion
sequence pattern. For example, the attacker can change
the time-to-live ﬁled in ICMP ping, to generate alerts that
look like to be originated from other OS platform, while
achieving the same reconnaissance goal. Such obfuscation
can be misleading for some alert correlation systems and
cause alert correlation failure.

2.1.2 Action insertion

Inserting overwhelming alerts can separate related attack
actions to affect the analysis engine, e.g., increasing miss-
classiﬁcation of attack strategy. Even more, overwhelming
alerts can cause Denial-of-Services (DoS) on the analysis
engine, because the capacity of all alert analysis engine are
limited [1], [10].

There are many ways to perform noise injection. The
simplest way is writing a script to keep performing scanning
or getting sensitive ﬁle actions from a target to trigger the
corresponding detection rules. Although such activity will
easily expose attacker’s IP and can be easily blocked by
system administrator, using such simple tricks to injecting
alerts on compromised host can be effective to dilute the
original attack traces. In addition, the self-throttling tech-
nique can be used to replay the actions happened before and
hide the most recent intrusion state, e.g., host discovering,
service scanning, privilege escalation, etc. Finally, Activity
splitting is another type of noise injection. Being aware of
certain patterns of probing that can be triggered by some
detection engines, one can split a malicious signature into
multiple steps, and fragment actions can look normal. For
example, a long sequence of failed log-in attempts is in-
dicative to a dictionary-based password brute-force attack.
Activity splitting will make such action more stealthy.

predictable TCP sequence number vulnerability and can be
completely anonymous when probing target host’s servers.
On the other hand, by carefully choosing encoding or en-
cryption schemes, an attacker’s critical actions may not be
observed with traditional NIDS alerts.

2.2 Obfuscated Attack Formulation

2.2.1 Attack model

We consider an attack strategy/model as a probabilistic
sequence model, e.g., Markov model or Hidden Markov
Model (HMM) [11], to describe the different possible attack
actions and capture the casual relationship of attack actions
using transition probabilities. More speciﬁcally, an attack
sequence is described as a vector of random variables and
each observation is an instance/sample of the attack model.
When obfuscated, the attack sequence is modeled by an-
other vector of random variables, where an obfuscation model
represents the obfuscation techniques probabilistically. The
joint distribution is the overall description for the attack
sequence that contains possible obfuscated observations. Be-
cause the clean attack sequence and the obfuscated sequence
are not independent, one needs to jointly treat the attack
model and the obfuscation model for probability inference.
Let discrete random variable Ω ∈ {0, 1, 2, 3, · · · } rep-
resents the set of possible attack actions (examples of attack
actions can be found in Fig. 7), the attack sequence is deﬁned
as a length-N vector random variable X, where random
variable Xk ∈ Ω, k ∈ {1, 2, 3, · · · , N } is deﬁned as the kth
observed action in the attack sequence X. An attack model
is a probabilistic sequence model to specify P (X), which is
shown in (1) as a Lth order Markov model.

P (X) = P (X1, · · · , XL)

N −L
(cid:89)

k=1

P (XL+k|XL+k−1, · · · , Xk)

(1)

where P (X1, · · · , XL) represents the initial distribution of
the Lth order Markov model. The Markov property (given
L observations past and further are independent) enables
the product form decomposition of P (X).

The attack model discussed here does not take the
obfuscated observations into account and it represents the
intended attack strategy of the attacker. The term clean
attack sequence is used to represent the sequences directly
generated from this attack model.

Let a random variable vector Y represents obfuscated
attack sequence. Y depends on X and P (Y|X) describes
the obfuscation model. As discussed in Section 2, other
than action alteration, in general X and Y in different
length. Therefore, we categorize obfuscation into two types,
according to the length of X and Y.

• Type-I obfuscation model: |X| = |Y|
• Type-II obfuscation model: |X| (cid:54)= |Y|

2.1.3 Action removal

2.2.2 Type-I model for action alteration

Action removal is the obfuscation technique where an at-
tacker hides critical actions that are indicative of the intru-
sion state. For example, Idle scanning [2] takes advantage of

For Type-I model, i.e., the clean and noise sequence have the
same length. One way to model the dependencies between
X and Y is to apply HMM as shown in (2).

IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

3

P (Y|X) =

N
(cid:89)

P (Yk|Xk)

X1

(2)

· · ·

XN/2

k=1
In HMM, observed event at time i only directly depends
on the corresponding hidden state. The emission probabil-
ity P (Yk = y|Xk = x) can be described by a discrete
function, g(x, y). HMM is not a perfect model for attack
obfuscation because there is no direct parameter to describe
the amount of obfuscations. In addition, the connections in
graphical notation are only from Xk to Yk, which is limited
in modeling real-world attacks.A more general obfuscation
model P (Y|X) may consider additional constraints on Y.
For example, one extension of (2) can be shown in (3).

Y1

Y2

· · ·

· · ·

YN −1

YN

(a) Graphical representation for action insertion

X1

X2

· · ·

· · ·

XN −1

XN

P (Y|X) =

1
(cid:0)N
M

(cid:1) I(|Y − X|H = M )

N
(cid:89)

k=1
k:Xk(cid:54)=Yk

P (Yk|Xk) (3)

Y1

· · ·

YN/2

where I(·) is the indicator function, | · |H represents the
Hamming distance between the two vectors.

The added parameter M can be interpreted as an esti-
mate of the percent (M out of N ) of attack actions the at-
tacker may change actions. It serves to complement g(x, y),
which describes the preference on which obfuscation action
is more likely to be chosen. The detail of the noise model
shown in (3) is described in [4]. On the other hand, we also
extend the basic HMM structure to second or higher-order
on X instead of just the ﬁrst-order. Note that the previous
works on attack sequence modeling [5] [12] [13] have shown
higher-order models are needed. Our extension also allows
assessing the impact of obfuscation as a function of the
percentage of attack actions altered, which will be described
in the simulation section.

2.2.3 Type-II model for action insertion and action removal
For Type-II model, X and Y have different lengths and
how much obfuscation exists will be directly reﬂected in
model structure. This work uses regularized structure as an
example to show the inference design. The model structure
can be easily changed, which will be discussed at the end of
this subsection.

Figure 1(a) shows an example of the action insertion
model, whereas for every clean action, one additional ob-
fuscated action can be injected for a sequence Y. In other
words, the current action Yk conditionally depends on the
previous action Yk−1 and the clean attack action Xk/2.

The obfuscation model P(Y|X) for Fig. 1(a) can be
described in (4). Similarly, for the action removal case, the
model structure for removing one out of every two clean
actions is shown in Fig. 1(b). Its The obfuscation model is
shown in (5).

(b) Graphical representation for action removal

Fig. 1. Graphical representation for action insertion and removal

The model structures shown here are examples to model
cyber attack with obfuscations, but not intended to suggest
that they are the most realistic or the only cases. This
work focuses on how to treat models of similar structure
to determine the limit to inference the clean actions from
obfuscated ones.

For example, in the attack action alteration case (Type-
I), one could model the clean sequence and noise sequence
model as auto-regressive HMM [14]. In such a case, noise
action Yi depends on the previous noise action Yi−1. For
the action insertion case (Type-II), one could model that the
injected noise is conditionally independent of any variable,
i.e., the attack randomly injects noise and there are no links
between the injected noise and the true observations. Other
examples for the Type-II model include extending it with
a higher-order dependency on X, adding constrains on Y,
and removing dependencies by setting a special parameter
of the general model.

3 PROBABILISTIC INFERENCE FOR OBFUSCATED
ATTACKS
3.1 Deﬁnition: Expected Classiﬁcation Accuracy (ECA)

The attack obfuscation models proposed enable one to
assess the impact under different attack obfuscation tech-
niques. We ﬁrst discuss about matching a clean attack
sequence X to pre-deﬁned models. The problem can be
described as ﬁnding the attack model C that maximize the
posterior P (C|X).

P (Y|X) = P (Y1|X1)P (Y2|Y1, X1)

N/2
(cid:89)

i=2

P (Y2i−1|Y2i−2, Xi)P (Y2i|Y2i−1, Xi)

(4)

arg max

C

P (C|X) = arg max

C

= arg max

C

P (X|C)P (C)
P (X)
P (X|C)P (C)

(6)

P (Y|X) = P (Y1|X1, X2)

N/2
(cid:89)

i=2

P (Yi|Yi−1, X2i−1, X2i)

(5)

To evaluate the impact caused by attack obfuscations,
it is necessary to understand the performance limit of the
classiﬁcation when attacks contain obfuscation. This work

IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

4

proposes to use Expected Classiﬁcation Accuracy (ECA)
to assess how having obfuscated observations may affect
the classiﬁcation accuracy. ECA is closely related to many
concepts in statistics, such as the irreducible error or Bayes
error rate [15]. This metric assumes that the true distribution of
attacks is known and expressed in statistical graph models.
However, even with the model of the attack distribution,
there may still be errors when making classiﬁcation. This
is because the true distribution of the different classes may
overlap. The term irreducible error or Bayes error rate describes
how much the overlap is, and the ECA is deﬁned as one
minus Bayes error. Given an obfuscated sequence Y, ECA
is deﬁned as

ECA :=

(cid:88)

Y

P (Y) max

C

P (C|Y)

(7)

Equation (7) can be explained as follows: for any given
obfuscated observation Y, P (C|Y ) can be calculated for
all attack models, and the noise sequence Y can be clas-
siﬁed into pre-deﬁned models by using arg maxC P (C|Y).
By doing such classiﬁcation, maxC P (C|Y) percent of all
classiﬁcation results will be correct. Summing over all pos-
sible Y will give us the mathematical expectation of the
classiﬁcation accuracy.

Distributing P (Y) into the max operation, (7) can be

written as

(cid:88)

Y
(cid:88)

Y

max
C

max
C

=

P (C|Y)P (Y)

P (C, Y) =

(cid:88)

Y

max
C

P (Y|C)P (C)

(8)

Here, the probability of a clean observation sequence
for a given attack model P (X|C), the prior of attack mod-
els P (C) and the noise model P (Y|X) are assumed to
be known. To calculate (cid:80)
Y maxC P (Y|C)P (C), two sub-
problems need to be solved:

• Given P (X|C) and P (Y|X), calculate P (Y|C).
• Given P (Y|C), calculate (cid:80)
The ﬁrst subproblem can be solved by an extension of
Message Passing Algorithm [16] using dynamic program-
ming, and the second subproblem can be approximated
by Monte-Carlo sampling with any desired precision and
conﬁdence.

Y maxC P (Y|C)P (C).

In summary, the problem addressed in our framework
can be described as how to calculate the performance
limit in the presence of obfuscated attack observations
for different obfuscation strategy P (X|Y). That is, calcu-
late (cid:80)
Y P (Y) maxC P (C|Y), given P (X|C), P (C), and
P (Y|X). The next subsection will show that this calcula-
tion, i.e., is computational challenging and requires efﬁcient
algorithms. Using the proposed algorithms to assess ECA,
security analysts will be able to assess and weigh the poten-
tial impact of attack obfuscation under different scenarios.

3.2 Dynamic programming for solving sub-problem 1

Probabilistic inference is the problem of calculating speciﬁc
marginal or conditional distributions for a given model.
The inference is computationally challenging because brute-
force calculation needs to account for the exponential num-
ber of terms. Performing exact inference for arbitrary model

structure of P (X) is a NP-hard problem [14]. However,
for some special structure of P (X) there exists efﬁcient
algorithm.

In HMM literature, there are classical algorithms to
perform exact inference. For example, the Viterbi algorithm
[11] efﬁciently calculates the most probable path of a clean
sequence by solving (9).

arg max

X

P (X|Y)

(9)

Likewise, one can efﬁciently calculate P (Y) for HMM,
because P (Y|X) is relatively simple for HMM. However,
some of the existing algorithms cannot be directly applied
to the Type-I and Type-II models proposed in this paper.
For Type-I model, constraint M on Y will affect the possible
values of X and the arg max operation will only apply to a
subset of X that satisﬁes the constraint. For Type-II model,
the length of X and Y are different.

Here we discuss two simpliﬁed cases to show the idea
of solving the inference problem for our models. After
discussing the simpliﬁed problem, the algorithm design for
general model strictures and extensions are given.

3.2.1 Inference on a chain structure

Suppose we want to solve the optimization problem shown
in (10).

max
X

N −1
(cid:89)

k=1

f (Xk, Xk+1)

(10)

The relationship for all the variables with a chain struc-
ture, where Xi only interact with Xi−1 and Xi+1. This
simpliﬁed problem is very similar to the ﬁrst-order Markov
model: according to the Markovian property, Xi−1 and Xi+1
are independent given Xi. Because the objective function
has such a special chain structure, one can use dynamic
programming techniques to solve the optimization problem.
Deﬁne a function Fi(a) as the cost of the best length-i
subsequence that ends with a symbol a.

i−1
(cid:89)

f (Xk, Xk+1)

(11)

Fi(a) = max
X1···Xi

k=1
s.t. Xi = a

With the chain structure, the only connections between
the subsequence X1 to Xi and the subsequence Xi to XN
is the variable Xi. The reason we set the constrain of the
subsequence ends with a symbol a is because we want
to decouple the interactions between two subsequences. Such
constrain will allow us to solve the problem in a smaller
scale, and derive recursion rules for extension, which leads
to the use of dynamic programming.

Let Fi(a) be the cost of the best length-i subsequence
and end with a symbol a as shown in (11). Equation (12)
gives the relationship between Fi(a) and Fi−1(a), that can
be used in Algorithm 1 to ﬁnd the optimal solution for (10).

Fi(a) = max

b

Fi−1(b) · f (b, a)

(12)

Algorithm 1 has a complexity of Θ(N · |Ω|2), where
N is the length of the sequence and |Ω| is the number
of the possible values of the random variables. This is a

IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

5

signiﬁcant improvement over the brute-force approach over
the exponential search space with Θ(|Ω|N ) complexity.

Algorithm 1: Inference on a chain structure

Input: Given the sequence length N , function f (x, y),

Xi ∈ Ω
Output: maxX
for a ∈ Ω do

(cid:81)N −1

k=1 f (Xk, Xk+1)

Initialize F2(a) = max
X1

f (X1, a)

end
for i ∈ 3, 4, · · · , N do
for a ∈ Ω do

Fi(a) = max

b

Fi−1(b) · f (b, a)

end

end
return max

a

(FN (a))

3.2.2 Inference on a chain structure with constraints

As discussed earlier, Type-I model has the additional con-
strain on Y. Here, we add the constraint to the chain struc-
ture to illustrate the algorithm design. The revised problem
is shown in (13). The major difference is that only a subset
of X needs to be considered. The subset of X depends on
M and Y, i.e., only M number of elements in X are allowed
to be different from a given vector Y.

(cid:88)

N −1
(cid:89)

X:|X−Y|H =M

i=1

f (Xi+1, Xi)

(13)

Comparing to the solution without the additional con-
straint, one can add another dimension to the dynamic
programming table to decompose the dependencies on the
constraint. We deﬁne function Fi,j(a) as

Fi,j(a) =

(cid:88)

i−1
(cid:89)

f (Xk, Xk+1)

(14)

X1,··· ,Xi
s.t. Xi = a

k=1

| < X1, · · · Xi > − < Y1, · · · Yi > |H = j

Let Fi,j(a) be the sum for (13) over the subsequence
X1, · · · , Xi. In addition, the subsequence X1, · · · , Xi
is
different from Y1, · · · , Yi by j elements. The relationship
between Fi,j(a), Fi−1,j(a) and Fi−1,j−1(a) can be described
in (15).

Fi,j(a) =






(cid:88)

(cid:88)

b

b

Fi−1,j(b) · f (b, a)

if a = Yi

Fi−1,j−1(b) · f (b, a)

if a (cid:54)= Yi

(15)

3.2.3 Calculating the noise sequence distribution for Type-I
and Type-II models

Using the recursion rule shown in (15), the complexity of
the algorithm is Θ(N · M · |Ω|2), where N is the length
of the sequence and M is the total number of elements

that are different between X and Y. The complexity can
be intuitively explained as follows: the dimension of the
dynamic programming table is N × M . For every entry, it
stores a function of a. The calculation for an entry requires
searching over the Ω space. In addition, evaluating the
function needs to sum over the variable b as shown in (15),
which needs another loop over Ω.

After discussing inference on a chain structure and infer-
ence with the additional constraint, we give the algorithm of
calculating obfuscated sequence distribution for Type-I and
Type-II models. To simplify the notation for the following
discussion, we drop the random variable C in P (Y|C) to
focus on a speciﬁc and given attack model and obfuscation
model.

For the action alteration case, let f (x, y) = P (Xi+1 =
y|Xi = x), g(x, y) = P (Yi = y|Xi = x). The dynamic
programming function Fi,j(A) is deﬁned in (16), where the
length of vector A is L and is used to decouple the Lth-
order Markov model and the recursion rules are shown in
(17).

For the action insertion case, let f (x, y) = P (Xi+1 =
y|Xi = x), g(x, y, z) = P (Y2i−1 = z|Y2i−2 = x, Xi = y),
φ(x, y, z) = P (Y2i = z|Y2i−1 = x, Xi = y). The dynamic
programming function is deﬁned in (18), which gives the
recursion rule.

For the action removal case, let f (x, y) = P (Xi+1 =
y|Xi = x), g(x, y, z, p) = P (Yi = p|Yi−1 = x, X2i−1 =
y, X2i = z). Then we expand and re-write the product
operation. The goal is to make the product subscript the
same, so we can deﬁne the dynamic programming function.
The dynamic programming function is shown in (20), with
the recursion rules shown in (21).

3.3 Approximate inference for solving sub-problem 2

This section discusses the second sub-problem: how to com-
pute the ECA for noise sequence Y, i.e., how to evaluate
(8), where P (Y|C) can be calculated from solving sub-
problem 1 and P (C) is known. Similar to the problem of
calculating P (Y|C) addressed before, calculating the ECA
in (8) also need to sum over an exponential number of terms.
However, unlike solving the problem of P (Y|C), to the best
of our knowledge, there is no efﬁcient algorithm to solve (8)
efﬁciently.

The challenge to solve (8) is due to the following. There is
no analytical expression for P (Y|C); instead, for any given
Y, we need to execute Algorithm 1 to get P (Y|C). Without
the appropriate structure to decompose the problem, the
dynamic programming concept does not apply any more.
In addition, the max operation makes the problem more
complicated and prohibits the decoupling of the problem
with an optimal structure.

We propose to approximate

(cid:88)

Y

max
C

P (Y|C)P (C)

with arbitrary precision and conﬁdence using Monte-Carlo
method. This is feasible because the samples of Y can be
effectively drawn and the sample average can be used to
estimate the mean as described in (22).

IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

Fi,j(A) =

(cid:88)

(cid:18)

X:X1,X2,··· ,Xi

P (X1, X2, · · · , XL)

i−L
(cid:89)

k=1

f (Xk, · · · , Xk+L)

i
(cid:89)

(cid:19)

g(Xk, Yk)

k=1
i:Xk(cid:54)=Yk

s.t. < Xi−L+1, · · · , Xi >= A

| < X1, · · · , Xi > − < Y1, · · · , Yi > |H = j

Fi,j(A) =






(cid:88)

B

B

(cid:88)

Fi−1,j(< B, A(cid:48) >) · f (< B, A >),

if AL = Yi

Fi−1,j−1(< B, A(cid:48) >) · f (< B, A >)g(AL, Yi),

if AL (cid:54)= Yi

Fi(a) =

(cid:88)

i−1
(cid:89)

X1,··· ,Xi

k=1

P (X1)f (Xk, Xk+1)

i
(cid:89)

k=2
s.t.

P (Y1|X1)P (Y2|Y1, X1)g(Y2k−2, Xk, Y2k−1)φ(Y2k−1, Xk, Y2k)

Xi = a

Fi(a) =

(cid:88)

b

Fi−1(b) · f (b, a) · g(Y2i−2, a, Y2i−1) · φ(Y2i−1, a, Y2i)

Fi(a) = P (X1)P (X2|X1)P (Y1|X1, X2)

(cid:88)

i
(cid:89)

f (X2k−2, X2k−1)f (X2k−1, X2k)

i
(cid:89)

k=2

g(Yk−1, X2k−1, X2k, Yk)

X1,··· ,X2i
s.t.

k=2
X2i = a

Fi(a) =

(cid:88)

b,c

Fi−1(b) · f (b, c) · f (c, a) · g(Yi−1, c, a, Yi)

6

(16)

(17)

(18)

(19)

(20)

(21)

Fig. 2. Recursion Rules for Action Insertion and Action Removal Inference Algorithm

Algorithm 2: Algorithm to estimate the ECA

Input: Given P (X|C), P (C), P (X|Y), sample size n
Output: ECA for sequence classiﬁcation
Set S =<>
for i ∈ {1, 2, · · · , n} do

Sample C0 from P (C)
Given sample C0, sample X0 from P (X|C0)
Given sample X0, sample Y0 from P (Y|X0)
Set V =<>
for c ∈ {1, 2, · · · , |C|} do

Given Y0, calculate P (Y0|c)
Append V with P (Y0|c)P (c)

end
Append S with max(V)/sum(V)

end
return max(S)/sum(S)

as maxC P (C|Y) and can be calculated from P (Y|C) and
P (C). That is

ψ(Y) = max

C

P (Y|C)P (C)
P (Y)

= max

C

P (Y|C)P (C)

(23)

Base on Hoeffding’s bound [17], we have

P

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

k=1

(cid:12)
(cid:12)
(cid:12)
ψ(Yk) − E[ψ(Y)]
(cid:12)
(cid:12)

(cid:33)

> (cid:15)

≤ δ

(24)

where n represents the number of samples and δ (cid:44)
2 exp(−2n(cid:15)2).

2(cid:15)2 log 2

Applying (24) with n ≥ 1

δ , with probability at
least 1 − δ, the difference between the approximation and
the true value is at most (cid:15). In the experiments shown in this
paper, we use 30,000 samples, which is larger than 26, 492
needed for (cid:15) = 0.01 and δ = 0.01. The approximation
algorithm is given in Algorithm 2.

P (Y)ψ(Y) ≈

(cid:88)

Y

1
n

n
(cid:88)

k=1

ψ(Yk)

(22)

where Yk are independent and identically distributed ran-
dom variables sampled from P (Y), and ψ(Y) is deﬁned

3.4 Approximate Inference Algorithm Efﬁciency

We verify the proposed algorithm as follows: for a small
observation length, it is possible to calculate the exact ECA
by summing over all X. We did the exact calculation for
observation length from 5 to 10 and use the proposed

IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

7

approximation algorithm for comparison. The simulation
results show the estimation is better than the theoretical
bounds. The theoretical bounds tell us the error should be
smaller than 0.01 with probability 99%. The experiments
of 10 repetitions show 0.003 max error for different obser-
vation lengths. This is not surprising since the Hoeffeding
bound is usually a loose bound.

The time cost1 comparison between approximation and
exact calculation is given in Figures 3 and 4. The exact cal-
culation time cost increases exponentially as the observation
length N increases. On the other hand, the time costs via
Monte-Carlo approximation have relatively small variations
(between 1 to 2.5 seconds) for different observation lengths.
Similar phenomena can be observed as the noise level M
or the action set size Ω increases. The reason is that no
matter how many possible values X can take, only a ﬁxed
number of samples (30, 000 in our case) are taken to estimate
the ECA. To verify the noise inference algorithm, we use
brute-force algorithm to calculate P (Y = Y0) by summing
over X that satisﬁes the constraints. In comparison, we use
dynamic programming to calculate P (Y = Y0). In sum-
mary, the time cost with Monte-Carlo sampling is within a
reasonable range for Algorithm 2.

Fig. 3. Brute force time cost for calculating P (Y|C)

Fig. 4. Proposed algorithm time cost for calculating P (Y|C)

4 IMPACT ANALYSIS OF OBFUSCATION MODELS
4.1 Simulation Design

The simulation setup is shown in Fig. 5. The experiment
considers 4 attack models and 5 obfuscation models. The
clean attack sequences are generated from the attack mod-
els. Mixed attack sequences from different models feed
into different attack obfuscation techniques. The sequence
classiﬁers for clean sequences and noise sequences assume
the full knowledge of the attack model but not the attack

1. The prototype system was written in Python and was executed on

a desktop with i5 CPU and 16G RAM.

obfuscation model. The likelihoods are compared for a given
observed sequence across different generative models to
perform classiﬁcation. On the other hand, the noise infer-
ence algorithm utilizes the knowledge of the attack obfus-
cation models to calculate the distribution of the obfuscated
sequence to make the sequence classiﬁer better. We compare
the classiﬁcation results for the clean attack sequence and
the noise attack sequence using the proposed inference
algorithm. The ECA deﬁned in Section 3.1 is the primary
performance metric.

Figure 6 gives the reference network diagram used for
the experiment. It describes a small enterprise network with
six subnets, eleven servers and four clusters of hosts (24
hosts in total). The whole network has 31 open services (15
types total) and interconnected via four routers. The attacker
began the attack from the Internet. The external servers
(web server and ﬁle server) were ﬁrst attacked with abuse
of functionality. After a few steps, the attacker obtained the
vulnerability information and performed a buffer overﬂow
attack on the ﬁle server and compromised the external
ﬁle server 192.168.1.3. Using this stepping stone, the at-
tacker compromised the internal server (Domain controller
192.168.3.1) and use it to probe the hosts in Department C
(192.168.30.x).

In general, one can model network attacks at various
levels with a combination of attributes reported by NIDS
and host logs. To demonstrate the use of the proposed
framework, this paper considers 15 classical, widely used
attack actions from ﬁve categories selected from MITRE’s
common attack pattern enumeration and classiﬁcation [18].
The attack categories and action space Ω are shown in Fig.
7.

The ﬁve categories (C-0 to C-4) show different levels
(stages) of the intrusion process. Abuse of functionality is
a low-proﬁle information-gathering step; taking advantage
of the function provided by the target system can achieve
a certain level of information collection without leaving
much malicious trace, because such functions are designed
to serve normal requests. For example, instead of scanning
the target web server to get the server version, one can try
to access a non-existing web page and observe the HTTP-
404-ERROR generated by the server, which can expose the
server platform and version. Network reconnaissance is a
category of high-proﬁle scanning in addition to abuse of
functionality. Actions in this category are essentially taking
advantage of the TCP/IP protocol, e.g., TRACEROUTE,
PING, NMAP, etc., to explore unknown environment. Prob-
abilistic techniques represent another type of exploration,
using a number of trials to identify vulnerabilities. For
example, fuzzing is widely used in software testing by
feeding the system with invalid, unexpected random inputs.
By observing the system feedback, an experienced attacker
can discover possible design ﬂaws, including the chance
of getting buffer overﬂow or code injection vulnerability,
which is part of category C-3 and can eventually compro-
mise the target machine. After compromising the target the
ultimate goal of an attack can be stealing sensitive data
or data excavation, e.g., generic cross-browser cross-domain
thefts [18], or data interception/sniffer.

We consider

two ﬁrst-order
four attack models,
(Strategy-1 and Strategy-2) and two second-order (Strategy-

IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

8

Fig. 5. Simulation overview for attack sequence classiﬁcation with obfuscations

Fig. 6. Network used for simulation

3 and Strategy-4) ones. As pointed out by Fava et.al. [19]
and Du et.al. [5], most attack behaviors can be captured
with ﬁrst and second-order Markov models. Furthermore,
higher order models can be too speciﬁc with high complex-
ity and perform poorly because of Bias-variance trade-off
[15]. The two models are inspired from real attacks in ICTF
hacking competition data set [20] [21] and CAIDA data
set [22] [23]. Attack Strategy-1 can be explained as a two
phases attack: reconnaissance and intrusion. The attacker is

more hesitant to switch between phases than stay within a
phase. The speciﬁc probability numbers in the table reﬂect
the characteristics of the attack, e.g., the automatic script
the attacker is using. In fact, our experience suggests that
the probabilities of action transitions are quite reliable for
detecting the attack tools such as Metasploit [24] or Nessus
[25]. Attack Strategy-2 shows a different attack strategy: the
attacker utilizes the reconnaissance actions throughout the
attack process, and perform speciﬁc exploits only sporad-

IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

9

• Abuse of functionality (C-0)

– Detect unpublicised web pages and services

(A-0-1)

– Directory traversal (A-0-2)
– Web server application ﬁngerprinting (A-0-3)

• Network reconnaissance (C-1)

Infrastructure-based footprinting (A-1-1)

–
– Host discovery (A-1-2)
–

Scanning for vulnerable software (A-1-3)

• Probabilistic techniques (C-2)

Fuzzing (A-2-1)
Screen temporary ﬁles for information (A-2-2)

–
–
– Client-server protocol manipulation (A-2-3)
– Dictionary-based password attack (A-2-4)

• Buffer overﬂow and code injection (C-3)

– Manipulating user-controlled variables (A-3-1)
– Command or script injection (A-3-2)
– Hijacking a privileged process / thread (A-3-3)

• Data leakage attacks (C-4)

– Data excavation attacks (A-4-1)
– Data interception / sniffer attacks (A-4-2)

Fig. 7. An example of action space (attack patterns)

ically. Attack Strategy-3 and Attack Strategy-4 are much
more complicated models since the second-order Markov
transition matrix has 15 times more parameters (the size
of action space). The overall idea for Attack Strategy-3 and
Attack Strategy-4 is very similar to Attack Strategy-1 and
Attack Strategy-2, respectively, but more parameters are
used to describe the speciﬁc attack behavior. For example,
Attack Strategy-3 describes the attacker spending more time
on C-1 reconnaissance stage and we have higher chance
to observe a long sequence of A-0-1. Attack Strategy-4 is
a high-proﬁle attack but with speciﬁc long sequences of
certain vulnerability attempts.

For action alternation obfuscation model, the Obfusca-
tion Strategy-1 is mostly trying to exchange actions in the
same category, such as Abuse of functionality, Network
reconnaissance, etc. For action insertion obfuscation, two
speciﬁc scenarios are designed for simulation. Obfuscation
Strategy-2, describes an attacker injecting independent noise
observations, which means that the injected actions have
nothing to do with the previous attack action and the ”clean
attack actions”. The injected actions have its own distribu-
tion conditionally independent to other random variables.
For example, the injected noisy actions can have 80% of
abuse of service action, and 20% of network reconnaissance
action. Obfuscation Strategy-3 describes a more complicated
action injection plan, the injected actions depend on the
clean actions. For example, in the network reconnaissance
stage and vulnerability attempt stage, the attacker would
have different preference to inject more actions in some
categories than other categories. Such noise injection plan
can be effective to confuse the alert analysis engine on the

intrusion stage assessment and conceal the intrusion stage and
the real intent.

For action removal obfuscation, Obfuscation Strategy-
4 describes the attacker attempting on different services
and vulnerabilities with a mixture of attack actions such
as buffer overﬂow and abuse of functionality over different
services. Note that, depending on the conﬁguration, an
NIDS may detect some but not all different actions. Obfusca-
tion Strategy-5 represents more stealthy and decoy attacks.
There are covering-up actions with intended actions. We
assume the covering-up actions conditionally depend on the
intended actions. This is different from executing random
actions because the goal of such obfuscation is to mislead
the analyst, the noise sequence Y is linked to the previous
noisy actions.

4.2 The Impact of Alteration, Insertion, vs. Removal

The algorithm proposed in this paper enables us to assess
the impact of attack obfuscations. We ﬁrst evaluate the
ECA with the three types of obfuscation models (Alter-
ation, Insertion vs. Removal). The algorithms proposed in
this paper calculate P (Y) efﬁciently, and P (X) can be
directly derived based on the given attack models. Note that
maxC P (C|Y) represents the best likelihood one can match
a given sequence Y to a model C, and the 1−maxC P (C|Y)
also implies the least error for the given sequence and
the models. To assess the overall impact to any sequence,
clean or obfuscated, that can occur under the attack models,
one will need to calculate (cid:80)
X P (X) maxC P (C|X) and
(cid:80)

Y P (Y) maxC P (C|Y).
The simulation results shown below can be interpreted
by comparing the “InfAlg” case with the “noise” case. The
“InfAlg” case gives the ECA when the proposed algorithms
are used to infer/classify the obfuscated attack sequences
to the original attack model, whereas the “noise” case is
when the obfuscated sequences are directly classiﬁed to
the best matched attack model without using the inference
algorithms. The improvement in ECA from the “noise” case
to the “InfAlg” case gives the recovery or improvement in
classiﬁcation accuracy using the proposed algorithms. In
addition, we present the “clean” case to reﬂect the ideal sce-
nario where classiﬁcation is performed on attack sequences
where no obfuscation is done, which is the best one can ever
recover for ECA.

Figure 8(a) shows the inference algorithm achieves more
effective ECA improvements for Noise Insertion (NI) and
Action Removal (AR) with approximately 20%, compar-
ing to the 5% in the Action Alteration (AA) case. It also
shows that the ECA increases as the observation length
increases, e.g., the more observations of the same attack
behavior, the easier one can classify sequences to the correct
attack models. Figure 8 compares the ECA when the ﬁrst
and second-order attack models are used. We observed
that when the obfuscation level increases, the performance
drops, especially for the second-order model case without
inference. At around the obfuscation level of 28%, the ob-
fuscated sequences without inference for the second-order
case actually begin to exhibit worse performance than that
for the ﬁrst-order case. Fortunately, even with the limited
knowledge of obfuscation, the optimal classiﬁcation rate can

IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

10

(a) Impact for action insertion models and action removal models

(b) Impact for ﬁrst and second-order attack models

Fig. 8. ECA for Noise Insertion (NI), Action Removal (AR) and Action Alteration (AA) obfuscations and comparison between the ﬁrst and second-
order attack models

be recovered, e.g., from 60% to 90% when the obfuscation
level is at 40%. Interestingly, the performance recovered
through inference for the second-order model case is better
than that for the ﬁrst-order case, at least up to the 45%
obfuscation level, which is very high. Generally speaking,
the higher the obfuscation level, the more improvements
one can achieve, for both ﬁrst and second-order cases. The
performance recovered through inference is closer to the
absolute limit exhibited by the clean curves for the second-
order model case than that for the ﬁrst-order model case, at
least when the obfuscation level is not too high.

4.3 The impact of obfuscation level estimation

One important parameter in obfuscation model for action
alteration is the obfuscation level, e.g., how much action
alteration exists in the sequence. In order to run the noise
inference algorithm, this parameter M is assumed to be
known. In real situation, one may argue that it is not
reasonable for security analysts to know how an attacker
performs obfuscation in such a detailed level. Therefore, in
this subsection, we want to investigate how the parameter
M impact the proposed algorithm, and how much impact
the inaccurate estimation of M can cause empirically. We
will show that, only an approximation of the noise level is
needed to get a reasonably good inference results. In other
words, as long as the analysts have a rough estimation of
the level of obfuscation, the proposed algorithm will be

useful to classify attack sequences. In our simulation, the
obfuscated attack sequences were created using the true ob-
fuscation level value Mtrue, while the estimated obfuscation
level Mest is intentionally set to deviate from Mtrue. The
algorithm is executed based on the Mest value, to assess
how ECA might be different when the estimated value is
not accurate.

Figure 9 shows the ECA under various obfuscation level
estimations. Figure 9(a) shows the inaccurate M when the
observation length N ranges from 10 to 60. The real ob-
fuscation level Mtrue is 40%. The estimated Mest ranges
from 20% to 60%. Obviously, ECA is the highest when
Mtrue = Mest = 40%. Also, the closer Mest is to Mtrue,
the better performance it has. Interestingly, the ECA does
not change much as Mest moves away from Mture, this is
true even for different observation lengths.

Figure 9(b) evaluates the effect of Mest as Mtrue in-
creases from 30% to 55%. We test the cases when |Mest −
Mtrue| = 10% and 20% with a ﬁxed observation length
of 40. The results are similar to that from Fig. 9(a). The
larger the difference between Mest and Mtrue the lower the
ECA. However the impact of this difference is insigniﬁcant.
Again these observations remain the same as the noise level
increases from 30% to 55%.

Figure 9(c) and 9(d) give the ECA when noise level
M approaches two extreme cases, e.g., M = 0% and
M = 100%. Note that, for M = 0% the inference algorithm

20304050Sequence Length0.60.70.80.91Expected Classification AccuracyCleanNINI InfAlgARAR InfAlgAAAA InfAlg0.10.20.30.40.50.60.70.80.91Noise Level MExpected Classification Accuracy  2nd order, Clean2nd order, Noise2nd order, InfAlg1st order, Clean1st order, Noise1st order, InfAlgIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

11

(a) Inaccurate M estimation with respect to different sequence length

(b) Inaccurate M estimation with respect to different noise level

(c) Inaccurate M estimation with respect to different noise level

(d) Inaccurate M estimation with respect to different noise level

Fig. 9. Action Alteration Obfuscation, Inaccurate Noise Model Estimation Impact Simulation

will not work, because P (Y|X) = 0. At the extreme low
noise level, inaccurate estimation of M will actually have
slightly worse ECA when using the inference algorithm.

As M approach 100%, the results are shown in Figure
9(d). In this case, the inference algorithm can always make
improvements and the inaccurate M estimation does not
affect the ECA too much. Furthermore, ECA value remains
similar for Mtrue ranging from 70% to 100%. This can be
explained with the combinatorial number of possible noise
sequences. Speciﬁcally, as discussed earlier, the possible
(cid:1) (|Ω| − 1)M . There-
noise sequences for a given M is
fore, the number of possible sequences will reduce when
M = 100%, e.g., the number of changes equals to the
sequence length. A similar trend was observed in the time
cost evaluation results (Fig. 3)

(cid:0)N
M

5 CONCLUSION

The mixture of organized cyber crimes and random attacks
against enterprise and government networks has led to
asymmetric cyber battleﬁelds ﬁlled with large-scale cyber

attacks. To obtain a timely situation awareness from over-
whelming and noisy observations, defense can beneﬁt from
probabilistic graphical models to inference true attacks from
obfuscated observations.

This work developed a general framework to model
attack alteration, insertion, and removal obfuscation strate-
gies. To evaluate the impact for speciﬁc attack scenarios, The
ECA metric has been proposed, which enables the study
of the beneﬁts and limitations of attack sequence modeling
and classiﬁcation. Comprehensive simulations on various
combination of attack models and obfuscation techniques
show that the noise insertion has the highest impact, i.e.,
gives the lowest ECA when breaking the attack sequence
pattern by random noise actions. Fortunately, the infer-
ence algorithm proposed can recover ECA well in most
cases. In general, our experimental results show that as the
observed sequence length increases, or as the obfuscation
level decreases, the ECA can improve. As the attack model
complexity increases, attack obfuscation can cause higher
impact, but the inference algorithm can also recover more.

1020304050600.50.60.70.80.91Sequence LengthExpected Classification Accuracy  CleanNoiseMtrue=40%, Mest=40%Mture=40%, Mest=30%Mtrue=40%, Mest=50%Mtrue=40%, Mest=20%Mtrue=40%, Mest=60%0.30.40.50.50.60.70.80.91Obfuscation Level MtrueExpected Classification Accuracy  CleanNoiseMtrue=MestMture−Mest=10%Mtrue−Mest=−10%Mtrue−Mest=20%Mtrue−Mest=−20%0.250.20.150.10.050.80.850.90.951Obfuscation Level MtrueExpected Classification Accuracy  CleanNoiseMtrue=MestMture−Mest=10%Mtrue−Mest=20%0.70.80.910.50.60.70.80.91Obfuscation Level MtrueExpected Classification Accuracy  CleanNoiseMtrue=MestMture−Mest=10%Mtrue−Mest=20%IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, SPECIAL ISSUE

12

The proposed inference algorithm is also shown to be robust
when the estimated level of obfuscation is not accurate.

The attack obfuscation modeling framework and the
inference algorithms developed here can be applied to other
contexts beyond analyzing network attacks. Any observed
sequences that might suffer from noise and require match-
ing to pre-deﬁned models can use this work to recover
the most likely original model or evaluate quantitatively
the optimal performance one can achieve to separate the
observed instances.

ACKNOWLEDGMENTS

This research is partial supported by National Science Foun-
dation Award #1526383.

REFERENCES

[1] T. H. Ptacek and T. N. Newsham, “Insertion, evasion, and denial
of service: Eluding network intrusion detection,” DTIC Document,
Tech. Rep., 1998.

[2] G. Lyon, Nmap Network Scanning: The Ofﬁcial Nmap Project Guide
Insecure Publishing,

to Network Discovery and Security Scanning.
2009.

[3] A. Fuchsberger, “Intrusion Detection Systems and Intrusion Pre-
vention Systems,” Information Security Technical Report, vol. 10,
no. 3, pp. 134–139, 2005.

[4] H. Du and S. Yang, “Probabilistic Inference for Obfuscated Net-
work Attack Sequences,” in Proceedings of the 44th Annual IEEE/I-
FIP International Conference on Dependable Systems and Networks
(DSN 2014).

IEEE, 2014, pp. 1–11.

[5] H. Du, D. Liu, J. Holsopple, and S. Yang, “Toward Ensemble
Characterization and Projection of Multistage Cyber Attacks,” in
Proceedings of the 19th IEEE International Conference on Computer
Communications and Networks (ICCCN’10), 2010, pp. 1–8.

[6] L. Wang, A. Liu, and S. Jajodia, “Using attack graphs for corre-
lating, hypothesizing, and predicting intrusion alerts,” Computer
Communications, vol. 29, no. 15, pp. 2917 – 2933, 2006.

[7] H. Du and S. Yang, “Discovering collaborative cyber attack
patterns using social network analysis,” in Proceedings of Social
Computing, Behavioral-Cultural Modeling and Prediction (SBP’10).
Springer, 2011, pp. 129–136.
S. J. Yang, H. Du, J. Holsopple, and M. Sudit, Attack Projection.
Springer, 2014.

[8]

[9] Defense Advanced Research Projects Agency (DARPA) Cyber
[Online].
https://www.fbo.gov/index?s=opportunity&mode=

Insider Threat Program.
Available:
form&tab=core&id=585e02a51f77af5cb3c9e06b9cc82c48

(Access Date: Oct. 2017).

[10] J. Haines, D. Ryder, L. Tinnel, S. Taylor, and D. Kewley Ryder,
“Validation of sensor alert correlators,” IEEE Security & Privacy,
vol. 1, no. 1, pp. 46–56, Jan. 2003.

[11] L. R. Rabiner, “A tutorial on hidden markov models and selected
applications in speech recognition,” Proceedings of the IEEE, vol. 77,
no. 2, pp. 257–286, 1989.

[12] H. Du and S. J. Yang, “Characterizing transition behaviors in Inter-
net attack sequences,” in Proceedings of the 20th IEEE International
Conference on Computer Communications and Networks (ICCCN’11),
2011, pp. 1–6.

[13] H. Du and S. Yang, “Temporal and spatial analyses for large-scale
cyber attacks,” Handbook of Computational Approaches to Counterter-
rorism, vol. 1, no. 2, pp. 559–578, 2013.

[14] K. P. Murphy, “Dynamic bayesian networks: representation, infer-
ence and learning,” Ph.D. dissertation, University of California,
2002.

[15] T. Hastie, R. Tibshirani et al., The Elements of Statistical Learning:
Data Mining, Inference and Prediction, 2nd ed. Springer, 2009.
[16] C. M. Bishop, Pattern Recognition and Machine Learning (Information
Secaucus, NJ, USA: Springer-Verlag New

Science and Statistics).
York, Inc., 2006.

[17] R. Serﬂing, “Probability inequalities for the sum in sampling
without replacement,” The Annals of Statistics, vol. 2, no. 1, pp.
39–48, 1974.

[18] Common Attack Pattern Enumeration and Classiﬁcation. URL:
[Online].

http://capec.mitre.org, Access Date: Aug.
Available: http://capec.mitre.org

2013.

[19] D. Fava, S. Byers, and S. Yang, “Projecting cyberattacks through
variable-length markov models,” IEEE Transactions on Information
Forensics and Security, vol. 3, no. 3, pp. 359–369, Sept. 2008.

[20] UCSB International Capture The Flag (ICTF) Hacking Competition
[Online]. Available:

(Access Date: Oct.

2017).

Data Set.
http://ictf.cs.ucsb.edu/

[21] N. Childers, B. Boe, L. Cavallaro, L. Cavedon, M. Cova, M. Egele,
and G. Vigna, “Organizing Large Scale Hacking Competitions,”
in Detection of Intrusions and Malware, and Vulnerability Assessment
(DIMVA), vol. 6201. Springer, 2010, pp. 132–152.

[22] The CAIDA UCSD network telescope two days in November
[Online]. Available:
(Access Date: Oct. 2017).

2008 dataset.
http://www.caida.org/data/overview/

[23] D. Moore, C. Shannon, G. Voelker, and S. Savage, “Network

telescopes: technical report,” Tech. Rep., 2004.

[24] D. Maynor and K. Mookhey, Metasploit toolkit for penetration testing,
exploit development, and vulnerability research. Syngress Publishing,
2007.

[25] J. Beale et al., Nessus network auditing. Syngress Publishing, 2004.

Haitao Du Dr. Haitao Du received his B.S. de-
gree in Telecommunications Engineering from
Xidian University, Xi’an, China in 2006, and his
Ph.D. degree in Computing and information sci-
ences from Rochester Institute of technology,
2014. He is currently a data scientist at K12 inc.
and driving the process building big data solu-
tions that achieve predictive modeling on student
academic performance and retention.

Shanchieh Jay Yang Dr. S. Jay Yang received
his B.S. degree in Electronics Engineering from
National Chiao-Tung University, Hsin-Chu, Tai-
wan in 1995, and his M.S. and Ph.D. degrees
in Electrical and Computer Engineering from
the University of Texas at Austin in, 1998 and
2001, respectively. He is currently a Professor
and the Department Head for the Department
of Computer Engineering at RIT. He and his
research group has developed several systems
and frameworks in the area of cyber attack mod-
eling for predictive situation, threat and impact assessment. He has
published more than sixty papers and was invited as a keynote speaker,
a panelist, and a guest speaker in various venues. He was a co-chair for
IEEE Joint Communications and Aerospace Chapter in Rochester NY
in 2005, when the chapter was recognized as an Outstanding Chapter
of Region 1. He has also contributed to the development of two Ph.D.
programs at RIT, and received Norman A. Miles Award for Academic
Excellence in Teaching in 2007.

