2
2
0
2

y
a
M
8
1

]

R
C
.
s
c
[

1
v
8
6
9
8
0
.
5
0
2
2
:
v
i
X
r
a

Monitoring Security of Enterprise
Hosts via DNS Data Analysis

Jawad Ahmed

A dissertation submitted in fulﬁllment

of the requirements for the degree of

Doctor of Philosophy

School of Electrical Engineering and Telecommunications

The University of New South Wales

June 2021

 
 
 
 
 
 
To my family

Abstract

Enterprise Networks are growing in scale and complexity, with heterogeneous con-
nected assets needing to be secured in diﬀerent ways. Nevertheless, virtually all
connected assets use the Domain Name System (DNS) for address resolution, and
DNS has thus become a convenient vehicle for attackers to covertly perform Com-
mand and Control (C&C) communication, data theft, and service disruption across
a wide range of assets. Enterprise security appliances that monitor network traﬃc
typically allow all DNS traﬃc through as it is vital for accessing any web service;
they may at best match against a database of known malicious patterns, and are
therefore ineﬀective against zero-day attacks. This thesis focuses on three high-
impact cyber-attacks that leverage DNS, speciﬁcally data exﬁltration, malware C&C
communication, and service disruption. Using big data (over 10B packets) of DNS
network traﬃc collected from a University campus and a Government research or-
ganization over a 6-month period, we illustrate the anatomy of these attacks, train
machines for automatically detecting such attacks, and evaluate their eﬃcacy in the
ﬁeld. The contributions of this thesis are three-fold:

• Our ﬁrst contribution tackles data exﬁltration using DNS. We analyze out-
going DNS queries to identify many stateless attributes such as the number
of characters, the number of labels, and the entropy of the domain name to
distinguish malicious data exﬁltration queries from legitimate ones. We train
our machines using ground-truth obtained from a public list of top 10K legit-
imate domains and empirically validate and tune our models to achieve over
98% accuracy in correctly distinguish legitimate DNS queries from malicious
ones, the latter coming from known malware domains as well as synthetically
generated using popular DNS exﬁltration tools.

• Our second contribution tackles malware C&C communication using DNS. We
analyze DNS outgoing queries to identify more than twenty families of DGA
(Domain Generation Algorithm)-enabled malware when communicating with
their C&C servers. We identify attributes of network traﬃc which commences
following the resolution of a DGA-based DNS query. We train three protocol-
speciﬁc one-class classiﬁer models, for HTTP, HTTPS and UDP ﬂows, using

i

public packet traces of known malware. We develop a monitoring system
which uses reactive rules to automatically and selectively mirror TCP/UDP
ﬂows (between internal hosts and malware servers) pertinent to DGA queries
for diagnosis by the trained models. We deploy our system in the ﬁeld and
evaluate its performance to show that it ﬂags more than 2000 internal as-
sets as potentially infected, generating more than a million suspicious ﬂows of
which more than 97% are veriﬁed to be malicious by an oﬀ-the-shelf intrusion
detection system.

• Our third contribution studies the use of DNS for service disruption. We an-
alyze incoming DNS messages, with a speciﬁc focus on non-existent (NXD)
DNS responses, to distinguish benign from malicious NXDs. We highlight two
attack scenarios based on their requested domain names. Using NXD behav-
ioral attributes of internal hosts, we develop multi-staged iForest classiﬁcation
models to detect internal hosts that are launching service disruption attacks.
We show how our models are able to detect infected hosts which generate
high-volume as well as low-volume distributed NXD-based attacks on public
resolvers and/or authoritative name servers with an accuracy of over 99% in
correctly classifying legitimate hosts.

Our work shines a light on a very important vector in enterprise security and
equips the enterprise network operator with the means to detect and block sophis-
ticated attackers who use DNS as a vehicle for malware C&C communication, data
exﬁltration, and service disruption.

ii

List of Publications

During the course of this thesis project, a number of publications have been made
based on the work presented here and are listed below for reference.

Journal Publications

1. J. Ahmed, H. Habibi Gharakheili, and V. Sivaraman, “Learning-Based Detec-
tion of Malicious Hosts by Analyzing Non-Existent DNS Responses,” (under
review at IEEE Globecom 2022) (Outcome of Chapter 5)

2. J. Ahmed, H. Habibi Gharakheili, C. Russell and V. Sivaraman, “Automatic
Detection of DGA-Enabled Malware Using SDN and Traﬃc Behavioral Mod-
eling,” in IEEE Transactions on Network Science and Engineering TNSE, May
2022, doi: 10.1109/TNSE.2022.3173591 (Outcome of Chapter 4)

3. J. Ahmed, H. Habibi Gharakheili, Q. Raza, C. Russell and V. Sivaraman,
“Monitoring Enterprise DNS Queries for Detecting Data Exﬁltration From
Internal Hosts," in IEEE Transactions on Network and Service Management,
vol. 17, no. 1, pp. 265-279, March 2020, doi: 10.1109/TNSM.2019.2940735.
(Outcome of Chapter 3)

Conference Publications

4. J. Ahmed, H. Habibi Gharakheili, Q. Raza, C. Russell and V. Sivaraman,
“Real-Time Detection of DNS Exﬁltration and Tunneling from Enterprise Net-
works," 2019 IFIP/IEEE Symposium on Integrated Network and Service Man-
agement (IM), Arlington, VA, USA, 2019, pp. 649-653. (Outcome of Chapter
3)

5. J. Ahmed, H. Habibi Gharakheili, Q. Raza, C. Russell and V. Sivaraman,
“Demo Abstract: A Tool to Detect and Visualize Malicious DNS Queries for

iii

Enterprise Networks," 2019 IFIP/IEEE Symposium on Integrated Network
and Service Management (IM), Arlington, VA, USA, 2019, pp. 729-730. (Out-
come of Chapter 3)

iv

Acknowledgment

In the name of Allah Almighty, the most beneﬁcent and the most merciful. All
praise to Him, who bestowed upon me the wisdom and strength to accomplish this
task gracefully. I owe acknowledgments and thanks to a number of people for their
guidance, support, and prayers that enabled me to conclude this work successfully.

First and foremost, I would like to express my appreciation to my supervisor Vijay
Sivaraman, had it not been for his constant guidance, instructions, encouragement,
and occasional timely reminders, I would certainly never have successfully completed
this thesis. His support has been extremely valuable. At the end of this long
collaboration, I have got nothing but a lot of respect for him. The second person
I owe a great deal of thanks and respect to is my joint supervisor, Hassan Habibi
Gharakheili, for his insightful and enlightening comments and opinions; he has been
a reliable source of information and yet a meticulous critic. His expertise played a
signiﬁcant part in shaping the ideas presented in this thesis. We had many in-depth
technical discussions which were especially valuable for my thesis. I strongly feel
that this journey would not have been possible without the support and nurturing
of Vijay and Hassan. I would like to thank my supervisor at CSIRO/Data61 Craig
Russell for his support and comments on my research work.

I would also like to express my great appreciation to the University of New South
Wales (UNSW) and CSIRO/Data 61 for providing all the resources and a pleasant
atmosphere for conducting research. Without their support, I would not have been
able to embark on this.

I would also like to thank my colleagues Minzhao, Ayyoob, Tara, Arunan, Chi-
naei, Iresha, Sharat, Arman, Lance, Qasim, and Asaf, Thank you all for your fruitful
discussions and unwavering support that made this journey a wonderful one.

I have been extremely fortunate to have wonderful friends who were instrumental
in making my journey to this point a memorable and pleasurable one. Tariq, Ali,

v

Saad, Talal, Awais, Atif, Haq, Ihsan, Kaleem, Zubair, Akhtar, Bilal, Ahmed, Ali
Raza, Usman, Saqib, Azhar, Yasir Rizwan, Yasir Malik, Badar, Babar and Sir Taj,
thank you for being there for me.

I cannot thank enough my parents for all their selﬂess love.

I would like to
acknowledge the numerous sacriﬁces they have made for me starting from the day I
was born and continuing to date. I would have been nothing without their love and
compassionate support. I would also like to thank my brother and my sisters, for
their love and aﬀection. Last, but certainly not least, I am very grateful to my wife
and my kids who have been very patient and supportive in the time I have been
away from them. Needless to say without my wife’s sacriﬁces, I would not have been
able to complete work on my thesis successfully.

vi

Contents

Abstract

List of Publications

Acknowledgment

List of Figures

List of Tables

1 Introduction

1.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.1 Data theft using Domain Name Queries . . . . . . . . . . . . .
1.1.2 Malware Command and Control Communication using DGAs
Service Disruption using Random Domain Names . . . . . . .
1.1.3
1.2 Research Contributions . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Thesis Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . .

i

iii

v

x

xiii

1
2
3
4
4
6
7

2 Security of DNS Infrastructure

2.1 DNS Infrastructure: Vulnerabilities and Challenges

9
. . . . . . . . . . 11
2.1.1 DNS Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.1.2
Steps for DNS Resolution in an Enterprise Network . . . . . . 11
2.1.3 Vulnerabilities and Challenges of Existing DNS Architecture . 13
2.2 DNS-Based Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
. . . . . . . . . . . . . . . . . . . . . 14
. . . . . . . . . . . . . . . . . . . . . . . . 17
2.3 Detection Systems in Cyber-Security . . . . . . . . . . . . . . . . . . 18
2.3.1 Knowledge-Based Detection . . . . . . . . . . . . . . . . . . . 18
2.3.2 Machine Learning-Based Detection: Anomaly Detection . . . . 19
2.3.3 Limitations of Existing Detection Mechanisms . . . . . . . . . 19
2.4 Review on Current State-of-the-Art DNS security . . . . . . . . . . . 20

2.2.1 Non Volumetric Attacks
2.2.2 Volumetric attacks

vii

2.4.1 Monitoring DNS Queries of Enterprise Hosts for DNS Exﬁl-

tration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

2.4.2 Behavioral Analysis of DGA-Enabled Malware of Enterprise

2.4.3

Hosts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Identifying Malicious Hosts by Analyzing DNS NXDs . . . . . 27
2.5 Research Gaps in Prior Works . . . . . . . . . . . . . . . . . . . . . . 28
2.5.1 Novelty of Our Approach . . . . . . . . . . . . . . . . . . . . . 29
2.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

3 Monitoring DNS Queries for Detecting Data Exﬁltration

31
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

3.1
3.2 DNS Queries of Enterprise Hosts: Data Collection and Attributes

Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.2.1 Our Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
. . . . . . . . . . . . . . 37
3.2.2 Query Name Attributes Engineering
3.2.2.1 Count of Characters . . . . . . . . . . . . . . . . . . 39
3.2.2.2 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . 41
Labels . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.2.2.3
3.3 Detection of Anomalous Queries . . . . . . . . . . . . . . . . . . . . . 46
3.3.1 Machine Training . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.3.2 Algorithms and Tuning Parameters . . . . . . . . . . . . . . . 47
3.4 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 50
3.4.1 Performance Metrics . . . . . . . . . . . . . . . . . . . . . . . 51
3.4.2 Evaluating Models using Known DNS Exﬁltration Data . . . . 54
3.4.3 Comparing Multi-Class Classiﬁer with One-Class Classiﬁer . . 57
3.4.4 Malicious DNS Queries from Enterprise Networks . . . . . . . 59
3.5 Real-Time Deployment in Campus Network . . . . . . . . . . . . . . 63
3.5.1 Visualizing Detection of Malicious DNS Queries . . . . . . . . 64
3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

4 Automatic Detection of DGA-Enabled Malware

67
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.2 Analyzing Network Traﬃc Data . . . . . . . . . . . . . . . . . . . . . 71
. . . . . . . . . . . . . . . . . . . . . . . . . . . 72
4.2.1 Our Datasets
4.2.2 DGA-Fueled Malware Families . . . . . . . . . . . . . . . . . . 74
4.2.3 Daily Activity Pattern of DGA-Based Domains
. . . . . . . . 77
Infected Enterprise Hosts . . . . . . . . . . . . . . . . . . . . . 81
4.2.4
4.2.5 Network Behavior of DGA-Fueled Malware . . . . . . . . . . . 82
. . . . 87

4.3 Modeling and Mirroring Traﬃc of Suspicious Malware Servers

viii

4.3.1 Modeling Traﬃc Behavior of Malware . . . . . . . . . . . . . . 88
4.3.1.1 Attributes and Classiﬁers for Malware Flows . . . . . 89
4.3.1.2 Model Training . . . . . . . . . . . . . . . . . . . . . 90
4.3.1.3 Model Validation . . . . . . . . . . . . . . . . . . . . 93
4.3.1.4 Models Testing . . . . . . . . . . . . . . . . . . . . . 95
4.3.2 Dynamic Traﬃc Selection using SDN . . . . . . . . . . . . . . 96
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
. . . . . . . . . . . . . . . . . . . . 100
4.4.1
4.4.2 Diagnosing DGA Flows . . . . . . . . . . . . . . . . . . . . . . 101
4.4.3
. . . . . . . . 103
4.4.4 Comparing Our Diagnosis Models with Zeek IDS . . . . . . . 108
4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

Infected Hosts Initiating Malicious DGA Flows

SDN-selected DGA Flows

4.4 Evaluation Results

5 Learning-Based Detection of Malicious Hosts by Analyzing Non-

110
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

Existent DNS Responses
5.1
5.2 Analyzing Two Variants of DNS Random Subdomain Attacks in Our

Campus Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
. . . . . . . . . . . . . . . . . . . . . . . . . 115
5.2.1 Attack Scenarios
5.2.1.1 Attack on Authoritative DNS Servers . . . . . . . . . 117
5.2.1.2 Attack on Open Resolvers . . . . . . . . . . . . . . . 119
5.2.2 Drawback of Using a Threshold for NXDs Attack Detection . 121
5.3 Multi-Staged Machine Learning Architecture . . . . . . . . . . . . . . 122
5.3.1
System Design . . . . . . . . . . . . . . . . . . . . . . . . . . 122
5.3.2 Model Training . . . . . . . . . . . . . . . . . . . . . . . . . . 126
5.4 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.4.1 Performance of Fine-Grained Model . . . . . . . . . . . . . . . 127
. . . . . . . . . . . . . 128
5.4.2 Performance of Coarse-Grained Model
5.4.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
5.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129

6 Conclusions and Future Work

131
6.1 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
6.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

document

135

ix

List of Figures

2.1 Key concepts covered in this chapter
. . . . . . . . . . . . . . . . . . 10
2.2 Domain Name Space. . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.3 DNS Architecture.
. . . . . . . . . . . . . . . . . . . . . . . . . 15
2.4 DNS Attack Categories.
. . . . . . . . . . . . . . . . . . . . 16
2.5 DNS Exﬁltration Attack - Basics.

3.1 Number of queries per unique primary domain, over a week (Rsch:

397K, Univ: 1.1M). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

3.2 CCDF of reputation rank: (a) Research institute, and (b) University

campus.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.3 Real-time number of queries. . . . . . . . . . . . . . . . . . . . . . . . 37
3.4 CCDF of number of characters in query name for: (a) Research in-

stitute, and (b) University campus.

. . . . . . . . . . . . . . . . . . . 39

3.5 Scatter density map of numerical fraction of characters vs.

total
length of query name for: (a) Research institute, and (b) University
campus.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

3.6 Scatter density map of upper-case fraction of characters vs.

total
length of query name for: (a) Research institute, and (b) University
campus.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

3.7 CCDF of entropy of query name for: (a) Research institute, and (b)

University campus.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

3.8 CCDF of length of labels in query name for: (a) Research institute,

and (b) University campus.

. . . . . . . . . . . . . . . . . . . . . . . 45

3.9 Attributes of DNS exﬁltration query names of: (a) DET tool, and (b)
Iodine tool, detected vs. undetected by the University model.
3.10 Number of DNS queries for top malicious domains over a day.
3.11 Web-UI of our real-time DNS exﬁltration and tunneling detector.
3.12 Filtering queries for a speciﬁc primary domain name.
3.13 Filtering queries with a minimum anomaly score.

. . . . 54
. . . . 62
. . 63
. . . . . . . . . 64
. . . . . . . . . . . 64

x

4.1 Aggregate load: (a) packet rate, and (b) bit rate, during peak hour

(2pm-3pm) of a weekday (31-May-2019).

. . . . . . . . . . . . . . . . 75

4.2 A weekly trace of DNS queries count: total versus DGA-based queries
(from 25-Nov-2019 to 1-Dec-2019) – blue circles highlight representa-
tive points to illustrate that count of DGA-enabled queries is at least
three orders of magnitude less than count of total DNS queries.
4.3 Time-trace of DGA-based DNS queries across 75 days (between 16-
Sep-2019 and 1-Dec-2019) – green squares represent no DGA queries,
red circles represent more than 15K DGA queries, and the black pen-
tagon highlights DGA queries daily count peaks at 30K.

. . . . . . . 77

. . . 76

4.4 Hourly histogram of: (a) total DNS queries, and (b) DGA-based DNS
queries, across 75 days (between 16-Sep-2019 and 1-Dec-2019).
4.5 Time-trace of daily DNS queries count for various DGA-enabled mal-

. . . 79

ware families: (a) ModPack, (b) Suppobox, and (c) Ramnit.

. . . . . 79

4.6 CCDF of TTL value in DGA-related DNS responses across represen-

tative malware families.

. . . . . . . . . . . . . . . . . . . . . . . . . 80

4.7 Communication pattern of a suspicious host with its C&C server of
Ramnit family: (a) sequence of HTTPS ﬂows, and (b) sequence of
packets in a selected HTTPS ﬂow. . . . . . . . . . . . . . . . . . . . . 84
4.8 Time trace of packets (outgoing/incoming) in a selected HTTP ﬂow. . 86
4.9 Clusters of K-means model for HTTP ﬂows.
. . . . . . . . . . . . . . 92
4.10 Attributes of misclassiﬁed ﬂows: (a,d) HTTP, (b, e) HTTPS, and

(c,f) UDP, across top performing classiﬁers.
4.11 System architecture of our detection system.
4.12 CCDF of number of suspicious ﬂows per host.
4.13 CCDF of malicious fraction of ﬂows per host in mix-malicious-benign

. . . . . . . . . . . . . . 95
. . . . . . . . . . . . . . 97
. . . . . . . . . . . . . 103

category. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

4.14 Time trace of active malicious ﬂows (between infected internal hosts
and malware servers) during the 50-day trial for: (a) aggregate of all
campus hosts, and (b) most active infected end-host (top) and a NAT
gateway (bottom).

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

4.15 Delay between DGA-based DNS responses and commencement of

their subsequent TCP/UDP ﬂow.

. . . . . . . . . . . . . . . . . . . . 106

4.16 CCDF of attributes: (a) packet count, and (b) duration, of suspicious

ﬂows. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107

4.17 Daily number of reactive ﬂow entries installed by the SDN controller

into the SDN switch, during the 50-day trial. . . . . . . . . . . . . . . 108

xi

5.1 Daily trace of incoming DNS response with errors (typo mistakes:

benign.) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

5.2 Daily trace of incoming DNS response with errors (Disposable do-

mains from Sophos antivirus: benign.)

. . . . . . . . . . . . . . . . . 115
5.3 Visual illustration of our data collection and various entities identiﬁed.116
5.4 Timetrace of incoming NX DNS responses. . . . . . . . . . . . . . . . 116
5.5 Time trace of an infected host in our dataset - a sudden rise in NXD

responses observed with in an hour only.

. . . . . . . . . . . . . . . . 117

5.6 Zoomed-in time trace of the suspicious host during the hour of interest

(8th January 2020 1:30 - 2:30pm). . . . . . . . . . . . . . . . . . . . . 117

5.7 CCDF of number of occurrences of unique NXDs per FQDN for the

suspicious host.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
5.8 Time trace of an infected host. . . . . . . . . . . . . . . . . . . . . . . 120
5.9 Time-trace of count of NXD responses for various infected hosts at
various time granularity (per hour, per min and per sec): (a)(d)(g)
Infected H1 (b)(e)(h) Infected H2, and (c) (f)(i) Infected H3. . . . . . 120
5.10 Overview of our proposed scheme. . . . . . . . . . . . . . . . . . . . . 122
5.11 Stack plot of number of incoming DNS responses in our campus net-

work over a day (26th Nov 2019).

. . . . . . . . . . . . . . . . . . . . 125

5.12 Stack plot of number of incoming DNS responses of a suspicious host

over a day (26th Nov 2019).

5.13 CCDF of number of occurrences of NXDs per host.

. . . . . . . . . . . . . . . . . . . . . . . 125
. . . . . . . . . . 126

xii

List of Tables

3.1 Summary of our dataset. . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.2 A sample list of malicious and normal DNS queries with unusual length. 38
3.3 Number

FQDN for

characters

domains

selected

in

of

in our dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.4 Entropy value for a sample list of query names.
. . . . . . . . . . . . 43
3.5 Labels pattern in query names for selected domains. . . . . . . . . . . 44
3.6 Summary of additional dataset (days 8-14) used for evaluation. . . . . 46
3.7 Detection accuracy of ground-truth instances after tuning.
. . . . . . 49
. . . . . . . . . . . . . . . 50
3.8 Anomaly detection for Research institute.
. . . . . . . . . . . . . . . 50
3.9 Anomaly detection for University campus.
. . . . . . . . . . . 50
3.10 Performance of our machine for trusted domains.
3.11 Anomaly detection combined with whitelisting for Research institute.
52
3.12 Anomaly detection combined with whitelisting for University campus. 52
3.13 Avg. anomaly score for research institute. . . . . . . . . . . . . . . . . 52
3.14 Avg. anomaly score for University campus. . . . . . . . . . . . . . . . 53
3.15 Avg. time complexity of our scheme.
. . . . . . . . . . . . . . . . . . 53
3.16 Samples of malicious queries (DET) along with their attributes de-

. . . . . . . 56
tected/undetected by the model of the research institute.
3.17 Anomaly score of queries publicly reported as DNS Exﬁltration.
. . . 57
3.18 Detecting wild malicious DNS queries from two enterprises. . . . . . . 58

4.1 DGA-related domain families found in the campus network (from 16-

Sep-2019 to 1-Dec-2019).

. . . . . . . . . . . . . . . . . . . . . . . . 74

4.2 Top ten most

frequently used DGA domain names

found

in the campus network. . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.3 Enterprise hosts making DGA-enabled queries. . . . . . . . . . . . . . 82
4.4 DGA-enabled malware and infected campus hosts found by analysis

of one-hour PCAP of full campus traﬃc.

. . . . . . . . . . . . . . . . 83

4.5 Distribution (µ and σ) of attributes value for malicious ﬂows in CTU-

13 dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
. . 92

4.6 Distribution (µ and σ) of attributes for K-means HTTP clusters.
4.7 Accuracy of HTTP models in correctly detecting malicious and benign

ﬂows. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94

4.8 Accuracy of HTTPS models in correctly detecting malicious and be-

nign ﬂows. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94

4.9 Accuracy of UDP models in correctly detecting malicious and benign

ﬂows. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95

xiii

4.10 Distribution of malware families among suspicious ﬂows, selected and

mirrored by our SDN system.

. . . . . . . . . . . . . . . . . . . . . . 100

4.11 Results of testing suspicious ﬂows against their corresponding EiF

models.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

5.1 Sample of FQDNs used by suspicious host.
5.2 Anomaly detection by ﬁne-grained model.
5.3 Anomaly detection by coarse-grained model.

. . . . . . . . . . . . . . . 118
. . . . . . . . . . . . . . . 128
. . . . . . . . . . . . . . 128

xiv

Chapter 1

Introduction

Contents

2.1 DNS Infrastructure: Vulnerabilities and Challenges . . . . . . . . 11

2.1.1 DNS Hierarchy . . . . . . . . . . . . . . . . . . . . . . .

11

2.1.2

Steps for DNS Resolution in an Enterprise Network . . .

11

2.1.3 Vulnerabilities and Challenges of Existing DNS Architec-

ture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13

2.2 DNS-Based Attacks . . . . . . . . . . . . . . . . . . . . . . . . . 14

2.2.1 Non Volumetric Attacks

. . . . . . . . . . . . . . . . . .

14

2.2.2 Volumetric attacks

. . . . . . . . . . . . . . . . . . . . .

17

2.3 Detection Systems in Cyber-Security . . . . . . . . . . . . . . . . 18

2.3.1 Knowledge-Based Detection . . . . . . . . . . . . . . . .

18

2.3.2 Machine Learning-Based Detection: Anomaly Detection

19

2.3.3

Limitations of Existing Detection Mechanisms . . . . . .

19

2.4 Review on Current State-of-the-Art DNS security . . . . . . . . 20

2.4.1 Monitoring DNS Queries of Enterprise Hosts for DNS Ex-

ﬁltration . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

1

Chapter 1.

Introduction

2.4.2 Behavioral Analysis of DGA-Enabled Malware of Enter-

prise Hosts . . . . . . . . . . . . . . . . . . . . . . . . . .

24

2.4.3

Identifying Malicious Hosts by Analyzing DNS NXDs . .

27

2.5 Research Gaps in Prior Works

. . . . . . . . . . . . . . . . . . . 28

2.5.1 Novelty of Our Approach . . . . . . . . . . . . . . . . . .

29

2.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

Cyber attacks are becoming more frequent and sophisticated. As a result, enter-

prise networks constantly face the threat of valuable and sensitive data being stolen

by cyber-attackers. Sophisticated attackers are increasingly exploiting the DNS ser-

vice for malicious activities such as exﬁltrating data as well as maintaining tunneled

command and control communications for malware. This is because DNS traﬃc is

usually allowed to pass through enterprise ﬁrewalls without deep inspection or state

maintenance, thereby providing a covert channel for attackers to encode low volumes

of data without fear of detection. Similarly, attackers are also exploiting DNS for

service disruption such as to launch Distributed Denial-of-Service (DDoS) attacks

on authoritative DNS servers and/or open resolvers - The victim is bombarded with

random DNS to utilize their available resources. A famous example of this attack is

Mirai attack on Dyn DNS architecture (in 2016). Unfortunately, Network operators

lack the methods and tools to determine whether outgoing/incoming DNS traﬃc of

an enterprise network is legitimate or cyber-breached.

1.1 Problem Statement

DNS is an essential protocol for the Internet that is used to resolve the domain

name like www.domain.com to its corresponding Internet Protocol (IP)address, e.g.

10.0.0.2. A host needs to ask its local recursive DNS resolver to resolve a domain.

The DNS data is commonly collected at the recursive DNS resolvers. The DNS

2

Chapter 1.

Introduction

protocol restricts the length of the domain name (for outbound queries) to 255

bytes containing letters, digits, and hyphens. Also, since the DNS protocol is used

mostly over the User Datagram Protocol (UDP), there is no guarantee that queries

will be replied based on their order of arrival.

1.1.1 Data theft using Domain Name Queries

One way for the attacker to exploit DNS is to register a domain (e.g., foo.com)

so that the attacker’s malware in a host victim can then encode valuable private

information (such as credit card numbers, login passwords or intellectual property)

into a DNS request of the form arbitrary-string.foo.com. This DNS request gets

forwarded by resolvers in the global domain name system to the authoritative server

for the foo.com domain (under the attacker’s control), which in turn sends a response

to the host victim. This provides the attacker with a low-rate but covert two-way

communication channel between a host victim and their C&C center.

Interestingly, enterprise ﬁrewalls are typically conﬁgured to allow all packets on

port 53 (used by DNS) since DNS is such a crucial service for virtually all ap-

plications. Some ﬁrewalls do oﬀer enhanced DNS protection. Still, they require

deep packet inspection of DNS messages to identify the covert channel and then

isolate domains that contain encoded data. The signiﬁcant resources needed for

this capability [1] and the resulting impact on ﬁrewall forwarding performance usu-

ally results in enterprise network operators disabling such features. This ability to

transit ﬁrewalls gives attackers a covert channel, albeit a low-rate one, by which to

exﬁltrate private data and to maintain communication with malware by tunneling

other protocols (e.g., Secure Shell Protocol (SSH), File Transfer Protocol (FTP) to

command-and-control centers. As one example, the remote access trojan DNS Mes-

senger [2] discovered in 2017 used DNS queries and responses to execute malicious

powerShell commands on compromised hosts.

3

Chapter 1.

Introduction

1.1.2 Malware Command and Control Communication using

DGAs

Malware-infected machines, forming a botnet, are typically managed remotely by

an adversary (aka botmaster) via a C&C channel. Cyber-criminals primarily use a

botnet for malicious activities such as stealing sensitive information, disseminating

spam, or launching denial-of-service attacks. Therefore, law enforcement agencies

routinely perform takedown operations on the blacklisted C&C servers [3], disrupting

their botnet activities.

In response to these eﬀorts, botmasters have developed

innovative approaches to protect their infrastructure. The use of DGAs is one of the

most eﬀective techniques that has gained increasing popularity [4].

Domain Generation Algorithm (DGA) make use of a “seed” (a random number

that is accessible to both the botmaster and the malware agent on infected hosts)

to generate a large number of custom domain names. Developing numerous time-

dependent domain names and registering only the relevant one(s) “just shortly”

before an attack allows a botnet to shift their C&C domains on the ﬂy and remain

invisible for longer [5]. The botmaster waits for the malware to successfully resolve

a Domain Name System (DNS) query for the registered domain, enabling the C&C

communications to take place. Note that even if a C&C server is taken oﬄine or

blacklisted, this process can simply be restarted and a new server can come online.

To date, more than 80 collections of DGA domains (each corresponding to a malware

family) have been recorded by DGArchive [6] and are publicly available.

1.1.3 Service Disruption using Random Domain Names

glsdns works in such a way that if a query is being asked from the DNS authoritative

name server or open resolver, it is an obligation on them to answer it even if the

query is non-existent on their ecosystem. Non-existent domains are of two types: (a)

4

Chapter 1.

Introduction

Popular search engines and Anti-viruses utilize random domains to convey the one-

time signal to their servers known as disposable domains (eg,“elb.amazonaws.com.cn”,

“cloudfront.net”, and ‘avts.mcafee.com”) - Benign domains also contain typographical

mistakes for example a user accidentally write “googel.com” instead of “google.com”;

(b) Malicious NXDs to launch a type of DDoS attack, DNS water torture attack [7]

also known as random subdomain attack by dynamically generating random strings

as the preﬁx of a victim domain. The DNS Water Torture Attack is a type of

DDoS attack on DNS servers. This attack aﬀects both authoritative servers and

open/recursive resolvers, but mainly it targets the former.

Cyber actors use bots (compromised devices) to send many randomly generated

domain names on their victim servers. The queried domain names relate to the

primary domain governed by its authoritative name server to tell the IP address of

that particular domain. Due to the high number of requests during the attack, the

victim authoritative servers and/or the recursive resolvers may have slow responses

to the queries being asked or potentially become unavailable. Although the problem

has been well understood over the last decade, it is mostly dealt with from the

perspective of the victim server by identifying the malicious queries. Therefore, we

see this as an opportunity to detect potentially infected hosts of an enterprise that

initiate non-existent queries to the outside world.

This thesis is structured into four parts to address the above-pointed issues. First,

we review the current literature based on DNS security. We discuss the available

solutions and identify the challenges and limitations in the state-of-the-art. We then

develop tools and models to detect DNS-based attacks in enterprise networks. After

a thorough analysis of real DNS traﬃc of our campus network, we extract numerous

meaningful attributes that can distinguish malicious from legitimate queries. We

highlight the prevalence and activity pattern of more than twenty families of DGA-

enabled malware across internal hosts. We develop a monitoring system that uses

Software Deﬁned Networking (SDN) reactive rules to automatically and selectively

5

Chapter 1.

Introduction

mirror Transmission Control Protocol (TCP)/UDP ﬂows pertinent to DGA queries

(between internal hosts and malware servers) for diagnosis by the trained models.

Finally, we draw insights into the use of DNS for service disruption.

1.2 Research Contributions

In the context DNS network security, the following can be considered as signiﬁcant

contributions made by this research:

1) Our ﬁrst contribution tackles data exﬁltration using DNS. We analyze out-

going DNS queries to identify many stateless attributes such as the number

of characters, the number of labels, and the entropy of the domain name to

distinguish malicious data exﬁltration queries from legitimate ones. We also

develop, tune, and train a machine-learning algorithm to detect anomalies in

DNS queries using a benign dataset of top-rank primary domains. To achieve

this, we have used 14 days-worth of DNS traﬃc from each organization. We

then implement our scheme on live 10 Gbps traﬃc streams from the network

borders of the two organizations, inject more than three million malicious DNS

queries generated by two exﬁltration tools, and show that our solution can

identify them with high accuracy. We compare our solution with the two-class

classiﬁer used in prior work. (Chapter 3)

2) Our second contribution tackles malware C&C communication using DNS.

We analyze DNS outgoing queries to identify more than twenty families of

DGA-enabled malware when communicating with their C&C servers. We draw

insights into the behavioral proﬁle of DGA-enabled malware ﬂows when com-

municating with C&C servers by analyzing a Packet Capture (PCAP) trace

(3.2B packets) collected during the peak hour from our campus network. We

identify malware traﬃc attributes and train three specialized one-class classi-

6

Chapter 1.

Introduction

ﬁer models using behavioral attributes of malware HTTP, HTTPS, and UDP

ﬂows obtained from a public dataset. We develop a monitoring system that

uses SDN reactive rules to automatically and selectively mirror TCP/UDP

ﬂows pertinent to DGA queries (between internal hosts and malware servers)

for diagnosis by the trained models. (Chapter 4)

3) Our third contribution studies the use of DNS for service disruption. We anal-

yse incoming DNS messages, with a speciﬁc focus on Non-eXistent-Domains

(NXD) DNS responses, to distinguish benign from malicious NXDs. We high-

light two attack scenarios based on their requested domain names. Using NXD

behavioral attributes of internal hosts, we develop multi-staged iForest classi-

ﬁcation models to detect internal hosts that are launching service disruption

attacks. We show how our models can detect infected hosts which generate

high-volume as well as low-volume distributed NXD-based attacks on pub-

lic resolvers and/or authoritative name servers with an accuracy of over 99%

incorrectly classifying legitimate hosts. (Chapter 5)

For each of the schemes developed above, we demonstrate the feasibility and

eﬃcacy of our methods through the implementation in our campus network

via simulation and practical results. We believe that our research will play an

important role in securing the internal hosts of any enterprise network using

its DNS traﬃc.

1.3 Thesis Organization

The rest of this thesis is organized as follows. First, chapter 2 surveys the landscape

of DNS based network security and highlights the shortcomings in the current state-

of-the-art, and discusses that there is still a need for thorough research based on

DNS security in enterprise networks. In Chapter 3, we propose real-time detection of

data exﬁltration in enterprise networks. Chapter 4 gives the extensive data analysis

7

Chapter 1.

Introduction

of our campus network to draw insights into the prevalence of DGA domains. It

also discusses how to identify the infected hosts that are communicating with C&C

servers by selectively mirrored their traﬃc using SDN. In Chapter 5, we propose

an architecture using multi-staged iForest classiﬁcation models to detect internal

hosts that are launching service disruption attacks. Finally, we conclude the thesis

in Chapter 6 with pointers to directions for future work.

8

Chapter 2

Security of Enterprise DNS

Infrastructure

Contents

3.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

3.2 DNS Queries of Enterprise Hosts: Data Collection and At-

tributes Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . 33

3.2.1 Our Dataset . . . . . . . . . . . . . . . . . . . . . . . . .

34

3.2.2 Query Name Attributes Engineering . . . . . . . . . . .

37

3.3 Detection of Anomalous Queries . . . . . . . . . . . . . . . . . . 46

3.3.1 Machine Training . . . . . . . . . . . . . . . . . . . . . .

46

3.3.2 Algorithms and Tuning Parameters . . . . . . . . . . . .

47

3.4 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . 50

3.4.1

Performance Metrics

. . . . . . . . . . . . . . . . . . . .

51

3.4.2

Evaluating Models using Known DNS Exﬁltration Data .

54

3.4.3 Comparing Multi-Class Classiﬁer with One-Class Classiﬁer 57

3.4.4 Malicious DNS Queries from Enterprise Networks . . . .

59

3.5 Real-Time Deployment in Campus Network . . . . . . . . . . . . 63

9

Chapter 2. Security of DNS Infrastructure

Figure 2.1: Key concepts covered in this chapter

3.5.1 Visualizing Detection of Malicious DNS Queries . . . . .

64

3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

With the exponential growth of networking devices in enterprise networks, cyber-

attacks are becoming increasingly sophisticated and intense. DNS is an essential

protocol used by every networking device for address resolution. Thus, attackers

have used DNS to perform malicious activities such as data theft, C&C communi-

cation, and launch DDoS attacks covertly. According to Forescout Research Labs

[8], new DNS vulnerabilities have the potential to impact millions of devices. This

chapter will give a thorough background of DNS infrastructure, vulnerabilities, and

the existing detection mechanisms. Furthermore, we will identify the existing gaps

in the current literature and why our research is essential to address some of the

existing gaps.

10

Chapter 2. Security of DNS Infrastructure

2.1 DNS Infrastructure: Vulnerabilities and Chal-

lenges

Before going into DNS security details, we ﬁrst discuss the domain name space and

steps involved in DNS address resolution. We then discuss the vulnerabilities and

challenges face by the DNS infrastructure. Fig. 2.1 gives an overview of key topics

covered in this chapter.

2.1.1 DNS Hierarchy

The basic function of DNS is to translate a human-readable domain name into an

IP address and vice versa. To achieve this, DNS uses a distributed hierarchical

domain server system as shown in Fig 2.2. It consists of a tree-based structure with

a maximum tree depth of 128. A tree is divided into zones. On the top, we have a

root domain containing all top-level domains (TLDs) or zones. The top-level domain

is further divided into domain names such as google or yahoo, which we labeled as

the second-level domain. Each domain is divided into subdomains, also known as

the third-level domain, such as docs or scholar. Finally, we have hostnames such as

ftp or www as highlighted in Fig 2.2.

2.1.2 Steps for DNS Resolution in an Enterprise Network

This section gives all the necessary steps for DNS resolution in an enterprise network.

Large enterprises usually focus on the security of their DNS infrastructure. Therefore

they use recursive resolvers and their authoritative DNS servers. Fig. 2.3 illustrates

a big picture of how a client obtains the IP address for a web server to get connected

with the server within an enterprise network. The ﬁrst step is that the client sends

a DNS request of www.scholar.google.com to the recursive resolver of the enterprise.

11

Chapter 2. Security of DNS Infrastructure

Figure 2.2: Domain Name Space.

The recursive resolver has a local cache memory where it saves the responses with IP

addresses for later use. It checks the current DNS request in its cache and forwards

the request to the root server if the domain name is not appeared in its cache (step 2).

The root servers have the resource details of all the top-level domain servers, such as

in this case, it will provide the IP address of the .com server (step 3). The recursive

resolver then contacts the com server for IP resolution of the second-level domain

(SLD) i.e., google (step 4). The TLD server (.com in this scenario) sends back

the IP address of the name server of google (step 5). The recursive resolver then

requests the IP address of www.scholar.google.com from the Google name server

(step 6). The google.com server responds to the query of recursive resolver with

the IP address of https://www.scholar.google.com (step 7). Finally, the recursive

resolver provides the IP address to the client as a response (step 8), and the client

gets connected to the www.scholar.google.com server.

12

Chapter 2. Security of DNS Infrastructure

Figure 2.3: DNS Architecture.

2.1.3 Vulnerabilities and Challenges of Existing DNS Archi-

tecture

Some enterprises do not restrict their clients and let them conﬁgure their own devices,

such as conﬁgure the IP address of their local DNS resolver and change it to any

other publicly available open DNS resolver. Similarly, there is an increase in using

personal devices due to the advancement in the Internet of Things (IoT) i.e., the

concept of Bring Your Own Device (BYOD) that is making a security loophole as

far as DNS security infrastructure is concerned. The attackers take advantage by

using DNS as a vehicle to perform malicious activities covertly.

DNS servers and open resolvers can cache most domain names and their IP

addresses and respond to anyone who may ask for the particular domain name.

However, DNS caches can be easily spoofed and manipulated by the attackers with

the help of Man-in-the-Middle (MitM) attacks such as DNS hijacking and DNS cache

poisoning attacks (explained brieﬂy in the next section).

To resolve some of the aforementioned issues, Internet Engineering Task Force

(IETF) introduced the domain name system security extensions (DNSSEC) in 2005

13

Internal DNSServerRecursive ResolverRoot ServerStep 1TLD ServerAuthoritativeServerOrganization’s IntranetInternetStep 2Step 3Step 4Step 5Step 6Step 7Step 8Chapter 2. Security of DNS Infrastructure

[9] to enhance the data authentication and data integrity of existing DNS architec-

ture using public-key cryptography (digital signing) for authentication. Similarly,

DNS over TLS (DoT) [10] and DNS over HTTPS (DoH) [11] have been introduced

by IETF in 2016 and 2018 respectively to encrypt the traditional DNS commu-

nication by using encrypted protocols [12]. However, this can incur an increased

overhead (4 times more bytes in any DNS query or response) as compared to the

traditional unencrypted DNS traﬃc [13]. Moreover, the signing and verifying oper-

ations in DNSSEC take valuable CPU time and memory. Due to these drawbacks,

the adoption rate of these encrypted DNS technologies is fairly low in enterprise

networks.

2.2 DNS-Based Attacks

The world of the Internet and networking is exposed to an overwhelming number of

DNS-based cyber-attacks and threats. In Fig. 2.4, we have classiﬁed DNS attacks

into two main categories; i.e., non-volumetric attacks and volumetric attacks. We

will discuss the anatomy of these attacks below.

2.2.1 Non Volumetric Attacks

This section lists the non-volumetric attacks on DNS protocol that can aﬀect any

enterprise network with ﬁnancial and reputational damages.

Domain Hijacking: Domain hijacking is a well-known type of DNS-based MitM

attack in which queries are incorrectly resolved by the attackers by redirecting them

to malicious websites. Attackers become successful in this attack type by: (a) in-

stalling malware on the victim’s PC or routers and changing the DNS server infor-

mation to redirect the DNS requests attacker’s control sites; (b) eavesdropping on

the local network and sending the spoofed response to the legitimate DNS request

14

Chapter 2. Security of DNS Infrastructure

Figure 2.4: DNS Attack Categories.

coming from the victim. In this scenario, when the legitimate response arrives, it is

ignored by the local DNS server.

Another way to hijack a domain is to collect personal information about the

actual owner of the domain to impersonate him and convince the domain registrar

to update the information or transfer the domain to another registrar they control.

DNS Cache Poisoning Attack: DNS cache poisoning attack aka DNS spoof-

ing is another example of MitM attack that follows a similar concept as that of DNS

hijacking i.e., to redirect the DNS requests to attackers controlled sites or DNS

servers. The anatomy of cache poisoning attack is to store incorrect and fake infor-

mation against the legitimate domain names to trap innocent users into malicious

activities. As a result, the internet traﬃc goes to the wrong places until the cached

data is ﬁxed.

DNS Exﬁltration Attack: The anatomy of the DNS exﬁltration attack is

shown in Fig.2.5. We see two hosts, one is benign, and the other is compromised with

malware (such as backdoor). The malware then encodes the personal information

(such as credit card data) in the subdomain of an attacker’s controlled primary

domain (e.g., maliciousDomain.com). The request then goes all the way from the

local resolver of the organization to the attacker’s domain. The attacker then decodes

the encoded sensitive information and uses it for criminal activities.

15

DNS AttacksDomain Hijacking DNS Exfiltration/ TunnelingPhantom Domain AttackRandom Subdomain AttackDNS Reflection and Amplification AttackAttack CategoriesAttack TypesNon Volumetric AttacksVolumetric AttacksDNS Domain Generation Algorithms (DGAs)DNS Cache Poisoning AttackChapter 2. Security of DNS Infrastructure

Figure 2.5: DNS Exﬁltration Attack - Basics.

DNS Domain Generation Algorithms (DGAs): DGA is a technique used

by attackers to generate new domain names (based on a predeﬁned algorithm) and

IP addresses for malware’s C&C servers. It uses a “seed” (a random number that

is accessible to both the botmaster and the malware agent on infected hosts) to

generate a large number of custom domain names. Generating numerous time-

dependent domain names and registering only the relevant one(s) “just shortly”

before an attack allows a botnet to shift their C&C domains on the ﬂy and remain

invisible for longer [5]. Some common examples of DGA-based malware include

“Gameover”, “Suppobox”, “Ramnit” and “Virut”.

The botmaster waits for the malware to successfully resolve a Domain Name

System (DNS) query for the registered domain, enabling the C&C communications

to take place. Note that even if a C&C server is taken oﬄine or blacklisted, this

process can simply be restarted, and a new server can come online. To date, more

than 80 collections of DGA domains (each corresponding to a malware family) have

been recorded by DGArchive [6] and are publicly available.

16

Chapter 2. Security of DNS Infrastructure

2.2.2 Volumetric attacks

In DNS data ﬂooding attacks, a common example is the Denial of Service (DoS)

attack that is a simple and powerful method to consume the available resources of

a victim having the intentions to deny its services to legitimate users [14]. Most

of the attacks on the Internet are performed by some of the hidden software. This

software is commonly known as bots [15] [16] [17].

Phantom Domain Attack: Phantom domain attack is another interesting yet

powerful attack that covertly uses DNS to target the victim DNS resolvers [18]. The

anatomy of the phantom attack is that it uses phantom domains that are being

controlled by the attackers, which provide a slow response or no response at all.

The bots are sending thousands of requests to the victim DNS resolvers to respond

to these phantom domains. The DNS server consumes resources while waiting for

responses, eventually exhausting all the available compute and memory resources.

Random Subdomain Attack: Random subdomain attack aka DNS Water

Torture attack or Slow Drip attack is a type of DDoS attack on authoritative DNS

servers and/or open resolvers in which the victim is bombarded with random non-

existent domain names (NXD) from attackers controlled machines such as bots.

It requests subdomains or hosts that do not exist, which consumes memory and

processing resources- eventually leading to degraded performance or failure. Mirai

botnet is an example of this attack on Dyn DNS architecture in 2016 [7, 19].

DNS Reﬂection and Ampliﬁcation Attack: DNS reﬂection and ampliﬁca-

tion attack is a major type of volumetric attack which makes use of the open DNS

resolvers over the Internet to take part in the attack against a speciﬁc victim [20].

The idea is to spoof the identity of the actual attacker so that the responses can go

to the spoofed IP address (real victim) and increase the magnitude of the attack by

asking for more than one type of DNS response which leads to increased response

packet size. Malicious actors send thousands of spoofed DNS queries to the open

17

Chapter 2. Security of DNS Infrastructure

DNS resolvers, which respond with the large-sized responses directed to the victim’s

IP address. The victim can be a stand-alone PC or an authoritative name server

which can be overwhelmed by the unsolicited responses - can cause slow performance

or outage depending on the available resources of the victim.

2.3 Detection Systems in Cyber-Security

Intrusion detection systems are the computer tools that are used to enhance the

security of computer networks. IDS plays an essential role in identifying the network

traﬃc and classifying it as benign or malicious. In addition, they can identify the

intrusions from an external source as well as the internal intrusions. Detection

systems are broadly classiﬁed into two main categories: Knowledge-based/Misuse-

based detection and Anomaly-based detection.

2.3.1 Knowledge-Based Detection

The working principle of Knowledge-based (Misuse-based) detection is that it only

classiﬁes the attacks or the malicious content, and the traﬃc pattern is previously

known. It is also known as signature-based detection because they depend on the

signatures of the attacks provided by the expert in network security. Snort [21] and

Bro [22] are the two open-source signature-based IDS that are widely used for this

speciﬁc purpose. The main advantage of using this type of intrusion detection is that

it detects the known attacks without overwhelming the system with false-positive

alarms [23]. However, to detect the latest attacks on the network, these systems

require a continuous feed of updates in systems. Due to the increase in categories

of attacks, it is challenging to update signatures of all attacks and vulnerabilities in

the network. Another disadvantage of this approach is that it burns many resources

in terms of updating the systems periodically. Thus, there is a deﬁnite need for

18

Chapter 2. Security of DNS Infrastructure

scalable intrusion detection systems to perform better for high-speed networks.

2.3.2 Machine Learning-Based Detection: Anomaly Detec-

tion

Big data analytics has become a hot topic in recent times in the ﬁeld of industry

and academia. Data has been increasing exponentially with time. Big data comes

into practice where the data sets are extensive and are diﬃcult to examine by tradi-

tional data processing methods [24]. To analyze the big data, we use the tools and

algorithms of Machine Learning (ML).

The anomaly-based detection approach somehow bridges the gap of misuse-based

detection by capturing the attacks that are not seen before and without using an

attack signature. The essential criteria to ﬁnd the anomaly is that the system is fed

with normal traﬃc. Now, the system is aware of the behavior of normal traﬃc. So,

it raises the alarm whenever it observes any traﬃc that diverges from this normal

behavior. The main drawback of this technique is the high number of false alarms in

some anomaly-based NIDS. The framework in [25] was designed to investigate the

botnets in the network [14]. The authors in [25] presented a novel system to detect

these bots in the overall network with the help of machine learning techniques by

looking at the high-level information about the network traﬃc. The framework is

based on a learning-based approach that automatically generates the bot detection

models.

2.3.3 Limitations of Existing Detection Mechanisms

ML security applications range from identifying malicious activity within a network

to predicting the attack at any period by observing the anomalies in the network

traﬃc. The bulk of the research falls on one of two sides, oﬄine analysis on a labeled

19

Chapter 2. Security of DNS Infrastructure

dataset with multiple ML algorithms [26] [27] or real-time analysis on lab-grown

data with a single ML algorithm [28]. The trade-oﬀ in this spectrum is between

computation and timeliness. The former approach emphasizes mass data extraction

(Deep Packet Inspection) but is not feasible due to the global trend towards end-to-

end encryption [29]. Attempts at using purely ﬂow-level information have yielded

mixed results depending on how the data was captured. [30].

Decision tree classiﬁers are not suitable for large volumes of benign traﬃc. Most

of the research works conducted in this area use disproportional datasets [31]. A

typical dataset is the KDD99 which has an attack-to-traﬃc ratio of 4:1, which is

acknowledged as a signiﬁcant limitation by the research community [32]. The advan-

tage of using neural network methods is that they are not limited to known attacks

or signatures [33]. These trained models can classify unknown attacks with credible

success [34]. The two major disadvantages of this model are the need for large vol-

umes of labeled data and time, and the predictions are computationally expensive

relative to other models.

2.4 Review on Current State-of-the-Art DNS secu-

rity

This section will focus on the current state-of-the-art DNS security, speciﬁcally

command-and-control (C&C) communication, data theft, and service disruption

across a wide range of assets.

20

Chapter 2. Security of DNS Infrastructure

2.4.1 Monitoring DNS Queries of Enterprise Hosts for DNS

Exﬁltration

Malware’s Perspective: From a security standpoint, the DNS protocol is an

excellent covert channel. According to IDC 2021 Global DNS threat report, DNS-

based malware is ranked second among commonly used attack vectors following

phishing attacks [35]. Due to its crucial internet role, misconﬁguration of the DNS

can lead to network disconnects. Therefore, security policies rarely restrict it (e.g.,

allowing resolutions only to speciﬁc domain names). In addition, the DNS protocol

is often less monitored than other Internet protocols (e.g., HTTP, FTP, and mail

transfer protocols). It follows that the use of the DNS protocol as a covert channel

has been a part of previous cyber campaigns, including a 56M credit cards theft from

Home Depot in 2014 [36], and a 25k credit cards theft from Sally Beauty [37]. During

the Covid-19 pandemic, ﬁnancial ﬁrms have been aﬀected the most by DNS-based

attacks with an average of over $1 Million [35].

In an enterprise network, the users within the network can still make the DNS

queries out, which can cause the data exﬁltration over the DNS. For example, Fire-

Eye has discovered the new malware named Multigrain that has stolen the credit

card data from the point of sale systems and send that information by exﬁltrating

data over the DNS [38]. BernhardPOS, NewposThings, and FrameworkPOS are

some other examples.

Feederbot [39] is one from the malware family which has used DNS for sending

its command and control messages to the botmaster [40]. Morto worm [41] has

been used by the attackers for sending the malware commands that were using the

DNS protocol via sending the DNS queries with the Query type of TXT. Wekby

pisloader [42]is also from the bad actors that are using their DNS server to send

and receive the command and control commands to and from the compromised

hosts. The information is encoded in the DNS requests. A remote access trojan

21

Chapter 2. Security of DNS Infrastructure

named DNSMessenger [43] was discovered in 2017, which uses DNS queries and

responses to execute the malicious PowerShell commands on compromised hosts. It

is a sophisticated attack as it does not require any ﬁle writing to the compromised

hosts. The technique uses DNS TXT records to extract the PowerShell commands

saved as DNS TXT records remotely.

Malicious Domains’ Perspective: DNS traﬃc has been analyzed to identify

malicious network activities [44, 45]. Studies in [46, 47] survey the available research

literature on the misuse of DNS protocol for various attacks. Common malicious

activities that utilize DNS include command and control (C&C) traﬃc tunneled over

DNS channel, circulating spam messages, transferring credit card numbers (or other

sensitive information), and hosting scams and phishing websites [4, 48]. Therefore,

it is important to proﬁle and detect these malicious activities. Over the last decade,

there has been an increasing amount of works [49–53] on identifying these malicious

activities mostly related to C&C communications [40, 54] and phishing [55].

Machine Learning Based Detection of DNS Exﬁltration and Tunneling:

Researchers have used three categories of methods for detecting DNS exﬁltration

and tunneling, namely statistical-based techniques [56–60], supervised multi-class

classiﬁcation [61–64], and unsupervised one-class classiﬁcation [65, 66].

Work in [56] proposed a method to ﬁnd maximum information that can be en-

coded in a sub-domain portion of a DNS query name to detect whether the query

contains encoded data or not. The authors used an information-theoretic approach,

namely the use of Kolmogorov complexity. The authors established an upper bound

on the volume of surreptitious communication by investigating inter-query time and

query record type. In [57], authors employed mutual information and principal com-

ponent analysis for dimensionality reduction based on consecutive DNS request and

response sizes. In [58–60], authors have proposed DNS tunnel detection using char-

acter frequency analysis. However, the detection criteria are based on the threshold

22

Chapter 2. Security of DNS Infrastructure

value for which attackers can go undetected easily.

In [61, 62] the authors employed a supervised learning-based model with logistic

regression to classify queries as either normal or exﬁltration. Buczak et al. [63] used

the Random Forest algorithm for the two-class classiﬁcation of benign and malicious

DNS queries. Similarly, Samuel et al.

[64] proposed a model to detect malicious

DNS query names (generated by malware-infected machines) using Random Forest.

However, attributes used in prior works to train the model are stateful (such as

tracking the inter-arrival time of DNS packets or the frequency of query type) or

require both DNS query and response messages (such as response length) [61, 63].

Also, this body of work essentially trains a model with both benign and malicious

instances (i.e., a two-class classiﬁer) and the accuracy of detecting malicious queries

dropped when a new family of attack is introduced (e.g., model accuracy varied

from 27% to 75% depending upon model parameters in [63]).

Deep Learning Based Detection of DNS Exﬁltration

Over the past two years, the interest of researchers on DNS exﬁltration has shifted

from ML-based detection systems to deep learning-based detection systems [67–

70]. Work in [70] employs a combination of Convolutional Neural Network (CNN)

and Long Short-Term Memory (LSTM). For LSTM, authors use one hidden layer

and they assumed that the ﬁrst 128 bytes/characters contain the actual message

information; therefore, they set the length of the hidden layer to 128. Similarly,

for CNN, the authors used three convolutional layers, two max-pooling layers, and

one softmax layer. Their model is trained on two classes of FQDN, i.e., benign and

malicious. Our model instead is trained on attributes of benign FQDNs, resulting in

a one-class classiﬁer. The key diﬀerence is its way of detecting anomalies and ability

to detect zero-day attacks (i.e., it ﬂags all anomalous domain names that deviate

from the attributes of the benign domain names). Although deep learning-based

approaches give a high percentage of accuracy, they require a huge amount of data

(typically more than a million instances) for improved performance. Furthermore,

23

Chapter 2. Security of DNS Infrastructure

Deep Learning-based methods demand an increased computing resources and add

more complexity to operational networks [71, 72].

2.4.2 Behavioral Analysis of DGA-Enabled Malware of En-

terprise Hosts

Malware behavioral analysis has been widely studied by many researchers [4, 25, 48,

73–76] using diﬀerent tools and techniques [77]. The most relevant works to ours can

be divided into three categories: (a) detection of malicious traﬃc based on unusual

DNS queries (predicting the presence of DGA domains) [4, 48, 64, 76, 78–84], (b)

network behavioral analysis of known malware and botnet by inspecting their traﬃc

data and/or metadata [25, 74, 75, 85–87], (c) use of SDN and/or programmable

networking in detecting cyber-attacks [88–91].

Malicious DNS Queries: DNS traﬃc has been analyzed to identify malicious

network activities [45, 48, 92]. Over the past decade, there has been an increasing

number of works [49–52] on detecting malicious network activities mostly related to

DNS exﬁltration, DNS tunneling, and C&C communications [40, 44, 54, 93, 94].

In the past, blacklists were used to detect C&C communications between servers

and infected hosts. However, blacklisting has been defeated by attackers since they

migrated from a static domain mapping to the use of algorithmically generated

domain names. In response to this change, researchers have attempted to automati-

cally detect DGA domains using statistical modeling of DNS traﬃc [4, 48, 64, 76, 78,

79], or machine/deep learning techniques [80–84, 95]. Antonakakis et al. [4] develop

a clustering-based method for detecting new (unknown) algorithmically generated

domains (AGD) as well as classifying known AGDs using supervised learning. The

authors evaluated their proposed solution in a large ISP network and found sev-

eral new families (unseen before) of DGAs, operating on the network. They used

statistical attributes including entropy and n-grams measures as well as structural

24

Chapter 2. Security of DNS Infrastructure

attributes such as length and label count in the domain name, extracted from NXD

(non-existent domains) responses on a per-host basis. Schuppen et al. [64] employ

manually-engineered features to train a binary classiﬁer using Random Forest algo-

rithm to determine whether domains in NXDomain-failed DNS queries (i.e., queries

to non-existent domain names) are benign or malicious. It remains unclear how this

model performs in classifying unseen malicious domains. We note that generating

a rich training dataset of benign NXDs is nontrivial, and a labeled benign dataset

may get polluted by some new (but unknown) malicious NXDs.

Malware and Botnet Behavioral Analysis: Flow-based analysis has been

used to detect malware and botnet traﬃc. However, monitoring high-rate network

traﬃc in large and complex enterprise environments is diﬃcult and computationally

expensive. Works in [25, 85–87, 96] develop ﬂow-based features like packet count and

distribution of packet length by analyzing every packet of the network to model the

patterns in encrypted traﬃc. Similarly, Anderson et al. [74, 75] employed supervised

learning algorithms to classify malware and benign traﬃc. Their model was trained

by a variety of host-level attributes, including packet size, ﬂows inter-arrival, DNS

query (e.g., TLD, TTL, domain rank in Alexa), HTTP data (such as server code,

content type, Accept-language, and location), and TLS data (such as TLS cipher

suites and TLS extension) that are measured over a while.

Our behavioral modeling approach diﬀers from prior works in three ways: (a) we

only monitor the behavior of selected ﬂows pertinent to certain servers (resolved by

DGA responses) instead of monitoring all traﬃc of every host on the network, (b)

we choose to extract statistical attributes of ﬂows (encrypted and unencrypted) to

train our models, without the need for inspecting payloads like HTTP content type

or TLS handshake cipher keys, and (c) our models are built by one-class classiﬁcation

algorithms and hence become sensitive to changes in any attribute while multi-class

models become sensitive to changes in only discriminative attributes. Also, it is

essential to note that we only use a database of known DGA domains and check

25

Chapter 2. Security of DNS Infrastructure

domain queries against this database in real-time instead of classifying a domain as

benign or malicious. Our objective is to detect infected hosts of enterprise networks

by monitoring their selected ﬂows.

Programmable Networking for Network Security:

Software-Deﬁned Networking (Programmable Networks) is an evolving paradigm

to manage the networks in a better way. SDN is a technology that decoupled the

control plane with a data plane (i.e. switches and routers). Now the network designs

are not vendor-centric that was the case in traditional networks. Then, the network

operators had to conﬁgure every device with low-level commands individually.

Programmable networking has recently gained popularity among researchers,

speciﬁcally for network security use-cases [88–91]. Gupta et al.

[88] develop scal-

able telemetry (i.e., partitioning diﬀerent types of traﬃc such as TCP and ICMP)

that can be used to collect and analyze the network traﬃc in real-time using a pro-

grammable data-plane. Furthermore, the authors show that their approach reduces

the workload of the overall system to be able to operate at a line rate. Similarly,

Zhang et al. [91] proposed an approach that uses P4 programmable switches for a

better defense mechanism against DDoS attacks. The authors considered the case of

volumetric DDoS attacks. They provided the defense strategies in a modular fashion

that can be adopted for each network and can be used for new defense strategies other

than just the DDoS. Although these prior works primarily leverage the programma-

bility features in the data plane, we instead employ the programmable control-plane

available by Openﬂow-based SDN to dynamically select suspicious ﬂows for diagnosis

by trained machine learning models.

In a relevant work, [97], Ceron et al. developed an automated system for oﬄine

malware analysis, recording the network behavior of a given malware in a controlled

sandbox environment orchestrated by an SDN controller. A known host on their

sandbox is infected by malware (from a set), and the SDN controller inspects ev-

26

Chapter 2. Security of DNS Infrastructure

ery packet from/to this infected host for taking required actions like rate-limiting,

blocking, or re-conﬁguring the topology, upon ﬁnding certain patterns in the packet

payload (i.e., regex signature) or headers (e.g., contacting speciﬁc IP address and/or

TCP/UDP port numbers).

2.4.3

Identifying Malicious Hosts by Analyzing DNS NXDs

Identifying Malicious Queries: To resolve the NXD attacks, speciﬁcally water

torture attacks, researchers come up with signiﬁcant countermeasures such as rate

limiting and IP address blocking. However, it can severely aﬀect the legitimate

users since the malicious queries can forward through the open resolvers or ISP

cache servers. Therefore, the IP address of the cache server will be blocked, and

the legitimate users will not have access to the ISP cache server. Similarly, if the

volume of queries exceeds the limit set in rate-limiting, it will block all queries, even

from legitimate users. Researchers [98, 99] have also examined domain names to

detect the malicious queries by focusing on NXD error responses only. Kazato et al.

[98] predicted whether a domain name included random words via a score calculated

by comparing bigrams of domain names of malicious domains and those of benign

domain names.

Identifying Attack on DNS Servers: A group of researchers [99, 100] have

identiﬁed NXD attacks on DNS servers by setting the threshold on the number of

non-existent domains. The approaches can be fruitful for heavy volume attacks

(such as bursty data) - moreover, choosing a threshold value would be challenging.

However, this approach does not work eﬃciently for the lightweight and distributed

NXDs, bypassing the threshold-based security systems.

27

Chapter 2. Security of DNS Infrastructure

2.5 Research Gaps in Prior Works

A considerable number of previous research investigations have focused on the detec-

tion of malicious domains [49–52]. The techniques used are mostly based on passive

DNS analysis. Leyla et al.

[50] came up with the EXPOSURE system to detect

the suspicious domains by extracting 15 features from the DNS traﬃc. The features

are divided into four main categories i.e, DNS query-name based, time-based, an-

swer based and TTL based. The authors have claimed that their system has been

validated in a real-world dataset containing 100 billion DNS requests. The system

captures the misused suspicious domains taking part in botnet command and con-

trol and spamming. Manos et al. [51] proposed a similar method by analyzing the

passive DNS queries. They gave NOTOS as a name to their approach, a dynamic

reputation system for the DNS domain names. They have extracted 41 features

grouped into two main categories i.e, network-based features and zone-based fea-

tures. The main idea is that malicious domains have unique characteristics that

can easily be separated from legitimate domains based on their extracted features.

Such approaches, however, have failed to address the information theft over the DNS

queries, and it does not detect the malicious domains in real-time.

Manos et al.

[48] proposed another system called Kopis, which is operating at

the upper DNS hierarchy i.e, the authoritative name servers and Top-level domain

servers, in contrast to NOTOS and EXPOSURE, which were operated at the local

recursive DNS servers. This approach has enhanced the visibility of the DNS mes-

sages. The same process is followed by [4] to capture the DNS traﬃc. However, the

aim is to identify the DGA (Domain Generation Algorithms) bots by focusing only

on the DNS queries with name error responses i.e, NXDomain responses. Although

extensive research has been carried out on malware detection over DNS protocol at

the upper DNS hierarchy, the features considered may not be that eﬀective at the

network level.

28

Chapter 2. Security of DNS Infrastructure

We believe that a two-class classiﬁcation approach (i.e., signature-based) is insuf-

ﬁcient to address new and increasing types of attacks. Also, obtaining “ground truth”

on a diverse set of malicious instances to train the classiﬁer is diﬃcult [101]. The

authors of [65] employed unsupervised machine learning algorithms (i.e., one-class

support vector machine and k-means) to detect DNS tunneling. Their primary focus

was to identify infected mobile devices using stateful attributes, including the time

between a DNS query and its corresponding response and the size of individual de-

vices’ DNS query/response. In [102], Homem et al. benchmarked the performance of

four algorithms (multi-class decision trees, support vector machine, K-nearest neigh-

bors, and neural networks) in identifying tunneled traﬃc (e.g., HTTP, HTTPS, and

FTP) over DNS. The authors used only three attributes of DNS packets, including

the size of IP packet, length of query name, and entropy of query name. Similar

to our approach, Nadler et al.

[66] proposed an anomaly-based solution to detect

low throughput data exﬁltration over DNS. This work evaluated the performance

of isolation forest and support vector machine learning algorithms. However, the

authors maintain several attributes for each primary domain over the last n hours

(e.g., rate of A and AAAA records, the average length of query name). This makes

it diﬃcult to detect malicious queries in real-time.

2.5.1 Novelty of Our Approach

To the best of our knowledge, our work in chapter 3 is the ﬁrst that presents

a thorough analysis of attributes for query names from operational enterprise net-

works. Our focus is on attributes of fully qualiﬁed domain names that can be

extracted in “real-time”, without a need for states (i.e., “stateless”) – we assume that

DNS traﬃc is not encrypted over TLS. We also provide fascinating insights into the

practical considerations of such a detection scheme. Our scheme can be extended by

collecting states only for those hosts that generate anomalous queries and ultimately

mitigate malicious DNS tunneling/exﬁltration – such mitigation is beyond the scope

29

Chapter 2. Security of DNS Infrastructure

of this research.

Our work in chapter 4 develops an automatic SDN-based system for detecting

malware-infected hosts in real-time by relying more on the behavioral activity proﬁle

of “selected ﬂows” rather than the content of packets. Further, our SDN switch does

not send any network packets to the controller ( allowing the solution to scale to

high rates). Instead, packets that need to be inspected in the software are sent as

copies on a separate interface of the switch, to which a software inspection engine is

attached.

In chapter 5, we have developed a monitoring system at the source side that can

detect the internal hosts generating high volume NXD attacks as well as distributed

NXD attacks by using multi-level machine learning algorithms. Existing research

focuses on detecting heavy volume NXD attacks at the reception side i.e., the au-

thoritative name server or the queried domain (primary domain). Instead, our work

is to detect the enterprise hosts generating attack traﬃc to other networks to protect

our campus network’s reputation.

2.6 Conclusion

Domain Name System (DNS) is an essential application layer protocol used by every

Internet-connected device to obtain the IP address of web servers. However, due to

its prevalence and importance, DNS traﬃc is rarely inspected by intrusion detection

systems, making this protocol vulnerable to various cyber-attacks. In this chapter,

we have studied and characterized DNS-based attacks. We have also discussed

the detection mechanisms and the underlying challenges and gaps in the existing

research. We investigate some of these critical research problems in the rest of this

thesis, beginning with data theft in enterprise networks.

30

Chapter 3

Monitoring Enterprise DNS Queries

for Detecting Data Exﬁltration from

Internal Hosts

Contents

4.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

4.2 Analyzing Network Traﬃc Data . . . . . . . . . . . . . . . . . . 71

4.2.1 Our Datasets

. . . . . . . . . . . . . . . . . . . . . . . .

72

4.2.2 DGA-Fueled Malware Families . . . . . . . . . . . . . . .

74

4.2.3 Daily Activity Pattern of DGA-Based Domains

. . . . .

77

4.2.4

Infected Enterprise Hosts . . . . . . . . . . . . . . . . . .

81

4.2.5 Network Behavior of DGA-Fueled Malware . . . . . . . .

82

4.3 Modeling and Mirroring Traﬃc of Suspicious Malware Servers

. 87

4.3.1 Modeling Traﬃc Behavior of Malware . . . . . . . . . . .

88

4.3.2 Dynamic Traﬃc Selection using SDN . . . . . . . . . . .

96

4.4 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . 100

4.4.1

SDN-selected DGA Flows

. . . . . . . . . . . . . . . . . 100

31

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

4.4.2 Diagnosing DGA Flows . . . . . . . . . . . . . . . . . . . 101

4.4.3

Infected Hosts Initiating Malicious DGA Flows

. . . . . 103

4.4.4 Comparing Our Diagnosis Models with Zeek IDS . . . . 108

4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

In the previous chapter, we highlighted the importance of DNS security in en-

terprise networks. In this chapter, we analyze DNS data to draw insights into the

characteristics of exﬁltrated domains and then develop a machine learning-based

method to detect the exﬁltrated domains in real-time in two enterprise networks.

Parts of this chapter have been published in [44, 93, 94].

3.1 Introduction

Cyber-criminals have exploited DNS to maintain covert communication channels

with compromised hosts. The resulting damages can be huge, amounting to several

million dollars in a single attack [103]. Based on a recent DNS security survey

of Infoblox [104], 46 percent of the businesses of North America and Europe have

exploited by DNS exﬁltration, and about 45 percent aﬀected by DNS tunneling.

Several high-proﬁle data exﬁltration breaches have been reported since 2014: the

Sally Beauty breach (a theft of 25K credit cards) [37] and FrameworkPOS malware

(a theft of 56M credit cards from Home Depot) [36] in 2014, BernhardPOS malware

[105] in 2015, MULTIGRAIN malware [38] in 2016, Win32.Backdoor.Denis [106]

in 2017, and UDPoS Malware [107] in 2018. In addition, there have been several

DNS tunneling incidents in which malware actors used their DNS servers to send

and receive the command and control commands to and from compromised hosts.

Examples include Feederbot [39], and botmaster [40], Morto worm [41], and Wekby

pisloader [42].

This chapter develops and validates a mechanism for real-time detection of DNS

32

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

exﬁltration and tunneling in two operational networks – a large University and a

mid-sized Government Research Institute. Our ﬁrst contribution is to collect and

conduct a thorough analysis of real DNS traﬃc from the two organizations over sev-

eral days and extract stateless attributes of DNS messages, such as length, entropy,

dots, numerics, uppercase characters, and the number of labels, that can distinguish

malicious from legitimate queries. Our second contribution is to develop, tune, and

train a machine-learning algorithm to detect anomalous DNS queries based on the

above attributes using a known dataset of benign domains as ground truth based on

14 days worth of DNS data from the two organizations. For our third contribution,

we implement our scheme on live 10 Gbps traﬃc streams from the network borders

of the two organizations, inject more than three million malicious DNS queries gen-

erated using two exﬁltration tools (our customized tool and an open-source tool)

and show that our scheme can identify such malicious activity with high accuracy.

We also show our one-class classiﬁer outperforms an existing two-class classiﬁer in

detecting unknown DNS exﬁltration attacks. We draw insights into anomalous DNS

queries detected by our models, looking into their anomaly scores, tracking query

counts in real-time, the number of enterprise hosts querying them, and investigat-

ing the TTL/Type ﬁelds of their corresponding responses. We make our tools and

datasets available to the public to facilitate further Research into this area.

3.2 DNS Queries of Enterprise Hosts: Data Collec-

tion and Attributes Extraction

In this section, we ﬁrst analyze the characteristics of DNS traﬃc (with a speciﬁc focus

on query names) collected from the border of two enterprise networks, a medium-

sized research institute and a large University campus. In both instances, the IT

department of the enterprise provisioned a full mirror (both inbound and outbound)

of their Internet traﬃc (each on a 10 Gbps interface) to our data collection system

33

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

from their border routers (outside of the ﬁrewall). We obtained appropriate ethics

clearances for this study (UNSW Human Research Ethics Advisory Panel approval

number HC17499, and CSIRO Data61 Ethics approval number 115/17). We ex-

tracted DNS packets from each enterprise Internet traﬃc stream in real-time by

conﬁguring rules to match incoming/outgoing IPv4 and IPv6 UDP packets on port

53 in an OpenFlow switch. The study here considers data collected over one week

from 30-07-2018 to 05-08-2018.

3.2.1 Our Dataset

Table 3.1 shows a summary of our dataset from each organization. We captured

a total of 249M and 589M DNS packets from the border of the two networks and

stored them in daily CSV ﬁles – each row in our dataset represents a timestamped

DNS packet including headers and payload. The data shows that 17% of total DNS

traﬃc is carried over IPv6 packets in both networks. Also, more than a third of

our records correspond to outgoing DNS queries generated by enterprise hosts – i.e.,

86.9M and 221M in the Research and University networks, respectively. We note

that our dataset also contains queries for unqualiﬁed domain names (i.e., 900K and

1.5M respectively in the Research and University networks) that are discarded in our

analysis – we use the cleaned dataset. Unqualiﬁed query names contain no delimiting

dots (e.g., “top 10 banks offering attractive home”) or their top-level-domain is pure

numeric (e.g., “129.178”). After removing unqualiﬁed names, outgoing DNS queries

in total span respectively 2.2M and 6.2M distinct fully qualiﬁed domain names

(FQDN).

These FQDNs are rooted themselves in 397K and 1.1M distinct primary domains

(i.e., one level under “com” or “co.uk”). Fig. 3.1 shows the number of queries for each

unique primary domain over the entire dataset, ordered from most queried on the

left, to least queried on the right. There is a small number of domains on the left

34

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.1: Summary of our dataset.

Research University

Total DNS packets

IPv4 DNS packets

IPv6 DNS packets

DNS queries

DNS responses

Total Outgoing DNS queries

Outgoing DNS queries (IPv4)

Outgoing DNS queries (IPv6)

Outgoing DNS queries (only qualiﬁed)

Unique query names (FQDN)

Unique primary domains

249M

206M

43M

142M

107M

86.9M

69.7M

17.2M

86M

2.2M

397K

589M

489M

100M

341M

248M

221M

177M

44M

219.5M

6.2M

1.1M

that predominate with very high query counts, followed by a long-tail of domains, all

of which receive a fairly small number of queries (i.e., less than 1000 over a week).

It is seen that the top 4K (out of 397K) and 9K (out of 1.1M) domains respectively

in the research institute and the University comprise the head in their respective

curve. For example, only three domains namely “akamaiedge.net”, “in-addr.arpa”, and

“akadns.net” contribute to 15% of total queries generated by University hosts. In

the research network, on the other hand, top three domains of “kaspersky-labs.com”,

“kas-labs.com”, and “in-addr.arpa” contribute to 17% of total queries. We note that

queries for “in-addr.arpa” correspond to reverse DNS lookups which are commonly

used by email servers to check and see if the message came from a valid server. Many

email servers will reject messages from any server that does not support reverse

lookups since spammers typically use invalid IP addresses.

In terms of queries “reputation”, we used Majestic dataset [108] which is free and

updates on a daily basis – Majestic is a reverse search engine that computes the

number and strength of links to a domain (it is a measure of trust instead of traﬃc

estimates)[109, 110]. To get a sense of reputation and probability of typical ranks,

35

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Figure 3.1: Number of queries per unique primary domain, over a week (Rsch: 397K,
Univ: 1.1M).

we show in Fig. 3.2 the complementary cumulative distribution function (CCDF) of

the reputation rank for primary domains queried in both organizations. We can see

that 44% of total queries, in both organizations, are not listed in the top 1M domains

of Majestic domains ranking (i.e., CSV dataset released on 7-Aug-2018). Also, only

32% and 34% of queries in each network are among the top 10K most popular

domains.

In our Majestic dataset, “google.com", “facebook.com, and “youtube.com"

are top three ranked domains respectively.

Considering the load of DNS queries generated by enterprise hosts, shown in

Fig. 3.3, we see that the number of packet-per-sec in the research network varies

between 50 to 400 depending on the day of week and peak/oﬀ-peak hours. For the

University network, on the other hand, a larger variation is observed – i.e., 150 to

more than 800 pps.

36

0200K400K600K800K1Morder of primary domains100101102103104105106107108number of queries<in-addr.arpa>:  Rsch(1, 8.3M); Univ(2, 7.3M)<kaspersky-labs.com>:  Rsch(2, 2.6M); Univ(260, 70K)<akamaiedge.net>:  Univ(1, 11.8M); Rsch(4, 2.5M)<google.com>:  Univ(3, 7M); Rsch(16, 456K)Rsch.Univ.Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

(a) Research institute.

(b) University campus.

Figure 3.2: CCDF of reputation rank: (a) Research institute, and (b) University
campus.

Figure 3.3: Real-time number of queries.

3.2.2 Query Name Attributes Engineering

We now look at the attributes of the query name (FQDN) in each DNS query gen-

erated by enterprise hosts that are relevant to diﬀerentiating benign and malicious

DNS queries traﬃc. Our aim is to use only “Stateless” attributes which can be de-

rived from individual DNS query packets, independent of time-series characteristics

of queried domains or hosts DNS activity – there is no overhead in computing these

37

1200K400K600K800K1MX: reputaion rank of primary domains0.40.50.60.70.80.91.0CCDF: P( rank > x )(10K, 0.72)(1M, 0.49)1200K400K600K800K1MX: reputaion rank of primary domains0.40.50.60.70.80.91.0CCDF: P( rank > x )(10K, 0.66)(1M, 0.46)TueWedThuFriSatSunMonTime (days)0100200300400500600700800Num. outgoing DNS queries (pps)Rsch.Univ.Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.2: A sample list of malicious and normal DNS queries with unusual length.

Query name (FQDN)

6e517f3.grp10.ping.adm.cdd2e9cde9fee9cdc8.cdd0e8e9c8fce9d2e9fecdc4.c597f097ce87c5d3.ns.a23-33-37-54-deploy-akamaitechnologies.com

Security

Malicious

708001701462b7fae70d0a28432920436f70797269676874.20313938352d32303031204d696372.6f736f667420436f72702e0d0a0d0a0.433a5c54454d503e.cspg.pw Malicious

PzMnPiosOD4nOCwuOzomPS4nNjovPS8uOzsnNCstODkjOCwoMwAA.29a.de

bwzm133h9gb3pp9s6l3mu7r73sh.arm2513pu79r9.1z19e1bgm1hwu8z6u2.9rzlkhbvi45gaag52t3rqtqd2t.p2gliv6gklwzvvlt2jp1z6li7v.avqs.mcafee.com

Malicious

Normal

0.19.6ce.71c.444.25.41.0.0.0.4.27.0.0.0.0.0.0.0.0.0.9efc95e03d7f3a4ae446ecd0d049e5ae9e016ee33703c9cb3506cad4bbd98bc.b.f.00.s.sophosxl.net Normal

p4-ces3lawazdkbw-qlrq5qalxdt7tycq-385202-i1-v6exp3.ds.metric.gstatic.com

ldap. tcp.AWS. sites.dc. msdcs.AD.us-east-1.ec2-utilities.amazonaws.com

Normal

Normal

attributes in real-time. Our attributes are inspired by various prior works (referred

against each attribute).

According to RFC 1035 [111], the total length of a domain name (dots included)

is restricted to 255 characters, and domain names are represented as a sequence of

“labels” separated by dots. The maximum length of a label is 63 characters. It has

been shown that DNS can be used for malicious purposes in the form of DNS tun-

neling or exﬁltration in which valuable information (e.g., credentials, credit card, or

control messages) is embedded in the sub-domain portion of a query name. Malware

applications typically embed stolen data [112] into the subdomain part of a DNS

query for a domain where the name server is under control of an attacker. A DNS

query for “exfiltrated-data.example.com” would be forwarded to the name server of

“example.com”, which would record “exﬁltrated-data” and decode and decrypt the

sensitive information from that subdomain ﬁeld.

Table 3.2 lists samples of malicious [2, 105, 113] and benign query names with

“unusual” length and string pattern. For example, the top two malicious query

names in this list respectively contain 129 and 136 characters. We note that the

sub-domain portion of these query names comprises random-looking strings with

a signiﬁcant number of upper-case and numerical characters, and is fairly long.

For example, the second malicious query name from the top (i.e., for “cspg.pw")

contains 38 numeric characters (i.e., 28%), and the third malicious query name

(i.e., for “29a.de") contains 38 numeric characters (i.e., 28%) contains 23 uppercase

letters (i.e., 39%). Given these observations, we deﬁne our attributes by three main

38

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

(a) Research institute.

(b) University campus.

Figure 3.4: CCDF of number of characters in query name for: (a) Research institute,
and (b) University campus.

(a) Research institute.

(b) University campus.

Figure 3.5: Scatter density map of numerical fraction of characters vs. total length
of query name for: (a) Research institute, and (b) University campus.

categories namely characters count, entropy (an indication of randomness) of string,

and length of discrete labels in the query name.

3.2.2.1 Count of Characters

The total number of characters is an important attribute since more characters imply

that the query name probably carries embedded information for an outside host. In

Fig. 3.4, we plot the distribution of character count for query names in our dataset

39

050100150200250300X: num. chars in query name10-810-710-610-510-410-310-210-1100CCDF: P( num. > x )totalnumericalupper-case050100150200250300X: num. chars in query name10-810-710-610-510-410-310-210-1100CCDF: P( num. > x )totalnumericalupper-case50100150200250total chars0.00.10.20.30.40.50.60.7numerical frac. of chars015000030000045000060000075000090000010500001200000number of points per pixel50100150200250total chars0.00.10.20.30.40.50.60.70.8numerical frac. of chars025000050000075000010000001250000150000017500002000000number of points per pixelChapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

(a) Research institute.

(b) University campus.

Figure 3.6: Scatter density map of upper-case fraction of characters vs. total length
of query name for: (a) Research institute, and (b) University campus.

to understand the typical value of these attributes.

Total count of characters in FQDN:[102] We can see that more than 99% of

host queries in both organizations contain less than 80 characters, as shown by black

cross markers in Fig. 3.4. Only a very small fraction of query names (i.e., about

0.3%) are really long, each with more than 100 characters. It is important to note

that antivirus tools tend to exchange legitimate data (i.e., for signature lookup) over

DNS [66]. For example, in Table 3.2 the ﬁrst two “normal” query names correspond

to “McAfee” and “Sophos” antivirus.

Interestingly, primary domains “mcafee.com"

with 1.9M queries (average query length of 84 characters), and “sophosxl.net" with

145K queries (average query length of 106 characters) are among top ten frequent

domains seen in our dataset from the Research institute and the University network

respectively as shown in Table 3.3. Since the exﬁltrated (or Command & Control)

message is carried by the sub-domain portion of an FQDN, we use the count of

characters in sub-domain [61] as our second attribute.

Additionally, we use the count of uppercase characters[62] and count of

numerical characters[62] in a query name to determine if it is benign or malicious.

This is because the fraction of uppercase and numerical characters becomes high in

40

50100150200250total chars0.00.20.40.60.8upper-case frac. of chars025000050000075000010000001250000150000017500002000000number of points per pixel50100150200250total chars0.00.20.40.60.8upper-case frac. of chars0400000800000120000016000002000000240000028000003200000number of points per pixelChapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.3: Number of characters in FQDN for selected domains
in our dataset.

primary domain # FQDN # unique FQDN frac. Numerical (%)

frac. Uppercase (%) avg. Length

mcafee.com

sophosxl.net

spotify.com

cnr.io

e5.sk

1.9M

145K

84K

121K

66K

571K

41K

819

113K

131

39.4

47.5

7

19.97

13.8

0.31e-3

0.11

0

70.08

0.15

84.01

106.7

41.7

209.8

129.06

encrypted/ encoded data [62] – however, not all encrypted data is malicious.

In

Fig. 3.4, it is seen that only about 1% of all queries in each organization contain

more than 30 numerical characters. Unsurprisingly, the upper-case character is very

rare in domain names generated by hosts in both enterprise networks – at least 98%

of queries contain no upper-case character, and less than 0.2% of queries contain

more than 10 capitals.

To better understand the distribution of various characters in query names, we

plot the scatter density maps of total characters count versus numerical fraction in

Fig. 3.5, and total characters count versus uppercase fraction in Fig. 3.6 – dark red

areas depict higher density of points and dark blue areas highlight the lower density

of points. In Fig. 3.5, it can be seen that the numerical fraction of a query name

typically stays below 20% (mostly less than 10%) when the query name has less

than 60 letters (i.e., dark red area on the left bottom of plots). Interestingly, for the

Research Institute shown in Fig. 3.5a, we observe a crowded region around 40% of

numerical letters when the FQDN length is between 60 to 80 characters. In Fig. 3.6,

we see that the fraction of uppercase letters is below 10% for short query names

(i.e., less than 80 characters for the Research Institute and less than 50 characters

for the University), and it tends to zero when query names get longer.

3.2.2.2 Entropy

Random (“not-readable”) sub-domains are common in DNS exﬁltration/tunneling

queries due to use of encryption and/or encoding [66]. Entropy [102] is a measure to

41

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

determine the degree of non-readability (or strength of encryption) and uncertainty

in a string. We use Shannon entropy [114] which takes a discrete random variable

X as input (i.e., DNS query name in our case), and mathematically is given by:

H(X) = −

N
(cid:88)

k=1

P (xk) log2 P (xk)

(3.1)

where P (xk) is the probability of the k-th symbol (i.e., lower-case/upper-case

letter, numerical, dot, or hyphen) in the input string X containing various characters

where N is the total number of unique characters. We note that only speciﬁc letters

can be used in a valid DNS query name [111] (i.e., 52 alphabetic and ten numeric

characters, a hyphen, and dot, thus N = 64). This means that the entropy value of

a query name will take a value between 0 and log2(64) = 6 [115]. Table 3.4 shows
the entropy value for a sample list of query names, both benign and malicious. For

example, the entropy of a simple query name such as “www.google.com" equals to

2.84, and it gets a higher value for a more random string such as the last entry in

Table 3.4, a query for “googleapis.com" whose entropy value is 5.27. We also observe

that the entropy value of malicious queries (highlighted in bold text) varies and is

not necessarily higher than that of benign queries. In Fig. 3.7, we plot the CCDF

of entropy for all FQDNs queried by hosts of the two organizations during a week.

It can be seen that the entropy value for more than 90% of query names is less than

4 in both networks, and having an entropy greater than 5 is less likely (i.e., lower

than 0.1%).

3.2.2.3 Labels

This category comprises two attributes of labels inside a FQDN. For example, in the

query name “www.scholar.google.com", there are four labels separated by dots. We

use the number of labels [63] as our sixth attribute. This is because DNS exﬁltra-

tion/tunneling traﬃc tends to use certain patterns of labels in their query names.

42

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.4: Entropy value for a sample list of query names.

Query name (FQDN)

www.google.com

202.135.201.205.23000000000012.sb-adfe2ko9.senderbase.org

708001701462b7fae70d0a28432920436f70797269676874.20313938352d32303031204d696372.6f736f667420436f72702e0d0a0d0a0.433a5c54454d503e.cspg.pw

0.19.6ce.71c.444.25.41.0.0.0.4.27.0.0.0.0.0.0.0.0.0.9efc95e03d7f3a4ae446ecd0d049e5ae9e016ee33703c9cb3506cad4bbd98bc.b.f.00.s.sophosxl.net

6e517f3.grp10.ping.adm.cdd2e9cde9fee9cdc8.cdd0e8e9c8fce9d2e9fecdc4.c597f097ce87c5d3.ns.a23-33-37-54-deploy-akamaitechnologies.com

PzMnPiosOD4nOCwuOzomPS4nNjovPS8uOzsnNCstODkjOCwoMwAA.29a.de

f4a55fc3f30keaayaayqivpqaggkbqggudp6hm-yacnusej1525121392-sonar.xy.fbcdn.net

DIYNBPRYA0K5CVUWA.ns1.logitech-usa.com

0ca7d.1.288.WYB52Q2ZPIU2SEUTDDDGEJDQFAO6F2C53AVC6IVAZZLR2PJHEWQWRFG6Z2NPQ3J.CQ4888.1d19d9c4.cnr.io

X2AR6GEQVHCSMXKFUNVIZU67PVMD5EF3N74E4TLOEOYK47WEXKMQ.hash.rocketeer.ct.googleapis.com

Entropy

2.84

3.75

3.92

3.98

4.50

4.59

4.78

4.86

5.10

5.27

(a) Research institute.

(b) University campus.

Figure 3.7: CCDF of entropy of query name for: (a) Research institute, and (b)
University campus.

Table 3.5 shows the label patterns for ﬁve selected domains in our dataset from

the Research institute network. We abstract a label pattern by an array (samples

are shown in the second column) whose elements indicate the length (i.e., char-

acter count) of the corresponding label in the query name – e.g., the pattern for

“www.scholar.google.com" is represented by (3,7,6,3). We see that queries for each of

the primary domains, listed in Table 3.5, appear in various number of patterns –

the primary domain is obtained by combining the top level domain (TLD) and the

second level domain (2LD) (e.g., in “www.scholar.google.com", the primary domain

is “google.com"). For example, the domain “sophosxl.net" is queried by 2208 distinct

43

0123456X: entropy of query name10-810-710-610-510-410-310-210-1100CCDF: P( entropy > x )0123456X: entropy of query name10-810-710-610-510-410-310-210-1100CCDF: P( entropy > x )Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.5: Labels pattern in query names for selected domains.

primary domain

sample patterns

# unique patterns avg # queries / pattern

sophosxl.net

mcafee.com

spotify.com

cnr.io

e5.sk

(1, 63, 63, 36, 16, 1, 2, 1, 8, 3)
(1, 63, 63, 18, 40, 1, 2, 1, 8, 3)

(3, 11, 7, 4, 4, 4, 3, 1, 26, 4, 6, 3)
(3, 11, 1, 4, 4, 4, 3, 1, 26, 4, 6, 3)

(48, 48, 48, 48, 16, 2, 7, 3)
(23, 2, 7, 3)

(5, 1, 3, 63, 63, 63, 30, 8, 3, 2)
(5, 1, 3, 63, 63, 63, 8, 8, 3, 2)

(63, 63, 63, 24, 1, 1, 2, 2)
(63, 63, 18, 1, 1, 2, 2)

2208

316

43

46

10

66

6K

1.9K

2.6K

660

label patterns during one-week period of our dataset, and each pattern is seen in

66 queries on average. For “e5.sk" domain, on the other hand, we observe only 10

unique patterns, each repeats more than 600 times.

Another interesting observation is that queries for three domains namely

“sophosxl.net", “cnr.io", and “e5.sk" have several labels with 63 characters (i.e., the

max limit according to RFC), whereas queries for “spotify.com" and “mcafee.com"

do not use label length greater than 48 and 26 characters respectively. For our last

two attributes, we use maximum label length [63] and average label length

[63] in a query name. Fig. 3.8 depicts the CCDF of the longest, and the average

label length for FQDNs observed in the two organizations. It is seen that for 90%

of queries, their longest label does not exceed 20 characters, and their average label

length is ten characters (or less) in both networks. On the other hand, only about

1% of queries have the longest label of more than 40 characters in the Research and

the University networks, respectively.

Summary: Our main achievement in this section is to identify and capture eight

attributes (from the query name section of each outgoing DNS request packet) that

collectively have strong predictive power in determining whether the query name

is normal or malicious. The attributes include: (1) Total count of characters in

FQDN, (2) count of characters in sub-domain, (3) count of uppercase characters,

(4) count of numerical characters, (5) entropy, (6) number of labels, (7) maximum

label length, and (8) average label length.

44

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

(a) Research institute.

(b) University campus.

Figure 3.8: CCDF of length of labels in query name for: (a) Research institute, and
(b) University campus.

45

010203040506070X: length of labels in query name10-810-710-610-510-410-310-210-1100CCDF: P( length > x )longestaverage010203040506070X: length of labels in query name10-810-710-610-510-410-310-210-1100CCDF: P( length > x )longestaverageChapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.6: Summary of additional dataset (days 8-14) used for evaluation.

Research University

Total Outgoing DNS queries

Outgoing DNS queries (IPv4)

Outgoing DNS queries (IPv6)

Outgoing DNS queries (only qualiﬁed)

Unique query names (FQDN)

Unique primary domains

79.6M

62.6M

17.0M

78.8M

2.1M

382K

228M

182M

46M

217M

6.1M

1.15M

3.3 Detection of Anomalous Queries

We now develop a machine learning technique to determine if a DNS query of an

enterprise host is normal or not (i.e., “anomaly detection”). By training a model with

only normal query names we aim to detect new/unknown malicious attacks (i.e.,

anomalous queries) which can be missed by the two-class classiﬁer. The machine

is invoked with the eight attributes of each DNS query explained in the previous

section. To validate the eﬃcacy of our models, in this section, we extend our dataset

by including additional records (78.8 M and 217 M qualiﬁed DNS queries from the

Research Institute and the University campus networks, respectively) collected over

the one week of 6-Aug-2018 to 12-Aug-2018 - a summary of this additional dataset

(i.e., days 8-14) is shown in Table 3.6. In total, we analyze 14 days worth of DNS

queries from the two enterprises.

3.3.1 Machine Training

We train our anomaly detection machine with benign data from four days of our

dataset – we keep the remaining ten days worth of data for testing. Ground truth

of benign domains in the literature is primarily drawn from highly ranked popular

domains [47]. For example, Alexa top-ranked domains are commonly used – Alexa

no longer publishes free top one million sites. However, we note that Alexa ranking

46

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

is based on the browsing behavior of Internet users (i.e., estimate of global traﬃc

to a domain). As a result, some malicious domains may appear among top K Alexa

domains due to a burst of requests from a high number of infected clients querying

them [116]. We, therefore, use an alternative, Majestic Million [108] that releases

a free dataset of top 1M domains and updates it daily. Majestic ranks sites by the

number of subnets linking to that site – it is a measure of trust instead of traﬃc

estimates [109, 110]. For the benign training instances, we only use the top 10,000

primary domains in the Majestic list. We also include FQDNs for “sophosxl.net”

domain which is not among the top 10K Majestic dataset – the Majestic dataset is

used as a reference of domain reputation to determine whether a queried domain is

benign or not.

3.3.2 Algorithms and Tuning Parameters

The objective is to maximize the detection of anomalous queries while reducing the

rate of false alarms (i.e., incorrectly detecting a normal query as anomalous or vice

versa). Many supervised machine-learning algorithms for detecting anomalies such

as one-class SVM and Replicator Neural Network suﬀer from high false alarms. They

are optimized for proﬁling the inlier behavior rather than detecting anomalies. We

employ “Isolation Forest (iForest)” [117] which is an eﬀective algorithm in detecting

anomalous instances in high-dimensional datasets with minimal memory and time

complexities.

The iForest algorithm [117] works based on the concept of isolation without

employing any distance or density measure. This algorithm aims to isolate test

instances by randomly selecting a feature and then randomly selecting a split value

from a range (within min and max obtained from training) values of the selected

feature. Then, the score is calculated as the number of conditions (path length) to

check for isolating a test instance. Note that isolating normal instances require more

47

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

conditions. The process is repeated several times to avoid issues due to randomness,

and the average path length is calculated and normalized.

Algorithm Tuning: We used scikit-learn and its APIs, an open-source

machine-learning package written in Python, to train and test our machine. We

have used three tuning parameters for iForest during the training phase, namely

the number of trees (n_estimators), height limit of trees (max_samples), and con-

tamination rate. We tune the value of each parameter while ﬁxing the other two

parameters and validate the accuracy of our machine for both benign and malicious

instances (that we have the ground truth)in both organizations. The default value

for the number of trees is 100, the height limit of trees is set to “auto” (implying 8

given the size of our dataset), and the contamination rate is 10%.

To tune the algorithm, we require ground truth for both benign and malicious

instances. Our ground-truth for benign instances are chosen based on the top 10K

domains of the Majestic list (§3.2.1)) – we have 1.7 M instances for the research

organization and 4.8 M for the university campus network). For the ground-truth

of malicious instances, we generated DNS exﬁltration queries with our open-source

tool, forked from an open-source project called “DNS Exﬁltration Toolkit” (DET)

[118]. We ran our tool on a machine inside the University network that exﬁltrates

the content of a CSV ﬁle containing 1000 samples of random credit card details

(obtained from [119]) to an authoritative name server under our control located

in the Research network. DET employs AES-256 encryption and uses two tuning

parameters: the max length of the query name (i.e., 30 to 218 characters) and the

max length of labels (i.e., 30 to 63 characters) to diversify our synthetic malicious

queries. We generated a total of 1.4M exﬁltration queries that are publicly available

at [120] in the form of a CSV ﬁle.

We found that setting the number of trees equal to 2 results in high accuracy

of more than 91% for benign and 63% for malicious instances – increasing this

parameter does not enhance the accuracy but increases the model size and prediction

48

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.7: Detection accuracy of ground-truth instances after tuning.

Research Institute
University Campus

Benign Malicious
98.44%
97.99%

95.07%
98.49%

time. Having ﬁxed the number of trees to 2 and the contamination rate to 10%,

we varied the height of trees from 1 to 20. The detection performance rises by

increasing the height limit of trees and gets stabilized at the value of 18 with the

best accuracy of more than 90% and 98% for ground-truth benign and malicious

instances, respectively. We then ﬁxed the number of trees to 2 and the height limit

of isolation trees to 18 to quantify the impact of contamination rate. Decreasing

the contamination rate from 10% to 2% improved the performance of our model for

both organizations as shown in Table 3.7, with the accuracy of more than 97% for

benign instances and more than 95% for malicious instances.

To summarize, we found the optimal value of tuning parameters equal to 2,

18, and 2% respectively for the number of trees, the height limit of trees, and

the contamination rate. Furthermore, for optimal tuning parameters, the iForest

algorithm sets the threshold value of anomaly score to 0.54, distinguishing normal

and anomalous instances.

Table 3.10 shows the performance of our machine (after tunning) for selected

benign instances – for cross-validation. It can be seen that the rate of false alarms

is mostly less than 5% in both organizations, though we see a higher false rate (i.e.,

more than 10%) for “in-addr.arpa" and “sophosxl.net" domains in the University

network. We found out that the attributes of some of FQDNs of primary domains

“in-addr.arpa" and “sophosxl.net" are similar to those of exﬁltration FQDNs (pos-

sibly benign DNS exﬁltration). In the next section, we will pre-ﬁlter instances for

these domains that are highly trusted (i.e., certainly benign) without passing them

to the anomaly detection machine.

49

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.8: Anomaly detection for Research institute.

Input

Output Days 1-4 Days 5-14

Benign domains (top 10K)

Others (beyond top 10K)

normal

anomalous

normal

anomalous

98.44%
1.56%
78.43%
21.57%

98.35%
1.65%
77.35%
22.65%

Table 3.9: Anomaly detection for University campus.

Input

Output Days 1-4 Days 5-14

Benign domains (top 10K)

Others (beyond top 10K)

normal

anomalous

normal

anomalous

97.99%
2.01%
70.57%
29.43%

97.83%
2.17%
63.38%
36.62%

Table 3.10: Performance of our machine for trusted domains.

primary domain

normal anomalous Avg. query length false-rate (%) normal anomalous Avg. query length false-rate (%)

Research institute

University campus

akadns.net

googleapis.com

gstatic.com

in-addr.arpa

mcafee.com

onmicrosoft.com

senderbase.org

sophosxl.net

spamhaus.org

spotify.com

2.6M

165K

207K

3.7M

1.9M

22K

1.1M

138K

12K

579

24K

1.6K

362

49K

735

1.6K

14K

6.5K

597

31

Top 100 domains

7.9M

135K

(e.g., google, apple)

38

76

69

26

84

51

66

103

31

45

20

0.91

0.96

0.17

1.32

0.03

6.55

1.32

4.44

4.7

5.08

1.68

7.6M

526K

835K

9.2M

635K

201K

2.2M

2.5M

947K

468K

191K

15K

986

1.1M

13K

1537

2816

394K

7.7K

1.2K

24M

351K

38

76

76

26

88

53

66

119

32

168

20

2.4

2.7

0.11

10.7

2.01

0.75

0.12

13.7

0.81

0.25

1.41

3.4 Performance Evaluation

In this section, we evaluate the eﬃcacy of our scheme by: (a) cross-validating and

testing the accuracy of the trained model for benign instances and quantifying the

performance in real-time on live 10 Gbps traﬃc streams from the two organizations,

(b) testing the detection rate for malicious DNS queries that we generate using our

customized tool (i.e., DET [118]) and an open-source tool (i.e., Iodine [121]), (c)

50

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

comparing our one-class classiﬁer with a two-class classiﬁer, and (d) drawing insights

into the top three anomalous domains for which malicious DNS queries are made in

the Research and University networks. Note that our proposed approach is generic

and hence can be readily used in diﬀerent organizations, but the model needs to be

trained by the speciﬁc data of each organization.

3.4.1 Performance Metrics

We begin with three performance metrics, namely accuracy, anomaly score, and

responsiveness of our models.

Accuracy: As mentioned in the previous section, we trained our model with

benign instances from 4 days’ worth of our data (i.e., Days 1-4), and tested with

all instances from Days 5-14 in addition to remaining instances from Days 1-4 that

were not used for training (i.e., “Others”). Tables 3.8 and 3.9 show the rate of

detection (i.e., normal versus anomalous) for the benign and Others instances in

the two networks – instances in the Benign category are among the top 10K of the

Majestic ranking list, and instances in the Others category are beyond 10K. It can

be seen that 98% of benign instances are correctly detected as normal during both

cross-validation (i.e., Days 1-4) and testing (i.e., Days 5-14) phases. We note that

our machine raises a false alarm for about 2% of benign domains, as highlighted in

bold text.

To address this, we populate a whitelist of domains that are highly trusted.

Our whitelist comprises only the top 100 domains from the Majestic ranking

dataset (e.g., “google.com”, “bbc.com”, “amazonaws.com”) as well as popular legiti-

mate (e.g., “akadns.net”, “in-addr.arpa”, “spotify.com”) and security services (e.g.,

“spamhaus.org”, “senderbase.org”). Note that these security services are using dispos-

able domains (i.e., “single-time use”) for the purpose of signaling over DNS queries

(e.g., “0.0.0.0.1.0.0.4e.135jg5e1pd7s4735ftrqweufm5.avqs.mcafee.com” [122]).

51

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.11: Anomaly detection combined with whitelisting for Research institute.

Input

Output Days 1-4 Days 5-14

Benign domains (top 10K)

Others (beyond top 10K)

normal

anomalous

normal

anomalous

98.92%
1.08%
83.50%
16.50%

99.20%
0.80%
87.48%
12.52%

Table 3.12: Anomaly detection combined with whitelisting for University campus.

Input

Output Days 1-4 Days 5-14

Benign domains (top 10K)

Others (beyond top 10K)

normal

anomalous

normal

anomalous

98.92%
1.08%
90.98%
9.02%

98.90%
1.10%
81.74%
18.26%

Table 3.13: Avg. anomaly score for research institute.

Input

Output Days 1-4 Days 5-14

Benign domains

Others

normal

anomalous

normal

anomalous

0.36

0.59

0.44

0.64

0.36

0.61

0.43

0.65

Employing whitelisted domains would slightly enhance detection. Our reﬁned

results are shown in Tables 3.11 and 3.12. We can see a slight reduction in the rate

of false alarms for benign domains – it is now capped at 1.20% for both networks, as

highlighted in bold text. We note there are a total of 10K (out of 923K) and 15K (out

of 1.4M) false alarms for benign instances in the Research and University network,

respectively. Although the rate of false positives is about 1%, network operators can

further reduce this by employing oﬀ-the-shelf intrusion detection systems (IDSs)

such as Zeek on ﬂagged instances. With their signatures of known malicious traﬃc,

Tools like Zeek can ﬁlter out those ﬂagged instances that are indeed benign. Note

that quantifying the rate of false alarms for detected anomalies under “Others” is

non-trivial due to the lack of ground-truth labels.

52

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.14: Avg. anomaly score for University campus.

Input

Output Days 1-4 Days 5-14

Benign domains

Others

normal

anomalous

normal

anomalous

0.39

0.57

0.43

0.63

0.39

0.58

0.43

0.62

Table 3.15: Avg. time complexity of our scheme.

extracting attributes

54 µsec
746 µsec
Total time per each query name 800 µsec

detecting anomalies

Anomaly Score: Anomaly detection algorithms use this score to determine

if an instance is classiﬁed as normal or abnormal. For the iForest algorithm, the

anomaly score varies from 0 to 1, where 0 means purely normal and 1 indicates a

deﬁnite anomaly. A value of an anomaly score of less than 0.5 is reasonable enough

to be interpreted as normal [117].

Tables 3.13 and 3.14 show the average anomaly score (i.e., normal versus anoma-

lous) for the benign and Others instances in the two networks. It can be seen that

the average anomaly score of benign instances during the cross-validation phase

(i.e., Days 1-4) is 0.36 and 0.39 which is well below the threshold value of 0.54

(obtained during model tuning in §3.3.2) for the Research Institute and University

networks respectively. Similarly, the average anomaly score of benign instances dur-

ing the testing phase (i.e., Days 5-14) is 0.36 and 0.40 for the Research Institute

and University networks respectively.

Responsiveness:

In terms of responsiveness, we have quantiﬁed the average

time for extracting eight attributes and anomaly detection (via running prediction

against the trained model) by testing more than 300 million DNS queries in our

dataset from the two enterprise networks. Our attributes extraction and anomaly

detection engines run on a virtual machine using 4 CPU cores, 6GB of memory,

53

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

and storage of 50GB. As shown in Table 3.15, on average it takes 800 µsec to

determine if a DNS query is normal or not. This indicates that our scheme can

process approximately 1250 DNS queries per second, well above the actual rate of

DNS queries in both organizations where the peak value is 800 DNS queries per

second, as shown in Fig. 3.3.

(a) DET tool.

Figure 3.9: Attributes of DNS exﬁltration query names of: (a) DET tool, and (b)
Iodine tool, detected vs. undetected by the University model.

(b) Iodine tool.

3.4.2 Evaluating Models using Known DNS Exﬁltration Data

In this subsection, we evaluate the eﬃcacy of our detection scheme using DNS ex-

ﬁltration data (i.e., ground-truth) including two large sets generated by our cus-

54

!"#$%&’"#$(%%&’#!&$$$$%&)*+!*%%&)$,!"))%%&’"+,+%%&$,-$%&($+%%&#!#%%&!+./012345678%9:;1<%92=/08=>?1@9;A%9:;1<%92=/0720<%92=/0B33?19;<?%9:;1<%92=/0C;D?A>;E%9:;1<%92=/0C;D?A;FG%9:;1<%92=/0H=DI2>;@/9:;1<%92=/0!"#"$#"%&’(&)*+&,-$./0"&./012345678%9:;1<%92=/08=>?1@9;A%9:;1<%92=/0720<%92=/0B33?19;<?%9:;1<%92=/0C;D?A>;E%9:;1<%92=/0C;D?A;FG%9:;1<%92=/0H=DI2>;@/9:;1<%92=/0$$%&++!*%%&(’$%%&’,!%&+$%&(#%%&$(10%"#"$#"%&’(&)*+&,-$./0"&!"##$%&"’&(%)**%(+)%+&+!&$$%+**&",&%&"(&+)%+&+,%-&(%’!&%+*./012345678$9:;1<$92=/08=>?1@9;A$9:;1<$92=/0720<$92=/0B33?19;<?$9:;1<$92=/0C;D?A>;E$9:;1<$92=/0C;D?A;FG$9:;1<$92=/0H=DI2>;@/9:;1<$92=/0!"#"$#"%&’(&)*+&,-$./0"&./012345678$9:;1<$92=/08=>?1@9;A$9:;1<$92=/0720<$92=/0B33?19;<?$9:;1<$92=/0C;D?A>;E$9:;1<$92=/0C;D?A;FG$9:;1<$92=/0H=DI2>;@/9:;1<$92=/0+)%+#!&$$%&++%),*,%(&%*10%"#"$#"%&’(&)*+&,-$./0"&Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

tomized DET tool and the open-source Iodine tool, and a small set collected from

publicly reported real malicious DNS queries.

Our DET Tool: We showed previously in Table 3.7 that our models for the

Research Institute and the University campus respectively were able to correctly

detect 95.07% and 98.49% of exﬁltration queries (generated by our DET tool) as

anomalous instances.

In Fig. 3.9a, we show the value of attributes for detected instances (blue oc-

tagon on the left) versus undetected instances (red octagon on the right) using the

model generated from data of the university campus. Even though undetected in-

stances were shorter both in total length and average label length, it is important

to note that there is a fair overlap of value range comparing detected (i.e., classi-

ﬁed as anomalous) with undetected instances (i.e., classiﬁed as normal) across all

attributes, suggesting that attributes collectively would determine a fairly accurate

output of our model. To explain it further, we look at the attributes of two pairs

of FQDNs generated from our DET tool (one classiﬁed as normal and one classiﬁed

as anomalous by the model of the research institute), as shown in Table 3.16 – nor-

mal and anomalous classiﬁed FQDNs are shown in bold and italic fonts respectively.

This is because we have obfuscated the actual primary domain used in the DET tool

for privacy reasons. For each pair, we investigate distinguishing factors given some

identical (or close) attributes, highlighted in bold in Table 3.16. We see six common

attributes (character count, numerical character count, number of dots, maximum

label length, average length of labels, and sub-domain character count). However,

entropy and upper-case attributes have relatively larger values in the detected in-

stance (i.e., italic text). Moving to the second example, where entropy, number

of dots, and upper-case characters count are very close in two instances, the query

length becomes an essential factor for the model detecting or missing a malicious

instance.

Iodine Tool: To further evaluate the eﬃcacy of our scheme, we used the Iodine

55

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.16: Samples of malicious queries (DET) along with their attributes detect-
ed/undetected by the model of the research institute.

Sample FQDNs

# chars entropy # numeric # dots # upp. max label. avg label # chars subdom.

s6wIrxk.363937356263363563663038333865.maliciousDomain.com

ClBQxLW.656661356534343938623539393265.maliciousDomain.com

J8tngo1.53061393230646235636634326137656436.maliciousDomain.com

dZrlKkg.645a726c4b6b677c217c337c217c3732613830656235373830.3333386331643631393837.maliciousDomain.com

58

58

63

101

3.98

4.24

4.15

4.15

31

30

37

64

3

3

3

4

2

6

2

3

30

30

35

50

13

13

15

19

39

39

44

82

tool [121] to generate an additional dataset of malicious DNS queries. First, we

exﬁltrated the same CSV ﬁle of 1000 samples of random credit card details similar

to our DET tool. It took approximately 8 seconds to transfer the entire CSV ﬁle.

Next, we wrote a Python script to repeat the process with a delay (between runs)

uniformly distributed between 20 and 40 seconds. We ran the script for three days.

As a result, we captured more than 2.2 million unique instances – our Iodine dataset

is also made publicly available [120]. Note that, unlike our customized DET tool, we

only used iodine with its default settings (i.e., no variation of parameters in DNS

queries). When this dataset was tested with our iForest model of the University

campus, except for 275 instances all others were correctly detected as anomalous.

The average anomaly score was 0.86 and 0.49 for correctly and incorrectly classiﬁed

instances, respectively. We also tested against the model of the Research Institute.

We found that a small number of malicious instances (1837 out of 2.2 million)

were missed – the average anomaly score was 0.67 and 0.45 (lower scores than the

university model) for correctly and incorrectly classiﬁed instances, respectively.

In Fig. 3.9b, we show the value of attributes for Iodine instances (detected versus

undetected) when tested against the university in the same way as we did in Fig. 3.9a

for DET instances. We can see that undetected instances (red octagon on the right)

have fewer numerical characters in their query name – 2 to 10 numerical chars versus

21 to 242 in detected instances. Additionally, it is observed that missed instances

are relatively short (total chars count of 34-42), with a few uppercase chars (up to

8), and contain short labels (average about 5). Note that the length of DNS queries

generated by iodine is typically longer (average of 207 chars). Still, we intentionally

diversiﬁed the query length (30 to 218 with an average of 64) in our custom DET

56

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.17: Anomaly score of queries publicly reported as DNS Exﬁltration.

Known Malicious FQDN

Anomaly Score

708001701462b7fae70d0a28432920436f70797269676874.20313938352d32303031204d696372.6f736f667420436f72702e0d0a0d0a0.433a5c54454d503e.cspg.pw

9ad9ca2.grp10.tt1.dcd2fed0d2fefecdc8d2c4c8c8fecdde.e3e29f9a9ff9cbc79fdae3fcc4 d2c8c4cdd0feded295e9e9e9e9e9e9feea.e9e9e9e9e9e9e9e9e9e9e9e9e9e

9e9e9e9e9e9e9e9e9e9e9e9e9e9e9e9e9.e9e9e9e9e9e9e9e9e9e9.ns.a23-33-37-54-deploy-akamaitechnologies.com

PzMnPiosOD4nOCwuOzomPS4nNjovPS8uOzsnNCstODkjOCwoMwAA.29a.de

9ad9ca2.grp10.tt2.dcc8c8d0c8fccdd2fcd0dcdec8c8cdc8.e6dcc8c8d0c8fccdd2fcd0dcdec8c8cdc8e9dcdcdec8ded2feded0d2c8fc.ns.a23-33-37-54-deploy-akamaitechnologies.com

ZTEZGKDFA0KNGUCQI.ns1.logitech-usa.com

WQPKBPRYA0IVDUQWI.ns1.logitech-usa.com

QRBJBPRYA0JBKUGVI.ns1.logitech-usa.com

SXLXBPRYA0IVDUKTI.ns1.logitech-usa.com

SXLXBPRYA0IVDUKTI.ns1.logitech-usa.com

6e517f3.grp10.ping.adm.cdd2e9cde9fee9cdc8.cdd0e8e9c8fce9d2e9fecdc4.c597f097ce87c5d3.ns.a23-33-37-54-deploy-akamaitechnologies.com

RoyNGBDVIAA0.0ffice36o.com

iucCGJDVIBDSNF3GK000.0ffice36o.com

viLxGJDVIBJAIMQGQ000.0ffice36o.com

gLtAGJDVIAJAKZXWY000.0ffice36o.com

TwGHGJDVIATVNVSSA000.0ffice36o.com

1QMUGJDVIA3JNYQGI000.0ffice36o.com

t0qIGBDVIAI0.0ffice36o.com

0.75

0.70

0.68

0.67

0.65

0.65

0.65

0.65

0.65

0.59

0.58

0.58

0.58

0.58

0.58

0.57

0.57

tool, resulting in a slightly higher percentage of missed instances.

Real malicious DNS queries: Additionally, we tested 17 samples of DNS

queries from known real malware reported on various forums [2, 105, 113, 123]. The

top ten instances correspond to POS malware, and the bottom eight instances were

recently found as part of a new attack targeting networks of a private airline company

[123]. Our trained model was able to detect all of them as abnormal instances. Table

3.17 gives the anomaly score of these known malicious domains. It can be seen that

values are well above the average anomaly score of benign instances shown in Table

3.13 and 3.14.

3.4.3 Comparing Multi-Class Classiﬁer with One-Class Clas-

siﬁer

Existing proposals have predominantly used stateful attributes (e.g., mean/variance

of time-interval between a pair of DNS query/response, frequency of A, AAAA,

TXT types resource records, or time between two DNS responses from a given

57

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Table 3.18: Detecting wild malicious DNS queries from two enterprises.

Anomalous domains Daily avg. # queries Daily avg. # enterprise hosts

Distinct types

Avg. TTL (sec)

Research Institute

imrworldwide.com

cnr.io

1rx.io

2o7.net

University Campus

360.cn

imrworldwide.com

adlooxtracking.com

20.9K

17.4K

2.5 K

1.2K

80.4K

69K

3K

12

4

10

8

203

122

51

A [64%], AAAA [36%]

TXT [99.9%], AAAA [0.01%]

AAAA [58.9%], A [41.1%]

A[53.4%], AAAA [46.6%]

A[98%], AAAA [1.8%], CNAME[0.02%]

A[97.5%], AAAA [2.5%]

A [88.1%], AAAA [11.9%]

1250

0

45

144

1781

3139

903

domain) along with a combination of stateless attributes to detect data theft over

DNS protocol that is fundamentally diﬀerent from our approach (i.e., using stateless

attributes only). Hence, we cannot compare the performance of our model with prior

research work. However, we compare the eﬃcacy of our proposed one-class classiﬁer

(i.e., iForest anomaly detector) with a two-class classiﬁcation model (i.e., Random

Forest in [63, 124]) using the stateless attributes considered in this chapter.

We build a new dataset for two-class classiﬁcation using 4-days worth of top

10K majestic million as benign (same as for the iForest model) and 1.4 million

records generated by our DET tool as malicious instances. We split this dataset

to 60% for training and 40% for testing. The accuracy of validation (on training

data) and testing is about 99% and 97% respectively, with the default parameters

of the Random Forest classiﬁer. Also, we obtain the conﬁdence-level of the Random

Forest model to assess its conﬁdence in making decision for test instances (benign or

malicious). The model displays an average conﬁdence of 99.9% for correctly classiﬁed

benign instances – this measure is 76% for misclassiﬁed benign instances. Similarly,

when we present malicious queries from the DET tool, the average conﬁdence is

99.9% and 70% for correct and incorrect predictions, respectively.

To further evaluate the eﬃcacy of Random Forest, we tested instances from the

Iodine dataset (not used in model training). The model correctly detected only

0.001% of Iodine instances (2543 out of 2.2M) as malicious. Therefore, we tuned

three parameters of the model, namely the number of trees (1 to 200), the number

of selected attributes for each tree (1 to 8), and depth of the tree (1 to 20). We

used the same method described in IV and found the optimal values equal to 8,

58

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

5, and 9 respectively for the number of trees, the number of attributes, the depth.

With these optimized parameters, the rate of correctly detecting Iodine queries as

malicious improved to 54.8%. However, the average conﬁdence of the tuned model

for correctly and incorrectly classiﬁed instances was 75% and 52% respectively – the

model conﬁdence becomes lower than its performance for the trained DET instances.

Moreover, we presented the 17 samples of publicly reported malicious DNS

queries (listed in Table 3.17) to the tuned Random Forest model and found that

13 out of 17 instances were misclassiﬁed. This again proves that multi-class clas-

siﬁers display poor resilience to morphed attacks that deviate from known attack

signatures.

3.4.4 Malicious DNS Queries from Enterprise Networks

We now look at real DNS queries of the two enterprises that were detected as anoma-

lies in §3.4.1. Focusing on “Others" in Tables 3.11 and 3.12 (i.e., domains that are

not among top 10K Majestic ranking list), we see about 12% and 18% of instances

in the Research and University networks respectively that are ﬂagged as anomalous

DNS queries. We have further analyzed these instances to gain insights into their

primary domains. We found that only a few primary domains contribute more than

80% of anomalous instances in both organizations. Table 3.18 lists these top do-

mains. Interestingly, domain “imrworldwide.com” is seen in both the Research and

University networks – this domain is known spyware that tracks web activity of

victim hosts [125]. We also see “adlooxtracking.com” the third top domain detected

in the University network which is a notorious domain that redirects web users to

phishing/unsafe webpages, resulting in freeware downloads [126]. In addition, our

model detects suspicious domains “2o7.net”, “cnr.io”, “1rx.io”, and “360.cn” with a

fairly signiﬁcant number of queries. However, we cannot verify that they are mal-

ware or spyware – this may need further investigations into end-hosts that generate

59

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

these queries.

Insights: To better understand these DNS queries detected as malicious, we

have further analyzed their corresponding DNS responses – note that DNS responses

are exclusively used in this section for drawing further insights into anomalous

queries. As mentioned above, Table 3.18 lists the top malicious primary domains

along with their statistics, including the daily average number of DNS queries gen-

erated for each domain, the daily average number of enterprise hosts querying for

each domain, distinct DNS types with their distribution, and the average of TTL

values (speciﬁed in their corresponding response). For the research institute, we can

see that “imrworldwide.com” is queried more than 20,000 times a day (on average)

and only 12 unique hosts (i.e., IP addresses) make these queries. Analyzing these IP

addresses, we found (by reverse lookup) that ﬁve of them are recursive resolvers of

the research institute – having recursive resolvers as querying hosts is also observed

for other anomalous domains. Focusing on seven hosts which are regular clients, four

were found actively making anomalous queries on four days, while the other three

hosts do not display malicious behavior over the rest of the week of our analysis

(though they are present on the network). This observation suggests that those four

regular hosts are possibly infected by malware or spyware. Moreover, we found that

of those four regular clients, three generated queries to all top malicious domains,

except “cnr.io” over the entire week in the research network. Consistently generat-

ing anomalous queries over the week is seen in two regular clients for “1rx.io”, and

one regular client for “2o7.net”.

Similarly, for the university campus, we see on average 80,000 daily queries for

“imrworldwide.com” from an average of 203 unique enterprise hosts. By reverse lookup

of host IP addresses, we found that seven of them are recursive resolvers and the re-

maining 196 hosts are regular clients – 150 of these clients consistently send queries

for “imrworldwide.com” during the entire week.

Interestingly 130 of them fall un-

der one subnet of size /24. Furthermore, we found that a total of 290 university

60

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

hosts generate queries for at least one of the top three malicious domains (i.e.,

“imrworldwide.com” or “adlooxtracking.com” or “360.cn”) – of these hosts, 35 make

queries for all of these top three malicious domains. By reverse lookup we discov-

ered that 6 of them are recursive resolvers and the remaining 29 hosts are from the

same subnet of size /24, indicating that this particular subnet might be infected by

malware or spyware.

We further investigated the type ﬁeld in DNS queries for these frequent mali-

cious domains.

“A”-type and “AAAA”-type records map domains to IPv4 and IPv6

addresses respectively. Our ﬁrst observation by looking at distinct types of anoma-

lous queries in Table 3.18 is that there is a much greater percentage use of IPv6 in

the Research Institute than in the University network. Secondly, we observe that

“TXT” strongly dominates the type of DNS queries for “cnr.io” in the Research In-

stitute which clearly indicates a data exﬁltration/tunneling over DNS [127]. Note

that sophisticated attackers tend to use other types (i.e., “A”, “AAAA”, “CNAME”, “NS”

and “MX” instead of “TXT") to hide their malicious activities over DNS.

DNS responses contain a Time-To-Live (TTL) ﬁeld in seconds, indicating the

duration of a DNS resource record to be cached on the host machine. According

to RFC 1033 [128], it is important to set an appropriate TTL value since very

low values result in overloading the DNS server, and very high values may limit

the ﬂexibility of changing resource records in real-time. According to RFC 1912

[129], it is recommended to set the TTL value between one to ﬁve days. But, CDN

(Content Distribution Network) services tend to use smaller TTLs for fast reaction to

dynamic resource changes. Unfortunately, malicious entities also use small TTLs for

minimizing their footprints and becoming more resistant against DNS blacklisting

[50].

In Table 3.18, we compute the average TTL for each of the top malicious

domains. We observe that malicious domains use relatively smaller TTLs (i.e., less

than an hour), for example it is set to 0 in all of the DNS responses for “cnr.io”.

Another example is “1rx.io” for which the average TTL is 45 seconds.

61

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

(a) Research institute.

(b) University campus.
Figure 3.10: Number of DNS queries for top malicious domains over a day.

We plot in Fig. 3.10 the query count (computed every 10 minutes) of top anoma-

lous primary domains on day 6-Aug-2018, as an example – missing points in this

ﬁgure correspond to zero query count over those 10-min epochs. Note that the mean

and standard deviation of the anomaly score is shown next to each domain name

in the legend. Our ﬁrst observation is that the query count for all primary domains

is higher during working hours (i.e., increasing an order of magnitude at about 8

am, staying at a certain level, and falling back at about 5 pm), though the primary

62

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Figure 3.11: Web-UI of our real-time DNS exﬁltration and tunneling detector.

domain “cnr.io” in Fig. 3.10a displays a fairly consistent pattern of query count over

a day (except one spike at around 2 pm).

Finally, looking at the anomaly score of queries for these selected malicious do-

mains (as shown in the legend of Fig. 3.10), the primary domain cnr.io has the

largest mean value 0.62 (the closer to 1 means more anomalous). The average score

for other malicious domains in both networks varies between 0.52 to 0.56 which is

well above the average score for benign instances (i.e., less than 0.40) reported in

Table 3.13 and 3.14. We acknowledge that the anomaly score is just an indica-

tor, that can be used to ﬂag suspicious domains. The network operators can then

perform further investigation into these identiﬁed domains.

3.5 Real-Time Deployment in Campus Network

This section presents our web-tool to access and visualize real-time detection of

malicious DNS queries for an enterprise network of a large university campus in

Sydney, Australia. We showcase how to visualize our real-time learning-based de-

tection engine operational on 10 Gbps traﬃc streams from the network border of

the university campus.

63

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

Figure 3.12: Filtering queries for a speciﬁc primary domain name.

Figure 3.13: Filtering queries with a minimum anomaly score.

3.5.1 Visualizing Detection of Malicious DNS Queries

We developed a tool to provide an intuitive user-interface for real-time monitoring of

outgoing DNS queries in enterprise networks using ReactJS. Our web-tool is publicly

available at [120].

This tool allows the user to visualize timestamped queries in real-time at various

time scales, including last 5 sec, 1 min, 10 min, 30 min, and 60 min, as shown

in Fig.

3.11. It also shows selected attributes for each query and the output of

our model. Records marked by a red cross correspond to DNS queries detected as

malicious by our machine learning model, while green ticks mark benign queries –

for privacy reasons, we do not show the FQDNs of live traﬃc in our web-tool. The

64

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

“’domain rank” shows the reputation of the primary domain obtained from Majestic

list [44] – if a primary domain is not found in top million Majestic list then its rank

is shown empty, for example tribdss.com in Fig. 3.11.

The user can enter a speciﬁc primary domain name, ﬁltering all records (in real-

time) associated with that primary domain. For example, in Fig. 3.12 the user has

selected google.com, the tool is showing the results speciﬁc to this ﬁltered domain.

Furthermore, our tool is capable of ﬁltering query records based on anomaly score

(computed by our model), as shown in Fig. 3.13 which shows all queries with the

anomaly score of 0.60 or more – anomaly score varies from 0 to 1, where 0 is least

anomalous and 1 is the most anomalous. This value is calculated by the machine

learning algorithm used while classifying a query name. Additionally, our tool has

a function to pause/resume the stream. If the user notices a malicious domain in

real-time, then (s)he can pause the visualization stream to analyze the attributes of

the malicious domain.

3.6 Conclusion

Enterprise networks are potential targets of cyber-attackers for stealing valuable and

sensitive data over DNS channels. We have developed and validated a mechanism for

real-time detection of DNS exﬁltration and tunneling from enterprise networks. By

analyzing DNS traﬃc from two organizations, we have identiﬁed attributes of DNS

query names that can be extracted eﬃciently in real-time distinguishing legitimate

from malicious queries. We then developed, tuned, and trained a machine-learning

algorithm to detect anomalies in DNS queries using a known dataset of benign

domains as ground truth. Lastly, we evaluated the eﬃcacy of our scheme on live 10

Gbps traﬃc streams from the borders of two enterprise campus networks by injecting

more than three million malicious DNS queries via DET and Iodine tools – our

tools and datasets are publicly available. We showed that our solution outperforms

65

Chapter 3. Monitoring DNS Queries for Detecting Data Exﬁltration

the two-class classiﬁer in detecting new malicious DNS queries. We have drawn

insights into anomalous DNS queries by their anomaly scores, the trace of query

count over time, enterprise hosts querying them, and TTL and Type ﬁelds of their

corresponding responses. Lastly, we demonstrated our web tool for visualizing our

learning based on the detection of anomalous DNS queries (DNS exﬁltration and

tunneling from enterprise networks) operational in our campus network.

66

Chapter 4

Automatic Detection of

DGA-Enabled Malware Using SDN

and Traﬃc Behavioral Modeling

Contents

5.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

5.2 Analyzing Two Variants of DNS Random Subdomain Attacks in

Our Campus Network . . . . . . . . . . . . . . . . . . . . . . . . 114

5.2.1 Attack Scenarios

. . . . . . . . . . . . . . . . . . . . . . 115

5.2.2 Drawback of Using a Threshold for NXDs Attack Detection121

5.3 Multi-Staged Machine Learning Architecture . . . . . . . . . . . 122

5.3.1

System Design . . . . . . . . . . . . . . . . . . . . . . . . 122

5.3.2 Model Training . . . . . . . . . . . . . . . . . . . . . . . 126

5.4 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . 127

5.4.1

Performance of Fine-Grained Model . . . . . . . . . . . . 127

5.4.2

Performance of Coarse-Grained Model

. . . . . . . . . . 128

5.4.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 129

67

Chapter 4. Automatic Detection of DGA-Enabled Malware

5.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129

In the previous chapter, we analyzed outgoing DNS queries to tackle the data

exﬁltration over DNS. This chapter combines Software Deﬁned Networking (SDN)

and machine learning to develop an accurate, cost-eﬀective, and scalable system for

detecting infected hosts communicating with external C&C servers, subsequent to

the resolution of DGA query names. Our solution dynamically selects network ﬂows

for diagnosis by trained models in real-time and relies more on the behavioral traﬃc

proﬁle than packet content. Parts of this chapter are published in IEEE Transactions

on Network Science and Engineering (TNSE) [130].

Our contributions are threefold: (1) We analyze full DNS traﬃc collected from

the border of our university campus network (2.4B records for 75 days) to highlight

the prevalence and activity pattern of more than twenty families of DGA-enabled

malware across internal hosts. We draw insights into the behavioral proﬁle of DGA-

enabled malware ﬂows when communicating with C&C servers by analyzing a Packet

Capture (PCAP) trace (3.2B packets) collected during the peak hour from our cam-

pus network; (2) We identify malware traﬃc attributes and train three specialized

one-class classiﬁer models using behavioral attributes of malicious HTTP, HTTPS,

and UDP ﬂows obtained from a public dataset. We develop a monitoring system

that uses SDN reactive rules to automatically and selectively mirror TCP/UDP ﬂows

pertinent to DGA queries (between internal hosts and malware servers) for diagnosis

by the trained models; and (3) We evaluate the eﬃcacy of our approach by testing

suspicious traﬃc ﬂows (selectively recorded by SDN reactive rules over a 50-day

trial) against our trained models, identify infected hosts from suspicious ﬂows, and

verify our detection with an oﬀ-the-shelf Intrusion Detection System (IDS) software

tool.

68

Chapter 4. Automatic Detection of DGA-Enabled Malware

4.1 Introduction

Cyber threats and data breaches continue to increase in both frequency and complex-

ity, placing businesses and individuals at constant risk. According to Cybersecurity

Ventures [131], cybercrime damages will cost the world $6 trillion annually by 2021.

Enterprises, small and large, remain among the top lucrative targets of automated

attacks [132, 133]. Enterprise networks are often complex, with applications that

rely on a mix of local and cloud-based services, and hence diﬃcult to manage se-

curely [134]. Enterprise hosts often include powerful servers, personal computing

devices, mobile phones, and unmanaged Internet of Things (IoTs). These devices

may use a mixture of statically or dynamically assigned addresses from several pub-

lic and private Internet Protocol (IP) address ranges. Poorly administrated assets,

like personal computers or unpatched servers [135], are not only potential victims

of cyber-attacks but are also sources of risk for other entities on the Internet. For

example, hosts sitting behind the enterprise border ﬁrewall can be infected by mal-

ware from phishing emails, security holes in browser plugins, or other infected local

devices.

Malware-infected machines forming a botnet are typically managed remotely by

an adversary (aka botmaster) via a C&C channel. Cyber-criminals primarily use a

botnet for malicious activities such as stealing sensitive information, disseminating

spam, or launching denial-of-service attacks. Therefore, law enforcement agencies

routinely perform takedown operations on the blacklisted C&C servers [3], disrupting

their botnet activities.

In response to these eﬀorts, botmasters have developed

innovative approaches to protect their infrastructure. The use of DGAs is one of the

most eﬀective techniques that has gained increasing popularity [4].

DGAs use a “seed” (a random number accessible to both the botmaster and

the malware agent on infected hosts) to generate a large number of custom domain

names. Generating numerous time-dependent domain names and registering only

69

Chapter 4. Automatic Detection of DGA-Enabled Malware

the relevant one(s) “just shortly” before an attack allows a botnet to shift their C&C

domains on the ﬂy and remain invisible for longer [5]. The botmaster waits for

the malware to successfully resolve a Domain Name System (DNS) query for the

registered domain, enabling the C&C communications to take place. Note that even

if a C&C server is taken oﬄine or blacklisted, this process can simply be restarted,

and a new server can come online. To date, more than 80 collections of DGA domains

(each corresponding to a malware family) have been recorded by DGArchive [6] and

are publicly available.

There exist a number of research works [44, 45, 48–52] that analyze DNS traces

to identify malicious activities, detecting C&C servers, infected hosts, or malicious

domains. However, their proposed methods largely require the extraction of infor-

mation from DNS packets, correlating queries and responses, and maintaining many

states over a reasonably long duration. All of these processing steps collectively de-

mand heavy compute resources and hence make it diﬃcult to scale cost-eﬀectively.

On the other hand, existing ﬁrewalls and intrusion detection systems rely primarily

on inspecting every packet traversing the network, which makes them expensive.

Further, correlating malicious DNS queries with subsequent C&C communication

ﬂows would signiﬁcantly impact their inferencing accuracy and cost.

In this chapter, we employ the SDN paradigm to judiciously combine selective

packet inspection (only DNS proactively) and ﬂow behavioral analysis (reactively)

to intelligently detect malware-infected hosts on the network. The novelty of this

chapter arises from the dynamic inter-relation of SDN and machine learning tech-

nologies for a sophisticated yet cost-eﬀective cyber-security solution. Commodity

SDN switches today can forward data very cost-eﬀectively at Terabits-per-second.

When combined with intelligent machine learning algorithms in software, it provides

the ﬂexibility and agility to deal with existing and emerging threats. This is ideal

for dynamically selecting ﬂows (after malicious DNS queries), and inferring their

behavioral health using trained models. We use public data of malware families to

70

Chapter 4. Automatic Detection of DGA-Enabled Malware

develop our machine learning models.

Our ﬁrst contribution highlights the prevalence and activity pattern of more

than twenty DGA-enabled malware families on internal hosts of a university campus

network by analyzing entire DNS traﬃc (consisting of 2.4B records) collected over

75 days from the network border (outside of the ﬁrewall). We also analyze a PCAP

trace of full campus Internet traﬃc (collected during the peak hour with a total

load of about 10Gbps) to draw insights into the behavioral pattern of DGA-enabled

malware ﬂows. For our second contribution, we identify key traﬃc attributes of

malware, and train one-class classiﬁer specialized models by attributes of malicious

HTTP, HTTPS, and UDP ﬂows obtained from a public dataset. We then develop

a monitoring system that uses SDN reactive rules to automatically and selectively

mirror TCP/UDP ﬂows (between enterprise hosts and malware servers pertinent to

DGA queries) for making inferences by the trained models. Finally, we evaluate

the eﬃcacy of our proposed model by testing suspicious traﬃc ﬂows (mirrored by

SDN reactive rules and recorded over a 50-day trial) against our trained models,

identify infected hosts from suspicious ﬂows, and verify our detection with an oﬀ-

the-shelf IDS software tool. Also, we compare the performance of our one-class and

multi-class models.

4.2 Analyzing Network Traﬃc Data: Prevalence of

DGA-Enabled Query Names and Network Be-

havior of Malware

In this section, we begin by analyzing the DNS traﬃc of a campus network to

demonstrate the prevalence of DGA-enabled domain names (obtained from a public

dataset) which are found in DNS queries of internal hosts. We, then, analyze a

one-hour PCAP trace of the entire campus traﬃc (in/out) to understand the net-

71

Chapter 4. Automatic Detection of DGA-Enabled Malware

work behaviors of internal hosts when they communicate with Internet-based servers

following their DGA-related DNS query.

4.2.1 Our Datasets

In this work, we use four diﬀerent datasets including (a) 75 daily DNS PCAPs

collected from the border of a university campus network, (b) a one-hour PCAP

trace of the entire traﬃc of the university campus network to/from the Internet, (c)

82 archived ﬁles containing more than 65 million domain names used by DGA-related

malware families, and (d) public network traces (PCAPs and NetFlow records) of

known malware and benign traﬃc.

PCAP Traces of DNS Traﬃc: We collected daily DNS PCAP traces from

the border of the University campus network. Each PCAP has a size of about

15 GB on average. The IT department of the campus network provisioned a full

mirror (both inbound and outbound) of its Internet traﬃc (on a 10 Gbps interface

each) to our data collection system from its border router (outside of the ﬁrewall).

We obtained appropriate ethics clearance (Human Research Ethics Advisory Panel

approval number HC17499) for this study. We extracted DNS packets from each

enterprise Internet traﬃc stream in real-time by conﬁguring rules to match incom-

ing/outgoing IPv4 and IPv6 UDP packets on port 53 in an OpenFlow switch. This

work analyzes data collected over 75 days from 16-Sept-2019 to 1-Dec-2019. Our

detailed analysis is described later in this section (§4.2.2, §4.2.3, and §4.2.4).

One-Hour PCAP Trace of Full Campus Traﬃc: In addition to DNS packet

traces, we recorded all incoming/outgoing packets (only the ﬁrst 96 bytes of each

packet) of the campus network using tcpdump tool during the peak hour (2pm-3pm)

of a weekday on 31st May 2019. This PCAP trace, with over 250 GB, consisting of

3.2B packets, represents traﬃc of large-scale enterprise networks. Fig. 4.1 shows the

aggregate load (packet rate and bit rate, moving averaged over 30-sec intervals) on

72

Chapter 4. Automatic Detection of DGA-Enabled Malware

the Internet link of the campus network. It can be seen that on average more than

a million packets per second are exchanged between internal hosts and the Internet,

resulting in an aggregate load of 10 Gbps. We will use this relatively large dataset

in §4.2.5 to highlight the behavioral proﬁle of malware ﬂows (needles in the haystack

of enterprise network traﬃc) pertinent to DGA queries.

DGArchive: A group of researchers conducted an extensive study [5] on several

families of DGA-based malware. Authors made their dataset (“DGArchive”) avail-

able to the public [6], and they have been consistently expanding their database over

time. We have used the latest version of the database (uploaded on 7th Jan 2019),

which contains domain names from 86 DGA families. We excluded four families for

our study because they overlap with legitimate domains like github.com, itunes.com,

or doodle.com. Such overlaps are seen because their algorithms are restricted to gen-

erate only six-letter strings that can lead to the generation of existing legitimate

domains. Note that datasets of these four families contain more than 20 million

records of domain names, and hence it was infeasible to check all domains, ﬁlter-

ing out the legitimate ones manually. Therefore, we used the ﬁles corresponding

to the remaining 82 malware families and developed our database of about 83 Mil-

lion records of domain names in total (65 Million unique records). Note that some

families share several records with other families – the details of the overlap across

various families can be found in [5]. We will use the DGArchive dataset in §4.2.2,

§4.2.3, and §4.2.4. We acknowledge that our detection method primarily relies upon

the knowledge-base of DGArchive and hence may miss some “novel” malicious query

names that are not captured by this database. Note that this limitation is inherent

to any signature-based detection method (ours included). This public database is

actively updated at a frequency of weeks to months. Therefore, in practice, one can

check this public repository daily or weekly to obtain the latest signatures (domain

names used by latest malware families).

Network Traces of Known Malware and Benign Traﬃc: Authors of [79]

73

Chapter 4. Automatic Detection of DGA-Enabled Malware

Table 4.1: DGA-related domain families found in the campus network (from 16-Sep-
2019 to 1-Dec-2019).

k
c
a
P
d
o
M

r
e
v
o
e
m
a
G

m
i
a
m
y
N

x
o
b
o
p
p
u
S

l
a
t
i

m
a
B

s
u
y
b
n
a
R

u
n
s
t
a
M

t
i
n
m
a
R

a
b
n
i
T

r
e
k
c
o
l
o
t
p
y
r
C

i
r
ﬁ
i
s
T

a
d
m
i
S

i
z
o
G

e
n
o
z
l
r
U

k
a
r
t
w
a
V

i
r
o
j
n
a
B

r
e
d
a
o
l
n
w
o
D

s
r
u
c
e
N

e
r
y
D

y
k
c
o
L

x
a
b
o
B

r
e
k
n
a
b
a
d
n
a
P

r
e
n
a
e
l
c
C

t
e
f
o
r
u
M

t
p
y
r
c
r
i
D

i
a
r
i

M

552,877

22,284

9,419

1,338

842

768

399

324

260

123

94

83

72

69

34

30

28

12

11

8

7

6

4

2

2

93.85% 3.78% 1.60%

0.77%

Malware family

DNS queries count

Unique domain names

6

327

305

38

396

175

13

15

77

7

13

2

6

1

8

5

1

3

4

2

4

3

2

1

1

2

1

released a public dataset called “CTU-13” that contains packet traces of malicious

traﬃc as well as labeled NetFlow records of benign traﬃc. Malicious traﬃc traces

consist of 13 PCAPs (76.8M packets) from network activities of seven real botnets

including Menti [136], Murlo [137], Neris [138], NSIS.ay [139], Rbot [140], Sogou

[141], and Virut [142] on Windows operating systems – executable binary ﬁles of

these malware were installed on lab computers [143] at the CTU University, Czech

Republic, in 2011. Benign traces contain 3.6M NetFlow records of normal traﬃc

(matching certain conditions [79]) from a controlled and known set of computers on

a testbed. We will use this dataset in §4.3 to train our classiﬁer models, validate,

and test their performance in distinguishing malicious ﬂows from benign ones. Our

analysis is motivated by evidence [144] that Web-based “reusable” tools for remote

command of malware are available for sale on the Internet. Also, malware writers

may generate a large number of polymorphic variants of the same malware using ex-

ecutable code obfuscation techniques, however, these variants will ultimately display

similar activity patterns when executed [145].

4.2.2 DGA-Fueled Malware Families

We begin by analyzing DGA-based DNS queries found in the campus network traﬃc.

Out of 2.4B DNS queries made (during 75 days) by internal hosts of the campus

network, about 589K were found in DGArchive, and hence considered as DGA-based

queries belonging to a total of 26 known families. Table 4.1 shows the breakdown

of query count across these families. It is seen that “ModPack” heavily dominates

74

Chapter 4. Automatic Detection of DGA-Enabled Malware

(a) Aggregate packet rate.

(b) Aggregate bit rate.

Figure 4.1: Aggregate load: (a) packet rate, and (b) bit rate, during peak hour
(2pm-3pm) of a weekday (31-May-2019).

Table 4.2:
in the campus network.

Top ten most

frequently used DGA domain names

found

Domain name

DGA family # occurrences

ModPack

ModPack

530,647

22,151

gvaq70s7he[.]ru

76236osm1[.]ru

vqponckshykx[.]in

uecrbipuperq[.]online

qipnhdggsteb[.]org

xllqwgtppipp[.]info

edyrsdetxwnu[.]info

rkcrurklbstr[.]in

Tinba

Tinba

Tinba

Tinba

Tinba

Tinba

jdlrshfmxkqdeprhypbejn[.]org

vwxwvcmicwnu[.]org

Gameover

Tinba

301

284

284

269

189

185

179

177

(93.85%), followed by “Tinba” and “Gameover” respectively, contributing to 3.78%

and 1.60% of total DGA-based queries – other 23 families are not very frequent

(their collective contribution is less than one percent).

These 26 families in total generated 1416 unique domain names. Table 4.2 lists

the top ten most frequently used domain names found in DGA queries and their

corresponding family. Surprisingly, only two domain names (i.e., gvaq70s7he[.]ru

and 76236osm1[.]ru) dominate almost all the queries for the ModPack family. Other

75

2 pm     2:10 pm     2:20 pm     2:30 pm     2:40 pm     2:50 pm     3 pmTime0.20.40.60.81.01.21.41.6Aggregate load (Mpps)incomingoutgoing2 pm     2:10 pm     2:20 pm     2:30 pm     2:40 pm     2:50 pm     3 pmTime02468101214Aggregate load (Gbps)incomingoutgoingChapter 4. Automatic Detection of DGA-Enabled Malware

Figure 4.2: A weekly trace of DNS queries count: total versus DGA-based queries
(from 25-Nov-2019 to 1-Dec-2019) – blue circles highlight representative points to
illustrate that count of DGA-enabled queries is at least three orders of magnitude
less than count of total DNS queries.

families like Tinba, however, use a variety of domain names in their DNS queries.

Various malware types participate in various malicious actions [5] such as unau-

thorized access to victim machines, stealing personal information, or actively taking

part in denial-of-service (DOS) attacks. “ModPack” [6] was found by the Canadian

Centre for Cyber Security (CCIRC) [146] which potentially relates to Andromeda

[147]. Andromeda is malware that infected millions of computers across the world

[148] to perform its botnet activities (i.e., to steal, to destroy websites, or to spread

malicious code). “Tinba” (Tiny banker was ﬁrst discovered in 2012), is a malware

program, targeting banking websites to steal online banking data [149]. Similarly,

“Gameover” Zeus looks for personal and sensitive data e.g., banking information,

customer data, and secret corporate information [150].

We note that the occurrence of DGA-based queries is at least three orders of

magnitude less than typical DNS queries made by internal hosts, as shown by a

76

Mon     Tue     Wed     Thu     Fri     Sat     SunTime103104105106107DNS queries countTotalDGA-basedChapter 4. Automatic Detection of DGA-Enabled Malware

Figure 4.3: Time-trace of DGA-based DNS queries across 75 days (between 16-Sep-
2019 and 1-Dec-2019) – green squares represent no DGA queries, red circles represent
more than 15K DGA queries, and the black pentagon highlights DGA queries daily
count peaks at 30K.

weekly trace in Fig. 4.2. Each data-point in this plot represents the number of

DNS queries over a 6-hour window. For example, it can be seen that on Monday

at 12pm (as highlighted by blue circles), out of 15.4M DNS queries sent out of

the campus network, only 5.4K are DGA-enabled. Such low-proﬁle activity allows

various malware families to go undetected in large enterprise networks [151].

4.2.3 Daily Activity Pattern of DGA-Based Domains

Let us now focus on the temporal activity pattern of various DGA-enabled malware

families. Fig. 4.3 illustrates the daily count of DGA queries during 75 days i.e., from

16-Sep-2019 to 1-Dec-2019. It can be seen that there is no speciﬁc pattern of daily

activity at an aggregate level. For certain days, i.e., mostly Thursdays, Fridays, and

Saturdays, they become completely inactive (zero queries as highlighted by green

squares). Some other days they are heavily active (more than 15K queries per day

77

1st Oct1st Nov1st DecTime05K10K15K20K25K30KQueries count per dayChapter 4. Automatic Detection of DGA-Enabled Malware

as highlighted by red circles). We observe a growing trend in the queries count daily

over this period peaking at 30K towards the end of Nov 2019 (as highlighted by the

black pentagon at the top right of the plot). Focusing on individual families, we

found that almost all families (except Tsiﬁri, Downloader, Pandabanker, and Mirai)

became active on the same day, resulting in signiﬁcant peaks between 28th Nov 2019

and 1st Dec 2019.

To better understand the time-of-day activity of DGA-enabled malware, we plot

in Fig. 4.4 the hourly histogram of DNS queries (overall versus malicious) count

during the 75-day study. Starting from total load in Fig. 4.4a, it can be seen that

the distribution of overall DNS queries reﬂects the daily activity pattern of users – it

starts rising in the morning (8-9am) when students and staﬀ come to the university

campus, peaks at around noon (12-1pm) when most of the users go online during

their lunch break, and starts falling in the afternoon (4-5pm) when the users leave

the campus network at the end of working hours. Moving to the distribution of

DGA queries in Fig. 4.4b, we observe that the probability of ﬁnding DGA queries

on the campus network is relatively higher from noon to midnight, and it is lower

between post-midnight and pre-noon (1am-11am). We note that the temporal ac-

tivity pattern of DGA queries does not correlate with that of the overall network

traﬃc (i.e., mostly benign), especially during afternoon and evening hours. This

increasing trend in malware activity starting from mid-day could be due to waking

times hard-coded in their software, possibly conﬁgured in a time zone diﬀerent from

ours. One may rightly ask how many internal hosts, of which type, and from where

in the network are these hosts making DGA-based DNS queries? We will provide

insights into possible infected hosts later in Table 4.3 (brieﬂy) and Section 4.3 (in

detail).

By analyzing the DNS activity pattern of various DGA families, we categorize

them into three groups, namely (a) Frequent and Heavy, (b) Frequent and Light, and

(c) Bursty. In Fig. 4.5, for each of these three categories, we illustrate the activity

78

Chapter 4. Automatic Detection of DGA-Enabled Malware

(a) Total DNS queries.

(b) DGA-based DNS queries.

Figure 4.4: Hourly histogram of: (a) total DNS queries, and (b) DGA-based DNS
queries, across 75 days (between 16-Sep-2019 and 1-Dec-2019).

(a) ModPack (frequent and heavy).

(b) Suppobox (frequent and light).

Figure 4.5: Time-trace of daily DNS queries count for various DGA-enabled malware
families: (a) ModPack, (b) Suppobox, and (c) Ramnit.

(c) Ramnit (bursty).

pattern of their representative family. Fig. 4.5a corresponds to the most frequent

and the heaviest family, the “ModPack”. We observe that ModPack is highly active

(thousands of queries) during most of the days (starting from 1st October), except

for nine days on which it becomes completely inactive. Moving to the “Suppobox"

79

1am4am8am12pm4pm8pm12amTime020M40M60M80M100M120M140M160M# total DNS queries / hr1am4am8am12pm4pm8pm12amTime05K10K15K20K25K30K# DGA-based queries / hr1st Oct1st Nov1st DecTime05K10K15K20KQueries count per day1st Oct1st Nov1st DecTime050100150200250Queries count per day1st Oct1st Nov1st DecTime0102030405060708090Queries count per dayChapter 4. Automatic Detection of DGA-Enabled Malware

Figure 4.6: CCDF of TTL value in DGA-related DNS responses across representative
malware families.

family in Fig. 4.5b representing a frequent and light family, it is seen that the

daily queries count is fairly low (less than 20), but it rarely goes inactive on a day.

Lastly, as a bursty pattern family shown in Fig. 4.5c, we see that ‘Ramnit" displays

two bursts (end of October and end of November) during the entire period of our

analysis, and it remains completely inactive otherwise. It is important to note that

these activities could be due to either a large number of infected hosts or certain

infected hosts make a large number of DGA queries – we will discuss in §4.2.4 the

challenge of identifying infected hosts purely based on DNS traﬃc. We also found

some forms of coordination across various DGA families in terms of their activity.

For example, Suppobox and Ramnit simultaneously became heavily active on 24th

Oct as well as towards the end of November (i.e., from 28-Nov to 01-Dec) as shown

in Figures 4.5b and 4.5c. We will highlight some examples of such coordination in

§4.2.5.

To further investigate the periodicity of these three representative families, we

extracted the TTL value from their DNS responses. Fig. 4.6 shows the CCDF of

TTL values for each family. It is seen that the ModPack family (dashed black lines

with cross markers) tends to use fairly short TTLs with an average of 373 seconds –

90% of ModPack DNS responses will live less than three minutes. The TTL values

80

100101102103104105TTL (sec)10-510-410-310-210-1100CCDF: P( TTL > x )ModPack (µ=373, σ=1697)Suppobox (µ=7247, σ=12573)Ramnit (µ=11349, σ=13057)Chapter 4. Automatic Detection of DGA-Enabled Malware

in the Suppobox family are relatively longer, averaging at 7247 seconds, while 50%

are more than 15 minutes. Lastly, the Ramnit family has the longest TTL values

with an average of 11349 seconds. Similar to the Suppobox family, Ramnit also has

50% of its responses with a TTL greater than 15 minutes. Overall, the distribution

of TTL values in various families explains (to a great extent) their frequency of

occurrence – the shorter the TTL, the more frequent they become. We will use (in

§4.3.2) TTL values for timeout setting of reactive ﬂow entries installed by the SDN

controller to enable scalable management of switch TCAM table.

4.2.4

Infected Enterprise Hosts

We now look at enterprise hosts that make DGA-enabled queries. Table 4.3 shows

that a very high majority (93.8%) of DGA queries are sourced from DNS recursive

resolvers, and hence the original querying end-hosts are invisible, except a limited

number of hosts (8 enterprise servers and 13 regular hosts) which are probably

conﬁgured to use public DNS resolvers directly (e.g., Google 8.8.8.8). We also note

that 3.7% of DGA queries are generated by 19 WiFi NAT gateways – the identity of

infected hosts (WiFi clients) that generate these queries remains unknown, as they

reside behind NAT gateways inside the network (we collect data from the border of

the network).

Obviously, enterprise recursive resolvers and NAT gateways hide the identity of

infected hosts, and hence host analysis purely based on DNS traﬃc will not suﬃce for

identifying infected hosts. To better understand the network activity of (possibly)

infected hosts following their suspicious DNS queries (found in DGArchive), we

analyze (in what follows) a 1-hour PCAP trace of full traﬃc dump from the campus

network. We focus on subsequent TCP/UDP traﬃc exchanged between Internet-

based C&C servers (following the response of DGA-based DNS queries) and their

respective enterprise hosts – we will show in §4.4 how our SDN-based method in

81

Chapter 4. Automatic Detection of DGA-Enabled Malware

Table 4.3: Enterprise hosts making DGA-enabled queries.

Querying hosts (# hosts)

# DGA queries [%]

Recursive resolvers (3)

Enterprise servers (8)

End hosts (13)

WiFi NAT gateways (19)

552,669 [93.82%]

14,217 [2.41%]

283 [0.05%]

21,929 [3.72%]

conjunction with trained models (§4.3) will be able to identify infected enterprise

hosts.

One may choose to inspect packets of enterprise DNS servers to gain richer

insights into the health of internal hosts purely based on DNS traﬃc. However,

obtaining all (incoming/outgoing) packets of various DNS servers requires signiﬁcant

changes to the distributed infrastructure of large enterprise networks [45]. Our

solution (§4.3), instead, is designed to be a “bump-in-the-wire” on the Internet link

of the enterprise network and provides diﬀerent visibility (a judicious combination

of DNS and selected subsequent TCP/UDP ﬂows) into activities of connected hosts

from a diﬀerent perspective. Furthermore, our detection system is transparent to

the network, requires minimal change to existing infrastructure, is easy to deploy,

and does not modify packets in any way.

4.2.5 Network Behavior of DGA-Fueled Malware

As explained in the previous section, purely monitoring DNS traﬃc does not lead

to ﬁnding the hosts that are indeed suspected of malware infection. Furthermore,

to draw insights into the behavior of suspected hosts (in terms of services used

and/or any possible coordination across infected hosts while communicating with

their corresponding C&C servers), we analyze a concise but fairly active period of

the full campus traﬃc dump. This full dump of the entire Internet traﬃc contains

3.2B packets of which only 2M packets are DNS – less than 0.1%.

82

Chapter 4. Automatic Detection of DGA-Enabled Malware

Table 4.4: DGA-enabled malware and infected campus hosts found by analysis of
one-hour PCAP of full campus traﬃc.

DGA-enabled families found in DNS queries of campus hosts

C&C servers identiﬁed in responses to DGA queries

C&C servers exchanged traﬃc with internal hosts

DGA-enabled families involved in C&C traﬃc exchange

Num. of malware-infected hosts exchanged traﬃc with C&C servers

End hosts

WiFi NAT gateways

Enterprise servers

Count

8

14

5

2

17

8

7

2

Table 4.4 summarizes our ﬁndings from this analysis. During this one hour, eight

DGA-enabled malware families were found in the DNS packets of this 1-hour PCAP

trace. Also, a total of 14 unique C&C servers were identiﬁed from DNS responses,

and only ﬁve of these C&C servers (corresponding to two DGA families) exchanged

TCP traﬃc with enterprise hosts following their DGA-based DNS resolution. Ana-

lyzing these follow-up TCP ﬂows, we found 17 hosts (we call them “suspicious hosts”)

communicated with the ﬁve C&C servers. By reverse DNS lookup, we veriﬁed that

8 of the suspicious hosts were regular end-hosts, 2 were enterprise servers, and 7

were WiFi NAT gateways. These 17 suspicious hosts accessed HTTP and HTTPS

services oﬀered by their corresponding C&C servers, generating 33 suspicious HTTP

ﬂows (75%) and 11 suspicious HTTPS ﬂows (25%) that collectively exchange a total

of only 365 packets which contribute to a tiny fraction (10−7) of total packets (3.2

B) recorded in this one-hour PCAP trace from campus Internet traﬃc.

Additionally, we analyzed the behavior of these suspicious ﬂows to highlight their

activity patterns or possibly ﬁnd coordination across suspicious hosts [152]. Fig. 4.7a

shows the pattern of communications (on a ﬂow basis) between a suspicious host

(a WiFi NAT gateway in this case) and a C&C server of Ramnit family with IP

address “ 89.185.44.100” resolved by a query for domain name “ lvxlicygng.com" (with

the response TTL value of 300 sec) – we veriﬁed this address is blacklisted [153].

83

Chapter 4. Automatic Detection of DGA-Enabled Malware

(a) Sequence of ﬂows.

(b) Sequence of packets in ﬂow “f2”, for example.

Figure 4.7: Communication pattern of a suspicious host with its C&C server of
Ramnit family: (a) sequence of HTTPS ﬂows, and (b) sequence of packets in a
selected HTTPS ﬂow.

We observe that this WiFi gateway (possibly on behalf of a number NATed hosts)

initiates six HTTPS ﬂows (each with a duration of less than a few seconds) to the

server in the time period between 2:10pm and 2:50pm. Note that two ﬂows f5

and f6 are established concurrently. The height of each ﬂow in the plot (Fig. 4.7a)

indicates the number of packets sent and received (outgoing direction shown by

84

2pm     2:10pm     2:20pm     2:30pm     2:40pm     2:50pm     3pmTime15105051015Packet count per minutef1f2f3f4f5,f6601 sec601 sec601 sec601 secOutgoingIncoming020040060080010001200Time (ms)    PacketsPout1Pout2, Pout3Pout4, Pout5, Pout6Pin1Pin2, Pin3, Pin4Pin5, Pin6OutgoingIncomingChapter 4. Automatic Detection of DGA-Enabled Malware

blue circles and incoming direction shown by red squares). Interestingly, there is a

clear periodicity in the arrival of these ﬂows – they are well spaced by 601 sec (≈10

minutes). Some of these ﬂows (e.g., f1 and f2) are symmetric in terms of incoming

and outgoing packet count, and some are asymmetric (e.g., f3 and f4).

To further understand the network activity of these suspicious ﬂows, we zoom

in on the arrival and size of individual packets within each ﬂow per direction. As

an example, we show in Fig.

4.7b the time trace of packets in the ﬂow f2. The

x-axis is time (in ms), and the y-axis indicates the direction (top row corresponds to

outgoing packets and the bottom row corresponds to incoming packets). Also, each

marker represents a packet (circles for outgoing and triangles for incoming), and the

markers size indicates the relative length of the corresponding packet. The duration

of this ﬂow is about 1100 ms over which six packets are sent, and six packets are

received. In this ﬂow, all incoming packets from the server have the same size of

60 bytes. Within less than 400ms from the commencement of the ﬂow, the three-

way handshake is completed, i.e., SYN (Pout1) → SYN-ACK (Pin1) → ACK (Pout2).

Right after establishing the TCP connection, the host sends 60 bytes data over SSL

(Pout3). In response, the server sends SSL data of 60 bytes (Pin2) followed by FIN-

ACK (Pin3) and TCP-RST (Pin4) packets. Next, the host sends SSL data of 139

bytes (Pout4), followed by ACK (Pout5) and FIN-ACK (Pout6) packets. Lastly, the

server sends two more TCP RST packets (Pin5 and Pin6) back-to-back. Observing

such activity patterns will help us (in §4.3.1) identify ﬂow-level attributes needed

for modeling malware traﬃc behavior. As stated earlier in Chapter 2, our main

objective in this chapter is to develop a “cost-eﬀective”, yet “accurate” solution for

detecting malicious ﬂows and infected hosts in “large-scale enterprise networks”. We

will use ﬂow-level attributes (as opposed to computationally expensive packet-level

attributes) to diagnose whether selected suspicious traﬃc is malicious, or not.

As another example, we show in Fig. 4.8 the ﬂow activity of an end-host estab-

lishing a suspicious HTTP ﬂow with its C&C server. It can be seen that in this

85

Chapter 4. Automatic Detection of DGA-Enabled Malware

Figure 4.8: Time trace of packets (outgoing/incoming) in a selected HTTP ﬂow.

case, the server sends some data (not just acknowledgments) to the internal host.

In each direction, 5 packets are exchanged between the end-host and the server over

this ﬂow with a duration of about 4600 ms. The three-way handshake (SYN:Pout1

→ SYN-ACK:Pin1 → ACK:Pout2) completes within the ﬁrst 160 ms. Following the

establishment of this connection, the end-host sends an HTTP GET request (Pout3 with

the size 420 bytes) to the server at the time about 4100 ms. The server responds

with an ACK (Pin2) followed by 1001 bytes HTTP OK (Pin3) – we were unable to inspect

the packet payload since in our PCAP trace packets are truncated to their ﬁrst 96

bytes. Right after that, the server initiates termination of this TCP ﬂow by sending

a FIN-ACK (Pin4) – this packet is followed by ACK (Pout4) and FIN-ACK (Pout5)

packets from the host, and the ﬁnal ACK (Pin5) from the server.

We also found two incidents of possible coordination [87] across suspicious hosts

while contacting their C&C servers at around 2:10pm and 2:20pm. In the ﬁrst in-

stance (at 2:10pm), two suspicious end-hosts simultaneously initiated HTTP ﬂows:

one host initiated four ﬂows. The other one initiated eight ﬂows both with a C&C

86

010002000300040005000Time (ms)    PacketsPout1, Pout2Pout3, Pout4, Pout5Pin1Pin2, Pin3, Pin4, Pin5OutgoingIncomingChapter 4. Automatic Detection of DGA-Enabled Malware

server from Suppobox family with IP address “ 184.168.131.241 (corresponding to do-

main name “ strengthstorm.net” with the response TTL value of 600 sec) – we veriﬁed

that this IP address is blacklisted [154]. In the second instance (at 2:20pm), a WiFi

NAT gateway and an end-host respectively initiated one and two HTTP ﬂows with

a C&C server from the Suppobox family with the same IP address “ 184.168.131.241".

Compute Cost: Our insights into network activities of malware were only ob-

tained by manually analyzing a short but representative PCAP trace of the campus

network. It is important to note that it becomes costly (practically infeasible) to

capture and analyze packets of all network ﬂows at high rates (10Gbps or more).

We also note that malicious traﬃc (speciﬁcally for malware and botnet

) often contributes to a tiny fraction of the total network traﬃc – the majority of

packets are benign. Therefore, it is needed to employ a systematic and scalable

method to capture only suspicious traﬃc (corresponding to servers that are resolved

as a result of DGA queries) and check whether it is malicious or not. In the next

section, we will leverage the ability of SDN to dynamically mirror suspicious traﬃc

ﬂows and develop learning-based models (based on insights obtained in this section)

to automatically detect malicious ﬂows associated to their infected hosts.

4.3 Modeling and Mirroring Traﬃc of Suspicious

Malware Servers

In this section, we begin by developing our protocol-specialist models (one corre-

sponding to each of HTTP, HTTPS, and UDP protocols) using CTU-13 network

traces (discussed in §4.2.1). We, next, develop a system to select, mirror auto-

matically, and diagnose traﬃc ﬂows corresponding to suspicious malware servers.

We employ SDN reactive rules to select and mirror suspicious traﬃc to our packet

processing engine. The engine feeds our trained models by a set of ﬂow attributes

87

Chapter 4. Automatic Detection of DGA-Enabled Malware

for diagnosis, determining whether these selected TCP/UDP ﬂows are malicious or

not.

4.3.1 Modeling Traﬃc Behavior of Malware

We use malware PCAP traces of the CTU-13 dataset [79]. Authors of [79] primarily

aimed to detect malicious “hosts” by developing clustering-based models from their

dataset. They employed host-level attributes, including the count of remote IP

addresses, count of remote/local transport port numbers, average packet size, and

the average count of packets transmitted over windows every two-minute. We note

that computing these attributes near real-time for every internal host (tracking

metadata of all packets) will be computationally prohibitive, especially at scale.

Also, traﬃc modeling at the host-level becomes slightly coarse-grained (aggregate

of benign and malicious ﬂows), resulting in reduced visibility into the activity of

individual malware ﬂows.

Our approach aims to characterize the behavior of

malware activity on a per-ﬂow basis and diagnoses a fraction of network ﬂows,

only those suspicious TCP/UDP ﬂows pertinent to a DGA-related DNS query. We

develop a set of machine learning models (a model per protocol) to determine if a ﬂow

is malicious or not. Many cybersecurity researchers [63, 79, 124] employed multi-

class decision trees to distinguish malicious and benign traﬃc. However, balancing

the training dataset to avoid overﬁtting remains a nontrivial challenge [155].

It

has been shown [93, 156] that one-class classiﬁers or anomaly detection models

can learn the distribution of training data (malicious ﬂows in the context of this

chapter) and detect any deviations (benign ﬂows) during the testing phase. Our

protocol-specialist models will generate “negative” output for malicious instances

and “positive” output otherwise. This use of one-class models means that each model

can be re-trained/updated (in case of extending the malware dataset), independent

of the other models.

88

Chapter 4. Automatic Detection of DGA-Enabled Malware

Table 4.5: Distribution (µ and σ) of attributes value for malicious ﬂows in CTU-13
dataset.

ﬂow volume (B) ﬂow duration (s) # pkts Avg. pkts size (B) ﬂow volume (B) ﬂow duration (s) # pkts Avg. pkts size (B)

Outgoing

Incoming

HTTP

HTTPS

UDP

(980, 666)

(7.3, 29.5)

(5, 1)

(172.5, 94.1)

(1470, 1589)

(1.5, 7.6)

(4, 1)

(270.5, 235.4)

(1597, 4392)

(761.3, 2832.3)

(12, 41)

(81.1, 59.4)

(5805, 47043)

(760.6, 2832.1)

(7, 20)

(187.4, 390.9)

(5968, 100239)

(708.8, 2326.2)

(14, 165)

(201.6, 164.6)

(5968, 100239)

(712.3, 12451.2)

(14, 165)

(201.6, 164.6)

4.3.1.1 Attributes and Classiﬁers for Malware Flows

Inspired by [79], we identify eight attributes on a per-ﬂow basis for malware traﬃc

– 4 per each direction (in/out). Our attributes of a malware ﬂow are as follow.

• ﬂow volume in bytes (in/out).

• ﬂow duration, i.e., the gap between the arrival time of the ﬁrst packet and the

last packet (in/out).

• number of packets (in/out).

• average packet size (in/out).

We computed the above attributes for all (labeled) malicious ﬂows in the CTU-13

dataset. We show in Table 4.5 the distribution (µ and σ) of raw values of attributes

across the entire dataset. Note that the malware ﬂows in the CTU-13 dataset are

from three protocols, namely HTTP, HTTPS, and UDP. By analyzing these values,

we observe that ﬂow volume, ﬂow duration, and packet count for both incoming

and outgoing directions are key attributes in characterizing the three categories of

malware ﬂows. As an example, considering the outgoing direction, the mean (µ)

volume of ﬂow in HTTP, HTTPS, and UDP malware is about 1000, 1600, and 6000

bytes, respectively (ﬁrst column of Table 4.5).

Let us make some high-level observations on the range of attributes across the

three types of malicious ﬂows. Note that the variation of ﬂow volume is much larger

in HTTPS (with σ ≈ 4400) and UDP (with σ ≈ 100, 000) ﬂows than in HTTP

ﬂows (with σ ≈ 700). A slightly similar pattern is observed in the ﬂow duration

89

Chapter 4. Automatic Detection of DGA-Enabled Malware

and packet count attributes. UDP and HTTPS ﬂows, compared to HTTP ﬂows, are

generally longer in duration (mean 700s versus mean 7s), and carry a larger number

of packets (mean 12 − 14 packets versus mean 5 packets). Such a clear distinction

between the three categories (HTTP, HTTPS, and UDP) can also be seen in the

values of attributes for the incoming direction. Therefore, we train three separate

models (each speciﬁc to a protocol), increasing the accuracy of detecting malware

ﬂows. We split the malicious data of each protocol-speciﬁc model into 60% (for

training and validation) and 40% (for testing only).

4.3.1.2 Model Training

We used scikit-learn and its APIs, an open-source machine-learning package writ-

ten in Python, to train and test our models.

Our prediction models are one-

class classiﬁers trained by four popular algorithms, namely Isolation Forest (iForest)

[117], Extended iForest (EiF) [157], K-means [158], and one-class support vector

machines (OC-SVM) [159] using attributes of malicious ﬂows obtained from the

CTU-13 dataset. The models classify a ﬂow: if the subject ﬂow is tested negative

by its corresponding model (i.e., HTTP, HTTP, or UDP), then it is classiﬁed as

“malicious"; otherwise, it is “benign". Later in this subsection, we will compare the

performance of one-class models against that of multi-class classiﬁers.

The iForest algorithm works based on the concept of isolation without employing

any distance or density measure. The algorithm divides instances into sub-samples

to construct a binary tree structure – by randomly selecting the attribute, and then

randomly selecting the split values from a range (within min and max obtained from

training) for that particular attribute – splitting values is always done by an “axis-

parallel” hyperplane (e.g., rectangular shape in 2D space of attributes). If the value

of a given instance is less than the split value, the point is directed to the left branch

of the tree structure otherwise, it goes to the right side branch. This branching is

recursively performed until either a predeﬁned height limit is approached or a single

90

Chapter 4. Automatic Detection of DGA-Enabled Malware

point is isolated in the dataset. The algorithm then marks the instances that travel

less into the tree structure as an anomaly, while those that travel deeper into the tree

structure are classiﬁed as benign. To avoid issues due to randomness, the process is

repeated several times, and the average path length is calculated and normalized.

For the training phase of the iForest models, we consider three tuning parameters,

namely the number of trees (n_estimators), height limit of trees (max_samples),

and contamination rate. For each of the three models, we tune the value of each

parameter while ﬁxing the other two parameters and validate the accuracy of our

specialized models for the malicious ﬂows in the CTU-13 dataset. The default value

for the number of trees is 100, the height limit of trees is set to “auto”, and the

contamination rate is 10%. After tuning the individual three models, we found the

optimal value of these tuning parameters: the number of trees equal to 10, the height

limit of trees equal to 8, and the contamination rate of 1% for all three models.

It has been recently shown [157] that while iForest is a computationally eﬃcient

algorithm, it suﬀers from a bias (aﬀecting the anomaly score) arisen by its use of

axis-parallel hyperplanes. Authors of [157], therefore, enhance the standard iFor-

est algorithm by developing Extended Isolation Forest (EiF), which performs data

splitting with “random-slope” hyperplanes. Our EiF models are tuned in the same

way as iForest.

The K-means algorithm ﬁnds groups of instances (aka clusters) for a given class

similar to one another. Its centroid identiﬁes every cluster, and an instance is as-

sociated with a cluster if the instance is closer to the centroid of that cluster than

any other cluster centroids. For better performance of K-means models, it is essen-

tial to pre-process our data and tune certain parameters. We begin by recording

each attribute’s Z-score (i.e., computing mean µ and standard deviation σ). We

then normalize our dataset instances by calculating the deviation from the mean

divided by the standard deviation for each attribute. To tune a K-means model,

we need to compute the optimal number of clusters that are obtained by the elbow

91

Chapter 4. Automatic Detection of DGA-Enabled Malware

Figure 4.9: Clusters of K-means model for HTTP ﬂows.
Table 4.6: Distribution (µ and σ) of attributes for K-means HTTP clus-
ters.

Outgoing

Incoming

# pkts ﬂow vol.

Pkt size ﬂow dur. # Pkts ﬂow vol.

Pkt size

ﬂow dur.

(5, 0.3)

(1103, 1645)

(214, 26)

(2, 7)

(4, 0.5)

(995, 325)

(218, 77)

(0.42, 1.4)

(7, 0.8)

(838, 216)

(124, 30)

(7, 19)

(7, 1.1)

(4993, 1222)

(767, 222)

(4.0, 15.1)

(4, 1.1)

(358, 138)

(83, 19)

(7, 32)

(3, 0.7)

(200, 95)

(71, 2)

(0.6, 2.6)

(7, 0.6)

(2457, 420)

(366, 55)

(1, 6)

(6, 0.9)

(2770, 992)

(449, 104)

(0.3, 0.8)

(5, 0.9)

(663, 118)

(125, 27)

(15, 44)

(5, 0.8)

(764, 295)

(172, 80)

(2.3, 9.7)

method [160]. By applying this method, we found the optimal number of clusters

for all three models to be equal to 5. We show in Fig. 4.9 the resulting (color-coded)

clusters of training instances for HTTP ﬂows. Note that our instances are multi-

dimensional (i.e., each instance contains 8 attributes), and thus cannot be easily

visualized. Therefore, we employ Principal Component Analysis (PCA) to project

the data instances onto two dimensions for illustration purposes.

Additionally, Table 4.6 provides further insights into attributes of these ﬁve clus-

ters. We make a few observations: these clusters cannot be distinctly identiﬁed by

their packet count attribute – almost similar across all clusters; cyan and black seem

to represent top-heavy clusters (cyan by total ﬂow volume in both directions and

black by the average size of incoming packets); comparing red (top row) and purple

(bottom row) clusters we ﬁnd that the average size of packets in the red cluster is

about 20% (incoming) to 70% (outgoing) larger than that of the purple cluster –

92

5.02.50.02.55.07.5Principal component 1420246Principal component 2Chapter 4. Automatic Detection of DGA-Enabled Malware

also, duration of ﬂows in the red cluster is one order of magnitude shorter than that

in the purple cluster.

OC-SVM is an algorithm that identiﬁes anomalous instances by constructing a

hyperplane boundary around expected training instances. It comes with three main

tuning parameters, namely Kernel, gamma, and nu with default values, respectively

equal to “radial basis function (rbf)", “scale" (inverse of product of attributes count

and attributes variance) and “0.5". We tune OC-SVM similar to iForest and EiF

where a parameter is ﬁxed, and others are varied to ﬁnd the best prediction. The

optimal tuning parameters are found to be as follows: “rbf” kernel, gamma equals

0.125, and nu equals 0.05.

4.3.1.3 Model Validation

We validate the performance of our trained models against training instances (the

only 60% of the malicious dataset since benign instances are not used for train-

ing our one-class models). Validation results are shown by top row in Tables 4.7

(HTTP model), 4.8 (HTTPS model), and 4.9 (UDP model).

It is observed that

three algorithms, namely iForest, EiF, and K-means, perform reasonably well (giv-

ing consistently high accuracy of more than 97% across the three protocol-specialist

models) during the validation phase. However, the OC-SVM algorithm performs

relatively poorly even for validation, with the best malicious detection rate of less

than 73% given by the UDP model.

To better understand the inferencing capability of top-performing algorithms

(i.e., iForest, EiF, and K-means) we now focus on misclassiﬁed ﬂows across these

models. Fig. 4.10 visualizes an approximation of two-dimensional regions for key

attributes of misclassiﬁed ﬂows. We found that a diagnosed ﬂow is misclassiﬁed

(with a probability of more than 90%) by respective models if its attributes fall in

the highlighted regions of Fig. 4.10. Let us start with iForest and EiF models on

93

Chapter 4. Automatic Detection of DGA-Enabled Malware

Table 4.7: Accuracy of HTTP models in correctly detecting malicious and benign
ﬂows.

True Positive False Positive True Negative False Negative

iForest

EiF

K-means

OC-SVM

RandomForest

Validation 98.71%

Testing

97.8%

Validation 99.05%

Testing

98.85%

Validation 98.27%

Testing

98.11%

Validation 63.18%

Testing

61.09%

Validation 94.13%

Testing

81.32%

-

6.45%

-

6.09%

-

8.36%

-

40.57%

4.32%

20.82%

-

93.55%

-

93.91%

-

91.64%

-

59.43%

95.68%

79.18%

1.29%

2.2%

0.95%

1.15%

1.73%

1.89%

36.82%

38.91%

5.87%

18.68%

Table 4.8: Accuracy of HTTPS models in correctly detecting malicious and benign
ﬂows.

True Positive False Positive True Negative False Negative

iForest

EiF

K-means

OC-SVM

RandomForest

Validation 98.96%

Testing

97.16%

Validation 99.13%

Testing

98.92%

Validation 98.42%

Testing

98.45%

Validation 65.59%

Testing

64.90%

Validation 93.89%

Testing

80.27%

-

5.49%

-

5.41%

-

8.08%

-

37.06%

3.27%

17.48%

-

94.51%

-

94.59%

-

91.92%

-

62.94%

96.73%

82.52%

1.04%

2.84%

0.87%

1.08%

1.73%

1.55%

34.41%

35.10%

6.11%

19.73%

the top row. First, it is seen that they misclassify those HTTP ﬂows which are

large in packet count and long in duration, as shown in Fig. 4.10a, while for HTTPS

and UDP ﬂows, having large packet count (more than 100 packets) will probably

lead to misclassiﬁcation, as shown in Figures 4.10b and 4.10c. K-means models, on

the other hand, tend to misclassify HTTP ﬂows with smaller volume but medium-

length (Fig. 4.10d), and HTTPS and UDP ﬂows with larger packet count (Fig. 4.10e

and 4.10f).

Taking these observations into account, none of these models seem distinct except

by their overall accuracy, and hence we choose EiF models for our trial evaluation

94

Chapter 4. Automatic Detection of DGA-Enabled Malware

Table 4.9: Accuracy of UDP models in correctly detecting malicious and benign
ﬂows.

True Positive False Positive True Negative False Negative

iForest

EiF

K-means

OC-SVM

RandomForest

Validation 96.96%

Testing

96.28%

Validation 97.60%

Testing

97.03%

Validation 98.76%

Testing

97.58%

Validation 72.91%

Testing

73.35%

Validation 95.32%

Testing

82.92%

-

7.86%

-

7.01%

-

7.97%

-

31.48%

3.79%

19.82%

-

92.14%

-

92.99%

-

92.03%

-

68.52%

96.21%

80.18%

3.04%

3.72%

2.40%

2.97%

1.24%

2.42%

27.09%

26.65%

4.68%

17.08%

(a) HTTP (iForest & EiF)

(b) HTTPS (iForest & EiF)

(c) UDP (iForest & EiF)

(d) HTTP (K-means)

(e) HTTPS (K-means)
Figure 4.10: Attributes of misclassiﬁed ﬂows: (a,d) HTTP, (b, e) HTTPS, and (c,f)
UDP, across top performing classiﬁers.

(f) UDP (K-means)

in §4.4.

4.3.1.4 Models Testing

Following validation, we quantify the performance of our trained one-class models

against testing malicious instances (the remaining 40% of malicious ﬂows) as well

as the entire set of benign instances. Testing results are shown by bottom row

95

20406080100120140160180200Flow duration (sec)01020304050607080Packet countiForestiForest & EiF51015202530Volume (KB)100150200250300350400Packet countiForestiForest & EiF050100150200250300350400450Volume (KB)100150200250300350400Packet countiForestiForest & EiF20406080100120140160180200Flow duration (sec)0.00.51.01.52.0Volume (KB)51015202530Volume (KB)100150200250300350400Packet count050100150200250300350400450Volume (KB)100150200250300350400Packet countChapter 4. Automatic Detection of DGA-Enabled Malware

in Tables 4.7 (HTTP model), 4.8 (HTTPS model), and 4.9 (UDP model). We

observe that EiF consistently gives the best accuracy for both malicious and benign

testing ﬂows, across the three models – true negatives of more than 97% and true

positives of at least 93%. Unsurprisingly, OC-SVM is found to perform very poorly

(compared with iForest, EiF, and K-means) during the testing phase with (malicious

and benign) detection rates of mostly less than 70%.

One-class versus Multi-class Models: Lastly, to highlight the shortcomings

of multi-class models in diagnosing the health of network traﬃc, we consider a

two-class (malicious and benign) Random Forest classiﬁer. It is trained with 60%

of the entire CTU-13 dataset, and its performance is tested with the remaining

40% of instances. Validation and testing results are shown by the last column of

Tables 4.7, 4.8, and 4.9. It can be seen that Random Forest’s detecting rates (true

positives and true negatives) are around 80%, which is lower than those of iForest,

EiF, and K-means particularly during the testing phase, though it gives acceptable

detection rates (≈ 95%) during the validation phase.

4.3.2 Dynamic Traﬃc Selection using SDN

Fig. 4.11 shows the functional blocks in our system architecture applied to a typical

enterprise network. Enterprise users are on the left and can be on an access network

(wired and/or wireless). The Internet is on the right. Our solution is designed to be

a “bump-in-the-wire” on the link at which traﬃc monitoring/management is desired

(active management) – an alternative approach is to feed our system a mirror of

all network traﬃc (passive monitoring). Our system is therefore transparent to the

network and does not modify packets in any way. Further, no packet is sent to

the SDN controller (for dynamic management of ﬂow rules); instead, selected traﬃc

(DNS packets, C&C ﬂows) that need inspection or diagnosis are sent as copies

on separate interfaces of the switch, to which specialized traﬃc analytics engines

96

Chapter 4. Automatic Detection of DGA-Enabled Malware

Figure 4.11: System architecture of our detection system.

(software inspection) are attached. This protects the controller from overload from

the data-plane, allowing it to scale to high rates and to serve other SDN applications.

Our solution comprises a DGA query ﬁnder (top right of Fig. 4.11), which is fed

by real-time incoming DNS responses, an SDN switch whose ﬂow-table rules will

be managed dynamically by API calls from the DGA ﬁnder to the SDN controller

(center of Fig. 4.11), a packet processing engine that extracts ﬂow attributes (of

suspicious network traﬃc only) feeding machine learning-based models (top left of

Fig. 4.11).

Note that our DGA query ﬁnder engine (handling DNS traﬃc of our university

Internet link with a peak load of about 10 Gbps) runs on a virtual machine with 6

CPU cores, 8GB of memory, and storage of 500GB. We believe that cost and com-

plexity of processing DNS packets in software can be reasonably managed at scale.

In terms of processing costs, empirical results of our previous research studies [44,

45] on traﬃc analysis of our university campus network show that DNS constitutes

a tiny fraction (less than 0.1%) of total network traﬃc by volume. This corrobo-

rates with the analysis of the one-hour full campus traﬃc trace collected during the

peak hour (§4.2.5), revealing that out of the total 3.2B packets only 2M are DNS –

about 0.06%. In terms of complexity of measurement, it is relatively easy to capture

97

Internetcopy incoming DNS responsePacket processing engine<IP of C&C server>SDN controllercopy all traffic to/from C&C serverflow attributesML modelsmalicious / benignin DGArchive? EnterpriseInsert reactive flow13SDN switchyes456our detection system2DGA query finder engineport 2port 1port 3port 4Chapter 4. Automatic Detection of DGA-Enabled Malware

DNS traﬃc with only a few ﬂow entries (i.e., mirroring IPv4 and IPv6 UDP packets

to/from port 53) in an SDN switch.

We believe that the key contribution of this chapter is our detection method

that dynamically (using SDN) and conﬁdently (using specialized classiﬁers) identiﬁes

malicious ﬂows established after DGA queries. Our system infers suspicious hosts

by employing a broad signature from the public database DGArchive. It veriﬁes its

initial inference by applying specialized one-class classiﬁers. The key advantage of

our method is its scalability and low rate of false alarms. Note that our detection

method primarily relies on the real-time mapping (dynamic and/or static) of DGA-

enabled malware domains to their IP addresses. The IP address of Internet-based

(C&C) servers is dynamically obtained from the respective DNS responses.

For the SDN switch, we use a fully Openﬂow 1.3 compliant NoviSwitch 2122

[161] which is controlled by the Ryu SDN controller [162]. The switch provides 240

Gbps of throughput, up to one million TCAM ﬂow-entries, and millions of exact-

match ﬂow-entries in DRAM, and we found it to amply cater to the requirements

of this project. Furthermore, we use a combination of proactive and reactive entries

in the switch ﬂow table. A proactive entry is statically pushed by the controller so

that all DNS response packets (i.e., UDP source port 53) received from the Internet

are forwarded (on port 2) and mirrored (on port 3) to the DGA ﬁnder, as shown

by step 1 in Fig. 4.11.

The ﬁnder looks up the queried domain name against

DGArchive.

If found, it extracts the IP address of the server resolved by DNS

responses and subsequently calls the SDN controller (step 2 ) that results in the

insertion of a reactive ﬂow-entry, shown by step 3 . Note that our malware detection

method is limited by the knowledge-base provided by the DGArchive repository.

This means that to detect new families of DGA-enabled malware ﬂows, whose DNS

query name is outside of this database, one needs to update the DGArchive – it has

been consistently maintained over the past ﬁve years.

One may also want to examine the registration date of domain names before

98

Chapter 4. Automatic Detection of DGA-Enabled Malware

mirroring the traﬃc, given the overlap between some DGA domains and legitimate

domains (discussed in §4.2.1). We believe that examining the registration date of

domains in “real-time” can be challenging since many TCP/UDP ﬂows (between

internal host and external C&C server) often commence shortly (less than 100ms)

after their DNS resolution (will be discussed later in §4.4, Fig. 4.15), and hence the

delay of registration lookup may cause missing the C&C communication ﬂow. In

addition, mirroring “selected ﬂows” which carry a fairly small number of packets

and are relatively short (will be discussed later in §4.4, Fig. 4.16) would not incur

signiﬁcant cost of software processing or switch TCAM entries (will be discussed

later in §4.4, Fig. 4.17).

Reactive rules that match the server’s IP address (two rules per server: one

matching source and one matching the destination IP address) are of the highest

priority and get installed as a consequence of DGA queries detected by the DGA

ﬁnder. To protect the SDN switch from TCAM exhaustion (scalable management of

TCAM usage), reactive rules are automatically timed out after a period equals to the

TTL obtained from their corresponding response. The reactive ﬂow entries provide

ﬁltered packets (to/from potential C&C servers) to the packet processing engine on

port 4 in step 4 . Our packet processing engine (run on a generic server conﬁgured

with Ubuntu version 16.04.4) analyzes suspicious traﬃc ﬁltered and mirrored by

the SDN switch. It constructs ﬂow-level attributes that are fed, in step 5 , to the

machine learning (ML) models for prediction. The models are one-class classiﬁers

(discussed in §4.3.1) distinguishing, step 6 , malicious ﬂows from benign ones. We

will describe in §4.4 the performance of our prototype under real traﬃc of our campus

network. Note that the proposed approach is generic and can be readily applied to

any context, given that specialized ML-models are trained by the malware dataset

of those domains (e.g., cloud-based or IoT-based setups).

99

Chapter 4. Automatic Detection of DGA-Enabled Malware

Table 4.10: Distribution of malware families among suspicious ﬂows, selected and
mirrored by our SDN system.

DGA-enabled malware (# C&C servers) # ﬂows [%]
35941 [63.10%]
ModPack (7)
20806 [36.53%]
Matsnu (2)
169 [0.30%]
Ramnit (2)
37 [0.06%]
Suppobox (8)
4 [0.01%]
Bamital (1)

4.4 Evaluation Results

We have implemented a fully functional system (shown in Fig. 4.11) and operated

it during a 50-day trial (3-Dec-2019 to 21-Jan-2020) under entire campus network

traﬃc. During this trial, our system automatically selected, mirrored, and recorded

suspicious DGA-based ﬂows (after DGA-based DNS responses) in real-time at the

line rate of up to 10 Gbps using reactive SDN rules. In addition to suspicious ﬂows,

the entire DNS traﬃc was recorded during this trial to correlate DGA queries with

their corresponding suspicious ﬂows. In this section, we evaluate the eﬃcacy of our

trained models by applying them (in oﬄine mode) to recorded suspicious ﬂows from

the trial, diagnosing their health (malicious or benign?).

4.4.1 SDN-selected DGA Flows

We perform a correlation between SDN-selected suspicious ﬂows and DNS responses

of DGA families. Table 4.10 summarizes the distribution of suspicious ﬂows (a total

of 56,957) that are associated with (DNS queries of) ﬁve DGA families listed in

rows.

It is seen that ModPack family dominates with 63.10% of ﬂows, followed

by the Matsnu family with 36.53% of ﬂows. Note that these ﬂows are exchanged

between enterprise hosts and a small number (a total of 20) of C&C servers on the

Internet – the number of unique servers (IP addresses) is listed in the bracket in

100

Chapter 4. Automatic Detection of DGA-Enabled Malware

front of their respective DGA family. Furthermore, we observe that the activity

of these servers varies across families. For example, seven servers associated with

the ModPack family handle about 36K ﬂows while 8 Suppobox servers exchange 37

ﬂows over the 50-day period of our experiment.

During our trial, we found that C&C servers of 14 DGA-enabled malware fami-

lies were successfully resolved (476K DNS responses). However, the C&C servers of

only 5 families were contacted by internal hosts, following their DNS resolution. We

observe that those 9 malware families which generated no C&C communications,

contribute to only 0.1% of the total resolved DGA queries. We note that the prob-

ability of communicating with an intended C&C server following a successful DNS

resolution varies across families. For example, the top two families display a com-

pletely diﬀerent pattern. ModPack, which dominates by making 35.9K ﬂows, had

475K DNS queries resolved. Apparently, most of these DNS queries (with an average

TTL value of ≈5 minutes) are made purely to keep their local cache updated by the

latest IP address of their intended server. On the other hand, Matsnu generated

20.8K ﬂows with only 22 DNS responses (with much longer TTL values averaged at

≈5 hours) during our 50-day trial. In this case, DNS queries are only made when cer-

tain communications are desired. Of the remaining three families, Ramnit behaves

similar to Matsnu by exchanging 169 C&C ﬂows with 14 DNS responses (average

TTL ≈4 hours), while Suppobox (37 ﬂows, 97 DNS responses, average TTL ≈1.5

hours) and Bamital (4 ﬂows, 11 DNS responses, average TTL ≈10 minutes) display

a pattern like ModPack.

4.4.2 Diagnosing DGA Flows

Of the total of 56,957 suspicious ﬂows (mirrored by our system during this trial

period), 35645 are HTTP, 19674 are HTTPS, and 1638 are UDP. Recall from the

previous section that we trained three EiF models (HTTP, HTTPS, and UDP), each

101

Chapter 4. Automatic Detection of DGA-Enabled Malware

with their respective malicious ﬂows extracted from the CTU-13 dataset.

Table 4.11 shows the testing results of suspicious ﬂows against their correspond-

ing EiF model. It can be seen that more than 90% of suspicious ﬂows across the

three types (HTTP, HTTPS, and UDP) have been classiﬁed as malicious. These

high detection rates verify the eﬃcacy of our trained models in diagnosing suspicious

DGA ﬂows.

In the absence of ground-truth data whether suspicious ﬂows are indeed mali-

cious or not, we further analyzed these selected ﬂows and their attributes. It is seen

that 99.94% of suspicious HTTP ﬂows are predicted as malicious. We found that a

vast majority of suspicious HTTP ﬂows (33.5K) consist of the only three-way hand-

shake (initiated by internal hosts) followed by a TCP RST (reset) packet sent by

the initiating host. Almost all of these 33.5K HTTP ﬂows are classiﬁed as malicious

– the CTU-13 dataset also had 810 ﬂows (17% of total HTTP ﬂows) of this kind.

Excluding these speciﬁc 33.5K HTTP ﬂows, again a vast majority (98.33%) of the

remaining 2046 suspicious HTTP ﬂows are predicted to be malicious, highlighting

the fact that their traﬃc behavior conforms to the norms of known malware (i.e.,

the CTU-13 dataset). Only 34 HTTP ﬂows are classiﬁed as benign that carry a large

number of packets (about 35 packets) compared to the malware norms (5 packets) –

this corroborates to a great extent with our observations from misclassiﬁed ﬂows dur-

ing model validation, shown in Fig. 4.10a. Similarly, we investigated the attributes

of suspicious HTTPS and UDP ﬂows that are classiﬁed as benign during trial evalu-

ation, and found that benign-predicated: HTTPS ﬂows contain an average of more

than 200 packets, carrying relatively high volume (≈25 KB) of traﬃc (conforming

to Fig. 4.10b); and UDP ﬂows contain 320 to 390 packets, resulting in ﬂow volume

of average 350 KB (conforming to Fig. 4.10c). In summary, suspicious ﬂows which

are classiﬁed as benign are probably misclassiﬁed by the EiF models. This means

that traﬃc ﬂows subsequent to a DGA-based query are likely to be malicious.

102

Chapter 4. Automatic Detection of DGA-Enabled Malware

Table 4.11: Results of testing suspicious ﬂows against their corresponding EiF mod-
els.

# suspicious ﬂows

% malicious

% benign

# internal hosts making suspicious ﬂows

# hosts with all ﬂows malicious

# hosts with malicious and benign ﬂows

# hosts with all ﬂows benign

HTTP HTTPS UDP Aggregate

35645
99.94%
0.06%

19674

1638
93.92% 92.71%
6.08% 7.29%

262

239

17

6

2367

1731

533

103

45

20

25

0

56957
97.63%
2.37%

2488

1818

567

103

Figure 4.12: CCDF of number of suspicious ﬂows per host.

4.4.3

Infected Hosts Initiating Malicious DGA Flows

We have identiﬁed malicious ﬂows succeeding DGA queries, but network operators

are more interested in identifying hosts infected by malware. We mapped all of

the suspicious ﬂows (dynamically selected and mirrored by our system) to their

corresponding hosts (inside the campus network) that initiated those ﬂows. These

hosts are in three categories, as shown by the bottom rows in Table 4.11: (a) hosts

with all of their suspicious ﬂows are predicted as malicious (“pure-malicious”), (b)

hosts with some of their suspicious ﬂows are predicted as malicious, and some as

benign (“mix-malicious-benign”), and (c) hosts with all of their suspicious ﬂows are

predicted as benign (“pure-benign”).

103

100101102103104105Num. suspicious flows per host10-410-310-210-1100CCDF: P( Num. > x )pure maliciousmix malicious/benignpure benignChapter 4. Automatic Detection of DGA-Enabled Malware

Figure 4.13: CCDF of malicious fraction of ﬂows per host in mix-malicious-benign
category.

It can be seen that at the aggregate level, shown in the last column of Table 4.11,

the pure-malicious category dominates with 1818 hosts, followed by mix-malicious-

benign and pure-benign, respectively with 567 and 103 hosts – across the campus

network, there are a total of 2488 internal hosts that generate some suspicious ﬂows

(HTTP and/or HTTP and/or UDP). Note that 302 of these hosts are NAT gateways

(each representing several wireless hosts), and the remaining 2186 are actual end-

hosts (clients or servers which are not NATed).

Focusing on the infected hosts, we found that they belong to 226 diﬀerent subnets

of size /24. Of these subnets, 34% have more than ten infected hosts, and 25%

have more than 20 infected hosts. These insights help network operators who want

to tighten the security posture of certain subnets, those that have some degree of

infection.

Next, we performed reverse lookups to infer the nature of infected hosts, and

found that: 302 hosts are WiFi NAT gateways (from two dedicated subnets) with

names as “ SSID-pat-pool-a-b-c-d.gw.univ-primay-domain” where “ a.b.c.d” is the public IP

address of the NAT gateway, and SSID is the WiFi SSID for the University cam-

pus network; 34 hosts are the Mac devices (spread across 23 subnets) with names

104

0.50.60.70.80.91.0Malicious fraction of flows10-310-210-1100CCDF: P( Frac. > x )Chapter 4. Automatic Detection of DGA-Enabled Malware

(a) Aggregate of all hosts.

(b) Most active infected end-host (top) and a
NAT gateway (bottom).

Figure 4.14: Time trace of active malicious ﬂows (between infected internal hosts
and malware servers) during the 50-day trial for: (a) aggregate of all campus hosts,
and (b) most active infected end-host (top) and a NAT gateway (bottom).

containing strings like “ mbp” or “ imac”; 616 hosts are returned with names including

strings like ‘desktop” that they indicate are a user desktop machine (Windows/Linux);

and the remaining 1433 hosts (spread across 186 subnets) are returned with no name

– they are also typically regular end-hosts.

We plot in Fig. 4.12 the CCDF of suspicious ﬂow count per host within each of the

three categories mentioned above. It can be seen that 99% of pure-benign hosts have

at most two suspicious ﬂows – rarely active. In the other two categories, instead,

far more suspicious ﬂows are observed per host (average of 22 ﬂows) – several hosts

in both of these very active categories have more than 1000 ﬂows. Finally, focusing

on the mix-malicious-benign category, we see that 50% of hosts have more than ten

suspicious ﬂows, represented by the tail of their CCDFs. Also, more than 80% of

hosts in the mix category have more than half of their suspicious ﬂows classiﬁed as

malicious – the CCDF plot in Fig.4.13 particularly illustrates the distribution of the

malicious fraction of ﬂows across all hosts of this category. For these reasons, we

deem the two active categories, consisting of 2385 hosts, to be likely “infected” by

malware.

We plot in Fig. 4.14 the daily time trace of active malicious ﬂows detected during

105

10 Dec18 Dec26 Dec3 Jan11 Jan19 JanTime01K2K3K4K5K6KNum. flows per day10 Dec18 Dec26 Dec3 Jan11 Jan19 Jan01K2K3Kmost active end-host10 Dec18 Dec26 Dec3 Jan11 Jan19 JanTime0250500750NAT gatewayNum. flows per dayChapter 4. Automatic Detection of DGA-Enabled Malware

Figure 4.15: Delay between DGA-based DNS responses and commencement of their
subsequent TCP/UDP ﬂow.

our trial. Each data-point represents the number of active ﬂows over a 24-hour

window. Looking at Fig. 4.14a, we observe that the activity of DGA-enabled C&C

communications across the aggregate of all campus hosts was relatively high during

the ﬁrst half of December (peaking at a total of ≈5500 ﬂows on 12th Dec), gradually

fell reaching to a complete no-show during holiday shutdown periods (between 25-

Dec-2019 and 8-Jan-2020), and afterward revived slowly. This pattern of malware

activity correlates (to a great extent) with the number of active users on the network,

suggesting infected regular hosts. Moving on to Fig. 4.14b, we see malicious ﬂow

activity of two hosts. The top subplot corresponds to the most active infected end-

host that generated a total of 24.8K malicious ﬂows (all preceded by DGA responses

of ModPack family). This host was active at the beginning of the trial (ﬁrst three

weeks in December) by making on average more than 1000 malicious ﬂows per day.

Still, its hyperactivity slowly diminished in the second half of our trial. The bottom

subplot is a NAT gateway that displays a diﬀerent pattern of malware activity. It

made a total of 14 malicious ﬂows (pertinent to Matsnu family) during December,

went silent for three weeks, and then suddenly became heavily active in the middle

of January by making about 700 malicious ﬂows from the Suppobox family. It is

expected to see a diversity of families in the NAT gateway since they make ﬂows on

behalf of a group of end-hosts.

106

020406080100Delay (ms)10-210-1100CCDF: P( delay > x )Chapter 4. Automatic Detection of DGA-Enabled Malware

(a) Packet count.

(b) Duration.

Figure 4.16: CCDF of attributes: (a) packet count, and (b) duration, of suspicious
ﬂows.

We show in Fig. 4.15 the CCDF plot of delay between DGA responses (resolu-

tion of DGA query for C&C server) and the commencement of the ﬁrst associated

TCP/UDP ﬂow. It can be seen that more than 90% ﬂows start in less than 2 ms

(very shortly) after their DNS resolution. Since ﬂow arrival delays do not go beyond

100 ms, it is crucial for our detection system to immediately insert a correspond-

ing reactive rule into the SDN switch, capturing and diagnosing the communication

between the internal host and the external C&C server.

Next, we analyze the size of mirrored traﬃc ﬂows which need to be analyzed in

software – suggesting the computing cost. It can be seen in Fig. 4.16a that reﬂected

suspicious ﬂows often carry a small number of packets – more than 86% of ﬂows

have less than 100 packets. Interestingly, these ﬂows are relatively short – as shown

in Fig. 4.16b more than 90% of ﬂows last less than 2 minutes, hence get timed out

quickly from the switch TCAM table. This measure is essential since TCAM is one

of the precious resources in our system. Considering the time trace of reactive rules

(during our 50-day trial) in Fig. 4.17, we see that no more than 400 entries per day

are installed into the SDN switch by the controller. These metrics collectively serve

as clear evidence of the cost-eﬀectiveness of our solution.

107

05001000150020002500300035004000Packet count10-310-210-1100CCDF: P( count > x )100200300400500Duration (min)10-310-210-1100CCDF: P( duration > x )Chapter 4. Automatic Detection of DGA-Enabled Malware

Figure 4.17: Daily number of reactive ﬂow entries installed by the SDN controller
into the SDN switch, during the 50-day trial.

4.4.4 Comparing Our Diagnosis Models with Zeek IDS

Lastly, we validate our results against logs of an open-source IDS Zeek (formerly

Bro) – which is a powerful network analysis tool, widely used, and consistently

maintained by the community for more than 20 years now [163]. Our main intention

is to compare our ML-based ﬂow-level approach with a decent rule-based packet-level

method in detecting malicious traﬃc.

Zeek performs a packet-based analysis and raises alarms if a known malicious

signature is found in a packet. We replayed the 50-day worth of selected traﬃc (of

suspicious ﬂows) onto Zeek to check how it ﬂags malicious activities. Of a total of

5.5 M packets, Zeek raised alarms for 23.7 K (0.4%). To compare with our ﬂow-level

analysis, we aggregate those packets ﬂagged by Zeek into ﬂows. It turns out 14,455

ﬂows (i.e., 25.3% of suspicious ﬂows), belonging to all of the ﬁve malware families

(Table 4.10), are detected as malicious by Zeek.

We found that all of Zeek ﬂagged ﬂows are a subset of malicious ﬂows classiﬁed

by our models. Starting from the HTTP ﬂows, we found a small overlap (only 1087

ﬂows, about 3%) in the outputs of our HTTP model and Zeek. This is mainly be-

108

10 Dec18 Dec26 Dec3 Jan11 Jan19 JanTime050100150200250300350400total number of reactive flow entries per dayChapter 4. Automatic Detection of DGA-Enabled Malware

cause those 33.5K HTTP ﬂows (94% of total) that only carry three-way handshakes

with a reset do not cause Zeek to raise any alarms when inspecting their packets.

Instead, the overlap was far better for HTTPS ﬂows – Zeek corroborates our HTTPS

model by ﬂagging 13368 ﬂows (≈ 68%) that we classiﬁed as malicious. Lastly, none

of the UDP ﬂows are ﬂagged by Zeek, probably because individual UDP packets did

not display any malicious pattern matching Zeek’s known signatures. Zeek raises

several diﬀerent alert types for malicious HTTP and HTTPS packets – about half

of malicious HTTP and HTTPS receive more than one alert type. The top four

alerts are:

“ above hole data without any acks”, “ bad TCP checksum”, “ possible split routing”,

and “ data before established”.

4.5 Conclusion

We have developed and validated a method for real-time selective mirroring net-

work ﬂows for diagnosis by trained models. We analyzed 75 days worth of DNS

traﬃc (2.4B records), highlighted the prevalence of more than twenty DGA-enabled

malware families across internal hosts, and obtained insights into their behavioral

patterns while communicating with their corresponding C&C server by analysis of

a large PCAP collected during peak hour. We identiﬁed their traﬃc attributes and

trained three one-class classiﬁer models specialized in HTTP, HTTPS, and UDP

protocols using public PCAP traces of known malware families. We then devel-

oped a system that continuously monitors DNS traﬃc and automatically (using

SDN reactive rules) selects and mirror communications between internal hosts and

malware C&C servers pertinent to DGA queries. We then evaluated the eﬃcacy of

our models by testing suspicious traﬃc ﬂows against our trained models, identiﬁed

infected hosts from suspicious ﬂows. Finally, we compared our detection approach

with software IDS Zeek. In the next chapter, we will identify the malicious hosts of

an enterprise network by analyzing non-existent DNS responses.

109

Chapter 5

Learning-Based Detection of

Malicious Hosts by Analyzing

Non-Existent DNS Responses

Contents

6.1 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131

6.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

The previous chapter highlighted the prevalence of DGA-enabled malware in our

campus network and identiﬁed infected hosts by selectively mirroring the suspicious

network traﬃc using SDN. This chapter explores the use of DNS for service disrup-

tion (through random subdomain attack) by analyzing the incoming DNS responses.

Parts of this chapter are under review at IEEE Globecom 2022.

The DNS Water Torture attack (aka Slow Drip or Random Subdomain attack) is

a type of DDoS attack on authoritative DNS servers and/or open resolvers, whereby

the victim is bombarded with random non-existent domains (NXDs) DNS requests,

exhausting their entire resources. A famous example of this attack was launched

by Mirai botnet on Dyn DNS architecture in 2016. Researchers have proposed

110

Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

solutions to detect these attacks; however, they predominantly apply certain static

thresholds to the count of NXD responses. This method can result in high false

positives and needs to be customized to the traﬃc pattern of victim DNS servers,

making it practically challenging for network operators to adopt them at the source

of potential attacks.

This chapter aims to detect possibly infected hosts of a university campus net-

work that take part in this speciﬁc type of DNS-based attacks. Our contributions

are threefold: (1) We analyze 120 days’ worth of DNS traﬃc collected from the

border of a large university campus network to draw insights into the characteristics

of non-existent domain (NXD) responses of incoming DNS packets. We discuss how

malicious NXDs diﬀer from benign ones and highlight two attack scenarios based on

their requested domain names; (2) We develop a method using multi-staged iForest

models to detect malicious internal hosts based on the attributes of their DNS activ-

ity; (3) We evaluate the eﬃcacy of our proposed method by applying it to live DNS

data streams in our university campus network. We show how our models can detect

infected hosts that generate high-volume and low-volume distributed non-existent

DNS queries with more than 99% accuracy of correctly classifying legitimate hosts.

5.1 Introduction

Over the last two decades, there has been tremendous growth in cyber-attacks and

malicious activities exploiting DNS protocol as the number of network devices grows

daily. DNS is a mission-critical service but open by design and rarely monitored

by the ﬁrewall compared to email, FTP, or HTTP. According to a security ﬁrm

[103], in 2017, the cost of damages caused by a DNS attack for a large organization

(3,000+ employees) was estimated as $2.2m. Attack on DNS infrastructure of Dyn

(providing DNS services to big companies such as AirBnB, Spotify, and Twitter)

caused by a malware known as Mirai 2016 is a famous example to explain water

111

Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

torture attack in which thousands of vulnerable IoT devices took part in sending

queries for random domains to the companies whose DNS infrastructure is operated

by Dyn [164]. According to the FBI, the attackers used random subdomain attacks

to target US-based state-level voter registration and information website in 2020

[165].

DNS works in such a way that if a query is being asked from the DNS author-

itative name server or open resolver, it is an obligation on them to answer it even

if the query is non-existent in their ecosystem. Non-existent domains are of two

types: (a) benign: popular search engines and anti-viruses utilize random-looking

domains to convey a one-time signal to their servers known as disposable domains

(e.g., elb.amazonaws.com.cn, cloudfront.net, and avts.mcafee.com). Benign domains may

also contain typo mistakes. For example, a user accidentally writes “ googel.com” in-

stead of “ google.com”; and (b) malicious: launch a type of DDoS attack, DNS water

torture attack [7] also known as random subdomain attack by dynamically generat-

ing random strings as the preﬁx of a victim domain. The DNS Water Torture Attack

is a type of DDoS attack on DNS servers. This attack aﬀects both authoritative

servers and open/recursive resolvers, but mainly it targets the former.

Cyber actors use bots (compromised devices) to send many randomly generated

domain names on their victim servers. The queried domain names relate to the

primary domain that is governed by its authoritative name server to return the IP

address of that particular domain. During the attack, due to the high number of re-

quests, the victim authoritative servers and/or the recursive resolvers may have slow

response to the queries being asked or potentially become unavailable. Although the

problem has been well understood over the last decade, it is mostly dealt with from

the perspective of the victim server by identifying the malicious queries. We see this

as an opportunity to detect potentially infected hosts of an enterprise that initiate

non-existent queries to the outside world. It is important for enterprises to imple-

ment certain cyber hygiene practices that aid in boosting an organization’s overall

112

Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

security posture as well as miniating their reputation on the Internet community by

preventing their internal hosts from attacking others.

Researchers have proposed a range of countermeasures to detect NXD attacks;

however their proposed methods are too simple (threshold-based) and hence fall

short when it comes to sophisticated attacks - with a diﬀerent objective being the

detection of victim primary domains. Instead, we develop a monitoring system at the

source that can detect enterprise hosts that generate high volumes of NXD attacks

and low and distributed NXD attacks.

In this chapter, we make the following three contributions: (1) We analyze 120

days’ worth of DNS traﬃc collected from the border of a large university campus net-

work to highlight and draw insights into the high volume of incoming Non-Existent

Domains (NXD) responses and to identify the diﬀerence between two scenarios of

water torture attack (attack on authoritative DNS server and/or open resolver) ;

(2) Based on the behavioral attributes, we develop a multi-staged iForest model to

classify the internal hosts (those receiving benign NX responses versus those taking

part in water torture attack) and (3) We evaluate the eﬃcacy of our proposed ap-

proach on live DNS data from the network border of a large university campus with

an accuracy of over 99% incorrectly classifying legitimate hosts.

The rest of the chapter is organized as follows: §2 discusses existing literature on

DNS security related to water torture attack. §3 describes the analysis of incoming

NXD responses in our university campus network. We describe our proposed scheme

in §4. We describe the eﬃcacy of our scheme in §5. Lastly, we conclude this Chapter

in §6.

113

Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

Figure 5.1: Daily trace of incoming DNS response with errors (typo mistakes: be-
nign.)

5.2 Analyzing Two Variants of DNS Random Sub-

domain Attacks in Our Campus Network

In this section, we ﬁrst analyze the prevalence of NX domains in our campus network.

We then look at the two variants of water torture attack - the ﬁrst case is when the

victim is an authoritative name server, and the second case is when the victim is an

open/recursive resolver. The study here considers data collected over four months

from 31st Oct 2019 to 28th Feb 2020. Details of our data collection can be found in

§3.2.

Benign incoming responses: Before exploring the anatomy of NXD attacks,

we discuss the two possible cases of incoming benign NXDs responses in an enterprise

network: (i) Typing mistakes, and (ii) Disposable domains used by antivirus tools

(benign data exﬁltration as described in Chapter 3). Fig. 5.1 shows a full day

trace in which a benign host just received 31 NXD responses due to typos in a day.

Similarly, Fig. 5.2 shows a host which received just 3 NXD responses all due to

disposable domains coming from the antivirus tools. Thus, it can be seen that the

benign hosts do not receive a massive number of NXDs.

114

12am     4am     8am     12pm     4pm     8pmTime0246810# incoming NXD responses (per 10-min)Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

Figure 5.2: Daily trace of incoming DNS response with errors (Disposable domains
from Sophos antivirus: benign.)

Illustration of Our Data Collection: Let us understand the anatomy of

water torture attack by taking a closer look at our data from a day. As shown in

Fig. 5.3, we show the visual illustration of our data collection for the incoming NX

responses. As we collect the DNS data from the border router, the identity of some

of the internal hosts gets hidden behind the UNSW recursive resolvers (RRs) and

NAT gateways. Hence the scope of this work is limited to those unhidden internal

hosts receiving NXD responses. Out of 1.4 M incoming NXDs on this day, 700K

responses were destined to 393 hosts. The responses are sourced from either open

resolvers and/or authoritative name servers. In what follows, we will highlight how

internal enterprise hosts behave during attack scenarios.

5.2.1 Attack Scenarios

This section discusses the attack scenarios i.e., attack on the authoritative name

server of the victim domain, and an attack on the open resolver. We ﬁrst plot in

Fig 5.4 the time trace of incoming NXD responses in our dataset. Each data-point

in this plot represents the number of NXD responses over a 6-hour window. We

observe that the typical values of the number of incoming NXDs are less than 500K

115

12am     4am     8am     12pm     4pm     8pmTime012# incoming NXD responses (per 10-min)Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

Figure 5.3: Visual illustration of our data collection and various entities identiﬁed.

Figure 5.4: Timetrace of incoming NX DNS responses.

domains. These domains are mostly benign (either typos or disposable domains).

However, some spikes can be seen in the plot highlighting some abnormal activities.

We further analyzed those days with spikes in the count of incoming NXD responses

and found out the two diﬀerent scenarios of NXD attacks as described in the following

subsection.

116

hH1hH2uH1uH2NATgatewaysNXD Responses400K NXD responsesunhidden hostspublic RRs & auth DNSservers300K NXD responsesHosts hidden behind RRsor NAT gateway..hHn..uH393RRs700K NXD responsesEnterprise NetworkInternetData collection point at the “border” of enterprise network31-Oct15-Nov30-Nov15-Dec30-Dec14-Jan29-Jan13-FebTime0500K1M1.5M2M2.5M3M3.5M# incoming NXD responses (per 6-hour)Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

Figure 5.5: Time trace of an infected host in our dataset - a sudden rise in NXD
responses observed with in an hour only.

Figure 5.6: Zoomed-in time trace of the suspicious host during the hour of interest
(8th January 2020 1:30 - 2:30pm).

5.2.1.1 Attack on Authoritative DNS Servers

Analyzing the spike on 8th January 2020, we found out that most of the incoming

NXD responses (350K) are destined to a regular host. To better understand the

behavior of that host in terms of incoming NXD responses, we plot a time trace

of activity for the entire duration of our dataset.

Interestingly, the host displays

unexpected (suspicious) behavior on 8th January 2020 as shown in Fig. 5.5. We

see no NXD activity other than one massive spike within an hour or two on 8th

117

31-Oct15-Nov30-Nov15-Dec30-Dec14-Jan29-Jan13-FebTime050K100K150K200K250K300K350K400K# incoming NXD responses (per 6-hour)1:30pm     1:40pm     1:50pm     2pm     2:10pm     2:20pmTime05K10K15K20K25K30K# incoming NXD responses (per min)Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

Table 5.1: Sample of FQDNs used by suspicious host.

S.No FQDNs

1

2

3

4

5

6

7

8

9

3rd4.ahrtv.cn

6nq0.ahrtv.cn

3guc.ahrtv.cn

a28.ahrtv.cn

w2l9v.ahrtv.cn

undertake.ahrtv.cn

disposal.ahrtv.cn

mengla.ahrtv.cn

launcher.ahrtv.cn

10

deliberately.ahrtv.cn

January.

Let us zoom into the host activity on a per-minute basis for those particular hours

to see a ﬁne-grained pattern of NXD arrivals (whether spread across hours uniformly

or bursty). As shown in Fig. 5.6, we observe that the number of NXD responses

starts to grow linearly at around 1:45pm, peaks at 26K and sharply becomes zero at

2:15pm. Analyzing their query names revealed that they all target a single primary

domain name ahrtv.cn - this highlights an attack on the corresponding authoritative

name server. Table 5.1 lists some FQDNs found in those queries where some of them

contain random subdomains e.g., “ 3guc.ahrtv.cn”, and contain dictionary words e.g.,

“ disposal.ahrtv.cn” in the subdomain part. In general, we ﬁnd out that the volume

of incoming NXDs from water torture attack is signiﬁcantly much greater than

disposable and benign NXD domains as shown in Fig. 5.1 and 5.2.

We then plotted the CCDF of the total number of occurrences of unique FQDNs

for this suspicious host, which can be seen in Fig. 5.7. In total, we found 350,695

NXD responses from “ ahrtv.cn” out of which 350,581 responses use unique FQDNs

(used just once as shown in Fig. 5.7 - more than 99.99% of FQDNs occurred just

118

Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

Figure 5.7: CCDF of number of occurrences of unique NXDs per FQDN for the
suspicious host.

once, which is a very unusual behavior).

5.2.1.2 Attack on Open Resolvers

We found out another form of NXD attacks in which an open resolver was the target.

After observing unusual spikes during the last week of Nov 2019 (shown in Fig. 5.4),

we focused on 26th Nov 2019 to better analyze the behavior of the involved host as

well as top queried FQDNs and primary domains. We found out that an internal

host of our campus made unusual queries to Google’s public DNS resolver 8.8.8.8 -

the query names were “ shu-Aspire-V3-572” and “ mtrnlab5” and hence not fully qualiﬁed

since they did not conform to a standard structure. We observed that the volume

of those non-standard queries goes to 400K NXDs per hour, highlighting a very

abnormal behavior by this internal host. Fig. 5.8 shows the time trace of incoming

NXD responses for this host in our dataset that shows a heavy NXD activity at the

end of Nov 2019 with peak incoming responses around 2.5M over a 6-hour window,

and another suspicious activity at the start of Feb 2020 with the peak number of

incoming responses crossing 300K per 6-hour.

119

100101102103Num of NXDs per FQDNs10-610-510-410-310-210-1100CCDF: P( Num > x )Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

Figure 5.8: Time trace of an infected host.

(a) H1 targeting an authoritative
server per hour.

(b) H2 targeting a recursive server
per hour.

(c) H3 targeting an authoritative
server per hour.

(d) H1 targeting an authoritative
server per minute.

(e) H2 targeting a recursive server
per minute.

(f) H3 targeting an authoritative
server per minute.

(g) H1 targeting an authoritative
server per second.

(h) H2 targeting a recursive server
per second.

(i) H3 targeting an authoritative
server per second.

Figure 5.9: Time-trace of count of NXD responses for various infected hosts at
various time granularity (per hour, per min and per sec): (a)(d)(g) Infected H1
(b)(e)(h) Infected H2, and (c) (f)(i) Infected H3.

120

31-Oct15-Nov30-Nov15-Dec30-Dec14-Jan29-Jan13-FebTime0500K1M1.5M2M2.5M3M# incoming NXD responses (per 6-hour)12am     4am     8am     12pm     4pm     8pmTime050K100K150K200K250K300K# incoming NXD responses (per hour)12am     4am     8am     12pm     4pm     8pmTime050K100K150K200K250K300K350K400K450K# incoming NXD responses (per hour)12am     4am     8am     12pm     4pm     8pmTime05K10K15K20K25K30K# incoming NXD responses (per hour)12am     4am     8am     12pm     4pm     8pmTime05K10K15K20K25K30K# incoming NXD responses (per min)12am     4am     8am     12pm     4pm     8pmTime01K2K3K4K5K6K7K8K# incoming NXD responses (per min)12am     4am     8am     12pm     4pm     8pmTime0100200300400500600700800# incoming NXD responses (per min)12am     4am     8am     12pm     4pm     8pmTime0100200300400500600# incoming NXD responses (per sec)12am     4am     8am     12pm     4pm     8pmTime0100200300400500600700# incoming NXD responses (per sec)12am     4am     8am     12pm     4pm     8pmTime010203040506070# incoming NXD responses (per sec)Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

5.2.2 Drawback of Using a Threshold for NXDs Attack De-

tection

In this section, we discuss the drawback of using threshold-based attack detection.

The ﬁrst point we want to make here is that various hosts behave diﬀerently on a

campus network. Therefore, setting a threshold for hosts would be challenging, given

the variation in the attack proﬁle. Fig. 5.9 illustrates the incoming NXD responses

during a day for three representative infected hosts at diﬀerent time granularity

(per hour, per minute, and per second). Let us ﬁrst compare the hourly count of

incoming NXDs across these possibly infected hosts. Fig. 5.9a depicts the time-trace

for the infected H1, the host is bursty that only became active at around 1pm in the

day, peaking at 250K incoming NXD responses during a day. Comparing H1 with

another infected host in Fig. 5.9b which is super active during the whole day with

300K incoming NXD responses on average. Based on the above two infected hosts,

one may choose a threshold of more than 200K NXD responses per hour. However,

for our third representative infected host (shown in Fig 5.9c), this threshold will

not be triggered and this infected host (H3) will go undetected. Also, we test the

thresholding method by changing the timescale to a per-minute basis. Fig. 5.9d

provides the time trace of infected host 1 with incoming NXDs from 5K to 26K in

a minute and comparing it with infected host 2 which ranges from 4K to 8K per

minute. We can set a threshold of incoming NXDs to 5K (any hosts receiving more

than 5K NXDs will be ﬂagged as malicious). The infected host 3 will pass from

this threshold criteria undetected as in Fig. 5.9f, we can see that incoming NXDs of

infected host 3 range from 100 to 700 NXDs. Similarly, in Fig. 5.9g, 5.9h, and 5.9i,

we compare the infected hosts on a per-second basis where the infected host 1 and

2 have the peak value of 500 and 600 respectively, whereas the infected host 3 has a

peak incoming NXDs count of 70.

121

Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

Figure 5.10: Overview of our proposed scheme.

5.3 Multi-Staged Machine Learning Architecture

In this section, we present the overall architecture of our method, and then we

discuss the details of our multi-stage machine learning algorithm.

5.3.1 System Design

Fig. 5.10 shows the structure of our detection system. An incoming NXD DNS

response triggers our system. First, we start monitoring the behavior of the internal

host which receives the NXD response. We track the number of NXD responses,

timestamp of NXD responses, and FQDNs of individual NXD responses. Upon

receiving the NXD response destined to the internal host, we start tracking all the

responses to that speciﬁc host to get the ratio of NXD responses versus all the other

response types. We have devised a new mechanism that uses cascaded machine

learning-based models. In stage-1, we use an iForest model to detect whether the

incoming NX response is exﬁltrated or not (the exﬁltrated response represents the

benign disposable domains generated by antivirus tools). We design our system

to detect volumetric NXD attack as well as distributed NXD attack. We devised

two approaches i.e., ﬁne-grained approach and coarse-grained approach. We then

122

is DNS RCode= NXDstart monitoring host (# NXDs,NXD timestamp,NXD FQDNs)start tracking other DNS responses of host (e.g., NoError)NoYesIncoming DNS responsesYesfine-grained  attributes processing (/sec)stage-1 iForest(exfiltrated or not)stage-2 iForest(anomaly)Coarse-grained  attributes processing (/30-sec)stage-2 iForest(anomaly)infected hostYesNoYesinfected hostNobenign hostChapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

process the attributes on a per-second basis (ﬁne-grained) and pass it to the stage 2

iForest model if the model identiﬁed that host as benign, we pass it to our coarse-

grained model to detect the distributed NXD attack over time. For that, we process

the attributes per 30 seconds interval (coarse-grained approach) and pass it to the

iForest model to classify the host as malicious or benign.

Our multi-staged ML is based on “Isolation Forest (iForest)” [117].In the ﬁrst

stage, we pass the FQDNs per host to the model that we used to detect DNS

exﬁltration in Chapter 3. However, our focus here is to detect the non-exﬁltrated

domains. The reason to choose non-exﬁltrated domains is that the water torture

attack queries do not conform to the attributes of exﬁltrated domains. We have

utilized our previously trained ML model from Chapter 3 to extract an attribute

i.e., fraction of non-exﬁltrated domains. We extract eight attributes (from the

query name section of each incoming NX DNS response packet) that collectively

have strong predictive power in determining whether the query name is exﬁltrated

or not (output of stage-1). The attributes include:

• Total count of characters in FQDN

• Total count of characters in sub-domain

• Total count of uppercase characters

• Total count of numerical characters

• Entropy

• Number of labels

• Maximum label length

• Average label length

We compute the fraction of non-exﬁltrated domains for each internal host as an

attribute for our next stage ML-based model.

123

Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

For the second stage, we train two iForest models: (a) one is ﬁne-grained for

detecting volumetric water torture attack (in terms of volume of DNS requests),

and (b) another is coarse-grained for detecting distributed water torture attack.

We consider the attributes discretely on a per-second basis for each host, feeding

the iForest model for the ﬁne-grained model. For the coarse-grained model, we

compute the attributes discretely on 30 seconds basis for each host. Based on the

NXD analysis from our campus network, we deﬁne the following attributes (for each

internal host) to detect the water torture attack in our second stage for both ﬁne

and coarse-grained iForest models (stage-2).

• Ratio of NXD response to other response types.

• Average inter-arrival time between NXD responses.

• Standard deviation of inter-arrival time between NXD responses.

• Fraction of non-exﬁltrated domains.

• Average number of labels in query names of NXD responses.

We will now explain the motivation behind choosing the above attributes for our

ML models. The ratio of NXD response to other response types is the most

important attribute for us to distinguish a malicious host from a benign one. To

explain this, let us take a look at Fig. 5.11 that shows the activity of all hosts of

our campus network on a day. The stackplot depicts that the number of No Error

responses is signiﬁcantly higher than that of NXDs, with the peak value of more

than 100K responses in a minute. In contrast, the NXD responses are less than 10K

for overall hosts. We then plot in Fig. 5.12 the number of DNS responses (per min)

for a suspicious host during that day. We can see that No Error responses peaked

at 2K per minute, whereas NXD responses (shown in red) peaked at 6K. Therefore,

the ratio of NXD to all the other responses becomes much greater than 1 from

12pm till 3am. We identify average and standard deviation inter-arrival time

124

Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

Figure 5.11: Stack plot of number of incoming DNS responses in our campus network
over a day (26th Nov 2019).

Figure 5.12: Stack plot of number of incoming DNS responses of a suspicious host
over a day (26th Nov 2019).

between NXD responses as other two main attributes for classiﬁcation based on

the analysis of benign and suspicious host as discussed in §5.2 where we showed

that benign NXDs are distributed in time and do not occur very often over a day as

opposed to malicious NXDs involved in a water torture attack. Similarly, fraction

125

12pm     4pm     8pm     12am     4am     8amTime020K40K60K80K100K120K140K# DNS Responses (per min)No ErrorNXDothers12pm     4pm     8pm     12am     4am     8amTime02K4K6K8K10K12K# DNS Responses (per min)No ErrorNXDothersChapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

Figure 5.13: CCDF of number of occurrences of NXDs per host.

of non-exﬁltrated domains gives the percentage of domains used other than

disposable domains and average number of labels of NXD responses captures

whether a domain is a disposable domain or a water torture attack domain. We note

that disposable domains are often long as shown in Table 3.2 for sophosxl.net and

contain more than 5 or 6 subdomains whereas the random NXD domains typically

have one subdomain [166].

5.3.2 Model Training

In this subsection, we provide details of our ML model training used in each stage.

Stage-1 Model: For the stage-1 model, we train an iForest model with benign

data from four days of our DNS dataset. For ground truth of benign domains we use

the same Majestic Million list that we used in Chapter 3. We use the same approach

that we used in §3.3 for training & tuning.

Stage-2 Model: To train the stage-2 model, we need a dataset of benign NXDs

that is quite challenging as there is no public dataset from enterprise hosts. There-

126

100101102103104105106Num of NXDs per host10-410-310-210-1100CCDF: P( Num > x )Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

fore, we construct our own dataset. To populate benign domains, we start with 4

days’ worth of NXD responses from our original DNS dataset. Fig. 5.13 shows the

CCDF of the total count of NXD responses per host. Interestingly, more than 55% of

hosts just receive one NXD response. We believed these hosts accidentally mistyped

a domain name, resulting in an NXD response. Therefore, we assumed those hosts

to be benign. Also, if a host just receives NXD responses for disposable domains

(generated by antivirus tools), we assume it as benign too (although the host may or

may not involve in other malicious activities, our primary focus is on NXD attack).

Note that this threshold value can be conﬁgured by the network administrator based

on the requirement for their network.

To achieve our objective of detecting NXD attacks at two levels of granularity

(ﬁne-grained and coarse-grained), we train two iForest models at stage-2, the ﬁrst

model is trained to detect the heavy volume of NXDs based on attributes com-

puted on a per-second basis. Similarly, the second model is trained with attributes

computer on a per 30-sec basis for a distributed NXD attack.

5.4 Performance Evaluation

In this section, we evaluate the eﬃcacy of our scheme by cross-validating and test-

ing the accuracy of the trained models for benign instances and quantifying their

performance on a full campus traﬃc stream.

5.4.1 Performance of Fine-Grained Model

In this subsection, we evaluate the performance of our ﬁne-grained ML model. Table

5.2 shows that benign hosts are classiﬁed as normal with an accuracy of more than

98% for both training and testing with false-positive rates of less than 2%. Our

objective here is to detect anomalous hosts when the models are applied to remaining

127

Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

Table 5.2: Anomaly detection by ﬁne-grained model.

Input

Output Days 1-4 Days 5-7

Benign hosts

Remaining hosts

normal

anomalous

normal

anomalous

99.6%
0.4%
94.1%
5.1%

98.5%
1.5%
93.3%
6.7%

Table 5.3: Anomaly detection by coarse-grained model.

Input

Output Days 1-4 Days 5-7

Benign hosts

Remaining hosts

normal

anomalous

normal

anomalous

99.4%
0.6%
90.6%
9.4%

98.1%
1.9%
90.1%
9.9%

hosts. We ﬁnd that 5.1% of the hosts are classiﬁed as anomalous. Further analysis

revealed that this model captures all the heavy volume NXD attacks with some false

positives. Some of the hosts classiﬁed as malicious are NAT gateways and recursive

resolvers due to their heavy activity of receiving NXD responses (actual end hosts

are hidden behind NAT gateways and resolvers, and hence, the detection of those

actual end hosts in this case is beyond the scope of this work).

5.4.2 Performance of Coarse-Grained Model

We next evaluate our coarse-grained model, which was trained by the iForest algo-

rithm. Table 5.3 summarizes the results. It can be seen that trained internal hosts

are correctly classiﬁed as benign with an accuracy of more than 99% during valida-

tion. Similarly, the benign hosts are correctly classiﬁed as benign with an accuracy

of 98% with a false-positive rate of less than 2%. These results are similar to those of

the ﬁne-grained model. However, our intent of using a coarse-grained model was to

detect distributed NX attacks. We can see here for the remaining hosts, the number

of hosts classiﬁed as normal is decreased drastically to 90%, whereas the anomalous

hosts are nearly 10%. When we analyzed the hosts classiﬁed as anomalous, we found

out that some hosts were taking part in water torture attack with a very low volume

of NXDs, but the requests were distributed in time.

128

Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

5.4.3 Discussion

To better understand these ﬁndings, we have further analyzed hosts that are detected

as anomalous in “remaining hosts” category. First, we look at the anomalous hosts for

our ﬁne-grained model. In total, we have 45 unique anomalous hosts. By analyzing

these IP addresses, we found (by reverse lookup) that 8 of them are NAT gateways

(the actual number of hosts are hidden behind these NAT gateways). Other 37 are

all regular end hosts coming from 5 diﬀerent subnets of size /24 of our university

campus.

Interestingly, out of these ﬁve subnets, 18 are from the same subnet,

indicating that this particular subnet might be infected by malware.

Moreover, we compare these numbers with the results of our coarse-grained

model. We found out that the anomalous hosts after adding our coarse-grained

model increased to 87, which shows that it also ﬂagged hosts involved in distributed

random subdomain attacks. Furthermore, out of these 87 unique anomalous hosts,

by reverse lookup, we found out that 11 of them are from NAT gateways and the

remaining 76 belong to regular end hosts coming from 9 diﬀerent subnet of size /24.

Out of those 76 hosts, 26 hosts fall under one subnet during the entire week. Upon

investigation, we found out the subnet is the same that our ﬁne-grained model ﬂags.

This shows that some hosts are possibly involved in high volume random subdomain

attack while other involved in distributed low-volume random subdomain attacks.

5.5 Conclusion

Enterprise networks are a potential target of cyber-attackers, speciﬁcally those ex-

ploiting DNS to perform various attacks. We have developed a multi-stage machine

learning-based solution to detect NXD attacks. First, by analyzing incoming NXD

responses from DNS traﬃc of our campus network, we identiﬁed the diﬀerence be-

tween two scenarios of random subdomain attacks on authoritative DNS servers and

129

Chapter 5. Learning-Based Detection of Malicious Hosts by Analyzing
Non-Existent DNS Responses

open resolver. We developed a method using multi-staged iForest models to classify

the malicious internal hosts that take part in water torture attack based on their

behavioral attributes. Lastly, we evaluated the eﬃcacy of our proposed approach on

live DNS data from the network border of a large university campus.

130

Chapter 6

Conclusions and Future Work

6.1 Conclusions

Enterprise networks are under enormous threat to cyber attacks. DNS is an essential

protocol used by every device connected with the network to map the domain name

with an IP address and vice versa. Unfortunately, attackers use DNS as a covert

channel as it is hardly policed by most enterprise networks’ ﬁrewalls and IDS sys-

tems. Existing mechanisms to detect the malicious activities in the network mostly

contain a knowledge-based approach that is fruitful for the known attacks but fails

when detecting zero-day attacks. This thesis studied three DNS-based cyber-attacks,

speciﬁcally data exﬁltration, DGA-based malware C&C communication, and service

disruption through NXDs. We analyzed the network traﬃc of our campus network

and a Government research organization over six months and showed the prevalence

of these attacks, devised the ML-based models to detect them, and evaluated the

eﬃcacy of our detection mechanism in the wild.

We summarize the essential contributions of this thesis towards achieving DNS

security.

131

Chapter 6. Conclusions and Future Work

• In our ﬁrst contribution, we tackled data exﬁltration using DNS. We analyzed

outgoing DNS queries and identiﬁed stateless attributes such as the number

of characters, the number of labels, and the entropy of the domain name

to distinguish malicious data exﬁltration queries from legitimate ones. We

trained our machines using ground-truth obtained from a public list of top

10K legitimate domains. We then empirically validated and tuned our models

to achieve over 98% accuracy in correctly distinguish legitimate DNS queries

from malicious queries.

• Our second contribution tackled malware C&C communication using DNS.

We analyzed DNS outgoing queries to identify more than twenty families of

DGA-enabled malware when communicating with their C&C servers. We have

identiﬁed the characteristics of malicious network traﬃc and have trained three

protocol-speciﬁc one-class classiﬁcation models for HTTP, HTTPS, and UDP

ﬂows using public packet traces of known malware.

In addition, we have

developed a monitoring system to automatically and selectively reﬂect TCP

/ UDP ﬂows related to DGA queries for diagnosis from trained models. We

deployed our system in the ﬁeld and evaluated its performance to show that it

had potentially infected more than 2,000 internal assets, causing over a million

suspicious ﬂows, 97% of which were oﬀ-the-shelf intrusion detection systems.

• For the third contribution, we studied the use of DNS for service disruption.

We analyzed incoming DNS messages to distinguish between benign and ma-

licious NXD, focusing on NXD DNS responses. We emphasized two attack

scenarios based on their requested domain name. We developed multi-staged

iForest classiﬁcation models at various time windows (such as per second and

30 seconds) to detect internal hosts launching a service disruption attack using

the internal host’s NXD behavior attributes. We showed our models were able

to capture high-volume and low-volume NXD based attacks with an accuracy

of 99% while classifying legitimate hosts.

132

Chapter 6. Conclusions and Future Work

6.2 Future Work

Our work is a signiﬁcant milestone in achieving a practical solution for the DNS

security of enterprise networks. However, our proposed methods can be further

improved and extended to make way for exciting future breakthroughs. We outlined

a few of them below.

• As seen in Chapter 3, we employed stateless attributes of the domain name

to detect the data exﬁltration. We drew insights into the practical considera-

tions of using our detection scheme. This scheme can be extended by collect-

ing states from the DNS traﬃc only for those hosts that generate anomalous

queries and ultimately mitigate malicious DNS tunneling/exﬁltration.

• In Chapter 4, we focused on detecting the infected hosts by selectively mirror-

ing the C&C communication based on a well-known DGA domains database

“DGArchive” containing more than 80 DGA malware families. Although we

can add new DGA-enabled malware domains in the database, the process is

still limited to the known malware. The module of the DGA query ﬁnder can

be replaced by detecting the DGA domains instead of comparing with just

the database to capture all the infected hosts in the enterprise network and

improve the overall eﬃcacy.

• We demonstrated in Chapter 5 how analysis of incoming NXD responses could

help identify infected hosts that launch volumetric attacks on Internet-based

servers. However, we observed a lack of research in identifying the infected

hosts generating the random subdomain attack that focuses on identifying

the victim, such as authoritative name server and recursive resolver. Thus,

although our study provides reasonable accuracy by identifying the source of

random subdomain attacks, we still believe this can be further enhanced by

using ML and deep learning-based models.

133

Chapter 6. Conclusions and Future Work

• Another future direction that can be explored includes the use of encrypted

DNS such as DNSSEC [9], DNS over TLS (DoT) [10] and DNS over HTTPS

(DoH) [11]. Other researchers can use our ML detection models and encrypted

DNS methods to enhance the security of existing DNS architecture, keeping

in mind the increased overhead.

We hope other researchers will explore the future directions identiﬁed above.

134

document

[1] S. Kathuria. (2015). “DNS Firewall is not a Next Generation Firewall.” Ac-
cessed on 10.06.2019, [Online]. Available: https://bit.ly/1NbDIRp.

[2] E. Brumaghin et al. (2017). “Covert Channels and Poor Decisions: The Tale
of DNSMessenger.” Accessed on 10.06.2019, [Online]. Available: https : / /
bit.ly/2R6OSEM.

[3] MIT Internet Policy Research Initiative. (2019). “Why Botnets Persist: De-
signing Eﬀective Technical and Policy Interventions.” Accessed on 15.04.2020,
[Online]. Available: https : / / internetpolicy . mit . edu / wp - content /
uploads/2019/09/publications-ipri-2019-02.pdf.

[4] M. Antonakakis, R. Perdisci, Y. Nadji, et al., “From throw-away traﬃc to
bots: Detecting the rise of dga-based malware.,” in USENIX security sympo-
sium, vol. 12, 2012.

[5] D. Plohmann et al., “A comprehensive measurement study of domain gener-

ating malware,” in Proc. USENIX Security, 2016, pp. 263–278.

[6] D. Plohmann. (2020). “DGArchive.” Accessed on 15.01.2020, [Online]. Avail-

able: https://dgarchive.caad.fkie.fraunhofer.de/.

[7]

[8]

(2014). “Water Torture: A Slow Drip DNS DDoS Attack.” Accessed on
20.03.2021, [Online]. Available: https://bit.ly/3291xdF.

(2021). “New DNS vulnerabilities have the potential to impact millions
of devices.” Accessed on 01.10.2021, [Online]. Available: https : / / www .
helpnetsecurity.com/2021/04/13/dns-vulnerabilities/.

[9] R. Arends, R. Austein, M. Larson, D. Massey, and S. Rose, “Dns security
introduction and requirements,” RFC 4033 (Proposed Standard), Tech. Rep.,
2005.

[10] Z. Hu, L. Zhu, J. Heidemann, et al., Speciﬁcation for DNS over Transport
Layer Security (TLS), RFC 7858, May 2016. doi: 10.17487/RFC7858. [On-
line]. Available: https://rfc-editor.org/rfc/rfc7858.txt.

135

document

[11] P. E. Hoﬀman and P. McManus, DNS Queries over HTTPS (DoH), RFC
8484, Oct. 2018. doi: 10.17487/RFC8484. [Online]. Available: https://rfc-
editor.org/rfc/rfc8484.txt.

[12] A. Nisenoﬀ, N. Feamster, M. A. Hoofnagle, and S. Zink, “User expectations

and understanding of encrypted dns settings,”

[13] S. Singanamalla, S. Chunhapanya, M. Vavruvsa, et al., “Oblivious dns over
https (odoh): A practical privacy enhancement to dns,” arXiv preprint
arXiv:2011.10121, 2020.

[14] J. Mirkovic and P. Reiher, “A taxonomy of ddos attack and ddos defense
mechanisms,” ACM SIGCOMM Computer Communication Review, vol. 34,
no. 2, pp. 39–53, 2004.

[15] E. Cooke, F. Jahanian, and D. McPherson, “The zombie roundup: Under-
standing, detecting, and disrupting botnets.,” SRUTI, vol. 5, pp. 6–6, 2005.

[16] F. C. Freiling, T. Holz, and G. Wicherski, “Botnet tracking: Exploring a root-
cause methodology to prevent distributed denial-of-service attacks,” in Euro-
pean Symposium on Research in Computer Security, Springer, 2005, pp. 319–
335.

[17] M. Abu Rajab, J. Zarfoss, F. Monrose, and A. Terzis, “A multifaceted ap-
proach to understanding the botnet phenomenon,” in Proceedings of the 6th
ACM SIGCOMM conference on Internet measurement, ACM, 2006, pp. 41–
52.

[18] A. Ramdas and R. Muthukrishnan, “A survey on dns security issues and mit-
igation techniques,” in 2019 International Conference on Intelligent Comput-
ing and Control Systems (ICCS), IEEE, 2019, pp. 781–784.

[19] Y. Afek, A. Bremler-Barr, and L. Shaﬁr, “Nxnsattack: Recursive dns ineﬃ-
ciencies and vulnerabilities,” in Proc. USENIX Security Symposium ), 2020,
pp. 631–648.

[20] T. Rozekrans, M. Mekking, and J. de Koning, “Defending against dns re-
ﬂection ampliﬁcation attacks,” University of Amsterdam System & Network
Engineering RP1, 2013.

[21] M. Roesch et al., “Snort: Lightweight intrusion detection for networks.,” in

Lisa, vol. 99, 1999, pp. 229–238.

[22] V. Paxson, “Bro: A system for detecting network intruders in real-time,”

Computer networks, vol. 31, no. 23-24, pp. 2435–2463, 1999.

[23] A. L. Buczak and E. Guven, “A survey of data mining and machine learn-
ing methods for cyber security intrusion detection,” IEEE Communications
Surveys & Tutorials, vol. 18, no. 2, pp. 1153–1176, 2016.

136

document

[24] L. Cui, F. R. Yu, and Q. Yan, “When big data meets software-deﬁned net-
working: Sdn for big data and big data for sdn,” IEEE network, vol. 30, no. 1,
pp. 58–65, 2016.

[25] F. Tegeler, X. Fu, G. Vigna, and C. Kruegel, “Botﬁnder: Finding bots in
network traﬃc without deep packet inspection,” in Proceedings of the 8th in-
ternational conference on Emerging networking experiments and technologies,
ACM, 2012, pp. 349–360.

[26] H. G. Kayacik, A. N. Zincir-Heywood, and M. I. Heywood, “Selecting fea-
tures for intrusion detection: A feature relevance analysis on kdd 99 intrusion
detection datasets,” in Proceedings of the third annual conference on privacy,
security and trust, 2005.

[27] A. K. Shrivas and A. K. Dewangan, “An ensemble model for classiﬁcation of
attacks with feature selection based on kdd99 and nsl-kdd data set,” Inter-
national Journal of Computer Applications, vol. 99, no. 15, 2014.

[28] S. Seufert and D. OBrien, “Machine learning for automatic defence against
distributed denial of service attacks,” in Communications, 2007, IEEE Inter-
national Conference on, IEEE, 2007, pp. 1217–1222.

[29] A. P. Felt, R. Barnes, A. King, et al., “Measuring https adoption on the web,”

in 26th USENIX Security Symposium, 2017, pp. 1323–1338.

[30] K. Giotis, C. Argyropoulos, G. Androulidakis, D. Kalogeras, and V. Maglaris,
“Combining openﬂow and sﬂow for an eﬀective and scalable anomaly detec-
tion and mitigation mechanism on sdn environments,” Computer Networks,
vol. 62, pp. 122–136, 2014.

[31] D. M. Farid, N. Harbi, and M. Z. Rahman, “Combining naive bayes and de-
cision tree for adaptive intrusion detection,” arXiv preprint arXiv:1005.4496,
2010.

[32] L. Koc, T. A. Mazzuchi, and S. Sarkani, “A network intrusion detection sys-
tem based on a hidden naive bayes multiclass classiﬁer,” Expert Systems with
Applications, vol. 39, no. 18, pp. 13 492–13 500, 2012.

[33] J. Shun and H. A. Malki, “Network intrusion detection system using neural
networks,” in Natural Computation, 2008. ICNC., IEEE, vol. 5, 2008, pp. 242–
246.

[34] G. Poojitha, K. N. Kumar, and P. J. Reddy, “Intrusion detection using arti-
ﬁcial neural network,” in Computing Communication and Networking Tech-
nologies (ICCCNT), 2010 International Conference on, IEEE, 2010, pp. 1–
7.

[35]

eﬃcientIP. (2021).
on 01.10.2021,
resources/idc-dns-threat-report-2021.

“IDC 2021 Global DNS Threat Report.” Accessed
[Online]. Available: https : / / www . efficientip . com /

137

document

[36] P. Rascagneres. (). “New FrameworkPOS variant exﬁltrates data via DNS
requests,” [Online]. Available: https : / / www . gdatasoftware . com / blog /
2014/10/23942- new- frameworkpos- variant- exfiltrates- data- via-
dns-requests.

[37] B. Krebs. (2014). “Deconstructing the 2014 Sally Beauty Breach.” Accessed

on 10.06.2019, [Online]. Available: https://bit.ly/1FTBvKZ.

[38] C. Lynch et al. (2016). “MULTIGRAIN – Point of Sale Attackers Make an
Unhealthy Addition to the Pantry.” Accessed on 10.06.2019, [Online]. Avail-
able: https://bit.ly/1pgvJxn.

[39] R. J. Dietrich. (2011). “Feederbot - a bot using dns as carrier for its cnc.”
Accessed on 10.06.2019, [Online]. Available: https://bit.ly/30ZZ3MN.

[40] C. J. Dietrich, C. Rossow, F. C. Freiling, et al., “On botnets that use dns
for command and control,” in Computer Network Defense (EC2ND), 2011
Seventh European Conference on, IEEE, 2011, pp. 9–16.

[41] C. Mullaney. (2011). “Morto worm sets a (DNS) record.” Accessed on

10.06.2019, [Online]. Available: https://symc.ly/2JXKfZS.

[42] T. Spring. (2016). “Wekby apt gang using DNS tunneling for command and
control.” Accessed on 10.06.2019, [Online]. Available: https : / / bit . ly /
1ONTN1s.

[43] S. Khandelwal. (). “New Fileless Malware Uses DNS Queries To Receive Pow-
erShell Commands,” [Online]. Available: https : / / thehackernews . com /
2017/03/powershell-dns-malware.html.

[44] J. Ahmed et al., “Real-Time Detection of DNS Exﬁltration and Tunneling
from Enterprise Networks,” in Proc. Integrated Network and Service Manage-
ment (IM), Washington, DC, USA, Apr. 2019, pp. 649–653.

[45] M. Lyu et al., “Mapping an Enterprise Network by Analyzing DNS Traﬃc,”
in Proc. Passive and Active Measurement (PAM), Puerto Varas, Chile, Mar.
2019, pp. 129–144.

[46] S. Zander et al., “A Survey of Covert Channels and Countermeasures in
Computer Network Protocols,” IEEE Communications Surveys & Tutorials,
vol. 9, no. 3, pp. 44–57, 2007.

[47] Y. Zhauniarovich et al., “A Survey on Malicious Domains Detection Through
DNS Data Analysis,” ACM Computing Surveys (CSUR), vol. 51, no. 4, p. 67,
2018.

[48] M. Antonakakis, R. Perdisci, W. Lee, N. Vasiloglou, and D. Dagon, “De-
tecting malware domains at the upper dns hierarchy.,” in USENIX security
symposium, vol. 11, 2011, pp. 1–16.

138

document

[49] S. Hao, N. Feamster, and R. Pandrangi, “An internet-wide view into dns
lookup patterns,” VeriSign Labs, School of Computer Science, Georgia Tech,
2010.

[50] L. Bilge, E. Kirda, C. Kruegel, and M. Balduzzi, “Exposure: Finding malicious

domains using passive dns analysis.,” in Ndss, 2011.

[51] M. Antonakakis, R. Perdisci, D. Dagon, W. Lee, and N. Feamster, “Building a
dynamic reputation system for dns,” in Proc. USENIX Security, Washington,
DC, Aug. 2010, isbn: 888-7-6666-5555-4.

[52] S. Hao, N. Feamster, and R. Pandrangi, “Monitoring the initial dns behavior
of malicious domains,” in Proc. ACM IMC, Berlin, Germany, Oct. 2011.

[53] M. Mowbray and J. Hagen, “Finding domain-generation algorithms by look-
ing at length distribution,” in Proc. IEEE software reliability engineering
workshops, 2014, pp. 395–400.

[54] M. Feily et al., “A survey of botnet and botnet detection,” in Proc. ACM

ESIST, Athens, Glyfada, Greece, Jun. 2009, pp. 268–273.

[55] M. Khonji et al., “Phishing detection: a literature survey,” IEEE Communi-

cations Surveys & Tutorials, vol. 15, no. 4, pp. 2091–2121, 2013.

[56] V. Paxson et al., “Practical Comprehensive Bounds on Surreptitious Com-
munication over DNS.,” in Proc. USENIX Security Symposium, Washington,
DC, USA, Aug. 2013, pp. 17–32.

[57] E. Cambiaso et al., “Feature transformation and Mutual Information for DNS
tunneling analysis,” in Proc. IEEE Ubiquitous and Future Networks (ICUFN),
Vienna, Austria, Mar. 2016, pp. 957–959.

[58] K. Born et al., “NgViz: Detecting DNS Tunnels Through N-gram Visualiza-
tion and Quantitative Analysis,” in Proc. ACM Cyber Security and Informa-
tion Intelligence Research Workshop(CSIIRW), Oak Ridge, Tennessee, USA,
Apr. 2010.

[59] K. Born et al., “Detecting DNS Tunnels using Character Frequency Analysis,”

in Proc. Annual Security Conference, Las Vegas, USA, Apr. 2010.

[60] C. Qi et al., “A Bigram based Real Time DNS Tunnel Detection Approach,”

Procedia Computer Science, vol. 17, pp. 852–860, 2013.

[61] J. Liu et al., “Detecting DNS Tunnel through Binary-Classiﬁcation Based
on Behavior Features,” in Proc. IEEE Trustcom/BigDataSE/ICESS, 2017,
pp. 339–346.

[62] A. Das, M.-Y. Shen, M. Shashanka, and J. Wang, “Detection of exﬁltration
and tunneling over dns,” in Machine Learning and Applications (ICMLA),
2017 16th IEEE International Conference on, IEEE, 2017, pp. 737–742.

139

document

[63] A. L. Buczak et al., “Detection of tunnels in PCAP data by random forests,”

in Proc. ACM CISRC, Oak Ridge, TN, USA, Apr. 2016, pp. 1–4.

[64] S. Schuppen et al., “FANCI: Feature-based Automated NXDomain Classiﬁ-

cation and Intelligence,” in Proc. USENIX Security, 2018, pp. 1165–1181.

[65] P. Engelstad et al., “Detection of DNS tunneling in mobile networks using
machine learning,” in Proc. Information Science and Applications, Springer,
2017, pp. 221–230.

[66] A. Nadler, A. Aminov, and A. Shabtai,

“Detection of malicious and
low throughput data exﬁltration over
the DNS protocol,” CoRR,
vol. abs/1709.08395, 2017. arXiv: 1709 . 08395. [Online]. Available: http :
//arxiv.org/abs/1709.08395.

[67] A. Darem, J. Abawajy, A. Makkar, A. Alhashmi, and S. Alanazi, “Visualiza-
tion and deep-learning-based malware variant detection using OpCode-level
features,” Future Generation Computer Systems, vol. 125, pp. 314–323, 2021.

[68] H.-h. Wang, L. Yu, S.-w. Tian, Y.-f. Peng, and X.-j. Pei, “Bidirectional LSTM
Malicious webpages detection algorithm based on convolutional neural net-
work and independent recurrent neural network,” Applied Intelligence, vol. 49,
no. 8, pp. 3016–3026, 2019.

[69] J. Zhang, L. Yang, S. Yu, and J. Ma, “A DNS tunneling detection method
based on deep learning models to prevent data exﬁltration,” in International
Conference on Network and System Security, Springer, 2019, pp. 520–535.

[70]

“DNS Covert Channel Detection Method Using the LSTM Model,” Comput-
ers & Security, p. 102 095, 2021, issn: 0167-4048. doi: https://doi.org/
10.1016/j.cose.2020.102095.

[71] N. C. Luong, D. T. Hoang, S. Gong, et al., “Applications of deep reinforce-
ment learning in communications and networking: A survey,” IEEE Commu-
nications Surveys & Tutorials, vol. 21, no. 4, pp. 3133–3174, 2019.

[72] A. R. Mohammed, S. A. Mohammed, and S. Shirmohammadi, “Machine
learning and deep learning based traﬃc classiﬁcation and prediction in soft-
ware deﬁned networking,” in 2019 IEEE International Symposium on Mea-
surements & Networking (M&N), IEEE, 2019, pp. 1–6.

[73] T.-F. Yen and M. K. Reiter, “Traﬃc aggregation for malware detection,”
in International Conference on Detection of Intrusions and Malware, and
Vulnerability Assessment, Springer, 2008, pp. 207–227.

[74] B. Anderson et al., “Identifying encrypted malware traﬃc with contextual

ﬂow data,” in Proc. ACM AISec, 2016, pp. 35–46.

[75] B. Anderson et al., “Machine learning for encrypted malware traﬃc classi-
ﬁcation: Accounting for noisy labels and non-stationarity,” in Proc. ACM
SIGKDD, Halifax NS Canada, Aug. 2017, pp. 1723–1732.

140

document

[76] S. Yadav et al., “Detecting algorithmically generated domain-ﬂux attacks
with dns traﬃc analysis,” IEEE/ACM ToN, vol. 20, no. 5, pp. 1663–1677,
2012.

[77] S. Talukder, “Tools and techniques for malware detection and analysis,” arXiv

preprint arXiv:2002.06819, 2020.

[78] S. Yadav, A. K. K. Reddy, A. N. Reddy, and S. Ranjan, “Detecting algorith-
mically generated malicious domain names,” in Proceedings of the 10th ACM
SIGCOMM conference on Internet measurement, 2010, pp. 48–61.

[79] S. Garcia et al., “An empirical comparison of botnet detection methods,”

computers & security, vol. 45, pp. 100–123, 2014.

[80] H. S. Anderson, J. Woodbridge, and B. Filar, “Deepdga adversarially-tuned
domain generation and detection,” in Proceedings of the 2016 ACM Workshop
on Artiﬁcial Intelligence and Security, 2016, pp. 13–21.

[81] R. Vinayakumar, K. Soman, and P. Poornachandran, “Detecting malicious
domain names using deep learning approaches at scale,” Journal of Intelligent
and Fuzzy Systems, vol. 34, no. 3, pp. 1355–1367, 2018.

[82] G. Marin et al., “Deepmal–deep learning models for malware traﬃc detection

and classiﬁcation,” arXiv preprint arXiv:2003.04079, 2020.

[83] C. Choudhary, R. Sivaguru, M. Pereira, et al., “Algorithmically generated
domain detection and malware family classiﬁcation,” in International Sympo-
sium on Security in Computing and Communication, Springer, 2018, pp. 640–
655.

[84] J. Spooren, D. Preuveneers, L. Desmet, P. Janssen, and W. Joosen, “De-
tection of algorithmically generated domain names used by botnets: A dual
arms race,” in Proceedings of the 34th ACM/SIGAPP Symposium on Applied
Computing, 2019, pp. 1916–1923.

[85] G. Gu, P. A. Porras, V. Yegneswaran, M. W. Fong, and W. Lee, “Bothunter:

Detecting malware infection through ids-driven dialog correlation..”

[86] G. Gu, R. Perdisci, J. Zhang, W. Lee, et al., “Botminer: Clustering analysis
of network traﬃc for protocol-and structure-independent botnet detection.,”
in USENIX security symposium, vol. 5, 2008, pp. 139–154.

[87] G. Gu, J. Zhang, and W. Lee, “Botsniﬀer: Detecting botnet command and

control channels in network traﬃc,” 2008.

[88] A. Gupta et al., “Sonata: Query-driven streaming network telemetry,” in Proc.
ACM Special Interest Group on Data Communication, Budapest, Hungary,
Aug. 2018, pp. 357–371.

[89] Y. Afek et al., “Network anti-spooﬁng with sdn data plane,” in Proc. IEEE

INFOCOM, Atlanta, GA, USA, 2017, pp. 1–9.

141

document

[90] S. K. Fayaz et al., “Bohatei: Flexible and elastic ddos defense,” in Proc.
USENIX Security, Washington, D.C, USA, Aug. 2015, pp. 817–832.

[91] M. Zhang et al., “Poseidon: Mitigating volumetric ddos attacks with pro-
grammable switches,” in Proc. USENIX NDSS, San Diego, California, USA,
Feb. 2020, pp. 1–18.

[92] S. Schiavoni et al., “Phoenix: Dga-based botnet tracking and intelligence,” in

Proc. Springer DIMVA, 2014, pp. 192–211.

[93] J. Ahmed, H. H. Gharakheili, Q. Raza, C. Russell, and V. Sivaraman, “Mon-
itoring enterprise dns queries for detecting data exﬁltration from internal
hosts,” IEEE Transactions on Network and Service Management, 2019.

[94] J. Ahmed et al., “A Tool to Detect and Visualize Malicious DNS Queries for

Enterprise Networks,” in Proc. IFIP/IEEE IM, IEEE, 2019, pp. 729–730.

[95] A. McDole et al., “Analyzing cnn based behavioural malware detection tech-

niques on cloud iaas,” arXiv preprint arXiv:2002.06383, 2020.

[96] R. Sivaguru et al., “Inline detection of dga domains using side information,”

arXiv preprint arXiv:2003.05703, 2020.

[97] J. M. Ceron et al., “Mars: An sdn-based malware analysis solution,” in 2016

IEEE ISCC, IEEE, 2016, pp. 525–530.

[98] Y. Kazato, K. Fukuda, and T. Sugawara, “Towards classiﬁcation of dns er-
roneous queries,” in Proceedings of the 9th Asian Internet Engineering Con-
ference, Chiang Mai, Thailand: Association for Computing Machinery, 2013,
pp. 25–32, isbn: 9781450324519. doi: 10.1145/2534142.2534146. [Online].
Available: https://doi.org/10.1145/2534142.2534146.

[99] Y. Takeuchi, T. Yoshida, R. Kobayashi, M. Kato, and H. Kishimoto, “Detec-
tion of the dns water torture attack by analyzing features of the subdomain
name,” Journal of Information Processing, vol. 24, no. 5, pp. 793–801, 2016.

[100] R. Alonso, R. Monroy, and L. A. Trejo, “Mining ip to domain name interac-
tions to detect dns ﬂood attacks on recursive dns servers,” Sensors, vol. 16,
no. 8, p. 1311, 2016.

[101] R. Sommer and V. Paxson, “Outside the Closed World: On Using Machine
Learning for Network Intrusion Detection,” in Proc IEEE Security and Pri-
vacy, Berkeley, CA, USA, May 2010, pp. 305–316.

[102]

I. Homem et al., “Harnessing predictive models for assisting network forensic
investigations of DNS tunnels,” in Proc. ADFSL Digital Forensics, Security
and Law, 2017, p. 79.

[103] Eﬃcient iP. (2017).

“The Global DNS Threat Survey.” Accessed on

02.02.2021, [Online]. Available: https://bit.ly/2HWYZ9k.

142

document

[104] A. Greenberg. (2014). “DNS attacks putting organizations at risk, survey
[Online]. Available: https : / / bit . ly /

ﬁnds.” Accessed on 10.06.2019,
2X7Az5Z.

[105]

a. vault alien. (2015). “BernhardPOS - New POS Malware.” Accessed on
10.06.2019, [Online]. Available: https://bit.ly/31kVKzZ.

[106] A. Shulmin et al. (2017). “Use of DNS Tunneling for C&C Communications.”
Accessed on 10.06.2019, [Online]. Available: https://bit.ly/2SwS1KY.

[107] R. Neumann et al. (2018). “UDPoS – exﬁltrating credit card data via DNS.”
Accessed on 10.06.2019, [Online]. Available: https://bit.ly/2wLapq4.

[108] T. M. Million. (2018). “Top 1 million website in the world,” [Online]. Avail-

able: http://downloads.majesticseo.com/majestic%5C_million.csv.

[109] W. Rweyemamu et al., “Clustering and the Weekend Eﬀect: Recommenda-
tions for the Use of Top Domain Lists in Security Research,” in Proc. Passive
and Active Measurement (PAM), Puerto Varas, Chile, Mar. 2019, pp. 161–
177.

[110] S. Kelkar et al., “Analyzing HTTP-Based Information Exﬁltration of Ma-
licious Android Applications,” in Proc. IEEE TrustCom/BigDataSE, New
York, USA, Aug. 2018, pp. 1642–1645.

[111] P. Mockapetris, “RFC 1035 Domain Names - Implementation and Speciﬁca-

tion,” Internet Engineering Task Force, 1987.

[112] M. Lee and J. Schultz. (2016). “Detecting DNS Data Exﬁltration.” Accessed

on 10.06.2019, [Online]. Available: https://bit.ly/2WbFI7Z.

[113] L. Mendieta. (2016). “Three Month FrameworkPOS Malware Campaign Nabs
43,000 Credit Cards from Point of Sale Systems.” Accessed on 10.06.2019,
[Online]. Available: https://bit.ly/2BSw166.

[114] C. E. Shannon, “A Mathematical Theory of Communication,” Bell System

Technical Journal, vol. 27, no. 3, pp. 379–423, 1948.

[115] B. J. Brewer, “Computing Entropies with Nested Sampling,” Entropy, vol. 19,

no. 8, p. 422, 2017.

[116] Y. Zhauniarovich et al., “A Survey on Malicious Domains Detection Through

DNS Data Analysis,” ACM Comput. Surv., vol. 51, no. 4, pp. 1–36, 2018.

[117] F. T. Liu et al., “Isolation forest,” in Proc. IEEE Data Mining, Pisa, Italy,

Dec. 2008, pp. 413–422.

[118] R. Qasim. (2018). “DET (extensible) Data Exﬁltration Toolkit.” Accessed on
10.06.2019, [Online]. Available: https://github.com/qasimraz/DET.

[119]

(2018). “MasterCard Credit Card Generator.” Accessed on 10.06.2019, [On-
line]. Available: https://bit.ly/2QLHBHo.

143

document

[120]

[121]

(2018). “DNS Exﬁltration Dataset.” Accessed on 10.06.2019, [Online]. Avail-
able: https://nozzle-data.sdn.unsw.edu.au/.

(2019). “kryo.se: iodine (IP-over-DNS, IPv4 over DNS tunnel).” Accessed on
10.06.2019, [Online]. Available: https://code.kryo.se/iodine/.

[122] Y. Chen et al., “DNS noise: Measuring the pervasiveness of disposable do-
mains in modern DNS traﬃc,” in Proc. Dependable Systems and Networks
(DSN), Atlanta, GA, USA, Jun. 2014, pp. 598–609.

[123] W. Mercer et al. (2018). “DNSpionage Campaign Targets Middle East.” Ac-
cessed on 10.06.2019, [Online]. Available: https://bit.ly/2BBXKZI.

[124] F. Allard et al., “Tunneling activities detection using machine learning tech-
niques,” Journal of Telecommunications and Information Technology, pp. 37–
42, 2011.

[125]

[126]

(2019). “Easy removal method of Imrworldwide.com infection.” Accessed on
10.06.2019, [Online]. Available: https://bit.ly/31hOTY6.

(2019). “How to remove adlooxtracking.” Accessed on 10.06.2019, [Online].
Available: https://bit.ly/2XoKNet.

[127] B. Yu et al., “Behavior Analysis based DNS Tunneling Detection and Clas-
siﬁcation with Big Data Technologies,” in Proc. International Conference on
Internet of Things and Big Data, Rome, Italy, 2016, pp. 284–290.

[128] M. Lotter, “RFC 1033: Domain Administrators operations guide,” Interna-

tional Engineering Task Force, 1987.

[129] D. Barr, “RFC 1912: Common DNS operational and conﬁguration errors,”

The Pennsylvania State University, Pennsylvania, 1996.

[130] J. Ahmed, H. Habibi Gharakheili, C. Russell, and V. Sivaraman, “Automatic
detection of dga-enabled malware using sdn and traﬃc behavioral modeling,”
IEEE Transactions on Network Science and Engineering, pp. 1–1, 2022. doi:
10.1109/TNSE.2022.3173591.

[131] Cybercrime Magazine. (2020). “Global Cybercrime Damages Predicted To
Reach $6 Trillion Annually By 2021.” Accessed on 28.04.2020, [Online]. Avail-
able: https://bit.ly/2YpyEZZ.

[132] Accenture Security.

(2019).

“The Cost of Cybercrime.” Accessed on

28.04.2020, [Online]. Available: https://accntu.re/2WgX3hx.

[133] Security Magazine. (2020). “As Cyber Attacks Become More Prevalent,
Here?s Why Your Small Business is at Risk.” Accessed on 28.04.2020, [On-
line]. Available: https://bit.ly/2YpyEZZ.

[134] Accenture Security. (2018). “Enhancing the Resilience of the Internet and
Communications Ecosystem Against Botnets and Other Automated, Dis-

144

document

tributed Threats.” Accessed on 20.04.2020, [Online]. Available: https : / /
bit.ly/2VTd925.

[135] Ars Technica. (2017). “Failure to patch two-month-old bug led to massive
Equifax breach.” Accessed on 10.04.2020, [Online]. Available: https://bit.
ly/3aSJgmO.

[136] Sophos. (2020). “Troj/Menti-Fam.” Accessed on 25.02.2020, [Online]. Avail-

able: https://bit.ly/38ttVrJ.

[137] Microsoft.

(2020).

“TrojanDownloader:Win32/Murlo.S.” Accessed

on

25.02.2020, [Online]. Available: https://bit.ly/2xeYZyt.

[138] T. Micro. (2020). “WORM:NERIS.A.” Accessed on 25.02.2020,

[Online].

Available: https://bit.ly/2wxU8YK.

[139] Fortinet. (2020). “NSIS.botnet.” Accessed on 25.02.2020, [Online]. Available:

https://bit.ly/39uofPh.

[140] F. Secure. (2020). “Backdoor:W32/RBot.” Accessed on 25.02.2020, [Online].

Available: https://bit.ly/2VKtoPr.

[141] Microsoft. (2020). “Program:W32/Sogou.” Accessed on 25.02.2020, [Online].

Available: https://bit.ly/38spEEE.

[142] F. Secure. (2020). “Virus:Win32/Virut.” Accessed on 25.02.2020, [Online].

Available: https://bit.ly/2TtmRqw.

[143] S. Garcia. (2020). “The CTU-13 Dataset.” Accessed on 20.01.2020, [Online].
Available: https://www.stratosphereips.org/datasets-ctu13.

[144] R. Perdisci et al., “Behavioral Clustering of HTTP-Based Malware and Signa-
ture Generation Using Malicious Network Traces,” in Proc. USENIX NSDI,
San Jose, California, Apr. 2010.

[145] B. Biggio et al., “Poisoning Behavioral Malware Clustering,” in Proc. ACM

AISec, Nov. 2014.

[146] G. of Canada. (2020). “Canadian Centre for Cyber Security.” Accessed on

21.01.2020, [Online]. Available: https://cyber.gc.ca/en/.

[147] T. I. Team. (2020).

“Andromeda under the microscope.” Accessed on

21.01.2020, [Online]. Available: https://bit.ly/32VPcJp.

[148] A. Griﬃn. (2020). “Andromeda taken down.” Accessed on 25.01.2020, [On-

line]. Available: https://bit.ly/32SGITa.

[149] blueliv. (2020). “Inside Tinba- DGA infection.” Accessed on 9.02.2020, [On-

line]. Available: https://bit.ly/39uDHLg.

[150] H. Security. (2020). “Everything You Need to Know about the Notorious
Zeus Gameover Malware.” Accessed on 9.02.2020, [Online]. Available: https:
//bit.ly/39tmX7o.

145

document

[151] D. Bonderud. (2020). “Combating High-Risk, Low-Noise Threats.” Accessed

on 29.01.2020, [Online]. Available: https://ibm.co/38u8FBW.

[152] B. AsSadhan et al., “Periodic behavior in botnet command and control chan-

nels traﬃc,” in Proc. IEEE GLOBECOM, 2009, pp. 1–6.

[153] V. Total. (2020). “Virus Total.” Accessed on 27.02.2020, [Online]. Available:

https://bit.ly/32SskdI.

[154] urlscan. (2020). “X.co urlscan.io.” Accessed on 27.02.2020, [Online]. Available:

https://bit.ly/38ocE3b.

[155] D. A. Cieslak et al., “Learning Decision Trees for Unbalanced Data,” in Proc.

MLKDD, Antwerp, Belgium, Sep. 2008, pp. 241–256.

[156] A. Sivanathan et al., “Detecting Behavioral Change of IoT Devices using
Clustering-Based Network Traﬃc Modeling,” IEEE Internet of Things Jour-
nal, vol. 7, no. 8, pp. 7295–7309, Aug. 2020.

[157] S. Hariri et al., “Extended Isolation Forest,” IEEE Transactions on Knowl-

edge and Data Engineering, pp. 1–1, 2019.

[158] C. Ding et al., “K-means clustering via principal component analysis,” in

Proc. ICML, Banﬀ, Alberta, Canada, Jul. 2004.

[159] B. Scholkopf et al., “Estimating the support of a high-dimensional distribu-

tion,” Neural computation, vol. 13, no. 7, pp. 1443–1471, 2001.

[160] D. J. Ketchen et al., “The application of cluster analysis in strategic man-
agement research: An analysis and critique,” Strategic management journal,
vol. 17, no. 6, pp. 441–458, 1996.

[161] NoviFlow. (2020). “NoviSwitch 2122 High Performance OpenFlow Switch.”
Accessed on 01.02.2020, [Online]. Available: https://bit.ly/3cyhvSx.

[162] R. Dev. (2020). “Ryu SDN Framework.” Accessed on 27.02.2020, [Online].

Available: https://osrg.github.io/ryu/.

[163] Zeek. (2020). “The Zeek Network Security Monitor.” Accessed on 19.02.2020,

[Online]. Available: https://zeek.org.

[164]

[165]

(2016). “Major cyber attack disrupts internet service across Europe and US.”
Accessed on 28.03.2021, [Online]. Available: https://bit.ly/3fP7LHN.

(2020). “FBI Outlines Technique Behind DDoS Attacks on US Voter Regis-
tration Website.” Accessed on 31.03.2021, [Online]. Available: https://bit.
ly/3wC0II7.

[166] X. Luo, L. Wang, Z. Xu, et al., “A large scale analysis of dns water torture at-
tack,” in Proceedings of the 2018 2nd International Conference on Computer
Science and Artiﬁcial Intelligence, 2018, pp. 168–173.

146

document

147

