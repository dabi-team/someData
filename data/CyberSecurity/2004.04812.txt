0
2
0
2

t
c
O
7
1

]

G
L
.
s
c
[

2
v
2
1
8
4
0
.
4
0
0
2
:
v
i
X
r
a

Deep Learning based Frameworks for Handling
Imbalance in DGA, Email, and URL Data
Analysis

Simran K1, Prathiksha Balakrishna2, Vinayakumar Ravi3,1, and Soman KP1

1 Center for Computational Engineering and Networking, Amrita School Of
Engineering, Amrita vishwa vidyapeetham, Coimbatore, India.
simiketha19@gmail.com
2 Graduate School, Computer Science Department
Texas State University. prathi.93april8@gmail.com
3 Center for Artiﬁcial Intelligence, Prince Mohammad Bin Fahd University, Khobar,
Saudi Arabia.
vinayakumarr77@gmail.com

Abstract. Deep learning is a state of the art method for a lot of ap-
plications. The main issue is that most of the real-time data is highly
imbalanced in nature. In order to avoid bias in training, cost-sensitive
approach can be used. In this paper, we propose cost-sensitive deep
learning based frameworks and the performance of the frameworks is
evaluated on three diﬀerent Cyber Security use cases which are Domain
Generation Algorithm (DGA), Electronic mail (Email), and Uniform Re-
source Locator (URL). Various experiments were performed using cost-
insensitive as well as cost-sensitive methods and parameters for both of
these methods are set based on hyperparameter tuning. In all experi-
ments, the cost-sensitive deep learning methods performed better than
the cost-insensitive approaches. This is mainly due to the reason that
cost-sensitive approach gives importance to the classes which have a
very less number of samples during training and this helps to learn all
the classes in a more eﬃcient manner.

Keywords: Cyber Security · Deep learning · Cost-sensitive learning ·
Imbalanced data.

1

Introduction

Cyber Security is an area which deals with techniques related to protecting data,
programs, devices, and networks from any attack, damage, and unauthorized
access [1]. There are various methods in Cyber Security to secure systems as
well as networks. The classical method is a signature-based system. A signature-
based system relies on regular expressions which give domain-level knowledge.
The main issue is that these signature-based systems can identify only described
malware and cannot detect novel types of attacks and even existing variant types
of attacks. In order to detect “0-day” malware, the researchers have followed the
application of machine learning [2].

 
 
 
 
 
 
2

Simran et al.

Recently, the application of deep learning architectures are employed in Cy-
ber Security use cases and these models can extract features implicitly whereas
machine learning algorithms require manual feature engineering [3], [4], [5], [6].
To classify DGA generated domains, diﬀerent deep learning approaches were
proposed in [7], [1], and [2]. The real-time datasets are highly imbalanced in
nature and in order to handle it properly the concept of data mining approaches
can be used. In this direction, numerous amount of research is being performed.
In [9], an LSTM based model is proposed to handle multiclass imbalanced data
for detection of DGA botnet. In this work, the LSTM model is adapted to be
cost-sensitive and performed better than other cost-insensitive approaches. The
main objectives of this paper are:

1. This work proposes a cost-sensitive deep learning based framework for Cyber

Security.

2. The performance of cost-sensitive approaches are evaluated on three diﬀerent

use cases in security namely, DGA, Email, and URL.

3. The performance of cost-sensitive models are compared with cost-insensitive

models.

4. Various hyperparameter tuning methods are employed to identify the opti-

mal network parameters and network structures.

The remaining of this paper is arranged in the following order: Section 2
documents a survey of the literature related to DGA, URL, and Email followed
by background related to NLP, deep learning, and cost-sensitive concepts in
Section 3. Section 4 provides a description of dataset. Section 5 describes the
details of the proposed architecture. Section 6 reports the experiments, results,
and observations made by the proposed architecture. Section 7 concludes the
paper with remakes on future work of research.

2 Literature Survey on deep learning based DGA, URL,

and Email data analysis

2.1 Domain Generation Algorithms (DGAs)

In [8], an LSTM network was proposed for real-time prediction of DGA generated
domains. This work implemented binary as well as multiclass classiﬁcation of
DGA. This network has a detection rate of 90% and a false positive (FP) rate of
1:10000. In [10], convolutional neural network (CNN) and LSTM deep learning
models were utilized to classify large amounts of real traﬃc. Simple steps were
followed to obtain pure DGA and non-DGA samples from real DNS traﬃc and
achieved a false positive rate of 0.01%. In [11], deep learning based approach
was proposed to classify domain name as malicious or benign. Performance of
various deep learning techniques like recurrent neural network (RNN), LSTM,
and classical machine learning approaches were compared. A highly scalable
framework was proposed by [12] for situational awareness of Cyber Security
threats. This framework analyses domain name system event data. Deep learning

Cost-Sensitive Deep learning based Framework for Cyber Security

3

approaches for detection and classiﬁcation of pseudo-random domain names were
proposed in [13]. Comparison between diﬀerent deep learning approaches like
LSTM, RNN, I-RNN, CNN, and CNN-LSTM was performed. RNN and CNN-
LSTM performed signiﬁcantly better than other models and got a detection rate
of 0.99 and 0.98 respectively. A data-driven approach was utilized in [14] to detect
malware-generated domain names. This approach uses RNN and has achieved
an F1-score of 0.971. A combined binary and multiclass classiﬁcation model was
proposed in [9] to detect DGA botnet. This model uses LSTM network and has
the capability to handle imbalanced multiclass data. A comprehensive survey on
detection of malicious domain using DNS data was performed by [15].

2.2 Uniform Resource Locator (URL)

A comprehensive and systematic survey to detect malicious URL using machine
learning methodologies was conducted by [16]. URLNet was proposed in [17]
which is an end-to-end system to detect malicious URL. This deep learning
framework contains word CNNs as well as character CNNs and has the capabil-
ity to learn nonlinear URL embedding directly from the URL. In [18], various
deep learning frameworks such as RNN, I-RNN, LSTM, CNN, CNN-LSTM were
utilized to classify real URL’s into malicious and benign at the character level.
LSTM and CNN-LSTM performed signiﬁcantly better than other models and
achieved an accuracy of 0.9996 and 0.9995 respectively. A comparative study
using shallow and deep networks was performed by [19] for malicious URL’s
detection. In this work, CNN-LSTM network outperformed other networks by
achieving an accuracy of 98%. In [20], three models namely, support vector ma-
chine (SVM) algorithm based on term frequency - inverse document frequency
(TF-IDF), logistic regression algorithm and CNN based on the word2vec fea-
tures were used to detect and predict malicious URLs. An online deep learning
framework was proposed in [21] for detecting malicious DNS and URL. The
framework utilized character-level word embedding and CNN. In this work, a
real-world data set was utilized and the models performed better than state-of-
art baseline methods. URLDeep was proposed in [22] to detect malicious URL’s.
This deep learning framework based on dynamic CNN can learn a non-linear
URL address. A cost-sensitive framework ﬁrstﬁlter was proposed in [23] to de-
tect malicious URL. This network can handle large-scale imbalanced network
data.

2.3 Electronic mail (Email)

A complete review for ﬁltering of email spam was proposed in [24]. Machine
learning based techniques and trends for email spam ﬁltering were also dis-
cussed in this work. In [25], a machine learning based approach was proposed to
classify email space. This framework basically classiﬁes an email into spam and
non-spam. This work also proposed a platform-independent progressive web app
(PWA). In [26], deep learning based frameworks were proposed for email clas-
siﬁcation. This work proposed LSTMs and CNNs network which outperformed

4

Simran et al.

baseline architectures. CNN performed better than LSTM and achieved an F1
score of 84.0%. [27] proposed a multi-modal framework based on model fusion
(MMA-MF) to classify email. This model fuses CNN and LSTM model. Image
part of the email is processed by the CNN model whereas the text part of the
email is sent to the LSTM model separately. Accuracy with a range between
92.64% to 98.48% is achieved by this method.

3 Background

This section discusses the details behind the text representation, deep learning
architectures and the concept of cost-sensitive model.

3.1 Text representation

Keras Embedding: Word embedding takes sequence and similarities into ac-
count to convert words into dense vectors of real numbers. Keras provides an
embedding layer with few parameters such as dictionary size, embedding size,
length of the input sequence and so on. These parameters are hyperparameters
and can have an impact on the performance. The weights are taken randomly
at ﬁrst. These weights are tuned during backpropagation with respect to other
deep learning layers. Generally, Keras embedding learns embedding of all the
words or characters in the training set but the input word or character should
be represented by a unique integer. Keras4 is neural network library available for
the public which has diﬀerent neural network building blocks like RNN, CNN,
etc and also other common layers like dropout, pooling, etc.

N-gram: From a given sequence of text, the continuous sequences of N items
are called are N-gram. N-gram with N = 1 is known as a unigram and it takes
one word/character at once. N = 2 and N = 3 are called bigram and tri-
gram respectively and will take two and three words/characters at a time. If n
words/characters are to be taken at once then N will be equal to n.

3.2 Machine Learning

Naive Bayes: is a simple but surprisingly powerful algorithm which is based
on Bayes theorem principle. Given the prior information of conditions that may
be related with the occasion, it ﬁnds the probability of occurrence of an event.

Decision Tree: is another supervised machine learning algorithm. The decision
tree is constructed by continuously splitting the data-dependent on certain pa-
rameters. Decision trees consist of leaves and nodes where leaves are the results of
each decision made and nodes are the decision processes. Iterative Dichotomiser
3 (ID3) algorithm is the most commonly used algorithm to produce these trees.
Using Decision Trees both classiﬁcation and regression are possible for discrete
and continuous data.
4 https://keras.io/

Cost-Sensitive Deep learning based Framework for Cyber Security

5

AdaBoost: is an ensemble machine learning classiﬁer like random forest which
utilizes a number of weak classiﬁers to make a strong classiﬁer. Many machine
learning algorithms performance can be boosted using AdaBoost. Training set
which is used to iteratively retrain the algorithm is chosen based on the accuracy
of previous training. At every iteration, there is a weight given to every trained
classiﬁer which is dependent on the accuracy achieved by the classiﬁer. The items
that were not correctly classiﬁed are given higher weights which makes them
have a higher probability in next classiﬁer. Classiﬁer which has an accuracy of
50 percent or more are given zero weight whereas negative weights are given to
classiﬁer which has accuracy less the 50 percent. As the number of iterations is
increased, the accuracy of the classiﬁer is improved.

Random Forest (RF): At ﬁrst, random forest delivers multiple decision trees.
These diﬀerent decision trees are then merged to get the correct classiﬁcation.
The accuracy of this algorithm is directly proportional to the number of decision
trees. Without hyperparameter tuning, random forest gives a very good perfor-
mance. To reduce overﬁtting, it utilizes the ensemble learning method while
making the decision trees. Diﬀerent types of data such as binary, numerical or
categorical can be given as input to this algorithm.

Support Vector Machine (SVM): is another supervised machine learning
algorithm which creates a hyperplane to split the data attributes between at
least two classes. Every data attribute is projected onto an n-dimensional space.
The hyperplane is created in a way such that the distance between the most
nearby point of each class and the hyperplane is maximized. Hard margin SVM
and soft margin SVM are the two type of SVM where hard margin SVM is the
SVM which draws a hyperplane in linear manner whereas soft margin SVM is
the SVM which draws the hyperplane in non-linear manner.

3.3 Deep Learning Architectures

Deep Neural Network (DNN):
is an advanced model of classical feed-
forward network (FNN). As the name indicates the DNN contains many hid-
den layers along with the input and output layer. When the number of layer
increases in FFN causes the vanishing and exploding gradient issue. To handle
these issues, the ReLU non-linear activation was introduced. ReLU helps to
protect weights from vanishing by the gradient error. Compared to other non-
linear functions, ReLU is more robust to the ﬁrst-order derivative function since
it does not become zero for high positive as well as high negative values.

Convolutional Neural Network (CNN): is the most commonly used deep
learning architecture in computer vision applications as it has the capability to
extract spatial features. The three layers in CNN are convolution layer, pooling
layer, and fully connected layer. Convolution layer contains ﬁlters that slide

6

Simran et al.

over the data to capture the optimal features and these features collectively are
termed as a feature map. The dimension of the feature map is high and to reduce
the dimension pooling layer is used. Min, max or average are the three pooling
operations. Finally, the pooling features are passed into a fully connected layer
for classiﬁcation. For binary classiﬁcation, sigmoid activation function is used
whereas for multiclass classiﬁcation, sof tmax activation function is used.

Long short-term memory (LSTM):
is a special type of recurrent neural
network. It takes care of the issue of exploding and vanishing gradient. A cell
of LSTM comprises of four major parts namely, input, state cell, three gates
and output. The concatenation of the previous output and present input is the
input of this LSTM cell. The focal piece of the LSTM cell is called as a state cell
which holds the information about the previous sequences. The three gates in
an LSTM cell are forget gate, input gate, and output gate. Which information
to be remembered or which information to forget is decided by the forget gate.
The information relevant to the present input is taken to the cell state by the
input gate. What information should be passed as the output of the LSTM cell
is decided by the output gate. The output of the present cell gets concatenated
with the input of the next cell.

CNN - LSTM: CNN - LSTM architecture was developed for spatial time series
prediction problems as LSTM alone cannot handle inputs with spatial structure
like images. It consists of CNN layers to exact the features of the input data and
LSTM for supporting sequence predictions. In the end, it is connected to a fully
connected layer to get the classiﬁed output.

3.4 Cost-sensitive Model

Models normally treat all samples equally which makes them sensitive to the
class imbalance problem. Class imbalance problem arises when there are classes
which have very small samples in comparison to other classes in the training data.
This problem can be handled using cost-sensitive models. The cost-sensitive
deep learning architectures consider all the samples equally. These models give
importance to the classes that have more number of samples during training
and limits the learning capability to the classes that have very less number of
samples. Cost-sensitive learning served as an important method in real-world
data mining applications and provides an approach to carefully handle the class
imbalance problem. Let’s assume that the samples have equal cost at ﬁrst. C[i, i]
indicates the misclassiﬁcation cost of the class i, which is generated using the
class distribution as

C[i, i] =

(cid:19)γ

(cid:18) 1
ni

(1)

Where γ ∈ [0, 1]. 0 indicates that cost-sensitive deep learning architectures
are diminished to cost-insensitive and 1 indicates that C[i, i] is inversely propor-
tional to the class size nj.

Cost-Sensitive Deep learning based Framework for Cyber Security

7

4 Description of the Data set

In this work, three diﬀerent data sets were utilized for the three use cases. For
DGA, domain names have to be classiﬁed as legitimate or malicious. For Email,
classiﬁcation result should be either legitimate or spam. For URL, URL should
be classiﬁed as legitimate or malicious. The data set is divided into train data
and test data. The train data set was used to train the models whereas the test
data set was used to test the trained models. The train and test dataset of DGA
composed of 38,276 legitimate, 53,052 malicious and 12,753 legitimate, 17,690
malicious domain name samples respectively. The legitimate domain names are
collected from Alexa5 and OpenDNS6 and DGA generated domain names are
collected from OSINT Feeds7. The train dataset is collected from November,
2017 to December 2017 and the test dataset is collected from January 2018 to
Feburary 2018. The train and test dataset of email composed of 19,337 legitimate,
24,665 spam and 8,153 legitimate, 10,706 email samples respectively. The train
and test dataset of email are collected from Enron8 and PU9. The train and
test dataset of URL composed of 23,374 legitimate, 11,116 malicious and 1,142
legitimate, 578 malicious samples respectively. The legitimate URLs are collected
from Alexa.com and DMOZ directory10 and malicious URLs are collected from
malwareurl.com, Phishtank.com, OpenPhish.org, malwaredomainlist.com, and
malwaredomains.com. The train dataset is collected from March, 2018 to April
2018 and the test dataset is collected from September, 2018 to October, 2018.
All the datasets are unique as well as the train and test datasets are disjoint to
each other.

5 Proposed Architecture

The proposed architecture is shown in Figure 1. The diagram located in top
shows the training process involved in cost-insensitive deep learning model. The
other diagram shows the training process involved in the cost-sensitive deep
learning model. In the cost-sensitive deep learning model, we introduce cost-
weights to make the classiﬁer to give importance to the classes which have very
less number of samples and give less importance to the classes which have more
number of samples. This enables to avoid imbalanced problems in classiﬁcation.

5 https://support.alexa.com/hc/en-us/articles/200449834-Does-Alexa-have-a-list-of-

its-top-ranked-websites

6 https://www.opendns.com/
7 https://osint.bambenekconsulting.com/feeds/
8 https://www.cs.cmu.edu/ enron/
9 http://www.aueb.gr/users/ion/data/PU123ACorpora.tar.gz
10 https://dmoz-odp.org/

8

Simran et al.

Fig. 1. Cost-insensitive and cost-sensitive deep learning based architectures.

6 Experiments, Results, and Observations

All the classical machine learning algorithms are implemented using Scikit-
learn11 and the deep learning models are implemented using TensorFlow12 with
Keras13 framework. All the models are trained on GPU enabled TensorFlow.
Various statistical measures are utilized in order to evaluate the performance of
the proposed classical machine learning and deep learning models.

We have trained various classical machine learning and deep learning model
which can be cost-sensitive or cost-insensitive using the trained datasets. The
performance of the trained models is evaluated on test data.

Table 1. Results for DGA Analysis.

Model
Naive Bayes 68.1
Decision Tree 79.7
82.8
AdaBoost
84.1
RF
85.2
SVM
86.8
DNN
94.3
CNN
LSTM
94.4
CNN-LSTM 95.2

95.4
CNN
LSTM
95.5
CNN-LSTM 95.6

Accuracy Precision Recall F1-score TN FP FN TP
64.4
9,653 8037
84.3
86.6
87.5
88.3
89.5
95.3
95.3
96.0

12,700 53
7,654 5,099 1091 16,599
16,920
8,300 4,453 770
16,954
8,651 4,102 736
17,010
8,932 3,821 680
17,002
9,434 3,319 688
17,457
11,263 1,490 233
17,269
11,457 1,296 421
15,716
11,478 1,275 174

45.5
99.3
93.8
76.5
95.6
79.2
95.8
80.5
96.2
81.7
96.1
83.7
98.7
92.1
97.6
93.0
99.0
93.2
Cost-sensitive models
99.5
93.2
99.6
93.2
99.7
93.2

11,464 1,289 97
11,470 1,283 74
11,467 1,286 59

17,593
17,616
17,631

96.2
96.3
96.3

All the models are parameterized as optimal parameters play a signiﬁcant role
in obtaining better performance. For machine learning algorithms, we have not

11 https://scikit-learn.org/
12 https://www.tensorﬂow.org/
13 https://keras.io/

Cost-Sensitive Deep learning based Framework for Cyber Security

9

Table 2. Results for Email Analysis.

Accuracy Precision Recall F1-score TN FP FN TP

Model
Naive Bayes 68.8
Decision Tree 82.9
91.3
AdaBoost
92.0
RF
92.3
SVM
93.0
DNN
93.6
CNN
LSTM
93.7
CNN-LSTM 94.0

94.2
CNN
LSTM
94.3
CNN-LSTM 94.7

62.2
87.1
92.7
93.2
93.3
94.1
94.5
94.6
94.8

45.3
99.4
95.4
80.0
97.1
88.6
96.7
89.9
94.4
92.3
98.6
90.0
96.4
92.6
97.5
91.9
92.2
97.6
Cost-sensitive models
97.4
92.7
97.6
92.7
98.3
92.8

95.0
95.1
95.5

5,855 4,851

8,122 31
4,521 2,527 487
6,815 1,338 310
6,984 1,169 349
599
7,304 849
6,980 1,173 145
382
7,326 827
270
7,239 914
253
7,270 883

10,138
10,396
10,357
10,107
10,561
10,324
10,436
10,453

7,334 819
7,333 820
7,341 812

276
254
187

10,430
10,452
10,519

done any hyperparameter tuning. We have used the default parameters of Scikit-
learn. To convert text into numerical values, Keras embedding was used with an
embedding dimension of 128. Diﬀerent architectures namely, DNN, CNN, LSTM,
and CNN-LSTM are used. For DNN, 4 hidden layers with units 512, 384, 256,
128 and a ﬁnally dense layer with 1 hidden unit. In between the hidden layer
dropout of 0.01 and batch normalization are used. Dropout was used to reduce
overﬁtting and batch normalization was used to increase the speed. In CNN,
64 ﬁlters with ﬁlter length 3 and followed by maxpooling with pooling length 2
are used. Followed by a dense layer with 128 hidden units and dropout of 0.3.
Finally a dense layer with one hidden unit. LSTM contains 128 memory blocks
followed by dropout of 0.3 and ﬁnally dense layer with one hidden unit. In CNN-
LSTM architecture, we connected CNN network with LSTM network. CNN has
64 ﬁlters with ﬁlter length 3, followed by a maxpooling layer having pooling
length 2. Followed by LSTM network having 50 memory blocks and ﬁnally a
dense layer with one hidden unit is added. All the experiments are run till 100
epochs with a learning rate of 0.01, and adam optimizer.

Table 3. Results for URL Analysis.

Model
Naive Bayes 45.1
Decision Tree 81.8
87.1
AdaBoost
90.0
RF
81.0
SVM
90.8
DNN
92.9
CNN
LSTM
93.4
CNN-LSTM 94.4

93.4
CNN
LSTM
94.5
CNN-LSTM 94.7

37.9
73.3
83.8
90.6
88.4
92.5
91.9
97.2
96.5

98.8
72.1
76.3
78.4
50.0
79.1
86.5
82.7
86.5

Accuracy Precision Recall F1-score TN FP FN TP
937 7
571
205
990
152 161 417
1,057 85 137 441
1,095 47 125 453
1,104 38 289 289
1,105 37 121 457
1,098 44 78 500
1,128 14 100 478
1,124 18 78 500

54.7
72.7
79.9
84.0
63.9
85.3
89.1
89.3
91.2
Cost-sensitive models
89.5
91.7
92.0

1,122 20 94 484
1,107 35 59 519
1,103 39 52 526

83.7
89.8
91.0

96.0
93.7
93.1

10

Simran et al.

The detailed performance analysis of all the models are reported in Table 1
for DGA analysis, Table 2 for Email analysis, and Table 3 for URL analysis. As
shown in the tables, the performance of deep learning models is better than the
machine learning models. More importantly, the performance of cost-sensitive
deep learning models is better than the cost-insensitive models. This is primarily
because cost-sensitive models can give certain weights to the classes which helps
to reduced overﬁtting and underﬁtting during training. We can see in all the
three Tables that the cost-sensitive hybrid network of CNN-LSTM performed
better than the other network like CNN and LSTM.

7 Conclusion and Future Work

This paper proposes a generalized cost-sensitive deep learning model for Cyber
Security use cases such as DGA, Email, and URL. However, the model can be
applied on other Cyber Security use cases also. The cost-sensitive hybrid model
composed of CNN and LSTM can extract spatial and temporal features and
can obtain better perform on any type of data sets. Implementing this model in
real time data analysis with Big data and Streaming can be considered a good
direction for future work.

Acknowledgements

This research was supported in part by Paramount Computer Systems and
Lakhshya Cyber Security Labs. We are grateful to NVIDIA India, for the GPU
hardware support to research grant. We are also grateful to Computational En-
gineering and Networking (CEN) department for encouraging the research.

References

1. Mahdavifar, S., & Ghorbani, A. A. (2019). Application of deep learning to cyber-

security: A survey. Neurocomputing, 347, 149-176.

2. Apruzzese, G., Colajanni, M., Ferretti, L., Guido, A., & Marchetti, M. (2018,
May). On the eﬀectiveness of machine and deep learning for cyber security. In
2018 10th International Conference on Cyber Conﬂict (CyCon) (pp. 371-390).
IEEE.

3. Vinayakumar, R., Alazab, M., Soman, K. P., Poornachandran, P., Al-Nemrat,
A., & Venkatraman, S. (2019). Deep Learning Approach for Intelligent Intrusion
Detection System. IEEE Access, 7, 41525-41550.

4. Vinayakumar, R., Soman, K. P., Poornachandran, P., & Menon, P. A deep-dive on
Machine learning for Cybersecurity use cases. In Machine Learning for Computer
and Cyber Security: Principle, Algorithms, and Practices. CRC Press.

5. Vinayakumar, R., & Soman, K. P. (2018). DeepMalNet: evaluating shallow and

deep networks for static PE malware detection. ICT express, 4(4), 255-258.

6. Vinayakumar, R., Soman, K. P., & Poornachandran, P. (2019). A Comparative
Analysis of Deep Learning Approaches for Network Intrusion Detection Systems
(N-IDSs): Deep Learning for N-IDSs. International Journal of Digital Crime and
Forensics (IJDCF), 11(3), 65-89.

Cost-Sensitive Deep learning based Framework for Cyber Security

11

7. Berman, D. S., Buczak, A. L., Chavis, J. S., & Corbett, C. L. (2019). A survey of

deep learning methods for cyber security. Information, 10(4), 122.

8. Woodbridge, J., Anderson, H. S., Ahuja, A., & Grant, D. (2016). Predicting do-
main generation algorithms with long short-term memory networks. arXiv preprint
arXiv:1611.00791.

9. Tran, D., Mac, H., Tong, V., Tran, H. A., & Nguyen, L. G. (2018). A LSTM
based framework for handling multiclass imbalance in DGA botnet detection.
Neurocomputing, 275, 2401-2413.

10. Yu, B., Gray, D. L., Pan, J., De Cock, M., & Nascimento, A. C. (2017, November).
Inline DGA detection with deep networks. In 2017 IEEE International Conference
on Data Mining Workshops (ICDMW) (pp. 683-692). IEEE.

11. Vinayakumar, R., Soman, K. P., & Poornachandran, P. (2018). Detecting mali-
cious domain names using deep learning approaches at scale. Journal of Intelligent
& Fuzzy Systems, 34(3), 1355-1367.

12. Vinayakumar, R., Poornachandran, P., & Soman, K. P. (2018). Scalable frame-
work for cyber threat situational awareness based on domain name systems data
analysis. In Big Data in Engineering Applications (pp. 113-142). Springer, Singa-
pore.

13. Vinayakumar, R., Soman, K. P., Poornachandran, P., & Sachin Kumar, S. (2018).
Evaluating deep learning approaches to characterize and classify the DGAs at
scale. Journal of Intelligent & Fuzzy Systems, 34(3), 1265-1276.

14. Lison, P., & Mavroeidis, V. (2017). Automatic detection of malware-generated

domains with recurrent neural models. arXiv preprint arXiv:1709.07102.

15. Zhauniarovich, Y., Khalil, I., Yu, T., & Dacier, M. (2018). A survey on malicious
domains detection through DNS data analysis. ACM Computing Surveys (CSUR),
51(4), 67.

16. Sahoo, D., Liu, C., & Hoi, S. C. (2017). Malicious URL detection using machine

learning: a survey. arXiv preprint arXiv:1701.07179.

17. Le, H., Pham, Q., Sahoo, D., & Hoi, S. C. (2018). URLnet: Learning a URL
representation with deep learning for malicious URL detection. arXiv preprint
arXiv:1802.03162.

18. Vinayakumar, R., Soman, K. P., & Poornachandran, P. (2018). Evaluating deep
learning approaches to characterize and classify malicious URL’s. Journal of In-
telligent & Fuzzy Systems, 34(3), 1333-1343.

19. Vazhayil, A., Vinayakumar, R., & Soman, K. P. (2018, July). Comparative Study
of the Detection of Malicious URLs Using Shallow and Deep Networks. In 2018 9th
International Conference on Computing, Communication and Networking Tech-
nologies (ICCCNT) (pp. 1-6). IEEE.

20. Abdi, F. D., & Wenjuan, L. MALICIOUS URL DETECTION USING CONVO-

LUTIONAL NEURAL NETWORK.

21. Jiang, J., Chen, J., Choo, K. K. R., Liu, C., Liu, K., Yu, M., & Wang, Y. (2017,
October). A deep learning based online malicious URL and DNS detection scheme.
In International Conference on Security and Privacy in Communication Systems
(pp. 438-448). Springer, Cham.

22. Wanda, P., & Jie, H. J. URLDeep: Continuous Prediction of Malicious URL with

Dynamic Deep Learning in Social Networks.

23. Vu, L., Nguyen, P., & Turaga, D. (2016). Firstﬁlter: a cost-sensitive approach
to malicious URL detection in large-scale enterprise networks. IBM Journal of
Research and Development, 60(4), 4-1.

24. Bhowmick, A., & Hazarika, S. M. (2016). Machine learning for e-mail spam ﬁlter-

ing: review, techniques and trends. arXiv preprint arXiv:1606.01042.

12

Simran et al.

25. Alurkar, A. A., Ranade, S. B., Joshi, S. V., Ranade, S. S., Sonewar, P. A., Mahalle,
P. N., & Deshpande, A. V. (2017, November). A proposed data science approach
for email spam classiﬁcation using machine learning techniques. In 2017 Internet
of Things Business Models, Users, and Networks (pp. 1-5). IEEE.

26. Eugene, L., & Caswell, I. (2017). Making a Manageable Email Experience with

Deep Learning.

27. Yang, H., Liu, Q., Zhou, S., & Luo, Y. (2019). A Spam Filtering Method Based

on Multi-Modal Fusion. Applied Sciences, 9(6), 1152.

