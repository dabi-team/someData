1
2
0
2

y
a
M
2
2

]

O
L
.
s
c
[

3
v
2
7
5
4
0
.
2
0
9
1
:
v
i
X
r
a

A Formal Approach to Physics-Based Attacks
in Cyber-Physical Systems
(Extended Version)∗

Ruggero Lanotte1, Massimo Merro2, Andrei Munteanu2, and Luca Viganò3

1DISUIT, Università dell’Insubria, Italy, ruggero.lanotte@uninsubria.it
2Dipartimento di Informatica, Università degli Studi di Verona, Italy,
{massimo.merro,andrei.munteanu}@univr.it
3Department of Informatics, King’s College London, UK, luca.vigano@kcl.ac.uk

May 25, 2021

Abstract

We apply formal methods to lay and streamline theoretical foundations to reason about Cyber-Physical
Systems (CPSs) and physics-based attacks, i.e., attacks targeting physical devices. We focus on a formal
treatment of both integrity and denial of service attacks to sensors and actuators of CPSs, and on the
timing aspects of these attacks. Our contributions are fourfold. (1) We deﬁne a hybrid process calculus to
model both CPSs and physics-based attacks. (2) We formalise a threat model that speciﬁes MITM attacks
that can manipulate sensor readings or control commands in order to drive a CPS into an undesired
state; we group these attacks into classes, and provide the means to assess attack tolerance/vulnerability
with respect to a given class of attacks, based on a proper notion of most powerful physics-based attack.
(3) We formalise how to estimate the impact of a successful attack on a CPS and investigate possible
quantiﬁcations of the success chances of an attack. (4) We illustrate our deﬁnitions and results by
formalising a non-trivial running example in Uppaal SMC, the statistical extension of the Uppaal model
checker; we use Uppaal SMC as an automatic tool for carrying out a static security analysis of our
running example in isolation and when exposed to three diﬀerent physics-based attacks with diﬀerent
impacts.

1

Introduction

1.1 Context and motivation

Cyber-Physical Systems (CPSs) are integrations of networking and distributed computing systems with
physical processes that monitor and control entities in a physical environment, with feedback loops where
physical processes aﬀect computations and vice versa. For example, in real-time control systems, a hierarchy
of sensors, actuators and control components are connected to control stations.

In recent years there has been a dramatic increase in the number of attacks to the security of CPSs, e.g.,
manipulating sensor readings and, in general, inﬂuencing physical processes to bring the system into a state
desired by the attacker. Some notorious examples are: (i) the STUXnet worm, which reprogrammed PLCs of
nuclear centrifuges in Iran [35]; (ii) the attack on a sewage treatment facility in Queensland, Australia, which

∗This document extends the paper “A Formal Approach to Physics-Based Attacks in Cyber-Physical Systems” that will

appear in ACM Transactions on Privacy and Security by providing proofs that are worked out in full details.

1

 
 
 
 
 
 
manipulated the SCADA system to release raw sewage into local rivers and parks [53]; (iii) the BlackEnergy
cyber-attack on the Ukrainian power grid, again compromising the SCADA system [31].

A common aspect of these attacks is that they all compromised safety critical systems, i.e., systems whose
failures may cause catastrophic consequences. Thus, as stated in [24, 25], the concern for consequences at the
physical level puts CPS security apart from standard information security, and demands for ad hoc solutions
to properly address such novel research challenges.

These ad hoc solutions must explicitly take into consideration a number of speciﬁc issues of attacks
tailored for CPSs. One main critical issue is the timing of the attack : the physical state of a system changes
continuously over time and, as the system evolves, some states might be more vulnerable to attacks than
others [33]. For example, an attack launched when the target state variable reaches a local maximum (or
minimum) may have a great impact on the whole system behaviour, whereas the system might be able to
tolerate the same attack if launched when that variable is far from its local maximum or minimum [34].
Furthermore, not only the timing of the attack but also the duration of the attack is an important parameter
to be taken into consideration in order to achieve a successful attack. For example, it may take minutes for a
chemical reactor to rupture [56], hours to heat a tank of water or burn out a motor, and days to destroy
centrifuges [35].

Much progress has been done in the last years in developing formal approaches to aid the safety veriﬁcation
of CPSs (e.g., [28, 18, 19, 49, 48, 4], to name a few). However, there is still a relatively small number of works
that use formal methods in the context of the security analysis of CPSs (e.g., [11, 10, 60, 50, 46, 1, 8, 58]).
In this respect, to the best of our knowledge, a systematic formal approach to study physics-based attacks,
that is, attacks targeting the physical devices (sensors and actuators) of CPSs, is still to be fully developed.
Our paper moves in this direction by relying on a process calculus approach.

1.2 Background

The dynamic behaviour of the physical plant of a CPS is often represented by means of a discrete-time
state-space model 1 consisting of two equations of the form

xk+1 = Axk + Buk + wk

yk = Cxk + ek

where xk ∈ Rn is the current (physical) state, uk ∈ Rm is the input (i.e., the control actions implemented
through actuators) and yk ∈ Rp is the output (i.e., the measurements from the sensors). The uncertainty
wk ∈ Rn and the measurement error ek ∈ Rp represent perturbation and sensor noise, respectively, and A, B,
and C are matrices modelling the dynamics of the physical system. Here, the next state xk+1 depends on
the current state xk and the corresponding control actions uk, at the sampling instant k ∈ N. The state xk
cannot be directly observed: only its measurements yk can be observed.

The physical plant is supported by a communication network through which the sensor measurements and
actuator data are exchanged with controller(s) and supervisor(s) (e.g., IDSs), which are the cyber components
(also called logics) of a CPS.

1.3 Contributions

In this paper, we focus on a formal treatment of both integrity and Denial of Service (DoS) attacks to physical
devices (sensors and actuators) of CPSs, paying particular attention to the timing aspects of these attacks.
The overall goal of the paper is to apply formal methodologies to lay theoretical foundations to reason about
and formally detect attacks to physical devices of CPSs. A straightforward utilisation of these methodologies
is for model-checking (as, e.g., in [19]) or monitoring (as, e.g., in [4]) in order to be able to verify security
properties of CPSs either before system deployment or, when static analysis is not feasible, at runtime to
promptly detect undesired behaviours. In other words, we aim at providing an essential stepping stone for

1See [62, 63] for a taxonomy of the time-scale models used to represent CPSs.

2

Actuators

ua
k

uk

wk

Plant

Logics

xk

ek

Sensors

yk

ya
k

Figure 1: MITM attacks to sensor readings and control commands

formal and automated analysis techniques for checking the security of CPSs (rather than for providing defence
techniques, i.e., mitigation [45]).

Our contribution is fourfold. The ﬁrst contribution is the deﬁnition of a hybrid process calculus, called

CCPSA, to formally specify both CPSs and physics-based attacks. In CCPSA, CPSs have two components:

• a physical component denoting the physical plant (also called environment) of the system, and containing

information on state variables, actuators, sensors, evolution law, etc., and

• a cyber component that governs access to sensors and actuators, and channel-based communication

with other cyber components.

Thus, channels are used for logical interactions between cyber components, whereas sensors and actuators
make possible the interaction between cyber and physical components.

CCPSA adopts a discrete notion of time [27] and it is equipped with a labelled transition semantics (LTS)
that allows us to observe both physical events (system deadlock and violations of safety conditions) and
cyber events (channel communications). Based on our LTS, we deﬁne two compositional trace-based system
preorders: a deadlock-sensitive trace preorder, (cid:118), and a timed variant, (cid:118)m..n, for m ∈ N+ and n ∈ N+ ∪ {∞},
which takes into account discrepancies of execution traces within the discrete time interval m..n. Intuitively,
,
simulates the execution traces of Sys 1
given two CPSs Sys 1
except for the time interval m..n; if n = ∞ then the simulation only holds for the ﬁrst m − 1 time slots.

, we write Sys 1 (cid:118)m..n Sys 2

and Sys 2

if Sys 2

As a second contribution, we formalise a threat model that speciﬁes man-in-the-middle (MITM) attacks
that can manipulate sensor readings or control commands in order to drive a CPS into an undesired state [55].2
Without loss of generality, MITM attacks targeting physical devices (sensors or actuators) can be assimilated
to physical attacks, i.e., those attacks that directly compromise physical devices (e.g., electromagnetic attacks).
As depicted in Figure 1, our attacks may aﬀect directly the sensor measurements or the controller commands:

• Attacks on sensors consist of reading and eventually replacing yk (the sensor measurements) with ya
.
k
• Attacks on actuators consist of reading, dropping and eventually replacing the controller commands uk

with ua
k

, aﬀecting directly the actions the actuators may execute.

We group attacks into classes. A class of attacks takes into account both the potential malicious activities
I on physical devices and the timing parameters m and n of the attack: begin and end of the attack. We
represent a class C as a total function C ∈ [I → P(m..n)]. Intuitively, for ι ∈ I, C(ι) ⊆ m..n denotes the set
of time instants when an attack of class C may tamper with the device ι.

In order to make security assessments on our CPSs, we adopt a well-known approach called Generalized
Non Deducibility on Composition (GNDC) [17]. Thus, in our calculus CCPSA, we say that a CPS Sys tolerates
an attack A if

Sys (cid:107) A (cid:118) Sys .

In this case, the presence of the attack A, does not change the (physical and logical) observable behaviour of
the system Sys, and the attack can be considered harmless.

2Note that we focus on attackers who have already entered the CPS, and we do not consider how they gained access to
the system, e.g., by attacking an Internet-accessible controller or one of the communication protocols as a Dolev-Yao-style
attacker [16] would do.

3

On the other hand, we say that a CPS Sys is vulnerable to an attack A of class C ∈ [I → P(m..n)] if
there is a time interval m(cid:48)..n(cid:48) in which the attack becomes observable (obviously, m(cid:48) ≥ m). Formally, we
write:

Sys (cid:107) A (cid:118)m(cid:48)..n(cid:48) Sys .

We provide suﬃcient criteria to prove attack tolerance/vulnerability to attacks of an arbitrary class C.
We deﬁne a notion of most powerful physics-based attack of a given class C, Top(C), and prove that if a CPS
tolerates Top(C) then it tolerates all attacks A of class C (and of any weaker class3). Similarly, if a CPS
is vulnerable to Top(C), in the time interval m(cid:48)..n(cid:48), then no attacks of class C (or weaker) can aﬀect the
system out of that time interval. This is very useful when checking for attack tolerance/vulnerability with
respect to all attacks of a given class C.

As a third contribution, we formalise how to estimate the impact of a successful attack on a CPS. As
expected, risk assessment in industrial CPSs is a crucial phase preceding any defence strategy implementa-
tion [54]. The objective of this phase is to prioritise among vulnerabilities; this is done based on the likelihood
that vulnerabilities are exploited, and the impact on the system under attack if exploitation occurs. In this
manner, the resources can then be focused on preventing the most critical vulnerabilities [44]. We provide
a metric to estimate the maximum perturbation introduced in the system under attack with respect to its
genuine behaviour, according to its evolution law and the uncertainty of the model. Then, we prove that the
impact of the most powerful attack Top(C) represents an upper bound for the impact of any attack A of
class C (or weaker).

Finally, as a fourth contribution, we formalise a running example in Uppaal SMC [15], the statistical
extension of the Uppaal model checker [5] supporting the analysis of systems expressed as composition of
timed and/or probabilistic automata. Our goal is to test Uppaal SMC as an automatic tool for the static
security analysis of a simple but signiﬁcant CPS exposed to a number of diﬀerent physics-based attacks with
diﬀerent impacts on the system under attack. Here, we wish to remark that while we have kept our running
example simple, it is actually non-trivial and designed to describe a wide number of attacks, as will become
clear below.

This paper extends and supersedes a preliminary conference version that appeared in [39].
The Uppaal SMC models of our system and the attacks that we have found are available at the repository

https://bitbucket.org/AndreiMunteanu/cps_smc/src/.

1.4 Organisation

In Section 2, we give syntax and semantics of CCPSA. In Section 3, we provide our running example and its
formalisation in Uppaal SMC. In Section 4, we ﬁrst deﬁne our threat model for physics-based attacks, then
we use Uppaal SMC to carry out a security analysis of our running example when exposed to three diﬀerent
attacks, and, ﬁnally, we provide suﬃcient criteria for attack tolerance/vulnerability, based on a proper notion
of most powerful attack. In Section 5, we estimate the impact of attacks on CPSs and prove that the most
powerful attack of a given class has the maximum impact with respect to all attacks of the same class (or of
a weaker one). In Section 6, we draw conclusions and discuss related and future work.

2 The Calculus

In this section, we introduce our Calculus of Cyber-Physical Systems and Attacks, CCPSA, which extends
the Calculus of Cyber-Physical Systems, deﬁned in our companion papers [37, 41], with speciﬁc features to
formalise and study attacks to physical devices.
Let us start with some preliminary notation.

3Intuitively, attacks of classes weaker than C can do less with respect to attacks of class C.

4

2.1 Syntax of CCPSA
Notation 1. We use x, xk for state variables (associated to physical states of systems), c, d for communication
channels, a, ak for actuator devices, and s, sk or sensors devices.

Actuator names are metavariables for actuator devices like valve, light, etc. Similarly, sensor names are
metavariables for sensor devices, e.g., a sensor thermometer that measures a state variable called temperature,
with a given precision.

Values, ranged over by v, v(cid:48), w, are built from basic values, such as Booleans, integers and real numbers;

they also include names.

Given a generic set of names N , we write RN to denote the set of functions assigning a real value to
each name in N . For ξ ∈ RN , n ∈ N and v ∈ R, we write ξ[n (cid:55)→ v] to denote the function ψ ∈ RN such that
ψ(m) = ξ(m), for any m (cid:54)= n, and ψ(n) = v. Given two generic functions ξ1 and ξ2 with disjoint domains
N1 and N2, respectively, we denote with ξ1 ∪ ξ2 the function such that (ξ1 ∪ ξ2)(n) = ξ1(n), if n ∈ N1, and
(ξ1 ∪ ξ2)(n) = ξ2(n), if n ∈ N2.

In general, a cyber-physical system consists of: (i) a physical component (deﬁning state variables, physical
devices, physical evolution, etc.) and (ii) a cyber (or logical) component that interacts with the physical
devices (sensors and actuators) and communicates with other cyber components of the same or of other CPSs.
Physical components in CCPSA are given by two sub-components: (i) the physical state, which is supposed

to change at runtime, and (ii) the physical environment, which contains static information.4

Deﬁnition 1 (Physical state). Let X be a set of state variables, S be a set of sensors, and A be a set of
actuators. A physical state S is a triple (cid:104)ξx, ξs, ξa(cid:105), where:

• ξx ∈ RX is the state function,

• ξs ∈ RS is the sensor function,

• ξa ∈ RA is the actuator function.

All functions deﬁning a physical state are total.

The state function ξx returns the current value associated to each variable in X , the sensor function ξs
returns the current value associated to each sensor in S and the actuator function ξa returns the current
value associated to each actuator in A.

Deﬁnition 2 (Physical environment). Let X be a set of state variables, S be a set of sensors, and A be a
set of actuators. A physical environment E is a 6-tuple (cid:104)evol , meas, inv , safe, ξw, ξe(cid:105), where:

• evol : RX × RA × RX → 2RX

is the evolution map,

• meas : RX × RS → 2RS

is the measurement map,

• inv ∈ 2RX

is the invariant set,

• safe ∈ 2RX

is the safety set,

• ξw ∈ RX is the uncertainty function,

• ξe ∈ RS is the sensor-error function.

All functions deﬁning a physical environment are total functions.

4Actually, this information is periodically updated (say, every six months) to take into account possible drifts of the system.

5

The evolution map evol models the evolution law of the physical system, where changes made on actuators
may reﬂect on state variables. Given a state function, an actuator function, and an uncertainty function, the
evolution map evol returns the set of next admissible state functions. Since we assume an uncertainty in our
models, evol does not return a single state function but a set of possible state functions.

The measurement map meas returns the set of next admissible sensor functions based on the current
state function. Since we assume error-prone sensors, meas does not return a single sensor function but a set
of possible sensor functions.

The invariant set inv represents the set of state functions that satisfy the invariant of the system. A
CPS that gets into a physical state with a state function that does not satisfy the invariant is in deadlock.
Similarly, the safety set safe represents the set of state functions that satisfy the safety conditions of the
system. Intuitively, if a CPS gets into an unsafe state, then its functionality may get compromised.

The uncertainty function ξw returns the uncertainty (or accuracy) associated to each state variable. Thus,
given a state variable x ∈ X , ξw(x) returns the maximum distance between the real value of x, in an arbitrary
w(x),
moment in time, and its representation in the model. For ξw, ξ(cid:48)
if ξw(x) ≤ ξ(cid:48)
for any x ∈ X . The evolution map evol is obviously monotone with respect to uncertainty: if ξw ≤ ξ(cid:48)
then
w
evol (ξx, ξa, ξw) ⊆ evol (ξx, ξa, ξ(cid:48)

w ∈ RX , we will write ξw ≤ ξ(cid:48)
w

Finally, the sensor-error function ξe returns the maximum error associated to each sensor in S.
Let us now deﬁne formally the cyber component of a CPS in CCPSA. Our (logical) processes build on
Hennessy and Regan’s Timed Process Language TPL [27], basically, CCS enriched with a discrete notion of
time. We extend TPL with two main ingredients:

w).

• two constructs to read values detected at sensors and write values on actuators, respectively;
• special constructs to represent malicious activities on physical devices.

The remaining constructs are the same as those of TPL.

Deﬁnition 3 (Processes). Processes are deﬁned as follows:

P, Q ::= nil (cid:12)

(cid:12) tick.P (cid:12)

(cid:12) P (cid:107) Q (cid:12)

(cid:12) π.P (cid:12)

(cid:12) φ.P (cid:12)

(cid:12) (cid:98)µ.P (cid:99)Q (cid:12)

(cid:12) if (b) {P } else {Q} (cid:12)

(cid:12) P \c (cid:12)

(cid:12) H(cid:104) ˜w(cid:105)

π ::= rcv c(x) (cid:12)
φ ::= read s(x) (cid:12)
µ ::= sniﬀ s(x) (cid:12)

(cid:12) snd c(cid:104)v(cid:105)
(cid:12) write a(cid:104)v(cid:105)
(cid:12) drop a(x) (cid:12)

(cid:12) forge p(cid:104)v(cid:105) .

We write nil for the terminated process. The process tick.P sleeps for one time unit and then continues as P .
We write P (cid:107) Q to denote the parallel composition of concurrent threads P and Q. The process π.P denotes
channel transmission. The construct φ.P denotes activities on physical devices, i.e., sensor reading and
actuator writing. The process (cid:98)µ.P (cid:99)Q denotes MITM malicious activities under timeout targeting physical
devices (sensors and actuators). More precisely, we support sensor sniﬃng, drop of actuator commands, and
integrity attacks on data coming from sensors and addressed to actuators. Thus, for instance, (cid:98)drop a(x).P (cid:99)Q
drops a command on the actuator a supplied by the controller in the current time slot; otherwise, if there are
no commands on a, it moves to the next time slot and evolves into Q.

The process P \c is the channel restriction operator of CCS. We sometimes write P \{c1, c2, . . . , cn} to
mean P \c1\c2 · · · \cn. The process if (b) {P } else {Q} is the standard conditional, where b is a decidable guard.
In processes of the form tick.Q and (cid:98)µ.P (cid:99)Q, the occurrence of Q is said to be time-guarded. The process
H(cid:104) ˜w(cid:105) denotes (guarded) recursion.

We assume a set of process identiﬁers ranged over by H, H1, H2. We write H(cid:104)w1, . . . , wk(cid:105) to denote a
recursive process H deﬁned via an equation H(x1, . . . , xk) = P , where (i) the tuple x1, . . . , xk contains all
the variables that appear free in P , and (ii) P contains only guarded occurrences of the process identiﬁers,
such as H itself. We say that recursion is time-guarded if P contains only time-guarded occurrences of the
process identiﬁers. Unless explicitly stated our recursive processes are always time-guarded.

In the constructs rcv c(x).P , read s(x).P , (cid:98)sniﬀ s(x).P (cid:99)Q and (cid:98)drop a(x).P (cid:99)Q the variable x is said to be
bound. This gives rise to the standard notions of free/bound (process) variables and α-conversion. A term is

6

closed if it does not contain free variables, and we assume to always work with closed processes: the absence
of free variables is preserved at run-time. As further notation, we write T {v/x} for the substitution of all
occurrences of the free variable x in T with the value v.

Everything is in place to provide the deﬁnition of cyber-physical systems expressed in CCPSA.

Deﬁnition 4 (Cyber-physical system). Fixed a set of state variables X , a set of sensors S, and a set of
actuators A, a cyber-physical system in CCPSA is given by two main components:

• a physical component consisting of

– a physical environment E deﬁned on X , S, and A, and
– a physical state S recording the current values associated to the state variables in X , the sensors

in S, and the actuators in A;

• a cyber component P that interacts with the sensors in S and the actuators A, and can communicate,

via channels, with other cyber components of the same or of other CPSs.

We write E; S (cid:111)(cid:110) P to denote the resulting CPS, and use M and N to range over CPSs. Sometimes, when the
physical environment E is clearly identiﬁed, we write S (cid:111)(cid:110) P instead of E; S (cid:111)(cid:110) P . CPSs of the form S (cid:111)(cid:110) P are
called environment-free CPSs.

The syntax of our CPSs is slightly too permissive as a process might use sensors and/or actuators that

are not deﬁned in the physical state. To rule out ill-formed CPSs, we use the following deﬁnition.

Deﬁnition 5 (Well-formedness). Let E = (cid:104)evol , meas, inv , safe, ξw, ξe(cid:105) be a physical environment, let S =
(cid:104)ξx, ξs, ξa(cid:105) be a physical state deﬁned on a set of physical variables X , a set of sensors S, and a set of actuators
A, and let P be a process. The CPS E; S (cid:111)(cid:110) P is said to be well-formed if: (i) any sensor mentioned in P is
in the domain of the function ξs; (ii) any actuator mentioned in P is in the domain of the function ξa.

In the rest of the paper, we will always work with well-formed CPSs and use the following abbreviations.

Notation 2. We write µ.P for the process deﬁned via the equation Q = (cid:98)µ.P (cid:99)Q, where Q does not occur in
P . Further, we write

• (cid:98)µ(cid:99)Q as an abbreviation for (cid:98)µ.nil(cid:99)Q,
• (cid:98)µ.P (cid:99) as an abbreviation for (cid:98)µ.P (cid:99)nil,
• snd c and rcv c, when channel c is used for pure synchronisation,
• tickk.P as a shorthand for tick . . . tick.P , where the preﬁx tick appears k ≥ 0 consecutive times.

Finally, let M = E; S (cid:111)(cid:110) P , we write M (cid:107) Q for E; S (cid:111)(cid:110) (P (cid:107) Q), and M \c for E; S (cid:111)(cid:110) (P \c).

2.2 Labelled transition semantics

In this subsection, we provide the dynamics of CCPSA in terms of a labelled transition system (LTS) in the
SOS style of Plotkin. First, we give in Table 1 an LTS for logical processes, then in Table 2 we lift transition
rules from processes to environment-free CPSs.

In Table 1, the meta-variable λ ranges over labels in the set {tick, τ, cv, cv, a!v, s?v, (cid:69)p!v, (cid:69)p?v}. Rules
(Outp), (Inpp) and (Com) serve to model channel communication, on some channel c. Rules (Read) and (Write)
denote sensor reading and actuator writing, respectively. The following three rules model three diﬀerent
MITM malicious activities: sensor sniﬃng, dropping of actuator commands, and integrity attacks on data
coming from sensors or addressed to actuators. In particular, rule ((cid:69)ActDrop (cid:69)) models a DoS attack to the
actuator a, where the update request of the controller is dropped by the attacker and it never reaches the
actuator, whereas rule ((cid:69)SensIntegr (cid:69)) models an integrity attack on sensor s, as the controller of s is supplied
with a fake value v forged by the attack. Rule (Par) propagates untimed actions over parallel components.
Rules (Res), (Rec), (Then) and (Else) are standard. The following four rules (TimeNil), (Sleep), (TimeOut) and

7

Table 1: LTS for processes

(Inpp)

rcv c(x).P

−
cv
−−−→ P {v/x}

P

cv

−−−→ P (cid:48) Q

cv

−−−→ Q(cid:48)

P (cid:107) Q

τ
−−→ P (cid:48) (cid:107) Q(cid:48)

−

read s(x).P

s?v
−−−→ P {v/x}

(Com)

(Read)

((cid:69)Sniﬀ(cid:69))

−

(cid:98)sniﬀ s(x).P (cid:99)Q

(cid:69)s?v
−−−−→ P {v/x}

((cid:69)Drop(cid:69))

−

snd c(cid:104)v(cid:105).P

cv

−−−→ P

P

λ
−−→ P (cid:48) λ (cid:54)= tick

P (cid:107) Q

λ
−−→ P (cid:48) (cid:107) Q

(Outp)

(Par)

(Write)

−

write a(cid:104)v(cid:105).P

a!v

−−−→ P

−

(cid:98)drop a(x).P (cid:99)Q

(cid:69)a?v
−−−−→ P {v/x}

((cid:69)Forge(cid:69))

p ∈ {s, a}

(cid:98)forge p(cid:104)v(cid:105).P (cid:99)Q

(cid:69)p!v
−−−−→ P

((cid:69)ActDrop(cid:69))

P

a!v
−−−→ P (cid:48) Q

(cid:69)a?v
−−−−→ Q(cid:48)

P (cid:107) Q

τ
−−→ P (cid:48) (cid:107) Q(cid:48)

((cid:69)SensIntegr(cid:69))

(cid:69)s!v
−−−−→ P (cid:48) Q

P

s?v
−−−→ Q(cid:48)

P (cid:107) Q

τ
−−→ P (cid:48) (cid:107) Q(cid:48)

(Res)

(Then)

P

λ
−−→ P (cid:48) λ (cid:54)∈ {cv, cv}

P \c

λ
−−→ P (cid:48)\c

= true P

b
(cid:74)

(cid:75)

if (b) {P } else {Q}

λ
−−→ P (cid:48)
λ
−−→ P (cid:48)

(Rec)

(Else)

P { ˜w/˜x}

λ

−−→ Q H(˜x) = P

H(cid:104) ˜w(cid:105)

λ

−−→ Q

= false Q

b
(cid:74)

(cid:75)

if (b) {P } else {Q}

λ
−−→ Q(cid:48)
λ
−−→ Q(cid:48)

(TimeNil)

(Timeout)

−
tick

−−−→ nil

nil

−

(cid:98)µ.P (cid:99)Q

tick

−−−→ Q

(Sleep)

−

tick.P

tick

−−−→ P

(TimePar)

P

tick
−−−→ P (cid:48) Q

tick
−−−→ Q(cid:48)

P (cid:107) Q

tick
−−−→ P (cid:48) (cid:107) Q(cid:48)

(TimePar) model the passage of time. For simplicity, we omit the symmetric counterparts of the rules (Com),
((cid:69)ActDrop (cid:69)), ((cid:69)SensIntegr (cid:69)), and (Par).

In Table 2, we lift the transition rules from processes to environment-free CPSs of the form S (cid:111)(cid:110) P for
S = (cid:104)ξx, ξs, ξa(cid:105). The transition rules are parametric on a physical environment E. Except for rule (Deadlock),
all rules have a common premise ξx ∈ inv : a system can evolve only if the invariant is satisﬁed by the
current physical state. Here, actions, ranged over by α, are in the set {τ, cv, cv, tick, deadlock, unsafe}. These
actions denote: internal activities (τ ); channel transmission (cv and cv); the passage of time (tick); and
two speciﬁc physical events: system deadlock (deadlock) and the violation of the safety conditions (unsafe).
Rules (Out) and (Inp) model transmission and reception, with an external system, on a channel c. Rule
(SensRead) models the reading of the current data detected at a sensor s; here, the presence of a malicious
action (cid:69)s!w would prevent the reading of the sensor. We already said that rule ((cid:69)SensIntegr (cid:69)) of Table 1
models integrity attacks on a sensor s. However, together with rule (SensRead), it also serves to implicitly
model DoS attacks on a sensor s, as the controller of s cannot read its correct value if the attacker is
currently supplying a fake value for it. Rule ((cid:69)SensSniﬀ(cid:69)) allows the attacker to read the conﬁdential value
detected at a sensor s. Rule (ActWrite) models the writing of a value v on an actuator a; here, the presence
of an attack capable of performing a drop action (cid:69)a?v prevents the access to the actuator by the controller.
Rule ((cid:69)ActIntegr(cid:69)) models a MITM integrity attack to an actuator a, as the actuator is provided with a
value forged by the attack. Rule (Tau) lifts non-observable actions from processes to systems. This includes
communications channels and attacks’ accesses to physical devices. A similar lifting occurs in rule (Time) for

8

Table 2: LTS for CPSs S (cid:111)(cid:110) P parametric on an environment E = (cid:104)evol , meas, inv , safe, ξw, ξe(cid:105)

(Out)

S = (cid:104)ξx, ξs, ξa(cid:105) P

cv

−−−→ P (cid:48)

ξx ∈ inv

S (cid:111)(cid:110) P

cv
−−−→ S (cid:111)(cid:110) P (cid:48)

(Inp)

S = (cid:104)ξx, ξs, ξa(cid:105) P

ξx ∈ inv

cv

−−−→ P (cid:48)
cv
−−−→ S (cid:111)(cid:110) P (cid:48)

S (cid:111)(cid:110) P

(SensRead)

P

s?v
−−−→ P (cid:48)

ξs(s) = v P

(cid:69)s!v
−−−−→(cid:54)
τ
−−→ (cid:104)ξx, ξs, ξa(cid:105) (cid:111)(cid:110) P (cid:48)

ξx ∈ inv

ξx ∈ inv
ξs(s) = v
τ
−−→ (cid:104)ξx, ξs, ξa(cid:105) (cid:111)(cid:110) P (cid:48)

(cid:104)ξx, ξs, ξa(cid:105) (cid:111)(cid:110) P
(cid:69)s?v
−−−−→ P (cid:48)
P
(cid:104)ξx, ξs, ξa(cid:105) (cid:111)(cid:110) P

((cid:69)SensSniﬀ(cid:69))

(ActWrite)

P

a!v
−−−→ P (cid:48)

ξ(cid:48)
a = ξa[a (cid:55)→ v] P
(cid:104)ξx, ξs, ξa(cid:105) (cid:111)(cid:110) P

τ
−−→ (cid:104)ξx, ξs, ξ(cid:48)

a(cid:105) (cid:111)(cid:110) P (cid:48)

(cid:69)a?v
−−−−→(cid:54)

ξx ∈ inv

((cid:69)ActIntegr(cid:69))

P

(cid:69)a!v
−−−−→ P (cid:48)
ξ(cid:48)
a = ξa[a (cid:55)→ v]
(cid:104)ξx, ξs, ξa(cid:105) (cid:111)(cid:110) P

τ
−−→ (cid:104)ξx, ξs, ξ(cid:48)

ξx ∈ inv
a(cid:105) (cid:111)(cid:110) P (cid:48)

(Tau)

P
(cid:104)ξx, ξs, ξa(cid:105) (cid:111)(cid:110) P

τ
−−→ P (cid:48)
τ
−−→ (cid:104)ξx, ξs, ξa(cid:105) (cid:111)(cid:110) P (cid:48)

ξx ∈ inv

(Deadlock)

S = (cid:104)ξx, ξs, ξa(cid:105)
deadlock
−−−−−−→ S (cid:111)(cid:110) P

S (cid:111)(cid:110) P

ξx (cid:54)∈ inv

(Time)

P

tick
−−−→ P (cid:48) S = (cid:104)ξx, ξs, ξa(cid:105) S(cid:48) ∈ next(E; S)

ξx ∈ inv

S (cid:111)(cid:110) P

tick
−−−→ S(cid:48) (cid:111)(cid:110) P (cid:48)

(Safety)

S = (cid:104)ξx, ξs, ξa(cid:105)

S (cid:111)(cid:110) P

ξx (cid:54)∈ safe
unsafe
−−−−−→ S (cid:111)(cid:110) P

ξx ∈ inv

timed actions, where next(E; S) returns the set of possible physical states for the next time slot. Formally,
for E = (cid:104)evol , meas, inv , safe, ξw, ξe(cid:105) and S = (cid:104)ξx, ξs, ξa(cid:105), we deﬁne:

next(E; S) def= (cid:8)(cid:104)ξ(cid:48)

x, ξ(cid:48)

s, ξ(cid:48)

a(cid:105) : ξ(cid:48)

x ∈ evol (ξx, ξa, ξw) ∧ ξ(cid:48)

s ∈ meas(ξ(cid:48)

x, ξe) ∧ ξ(cid:48)

a = ξa

(cid:9) .

Thus, by an application of rule (Time) a CPS moves to the next physical state, in the next time slot.
Rule (Deadlock) is introduced to signal the violation of the invariant. When the invariant is violated, a
system deadlock occurs and then, in CCPSA, the system emits a special action deadlock, forever. Similarly,
rule (Safety) is introduced to detect the violation of safety conditions. In this case, the system may emit a
special action unsafe and then continue its evolution.

Summarising, in the LTS of Table 2 we deﬁne transitions rules of the form S (cid:111)(cid:110) P

on some physical environment E. As physical environments do not change at runtime, S (cid:111)(cid:110) P
α
−−→ E; S(cid:48) (cid:111)(cid:110) P (cid:48), thus providing the LTS for all CPSs in CCPSA.
entails E; S (cid:111)(cid:110) P

α
−−→ S(cid:48) (cid:111)(cid:110) P (cid:48), parametric
α
−−→ S(cid:48) (cid:111)(cid:110) P (cid:48)

Remark 1. Note that our operational semantics ensures that malicious actions of the form (cid:69)s!v (integrity/DoS
attack on sensor s) or (cid:69)a?v (DoS attack on actuator a) have a pre-emptive power. These attacks can always
prevent the regular access to a physical device by its controller.

2.3 Behavioural semantics

Having deﬁned the actions that can be performed by a CPS of the form E; S (cid:111)(cid:110) P , we can easily concatenate
these actions to deﬁne the possible execution traces of the system. Formally, given a trace t = α1 . . . αn, we
αn−−−→, and we will use the function #tick(t) to get the number
t
will write
−−→ as an abbreviation for
of occurrences of the action tick in t.

α1−−−→ . . .

9

The notion of trace allows us to provide a formal deﬁnition of system soundness: a CPS is said to be

sound if it never deadlocks and never violates the safety conditions.

Deﬁnition 6 (System soundness). Let M be a well-formed CPS. We say that M is sound if whenever
M

−−→ M (cid:48), for some t, the actions deadlock and unsafe never occur in t.

t

In our security analysis, we will always focus on sound CPSs.
We recall that the observable activities in CCPSA are: time passing, system deadlock, violation of safety
conditions, and channel communication. Having deﬁned a labelled transition semantics, we are ready to
formalise our behavioural semantics, based on execution traces.

α==⇒ means
We adopt a standard notation for weak transitions: we write =⇒ for (
−−→=⇒, and ﬁnally ˆα=⇒ denotes =⇒ if α = τ and α=⇒ otherwise. Given a trace t = α1. . .αn, we write

τ
−−→)∗, whereas

α

=⇒
ˆt==⇒ as an abbreviation for (cid:99)α1===⇒ . . . (cid:99)αn===⇒.

Deﬁnition 7 (Trace preorder). We write M (cid:118) N if whenever M

t

−−→ M (cid:48), for some t, there is N (cid:48) such that

ˆt==⇒ N (cid:48).

N

Remark 2. Unlike other process calculi, in CCPSA our trace preorder is able to observe (physical) deadlock due
to the presence of the rule (Deadlock) and the special action deadlock: whenever M (cid:118) N then M eventually
deadlocks if and only if N eventually deadlocks (see Lemma 1 in the appendix).

Our trace preorder can be used for compositional reasoning in those contexts that don’t interfere on physical
devices (sensors and actuators) while they may interfere on logical components (via channel communication).
In particular, trace preorder is preserved by parallel composition of physically-disjoint CPSs, by parallel
composition of pure-logical processes, and by channel restriction. Intuitively, two CPSs are physically-disjoint
if they have diﬀerent plants but they may share logical channels for communication purposes. More precisely,
physically-disjoint CPSs have disjoint state variables and disjoint physical devices (sensors and actuators).
As we consider only well-formed CPSs (Deﬁnition 5), this ensures that a CPS cannot physically interfere
with a parallel CPS by acting on its physical devices.
x, ξi

e(cid:105) be physical states and physical
environments, respectively, associated to sets of state variables Xi, sets of sensors Si, and sets of actuators
Ai, for i ∈ {1, 2}. For X1 ∩ X2 = ∅, S1 ∩ S2 = ∅ and A1 ∩ A2 = ∅, we deﬁne:

a(cid:105) and Ei = (cid:104)evol i, meas i, inv i, safe i, ξi

Formally, let Si = (cid:104)ξi

w, ξi

s, ξi

such that: ξx = ξ1

• the disjoint union of the physical states S1 and S2, written S1 (cid:93) S2, to be the physical state (cid:104)ξx, ξs, ξa(cid:105)
a ∪ ξ2
a
• the disjoint union of the physical environments E1 and E2, written E1 (cid:93) E2, to be the physical

, and ξa = ξ1

, ξs = ξ1

s ∪ ξ2
s

x ∪ ξ2
x

;

environment (cid:104)evol , meas, inv , safe, ξw, ξe(cid:105) such that:

1. evol = evol 1 ∪ evol 2
2. meas = meas 1 ∪ meas 2
3. S1 (cid:93) S2 ∈ inv iﬀ S1 ∈ inv 1 and S2 ∈ inv 2
4. S1 (cid:93) S2 ∈ safe iﬀ S1 ∈ safe 1 and S2 ∈ safe 2
5. ξw = ξ1
6. ξe = ξ1

w ∪ ξ2
w
.
e ∪ ξ2
e

Deﬁnition 8 (Physically-disjoint CPSs). Let Mi = Ei; Si (cid:111)(cid:110) Pi, for i ∈ {1, 2}. We say that M1 and M2 are
physically-disjoint if S1 and S2 have disjoint sets of state variables, sensors and actuators. In this case, we
write M1 (cid:93) M2 to denote the CPS deﬁned as (E1 (cid:93) E2); (S1 (cid:93) S2) (cid:111)(cid:110) (P1 (cid:107) P2).

10

A pure-logical process is a process that may interfere on communication channels but it never interferes
on physical devices as it never accesses sensors and/or actuators. Basically, a pure-logical process is a TPL
process [27]. Thus, in a system M (cid:107) Q, where M is an arbitrary CPS, a pure-logical process Q cannot interfere
with the physical evolution of M . A process Q can, however, deﬁnitely interact with M via communication
channels, and hence aﬀect its observable behaviour.

Deﬁnition 9 (Pure-logical processes). A process P is called pure-logical if it never acts on sensors and/or
actuators.

Now, we can ﬁnally state the compositionality of our trace preorder (cid:118) (the proof can be found in the

appendix).

Theorem 1 (Compositionality of (cid:118)). Let M and N be two arbitrary CPSs in CCPSA.

1. M (cid:118) N implies M (cid:93) O (cid:118) N (cid:93) O, for any physically-disjoint CPS O;

2. M (cid:118) N implies M (cid:107) P (cid:118) N (cid:107) P , for any pure-logical process P ;

3. M (cid:118) N implies M \c (cid:118) N \c, for any channel c.

The reader may wonder whether our trace preorder (cid:118) is preserved by more permissive contexts. The
answer is no. Suppose that in the second item of Theorem 1 we allowed a process P that can also read on
sensors. In this case, even if M (cid:118) N , the parallel process P might read a diﬀerent value in the two systems at
the very same sensor s (due to the sensor error) and transmit these diﬀerent values on a free channel, breaking
the congruence. Activities on actuators may also lead to diﬀerent behaviours of the compound systems:
M and N may have physical components that are not exactly aligned. A similar reasoning applies when
composing CPSs with non physically-disjoint ones: noise on physical devices may break the compositionality
result.

As we are interested in formalising timing aspects of attacks, such as beginning and duration, we propose
a timed variant of (cid:118) up to (a possibly inﬁnite) discrete time interval m..n, with m ∈ N+ and n ∈ N+ ∪ ∞.
Intuitively, we write M (cid:118)m..n N if the CPS N simulates the execution traces of M in all time slots, except
for those contained in the discrete time interval m..n.

Deﬁnition 10 (Trace preorder up to a time interval). We write M (cid:118)m..n N , for m ∈ N+ and n ∈ N+ ∪ {∞},
with m ≤ n, if the following conditions hold:

• m is the minimum integer for which there is a trace t, with #tick(t)=m−1, s.t. M

−−→ and N (cid:54)

t

ˆt==⇒;

• n is the inﬁmum element of N+ ∪ {∞}, n ≥ m, such that whenever M

t1−−→ M (cid:48), with #tick(t1) = n − 1,

there is t2, with #tick(t1) = #tick(t2), such that N

t2−−→ N (cid:48), for some N (cid:48), and M (cid:48) (cid:118) N (cid:48).

In Deﬁnition 10, the ﬁrst item says that N can simulate the traces of M for at most m−1 time slots;
whereas the second item says two things: (i) in time interval m..n the simulation does not hold; (ii) starting
from the time slot n+1 the CPS N can simulate again the traces of M . Note that inf(∅) = ∞. Thus, if
M (cid:118)m..∞ N , then N simulates M only in the ﬁrst m − 1 time slots.
Theorem 2 (Compositionality of (cid:118)m..n). Let M and N be two arbitrary CPSs in CCPSA.

1. M (cid:118)m..n N implies that for any physically-disjoint CPS there are m(cid:48), n(cid:48) ∈ N+ ∪ ∞, with m(cid:48)..n(cid:48) ⊆ m..n

such that M (cid:93) O (cid:118)m(cid:48)..n(cid:48) N (cid:93) O;

2. M (cid:118)m..n N implies that for any pure-logical process P there are m(cid:48), n(cid:48) ∈ N+ ∪ ∞, with m(cid:48)..n(cid:48) ⊆ m..n

such that M (cid:107) P (cid:118)m(cid:48)..n(cid:48) N (cid:107) P ;

3. M (cid:118)m..n N implies that for any channel c there are m(cid:48), n(cid:48) ∈ N+ ∪ ∞, with m(cid:48)..n(cid:48) ⊆ m..n such that

M \c (cid:118)m(cid:48)..n(cid:48) N \c.

The proof can be found in the appendix.

11

Actuator

Engine

cool

Ctrl

sync

IDS

Sensor

st

Figure 2: The main structure of the CPS Sys

3 A Running Example

In this section, we introduce a running example to illustrate how we can precisely represent CPSs and a
variety of diﬀerent physics-based attacks. In practice, we formalise a relatively simple CPS Sys in which
the temperature of an engine is maintained within a speciﬁc range by means of a cooling system. We wish
to remark here that while we have kept the example simple, it is actually far from trivial and designed to
describe a wide number of attacks. The main structure of the CPS Sys is shown in Figure 2.

3.1 The CPS Sys

The physical state State of the engine is characterised by: (i) a state variable temp containing the current
temperature of the engine, and an integer state variable stress keeping track of the level of stress of the
mechanical parts of the engine due to high temperatures (exceeding 9.9 degrees); this integer variable ranges
from 0, meaning no stress, to 5, for high stress; (ii) a sensor st (such as a thermometer or a thermocouple)
measuring the temperature of the engine, (iii) an actuator cool to turn on/oﬀ the cooling system.

The physical environment of the engine, Env , is constituted by: (i) a simple evolution law evol that
increases (respectively, decreases) the value of temp by one degree per time unit, when the cooling system is
inactive (respectively, active), up to the uncertainty of the system; the variable stress is increased each time
the current temperature is above 9.9 degrees, and dropped to 0 otherwise; (ii) a measurement map meas
returning the value detected by the sensor st, up to the error associated to the sensor; (iii) an invariant set
saying that the system gets faulty when the temperature of the engine gets out of the range [0, 50], (iv) a
safety set to express that the system moves to an unsafe state when the level of stress reaches the threshold
5, (v) an uncertainty function in which each state variable may evolve with an uncertainty δ = 0.4 degrees,
(vi) a sensor-error function saying that the sensor st has an accuracy (cid:15) = 0.1 degrees.

Formally, State = (cid:104)ξx, ξs, ξa(cid:105) where:

• ξx ∈ R{temp,stress} and ξx(temp) = 0 and ξx(stress) = 0;

• ξs ∈ R{st} and ξs(st) = 0;

• ξa ∈ R{cool} and ξa(cool ) = oﬀ; for the sake of simplicity, we can assume ξa to be a mapping

{cool } → {on, oﬀ} such that ξa(cool ) = oﬀ if ξa(cool ) ≥ 0, and ξa(cool ) = on if ξa(cool ) < 0;

and Env = (cid:104)evol , meas, inv , safe, ξw, ξe(cid:105) with:

• evol (ξi

x, ξi

a, ξw) is the set of functions ξ ∈ R{temp,stress} such that:

– ξ(temp) = ξi

x(temp) + heat(ξi
(active cooling), and heat(ξi

– ξ(stress) = min(5 , ξi

a, cool ) + γ, with γ ∈ [−δ, +δ] and heat(ξi
a, cool ) = +1 if ξi

a(cool ) = oﬀ (inactive cooling);
x(temp) > 9.9; ξ(stress) = 0, otherwise;
x(temp) + (cid:15)](cid:9);

x(stress)+1) if ξi
x(temp) − (cid:15) , ξi

• meas(ξi

• inv = {ξi

x, ξe) = (cid:8)ξ : ξ(st) ∈ [ξi
x : 0 ≤ ξi

x(temp) ≤ 50};

a, cool ) = −1 if ξi

a(cool ) = on

12

Figure 3: Three possible evolutions of the CPS Sys

• safe = {ξi

x : ξi

x(stress) < 5} (we recall that the stress threshold is 5);

• ξw ∈ R{temp,stress}, ξw(temp) = 0.4 = δ and ξw(stress) = 0;

• ξe ∈ R{st} and ξe(st) = 0.1 = (cid:15).

For the cyber component of the CPS Sys, we deﬁne two parallel processes: Ctrl and IDS . The former
models the controller activity, consisting in reading the temperature sensor and in governing the cooling
system via its actuator, whereas the latter models a simple intrusion detection system that attempts to
detect and signal anomalies in the behaviour of the system [23]. Intuitively, Ctrl senses the temperature
of the engine at each time slot. When the sensed temperature is above 10 degrees, the controller activates
the coolant. The cooling activity is maintained for 5 consecutive time units. After that time, the controller
synchronises with the IDS component via a private channel sync, and then waits for instructions, via a
channel ins. The IDS component checks whether the sensed temperature is still above 10. If this is the case,
it sends an alarm of “high temperature”, via a speciﬁc channel, and then tells Ctrl to keep cooling for 5
more time units; otherwise, if the temperature is not above 10, the IDS component requires Ctrl to stop the
cooling activity.

Ctrl = read st(x).if (x > 10) {Cooling} else {tick.Ctrl }

Cooling = write cool(cid:104)on(cid:105).tick5.Check

Check = snd sync.rcv ins(y).if (y = keep_cooling) {tick5.Check } else {write cool (cid:104)oﬀ(cid:105).tick.Ctrl }

IDS = rcv sync.read st(x).if (x > 10) {snd alarm(cid:104)high_temp(cid:105).snd ins(cid:104)keep_cooling(cid:105).tick.IDS }

else {snd ins(cid:104)stop(cid:105).tick.IDS } .

Thus, the whole CPS is deﬁned as:

Sys = Env ; State (cid:111)(cid:110) (Ctrl (cid:107) IDS )\{sync, ins}

For the sake of simplicity, our IDS component is quite basic: for instance, it does not check whether the
temperature is too low. However, it is straightforward to replace it with a more sophisticated one, containing
more informative tests on sensor values and/or on actuators commands.

Figure 3 shows three possible evolutions in time of the state variable temp of Sys: (i) the ﬁrst one (in
red), in which the temperature of the engine always grows of 1 − δ = 0.6 degrees per time unit, when the
cooling is oﬀ, and always decrease of 1 + δ = 1.4 degrees per time unit, when the cooling is on; (ii) the second

13

time01020304050actual temperature (deg)024681012Figure 4: Uppaal SMC model for the physical component of Sys

one (in blue), in which the temperature always grows of 1 + δ = 1.4 degrees per time unit, when the cooling
is oﬀ, and always decreases of 1 − δ = 0.6 degrees per time unit, when the cooling is on; (iii) and a third one
(in yellow), in which, depending on whether the cooling is oﬀ or on, at each time step the temperature grows
or decreases of an arbitrary oﬀset lying in the interval [1 − δ, 1 + δ].

Our operational semantics allows us to formally prove a number of properties of our running example.

For instance, Proposition 1 says that the Sys is sound and it never ﬁres the alarm.

Proposition 1. If Sys

t

−−→ for some trace t = α1 . . . αn, then αi ∈ {τ, tick}, for any i ∈ {1, . . . , n}.

Actually, we can be quite precise on the temperature reached by Sys before and after the cooling: in each
of the 5 rounds of cooling, the temperature will drop of a value lying in the real interval [1−δ, 1+δ], where δ
is the uncertainty.

Proposition 2. For any execution trace of Sys, we have:

• when Sys turns on the cooling, the value of the state variable temp ranges over (9.9, 11.5];
• when Sys turns oﬀ the cooling, the value of the variable temp ranges over (2.9, 8.5].

The proofs of the Propositions 1 and 2 can be found in the appendix. In the following section, we will
verify the safety properties stated in these two propositions relying on the statistical model checker Uppaal
SMC [15].

3.2 A formalisation of Sys in Uppaal SMC
In this section, we formalise our running example in Uppaal SMC [15], the statistical extension of the
Uppaal model checker [5] supporting the analysis of systems expressed as composition of timed and/or
probabilistic automata. In Uppaal SMC, the user must specify two main statistical parameters α and (cid:15),
ranging in the interval [0, 1], and representing the probability of false negatives and probabilistic uncertainty,
respectively. Thus, given a CTL property of the system under investigation, the tool returns a probability
estimate for that property, lying in a conﬁdence interval [p − (cid:15), p + (cid:15)], for some probability p ∈ [0, 1], with an
accuracy 1 − α. The number of necessary runs to ensure the required accuracy is then computed by the tool
relying on the Chernoﬀ-Hoeﬀding theory [12].

14

Figure 5: Uppaal SMC model for the network component of Sys

Figure 6: Uppaal SMC model for the logical component of Sys

3.2.1 Model

The Uppaal SMC model of our use case Sys is given by three main components represented in terms of
parallel timed automata: the physical component, the network, and the logical component.

The physical component, whose model is shown in Figure 4, consists of four automata: (i) the _Engine_
automaton that governs the evolution of the variable temp by means of the heat and cool functions; (ii) the
_Sensor_ automaton that updates the global variable sens at each measurement request; (iii) the _Actuator_
automaton that activates/deactivates the cooling system; (iv) the _Safety_ automaton that handles the
integer variable stress, via the update_stress function, and the Boolean variables safe and deadlocks, associated
to the safety set safe and the invariant set inv of Sys, respectively.5 We also have a small automaton to
model a discrete notion of time (via a synchronisation channel tick) as the evolution of state variables is
represented via diﬀerence equations.

The network, whose model is given in Figure 5, consists of two proxies: a proxy to relay actuator commands
between the actuator device and the controller, a second proxy to relay measurement requests between the
sensor device and the logical components (controller and IDS).

The logical component, whose model is given in Figure 6, consists of two automata: _Ctrl_ and _IDS_
to model the controller and the Intrusion Detection System, respectively; both of them synchronise with their
associated proxy copying a fresh value of sens into their local variables (sens_ctrl and sens_ids, respectively).
Under proper conditions, the _IDS _ automaton ﬁres alarms by setting a Boolean variable alarm.

5In Section 6.2, we explain why we need to implement an automaton to check for safety conditions rather than verifying a

safety property.

15

3.2.2 Veriﬁcation

We conduct our safety veriﬁcation using a notebook with the following set-up: (i) 2.8 GHz Intel i7 7700 HQ,
with 16 GB memory, and Linux Ubuntu 16.04 operating system; (ii) Uppaal SMC model-checker 64-bit,
version 4.1.19. The statistical parameters of false negatives (α) and probabilistic uncertainty ((cid:15)) are both
set to 0.01, leading to a conﬁdence level of 99%. As a consequence, having ﬁxed these parameters, for each
of our experiments, Uppaal SMC run a number of runs that may vary from a few hundreds to 26492 (cf.
Chernoﬀ-Hoeﬀding bounds).

We basically use Uppaal SMC to verify properties expressed in terms of time bounded CTL formulae of the
6, where t1 and t2 are time instants according to the discrete representation
form (cid:3)[t1,t2]eprop and ♦[0,t2]eprop
of time in Uppaal SMC. In practice, we use formulae of the form (cid:3)[t1,t2]eprop to compute the probability
that a property eprop
7 holds in all time slots of the time interval t1..t2, whereas with formulae of the form
♦[0,t2]eprop we calculate the probability that a property eprop holds in a least one time slot of the time interval
0..t2.

Thus, instead of proving Proposition 1, we verify, with a 99% accuracy, that in all possible executions
that are at most 1000 time slots long, the system Sys results to be sound and alarm free, with probability
0.99. Formally, we verify the following three properties:

• (cid:3)[1,1000](¬deadlocks), expressing that the system does not deadlock;

• (cid:3)[1,1000](safe), expressing that the system does not violate the safety conditions;

• (cid:3)[1,1000](¬alarm), expressing that the IDS does not ﬁre any alarm.

Furthermore, instead of Proposition 2, we verify, with the same accuracy and for runs of the same length
(up to a short initial transitory phase lasting 5 time instants) that if the cooling system is oﬀ, then the
temperature of the engine lies in the real interval (2.9, 8.5], otherwise it ranges over the interval (9.9, 11.5].
Formally, we verify the following two properties:

• (cid:3)[5,1000](Cooling_oﬀ =⇒ (temp > 2.9 ∧ temp ≤ 8.5))

• (cid:3)[5,1000](Cooling_on =⇒ (temp > 9.9 ∧ temp ≤ 11.5)).

The veriﬁcation of each of the ﬁve properties above requires around 15 minutes. The Uppaal SMC models of
our system and the attacks discussed in the next section are available at the repository

https://bitbucket.org/AndreiMunteanu/cps_smc/src/.

Remark 3. In our Uppaal SMC model we decided to represent both uncertainty of physical evolution (in
the functions heat and cool of _Engine_) and measurement noise (in _Sensor_) in a probabilistic manner
via random extractions. Here, the reader may wonder whether it would have been enough to restrict our SMC
analysis by considering only upper and lower bounds on these two quantities. Actually, this is not the case
because such a restricted analysis might miss admissible execution traces. To see this, suppose to work with a
physical uncertainty that is always either 0.4 or −0.4. Then, the temperature reached by the system would
always be of the form n.k, for n, k ∈ N and k even. As a consequence, our analysis would miss all execution
traces in which the system reaches the maximum admissible temperature of 11.5 degrees.

4 Physics-based Attacks

In this section, we use CCPSA to formalise a threat model of physics-based attacks, i.e., attacks that can
manipulate sensor and/or actuator signals in order to drive a sound CPS into an undesired state [55]. An
attack may have diﬀerent levels of access to physical devices; for example, it might be able to get read access

6The 0 in the left-hand side of the time interval is imposed by the syntax of Uppaal SMC.
7eprop is a side-eﬀect free expression over variables (e.g., clock variables, location names and primitive variables) [5].

16

to the sensors but not write access; or it might get write-only access to the actuators but not read-access.
This level of granularity is very important to model precisely how physics-based attacks can aﬀect a CPS [13].
In CCPSA, we have a syntactic way to distinguish malicious processes from honest ones.

Deﬁnition 11 (Honest system). A CPS E; S (cid:111)(cid:110) P is honest if P is honest, where P is honest if it does not
contain constructs of the form (cid:98)µ.P1(cid:99)P2.

We group physics-based attacks in classes that describe both the malicious activities and the timing
aspects of the attack. Intuitively, a class of attacks provides information about which physical devices are
accessed by the attacks of that class, how they are accessed (read and/or write), when the attack begins and
when the attack ends. Thus, let I be the set of all possible malicious activities on the physical devices of a
system, m ∈ N+ be the time slot when an attack starts, and n ∈ N+ ∪ {∞} be the time slot when the attack
ends. We then say that an attack A is of class C ∈ [I → P(m..n)] if:

1. all possible malicious activities of A coincide with those contained in I;

2. the ﬁrst of those activities may occur in the mth time slot but not before;

3. the last of those activities may occur in the nth time slot but not after;

4. for ι ∈ I, C(ι) returns a (possibly empty) set of time slots when A may read/tamper with the device ι

(this set is contained in m..n);

5. C is a total function, i.e., if no attacks of class C can achieve the malicious activity ι ∈ I, then C(ι) = ∅.

Deﬁnition 12 (Class of attacks). Let I = {(cid:69)p? : p ∈ S ∪ A} ∪ {(cid:69)p! : p ∈ S ∪ A} be the set of all possible
malicious activities on physical devices. Let m ∈ N+, n ∈ N+ ∪ {∞}, with m ≤ n. A class of attacks
C ∈ [I → P(m..n)] is a total function such that for any attack A of class C we have:

(i) C(ι) = {k : A

t
−−→

ιv

−−−→ A(cid:48) ∧ k = #tick(t) + 1}, for ι ∈ I,

(ii) m = inf{ k : k ∈ C(ι) ∧ ι ∈ I },

(iii) n = sup{ k : k ∈ C(ι) ∧ ι ∈ I }.

Along the lines of [17], we can say that an attack A aﬀects a sound CPS M if the execution of the
compound system M (cid:107) A diﬀers from that of the original system M , in an observable manner. Basically, a
physics-based attack can inﬂuence the system under attack in at least two diﬀerent ways:

• The system M (cid:107) A might deadlock when M may not; this means that the attack A aﬀects the availability
of the system. We recall that in the context of CPSs, deadlock is a particular severe physical event.

• The system M (cid:107) A might have non-genuine execution traces containing observables (violations of safety
conditions or communications on channels) that can’t be reproduced by M ; here the attack aﬀects the
integrity of the system behaviour.

Deﬁnition 13 (Attack tolerance/vulnerability). Let M be an honest and sound CPS. We say that M is
tolerant to an attack A if M (cid:107) A (cid:118) M . We say that M is vulnerable to an attack A if there is a time
interval m..n, with m ∈ N+ and n ∈ N+ ∪ {∞}, such that M (cid:107) A (cid:118)m..n M .

Thus, if a system M is vulnerable to an attack A of class C ∈ [I → P(m..n)], during the time interval
m(cid:48)..n(cid:48), then the attack operates during the interval m..n but it inﬂuences the system under attack in the
time interval m(cid:48)..n(cid:48) (obviously, m(cid:48) ≥ m). If n(cid:48) is ﬁnite, then we have a temporary attack, otherwise we have a
permanent attack. Furthermore, if m(cid:48) − n is big enough and n − m is small, then we have a quick nasty attack
that aﬀects the system late enough to allow attack camouﬂages [24]. On the other hand, if m(cid:48) is signiﬁcantly
smaller than n, then the attack aﬀects the observable behaviour of the system well before its termination and
deadlock
−−−−−−−→,
the CPS has good chances of undertaking countermeasures to stop the attack. Finally, if M (cid:107) A

t
−−→

17

Figure 7: Uppaal SMC model for the attacker Am of Example 1

for some trace t, then we say that the attack A is lethal, as it is capable to halt (deadlock) the CPS M . This
is obviously a permanent attack.

Note that, according to Deﬁnition 13, the tolerance (or vulnerability) of a CPS also depends on the
capability of the IDS component to detect and signal undesired physical behaviours. In fact, the IDS
component might be designed to detect abnormal physical behaviours going well further than deadlocks and
violations of safety conditions.

According to the literature, we say that an attack is stealthy if it is able to drive the CPS under attack
into an incorrect physical state (either deadlock or violation of the safety conditions) without being noticed
by the IDS component.

4.1 Three diﬀerent attacks on the physical devices of the CPS Sys

In this subsection, we present three diﬀerent attacks to the CPS Sys described in Section 3.

Here, we use Uppaal SMC to verify the models associated to the system under attack in order to detect

deadlocks, violations of safety conditions, and IDS failures.

Example 1. Consider the following DoS/Integrity attack on the the actuator cool , of class C ∈ [I →
P(m..m)] with C((cid:69)cool?) = C((cid:69)cool!) = {m} and C(ι) = ∅, for ι (cid:54)∈ {(cid:69)cool?, (cid:69)cool!}:

Am = tickm−1.(cid:98)drop cool(x).if (x=oﬀ) {forge cool(cid:104)oﬀ(cid:105)} else {nil}(cid:99) .

Here, the attack Am operates exclusively in the mth time slot, when it tries to drop an eventual cooling
command (on or oﬀ ) coming from the controller, and fabricates a fake command to turn oﬀ the cooling
system. Thus, if the controller sends in the mth time slot a command to turn oﬀ the coolant, then nothing
bad happens as the attack will put the same message back. On the hand, if the controller sends a command to
turn the cooling on, then the attack will drop the command. We recall that the controller will turn on the
cooling only if the sensed temperature is greater than 10 (and hence temp > 9.9); this may happen only if
m > 8. Since the command to turn the cooling on is never re-sent by Ctrl , the temperature will continue to
rise, and after only 4 time units the system may violate the safety conditions emitting an action unsafe, while
the IDS component will start sending alarms every 5 time units, until the whole system deadlocks because the
temperature reaches the threshold of 50 degrees. Here, the IDS component of Sys is able to detect the attack
with only one time unit delay.

Proposition 3. Let Sys be our running example and Am be the attack deﬁned in Example 1. Then,

• Sys (cid:107) Am (cid:118) Sys, for 1 ≤ m ≤ 8,

• Sys (cid:107) Am (cid:118)m+4..∞ Sys, for m > 8.

In order to support the statement of Proposition 3 (see also proof in appendix) we verify our Uppaal SMC
model of Sys in which the communication network used by the controller to access the actuator is compromised.
More precisely, we replace the _Proxy_Actuator_ automaton of Figure 5 with a compromised one, provided
in Figure 7, that implements the malicious activities of the MITM attacker Am of Example 1.

18

Figure 8: Probability results of ♦[0,m](Cooling_on ∧ global_clock ≥ m) by varying m in 1..300

We have done our analysis, with a 99% accuracy, for execution traces that are at most 1000 time units

long and restricting the attack time m in the time interval 1..300. The results of our analysis are:

• when m ∈ 1..8, the attack is harmless as the system results to be safe, deadlock free and alarm free,

with probability 0.99;

• when m ∈ 9..300, we have the following situation:

– the probability that at the attack time m the controller sends a command to activate the cooling
system (thus, triggering the attacker that will drop the command) can be obtained by verifying
the property ♦[0,m](Cooling_on ∧ global_clock ≥ m); as shown in Figure 8, when m grows in
the time interval 1..300, the resulting probability stabilises around the value 0.096;

– up to the m+3th time slot the system under attack remains safe, i.e., both properties (cid:3)[1,m+3](safe)

and (cid:3)[1,m+3](¬deadlock ) hold with probability 0.99;

– up to the m+4th time slot no alarms are ﬁred, i.e., the property (cid:3)[1,m+4](¬alarm) holds with

probability 0.99 (no false positives);

– in the m+4th time slot the system under attack might become unsafe as the probability, for
m ∈ 9..300, that the property ♦[0,m+4](¬safe) is satisﬁed stabilises around the value 0.095;8
– in the m+5th time slot the IDS may ﬁre an alarm as the probability, for m ∈ 9..300, that the

property ♦[0,m+5](alarm) is satisﬁed stabilises around the value 0.094;9

– the system under attack may deadlock as the property ♦[0,1000](deadlocks) is satisﬁed with

probability 0.096.10

8Since this probability coincides with that of ♦[0,m](Cooling_on ∧ global_clock ≥ m), it appears very likely that the
activation of the cooling system in the mth time slot triggers the attacker whose activity drags the system into an unsafe state
with a delay of 4 time slots.

9As the two probabilities are pretty much the same, and (cid:3)[1,m+3](safe) and (cid:3)[1,m+4](¬alarm) hold, the IDS seems to be

quite eﬀective in detecting the violations of the safety conditions in the m+4th time slot, with only one time slot delay.

10Since the probabilities are still the same, we argue that when the system reaches an unsafe state then it is not able to recover

and it is doomed to deadlock.

19

0850100150200250Attack time0.00.20.40.60.81.0ProbabilityFigure 9: Uppaal SMC model for the attacker Am of Example 2

Example 2. Consider the following DoS/Integrity attack to the sensor st, of class C ∈ [I → P(2..∞)] such
that C((cid:69)st?) = {2}, C((cid:69)st!) = 2..∞ and C(ι) = ∅, for ι (cid:54)∈ {(cid:69)st!, (cid:69)st?}. The attack begins is activity in the
time slot m, with m > 8, and then never stops:

Am = tickm−1.A

A = (cid:98)sniﬀ st(x).if (x ≤ 10) {B(cid:104)x(cid:105)} else {tick.A}(cid:99)

B(y) = (cid:98)forge st(cid:104)y(cid:105).tick.B(cid:104)y(cid:105)(cid:99)B(cid:104)y(cid:105) .

Here, the attack Am behaves as follows. It sleeps for m − 1 time slots and then, in the following time slot,
it sniﬀs the current temperature at sensor st. If the sensed temperature v is greater than 10, then it moves to
the next time slot and restarts sniﬃng; otherwise from that time on it will keep sending the same temperature
v to the logical components (controller and IDS). Actually, once the forgery activity starts, the process Ctrl
will always receive a temperature below 10 and will never activate the cooling system (and consequently the
IDS). As a consequence, the system under attack Sys (cid:107) A will ﬁrst move to an unsafe state until the invariant
will be violated and the system will deadlock. Indeed, in the worst execution scenario, already in the m+1th
time slot the temperature may exceed 10 degrees, and after 4 tick-actions, in the m+5th time slot, the system
may violate the safety conditions emitting an unsafe action. Since the temperature will keep growing without
any cooling activity, the deadlock of the CPS cannot be avoided. This is a lethal attack, as it causes a shut
down of the system; it is also a stealthy attack as it remains undetected because the IDS never gets into
action.

Proposition 4. Let Sys be our running example and Am, for m > 8, be the attack deﬁned in Example 2.
Then Sys (cid:107) Am (cid:118)m+5..∞ Sys.

Here, we verify the Uppaal SMC model of Sys in which we assume that its sensor device is compromised
(we recall that our MITM forgery attack on sensors or actuators can be assimilated to device compromise).
The interested reader may ﬁnd the proof in the appendix. In particular, we replace the _Sensor_ automaton
of Figure 4 with a compromised one, provided in Figure 9, and implementing the malicious activities of the
MITM attacker Am of Example 2.

We have done our analysis, with a 99% accuracy, for execution traces that are at most 1000 time units

long and restricting the attack time m in the integer interval 9..300. The results of our analysis are:

• up to the m+4th time slot the system under attack remains safe, deadlock free, and alarm free, i.e., all
three properties (cid:3)[1,m+4](safe), (cid:3)[1,m+4](¬deadlock ), and (cid:3)[1,m+4](¬alarm) hold with probability 0.99;

• in the m+5th time slot the system under attack might become unsafe as the probability, for m ∈ 9..300,

that the property ♦[0,m+5](¬safe) is satisﬁed stabilises around 0.104;

• the system under attack will eventually deadlock not later that 80 time slots after the attack time m,

as the property (cid:3)[m+80,1000](deadlocks) is satisﬁed with probability 0.99;

• ﬁnally, the attack is stealthy as the property (cid:3)[1,1000](¬alarm) holds with probability 0.99.

20

Figure 10: Uppaal SMC model for the attacker An of Example 3

Now, let us examine a similar but less severe attack.

Example 3. Consider the following DoS/Integrity attack to sensor st, of class C ∈ [I → P(1..n)], with
C((cid:69)st!) = C((cid:69)st?) = 1..n and C(ι) = ∅, for ι (cid:54)∈ {(cid:69)st!, (cid:69)st?}:

An = (cid:98)sniﬀ st(x).(cid:98)forge st(cid:104)x−4(cid:105).tick.An−1(cid:99)An−1(cid:99)An−1,
A0 = nil .

for n > 0

In this attack, for n consecutive time slots, An sends to the logical components (controller and IDS) the
current sensed temperature decreased by an oﬀset 4. The eﬀect of this attack on the system depends on the
duration n of the attack itself: (i) for n ≤ 8, the attack is harmless as the variable temp may not reach
a (critical) temperature above 9.9; (ii) for n = 9, the variable temp might reach a temperature above 9.9
in the 9th time slot, and the attack would delay the activation of the cooling system of one time slot; as a
consequence, the system might get into an unsafe state in the time interval 14..15, but no alarm will be ﬁred;
(iii) for n ≥ 10, the system may get into an unsafe state in the time slot 14 and in the following n + 11 time
slots; in this case, this would not be stealthy attack as the IDS will ﬁre the alarm with a delay of at most two
time slots later, rather this is a temporary attack that ends in the time slot n + 11.

Proposition 5. Let Sys be our use case and An be the attack deﬁned in Example 3. Then:

• Sys (cid:107) An (cid:118) Sys, for n ≤ 8,

• Sys (cid:107) An (cid:118)14..15 Sys, for n = 9,

• Sys (cid:107) An (cid:118)14..n+11 Sys, for n ≥ 10.

Here, we verify the Uppaal SMC model of Sys in which we replace the _Proxy_Sensor_ automaton
of Figure 5 with a compromised one, provided in Figure 10, and implementing the MITM activities of the
attacker An of Example 3. The interested reader may ﬁnd the proof in the appendix.

We have done our analysis, with a 99% accuracy, for execution traces that are at most 1000 time units
long, and assuming that the duration of the attack n may vary in the integer interval 1..300. The results of
our analysis are:

• when n ∈ 1..8, the system under attack remains safe, deadlock free, and alarm free, i.e., all three
properties (cid:3)[1,1000](safe), (cid:3)[1,1000](¬deadlock ), and (cid:3)[1,1000](¬alarm) hold with probability 0.99;

• when n = 9, we have the following situation:

– the system under attack is deadlock free, i.e., the property (cid:3)[1,1000](¬deadlock ) holds with proba-

bility 0.99;

– the system remains safe and alarm free, except for the time interval 14..15, i.e., all the follow-
ing properties (cid:3)[1,13](safe), (cid:3)[1,13](¬alarm), (cid:3)[16,1000](safe), and (cid:3)[16,1000](¬alarm) hold with
probability 0.99;

21

– in the time interval 14..15, we may have violations of safety conditions, as the property ♦[0,14](¬safe ∧
global_clock ≥ 14) is satisﬁed with a probability 0.62, while the property ♦[0,15](¬safe ∧
global_clock ≥ 15) is satisﬁed with probability 0.21; both violations are stealthy as the property
(cid:3)[14,15](¬alarm) holds with probability 0.99;

• when n ≥ 10, we have the following situation:

– the system is deadlock free, i.e., the property (cid:3)[1,1000](¬deadlock ) holds with probability 0.99;
– the system remains safe except for the time interval 14..n+11, i.e., the two properties (cid:3)[1,13](safe)

and (cid:3)[n+12,1000](safe) hold with probability 0.99;

– the system is alarm free except for the time interval n+1..n+11, i.e., the two properties (cid:3)[0,n](¬alarm)

and (cid:3)[n+12,1000](¬alarm) hold with probability 0.99;

– in the 14th time slot the system under attack may reach an unsafe state as the probability, for
n ∈ 10..300, that the property ♦[0,14](¬safe ∧ global_clock ≥ 14) is satisﬁed stabilises around
0.548;

– once the attack has terminated, in the time interval n+1..n+11, the system under attack has
good chances to reach an unsafe state as the probability, for n ∈ 10..300, that the property
♦[0,n+11](¬safe ∧ n+1 ≤ global_clock ≤ n+11) is satisﬁed stabilises around 0.672;

– the violations of the safety conditions remain completely stealthy only up to the duration n of
the attack (we recall that (cid:3)[0,n](¬alarm) is satisﬁed with probability 0.99); the probability, for
n ∈ 10..300, that the property ♦[0,n+11](alarm) is satisﬁed stabilises around 0.13; thus, in the time
interval n+1..n+11, only a small portion of violations of safety conditions are detected by the IDS
while a great majority of them remains stealthy.

4.2 A technique for proving attack tolerance/vulnerability

In this subsection, we provide suﬃcient criteria to prove attack tolerance/vulnerability to attacks of an arbitrary
class C. Actually, we do more than that: we provide suﬃcient criteria to prove attack tolerance/vulnerability
to all attacks of any class C (cid:48) that is somehow “weaker” than a given class C.

Deﬁnition 14. Let Ci ∈ [I → P(mi..ni)], for i ∈ {1, 2}, be two classes of attacks, with m1..n1 ⊆ m2..n2.
We say that C1 is weaker than C2, written C1 (cid:22) C2, if C1(ι) ⊆ C2(ι) for any ι ∈ I.

Intuitively, if C1 (cid:22) C2 then: (i) the attacks of class C1 might achieve fewer malicious activities than any
attack of class C2 (formally, there may be ι ∈ I such that C1(ι) = ∅ and C2(ι) (cid:54)= ∅); (ii) for those malicious
activities ι ∈ I achieved by the attacks of both classes C1 and C2 (i.e., C1(ι) (cid:54)= ∅ and C2(ι) (cid:54)= ∅), if they may
be perpetrated by the attacks of class C1 at some time slot k ∈ m1..n1 (i.e., k ∈ C1(ι)) then all attacks of
class C2 may do the same activity ι at the same time k (i.e., k ∈ C2(ι)).

The next objective is to deﬁne a notion of most powerful attack (also called top attacker ) of a given class
C, such that, if a CPS M tolerates the most powerful attack of class C then it also tolerates any attack
of class C (cid:48), with C (cid:48) (cid:22) C. We will provide a similar condition for attack vulnerability: let M be a CPS
vulnerable to Top(C) in the time interval m1..n1; then, for any attack A of class C (cid:48), with C (cid:48) (cid:22) C, if M is
vulnerable to A then it is so for a smaller time interval m2..n2 ⊆ m1..n1.

Our notion of top attacker has two extra ingredients with respect to the physics-based attacks seen up to
now: (i) nondeterminism, and (ii) time-unguarded recursive processes. This extra power of the top attacker
is not a problem as we are looking for suﬃcient criteria.

With respect to nondeterminism, we assume a generic procedure rnd () that given an arbitrary set
Z returns an element of Z chosen in a nondeterministic manner. This procedure allows us to express
nondeterministic choice, P ⊕ Q, as an abbreviation for the process if (rnd ({true, false})) {P } else {Q}. Thus,
let ι ∈ {(cid:69)p? : p ∈ S ∪ A} ∪ {(cid:69)p! : p ∈ S ∪ A}, m ∈ N+, n ∈ N+ ∪ {∞}, with m ≤ n, and T ⊆ m..n, we deﬁne

22

the attack process Att(ι, k, T )11 as the attack which may achieve the malicious activity ι, at the time slot k,
and which tries to do the same in all subsequent time slots of T . Formally,

Att((cid:69)a?, k, T ) = if (k ∈ T ) {((cid:98)drop a(x).Att((cid:69)a?, k, T )(cid:99)Att((cid:69)a?, k+1, T )) ⊕ tick.Att((cid:69)a?, k+1, T )}

else {if (k < sup(T )) {tick.Att((cid:69)a?, k+1, T )} else {nil}}

Att((cid:69)s?, k, T ) = if (k ∈ T ) {((cid:98)sniﬀ s(x).Att((cid:69)s?, k, T )(cid:99)Att((cid:69)s?, k+1, T )) ⊕ tick.Att((cid:69)s?, k+1, T )}

else {if (k < sup(T )) {tick.Att((cid:69)s?, k+1, T )} else {nil}}

Att((cid:69)p!, k, T ) = if (k ∈ T ) {((cid:98)forge p(cid:104)rnd (R)(cid:105).Att((cid:69)p!, k, T )(cid:99)Att((cid:69)p!, k+1, T )) ⊕ tick.Att((cid:69)p!, k+1, T )}

else {if (k < sup(T )) {tick.Att((cid:69)p!, k+1, T )} else {nil}} .

Note that, for T = ∅, we assume sup(T ) = −∞. We can now use the deﬁnition above to formalise the

notion of most powerful attack of a given class C.

Deﬁnition 15 (Top attacker). Let C ∈ [I → P(m..n)] be a class of attacks. We deﬁne

Top(C) =

(cid:89)

ι∈I

Att(ι, 1, C(ι))

as the most powerful attack, or top attacker, of class C.

The following theorem provides soundness criteria for attack tolerance and attack vulnerability.

Theorem 3 (Soundness criteria). Let M be an honest and sound CPS, C an arbitrary class of attacks, and
A an attack of a class C (cid:48), with C (cid:48) (cid:22) C.

• If M (cid:107) Top(C) (cid:118) M then M (cid:107) A (cid:118) M .

• If M (cid:107) Top(C) (cid:118)m1..n1 M then either M (cid:107) A (cid:118) M or M (cid:107) A (cid:118)m2..n2 M , with m2..n2 ⊆ m1..n1.

Corollary 1. Let M be an honest and sound CPS, and C a class of attacks. If Top(C) is not lethal for M
then any attack A of class C (cid:48), with C (cid:48) (cid:22) C, is not lethal for M . If Top(C) is not a permanent attack for M ,
then any attack A of class C (cid:48), with C (cid:48) (cid:22) C, is not a permanent attack for M .

The following example illustrates how Theorem 3 could be used to infer attack tolerance/vulnerability

with respect to an entire class of attacks.

Example 4. Consider our running example Sys and a class of attacks Cm, for m ∈ N, such that Cm((cid:69)cool?) =
Cm((cid:69)cool!) = {m} and Cm(ι) = ∅, for ι (cid:54)∈ {(cid:69)cool?, (cid:69)cool!}. Attacks of class Cm may tamper with the actuator
cool only in the time slot m (i.e., in the time interval m..m). The attack Am of Example 1 is of class Cm.

In the following analysis in Uppaal SMC of the top attacker Top(Cm), we will show that both the
vulnerability window and the probability of successfully attacking the system represent an upper bound for
the attack Am of Example 1 of class Cm. Technically, we verify the Uppaal SMC model of Sys in which we
replace the _Proxy_Actuator_ automaton of Figure 5 with a compromised one, provided in Figure 11, and
implementing the activities of the top attacker Top(Cm). We carry out our analysis with a 99% accuracy, for
execution traces that are at most 1000 time slots long, limiting the attack time m to the integer interval
1..300.

To explain our analysis further, let us provide details on how Top(Cm) aﬀects Sys when compared to the

attacker Am of class Cm seen in the Example 1.

• In the time interval 1..m, the attacked system remains safe, deadlock free, and alarm free. Formally, the
three properties (cid:3)[1,m](safe), (cid:3)[1,m](¬deadlock ) and (cid:3)[1,m](¬alarm) hold with probability 0.99. Thus,
in this time interval, the top attacker is harmless, as well as Am.

11In case of sensor sniﬃng, we might avoid to add this speciﬁc attack process as our top attacker process can forge any possible

value without need to read sensors.

23

Figure 11: Uppaal SMC model for the top attacker Top(Cm) of Example 4

Figure 12: Results of ♦[0,m+3](deadlock ∧ global _clock ≥ m + 1) by varying the attack time m

• In the time interval m+1..m+3, the system exposed to the top attacker may deadlock when m ∈ 1..8;
for m > 8 the system under attack is deadlock free (see Figure 12). This is because the top attacker,
unlike the attacker Am, can forge in the ﬁrst 8 time slots cool-on commands turning on the cooling and
dropping the temperature below zero in the time interval m+1..m+3. Note that no alarms or unsafe
behaviours occur in this case, as neither the safety process nor the IDS check whether the temperature
drops below a certain threshold. Formally, the properties (cid:3)[m+1,m+3](safe) and (cid:3)[m+1,m+3](¬alarm)
hold with probability 0.99, as already seen for the attacker Am.

• In the time interval m+4..1000, the top attacker has better chances to deadlock the system when
compared with the attacker Am (see Figure 13). With respect to safety and alarms, the top at-
tacker and the attacker Am have the same probability of success (the properties (cid:3)[m+4,1000](safe) and
(cid:3)[m+4,1000](¬alarm) return the same probability results).

This example shows how the veriﬁcation of a top attacker Top(C) provides an upper bound of the
eﬀectiveness of the entire class of attacks C, in terms of both vulnerability window and probability of
successfully attack the system. Of course, the accuracy of such approximation cannot be estimated a priori.

5

Impact of a physics-based attack

In the previous section, we have grouped physics-based attacks by focussing on the physical devices under
attack and the timing aspects of the attack (Deﬁnition 12). Then, we have provided a formalisation of when

24

Figure 13: Results of ♦[0,1000](deadlock ∧ global _clock ≥ m + 4) by varying the attack time m

a CPS should be considered tolerant/vulnerable to an attack (Deﬁnition 13). In this section, we show that
these two formalisations are important not only to demonstrate the tolerance (or vulnerability) of a CPS
with respect to certain attacks, but also to evaluate the disruptive impact of those attacks on the target
CPS [21, 44].

The goal of this section is to provide a formal metric to estimate the impact of a successful attack on the
physical behaviour of a CPS. In particular, we focus on the ability that an attack may have to drag a CPS
out of the correct behaviour modelled by its evolution map, with the given uncertainty.

w]; S (cid:111)(cid:110) P .

w, ξe(cid:105); similarly, for M = E; S (cid:111)(cid:110) P we write M [ξw ← ξ(cid:48)

Recall that evol is monotone with respect to the uncertainty. Thus, as stated in Proposition 6, an
increase of the uncertainty may translate into a widening of the range of the possible behaviours of the CPS.
In the following, given the physical environment E = (cid:104)evol , meas, inv , safe, ξw, ξe(cid:105), we write E[ξw ← ξ(cid:48)
w]
w] for
as an abbreviation for (cid:104)evol , meas, inv , safe, ξ(cid:48)
E[ξw ← ξ(cid:48)
Proposition 6 (Monotonicity). Let M be an honest and sound CPS with uncertainty ξw. If ξw ≤ ξ(cid:48)
M

t
−−→ M (cid:48) then M [ξw ← ξ(cid:48)
However, a wider uncertainty in the model does not always correspond to a widening of the possible
behaviours of the CPS. In fact, this depends on the intrinsic tolerance of a CPS with respect to changes in
the uncertainty function. In the following, we will write ξw + ξ(cid:48)
w ∈ RX such that
w
w(x) = ξw(x) + ξ(cid:48)
ξ(cid:48)(cid:48)
Deﬁnition 16 (System ξ-tolerance). An honest and sound CPS M with uncertainty ξw is said ξ-tolerant,
for ξ ∈ RX and ξ ≥ 0, if

to denote the function ξ(cid:48)(cid:48)

w(x), for any x ∈ X .

t
−−→ M (cid:48)[ξw ← ξ(cid:48)

w and

w].

w]

ξ = sup (cid:8)ξ(cid:48) : M [ξw ← ξw + η] (cid:118) M, for any 0 ≤ η ≤ ξ(cid:48)(cid:9).

Intuitively, if a CPS M has been designed with a given uncertainty ξw, but M is actually ξ-tolerant, with
ξ > 0, then the uncertainty ξw is somehow underestimated: the real uncertainty of M is given by ξw + ξ.
This information is quite important when trying to estimate the impact of an attack on a CPS. In fact, if a
system M has been designed with a given uncertainty ξw, but M is actually ξ-tolerant, with ξ > 0, then an
attack has (at least) a “room for manoeuvre” ξ to degrade the whole CPS without being observed (and hence
detected).

Let Sys be our running example. In the rest of the section, with an abuse of notation, we will write

Sys[δ ← γ] to denote Sys where the uncertainty δ of the variable temp has been replaced with γ.

25

Example 5. The CPS Sys is 1
Since δ + ξ = 8
20 + 1
in the appendix.

20 = 9

20 -tolerant as sup (cid:8)ξ(cid:48) : Sys[δ ← δ+η] (cid:118) Sys, for 0 ≤ η ≤ ξ(cid:48)(cid:9) is equal to 1
20 .
20 , then this statement relies on the following proposition whose proof can be found

Proposition 7. We have

• Sys[δ ← γ] (cid:118) Sys, for γ ∈ ( 8

20 , 9
• Sys[δ ← γ] (cid:54)(cid:118) Sys, for γ > 9
20 .

20 ),

Now everything is in place to deﬁne our metric to estimate the impact of an attack.

Deﬁnition 17 (Impact). Let M be an honest and sound CPS with uncertainty ξw. We say that an attack A
has deﬁnitive impact ξ on the system M if

ξ = inf (cid:8)ξ(cid:48) : ξ(cid:48) ∈ RX ∧ ξ(cid:48)>0 ∧ M (cid:107) A (cid:118) M [ξw ← ξw + ξ(cid:48)](cid:9).

It has pointwise impact ξ on the system M at time m if

ξ= inf (cid:8)ξ(cid:48) : ξ(cid:48) ∈ RX ∧ ξ(cid:48)>0 ∧ M (cid:107) A (cid:118)m..n M [ξw ← ξw + ξ(cid:48)], n ∈ N+ ∪ {∞}(cid:9).

Intuitively, the impact of an attacker A on a system M measures the perturbation introduced by the
presence of the attacker in the compound system M (cid:107) A with respect to the original system M . With this
deﬁnition, we can establish either the deﬁnitive (and hence maximum) impact of the attack A on the system
M , or the impact at a speciﬁc time m. In the latter case, by deﬁnition of (cid:118)m..n, there are two possibilities:
either the impact of the attack keeps growing after time m, or in the time interval m+1, the system under
attack deadlocks.

The impact of Top(C) provides an upper bound for the impact of all attacks of class C (cid:48), C (cid:48) (cid:22) C, as

shown in the following theorem (proved in the appendix).

Theorem 4 (Top attacker’s impact). Let M be an honest and sound CPS, and C an arbitrary class of
attacks. Let A be an arbitrary attack of class C (cid:48), with C (cid:48) (cid:22) C.

• The deﬁnitive impact of Top(C) on M is greater than or equal to the deﬁnitive impact of A on M .
• If Top(C) has pointwise impact ξ on M at time m, and A has pointwise impact ξ(cid:48) on M at time m(cid:48),

with m(cid:48) ≤ m, then ξ(cid:48) ≤ ξ.

In order to help the intuition on the impact metric deﬁned in Deﬁnition 17, we give a couple of examples.

Here, we focus on the role played by the size of the vulnerability window.

Example 6. Let us consider the attack An of Example 3, for n ∈ {8, 9, 10}. Then,

• A8 has deﬁnitive impact 0 on Sys,
• A9 has deﬁnitive impact 0.23 on Sys,
• A10 has deﬁnitive impact 0.4 on Sys.

Formally, the impacts of these three attacks are obtained by calculating

inf{ξ(cid:48) : ξ(cid:48) > 0 ∧ Sys (cid:107) An (cid:118) Sys[δ ← δ + ξ(cid:48)]} ,

for n ∈ {8, 9, 10}. Attack A9 has a very low impact on Sys as it may drag the system into a temporary unsafe
state in the time interval 14..15, whereas A10 has a slightly stronger impact as it may induce a temporary
unsafe state during the larger time interval 14..21. Technically, since δ + ξ = 0.4 + 0.4 = 0.8, the calculation
of the impact of A10 relies on the following proposition whose proof can be found in the appendix.

Proposition 8. Let A10 be the attack deﬁned in Example 3. Then:

• Sys (cid:107) A10 (cid:54)(cid:118) Sys[δ ← γ], for γ ∈ (0.4, 0.8),

26

• Sys (cid:107) A10 (cid:118) Sys[δ ← γ], for γ > 0.8.

On the other hand, the attack provided in Example 2, driving the system to a (permanent) deadlock state,

has a much stronger impact on the CPS Sys than the attack of Example 3.

Example 7. Let us consider the attack Am of Example 2, for m > 8. As already discussed, this is a stealthy
lethal attack that has a very severe and high impact. In fact, it has a deﬁnitive impact of 8.5 on the CPS Sys.
Formally,

8.5 = inf (cid:8) ξ(cid:48) : ξ(cid:48) > 0 ∧ Sys (cid:107) Am (cid:118) Sys[δ ← δ + ξ(cid:48)](cid:9).
Technically, since δ + ξ = 0.4 + 8.5 = 8.9, what stated in this example relies on the following proposition whose
proof can be found in the appendix.

Proposition 9. Let Am be the attack deﬁned in Example 2. Then:

• Sys (cid:107) Am (cid:54)(cid:118) Sys[δ ← γ], for γ ∈ (0.4, 8.9),
• Sys (cid:107) Am (cid:118) Sys[δ ← γ], for γ > 8.9.

Thus, Deﬁnition 17 provides an instrument to estimate the impact of a successful attack on a CPS in
terms of the perturbation introduced both on its physical and on its logical processes. However, there is at
least another question that a CPS designer could ask: “Is there a way to estimate the chances that an attack
will be successful during the execution of my CPS?” To paraphrase in a more operational manner: how many
execution traces of my CPS are prone to be attacked by a speciﬁc attack? As argued in the future work, we
believe that probabilistic metrics might reveal to be very useful in this respect [40, 43].

6 Conclusions, related and future work

6.1 Summary

We have provided theoretical foundations to reason about and formally detect attacks to physical devices of
CPSs. A straightforward utilisation of these methodologies is for model-checking or monitoring in order to be
able to formally analyse security properties of CPSs either before system deployment or, when static analysis
is not feasible, at runtime to promptly detect undesired behaviours. To that end, we have proposed a hybrid
process calculus, called CCPSA, as a formal speciﬁcation language to model physical and cyber components
of CPSs as well as MITM physics-based attacks. Note that our calculus is general enough to represent
Supervisory Control And Data Acquisition (SCADA) systems as cyber components which can easily interact
with controllers and IDSs via channel communications. SCADA systems are the main technology used by
system engineers to supervise the activities of complex CPSs.

Based on CCPSA and its labelled transition semantics, we have formalised a threat model for CPSs by
grouping physics-based attacks in classes, according to the target physical devices and two timing parameters:
begin and duration of the attacks. Then, we developed two diﬀerent compositional trace semantics for CCPSA
to assess attack tolerance/vulnerability with respect to a given attack. Such a tolerance may hold ad inﬁnitum
or for a limited amount of time. In the latter case, the CPS under attack is vulnerable and the attack aﬀects
the observable behaviour of the system only after a certain point in time, when the attack itself may already
be achieved or still working.

Along the lines of GNDC [17], we have deﬁned a notion of top attacker, Top(C), of a given class of attacks
C, which has been used to provide suﬃcient criteria to prove attack tolerance/vulnerability to all attacks of
class C (and weaker ones).

Then, we have provided a metric to estimate the maximum impact introduced in the system under attack
with respect to its genuine behaviour, according to its evolution law and the uncertainty of the model. We
have proved that the impact of the most powerful attack Top(C) represents an upper bound for the impact
of any attack A of class C (and weaker ones).

Finally, we have formalised a running example in Uppaal SMC [15], the statistical extension of the
Uppaal model checker [5]. Our goal was to test Uppaal SMC as an automatic tool for the static security

27

analysis of a simple but signiﬁcant CPS exposed to a number of diﬀerent physics-based attacks with diﬀerent
impacts on the system under attack. Here, it is important to note that, although we have veriﬁed most of the
properties stated in the paper, we have not been able to capture time properties on the responsiveness of the
IDS to violations of the safety conditions. Examples of such properties are: (i) there are time slots m and k
such that the system may have an unsafe state at some time n > m, and the IDS detects this violation with
a delay of at least k time slots (k being a lower bound of the reaction time of the IDS), or (ii) there is a time
slot n in which the IDS ﬁres an alarm but neither an unsafe state nor a deadlock occurs in the time interval
n−k..n+k: this would provide a tolerance of the occurrence of false positive. Furthermore, Uppaal SMC
does not support the veriﬁcation of nested formulae. Thus, although from a designer’s point of view it would
have been much more practical to verify a logic formula of the form ∃♦((cid:3)[t,t+5]temp > 9.9) to check safety
and invariant conditions, in Uppaal SMC we had to implement a _Safety_ automaton that is not really
part of our CPS (for more details see the discussion of related work).

6.2 Related work

A number of approaches have been proposed for modelling CPSs using hybrid process algebras [14, 7, 57, 52, 20].
Among these approaches, our calculus CCPSA shares some similarities with the φ-calculus [52]. However,
unlike CCPSA, in the φ-calculus, given a hybrid system (E, P ), the process P can dynamically change the
evolution law in E. Furthermore, the φ-calculus does not have a representation of physical devices and
measurement law, which are instead crucial for us to model physics-based attacks that operate in a timely
fashion on sensors and actuators. More recently, Galpin et al. [20] have proposed a process algebra in which
the continuous part of the system is represented by appropriate variables whose changes are determined by
active inﬂuences (i.e., commands on actuators).

Many good surveys on the security of cyber-physical systems have been published recently (see, e.g.,
[23, 62, 2, 63]), including a survey of surveys [22]. In particular, the surveys [63, 62] provide a systematic
categorisation of 138 selected papers on CPS security. Among those 138 papers, 65 adopt a discrete notion of
time similar to ours, 26 a continuous one, 55 a quasi-static time model, and the rest use a hybrid time model.
This study encouraged us in adopting a discrete time model for physical processes rather than a continuous
one. Still, one might wonder what is actually lost when one adopts a discrete rather than a continuous time
model, in particular when the attacker has the possibility to move in a continuous time setting. A continuous
time model is, of course, more expressive. For instance, Kanovich et al. [32] identiﬁed a novel vulnerability in
the context of cryptographic protocols for CPSs in which the attacker works in a continuous-time setting to
fool discrete-time veriﬁers. However, we believe that, for physics-based attacks, little is lost by adopting a
discrete time model. In fact, sensor measurements and actuator commands are elaborated within controllers,
which are digital devices with an intrinsic discrete notion of time. In particular, with respect to dropping of
actuator commands and forging of sensor measurements, there are no diﬀerences between discrete-time and
continuous-time attackers given that to achieve those malicious activities the attacker has to synchronise
with the controller. Thus, there remain only two potential malicious activities: sensor sniﬃng and forging of
actuator commands. Can a continuous-time attacker, able to carry out these two malicious activities, be
more disruptive than a similar attacker adopting a discrete-time model? This would only be the case when
dealing with very rare physical processes changing their physical state in an extremely fast way, faster than
the controller which is the one dictating the discrete time of the CPS. However, we believe that CPSs of this
kind would be hardly controllable as they would pose serious safety issues even in the absence of any attacker.
The survey [23] provides an exhaustive review of papers on physics-based anomaly detection proposing a
uniﬁed taxonomy, whereas the survey [2] presents the main solutions in the estimation of the consequences of
cyber-attacks, attacks modelling and detection, and the development of security architecture (the main types
of attacks and threats against CPSs are analysed and grouped in a tree structure).

Huang et al. [30] were among the ﬁrst to propose threat models for CPSs. Along with [33, 34], they

stressed the role played by timing parameters on integrity and DoS attacks.

Gollmann et al. [24] discussed possible goals (equipment damage, production damage, compliance violation)
and stages (access, discovery, control, damage, cleanup) of physics-based attacks. In this article, we focused
on the “damage” stage, where the attacker already has a rough idea of the plant and the control architecture

28

of the target CPS.As we remarked in Section 1, here we focus on an attacker who has already entered the
CPS, without considering how the attacker gained access to the system, which could have happened in several
ways, for instance by attacking an Internet-accessible controller or one of the communication protocols.

Almost all papers discussed in the surveys mentioned above [63, 23, 2] investigate attacks on CPSs and
their protection by relying on simulation test systems to validate the results, rather than formal methodologies.
We are aware of a number of works applying formal methods to CPS security, although they apply methods,
and most of the time have goals, that are quite diﬀerent from ours. We discuss the most signiﬁcant ones on
the following.

Burmester et al. [11] employed hybrid timed automata to give a threat framework based on the traditional
Byzantine faults model for crypto-security. However, as remarked in [55], physics-based attacks and faults
have inherently distinct characteristics. Faults are considered as physical events that aﬀect the system
behaviour where simultaneous events don’t act in a coordinated way, whereas cyber attacks may be performed
over a signiﬁcant number of attack points and in a coordinated way.

In [59], Vigo presented an attack scenario that addresses some of the peculiarities of a cyber-physical
adversary, and discussed how this scenario relates to other attack models popular in the security protocol
literature. Then, in [60] Vigo et al. proposed an untimed calculus of broadcasting processes equipped with
notions of failed and unwanted communication. These works diﬀer quite considerably from ours, e.g., they
focus on DoS attacks without taking into consideration timing aspects or impact of the attack.

Cómbita et al. [13] and Zhu and Basar [64] applied game theory to capture the conﬂict of goals between
an attacker who seeks to maximise the damage inﬂicted to a CPS’s security and a defender who aims to
minimise it [42].

Rocchetto and Tippenhauer [51] introduced a taxonomy of the diverse attacker models proposed for CPS
security and outline requirements for generalised attacker models; in [50], they then proposed an extended
Dolev-Yao attacker model suitable for CPSs. In their approach, physical layer interactions are modelled as
abstract interactions between logical components to support reasoning on the physical-layer security of CPSs.
This is done by introducing additional orthogonal channels. Time is not represented.

Nigam et al. [46] worked around the notion of Timed Dolev-Yao Intruder Models for Cyber-Physical
Security Protocols by bounding the number of intruders required for the automated veriﬁcation of such
protocols. Following a tradition in security protocol analysis, they provide an answer to the question: How
many intruders are enough for veriﬁcation and where should they be placed? They also extend the strand
space model to CPS protocols by allowing for the symbolic representation of time, so that they can use the
tool Maude [47] along with SMT support. Their notion of time is however diﬀerent from ours, as they focus
on the time a message needs to travel from an agent to another. The paper does not mention physical devices,
such as sensors and/or actuators.

There are a few approaches that carry out information ﬂow security analysis on discrete/continuous
models for CPSs. Akella et al. [1] proposed an approach to perform information ﬂow analysis, including
both trace-based analysis and automated analysis through process algebra speciﬁcation. This approach has
been used to verify process algebra models of a gas pipeline system and a smart electric power grid system.
Bodei et al. [9] proposed a process calculus supporting a control ﬂow analysis that safely approximates the
abstract behaviour of IoT systems. Essentially, they track how data spread from sensors to the logics of
the network, and how physical data are manipulated. In [8], the same authors extend their work to infer
quantitative measures to establish the cost of possibly security countermeasures, in terms of time and energy.
Another discrete model has been proposed by Wang [61], where Petri-net models have been used to verify
non-deducibility security properties of a natural gas pipeline system. More recently, Bohrer and Platzer [10]
introduced dHL, a hybrid logic for verifying cyber-physical hybrid-dynamic information ﬂows, communicating
information through both discrete computation and physical dynamics, so security is ensured even when
attackers observe continuously-changing values in continuous time.

Huang et al. [29] proposed a risk assessment method that uses a Bayesian network to model the attack
propagation process and infers the probabilities of sensors and actuators to be compromised. These probabili-
ties are fed into a stochastic hybrid system (SHS) model to predict the evolution of the physical process being
controlled. Then, the security risk is quantiﬁed by evaluating the system availability with the SHS model.

29

As regards tools for the formal veriﬁcation of CPSs, we remark that we tried to verify our case study using
model-checking tools for distributed systems such as PRISM [36], Uppaal [6], Real-Time Maude [47], and
prohver within the MODEST TOOLSET [26]. In particular, as our example adopts a discrete notion of time, we
started looking at tools supporting discrete time. PRISM, for instance, relies on Markov decision processes or
discrete-time Markov chains, depending on whether one is interested in modelling nondeterminism or not. It
supports the veriﬁcation of both CTL and LTL properties (when dealing with nonprobabilistic systems). This
allowed us to express the formula ∃♦((cid:3)[t,t+5]temp > 9.9) to verify violations of the safety conditions, avoiding
the implementation of the _Safety_ automaton. However, using integer variables to represent state variables
with a ﬁxed precision requires the introduction of extra transitions (to deal with nondeterministic errors),
which signiﬁcantly complicates the PRISM model. In this respect, Uppaal appears to be more eﬃcient than
PRISM, as we have been able to concisely express the error occurring in integer state variables thanks to the
select() construct, in which the user can ﬁx the granularity adopted to approximate a dense interval. This
discrete representation provides an under-approximation of the system behaviour; thus, a ﬁner granularity
translates into an exponential increase of the complexity of the system, with obvious consequences on the
veriﬁcation performance. Then, we tried to model our case study in Real-Time Maude, a completely diﬀerent
framework for real-time systems, based on rewriting logic. The language supports object-like inheritance
features that are quite helpful to represent complex systems in a modular manner. We used communication
channels to implement our attacks on the physical devices. Furthermore, we used rational variables for a more
concise discrete representation of state variables. We have been able to verify LTL and T-CTL properties,
although the veriﬁcation process resulted to be quite slow due to a proliferation of rewriting rules when ﬁxing
a reasonable granularity to approximate dense intervals. As the veriﬁcation logic is quite powerful, there is
no need to implement an ad hoc process to check for safety. Finally, we also tried to model our case study
in the safety model checker prohver within the MODEST TOOLSET (see [38]). We speciﬁed our case study in
the high-level language HMODEST, supporting: (i) diﬀerential inclusion to model linear CPSs with constant
bounded derivatives; (ii) linear formulae to express nondeterministic assignments within a dense interval; (iii)
a compositional programming style inherited from process algebra; (iv) shared actions to synchronise parallel
components. However, we faced the same performance limitations encountered in Uppaal. Thus, we decided
to move to statistical model checking.

Finally, this article extends the preliminary conference version [39] in the following aspects: (i) the calculus
has been slightly redesigned by distinguishing physical state and physical environment, adding specifying
constructs to sniﬀ, drop and forge packets, and removing, for simplicity, protected physical devices; (ii) the
two trace semantics have been proven to be compositional, i.e., preserved by properly deﬁned contexts;
(iii) both our running example Sys and the attacks proposed in Examples 1, 2, 3 and 4 have been implemented
and veriﬁed in Uppaal SMC.

6.3 Future work

While much is still to be done, we believe that our paper provides a stepping stone for the development
of formal and automated tools to analyse the security of CPSs. We will consider applying, possibly after
proper enhancements, existing tools and frameworks for automated security protocol analysis, resorting to
the development of a dedicated tool if existing ones prove not up to the task. We will also consider further
security properties and concrete examples of CPSs, as well as other kinds of physics-based attacks,such as
delays in the communication of measurements and/or commands, and periodic attacks, i.e., attacks that
operate in a periodic fashion inducing periodic physical eﬀects on the targeted system that may be easily
confused by engineers with system malfunctions. This will allow us to reﬁne the classes of attacks we have
given here (e.g., by formalising a type system amenable to static analysis), and provide a formal deﬁnition of
when a CPS is more secure than another so as to be able to design, by progressive reﬁnement, secure variants
of a vulnerable CPSs.

We also aim to extend the behavioural theory of CCPSA by developing suitable probabilistic metrics to take
into consideration the probability of a speciﬁc trace to actually occur. We have already done some progress
in this direction for a variant of CCPSA with no security features in it, by deﬁning ad hoc compositional
bisimulation metrics [41]. In this manner, we believe that our notion of impact might be reﬁned by taking

30

into account quantitative aspects of an attack such as the probability of being successful when targeting a
speciﬁc CPS. A ﬁrst attempt on a (much) simpler IoT setting can be found in [40].

Finally, with respect to automatic approximations of the impact, while we have not yet fully investigated
the problem, we believe that we can transform it into a “minimum problem”. For instance, if the environment
uses linear functions, then, by adapting techniques developed for linear hybrid automata (see, e.g., [3]), the set
of all traces with length at most n (for a ﬁxed n) can be characterised by a system of ﬁrst degree inequalities,
so the measure of the impact could be translated into a linear programming problem.

Acknowledgements

We thank the anonymous reviewers for their insightful and careful reviews. Massimo Merro and Andrei
Munteanu have been partially supported by the project “Dipartimenti di Eccellenza 2018–2022” funded by
the Italian Ministry of Education, Universities and Research (MIUR).

References

[1] R. Akella, H. Tang, and B. M. McMillin. Analysis of information ﬂow security in cyber-physical systems.

International Journal of Critical Infrastructure Protection, 3(3–4):157–173, 2010.

[2] R. Alguliyev, Y. Imamverdiyev, and L. Sukhostat. Cyber-physical systems and their security issues.

Computers in Industry, 100:212–223, 2018.

[3] R. Alur, C. Courcoubetis, N. Halbwachs, T. A. Henzinger, P.-H. Ho, X. Nicollin, A. Olivero, J. Sifakis,
and S. Yovine. The algorithmic analysis of hybrid systems. Theoretical Computer Science, 138(1):3–34,
1995.

[4] E. Bartocci, J. Deshmukh, A. Donzé, G. Fainekos, O. Maler, D. Ničković, and S. Sankaranarayanan.
Speciﬁcation-Based Monitoring of Cyber-Physical Systems: A Survey on Theory, Tools and Applications.
In Lectures on Runtime Veriﬁcation — Introductory and Advanced Topics, LNCS 10457, pages 135–175.
Springer, 2018.

[5] G. Behrmann, A. David, and K. G. Larsen. A Tutorial on Uppaal. In Formal Methods for the Design of
Real-Time Systems. SFM-RT 2004, volume 3185 of Lecture Notes in Computer Science, pages 200–236.
Springer, 2004.

[6] G. Behrmann, A. David, K. G. Larsen, J. Håkansson, P. Pettersson, W. Yi, and M. Hendriks. UPPAAL

4.0. In Quantitative Evaluation of Systems, pages 125–126. IEEE Computer Society, 2006.

[7] J. A. Bergstra and C. A. Middelburg. Process Algebra for Hybrid Systems. Theoretical Computer

Science, 335(2–3):215–280, 2005.

[8] C. Bodei, S. Chessa, and L. Galletta. Measuring security in iot communications. Theoretical Computer

Science, pages 100–124, 2019.

[9] C. Bodei, P. Degano, G.-L. Ferrari, and L. Galletta. Tracing where IoT data are collected and aggregated.

Logical Methods in Computer Science, 13(3:5):1–38, 2019.

[10] B. Bohrer and A. Platzer. A Hybrid, Dynamic Logic for Hybrid-Dynamic Information Flow.

In

ACM/IEEE Symposium on Logic in Computer Science, pages 115–124. ACM, 2018.

[11] M. Burmester, E. Magkos, and V. Chrissikopoulos. Modeling security in cyber-physical systems.

International Journal of Critical Infrastructure Protection, 5(3–4):118–126, 2012.

[12] H. Chernoﬀ. A Measure of Asymptotic Eﬃciency for Tests of a Hypothesis Based on the sum of

Observations. The Annals of Mathematical Statistics, 23(4):493–507, 1952.

31

[13] L. F. Cómbita, J. Giraldo, A. A. Cárdenas, and N. Quijano. Response and reconﬁguration of cyber-
physical control systems: A survey. In Colombian Conference on Automatic Control, pages 1–6. IEEE,
2015.

[14] P. J. L. Cuijpers and M. A. Reniers. Hybrid process algebra. The Journal of Logic and Algebraic

Programming, 62(2):191–245, 2005.

[15] A. David, K. G. Larsen, A. Legay, M. Mikučionis, and D. B. Poulsen. Uppaal SMC Tutorial. International

Journal on Software Tools for Technology Transfer, 17(4):397–415, 2015.

[16] D. Dolev and A. C. Yao. On the security of public key protocols. IEEE Transactions on information

theory, (2):198–208, 1983.

[17] R. Focardi and F. Martinelli. A Uniform Approach for the Deﬁnition of Security Properties. In Formal

Methods, volume 1708 of Lecture Notes in Computer Science, pages 794–813. Springer, 1999.

[18] G. Frehse. PHAVer: Algorithmic Veriﬁcation of Hybrid Systems Past HyTech. International Journal on

Software Tools for Technology Transfer, 10(3):263–279, 2008.

[19] G. Frehse, C. Le Guernic, A. Donzé, S. Cotton, R. Ray, O. Lebeltel, R. Ripado, A. Girard, T. Dang, and
O. Maler. SpaceEx: Scalable Veriﬁcation of Hybrid Systems. In Computer Aided Veriﬁcation, volume
6806 of Lecture Notes in Computer Science, pages 379–395. Springer, 2011.

[20] V. Galpin, L. Bortolussi, and J. Hillston. HYPE: Hybrid modelling by composition of ﬂows. Formal

Aspects of Computing, 25(4):503–541, 2013.

[21] B. Genge, I. Kiss, and P. Haller. A system dynamics approach for assessing the impact of cyber attacks
on critical infrastructures. International Journal of Critical Infrastructure Protection, 10:3–17, 2015.

[22] J. Giraldo, E. Sarkar, A. A. Cárdenas, M. Maniatakos, and M. Kantarcioglu. Security and Privacy in

Cyber-Physical Systems: A Survey of Surveys. IEEE Design & Test, 34(4):7–17, 2017.

[23] J. Giraldo, D. I. Urbina, A. A. Cárdenas, J. Valente, M. Faisal, J. Ruths, N. O. Tippenhauer, H. Sandberg,
and R. Candell. A Survey of Physics-Based Attack Detection in Cyber-Physical Systems. ACM Computing
Surveys (CSUR), 51(4):76:1–76:36, 2018.

[24] D. Gollmann, P. Gurikov, A. Isakov, M. Krotoﬁl, J. Larsen, and A. Winnicki. Cyber-Physical Systems
Security: Experimental Analysis of a Vinyl Acetate Monomer Plant. In Proceedings of the 1st ACM
Workshop on Cyber-Physical System Security, pages 1–12. ACM, 2015.

[25] D. Gollmann and M. Krotoﬁl. Cyber-Physical Systems Security. In The New Codebreakers – Essays
Dedicated to David Kahn on the Occasion of His 85th Birthday, volume 9100 of Lecture Notes in Computer
Science, pages 195–204. Springer, 2016.

[26] A. Hartmanns and H. Hermanns. The Modest Toolset: An Integrated Environment for Quantitative
Modelling and Veriﬁcation. In Tools and Algorithms for the Construction and Analysis of Systems,
volume 8413 of Lecture Notes in Computer Science, pages 593–598. Springer, 2014.

[27] M. Hennessy and T. Regan. A Process Algebra for Timed Systems. Information and Computation,

117(2):221–239, 1995.

[28] T. A. Henzinger, P.-H. Ho, and H. Wong-Toi. HYTECH: A Model Checker for Hybrid Systems.

International Journal on Software Tools for Technology Transfer, 1(1–2):110–122, 1997.

[29] K. Huang, C. Zhou, Y.-C. Tian, S. Yang, and Y. Qin. Assessing the Physical Impact of Cyberattacks
on Industrial Cyber-Physical Systems. IEEE Transactions on Industrial Electronics, 65(10):8153–8162,
2018.

32

[30] Y.-L. Huang, A. A. Cárdenas, S. Amin, Z.-S. Lin, H.-Y. Tsai, and S. Sastry. Understanding the physical
and economic consequences of attacks on control systems. International Journal of Critical Infrastructure
Protection, 2(3):73–83, 2009.

[31] ICS-CERT.

Cyber-Attack Against Ukrainian Critical

Infrastructure.

https://ics-cert.us-

cert.gov/alerts/IR-ALERT-H-16-056-01, 2015.

[32] M. Kanovich, T. B. Kirigin, V. Nigam, A. Scedrov, and C. Talcott. Discrete vs. Dense Times in the
Analysis of Cyber-Physical Security Protocols. In Principles of Security and Trust, volume 9036 of
Lecture Notes in Computer Science, pages 259–279. Springer, 2015.

[33] M. Krotoﬁl and A. A. Cárdenas. Resilience of Process Control Systems to Cyber-Physical Attacks. In
NordSec 2013: Secure IT Systems, volume 8208 of Lecture Notes in Computer Science, pages 166–182.
Springer, 2013.

[34] M. Krotoﬁl, A. A. Cárdenas, J. Larsen, and D. Gollmann. Vulnerabilities of cyber-physical systems
to stale data – Determining the optimal time to launch attacks. International Journal of Critical
Infrastructure Protection, 7(4):213–232, 2014.

[35] D. Kushner. The real story of stuxnet. IEEE Spectrum, 50(3):48–53, 2013.

[36] M. Z. Kwiatkowska, G. Norman, and D. Parker. PRISM 4.0: Veriﬁcation of Probabilistic Real-Time
Systems. In Computer Aided Veriﬁcation, volume 6806 of Lecture Notes in Computer Science, pages
585–591. Springer, 2011.

[37] R. Lanotte and M. Merro. A Calculus of Cyber-Physical Systems. In Language and Automata Theory

and Applications, volume 10168 of Lecture Note in Computer Science, pages 115–127. Springer, 2017.

[38] R. Lanotte, M. Merro, and A. Munteanu. A Modest Security Analysis of Cyber-Physical Systems: A
Case Study. In Formal Techniques for Distributed Objects, Components, and Systems, volume 10854 of
Lecture Notes in Computer Science, pages 58–78. Springer, 2018.

[39] R. Lanotte, M. Merro, R. Muradore, and L. Viganò. A Formal Approach to Cyber-Physical Attacks. In

Computer Security Foundations Symposium, pages 436–450. IEEE Computer Society, 2017.

[40] R. Lanotte, M. Merro, and S. Tini. Towards a Formal Notion of Impact Metric for Cyber-Physical
Attacks. In Integrated Formal Methods, volume 11023 of Lecture Notes in Computer Science, pages
296–315. Springer, 2018.

[41] R. Lanotte, M. Merro, and S. Tini. A Probabilistic Calculus of Cyber-Physical Systems. Information

and Computation, 2020.

[42] M. H. Manshaei, Q. Zhu, T. Alpcan, T. Bacşar, and J.-P. Hubaux. Game theory meets network security

and privacy. ACM Computer Surveys, 45(3):25, 2013.

[43] M. Merro, J. Kleist, and U. Nestmann. Mobile objects as mobile processes. Information and Computation,

177(2):195–241, 2002.

[44] J. Milošević, D. Umsonst, H. Sandberg, and K. H. Johansson. Quantifying the Impact of Cyber-Attack
Strategies for Control Systems Equipped With an Anomaly Detector. In European Control Conference
(ECC), pages 331–337. IEEE, 2018.

[45] A. F. Murillo Piedrahita, V. Gaur, J. Giraldo, A. A. Cárdenas, and S. J. Rueda. Virtual incident

response functions in control systems. Computer Networks, 135:147–159, 2018.

[46] V. Nigam, C. Talcott, and A. A. Urquiza. Towards the Automated Veriﬁcation of Cyber-Physical
Security Protocols: Bounding the Number of Timed Intruders. In Computer Security - ESORICS 2016,
volume 9879 of Lecture Notes in Computer Science, pages 450–470. Springer, 2016.

33

[47] P. C. Ölveczky and J. Meseguer. Semantics and pragmatics of Real-Time Maude. Higher-Order and

Symbolic Computation, 20(1–2):161–196, 2007.

[48] A. Platzer. Logical Foundations of Cyber-Physical Systems. Springer, 2018.

[49] J.-D. Quesel, S. Mitsch, S. M. Loos, N. Aréchiga, and A. Platzer. How to model and prove hybrid
systems with KeYmaera: a tutorial on safety. International Journal on Software Tools for Technology
Transfer, 18(1):67–91, 2016.

[50] M. Rocchetto and N. O. Tippenhauer. CPDY: Extending the Dolev-Yao Attacker with Physical-Layer
Interactions. In Formal Methods and Software Engineering, volume 10009 of Lecture Notes in Computer
Science, pages 175–192. Springer, 2016.

[51] M. Rocchetto and N. O. Tippenhauer. On Attacker Models and Proﬁles for Cyber-Physical Systems. In
Computer Security - ESORICS 2016, volume 9879 of Lecture Notes in Computer Science, pages 427–449.
Springer, 2016.

[52] W. C. Rounds and H. Song. The φ-calculus: A Language for Distributed Control of Reconﬁgurable
Embedded Systems. In Hybrid Systems: Computation and Control, volume 2623 of Lecture Notes in
Computer Science, pages 435–449. Springer, 2003.

[53] J. Slay and M. Miller. Lessons Learned from the Maroochy Water Breach. In Critical Infrastructure

Protection, IFIP 253, pages 73–82. Springer, 2007.

[54] Swedish Civil Contigencies Agency. Guide to increased security in industrial information and control

systems. 2014.

[55] A. Teixeira, I. Shames, H. Sandberg, and K. H. Johansson. A secure control framework for resource-limited

adversaries. Automatica, 51:135–148, 2015.

[56] U.S. Chemical Safety and Hazard Investigation Board, T2 Laboratories Inc. Reactive Chemical Explosion:

Final Investigation Report. Report No. 2008-3-I-FL, 2009.

[57] D. A. van Beek, K. L. Man, M. A. Reniers, J. E. Rooda, and R. R. Schiﬀelers. Syntax and consistent
equation semantics of hybrid chi. The Journal of Logic and Algebraic Programming, 68(1-2):129–210,
2006.

[58] P. Vasilikos, F. Nielson, and H. Riis Nielson. Secure Information Release in Timed Automata. In
Principles of Security and Trust, volume 10804 of Lecture Notes in Computer Science, pages 28–52.
Springer, 2018.

[59] R. Vigo. The Cyber-Physical Attacker. In Computer Safety, Reliability, and Security, volume 7613 of

Lecture Notes in Computer Science, pages 347–356. Springer, 2012.

[60] R. Vigo, F. Nielson, and H. Riis Nielson. Broadcast, Denial-of-Service, and Secure Communication. In
Integrated Formal Methods, volume 7940 of Lecture Notes in Computer Science, pages 412–427. Springer,
2013.

[61] J. Wang and H. Yu. Analysis of the Composition of Non-Deducibility in Cyber-Physical Systems. Applied

Mathematics & Information Sciences, 8:3137–3143, 2014.

[62] Y. Zacchia Lun, A. D’Innocenzo, I. Malavolta, and M. D. Di Benedetto. Cyber-Physical Systems Security:

a Systematic Mapping Study. CoRR, abs/1605.09641, 2016.

[63] Y. Zacchia Lun, A. D’Innocenzo, F. Smarra, I. Malavolta, and M. D. Di Benedetto. State of the art of
cyber-physical systems security: An automatic control perspective. Journal of Systems and Software,
149:174–216, 2019.

34

[64] Q. Zhu and T. Basar. Game-Theoretic Methods for Robustness, Security, and Resilience of Cyberphysical
Control Systems: Games-in-Games Principle for Optimal Cross-Layer Resilient Control Systems. IEEE
Control Systems Magazine, 35(1):46–65, 2015.

A Proofs

A.1 Proofs of Section 2

As already stated in Remark 2, our trace preorder (cid:118) is deadlock-sensitive. Formally,

Lemma 1. Let M and N be two CPSs in CCPSA such that M (cid:118) N . Then, M satisﬁes its system invariant
if and only if N satisﬁes its system invariant.

Proof. This is because CPSs that don’t satisfy their invariant can only ﬁre deadlock actions.

Proof of Theorem 1. We prove the three statements separately.

ˆt==⇒ N (cid:48) (cid:93) O(cid:48). The proof is by induction on the

t
−−→ M (cid:48) (cid:93) O(cid:48) entails N (cid:93) O
t
−−→ M (cid:48) (cid:93) O(cid:48).

1. Let us prove that M (cid:93) O
length of the trace M (cid:93) O
As M (cid:118) N , by an application of Lemma 1 it follows that either both M and N satisfy their respective
invariants or they both don’t. In the latter case, the result would be easy to prove as the systems can
only ﬁre deadlock actions. Similarly, if the system invariant of O is not satisﬁed, then M (cid:93) O and N (cid:93) O
can perform only deadlock actions and again the result would follow easily. Thus, let us suppose that
the system invariants of M , N and O are satisﬁed.
Base case. We suppose M = E1; S1 (cid:111)(cid:110) P1, N = E2; S2 (cid:111)(cid:110) P2, and O = E3; S3 (cid:111)(cid:110) P3. We proceed by case
analysis on why M (cid:93) O

α
−−→ M (cid:48) (cid:93) O(cid:48), for some action α.

• α = cv. Suppose M (cid:93) O

cv
−−−→ M (cid:48) (cid:93) O(cid:48) is derived by an application of rule (Out). We have two

possible cases:

cv

−−−→ P (cid:48)

−−−→ P (cid:48)
3

−−−→ P (cid:48)
1

, for some P (cid:48)
3

, for some P (cid:48)
1

, because P3
cv

– either P1 (cid:107) P3
cv
– or P1 (cid:107) P3

cv
−−−→ P1 (cid:107) P (cid:48)
3
1 (cid:107) P3, because P1

, O(cid:48) = S3 (cid:111)(cid:110) P (cid:48)
3
, and M = S1 (cid:111)(cid:110) P (cid:48)
1
cv
In the ﬁrst case, by an application of rule (Par) we derive P2 (cid:107) P3
−−−→ P2 (cid:107) P (cid:48)
3
system invariants of N and O are satisﬁed, we can derive the required trace N (cid:93) O
−−−→ N (cid:93) O(cid:48)
and the invariant of M is
by an application of rule (Out). In the second case, since P1
−−−→ P (cid:48)
1
cv
satisﬁed, by an application of rule (Out) we can derive M
−−−→ M (cid:48). As M (cid:118) N , there exists a
trace N (cid:98)cv===⇒ N (cid:48), for some system N (cid:48). Thus, by several applications of rule (Par) we can easily
derive N (cid:93) O (cid:98)cv===⇒ N (cid:48) (cid:93) O = N (cid:48) (cid:93) O(cid:48), as required.

, and M (cid:48) = M ,
and O(cid:48) = O.

. Since both

cv

cv

• α = cv. Suppose M (cid:93) O

cv
−−−→ M (cid:48) (cid:93) O(cid:48) is derived by an application of rule (Inp). This case is

similar to the previous one.

• α = τ . Suppose M (cid:93) O

τ
−−→ M (cid:48) (cid:93) O(cid:48) is derived by an application of rule (SensRead). We have two

possible cases:

s?v
−−−→ P (cid:48)
3

, for some P (cid:48)
3

, P1 (cid:107) P3

(cid:69)s!v
−−−−→(cid:54)

(and hence

s?v
−−−→ P (cid:48)
1

, for some P (cid:48)
1

, P1 (cid:107) P3

(cid:69)s!v
−−−−→(cid:54)

(and hence

– either P1 (cid:107) P3
(cid:69)s!v
−−−−→(cid:54)
– or P1 (cid:107) P3

P3

(cid:69)s!v
−−−−→(cid:54)

P1

s?v
−−−→ P1 (cid:107) P (cid:48)
3

because P3
,

), M (cid:48) = M and O(cid:48) = S3 (cid:111)(cid:110) P (cid:48)
3
1 (cid:107) P3 because P1

s?v
−−−→ P (cid:48)

) and M (cid:48) = S1 (cid:111)(cid:110) P (cid:48)
1

and O(cid:48) = O.

35

(cid:69)s!v
−−−−→(cid:54)

In the ﬁrst case, by an application of rule (Par) we derive P2 (cid:107) P3

s?v
−−−→ P2 (cid:107) P (cid:48)
3

. Moreover from

and since the sets of sensors are always disjoint, we can derive P2 (cid:107) P3

. Since
P3
τ
−−→ N (cid:93) O(cid:48) by an application
both invariants of N and O are satisﬁed, we can derive N (cid:93) O
s?v
and the invariant of M is
of rule (SensRead), as required. In the second case, since P1
−−−→ P (cid:48)
1
τ
satisﬁed, by an application of rule (SensRead) we can derive M
. As
−−→ M (cid:48) with M (cid:48) = S1 (cid:111)(cid:110) P (cid:48)
1
M (cid:118) N , there exists a derivation N ˆτ==⇒ N (cid:48), for some N (cid:48). Thus, we can derive the required trace
N (cid:93) O ˆτ==⇒ N (cid:48) (cid:93) O by an application of rule (Par).

(cid:69)s!v
−−−−→(cid:54)

• α = τ . Suppose that M (cid:93) O

τ
−−→ M (cid:48) (cid:93) O(cid:48) is derived by an application of rule ((cid:69)SensSniﬀ (cid:69)). This

case is similar to the previous one.

• α = τ . Suppose that M (cid:93) O

τ
−−→ M (cid:48) (cid:93) O(cid:48) is derived by an application of rule (ActWrite). This

case is similar to the case (SensRead).

• α = τ . Suppose that M (cid:93) O

τ
−−→ M (cid:48) (cid:93) O(cid:48) is derived by an application of rule ((cid:69)AcIntegr (cid:69)). This

case is similar to the case ((cid:69)SensSniﬀ (cid:69)).

• α = τ . Suppose that M (cid:93) O

τ
−−→ M (cid:48) (cid:93) O(cid:48) is derived by an application of rule (Tau). We have four

possible cases:

τ
−−→ P (cid:48)

– P1 (cid:107) P3

cv

cv

cv

, or P1

and P3

−−−→ P (cid:48)
1

−−−→ P (cid:48)
1

1 (cid:107) P (cid:48)
3
cv
−−−→ P (cid:48)
3

by an application of rule (Com). We have two sub-cases: either
. We prove
and P3
P1
the ﬁrst case, the second one is similar. As the invariant of M is satisﬁed, by an application
cv
−−−→ ˆτ==⇒ N (cid:48),
of rule (Out) we can derive M
for some N (cid:48) = E2; S(cid:48)
, by several applications of rule (Par) and one of
(cid:111)(cid:110) P (cid:48)
2
2
rule (Com) we derive N (cid:93) O (cid:98)cv===⇒ N (cid:48) (cid:93) O(cid:48), as required.
or P1 (cid:107) P3
by an application of either rule ((cid:69)ActDrop (cid:69)) or rule ((cid:69)SensIntegr (cid:69)). This

1 (cid:107) P3 by an application of (Par). This case is easy.

cv
−−−→ M (cid:48). As M (cid:118) N , there exists a trace N ˆτ==⇒

, for some P (cid:48)
1

τ
−−→ P1 (cid:107) P (cid:48)
3
τ
1 (cid:107) P (cid:48)
−−→ P (cid:48)
3

−−−→ P (cid:48)
3

−−−→ P (cid:48)
3

. As P3

and P (cid:48)
3

τ
−−→ P (cid:48)

cv

– P1 (cid:107) P3
– P1 (cid:107) P3

case does not apply as the sets of actuators of M and O are disjoint.

– P1 (cid:107) P3

1 (cid:107) P (cid:48)
3
case does not apply to parallel processes.

τ
−−→ P (cid:48)

by the application of on rule among (Res), (Rec), (Then) and (Else). This

• α = deadlock. Suppose that M (cid:93)O

deadlock
−−−−−−−→ M (cid:48) (cid:93)O(cid:48) is derived by an application of rule (Deadlock).

This case is not admissible as the invariants of M , N and O are satisﬁed.

• α = tick. Suppose that M (cid:93) O

(cid:111)(cid:110) P (cid:48)
3

and P (cid:48)
3

, for some P (cid:48)
1

tick
−−−→ M (cid:48) (cid:93) O(cid:48) is derived by an application of rule (Time). This
, with

and O = E3; S(cid:48)
3
can only be derived by an

1 (cid:107) P (cid:48)
3
3 ∈ next(E3; S3). As P1 (cid:107) P3

tick
, M (cid:48) = E1; S(cid:48)
(cid:111)(cid:110) P (cid:48)
implies P1 (cid:107) P3
−−−→ P (cid:48)
1
1
tick
1 ∈ next(E1; S1) and S(cid:48)
S(cid:48)
1 (cid:107) P (cid:48)
−−−→ P (cid:48)
3
tick
. Since the invariant of
application of rule (TimePar), it follows that P1
and P3
−−−→ P (cid:48)
3
tick
(cid:111)(cid:110) P (cid:48)
−−−→ M (cid:48) with M (cid:48) = E1; S(cid:48)
M is satisﬁed, by an application of rule (Time) we can derive M
1
1
ˆτ==⇒ N (cid:48), for some N (cid:48) = E2; S(cid:48)
As M (cid:118) N , there exists a derivation N ˆτ==⇒ N (cid:48)(cid:48)
(cid:111)(cid:110) P (cid:48)
,
2
2 ). By several applications of rule
, with S(cid:48)(cid:48)(cid:48)
, N (cid:48)(cid:48)(cid:48) = E2; S(cid:48)(cid:48)(cid:48)
N (cid:48)(cid:48) = E2; S(cid:48)(cid:48)
2
2
ˆτ==⇒ N (cid:48) (cid:93) O(cid:48). In order to conclude
(Par) we can derive that N (cid:93) O ˆτ==⇒ N (cid:48)(cid:48) (cid:93) O and N (cid:48)(cid:48)(cid:48) (cid:93) O(cid:48)
tick
−−−→ N (cid:48)(cid:48)(cid:48) (cid:93) O(cid:48). By the deﬁnition of rule (Time), from
the proof, it is suﬃcient to prove N (cid:48)(cid:48) (cid:93) O
, by an application of rule (TimePar)
. As P3
N (cid:48)(cid:48)
it follows that P (cid:48)(cid:48)
3 ∈ next(E3; S3) we can
. Since S(cid:48)(cid:48)(cid:48)
2 ) ∪ next(E3; S3). By an application of rule (Time) we have
derive that S(cid:48)(cid:48)(cid:48)

tick
−−−→ N (cid:48)(cid:48)(cid:48) it follows that P (cid:48)(cid:48)
2
tick
2 (cid:107) P (cid:48)
−−−→ P (cid:48)(cid:48)(cid:48)
3
3 ∈ next(E2; S(cid:48)(cid:48)

tick
−−−→ P (cid:48)
3
2 ∈ next(E2; S(cid:48)(cid:48)

tick
−−−→ N (cid:48)(cid:48)(cid:48)
2 ∈ next(E2; S(cid:48)(cid:48)

tick
−−−→ P (cid:48)(cid:48)(cid:48)
2

2 ) and S(cid:48)

tick
−−−→ P (cid:48)
1

2 (cid:93) S(cid:48)

2 (cid:107) P3

(cid:111)(cid:110) P (cid:48)(cid:48)(cid:48)
2

(cid:111)(cid:110) P (cid:48)(cid:48)
2

2

.

N (cid:48)(cid:48) (cid:93) O

tick
−−−→ N (cid:48)(cid:48)(cid:48) (cid:93) O(cid:48) and hence N (cid:93) O (cid:99)tick===⇒ N (cid:48) (cid:93) O(cid:48), as required.

36

• α = unsafe. Suppose that M (cid:93) O

is similar to the case α = cv by considering the fact that ξx (cid:54)∈ safe implies that ξx ∪ξx
for any ξx

(cid:48) and any safe (cid:48).

unsafe
−−−−−→ M (cid:48) (cid:93) O(cid:48) is derived by an application of rule (Safety). This
(cid:48) (cid:54)∈ safe ∪safe (cid:48),

Inductive case. We have to prove that M (cid:93) O = M0 (cid:93) O0
N0 (cid:93) O0
actions and resort to the base case to handle the nth action.

αn−−−→ Mn (cid:93) On implies N (cid:93) O =
(cid:99)α1===⇒ · · · (cid:99)αn===⇒ Nn (cid:93) On. We can use the inductive hypothesis to easily deal with the ﬁrst n − 1

α1−−−→ · · ·

2. We have to prove that M (cid:118) N implies M (cid:107) P (cid:118) N (cid:107) P , for any pure-logical process P . This is a
special case of (1) as M (cid:107) P = M (cid:93) (∅; ∅ (cid:111)(cid:110) P ) and N (cid:107) P = N (cid:93) (∅; ∅ (cid:111)(cid:110) P ), where ∅; ∅ (cid:111)(cid:110) P is a CPS
with no physical process in it, only logics.

3. We have to prove that M (cid:118) N implies M \c (cid:118) N \c, for any channel c. For any derivation
t
−−→ M (cid:48) with c not occurring in t. Since M (cid:118) N , it follows
ˆt==⇒ N (cid:48)\c, as

ˆt==⇒ N (cid:48), for some N (cid:48). Since c does not appear in t, we can easily derive that N \c

t
−−→ M (cid:48)\c we can easily derive that M

M \c
that N
required.

In order to prove Theorem 2 we adapt to CCPSA two standard lemmata used in process calculi theory to

compose and decompose the actions performed by a compound system.

Lemma 2 (Decomposing system actions). Let M and N be two CPSs in CCPSA. Then,

• if M (cid:93) N
• if M (cid:93) N
• if M (cid:93) N

tick
−−−→ M (cid:48) (cid:93) N (cid:48), for some M (cid:48) and N (cid:48), then M
deadlock
−−−−−−−→ M or N
−−−−−−−→ M (cid:93) N , then M
τ
−−→ M (cid:48) (cid:93) N (cid:48), for some M (cid:48) and N (cid:48), due to a channel synchronisation between M and N ,

tick
−−−→ M (cid:48) and N
deadlock
−−−−−−−→ N ;

tick
−−−→ N (cid:48);

deadlock

cv

• if M (cid:93) N

−−−→ M (cid:48) and N

−−−→ M (cid:48) and N
then either M
α
−−→ M (cid:48) (cid:93) N (cid:48), for some M (cid:48) and N (cid:48), α (cid:54)= tick, not due to a channel synchronisation between
α
−−→ N and M = M (cid:48).

α
−−→ M and N = N (cid:48), or N

−−−→ N (cid:48), for some channel c;

M and N , then either M

−−−→ N (cid:48), or M

cv

cv

cv

Lemma 3 (Composing system actions). Let M and N be two CPSs of CCPSA. Then,

• If M
• If N

tick
−−−→ M (cid:48) and N
deadlock
and M
−−−−−−−→(cid:54)

N (cid:93) M (cid:48).

tick
−−−→ N (cid:48), for some M (cid:48) and N (cid:48), then M (cid:93) N
α
−−→ M (cid:48), for some M (cid:48) and α (cid:54)= tick, then M (cid:93) N

tick
−−−→ M (cid:48) (cid:93) N (cid:48);

α
−−→ M (cid:48) (cid:93) N and N (cid:93) M

α
−−→

Proof of Theorem 2. Here, we prove case (1) of the theorem. The proofs of cases (2) and (3) are similar to
the corresponding ones of Theorem 1.

We prove that M (cid:118)m..n N implies that there are m(cid:48), n(cid:48) ∈ N+ ∪ ∞, with m(cid:48)..n(cid:48) ⊆ m..n such that

M (cid:93) O (cid:118)m(cid:48)..n(cid:48) N (cid:93) O. We prove separately that m(cid:48) ≥ m and n(cid:48) ≤ n.

• m(cid:48) ≥ m. We recall that m, m(cid:48) ∈ N+. If m = 1, then we trivially have m(cid:48) ≥ 1 = m. Otherwise, since m
ˆt==⇒,
t
−−→ and N (cid:54)
is the minimum integer for which there is a trace t, with #tick(t) = m − 1, such that M
ˆt==⇒. As done in the
then for any trace t, with #tick(t) < m − 1 and such that M
proof of case (1) of Theorem 1, we can derive that for any trace t, with #tick(t) < m − 1 and such that

t
−−→, it holds that N

M (cid:93) O

t
−−→ it holds that N (cid:93) O

ˆt==⇒. This implies the required condition, m(cid:48) ≥ m.

• n(cid:48) ≤ n. We recall that n is the inﬁmum element of N+ ∪ {∞}, n ≥ m, such that whenever M

t1−−→ M (cid:48),
t2−−→ N (cid:48), for some N (cid:48), and
with #tick(t1) = n − 1, there is t2, with #tick(t1) = #tick(t2), such that N
t
M (cid:48) (cid:118) N (cid:48). Now, if M (cid:93) O
−−→ M (cid:48) (cid:93) O(cid:48), with #tick(t) = n − 1, by Lemma 2 we can split the trace t
by extracting the actions performed by M and those performed by O. Thus, there exist two traces
t3−−→ O(cid:48), with #tick(t1) = #tick(t3) = n − 1 whose combination has generated
M
t1−−→ M (cid:48) we know that there is a trace t2,
the trace M (cid:93) O

t
−−→ M (cid:48) (cid:93) O(cid:48). As M (cid:118)m..n N , from M

t1−−→ M (cid:48) and O

37

t3−−→ O(cid:48), by an application of Lemma 3 we can build a trace N (cid:93) O

t2−−→ N (cid:48) and
with #tick(t1) = #tick(t2), such that N
t(cid:48)
−−→ N (cid:48) (cid:93) O(cid:48), for some t(cid:48) such
O
that #tick(t) = #tick(t(cid:48)) = n − 1. As M (cid:48) (cid:118) N (cid:48), by Theorem 1 we can derive that M (cid:48) (cid:93) O(cid:48) (cid:118) N (cid:48) (cid:93) O(cid:48).
This implies that n(cid:48) ≤ n.

t2−−→ N (cid:48), for some N (cid:48), and M (cid:48) (cid:118) N (cid:48). Since N

A.2 Proofs of Section 3

In order to prove Proposition 1 and Proposition 2, we use the following lemma that formalises the invariant
properties binding the state variable temp with the activity of the cooling system.

Intuitively, when the cooling system is inactive the value of the state variable temp lays in the real interval
[0, 11.5]. Furthermore, if the coolant is not active and the variable temp lays in the real interval (10.1, 11.5],
then the cooling will be turned on in the next time slot. Finally, when active the cooling system will remain
so for k ∈ 1..5 time slots (counting also the current time slot) with the variable temp being in the real interval
(9.9 − k∗(1+δ), 11.5 − k∗(1−δ)].

Lemma 4. Let Sys be the system deﬁned in Section 3. Let

Sys = Sys1

t1−−−→

tick
−−−→ Sys2

t2−−−→

tick

−−−→ · · ·

tn−1−−−−−→

tick
−−−→ Sysn

such that the traces tj contain no tick-actions, for any j ∈ 1..n−1, and for any i ∈ 1..n, Sysi = Si (cid:111)(cid:110) Pi with
Si = (cid:104)ξi

a(cid:105). Then, for any i ∈ 1..n−1, we have the following:

x, ξi

s, ξi

a(cool ) = oﬀ then ξi
1. if ξi
ξi
x(stress) = 1, otherwise;

x(temp) ∈ [0, 11.1 + δ]; with ξi

x(stress) = 0 if ξi

x(temp) ∈ [0, 10.9 + δ], and

2. if ξi
ξi+1
x

a(cool ) = oﬀ and ξi
(stress) ∈ 1..2;

x(temp) ∈ (10.1, 11.1 + δ] then, in the next time slot, ξi+1

a

(cool ) = on and

3. if ξi
ξi−k
a
otherwise, ξi

a(cool ) = on then ξi
(cool ) = oﬀ and ξi−j
a
x(stress) = 0.

x(temp) ∈ (9.9 − k∗(1+δ), 11.1 + δ − k∗(1−δ)], for some k ∈ 1..5 such that
x(stress) ∈ 1..k+1,

(cool ) = on, for j ∈ 0..k−1; moreover, if k ∈ 1..3 then ξi

Proof. Let us write vi and si to denote the values of the state variables temp and stress, respectively, in the
x(stress) = si. Moreover, we will say that the coolant is active (resp.,
systems Sysi , i.e., ξi
is not active) in Sysi if ξi

x(temp) = vi and ξi

a(cool ) = on (resp., ξi

a(cool ) = oﬀ).

The proof is by mathematical induction on n, i.e., the number of tick-actions of our traces.
The case base n = 1 follows directly from the deﬁnition of Sys.
Let us prove the inductive case. We assume that the three statements hold for n − 1 and prove that they

also hold for n.

1. Let us assume that the cooling is not active in Sysn . In this case, we prove that vn ∈ [0, 11.1 + δ], with

and sn = 0 if vn ∈ [0, 10.9 + δ], and sn = 1 otherwise.
We consider separately the cases in which the coolant is active or not in Sysn−1
• Suppose the coolant is not active in Sysn−1 (and not active in Sysn ).

By the induction hypothesis we have vn−1 ∈ [0, 11.1 + δ]; with sn−1 = 0 if vn−1 ∈ [0, 10.9 + δ], and
sn−1 = 1 otherwise. Furthermore, if vn−1 ∈ (10.1, 11.1 + δ], then, by the induction hypothesis, the
coolant must be active in Sysn . Since we know that in Sysn the cooling is not active, it follows
that vn−1 ∈ [0, 10.1] and sn = 0. Furthermore, in Sysn the temperature will increase of a value
laying in the real interval [1 − δ, 1 + δ] = [0.6, 1.4]. Thus, vn will be in [0.6, 11.1 + δ] ⊆ [0, 11.1 + δ].
Moreover, if vn−1 ∈ [0, 9.9], then the state variable stress is not incremented and hence sn = 0
with vn ∈ [0 + 1 − δ , 9.9 + 1 + δ] = [0.6 , 10.9 + δ] ⊆ [0 , 10.9 + δ]. Otherwise, if vn−1 ∈ (9.9, 10.1],
then the state variable stress is incremented, and hence sn = 1.

38

• Suppose the coolant is active in Sysn−1 (and not active in Sysn ).

By the induction hypothesis, vn−1 ∈ (9.9 − k ∗ (1 + δ), 11.1 + δ − k ∗ (1 − δ)] for some k ∈ 1..5 such
that the coolant is not active in Sysn−1 −k and is active in Sysn−k , . . . , Sysn−1 .
The case k ∈ {1, . . . , 4} is not admissible. In fact if k ∈ {1, . . . , 4} then the coolant would be
active for less than 5 tick-actions as we know that Sysn is not active. Hence, it must be k = 5.
Since δ = 0.4 and k = 5, it holds that vn−1 ∈ (9.9 − 5 ∗ 1.4, 11.1 + 0.4 − 5 ∗ 0.6] = (2.8, 8.6] and
sn−1 = 0. Moreover, since the coolant is active for 5 time slots, in Sysn−1 the controller and
the IDS synchronise together via channel sync and hence the IDS checks the temperature. Since
vn−1 ∈ (2.8, 8.6] the IDS process sends to the controller a command to stop the cooling, and the
controller will switch oﬀ the cooling system. Thus, in the next time slot, the temperature will
increase of a value laying in the real interval [1 − δ, 1 + δ] = [0.6, 1.4]. As a consequence, in Sysn
we will have vn ∈ [2.8 + 0, 6, 8.6 + 1.4] = [3.4, 10] ⊆ [0, 11.1 + δ]. Moreover, since vn−1 ∈ (2.8, 8.6]
and sn−1 = 0, we derive that the state variable stress is not increased and hence sn = 0, with
vn ∈ [3.4, 10] ⊆ [0, 10.9 + δ].

2. Let us assume that the coolant is not active in Sysn and vn ∈ (10.1, 11.1 + δ]; we prove that the coolant
is active in Sysn+1 with sn+1 ∈ 1..2. Since the coolant is not active in Sysn , then it will check the
temperature before the next time slot. Since vn ∈ (10.1, 11.1 + δ] and (cid:15) = 0.1, then the process Ctrl
will sense a temperature greater than 10 and the coolant will be turned on. Thus, the coolant will be
active in Sysn+1 . Moreover, since vn ∈ (10.1, 11.1 + δ], and sn could be either 0 or 1, the state variable
stress is increased and therefore sn+1 ∈ 1..2.

3. Let us assume that the coolant is active in Sysn ; we prove that vn ∈ (9.9 − k ∗ (1 + δ), 11.1 + δ − k ∗ (1 − δ)]
for some k ∈ 1..5 and the coolant is not active in Sysn−k and active in Sysn−k +1 , . . . , Sysn . Moreover,
we have to prove that if k ≤ 3 then sn ∈ 1..k+1, otherwise, if k > 3 then sn = 0.
We prove the ﬁrst statement. That is, we prove that vn ∈ (9.9 − k ∗ (1 + δ), 11.1 + δ − k ∗ (1 − δ)], for some
k ∈ 1..5, and the coolant is not active in Sysn−k , whereas it is active in the systems Sysn−k +1 , . . . , Sysn .
We separate the case in which the coolant is active in Sysn−1 from that in which is not active.

• Suppose the coolant is not active in Sysn−1 (and active in Sysn ).

In this case k = 1 as the coolant is not active in Sysn−1 and it is active in Sysn . Since k = 1, we
have to prove vn ∈ (9.9 − (1 + δ), 11.1 + δ − (1 − δ)].
However, since the coolant is not active in Sysn−1 and is active in Sysn it means that the coolant
has been switched on in Sysn−1 because the sensed temperature was above 10 (since (cid:15) = 0.1 this
may happen only if vn−1 > 9.9). By the induction hypothesis, since the coolant is not active in
Sysn−1 , we have that vn−1 ∈ [0, 11.1 + δ]. Therefore, from vn−1 > 9.9 and vn−1 ∈ [0, 11.1 + δ] it
follows that vn−1 ∈ (9.9, 11.1+δ]. Furthermore, since the coolant is active in Sysn , the temperature
will decrease of a value in [1 − δ, 1 + δ] and therefore vn ∈ (9.9 − (1 + δ), 11.1 + δ − (1 − δ)], which
concludes this case of the proof.

• Suppose the coolant is active in Sysn−1 (and active in Sysn as well).

By the induction hypothesis, there is h ∈ 1..5 such that vn−1 ∈ (9.9−h∗(1+δ), 11.1+δ −h∗(1−δ)]
and the coolant is not active in Sysn−1 −h and is active in Sysn−h , . . . , Sysn−1 .
The case h = 5 is not admissible. In fact, since δ = 0.4, if h = 5 then vn−1 ∈ (9.9 − 5 ∗ 1.4, 11.1 +
δ − 5 ∗ 0.6] = (2.8, 8.6]. Furthermore, since the cooling system has been active for 5 time instants,
in Sysn−1 the controller and the IDS synchronise together via channel sync, and the IDS checks
the received temperature. As vn−1 ∈ (2.8, 8.6], the IDS sends to the controller via channel ins the
command stop. This implies that the controller should turn oﬀ the cooling system, in contradiction
with the hypothesis that the coolant is active in Sysn .
Hence, it must be h ∈ 1..4. Let us prove that for k = h + 1 we obtain our result. Namely, we have
to prove that, for k = h + 1, (i) vn ∈ (9.9 − k ∗ (1 + δ), 11.1 + δ − k ∗ (1 − δ)], and (ii) the coolant
is not active in Sysn−k and active in Sysn−k +1 , . . . , Sysn .
Let us prove the statement (i). By the induction hypothesis, it holds that vn−1 ∈ (9.9 − h ∗ (1 +
δ), 11.1 + δ − h ∗ (1 − δ)]. Since the coolant is active in Sysn , the temperature will decrease Hence,
vn ∈ (9.9 − (h + 1) ∗ (1 + δ), 11.1 + δ − (h + 1) ∗ (1 − δ)]. Therefore, since k = h + 1, we have that

39

vn ∈ (9.9 − k ∗ (1 + δ), 11.1 + δ − k ∗ (1 − δ)].
Let us prove the statement (ii). By the induction hypothesis the coolant is not active in Sysn−1 −h
and it is active in Sysn−h , . . . , Sysn−1 . Now, since the coolant is active in Sysn , for k = h + 1, we
have that the coolant is not active in Sysn−k and is active in Sysn−k +1 , . . . , Sysn , which concludes
this case of the proof.

Thus, we have proved that vn ∈ (9.9 − k ∗ (1 + δ), 11.1 + δ − k ∗ (1 − δ)], for some k ∈ 1..5; moreover,
the coolant is not active in Sysn−k and active in the systems Sysn−k +1 , . . . , Sysn .
It remains to prove that sn ∈ 1..k+1 if k ≤ 3, and sn = 0, otherwise.
By inductive hypothesis, since the coolant is not active in Sysn−k , we have that sn−k ∈ 0..1. Now,
for k ∈ [1..2], the temperature could be greater than 9.9. Hence if the state variable stress is either
increased or reset, then sn ∈ 1..k+1, for k ∈ 1..3. Moreover, since for k ∈ 3..5 the temperature is below
9.9, it follows that sn = 0 for k > 3.

Proof of Proposition 1. Since δ = 0.4, by Lemma 4 the value of the state variable temp is always in the
real interval [0, 11.5]. As a consequence, the invariant of the system is never violated and the system never
deadlocks. Moreover, after 5 time units of cooling, the state variable temp is always in the real interval
(9.9 − 5 ∗ 1.4, 11.1 + 0.4 − 5 ∗ 0.6] = (2.9, 8.5]. Hence, the process IDS will never transmit on the channel
alarm.

Finally, by Lemma 4 the maximum value reached by the state variable stress is 4 and therefore the system

does not reach unsafe states.

Proof of Proposition 2. Let us prove the two statements separately.

• Since (cid:15) = 0.1, if process Ctrl senses a temperature above 10 (and hence Sys turns on the cooling) then
the value of the state variable temp is greater than 9.9. By Lemma 4, the value of the state variable
temp is always less than or equal to 11.1 + δ. Therefore, if Ctrl senses a temperature above 10, then
the value of the state variable temp is in (9.9, 11.1 + δ].

• By Lemma 4 (third item), the coolant can be active for no more than 5 time slots. Hence, by Lemma 4,
when Sys turns oﬀ the cooling system the state variable temp ranges over (9.9 − 5 ∗ (1 + δ), 11.1 + δ −
5 ∗ (1 − δ)].

A.3 Proofs of Section 4

Proof of Proposition 3. We distinguish the two cases, depending on m.

• Let m ≤ 8. We recall that the cooling system is activated only when the sensed temperature is above
10. Since (cid:15) = 0.1, when this happens the state variable temp must be at least 9.9. Note that after
m−1 ≤ 7 tick-actions, when the attack tries to interact with the controller of the actuator cool , the
variable temp may reach at most 7 ∗ (1 + δ) = 7 ∗ 1.4 = 9.8 degrees. Thus, the cooling system will not
be activated and the attack will not have any eﬀect.

• Let m > 8. By Proposition 1, the system Sys in isolation may never deadlock, it does not get into an
unsafe state, and it may never emit an output on channel alarm. Thus, any execution trace of the
system Sys consists of a sequence of τ -actions and tick-actions.
In order to prove the statement it is enough to show the following four facts:
– the system Sys (cid:107) Am may not deadlock in the ﬁrst m + 3 time slots;
– the system Sys (cid:107) Am may not emit any output in the ﬁrst m + 3 time slots;
– the system Sys (cid:107) Am may not enter in an unsafe state in the ﬁrst m + 3 time slots;
– the system Sys (cid:107) Am has a trace reaching un unsafe state from the (m+4)-th time slot on, and

until the invariant gets violated and the system deadlocks.

40

1−δ (cid:101) = (cid:100) 10.1

1+δ (cid:101) = (cid:100) 10.1

The ﬁrst three facts are easy to show as the attack may steal the command addressed to the actuator
cool only in the m-th time slot. Thus, until time slot m, the whole system behaves correctly. In
particular, by Proposition 1 and Proposition 2, no alarms, deadlocks or violations of safety conditions
occur, and the temperature lies in the expected ranges. Any of those three actions requires at least
further 4 time slots to occur. Indeed, by Lemma 4, when the cooling is switched on in the time slot m,
the variable stress might be equal to 2 and hence the system might not enters in an unsafe state in the
ﬁrst m + 3 time slots. Moreover, an alarm or a deadlock needs more than 3 time slots and hence no
alarm can occur in the ﬁrst m + 3 time slots.
Let us show the fourth fact, i.e., that there is a trace where the system Sys (cid:107) Am enters into an unsafe
state starting from the (m+4)-th time slot and until the invariant gets violated.
Firstly, we prove that for all time slots n, with 9 ≤ n < m, there is a trace of the system Sys (cid:107) Am in
which the state variable temp reaches the values 10.1 in the time slot n.
1.4 (cid:101) = 8 time units,
The fastest trace reaching the temperature of 10.1 degrees requires (cid:100) 10.1
0.6 (cid:101) = 17 time units. Thus, for any time slot n, with 9 ≤ n ≤ 18,
whereas the slowest one (cid:100) 10.1
there is a trace of the system where the value of the state variable temp is 10.1. Now, for any of those
time slots n there is a trace in which the state variable temp is equal to 10.1 in all time slots n + 10i < m,
with i ∈ N. Indeed, when the variable temp is equal to 10.1 the cooling might be activated. Thus, there
is a trace in which the cooling system is activated. We can always assume that during the cooling the
temperature decreases of 1 + δ degrees per time unit, reaching at the end of the cooling cycle the value
of 5. This entails that the trace may continue with 5 time slots in which the variable temp is increased
of 1 + δ degrees per time unit; reaching again the value 10.1. Thus, for all time slots n, with 9 ≤ n < m,
there is a trace of the system Sys (cid:107) Am in which the state variable temp is 10.1 in n.
As a consequence, we can suppose that in the m−1-th time slot there is a trace in which the value of
the variable temp is 10.1. Since (cid:15) = 0.1, the sensed temperature lays in the real interval [10, 10.2]. Let
us focus on the trace in which the sensed temperature is 10 and the cooling system is not activated. In
this case, in the m-th time slot the system may reach a temperature of 10.1 + (1 + δ) = 11.5 degrees
and the variable stress is 1.
The process Ctrl will sense a temperature above 10 sending the command cool!on to the actuator cool .
Now, since the attack Am is active in that time slot (m > 8), the command will be stolen by the attack
and it will never reach the actuator. Without that dose of coolant, the temperature of the system will
continue to grow. As a consequence, after further 4 time units of cooling, i.e. in the m+4-th time slot,
the value of the state variable stress may be 5 and the system enters in an unsafe state.
After 1 time slots, in the time slot m + 5, the controller and the IDS synchronise via channel sync, the
IDS will detect a temperature above 10, and it will ﬁre the output on channel alarm saying to process
Ctrl to keep cooling. But Ctrl will not send again the command cool!on. Hence, the temperature would
continue to increase and the system remains in an unsafe state while the process IDS will keep sending
of alarm(s) until the invariant of the environment gets violated.

Proof of Proposition 4. As proved for Proposition 3, we can prove that there is a trace of the system Sys (cid:107) Am
in which the state variable temp reaches a value greater than 9.9 in the time slot m, for all m > 8. The
process Ctrl never activates the Cooling component as it will always detect a temperature below 10. Hence,
after further 5 tick-actions (in the m + 5-th time slot) the system will violate the safety conditions emitting
an unsafe action. So, since the process Ctrl never activates the Cooling, the temperature will increase its
value until the invariant will be violated and the system will deadlock. Thus, Sys (cid:107) Am (cid:118)m+5..∞ Sys.

In order to prove Proposition 5, we introduce Lemma 5. This is a variant of Lemma 4 in which the CPS
Sys runs in parallel with the attack An deﬁned in Example 3. Here, due to the presence of the attack, the
temperature is 4 degrees higher when compared to the system Sys in isolation.

Lemma 5. Let Sys be the system deﬁned in Section 3 and An be the attack of Example 3. Let

Sys (cid:107) An = Sys1

t1−−−→

tick
−−−→ . . . Sysn−1

tn−1−−−−−→

tick
−−−→ Sysn

41

x, ξi
• if ξi
• if ξi
• if ξi

such that the traces tj contain no tick-actions, for any j ∈ 1..n−1, and for any i ∈ 1..n Sysi = Si (cid:111)(cid:110) Pi with
Si = (cid:104)ξi

a(cid:105). Then, for any i ∈ 1..n−1 we have the following:

s, ξi

a(cool ) = oﬀ, then ξi
a(cool ) = oﬀ and ξi
a(cool ) = on, then ξi

x(temp) ∈ [0, 11.1 + 4 + δ];
x(temp) ∈ (10.1 + 4, 11.1 + 4 + δ], then we have ξi+1
x(temp) ∈ (9.9 + 4 − k ∗ (1 + δ), 11.1 + 4 + δ − k ∗ (1 − δ)], for some k ∈ 1..5,

(cool ) = on;

a

such that ξi−k

(cool ) = oﬀ and ξi−j

(cool ) = on, for j ∈ 0..k−1.

a

a

Proof. Similar to the proof of Lemma 4.

Now, everything is in place to prove Proposition 5.

Proof of Proposition 5. Let us proced by case analysis.

• Let 0 ≤ n ≤ 8. In the proof of Proposition 3, we remarked that the system Sys in isolation may sense a
temperature greater than 10 only after 8 tick-actions, i.e., in the 9-th time slot. However, the life of the
attack is n ≤ 8, and in the 9-th time slot the attack is already terminated. As a consequence, starting
from the 9-th time slot the system will correctly sense the temperature and it will correctly activate the
cooling system.

• Let n = 9. The maximum value that may be reached by the state variable temp after 8 tick-actions,
i.e., in the 9-th time slot, is 8 ∗ (1 + δ) = 8 ∗ 1.4 = 11.2. However, since in the 9-th time slot the attack
is still alive, the process Ctrl will sense a temperature below 10 and the system will move to the next
time slot and the state variable stress is incremented. Then, in the 10-th time slot, when the attack is
already terminated, the maximum temperature the system may reach is 11.2 + (1 + δ) = 12.6 degrees
and the state variable stress is equal to 1. Thus, the process Ctrl will sense a temperature greater
than 10, activating the cooling system and incrementing the state variable stress. As a consequence,
during the following 4 time units of cooling, the value of the state variable temp will be at most
12.6 − 4 ∗ (1 − δ) = 12.6 − 4 ∗ 0.6 = 10.2, and hence in the 14-th time slot, the value of the state variable
stress is 5 and the system will enter in an unsafe state. In the 15-th time slot, the value of the state
variable stress is still equal to 5 and the system will still be in an unsafe state. However, the value of
the state variable temp will be at most 12.6 − 5 ∗ (1 − δ) = 12.6 − 5 ∗ 0.6 = 9.6 which will be sensed by
process IDS as at most 9.7 (sensor error (cid:15) = 0.1). As a consequence, no alarm will be turned on and
the variable stress will be reset. Moreover, the invariant will be obviously always preserved.
As in the current time slot the attack has already terminated, from this point in time on, the system
will behave correctly with neither deadlocks or alarms.

• Let n ≥ 10. In order to prove that Sys (cid:107) An (cid:118)[14,n+11] Sys, it is enough to show that:

– the system Sys (cid:107) An does not deadlock;
– the system Sys (cid:107) An may not emit any output in the ﬁrst 13 time slots;
– there is a trace in which the system Sys (cid:107) An enters in an unsafe state in the 14-th time slot;
– there is a trace in which the system Sys (cid:107) An is in an unsafe state in the (n+11)-th time slot;
– the system Sys (cid:107) An does not have any execution trace emitting an output along channel alarm

or entering in an unsafe state after the n + 11-th time slot.

As regards the ﬁrst fact, since δ = 0.4, by Lemma 5 the temperature of the system under attack will
always remain in the real interval [0, 15.5]. Thus, the invariant is never violated and the trace of the
system under attack cannot contain any deadlock-action. Moreover, when the attack terminates, if the
temperature is in [0, 9.9], the system will continue his behaviour correctly, as in isolation. Otherwise,
since the temperature is at most 15.5, after a possible sequence of cooling cycles, the temperature will
reach a value in the interval [0, 9.9], and again the system will continue its behaviour correctly, as in
isolation.
Concerning the second and the third facts, the proof is analogous to that of case n = 9.
Concerning the fourth fact, ﬁrstly we prove that for all time slots m, with 9 < m ≤ n, there is a trace
of the system Sys (cid:107) An in which the state variable temp reaches the values 14 in the time slot m. Since

42

1.4 (cid:101) = 14

1+δ (cid:101) = (cid:100) 14

1−δ (cid:101) = (cid:100) 14

the attack is alive at that time, and (cid:15) = 0.1, when the variable temp will be equal to 14 the sensed
temperature will lay in the real interval [9.9, 10.1].
1.4 = 10 time units,
The fastest trace reaching the temperature of 14 degrees requires (cid:100) 14
whereas the slowest one (cid:100) 14
0.6 (cid:101) = 24 time units. Thus, for any time slot m, with 9 < m ≤ 24,
there is a trace of the system where the value of the state variable temp is 14. Now, for any of those
time slots m there is a trace in which the state variable temp is equal to 14 in all time slots m + 10i < n,
with i ∈ N. As already said, when the variable temp is equal to 14 the sensed temperature lays in the
real interval [9.9, 10.1] and the cooling might be activated. Thus, there is a trace in which the cooling
system is activated. We can always ﬁnd a trace where during the cooling the temperature decreases of
1 + δ degrees per time unit, reaching at the end of the cooling cycle the value of 5. Thus, the trace
may continue with 5 time slots in which the variable temp is increased of 1 + δ degrees per time unit;
reaching again the value 14. Thus, for all time slots m, with 9 < m ≤ n, there is a trace of the system
Sys (cid:107) An in which the state variable temp has value 14 in the time slot m.
Therefore, we can suppose that in the n-th time slot the variable temp is equal to 14 and, since the
maximum increment of temperature is 1.4, the the variable stress is at least equal to 1. Since the attack
is alive and (cid:15) = 0.1, in the n-th time slot the sensed temperature will lay in [9.9, 10.1]. We consider the
case in which the sensed temperature is less than 10 and hence the cooling is not activated. Thus, in
the n+1-th time slot the system may reach a temperature of 14 + 1 + δ = 15.4 degrees and the process
Ctrl will sense a temperature above 10, and it will activate the cooling system. In this case, the variable
stress will be increased. As a consequence, after further 5 time units of cooling, i.e. in the n+6-th time
slot, the value of the state variable temp may reach 15.4 − 5 ∗ (1 − δ) = 12.4 and the alarm will be ﬁred
and the variable stress will be still equal to 5. After 4 time unit in the n+10-th time slot the state
variable temp may reach 12.4 − 4 ∗ (1 − δ) = 10 and the variable stress will be still equal to 5 and the
system will be in an unsafe state. So in the n+11-th time slot stress will be still equal to 5 and the
system will be in an unsafe state.
Concerning the ﬁfth fact, by Lemma 5, in the n+1-th time slot the attack will be terminated and the
system may reach a temperature that is, in the worst case, at most 15.5. Thus, the cooling system
may be activated and the variable stress will be increased. As a consequence, in the n+11-th time slot,
the value of the state variable temp may be at most 15.5 − 10 ∗ (1 − δ) = 15.5 − 10 ∗ 0.6 = 9.5 and the
variable stress will be reset to 0. Thus, after the n + 11-th time slot, the system will behave correctly,
as in isolation.

In order to prove Theorem 3, we introduce the following lemma.

Lemma 6. Let M be an honest and sound CPS, C an arbitrary class of attacks, and A an attack of a class
C (cid:48) (cid:22) C. Whenever M (cid:107) A

−−→ M (cid:48) (cid:107) A(cid:48), then M (cid:107) Top(C)

ˆt==⇒ M (cid:48) (cid:107) (cid:81)

ι∈I Att(ι, #tick(t)+1, C(ι)).

t

Proof. Let us deﬁne Toph(C) as the attack process (cid:81)
proof is by mathematical induction on the length k of the trace t.
Base case. k = 1. This means t = α, for some action α. We proceed by case analysis on α.

ι∈I Att(ι, h, C(ι)). Then, Top1(C) = Top(C). The

• α = cv. As the attacker A does not use communication channels, from M (cid:107) A

cv
−−−→ M (cid:48) (cid:107) A(cid:48) we
cv
−−−→ M (cid:48). Thus, by applications of rules (Par) and (Out) we derive

can derive that A = A(cid:48) and M
cv
−−−→ M (cid:48) (cid:107) Top1(C) = M (cid:48) (cid:107) Top(C).
M (cid:107) Top(C)

• α = cv. This case is similar to the previous one.

• α = τ . There are ﬁve sub-cases.

– Let M (cid:107) A

A performs only malicious actions, from M (cid:107) A

τ
−−→ M (cid:48) (cid:107) A(cid:48) be derived by an application of rule (SensRead). Since the attacker
τ
−−→ M (cid:48) (cid:107) A(cid:48) we can derive that A = A(cid:48) and

43

s?v
−−−→ P (cid:48) for some process P and P ’ such that M = E; S (cid:111)(cid:110) P and M (cid:48) = E; S (cid:111)(cid:110) P (cid:48). By
P
considering rnd ({true, false}) = false for any process Att(ι, 1, C(ι)), we have that Top(C) can only
perform a tick action, and Top(C)
. Hence, by an application of rules (Par) and (SensRead)
we derive M (cid:107) Top(C)

τ
−−→ M (cid:48) (cid:107) Top1(C) = M (cid:48) (cid:107) Top(C).

(cid:69)s!v
−−−−→(cid:54)

– Let M (cid:107) A

τ
−−→ M (cid:48) (cid:107) A(cid:48) be derived by an application of rule (ActWrite). This case is similar to

the previous one.

– Let M (cid:107) A

– Let M (cid:107) A

Top(C)
M (cid:107) Top(C)
– Let M (cid:107) A

τ
−−→ M (cid:48) (cid:107) A(cid:48) be derived by an application of rule ((cid:69)SensSniﬀ (cid:69)). Since M is sound
(cid:69)s?v
−−−−→ A(cid:48). This entails 1 ∈ C (cid:48)((cid:69)s?) ⊆ C((cid:69)s?). By assum-

it follows that M = M (cid:48) and A
ing rnd ({true, false}) = true for the process Att((cid:69)s?, 1, C((cid:69)s?)), it follows that Top(C)
Top1(C) = Top(C). Hence, by applying the rules (Par) and ((cid:69)SensRead (cid:69)) we derive M (cid:107) Top(C)
M (cid:48) (cid:107) Top1(C) = M (cid:48) (cid:107) Top(C).
τ
−−→ M (cid:48) (cid:107) A(cid:48) be derived by an application of rule ((cid:69)ActIntgr (cid:69)). Since M is sound
(cid:69)a!v
it follows that M = M (cid:48) and A
−−−−→ A(cid:48). As a consequence, 1 ∈ C (cid:48)((cid:69)a!) ⊆ C((cid:69)a!). By
assuming rnd ({true, false})=true and rnd (R) = v for the process Att((cid:69)a!, 1, C((cid:69)a!)), it follows that
(cid:69)a!v
−−−−→ Top1(C) = Top(C). Thus, by applying the rules (Par) and ((cid:69)ActIntegr (cid:69)) we derive

(cid:69)s?v
−−−−→
τ
−−→

τ
−−→ M (cid:48) (cid:107) Top1(C) = M (cid:48) (cid:107) Top(C).
τ
−−→ M (cid:48) (cid:107) A(cid:48) be derived by an application of rule (Tau). Let M = E; S (cid:111)(cid:110) P and
τ
M (cid:48) = E(cid:48); S (cid:111)(cid:110) P (cid:48). First, we consider the case when P (cid:107) A
−−→ P (cid:48) (cid:107) A(cid:48) is derived by an application
of either rule ((cid:69)SensIntegr (cid:69)) or rule ((cid:69)ActDrop (cid:69)). Since M is sound and A can perform only
s?v
a!v
−−−→ P (cid:48) and
−−−→ P (cid:48) and A
malicious actions, we have that: (i) either P

(cid:69)s!v
−−−−→ A(cid:48) (ii) or P

(cid:69)a?v
−−−−→ A(cid:48). We focus on the ﬁrst case as the second one is similar.

(cid:69)s!v
−−−−→ A(cid:48), we derive 1 ∈ C (cid:48)((cid:69)s!) ⊆ C((cid:69)s!), and Top(C)

A
(cid:69)s!v
−−−−→ Top1(C) = Top(C), by
Since A
assuming rnd ({true, false}) = true and rnd (R) = v for the process Att((cid:69)s!, 1, C((cid:69)s!)). Thus, by
τ
−−→ M (cid:48) (cid:107) Top1(C) = M (cid:48) (cid:107)
applying the rules ((cid:69)SensIntegr (cid:69)) and (Tau) we derive M (cid:107) Top(C)
Top(C), as required.
τ
To conclude the proof we observe that if P (cid:107) A
−−→ P (cid:48) (cid:107) A(cid:48) is derived by an application
of a rule diﬀerent from ((cid:69)SensIntegr (cid:69)) and ((cid:69)ActDrop (cid:69)), then by inspection of Table 1 and by
deﬁnition of attacker, it follows that A can’t perform a τ -action since A does not use channel
communication and performs only malicious actions. Thus, the only possibility is that the τ -action
is performed by P in isolation. As a consequence, by applying the rules (Par) and (Tau), we derive
M (cid:107) Top(C)

τ
−−→ M (cid:48) (cid:107) Top1(C) = M (cid:48) (cid:107) Top(C).

tick
−−−→ A(cid:48). Hence, it suﬃces to prove that Top(C)

• α = tick. In this case the transition M (cid:107) A
tick
−−−→ M (cid:48) and A

tick
−−−→ M (cid:48) (cid:107) A(cid:48) is derived by an application of rule (Time)
tick
−−−→ Top2(C). We
because M
tick
consider two cases: 1 ∈ C(ι) and 1 (cid:54)∈ C(ι).
−−−→
Att(ι, 2, C(ι)) can be derived by assuming rnd ({true, false}) = false. Moreover, since rnd ({true, false}) =
false the process Att(ι, 1, C(ι)) can only perform a tick action. If 1 (cid:54)∈ C(ι), then the process Att(ι, 1, C(ι))
tick
can only perform a tick action. As a consequence, Att(ι, 1, C(ι))
−−−→
Top2(C). By an application of rule (Time), we derive M (cid:107) Top(C)

If 1 ∈ C(ι), then the transition Att(ι, 1, C(ι))

tick
−−−→ Att(ι, 2, C(ι)) and Top(C)

tick
−−−→ M (cid:48) (cid:107) Top2(C).

• α = deadlock. This case is not admissible because M (cid:107) A

deadlock
−−−−−−−→ M (cid:48) (cid:107) A(cid:48) would entail M

deadlock
−−−−−−−→

M (cid:48). However, M is sound and it can’t deadlock.

• α = unsafe. Again, this case is not admissible because M is sound.

44

ˆt==⇒ M (cid:48) (cid:107)
Inductive case (k > 1). We have to prove that M (cid:107) A
Top#tick(t)+1(C). Since the length of t is greater than 1, it follows that t = t(cid:48)α, for some trace t(cid:48) and some
α
action α. Thus, there exist M (cid:48)(cid:48) and A(cid:48)(cid:48) such that M (cid:107) A
−−→ M (cid:48) (cid:107) A(cid:48). By inductive
ˆt(cid:48)
==⇒ M (cid:48)(cid:48) (cid:107) Top#tick(t(cid:48))+1(C). To conclude the proof, it is enough to
hypothesis, it follows that M (cid:107) Top(C)
ˆα==⇒ M (cid:48) (cid:107) Top#tick(t)+1(C). The reasoning
show that M (cid:48)(cid:48) (cid:107) A(cid:48)(cid:48)
is similar to that followed in the base case, except for actions α = deadlock and α = unsafe that need to be
treated separately. We prove the case α = deadlock as the case α = unsafe is similar.

α
−−→ M (cid:48) (cid:107) A(cid:48) implies M (cid:48)(cid:48) (cid:107) Top#tick(t(cid:48))+1(C)

t
−−→ M (cid:48) (cid:107) A(cid:48) implies M (cid:107) Top(C)

t(cid:48)
−−→ M (cid:48)(cid:48) (cid:107) A(cid:48)(cid:48)

Let M = E; S (cid:111)(cid:110) P . The transition M (cid:48)(cid:48) (cid:107) A

deadlock
−−−−−−−→ M (cid:48) (cid:107) A(cid:48) must be derived by an application of rule
(Deadlock). This implies that M (cid:48)(cid:48) = M (cid:48), A(cid:48)(cid:48) = A(cid:48) and the state function of M is not in the invariant set inv .
Thus, by an application of rule (Deadlock) we derive

M (cid:48)(cid:48) (cid:107) Top#tick(t(cid:48))+1(C)

deadlock
−−−−−−−→ M (cid:48) (cid:107) Top#tick(t(cid:48))+1(C).

Since #tick(t) + 1 = #tick(t(cid:48)) + #tick(deadlock) + 1 = #tick(t(cid:48)) + 1, it follows, as required, that

M (cid:48)(cid:48) (cid:107) Top#tick(t(cid:48))+1(C)

deadlock
−−−−−−−→ M (cid:48) (cid:107) Top#tick(t)+1(C) .

Everything is ﬁnally in place to prove Theorem 3.

Proof of Theorem 3. We have to prove that either M (cid:107) A (cid:118) M or M (cid:107) A (cid:118)m2..n2 M , for some m2 and n2
such that m2..n2 ⊆ m1..n1 (m2 = 1 and n2 = ∞ if the two systems are completely unrelated). The proof
proceeds by contradiction. Suppose that M (cid:107) A (cid:54)(cid:118) M and M (cid:107) A (cid:118)m2..n2 M , with m2..n2 (cid:54)⊆ m1..n1. We
distinguish two cases: either n1 = ∞ or n1 ∈ N+.

If n1 = ∞, then it must be m2 < m1. Since M (cid:107) A (cid:118)m2..n2 M , by Deﬁnition 10 there is a trace t, with
ˆt==⇒. Since

ˆt==⇒. By Lemma 6, this entails M (cid:107) Top(C)

#tick(t) = m2−1, such that M (cid:107) A
M (cid:54)

t
−−→ and M (cid:54)
ˆt==⇒ and #tick(t) = m2−1 < m2 < m1, this contradicts M (cid:107) Top(C) (cid:118)m1..n1 M .
If n1 ∈ N+, then m2 < m1 and/or n1 < n2, and we reason as in the previous case.

A.4 Proofs of Section 5

In order to prove Proposition 7, we need a couple of lemmas.

Lemma 7 is a variant of Lemma 4. Here the behaviour of Sys is parametric on the uncertainty.

Lemma 7. Let Sys be the system deﬁned in Section 3, and 0.4 < γ ≤ 9

20 . Let

Sys[δ ← γ] = Sys1

t1−−−→

tick
−−−→ Sys2 · · ·

tn−1−−−−−→

tick
−−−→ Sysn

such that the traces tj contain no tick-actions, for any j ∈ 1..n−1, and for any i ∈ 1..n Sysi = Si (cid:111)(cid:110) Pi with
Si = (cid:104)ξi

a(cid:105). Then, for any i ∈ 1..n−1 we have the following:

s, ξi

x(temp) ∈ [0, 11.1 + γ] and ξi

x(stress) = 0 if ξi

x(temp) ∈ [0, 10.9 + γ] and,

x, ξi
• if ξi

a(cool ) = oﬀ then ξi
x(stress) = 1;

otherwise, ξi

• if ξi
• if ξi
ξi−k
a
otherwise, ξi

x(temp) ∈ (10.1, 11.1 + γ] then ξi+1
a(cool ) = oﬀ and ξi
a(cool ) = on then ξi
x(temp) ∈ (9.9 − k ∗ (1 + γ), 11.1 + γ − k ∗ (1 − γ)], for some k ∈ 1..5, such that
(cool ) = oﬀ and ξi−j
x(stress) ∈ 1..k+1,
a
x(stress) = 0.

(cool ) = on, for j ∈ 0..k−1; moreover, if k ∈ 1..3 then ξi

(cool ) = on and ξi+1

(stress) ∈ 1..2;

x

a

Proof. Similar to the proof of Lemma 4. The crucial diﬀerence w.r.t. the proof of Lemma 4 is limited to the
x(stress) = 0, when k ∈ 4..5. Now, after 3
second part of the third item. In particular the part saying that ξi
time units of cooling, the state variable stress lays in the integer interval 1..k+1 = 1..4. Thus, in order to

45

x(stress) = 0, when k ∈ 4..5, the temperature in the third time slot of the cooling must be less than or
have ξi
equal to 9.9. However, from the ﬁrst statement of the third item we deduce that, in the third time slot of
cooling, the state variable temp reaches at most 11.1 + γ − 3 ∗ (1 − γ) = 8.1 + 4γ. Thus, Hence we have that
8.1 + 4γ ≤ 9.9 for γ ≤ 9
20

.

The following lemma is a variant of Proposition 1.

Lemma 8. Let Sys be the system deﬁned in Section 3 and γ such that 0.4 < γ ≤ 9
for some t = α1 . . . αn, then αi ∈ {τ, tick}, for any i ∈ 1..n.

20 . If Sys[δ ← γ]

t

−−→ Sys(cid:48),

, the system will never deadlock.

Proof. By Lemma 7, the temperature will always lay in the real interval [0, 11.1 + γ]. As a consequence, since
γ ≤ 9
20
Moreover, after 5 tick action of coolant the state variable temp is in (9.9 − 5 ∗ (1 + γ), 11.1 + γ − 5 ∗
(1 − γ)] = (4.9 − 5γ , 6.1 + 6γ]. Since (cid:15) = 0.1, the value detected from the sensor will be in the real interval
20 ≤ 10, and
(4.8 − 5γ , 6.2 + 6γ]. Thus, the temperature sensed by IDS will be at most 6.2 + 6γ ≤ 6.2 + 6 ∗ 9
no alarm will be ﬁred.

Finally, the maximum value that can be reached by the state variable stress is k+1m for k = 3. As a

consequence, the system will not reach an unsafe state.

The following Lemma is a variant of Proposition 2. Here the behaviour of Sys is parametric on the

uncertainty.

Lemma 9. Let Sys be the system deﬁned in Section 3 and γ such that 0.4 < γ ≤ 9
trace of Sys[δ ← γ] we have the following:

20 . Then, for any execution

• if either process Ctrl or process IDS senses a temperature above 10 then the value of the state variable

temp ranges over (9.9, 11.1 + γ];

• when the process IDS tests the temperature the value of the state variable temp ranges over (9.9 − 5 ∗

(1 + γ), 11.1 + γ − 5 ∗ (1 − γ)].

Proof. As to the ﬁrst statement, since (cid:15) = 0.1, if either process Ctrl or process IDS senses a temperature
above 10 then the value of the state variable temp is above 9.9. By Lemma 7, the state variable temp is less
than or equal to 11.1 + γ. Therefore, if either process Ctrl or process IDS sense a temperature above 10
then the value of the state variable temp is in (9.9, 11.1 + γ].

Let us prove now the second statement. When the process IDS tests the temperature then the coolant
has been active for 5 tick actions. By Lemma 7, the state variable temp ranges over (9.9 − 5 ∗ (1 + γ), 11.1 +
γ − 5 ∗ (1 − γ)].

Everything is ﬁnally in place to prove Proposition 7.

Proof of Proposition 7. For (1) we have to show that Sys[δ ← γ] (cid:118) Sys, for γ ∈ ( 8
holds by Lemma 8.

20 , 9

20 ). But this obviously

As regards item (2), we have to prove that Sys[δ ← γ] (cid:54)(cid:118) Sys, for γ > 9
20

. By Proposition 1 it is enough
to show that the system Sys[δ ← γ] has a trace which either (i) sends an alarm, or (ii) deadlocks, or (iii)
enters in an unsafe state. We can easily build up a trace for Sys[δ ← γ] in which, after 10 tick-actions, in the
11-th time slot, the value of the state variable temp is 10.1. In fact, it is enough to increase the temperature
of 1.01 degrees for the ﬁrst 10 rounds. Notice that this is an admissible value since, 1.01 ∈ [1 − γ, 1 + γ], for
any γ > 9
. Being 10.1 the value of the state variable temp, there is an execution trace in which the sensed
20
temperature is 10 (recall that (cid:15) = 0.1) and hence the cooling system is not activated but the state variable
stress will be increased. In the following time slot, i.e., the 12-th time slot, the temperature may reach at most
the value 10.1 + 1 + γ and the state variable stress is 1. Now, if 10.1 + 1 + γ > 50 then the system deadlocks.
Otherwise, the controller will activate the cooling system, and after 3 time units of cooling, in the 15-th time
slot, the state variable stress will be 4 and the variable temp will be at most 11.1 + γ − 3(1 − γ) = 8.1 + 4γ.
Thus, there is an execution trace in which the temperature is 8.1 + 4γ, which will be greater than 9.9 being

46

. As a consequence, in the next time slot, the state variable stress will be 5 and the system will enter

γ > 9
20
in an unsafe state.

This is enough to derive that Sys[δ ← γ] (cid:54)(cid:118) Sys, for γ > 9
20

.

Proof of Theorem 4. We consider the two parts of the statement separately.
Deﬁnitive impact. By an application of Lemma 6 we have that M (cid:107) A

ˆt==⇒.
This implies M (cid:107) A (cid:118) M (cid:107) Top(C). Thus, if M (cid:107) Top(C) (cid:118) M [ξw ← ξw+ξ], for ξ ∈ R ˆX , ξ > 0, then, by
transitivity of (cid:118), it follows that M (cid:107) A (cid:118) M [ξw ← ξw+ξ].

t
−−→ entails M (cid:107) Top(C)

Pointwise impact. The proof proceeds by contradiction. Suppose ξ(cid:48) > ξ. Since Top(C) has a pointwise

impact ξ at time m, it follows that ξ is given by:

inf (cid:8)ξ(cid:48)(cid:48) : ξ(cid:48)(cid:48)∈R ˆX ∧ M (cid:107) Top(C) (cid:118)m..n M [ξw ← ξw+ξ(cid:48)(cid:48)], n ∈ N+ ∪ ∞(cid:9).

Similarly, since A has a pointwise impact ξ(cid:48) at time m(cid:48), it follows that ξ(cid:48) is given by

inf (cid:8)ξ(cid:48)(cid:48) : ξ(cid:48)(cid:48)∈R ˆX ∧ M (cid:107) A (cid:118)m(cid:48)..n M [ξw ← ξw+ξ(cid:48)(cid:48)], n ∈ N+ ∪ ∞(cid:9).

Now, if m = m(cid:48), then ξ ≥ ξ(cid:48) because M (cid:107) A

ˆt==⇒ due to an application
of Lemma 6. This is contradiction with the fact that ξ < ξ(cid:48). Thus, it must be m(cid:48) < m. Now, since
both ξ and ξ(cid:48) are the inﬁmum functions and since ξ(cid:48) > ξ, there are ξ and ξ(cid:48), with ξ≤ξ≤ξ(cid:48)≤ξ(cid:48) such that:
(i) M (cid:107) Top(C) (cid:118)m..n M [ξw ← ξw+ξ], for some n; (ii) M (cid:107) A (cid:118)m(cid:48)..n(cid:48) M [ξw ← ξw+ξ(cid:48)], for some n(cid:48).

t
−−→ entails M (cid:107) Top(C)

From M (cid:107) A (cid:118)m(cid:48)..n(cid:48) M [ξw ← ξw+ξ(cid:48)] it follows that there exists a trace t with #tick(t) = m(cid:48) − 1 such
ˆt==⇒. Since ξ ≤ ξ(cid:48), by monotonicity (Proposition 6), we deduce that

t
−−→ and M [ξw ← ξw+ξ(cid:48)] (cid:54)

that M (cid:107) A

M [ξw ← ξw+ξ] (cid:54)

ˆt==⇒. Moreover, by Lemma 6 M (cid:107) A

t
−−→ entails M (cid:107) Top(C)

ˆt==⇒.

Summarising, there exists a trace t(cid:48) with #tick(t(cid:48)) = m(cid:48) − 1 such that M (cid:107) Top(C)

t(cid:48)
−−→ and M [ξw ←
ˆt(cid:48)
==⇒. However, this, together with m(cid:48) < m, is in contradiction with the fact (i) above saying that
ξw+ξ] (cid:54)
M (cid:107) Top(C) (cid:118)m..n M [ξw ← ξw+ξ], for some n. As a consequence it must be ξ(cid:48) ≤ ξ and m(cid:48) ≤ m. This
concludes the proof.

Proof of Proposition 8. Let us prove the ﬁrst sub-result. From Proposition 5 we know that Sys (cid:107) A10 (cid:118)14..21
Sys. In particular we showed that the system Sys (cid:107) A10 has an execution trace which is in an unsafe state
from the 14-th to the 21-th time interval and ﬁres only one alarm in the 16-th time slot, and which cannot be
matched by Sys.

Hence, if in the 14-th time slot the the system is in unsafe state, then the temperature in the 9-th time
slot must be greater than 9.9. Moreover, to ﬁre an allarm in the in the 16-th time slot, the cooling must be
activated in the 11-th time slot and hence the temperature in the 10-th time slot must be less or equal than
10.1 (recall that (cid:15) = 0.1). But this is impossible since in the 9-th time slot temp is greater than 9.9, and, the
minimum increasing of the temperature is 1 − γ = 0.2.

As a consequence, for γ ≤ 0.8 we have

Let us prove the second sub-result. That is,

Sys (cid:107) A10 (cid:54)(cid:118) Sys{γ/δ} .

Sys (cid:107) A10 (cid:118) Sys{γ/δ}

for γ > 0.8. Formally, we have to demonstrate that whenever Sys (cid:107) A10
Sys{γ/δ}
temporanely attack, the trace t does not contain deadlock-action. We distinguish four possible cases.

t
−−→, for some trace t, then
ˆt==⇒ as well. Let us do a case analysis on the structure of the trace t. We notice that, since A10 is a

47

• The trace t contains contains only τ -, tick-, alarm- and unsafe-actions. Firstly, notice that the system
Sys (cid:107) A10 may produce only one output on channel alarm, in the 16-th time slot. After that, the trace
will have only τ - and tick-actions. In fact, in Proposition 5 we provided a trace in which, in the 16-th
time slot, the state variable temp reaches the value 10.5 and an output on channel alarm is emitted.
This is the maximum possible value for variable temp in that point in time. After the transmission of
the alarm, the system Sys (cid:107) A10 activates the cooling for the following 5 time slots. Thus, in the 21-th
time slot, the temperature will be at most 10.5 − 5 ∗ (1 − δ) = 10.5 − 5 ∗ (0.6) = 7.5, and no alarm is
ﬁred. From that time on, since the attack A10 terminated is life in the 10-th time slot, no other alarms
will be ﬁred. Moreover, in Proposition 5, we have showed that Sys (cid:107) A10 is in an unsafe state from the
14-th to the 21-th time interval.
Summarising Sys (cid:107) A10 is in an unsafe state from the 14-th to the 21-th time interval and ﬁres only
one alarm in the 16-th time slot.
By monotonicity (Proposition 6), it is enough to show that such a trace exists for Sys[δ ← γ], with
0.8 < γ ≤ 0.81. In fact, if this trace exists for 0.8 < γ ≤ 0.81, then it would also exist for γ > 0.81. In
the following, we show how to build the trace of Sys[δ ← γ] which simulates the trace t of Sys (cid:107) A10.
We can easily build up a trace for Sys{γ/δ} in which, after 8 tick-actions, in the 9-th time slot, the value
of the state variable temp is in 9.1 + γ. In fact, it is enough to increase the temperature of a value 9.1+γ
degrees for the ﬁrst 8 rounds. Notice that this is an admissible value since, 9.1+γ
is in [1 − γ, 1 + γ], for
any 0.8 < γ ≤ 0.81. Moreover, since γ > 0.8, in the 9-th time slot we have that 9.1 + γ > 9.9. Now, in
the in the 10-th time slot, temp may reach 9.1 + γ + (1 − γ) = 10.1. Being 10.1 the value of the state
variable temp, there is an execution trace in which the sensed temperature is 10 (recall that (cid:15) = 0.1)
and hence the cooling system is not activated. However, in the following time slot, i.e. the 11-th time
slot, the temperature may reach the value 11.8, imposing the activation of the cooling system (notice
1.7 is an admissible increasing).
Summarizing, in the 9-th time slot the temperature is greater than 9.9 and in the 11-th time slot the
cooling system is activated with a temperature equal to 11.8. The thesis follows from the following two
facts:

8

8

– Since in the 9-th time slot the temperature is greater than 9.9, then in the 14-th time slot the
system enters in an unsafe state. Since in the 11-th time slot the cooling system is activated with a
temperature equal to 11.8, then, in the 20-th time slot, after 9 time units of cooling, the temperature
may reach the value 11.8 − 9(1 − γ) = 2.8 + 9γ which will be greater than 9.9 being γ > 0.8. Hence,
in the 21-th time slot, the system still be in an unsafe state. Finally, in the 21-th time slot, after
10 time units of cooling, the temperature may reach the value 11.8 − 10(1 − γ) = 1.8 + 10γ which
will be less or equal to 9.9 being γ ≤ 0.81. Hence, in the 22-nd time slot, the variable stress is
reset to 0 and the system enters in a safe state. From that time on, since Sys{γ/δ} can mimic all
traces of Sys, we can always choose a trace which does not enter in an unsafe state any more.
Summarising Sys (cid:107) A10 is in an unsafe state from the 14-th to the 21-th time interval.

– Since in the 11-th time slot the cooling system is activated with a temperature equal to 11.8, then,
in the 16-th time slot, the temperature may reach the value 11.8 − 5(1 − γ) = 6.8 + 5γ. Since
(cid:15) = 0.1, the sensed temperature would be in the real interval [6.7 + 5γ, 6.9 + 5γ]. Thus, the sensed
temperature is greater than 10 being γ > 0.8. Thus, the alarm will be transmitted, in the 16-th
time slot, as required. After the transmission on channel alarm, the system Sys{γ/δ} activates the
cooling for the following 5 time slots. As a consequence, in 21-th time slot, the temperature will
be at most 6.8 + 5γ − 5 ∗ (1 − γ) = 1.8 + 10γ. Since we assumed 0.8 < γ ≤ 0.81 the temperature
will be well below 10 and no alarm will be sent. From that time on, since Sys{γ/δ} can mimic all
traces of Sys, we can always choose a trace which does not ﬁre the alarm any more.
Summarising Sys (cid:107) A10 ﬁres only one alarm in the 16-th time slot.

• The trace t contains contains only τ -, tick- and unsafe-actions. This case is similar to the previous one.
• The trace t contains only τ -, tick- and alarm-actions. This case cannot occur. In fact, an alarm-action

can not occur without un unsafe-action.

48

• The trace t contains only τ - and tick-actions. If the system Sys (cid:107) A10 has a trace t which contains only
τ - and tick-actions then, by Proposition 1, the system Sys in isolation must have a similar trace with
the same number of tick-actions. By an application of Proposition 6, as δ < γ, any trace of Sys can be
simulated by Sys{γ/δ}. As a consequence, Sys{γ/δ}

ˆt==⇒.

This is enough to derive that:

Sys (cid:107) A10 (cid:118) Sys{γ/δ} .

Proof of Proposition 9. Let us prove the ﬁrst sub-result. As demonstrated in Example 2, we know that
Sys (cid:107) A (cid:118)14..∞ Sys because in the 14-th time slot the compound system will violate the safety conditions
emitting an unsafe-action until the invariant will be violated. No alarm will be emitted.

Since the system keeps violating the safety condition the temperature must remain greater than 9.9.
As proved for Lemma 7 we can prove that we have that the temperature is less than or equal to 11.1 + γ.
Hence, in the time slot before getting in deadlock, the temperature of the system is in the real interval
(9.9, 11.1 + γ]. To deadlock with one tick action and from a temperature in the real interval (9.9, 11.1 + γ],
either the temperature reaches a value greater than 50 (namely, 11.1 + γ + 1 + γ > 50) or the temperature
reaches a value less than 0 (namely, 9.9 − 1 − γ < 0 ). Since γ ≤ 8.9, both cases can not occur. Thus, we
have that

Let us prove the second sub-result. That is,

Sys (cid:107) A (cid:54)(cid:118) Sys[δ ← γ] .

Sys (cid:107) A (cid:118) Sys[δ ← γ]

for γ > 8.9. We demonstrate that whenever Sys (cid:107) A
will proceed by case analysis on the kind of actions contained in t. We distinguish four possible cases.

t
−−→, for some trace t, then Sys[δ ← γ]

ˆt==⇒ as well. We

• The trace t contains contains only τ -, tick-, unsafe- and deadlock-actions. As discussed in Example 2,
Sys (cid:107) A (cid:118)14..∞ Sys because in the 14-th time slot the system will violate the safety conditions emitting
an unsafe-action until the invariant will be broken. No alarm will be emitted. Note that, when Sys (cid:107) A
enters in an unsafe state then the temperature is at most 9.9 + (1 + δ) + 5(1 + δ) = 9.9 + 6(1.4) = 18.3.
Moreover, the fastest execution trace, reaching an unsafe state, deadlocks just after (cid:100) 50−18.3
1.4 (cid:101) =
23 tick-actions. Hence, there are m, n ∈ N, with m ≥ 14 and n ≥ m + 23, such that the trace t of
Sys (cid:107) A satisﬁes the following conditions: (i) in the time interval 1..m − 1 the trace t of is composed by
τ - and tick-actions; (ii) in the time interval m..(n − 1), the trace t is composed by τ -, tick- and unsafe-
actions; (iii) in the n-th time slot the trace t deadlocks.
By monotonicity (Proposition 6), it is enough to show that such a trace exists for Sys[δ ← γ], with
8.9 < γ < 9. In fact, if this trace exists for 8.9 < γ < 9, then it would also exist for γ ≥ 9. In the
following, we show how to build the trace of Sys[δ ← γ] which simulates the trace t of Sys (cid:107) A. We
build up the trace in three steps: (i) the sub-trace from time slot 1 to time slot m−6; (ii) the sub-trace
from the time slot m−5 to the time slot n−1; (iii) the ﬁnal part of the trace reaching the deadlock.

1+δ (cid:101) = (cid:100) 31,7

(i) As γ > 8.9 (and hence 1 + γ > 9.9), the system may increment the temperature of 9.9 degrees
after a single tick-action. Hence, we choose the trace in which the system Sys[δ ← γ], in the
second time slot, reaches the temperature equal to 9.9. Moreover, the system may maintain
this temperature value until the (m−6)-th time slot (indeed 0 is an admissible increasing since
0 ∈ [1 − γ, 1 + γ] ⊇ [−7.9, 10.9]) . Obviously, with a temperature equal to 9.9, only τ - and
tick-actions are possible.

(ii) Let k ∈ R such that 0 < k < γ − 8.9 (such k exists since γ > 8.9). We may consider an increment
of the temperature of k. This implies that in the (m−5)-th time slot, the system Sys[δ ← γ] may
reach the temperature 9.9 + k. Note that k is an admissible increment since 0 < k < γ − 8.9

49

and 8.9 < γ < 9 entails k ∈ (0, 0.1). Moreover, the system may maintain this temperature value
until the (n−1)-th time slot (indeed, as said before, 0 is an admissible increment). Summarising
from the (m−5)-th time slot to the (n−1)-th time slot, the temperature may remain equal to
9.9 + k ∈ (9.9, 10). As a consequence, from the m-th time slot to the (n−1)-th time slot the system
Sys[δ ← γ] may enter in an unsafe state. Thus, an unsafe-action may be performed in the time
interval m..(n−1). Moreover, since (cid:15) = 0.1 and the temperature is e 9.9 + k ∈ (9.9, 10), we can
always assume that the cooling is not activated until the (n−1)-th time slot. This implies that
neither alarm nor deadlock occur.

(iii) At this point, since in the (n−1)-th time slot the temperature is equal to 9.9 + k ∈ (9.9, 10) (recall
that k ∈ (0, 1)), the cooling may be activated. We may consider a decrement of 1+γ. In this manner,
in the n-th time slot the system may reach a temperature of 9.9 + k − (1 + γ) < 9.9 + 0 − 1 − 8.9 = 0
degrees, and the system Sys[δ ← γ] will deadlock.

Summarising, for any γ > 8.9 the system Sys[δ ← γ] can mimic any trace t of Sys (cid:107) A.

• The trace t contains contains only τ -, tick- and unsafe-actions. This case is similar to the previous one.
• The trace t contains only τ -, tick- and alarm-actions. This case cannot occur. In fact, as discussed in
Example 2, the process Ctrl never activates the Cooling component (and hence also the IDS component,
which is the only one that could send an alarm) since it will always detect a temperature below 10.
• The trace t contains only τ - and tick-actions. If the system Sys (cid:107) A has a trace t that contains only τ -
and tick-actions, then, by Proposition 1, the system Sys in isolation must have a similar trace with
the same number of tick-actions. By an application of Proposition 6, as δ < γ, any trace of Sys can be
simulated by Sys[δ ← γ]. As a consequence, Sys[δ ← γ]

ˆt==⇒.

This is enough to obtain what required:

Sys (cid:107) A (cid:118) Sys[δ ← γ] .

50

