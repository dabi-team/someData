7
1
0
2

c
e
D
0
2

]

R
C
.
s
c
[

1
v
1
7
6
7
0
.
2
1
7
1
:
v
i
X
r
a

Tracking Cyber Adversaries with Adaptive
Indicators of Compromise

Justin E. Doak (JD)+, Joe B. Ingram, Sam A. Mulder, John H. Naegle,
Jonathan A. Cox*, James B. Aimone, Kevin R. Dixon, Conrad D. James
Sandia National Laboratories (SNL)
Albuquerque, NM, USA
Email: {jedoak, jbingra, samulde, jhnaegl, jbaimon, krdixon, cdjame}@sandia.gov
+Contact Author, *Present Address: Netradyne, San Diego, CA, USA joncox@alum.mit.edu

David R. Follett
Lewis Rhodes Laboratories
West Concord, MA, USA
Email: drfollett@earthlink.net

Abstract—A forensics investigation after a breach often uncov-
ers network and host indicators of compromise (IOCs) that can
be deployed to sensors to allow early detection of the adversary
in the future. Over time, the adversary will change tactics,
techniques, and procedures (TTPs), which will also change the
data generated. If the IOCs are not kept up-to-date with the
adversary’s new TTPs, the adversary will no longer be detected
once all of the IOCs become invalid. Tracking the Known (TTK)
is the problem of keeping IOCs, in this case regular expressions
(regexes), up-to-date with a dynamic adversary. Our framework
solves the TTK problem in an automated, cyclic fashion to
bracket1 a previously discovered adversary. This tracking is
accomplished through a data-driven approach of self-adapting
a given model based on its own detection capabilities.

In our initial experiments, we found that the true positive rate
(TPR) of the adaptive solution degrades much less signiﬁcantly
over time than the na¨ıve solution, suggesting that self-updating
the model allows the continued detection of positives (i.e., adver-
saries). The cost for this performance is in the false positive rate
(FPR), which increases over time for the adaptive solution, but
remains constant for the na¨ıve solution. However, the difference in
overall detection performance, as measured by the area under the
curve (AUC), between the two methods is negligible. This result
suggests that self-updating the model over time should be done
in practice to continue to detect known, evolving adversaries.

Keywords: Cyber Defense Technologies and Strategies, Novel
Security Tools, Network Security, Innovative Tools for Cyber
Defense Technologies, Intrusion Detection Techniques

Full/Regular Research Papers, CSCI-ISCW

I. INTRODUCTION

Once a corporate or government entity discovers that its
network has been compromised, a response team begins an
investigation to respond to the breach. As part of the inves-
tigation, heavy use is made of memory, disk, and network
forensic capabilities. The cyber security analysts look for
IOCs, such as IP addresses, domain names, ﬁle hashes, and
Windows Registry keys, that will allow them to automatically
detect the adversary in the future. Additionally, the analyst
may create other more advanced signatures, such as regular
expressions, to detect the adversary. As adversaries change
their TTPs, some of the IOCs lose their effectiveness. If all of
the IOCs become invalid, the adversary will be able to engage

1“Bracket” in this context means to contain a known adversary. We contain
him or her by keeping deployed IOCs up-to-date with his or her current TTPs.

in malicious activities without being detected. It is likely that
the adversary will be rediscovered after a future breach and
ensuing incident response and then this process of extracting
and deploying IOCs to sensors is repeated. A better approach
to this problem would be to automatically adapt the IOCs
so that known adversaries can continue to be tracked as they
change their TTPs and the data they generate correspondingly
drifts over time.

The TTK problem is one of the fundamental challenges in
cyber security. The complex adversary and defender dynamics
that are inherent in this problem can be modeled as a non-
cooperative game in game theory [5]. Adversaries are aware
that defenders have deployed signatures to sensors to detect
their activity, and this leads them to constantly change TTPs
such as command and control (C&C) messaging, the IPs and
domains of servers used in C&C, and exploits directed at dis-
covered vulnerabilities. In addition, many of these indicators
are extremely brittle, which makes it easy for the adversary to
intentionally manipulate his or her footprint to avoid detection.
For example, if an IOC is a key in a Registry entry, the
adversary simply needs to write to a different location and
then the indicator will be invalidated. If the indicator is the
IP address of a server used in C&C communication,
the
adversary simply needs to use another IP address and the IOC
is no longer valid. Similarly, the defender is also aware that
the adversaries are constantly changing their tactics to avoid
detection. Manually updating the IOCs to continue to bracket
the adversary as behavior changes is not possible given current
resource allocations.

In this work, we describe a framework for solving the
Tracking the Known problem. Starting with a base model of
indicators, our framework automatically updates the current
model based on its predicted labels on the data stream. The
intent is to self-adapt the model to concept drift in the data
stream via a data-driven approach. In cyber defense, analysts
often rely on regular expressions as indicators to detect adver-
saries. A regular expression is a sequence of characters that
concisely represents a search pattern. Therefore, as an initial
demonstration, we explore inducing regular expressions from
labeled data. Many such algorithms have been provided for
learning regular expressions due to the Regex Golf problem

 
 
 
 
 
 
[16]: given two datasets of strings, create the shortest regular
expression that matches all the strings in one set while not
matching any of the strings in the other.

II. RELATED WORK

To the best of our knowledge, there has been no prior work
on the TTK problem. However, the notion of adapting or
updating a model based on its own outcomes is known as
self-training in the semi-supervised learning literature [11].

Regular expressions can be used to express a regular lan-
guage, which is a formal language in theoretical computer
science. Finite automata can be used to recognize regular
languages. In fact, regular expressions and ﬁnite automata are
known to be equivalent by Kleene’s theorem [8]. Argyros et
al. inferred web application ﬁrewall ﬁlters (i.e., regular ex-
pressions) by generating queries and observing the responses.
They showed an improvement over existing automata learning
algorithms by reducing the number of required queries by 15×
through the use of symbolic representations [1]. This has obvi-
ous overlap with Regex Golf since the queries that are ﬁltered
can be considered part of the adversary dataset and those
that are passed can be considered part of the non-adversary
dataset. Prasse et al. attempted to infer a regular expression
that matched a given set of strings (i.e., email messages) and
was as close as possible to the regular expression that a human
expert would have created. They applied their method to the
problem of identifying spam email messages. Their technique
frequently predicted the exact regular expression a human
expert would have created or the predicted regular expression
was accepted by the expert with little modiﬁcation [12].
Becchi et al. showed how ﬁnite automata can be extended to
accommodate Perl-Compatible Regular Expressions (PCRE)
[3].

Regular expressions have been used in many applications to
solve challenging cyber security problems. Micron Technol-
ogy, Inc. built a massively parallel semiconductor architecture
that directly implemented regular expressions in hardware [4].
The Machine Learning Lab at the University of Trieste [18]
recently used genetic programming to learn regular expres-
sions [2]. The Norvig solution [10] discussed in Section IV
was used as their baseline for comparison.

III. TRACKING THE KNOWN

The TTK problem is essentially a decision-theoretic prob-
lem on a data stream D. Given x ∼ D, where x represents
some cyber datum/event (e.g., an HTTP session, Windows
Registry key, IP address),
to
determine if x was generated by an adversary or not, which is
typically labeled by y ∈ {0, 1}, where y = 1 denotes the class
of interest (i.e., the positive class). However, if the model is
not updated against the stream, its performance will degrade
over time as the data stream D drifts.

is to use a model

the goal

Algorithm 1 shows the process for updating the model in the
TTK framework. For a given window w, the current model is
used to label an event x and then x is added to the appropriate
set (P for a positive prediction and N for a negative). At the

Fig. 1. High-level View of the Implementation of the TTK Framework

Algorithm 1: Algorithm for Self-Updating Existing Model
Input: Current model, m; Window size, w;
Data stream, D; Algorithm, A

Output: Updated model: ˆm
P = {}
N = {}
for i = 1 to w do

x ∼ D
ˆy = m(x)
if ˆy == 1 then

(cid:46) Draw event from stream
(cid:46) Get model’s prediction

(cid:46) Add event to positive set
P = P ∪ {x}

else

(cid:46) Add event to negative set
N = N ∪ {x}

end

end
ˆm = m ∪ A(P, N )
return ˆm

(cid:46) Update model

end of the window, an algorithm A is used to derive a new
model ˆm, which is then concatenated with the current model
in an ensemble-like fashion. (Some algorithms might update
m directly.) Then, the process is repeated on a new window.
For the use case considered in this paper, the model is
a list of regular expressions that attempts to match on the
positive class, potentially indicating the presence of adversarial
activity. However, the framework is generic and, for instance,
the model could be induced using other machine learning
algorithms. Basically, any algorithm that can categorize x as
being generated by the adversary or not could be utilized by
this framework.

In order to solve the TTK problem, we created an auto-
mated, cyclic pipeline to keep models of IOCs up-to-date with
the data generated by an adversary. Fig. 1 shows a high-level
view of the implementation of our system. The cyclic nature
of the pipeline can be described by the following stages:

1) Collect Data: As cyber data is extremely voluminous,
it must be captured, processed, and stored in a near real-time,
streaming manner. Therefore, a Data Stream Processor must be
efﬁcient enough to process streaming cyber data without losing
events. Additionally, custom, high-performance, neuromorphic
hardware was utilized to further increase performance where
possible.

The Event Labeler is used to warm-start

the detection
system by annotating events based on alternative mechanisms,
such as a threat feed or an analyst’s detection rules. Addition-
ally, a human-in-the-loop might also be employed to relabel
events and adapt the system.

2) Ingest Data: A high-speed Ingest Engine was developed
to ingest the processed cyber data into a High-Performance
Database. This database stores historical cyber events that
have been annotated. This historical data allows models to
be developed and validated before deployment.

3) Partition Data: Depending on the task of interest, a user
can partition the data as needed and derive a model based on
that data.

4) Generate Model: The model can be induced from any
algorithm that can discriminate between two or more sets, such
as supervised machine learning.

the model

is validated against

5) Validate Model: Before being deployed to the produc-
tion system,
the historical,
annotated data. Standard statistical machine learning perfor-
mance metrics are calculated (e.g., true positive rate, false
positive rate, etc.). If the model performs well historically,
it is deployed to the production system.

6) Deploy Model: During this stage, the new model is
deployed and used by the Data Stream Processor to process
and annotate incoming cyber data.

IV. LEARNING REGULAR EXPRESSIONS

In this work, our approach to solving TTK requires an
algorithm for learning regular expressions: given two datasets
of strings, create the shortest regular expression that matches
all the strings in one dataset while not matching any of the
strings in the other dataset. As long as any of the IOCs remain
valid, our solution continues to track the adversary and hence
identify the data that he or she produces. This data goes in
one dataset while data not generated by the adversary goes in
another. The algorithm for deriving regular expressions is then
repeated to identify a more robust set of regular expressions
for bracketing the adversary and then those new, more current,
rules are deployed.

If it is assumed that it is possible to create two distinct sets
as required by Norvig’s algorithm, there is a straightforward
solution: combine the full contents of the adversary dataset
using logical disjunction (i.e., the OR operator) to create a
very long regular expression that provides a perfect solution.
Of course, this solution is unwieldy, so we want to optimize
the size of the regular expression required to separate the two
sets. A regular expression to do this will consist of some set
of matching conditions OR-ed together. This problem can be
generalized as the set cover problem, usually stated as:

Given a set of elements U = 1, 2, ..., n and a collection S
of m sets whose union equals U , identify the smallest sub-
collection of S whose union equals U .

As an example, consider U as all integers from 1 to 10,

and S = {{1, 2}, {2, 3, 4, 5}, {2, 4, 6}, {4, 6, 8}, {1, 3, 5},
{7, 9}, {1, 10}}. The union of all sets in S is U , mean-
ing S covers U , but we
sub-
collection that
case,
{{1, 10}, {2, 3, 4, 5}, {4, 6, 8}, {7, 9}} also covers U .

smaller
In this

this property.

can ﬁnd a

still has

Optimizing to ﬁnd the smallest collection that covers the
original set is an NP-hard problem, with the decision version2
of the problem proved NP-complete in [7]. Given this, the best
way to approach the problem is through approximation.

Norvig’s algorithm begins by generating a set of regex
components as follows. Each dataset is ﬁrst broken into n-
grams with n ranging in size from 1 to the length of the longest
string in the dataset and then a set of all possible subsequences
of these sizes is created. For this set, a ’.’ is then added in
every possible iteration of these subsequences replacing some
number of characters. Next, regular expression components
are created by inserting special repetition characters, such
as ’+’, ’*’, or ’?’, after each character that
is not ’.’ in
every possible combination. From this set, any component that
matches anything in the non-adversary dataset is ﬁltered out.
This provides a set of components to choose from that match
at least one string in the adversary set and no strings in the
non-adversary set.

These components are ranked based on how many strings
they match and the best
is added to a solution set. The
strings already covered are removed from consideration, and
the remaining components are again ranked by how many of
the remaining strings they match. This process is repeated until
all strings are covered. This is a purely greedy algorithm.

V. CASE STUDY: SELF-ADAPTIVE BLOCK LIST

In order to adequately test the TTK framework, we selected
a common cyber security problem that could be solved by
our framework, collected and processed data for an extended
period of time, and performed experiments to validate our so-
lution. A common task in network defense involves developing
and deploying rules to detect and block potentially malicious
network trafﬁc. Therefore, we selected this problem as an
initial case study.

A. Collect Data

The raw data are the ingress and egress packets collected
at a corporate network border. The majority of this data are
Hypertext Transfer Protocol (HTTP) and HTTP over Transport
Layer Security (HTTPS) ﬂows initiated by users on the inside
of the network as they visit various sites on the internet. A
streaming analysis tool was used to decode the HTTP ﬂows
and extract the metadata/features.

Note that

the Event Labeler attached the bootstrapping
labels from a blacklist to the HTTP ﬂows. After these labels

2The optimization problem involves ﬁnding a solution, while the decision

version involves determining if a solution exists.

been loaded into the database. In an actual cyber security
context, these initial labels (or more generally IOCs) would
have been extracted by cyber security analysts in the context
of a forensics investigation into a breach.

The output of the Data Stream Processor is a set of tab-
separated values (TSV) ﬁles containing the HTTP metadata
and the labels of any regular expressions that matched. We
developed an ingest engine to read the TSV ﬁles and then,
using the Apache Phoenix [15] API,
loaded those into a
database, Apache HBase [14].

C. Partition Data

Our solution required that the incoming HTTP ﬂows be split
into the dataset of interest and the dataset not of interest,
to
based on the speciﬁc task that an analyst might want
automate. Then, a set of regular expressions to detect the
dataset of interest was derived and deployed to the sensor.
After deployment, it was the regular expressions themselves,
not the bootstrapping labels from the blacklist, that bifurcated
the incoming ﬂows into the appropriate datasets.

D. Generate Model

Once the data has been split into the two datasets, we
play Regex Golf to generate the more up-to-date regular
expressions to bracket the threat. Our implementation is based
on a solution by Peter Norvig [9], [10]. Norvig’s solution
requires that the datasets be disjoint, i.e., no string can appear
in both the adversary and the non-adversary datasets. This
assumption may be problematic in the cyber security context.
For example, a network sensor may detect a downloaded ﬁle
that could appear both in conjunction with a malware toolset
but also occur normally as part of a legitimate toolset. A
good example of this is the netcat program, which is often
an indicator of malware, but is also a legitimate sysadmin
tool. In fact, many antivirus products will ﬂag a zip ﬁle if
it contains netcat as potential malware. Norvig’s algorithm
will also always ﬁnd a perfect solution, i.e., it will ﬁnd a set
of regular expressions that match everything in the adversary
dataset and nothing in the non-adversary dataset. Relaxing
these two constraints is an area of future work that might lead
to an algorithm that is better-suited for deployment. Section IV
provides more detail on this algorithm.

E. Validate Model

A method to validate regular expressions after they were
generated by the Regex Golf model was developed. First,
a vector of tuples for each dataset showing the true class
of each string (e.g., adversary) and the class predicted by
the algorithm was created. These vectors were passed to the
scikit-learn library [19] to calculate standard metrics
such as precision, recall, accuracy, etc. Given the limitations
of the current algorithm, these metrics are all currently perfect.
However, if we relax the perfect classiﬁcation constraint, these
metrics could become meaningful.

Fig. 2. Execution Time as Regular Expressions are Added for CPU and NPU

were attached, the initial adversary and non-adversary datasets
were created from which the initial regular expressions were
derived. The Event Labeler was no longer needed at this
point as future labels were provided by the regular expressions
themselves.

Two different methods were developed to process the reg-
ular expression matching. For ease of initial development of
the overall system, the WaterSlide open source software tool
was used, which is a modular metadata processing engine
that is highly optimized for processing streaming data [13].
WaterSlide has a module that uses the standard re2 library, a
highly optimized regular expression engine, for processing reg-
ular expressions [17]. While the re2 library is very efﬁcient,
processing complex regular expressions is computationally
intensive and represented a signiﬁcant performance bottleneck
for the system.

As part of the research to develop the TTK system, a module
for WaterSlide was developed that utilizes an FPGA-based
regular expression processing accelerator, i.e., the Neural Pro-
cessing Unit (NPU) [6]. The architecture for the NPU was mo-
tivated by the latest understanding of neuromorphic processing
models and greatly accelerated the processing of many regular
expressions as demonstrated in Fig. 2. The implementation of
the NPU was a PCI attached FPGA development board. The
NPU API was transparently implemented, completely hiding
the complexity of the NPU in a generally available library.

B. Ingest Data

The output of the data collection, the HTTP metadata and
the labels of any regular expressions that matched, was loaded
into a database. One of the challenges was how to seed
or bootstrap the system to create the initial set of regular
that our approach relied on a dataset
expressions. Recall
representing adversarial activity and a dataset representing
non-adversarial activity to generate the regular expressions.
Initially, there were no deployed regular expressions, hence
they could not be used to split the incoming stream into the
two datasets. To create the initial datasets, a blacklist [20]
was used to classify the various domains into, e.g., ads, news,
e-commerce, banking, and dating after the HTTP data had

F. Deploy Model

The regular expressions learned by the Regex Golf model
were written to a ﬁle, along with appropriate labels, after the
generation and validation processes. The streaming analysis
engine recognized that the ﬁle had been modiﬁed, which trig-
gered a reload of that ﬁle and its associated regular expressions
and labels. These regular expressions were compiled by the
NPU and then the NPU was used to accelerate application
of the regular expressions to the network data. The system
has now returned to the beginning of the pipeline and data
was collected using these newly-deployed regular expressions.
This created new adversary and non-adversary datasets and the
automated, cyclic process continued.

VI. EXPERIMENTAL RESULTS

As an initial experiment, we investigated the problem of
tracking advertising domains. Data was collected at a corporate
network border for two weeks, resulting in approximately 23
million unique HTTP ﬂows. Of those ﬂows, roughly 34%
were identiﬁed as advertising domains by the list of domain
categories, which was used as ground truth.

The TTK problem is analogous to ﬁnding positive examples
in a binary classiﬁcation task. Therefore, some common met-
rics for estimating classiﬁcation performance were used. The
true positive rate (TPR) is the fraction of positive examples
that were correctly identiﬁed by the model and is represented
as TPR = TP/P where TP is the number of positives correctly
classiﬁed and P is the total number of positive samples. The
false positive rate (FPR) is the fraction of negative examples
(non-advertising domains) that were identiﬁed as advertising
domains. It is deﬁned as FPR = FP/N, where FP is the number
of negatives incorrectly classiﬁed and N is the total number
of negative samples. A common metric for summarizing the
detection performance is the Receiver Operating Characteristic
(ROC) curve, which plots the FPR (on the x-axis) against the
TPR (on the y-axis). Computing the area under the ROC curve
(AUC) provides a single metric for summarizing detection
performance. An AUC value of 1.0 represents perfect detection
performance, while an AUC score of 0.5 means that the model
does no better than random.

Fig. 3 shows the cumulative performance for window sizes
of 1,000 and 10,000. The window size indicates how many
domains are used to seed the models, as well as how often
metrics are collected. The na¨ıve solution is simply the list of
advertising domains seen in the ﬁrst window and is analogous
to a standard blocklist that only utilizes domain names. For the
na¨ıve solution, the FPR did not change from 0.0 because our
ground truth doesn’t change. In other words, the same domains
that were marked ads in the beginning are still labeled ads as
we continue the experiment. We also see that for the na¨ıve
solution the TPR went down because new domains were seen
that were in the advertising category, but weren’t in the original
list of advertising domains used for detection.

For the Regex Golf solution, the FPR went up because the
learned regular expression marked domain names as advertise-
ments that were not. This increase is an unfortunate side effect

of the self-training paradigm in that errors tend to propagate.
On the other hand, the TPR remained steady or decreased
only slightly because the learned regular expression had some
generalizability and could correctly recognize some previously
unseen domains as advertisements.

For the smaller window size, the na¨ıve solution shows a
60% decrease in ability to detect positives, while the Regex
Golf solution only shows a 22% decrease over time with a
negligible difference in overall detection performance. For the
larger window size, the na¨ıve solution shows a 50% decrease in
ability to detect positives, while the Regex Golf solution only
shows a 33% decrease over time. The Regex Golf solution
has a 6% lower overall detection performance than the na¨ıve
solution. In summary,
the Regex Golf
solution is ﬁnding more positive instances with only a slight
degradation in performance, as indicated by the slightly lower
AUC scores.

is apparent

that

it

VII. CONCLUSIONS AND FUTURE WORK

In this work, we demonstrated the ability to keep IOCs (i.e.,
regular expressions) up-to-date with an evolving adversary.
This will allow network sensors to continue to bracket these
existing threats as they change their TTPs. Not losing the
ability to detect these adversaries will save countless hours
in incident response because they will be identiﬁed before
they have breached our networks or at
least before they
have spread laterally. It will also prevent these adversaries
from accomplishing their objectives, e.g., exﬁltrating desired
information.

The current solution for generating regular expressions
requires that the two datasets are disjoint. This constraint is not
realistic to impose in a production cyber security context. In
addition, the current model will always ﬁnd a perfect solution;
as the datasets grow larger, this may prevent the model from
ever converging. Relaxing this constraint could lead to much
faster build times, again advantageous in a production cyber
security context.

Currently, our system is only utilizing a single feature: the
domain name. By utilizing other features of HTTP trafﬁc, we
anticipate even better results. Given the performance increase
provided by the NPU, deploying a larger set of regular expres-
sions that match on different features is feasible, perhaps by
using a custom set multicover optimization. Additionally, our
framework allows for more complex models to be employed.
Any algorithm that can partition data or solve a learning
task can be supported by the framework. Therefore, trying
more advanced methods, such as supervised learning or deep
learning, is also an area of future work. Deep learning can also
be used in the Data Stream Processor to transform features into
more appropriate representations for learning.

Finally, we would like to deploy this in cyber security oper-
ations using the regular expressions discovered in a forensics
investigation as the ﬁrst set of deployed regular expressions.
We will then be able to quantify how well the system brackets
an adversary and their changing TTPs over time.

Fig. 3. AUC for a) window size 1,000 and b) window size 10,000. TPR/FPR for c) window size 1,000 and d) window size 10,000.

ACKNOWLEDGMENT
The authors acknowledge ﬁnancial support from Sandia Na-
tional Laboratories’ Laboratory Directed Research and Devel-
opment Program, and speciﬁcally the Hardware Acceleration
of Adaptive Neural Algorithms (HAANA) Grand Challenge
Project. Sandia National Laboratories is a multimission labora-
tory managed and operated by National Technology and Engi-
neering Solutions of Sandia, LLC., a wholly owned subsidiary
of Honeywell International, Inc., for the U.S. Department
of Energy’s National Nuclear Security Administration under
contract de-na0003525.

REFERENCES

[1] G. Argyros, I. Stais, A. Kiayias, and A. D. Keromytis, “Back in black:
towards formal, black box analysis of sanitizers and ﬁlters,” in Security
and Privacy (SP), 2016 IEEE Symposium on.
IEEE, 2016, pp. 91–109.
[2] A. Bartoli, A. De Lorenzo, E. Medvet, and F. Tarlao, “Playing regex
golf with genetic programming,” in Proceedings of the 2014 Annual
Conference on Genetic and Evolutionary Computation. ACM, 2014,
pp. 1063–1070.

[3] M. Becchi and P. Crowley, “Extending Finite Automata to Efﬁciently
Match Perl-compatible Regular Expressions,” in Proceedings of the 2008
ACM CoNEXT Conference. ACM, 2008, p. 25.

[4] P. Dlugosch, D. Brown, P. Glendenning, M. Leventhal, and H. Noyes,
“An efﬁcient and scalable semiconductor architecture for parallel au-
tomata processing,” IEEE Transactions on Parallel and Distributed
Systems, vol. 25, no. 12, pp. 3088–3098, 2014.

[5] C. T. Do, N. H. Tran, C. Hong, C. A. Kamhoua, K. A. Kwiat, E. Blasch,
S. Ren, N. Pissinou, and S. S. Iyengar, “Game Theory for Cyber Security
and Privacy,” ACM Computing Surveys (CSUR), vol. 50, no. 2, p. 30,
2017.

[6] D. Follett, D. Townsend, G. Karpman, J. Naegle, R. Suppona, J. Ai-
mone, and C. James, “Neuromorphic Data Microscope,” Neuromorphic
Computing Symposium, forthcoming.

[7] R. M. Karp, “Reducibility among combinatorial problems,” in Complex-

ity of computer computations. Springer, 1972, pp. 85–103.

[8] S. C. Kleene, “Representation of events in nerve nets and ﬁnite au-

tomata,” DTIC Document, Tech. Rep., 1951.

[9] P. Norvig. (2014) xkcd 1313: Regex Golf. Retrieved 2017-11-10.
[Online]. Available: http://nbviewer.jupyter.org/url/norvig.com/ipython/
xkcd1313.ipynb?create=1

[10] ——. (2014) xkcd 1313: Regex Golf (Part 2: Inﬁnite Problems).
Retrieved 2017-11-10. [Online]. Available: http://nbviewer.jupyter.org/
url/norvig.com/ipython/xkcd1313-part2.ipynb

[11] B. S. Olivier Chapelle and A. Zien, Eds., Semi-Supervised Learning.

MIT Press, 2006.

[12] P. Prasse, C. Sawade, N. Landwehr, and T. Scheffer, “Learning to
identify concise regular expressions that describe email campaigns,”
Journal of Machine Learning Research, vol. 16, pp. 3687–3720, 2015.
[13] Open Source Community. (2016) WaterSlide. Retrieved 2017-11-10.
[Online]. Available: https://github.com/waterslideLTS/waterslide

[14] Apache Software Foundation.

(2017) Apache HBase. Retrieved

2017-11-10. [Online]. Available: https://hbase.apache.org/

[15] ——.

(2017) Apache Phoenix. Retrieved 2017-11-10.

[Online].

Available: https://phoenix.apache.org/

[16] Explain xkcd.

(2016) 1313: Regex Golf. Retrieved 2017-11-10.
[Online]. Available: https://www.explainxkcd.com/wiki/index.php/1313:
Regex Golf

(2010)

[17] Google.
ular
line].
re2-principled-approach-to-regular.html

Expression Matching.

Reg-
[On-
http://google-opensource.blogspot.com/2010/03/

to
2017-11-10.

Principled

Available:

Approach

Retrieved

re2:

A

[18] Machine Learning Lab.

(2017) Machine Learning Lab. Retrieved

2017-11-10. [Online]. Available: http://machinelearning.inginf.units.it/

[19] Open Source Community. (2017) scikit-learn. Retrieved 2017-11-10.

[Online]. Available: http://scikit-learn.org/

[20] URLBlacklist.com.

(2017) URLBlacklist.com. Website no longer

available. [Online]. Available: http://www.urlblacklist.com/

