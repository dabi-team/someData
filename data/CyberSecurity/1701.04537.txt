Cloud Resource Allocation for
Cloud-Based Automotive Applications

Zhaojian Lia, Tianshu Chub, Ilya V. Kolmanovskya, Xiang Yind,âˆ—, Xunyuan Yinc

aDepartment of Aerospace Engineering, The University of Michigan, Ann Arbor, MI 48109, USA.
bDepartment of Civil and Environmental Engineering, Stanford University, CA 94305, USA.
cDepartment of Chemical and Materials Engineering, University of Alberta, Edmonton, AB T6G 1H9, Canada.
dDepartment of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA.

7
1
0
2

n
a
J

7
1

]

Y
S
.
s
c
[

1
v
7
3
5
4
0
.
1
0
7
1
:
v
i
X
r
a

Abstract

There is a rapidly growing interest in the use of cloud computing for automotive vehicles to facilitate computation and data intensive
tasks. Eï¬ƒcient utilization of on-demand cloud resources holds a signiï¬cant potential to improve future vehicle safety, comfort, and
fuel economy. In the meanwhile, issues like cyber security and resource allocation pose great challenges. In this paper, we treat the
resource allocation problem for cloud-based automotive systems. Both private and public cloud paradigms are considered where
a private cloud provides an internal, company-owned internet service dedicated to its own vehicles while a public cloud serves all
subscribed vehicles. This paper establishes comprehensive models of cloud resource provisioning for both private and public cloud-
based automotive systems. Complications such as stochastic communication delays and task deadlines are explicitly considered. In
particular, a centralized resource provisioning model is developed for private cloud and chance constrained optimization is exploited
to utilize the cloud resources for best Quality of Services. On the other hand, a decentralized auction-based model is developed for
public cloud and reinforcement learning is employed to obtain an optimal bidding policy for a â€œselï¬shâ€ agent. Numerical examples
are presented to illustrate the eï¬€ectiveness of the developed techniques.

Keywords: Resource Allocation, Vehicle-to-Cloud, Chance Constrained Optimization, Communication Delays, Deep
Deterministic Policy Gradient, Reinforcement Learning

1. Introduction

There is growing interest in employing cloud computing in
automotive applications [4, 8, 13, 16, 24, 29â€“31]. Ready access
to distributed information and computing resources can enable
computation and data intensive vehicular applications for im-
proved safety, drivability, fuel economy, and infotainment. Sev-
eral cloud-based automotive applications have been identiï¬ed.
For instance, a cloud-based driving speed optimizer is studied
in [21] to improve fuel economy for everyday driving. In [15],
a cloud-aided comfort-based route planner is prototyped to im-
prove driving comfort by considering both travel time and ride
comfort in route planning. A cloud-based semi-active suspen-
sion control is studied in [14] to enhance suspension perfor-
mance by utilizing road preview and powerful computation re-
sources on the cloud.

As such, cloud computing has been both an immense oppor-
tunity and a crucial challenge for vehicular applications: oppor-
tunity because of the great potential to improve safety, comfort,

âˆ—The material in this paper was not presented at any IFAC conference. Cor-

responding author.

Email addresses: zhaojli@umich.edu (Zhaojian Li),

cts1988@stanford.edu (Tianshu Chu), ilya@umich.edu
(Ilya V. Kolmanovsky), xiangyin@umich.edu (Xiang Yin),
xunyuan@ualberta.ca (Xunyuan Yin)

Preprint submitted to Mechatronics

and enjoyment; challenge because cyber-security and resource
allocation are critical issues that need to be carefully consid-
ered. A cloud resource allocation scheme determines how a
cloud server such as Amazon â€œEC2â€ or Google Cloud Platform
distributes resources to its many clients (vehicles in our context)
eï¬ƒciently, eï¬€ectively, and proï¬tably. This allocation design
becomes even more challenging when it comes to cloud-based
automotive systems in which issues like communication delays
and task deadlines arise. These complexities make a good re-
source allocation design a non-trivial, yet important task.

Not surprisingly, extensive studies have been dedicated to the
development of eï¬ƒcient and proï¬table cloud resource alloca-
tion schemes. A dynamic bin packing method, MinTotal, is
developed in [12] to minimize the total service cost. In [5], a
distributed and hierarchical component placement algorithm is
proposed for large-scale cloud systems. A series of game theo-
retical cloud resource allocation approaches have also been de-
veloped, see e.g., [2, 3, 10, 18]. However, as far as the authors
are aware, a resource allocation scheme for cloud-based au-
tomotive systems that accounts for communication delays and
task deadlines is still lacking.

In this paper, we develop resource allocation schemes for
cloud-based automotive systems that optimally tradeoï¬€ costs
and Quality of Service (QoS) with the presence of stochastic
In particular, we
communication delays and task deadlines.

January 18, 2017

 
 
 
 
 
 
consider allocation schemes under two cloud paradigms, pri-
vate and public cloud. A private cloud is a company-owned re-
source center which provides computation, storage and network
communication services and is only accessible by cars made by
the car company. The private cloud therefore has a high level
of security and information is easy and safe to share and man-
age. On the other hand, a public cloud relies on a third-party
service provider (e.g., Amazon EC2) that provides services to
all subscribed vehicles. A public cloud can eliminate the capi-
tal expenses for infrastructure acquisition and maintenance, and
can provide the service on an as-needed basis.

The objectives of resource allocation are quite diï¬€erent be-
tween private and public cloud paradigms. Since the private
cloud resources are pre-acquired, the company basically â€œuse
them or waste themâ€. Therefore, the goal of private cloud re-
source allocation is to best utilize its resources to provide good
QoS to its subscribed vehicles. Since the information exchange
between vehicles and the server is more secure and convenient,
the resource allocation can be achieved in a centralized manner.
On the other hand, public cloud provides services to subscribed
vehicles from a variety of makers, e.g., Ford, GM, Toyota, etc.
Due to security and privacy issues, these vehicles typically will
not share their information nor be interested in coordination;
hence each vehicle becomes a â€œselï¬shâ€ agent. The goal of each
agent is to minimize its service cost while maintaining good
QoS.

In this work, we develop mathematical models to formalize
the resource allocation problems for both private and public
cloud paradigms. Stochastic communication delays and on-
board task deadlines are explicitly considered. A centralized
resource-provisioning scheme is developed for private cloud
and chance constrained optimization is employed to obtain an
optimal allocation strategy. On the other hand, an auction-based
bidding framework is developed for public cloud and reinforce-
ment learning is exploited to train an optimal bidding policy
to minimize the cost while maintaining good QoS. Numerical
examples are presented to demonstrate the eï¬€ectiveness of the
proposed schemes.

The main contributions of this paper include the follow-
ing. Firstly, compared to the previous literature on cloud re-
source allocation, issues important to automotive vehicles such
as communication delays and onboard task deadlines are ex-
plicitly treated in this paper. Secondly, resource allocation
within a private cloud paradigm is formalized as a centralized
resource partitioning problem. Chance constrained optimiza-
tion techniques are employed to obtain the optimal partitioning
by solving a convex optimization problem. Thirdly, a decentral-
ized, auction-based bidding framework is developed for public
cloud-based resource allocation and the best response dynamics
assuming a constant time delay and bidding is derived. Further-
more, a Deep Deterministic Policy Gradient (DDPG) algorithm
is exploited to train the optimal bidding policy with stochastic
time delay and unknown bidding from other vehicles. Sensitiv-
ity analysis is also performed to show how the bidding policy
can change by varying task parameters such as workload and
deadline.

The rest of our paper is organized as follows. Section 2 de-

scribes the model of cloud resource provisioning for private
cloud-based automotive systems. The problem formulation and
a chance constrained optimization approach are also presented.
In Section 3, a numerical example is given to illustrate the allo-
cation scheme for private cloud. The resource allocation prob-
lem with a public cloud is formalized in Section 4. The best
response dynamics with constant time delay and bidding is also
derived. A DDPG algorithm is exploited in Section 5 to train
the optimal bidding policy with stochastic time delay and un-
known bidding from other vehicles. A numerical case study
is also presented with sensitivity analysis on task parameters.
Finally, conclusions are drawn in Section 6.

2. Centralized Resource Allocation with a Private Cloud

It is more secure and manageable for automotive manufactur-
ers to acquire and maintain its own private cloud infrastructure
which provides computation, data storage and network services
only to vehicles made by the manufacturer. A schematic di-
agram of resource allocation for private cloud-based automo-
tive systems is illustrated in Figure 1. Suppose that a set of
cloud-based vehicular applications are available (e.g., cloud-
based route planning, cloud-based suspension control, etc.) and
we consider a general case that each vehicle runs a subset of
these applications. Let us consider a total number of N appli-
cations running on M vehicles as in Figure 1. Each application
i, i = 1, 2, . . . , N, corresponds to a periodic task associated with
a tuple, Ti = {Ti, wi, di, Ï„i}, where

â€¢ Ti is the period of task i in seconds;

â€¢ wi is the workload of task i in million instructions;

â€¢ di â‰¤ Ti is the deadline of task i in seconds;

â€¢ Ï„i is a random time delay of the communication channel

associated with task i in seconds.

For each task i, the Quality of Service (QoS) is characterized

by the following cost function adopted from [32]:

Ci(Î³i; Ï„i) =

+ Ï„i),

ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³

Bi( wi
Î³i
Mi,

+ Ï„i â‰¤ di

if wi
Î³i
Otherwise,

(1)

where Î³i is the process rate that the cloud resource allocator as-
signs to task i and (cid:80)N
i=1 Î³i = Î³ with Î³ being the total resource
available on the cloud; Bi(Â·) : R+ â†’ R+ is a non-decreasing
function reï¬‚ecting the QoS of task i; Mi â‰¥ Bi(di) is a pos-
itive scalar representing the penalty for missing the deadline;
the condition wi
+ Ï„i > di indicates that the deadline has been
Î³i
missed. Note that task priorities are reï¬‚ected in the deadline-
missing penalty Mi. For safety-critical tasks (e.g., cloud-based
functions involved in powertrain or vehicle control), a large
penalty, M, should be given while a small M can be assigned to
some non-critical tasks such as online video streaming.

Since a private cloud is a pre-acquired â€œuse it or waste itâ€
capability, the goal of resource allocation for private cloud-
based automotive systems is to distribute the cloud resources

2

of missing a deadline can be characterized by a small Î± while
larger Î± can be used for applications with mild consequences of
missing a deadline.

Note that the deadline-missing penalty M and the chance
constraint Î± are transformable. For instance, one can use the
following function to map deadline-missing penalties to chance
constraints:

Î±i = Î±max + Î±min âˆ’ Î±max
Mmax âˆ’ Mmin

(Mi âˆ’ Mmin),

(4)

where Î±min and Î±max are, respectively, the lower and upper
bounds of the chance constraints while Mmin and Mmax are
the corresponding lower and upper bounds of the deadline-
violation penalties, respectively. These parameters need to be
chosen compatibly to reï¬‚ect the same QoS requirements. The
example transformation in (4) is illustrated in Figure 2.

Figure 2: Linear mapping from deadline-missing penalty to
chance constraint.

Figure 1: Schematic diagram of private cloud-based resource
allocation.

to the N tasks such that the total expected QoS cost as in (1) is
minimized. Basically, the cloud collects task information (i.e.,
workload, deadline, time delay statistics1) of the N tasks and
determines how optimally to partition the total resources into N
parts so that the expected QoS cost is minimized. The problem
can be mathematically formalized as a constrained optimization
problem

J(Î“) = E(cid:2)

min
Î“

Subject to:

N(cid:88)

Ci(Î³i; Ï„i)(cid:3)

i=1
N(cid:88)

Î³i = Î³,

i=1
Î³i â‰¥ 0, âˆ€i = 1, Â· Â· Â· , N

(2)

where Î“ = [Î³1, Î³2, Â· Â· Â· , Î³N]T is the vector of process rates to be
optimized.

We note that the problem (2) is challenging to solve due to
the randomness of communication delay Ï„i and the discontinu-
ity of the cost function represented in (1). Motivated by the
chance constrained formulation in Stochastic Model Predictive
Control developments [7, 20, 22], we re-formulate problem (2)
by imposing chance constraints. Instead of penalty for missing
deadline as in (1), we impose chance constraints for missing
deadlines of the form,

Now let us assume that the communication delays can be
modeled as independent Gaussian random variables, i.e., Ï„i âˆ¼
N( Â¯Ï„i, Ïƒ2
i ). From basic probability theory, the probability of the
delay taking values between a and b,

Pr(a < Ï„i â‰¤ b) = 1
2

erf(

b âˆ’ Â¯Ï„iâˆš
2Ïƒi

) âˆ’

1
2

erf(

a âˆ’ Â¯Ï„iâˆš
2Ïƒi

),

(5)

where erf(x) = 2âˆš
Ï€

(cid:82) x
0

eâˆ’t2 dt is the error function.

As a result, from (3) and (5), it follows that

Pr(

wi
Î³i

+ Ï„i â‰¤ di) â‰¥ 1 âˆ’ Î±i,

(3)

Pr(Ï„i â‰¤ di âˆ’

wi
Î³i

) = 1
2

erf(

âˆ’ Â¯Ï„i

di âˆ’ wi
Î³i
âˆš
2Î³i

) + 1
2

â‰¥ 1 âˆ’ Î±i.

(6)

where Î±i âˆˆ (0, 1) is a scalar representing the chance constraint
of missing a deadline, i = 1, Â· Â· Â· , N. The notion of Î± can be in-
terpreted as the upper limit of deadline missing rate speciï¬ed in
the QoS requirements. Applications with harsh consequences

1Note that the task period Ti is not used here but we include it as one of
the four task attributes for completeness. The task period will appear when it
comes to the public cloud-based resource allocation in Section 4.

3

We next apply the inverse error function erfâˆ’1(Â·) to both sides

of (6). Since erfâˆ’1(Â·) is continuous and increasing, we have
âˆš

di âˆ’

âˆ’ Â¯Ï„i â‰¥

2Ïƒierfâˆ’1(1 âˆ’ 2Î±i).

(7)

wi
Î³i

Note that (7) requires the term di âˆ’ Â¯Ï„i âˆ’

2Ïƒierfâˆ’1(1 âˆ’ 2Î±i)
to be positive so that Î³i is feasible. This condition means that

âˆš

ğœ¸ğŸ  ğœ¸ğŸ  ğ‰ğŸ  ğ‰ğŸ  ğ‰ğ’Š  ğ‰ğ’Š+ğŸ  ğ‰ğ‘µâˆ’ğŸ  ğ‰ğ‘µ  ğœ¸ğ’Š  ğœ¸ğ’Š+ğŸ  ğœ¸ğ‘µâˆ’ğŸ  ğœ¸ğ‘µ  ğœ¸ ğ‘‡1 ğ‘‡2 ğ‘‡ğ‘– ğ‘‡ğ‘–+1 ğ‘‡ğ‘âˆ’1 ğ‘‡ğ‘ Cloud  Resources Resource Allocator ğ›¼ğ‘šğ‘ğ‘¥ ğ›¼ğ‘šğ‘–ğ‘› ğ‘€ğ‘šğ‘–ğ‘› ğ‘€ğ‘šğ‘ğ‘¥ ğ‘€ ğ›¼ ğ‘€ğ‘– ğ›¼ğ‘– (10)

4. Decentralized resource allocation for public cloud

the mean of the delay Â¯Ï„i cannot be greater than the deadline di.
Also, given deadline di, delay mean Â¯Ï„i and standard deviation
Ïƒi, the minimum achievable chance constraint level, Î±âˆ— is

Î±âˆ— = 1
2

âˆ’

er f (di âˆ’ Â¯Ï„i)
âˆš

2

2Ïƒi

,

(8)

which deï¬nes a maximum performance bound regardless of al-
located resources. For example, if di = 0.3, Â¯Ï„i = 0.2, and
Ïƒi = 0.1, then from (8) we have Î±âˆ— = 0.3976, which means that
no matter how many resources are allocated for task i, the prob-
ability of missing a deadline is no less than Î±âˆ— = 0.3976 due to
communication delays. On the other hand, Î±âˆ— < 0 means that
the probability of not missing deadline can be inï¬nitely close to
1 with enough resources.

Re-arranging terms in (7) leads to
wi

Î³i â‰¥ Ïi =

âˆš

di âˆ’ Â¯Ï„i âˆ’

2Ïƒierfâˆ’1(1 âˆ’ 2Î±i)

.

(9)

So far we showed that (9) and (3) are equivalent. Therefore,

the problem in (2) can be re-stated as:

J(Î“) = E(cid:2)

min
Î“

N(cid:88)

i=1

Bi(

wi
Î³i

+ Ï„i)(cid:3)

subject to:

N(cid:88)

Î³i = Î³,

i=1
Î³i â‰¥ Ïi > 0, âˆ€i = 1, 2, . . . , N,

where Ïi are assumed to be positive and deï¬ned by (9).

Note that if we choose B(Â·) to be a convex function of Î³i,
as we will show in the next section, problem (10) reduces to
a convex optimization problem which can be eï¬ƒciently solved
with good scalability.

3. A numerical example with a linear QoS function

In this section, we consider a linear QoS function in the form
+ Ï„i) with bi > 0. The problem (10)

+ Ï„i) = bi Â· ( wi
Î³i

of Bi( wi
Î³i
becomes

N(cid:88)

i=1

bi Â· (

wi
Î³i

+ Â¯Ï„i)

min
Î“

J(Î“) =

subject to:

N(cid:88)

Î³i = Î³,

i=1
Î³i â‰¥ Ïi > 0, âˆ€i = 1, 2, . . . , N.

(11)

We show that the above problem is a convex optimization
problem. We ï¬rst demonstrate that the cost function J(Î“) in
(11) is strictly convex in the domain {Î“ = [Î³1, Â· Â· Â· , Î³N]T : Î³i >
0, âˆ€i = 1, 2, Â· Â· Â· N}. Towards that end, we compute the Hessian
of the cost function J as

Hxx(J(Î“)) = diag{

2b1w1
Î³3
1

, Â· Â· Â· ,

2bNwN
Î³3
N

},

(12)

4

where Hxx(Â·) represents the Hessian matrix and diag{Â·} denotes
the diagonal matrix with the arguments as the entries in the di-
agonal. Since bi, wi, Î³i are positive for i = 1, 2, Â· Â· Â· , N, we have
Hxx(J(Î“)) being positive deï¬nite, which means that the cost
function J(Î“) is strictly convex. Furthermore, the constraints
in (11) are polytopic. Therefore, (11) is a convex optimiza-
tion problem that can be eï¬ƒciently solved by many numerical
solvers. This means even if N is large, an optimal resource al-
location can be eï¬ƒciently computed.

We next give a numerical example with four tasks. The pa-
rameters are given in Table 1 and we consider a total resource of
1, i.e., Î³ = 1. The fmincon function in MATLAB was exploited
to solve (11) and the optimized allocation strategy is,

Î³2 = 0.1495,

Î³3 = 0.3585,

Î³1 = 0.1608,

Î³4 = 0.3312.
(13)
To verify the chance constraints with the optimized allocation
scheme, we run simulations under the allocation policy (13) for
106 times with the random delays speciï¬ed in Table 1. The
missing deadline violation rates for the four applications are,
respectively, 0.0961, 0.0482, 0.01991, and 1.3 âˆ— 10âˆ’5, which
are all smaller than the speciï¬ed chance constraints in Table 1.
This means that the speciï¬ed chance constraints are satisï¬ed
under the allocation scheme (13).

paradigm

4.1. Problem formulation

An automotive manufacturer may choose to subscribe its
cloud-based automotive applications to a public cloud with-
out acquiring and maintaining its own infrastructure. A pub-
lic cloud such as Amazon EC2 oï¬€ers an on-demand and pay-
as-you-go access over a shared pool of computation resources.
This public cloud provides services to a large number of auto-
mobiles from a variety of manufacturers. These vehicles may
not want to share either their resource policies or task infor-
mation with other vehicles, which makes it impossible to run a
centralized allocation scheme as in the private cloud paradigm.
Instead, each vehicle becomes a â€œselï¬shâ€ client that seeks to
minimize its own cost while maintaining good QoS.

We consider a decentralized auction-based resource alloca-
tion model as illustrated in Figure 3. For the considered ve-
hicle, let N denote the number of tasks that are running in
the vehicle and each task is associated with the same tuple
Ti = {Ti, wi, di, Ï„i} as deï¬ned in Section 2. The public cloud
is running an auction-based resource allocation scheme, that is,
i = 1, Â· Â· Â· , N, submits a bid pi (in US
each vehicular task i,
dollars per second) and obtains a proportion of the total cloud
resources as:

Î³i = pi
P

Â· Î³ =

pi
Pâˆ’ + (cid:80)N

i=1 pi

Â· Î³,

(14)

where P is the sum of all bids the cloud receives from all ve-
hicles; Pâˆ’ = P âˆ’ (cid:80)N
i=1 pi is the cumulative bid from all other
vehicles; and Î³ quantiï¬es the total resources available on the
cloud.

Table 1: Parameters for numerical example

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Task

Attribute
Workload (wi, in Million instructions)
Deadline (di, in seconds)
QoS cost scalar (bi, in $/s)
Delay mean (Â¯Ï„i, in seconds)
Delay standard deviation (Ïƒi, in second)
Chance limit (Î±âˆ—
i from (8))
Chance constraint (Î±i, unitless)

One

Two

Three

Four

0.02
0.25
1
0.1
0.02
-2.47
0.1

0.03
0.35
2
0.1
0.03
-2.76
0.05

0.1
0.4
2
0.08
0.02
-5.67
0.02

0.12
0.6
3
0.11
0.03
-5.53
0.01

Figure 3: Schematic diagram of public cloud resource alloca-
tion

Since there are many other vehicles subscribed to the public
i=1 pi. From (14)

cloud, it is reasonable to assume that Pâˆ’ >> (cid:80)N
it follows that

Î³i â‰ˆ

pi
Pâˆ’ Î³,

(15)

which implies that the bidding policy of the tasks can be con-
sidered independently.

We consider a general bidding model in which the time pe-
riod between the beginning of a period and the deadline is com-
posed of multiple bidding steps. As illustrated in Figure 4, there
are l, l â‰¥ 1, bidding steps before the deadline in each task pe-
riod. With the QoS cost modeled in (1), the overall cost of task
i in its period Ti is,

Ji =

l(cid:88)

t=1

pi,t Â· ts + Ci(Î³t; Ï„i),

(16)

where pi,t is the bidding for task i at bidding step t; ts = d
l is the
bidding time interval; and Ci(Î³t; Ï„i) is the QoS cost deï¬ned in
(1).

The goal of the vehicle is to ï¬nd an optimal bidding policy
to minimize the accumulated cost (16) for each task. We next
derive the optimal bidding strategy with a preliminary assump-

Figure 4: A general bidding model with l bidding steps in one
task period.

tion that Pâˆ’ and Ï„ are known and constant. In Section 5, this
assumption is removed.

4.2. Best response dynamics with constant Pâˆ’ and Ï„

In this subsection, we seek to ï¬nd the optimal bidding if bids
from other vehicles (Pâˆ’) and the communication delays (Ï„) are
known and constant. Since Pâˆ’ is constant, all it matters is the
total bidding. For task i, the optimal average bidding in the
interval [0, di âˆ’ Ï„i] is deï¬ned as
pâˆ—
i

Ji(pi)

pi Â· (di âˆ’ Ï„i) + Ci(pi; Ï„),

(17)

= argmin
pi
(cid:44) argmin
pi

where Ci(pi; Ï„) is deï¬ned in (1) and from (15) it follows that

Ci(pi; Ï„i) =

ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³

Bi( wiPâˆ’
piÎ³
Mi,

+ Ï„i),

if pi â‰¥ wiPâˆ’
(diâˆ’Ï„i)Î³
Otherwise.

As a result, the overall cost function J(pi) becomes
if pi â‰¥ wiPâˆ’
(diâˆ’Ï„i)Î³
Otherwise.

pi Â· (di âˆ’ Ï„i) + Bi( wiPâˆ’
piÎ³
pi Â· (di âˆ’ Ï„i) + Mi,

Ji(pi) =

+ Ï„i),

ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³

(18)

(19)

Consider the linear QoS function Bi(x) = bi Â· x with bi > 0
as in Section 3. Then there are two local minimizers in (19):
= 0), the other corresponds
one associated with no bidding (pâˆ—
i
to the optimal bidding with no deadline missing. The second
minimizer can be represented as

(cid:115)

(cid:110)

pâˆ—
i

= max

biwiPâˆ’
(di âˆ’ Ï„i)Î³

,

wiPâˆ’
(di âˆ’ Ï„i)Î³

(cid:111)
,

(20)

5

CloudResourcesğ›¾ğ’‘ğŸğ’‘ğŸğ’‘ğ‘µğ‘·âˆ’ğœ¸ğŸğœ¸ğŸğœ¸ğ‘µğœ¸âˆ’Auction Agent1 2 3 ğ‘™ ğ‘» ğ‘» ğ‘» ğ’… ğ’… ğ’… Bidding Horizon (cid:113)

+ Ï„i), i.e.,

depending whether the minimizer of the function pi Â· (di âˆ’ Ï„i) +
Bi( wiPâˆ’
biwiPâˆ’
Î³Â·(diâˆ’Ï„i) , can avoid deadline missing. An
piÎ³
example of the cost function (19) with wi = 0.06, d = 0.4, Ï„ =
0.1, bi = 8, Pâˆ’ = 20, Mi = 5, ts = 0.05, and Î³ = 10 is illustrated
in Figure 5. The global minimizer is

= 1.7889.

(cid:113)

biwiPâˆ’
Î³Â·ts

Figure 5: Overall cost as a function of total bidding with wi =
0.06, d = 0.4, Ï„ = 0.1, bi = 8, Pâˆ’ = 20, Mi = 5, ts = 0.05, and
Î³ = 10.

The assumption that the Pâˆ’ and Ï„ are constant and known is
unrealistic in many applications. We next exploit a reinforce-
ment learning framework to obtain the optimal bidding policy
with no assumptions on the prior knowledge of Pâˆ’ and Ï„.

5. Training optimal bidding policy using RL

5.1. Introduction to RL

Reinforcement learning (RL) is a data-driven approach for
adaptively evolving optimal control policies based on the real-
time measurements. Unlike traditional methods, RL models the
stochastic ambiguity within the framework of Markov decision
processes (MDP) [6], and learns the policy according to tran-
sition data observations [25]. There are commonly three types
of RL algorithms: Q-learning, policy gradient, and actor-critic.
The Q-learning (or approximate Q-learning) is the traditional
RL algorithm that learns a Q-function QÎ¸(s, a) with model pa-
rameter Î¸ to estimate the delayed total reward of the current
state and action a, and performs control as Ë†a âˆˆ argmaxa QÎ¸(s, a)
[27] based on the learned policy. The Q-learning updates the
Q-function parameters based on each observed temporal diï¬€er-
ence using stochastic gradient descent:

Î¸ â† Î¸ + Î·âˆ†tÏ†(st, at),

(21)

where Î· is the learning rate, Ï†(st, at) is the input feature vector
for the learning model, and âˆ†t is the sampled temporal diï¬€er-
ence with stage reward rt and discount factor Î±:

âˆ†t = rt + Î± max

a

QÎ¸(st+1, a) âˆ’ QÎ¸(st, at).

(22)

6

In the policy gradient approach, the stochastic optimal action
distribution Ï€Î¸(a|s) with model parameter Î¸ is learned directly,
and control action is determined as Ë†a âˆˆ argmaxa Ï€Î¸(a|s) [26].
Policy gradient updates the policy distribution based on each
observed advantage function:

Î¸ â† Î¸ + Î·âˆ‡Î¸ log Ï€Î¸(at|st)( Ë†QÏ€(st, at) âˆ’ Ë†V(st)),

(23)

where Ë†QÏ€(st, at) is the sampled Q-value of (st, at) by following
policy Ï€Î¸ and Ë†V(st) is the sampled optimal value of st.

The actor-critic approach can be regarded as a combination
of both since it learns both the policy Ï€Î¸(a|s) and the corre-
sponding Q-function of the policy QÏ€
Î²(s, a) [11]. The details of
the actor-critic updates will be covered in the following subsec-
tion.

All the above algorithms typically assume the discrete action
space which, in particular, simpliï¬es the search for Ë†a. However,
in our resource allocation problem, it is more natural to consider
a continuous action space since the bid should be a continuous
numerical number. For this case, deterministic policy gradient
algorithm was developed recently that allows to directly learn
the policy Âµ(s) instead of the policy distribution Ï€(a|s), and the
control is simply performed as Ë†a = Âµ(s) [23]. Then instead of
the traditional (cid:15)âˆ’greedy exploration or Boltzmann exploration
for Q-learning, we need to perform Ornstein-Uhlenbeck noise
[28] to explore with the deterministic continuous policy.

In the following sections, we ï¬rst formulate the bidding-
based resource allocation problem. We then propose the corre-
sponding MDP formulation for this stochastic optimal control
problem. Further, we implement a deep network based actor-
critic algorithm to learn the optimal bidding strategy using de-
terministic policy gradient. Finally, we evaluate the perfor-
mance of this algorithm using various numerical experiments.

5.2. Training Optimal Bidding policy with deep deterministic

policy gradient

In this section, we exploit RL to seek the optimal bidding
policy. Towards that end, we ï¬rst model the bidding process as
a Markov Decision Process (MDP), M = {S, A, P, r, Î±}, where

â€¢ S = {wt, âˆ†wt, atâˆ’1, dt} represents the state space, where
wt is the remaining workload at time t; âˆ†wt = wtâˆ’1 âˆ’ wt
is the recent processed work; atâˆ’1 represents the last bid,
and dt represents the remaining time until deadline. At
the beginning of each period, the initial state is simply
s0 = [w, 0, 0, d];

â€¢ A âˆˆ [0, +âˆ) is the action space representing the bidding

for the task;

â€¢ P is the transition matrix where each element Pa

ss(cid:48) =
P[S t+1 = s(cid:48)|S t = s, At = a] is the probability that the
system transfers to s(cid:48) from s given the current action a.

â€¢ r is a stage reward function given after each bidding to
guide the optimal decision-making: r(s, a) = E[rt|S t =
s, At = a].

0123451.522.533.544.555.5Total bidding pi ($)Overall cost Ji(pi) ($)â€¢ Î± âˆˆ [0, 1) is a discount factor that diï¬€erentiates the impor-

tance of future rewards and present rewards.

Finally, we update the parameter of the actor policy function
based on the Q-function estimation

Since the bidding of other vehicles is unknown so the transi-
tion matrix P, traditional MDP optimizers such as Policy itera-
tion and Value iteration cannot be directly applied. In this study,
we exploit RL to learn an optimal bidding policy for each appli-
cation. Furthermore, since the action space A = [0, +âˆ) is con-
tinuous, approximate Q-learning and stochastic policy gradient
algorithms cannot be applied without action discretization. To
resolve this diï¬ƒculty, we exploit the deterministic actor-critic
(DAC) algorithm.
In particular, we learn both a parameter-
: S â†’ A to perform
ized deterministic policy function ÂµÎ¸
the bidding action, and another parameterized critic Q-function
QÎ² : S Ã— A â†’ R to evaluate the bidding strategy. The bidding
and learning procedure with a typical DAC algorithm is:

1. At each time step t â‰¤ l, observe the state st.

2. Perform a bid Ë†at based on the actor policy plus
Ë†at =

some random exploration perturbations,
ÂµÎ¸(st)+perturbations.

i.e.,

3. Receive the cloud resource allocation Î³t, and update the

states as

âˆ†wt+1 = Î³t Â· ts, wt+1 = wt âˆ’ âˆ†wt+1,

dt+1 = dt âˆ’ ts. (24)

4. Terminate whenever the procedure is completed: wt+1 â‰¤

0, dt+1 â‰¥ 0, or aborted: wt+1 > 0, dt+1 = 0.

5. Receive the current reward. If the procedure is aborted, re-
ceive a deadline-missing penalty rt = âˆ’M; if the procedure
is completed, a cost rt = âˆ’b Â· t with b be a positive scalar
representing the QoS cost coeï¬ƒcient is received; other-
wise the agent receives the following state stage cost

rt = âˆ’Ë†at Â· ts.

(25)

In Step 2, instead of performing an (cid:15)âˆ’greedy exploration
over the entire action space, we add some Ornstein-Uhlenbeck
noises into Ë†at to explore actions in the vicinity. A replay buï¬€er
is employed to store recent transitions (st, at, st+1, rt) so that ran-
dom transitions can be sampled to train the parameterized mod-
els to reduce the eï¬€ects of data correlation [19]. When the re-
play buï¬€er is ï¬lled, we can update both Î² and Î¸ in the models
by exploiting W randomly selected transitions from the buï¬€er.
The update of Î² is similar to the one in Q-learning: First we
estimate the temporal diï¬€erence from each selected transition:

âˆ†t = rt + Î±QÎ²(st+1, ÂµÎ¸(st+1)) âˆ’ QÎ²(st, at),

(26)

where Î± âˆˆ [0, 1) is the discount factor. We then update the
parameter in the critic Q-function using stochastic gradient de-
scent, i.e.,

Î² â† Î² +

Î·Î²
W

W(cid:88)

t=1

âˆ†tâˆ‡Î²QÎ²(st, at).

(27)

7

Î¸ â† Î¸ + Î·Î¸
W

W(cid:88)

t=1

âˆ‡Î¸ÂµÎ¸(st)âˆ‡aQÎ²(st, a)|a=ÂµÎ¸(s).

(28)

Here Î·Î² and Î·Î¸ are positive constants representing the learn-
ing rates. In this study, we apply deep neural networks as the
approximation functions for both the actor and critic. This spe-
ciï¬c implementation of the deterministic actor-critic (DAC) al-
gorithm is referred to as the deep deterministic policy gradi-
ent (DDPG) [17]. Furthermore, techniques such as experience
replay [19] and batch normalization [9] are also employed to
improve the learning performance.

The complete DDPG algorithm is shown in Algorithm 6. The
algorithm parameters include: the discount factor Î±, learning
rates Î·Î², and Î·Î¸ in (26), (27), and (28), respectively; bidding
horizon l as in Figure 4; replay buï¬€er size K; mini-batch size
W, W < K; task workload w; task deadline d; total cloud re-
source Î³, and parameter smoothing scalar Î´ âˆˆ (0, 1). Speciï¬-
cally, Line 1 initializes the network parameters and the target
network parameters for smooth updating. Line 2 initializes an
experience replay D that stores the K most recent transition
samples. At the beginning of each training episode (periodic
task), we reset the state and sample time delay. At each time
step, Line 6 performs exploration with some sampled Ornstein-
Uhlenbeck noise (cid:15)OU; Line 7 samples Pâˆ’ from the environment;
Lines 8-9 observe the system transition and add the current tran-
sition sample to the replay buï¬€er; Lines 10-12 update the net-
works based on the sampled minibatch from the experience re-
play; Line 13 updates the corresponding target networks with a
weighted sum to smooth the training.

5.3. Numerical experiments

5.3.1. Simulation setup

In this subsection, we perform simulations to illustrate the
DDPG approach in Algorithm 1. Four tasks are considered
in the host vehicle. The task speciï¬cations are listed in Ta-
ble 2. The bidding policy of each application is trained sep-
arately. We deï¬ne the total cloud resource as Î³ = 1 mil-
lion instructions/second. The time delays of all applications
are assumed to be the same and are Ï„ âˆ¼ |N(0.1, 0.0025)| in
seconds. The bidding period ts is set to 0.05 seconds so 20d
gives bidding horizon l in steps and 20Ï„ gives the delay in
steps. We sample Pâˆ’ from an Ornstein-Uhlenbeck process with
Âµ = 33, Î¸ = 1, Ïƒ = 1.5 $/second to reï¬‚ect similar prices from
Amazon EC2 [1]. Three sampled trajectories of the Ornstein-
Uhlenbeck process are illustrated in Figure 7. We set b = 2 in
the cost functions for all tasks.

For DDPG training, we train 5000 task periods with Î± =
0.99, Î´ = 0.001, K = 50000, M = 32. The actor network
we use has two hidden layers with sizes 20 and 15, and the
learning rate Î·Î¸ = 0.00001. We build the critic network using
the same structure with learning rate is Î·Î² = 0.0001. We also
bound the bidding at each time step as 1.5 $/0.05 second to

Table 2: Parameters of vehicular applications for simulation.

(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)

Application

Attribute

One

Two Three

Four

Workload (w, in million instructions)
Deadline (d, in seconds)
Penalty for missing deadline (M, in $)

0.02
0.5
2

0.06
0.4
2

0.1
0.4
10

0.12
0.6
10

open-source package DDPG2.

5.3.2. Training results

We train the actor and critic networks with the simulation
setup as described above. The training history of the bidding
policy for the four applications is shown in Figure 8. The ï¬gure
shows the total rewards (line is the average value and shade is
the standard deviation) over 20 testing episodes from every 100
training episodes. As we can observe, the best bidding policy
for application 2 is not bidding since the cost of bidding so that
the deadline is not missing is more than the deadline missing
penalty.

However, the bidding policies for tasks 1, 3, and 4 do not
converge. The reason is that as we show in Section 4.2, there
are two local minima in the cost function: no bidding, and min-
imum bidding for completing the job before deadline. Note that
the second minimizer is unstable since a further small reduction
on bidding may result in the convergence to the ï¬st minimizer.
So instead of using the DDPG model after the entire training,
the ï¬nal choice of our model is the best model during the train-
ing procedure based on the testing results as in Figure 8.

Figure 6: Algorithm 1

Figure 8: Total rewards vs. training episodes. Left: application
1 and 2, right: application 3 and 4.

Next, in order to validate the optimality of our obtained pol-
icy, we investigate the trained policies with ï¬xed Pâˆ’ = 33
$/second and Ï„ = 0.1 second so that we can compare with the
analytical form of best bidding in Section 4.2. The results and
the comparison are listed in Table 3. We can see that DDPG
almost captures either of the two local minima, depending on
the amount of penalty. We also estimate the equivalent bidding
rate as

Ë†p =

(cid:80)l

t=1 at
d âˆ’ Ï„

,

(29)

so we can compare it with the optimal bidding rate given in
(20). We can see there is a small diï¬€erence between Ë†p and pâˆ—,

Figure 7: Three sampled Pâˆ’ bidding trajectories from Ornstein-
Uhlenbeck process.

scale the output of DDPG. Our implementation is based on an

2Package site: https://github.com/songrotek/DDPG.

8

05101520Time steps293031323334353637Totoal bids010002000300040005000Training episodesâˆ’4.0âˆ’3.5âˆ’3.0âˆ’2.5âˆ’2.0âˆ’1.5âˆ’1.0âˆ’0.50.0Total rewardsapp1app2010002000300040005000Training episodesâˆ’14âˆ’12âˆ’10âˆ’8âˆ’6âˆ’4âˆ’20Total rewardsapp3app4which may be due to the fact that we discretize the time hori-
zon and round up the completion time to steps of 0.05 second.
Figure 9 illustrates the detailed bidding policy at each time step
for each application, where the y axis shows the actual bid per
step instead of the bidding rate for easier comparison. We can
see that DDPG tends to complete the process with fewer time
steps but to split the bids equally among these steps.

Figure 9: Trained bidding vs. time steps. Left: application 1
and 2, right: application 3 and 4.

5.3.3. Sensitivity analysis

In this subsection, we perform sensitivity analysis of diï¬€er-
ent parameters, i.e., investigate how the bidding policy changes
over task parameter variations. We choose application 2 to ana-
lyze how workload, deadline, and deadline-missing penalty can
inï¬‚uence the obtained bidding policy. We ï¬rst ï¬x w = 0.06
million instruction, d = 0.4 second, and let M = 2, 2.5, 3, 3.5
$, respectively. The results are shown in Figure 10. We can
see as the penalty increases, the bidding strategy switches from
zero-bidding to minimum bidding for job completion. How-
ever, when M = 2.5$, the total rewards of these two bidding
strategies are very close, so the learned policy is slightly worse
than the optimal policy with total bid 2.06$ and assigned work-
load 0.06 million instructions. When M increases further, the
learned policy becomes stable and optimal.

Figure 10: The impact of penalty on trained policy (in $). Left:
rewards during training procedure, right: total bid vs. penalty.

Next, we ï¬x d = 0.4 second, M = 3.5 $, and change
w = 0.06, 0.08, 0.1, 0.12 million instruction, respectively. The
results are shown in Figure 11. We can see as the workload in-
creases, the total bid also increases accordingly. When the total
bid and QoS cost becomes larger than the penalty, the bidding
strategy switches to zero-bidding.

Finally, we ï¬x w = 0.06 million instruction, M = 3.5 $,
and change d = 0.15, 0.2, 0.3, 0.4 second. Similar results can
be observed in Figure 12: when the deadline is long enough,

9

Figure 11: The impact of workload on trained policy (in million
instructions). Left: rewards during training procedure, right:
total bid vs. workload.

DDPG always bids, when the deadline is so short (d âˆ’ Ï„ may
be a single step) that the application can not be completed even
with the maximum bid bound, DDPG switches to zero-bidding.

Figure 12: The impact of deadline on trained policy (in sec-
onds). Left: rewards during training procedure, right: total bid
vs. deadline.

6. Conclusions

In this paper, we studied the problem of resource allocation
for cloud-based automotive systems. Resource provisioning
under both private and public cloud paradigms were modeled
and treated. Task deadlines and random communication delays
were explicitly considered in these models. In particular, a cen-
tralized resource provisioning scheme was used to model the
dynamics of private cloud provisioning and chance-constrained
optimization was exploited to utilize the cloud resource to min-
imize the Quality of Service (QoS) cost while satisfying speci-
ï¬ed chance constraints. A decentralized auction-based bidding
scheme was developed to model the public cloud resource pro-
visioning. Best dynamics with constant bidding and constant
time delays were ï¬rst derived and a deep deterministic policy
gradient was exploited to obtain the best bidding policy with
random time delays and no prior knowledge on the random bid-
ding from other vehicles. Numerical examples were presented
to demonstrate the developed framework. We showed how the
optimal bidding policy changes with parameters such as task
workload and deadline.

References

[1] Amazon

EC2.

https://aws.amazon.com/ec2/pricing/

on-demand/. Accessed: 2016-12-24.

0.00.51.01.52.0Time stepsâˆ’0.50.00.51.01.5Bidsapp1app20123456Time stepsâˆ’0.50.00.51.01.5Bidsapp3app4010002000300040005000Training episodesâˆ’4.0âˆ’3.5âˆ’3.0âˆ’2.5âˆ’2.0âˆ’1.5Total rewards22.533.52.02.53.03.53enalties0.00.51.01.52.02.5Total bids0100020003000400050007raining episodesâˆ’7âˆ’6âˆ’5âˆ’4âˆ’3âˆ’27otal rewards0.060.080.100.120.050.060.070.080.090.100.110.12Workloadsâˆ’0.50.00.51.01.52.02.53.07oWal bids010002000300040005000Training episodesâˆ’6.0âˆ’5.5âˆ’5.0âˆ’4.5âˆ’4.0âˆ’3.5âˆ’3.0âˆ’2.5âˆ’2.0âˆ’1.5Total rewards0.150.20.30.40.100.150.200.250.300.350.40DeDdlines0.00.51.01.52.02.53.03.5TotDl bidsTable 3: Bidding policies for vehicular applications with ï¬xed environment.

(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)

Application

Variable

One

Two

Three

Four

Best episode
Best average total reward
Total bid (in $)
Equivalent bidding rate (in $/second)
Optimal bidding rate (in $/second)
Assigned workload (in million instructions)
Completion time (in seconds)

2200
-0.96
0.72
1.80
1.83
0.02
0.05

4700
-2.00
0.00
0
0
0.00
NA

700
-3.94
3.39
11.30
11.00
0.10
0.15

4000
-4.75
4.36
8.72
7.92
0.13
0.20

[20] M. Ono. Closed-loop chance-constrained mpc with probabilistic resolv-
In 2012 IEEE 51st IEEE Conference on Decision and Control

ability.
(CDC), pages 2611â€“2618, Dec 2012.

[21] E. Ozatay, S. Onori, J. Wollaeger, U. Ozguner, G. Rizzoni, D. Filev,
J. Michelini, and S. Di Cairano. Cloud-based velocity proï¬le optimiza-
tion for everyday driving: A dynamic-programming-based solution. IEEE
Transactions on Intelligent Transportation Systems, 15(6):2491â€“2505,
Dec 2014.

[22] Alexander T. Schwarm and Michael Nikolaou. Chance-constrained model

predictive control. AIChE Journal, 45(8):1743â€“1752, 1999.

[23] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wier-
stra, and Martin Riedmiller. Deterministic policy gradient algorithms.
In Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st Inter-
national Conference on Machine Learning (ICML-14), pages 387â€“395.
JMLR Workshop and Conference Proceedings, 2014.

[24] M. Sookhak and H. Yu, F.and Tang. Secure data sharing for vehicular
In Ad Hoc Networks, pages

ad-hoc networks using cloud computing.
306â€“315. Springer, 2017.

[25] R.S. Sutton and A.G. Barto. Reinforcement learning: an introduction.

Neural Networks, IEEE Transactions on, 9(5):1054â€“1054, 1998.

[26] R.S. Sutton, D.A. McAllester, S.P. Singh, Y. Mansour, et al. Policy gradi-
ent methods for reinforcement learning with function approximation. In
NIPS, volume 99, pages 1057â€“1063, 1999.

[27] C. SzepesvÂ´ari. Algorithms for reinforcement learning. Synthesis Lectures
on Artiï¬cial Intelligence and Machine Learning, 4(1):1â€“103, 2010.
[28] G.E. Uhlenbeck and L.S. Ornstein. On the theory of the brownian motion.

Physical review, 36(5):823, 1930.

[29] M. Whaiduzzaman, M. Sookhak, A. Gani, and R. Buyya. A survey on
vehicular cloud computing. Journal of Network and Computer Applica-
tions, 40:325â€“344, 2014.

[30] G. Yan, D. Wen, S. Olariu, and M.C. Weigle. Security challenges in vehic-
ular cloud computing. IEEE Transactions on Intelligent Transportation
Systems, 14(1):284â€“294, 2013.

[31] K. Zheng, H. Meng, P. Chatzimisios, L. Lei, and X. Shen. An SMDP-
based resource allocation in vehicular cloud computing systems. IEEE
Transactions on Industrial Electronics, 62(12):7920â€“7928, 2015.
[32] Z. Zhou and N. Bambos. A general model for resource allocation in utility
computing. In 2015 American Control Conference (ACC), pages 1746â€“
1751, July 2015.

[2] D. Ardagna, B. Panicucci, and M. Passacantando. A game theoretic for-
mulation of the service provisioning problem in cloud systems. In Pro-
ceedings of the 20th international conference on World wide web, pages
177â€“186. ACM, 2011.

[3] D. Ardagna, B. Panicucci, and M. Passacantando. Generalized nash equi-
libria for the service provisioning problem in cloud systems. IEEE Trans-
actions on Services Computing, 6(4):429â€“442, Oct 2013.

[4] N. Bajcinca. Wireless cars: A cyber-physical approach to vehicle dynam-

ics control. Mechatronics, 30:261â€“274, 2015.

[5] M. Barshan, H. Moens, and F. De Turck. Design and evaluation of a scal-
able hierarchical application component placement algorithm for cloud
In 10th International Conference on Network and
resource allocation.
Service Management (CNSM) and Workshop, pages 175â€“180, Nov 2014.
[6] R. Bellman. A markovian decision process. Technical report, DTIC Doc-

ument, 1957.

[7] M. Dolgov, G. Kurz, and U. D. Hanebeck. Chance-constrained model
predictive control based on box approximations. In 2015 54th IEEE Con-
ference on Decision and Control (CDC), pages 7189â€“7194, Dec 2015.
[8] D. Filev, J. Lu, and D. Hrovat. Future mobility: Integrating vehicle control
with cloud computing. Mechanical Engineering, 135(3):S18, 2013.
[9] S. Ioï¬€e and C. Szegedy. Batch normalization: Accelerating deep
arXiv preprint

network training by reducing internal covariate shift.
arXiv:1502.03167, 2015.

[10] R. Johari, S. Mannor, and J. N. Tsitsiklis. Eï¬ƒciency loss in a network
resource allocation game: the case of elastic supply. IEEE Transactions
on Automatic Control, 50(11):1712â€“1724, Nov 2005.
[11] V.R. Konda and J.N. Tsitsiklis. Actor-critic algorithms.

In NIPS, vol-

ume 13, pages 1008â€“1014, 1999.

[12] Y. Li, X. Tang, and W. Cai. Dynamic bin packing for on-demand cloud
resource allocation. IEEE Transactions on Parallel and Distributed Sys-
tems, 27(1):157â€“170, Jan 2016.

[13] Z. Li. Developments in Estimation and Control for Cloud-Enabled Auto-

motive Vehicles. PhD thesis, The University of Michigan, 2016.

[14] Z. Li, I. Kolmanovsky, E. Atkins, J. Lu, D. Filev, and J. Michelini. Cloud
aided semi-active suspension control. In Computational Intelligence in
Vehicles and Transportation Systems (CIVTS), 2014 IEEE Symposium on,
pages 76â€“83, Dec 2014.

[15] Z. Li, I. V. Kolmanovsky, E. M. Atkins, J. Lu, D. P. Filev, and Y. Bai.
Road disturbance estimation and cloud-aided comfort-based route plan-
ning. IEEE Transactions on Cybernetics, PP(99):1â€“13, 2016.

[16] H. Liang, L. Cai, D. Huang, X. Shen, and D. Peng. An SMDP-based ser-
vice model for interdomain resource allocation in mobile cloud networks.
IEEE Transactions on Vehicular Technology, 61(5):2222â€“2232, 2012.
[17] T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and
D. Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.

[18] I. Menache, A. Ozdaglar, and N. Shimkin. Socially optimal pricing of
cloud computing resources. In Proceedings of the 5th International ICST
Conference on Performance Evaluation Methodologies and Tools, pages
322â€“331. ICST (Institute for Computer Sciences, Social-Informatics and
Telecommunications Engineering), 2011.

[19] V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare,
A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529â€“
533, 2015.

10

