Cloud Resource Allocation for
Cloud-Based Automotive Applications

Zhaojian Lia, Tianshu Chub, Ilya V. Kolmanovskya, Xiang Yind,∗, Xunyuan Yinc

aDepartment of Aerospace Engineering, The University of Michigan, Ann Arbor, MI 48109, USA.
bDepartment of Civil and Environmental Engineering, Stanford University, CA 94305, USA.
cDepartment of Chemical and Materials Engineering, University of Alberta, Edmonton, AB T6G 1H9, Canada.
dDepartment of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA.

7
1
0
2

n
a
J

7
1

]

Y
S
.
s
c
[

1
v
7
3
5
4
0
.
1
0
7
1
:
v
i
X
r
a

Abstract

There is a rapidly growing interest in the use of cloud computing for automotive vehicles to facilitate computation and data intensive
tasks. Eﬃcient utilization of on-demand cloud resources holds a signiﬁcant potential to improve future vehicle safety, comfort, and
fuel economy. In the meanwhile, issues like cyber security and resource allocation pose great challenges. In this paper, we treat the
resource allocation problem for cloud-based automotive systems. Both private and public cloud paradigms are considered where
a private cloud provides an internal, company-owned internet service dedicated to its own vehicles while a public cloud serves all
subscribed vehicles. This paper establishes comprehensive models of cloud resource provisioning for both private and public cloud-
based automotive systems. Complications such as stochastic communication delays and task deadlines are explicitly considered. In
particular, a centralized resource provisioning model is developed for private cloud and chance constrained optimization is exploited
to utilize the cloud resources for best Quality of Services. On the other hand, a decentralized auction-based model is developed for
public cloud and reinforcement learning is employed to obtain an optimal bidding policy for a “selﬁsh” agent. Numerical examples
are presented to illustrate the eﬀectiveness of the developed techniques.

Keywords: Resource Allocation, Vehicle-to-Cloud, Chance Constrained Optimization, Communication Delays, Deep
Deterministic Policy Gradient, Reinforcement Learning

1. Introduction

There is growing interest in employing cloud computing in
automotive applications [4, 8, 13, 16, 24, 29–31]. Ready access
to distributed information and computing resources can enable
computation and data intensive vehicular applications for im-
proved safety, drivability, fuel economy, and infotainment. Sev-
eral cloud-based automotive applications have been identiﬁed.
For instance, a cloud-based driving speed optimizer is studied
in [21] to improve fuel economy for everyday driving. In [15],
a cloud-aided comfort-based route planner is prototyped to im-
prove driving comfort by considering both travel time and ride
comfort in route planning. A cloud-based semi-active suspen-
sion control is studied in [14] to enhance suspension perfor-
mance by utilizing road preview and powerful computation re-
sources on the cloud.

As such, cloud computing has been both an immense oppor-
tunity and a crucial challenge for vehicular applications: oppor-
tunity because of the great potential to improve safety, comfort,

∗The material in this paper was not presented at any IFAC conference. Cor-

responding author.

Email addresses: zhaojli@umich.edu (Zhaojian Li),

cts1988@stanford.edu (Tianshu Chu), ilya@umich.edu
(Ilya V. Kolmanovsky), xiangyin@umich.edu (Xiang Yin),
xunyuan@ualberta.ca (Xunyuan Yin)

Preprint submitted to Mechatronics

and enjoyment; challenge because cyber-security and resource
allocation are critical issues that need to be carefully consid-
ered. A cloud resource allocation scheme determines how a
cloud server such as Amazon “EC2” or Google Cloud Platform
distributes resources to its many clients (vehicles in our context)
eﬃciently, eﬀectively, and proﬁtably. This allocation design
becomes even more challenging when it comes to cloud-based
automotive systems in which issues like communication delays
and task deadlines arise. These complexities make a good re-
source allocation design a non-trivial, yet important task.

Not surprisingly, extensive studies have been dedicated to the
development of eﬃcient and proﬁtable cloud resource alloca-
tion schemes. A dynamic bin packing method, MinTotal, is
developed in [12] to minimize the total service cost. In [5], a
distributed and hierarchical component placement algorithm is
proposed for large-scale cloud systems. A series of game theo-
retical cloud resource allocation approaches have also been de-
veloped, see e.g., [2, 3, 10, 18]. However, as far as the authors
are aware, a resource allocation scheme for cloud-based au-
tomotive systems that accounts for communication delays and
task deadlines is still lacking.

In this paper, we develop resource allocation schemes for
cloud-based automotive systems that optimally tradeoﬀ costs
and Quality of Service (QoS) with the presence of stochastic
In particular, we
communication delays and task deadlines.

January 18, 2017

 
 
 
 
 
 
consider allocation schemes under two cloud paradigms, pri-
vate and public cloud. A private cloud is a company-owned re-
source center which provides computation, storage and network
communication services and is only accessible by cars made by
the car company. The private cloud therefore has a high level
of security and information is easy and safe to share and man-
age. On the other hand, a public cloud relies on a third-party
service provider (e.g., Amazon EC2) that provides services to
all subscribed vehicles. A public cloud can eliminate the capi-
tal expenses for infrastructure acquisition and maintenance, and
can provide the service on an as-needed basis.

The objectives of resource allocation are quite diﬀerent be-
tween private and public cloud paradigms. Since the private
cloud resources are pre-acquired, the company basically “use
them or waste them”. Therefore, the goal of private cloud re-
source allocation is to best utilize its resources to provide good
QoS to its subscribed vehicles. Since the information exchange
between vehicles and the server is more secure and convenient,
the resource allocation can be achieved in a centralized manner.
On the other hand, public cloud provides services to subscribed
vehicles from a variety of makers, e.g., Ford, GM, Toyota, etc.
Due to security and privacy issues, these vehicles typically will
not share their information nor be interested in coordination;
hence each vehicle becomes a “selﬁsh” agent. The goal of each
agent is to minimize its service cost while maintaining good
QoS.

In this work, we develop mathematical models to formalize
the resource allocation problems for both private and public
cloud paradigms. Stochastic communication delays and on-
board task deadlines are explicitly considered. A centralized
resource-provisioning scheme is developed for private cloud
and chance constrained optimization is employed to obtain an
optimal allocation strategy. On the other hand, an auction-based
bidding framework is developed for public cloud and reinforce-
ment learning is exploited to train an optimal bidding policy
to minimize the cost while maintaining good QoS. Numerical
examples are presented to demonstrate the eﬀectiveness of the
proposed schemes.

The main contributions of this paper include the follow-
ing. Firstly, compared to the previous literature on cloud re-
source allocation, issues important to automotive vehicles such
as communication delays and onboard task deadlines are ex-
plicitly treated in this paper. Secondly, resource allocation
within a private cloud paradigm is formalized as a centralized
resource partitioning problem. Chance constrained optimiza-
tion techniques are employed to obtain the optimal partitioning
by solving a convex optimization problem. Thirdly, a decentral-
ized, auction-based bidding framework is developed for public
cloud-based resource allocation and the best response dynamics
assuming a constant time delay and bidding is derived. Further-
more, a Deep Deterministic Policy Gradient (DDPG) algorithm
is exploited to train the optimal bidding policy with stochastic
time delay and unknown bidding from other vehicles. Sensitiv-
ity analysis is also performed to show how the bidding policy
can change by varying task parameters such as workload and
deadline.

The rest of our paper is organized as follows. Section 2 de-

scribes the model of cloud resource provisioning for private
cloud-based automotive systems. The problem formulation and
a chance constrained optimization approach are also presented.
In Section 3, a numerical example is given to illustrate the allo-
cation scheme for private cloud. The resource allocation prob-
lem with a public cloud is formalized in Section 4. The best
response dynamics with constant time delay and bidding is also
derived. A DDPG algorithm is exploited in Section 5 to train
the optimal bidding policy with stochastic time delay and un-
known bidding from other vehicles. A numerical case study
is also presented with sensitivity analysis on task parameters.
Finally, conclusions are drawn in Section 6.

2. Centralized Resource Allocation with a Private Cloud

It is more secure and manageable for automotive manufactur-
ers to acquire and maintain its own private cloud infrastructure
which provides computation, data storage and network services
only to vehicles made by the manufacturer. A schematic di-
agram of resource allocation for private cloud-based automo-
tive systems is illustrated in Figure 1. Suppose that a set of
cloud-based vehicular applications are available (e.g., cloud-
based route planning, cloud-based suspension control, etc.) and
we consider a general case that each vehicle runs a subset of
these applications. Let us consider a total number of N appli-
cations running on M vehicles as in Figure 1. Each application
i, i = 1, 2, . . . , N, corresponds to a periodic task associated with
a tuple, Ti = {Ti, wi, di, τi}, where

• Ti is the period of task i in seconds;

• wi is the workload of task i in million instructions;

• di ≤ Ti is the deadline of task i in seconds;

• τi is a random time delay of the communication channel

associated with task i in seconds.

For each task i, the Quality of Service (QoS) is characterized

by the following cost function adopted from [32]:

Ci(γi; τi) =

+ τi),





Bi( wi
γi
Mi,

+ τi ≤ di

if wi
γi
Otherwise,

(1)

where γi is the process rate that the cloud resource allocator as-
signs to task i and (cid:80)N
i=1 γi = γ with γ being the total resource
available on the cloud; Bi(·) : R+ → R+ is a non-decreasing
function reﬂecting the QoS of task i; Mi ≥ Bi(di) is a pos-
itive scalar representing the penalty for missing the deadline;
the condition wi
+ τi > di indicates that the deadline has been
γi
missed. Note that task priorities are reﬂected in the deadline-
missing penalty Mi. For safety-critical tasks (e.g., cloud-based
functions involved in powertrain or vehicle control), a large
penalty, M, should be given while a small M can be assigned to
some non-critical tasks such as online video streaming.

Since a private cloud is a pre-acquired “use it or waste it”
capability, the goal of resource allocation for private cloud-
based automotive systems is to distribute the cloud resources

2

of missing a deadline can be characterized by a small α while
larger α can be used for applications with mild consequences of
missing a deadline.

Note that the deadline-missing penalty M and the chance
constraint α are transformable. For instance, one can use the
following function to map deadline-missing penalties to chance
constraints:

αi = αmax + αmin − αmax
Mmax − Mmin

(Mi − Mmin),

(4)

where αmin and αmax are, respectively, the lower and upper
bounds of the chance constraints while Mmin and Mmax are
the corresponding lower and upper bounds of the deadline-
violation penalties, respectively. These parameters need to be
chosen compatibly to reﬂect the same QoS requirements. The
example transformation in (4) is illustrated in Figure 2.

Figure 2: Linear mapping from deadline-missing penalty to
chance constraint.

Figure 1: Schematic diagram of private cloud-based resource
allocation.

to the N tasks such that the total expected QoS cost as in (1) is
minimized. Basically, the cloud collects task information (i.e.,
workload, deadline, time delay statistics1) of the N tasks and
determines how optimally to partition the total resources into N
parts so that the expected QoS cost is minimized. The problem
can be mathematically formalized as a constrained optimization
problem

J(Γ) = E(cid:2)

min
Γ

Subject to:

N(cid:88)

Ci(γi; τi)(cid:3)

i=1
N(cid:88)

γi = γ,

i=1
γi ≥ 0, ∀i = 1, · · · , N

(2)

where Γ = [γ1, γ2, · · · , γN]T is the vector of process rates to be
optimized.

We note that the problem (2) is challenging to solve due to
the randomness of communication delay τi and the discontinu-
ity of the cost function represented in (1). Motivated by the
chance constrained formulation in Stochastic Model Predictive
Control developments [7, 20, 22], we re-formulate problem (2)
by imposing chance constraints. Instead of penalty for missing
deadline as in (1), we impose chance constraints for missing
deadlines of the form,

Now let us assume that the communication delays can be
modeled as independent Gaussian random variables, i.e., τi ∼
N( ¯τi, σ2
i ). From basic probability theory, the probability of the
delay taking values between a and b,

Pr(a < τi ≤ b) = 1
2

erf(

b − ¯τi√
2σi

) −

1
2

erf(

a − ¯τi√
2σi

),

(5)

where erf(x) = 2√
π

(cid:82) x
0

e−t2 dt is the error function.

As a result, from (3) and (5), it follows that

Pr(

wi
γi

+ τi ≤ di) ≥ 1 − αi,

(3)

Pr(τi ≤ di −

wi
γi

) = 1
2

erf(

− ¯τi

di − wi
γi
√
2γi

) + 1
2

≥ 1 − αi.

(6)

where αi ∈ (0, 1) is a scalar representing the chance constraint
of missing a deadline, i = 1, · · · , N. The notion of α can be in-
terpreted as the upper limit of deadline missing rate speciﬁed in
the QoS requirements. Applications with harsh consequences

1Note that the task period Ti is not used here but we include it as one of
the four task attributes for completeness. The task period will appear when it
comes to the public cloud-based resource allocation in Section 4.

3

We next apply the inverse error function erf−1(·) to both sides

of (6). Since erf−1(·) is continuous and increasing, we have
√

di −

− ¯τi ≥

2σierf−1(1 − 2αi).

(7)

wi
γi

Note that (7) requires the term di − ¯τi −

2σierf−1(1 − 2αi)
to be positive so that γi is feasible. This condition means that

√

𝜸𝟏  𝜸𝟐  𝝉𝟏  𝝉𝟐  𝝉𝒊  𝝉𝒊+𝟏  𝝉𝑵−𝟏  𝝉𝑵  𝜸𝒊  𝜸𝒊+𝟏  𝜸𝑵−𝟏  𝜸𝑵  𝜸 𝑇1 𝑇2 𝑇𝑖 𝑇𝑖+1 𝑇𝑁−1 𝑇𝑁 Cloud  Resources Resource Allocator 𝛼𝑚𝑎𝑥 𝛼𝑚𝑖𝑛 𝑀𝑚𝑖𝑛 𝑀𝑚𝑎𝑥 𝑀 𝛼 𝑀𝑖 𝛼𝑖 (10)

4. Decentralized resource allocation for public cloud

the mean of the delay ¯τi cannot be greater than the deadline di.
Also, given deadline di, delay mean ¯τi and standard deviation
σi, the minimum achievable chance constraint level, α∗ is

α∗ = 1
2

−

er f (di − ¯τi)
√

2

2σi

,

(8)

which deﬁnes a maximum performance bound regardless of al-
located resources. For example, if di = 0.3, ¯τi = 0.2, and
σi = 0.1, then from (8) we have α∗ = 0.3976, which means that
no matter how many resources are allocated for task i, the prob-
ability of missing a deadline is no less than α∗ = 0.3976 due to
communication delays. On the other hand, α∗ < 0 means that
the probability of not missing deadline can be inﬁnitely close to
1 with enough resources.

Re-arranging terms in (7) leads to
wi

γi ≥ ρi =

√

di − ¯τi −

2σierf−1(1 − 2αi)

.

(9)

So far we showed that (9) and (3) are equivalent. Therefore,

the problem in (2) can be re-stated as:

J(Γ) = E(cid:2)

min
Γ

N(cid:88)

i=1

Bi(

wi
γi

+ τi)(cid:3)

subject to:

N(cid:88)

γi = γ,

i=1
γi ≥ ρi > 0, ∀i = 1, 2, . . . , N,

where ρi are assumed to be positive and deﬁned by (9).

Note that if we choose B(·) to be a convex function of γi,
as we will show in the next section, problem (10) reduces to
a convex optimization problem which can be eﬃciently solved
with good scalability.

3. A numerical example with a linear QoS function

In this section, we consider a linear QoS function in the form
+ τi) with bi > 0. The problem (10)

+ τi) = bi · ( wi
γi

of Bi( wi
γi
becomes

N(cid:88)

i=1

bi · (

wi
γi

+ ¯τi)

min
Γ

J(Γ) =

subject to:

N(cid:88)

γi = γ,

i=1
γi ≥ ρi > 0, ∀i = 1, 2, . . . , N.

(11)

We show that the above problem is a convex optimization
problem. We ﬁrst demonstrate that the cost function J(Γ) in
(11) is strictly convex in the domain {Γ = [γ1, · · · , γN]T : γi >
0, ∀i = 1, 2, · · · N}. Towards that end, we compute the Hessian
of the cost function J as

Hxx(J(Γ)) = diag{

2b1w1
γ3
1

, · · · ,

2bNwN
γ3
N

},

(12)

4

where Hxx(·) represents the Hessian matrix and diag{·} denotes
the diagonal matrix with the arguments as the entries in the di-
agonal. Since bi, wi, γi are positive for i = 1, 2, · · · , N, we have
Hxx(J(Γ)) being positive deﬁnite, which means that the cost
function J(Γ) is strictly convex. Furthermore, the constraints
in (11) are polytopic. Therefore, (11) is a convex optimiza-
tion problem that can be eﬃciently solved by many numerical
solvers. This means even if N is large, an optimal resource al-
location can be eﬃciently computed.

We next give a numerical example with four tasks. The pa-
rameters are given in Table 1 and we consider a total resource of
1, i.e., γ = 1. The fmincon function in MATLAB was exploited
to solve (11) and the optimized allocation strategy is,

γ2 = 0.1495,

γ3 = 0.3585,

γ1 = 0.1608,

γ4 = 0.3312.
(13)
To verify the chance constraints with the optimized allocation
scheme, we run simulations under the allocation policy (13) for
106 times with the random delays speciﬁed in Table 1. The
missing deadline violation rates for the four applications are,
respectively, 0.0961, 0.0482, 0.01991, and 1.3 ∗ 10−5, which
are all smaller than the speciﬁed chance constraints in Table 1.
This means that the speciﬁed chance constraints are satisﬁed
under the allocation scheme (13).

paradigm

4.1. Problem formulation

An automotive manufacturer may choose to subscribe its
cloud-based automotive applications to a public cloud with-
out acquiring and maintaining its own infrastructure. A pub-
lic cloud such as Amazon EC2 oﬀers an on-demand and pay-
as-you-go access over a shared pool of computation resources.
This public cloud provides services to a large number of auto-
mobiles from a variety of manufacturers. These vehicles may
not want to share either their resource policies or task infor-
mation with other vehicles, which makes it impossible to run a
centralized allocation scheme as in the private cloud paradigm.
Instead, each vehicle becomes a “selﬁsh” client that seeks to
minimize its own cost while maintaining good QoS.

We consider a decentralized auction-based resource alloca-
tion model as illustrated in Figure 3. For the considered ve-
hicle, let N denote the number of tasks that are running in
the vehicle and each task is associated with the same tuple
Ti = {Ti, wi, di, τi} as deﬁned in Section 2. The public cloud
is running an auction-based resource allocation scheme, that is,
i = 1, · · · , N, submits a bid pi (in US
each vehicular task i,
dollars per second) and obtains a proportion of the total cloud
resources as:

γi = pi
P

· γ =

pi
P− + (cid:80)N

i=1 pi

· γ,

(14)

where P is the sum of all bids the cloud receives from all ve-
hicles; P− = P − (cid:80)N
i=1 pi is the cumulative bid from all other
vehicles; and γ quantiﬁes the total resources available on the
cloud.

Table 1: Parameters for numerical example

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Task

Attribute
Workload (wi, in Million instructions)
Deadline (di, in seconds)
QoS cost scalar (bi, in $/s)
Delay mean (¯τi, in seconds)
Delay standard deviation (σi, in second)
Chance limit (α∗
i from (8))
Chance constraint (αi, unitless)

One

Two

Three

Four

0.02
0.25
1
0.1
0.02
-2.47
0.1

0.03
0.35
2
0.1
0.03
-2.76
0.05

0.1
0.4
2
0.08
0.02
-5.67
0.02

0.12
0.6
3
0.11
0.03
-5.53
0.01

Figure 3: Schematic diagram of public cloud resource alloca-
tion

Since there are many other vehicles subscribed to the public
i=1 pi. From (14)

cloud, it is reasonable to assume that P− >> (cid:80)N
it follows that

γi ≈

pi
P− γ,

(15)

which implies that the bidding policy of the tasks can be con-
sidered independently.

We consider a general bidding model in which the time pe-
riod between the beginning of a period and the deadline is com-
posed of multiple bidding steps. As illustrated in Figure 4, there
are l, l ≥ 1, bidding steps before the deadline in each task pe-
riod. With the QoS cost modeled in (1), the overall cost of task
i in its period Ti is,

Ji =

l(cid:88)

t=1

pi,t · ts + Ci(γt; τi),

(16)

where pi,t is the bidding for task i at bidding step t; ts = d
l is the
bidding time interval; and Ci(γt; τi) is the QoS cost deﬁned in
(1).

The goal of the vehicle is to ﬁnd an optimal bidding policy
to minimize the accumulated cost (16) for each task. We next
derive the optimal bidding strategy with a preliminary assump-

Figure 4: A general bidding model with l bidding steps in one
task period.

tion that P− and τ are known and constant. In Section 5, this
assumption is removed.

4.2. Best response dynamics with constant P− and τ

In this subsection, we seek to ﬁnd the optimal bidding if bids
from other vehicles (P−) and the communication delays (τ) are
known and constant. Since P− is constant, all it matters is the
total bidding. For task i, the optimal average bidding in the
interval [0, di − τi] is deﬁned as
p∗
i

Ji(pi)

pi · (di − τi) + Ci(pi; τ),

(17)

= argmin
pi
(cid:44) argmin
pi

where Ci(pi; τ) is deﬁned in (1) and from (15) it follows that

Ci(pi; τi) =





Bi( wiP−
piγ
Mi,

+ τi),

if pi ≥ wiP−
(di−τi)γ
Otherwise.

As a result, the overall cost function J(pi) becomes
if pi ≥ wiP−
(di−τi)γ
Otherwise.

pi · (di − τi) + Bi( wiP−
piγ
pi · (di − τi) + Mi,

Ji(pi) =

+ τi),





(18)

(19)

Consider the linear QoS function Bi(x) = bi · x with bi > 0
as in Section 3. Then there are two local minimizers in (19):
= 0), the other corresponds
one associated with no bidding (p∗
i
to the optimal bidding with no deadline missing. The second
minimizer can be represented as

(cid:115)

(cid:110)

p∗
i

= max

biwiP−
(di − τi)γ

,

wiP−
(di − τi)γ

(cid:111)
,

(20)

5

CloudResources𝛾𝒑𝟏𝒑𝟐𝒑𝑵𝑷−𝜸𝟏𝜸𝟐𝜸𝑵𝜸−Auction Agent1 2 3 𝑙 𝑻 𝑻 𝑻 𝒅 𝒅 𝒅 Bidding Horizon (cid:113)

+ τi), i.e.,

depending whether the minimizer of the function pi · (di − τi) +
Bi( wiP−
biwiP−
γ·(di−τi) , can avoid deadline missing. An
piγ
example of the cost function (19) with wi = 0.06, d = 0.4, τ =
0.1, bi = 8, P− = 20, Mi = 5, ts = 0.05, and γ = 10 is illustrated
in Figure 5. The global minimizer is

= 1.7889.

(cid:113)

biwiP−
γ·ts

Figure 5: Overall cost as a function of total bidding with wi =
0.06, d = 0.4, τ = 0.1, bi = 8, P− = 20, Mi = 5, ts = 0.05, and
γ = 10.

The assumption that the P− and τ are constant and known is
unrealistic in many applications. We next exploit a reinforce-
ment learning framework to obtain the optimal bidding policy
with no assumptions on the prior knowledge of P− and τ.

5. Training optimal bidding policy using RL

5.1. Introduction to RL

Reinforcement learning (RL) is a data-driven approach for
adaptively evolving optimal control policies based on the real-
time measurements. Unlike traditional methods, RL models the
stochastic ambiguity within the framework of Markov decision
processes (MDP) [6], and learns the policy according to tran-
sition data observations [25]. There are commonly three types
of RL algorithms: Q-learning, policy gradient, and actor-critic.
The Q-learning (or approximate Q-learning) is the traditional
RL algorithm that learns a Q-function Qθ(s, a) with model pa-
rameter θ to estimate the delayed total reward of the current
state and action a, and performs control as ˆa ∈ argmaxa Qθ(s, a)
[27] based on the learned policy. The Q-learning updates the
Q-function parameters based on each observed temporal diﬀer-
ence using stochastic gradient descent:

θ ← θ + η∆tφ(st, at),

(21)

where η is the learning rate, φ(st, at) is the input feature vector
for the learning model, and ∆t is the sampled temporal diﬀer-
ence with stage reward rt and discount factor α:

∆t = rt + α max

a

Qθ(st+1, a) − Qθ(st, at).

(22)

6

In the policy gradient approach, the stochastic optimal action
distribution πθ(a|s) with model parameter θ is learned directly,
and control action is determined as ˆa ∈ argmaxa πθ(a|s) [26].
Policy gradient updates the policy distribution based on each
observed advantage function:

θ ← θ + η∇θ log πθ(at|st)( ˆQπ(st, at) − ˆV(st)),

(23)

where ˆQπ(st, at) is the sampled Q-value of (st, at) by following
policy πθ and ˆV(st) is the sampled optimal value of st.

The actor-critic approach can be regarded as a combination
of both since it learns both the policy πθ(a|s) and the corre-
sponding Q-function of the policy Qπ
β(s, a) [11]. The details of
the actor-critic updates will be covered in the following subsec-
tion.

All the above algorithms typically assume the discrete action
space which, in particular, simpliﬁes the search for ˆa. However,
in our resource allocation problem, it is more natural to consider
a continuous action space since the bid should be a continuous
numerical number. For this case, deterministic policy gradient
algorithm was developed recently that allows to directly learn
the policy µ(s) instead of the policy distribution π(a|s), and the
control is simply performed as ˆa = µ(s) [23]. Then instead of
the traditional (cid:15)−greedy exploration or Boltzmann exploration
for Q-learning, we need to perform Ornstein-Uhlenbeck noise
[28] to explore with the deterministic continuous policy.

In the following sections, we ﬁrst formulate the bidding-
based resource allocation problem. We then propose the corre-
sponding MDP formulation for this stochastic optimal control
problem. Further, we implement a deep network based actor-
critic algorithm to learn the optimal bidding strategy using de-
terministic policy gradient. Finally, we evaluate the perfor-
mance of this algorithm using various numerical experiments.

5.2. Training Optimal Bidding policy with deep deterministic

policy gradient

In this section, we exploit RL to seek the optimal bidding
policy. Towards that end, we ﬁrst model the bidding process as
a Markov Decision Process (MDP), M = {S, A, P, r, α}, where

• S = {wt, ∆wt, at−1, dt} represents the state space, where
wt is the remaining workload at time t; ∆wt = wt−1 − wt
is the recent processed work; at−1 represents the last bid,
and dt represents the remaining time until deadline. At
the beginning of each period, the initial state is simply
s0 = [w, 0, 0, d];

• A ∈ [0, +∞) is the action space representing the bidding

for the task;

• P is the transition matrix where each element Pa

ss(cid:48) =
P[S t+1 = s(cid:48)|S t = s, At = a] is the probability that the
system transfers to s(cid:48) from s given the current action a.

• r is a stage reward function given after each bidding to
guide the optimal decision-making: r(s, a) = E[rt|S t =
s, At = a].

0123451.522.533.544.555.5Total bidding pi ($)Overall cost Ji(pi) ($)• α ∈ [0, 1) is a discount factor that diﬀerentiates the impor-

tance of future rewards and present rewards.

Finally, we update the parameter of the actor policy function
based on the Q-function estimation

Since the bidding of other vehicles is unknown so the transi-
tion matrix P, traditional MDP optimizers such as Policy itera-
tion and Value iteration cannot be directly applied. In this study,
we exploit RL to learn an optimal bidding policy for each appli-
cation. Furthermore, since the action space A = [0, +∞) is con-
tinuous, approximate Q-learning and stochastic policy gradient
algorithms cannot be applied without action discretization. To
resolve this diﬃculty, we exploit the deterministic actor-critic
(DAC) algorithm.
In particular, we learn both a parameter-
: S → A to perform
ized deterministic policy function µθ
the bidding action, and another parameterized critic Q-function
Qβ : S × A → R to evaluate the bidding strategy. The bidding
and learning procedure with a typical DAC algorithm is:

1. At each time step t ≤ l, observe the state st.

2. Perform a bid ˆat based on the actor policy plus
ˆat =

some random exploration perturbations,
µθ(st)+perturbations.

i.e.,

3. Receive the cloud resource allocation γt, and update the

states as

∆wt+1 = γt · ts, wt+1 = wt − ∆wt+1,

dt+1 = dt − ts. (24)

4. Terminate whenever the procedure is completed: wt+1 ≤

0, dt+1 ≥ 0, or aborted: wt+1 > 0, dt+1 = 0.

5. Receive the current reward. If the procedure is aborted, re-
ceive a deadline-missing penalty rt = −M; if the procedure
is completed, a cost rt = −b · t with b be a positive scalar
representing the QoS cost coeﬃcient is received; other-
wise the agent receives the following state stage cost

rt = −ˆat · ts.

(25)

In Step 2, instead of performing an (cid:15)−greedy exploration
over the entire action space, we add some Ornstein-Uhlenbeck
noises into ˆat to explore actions in the vicinity. A replay buﬀer
is employed to store recent transitions (st, at, st+1, rt) so that ran-
dom transitions can be sampled to train the parameterized mod-
els to reduce the eﬀects of data correlation [19]. When the re-
play buﬀer is ﬁlled, we can update both β and θ in the models
by exploiting W randomly selected transitions from the buﬀer.
The update of β is similar to the one in Q-learning: First we
estimate the temporal diﬀerence from each selected transition:

∆t = rt + αQβ(st+1, µθ(st+1)) − Qβ(st, at),

(26)

where α ∈ [0, 1) is the discount factor. We then update the
parameter in the critic Q-function using stochastic gradient de-
scent, i.e.,

β ← β +

ηβ
W

W(cid:88)

t=1

∆t∇βQβ(st, at).

(27)

7

θ ← θ + ηθ
W

W(cid:88)

t=1

∇θµθ(st)∇aQβ(st, a)|a=µθ(s).

(28)

Here ηβ and ηθ are positive constants representing the learn-
ing rates. In this study, we apply deep neural networks as the
approximation functions for both the actor and critic. This spe-
ciﬁc implementation of the deterministic actor-critic (DAC) al-
gorithm is referred to as the deep deterministic policy gradi-
ent (DDPG) [17]. Furthermore, techniques such as experience
replay [19] and batch normalization [9] are also employed to
improve the learning performance.

The complete DDPG algorithm is shown in Algorithm 6. The
algorithm parameters include: the discount factor α, learning
rates ηβ, and ηθ in (26), (27), and (28), respectively; bidding
horizon l as in Figure 4; replay buﬀer size K; mini-batch size
W, W < K; task workload w; task deadline d; total cloud re-
source γ, and parameter smoothing scalar δ ∈ (0, 1). Speciﬁ-
cally, Line 1 initializes the network parameters and the target
network parameters for smooth updating. Line 2 initializes an
experience replay D that stores the K most recent transition
samples. At the beginning of each training episode (periodic
task), we reset the state and sample time delay. At each time
step, Line 6 performs exploration with some sampled Ornstein-
Uhlenbeck noise (cid:15)OU; Line 7 samples P− from the environment;
Lines 8-9 observe the system transition and add the current tran-
sition sample to the replay buﬀer; Lines 10-12 update the net-
works based on the sampled minibatch from the experience re-
play; Line 13 updates the corresponding target networks with a
weighted sum to smooth the training.

5.3. Numerical experiments

5.3.1. Simulation setup

In this subsection, we perform simulations to illustrate the
DDPG approach in Algorithm 1. Four tasks are considered
in the host vehicle. The task speciﬁcations are listed in Ta-
ble 2. The bidding policy of each application is trained sep-
arately. We deﬁne the total cloud resource as γ = 1 mil-
lion instructions/second. The time delays of all applications
are assumed to be the same and are τ ∼ |N(0.1, 0.0025)| in
seconds. The bidding period ts is set to 0.05 seconds so 20d
gives bidding horizon l in steps and 20τ gives the delay in
steps. We sample P− from an Ornstein-Uhlenbeck process with
µ = 33, θ = 1, σ = 1.5 $/second to reﬂect similar prices from
Amazon EC2 [1]. Three sampled trajectories of the Ornstein-
Uhlenbeck process are illustrated in Figure 7. We set b = 2 in
the cost functions for all tasks.

For DDPG training, we train 5000 task periods with α =
0.99, δ = 0.001, K = 50000, M = 32. The actor network
we use has two hidden layers with sizes 20 and 15, and the
learning rate ηθ = 0.00001. We build the critic network using
the same structure with learning rate is ηβ = 0.0001. We also
bound the bidding at each time step as 1.5 $/0.05 second to

Table 2: Parameters of vehicular applications for simulation.

(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)

Application

Attribute

One

Two Three

Four

Workload (w, in million instructions)
Deadline (d, in seconds)
Penalty for missing deadline (M, in $)

0.02
0.5
2

0.06
0.4
2

0.1
0.4
10

0.12
0.6
10

open-source package DDPG2.

5.3.2. Training results

We train the actor and critic networks with the simulation
setup as described above. The training history of the bidding
policy for the four applications is shown in Figure 8. The ﬁgure
shows the total rewards (line is the average value and shade is
the standard deviation) over 20 testing episodes from every 100
training episodes. As we can observe, the best bidding policy
for application 2 is not bidding since the cost of bidding so that
the deadline is not missing is more than the deadline missing
penalty.

However, the bidding policies for tasks 1, 3, and 4 do not
converge. The reason is that as we show in Section 4.2, there
are two local minima in the cost function: no bidding, and min-
imum bidding for completing the job before deadline. Note that
the second minimizer is unstable since a further small reduction
on bidding may result in the convergence to the ﬁst minimizer.
So instead of using the DDPG model after the entire training,
the ﬁnal choice of our model is the best model during the train-
ing procedure based on the testing results as in Figure 8.

Figure 6: Algorithm 1

Figure 8: Total rewards vs. training episodes. Left: application
1 and 2, right: application 3 and 4.

Next, in order to validate the optimality of our obtained pol-
icy, we investigate the trained policies with ﬁxed P− = 33
$/second and τ = 0.1 second so that we can compare with the
analytical form of best bidding in Section 4.2. The results and
the comparison are listed in Table 3. We can see that DDPG
almost captures either of the two local minima, depending on
the amount of penalty. We also estimate the equivalent bidding
rate as

ˆp =

(cid:80)l

t=1 at
d − τ

,

(29)

so we can compare it with the optimal bidding rate given in
(20). We can see there is a small diﬀerence between ˆp and p∗,

Figure 7: Three sampled P− bidding trajectories from Ornstein-
Uhlenbeck process.

scale the output of DDPG. Our implementation is based on an

2Package site: https://github.com/songrotek/DDPG.

8

05101520Time steps293031323334353637Totoal bids010002000300040005000Training episodes−4.0−3.5−3.0−2.5−2.0−1.5−1.0−0.50.0Total rewardsapp1app2010002000300040005000Training episodes−14−12−10−8−6−4−20Total rewardsapp3app4which may be due to the fact that we discretize the time hori-
zon and round up the completion time to steps of 0.05 second.
Figure 9 illustrates the detailed bidding policy at each time step
for each application, where the y axis shows the actual bid per
step instead of the bidding rate for easier comparison. We can
see that DDPG tends to complete the process with fewer time
steps but to split the bids equally among these steps.

Figure 9: Trained bidding vs. time steps. Left: application 1
and 2, right: application 3 and 4.

5.3.3. Sensitivity analysis

In this subsection, we perform sensitivity analysis of diﬀer-
ent parameters, i.e., investigate how the bidding policy changes
over task parameter variations. We choose application 2 to ana-
lyze how workload, deadline, and deadline-missing penalty can
inﬂuence the obtained bidding policy. We ﬁrst ﬁx w = 0.06
million instruction, d = 0.4 second, and let M = 2, 2.5, 3, 3.5
$, respectively. The results are shown in Figure 10. We can
see as the penalty increases, the bidding strategy switches from
zero-bidding to minimum bidding for job completion. How-
ever, when M = 2.5$, the total rewards of these two bidding
strategies are very close, so the learned policy is slightly worse
than the optimal policy with total bid 2.06$ and assigned work-
load 0.06 million instructions. When M increases further, the
learned policy becomes stable and optimal.

Figure 10: The impact of penalty on trained policy (in $). Left:
rewards during training procedure, right: total bid vs. penalty.

Next, we ﬁx d = 0.4 second, M = 3.5 $, and change
w = 0.06, 0.08, 0.1, 0.12 million instruction, respectively. The
results are shown in Figure 11. We can see as the workload in-
creases, the total bid also increases accordingly. When the total
bid and QoS cost becomes larger than the penalty, the bidding
strategy switches to zero-bidding.

Finally, we ﬁx w = 0.06 million instruction, M = 3.5 $,
and change d = 0.15, 0.2, 0.3, 0.4 second. Similar results can
be observed in Figure 12: when the deadline is long enough,

9

Figure 11: The impact of workload on trained policy (in million
instructions). Left: rewards during training procedure, right:
total bid vs. workload.

DDPG always bids, when the deadline is so short (d − τ may
be a single step) that the application can not be completed even
with the maximum bid bound, DDPG switches to zero-bidding.

Figure 12: The impact of deadline on trained policy (in sec-
onds). Left: rewards during training procedure, right: total bid
vs. deadline.

6. Conclusions

In this paper, we studied the problem of resource allocation
for cloud-based automotive systems. Resource provisioning
under both private and public cloud paradigms were modeled
and treated. Task deadlines and random communication delays
were explicitly considered in these models. In particular, a cen-
tralized resource provisioning scheme was used to model the
dynamics of private cloud provisioning and chance-constrained
optimization was exploited to utilize the cloud resource to min-
imize the Quality of Service (QoS) cost while satisfying speci-
ﬁed chance constraints. A decentralized auction-based bidding
scheme was developed to model the public cloud resource pro-
visioning. Best dynamics with constant bidding and constant
time delays were ﬁrst derived and a deep deterministic policy
gradient was exploited to obtain the best bidding policy with
random time delays and no prior knowledge on the random bid-
ding from other vehicles. Numerical examples were presented
to demonstrate the developed framework. We showed how the
optimal bidding policy changes with parameters such as task
workload and deadline.

References

[1] Amazon

EC2.

https://aws.amazon.com/ec2/pricing/

on-demand/. Accessed: 2016-12-24.

0.00.51.01.52.0Time steps−0.50.00.51.01.5Bidsapp1app20123456Time steps−0.50.00.51.01.5Bidsapp3app4010002000300040005000Training episodes−4.0−3.5−3.0−2.5−2.0−1.5Total rewards22.533.52.02.53.03.53enalties0.00.51.01.52.02.5Total bids0100020003000400050007raining episodes−7−6−5−4−3−27otal rewards0.060.080.100.120.050.060.070.080.090.100.110.12Workloads−0.50.00.51.01.52.02.53.07oWal bids010002000300040005000Training episodes−6.0−5.5−5.0−4.5−4.0−3.5−3.0−2.5−2.0−1.5Total rewards0.150.20.30.40.100.150.200.250.300.350.40DeDdlines0.00.51.01.52.02.53.03.5TotDl bidsTable 3: Bidding policies for vehicular applications with ﬁxed environment.

(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)(cid:96)

Application

Variable

One

Two

Three

Four

Best episode
Best average total reward
Total bid (in $)
Equivalent bidding rate (in $/second)
Optimal bidding rate (in $/second)
Assigned workload (in million instructions)
Completion time (in seconds)

2200
-0.96
0.72
1.80
1.83
0.02
0.05

4700
-2.00
0.00
0
0
0.00
NA

700
-3.94
3.39
11.30
11.00
0.10
0.15

4000
-4.75
4.36
8.72
7.92
0.13
0.20

[20] M. Ono. Closed-loop chance-constrained mpc with probabilistic resolv-
In 2012 IEEE 51st IEEE Conference on Decision and Control

ability.
(CDC), pages 2611–2618, Dec 2012.

[21] E. Ozatay, S. Onori, J. Wollaeger, U. Ozguner, G. Rizzoni, D. Filev,
J. Michelini, and S. Di Cairano. Cloud-based velocity proﬁle optimiza-
tion for everyday driving: A dynamic-programming-based solution. IEEE
Transactions on Intelligent Transportation Systems, 15(6):2491–2505,
Dec 2014.

[22] Alexander T. Schwarm and Michael Nikolaou. Chance-constrained model

predictive control. AIChE Journal, 45(8):1743–1752, 1999.

[23] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wier-
stra, and Martin Riedmiller. Deterministic policy gradient algorithms.
In Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st Inter-
national Conference on Machine Learning (ICML-14), pages 387–395.
JMLR Workshop and Conference Proceedings, 2014.

[24] M. Sookhak and H. Yu, F.and Tang. Secure data sharing for vehicular
In Ad Hoc Networks, pages

ad-hoc networks using cloud computing.
306–315. Springer, 2017.

[25] R.S. Sutton and A.G. Barto. Reinforcement learning: an introduction.

Neural Networks, IEEE Transactions on, 9(5):1054–1054, 1998.

[26] R.S. Sutton, D.A. McAllester, S.P. Singh, Y. Mansour, et al. Policy gradi-
ent methods for reinforcement learning with function approximation. In
NIPS, volume 99, pages 1057–1063, 1999.

[27] C. Szepesv´ari. Algorithms for reinforcement learning. Synthesis Lectures
on Artiﬁcial Intelligence and Machine Learning, 4(1):1–103, 2010.
[28] G.E. Uhlenbeck and L.S. Ornstein. On the theory of the brownian motion.

Physical review, 36(5):823, 1930.

[29] M. Whaiduzzaman, M. Sookhak, A. Gani, and R. Buyya. A survey on
vehicular cloud computing. Journal of Network and Computer Applica-
tions, 40:325–344, 2014.

[30] G. Yan, D. Wen, S. Olariu, and M.C. Weigle. Security challenges in vehic-
ular cloud computing. IEEE Transactions on Intelligent Transportation
Systems, 14(1):284–294, 2013.

[31] K. Zheng, H. Meng, P. Chatzimisios, L. Lei, and X. Shen. An SMDP-
based resource allocation in vehicular cloud computing systems. IEEE
Transactions on Industrial Electronics, 62(12):7920–7928, 2015.
[32] Z. Zhou and N. Bambos. A general model for resource allocation in utility
computing. In 2015 American Control Conference (ACC), pages 1746–
1751, July 2015.

[2] D. Ardagna, B. Panicucci, and M. Passacantando. A game theoretic for-
mulation of the service provisioning problem in cloud systems. In Pro-
ceedings of the 20th international conference on World wide web, pages
177–186. ACM, 2011.

[3] D. Ardagna, B. Panicucci, and M. Passacantando. Generalized nash equi-
libria for the service provisioning problem in cloud systems. IEEE Trans-
actions on Services Computing, 6(4):429–442, Oct 2013.

[4] N. Bajcinca. Wireless cars: A cyber-physical approach to vehicle dynam-

ics control. Mechatronics, 30:261–274, 2015.

[5] M. Barshan, H. Moens, and F. De Turck. Design and evaluation of a scal-
able hierarchical application component placement algorithm for cloud
In 10th International Conference on Network and
resource allocation.
Service Management (CNSM) and Workshop, pages 175–180, Nov 2014.
[6] R. Bellman. A markovian decision process. Technical report, DTIC Doc-

ument, 1957.

[7] M. Dolgov, G. Kurz, and U. D. Hanebeck. Chance-constrained model
predictive control based on box approximations. In 2015 54th IEEE Con-
ference on Decision and Control (CDC), pages 7189–7194, Dec 2015.
[8] D. Filev, J. Lu, and D. Hrovat. Future mobility: Integrating vehicle control
with cloud computing. Mechanical Engineering, 135(3):S18, 2013.
[9] S. Ioﬀe and C. Szegedy. Batch normalization: Accelerating deep
arXiv preprint

network training by reducing internal covariate shift.
arXiv:1502.03167, 2015.

[10] R. Johari, S. Mannor, and J. N. Tsitsiklis. Eﬃciency loss in a network
resource allocation game: the case of elastic supply. IEEE Transactions
on Automatic Control, 50(11):1712–1724, Nov 2005.
[11] V.R. Konda and J.N. Tsitsiklis. Actor-critic algorithms.

In NIPS, vol-

ume 13, pages 1008–1014, 1999.

[12] Y. Li, X. Tang, and W. Cai. Dynamic bin packing for on-demand cloud
resource allocation. IEEE Transactions on Parallel and Distributed Sys-
tems, 27(1):157–170, Jan 2016.

[13] Z. Li. Developments in Estimation and Control for Cloud-Enabled Auto-

motive Vehicles. PhD thesis, The University of Michigan, 2016.

[14] Z. Li, I. Kolmanovsky, E. Atkins, J. Lu, D. Filev, and J. Michelini. Cloud
aided semi-active suspension control. In Computational Intelligence in
Vehicles and Transportation Systems (CIVTS), 2014 IEEE Symposium on,
pages 76–83, Dec 2014.

[15] Z. Li, I. V. Kolmanovsky, E. M. Atkins, J. Lu, D. P. Filev, and Y. Bai.
Road disturbance estimation and cloud-aided comfort-based route plan-
ning. IEEE Transactions on Cybernetics, PP(99):1–13, 2016.

[16] H. Liang, L. Cai, D. Huang, X. Shen, and D. Peng. An SMDP-based ser-
vice model for interdomain resource allocation in mobile cloud networks.
IEEE Transactions on Vehicular Technology, 61(5):2222–2232, 2012.
[17] T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and
D. Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.

[18] I. Menache, A. Ozdaglar, and N. Shimkin. Socially optimal pricing of
cloud computing resources. In Proceedings of the 5th International ICST
Conference on Performance Evaluation Methodologies and Tools, pages
322–331. ICST (Institute for Computer Sciences, Social-Informatics and
Telecommunications Engineering), 2011.

[19] V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare,
A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529–
533, 2015.

10

