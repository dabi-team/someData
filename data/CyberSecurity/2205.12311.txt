Fast & Furious: Modelling Malware
Detection as Evolving Data Streams

Fabr´ıcio Ceschina (fjoceschin@inf.ufpr.br), Marcus Botacina

(mfbotacin@inf.ufpr.br), Heitor Murilo Gomesb (heitor.gomes@vuw.ac.nz),

Felipe Pinag´ea (fapinage@inf.ufpr.br), Luiz S. Oliveiraa

(lesoliveira@inf.ufpr.br), Andr´e Gr´egioa (gregio@inf.ufpr.br)

a Department of Informatics, Federal University of Paran´a – Curitiba, Brazil

b School of Engineering and Computer Science, Victoria University of Wellington –

Wellington, New Zealand

Corresponding Author:

Fabr´ıcio Ceschin

Rua Cel. Francisco Her´aclito dos Santos, 100 – Curitiba, Paran´a, Brazil

Tel: +55 (41) 99658-8299

Email: fjoceschin@inf.ufpr.br

2
2
0
2

g
u
A
6
1

]

R
C
.
s
c
[

3
v
1
1
3
2
1
.
5
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Fast & Furious: On the Modelling of Malware
Detection as an Evolving Data Stream

Fabr´ıcio Ceschina, Marcus Botacina, Heitor Murilo Gomesb, Felipe Pinag´ea,
Luiz S. Oliveiraa, Andr´e Gr´egioa

aFederal University of Paran´a – Cel. Francisco Her´aclito dos Santos, 100 – Curitiba,
Paran´a, Brazil
bVictoria University of Wellington – Cotton Building, Victoria University of Wellington,
Kelburn, Wellington 6012, New Zealand

Abstract

Malware is a major threat to computer systems and imposes many challenges

to cyber security. Targeted threats, such as ransomware, cause millions of dol-

lars in losses every year. The constant increase of malware infections has been

motivating popular antiviruses (AVs) to develop dedicated detection strategies,

which include meticulously crafted machine learning (ML) pipelines. However,

malware developers unceasingly change their samples’ features to bypass detec-

tion. This constant evolution of malware samples causes changes to the data

distribution (i.e., concept drifts) that directly aﬀect ML model detection rates,

something not considered in the majority of the literature work. In this work,

we evaluate the impact of concept drift on malware classiﬁers for two Android

datasets: DREBIN (≈130K apps) and a subset of AndroZoo (≈285K apps). We

used these datasets to train an Adaptive Random Forest (ARF) classiﬁer, as well

as a Stochastic Gradient Descent (SGD) classiﬁer. We also ordered all datasets

samples using their VirusTotal submission timestamp and then extracted fea-

tures from their textual attributes using two algorithms (Word2Vec and TF-

IDF). Then, we conducted experiments comparing both feature extractors, clas-

∗Corresponding author.
Email addresses: fjoceschin@inf.ufpr.br (Fabr´ıcio Ceschin), mfbotacin@inf.ufpr.br
(Marcus Botacin), heitor.gomes@vuw.ac.nz (Heitor Murilo Gomes), fapinage@inf.ufpr.br
(Felipe Pinag´e), lesoliveira@inf.ufpr.br (Luiz S. Oliveira), gregio@inf.ufpr.br (Andr´e
Gr´egio)

Preprint submitted to Expert Systems with Applications

August 17, 2022

siﬁers, as well as four drift detectors (Drift Detection Method, Early Drift De-

tection Method, ADaptive WINdowing, and Kolmogorov-Smirnov WINdowing)

to determine the best approach for real environments. Finally, we compare

some possible approaches to mitigate concept drift and propose a novel data

stream pipeline that updates both the classiﬁer and the feature extractor. To

do so, we conducted a longitudinal evaluation by (i) classifying malware sam-

ples collected over nine years (2009-2018), (ii) reviewing concept drift detection

algorithms to attest its pervasiveness, (iii) comparing distinct ML approaches

to mitigate the issue, and (iv) proposing an ML data stream pipeline that out-

performed literature approaches, achieving an improvement of 22.05 percentage

points of F1Score in the DREBIN dataset, and 8.77 in the AndroZoo dataset.

Keywords: Machine Learning, Data Streams, Concept Drift, Malware

Detection, Android

1. Introduction

Countering malware is a major concern for most networked systems, since

they can cause billions of dollars in loss (SecurityVentures, 2018). The growth of

malware infections (CTONetworks, 2017) enables the development of multiple

detection strategies, including machine learning (ML) classiﬁers tailored for mal-

ware, which have been adopted by the most popular AntiViruses (AVs) (Gan-

dotra et al., 2014; Kantchelian et al., 2013; Jordaney et al., 2017). However,

malware samples are very dynamic pieces of code – usually distributed over the

Web (Chang et al., 2013) and constantly evolving to survive – quickly turning

AVs into outdated mechanisms that present lower detection rates over time.

This phenomenon is known as concept drift (Gama et al., 2014), and requires

AVs to periodically update their ML classiﬁers (Ceschin et al., 2018). Malware

concept drift is an understudied problem in the literature, with the few works

that address it usually focusing on achieving high accuracy rates for a tempo-

rally localized dataset, instead of aiming for long-term detection due to malware

evolution. Moreover, the community considers ML a powerful ally for malware

2

detection, given its ability to respond faster to new threats (Gibert et al., 2020).

Being the most used operating system worldwide, Android has more than

2 billion monthly active devices (Fergus Halliday, 2018), with almost 40% of

prevalence in the operating system market, surpassing Microsoft Windows in

2018 (StatCounter, 2018). As a widespread platform, Android is more aﬀected

by malware evolution and distribution, rendering its AVs vulnerable to concept

drift eﬀects, which makes the need to adapt existing solutions.

In this work, we evaluate the impact of concept drift on malware classi-

ﬁers for two Android datasets: DREBIN (Arp et al., 2014) (≈130K apps) and a

subset of AndroZoo (Allix et al., 2016) (≈285K apps). For our longitudinal eval-

uation, we collected malware samples over nine years (2009-2018) and used them

to train an Adaptive Random Forest (ARF) classiﬁer (Gomes et al., 2017), as

well as a Stochastic Gradient Descent (SGD) classiﬁer (Pedregosa et al., 2011).

Our goal is to answer the following questions: (i) is concept drift a generalized

phenomenon and not an issue of a particular dataset? (ii) is it important to

update the feature extractor (and not only the classiﬁer) when a concept drift is

detected? (iii) how can we consider the feature extractor in the malware detec-

tion pipeline? (iv) which drift detector is the best for this task? (v) is concept

drift somehow related to changes in the Android ecosystem? To do so, we or-

dered all datasets samples using their VirusTotal (VirusTotal, 2018) submission

timestamp and then extracted features from their textual attributes using two

algorithms (Word2Vec (Mikolov et al., 2013) and TF-IDF (Salton et al., 1975)).

After that, we conducted experiments comparing both feature extractors, classi-

ﬁers, as well as four drift detectors (Drift Detection Method (Gama et al., 2014),

Early Drift Detection Method (Baena-Gar´cıa et al., 2006), ADaptive WINdow-

ing (Bifet & Gavald`a, 2007), and Kolmogorov-Smirnov WINdowing (Raab et al.,

2020)) to determine the best approach for real environments. We also compare

some possible approaches to mitigate concept drift and propose a novel method

based on the data stream pipeline that outperformed current solutions by up-

dating both the classiﬁer and the feature extractor. We highlight the need for

also updating the feature extractor, given that it is as essential as the classi-

3

ﬁer itself to achieve increased detection rates due to new features appearing

over time. Therefore, we propose a realistic data stream learning pipeline, in-

cluding the feature extractor in the loop. We also show with our experiments

that concept drift is a generalized phenomenon in Android malware. Finally,

we discuss the impact of changes on the Android ecosystem in our classiﬁers

by comparing feature changes detected over time, and the implications of our

ﬁndings.

It is worth notice that all code and datasets used will be publicly

available1, as well as the implementation of our data stream learning pipeline

using scikit-multiflow (Montiel et al., 2018)2, an open-source ML framework

for stream data.

This article is organized as follows: we compare our work with the litera-

ture in Section 2; we introduce our methodology in Section 3; we describe the

machine learning background in Section 4; we present the experiments results

in Section 5; we discuss our ﬁndings in Section 6, and draw our conclusions in

Section 7.

2. Related Work

The literature on malware detection using machine learning is extensive (Gan-

dotra et al., 2014). Usually, the primary concern of most works is to achieve

100% of accuracy using diﬀerent representations and models, ignoring the fact

that malware samples evolve as time goes by. Few papers consider concept drift

in this context, such as Masud et al., which were (at the best of our knowledge)

the ﬁrst to treat malware detection as a data stream classiﬁcation problem like

us, but proposing an ensemble of classiﬁers trained from consecutive chunks of

data using v-fold partitioning of them and reducing classiﬁcation error com-

pared to other ensembles (Masud et al., 2008). They also presented a feature

extraction and selection technique for data streams that do not have any ﬁxed

feature set (the opposite of our approach) based on information gain. Bach et al.

1https://www.kaggle.com/datasets/fabriciojoc/fast-furious-malware-data-stream
2https://github.com/fabriciojoc/scikit-multiflow

4

combined two models: a stable one, based on all data, and a reactive one, based

on a short window of recent data, to determine when to replace the current

stable model by computing the diﬀerence in their accuracy, assuming that the

stable one performs worse than the reactive when the concept changes (Bach

& Maloof, 2008), very similar to what we do when the drift detector reaches a

warning level.

Singh et al. proposed two measures to track concept drift in static features

of malware families (which we do by using concept drift detectors, but in a mal-

ware detection task): relative temporal similarity (based on the similarity score

between two time-ordered pairs of samples and are used to infer the direction of

concept drift) and meta-features (based on summarization of information from

a large number of features) (Singh et al., 2012), claiming to provide paths to

further exploration of drift in malware detection models. Narayanan et al. pre-

sented an online machine learning based framework, named DroidOL to handle

concept drift and detect malware (Narayanan et al., 2016) using control-ﬂow

sub-graph features in an online classiﬁer, which adapts to the malware drift by

updating the model more aggressively when the error is large and less aggres-

sively, otherwise, which can also be done by drift detectors. Deo et al. proposed

the use of Venn-Abers predictors to measure the quality of classiﬁcation tasks

and identify concept drift (Deo et al., 2016).

Jordaney et al. presented Transcend, a framework to identify concept drift in

classiﬁcation models which compares the samples used to train the models with

those seen during deployment, computing algorithm credibility and conﬁdence

to measure the quality of the produced results and detect concept drift (Jor-

daney et al., 2017). Anderson et al. have shown that, by using reinforcement

learning to generate adversarial samples, it is possible to retrain a model and

make these attacks less eﬀective, also protecting it against possible drifts, given

that this technique hardens a model against worst-case inputs (Anderson et al.,

2018), something that can be improved even more by retraining the feature

extractor as we do. Pendlebury et al. reported that some results are inﬂated

by spatial bias, caused by the wrong distribution of training and testing sets,

5

and temporal bias, caused by incorrect time splits into these same sets (Pendle-

bury et al., 2018), which is one of the reasons why we collected the timestamps

in our datasets and used data streams. They introduced an evaluation frame-

work called Tesseract to compare malware classiﬁers in a realistic setting and

conﬁrmed that earlier published results are biased. We reinforce that in our

ﬁndings by comparing diﬀerent experiments with a real-world simulation us-

ing data streams. Ceschin et al. compared a set of experiments that use batch

machine-learning models with ones that take into account the change of concept

(data streams), emphasizing the need to update the decision model immediately

after a concept drift occurs (Ceschin et al., 2018). In contrast, we do not update

only the decision model, but also the feature extractor when drift occurs. The

authors also show that the malware concept drift is related to their concept

evolution due to the appearance of new malware families.

Mariconti et al. created MaMaDroid, an Android malware detection sys-

tem that uses a behavioral model, in the form of a Markov chain, to extract

features from the API calls performed by an app (Onwuzurike et al., 2019). The

solution proposed was tested with a dataset containing 44K samples, collected

over six years, and presented a good detection rate and the ability to keep its

performance for long periods (at least ﬁve years according to their experiments).

We increased the length of the observation window by using malware samples

collected over nine years. Cai et al. compared their approach (Cai, 2018; Cai &

Jenkins, 2018), which uses 52 selected metrics concerning sensitive data accesses

via APIs (using dynamic analysis) as features, with MaMaDroid. According

to their experiments, their approach managed to remain stable for ﬁve years,

while MaMaDroid only kept the same performance for two years. A very

similar approach was compared with other four methods in literature and all of

them presented an overall f1score bellow 72% (Fu & Cai, 2019).

Xu et al. proposed DroidEvolver, an Android malware detection system that

can be automatically updated without any human involvement, requiring nei-

ther retraining nor true labels to update itself (Xu et al., 2019). The authors use

online learning techniques with evolving feature sets and pseudo labels, keeping

6

a pool of diﬀerent detection models and calculating a juvenilization indicator

that determines when to update its feature set and each detection model. We

compared our approach with DroidEvolver and showed that it outperformed it.

Finally, Gibert et al. presented research challenges of state-of-the-art techniques

for malware classiﬁcation, exemplifying the concept drift as one of them (Gibert

et al., 2020).

Zhang et al. designed APIGraph, a framework to detect evolved Android

malware that groups similar API calls into clusters based on the Android API

oﬃcial documentation (Zhang et al., 2020). Applying the aforementioned tech-

nique to other Android malware classiﬁers, the authors reduced the labeling

eﬀorts when combined with Tesseract (Pendlebury et al., 2018).

Finally, diﬀerent from other approaches listed, Massimo Ficco presented an

ensemble detector that exploits diversity in the detection algorithms by using

both generic and specialized detectors (trained to detect certain malware types).

The author also presents a mechanism that explores how the length of the

observation time window can aﬀect the detection accuracy and speed of diﬀerent

combinations of detectors during the detection (Ficco, 2022).

Our main contribution in this work is to apply data stream based machine

learning algorithms to malware classiﬁcation, proposing an important improve-

ment in the pipeline that makes feature vectors correspond to the actual concept.

The proposed solution, to the best of our knowledge, was not considered before

and is as important as updating the classiﬁer itself. To test and validate our

proposal, we use two datasets containing almost 415K android apps, showing

that it outperforms traditional data stream solutions. We also include an anal-

ysis of how certain features change over time, correlating it with a cybersecurity

background.

7

3. Methodology

3.1. Threat Model and Assumptions

Our threat model considers an antivirus engine (AV) for the Android plat-

form since it is the market share leader. Thus, successful malware infections

aﬀect a large number of users. For scientiﬁc purposes, we considered an AV en-

tirely based on Machine Learning (ML) as implemented by many of the malware

detectors cited in Section 2. In practice, however, it does not imply that an AV

must use only this detection method: it can be complemented with any other

detection approach envisioned by the AV vendor. Our detection model is com-

pletely static (features are retrieved directly from the APK ﬁles) because static

malware detection is the most popular and fastest way to triage malware sam-

ples.Similarly to the above discussion, the use of static detectors do not imply

that an AV should not use dynamic components, but that our research focuses

on improving the static detection component. In our proposed AV model, the

APK ﬁles are inspected as soon as they are placed in the Android ﬁlesystem,

i.e., it does not rely on any other system information except the APK ﬁles them-

selves. It is worth emphasizing that our goal is not to implement an actual AV

but to highlight the need for updating ML models based on classiﬁers. There-

fore, we simulated the behavior of an online AV using oﬄine experiments that

use data streams, simplifying our solution implementation. The details of the

simulated ML model are presented below.

3.2. Data Stream

Since our goal is to evaluate the occurrence of concept drift in malware clas-

siﬁers, we analyzed malware detection evolution over time using a data stream

abstraction for the input data. In a traditional data stream learning problem

that includes concept drift, the classiﬁer is updated with new samples when

a change occurs—usually the ones that caused the drift (Gama et al., 2014).

Our data stream pipeline also considers the feature extractor under changes,

according to the following ﬁve steps shown in Figure 1:

8

Figure 1: Data Stream Pipeline. Every time a new sample is obtained from the data

stream, its features are extracted and presented to a classiﬁer, generating a prediction, which

is used by a drift detector that deﬁnes the next step: update the classiﬁer or retrain both

classiﬁer and feature extractor.

1. Obtain a new sample X from the raw data stream;

2. Extract features from X using a feature extractor E, trained with previous

data;

3. Predict the class of the new sample X using the classiﬁer C, trained with

previous data;

4. With the prediction from C, update the drift detector D to check the drift

level (deﬁned by authors (Montiel et al., 2018));

5. According to the drift level, three paths can be followed, all of them mak-

ing the pipeline restarts at Step 1:

a Normal: incrementally update C with X;

b Warning: incrementally update C with X and add X to a buﬀer;

9

PredictionDriftWarningNormalBuﬀerRaw Data StreamFeature ExtractorClassiﬁer(2) Extract Features(1) Get Next Sample(3) Predict Class(5c) Retrain(4) Update Drift Detector(5a/b) Updatec Drift: retrain both E and C using only the data collected during

the warning level (from the buﬀer build during this level), creating a

new extractor and classiﬁer.

3.3. Datasets

A challenge to detect concept drift in malware classiﬁers is to properly iden-

tify the sample’s date to allow temporal evaluations. Since malware samples

are collected in the wild and they may be spreading for some time, no actual

creation date is available. As an approximation for it, we considered each sam-

ple’s ﬁrst appearance in VirusTotal (VirusTotal, 2018), a website that analyzes

thousands of samples every day. Malware samples were ordered by their “ﬁrst

seen” dates, which allows us to create a data stream representing a real-world

scenario, where new samples are released daily, thus requiring malware classiﬁers

to be updated (Pendlebury et al., 2018).

In our experiments, we considered attributes vectors provided by the au-

thors of DREBIN (Arp et al., 2014), composed of ten textual attributes (API

calls, permissions, URLs, etc), which are publicly available to download and

contain 123, 453 benign and 5, 560 malicious Android applications. We show

DREBIN’s distribution in Figure 2a. We also considered a subset of Android

applications reports provided by AndroZoo API (Allix et al., 2016), composed

of eight textual attributes (resources names, source code classes and methods,

manifest permissions etc.) and contains 213, 928 benign and 70, 340 malicious

applications. We show the distribution of our AndroZoo subset in Figure 2b:

it keeps the same goodware and malware distribution as the original dataset,

which originally is composed by most of 10 million apps.

It is important to notice that we are using the attributes that were already

extracted statically from them (Arp et al., 2014; Allix et al., 2016), since we do

not have access to applications binaries, packages, or source codes. In addition,

the timing information provided by the authors of the DREBIN (Arp et al.,

2014) and VirusTotal diﬀers. According to Arp et al. (Arp et al., 2014), their

samples were collected from August 2010 to October 2012. However, our version

10

(a) DREBIN dataset distribution by

(b) AndroZoo dataset distribution by

month. Very few samples are shown in 2009

month. The majority of samples are in 2016,

(not shown), 2010, 2013 (not shown) and 2014

with fewer malware showing in 2017. Febru-

(not shown). The majority of them are in 2011

ary 2016 and March 2016 are the months with

and 2012.

more prevalence of goodware and malware,

repectively.

Figure 2: Datasets distribution over time.

is based on VirusTotal appearance date, which showed us that very few sam-

ples were already analyzed by VirusTotal (48 in 2009) and some of them were

just analyzed after the establishment of the collection (37 in 2013 and 2014),

probably following the dataset publicly release. We do not show these samples

in Figure 2 for better visualization.

Furthermore, both datasets reﬂect two characteristics of the real world that

challenge the development of eﬃcient ML malware detectors: (i) long-term vari-

ations (2009-2014, for DREBIN and 2016-2018 for AndroZoo); and (ii) class

imbalance. For example, in DREBIN, whereas more than 40K goodware were

collected in Sep/2009,only 1,750 malware samples were collected in the same

period. Whereas class imbalance is out of this work’s scope, we considered both

datasets as suitable for our evaluations due to the long-term characteristic,

which challenges ML classiﬁers to learn multiple concepts over time. Finally,

to evidence the diﬀerence of both datasets, we created heat maps containing

the prevalence of a subset of malware families created using the intersection of

families from them (54 families), as shown in Figures 3a and 3b.

11

JMMJSNJMMJSNJMMJSNMonth100101102103104Number of Samples (log)201020112012DREBIN Samples DistributionGoodwareMalwareJMMJSNJMMJSNJMMJMonth100101102103104105Number of Samples (log)201620172018AndroZoo Samples DistributionGoodwareMalware(a) DREBIN dataset.

(b) AndroZoo dataset.

Figure 3: Malware family distribution. Intersection of families from both datasets shows

evidences of class evolution, given that they are very diﬀerent.

4. Machine Learning Algorithms

4.1. Representation

A typical way to represent malware for ML is to create a vector using the

sample textual attributes, such as API calls, permissions, URLs, providers,

intents activities, service receivers, and others.

In our work, we represented

12

201120122013YearackpostsadrdbasebridgebeanbotbosmcoogoscoshadogowardougalekdroiddreamdroidkungfudroidrooterfakeflashfakengryfakerunfaketimerfujacksgamexgappusinginmastergmusegpspyhamobiconosysimlogjifakejsmshiderkoomerksappmaistealermaniamobilespynandroboxopfakepenethopiratesplanktonrooterrootsmartsaivaseawethsendpaysmforwsmsbombersmsspyspysetstealthcellsteekstinitertapsnaketesbovdloaderyzhczitmoMalware FamilyDREBIN Malware Family DistributionJANMARMAYJULSEPNOVJANMARMAYJULSEPNOVJANMARMAYJULMonth0.00.20.40.60.81.0Prevalence201620172018YearackpostsadrdbasebridgebeanbotbosmcoogoscoshadogowardougalekdroiddreamdroidkungfudroidrooterfakeflashfakengryfakerunfaketimerfujacksgamexgappusinginmastergmusegpspyhamobiconosysimlogjifakejsmshiderkoomerksappmaistealermaniamobilespynandroboxopfakepenethopiratesplanktonrooterrootsmartsaivaseawethsendpaysmforwsmsbombersmsspyspysetstealthcellsteekstinitertapsnaketesbovdloaderyzhczitmoMalware FamilyAndroZoo Malware Family DistributionJANMARMAYJULSEPNOVJANMARMAYJULSEPNOVJANMARMAYJULMonth0.00.20.40.60.81.0Prevalencethem using Word2Vec (Mikolov et al., 2013) and TF-IDF (Salton et al., 1975),

since they are widely used representations for text classiﬁcation. Besides, both

use the training data to generate the representation of all samples, which allows

us to test our hypothesis that the drift aﬀects not only the classiﬁer but also

the feature extractor.

4.2. Concept Drift Detectors

We used four concept drift detection algorithms in our work: DDM (Drift De-

tection Method (Gama et al., 2014)), EDDM (Early Drift Detection Method (Baena-

Gar´cıa et al., 2006)), ADWIN (ADaptive WINdowing (Bifet & Gavald`a, 2007)),

and KSWIN (Kolmogorov-Smirnov WINdowing) (Raab et al., 2020). Both

DDM and EDDM are online supervised methods based on sequential error mon-

itoring, i.e., each incoming example is processed separately estimating the se-

quential error rate. Therefore, they assume that the increase of consecutive

error rate suggests the occurrence of concept drift. DDM directly uses the error

rate, while EDDM uses the distance-error rate, which measures the number of

examples between two errors (Baena-Gar´cıa et al., 2006). These errors trigger

two levels: warning and drift. The warning level suggests that the concept starts

to drift, then an alternative classiﬁer is updated using the samples which rely on

this level. The drift level suggests that the concept drift occurred, and the al-

ternative classiﬁer build during the warning level replaces the current classiﬁer.

ADWIN keeps statistics from sliding windows of variable size, used to compute

the average of the change observed by cutting them in diﬀerent points. If the

diﬀerence between two windows is greater than a pre-deﬁned threshold, a con-

cept drift is detected, and the data from the ﬁrst window is discarded (Bifet &

Gavald`a, 2007). KSWIN uses a sliding window of ﬁxed size to compare the most

recent samples of the window with the remaining ones by using Kolmogorov-

Smirnov (KS) statistical test (Raab et al., 2020). Diﬀerent from the other two

methods, ADWIN and KSWIN has no warning level, which made it necessary

to adapt our data stream ML pipeline. For instance, when a change occurs in

ADWIN, out of the window data is discarded and the remaining samples are

13

used to retrain the both the classiﬁer and feature extractor. Finally, when a

change occurs in KSWIN, only the most recent samples of the window are kept

and used to retrain the classiﬁer and feature extractor.

4.3. Classiﬁers

In all evaluations, we developed classiﬁcation models leveraging Word2Vec

and TF-IDF representations and normalized using a MinMax technique (Pe-

dregosa et al., 2011). For all cases, we used as classiﬁers Adaptive Random

Forest (Gomes et al., 2017) without its internal drift detectors (working as a

Random Forest for data streams), since its widespread use in the malware detec-

tion literature and has the best overall performance (Ceschin et al., 2018), and

Stochastic Gradient Descent (SGD) classiﬁer (Pedregosa et al., 2011), which

is one of the fastest online classiﬁers in scikit-learn (Pedregosa et al., 2011).

Both the classiﬁer and drift detectors were conﬁgured using the same hyper-

parameters proposed by the authors (Gomes et al., 2017; Montiel et al., 2018).

5. Experiments

5.1. The Best-Case Scenario for AVs (ML Cross-Validation)

In the ﬁrst experiment, we classify all samples together to compare which

feature extraction algorithm is the best and report baseline results. We tested

several parameters for both algorithms and ﬁxed the vocabulary size in 100 for

TF-IDF (top-100 features ordered by term frequency) and created projections

with 100 dimensions for Word2Vec, resulting in 1, 000 and 800 features for each

app in both cases, for DREBIN and AndroZoo, respectively. All results are

reported after 10-fold cross-validation procedures, a method commonly used in

ML to evaluate models because its results are less prone to biases (note that

we are training new classiﬁers and feature extractors at every iteration of the

cross-validation process). In practice, folding the dataset implies that the AV

company has a mixed view of both past and future threats, despite temporal

eﬀects, which is the best scenario for AV operation and ML evaluation.

14

Classiﬁer Algorithm

DREBIN Dataset

AndroZoo Dataset

Accuracy F1Score Recall Precision Accuracy F1Score Recall Precision

Random

Word2Vec

99.09%

88.73%

82.12%

96.48%

90.52%

76.27%

66.03%

90.29%

Forest

TF-IDF

99.23%

90.63% 85.85%

96.31%

91.54%

79.30% 70.25% 91.03%

SGD

Word2Vec

98.29%

78.28%

70.90%

87.36%

85.41%

60.05%

47.52%

81.54%

TF-IDF

98.63%

83.26% 78.49% 88.66%

88.74%

71.57% 61.43% 85.73%

Table 1: Cross-Validation. Mixing past and future threats is the best scenario for AVs in

both datasets.

Table 1 presents the results obtained in this experiment for both DREBIN

and AndroZoo datasets using Adaptive Random Forest (ARF) and Stochastic

Gradient Descent (SGD), highlighting the performance of TF-IDF, which was

better than Word2Vec in all metrics, except in precision when classifying the

DREBIN dataset. It means that Word2Vec is slightly better to detect goodware

(particularly in DREBIN dataset) since its precision is higher (i.e., fewer FPs)

and TF-IDF is better to detect malware due to its higher recall (i.e., less FNs).

In general, we conclude that TF-IDF is better than Word2Vec since its accuracy

and f1score are higher. Moreover, we notice that its f1score is not as high as the

accuracy, which is near 100%, indicating that one of the classes (malware) is

more diﬃcult to predict than the other (goodware). Regardless of small diﬀer-

ences, we observe that ML classiﬁers perform signiﬁcantly well when samples of

all periods are mixed, even in a more complex dataset such as AndroZoo, since

they can learn features from all periods.

5.2. On Classiﬁcation Failure (Temporal Classiﬁcation)

Although currently used classiﬁcation methodology helps reducing dataset

biases, it would demand knowledge about future threats to work properly. AV

companies train their classiﬁers using data from past samples and leverage them

to predict future threats, expecting to present the same characteristics as past

ones. However, malware samples are very dynamic, thus this strategy is the

worst-case scenario for AV companies. To demonstrate the eﬀects of predicting

future threats based on past data, we split our datasets in two: we used the ﬁrst

15

Classiﬁer Algorithm

DREBIN Dataset

AndroZoo Dataset

Accuracy F1Score Recall Precision Accuracy F1Score Recall Precision

Random

Word2Vec

97.66%

62.58%

46.31%

96.47%

87.55%

53.95%

38.71%

88.96%

Forest

TF-IDF

98.20%

73.26% 58.36% 98.39%

88.20%

57.13% 41.71% 90.63%

SGD

Word2Vec

97.52%

65.14%

55.08%

79.70%

85.81%

47.04%

33.44%

79.28%

TF-IDF

98.15%

75.18% 66.42% 86.61%

86.96%

52.79% 38.68% 83.07%

Table 2: Temporal Evaluation. Predicting future threats based on data from the past is

the worst-case for AVs.

half (oldest samples) to train our classiﬁers, which were then used to predict

the newest samples from the second half. The results of Table 2 indicate a drop

in all metrics when compared to the 10-fold experiment in both DREBIN and

AndroZoo datasets and also sugest the occurrence of concept drift on malware

samples, given that the recall is much smaller than the previous experiment.

Due to dataset imbalance in both datasets, we notice a bias toward goodware

detection (very small accuracy decrease) and signiﬁcant qualitative diﬀerences

for f1score and recall (much worse than before). The precision score was very

similar to the cross-validation experiment, and better in the case of TF-IDF

when classifying DREBIN, showing that goodware samples remain similar in

the “future” while malware samples evolved. Word2Vec and TF-IDF lost, in

average, about 16 percentage points of their f1score in DREBIN and about 19

percentage points for this same metric in AndroZoo. In addition, the model’s

recall rates signiﬁcantly dropped in both Word2Vec and TF-IDF when classi-

fying DREBIN and AndroZoo, regardless of the models used. This indicates

that detecting unforeseen malware only from a single set of past data is a hard

task. These results highlight the need of developing better continuous learning

approaches for eﬀective malware detection.

5.3. Real-World Scenario (Windowed Classiﬁer)

Since static classiﬁers are a bad strategy, AV companies adopt continuous

updating procedures as samples are collected and identiﬁed. From a ML per-

spective, they adopt an incremental stream learning method (Pinage et al.,

16

2016), which we call Incremental Windowed Classiﬁer (IWC). Notice that this

is the same approach used by other authors in the literature, but they pro-

pose the use of distinct attributes and features only, instead of a new stream

pipeline (Zhang et al., 2020; Cai, 2020). To evaluate the impact of this approach,

we divided the datasets into two groups, one containing samples released until

a certain month of a year for training, and the other with samples released one

month after that said month for testing. For example, considering Jan/2012, the

training set contained samples that were created until Jan/2012 (cumulative)

and the validation set contained samples created only in Feb/2012 (a month

later). We tested both Adaptive Random Forest (ARF) (Gomes et al., 2017)

and Stochastic Gradient Descent (SGD) classiﬁer (Pedregosa et al., 2011) with

TF-IDF (100 features for each textual attribute), given its better performance

in previous experiments, and excluded months with no samples. Every month,

both classiﬁer and feature extractor were retrained, generating new classiﬁers

and feature extractors.

The results for the DREBIN dataset shown in Figures 4a and 4c indicate

a drop in both precision and recall rates. For Adaptive Random Forest, preci-

sion drops to about 85% in April/2012 and increases in the following months to

almost 100% in August/2013. The Recall starts with the worst result (about

40%), in January/2012 and reaches almost 100% in August/2013. The average

recall was 70.3% and average precision, 95.36%. For SGD, the worst preci-

sion is reported in October/2012 (about 30%), increasing to almost 100% in

August/2013. The same happen with the recall, which drops to almost 20%

in October/2012 and increases to almost 100% in August/2013. Finally, the

average recall was 66.78% and average precision, 85.22%. On the one hand,

the growth of precision and recall in some periods indicate that continuously

updating the classiﬁer might increase classiﬁcation performance in comparison

to using a single classiﬁer tested with samples from a past period because the

classiﬁer learns new patterns for the existing concepts (features) that may have

been introduced over time. On the other hand, the drop in the precision and

recall rates in some periods indicate the presence of new concepts (e.g., new

17

(a) Adaptive Random Forest and DREBIN

(b) Adaptive Random Forest and AndroZoo

dataset.

dataset.

(c) SGD and DREBIN dataset.

(d) SGD and AndroZoo dataset.

Figure 4: Continuous Learning. Recall and precision while incrementally retraining both

classiﬁer (Adaptive Random Forest and Stochastic Gradient Descent classiﬁer) and feature

extractor.

malware features) that were not fully handled by the classiﬁers. When looking

at the AndroZoo dataset results, as shown in Figures 4b and 4d, it is possible to

note a drastic fall in recall as time goes by in both cases, dropping from almost

70% (ARF) and 60% (SGD) in January/2017 to less than 5% in March/2019,

not exceeding 10% by the end of the stream. This indicates that AndroZoo is

a much more complex dataset with very diﬀerent malware in distinct periods,

given that the recall did not behave as this same experiment using the DREBIN

dataset. In contrast, the precision remained unstable. For Adaptive Random

Forest, it reaches 100% in some periods, such as May/2017, and drops to almost

50%, in June/2017, with quite similar behavior from the DREBIN dataset. For

SGD, it also reaches 100% in some periods, such as April/2017, and drops to

18

12345678910...8Month0.20.30.40.50.60.70.80.91.0Recall X Precision20122013ARF & DREBIN Incremental Windowed Classifier UpdateRecallPrecision12345689121234567Month0.00.20.40.60.81.0Recall X Precision20172018RecallPrecisionARF & AndroZoo Incremental Windowed Classifier Update12345678910...8Month0.20.30.40.50.60.70.80.91.0Recall X PrecisionRecallPrecisionSGD & DREBIN Incremental Windowed Classifier Update20122013123458912134567Month0.00.20.40.60.81.0Recall X Precision20172018RecallPrecisionSGD & AndroZoo Incremental Windowed Classifier Updatealmost 30% in April/2018. These results indicate that more than continuous

retraining, AV companies need to develop classiﬁers fully able to learn new

concepts.

5.4. Concept Drift Detection using Data Stream Pipeline (Fast & Furious –

F&F)

Although the previous results indicate that continuously learning from the

past is the best strategy for detecting forthcoming threats, concept drift remains

challenging, even more when considering results from the AndroZoo dataset.

Therefore, we present an evaluation of a drift detector approach that could be

deployed by AV companies to automatically update their classiﬁer models. In

our experiments, we used both Adaptive Random Forest (Gomes et al., 2017)

and Stochastic Gradient Descent as the classiﬁers, DDM, EDDM, ADWIN, and

KSWIN as drift detectors (Montiel et al., 2018) (with the same parameters as

the authors), and TF-IDF (using 100 features for each textual attribute) as

a feature extractor. In the DREBIN dataset, we initialized the base classiﬁer

with data from the ﬁrst year, given that in the ﬁrst months we have just a few

malware showing up. In the AndroZoo dataset, we initialized it with data from

the ﬁrst month. We tested all drift detection algorithms in two circumstances

when creating a new classiﬁer, according to our data stream pipeline: (i) Up-

date: we just update the classiﬁer with new samples (collected in warning level

or ADWIN window), which reﬂects the fastest approach for AV companies to

react to new threats; (ii) Retrain: we extract all features from the raw data

(also collected in warning level or ADWIN window) and all models are built

from scratch again, i.e., both classiﬁer and feature extractor are retrained and a

new vocabulary is built based on the words in the training set for each textual

attribute (more complete approach, but time-consuming for AV companies).

To compare our solution with another similar in the literature, we implemented

our version of DroidEvolver (Xu et al., 2019), replicating their approach using

the same feature representation as ours. It is important to report here that we

tried to use the authors’ source code, but they were not working properly due

19

to dependencies that could not be installed. Thus, we implemented an approx-

imation of their method by analyzing their paper and code, using τ0 = 0.3 and

τ1 = 0.7 with three compatible online learning methods from scikit-learn (Pe-

dregosa et al., 2011): SGDClassifier, PassiveAggressiveClassifier, and

Perceptron. Finally, during the execution of DroidEvolver, we noticed that it

was detecting at least one drift in one of its classiﬁers each iteration, making it

necessary to create a new parameter that checks for drift in an interval of steps

(iterations), in our case 500 steps was selected according to our analysis.

Table 3 presents the results for DroidEvolver and our methods for both

datasets. For DREBIN, when using Adaptive Random Forest, EDDM outper-

forms DDM methods’ classiﬁcation performance in all scenarios, which was the

opposite when using SGD classiﬁer. For both classiﬁers, ADWIN outperforms

EDDM, DDM, and KSWIN, providing the best overall performance for retrain-

ing as well as for learning new features. However, only when using Adaptive

Random Forest, ADWIN with the update is better for precision, making it

slightly better in reducing false positives, and KSWIN is better for recall, mak-

ing it better in reducing false negatives. Moreover, retraining both the feature

extractor and classiﬁer makes it detect fewer drift points than the updating ap-

proach (18 vs. 20 when using ARF, 13 vs 14 when using SGD). Overall, SGD

outperformed Adaptive Random Forest when classifying DREBIN, presenting

an improvement in f1score of almost 16 percentage points. In comparison to

DroidEvolver, our approach outperformed it in all metrics, with a relatively

large margin.

In Figure 5, it is possible to observe the prequential error of

Adaptive Random Forest with ADWIN using Update (Figure 5a) and Retrain

(Figure 5b) strategies. Despite the Retrain strategy is similar to the update

approach in some points, it has less drift points, a lower prequential error and,

consequently, a better classiﬁcation performance.

When classifying AndroZoo, DDM outperforms EDDM, which was not a

good drift detector for this speciﬁc dataset. ADWIN with retraining was the best

method again, detecting 50 and 33 drift points versus 78 and 30 when using the

update approach with Adaptive Random Forest and SGD classiﬁer, respectively,

20

Classiﬁer

Method

Model

DroidEvolver

Pool

(Xu et al., 2019)

DREBIN Dataset

AndroZoo Dataset

Accuracy F1Score Recall Precision Drifts Accuracy F1Score Recall Precision Drifts

97.27%

67.14%

59.14%

77.64%

69

87.09%

66.28%

56.17%

80.83%

22

80%

70.3%

95.36%

N/A

82.99%

11.4%

79.61%

N/A

IWC

DDM (U)

DDM (R)

96.8%

98.3%

98.4%

79.19%

68.38%

94.04%

80.54%

70.27%

94.32%

Adaptive

EDDM (U)

98.53%

82.27%

71.84%

96.26%

Random

EDDM (R)

98.57%

82.85%

73.09%

95.6%

Forest

ADWIN (U)

98.58%

82.66%

71.35%

98.21%

ADWIN (R)

98.71%

84.44% 74.17%

98.02%

KSWIN (U)

98.38%

80.82%

72.19%

91.80%

KSWIN (R)

98.55%

83.01% 74.96%

93.00%

8.25%

63.5%

88.19%

70.84%

80.11%

87.96%

69.92%

61.94%

80.27%

78.28%

39.52%

37.23%

42.11%

77.91%

39.31%

37.52%

41.28%

86.41%

65.86%

58.02%

76.15%

89.6%

75.05% 69.23%

81.93%

89.22%

71.78%

60.66%

87.88%

89.66%

73.22%

62.56%

88.26%

14

24

118

17

78

50

70

50

8

9

27

14

20

18

10

9

IWC

96.33%

73.40%

66.78%

85.22%

N/A

82.63%

9.16%

6.61%

75.71%

N/A

DDM (U)

98.84%

87.56%

86.29%

88.88%

DDM (R)

98.85%

87.67%

86.69%

88.66%

EDDM (U)

98.85%

87.63%

86.43%

88.87%

SGD

EDDM (R)

98.78%

87.05%

86.61%

87.49%

ADWIN (U)

98.95%

88.68%

87.03%

90.38%

ADWIN (R)

99.00%

89.19% 87.38% 91.08%

KSWIN (U)

98.93%

88.45%

86.83%

90.13%

KSWIN (R)

98.85%

87.71%

87.28%

88.15%

3

3

11

20

14

13

1

2

88.05%

71.17%

65.30%

78.20%

89.10%

73.81%

67.98%

80.73%

82.99%

42.50%

32.97%

59.75%

82.77%

41.80%

32.45%

58.72%

89.39%

73.98%

66.74%

82.97%

89.49%

74.31% 67.25%

83.01%

88.23%

72.41% 68.34%

77.00%

85.34%

67.05%

65.99%

68.14%

7

4

85

73

30

33

48

70

Table 3: Overall results. Considering IWC, DroidEvolver (Xu et al., 2019), F&F (U)pdate

and (R)etrain strategies with multiple drift detectors and classiﬁers (Random Forest and

SGD).

outperforming all other methods, including DroidEvolver again. Finally, these

results suggest that AV companies should keep investigating speciﬁc samples to

increase overall detection.

5.5. Multiple Time Spans

In the previous experiment, we showed that the use of drift detectors with

the Retrain strategy improved the classiﬁcation performance. Although the

experiment simulates a real-world stream, there might be potential biases from

the use of the very same training and test sets. By proposing a new experiment,

we mitigated the risk of biasing evaluations, since we have tested our solutions

under diﬀerent circumstances by using multiple time spans and reporting the

average result.

To evaluate the eﬀectiveness of our approach in multiple conditions, we ap-

plied our data stream pipeline to diﬀerent training and test sets of our dataset:

we splitted the two datasets (sorted by the samples’ timestamps) into eleven

21

(a) Adaptive Random Forest with ADWIN

(b) Adaptive Random Forest with ADWIN

and Update.

and Retrain.

Figure 5: F&F (U)pdate and (R)etrain prequential error and drift points as time

goes by when using Adaptive Random Forest. Despite being similar in some points,

fewer drift points are detected when retraining the feature extractor, reducing the prequential

error and increasing classiﬁcation performance.

folds (thus 10 consecutive epochs), with every fold containing the same amount

of data, similar to a k -fold cross-validation scheme. However, instead of using

a single set for training as done in each iteration of k -fold cross-validation, we

increment the training set i with the fold i+1, and remove it from the test set at

every iteration. This way, we create a cumulative training set and simulate the

same scenario as the previous experiment, but starting the stream in diﬀerent

parts. In the end, we produced ten distinct results that present the eﬀectiveness

of our method under varied conditions, which worked as a k -fold cross-validation.

However, we focused on collecting the F1Score of each iteration).

In the current experiment, we used all the methods presented in the pre-

vious section: both classiﬁers (Adaptive Random Forest and SGD), all drift

detectors (DDM, EDDM, ADWIN, and KSWIN), and both methods ((U)pdate

and (R)etrain). To accomplish a better presentation of the results, we chose

a boxplot visualization containing the distribution of all F1Scores (black dots)

and their average for each method (white dots), as shown in Figure 6.

In Figures 6a and 6c, we present the results for the DREBIN dataset using

Adaptive Random Forest and SGD as classiﬁers, respectively. The IWC method

performed much better than the others with Random Forest; in the case of using

22

(a) Adaptive Random Forest and DREBIN

(b) Adaptive Random Forest and AndroZoo

dataset.

dataset.

(c) SGD and DREBIN dataset.

(d) SGD and AndroZoo dataset.

Figure 6: Multiple Time Spans. F1Score distribution when experimenting with ten diﬀer-

ent sets of training and test samples. Black dots represent the distribution of all F1Scores,

whereas white dots represent their average for each applied method.

ten folds to predict the last one, it reaches a better performance than the other

methods, which indicates that the last fold may not be aﬀected by concept drift.

However, when we look at the other methods, KSWIN with retrain performed

best, achieving a higher average F1Score and a low standard deviation. Also,

as we saw in the previous section, SGD performed better than the Adaptive

Random Forest applied to this dataset.

In Figures 6b and 6d, we present the results for the AndroZoo dataset using

Adaptive Random Forest and SGD as classiﬁers, respectively. In this scenario,

IWC performance is much worse than any other method, which we believe is

evidenced due to the complexity of this dataset (almost 285K samples). Again,

in both classiﬁers, KSWIN with retrain method performed best, achieving the

higher average F1Score, despite Adaptive Random Forest presenting better over-

23

62646668707274767880828486889092F1Score (%)IWCDDM (U)DDM (R)EDDM (U)EDDM (R)ADWIN (U)ADWIN (R)KSWIN (U)KSWIN (R)DREBIN - Multiple Time Spans - Random Forest5456586062646668707274767880828486F1Score (%)IWCDDM (U)DDM (R)EDDM (U)EDDM (R)ADWIN (U)ADWIN (R)KSWIN (U)KSWIN (R)AndroZoo - Multiple Time Spans - Random Forest62646668707274767880828486889092F1Score (%)IWCDDM (U)DDM (R)EDDM (U)EDDM (R)ADWIN (U)ADWIN (R)KSWIN (U)KSWIN (R)DREBIN - Multiple Time Spans - SGD38414447505356596265687174778083F1Score (%)IWCDDM (U)DDM (R)EDDM (U)EDDM (R)ADWIN (U)ADWIN (R)KSWIN (U)KSWIN (R)AndroZoo - Multiple Time Spans - SGDall results and being almost 4 percentage points higher than SGD.

Finally, the analysis of the results allows us to observe that (i) the more

data we have in the data stream, the most diﬃcult it becomes for IWC to keep

a good performance; (ii) using KSWIN drift detector with retrain is the most

recommended method for static android malware detection data streams; and

(iii) we ensure that our results do not have potential biases.

5.6. Understanding Malware Evolution

After we conﬁrmed that concept drift is prevalent in these malware datasets,

we delved into evolution details to understand the reasons behind it and the

lessons that mining a malware dataset might teach us. To do so, we analyzed

vocabulary changes over time for both datasets and correlated our ﬁndings of

them.

DREBIN Dataset Evolution. Figure 7a shows API calls’ vocabulary change for

the ﬁrst six detected drift points that actually presented changes (green and

red words mean introduced and removed from the vocabulary, respectively).

We identiﬁed periodic trends and system evolution as the two main reasons

for the constant change of malware samples. The ﬁrst occurs because mal-

ware creators compromise systems according to the available infection vectors

(e.g., periodic events exploitable via social engineering). Also, attackers usu-

ally shift their operations to distinct targets when their infection strategies

become so popular that AVs detect them. Therefore, some features periodi-

cally enter and leave the vocabulary (e.g., setcontentview). The second oc-

curs due to changes in the Android platform, causing reactions from malware

creators to handle the new scenario, either by supporting newly introduced

APIs (e.g., getauthtoken (Android, 2018a)), or by unsupporting deprecated

and modiﬁed APIs (e.g., deleted keyguard (Android, 2018b) or the deprecated

fbreader (FBReader, 2018) intent). Handling platform evolution is required to

keep malware samples working on newer systems, whereas ensuring malicious-

ness. In this sense, the removal of a feature like DELETE PACKAGE permission

from the vocabulary can be explained by the fact that Android changed per-

24

(a) Vocabulary change timeline for API calls using DREBIN dataset.

(b) Vocabulary change timeline for manifest and resources using AndroZoo dataset.

Figure 7: Vocabulary changes for both datasets. Many signiﬁcant features are removed

and added as time goes by.

mission’s transparency behavior to an explicit conﬁrmation request (Android,

2016), which makes malware abusing those features less eﬀective. The most

noticeable case of malware evolution involves SMS sending, a feature known to

be abused by plenty of samples (Sarma et al., 2012; Hamandi et al., 2012; Luo

et al., 2013). We identiﬁed that APIs were periodically added and removed

from the vocabulary (e.g., sendmultiparttextmessage, part of the Android

SMS subsystem (Android, 2018c), had its use restricted by Android over time,

until being ﬁnally blocked (Cimpanu, 2018)). This will certainly cause modiﬁ-

cations in newer malware and probably incur in classiﬁers’ drifting once again.

Therefore, considering concept drift is essential to deﬁne any realistic threat

model and malware detector.

AndroZoo Dataset Evolution. We conducted the same experiments performed

on the Drebin dataset on the AndroZoo dataset to evaluate dataset inﬂuence

on concept drift occurrence, as shown in Figure 7b. We highlight that de-

veloping a malware classiﬁer for the AndroZoo dataset is a distinct challenge

than developing a malware classiﬁer for the Drebin dataset because the Andro-

Zoo dataset presents more malware families (1.166 distinct families according

25

02/10/201202/14/201202/15/201203/21/201205/03/201207/02/201209/02/2012browsercontactspeoplesendmultiparttextmessagewiﬁlockaudiomanagersendmultiparttextmessagesetcontentviewgetsubscriberidsendmultiparttextmessagesendmultiparttextmessagepeoplegetauthtokeninvalidateauthtokensendmultiparttextmessagecontactssetcontentviewdisablekeyguard+-+-+-+-+-+-AddedFeaturesRemovedFeaturesto Euphony (Hurier et al., 2017) labels) than the Drebin dataset (178 distinct

families according to Euphony labels). The diﬀerence in the dataset complexity

is reﬂected in the number of drift points identiﬁed for each experiment. Whereas

we identiﬁed 18 drift points for the Drebin dataset, the AndroZoo dataset pre-

sented 50 drift points. Despite the diﬀerence in the number of drifting points,

the reasons behind such drifts have been revealed very similar when we delve

into vocabulary change details. When we look to the manifest action vo-

cabulary, we notice that the sms received feature leaves and returns to the

vocabulary many times. This behavior reﬂects the arms race between Google

and attackers for SMS sending permission (Cimpanu, 2018). This same occur-

rence has been observed in the DREBIN dataset. Similarly, when we look to the

manifest category vocabulary, we notice that the facebook feature also leaves

and returns to the vocabulary many times. This happens because the attackers

often rely on Facebook name for distributing “Trojanized” applications that re-

semble that original Facebook app. Although Google often removes these apps

from the oﬃcial store, attackers periodically come up with new ways of bypass-

ing AppStore’s veriﬁcation routines, thus resulting in the presented infection

waves regarding the Facebook name. Finally, this same behavior is observed

in the resource entry vocabulary regarding the amazonaws feature. Attackers

often store their malicious payloads on popular cloud servers to avoid detec-

tion (Rossow et al., 2013). Even though Amazon sinkhole malicious domains

when detected, attackers periodically ﬁnd new ways to survive security scans,

thus also resulting in the observed vocabulary waves.

The results of our experiments show that despite presenting distinct com-

plexities, both datasets have undergone drift occurrences. This shows that

concept drift is not a dataset-exclusive issue, but a characteristic inherently

associated with the malware classiﬁcation problem. Therefore, overall malware

classiﬁcation research work (and primarily the ones leveraging these two pop-

ular datasets) should consider concept drift occurrence in their evaluations to

gather more accurate samples information.

26

6. Discussion

We here discuss our ﬁndings and their implications.

What the metrics say. While the accuracy of distinct classiﬁers is very

similar in some experiments, we highlight that malware detectors must be eval-

uated using the correct metrics, as any binary classiﬁcation. Therefore, we rely

on f1score, recall, and precision to gain further knowledge on malware detec-

tion. AV companies should adopt the same reasoning when evaluating their

classiﬁers. Experiments 5.2 and 5.3 indicate Android malware evolution, which

imposes diﬃculties for established classiﬁers and corroborates the fact that sam-

ples changed. However, goodware generally kept the same concept over time,

suggesting that creating benign proﬁles may be better than modeling everchang-

ing malicious activities.

Feature drift, concept drift and evolution. Using a ﬁxed ML model is

not enough to predict future threats. In experiment 5.2, we notice a signiﬁcant

drop in f1score and recall (a metric that indicates the ability to correctly classify

malware). Hence, a ﬁxed ML model is prone to misclassify novel threats. Be-

sides, continuous learning helps increasing detection but is still subject to drift,

as shown by AndroZoo in experiment 5.3, when recall remains below 10% for a

long period. If we compare experiments 5.3 and 5.4 when classifying DREBIN,

it is possible to observe that the Incremental Windowed Classiﬁer update still

outperforms DDM with the update when using Adaptive Random Forest, but is

not better than DDM with retraining (except in precision). However, when com-

paring it with EDDM and ADWIN, we notice that both are signiﬁcantly better,

and it is still highly prone to drift (Figure 4). In response to these results, con-

cept drift detectors help to improve classiﬁcation performance. When comparing

DDM with retraining and EDDM and ADWIN (with both update and retrain)

to IWC (Table 3), we can see that the former outperformed the latter, advo-

cating the need of using drift detectors. This is evident when using SGD, once

IWC was outperformced by all other methods, and is even more evident when

classifying AndroZoo, given that IWC lost the ability to detect new malware as

27

time goes by. In this case, ADWIN with retraining was able to outperform all

the other methods again, even a closely related work (DroidEvolver (Xu et al.,

2019)), giving us insight that this is a valid method to be used in practice. In

addition, even more important than just using a drift detector, reconsidering

the classiﬁer’s feature set is required to completely overcome concept drift. Ac-

cording to experiment 5.4, we can infer that retraining the classiﬁer and feature

extraction models every time a drift occurs (using the data stream pipeline we

proposed) is better than just updating only the classiﬁer. This implies that not

only the representation of malware becomes obsolete as time goes by, but also

the vocabulary used to build them. It means that every textual attribute used

by applications can change, i.e., new API calls, permissions, URLs, for example,

emerge, requiring a vocabulary update, i.e., it indicates that discovering new

speciﬁc features might help in increasing detection rates. Thus, we conclude

that malware detection is not just a concept drift problem, but also in essence

a feature drift detection problem (BAR, 2017).

Our solution in practice. To implement our solution in practice, we need to

model the installation, update, and removal of applications as a stream. This

can be done by considering the initial set of applications installed in a stock

phone as ground truth and thus subsequent deployment of applications as the

stream. To reduce the delay between the identiﬁcation of a drift point and the

classiﬁer update, ideally, multiple classiﬁers (the current one and the candidates

to replace it) should be running in parallel. However, this parallel execution

is too much expensive to be performed on endpoints. Therefore, we consider

that the best usage scenario for our approach is its deployment on the App’s

Stores distributing the applications. Therefore, each time a new application

is submitted to an App Store, it is veriﬁed according to our cycle. To cover

threats distributed by alternative markets, this concept can be extended to any

centralized processing entity. For instance, an AV can upload the suspicious

application to a cloud server and perform its checks according to our cycle. To

speed up the upload process, the AV can make the current feature extractor

available to the endpoint so as the endpoint does not need to upload the entire

28

application but only its feature. In this scenario, the AV update is not composed

of new signatures, but of new feature extractors.

Drift and evolution are common problems in malware detection. De-

spite being more complex, (more apps and malware families) all characteristics

present in DREBIN are drastically shown in AndroZoo, evidencing that feature

drift, concept drift, and evolution are present in malware detection problems

in practice and not only in a single dataset. This suggests that AV companies

should keep enhancing their models and knowledge database constantly.

Limitations and future work. One of the limitations of our approach is that

it relies on ground truth labels that may be available with a certain delay, given

that known goodware and malware are needed in order to train and update

the classiﬁer (as any AV). Thus, future researches to reduce these delays are

an important step to improve any ML solution that does not rely on their own

labels, such as DroidEvolver (Xu et al., 2019) (outperformed by our approach).

It is important to note that our work considers only the attributes vectors

provided by DREBIN (Arp et al., 2014) and AndroZoo (Allix et al., 2016)

datasets’ authors. Due to this fact, we were unable to consider other types of

attributes, such as API semantics (Zhang et al., 2020) or behavioral proﬁling

(dynamic analysis) (Cai, 2020). Therefore, we cannot directly compare our work

with theirs, due to diﬀerent threat models. To compare diﬀerent attributes

using our data stream pipeline under the same circumstances, it is necessary to

download all the APKs from both datasets. This is left as future work.

Finally, we make our data stream learning pipeline available as an exten-

sion of scikit-multiflow (Montiel et al., 2018), aiming at encouraging other

researchers to contribute with new strategies that address feature and concept

drift.

7. Conclusion

In this article, we evaluated the impact of concept drift on malware classi-

ﬁers for Android malware samples to understand how fast classiﬁers expire. We

29

analyzed ≈415K sample Android apps from two datasets (DREBIN and Andro-

Zoo) collected over nine years (2009-2018) using two representations (Word2Vec

and TF-IDF), two classiﬁers (Adaptive Random Forest and Stochastic Gradi-

ent Descent classiﬁer) and four drift detectors (DDM, EDDM, ADWIN, and

KSWIN). Our results show that resetting the classiﬁer only after changes are

detected is better than periodically resetting it based on a ﬁxed window length.

We also point the need to update the feature extractor (besides the classiﬁer)

to achieve increased detection rates, due to new features that may appear over

time. This strategy was the best in all scenarios presented, even using a complex

dataset, such as AndroZoo. The implementation is available as an extension to

scikit-multiflow (Montiel et al., 2018)3. Our results highlight the need for

developing new strategies to update the classiﬁers inherent to AVs.

References

(2017). A survey on feature drift adaptation: Deﬁnition, benchmark, challenges

and future directions. Journal of Systems and Software, .

Allix, K., Bissyand´e, T. F., Klein, J., & Le Traon, Y. (2016). Androzoo: Col-

lecting millions of android apps for the research community. In Int. Conf.

on Min. Soft. Rep..

Anderson, H. S., Kharkar, A., Filar, B., Evans, D., & Roth, P. (2018). Learn-

ing to evade static pe machine learning malware models via reinforcement

learning. arXiv:1801.08917.

Android (2016). Android 7.0 behavior changes.

https://tinyurl.com/

yxlpc4gb.

Android (2018a). Accountmanager. https://tinyurl.com/ybsdz76e.

Android (2018b). Device admin deprecation. https://tinyurl.com/yygfkc3m.

3https://github.com/fabriciojoc/scikit-multiflow

30

Android (2018c). Smsmanager. https://tinyurl.com/y3sz4zzw.

Arp, D., Spreitzenbarth, M., Hubner, M., Gascon, H., & Rieck, K. (2014).

Drebin: Eﬀective and explainable detection of android malware in your

pocket. In NDSS .

Bach, S. H., & Maloof, M. A. (2008). Paired learners for concept drift. In IEEE

Int. Conf. on Data Mining.

Baena-Gar´cıa, M., del Campo- ´Avila, J., Fidalgo, R., Bifet, A., Gavald`a, R., &

Morales-Bueno, R. (2006). Early drift detection method.

Bifet, A., & Gavald`a, R. (2007). Learning from time-changing data with adap-

tive windowing. In SIAM Int. Conf. on Data Mining.

Cai, H. (2018). A preliminary study on the sustainability of android malware

detection. arXiv:1807.08221.

Cai, H. (2020). Assessing and improving malware detection sustainability

through app evolution studies. ACM Trans. Softw. Eng. Methodol., 29 .

URL: https://doi.org/10.1145/3371924. doi:10.1145/3371924.

Cai, H., & Jenkins, J. (2018). Towards sustainable android malware de-

tection.

In Proceedings of the 40th International Conference on Soft-

ware Engineering: Companion Proceeedings ICSE ’18 (p. 350–351). New

York, NY, USA: Association for Computing Machinery. URL: https:

//doi.org/10.1145/3183440.3195004. doi:10.1145/3183440.3195004.

Ceschin, F., Pinage, F., Castilho, M., Menotti, D., Oliveira, L. S., & Gregio,

A. (2018). The need for speed: An analysis of brazilian malware classifers.

IEEE Sec.&Priv., .

Chang, J., Venkatasubramanian, K. K., West, A. G., & Lee, I. (2013).

Analyzing and defending against web-based malware. ACM Comput.

Surv., 45 . URL: https://doi.org/10.1145/2501654.2501663. doi:10.

1145/2501654.2501663.

31

Cimpanu, C. (2018). Google restricts which android apps can request call log

and sms permissions. https://tinyurl.com/y3y5gfhx.

CTONetworks (2017). Malware infections grow 4x in just one quarter. https:

//tinyurl.com/yyyugke7.

Deo, A., Dash, S. K., Suarez-Tangil, G., Vovk, V., & Cavallaro, L. (2016). Pre-

science: Probabilistic guidance on the retraining conundrum for malware

detection. In Proceedings of the ACM Workshop on Artiﬁcial Intelligence

and Security.

FBReader (2018). for old android devices. https://tinyurl.com/y2odlk9w.

Fergus Halliday (2018). What are the most common kinds of Android malware?

https://tinyurl.com/wcmr9m2.

Ficco, M. (2022). Malware analysis by combining multiple detectors and ob-

servation windows. IEEE Trans. Comput., 71 , 1276–1290. URL: https:

//doi.org/10.1109/TC.2021.3082002. doi:10.1109/TC.2021.3082002.

Fu, X., & Cai, H. (2019). On the deterioration of learning-based malware de-

tectors for android.

In 2019 IEEE/ACM 41st International Conference

on Software Engineering: Companion Proceedings (ICSE-Companion) (pp.

272–273). doi:10.1109/ICSE-Companion.2019.00110.

Gama, J. a., ˇZliobait˙e, I., Bifet, A., Pechenizkiy, M., & Bouchachia, A. (2014).

A survey on concept drift adaptation. ACM Comput. Surv., .

Gandotra, E., Bansal, D., & Sofat, S. (2014). Malware analysis and classiﬁca-

tion: A survey. Journal of Information Security, .

Gibert, D., Mateu, C., & Planes, J. (2020). The rise of machine learning for

detection and classiﬁcation of malware: Research developments, trends and

challenges. Journal of Network and Computer Applications, .

32

Gomes, H. M., Bifet, A., Read, J., Barddal, J. P., Enembreck, F., Pfharinger,

B., Holmes, G., & Abdessalem, T. (2017). Adaptive random forests for

evolving data stream classiﬁcation. Machine Learning, .

Hamandi, K., Elhajj, I. H., Chehab, A., & Kayssi, A. (2012). Android sms

botnet: A new perspective. In Int. Symp. on Mobility Management and

Wireless Access.

Hurier, M., Suarez-Tangil, G., Dash, S. K., Bissyand´e, T. F., Le Traon, Y.,

Klein, J., & Cavallaro, L. (2017). Euphony: Harmonious uniﬁcation of

cacophonous anti-virus vendor labels for android malware. In Int. Conf. on

Min. Soft. Rep..

Jordaney, R., Sharad, K., Dash, S. K., Wang, Z., Papini, D., Nouretdinov, I.,

& Cavallaro, L. (2017). Transcend: Detecting concept drift in malware

classiﬁcation models. In USENIX Security Symposium.

Kantchelian, A., Afroz, S., Huang, L., Islam, A. C., Miller, B., Tschantz, M. C.,

Greenstadt, R., Joseph, A. D., & Tygar, J. D. (2013). Approaches to

adversarial drift. In ACM Workshop on A.I. and Security.

Luo, W., Xu, S., & Jiang, X. (2013). Real-time detection and prevention of

android sms permission abuses. In Int. W. on Sec. in Embedded Sys. and

Smartphones.

Masud, M. M., Al-Khateeb, T. M., Hamlen, K. W., Gao, J., Khan, L., Han, J.,

& Thuraisingham, B. (2008). Cloud-based malware detection for evolving

data streams. ACM Trans. Manage. Inf. Syst., .

Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). Dis-

tributed representations of words and phrases and their compositionality.

CoRR, . arXiv:1310.4546.

Montiel, J., Read, J., Bifet, A., & Abdessalem, T. (2018). Scikit-multiﬂow: A

multi-output streaming framework. Journal of Machine Learning Research,

.

33

Narayanan, A., Yang, L., Chen, L., & Jinliang, L. (2016). Adaptive and scalable

android malware detection through online learning. In IJCNN .

Onwuzurike, L., Mariconti, E., Andriotis, P., Cristofaro, E. D., Ross, G., &

Stringhini, G. (2019). Mamadroid: Detecting android malware by build-

ing markov chains of behavioral models (extended version). ACM Trans.

Priv. Secur., 22 . URL: https://doi.org/10.1145/3313391. doi:10.

1145/3313391.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,

O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas,

J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay,

E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine

Learning Research, .

Pendlebury, F., Pierazzi, F., Jordaney, R., Kinder, J., & Cavallaro, L. (2018).

TESSERACT: eliminating experimental bias in malware classiﬁcation

across space and time. CoRR, . arXiv:1807.07838.

Pinage, F. A., dos Santos, E. M., & da Gama, J. M. P. (2016). Classiﬁcation

systems in dynamic environments. WIREs: Data Mining and Knowledge

Discovery, .

Raab, C., Heusinger, M., & Schleif, F.-M.

(2020).

Reactive soft

prototype computing for concept drift

streams.

Neurocomputing,

416 , 340–351. URL: http://dx.doi.org/10.1016/j.neucom.2019.11.

111. doi:10.1016/j.neucom.2019.11.111.

Rossow, C., Dietrich, C., & Bos, H. (2013). Large-scale analysis of malware

downloaders. In U. Flegel, E. Markatos, & W. Robertson (Eds.), Detection

of Intrusions and Malware, and Vulnerability Assessment. Berlin, Heidel-

berg.

Salton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic

indexing. Commun. ACM , .

34

Sarma, B. P., Li, N., Gates, C., Potharaju, R., Nita-Rotaru, C., & Molloy, I.

(2012). Android permissions: A perspective combining risks and beneﬁts.

In Symp. on Access Control Models and Technologies.

SecurityVentures (2018). Global ransomware damage costs predicted to exceed

$8 billion in 2018. https://tinyurl.com/y499nvsh.

Singh, A., Walenstein, A., & Lakhotia, A. (2012). Tracking concept drift in

malware families. In Proceedings of the ACM Workshop on Security and

Artiﬁcial Intelligence.

StatCounter (2018). Operating System Market.

https://tinyurl.com/

tykvtqm.

VirusTotal (2018). Online malware and url scanner. https://virustotal.

com/.

Xu, K., Li, Y., Deng, R., Chen, K., & Xu, J. (2019). Droidevolver: Self-evolving

android malware detection system. In 2019 IEEE EuroS&P .

Zhang, X., Zhang, Y., Zhong, M., Ding, D., Cao, Y., Zhang, Y., Zhang, M.,

& Yang, M. (2020). Enhancing state-of-the-art classiﬁers with api se-

mantics to detect evolved android malware.

In Proceedings of the 2020

ACM SIGSAC Conference on Computer and Communications Security

CCS ’20 (p. 757–770). New York, NY, USA: Association for Com-

puting Machinery. URL: https://doi.org/10.1145/3372297.3417291.

doi:10.1145/3372297.3417291.

35

