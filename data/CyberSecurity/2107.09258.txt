A Markov Game Model for AI-based Cyber
Security Attack Mitigation

Hooman Alavizadeh1, Julian Jang-Jaccard1, Tansu Alpcan2 and Seyit A. Camtepe3
1 School of Natural and Computational Sciences,
Massey University, Auckland, New Zealand
2 Department of Electrical and Electronic Engineering,
University of Melbourne, Parkville, Australia
3 CSIRO Data61, Australia
Email: {h.alavizadeh, j.jang-jaccard}@massey.ac.nz

1
2
0
2

l
u
J

0
2

]
T
G
.
s
c
[

1
v
8
5
2
9
0
.
7
0
1
2
:
v
i
X
r
a

Abstract—The new generation of cyber threats leverages ad-
vanced AI-aided methods, which make them capable to launch
multi-stage, dynamic, and effective attacks. Current cyber-
defense systems encounter various challenges to defend against
such new and emerging threats. Modeling AI-aided threats
through game theory models can help the defender to select
optimal strategies against the attacks and make wise decisions
to mitigate the attack’s impact. This paper ﬁrst explores the
current state-of-the-art in the new generation of threats in
which AI techniques such as deep neural network is used for
the attacker and discusses further challenges. We propose a
Markovian dynamic game that can evaluate the efﬁciency of
defensive methods against the AI-aided attacker under a cloud-
based system in which the attacker utilizes an AI technique to
launch an advanced attack by ﬁnding the shortest attack path.
We use the CVSS metrics to quantify the values of this zero-sum
game model for decision-making.

Index Terms—Markovian Game; AI-based threats; Cloud

computing; Game theory; Attack models

I. INTRODUCTION

A strong cyber defense system should be able to defend
against
the new generation of cyber threats [1], [2]. AI-
powered threats are emerging attacks that use AI capabilities to
launch various types of attacks to the system such as targeted
attacks, adaptive attacks [3], DeepLocker [4], and so forth [5].
Although the traditional adversarial attacks were relatively
easier to detect and defend as the attack patterns were algorith-
mic, the emerging attacks leverage AI features such as machine
learning (ML) and deep learning to make malware much more
evasive and pervasive. In addition, the AI techniques combined
with automation can make the malware more scalable and
make them able to attack the targets without human inter-
vention [5].

Game theory is a mathematical model based on a decision-
responsive manner that makes a variation on strategies for
each player according to the decision or movements of other
players. In fact, game theory can be deﬁned as studying
the cooperation and conﬂict between rational and intelligent
decision-makers based on the principles of using mathematical
models. Game Theory models have been extensively studied
for their use in cyber security and have shown to be very
effective in the evaluation of defensive systems and addressing
the security of networks and systems [6], [7], [8].

In this paper, we consider an advanced attacker which is able
to utilize AI techniques such as deep neural network (DNN) to
ﬁnd their target in a networked system fast and effectively. We
assume that the attacker can leverage deep learning techniques
to estimate the shortest attack path in a networked model (such
as those proposed in [9], [10], [11]). This enables the attackers
to be resilient against dynamic defense techniques [12], [13].
Moreover, we show how game theory can be leveraged to
evaluate the dynamic defensive scenarios for avoiding these
kinds of AI-powered attacks.

Concretely, we address the speciﬁc problem in which the
AI-aided attacker can ﬁnd the shortest attack path in a cloud.
The cloud system and attack model are modeled as a zero-
sum Markov Game. The cloud model is based on an Attack
Graph (AG) representing the states of the game. We deﬁne a
zero-sum Markovian game model that captures the capabilities
of the attacker and defender. In this case, the attacker can
choose the actions that exploit the real-world vulnerabilities
reported in the Common Vulnerabilities and Exploits (CVEs)
through National Vulnerability Database (NVD). Then, the
defender’s actions are modeled based on a dynamic defense
consisting of the placement of detection systems (such as IDS)
to the hosts in the cloud. We design the rewards of this game
by leveraging the Common Vulnerability Scoring Systems
(CVSS) values such as attack impact and exploitability. This
helps the defender to select appropriate strategies using the
limited number of monitoring actions for each state of the
game. The defender action is the placement of an IDS in a
host in the cloud to monitor and detect any prospective threats.
However, the defense action may incur some performance
degradation and cost to the defender.

In this paper, we utilized a Markovian game model
to
analyze AI-aided attacks for a cloud model which can help
the defender to mitigate attacker rewards using a dynamic IDS
placement strategy. The main contributions of this paper are
as follows:

• We provide an up-to-date review on the new generation
of threats such as AI-powered threats and categorize
them based on two main threat types which are AI-aided
attacks and AI-embedded attacks.

• We discuss the AI-aided attack approaches such as those

 
 
 
 
 
 
using graph-based AI techniques to ﬁnd the targets in
a networked system effectively. We investigate in depth
a threat model which is able to utilize Deep Neural
Network (DNN) to ﬁnd the shortest attack path in a cloud
to launch the attack efﬁciently.

• We propose a Markovian game model that can evaluate
the effectiveness of the state-of-art defense mechanisms
against the AI-aided attacks in which the attacker utilizes
an AI technique such as DNN to estimate the shortest
attack path in a cloud system. We quantify the game
parameters based on a zero-sum game and CVSS values.
• We offer the formal mathematical deﬁnitions for the
proposed game model. We also determine the proba-
bilistic values of the states of the game. Finally, we
clearly formalize, analyze, and quantify actions and states
transition probabilities for the game model.

The rest of the paper is organized as follows. Section
II discusses some essential concepts which are related to
this paper including AI-powered threats categorization. The
related work is given in Section III. In Section V, we deﬁne
the necessary concepts, deﬁnitions, mathematical notations,
and propose our Markovian game model. Discussion and
limitations of the current study are presented in Section VI.
Finally, we conclude the paper in Section VII.

II. AI-POWERED THREATS

In these days, attackers use innovative and smart methods
for launching various types of attacks. It includes delivering
malicious activities, exploiting zero-day vulnerabilities, and
use of deep neural networks to ﬁnd the targets effectively.
Thus, the cyber defensive system should be able to deal with
a wide range of more intelligent, persistent, and sophisticated
attacks equipped with more advanced technologies such as
AI-powered attacks. In [14], the authors investigated the AI-
powered cyber attacks and mapped them onto a proposed
framework with new threats including the classiﬁcation of
several aspects of threats that use AI during the cyber-attack
life cycle. We categorize the AI-powered attacks based on AI-
aided and AI-embedded attacks as shown in Figure 1. AI-
aided attacks are of those who leverage AI to launch the
attacks effectively. In this type, the intelligent attackers use
AI techniques. However, in AI-embedded attack, the threats
are weaponized by AI themselves such as Deep locker [4].

In AI-aided attacks, the attackers use AI to launch various
attacks based on different targets such as adaptive attacks
or multi-stage attacks which need knowledgeable attackers
equipped with prior knowledge of the target system. Moreover,
adaptive attacks are known as intelligent attacks. In these
types of attacks, the attacks can be adapted to the dynamic
environment that changes the system’s conditions. These types
of attackers are intelligent enough so that they can wisely
manage their resource limitations while they opportunistically
try to compromise the entire system such as executing adaptive
attacks [15]. The attacker could launch various AI-based tech-
niques to detect and recognize the target network, vulnerabili-
ties, and valuable targets [14]. Moreover, the new type of AI-

Fig. 1: AI-powered threats categorization.

aided attackers are able to use high intelligence and sufﬁcient
resources to launch increasingly more intelligent, sophisticated
and persistent attacks. This enables the attackers to ﬁnd
valuable targets in the network, ﬁnd attack path efﬁciently, and
obtain highly sensitive information [15], [16]. In particular,
attackers are intelligent enough to (i) optimize the attack; for
instance, they can estimate the most efﬁcient attack path in
a networked system with lower cost and higher beneﬁts [17],
(ii) execute multi-stage attacks including reconnaissance phase
by scanning the system prior to real attacks exploiting the
system’s components (such as virtual machines). They are
able to continue to the attack by exploiting other system’s
components after they break into the system [3].

In contrast,

the AI capabilities are embedded into the
malware or threat itself in the AI-embedded attacks. Such as
targeted attacks and Deep Locker [4] in which the current
defensive technologies cannot easily detect and defend against
those types of attacks. Deeplocker uses neural network tech-
nologies and includes three layers of concealment which make
the Deeplocker evasive to detect. In the ﬁrst layer, the target is
concealed (who and what the target is). The second layer is the
target instance concealment which hides the speciﬁcs. Finally,
the third layer conceals the malicious intent which encrypts the
payload and hides the attack technique [4]. Another example
of AI-embedded attacks is the new generation of botnet attacks
which are very difﬁcult to be detected by the current decisive
strategies [5]. They utilized deep neural networks such as
convolutional neural networks (CNN) to establish a covert
channel between bots and botmasters.

However, in this paper, we only consider AI-aided attacks
in which the attacker can use a deep neural network to ﬁnd
out the shortest attack path in a targeted network.

III. RELATED WORK

A. Graph-based AI-aided Threats

The application of Artiﬁcial Intelligence (AI) such as Ge-
netic Algorithms and machine learning in the graph theory-

TABLE I: Comparison of Graph-based techniques which can be leveraged by attackers to ﬁnd attack path such as shortest
attack path in the network

Approach

Technique

Advantages

Disadvantages

Practicality

Classic search
strategies

BFS/DFS

Fast for small size

Dijkstra

Traditional AI-based A∗

–Fast for small size
–Consider priority

Good performance

Learning-based AI

Machine Learning

Neural Network

Deep Learning

Scalable
–Scalable
–Adaptive
–Scalable
–Adaptive

Evolutionary
Computation (EC)

Genetic Algorithm
(GA)

Fast approach

–Not scalable
–Time complexity
–Exhaustive search
–Not scalable
– Resource consumption
–Needs heuristic
–Real implementation
Need Training

Need Training

Need Training

–Low scalability
– not optimal

High

High

Low

Medium

Medium

Resilience to
dynamic defense

Verylow

Low

Low

High

High

Ref.

[18]

[19]

[20]

[21]

[21]

Medium

Very High

[9], [10]

Low

Medium

[18], [22]

based problems such as ﬁnding the fastest, shortest, or cheap-
est path in a network has been studied in various research, the
application of Genetic Algorithm [18], Neural Network [21],
Deep Learning [9], [10] and so on. However, the application of
graph theory-based and learning-based approaches for ﬁnding
the shortest and most suitable paths in a network can be lever-
aged by attackers to launch their attacks effectively. Finding
the attack path from source to target in a graphical attack
model is an important capability for the attackers [23], [24].
This enables the attacker to reach the target with minimum
effort and cost.

1) Classical Search Strategies: The classic algorithms for
calculating shortest paths are Breadth-ﬁrst search (BFS), Di-
jkstra’s, and Bellman-Ford. Dijkstra’s is the most used case
of ﬁnding the shortest path between two speciﬁc nodes with
no available path cost heuristic. The earliest work on neural-
based solutions to the shortest path was motivated by com-
munications and packet routing, where approximate methods
faster than the classical algorithms were desired. These operate
quite differently from today’s neural networks,
they used
iterative back-propagation to solve the shortest path on a
speciﬁc graph [21]. Groundbreaking is another approach to
the problem was DeepMind’s Differentiable neural computers,
which computed shortest paths across the large graph of
geographical places. The method is by taking the graph as
a sequence of connection tuples and learning a step-by-step
algorithm using a read-write memory. It was provided with
a learning curriculum, gradually increasing the graph size.
However, the traditional methods have scalability problem for
larger graphs. For instance, an efﬁcient implementations of
Dijkstra can compute the shortest paths for a node to others
in O(nlogn + m) time in a graph with n nodes and m edges.
2) Traditional AI-based Strategies: The A∗ search algo-
rithm uses heuristic-based techniques to compute the shortest
path (it also is known as light generalization of Dijkstra algo-
rithm). Practically, the A∗ algorithm as quickly as Dijkstra’s
those methods suffer from high time
algorithm. However,
complexity and difﬁculties in ﬁnding the appropriate heuristic.
There are many methods to obtain the shortest attack path in a
network such as the classical Dijkstra algorithm and traditional

AI-based such as A∗. However, from the attacker’s point of
view, the shortest attack path has to be computed within a very
short period of time and scalable manner in order to support
time-constrained attacks which can be detected and deceived
by asynchronous defensive mechanisms which change the
attack surface periodically [12]. However, attackers prefer to
use more intelligent approaches to ﬁnd their targets using the
shortest attack path [23]. Many studies investigated the opti-
mized method for ﬁnding the shortest path from the attacker’s
perspective. For instance, in [23], authors proposed an optimal
algorithm for APT attackers which enables the attacker to ﬁnd
the shortest attack path from multiple sources in a graphical
attack model using a Bayesian network. Moreover, in [18], the
authors proposed an approach to ﬁnd the shortest path using a
learning-based Genetic Algorithm (GA) based on principles of
stochastic search and optimization techniques which is a fast
algorithm with minimum failure ratio. However, most of the
classical search and traditional methods are not able to satisfy
the essential requirements for high-speed and fast response in
large-scale applications, adaptation with dynamic graphs, real-
world problems, and so on. Moreover, ﬁnding global optimal
solutions is difﬁcult in these approaches while it is easier to
ﬁnd global solutions in the classical shortest path approaches
and pulse-coupled neural network (PCNN) [25].

3) Learning-based AI approaches: To address the draw-
backs of traditional methods, some neural network architec-
tures have been designed for ﬁnding shortest path. In fact,
neural network can be used to either ﬁnd or estimate the
shortest path on a certain network effectively [25], [11], [10].
For instance, in [11], a dynamic algorithm is developed to
compute shortest path for large-scale networks through a mod-
iﬁed model of pulse-coupled neural networks (M-PCNNs).
However, their method could be ineffective when multiple
link changes occur. Thus, this method is not useful for the
attackers when the defender makes the system dynamic by
changing the attack surface. In [25], the authors proposed the
time-dependent shortest path problem which is able to ﬁnd the
globally optimal solution through a time-delay neural network
(TDNN) framework. Moreover, in [10], the authors proposed
a novel method that is able to estimate the nodes’ distances

through embedding graphs into an embedding space. In the
proposed method, they used leveraged a feed-forward neural
network which used vectors obtained from two recent graph
embedding techniques. Finally, The method can produce the
shortest distances for the majority of node pairs.

B. Game Theory Review

This section provides an overview of research on security
defensive methods that use game-theoretic approaches. We
present a selected set of works to highlight the application of
game theory in addressing different forms of security-related
problems. Game theory has long been applied to study network
security [26], [6]. Regarding the use of Bayesian game models,
Alpcan and Basar [27] proposed a security game between
attacker and intrusion detection system (IDS) in the sensor
network. They modeled their solution based on a ﬁnite Markov
chain, Markov decision process, and Q-learning. Sheyner et
al [28] presented a formal cost-beneﬁt analysis for the attacks
on a given network equipped with security measures for
defending on the network attacks. In [29], the authors provided
a method for attack graph construction based on network
reconﬁguration through parallel computing method. The pro-
posed method can leverage the strategic reason information of
attacks in large-scale systems.

Alpcan & Basar

[30], proposed a two-player non-
cooperative and non-zero-sum game to address the attack
defense problem in the sensor networks. In their proposed
game model, they assumed that the players have complete
information about the system and the payoff functions of each
other based on each player’s optimal strategy. However, the
drawback of the proposed game model is that the players
the game. Consequently,
have complete information about
various relevant research introduced the game theory and
studied the optimal strategy of network attack and defense.
Moreover, in [31], the authors proposed the offensive and
defensive game model which utilized a network optimal active
defense method. They solved a Nash equilibrium condition
between the the attacker and defender to ﬁnd the optimal attack
and defense strategy. However, their main idea is to address
the network security based on reducing the loss of security
loss and risk management. In [32], the authors proposed a
non-cooperative non-zero-sum game model based on network
security decision-making method for obtaining the optimal
attack and defense evaluation. The proposed technique is able
to generate an optimal strategy for attacker and defender by
analyzing the interactions of both attackers and defenders.
In [33], the authors used the offensive and defensive differen-
tial game and proposed a network security defense decision-
making method. Based on their security evolution model, it
analyzes the security state changing process of the network
system and consequently generates the attack and defense dif-
ferential game model. The proposed technique could provide
the optimal defense strategy selection through the saddle point
strategy selection. However, in the proposed game models,
there is a lack of evaluation of more intelligent attackers based
on the game models for defender and intelligent attackers.

IV. THREAT MODEL

In this section, we demonstrated how a deep neural network
can be used to estimate the shortest path in a graph efﬁciently.
However, this model can be used by adversaries to learn the
targeted system and launch a fast and effective attack toward
the system. In [10], the authors proposed a novel method
for estimation of the shortest path distance between two
nodes through vector embedding generated by deep learning
approaches. The procedure undergoes three main phases:

• Graph analysis and embeddings: The ﬁrst step is to
learn the nodes’ mapping to a low dimension space of
features maximizing the likelihood of preserving network
neighborhoods of nodes using a graph embedding tech-
nique [34]. An approach that is used for graph embedding
is called the encoder-decoder approach which can encode
the graph based on each node’s attributes to a low-
dimension vector and then decode the graph and related
information from the low-dimension learned embeddings.
However, graph embeddings include all required informa-
tion for downstream machine learning tasks. The encod-
ing function can be formulated as Enc : V → Rd based
on [35], where zi denoted as the embedding for the node
vi ∈ V . Then, the decoder is deﬁned as a function accept-
ing a set of node embeddings and consequently decoding
a user-speciﬁed graph statistics from the embeddings such
that Dec : Rd × Rd → R+.

• Training set collection: In this step, the training set can be
achieved by computing the actual shortest path distances
from a group of nodes to all of the remaining nodes.
The graph can be formally deﬁned as G(V, E) with n
nodes and m edges. Based on two nodes s, t ∈ V , it
can be deﬁned as ns,t = {s, u1, u2, ..., ul − 1, t} to be a
path of length |ns, t| = l between s and t, if {u1, ..., ul}
∈ V and (s, u1), (u1, u2), ..., (ul − 1, t) ∈ E, and let
ns,t be the set of all paths from s to t. Moreover, let
deﬁne dG(s, t) as the shortest path length between any
two nodes s, t ∈ V . Then, a graph embedding technique
described before can produce a vector embedding for the
graph G for every node v ∈ V as θ(v) ∈ Rd. To collect
the training samples, the training pairs of the entire graph
G need to be extracted which computes the actual shortest
distances from each landmark (group of nodes which
denotes as l) to all of the remaining nodes using BFS
search. It ﬁnally yields l(n − l) training pairs. However,
for testing pairs, the same strategy as training pairs can be
used through considering a smaller set of landmarks and
performing BFS traversals from landmarks to the other
nodes which can generate a set of unseen pairs.

• Neural Network Training: The vectors of the training set
(network embedding vector) will be fed into the feed-
forward neural network (FNN) to estimate the distance
between the nodes. Given a training pair < θ(v), θ(U ) >,
a joint representation as the input to the neural network
can be generated through applying the binary operations
such as concatenation, subtraction, point-wise multiplica-

TABLE II: Hosts vulnerabilities and exploitability information
(|V | is the number of vulnarabilities and Exploitability is the
maximum exploitability of all vulnerabilities for a host)

VM

vm1
vm2
vm3
vm4
vm5
vm6
vm7
vm8
vm9
DB

Current Host

h1
h2
h1
h3
h2
h3
h3
h4
h4
h5

Vulnerabilities (V)
|V |
4
4
3
3
2
1
1
1
1
1

Exploitability (e)
0.53
0.55
0.51
0.49
0.47
0.45
0.43
0.43
0.43
0.43

Impact

10
8
9
8
9
9
10
9
10
10

We modeled a cloud system consisting of 10 Virtual Ma-
chines (VMs) distributed in 5 different physical servers or
hosts in the cloud. We assume that only the VMs in host 1
(denoted as h1) are connected to the internet and are the entry
point of the system. The cloud model is demonstrated as in
Figure 3. Each VM has a number of vulnerabilities associated
with the Operating System (OS) it uses as in Table II. Thus,
the attack model can be represented as a directed attack graph.
Let AG = (V, E) be a graph, where V is a set of all the nodes
and E is a set of all the edges. The aim of the attacker is to
obtain the shortest attack path (SAP) which is a path between
two nodes without considering the weight of the attack path.
Note that the weight of the edges determines the exploitability
e of the connected VM based on Table II.

1) Attack Model: In this paper, we model an omniscient
attacker which is able to traverse to reach the goal provided
the perceived vulnerabilities exist. The attacker can launch AI-
aided attacks using AI techniques to utilize multiple attack
vectors by pursuing different exploits [23]. The attack usu-
ally comprises a sequence of transitions over time traversing
the cloud network from one node to the others. As the
attacker is an intelligent threat actor, he can use a variety
of reconnaissance techniques to surveil the network for such
vulnerabilities.

General capabilities. We assume that the attacker is able to use
various reconnaissance tools and techniques to gain enough
information about the target system. The attacker has some
information regarding the cloud hosts, VMs, and network
gained through the various network and vulnerability scanning
tools such as Nessus, Open Vulnerability Assessment Scanner
(OpenVAS), etc. The attacker can likewise infer adjacent
networks by considering his subnet address and broadcast
address. He further can have the topological overview of the
associated subnets should he get hold of the corresponding
routing tables.

AI-based capabilities.We assume that the attacker is an AI-
aided attacker which is able to leverage AI capabilities to
gain valuable information about the system, targets, and attack
paths. In this paper, we assume that the attacker can leverage
AI to estimate the shortest attack path in the modeled cloud
is
system. The shortest attack path for the cloud model

Fig. 2: Shortest path estimation using Deep Neural Network
steps.

tion, and average over the vector embeddings. The FNN
consists of an input layer, a hidden layer, and an output
layer and the size of the input layer depends on the binary
operation on vector embeddings. Finally, the real-valued
distance can be obtained by the neural network mappings
of the input vectors to an output.

However, as demonstrated in Figure 2, the advanced at-
tackers can utilized AI techniques such as those as discussed
above to ﬁnd their target effectively and fast and can efﬁciently
leverage deep learning techniques for ﬁnding the shortest path
in a graph (such as those proposed in [9], [10], [11]) which is
resilience against the dynamic defense and also can provide the
scalability of the model such as large cloud or enterprises. In
this paper, we demonstrate how game theory can be leveraged
to evaluate the defensive scenarios for avoiding these kinds of
AI-powered attacks.

V. GAME THEORY FOR AI-AIDED ATTACKS AND
DYNAMIC DEFENCE EVALUATION

In this section, we propose the system model for both attack
and defense including the capabilities of AI-aided attackers
which make them able to effectively ﬁnd the shortest attack
path in the modeled system. We then deﬁne and propose a
zero-sum dynamic game model which can capture and evaluate
various attacker actions and defenders in different discrete
states of the game.

A. Preliminaries

In this section, we introduce the notation and deﬁnitions
used throughout this paper including the cloud model, attack,
model, game theory deﬁnition, zero-sum Markovian game
model, and so forth.

Adversarial GraphGenerationGraph EmbeddingTraining SetGenerationGZiGenerate graphs such asadversarial GANInput: embedded vectorsOutput: training set pairsDNN moduleZi, trained pairsInput: training set pairsOutput: shortest path estimationStartEndUsing graph embedding techniquessuch as encode-decode technique forlearning model inputs0.43

=

e

DB

h8

e=0.43

0.4 5

=

e

e

=

h6

e

=

0.4
3

3
4
.
0
=
e

h9

0

.

4

5

e = 0 . 4 3

h7

e = 0.43

h3

e = 0.51

h1

9
0.4
=
e

e=0.45

e

e

=

0

.

5

3

A

=

0

.

4

7

h4

7
4
.
0
=
e

e

=

0
.
4

3

h5

e

=

0

.

4

9

5
0.5
=
e

e = 0 . 4 7

h2

Fig. 3: (a) The cloud-model including 10 VMs in different hosts (servers). (b) Attack graph model corresponding to the
cloud-model (capturing the connectivities of the VMs, note that the shortest attack path is shown as dashed lines).

(a)

(b)

highlighted as a dashed line in Figure 3b. We assume that the
attacker can leverage Deep learning Techniques (as in [10]) to
estimate the shortest path attack from the entry point of the
system (i.e. internet) to the targeted host (DB) in the system.
The attacker can exploit the vulnerabilities existing on each
host with the probabilities deﬁned based on the CVSS metrics.
Note that for each attack step, the attacker needs to estimate
the shortest attack path again as the system is dynamic and
may be changed. However, for each attack step in an attack
path, the attacker incurs some expenses such as costs and time.
Attacker’s goal and actions. The goal of the attacker is also
well deﬁned, which in this case is to exploit the database (DB)
in the cloud through compromising the vulnerabilities existing
on each VM in the attack path. The attacker has the ability to
perform various actions. First, the attacker can take no action
to hide. Once the attacker estimates the shortest attack path
through AI techniques described in Section IV, the attacker
undertakes various actions to exploit the vulnerabilities of each
VM in the attack path and ﬁnally exploit the target. Note that
after exploiting a VM the attacker reaches a new state. We
assume that the attack mounted by the attacker is monotonic
which means once an attacker has reached a certain state, they
do not need to go back to any previous state, when targeting
a speciﬁc goal.

B. Game Model Deﬁnition

1) Game Model Assumption: We assume that the game
for the deﬁned cloud model deﬁned in Section V-A can be
modeled as a zero-sum Markov game in which the defender
tries to place the IDS in the cloud’s host to detect the attacker
and defend against the attacker’s action. These game attacks
can be modeled using a Markovian model with ﬁnite states
based on the attack. We also assume that both the players have
full observability of the state in which they are. Additionally,
we assume that the attacker can remain undetected in any VM
in the cloud until it attempts to attack the other connected VM
by exploiting the targeted VM vulnerabilities.

2) Markovian Game Model: We model the attacker and
a defender scenario as a two-player zero-sum Markov game
leveraging the information in the cloud model and correspond-
ing AG represented in Figure 3b. We also assume that the
states and actions of the model are both discrete and ﬁnite.
Moreover, the transition from each state and consequently
the corresponding reward for each state depends on players’
actions for that speciﬁc state (this also can be modeled based
on previous states and actions which are beyond the scope
of this paper). We formally deﬁne a zero-sum Markov Game
model based on obvious Markovian assumption and explain
how each of these parameters are obtained in our cloud
model. A Markov game for two players (in here, attacker and
defender) can be deﬁned by a tuple (S,AA, AD, T, R) where,
• S = {s0, s1, s2, . . . , sr} denoted the ﬁnite states of the

game where in here |S| = max(len(api ∈ AP )).

• AA = {aA

0 , aA

1 , aA

2 , ...} is the attacker’s set of actions.
The defender can have a set of actions as DA =
{aD

0 , aD

1 , aD

2 , ...}.

• T = (s, aA, aD, s(cid:48)) is a States’ Transition where the
current state s ∈ S will be changes to s(cid:48) ∈ S upon
the actions come from both attacker and defender respec-
tively. However, each transition has a probability which
is denoted by tp(T ).

• RA(s; aA; aD) is the reward obtained by attacker if in
state s, attacker and defender take the actions aA and
aD respectively. However, the reward can be negative
−RA(s; aA; aD) is the attacker choose a wrong action.
• λp ∈ [0, 1) is deﬁned as the discount factor for the

corresponding player p.

3) Reward function: The reward or payoff function depends
on the actions taken by the attacker and the defender in each
state of the game. We use the CVSS values deﬁned in Table II
for the reward or penalties associated with the successful or
unsuccessful actions taken by either attacker or defender.

To quantify reward values we use the important variables
such as the impact of an attack and cost of defense (Cdef ), we

TABLE III: Payoff matrix formalization based on aA, aD for the game states s0–s4
s0: Initial State (no exploit)

A/D

No-act

Def-h1

Def-h2

No-att
E(vm1 ∈ h1)
E(vm2 ∈ h2)

0, 0
Ivm1 , −Ivm1
Ivm2 , −Ivm2

Cdef , −Cdef
Cdef , −Cdef
Ivm1 + Cdef , −(Ivm1 + Cdef )
−(Ivm1 − Cdef ), Ivm1 − Cdef
Ivm2 + Cdef , −(Ivm2 + Cdef ) −(Ivm2 − Cdef ), Ivm2 − Cdef

s1: Transition State (vm2 ∈ h2 exploited)

A/D

No-act

Def-h3

Def-h2

No-att
E(vm4 ∈ h3)
E(vm5 ∈ h2)

0, 0
Ivm4 , −Ivm4
Ivm5 , −Ivm5

Cdef , −Cdef
Cdef , −Cdef
Ivm4 + Cdef , −(Ivm4 + Cdef )
−(Ivm4 − Cdef ), Ivm4 − Cdef
Ivm5 + Cdef , −(Ivm5 + Cdef ) −(Ivm5 − Cdef ), Ivm5 − Cdef

s2: Transition State (vm5 ∈ h2 exploited)

A/D

No-act

Def-h3

Def-h4

No-att
E(vm7 ∈ h3)
E(vm9 ∈ h4)

0, 0
Ivm7 , −Ivm7
Ivm9 , −Ivm9

Cdef , −Cdef
Cdef , −Cdef
−(Ivm7 − Cdef ), Ivm7 − Cdef
Ivm7 + Cdef , −(Ivm7 + Cdef )
Ivm9 + Cdef , −(Ivm9 + Cdef ) −(Ivm9 − Cdef ), Ivm9 − Cdef

s3: Transition State (vm9 ∈ h4 exploited)

A/D

No-act

Def-h3

Def-h5

No-att
E(vm6 ∈ h3)
E(DB ∈ h5)

0, 0
Ivm6 , −Ivm6
IDB, −IDB

Cdef , −Cdef
−(Ivm6 − Cdef ), Ivm6 − Cdef
IDB + Cdef , −(IDB + Cdef )

Cdef , −Cdef
Ivm6 + Cdef , −(Ivm6 + Cdef )
−(IDB − Cdef ), IDB − Cdef

s4: Final State (DB exploited)

used CVSS metrics that provide the Impact (I) for a speciﬁc
VM (Ivmi ), Exploitability Scores (e), and other relevant met-
rics. Ivmi is a metric that computes the damage imposed to
the VM by computing all impacts on the resources through an
attack. For instance, Ivm4 = 8 is the attack impact value on the
VM vm4 based on the related impact metrics of vulnerabilities
in CVSS represented in Table II. The rewards matrix for
attackers is formulated as Equation 1.



RA

aA,aD =




0
Cdef
Ivmi +Cdef
Ivmi
-(Ivmi -Cdef )

if aA ⊂ Ø
if aA ⊂ Ø, aD (cid:54)⊂ Ø
if aA=E(vmi), aD (cid:54)⊂ Ø, vmi /∈ H(ids)
if aA=E(vmi), aD ⊂ Ø
if aA=E(vmi), aD (cid:54)⊂ Ø, vmi ∈ H(ids)
(1)
Note that H(ids) is a function that returns the host in which
ids has been located. For instance, if the defender locate the
IDS in Host h4, then H(ids) returns h4. As the game is a
zero-sum game the reward for the defender is as equation 2.

RD

aA,aD = −1 ∗ RA

aA,aD

(2)

TABLE IV: Payoff Matrix quantifying based on zero-sum
game and CVSS values

s0: Initial State (no exploit)

A/D

No-att
E(vm1 ∈ h1)
E(vm2 ∈ h2)

No-act

0, 0
10, −10
8, −8

Def-h1

Def-h2

2, −2
−8, 8
10, −10 −6, 6

2, −2
12, −12

s1: Transition State (vm2 ∈ h2 exploited)

A/D

No-att
E(vm4 ∈ h3)
E(vm5 ∈ h2)

No-act

0, 0
8, −8
9, −9

Def-h3

Def-h2

2, −2
−6, 6
11, −11 −7, 7

2, −2
10, −10

s2: Transition State (vm5 ∈ h2 exploited)

A/D

No-att
E(vm7 ∈ h3)
E(vm9 ∈ h4)

No-act

0, 0
10, −10
10, −10

Def-h3

Def-h4

2, −2
−8, 8
12, −12 −8, 8

2, −2
12, −12

s3: Transition State (vm9 ∈ h4 exploited)

A/D

No-att
E(vm6 ∈ h3)
E(DB ∈ h5)

No-act

0, 0
9, −9
10, −10

Def-h3

Def-h5

2, −2
−7, 7
12, −12 −8, 8

2, −2
11, −11

As stated earlier, the formulation of the reward function is
based on CVSS values and mainly the impact of the attack on
a targeted VM. If the defender and the attacker do not take
any action such that aA ⊂ Ø, aD (cid:54)⊂ Ø both get zero rewards.
Moreover, if the attacker doesn’t attack (no-att) while the
defender place the IDS to any host in the cloud to secure
any hosts, the defender incurs a cost for the defense (−Cdef )
and gets a negative reward. However, if the attacker attacks
on a VM vmi while the defender place the IDS to detect
attacks on the host in which the targeted VM vmi is located

such that vmi ∈ H(ids), then the defender gets the reward
for avoiding the attack impact on that VM (Ivmi), but as the
defender incurs some costs for the defense the total reward of
successful defense is formulated as Ivmi − Cdef . For instance,
suppose that the cost of defense is 2 units (for both successful
or unsuccessful defense). Then, if attacker exploits VM vm1
and defender put IDS on the host h1 (vm1 ∈ h1), the defender
gain a total reward of 7 which is as RD = Ivm1 − Cdef =

9 − 2 while the attacker is penalized by -7 unit. In contrast,
if the attacker attacks on a VM vmi while the defender place
the IDS to detect attacks on the host in which the targeted
VM vmi is not located such that vmi /∈ H(ids), then the
defender gets the penalty for wrong defense and incurs the
impact of the attack on that VM plus the cost of wrong defense
which is −(Ivmi +Cids) while the attacker reward would be as
Ivmi + Cids based on the zero-sum deﬁnition. For instance, if
attacker exploits VM vm1 and defender put IDS on the host h2
(vm1 /∈ h2), the defender gets a negative reward of -11 which
is as the sum of the impact of attack on that VM and the cost
of defense as RD = −1 ∗ (Ivm1 + Cdef ) = −1 ∗ (9 + 2). Then,
the attacker gets rewards of 11 which is RA = −RD. Lastly, if
the attacker attacks on a VM and the defender takes no action
then the attacker gains the reward for the successful attack
which is equivalent to the impact of the attack on exploited
VM vmi as RA = Ivmi while the defender gets a negative
reward as RD = −Ivmi.

A normal-form zero-sum reward matrix for the four states
of the game in the Markov game is shown in Table IV which is
quantiﬁed based on the CVSS values and the reward function
formulation explained before.

4) States, actions and transitions: The Markov model of
the proposed game is illustrated in Figure 4 which captures the
transitions and associated probabilities in which the attacker
tries to ﬁnd the shortest attack path to exploit DB (based on
Figure 3a).

States. It represents the state attacker/defender currently have
in the cloud over different preformed actions. We extract the
information from the shortest path in the cloud attack graph
to deﬁne the states. For instance, for the attacker, initial state
s0 = (Host; U ser), if the successful execution of the exploit
of VM vm2 is performed by the attacker E(vm2), the attacker
can transition to another state s1 = (H1; Attacker).
Actions and state transitions. Based on the system model rep-
resented in Figure 3b, the attacker has at most three possible
actions in each state. The attacker can choose no attack (no-
att or Ø) or attack to another adjacent VM by exploiting the
vulnerabilities of targeted VM. Thus, for each state the maxi-
mum actions can be deﬁned as M ax(Deg(vmi ∈ H)+1 = 3.
For instance, in s0, the action space for the attacker can be as
aA
= E(vm2). Similarly, the
0,s0
defender has its own possible actions to defend (Def ) hosts.
For instance, the defender can perform no defence (N o-act
or Ø). All possible actions for the defender in state s0 is as
aD
0,s0

= E(vm1), aA

= D(h1), aD

= D(h2).

= Ø, aA

= Ø, aD

1,s0

2,s0

1,s0

2,s0

C. Probabilistic Model and Game Solving

1) Uniform Random Strategy (URS): We assume that the
defender uses Uniform Random Strategy (URS) where the
defender selects the actions aD
s ∈ DA based on a uniform
probability distribution over its possible actions in the corre-
sponding state. The decision-making process can be viewed
as randomization that chooses the next valid state based on
a speciﬁc probability distribution over states sq ∈ S, then

choosing the next host for IDS placement from a uniform
distribution to become speciﬁc instances of randomization.
We choose URS as the baseline of the defender’s strategy.
For instance, based on the initial state s0 shown in Table IV,
The defender can select the mixed strategy of placing IDS on
host h1, host h2 in the cloud, or taking no action. Thus, the
selection of actions is uniformly distributed for the defender
with the equal probability of 3.33. However, many studies
claim that defender’s strategy selection can be performed as
pure or URS for dynamic defense [36].

2) Maxmin strategy: In this game, the attacker PA aims to
maximize his expected discounted reward and the defender PD
tries to choose the actions that minimize the expected reward
for the attacker. Thus, the maxmin strategy can be considered
for calculating the expected reward of PA in the Markov
game. Given Q(s, aA, aD), an agent is able to maximize the
reward using the greedy strategy by always selecting the action
having the highest Q-value. This strategy is considered as a
greedy strategy because it treats Q(s, aA, aD) as a surrogate
for immediate reward and then acts to maximize its immediate
gain. However, it is optimal because the Q-function is an
accurate summary of future rewards.

We now deﬁne the quality of an action or the Q value used
to represent the expected reward the attacker (A) will get for
choosing the action aA ∈ AA while the defender chooses
aD ∈ DA.

Q(s, aA, aD) = R(s, aA, aD) + λ

T (s, aA, aD, s(cid:48))

(cid:88)

s(cid:48)

While the value of a state s ∈ S in a Markov game is as

V (s) = maxπ(s) minaD

Q(s, aA, aD).πaA

(cid:88)

aA

However, the defender tries to minimize the attacker’s reward
by placing IDS in the various hosts in the cloud that are a part
of the attack paths in the attack graph, while the attacker aims
to use a mixed policy π(s) over it possible actions in aD to
maximize its total reward. Thus, the Markov Game is useful
framework for the defender to model the attacker’s policy
so that they can take necessary actions and countermeasures
by making decision in each state to minimize the expected
attacker’s utility.

1,s0

0,s0

2,s0

, aA

, aA

} where aA

3) Transitions Probabilities Assignment: The actions for
attackers and defender are considered separately for each
states. For instance, the attacker action space in the initial
state S0 is as AA,s0 = {aA
= Ø
which indicates the attacker takes no action/attack (N o-att)
to avoid detection, aA
= E1 which implies that the attacker
exploits VM vm1 (note that E(vmi) is shortly denoted as
Ei), and aA
= E2 which means exploiting of VM vm2 or
E(vm2). The probability of attack access through considering
all possible actions for each state sj, denoted as p(ASsj ), can
be deﬁned as the Equation (3). This means the attacker can
launch a successful attack by taking only one successful action

2,s0

0,s0

1,s0

Fig. 4: Markov model of the game with ﬁnite states and deterministic probabilities based on the shortest attack path.

in that state (note that the action N o-att is not considered as
a successful attack action).

p(AS)sq = 1 −

(cid:89)

(cid:16)

1 − e(aA

j,sq

(cid:17)
)

(3)

aA

j,sq

∈AA,sq −{Ø}

j,sq

Note that e(aA

) is the probability of attack success by
taking the speciﬁc action aA
in a state sq which is the ex-
j
ploitability of the targeted VM based on Table II. For instance,
exploiting of vm1 is an action of the attacker aA
= E1, then
e(aA

1,s0
Now we deﬁne the probability that the attacker chooses a

) is e(E1) = e(vm1) = 0.53.

1,s0

speciﬁc action aA

z in a current state Sq as Equation (4).

p(aA

z,sq

) =

(cid:80)

aA

j,sq

e(aA

)
z,sq
(cid:16)

∈AA,sq

e(aA

j,sq

(cid:17)

)

(4)

For instance, the probability that attacker takes action aA
1,s0
in state s0 which means that the attacker prefers to exploit
vm1 (denoted as p(aA

) or p(E1)) is calculated as:

1,s0

p(aA

1,s0

) =

e(aA

)

1,s0
) + e(aA

2,s0

e(aA

1,s0

=

)

e(E1)
e(E1) + e(E2)

Based on the above equation, the result of p(E1) is as
0.53
1.08 ≈ 0.49. Similarly, the probability of the attacker choose
the second action p(E2) is computed as p(aA
) = p(E2) ≈
0.51.

2,s0

We then deﬁne the transition probability for attackers only

for a speciﬁc attack action (aA

z ) as Equation (6).

τ (aA

z,sq

) =

(cid:40)p(aA

z,sq
1 − (cid:80)

).e(z, sq)

aA

j,sq

∈AA,sq

(cid:16)

p(aA

j,sq

).e(j, sq)

if aA
(cid:17)

z,sq (cid:54)⊂ Ø
otherwise
(5)

2,s0

For instance, the τ (aA

) = τ (E2) = 0.51 ∗ 0.55 = 0.28
which is the product of the probability that the attacker choose
action aA
2,s0 and the attack success probability of the related
attack action e(aA
). We then assume that the probability of
defender’s actions for each state of the same are uniformly
distributed. Thus, the transition probability for defender only
for n speciﬁc defend action (aD
z ) for the state sq is deﬁned as:

2,s0

τ (aD

z,sq

) =

1
|DA,sq |

,

(6)

where |DA,sq | is the numbers of actions for defender. For
instance, if the defender has three actions such as no action
(no-act), defend host h1, and defend host h2, then τ (aD
) =
τ (D2) ≈ 0.33. Note that D2 ∈ DA indicates the defend of
host h2 (placement of IDS in host h2).

2,s0

Now we deﬁne the transaction probability based on both

attacker’s and defender’s actions as Equation (7).

tp(s, aA

z , aD

z , s(cid:48)) = τ (aA

z,s).τ (aD

z,s)

(7)

For example,

the transition probability for T0,1 =

(s0, aA, aD, s1) can be computed as:

tp(T0,1) = tp(s0, aA, aD, s1) = τ (E2).τ (D2) ∪ τ (E2).τ (Ø)

which yields p(T0,1) ≈ 0.19. Similarly, the transition from
state s0 to s0 can be deﬁned as T0,0 = (s0, aA, aD, s0) and
its probability is computed as:

tp(T0,0) = tp(s0, aA, aD, s0) = τ (Ø).τ (Ø) ∪ τ (Ø).τ (D1)
∪τ (Ø).τ (D2) ≈ 0.41.

(8)

Likewise, all the Markovian model transitions for the states
of the game and the transition probabilities are computed
and illustrated in Figure 4. Note that the Markovian game

is modeled based on the attacker using the shortest attack
path to ﬁnd the target for each state of the game and the
probabilities are deﬁned and formulated based on the URS
for this game. However, transitions probabilities assignment
though Q-learning is not considered in this paper and will be
considered in our future work.

VI. DISCUSSIONS AND LIMITATIONS

This paper attempts to initiate a discussion regarding game
theory evolution which can be able to evaluate AI-aided
threats and making appropriate decisions on possible defensive
strategies. In this paper, we only considered an example of
AI-aided attacks which is able to leverage the deep neural
networks to ﬁnd the shortest attack path in a networked system.
However, a more capable game model needs to be proposed to
be able to defend against AI-embedded attacks such as Deep
locker. We summarize some limitation and further challenges
as follows:
Limitation and Challenges. Following challenges need to be
investigated more in the current game theory models against
the next generation of cyber threats.

• In this paper, we only modeled an attacker that is able
to estimate the shortest attack path in the modeled cloud.
However, other efﬁcacy factors need also be considered
in the model such as maximum exploitability (M E). It
can ensure that the attacker traverses the attack paths
which yields the maximum probability of success. The
omniscient can leverage AI techniques as deﬁned in
Section IV and [23] to estimate and probe not only the
shortest path but also the best attack path in the network.
Thus,
the attacker can ﬁnd the attack path with the
maximum exploitability values based on CVSS values.
In fact, the attacker goes through the path(s) with the
highest attack success probability values. Based on the
attacker’s point of view, M E can be deﬁned as the
following equation.

M E = min
ap∈AP

(cid:16) (cid:88)

(cid:17)

e(vmi)

vmi∈ap

However, in this paper, the main focus is on the attacker
using DNN to ﬁnd the shortest attack path and we aim
to further consider other attack strategies such as M E
estimation in our future work.

• We only considered our proposed game theory model for
a small cloud model. However, this can be extended to
larger cloud models in which the attacker can ﬁnd the
shortest attack path through DNN more efﬁciently. The
large cloud system can be modeled using an AG, see
Figure 5. In this case, the maximum number of states
(S) in the game model can is determined as the maximum
number of VMs in the shortest attack path:
SAP = min(cid:0)len(api ∈ AP )(cid:1),
then, |S| = SAP As the model is based on the shortest
attack path, the model is not faced with a state explosion
problem.

Fig. 5: AG generated for a large cloud model. The AI-aided
attacker can leverage DNN to estimate the shortest attack path
effectively in large cloud model.

• An important limitation of the game-theoretic approach
in cyber security is the difﬁculty to perfectly quantifying
parameters of cyberspace. However, this might affect the
decision-making process by defenders. However,
is
important to leverage Machine learning-based approaches
to identify parameters and values for both attacker and
defender.

it

• The application of Nash equilibrium requires both attack-
ers and defenders to choose their own optimal strategies
at the same time which the process is difﬁcult to be
achieved in the reality especially for the new generation
of threats.

• However, in most game theory models, the investigations
are based on the hypothesis for both sides of attacker
and defender and are completely rational. Both parties
know how to realize the maximization of their reward
values. However, the attack-defense information of the
actual network is and intimate and asymmetric as the
strategies’ rewards could be private information for the
game players.

VII. CONCLUSION

This paper ﬁrst discusses the different types of AI-powered
attacks categorized as AI-aided and AI-embedded attacks.
Then the application of game theory to model various cyber
threat scenarios is reviewed. Then, a cloud system is proposed
in which an AI-aided attacker can ﬁnd the shortest attack path
in the network effectively using a deep neural network. We
proposed a zero-sum Markovian game model which is able
to model AI-aided attacks based on ﬁnite states which can
help the defender to make appropriate decision to mitigate
the attack impact. This paper demonstrates the potential of
Markovian game theory models in strengthening the decision-

vmvmvm151.26.25.163vmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvm202.154.33.129Servervmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvm192.168.26.35vmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmDBvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvm129.25.36.129vmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvm160.25.141.43vmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvmvm[20] A. V. Goldberg and C. Harrelson, “Computing the shortest path: A∗
search meets graph theory.” in SODA, vol. 5. Citeseer, 2005, pp. 156–
165.

[21] N. Koji´c, I. Reljin, and B. Reljin, “Neural network for optimization of
routing in communication networks,” Facta universitatis-series: Elec-
tronics and Energetics, vol. 19, no. 2, pp. 317–329, 2006.

[22] A. Syarif, K. Muludi, R. Adrian, and M. Gen, “Solving fuzzy shortest
path problem by genetic algorithm,” in IOP Conference Series: Materials
Science and Engineering, vol. 332, no. 1.
IOP Publishing, 2018, p.
012003.

[23] A. Zimba, H. Chen, and Z. Wang, “Bayesian network based weighted
apt attack paths modeling in cloud computing,” Future Generation
Computer Systems, vol. 96, pp. 525–537, 2019.

[24] X. Liu, “A network attack path prediction method using attack graph,”
Journal of Ambient Intelligence and Humanized Computing, pp. 1–8,
2020.

[25] W. Huang, C. Yan, J. Wang, and W. Wang, “A time-delay neural network
for solving time-dependent shortest path problem,” Neural Networks,
vol. 90, pp. 21–28, 2017.

[26] T. Alpcan, Y. Vorobeychik, J. S. Baras, and G. D´an, Decision and
Game Theory for Security: 10th International Conference, GameSec
2019, Stockholm, Sweden, October 30–November 1, 2019, Proceedings.
Springer Nature, 2019, vol. 11836.

[27] T. Alpcan and T. Basar, “An intrusion detection game with limited
observations,” in 12th Int. Symp. on Dynamic Games and Applications,
Sophia Antipolis, France, vol. 26, 2006.

[28] S. Jha, O. Sheyner, and J. Wing, “Two formal analyses of attack graphs,”
in Proceedings 15th IEEE Computer Security Foundations Workshop.
CSFW-15.

IEEE, 2002, pp. 49–63.

[29] A. Chowdhary, S. Pisharody, and D. Huang, “Sdn based scalable mtd
solution in cloud network,” in Proceedings of the 2016 ACM Workshop
on Moving Target Defense, 2016, pp. 27–36.

[30] T. Alpcan and T. Basar, “A game theoretic approach to decision and
analysis in network intrusion detection,” in 42nd IEEE International
Conference on Decision and Control (IEEE Cat. No. 03CH37475),
vol. 3.

IEEE, 2003, pp. 2595–2600.

[31] W. Jiang, B.-X. Fang, Z.-H. Tian, and H.-L. Zhang, “Evaluating network
security and optimal active defense based on attack-defense game
model,” Chinese Journal of Computers, vol. 32, no. 4, pp. 817–827,
2009.

[32] L. Gang, Z. Hong, and L. Qian-mu, “Network security optimal attack
and defense decision-making method based on game model,” Journal of
Nanjing university of science and technology, vol. 38, pp. 12–21, 2014.
[33] H. Zhang, T. Li, and S. Huang, “Network defense decision-making
method based on attack-defense differential game,” Acta Electronica
Sinica, vol. 46, no. 6, pp. 1428–1435, 2018.

[34] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for
networks,” in Proceedings of the 22nd ACM SIGKDD international
conference on Knowledge discovery and data mining, 2016, pp. 855–
864.

[35] W. L. Hamilton, R. Ying, and J. Leskovec, “Representation learning on
graphs: Methods and applications,” arXiv preprint arXiv:1709.05584,
2017.

[36] R. Zhuang, S. A. DeLoach, and X. Ou, “Towards a theory of moving
target defense,” in Proceedings of the First ACM Workshop on Moving
Target Defense, 2014, pp. 31–40.

making based on the capabilities of AI-based threats and
ﬁnding optimized strategies in decision making comparing
with other time-consuming optimization techniques. However,
there are several critical challenges that need to be further
studied and addressed before modeling the AI-based game
theory models. We hope that our discussion and our initial
design of an experimental model can trigger more profound
research to make the game theory more viable for novel cyber
threat evaluation.

REFERENCES

[1] J. Hou, Q. Li, S. Cui, S. Meng, S. Zhang, Z. Ni, and Y. Tian, “Low-
internet,” The

cohesion differential privacy protection for industrial
Journal of Supercomputing, vol. 76, no. 11, pp. 8450–8472, 2020.
[2] M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garﬁnkel,
A. Dafoe, P. Scharre, T. Zeitzoff, B. Filar et al., “The malicious use
of artiﬁcial intelligence: Forecasting, prevention, and mitigation,” arXiv
preprint arXiv:1802.07228, 2018.

[3] P. Hu, H. Li, H. Fu, D. Cansever, and P. Mohapatra, “Dynamic defense
strategy against advanced persistent threat with insiders,” in Proceedings
of the IEEE INFOCOM, 2015, pp. 747–755.

[4] M. P. Stoecklin, “Deeplocker: How ai can power a stealthy new breed

of malware,” Security Intelligence, August, vol. 8, 2018.

[5] Z. Wang, C. Liu, X. Cui, J. Zhang, D. Wu, J. Yin, J. Liu, Q. Liu, and
J. Zhang, “Ai-powered covert botnet command and control on osns,”
arXiv preprint arXiv:2009.07707, 2020.

[6] M. H. Manshaei, Q. Zhu, T. Alpcan, T. Bas¸ar, and J.-P. Hubaux, “Game
theory meets network security and privacy,” ACM Computing Surveys
(CSUR), vol. 45, no. 3, pp. 1–39, 2013.

[7] A. Attiah, M. Chatterjee, and C. C. Zou, “A game theoretic approach to
model cyber attack and defense strategies,” in 2018 IEEE International
Conference on Communications (ICC).

IEEE, 2018, pp. 1–7.

[8] S. Rass, A. Alshawish, M. A. Abid, S. Schauer, Q. Zhu, and H. De Meer,
“Physical intrusion games—optimizing surveillance by simulation and
game theory,” IEEE Access, vol. 5, pp. 8394–8407, 2017.

[9] F. S. Rizi, J. Schloetterer, and M. Granitzer, “Shortest path distance
approximation using deep learning techniques,” in 2018 IEEE/ACM
International Conference on Advances in Social Networks Analysis and
Mining (ASONAM).
IEEE, 2018, pp. 1007–1014.

[10] F. Salehi Rizi, J. Schloetterer, and M. Granitzer, “Shortest path distance
approximation using deep learning techniques,” arXiv e-prints, pp.
arXiv–2002, 2020.

[11] H. Qu, Z. Yi, and S. X. Yang, “Efﬁcient shortest-path-tree computation
in network routing based on pulse-coupled neural networks,” IEEE
transactions on cybernetics, vol. 43, no. 3, pp. 995–1010, 2013.
[12] J.-H. Cho, D. P. Sharma, H. Alavizadeh, S. Yoon, N. Ben-Asher, T. J.
Moore, D. S. Kim, H. Lim, and F. F. Nelson, “Toward proactive, adaptive
defense: A survey on moving target defense,” IEEE Communications
Surveys & Tutorials, vol. 22, no. 1, pp. 709–745, 2020.

[13] H. Alavizadeh, H. Alavizadeh, D. S. Kim, J. Jang-Jaccard, and M. N.
Torshiz, “An automated security analysis framework and implementation
for mtd techniques on cloud,” in International Conference on Informa-
tion Security and Cryptology. Springer, 2019, pp. 150–164.

[14] N. Kaloudi and J. Li, “The AI-based cyber threat landscape: A survey,”
ACM Computing Surveys (CSUR), vol. 53, no. 1, pp. 1–34, 2020.
[15] F. Tramer, N. Carlini, W. Brendel, and A. Madry, “On adaptive attacks to
adversarial example defenses,” arXiv preprint arXiv:2002.08347, 2020.
[16] E. M. Hutchins, M. J. Cloppert, and R. M. Amin, “Intelligence-driven
computer network defense informed by analysis of adversary campaigns
and intrusion kill chains,” Leading Issues in Information Warfare &
Security Research, vol. 1, 2011.

[17] X. Feng, Z. Zheng, D. Cansever, A. Swami, and P. Mohapatra, “A
signaling game model for moving target defense,” in Proceedings of
the IEEE INFOCOM, 2017, pp. 1–9.

[18] A. Bagheri, M. R. Akbarzadeh Totonchi et al., “Finding shortest path
with learning algorithms,” International Journal of Artiﬁcial Intelli-
gence, vol. 1, 2008.

[19] Y. Deng, Y. Chen, Y. Zhang, and S. Mahadevan, “Fuzzy dijkstra algo-
rithm for shortest path problem under uncertain environment,” Applied
Soft Computing, vol. 12, no. 3, pp. 1231–1237, 2012.

