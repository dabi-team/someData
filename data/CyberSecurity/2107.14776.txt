1
2
0
2

l
u
J

0
3

]

R
C
.
s
c
[

1
v
6
7
7
4
1
.
7
0
1
2
:
v
i
X
r
a

Synthetic ﬂow-based cryptomining attack generation
through Generative Adversarial Networks

Alberto Mozo1,*, Ángel González-Prieto2,3, Antonio Pastor4, Sandra Gómez-Canaval1, and
Edgar Talavera1

1Universidad Politécnica de Madrid, Madrid, Spain.
2Universidad Complutense de Madrid, Madrid, Spain.
3Instituto de Ciencias Matemáticas (CSIC-UAM-UCM-UC3M), Madrid, Spain.
4Telefónica I+D, Madrid, Spain
*a.mozo@upm.es

Abstract

Due to the growing rise of cyber attacks in the Internet, ﬂow-based data sets are crucial to increase
the performance of the Machine Learning (ML) components that run in network-based intrusion detection
systems (IDS). To overcome the existing network traﬃc data shortage in attack analysis, recent works
propose Generative Adversarial Networks (GANs) for synthetic ﬂow-based network traﬃc generation. Data
privacy is appearing more and more as a strong requirement when processing such network data, which
suggests to ﬁnd solutions where synthetic data can fully replace real data. Because of the ill-convergence
of the GAN training, none of the existing solutions can generate high-quality fully synthetic data that can
totally substitute real data in the training of IDS ML components. Therefore, they mix real with synthetic
data, which acts only as data augmentation components, leading to privacy breaches as real data is used.

In sharp contrast, in this work we propose a novel deterministic way to measure the quality of the syn-
thetic data produced by a GAN both with respect to the real data and to its performance when used for
ML tasks. As a byproduct, we present a heuristic that uses these metrics for selecting the best performing
generator during GAN training, leading to a stopping criterion. An additional heuristic is proposed to select
the best performing GANs when diﬀerent types of synthetic data are to be used in the same ML task. We
demonstrate the adequacy of our proposal by generating synthetic cryptomining attack traﬃc and normal
traﬃc ﬂow-based data using an enhanced version of a Wasserstein GAN. We show that the generated syn-
thetic network traﬃc can completely replace real data when training a ML-based cryptomining detector,
obtaining similar performance and avoiding privacy violations, since real data is not used in the training of
the ML-based detector.

Keywords: Network traﬃc generation, Generative Adversarial Networks, cryptomining, Jaccard index,
Cyber-range.

Introduction

Cybersecurity and large-scale network traﬃc analysis are two important areas receiving considerable attention
over the last few years. Among other reasons, this is due to the necessity of empowering the telecom industry
to adopt suitable mechanisms to face emerging and sophisticated cyberattacks. Nowadays, Internet Service
Providers (ISPs) and their clients are exposed to a growing rise in the number and type of threats (e.g., network
attacks, data theft over the wire), some of which also attack at the application level using the network for
identity theft, phishing, or malware distribution. In general terms, these threats severely put QoE (Quality
of Experience) at risk, undermining services, network resources, and users’ conﬁdence.
In this context, one
promising solution is the use of Machine and Deep Learning (MDL) techniques to address the appearance of
new points of vulnerability and exposure to new attack vectors [1–3]. At the same time, malicious agents are
moving forward in the same direction to use MDL for their activities or to deceive MDL inference engines [4].
The application of MDL techniques requires the availability of considerable amounts of data to take advantage
of their powerful learning processes. Telecom data management processes are not well suited to oﬀer these
required data sets as they exhibit a set of problems not only are related with the gathering and sharing of data
but also with their processing in a MDL pipeline. This situation represents a considerable drawback since data

1

 
 
 
 
 
 
gathering and processing tasks in the telecom industry have been optimized to guarantee services and billing.
Indeed, they are not prepared with speciﬁc MDL data processing techniques. Moreover, the applicability of
MDL algorithms should take into account the evolution of attack patterns over time, which implies to produce
periodically additional volumes of relevant data for training new MDL models.

Moreover, a great percentage of MDL techniques used in Intrusion Detection Systems (IDS) are the so-
called supervised techniques that require labelled data sets to train and validate MDL models. As in many
other domains, telecom industry faces the impossibility of having labelled data sets or developing eﬃcient and
accurate processes to label them. Since network traﬃc is generated by end users and applications, it can be
challenging for an ISP to identify and label the nature of network traﬃc at the detailed level required by MDL
techniques. This diﬃculty is exploited by cyber criminals, who seek to mix cyber attacks with normal traﬃc
by encrypting it over common TCP ports (e.g., Transport Layer Security (TLS) using TCP/443 (HTTPS)).
Although unsupervised techniques that do not need labelled data sets can be applied in some scenarios, a
signiﬁcant number of sophisticated attacks require supervised MDL methods to be detected.

Even if eﬃcient mechanisms for labelling data sets can be implemented, data are increasingly protected
by the legal regulations that governments impose to guarantee the privacy of their contents (e.g., European
General Data Protection Regulation (GDPR)). These restrictions may discourage the use of real data sets for
MDL training and validation purposes. For a suitable advance on cybersecurity research, and speciﬁcally, on
threat detection in network traﬃc, the telecom industry requires novel methods to generate labelled data sets
to be used in MDL training and validation processes.

In the last decade, Generative Adversarial Networks (GANs) [5] have gained signiﬁcant attention due to
its ability to generate synthetic data simulating realistic media such as images, text, audio and videos [6–9].
Nowadays, GANs are broadly studied and applied through academic and industrial research in diﬀerent domains
beyond media (e.g., natural language processing, medicine, electronics, networking, and cybersecurity). In short,
a GAN model is represented by two independent neural networks (the generator and the discriminator) that
compete to learn and reproduce the distribution of a real data set. After a GAN has been trained, its generator
can produce as many synthetic examples as necessary, providing an eﬃcient mechanism for solving the lack of
labelled data sets and potential privacy restrictions.

In this context, this work proposes the application of GANs to generate synthetic ﬂow-based network traﬃc
that mimicks cryptomining attacks and normal traﬃc. In contrast to most of the proposed works that are based
on data augmentation solutions, we aim to generate synthetic data that can fully replace real data (attacks and
normal traﬃc). Therefore, MDL models trained with synthetic data will obtain a similar performance to MDL
models trained with real data when both are tested and deployed in real-time scenarios.

This solution has two clear advantages: Firstly, addressing the existing shortage of publicly available network
traﬃc datasets containing attacks and normal traﬃc and secondly, avoiding the privacy violations that could
appear when real data is used in MDL training and testing processes.

In the light of these advantages, interesting applications can be devised. The ﬁrst one is related to MDL cross-
developments. Providing labelled data that does not incur in privacy breaches can foster cross-development of
MDL components by third parties. For example, a telecom provider developing ML-based components to be part
of an IDS, receives synthetic data from a telecom operator to train and validate these ML-based components.
As the synthetic data have been generated from real data using GANs, the ML component after training will
reach the desired level of performance and furthermore, no breach of data privacy will be raised as the telecom
operator is not sharing any real data with the telecom provider.

In addition, the solution proposed in this work is useful for application in Cyber-range exercises. Cyber
ranges are well deﬁned controlled virtual environments used in cybersecurity training as an eﬃcient way for
trainees (e.g. cyber-security personnel) to gain practical knowledge through hands on activities [10, 11].

Synthetic ﬂow-based network traﬃc and attacks generated by GANs can be used in cyber ranges to generate
diﬀerent data for a concrete type of exercise and avoid blue teams learning such exercise always with the same
data. Having trained a GAN model to replicate a given type of attack (or normal traﬃc), we can generate
as many attacks of such type as required. Therefore, even if the blue team repeats an exercise several times,
the analised attacks and normal traﬃc are not going to be exactly the same in each run of the exercise. In
addition, red teams can use GANs in penetration testing (pentest) exercises to generate realistic attacks that
never contain the same attack data even if the launched attacks are of the same type. Thus, the robustness
of an IDS against a type of attack can be evaluated launching many diﬀerent synthetic samples of the same
attack.

Furthermore, a Cyber-range can import from third parties data sets containing attacks and normal traﬃc
that are subject to privacy or anonymity restrictions. As the network data used in the exercises by the blue and
red teams are the synthetic ones generated by the GANs, no breach of privacy appears during the realization
of such exercises. Moreover, exporting attacks and normal data (e.g. to other platforms in a federated cyber

2

range) can be done without incurring in any privacy violation as the exported data to be shared with a third
entity are exclusively the synthetic network traﬃc generated by the GANs.

These ideas have been applied by the authors of the manuscript in the H2020 SPIDER project [12] that
proposes a cyber-range solution that is speciﬁcally designed to train cybersecurity experts in telecommunications
environments and covers all cybersecurity situations in a 5G network environment. As a novelty in cyber-ranges,
SPIDER brings a way to seamlessly integrate ML-based components and GANs to be used as part of blue and
red team toolboxes. The GAN models proposed in this manuscript will be used in SPIDER as the basic building
block of the Synthetic Network Traﬃc Generator to obtain synthetic network traﬃc data (attacks and well-
behaved connections) that reproduce the statistical distribution of real traﬃc to be used later in cyber range
exercises.

Proposal

To demonstrate the applicability of our proposal, we select a cryptomining attack scenario. Cryptomining is a
paradigmatic cryptojacking attack that is gaining momentum in these days. Cryptomining attacks concern the
network traﬃc generated by cybercriminals that create tailored and illegal processes for catching computational
resources from users’ devices without their consent to use them in the beneﬁt of the criminal for mining
cryptocurrencies. It has been shown that these malicious connections can be detected in real-time with decent
accuracy even at the very beginning of the connection’s lifetime by using an ML classiﬁer [13].

Our goal is to obtain WGAN synthetic traﬃc of suﬃcient quality to allow a complete replacement of real
data by synthetic samples during the training of a ML-based crytomining attack detector. This property ensures
that we will not violate any privacy restriction and sets our proposal apart from existing works that only propose
data augmentation solutions based on mixing real with synthetic data.

To generate ﬂow-based information that replicates normal traﬃc and cryptomining connections, we apply
two Wasserstein GANs [14] to generate both types of network traﬃc separately. Unlike current solutions, our
WGANs replicate not only already completed connections, but also connections at diﬀerent stages of their
lifetime. Regarding that successful GAN training is still an open research problem [15], we propose to evaluate
a set of GAN enhancements to measure their impact on the convergence of the WGAN training and the quality
of the synthetic data generated.

In addition, we propose two new metrics based on the L1 distance and the Jaccard coeﬃcient to measure
the quality of the synthetic data generated by GANs with respect to their similarity with real data. These
new metrics consider the joint distribution of all variables of the ﬂow-based data rather than the mean over the
distance of each variable as other works propose.

To the best of our knowledge and due to the ill-convergence of GANs, none of the existing works propose
a clear stopping criterion during GAN training to obtain the best performing synthetic data. To address this
problem, we propose a simple heuristic for selecting the best performing GANs when it is required to fully replace
real data with synthetic data for training MDL models. This heuristic exploits our experimental observations
in three aspects:

1. When synthetic data is used for training a MDL model, its performance is not well correlated with any
quality metric that can be applied to the synthetic data, and therefore, it is required to train and validate a
MDL model with a sample of the synthetic data to obtain the ML performance (e.g. F1-score on testing).
2. After having trained thousands of GANs models, we observed that the performance value we used (F1-
score) tends to oscillate during training, and therefore, stopping training because the cost function is not
decreasing or the quality of the data is not improving is not an appropriate criterion as performance could
improve after additional training. In order to obtain the best performing GAN we suggest to measure
GAN performance at the end of each mini-bath training.

3. Even if we select the best performing GAN for each type of traﬃc, it is not guaranteed that their joint
performance will be the best when both types of synthetic data are combined to fully replace the real data
during the training of MDL models.

The ﬁrst and second observations involve evaluating the performance of each generator obtained at the end
of a mini-batch training stage to select the best one. The third observation indicates that even when selecting
the best performing generator for each type of traﬃc, the performance obtained when we mix them may not be
the best, and therefore we propose a heuristic for ﬁnding the best performing combination. A random selection
of the intermediate GAN models obtained during training works well, requiring only a moderate number of
samples. Nevertheless, we observed that when F 1 − score on testing was used as the performance metric,
reducing the sample GAN universe to the top-10 best performing for each type of traﬃc produced similar
results requiring a signiﬁcantly fewer number of samples.

3

Experiments

To demonstrate the proposed solution, we ran an extensive set of experiments. The data sets used in our
experiments were previously generated in a realistic network digital twin called the Mouseworld lab [16, 17].
In this lab, we launched real clients and servers that interacted with other hosts located in diﬀerent places in
the Internet and collected the generated traﬃc composed of encrypted and non-encrypted ﬂows (normal traﬃc
and cryptomining connections). A set of 59 statistical features were extracted from each TCP connection in
real time each time a new packet was received. We carefully selected a reduced set of 4 features for our GAN
experiments.

We trained independently two WGANs, one for each type of traﬃc, conﬁguring them with a rich set of
hyperparameters. We performed a blind random search in the hyperparameter space of each WGAN and to
select the best conﬁguration of hyperparameters we used the F1-score obtained in a nested ML-model that was
executed after each train epoch. For each type of traﬃc, we selected the WGAN that obtained the best F1-score
for the nested classiﬁer in any of its epochs. In addition, we compute the two proposed metrics (L1 distance
and Jaccard) on each WGAN to analyse the similarity of the synthetic traﬃc they are generating with respect
to the real data. Given that for each winning WGAN, we have as many intermediate generators as the train
steps we run, the previously proposed heuristic is run to choose pairs of generators that produce good results
in a global nested evaluation of both WGANs. Thus, we avoid testing all possible combinations of generators
from the two WGANs.

To measure the quality of the synthetic data generated by our GANs, we train a ML classiﬁer for detecting
cryptomining connections in real-time using only a combination of the synthetic traﬃc generated by the two
WGANs (normal traﬃc and cryptomining attacks). Another ML classiﬁer conﬁgured with the same set of
hyperparameters is trained using real data. Both models are tested against a second set of real data to measure
whether the ML model trained exclusively with synthetic data performs at the same level than the model
trained with real data. As a baseline for the quality of synthetic traﬃc, we use a naive approach based on a
noise generator added to the averages of the variables and as an upper bound, we consider the results obtained
with real traﬃc. In addition, we run the same experiments but adding some extensions to the standard WGAN
conﬁgurations to analise whether the performance of the nested ML or the convergence of the training process
increase when these heuristics are considered.

Paper structure

The rest of the paper is structured as follows: Section 1 presents the related work in this topic and the
manuscript contributions are detailed in subsection 1.1. The problem setting is shown in Section 2. The
proposed model is depicted in Section 3 and the model architecture, generator custom activation functions and
WGAN enhancements and heuristics are explained in speciﬁc subsections. The proposed quality and similarity
metrics are detailed in Section 4. The empirical evaluation using standard WGANs is presented in Section 5
and the eﬀects of several improvements and variants are shown in Section 6. We conclude and summarize with
some interesting open research questions in Section 7.

1 Related work

Over the past, a few diﬀerent research works have targeted the generation of synthetic network traﬃc using
GANs [18], although the majority of them only propose data augmentation solutions that are not applicable in
scenarios in which data privacy must be guaranteed as they use a combination of real and synthetic data.

A recent work [19] proposes a GAN approach to generate network traﬃc that mimics the traﬃc from
Facebook’s chat to modify the behavior of a real malware imitating a normal traﬃc proﬁle based on input
parameters from the GAN . This approach is able to modify malware communication in order to avoid detection.
MalGAN [20] is a GAN that can generate synthetic adversarial malware examples, which are able to bypass
black-box machine learning based detection models. Bot-GAN [21] presents a GAN-based framework to augment
botnet detection models generating synthetic network data. An augmentation method based on AC-GANs was
proposed to generate synthetic data samples for balancing network data sets containing only two classes of
traﬃc [22]. No details are given of either the structure of the GAN or how the experiments were evaluated,
making it impossible to reproduce the proposed solution or measure the synthetic data quality. Another similar
work [23] used GANs and the “ISCX VPN nonVPN" traﬃc dataset [24] but without proposing any evaluation
method to contrast the obtained results.

A Deep Convolutional Generative Adversarial Network (DCGAN) was recently proposed as a semisupervised
data augmentation solution [25]. Samples generated by the DCGAN generator as well as unlabeled data are

4

used to improve the performance of a basic CNN classiﬁer trained on a few labeled samples from the "ISCX
VPN nonVPN" traﬃc dataset. Another work [26] presents a Cycle-GAN to augment and balance the ADFA-LD
dataset containing system calls of small footprint attacks . Foot-prints are converted into images to be processed
by Cycle-GANs in the standard way. A data augmentation method using the NSL-KDD data set is proposed
to generate adversarial attacks that can deceive and evade an intrusion detection system [27]. No details are
provided on the network structure and hyperparameters used during training, which prevents experimental
replicability. In addition, the proposed solution only uses ﬂow statistics after the connection has ﬁnished, which
restricts its applicability to forensic scenarios.

PAC-GAN [28] proposes a diﬀerent approach to generate packets instead of ﬂow-based data. Authors assume
that the bytes in a packet have a topological structure in order to apply convolutional neural networks in the
generator. The generator implements 3 types of request packets (Ping request, HTTP Get and DNS request).
Only request packets are generated because the quality metric applied only counts the number of responses
received when these synthetic packets are sent to a server. Unfortunately, this metric cannot detect whether
the GAN is learning the input packets by heart and simply replicating them at the output. The limited number
of packet types that can be generated, together with the fact that they do not propose a realistic metric for
measuring the quality of the data generated, discourages the use of this solution in realistic environments.

A close work to our research is proposed in [29] where three GAN variations are used to generate ﬂow-based
network traﬃc information in unidirectional Netﬂow format. As GANs generate continuous values, this work
presents a method inspired by Word2Vec from the NLP domain to transform categorical variables (e.g., IP
addresses) into continuous variables. The dataset used as input to the proposed GANs is a publicly available
CIDDS-001 data set [30] that contains a mixture of normal traﬃc and attack connections. Unfortunately, only
one record per connection is stored containing the ﬁnal status of ﬂow variables, which precludes its application
in real-time IDS scenarios where ﬂow-based information is needed to be generated recurrently along the life
of the connection to detect malicious ﬂows in their early beginning. In addition, the authors propose to use
as quality metrics the Euclidean distance between variables and manual techniques such as visual inspection
and domain value checking done by experts that are impractical from a scalability perspective. Moreover, the
Euclidean distance measures each variable separately when what needs to be measured is the distance of the
synthetic distribution from the real distribution but considering both as multivariate distributions. Finally, the
authors do not propose as validation metric a performance evaluation of a ML-based attack detector trained
with the synthetically generated ﬂow-based data.

Cyber ranges are well deﬁned controlled virtual environments used in cybersecurity training as an eﬃcient
way for trainees to gain practical knowledge through hands on activities. Several works have been proposed to
study and classify the concept of a cyber range covering diﬀerent types of cyber ranges and security testbeds
[31–33]. Although in recent years Artiﬁcial Intelligence and Machine Learning based technologies have been
started to be actively used for cyber defense purposes [34–36], their inclusion in cyber ranges is still in their
infancy.

The H2020 SPIDER project [12], in which we are applying the main results of this manuscript, proposes
a cyber range solution, covering all cybersecurity situations in a 5G network environment, where ML-based
components can be seamlessly integrated in blue and red team toolboxes. The SPIDER framework is able to
produce oﬀensive and defensive artifacts using Synthetic Network Traﬃc Generators that can emulate speciﬁc
types of attacks and normal traﬃc. To the best of our knowledge, only the H2020 SPIDER project is proposing
to apply GANs to generate such synthetic attacks and normal traﬃc that can be used by red and blue teams.
Thus, red teams can use the synthetic attacks to break the robustness of an IDS during pentest exercises and blue
teams can learn how to reconﬁgure IDS defenses when faced with diﬀerent synthetic attacks. This novel feature
circumvents the current limitations of cyber range commercial products that, using a ﬁxed set of previously
stored attacks, only generate slight variations of these attacks mainly based on adding noise combinations to
the mean values.

1.1 Contribution

The main diﬀerences of the previous proposals and our work are: (i) Unlike the prevailing existing data augmen-
tation solutions, we obtain synthetic ﬂow-based traﬃc that can fully replace real data and therefore, this solution
can be applied in scenarios where data privacy must be guaranteed, (ii) existing metrics measure the diﬀerences
of synthetic and real data independently for each each variable, but we propose a set of new metrics to measure
more realistically the similarity of the two as joint multivariate distributions, (iii) due to the ill-convergence of
the GAN training, none of the published papers mention a clear stopping criterion during training for selecting
the best performing GAN, but we propose a simple heuristic that selects such GAN by measuring the perfor-
mance of a ML task in which real data is fully replaced by synthetic data for the task training, (iv) the proposed

5

Figure 1: GAN architecture used as reference model.

heuristic is extended to eﬃciently choose generators from diﬀerent GANs to generate a combination of high
quality synthetic data to be used in the same ML task – in our scenario we generate a mixture of two diﬀerent
types of synthetic traﬃc to train a ML-based cryptomining attack detector–, (v) our solution does not generate
one ﬂow-based element at the end of the connection, but a set of elements representing diﬀerent instants of a
connection throughout its lifetime, which allows its usage and deployment in real-time scenarios, and (vi) we
selected a recently appeared cryptomining attack as a paradigmatic use case to demonstrate the feasibility of
our proposal and how it can be integrated into a next-generation cyber range.

2 Problem setting

The framework of this work is a network environment in which real clients and servers compete to exchange
diﬀerent types of traﬃc sharing Internet connectivity. On one side, normal traﬃc clients interact with servers
(web surﬁng, video and audio streaming, cloud storage, ﬁle transfer, email and P2P among others) but, on
the other side, cryptomining clients connect to real mining pools generating a certain amount of cryptomining-
related traﬃc that competes with the normal traﬃc for processing time and/or bandwidth. In this context,
cybercriminals can populate their cryptocurrency wallets by using botnets or run illegal processes in browsers
to surreptitiously add victims’ computer resources without their consent and spend computational resources for
the criminal’s beneﬁt.

For this reason, it is crucial to develop a system capable of identifying cryptomining-related traﬃc so that
an attack detector can limit the bandwidth dedicated to this type of traﬃc or, in a extreme situation, block
the connection. To be usable for this purpose, this detection must be conducted in real-time. Nowadays,
some solutions for cryptomining detection are based on ML-based binary classiﬁers, such as random forest,
decision trees or logistic regression. In order to prepare these models, it is necessary to feed them with lots
of labelled data. With this data, during the so-called training process, an optimization algorithm adjusts the
internal parameters of the model to extract the patterns that identify the cryptomining traﬃc with respect to
the normal traﬃc so that this knowledge can later be used to classify the traﬃc. Despite this is the customary
procedure in ML, in this scenario this solution poses an important privacy problem. Indeed, the data needed
for the training process is typically an excerpt of the real traﬃc crossing the node, for which a thorough oﬄine
analysis of its nature has to be conducted to identify the cryptomining traﬃc.

Regarding that companies are reluctant to share their data with third party developers and governments
concerned about privacy issues are imposing limitations to telecom providers for accessing user data or inspecting
packet payload, the aim of this work is to construct a generative model able to substitute these data belonging
to real clients by fully synthetic data with no information of the original clients. With this solution at hand, the
Internet service providers are able to generate as much new anonymous data as needed to train their ML models
or share it with third party developers without generating any privacy breach or jeopardizing the privacy of
their users.

The generated data should be as faithful to the real data as possible in the sense that they should provide
signiﬁcant information to the ML models to extract the underlying patterns distinguishing between normal and
cryptomining traﬃc. In this sense, the performance of the generative models will be evaluated based on their
ability to create high-quality synthetic data. Explicitly, the generated data must lead to a similar performance
of the ML-based models when tested against models trained with real traﬃc, even though they were trained
with synthetic data instead of real data.

6

As the original dataset to be replicated, we shall use a collection of ﬂow-based statistics extracted from 4
hours of real traﬃc that was generated in a realistic network digital twin called the Mouseworld lab. This data
was gathered in a controlled location of the Internet in two diﬀerent instants of time: the ﬁrst gathering will be
used as the training dataset to be replicated and the second will be stored as the testing dataset. The details
of this process can be found in Section 5.1.

For each traﬃc ﬂow (TCP connection), we compute a set of 59 statistical variables describing the ﬂow.
The computation is carried out each time a packet for this connection is received or when a timeout ﬁres.
For our GAN experiments, 4 variables were selected: (a) number of bytes sent from the client, (b) average
round-trip time observed from the server, (c) outbound bytes per packet, and (d) ratio of packets_inbound /
packets_outbound. It is worth noting that other sets of statistical variables are representatives of a ﬂow (TCP
connection) and may be computed. In fact, the Tstat tool [37] used for this task extracts and computes a total
of 140 variables. The four chosen features were selected as they exhibit several interesting properties for our
generative experiments.

• These four features themselves lead to a good performance in a standard ML classiﬁer when used to

classify between normal and criptomining traﬃc.

• Each feature exhibits a diﬀerent statistical behaviour, which allows to demonstrate that the proposed

solution can replicate a variety of data distributions and not only the normal one.

• The average value of each feature in the two types of traﬃc (normal and cryptomining) were close and,
therefore, the traﬃc ML classiﬁer needed to learn something subtler from the data features than their
means. This property allows us to quantify the improvement obtained by using the proposed generative
model versus a naïve Gaussian generator that produces data around a mean value. This is particularly
interesting when we want to replicate data distributions that do not follow a normal distribution or have
some hard domain constrains such as non-negativity or discrete distributions, such as those fulﬁlled by
the chosen features.

3 Proposed model

The solution we propose to address the problem described in Section 2 is based on Generative Adversarial
Networks (GANs), as introduced by Goodfellow [5]. A GAN network is a generative model in which two
neural networks compete to improve their performance. To be precise, we have a d-dimensional random vector
X : Ω → Rd, deﬁned on a certain probability space Ω, that returns instances of a certain phenomenon that we
would like to replicate. Usually, we have that Ω = {x1, . . . , xN } is a large dataset of vectors xi ∈ Rd and X just
picks randomly (typically uniformly) an element xi ∈ Ω. Moreover, in standard applications of GANs, we have
that the instances xi are images represented by their pixel map and the objective of the GAN is to generate
new images as similar as possible to the ones in the dataset.

For this purpose, a classical GAN proposes to put two neural networks to compete: a neural network G,
called the generative network, and another neural network D, called the discriminant. The discriminant is a
network computing a function D : Rd → R that is trained to solve a typical classiﬁcation problem: given x ∈ Rd,
D(x) is intended to predict whether x = X(ω) for some ω ∈ Ω or not i.e. whether x is compatible with being
a real instance or it is a fake datum. Observe that, along this paper, we will follow the convention that D(x)
is the probability of being real, so D(x) = 1 means that D is sure that x is real and D(x) = 0 means that
D is sure that x is fake. On the other hand, the generative network computes a function G : Rl → Rd. The
idea of this function is that Rl will be endowed with a probability distribution λ, typically a spherical normal
distribution or a uniform distribution on the unit cube. The probability space Λ = (Rl, λ) is called the latent
space and the goal of the generator network is to tune G in such a way that the random variable G : Λ → Rd
distributes as similar as possible to X.

The competition appears because the networks D and G try to improve non-simultaneously satifactible
objectives. On the one hand, D tries to improve its performance in the classiﬁcation problem but, on the other
hand, G tries to generate as best results as possible to cheat D. To be precise, observe that the perfect result
for the classiﬁcation problem for D is D(x) = 1 is x is an instance of X and D(x) = 0 if not. Hence, the mean
error made by D is

E = EΩ [1 − D(X)] + EΛ [D(G)] = 1 − EΩ [D(X)] + EΛ [D(G)] ,
where EΩ and EΛ denote the mathematical expectation on Ω and Λ respectively. In this way, the objective of
D is to minimize E while the objective of G is to maximize it. It is customary in the literature to consider as

7

objective the function 1 − E and to weight the error with a certain concave function f : R → R. In this way,
the ﬁnal cost function is

F(D, G) = EΩf [D(X)] + EΛf [−D(G)]

and the objective of the game is

min
G

max
D

F(D, G) = min

G

max
D

EΩf [D(X)] + EΛf [−D(G(Z))] .

Typical choices for the weight function f are f (s) = − log(1+exp(−s)), as in the original paper of Goodfellow [5],
or f (s) = s as in the Wasserstein GAN (WGAN) [14]. This operation method can be depicted schematically as
in Figure 1.

Despite the simplicity of the formulation of the cost function, the optimization problem is far from being
trivial. The best scenario would be to obtain a so-called Nash equilibrium for the game, that is, a pair of
discriminant and generative networks (D0, G0) such that the function D (cid:55)→ F(D, G0) has a local maximum at
D = D0 and the function G (cid:55)→ F(D0, G) has a local minimum at G = G0. In other words, at a Nash equilibrium,
neither D or G can improve their result unilaterally. Based on this idea, the classical training method as
proposed by Goodfellow in [5] is alternating optimization of D and G using classical gradient descend-based
backpropagation. Despite that this method may provoke some convergence issues, as mentioned below, it is a
widely used learning algorithm due to its simplicity and direct implementation using standard machine learning
libraries like Keras or TensorFlow.

To be precise, the algorithm proposed by Goodfellow suggests to freeze the internal weights of G and to use it
to generate a batch of fake examples from Λ. With this set of fake instances and another batch of real instances
created using X (i.e. sampling randomly from the dataset of real instances), we train D to improve its accuracy
in the classiﬁcation problem with the usual backpropagation (i.e. gradient descent) method. Afterwards, we
freeze the weights of D and we sample a batch of latent data of Λ (i.e. we sample randomly noise using the
latent distribution) and we use it to train G using gradient descent for G with objective f (−D(G(z))). We can
alternate this process as many times as needed until we reach the desired performance.

As noted in a recent work [38], the game to be optimized is not a convex-concave problem, so in general the
convergence of the usual training methods is not guaranteed. Under some assumptions on the behaviour of the
game around the Nash equilibrium points, it is proved that the usual gradient descent optimization is locally
asymptotically stable [38]. However, the hypotheses needed to apply this result are quite strong and seem to
be unfeasible in practice. For instance, it has been published an example of a very simple GAN, the so-called
Dirac GAN, for which the usual gradient descend does no converge [39].

For this reason, several heuristic methods for stabilizing the training of GANs have been proposed such as
feature matching, minibatch discrimination, and semi-supervised training [40] as well as approaches changing the
weight function f as the Wasserstein GAN [14]. The most promising approaches are based on the modiﬁcation
of the usual alternating gradient descending optimization such as the introduction of instance noise [41, 42]
and regularization methods based on gradient penalty [43]. A recent work [44] proposes a formal study of the
dynamics of the GAN training process, but due to the complexity of the analysis, two simpliﬁed neural network
architectures and a torus space were considered. For a thorough analysis of the diﬀerent methods for stabilizing
the training of GANs, see [39].

The study of the type of network and the best architecture for D and G have been intensively studied in the
literature. Convolutional neural networks for D and deconvolutional networks for G seem to be good choices
for image generation and discrimination [45], as the set of variables (i.e pixels) in an image have topological
information that can be exploited by these convolutional networks. However, due to their simplicity, fully
connected NN (multilayer perceptrons) have also been applied to GANs with pretty good performance [5], in
particular when no topological information is contained in the variables to be replicated.

3.1 Architecture

Aiming to mimic two types of behaviour (cryptomining attacks and well-behaved connections), in preliminary
experiments, we adopted a well-known conditional GAN model, the so-called Auxiliary Classiﬁer GANs (AC-
GAN) [46], as the architecture to generate at the same time the two types of traﬃc variables. As this strategy
did not produce an adequate performance when replicating the two types of traﬃc and generated signiﬁcant
oscillations in the convergence process, we opted to use a diﬀerent approach. We conjecture that the oscillatory
behaviour could be caused by the fact that cryptomining connections follow a very speciﬁc statistical pattern,
and on the contrary, the normal traﬃc connections are made of a mixture of many diﬀerent connections that
globally exhibit a nearly random behaviour. In addition, there is a great imbalance in the number of data for
each of the two types of connections, which might incline the GAN training to obtain one distribution closer to
the real data than the other.

8

Assuming that the two types of traﬃc are independent each other and therefore it is not necessary to use
one for the synthetic generation of the other, we ﬁnally proposed to train independently two standard GANs,
one for normal traﬃc (i.e. well-behaved connections) and the other for the cryptomining connections. The
reference architecture for these GANs is shown in Figure 1. As previously explained, this model is composed of
two networks, a generator and a discriminator, competing between them. The generator is input with a random
noise vector and produces a synthetic sample. The discriminator receives real and fake samples as input and
tries to classify them appropriately in their correct category. During training, the goal of the generator is to
learn how to produce fake samples that can be classiﬁed as real by the discriminator. On the contrary, the goal
of the discriminator is to learn how to diﬀerentiate real from fake examples.

To get rid of the mode collapse problems that frequently appear during GAN training, we adopted as
a reference model the WGAN architecture [14], in which a Wasserstein loss function is used as loss function
instead of a standard cross-entropy function. A detailed explanation of why using W-GANs over standard GANs
enhances the convergence during the GAN training process can be found in [47]. In addition to the replacing of
the loss function, we tested two diﬀerent strategies to enforce the required Lipschitz constraint in this function.
Initially, a radical weight clipping strategy, as suggested in [14], was applied. Later, we replace weight clipping
with a more elaborated gradient penalty approach [48]. It is worth noting that in our experiments, none of
them produced a signiﬁcant enhancement in the convergence of the GAN training and in many occasions, we
observed that the gradient penalty heuristic even produced signiﬁcant oscillations. Therefore, we ﬁnally chose
a WGAN architecture with no additional strategy to enforce the Lipschitz constraint and the discriminator
was optimized using only small learning rates and a new adaptive mini-batch procedure as heuristics to avoid
reaching mode collapse situations.

We selected fully connected neural networks (FCNNs) as the architectural model for both the discriminant
and the generative networks. This decision was based on the observation that the statistical nature of the 4
features to be synthetically replicated did not exhibit any topological structure or time relationship among them
and therefore, convolutional (CNNs) or recurrent networks (e.g. LSTMs) respectively would not provide any
advantage with respect to FCNNs. We observed in preliminary experiments that very deep networks with a
large number of hidden layers or units did not generate signiﬁcant improvements in performance and on the
contrary, they enlarged convergence times and produced non-negligible oscillations in the convergence during
the training process. This eﬀect could be explained by the fact that the cryptomining classiﬁcation problem
does not need very complex models to obtain a decent accuracy [13]. Therefore, we selected a moderate number
of hidden layers (between 3 and 5) for generator and discriminator networks.

To provide the generator with more complex nonlinear capabilities to learn how to fool the discriminator,
we used hyperbolic tangent and Leaky-ReLU functions [49] as activation functions in the neurons of its hidden
layers. In the case of the discriminator only LeakyReLUs were used. Regarding that the discriminator does not
play as a direct critic as in standard GANs but as a helper for estimating the Wasserstein distance between real
and generated data distributions, the activation of its output layer is a linear function. As the generator has to
produce synthetic samples close to the real data, we considered two possibilities for the activation functions of
its output layer: linear and domain-customized functions. A detailed explanation of the rationale and trade-oﬀs
of using domain-customized activation functions instead of linear functions is presented in the next subsection
3.2.

In addition, we selected a well-known heuristic for training WGANs from the literature [48] and designed some
new ones to test if they oﬀered any advantage in the convergence of the training process or in the performance
of the synthetic data during the attack detection process. In subsection 3.3 we detail the applied heuristics: (i)
adaptive mini-batches on training, (ii) noise addition, (iii) multiple-embedding and (iv) complementary traﬃc
addition.

3.2 Custom activation function for the real data domain

An important issue in the generation of synthetic replicas of network traﬃc variables is that real data are
sometimes not normally distributed. In general, telecom domain variables used in ML are usually statistical
data representing the evolution of ﬂow variables such as counters, accumulators, and ratios that always take
positive values. In our real-time experiments, ﬂows are monitored periodically from start to ﬁnish and therefore,
the collected values for some of these variables are not normally distributed and tend to follow an exponential
distribution with many occurrences of values near to 0 and a long tail of large values appearing very rarely.
Figure 2 shows the frequency distribution histograms of the four variables extracted from the normal traﬃc and
cryptomining connections we used in our experiments. Generator networks usually have a linear activation in
the neurons of their output layer, which produces output variables following a normal distribution with mean
that of the distribution of the real data. When the real data follow a diﬀerent distribution (e.g. exponential),

9

(a) Label 0. Feature 1

(b) Label 0. Feature 2

(c) Label 0. Feature 3

(d) Label 0. Feature 4

(e) Label 1. Feature 1

(f) Label 1. Feature 2

(g) Label 1. Feature 3

(h) Label 1. Feature 4

Figure 2: Frequency distribution histogram of the 4 variables extracted from normal (Label 0) and cryptomining
(label 1) traﬃc. Label 0 (6a, 6b, 6c, 6d and 6e) and label 1 (6f, 6g, 6h, 6i and 6j)

using linear activation functions in the output layer of the generator can produce synthetic data outside the
domain of the real data. For example, if we consider a variable representing an accumulator, only positive values
are possible in the domain of real data. However, the generator will produce synthetic data with the same mean
as the real data distribution (exponential) but following a normal distribution that contains negative data not
existing in the real domain (elements of the leftmost part of the bell-shaped distribution). This anomaly can
be observed graphically by superimposing the exponential and normal density curves on the mean value. A set
of points appears on the leftmost part of the bell-shaped curve but not on the exponential curve. These points
will take negative values and therefore, they do not exist in the real data domain. Furthermore, the bell-shaped
curve is not containing the rare elements appearing in the rightmost part of the long tail of the real data. It
is worth noting that this problem has not attracted much attention in the literature since most of the work on
GANs has been done for image generation where a normal distribution of pixel values is tolerated quite well by
the human eye.

To mitigate this problem, we propose to use speciﬁc activation functions in the output neurons of the genera-
tor to adjust as much as possible the data distributions of the generator outputs to the statistical distribution of
the real variables and thus, avoid the generation of negative values outside the domain of such variables. To try
to replicate variables that follow an exponential distribution, we propose to use ReLU functions in the generator
as activation functions of the neurons at the output layer. The ReLU function only generates positive values due
to its non-linear behaviour (the output is the input value for positive values and 0 for negative values). In order
to provide a smoother transition at values close to 0, we experimentally observed that a Leaky-ReLU function
with a very small slope for negative values performed better than a pure ReLU function. It is worth noting
that the use of a Leaky-ReLU function will generate a marginal number of samples with negative values that
can be easily ﬁltered out later in a post-processing step. Nevertheless, further research work should investigate
new activation functions to perfectly match the statistical distribution of real variables.

3.3 Heuristics

We designed three novel mechanisms and applied a well-known heuristic based on adding noise to the discrim-
inator and tested each of them to see if they could impact on the training convergence or the quality of the
generated synthetic data. These heuristics are the following: (i) an adaptive number of mini-batch cycles are
applied to the training of discriminator and generator networks to avoid the occurrence of the so-called collapse
mode and balance the learning speed of both networks, (ii) diﬀerent types of noise are added to the discriminator
inputs to slow down its learning speed, (iii) a multi-point embedding of a single class is added to the input
layer of the generator for augmenting the variety of the latent noise vector and (iv) real traﬃc of the class not
modeled in the GAN is added jointly with the set of fake examples to slow down the learning process of the
discriminator.

10

3.3.1 Adaptive mini-batches on training

This heuristic aims to avoid the occurrence of the so-called collapse mode during training when one network
learns faster than the other, which ﬁnally produces that the slower network cannot learn any more. This
anomaly was observed in preliminary experiments when a generator was not able to fool the discriminator with
any synthetic example or on the contrary, when the discriminator could not identify any synthetic example as
fake.

As previously described, the standard GAN training consists of a mini-batch stochastic gradient descent
training algorithm in which each mini-batch training consists of the execution of one train_batch for the
discriminator followed by one train_batch for the generator.
In this way, the discriminator and generator
networks are trained in such a way that when one network is trained the learning of the other is blocked. We
propose a modiﬁed training procedure to avoid the blocking problems that we observed during preliminary
experiments when we used such standard training process. The modiﬁed training procedure is described in
Figure 3 and consists on executing a variable number of train_batch for the discriminator and generator
networks. Each network is trained until a minimum value of successful elements are correctly classiﬁed at the
end of the mini-batch training. For the discriminator, we force extra train_batch cycles until the ratios of
real and fake examples that are correctly classiﬁed are greater than two pre-established thresholds. For the
generator, the ratio of incorrectly classiﬁed samples (i.e., fake samples that are considered as real samples by
the discriminator) must be greater than a predetermined threshold at the end of the mini_batch training.
Otherwise, additional training cycles are added. This heuristic avoids situations in which the discriminator or
the generator learns faster than its opponent and ﬁnally blocks the learning evolution of its counterpart. In
this situation, some of these ratios reach zero and the slow-learning network is no longer able to learn anymore
during subsequent training steps and therefore, improve these ratios.

During our experiments, we observed that using moderate ratios of 0.1 or greater for the generator produced
oscillations in the convergence of the training process that disappeared when smaller values were used (0.05 for
the generator and 0.1 for both ratios of the discriminator). Although no signiﬁcant improvement on performance
or convergence speed was observed in our experiments when adaptive mini-batches were activated and these
small ratios were used, the previously observed blocking situations disappeared.

repeat

train_batch(discriminator)
preds ← discriminator.predict(sample(real, synthetic))

Figure 3 Enhanced GAN training procedure with adaptive mini-batches
1: procedure GAN_train_batch
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: end procedure

train_batch(generator)
preds ← discriminator.predict(generator.predict(noise))
ratio_f ake_pass ← len(preds = real_label)/len(preds)

until T P (preds) and T N (preds) are in range
repeat

until ratio_f ake_pass > min_ratio_f ake_pass

3.3.2 Noise addition

This heuristic aims to slow down the learning rate of the discriminator in order that the generator can learn
how to fool the discriminator in each mini-batch training cycle. It is worth noting that this heuristic works in a
complementary way to the adaptive learning rates that the optimization algorithm applies to each mini-batch
training.

In order to avoid disjoint distributions, the authors in [48] suggested to add continuous noises to the inputs
of the discriminator to artiﬁcially "spread out" the distribution and to create higher chances for two probability
distributions to have overlaps. To this end, we add several types of noises that are conﬁgured as hyperparameters
during the training. Three diﬀerent types of noise (Uniform or Gaussian with mean 0 and conﬁgurable standard
deviation) can be stacked: noise added to (i) fake examples, (ii) to all (real and fake) examples; and (iii) a
conﬁgurable percentage of fake and real labels are changed to its opposite value.

Our preliminary experiments revealed that even moderate amounts of noise did not allow the generator to
learn adequately and the quality of the obtained synthetic data was really poor as reﬂected by the distance
metrics with respect to the real data. Moreover, when the synthetic data was used for substituting real data in
the training of a traﬃc classiﬁer, the F1-score obtained was also smaller. When a large amount of noise is added,
the rightmost values of the long tail distribution of real variables tend to disappear in the discriminator and
therefore, they will not be learnt by the generator. In fact, it can be observed that the synthetic distributions of

11

these variables tended to be grouped around the mean of the variables. On the contrary, small amounts of noise
did not produce any bad eﬀect on the convergence or quality of synthetic data, but neither did they generate
any signiﬁcant improvement.

3.3.3 Multi-point single-class embedding

We designed a new way to input latent noise to the generator. Instead of generating a noise vector from a
uniform distribution in a large interval of values [−K, K], we provided latent vectors generated uniformly at
random from smaller intervals of values and centered at diﬀerent points in the latent space that also were
selected uniformly at random. An additional hyperparameter allows to also train the optimal location of the
centroids in the latent space. The rationale of this heuristic was to explore whether it was easier to train a GAN
with small random bubbles of latent vectors than to use a single latent vector from a larger range of random
values.

We implemented the centroids of these random bubbles using an embedding layer of the same dimension
than the latent vector. The input to the embedding layer was a number representing the bubble and the output
were the coordinates of this bubble in the latent space. The latent noise is generated by drawing a sample from
a normal or uniform distribution. This value is added to the bubble centroid coordinates to generate a point
(latent vector) around this location to be fed into the generator network. The embedding layer weights can also
be learnt during training to ﬁnd an optimal location of the bubbles in the latent space, or they can be chosen
at random and frozen during GAN training.

Although this heuristic was only superﬁcially investigated and we did not observe any signiﬁcant improve-
ment in convergence or synthetic data quality, future work should explore more carefully the implications of
using these latent bubbles.

3.3.4 Complementary data

When training the discriminator using network traﬃc of type X, it is possible to mix a conﬁgurable ratio of
real examples of the other type of traﬃc Y with fake examples obtained from the generator. The rationale
of this heuristic was to avoid that the discriminator overﬁtted on the fake data produced by the generator
at each mini-batch as it has to also learn to diﬀerentiate the other type of real traﬃc Y. During preliminary
experiments, we did not observe any signiﬁcant advantage when this heuristic was included with diﬀerent ratios
of Y examples ranging from 0.1 to 0.5.

4 Performance metrics

We propose to evaluate GANs performance using two diﬀerent types of metrics. The ﬁrst set of metrics is
inspired by the L1 functional distance and the Jaccard coeﬃcient and aim to quantify the similarity of the
synthetic data with respect to the real data from a statistical perspective and considering the joint distribution
of data features. On the other hand, the second set of metrics attempts to quantify the performance of synthetic
data when it is used as a substitute for real data in the training of a ML classiﬁer that is trained to distinguish
between normal and cryptomining traﬃc. These two types of metrics will be used to compare the similarity
between real and synthetic distributions and will also be applied to implement a stopping criterion for GAN
training to select generators that produce high-quality synthetic data.

To the best of our knowledge, this is the ﬁrst time that L1 metric and Jaccard coeﬃcient are used for deﬁning
metrics to compare synthetic and real data in GANs and furthermore, there is no other work that proposes to
use the two types of metrics to implement a stopping criterion for GAN training.

4.1 L1-metric and Jaccard index
The ﬁrst two metrics we introduce try to measure the diﬀerence between the probabilistic distribution of the
real data and the one of the synthetic data. They are based on two well-known statistical coeﬃcients applied
for hypothesis testing and probabilistic distances.

For the convenience of the reader, we brieﬂy review some relevant deﬁnitions. Suppose that X and Y are
two independent continuous d-dimensional random vectors with probability density functions fX , fY : Rd → R.
To measure the distance between X and Y , we can consider the L1-metric between their density functions as

dL1(X, Y ) =

(cid:90)

Rd

|fX (s) − fY (s)| ds.

12

Notice that dL1 (X, Y ) = 0 if and only if fX = fY almost sure and thus X = Y almost sure.

Additionally, we can also compare the supports of X and Y through the standard Jaccard coeﬃcient [50].
Let supp(fX ) be the support of the function fX , that is, the closure of the set of points s ∈ Rd such that
fX (s) (cid:54)= 0. Then, the Jaccard index of X and Y is given by

J(X, Y ) =

|supp(fX ) ∩ supp(fY )|
|supp(fX ) ∪ supp(fY )|

,

where |A| denotes the Lebesgue measure of a measurable set A ⊆ Rd. Notice that supp(fX ) ∩ supp(fY ) ⊆
supp(fX ) ∪ supp(fY ) so 0 ≤ J(X, Y ) ≤ 1 and, the larger the index, the more similar the supports. Indeed,
perfect agreement of the supports is achieved if and only if J(X, Y ) = 1.

Nevertheless, in this form these ideas can only be applied theoretically in a scenario where the density
functions are perfectly known. This obviously does not hold in a practical situation. However, the previous
deﬁnitions can be straightforwardly extended to the sampling setting by replacing the probability density
function by the histogram of a sample.

Suppose that we have samples x1, . . . , xn of a random vector X, with xi = (x1

i ). From them, we can
estimate the density function of X through the histogram function hX : Rd → R. For this purpose, choose a
partition of supp(fX ) ∪ supp(fY ) into d-dimensional cubes

i , . . . , xd

supp(fX ) ∪ supp(fY ) =

(cid:96)
(cid:71)

k=1

Ck.

A common choice for this partition is constructed as follows. Let mj = mini(xj
i ) be the
maximum and minimum of the estimated support of the j-th component of X. Take an uniform partition
mj = sj
w = M j of the interval [mj, M j]. Then, the cubes of the partition are given by the
product of intervals Ci1,i2,...,id = [s1

i ) and M j = maxi(xj

) for 1 < i1, . . . id ≤ w.

1 < . . . < sj

) × . . . × [sd

0 < sj

) × [s2

i1−1, s1
i1

i2−1, s2
i2

id−1, sd
id

In any case, given a partition Ck, we deﬁne the histogram function to be

hX (s) =

1
n

(cid:96)
(cid:88)

(cid:32) n
(cid:88)

k=1

i=1

(cid:33)

χCk (xi)

χCk (s),

(1)

where χCk : R → R is the characteristic function of the cube Ck, that is, χCk (s) = 1 if s ∈ Ck and is 0 otherwise.
In other words, if s belongs to the bin Ck, then hX (s) is the average of the number of samples xj lying in the
d-dimensional cube Ck. Recall that the integral of hX is strongly related to the empirical cumulative probability
function which, by the Glivenko-Cantelli theorem [51], converges almost surely to the real cumulative probability
function. In this way, for large samples, it may be expected that hX estimates rather faithfully the real density
function fX .

In particular, this histogram function allows us to estimate the aforementioned metrics. Suppose that we
have samples x1, . . . , xn and y1, . . . , ym of random variables X and Y , respectively. Choose a common partition
of the union of the supports of the samples. Then, we deﬁne the sampling L1-metric to be
{Ck}(cid:96)

k=1

dsmp
L1 (X, Y ) =

(cid:90)

Rd

|hX (s) − hY (s)| ds =

(cid:96)
(cid:88)

k=1

|hX (Ck) − hY (Ck)|Vol(Ck) = L

(cid:96)
(cid:88)

k=1

|hX (Ck) − hY (Ck)|.

Here, Vol(Ck) denotes the Lebesgue measure of the cube (its volume), hX (Ck) refers to the value of hX at any
point of the cube Ck (recall that hX is constant on the cubes). Finally, in the last equality, we have supposed
that the partition is uniform and we set L = (cid:81)d
j=1(M j − mj)/w. In analogy with the the purely probabilistic
case dsmp
L1 (X, Y ) = 0 if and only if the number of samples of X and Y in each are equal, if the bins of the
partition are the same.

In a similar vein, the Jaccard index can be estimated from the histograms. Let supp(hX ), supp(fY ) be the

supports of the histograms. Then we deﬁne the sampling Jaccard index as

J smp(X, Y ) =

|supp(hX ) ∩ supp(hY )|
|supp(hX ) ∪ supp(hY )|

.

Again, this coeﬃcient takes values in the interval [0, 1] and the larger the value of J(X, Y ) the more similar the
empirical supports.

In our particular case of GANs, we shall apply these coeﬃcients to measure the similarity between the real
and the synthesized data. Let x1, . . . , xn the real instances of the dataset X to be replicated. Given a generator

13

network G, we extract a suﬃciently large sample yG
Jaccard index of the generator G are just

1 , . . . , yG
m

of generated data Y . Then, the L1 metric and the

dL1(G) = dsmp

L1 (X, Y G),

J(G) = J smp(X, Y G).

4.2 Nested ML performance

The second set of metrics attempts to quantify the performance of synthetic data when it is used as a substitute
for real data for training a ML classiﬁer to distinguish between normal and cryptomining internet traﬃc.

To be precise, let C : Rd → {0, 1} be a binary classiﬁer. It attempts to take an instance x = (x1, . . . , xd) ∈ Rd
(which, in our case, represents the d features of a internet connection) and to predict its class C(x) ∈ {0, 1}
(the type of traﬃc in our setting). Once the classiﬁer C has been trained, its accuracy can be measured against
the test split of the dataset, where the real classes Ytest = (y1, . . . , yn), with yi ∈ {0, 1}, of a bunch of instances
Xtest = (x1, . . . , xn), with xi ∈ Rd, are known. In that case, we deﬁne precision and recall as the quantities

Precision(C) =

|{xi ∈ Xtest | C(xi) = 1 and yi = 1}|
|{xi ∈ Xtest | C(xi) = 1}|

,

Recall(C) =

|{xi ∈ Xtest | C(xi) = 1 and yi = 1}|
|{xi ∈ Xtest | yi = 1}|

.

Here, |X| stands for the number of elements of the set X. In other words, 1 − Precision(C) is the rate of false
positives and 1 − Recall(C) is the rate of false negatives of the class λ. In general, to combine both coeﬃcients,
it is customary to consider the F1 as the harmonic mean

F1-score(C) = 2

Precision(C) · Recall(C)
Precision(C) + Recall(C)

.

Additionally, these metrics can be complemented with the so-called confusion matrix. It is a 2 × 2 matrix

that compares the real labels of each instance with the predicted label, in the form

(cid:18)|{xi | C(xi) = 0 and yi = 0}|
|{xi | C(xi) = 0 and yi = 1}|

|{xi | C(xi) = 1 and yi = 0}|
|{xi | C(xi) = 1 and yi = 1}|

(cid:19)

In other words, the diagonal entries are the correct classiﬁed instances and the oﬀ-diagonal entries are the false
positives (upper-right corner) and the false negatives (botton-left corner). In this way, the confusion matrix
allows us to identify more precisely the ﬂaws of the classiﬁer C, in comparison with the precision, recall and F1
measures, which are raw means.

With this notions at hand, the quality of a GAN will be evaluated as follows. Suppose that, as explained in
Section 3.1, we have trained GANs (Λ0, G0, D0) and (Λ1, G1, D1) to synthesize data with label 0 (real traﬃc)
of the
and 1 (cryptomining traﬃc) respectively. Choose N, M > 0 and draw samples x0
latent spaces Λ0 and Λ1 respectively. Then, using the generators G0 and G1, we create a new fully synthetic
training dataset

1, . . . , x1
M

1, . . . , x0
N

and x1

Xtrain = (cid:8)G0(x0

1), . . . , G0(x0

N ), G1(x1

1), . . . , G1(x1

M )(cid:9) ,

Ytrain = {0, . . . , 0
(cid:124) (cid:123)(cid:122) (cid:125)
N times

, 1, . . . , 1
(cid:124) (cid:123)(cid:122) (cid:125)
M times

},

with N + M instances.

With this new dataset (Xtrain, Ytrain), we train a standard ML classiﬁer C (say, a random forest classiﬁer).
Then, screening the precision, recall, and F1-score of C against a test split made of real data, we are able to
measure the quality of the generated data: the higher these measures, the better the data. Hence, large values of
these coeﬃcients point out that the synthetic data generated by G0 and G1 can be used to faithfully substitute
the real instances. Observe that no real traﬃc is used for such training purposes, although real traﬃc is always
used for testing. As previously stated, this is a diﬀerentiating characteristic of our work with respect to existing
solutions. Data augmentation solutions are proposed in these previous works, where synthetic data is mixed
with real data during training, generating data privacy breaches as real data is used.

Several variants of this proposal can be considered. First, instead of creating the dataset with fully-trained
GANs (Λ0, G0, D0) and (Λ1, G1, D1), we can compute these coeﬃcients at each of the training epochs of the
GAN. In this way, we are able to screen the evolution of the training and to relate it to the quality of the
generated data. In particular, this idea enables a stopping criterion: when the GAN training epochs do not
produce any signiﬁcant enhancement, the training process is stopped. As previously commented, one of the
current open issues of GANS is how to optimize their training as oscillatory behaviours appear frequently during
training. Moreover, none of the related works listed in Section 1 explain what stopping criterion they applied
to obtain their fully trained GANs. Therefore, our proposal for measuring GAN performance in ML tasks at
each epoch provides a way to implement such stopping criterion.

14

Additionally, we can also evaluate the marginal quality of each of the generators. In the previous approach,
we generated the dataset (Xtrain, Ytrain) by using synthetic samples both for label 0 and 1. However, if we want
to test the quality when generating only one of the labels, say label 0, the dataset (Xtrain, Ytrain) can be also
created by mixing synthetic samples of label 0 with real samples of label 1. In this way, the corresponding ML
accuracy coeﬃcients will only measure the ability of G0 to generate label 0, regardless of the ﬁtness of G1.

Regarding that we will use two diﬀerent WGANs for each type of traﬃc, the ﬁrst approach would imply to
evaluate the ML performance of G0 at each epoch with all generators obtained during G1 training and conversely,
at each G1 epoch we should combine its generator with all G0 generators to measure ML performance. Hence,
assuming we trained a pair of GAN for j and k epochs respectively, we would require j × k evaluations of the
ML task (training and testing) to obtain the full set of metrics.

In order to implement the stopping criterion more eﬃciently, we opted for the second approach, in which
we evaluate separately the marginal quality of each of the generators. In this way, each WGAN can be trained
in parallel without requiring the other to evaluate their joint performance and therefore, each training can be
stopped at diﬀerent epochs when no signiﬁcant enhancement is observed. When both WGANs are trained
and the joint performance has to be computed, instead of generating the Cartesian product (j × k) of the two
sets of generators and running the corresponding ML evaluations, we observed experimentally that drawing
roughly a dozen samples by choosing uniformly at random one generator of each type of traﬃc tends to produce
results equivalent to the brute force approach of trying all possible combinations. It is worth noting that more
elaborated strategies can be applied as ordering the generators of each type of traﬃc by some metric (e.g.,
F1-score) and choosing generators at random only from the subset containing the best generators.

Finally, notice that there is plenty of freedom for choosing the number of generated samples N and M . A ﬁrst
decision would be to choose N and M to be in the same range as the number of samples in the original dataset.
This leads to a synthetic dataset with very similar characteristics to the original one in terms of balancing
between classes. However, other set-ups can be tested like drastically increasing the number of samples in
the generated dataset or to balance the number of instances of each class to ease the task of the classiﬁer.
These possibilities will be explored in Section 5.
It is worthy to mention that, even though the potential
balance between classes achieved with this method is similar to what can be obtained with data augmentation
procedures, the proposed solution is way stronger than standard data augmentation: the generated data is not
a simple enrichment of the original dataset but a completely new dataset.

5 Empirical evaluation

We designed a set of experiments to demonstrate that GANs can be utilised for generating high-quality synthetic
data that replicates the statistical properties of real data while maintaining their privacy. Furthermore, in these
experiments we aim to prove that synthetically generated data can be utilised for totally substituting real data
in ML training processes while keeping the same performance than ML models trained with real.

In this section, we ﬁrst summarize the testbed on which we conducted our experiments and how we collected

the data, then the experimental setup is detailed and ﬁnally, the experimental results are depicted.

5.1 Testbed for data collection

The data sets used in our GAN experiments were previously generated in a realistic network scenario called the
Mouseworld lab [16]. The Mouseworld is a network digital twin created at Telefónica R+D facilities that allows
deploying complex network scenarios in a controlled way. In the Mouseworld, realistic labeled data sets can be
generated to train supervised ML components and validate both supervised and unsupervised ML solutions.
The Mouseworld Lab provides a way to launch real clients and servers, collect the traﬃc generated by them
and the recipients outside the Mouseworld on the Internet, and add labels to the traﬃc automatically.

To obtain the training and testing datasets used in our GAN experiments, we deployed in the Mouseworld
thirty virtual machines for the generation of regular traﬃc (i.e. web, video and shared-folder ﬂows) to internal
Mouseworld servers and to external servers located in the Internet. The IXIA BreakingPoint tool was also
conﬁgured to generate and inject synthetic patterns of various Internet network services (web, multimedia,
shared-folder, email and P2P). All traﬃc generated was composed of encrypted and non-encrypted ﬂows. In
addition, we created three cryptomining Linux virtual machines in which we installed well-known cryptomining
clients for mining the Monero cryptocurrency, which is commonly used for illegal purposes. The cryptomining
clients were connected to public mining pools using non-encrypted TCP and encrypted TLS connections.

We deployed in the Mouseworld four experiments with diﬀerent cryptomining protocols [13]. Each exper-
iment was run for one hour with an average packet rate of approximately 1000 packets per second, which
generated data sets with 8 millions of ﬂow-based entries containing statistics of the TCP connections of which 4

15

thousands were related to cryptomining connections. Normal connections were labelled with 0 and cryptomining
ones with 1. The four obtained data sets were split in two separate subsets for training and testing purposes.
Speciﬁcally, the data sets from experiments 1 and 4 were joined in DS1 (training) data set and the other two
data sets collected in experiments 2 and 3 were combined into DS2 (testing) data set. In this way, DS1 and DS2
can be considered of the same nature as they contain similar percentages of encrypted and non-encrypted traﬃc,
types of internet services and cryptomining protocol ﬂows. Considering the small amount of traﬃc generated
by cryptomining protocols compared to normal traﬃc, it is worth noting that an imbalance in the number of
cryptomining ﬂows versus normal traﬃc appeared in both data sets. In each experiment, around 400K samples
of label 0 (normal traﬃc) appeared against 4K instances of label 1 (cryptomining traﬃc).

As previously commented, the goal of this work is to generate synthetic traﬃc that can substitute real traﬃc
for training a ML-based traﬃc classiﬁer that predicts with similar performance than a model trained with real
data. Nowadays, most machine and deep learning techniques use ﬂow descriptions as input to machine learning
models. These descriptions are composed of a set of features that are typically statistical data obtained from
externally observable traﬃc attributes such as duration and volume per ﬂow, inter-packet arrival time, packet
size and byte proﬁles. Therefore, our GAN experiments will try to generate synthetic replicas of some statistical
data that can be used as features to be input to a machine learning based network traﬃc classiﬁer. In the data
gathering experiments, a set of 59 statistical features were extracted from each TCP connection.

We selected a reduced set of 4 of these 59 features for our GAN experiments (the rationale of this choice is
detailed in Section 2): (a) number of bytes sent from the client, (b) average round-trip time observed from the
server, (c) outbound bytes per packet, and (d) ratio of packets_inbound / packets_outbound.

Therefore, we obtained a reduced version of DS1 and DS2 containing only the 4 selected features for train-
ing and testing our GANs. It is worth noting that other subsets of features were considered in preliminary
experiments obtaining GANs with a similar performance to the ones shown in this paper.

5.2 Experimental setup

To perform our experiments, we designed and trained independently two WGANs, one for each type of traﬃc,
applying the set of hyperparameters detailed in Table 1. Using this table as a reference, and taking into account
that in general the training process of a single WGAN took one week on average, we performed a blind random
search in the hyperparameter space but guided by the F1-score obtained in a nested ML-model that was executed
after each train epoch and evaluating the marginal quality of the generator at each epoch (see subsection 4.2).
In this way, we were able to independently adjust most of the parameters and observe whether or not the
introduced modiﬁcations in a hyperparameter generated an improvement in the F1-score of the classiﬁer. The
results are consistent throughout diﬀerent executions, and the top generative models in diﬀerent runs of the
random search algorithm return similar performance metrics.

As a nested ML classiﬁer, we used a Random Forest model with 300 trees, which proved to be the best
solution for classiﬁcation when trained with the original (non-synthetic) dataset (c.f. Section 5.3.1 ). As shown
in a previous work that used the real datasets DS1 and DS2 for training and testing [13], the performance
of neural networks-based classiﬁers was quite poor as these models showed signiﬁcant overﬁtting even after
applying regularization procedures. On the contrary, Random Forest models exhibited very good performance.
For each label, the WGAN selected was the one that obtained the best F1-score for the nested classiﬁer
in any of its epochs. It is worth noting that this method allows us to ﬁnd the set of hyperparameter values
that produces the best performance on a single WGAN for a type of traﬃc and therefore, it is not guaranteed
that the best WGAN for label “0” (normal traﬃc) works well in combination with the best WGAN for label
“1” (cryptomining traﬃc). Moreover, we observed that combining the generators of each WGAN that obtained
the best F1-score during partial nested evaluations, did not guarantee to obtain the best synthetic dataset
when used in a nested evaluation of both types of traﬃc. In fact, we ran a simple heuristic to select pairs of
generators that produced decent results during the nested evaluation without testing all possible combinations
of generators from the two WGANS. These limitations highlight that future works should explore methods that
enable the search of the best hyperparameters in both WGANs at the same time.

Hyperparameters in Table 1 are grouped in four categories: (1) common parameters of the FCNN architecture
for generator and discriminator, (2) generator parameters, (3) discriminator parameters and (4) adaptive mini-
batch training parameters. The hyperparameter space is detailed in "Range of values" column. The text
"Fixed value" indicates that the hyperparameter was explored in a preliminary phase before executing the
random search and therefore, the value was previously determined and no random search was performed on it.
Ranges described by a list of values in brackets indicate that the random search was performed by randomly
choosing an element from the list. Conversely, two values in square brackets denote the minimum and maximum
of the range of values to be considered in the random selection.

16

Table 1: GAN hyperparameters.

Range of values

# layers

[2..6]

Genera-
tor/discriminator
FCNN architec-
tures

Generator

# units per layer
Output Activation
Output ﬁltering with discrim-
inator
(>0, percentile)
Latent vector
with Embedding (categories)
latent vector
noise
for
(distr,std)
batch normalization
regularization: L2, dropout

vector

latent

Discriminator

Adaptive mini-
batch

learning rate

LeakyRelu alpha
Percentage of tanh/LeakyRelu
in internal units
Noise in fakes
(distribution,std)
Noise in all examples
(distribution,std)
Ratio Label change
batch normalization
regularization: L2, dropout
LeakyRelu alpha
learning rate

generator. ratio fake pass

discriminator. ratio TP
discriminator. ratio TN

[100..10000]
(linear,custom)

(True,False)
[0..100]

(True,False)
[1..20]
Fixed 123
(normal, uniform)
std=[0.1..100]
[True..False]
Fixed values (0,0)
Default
(0.001)
Fixed value (0.15)

value

[0..100]

(normal,uniform)
[0..20]
(normal,uniform)
[0..20]
[0..20]
[True..False]
[0..2], [0..30]
Fixed value (0.2)
[0.0001..0.001]

Fixed (0.3)

Fixed (0.01)
Fixed (0.01)

As optimization algorithms, we used Adam for generators and RMSProp for discriminators. The typical
binary cross-entropy loss function was substituted by the Wasserstein loss. Assuming that the discriminator
generates a positive value if an example is classiﬁed as a real example or negative if the example is considered
as fake, the Wasserstein loss multiplies the output of the critic (i.e., the discriminator) by −1 (real examples)
or 1 (fake examples). For label "0" (normal traﬃc), we set the size of each minibatch as a ratio (0.002) of the
total number of examples (400, 000) and for label "1" (cryptomining traﬃc), we set this ratio to 0.02 of the
total number of samples (4, 000).

Although it has been reported that activating batch normalization in generators can produce correlations
in the generated samples, we decided to include it as a hyperparameter after observing that generators without
batch normalization exhibited a lack of convergence on many occasions.

To provide the generator with more complex non-linear capabilities to learn how to cheat the discriminator,
we used Leaky-ReLU activation functions with their slope parameter set to α = 1.5 and hyperbolic tangent
as activation functions in the neurons of its hidden layers. The ratio of hyperbolic tangents over the total of
activation functions was a conﬁgurable hyperparameter. For the discriminator, we only used Leaky-ReLUs in
the hidden layers setting a more aggressive value α = 0.2.

5.3 Experimental results

In this section, we review the results obtained in the conducted experiments when real data sets are replaced by
fully synthetic datasets. For this purpose, we shall compare the performance obtained by a ML classiﬁer when
trained with (1) real data (DS1 dataset), (2) data generated through a simple mean-based generator (working
as baseline) and (3) a synthetic dataset generated with a standard WGAN (with no variants or improvements
implemented). The eﬀects of using an improved WGAN are analysed in section 6.

17

5.3.1 Real data

A Random Forest was trained to classify the original real dataset into normal and cryptomining-based traﬃc.
We chose Random Forest due to its well-known good performance in classiﬁcation tasks and in particular, when
cryptomining and normal traﬃc has to be classiﬁed [13]. A hyperparameter tuning was conducted through a
grid search on the number of classiﬁcation trees used, ranging from 10 to 600 estimators. No depth limit was
applied to trees. This performance was evaluated against a validation split excerpted from the training data
set. The experiments showed that using more than 300 estimators did not produced any signiﬁcant increase in
F1-score. After this hyperparameter tuning, the model was re-trained with the whole training dataset (DS1)
and evaluated against the test dataset (DS2). In order to analyse the impact of the decision threshold of the
classiﬁer on the number of false positives and negatives, several thresholds for the model to distinguish between
the 0 and the 1 class were tested, with possible values 0.2, 0.4, 0.5 (default value), 0.6 and 0.8.

The results obtained in testing are shown in Table 2. and point out that the performance of the classiﬁer
against the original dataset is very high, with a F1-score of 0.962 with the best threshold (and slightly worse
with the default threshold). Note that the confusion matrix shows that most of the wrongly classiﬁed instances
are false negatives, i.e. cryptomining traﬃc (class 1) samples that are classiﬁed as normal traﬃc (class 0). Only
a few false positives were observed. It should be noted that in certain scenarios this percentage of false positives
may not be desirable as it would mean that users may be suﬀering from surreptitious use of their resources that
would not be detected. On the contrary, a non-negligible number of false negatives could imply extra eﬀorts as
false alarms will be raised in the detection system, which could imply a individual treatment of each of them.
Recall that the number of classes in the real dataset are greatly unbalanced, with around 400K samples of
label 0 (normal traﬃc) against 4K instances of label 1 (cryptomining traﬃc). This is in perfect agreement with
the fact that the previously trained model might tend to predict more frequently class 0 than class 1, so there
are few false positives. For this reason, additionally to the standard training with the whole dataset, we tested
the performance after applying a random subsampling strategy that extracts a balanced training dataset with
approximately 4K instances per class . However, with this strategy, the results of the Random Forest model
worsen. The best threshold for the balanced dataset achieves a F1-score signiﬁcantly smaller than the default
value for the unbalanced one (and with the default value, the results are much worse). The confusion matrix
in this case inverts the trend, and most of the wrong classiﬁed instances are false negative (with a fairly high
rate). It is worth mentioning that, however, the number of false negatives drastically decreases with respect to
the unbalance setting.

5.3.2 Naïve mean-based generator

In this section, we evaluate the performance of the previously described ML classiﬁer when the training
dataset is fully substituted with a synthetic dataset generated through a simple mean-based generator. This
will serve as a baseline for the upcoming experiments.

This mean-based generator was created by computing the mean and variance of each of the four features
of the dataset per class. With these data, a completely new dataset was generated by drawing samples from
a multivariate normal centered at the means with diagonal covariance matrix (i.e., each feature is drawn inde-
pendently) for each class. Afterwards, we fed the Random Forest model with 300 classiﬁcation trees with this
synthetic dataset and we analyzed its performance on the (real) test data set (DS2). The results are shown in
Table 3.

Table 3 shows that the performance is much worse with this naïvely generated dataset. For the best setup
(threshold 0.8 with the unbalanced dataset) the F1-score obtained is 0.732, signiﬁcantly smaller than any result
got with the real dataset. The situation for the balanced dataset is even worse, with a best F1-score of 0.601.
In both cases, the confusion matrix also shows a concerning phenomenon: the number of false positives is very
large, even greater than the number of true positives.

Since the generating method for the dataset is intrinsically stochastic (new instances are generated by
sampling a random vector), in Figure 8 we plot the histogram of the obtained F1-scores for several runs of
the generative method. None of the tested instances were able to reach a F1-score greater than 0.76. This
strengthen the values shown in Table 3, showing that they are actually statistically consistent and diﬀerent runs
of the generative method return similar results.

These results evidence that the naïve mean-based generator is not a good approach for generating a synthetic
dataset when the data distributions are not easily separable as it happens in our scenario (Section 2). The means
of the selected features are so close that the generated features of diﬀerent classes overlap and do not capture
the real distribution of the data. Hence, these results point out that a much subtler method of generation is
required to obtain a compelling performance.

18

Table 2: Baseline results using real data for training.

Table 3: Baseline results using a naive mean-based
generator for training.

Dataset

Training 400K/4K
Real dataset

Training 4K/4K
Real dataset

Quality Measure
Threshold
F1-score
Confusion
matrix

Threshold
F1-score
Confusion
matrix

Best
0.4
0.962

Default
0.5
0.928

Dataset

399817
459

183
3929

399877
1008

123
3380

Training 400K/4K
Mean-based generation

0.8
0.919

0.5
0.793

398602
197

1398
4191

394172
62

5828
4326

Training 4K/4K
Mean-based generation

Quality Measure
Threshold
F1-score
Confusion
matrix

Threshold
F1-score
Confusion
matrix

Best
0.8
0.732

Default
0.5
0.664

396318
1894

3682
2493

390060
1416

9940
2971

0.8
0.601

0.5
0.583

377352
839

22648
3548

370528
537

29472
3850

Table 4: Performance of synthetic traﬃc generated by
standard WGANs. Results on testing using the best
models on training.

Table 5: Performance of synthetic traﬃc when gener-
ators use custom activation functions in the output.
Results on testing using the best models on training.

Dataset

Training 400K/4K
Policy 1) dataset

Quality Measure
Threshold
F1-score
Confusion
matrix

Training 400K/4K
Policy 2) dataset

Training 4K/4K
Policy 3) dataset

Threshold
F1-score
Confusion
matrix

Threshold
F1-score
Confusion
matrix

Best
0.4
0.936

Default
0.5
0.933

399926
927

74
3461

399962
998

38
3390

0.8
0.927

0.5
0.915

399449
701

551
3687

399601
983

399
3405

0.8
0.878

0.5
0.835

399030
1108

970
3280

396381
315

3619
4073

Dataset

Training 400K/4K
Policy 1) dataset

Quality Measure
Threshold
F1-score
Confusion
matrix

Training 400K/4K
Policy 2) dataset

Training 4K/4K
Policy 3) dataset

Threshold
F1-score
Confusion
matrix

Threshold
F1-score
Confusion
matrix

Best
0.8
0.649

Default
0.5
0.555

382755
241

17245
4146

357385
96

42615
4291

0.8
0.622

0.5
0.559

377693
163

22307
4224

358897
82

41103
4305

0.8
0.532

0.5
0.467

345452
21

54548]
4366

298568
0

101432
4387

Table 6: Performance of synthetic traﬃc generated by
standard WGANs after ﬁltering fake samples by dis-
criminator. Results on testing using the best models
on training.

Dataset

Training 400K/4K
Filtering out fakes

Quality Measure
Threshold
F1-score
Confusion
matrix

Best
0.6
0.925

Default
0.5
0.914

399511
767

489
3621

399221
722

779
3666

Table 7: Performance of synthetic traﬃc generated by
standard WGANs by sampling generators with elitism
among the top 10 models in training sorted by F1-
score. Results on testing.

Dataset

Training 400K/4K
Top 10 in F1-score

Quality Measure
Threshold
F1-score
Confusion
matrix

Best
0.4
0.951

Default
0.5
0.950

399800
608

200
3780

399832
658

168
3730

5.3.3 Standard GAN

Table 8: Performance of synthetic traﬃc generated by
standard WGANs by sampling generators with elitism
among the top 10 models in training sorted by L1-
distance and Jaccard index. Results on testing.

Dataset

Training 400K/4K
Top 10 L1-distance

Training 400K/4K
Top 10 Jaccard index

Quality Measure
Threshold
F1-score
Confusion
matrix

Threshold
F1-score
Confusion
matrix

Best
0.8
0.869

Default
0.5
0.858

399491
1506

509
2882

398536
1094

1464
3294

0.8
0.884

0.5
0.826

399033
1031

967
3357

396881
720

3119
3668

In this section, we discuss the performance results attained by a simple WGAN with linear activation functions
in the output layer. No extension of the method is implemented. This kind of models are sometimes referred
to as ‘vainilla GANs’ in the literature due to their simplicity.

Recall from subsection 3.1 that two WGANs were trained to generate samples corresponding to label 0
(normal traﬃc) and label 1 (cryptomining traﬃc). Using the variables and intervals deﬁned in table 1, the
hyperparameter setup for the two WGANs was conducted through a random search by screening the F1-score
obtained by a nested Random Forest model that evaluated the marginal quality of the generator in testing with
respect to the corresponding label (subsection 5.2).

The evaluation of the marginal quality of each hyperparameter conﬁguration was done at the end of each
mini-batch training phase, computing the marginal F1-score and saving the stage of the WGAN for later use.
Hence, instead of a single trained WGAN, we got many diﬀerent WGAN models with the same hyperparameter
setup but at diﬀerent stages of the training. Thus, each WGAN hyperparameter conﬁguration generated as many
F1-scores as mini-batch training steps. Finally, for each type of traﬃc, we selected the WGAN conﬁguration
that obtained the best F1-score in any of its mini-batches.

19

For normal traﬃc, the set of hyperparameters that produced the best performing WGAN was as follows:

• Generator. Architecture:

linear, latent vector size: 123,
multipoint single-class embedding: False, noise for latent vector: Normal (0,5), batch normalization:
True, Percentage of tanh: 15%

[123,200,500,3000,500,4], output activation:

• Discriminator. Architecture:

[4,380,800,600,177,23,1], output ﬁltering: False, noise in input (real and
fakes): N(0,0.02), noise in fakes: N(0,0), ratio label change: 0, batch normalization: True, regularization
0.02, dropout:0.1, learning rate RMS-Prop:0.001.

For cryptomining connections, the set of hyperparameters that produced the best performing WGAN was

as follows:

• Generator. Architecture: [123,600,3000,1000,4], output activation: linear, latent vector size: 123, multi-
point single-class embedding: False, noise for latent vector: uniform (0,3.5), batch normalization: True,
Percentage of tanh: 5%

• Discriminator. Architecture: [4,280,903,500,23,1], output ﬁltering: False, noise in input (real and fakes):
N(0,0.01), noise in fakes: N(0,0), ratio label change: 0, batch normalization: True, regularization 0.05,
dropout:0.15, learning rate RMS-Prop:0.001.

All WGANs were trained at least 1000 mini-batch steps for label "0" and 1400 for label "1". Recall that
we have around 400,000 samples of label "0" and only 4,000 of label "1" in DS1 and DS2 (training and testing
datasets). The size of label "1" mini-batches was conﬁgured 10 times less than label "0" mini-batches, and
therefore, one epoch of label "0" implied 500 mini-batch train steps, and one epoch of label "1" consisted of
only 50 mini-batch train steps. This is the reason label "1" WGANs were trained with more mini-batches than
label "0" WGANs in the same period of time.

Having selected the best performing WGAN conﬁgurations for each type of traﬃc, and in order to compare
the quality of the synthetic traﬃc with respect to the real traﬃc, one generator of each type of traﬃc was
chosen from all mini-batch models previously saved during training. Using this pair of generators, we obtained
a combination of samples of the two types of traﬃc that formed a fully synthetic dataset. The synthetic dataset
was used to feed the training of a nested Random Forest classiﬁer (with 300 trees) that was subsequently tested
with real data (DS2 dataset). The selection of a pair of generators and the number of samples produced by
each of them was done by using the following three policies:

1. An unbalanced dataset (with 400K/4K instances) is generated by picking at random one model for each

label among the partially trained models.

2. An unbalanced dataset (with 400K/4K instances) is generated by picking at random two models for each
label. The instances of the synthetic dataset were obtained by mixing the outputs of the two chosen
generative models, in the expectancy of increasing the variety and diversity of the synthetic dataset.

3. A balanced dataset (with 4K/4K instances) is generated by picking at random one model for each label.

For each policy, the selection of the pair of generators (one for each type of traﬃc) was drawn 20 times
uniformly at random from the generators of each WGAN conﬁguration. The F1-scores obtained at the end of
these 20 experiments are shown in Figure 9 (Appendix I). It is worth noting that sampling pairs of generators
allow us to study the statistical distribution of the standard WGAN quality metrics without evaluating all
possible combinations of generators.

The results reached by the best model in these experiments are shown in Table 4. The quality measures
point out that the best results were obtained when we applied policy 1), with a slightly better performance
compared to the results got when real data was used (Table 2) and much better performance than for the naïve
mean-based generator (Table 3).
In contrast, policy 2) reached a lower performance than the single-model
policy. However, if we look at the histograms depicted in Figure 9 (subplots (a) and (b)), we observe that even
though the best model is obtained with policy 1), the datasets obtained by mixing two models, as provided by
policy 2), tend to be less dependent on the sampled models, with a more uniform distribution of the F1-scores.
This points out that the variety and diversity generated by using policy 1) tends to generate a large amount
of information in the dataset, which can be exploited by the nested ML model. Should we increase even more
the variability of the dataset by mixing two models as in policy 2), the results tend to be more consistent
between executions, even though the best results are slightly worse due to the added spurious noise. Moreover,
in the line of the results obtained in subsection 5.3.1, the policy 3) with a generated balanced dataset did not
reach compelling results.

20

5.3.4 Evolution of the training process for standard GAN

To analyze more deeply the results of Section 5.3.3, in this subsection we shall evaluate the evolution of the
diﬀerent quality measures of the best standard WGAN model throughout its training process.

In Figures 4 and 5, we show the evolution of the diﬀerent metrics described in Section 4 for the diﬀerent
training epochs of the best WGAN model for label 0 and label 1, respectively. In particular, Figure 4a shows
the evolution of the F1-score in testing when generated data for the label 0 (normal traﬃc) is mixed with real
data for label 1 (cryptomining traﬃc), while Figure 5a depicts the evolution of the F1-score when label 1 is
generated and real data is used for label 0. As we can observe, for both labels the obtained results are quite
consistent along the training process (beware of the scale of the plots), with some marked drops for label 0 that
are rapidly recovered, probably due to drastic changes in the generator network.

However, this constant tendency in the F1-score is in sharp contrast with the evolution indicators described
in Section 4 of statistical nature, such as the L1-distance (Figures 4b and 5b) and the Jaccard index of the
support (Figures 4c and 5c) with respect to the original distribution. For these statistical coeﬃcients, we
observe that a longer training usually leads to generated samples of better quality (smaller L1 distance and
larger Jaccard index), as predicted by the theoretical convergence results for GANs. This is a very interesting
observation, since it evidences that a better performance of the generated data in a nested ML is not directly
related with a better ﬁt with the original distribution. In this manner, classical measures of the goodness of
ﬁt are not good estimators of the information contained in the generated data, as can be exploited by a nested
ML model. This observation leads us to conjecture that these metrics are not reﬂecting some quality parameter
indicating that the synthetic data lack some essential information that the real data does.

Analysing F1-scores of each type of traﬃc, it can be observed that label "0" WGAN generates synthetic data
that performs worse than label "1" WGAN when the synthetic data is used to train a Random Forest classiﬁer.
Label "0" synthetic traﬃc does not achieve a F1-score greater than 0.58 but label "1" synthetic traﬃc obtains
F1-scores greater than 0.9. Recall that label "0" traﬃc is a complex mixture of web, video, shared-folder and
other protocols and on the contrary, label "1" traﬃc is a more homogeneous traﬃc generated by four types
of cryptomining protocols. We conjecture that the complexity of the former is more diﬃcult to be learnt and
generated by WGANs than the latter.

To strengthen these ideas, in Figure 6 we compare the histogram of real (red color) and synthetic (blue
color) data along the training process. In X axis we order the intervals s0 < s1 < . . . < s(cid:96) by hX (s) values (as
deﬁned in equation 1) of the real data from smallest to largest. As we can observe, initially the ﬁt to the target
distribution is very poor but rapidly the GANs are able to detect and replicate the most frequent values. It is
worth noting that during the ﬁrst epochs, WGANs generate a signiﬁcant number of nonexistent values (left side
of the curves) that tend to disappear as training progresses. For large times, the real and synthetic distributions
are quite similar in agreement with the decrease of the L1-distance and the increase of the Jaccard index.

To ﬁnish this section, it is worth mentioning that, even though the theoretical results predict an asymptotic
convergence of the generated data to the original distribution, Figures 4b and 4c seem to point out that this
evolution tends to stuck. This stagnation of the quality measures may be caused by the complexity of the
original data, which follow very complicated distributions with domain constrains such as non-negativity of
discreteness, in contrast with the usual graphical data that is usually taken as input for GANs. However,
further research is needed to clarify these issues.

6 Eﬀects of improvements and variants

In this section, we shall discuss the eﬀect of the diﬀerent variants of GANs introduced in Section 3 for the
problem of generating synthetic traﬃc data. The setting will be the same as in Section 5.3.3, in which we shall
use the performance of a nested ML model as a quantitative metric to compare the variants with the standard
WGAN architecture.

6.1 Custom activation function

In this section, we will evaluate the performance of the GAN after applying the improvement described in
Section 3.2. Roughly speaking, recall that this improvement consists in changing the activation function of the
output layer of the generator network to a Leaky-ReLU function with a small slope. Thanks to this plug-in, we
are able to preserve the domain semantic constraints such as non-negative values for counters. Recall that linear
activation functions tend to produce bell-shaped data distributions and so, negative values can be generated
(left side of the data distribution curve). If the output variable in this linear activation function is a counter or
an accumulator, we would be generating nonexistent values.

21

(a) F1-score on testing

(b) L1 distance

(c) Jaccard index

(d) Jaccard index from percentile 1

Figure 4: Evolution of F1-score on testing, L1 distance and Jaccard index using GAN generator for label 0.
The x-axis represents the GAN training epochs.

The obtained results are shown in Table 5 and Figure 10. As we can observe, the results are clearly worse than
the ones obtained with the standard WGAN in subsection 5.3.4. In contrast to the observation of subsection
5.3.3, the performance of the nested ML model seems to be independent of applying sampling policy 1) or 2).
When the synthetic dataset is balanced using policy 3), the performance slightly gets worse.

However, it is remarkable from these results that the number of false negatives suﬀered by the ML model
drastically decreases in this case in comparison with the standard GAN. The rationale behind this fact is that,
even though the global quality of the data is not as good as with the standard GAN, the preservation of the
domain constraints allows the system to be more aggressive in the distinction of cryptomining traﬃc (obviously,
with the drawback of a large rate of false positives). This output may be very useful in those scenarios in which
the skipping a ﬂow of cryptomining traﬃc is very penalized (e.g., due to security issues) but getting a high
rate of false positives is not so serious (say, because the only consequence is that the connection is artiﬁcially
restarted). In these scenarios, the solution based on customized activation functions would be the choice.

In addition, the use of custom activation functions in the generator output layer will be appropriate if our
goal is not only to use synthetic data to train a ML-based classiﬁer (e.g., a cryptomining attack detector), but
to obtain synthetic data that can be used in other applications in which it is crucial that the data do not contain
any nonexistent value (e.g. counters with negative values).

6.2 Discriminator as quality assurance

Another interesting approach to obtain high-quality generated features is to use the discriminator network of
the GAN as a quality assessor. To be precise, after training the GAN, instead of using all samples synthesized
by the generator network, we add to the generated dataset only those that were classiﬁed as real samples by
the discriminator agent (D(x) > 0), while the samples judged as fake (D(x) ≤ 0) are ruled out.

In this manner, only those samples that were competitive enough to cheat the discriminator were selected.

22

(a) F1-score on testing

(b) L1 distance

(c) Jaccard index

(d) Jaccard index from percentile 1

Figure 5: Evolution of F1-score on testing, L1 distance and Jaccard index using GAN generator for label 1.
The x-axis represents GAN training epochs.

(a) Label 0. Epoch 1

(b) Label 0. Epoch 5 (c) Label 0. Epoch 200 (d) Label 0. Epoch 800 (e) Label 0.

Epoch

1000

(f) Label 1. Epoch 1

(g) Label 1. Epoch 5 (h) Label 1. Epoch 200 (i) Label 1. Epoch 800 (j) Label 1. Epoch 1500

Figure 6: Comparison of synthetic (blue) and real (red) data distributions using GAN generators for label 0
(6a, 6b, 6c, 6d and 6e) and label 1 (6f, 6g, 6h, 6i and 6j) in diﬀerent epochs (1, 5, 200, 800 and 1500). The
4-dimensional vector has been ﬂattened into by sorting by frequency in ascending order on the x-axis.

Obviously, this requires to generate signiﬁcantly more examples than the strictly needed since most of them will
be ﬁltered out. However, notice that the generative process corresponds to a feedforward procedure and thus is
quite fast, so the required time is not signiﬁcantly larger.

To test this idea, an experiment was carried out following the sampling policy 1. The results of this analysis
are shown in Table 6 and Figure 12. As we can observe, the F1-scores obtained by the nested ML model are

23

similar to the ones obtained with a simple GAN, even after applying this ﬁltering. Maybe, high values are
obtained slightly more consistently with this ﬁltering approach, but the results evidence that the improvement
is not signiﬁcative.

6.3 Elitism by F1-score
Recall from subsection 5.3.3 that with the generation of synthetic data through standard WGANs for each type
of traﬃc, models were chosen randomly from all models obtained during the training phase. In this section, we
shall explore a diﬀerent strategy for sampling more eﬃciently the pair of models for data generation. Instead
of picking a random model among all epochs, we will only draw samples from the top 10 models obtained
throughout all the training epochs, in the sense that they achieved the best F1-scores. With this strategy, we
aim to apply some type of elitism that prevents a drastic fall of the performance due to a random choice of
a bad generator. In addition, this strategy signiﬁcantly reduces the number of combinations that we have to
evaluate to ﬁnd which pair of generators produces the best performing synthetic data.

For each type of traﬃc, we select the top 10 models sorted by F1-score. Drawing a sample uniformly at
random from each top 10 subset, we obtain a pair of generators with which we generate the synthetic dataset
by following policy 1), as described in subsection 5.3.3. The random selection of the pair of generators was done
20 times and the testing results obtained after using the synthetic data for training the Random Forest classiﬁer
are shown in Table 7. These results evidence that this strategy is very eﬀective to increase the performance
of the nested ML method. The obtained level of F1-score outperforms the ones obtained with the standard
sampling (Table 4), and are slightly below the obtained results with real data (Table 2) for the best choice
of hyperparameter and even outperforms them with the default value. Additionally, the histograms plotted in
Figure 11 evidence that with the elitism strategy, the results are much more consistent among executions and
most of the results are around a F1-score value of 0.95 for any choice of the threshold.

Therefore, these results point out that this solution allows us to reach comparable results with fully synthetic
datasets with respect to the ones obtained with real data. Moreover, the obtained data are robust, consistently
leading to high values of F1-scores. This also allows us to speed up the method of choosing the right generators.
It is not necessary to conduct an exhaustive and long training with many epochs but, instead, it is better to
save a small number of very good generators and to mix their results. Furthermore, if the number of training
epochs is suﬃciently large, the exploration of the best combination of the pair of generators of the two traﬃc
types can be limited to the evaluation of the subset combinations of the best top-K subsets of each traﬃc type.

6.4 Elitism by statistical measures

In this section, we shall explore a variant of the strategy used in the previous subsection 6.3 for sampling with
elitism. Again, we will sample the model used for generating the dataset from the top 10 models in training.
However, now, instead of using F1-score as quality measure, we shall use the statistical measures described in
Section 4 of L1 distance and Jaccard index to sort the models. This analysis is useful to determine whether the
statistical measures are a faithful quality control of the performance expected in a nested ML model.

As in subsection 6.3, policy 1 was used for generating the dataset. The results are shown in Table 8. As
the results evidence, the use of these statistical coeﬃcients as quality measures leads to a substantial fall in the
observed performance. This trend is also shown in the histograms plotted in Figure 13, where we observe that
the F1-scores obtained with this strategy are consistently lower than the ones reached with standard WGAN
data (Figure 7a and Table 4) and with elitism by F1-score (Figure 11 and Table 7). It is worth noting that the
results obtained with L1 distance as a measure of quality tend to generalize better to the test split than the one
obtained using Jaccard index. This is compatible with the observation that measuring the L1 distance between
the distributions is a much more complete comparison than just comparing their supports.

Consequently, the results of this section conﬁrm the observation of subsection 5.3.4. Even though the
theoretical results presented in the literature guarantee the convergence of the synthetic distribution to the
original distribution, this convergence may not be correlated with a better performance of a nested ML model.
Future works should explore new statistical metrics that better capture the essence of the data that optimization
algorithms utilise to train ML models.

7 Conclusions and future work

We propose a WGAN architecture to generate synthetic ﬂow-based network traﬃc that can fully replace real
traﬃc with two complementary goals: (1) avoiding privacy breaches when sharing data with third parties or
deploying data augmentation solutions and (2) obtaining a nearly unlimited source of synthetic data that is

24

similar to the real data from a statistical perspective and can be utilised to fully substitute real data in ML
training processes while keeping the same performance as ML models trained with real data.

To demonstrate the feasibility of our solution, we adopted a recently appeared cryptomining attack scenario
in which two types of network traﬃc were considered: cryptomining attack connections and normal traﬃc
consisting of web, video, P2P and email among others. A set of four ﬂow-based variables were selected to
represent each connection in real-time. These variables have previously demonstrated their usefulness in ML-
based cryptomining attack detectors and allow to compare the performance of GAN generators with respect to
naive approaches.

Instead of convolutional or recurrent networks, fully connected neural networks were used in WGANs archi-
tectures as no topological structure was present in the four selected features. Each type of traﬃc was modeled
with two diﬀerent WGAN architectures and using a rich set of hyperparameters we run an extensive number of
WGAN trainings. Several enhancements were proposed to improve the simple WGAN performance: a custom
activation function to better adapt the generator output to the data domain, and several heuristics to apply
during GAN training to adapt the learning speeds of the generator and the discriminator. Although the quality
of the resultant synthetic data is similar when custom activation functions were applied in the output layer of
the generator, the signiﬁcant amount of non-existent synthetic data appearing in the simple GAN was almost
entirely eliminated when these custom functions were used. We observed on a few occasions during WGAN
training that when the learning process was temporarily blocked by one network, the adaptive mini-batches
heuristic rapidly managed to rebalance the learning process. However, in our experiments, none of the other
heuristics showed a signiﬁcant eﬀect on the quality of the synthetic data or the speed of convergence of the
training process. Future work should investigate these heuristics more in depth to determine whether they can
modulate or speed up the ill-convergence of the GAN training.

Due to the lack of metrics to measure the similarity of synthetic and real data in the network traﬃc domain,
we deﬁned two new metrics based on the L1 distance and the Jaccard index to measure the quality of our
synthetic data by comparing the join statistical distribution of synthetic and real data variables. Regarding
the ill-convergence of GAN training, we propose a simple heuristic to be used as stopping criterion for GAN
training. This heuristic selects the intermediate generator that produces the best performing synthetic data
when used to train a ML-based cryptomining attack detector. In this context, the synthetic data performance
metric is the F1-score obtained by the ML-based attack detector in testing. For hyperparameter search, this
heuristic is naturally extended to select the best WGAN conﬁguration as the one with the largest F1-score in
any of its intermediate generators.

It is worth noting that larger F1-scores were obtained when ﬂow-based variables of cryptomining connections
were generated than when normal traﬃc connections were replicated. We conjecture that the data distribution
of normal connections is by far much complex as it is composed of many types of traﬃc (e.g., web, video, email,
P2P) and on the contrary, cryptomining connections although generated with four diﬀerent protocols, share
similar behaviour and therefore, their statistical patterns are much easier to replicate by WGANs. We increased
the size of the WGAN architecture (layers and units) and the number of training epochs without being able to
improve the F1-score to the levels we achieved with the cryptomining WGAN. Future work should investigate
how to break down such a complex data distribution into simpler distributions that can be easily replicated by
WGANs.

Although in our experiments decreases on L1 and Jaccard metrics coincided with increases in the performance
of synthetic data in ML tasks (F1-score), we did not observe a strong correlation between the two types of
metrics and therefore, we cannot apply the former instead of the latter, which entails greater computational
costs. Future work should explore new computationally simple metrics that can accurately replace the costly
evaluation of synthetic data performance in ML tasks we carried out during GAN training.

We experimentally observed that using the best generator of two WGANs trained with diﬀerent real data
distributions to blend their synthetic data does not produce the best performance results when applied to the
same ML task. Therefore, we propose for each type of data to select one generator uniformly at random from the
set of intermediate generators obtained during the training of the GAN. Having obtained the blend of synthetic
data to train the ML classiﬁer, the F1-score is computed on testing. Elitism on F1-score showed that the number
of draws needed to achieve good performance decreased dramatically with respect to pure random selection.
Other elitisms based on L1 distance and Jaccard index were experimented with, but the obtained results were
not good and strengthen previous observations on the lack of strong correlation between these metrics and
F1-score. Future work should investigate why the combined synthetic data from the best performing generators
of these two GANs do not produce the best performance when applied to the same ML task.

In addition to the previous open questions, this manuscript points to several interesting challenges to be

researched in future works:

• Our custom activation functions only approximate exponential-like variable distributions using hand-

25

crafted LeakyRelu functions. Further research is needed to ﬁnd activation functions that can ﬁt any data
distribution and in particular discrete and non-continous.

• Synthetic data generated by a GAN provides a way to circumvent real data privacy restrictions but a

thorough and formal study on the reverse engineering of synthetic data should be conducted.

• Ill-Convergence and oscillatory behaviour during GAN training is one of the key problems to be solved
in GAN topic. Minimizing and maximizing partially in turns the cost function with respect to diﬀerent
variables tends to generate such oscillations and therefore, GAN optimization should be done in a more
eﬀective way.

• It is crucial to design computationally simple metrics that are strongly correlated with the performance
of the synthetic data in ML tasks. These metrics could be used to drive the cost function during the
GAN learning process, which would allow to implement an eﬃcient stopping criterion. Furthermore,
these metrics should be consistent when mixing synthetic data from two diﬀerent distributions. Contrary
to what we have observed in our experiments, it would be desirable that if the best generators from
two diﬀerent distributions are selected, the performance of the synthetic data obtained from the mixture
should still be the best when the combined synthetic data is applied to the same ML task.

• The synthetically generated ﬂow-based variables represent the state of a connection at a speciﬁc instant
in time. It would be interesting to develop new GAN architectures to generate synthetic time series of
these variables for use in the training of more complex IDS.

Acknowledgements

This work was partially supported by the European Union’s Horizon 2020 Research and Innovation Programme
under Grant 833685 (SPIDER) and Grant 101015857 (Teraﬂow). The second author acknowledges the hospi-
tality of the Department of Mathematics at Universidad Autónoma de Madrid, where part of this work was
conducted. The second author has been partially supported by Spanish Ministerio de Ciencia e Innovación
through project PID2019-106493RB-I00 (DL-CEMG).

References

[1] E. U. A. for Cybersecurity, “Threat landscape for 5G networks report.” https://www.enisa.europa.eu/

publications/enisa-threat-landscape-report-for-5g-networks. Accessed: 2021-04-30.

[2] D. Dasgupta, Z. Akhtar, and S. Sen, “Machine learning in cybersecurity: a comprehensive survey,” The

Journal of Defense Modeling and Simulation, p. 1548512920951275, 2020.

[3] S. Mahdavifar and A. A. Ghorbani, “Application of deep learning to cybersecurity: A survey,” Neurocom-

puting, vol. 347, pp. 149–176, 2019.

[4] E. U. A.

for

Law Enforcement Cooperation,

tiﬁcial
malicious-uses-and-abuses-of-artificial-intelligence. Accessed: 2011-04-30.

intelligence.”

ar-
https://www.europol.europa.eu/publications-documents/

“Malicious

abuses

uses

and

of

[5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,
“Generative Adversarial Nets,” in Advances in Neural Information Processing Systems 27 (Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, eds.), pp. 2672–2680, Curran Associates,
Inc., 2014.

[6] Z. Wang, Q. She, and T. E. Ward, “Generative Adversarial Networks in computer vision: A survey and

taxonomy,” arXiv preprint arXiv:1906.01529, 2019.

[7] N. Gao, H. Xue, W. Shao, S. Zhao, K. K. Qin, A. Prabowo, M. S. Rahaman, and F. D. Salim, “Generative

Adversarial Networks for spatio-temporal data: A survey,” arXiv preprint arXiv:2008.08903, 2020.

[8] A. Jabbar, X. Li, and B. Omar, “A survey on Generative Adversarial Networks: Variants, applications,

and training,” arXiv preprint arXiv:2006.05132, 2020.

[9] Z. Pan, W. Yu, X. Yi, A. Khan, F. Yuan, and Y. Zheng, “Recent progress on Generative Adversarial

Networks (GANs): A survey,” IEEE Access, vol. 7, pp. 36322–36333, 2019.

26

[10] S. W. Neville and K. F. Li, “The rational for developing larger-scale 1000+ machine emulation-based re-
search test beds,” in 2009 International Conference on Advanced Information Networking and Applications
Workshops, pp. 1092–1099, IEEE, 2009.

[11] B. Ferguson, A. Tall, and D. Olsen, “National cyber range overview,” in 2014 IEEE Military Communica-

tions Conference, pp. 123–128, IEEE, 2014.

[12] C. Xenakis, A. Angelogianni, E. Veroni, E. Karapistoli, M. Ghering, N. Gerosavva, V. Machamint,
P. Polvanesi, A. Brignone, J. N. Mendoza, and A. Pastor, “The SPIDER concept: A Cyber Range as
a Service platform,” Sept. 2020.

[13] A. Pastor, A. Mozo, S. Vakaruk, D. Canavese, D. R. López, L. Regano, S. Gómez-Canaval, and A. Lioy,
“Detection of encrypted cryptomining malware connections with machine and deep learning,” IEEE Access,
vol. 8, pp. 158036–158055, 2020.

[14] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,” 2017.

[15] Y. Bengio, Y. Lecun, and G. Hinton, “Deep learning for ai,” Communications of the ACM, vol. 64, no. 7,

pp. 58–65, 2021.

[16] A. Pastor, A. Mozo, D. R. Lopez, J. Folgueira, and A. Kapodistria, “The mouseworld, a security traf-
ﬁc analysis lab based on nfv/sdn,” in Proceedings of the 13th International Conference on Availability,
Reliability and Security, pp. 1–6, 2018.

[17] A. Mozo, J. L. López-Presa, and A. F. Anta, “A distributed and quiescent max-min fair algorithm for

network congestion control,” Expert Systems with Applications, vol. 91, pp. 492–512, 2018.

[18] H. Navidan, P. F. Moshiri, M. Nabati, R. Shahbazian, S. A. Ghorashi, V. Shah-Mansouri, and D. Win-
dridge, “Generative Adversarial Networks (GANs) in networking: A comprehensive survey & evaluation,”
Computer Networks, p. 108149, 2021.

[19] M. Rigaki and S. Garcia, “Bringing a GAN to a knife-ﬁght: Adapting malware communication to avoid

detection,” in 2018 IEEE Security and Privacy Workshops (SPW), pp. 70–75, May 2018.

[20] W. Hu and Y. Tan, “Generating adversarial malware examples for black-box attacks based on GAN,”

CoRR, vol. abs/1702.05983, 2017.

[21] C. Yin, Y. Zhu, S. Liu, J. Fei, and H. Zhang, “An enhancing framework for botnet detection using Generative
Adversarial Networks,” in 2018 International Conference on Artiﬁcial Intelligence and Big Data (ICAIBD),
pp. 228–234, May 2018.

[22] L. Vu, C. T. Bui, and Q. U. Nguyen, “A deep learning based method for handling imbalanced problem in
network traﬃc classiﬁcation,” in Proceedings of the Eighth International Symposium on Information and
Communication Technology, SoICT 2017, (New York, NY, USA), p. 333–339, Association for Computing
Machinery, 2017.

[23] Z. Wang, P. Wang, X. Zhou, S. Li, and M. Zhang, “FLOWGAN: unbalanced network encrypted traf-
ﬁc identiﬁcation method based on GAN,” in 2019 IEEE Intl Conf on Parallel Distributed Processing with
Applications, Big Data Cloud Computing, Sustainable Computing Communications, Social Computing Net-
working (ISPA/BDCloud/SocialCom/SustainCom), pp. 975–983, 2019.

[24] G. Draper-Gil, A. H. Lashkari, M. S. I. Mamun, and A. A. Ghorbani, “Characterization of encrypted and
VPN traﬃc using time-related,” in Proceedings of the 2nd international conference on information systems
security and privacy (ICISSP), pp. 407–414, 2016.

[25] A. S. Iliyasu and H. Deng, “Semi-supervised encrypted traﬃc classiﬁcation with deep convolutional Gener-

ative Adversarial Networks,” IEEE Access, vol. 8, pp. 118–126, 2020.

[26] M. Salem, S. Taheri, and J. S. Yuan, “Anomaly generation using Generative Adversarial Networks in
host-based intrusion detection,” in 2018 9th IEEE Annual Ubiquitous Computing, Electronics Mobile Com-
munication Conference (UEMCON), pp. 683–687, 2018.

[27] Z. Lin, Y. Shi, and Z. Xue, “IDSGAN: Generative Adversarial Networks for attack generation against

intrusion detection,” CoRR, vol. abs/1809.02077, 2018.

27

[28] A. Cheng, “PAC-GAN: Packet generation of network traﬃc using Generative Adversarial Network,” in 2019
IEEE 10th Annual Information Technology, Electronics and Mobile Communication Conference (IEM-
CON), pp. 0728–0734, Oct 2019.

[29] M. Ring, D. Schlör, D. Landes, and A. Hotho, “Flow-based network traﬃc generation using Generative

Adversarial Networks,” Computers and Security, vol. 82, pp. 156 – 172, 2019.

[30] M. Ring, S. Wunderlich, D. Grüdl, D. Landes, and A. Hotho, “Creation of ﬂow-based data sets for intrusion

detection,” Journal of Information Warfare, vol. 16, pp. 40–53, 2017.

[31] J. Davis and S. Magrath, “A survey of cyber ranges and testbeds,” Defense Technical Information Center,

2013.

[32] M. M. Yamin, B. Katt, and V. Gkioulos, “Cyber ranges and security testbeds: Scenarios, functions, tools

and architecture,” Computers & Security, vol. 88, p. 101636, 2020.

[33] E. Ukwandu, M. A. B. Farah, H. Hindy, D. Brosset, D. Kavallieros, R. Atkinson, C. Tachtatzis, M. Bures,
I. Andonovic, and X. Bellekens, “A review of cyber-ranges and test-beds: current and future trends,”
Sensors, vol. 20, no. 24, p. 7148, 2020.

[34] M. M. Yamin, M. Ullah, H. Ullah, and B. Katt, “Weaponized AI for cyber attacks,” Journal of Information

Security and Applications, vol. 57, p. 102722, 2021.

[35] F. Kamoun, F. Iqbal, M. A. Esseghir, and T. Baker, “AI and machine learning: A mixed blessing for
cybersecurity,” in 2020 International Symposium on Networks, Computers and Communications (ISNCC),
pp. 1–7, IEEE, 2020.

[36] N. Kaloudi and J. Li, “The AI-based cyber threat landscape: A survey,” ACM Computing Surveys (CSUR),

vol. 53, no. 1, pp. 1–34, 2020.

[37] A. Finamore, M. Mellia, M. Meo, M. M. Munafo, P. Di Torino, and D. Rossi, “Experiences of internet

traﬃc monitoring with tstat,” IEEE Network, vol. 25, no. 3, pp. 8–14, 2011.

[38] V. Nagarajan and J. Z. Kolter, “Gradient descent GAN optimization is locally stable,” in Advances in
Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems
2017, 4-9 December 2017, Long Beach, CA, USA, pp. 5585–5595, 2017.

[39] L. M. Mescheder, A. Geiger, and S. Nowozin, “Which training methods for GANs do actually converge?,”
in Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
Stockholm, Sweden, July 10-15, 2018, pp. 3478–3487, 2018.

[40] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved techniques for

training GANs,” arXiv preprint arXiv:1606.03498, 2016.

[41] C. K. Sønderby, J. Caballero, L. Theis, W. Shi, and F. Huszár, “Amortised MAP inference for image super-
resolution,” in 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings, 2017.

[42] M. Arjovsky and L. Bottou, “Towards principled methods for training Generative Adversarial Networks,”
in 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017.

[43] K. Roth, A. Lucchi, S. Nowozin, and T. Hofmann, “Stabilizing training of Generative Adversarial Networks
through regularization,” in Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 2018–2028,
2017.

[44] Á. González-Prieto, A. Mozo, E. Talavera, and S. Gómez-Canaval, “Dynamics of Fourier modes in torus

Generative Adversarial Networks,” Mathematics, vol. 9, no. 4, p. 325, 2021.

[45] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning with deep convolutional
Generative Adversarial Networks,” in 4th International Conference on Learning Representations, ICLR
2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.

28

[46] A. Odena, C. Olah, and J. Shlens, “Conditional image synthesis with Auxiliary Classiﬁer GANs,” in Inter-

national conference on machine learning, pp. 2642–2651, PMLR, 2017.

[47] L. Weng, “From GAN to WGAN,” arXiv preprint arXiv:1904.08994, 2019.

[48] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville, “Improved training of Wasserstein

GANs,” arXiv preprint arXiv:1704.00028, 2017.

[49] A. L. Maas, A. Y. Hannun, A. Y. Ng, et al., “Rectiﬁer nonlinearities improve neural network acoustic

models,” in Proc. icml, vol. 30, p. 3, Citeseer, 2013.

[50] T. T. Tanimoto, “Elementary mathematical theory of classiﬁcation and prediction,” Internal IBM Technical

Report, 1958.

[51] A. W. van der Vaart and J. A. Wellner, “Glivenko-Cantelli theorems,” in Weak Convergence and Empirical

Processes, pp. 122–126, Springer, 1996.

29

Appendix I: Figures

(a) F1-score on testing (left) and training (right) using real data for training and testing (400K/4K distribution).

(b) F1-score on testing (left) and training (right) using real data for training and testing (4K/4K distribution).

Figure 7: F1-score on testing (left) and training (right) using real data with 400K/4K (a) and 4K/4K (b) class
distributions for training (subsection 5.3.1). Results for decision thresholds of 0.2, 0.4, 0.5, 0.6 and 0.8 are
represented.

30

(a) Unbalanced dataset with the original 400K/4K distribution.

(b) Balanced dataset with 4K/4K distribution.

Figure 8: F1-score on testing (left) and training (right) using a naïve mean-based generator with unbalanced
and balanced datasets for training (subsection 5.3.2). Results for decision thresholds of 0.2, 0.4, 0.5, 0.6 and
0.8 are represented.

31

(a) Policy 1). Training with 400K/4K distribution and one generator chosen uniformly at random

(b) Policy 2). Training with 400K/4K distribution and a mix of two generators is chosen uniformly at random

(c) Policy 3). Training with 4K/4K distribution and one generator chosen uniformly at random

Figure 9: F1-score on testing (left) and training (right) using a standard GAN generator and sampling policies
1), 2) and 3) (subsection 5.3.3). Results for decision thresholds of 0.2, 0.4, 0.5, 0.6 and 0.8 are represented.

32

(a) Policy 1). Training with 400K/4K distribution and one generator chosen uniformly at random

(b) Policy 2). Training with 400K/4K distribution and a mix of two generators is chosen uniformly at random

(c) Policy 3). Training with 4K/4K distribution and one generator chosen uniformly at random

Figure 10: F1-score on testing (left) and training (right) using a generator with custom activation functions at
the output and policies 1), 2) and 3) (subsection 6.1). Results for decision thresholds of 0.2, 0.4, 0.5, 0.6 and
0.8 are represented.

33

(a) Training with 400K/4K distribution. 1 generator chosen uniformly at random

(b) Training with 400K/4K distribution. 1 generator chosen uniformly at random
ﬁltering positive values

(c) Training with 4K/4K distribution. 1 generator chosen uniformly at random

Figure 11: F1-score on testing (left) and training (right) with sampling elitism among the top 10 models in
training sorted by F1-score (subsection 6.3). Results for decision thresholds of 0.2, 0.4, 0.5, 0.6 and 0.8 are
represented.

34

Figure 12: F 1 − score on testing (left) and training (right) using the discriminator as a quality assurance
ﬁlter (subsection 6.2). Training with 400K/4K distribution. Results for decision thresholds of 0.2, 0.4, 0.5,
0.6 and 0.8 are represented.

(a) Using top 10 models sorted by L1 distance

(b) Using top 10 models sorted by Jaccard index

Figure 13: F 1 − score on testing (left) and training (right) with sampling elitism using policy 1). Elitism of
the top 10 sorted by statistical coeﬃcients (subsection 6.4. Results for decision thresholds of 0.2, 0.4, 0.5, 0.6
and 0.8 are represented.

35

