1
2
0
2

b
e
F
9

]

R
C
.
s
c
[

1
v
1
6
6
4
0
.
2
0
1
2
:
v
i
X
r
a

Security and Privacy for Artificial Intelligence:
Opportunities and Challenges

AYODEJI OSENI∗, The University of New South Wales @ ADFA, Australia
NOUR MOUSTAFA∗, The University of New South Wales @ ADFA, Australia
HELGE JANICKE, The Cyber Security Cooperative Research Centre (CSCRC), Australia
PENG LIU, Penn State, University Park, USA
ZAHIR TARI, RMIT University, Australia
ATHANASIOS VASILAKOS, The University of Technology Sydney, Australia

The increased adoption of Artificial Intelligence (AI) presents an opportunity to solve many socio-economic
and environmental challenges; however, this cannot happen without securing AI-enabled technologies. In
recent years, most AI models are vulnerable to advanced and sophisticated hacking techniques. This challenge
has motivated concerted research efforts into adversarial AI, with the aim of developing robust machine and
deep learning models that are resilient to different types of adversarial scenarios. In this paper, we present
a holistic cyber security review that demonstrates adversarial attacks against AI applications, including
aspects such as adversarial knowledge and capabilities, as well as existing methods for generating adversarial
examples and existing cyber defence models. We explain mathematical AI models, especially new variants of
reinforcement and federated learning, to demonstrate how attack vectors would exploit vulnerabilities of AI
models. We also propose a systematic framework for demonstrating attack techniques against AI applications,
and reviewed several cyber defences that would protect the AI applications against those attacks. We also
highlight the importance of understanding the adversarial goals and their capabilities, especially the recent
attacks against industry applications, to develop adaptive defences that assess to secure AI applications. Finally,
we describe the main challenges and future research directions in the domain of security and privacy of AI
technologies.

Additional Key Words and Phrases: Adversarial Attacks, Adversarial Examples, Deep Learning, Machine
Learning, Privacy, Security

ACM Reference Format:
Ayodeji Oseni, Nour Moustafa, Helge Janicke, Peng Liu, Zahir Tari, and Athanasios Vasilakos. 2020. Security
and Privacy for Artificial Intelligence: Opportunities and Challenges. J. ACM 37, 4, Article 111 (August 2020),
35 pages. https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
Recent advances in technology, coupled with growth in computational capacities, have led to
the adoption of Artificial Intelligence (AI) techniques in several applications [99]. For example,

Authors’ addresses: Ayodeji Oseni, ayodeji.s.oseni@gmail.com, The University of New South Wales @ ADFA, Canberra,
Australia; Nour Moustafa, nour.moustafa@unsw.edu.au, The University of New South Wales @ ADFA, Canberra, Australia;
Helge Janicke, helge.janicke@cybersecuritycrc.org.au, The Cyber Security Cooperative Research Centre (CSCRC), Perth,
Beijing Shi, Australia; Peng Liu, pliu@ist.psu.edu, Penn State, University Park, Pennsylvania, USA; Zahir Tari, zahir.tari@
rmit.edu.au, RMIT University, Melbourne, Australia; Athanasios Vasilakos, Athanasios.Vasilakos@uts.edu.au, The University
of Technology Sydney, Sydney, Australia.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2020 Association for Computing Machinery.
0004-5411/2020/8-ART111 $15.00
https://doi.org/10.1145/1122445.1122456

111

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

 
 
 
 
 
 
111:2

Oseni and Moustafa, et al.

machine learning models are being used to drive innovation in the area of health care, gaming
and finance, while autonomous car manufacturers rely on the deep learning models to create
pipelines for self-driving cars. Machine learning (ML) models and, more recently, deep learning
(DL) algorithms used in several AI systems today make it possible to automate tasks and processes,
thereby introducing new capabilities and functionalities that were not previously possible. For
instance, in 2019, DeepMind’s AlphaStar, an AI system based on deep reinforcement learning,
reached the Grandmaster level in the video game, StarCraft II, by beating several high-level human
players [105].

Despite the noticeable success and benefits of using machine learning, many of the machine
learning models in use today are vulnerable to several adversarial examples, where adversaries seek
to violate the confidentiality, integrity or availability of machine learning models by using inputs
that are specifically crafted to cause the models to make false predictions [10, 88]. In many cases,
AI systems are designed with no considerations for security, making them highly vulnerable to
adversarial examples. Adversarial attacks on AI systems can occur either at the training or testing
phase of machine learning [75]. At the training phase, an adversary can inject malicious data into a
training dataset to manipulate input features or data labels. These attacks, referred to as poisoning
attacks in literature, can easily be carried out in applications that use training data from untrusted
sources. Barreno et al. [10] demonstrated how an adversary, with knowledge of a machine learning
model, can change the original distribution of a training dataset by modifying the training data.
Attacks at the testing phase, known as evasion attacks, are the most prevalent type of attacks on
machine learning models as they exploit the vulnerabilities of a model to generate adversarial
examples, which are then used to evade the model at test time [75].

The adoption of AI techniques in many applications presents a unique opportunity to solve
many socio-economic and environmental challenges. However, this cannot happen without focused
research on securing these technologies. The field of adversarial machine learning has recently been
receiving significant research interest, and continuing focus in this area will undoubtedly ensure
the ubiquitous spread of transformative AI technologies. As AI becomes increasingly integrated
into different aspects of human activities and lifestyles, robust algorithms are essential to the
progression of a protected and safe innovative future.

Motivation – Adversarial examples make the machine learning models used in many AI systems
vulnerable to adversarial attacks. An adversarial attack on the confidentiality of a machine learning
model would seek to expose the model structure, the training or test dataset [99], thereby impacting
the privacy of data sources. Adversarial examples could also be used to attack the input integrity of
machine learning models, thereby exploiting the imperfections made by the learning algorithm
during the model’s training phase [96]. Attacks on availability fall within the realms of adversarial
behaviours which prevents legitimate users from being able to access a model’s outputs or features.
The security and privacy of AI systems against cyber adversaries have individually attracted
attention in the research community over the last few years [30, 99, 102, 104, 138]. However,
there has been minimal effort to review the literature and experimental results that illustrate a
comparative analysis of AI security and privacy, as we present in this paper.

Our Contributions – We summarize the main contributions of our work in five aspects:

• We present a detailed analysis of previous related surveys. This constitutes a contribution
to literature through the identification of limitations in previous surveys related to the
development of secure AI applications, as we addressed in this work.

• We present a brief overview of machine learning task categories, including deep learning

and federation learning

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:3

• We present a new adversarial attack framework that illustrates advanced attacks that would

exploit AI applications and measures their threats.

• We present a new defence framework that demonstrates cyber defence methods for protecting

AI systems against adversarial attacks

• We discuss challenges in this domain and provide suggestions on future research directions.

The remainder of this paper is structured as follows: First, in Section 2, we explain the most recent
surveys and reviews that attempt to explain security or privacy insights of AI applications. Second,
we provide an overview of machine and deep learning task categories while also considering
federated learning in Section 3. Third, we discuss the most complete set of attack types that would
breach AI applications, including real-world attack scenarios and motives, in Section 4. Fourth,
we explain in Section 5 defense methods and techniques that would protect AI models against
cyber adversaries. Lastly, we explain lessons learned and future research directions based on our
systematic review analysis, in section 6.

2 MOST RECENT REVIEWS
To develop an overview of academic activity relating to security and privacy of AI technologies,
we identify and review the most recent surveys and reviews conducted between 2018 and 2020 as
presented in Table 1. A total of 16 high-quality systematic reviews were identified; 11 of which
were published in peer-reviewed journals such as IEEE Access, Elsevier, ACM, Applied Sciences
and ScienceDirect. An analysis of these reviews is presented as follows.

From a data-driven view, Liu et al. [75] presented a systematic survey on the existing security
threats and corresponding defensive techniques during the training and testing phases of machine
learning. Given the lack of a comprehensive literature review covering the security threats and
defensive techniques during the two phases of machine learning, their foundational work provided
a detailed summary of existing adversarial attack techniques against machine learning and counter-
measures. Particularly, they presented a detailed description of machine learning and introduced
the concept of adversarial machine learning. While their survey has informed further research into
adversarial machine learning, the authors did not review existing security threats on reinforcement
learning.

The first comprehensive survey in the domain of adversarial attacks on deep learning in computer
vision was undertaken by Akhtar and Mian [3]. Reviewing existing and proposed defenses for
adversarial attack methods, this survey includes a discussion of adversarial attack methods that have
been used successfully against deep neural networks in both ’laboratory settings’ and real-world
scenarios. This work is however limited as it presents the most ’influential’ and ’interesting’ attacks
on deep learning in the restricted context of computer vision.

Based on a growing recognition that machine learning models are increasingly vulnerable to a
wide range of adversarial capabilities, Papernot et al. [99] carried out a systematic review of ML
security and privacy with specific focus on adversarial attacks on machine learning systems and
their countermeasures. In this article, the authors considered ML threat model from the perspective
of a data pipeline and reviewed recent attacks on ML and their defenses at the training and testing
stages. Their review also covered existing works in the area of differential privacy. A review of the
current landscape of research in the area of adversarial machine learning presented by Thomas and
Tabrizi [122] analyses the results and trends in adversarial machine learning research. However,
this work did not specifically focus on any of the attacks on ML systems and it lacks the depth of
many review papers in this domain.

On the security and privacy of deep learning systems in several applications, Bae et al. [7]
presents a systematic review of recent research. Covering the foundations of deep learning and

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:4

Oseni and Moustafa, et al.

Table 1. Recent Surveys and Reviews in the area of Security and Privacy in AI

Main Contributions of Work

Limitations of Work

Reference

Liu et al. [75]

Publication
Date
February 2018

Main Area of
Focus
Machine
Learning

Research
Method
Systematic
review

Akhtar and Mian
[3]

February 2018

Deep Learning

Papernot et al. [99]

April 2018

Supervised
Learning
(Classification)

Thomas
Tabrizi [122]

and

July 2018

Machine
Learning

Bae et al. [7]

July 2018
(pre-print)

Deep Learning

Chakraborty et al.
[27]

September
2018 (pre-print)

Deep Learning

Li et al. [73]

October 2018
(pre-print)

Deep Learning

Systematic
review

Systematic
review

Semi-
systematic
review

Systematic
review

Systematic
review

Systematic
review

Reviewed a cross-section of security
threats and defensive techniques dur-
ing stages of ML from a data-driven
view
Provided a comprehensive survey of ad-
versarial attacks on DL in computer vi-
sion
Cataloging of attacks and defenses in
ML. Introduced a unified threat model
in describing security and privacy is-
sues in ML systems
Surveyed the current landscape of re-
search in the area of adversarial ma-
chine learning with analysis of results
and the trends in research
Surveyed possible attacks and current
defense methods on DL. Taxonomized
approaches to Privacy in DL
Provided a summary of recent ad-
vances in adversarial attacks with their
countermeasures.
Reviewed some of the attack and defen-
sive strategies in deep neural networks

Ozdag [95]

November 2018

Deep Learning

Systematic
review

Reviewed types of adversarial attacks
and defenses in deep neural networks

Biggio and Roli [19]

December 2018

Machine
Learning

Systematic
review

Qiu et al. [103]

March 2019

Deep Learning

Systematic
review

Reviewed works focusing on security
of machine learning, pattern recogni-
tion, and deep neural networks
Reviewed recent studies on AI adver-
sarial attack and defense technologies

Wang et al. [126]

August 2019

Machine
learning

Systematic Re-
view

Wiyatno et al. [128]

November 2019
(pre-print)

Supervised
Learning
(Classification)

Systematic
review

Pitropakis
[102]

et

al.

November 2019

Machine
Learning

Systematic
review

Li et al. [74]

February 2020

Supervised
Learning

Systematic
review

Ren et al. [104]

March 2020

Deep Learning

Zhang et al. [138]

April 2020

Deep Learning

Systematic
review

Systematic
review

adversarial

review of

Review of the security properties of ma-
chine learning algorithms in adversar-
ial settings
In-dept
at-
tacks, defenses and discussion of the
strengths and weaknesses of each
method
Provided a comprehensive taxonomy
and systematic
review of attacks
against machine learning systems
Provides a complimentary summary
of adversarial attacks and defenses
for Cyber-Physical systems beyond the
field of computer vision
Provided summarized review of adver-
sarial attacks and defensive techniques
in deep learning
Reviewed research efforts on gener-
ating adversarial examples on textual
deep neural networks

Review did not include existing
threats on Reinforcement Learning

Restrictive context of computer vi-
sion

Article primarily focused on at-
tacks and defenses relating to ML
classification

Lacks the depth of most systematic
review papers in this domain

Survey paper only focused on secu-
rity and privacy issues in DL

Paper did not address ways in
which countermeasures can be im-
proved
Selective review of a small fraction
of attack and defensive strategies in
deep neural networks and mainly
in context of computer vision.
Survey paper lacks the depth of
some of the other reviews on adver-
sarial attacks in the DL domain
Review only focused on adversarial
attacks in the context of computer
vision and cybersecurity
Work did not cover some of the pri-
vacy attacks on traditional machine
learning algorithms such as Naive
Bayes, decision trees and random
forests
Work only reviewed a small frac-
tion of adversarial attacks against
ML systems
Paper only focused on adversarial
attack methods and defenses which
apply to ML classification tasks

Paper did not include review of de-
fenses against the adversarial at-
tacks
Survey paper was based solely on
cyber-physical systems

Paper focused mainly on attacks
and defenses in the context of com-
puter vision
Survey paper did not cover many
of the defensive techniques for at-
tacks on deep learning models

some of the privacy-preserving techniques in literature, the article introduces the concept of Secure
AI and provides an extensive analysis of many of the attacks that have successfully been used
against deep learning as well as their defense techniques. Approaches to privacy in deep learning
are taxonomized and future research directions are presented based on identified gaps. A review of
different types of adversarial attacks and their defences reported by Chakraborty et al. [27] provides
a summary of adversarial attack types and proposes defenses through analysis of different threat
models and attack scenarios. The attacks and countermeasures reviewed were not restrictive to
specific deep learning applications. Particularly, the need for robust deep learning architectures is
suggested as countermeasures against adversarial attacks. However, the article did not provide steps
in which this can be achieved. Such steps might have been included in future research directions.

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:5

An introduction into the foundations of adversarial machine learning presented by Li et al. [73]
focuses on recent attack and defensive strategies in deep neural networks. This work describes some
of the metrics used in literature for measuring perturbations of input samples. Whilst including
adversarial attacks in reinforcement learning, the work only covered a small fraction of the attack
and defensive strategies in deep neural networks that have been proposed in literature. A review of
some of the well-known adversarial attacks and defenses against deep neural networks provided by
Ozdag [95] reports some of the solution models and results presented in the NIPS 2017 Adversarial
Learning competition organized by Google Brain. Though showcasing relevant information, the
survey did not cover many of the adversarial attacks that have been demonstrated against deep
neural networks.

Biggio and Roli [19] in their paper provided an overview of the state of research in the area of
adversarial machine learning covering a period of about ten years mainly in the context of computer
vision and cybersecurity tasks. The paper covers foundations of deep learning threat models, attack
types and their timeline, as well as misconceptions relating to the security evaluation of machine
learning algorithms. Furthermore, the paper suggests the need for research into the design of
machine learning models that are against adversarial attacks. Similar to other review papers, Qiu
et al. [103] presented a comprehensive review of recent research progress on adversarial attacks
against deep learning and their defenses. The review focused on attack methods at both the training
and testing stages of machine learning in the contexts of computer vision, image classification,
natural language processing, cybersecurity and the cyber-physical world. The review also includes
a number of proposed defensive strategies in literature. While this work comprehensively reviewed
several demonstrated attacks against AI systems and their defenses, it did not cover some of the
privacy issues associated with traditional machine learning algorithms.

A review of the security properties of machine learning algorithms in adversarial settings pre-
sented by Wang et al. [126] is also similar to other recent reviews in this area, covering foundations
of adversarial machine learning. Some of the adversarial attacks that have been demonstrated in
literature as well as their countermeasures are also included. A comprehensive review of adversarial
attack methods against machine learning models in the visual domain is reported by Wiyatno
et al. [128] with specific focus on the application of adversarial examples to machine learning
classification tasks. Discussing the strengths and weaknesses of each of the reviewed adversarial
attacks and defences, the review covered many of the attacks against machine learning systems in
literature. Our work significantly expands their work with an analysis of attacks on unsupervised
and reinforcement learning, which is not included in [128].

A taxonomy and review of attacks against machine learning systems undertaken by Pitropakis
et al. [102] categorizes attacks based on their key characteristics in order to understand the existing
attack landscape towards proposing appropriate defenses. This work adequately covers many of the
adversarial attacks on machine learning in the context of intrusion detection, spam filtering, visual
recognition and other applications. A limitation of this work however is that it did not include
review of defenses against the adversarial attacks. A review of adversarial attacks and potential
defenses in non-camera sensor-based cyber-physical systems by Li et al. [74] focuses on adversarial
attacks on surveillance sensor data, audio data and textual input, and their defensive strategies.
The work introduces a general workflow which could be used to describe adversarial attacks on
cyber-physical systems, thus covering recent adversarial attacks against cyber-physical systems. It
is however important to note that only classification and regression tasks are covered.

A paper on the review of adversarial attacks and defenses in deep learning by Ren et al. [104]
introduces foundational concepts of adversarial attacks and then provides summarized review
of adversarial attacks and defensive techniques in deep learning. While some of the adversarial
attacks reviewed in this work are based on recent publications, the paper emphasises only attacks

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:6

Oseni and Moustafa, et al.

applicable to the context of computer vision. Recent research efforts focusing on adversarial attacks
on deep learning models in the context of natural language processing (NLP) surveyed by Zhang
et al. [138] covers the foundations of adversarial attacks and deep learning techniques in natural
language processing. The survey paper includes summaries of black-box and white-box attacks
methods that have been demonstrated on deep learning models in NLP. They also reviewed two
common defensive techniques that have been proposed in literature for achieving robust textual
deep neural networks, namely adversarial training and knowledge distillation. While this work
covers most of the attacks that have been demonstrated against textual deep neural networks to
date, it did not cover many of the defensive techniques that have been proposed in this area.

3 MACHINE LEARNING OVERVIEW
The idea of machine learning (ML) is not new; it has been since the 1970s when the first set of
algorithms were introduced [77]. Machine learning deals with the problem of extracting features
from data in order to solve many predictive tasks; examples of which are forecasting, anomaly
detection, spam filtering and credit risk assessment. Its primary goal is to predict results based on
some input data. Data is a fundamental component of every machine learning system. For instance,
in order to predict if an email is a spam or not, a machine would need to have been trained with
samples of spam messages - the more diverse the data used in training a machine, the better the
result of the prediction. Input data in machine learning is typically divided into training and test
data. Training data is used in developing a machine learning model; and once the accuracy of
prediction of the model is satisfactory, test data is then fed into the model.

The main components of Machine Learning are: tasks, models and features [42]. Tasks refer
to the problems that can be solved using machine learning. Many machine learning models are
designed to solve only a small number of tasks. Models define the output of machine learning.
They are simply trained by sample data in order to process additional data for making predictions.
Features are an essential component of machine learning as they are characteristics of the input
data which simplifies learning of patterns between the input data and output data. Algorithms are
used to solve learning problems or tasks. Flach [42] described machine learning as the art of using
right features to develop the right models used to solve a given problem.

Fig. 1. Machine Learning Task Categories

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:7

The tasks that are solved using machine learning are commonly divided into: Supervised Learning,
Unsupervised Learning, Semi-supervised Learning and Reinforcement Learning [71], as detailed
below. Fig. 1 shows the machine learning task categories.

Supervised Learning is a machine learning approach where an algorithm learns patterns from
labelled training dataset and uses this for prediction or classification [121]. In supervised learning,
the training dataset is either pre-categorised or numerical. Supervised learning tasks can be grouped
into either Classification and Regression techniques. On the other hand, Unsupervised learning
deals with unlabeled input data and the learning algorithms focuses on analyzing similarities among
elements of the input data to infer meaningful features [71, 121] which are then extracted to create
possible output labels. As the name implies, Semi-supervised learning is an approach to machine
learning that combines elements of supervised learning and unsupervised learning. It extend
elements of supervised learning type to include additional information typical of the unsupervised
learning and vice versa [139]. Reinforcement learning is a machine learning method that enables
an agent to learn in an interactive environment through trial and error using feedback that it obtains
from its own actions and experiences [117, 121]. The reinforcement learning problem involves
an agent which interacts with its environment by performing certain actions and then receiving
rewards for its actions. The goal of this learning approach is therefore to learn how to take actions
in order to maximize the reward.

3.1 Deep Learning Methods
Deep learning is a specialized field of machine learning based on deep artificial neural networks. The
machine learning methods discussed so far in this section are today referred to as Shallow Learning
due to the requirement of a feature engineer to identify relevant characteristics from the input
data [4]. A common property of many of the shallow learning techniques is their relatively simple
architecture often consisting of a single layer used to convert the input data into a problem-specific
feature space [37]. In contrast, the more recent Deep Learning techniques rely on multi-layered
representation and abstraction of the input data to perform complex learning tasks and feature
selection [4, 71]. While many shallow learning algorithms have been effective in solving many
well-constrained problems, they have been found to perform poorly in problems that involve
extracting well-represented features from data such as in areas of computer vision and natural
language processing [36]. However, deep learning solves this problem by building multiple layers
of simple features to represent a complex concept [136]. The vast increase in the amount of data
available nowadays and the increased chip processing abilities has played an important role in the
development of deep learning architectures [127].

3.2 Federated Learning
The concept of federated learning was originally introduced by Google in 2016 [81, 134]. This
learning approach enables the training of a centralized model on unevenly distributed data spread
over federated network of nodes [20, 65]. The primary motivation for federated learning arises from
the need to train models based on data from users’ mobile devices which cannot be stored centrally
data centers due to privacy concerns [64]. Federated learning provides distinct privacy advantages
compared to the other machine learning types due to the fact that only minimal updates necessary
to improve a particular model is transmitted for federated learning, and this data is dependent on
the training objective. The performance of federated learning improves with more data, that is, the
more the nodes available to train the model, the better the model.

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:8

Oseni and Moustafa, et al.

4 ATTACKS ON AI SYSTEMS: A HOLISTIC REVIEW
The widespread use of AI technologies and the fact that most of these technologies are vulnerable
to adversarial attacks is concerning. Adversarial attacks are often in the form of manipulations
of the input to an ML or DL model with the aim of causing a misclassification of the input data.
Adversarial attacks on AI models differ and depends largely on the task category to which the
algorithm belongs. In other words, threats to AI systems, in general, are mostly the same, but the
approaches to exploitation differ depending on the algorithm in use. In this section we first present
a result of literature search covering attacks on AI systems over the last 10 years and then present
a detailed analysis of the identified literature. Literature covering attacks on AI systems were
identified through Google Scholar search using key words such as Adversarial Attacks, Adversarial
Examples, Adversarial Machine Learning, Attacks and Defenses on Deep Learning, Attacks and
Defenses on Machine Learning, Membership Inference Attacks, Inversion Attacks, Extraction
Attacks, Evasion Attacks and Poisoning Attacks. Many of the papers selected were based on works
carried out between 2010 and 2020 and the results of the search are as presented in Table 2 and
Table 3. To effectively assess the security and privacy of AI systems, it is important to understand
the attack surface of the system, and the attack vectors that an attacker can potentially use to
exploit the system. We therefore draw insights from existing frameworks [9, 13, 16, 58, 87, 99] and
present a new framework as a holistic approach to the quantitative analysis of adversarial attacks
on AI models (Fig. 2).

4.1 AI Attack Surface
The attack surface of an AI system is the total sum of vulnerabilities the AI model is exposed to
during both the training and testing phases. It can also be described as a list of inputs that an
adversary can use to attempt an attack on the system. As shown in Fig.2, the attack surface of
an AI system can be viewed in terms of a generalized data processing pipeline which comprises
of the training/test input data or objects, the learning algorithm/model, and the output data. At
the testing stage, the input features are processed by the machine learning model to produce the
class probabilities, which are further communicated to an external system in the form of an action
to be acted upon. An adversary can attempt to attack this system by poisoning the training data,
corrupting the learning model or tampering with the class probabilities.

4.2 Attacker’s Goals
Adversarial goals can be broadly described based on the security properties of an information
system, namely: confidentiality, integrity, availability and privacy. An adversary’s goal in attacking
the confidentiality of an AI system is to gather insights about the internals of the learning model or
dataset and to use this information to carry out more advanced attacks. In other words, an attack
on confidentiality can be targeted towards a model and its parameters or the training data [99]. The
goal of an attack on integrity is to modify the AI logic, through interaction with the system either
at the learning or inference stage, while also controlling the model’s outputs. In the example of a
spam filter, an adversary can poison some data in the training dataset with the goal of changing the
classification boundary such that a legitimate email is instead classified as spam and the adversary
can evade detection without compromising normal system operations. According to Papernot et al.
[98], four goals that impacts the integrity of a deep learning system are: (1) confidence reduction,
(2) misclassification (3) targeted misclassification (4) source/target misclassification.

In adversarial settings, the availability of an AI solution can be attacked with the goal of disabling
the system’s functionality. For instance, an AI system can be flooded with incorrectly classified
objects with the aim of causing the model to be unreliable or inconsistent in the target environment

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:9

Fig. 2. Proposed Framework for analysis of adversarial attacks against AI models

[99]. Attacks on the availability of a machine learning system could lead to many classification
errors, making the system effectively unusable. Attacks on confidentiality and privacy are related
in goal and method. An adversary seeks to violate the privacy of a machine learning system with
the goal of causing it to reveal information about the training data and model. Potential privacy
risks may involve an adversary with partial information about a training sample, attempting to
manipulate a model into revealing information about unknown portions of the sample [58] or the
extraction of the training dataset using the model’s predictions.

4.3 Attacker’s Knowledge and Capabilities
An attacker’s knowledge of a machine learning system can be defined based on the knowledge of
single components involved in the design of the system [13, 16]. An attacker can have different
levels of knowledge of the system such as training data D, features X, learning algorithm 𝑓 ,
objective function L, and parameters w. The adversary’s knowledge can therefore be characterised
as a vector 𝜃 ∈ Θ where 𝜃 = (D, X, 𝑓 , w). On the other hand, adversarial capabilities refers to
the level of information or knowledge about the machine learning system that is available to the
adversary [27]. For instance, in the example of a spam detector pipeline described earlier in this

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:10

Oseni and Moustafa, et al.

Table 2. Survey of Training Phase Attacks on AI Systems

Reference

Biggio et al. [18]

Year

2012

Attack
Type
Poisoning

Biggio et al. [17]

2012

Poisoning

Biggio et al. [14]

2013

Poisoning

Newell et al. [93]

2014

Poisoning

Biggio et al. [13]

2014

Poisoning

Biggio et al. [16]

2014

Poisoning

Mozaffari-Kermani
et al. [86]

2015

Poisoning

Li et al. [72]

2016

Poisoning

Muñoz-González
et al. [87]

2017

Poisoning

and

Burkard
Lagesse [22]
Yang et al. [133]

2017

Poisoning

2017

Poisoning

Shi and Sagduyu
[111]
Gu et al. [50]

2017

Poisoning

2017

Backdoor

Jagielski et al. [60]

2018

Poisoning

Shafahi et al. [108]

2018

Poisoning

Suciu et al. [116]

2018

Poisoning

Lovisotto et al. [78]

2019

Poisoning

Jiang et al. [61]

2020

Poisoning

Kloft and Laskov
[62]

2020

Poisoning

ML Algorithm

ML Application

Dataset

Attacker’s Goal

Attacker’s Knowl-
edge

Support Vector
Machine
Principal
Component
Analysis
Principal
Component
Analysis
Support Vector
Machine, Naive
Bayes
Support Vector
Machine
v-Support Vector
Machine
Best-first decision
tree, Ripple-down
rule learner, Naive
Bayes decision tree,
Nearest-neighbor
classifier,
Multilayer
Perceptron
Collaborative
Filtering
Multi-Layer
Perceptrons,
Logistic Regression,
Single-layer
Artificial Neural
Network,
Convolutional
Neural Network
Support Vector
Machines
Feed-forward
Neural Network,
Convolutional
Neural Network
Feed-forward
Neural Network
Convolutional
Neural Network
Linear Regression

Convolutional
Neural Network
Convolutional
Neural Network,
Linear Support
Vector Machine,
Random Forest

Convolutional
Neural Network
(FaceNet, VGG16,
ResNet-50)
Convolutional
Neural Network
Centroid

Handwritten digit
recognition
Adaptive biometric
recognition system

Adaptive biometric
verification system

MNIST dataset

2400 face images

Integrity attack

White-box

2400 face images

Integrity attack

Grey-box

Sentiment Analysis

Twitter ’tweets’

Integrity attack

Black-box, White-
box

Image recognition

MNIST

Integrity attack

White-box

Network intrusion
detection
Healthcare

Network packets

Medical dataset

Indiscriminate
integrity attack
Targeted integrity
attack

Grey-box

Grey-box

Collaborative
Filtering
Spam filtering,
Malware detection,
Handwritten digit
recognition

MovieLens

Spambase,
Ransomware,
MNIST

Integrity and
Availability attack
Integrity attack

White-box

White-box

Data Streams

Image
Classification

Integrity attack

White-box

MNIST, CIFAR-10

Integrity attack

White-box

Text and Image
Classification
Digit Recognition

Reuters-21578,
Flower dataset
MNIST

Integrity attack

Black-box

Availability attack

Warfarin Dose
Prediction, Loan
Assessment, House
pricing prediction
Image
Classification
Image Recognition,
Android Malware
detector,
Twitter-based
exploit predictor,
Data breach
predictor
Biometric system

Healthcare dataset,
Loan dataset and
House pricing
dataset
CIFAR-10

CIFAR-10, Dreblin
dataset, Twitter
tweets

Poisoning
availability attack

White-box, Black-
box

Integrity attack

Grey-box

Integrity attack

Grey-box

VGGFace,
VGGFace2

Integrity attack

White-box

Image Recognition

BelgiumTS, GTSRB

Integrity attack

Black-box

Centroid Anomaly
Detection

real HTTP traffic

Violate system
integrity

White-box

section, an adversary with access to or knowledge of the spam detector model used for classification
has ’better’ adversarial capabilities on the system relative to an adversary with only access to the
’tokenized’ text data or incoming email.

According to Papernot et al. [99], the adversarial capabilities in machine learning systems can
be classified based on how they relate to the training and testing phases, as explained below. Fig.

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:11

Attacker’s
Goal
Misclassification

Attacker’s
Knowledge
Black-box

Indiscriminate
integrity
violation
Targeted
integrity
violation
Confidence
reduction

White-box

Grey-box

Black-box

Misclassification White-box

Misclassification White-box

Misclassification White-box

Misclassification White-box

White-box

Targeted Mis-
classification,
Misclassifica-
tion

Misclassification White-box

Targeted Mis-
classification

Targeted Mis-
classification

Targeted Mis-
classification

Black-box

White-box

White-box

Table 3. Survey of Testing Phase Attacks on AI Systems

Reference

Year

Szegedy et al. [118]

2013

Attack
Type
Evasion

Biggio et al. [16]

2014

Oracle

ML Algorithm

ML Application

Dataset

Convolutional Neural Network
(AlexNet, QuocNet), Fully
Connected Network,
Autoencoder
Support Vector Machine,
Logistic Regression

Handwritten Digit
Recognition, Image
Classification

MNIST,
ImageNet

Spam Filtering

TREC 2007

Biggio et al. [16]

2014

Oracle

Multimodal System

Biometric
authentication

NIST Biometric
Score set

Laskov et al. [68]

2014

Evasion

Nguyen et al. [94]

2015

Evasion

Papernot et al. [98]

2016

Evasion

Moosavi-Dezfooli
et al. [85]

2016

Evasion

Kurakin et al. [66]

2016

Evasion

Sharif et al. [110]

2016

Evasion

Support Vector Machine,
Random Forest
Convolutional Neural Networks

Convolutional Neural Network
(LeNet)
Fully Connected Network,
Convolutional Neural Network
(LeNet, CaffeNet, GoogLeNet)
ImageNet Inception classifier

VGG-Face Convolutional Neural
Network

PDF Malware
Detection
Image
Classification
Handwritten Digit
Recognition
Handwritten Digit
Recognition, Image
Classification
Image
Classification
Facial Recognition
System

Contagio and
Operational
ImageNet,
MNIST
MNIST

MNIST,
CIFAR-10,
ImageNet
ImageNet

PubFig Image
database

Grosse et al. [48]

2016

Evasion

Feed Forward Neural Network

Malware Detection

Sarkar et al. [107]

2017

Evasion

Convolutional Neural Network

Carlini and Wagner
[25]

Baluja and Fischer
[8]

Moosavi-Dezfooli
et al. [84]

2017

Evasion

Convolutional Neural Network

2017

Evasion

Convolutional Neural Network,
Fully Connected Network

2017

Evasion

Convolutional Neural
Network(CaffeNet, VGG-F,
VGG-16,VGG-19,GoogLeNet,
ResNet-152)
Convolutional Neural Network
(Inception-v3)

Chen et al. [32]

2017

Evasion

Adate et al. [2]

2017

Evasion

Convolutional Neural Network

Madry et al. [80]

2018

Evasion

Convolutional Neural Network

Samanta and Mehta
[106]

2017

Evasion

Convolutional Neural Network

Dang et al. [35]

2017

Evasion

Hu and Tan [57]

2017

Evasion

Ilyas et al. [59]

2018

Evasion

SVM, Random Forest, Decision
tree
Random Forest, Logistic
Regression, Decision Trees,
Support vector Machines,
Multi-layer Perceptron (MLP)
Convolutional Neural Network

Pengcheng et
[101]
Dong et al. [39]

al.

2018

Evasion

Convolutional Neural Network

2018

Evasion

Convolutional Neural Network

He et al. [51]

2018

Evasion

Convolutional Neural Network

Chen et al. [31]

2018

Evasion

Convolutional Neural Network

Handwritten Digit
Recognition, Image
Classification
Handwritten Digit
Recognition, Image
Classification
Handwritten Digit
Recognition, Image
Classification
Image Recognition

Image
Classification

Image
Classification
Image
Classification
Sentiment
Analysis, Gender
Detection
PDF Malware
Detection
Malware Detection

Image
Classification
Image
Classification
Image
Classification
Image
Classification
Image
Classification

Carlini and Wagner
[26]

2018

Evasion

Recurrent Neural Network
(DeepSpeech)

Automatic Speech
Recognition

Eykholt et al. [40]

2018

Evasion

Convolutional Neural Networks

Chen et al. [29]

2018

Evasion

Su et al. [115]

2019

Evasion

Convolutional Neural Networks
(Inception-v3)
AllConv, NiN, VGG16 network

Stop Sign
Recognition
Image Captioning

Image
Classification

DREBIN
Android
malware dataset
MNIST,
CIFAR-10

MNIST,
CIFAR-10,
ImageNet
MNIST,
ImageNet

ILSVRC 2012

Misclassification White-box

MNIST,
CIFAR-10,
ImageNet
MNIST

Misclassification,
Targeted Mis-
classification

Black-box

Misclassification White-box

MNIST,
CIFAR-10
IMDB movie
review, Twitter
tweets
Contagio

Dataset crawled
from
malwr.com

ImageNet

MNIST,
CIFAR-10
ImageNet

MNIST,
CIFAR-10
MNIST,
CIFAR-10,
ImageNet
Mozilla
Common Voice
dataset
LISA, GTSRB

Microsoft
COCO
CIFAR-10,
ImageNet

Misclassification White-box,
Black-box
White-box

Confidence
Reduction

Misclassification

Black-box

Confidence
reduction

Black-box

Targeted Mis-
classification
Misclassification

Black-box

Black-box

Misclassification White-box,
Black-box
Misclassification White-box

Targeted Mis-
classification

Targeted Mis-
classification

White-box

White-box

Targeted Mis-
classification
Targeted Mis-
classification
Misclassification

White-box

White-box

Black-box

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:12

Oseni and Moustafa, et al.

3 shows a relationship between an adversary’s knowledge of a machine learning system and the
adversary’s capabilities.

Fig. 3. Attacker’s Capabilities

Training Phase Capabilities. – Attacks during the training phase seek to learn, influence or alter
the performance of the model. The most straightforward attack on the training phase is one in
which the adversary attempts to read or access a portion or all of the training data. An adversary
that does not have access to the training data nor the learning algorithm can carry out a data
injection attack by adding adversarial data to the existing training dataset. An adversary with
knowledge of the training dataset but not the learning algorithm can directly poison the data or
label (in supervised learning datasets) before it is used to train the target model. This type of attack
capability is referred to as data/label modification. Lastly, an adversary with knowledge about
the internals of an algorithm can carry out a logic corruption attack by altering the learning logic.
Logic corruption attacks are the most advanced attacks on the training phase of a machine learning
system and are difficult to defend against.

Testing Phase Capabilities. – Testing phase attacks are referred to as exploratory attacks and
according to Barreno et al. [10], these attacks do not alter the training process nor influence
the learning, but rather attempt to discover information about the state of the learner. Inference
attacks rely on information about the model and its use in the target environment. As illustrated in
Fig. 3, adversarial capabilities at the testing phase can be broadly classified into either black-box
or white-box attacks. In a White-box attack setting, the adversary has knowledge of everything
about the model, including its training data, architecture, parameters, intermediate computations
at hidden layers, as well as any hyper-parameters used for predictions [19], so we characterize
white-box adversary’s knowledge 𝜃𝑊 𝑏 = (D, X, 𝑓 , w).Black-box attacks assumes that the adversary
has no knowledge about the model but is able to use information about previous input/output
pairs to infer vulnerabilities in the model [99]. The model parameters and architecture are not
accessible to the adversary as in the case of Machine Learning as a Service (MLaaS) platforms [90],
so 𝜃𝐵𝑏 = ( ˆD, ˆX, ˆ𝑓 , ˆw).

4.4 Attack Strategy
An attack strategy can be described as the method used by an adversary to modify the training and
test dataset in other to optimize an attack [126]. Thus, given an adversary’s knowledge 𝜃 ∈ Θ and

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:13

a set of adversarial examples D ′
objective function A (D ′
D ′

𝑐 ∈ Φ(D𝑐 ), an adversary’s goal can be expressed in terms of an
𝑐, 𝜃 ) ∈ R which is a measure of the effectiveness of an attack with samples

𝑐 . An adversary’s optimal attack strategy can therefore be expressed as:

D∗

𝑐 ∈ argmax
D′𝑐 ∈Φ( D𝑐 )

A (D ′

𝑐, 𝜃 )

(1)

4.5 Adversarial Attack Types
The most common attack types against AI systems are discussed in this section based on the
techniques used by an attacker.

4.5.1 Poisoning Attacks. Poisoning attacks, sometimes referred to as causative attacks, are staged at
the training phase where the adversary alters the training dataset by editing, injecting or removing
samples with the aim of changing the decision boundary of the target model [99]. They can target
the integrity of adaptive or online classifiers, as well as classifiers being retrained on data collected
at test time [13]. Some examples of poisoning attacks on AI systems are presented in Table 2.
According to Muñoz-González and Lupu [89], many of the data used in training machine learning
systems come from untrusted sources, so the problem of poisoning attacks can be described as
related to the reliability of large amount of data collected by these systems. While an adversary
may be unable to directly access an existing training dataset, the ability to provide new training
data via web-based repositories and honeypots provides an opportunity to poison training data
[18].

Some literature [13, 19, 89] have described two main scenarios of poisoning attacks against

multi-class classification systems namely:

Error-generic poisoning attacks - These are the most common poisoning attacks in which the
attackers aim to cause a denial of service attack in the system, by producing as many misclassifi-
cations on targeted data points as possible irrespective of the classes in which they occur. Using
formulation of the attack strategy in Eq.1, the error-generic poisoning attack can be expressed as:

D∗

𝑝 ∈ argmax
D𝑝 ∈Φ( D𝑝 )

A (D𝑝, 𝜃 ) = L (D𝑡𝑎𝑟𝑔𝑒𝑡, w(D𝑝 ))

(2)

where the objective function an adversary aims to maximize, A, is defined in terms of loss
function, L, computed on set of points, D𝑡𝑎𝑟𝑔𝑒𝑡 , targeted by the attacker. The loss function, L, is a
function of parameters, w, which depends on injected poisoning points, D𝑝 .

Error-specific poisoning attacks - The aim of an adversary in error-specific poisoning attacks is to

cause specific misclassifications [89]. The attack strategy in this case can be expressed as:

D∗

𝑝 ∈ argmax
D𝑝 ∈Φ( D𝑝 )

A (D𝑝, 𝜃 ) = argmax
D𝑝 ∈Φ( D𝑝 )

−L (D ′

𝑡𝑎𝑟𝑔𝑒𝑡, w(D𝑝 ))

(3)

where D𝑡𝑎𝑟𝑔𝑒𝑡 contains the same samples as D ′

𝑡𝑎𝑟𝑔𝑒𝑡 but with labels chosen by the adversary
based on desired misclassifications. The loss function L is expressed with negative sign because
the adversary aims to minimize loss on the desired labels. Therefore, Eq.3 can equivalently be
expressed as:

D∗

𝑝 ∈ argmin
D𝑝 ∈Φ( D𝑝 )

L (D ′

𝑡𝑎𝑟𝑔𝑒𝑡, w(D𝑝 ))

(4)

4.5.2 Oracle Attacks. These are exploratory attacks where an adversary uses samples to collect
and infer information about a model or its training data [119]. Many classifiers are considered the
intellectual property of the organizations they belong to; for instance in Machine Learning as a

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:14

Oseni and Moustafa, et al.

Service (MLaaS) platforms such as Microsoft Machine Learning, Amazon Machine Learning and
Google Prediction API. In these environments, simple APIs are made available for the customer to
upload data and for training and querying models [112]. An adversary can perform an oracle attack
against such models by using an API to feed inputs into the learning model in order to observe the
model’s outputs. An adversary with no knowledge of a classifier can use input-output pairings
obtained from oracle attacks to train a surrogate model that operates much like the target model
[111]. The steps an adversary uses to carry out an oracle attack are described in Fig. 4. Without
prior knowledge of the original classifier and class information of each sample, the adversary can
only feed some test data into the classifier and collect the resulting labels which are then used to
train a surrogate classifier, that is, a functionally equivalent classifier. The surrogate classifier can
then be used to generate adversarial examples for use in evasion attacks against the target model.
In oracle attacks, sometimes referred to as exploratory attacks, an adversary uses samples to collect
and infer information about a model or its training data. Oracle attack types include membership
inference attacks, inversion attacks and extraction attacks [119].

Many machine learning models are considered the intellectual property of the organizations
that created them. For instance, Microsoft Machine Learning, Amazon Machine Learning and
Google Prediction API are all Machine Learning as a Service (MLaaS) platforms and are owned
and operated by Microsoft, Amazon and Google respectively. In these environments, simple APIs
are made available for customers to upload data, and for training and querying models [112]. An
adversary can perform an oracle attack against such models by using an API to feed inputs into
the learning model in order to observe the model’s outputs. An adversary with no knowledge of a
target model can use input-output pairings obtained from oracle attacks to train a surrogate model
which is functionally equivalent to the target model [111]. The surrogate model can then be used
to generate adversarial examples for use in evasion attacks against the target model. Oracle attacks
include membership inference attacks, inversion attacks and extraction attacks.

Fig. 4. Steps of an Oracle Attack

Membership Inference Attacks - In membership inference attacks, the adversary seeks to determine
if a given data point belongs to the training dataset analyzed to learn the model’s parameters [99].
For instance, an adversary can violate the privacy of a clinical patient by using membership
inference attack to know if the patient’s health record was part of the dataset used in training a
model that diagnoses a particular disease. Shokri et al. [112] demonstrated how the membership
inference attacks can be carried out against black box models by exploiting the difference in the
target model’s behaviour on data ’seen’ during training versus data seen for the first time.

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:15

Inversion Attacks - Model inversion attacks, referred to as training data extraction attacks in
some literature, enables an adversary to reconstruct training inputs from a model’s predictions
[43]. The extracted inputs are not specific points but rather an average representation of data that
belong to a class. Fredrikson et al. [44] introduced inversion attacks by considering a case where an
adversary is able to infer a patient’s genetic marker from the predictions of a linear model designed
to predict a stable dosage of the drug Warfarin. This approach clearly illustrates privacy issues
relating to providing API access to ML models trained on sensitive data [99].

Extraction Attacks - Model extraction attacks involves an adversary who uses observed input-
output pairs, (𝑥𝑖, 𝑦𝑖 ), from the predictions of some target model, 𝑓 , to extract a set of parameters
and attempts to learn a model, ˆ𝑓 , that closely approximates 𝑓 . Tramèr et al. [125] demonstrated
extraction attacks against two ML-as-a-Service models via their exposed APIs. While extraction
attacks could subvert model monetization, it also violates training data privacy and facilitates
evasion attacks.

4.5.3 Evasion Attacks. Evasion attacks are the most common type of attacks on AI systems [27].
An adversary directly manipulates the input samples at test time to avoid detection [16]. While the
evasion attack scenarios explored in many literature have mainly focused on machine learning
classification tasks, deep neural networks have been found to be highly vulnerable to this kind of
threat [12, 89, 96, 118]. However, deep networks also have the advantage of being able to represent
functions that can resist adversarial perturbations unlike other shallow linear models [45]. Evasion
attacks do not alter the behaviour of a system but instead exploit vulnerabilities in the system using
adversarial examples to produce the desired errors. The approach used in evasion attacks is based on
the rationale that without prior knowledge of a classifier’s decision function, an adversary can learn
a surrogate classifier using a surrogate data to reliably evade the targeted classifier. Adversarial
samples are often perceptually indistinguishable from the ’clean’ samples but yet pose serious
security threats for machine learning applications [67]. Some examples of evasion attacks on AI
system are presented in Table 3.

Melis et al. [82] described two possible evasion attack settings based on multi-class classifiers
namely: Error-generic evasion attacks and Error-specific evasion attacks. Many evasion attack
scenarios can be derived from these attack categories.

Error-generic evasion attacks - These refers to attacks where the adversary is simply interested in
misleading classification at test time irrespective of the output class predicted by the classifier. In
multi-class classifiers, the predicted class 𝑐∗ is a class for which the discriminant function, 𝑓𝑘 (𝑥),
for a given input sample, 𝑥, is maximum:

The error-generic evasion attack problem can be formulated mathematically as:

𝑐∗ = argmax
𝑘=1,...,𝑐

𝑓𝑘 (𝑥)

𝑥 ∗
𝑒 ∈ argmin
𝑥𝑒 ∈Φ(𝑥𝑒 )

𝑓𝑘 (𝑥𝑒 ) − max
𝑗≠𝑘

𝑓𝑗 (𝑥𝑒 ), s.t. 𝑑 (𝑥, 𝑥𝑒 ) ⩽ 𝑑𝑚𝑎𝑥

(5)

(6)

where 𝑓𝑘 (𝑥𝑒 ) is the discriminant function associated with the true class 𝑘 of the source sample
𝑥 and max𝑗≠𝑘 𝑓𝑗 (𝑥𝑒 ) is the discriminant function of the closest competing incorrect class. The
optimization problem constraint 𝑑 (𝑥, 𝑥𝑒 ) ⩽ 𝑑𝑚𝑎𝑥 limits the perturbation, 𝑑𝑚𝑎𝑥 between the original
sample, 𝑥, and the adversarial sample, 𝑥𝑒 , point given in terms of distance in the input space [89].
Error-specific evasion attacks - In this evasion attack setting, the adversary is interested in
misleading classification with the aim of producing a specific type of error. The problem of error-
specific evasion attacks is formulated similarly as the error-generic evasion attack with the exception

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:16

Oseni and Moustafa, et al.

that the objective function is maximized and 𝑓𝑘𝑒
the class 𝑘𝑒 which the adversarial example should be assigned [82]:

denotes the discriminant function associated with

𝑥 ∗
𝑒 ∈ argmin
𝑥𝑒 ∈Φ(𝑥𝑒 )

(cid:16)

max
𝑗≠𝑘𝑒

(cid:17)

𝑓𝑗 (𝑥𝑒 )

− 𝑓𝑘𝑒 (𝑥𝑒 ), s.t. 𝑑 (𝑥, 𝑥𝑒 ) ⩽ 𝑑𝑚𝑎𝑥

(7)

4.6 Adversarial Examples: Crafting Methods
This is an important research direction as it provides guidance on vulnerabilities in AI systems
which could potentially be exploited by an adversary. In this section, we review some of the methods
for generating adversarial examples in literature.

4.6.1 Box-Constrained L-BFGS. Szegedy et al. [118] made a groundbreaking discovery on the
stability of neural network networks to small imperceptible input perturbations and showed how
these perturbations could arbitrarily change the network’s prediction. They formalized the problem
of finding the optimal perturbation in terms of the L2 norm. To find the optimal perturbation 𝜂 for a
given clean input image X ∈ R𝑚 and target label 𝑙 ∈ {1, ..., 𝑘 }, the aim is to solve a box-constrained
optimization problem:

min
𝜂

∥𝜂 ∥2

𝑠.𝑡 . 𝐹 (X + 𝜂) = 𝑙; X + 𝜂 ∈ [0, 1]𝑚

(8)

where 𝐹 : R𝑚 → {1...𝑘 } is the deep neural network classifier which is assumed to have an
associated continuous loss function L : R𝑚 × {1...𝑘 } → R+. The problem of Eq.8 is difficult to
solve, so Szegedy et al. proposed an approximation using the box-constrained L-BFGS:

𝑐 ∥𝜂 ∥ + L (X + 𝜂, 𝑙) 𝑠.𝑡 . X + 𝜂 ∈ [0, 1]𝑚

min
𝜂

(9)

An approximate solution to Eq. 9 is obtained by performing a line-search to find the minimum
constant 𝑐 > 0 for which the minimizer 𝜂 satisfies 𝐹 (X + 𝜂) = 𝑙. According to Moosavi-Dezfooli
et al. [85], this method of generating adversarial perturbations is less efficient and does not scale to
large datasets.

Fast Gradient Sign Method (FGSM). Goodfellow et al. [45] introduced FGSM, an efficient
4.6.2
method for generating adversarial examples in the computer vision context. Their work established
that the linear behaviour of deep neural network is the main reason they are vulnerable to adversarial
examples. The Fast Gradient Sign Method is based on linearizing the cost function of a deep neural
network and solving for the perturbation 𝜂 that maximizes the 𝐿∞ norm. In closed form:

𝜂 = 𝜖𝑠𝑖𝑔𝑛(▽X𝐽 (X, 𝑦))

(10)

˜X = X + 𝜂
where 𝜖 is a hyper-parameter to be selected by the adversary, X represents the input to the model, 𝑦
represents the targets associated with X and ▽X𝐽 (X, 𝑦) is the gradient of the loss function w.r.t the
input X, and can be computed via back-propagation. According to Kurakin et al. [67], this method
is simple, fast and computationally efficient as it does not require iterative procedure to compute
the adversarial examples. Nevertheless, the FGSM method only provides a coarse approximation of
the optimal perturbation vectors [85].

(11)

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:17

4.6.3 Basic Iterative Method (BIM). The Basic Iterative Method (BIM) is simply an extension of the
FGSM and was introduced by Kurakin et al. [66]. It is an iterative method for crafting adversarial
examples by applying the FGSM technique multiple times with small step size:

˜X0 = X,

˜X𝑁 +1 = Clip

X,𝜖

(cid:26)

˜X𝑁 + 𝛼𝑠𝑖𝑔𝑛(▽X𝐽 ( ˜X𝑁 , 𝑦))

(cid:27)

(12)

Jacobian-based Saliency Map Attack (JSMA). The JSMA was introduced by Papernot et al. [98]
4.6.4
as a class of algorithms which an adversary can use to reliably generate adversarial examples based
on a precise understanding of the input features of a deep neural network that most significantly
impacts the output. This approach is distinct from previous approaches for generating adversarial
examples because rather than using the gradient of the DNN’s cost function to compute the
perturbation vector, it instead computes the forward derivative of the neural network directly and
then uses the concept of adversarial saliency maps to highlight regions of the input domain that
leads to significant changes in the network outputs. The forward derivative 𝐽F (X) for a given
sample X, also referred to as the Jacobian of the multidimensional function F learned by the DNN
during training is expressed as:

𝐽F (X) =

𝜕F(X)
𝜕X

=

(cid:21)

(cid:20) 𝜕F𝑗 (X)
𝜕𝑥𝑖

𝑖 ∈1...𝑀,𝑗 ∈1...𝑁

(13)

Based on the experimental evaluations carried out in their work on LeNet, a well-studied deep
neural network used for handwritten digit recognition, Papernot et al. [98] demonstrated that the
JSMA algorithms can reliably produce adversarial examples which can cause misclassifications in
specific target DNN models.

Iterative Least-Likely Class Method (ILLC). The iterative least-likely class method (ILLC),
4.6.5
introduced by Kurakin et al. [66], is another technique that has been demonstrated in literature for
generating adversarial examples for targeted attacks against deep neural networks in the computer
vision context. In contrast to FGSM and BIM, this method enables an adversary to create adversarial
examples that will be classified by a DNN as a specific desired target class. The least-likely class
according to the prediction of a trained DNN on image X is usually very different from the true
class and can be expressed as shown in Eq. 14.

According to Kurakin et al. [67], the procedure for generating an adversarial image which is

𝑦𝐿𝐿 = argmin

𝑦

{𝑝 (𝑦|X)}

(14)

classified as 𝑦𝐿𝐿 is:

˜X0 = X,

˜X𝑁 +1 = 𝐶𝑙𝑖𝑝X,𝜖

(cid:26)

˜X𝑁 − 𝛼𝑠𝑖𝑔𝑛(▽X𝐽 ( ˜X𝑁 , 𝑦𝐿𝐿))

(cid:27)

(15)

4.6.6 Universal Perturbations for Steering to Exact Targets (UPSET). Another method for generating
adversarial images that looks similar to the input but fools classifiers is the method known as
universal perturbations for steering to exact targets (UPSET) proposed by Sarkar et al. [107]. Given
an 𝑛 class setting, UPSET seeks to produce 𝑛 universal perturbations 𝜂 𝑗 , 𝑗 ∈ {1, 2, ..., 𝑛} such that
when the perturbation is added to any image that does not belong to target class 𝑗, the perturbed
image is classified as being from target class 𝑗. UPSET uses a residual generating network 𝑅 which
uses target class 𝑡 and outputs a perturbation 𝜂𝑡 = 𝑅(𝑡) which is of similar dimension to input
image X. The adversarial image ˜X is expressed as:

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:18

Oseni and Moustafa, et al.

˜X = max(min(𝑠 × 𝑅(𝑡) + X, 1), −1),

(16)

where 𝑠 is a scaling factor and the pixel values in X are normalized to be within [-1, 1].

4.6.7 Antagonistic Network for Generating Rogue Images (ANGRI). The Antagonistic Network for
Generating Rogue Images (ANGRI) is another method for generating adversarial images proposed
by Sarkar et al. [107]. As opposed to UPSET which produces image-agnostic perturbations, ANGRI
instead produces image-specific perturbations as its output depends on the input image. Given an
input image, X, belonging to class 𝐶X and a target class, 𝑡 ≠ 𝐶X, ANGRI generates an adversarial
image, ˜X, which the classifier classifies as an image from target class 𝑡.

4.6.8 DeepFool. DeepFool, introduced by Moosavi-Dezfooli et al. [85] is an iterative method
for computing adversarial examples that fools deep neural networks. In comparison to both the
box-constrained L-BFGS and FGSM methods discussed earlier, DeepFool is more computationally
efficient and generates adversarial perturbations that are more imperceptible. The DeepFool method
is based on the assumption that neural networks are linear with each class separated by hyperplanes
[25]. Therefore the minimum perturbation of a linearized binary classifier is computed as:

argmin
𝜂𝑖

∥𝜂𝑖 ∥2 𝑠.𝑡 . 𝑓 (𝑥𝑖 ) + ▽𝑓 (𝑥𝑖 )𝑇 𝜂𝑖 = 0.

(17)

: R𝑛 → R is an arbitrary
where 𝜂𝑖 is the perturbation at iteration 𝑖, 𝑥 is the input image and 𝑓
image classification function. Moosavi-Dezfooli et al. also extended DeepFool multi-class classi-
fiers. Experimental results presented in their work from testing the DeepFool algorithm on CNN
architectures using the MNIST, CIFAR-10 and ImageNet classification datasets show that method
accurately estimates the minimum perturbation required to change classification labels of deep
neural networks.

4.6.9 Carlini & Wagner. Carlini and Wagner [25] introduced one of the most common and effective
algorithms for crafting adversarial examples to date. Their approach is also based on the formulation
of adversarial examples introduced by Szegedy et al. [118]. Given the problem of finding an
adversarial example for input image X:

minimize D (X, X + 𝛿)
such that C(X + 𝛿) = 𝑡,
X + 𝛿 ∈ [0, 1]𝑛

(18)

The goal therefore is to find a small perturbation 𝛿 that minimizes some distance metric D (X, X+
𝛿), usually specified in terms of 𝐿𝑝 norms, in order to cause a misclassification. Constraint C(X+𝛿) =
𝑡 ensures that the input image is mis-classified and is highly non-linear. This constraint makes the
optimization problem of Eq.18 difficult to solve and while Szegedy et al. [118] solved this problem
using the L-BFGS technique, Carlini and Wagner solved the problem by re-formulating it and
expressing the constraint as an objective function 𝑓 such that when C(X + 𝛿) = 𝑡 is satisfied,
𝑓 (X + 𝛿) ≤ 0 is also satisfied. After instantiating distance metric D with an 𝐿𝑝 norm, the Carlini
and Wagner formulated optimization problem becomes:

minimize ∥𝛿 ∥𝑝 + 𝑐 · 𝑓 (X + 𝛿)
such that X + 𝛿 ∈ [0, 1]𝑛

(19)

Constraint X + 𝛿 ∈ [0, 1]𝑛, expressed as a box-constraint ensures that the adversarial example is
a valid image. To find adversarial examples that will have a low 𝐿2 metric distortion, Carlini and

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:19

Wagner used a change of variable method to optimize a new variable 𝑤 rather than variable 𝛿,
setting:

𝛿𝑖 =

1
2

(tanh(𝑤𝑖 ) + 1) − x𝑖

(20)

4.6.10 Houdini. Houdini, introduced by Cisse et al. [33], is an method for reliably generating
adversarial examples which are directly tailored for a given combinatory or non-differentiable
task loss of interest. Different from many existing methods which are mainly applicable to image
classification tasks, Houdini is a more flexible approach which has been successfully applied to
a range of applications such as voice recognition, human pose estimation [129] and semantic
segmentation. According to Cisse et al., the problem of finding an adversarial example which
can fool a neural network model 𝑓𝜃 with respect to task loss 𝑙 (.) for a chosen p-norm and noise
parameter 𝜖 involves solving:

˜𝑥 = argmax
˜𝑥: ∥ ˜𝑥−𝑥 ∥𝑝 ≤𝜖

𝑙 (𝑦𝜃 ( ˜𝑥), 𝑦)

(21)

In most cases, the task loss 𝑙 (.) is a combinatory quantity that is difficult to optimize. As such, it
is replaced with a differentiable surrogate loss ¯𝑙 (𝑦𝜃 ( ˜𝑥), 𝑦). For a given example (𝑥, 𝑦), Cisse et al.
proposed a surrogate named Houdini and defined as:

¯𝑙𝐻 (𝜃, 𝑥, 𝑦) = P𝛾 ∼N (0,1)

(cid:20)

𝑓𝜃 (𝑥, 𝑦) − 𝑓𝜃 (𝑥, ˆ𝑦) < 𝛾

(cid:21)

· 𝑙 (ˆ𝑦, 𝑦)

(22)

The first term of Eq. 22 is the stochastic margin and reflects the confidence of the neural network
model in its prediction while the second term is the task loss.

4.6.11 Universal Adversarial Perturbations (UAP). Universal adversarial perturbations, introduced
by Moosavi-Dezfooli et al. [84], is a systematic algorithm for generating quasi-imperceptible
universal perturbations for deep neural networks. In their work, they demonstrated how the UAP
algorithm can be used to generate a single perturbation vector that can be used to fool neural
networks on most natural images. Different from the notion of adversarial perturbations presented
in [118], Moosavi-Dezfooli et al. examined universal perturbations that are common to most data
points belonging to a data distribution. Given a distribution of images 𝜇 ∈ R𝑑 and classification
function ˆ𝑘 that outputs estimated label ˆ𝑘 (𝑥) for each image input 𝑥 ∈ R𝑑 , the focus is to obtain
universal perturbation vectors 𝜐 ∈ R𝑑 that can fool ˆ𝑘 on most data points sampled from 𝜇 such that:

The UAP algorithm seeks a universal perturbation 𝜐 that satisfies the constraints:

ˆ𝑘 (𝑥 + 𝜐) ≠ ˆ𝑘 (𝑥) 𝑓 𝑜𝑟 ”𝑚𝑜𝑠𝑡” 𝑥 ∼ 𝜇

∥𝜐 ∥𝑝 ≤ 𝜉,

(cid:16)ˆ𝑘 (𝑥 + 𝜐) ≠ ˆ𝑘 (𝑥)

(cid:17)

≥ 1 − 𝛿

P
𝑥∼𝜇

(23)

(24)

(25)

where parameter 𝜉 controls the magnitude of 𝜐 and 𝛿 quantifies the target fooling rate for all
samples images from distribution 𝜇.

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:20

Oseni and Moustafa, et al.

4.6.12 ATNs. An Adversarial Transformation Networks (ATN) is a feed-forward neural network
introduced by Baluja and Fischer [8] for generating adversarial examples. The ATN is formally
expressed as a neural network:

𝑔𝑓 ,𝜃 (x) : x ∈ X → x′
where 𝜃 is the parameter vector of 𝑔, 𝑓 is the target network or set of networks, and x′ ∼ x but
argmax 𝑓 (x) ≠ argmax 𝑓 (x′). Obtaining 𝑔𝑓 ,𝜃 requires solving the optimization problem:

(26)

argmin
𝜃

∑︁

x𝑖 ∈X

𝛽𝐿X

(cid:0)𝑔𝑓 ,𝜃 (x𝑖 ), x𝑖 (cid:1) + 𝐿Y

(cid:0)𝑓 (𝑔𝑓 ,𝜃 (x𝑖 )), 𝑓 (x𝑖 )(cid:1)

(27)

where 𝐿X is the loss function in the input space, 𝐿Y is the loss function on the output space
of 𝑓 and 𝛽 is the weight that balances both loss functions. Baluja and Fischer presented two
different approaches for generating adversarial examples using an ATN: (1) training the ATN to only
generate the perturbation to x, or (2) training the ATN to generate an adversarial autoencoding of x.
Experimental results show that ANTs can generate a wide range of adversarial examples targeting
a single network. The authors argued that the unpredictable diversity of ANTs makes it a viable
attack that can bypass many defenses, including those trained using adversarial examples from
existing methods.

4.6.13 Projected Gradient Descent (PGD). The PGD attack, proposed by Madry et al. [80], is a
white-box 𝑙∞ attack that generates adversarial examples by using the local first order information
about the neural network. In contrast to other 𝑙∞-bounded attacks such as FGSM [45], the PGD is a
more powerful multi-step attack as it doesn’t limit the amount of time and effort an adversary can
invest into finding the best attack. The PGD attack algorithm is based on the same intuition as the
FGSM which involves solving the saddle-point problem:

min
𝜃

E(𝑥,𝑦)∼D

(cid:20)

max
𝛿 ∈𝑆

(cid:21)

𝐿(𝜃, 𝑥 + 𝛿, 𝑦)

(28)

where D is the data distribution over pairs of examples 𝑥 ∈ R𝑑 and corresponding labels 𝑦 ∈ [𝑘],
and 𝜃 is the set of model parameters.

4.6.14 One Pixel. Su et al. [115] proposed a method for generating one-pixel adversarial pertur-
bations, based on differential evolution, which can be used for low dimension black-box attacks
against deep neural networks. In formalizing the one pixel method, Su et al. represented an input
image by an 𝑛-dimensional vector x = (𝑥1, ..., 𝑥𝑛), with each scalar elements also corresponding
to one pixel. Given a target image classifier 𝑓 with original input image x correctly classified as
class 𝑡, the probability of x being classified as belonging to 𝑡 is 𝑓𝑡 (x). The vector 𝑟 (x) = (𝑟1, ..., 𝑟𝑛)
is an additive adversarial perturbation with respect to x. The goal therefore is to find the optimized
solution 𝑟 (x)∗:

maximize
𝑟 (x) ∗

𝑓𝑎𝑑𝑣 (x + 𝑟 (x)) such that ∥𝑟 (x) ∥0 ≤ 𝑑

(29)

where 𝑑 is a small number and in the case of one-pixel attack, 𝑑 = 1.

While one pixel has only so far been demonstrated in deep neural networks for image classifica-
tion tasks, the authors argue that it can potentially be extended to other deep learning applications
such as natural language processing and speech recognition in future works.

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:21

4.6.15 OPTMARGIN. He et al. [51] introduced the OPTMARGIN attack which can be used to
generate low-distortion adversarial examples that are robust to small perturbations. He et al.
demonstrated in their work that adversarial examples generated using the OPTMARGIN attack
can successfully evade the region-based classification defense method introduced in [23], as a
robust defense against low-distortion adversarial examples. The OPTMARGIN attack involves
creating a surrogate model 𝑓𝑖 (𝑥), which is functionally equivalent to the region-based classifier
[23], and uses the surrogate model to classify a small number 𝑛 of perturbed data points, such that
𝑓𝑖 (𝑥) = 𝑓 (𝑥 + 𝑣𝑖 ); where 𝑖 = 1, ..., 𝑛, 𝑓 is the point classifier used in the region-based classifier and
𝑣𝑖 are perturbations applied to input 𝑥. If 𝑍 (𝑥) is the |𝐶 |-dimensional vector of class weights that 𝑓
internally uses to classify 𝑥, then the loss function for the surrogate model with data point 𝑖 can be
expressed as:

𝑙𝑖 (𝑥 ′) = 𝑙 (𝑥 ′ + 𝑣𝑖 ) = max

(cid:18)

− 𝜅, 𝑍 (𝑥 ′ + 𝑣𝑖 )𝑦 − max (cid:8)𝑍 (𝑥 ′ + 𝑣𝑖 ) 𝑗 : 𝑗 ≠ 𝑦(cid:9)

(cid:19)

(30)

where 𝜅 is the confidence margin. In the OPTMARGIN attack, 𝜅 is set to 0, which implies that the
model simply misclassifies its input. As an extension to the Carlini & Wagner attacks [25], the
OPTMARGIN’s objective function uses the sum of the loss function for each perturbed data point,
resulting in the minimization problem:

minimize ∥𝑥 ′ − 𝑥 ∥2

2 + 𝑐 · (𝑙1(𝑥 ′) + ... + 𝑙𝑛 (𝑥 ′))

(31)

4.6.16 EAD. The Elastic-Net Attacks to DNNs (EAD) was introduced by Chen et al. [31] as a novel
approach for crafting adversarial examples. According to Chen et al., EAD generalizes the Carlini &
Wagner 𝐿2 attack [25] and is able to craft more effective adversarial examples based on 𝐿1 distortion
metrics. Given a benign input image x0 and correct label 𝑡0, the loss function 𝑓 for crafting EAD
adversarial examples for with respect to a benign labelled input image is expressed as:

minimize
x

𝑐 · 𝑓 (x, 𝑡) + 𝛽 ∥x − x0 ∥1 + ∥x − x0∥2
2

subject to x ∈ [0, 1]𝑝,

(32)

where x is defined as the adversarial example of x0 with target class 𝑡 ≠ 𝑡0, 𝑐, 𝛽 ≥ 0 are regularization
parameters of loss function 𝑓 and and the 𝐿1 penalty respectively. The loss function 𝑓 (x, 𝑡) for
targeted attacks is defined as:

𝑓 (x, 𝑡) = max

(cid:26)

max
𝑗≠𝑡

[Logit(x)] 𝑗 − [Logit(x)]𝑡 , −𝜅

(cid:27)

,

(33)

where Logit(x) = [[Logit(x)]1, ..., [Logit(x)]𝐾 ] ∈ R𝐾 is the logit layer representation of adver-
sarial example x in the DNN, 𝐾 is the number of classes and 𝜅 ≥ is a confidence parameter.

In evaluating the performance of EAD, Chen et al. performed extensive experimental analysis
using MNIST, CIFAR-10 and ImageNet datasets and found that adversarial examples generated
by EAD were able to successfully evade deep neural networks trained using defensive distillation
[100] in a similar manner as the Carlini & Wagner attacks [25].

4.6.17 Robust Physical Perturbations (RP2). Eykholt et al. [40] introduced the RP2 method for
generating adversarial examples under different physical-world conditions. This method generates
adversarial perturbations which can cause targeted misclassification in deep neural networks where
the adversary has full knowledge of the model. The RP2 algorithm was derived in two stages: (1)
using the optimization method to generate adversarial perturbation 𝛿 for single-image 𝑥 without

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:22

Oseni and Moustafa, et al.

considering any physical conditions, (2) update the algorithm by considering different physical con-
ditions. The single-image optimization problem without consideration for environmental conditions
is given as:

min 𝐻 (𝑥 + 𝛿, 𝑥), s.t. 𝑓𝜃 (𝑥 + 𝛿) = 𝑦∗
(34)
where 𝑓𝜃 (·) is the target classifier, 𝐻 is a chosen distance function, and 𝑦∗ is the target class. Solving
Eq. 34 required Eykholt et al. to reformulate it in Lagrangian-relaxed form as:

argmin
𝛿

𝜆∥𝛿 ∥𝑝 + 𝐽

(cid:18)

(cid:19)

𝑓𝜃 (𝑥 + 𝛿), 𝑦∗

(35)

where 𝐽 (·, ·) is the loss function and 𝜆 is a hyper-parameter that controls regularization of the
distortion.

After accounting for environmental conditions, the spatially-constrained perturbation optimiza-

tion problem is defined as:

argmin
𝛿

𝜆∥𝑀𝑥 · 𝛿 ∥𝑝 + NPS + E𝑥𝑖 ∼𝑋 𝑉 𝐽

(cid:18)

𝑓𝜃 (𝑥𝑖 + 𝑇𝑖 (𝑀𝑥 · 𝛿)), 𝑦∗

(cid:19)

(36)

where 𝑇𝑖 (·) is the alignment function that maps object transformation to the perturbation transfor-
mations.

Show-and-Fool. This is a novel optimization method for crafting adversarial examples in
4.6.18
neural image captioning proposed by Chen et al. [29]. The Show-and-Fool algorithm provides two
different approaches for crafting adversarial examples namely (1) targeted caption method and
(2) targeted keyword method. The process for crafting the adversarial examples is formulated as
optimization problems with objective functions that adopts the hybrid CNN-RNN architecture. For
a given input image x, an adversarial example can be obtained by solving the optimization problem:

𝑐 · loss(x + 𝛿) + ∥𝛿 ∥2
2

s.t. x + 𝛿 ∈ [−1, 1]𝑛

min
𝛿

(37)

where 𝛿 is the adversarial perturbation to x, ∥𝛿 ∥2
is an ℓ2 distance metric between
the benign image and the adversarial image, loss(·) is an attack loss function and term 𝑐 > 0 is a
regularization constant. For a targeted caption denoted by 𝑆 = (𝑆1, 𝑆2, ..., 𝑆𝑡, ..., 𝑆𝑁 ), where 𝑆𝑡 is the
index of the 𝑡-th word in the vocabulary V and 𝑁 is the length of caption 𝑆, the loss function is
given as:

2 = ∥(x + 𝛿) − x∥2

2

loss𝑆,logits (x + 𝛿) =

(cid:26)

max

𝑁 −1
∑︁

𝑡 =2

− 𝜖, max
𝑘≠𝑆𝑡

{𝑧 (𝑘)
𝑡

} − 𝑧 (𝑆𝑡 )
𝑡

(cid:27)

,

(38)

where 𝜖 > 0 is the confidence level which accounts for the difference between max𝑘≠𝑆𝑡 {𝑧 (𝑘)
}
and 𝑧 (𝑆𝑡 )
] ∈ R |V | is a vector of 𝑙𝑜𝑔𝑖𝑡𝑠 for each possible word in the
𝑡
vocabulary. Based on applying the loss function of Eq. 38 to Eq. 37, the targeted caption method for
a given targeted caption 𝑆 is formulated as:

, and 𝑧𝑡 := [𝑧 (1)

, ..., 𝑧 ( |V |)
𝑡

, 𝑧 (2)
𝑡

𝑡

𝑡

𝑐 ·

min
𝑤 ∈R𝑛

𝑁 −1
∑︁

(cid:26)

max

𝑡 =2

−𝜖, max
𝑘≠𝑆𝑡

{𝑧 (𝑘)
𝑡

} − 𝑧 (𝑆𝑡 )
𝑡

(cid:27)

+ ∥tanh(𝑤 + 𝑦) − tanh(𝑦)∥2
2

.

(39)

Similarly for targeted keyword denoted by K := {𝐾1, ..., 𝐾𝑀 } ⊂ V, the loss function is given as:

𝑀
∑︁

𝑗=1

(cid:26)
𝑔𝑡,𝑗 (max{−𝜖, max
𝑘≠𝐾𝑗

min
𝑡 ∈ [𝑁 ]

{𝑧 (𝑘)
𝑡

} − 𝑧 (𝐾𝑡 )
𝑡

})

(cid:27)

(40)

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:23

In evaluating the effectiveness of the Show-and-Fool method, Chen et al. [29] carried out extensive
experimental analysis using the Microsoft COCO dataset and the inception-v3 model. They showed
that adversarial examples generated using the targeted caption and keyword methods are highly
effective against neural image captioning systems and are highly transferable to other models, even
those with different architectures.

5 DEFENSES AGAINST AI SYSTEM ATTACKS
In this section, we categorize defenses against AI system attacks based on whether they provide
complete defense against adversarial examples or whether they simply detect and reject the
adversarial examples. We then use this categorization, shown in Fig. 5, for our review of existing
defense techniques. Complete defenses can be characterized by whether they apply to attacks
launched against the training or testing phases of the system operation [119].

Fig. 5. Taxonomy of Defenses against AI system attacks

5.1 Defenses against Training Phase Attacks
Poisoning attacks, like many other attacks on AI systems make learning inherently more difficult.
Many training attack defense mechanisms are based on the assumption that poisoned samples are
usually out of the expected input distribution [99]. A number of techniques have been proposed as
defenses against these training-time attacks, and some of these techniques are discussed as follows:

5.1.1 Data Sanitization. Data Sanitization is a poisoning attack defense strategy that involves
filtering out contaminated samples from a training dataset before using it to train a model [28].
Cretu et al. [34] introduced a novel data sanitization technique for defending poisoning attack
on out-of-the-box anomaly detection classifiers. Nelson et al. also presented a data sanitization
technique [91, 92] for defending against poisoning attacks known as Reject On Negative Impact
(RONI). This technique was used to successfully filter out dictionary attack messages on the
SpamBayes spam filter, and it accurately identified all of the attack emails without flagging any of

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:24

Oseni and Moustafa, et al.

the non-spam emails. While data sanitization techniques have been effective in defending against
some data poisoning attacks developed without explicitly considering defenses, Koh et al. [63]
presented three new data poisoning attacks that can simultaneously evade a broad range of data
sanitization defenses. The success of their work against anomaly-based data sanitization defenses
simply suggests that more research effort is required in finding effective defenses against poisoning
attacks.

5.1.2 Robust Statistics. As opposed to data sanitization, this defense approach does not attempt
to detect poisoned samples, but instead focuses on design of robust models against poisoning
attacks. In their work, Biggio et al. [15] investigated the use of multiple classifier systems (MCSs)
for improving the robustness of pattern recognition models in adversarial settings based on the
principle that more than one classifier has to be evaded in order to make the system ineffective.
The results they obtained from the experimental investigations suggested that randomisation-
based MCS construction techniques can be used to improve the robustness of linear classifiers
in adversarial settings. Also, Biggio et al. [11] experimentally demonstrated that ’bagging’, an
acronym for bootstrap aggregating, is an effective defense technique against poisoning attacks
irrespective of the base classification algorithm. Bagging was originally proposed by Breiman [21]
as a technique for improving classifier accuracy by generating multiple versions of a classifier and
using these to get an aggregate classifier, and it has proved to be effective especially when applied
to classifiers whose predictions vary significantly with little variation in training data.

5.2 Defenses against Testing (Inference) Phase Attacks
Defenses against testing-phase attacks include various model robustness improvements, differential
privacy and homomorphic encryption.

5.2.1 Robustness Improvements. A machine learning model may achieve robustness by being
able to detect and reject adversarial examples. While these robustness improvement techniques
are defenses against testing-phase attacks, they are deployed during the training phase that pre-
cedes testing. Some of the robustness improvement techniques that have been well-researched
includes Adversarial Training, Defensive Distillation, Ensemble Method, Gradient Masking, Feature
Squeezing and Reformers/Autoencoders [119].

Adversarial Training. Adversarial training seeks to improve the robustness of a machine learning
model by proactively generating adversarial examples and augmenting them with training data as
part of the training process. [123]. These adversarial examples are perturbed examples found by
optimizing the input to maximize the model’s prediction error [118]. While adversarial training
may be time consuming due to the number of iterative computations required to form a robust
model, it has proved to be resistant to white-box attacks [109] if the perturbations computed during
training closely maximizes the model’s loss [80]. On the other hand, deep neural network models
that are adversarially trained using fast single-step methods are found to remain vulnerable to
simple black-box attacks [123].

Defensive Distillation. Distillation is a technique designed for transferring knowledge from an
ensemble of models or large highly regularized models to smaller distilled models while preserving
the prediction accuracy [99]. The technique was formally introduced by Hinton et al. [54], after
being initially suggested by Ba and Caruana [6] as a method for smoothing a model’s prediction
accuracy by training shallow neural networks to mimic deeper neural networks for the benefit
of deploying deep learning in computationally constrained devices. Papernot et al. [100] also
presented defensive distillation as a technique for training models that are more robust to input
perturbations. Their experiments demonstrated that distilled DNN-based models are more resilient

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:25

to adversarial examples. The applicability of this defense against other testing-phase attacks were
also demonstrated in [45, 98]. Despite the benefits of defensive distillation and it’s strong guarantees
against adversarial examples, Carlini and Wagner [25] experimentally demonstrated how their
attacks using high-confidence adversarial examples can break defensive distillation

Gradient Masking. Gradient masking is a technique that seeks to reduce the sensitivity of a DNN
model to small input perturbations [119]. In this defense strategy, the first order derivatives of
a model is computed with respect to its input and these derivatives are then minimized during
the learning phase [99], resulting in a technique that conceals a model’s gradient information to
an adversary seeking to exploit the gradient-based information. While it is sensible to reduce a
model’s sensitivity to small changes in the input as a way of defending against adversarial examples,
experiments performed by Papernot et al. [97] revealed the flaws in the gradient masking technique
by demonstrating how it can be evaded by a black-box attack.

Feature Squeezing. This is a state-of-the-art technique for detecting adversarial examples by
reducing the feature input spaces available to an adversary. It achieves this by combining many
samples that correspond to different feature vectors in the original input space into a single sample.
Xu et al. [131] demonstrated two types of feature squeezing methods in the image space: (1) image
color depth reduction and (2) smoothening to reduce variation among pixels (median smoothening), as
accurate and robust detection techniques against adversarial inputs. They extended this work in
[132] where they showed that the median smoothening is the most effective squeezer in mitigating
the Carlini & Wagner attack. While feature squeezing corrupts input features that adversaries rely
on for their attacks, experimental evaluations [120] show that it could potentially cause degradation
of accuracy on classifying benign inputs.

Ensemble Method. Ensemble methods are learning algorithms used to improve classification
decisions of supervised learning models [38]. While many ensemble methods have been introduced
in literature over the years, they have only recently been considered as a method for improving
defenses of machine learning models against adversarial attacks. As a method of increasing the
robustness of convolutional neural networks to adversarial attacks, Abbasi and Gagné [1] proposed
the use of an ensemble of diverse specialist classifiers to detect and reject adversarial examples while
accepting benign samples based on confidence. The authors showed that this method can be used
to reduce the prediction confidence for adversarial examples while preserving the confidence of
clean samples to a certain extent. However, He et al. [52] demonstrated that ensembles constructed
using this method can be easily evaded by an adaptive adversary who can effectively generate
adversarial examples with low distortions. Strauss et al. [114] considered ensemble methods as
sole defense against adversarial attacks. Compared to existing ensemble methods, their method
improves prediction confidence for clean samples while increasing robustness against adversarial
examples at the cost of increased computational complexity. Liu et al. [76] proposed the Random
Self-Ensemble (RSE), a defense algorithm that combines the concept of randomness and ensemble
to improve the robustness of neural networks. Using experimental results, the authors were able to
demonstrate that this method can generalize well and is an effective defense against attacks such
as the Carlini & Wagner attack [25].

Reformers/Autoencoders. A reformer is a network that takes an input 𝑥 and reconstructs it to 𝑥 ′
before passing it to a classifier [135]. An ideal reformer is not expected to change the prediction
accuracy of benign examples, however it is expected to change adversarial examples such that
the reformed examples are close to the benign examples. An autoencoder is an artificial neural
network that comprises of an encoder and a decoder, where the encoder learns a set of hidden
representation for a given input data and the decoder reconstructs the output using the hidden

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:26

Oseni and Moustafa, et al.

representation Gu and Rigazio [49]. Meng and Chen [83] introduced MagNet, a robust defense
against adversarial attacks on deep neural networks. MagNet, as a framework, incorporates a
reformer and a detector. The detector, implemented using an autoencoder, calculates reconstruction
error and rejects examples with high reconstruction errors. As an attack-independent defense
method, experimental results show that MagNet achieved 99% classification accuracy on adversarial
examples generated by the most of the attacks considered.

5.2.2 Differential Privacy. Lecuyer et al. [69] proposed the use of differential privacy as a means
of improving the robustness of deep neural networks against adversarial attacks. In the machine
learning context, differential privacy involves the introduction of randomness to training data or
model outputs in order to limit the disclosure of private information of individual records included
in the training data. Lecuyer et al. [70] introduced PixelDP, a certified defense against adversarial
examples based on differential privacy. When applied to deep learning models, differential privacy
helps to protect training data from inversion attacks aimed at reconstructing the training data from
model parameters [7]. Lecuyer et al. described PixelDP as a scalable, robust defense which applies
broadly to different DNN model types. Experimental results show PixelDP to provide more accurate
predictions under the euclidean norm attack compared to other certified defenses; however, it adds
some computational overhead for training and testing.

5.2.3 Homomorphic Encryption. Recent advances in homomorphic encryption schemes, specifically
the fully homomorphic encryption, enable certain operations, such as addition and multiplication,
to be carried out on encrypted data without the need to decrypt the data [5]. Homomorphic
encryption has been proposed in recent literature as a means of achieving the privacy-preservation
requirements for sensitive data when using predictive models managed by third-parties, such as
in the case of machine learning as a service platforms. Xie et al. [130] introduced crypto-nets,
a neural network which makes predictions over encrypted data and results in encrypted form.
Through theoretical and practical analysis, the authors demonstrated that the computational
complexity involved in applying neural networks to encrypted data makes crypto-nets infeasible
in specific scenarios. Hesamifard et al. [53] introduced CryptoDL, a solution for implementing
deep neural network algorithms over encrypted data. The primary components of CryptoDL are
deep convolutional neural networks, trained using low degree polynomials, and homomorphic
encryption. The authors designed CryptoDL to be trained using low degree polynomials as this is
essential for efficient homomorphic encryption schemes and overcomes the practical limitations of
other solutions that use homomorphic encryption, such as crypto-nets. Experimental results show
that CryptoDL is scalable and provides efficient and accurate privacy-preserving predictions.

5.3 Detection-only defenses
Detection-only defenses have recently attracted research attention due to the limitations of existing
complete defenses. Works by Carlini and Wagner [24] and He et al. [52] have shown that many
of the complete defenses can easily be broken or evaded. The goal of detection-only defenses is
to provide a distinction between clean examples and adversarial examples, and to filter out the
adversarial examples for reliable classification. We now discuss some of the proposed detection-only
defenses.

5.3.1 Kernel Density Estimation (KDE) based detector. The kernel density estimation method was
proposed by Feinman et al. [41] for detecting adversarial examples in deep neural networks. It uses
the kernel density estimates calculated in the feature space of the last hidden layer of a neural
network to detect points that lie far from the data manifold. According to Feinman et al., the density
estimate for a point 𝑥 with predicted class 𝑡 is given as:

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

ˆ𝐾 (𝑥, 𝑋𝑡 ) =

∑︁

𝑥𝑖 ∈𝑋𝑡

𝑘𝜎 (𝜙 (𝑥), 𝜙 (𝑥𝑖 ))

111:27

(41)

where 𝜙 (𝑥) is the last hidden layer activation vector for point 𝑥, 𝑋𝑡 is the training set of class 𝑡 and
𝜎 is the tuned bandwidth. Feinman et al. showed that this defense is effective against the FGSM,
BIM, JSMA and C&W attacks based on experiments using the MNIST, CIFAR-10 and SVHN datasets.
However, Carlini and Wagner [24] later demonstrated how to break the KDE by using the C&W to
generate adversarial examples for MNIST with increased distortion.

5.3.2 Bayesian Neural Network Uncertainty based detector. The Bayesian Neural Network Un-
certainty method for identifying adversarial examples was proposed by Feinman et al. [41] and
is used to detect points that lie in the low confidence regions of the input space. According to
Feinman et al., this method can be used to obtain additional information about model confidence
which are not normally available using methods based on distance metrics such as the KDE. The
proposed bayesian uncertainty method is based on adding randomness to the neural network by
using dropout, a method introduced in [113] for reducing overfitting when training deep neural
networks. The uncertainty estimate of the neural network on a given instance 𝑥 ∗ and stochastic
predictions { ˆ𝑦∗
1

𝑇 } can be computed as:

, ..., ˆ𝑦∗

𝑈 (𝑥 ∗) =

1
𝑇

𝑇
∑︁

𝑖=1

𝑇 ˆ𝑦∗

ˆ𝑦∗
𝑖

𝑖 −

(cid:32)

1
𝑇

𝑇
∑︁

𝑖=1

ˆ𝑦∗
𝑖

(cid:33)𝑇 (cid:32)

(cid:33)

ˆ𝑦∗
𝑖

1
𝑇

𝑇
∑︁

𝑖=1

(42)

Based on experimental analysis using the LeNet convolutional neural network trained with dropout
rate of 0.5, Feinman et al. found the Bayesian uncertainty method to be effective in detecting
adversarial examples crafted using a wide variety of attack methods. However, Carlini and Wagner
[24] demonstrated that this method can also be vulnerable to adversarial attack.

5.3.3 Maximum Mean Discrepancy based detector. Grosse et al. [47] proposed the use of the
Maximum Mean Discrepancy (MMD), a statistical hypothesis test, to detect adversarial examples
from a given input. The MMD test is based on the framework of two-sample statistical hypothesis
testing used to determine whether samples 𝑋1 and 𝑋2 are drawn from then same distribution. If
sample 𝑋1 is drawn from distribution 𝑝 and sample 𝑋2 is drawn from distribution 𝑞, null hypothesis
𝐻0 states that 𝑝 = 𝑞 while the alternative hypothesis 𝐻𝐴 indicates that 𝑝 ≠ 𝑞. The statistical test
takes the two samples as its input and distinguishes between 𝐻0 and 𝐻𝐴.

The MMD test is a kernel-based test introduced by Gretton et al. [46] for use when considering
data with high dimensionality. Using it as a detection technique means Grosse et al. had to focus
on the asymptotic distribution of the unbiased MMD. This algorithm involves using a subsampling
method to draw samples from the data available with replacement in order to consistently estimate
the distribution of the MMD under null hypothesis.

While Grosse et al. found that the MMD can statistically distinguish adversarial examples from
clean examples, Carlini and Wagner [24] later demonstrated that the MMD fails to detect attacks
when targeted adversarial examples crafted with the C&W attack algorithm are used.

Local Intrinsic Dimensionality based detector. Ma et al. [79] proposed the use of Local Intrinsic
5.3.4
Dimensionality (LID) to identify adversarial examples from a given input. In contrast to other
detection-only defenses which are based on density distribution in dataset, the LID-based detector
characterizes the intrinsic dimensionality of adversarial regions of deep neural networks based on
local distance distribution from a reference data point to its neighbors [55, 56]. In their work, Ma

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:28

Oseni and Moustafa, et al.

et al. demonstrated how well LID estimates can be used to detect adversarial examples by using the
Maximum Likelihood Estimator (MLE) of LID to approximate the true distance distribution.

Formally, given a reference data sample 𝑥 ∈ 𝑝, where 𝑝 is the data distribution, the maximum

likelihood estimator of the LID at 𝑥 is defined as:

(cid:100)LID(𝑥) = −

(cid:32)

1
𝑘

𝑘
∑︁

log

𝑟𝑖 (𝑥)
𝑟𝑘 (𝑥)

(cid:33) −1

(43)

𝑖=1
Ma et al. demonstrated how LID characteristics can facilitate the identification of adversarial ex-
amples crafted using a wide range of attack algorithms. They also showed how the characterization
of adversarial regions could be used as features in an adversarial example detection process.

6 CHALLENGES AND FUTURE WORK
In previous sections, we presented a comprehensive review of security and privacy issues in AI
systems covering many machine learning models. In this section, we discuss some challenges in
the field and provide suggestions on future research directions.

6.1 Transferability of adversarial examples
Many machine learning models, including deep neural networks, are subject to the transferability
of adversarial examples [128]. The transferability phenomenon means that adversarial examples
crafted to fool a particular model can easily be used to fool other models. This property poses
a security challenge for many deep neural networks because adversarial examples generated
from one model can be used to attack another model without knowledge of the target model’s
parameters [136], thereby enabling black-box attacks [96]. While many of the works reviewed in
this paper show real evidence of transferability of adversarial examples, the fundamental reasons
why adversarial examples transfer are not well understood. Based on experimental analysis in [124],
the hypothesis on the ubiquity of transferability does not always hold and there are suggestions
that transferability property is not inherent in non-robust models despite existence of adversarial
examples. Therefore, research efforts focused on understanding the transferability phenomenon is
essential for creating robust machine learning models.

6.2 Evaluating robustness of defense methods
Many of the existing defenses against adversarial attacks have already been broken or bypassed,
raising concerns about the lack of thorough evaluation of the robustness of existing defense
methods. In their work, Carlini and Wagner [24] reviewed ten existing methods for detecting
adversarial examples and demonstrated how easy it is for an adversary to bypass these methods
by constructing new cost functions. Their work shows that existing defenses are not robust
against adaptive adversaries while also lacking thorough security evaluations. In Section 5.2.2 we
discussed Differential Privacy as a method for improving the robustness of deep learning models
against inversion attacks aimed at reconstructing training data from model parameters. Differential
Privacy adds randomness to the training data as a means of limiting the disclosure of private
information included within the training dataset. While the efficacy of differential privacy has been
demonstrated in a number of experiments, there are no practical evaluations or metrics that can be
used to measure if the differential privacy bounds are strong enough [7]. Also in Section 5.2.3, we
discussed homomorphic encryption as an encryption scheme that can be used to preserve privacy
of sensitive data used in many predictive models. While recent implementations of homomorphic
encryption such as CryptoDL [53] have demonstrated high prediction rates, the prediction accuracy
is not at par with many state-of-the-art deep models and is found to be incompatible with deeper

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:29

models. A future research direction should, therefore, seek to investigate the properties that defense
methods need to have in order to guarantee robustness against adaptive adversaries.

6.3 Difficulty in controlling the magnitude of adversarial perturbations
The different methods for generating adversarial examples impose small imperceptible input
perturbations in order to change a neural network’s prediction. However, determining the exact
magnitude of perturbations required to fool a neural network is difficult, because input perturbations
that are too small cannot generate adversarial examples, and perturbations that are too large are
not imperceptible [137]. Therefore, being able to control the magnitude of input perturbations
poses an ongoing challenge.

6.4 Lack of research focus on attacks beyond classification tasks
Convolutional neural networks have been hugely successful in computer vision applications. As a
result, a majority of the existing methods for generating adversarial examples apply to computer
vision applications such as image recognition and object detection which are part of the machine
learning classification tasks [45, 67, 84, 85, 98, 107]. While some attention has been given to
adversarial attacks on other machine learning tasks, these are only a handful in comparison to the
attention given to classification tasks. It has been established in many literature that all machine
learning models, including deep neural networks, are vulnerable to adversarial attacks. Therefore,
more research effort on the adversarial threats facing other machine learning task categories, such
as reinforcement learning, is required.

6.5 Evolving threat of unknown unknowns
Unknown unknowns pose a significant threat to machine learning systems deployed in adversarial
environments, similar to how they are a real threat in many cybersecurity problems such as malware
and intrusion detection Biggio and Roli [19]. Unlike, known unknowns which are used to model
attacks in adversarial machine learning, unknown unknowns are often unpredictable and can cause
machine learning models to misclassify with high-confidence due to inputs that are significantly
different from the known training data. For machine learning systems to have the capability to
detect unknown unknowns using robust techniques for anomaly detection, new research paths
would need to be explored in this area.

6.6 Randomization of classifier’s decision boundary
Introducing some randomization in the placement of the decision boundary for classifiers has been
proposed as a method for improving the classifier’s security against evasion attacks Barreno et al.
[10]. Surprisingly, there has only been a few attempts in literature at investigating this kind of
randomization as a viable defense method. While randomization does indeed increase the amount
of work that an adversary would need to do in order to move the decision boundary past a targeted
point, Barreno et al. acknowledged that a major challenge with using randomization is that it can
also increase the classifiers initial error rate, thereby degrading its performance on clean data. The
problem of finding an appropriate amount of randomization that can be introduced into a model to
achieve robustness against adversarial attacks remains an open problem.

7 CONCLUSION
The field of adversarial machine learning has received significant research attention in recent
years. In particular, many research works have explored adversarial attacks on machine learning
models in the context of computer vision and image recognition, natural language processing and
cybersecurity. Several defense methods have also been proposed with research efforts focused on

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:30

Oseni and Moustafa, et al.

evaluating the effectiveness of these defense methods against the continually evolving adversarial
attacks.

In this work, we started with a review of recent surveys papers focusing on the issues of security
and privacy in AI. We find that many of these existing surveys did not cover attacks and defenses
across all machine learning task categories, with most only focusing on deep neural networks in the
context of computer vision, natural language processing and cybersecurity. As a basis for describing
adversarial attacks on machine learning models, we start by providing a theoretical background of
machine learning task categories and make a clear distinction between shallow learning methods
and the more recent deep learning methods. We then present a new framework for the holistic
review of adversarial attacks on AI systems by first describing an adversary’s goals, knowledge
and capabilities, followed by a comprehensive analysis of adversarial attacks and defense methods
covering many machine learning models.

REFERENCES

[1] Mahdieh Abbasi and Christian Gagné. 2017. Robustness to Adversarial Examples through an Ensemble of Specialists,
In International Conference on Learning Representations (ICLR), Workshop Track. ArXiv e-prints. https://arxiv.org/
abs/1702.06856

[2] Amit Adate, Rishabh Saxena, et al. 2017. Understanding How Adversarial Noise Affects Single Image Classification.

In International Conference on Intelligent Information Technologies. Springer, 287–295.

[3] Naveed Akhtar and Ajmal Mian. 2018. Threat of adversarial attacks on deep learning in computer vision: A survey.

IEEE Access 6 (2018), 14410–14430.

[4] Giovanni Apruzzese, Michele Colajanni, Luca Ferretti, Alessandro Guido, and Mirco Marchetti. 2018. On the
effectiveness of machine and deep learning for cyber security. In 2018 10th International Conference on Cyber Conflict
(CyCon). IEEE, 371–390.

[5] Louis JM Aslett, Pedro M Esperança, and Chris C Holmes. 2015. A review of homomorphic encryption and software

tools for encrypted statistical machine learning. stat 1050 (2015), 26.

[6] Jimmy Ba and Rich Caruana. 2014. Do deep nets really need to be deep?. In Advances in neural information processing

systems. 2654–2662.

[7] Ho Bae, Jaehee Jang, Dahuin Jung, Hyemi Jang, Heonseok Ha, and Sungroh Yoon. 2018. Security and privacy issues

in deep learning. arXiv preprint arXiv:1807.11655 (2018).

[8] Shumeet Baluja and Ian Fischer. 2017. Adversarial transformation networks: Learning to generate adversarial

examples. arXiv preprint arXiv:1703.09387 (2017).

[9] Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. 2010. The security of machine learning. Machine

Learning 81, 2 (2010), 121–148.

[10] Marco Barreno, Blaine Nelson, Russell Sears, Anthony D Joseph, and J Doug Tygar. 2006. Can machine learning be

secure?. In Proceedings of the 2006 ACM Symposium on Information, computer and communications security. 16–25.

[11] Battista Biggio, Igino Corona, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli. 2011. Bagging classifiers for fighting
poisoning attacks in adversarial classification tasks. In International workshop on multiple classifier systems. Springer,
350–359.

[12] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel Laskov, Giorgio Giacinto, and
Fabio Roli. 2013. Evasion attacks against machine learning at test time. In Joint European conference on machine
learning and knowledge discovery in databases. Springer, 387–402.

[13] Battista Biggio, Igino Corona, Blaine Nelson, Benjamin IP Rubinstein, Davide Maiorca, Giorgio Fumera, Giorgio
Giacinto, and Fabio Roli. 2014. Security evaluation of support vector machines in adversarial environments. In
Support Vector Machines Applications. Springer, 105–153.

[14] Battista Biggio, Luca Didaci, Giorgio Fumera, and Fabio Roli. 2013. Poisoning attacks to compromise face templates.

In 2013 International Conference on Biometrics (ICB). IEEE, 1–7.

[15] Battista Biggio, Giorgio Fumera, and Fabio Roli. 2010. Multiple classifier systems for robust classifier design in

adversarial environments. International Journal of Machine Learning and Cybernetics 1, 1-4 (2010), 27–41.

[16] Battista Biggio, Giorgio Fumera, and Fabio Roli. 2014. Security Evaluation of Pattern Classifiers under Attack. IEEE

Transactions on Knowledge and Data Engineering 26, 4 (2014), 984–996.

[17] Battista Biggio, Giorgio Fumera, Fabio Roli, and Luca Didaci. 2012. Poisoning adaptive biometric systems. In Joint
IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern
Recognition (SSPR). Springer, 417–425.

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:31

[18] Battista Biggio, Blaine Nelson, and Pavel Laskov. 2012. Poisoning Attacks against Support Vector Machines. In
Proceedings of the 29th International Conference on International Conference on Machine Learning (Edinburgh, Scotland)
(ICML’12). Omnipress, Madison, WI, USA, 1467–1474.

[19] Battista Biggio and Fabio Roli. 2018. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern

Recognition 84 (2018), 317–331.

[20] K. A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloé M
Kiddon, Jakub Konečný, Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage,
and Jason Roselander. 2019. Towards Federated Learning at Scale: System Design. In SysML 2019. https://arxiv.org/
abs/1902.01046

[21] Leo Breiman. 1996. Bagging predictors. Machine learning 24, 2 (1996), 123–140.
[22] Cody Burkard and Brent Lagesse. 2017. Analysis of causative attacks against SVMs learning from data streams. In

Proceedings of the 3rd ACM on International Workshop on Security And Privacy Analytics. 31–36.

[23] Xiaoyu Cao and Neil Zhenqiang Gong. 2017. Mitigating evasion attacks to deep neural networks via region-based

classification. In Proceedings of the 33rd Annual Computer Security Applications Conference. 278–287.

[24] Nicholas Carlini and David Wagner. 2017. Adversarial examples are not easily detected: Bypassing ten detection

methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. 3–14.

[25] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of neural networks. In 2017 ieee

symposium on security and privacy (sp). IEEE, 39–57.

[26] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted attacks on speech-to-text. In 2018

IEEE Security and Privacy Workshops (SPW). IEEE, 1–7.

[27] Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopadhyay. 2018.

Adversarial attacks and defences: A survey. arXiv preprint arXiv:1810.00069 (2018).

[28] Patrick PK Chan, Zhi-Min He, Hongjiang Li, and Chien-Chang Hsu. 2018. Data sanitization against adversarial
label contamination based on data complexity. International Journal of Machine Learning and Cybernetics 9, 6 (2018),
1039–1052.

[29] Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh. 2018. Attacking Visual Language Grounding
with Adversarial Examples: A Case Study on Neural Image Captioning. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers). 2587–2597.

[30] Jialu Chen, Jun Zhou, Zhenfu Cao, Athanasios V Vasilakos, Xiaolei Dong, and Kim-Kwang Raymond Choo. 2019.
Lightweight Privacy-preserving Training and Evaluation for Discretized Neural Networks. IEEE Internet of Things
Journal 7, 4 (2019), 2663–2678.

[31] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. 2018. Ead: elastic-net attacks to deep neural

networks via adversarial examples. In Thirty-second AAAI conference on artificial intelligence.

[32] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017. Zoo: Zeroth order optimization
based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM
Workshop on Artificial Intelligence and Security. 15–26.

[33] Moustapha M Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. 2017. Houdini: Fooling deep structured visual
and speech recognition models with adversarial examples. In Advances in neural information processing systems.
6977–6987.

[34] Gabriela F Cretu, Angelos Stavrou, Michael E Locasto, Salvatore J Stolfo, and Angelos D Keromytis. 2008. Casting out
demons: Sanitizing training data for anomaly sensors. In 2008 IEEE Symposium on Security and Privacy (sp 2008). IEEE,
81–95.

[35] Hung Dang, Yue Huang, and Ee-Chien Chang. 2017. Evading classifiers by morphing in the dark. In Proceedings of

the 2017 ACM SIGSAC Conference on Computer and Communications Security. 119–133.

[36] Li Deng. 2012. Three classes of deep learning architectures and their applications: a tutorial survey. APSIPA transactions

on signal and information processing (2012).

[37] Li Deng. 2014. A tutorial survey of architectures, algorithms, and applications for deep learning. APSIPA Transactions

on Signal and Information Processing 3 (2014).

[38] Thomas G Dietterich. 2000. Ensemble methods in machine learning. In International workshop on multiple classifier

systems. Springer, 1–15.

[39] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. 2018. Boosting adversarial
attacks with momentum. In Proceedings of the IEEE conference on computer vision and pattern recognition. 9185–9193.
[40] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi
Kohno, and Dawn Song. 2018. Robust physical-world attacks on deep learning visual classification. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. 1625–1634.

[41] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. 2017. Detecting adversarial samples from

artifacts. arXiv preprint arXiv:1703.00410 (2017).

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:32

Oseni and Moustafa, et al.

[42] Peter Flach. 2012. Machine learning: the art and science of algorithms that make sense of data. Cambridge University

Press.

[43] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion attacks that exploit confidence informa-
tion and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security. 1322–1333.

[44] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. 2014. Privacy in
pharmacogenetics: An end-to-end case study of personalized warfarin dosing. In 23rd {USENIX} Security Symposium
({USENIX} Security 14). 17–32.

[45] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. EXPLAINING AND HARNESSING ADVERSARIAL

EXAMPLES. stat 1050 (2015), 20.

[46] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. 2012. A kernel

two-sample test. The Journal of Machine Learning Research 13, 1 (2012), 723–773.

[47] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. 2017. On the (Statistical)

Detection of Adversarial Examples. CoRR abs/17 (2017). https://publications.cispa.saarland/1142/

[48] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. 2016. Adversarial

Perturbations Against Deep Neural Networks for Malware Classification. arXiv:1606.04435 [cs.CR]

[49] Shixiang Gu and Luca Rigazio. 2014. Towards deep neural network architectures robust to adversarial examples.

arXiv preprint arXiv:1412.5068 (2014).

[50] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying vulnerabilities in the machine

learning model supply chain. arXiv preprint arXiv:1708.06733 (2017).

[51] Warren He, Bo Li, and Dawn Song. 2018. Decision boundary analysis of adversarial examples. In 6th International

Conference on Learning Representations, ICLR 2018.

[52] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. 2017. Adversarial example defenses:
ensembles of weak defenses are not strong. In Proceedings of the 11th USENIX Conference on Offensive Technologies.
15–15.

[53] Ehsan Hesamifard, Hassan Takabi, and Mehdi Ghasemi. 2017. Cryptodl: Deep neural networks over encrypted data.

arXiv preprint arXiv:1711.05189 (2017).

[54] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint

arXiv:1503.02531 (2015).

[55] Michael E Houle. 2017. Local intrinsic dimensionality I: an extreme-value-theoretic foundation for similarity

applications. In International Conference on Similarity Search and Applications. Springer, 64–79.

[56] Michael E Houle, Erich Schubert, and Arthur Zimek. 2018. On the correlation between local intrinsic dimensionality

and outlierness. In International Conference on Similarity Search and Applications. Springer, 177–191.

[57] Weiwei Hu and Ying Tan. 2017. Generating adversarial malware examples for black-box attacks based on gan. arXiv

preprint arXiv:1702.05983 (2017).

[58] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and J Doug Tygar. 2011. Adversarial machine

learning. In Proceedings of the 4th ACM workshop on Security and artificial intelligence. 43–58.

[59] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-box adversarial attacks with limited queries

and information. arXiv preprint arXiv:1804.08598 (2018).

[60] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. 2018. Manipulating
machine learning: Poisoning attacks and countermeasures for regression learning. In 2018 IEEE Symposium on Security
and Privacy (SP). IEEE, 19–35.

[61] Wenbo Jiang, Hongwei Li, Sen Liu, Xizhao Luo, and Rongxing Lu. 2020. Poisoning and evasion attacks against deep
learning algorithms in autonomous vehicles. IEEE transactions on vehicular technology 69, 4 (2020), 4439–4449.
[62] Marius Kloft and Pavel Laskov. 2012. Security analysis of online centroid anomaly detection. The Journal of Machine

Learning Research 13, 1 (2012), 3681–3724.

[63] Pang Wei Koh, Jacob Steinhardt, and Percy Liang. 2018. Stronger data poisoning attacks break data sanitization

defenses. arXiv preprint arXiv:1811.00741 (2018).

[64] Jakub Konečn`y, H Brendan McMahan, Daniel Ramage, and Peter Richtárik. 2016. Federated optimization: Distributed

machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527 (2016).

[65] Jakub Konečn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. 2016.
Federated Learning: Strategies for Improving Communication Efficiency. In NIPS Workshop on Private Multi-Party
Machine Learning. https://arxiv.org/abs/1610.05492

[66] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial examples in the physical world. arXiv preprint

arXiv:1607.02533 (2016).

[67] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial machine learning at scale. arXiv preprint

arXiv:1611.01236 (2016).

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:33

[68] Pavel Laskov et al. 2014. Practical evasion of a learning-based classifier: A case study. In 2014 IEEE symposium on

security and privacy. IEEE, 197–211.

[69] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. 2018. On the connection

between differential privacy and adversarial robustness in machine learning. stat 1050 (2018), 9.

[70] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. 2019. Certified robustness to
adversarial examples with differential privacy. In 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 656–672.
[71] In Lee and Yong Jae Shin. 2020. Machine learning for enterprises: Applications, algorithm selection, and challenges.

Business Horizons 63, 2 (2020), 157–170.

[72] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. 2016. Data poisoning attacks on factorization-based

collaborative filtering. In Advances in neural information processing systems. 1885–1893.

[73] Guofu Li, Pengjia Zhu, Jin Li, Zhemin Yang, Ning Cao, and Zhiyi Chen. 2018. Security matters: A survey on adversarial

machine learning. arXiv preprint arXiv:1810.07339 (2018).

[74] Jiao Li, Yang Liu, Tao Chen, Zhen Xiao, Zhenjiang Li, and Jianping Wang. 2020. Adversarial Attacks and Defenses on

Cyber-Physical Systems: A Survey. IEEE Internet of Things Journal (2020).

[75] Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, and Victor CM Leung. 2018. A survey on security threats and

defensive techniques of machine learning: A data driven view. IEEE access 6 (2018), 12103–12117.

[76] Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. 2018. Towards robust neural networks via random

self-ensemble. In Proceedings of the European Conference on Computer Vision (ECCV). 369–385.

[77] P. Louridas and C. Ebert. 2016. Machine Learning. IEEE Software 33, 5 (Sep. 2016), 110–115. https://doi.org/10.1109/

MS.2016.114

[78] Giulio Lovisotto, Simon Eberz, and Ivan Martinovic. 2019. Biometric Backdoors: A Poisoning Attack Against

Unsupervised Template Updating. arXiv preprint arXiv:1905.09162 (2019).

[79] Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Dawn Song, Michael E
Houle, and James Bailey. 2018. Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality. In
International Conference on Learning Representations.

[80] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep

Learning Models Resistant to Adversarial Attacks. In International Conference on Learning Representations.

[81] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017. Communication-
efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics. PMLR, 1273–1282.
[82] Marco Melis, Ambra Demontis, Battista Biggio, Gavin Brown, Giorgio Fumera, and Fabio Roli. 2017. Is deep learning
safe for robot vision? adversarial examples against the icub humanoid. In Proceedings of the IEEE International
Conference on Computer Vision Workshops. 751–759.

[83] Dongyu Meng and Hao Chen. 2017. Magnet: a two-pronged defense against adversarial examples. In Proceedings of

the 2017 ACM SIGSAC Conference on Computer and Communications Security. 135–147.

[84] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. 2017. Universal adversarial

perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1765–1773.

[85] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016. Deepfool: a simple and accurate
method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition.
2574–2582.

[86] M Mozaffari-Kermani, S Sur-Kolay, A Raghunathan, and NK Jha. 2015. Systematic Poisoning Attacks on and Defenses
for Machine Learning in Healthcare. IEEE journal of biomedical and health informatics 19, 6 (2015), 1893–1905.
[87] Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu, and
Fabio Roli. 2017. Towards poisoning of deep learning algorithms with back-gradient optimization. In Proceedings of
the 10th ACM Workshop on Artificial Intelligence and Security. 27–38.

[88] Luis MUÑOZ-GONZÁLEZ, Javier CARNERERO-CANO, T Kenneth, and Emil C LUPU. 2019. Challenges and Advances
in Adversarial Machine Learning. Resilience and Hybrid Threats: Security and Integrity for the Digital World 55 (2019),
102.

[89] Luis Muñoz-González and Emil C. Lupu. 2019. The Security of Machine Learning Systems. Springer International

Publishing, Cham, 47–79.

[90] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacy analysis of deep learning: Passive and
active white-box inference attacks against centralized and federated learning. In 2019 IEEE Symposium on Security
and Privacy (SP). IEEE, 739–753.

[91] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein, Udam Saini, Charles
Sutton, JD Tygar, and Kai Xia. 2009. Misleading learners: Co-opting your spam filter. In Machine learning in cyber
trust. Springer, 17–51.

[92] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein, Udam Saini, Charles A
Sutton, J Doug Tygar, and Kai Xia. 2008. Exploiting Machine Learning to Subvert Your Spam Filter. LEET 8 (2008),

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

111:34

1–9.

Oseni and Moustafa, et al.

[93] Andrew Newell, Rahul Potharaju, Luojie Xiang, and Cristina Nita-Rotaru. 2014. On the practicality of integrity
attacks on document-level sentiment analysis. In Proceedings of the 2014 Workshop on Artificial Intelligent and Security
Workshop. 83–93.

[94] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep neural networks are easily fooled: High confidence predictions
for unrecognizable images. In Proceedings of the IEEE conference on computer vision and pattern recognition. 427–436.
[95] Mesut Ozdag. 2018. Adversarial attacks and defenses against deep neural networks: a survey. Procedia Computer

Science 140 (2018), 152–161.

[96] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. 2016. Transferability in machine learning: from phenomena

to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277 (2016).

[97] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2016.
Practical black-box attacks against deep learning systems using adversarial examples. arXiv preprint arXiv:1602.02697
1, 2 (2016), 3.

[98] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. 2016.
The limitations of deep learning in adversarial settings. In 2016 IEEE European symposium on security and privacy
(EuroS&P). IEEE, 372–387.

[99] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael P Wellman. 2018. SoK: Security and privacy in

machine learning. In 2018 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 399–414.

[100] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a defense to
adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP). IEEE,
582–597.

[101] Li Pengcheng, Jinfeng Yi, and Lijun Zhang. 2018. Query-efficient black-box attack by active learning. In 2018 IEEE

International Conference on Data Mining (ICDM). IEEE, 1200–1205.

[102] Nikolaos Pitropakis, Emmanouil Panaousis, Thanassis Giannetsos, Eleftherios Anastasiadis, and George Loukas. 2019.

A taxonomy and survey of attacks against machine learning. Computer Science Review 34 (2019), 100199.

[103] Shilin Qiu, Qihe Liu, Shijie Zhou, and Chunjiang Wu. 2019. Review of artificial intelligence adversarial attack and

defense technologies. Applied Sciences 9, 5 (2019), 909.

[104] Kui Ren, Tianhang Zheng, Zhan Qin, and Xue Liu. 2020. Adversarial Attacks and Defenses in Deep Learning.

Engineering (2020).

[105] Sebastian Risi and Mike Preuss. 2020. Behind DeepMind’s AlphaStar AI that Reached Grandmaster Level in StarCraft

II. KI-Künstliche Intelligenz 34, 1 (2020), 85–86.

[106] Suranjana Samanta and Sameep Mehta. 2017. Towards crafting text adversarial samples. arXiv preprint arXiv:1707.02812

(2017).

[107] Sayantan Sarkar, Ankan Bansal, Upal Mahbub, and Rama Chellappa. 2017. UPSET and ANGRI: Breaking high

performance image classifiers. arXiv preprint arXiv:1707.01159 (2017).

[108] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein.
2018. Poison frogs! targeted clean-label poisoning attacks on neural networks. In Advances in Neural Information
Processing Systems. 6103–6113.

[109] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis,
Gavin Taylor, and Tom Goldstein. 2019. Adversarial training for free!. In Advances in Neural Information Processing
Systems. 3353–3364.

[110] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2016. Accessorize to a crime: Real and
stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 acm sigsac conference on computer and
communications security. 1528–1540.

[111] Yi Shi and Yalin E Sagduyu. 2017. Evasion and causative attacks with adversarial deep learning. In MILCOM 2017-2017

IEEE Military Communications Conference (MILCOM). IEEE, 243–248.

[112] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against

machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 3–18.

[113] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout:
a simple way to prevent neural networks from overfitting. The journal of machine learning research 15, 1 (2014),
1929–1958.

[114] Thilo Strauss, Markus Hanselmann, Andrej Junginger, and Holger Ulmer. 2017. Ensemble methods as a defense to

adversarial perturbations against deep neural networks. arXiv preprint arXiv:1709.03423 (2017).

[115] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. 2019. One pixel attack for fooling deep neural networks.

IEEE Transactions on Evolutionary Computation 23, 5 (2019), 828–841.

[116] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor Dumitras. 2018. When does machine
learning {FAIL}? generalized transferability for evasion and poisoning attacks. In 27th {USENIX} Security Symposium

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

Security and Privacy for Artificial Intelligence: Opportunities and Challenges

111:35

({USENIX} Security 18). 1299–1316.

[117] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press. 3–86 pages.
[118] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.
Intriguing properties of neural networks. In International Conference on Learning Representations. http:

2014.
//arxiv.org/abs/1312.6199

[119] Elham Tabassi, Kevin J Burns, Michael Hadjimichael, Andres D Molina-Markham, and Julian T Sexton. 2019. A

Taxonomy and Terminology of Adversarial Machine Learning.

[120] Guanhong Tao, Shiqing Ma, Yingqi Liu, and Xiangyu Zhang. 2018. Attacks meet interpretability: Attribute-steered

detection of adversarial samples. In Advances in Neural Information Processing Systems. 7717–7728.

[121] Oliver Theobald. 2017. Machine Learning for Absolute Beginners (second ed.). Scatterplot Press.
[122] Sam Thomas and Nasseh Tabrizi. 2018. Adversarial machine learning: A literature review. In International Conference

on Machine Learning and Data Mining in Pattern Recognition. Springer, 324–334.

[123] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Drew McDaniel. 2018.
Ensemble adversarial training: Attacks and defenses. In 6th International Conference on Learning Representations, ICLR
2018.

[124] Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. 2017. The space of transferable

adversarial examples. arXiv preprint arXiv:1704.03453 (2017).

[125] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. 2016. Stealing machine learning

models via prediction apis. In 25th {USENIX} Security Symposium ({USENIX} Security 16). 601–618.

[126] Xianmin Wang, Jing Li, Xiaohui Kuang, Yu-an Tan, and Jin Li. 2019. The security of machine learning in an adversarial

setting: A survey. J. Parallel and Distrib. Comput. 130 (2019), 12–23.

[127] Ian H. Witten, Eibe Frank, Mark A. Hall, and Christopher J. Pal. 2017. Chapter 10 - Deep learning. In Data Mining:
Practical Machine Learning Tools and Techniques (fourth ed.), Ian H. Witten, Eibe Frank, Mark A. Hall, and Christopher J.
Pal (Eds.). Morgan Kaufmann, 417 – 466.

[128] Rey Reza Wiyatno, Anqi Xu, Ousmane Dia, and Archy de Berker. 2019. Adversarial Examples in Modern Machine

Learning: A Review. arXiv preprint arXiv:1911.05268 (2019).

[129] Chaowei Xiao, Ruizhi Deng, Bo Li, Taesung Lee, Benjamin Edwards, Jinfeng Yi, Dawn Song, Mingyan Liu, and Ian
Molloy. 2019. AdvIT: Adversarial Frames Identifier Based on Temporal Consistency in Videos. In Proceedings of the
IEEE International Conference on Computer Vision. 3968–3977.

[130] Pengtao Xie, Misha Bilenko, Tom Finley, Ran Gilad-Bachrach, Kristin Lauter, and Michael Naehrig. 2014. Crypto-nets:

Neural networks over encrypted data. arXiv preprint arXiv:1412.6181 (2014).

[131] Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature squeezing: Detecting adversarial examples in deep neural

networks. arXiv preprint arXiv:1704.01155 (2017).

[132] Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature squeezing mitigates and detects carlini/wagner adversarial

examples. arXiv preprint arXiv:1705.10686 (2017).

[133] Chaofei Yang, Qing Wu, Hai Li, and Yiran Chen. 2017. Generative poisoning attack method against neural networks.

arXiv preprint arXiv:1703.01340 (2017).

[134] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine learning: Concept and applications.

ACM Transactions on Intelligent Systems and Technology (TIST) 10, 2 (2019), 1–19.

[135] Zhizhou Yin, Wei Liu, and Sanjay Chawla. 2019. Adversarial Attack, Defense, and Applications with Deep Learning

Frameworks. In Deep Learning Applications for Cyber Security. Springer, 1–25.

[136] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. 2019. Adversarial examples: Attacks and defenses for deep learning.

IEEE transactions on neural networks and learning systems 30, 9 (2019), 2805–2824.

[137] Jiliang Zhang and Chen Li. 2019. Adversarial examples: Opportunities and challenges. IEEE transactions on neural

networks and learning systems (2019), 1–16.

[138] Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning
Models in Natural Language Processing: A Survey. ACM Transactions on Intelligent Systems and Technology (TIST) 11,
3 (2020), 1–41.

[139] Xiaojin Zhu and Andrew B Goldberg. 2009. Introduction to semi-supervised learning. Synthesis lectures on artificial

intelligence and machine learning 3, 1 (2009), 1–130.

J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2020.

