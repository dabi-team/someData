2
2
0
2

r
a

M
4
2

]

R
C
.
s
c
[

1
v
9
7
1
3
1
.
3
0
2
2
:
v
i
X
r
a

Automatic User Proﬁling in Darknet Markets

Claudia Peersman
Matthew Edwards
Emma Williams
Awais Rashid

March 25, 2022

 
 
 
 
 
 
Automatic User Proﬁling in Darknet Mar-
kets – a Scalability Study

To extend our qualitative analysis we reviewed a range of Natural Language
Processing and Text Categorisation techniques that could be deployed on on-
line communications in Darknet Marketplaces. Our aim was to remove the
observer-observed eﬀect, which would allow us to explore cybercriminal inter-
actions without having to rely on the self-reporting of individual users. While
recent advances in these ﬁelds have shown promising results with regard to de-
tecting demographic characteristics, such as age group and gender, in several
text genres by automatically analysing the variation of an author’s linguistic fea-
tures, applying such techniques on underground cybercriminal communications
diﬀers from more general applications in that its deﬁning characteristics are both
domain and process dependent. This gives rise to a number of challenges of
which contemporary research has only scratched the surface. More speciﬁcally, a
text mining approach applied on online communications typically has no control
over the dataset size – the number of available communications will vary across
users. Hence, an automated system has to be robust towards limited data avail-
ability. Additionally, the quality of the data cannot be guaranteed. As a result,
the approach needs to be tolerant to a certain degree of linguistic noise (for ex-
ample, abbreviations, non-standard language use, spelling variations and errors,
non-standard use of punctuation). Finally, in the context of cybercriminal fora,
it has to be robust towards deceptive or adversarial behaviour, i.e. oﬀenders
who attempt to hide their identity and criminal intentions (obfuscation) or who
assume a false digital persona (imitation), potentially using coded language.

In this study, we investigate the scalability of state-of-the-art user proﬁling
technologies across diﬀerent online domains. More speciﬁcally, this work aims
to understand the reliability and limitations of current computational stylometry
approaches when these are applied to underground fora in which user popula-
tions potentially diﬀer from other online platforms (predominantly male, younger
age and greater computer use) and cyber oﬀenders who attempt to hide their
identity. Because no ground truth is available and no validated criminal data
from historic investigations is available for validation purposes, we have collected
new data from clearweb forums that do include user demographics and could be
more closely related to underground fora in terms of user population (e.g., tech
communities) than commonly used social media benchmark datasets showing a
more balanced user population.

Additionally, a key question when designing a user proﬁling system that could
be used to support cybercrime investigations, is if the methodology will remain
useful when it is confronted with adversarial text samples. Initial work in this area
is not very encouraging. The authors of [1], for example, demonstrated that even
state-of-the-art authorship identiﬁcation methods could be reduced to random
behaviour when they are confronted with passages that include obfuscation, in
which people attempt to hide their identity, or imitation, during which people
try to mimic an other author’s writing style [2]. However, like previous age
prediction studies most adversarial stylometry research often included relatively
large samples of texts in their experiments . As a result, it could very well
be that the number of clues revealing an author’s own writing style were too
limited to stand out between the majority of deceptive features in these larger
text samples.

Within the context of the present study, it was our hypothesis that a user’s
stylistic ﬁngerprint (or rather “writing print”) might unwittingly ﬂow through
in (parts of) their communications. Hence, this study also investigates whether
a message-based approach, in which predictions are made on the level of the
individual post and aggregated to the user level
in post-processing, enables
the system to identify potential clues of deception more accurately than the
traditional user-based approach that renders predictions directly on the user
level.

The resulting models are evaluated using the PAN dataset [3] and two
additional datasets collected from the clearweb (the Goodreads and Pornhub
datasets). Finally, the best performing models are applied to the DNM dataset.
The ﬁndings from this analysis were used to inform AMoC’s ﬁnal qualitative
framework of cyber oﬀender characteristics.

Detecting Age and Gender Online

Dataset

While the DNM Corpus described below generates material suitable for unsu-
pervised learning approaches, it does not provide “ground truth” for training
and testing user proﬁling models. Hence, the performance of our user proﬁling
techniques needed to be assessed through ﬁrst establishing a baseline in sim-
ilar non-criminal data for which ground-truth demographics are available. For
this purpose, we used the PAN Corpus (2012 – 2017), which contains diﬀerent
corpora collected from the author proﬁling tasks at PAN 2013, 2014 and 2015

[4, 5, 3] and covers three online media genres (blogs, Twitter feeds and unspec-
iﬁed social media postings). All corpora contain metadata about gender and
age group (18–24, 25–34, 35–49, 50–XX). An overview of the Pan dataset is
provided in Table 1.

Table 1: Number of messages per age and gender group in the (English) Pan
dataset.

Age Group Female Male
5,856
5,749
1,797
917
14,319

5,572
6,243
2,005
1,013
14,833

18-24
25-34
35-49
50-XX
Total

Total
11,428
11,992
3,802
1,930
29,152

Feature Selection

Text mining studies typically include (combinations of) lexical, character and
syntactic features in their experiments.
In this study, a new feature type is
introduced, namely chatspeak features. We describe the NLP procedures for
extracting these features at the end of this section.

Lexical features. Unlike some previous studies on age or gender prediction, we
did not create a dictionary of hand-picked features that are likely to distinguish
between diﬀerent age or gender categories. Instead, we applied a data-oriented
approach by extracting all token unigram features (i.e., a Bag-of-Words model),
including emoticons, allowing for a more complete picture of the age and gen-
der related linguistic variation in the data. To enable a similar data-oriented
approach for content and function words individually, a list of function words
is required. Although such a list in itself is limited in Standard English, when
analysing online social media messages, the approach is confronted with numer-
ous linguistic variations (e.g., “ive” instead of “I’ve”). Therefore, we extended
the list of standard function words with the most commonly used non-standard
function words found through manual inspection of the datasets. The content
word features included all other words and emoticons. Additionally, to include
context information in the experiments, we extracted token bigrams.

Character features. As character n-grams have been shown to be useful
for tracing stylometric evidence beyond topic and genre [6] and proven to be
reliable when dealing with limited data [7, 8], we included character bi-, tri- and
tetragrams in the user proﬁling experiments.

Chatspeak features. In chatspeak, paralinguistic and non-verbal cues — that

are present in spoken discourse, but absent in the formal written repertoire —
are often compensated by other linguistic features, such as emoticons, character
ﬂooding and the use of upper-case and punctuation ﬂooding to express em-
phasis [9]. Hence, information on the presence of these paralinguistic features
were also included in the document representations. More speciﬁcally, prior
to pre-processing, the occurrence of character and punctuation ﬂooding were
extracted and represented as “[char ﬂood]” and “[punct ﬂood]” features, and
the occurrence of non-standard capitalisation as “[char upper]”. Additionally,
emoticons and other combinations of characters and punctuation were repre-
sented as “[emoji]” features. Finally, for each token, we extracted information
on its “standardness”: a word was labelled non-standard when it was analysed
as such by Language Tool1. We also included the rule id, category and rule issue
type for each non-standard word as additional features. An example is provided
in Table 2.

Table 2: Language Tool output for this PAN post: “ppl who stan her at this
point are as ignorant as iggy stans i love azealias insite on racism but her mess
makes it invalid”
Non-Standard Language
Use
Sentence does not start
with an uppercase let-
stans,
ter,
i, azealias,
insite, no
comma before ‘but’

TYPOS,
CASING,
TYPOS, TYPOS, TY-
POS, TYPOS, TYPOS,
PUNCTUATION

Category

Rule Id

stan,

iggy,

mis-
typographical,
spelling,
misspelling,
misspelling, misspelling,
misspelling, misspelling,
typographical

Rule Issue Type

UPPERCASE SENTENCE START,
MORFOLOGIK RULE EN,
MORFOLOGIK RULE EN,
MORFOLOGIK RULE EN,
I LOWERCASE,
FOLOGIK RULE EN,
FOLOGIK RULE EN,
COMMA COMPOUND SENTENCE 2

MOR-
MOR-

Corpus-based Semantic Embeddings

Corpus-based semantic embeddings exploit statistical properties of a text to em-
bed words in vectorial space. We employ two diﬀerent approaches to generating
corpus-based semantic embeddings:

• For each content word we obtained vector representations from a model
that was pre-trained on Twitter data (glove.twitter.27B)2. Using word
embeddings allows for detecting semantic similarities between words based
on their distributional properties in large corpora, which could boost the
performance of a user proﬁling model.

1https://pypi.org/project/language-tool-python/
2https://nlp.stanford.edu/projects/glove

• We also included Latent Semantic Analysis (LSA) [10] to enable a compar-
ison with the results described in [3]. LSA is an unsupervised Information
Retrieval technique for extracting and representing the contextual-usage
meaning of words in a collection of documents.

Machine Learning

For the purpose of this study, a range of diﬀerent machine learning methods were
explored. More speciﬁcally, for classiﬁcation, we examined the performance of
the following algorithms3.

• Support Vector Classiﬁcation. Because SVMs have demonstrated robust-
ness to high-dimensional data and imbalanced text mining problems (e.g.,
[12, 13]), two SVM implementations were included in the experiments: C-
Support Vector Classiﬁcation (with RBF kernel, c = 2048.0) (SVC) and a
linear SVM with Stochastic Gradient Descent (SGD)4 training. The ﬁrst
is standard in a text mining approach. The latter was included because of
its increasing popularity in “big data” ML applications.

• Na¨ıve Bayes (NB). The Na¨ıve Bayes algorithm is popularly used for tra-
ditional text classiﬁcation purposes and has shown tolerance to missing
values [15]. For classiﬁcation, a Multinomial NB (gender detection) and
a Complement Naive Bayes algorithm (age identiﬁcation) were used, be-
cause the algorithms outperformed the Gaussian and Bernouilli NB algo-
rithms during the preliminary experiments. The Complement Naive Bayes
classiﬁer was designed to correct the “severe assumptions” made by the
standard Multinomial Naive Bayes classiﬁer and was found particularly
suited for imbalanced data sets [14].

• k-Nearest Neighbor (k-NN). The k-Nearest Neighbor algorithm was sug-
gested by [8] for performing authorship attribution “in the wild” and has
demonstrated robustness towards overﬁtting [15]. The k parameter was
experimentally determined for each training session. Neighbor weights
were assigned proportionally to the inverse distance from each test in-
stance.

3The NN model was trained using the Keras [11] interface for deep learning. Keras serves
as a high-level API for TensorFlow (https://www.tensorﬂow.org). The other classiﬁers were
trained using the Scikit-learn [10] machine learning package.

4When applying SGD training, the gradient of the loss is estimated per instance and the

model is updated along the way with the learning rate [14].

• Random Forest (RF). As the Random Forest algorithm has shown toler-
ance to missing values and irrelevant attributes in prior text categorisation
research [15], which could be relevant to the task at hand, it was included
in the experiments. The number of trees in the forest is set to 10; the
Gini impurity function [16] was used to split a decision tree.

• Ridge Classiﬁer (RC). This algorithm treats a problem as a regression task
and is shown to be signiﬁcantly faster than, for example logistic regression
algorithms, when analysing a high number of classes [14].

• Passive Aggressive Classiﬁer (PAC). The algorithm is “passive” whenever
the loss is zero. However, even a slightly positive loss “aggressively” forces
an update on the algorithm’s hypothesis [17]. We included this algorithm
in our experiments, because passive-aggressive algorithms are designed
for large-scale learning and they have shown robustness to handling sparse
datasets [14].

• Multi-Layer Perceptron (MLP). An MLP classiﬁer was trained using Back-
propagation. The log-loss function was optimised using stochastic gradient
descent as proposed in [18] (‘adam’), which has shown eﬃciency towards
large datasets [14]. Also, MLPs have been used to derive robust features
for e.g., speaker recognition [19].

• Long Short Term Memory Recurrent Neural Network (NN). LSTM is an
artiﬁcial recurrent neural network (RNN) architecture used in the ﬁeld
of deep learning. The authors of [20] have shown the superiority of this
method over other algorithms for short text sentiment classiﬁcation across
diﬀerent platforms.

The parameters reported above were optimised during the preliminary exper-

iments on a validation sample of the training data.

Post-level vs. User-level Experiments

To enable a valid comparison between the aggregated message-level and the
user-level approach, we extracted all users who had produced at least ﬁfteen
posts from the PAN Corpus and created two datasets for our experiments: the
post level and the user level datasets, in which the document representations
per user contain 1 and all messages, respectively. None of the postings were
discarded, only regrouped.

The post level dataset was randomly divided into a 50% training set, a 30%
aggregation set and a 20% test set. The user level dataset was split into a

80% training and a 20% test set. Both test sets contained the same users and
messages. During the splitting, the messages were clustered so that no user was
present in two diﬀerent sets. Distributing users rather than messages ensured
that no user in training also appeared in the aggregation or test sets, which
prevented overﬁtting of user-speciﬁc features.

Evaluation of the classiﬁcation system proceeded as follows:

1. To aggregate the predictions on the message level to the user level, the
classiﬁer produced labels for the aggregation set. The aggregation method
was developed using an ensemble classiﬁer on this output, and performance
was established through ﬁve-fold cross-validation on the aggregation set.

2. For ﬁnal validation of the message-level approach, individual classiﬁers
were trained on the training set, produced labels for the aggregation set
and the test set; the ensemble model was trained on the aggregation set,
and its predictions taken for the test set.

3. User-level classiﬁers were trained and evaluated through ten-fold cross-
validation within the user level training set. The best performing classiﬁer
was then used to produce predictions for the test set, which contained
data identical to the test set of the message-level experiments.

The Message-based Approach

Social network messages diﬀer from other online text genres, such as e-mails or
blogs, in several aspects. The length of each instance is usually much shorter,
their vocabulary and grammatical structure are often non-standard and the dis-
tribution of lengths is very similar: the average post length in the Pan dataset
is 11.1 tokens, with on average 97.2 posts per user. In this section, we examine
the behaviour of a text mining approach to automatically identify social me-
dia users’ age group and gender under the complex conditions of short social
network postings containing non-standard language use.

To evaluate diﬀerent aspects of methodological design, ﬁrst, we conducted
a series of experiments, in which we examined the performance of three feature
selection techniques (Document Frequency, Chi Square (χ2) and Mutual Infor-
mation) and four feature representation methods (tf-idf, binary, absolute and
l2-normalisation) on the message-level, using the Pan dataset in a ten-fold cross
validation set-up. When analysing the eﬀect of each aspect on the performance
of the model, the other factors were kept constant to allow for a valid compari-
son. Next, we used the best performing model to compare the performance of

lexical, character, syntactic, word embedding and chatspeak features using nine
diﬀerent machine learning techniques (SVC, SGD, NB, k-NN, RF, RC, PAC,
MPL and NN) for both tasks. The results for each task are described below.

Age Group Identiﬁcation

As can be seen in Table 1, the PAN dataset is highly skewed with regard
to the number of posts available for each age group. Hence, we calculated a
random baseline using a stratiﬁed random classiﬁcation strategy, which generates
random predictions while respecting the training set class distribution. For the
age group identiﬁcation task, this resulted in a random baseline f-score of 39.2%
for 18–24; 41.1 % for 25–34; 12.6% for 35–49; and 6.4% for 50–XX; and an
average micro f-score for this task of 34.4%.

With regard to feature selection, almost every machine learning approach
beneﬁted from reducing its dataset’s dimensionality to its 10,000 most discrim-
inative features using the Chi Square method. Additionally, our models tended
to yield slightly better results when using a binary representation method in our
preliminary experiments. Hence, these were used when setting up the full exper-
imental regime using diﬀerent feature types and machine learning algorithms.

Our Long Short Term Memory Recurrent Neural Network (NN) was trained
using the Glove Twitter word embeddings model (glove.twitter.27B)5. Hence,
we could only produce results using word embeddings. We included the Rectiﬁed
Linear Unit activation function (ReLu=32) and the Softmax activation function,
along with an LSTM layer containing 128 memory units. Because age group
identiﬁcation is a non-binary task, categorical crossentropy was used as the
loss function, and the Adam algorithm [18] as optimizer. Finally, the model
took 30 passes (epochs) over each training partition in the dataset for periodic
evaluation.

As can be seen in Table 3, the best overall performance of 59.5% (micro
f-score) signiﬁcantly outperforms the random baseline for age group identiﬁ-
cation and was achieved when training the Complement Naive Bayes classiﬁer
(NB) using character n-gram features. When analysing the results for each age
group individually, we found that character n-grams also outperformed the other
feature types for 3 out of 4 age groups, but in some cases diﬀerent algorithms
produced better results than the NB classiﬁer, resulting in a best f-score of
71.0% for 18–24 (NB), 33.1% for 35–49 (NB) and 26.3% for 50-XX (SGD).
Interestingly, we found that chatspeak features (i.e., non-standard language use
and paralinguistic features, see Section ) yielded the best results for the 25–34
age group (67.7%, NB). This is in line with the work of [21], who found a

5https://nlp.stanford.edu/projects/glove

slight increase in the chatspeak word probability between the ages of 29 and
33 in a (Flemish) Dutch dataset of online social media messages. However,
these ﬁndings diﬀer notably with previous spoken discourse studies describing
the Age Grading principle, which states that the usage of non-standard linguis-
tic varieties tends to peak during adolescence (15–17 years old), but as social
pressure increases and the use of standard language becomes more important,
(for example, for building a career or raising children), people are more inclined
to adapt to society’s norms. Hence, the use of standard (or prestige) forms was
found to peak between the ages of 30 and 55 [22]. This could be linked to
the fact that this age group entails the ﬁrst generation that acquired “the art”
of online chatting. As a result, this could indicate that the use of chatspeak
in social media communications is not only attractive for adolescents, but also
— to some extent — for people that are currently in their early thirties, be-
cause it distinguishes them from the older age groups that did not learn to chat
during adolescence. However, this hypothesis requires further research to be
conﬁrmed. We provide an overview of the f-scores per age group and machine
learning method in Table .

Table 3: Micro F-scores (%) per feature type and machine learning algorithm
for age group detection.

Features
BOW
Content Words
bigrams
char n-grams
Chatspeak Feats.
Word Embeddings
LSA

SVC KNN
40.6
54.3
39.4
53.9
28.2
41.6
45.9
56.5
44.6
43.4
52.8
54.0
47.3
50.1

RF
53.6
53.5
48.8
55.7
46.4
54.3
50.1

NB
56.9
56.6
50.3
59.5
40.6
/
/

RC
56.6
56.1
48.4
53.7
43.6
52.6
48.6

SGD PAC MPL
54.7
53.6
57.8
53.5
53.3
53.7
50.2
46.9
47.6
58.9
56.7
58.3
45.8
39.6
43.6
53.8
48.8
53.1
51.4
44.4
48.6

DL
/
/
/
/
/
48.7
/

Gender Detection

We performed a similar series of experiments to investigate the eﬀect of
methodological design on a text mining approach when distinguishing between
male and female users in the PAN dataset, which is almost balanced with regard
to the number of messages per gender category. Hence, for the gender detection
task, we calculated the following random baseline scores: an F-score of 49.0%
for male and 50.9% for female; and, as can be expected, a micro F-score of
50.0% for this task.

Our deep learning architecture (NN) was again trained using the Glove Twit-
ter word embeddings model (glove.twitter.27B) 6. We also included a 1D con-
volution layer with Rectiﬁed Linear Unit activation function (with 32 ﬁlters and
kernel size = 3) and a layer with the Sigmoid activation function. Because we
approached gender identiﬁcation as a binary task, binary cross-entropy was used

6http://www.aclweb.org/anthology/D14-1162

Table 4: F-scores (%) per age group and machine learning model on the post
level.
Age
Cat.

SGD PAC MPL

Features

KNN

SVC

NB

RC

DL

RF

18–24

25–34

35–49

50–XX

BoW
Content Words
Bigrams
Char. n-grams
Chatspeak Feats.
Word embeddings
LSA
BoW
Content Words
Bigrams
Char. n-grams
Chatspeak Feats.
Word embeddings
LSA
BoW
Content Words
Bigrams
Char. n-grams
Chatspeak Feats.
Word embeddings
LSA
BoW
Content Words
Bigrams
Char. n-grams
Chatspeak Feats.
Word embeddings
LSA

68.1
67.6
60.4
70.7
30.6
69.8
62.0
61.3
60.4
40.1
63.7
67.2
63.5
59.8
14.1
14.6
8.0
14.8
7.9
2.1
6.0
6.2
7.8
5.1
7.8
0.3
1.7
3.5

56.3
56.1
57.2
56.9
47.8
64.6
58.3
39.6
36.0
11.3
50.1
56.6
55.7
52.7
10.9
13.9
4.4
15.3
14.5
27.2
17.0
10.6
10.5
7.1
11.2
13.3
14.3
6.9

65.4
63.4
53.4
69.9
39.3
68.3
62.6
60.5
59.6
60.5
63.8
65.0
62.7
57.8
16.6
23.4
15.6
9.2
13.6
9.7
9.0
12.3
15.4
11.7
9.8
12.9
4.9
6.5

69.0
68.0
60.2
71.0
20.8
/
/
62.8
62.4
57.8
62.7
67.7
/
/
25.2
26.1
18.4
33.1
6.0
/
/
9.3
11.9
6.9
22.5
0.0
/
/

67.5
67.1
56.5
63.5
32.0
67.8
59.7
60.2
59.7
55.4
57.7
67.0
61.3
59.7
30.0
29.5
19.3
26.4
7.6
4.9
2.9
20.5
19.9
12.3
23.4
0.0
0.1
2.0

68.8
68.1
59.2
68.6
31.5
67.5
61.2
61.5
60.3
51.1
61.9
66.9
61.8
58.1
31.1
29.8
19.2
31.0
9.0
7.5
3.6
20.5
19.9
11.3
26.3
0.0
0.3
1.3

62.8
63.9
59.7
66.3
30.7
61.8
55.7
57.6
56.2
48.1
60.9
59.3
50.4
49.7
29.0
27.5
20.6
30.1
7.9
26.0
15.3
21.5
21.9
14.0
24.3
4.1
4.9
1.2

64.2
63.2
56.4
69.0
36.0
64.6
61.5
58.8
57.8
58.9
63.5
66.9
58.3
58.3
29.3
27.7
21.9
30.8
11.4
27.3
20.3
22.1
19.6
14.5
24.9
8.9
12.9
8.8

/
/
/
/
/
61.7
/
/
/
/
/
/
55.0
/
/
/
/
/
/
12.8
/
/
/
/
/
/
0.8
/

as the loss function, and the Adam algorithm [18] as optimizer. Finally, the
model also took 30 passes (epochs) over each training partition in the dataset
for periodic evaluation.

As can be seen in Table 5, the results from the gender classiﬁcation experi-
ments are similar to those of the age prediction models: the best micro f-score
on the post level of 63.8% again signiﬁcantly outperforms the random baseline
for gender detection was achieved when training the NB classiﬁer (Multinomial
Na¨ıve Bayes) using character n-gram features. When analysing the results for
the female and male categories individually, we found that BoW features out-
performed all other feature types when identifying females, which resulted in
a 66.9% f-score using NB. For the male class, the best result of 66.1% was
achieved when training the k-NN algorithm on bigram features. We provide an
overview of the f-scores per gender category and machine learning method in
Table .

Boosting Strategies

Based on the results of the systematic study of diﬀerent aspects of method-

Table 5: Micro F-scores (%) per feature type and machine learning algorithm
for gender detection.

Features
BoW
Content Words
Bigrams
Char n-grams
Word Embeddings
LSA

SVC KNN
53.3
61.7
54.7
61
40.2
53
55.5
63.2
60
62.1
54.5
56.9

RF
60
60.5
54
62.5
59.9
56.3

NB
62.4
62.1
56.1
63.8
/
/

RC
61.3
60.7
55.4
60.1
60.2
55.8

SGD PAC MPL
60.8
60.5
61.9
60.6
59
61.2
56.3
56.3
56
63.7
62.2
62.9
61.1
52.7
58.7
55.9
50.2
53.7

NN
/
/
/
/
60.4
/

Table 6: F-scores (%) per gender group and machine learning model on the post
level.

Gender

Male

Female

Feature Types
BoW
Content Words
Bigrams
Char. n-grams
Chatspeak Feats.
Word Embeddings
LSA
BoW
Content Words
Bigrams
Char. n-grams
Chatspeak Feats.
Word Embeddings
LSA

SVC
61.1
58.8
50.1
62.3
60.6
59.6
59.1
62.2
63.1
55.8
64.0
50.8
64.4
54.8

KNN
46.8
56.9
66.1
52.5
59.3
58.8
55.2
59.6
52.6
15.1
58.4
52.5
61.1
53.8

RF
56.1
57.3
46.3
60.2
59.9
58.2
53.0
63.7
63.6
61.5
64.7
53.5
61.6
59.5

NB
57.6
57.8
48.8
60.9
63.8
/
/
66.9
66.1
63.1
66.6
41.6
/
/

RC
59.1
58.0
52.8
59.4
61.6
58.3
56.9
63.4
63.4
57.9
60.8
50.9
62.2
54.8

SGD PAC MPL
59.2
60.5
60.4
59.3
57.0
58.2
50.9
57.6
56.7
62.8
61.8
62.4
60.3
45.0
61.3
61.1
53.5
55.6
56.9
40.3
57.0
62.4
60.5
63.3
61.9
61.0
64.1
61.6
55.1
55.2
64.6
62.6
63.4
52.1
58.9
48.2
61.0
51.8
61.7
55.0
59.8
50.4

NN
/
/
/
/
/
60.6
/
/
/
/
/
/
60.2
/

ological design presented above, we examined two diﬀerent strategies to boost
the performance for automatic user proﬁling using only a single message per
user: a feature union approach and a balancing strategy approach.

Because the experiments that were based on character n-gram features
yielded the best micro F-score, in the next step we combined them with other
single feature types. There was only one combination that was able to slightly
improve upon the original performance of the character n-gram features for all
age categories: when merging character n-grams with chatspeak features, the
NB classiﬁer achieved a micro f-score of 59.8%, a 70.8% f-score for 18–24,
62.3% for 25–34, 32.5% for 35–49, and 23.8% for 50–XX. Any other combina-
tions of single feature types (including threefold combinations) did not produce
better results.

Secondly, to create a good reﬂection of reality, up until this point, a highly
skewed data distribution was adopted during each age identiﬁcation experiment.
To investigate the eﬀect of data distributions on age group identiﬁcation, we
balanced the data in training while maintaining the original skewed data dis-
tribution in the test partitions. We found that balancing the dataset in each
training partition only, led to a considerably higher recall score for the minority
classes when predicting age group compared to the imbalanced data experiments

described above, but the precision decreased considerably, leading to a slightly
lower micro f-score. The results are shown in Figure 1.

Similar to the feature union experiments for age group detection, the best
performing single feature type for gender detection was merged with other types
to examine which combinations could boost the performance of the gender clas-
siﬁer. However, none of the combinations was able to outperform the character
n-gram model.

Figure 1: Results of the boosting strategies (% f-scores) for age group identiﬁ-
cation.

Aggregating Predictions to the User Level

For the ensemble model, we examined the performance of eight diﬀerent ma-
chine learning approaches (C-SVC, k-NN, RF, NB, RC, SGD, PAC and MPL),
using a 5-fold cross validation set-up on the aggregation data partition, to au-
tomatically aggregate the predictions on the post level to the user level.

As can be seen in Figure 2, the performance of the age group identiﬁcation
model increased signiﬁcantly for all learners. This time, the Random Forest (RF)
outperformed the other models for both age group identiﬁcation, yielding an
average micro f-score of 71.0% on the user level of the aggregation partitions.
For gender detection, training the Passive Aggressive Classiﬁer (PAC) on the
output labels rendered for the aggregation set produced a best micro f-score of
73.2%.

Finally, the best performing post level age identiﬁcation model (character
n-grams + chatspeak features) was retrained on the training and aggregation
partitions using the NB classiﬁer, rendering predictions on the post level for each

Figure 2: Results of the ensemble learners (% micro f-scores) for age group and
gender identiﬁcation on the aggregation partitions.

message in the test set. Next, the RF aggregation classiﬁer was trained on the
predictions in the aggregation partition and provided a ﬁnal decision on the user
level of each user in the test set.
Interestingly, the performance did not drop,
resulting in a ﬁnal 73.2% micro f-score and an accuracy score of 76.7%,
an f-score of 88.9% for 18–24; 74.5% for 25–34; 37.5% for 35–49 and
100.0% for 50–XX on the user level.

Similarly, we used the best gender detection model (character n-grams, NB)
to produce labels on the aggregation and test partitions on the post level and
trained the PAC classiﬁer on the aggregation partition. This resulted in a
ﬁnal accuracy score of 86.7% and a micro f-score of 86.8% for gender
detection, and a ﬁnal 87.5% f-score for the female class and a 85.7%
for the male class.

In the next section we compare our results to a user-based approach, which

renders predictions directly on the user level.

The User-based Approach

For each experiment described in this section, we collected, preprocessed and
represented all messages from the same user in a single instance vector. This
way, the user-based system directly labels users and no further aggregation steps
are required. To enable a valid comparison with the results of the message-
level experiments, we ﬁrst performed 10-fold cross validation within the training

partition and evaluated the best performing model on exactly the same test data
as we used in the aggregated message-level experiments.

As can be seen in Figure 3, for age group identiﬁcation, the best micro f-
score on the user level of 73.2% was achieved when training a k-NN classiﬁer
(k = 3) on word embeddings. Because no other feature combinations produced
better results, this model was used to produce a ﬁnal decision on the user level
of each user in the test partition, which resulted in a micro f-score of 72.0%,
an accuracy score of 73.3%, an f-score of 76.6% for 18–24; 76.6% for
25–34; 52.6% for 35–49 and 85.7% for 50–XX on the user level.

Figure 3: Average micro f-scores (%) per age group and machine learning model
on the user level in training.

Figure 4 shows that, with regard to gender detection, the NB classiﬁer trained
on BoW features outperformed all other feature types and learners in our user
level experiments. Hence, this model was used to produce labels for each user in
the test partition. This resulted in a micro f-score of 76.5%, an accuracy
of 68.3%, an f-score of 74.6% for the female class and 79.2% for the
male category.

In the next section, we set up a cross domain experiment, in which we eval-
uate the performance of both approaches when applied on two newly collected

Figure 4: Average micro f-scores (%) per gender group and machine learning
model on the user level in training.

clearweb datasets. Finally, the best performing models are applied to the DNM
dataset, which contains Darknet Market conversations between cyber oﬀenders,
in Section .

Cross Domain User Proﬁling

Datasets

To enable an estimation of the performance of our user proﬁling models across
diﬀerent online domains, we have developed scraping techniques to collect new
data from clearweb forums that (i) include user demographics (e.g., Goodreads)7,
and (ii) are estimated to relate more closely to underground forums in terms of
user population than the traditional social media benchmark datasets used in
related work (Pornhub8). The general user demographics for these new clearweb

7https://www.goodreads.com/
8https://www.pornhub.com/

forums are presented in Table 7. We display the top-5 age groups for Goodreads
and Pornhub in Table 8. In addition, the results for top 5 countries are shown
in Table 9.

Community

Total users

100,061

67%

1%

Male

32%

Gender
Female Others

Messages
(avg.)

199,499

76%

14%

10%

26

9

Goodreads
(GR)
Pornhub
(PH)

Table 7: General user demographic per clearweb forum

Top 5 age groups

[20, 30)

[30, 40)

[40, 50)

[50, 60)

GR
PH

24.8%
44.2%

23.4%
30%

22.2%
12%

11.9%
6%

[60, 70) in GR
<= 20 in PH
5.8%
4.4%

Table 8: Top 5 age groups for Goodreads and Pornhub

Top-1

Top-2

Top-3

Top-4

Top-5

GR
U.S. (59%)
PH U.S. (52.3%)

U.K. (6.8%)
U.K.(7.3%)

Canada (4%)
Canada (4.7%)

India (2.7%)
Germany (3.3%)

Italy (2.6%)
France (2.5%)

Table 9: Top 5 countries per clearweb forum

Experiments and Results

For these experiments, we treated the additional datasets like the test partition
of the PAN dataset as described earlier, performing the same preprocessing,
feature extraction and machine learning techniques.

As was shown in the previous section, the two additional datasets used for
these experiments show a highly divergent data gender distribution compared to
the PAN dataset, which was almost completely balanced. With regard to age
group distribution, the PH dataset shows a similar distribution over our previ-
ously used categories, while the GR dataset shows a more balanced distribution
over the 3 youngest groups. As these additional datasets were collected from
platforms covering highly divergent topics compared to the content discussed in
the PAN dataset, it was expected that the performance would decrease. Hence,
our key research question was whether our models could still outperform the
random baselines when applied in a cross domain set-up.

As can be seen in Table , the performance of our message-based and user
level models for age group identiﬁcation dropped signiﬁcantly when applied to
both the PH and the GR datasets, resulting in random-like behaviour. This can

be explained by the models overﬁtting on topic-speciﬁc age-related diﬀerences in
language use expressed in the PAN dataset, but also potentially by the diﬀerence
in time of collection, especially given the speed in which new linguistic varieties
emerge in online communities. These ﬁndings highlight the need of platform-
speciﬁc ground truth data for such analysis.

Contrary to our age group identiﬁcation experiments, the gender detection
models showed a higher robustness within our cross domain set-up (see Table
).
Interestingly, the user level approach yielded a considerably higher f-score
(69.0%) than the post level classiﬁer (56.4%) when applied to the PH dataset,
but the post level approach outperformed the user level classiﬁer on the GR
dataset (72.2% vs. 62.1%). Although this performance might not be suﬃ-
ciently accurate to label a DNM user as either male or female in the context of
a cybercrime investigation, the gender models could allow for a more systematic
estimation of the gender distributions in Darknet Markets than the self-reporting
surveys described in prior work. In the next section, we discuss our ﬁnal quan-
titative analysis of the DNM dataset.

Table 10: Micro f-scores and Accuracy for age group identiﬁcation using the
post level approach (PL) and the user level approach (UL).

Dataset

Pornhub
Goodreads

PL Class.

UL Class.

Micro F Acc. Micro F Acc.
28.4
25.7
26.0
23.1

29.2
25.3

21.6
25.1

Table 11: Micro f-scores and Accuracy for gender detection using the post level
approach (PL) and the user level approach (UL).

PL Class.

UL Class.

Micro F Acc. Micro F Acc.
65.0
56.4
62.0
72.2

49.7
72.3

69.0
62.1

Dataset

Pornhub
Goodreads

DNM Analysis

Dataset

For this analysis, we make use of over 2.5 million posts drawn from over 150,000
users from 35 cybercriminal communities, drawn from the DNM Corpus: a large

dataset collected between 2013 and 2015 [23]. In particular, we targeted discus-
sion fora within this collection, which acted as support areas for underground
marketplaces dealing in a number of diﬀerent illicit goods. Table 12 gives a
breakdown of the data available for each community. Communities ranged from
successfully established markets with thousands of users (though not all were
always active posters) to small sites that never moved beyond a handful of initial
users.

Table 12: Breakdown of the communities targeted for proﬁling in our case study

Community
Silk Road 2
Silk Road
Evolution
Abraxas
Agora
BlackMarketReloaded
Nucleus
TheHub
Pandora
BlackBank
TheMajesticGarden
Utopia
Diabolus
Kingdom
ProjectBlackFlag
CannabisRoad2
CannabisRoad3
Bungee54
Panacea
TorBazaar
TheRealDeal
Hydra
Kiss
Andromeda
OutlawMarket
Revolver
TorEscrow
DarkBay
Dogeroad
DarknetHeroes
Havana
Tom
GreyRoad
Tortuga
MrNiceGuy

Posts
882,418
846,077
509,225
276,300
84,914
80,467
65,175
58,642
49,023
32,817
26,121
14,458
11,456
10,285
6,131
5,842
4,905
3,325
2,241
2,205
1,049
937
933
894
689
660
490
332
300
190
181
144
43
37
25

Users
26,163
52,383
33,743
1,607
6,153
7,006
9,478
7,337
8,729
2,381
1,858
4,392
2,151
856
330
2,139
1,903
1,510
520
902
115
276
145
1,601
2,007
85
294
484
118
793
77
4,120
24
7
6

The raw data provided in [23] captures fora as scraped at several semi-regular
intervals by the dataset authors. This leads to heavy redundancy within the data,
as threads may be captured at multiple times. However, this redundancy is also
useful, as it helps to guard against intermittent faults in the crawling process.
Our approach to parsing the data takes a latest-version-ﬁrst view – of all pages
captured within the crawling process, we treat as canonical the most recent
version, only parsing older pages where they were not captured in later scrapes.
We note that capturing pages from older scrapes is an important step in handling

this data, as many thousands of threads and user proﬁle pages are not present
at all in the most recent scrapes of each forum. Diﬀerences could be attributed
to crawling failures in later scrapes, incomplete coverage as part of the crawling
processes, or to administrator action in taking down or hiding discussion threads
over time.

Parsing of the data proceeded in two stages within the scrape history of
each community. First, user proﬁle pages were processed to build up a dataset
of users and associated information from their proﬁle pages (e.g., PGP public
keys, membership status). Next, discussion thread pages were parsed in order to
associate posts (including textual content and metadata such as posting time,
subforum, etc.) with the user that authored them. Where quotations of other
users could be identiﬁed within the text of a user’s post, these quotations were
separated from the authored text, to avoid contamination of proﬁling analysis.
It sometimes occurred that user proﬁle pages were not captured in the scrapes
due to sites protecting access to those pages, or where users were observed
posting for whom no proﬁle page had been seen (either due to people using
guest accounts, or due to incomplete coverage of proﬁle pages in the crawls).
In these cases, new user entries were created on the ﬂy during the second stage
of parsing, using such metadata as was available about the author account from
the post metadata.

Experiments and Results

Based on our results described in the previous section, we applied the post level
age group identiﬁcation model and the user level gender detection model on
each DNM forum. All users that produced at least 1 message were included in
the analysis.

The results of our analysis are shown in Table 13. With regard to age
group identiﬁcation (keeping the caveat in mind that we did not have access to
any ground truth data about DNM user demographics and our models showed
a considerable drop in performance for this task when applied across diﬀerent
online domains), our results seem to conﬁrm prior work in this area and are in line
with the ﬁndings of our qualitative analysis: the majority of users were labelled as
being between 18 and 24 (58.9%), with the second largest group of users labelled
as 25 to 34 years old (21.1%). However, the results for gender detection diﬀer
remarkably from previous studies relying on self-reporting interviews with cyber
oﬀenders, suggesting that women may be more engaged in Darknet Markets
than traditionally considered. However, this hypothesis requires further research
that includes veriﬁcation of our models against writing prints of arrested cyber
oﬀenders to be conﬁrmed.

Despite the challenges of the Natural Language Processing experiments de-
scribed in this Appendix, combining all aspects of the work described previously
with the ﬁndings from this study did enable a novel perspective on cybercriminal
motivations and characteristics, resulting in a useful framework for developing
appropriate socio-technical interventions (e.g., diverting early stage oﬀenders to
more positive outlets for their skills). To conclude our work, Figure 5 provides an
overview of the characteristics and motivations highlighted by the literature and
contrasts these with the characteristics emerging from our qualitative analysis of
contemporary experiences and perspectives of practitioners working within the
cybercrime ﬁeld9, and the results from the quantitative, text mining analyses
presented in this study.

Figure 5: Contrasting characteristics identiﬁed through the three sources

Conclusion

To extend our qualitative analysis discussed above, we reviewed a range of Nat-
ural Language Processing and Text Categorisation techniques that could be
deployed on online communications in Darknet marketplaces. Our aim was to
remove the observer-observed eﬀect, which would allow us to explore cybercrim-
inal interactions without having to rely on the self-reporting of individual users.
While recent advances in these ﬁelds have shown promising results with regard to
detecting demographic characteristics, such as age group and gender, in several

9Based on quantitative and qualitative analysis of results from a survey of 16 practitioners

text genres by automatically analysing the variation of an author’s linguistic fea-
tures, applying such techniques on underground cybercriminal communications
diﬀers from more general applications in that its deﬁning characteristics are both
domain and process dependent. This gives rise to a number of challenges of
which contemporary research has only scratched the surface. More speciﬁcally, a
text mining approach applied on online communications typically has no control
over the dataset size – the number of available communications will vary across
users. Hence, an automated system has to be robust towards limited data avail-
ability. Additionally, the quality of the data cannot be guaranteed. As a result,
the approach needs to be tolerant to a certain degree of linguistic noise (for ex-
ample, abbreviations, non-standard language use, spelling variations and errors,
non-standard use of punctuation). Finally, in the context of cybercriminal fora,
it has to be robust towards deceptive or adversarial behaviour, i.e. oﬀenders
who attempt to hide their identity and criminal intentions (obfuscation) or who
assume a false digital persona (imitation), potentially using coded language.

Despite the challenging characteristics of this text genre for natural language
processing, the present study showed that it is feasible to improve upon random
baseline performance for both age group and gender classiﬁcation when training
on highly sparse, skewed datasets of on average 11.1 tokens per message, which
also contain linguistically noisy text samples. Moreover, our message-based
approach, in which we aggregate predictions on the post level to the user level
using an ensemble learning method, outperformed the traditional user-based
approach in which predictions are rendered directly on the user level for both
age group identiﬁcation and gender detection. This seems to be in line with our
hypothesis that users’ “writing prints” might unwittingly ﬂow through in parts
of their communications and that such clues can be identiﬁed more accurately
when training on smaller text samples. These ﬁndings also seem to support and
the human stylome hypothesis previously described.

Applying our models across diﬀerent online social media datasets (with a
divergent data distribution over the diﬀerent categories to be detected and
completely divergent topics discussed), resulted in a considerable drop of the
performance for age group identiﬁcation, highlighting the need for up-to-date
and domain-speciﬁc ground truth data to maintain a useful performance for this
diﬃcult, non binary, highly imbalanced classiﬁcation task.

However, with regard to gender detection, our models were still able to
signiﬁcantly outperform the random baseline performance for both additional
online platforms. Although this performance would not be suﬃciently accurate
to label a DNM user as either male or female with great conﬁdence in a cy-
bercrime investigation, the model does allow for a more systematic estimation
of the gender distributions in Darknet Markets than the self-reporting surveys

described in prior work.

Based on our ﬁndings, we performed a ﬁnal analysis of the key DNM dataset
available to the AMoC team. Keeping the caveat in mind that we did not have
access to any ground truth data about DNM user demographics and our models
showed a considerable drop in performance for age group identiﬁcation when
applied across diﬀerent online domains, our results seemed to conﬁrm previous
ﬁndings in literature and our qualitative analysis: the majority of users were
labelled as being between 18 and 24 (58.9%), with the second largest group of
users labelled as 25 to 34 years old (21.1%). However, the results for gender
detection diﬀered remarkably from previous studies relying on self-reporting in-
terviews with cyber oﬀenders, which could suggest that women may be more
engaged in Darknet Markets than traditionally considered. This ﬁnding is re-
ﬂected in recent work by [24] on gendering research on online illegal drug mar-
kets, who criticise the assumptions that illegal drug selling is essentially a male
dominated activity and that the peripheral role of women in illegal drug selling
is likely to be reproduced online. Instead, they argue that the relative anonymity
aﬀorded by Darknet Markets might hold a particular appeal for women, both as
buyers and vendors of illegal drugs: selling drugs might be perceived as a way
for women to achieve ﬁnancial stability, independence or a sense of empower-
ment. Furthermore, the authors of [25] found that women represented up to a
third of online lifestyle drug purchasers (e.g., weight loss drugs, painkillers and
sedatives). Hence, such drug advertisements could be targeting women both on
clearnet and in Darknet Markets. However, these hypotheses require further re-
search that includes veriﬁcation of our models against writing prints of arrested
cyber oﬀenders to be conﬁrmed.

Table 13: Results for the DNM dataset per age and gender group based on the
age group and gender detection models (no ground truth available).

Forum
abraxas
agora
andromeda
blackbank
bmr
bungee54
cannabisroad2
cannabisroad3
crackingarena
crackingﬁre
darkbay
darknetheroes
diabolus
dogeroad
evolution
greyroad
hackhound
havana
hydra
kingdom
kiss
mrniceguy
nucleus
outlawmarket
panacea
pandora
pbf
revolver
silkroad
silkroad2
thehub
themajesticgarden
therealdeal
tom
torbazaar
torescrow
tortuga
utopia
webkill
Total (%)

18-24
282
3,277
118
1,536
3,917
240
941
936
6,063
6,790
29
21
358
33
13,178
7
350
11
125
304
56
2
2,694
84
92
2,803
167
42
25,461
14,973
2,230
1,079
47
20
102
47
8
733
769
58.9

25-34 35-49 50-XX Female Male
349
21
3,215
295
157
12
1,129
102
3,935
372
179
16
675
71
671
78
5,199
2,317
7,642
2,191
34
1
1
26
380
27
28
4
11,637
1,304
0
6
466
65
5
18
157
17
302
36
70
8
0
5
2,416
270
95
11
79
7
2,908
243
192
25
34
6
20,084
1,839
12,892
963
2,552
222
967
85
51
6
1
23
109
10
65
3
1
4
690
75
630
46
52.4
7.0

232
2,933
113
1,250
3,069
191
852
860
6,759
6,839
37
20
228
31
10,301
6
325
18
119
220
50
1
2,113
91
86
2,252
127
46
19,194
10,232
1,765
868
55
28
87
26
8
564
715
47.6

204
1,885
101
497
1,956
66
344
353
1,269
2,112
23
15
159
15
4,647
3
230
14
92
108
36
2
1,075
68
29
1,659
81
20
7,437
5,025
1,369
489
36
20
55
28
2
303
372
21.1

74
691
39
244
759
48
171
164
2,309
3,388
18
9
64
7
2,809
2
146
6
42
74
20
2
490
23
37
455
46
12
4,541
2,163
496
182
17
10
29
13
1
143
158
13.0

Bibliography

[1] M. Brennan, S. Afroz, and R. Greenstadt, “Adversarial stylometry: Circum-
venting authorship recognition to preserve privacy and anonymity,” ACM
Transactions on Information and System Security (TISSEC), vol. 15, no. 3,
p. 12, 2012.

[2] M. R. Brennan and R. Greenstadt, “Practical attacks against authorship

recognition techniques.” in IAAI, 2009.

[3] F. Rangel, P. Rosso, M. Potthast, B. Stein, and W. Daelemans, “Overview
sn, 2015, p.

of the 3rd author proﬁling task at PAN 2015,” in CLEF.
2015.

[4] F. Rangel, P. Rosso, M. Koppel, E. Stamatatos, and G. Inches, “Overview
of the author proﬁling task at PAN 2013,” in CLEF Conference on Mul-
tilingual and Multimodal Information Access Evaluation. CELCT, 2013,
pp. 352–365.

[5] F. Rangel, P. Rosso, I. Chugur, M. Potthast, M. Trenkmann, B. Stein,
B. Verhoeven, and W. Daelemans, “Overview of the 2nd author proﬁling
task at PAN 2014,” in CLEF 2014 Evaluation Labs and Workshop Working
Notes Papers, Sheﬃeld, UK, 2014, 2014, pp. 1–30.

[6] R. Sarawgi, K. Gajulapalli, and Y. Choi, “Gender attribution: tracing stylo-
metric evidence beyond topic and genre,” in Proceedings of the Fifteenth
Conference on Computational Natural Language Learning. Association for
Computational Linguistics, 2011, pp. 78–86.

[7] C. Peersman, “Detecting deceptive behaviour in the wild: text mining for
online child protection in the presence of noisy and adversarial social media
communications,” Ph.D. dissertation, Lancaster University, 2018.

[8] K.

Luyckx,

Scalability

issues

in

authorship

attribution.

ASP/VUBPRESS/UPA, 2011.

[9] D. Crystal, “Language and the Internet,” Cambridge, CUP, 2001.

24

[10] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al., “Scikit-learn:
Machine learning in python,” the Journal of machine Learning research,
vol. 12, pp. 2825–2830, 2011.

[11] F. Chollet et al., “Keras,” https://github.com/fchollet/keras, 2015.

[12] F. Sebastiani, “Machine learning in automated text categorization,” ACM

computing surveys (CSUR), vol. 34, no. 1, pp. 1–47, 2002.

[13] C. Seiﬀert, T. M. Khoshgoftaar, J. Van Hulse, and A. Folleco, “An empirical
study of the classiﬁcation performance of learners on imbalanced and noisy
software quality data,” Information Sciences, vol. 259, pp. 571–595, 2014.

[14] scikit-learn, “Scikit-learn user guide,” https://scikit-learn.org/stable/.

[15] S. B. Kotsiantis, I. Zaharakis, and P. Pintelas, “Supervised machine learn-
ing: A review of classiﬁcation techniques,” Emerging artiﬁcial intelligence
applications in computer engineering, vol. 160, pp. 3–24, 2007.

[16] F. Breiman, “Olshen, and stone,” Classiﬁcation and Regression trees, 1984.

[17] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer, “On-

line passive aggressive algorithms,” 2006.

[18] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[19] L. P. Heck, Y. Konig, M. K. S¨onmez, and M. Weintraub, “Robustness to
telephone handset distortion in speaker recognition by discriminative feature
design,” Speech Communication, vol. 31, no. 2-3, pp. 181–192, 2000.

[20] J. Nowak, A. Taspinar, and R. Scherer, “Lstm recurrent neural networks
for short text and sentiment classiﬁcation,” in International Conference on
Artiﬁcial Intelligence and Soft Computing. Springer, 2017, pp. 553–562.

[21] C. Peersman, W. Daelemans, R. Vandekerckhove, B. Vandekerckhove,
and L. Van Vaerenbergh, “The eﬀects of age, gender and region on
non-standard linguistic variation in online social networks,” arXiv preprint
arXiv:1601.02431, 2016.

[22] S. Tagliamonte, Variationist sociolinguistics: Change, observation, inter-

pretation.

John Wiley & Sons, 2012, vol. 40.

[23] G. Branwen, N. Christin, D. D´ecary-H´etu, R. M. Andersen, StExo,
E. Presidente, Anonymous, D. Lau, D. K. Sohhlz, V. Cakic, V. Buskirk,
Whom, M. McKenna, and S. Goode, “Dark net market archives,
2011-2015,” https://www.gwern.net/DNM-archives, July 2015, accessed:
209-08-01. [Online]. Available: https://www.gwern.net/DNM-archives

[24] J. Fleetwood, J. Aldridge, and C. Chatwin, “Gendering research on online
illegal drug markets,” Addiction Research & Theory, vol. 28, no. 6, pp.
457–466, 2020.

[25] R. Koenraadt and K. van de Ven, “The internet and lifestyle drugs: an
analysis of demographic characteristics, methods, and motives of online
purchasers of illicit lifestyle drugs in the netherlands,” Drugs: Education,
Prevention and Policy, vol. 25, no. 4, pp. 345–355, 2018.

