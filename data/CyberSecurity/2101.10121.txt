Game-Theoretic and Machine Learning-based
Approaches for Defensive Deception: A Survey

Mu Zhu, Ahmed H. Anwar, Zelin Wan, Jin-Hee Cho, Charles Kamhoua, and Munindar P. Singh

1

1
2
0
2

y
a
M
8

]

R
C
.
s
c
[

2
v
1
2
1
0
1
.
1
0
1
2
:
v
i
X
r
a

Abstract—Defensive deception is a promising approach for cy-
ber defense. Via defensive deception, the defender can anticipate
attacker actions; it can mislead or lure attacker, or hide real
resources. Although defensive deception is increasingly popular
in the research community, there has not been a systematic
investigation of its key components, the underlying principles,
and its tradeoffs in various problem settings. This survey paper
focuses on defensive deception research centered on game theory
and machine learning, since these are prominent families of
artiﬁcial intelligence approaches that are widely employed in
defensive deception. This paper brings forth insights, lessons, and
limitations from prior work. It closes with an outline of some
research directions to tackle major gaps in current defensive
deception research.

Index Terms—Defensive deception, machine learning, game

theory

A. Motivation

I. INTRODUCTION

Conventional security mechanisms, such as access controls
and intrusion detection, help deal with outside and inside
threats but inadequately resist attackers subverting controls or
posing new attacks. Deception is a distinct line of defense
aiming to thwart potential attackers. The key idea of deception
is to manipulate an attacker’s beliefs to mislead their decision
making, inducing them to act suboptimally. Since the beneﬁts
of leveraging the core ideas of defensive deception have
been realized in the cybersecurity research community, there
have been non-trivial efforts to develop intelligent defensive
deception techniques.

Two main promising directions to develop defensive decep-
tion techniques are observed in the literature. First, strategies
of an attacker and defender have been commonly modeled
based on game-theoretic approaches where the defender takes
defensive deception strategies with the aim of creating con-
fusion for attackers or misleading them to choose less opti-
mal or poor strategies. Second, machine learning (ML)-based
defensive deception techniques have been proposed to create
decoy objects or fake information that mimic real objects or
information to mislead or lure attackers.

The synergistic merit of combining GT and ML has been
recognized in the cybersecurity literature [1], such as using
game theoretic defenses against adversarial machine learning

Zelin Wan and Jin-Hee Cho are with the Department of Computer Science,
Virginia Tech, Falls Church, VA, USA. Email: {zelin, jicho}@vt.edu. Email:
abdullahzubair@vt.edu. Mu Zhu and Munindar P. Singh are with the De-
partment of Computer Science, North Carolina State University, Raleigh, NC
27695. Email: {mzhu5, mpsingh}@ncsu.edu. Ahmed H. Anwar and Charles
A. Kamhoua are with the US Army Research Laboratory, Adelphi, MD, USA.
Email: a.h.anwar@knights.ucf.edu; charles.a.kamhoua.civ@mail.mil.

attacks [2, 3] or generative adversarial models for creating
deceptive objects [1]. However, little work has explored the
synergies between GT and ML to formulate various cyberse-
curity problems. In particular, since players’ effective learning
of their opponents’ behavior is critical to the accuracy of
their beliefs of the opponents’ types or next moves, using
ML for the players to form their beliefs can contribute to
generating optimal plays under a certain environment. In ad-
dition, when developing defensive deception techniques, ML-
based approaches can provide better prediction of attackers
or high similarity in creating deceptive object based on a
large volume of data available. However, they may not provide
effective strategic solutions under uncertainty which has been
well explored in game theoretic approaches. Therefore, this
survey paper was motivated to facilitate future research taking
hybrid defensive deception approaches that can leverage both
GT and ML.

In order to distinguish the key contributions of our paper
compared to the existing survey papers, we discuss the existing
survey papers on defensive deception techniques and clarify
the differences between our paper and them in the following
section.

B. Comparison with Existing Surveys

Several studies have conducted surveys of defensive decep-

tion techniques [4, 5, 6, 7, 8].

Almeshekah and Spafford [8] presented how a defensive
deception has been considered in the cyber security defense
domain. To be speciﬁc, the authors discussed the following
three phrases in considering a defensive deception technique:
planning, implementing and integrating, and monitoring and
evaluating. In particular,
this paper discussed the models
of planning deceptions in terms of affecting an attacker’s
perception which can be misled for a defender to achieve
a system’s security goals. However,
this survey paper is
limited its contribution to modeling and integrating defensive
deception to a limited set of attackers. In addition, this work
did not consider a variety of network environments which
should be considered in implementing defensive deception
techniques.

Rowe and Rrushi [7] classiﬁed defensive deception tech-
niques in terms of impersonation, delays, fakes, camouﬂage,
false excuses, and social engineering. They not only introduced
the background on deception technologies but also explored
the calculation of detectability and effectiveness of defensive
deception. However, their survey of game-theoretic defensive
deception is limited and lacks discussion of the state-of-the-art
techniques.

 
 
 
 
 
 
TABLE I
COMPARISON OF OUR SURVEY PAPER WITH THE EXISTING SURVEYS OF DEFENSIVE DECEPTION

Key Criteria

Our Survey
(2020)

Lu et al. [4]
(2020)

Pawlick et al.
[5] (2019)

Han et al. [6]
(2018)

Provides concepts
Provides key taxonomies
Includes ML approaches
Includes game-theoretic approaches
Describes attack types countermeasured
Describes metrics
Describes evaluation testbeds
Is not limited to speciﬁc application domains
Discusses pros and cons of relevant techniques
Discusses insights, lessons, and limitations
Discusses future research directions

✔
✔
✔
✔
✔
✔
✔
✔
✔
✔
✔

✔
✔
✘
▲
✘
✘
✘
▲
▲
▲
✔

✔
✔
✘
✔
▲
▲
✘
✘
▲
▲
✔

✔
✔
✘
▲
▲
▲
▲
✔
✔
✔
✔

✔: Fully addressed; ▲: Partially addressed; ✘: Not addressed at all.

Rowe and
Rrushi [7]
(2016)
✔
✔
✘
▲
✔
▲
▲
✔
✔
✔
▲

2

Almeshekah and
Spafford [8]
(2014)
✔
✔
✘
✘
▲
▲
▲
▲
▲
▲
✘

Han et al. [6] surveyed defensive deception techniques based
on four criteria, including the goal, unit, layer, and deploy-
ment of deception. They surveyed theoretical models used
for defensive deception techniques as well as the generation,
placement, deployment, and monitoring of deception elements.
Han et al. discussed the tradeoffs between various deceptive
techniques in relation to whether they are deployed at the
network, system, application, or data layers. Their discussion
of game-theoretic deception, however, is not comprehensive.
Pawlick et al. [5] conducted an extensive survey on de-
fensive deception taxonomies and game-theoretic defensive
deception techniques that have been used for cybersecurity
and privacy. The authors discussed the main six different types
of deception categories: perturbation, moving target defense,
obfuscation, mixing, honey-X, and attacker engagement. Their
paper surveyed 24 papers published over 2008–2018, and
deﬁned related taxonomies to develop their own classiﬁcation
of game-theoretic defensive deception techniques. This work
is interesting to treat moving target defense and obfuscation as
subcategories under defensive deception. This paper discussed
the common game-theoretic approaches used for developing
defensive deception techniques, such as Stackelberg, Nash,
and signaling game theories. However, the survey and analysis
of the existing game-theoretic defensive techniques conducted
in this paper are limited to game-theoretical analysis without
considering realistic network environments where ML-based
defensive deception techniques or combination of these two
(i.e., game theory and ML) may provide more useful insights
and promising research directions.

Recently, Lu et al. [4] conducted a brief survey based on the
processes of defensive deception consisting of three phases:
planning of deception, implementation and deployment of
deception, and monitoring and evaluation of deception. The
authors discussed deception techniques based on information
dissimulation to hide real information and information sim-
ulation to focus on attackers. This work brieﬂy discussed
game-theoretic defensive deception and mainly focused on
discussing challenges and limitations of the current research.
However, only a small fraction of the literature was included.
In addition, this paper did not discuss ML-based defensive
deception approaches.

Some survey papers mainly focused on defensive deception
techniques for a particular type of attacks or particular decep-
tion techniques. Carroll and Grosu [9] investigated the effects
of deception on the game-theoretic interactions between an

attacker and defender of a computer network. They examined
signaling games and related Nash equilibrium. However, this
investigation only focused on honeypots technology while
game-theoretic analysis of the deception is limited in examin-
ing the interactions between an attacker and a defender in the
signaling games. Virvilis et al. [10] surveyed partial defensive
deception techniques that can be used to mitigate Advanced
Persistent Threats (APTs).

In Table I, we summarized the key contributions of our
survey paper, compared to those of the ﬁve existing survey
papers [4, 5, 6, 7, 8] based on several key criteria.

C. Key Contributions

In this paper, we made the following key contributions:
1) We provided a novel classiﬁcation scheme that charac-
terizes a defensive deception technique in terms of its
conceptual deception categories, presence of an object
(i.e., physical or virtual), expected effects after applying
deception, ultimate goal (i.e., for asset protection or attack
detection), and activeness (i.e., active or passive or both).
This provides an in-depth understanding of each deception
technique and insights on how it can be applied to support
a system’s security goal.

2) We discussed key design principles of defensive deception
techniques in terms of what-attacker-to-deceive, when-to-
deceive, and how-to-deceive. In addition, based on the key
properties of defensive deception techniques, we identiﬁed
the key beneﬁts and caveats when developing defensive
deception techniques leveraging game theory and ML al-
gorithms.

3) We discussed game-theoretic and ML-based defensive de-
ception techniques based their types along with pros and
cons. In addition, using the classiﬁcation scheme we intro-
duced in Section II, we discussed both game-theoretic and
ML algorithms.

4) We also surveyed attacks that are handled by the existing
game-theoretic and ML-based defensive deception tech-
niques. Accordingly, we discussed what attacks are more or
less considered in the literature by the defensive deception
techniques.

5) We surveyed how defensive deception techniques are
mainly considered to deal with the challenges of different
network environments as application domains and dis-
cussed pros and cons of the deployed game-theoretic or
ML-based defensive deception techniques.

3

6) We examined what

types of metrics and experiment
testbeds are more or less used in game-theoretic or ML-
based defensive deception techniques to prove their effec-
tiveness and efﬁciency.

7) We extensively discussed lessons and insights learned and
limitations observed from the defensive deception tech-
niques surveyed in this work. Based on these insights and
limitations learned, we suggested promising future direc-
tions for game-theoretic and ML-based defensive deception
research.

Note that the scope of this paper is mainly focused on sur-
veying game-theoretic (GT) or ML-based defensive deception
techniques and discussing insights,
limitations, or lessons
learned from this extensive survey. Hence, some defensive
deception techniques that are not using either game-theoretic
approaches or ML are excluded in this survey paper.

• Section VIII presents metrics to measure effectiveness and
efﬁciency of the existing defensive deception techniques
using game theory and ML. In addition, this section sur-
veys evaluation testbeds used for validating those existing
defensive deception techniques surveyed in this work.

• Section VII discusses how game theoretic or ML-based
defensive deception techniques have been developed for
different application domains, such as enterprise networks,
cyberphysical systems (CPS), cloud web-based networks,
Internet of Things (IoT), software-deﬁned networks (SDNs),
and wireless networks.

• Section IX summarizes the insights and lessons learned by
answering the key research questions raised in Section I-D.
In addition, this section discusses the limitations found from
the defensive deception techniques surveyed in this work
and suggests promising future research directions.

D. Research Questions

We address the following research questions in this paper.
RQ Characteristics: What key characteristics of defensive
deception distinguish it from other defensive techniques?
RQ Metrics: What metrics are more or less used to mea-
sure the effectiveness and efﬁciency of the existing game-
theoretic or ML-based defensive deception techniques?
RQ Principles: What key design principles help maximize
the effectiveness and efﬁciency of defensive deception tech-
niques?

RQ GT: What are the key design features when a defensive
deception technique is devised using game theory (GT)?
RQ ML: What are the key design features when a defensive

deception technique is developed using ML?

RQ Applications: How should different defensive deception
techniques be applied in different application domains?
We answered these questions in Section IX-A.

E. Structure of the Paper

The rest of this paper is structured as follows:

• Section II provides the concept of deception and taxonomies

related to defensive deception.

• Section III discusses the key principles of designing a
defensive deception technique. In addition,
this section
clariﬁes the key distinctive characteristics of the defensive
deception techniques, compared to other defense techniques
that achieve a same defense goal.

• Section IV explains the key components in using game-
theoretic defensive deception and surveys the existing game-
theoretic defensive deception techniques along with the
discussions of their pros and cons.

• Section V discusses the key components in leveraging ML
techniques to develop defensive deception techniques. In
addition, this section extensively surveys the existing ML-
based defensive deception techniques and addresses their
pros and cons.

• Section VI describes attack types countered by the ex-
isting game-theoretic and ML-based defensive deception
techniques.

II. TAXONOMIES OF DEFENSIVE DECEPTION

Deception has been heavily used by attackers which perform
a variety of attacks at different levels of applications in both
computer and social systems. In this section, we limit our
discussions of deception in terms of a defender’s perspective.
This section mainly includes the discussions of formal models
of deception, related common taxonomies used in developing
defensive deception techniques, and their distinctive charac-
teristics.

A. Formal Models of Cyber Deception

In this section, we discuss formal models of cyber deception
that have been discussed in the literature. Although there are
numerous formal models of cyber deception, we limit our
discussion to formal modeling approaches based on logic,
probabilistic logic, and hypergame theory.

1) Logic-based Cyber Deception: Deception planning has
been studied based on logical reasoning of the key components
of deception and their states (i.e., status). Jafarian and Niakan-
lahiji [11] and Takabi and Jafarian [12] proposed deception
modeling logic that provides a formal deception model, DM,
as:

DM = {facts, beliefs, actions,
causation rules, attackers, goal, budget}

(1)

Each component of the DM is:
• Facts: The facts indicate a deception problem that can be
described by (1) attributes of a system (e.g., conﬁguration
parameters, such as an operation system of a host, a server’s
criticality, existing routes between hosts), (2) data types
of each attribute (e.g., Boolean, ordered, integral), and (3)
actual values assigned to the attributes.

• Beliefs: A belief indicates an attacker’s beliefs on the set of
attributes, which may not be aligned with the true states of
the attributes.

• Actions: An action refers to a defender’s action to manip-
ulate the attacker’s belief on an attribute. Attributes can
be actionable or derivative. Actionable attributes can be
directly manipulated to inﬂuence the attacker’s belief by

taking a deceptive action. Derivable attributes cannot be
directly manipulated but can be indirectly derived from the
attacker’s beliefs over other attributes. When modeling a set
of actions, each action should be plausible to the attacker
so the deception action is promising enough to deceive the
attacker. In addition, the action should consider how much
it costs.

• Causation rules: The causation rules apply to each attribute
to determine what (e.g., facts or actions) can cause the
manipulation of an attacker’s belief on the attribute.

• Attackers: A set of attacker types can be modeled based
on the probability distribution over the set when the prob-
ability information of an attacker being a particular type is
available.

• Goal & Budget: The concrete beneﬁt of successful or failed
deception should be estimated in terms of defense effec-
tiveness, defense cost (including both monetary budget),
expected effect (i.e., a required level of achieved system
performance/security) or other associated impact (e.g., in-
teroperability with other defense or service mechanisms).
Rowe [13] proposed a deception planning method to system-
atically provide logically consistent deceptions when users are
detected as suspicious or malicious by an intrusion detection
system. Rowe [13] proposed an attack modeling technique
that can consider realistic attackers characterized by their
targets (e.g., ﬁles, a network, status such as log-in or admin-
istrator privileges), the status of the targets (e.g., existence,
authorization, readiness, or operability), and their parameters
(e.g., ﬁle size, network bandwidth, a number of sites, or a
length of the password). By using an inference rule based
on the status, an attacker can derive the status of a targeted
resource. To prevent the attackers from not achieving their
goal, deception tactics can be used to deny the service the
attacker requested by generating error messages, such as ‘a
ﬁle not transferable’, ‘a ﬁle not recognizable’, ‘protection
violation’, or requesting providing credentials (e.g., password
or other credential information).

2) Probabilistic Cyber Deception: This type of formal
models mainly studies how a developed defensive deception
technique can manipulate an attacker’s belief and accordingly
inﬂuence its action based on its belief in a probabilistic
manner. Jajodia et al. [14] proposed a probabilistic logic for
cyber deception where a defender sends true or fake results
for the scans by an attacker to maximize the probability
of successful deception (i.e., maximum manipulation of the
attacker’s beliefs) while measuring its effectiveness in terms
of the probability distribution of the attacker’s belief on system
attributes (i.e., how much information the attacker has obtained
towards a target system accurately). Crouse et al. [15] pro-
posed probabilistic models of reconnaissance-based defense
with the aim of providing in-depth understanding on the effect
of the defense on system security. The authors quantiﬁed
attack success under various network conditions in terms of a
network size, a size of deployment, and a number of vulnerable
system components. The proposed deception models provide
performance analysis of probabilistic triggering of network
address shufﬂing and deployment of honeypots when these
defense mechanisms are considered separately or in concert.

4

In prior work [16], we used Stochastic Petri Nets to model
an attack-defense hypergame with the aim of examining how
an attacker’s beliefs manipulated by a defensive deception
can affect its decision making, resulting in attack failure.
As Jajodia et al. [14], Crouse et al. [15] and Cho et al.
[16] observe, probabilistic models of deception games can
effectively capture dynamic interactions between an attacker
and a defender under uncertain environments.

3) Cyber Deception Hypergames: Much defensive decep-
tion research uses game theory to formulate an attack-defense
game. In particular, to effectively deal with uncertainty in real
scenarios, a sequential attack-defense game where the defender
uses defensive deception has been formulated based on hyper-
game theory. Ferguson-Walter et al. [17] formulated a cyber
deception game as a sequential game as G = (P, M, Θ, u, T ),
where P refers to a set of players, M refers to a set of actions,
such as M = {Mi} for player i, Θ is a set of strategies,
Θ = {Θi} for player i, u is a utility function, u = {ui} for
player i, and T is a sequence of moves when players take turns
in sequence. By using the formulation of a regular, sequential
game, a cyber deception can be formulated as (G, GA, GD)
where GA and GD are derived games. In G, a move history
of an attacker, MA = {mA} = {mA
r }, and a
defender, MD = {mD} = {mD
s }, respectively,
refers to the sequence of moves in the game where r + s ≤ T .
For simplicity, the move histories of the attacker and defender
can be denoted by m = {mA, mD}. Given that MX|Y =
mX|Y refers to Y ’s perception of the set of X’s actions (i.e.,
it is not necessarily true for MX|Y = MX ), the attacker (A)’s
beliefs about the move sequence, m, can be represented by
m∗|A = {mA|A, mD|A}. Similar to the notation of the moves
perceived by A, we can also obtain u∗|A = (uA|A, uD|A),
which is A’s perception of u = {uA, uD} and Θ∗|A =
{ΘA|A, ΘD|A}, which is A’s perception of sets of strategies
for ΘA and ΘD. Hence, the derived game can be denoted by
GA = {P, M∗|A, u∗|A, T }, which is A’s perception towards
the game G, and GD = {P, M∗|D, u∗|D, T }, which is D’s
perception towards the game G. If an attack-defense game
does not use the concept of hypergame, it simply relies on the
game formulation of G.

1 , mA
2 , . . . , mD

2 , . . . , mA

1 , mD

B. Concepts of Defensive Deception

The conventional concept of military deception refers to
actions taken to intentionally mislead an enemy about one’s
strengths and weaknesses, intents, and tactics [18]. The de-
fender employs deception to manipulate the enemy’s actions
to advance one’s mission [18]. Defensive deception applies
to a wide spectrum of interactions between an attacker and
a defender under conﬂict situations [18]. The concept of
defensive deception initiated in military environments has
been introduced to cyber domains with the name of cyberde-
ception. Almeshekah and Spafford [19] reﬁned the concept
of cyberdeception by Yuill [20] as “planned actions taken
to mislead and/or confuse attackers and to thereby cause
them to take (or not take) speciﬁc actions that aid computer-
security defenses.” Although the concept of deception is highly
multidisciplinary [21], the common idea behind deception is

TABLE II
TAXONOMIES AND CLASSIFICATION OF DEFENSIVE DECEPTION TECHNIQUES

Deception Tactic
Masking
Repackaging
Dazzling
Mimicking
Inventing
Decoying
Bait
Camouﬂaging
Concealment
False Information

Lies
Display

Presence of a Real Object
True
True
True
False
False
True
True
True
True
False

Expected Effect
Blending, Hiding, Misleading
Blending, Hiding, Misleading
Hiding, Confusing, Misleading
Luring, Misleading
Luring, Misleading
Luring, Misleading
Luring, Misleading
Blending, Hiding, Misleading
Hiding, Misleading
Luring, Confusing, Misleading

False
True

Hiding, Misleading
Hiding, Misleading

Ultimate Goal
Asset Protection
Asset Protection
Asset Protection
Attack Detection
Attack Detection
Attack Detection
Attack Detection
Asset Protection
Attack Detection
Asset Protection or
Attack Detection
Asset Protection
Asset Protection

5

Activeness
Passive
Passive
Passive
Passive
Active
Active
Active
Passive
Active
Active or Passive

Passive
Passive

agreed as a way to mislead an entity to form a false belief and
control its behavior based on it [7]. Therefore, when deception
is successfully executed, a deceivee responds suboptimally,
which provides beneﬁts for a deceiver and results in achieving
the deceiver’s goal. Rowe and Rrushi [7] emphasized ‘ma-
nipulation’ as the core concept of deception where deception
encourages a deceivee to do something a deceiver wants or
discourages the deceivee from doing something the deceiver
does not want.

As the discussion of deception in this work is limited
to defensive deception, we consider a defender’s deception
against an attacker to achieve the defender’s goal.

C. Taxonomies of Defensive Deception

Deception can be applied with certain intent, either ma-
licious or non-malicious intent [22]. In particular, deception
with malicious intent has been used as an attacker’s strategy
to deceive a defense system. For example, spammers are
paid from clicking deceptive advertisements [23], malicious
users disseminate phishing links to obtain credentials from
victim users [24], or social and political bots propagate false
information to inﬂuence public opinions [25].

As we consider the concept of deception in the context of
defensive deception, we limit our discussions of deception
‘with intent’ where the intent is to defend a given system
against attackers. In this section, we discuss taxonomies in
terms of the following aspects: (i) what conceptual techniques
are used; (ii) whether a given deception uses a true object or
not (i.e., a true object exists but is hidden to deceive or a fake
object is created to deceive); (iii) what effects are expected
when successfully deceiving an opponent; (iv) what is an
ultimate goal of the deception, such as detecting an attacker
or protecting a system; and (v) whether a given deception
is active (mainly for attack detection) or passive (mainly for
attack protection) to deceive an opponent.
1) Conceptual Deception Tactics:

In the literature [19,
26, 27, 28], we found the following conceptual deception
techniques:

• Masking [19, 26]: This refers to hiding certain information

or data in the background.

defender can create honey ﬁles [29], such as creating trap
ﬁles that look like regular ﬁles.

• Dazzling [19, 26]: This is a different way to hide something
that
is hard to blend in with the background or to be
repackaged as something else. A real object can be hidden
by overshadowing it, aiming to be undetected as the real,
true object. For example, an intruder can be confused by
receiving many error messages.

• Mimicking [19, 26]: This refers to imitating aspects of a real
object. This is often used to look honeypots as real nodes
or interfaces to attackers.

• Inventing [19, 26]: A defender can create a new fake object
to lure attackers. For instance, a software can be presented
in a honeypot for attackers to download it, which allows
collecting the attackers’ personal data.

• Decoying [19, 26]: A deceiver attracts an attacker’s attention
away from critical assets that should be protected in a
system. For example,
the deceiver can provide publicly
available false information about the system conﬁgurations
to hinder reconnaissance attacks.

• Bait: Deception can use truth [27] to earn trust from a
deceiver. That is, a defender can present attackers with
correct but low-grade information to lure them. The truth
can be the information that an attacker already has, or the
deceiver needs to sacriﬁce some correct sensitive data to
effectively lure attackers.

The military literature provides the following taxonomy for

cyberdeception [28].

• Concealment [28]: This deception is similar to hiding in the
classical taxonomy. For the defensive purpose, it helps for
honeypots to conceal their user-monitoring software so they
look more like normal machines.

• Camouﬂage [28]: This deceives an attacker by blending a
real object into a background or environment. For example,
valuable ﬁles and programs can be given misleading names
to make it difﬁcult for an attacker to ﬁnd them.

• False information [28]: Fake information (or disinformation)
can be planted to mislead an attacker in cyberspace. How-
ever, most false information about cybersystems is easy to
be checked by testing or trying it out. Therefore, this kind
of deception cannot fool attackers for a long time.

• Repackaging [19, 26]: This means to hide a real object as
something else to hide the real object. A defender can make
a vulnerability look like something else. For example, a

• Lies [28]: This provides deceptive information, which is
false. However, lies are distinguished from disinformation
because they are often provided as responses to questions

or requests. In this sense, even if this deception uses false
information, it implies a passive action because lies may not
be used unless it is requested. Lies could be more effective
than explicitly denying access as they encourage an attacker
to continue wasting time trying to access the resource in
different ways and at different times.

• Displays [28]: This deception is similar to inventing as it
also aims at misleading objects or information. However,
unlike the inventing, which creates a new, fake object, ‘dis-
plays’ can be applied to a real object (i.e., an object which
is actually in a give system) by displaying it differently.

2) Presence of Actual Objects or Information: Deception
can be applied when an actual, true object exists or no actual
object exists. To be more speciﬁc, a defender can use true
objects or information but may want to hide it by lying or
providing false information towards it. For example, even if a
system uses Windows as an operating system but may lie it
uses Unix. But the system is using a certain OS in deed. On the
other hand, a fake object, which has not existed anywhere, can
be created for the purpose of creating uncertainty or confusion,
which can lead an opponent to reach a suboptimal or poor
decision, such as honey tokens or honey ﬁles.

3) Expected Effects (or Intents) of Defensive Deception:
Via defensive deception, one or more effects are expected
as the process of achieving the goal of deception, making
an attacker choose a suboptimal choice in its strategies. We
categorize these expected effects in terms of the following ﬁve
aspects:

• Hiding: This effect is addressed when deception is used
to change the display of a real object into something else.
Some obfuscation defense techniques also use hiding data
to slow down the escalation of attackers or increase attack
complexity [8, 7].

• Luring [19, 26]: This effect is shown when a fake object is
demonstrated to an attacker where the object looks like a
real object an attacker is interested in. The typical examples
are honeypots to lure attackers or honey ﬁles to send false
information to the attackers.

• Misleading [30]: Most deception techniques have this effect
as the last step to achieve the ultimate goal of deception,
misleading an attacker to choose a suboptimal action.

• Blending: This effect is made when a real object can be well
blended into background or environments. Its effectiveness
relies on an environment and in altering the appearance
of things to hide. For instance, give password ﬁles a non-
descriptive name to elude automated searches run by hackers
looking for ﬁles named ‘pass’ [31]. Additionally, blend-
ing hidden objects with the environment can be achieved
through altering the environment instead. For example, a
defender can create noise in the environment through adding
bogus ﬁles to make it harder to ﬁnd critical ones.

• Confusing [8]: This effect comes from perceived uncertainty
caused by a lack of evidence, conﬂicting evidence, or failing
in discerning observations (e.g., cannot pinpoint whether a
car on the street is red or blue) [8, 32]. A defender can
confuse an attacker by presenting both truths and deceits [8].

Some deception techniques use more than one technique,

6

such as bait-based deception that includes lies to increase
attack cost or complexity [33]. Bowen et al. [34] designed
a trap-based defense technique to increase the likelihood of
detecting an insider attack which combines several deception
techniques like camouﬂage, false information, and creating
fake objects. First, the deception system embeds a watermark
in the binary format of the document ﬁle to detect when
the decoy is loaded. Moreover, the content of each decoy
document includes several types of “bait” information, such as
online banking logins, login accounts for online servers, and
web-based email accounts. A “beacon” is embedded in the
decoy document that signals a remote web site upon opening
of the document.

4) Ultimate Goals of Defensive Deception in Cyberspace:
The ultimate goal of a defender using deception techniques is
either protecting assets or detecting attackers or both. Although
some deception techniques allow the defender to achieve both
asset protection and attack detection, deception by hiding is
more likely to protect system assets while deception by using
false objects or information is to catch attackers.

5) Activeness of Defensive Deception: Caddell [35] catego-
rized deception as passive vs. active. Passive deception uses
hiding the valuable assets or their capabilities and information
from attackers. On the other hand, active deception tends to
use fake, false information aiming for the attacker to form false
beliefs, leading to choosing suboptimal decisions. However,
even if false information or lies are used, the defense goal can
be either protecting assets or detecting attackers. Hence, the
distinction of active deception and passive deception may not
be always clear.

In Table II, we summarized how a particular deception
technique can be understood based on the four key aspects:
the presence of actual objects or information used in decep-
tion, expected effects (or intents), ultimate goal(s), and the
activeness (or passiveness) of the used deception technique.

III. DESIGN PRINCIPLES AND UNIQUE PROPERTIES OF
DEFENSIVE DECEPTION

In this section, we discuss the four key design principles of
defensive deception techniques. In addition, we address unique
properties of defensive deception and their key merits and
caveats when using defensive deception techniques. Further,
we discuss how the defensive deception techniques differ
from other similar defense techniques, such as moving target
defense or obfuscation techniques.

A. Design Principles of Defensive Deception

In this section, we discuss the design principles of defensive
deception in terms of the four aspects: what-attacker-to-
deceive (i.e., what type of an attacker to deceive), when-to-
deceive (i.e., when deception can be used), how-to-deceive
(i.e., what particular deception technique can be used to
deceive an attacker). We discuss each principle as follows:

1) What-Attacker-to-Deceive: This design principle asks
to determine what type of an attacker a defender wants to
deceive. For example, if the defender targets to deceive at-
tackers performing reconnaissance attacks as outside attackers,

it may aim to deceive them by providing false information
about a system conﬁguration (e.g., saying using Windows
operating system (OS) even if it actually uses Unix OS). In
addition, if a valuable system asset should be protected from
attackers aiming to exﬁltrate conﬁdential information to an
outside network, the defender may deploy a honeypot which
mimics a real node with highly conﬁdential information (e.g.,
a database). Hence, determining what-attacker-to-deceive is
to decide what attackers to target by the defender. Since
developing a defensive deception technique incurs cost, there
should be in-depth analysis and investigation on whether a
given attacker should be prevented or detected by a defen-
sive deception technique in terms of cost, effectiveness, and
efﬁciency of its deployment.

2) When-to-Deceive: This design principle refers to de-
termining when a deception technique should be used in
terms of the attack stage of a given attacker in the cyber
kill chain (CKC) [36, 37]. The six CKC stages include
reconnaissance, delivery, exploitation, command and control,
lateral movement, and data exﬁltration [37]. Outside attackers
mainly perform attacks in the stage of reconnaissance and
delivery stages with the aim of penetrating into a target system.
On the other hand, inside attackers aim to perform attacks to
exﬁltrate conﬁdential information to the outside, unauthorized
parties. For example, when more scanning attacks by the
outside attackers are detected in the reconnaissance stage,
the defender can use fake patches with false vulnerability
information, which can lure the attacker to honeypots. After
the attacker successfully got into the system, becoming the
inside attacker, the defender can use honey ﬁles or tokens
to lure the attacker which may think of exploiting them to
perform attacks with the false information. However, there
are also legacy defense mechanisms, such as access control,
intrusion detection and prevention, or emerging technologies,
such as moving target defense as the alternative mechanisms
to deal with the same types of attackers. In such cases, there
should be utility analyses based on losses and gains in terms
of the timing of using a defensive deception technique.

3) How-to-Deceive: The design decision on how-to-deceive
type
in employing defensive deception is related to what
of a defensive deception technique to use. The conceptual
deception techniques have been discussed in Section II-C, such
as masking, mimicking, decoying, false information, baits, and
so forth. However, what speciﬁc technique to use for defensive
deception is more related to what technology to use to achieve
deception. Some example defensive deception technologies
include honeypots, honey ﬁles, honey tokens, fake patches,
fake network topologies, fake keys, or bait ﬁles [6].

B. Beneﬁts and Caveats of Defensive Deception Techniques

In this section, we discuss the key beneﬁts of using defen-
sive deception techniques as well as the caveats of developing
and employing them.

1) Key Beneﬁts of Defensive Deception Techniques:

• Defensive deception is relatively cost-effective compared
to other defense strategies because its deployment cost is
relatively low while showing relatively high effectiveness to

7

mislead attackers. For example, honey ﬁles or honey tokens
are relatively simple to deploy and maintain, compared to
other traditional cybersecurity mechanisms, such as access
control or intrusion detection.

• Defensive deception provides complementary defense ser-
vices to other legacy defense mechanisms, such as intrusion
detection or prevention. For example, honeypots are well
known as an effective monitoring mechanism that can pro-
vide additional attack features to enhance intrusion detection
as well as proactively to protect a system from intrusions
before they actually launch attacks to targets.

• Various types of defensive deception techniques are deploy-
able at multiple layers of systems, such as network, system,
application, and data layers [6], without signiﬁcant changes
of existing system architectures. High deployability of de-
fensive deception techniques also provides design ﬂexibility
as well as defense-in-depth (DiD) capability with multiple
layers of protections.

• Automated cyberdeception techniques, such as obfuscating
session information, data ﬂow, and software’s code ob-
fuscation, have signiﬁcantly improved security to counter
automated attacks.
2) Key Caveats of Defensive Deception Techniques:

• Since it is highly challenging to obtain an attacker’s motiva-
tion, intent, and goal in launching and executing a particular
attack,
to choose an optimal defensive
deception strategy based on the estimated risk and beneﬁt
associated with a given defensive deception strategy.

is not

trivial

it

• Most honey-X techniques (e.g., honeypots, honey ﬁles,
honey tokens) aim to mislead attackers to choose suboptimal
or poor choices in launching attacks by false information.
This may introduce extra procedures or protocols for normal
users or a defender not to be confused by them.

• Due to the nature of deception, the effectiveness of defensive
deception requires continual conﬁguration, reconﬁguration,
and implementation process. Otherwise, adversaries would
be able to easily distinguish deceptive devices unless decep-
tion is automated like software obfuscating techniques.
• With the emergence of clever attackers, there is a growing
demand for high-interactive honeypots or elaborated decep-
tion strategies that cannot be easily identiﬁed by attackers.
This brings a higher defense cost and more difﬁculty in the
management for a defense system than a traditional system
without defensive deception mechanisms.

C. Distinctions between Defensive Deception and Other Sim-
ilar Defense Techniques

Defensive deception is often mentioned as moving target
defense or vice versa. In addition, obfuscation has been used
to achieve the same aim like defensive deception. In this
section, we discuss how the roles and natures of these three
techniques differ from each other as well how they overlap in
their functionalities and aims.

1) Distinction with Moving Target Defense (MTD): MTD
is similar to defensive deception in terms of its aim to increase
leading to deterring
confusion or uncertainty of attackers,
level or failing
the escalation of their attacks to a next

8

their attacks. However, the key distinction is that MTD does
not use any false information to actively mislead attackers
while defensive deception often involves using false objects
or information for the attackers to form false beliefs and be
misled to make suboptimal or poor attack decisions. MTD
relates its key functionality mainly with how to change system
conﬁgurations more effectively and efﬁciently while defensive
deception more involves the exploitation of manipulating the
attacker’s perception. In this sense, if defensive deception is
well deployed based on a solid formulation of manipulating
the attacker’s perception, it can be more cost effective than
deploying MTD. Ward et al. [38] considered MTD as part of
defensive deception while Cho et al. [39] treated defensive
deception as part of MTD. The distinction between these
two is not crystal clear because defensive deception can
be deployed using MTD techniques, such as randomness or
dynamic changes of system conﬁgurations (e.g., dynamically
assigning decoys in a network) while it may not use false
information all the time (e.g., baits or omission such as no
response).

Fig. 1. Relationships between defensive deception, moving target defense,
and defensive obfuscation.

IV. GAME-THEORETIC DEFENSIVE DECEPTION (GTDD)
TECHNIQUES

In this section, we discuss the key component of modeling
defensive deception techniques using game theory. In addition,
we discussed a wide range of defensive deception techniques
that have been considered based on various types of game
theory along with their pros and cons.

2) Distinction with Obfuscation: Obfuscation techniques
have been used by an attacker to obfuscate malware [40],
metamorphic viruses [41], or malicious JavaScript code [42].
In terms of a defender’s perspective, obfuscation techniques
have been used to obfuscate private information [43], or Java
bytecode for decompiling [44]. Although obfuscation tech-
niques are often mentioned as the part of defensive deception
techniques, they have been studied as a separate research
area for decades. Chan and Yang [44] conducted an extensive
survey on obfuscation techniques enhancing system security
and discussed the key aims of obfuscation techniques as: (i)
increasing the difﬁculty of reverse engineering of the program
logic; (ii) mitigating the exploitability of vulnerabilities by at-
tackers; (iii) preventing modiﬁcations by unauthorized parties;
and (iv) hiding data or information. Although the concept of
defensive deception has gained popularity of late while obfus-
cation techniques have been applied for decades, it is quite
obvious that the aim of defensive obfuscation is well aligned
with that of defensive deception. However, a subtle difference
lies in that defensive deception involves cognitive aspects of
an attacker’s perception while defensive obfuscation is directly
involved with achieving security goals (e.g., conﬁdentiality,
data integrity, availability). Defensive obfuscation techniques
are also used in combination with diversiﬁcation [44] (e.g.,
diversifying systems, network conﬁgurations, or components),
which is a well-known concept used by MTD techniques.

Based on our understanding on the functionalities and aims
of defensive deception, MTD, and defensive obfuscation, we
would like to hold our view based on Fig. 1, which shows how
each domain is related to each other. Basically, our view is that
a defensive obfuscation technique can belong to either MTD or
defensive deception. In addition, there are also an overlapping
area of MTD and defensive deception where some defensive
techniques require using the features of both concepts.

A. Key Components of Game-Theoretic Defensive Deception

Game theory has been used extensively in modeling cyber-
security problems as a framework to model the following key
elements of cybersecurity problems: a defender and attacker as
players, actions as attack and defense strategies, observability
of a system and an opponent’s strategies, or system dynamics.
We discuss these key elements of cybersecurity games as
follows.

1) Players: Most research taking game-theoretic defensive
deception approaches model a two-player game where the
two players are an attacker and a defender using defensive
deception [45, 46, 47]. However, even in a two-player game,
some deception games modeled different types of players
under each player. Pawlick and Zhu [47] modeled a two-player
signaling game where a defender as a sender can be either a
normal system or a honeypot while an attacker as a receiver
has one type. Depending on the type, the defender takes its
strategy. Pawlick et al. [48] introduced a three-player game
between a cloud defender, an attacker and a device which is
connected with the cloud where the defender can send false
signals to deceive the attacker.

2) (In) Complete Information: A complete information
game allows players to have full knowledge of the game
parameters, such as possible actions to be taken by other
players, a reward function, and the current state of the game
in case of dynamic or multistage games. A game with com-
plete information is sometimes considered to be impractical.
The assumption of the defender knowing the set of attack
actions to be taken by its adversary is not realistic due to
inherent uncertainty in a given context. On the other hand,
an incomplete information game represents a practical class
of games where one or more players may or may not know
some information about the other players [49]. For instance,
a player may not fully know other players’ types, strategies,
and payoff functions.

3) (Im) Perfect Information: Imperfect information game
represents a game in which a player may not know what
exact actions have been played by other players in a given
game. This makes it computationally prohibitive to track the
history of actions in case of a multistage game. However,
all players may know their opponents’ types, strategies, and
payoff functions, which can form a complete, imperfect in-
formation game. This imperfect information game is often
assumed in cybersecurity games between an attacker and a
defender [50, 51].

4) Partial Observability: Players acting upon dynamic sys-
tems are modeled as dynamic and stochastic games. In this
case,
the system/environment state changes over time and
depends on the actions taken by all the players. However, in
some scenarios, one or more players will not fully observe
the state. This results in a partially observable game. If
the system environment is affected by an exogenous factor,
the state transition is stochastic which results in a partially
observable stochastic game (POSG) [52]. In the special case
of POSG with only a single player, the game turns to a partially
observable Markov dynamic process [53]. POSG model deals
with the most general form of games that capture different
game settings, such as incomplete information as well as
imperfect information (i.e., imperfect monitoring). Therefore,
solving such a game efﬁciently is still an open research
question.

5) Bounded Rationality: Both the rational and bounded
rational players are commonly applied in a game between an
attacker and a defender. A rational player can always choose an
optimal strategy to maximize its expected utility. However, a
bounded rational player has only limited resources and cannot
afford an unlimited search to ﬁnd an optimal action [54].

6) Pure or Mixed Strategies: Two main strategies in game
theory are pure strategy or mixed strategy [54]. A pure strategy
means a player’s strategy is determined with the probability 0
or 1. In contrast, a mixed strategy is a probability distribution
of several pure strategies. For example, Clark et al. [55] used
pure strategies to perform deceptive routing paths to defend
against jamming attacks in a two-stage game using Stackelberg
game theory. Zhu et al. [56] also used deceptive routing
paths against jamming attacks but identiﬁed an optimal routing
considering resource allocation based on mixed strategies.

7) Uncertain Future Reward in Utility: Due to the inherent
uncertainty caused by multiple variables that may control the
future states, a player’s utility cannot be guaranteed [54].
In game-theoretic defensive deception research, uncertainty
is mainly caused by non-stationary environmental conditions
and non-deterministic movements (or actions) by opponent
players. In a signaling game, uncertainty is caused by both the
movement of an opponent player and its type [47, 57] where
a player can take an action after it knows its opponent’s type.
8) Utilities: For a player to take an optimal action, it is
critical to know both its own payoff (or utility or reward)
function and its opponent’s payoff function. In a static, one-
time game, a reward per strategy is based on action proﬁles
(i.e., a set of actions each player can take) of all players [54].
However, in a sequential game (i.e., a repeated game), a reward
can be estimated based on the history of action proﬁles from

9

the beginning to the end of the game. Hence, in such games, a
player aims to maximize the expected average or accumulated
reward over the period of the sequential game [50].

9) Full Game or Subgame: A full game represents a game
where each player considers a set of all possible actions to
select one action to maximize its utility. A subgame indicates
a subset of the full game where a player considers a subset of
all possible actions when selecting one action to maximize its
utility. Particularly, in sequential games, a subgame includes
a single node and all its successors [54]. It is similar to
a subtree in a tree-structure sequential game, which refers
to an extensive form game. House and Cybenko [58] used
the hypergame theory to model the interactions between an
attacker and a defender. Hypergame uses a subgame to model
each player’s different view on the game and corresponding
strategies under the subgame. In addition, Zhang et al. [59] and
Xu and Zhuang [60] used Subgame Perfect Nash Equilibrium
(SPNE) [61] (i.e., a Nash equilibrium in every subgame) to
deal with the resource allocation for both the defender and
attacker in non-hypergames.

10) Static Game vs. Dynamic Game:

In game theory,
a static game is considered as one-time interaction game
where players move simultaneously. Hence, a static game
is a game of imperfect information. On the other hand, a
dynamic game is considered as a sequential game assuming
that players interact through multiple rounds of interactions as
repated games. Although most dynamic games are considered
sequential games with perfect information, if a game considers
partial (or imperfect) observability, the dynamic game can be
sequential across rounds of games but within a subgame, it
can be a game of imperfect information (i.e., players know
other players’ moves in previous rounds but do not know their
moves in a current round).

Game theory has provided a powerful mathematical tool
for solving strategic decision making in various dynamic
settings. Since the powerful capability of game theory has been
clearly proven based on the achievement of a number of game
theorists as Nobel Prize Laureates in the past [54], there is no
doubt that game theory has substantially inspired in solving
highly complicated, dynamic decision-making problems with
mathematically validated solutions. Although game theory has
been substantially explored to provide solutions with solid
theoretical proofs, one of its common limitations has been
discussed in terms of too strong assumptions to derive Nash
Equilibria, such as common knowledge about probability dis-
tributions of Nature’s moves or players’ correct beliefs towards
opponents’ moves. As game theory has a long history since
1700s [62], its long evolution has made signiﬁcant advance-
ment in considering more and more realistic aspects of real-
world problems. The exemplary efforts in addressing realistic
scenarios include the consideration of bounded rationality,
decision making under uncertainty using the concept of mixed
strategies, games of imperfect or incomplete information,
hypergame with different subjective views of players, games
with partially observable Markov Decision Process (POMDP),
subjective beliefs, subjective rationality, and so forth [54].
We believe these efforts have obviously opened a door for
game theory to be highly applicable in real systems, including

systems concerning cybersecurity. In the context of developing
defensive deception techniques, game theory can contribute
to manipulating an attacker’s beliefs and accordingly leading
the attacker to miscalculate utilities under uncertainty with
the aim of failing the attacker. In addition, how frequently a
certain defense should be executed or what defense strategy
to choose can be also determined based on game theoretic
decision methods.

B. Common Games Formulated for Defensive Deception

We now discuss popular game theories used to formulate
various defensive deception techniques. We will discuss the
following games:

• Bayesian games: A Bayesian game is a game of incomplete
information which means part of or all players do not
know their opponents’ types, actions, or payoffs where
each player knows their own type, a set of actions, and
corresponding payoffs. Hence, each player has a subjective
prior probability distribution of their opponent’s type [63].
• Stackelberg games: Stackelberg game refers to a sequential
two-player game where the ﬁrst player is a leader and the
second player is a follower. In a stage game, the leader
moves ﬁrst, then the followers observe the action of leader
and select
their strategy based on the observation [64].
Hence, the ﬁrst player takes the ﬁrst-mover-advantage which
can lead the second player’s move to their intended direc-
tion [54].

• Signaling games: This is a sequential Bayesian game where
information) to another
one player sends a signal (i.e.,
player. When the player sends false information, it can be
most costly [54].

• Stochastic games: This is a multistage game in which the
game evolves from one stage (state) to another following
a stochastic distribution. The transition probability matrix
depends on a set of action proﬁles of both players and
may also depends on random exogenous factors of the game
environment. The game dynamics determine the payoffs of
both players [65].

• Games with Partially Observable Markov Decision Process
(POMDP): The POMDP considers to model an agent’s
decision process where the agent selects actions to maximize
a reward of the system. However, the agent cannot directly
perceive the system state, but it can obtain observations
depending on the state. The agent can maintain a belief of a
system state based on the observations. Formally, a POMDP
model can be represented by a tuple (S, A, Ω, T, R, O) [53],
where S represents a set of states, A represents a set of
actions, Ω represents a set of observations, T is a state
transition function, R is a reward function, and O is a
set of conditional observation probabilities. As an extended
version of the POMDP, the Interactive-POMDP (IPOMDP)
presents a multiagent setting which allows an agent to model
and predict behavior of other agents [66]. A Bayes-Adaptive
POMDP (BA-POMDP) assumes that transition and obser-
vation probabilities are unknown or partially known [67].

10

C. Game-Theoretic Defensive Deception

In this section, we discuss game-theoretic defensive de-
ception techniques used in the literature. We use the follow-
ing classiﬁcation to discuss game-theoretic defensive decep-
tion techniques for asset protection: honeypots, honeywebs,
honeynets, obfuscation, deceptive signals, multiple defensive
deception techniques, fake objects, honey patches, deceptive
network ﬂow. Some studies used empirical game-theoretic
experiment settings where players are human subjects. We also
discussed them to show how game theory has been used to
develop defensive deception in empirical experimental settings
with human-in-the-loop.

1) Honeypots: A honeypot has been studied as the most
common defensive deception strategy in the literature. A
honeypot is mainly used in two forms: low interaction hon-
eypots (LHs) and high interaction honeypots (HHs) [6]. The
differences between LHs and HHs are based on different
deception detectability and deployment cost. HHs provide
lower detectability by attackers than LHs; However, HHs
incurs higher deployment cost than LHs [6].

Pawlick and Zhu [47] employed a signaling game to develop
a honeypot-based defense system where an attacker has the
ability to detect honeypots. The authors investigated multiple
models of signaling games with or without evidence when
complete information is available or not. In the cheap-talk
games with evidence (i.e., a signaling game with deception
detection), extended from cheap-talk games (i.e., costless com-
munication signaling game between a sender and receiver),
the receiver (i.e., an attacker) is modeled to detect deception
(i.e., a honeypot) with some probability. They found that the
attacker’s ability to detect deception does not necessarily lower
down the defender’s utility.

The signaling game with evidence has been also applied to
model the honeypot selection or creation in other works [68,
69]. Pawlick et al. [68] used a signaling game with evidence
where the evidence is estimated based on a sender type and
a transmitted message. The authors extended the signaling
game with a detector with probabilistic evidence of deception.
However, this signal is vulnerable to attackers which can
analyze the signal, leading to detecting the deception and
causing the evidence leakage. P´ıbil et al. [69] introduced a
game-theoretic high-interaction honeypot to waste attackers’
resources and efforts. The authors designed a honeypot se-
lection game (HSG) as a two-player zero-sum extensive-form
game with imperfect and incomplete information. A honeypot
is designed to mimic servers with high importance to provide
cost-effective defensive deception.

La et al. [70] proposed a two-player attacker-defender
deception-based Bayesian game in an IoT network. They used
honeypots as a deceptive defense mechanism. They studied
the Bayesian equilibrium in one shot and repeated games.
Their ﬁndings showed the existence of a certain frequency
of attacks beyond which both players will primarily take
deception as their strategy. The authors used a honeypot
as a deceptive strategy where a game is considered under
incomplete information.

C¸ eker et al. [71] proposed a deception-based defense mech-
anism to mitigate Denial-of-Service (DoS) attacks. The de-

fender deploys honeypots to attract the attacker and retrieve
information about the attacker’s real intent. They used sig-
naling games with perfect Bayesian equilibrium to model the
interactions between the defender and attacker. Basak et al.
[72] used cyber deception tools to identify an attacker type
as early as possible to take better defensive strategies. The
attacker’s type is reﬂected in actions and goals when planning
an attack campaign. The authors leveraged on a multistage
Stackelberg Security game to model the interaction between
the attacker and defender. In the game, the defender is a
leader taking strategies considering the attacker’s strategy. As
a follower, the attacker selects its strategy after observing
the leader’s strategy. Through a game-theoretic approach, the
defender selects deception actions (e.g., honeypots) to detect
speciﬁc attackers as early as possible in an attack.

Mao et al. [46] used honeypots as a defense strategy in
a non-cooperative Bayesian game with imperfect, incomplete
information where an attacker is a leader and a defender
is a follower. The authors considered a player’s perception
towards possible motivation, deceptions, and payoffs of the
game. Kiekintveld et al. [45] discussed several game models
that address strategies to deploy honeypots, including a basic
honeypot selection game. They ﬁrst developed a honeypot
selection game as a two-player zero-sum extensive-form game
with imperfect and incomplete information in [69]. Then they
extended the game to allow additional probing actions by the
attacker in which attack strategies are represented based on
attack graphs [73, 74]. The authors conducted experimental
performance analysis to compare the performance of these
model and discussed the advantages and disadvantages of the
considered game-theoretic models in the defensive deception
research.

Durkota et al. [75] leveraged a Stackelberg game where an
attacker follows an attack graph and a defender can deploy
a honeypot to deceive the attacker based on an efﬁcient
optimal strategy searching method using Markov Decision
Processing (MDP) and sibling-class pruning. Kulkarni et al.
[76], Milani et al. [77], Tsemogne et al. [78] investigated
game-theoretic cyberdeception techniques played on attack
graphs. Kulkarni et al. [76] developed a zero-sum, hypergame
of incomplete information for evaluating the effectiveness of
the proposed honeypot allocation technique. Milani et al. [77]
developed a Stackelberg game to allocated defensive resources
and manipulate a generated attack graph. Tsemogne et al. [78]
studied the malicious behavior through an epidemic model and
solved a one-sided partially observable stochastic game for
cyberdeception where a defender distributes a limited number
of honeypots.

Wagener et al. [79] addressed the issue of a honeypot
being overly restrictive or overly tolerant by proposing a self-
adaptive, game theoretic honeypot. The authors modeled the
game between an attacker and a defender by reusing the deﬁni-
tions proposed in [80]. They applied game theory to compute
the optimal strategy proﬁles based on the computation of Nash
Equilibrium. The game theory directs the conﬁguration and
a reciprocal action of the high-interaction honeypot. In their
experiment, this technique is capable of deriving an optimal
strategy based on Nash equilibrium with rational attackers.

11

Aggarwal et al. [81, 82] discussed a sequential, incomplete
information game to model the attackers’ decision-making in
the presence of a honeypot and combined this model with
instance-based learning (IBL). The authors introduced two
timing-based deception by applying early or late deception
in the rounds of games and investigated the effect of timing
and the extent of deception. Their results showed that high
amount of using deception and late timing of deception can
effectively decrease attack actions on the network. However,
using a different timing and amount of deception did not
introduce any difference in attacking on a honeypot action.

Various honeypot allocation methods for deception over
attack graphs have been studied based on game theoretic
approaches that considers uncertainty using partially observ-
able MDP (POMDP) and stochastic games (POSG) [83, 84].
Anwar et al. [85] developed a static game model to protect a
connected graph of targeted nodes against an attacker through
honeypot placements based on node features and signiﬁcance.
The game model is extended to study the game dynamics as a
stochastic allocation game in [86]. To resolve high complexity
of attack graph-based honeypot allocation games, the authors
proposed a heuristic approach.

Nan et al.

[87] and Nan et al.

[88] provided Nash
equilibrium-based solutions for a defender to intelligently
select the nodes that should be used for performing a com-
putation task while deceiving the attacker into expending
resources for attacking fake nodes. Nan et al. [87] investigated
a two-player static game of complete information with mixed-
strategy, where the system selects a node to place the deception
source and the attacker selects a node to compromise. Nan
et al. [88] constructed a static game with mixed-strategy, where
both players select one strategy (target node) with probability
distribution. The defender selects a node to install defense
software while the attacker selects a target node to compromise
and avoid being detected by the anti-malware software.

Pros and Cons: A honeypot is the most popular defensive
deception technology which has matured over decades. Since
its aim is not
to introduce any additional vulnerabilities,
if an attacker is successfully lured by the honeypots, the
honeypots can protect the existing system components (i.e.,
system assets) while collecting additional attack intelligence
which can lead to improving intrusion detection with new
attack signatures. However, maintaining honeypots incurs ex-
tra cost. In addition, there is still a potential performance
degradation due to the existence of the honeypots, such as
additional routing paths or resource consumption. However,
the drawbacks of honeypots have been little investigated. In
addition, as more intelligent, sophisticated attackers have been
emerged recently, it becomes more challenging to develop
realistic honeypots for effectively deceiving attackers in terms
of both complexity and cost.

2) Honeywebs: El-Kosairy and Azer [89] proposed a web
server framework that provides a web application ﬁrewall with
a honeyweb to detect malicious trafﬁcs and forward suspi-
cious trafﬁcs to honeypot servers. This work formulated the
interactions between the attacker (i.e., malicious trafﬁcs) and
the defender (i.e., honey servers) using a game of imperfect

information (i.e., players do not know their opponent’s moves)
and complete information (i.e., players know their opponent’s
type, action, and payoff function) because they consider si-
multaneous moves by both the attacker and defender. Typi-
cally, this research combines deception technologies, including
honey token, honeypot, and decoy document. In the game
deﬁnition, the main action of a defender is implementing a
virtual machine (VM) to consume the attacker’s effort and
resources.

Pros and Cons: Honeywebs are rarely used particularly
based on game-theoretic approaches. They are used to assist
honeypots along with honey tokens or honey ﬁles. Developing
fake webpages incurs less cost while its role is more like an
assistant to support the honeypots. Hence, honeywebs may not
be used without other defensive technologies.

3) Honeynets: Garg and Grosu [90] considered an attacker
and a honeynet system (i.e., a defender) as players in a
strategic non-cooperative game. The framework is based on
information. In their game
extensive games of imperfect
model, the defender makes deception about the placement of
a honeypot while the attacker can probe the target host to
identify their true roles with some probability. The attacker’s
utility considers the cost of both probing and compromising a
host or honeypot. They studied the mixed strategy equilibrium
solutions of these games and showed how they are used to
determine the strategies of the honeynet system.

Dimitriadis [91] proposed a honeynet architecture based on
an attacker-defender game. This architecture is called 3GH-
NET (3G-based Honeynets) and aimed to enhance the security
of the 3G core network by defending against DDoS and node
compromise attack. The main components of this 3GHNET are
two gateways with a set of strategies to control and capture
the data ﬂow between two nodes. In their emulation, each
gateway and attacker are considered as individual players. The
3GHNET-G is a two-player, non-cooperative and zero-sum
game. Leveraging on Nash equilibrium, the authors calculated
the payoff matrix to assist the players to search for the optimal
strategy.

Pros and Cons: The honeynet is a system architecture and
mainly designed to deal with inside attackers and allow a
system manager to monitor threats and learn from them. The
Honeynet Project [92] is a typical example of a honeynet
consisting of multiple honeypots applied in practice. Since the
honeynet works with multiple honeypots, how to optimally de-
ploy and work together among them needs more investigation
based on both the effectiveness of deception and additional
deployment costs.

4) Obfuscation: Shokri [93] modeled a leader-follower
game (i.e., Stackelberg game) between a designer of an obfus-
cation technique and a potential attacker and designed adaptive
mechanisms to defend against optimal inference attacks. They
assumed that the users plan to protect sensitive information
when sharing their data with untrustful entities. This design
allows the users to obfuscate the data with noises before
sharing it. The attacker can observe valuable information of
users and noises caused by obfuscation. They provided linear
program solutions to search for an optimization problem that

12

provably achieves a minimum utility loss under those privacy
bounds.

Hespanha et al. [94] proposed a defensive deception frame-
work based on non-cooperative, zero-sum, stochastic games
with partial information. They analyzed how a defender in
a competitive game can manipulate information available to
its opponents to maximize the effectiveness of the defensive
deception. They found that there exists an optimal amount of
information to be presented to effectively deceive an attacker.

Pros and Cons: Compared to other defensive deception
techniques, such as honey-X techniques, the key beneﬁt of
data obfuscation is easy deployability with low cost. However,
adding noise into normal
information can also confuse a
defender or a legitimate user. On the other hand, most data
obfuscation research mainly aims to develop a technique of
how to hide real information rather than how to detect an
attacker.

5) Deceptive Signals: In many game-theoretic approaches
that consider defensive deception as a defense strategy, de-
ception is simply used as a signal to deceive an opponent
player. Pawlick et al. [48] modeled a game of three players,
consisting of a cloud defender, an attacker and a device, in
a cloud-based system that can deal with advanced persistent
threats (APTs). The authors designed the so-called FlipIt game
to model the interactions between an attacker and a defender
where signaling games are used to model the interactions
between the device and the cloud which may be compromised
with a certain probability. Xu and Zhuang [60] designed a
game between an attacker and a defender where the attacker
has the ability to investigate the vulnerability of a target and
the defender can apply defensive technologies to change the
vulnerability of a certain device. This work mainly studied
how the attackers’ preparation for launching an attack affects
the effectiveness of defensive deception strategies. As the re-
sult, the authors applied the subgame perfect Nash Equilibrium
(SPNE) to analyze the strategic interactions of the terrorist’s
costly learning and the defender’s counter-learning.

Yin et al. [95] leveraged a fake resource or converted a real
resource to secrete recourse to mislead attackers. They applied
a Stackelberg game to model the interactions between an
attacker and a defender. The authors conducted the mathemat-
ical analysis of a game using both pure and mixed strategies
played by the defender. They provided an optimal strategy for
the defender and calculated the accuracy rate of identifying
when the attacker can deploy unlimited surveillance before
attacking. Hor´ak et al. [96] used one-sided POSG (partially
observable stochastic games), in which a defender has perfect
information while other players have imperfect information. In
this work, an attacker and defender take sequences of actions
to either deceive an opponent or attempt to obtain true actions
taken by the opponent. This work assumed that a rational
attacker has knowledge towards its opponent and its actions
to take. However, the attacker has no knowledge of a network
topology which introduces a hurdle to identify a target and
whether the defender is aware of its presence in the network.
Ferguson-Walter et al. [17] proposed a hypergame-based
defensive deception scenario where players have imperfect

and incomplete information. Based on the key concept of a
hypergame, the authors modeled an individual player’s own
game where the game structure and payoffs may be manip-
ulated by an opponent player. However, this work assumed
an asymmetric view by the attacker and defender in that
the attacker does not know the defenders taking defensive
deception strategies while the defender can know the attacker’s
true payoff and game structure.

Bilinski et al. [97] introduced game-theoretic deception
procedures based on a masking game in which a defender
masks the true nature of a device. In this game, an attacker
can ask the defender whether a given device is real or fake
at each round of the game. The defender needs to pay cost
if it lies. After several rounds, the attacker needs to choose a
target to attack. The authors investigated this scenario by con-
ducting a mathematical analysis for non-adaptive and adaptive
game models. They also designed a Stackelberg game model
where the attacker is a leader and the defender is a follower.
Through an Markov Decision Process (MDP) simulation, they
investigated the potential behavior of an attacker at noncritical
points.

Pros and Cons: This deceptive signal is an abstract way
to apply a game theory concept to model a deception game
between an attacker and a defender. Due to the nature of the
abstracted game formulation, it has a high ﬂexibility that can
use various types of deception techniques. On the other hand,
it is quite challenging to model a cybersecurity problem in a
particular context as the proposed deception game framework
does not use speciﬁc defensive deception techniques.

6) Multiple Defensive Deception Techniques: Some exist-
ing approaches leverage a set of cyberdeception technologies
to model a set of strategies by a defender. Chiang et al. [98]
discussed defensive deception to improve system security and
dependability based on an SDN environment by utilizing a
set of honey technologies. The authors discussed what are the
critical requirements to realize effective defensive deception
and identiﬁed promising evaluation methods including metrics
and evaluation testbeds. Chiang et al. [98] constructed a dy-
namic game of complete information where the attacker does
not know the strategies chosen by defender. Huang and Zhu
[99] discussed several defensive deception techniques, such as
honeypots, fake personal proﬁles, or multiple game models.
They used a static Bayesian game to capture stealthy and
deceptive characteristics of an attacker and advance this model
by applying asymmetric information one-shot game. They
demonstrated the performance of their proposed techniques
under APT based on the Tennessee Eastman (TE) process as
their case study.
Pros and Cons: Since multiple defensive deceptions are
combined and used as a set of a defender’s strategies, the
defender can make more choices to defend against attackers
based on different merits that can introduce to different types
of attacks. In particular, to deal with APT attackers which can
perform multi-staged attacks, using various types of defensive
deception can provide more relevant resources to deal with
them. However, combing more than one deception techniques
introduces additional deployment costs. In addition, it is not

13

trivial to identify a optimal combination of multiple defensive
deception techniques.

7) Fake Objects: Mohammadi et al. [57] analyzed two-
player signaling games with a defensive fake avatar for select-
ing an optimal strategy under various scenarios. A defender
can use an avatar in the signaling game as a fake internal user
to identify an external attacker by interacting with external
users. This game considered the payoff of two players and
derived an optimal threshold of raising alarms. In their exper-
iment, the expected payoff is used a metric to guide actions of
the avatar or the defender. Unlike other works using signaling
games [47, 48], their signaling games put the defender as a
second mover (i.e., a receiver) while an attacker is a ﬁrst mover
(i.e., a sender) where the games are played with incomplete
information. The key idea of using the signaling games is
creating uncertainty for the attacker because the defender uses
a fake identity which can make the attacker doubtful about
whether or not the receiver is a real user.

Casey et al. [100] discussed interactions between an insider
attacker and a defender of an organization. They considered
a hostile agent that may obtain organization surface and harm
the whole system. In order to mitigate this kind of threats, the
authors proposed a honey surface to confuse the malicious
agent by designing a basic compliance signaling game to
model the interaction between agents and the organization.
Casey et al. [101] further discussed the insider threat
in
an organization and applied a signaling game to model the
interactions between agents with learning intelligence and a
defender.

Thakoor et al. [102] introduced a general-sum game, named
Cyber Camouﬂage Games (CCGs), to model the interactions
between a defender and an attacker performing reconnais-
sance attacks. The defender can mask the machine in the
network with fake information, such as an operating system,
to mitigate the effect of reconnaissance attacks. To identify an
optimal strategy, they introduced the Fully Polynomial Time
Approximation Scheme (FPTAS) given constraints and applied
Mixed Integer Linear Program (MILP) to search for an optimal
strategy.

[103]

Zhu et al.

simulated the inﬁltration of social
honeybots-based defense into botnets of social networks. The
authors proposed a framework, so-called SODEXO (SOcial
network Deception and EXploitation) consisting of Honeybot
Deployment (HD), Honeybot Exploitation (HE), and Protec-
tion and Alert System (PAS). HD is composed of a moderate
number of honeybots. The HE considered the dynamics and
utility optimization of honeybots and botmaster by a Stack-
elberg game model. The PAS chose an optimal deployment
strategy based on the information gathered by honeybots.
The results showed that a small number of honeybots can
signiﬁcantly decrease the infected population (i.e., a botnet)
in a large social network.

Pros and Cons: Although using fake identities or avatars
as a defensive deception technique is less costly and rela-
tively simple, it can introduce confusion to normal users and
accordingly increase false alarms. However, little work has
investigated the adverse effect of using fake identities, such as

introducing confusion for legitimate users or producing extra
vulnerabilities. In addition, the effect of intelligent attackers
with high deception detectability has not been studied.

8) Honey Patches: A honey patch mainly has two compo-
nents: (1) A traditional patch to ﬁx known software vulnera-
bilities; and (2) additional code to mislead an attacker to fake
software vulnerabilities [104]. In the literature, fake patches
are also called honey patches [105] or ghost patches [104].

Araujo et al. [105] coined the term honey patches that can
function as same as a regular patch; but it has the ability
to efﬁciently redirect an attacker to a decoy, and allow the
attacker to achieve a fake success. The game used in [105] is
a dynamic game of incomplete information where the payoff
of attacker is unknown to defender. In addition, the authors
provided a strategy allowing a web server administrator to
transfer regular patches into honey patches. Enterprise scale
systems are often exposed by the vulnerabilities due to the lack
of boundary checking and subtle program misbehavior [104].
A common way to eliminate the vulnerabilities is releasing
security patches. However, as the security patches contain the
location and types of vulnerability to be patched, an attacker
could obtain a blueprint of system vulnerabilities by analyzing
the security patches from the past. As a solution, Avery and
Spafford [104] provided a fake patch technique for misleading
the attackers that try to analyze the historical patches. In
particularly, this fake patch technique is designed to protect
the patch that ﬁxes input validation vulnerabilities. This fake
patch can contain two parts. One is creating a decoy patch
ﬁle to seduce the attacker and attract the attacker’s attention
away from the real patch ﬁle and the other is to alert defenders
of potential intrusions or exﬁltration attempts. This decoy ﬁle
is generated by a modiﬁed technique [34]. Another part is
a bogus control ﬂow for programs. This alternation misleads
the reverse engineering, but does not change the output of the
program. In their experiment, this technique was evaluated
in [106] where the program runtime and program analysis
measure the effectiveness of this technique. The game and
its components in [104] were not rigorously presented since
it mainly focused on the technicalities of developing fake
honeypots. However, the deception and the evaluation of its
reward represents a normal form one-shot game. Such games
were clearly presented in [107] where the authors evaluated the
effectiveness of three deceptive patches (i.e., faux, obfuscated,
and active response) and applying them into the proposed
game-based module which provides the security guidelines.
In addition, they found that many techniques are unable to
meet the proposed security deﬁnition while emphasizing the
importance of assessing deceptive patches based on a clear
and meaningful security deﬁnition.

Cho et al. [16] modeled a deception game based on hyper-
game theory in which an attacker and a defender have different
perceptions in a given game. This work examined how a
its decision making to
player’s (mis)perception can affect
choose strategies to take, which can affect the player’s utility in
the given game. This work used Stochastic Perti Nets to build
a probabilistic model of the hypergame where the defender
uses fake patches as a deception strategy for misleading the
attacker to believing in the fake patches and choose a non-

14

vulnerable node as a target to compromise, which can lead
the attack failure.

Pros and Cons: Honey patches are known as effective decep-
tion techniques to deceive attackers with low cost. However,
as discussed in [107], when some systematic and meaningful
security assessments are conducted to validate the security
of the honey patches, some honey patches fail to pass those
security criteria, such as indistinguishablity between real and
fake patches, vulnerabilities of fake patches, or cryptographic
breakability for obfuscated messages [107].

9) Deceptive Network Flow: Clark et al. [55] proposed the
deceptive network ﬂow, representing the ﬂow of randomly
generated dummy packets. They assumed that both of real
and deceptive packets are encrypted. Hence, an attacker cannot
distinguish between them and may spend limited resources on
targeting a false ﬂow. This work used this deceptive ﬂow to
lure the attacker and waste its resource to assist those real
packets to protect the network from jamming attacks. The
main challenges of developing defensive network ﬂow are: (1)
applying deceptive packets may increase the risk of congestion
and incur extra delay for the delivery of real packets; and
(2) the source node has a limited capacity to generate and
transfer packets, requiring a balance between real and fake
ﬂows. To mitigate this adverse effect, the authors designed a
two-stage game model to obtain deception strategies at pure-
strategy Stackelberg equilibrium. They considered two types
of source nodes: (1) selﬁsh nodes aiming to maximize its own
utility; and (2) altruistic nodes considering the congestion of
other sources when choosing a ﬂow rate. Their results proved
that altruistic node behavior improves the overall utility of
the sources. Similarly, Zhu et al. [56] considered a single
source selecting routing paths for real and deceptive ﬂows.
Before sending a deceptive ﬂow, the source node is allowed
to choose the rate of deceptive and real ﬂow as well as the
path of the deceptive ﬂow. This work introduced the solution
concepts, such as the path Stackelberg equilibrium (PSE), the
rate Stackelberg equilibrium (RSE), and their mixed strategy
counterparts of the game. Their results proved that there exist
such equilibria. Miah et al. [108] designed a deceptive network
ﬂow system, called Snaz, to mislead the attacker performing
reconnaissance attacks. They model the interaction between
the attacker and the defender with a two players non-zero-
sum Stackelberg game. Sayin and Bas¸ar [109] proposed a
deceptive signaling game framework to deal with APT attacks
in cyber-physical systems. Under the attacks aiming to obtain
system intelligence via scanning or reconnaissance attacks,
this work crafted bait information to lure the attackers. This
work leveraged the concept of game-theoretic hierarchical
equilibrium and solved a semi-deﬁnite programming problem
where the defender does not have perfect information by con-
sidering partial or noisy observations or uncertainty towards
the attacker’s goal.

Pros and Cons: Deceptive network ﬂow is a new defensive
deception approach to effectively mitigate some malicious
actions which cannot be defended by traditional defensive de-
ception technologies, such as malicious network ﬂow scanning
and ﬁngerprint. However, it is highly challenging to balance

fake and real network ﬂows that minimizes any additional
network performance degradation.

In Table III, we summarized the ﬁve key aspects (i.e.,
deception techniques, categories, expected effects, attacks
countermeasured, and application domain) of a given game-
theoretic defensive deception techniques surveyed in this work.

D. Empirical Game Experiments Using Human Subjects

As we observed in Section IV-C, most deception game-
theoretic works have been studied based on simulation
testbeds. However, some cognitive decision making scientists
also conducted empirical experiments using human subjects
which are assigned as attackers or defenders to consider a
deception game. Cranford et al. [118, 119] conducted an
empirical experiment to realize a signaling game between an
attacker and a defender where the defender uses deceptive
signals. In this experiment, the players are humans with limited
cognition, reﬂecting bounded rationality in game theory. To
measure the effectiveness of defensive deception techniques,
Ferguson-Walter et al. [120] and Ferguson-Walter [121] also
conducted an empirical experiment with 130 red team mem-
bers as participants in a network penetration study at different
deception scenarios. For instance, different types of decoy
devices are described explicitly for the existence of deceptive
defensive techniques to the participant. The results showed that
when the participants are aware of the deception, they took
much more time before taking any action. This shows that
deception made them slower to move and attack. Aggarwal
et al. [122] developed a simulation tool, called HackIt, to study
attacks in a network reconnaissance stage where participants
studied the effect of introducing deception at different timing
intervals. Their results showed that the attacker performed
attacks on the honeypots more often than real machines.

Pure game theorists may argue that this type of empirical
game experiments is not a game-theoretic deception game.
However, we discuss these empirical deception game studies to
give a chance to be aware of human subjects-based empirical
game experiments that can at least partly follow the structure
of a traditional game theory, consisting of two or more
players, their bounded rationality, their opponent strategies and
corresponding utilities that can affect their decision making.

V. MACHINE-LEARNING-BASED DEFENSIVE DECEPTION
(MLDD) TECHNIQUES

ML-based applications become more popular than ever in
various domains, including cybersecurity. ML techniques have
been extensively adopted for automating attacks and learning
system behaviors in the cyberdeception domain [123]. In this
section, we ﬁrst discuss the key steps to implement ML-based
approaches and conduct an extensive survey on ML-based
defensive deception techniques in the literature.

A. Key Steps of Implementing ML-Based Defensive Deception

Like other ML-based applications, ML-based deception
techniques also commonly used the following steps to detect
malicious activities:

15

• Dataset Generation: ML-based approaches have been
mainly used in detecting attackers or identifying malicious
activities based on the information collected in honeynets
or honeypots [124].

• Dataset Collection: The success of ML techniques hugely
depends on whether reliable datasets are available or not.
However, obtaining reliable cybersecurity datasets is not
trivial based on the following reasons: (i) such datasets are
often conﬁdential due to security reasons of a company
or an organization owning the datasets; and (ii) there are
inherent unknown attacks which make it difﬁcult to obtain
accurate and complete annotated datasets. Cybersecurity
related datasets have been more available as more security
applications are using ML-based approaches [125].

• Pre-Processing: Datasets should be carefully pre-processed
ahead of the learning step to eliminate unwanted noises
from the dataset. This includes data cleaning, editing, and
reduction [126].

• Feature Extraction: It is challenging to analyze complex
data because of the large number of variables involved. The
goal of feature extraction is to construct a combination of
the relevant variables to describe the data with sufﬁcient
accuracy.

• Training: Training dataset is a dataset of examples used dur-
ing the learning process and is used to learn the parameters
of the detector [126, 127].

• Testing: A testing dataset is a set of examples used to
evaluate the performance (i.e, detection accuracy) of a
trained detector [127].

• Evaluation: Independent and new data are used to evaluate
the performance of a detector to avoid overﬁtting problems
in the data [127].

B. Common ML Techniques Used for Defensive Deception

We discuss popular ML techniques used to develop various
DD techniques in the literature. For the easy understanding of
readers who may not have enough background on machine
learning, we give the brief overview of the following ML
techniques:
• Support Vector Machine (SVM): SVMs can deal with a
set of supervised learning models and be applied to solve
classiﬁcation and regression problems [140]. SVM uses the
hypothesis space of linear functions in a high dimensional
feature space to analyze data. SVM is popular due to its
simple training process. However, SVM is not efﬁcient for
large or highly noisy datasets

• K-Means: This technique uses an iterative reﬁnement to
partition n observations into k clusters and guarantee that
each observation belongs to a cluster with the nearest
mean [141].

• Expectation Maximization (EM): This is a statistical method
to search for maximum likelihood parameters. It consists
of two major steps: (1) an expectation step, which creates
a function for the expectation of the likelihood evaluated
using the current estimate for the parameters; and (2) a
maximization step, which computes a new estimate of the
parameters [142]

Ref.

[16]

[17]

[45]

[46]

[47]

[48]

[55]

[56]

[57]

[68]

[69]

[70]

[71]

[72]

[74]

[75]

[79]

[81]

[82]

[83]

[85]

[86]

[87]

[89]

[90]

[91]

[94]

[95]

[96]

[97]

[98]

[99]

Deceptive
signal
Honeypot

Honeypot

Honeypot

Deceptive
signal
Dummy packet
generation
Deceptive
network ﬂow
Fake avatar

Honeypot

Honeypot

Honeypot

Honeypot

Honeypot

Honeypot

Honeypot

Honeypot

Honeypot

Honeypot

Deceptive
signal
Honeypot

Honeypot

Honeypot

Honeywebs

Honeynet

Honeypot

Obfuscation

Deceptive
signal
Deceptive
signal

Deceptive
signal
Honey-X
technologies

Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection

Asset protection

Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection

Asset protection;
Attack detection
Asset protection

Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection

Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection

Asset protection

Asset protection;
Attack detection

Asset protection;
Attack detection
Asset protection;
Attack detection

TABLE III
SUMMARY OF GAME-THEORETIC-BASED DEFENSIVE DECEPTION TECHNIQUES

DD technique

Goal

Tactic

Expected effect

Main attacks

Fake patch

Asset protection

Mimicking; False
information; Lies
Mimicking; Decoying

Luring; Confusing;
Misleading
Luring; Misleading

APT

NC

Game type

Hypergame

Hypergame

Mimicking; Decoying

Luring; Misleading

NC/Probing

General-sum

Mimicking; Decoying

Luring; Misleading

Mimicking; Decoying

Luring; Misleading

Mimicking; Masking;
Lies; Decoying
Mimicking; Masking

Asset protection

Mimicking; Masking

Asset protection

Mimicking; Decoying

Misleading; Luring;
Confusing
Misleading; Hiding;
Confusing
Misleading; Hiding;
Confusing
Asset protection

Mimicking; Decoying

Luring; Misleading

Mimicking; Decoying

Luring; Misleading

Mimicking; Decoying

Luring; Misleading

Mimicking; Decoying

Luring; Misleading

Mimicking; Decoying

Luring; Misleading

Mimicking; Decoying

Luring; Misleading

Mimicking; Decoying

Luring; Misleading

Mimicking; Decoying

Luring; Misleading

Jamming

Stackelberg

Wireless

APT

NC

APT

Bayesian

Signaling

Three-player signaling

Jamming

Multi-stage stochastic

NC

NC

NC

NC

DoS

APT

APT

NC

NC

Signaling

Signaling

General-sum

Bayesian

Signaling

General-sum

Static

Stackelberg

General-Sum

Mimicking; Decoying

Luring; Misleading

NC/probing

Mimicking; Decoying

Luring; Misleading

NC/probing

Non-cooperative
sequential; incomplete
information
Sequential

Mimicking; Masking;
Decoying
Mimicking; Decoying

Misleading; Luring;
Confusing
Luring; Misleading

Mimicking; Decoying

Luring; Misleading

Mimicking; Decoying

Luring; Misleading

Decoying; Mimicking

Decoying; Mimicking

Decoying; Mimicking

Masking

Masking; Camouﬂaging;
Mimicking
Misleading; Hiding;
Mimicking; Decoying

Misleading; Hiding;
Mimicking; Decoying
Misleading; Hiding;
Mimicking; Decoying

Blending; Misleading;
Luring
Asset protection; Attack
detection
Blending; Misleading;
Luring
Misleading; Confusing

Hiding; Blending,
Hiding; Misleading
Misleading; Hiding;
Luring; Confusing;
Blending
Asset protection; Attack
detection
Misleading; Hiding;
Luring; Confusing;
Blending
Misleading; Hiding;
Luring; Confusing;
Blending
Hiding; Misleading

NC/Probing

Stochastic with POMDP

NC

NC

NC

Web attack

NC

DDoS

NC

Reconnaissance

NC

NC

APT

APT

Insider

Insider

Recon.

Stochastic

Stochastic

Static

Complete, imperfect
game
Zero-sum static

Zero-sum static

Non-cooperative
stochastic
Stackelberg

Stochastic

Stackelberg

General-sum

Dynamic Bayesian

Signaling

Signaling

General-sum

Deceptive
signal

Asset protection;
Attack detection

Misleading; Hiding;
Mimicking; Decoying

[100]

Honey surface

Asset protection

Mimicking; Decoying

[101]

Honey surface

Asset protection

Mimicking; Decoying

Hiding; Misleading

[102]

Fake objects

Asset protection

Mimicking; Decoying

Hiding; Misleading

[103]

[109]

[110]

Social
honeypots
Deceptive
network ﬂow
Honeypot

Asset protection;
Attack detection
Asset protection

Asset protection;
Attack detection

Decoying; Mimicking

Mimicking; Masking

Decoying; Mimicking

Blending; Misleading;
Luring
Misleading; Hiding;
Confusing
Blending; Misleading;
Luring

Social bots

Stackelberg

Recon./Probing

APT

Non-zero-sum
Stackelberg
POMDP

16

Domain

No domain
speciﬁed
No domain
speciﬁed
CPS

SDN

No domain
speciﬁed
Cloud Web

No domain
speciﬁed
No domain
speciﬁed
No domain
speciﬁed
No domain
speciﬁed
IoT

No domain
speciﬁed
No domain
speciﬁed
IoT

No domain
speciﬁed
No domain
speciﬁed
No domain
speciﬁed

No domain
speciﬁed
IoT

IoT

IoT

IoT

Cloud Web

No domain
speciﬁed
Wireless

No domain
speciﬁed
No domain
speciﬁed
No domain
speciﬁed

No domain
speciﬁed
SDN

No domain
speciﬁed

No domain
speciﬁed
No domain
speciﬁed
No domain
speciﬁed
No domain
speciﬁed
CPS

SDN

TABLE III
CONTINUED: SUMMARY OF GAME-THEORETIC-BASED DEFENSIVE DECEPTION TECHNIQUES

DD technique

Goal

Tactic

Expected effect

Main attacks

Ref.

[111]

[112]

[113]

[114]

Honeypot

Deception
signal
Honeypot

Honeypot

[115]

Fake nodes

[116]

Fake objects

[117]

Feature
deception

Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection;
Attack detection
Asset protection

Decoying; Mimicking

Decoying; Mimicking

Decoying; Mimicking

Mimicking; Decoying

Blending; Misleading;
Luring
Blending; Misleading;
Luring
Blending; Misleading;
Luring
Luring; Misleading

Decoying

Hiding; Misleading

APT

NC

APT

NC

APT

Mimicking; Decoying

Hiding; Misleading

Recon.

Asset protection

Lies

Misleading

Reconnaissance

TABLE IV
SUMMARY OF MACHINE LEARNING-BASED DEFENSIVE DECEPTION TECHNIQUES

Game type

POMDP

Stackelberg

Q-learning-based

Zero-sum static

POMDP

Signaling

RL-based feature
deception

Ref.

[128]

DD technique

Goal

Honeypot

Attack detection

Tactic

Mimicking

Expected effect

Attack detection

Main attacks

ML technique

Anomaly network
trafﬁc

Fake liker
(crowdturﬁng)
Malicious proﬁle

Call fraud; Identity
theft
Social spammers

C4.5, Bayesian
Network, Decision
Table, and
Na¨ıve-Bayes
Supervised ML

ML toolkit

ML toolkit

ML toolkit

Social spammers

ML toolkit

Luring

Luring

Luring

Luring

Blending;
Misleading; Luring
Luring; Confusing

APT

Insider

[129]

Social honeypot

Attack detection

Mimicking

[130]

Social honeypot

[131]

Honeypot

[132]

Social honeypot

[133]

Social honeypot

[134]

[135]

[136]

[137]
[138]

[139]

Decoy

Bait-based
Deception
Bait-based
Deception
Honey proﬁle
Bait-based
Deception
Decoy data

Mimicking; Decoying

Luring; Misleading

Attacker
identiﬁcation
Asset protection;
Attack detection
Identifying
spammers
Identifying
spammers
Asset protection;
Attack detection
Asset protection;
Attack detection
Attack protection

Mimicking

Mimicking; Decoying

Mimicking; Decoying

Decoying; Mimicking

Mimicking; Lies

Mimicking; Lies

Luring; Confusing

Zero-day

Asset protection
Attack detection

Mimicking; Decoying
Mimicking

Hiding; Misleading
Luring

Spamming
Zero-day

Attack detection

Lies

Misleading

Data leakout

RNN

SVM

NLP

SVM
NLP

DL

17

Domain

SDN

Wireless

No domain
speciﬁed
IoT

No domain
speciﬁed
No domain
speciﬁed
No domain
speciﬁed

Domain

No domain
speciﬁed

OSN

No domain
speciﬁed
No domain
speciﬁed
OSN

OSN

CPS

No domain
speciﬁed
No domain
speciﬁed
OSN
No domain
speciﬁed
No domain
speciﬁed

• Hierarchical Grouping: This is a well-established classiﬁ-
cation method for cluster analysis which seeks to build a
hierarchy of clusters [143]. For example, plants and animals
may be grouped into a smaller number of mutually exclusive
classes with respect to their genetic characteristics.

• Bayesian Network (BayesNet): This is a classiﬁer that learns
the conditional probability of each attribute from training
data while assuming strong independence [144]. That is,
the underlying probability model would be an independent
feature model such that the presence of a particular feature
of a class is unrelated to the presence of any other features.
For example, BayesNets could represent the probabilistic
relationships between diseases and symptoms.

• Decision Tree (DT): DT is a widely used approach for
multistage and sequential decision making. The key idea
of DT lies in breaking up a complex decision into a union
of several simpler decisions, leading to the intended desired
solution [145].

• C4.5 Algorithm: This is a method to generate a decision
tree based on the concept of information entropy in which
the DT generated by C4.5 is widely used for data classiﬁ-
cation [146].

• Na¨ıve-Bayes Algorithm: This is a widely applied classiﬁ-

cation algorithm based on Bayesian Theory and statistical
analysis [147]. This algorithm uses the prior probability and
posterior probability of the training data to calculate the
probability of type (posterior probability) of a given sample.
• Deep Neural Networks: DNN is a neural network with at
least one hidden layer and it has been used as a framework
for deep learning. For each node (neuron), its output is
a non-linear function of a weighted sum of its inputs.
The DNN model is trained by back propagation, which
changes the weight of each node. DNNs have been used for
natural language processing, image recognition, and disease
diagnosis.

C. ML-Based Defensive Deception

In this section, we discuss ML-based defensive deception
techniques that have been discussed in the literature. We
discuss the following DD techniques along with what ML
techniques are used to develop them: Social honeypots, honey
ﬁles, honeypots or decoy systems, bait-based deception, fake
services, obfuscation, and decoy data.

1) Social Honeypots: In online social networks (OSNs),
the so-called social honeypots, as good bots in contrast to bad
bots, have been studied by creating social network avatars.

Lee et al. [132] proposed an ML-based mechanism to
discover social spammers. The authors used social honey-
pots as a defensive deception technique to monitor spammer
activities. The data from spam and legitimate proﬁles are
used to improve an ML-based classiﬁer. Thus, the system
can classify spam proﬁles automatically. The paper compared
true positive rate (TPR) and false negative rate (FNR) of
various ML-based classiﬁers to show the effectiveness of
the proposed mechanism. Lee et al. [133] developed social
honeypots to detect content polluters in Twitter. Sixty social
honeypot accounts are created to follow other social honeypot
accounts and post four types of tweets to each other. This
work categorized the collected user features into nine classes
based on the Expectation-Maximization (EM) algorithm. They
classiﬁed the content polluters based on Random Forest and
enhanced the results by using standard boosting and bagging
and different feature group combinations.

El Idrissi Younes et al. [130] developed social honeypots
to discover malicious proﬁles. The authors employed feature-
based strategies as well as honeypot feature-based strategies to
collect data of the malicious proﬁles and analyzed the collected
data based on a suite of ML algorithms provided by Weka
ML toolkit (e.g., logistic regression, Bayes classiﬁer, support
vector machine (SVM), K-Means, Expectation Maximization,
or hierarchical grouping). Zhu [148] introduced the concept of
‘active honeypots’ as active Twitter accounts that can capture
more than 10 new spammers every day. They identiﬁed 1,814
accounts from the Twitter and examined the key features of
active honeypots. In addition, using a suite of ML algorithms,
such as SVM, logistic regression, J48 Tree, Bagging, and
AdaBoost M1, the authors examined the impact of unbalanced
datasets on the detection accuracy of the different ML algo-
rithms.

Badri Satya et al. [129] also used social honeypot pages to
collect fake likers on Facebook where fake likes are provided
by paid workers. The authors obtained four types of user
proﬁles and behavioral features as the distinctive patterns
of fake likers. This work evaluated the robustness of their
ML-based detection algorithms based on synthetic datasets
which have been modiﬁed to reﬂect individual and coordinated
attack models. Yang et al. [149] proposed a passive social
honeypot to collect a spammer’s preferences by considering
various behaviors of the social honeypot. The considered
tweet design features include tweet frequency, tweet keywords,
and tweet topics as well as the behaviors of famous users’
accounts and application installation. The authors conducted
in-depth analysis on what types of social honeypot features
can introduce a higher rate of collecting social adversaries
and enhanced the social honeypots based on the result. The
improved honeypot has shown 26 times faster than a normal
social honeypot in collecting social adversaries based on the
evaluation using a random forest classiﬁer.

Pros and Cons: The works discussed in [130, 132, 133, 149]
mainly relied on luring deception techniques to attract spam-
mers in OSNs and collect more attack intelligence particularly
in terms of novel behavioral characteristics of spammers.
Some limitations of the existing social honeypots include: (i)

18

the current efforts are mainly to detect spammers, not other
social network attacks (e.g., cybergroomers, human trafﬁcking,
cyberstalking, or cyberbullying); (ii) most features are based
on user behaviors, but not really network topological features
as an attacker’s characteristics; (iii) most detectors are applied
on static datasets to evaluate the detection of spammer. This
implies that there is no theoretical simulation framework that
considers dynamic interactions between an attacker and a
defender; and (iv) there is a lack of studies investigating the
effectiveness of social honeypots in terms of how quickly more
attack intelligence can be collected and what types of attack
intelligence are collected.

2) Honey Proﬁles: Stringhini et al. [137] analyzed 900
honey proﬁles for detecting spammers in three online social
networks, including MySpace, Facebook, and Twitter. The
authors collected users’ activity data for a year where the user
datasets include both spammers’ and legitimate users’ proﬁles
and the honey proﬁles are spread in different geographic
networks. Further, this work captured both spam proﬁles and
spam campaigns based on the shared URL using machine
learning techniques (e.g., SVM).

Pros and Cons: Honey proﬁles allow identifying a large set
of attackers, such as a large-scale spam bots, which lead to the
detection of large-scale and coordinated campaigns. However,
the existing spam detectors using honey proﬁles is not generic
and needs to be tailored depending on a different social media
platform.

3) Honeypots or Decoy Systems: Nanda et al. [128] used a
suite of machine learning algorithms to train historical network
attack data in software-deﬁned networks for the development
of quality honeypots. This ML-based method is to identify
potential malicious connections and attack destinations. The
authors employed four well-known ML algorithms, including
C4.5, Bayesian Network (BayesNet), Decision Table (DT), and
Na¨ıve-Bayes to predict potential victim hosts based on the
historical data. To prevent call fraud and identity theft attacks,
Krueger et al. [131] proposed a method called PRISMA (PRo-
tocol Inspection and State Machine Analysis). This method is
developed to infer a functional state machine and a message
format of a protocol from only network trafﬁc. The authors
experimented to validate the performance of the PRISMA
based on three real-world network traces datasets, including
10,000 to 2 million messages in both binary and textual pro-
tocols. Their experiments proved that the PRISMA provides
the capability to simulate complete and correct sessions with
the learned models and to execute different states for malware
analysis.

Hofer et al. [134] discussed the attributes of cyber-physical
decoys to design a system with these attributes to develop
deception decoys. The authors integrated the deception decoys
into real systems to make them harder to detect and more
appealing as targets. To increase the ﬁdelity of the added decoy
devices to the physical system, three attributes, a protocol,
variables, and a logic of the deployed decoy devices, are
maintained. The authors trained a recurrent neural network
(RNN) using a dataset collected for a year to learn such system
attributes.

Some defensive deception games have been studied by
considering game theoretic reinforcement learning (RL) where
RL is considered in formulating players (i.e., attackers or
defenders)’s utilities. That is, in the RL-based game formula-
tion, players use an RL to identify an optimal strategy where
the RL’s reward function considers gain and loss based on
the player’s belief towards an opponent’s move. RL-based
deception games are studied in [113, 115]. Al Amin et al.
[115] proposed an online deception approach that designs
and places network decoys considering scenarios where a
defender’s action inﬂuences an attacker to dynamically change
its strategies and tactics while maintaining the trade-off be-
tween availability and security. The defender maintains a belief
consisting of a security state while the resultant actions are
modeled as Partially Observable Markov Decision Process
(POMDP). The defender uses an online deception algorithm
to select actions based on the observed attacker behavior
using a POMDP model. This embedded reinforcement learning
the defender belief about
(RL)-based model assumes that
the attacker’s progress is observed through an network-based
intrusion detection system (NIDS). The defender hence takes
actions that attract the attacker toward decoy nodes. Wang
et al. [113] identiﬁed an optimal deployment strategy for
deception resource, such as honeypots. Speciﬁcally, the au-
thors developed a Q-learning algorithm for an intelligent de-
ployment policy for deception resources to dynamically place
deception resources as the network security state changes. This
work considered an attacker-defender game by analyzing an
attacker’s strategy under uncertainty and a defender’s strategies
with several deployment location policies. This work used RL
with a Q-learning training algorithm that identiﬁes an optimal
deployment policy for deception resources.

Pros and Cons: In honeypots, ML-based approach is mainly
used to detect attacks based on the attack intelligence gathered
in honeypots. Hence, this research direction is like developing
an intrusion detection mechanism, rather than creating better
quality defensive deception techniques. Various types of ML-
based techniques can be highly leveraged in order to create
real-like honeypots and its effectiveness can be measured
based on the volume of attackers caught in honeypots and
the diversity of attacker types.

4) Bait-based Deception: Ben Salem and Stolfo [135]
developed a bait-based deception to improve the detection of
impersonation attacks by using the features combining a user
behavior proﬁling technique with a baiting deception tech-
nique. This work used highly crafted and well-placed decoy
documents to bait attackers and deployed a detector that uses
SVM to model the user’s search behavior. Whitham [136] used
bait-based deception technique to study and evaluate honey
ﬁles in military networks. The main challenge in military
networks is the need to utilize third party resources such as
cloud services. Third party servers are directly connected to
the Internet and may store sensitive material and information
which represents a security challenge. The author proposed
three new automated honey ﬁle designs that minimize the
replication of classiﬁed material, yet still remain enticing to
malicious software or user driven searches. Moreover, this

19

work presented an NLP-based content generation design for
honey ﬁles.

Pros and Cons: Although bait-based deception can contribute
to increasing intrusion detection by collecting more intelli-
gence during the time of the deception, there is no performance
guarantees based on the bait-based deception because the
attackers may not be interested in them. In addition, to more
effectively attract attackers, if more important information is
used as a bait, it may introduce some vulnerability or risk when
intelligent attackers can derive clues of system vulnerabilities
based on the bait. Thus, mixing real with fake information
to avoid too high a risk can provide an alternative, such as
semi-bait-based deception.

5) Fake Services: To increase the effectiveness and efﬁ-
ciency of defensive deception techniques, real systems have
been used to deceive attackers. ML-based fake system modiﬁ-
cation is used as a defensive deception mechanism [110, 111]
using POMDP to consider uncertainty in learning attack
behavior. The authors designed a deception server that can
modify IP addresses of fake network via DHCP server, DNS
server, and forwarding ARP requests by appropriate ﬂow
rules to the deception server. In addition, the deception server
is designed to modify the requests and sent it back to the
requesting host. Shi et al. [117] considered the changes of
system features and conﬁgurations to deceive an attacker after
learning the attacker’s preferences and its behavior from attack
data.

Pros and Cons: Proving fake services to attackers can ef-
fectively increase attack cost or complexity which can delay
launching attacks or make the escalation of attack fail. How-
ever, this technique can be deployed based on the assumption
of accurate detection of intrusions. As the current intrusion
detection mechanisms suffer from high false positives in
practice, if a normal user is treated as an intrusion and fake
services are provided to the user, it hinders normal availability
of service provisions to legitimate users.

6) Decoy Data: Machine and deep learning (DL) algo-
rithms have played a key role in developing decoy data,
creating decoy objects or honey information. Abay et al.
[139] took a decoy data generation approach to fool attackers
without degrading system performance by leveraging DL. To
be speciﬁc, their approach employs unsupervised DL to form
a generative neural network that samples HoneyData.

Pros and Cons: Since the defender launches false information
injection (or data disruption) attack while an attacker stays in
a system, this technique will ’scare off’ attackers performing
highly detectable attacks. However, it can also be disruptive
to ordinary users if fake or decoy information is allowed to
remain in the system.

As we observed in the extensive discussions of ML-based
defensive deception techniques above, ML-based defensive
deception has been addressed mainly with the following aims:
(1) Protecting system components by hiding real components
or creating fake things (i.e., honey-X) that look like real; and
(2) Detecting attacks (or identifying attackers) by creating
the
fake objects (e.g., honeypots) and accordingly attract
attackers to collect attack intelligence. Therefore, what and

how ML techniques are used is closely related to the quality
of defensive deception techniques whose success is closely
related to how well it can deceive attackers, representing the
quality of deception.

In Table IV, we summarized the ﬁve key aspects (i.e.,
deception techniques, categories, expected effects, attacks
countermeasured, and application domain) of a given ML-
based defensive deception techniques surveyed in this work.

VI. ATTACKS COUNTERMEASURED BY DEFENSIVE
DECEPTION TECHNIQUES

A variety of attacks have been countered by existing defen-

sive deception techniques as follows:
• Scanning (or reconnaissance) attacks [46, 60, 97, 98, 107,
108, 109, 110]: An outside attacker may aim to obtain a
target system’s information and identify vulnerable system
component to penetrate into the target system. This attack
is often considered as part of the cyber kill chain in APT
attacks. In the literature, the scanning attacks are categorized
as two types as follows:
– Passive monitoring attacks [60, 97, 108]: Leveraging
compromised nodes (e.g., routers or switches), attackers
can scan trafﬁcs passing through the nodes and analyze
packets to gather information about hosts. Passive moni-
toring can make the attackers achieve active nodes’ dis-
covery, OS, roles, up-time, services, supporting protocols,
and IP network conﬁguration [150]. However, passive
monitoring cannot identify services that do not run on
well-known ports or are misdirected without protocol-
speciﬁc decoders [151].

– Active probing attacks [46, 98, 107, 109, 110]: Through
sending probing packets to a potential target,
the at-
tacker can obtain the machine’s information, includes OS,
opened ports, and installed application version. Although
active probing can allow the attackers to collect more
system conﬁgurations, it is relatively easy to be detected
by traditional defensive technologies, such as IDS [151].
• Network ﬁngerprinting [116]: To analyze a target network
and search for an ideal target node, an attacker can use
ﬁngerprint tools to identify speciﬁc features of a network
or a device, such as a network protocol or an OS running
on a remote machine. Common ﬁngerprint tools include
Nmap [152], P0f [153], and Xprobe [154]. Some deception
techniques, such as deceptive network ﬂows and honeypots,
focuses on addressing this kind of attacks.

• (Distributed) Denial-of-Service (DoS) [71, 91]: DoS attacks
are mainly to make legitimate users be denied from proper
services. This attack can overwhelm or ﬂood network ﬂow
to a targeted machine to disrupt normal functions or com-
munications. A common DoS attack is ping ﬂooding by
leveraging ICMP (ping) packets to overwhelm a target [155].
In distributed environments, distributed DoS (DDoS) ﬂood-
ing attacks can be performed with the intent
to block
legitimate users from accessing network resource(s). With
ﬂooding packets to a target, DDoS attackers can exhaust
network bandwidth or other resources, such as a server’s
CPU, memory or I/O bandwidth. Unlike DoS attacks, DDoS

20

attackers can remotely control a large number of devices as
its botnet can continuously send a large amount of trafﬁcs
to a target to block the normal functions and services [156].
• Malware [131]: Malware refers to any software aiming to in-
troduce some damage to a system component, such as server,
client, or network. Honeypot-based deception approaches
have been popularly used for malware analysis [131].

• Privacy attacks [93]: An attacker can identify private in-
formation while training and processing data to develop
defensive deception techniques. Obfuscation is often used
to defend against privacy attacks.

• Advanced persistent threats (APT) [16, 48, 68, 70, 99, 117,
134]: An APT attack is the most well-known sophisticated
attack performing different types of attacks in the stages of
the cyber kill chain (CKC) [37]. In particular, game-theoretic
defensive deception research has considered the APT attack
but mostly focused on the attacks during the reconnaissance
stage.

• Spamming [132, 133, 137, 148, 149]: Online users may re-
ceive unsolicited messages (spam), ranging from advertising
to phishing messages [157].

• Malicious or fake proﬁles [130] (a.k.a. Sybil attacks): OSN
attackers can create a large number of fake identities to
achieve their own selﬁsh goal based on others’ personal
information, such as e-mail, physical addresses, date of
birth, employment date, or photos.

• Loss of conﬁdentiality,

• Fake likers by crowdturﬁng [129]: Human attackers, paid by
malicious employers, can disseminate false information to
achieve the malicious employers’ purposes via crowdsourc-
ing systems [158]. This is called crowdturﬁng and mostly
aimed to spread fake information to mislead people’s beliefs.
integrity, and availability (CIA)
[101, 118, 119]: An insider attacker, called a traitor [159],
can leak out conﬁdential
information to the outside as
a legitimate user. Further, by using its legitimate status
with the authorization of a target device, it can perform
attacks (e.g., illegal access, modiﬁcation or transformation
of conﬁdential information) that can lead to loss of integrity
and availability.

• Impersonation attacks [135]: An inside attacker can mas-
querade a legitimate user’s identity to access resources in a
target system component [159].

• Jamming attack [55, 56, 112]: A jamming attack can be
seen as a subset of DoS attacks. The difference is that
jamming attacks mainly focus on incurring trafﬁcs in wire-
less networks while the DoS attacks can be applicable in
all networks. The jamming attack is relatively simple to
archive. By keeping ﬂooding packets, a jamming attacker,
also named jammer, can effectively block the communica-
tion on a wireless channel, disrupt the normal operation,
cause performance issues, and even damage the control
system [160].

• Node compromise (NC) [17, 45, 47, 68, 69, 70, 72, 74,
75, 79, 81, 82, 83, 85, 86, 87, 88, 90, 96, 97, 114]: Some
research does not specify the details of attack process. The
authors only use “device compromising” to represent an
attack. Some research discusses that an attacker can probe
a target before attacking [17, 45, 69, 70, 74, 79, 81, 82,

83, 85, 86, 87, 88, 90, 114] while others only discuss the
attacking actions [47, 68, 72, 75, 96, 97].

VII. APPLICATION DOMAINS FOR DEFENSIVE DECEPTION
TECHNIQUES

21

• Web attack [89]: An attacker can leverage existing vulnera-
bilities in web applications to gather sensitive data and gain
unauthorized access to the web servers.

• Zero-day attack [136, 138]: An attacker can exploit the
vulnerability period where an unknown vulnerability is
not mitigated by a defense system. For example, before
a vulnerability is patched by the system, the attacker can
exploit the vulnerability to launch the attack, which is called
zero-day attack. Often times, defensive deception along with
moving target defense (e.g., network address or topology
shufﬂing [161]) can mitigate this attack effectively.

21

3

3

Node Compromise (NC)
Scanning or Reconnaissance

APT
Loss of CIA
Jamming attack

DoS or DDoS
Network Fingerprinting
Privacy attack

5

Web attack

111

2

7

Fig. 2.
theoretic defensive deception approaches.

Types and frequency of attacks considered in the existing game-

1

1

5

1

1

1

Spamming

APT
Zero-day attack
Scanning or Reconnaissance

Malware
Malicious or fake proﬁles
Crowdturﬁng
Impersonation

2

2

Fig. 3. Types and frequency of attacks considered in the existing machine-
learning defensive deception approaches.

In Figs. 2 and 3, we summarized the number of game-
theoretic or ML-based defensive deception (DD) techniques
that handled each type of attacks discussed in this paper. Note
that the surveyed papers in this work were collected based on
the following keywords: cyber deception, defensive deception,
game theoretic defensive deception, and machine learning-
based defensive deception. The surveyed papers in this work
were published from 1982 to 2020. As shown in Fig. 2, the
majority of game-theoretic DD techniques considered node
capture or compromise most and scanning (or reconnaissance)
attack as the second most. As illustrated in Fig. 3, ML-based
DD techniques considered spamming or malicious/fake proﬁle
attacks most as most ML-based approaches are mainly used
to detect attacks in honeypots.

In this section, we provide how existing defensive de-
ception techniques have been studied in different network
environments or platforms. To be speciﬁc, we described the
characteristics of each network environment and correspond-
ing defensive deception techniques using game-theoretic or
ML-based approaches, attacks considered, and metrics and
experiment testbeds used.

A. Non-Domain Speciﬁc Applications

An enterprise network is a common system that is homo-
geneously conﬁgured to be operated in a static conﬁgura-
tion [162]. Hence, an attacker can easily plan and perform
its attack successfully and accordingly penetrate into the
system without experiencing high difﬁculty but using dynamic
strategies to defeat defense strategies.

1) DD Techniques: The common defensive deception (DD)
techniques used in this network environment include various
types of honey information, such as fake honey ﬁles [136] or
honeypots [47, 68, 69, 71, 103, 163].

2) Main Attacks:

In enterprise networks,

the following
attacks have been countered by game-theoretic (GT) or ML-
based DD, including insider threat and sophisticated adver-
saries, such as APTs [96], zero-day attacks [136, 138], reverse
engineering attacker using security patch Avery and Spafford
[104], or DoS attack [71].

3) Key GT and ML Methods: For game-theoretic DD
techniques deployed in this network environment, signal-
ing games [47, 71, 68] or Stackelberg game [103] have
been commonly used. Some different game types, such as
Metagames/Expected Utility [69] or Bayesian Game using
Bayesian equilibrium [71] have been also considered to
develop DD techniques in this network context. For ML-
based DD techniques, Natural Language Processing (NLP)
techniques [136, 138] have been used to develop honey ﬁles.
4) Pros and Cons: Majority of game-theoretic DD ap-
proaches has considered an enterprise network without provid-
ing the details of a network model with the aim of examining
a proposed game-theoretic framework with solid theoretical
analysis, such as identifying optimal strategies or solutions
based on Nash Equilibria. However, highly theoretical game-
theoretic DD approaches may not provide concrete details on
how to design DD techniques considering the characteristics of
a given network environment in terms of resource availability,
system security and performance/services requirements, or net-
work and environmental dynamics. In addition, some general
game-theoretic DD approaches do not specify particular DD
techniques, but can provide a general idea of modeling a
DD technique. However, this lacks details, showing limited
applicability in real systems. Further, enterprise networks are
mostly highly complex systems which may need to deal with a
wide range of attack types. Due to the abstract nature of game-
theoretic approaches proposed for the enterprise networks,
they do not provide how to model and simulate speciﬁc types
of attacks.

B. General Cyber-Physical Systems (CPSs)

A CPS is a system that can provide communications be-
tween humans via cyber capabilities and physical infrastruc-
ture or platforms [164]. The CPS has been advanced with
the cyber capabilities of communications, networking, sensing,
and computing as well as physical capabilities with materials,
sensors, actuators, and hardware. The CPS is uniquely distin-
guished from other platforms due to the presence of both cyber
and physical aspects of systems and their coordination [164].
CPSs include wireless sensor networks, Internet of Things
(IoT), software-deﬁned networks (SDNs), and industrial con-
trol systems (ICSs) [165, 166]. Although we discuss each of
these networks in separate sections, we include this section
particularly to discuss defensive deception techniques that are
designed for ‘general CPSs’ without specifying a particular
platform.

1) DD Techniques: Although defensive deception tech-
niques have been deployed in CPS environments, their appli-
cability seems not limited to only CPS environments, rather
applicable in any environments. We found bait information
and honeypots are applied to the CPS environments for intro-
ducing confusion or uncertainty to attackers or luring them to
honeypots for collecting attack intelligence [45, 109, 134].

2) Main Attacks: game-theoretic or ML-based DD tech-
niques in this environment mainly handled node compro-
mise [45], scanning or reconnaissance attacks [109], or APT
attacks [134].

3) Key GT and ML Methods: A signaling game theory
was used for a defender to identify an optimal strategy using
DD techniques [109]. In addition, how to select honeypots
to maximize confusion or uncertainty to attackers is also
studied in [45]. To create decoy devices that look like real,
RNN (Recurrent Neural Networks) was also used based on
observations of device behaviors for a year in a CPS [134].

4) Pros and Cons: Since a CPS is a popular network
environment and commonly used in real systems, developing
DD techniques for the CPS can have high applicability in
diverse CPS contexts. However, the DD techniques developed
for the CPS did not really consider much about its unique
characteristics, such as the features of having both cyber and
physical aspects, and challenges of the environment itself.
Hence, we do not really observe much differences between
the CPS’s DD and the enterprise network’s DD techniques.
In addition, physical honeypots are sometimes recommended
when dealing with highly intelligent attackers. However, such
honeypot deployment and maintenance often come with higher
deployment and management cost.

C. Cloud-based Web Environments

Cloud-based web applications become more common and
popular than ever to deal with critical
tasks which bring
security as a prime concern. Defensive deception is one of
promising directions to defend against web attacks [167].
Defensive deception techniques can be utilized to defend
against advanced web attacks that cannot be well handled
by existing signature or anomaly-based intrusion detection or
prevention mechanisms [167].

22

1) DD Techniques: In this environment, honey-X [89] or
fake signals [48] are used as defensive deception techniques,
such as honeywebs using honey ﬁles, honey tokens, decoy
resources, or honeypots.

2) Main Attacks: Game-theoretic or ML-based DD tech-
niques in cloud environments have mainly considered general
web attacks [89], or multi-staged APT attacks [48].

3) Key GT and ML Methods: Signaling games with com-
plete, imperfect information have been used to model attack-
defense games [48, 89].

4) Pros and Cons: While game-theoretic approaches pro-
posed a generic game-theoretic DD framework [48, 89], ML-
based obfuscation technique is fairly unique as it aims to
defend against privacy attacks to deal with adversarial ML
examples. However, we do not really observe concrete design
features of a given DD technique only for the cloud computing
environment.

D. Internet of Things (IoT)

IoT network environments become highly popular and have
been recognized as one of CPSs with the capability to pro-
vide effective services to users. IoT has been also special-
ized for particular domains such as Internet-of-Battle-Things
(IoBT) [168], Industrial-Internet-of-Things (IIoT) [169], or
Internet-of-Health-Things (IoHT) [170]. In addition, IoT em-
braces a large number of CPS network environments, such
as wireless sensor networks (WSNs), mobile ad hoc networks
(MANETs), or smart city environments consisting of sensors
and other intelligence machines [171]. IoT has been popular
considered as a main platform of deploying cyberdeception
technologies [172].

1) DD Techniques: Similar to other environments, honey-
pot enabled IoT networks have been mainly considered by
solving the optimal deployment of honeypots [70, 71].

2) Main Attacks: In IoT environments, game-theoretic or
ML-based DD techniques aimed to defend against reconnais-
sance and probing attacks, DoS attacks [71], packet dropping
attacks [71], or APT attacks [70, 74].

3) Key GT and ML Methods: Bayesian games with incom-
plete information and meta games are considered for a player
who is unsure of a type of attackers [70]. IoTs in battleﬁelds
is referred to IoBTs where deception games introduced in
[74, 83, 85, 86, 87, 114]

4) Pros and Cons: Since honeypots are fake nodes mim-
icking the behavior of a regular node, adding a honeypot does
not change the hierarchy of the IoT network or the interface
of IoT gateways. A game-theoretic honeypot technique is
the only technique applied to this domain. However, the IoT
naturally can generate a large amount of data for ML-based
DD techniques which can create real-like decoys or enhance
detecting inside and outside attackers.

E. Software-Deﬁned Networks (SDNs)

The SDN paradigm separates data plane processing (e.g.,
packet forwarding) from control-plane processing (e.g., rout-
ing decisions) [173]. The OpenFlow protocol [174] acts as
an API between network switches and a logically centralized

TABLE V
APPLICATION DOMAINS OF GAME-THEORETIC OR MACHINE LEARNING-BASED DEFENSIVE DECEPTION TECHNIQUES

23

Application domain

Non-domain speciﬁc
environment:
GT-Based [16, 17,
47, 56, 57, 68, 69,
71, 72, 75, 79, 81, 82,
90, 94, 95, 96, 97, 99,
100, 101, 102, 103,
113, 115, 116, 117];
ML-Based [128, 129,
130, 131, 132, 133,
134, 135, 136, 137,
138, 139]

General
Cyber-Physical
Systems:
GT-Based [45, 109];
ML-based [134]

Main attacks

Zero-day attacks,
APTs, masquerade
attacks, reverse
engineering for
security patch, DoS
attack, optimal
inference attacks,
reconnaissance
attack, spam, social
spam, malicious
proﬁles

Defensive deception
techniques
Honey ﬁles,
honeypots,
honeywebs,
honeynets, honey
patch, honey proﬁles,
honey surface,
honeybots, HMAC,
obfuscation,
deceptive signal,
Fake Identities,
deceptive network
ﬂow, social
honeypots, bait-based
deception, fake
services

Crafted bait
information,
honeypot

Reconnaissance
attacks, node
compromise, APT

Key GT/ML
techniques
Signaling games,
Stackelberg game,
Metagames/Expected
Utility, Bayesian
Game using Bayesian
equilibrium, subgame
perfect Nash
Equilibrium (SPNE),
NLP techniques,
hypergame,
Stackelberg game,
one-sided POSG,
BayesNet, Decision
Table, Na¨ıve-Bayes,
Reinforcement
Learning, SVM
Signaling game
theory, RNN

Cloud web-based
environments:
GT-Based [48, 89]

Internet-of-Things:
GT-Based [70, 74,
83, 85, 86, 87, 114]

Honeywebs (using
honeytokens, honey
ﬁles, decoy
resources,
honeypots), deceptive
signals, and
obfuscation
Honeypot

General web attacks,
APT attacks, privacy
attacks

Signaling game
theory, a suite of ML
classiﬁers

Reconnaissance and
probing attacks, DoS
attacks, packet
dropping attacks,
APTs

Bayesian games,
meta games,
signaling game with
perfect Bayesian
equilibrium

Software-deﬁned
networks:
GT-Based [46, 98,
110, 111]

Honeypot

Reconnaissance
attacks

Bayesian game

Wireless networks:
GT-
Based [55, 91, 112]

Deceptive network
ﬂow, honeynet

Jamming attacks, 3G
core network attacks

A non-cooperative
non-zero-sum static
game

Online social
networks:
ML-Based [129, 132,
133, 137]

Social Honeypot,
honey proﬁles

Fake liker
(crowdturﬁng), social
spammers, spamming

Supervised ML, ML
toolkit, SVM

Pros

Cons

Most game-theoretic
DD approaches
propose a general
deception game
framework without
specifying a certain
domain environment,
which can have high
applicability
regardless of domain
environments.

Due to a wide range
of CPS applications,
developing DD
techniques will have
high values and
applicabilities in
diverse CPS
environments.

Multiple DD
frameworks are
provided with high
applicability to deal
with a wide range of
web attacks.

Since honeypots are
fake nodes
mimicking the
behavior of a regular
node, adding a
honeypot does not
change the hierarchy
of the IoT network
or the interface of
IoT gateways.

The key advantage of
using SDN-based
defensive deception
approaches is their
easy deployability.
For example, the
existence of an SDN
controller enables
easily camouﬂaging a
network topology or
hiding vital nodes
through ﬂow trafﬁc
control.
Speciﬁc design
features to deal with
key concerns of
wireless networks are
provided.

Learning the
characteristics of
attack behavior and
attract spammers in
OSN

Due to the nature of a
general approach, there is
high overhead to conver
the general approach to a
domain-speciﬁc approach.

DD techniques for the
CPS do not really reﬂect
unique challenges of the
CPS environments.
Physical honeypot
deployment and
maintenance often come
with higher deployment
and management cost.
Some detailed designs
should be considered to
deal with unique
challenges of cloud
environments.

A game-theoretic
honeypot technique is the
only technique that
applied to this domain.
However, the IoT
naturally can generate a
large amount of data for
ML-based DD techniques
which can create decoys
that looks like real or
enhance detect inside and
outside attackers.
Most existing approaches
use a single SDN
controller, which expose
a single point of failure.

If a honeynet architecture
is heavily dependent
upon the accuracy of
deployed honeypots, the
formulated game may not
work under different
deployment settings.
Detectors are applied on
static datasets to evaluate
the detection of spammer
and honey proﬁles are
not generic and depend
on the platform type.

decision maker, called the OpenFlow controller. In this pro-
tocol, network switches cache data-plane ﬂow rules. When a
switch receives a packet and does not know how to forward it
according to its cached rules, the switch sends an “elevation”
request containing the original packet and a request for the
guidance to the controller. The controller examines the packet
and sends a set of rules that the switch should add to the
data plane cache for forwarding packets [175]. deception
techniques proposed in [110, 111] were designed to secure
SDN.

1) DD Techniques: Honeypots are popularly used as a
defense strategy in game-theoretic defensive deception frame-
work [46, 98] for taking dynamic, adaptive defense strategies.
2) Main Attacks: Common attack behaviors considered in-
clude scanning or reconnaissance attacks to obtain information
and intelligence towards a target system by scanning network
addresses (e.g., IP or port numbers) aiming to obtain defense
information, network mapping, and inside information [98]

3) Key GT and ML Methods: Bayesian game theory has
been popularly adopted under various conditions [46], such
as imperfect information, incomplete information, information
sets, and perfect Bayesian equilibrium.

4) Pros and Cons: The key advantage of using SDN-based
defensive deception approaches is their easy deployability. For
example, the existence of an SDN controller enables easily
camouﬂaging a network topology or hiding vital nodes through
ﬂow trafﬁc control. Most existing approaches use a single SDN
controller, which exposes a single point of failure.

F. Wireless Networks

Wireless communications are everywhere these days and
more common than wired communications due to their
easy deployment and efﬁciency. However, when network re-
sources are scarce, bandwidth constraints or unreliable wire-
less medium become main issues to be resolved. Game-
theoretic or ML-based DD techniques are also proposed to
defend against various types of attacks exploiting vulnerabil-
ities of wireless network environments.

1) DD Techniques: A deceptive network ﬂow is proposed
to generate the ﬂow of random dummy packets in multihop
wireless networks [55]. A honeynet architecture address mo-
bile network security (3G networks) [91] was developed to
enhance the security of the core network of mobile telecom-
munication systems. In [91], a gateway was designed to
control and capture network packets as well as investigate
and protect other information systems from attacks launched
from potentially compromised systems inside the honeynet.
Defensive deception is used to transmit fake information
over fake channels to mitigate jamming attacks in wireless
networks [112].

2) Main Attacks: Game theoretic or ML-based DD tech-
niques have defended against jamming attacks [55] in mul-
tihop wireless networks and 3G core network attacks which
compromise servers or gateways that support nodes providing
general packet radio services in the mobile networks [91].

24

attacker and a defender [91]. Deceptive routing paths are also
designed based on a two-stage game using Stackelberg game
theory [55]. Deceptive power transmission allocation based on
game theory is applied to defender wireless networks against
jamming attacks [112].

4) Pros and Cons: Speciﬁc design features to deal with
key concerns of wireless networks, such as multihop commu-
nications or mobile wireless security, are helpful to implement
real systems based on given game-theoretic DD technologies.
However,
if a honeynet architecture heavily
depends upon the accuracy of deployed honeypots, the for-
mulated game framework may not guarantee the same level
of effectiveness under different deployment settings.

like in [91],

G. Online Social Networks

Due to the large scales of OSNs and their signiﬁcant inﬂu-
ence in social, economical, and political aspects, AI and ML
communities have recognized high challenges in developing
DD techniques in the OSN platforms. Hence, there have been
signiﬁcant efforts made to secure OSNs from malicious users
(e.g., fake users and spammers) by developing various social
defensive deception techniques [129, 132, 133, 137].

1) DD Techniques: The authors in [129, 132, 133] specif-
ically used social honeypots primarily to attract and identify
attackers. Social honeypots are capable of learning the be-
havioural models of malicious users and collect a spammer’s
preferences. Stringhini et al. [137] analyzed 900 honey pro-
ﬁles for detecting spammers across MySpace, Facebook, and
Twitter for classiﬁcation and identiﬁcation purposes.

2) Main Attacks: OSNs are mainly targeted by fake likers
and spammers that would like to exploit the existing dynamics
of OSNs to achieve a speciﬁc goals [129, 132, 133, 137].

3) Key GT and ML Methods: Traditional ML tool kits such
as SVM have been used to analyze the data collected by social
honeypots and honey proﬁles.

4) Pros and Cons: Social honeypots allows learning the
characteristics of attack behavior and honey ﬁles can attract
spammers to protect real ﬁles from them in OSN. However,
detectors are applied on static datasets to evaluate the detection
of spammers. In addition, honey proﬁles are not generic and
should be customized to a speciﬁc platform type.

For the convenience of easy look-up for game-theoretic
or ML-based DD techniques based on different application
domains, we summarized our discussions of this section in
Tables III and IV.

VIII. EVALUATION OF DEFENSIVE DECEPTION
TECHNIQUES: METRICS AND TESTBEDS

In this section, we survey what types of metrics are used to
measure the effectiveness and efﬁciency of defensive deception
techniques. In addition, we address what types of testbeds
are employed to evaluate the effectiveness and efﬁciency of
defensive deception (DD) techniques.

A. Metrics

3) Key GT and ML Methods: A non-cooperative, non-zero-
sum static game is used to model the interactions between an

In the literature, the following metrics are used to measure
the effectiveness of existing defensive deception techniques:

• Detection accuracy [5, 42, 68, 72, 98, 129, 130, 132, 133,
135, 139, 149]: This is often measured by the AUC (Area
Under the Curve) of ROC (Receiver Operating Characteris-
tic). AUC is used to measure the accuracy of detection by
showing TPR (True Positive Rate) as FPR (False Positive
Rate) increases. Abay et al. [139] used a classiﬁer accuracy
as a metric to evaluate the effectiveness of the developed
Honey data to deceive the attacker. Badri Satya et al. [129]
and Chiang et al. [98] used an algorithm to discover fake
Liker in social networks. The authors in [42, 135] evaluated
a masquerade attack detector based on AUC. ROC is also
used to measure detection accuracy of social honeypot-
based approach against social spammers [132, 133] and
malicious account [130, 149] in online social networks.
Pawlick et al. [5, 68] evaluated their deception mechanism
based on different metrics, including the detector accuracy
developed by the adversary to discover deception.

• Mean time to detect attacks [72]: The effectiveness of a
detection mechanism is also measured by how early their
deception technique can capture the attacker.

• Utility [5, 45, 55, 56, 57, 59, 68, 69, 74, 77, 78, 83,
85, 86, 87, 88, 90, 96, 99, 101, 102, 112, 113, 114]: In
game-theoretic DD techniques, a player (either an attacker
or a defender)’s utility (or payoff) is considered as one
of key metrics to evaluate a deception game between the
attacker and defender. Commonly, the utility of taking a
defensive deception strategy is formulated based on the
deployment cost and the confusion increased to an attacker.
Some signaling games model the interactions between an
attacker, a benign client and a defender. As a result, some of
these studies considered the impact to normal as another cost
of deception technology [113, 116]. A defender’s expected
utility is a common metric to be maximized as the system’s
objective, as shown in game-theoretic or ML-based DD
techniques in [5, 45, 56, 59, 68, 69, 74, 77, 78, 88, 90,
96, 99, 102, 110, 111].

• Probabilities of an attacker taking certain actions [79, 87,
97, 116, 118, 119]: An attacker’s actions (i.e., choices)
in proceeding attacks are also considered as a metric to
measure the effectiveness of defensive deception (e.g., hon-
eypots). For example, in [79], the following probabilities
of an attacker’s action are considered: a probability of an
attacker retrying a failure command, a probability of an
attacker choosing an alternative strategy, and a probability of
an attacker leaving a game. The probability of an attacker to
successfully control the network and overcome the deception
is used to evaluate the effectiveness of cyberdeception
in [97, 116] as well as the probability of launching an attack
as in [118, 119].

In addition, we also found the following metrics to cap-
ture the efﬁciency of defensive deception (DD) techniques
discussed in our survey paper:

• Round trip-time [41, 44, 72, 105, 122, 176]: The key role of
DD is to delay the adversary processes [105]. Increasing the
time required to crack an obfuscated code is the evaluation
metric adopted in [44, 176], as well as the complexity of
the deobfuscation process [41]. The HackIt deception tool

25

in [122] successfully increased the time taken by hackers to
exploit a system. An attacker’s deception detection time is
of great signiﬁcance to evaluate the effectiveness of cyber
deception as in [72].

• Runtime [104, 107, 117]: This evaluates the cost of au-
tomated deception and obfuscation techniques. Avery and
Spafford [104] and Avery and Wallrabenstein [107] eval-
uated the performance of the developed fake patches de-
ception technique based on this runtime metric. Similarly,
the learning time is the metric used to evaluate the learning
the attacker’s behavior model from attack data introduced
in [117].

In Figs. 4 and 5, we summarized the types and frequency
of metrics measuring performance and security of the DD
techniques surveyed in this work. As shown in Fig. 4, a
player’s utility is the most common metric among all metrics
used for the surveyed game-theoretic DD techniques. On the
other hand, in Fig. 5, ML-based DD approaches heavily relied
on detection accuracy using such metrics as AUC, TPR, and
FPR metrics.

5

2

5

Detection accuracy
Utility

6

Probabilities of attacker actions
Round trip-time

Runtime of automated DD

22

Fig. 4. Types and frequency of metrics measuring performance and security
of game-theoretic defensive deception techniques.

1

2

3

Detection accuracy
Utility
Round trip-time

Runtime time

8

Fig. 5. Types and frequency of metrics measuring performance and security
of ML-based defensive deception techniques.

B. Evaluation Testbeds

In this section, we classify evaluation testbeds of the ex-
isting game theoretic or ML-based DD techniques surveyed
in this work in terms of the four classes: probability-based,
simulation-based, emulation-based, and real testbed-based. We
discuss each of the evaluation testbeds and discuss the general
trends observed from the survey.

1) Probability Model-Based Evaluation [16, 177]: Stochas-
tic Petri Nets (SPN) probability models have been used to
evaluate the performance of an integrated defense system
that combines multiple deceptive defense techniques [177].
In addition, the SPN is used to measure the performance
of the deceptive defender and the attacker in a hypergame
model [16].

2) Simulation Model-Based Evaluation [5, 41, 42, 45, 55,
57, 58, 59, 68, 69, 70, 72, 74, 76, 77, 78, 83, 85, 86, 87, 88,
90, 95, 96, 97, 98, 99, 102, 104, 106, 110, 111, 112, 114,
115, 116, 117, 123, 129, 135, 139, 178, 179]: In order to
simulate benign network trafﬁc, Rahman et al. [116] generated
network ﬂow by the Internet Trafﬁc Archive [180], using
Nmap to test their deception technology. [98, 178] to simulate
their deception system and combine it with their proposed
work. Thakoor et al. [102] used the CyberVan [179, 181]
as a network simulation testbed to simulate their game-
theoretic cyberdeception techniques. Al Amin et al. [115] and
Bilinski et al. [97] proposed a deceptive system which was
simulated using a Markov decision processes. Several studies
also simulated ML-based deceptive algorithms or classiﬁers to
examine its accuracy [110, 111, 115, 117, 123, 129, 135, 139].
A masquerade attack is simulated to evaluate the developed
detection techniques based on deception [42, 135]. Various
types of game-theoretic based deception framework have been
evaluated based on simulation models [45, 55, 57, 58, 59,
68, 69, 70, 72, 74, 76, 77, 78, 83, 85, 86, 87, 88, 90, 95,
96, 97, 98, 99, 102, 104, 106, 114, 116]. Simulation-based
models showing a proof of concept have been developed to
demonstrate the runtime of the proposed automated deception
algorithm [104, 106].

3) Emulation Testbed-Based Evaluation [44, 113, 175]:
Emulation network environments have been proposed to dy-
namically place deception resources [113], deploy code obfus-
cation techniques on real-like decompilers [44], and to deploy
DD techniques in emulated SDN-based data center network
testbed.

4) Real Testbed-Based Evaluation [43, 81, 82, 105, 122,
130, 131, 132, 133, 149]: An application for data collection
has been developed [81, 82, 122]. Experiments with human-
in-the-loop with human participants (e.g. an attacker or a
defender) have been conducted [81, 82, 122], such as HackIt
to evaluate the effectiveness of deception on participating
hackers under different deception scenarios. Socialbots on
Twitter real networks are signiﬁcantly leveraged to detect
and study content polluters and understand how spammers
choose their spamming targets to ultimately create a malicious
proﬁle classiﬁer [130, 132, 133, 149]. In addition, various DD
techniques are validated under real settings, such as honey-
patches-based deception system [105], location obfuscation for
preserving users’ privacy [43], and PRISMA (PRotocol In-
spection and State Machine Analysis) tool capable of learning
and generating communication sessions for deception [131].
In Figs. 6 and 7, we summarized the frequency of par-
ticular testbeds used in developing game-theoretic and ML-
based defensive deception techniques surveyed in this paper,
respectively. The overall trends observed were that simulation-
based evaluation was dominant in both GT and ML-based

1

4

26

Probability model-based

Simulation model-based
Real testbed-based

31

Evaluation testbeds for game-theoretic defensive deception tech-

Simulation model-based
Emulation model-based
Real testbed-based

5

1

Fig. 6.
niques.

8

Fig. 7. Evaluation testbeds for ML-based defensive deception techniques.

DD approaches. Although ML-based DD techniques are less
explored than GT-based DD techniques, it is noticeable that
more than 40 percent of ML-based DD techniques surveyed
in this paper used the real
testbed-based evaluation. This
is natural that ML-based approaches basically need actual
datasets to analyze. Another noticeable ﬁnding is that there
has been a lack of probability model-based approaches. This
would be because it is highly challenging to model a complex
system environment characterized by many design parameters
based on mathematical models.

IX. CONCLUSIONS AND FUTURE WORK

In this section, we discussed insights and lessons learned
and limitations from the extensive survey conducted in this
work. In addition, based on the conducted survey, we provided
a set of promising future research directions.

A. Insights and Lessons Learned

We now revisit and answer the research questions raised in

Section I-D.
RQ Characteristics: What key characteristics of defensive
deception distinguish it from other defensive techniques?
Answer: Unlike traditional defense mechanisms, deception
requires a certain level of risk as it requires some interactions
with attackers with the aim of confusing or misleading
them. Particularly, if the goal of defense requires long term
deception, it is inevitable to face risk. In this case, defensive

deception can be used with other legacy defense mecha-
nisms such as intrusion prevention or detection mechanisms
to avoid too high risk. Moving target defense (MTD) or
obfuscation techniques share a similar defense goal with
defensive deception, such as increasing confusion or uncer-
tainty for attackers. However, unlike MTD or obfuscation
which changes system conﬁguration or information based
on the existing resources of a system, defensive deception
can create false objects or information with the aim of
misleading an attacker’s cognitive perception or forming a
misbelief for the attacker to choose a sub-optimal or poor
attack strategy.

RQ Metrics: What metrics are more or less used to measure
the effectiveness and efﬁciency of the existing game-theoretic
or ML-based defensive deception techniques?
Answer: As surveyed in Section VIII along with Figs. 4
and 5, the effectiveness of GT-based deception is mainly
measured based on utility. On the other hand, ML-based
deception is mainly evaluated according to the accuracy
of classiﬁers or intrusion detectors. The existing metrics
observed in the surveyed literature for game-theoretic or
ML-based DD approaches do not capture the direct impact
of defensive deception. That is, they do not quantify the
uncertainty or confusion induced by the different deceptive
techniques. Moreover, new metrics should be developed
to measure the effectiveness of deception in misleading
the attackers, such as lightweight deception with less or
minimum defense cost in deploying or maintaining a given
deception technique.
According to Section VIII along with Figs. 4 and 5, GT-
based deception techniques have been mainly evaluated in
terms of the defender’s utility. However, ML-based decep-
tion techniques focused on improving the accuracy of classi-
ﬁers or intrusion detectors. However, the existing evaluation
of GT-based defensive deception techniques rarely consider
utility analyses based on the losses and gains in terms of the
timing of using different types of deceptions. In addition,
although it is important to use metrics capturing decep-
tion cost introduced by its deployment and maintenance
and performance degradation caused by running deception
techniques, there has been much less effort to use those
metrics. Furthermore, measuring the extent of attackers
deviating from the optimal course of actions can be another
meaningful metric to evaluate the effectiveness of deception
in attack-defense games.

RQ Principles: What key design principles help maximize
the effectiveness and efﬁciency of defensive deception tech-
niques?
Answer: It is critical to make three key design decisions,
which are what-attacker-to-deceive, when-to-deceive, and
how-to-deceive. These three design decisions should be
determined based on critical tradeoffs between effectiveness
and efﬁciency of a developed DD technique. The what-
attacker-to-deceive question should be answered based on
what attacks are targeted by a defender and what attacks
should be commonly handled in a given application domain

27

because each application domain generates distinctive chal-
lenges. When-to-deceive is related to the fact that depending
on when a given defensive deception technique is used to
deal with attackers in a certain stage of the cyber kill chain,
the extent of deployability, effectiveness, efﬁciency, and cost
of using a given DD technique can be affected. For example,
when-to-deceive can be decided according to how complex
the deployment of such DD technique or how effective the
DD technique is in terms of the timing. In addition, it can
consider how costly deception is in terms of the availability
of the resources. In some settings, deception is needed
only temporarily to delay an attack until a defense system
needs to take enough time for its reconﬁguration or de-
ploying/performing a certain defense operation (e.g., deceive
an attacker until the MTD is executed to fully reconﬁgure
an OS). Each type of a DD technique has its own unique
characteristics and corresponding expected outcomes. Given
what attack to deal with by determining what-to-deceive and
when to deceive the attacker (i.e., in the attack stage), the
how-to-deceive question is related to answering (1) what
DD techniques are better than other DD techniques under
given resources and (2) what quality of deception is most
appropriate to deal with the given attacks. For example, if
a given defense system can afford to deploy high-quality
honeypots to deal with highly intelligent attackers, high-
interaction honeypots would be a good choice. However,
if the defense system has a limited budget or resources
available and wants to deploy a lightweight DD technique
temporarily (e.g., before a new setting of MTD is fully
reconﬁgured), a honey ﬁle or honey token can be used with
low cost but they can be detected by intelligent attackers
soon. Therefore, depending on the defense cost budget, the
system’s resource availability, or the expected outcome for a
DD technique to be used in a given time, a different type of
DD techniques with a different level of deception quality can
be adopted to meet the expected effectiveness and efﬁciency.

RQ GT: What are the key design features when a defensive

deception technique is devised using game theory?
Answer: Game-theoretic DD techniques mainly adopt an
attack-defense framework where the attacker and defender
interact with a conﬂict of interest scenario, which often use
a form of zero-sum games. However, as multiple players in
each party (i.e., multiple attackers for collusive attacks or
multiple defenders for cooperative defense) can involve in
a given attack-defense game, general-sum games are often
considered as well. To be more speciﬁc, we also summarized
a game type used in each paper that proposes a game
theoretic defensive deception in Table III. Game-theoretic
DD focuses on identifying optimal defense strategies to
confuse or mislead an attacker by increasing uncertainty.
Three design features are: (i) each player needs to have a
clear goal and a corresponding clear utility function that re-
ﬂects the player’s intent, tactics, and resources; (ii) modeling
and simulating how to increase confusion or uncertainty via
different types of DD techniques should be elaborated; and
(iii) metrics to measure effectiveness and efﬁciency should

be articulated.

RQ ML: What are the key design features when a defensive

deception technique is developed using ML?
Answer: ML-based DD techniques learn and detect attack
behaviors. However, ML can improve DD by considering
the following: (i) We need to consider what types of datasets
use to develop DD techniques. To develop believable fake
objects presupposes good datasets for mimicking real objects
and evaluating fake objects. Along these lines,
targeted
attack datasets should be used to model attackers and
develop honey resources; (ii) ML-based DD approaches
should adopt relevant metrics to capture their effectiveness
and efﬁciency. So far, classiﬁcation accuracy is the only
metric to capture ML-based honeypots. Additional metrics
should be developed to capture the quality of ML-based DD
techniques; and (iii) A fully automated deception is highly
desirable for ML-based DD techniques, such as generating
deceptive trafﬁcs and network topologies.

RQ Applications: How should different defensive deception
techniques be applied in different application domains?
As the enterprise network is a complex domain, multiple
honey-X techniques, possibly in combination, are applica-
ble. For simpler domains, an easy-to-deploy honeypot is
effective and widely used. However, developing high quality
defensive deception techniques often incurs more cost or
higher complexity. For example, deploying more honeypots
may increase the chance to mislead an attacker. However,
too many honeypots also increase unnecessary costs and
confuse the administrator or legitimate users. Depending
on applications, DD techniques should be applied in an
automated manner. For example, online social networks
require deception for detecting attacks or collecting attack
intelligence since social spammers do not aim to gain control
but to inﬂuence other users in the network.

As you can observe in our extensive survey, many more
game theoretic defensive deception studies have been con-
ducted while ML-based DD studies are limited to developing
honey-X-based approaches aiming to detect more attacks. Via
our extensive survey, we could clearly observe the main beneﬁt
of using game theoretic DD techniques is their capability to
allow players to make strategic decisions and take various
strategies upon observed dynamics of a given game over
time. In addition, since game theory provides a mathematical
framework to model the interactions between attackers and
defenders, it can be more extensively applied in developing
various types of DD techniques as a general framework.
However, as we also observed in existing work, although game
theoretic DD has powerful capability with solid mathematical
proofs in developing DD techniques, their validation in secu-
rity and performance has been heavily limited to the theoretical
or simulation-based veriﬁcation. However, ML-based DD ap-
proaches have been applied to mainly honey-X applications,
mostly developing honeypots. Unlike other DD techniques, the
main goal of honeypots are two-fold: protecting system assets
by creating fake objects, which are honeypots and detecting at-
tackers by luring them to the honeypots. Although the original
purpose of DD more focuses on protecting the system assets,

28

which is more like passive defense, the effectiveness of honey-
pots has been mainly measured its role in detecting attacks in
terms of the detection accuracy. That is, ML-based honeypots
have been developed by creating a fake object which looks like
real with the aim of attracting more attackers. Compared to
GT-based DD approaches, ML-based DD approaches are less
dynamic because ML techniques are mainly used to improve
the quality of deception in its development time and as a
classiﬁer to detect attacks. Although reinforcement learning
as one of promising ML techniques as been used to develop
a DD technique along with game theory [113, 115], hybrid
approaches incorporating ML into game theory to develop DD
techniques are rarely taken. We discuss examples of how these
can be incorporated into a hybrid DD technique in the future
research directions discussed in Section IX-C.

B. Limitations of Current Defensive Deception

From our extensive survey on the state-of-the-art defensive
deception (DD) techniques using game theory and ML, we
found the following limitations of the existing approaches:
• Although many DD techniques have used game-theoretic ap-
proaches, they have mainly focused on theoretical validation
based on Nash equilibrium and the identiﬁcation of optimal
solutions, in which attacks are not often detailed enough but
simply considered attacks compromising other nodes. Often
times attacks are complicated and performed with more than
one episode or multistage over time, such as APT attacks.
Therefore, although we can beneﬁt from modeling simple
attack processes, such as active reconnaissance or insider
attack, it is still challenging to deploy the game-theoretic
DD in real systems and model attacks based on a complex
cyber kill chain.

• We observed that an enterprise networking is the only
domain that is well developed. Research effort in other
domains has been limited and mainly focused on recon-
naissance attacks. IoT and SDN environments are capable
of generating large amounts of trafﬁc ﬂow data, which can
be used to train ML models to identify attackers. However,
current DD approaches for those domains lack using ML.
• Due to high complexity of action spaces, most game-
theoretic DD has dealt with a limited set of actions for an
attacker and defender, which cannot accurately model an
attack or defensive process when the attacker can perform
multiple attacks following the multiple stages of attack
processes, such as cyber kill chain in APT attacks.

• When multiple attacks arrive in a system, considering the
interactions only between one attacker and one defender
is not sufﬁcient to capture the practical challenge. How to
interpret a response towards a certain action (e.g., who is
responding to what action when there are multiple attackers)
is not clear although it is critical to model the interactions
between the attacker and defender.

• Majority of game-theoretic defensive deception approaches
are studied at an abstract level by using game-theoretical
analysis without explicitly addressing design challenges
derived from the network environment or platform the de-
fensive deception technique is deployed. When some ideas

are adopted and applied in a certain network environment, it
is quite challenging how to deploy the given game-theoretic
work into a given platform due to lack of details available
for the deployment process.

• The accuracy of ML-based deception techniques depends on
data availability regarding the attacker identity, techniques,
and targets. Practically, such data are not available for
the defender, which signiﬁcantly limits training ML-based
classiﬁers or detectors. Moreover, such models tend to
assume that the attacker is acting normally toward its target.
However, if an attacker decided to deceive a defender to
remain stealthy, the effectiveness of defensive deception
techniques may not be clearly measured.

• Compared to game-theoretic defensive deception tech-
niques, ML-based defensive deception has been much less
studied. The majority of using ML techniques in defensive
deception is to detect attacks in honeypots. Only a few
works have used ML to mimic real objects or information
for developing honey objects or information.

• Although game-theoretic or ML-based defensive deception
has been studied, there have been a few studies that combine
both to develop hybrid defensive deception techniques,
particularly incorporating reinforcement learning (RL) into
players’ utility functions and using RL to identify the
players’ optimal strategies.

• Evaluation metrics of defensive deception are limited. Most
game-theoretic DD approaches mainly used metrics in game
theory, such as utility or probability of taking attack strate-
gies. Most ML-based DD approaches were mainly studied
to achieve attack detection, which leads to using detection
accuracy as a major metric. These metrics, utility and
detection accuracy, do not fully capture the effectiveness
and efﬁciency of DD approaches based on the core merit of
using deception.

• Evaluation testbeds are mostly based on simulation mod-
els. Although some ML-based DD approaches used real
testbeds,
they are mostly social honeypots deployed in
social media platforms. In particular, game-theoretic DD
approaches remain highly theoretical analysis.

C. Future Research Directions

We suggest the following research directions as promising

future research:
• Need more metrics to measure the quality of honey-
X technologies: The quality of honey-X has been captured
mainly based on detection accuracy (see Figs. 4 and 5) even
if the key role of honey-X is to protect assets as well as to
detect attacks. Hence, there should be more diverse metrics
to measure the roles of both protecting assets and detecting
attacks. The example metrics can include an attacker’s
perceive uncertainty level, the amount of missing vulnerable
the amount of novel attack vectors
assets by attackers,
collected via honeypots, or the amount of important system
components attacked. In addition, the efﬁciency of running
honey-X technologies (i.e., defense cost) or performance
degradation due to the deception deployment should be
considered for a defender to choose an optimal strategy

29

based on multiple criteria including those multiple metrics
as objectives.

• Consider more sophisticated, intelligent attackers: The
attacker behaviors in the literature mostly consider simple
attacks which are not more intelligent than the defender
behaviors, which are not realistic in practice. In particular,
the
a highly intelligent attacker may be able to detect
deception deployed by the defender. Further, in most ex-
isting game theoretic approaches using Stackelberg games,
the defender is a leader while the attacker is a follower
where there is a ﬁrst-mover advantage in such sequential
games. This may not be true in real-world cyber settings. In
addition, when APT attacks are considered, most existing
approaches consider only reconnaissance attacks. In future
work, we need to consider the APT attackers performing
multistaged attacks following the cyber kill chain (CKC)
stages (i.e., including reconnaissance, delivery, exploitation,
command and control (C2), lateral movement, and data
exﬁltration).

• Measure the quality of defensive deception: The quality
of a defensive deception technique should be determined
based on how well it deceives attackers. That is, the quality
of defensive deception should be measured by the attacker’s
view and actions based on its belief towards the defender’s
moves. However, existing work mainly uses system metrics
as a proxy to measure the quality of a defensive deception
technique deployed by the defender. Some example metrics
can be the degree of uncertainty perceived by attackers,
the degree of the discrepancy between what the attackers
perceived towards the defense system and the ground truth
system states, or service availability without being disrupted
by deployed defensive deception techniques.

• Develop hybrid defensive deception techniques based
on both game theory and ML: Machine learning-based
game theoretic approaches (e.g., game theory plus deep
reinforcement learning) have been considered in developing
other defense techniques [182, 183]. However, only a few
studies [113, 115] used hybrid approaches of incorporating
reinforcement learning (RL) with game theory where we
treat RL as one of ML techniques. RL’s reward functions
can be used to formulate players’ utility functions and let
an RL agent identify an optimal strategy as other attack-
defense games [182, 183] leverage RL or deep RL (DRL).
In addition, ML can be used for players to estimate their
own beliefs to decide what strategy to take or opponents’
beliefs to predict their moves. In addition, when players use
a game-theoretic approach to take an optimal strategy, a set
of DD techniques can be developed using ML technologies
to increase the quality of the deception by mimicking real
objects or information.

• Measure cost or service availability under the deploy-
ment of defensive deception techniques: As discussed in
Section IX-B, most game-theoretic DD studies used utility
and probabilities of taking strategies while ML-based DD
approaches mainly used detection accuracy as the major
metric. However, the drawbacks introduced by deploying
defensive deception techniques have not been sufﬁciently
discussed. In addition, some strong assumptions, such as no

impact introduced to the defender system, are made while
deploying defensive deception techniques. By considering
metrics to measure performance degradation introduced by
a used defensive deception, we can measure a broader aspect
of the quality of the defensive deception deployed in a
system based on the classical tradeoff between security and
performance.

• Build a realistic testbed to evaluate game theoretic
defensive deception techniques: Game-theoretic DD ap-
proaches have been dominantly studied based on the the-
oretical validation via mathematical proof or experimental
analysis via simulation models. More realistic emulation or
real testbeds should be reﬂected to validate game theoretic
DD approaches considering highly intelligent attackers and
highly uncertain network environments. To this end, there
should be clear designs on how to deal with uncertainty
in terms of how much knowledge is known for an attacker,
what system information (i.e., a defender’s information) can
be known to the attacker, and what network environment
information or dynamics are known to both the defender
and the attacker. How much and how accurate information
is available to players and how their beliefs are formulated
under real settings are critical points to tackle because strong
assumptions on common knowledge in game theory cannot
hold in real environments. The ﬁrst step to tackle this
problem is to build a game where two players may have
asynchronous information with imperfect or incomplete
information, such as a hypergame [184].

• Develop a domain-speciﬁc game theoretic framework
that can provide defensive deception as a security
service: Based on the observation of Table III, we ﬁnd
many game theoretic frameworks are developed as gen-
eral frameworks without specifying an domain application.
This would be helpful for general understanding about the
core idea of the method itself. However, it misses how
to design defensive deception techniques for a speciﬁc
domain environment which may pose challenges in terms of
resources available (e.g., bandwidth, computational power),
environmental dynamics, or other characteristics of common
adversarial attacks. Providing more speciﬁc applications and
corresponding system designs for developing and deploying
defensive deception techniques will lead to their improved
applicability into the real environments.

• Identify highly compatible conﬁgurations of using both
defensive deception and legacy defense services: It is
not necessarily true to always use defensive deception
techniques to handle all types of attacks. In addition, it is not
always true that DD techniques should be always deployed
with legacy defense services, such as intrusion prevention,
detection, and response systems. This is because some
combinations of deploying defensive deception techniques
with legacy defense services, such as using honeypots with
intrusion detection and response systems, may introduce
overlapping effect which is not really efﬁcient (e.g., both
honeypots and IDS may do a same job). We should develop
a more systematic method to ﬁgure out how to synergisti-
cally leverage both defense services to provide cost-effective
defense services.

30

ACKNOWLEDGEMENTS

This research was partly sponsored by the Army Research
Laboratory and was accomplished under Cooperative Agree-
ment Number W911NF-19-2-0150. In addition, this research
is also partly supported by the Army Research Ofﬁce under
Grant Contract Numbers W91NF-20-2-0140 and W911NF-17-
1-0370. The views and conclusions contained in this document
are those of the authors and should not be interpreted as
representing the ofﬁcial policies, either expressed or implied,
of the Army Research Laboratory or the U.S. Government.
The U.S. Government is authorized to reproduce and distribute
reprints for Government purposes notwithstanding any copy-
right notation herein.

REFERENCES

F.

Fang,

[1] C. Kamhoua, C. Kiekintveld,

and
Q. Zhu, Game Theory and Machine Learning for
Cyber Security. Wiley, 2021. [Online]. Available:
https://books.google.com/books?id=EBxszQEACAAJ
[2] P. Dasgupta and J. Collins, “A survey of game
theoretic approaches for adversarial machine learning
in cybersecurity tasks,” AI Magazine, vol. 40,
[Online]. Available:
no. 2, pp. 31–43, Jun. 2019.
https://ojs.aaai.org/index.php/aimagazine/article/view/2847

[3] Y. Zhou, M. Kantarcioglu, and B. Xi, “A survey of game
theoretic approach for adversarial machine learning,”
WIREs Data Mining and Knowledge Discovery, vol. 9,
no. 3, p. e1259, 2019.

[4] Z. Lu, C. Wang, and S. Zhao, “Cyber deception for
computer and network security: Survey and challenges,”
arXiv preprint arXiv:2007.14497, 2020.

[5] J. Pawlick, E. Colbert, and Q. Zhu, “A game-theoretic
taxonomy and survey of defensive deception for cyber-
security and privacy,” ACM Computing Surveys (CSUR),
vol. 52, no. 4, pp. 1–28, 2019.

[6] X. Han, N. Kheir, and D. Balzarotti, “Deception tech-
niques in computer security: A research perspective,”
ACM Computing Surveys, vol. 51, no. 4, Jul. 2018.
[7] N. C. Rowe and J. Rrushi, Introduction to Cyberdecep-

tion. Springer, 2016.

[8] M. H. Almeshekah and E. H. Spafford, “Planning and
integrating deception into computer security defenses,”
in Proceedings of the New Security Paradigms Work-
shop, 2014, pp. 127–138.

[9] T. E. Carroll and D. Grosu, “A game theoretic investi-
gation of deception in network security,” Security and
Communication Networks, vol. 4, no. 10, pp. 1162–
1172, 2011.

[10] N. Virvilis, B. Vanautgaerden, and O. S. Serrano,
“Changing the game: The art of deceiving sophisticated
attackers,” in Proceedings of the 6th International Con-
ference On Cyber Conﬂict (CyCon).
IEEE, 2014, pp.
87–97.

[11] J. H. Jafarian and A. Niakanlahiji, “A deception plan-
ning framework for cyber defense,” in Proceedings of
the 53rd Hawaii International Conference on System
Sciences, 2020.

[12] H. Takabi and J. H. Jafarian, “Insider threat mitigation
using moving target defense and deception,” in Proceed-
ings of the 2017 International Workshop on Managing
Insider Security Threats, 2017, pp. 93–96.

[13] N. C. Rowe, “Finding logically consistent resource-
deception plans for defense in cyberspace,” in 21st
International Conference on Advanced Information
Networking and Applications Workshops (AINAW’07),
vol. 1.

IEEE, 2007, pp. 563–568.

[14] S. Jajodia, N. Park, F. Pierazzi, A. Pugliese, E. Serra,
G. I. Simari, and V. S. Subrahmanian, “A probabilistic
logic of cyber deception,” IEEE Transactions on In-
formation Forensics and Security, vol. 12, no. 11, pp.
2532–2544, 2017.

[15] M. Crouse, B. Prosser, and E. W. Fulp, “Probabilistic
performance analysis of moving target and deception
reconnaissance defenses,” in Proceedings of the 2nd
ACM Workshop on Moving Target Defense, 2015, pp.
21–29.

[16] J.-H. Cho, M. Zhu, and M. P. Singh, Modeling and
Analysis of Deception Games based on Hypergame
Theory. Cham, Switzerland: Springer Nature, 2019,
ch. 4, pp. 49–74.

[17] K. Ferguson-Walter, S. Fugate, J. Mauger, and M. Ma-
jor, “Game theory for adaptive defensive cyber decep-
tion,” in Proceedings of the 6th Annual Symposium on
Hot Topics in the Science of Security, 2019, pp. 1–8.

[18] W. L. Sharp, “Military deception,” Joint War-Fighting
Center, Doctrine and Education Group, Technical Re-
port 3-13.4, 2006.

[19] M. H. Almeshekah and E. H. Spafford, “Cyber security
Springer, 2016, pp.

deception,” in Cyber Deception.
23–50.

[20] J. J. Yuill, “Defensive computer-security deception op-
erations: Processes, principles and techniques,” Ph.D.
dissertation, North Carolina State University, 2006.
[21] Z. Guo, J.-H. Cho, I.-R. Chen, S. Sengupta, M. Hong,
and T. Mitra, “Online social deception and its counter-
measures for trustworthy cyberspace: A survey,” 2020.
[22] Z. Guo, J.-H. Cho, R. Chen, S. Sengupta, M. Hong, and
T. Mitra, “Online social deception and its countermea-
sures: A survey,” IEEE Access, 2020.

report
state
2013
[23] Nextgate.
[Online]. Available:
of
https://www.slideshare.net/prayukth1/2013-state-of-social-media-spam-research-report

(2019) Research

social media

spam.

[24] M. Vergelis, T. Shcherbakova, and T. Sidorina. (2019)
Spam and phishing in Q1 2019. [Online]. Available:
https://securelist.com/spam-and-phishing-in-q1-2019/90795/

[25] M. Forelle, P. Howard, A. Monroy-Hern´andez, and
S. Savage, “Political bots and the manipulation
of public opinion in venezuela,” arXiv Preprint
arXiv:1507.07109, 2015.

[26] J. B. Bell and B. Whaley, Cheating and Deception.

Transaction, New York, 1991.

[27] M. Bennett and E. Waltz, Counterdeception Principles
and Applications for National Security. Artech House,
2007.

[28] J. F. Dunnigan and A. A. Noﬁ, Victory and deceit,

31

2nd ed. Writers Club Press, 2001.

[29] J. Yuill, M. Zappe, D. Denning, and F. Feer, “Hon-
eyﬁles: Deceptive ﬁles for intrusion detection,” in Pro-
ceedings of the 5th Annual IEEE Information Assurance
Workshop, 2004, pp. 116–122.

[30] D. C. Daniel and K. L. Herbig, Strategic Military

Deception. Pergamon, 1982.

[31] J. Yuill, D. Denning, and F. Feer, “Using deception
to hide things from hackers: Processes, principles, and
techniques,” Journal of Information Warfare, vol. 5,
no. 3, pp. 26–40, 2006.

[32] A. Jøsang, J. Cho, and F. Chen, “Uncertainty character-
istics of subjective opinions,” in 2018 21st International
Conference on Information Fusion (FUSION), 2018, pp.
1998–2005.

[33] A. D. Keromytis and S. J. Stolfo, “Systems, methods,
and media for generating bait information for trap-based
defenses,” Aug. 2014, US Patent 8,819,825.

[34] B. M. Bowen, S. Hershkop, A. D. Keromytis, and
S. J. Stolfo, “Baiting inside attackers using decoy doc-
uments,” in International Conference on Security and
Privacy in Communication Systems.
Springer, 2009,
pp. 51–70.

[35] J. W. Caddell, “Deception 101-primer on deception,”

DTIC Document, Tech. Rep., 2004.

[36] P. Chen, L. Desmet, and C. Huygens, “A study on
Advanced Persistent Threats,” in Proceedings of the
IFIP International Conference on Communications and
Multimedia Security. Springer, 2014, pp. 63–72.
[37] H. Okhravi, M. A. Rabe, W. G. Leonard, T. R. Hobson,
D. Bigelow, and W. W. Streilein, “Survey of cyber
moving targets,” Lexington Lincoln Lab, MIT, TR 1166,
2013.

[38] B. C. Ward, S. R. Gomez, R. Skowyra, D. Bigelow,
J. Martin, J. Landry, and H. Okhravi, “Survey of cyber
moving targets second edition,” MIT Lincoln Labora-
tory Lexington United States, Tech. Rep., 2018.
[39] J.-H. Cho, D. P. Sharma, H. Alavizadeh, S. Yoon,
N. Ben-Asher, T. J. Moore, D. S. Kim, H. Lim, and
F. F. Nelson, “Toward proactive, adaptive defense: A
survey on moving target defense,” IEEE Communica-
tions Surveys Tutorials, pp. 1–1, 2020, early access.
[40] I. You and K. Yim, “Malware obfuscation techniques:
A brief survey,” in Proceedings of the International
Conference on Broadband, Wireless Computing, Com-
munication and Applications, 2010, pp. 297–300.
[41] J.-M. Borello and L. M´e, “Code obfuscation techniques
for metamorphic viruses,” Journal in Computer Virol-
ogy, vol. 4, no. 3, pp. 211–220, 2008.

[42] W. Xu, F. Zhang, and S. Zhu, “The power of ob-
fuscation techniques in malicious JavaScript code: A
measurement study,” in Proceedings of
the 7th In-
ternational Conference on Malicious and Unwanted
Software, 2012, pp. 9–16.

[43] C. A. Ardagna, M. Cremonini, E. Damiani, S. De Cap-
itani di Vimercati, and P. Samarati, “Location privacy
protection through obfuscation-based techniques,” in
Data and Applications Security XXI, S. Barker and G.-J.

32

Ahn, Eds. Berlin: Springer, 2007, pp. 47–60.

no. 3, pp. 282–291, 2019.

[44] J.-T. Chan and W. Yang, “Advanced obfuscation tech-
niques for Java bytecode,” Journal of Systems and
Software, vol. 71, no. 1, pp. 1–10, Apr. 2004.

[45] C. Kiekintveld, V. Lis`y, and R. P´ıbil, “Game-theoretic
foundations for the strategic use of honeypots in net-
work security,” in Cyber Warfare. Springer, 2015, pp.
81–101.

[46] D. Mao, S. Zhang, L. Zhang, and Y. Feng, “Game
theory based dynamic defense mechanism for SDN,”
in International Conference on Machine Learning for
Cyber Security. Springer, 2019, pp. 290–303.

[47] J. Pawlick and Q. Zhu, “Deception by design: Evidence-
based signaling games for network defense,” arXiv
preprint arXiv:1503.05458, 2015.

[48] J. Pawlick, S. Farhang, and Q. Zhu, “Flip the cloud:
Cyber-physical signaling games in the presence of ad-
vanced persistent threats,” in Proceedings of the Inter-
national Conference on Decision and Game Theory for
Security. Springer, 2015, pp. 289–308.

[49] A. Kajii and S. Morris, “The robustness of equilibria to
incomplete information,” Econometrica: Journal of the
Econometric Society, pp. 1283–1309, 1997.

[50] T. Bas¸ar and G. J. Olsder, Dynamic Noncooperative

Game Theory. Siam, 1999, vol. 23.

[60] J. Xu and J. Zhuang, “Modeling costly learning and
counter-learning in a defender-attacker game with pri-
vate defender information,” Annals of Operations Re-
search, vol. 236, no. 1, pp. 271–289, 2016.

[61] A. Mas-Colell, M. D. Whinston, and J. R. Green,
Microeconomic theory. Oxford University Press New
York, 1995, vol. 1.

[62] D. Bellhouse, “The problem of waldegrave,” Electronic
Journal for the History of Probability and Statistics,
vol. 3, no. 2, pp. 1–12, 2007.

[63] J. C. Harsanyi, “Games with incomplete information
played by “Bayesian” players, I–III Part I. the basic
model,” Management science, vol. 14, no. 3, pp. 159–
182, 1967.

[64] H. Von Stackelberg, Market structure and equilibrium.

Springer Science & Business Media, 2010.

[65] L. S. Shapley, “Stochastic games,” Proceedings of the
national academy of sciences, vol. 39, no. 10, pp. 1095–
1100, 1953.

[66] P. J. Gmytrasiewicz and P. Doshi, “A framework for
sequential planning in multi-agent settings,” Journal
of Artiﬁcial Intelligence Research, vol. 24, pp. 49–79,
2005.

[67] S. Ross, B. Chaib-draa, and J. Pineau, “Bayes-adaptive

[51] L. Phlips, The economics of

imperfect

information.

pomdps.” in NIPS, 2007, pp. 1225–1232.

Cambridge University Press, 1988.

[52] E. A. Hansen, D. S. Bernstein, and S. Zilberstein, “Dy-
namic programming for partially observable stochastic
games,” in AAAI, vol. 4, 2004, pp. 709–715.

[53] A. R. Cassandra, “A survey of POMDP applications,”
in Working notes of AAAI 1998 fall symposium on
planning with partially observable Markov decision
processes, vol. 1724, 1998.

[54] S. Tadelis, Game Theory: An Introduction. Princeton

University Press, 2013.

[55] A. Clark, Q. Zhu, R. Poovendran, and T. Bas¸ar, “Decep-
tive routing in relay networks,” in Proceedings of the
International Conference on Decision and Game Theory
for Security. Springer, 2012, pp. 171–185.

[56] Q. Zhu, A. Clark, R. Poovendran, and T. Bas¸ar, “De-
ceptive routing games,” in Proceedings of the 51st IEEE
Conference on Decision and Control (CDC).
IEEE,
2012, pp. 2704–2711.

[57] A. Mohammadi, M. H. Manshaei, M. M. Moghaddam,
and Q. Zhu, “A game-theoretic analysis of deception
over social networks using fake avatars,” in Proceedings
of the International Conference on Decision and Game
Theory for Security. Springer, 2016, pp. 382–394.
[58] J. T. House and G. Cybenko, “Hypergame theory ap-
plied to cyber attack and defense,” in Proceedings of the
SPIE Conference on Sensors, and Command, Control,
Communications, and Intelligence (C3I) Technologies
for Homeland Security and Homeland Defense IX, vol.
766604, May. 2010.

[59] X. Zhang, K. W. Hipel, B. Ge, and Y. Tan, “A game-
theoretic model for resource allocation with deception
and defense efforts,” Systems Engineering, vol. 22,

[68] J. Pawlick, E. Colbert, and Q. Zhu, “Modeling and
analysis of leaky deception using signaling games with
evidence,” IEEE Transactions on Information Forensics
and Security, vol. 14, no. 7, pp. 1871–1886, 2018.
[69] R. P´ıbil, V. Lis`y, C. Kiekintveld, B. Boˇsansk`y, and
M. Pˇechouˇcek, “Game theoretic model of strategic hon-
eypot selection in computer networks,” in Proceedings
of the International Conference on Decision and Game
Theory for Security. Springer, 2012, pp. 201–220.
[70] Q. D. La, T. Q. Quek, J. Lee, S. Jin, and H. Zhu, “De-
ceptive attack and defense game in honeypot-enabled
networks for the Internet-of-Things,” IEEE Internet of
Things Journal, vol. 3, no. 6, pp. 1025–1035, 2016.

[71] H. C¸ eker, J. Zhuang, S. Upadhyaya, Q. D. La, and B.-
H. Soong, “Deception-based game theoretical approach
to mitigate DoS attacks,” in Proceedings of the Inter-
national Conference on Decision and Game Theory for
Security. Springer, 2016, pp. 18–38.

[72] A. Basak, C. Kamhoua, S. Venkatesan, M. Gutierrez,
A. H. Anwar, and C. Kiekintveld, “Identifying stealthy
attackers in a game theoretic framework using decep-
tion,” in International Conference on Decision and
Game Theory for Security.
Springer, 2019, pp. 21–
32.

[73] A. N. Kulkarni, H. Luo, N. O. Leslie, C. A. Kamhoua,
and J. Fu, “Deceptive labeling: Hypergames on graphs
for stealthy deception,” IEEE Control Systems Letters,
July, 2021.

[74] B. Xi and C. A. Kamhoua, A Hypergame-Based De-
fense Strategy Toward Cyber Deception in Internet of
Battleﬁeld Things (IoBT). Wiley Online Library, 2020,
pp. 59–77.

[75] K. Durkota, V. Lis`y, B. Boˇsansk`y, and C. Kiekintveld,
“Optimal network security hardening using attack graph
games,” in Proceedings of the 24th International Joint
Conference on Artiﬁcial Intelligence, 2015.

[76] A. N. Kulkarni, J. F. H. Luo, C. A. Kamhoua, and
N. N. Leslie, “Decoy allocation games on graphs with
temporal logic objectives,” in International Conference
on Decision and Game Theory for Security. Springer,
2020.

[77] S. Milani, W. Shen, K. S. Chan, S. Venkatesan, N. O.
Leslie, C. A. Kamhoua, and F. Fang, “Harnessing the
power of deception in attack graph games,” in Interna-
tional Conference on Decision and Game Theory for
Security. Springer, 2020.

[78] O. Tsemogne, Y. Hayel, C. A. Kamhoua, and G. Deu-
goue, “Partially observable stochastic games for cyber
deception against epidemic process,” in International
Conference on Decision and Game Theory for Security.
Springer, 2020.

[79] G. Wagener, A. Dulaunoy, T. Engel et al., “Self adaptive
high interaction honeypots driven by game theory,” in
Symposium on Self-Stabilizing Systems. Springer, 2009,
pp. 741–755.

[80] A. Greenwald, “Matrix games and nash equilibrium,”
Lecture in game-theoretic artiﬁcial intelligence, Brown
University CS Dep. Providence, US, 2007.

[81] P. Aggarwal, C. Gonzalez, and V. Dutt, “Cyber-security:
Role of deception in cyber-attack detection,” in Ad-
vances in Human Factors in Cybersecurity. Springer,
2016, pp. 85–96.

[82] ——, “Modeling the effects of amount and timing of de-
ception in simulated network scenarios,” in 2017 Inter-
national Conference On Cyber Situational Awareness,
Data Analytics And Assessment (Cyber SA).
IEEE,
2017, pp. 1–7.

[83] A. H. Anwar and C. Kamhoua, “Game theory on attack
graph for cyber deception,” in International Conference
on Decision and Game Theory for Security. Springer,
2020.

[84] C. A. Kamhoua, “Game theoretic modeling of cyber
deception in the internet of battleﬁeld things,” in 2018
56th Annual Allerton Conference on Communication,
Control, and Computing (Allerton).
IEEE, 2018, pp.
862–862.

[85] A. H. Anwar, C. Kamhoua, and N. Leslie, “A game-
theoretic framework for dynamic cyber deception in
internet of battleﬁeld things,” in Proceedings of the
16th EAI International Conference on Mobile and Ubiq-
uitous Systems: Computing, Networking and Services,
2019, pp. 522–526.

[86] A. H. Anwar, C. Kamhoua, and N. Leslie, “Honeypot
allocation over attack graphs in cyber deception games,”
in 2020 International Conference on Computing, Net-
working and Communications (ICNC), 2020, pp. 502–
506.

[87] S. Nan, S. Brahma, C. A. Kamhoua, and N. Leslie,
“Behavioral cyber deception: A game and prospect the-
oretic approach,” in 2019 IEEE Global Communications

33

Conference (GLOBECOM).

IEEE, 2019, pp. 1–6.

[88] S. Nan, S. Brahma, C. A. Kamhoua, and L. L.
Njilla, On Development of a Game-Theoretic Model for
Deception-Based Security. Wiley Online Library, 2020,
pp. 123–140.

[89] A. El-Kosairy and M. A. Azer, “A new web decep-
the 1st
tion system framework,” in Proceedings of
International Conference on Computer Applications &
Information Security (ICCAIS).
IEEE, 2018, pp. 1–10.
[90] N. Garg and D. Grosu, “Deception in honeynets: A
game-theoretic analysis,” in Proceedings of the IEEE
SMC Information Assurance and Security Workshop.
IEEE, 2007, pp. 107–113.

[91] C. K. Dimitriadis, “Improving mobile core network
security with honeynets,” IEEE Security & Privacy,
vol. 5, no. 4, pp. 40–47, 2007.

[92] L. Spitzner, “The honeynet project: Trapping the hack-
ers,” IEEE Security & Privacy, vol. 1, no. 2, pp. 15–23,
2003.

[93] R. Shokri, “Privacy games: Optimal user-centric data
obfuscation,” Proceedings on Privacy Enhancing Tech-
nologies, vol. 2015, no. 2, pp. 299–315, 2015.

[94] J. P. Hespanha, Y. S. Ateskan, H. Kizilocak et al.,
“Deception in non-cooperative games with partial in-
formation,” in Proceedings of the 2nd DARPA-JFACC
Symposium on Advances in Enterprise Control, 2000,
pp. 1–9.

[95] Y. Yin, B. An, Y. Vorobeychik, and J. Zhuang, “Optimal
deceptive strategies in security games: A preliminary
study,” in Proceedings of
the AAAI Conference on
Artiﬁcial Intelligence, 2013.

[96] K. Hor´ak, Q. Zhu, and B. Boˇsansk`y, “Manipulating ad-
versary’s belief: A dynamic game approach to deception
by design for proactive network security,” in Proceed-
ings of the International Conference on Decision and
Game Theory for Security.
Springer, 2017, pp. 273–
294.

[97] M. Bilinski, K. Ferguson-Walter, S. Fugate, R. Gabrys,
J. Mauger, and B. Souza, “You only lie twice: A multi-
round cyber deception game of questionable veracity,”
in International Conference on Decision and Game
Theory for Security. Springer, 2019, pp. 65–84.
[98] C.-Y. J. Chiang, S. Venkatesan, S. Sugrim, J. A.
Youzwak, R. Chadha, E. I. Colbert, H. Cam, and
M. Albanese, “On defensive cyber deception: A case
study using SDN,” in Proceedings of the IEEE Military
Communications Conference (MILCOM).
IEEE, 2018,
pp. 110–115.

[99] L. Huang and Q. Zhu, “Dynamic Bayesian games
for adversarial and defensive cyber deception,” in Au-
tonomous Cyber Deception. Springer, 2019, pp. 75–97.
[100] W. A. Casey, Q. Zhu, J. A. Morales, and B. Mishra,
“Compliance control: Managed vulnerability surface in
social-technological systems via signaling games,” in
Proceedings of the 7th ACM CCS International Work-
shop on Managing Insider Security Threats, 2015, pp.
53–62.

[101] W. Casey, J. A. Morales, E. Wright, Q. Zhu, and

B. Mishra, “Compliance signaling games: Toward mod-
eling the deterrence of insider threats,” Computational
and Mathematical Organization Theory, vol. 22, no. 3,
pp. 318–349, 2016.

[102] O. Thakoor, M. Tambe, P. Vayanos, H. Xu, C. Kiek-
intveld, and F. Fang, “Cyber camouﬂage games for
strategic deception,” in International Conference on
Decision and Game Theory for Security.
Springer,
2019, pp. 525–541.

[103] Q. Zhu, A. Clark, R. Poovendran, and T. Bas¸ar, “De-
ployment and exploitation of deceptive honeybots in
the 52nd IEEE
social networks,” in Proceedings of
Conference on Decision and Control.
IEEE, 2013,
pp. 212–219.

[104] J. Avery and E. H. Spafford, “Ghost patches: Fake
patches for fake vulnerabilities,” in Proceedings of the
32nd IFIP TC 11 International Conference on ICT Sys-
tems Security and Privacy Protection, S. De Capitani di
Rome: Springer
Vimercati and F. Martinelli, Eds.
International Publishing, 2017, pp. 399–412.

[105] F. Araujo, K. W. Hamlen, S. Biedermann, and
S. Katzenbeisser, “From patches to honey-patches:
Lightweight attacker misdirection, deception, and disin-
formation,” in Proceedings of the ACM SIGSAC Confer-
ence on Computer and Communications Security. New
York, NY, USA: ACM, 2014, pp. 942–953.

[106] C. Cadar, D. Dunbar, and D. R. Engler, “KLEE: Unas-
sisted and automatic generation of high-coverage tests
for complex systems programs,” in Operating Systems
Design and Implementation, vol. 8, 2008, pp. 209–224.
[107] J. Avery and J. R. Wallrabenstein, “Formally modeling
deceptive patches using a game-based approach,” Com-
puters & Security, vol. 75, pp. 182–190, 2018.
[108] M. Miah, N. Sharmin, I. Anjum, M. Zhu, C. Kiek-
intveld, W. H. Enck, and M. P. Singh, “Optimizing
vulnerability-driven honey trafﬁc using game theory,” in
Proceedings of the Workshop on Artiﬁcial Intelligence
for Cyber Security (AICS), New York, Feb. 2020, pp.
1–7.

[109] M. O. Sayin and T. Bas¸ar, “Deception-as-defense
framework for cyber-physical systems,” arXiv preprint
arXiv:1902.01364, 2019.

[110] M. Al Amin, S. Shetty, L. Njilla, D. Tosh, and
C. Kamouha, “Attacker capability based dynamic de-
ception model for large-scale networks,” EAI Endorsed
Transactions on Security and Safety, vol. 6, no. 21,
2019.

[111] M. A. R. Al Amin, S. Shetty, L. L. Njilla, D. K. Tosh,
and C. A. Kamhoua, “Dynamic cyber deception using
partially observable monte-carlo planning framework,”
Modeling and Design of Secure Internet of Things, pp.
331–355, 2020.

[112] S. Nan, S. Brahma, C. A. Kamhoua, and N. O.
Leslie, “Mitigation of jamming attacks via deception,”
in 2020 IEEE 31st Annual International Symposium on
Personal, Indoor and Mobile Radio Communications.
IEEE, 2020, pp. 1–6.

[113] S. Wang, Q. Pei, J. Wang, G. Tang, Y. Zhang, and

34

X. Liu, “An intelligent deployment policy for decep-
tion resources based on reinforcement learning,” IEEE
Access, vol. 8, pp. 35 792–35 804, 2020.

[114] A. H. Anwar, C. Kamhoua, and N. Leslie, “Honeypot
allocation over attack graphs in cyber deception games,”
in 2020 International Conference on Computing, Net-
working and Communications (ICNC).
IEEE, 2020,
pp. 502–506.

[115] M. A. R. Al Amin, S. Shetty, L. Njilla, D. K. Tosh,
and C. Kamhoua, “Online cyber deception system using
partially observable Monte-Carlo planning framework,”
in Proceedings of the International Conference on Secu-
rity and Privacy in Communication Systems. Springer,
2019, pp. 205–223.

[116] M. A. Rahman, M. H. Manshaei, and E. Al-Shaer, “A
game-theoretic approach for deceiving remote operating
system ﬁngerprinting,” in Proceedings of
the IEEE
Conference on Communications and Network Security
(CNS).

IEEE, 2013, pp. 73–81.

[117] Z. R. Shi, A. D. Procaccia, K. S. Chan, S. Venkatesan,
N. Ben-Asher, N. O. Leslie, C. Kamhoua, and F. Fang,
“Learning and planning in feature deception games,” in
International Conference on Decision and Game Theory
for Security. Springer, 2020.

[118] E. Cranford, C. Gonzalez, P. Aggarwal, S. Cooney,
M. Tambe, and C. Lebiere, “Adaptive cyber deception:
Cognitively informed signaling for cyber defense,” in
Proceedings of the 53rd Hawaii International Confer-
ence on System Sciences, 2020.

[119] E. A. Cranford, C. Gonzalez, P. Aggarwal, S. Cooney,
M. Tambe, and C. Lebiere, “Towards personalized
deceptive signaling for cyber defense using cognitive
models,” in Proceedings of the 17th Annual Meeting of
the ICCM, Montreal, CA, vol. 56, 2019.

[120] K. Ferguson-Walter, T. Shade, A. Rogers, M. C. S.
Trumbo, K. S. Nauer, K. M. Divis, A. Jones, A. Combs,
and R. G. Abbott, “The tularosa study: An experimental
design and implementation to quantify the effectiveness
of cyber deception.” Sandia National Lab (SNL-NM),
Albuquerque, NM (United States), Tech. Rep., 2018.

[121] K. J. Ferguson-Walter, “An empirical assessment of the
effectiveness of deception for cyber defense,” Ph.D. dis-
sertation, University of Massachusetts Amherst, 2020.
[122] P. Aggarwal, A. Gautam, V. Agarwal, C. Gonzalez,
and V. Dutt, “HackIt: A human-in-the-loop simulation
tool for realistic cyber deception experiments,” in In-
ternational Conference on Applied Human Factors and
Ergonomics. Springer, 2019, pp. 109–121.

[123] E. Al-Shaer, J. Wei, K. W. Hamlen, and C. Wang,
“Towards intelligent cyber deception systems,” in Au-
tonomous Cyber Deception.
Springer, 2019, pp. 21–
33.

[124] J. Song, H. Takakura, Y. Okabe, M. Eto, D. Inoue, and
K. Nakao, “Statistical analysis of honeypot data and
building of kyoto 2006+ dataset for nids evaluation,” in
Proceedings of the First Workshop on Building Analysis
Datasets and Gathering Experience Returns for Secu-
rity, 2011, pp. 29–36.

[125] C. Snijders, U. Matzat, and U.-D. Reips, ““big data”:
Big gaps of knowledge in the ﬁeld of Internet science,”
International Journal of Internet Science, vol. 7, no. 1,
pp. 1–5, 2012.

[126] J. Han, J. Pei, and M. Kamber, Data Mining: Concepts

and Techniques. Elsevier, 2011.

[127] K. Ron and P. Foster, “Special issue on applications
of machine learning and the knowledge discovery pro-
cess,” Journal of Machine Learning, vol. 30, pp. 271–
274, 1998.

[128] S. Nanda, F. Zafari, C. DeCusatis, E. Wedaa, and
B. Yang, “Predicting network attack patterns in SDN
using machine learning approach,” in Proceedings of the
IEEE Conference on Network Function Virtualization
and Software Deﬁned Networks (NFV-SDN).
IEEE,
2016, pp. 167–172.

[129] P. R. Badri Satya, K. Lee, D. Lee, T. Tran, and J. J.
Zhang, “Uncovering fake likers in online social net-
works,” in Proceedings of the 25th ACM International
on Conference on Information and Knowledge Manage-
ment. ACM, 2016, pp. 2365–2370.

[130] E. B. El Idrissi Younes, E. M. Fatna, and M. Nisrine,
“A security approach for social networks based on
honeypots,” in 2016 4th IEEE International Colloquium
on Information Science and Technology (CiSt), 2016,
pp. 638–643.

[131] T. Krueger, H. Gascon, N. Kr¨amer, and K. Rieck,
“Learning stateful models for network honeypots,” in
Proceedings of the 5th ACM workshop on Security and
artiﬁcial intelligence, 2012, pp. 37–48.

[132] K. Lee, J. Caverlee, and S. Webb, “Uncovering social
spammers: Social honeypots + machine learning,” in
Proceedings of the 33rd International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, 2010, pp. 435–442.

[133] K. Lee, B. D. Eoff, and J. Caverlee, “Seven months with
the devils: A long-term study of content polluters on
Twitter,” in Proceedings of the 5th International AAAI
Conference on Weblogs and Social Media, 2011, pp.
185–192.

[134] W. Hofer, T. Edgar, D. Vrabie, and K. Nowak, “Model-
driven deception for control system environments,” in
Proceedings of the IEEE International Symposium on
Technologies for Homeland Security (HST).
IEEE,
2019, pp. 1–7.

[135] M. Ben Salem and S. Stolfo, “Combining baiting and
user search proﬁling techniques for masquerade detec-
tion,” Journal of Wireless Mobile Networks, Ubiquitous
Computing and Dependable Applications, vol. 3, no. 1,
Mar. 2012.

[136] B. Whitham, “Minimising paradoxes when employing
honeyﬁles to combat data theft in military networks,”
in Proceedings of the Military Communications and
Information Systems Conference (MilCIS). IEEE, 2016,
pp. 1–6.

35

ence. ACM, 2010, pp. 1–9.

[138] B. Whitham, “Automating the generation of enticing
text content for high-interaction honeyﬁles,” in Proceed-
ings of the 50th Hawaii International Conference on
System Sciences, 2017.

[139] N. C. Abay, C. G. Akcora, Y. Zhou, M. Kantarcioglu,
and B. Thuraisingham, “Using deep learning to generate
relational HoneyData,” in Autonomous Cyber Decep-
tion. Springer, 2019, pp. 3–19.

[140] C. Cortes and V. Vapnik, “Support-vector networks,”

Machine learning, vol. 20, no. 3, pp. 273–297, 1995.

[141] W. contributors. K-means clustering.
[142] T. K. Moon, “The expectation-maximization algorithm,”
IEEE Signal processing magazine, vol. 13, no. 6, pp.
47–60, 1996.

[143] J. H. Ward Jr, “Hierarchical grouping to optimize an
objective function,” Journal of the American statistical
association, vol. 58, no. 301, pp. 236–244, 1963.
[144] N. Friedman, D. Geiger, and M. Goldszmidt, “Bayesian
network classiﬁers,” Machine learning, vol. 29, no. 2,
pp. 131–163, 1997.

[145] S. R. Safavian and D. Landgrebe, “A survey of decision
tree classiﬁer methodology,” IEEE transactions on sys-
tems, man, and cybernetics, vol. 21, no. 3, pp. 660–674,
1991.

[146] J. R. Quinlan, C4. 5: programs for machine learning.

Elsevier, 2014.

[147] P. Domingos and M. Pazzani, “On the optimality of
the simple bayesian classiﬁer under zero-one loss,”
Machine learning, vol. 29, no. 2, pp. 103–130, 1997.

[148] H. Zhu, “Fighting against social spammers on Twitter
by using active honeypots,” Ph.D. dissertation, McGill
University Libraries, 2015.

[149] C. Yang, J. Zhang, and G. Gu, “A taste of tweets:
Reverse engineering Twitter spammers,” in Proceedings
the 30th Annual Computer Security Applications
of
Conference. ACM, 2014, pp. 86–95.

[150] D. Montigny-Leboeuf and F. Massicotte, “Passive net-
work discovery for real time situation awareness,” Com-
munications Research Centre Ottawa (Ontario), Tech.
Rep., 2004.

[151] G. Bartlett, J. Heidemann, and C. Papadopoulos, “Un-
derstanding passive and active service discovery,” in
Proceedings of the 7th ACM SIGCOMM conference on
Internet measurement, 2007, pp. 57–70.
[152] G. Lyon, “Nmap website,” https://nmap.org/.
[153] M. Zalewski.

[Online]. Available:

(2014) p0f v3.
https://lcamtuf.coredump.cx/p0f3/

[154] O. Arkin and F. Yarochkin, “Xprobe v2. 0,” Fuzzy”
Approach to Remote Active Operating Systems Finger-
printing,” http://www. xprobe2. org, 2002.

[155] T. Peng, C. Leckie, and K. Ramamohanarao, “Survey
of network-based defense mechanisms countering the
DoS and DDoS problems,” ACM Computing Surveys
(CSUR), vol. 39, no. 1, pp. 3–es, 2007.

[137] G. Stringhini, C. Kruegel, and G. Vigna, “Detecting
spammers on social networks,” in Proceedings of The
26th Annual Computer Security Applications Confer-

[156] S. T. Zargar, J. Joshi, and D. Tipper, “A survey of
defense mechanisms against distributed denial of ser-
vice (DDoS) ﬂooding attacks,” IEEE Communications

Surveys & Tutorials, vol. 15, no. 4, pp. 2046–2069,
2013.

[157] S. Rathore, P. K. Sharma, V. Loia, Y.-S. Jeong, and
J. H. Park, “Social network security: Issues, challenges,
threats, and solutions,” Information Sciences, vol. 421,
pp. 43–69, 2017.

[158] G. Wang, C. Wilson, X. Zhao, Y. Zhu, M. Mohanlal,
H. Zheng, and B. Y. Zhao, “Serf and turf: Crowdturﬁng
for fun and proﬁt,” in Proceedings of the 21st Interna-
tional Conference on World Wide Web. ACM, 2012,
pp. 679–688.

[159] M. B. Salem, S. Hershkop, and S. J. Stolfo, “A survey
of insider attack detection research,” in Insider Attack
and Cyber Security. Springer, 2008, pp. 69–90.
[160] K. Grover, A. Lim, and Q. Yang, “Jamming and anti–
jamming techniques in wireless networks: A survey,”
International Journal of Ad Hoc and Ubiquitous Com-
puting, vol. 17, no. 4, pp. 197–215, 2014.

[161] B. Djamaluddin, A. Alnazeer, and F. Azzedin, “Web
deception towards moving target defense,” in 2018
International Carnahan Conference on Security Tech-
nology (ICCST).

IEEE, 2018, pp. 1–5.

[162] S. Kandula, R. Mahajan, P. Verkaik, S. Agarwal,
in
J. Padhye, and P. Bahl, “Detailed diagnosis
enterprise networks,” SIGCOMM Comput. Commun.
Rev., vol. 39, no. 4, pp. 243–254, Aug. 2009. [Online].
Available: https://doi.org/10.1145/1594977.1592597

[163] N. C. Rowe, B. T. Duong, and E. J. Custy, “Fake
honeypots: A defensive tactic for cyberspace,” in Proc.
of the IEEE Workshop on Information Assurance, 2006,
pp. 223–230.

[164] R. Poovendran, “Cyber-Physical Systems: Close en-
counters between two parallel worlds,” Proceedings of
the IEEE, vol. 98, no. 8, pp. 1363–1366, Aug. 2010.

[165] P. Kathiravelu and L. Veiga, “Sd-cps:

taming the
challenges of cyber-physical systems with a software-
deﬁned approach,” in 2017 Fourth International Con-
ference on Software Deﬁned Systems (SDS).
IEEE,
2017, pp. 6–13.

[166] D. Serpanos, “The cyber-physical systems revolution,”

Computer, vol. 51, no. 3, pp. 70–73, 2018.

[167] A. I. M. Efendi, Z. Ibrahim, M. N. A. Zawawi, F. Abdul
Rahim, N. A. M. Pahri, and A. Ismail, “A survey on
deception techniques for securing web application,” in
2019 IEEE 5th Int’l Conf. on Big Data Security on
Cloud (BigDataSecurity), 2019, pp. 328–331.

[168] A. Kott, A. Swami, and B. West, “The Internet of Battle

Things,” IEEE Computer, Dec. 2016.

[169] S. Pinto, T. Gomes, J. Pereira, J. Cabral, and A. Tavares,
“Iioteed: An enhanced, trusted execution environment
for industrial iot edge devices,” IEEE Internet Comput-
ing, vol. 21, no. 1, pp. 40–47, 2017.

[170] J. J. P. C. Rodrigues, D. B. De Rezende Segundo, H. A.
Junqueira, M. H. Sabino, R. M. Prince, J. Al-Muhtadi,
and V. H. C. De Albuquerque, “Enabling technologies
for the internet of health things,” IEEE Access, vol. 6,
pp. 13 129–13 141, 2018.

[171] M. Kocakulak and I. Butun, “An overview of wire-

36

less sensor networks towards Internet of Things,” in
Proceedings of the IEEE 7th Annual Computing and
Communication Workshop and Conference (CCWC).
IEEE, 2017, pp. 1–6.

[172] A. Alshammari, D. B. Rawat, M. Garuba, C. A.
Kamhoua, and L. L. Njilla, Deception for Cyber Ad-
versaries: Status, Challenges, and Perspectives. Wiley
Online Library, 2020, pp. 141–160.

[173] D. C. MacFarland and C. A. Shue, “The SDN shuf-
ﬂe: Creating a moving-target defense using host-based
software-deﬁned networking,” in ACM Workshop on
Moving Target Defense, 2015, pp. 37–41.

[174] K. Benton, L. J. Camp, and C. Small, “Openﬂow
vulnerability assessment,” in Proceedings of the 2nd
ACM SIGCOMM Workshop on Hot Topics in Software
Deﬁned Networking, 2013, pp. 151–152.

[175] A. Dixit, F. Hao, S. Mukherjee, T. Lakshman, and
R. Kompella, “Towards an elastic distributed SDN
controller,” in Proceedings of the 2nd ACM SIGCOMM
Workshop on Hot Topics in Software Deﬁned Network-
ing, 2013, pp. 7–12.

[176] J.-Y. Cai, V. Yegneswaran, C. Alfeld, and P. Barford,
“An attacker-defender game for honeynets,” in Inter-
national Computing and Combinatorics Conference.
Springer, 2009, pp. 7–16.

[177] J.-H. Cho and N. Ben-Asher, “Cyber defense in breadth:
Modeling and analysis of integrated defense systems,”
The Journal of Defense Modeling and Simulation,
vol. 15, no. 2, pp. 147–160, 2018.

[178] C.-Y.

J. Chiang, Y. M. Gottlieb, S.

J. Sugrim,
R. Chadha, C. Serban, A. Poylisher, L. M. Marvel,
and J. Santos, “ACyDS: An adaptive cyber deception
system,” in 2016 IEEE Military Communications Con-
ference.

IEEE, 2016, pp. 800–805.
[179] R. Chadha, T. Bowen, C.-Y. J. Chiang, Y. M. Got-
tlieb, A. Poylisher, A. Sapello, C. Serban, S. Sugrim,
G. Walther, L. M. Marvel et al., “CyberVAN: A cyber
security virtual assured network testbed,” in MILCOM
2016-2016 IEEE Military Communications Conference.
IEEE, 2016, pp. 1125–1130.

[180] P. Danzig, J. Mogul, V. Paxson, and M. Schwartz.
(2008) The Internet trafﬁc archive. [Online]. Available:
http://ita.ee.lbl.gov/html/traces.html

[181] L. H. Pham, M. Albanese, R. Chadha, C.-Y. J. Chiang,
S. Venkatesan, C. Kamhoua, and N. Leslie, “A quantita-
tive framework to model reconnaissance by stealthy at-
tackers and support deception-based defenses,” in 2020
IEEE Conference on Communications and Network
Security (CNS).
IEEE, 2020, pp. 1–9.

[182] Y. Liu, H. Wang, M. Peng, J. Guan, J. Xu, and Y. Wang,
“DeePGA: A privacy-preserving data aggregation game
in crowdsensing via deep reinforcement learning,” IEEE
Internet of Things Journal, vol. 7, no. 5, pp. 4113–4127,
2020.

[183] Q. Xu, Z. Su, and R. Lu, “Game theory and reinforce-
ment learning based secure edge caching in mobile
social networks,” IEEE Transactions on Information
Forensics and Security, vol. 15, pp. 3415–3429, 2020.

[184] C. Bakker, A. Bhattacharya, S. Chatterjee, and D. L.
Vrabie, “Metagames and hypergames for deception-
robust control,” ACM Transactions on Cyber-Physical
Systems, vol. 5, no. 3, pp. 1–25, 2021.

37

