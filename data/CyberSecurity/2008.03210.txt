0
2
0
2

g
u
A
7

]

R
C
.
s
c
[

1
v
0
1
2
3
0
.
8
0
0
2
:
v
i
X
r
a

Chapter 6

A Theory of Hypergames on

Graphs for Synthesizing

Dynamic Cyber Defense with

Deception

Abhishek N. Kulkarni1 and Jie Fu2*

1Robotics Engineering Program, Worcester Polytechnic Institute,
100 Institute Rd., Worcester, MA, US.
2Dept. of Electrical and Computer Engineering, Robotics Engineering Program, Worcester
Polytechnic Institute, 100 Institute Rd., Worcester, MA,US

*Corresponding Author: Jie Fu; jfu2@wpi.edu

Abstract: In this chapter, we present an approach using formal meth-

ods to synthesize reactive defense strategy in a cyber network, equipped

with a set of decoy systems. We ﬁrst generalize formal graphical secu-

rity models–attack graphs–to incorporate defender’s countermeasures in a

game-theoretic model, called an attack-defend game on graph. This game

captures the dynamic interactions between the defender and the attacker

and their defense/attack objectives in formal logic. Then, we introduce a

class of hypergames to model asymmetric information created by decoys

in the attacker-defender interactions. Given qualitative security speciﬁca-

tions in formal logic, we show that the solution concepts from hypergames

1

 
 
 
 
 
 
and reactive synthesis in formal methods can be extended to synthesize ef-

fective dynamic defense strategy using cyber deception. The strategy takes

the advantages of the misperception of the attacker to ensure security spec-

iﬁcation is satisﬁed, which may not be satisﬁable when the information is

symmetric.

Keywords: Attack Graphs, Hypergame, Formal Methods

6.1. Introduction

Cyber deception is a key technique in network defense. With cyber decep-

tion, the defender creates uncertainties and unknowns for the attacker. By

doing so, the attacker’s strategy in exploiting the system becomes less eﬀec-

tive, thus, resulting in improved security and safety of the network. In this

chapter, we investigate a formal methods approach for synthesizing defensive

strategies in cyber network systems with cyber deception. We employ formal

security speciﬁcations to express a rich class of desired properties. For exam-

ple, a defender may need to satisfy a safety property in terms of preventing

the attacker from reaching critical data server. He may also need to satisfy a

liveness property stating that a service should eventually be provided to the

user after being made temporarily unavailable. Given formal security speciﬁ-

cations, formal synthesis is to compute a defense strategy, if exists, with which

the defender can provably satisfy his speciﬁcation against all possible actions

from the attacker.

Formal methods have been employed to verify the security of network sys-

tems. Formal graphical security models such as attack graphs [Jha et al., 2002]

and attack trees [Schneier, 2007] are used in model-based veriﬁcation of system

security. An attack graph captures multiple paths that an attacker can carry

2

out by exploiting vulnerabilities and their dependencies in a network to reach

the attack goal. Given an attack graph, the formal security speciﬁcation can

be veriﬁed using model checking algorithms for transition systems [Baier and

Katoen, 2008]. An attack tree builds a tree structure that describes how the

attacker can achieve his goal by achieving a set of subgoals. The root of the

tree is the main attack goal and the leaves of the tree are elementary attack

subgoals. The internal tree nodes shows the logical dependency between sub-

goals at diﬀerent levels of the tree. To incorporate defender’s counter-measures,

attack-defense trees [Kordy et al., 2010, 2014] are proposed to capture the de-

pendencies between actions and subgoals for both attacker and defender. These

models are used in verifying quantitative security properties in temporal logic

[Aslanyan et al., 2016, Hansen et al., 2017, Kordy and Wide(cid:32)l, 2018]. The ma-

jor limitation of attack trees is that it does not characterize network status

changes under the attack actions and thus may fail to generate some attack

scenarios. It is also noted that these formal graphical models do not capture

the asymmetric information between the attacker and the defender due to cy-

ber deception. Speciﬁcally, these models assume both defender and attacker

knows the game they are playing, while as with cyberdeception, the defender

intentionally introduces incorrect or uncertain information about the game to

the attacker.

Active deception [Jajodia et al., 2016] employs decoy systems and other

defenses, including access control and online network reconﬁguration, to con-

duct deceptive planning against the intrusion of malicious users who have been

detected and conﬁrmed by sensing systems. To design defense strategies with

deception, game theory has been employed [Hor, 2012, Al-Shaer et al., 2019,

Horak et al., 2019, Cohen, 2006, Zhu and Rass, 2018]. These game-theoretic

3

models express the attacker and defender’s objectives using reward/loss func-

tions. In [Horak et al., 2019], a partially observable stochastic game is for-

mulated to capture the interaction between an attacker and a defender with

one-sided partial observations. The attacker is to exploit and compromise the

system without being detected and has complete observation. The defender is

to detect the attacker and reconﬁgure the honeypots. Hor [2012] consider the

case when the attacker has incomplete information and forms a belief about

the defender’s unit. Players employ Bayesian rules to update the belief about

the state in the game. Leveraging the attacker’s incomplete information, the

defender may mislead the attacker’s belief and thus his actions to minimize

the damage to the network measured by a state-dependent loss function. How-

ever, reward and loss functions are not expressive enough to capture more

complex qualitative defense/attack objectives studied in attack graph, such

as safety and temporally extended attack goals. These objectives can be cap-

tured succinctly using temporal logic [Manna and Pnueli, 1992]. When formal

speciﬁcation is used in specifying defense objectives, there is a lack of formal

synthesis methods which employ cyber deception to ensure the security goals

are met.

We study the problem of formal synthesis of secured network systems with

active cyber deception. We view the interactions between the defender and the

attacker as a two-player game played on a ﬁnite graph. Combining the game

graph abstraction with the logical security speciﬁcations, we construct a model

of an attack-defend game as a game on a graph with temporal logic objectives

[Pnueli and Rosner, 1989, Chatterjee and Henzinger, 2012]. This game includes

both the controllable and uncontrollable actions to represent the actions and

exploits by the defender and the attacker, respectively.

4

In such a game between a defender and an attacker, the attacker plays

with incomplete information, if he does not know the locations of honeypots.

Furthermore, if the attacker mistakes a honeypot as a critical host, then we

say that he has a misperception about the game. We extend the theory of

hypergame to reason about the asymmetric incomplete information between

players and to enable synthesis of deceptive strategies. A hypergame [Bennett,

1980, Vane, 2000, Kovach et al., 2015] is a game of perceptual games, i.e., games

perceived by individual players given the information available to them and the

higher-order information known to them, i.e., what the player knows about the

information known to the opponent. Based on the hypergame modeling, the

key questions are: how will the attacker carry out his attack mission, given his

incomplete or incorrect information? And, how to synthesize eﬀective defense

strategies, which leverage the defender’s private information to ensure that the

defender’s logical security speciﬁcations are satisﬁed?

Our insight is that deception with honeypots can create a misperception

about the labeling function of the attack-defend game. A labeling function re-

lates an outcome—a sequence of states in the game graph—to the properties

speciﬁed in logic. When honeypots are introduced, an attacker might mislabel

a honeypot as a critical host and pursue to reach it. Under this formulation,

our main algorithmic contribution is the solution of hypergames under label-

ing misperception and linear temporal logic objectives. Our solution approach

includes two steps: The ﬁrst step is to synthesize the rational attack strategy

perceived by the attacker using solutions of omega-regular games [Chatterjee

and Henzinger, 2012, Zielonka, 1998]. The synthesized strategy serves as a pre-

dictive model of the attacker’s rational behavior, which is then used to reﬁne

the original game graph to eliminate actions perceived to be irrational from

5

the attacker’s perspective. In the second step, a level-2 hypergame is solved,

yielding a deceptive defense strategy, if one exists, that ensures a speciﬁcation

is satisﬁed with probability one, given the misperception of the attacker. A

case study is employed to illustrate how to apply the game-theoretic reasoning

to synthesize deceptive strategies.

We structure the remainder of the chapter so as to provide rigorous math-

ematical treatment of the topic for a reader familiar with formal methods, and

support it with elaborate descriptions, discussions and examples to illustrate

our approach to a reader new to the area.

6.2. Attack-Defend Games on Graph

In this section, we introduce a model, called An Attack-Defend (AD) Game

on a Graph, that augments the attack graph model with the defense actions

available to the defender. Our AD game on graph model resembles the game

on graph model, which is commonly used in reactive synthesis [Pnueli and

Rosner, 1989].

Formally, an AD game on graph can be written as a tuple G = (cid:104)G, ϕ(cid:105) where

the two main components are (i) G: a game arena, and (ii) ϕ: the Boolean payoﬀ

function (Linear Temporal Logic (LTL) speciﬁcation) of the defender. Let us

understand each of the component in more detail.

6.2.1. Game Arena

A game arena is a transition system with labels assigned to the states. It cap-

tures diﬀerent conﬁgurations of the network and the actions that the attacker

6

and the defender may use to change the current conﬁguration. A conﬁguration

of system is a set of state variables that jointly deﬁne the current state of the

system. For instance, a state variable may be a collection of the current host

compromised by the attacker, IP addresses of diﬀerent hosts over the network,

an enumeration of services running over each host, or a list of users currently

accessing the hosts with their privileges (root, user, none). Suppose that there

are n state variables and we denote the i-th state variable as Xi, then the

domain of a state-space can be given by S = X1 × X2 × . . . × Xn. Given this

notion of state, we formally deﬁne a game arena as follows:

Deﬁnition 6.1(Arena): A turn-based, deterministic game arena between

two players P1 (defender, pronoun “he”) and P2 (attacker, pronoun “she”) is

a tuple

G = (cid:104)S, A, T, AP, L(cid:105),

whose components are deﬁned as follows:

• S = S1 ∪ S2 is a ﬁnite set of states partitioned into two sets S1 and S2.

At a state in S1, P1 chooses an action and at a state in S2, P2 selects an

action.

• A = A1 ∪ A2 is the set of actions. A1 (resp., A2) is the set of actions for

P1 (resp., P2);

• T : (S1 × A1) ∪ (S2 × A2) → S is a deterministic transition function that

maps a state-action pair to a next state.

• AP is the set of atomic propositions.

• L : S → 2AP is the labeling function that maps each state s ∈ S to a set

L(s) ⊆ AP of atomic propositions that evaluate to true at that state.

7

Figure 6.1: Conﬁguration of the system.

The last two components of the game arena are related to the security

speciﬁcations.

We discuss an example to illustrate the above concept.

Example 6.1: Consider the system as shown in Fig. 6.1 consisting of three

hosts with platform diversity. Each host can hold up to two Virtual Machine

(VM)s with diﬀerent operation systems and services. For a ﬁxed conﬁguration

of VMs, a fragment of the attack graph can be generated based on the set of

known vulnerabilities, as shown in Fig. 6.2a.

(a) A fragment of the attack graph corresponding to one ﬁxed conﬁguration.

(b) (A fragment of) the attack-defend game arena.

Figure 6.2: The comparison between the attack graph and the attack-defend
game arena.

8

In reactive defense, the defender has detected the attacker is in host 0 and

can exploit the vulnerability to gain user access to host 1. In that case, the

defender can change the platform in host 1 to be VM3, for which the attack

action will not be eﬀective. The interaction is then captured in the game on

graph, shown in Fig. 6.2. The action of the defender can also be stopping or

running a service at the managed endpoints, which are omitted. We distinguish

the set of states into square states at which the defender makes a move and

circle states at which the attacker makes a move.

6.2.2. Specifying the security properties in LTL

We consider qualitative formal speciﬁcations for defender and attacker objec-

tives. Diﬀerent from quantitative utility functions in terms of costs, qualitative

logic formulas capture hard security constraints that the network defense sys-

tem must satisfy.

The defender has two types of goals, namely (i) operational objectives;

such as the services should eventually be available to the legitimate users, and

(ii) defense objectives; such as the attacker should never be able to compro-

mise servers with sensitive information. However, the intention of attacker is

often unknown. Thus, we consider the worst-case scenario where the attacker’s

objective is to violate the security goal of the defender.

We choose to express the security goal of the defender using LTL [Manna

and Pnueli, 1992]. LTL allows us to express the security properties of system

with respect to time. We shall now present the formal syntax and semantics

of LTL and then discuss several examples.

Let AP be a set of atomic propositions. Linear Temporal Logic (LTL) has

9

the following syntax,

ϕ := (cid:62) | ⊥ | p | ϕ | ¬ϕ | ϕ1 ∧ ϕ2 | (cid:13)ϕ | ϕ1 U ϕ2,

where

• (cid:62), ⊥ represent universally true and false, respectively.

• p ∈ AP is an atomic proposition.

• (cid:13) is a temporal operator called the “next” operator (see semantics below).

• U is a temporal operator called the “until” operator (see semantics below).

Let Σ := 2AP be the ﬁnite alphabet. Given a word w ∈ Σω, let w[i] be the

i-th element in the word and w[i . . .] be the subsequence of w starting from

the i-th element. For example, w = abc, w[0] = a and w[1 . . .] = bc. Formally,

we have the following deﬁnition of the semantics:

• w |= p if p ∈ w[0];

• w |= ¬p if p /∈ w[0];

• w |= ϕ1 ∧ ϕ2 if w |= ϕ1 and w |= ϕ2.

• w |= (cid:13)ϕ if w[1 . . .] |= ϕ.

• w |= ϕ U ψ if ∃i ≥ 0, w[i . . .] |= ψ and ∀0 ≤ j < i, w[j . . .] |= ϕ.

From these two temporal operators ((cid:13), U ), we deﬁne two additional temporal

operators: (cid:51) “eventually” and (cid:50) “always”. Formally, (cid:51) ϕ = (cid:62) U ϕ and (cid:50) ϕ =

¬(cid:51) ¬ϕ. For details about the syntax and semantics of LTL, the readers are

referred to [Manna and Pnueli, 1992].

Here we present some examples. Suppose root(2) is an atomic proposition

that the attacker has root privilege on host 2, then a safety property that

attacker never has root privilege on host 2 can be written in LTL as a for-

mula ϕ1 = (cid:50) ¬root(2), which is read as “proposition root(2) is always false.”

10

Similarly, a property that the attacker ﬁrst gains a user privilege on host 1

and then a root privilege on host 2 can be expressed using an LTL formula

ϕ2 = (cid:51) (user(1) ∧ (cid:51) root(2)), which is read as “eventually proposition user(1)

becomes true and then the proposition root(2) becomes true.” In general, it

is also possible to express properties such as recurrence (some event occurs

inﬁnitely often) or persistence (some property eventually becomes true, and

remains true thereafter) using LTL. However, in this chapter, we restrict our-

selves to a sub-class of LTL called syntactically co-safe LTL (scLTL) [Kupfer-

man and Vardi, 2001]. Using scLTL we can reason about the reachability and

safety1 properties.

This concludes a brief introduction to the concept of AD games on graphs;

which do not model the asymmetric incomplete information available with

the players. In the next section, we extend the notion of hypergames that

incorporate the diﬀerent perceptions that players may have due to incomplete

information available to them.

6.3. Hypergames on Graphs

A hypergame models the situation where diﬀerent players perceive their inter-

action with other players diﬀerently, and consequently play diﬀerent games in

their own minds depending on their perception. We consider the case where

the diﬀerence in perception arises because of incomplete and potentially in-

correct information. For instance, suppose a subset of nodes in the network

are honeypots, the attacker may mistake these to be true hosts. We formulate

1Safety and reachability are dual problems. Hence, reasoning about the safety objectives

can be done by reasoning about the dual reachability problem.

11

a hypergame to model the interaction between the defender and the attacker

given asymmetric information.

First, let’s review the deﬁnition of hypergames.

Deﬁnition 6.2(Hypergame [Bennett, 1980, Vane, 2000]): Given two

players, a game perceived by player 1 is denoted by G1, and a game perceived

by player 2 is denoted by G2. A level-1 hypergame is deﬁned as a tuple

HG1 = (cid:104)G1, G2(cid:105),

In a level-1 hypergame, none of the player’s is aware of other player’s percep-

tion.

When one player becomes aware of the other player’s (mis)perception, the

interaction is captured by a level-2 two-player hypergame, deﬁned as a tuple,

HG2 = (cid:104)HG1, G2(cid:105).

where P1 perceives the interaction as a level-1 hypergame (as P1 is aware of

P2’s game G2 in addition to his own) and P2 perceives the interaction as the

game G2.

We refer to the games G1 (resp., G2) as P1’s (resp., P2’s) perceptual game

in level-1 hypergame, and HG1 as P1’s perceptual game in level-2 hypergame.

As P2 is not aware that she might be misperceiving the game, her perceptual

game in level-2 hypergame is still G2.

In general, if P1 computes his strategy by solving an (m − 1)-th level

hypergame and P2 computes her strategy using an n-th level hypergame with

n < m, then the resulting hypergame is said to be a level-m hypergame given

12

as

HGm = (cid:104)HGm−1

1

, HGn

2 (cid:105).

Next, we show that by introducing honeypots, the attacker’s perceptual

game deviates from the actual game. This mismatch occurs in the labeling

function. Recall that a labeling function L assigns every state in the game

arena with a subset of atomic propositions that are true at that state. Let us

consider a network with decoys where attacker is not aware of which hosts are

decoys. Suppose p is a proposition that a host h is a decoy. Then, defender’s

labeling function, say L1, labels h correctly as a decoy. However, the attacker’s

labeling function, say L2, will incorrectly label h as a regular host. Given a

path ρ ∈ S∗ in the game arena, this path may satisfy the security speciﬁcation

as L1(ρ) |= ϕ, in which case the defender obtains payoﬀ 1 and the attacker

obtains payoﬀ 0. However, due to misperception in the labeling, the attacker

may have L2(ρ) |= ¬ϕ and thus have a misperception of the payoﬀ of the path.

We capture this misperception and asymmetric information using the new class

of hypergames, deﬁned as follows:

Deﬁnition 6.3(A Hypergame on a Graph with One-sided Mispercep-

tion of Labeling Function): Let G1 = (cid:104)S, A, T, AP, L1(cid:105) be the game arena

as constructed by P1. Similarly, let G2 = (cid:104)S, A, T, AP, L2(cid:105) be a game arena as

constructed by P2 based on her perception. Let ϕ be the defense objective of

P1. Then, we construct two games G1 = (cid:104)G1, ϕ(cid:105) and G2 = (cid:104)G2, ϕ(cid:105). When P1

is aware of P2’s misperception, i.e. P1 knows L2 and, therefore, G2, we have

the model of their interaction as a hypergame of level-2,

HG2 = (cid:104)HG1, G2(cid:105),

13

where HG1 = (cid:104)G1, G2(cid:105) is a hypergame of level-1 and is P1’s perceptual game.

P2’s perceptual game is G2. We say HG2 to be a hypergame on a graph with

one-sided misperception when the labeling function of P1 coincides with the

ground-truth labeling function, i.e. L1 = L.

6.4. Synthesis of Provably-Secure De-

fense Strategies using Hypergames

on Graphs

Given the hypergame model, we present a solution approach to automatically

synthesize a strategy for defender such that, for every possible action of at-

tacker, the strategy ensures that the security goals (i.e., ϕ) of defender are

satisﬁed. In order to understand the synthesis approach for hypergame, we

ﬁrst look at the conventional solution approach used for a game on graph

[McNaughton, 1993, Zielonka, 1998].

6.4.1. Synthesis of Reactive Defense Strategies

Recall that in an AD game on graph model; G = (cid:104)G, ϕ(cid:105), we assume that the

information available to both players is complete and symmetric. Under this

assumption, the solution for game on graph can be computed by constructing

a game transition system and then using an algorithm to identify the winning

regions for the attacker and the defender.

Before we introduce the game transition system, let us visit the equivalence

14

of an scLTL speciﬁcation with a Deterministic Finite-State Automaton (DFA).

Deﬁnition 6.4(Speciﬁcation DFA): A DFA is a tuple,

A = (cid:104)Q, Σ, δ, I, F (cid:105),

where

• Q is a ﬁnite set of DFA states.

• Σ = 2AP is an alphabet.

• δ : Q×Σ → Q is a deterministic transition function. The transition function

can be extended recursively as: δ(q, uv) = δ(δ(q, u), v) for some u, v ∈ Σ∗.

• I ∈ Q is a unique initial state.

• F ⊆ Q is a set of ﬁnal states.

A word w = σ0σ1 . . . σn is accepted by the DFA if and only if δ(q0, w) ∈ F .

Given an scLTL speciﬁcation ϕ, a DFA A is called a speciﬁcation DFA when

every word w deﬁned over the alphabet Σ that satisﬁes w |= ϕ is accepted by

DFA A.

Using this notion of equivalence between a scLTL formula and DFA, we

deﬁne the game transition system as follows:

Deﬁnition 6.5(Game Transition System): Let A = (cid:104)Q, Σ, δ, I, F (cid:105) be

a DFA equivalent to the speciﬁcation ϕ. Then, given G = (cid:104)G, ϕ(cid:105), the game

transition system, represented as G ⊗ A, is the following tuple:

G ⊗ A = (cid:104)S × Q, A, ∆, (s0, q0), S × F (cid:105),

where

15

• S × Q is a set of states partitioned into P1’s states S1 × Q and P2’s states

S2 × Q;

• A = A1 ∪ A2 is the same set of actions as labeled transition system G;

• ∆ : (S1 × Q × A1) ∪ (S2 × Q × A2) → S × Q is a deterministic transition

function that maps a game state (s, q) and an action a to a next state (s(cid:48), q(cid:48))

where s(cid:48) = T (s, a, s(cid:48)) and q(cid:48) = δ(q, L(s(cid:48))).

• (s0, q0) ∈ S × Q where q0 = δ(I, L(s0)) is an initial state of the game

transition system; and

• S × F ⊆ S × Q is a set of ﬁnal states.

The following theorem is a well-known result in game theory [McNaughton,

1993, Zielonka, 1998].

Theorem 6.1(Determinacy of Game on Graph): All two-player zero-

sum deterministic turn-based games on graph are determined.

Thm. 6.1 is a very important result because it provides us with a char-

acterization of the game state-space. It states that, at any state in the game

transition system, either the defender or the attacker has a winning strategy.

In other words, the state space of game transition system is divided into two

sets, one consisting of states from which the defender is guaranteed to satisfy

his security objectives, and the second consisting of states from which attacker

has a strategy to violate the defender’s objectives.

Example 6.2: We illustrate the game on graph using a toy example. Con-

sider a network system where the defender can switch between two network

topologies, giving rise to two attack graph under two network topologies (Shown

in Fig. 6.3). For simplicity, as the graph is deterministic, we omit the attack

16

action labels on the graph. Incorporating defender’s actions into the attack

0

1

2

3

0

3

1

2

(a) Attack graph under topology A.

(b) Attack graph under topology B.

Figure 6.3: The attack graphs under diﬀerent network topologies.

graph, we obtain the arena of the game, shown in Fig. 6.4. A circle state (0, A)

can be understood as the attacker is at node 0, the network conﬁguration is

A, and it is attacker’s turn to make a transition. A square state (1, A) can be

understood as the attacker is at node 1, the network conﬁguration is A, and it

is defender’s turn to make a switch. A transition from circle (0, A) to square

(1, A) means that the attacker exploits a vulnerability on node 1 and reach node

1. The goal of the attacker is node 3. That is, if the attacker can reach any

of the square states (3, A) or (3, B), then she wins the game. The goal of the

defender is to prevent the attacker from reaching the goal. In this game, we

can compute the attacker’s strategy shown in Fig. 6.4 where red, dashed edges

indicate the choice of attacker. For example, if the attacker is at host 1 given

topology B, she reaches host 2. If the defender switches to A, then she will take

action to reach square (3, A). If the defender switches to B, then she will take

action to reach square (3, B). In this game, there is no winning strategy for the

defender given the initial state of the game. In fact, the winning region of the

defender is empty.

Now, let’s consider a diﬀerent speciﬁcation of the attacker: (cid:51) (2 ∧ (cid:51) 3).

That is, the attacker must reach node 2 ﬁrst and then node 3. The LTL formula

translates to DFA in Fig. 6.5. Given the new speciﬁcation, we construct the

game transition system in Fig. 6.6. An example of transition (1, A, q0, circle) →

17

1, A

3, A

start

0, A

1, A

2, A

2, A

1, B

2, B

2, B

3, B

Figure 6.4: The game transition system given topology switching with simple
attacker’s reachability objective.

0, 1, 3

2, 0, 1

start

q0

2

q1

3

q2

Figure 6.5: The automaton representing the attacker’s objective

(2, A, q1, square), where circle, square indicate the shapes of the nodes, is deﬁned
2−→ q1. Given this LTL task, the

jointly by (1, A, circle) → (2, A, square), and q0

winning strategy of the attacker is indicated with red and dashed edges. It is

noted that when the attacker is at node 1, she will not choose to reach 3 but to

reach 2, required by the new speciﬁcation.

1, A, q0

3, A, q0

start

0, A, q0

1, A, q0

2, A, q1

2, A, q1

3, A, q2

1, B, q0

2, B, q1

2, B, q1

3, B, q2

Figure 6.6: The game transition system for LTL co-safe formula (cid:51) (2 ∧ (cid:51) 3).
The red edges are the attacker’s strategy.

18

6.4.2. Synthesis of Reactive Defense Strategies

with Cyber Deception

When the attacker has a one-sided misperception of labeling function, as de-

ﬁned in Def. 6.3, the defender might strategically utilize this misperception to

deceive the attacker into choosing a strategy that is advantageous to the de-

fender. To understand when the defender might have such a deceptive strategy

and how to compute it, we study the solution concept of hypergame.

Solution Approach A hypergame HG2 = (cid:104)HG1, G2(cid:105) is deﬁned using two

games, namely G1 and G2. Under one-sided misperception of labeling function,

defender is aware of both games. Therefore, to synthesize a deceptive strategy,

the defender must take into account the strategy that the attacker will use,

based on her misperception. That is, the defender must solve two games: Game

G2 to identify the set of states in the game transition system G2 ⊗ A that the

attacker perceives as winning for her under labeling function L2, and game G1

to identify the set of states in the game transition system G1 ⊗ A that are

winning for the defender under (ground-truth) labeling function L = L1. After

solving the two games, the defender can integrate the solutions to obtain a set

of states, at which the attacker makes mistakes due to the diﬀerence between

L2 and L. Let us introduce a notation to denote these sets of winning states.

• G1: P1’s winning region is Win1 ⊆ S × Q and P2’s winning region is Win2 ⊆

S × Q.

• G2: P1’s winning region is WinP 2

1 ⊆ S×Q and P2’s winning region is WinP 2

2 ⊆

S × Q.

Figure 6.7 provides a conceptual representation partitions of the state-

19

Figure 6.7: Illustration of the partition given by diﬀerent perceptual game.

space of a game transition system. Due to misperception, the set of states are

partitioned into the following regions,

• Win1: is a set of states from which P1 can ensure satisfaction of security

objectives, even if P2 has complete and correct information. Thus, P1 can

take the winning strategy π1.

• WinP 2

1 ∩ Win2: is a set of states where P2 is truly winning, but perceives the

states to be losing for her; due to misperception. Thus, P2 may either give

up the attack mission or play randomly.

• WinP 2

2 ∩ Win2: is a set of states in which P2 is truly winning and perceives

those states to be winning. In this scenario, she will carry out the winning

strategy πP 2

2 . However, this strategy can be diﬀerent from the true winning

strategy π2 that P2 should have played if she had complete and correct

information. This diﬀerence creates unique opportunities for P1 to enforce

security of the system.

To compute deceptive strategy, the defender must reason about (a) how

the attacker responds given her perception? and (b) how does the attacker

expect the defender to respond, given her perception? It is noted that given the

winning regions WinP 2

1

(resp. WinP 2

2 ), there exists more than one strategies that

P2 perceives to be winning for P2 (resp. for P1). The problem is to compute

20

a strategy for the defender π∗

1, if exists, such that no matter which perceptual

winning strategy that P2 selects, P1 can ensure the security speciﬁcation is

satisﬁed surely, without contradicting the perception of P2.

Given P2’s perceptual game G2 = (S × Q, A, ∆, (s0, q0,2), S × F ), there

can be inﬁnitely many such almost-sure winning mixed strategies for P2 [Ber-

net et al., 2002], we take an approximation of the set of almost-sure winning

strategy as a memoryless set-based strategy as follows.

2 (s, q) = {a | ∆2 ((s, q), a) ∈ WinP 2
πP 2

2 }.

(6.1)

In other words, P2 can select any action as long as she can stay within her

perceived winning region WinP 2

2 . Given the rational player 2, for a given state

(s, q), an action a that is not in πP 2

2 (s, q) is irrational as it drives P2 from the

perceived sure winning region to the perceived losing region.

For a state (s, q) ∈ WinP 2

1 ∩ Win2, P2 perceives P1 to be winning under

labeling function L2, when she is truly winning under ground-truth labeling

function L. In this case, P1’s deceptive strategy should conform to P2’s per-

ceptual winning strategy for P1. Otherwise, P2 would know that she is misper-

ceiving the game when she observes P1 deviating from his rational behavior

in the perceptual game of P2. When P1’s action is inconsistent from what P2

perceives P1 should do, then P2 knows that she have misperception about the

game.

Again, we take an approximation of the set of P1’s almost-sure winning

strategy perceived by P2 as a memoryless set-based strategy as follows.

1 ((s, q)) = {a | ∆2((s, q), a) ∈ WinP 2
πP 2

1 }.

(6.2)

21

Next, by removing P2’s actions from G1 that P2 perceives to be irrational

as well as P1’s actions that contradicts P2’s perception, we obtain a diﬀer-

ent game. Now, we incorporate the knowledge of P1’s actions that P2 would

perceive to be irrational into the hypergame model. This results in a modiﬁed

hypergame model as deﬁned below.

Deﬁnition 6.6: Given the games G1 = G1 ⊗ A constructed using the true

labeling function L and G2 = G2 ⊗ A with P2’s misperceived labeling function

L2, the deceptive sure-winning strategy of P1 is the sure-winning strategy of

the following game:

HG = (S × Q × Q, A, ¯∆, (s0, q0, p0), Win1 × Q),

where the transition function ¯∆ is deﬁned such that

• For (s, q, p) ∈ S1 × Q × Q \ (Win1 × Q), if (s, p) ∈ WinP 2

1 , then actions

in πP 2

1 (s, p) are enabled. Otherwise, all actions a ∈ A1 are enabled. For
each enabled action a, let ¯∆((s, q, p), a) = (s(cid:48), q(cid:48), p(cid:48)) where s(cid:48) = T (s, a),

q(cid:48) = δ(q, L(s(cid:48))) and p(cid:48) = δ(p, L2(s(cid:48))).

• For (s, q, p) ∈ S2 × Q × Q \ (Win1 × Q), if (s, p) ∈ WinP 2

2 , then actions

in πP 2

2 (s, p) are enabled. Otherwise, all actions a ∈ A2 are enabled. For
each enabled action a, let ¯∆((s, q, p), a) = (s(cid:48), q(cid:48), p(cid:48)) where s(cid:48) = T (s, a),

q(cid:48) = δ(q, L(s(cid:48))) and p(cid:48) = δ(p, L2(s(cid:48))).

• The initial state is (s0, q0, p0) where (s, q0) is the initial state in G1 and

(s0, p0) is the initial state in G2.

The transition function can be understood as follows. At a P1 state, when

P2 perceives a state (s, q, p) to be winning for P1, the permissive actions in

22

πP 2
1 of P1 are enabled at that state. Otherwise, P2 would assume that P1 may

choose any action from A1. Similarly, at a P2 state, when P2 perceives a state

to be winning for herself, she might choose any action from her permissive

action set πP 2

2 . Whereas, if P2 perceives the current state (s, q, p) to be losing

for her, given her perception, she would choose any action from A2.

Lemma 6.1: The sure-winning strategy π∗

1 of the game HG in Def. 6.6 is

stealthy for any state (s, q, p) where (s, q) ∈ Win2 as it does not reveal any

information with which P2 can deduce that some misperception exists.

Proof.

In P2’s perceived winning region WinP 2

2

for herself, any strategy

of P1 is losing. Thus π∗

1 will not contradict P2’s perception. In P2’s perceived

winning region WinP 2

1

for P1, an action, which is selected by π∗

1 (if deﬁned for

that state), ensures that P1 to stay within WinP 2

1 and thus will not contradict

the perception of P2. In both cases, P2 will not deduce the fact that there is

a misperception.

Example 6.3(Continued): Let us continue with the toy example and the

simple reachability objective (cid:51) 3 (eventually reach node 3). Suppose the node 2

is a decoy. Then the attacker’s labeling function L2 diﬀers from the defender’s

labeling function L: L2((2, X)) = ∅ and L((2, X)) = decoy where X ∈ {A, B}.

As the attacker is to avoid reaching decoys, if she knows the true labeling L,

then at the attacker’s circle state (1, A) in Fig. 6.8, she will not choose to reach

node 2 (and thus the square state (2, A)). The attacker’s strategy given the true

labeling function is given in Fig. 6.8. In this game, the attacker has no winning

strategy to reach 3 from 0 in the true game as the defender can choose to switch

to topology B ((1, A, square) → (1, B, circle)). However, with the misperception

on the labeling function, the attacker believes that she has a winning strategy

23

from node 0 (see the attacker’s strategy in Fig. 6.4).

1, A

3, A

start

0, A

1, A

2, A

2, A

1, B

2, B

2, B

3, B

Figure 6.8: The game transition system and the attacker’s winning strategy
given that she is to reach node 3 and knows that node 2 is decoy.

Consider now the initial state is (1, A, circle), that is, the attacker has com-

promised node 1 and the current network topology is A. If the attacker knows

2 is a decoy, then she will choose to reach 3 deterministically. If the attacker

does not know 2 is a decoy, then she is indiﬀerent to reaching node 2 or node

3, because in her perception, these two actions ensures that with probability

one, she can reach node 3 in ﬁnitely many steps. Given this analysis, it is not

diﬃcult to see that (1, A, circle) is sure-winning for the attacker given the true

game, but positive winning for the defender given the perceptual game of the

attacker, with the incorrect labeling. Here, positive winning means that the de-

fender wins with a positive probability–that is, the probability when the attacker

makes mistakes (visiting decoy node 2) due to her misperception.

Finally, we construct the hypergame HG in Fig. 6.9 where the defender’s

objective is ¬p U decoy where p is an atomic proposition that evaluates true

when node 3 is compromised. The labeling functions are: For X ∈ {A, B},

L(1, X) = L2(1, X) = ∅, L(3, X) = L2(3, X) = {p} and L(2, X) = decoy

but L2(2, X) = ∅. The states (shaded, red) are when the attacker’s perceived

24

(1, A), q0, q0

(3, A), q1, q1

start

∅

q0

(cid:62)

q1

p

decoy

q2

(cid:62)

start

(0, A), q0, q0

(1, A), q0, q0

(2, A), q2, q0

(2, A), q2, q0

(3, A), q2, q1

(1, B), q0, q0

(2, B), q2, q0

(2, B), q2, q0

(3, B), q2, q0

(a)

(b)

Figure 6.9: (a)The DFA for the defender’s objective ¬p U decoy. (b) The
game that P1 uses to compute deceptive sure-winning strategy. The red and
dashed edges are perceived winning actions of the attacker.

automaton state diﬀers from the defender’s automaton state. For example,

((2, A), q2, q0, circle) means that the defender knows that the attacker reached

a decoy but the attacker is unaware of this fact.

6.5. Case Study

We consider a simple network system illustrated in Fig. 6.10.

Figure 6.10: A example of network system.

In this network, each host runs a subset Servs = {0, 1, 2} of services. A

user in the network can have one of the three login credentials credentials =

25

Table 6.1: The pre- and post-conditions of vulnerabilities.

Vulnerability ID Pre- and Post- Conditions

0

1

2

Pre : c ≥ 1, service 0 running on target host,
Post : c = 2, stop service 0 on the target, reach target host.

Pre: c ≥ 1, service 1 running on the target host,
Post : reach target host.

Pre: c ≥ 1, service 2 running on the target host
Post : c = 2, reach target host.

Table 6.2: The defender’s options.

Host ID Services Non-critical Services

0
1
2
3
4
5

{0, 1, 2}
{1, 0}
{1, 2}
{0, 1, 2}
{0, 1}
{0, 1, 2}

∅
∅
∅
{0, 1}
∅
∅

{0, 1, 2} standing for “no access” (0), “user” (1), and “root” (2). There are

a set of vulnerabilities in the network, each of which is deﬁned by a pre-

condition and a post-condition. The pre-condition is a Boolean formula that

speciﬁes the set of logical properties to be satisﬁed for an attacker to exploit the

vulnerability instance. The post-condition is a Boolean formula that speciﬁes

the logical properties that can be achieved after the attacker has exploited that

vulnerability. The set of vulnerabilities are given in Table 6.1 and are generated

based on the vulnerabilities described in [Jha et al., 2002].

The defender can temporally suspend noncritical services from servers. To

incorporate this defense mechanism, we assign each host a set of noncritical

services that can be suspended from the host. In Table 6.2, we list the set

26

of services running on each host, and a set of noncritical services that can be

suspended by the defender. Other defenses can also be considered. For example,

if the network topology can be reconﬁgured online, then the state in the game

arena should keep track of the current topology conﬁguration of the network.

In our experiment, we consider simple defense actions. However, our method

extends to more complex defense mechanisms.

The attacker, at a given attacker’s state, can exploit any existing vulner-

ability on the current host. The defender, at a defender’s state, can choose

to suspend a noncritical service on any host in the network. The attacker’s

objective is expressed using scLTL formula

ϕ2 = ¬decoy U p2 ∧ ¬decoy U p5,

where pi means that the attacker has compromised host i and gained user or

root access on that machine. However, the attacker does not know the location

of the decoys. In this system, decoy is host 4.

The following result is obtained from hypergame analysis: at the initial

state, the attacker is at host 0 with user access.

• The initial state is perceived to be winning by the attacker.

• Assuming complete, symmetric information, the size of winning region for

the defender is 131.

• With asymmetric information, when the attacker plays a perceived winning

strategy, the size of winning region for the defender is 193–which is greater

than that with symmetric information.

• The defender has a winning strategy to prevent the attacker from achieving

her objective in the network, using the solution of the hypergame.

27

It is noted that, using the winning region of the defender, we know for a

given initial state, whether the security speciﬁcation is satisﬁed. For a diﬀerent

initial state, for example, the attacker visited host 2 with user privileges, we can

directly examine whether the security is ensured by checking if the new initial

state is in the winning region for the defender. This set provide us important

insight to understand the weak points in the network, and can be used for

guide the decoy allocation.

6.6. Conclusion

The goal of formal synthesis is to design dynamic defense with guarantees on

critical security speciﬁcations in a cyber network. In this chapter, we intro-

duced a game on graph model for capturing the attack-defend interactions

in a cyber network for reactive defense, subject to security speciﬁcations in

temporal logic formulas. In reactive defense, the defender can take actions in

response to the exploit actions of the attacker. We introduced a hypergame for

games on graphs to capture payoﬀ misperception for the attacker caused by

the decoy systems. The solution concept of hypergames enables us to synthe-

size eﬀective defense strategy given the attacker’s misperception of the game,

without contradicting the belief of the attacker. There are multiple extensions

from this study: The framework assumes asymmetric information but complete

observations for both defender and attacker. It is possible to extend for defense

design with a partially observable defender/attacker. By examining the win-

ning region, it provides important insights for the resource allocation of decoy

systems.

28

Bibliography

Ehab Al-Shaer, Jinpeng Wei, Kevin W Hamlen, and Cliﬀ Wang. Dynamic

bayesian games for adversarial and defensive cyber deception.

In Au-

tonomous Cyber Deception, pages 75–97. Springer, 2019.

Zaruhi Aslanyan, Flemming Nielson, and David Parker. Quantitative Veriﬁca-

tion and Synthesis of Attack-Defence Scenarios. In 2016 IEEE 29th Com-

puter Security Foundations Symposium (CSF), pages 105–119, June 2016.

doi: 10.1109/CSF.2016.15.

Christel Baier and Joost-Pieter Katoen. Principles of Model Checking (Repre-

sentation and Mind Series). The MIT Press, 2008. ISBN 978-0-262-02649-9.

Peter G Bennett. Hypergames: developing a model of conﬂict. Futures, 12(6):

489–507, 1980.

Julien Bernet, David Janin, and Igor Walukiewicz. Permissive strategies:

From parity games to safety games. RAIRO - Theoretical Informatics and

Applications, 36(3):261–275, July 2002. ISSN 0988-3754, 1290-385X. doi:

10.1051/ita:2002013.

Krishnendu Chatterjee and Thomas A Henzinger. A survey of stochastic

omega-regular games. Journal of Computer and System Sciences, 78(2):

394–413, 2012. ISSN 0022-0000. doi: http://dx.doi.org/10.1016/j.jcss.2011.

05.002. Games in Veriﬁcation.

Fred Cohen. The use of deception techniques: Honeypots and decoys.

In

Handbook of Information Security 3.1. 2006.

Ren´e Rydhof Hansen, Peter Gjøl Jensen, Kim Guldstrand Larsen, Axel Legay,

and Danny Bøgsted Poulsen. Quantitative evaluation of attack defense trees

29

using stochastic timed automata. In International Workshop on Graphical

Models for Security, pages 75–90. Springer, 2017.

Karel Hor. Manipulating adversary’s belief: A dynamic game approach to

deception by design for proactive network security. 7638:273–294, 2012.

ISSN 03029743. doi: 10.1007/978-3-642-34266-0.

Karel Horak, Branislav Boˇsansk´y, Christopher Kiekintveld, and Charles

Kamhoua. Compact representation of value function in partially observ-

able stochastic games. pages 350–356. International Joint Conferences on

Artiﬁcial Intelligence Organization, 2019. doi: 10.24963/ijcai.2019/50.

Sushil Jajodia, V. S. Subrahmanian, Vipin Swarup, and Cliﬀ Wang. Cyber

deception: Building the scientiﬁc foundation. 2016. ISBN 9783319326993.

doi: 10.1007/978-3-319-32699-3.

S. Jha, O. Sheyner, and J. Wing. Two formal analyses of attack graphs.

Proceedings of the Computer Security Foundations Workshop, 2002-Janua:

49–63, 2002. ISSN 10636900. doi: 10.1109/CSFW.2002.1021806.

Barbara Kordy and Wojciech Wide(cid:32)l. On Quantitative Analysis of At-

tack–Defense Trees with Repeated Labels. In Lujo Bauer and Ralf K¨usters,

editors, Principles of Security and Trust, Lecture Notes in Computer Sci-

ence, pages 325–346, Cham, 2018. Springer International Publishing. ISBN

978-3-319-89722-6. doi: 10.1007/978-3-319-89722-6 14.

Barbara Kordy, Sjouke Mauw, Saˇsa Radomirovi´c, and Patrick Schweitzer.

Foundations of attack–defense trees. In International Workshop on Formal

Aspects in Security and Trust, pages 80–95. Springer, 2010.

30

Barbara Kordy, Ludovic Pi`etre-Cambac´ed`es, and Patrick Schweitzer. DAG-

based attack and defense modeling: Don’t miss the forest for the attack

trees. Computer Science Review, 13-14:1–38, November 2014. ISSN 1574-

0137. doi: 10.1016/j.cosrev.2014.07.001.

Nicholas S. Kovach, Alan S. Gibson, and Gary B. Lamont. Hypergame theory:

A model for conﬂict, misperception, and deception. Game Theory, 2015:

1–20, 2015. ISSN 2356-6930. doi: 10.1155/2015/570639.

Orna Kupferman and Moshe Y Vardi. Model checking of safety properties.

Formal Methods in System Design, 19(3):291–314, 2001.

Zohar Manna and Amir Pnueli. The Temporal Logic of Reactive and Concur-

rent Systems: Speciﬁcation. Manna,Z.;Pnueli,A.:Temporal Logic of Reactive

Systems. Springer-Verlag, New York, 1992. ISBN 978-0-387-97664-8. doi:

10.1007/978-1-4612-0931-7.

Robert McNaughton. Inﬁnite games played on ﬁnite graphs. Annals of Pure

and Applied Logic, 65(2):149–184, 1993.

Amir Pnueli and Roni Rosner. On the synthesis of an asynchronous reactive

module. pages 652–671, 1989.

Bruce Schneier. Attack Trees. http://www.schneier.com/paper-attacktrees-

ddj-ft.html, August 2007.

Russell Richardson III Vane. Using Hypergames to Select Plans in Competitive

Environments. PhD thesis, 2000.

Quanyan Zhu and Stefan Rass. On multi-phase and multi-stage game-theoretic

modeling of advanced persistent threats. IEEE Access, 6:13958–13971, 2018.

ISSN 21693536. doi: 10.1109/ACCESS.2018.2814481.

31

Wies(cid:32)law Zielonka. Inﬁnite games on ﬁnitely coloured graphs with applications

to automata on inﬁnite trees. Theoretical Computer Science, 200(1-2):135–

183, 1998. ISSN 03043975. doi: 10.1016/S0304-3975(98)00009-7.

32

