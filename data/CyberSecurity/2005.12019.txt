Malware Detection at the Microarchitecture Level using Machine Learning

Techniques

A THESIS

Presented to the Department of Computer Engineering and Computer Science

California State University, Long Beach

In Partial Fulﬁllment

of the Requirements for

the University Honors Program Certiﬁcate

By Abigail Kwan

Spring 2020

0
2
0
2

y
a
M
5
2

]

R
C
.
s
c
[

1
v
9
1
0
2
1
.
5
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
Malware Detection at the Microarchitecture Level using Machine Learning

ABSTRACT

Techniques

By

Abigail Kwan

Spring 2020

Detection of malware cyber-attacks at the processor microarchitecture level has re-

cently emerged as a promising solution to enhance the security of computer systems. Se-

curity mechanisms, such as hardware-based malware detection, use machine learning al-

gorithms to classify and detect malware with the aid of Hardware Performance Counters

(HPCs) information. The ML classiﬁers are fed microarchitectural data extracted from

Hardware Performance Counters (HPCs), which contain behavioral data about a soft-

ware program. These HPCs are captured at run-time to model the program’s behavior.

Since the amount of HPCs are limited per processor, many techniques employ feature re-

duction to reduce the amount of HPCs down to the most essential attributes. Previous

studies have already used binary classiﬁcation to implement their malware detection after

doing extensive feature reduction. This results in a simple identiﬁcation of software be-

ing either malware or benign. This research comprehensively analyzes diﬀerent hardware-

based malware detectors by comparing diﬀerent machine learning algorithms’ accuracy

with binary and multi-class classiﬁcation models. Our experimental results indicate that

when compared to complex machine learning models (e. g. Neural Network and Logistic),

light-weight J48 and JRip algorithms perform better in detecting the malicious patterns

even with the introduction of multiple types of malware. Although their detection accu-

racy slightly lowers, their robustness (Area Under the Curve) is still high enough that they

deliver a reasonable false positive rate.

ii

ACKNOWLEDGEMENTS

I would like to thank my mentor, Dr. Hossein Sayadi, for introducing me to this topic

and guiding me throughout this process. I would also like to thank Mr. Kevin and Kent

Peterson for the P2S Engineering Honors Scholarship. Lastly, I would like to thank my

family and friends for supporting me through my college journey.

iii

TABLE OF CONTENTS

ABSTRACT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

ii

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

iii

LIST OF TABLES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

vi

LIST OF FIGURES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

vii

1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 Motivation for Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1 Types of Malware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Hardware Performance Counters . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Machine Learning Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

2

4

6

6

7

7

2.4 Binary vs. Multiclass Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . .

10

3 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1 Dataset

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 WEKA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3 Feature Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4 Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Results and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.1 Accuracy of Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.2 Comparison of Receiver Operating Characteristics . . . . . . . . . . . . . . .

11

11

11

13

15

16

16

16

iv

5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.1 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

22

REFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

v

LIST OF TABLES

1

Binary Classiﬁcation Order of Attributes . . . . . . . . . . . . . . . . . . . .

2 Multiclass Classiﬁcation Order of Attributes . . . . . . . . . . . . . . . . . .

14

15

3

4

Binary Classiﬁcation ROC Area for Diﬀerent Number of HPCs . . . . . . . .

16

J48 and Logistic Multiclass Classiﬁcation Models’ ROC Area for Varying

Numbers of HPCs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

18

vi

LIST OF FIGURES

1

The Multilayer Perceptron neural network has hidden layers that are not di-

rectly exposed to the input. This particular ﬁgure is a feed-forward network.

Its purpose is to approximate some function f(x).

. . . . . . . . . . . . . . .

8

2 WEKA’s Explorer application contains several tools that can be used for

applying machine learning. The Classify tab is where the algorithms are

applied to create a model that can be used for test sets. . . . . . . . . . . . .

12

3

The feature selection process selects relevant predictors from a training set

in order to increase the accuracy of the prediction variable. In this model,

orange represents redundant or impure classiﬁers while green represents the

desired classiﬁer.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Overview of run-time hardware-based malware detection approach [1]

. . . .

Accuracy of Classiﬁcation Models per Number of HPCs . . . . . . . . . . . .

Binary Classiﬁcation ROC Graphs with Varying Number of HPCs . . . . . .

4

5

6

7 Multiclass Classiﬁcation 4HPCs ROC Graphs

. . . . . . . . . . . . . . . . .

14

15

17

19

21

vii

CHAPTER 1

INTRODUCTION

As the world is increasingly connected through the internet, many devices are prone

to security threats and malware attacks. Malware is a piece of code designed to perform

various malicious activities, such as destroying the data, stealing information, running de-

structive or intrusive programs on devices to perform Denial-of- Service (DoS) attacks,

and gaining root access without the consent of user. These programs are used to compro-

mise user data and cripple networks [2, 3]. The recent proliferation of computing devices

in mobile and Internet-of-Things (IoTs) domains further exacerbates the malware attacks

and calls for eective malware detection techniques [1, 4]. A recent survey showed that the

number of security incidents in 2014 rose to 42.8 million incidents [5]. Despite the number

of antivirus software out on the market, malware still persist due to the broad number of

virus variations that could exist.

Conventional signature-based and semantic-based malware detection methods mostly

impose signiﬁcant computational overhead to the system. Furthermore, they are unable to

detect unknown threats making them unsuitable for devices with limited available comput-

ing and memory resources [6]. The emergence of new malware threats requires patching

or updating the software-based malware detection solutions (such as oﬀ-the-shelf anti-

virus) that need a vast amount of memory and hardware resources, which is not feasible

for emerging computing systems specially in embedded mobile and IoT devices [3]. A typ-

ical antivirus software uses static characteristics of malware to detect if it is a threat [7].

This is extremely exploitable as it requires an up-to-date database in which a new malware

won’t be detected [8]. Since software antivirus programs can be bypassed, researchers have

begun studying hardware-based malware detection in order to increase security for com-

puters and networks.

1

Recent works have demonstrated that malware can be diﬀerentiated from normal ap-

plications by classifying anomalies using Machine Learning (ML) techniques in low-level

microarchitectural feature spaces captured by Hardware Performance Counters (HPCs)

[1, 6, 7]. The HPCs are a set of special purpose registers built into modern microproces-

sors to capture the trace of hardware-related events for a running program [9]. ML-based

malware detectors can be implemented in microprocessor hardware with signiﬁcantly low

overhead as compared to software-based methods, as detection inside the hardware is very

fast within few clock cycles [1, 5].

1.1 Related Work

In this section, we discuss recent related work for hardware-based malware detection.

The work in [10] had a similar approach when it came to data collection and classiﬁca-

tion. They also implemented the machine learning algorithms into software and hardware.

They found that the software implementation had a large overhead, which caused the algo-

rithm to lag behind due to latency. The hardware implementation was done using Vivado

High-Level-Synthesis, which allowed them to collect data on power estimation, data, and

latency. Their results found that more simple classiﬁers such as OneR were more eﬃcient

when implemented on hardware. Even though it was not as accurate, it was faster, took

less power, and the least amount of area.

Besides regular machine learning methods, some other studies have examined hardware-

based detection using ensemble learning. Researchers in [1] used ensemble learning in their

hardware implementation in order to ﬁnd out how it could improve malware detection.

The two techniques they used were boosting and bagging. Boosting is a technique that

weighs each training dataset and adjusts the weights based on the overall accuracy of the

model. Bagging, on the other hand, is another ensemble learning technique that takes a

statistical value from multiple random samples and uses it to train the ML models. They

2

compared the robustness and the accuracy of regular classiﬁers with boosted classiﬁers.

They found that boosting techniques improved the classiﬁcation by as much as 17% with a

much lower amount of HPCs.

In addition, a recent work in [6] proposed a two-stage machine learning-based ap-

proach for run-time malware detection in which the ﬁrst level classiﬁes applications us-

ing a multiclass classiﬁcation technique into either benign or one of the malware classes

(Virus, Rootkit, Backdoor, and Trojan). In the second level, to have a high detection per-

formance, the authors deploy an ML model that works best for each class of malware and

further apply eﬀective ensemble learning to enhance the performance of hardware-based

malware detection. The work in [3] also proposed an eﬀective machine learning-based

hardware-assisted malware detection framework for embedded devices which only utilizes a

limited number (only 4) of low-level features in a microprocessor i.e., HPC events to facili-

tate the run-time malware detection.

Machine learning and ensemble learning are not the only methods to implement hardware-

based malware detection. The researchers at [11] used Akoman, a malware detection tech-

nique that builds behavioral signatures for malware detection. The technique collects sig-

natures and creates an aggregation matrix. After that, it applies fast and exact signatures

to compare the program to likely malware families in order to determine whether or not it

is actually benign. They tested Akoman using 36 benign Linux programs and 13 families

of Linux malware programs. They found that Akoman can achieve an acceptable perfor-

mance based on its metrics for average precision, recall, and F-measure.

Recent research have also dealt with the theoretical side of hardware-based malware

detection. The study in [12] analyzed the security guarantees of hardware-based malware

detection using 4 HPCs. They calculated the probability of malware being detected when

HPCs are monitored at a certain sampling interval. They used control-ﬂow graphs to out-

3

line programs. Their research found that it is diﬃcult for malware to match all possible

HPCs, with the probability of matching four HPC parameters is exp−40. This means that

HPCs are highly secure compared to normal antivirus software since malware would have

a diﬃcult time guessing the right HPCs to pass undetected.

1.2 Motivation for Research

Hardware-based malware detection has become an increasingly important topic in the

ﬁeld of cybersecurity. Implementation at a hardware level reduces the chances of malware

subverting protection mechanisms[1, 7].Previous research has already determined classiﬁ-

cation of several malware types using memory access operations[13]. The researchers were

able to classify various malware types based on the number of memory accesses. There

also exists research that deal with binary classiﬁcation of malware using hardware per-

formance counters (HPC) [1, 10]. According to their ﬁndings, simple decision tree algo-

rithms such as OneR were more eﬀective in hardware implementation compared to higher

accuracy algorithms such as logistics and MultiLayerPerceptron. Simple classiﬁers were

found to be more cost-eﬀective when it came to execution time, accuracy per logic area,

and number of HPC used. This thesis will expand on these ﬁndings to determine if certain

machine learning methods are more eﬃcient with diﬀerentiating between multiple types of

malware. Instead of only binary classiﬁcation, this thesis will explore how certain machine

learning methods perform in detecting ﬁve diﬀerent types of malware: backdoor, rootkit,

trojans, viruses, and worms.

The thesis is organized as follows. Background information concerning malware, HPCs,

and machine learning algorithms are detailed in chapter 2. The experimental setup about

the dataset and approach used is described in chapter 3. In chapter 4, we discuss the re-

sults collected and provide an analysis of the diﬀerences between binary and multiclass

classiﬁcation amongst the various ML classiﬁers. Finally, in chapter 5, the conclusion and

4

future work is presented for this thesis.

5

CHAPTER 2

BACKGROUND

2.1 Types of Malware

Malware can have multiple infection vectors. One way is through exploiting vulner-

able services over a network[14]. Large scale installations of systems with the same vul-

nerability can allow malicious software to infect automatically. Another way malware can

infect systems is through drive-by downloads[14]. The malware downloads automatically

by exploiting a web browser vulnerability. Because of the variety in infection vectors and

methods, malware can be categorized based on the purposes and ways they are able to

reach vulnerable systems[15]. For the purposes of this thesis, this section gives an overview

of the ﬁve diﬀerent malware types that we are testing.

• Backdoor Backdoors are methods used to bypass regular authentication or encryp-

tion in a system. Backdoors are used alongside Trojans so attackers can have access

to a remote computer or network.

• Rootkit Rootkits are designed to be stealthy software that hide certain processes

from a computer system. [14, 15].

• Trojans: Trojans are software programs that perform malicious attacks under the

guise of being a regular program. [15]

• Viruses: Viruses are dependent malware that can attach to other system programs.

When it is executed, the aﬀected area gets infected.[ye] It spreads because it can in-

fect not only local ﬁles, but also ﬁles on shared servers, thereby aﬀecting other com-

puters as well [14].

• Worms: Worms are programs that are able to run independently. They propagate

by copying itself from an infected host into another machine, usually through the

6

operating system. [15]

Because of the numerous types of malware, a diversiﬁed dataset is needed in order to

design an eﬃcient malware detection system. Features must be collected from hardware

performance counters (HPCs) using data mining.

2.2 Hardware Performance Counters

Hardware performance counters are registers built into a microprocessor in order to

store data about hardware events [1, 9, 10]. These registers give information about the

run-time behavior of software programs. The advantage of using HPCs is increased secu-

rity. Unlike antivirus software which characterize malware based on static characteristics,

HPCs characterize based on a program’s actual behavior[16]. HPCs are also not accessi-

ble to malware. The number of HPCs that are available on a processor vary depending on

the model[9]. For example, an Intel Atom processor only has 4 HPCs to monitor micro-

architectural behavior at run-time [10]. Because of this limitation, it’s important to nar-

row down the number of features so that the machine learning model can be implemented

on hardware more eﬃciently to detect the malware.

2.3 Machine Learning Methods

In a supervised learning process, a training set is used to create a classiﬁcation model

[17]. The learning algorithm is given a set of N samples with A attributes such that when

given new data, it accurately predicts the class[18]. The training examples are tuples (x,

y), where x refers to the sample and y refers to the class[18]. In this thesis, the data will

be put through ﬁve machine learning algorithms: Multilayer Perceptron (MLP), OneR,

Logistic, JRip, and J48.

Multilayer Perceptron : The Multilayer Perceptron (MLP) (Figure 1) algorithm is a neu-

ral network that makes predictions based on mapping. It contains multiple layers

with one or more hidden layers [19]. MLP is a type of logistic regression where the

7

FIGURE 1. The Multilayer Perceptron neural network has hidden layers that
are not directly exposed to the input. This particular ﬁgure is a feed-forward
network. Its purpose is to approximate some function f(x).

input is transformed based on weighting values. The weights on each node can be

changed after data is processed and the error is calculated. After the weights are

calculated, the weights are plugged into an activation function to approximate the

output. The MLP function is deﬁned as[20]:

fM LP : Rn0 −→ R, x = (x1, x2, ...xn0) −→ y

(2.1)

Where x refers to the number of inputs while R refers to the activation function.

The standard activation function used in most MLPs is σ(t) = 1/(1 + e−t) [20]. The

activation function is used after the hidden layers have been calculated.

OneR : The OneR is a simple decision tree based algorithm. It builds one rule for each

attribute in the training set, then selects the rule with the smallest error[21]. The

rule is built by ﬁnding out the most frequent class, which is simply the class that

appears the most for that value. The algorithm is as follows[22]:

8

1 For each attribute x, form a rule:

2

3

4

5

6

7

For each value y from the domain of x,

Select the set of instances where y has value x,

Let c be the most frequent class in that set.

Add the following clause to the rule for x:

if x has value y then the class is c

Calculate the classification accuracy of this rule

8 Use the rule with the highest classification accuracy.

Logistic Regression : Logistic Regression is a linear classiﬁer that uses assumes the out-

come is a linear function of independent variables. The probability of the linear com-

bination in logistic regression can be represented with the following formula:[23]

π(x) = P r(y = 1|x)

(2.2)

This formula can be further expanded by deﬁning it as:

π(x) = P r(y = 1|x; β) = 1/(1 + exp(−β0 − βT x)

(2.3)

β0 is a constant that refers to the bias in the equation. βT x refers to unknown values

in the model. Therefore, logistic regression models the probability of a variable y

taking on the value y = 1 depending on a number of independent variables x.

JRip : JRip is a rule-based classiﬁer which creates propositional rules that can be used

to classify elements[24]. It takes the instances in the dataset and evaluates them in

increasing order [25]. JRip then generates a set of rules and attributes for that par-

ticular class before moving on to the next class.

9

J48 : J48 is a lightweight classiﬁer that creates a decision tree using the C4.5 algorithm.

The C4.5 algorithm is a decision tree algorithm that uses gain ratio to select the best

”splitting” feature when making a decision tree[26]. Gain Ratio is deﬁned as

GainRatio(A) =

Gain(A)
SplitInf o(A)

,

(2.4)

where Gain(A) represents Information Gain for feature A and SplitInfo(A) represents

the potential information generated by splitting the set D over n outcomes according

to feature A. SplitInfo can be further deﬁned as

SplitInf oA(D) = −

k
(cid:88)

j=1

|Dj|
|D|

× log2

(cid:18) |Dj|
|D|

(cid:19)

.

(2.5)

2.4 Binary vs. Multiclass Classiﬁcation

Binary classiﬁcation is the simplest type of machine problem that takes an element

and classiﬁes it into one of two groups. It determines the grouping of an element by look-

ing at its attributes. In the context of malware detection, binary classiﬁcation determines

whether a program is malware or benign. What makes multiclass classiﬁcation diﬀerent

from binary classiﬁcation is the number of classes that must be taken into account. The

set of target classes can grow dynamically and is not ﬁxed, which can make it increasingly

complex [27]. For malware detection, it means that the machine learning algorithm must

identify if a software is a speciﬁc type of malware.

10

CHAPTER 3

EXPERIMENTAL SETUP

3.1 Dataset

For training and testing the machine learning models we used a per-class dataset of

microarchitectural features in [6]. The applications were executed on an Intel Xeon X5550

machine running Ubuntu 14.04 with Linux 4.4 Kernel. The dataset contains complete

samples collected from HPCs for both benign and malware. The malware data can be fur-

ther divided into ﬁve categories. There are 16 features collected for this dataset. Some of

the features included are bus cycles, cache misses, branch instructions, etc. The dataset

was collected using Perf, which is a tool available on the Linux operating system. Perf can

measure multiple events simultaneously [6]. Perf collects the data based on the execution

of both benign and malware applications. The programs are run in isolated environments

called Linux Containers which provide access to actual performance instead of emulated

HPCs[6]. A total of about 12000 samples were taken from the dataset and combined in

Excel to achieve a 70% - 30% data split. For binary classiﬁcation, the data was combined

to ensure about 70% malware - 70% benign for the training set and 30% malware - 30%

benign for the test set. For multiclass classiﬁcation, the same number of samples was used

except the malware data was replaced with more speciﬁc classiﬁers. Instead of just mal-

ware, it would be labeled trojan, rootkit, etc.

3.2 WEKA

For this thesis, we are using WEKA to process the dataset and obtain the results for

classiﬁcation accuracy. WEKA is a software tool developed by researchers at the Univer-

sity of Waikato that contains a collection of machine learning algorithms for data mining.

For this experiment, we are using the Explorer application of WEKA for both feature se-

lection and classiﬁcation. The Explorer application has several tabs that aid in process-

11

FIGURE 2. WEKA’s Explorer application contains several tools that can be
used for applying machine learning. The Classify tab is where the algorithms
are applied to create a model that can be used for test sets.

ing the dataset. For feature selection, the Select Attributes tab is used. This tab contains

the Attribute Evaluator which can be set by the user to select diﬀerent types of attribute

evaluation methods. It also has a section where the search method can be selected. For

classiﬁcation, the machine learning algorithms are selected under the Classify tab. In this

tab, the algorithm can be selected under the Classiﬁer section. This tab also allows for

customization for testing under the test options section. The results of the test set can be

seen in the Classiﬁer output window. An example of how the Classify tab looks can be

seen in Figure 2.

12

3.3 Feature Selection

Feature selection must be used on the data in order to ﬁnd the best possible classiﬁers

to use with the limited amount of HPCs [1, 6] (Figure 3). Previous studies have indicated

that many algorithms do poorly with large numbers of irrelevant features[18]. Performing

feature selection on the data can also increase the accuracy of the learning algorithm.

For the purposes of this thesis, the feature selection algorithm that will be used is In-

formation Gain. Information Gain makes use of entropy, which is a common quantity in

information theory associated with any random variable. Since entropy measures impuri-

ties in the training set, we can deﬁne Information Gain as the entropy of the training set

minus the average entropy of the child nodes. Information Gain is represented using the

formula[28]:

IG(X, y) = H(X) − H(X|y)

(3.1)

IG(X, y) refers to the information in the dataset for a speciﬁc random variable y. H(X)

refers to the entropy for the training set, while H(X—y) refers to the conditional entropy

based on the random variable y. After the data is collected, the extracted data must go

through pre-processing. The data is subdivided into 70% - 30% split with 70% for the

training set and 30% for the test set. Since HPCs only have a limited amount of registers,

the number of attributes per data must be reduced. Using WEKA’s Attribute Evaluator,

features are reduced using Information Gain Attribute Evaluation. The search method

used is Ranker. Features are then reduced by taking the top 8 features from the results of

the training set, then further reduced down up until it reaches 2 features.

For the purpose of this thesis, we used feature selection for both binary and multi-

classiﬁcation. As seen in Table 1 and Table 2, the top 8 features for both types of classiﬁ-

cation is very similar. The order only diﬀers slightly, which tells us that these top features

are the ones that matter most for malware detection for this speciﬁc dataset. L1-dcache-

13

FIGURE 3. The feature selection process selects relevant predictors from a
training set in order to increase the accuracy of the prediction variable. In this
model, orange represents redundant or impure classiﬁers while green
represents the desired classiﬁer.

TABLE 1. Binary Classiﬁcation Order of Attributes

loads and L1-dcache-stores are the top 2 HPCs after extensive feature selection, which

means that these two HPCs are really important to use when implementing the actual ma-

chine learning onto hardware.

14

TABLE 2. Multiclass Classiﬁcation Order of Attributes

FIGURE 4. Overview of run-time hardware-based malware detection approach
[1]

3.4 Classiﬁcation

Once the desired features have been determined, the training set must go through

WEKA’s Classiﬁer. For each classiﬁer, we supply the test set. WEKA then details the

accuracy of each classiﬁer in the output section. It also provides a confusion matrix that

details how many were classiﬁed correctly for each attribute. The results also detail impor-

tant information such as ROC Area and false positive rate. After obtaining the results, we

graph the ROC Area by visualizing the threshold curve and extracting the true positive

and false positive rates. The general approach for the classiﬁcation was based on a previ-

ous study[1]. An overview can be seen in the Figure 4.

15

CHAPTER 4

RESULTS AND ANALYSIS

4.1 Accuracy of Classiﬁcation

To evaluate the eﬃciency of the number of HPCs used, the percentage accuracy of

each classiﬁcation was calculated. The accuracy was calculated for diﬀerent numbers of

HPCs (16, 8, 4, and 2). As seen in Figure 5, the accuracy of the classiﬁers tend to go down

as the number of HPCs decreased. More complex algorithms such as Logistic and MLP

had the lowest percentage accuracy. OneR had no discernible decrease because the al-

gorithm only relies on one HPC to predict the behavior. For the multiclass classiﬁcation

model, the accuracy took a hit for all ﬁve classiﬁers. Since the same amount of malware

samples were used for both binary and multiclass classiﬁcation, having more speciﬁc classes

reduced the accuracy of malware identiﬁcation. Figure 5 shows how the accuracy is much

lower for all classiﬁers. Noticeably, the more complex logistic and MLP did better than the

OneR when it came to classifying diﬀerent types of malware.

4.2 Comparison of Receiver Operating Characteristics

Another metric to observe is the Receiving Operating Characteristics (ROC) result.

ROC measures the Area Under the Curve (AUC) for evaluating the robustness of each

ML classiﬁer[1, 4]. The ROC corresponds to the probability that the classiﬁer correctly

identiﬁed which application is malware and which is benign. Using this data, it is possible

to ﬁnd the threshold where the classiﬁer achieves 100% accurate positive identiﬁcation. As

TABLE 3. Binary Classiﬁcation ROC Area for Diﬀerent Number of HPCs

16

(a) Binary Classiﬁcation

(b) Multiclass Classiﬁcation

FIGURE 5. Accuracy of Classiﬁcation Models per Number of HPCs

17

TABLE 4. J48 and Logistic Multiclass Classiﬁcation Models’ ROC Area for
Varying Numbers of HPCs

seen in Table 3, the ROC Area results correspond to the accuracy of the ML classiﬁers. A

notable result is the sharp drop for the MLP classiﬁer when it goes from 4 HPC down to

2HPC. J48 also dropped but didn’t lose as much ROC compared to the other classiﬁers.

This result tells us that having 4 HPCs is an ideal amount of HPCs without losing too

much accuracy for binary classiﬁcation. The visualization of the ROC area can be seen in

Figure 6 for 8HPCs and 4HPCs. From these graphs, we can see that the reduction from 8

HPCs down to 4 HPCs reduces the true positive rate of most ML classiﬁers. It also shows,

however, that some classiﬁers do well better than others even with feature reduction. For

example, JRip and J48 still did better than the other classiﬁers, with J48 maintaining an

ROC Area over 0.9 for all diﬀerent number of HPCs. This gives valuable insight on which

ML Classiﬁer is best depending on the number of HPCs available.

Unlike binary classiﬁcation, the ROC area in multiclass classiﬁcation models vary de-

pending on the type of malware class. For this thesis, we show the multiclass classiﬁcation

results for two machine learning algorithms: J48 and Logistic (Table 4). J48 outperformed

18

(a) 8 HPCs

(b) 4 HPCs

FIGURE 6. Binary Classiﬁcation ROC Graphs with Varying Number of HPCs

19

the rest of the classiﬁers, on average. It did worse than logistic at identifying certain mal-

ware when the number of HPCs were high, but it did better when the number of HPCs

were low. The ROC Area for 4HPCS of both J48 and logistic are represented as AUC

graphs in Figure 7. As observed, the logistic multiclass classiﬁcation model really suﬀers

in performance when there are a low amount of HPCS. The J48 model, on the other hand,

seems to have consistent performance for all types of malware.

From this we can conclude that the complexity of an algorithm has no advantage

on accurately detecting multiple classes of malware. Even though logistic was more ac-

curate at 8-16 HPCs, that scenario is usually unlikely to be implemented at a hardware

level due to the limitations in the number of HPCs per processor. Therefore, implement-

ing lightweight algorithms such as J48 may be more eﬃcient when implementing multi-

classiﬁcation in the hardware level.

20

(a) Logistic

(b) J48

FIGURE 7. Multiclass Classiﬁcation 4HPCs ROC Graphs

21

CHAPTER 5

CONCLUSION

Hardware-based malware detection is an emerging ﬁeld in cybersecurity that imple-

ments machine learning-based malware detectors onto microprocessors. Hardware-based

detectors rely on machine learning classiers and use microarchitectural events captured by

Hardware Performance Counters (HPCs) at run-time to identify the malicious patterns.

Recent studies have shown that hardware performance and detection accuracy highly de-

pendon the type of machine learning used, as well as the type of HPCs used. Previous

works have used binary classiﬁcation to test the implementation of hardware-based mal-

ware detection. In this thesis, we compared and contrasted the results of binary and mul-

ticlass classiﬁcation models to ﬁnd out the diﬀerences when it comes to accuracy and de-

tection performance. In our results, we found that the multiclass classiﬁcation models de-

pended on similar features as the binary classiﬁcation models. We also found that out of

the ﬁve machine learning algorithms used, the most eﬃcient methods were J48 and JRip,

which both performed well even with the introduction of multiple types of malware. The

robustness of the J48 algorithm was also proven further with the ROC graphs for both bi-

nary and multiclass classiﬁcation. Given the variety of current malware, it is more eﬃcient

to implement this type of algorithm for hardware-based detection in order to better detect

diﬀerent malware.

5.1 Future Work

Currently, we are working on an ongoing research to implement multiclass classiﬁ-

cation machine learning detectors in MATLAB for hardware-based malware detection.

MATLAB has a toolbox called HDL Coder that allows for the synthesis of MATLAB code

into VHDL. Using HDL Coder as a tool for hardware-based malware detection is a new di-

rection in this research as previous works have used Vivado High-Level Synthesis instead.

22

Currently, I am working with the OneR algorithm as a case study for testing the con-

version process of HDL Coder. I implemented the OneR algorithm in MATLAB by fol-

lowing Holte’s 1R Classiﬁer [22]. The current MATLAB code takes in a set of vectors as

a training set and outputs the top feature after calculating the accuracy of each feature.

Data is ﬁrst sorted into temporary vectors based on their class (malware or benign). The

values in these vectors are then quantisized to ﬁnd the right rules for each feature. After

that, the values are tested for each feature using the rules that were created from quan-

tisization. To calculate the accuracy, it ﬁrst counts how many values were predicted cor-

rectly compared to their original class in the training set. The code then takes that total

and divides it by the original number of data in the training set. The resulting percentage

is the accuracy of that feature. We are working towards proposing the hardware imple-

mentations of binary and multiclass classiﬁcation models used for hardware-based malware

detection and extracting the hardware overhead and resource utilization characteristics of

the detectors such area, latency, and power consumption.

23

REFERENCES

[1] Hossein Sayadi, Nisarg Patel, Sai Manoj P.D., Avesta Sasan, Setareh Rafatirad, and

Houman Homayoun. Ensemble Learning for Eﬀective Run-Time Hardware-Based

Malware Detection: A Comprehensive Analysis and Classiﬁcation. In 2018 55th

ACM/ESDA/IEEE Design Automation Conference (DAC), pages 1–6, San Fran-

cisco, CA, June 2018. IEEE.

[2] Sai Manoj Pudukotai Dinakarrao, Hossein Sayadi, Hosein Mohammadi Makrani,

Cameron Nowzari, Setareh Rafatirad, and Houman Homayoun. Lightweight node-

level malware detection and network-level malware conﬁnement in iot networks.

In 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE),

pages 776–781. IEEE, 2019.

[3] Hossein Sayadi, Hosein Mohammadi Makrani, Onkar Randive, Sai Manoj PD, Setareh

Rafatirad, and Houman Homayoun. Customized machine learning-based hardware-

assisted malware detection in embedded devices.

In 2018 17th IEEE Interna-

tional Conference On Trust, Security And Privacy In Computing And Communi-

cations/12th IEEE International Conference On Big Data Science And Engineer-

ing (TrustCom/BigDataSE), pages 1685–1688. IEEE, 2018.

[4] Hossein Sayadi, Sai Manoj P D, Amir Houmansadr, Setareh Rafatirad, and Houman

Homayoun. Comprehensive assessment of run-time hardware-supported malware

detection using general and ensemble learning. In Proceedings of the 15th ACM

International Conference on Computing Frontiers, pages 212–215, 2018.

[5] Meltem Ozsoy, Khaled N. Khasawneh, Caleb Donovick, Iakov Gorelik, Nael Abu-

Ghazaleh, and Dmitry Ponomarev. Hardware-Based Malware Detection Using

Low-Level Architectural Features. IEEE Transactions on Computers, 65(11):3332–

24

3344, November 2016.

[6] Hossein Sayadi, Hosein Mohammadi Makrani, Sai Manoj Pudukotai Dinakarrao,

Tinoosh Mohsenin, Avesta Sasan, Setareh Rafatirad, and Houman Homayoun.

2SMaRT: A Two-Stage Machine Learning-Based Approach for Run-Time Special-

ized Hardware-Assisted Malware Detection. In 2019 Design, Automation & Test

in Europe Conference & Exhibition (DATE), pages 728–733, Florence, Italy, March

2019. IEEE.

[7] John Demme, Matthew Maycock, Jared Schmitz, Adrian Tang, Adam Waksman,

Simha Sethumadhavan, and Salvatore Stolfo. On the feasibility of online malware

detection with performance counters. In Proceedings of the 40th Annual Interna-

tional Symposium on Computer Architecture - ISCA ’13, pages 559–570, Tel-Aviv,

Israel, 2013. ACM Press.

[8] Anusha Damodaran, Fabio Di Troia, Corrado Aaron Visaggio, Thomas H. Austin,

and Mark Stamp. A comparison of static, dynamic, and hybrid analysis for mal-

ware detection. Journal of Computer Virology and Hacking Techniques, 13(1):1–

12, February 2017.

[9] Xueyang Wang, Charalambos Konstantinou, Michail Maniatakos, Ramesh Karri, Ser-

ena Lee, Patricia Robison, Paul Stergiou, and Steve Kim. Malicious Firmware

Detection with Hardware Performance Counters. IEEE Transactions on Multi-

Scale Computing Systems, 2(3):160–173, July 2016.

[10] Nisarg Patel, Avesta Sasan, and Houman Homayoun. Analyzing Hardware Based

Malware Detectors. In Proceedings of the 54th Annual Design Automation Confer-

ence 2017 on - DAC ’17, pages 1–6, Austin, TX, USA, 2017. ACM Press.

[11] Niloofar S. Alizadeh and Mahdi Abadi. Akoman: Hardware-Level Malware Detection

Using Discrete Wavelet Transform.

In 2018 IEEE International Conference on

25

Smart Computing (SMARTCOMP), pages 476–481, Taormina, June 2018. IEEE.

[12] Kanad Basu, Prashanth Krishnamurthy, Farshad Khorrami, and Ramesh Karri. A

Theoretical Study of Hardware Performance Counters-Based Malware Detection.

IEEE Transactions on Information Forensics and Security, 15:512–525, 2020.

[13] Sergii Banin and Geir Olav Dyrkolbotn. Multinomial malware classiﬁcation via low-

level features. Digital Investigation, 26:S107–S117, July 2018.

[14] Manuel Egele, Theodoor Scholte, Engin Kirda, and Christopher Kruegel. A survey

on automated dynamic malware-analysis techniques and tools. ACM Computing

Surveys, 44(2):1–42, February 2012.

[15] Yanfang Ye, Tao Li, Donald Adjeroh, and S. Sitharama Iyengar. A Survey on Mal-

ware Detection Using Data Mining Techniques. ACM Computing Surveys, 50(3):1–

40, October 2017.

[16] Xueyang Wang, Sek Chai, Michael Isnardi, Sehoon Lim, and Ramesh Karri. Hardware

Performance Counter-Based Malware Identiﬁcation and Detection with Adaptive

Compressive Sensing. ACM Transactions on Architecture and Code Optimization,

13(1):1–23, April 2016.

[17] Hossein Sayadi, Nisarg Patel, Avesta Sasan, and Houman Homayoun. Machine

learning-based approaches for energy-eﬃciency prediction and scheduling in com-

posite cores architectures. In 2017 IEEE International Conference on Computer

Design (ICCD), pages 129–136. IEEE, 2017.

[18] Jos Augusto Baranauskas, Oscar Picchi Netto, Srgio Ricardo Nozawa, and Alessan-

dra Alaniz Macedo. A tree-based algorithm for attribute selection. Applied Intelli-

gence, 48(4):821–833, April 2018.

[19] Ana Pano-Azucena, Esteban Tlelo-Cuautle, Sheldon Tan, Brisbane Ovilla-Martinez,

and Luis de la Fraga. FPGA-Based Implementation of a Multilayer Perceptron

26

Suitable for Chaotic Time Series Prediction. Technologies, 6(4):90, October 2018.

[20] S. Trenn. Multilayer Perceptrons: Approximation Order and Necessary Number of

Hidden Units. IEEE Transactions on Neural Networks, 19(5):836–844, May 2008.

[21] K. Deeba and B. Amutha. Classiﬁcation Algorithms of Data Mining. Indian Journal

of Science and Technology, 9(39), October 2016.

[22] C.G. Nevill-Manning, G. Holmes, and I.H. Witten. The development of Holte’s 1R

classiﬁer.

In Proceedings 1995 Second New Zealand International Two-Stream

Conference on Artiﬁcial Neural Networks and Expert Systems, pages 239–242,

Dunedin, New Zealand, 1995. IEEE Comput. Soc. Press.

[23] Che Ngufor and Janusz Wojtusiak. Extreme logistic regression. Advances in Data

Analysis and Classiﬁcation, 10(1):27–52, March 2016.

[24] Ivy M. Tarun, Bobby D. Gerardo, and Bartolome T. Tanguilig III. Generating Li-

censure Examination Performance Models Using PART and JRip Classiﬁers: A

Data Mining Application in Education. International Journal of Computer and

Communication Engineering, 3(3):202–207, 2014.

[25] Neeraj Bhargava, Aakanksha Jain, Abhishek Kumar, and Dac-Nhuong Le. Detection

of Malicious Executables Using Rule Based Classiﬁcation Algorithms. pages 35–38,

January 2018.

[26] Longjie Li, Yang Yu, Shenshen Bai, Ying Hou, and Xiaoyun Chen. An Eﬀective Two-

Step Intrusion Detection Approach Based on Binary Classiﬁcation and $k$ -NN.

IEEE Access, 6:12060–12073, 2018.

[27] Meng Joo Er, Rajasekar Venkatesan, and Ning Wang. An online universal classiﬁer

for binary, multi-class and multi-label classiﬁcation. In 2016 IEEE International

Conference on Systems, Man, and Cybernetics (SMC), pages 003701–003706, Bu-

dapest, Hungary, October 2016. IEEE.

27

[28] Marco Lippi, Manfred Jaeger, Paolo Frasconi, and Andrea Passerini. Relational infor-

mation gain. Machine Learning, 83(2):219–239, May 2011.

28

