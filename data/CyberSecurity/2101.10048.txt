A Process 

to Facilitate 

Automated 
Testing 
Cybersecurity 

Automotive 

Stefan 

Marksteiner 

Nadja Marko 
Vehicle 
stefan.marksteiner@avl.com 

Virtual 

AVL List 

Email: 

Email: 

Resarch 

Andre Smulders 

TNO 

nadja.marko@v2c2.at 

Email: 

andre.smulders@tno.nl 

Stelios 

Karagiannis 

Beyond Vision 

Florian 
AVL Software 

Stahl 
& Functions 

Hayk Hamazaryan 

ZF Friedrichshafen 
hayk.hamazaryan@zf.com 

Email: 

stelios.karagiannis@beyond-vision.pt 

florian.stahl@avl.com 

Email: 

Email: 

Rupert Schlick 
Institute 

of Technology 

Stefan Kraxberger 

Seclnto 

Alexandr 

Vasenev 
Centre ESI (TNO) 

Joint Innovation 

rupert.schlick@ait.ac.at 

stefan.kraxberger@secinto.com 

alexandr.vasenev@tno.nl 

Email: 

Email: 

Austrian 
Email: 

vehicles 

information 

become increasingly 

digital­
solutions 
technology-based 

assistance 

systems 
are complex 

These systems 

and increasing 

outside 

and intercon­
exposure 

and vehicle-to-x 

demand for more cyber-secure 

Abstract-Modem 

rising 

driving 

processes. 

Rising complexity 
a steadily 

ized with advanced 
like advanced 
communications. 
nected. 
has created 
systems. 
issued standards 
development 
be validated 
for more thorough, 
generally 
and optimized. 
for cybersecurity 
a structured 
automotive 
method so far. Despite 
framework, 
the process 
menters 

and verified. 
quicker 
manual testing 

to utilize 

testing 

Thus, also standardization 
and regulations 

bodies and regulators 

to prescribe 
however, 

more secure 
also has to 

This security, 
In order to keep pace with the need 

and comparable 

testing, 

today's 

processes 

have to be structured 

Based on existing 
engineering, 
process 

and emerging 
this paper therefore 
for verifying 

standards 
outlines 

and validating 

cybersecurity, 

for which 
presenting 
is flexible 

there is no standardized 
a commonly 
in order to allow imple­

structured 

their own, accustomed 

toolsets. 

and 

example 

systems. 

regulation 

incidents 
demand a substantially 
engineering 

and testing 

being a recent 

[6]. The rising 

This also creates 

requirements 

considerations 

amount of cybersecurity 

be­
effort is ISO/SAE DIS 21434 [5]. Also, regulators 
gin to take cybersecurity 
into account; 
the most prominent 
by the United Nations 
the regulators' 
higher 
of automotive 
higher efficiency 
of automotive 
automotive 
non-reproducible 
unstructured, 
to giving a standardized 
An approach 
is therefore 
grade testing 
process 
challenges 
with these upcoming 
prerequisite 
to automate 
This paper therefore 
standardized 

in this domain. 
to such a 

method 
which makes a standardized 

and more art than crafts. 

testing necessary. 

cybersecurity 

testing 
cybersecurity 

steps of testing 

an approach 

and will also be a 

necessary 

Currently, 
is mostly not holistic, 

and industrial­
to cope 

presents 

process. 

the need for 

testing 

Index Terms-Security, 
Cybersecurity, 
Process 
Verification, 

Validation, 

Testing, Automotive, 

1.INT RODUCT ION

of 

is likely 

The rising 

to the outside 

of modern automotive 

driving 
driving 

systems 
to assure their cyberse­
true due to the utilization 
(ADAS) and au­

assistance  systems 
(AD) and the exposure 
(V2X) functions. 
to accelerate 

complexity 
difficult 
make it increasingly 
curity. 
This is especially 
advanced 
tonomous 
by to vehicle-to-x 
technology 
leading 
selling 
opments 
Furthermore, 
eratingly 
benevolent 
[4]. This has also been recognized 
bodies -currently, 

cybersecurity 
rise of events 
criminal 
results 

an exponential 
ratio between 
research 

manufacturers 
model with V2X off-the-shelf 
incidents 

This usage of new 
even more; also market­
to equip their most­

(e.g. [2], [3]). 
and an accel­
activities 
versus 
can be observed 

the most important 

are beginning 

facilitate 

[1]. These devel­

security 

adverse 

as follows: 
III definitions, 

This paper structures 
work, Section 

related 
describes 
scribes the proposed 
and, finally, Section 

II contains 
IV 
while Section 
Section 
some process-intemal 
V de­
relations. 
security 
testing process 
this paper. 

automotive 
VI concludes 

Section 

II. REL ATED WORK

generic 

of creating 
to conduct 

The importance 
testbeds 

testing 
automated 
been highlighted 

frameworks 
tests 
security 
in the past. 

or security 
in automotive 
has already 
More specifically, 
fuzz testing 
to industry-specific 
Area Network (CAN bus) and the vehicle's 
control 

methods in accordance 
such as the Controller 
electronic 

unit (ECU) [7], [8], [9], [10] have been created. 

technologies 

Similarly, 

there are frameworks 

by standardization 
standardization 

atic methods of security 
tooth, 
gent Transportation 

testing 
Ad Hoc Networks 

Vehicular 

Systems 

[11], 
(ITS) and road services 

system­
that address 
Blue­
for automotive 

(VANETS) in Intelli­

© 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in 
any current or future media, including reprinting/republishing this material for advertising or promotional 
purposes,creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted 
component of this work in other works.

[12],  [13].  Finally  there  are  integrated  security  testing 
frameworks that improve the standardized methods  [9], 
[14],  [15],  [16],  [17].  All  of  these  works,  however,  do 
not encompass a defined process for automated security 
testing  of  complete  automotive  systems  in  a  holistic 
manner. This work therefore complements the standards 
with  a  structured  testing  approach  and  underpins  the 
technical  testing  solutions  with  a  structured  workflow 
method. As the upcoming ISO/SAE 21434 [5] is regarded 
to  become  the  most  important  guideline,  the  process 
aligns  to  it  (see  V).  There  is  a  supplement  to  the  ISO 
standard regarding testing (ISO/WO PAS  5112),  which, 
however,  is in  a larval  state. 

III.  DEFINITIONS 

Testing in the context of this paper  means verification 
and validation in the sense of ISO/SAE DIS 21434 (Sec­
tions  10.4.2  and  11)  [5].  An  Item,  according  to  the  ISO 
standard  mentioned  above,  is  a  system  or  combination 
thereof  to  implement  a  function  at  the  vehicle  level.  In 
the  sense  of  this  process,  an  item  is  understood  as  a 
technical  concept  that  defines  such  a  system.  A security 
goal  is  a  desired,  security-related  property  of  an  item, 
which is analyzed for threats and risks that lead to security 
requirements  that  are  collected  in  a  security  concept.  A 
System-under-test  (SUT)  is  a  concrete  technology  unit, 
e.g.  a  vehicle,  a  single  Electronic  Control  Unit  (ECU) 
or  a  software,  that  concretely  instantiates  an  item  that 
is  an  examination  subject  in  this  process.  Therefore,  an 
item can address multiple SUTs(e.g. different cars of the 
same  type).  A  fest  system  is  the  active  unit  that  carries 
out  the  test  on  an  SUT.  The  proprietor  of  an  SUT  or 
item,  respectively,  is  an  item  owner.  The  user  of  a  test 
system  is  a  fest  operator.  The  compound  of  SUTs  and 
test system(s) including interfaces and surroundings (e.g. 
an automotive testbed) constitute a fest environment.  Test 
scenarios  in  the  context  of  this  paper  are  abstract  test 
descriptions  that  define  what  to  test  by  which  means, 
consisting  of  fest  patterns  as  their  atomic  elements  that 
describe  single  scenario  stages.  They  are  derived  from 
a  security  analysis  and  requirements  definition  of  the 
respective  item.  Test  cases  are  the  concretization  of  sce­
narios for a specific SUT, consisting of fest scripts, which 
are executable tests that run on a test system and target 
an  item. 

IV.  RELATION BETWEEN TEST  SCENARIOS AND TEST 
CASES 

The  reason  to  provide  abstract  test  descriptions,  that 
preempt  some  test  case  generation  (tcg)  operations,  is 
portability to other items. Although test scenarios derive 
from threat assessments, they are described generic, con­
veniently in a domain specific language (DSL - e.g. [18]). 
Test  patterns  are,  consequently,  generic  means  to  test  a 
part of the system - e.g. sending a CAN message with a 
break  signal  (e.g.  SEND CAN_MSG()) - that  constitute 

a  scenario.  These  means  concretized  with  test  scripts 
(attack steps - e.g . ./ cansend canO 7df#02010d) 
that even­
tually  form test  cases (concretizations of test scenarios -
see Sections V-E and V-F). The difference is that scenarios 
and pattems only generically describe what to do, while 
the cases and scripts are concrete elaborations how to do 
it. The test cases therefore augment scenarios during the 
tcg with information (e.g. CAN messages) from a specific 
SUT  in  order  to  test  it.  The  purpose  of  the  test  cases 
is  to  allow  for  automated  testing  using  an  appropriate 
framework.  Figure  1  illustrates  the  coherence  between 
the test scenarios  with their  pattems and the cases with 
their  scripts:  scenarios  consist  of  abstract  pattems  and 
cases consist of concrete scripts. 

Figure  1.  Relation between generic test scenarios/patterns and con­
crete test cases / scripts 

V.  AUTOMOTIVE SECURITY PROCESS 

This section outlines the security testing process which 

consists of the following activities: 

1)  Define Item; 
2)  Perform  Risk and Threat  Analysis; 
3)  Define Security Concept (testing  requirements); 
4)  Plan Test and Develop Scenarios; 

a)  Define Penetration Test Scenarios; 
b)  Define Functional and Interface Test Scenarios; 
c)  Define Fuzz  Testing Scenarios; 
d)  Define Vulnerability Scanning  Scenarios; 

5)  Select Test Scripts; 

a)  Develop Test Scripts; 
b)  Validate Test Scripts; 

6)  Generate Test Cases; 
7)  Perform Test; 

a)  Prepare Test Environment; 
b)  Execute  Test  Cases 
8)  Generate Test Reports. 

The  process  is  based  on  the  security  testing  sections 
of  ISO/SAE  (DIS)  21434  [5].  The  first  three  activities 

1. Define item

Fe ature s,
pre limin ary
architecture o f
SUT

Start

2. Perform Risk and
Threat Analysis

ite m de finitio n, se curity
go als

threats, Risk
me tric

threats,
risk metric

Kn own
T h reats

3. Define security
concept

Vu lnera bilitie s

security
req uirem ents

4. Plan test and develop scenarios

4a. Define
Penetration test
scenarios

4b. Define functional
and interface test
scenarios

4c. Define fuzz
testing scenarios

4d. Define
vulnerability
scanning scenarios

test
sce nario s

test
sce nario s

SUT Da tabase

SUT-spe cific informatio n

5. Select test scripts

Test
sce nario s

fou nd
vul nerab ilitie s

Test
scripts

Test
cases

Test scripts

Test scripts

Test scripts

Test cases

6. Generate test
cases

5a. Develop test
scripts

test
scripts

val idatio n
result

5b. Validate test
scripts

exe cuta ble te st
cases

7. Perform Test

int erface dete rmin ation ,
cal ibrati on

en vironm ent
de script ion

7b.Execute Test
Cases

7a. Prepare test
environment

Test cases

Test results

Test Results (RAW)

Test results

Test reports

test
results

8. Generate test
report

Test
rep orts

End

requirements are the input for the test planning, as they 
should  be  verified  with  regard  to  the  consistency  with 
the  security  goals  and  the  item's  functionality  [5].  The 
following  steps  derive  such  requirements  [25] 

1)  Collect  the results from the threat analysis; 
2)  Define threat countermeasures; 
3)  Map the resulting threats to countermeasures. 
This method allows for using different attack mitigation 
techniques  as  building  blocks  that  can  later  be  referred 
by multiple requirements. The security requirements de­
rive the relevant scenarios for the tests and give direction 
on what needs to be  tested. 

D. Plan  Test and  Develop  Scenarios 

A security test plan should organize the security test-

ing process and contain the following elements  [26]: 

•  Purpose/Objectives; 
•  SuT overview and Test scope; 
•  Risk analysis; 
•  Test  strategy and requirements; 
•  Test  environment  (Hardware- or  Software-in-the­

Loop,  actual  vehicle,  etc.); 

•  Test case  specifications; 
•  Test  execution and termination  criteria. 

This  also  corresponds  to  a  verification  and  a  validation 
specification according to ISO/SAE DIS 21434 (10.5 and 
11.5, respectively)  [5]. The output of this activity is a set 
of  defined  test  scenarios  which,  dependent  on  the  risk 
level  and  attack  feasibility,  apply  different  techniques. 
Test scenarios are between system requirements and test 
cases  [27]  and  are  abstract  test  descriptions  (consisting 
of  test  patterns)  that  define  which  vulnerabilities  and 
requirements  are  specifically  tested  with  which  meth­
ods.  Testing  methods,  based on  the  identified  risks and 
threats,  are  [5]: 

•  Functional  testing 
•  Interface testing 
•  Static code analysis; 
•  Penetration testing; 
•  Vulnerability scanning; 
•  Fuzz testing. 

A  Test  pattern  is  the  generic  description  of  a  single 
step  inside  a  test  (normally  an action  during  an  attack) 
including  potentially  used  tools,  but  not  specific  to  an 
SuT.  These  methods  should  be made  concrete  with  test 
scripts  (containing  e.g.  an  exploit)  that  eventually  form 
test cases (see Section IV  for the coherence between test 
scenarios  and  test  cases).  Scenarios  are  derived  by  re­
quirements analysis, equivalences classes, boundary val­
ues  analysis  and  error  guessing  [5].Furthermore,  attack 
patterns  are  derived  from  generalizing  existing  attacks 
that  may  be  derived  from  open  databases  (for  well­
known attacks) or intrusion detection signatures as well 
as  actual  attack analyses. For  an  automated  test system 
implementing  the process, the availability of attacks for 

the derived models ( or constituting components, respec­
tively)  determines  the  used  test  patterns  and,  thus,  the 
test plan,  including the test  methods. 

1)  Define Penetration  Test Scenarios:  Penetration testing 
is the legal and authorized process of exploiting systems 
in  order  to  retrieve  information  which  is  important 
for  enhancing  security  of  the  system.  Penetration  tests 
focus  on  specific  aspects  of  security  and  are  deployed 
manually  or semi-automatic. To  extend  the  capabilities, 
global-based  adversarial  activities  must  be  deployed  to 
maintain  a  holistic  view  of  the  system  and  deploy  se­
curity tests from the adversary's perspective. The above 
methods are called red team assessments which usually 
include penetration tests; however, such methods extend 
the whole process  [28]. A successful  penetration testing 
methodology  will  discover  functional  weaknesses,  de­
sign  flaws  and  provide  recommendations  for  security 
improvement  [29]. To deploy penetration  test  scenarios, 
the scope and the context for deployment of appropriate 
attack  strategies  with  respect  to  the  system's  potential 
weaknesses must be defined. In penetration testing, it is 
possible to attack vehicles without in-depth knowledge 
(black box) or from the inside (white box - meaning that 
some  or  full  information  is  available  to  the  red  team). 
The process suggests cyber kill chain [30] and attack trees 
[31],  where  the  latter  approach  allows  for  automated 
decision making for  generating attack vectors. 

2)  Define  Functional  and  Interface  Security  Testing  Sce­
narios:  Functional tests assess the system's adherence to 
its  functional  requirements  (correctness)and  take  place 
throughout  the  whole  process  and at different levels  of 
abstraction.  Testing  security  functions  focuses  on  test­
ing  the  security  requirements.  Typical  security  require­
ments  may  include  specific  elements  of  confidentiality, 
integrity,  authentication,  availability,  authorization  and 
non-repudiation. There are two possibilities of formulat­
ing  security  requirements:  1.  positive  requirements  and 
2.  negative  requirements.  Positive  formulated  require­
ments  describe  how  a  security  function  should  work. 
Negative  requirements  state  behaviour  that  the  soft­
ware  should  not  exhibit.  The  mapping  of  requirements 
to  specific  software  artifacts  could  be  problematic  for 
such  requirements,  since  this  kind  of  requirement  is 
not implemented in  a specific place[26].  When negative 
requirements are tested, security testers look for common 
mistakes  and  test  assumed  weaknesses  in  the  applica­
tion.  The  emphasis  is  on  finding  vulnerabilities,  often 
by  executing  misuse  tests.  To  derive  the  test  cases,  the 
following steps need to be  carried  out: 

1)  Identify functions expected  to  perform. 
2)  Create test cases based on the function methods. 
3)  Determine the output based on the function speci­

fications. 

3)  Define  Fuzzing  Scenarios:  Fuzzing  is  a  technique 
to  use  random  input  in  order  to  put  an  SUT  into  a 
non-intended  state  to  uncover  errors,  which  could  be 

testing 

[32]. However, 
does not have to be complete 

(combining 

listening 
[33]. A 
valid and ran­

a monitoring 
system 
the 
that determines 

mechanism, 

of a generator 

than structured 

of fuzz testing 
to an SUT using passive 

more efficient 
randomness 
but adapted 
fuzzer consists 
dom parts), 
a delivery 
and a test oracle [34]. The oracle, 
test result 
communications 
as well [35]. Using fuzzing 
to attack 
[36]. In principle, 
interface 

(i. e. pass/fail), 
or using specific 

without 
any component 

automobiles 

can be fuzzed. 

is obtained 

by monitoring 
(like XCP) 
it is possible 

protocols 
techniques, 
any in-depth knowledge 
that shows an external 

SUT; 

• Specifics 
ability 

and carried 
beforehand. 
of test patterns, 

is optional 
present 
tations 
in the scenario 
SUT. A test script 
• The testing 

tool(s) 
may be derived 
• Needed parameters 

oracle 

out if no appropriate 

test script 
are concrete implemen­

The scripts 

is 

making use of the tools outlined 
towards 

a specific 
description targeting 

is an executable 

script 
tobe used (parameters, 

that contains: 

interfaces, 

from the test scenario; 

and information 

specific 

to the 

can [37]: 
Fuzz testing 
• be used to reverse 
• be used to disrupt 

engineer 

messages; 

vehicle 
s communication 

net­

vehicle' 

work; 

• be a form of cyber attack; 
component 
• lead to vehicle 
large test space, fuzzing 

damage. 

For a significantly 
combined with combinatorics 
run in parallel 
is not covered. 

as long as a test series 

to select 

test cases and be 
runs and the space 

should be 

of the test system (e.g. using Linux, avail-

compiler/interpreter, 

attack 

actual 

scripts 

of a certain 
to test scenarios, 
by observing 

Similar 
open sources 
created 
various 
approaches 
ensure that the case generation 
use scripts 
1) Either 
2) Or develop 

etc.). 
are derived 
from 
are 
an SUT or they are derived 
from 
trees [39]. To 
like attack 
step (Section 
V-F) can re­
the current 
script; 
to the specifications 

from the database, 
match an existing 
a test script 

by analysing 
structured 

step should: 

Test scripts 

attacks. 

of the 

test scenario. 

In the latter 
the SUT or further 
interpretation 

case, extensive 
specifics 
file for particular 

about 
technical 
knowledge 
might be needed (e.g., 
an 
CAN messages). 

4)  Vulnerability Scanning Scenarios:  Vulnerability 

scan­

called 

ning uses tools, 
pare a vulnerability 
tained from a network 
[38]. 
ties in the network 

vulnerability 

scanners, 
that com­
ob­
with the information 

database 

scan to find possible 

vulnerabili­

enumerates 

known software 

vul­
baseline 
of 
a comprehensive 
vulnerabil­

effective 

To perform 
vulnerabilities. 
the tools should be selected 

based on the 

A scanner typically 
and provide 

nerabilities 
existing 
ity scanning, 
scanning 
scope. 
the vulnerability 
scanning is: 
for using vulnerability 

scanning scenarios. 

A typical 

scenario 

This scope is needed to define and create 

• Define which system to scan (i.e. 
thereof); 

ponents 

the SUT or com-

2)  Validate  Test  Scripts:  This  optional  activity 

applies 

to validate 

them before 

are tried out on simulated 
or actual 
from the test oracle) 

test environment. 

results. 

are 
SUTs should be 
and negative 
In 

results 

tests. 

acquired 

test scripts 

New test scripts 
SUTs in a simulated 
outcomes 
(derived 
to actually 

to newly developed 
actual 
or actual 
Expected 
compared 
chosen in a way that both positive 
can be obtained 
order to validate 
fulfill 
actual 
script 
thereof 

any prerequisite 
test). 
should contain 
that include: 

in specific 
a test script, 

to test cases, 

different 

Similar 

set in the test script 

well-defined 

conditions. 
the environment  should 
(similar 
to an 
of the test 
the validation 
SUTs or configurations 

• Define the tool that should be used for the scanning; 
• Perform 
• Analyze the resulting 

report (i.e. 

identify 

the scan; 

relevant 

1) an SUT configuration 
itive validation); 
2) an SUT configuration 

vulnerabilities); 

(negative 

validation); 

with a successful 

attack 

(pos­

attack 
with an unsuccessful 

• Specify 

further 
For automation, 
serve as an input for other scenarios. 

tasks. 
(in machine readable 

analysis/testing 

the results 

form) 

3) several 

edge cases. 

The validation 
in actual 
coverages 

tests (see Section 

V-F). 

test coverage 

should be comparable 

to 

E.  Select  Test  Scripts 

F.  Generate  Test  Cases 

concerns the transition 

(from the test scenarios) 

addressing 

A test case includes 

items from this non­

multiple 
and extended 

exhaustive 

list derived 

from [40], [41], [42]: 

(found in the threat 
tobe executed 

onto a specific 

assessment) 

• Test purpose 
• SUT /Function description (including 

and objectives; 

test 

of generic 
vulner­
into concrete 
SUT. Test scripts 

from a database, 

if available, 

or otherwise 

This section 
descriptions 
abilities 
test scripts 
are selected 
developed. 

1)  Develop  Test Scripts:  The purpose 

is to populate 
particularly 

general, 
tests, 
plan and implement 

attacks. 

The scripts 

a test script 

database 

defined test patterns. 

in 

of this activity, 
with relevant 
to the 
This activity 

correspond 

software 

/hardware 
• Environmental 
• Procedural 
• Test activities 
• Expected 

needs including 
requirements, 

and input data; 

/ firmware configurations 

); 
dependencies; 

test setup and condition; 

tion criteria 

results, completion, 
criteria 

(Pass/Fail 

including 

stopping 

and resump­

metrics); 

•  Traceability to related requirements and threats; 
•  Variability and  quality attributes. 

In  the  context  of  this  process,  the  test  case  generation 
(tcg)  is  the  fusion  of  a  generic  test  scenario  (Section 
V-D) and the test scripts (Section V-E) that are specific to 
a  distinct  SUT.  Augmenting  the  scenarios  with  specific 
information  from  an  SUT  database  translates  the  test 
scenarios  into  executable  test  cases.  With  both  parts 
available in  machine-readable  form, this activity is easy 
to  automate.  Combinatorial  testing  [43]  allows  for  an 
efficient  coverage/ effort  ratio.  The  tcg  can  re-use  the 
threat  modelling  outcomes  in  conjunction  with  a  test 
script  database,  giving  the  opportunity  of  automating 
the process using a framework (e.g. [44]) If a clear model 
is lacking completely, test coverage is most important. 

G.  Perform  Testing 

To  execute the test, a test environment shall be  estab­
lished using an description from the scenarios. The envi­
ronment  description  contains  all  required  prerequisites, 
while the test cases contain  the  performed operations 

1)  Prepare Test Environment:  Two inputs are needed to 
prepare  a  test  environment:  (a)  an  environment  config­
uration and (b) interface  descriptions.  The resulting test 
environment  template  is  then  used  to  execute  tests.  A 
configuration consists primarily of the system under test 
(SUT)  and  applicable  test  categories,  including  system 
and service preconditions. Interface descriptions contain 
their  stimulation  and  provisions,  as  well  as  verification 
procedures for their claimed properties. For automation, 
they  are  organized  in  an  object-oriented,  serializable 
manner.  The  resulting  combination  of  the  environment 
and  the  interface  description  form  a  test  environment 
template to applied with different test cases from diverse 
categories,  ideally  in  a  microservices-based,  container­
ized style. This activity also includes saving a pre-attack 
state  of  the  SUT  and  a  clean-up  procedure  after  the 
conducted test (e.g. if a test involves flashing  ECUs). 

2)  Execute  Test  Cases:  Each  test  case  consists  of  a 
sequence  of  test  scripts  (as  minimum  verifiable  actions 
- MVAs)  that  can  be  combined  to  form  more  complex 
sequences. Resulting sequences  can be combined  again. 
The combinations can also contain permutations and re­
organizations of scripts. For automation, final commands 
take  a  shell-executable  form.  Test  cases  create  specific 
outputs on  defined  interfaces.  This output  is consumed 
by an  interface  module  that  transforms  the  output  into 
a  correct  call  for  the  associated  physical  interface  and 
the  retumed  response.  A  completed  test  case  output  is 
subsequently  converted  into  a  standardized  test  result. 
Test  results  are  stored  and  used  as  input  for  other  test 
cases,  further  analysis,  and  reporting  (see  V-H),  includ­
ing relevant meta  data in a standardized format. 

H.  Generate  Test Report 

A test report is a presentation of the combined results 

of the process,  it should contain: 
•  A management  summary; 
•  An SUT description; 
•  start time and duration; 
•  An aggregated overview (dashboard); 
•  The approach/method used; 
•  Findings  (passed and failed  tests); 

For the executed tests, pass and fail information (and in 
case of failed tests:  sufficient  information  to understand 
the problem) must be given. In both cases, reference links 
to goals, requirements, used tools, the raw data, the test 
results (including risk levels and severity categorization 
and conflicts with regulations, policies or best practices) 
and information about aspects that were not tested (not 
planned, technical problems, lack of time, funds or tools, 
etc.)  need  to  be  included.  The  testing  report  should 
also correspond to a verification and a validation report 
according to ISO/SAE DIS 21434  (10.5 and 11.5)  [5]. 

VI.  CONCL US ION  AND  ÜUT LOOK 

This  paper  outlined  a  process  for  testing  the  cyber­
security  of  (particularly  automotive)  systems  to  fill  the 
gap  between existing standards for automotive security 
engineering  and  their  hands-on,  actual-system  testing. 
The process provides a comprehensive, automatable ap­
proach for system testing based on ISO/SAE DIS 21343. 
Due  to  rising  complexity  and  regulators'  requirements 
this  is  necessary  as  it  facilitates  a  conceivable  need 
for industrializing automotive cybersecurity testing. The 
process  itself  is  arranged  generically  in  order  to  allow 
for  using  already  existing  procedures  (e.g.  a  present 
risk  assessment  process)  not  mandating  any  specific 
method.  Future  work  will  therefore  involve  a  reference 
implementation on both processual  and technical level. 

ACKNOWLEDGEMENT 

This  work was supported  by  the H2020-ECSEL  pro­
gram of the European Commission; grant no. 783119, SE­
CREDAS project. Special thanks to Rosita Jupri, Behrooz 
Sangcholie and Rauli Kaksonen for their help. 

REFERENCES 

[1]  Vokswagen,  "Eighth-generation  Volkswagen  Golf  GTI  makes 
global  debut  at  the  Geneva  Motor  Show,"  2020,  [Online 
press 
[Online].  Available: 
https:/ / www.media.vw.com/ releases / 1261 

retrieved  2020-07-16]. 

release; 

[2]  K. Koscher,  A. Czeskis, F.  Roesner, S. Patel,  T. Kohno, S. Check­
oway, D. McCoy, B. Kantor, D. Anderson, H. Shacham, and S. Sav­
age, "Experimental security analysis of a modern automobile," in 
2010 IEEE Symposium an Security and Privacy, 2010, pp.  447-462. 
[3]  C.  Miller  and  C.  Valasek,  "Remote  exploitation  of  an  unaltered 

passenger vehicle," Black Hat USA, 2015. 

[4]  Upstream  Security,  "Upstream  Security  Global  Automotive  Cy­

bersecurity Report," Upstream Security, Tech. Rep., 2020. 

[5]  International  Organization  for  Standardization  and  Society  of 
Automotive  Engineers,  "Road  Vehicles  - Cybersecurity  Engi­
neering,"  International  Standard,  International  Organization  for 
Standardization, ISO/IEC Standard  "21434",  2019. 

[6]  United Nations  Economic  and  Social  Council  - Economic  Com­
mission for Europe, "UN Regulation on uniform provisions con­
cerning  the  approval  of  vehicles  with  regard  to  cyber  security 
and  of  their  cybersecurity  management  systems,"  UNECE,  UN 
Regulation ECE/TRANS/WP.29/2020/79,  2020. 

[7]  D. S.  Fowler,  M. Cheah, S. A. Shaikh, and J. Bryans, "Towards a 
testbed  for  automotive  cybersecurity,"  in 2017 IEEE  International 
Conference  on  Software  Testing,  Verification  and  Validation  (ICST), 
2017, pp. 540-541. 

[8]  R. Kurachi and T. Fujikura, "Proposal of hils-based in-vehicle net­
work security verification environment," in WCX World Congress 
Experience.  SAE International, apr 2018. 

[9]  E.  dos  Santos,  A.  Simpson,  and  D.  Schoop,  "A  formal 
model  to  facilitate  security  testing  in  modern  automotive 
systems,"  Electronic  Proceedings  in  Theoretical  Computer  Science, 
vol.  271,  pp.  95-104,  May  2018. 
[Online].  Available: 
http://dx.doi.org/10.4204/EPTCS.271.7 

[10]  T. Huang, J. Zhou, and A. Bytes, "Atg: An attack traffic generation 
tool for security testing of in-vehicle can bus," in Proceedings of the 
13th International Conference on Availability, Reliability and Security, 
ser. ARES 2018.  New York, NY, USA: ACM, 2018. 

[11]  M.  Cheah,  S.  A.  Shaikh,  0.  Haas,  and  A.  Ruddle,  "Towards 
a  systematic  security  evaluation  of  the  automotive  bluetooth 
interface," Vehicular Communications, vol. 9, pp. 8 - 18, 2017. 
[12]  M. R. Friesen and R. D. McLeod, "Bluetooth in intelligent trans­
portation  systems:  a  survey,"  International  Journal  of Intelligent 
Transportation Systems Research, vol. 13, no. 3, pp. 143-153, 2015. 
[13]  M. Cheah, S. A. Shaikh, J. Bryans, and H. N. Nguyen, "Combining 
third party components securely in automotive systems," in IFIP 
International Conference on Information Security Theory and Practice. 
Springer, 2016, pp. 262-269. 

[14]  J.-P.  Monteuuis,  A.  Boudguiga,  J.  Zhang,  H.  Labiod,  A.  Servel, 
and  P. Urien,  "Sara:  Security  automotive  risk  analysis  method," 
in Proceedings of the 4th  ACM  Workshop  on  Cyber-Physical  System 
Security.  New York, NY, USA: ACM, 2018, pp. 3-14. 

[15]  L.  Ming,  G.  Zhao,  M.  Huang,  X.  Kuang,  J.  Zhang,  H.  Cao, 
and  F.  Xu,  "A  general  testing  framework  based  on  veins  for 
securing vanet applications," in 2018 IEEE SmartWorld,  Ubiquitous 
Intelligence Computing,  Advanced  Trusted  Computing,  Scalable Com­
puting Communications, Cloud Big Data Computing, Internet of People 
and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBD­
Com/IOP/SCI), 2018, pp.  2068--2073. 

[16]  H. Srinivasan and K. Sarac, "A  sip  security  testing framework," 
in 2009 6th IEEE Consumer Communications and Networking Confer­
ence, 2009, pp. 1-5. 

[17]  S. Hagerman, A. Andrews, and S. Oakes, "Security testing of an 
unmanned aerial vehicle (uav)," in 2016 Cybersecurity Symposium 
(CYBERSEC), 2016, pp. 26-31. 

[18]  C.  Michel  and  L.  Me,  "Adele:  An  attack  description  language 
for knowledge-based intrusion detection,"  in Trusted  Information, 
M. Dupuy and P. Paradinas, Eds.  Springer US, 2001, pp. 353-368. 
[19]  D.  R.  Crow,  S.  R.  Graham,  and  B. J.  Borghetti,  "Fingerprinting 
vehicles  with  can  bus  data  samples,"  in  ICCWS  2020  15th  In­
ternational  Conference  on  Cyber  Warfare  and  Security.  Academic 
Conferences and publishing limited, 2020, p. 110. 

[20]  B. K. Aichernig, R. Bloem, M. Ebrahimi,  M. Tappler, and J. Win­
ter,  "Automata  learning for  symbolic execution,"  in 2018  Formal 
Methods in Computer Aided Design (FMCAD), 2018, pp. -9. 
[21]  D.  Ward, 1.  Ibarra, and A.  Ruddle,  "Threat  analysis  and risk as­
sessment in automotive cyber security," SAE International Journal 
of Passenger Cars-Electronic and Electrical Systems, vol. 6, no. 2013-
01-1415, pp. 507-513, 2013. 

[22]  J.  Barnat,  L.  Brim,  V.  Havel,  J.  Havlfcek,  J.  Kriho,  M.  Lenco, 
P.  Rockai,  V.  Still,  and J.  Weiser,  "Divine  3.0  - an  explicit-state 
model checker for multithreaded c & c++ programs," in Computer 
Aided  Verification,  N.  Sharygina  and  H.  Veith,  Eds. 
Berlin, 
Heidelberg: Springer, 2013, pp. 863--868. 

[23]  M.  Islam,  C.  Sandberg,  A.  Bokesand,  T.  Olovsson,  H.  Broberg, 
P. Kleberger,  A.  Lautenbach,  A.  Hansson,  A.  Söderberg-Rivkin, 
and  S.  P.  Kadhirvelan,  "Security  Models,"  HEAVENS  Project, 
HEAVENS Project Deliverable D2, 2014. 

[24]  G. Macher, H. Sporer, R. Bedach, E. Armengaud, and C. Kreiner, 
"Sahara:  A  security-aware  hazard  and  risk  analysis  method," 

in  2015  Design,  Automation  Test  in  Europe  Conference  Exhibition 
(DATE), 2015, pp.  621-624. 

[25]  S.  Marksteiner,  H.  Vallant,  and  K.  Nahrgang,  "Cyber  security 
requirements engineering for low-voltage distribution smart grid 
architectures using  threat  modeling," Journal  of Information Secu­
rity and Applications, vol. 49, p. 102389, 2019. 

[26]  C.  Michael,  K.  van  Wyk,  and  W.  Radosevich,  "Risk­
and  Functional  Security  Testing,"  Cybersecurity 
(CISA),  Tech.  Rep., 
[Online].  Available: 

Based 
and 
2005, 
https://www.us-cert.gov/bsi/ articles/best-practices/security­
testing/risk-based-and-functional-security-testing 

Infrastructure  Security  Agency 
retrieved  at  April  17,  2020. 

[27]  Wei-Tek  Tsai,  Xiaoying  Bai,  R.  Paul,  and  Lian  Yu,  "Scenario­
based  functional  regression  testing,"  in 25th  Annual International 
Computer Software and Applications Conference, 2001, pp.  496-501. 
[28]  B.  J.  Wood  and  R.  A.  Duggan,  "Red  teaming  of  advanced  in­
formation assurance concepts," in Proceedings DARPA Information 
Survivability Conference and Exposition, vol. 2, 2000, pp. 112-118. 

[29]  R. R. Linde, "Operating system penetration," in Proceedings of the 
May 19-22, 1975, National Computer Conference and  Exposition, ser. 
AFIPS '75.  New York, NY, USA: ACM, 1975, pp. 361-368. 
[30]  1. Tarnowski, "How to use cyber kill chain model to build cyber­
security?" European Journal of Higher Education IT,  2017. [Online]. 
Available: http:/ /www. eunis. org/ download/TNC2017 /TNC17-
IreneuszTarnowski-cybersecurity.pdf 

[31]  S.  Mauw  and  M.  Oostdijk,  "Foundations  of  attack  trees,"  in 
Information Security and  Cryptology - ICISC 2005,  D. H. Won and 
S. Kirn, Eds.  Berlin, Heidelberg: Springer, 2006, pp. 186-198. 

[32]  J. W. Duran and S. Ntafos, "A report on random testing," in Pro­
ceedings of the  5th International Conference on Software Engineering, 
ser. ICSE '81.  IEEE Press, 1981, pp. 179-183. 

[33]  D. S. Fowler, J. Bryans, M. Cheah, P. Wooderson, and S. A. Shaikh, 
"A method for constructing automotive cybersecurity tests, a can 
fuzz  testing  example,"  in  9th International Conference on  Software 
Quality,  Reliability and Security Companion, 2019, pp. 1-8. 

[34]  R.  McNally, K. Yiu,  D. Grove,  and  D.  Gerhardy, "Fuzzing:  The 
state  of  the  art,"  Defence  Science  and  Technology Organisation 
Edinburgh (Australia), DSTO-TN "1043", 2012. 

[35]  P.  Lapczynski, H. Heinemann,  T.  Schöneberger,  and E. Metzker, 
"Automatically generating fuzz tests from automotive communi­
cation databases," in 5th escar  USA, Detroit,  isits AG, 2017. 
[36]  H.  Lee, K.  Choi,  K.  Chung,  J.  Kirn,  and  K.  Yim,  "Fuzzing  can 
packets into automobiles," in 2015 IEEE 29th International Confer­
ence on Advanced Information Networking and Applications, 2015, pp. 
817--821. 

[37]  D.  S.  Fowler, J.  Bryans,  S.  A.  Shaikh,  and  P.  Wooderson,  "Fuzz 
testing for automotive cyber-security," in 2018 48th Annual IEEE/I­
FIP  International  Conference  on  Dependable  Systems  and  Networks 
Workshops (DSN-W), 2018, pp.  239-246. 

[38]  H. Holm, T. Sommestad, J. Almroth, and M. Persson, "A quantita­
tive evaluation of vulnerability scanning," Information Management 
& Computer Security, 2011. 

[39]  W. Nichols, Z. Hili, P. Hawrylak, J. Haie, and M. Papa, "Automatie 
generation  of  attack  scripts  from  attack  graphs,"  in  International 
Conference on Data Intelligence and Security, 2018, pp.  267-274. 
[40]  P. M. Kamde, V. D. Nandavadekar, and R. G. Pawar, "Value of test 
cases in software testing," in 2006 IEEE International Conference on 
Management of Innovation and Technology, vol. 2, 2006, pp. 668--672. 
[41]  M.  Nahas  and  R.  Bautista-Quintero,  "Applying  the  scheduler 
test case technique to verify scheduler implementations in multi­
processor time-triggered embedded systems," American Journal of 
Engineering Research (AJER), vol. 5, no. 9, pp. 93-104, 2016. 
[42]  K. Heussen, C.  Steinbrink, 1. F.  Abdulhadi, V.  H. Nguyen, M. Z. 
Degefa,  J.  Merino,  T.  V.  Jensen,  H.  Guo,  0.  Gehrke,  D.  E.  M. 
Bondy et al., "Erigrid holistic test description for validating cyber­
physical energy systems,"  Energies,  vol. 12, no. 14, p. 2722, 2019. 
[43]  D.  R. Kuhn,  R.  N.  Kacker,  and  Y.  Lei,  "Practical  combinatorial 
testing," NIST Special Publication, National Institute of Standards 
and Technology, SP  800-142, 2010. 

[44]  S. Marksteiner and Z. Ma, "Approaching the automation of cyber 
security testing of connected vehicles," in Proceedings of the Central 
European  Cybersecurity  Conference  2019,  ser.  CECC  2019.  New 
York, NY, USA: ACM, 2019. 

