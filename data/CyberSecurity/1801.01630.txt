Secure Sensor Design Against Undetected
Inﬁltration: Minimum Impact-Minimum Damage

Muhammed O. Sayin and Tamer Bas¸ar, Life Fellow, IEEE

1

8
1
0
2

n
a
J

5

]

Y
S
.
s
c
[

1
v
0
3
6
1
0
.
1
0
8
1
:
v
i
X
r
a

Abstract—We propose a new defense mechanism against un-
detected inﬁltration into controllers in cyber-physical systems.
To this end, we cautiously design the outputs of the sensors that
monitor the state of the system. Different from the defense mech-
anisms that seek to detect inﬁltration, the proposed approach
seeks to minimize the damage of possible attacks before they
have been detected. Controller of a cyber-physical system could
have been inﬁltrated into by an undetected attacker at any time
of the operation. Disregarding such a possibility and disclosing
system’s state without caution beneﬁts the attacker in his/her
malicious objective. Therefore, secure sensor design can improve
the security of cyber-physical systems further when incorporated
along with other defense mechanisms. We, speciﬁcally, consider
a controlled Gauss-Markov process, where the controller could
have been inﬁltrated into at any time within the system’s oper-
ation. In the sense of game-theoretic hierarchical equilibrium,
we provide a semi-deﬁnite programming based algorithm to
compute the optimal linear secure sensor outputs and analyze
the performance for various scenarios numerically.

Index Terms—Stackelberg games, Stochastic control, Cyber-
physical systems, Security, Advanced persistent threats, Sensor
design, Semi-deﬁnite programming.

I. INTRODUCTION

C YBER-PHYSICAL systems, incorporating both physical

and cyber parts, e.g., process control systems, robotics,
smart grid, and autonomous vehicles, have resulted in new and
distinct challenges for control system design, e.g., speciﬁcally,
security-related challenges due to cyber attacks [1], [2]. Dif-
ferent from external random disturbances, cyber attacks can
be very target speciﬁc and persistent by attacking stealthily
for long term beneﬁts. Recently in 2014, cyber-physical,
e.g., process control, systems in energy and pharmaceutical
industries have been inﬁltrated into by Dragonﬂy Malware,
which intervened in the systems over a long period of time
without being detected [3]. However, isolation from the cyber
networks are also not perfectly sufﬁcient any more. In 2010,
StuxNet Worm caused substantial damage on certain “isolated”
supervisory control and data acquisition (SCADA) systems
[4]. Therefore, developing novel formal security mechanisms
against advanced and persistent threats that can cause sub-
stantial damage without being detected plays a vital role in
the security of cyber-physical systems.

Beyond exploiting uncertainties in the systems, e.g., due
to random disturbances, advanced and persistent attackers can
also seek to deceive the detection mechanisms by manipulating

This research was supported by the U.S. Ofﬁce of Naval Research (ONR)
MURI grant N00014-16-1-2710. The authors are with the Department of Elec-
trical and Computer Engineering, University of Illinois at Urbana-Champaign,
Urbana, IL 61801 USA. E-mail: {sayin2,basar1}@illinois.edu

monitoring signals used by the detectors. For example, in [5],
the authors have introduced false data injection attacks, where
the attackers can inject data into the sensor outputs, in the con-
text of state estimation, and characterized undetectable attacks.
Based on the deceptive attacker model in [5], the existing
studies mainly focus on characterizing the vulnerabilities of
control systems against such undetectable attacks (which can
deceive the detectors) and designing counter measures to be
able to detect them.

In discrete-time linear-quadratic-Gaussian (LQG) systems,
in [6], the authors have introduced replay attacks and proposed
a defense mechanism against such attacks. Replay attacks take
place during the steady state of the system, and the attacker
records and replays the sensor outputs so that the detectors
using those signals cannot detect any anomalies. Note that
the signals are expected to be similar at steady state. As a
defense mechanism, the authors have proposed to inject an
independent signal into the control input to detect such attacks
in the expense of degraded control performance. An optimal
defense strategy with respect to the probability of detection
has been formulated in [7]. Again in LQG systems, in [8], the
same set of authors have introduced integrity attacks, where
the attacker can inject data into sensor outputs and control
inputs, and characterized the reachable set that the attacker
can drive the system to without being detected (via innovation
based failure detectors [9]). They have also provided necessary
conditions for unbounded reachable set, i.e., conditions where
the attacker can destabilize the system.

Within deterministic control scenarios, in [10], the authors
have analyzed zero-dynamics attacks that do not depend
on online information, i.e., open-loop stealthy attacks, and
provided an algorithm to reveal all such attacks by adding
new measurements, similar to [6]. Again within deterministic
control scenarios, in [11], the authors have provided a uni-
ﬁed framework for false data injection and replay attacks,
and formulated the limitations of monitoring-based detection
mechanisms. In [12], the authors have analyzed tolerance of
control systems to false data injection attacks on a subset of
sensors in the deterministic settings and proposed decoding
schemes to estimate the state via corrupted measurements.
They have also introduced a secure control loop that can
enhance the decoding performance and, correspondingly, the
resilience of the system.

The attackers can also have adversarial control objectives.
In [13], [14], the authors have analyzed such attacks, where the
attacker both seeks to be undetected and drive the state of the
system according to his/her adversarial goal by manipulating
inputs together. Recently,
both sensor outputs and control

 
 
 
 
 
 
[15] has analyzed optimal attack strategies to maximize the
quadratic cost of a system with linear Gaussian dynamics
without being detected, where the stealthiness is measured in
terms of the Kullback-Leibler distance between the realized
and the desired state behaviors. In the optimal attack, the
attacker injects independent Gaussian noise having certain
variance into the control input. In another recent study [16],
the authors have proposed linear encoding schemes for sensor
outputs of an LQG system in order to enhance detectability of
false data injection attacks while the coding matrix is assumed
to be unknown by the attackers, which can be mitigated
via time-varying coding matrices. In spite of these extensive
studies, we still have signiﬁcant and yet unexplored problems
about how to enhance security against undetected attacks, i.e.,
impact of attacks before detection.

In this paper, we address primarily the following two
questions: “If we have already designed the sensor outputs,
to what extent would we have secured the system against
undetected inﬁltration into the controllers?” Further, “what
would be the best
linear sensor outputs that can lead to
both minimum impact and minimum damage on the ordinary
operations of the systems?” The damage due to inconspicuous
(undetectable, or difﬁcult to detect) attacks with long term
control objectives is our main concern in this paper. We can
classify such attacks as “advanced and persistent threats”,
since they are advanced by being very target speciﬁc and
persistent by being inconspicuous. Therefore, we propose to be
cautious while disclosing the state information to the controller
due to the possibility of undetected inﬁltration. However,
as a system designer, we should not take precautions as if
the cyber part of the system is compromised due to just a
possibility, since that would impact the ordinary operations of
the system substantially. Combining these seemingly opposing
goals all together, we seek to design sensor outputs cautiously
with minimum impact and minimum damage on the system’s
operations.

To obtain explicit results, we speciﬁcally consider systems
with linear Gaussian dynamics and quadratic control objec-
tives, which have various applications in industry [15] from
manufacturing processes to aerospace control. We consider the
possibility of inﬁltration into the controller of the system by
various attackers at any stage within the time horizon. The at-
tackers have long term control objectives and attack stealthily.
To this end, they include soft constraints on the energy of the
state and the deviation of the constructed control inputs from
system-desired ones, which would have been constructed if the
attacker rather had a friendly objective. Such constraints are,
especially, against the detection mechanisms that take actions
when such deviations exceed certain thresholds.

We note that the sensors could also be inﬁltrated into by
the attackers, which can cancel the proposed approach via a
shortcut to the state if the sensors have access to the state
realizations. To mitigate that, we consider the scenarios where
the sensors do not have access to the actual state realizations.
All the sensor strategies, deﬁning the relation between the state
and the sensor output, are selected beforehand to minimize
the expected loss and ﬁxed (can be time-variant, yet not
controlled) during the operation. Therefore, we can design

2

the sensor outputs off-line, i.e., in advance, which leads to
a hierarchical structure between the sensor and the controller
of the system (even when he/she is adversarial).

Due to the stochastic nature of the problem, i.e., due to
the state noise, any open-loop control strategy of an attacker
could not drive the system in his/her desired path effectively
[17]. Therefore, regardless of whether the controller has an
adversarial objective or not, he/she needs to construct a closed-
loop control input based on the designed sensor outputs while
knowing the relationship between the sensor output and the
state. This implies that the interaction between the sensor and
the controller of the system could be analyzed as a game-
theoretic hierarchical equilibrium, where the sensor leads the
game by announcing his/her strategies beforehand. Therefore,
while designing the sensor outputs, we should consider both
adversarial and friendly control outputs and the possibility of
inﬁltration over the time horizon.

to be linear,

Particularly, we seek to formulate the best linear sensor
strategies for controlled Gauss-Markov processes. We consider
a different time scale for the inﬁltration into the system and
we formulate the optimal sensor strategies in a Bayesian
setting based on given inﬁltration statistics. Note that since
the sensor strategies are set
the problem is
an LQG control problem and correspondingly the optimal,
friendly, control policy is linear in the conditional estimate
of the state given all the sensor outputs. We ﬁrst compute
the best control inputs of friendly and adversarial controllers
for any given linear sensor strategies and any time when they
become in charge of the controller. Corresponding to these
optimal strategies, we provide a semi-deﬁnite programming
(SDP) based algorithm to design the optimal (memoryless)
linear secure sensor strategies. Furthermore, we analyze the
sensitivity of the design against inaccurate perception of the
underlying statistics numerically. We note that in [18], we
have introduced secure sensor design against advanced and
threats in cyber-physical systems, but have not
persistent
completely solved the problem. Here, we consider different
attack models that have soft constraints on the energy of the
state and the deviation of the constructed control inputs from
the system-desired ones, and we consider more comprehensive
scenarios, where the controller can be inﬁltrated into or an
adversarial inﬁltration could be detected at any time within
the time horizon.

In the design of secure cyber-physical systems, each addi-
tional security layer leads to new monetary and computational
costs [19]. In particular, there are fundamental trade-offs in
terms of investment on the security mechanisms and the value
of the protected assets or securing the system and maintaining
the ordinary operations. In order to offer better trade-offs, we
aim to propose a defense mechanism that does not require any
additional online computational load with minimum impact
and minimum damage on the ordinary operations of the
system. To summarize, we can list the main contributions of
this paper as follows:

• We introduce secure sensor design against various in-
conspicuous attackers with control objectives, which can
inﬁltrate into the controller of a cyber-physical system at
any time during the operation.

• Given any linear sensor strategies and the underlying
linear quadratic Gaussian dynamics, we compute the
optimal attack strategies depending on the inﬁltration
time.

• We provide a practical algorithm to compute the optimal
linear memoryless sensor strategies in the sense of game-
theoretic hierarchical equilibrium.

• We also analyze sensitivity of the proposed algorithm

against inaccurate perception of attack statistics.

The paper is organized as follows: In Section II, we
provide the secure sensor design framework. In Section III,
we formulate the associated multi-stage Bayesian Stackelberg
game. In Section IV, we characterize the optimal controller
response strategies for given sensor strategies. We compute
the corresponding optimal sensor strategies in Section V. In
Section VI, we examine the performance of the proposed
scheme under various scenarios numerically. We conclude
the paper in Section VII with several remarks and possible
research directions.
Notations: For an ordered set of parameters, e.g., x1, · · · , xn,
we deﬁne x[k,l] := xk, · · · , xl, where 1 ≤ k ≤ l ≤ n. N(0, .)
denotes the multivariate Gaussian distribution with zero mean
and designated covariance. We denote random variables by
bold lower case letters, e.g., xxx. For a random variable xxx, ˆxˆxˆx is
another random variable corresponding to its posterior belief
conditioned on certain other random variables that will be
apparent from the context. For a vector x and a matrix A, x(cid:48) and
A(cid:48) denote their transposes, and (cid:107)x(cid:107) denotes the Euclidean (L2)
norm of the vector x. For a matrix A, tr{A} denotes its trace.
We denote the identity and zero matrices with the associated
dimensions by I and O, respectively, while 1 (or 0) denotes a
vector whose entries are all 1 (or 0). For positive semi-deﬁnite
matrices A and B, A (cid:23) B means that A − B is also a positive
semi-deﬁnite matrix. A ⊗ B denotes the Kronecker product of
the matrices A and B.

II. PROBLEM FORMULATION

Consider a controlled stochastic system described by the

following equations:

xxxk+1 = Axxxk + Buuuk +vvvk,
yyyk = Duuuk +wwwk,

(1)

(2)

for k = 1, 2, . . . , n, where1 A ∈ Rm×m, B ∈ Rm×r, D ∈ Rr×r,
and xxxk ∼ N(0, Σk), k = 1, . . . , n. The additive state and control
input noise sequences {vvvk} and {wwwk} are white Gaussian
vector processes, i.e., vvvk ∼ N(0, Σv) and wwwk ∼ N(0, Σw); and are
independent of the initial state xxx1 and of each other. We assume
that the matrix A is non-singular, and the auto-covariance
matrices Σ1 and Σv are positive deﬁnite while Σw is positive
semi-deﬁnite. The closed loop control vector uuuk ∈ Rr is given
by

uuuk = γk(sss[1,k]),

(3)

1Even though we consider time invariant matrices A, B, and D for notational
simplicity, the provided results could be extended to time-variant cases rather
routinely. Furthermore, we consider all the random parameters to have zero
mean; however, the derivations can be extended to non-zero mean case in a
straight-forward way.

3

Fig. 1: Cyber physical system including a sensor and a
controller.

where γk(·) can be any Borel measurable function from Rmk
to Rr. The sensor output sssk ∈ Rm is given by

sssk = ηk(xxxk),
where ηk(·) can be any linear function from Rm to Rm. And
yyyk ∈ Rr, given by (2), denotes the noisy observation of the
control input uuuk.

(4)

We have two separate agents: Sensor (S) and Controller
(C). At each stage k = 1, . . . , n, the agents construct sssk and
uuuk according to their own objectives. In particular, S chooses
ηk(·) from the strategy space ϒ, which, for each k, is the
set of all linear functions from Rm to Rm, i.e., ηk ∈ ϒ and
sssk = ηk(xxxk). This implies that for each ηk ∈ ϒ, there exists a
matrix Lk ∈ Rm×m such that

sssk = L (cid:48)
almost surely on Rm. C chooses γk(·) from the strategy space
Γk, which is the set of all Borel measurable functions from
Rmk to Rr, i.e., γk ∈ Γk and uuuk = γk(sss[1,k]).

kxxxk,

(5)

Sensor and Controller Objective. As in a stochastic control
scenario [17], S and C can have a common ﬁnite horizon2
quadratic cost function:

,

(cid:41)

(6)

+ (cid:107)uuuk(cid:107)2
RF

(cid:107)xxxk+1(cid:107)2
QF

JF(η[1,n]; γ[1,n]) = E

(cid:40) n
∑
k=1
where3 QF ∈ Rm×m is positive semi-deﬁnite and RF ∈ Rr×r
is positive deﬁnite. Note that uuuk = γk(sss[1,k]) while sssk = ηk(xxxk)
almost surely. Correspondingly, S could disclose the state xxxk
directly so that C could drive the state in their commonly
desired path [17], [20]. However, in a cyber physical system,
the system is vulnerable to adversarial inﬁltration attacks that
seek to drive the state of the system away from the system’s
desired target as seen in Fig. 1. We call such attacks “advanced
persistent threats”, which are advanced by being very target
speciﬁc, i.e., the attacker knows the underlying state recursion,

2E.g., horizon length is n.
3For notational simplicity, we consider time-invariant QF and RF. However,
the provided results could be extended to time-variant cases rather routinely.

Sensor State (Sensor Input) Sensor Output Control Input Controller Friend Adversary ? Infiltration Attacks Infiltration Detector Noisy Observation of Control Input Passive Defense Active Defense and persistent by avoiding inﬁltration detection. Therefore, S,
i.e., the sensor designer, should anticipate the likelihood of
adversarial inﬁltration into C, i.e., the possibility that C can
be an adversary, and select ηk ∈ ϒ accordingly.
Remark 1. We note that the sensor output sssk only depends
on the current state xxxk, i.e., ηk is memoryless. Otherwise, S
would need to have access to the state information in order to
store and to be able to use them in the future stages. However,
similar to C, S can also be inﬁltrated into by the attackers,
which would neutralize S’s effort to design sensor outputs
strategically via a shortcut to the state information.
Remark 2. In control system design, sensors are designed
and implemented in advance, and system engineers design the
controllers knowing the relation between the sensor output
and the underlying state. Correspondingly, an attacker that
has inﬁltrated into the system can be aware of how the sensor
outputs have been constructed and can design his/her attack
accordingly. Therefore, there exists a hierarchy between S and
C such that C can have access to S’s strategies.

Inﬁltration Detection. We note that if the control inputs
could have been monitored perfectly, then any deviation of
the control input from the system-desired one could have
been detected instantly since both sensor outputs and control
inputs will be accessible. Therefore, in this paper, we address
the scenarios where the control input cannot be monitored
perfectly. As an example of such scenarios, the inﬁltration
detection mechanism can have access to noisy control input
observation yyyk ∈ Rr and the state xxxk as seen in Fig. 1. In the
scope of this work, we will not consider the details of how
the inﬁltration detector operates except that the advanced and
persistent attackers are aware of the presence of an inﬁltration
detector that can have access to the state (correspondingly the
sensor outputs) and noisy versions of the control inputs.

Inconspicuous Inﬁltration Attacks into C. As seen in Fig.
1, C is under inﬁltration attacks by the (advanced) attack-
ers A1, . . . , At over the time horizon k = 1, . . . , n and such
attacks may be successful or not in inﬁltrating into C. As
mentioned earlier, being advance refers to being target speciﬁc
with knowledge about underlying system dynamics while also
avoiding detection mechanisms by attacking inconspicuously.
Therefore, C can be a friend or an adversary within the
time horizon while S may not know C’s type surely until an
inﬁltration detection takes place, which may be less likely due
to inconspicuousness of the attacks. C observes sss[1,k], knows
S’s strategies η[1,k] due to a hierarchy between the agents,
and, via a strategy γk ∈ Γk, can construct a closed-loop control
input uuuk, yet the state and the control input can be monitored
by the inﬁltration detector. Therefore, as an attack model, we
consider the situation where the attacker Ai, i = 1, . . . ,t, selects
γAi,k ∈ Γk, k = κ, . . . , n, where κ denotes the inﬁltration time,
to minimize the cost function:

JAi,κ (η[1,n]; ·, γAi,[κ,n]) = E

(cid:107)xxxAi,k+1 − zi(cid:107)2

QAi

(cid:26) n
∑
k=κ
+λi(cid:107)xxxAi,k+1(cid:107)2
QF

+ (cid:107)uuuAi,k −uuuF,k(cid:107)2
RAi

(cid:27)

,

(7)

where λi ≥ 0, “·” as an argument of the cost function (7)
refers to the C’s strategies γ1, . . . , γκ−1, which are not selected

4

Fig. 2: Possible transitions among the agents F, A1, and A2 to
be in charge of C within the time horizon.

by Ai; and zi ∈ Rm is the desired state that the adversary seeks
to drive the system to. The matrices4 QAi ∈ Rm×m are positive
semi-deﬁnite, and RAi ∈ Rr×r are positive deﬁnite. Here, xxxAi,k
denotes the state driven by the adversarial control input uuuAi,k;
while uuuF,k denotes the control input that would have been
constructed if C was a friend.

Particularly, the last two terms in (7) are soft constraints
to avoid inﬁltration detection by being close to the expected
behavior of the system, e.g., small energy of the state xxxAi,k+1,
and small deviations of uuuAi,k from uuuF,k. Note that λi ≥ 0 can
also be zero. We also note that deviation of the state xxxAi,k+1
from the system-desired xxxF,k+1 is equivalent to

xxxAi,k+1 −xxxF,k+1 = B(uuuAi,k −uuuF,k).
Furthermore, deviation of the observed control input5 yyyAi,k
from the system desired yyyF,k is equivalent to

yyyAi,k+1 −yyyF,k+1 = D(uuuAi,k −uuuF,k).

Therefore, via RAi ∈ Rr×r,
the attacker can take precau-
tions against the thresholding-based detection mechanisms that
check the deviation of the state or the control input from the
system-desired ones. We also note that uuuF would have been
constructed by C if there were no inﬁltration. Correspondingly,
the attacker can construct uuuAi,k by adding another signal on
top of uuuF,k such that uuuAi,k = uuuF,k + δuuu Ai,k as in [13], [14].
Therefore, the last term in (7) also corresponds to a soft energy
constraint on δuuu Ai,k.

In the following section, we provide a game theoretical

formulation to analyze the interaction between the agents.

III. HIERARCHICAL EQUILIBRIUM FORMULATION

Cyber-physical systems are vulnerable to inﬁltration attacks.
Within the time horizon various attackers can inﬁltrate into the
system as well as defense mechanism can detect inﬁltration
and take appropriate actions accordingly. As an example,
Fig. 2 demonstrates possible transitions among the agents,
F, A1, and A2, to be in charge of C. When an attacker,
e.g., A1, inﬁltrates into the system, A1 becomes in charge
of C and can construct the control input according to his/her
adversarial objective. Furthermore, when the attackers are not

4For notational simplicity, we consider time-invariant λi, QAi , RAi .
5By depending on uuuAi,k, observed control inputs also depend on Ai’s ac-
tions. Therefore, we show this dependence explicitly by yyyAi,k := yyyk. Similarly,
the system-desired observation, i.e., F would have been in charge of C, is
denoted by yyyF,k.

Infiltration by  Infiltration by       into the system already under attack Exfiltration of    from the system e.g., due to detection communicating with each other, an attacker, e.g., A2, may not
know whether the system is already under attack or not and
correspondingly may inﬁltrate into the system while A1 is still
in charge of C. Furthermore, there can be inﬁltration detection
due to the active defense mechanisms monitoring the control
and sensor inputs as seen in Fig. 1.
that when inﬁltration has been
Remark 3. We point out
detected, the system could prefer to use different sensor outputs
since the uncertainty about C has been removed. As an
example, if the attacker cannot be forced to exﬁltrate from
the system immediately, the system can prefer to use sensor
outputs designed speciﬁcally against the adversarial objectives
of the attacker. Or if F becomes in charge of C after detection,
as long as the system ensures F is in charge, direct state
disclosure can be preferred. Therefore, we consider scenarios
where the system switches his/her operation mode and uses
different sensor outputs, once an inﬁltration has been detected.
In order to model uncertainty of the transitions between
the agents to be in charge of C explicitly, we consider a
jump process {θθθ j ∈ Θ}, where Θ := {F, A1, . . . , At , T} and
transitions can occur on a different time scale called transition
time, e.g., κ ∈ {δ , 2δ , . . . , Nδ }, where δ ∈ Z and N := (cid:100)n/δ (cid:101).
The state T denotes the switch to a different operation mode,
e.g., due to inﬁltration detection. Therefore, when the process
jumps to the state T, the horizon practically terminates for S.
Correspondingly, for a given sequence of the jump process:
θ1, . . . , θN, if there is a jump to T, we let NT denote the index
of the last state before the jump, and otherwise NT = N +1, and
let ¯h(θ[1,N]) be an ordered set including the non-terminating
transition times, and 1 and min{NT δ , n + 1}. As an illustrative
example, let n = 100 and δ = 30 and consider the situation
where F is in charge of C initially while A1 inﬁltrates into
C at k = 60 and becomes in charge until detection at k = 90.
Therefore, we have ¯h(θ[1,N]) = {1, 60, 90}, which implies that
while θ1 = θ2 = F is in charge during the interval [1, 60),
θ3 = A1 becomes in charge during the interval [60, 90) and
the system switches its mode at k = 90.

The underlying state recursion is common knowledge to
both S and C (even if C can be an adversary). The type of C
and, if C is an adversary, his/her objective are not known by S.
However, S knows the statistics of the jump process {θθθ j}. As
also noted in Remark 1, there is also a hierarchy [21] between
the agents in the announcement of the strategies such that S
leads the game by announcing and sticking to his/her strategies
in advance, i.e., C knows η[1,n] in advance. Therefore, we can
model such a scheme as a multi-stage Bayesian Stackelberg
game, in which S is the leader.
Remark 4. When the inﬁltration detector has access to the
sensor outputs sssk but not the states xxxk, even though the attacker
can also inject false data into the sensor outputs in order to
avoid detection as in integrity attacks, e.g., [8], [13], [14],
due to the “stochastic” state recursion (1), the attacker still
needs the actual sensor outputs, which are designed by the
system designer in advance. Therefore, secure sensor design
framework can also play a crucial role for the security of the
systems against integrity attacks.

The agents S and C aim to minimize their cost functions by
choosing the strategies η[1,n] and γ[1,n] while each strategy im-

5

plicitly depends on the other. Due to the hierarchy, C’s strate-
gies γθ ,k ∈ Γk, θ ∈ {F, A1, . . . , At }, depending on his/her type,
can also depend on S’s strategies η[1,k] and when the agent
becomes in charge of C. In order to show these dependences
explicitly, henceforth, we denote C’s strategies by γ (κ)
θ ,k (η[1,k]),
which implies γ (κ)
θ ,k (η[1,k])(sss[1,k]) := γθ ,k(sss[1,k]). Then, for given
n
S strategies η[1,n], we let Πθ ,κ (η[1,n]) ⊂ ⨉
k=κ Γk be the reaction
set of the agent θ ∈ {F, A1, . . . , At } who becomes in charge of
C at κ. And these reaction sets are given by:

(cid:16)
η[1,n]; ·, γ (κ)

(cid:17)
θ ,[κ,n](η[1,n])

,

Jθ ,κ

Πθ ,κ (η[1,n]) := argmin
(κ)
θ ,k ∈Γk
γ
k=κ,...,n
(cid:110)
γ (κ)
θ ,κ (η[1,κ]), . . . , γ (κ)

(cid:111)

θ ,n (η[1,n])

θ ,[κ,n](η[1,n]) :=

where γ (κ)
. Due to
the positive deﬁniteness assumptions on Rθ , in the following
section, we will show that for each θ ∈ {F, A1, . . . , At }, the
corresponding reaction set Πθ ,κ is an equivalence class such
that all γ (κ)
θ ,[κ,n] ∈ Πθ ,κ lead to the same random variable uuu∗
θ ,k
almost everywhere on Rr. Therefore, the pair of strategies:

(cid:104)
η ∗
[1,n];

(cid:16)
γ (κ)∗
θ ,[κ,n], θ ∈ Θ, κ ∈ ¯h

(cid:17)(cid:105)

(8)

attains the Stackelberg equilibrium provided that

η ∗
[1,n] = argmin
ηk∈ϒ,
k=1,...,n

E






Jκ+
F,κ

∑
κ∈¯h(θθθ [1,N])

(cid:16)
η[1,n]; ·, γ (κ)∗

(cid:17)
θθθ j,[κ,n](η[1,n])






Jθ ,κ

(cid:16)
η[1,n]; ·, γ (κ)

(cid:17)
θ ,[κ,n](η[1,n])

,

(9a)

(9b)

γ (κ)∗
θ ,[κ,n](η[1,n]) = argmin
(κ)
θ ,k ∈Γk,
γ
k=κ,...,n

Jκ+
F,κ

−JF,κ+

(cid:16)
η[1,n], ; ·, γ (κ)∗

(cid:17)
θθθ j,[κ,n](η[1,n])

(cid:16)
η[1,n]; ·, γ (κ)∗
:= JF,κ
(cid:16)
η[1,n]; ·, γ (κ)∗

where the expectation is taken over {θθθ j}, κ+ is the next
transition time after κ in ¯h(θ[1,N]), θθθ j refers to the agent that
will be in charge of C in the interval [κ, κ+), and we deﬁne
(cid:17)
θθθ j,[κ,n](η[1,n])
(cid:17)
θθθ j,[κ+,n](η[1,n])
as the cost in impact since the agent θθθ j is in charge of C during
[κ, κ+) even though he/she has selected his/her strategies as
if he/she will be in charge until the end of the horizon.
Remark 5. Even though S has access to the statistics of the
jump process, computation of the expectation (9a) over all
possible sequences {θθθ 1, . . . ,θθθ N} is computationally expensive
since there are O(tN) sequences, where t is the number of
attackers. Note that most of these sequences have relatively
low probability. As an example, multiple successful attacks by
different attackers consecutively can be considered as a rare
event. Therefore, we let Ω ⊂ ΘN be the set of selected, typical,
sequences. For example, Ω can include the sequences such that
there will be at most one inﬁltration (by any of the attackers)
and the inﬁltration may or may not be detected until the end
of the time horizon. Then, S will consider O(tN2) sequences.
Note that each sequence corresponds to a different inﬁltration
scenario. Also let µ(θθθ [1,N]) denote the normalized measure of
a sequence θθθ [1,N] ∈ Ω, i.e., ∑θθθ [1,N]∈Ω µ(θθθ [1,N]) = 1.

Based on Remark 5, the pair (8) leads to the Stackelberg

equilibrium provided that

η ∗
[1,n] = argmin
ηk∈ϒ,
k=1,...,n

∑
θθθ [1,N]∈Ω

µ(θθθ [1,N])∑

κ∈¯h(θθθ [1,N])

Jκ+
F,κ

(cid:16)
η[1,n]; ·, γ (κ)∗

(cid:17)
θθθ j,[κ,n](η[1,n])

6

The sequence { ˇQF,k} is deﬁned through the following discrete-
time Riccati equation:
ˇQF,k = QF + A(cid:48) (cid:16)
ˇQF,n+1 = QF.

ˇQF,k+1 − ˇQF,k+1B∆−1

F,kB(cid:48) ˇQF,k+1

(13b)

(13a)

A,

(cid:17)

Jθ ,κ

(cid:16)
η[1,n]; ·, γ (κ)

(cid:17)
θ ,[κ,n](η[1,n])

,

(10a)

(10b)

Then, through a change of variables [23], friendly type C’s
objective (11) can be written as

γ (κ)∗
θ ,[κ,n](η[1,n]) = argmin
(κ)
θ ,k ∈Γk,
γ
k=κ,...,n

Remark 6. We note that any brute force approach, trying
to solve the optimization problem (10a) numerically (since it
is a ﬁnite dimensional problem due to linear memoryless S
strategies), e.g., via particle swarm optimization [22], needs
to ﬁnd n matrices with m × m dimensions, corresponding to
nm2 parameters, where m is the dimension of the state and n
is the number of stages (i.e., time horizon). In particular, we
would be searching for a point in R(nm2) dimensional space, in
addition to the computational load to compute the cost (10a)
associated with those points. Furthermore, the result of such
a numerical approach would only imply a local optimum, and
not the global one.

In the following sections, we analyze the equilibrium

achieving strategies.

IV. OPTIMAL FOLLOWER (CONTROLLER) REACTIONS

For any given S strategies, η[1,n], we aim to compute the
corresponding reactions γ (κ)
θ ,k (η[1,k]) for θ ∈ {F, A1, . . . , At },
k = κ, . . . , n, and κ ∈ {δ , . . . , Nδ }. To this end, we ﬁrst provide
friendly C reactions for given sensor strategies and then, we
compute adversarial C reactions correspondingly.

A. Optimal Agent-F Reaction

Based on Remark 3, secure sensor designer is only inter-
ested in the reaction of F when he/she is in charge of C
starting from time k = 1. We note that given linear memoryless
S strategies, the problem is an LQG control problem for F
[17]. In the following, for completeness, we will derive the
corresponding optimal control inputs.

In order to facilitate the subsequent analysis, we can rewrite
the state equations (1)-(4) and the cost function (6) without al-
tering the optimization problem. Particularly, after completing
the squares [17], [23], the friendly objective (6) is equivalent
to:

E(cid:107)uuuF,k + KF,kxxxF,k(cid:107)2

∆F,k

+ GF,

(11)

n
∑
k=1

min
γF,k∈Γk
k=1,...,n

where6

∆F,k = B(cid:48) ˇQF,k+1B + RF,
F,kB(cid:48) ˇQF,k+1A,
KF,k = ∆−1

GF = tr{Σ1( ˇQF,1 − QF)} +

n
∑
k=1

tr{Σv ˇQF,k+1}.

(12a)

(12b)

(12c)

n
∑
k=1

min
γF,k∈Γk
k=1,...,n

E(cid:107)uuuo

F,k + KF,kxxxo

F,k(cid:107)2

∆F,k

+ GF

(14)

subject to (12)-(13) and for k = 1, · · · , n,

k +vvvk and xxxo

k+1 = Axxxo
xxxo
1 = xxx1,
F,k = uuuF,k + KF,kBuuuF,k−1 + · · · + KF,kAk−2BuuuF,1,
uuuo

(15a)

(15b)

Note that, now, the process {xxxo
k} is independent of how the
control inputs uuuF,k (and uuuo
F,k) are constructed while the sensor
outputs by depending on the current state xxxF,k also depend on
the previous control inputs.

Applying the Principle of Optimality to (14), in view of
(5), leads to the result that the last stage optimal transformed
control input is given by

F,n = −KF,nE{xxxo
uuuo∗
= −KF,nE{xxxo

n|sss[1,n]}
n|L (cid:48)

1xxxF,1, . . . , L (cid:48)

nxxxF,n}.

However, by (15a), we have

L (cid:48)

kxxxF,k = L (cid:48)

kxxxo

k BuuuF,k−1 + . . . + L (cid:48)

k Ak−2BuuuF,1
,
(cid:125)

(cid:123)(cid:122)

k + L (cid:48)
(cid:124)

(16)

(17)

where the underbraced term is σ -sss[1,k−1] measurable, for k =
1, . . . , n. Therefore, (16) is equivalent to

F,n = −KF,nE{xxxo
uuuo∗

n|L (cid:48)

1xxxo

1, . . . , L (cid:48)

nxxxo
n}

(18)

and does not depend on previous control inputs. Therefore, by
induction, we can conclude that the problem entails classical
information and for k = 1, . . . , n,
transformed
control input is given by

the optimal

F,k = −KF,kE{xxxo
uuuo∗

k|sss[1,k]}

(19)

almost everywhere on Rr, for k = 1, . . . , n, which would also
imply the uniqueness of the best F reactions and singleton
reaction set ΠF,1. Then, by (15b) and (19), the optimal control
inputs are given by



uuu∗





F,n...



uuu∗
F,1
(cid:124) (cid:123)(cid:122) (cid:125)
=: uuu∗
F



= −





Ir KF,nB KF,nAB ... KF,nAn−2B
Ir KF,n−1B ··· KF,n−1An−3B
··· KF,n−2An−4B
...

Ir

(cid:124)

Ir

(cid:123)(cid:122)
=: ΦF

−1

(cid:34) KF,n



(cid:124)
(cid:125)

...

KF,1

(cid:123)(cid:122)
=: KF

where ˆxˆxˆxo

k := E{xxxo

k|sss[1,k]}, and equivalently:

=: ˆxˆxˆxo
(cid:122) (cid:125)(cid:124) (cid:123)(cid:34) ˆxˆxˆxo
(cid:35)
(cid:35)
n...
,

ˆxˆxˆxo
1

(cid:125)

(20)

(21)

6Note that ∆F,k is invertible since RF (cid:31) O and QF (cid:23) O.

uuu∗
F = −Φ−1

F KF ˆxˆxˆxo.

B. Optimal Adversarial Reaction (Attack)

Here, we compute the optimal attack strategies with control
objective (7). Different from Subsection IV-A, now, we also
consider the scenarios where Ai can inﬁltrate into C at k > 1.
That would imply that an attacker A j can inﬁltrate into
C that has already been inﬁltrated into by another attacker
Ai. Furthermore, the attacker may not know the underlying
statistics of the jump process corresponding to the transitions
among the agents that can be in charge of C. Correspondingly,
the attacker may not know how the state has been driven until
he/she has inﬁltrated into. However, as also noted in Remark
4, consecutive succesful inﬁltration by different attackers can
be considered as a rare event relative to the inﬁltration of the
attacker into C while F was in charge. Therefore, as an attack
model, we consider the scenarios where the attackers can
assume that F was in charge of C before they have inﬁltrated
into.

Next, we aim to rewrite the state equations and the cost
functions as in (11) and (14) for the minimization of the
adversarial objectives (7). For k = κ, . . . , n, let

δuuu Ai,k := uuuAi,k −uuuF,k.

(22)

Then, instead of (1), consider the following recursion:






xxxAi,k+1
uuuF
zi

 =

(cid:20) A
O

(cid:124)



×



(cid:124)

xxxAi,k
uuuF
zi
(cid:123)(cid:122)
= ¯x¯x¯xAi,k

Om×(n−k)r B Om×(k−1)r Om
Im+nr
(cid:123)(cid:122)
=: ¯Ak

(cid:21)

(cid:125)



+



(cid:125)

(cid:20)

(cid:124)

B
O(m+nr)×r
(cid:123)(cid:122)
=: ¯B

(cid:21)

(cid:125)

δuuu Ai,k +

vvvk,

(cid:21)

(cid:20) Im
O
(cid:124) (cid:123)(cid:122) (cid:125)
=: E

which can be written in compact form as

¯x¯x¯xAi,k+1 = ¯Ak ¯x¯x¯xAi,k + ¯B δuuu Ai,k + Evvvk.

Correspondingly, the objective (7) can be rewritten as

n
∑
k=κ

E

(cid:110)
(cid:107) ¯x¯x¯xAi,k+1(cid:107)2
¯QAi

+ (cid:107) δuuu Ai,k(cid:107)2
RAi

(cid:111)

,

where

¯QAi :=

(cid:20) QAi

+λiQF O −QAi
O
−QAi

O O
O QAi

(cid:21)

(cid:23) O.

(23)

(24)

(25)

We point out the resemblance between (24) and (6).

As in (11), we can rewrite the cost function (24) without al-
tering the optimization problem. After completing the squares,
the adversarial objective (24) is equivalent to:

E(cid:107) δuuu Ai,k + KAi,k ¯x¯x¯xAi,k(cid:107)2

∆Ai,k

+ GAi,κ ,

(26)

n
∑
k=κ

min
(κ)
Ai,k∈Γk
γ
k=κ,...,n

where

∆Ai,k = ¯B(cid:48) ˇQAi,k+1 ¯B + RAi,
¯B(cid:48) ˇQAi,k+1 ¯Ak,
KAi,k = ∆−1
Ai,k

GAi,κ = tr{ ¯ΣAi,κ ( ˇQAi,κ − ¯QAi)} +

(27a)

(27b)

n
∑
k=κ

tr{ ¯Σv ˇQAi,k+1},

(27c)

7

and ¯ΣAi,κ := E{ ¯x¯x¯xAi,κ ¯x¯x¯x(cid:48)
Ai,κ }, ¯Σv := EΣvE(cid:48). Note that ¯x¯x¯xAi,κ (cor-
respondingly ¯ΣAi,κ ) does not depend on Ai’s strategies γ (κ)
Ai,[κ,n],
and instead depends on γF,[1,κ−1]. The sequence { ˇQAi,k} is
deﬁned through the following discrete-time Riccati equation:

ˇQAi,k = ¯QAi + ¯A(cid:48)
k
ˇQAi,n+1 = ¯QAi.

(cid:16)
ˇQAi,k+1 − ˇQAi,k+1 ¯B∆−1
Ai,k

¯B(cid:48) ˇQAi,k+1

(cid:17)
¯Ak,

(28)

We emphasize that KAi,k, ∆Ai,k, and ˇQAi,k do not depend on
the inﬁltration time κ.

And corresponding to (14), the adversarial objective (26)

can be written as

n
∑
k=κ

min
γAi,k∈Γk
k=κ,...,n

E(cid:107) δuuu o

Ai,k + KAi,k ¯x¯x¯xo

Ai,k(cid:107)2

∆Ai,k

+ GAi,κ

(29)

Ai,1 = ¯x¯x¯xAi,1,

Ai,k + Evvvk and ¯x¯x¯xo

subject to (27)-(28) and for k = 1, . . . , n,
¯x¯x¯xo
Ai,k+1 = ¯Ak ¯x¯x¯xo
(30a)
δuuu o
Ai,k = δuuu Ai,k + KAi,k ¯B δuuu Ai,k−1 + KAi,k ¯Ak−1 ¯B δuuu Ai,k−2 + . . .
+ KAi,k ¯Ak−1 · · · ¯Aκ+1 ¯B δuuu Ai,κ + KAi,k ¯Ak−1 · · · ¯Aκ ¯B δuuu κ−1 + . . .
+ KAi,k ¯Ak−1 · · · ¯A2 ¯B δuuu 1.
(30b)
Note that in (29), GAi,κ does not depend on the adversary’s
optimization arguments even though it depends on uuuF due to
¯ΣAi,κ in (27c). However, F does not consider the impact of uuuF
on GAi,κ while selecting γF,[1,n] since (29) is the cost function
of Ai, and not of F. Note also that if F is in charge of C before
Ai has inﬁltrated into C, for k = 1, . . . , κ − 1, we have

δuuu k = 0.

(31)

Even though the process { ¯x¯x¯xo
Ai,k} is independent of how the
control inputs δuuu Ai,k (and δuuu o
Ai,k) are constructed, the sensor
outputs sssk, k = κ, . . . , n, depend on the taken actions, i.e., uuuAi,k.

Similar to (16), the Principle of Optimality yields

δuuu o∗

Ai,n = −KAi,nE{ ¯x¯x¯xo

Ai,k|L (cid:48)

1xxxAi,1, . . . , L (cid:48)

nxxxAi,n}.

(32)

k Ak−κ−1BuuuAi,κ
(cid:125)
(33)

Then, irrespective of (30a) and (23), we have
k BuuuAi,k−1 + . . . + L (cid:48)
n + L (cid:48)
(cid:124)

kxxxAi,k = L (cid:48)

kxxxo

L (cid:48)

(cid:123)(cid:122)

k Ak−2BuuuF,1
+ . . . + L (cid:48)
,
(cid:125)
(cid:123)(cid:122)
(cid:124)
k evolves according to (15a) and the underbraced terms

where xxxo
are σ -sss[1,k−1] measurable. Therefore, (32) is equivalent to

δuuu o∗

Ai,n = −KAi,nE{ ¯x¯x¯xo

Ai,k|L (cid:48)

1xxxo

1, . . . , L (cid:48)

nxxxo
n}

(34)

and does not depend on the previous control inputs uuuAi,[κ,n−1].
Remark 7. We emphasize that if Ai does not know which
agent was in charge of C before κ, then the relation between
uuu[1,κ−1] and the corresponding sensor outputs sss[1,κ−1] would
not be known by Ai and correspondingly (34) would not be
equivalent of (32). Furthermore, the equivalence does also
not hold if another attacker had already inﬁltrated when Ai
inﬁltrates since Ai has assumed that F was in charge before the
inﬁltration. Note also that S is only interested in the scenarios
there is no successful consecutive
where θθθ [1,N] ∈ Ω,
inﬁltration by different attackers. A detailed analysis of the

i.e.,

consecutive successful
information is left as future work.

inﬁltration and attacks with partial

By induction, we can conclude that the optimal transformed

l < k, follows since
l |sss[1,l]} is σ -sss[1,k] measurable if l < k. Therefore, (41) can

if l ≥ k; and the second case,
E{xxxo
be written as

i.e.,

8

control inputs of Ai are given by
δuuu o∗

Ai,k = −KAi,kE{ ¯x¯x¯xo

Ai,k|sss[1,k]},

(35)

for k = κ, . . . , n, almost everywhere on Rr. This implies the
uniqueness of the best Ai reactions and singleton reaction set
ΠAi,κ . Then, by (30b), we have







(cid:125)

=






(cid:124)







(cid:124)

δuuu o
δuuu o

Ai,n
Ai,n−1

...
δuuu o
Ai,κ
(cid:123)(cid:122)
=: δuuu (κ)o

Ai

Ir KAi,n ¯B ··· KAi,n ¯An−1... ¯Aκ+1 ¯B
··· KAi,n−1 ¯An−2... ¯Aκ+1 ¯B
...
...

Ir

Ir

(cid:123)(cid:122)

∈R(n−κ+1)r×(n−κ+1)r

=: Φ

(κ)
Ai











(cid:125)

(cid:124)






(cid:125)

δuuu Ai,n
δuuu Ai,n−1
...
δuuu Ai,κ
(cid:123)(cid:122)
=: δuuu (κ)
Ai

which can also be written as
δuuu (κ)o
Ai

= Φ(κ)
Ai

δuuu (κ)
Ai

while (35) leads to

δuuu (κ)o∗
Ai

= −

(cid:34) KAi,n

...

(cid:124)

KAi,κ

(cid:123)(cid:122)
=: K(κ)
Ai



(cid:35)



(cid:125)

E{ ¯x¯x¯xo

E{ ¯x¯x¯xo

Ai,n|sss[1,n]}
...
Ai,κ |sss[1,κ]}

(36)

(37)

(38)



 .

Next, we seek to compute E{ ¯x¯x¯xo

Ai,k|sss[1,k]} in (38) in terms of

ˆxˆxˆxo. To this end, let us take a closer look at (30a):









ˇxˇxˇxk+1
uuuF
zi

 =

(cid:20) A
O

· · · B · · · Om
Im+nr

(cid:21)



ˇxˇxˇxk
uuuF
zi

(cid:20) Im
O

(cid:21)

vvvk,

+

where we introduce ˇxˇxˇxk, which is given by

ˇxˇxˇxk = xxxo

k + BuuuF,k−1 + ABuuuF,k−2 + · · · + Ak−2BuuuF,1.

(39)

E{uuuF|sss[1,k]} = −Φ−1

F KF










(cid:124)

O

O

An−k
...
A
Im
O

(cid:123)(cid:122)
=: Lk










(cid:125)

O

I(k−1)m

ˆxˆxˆxo,

(42)

where the middle block is the kth block column from the right.
Hence, we can rewrite (40) as

E{ ¯x¯x¯xo

Ai,k|sss[1,k]} =

F KFLk

(cid:34) Ek−ΨkΦ−1
−Φ−1
F KFLk
Om
(cid:123)(cid:122)
=: Fk

(cid:124)

(cid:35)

(cid:125)

ˆxˆxˆxo +

(cid:21)
,

(cid:20) Om×1
Onr×1
zi
(cid:124) (cid:123)(cid:122) (cid:125)
=: zi

(43)

where Ek := [ Om×(n−k)m Im Om×(k−1)m ] is the indicator matrix such
that E{xxxo
k|sss[1,k]} = Ek ˆxˆxˆxo, k = 1, . . . , n. Then, by (35), (38), and
(43), we have

δuuu (κ)o∗
Ai

= −K(κ)
Ai

ˆxˆxˆxo + 1n−κ+1 ⊗ zi









.

(44)









(cid:35)

(cid:34) Fn...

Fκ
(cid:124) (cid:123)(cid:122) (cid:125)
=: F (κ)

Therefore, (44) and (37) lead to
(cid:16)
F (κ) ˆxˆxˆxo + 1n−κ+1 ⊗ zi

= −(Φ(κ)
Ai

)−1K(κ)
Ai

δuuu (κ)∗
Ai

(cid:17)
.

Note that by (31) and (21), we obtain

uuu(κ)∗
Ai

= −

(cid:16)
(Φ(κ)
)−1K(κ)
Ai
Ai
)−1K(κ)
− (Φ(κ)
Ai
Ai

F (κ) + [ I(n−κ+1)r O(n−κ+1)r×(κ−1)r ] Φ−1
(cid:0)1n−κ+1 ⊗ zi

(cid:1).

(45)

F KF

(cid:17)

ˆxˆxˆxo



 .

In the following theorem, we recap these results.

Theorem 1. Given S’s strategies η[1,n], C’s optimal reactions
γ (κ)
θ ,k (η[1,k]), where θ ∈ {F, A1, . . . , At }, are given by (21) or
(45) depending on whether C is a friend or an adversary,
respectively. These reaction strategies are unique.

In the following section, we formulate S’s optimal strategies.

Then, we have



 =










ˇxˇxˇxn

ˇxˇxˇxn−1...

ˇxˇxˇx1

xxxo
n
xxxo

n−1...

xxxo
1




 +





...

O B AB ··· An−2B
O O B ··· An−3B









uuuF,n

uuuF,n−1...

uuuF,1

...

O

(cid:125)

(cid:124)

O O ···

···
(cid:123)(cid:122)
=: Ψ
n · · · Ψ(cid:48)
Ai,k|sss[1,k]} can be written as
(cid:21)
k |sss[1,k]}+ΨkE{uuuF|sss[1,k]}

(cid:20) E{xxxo

Let Ψ be partitioned as Ψ = [Ψ(cid:48)
ΨkuuuF. Therefore, E{ ¯x¯x¯xo

1](cid:48) such that ˇxˇxˇxk = xxxo

k +

E{ ¯x¯x¯xo

Ai,k|sss[1,k]} =

E{uuuF|sss[1,k]}
zi

Furthermore, (21) leads to

E{uuuF|sss[1,k]} = −Φ−1

F KF

V. OPTIMAL LEADER (SENSOR) ACTIONS

.

(40)

For any given η[1,n], Theorem 1 provides the unique optimal
reactions of friendly and adversarial agents. Now, for each se-
quence θ[1,N] ∈ Ω, we aim to compute the optimal S strategies
that minimize

(cid:34) E{E{xxxo

n|sss[1,n]}|sss[1,k]}

...
1|sss1}|sss[1,k]}

E{E{xxxo

(cid:35)

.

(41)

µ(θθθ [1,N]) ∑

∑
θθθ [1,N]∈Ω

κ∈¯h(θθθ [1,N])

κ+−1
∑
k=κ

E

(cid:110)

(cid:107)xxxk+1(cid:107)2
QF

+ (cid:107)uuu(κ)∗

θθθ j,k (cid:107)2
RF

(cid:111)

.

(46)

Note that we have

E{E{xxxo

l |sss[1,l]}|sss[1,k]} =

(cid:26) E{xxxo
E{xxxo

l |sss[1,k]}
l |sss[1,l]}

if l ≥ k
if l < k

,

where the ﬁrst case, i.e., l ≥ k, follows due to the iterated ex-
pectations with nested conditioning sets, i.e., {sss[1,l]} ⊇ {sss[1,k]}

To this end, we ﬁrst seek to write uuu(κ)∗
θ ,k , θ ∈ {F, A1, . . . , At },
derived in (21) and (45), in the same form. By (21), we have

ˆxˆxˆxo.

F = − Φ−1
uuu∗

F KF
(cid:124) (cid:123)(cid:122) (cid:125)
=: T (1)
F

(47)

Correspondingly, (45) leads to

uuu(κ)∗
Ai

−

(cid:122)
(Φ(κ)
Ai
)−1K(κ)
Ai

= −
(cid:16)
(Φ(κ)
Ai

=: Z(κ)
Ai
(cid:125)(cid:124)
(cid:0)1n−κ+1 ⊗ zi
)−1K(κ)
Ai
F (κ) + [ I(n−κ+1)r O(n−κ+1)r×(κ−1)r ] Φ−1

(cid:123)
(cid:1)

(cid:124)

(cid:123)(cid:122)
=: T (κ)
Ai

F KF

ˆxˆxˆxo.

(cid:17)

(cid:125)

Let Z(1)
obtain

F = 0 be a zero vector, then for θ ∈ {F, A1, . . . , At }, we

(48)

uuu(κ)
θ = −T (κ)

ˆxˆxˆxo − Z(κ)
θ ,

θ

θ
is a (n−κ +1)r ×nm matrix and Z(κ)

where T (κ)
θ ∈ R(n−κ+1)r.
However, in (46), only uuu(κ)∗
θθθ j,k for k = κ, . . . , κ+ −1 are included.
Let κT be the last non-terminating state transition time, and nT
denote the termination time corresponding to a jump to state
T or end of horizon. Then, by (48), for a given realization of
the process {θθθ j}, e.g., θ[1,N] ∈ Ω, we have

=:uuu∗

∈RnT r

θ[1,N]
(cid:125)(cid:124)
MκT uuu

(cid:122)


(cid:123)


(κT )∗

∈RnT r×nm

=: Tθ[1,N]
(cid:122)


(cid:125)(cid:124)
MκT T

(κT )

(cid:123)

 ˆxˆxˆxo −


∈RnT r

(cid:123)


=: Zθ[1,N]
(cid:122)
(cid:125)(cid:124)

MκT Z
...
M1Z(1)
θ1

(κT )
θ jT











 ,


 = −

θ jT...
M1uuu(1)∗
θ1

θ jT...
M1T (1)
θ1
where Mκ ∈ R(κ+−κ)r×(n−κ+1)r is given by Mκ = [ O I(κ+−κ)r ].
Therefore, we obtain
uuu∗
θ[1,N]

= −Tθ[1,N] ˆxˆxˆxo − Zθ[1,N].
Even though S constructs a single set of strategies {ηk ∈
ϒ} without knowing C’s type, the resulting sensor outputs
{sssk = ηk(xxxk)} depend on the state xxxk, hence C’s type and
correspondingly θ[1,N]. However, as shown in Section IV, the
problem entails classical information and ˆxˆxˆxo does not depend
on θ[1,N]. Therefore, let uuu∗
θ[1,N],k be the corresponding control
input at time k according to (49) for a given realization θ[1,N].
Then, the objective function (46) can be written as
nT
∑
k=1

(cid:110)
(cid:107)xxxθθθ [1,N],k+1(cid:107)2
QF

θθθ [1,N],k(cid:107)2
RF

µ(θθθ [1,N])

+ (cid:107)uuu∗

∑
θθθ [1,N]∈Ω

(49)

E

(cid:111)

.

min
ηk∈ϒ,
k=1,...,nT

(50)

,

(51)

(52a)

(52b)

(52c)

Note that the inner summation can be written as
θθθ [1,N],k + K[nT ]

S,k xxxθθθ [1,N],k(cid:107)2

E(cid:107)uuu∗

+ G[nT ]
S

∆

[nT ]
S,k

nT
∑
k=1

where

S,k+1B + RF
S,k )−1B(cid:48) ˇQ[nT ]

S,k = B(cid:48) ˇQ[nT ]
∆[nT ]
S,k = (∆[nT ]
K[nT ]
S = tr{Σ1( ˇQ[nT ]
G[nT ]

S,k+1A

S,1 − QF)} +

nT
∑
k=1
Similar to (13), the sequence { ˇQ[nT ]
following discrete-time Riccati equation:
S,k+1B(∆[nT ]

S,k+1 − ˇQ[nT ]

S,k = QF + A(cid:48)( ˇQ[nT ]
ˇQ[nT ]
ˇQS,nT +1 = QF.

tr{Σv ˇQ[nT ]

S,k+1}.

S,k } is deﬁned through the

S,k )−1B(cid:48) ˇQ[nT ]

S,k+1)A,

nT
∑
k=1
where

∆[nT ]
S

K[nT ]
S

9

(54)

+ G[nT ]
S

.

[nT ]
S,k

E(cid:107)uuuo∗

Then, by (14), (51) leads to
nT
∑
k=1
subject to (52a), where xxxo
θθθ [1,N],k + K[nT ]

θθθ [1,N],k + K[nT ]
S,k xxxo

k(cid:107)2
∆

k evolves according to (15a), and
θθθ [1,N],k−1 + . . . + K[nT ]

S,k Buuu∗

S,k Ak−2Buuu∗

uuuo∗
θθθ [1,N],k = uuu∗
θθθ [1,N],1.
We point out that due to time-invariant QF and RF, we have
ˇQ[nT ]
S,k = ˇQF,k+n−nT ,
K[nT ]
S,k = KF,k+n−nT ,
∆[nT ]
S,k = ∆F,k+n−nT ,
where ˇQF,k, KF,k, and ∆F,k are deﬁned in (14). Therefore, the
summation can be written as
θθθ [1,N],k+K[nT ]
S,k xxxo

+ K[nT ]
S

= E(cid:107)uuuo∗

(cid:107)uuuo∗

(56)

(55)

θθθ [1,N]

,

xxxo(cid:107)2
∆

[nT ]
S

k(cid:107)2
∆

[nT ]
S,k

:= [ InT r OnT r×(n−nT )r ]

(cid:34) ∆F,n

...

(cid:35)

(cid:104)

InT r
O(n−nT )r×nT r

(cid:105)

,

:= [ InT r OnT r×(n−nT )r ] KF

(cid:104)

(cid:105)

[ OnT m×(n−nT )m InT m ] .

∆F,1
InT m
O(n−nT )m×nT m
, uuuo∗

θθθ [1,N]

θθθ [1,N]

Furthermore, in terms of uuu∗

is given by

Ir K

[nT ]
S,nT
Ir







uuuo∗
θθθ [1,N]

=

[nT ]
AnT −2B
S,nT
[nT ]
S,nT −1AnT −3B

B ... K

··· K

...







uuu∗
θθθ [1,N]

Ir
(cid:104)
= [ InT r OnT r×(n−nT )r ] ΦF

InT r
O(n−nT )r×nT r

(cid:105)

uuu∗
θθθ [1,N]

.

Next, we introduce the parameters:
(cid:2) I
O
(cid:2) I
O

:= − [ I O ] ΦF

:= − [ I O ] ΦF

Ξ[nT ]
θθθ [1,N]
ξ [nT ]
θθθ [1,N]

(cid:3) Tθθθ [1,N]
(cid:3) Zθθθ [1,N]

,

(57)

(58)

(59)

almost everywhere on RrnT ×mn and RrnT , respectively, so that

uuuo∗
θθθ [1,N]

= Ξ[nT ]
θθθ [1,N]

ˆxˆxˆxo + ξ [nT ]
θθθ [1,N]

.

Therefore, the objective (50) can be written as
µ(θθθ [1,N]) (cid:0)E(cid:107)Ξθθθ ˆxˆxˆxo + ξθθθ + Kxxxo(cid:107)2

∆ + G(cid:1) ,

min
ηk∈ϒ,
k=1,...,n

∑
θθθ [1,N]∈Ω

(60)

(61)

where for notational simplicity, we let Ξθθθ := Ξ[nT ]
θθθ [1,N]
ξ [nT ]
θθθ [1,N]
term in (61) yields

, and G := G[nT ]

, K := K[nT ]

, ∆ := ∆[nT ]

S

S

S

. The expectation

, ξθθθ :=

+ 2E{( ˆxˆxˆxo)(cid:48)Ξ(cid:48)
+ 2E{ξ (cid:48)

E(cid:107)Ξθθθ ˆxˆxˆxo + ξθθθ + Kxxxo(cid:107)2

∆ = E{( ˆxˆxˆxo)(cid:48)Ξ(cid:48)

θθθ ∆Ξθθθ ˆxˆxˆxo}
θθθ ∆Kxxxo} + E{(xxxo)(cid:48)K(cid:48)∆K(xxxo)}

θθθ ∆(Ξθθθ ˆxˆxˆxo + Kxxxo)} + E{ξ (cid:48)
ˆxˆxˆxo and xxxo have zero mean and are independent
θθθ ∆(Ξθθθ ˆxˆxˆxo +

Note that
of θθθ [1,N] by (18) and (34). Correspondingly, E{ξ (cid:48)
Kxxxo)} = 0. Furthermore, the terms

θθθ ∆ξθθθ }.

(62)

(53)

E{(xxxo)(cid:48)K(cid:48)∆K(xxxo)} = tr{E{xxxo(xxxo)(cid:48)}K(cid:48)∆K}

and

θθθ ∆}

E{ξ (cid:48)

θθθ ∆ξθθθ } = tr{ξθθθ ξ (cid:48)
do not depend on the optimization arguments η[1,n]. Therefore,
(62) yields
E(cid:107)Ξθθθ ˆxˆxˆxo + ξθθθ + Kxxxo(cid:107)2

∆ = tr (cid:8)E{ ˆxˆxˆxo( ˆxˆxˆxo)(cid:48)}Ξ(cid:48)

+ tr{E{xxxo( ˆxˆxˆxo)(cid:48)}Ξ(cid:48)
+ tr (cid:8)E{xxxo(xxxo)(cid:48)}K(cid:48)∆K} + tr{ξθθθ ξ (cid:48)

θθθ ∆Ξθθθ
θθθ ∆K} + tr{E{ ˆxˆxˆxo(xxxo)(cid:48)}K(cid:48)∆Ξθθθ }
θθθ ∆(cid:9) .
(63)

(cid:9)

and

We

aim to

E{xxxo(xxxo)(cid:48)}. Let Σo

compute E{ ˆxˆxˆxo( ˆxˆxˆxo)(cid:48)}, E{xxxo( ˆxˆxˆxo)(cid:48)},
k := E{xxxo


k(xxxo
k)(cid:48)}. Then, we have
··· An−1Σo
Σo
n
1
··· An−2Σo
Σo
n−1A(cid:48)
1
...
...
...
1(An−1)(cid:48) Σo
Σo
Σo
1
Furthermore, let Hk := E{ ˆxˆxˆxo
k( ˆxˆxˆxo
written as

AΣo
Σo
n−1
...
1(An−2)(cid:48)

Σo := E{xxxo(xxxo)(cid:48)} =






n−1

···

k)(cid:48)}. Then, E{ ˆxˆxˆxo( ˆxˆxˆxo)(cid:48)} can be


 .

(64)

1)(cid:48)}

(cid:35)

E{ ˆxˆxˆxo( ˆxˆxˆxo)(cid:48)} =






=

(cid:34) E{ ˆxˆxˆxo

E{ ˆxˆxˆxo

n( ˆxˆxˆxo
...
1( ˆxˆxˆxo
Hn
Hn−1A(cid:48)
...

n( ˆxˆxˆxo
...
1( ˆxˆxˆxo

n)(cid:48)} ··· E{ ˆxˆxˆxo
...
n)(cid:48)} ··· E{ ˆxˆxˆxo
AHn−1
Hn−1

...

1)(cid:48)}
··· An−1H1
··· An−2H1
...
...






(65)

since for l < k, we have

H1(An−1)(cid:48) H1(An−2)(cid:48)

···

H1

E{ ˆxˆxˆxo

l ( ˆxˆxˆxo

k)(cid:48)|sss[1,l]}}

l ( ˆxˆxˆxo
E{ ˆxˆxˆxo

k)(cid:48)} = E{E{ ˆxˆxˆxo
(a)
= E{ ˆxˆxˆxo
l
(b)
l ( ˆxˆxˆxo
= E{ ˆxˆxˆxo

k|sss[1,l]}}
l )(cid:48)}(Ak−l)(cid:48),
where (a) holds since ˆxˆxˆxo
l is σ -sss[1,l] measurable, and (b) follows
due to the iterated expectations with nested conditioning sets,
i.e., {sss[1,l]} ⊆ {sss[1,k]}. We also note that for l ≤ k, E{ ˆxˆxˆxo
k)(cid:48)} =
E{ ˆxˆxˆxo
l , which is σ -sss[1,l]
measurable, are independent of each other and {vvvk} is a zero-
mean white noise process. This leads to
l (xxxo
l (xxxo
l )(cid:48)}

l )(cid:48)} = E{E{ ˆxˆxˆxo
l ( ˆxˆxˆxo
= E{ ˆxˆxˆxo

l )(cid:48)}(Ak−l)(cid:48) since vvv j,

j > l, and ˆxˆxˆxo

l )(cid:48)|sss[1,l]}}

E{ ˆxˆxˆxo

l (xxxo

l (xxxo

due to the law of iterated expectations. This implies that
k)(cid:48)} = E{ ˆxˆxˆxo
= E{ ˆxˆxˆxo

l )(cid:48)}(Ak−l)(cid:48)
k)(cid:48)}

l ( ˆxˆxˆxo
l ( ˆxˆxˆxo

E{ ˆxˆxˆxo

l (xxxo

and correspondingly E{ ˆxˆxˆxo(xxxo)(cid:48)} = E{ ˆxˆxˆxo( ˆxˆxˆxo)(cid:48)}.

Next, we can rewrite (61) as











Hn
Hn−1A(cid:48)
...

AHn−1
Hn−1

...

··· An−1H1
··· An−2H1
...
...

H1(An−1)(cid:48) H1(An−2)(cid:48)

···

H1

tr

min
ηk∈ϒ,
k=1,...,n

where




 Π






+ Πo,

(66)

Π := ∑

θθθ [1,N]∈Ω
Πo := ∑

θθθ [1,N]∈Ω

µ(θθθ [1,N]){Ξ(cid:48)

θθθ ∆Ξθθθ + Ξ(cid:48)

θθθ ∆K + K(cid:48)∆Ξθθθ },

µ(θθθ [1,N])(tr{ΣoK(cid:48)∆K} + tr{ξθθθ ξ (cid:48)

θθθ ∆} + G),

(67a)

(67b)

10

TABLE I: Computation of secure sensor strategies.

Algorithm 1: Secure Sensor Design

Compute Vk’s:

Compute KF,k, ∆F,k, and KAi,k for k = 1, . . . , n and i = 1, . . . ,t

via (12) and (27).
Compute ΦF by (20), Φ(1)
Ai
Compute Π and Πo, given by (67), by computing Ξθ and ξθ

by (36), and F (1) by (44).

for all θθθ [1,N] ∈ Ω.

Then, compute Vk, k = 1, . . . , n, via (69).

SDP Problem:

Solve the SDP problem on the left hand side of (70) through

a numerical toolbox, e.g., CVX [25], [26], and obtain the
solutions S∗
0 = O.

k , for k = 1, . . . , n.

Set S∗

Equilibrium achieving sensor strategies:

Compute the corresponding idempotent matrices Pk,∀k, by

using S∗

k , ∀k, and (71).

Compute the eigen decompositions: Pk = UkΛkU (cid:48)
k.
Compute Lk, ∀k, by using S∗
k−1,Uk, Λk, and (75).
And ηk(xxxk) = L (cid:48)

kxxxk.

which are independent of the optimization arguments. Hence,
the optimization problem (66) faced by S can be written as an
afﬁne function of Hk’s as follows:

n
∑
k=1

min
ηk∈ϒ,
k=1,...,n

tr{VkHk} + Πo,

(68)

for certain symmetric deterministic matrices7 Vk ∈ Sm, k =
1, . . . , n, which are given by

Vk := Πk,k +

n
∑
l=k+1

Πk,lAl−k + (Al−k)(cid:48)Πl,k,

(69)

and Πk,l is the corresponding m × m sub-block of Π.

k( ˆxˆxˆxo

k = E{xxxo

As a secure sensor designer, we seek to solve this non-
linear optimization problem (68). Note that Hk = E{ ˆxˆxˆxo
k)(cid:48)}
and ˆxˆxˆxo
k|sss[1,k]}. However, as pointed out in Remark
6, a brute force approach is computationally expensive. To
this end, we employ the approach in [24], which considers
another optimization problem that bounds the original one
from below, and then, compute strategies for the original
problem, which optimize the lower bound. Based on this, the
following theorem characterizes equilibrium achieving secure
sensor strategies.
Theorem 2. The optimal linear secure sensor strategies can
be computed via Algorithm 1, described in Table I.

Proof. Based on Lemma 3 in [24], by characterizing neces-

sary conditions on Hk’s, we have

∑n

min Sk∈Sm,
k=1,...,n
s.t. Σo
j (cid:23) S j (cid:23) AS j−1A(cid:48) ∀ j

k=1 tr{VkSk} ≤ min ηk∈ϒ,
k=1,...,n

∑n

k=1 tr{VkHk},

(70)

7Sm denotes the set of m × m symmetric matrices.

j (xxxo

j = E{xxxo

k = AS∗
S∗

where Σo
j )(cid:48)}, S0 := O. Note that the left hand side
of (70) is an SDP problem. By invoking Theorem 4 in [24],
we can characterize the solution, S∗

n, as
k−1A(cid:48))1/2Pk(Σo
k−1A(cid:48) + (Σo
k−1A(cid:48))1/2, (71)
k − AS∗
k − AS∗
0 = O and Pk ∈ Sm is a certain
for k = 1, . . . , n, where S∗
symmetric idempotent matrix. Note that by solving the SDP
problem numerically, we can compute the corresponding Pk’s.
Next, say that S employs memoryless linear policies sssk =

1, . . . , S∗

ηk(xxxk) = L (cid:48)

kxxxk. Then, by (17) and (33), we have
1xxxo

k = E{xxxo
ˆxˆxˆxo

1, . . . , L (cid:48)

k|L (cid:48)

kxxxo

k}.

which can also be written as
k−1 + (Σo

k = A ˆxˆxˆxo
ˆxˆxˆxo

k − AHk−1A(cid:48))Lk(L (cid:48)
k − A ˆxˆxˆxo
× L (cid:48)

k (xxxo

k−1),

k (Σo

k − AHk−1A(cid:48))Lk)†

(72)

ˆxˆxˆxo
−1 := 0 and H0 := O. Therefore, Hk =

k( ˆxˆxˆxo

for k = 1, . . . , n,
E{ ˆxˆxˆxo
Hk = AHk−1A(cid:48) + (Σo

k)(cid:48)} is given by

k − AHk−1A(cid:48))Lk(L (cid:48)
k − AHk−1A(cid:48)).
× L (cid:48)

k (Σo

k (Σo

k − AHk−1A(cid:48))Lk)†
(73)

We emphasize the resemblance between (71) and (73). In
k − AHk−1A(cid:48))1/2Lk, k = 1, . . . , n,
particular, if we set
(73) yields

¯Lk := (Σo

Hk = AHk−1A(cid:48) + (Σo

k − AHk−1A(cid:48))1/2 ¯Lk( ¯L (cid:48)
k
× (Σo
k − AHk−1A(cid:48))1/2,

¯Lk)† ¯L (cid:48)
k

(74)

¯Lk( ¯L (cid:48)
where
k
just like Pk in (71).

¯Lk)† ¯L (cid:48)

k is also a symmetric idempotent matrix

Therefore, given Pk’s, let Pk = UkΛkU (cid:48)

k be the eigen decom-

position, and set

¯Lk = UkΛk, i.e., set
k − AS∗

Lk = (Σo

k−1A(cid:48))−1/2UkΛk.

(75)

Then, we obtain Hk = S∗
k, which implies that S’s optimal
strategies are given by (75) while the optimal control inputs
for both friendly and adversarial C are given by (21) or (45),
(cid:3)
respectively.
Remark 8. We have considered that S knows the underlying
state transition probabilities, i.e., the statistics of the jump
process {θθθ j}. However, for a robust design against inaccurate
perception of the statistics, S can design the sensor outputs by
being cautious for the worst case scenario. For all possible
measures of θθθ [1,N] ∈ Ω, we can recompute Π and Πo in (67).
Let V denote the set of the corresponding (V1, . . . ,Vn) tuples,
which can be computed via (69). Then, by (70), the worst case
scenario is equivalent to

max
V[1,n]∈V

min
Sk∈Sm,
k=1,...,n

n
∑
k=1

tr{VkSk}

(76)

subject to Σ j (cid:23) S j (cid:23) AS j−1A(cid:48), j = 1, . . . , n and S0 = O. If V is
compact and convex, there exists a saddle point equilibrium
by the Minimax theorem [21]. Otherwise, we can consider the
convex hull of V , denoted by V c for computational simplicity
in addition to robustness. A detailed analysis in that direction
is left as future work.

In the following section, we provide several numerical
examples examining the performance of secure sensor design.

11

TABLE II: Comparison of
schemes for the cases shown in Fig. 3.

ˆJS, deﬁned in (78), of different

Cases

Probability

Classical

Secure

No Sensor
Output

1
2
3
4
5
6
7
8
9
10
11
12
13

0.700
0.025
0.025
0.025
0.025
0.025
0.025
0.025
0.025
0.025
0.025
0.025
0.025

Average

0
10.5
30.7
57.2
11.3
37.8
17.6
32.2
68.9
27.8
104.3
63.2
26.5

12.2

0.1
1.9
13.2
32.0
2.5
21.3
10.0
20.7
43.8
14.3
66.2
36.7
13.6

7.0

476
295.3
609.0
886.9
604.6
882.4
877.8
884.7
896.9
612.2
908.7
624.0
302.6

821.2

VI. ILLUSTRATIVE EXAMPLES

As numerical illustrations, we consider two different scenar-
ios: Scenario 1, where we compare the performance of the pro-
posed secure sensors with classical sensors that disclose xxxk to
C directly, and Scenario 2, where we analyze the robustness of
the proposed scheme against inaccurate perception of the state
transition statistics, i.e., µP(θ[1,N]) (cid:54)= µ(θ[1,N]) for θ[1,N] ∈ Ω,
where µP denotes the perceived statistics while µ denotes the
actual ones. We set time horizon n = 100, the state’s dimension
m = 8, and the control input’s dimension r = 2. The matrices in
the state recursion (1) are set randomly according to uniform
distribution such that A is not a singular matrix and scaled
by 0.1 in order to avoid computational instability. In order
to construct auto-covariance matrices Σ1 and Σv, we draw
a number from the uniform distribution on the unit interval
[0, 1] for each entry of a matrix D. Then, we can construct
a positive-deﬁnite covariance matrix by (D + D(cid:48))/2 + 2m I,
where the last term ensures that the constructed matrix is
diagonally dominant, and therefore, positive-deﬁnite. Then,
we have scaled Σv by 10 since the sensor outputs play more
essential role for C when the state noise variance is larger.

Scenario 1 - Performance Comparison. We speciﬁcally
consider the scenario where there are two adversaries, who
seek to regularize state xxxk around z1, z2 ∈ Rm that are drawn
from a multivariate Gaussian distribution. The positive semi-
deﬁnite weight matrices in the cost functions are set such that
(cid:104) O O
O Im/4

(cid:104) O O
O Im/2

, QA2 =

, QA1 =

QF =

(cid:104) Im/2 O
O O

(77)

(cid:105)

(cid:105)

(cid:105)

,

and RF = RA1 = RA2 = Ir while λ1 = λ2 = 0.1. The state
transition interval is set δ = 35 and Fig. 3 shows the possible
state transitions, e.g., θθθ [1,N] ∈ Ω, within the time horizon. We
consider the situation where the normalized measure of no-
inﬁltration case, i.e., θ[1,N] = {F, F, F}, referred to as Case-1,
is 0.7 while all the other cases, i.e., Cases 2-13, have the same
measure, as tabulated in Table II.

In Table II, we compare the performances of three different
schemes for the cases shown in Fig. 3. The classical scheme
refers to a sensor, who discloses xxxk directly to C, while no

12

Lk
Fig. 4: Singular values of the normalized gain matrix
(cid:107)Lk(cid:107) .
Ln has rank 4 while all the others L1, . . . , Ln−1 have rank 2.

TABLE III: Comparison of ˆJS in terms of µ and µP.

Cases

Actual
Probabilities

Accurate
Perception

Perceived
Probabilities

Inaccurate
Perception

1
2
3
4
5
6
7
8
9
10
11
12
13

0.700
0.025
0.025
0.025
0.025
0.025
0.025
0.025
0.025
0.025
0.025
0.025
0.025

Average

0.1
1.9
13.2
32.0
2.5
21.3
10.0
20.7
43.8
14.3
66.2
36.7
13.6

7.0

0.85
0.025
0.025
0.025
0.025
0.025
0.025
0
0
0
0
0
0

Average

0.0
1.9
13.3
32.1
2.5
21.3
10.0
24.8
53.0
19.3
80.0
46.3
18.2

8.1

as tabulated in Table III. Particularly, S is not aware of the
attacker A2, and designs the secure sensor outputs accordingly
even though the actual underlying statistics are as in Scenario
1, which is also provided in Table III. We have observed
that the performance degrades due to inaccurate perception of
the statistics; however, the proposed scheme still outperforms
classical scheme, whose performance is tabulated in Table II,
in individual cases, Cases 2 − 13, and on the average. We note
that since, in the inaccurate perception of the statistics, Case
1 has relatively higher probability, i.e., 0.85 > 0.70, we have
observed slight improvement in the performance in that case.

VII. CONCLUSION

In this paper, we have proposed and addressed secure
sensor design problem for cyber-physical systems with linear
Gaussian dynamics against
threats
with quadratic control objectives. By designing sensor outputs
cautiously in advance, we have sought to minimize the damage
that can be caused by undetected target-speciﬁc threats. To

the advanced persistent

Fig. 3: Possible state transitions within the time horizon.

sensor output refers to open-loop control of the system, i.e.,
sssk = 0 almost everywhere on Rm. As a performance measure
for each θθθ [1,N] ∈ Ω, we consider (51), where the last term G[nT ]
,
which does not depend on S’s strategies η[1,n], is excluded, i.e.,

S

ˆJS(θθθ [1,N]) :=

nT
∑
k=1

E(cid:107)uuuθθθ [1,N]

and, on the average,

+ K[nT ]

S,k xxxθθθ [1,N],k(cid:107)2

∆

(78)

[nT ]
S,k

µ(θθθ [1,N]) ˆJS(θθθ [1,N]).

(79)

∑
θθθ [1,N]∈Ω

In Table II, we have observed that open-loop control of the
system leads to inferior performance compared to the other
schemes in all the cases. The classical scheme outperforms
the proposed secure sensor scheme only in Case 1 yet slightly.
Note that Case 1 is the best case scenario, where there is no
inﬁltration into C. In all the other cases, the proposed scheme
outperforms the classical scheme substantially. Even though
the best case scenario is relatively likely compared to the
other cases, on the average, the proposed scheme outperforms
the classical scheme also substantially and achieves 40%
enhancement in the performance.

Furthermore, in Fig. 4, we have plotted the time evolution of
Lk
(cid:107)Lk(cid:107) . Note
the singular values of the normalized gain matrix
Lk
that the gain matrix Lk and the normalized gain matrix
(cid:107)Lk(cid:107)
leads to the same performance, while the normalized one is
preferred in Fig. 4 for better demonstration. We also note that
while Lk ∈ Rm×m, all the gain matrices have rank less than
m = 8, i.e., L1, . . . , Ln−1 have rank 2 and the last stage gain
matrix Ln has rank 4.

Scenario 2 - Robustness Analysis. We examine the robust-
ness of the proposed scheme for inaccurate perception of the
underlying state transition statistics. To this end, we consider
the setup in Scenario 1; however, now S perceives the statistics

1 2 3 4 5 6 7 8 9 10 12 13 11 10203040506070809010000.20.40.60.81kSingular Values of Gain Matrix13570100κ13

[18] M. O. Sayin and T. Bas¸ar, “Secure sensor design for cyber-physical
systems against advanced persistent threats,” in Proc. Int. Conf. on
Decision and Game Theory for Security on Lecture Notes in Computer
Science, S. Rass, B. An, C. Kiekintveld, F. Fang, and S. Schauder, Eds.,
vol. 10575. Vienna, Austria: Springer, Oct. 2017, pp. 91–111.
[19] P. Brangetto and M. K.-S. Aubyn, “Economic aspects of national
cyber security strategies,” NATO Cooperative Cyber Defense Centre of
Excellence Tallinn, Estonia, Tech. Rep., 2015.

[20] D. Liberzon, Calculus of Variations and Optimal Control Theory: A

Concise Introduction. Princeton University Press, 2011.

[21] T. Bas¸ar and G. Olsder, Dynamic Noncoopertative Game Theory.
Society for Industrial Mathematics (SIAM) Series in Classics in Applied
Mathematics, 1999.

[22] J. Kennedy and R. Eberhart, “Particle swarm optimization,” in Proc.

IEEE Int. Conf. Neural Networks, 1995, pp. 1942–1948.

[23] R. Bansal and T. Bas¸ar, “Simultaneous design of measurement and
control strategies for stochastic systems with feedback,” Automatica,
vol. 25, no. 5, pp. 679–694, 1989.

[24] M. O. Sayin, E. Akyol, and T. Bas¸ar, “Hierarchical multi-stage Gaussian
signaling games: Strategic communication and control,” Automatica,
submitted for publication, available at ArXiv 1609.09448, 2017.
[25] M. Grant and S. Boyd, “CVX: Matlab software for disciplined convex

programming, version 2.1,” http://cvxr.com/cvx, Mar. 2014.

[26] ——, “Graph implementations for nonsmooth convex programs,” in
Springer-Verlag Limited,

Recent Advances in Learning and Control.
2008, pp. 95–110.

Muhammed O. Sayin is currently pursuing the Ph.D. degree in Electrical and
Computer Engineering from the University of Illinois at Urbana-Champaign
(UIUC). He received the B.S. and M.S. degrees in Electrical and Electronics
Engineering from Bilkent University, Ankara, Turkey, in 2013 and 2015,
respectively. His current research interests include signaling games, dynamic
games and decision theory, and cyber-physical systems.

Tamer Bas¸ar is with the University of Illinois at Urbana-Champaign, where
he holds the academic positions of Swanlund Endowed Chair; Center for
Advanced Study Professor of Electrical and Computer Engineering; Research
Professor at the Coordinated Science Laboratory; and Research Professor at
the Information Trust Institute. He is also the Director of the Center for
Advanced Study.

He received B.S.E.E. from Robert College, Istanbul, and M.S., M.Phil, and
Ph.D. from Yale University. He is a member of the US National Academy
of Engineering, member of the European Academy of Sciences, and Fellow
of IEEE, IFAC (International Federation of Automatic Control) and SIAM
(Society for Industrial and Applied Mathematics), and has served as president
of IEEE CSS (Control Systems Society), ISDG (International Society of
Dynamic Games), and AACC (American Automatic Control Council). He has
received several awards and recognitions over the years, including the highest
awards of IEEE CSS, IFAC, AACC, and ISDG, the IEEE Control Systems
Award, and a number of international honorary doctorates and professorships.
He has over 800 publications in systems, control, communications, and
dynamic games, including books on non-cooperative dynamic game theory,
robust control, network security, wireless and communication networks, and
stochastic networked control. He was the Editor-in-Chief of Automatica
between 2004 and 2014, and is currently editor of several book series. His
current research interests include stochastic teams, games, and networks;
distributed algorithms; security; and cyber-physical systems.

this end, we have modeled the problem formally in a game-
theoretical setting. We have determined the optimal control
inputs for both friendly and adversarial objectives for given
linear sensor strategies. Then, we have provided an algorithm
to compute the optimal linear secure sensor strategies that lead
to the equilibrium. We note that without linearity assump-
tion on the sensor outputs, the problem entails non-classical
information model due to distinct objectives of the agents.
Furthermore, for general, e.g., nonlinear, sensor outputs, the
corresponding optimal control policies could not be unique
and could not even be expressed in closed form [17].

Some future directions of research on this topic include: •
Formulation of secure sensor design strategies when the sensor
has access to noisy observations, or for, e.g., robust control or
feedback stability of the systems. • Here, we have considered
the scenarios, where the attackers have perfect knowledge
about the underlying state recursion. Another interesting ex-
tension would be to analyze the scenarios, where the attackers
can only have partial knowledge. In such scenarios, intuitively,
sensor outputs could play relatively more powerful roles since,
without caution, sensors might help the attackers to recover the
unknown part of the system dynamics.

REFERENCES

[1] J. Giraldo, E. Sarkar, A. A. Cardenas, M. Maniatakos, and M. Kantar-
cioglu, “Security and privacy in cyber-physical systems: A survey of
surveys,” IEEE Design & Test, vol. 34, pp. 7–17, 2017.

[2] A. Humayed, J. Lin, F. Li, and B. Luo, “Cyber-physical systems security
– A survey,” IEEE Internet of Things Journal, vol. 4, no. 6, 2017.
[3] N. Nelson, “The impact of Dragonﬂy malware on industrial control

systems,” The SANS Institute, 2016.

[4] S. Karnouskos, “Stuxnet worm impact on industrial cyber-physical
system security,” in Proc. IEEE Industrial Electronics Society, 2011.
[5] Y. Liu, P. Ning, and M. K. Reiter, “False data injection attacks against
state estimation in electric power grids,” ACM Trans. Information and
System Security, vol. 14, no. 1, 2009.

[6] Y. Mo and B. Sinopoli, “Secure control against replay attacks,” in Proc.
47th Allerton Conf. Communication, Control, and Computing, 2009.
[7] Y. Mo, R. Chabukswar, and B. Sinopoli, “Detecting integrity attacks on
SCADA systems,” IEEE Trans. Control Syst. Tech., vol. 22, no. 4, 2014.
[8] Y. Mo and B. Sinopoli, “Integrity attacks on cyber-physical systems,”
in Proc. 1st ACM Int. Conf. High Conﬁdence Networked Systems, 2012,
pp. 47–54.

[9] A. S. Willsky, “A survey of design methods for failure detection in
dynamic systems,” Automatica, vol. 12, no. 6, pp. 601–611, 1976.
[10] A. Teixeira, I. Shames, H. Sandberg, and K. H. Johansson, “Revealing
stealthy attacks in control systems,” in Proc. 50th Allerton Conf.
Communication, Control, and Computing, 2012.

[11] F. Pasqualetti, F. D¨orﬂer, and F. Bullo, “Attack detection and identiﬁca-
tion in cyber-physical systems,” IEEE Trans. Autom. Control, vol. 58,
no. 11, 2013.

[12] H. Fawzi, P. Tauada, and S. Diggavi, “Secure estimation and control for
cyber physical systems under adversarial attacks,” IEEE Trans. Autom.
Control, vol. 59, no. 6, pp. 1454–1467, 2014.

[13] Y. Chen, S. Kar, and J. M. F. Moura, “Cyber physical attacks with
control objectives and detection constraints,” in Proc. 55th IEEE Conf.
on Decision and Control (CDC), 2016, pp. 1125–1130.

[14] ——, “Cyber physical attacks constrained by control objectives,” in
Proc. Americal Control Conference (ACC), 2016, pp. 1185–1190.
[15] R. Zhang and P. Venkitasubramaniam, “Stealthy control signal attacks
in linear quadratic Gaussian control systems: Detectability reward trade-
off,” IEEE Trans. Inf. Forensics and Security, vol. 12, no. 7, pp. 1555–
1570, 2017.

[16] F. Miao, Q. Zhu, M. Pajic, and G. J. Pappas, “Coding schemes for
securing cyber-physical systems against stealthy data injection attacks,”
IEEE Trans. Autom. Control, 2017.

[17] P. R. Kumar and P. Varaiya, Stochastic systems: Estimation, identiﬁca-
tion and adaptive control. Prentice Hall, Englewood Cliffs, NJ, 1986.

