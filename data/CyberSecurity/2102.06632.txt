1
2
0
2

b
e
F
2
1

]

R
C
.
s
c
[

1
v
2
3
6
6
0
.
2
0
1
2
:
v
i
X
r
a

Deep Reinforcement Learning for Backup Strategies against
Adversaries
Preprint, compiled February 15, 2021

Pascal Debus1∗, Nicolas Müller2, and Konstantin Böttinger3

1,2,3Fraunhofer AISEC, Garching, Germany

Abstract
Many defensive measures in cyber security are still dominated by heuristics, catalogs of standard procedures,
and best practices. Considering the case of data backup strategies, we aim towards mathematically modeling the
underlying threat models and decision problems. By formulating backup strategies in the language of stochastic
processes, we can translate the challenge of ﬁnding optimal defenses into a reinforcement learning problem.
This enables us to train autonomous agents that learn to optimally support planning of defense processes. In
particular, we tackle the problem of ﬁnding an optimal backup scheme in the following adversarial setting:
Given k backup devices, the goal is to defend against an attacker who can infect data at one time but chooses to
destroy or encrypt it at a later time, potentially also corrupting multiple backups made in between. In this setting,
the usual round-robin scheme, which always replaces the oldest backup, is no longer optimal with respect to
avoidable exposure. Thus, to ﬁnd a defense strategy, we model the problem as a hybrid discrete-continuous
action space Markov decision process and subsequently solve it using deep deterministic policy gradients. We
show that the proposed algorithm can ﬁnd storage device update schemes which match or exceed existing
schemes with respect to various exposure metrics.

1 Introduction

An essential component of every IT infrastructure is a reliable
backup system which minimizes the damage caused by both
natural accidents (ﬁre, disk crash, water) as well as cyber attacks.
While such natural accidents are immediately apparent, cyber
attacks aim at stealthy infection and often come in two phases.
After infection, the objective (e.g. encryption or deletion of ﬁles)
is delayed in order to infect newly made backups as well. A
well-known example of this is ransom-ware, which encrypts a
user’s hard drive and tries to extort payment in Bitcoin for the
restoration of the ﬁles [1].

Thus, a user is faced with the following problem: There is a
ﬁxed number of available backup devices, each of which can
hold only a single backup of the database at a speciﬁc point in
time. The user must consecutively decide when and also which
of the older backups is overwritten with the most recent state
of the database while not knowing if and when an infection has
occurred.

Since backups made after time of infection are corrupted, a
good backup strategy has to deal with the conﬂicting goals of
keeping backups available which are ﬁrstly old enough to not be
infected, while secondly as recent as possible such that not too
much information is needlessly lost. In this setting, the so-called
round-robin scheme, which always replaces the oldest backup,
is no longer optimal with respect to avoidable exposure.

While a single user may easily add more backup devices, this
problem is much more relevant for companies operating on large
volumes of business critical data. For these, adding more backup
devices comes at a signiﬁcant cost due to the sheer volume of
their data, as well as maintenance and privacy concerns. Even
though the costs of transferring data into long-term cloud storage
such as Amazon Glacier are low, this is not a feasible remedy
since most of the costs actually occur when data needs to be

retrieved again. Depending on the pricing model the retrieval
process is usually bandwidth-limited and generally much slower
when compared to on-site backup devices. This is a problem
when the time to recovery is critical, e.g. in production sites
where every hour of outage causes a signiﬁcant loss. Thus,
ﬁnding a strategy to optimally utilize a given number of backup
devices is of great interest. By measuring the performance of
such a strategy using loss exposure metrics, we can cast the
problem as optimization problem and solve it to obtain concrete
schemes, as shown by [2], [3].

Additional value can be added by treating the optimization as
learning process of an autonomous agent driven by the loss expo-
sure feedback: Such an agent will not only provide a performant
scheme but will also be able to deal with deviations from that
scheme, as they will likely occur in any real world application.

We contribute to solving this problem as follows:

• We formalize the problem as a hybrid discrete-
continuous action space Markov decision process and
thus transform it to a reinforcement learning problem.
• We propose an algorithm that learns to ﬁnd update
sequences and time steps which minimize mean avoid-
able exposure metrics based on an extension of deep de-
terministic policy gradients for hybrid actions spaces.
• We demonstrate that the algorithm produces backup se-
quences and steps sizes that perform better with respect
to mean avoidable exposure metrics than the reference
algorithm that strictly enforces a worst-case optimal
lower bound at every step.

• Finally, to stipulate further research, we publish our

implementation.2

2Repository link available upon publication

*pascal.debus@aisec.fraunhofer.de

 
 
 
 
 
 
Preprint – Deep Reinforcement Learning for Backup Strategies against Adversaries

2

The paper is structured as follows: Section 2 summarizes re-
lated work on checkpointing algorithms and deep reinforcement
learning. Section 3 describes the relevant theory of deep rein-
forcement learning algorithms, in particular deep deterministic
policy gradients (DDPG). Subsequently, Section 4 presents the
formalization of the backup problem and how to translate it
to a Markov decision process. In Section 5 we present details
on the implementation and necessary modiﬁcations of DDPG
to apply it to hybrid discrete-continuous actions spaces. Sec-
tion 6 shows the results of applying the suggested algorithm
to ﬁnd schemes for some concrete k and discusses the results.
Finally, Section 7 concludes the paper and suggests some further
research directions.

2 Related Work

2.1 Checkpointing Algorithms and Backup Strategies

The research on checkpointing and backup strategies is a very
large and diverse ﬁeld with varying objectives. Some early work
like [4, 5] focuses on ﬁnding the optimal interval length between
checkpoints/backups based on some stochastic model of fault
distribution or system load in order to maximize availability.
[6] considers backup schemes based on incremental and full
backups and optimizes the length between full backups given
a cost model for the overhead of incremental and full backups.
Other approaches such as [7] concentrate on very speciﬁc en-
vironments like remote backups of mobile devices and try to
optimize power consumption. In summary, most related work
is centered around ﬁnding the single optimal interval length in
an equidistant setting with respect to some metric and environ-
ment model. In contrast, there is not much work on strategies
in an adversarial setting and the more realistic case of a limited
number of non-equidistant checkpoints.

The problem of ﬁnding an optimal backup strategy in the face
of cyber attacks has been ﬁrst brought up by [3] in the more
general context of online checkpointing algorithms and was
initially mathematically formalized in [2]. A checkpoint is any
memorized state of a computation system that can be used to roll
back computation from time T to an earlier time, Tcheckpoint < T ,
without having to restart from the system’s initial state. Other
applications are, for example, the execution state of a program
in an interactive debugging session: While looking for the root
cause of the bug, the developer might accidentally step over the
problematic code sections and is then faced with the problem of
reproducing the execution state just before the problem. In this
setting, avoidable exposure does not refer to a loss of data but
the loss of working time when having to restore execution states
from a very old checkpoint.

The problem of minimizing avoidable exposure has its roots in
mathematical discrepancy theory (see, for example, [8]) which
tries to characterize unavoidable deviations of sequences from a
uniform distribution. A natural metric to measure discrepancy
is the worst case ratio between actual and ideal interval lengths
formed by the sequence.
In their work, Bar-On et al. [3] identify necessary and suﬃcient
properties that must hold for schemes with worst-case discrep-
ancy guarantees and subsequently derive tight upper and lower
discrepancy bounds for checkpointing schemes on k ≤ 10 check-
points as well as an asymptotic upper and lower discrepancy

bound of ln 4 for large k. These bounds are also computationally
veriﬁed and an algorithm is provided which generates candi-
date periodic update sequences. The corresponding step size
sequence is then computed by ﬁnding feasible solutions to a
ﬁnite subprogram of an inﬁnite linear program whose constraint
set is given by the initially proven properties.

In contrast to this, our approach uses reinforcement learning
to learn a policy that minimizes the mean discrepancy for an
arbitrary initial or intermediate set of checkpoints.

2.2 Deep Reinforcement Learning

We solve the backup problem using an extension of the deep
deterministic policy gradient (DDPG) algorithm. This algorithm
was initially proposed by [9] and was used to simulate a number
of physical tasks, such as cartpole swing-up and car driving.
DDPG itself is an extension of the Deep Q Network (DQN)
algorithm by [10], which has been widely popularized due to its
human level performance in Atari Video Games.
Deep Q-Learning made it possible to eﬀectively handle continu-
ous or very large discrete state spaces using neural network based
value-function approximation. DDPG extended this approach
to continuous action spaces by using a policy network that is
connected to the DQN within an actor-critic infrastructure[11].

Finally, Hausknecht and Stone [12] suggested a method to deal
with parameterized action spaces, i.e. a set of discrete actions
augmented by one or more continuous parameters, which they
applied to the domain of simulated RoboCup soccer. In this
approach, the discrete action with the highest score is chosen,
along with its parameters.

For our implementation, we also follow this general idea but
treat the step size as independent output instead of a parameter of
the discrete action such that the action space can be considered
hybrid rather than parameterized.

3 Background: Deep Reinforcement Learning

In this section, we provide the necessary background on rein-
forcement learning. The framework of Markov decisions pro-
cesses (MDPs) is usually the best way to formally analyze most
reinforcement learning algorithms. A Markov decision problem
is deﬁned as the tuple (S, A, P, r, γ), where S, A denote the
state space and action space, respectively, P the state transition
probability matrix, r the reward function and lastly, γ is the
discount factor for future rewards.

3.1 Q-Learning and Bellman equation
Consider an MDP with state space S and action space A = Rn.
Let E denote the (potentially stochastic) environment which
determines state distribution, transition as well as reward dynam-
ics.

The agent’s behavior is determined by a (generally stochastic)
policy π, which assigns to each state s ∈ S a probability dis-
tribution P(A) over all possible actions. At the core of many
reinforcement learning algorithms are value functions such as
the action-value function Qπ(st, at) which is deﬁned in equa-
tion (1) to be the expected total discounted reward Rt for each

Preprint – Deep Reinforcement Learning for Backup Strategies against Adversaries

3

state-action combination at time t when following the policy π
after the ﬁrst action at.

Qπ(st, at) = Eri≥t,si>t∼E
ai>t∼π

[Rt|st, at]

(1)

= Ert,st+1∼E[r(st, at) + γEat+1∼E[Qπ(st+1, at+1)]]

device speciﬁed by dn. Hence, tn is an unbounded, monotoni-
cally increasing sequence while dn is a label from {0, 1, . . . , k}.
Since the storage devices are assumed to be identical and diﬀer
only by the age of the backup, they can be relabelled after each
update action such that the label l ∈ {0, 1, . . . , k} always refers
to the (l + 1)th oldest backup.

The second equality in equation (1) is also known as Bellman
equation and provides a way to compute the expected reward
values recursively. In the case of a deterministic policy µ : S →
A, the action is known to be µ(st+1) and the Bellman equation
can be simpliﬁed to

(2)

Qµ(st, at) = Ert,st+1∼E[r(st, at) + γQµ(st+1, µ(st+1))].
Since the action is not a random variable any more, the ex-
pectation does not depend on it which enables the agent to
learn the value function for µ oﬀ-policy, i.e. by following
If µ is the greedy policy,
a stochastic exploration policy.
µ(s) = arg maxa Q(s, a), this algorithm is known as Q-learning.
For large state spaces one usually has to resort to parameterized
approximations Qµ(s, a|θQ) of the value function, using a (deep)
neural network with weights θQ, which is consequently referred
to as deep Q-learning.

Following the greedy policy for continuous a, however, poses a
problem because ﬁnding the arg max is essentially an optimiza-
tion problem that needs to be solved for every step.

3.2 Deep Deterministic Policy Gradient (DDPG)

The DDPG algorithm avoids this problem by also using a param-
eterized approximation µ(s|θµ) of the deterministic policy which
is connected to the action-value function approximator by the
actor-critic architecture [11]. The critic learns the action-value
function using deep Q-learning, while the actor uses gradient
ascent to learn weights θµ such that the performance objective
J(µθ) = Esi∼ρµ[R1] is maximized, where ρµ denotes the density
of the discounted state distribution (when following µ).

The quantitative relation between the actor and the critic approx-
imators and their gradients is given by the Deterministic Policy
Gradient Theorem stated by [13]:

∇θµ J(µθ) = Est∼ρµ [∇θµ Qµ(s, a|θQ)|s=st,a=µ(st|θµ)]
(3)
For an oﬀ-policy actor-critic method, however, which uses a
stochastic exploration policy π, this relation only holds approxi-
mately:

4.1.1

Snapshots and Updates

At any time T , the state of the backup scheme can be be de-
scribed by a snapshot S = (T1, T2, ..., Tk) of times at which each
backup device was last updated. Consequently, an update action
(d, t) = (2, T ) will remove T2 from the snapshot, decrement the
label of all devices with label l > 2 and append T as new Tk to
the snapshot, as illustrated by Figure 1.

Figure 1: Illustration of update action (d, t) = (2, T ). Source:
[3]

4.1.2 Attack Scenario

In an attack scenario, there are two distinct time points: a secret
infection time T (cid:48) and an execution time T (cid:48)(cid:48) that will become im-
mediately apparent. The total recovery cost is then proportional
to T (cid:48)(cid:48) − Ti where i = arg maxk{Tk|Tk < T (cid:48)}, which corresponds
to resetting the system to the backup fabricated at time Ti. The
part of the cost proportional to T (cid:48)(cid:48) − T (cid:48) is unavoidable, since
backups fabricated during [T (cid:48), T (cid:48)(cid:48)] are corrupted as well such
that only the part proportional to T (cid:48) − Ti, the avoidable loss, can
be subject to minimization, as shown in Figure 2.

Figure 2: Unavoidable cost for attack with T (cid:48) ∈ [T2, T3]. Source:
[3]

∇θµ J(π) ≈ Est∼ρµ[∇θµ Qµ(s, a|θQ)|s=st,a=µ(st|θµ)]

(4)

4.1.3 Threat Model

4 Backup Strategy as Markov Decision process

In this section we show how the problem can be formulated
as Markov decision process (MDP). In the following we ﬁrst
summarize the original formalization and important concepts
introduced by [2] and [3], then transform them to the domain of
MDPs by deﬁning scale invariant state and action spaces, the
transition dynamics and the reward function .

4.1 Formalization of the backup problem

A backup scheme based on k storage devices is fully determined
by an inﬁnite sequence of update actions (dn, tn), where tn de-
notes the time when the nth backup is written on the storage

It is the attacker’s objective to maximize the defender’s recovery
cost, either to increase the ransom that can be demanded or
to increase the damage to the defender’s assets. Due to the
information asymmetry, additional cost can be inﬂicted on the
defender by having the infection time T (cid:48) just before the next
scheduled update Tk+1 is performed. Moreover, this extra cost
is maximal if [Tk, Tk+1] happens to be the largest interval in the
snapshot once T (cid:48)(cid:48) is reached.

Hence, the achievable additional cost depends on the attacker’s
knowledge of the backup schedule. For an attacker from the
outside it is reasonable to assume that T (cid:48) is uniformly distributed
within every given interval [Ti, Ti+1] due to lack of knowledge of
the schedule, uncertainties through the infection vector, or both.
For an insider, on the other hand, the distribution will be biased

Old:t0T1T2T3TkTNew:t0T1T2Tk−1Tkt0T1T2T3Tk×T′T′′UnavoidablecostActualcostPreprint – Deep Reinforcement Learning for Backup Strategies against Adversaries

towards Ti+1 and will be most likely concentrated immediately
before Ti+1 for an attacker with perfect knowledge.

In any case, the defender seeks to minimize this additional ex-
posure but has to trade oﬀ strict worst case exposure guarantees
for lower average exposure depending on the expected attacker
knowledge. Intuitively, this means that for attackers with limited
knowledge, the defender will achieve a lower loss on average
than if he had committed to the bounded worst-cast scheme. In
the case of a perfect knowledge attacker the loss will of course
be lower for the latter scheme.

4.1.4 Discrepancy and q-eﬃciency

From the perspective of the defender, the worst case avoidable
loss is realized if an attack occurs with T (cid:48) located right before
the end of the longest interval of any given snapshot S . Since the
longest interval is minimal for snapshots with exactly uniformly
distributed update times, the optimal strategy for the defender is
to keep the snapshots uniform at all time. However, due to the
fact that only one backup can be replaced per update action, this
is not possible for k > 2: Starting from an initially uniformly
distributed snapshot, any update action will result in a snapshot
with an interval at least twice as long as all others.

Thus, deviations from the uniform distribution are unavoidable
which links the problem to discrepancy theory and leads to the
important concepts of q-compliance and q-eﬃciency. An interval
deﬁned by update times Ti − Ti−1 is q-compliant at time T if
Ti −Ti−1 ≤ q T
k . As a consequence, the q-value can be interpreted
as the ratio between actual and ideal interval length. Finally, a
backup scheme is q-eﬃcient if all intervals in all its snapshots
are q-compliant. It is clear that this natural discrepancy metric q
needs to be minimized to obtain a good backup scheme.

Finally, it must be discussed if assuming the presented security
threat model and minimizing exposure with respect to the just
introduced metric will have adverse eﬀects on safety threats such
as naturally occurring data loss due to disk crash, ﬁre or similar
natural hazards. However, in this case we just have T (cid:48) = T (cid:48)(cid:48)
and
the exposure reduces to the length of just the last interval. Since
the schemes try to keep the snapshots as uniform as possible,
this will naturally hold also for the last interval: In half of the
cases the last interval might a bit longer than the other intervals
in the snapshot and in the half a bit shorter such that on average
the scheme is neutral with respect to safety threats.

4.2 State Space

4

(5)

following state deﬁnition.

S :=





(S 1, S 2, . . . , S k) ∈ (0, 1)k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

k(cid:88)

i=1



S i = 1


.

With this state representation, S i = Ti − Ti−1 (with T0 := 0)
represents the time period between two consecutive backups,
normalized to the unit interval. After each update action, the
vector is renormalized such that its components sum to one
again. This avoids endlessly growing numbers and also ensures
that the state-based reward calculation always produces rewards
on the same scale.

4.3 Action Space

The available actions also follow directly from the problem
statement and just need to undergo a consistent scaling scheme.
The action space is the product of a discrete and a continuous
space:

A := Ad × Ac = {a = (d, D) ∈ {0, 1, . . . , k − 1} × (1, 2]}

(6)

For every step one has to to decide which (discrete action) old
backup will be overwritten and when (continuous action) it will
be overwritten. We restrict the discrete choice to the devices with
label d ∈ {0, 1, . . . , k − 1}, explicitly excluding the most recent
backup devices because, as also shown by [3], no q-eﬃcient
scheme will update it, which is also intuitively clear.

The continuous action parameter D is restricted to the domain
(1, 2] and can be interpreted as the rescaling factor for the next
state. This restriction is not very strict, because choosing a next
step of 2 results in a state where the last interval in the snapshot
is as big as the union of all remaining intervals, consequently
leading to a very non-uniformly distributed state in the next step.

4.4 Transitions

Finally, the transition to a new state is performed by merging the
dth interval of S with its successor, appending the new time step
D at the end and subsequently dividing all other components by
D:

Pa
S S (cid:48) : S × A → S
(S , a) (cid:55)→ P(S , a) = S (cid:48) = (S (cid:48)
S d + S d+1
D

(cid:18) S 1
D

S (cid:48) =

, . . . ,

1, S (cid:48)

2, . . . , S (cid:48)
k)
S d+2
D

, . . . ,

S k
D

,

(7)

(cid:19)
, (D − 1)

.

The transition dynamics deﬁned this way are completely deter-
ministic.

The concept of the snapshot translates almost directly to the
concept of a state in a Markov decision process (MDP). However,
some adjustments are necessary since the times speciﬁed in the
snapshot are unbounded which leads to conceptual problems in
the design of the reward function as well as numerical problems
concerning the implementation.

With k given backup devices and the goal of keeping the backups
as uniformly distributed as possible the time periods between
new updates grow basically at the same rate as the periods
between backup times that are part of the snapshot. Therefore,
we consider the problem scale-invariant which motivates the

4.5 Reward Functions

Since the q-value introduced in the notion of q-compliant snap-
shots measures the deviation from equidistribution (i.e.
the
discrepancy) of the update time sequence, we will base the re-
ward function on the inverse of the q value in order to encourage
low discrepancy throughout the updates:

˜r : S → R,

˜r(S ) =

1
k maxi S i

∈

(cid:21)

, 1

(cid:18) 1
k

(8)

To improve stability of learning and make rewards for diﬀerent
values comparable, we subsequently scale rewards to (−λ, λ]

Preprint – Deep Reinforcement Learning for Backup Strategies against Adversaries

5

using a hyperparameter λ > 0 such that we arrive at

r(s, λ) = −λ + 2λ
k − 1

(cid:32)

1
maxi S i

(cid:33)
− 1

.

ﬁrst k − 1 units, representing the relaxed discrete actions, and
sigmoid activation for the last unit, representing the continuous
step size.

(9)

Finally, since optimal policies for an MDP are those which
minimize the expected (discounted) reward, there is a subtle
diﬀerence in the optimization objective compared to [3]. The
algorithm will not strictly stay below a given discrepancy bound,
but minimize the mean discrepancy of the resulting backup
scheme.

5 Implementation

In this section we present details on the implementation of the
proposed algorithm. Since the algorithm is based on DDPG, we
start by showing how we incorporate the hybrid action space
and the resulting implications for discrete exploration this brings
about. Finally, we show the overall network structure and the
employed stability-improving techniques. An overview of all
hyper parameters and their respective values for our experiments
can be found in Table 3.

5.1 Discrete Action Relaxation and Exploration

The DDPG algorithm is designed to output continuous actions.
To tackle the problem of the discrete part of the hybrid action
space, we encode the k − 1 diﬀerent actions using one-hot vec-
tors and subsequently relax the problem by allowing continuous
values a(d)
i ∈ [0, 1] in each component of the vector. The normal-
ized vector can then be interpreted as a probability distribution
P over the discrete actions a(d)
i ∈ {0, 1, . . . , k − 2} which is easily
produced, e.g. by the actor network using softmax activation for
the output layer.

At this point it is possible to choose the next action d stochasti-
cally by sampling from the distribution induced by a(d). Never-
theless, since the overall model of the problem is deterministic,
we decided to choose d deterministically by d = arg maxi a(d)
.
i
To ensure suﬃcient exploration, on the other hand, we make the
discrete policy ε-greedy and sample uniformly from the discrete
action space in each exploration step.

Concerning the input, the k-dimensional state vector is passed to
the ﬁrst layer of both networks. For the critic network, however,
there are additional fully connected layers with 64 units and
ReLU activation and 64 units and linear activation that prepro-
cess the state input, as well as a fully connected layer with 64
units which takes the k-dimensional relaxed action represen-
tation as input. The output of those two preprocessing layers
are subsequently concatenated and passed to the core network
described above.

Since the number of units in the hidden layer is rather small,
we also experimented with some variations of this architecture,
such as fully connected layers with up 512 units and also tried to
improve the training process using priority replay [14] or Monte
Carlos tree search algorithms [15]. Nevertheless, none of these
modiﬁcations lead to a signiﬁcant improvement of the resulting
policy or stability of the training.

Figure 3: Architecture of actor and critic networks.

5.2 Continuous exploration

5.4

Stability

Concerning exploration in the intrinsically continuous action
space, we use the following stochastic exploration policy,

π(st|θµ

t ) := µ(cid:48)(st|θµ

t ) = µ(st|θµ

t ) + N,

(10)

where the noise distribution N is given by an Ornstein-
Uhlenbeck process with parameters θ = 0.3 and σ = 0.1 as
suggested by [10].

5.3 Network Architecture

The network architecture is shown in Figure 3 and follows the
actor-critic paradigm. The core of both actor and critic network
consists of two fully connected layers with each 64 units using
rectiﬁed linear activation functions (ReLUs). The output layer
of the critic network has k units with linear activations, whereas
for the actor network the output uses softmax activation for the

To achieve a stable and convergent learning processes we employ
the following techniques. First, as already shown in equation
(9), we transform all rewards to the same scale (−λ, λ]. In our
experiments a value of λ = 5 leads to a stable learning process
across all k, while for larger values the softmax output often got
stuck producing very low entropy discrete action distributions
resulting in update actions that would always update the same
device.

Second, we use target networks for both the actor and the critic
networks, where weights are updated towards the original net-
works’ weights with some lag parameter τ = 0.001 according to
θQ(cid:48),µ(cid:48) = τθQ,µ + (1 − τ)θQ(cid:48),µ(cid:48)
, as suggested by [10]. Finally, again
following the approach in [10], a replay buﬀer of size 1000000 is
used, from which mini-batches of size 32 are uniformly sampled
to update the networks.

FC 64ReLUFC 64ReLUFC 1SigmoidFC k-1SoftmaxContinuous ActionContinuous ActionRelaxed Discrete ActionsAction ValuesContinuous ActionStateStateRelaxed DiscreteActionsActor NetworkTrainingFC 64ReLUFC 64ReLUFC kLinearFC 64ReLUFC 64LinearFC 64LinearCritic NetworkPreprint – Deep Reinforcement Learning for Backup Strategies against Adversaries

6

6 Results

In this section we present the results of applying our algorithm
to the backup problem for some values of k and compare them
to schemes suggested in [3].

6.1 Evaluation Method

A straightforward way of evaluating the scheme would be a
simulation where we just sample various attack times T (cid:48) , T (cid:48)(cid:48)
and
measure the resulting (avoidable) data loss for multiple runs of
the backup scheme. However, this would result in measurements
where the maximally possible damage inﬂicted on the defender
is not realized most of the times which makes the algorithms
hard to compare. Since we are comparing against a worst-case
guarantee baseline algorithm, we hence wish to make the setup
as adversarial as possible.

Taking the threat model described in Section 4.1.3 into account,
this means that a worst case (perfect knowledge) attack (i.e. T (cid:48)
happened just before the end of the largest interval in the current
snapshot) is assumed to become known (i.e. T (cid:48)(cid:48)
is revealed) after
each update action. The most important metrics in this evalua-
tion are the q-values which measure discrepancy and therefore
the maximal extra cost inﬂicted on a defender. The q-values are
computed for every snapshot resulting from an update action
and then aggregated to mean q-values ( ¯q) and max q-values
(max q). Consequently, max q measures the maximal extra cost
over the whole run time of the scheme where the attacker can
choose the worst interval in the worst snapshot. Put diﬀerently,
when looking at all sets of k consecutive backups, what was
the worst additional loss that could have occurred. ¯q measures
the average extra costs where the attacker can only choose the
worst interval in some snapshot, i.e. if k consecutive backups
are chosen at random, what is additional loss that could have
occurred on average.

The evaluation of the algorithms are performed by running the
resulting schemes with 250 update actions and collecting in-
formation on the state sequence, update actions (step size and
backup device) and the obtained reward in order to compute
summary statistics. Since the initial state is always uniformly
distributed, which, as discussed, leads to high q-values in the
following step(s), we exclude the ﬁrst 50 actions from the maxi-
mum computation. The validation run results for the proposed
algorithm are displayed in Table 2, whereas the corresponding
values for the q-eﬃcient schemes from [3] are displayed in Table
1. The columns report mean reward r, mean discrepancy q, max-
imum discrepancy max q, mean step size D and the sequence of
update devices.

6.2 Discussion

The ﬁrst thing that becomes immediately apparent is that both
algorithm produce almost identical schemes in terms of backup
device sequence and step size for k = 2, 3, 4, 5. In particular, for
the case k = 3 for which it was proven in [3] that the step size
√
must be equal to the golden ratio 1+
5
2 ≈ 1.618, we can also
observe convergence to this value throughout the learning. The
only signiﬁcant deviation in performance between the algorithms
happens for the case k = 4. In this case, mean reward and
consequently mean q-value are much better for the learning

r

q (max q)

D

Sequence

k

2
3
4
5
6
7
8

5.000
3.086
2.477
2.689
2.289
2.316
2.259

1.500 (1.500)
1.528 (1.528)
1.540 (1.540)
1.471 (1.471)
1.506 (1.513)
1.484 (1.497)
1.478 (1.485)

2.000
1.618
1.346
1.325
1.242
1.209
1.161

9
10

2.337
2.189

1.455 (1.475)
1.469 (1.474)

1.163
1.133

11

2.183

1.462 (1.466)

1.124

(0)
(0)
(0, 2)
(0, 2)
(0, 1, 2, 0, 2, 4)
(0, 2, 3, 0, 4, 2)
(0, 1, 3, 6, 4, 2, 0, 6,
4, 2, 6, 0, 3, 1, 3, 4)
(0, 4, 2, 4, 0, 4, 5, 2)
(0, 4, 2, 4, 0, 4, 5, 2,
0, 4, 8, 2, 4, 8)
(0, 5, 2, 5, 8, 0, 5, 1,
2, 4, 5, 0, 5, 1, 9, 5,
2, 5)

Table 1: Results for schemes from [3] (reference method).

algorithm, at the cost of a higher max q value. Hence, this is the
ﬁrst instance, where the trade-oﬀ between mean q-values and
maximum q value can be observed.
Next, for all k > 5 similar eﬀects can be observed. The proposed
algorithm performs consistently better for all the mean metrics
at the cost of a higher maximum q-value. We can also observe
that for these larger values of k, our algorithm produces diﬀerent
update device sequences. For most of the produced sequences a
periodic pattern can be observed, however others do not show
periodicity within a series of 25 consecutive updates. The case
k = 13 produces the worst results in terms of mean and maxi-
mum discrepancy because it did not converge to a meaningful
sequence which must, as also shown by [3], contain the oldest
backup device with label 0.
Finally, the aforementioned trade-oﬀ between optimizing for
worst-case and average case is also present in many other ﬁelds
and often average-case optimality is preferred: Quicksort and
the Simplex algorithm are just two examples of algorithms with
great average case performance in their respective ﬁelds that
are employed very often in practice despite their non-optimality
in the worst case. Bringing the discussion back to applications
in the security on might ask what knowledge and capabilities
attacker most likely has with respect to the threat model dis-
cussed in Section 4.1.3. An almost perfect knowledge attack
occurs most likely for the case of an advanced persistent threat
or an insider with elevated privileges and in this case the whole
backup infrastructure along with all the backups might very well
be also within the attacker’s reach such that the task of mini-
mizing avoidable cost becomes ultimately obsolete. However,
the success of recent attacks based on ransomware cryptoworms
such as WannaCry or NotPetya may indicate that rather than
one perfectly executed fatal attack, the dominant attack pat-
tern is actually multiple limited knowledge or even automated
zero-knowledge attacks. In this case, again, better average case
performance might be more suitable.

Preprint – Deep Reinforcement Learning for Backup Strategies against Adversaries

7

r

q (max q)

D

Sequence

k

2
3
4
5
6
7
8
9
10
11
12

5.000
3.084
3.309
2.688
2.968
2.522
2.758
2.430
2.692
2.239
2.315

1.500 (1.500)
1.528 (1.528)
1.460 (1.664)
1.472 (1.473)
1.442 (1.753)
1.456 (1.582)
1.435 (1.766)
1.445 (1.578)
1.432 (1.838)
1.473 (1.656)
1.453 (1.685)

2.000
1.618
1.415
1.324
1.262
1.222
1.188
1.166
1.148
1.139
1.121

13

1.114

1.704 (1.928)

1.135

14

2.257

1.456 (1.981)

1.091

15

1.987

1.494 (1.766)

1.099

16
17

1.959
1.745

1.497 (1.768)
1.526 (1.754)

1.091
1.087

18

1.653

1.558 (1.996)

1.084

19

1.414

1.611 (1.809)

1.078

20

1.517

1.582 (1.992)

1.074

(0)
(0)
(0, 2)
(0, 2)
(0, 2, 2)
(0, 3, 1, 2, 3, 0, 3)
(0, 4, 2, 4)
(0, 2, 3)
(0, 5, 3, 2, 3)
(0, 1, 4, 4, 5)
(0, 2, 5, 6, 0, 6, 1, 6,
2, 6, 2, 6)
(6, 4, 6, 2, 6, 6, 4, 6,
2, 6)
(6, 4, 6, 2, 0, 6, 4, 6,
2, 6, 0, 6, 2, 6, 3, 6,
0, 6, 2, 3, 4, 0, 6, 4,
12)†
(0, 7, 4, 7, 0, 4, 7, 1,
7, 4, 7, 1, 7, 4, 7, 0,
7, 4, 7, 0, 4, 7, 0, 7,
4)†
(0, 8, 2, 5, 8, 2, 8, 5)
(0, 8, 3, 8, 3, 8, 0, 8,
3, 8)
(8, 2, 8, 8, 0, 8, 2, 8,
2, 8, 0, 8, 2, 8, 2, 8,
8, 2, 8, 0, 8, 2, 8, 2,
8)†
(0, 7, 8, 4, 0, 7, 8, 4,
7)
(8, 3, 0, 7, 7, 3, 7, 8,
0, 7, 3, 7, 8, 8, 3, 0,
7, 3, 7, 7, 8, 3, 7, 0,
7)†

Table 2: Results for the learned policy (proposed method).

7 Conclusion and Future Work

In this paper we formalize the backup strategy against adver-
saries problem as a Markov decision process model that can be
eﬃciently solved using reinforcement learning techniques.

By using the q-value, a natural discrepancy metric, as basis
for our reward function we suggest a learning algorithm based
on deep deterministic policy gradients that learns to minimize
discrepancy over the run time of the backup scheme.

In experiments with our prototype implementation we demon-
strate that our suggested algorithm produces almost identical
backup schemes for k < 5, while obtaining better mean reward
and q-value scores for higher k. Due to the universal function
approximation property of the policy network, the algorithm is
not just a ﬁxed set of update rules and step sizes but can also

deal with deviations from the schedule (as they might occur in
real world scenarios) and is able to dynamically drive the backup
scheme back to the optimal path.

We think there is great potential in the mathematical modelling
and formalization of problems in IT Security and have provided
one example how reinforcement learning algorithms and conse-
quently (semi-)autonomous agents could help to guide planning
and security processes in the future.

Acknowledgements

We thank Bar-On et al. [3] for providing us with their implementation
and in particular Rani Hod for valuable information on how to use it.

References

[1] R. Upadhyaya and A. Jain. Cyber ethics and cyber crime:
A deep dwelved study into legality, ransomware, under-
ground web and bitcoin wallet. In 2016 International Con-
ference on Computing, Communication and Automation
(ICCCA), pages 143–148, April 2016.

[2] Karl Bringmann, Benjamin Doerr, Adrian Neumann, and
Jakub Sliacan. Online checkpointing with improved worst-
case guarantees. volume 27, 02 2013.

[3] Achiya Bar-On, Itai Dinur, Orr Dunkelman, Rani Hod,
Nathan Keller, Eyal Ronen, and Adi Shamir. Tight Bounds
In 45th Interna-
on Online Checkpointing Algorithms.
tional Colloquium on Automata, Languages, and Program-
ming (ICALP 2018), volume 107, pages 13:1–13:13, 2018.

[4] John W Young. A ﬁrst order approximation to the optimum
checkpoint interval. Communications of the ACM, 17(9):
530–531, 1974.

[5] Erol Gelenbe. On the optimum checkpoint interval. Jour-

nal of the ACM, 26(2):259–270, 1979.

[6] S Nakamura, C Qian, S Fukumoto, and T Nakagawa. Opti-
mal backup policy for a database system with incremental
and full backups. Mathematical and computer modelling,
38(11-13):1373–1379, 2003.

[7] Sung-Hwa Lim, Se Won Lee, Byoung-Hoon Lee, and
Seongil Lee. Power-aware optimal checkpoint intervals
for mobile consumer devices. IEEE Transactions on Con-
sumer Electronics, 57(4):1637–1645, 2011.

[8] József Beck. Irregularities of distribution. i. Acta Mathe-

matica, 159(1):1–49, 1987.

[9] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,
Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and
Daan Wierstra. Continuous control with deep reinforce-
ment learning. 2016. URL http://arxiv.org/abs/
1509.02971.

[10] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex
Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A.
Riedmiller. Playing atari with deep reinforcement learning.
CoRR, abs/1312.5602, 2013. URL http://arxiv.org/
abs/1312.5602.

Preprint – Deep Reinforcement Learning for Backup Strategies against Adversaries

8

[11] Richard S. Sutton, David McAllester, Satinder Singh, and
Yishay Mansour. Policy gradient methods for reinforce-
ment learning with function approximation. In Proceed-
ings of the 12th International Conference on Neural Infor-
mation Processing Systems, NIPS’99, pages 1057–1063,
Cambridge, MA, USA, 1999. MIT Press. URL http://
dl.acm.org/citation.cfm?id=3009657.3009806.

[12] Matthew Hausknecht and Peter Stone. Deep reinforcement
learning in parameterized action space. In Proceedings of
the International Conference on Learning Representations
(ICLR), May 2016.

[13] David Silver, Guy Lever, Nicolas Heess, Thomas Degris,
Daan Wierstra, and Martin A. Riedmiller. Deterministic
policy gradient algorithms. In ICML, volume 32 of JMLR
Workshop and Conference Proceedings, pages 387–395.
JMLR.org, 2014. URL http://dblp.uni-trier.de/
db/conf/icml/icml2014.html#SilverLHDWR14.
[14] Tom Schaul, John Quan, Ioannis Antonoglou, and David
Silver. Prioritized experience replay. In 4th International
Conference on Learning Representations, ICLR 2016, San
Juan, Puerto Rico, May 2-4, 2016, Conference Track Pro-
ceedings, 2016. URL http://arxiv.org/abs/1511.
05952.

[15] Cameron B Browne, Edward Powley, Daniel Whitehouse,
Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen,
Stephen Tavener, Diego Perez, Spyridon Samothrakis, and
Simon Colton. A survey of monte carlo tree search meth-
ods. IEEE Transactions on Computational Intelligence
and AI in games, 4(1):1–43, 2012.

[16] François Chollet et al. Keras. https://keras.io, 2015.

Preprint – Deep Reinforcement Learning for Backup Strategies against Adversaries

9

Parameter Description

Neps
Nsteps
λ
lr
la
τ
γ
(cid:15)d,min

∆(cid:15)d
(cid:15)c,min

∆(cid:15)c
µOU
θOU
σOU
NB
Nb

number of episodes
number of steps/actions per episode
reward scale
learning rate critic
learning rate actor
lag parameter target update
reward discount rate
minimum epsilon for
(cid:15)-greedy exploration (discrete)
epsilon increments for linear annealing
minimum epsilon for
continuous exploration (noise scale)
epsilon increments for linear annealing
Ornstein-Uhlenbeck mean
Ornstein-Uhlenbeck mean reversion speed
Ornstein-Uhlenbeck standard deviation
size of the replay buﬀer
Minibatch Size

Table 3: Hyperparameters

Value

600
10000
5.0
0.001
0.0001
0.001
0.99

0.1
1/(3e6)

0.1
1/(3e6)
0.0
0.3
0.1
1000000
32

8 Experiments Details

The implementation is written in Keras [16] and follows the
actor, critic architecture. For training of both actor and critic the
Adam optimizer was used. Any additional hyperparameters are
collected in the Table 3.

