1
2
0
2

r
a

M
5
1

]
P
A

.
t
a
t
s
[

1
v
0
5
4
8
0
.
3
0
1
2
:
v
i
X
r
a

Modeling Multivariate Cyber Risks: Deep Learning

Dating Extreme Value Theory

Mingyue Zhang Wu(cid:63)

Jinzhu Luo(cid:5) Xing Fang(cid:5) Maochao Xu(cid:63)(cid:63)∗ Peng Zhao(cid:63)

(cid:63) School of Mathematics and Statistics, Jiangsu Normal University, China.
(cid:5) School of Information Technology, Illinois State University, USA.
(cid:63)(cid:63) Department of Mathematics, Illinois State University, USA

March 16, 2021

Abstract

Modeling cyber risks has been an important but challenging task in the domain of cyber

security. It is mainly because of the high dimensionality and heavy tails of risk patterns.

Those obstacles have hindered the development of statistical modeling of the multivariate

cyber risks. In this work, we propose a novel approach for modeling the multivariate cyber

risks which relies on the deep learning and extreme value theory. The proposed model

not only enjoys the high accurate point predictions via deep learning but also can provide

the satisfactory high quantile prediction via extreme value theory. The simulation study

shows that the proposed model can model the multivariate cybe risks very well and provide

satisfactory prediction performances. The empirical evidence based on real honeypot attack

data also shows that the proposed model has very satisfactory prediction performances.

Keywords Cyber attacks; GPD; heavy tail; high-dimensional dependence; LSTM.

1 Motivation and Introduction

Cyber risk has become one of the most emerging risks in recent decades which can cause disas-
trous consequences and tremendous monetary losses [13, 22, 26]. This is fundamentally caused
by the fact that cyberspace is extremely diﬃcult to secure because of its complexity (e.g., an at-
tacker can launch cyber attacks from anywhere in the world, and cyber systems have a very large
“surface” of software vulnerabilities that can be exploited to penetrate into them). What makes

∗Correspondence: mxu2@ilstu.edu

1

 
 
 
 
 
 
things worse is that cyber systems have become underlying pillars for various infrastructures
and physical systems. For example, there are growing concerns about cyber threats to critical
infrastructures because of the linkage between cyber and physical systems [21].
In addition,
cyber attacks causes billions of dollars in losses each year based on the research by the Ponemon
Institute and published by IBM Security [11]. In light of the risk and potential consequences of
cyber risks, securing cyberspace has become not only a mission of homeland security but also
an emerging task for private companies. This clearly calls for research into cyber risk modeling,
of which risk prediction is an important task.

In this work, we investigate a particular challenge that is encountered when modeling and
analyzing cyber risk, namely the modeling of multivariate cyber risks. This challenge is imposed
by the high-dimensional dependence among cyber attacks and extreme attacks during a short
time period. The dependence among cyber attacks is very common which is the nature of cyber
risks. For example, the Distributed Denial of Service (DDoS) attack, a large-scale DoS attack
where the perpetrator often uses thousands of hosts infected with malware to launch attacks
to a target or multi targets (e.g., ports, computers, severs), is in nature high-dimensional and
dependent. In particular, the DDos attacks are often extreme during a short time period. The
defender often needs to estimate or predict the cyber risks for the purpose of adjusting the
defense posture in practice. However, this is very challenging because of the high-dimensionality
and heavy tails (i.e., extreme attacks during a short period) exhibited by cyber attack data
[28, 29, 27, 22], which hinders the development of modeling multivariate cyber risks. Therefore,
in the literature of statistical modelings, there are only few studies investigating multivariate
[27] proposed a vine copula approach for modeling the
cyber risks. For example, Xu et al.
dependence among the time series of the number of cyber attacks, and the dependence between
the time series of the number of attacked computers. Peng et al. [22] developed a copula-GARCH
model, which also uses vine copulas to model the multivariate dependence among cyber attacks.
Ling et al.
[18] proposed a vector autoregression (VAR) approach to identifying a geospatial
and temporal patterns in the cyberattacks by considering the long range dependence.

In recent years, there is a growing interest in developing deep learning models for time series
forecasting [17, 5, 23]. For multivariate time series forecasting, Goel et al. [7] proposed a hybrid
R2N2 (residual recurrent neural networks) model, which ﬁrst models the time series with a simple
linear model (e.g., VAR) and then models its residual errors using recurrent neural networks
(RNNs). Lai et al. [16] developed a long- and short-term time-series network (LSTNet) to model
the multivariate time series. The main idea is to use the convolution neural network and the
recurrent neural network (RNN) to model short-term local dependence and long-term trends for
time series. Che et al. [1] studied a deep learning model based on gated recurrent unit (GRU)
for modeling multivariate time series with missing data. Wang et al. [25] presented an end-to-
end framework called deep prediction interval and point estimation, which can simultaneously
perform point estimation and uncertainty quantiﬁcation for multivariate time series forecasting.
The main approach is to model the loss function by penalizing the loss of point estimation and

2

the loss of prediction interval based on the deep learning model of long short-term memory
network. To the best of our knowledge, there is no deep learning framework for modeling the
multivariate cyber attack data in the domain of cyber security.

In this work, we propose a novel framework which is diﬀerent from those in the literature for
modeling and predicting the multivariate cyber attack data. Speciﬁcally, we develop a hybrid
model by combining deep learning and extreme value theory [4, 20]. That is, we ﬁrst model
the multivariate attack time series via a deep learning model which aims to capture the high-
dimensional dependence via the deep learning network. We then model the residuals exhibiting
heavy tail via the extreme value theory. The proposed approach not only can provide accurate
point prediction but also capture the tails very well (i.e., predicting the extreme attacks). This
novel framework is particularly useful in practice as it can guide the defender to prepare the
resource for both the regular attack (in terms of mean prediction) and worst attack (in terms of
tail prediction) scenarios.

The rest of the paper is organized as follows.

In Section 2, we present the preliminaries
on the deep learning model and extreme value theory. In Section 3, we discuss the proposed
the framework for model ﬁtting and prediction. Section 4 assesses the proposed approach via
simulation studies. In Section 5, we study two real honeypot attack data. In the ﬁnal section,
we conclude our results and present a discussion towards some limitations and future work.

2 Preliminaries

2.1 Long short-term memory (LSTM)

The LSTM network, a special kind of RNNs, introduced in [9] has been received remarkable
attention because of the ability to learn long term patterns in sequential data and tremendously
improve the prediction accuracy compared to other deep learning models. Therefore, this deep
learning model has been successfully applied in many areas [8].

Figure 1: A LSTM block at step t.

Figure 1 illustrates a LSTM block. Compared to regular RNN, the LSTM employs a

3

diﬀerent approach for the activation. Speciﬁcally, given time series data y1, . . . , yt where
yi = (y1,i, . . . , yn,i)(cid:62), i = 1, . . . , t, the activation ht of LSTM at step t is computed based
on four pieces: gate input (a.k.a, information gate), the forget gate, the output gate, and the
cell gate. The information gate input at step t is

it = σ(Wia · ht−1 + Wiy · yt),

where σ is the sigmoid function, and W’s are the parameter matrices of the network learned
during the training. The forget gate input and the output gate input are computed as

ft = σ(Wf a · ht−1 + Wf y · yt),

ot = σ(Woa · ht−1 + Woy · yt).

The cell gate input is computed as

Ct = ft (cid:12) Ct−1 + it (cid:12) kt,

where (cid:12) represents the element-wise Hadamard product, the Ct−1 is the cell state information
from the previous step, and kt is

where tanh is the hyperbolic tangent function. Finally, the activation at step t is computed as

kt = tanh(Wca · ht−1 + Wcy · yt)

ht = ot (cid:12) tanh(Ct),

where h(t) ∈ Rn is the ﬁnal cell output.

2.2 GRU and mLSTM

As the variants of LSTM, GRU [2] and Mulitplicative LSTM (mLSTM) [15] have also received
much attention recently due to their extraordinary prediction abilities. The GRU uses less
trainable parameters than LSTM, by having only two gates, the reset gate (rt) and the update
gate (zt). Its computational process is as follows

zt = σ(Wzy · yt + Wza · ht−1),

rt = σ(Wry · yt + Wra · ht−1),
ˆht = tanh(Why · yt + Wha(rt (cid:12) ht−1)),
ht = (1 − zt) (cid:12) ht−1 + zt (cid:12) ˆht.

Compared to the LSTM, the mLSTM simply replaces ht−1 with an intermediate state, mt, in
order to calculate it, ft, ot, and kt, where mt is

For more and detailed discussions on the deep learning models, please refer to [9, 8]

mt = Wma · ht−1 (cid:12) Wmy · yt.

4

3 Deep learning framework for multivariate cyber risks

Let yt = (y1,t, . . . , yn,t)(cid:62) be a vector of attack time series. In practice, the dimension of n can
be very large, and particularly, the dependence among cyber risks can be very nonlinear. This
motivates us to use the following approach for analyzing multivariate cyber risks, which utilizes
the advantage of high accurate point estimates of deep learning and the advantage of EVT for
modeling the high quantiles. Speciﬁcally, the proposed model is as follows,

yt = f (yt−1, . . . , yt−p) + (cid:15)t,

where (cid:15)t = ((cid:15)1,t, · · · , (cid:15)n,t)(cid:62) is an unobservable zero mean white noise vector process, and f is a
function by mapping the vector (yt−1, . . . , yt−p) to the mean vector of yt. Note that a special
case is the VAR,

yt = µ0 + A1yt−1 + . . . + Apyt−p + (cid:15)t,

where Ai are coeﬃcient matrices, i = 1, . . . , p, and µ0 is an n-dimensional constant vector. We
propose to model the mean of yt by

ˆyt = ˆf (yt−1, . . . , yt−p)

via deep learning approach, while the tails of yt are estimated by EVT approach.

The proposed framework involves the following three stages.

Stage 1: Capturing the multivariate dependence via deep learning

In multivariate time series modeling and prediction, the most challenging part is to ﬁnd a
suitable f for capturing the complex dependence pattern. Deep learning as an emerging tool for
multivariate time series modeling and forecasting has an extraordinary forecasting performance
[6, 7]. Therefore, we develop the following procedure for modeling the mean function f via deep
leaning.

Given time series y1, . . . , yt−1, we deﬁne p as the number of input data points in each of
them, and p also indicates the number of time steps to look back (i.e., lags). The prediction
process can be formally described as:

ˆyt = ˆf (Yt−1,p),

where Yt−1,p = (yt−1, . . . , yt−p) ∈ Rn×p is the input matrix. We apply the following objective
function to measure the error generated by the model,

J =

1
(T − t + 1)n

T
(cid:88)

i=t

||yi − ˆyi||2

2 + λ · ||W||2
2,

(3.1)

where λ is the tuning parameter, || · ||2
2 represents the squared L2 norm, and W represents all
the training parameters in the learning process. For instance, if the network is LSTM, then

5

W = {Wza, Wzy, Wra, Wry, Wha, Why}. The optimization is deﬁned as

W∗ = arg min
W

J,

which can be solved by using the gradient descent method [14]. To select the best deep learning
model, we perform a manual grid search by setting the hyper-parameters of the networks as
follows:

• Network models: M1-LSTM, M2-mLSTM, M3-GRU;

• Number of layers: {1, 2, 3}; Size of each layer: {16, 32, 64};

• Batch size: {5,10}; Bidirectional: {True, False}; p = {1, 2, 3, 4, 5};

• Penalty parameter: λ = {0.01, 0.001}; Initial learning rate: γ = {0.01, 0.001};

• Number of training epochs: {40, 50, 60, 70, 80}.

Algorithm 1 presents the detailed procedure for selecting the best deep learning model.

State 2: Modeling high quantiles via extreme value theory

After ﬁxing the deep learning model, the ﬁtted values at time t are

and the residuals are

ˆyt = ˆf (Yt−1,p),

et = yt − ˆyt,

where et = (e1,t, . . . , en,t)(cid:62). The second stage is to model the residuals by some statistical
distribution. Since the heavy tails are often observed in the attack data, we propose to model
the high quantiles of the residuals by the EVT approach. This is in principle in line with the
two-stage pseudo-maximum-likelihood approach in [19].

Recall that a popular EVT method is known as the peaks over threshold approach [20,
4]. Speciﬁcally, given a sequence of i.i.d. observations X1, . . . , Xn, the excesses Xi − µ with
respect to some suitably large threshold µ can be modeled by, under certain mild conditions,
the Generalized Pareto Distribution (GPD). The survival function of the GPD is:

¯G(x) = 1 − G(x) =






(cid:18)

1 + ξ

exp

(cid:19)−1/ξ

x − µ
σ
+
(cid:111)
x − µ
σ

−

(cid:110)

,

ξ (cid:54)= 0,

,

,

ξ = 0.

(3.2)

where x ≥ µ if ξ ∈ R+ and x ∈ [µ, µ − σ/ξ] if ξ ∈ R−, and ξ and σ are respectively called the
shape and scale parameters.

6

Algorithm 1 Algorithm for deep learning model training and selection.
INPUT: Historical time series data: training part {(t, yt)|t = 1, . . . , m}, and validation part
{(t, yt)|t = m + 1, . . . , T }; model M = {M1, M2, M3}; layers l = {1, 2, 3}; size s = {16, 32, 64};
bidirectional d = {True, False}; epoch b = {40, 50, 60, 70, 80}; λ = {0.01, 0.001}; A = φ.

1: for p ∈ {1, 2, 3, 4, 5}, r ∈ {5, 10} do

2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

Split the data set into mini-batch of size r;

for γ ∈ {0.01, 0.001}, λ ∈ {0.01, 0.001} do

for i ∈ {1, 2, 3}, d = {True, False} do

for l ∈ {1, 2, 3}, s ∈ {16, 32, 64}, b ∈ {40, 50, 60, 70, 80} do

Randomly initialize Mi based on l, s, and d, with all the parameters saved in W;

end for

j ← 0;

while j <= b do

for each batch from the training data do

Compute J by performing forward propagation using λ;

Update W using gradient descent based on J with the learning rate γ;

end for

Compute mean square error (MSE) by performing forward propagation on the

validation data;

if MSE is not dropping then

A ← Mi;
break;

end if

j ← j + 1;

end while

end for

end for

23: end for
24: return M ∗ ∈ A, where M ∗ has the lowest MSE;

OUTPUT: Deep learning model M ∗ with the corresponding parameters W ∗, and ﬁtted values
ˆyt, t = 1, . . . , T .

7

Stage 3: Prediction and and evaluations

After we determine the deep learning model and residual distribution, then we can use the
forward propagation to predict the mean

The q-quantile of yi,t+1 can be predicted as

ˆyt+1 = ˆf (Yt,p).

ˆyi,t+1,q = ˆyi,t+1 + G−1

i (q),

(3.3)

where G−1
i (q) is the q-quantile of Gi as deﬁned in Eq.
Algorithm 2 presents the detailed procedure for the prediction.

(3.2) for time series i, i = 1, . . . , n.

Algorithm 2 Algorithm for predicting the mean and high quantiles.
INPUT: Deep learning model M ∗ from Algorithm 1; ﬁtted values {(t, ˆyt)|t = 1, . . . , T };
training and validation data {(t, yt)|t = 1, . . . , T }; testing data {(t, yt)|t = T + 1, . . . , N }; q.

1: for j = T . . . , N − 1 do

2:

3:

4:

5:

Compute residuals et = yt − ˆyt, t = 1, . . . , j;
Fit Gi in Eq. (3.2) to ei,t by setting threshold µi, i = 1, . . . , n, t = 1, . . . , j;
Predict ˆyj+1 by performing forward propagation based on Model M ∗;
Predict the q-quantile of ˆyi,j+1,q by using Eq. (3.3);

6: end for

7: return {(t, yt)|t = T + 1, . . . , N };

OUTPUT: Predicted values ˆyt and q-quantile ˆyt,q, t = T + 1, . . . , N .

The prediction performance for the mean prediction are evaluated based on MSE and mean

absolute percentage error (MAPE) [10].

In order to assess the prediction accuracy of quantiles, we propose using the Value-at-Risk
(VaR) [20] metric because it is directly related to the quantities of interest. Recall that for
a random variable Xt, the VaR at level α for some 0 < α < 1 is deﬁned as VaRα(t) =
inf {l : P (Xt ≤ l) ≥ α}. An observed value that is greater than the predicted VaRα(t) is called
a violation, indicating inaccurate prediction.
In order to evaluate the prediction accuracy of
the VaR values, we adopt the following two popular tests [3]: (i) the unconditional coverage
test, denoted by LRuc, which evaluates whether or not the fraction of violations is signiﬁcantly
diﬀerent from the model’s violations; (ii) the conditional coverage test, denoted by LRcc, which
is a joint likelihood ratio test for the independence of violations and unconditional coverage.

8

4 Simulation study

In this section, we perform a simulation study on assessing the performance of proposed ap-
proach. To mimicking the attack data with heavy tails, we randomly generate two data sets
with size 5000 × 5 from the following models. In each experiment, the simulated data is split
into three parts: the ﬁrst 3500 is used for training, and the following 500 observations is used
for the validation. The testing data is the last 1000 observations.

• VAR with heavy tail. The VAR model is set as follows.

yt = µ0 + A1yt−1 + A2yt−2 + (cid:15)t,

where t = 1, . . . , 5000, µ0 = (µ1,0, . . . , µ5,0), yt = (y1,t, . . . , y5,t), A1 and A2 are 5 × 5
coeﬃcient matrices. In the experiment, A1 and A2 are generated from uniform distribution
[−0.2, 0.2], and µi,0 = 100, i = 1, . . . , 5. The error (cid:15)t = ((cid:15)1,t, . . . , (cid:15)5,t) has zero mean, and
(cid:15)i,t/σ, i = 1, . . . , 5, follows a skewed-t distribution with density as follows

g(z) =

2
ξ + ξ−1

(cid:2)tν(ξz)I(z < 0) + tνv (cid:0)ξ−1z(cid:1) I(z ≥ 0)(cid:3) ,

(4.1)

where I(·) is the indicator function, ξ > 0 is the skewness parameter, and

tν(z) =

Γ((ν + 1)/2)
√
νπΓ(ν/2)

(cid:2)1 + z2/ν(cid:3)−(ν+1)/2

with the shape parameter ν > 0. In the experiment, the parameters are set as σ = 20,
ξ = 1.5, and ν = 3.

Figure 2 plots the simulated time series, and we observe there exist large values which
indicate the heavy tail. We employ Algorithm 1 to train and validate the deep learning
model on the training and validation data sets. The residuals are used to ﬁt the distribution
G in Eq. (3.2) with the threshold setting to be .9-quantile of residuals for each time series
which is determined via the mean residual life plot [20].

The prediction is performed via Algorithm 2, where the threshold is set to .9-quantile of
residuals. The prediction performance is reported in Table 1. For the comparison purpose,
we also report the prediction performance of VAR, where the lag p is selected via AIC
criterion. The VAR model is considered as a benchmark model in the sequel discussion.
It is seen from Table 1 that the proposed model outperforms the benchmark model for
all the time series in terms of MAPE and MSE for the point prediction. Particularly, the
MSEs are signiﬁcantly improved by the deep learning approach.

For the quantiles prediction, we plot the violations for the proposed model and benchmark
model in Figure 3(a) and 3(d), respectively. It is seen that the observed number of viola-
tions are fairly close to the expected ones by the proposed approach from .95 to .98 levels.
The benchmark model underestimates the number of violations at the .95 level. For the

9

(a) Time series 1

(b) Time series 2

(c) Time series 3

(d) Time series 4

(e) Time series 5

Figure 2: Time series plots of simulated VAR with skewed-t tails.

Table 1: Prediction performances based on MAPE and MSE for the proposed model and bench-

mark model.

Series

Deep

Benchmark

MAPE

MSE

MAPE

MSE

0.1116

225.4169

0.1768

356.6563

0.1152

211.2983

0.1908

357.2598

0.0542

285.8806

0.0841

408.9762

0.0771

226.0736

0.123

382.4075

0.1716

191.3857

0.2839

378.5063

1

2

3

4

5

10

TimeNumber0100020003000400050000100200300400TimeNumber010002000300040005000−400−2000200400600800TimeNumber0100020003000400050000100200300400500TimeNumber010002000300040005000100200300400TimeNumber010002000300040005000−100−50050100150200250(a) Violations-Deep+EVT

(b) LRuc-Deep+EVT

(c) LRcc-Deep+EVT

(d) Violations-Benchmark

(e) LRuc-Benchmark

(f) LRcc-Benchmark

Figure 3: Numbers of violations and p-values of LRuc and LRcc tests for the proposed model and

benchmark model, where ’Exp.’ represents expected number of violations, and x-axis represents

the VaR level α.

11

0.950.960.970.980.99102030405060LevelNumberExp.123450.950.960.970.980.990.00.20.40.60.81.0LevelLRuc123450.950.960.970.980.990.00.20.40.60.81.0LevelLRcc123450.950.960.970.980.991020304050LevelNumberExp.123450.950.960.970.980.990.00.20.40.60.81.0LevelLRuc123450.950.960.970.980.990.00.20.40.60.81.0LevelLRcc12345.99 level, it is seen that the proposed approach underestimates the number of violations
for time series 4 and 5, which result in relatively smaller p-values of LRuc and LRcc tests
in Figures 3(b) and 3(c). For the benchmark model, we observe that it underestimates
four time series at level .99 from Figure 3(d).

We conclude that the proposed approach has an overall satisfactory prediction performance
for high quantiles from level .95 to .98.

• Copula+AR-GARCH with heavy tail. For this simulation, we assume that the mean part

follows AR(1) process

where i = 1, . . . , 5, and

Yi,t − µ = φ1(Yi,t−1 − µ) + (cid:15)i,t,

(cid:15)i,t = σi,tZi,t

with Zi,t being the innovations that are identically distributed with skewed-t density g(·)
in Eq. (4.1) and the dependence structure is speciﬁed via R-vine copula, and σt follows a
standard GARCH(1,1) model, i.e.,

i,t = w + α1(cid:15)2
σ2

i,t−1 + β1σ2

i,t−1.

In the experiment, the parameters are set as follows

(µ, φ1, w, α1, β1, ξ, ν) = (50, .6, .5, .05, .8, 1.5, 3).

To simulate the dependence among Zi,t, we generate 5-dimension multivariate uniform dis-
tribution, where the dependence structure is speciﬁed via the R-vine copula. Speciﬁcally,
the R-vine tree matrix is as follows



2 0 0 0 0










5 3 0 0 0

3 5 4 0 0

1 1 5 5 0

4 4 1 1 1












and the family matrix is set as












0 0 0 0 0

1 0 0 0 0

3 3 0 0 0

.

4 4 4 0 0

4 1 1 3 0












12

The parameter matrix is set as follows



0

0

0

0.2

0.9 1.1

0

0

0

1.5 1.6 1.9










0

0

0

0

0

0

0

0












.

3.9 0.9 0.5 4.8 0

A 5000×5 matrix from the above R-vine structure is generated, which is used for generating
the dependent Zi,t, i = 1, . . . , 5.

(a) Time series 1

(b) Time series 2

(c) Time series 3

(d) Time series 4

(e) Time series 5

Figure 4: Time series plots of simulated Copula+AR-GARCH with student-t tail.

The simulated time series plots are displayed in Figure 4, and the extreme values can also
be observed.

We again employ Algorithm 1 to train and validate the deep learning model, and the
It is discovered that the
residuals are used to ﬁt the GPD distribution in Eq.
threshold of 90th percentile of residuals can produce satisfactory ﬁtting performance for

(3.2).

13

TimeNumber01000200030004000500030405060TimeNumber0100020003000400050004045505560TimeNumber010002000300040005000354045505560TimeNumber010002000300040005000354045505560TimeNumber01000200030004000500030405060all the time series. Therefore, in the prediction procedure of Algorithm 2, the threshold is
ﬁxed at 90th percentile of residuals for each time series. Table 2 presents the performances

Table 2: Prediction performances based on MAPE and MSE for the proposed model and ben-

markmodel.

Series

Deep

Benchmark

MAPE MSE MAPE MSE

0.0185

2.4612

0.032

4.8339

0.0189

2.3415

0.0324

4.6922

0.0198

2.5678

0.0325

4.7743

0.0190

2.2672

0.0313

4.3606

0.0183

2.3662

0.0323

4.7806

1

2

3

4

5

of point predictions for both proposed model and benchmark model. It is seen that the
proposed model can signiﬁcantly outperform the benchmark model in terms of both metrics
for all the time series.

For the high quantiles, the number of violations are displayed in Figures 5(a) and 5(d). It
is seen that the proposed model has a very satisfactory prediction performance compared
to that of benchmark model. The large p-values of LRuc and LRcc tests in Figures 5(b)
and 5(c) compared to those in 5(e) and 5(f) also conﬁrm that the proposed approach has
an accurate prediction performance for the high quantiles.

To conclude, the proposed model has satisfactory prediction performances for both the point
prediction and high quantile prediction. It can signiﬁcantly outperform the benchmark model.

5 Empirical study

In this section, we study two real attack data which are collected by the honeypot instrument
[24].

5.1 Honeypot data I with 9 dimensions

This honeypot data is publicly available on the web [12], which was collected via Amazon
Web Service (AWS) virtual honeypots across the world. The dataset has 9 honeypot hosts
(EU, Oregon, Singapore, SA, Tokyo, Norcal1, Norcal2, US-East, Sydney), and the attacks were
recorded between 03/03/2013 to 09/08/2013, which includes 451,581 events. The recorded attack
data includes attack time, targeted host, attackers’ IP addresses and origin countries. This

14

(a) Violations-Deep+EVT

(b) LRuc-Deep+EVT

(c) LRcc-Deep+EVT

(d) Violations-Benchmark

(e) LRuc-Benchmark

(f) LRc-Benchmark

Figure 5: Numbers of violations and p-values of LRuc and LRcc tests for proposed model, and

true model.

15

0.950.960.970.980.99102030405060LevelNumberExp.123450.950.960.970.980.990.00.20.40.60.81.0LevelLRuc123450.950.960.970.980.990.00.20.40.60.81.0LevelLRcc123450.950.960.970.980.991020304050LevelNumberExp.123450.950.960.970.980.990.00.20.40.60.81.0LevelLRuc123450.950.960.970.980.990.00.20.40.60.81.0LevelLRcc12345dataset was discussed in [18] based on the daily aggregation, where the long range dependence
model was discovered and incorporated into the modeling process.

Daily aggregation. The daily aggregated data has 188 observations, we leave the last 38
observations as the prediction evaluation as that in [18] where the predicted MSE is reported.
We employ Algorithm 1 to build the deep learning model where the training data has 100
observations, and validation data has 50 observations. The prediction performance based on
Algorithm 2 is reported in Table 3. The proposed deep learning has a satisfactory prediction

Table 3: Prediction performances for log-transformed daily aggregated honeypot attack data.

Series

Deep

LRD+VAR in [18]

MAPE MSE

EU

0.0685

0.2058

Oregon

0.0118

0.0096

Singapore

0.0144

0.0145

SA

0.0284

0.0319

Tokyo

0.0418

0.4225

Norcal1

0.0262

0.0282

Norcal2

0.0287

0.0358

US-East

0.0666

0.3726

Sydney

0.0554

0.1126

MSE

0.2540

0.0138

0.0133

0.0567

0.4733

0.0445

0.0692

0.3848

0.1374

performance. In particular, it outperforms the approach in [18] for all the attack time series
except for Singapore based on MSEs (0.0145 vs 0.0133). Since the daily aggregated time series
only has a total of 188 observations for each host, the high quantile predictions are not performed.

Hourly aggregation.
In practice, the network defender is often interested in assessing the
hourly attacks [28, 29, 22]. In this section, we study the performance of proposed model based on
the hourly aggregated data which has 4512 observations for each host. The hourly aggregated
time series are plotted in Figure 6.
It is observed that there are extreme attacks for all the
hosts. Particularly, there exist tremendous numbers of attacks during some short periods for
Oregon, Singapore, and Tokyo hosts. The data is split into three parts: training with 3500
observations, validation with 500 observations, and the last 512 observations are used for the
prediction evaluation. The Algorithm 1 is employed on the training and validation datasets to
select the best deep learning model, and Algorithm 2 is used for the prediction.

16

(a) EU

(b) Oregon

(c) Singapore

(d) SA

(e) Tokyo

(f) Norcal1

(g) Norcal2

(h) US-East

(i) Sydney

Figure 6: Time series plots of hourly aggregated attack data for all the hosts.

17

TimeNumber01000200030004000010203040TimeNumber010002000300040000100200300400500TimeNumber010002000300040000100200300400500600TimeNumber010002000300040000102030405060TimeNumber010002000300040000200040006000800010000TimeNumber0100020003000400001020304050TimeNumber010002000300040000204060TimeNumber01000200030004000050100150200TimeNumber01000200030004000050100150200The prediction performance is reported in Table 4. For the comparison purpose, the predic-
tion performance of benchmark VAR model is also reported. For the proposed approach, we can
observe that it can signiﬁcantly outperform the benchmark model in terms of MAPE metric.
For the MSE metric, it is seen that the proposed approach can also outperform the bencmark
model for all the time series except for SA (23.7633 vs 14.6417). For Tokyo time series, it is
seen that the MSEs are very large for both models. This is mainly because of one extreme
observation which can not be predicted well.

Table 4: Prediction performances for daily aggregated honeypot attack data.

Series

Deep

Benchmark

MAPE

MSE

MAPE

MSE

EU

0.3067

15.5700

0.4055

15.7607

Oregon

0.1826

34.6819

0.2446

35.9174

Singapore

0.1756

29.5333

0.2372

34.3774

SA

0.3248

23.7633

0.4643

14.6417

Tokyo

0.6727

230565.6248

1.0707

321396.0317

Norcal1

0.3225

12.0416

0.4668

16.1691

Norcal2

0.3568

23.3879

0.4857

25.7881

US-East

0.4124

12.4906

0.5408

18.9909

Sydney

0.2916

13.6534

0.4649

21.834

For the high quantile prediction, we study the proposed model at the aggregate level. Specif-
ically, we take the log-transformation on both the observations and aggregated ﬁtted values on
the training and validation dataset, and calculate the residuals,

et = log (st) − log (ˆst) ,

i=1 yi, ˆst = (cid:80)t

where st = (cid:80)t
i=1 yi, and t = 1, . . . , 4000. The GPD distribution is then ﬁtted
to {et}t=1,...,4000 with the threshold of .9-quantile of {et}t=1,...,4000. The estimated parameters
are ξ = 0.3710 with standard error 0.0827, and σ = 0.0981 with standard error 0.0100, which
are signiﬁcant. The QQ- and PP- plots are displayed in Figure 7. It is seen that PP- plot is
satisfactory. However, the tails of QQ-plot is oﬀ the diagonal line, which is mainly caused by
the extreme large values in the attack data. Although there are several points oﬀ the diagonal
line, we still use the ﬁtted GPD distribution to predict the high quantiles as it does not aﬀect
the main conclusion.

18

(a) QQ-plot

(b) PP-plot

Figure 7: QQ- and pp- plots of honeypot data ﬁtted by the GPD distribution.

Algorithm 2 is employed on the attack data at the aggregated level. The assessment of
predicted ˆst, t = 4001, . . . , 4512 is shown in Table 5. It is seen that the prediction performance
is very satisfactory. The predicted numbers of violations are quite close to the true numbers
of violations. They pass all the tests based on LRuc and LRcc tests except that at .98 level of
LRcc test is relatively small (0.0241).

Table 5: The p-values of the VaR tests of the predicted violations for α = .92, .94, .94, .96, .98.

‘Ob.’ represents the observed number of violations and ‘Exp.’ represents the expected number

of violations.

α

Exp. Ob.

LRuc

LRcc

.92

.94

.95

.96

.98

41

31

26

21

10

41

31

23

17

9

0.9948

.7118

0.9585

0.9944

0.5919

0.5828

0.4192

0.2124

0.6894

0.0241

We conclude that the proposed approach has an overall satisfactory prediction performance.

19

0.51.01.52.00.51.01.52.0Model QuantilesEmpirical Quantiles0.00.20.40.60.81.00.00.20.40.60.81.0Empirical ProbabilitiesModel Probabilities5.2 Honeypot data II with 69 dimensions

In this section, we study the other honeypot data which was used in [22, 28]. The dataset was
collected by a low-interaction honeypot during 4 November 2010 to 21 December 2010 with a
total number of 1123 hours, which has 69 consecutive IP addresses. Each TCP ﬂow initiated by
a remote computer and an unsuccessful TCP handshake are deemed as attacks [28]. The data is
split into three parts: the training with 750 observations, the validation with 150 observations,
and the rest 223 observations are used for the prediction evaluation. The data is log-transformed
to reduce the skewness.

We employ Algorithm 1 for training and validating the deep learning model, and Algorithm

2 for the prediction. The performance of point predictions are shown in Table 6.

Table 6: Summary statistics of predictions of proposed approach and benchmark model for

MAPE and MSE based on the log-transformed attack data.

Statistic

N Mean

St. Dev. Min

Q1

Median

Q3

Max

MAPE

MSE

MAPE

MSE

69

69

69

69

0.091

0.316

0.017

0.180

Deep

0.036

0.114

Benchmark

0.084

0.197

0.088

0.253

0.095

0.324

0.142

1.028

0.150

0.718

0.033

0.184

0.041

0.152

0.139

0.625

0.162

0.716

0.167

0.861

0.189

1.092

It is observed that the proposed deep learning model has a very satisfactory prediction perfor-
mance. Particularly, the mean (0.091) and median (0.088) of MAPEs by the proposed approach
are much smaller than the mean (0.180) and median (0.162) of MAPEs by the benchmark model.
For MSEs, the proposed approach also signiﬁcantly outperforms the benchmark model in terms
of mean (0.316 vs 0.718) and median (0.253 vs 0.716). The boxplots are further shown in Figure
8. It is also seen that the proposed approach signiﬁcantly outperforms the benchmark model.

For the high quantiles, we also study the prediction performance at the aggregate level as it
corresponds to the network level in practice where the defense can take place. Speciﬁcally, we
calculate the residuals,

et = st − ˆst,

i=1 y(cid:48)

i, ˆst = (cid:80)t

where st = (cid:80)t
i, and y(cid:48)s are the log-transformed attack data, t = 1, . . . , 900.
The GPD distribution is then ﬁtted to {et}t=1,...,900 with the threshold to be .9-quantile of
{et}t=1,...,900. The estimated parameters are ξ = 0.2116 with standard error 0.1226, and σ =
12.5542 with standard error 2.0010, which are signiﬁcant at .1 level. The QQ- and PP- plots are

i=1 ˆy(cid:48)

20

(a) MAPE-Deep

(b) MAPE-Benchmark

(c) MSE-Deep

(d) MSE-Benchmark

Figure 8: Boxplots of MAPEs and MSEs based on the proposed model and benchmark model

for the log-transformed attack data.

21

0.040.060.080.100.120.140.050.100.150.20.40.60.81.00.20.40.60.81.0(a) QQ-plot

(b) PP-plot

Figure 9: QQ- and pp- plots of the log-transformed attack data ﬁtted by the GPD distribution.

displayed in Figure 9. It is seen that PP- plot is satisfactory. The tails of QQ-plot is slightly oﬀ
the diagonal line. This is again caused by the extreme large values in the attack data. But the
overall ﬁtting is fairly well.

For the high quantile prediction, Algorithm 2 is employed to the testing data, where the
threshold is set to be the .9-quantile of residuals. During the prediction process, we retrain the
deep learning model three times by using Algorithm 1 to further improve the prediction accuracy.
The p-values of VaR tests are presented in Table 7. We ﬁrst observe that the proposed model
slightly underestimates the number of violations, particularly at the level .95 (11 vs 17), which
is mainly due to the extreme large attacks. The other levels have large p-values which indicates
that the proposed model can predict them well.

To conclude, the proposed model has overall satisfactory ﬁtting and prediction performances.

6 Conclusion and discussion

In this work, we propose a novel approach to predicting the multivariate cyber risks. The key
idea is to provide the accurate point prediction by training the deep learning network, while
employ the extreme value theory for modeling and predicting the high quantiles. One particular
advantage of proposed approach is that it can easily handle high dimensional cyber risks thanks
to the deep learning architectures, and further, the deep learning model can be retrained to
improve the prediction accuracy if needed. The simulation and empirical studies conﬁrm the
feasibility of proposed approach and satisfactory ﬁtting and prediction performances.

22

2040608010012020406080100120Model QuantilesEmpirical Quantiles0.00.20.40.60.81.00.00.20.40.60.81.0Empirical ProbabilitiesModel ProbabilitiesTable 7: The p-values of the VaR tests of the predicted violations for α = .95, .96, .97, .98, .99.

‘Ob.’ represents the observed number of violations and ‘Exp.’ represents the expected number

of violations.

α

Exp. Ob.

LRuc

LRcc

.95

.96

.97

.98

.99

11

9

7

4

2

17

12

11

5

4

0.0846

0.0235

0.2960

0.2022

0.1117

0.2392

0.7773

0.8548

0.2738

0.5100

There are some limitations for the current study. Since we model the multivariate dependence
via deep learning architectures, the dependence is treated as a ‘black-box’. This causes the loss
of interpreting the dependence relationship among cyber risks compared to the vine copula
approach [22], which is the common issue of deep learning approach. The real attack datasets
studied in the current work were collected by honeypots. The other attack data (e.g. collected
by telescope or other instruments) may exhibit diﬀerent phenomena, which should be carefully
analyzed before employing the approach developed in our work.

The current work can be extended in several directions. For example, the deep learning
model(s) may be further developed for more accurate ﬁtting and prediction [5]. Second, for the
complex multivariate cyber risks, the GPD distribution may not be enough for capturing the
tail behavior. Then, the non-stationary extreme value distribution may be exploited for the
possible modeling [4, 20].

References

[1] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Re-
current neural networks for multivariate time series with missing values. Scientiﬁc reports,
8(1):1–12, 2018.

[2] Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using
rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078,
2014.

[3] Peter F Christoﬀersen. Evaluating interval forecasts. International economic review, pages

841–862, 1998.

23

[4] Laurens De Haan and Ana Ferreira. Extreme value theory: an introduction. Springer Science

& Business Media, 2007.

[5] Li Deng, Dong Yu, et al. Deep learning: methods and applications. Foundations and

Trends® in Signal Processing, 7(3–4):197–387, 2014.

[6] Xing Fang and Zhuoning Yuan. Performance enhancing techniques for deep learning models
in time series forecasting. Engineering Applications of Artiﬁcial Intelligence, 85:533–542,
2019.

[7] Hardik Goel, Igor Melnyk, and Arindam Banerjee. R2n2: Residual recurrent neural net-
works for multivariate time series forecasting. arXiv preprint arXiv:1709.03159, 2017.

[8] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

http://www.deeplearningbook.org.

[9] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation,

9(8):1735–1780, 1997.

[10] Rob J Hyndman and Anne B Koehler. Another look at measures of forecast accuracy.

International journal of forecasting, 22(4):679–688, 2006.

[11] Ponemon Institute. Cost of a data breach report. https://www.ibm.com/security/

digital-assets/cost-data-breach-report/#/, September 2020.

[12] Jay Jacobs and Bob Rudis.

Data-driven security:
https:
//datadrivensecurity.info/blog/pages/dds-dataset-collection.html, September
2020.

dataset collection.

[13] Julian Jang-Jaccard and Surya Nepal. A survey of emerging threats in cybersecurity. Jour-

nal of Computer and System Sciences, 80(5):973–993, 2014.

[14] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv

preprint arXiv:1412.6980, 2014.

[15] Ben Krause, Liang Lu, Iain Murray, and Steve Renals. Multiplicative lstm for sequence

modelling. arXiv preprint arXiv:1609.07959, 2016.

[16] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-
term temporal patterns with deep neural networks. In The 41st International ACM SIGIR
Conference on Research & Development in Information Retrieval, pages 95–104, 2018.

[17] Martin L¨angkvist, Lars Karlsson, and Amy Loutﬁ. A review of unsupervised feature learning

and deep learning for time-series modeling. Pattern Recognition Letters, 42:11–24, 2014.

24

[18] Xing Ling, Yeonwoo Rho, and Chee-Wooi Ten. Predicting global trend of cybersecurity on
continental honeynets using vector autoregression. In 2019 IEEE PES Innovative Smart
Grid Technologies Europe (ISGT-Europe), pages 1–5. IEEE, 2019.

[19] Alexander J McNeil and R¨udiger Frey. Estimation of tail-related risk measures for het-
eroscedastic ﬁnancial time series: an extreme value approach. Journal of empirical ﬁnance,
7(3):271–300, 2000.

[20] Alexander J McNeil, R¨udiger Frey, and Paul Embrechts. Quantitative risk management:

concepts, techniques, and tools. Princeton university press, 2010.

[21] Department of Homeland Security. National critical infrastructure security and resilience re-
search and development plan. http://publish.illinois.edu/ciri-new-theme/files/
2016/09/National-CISR-RD-Plan-Nov-2015.pdf, November 2015.

[22] Chen Peng, Maochao Xu, Shouhuai Xu, and Taizhong Hu. Modeling multivariate cyberse-

curity risks. Journal of Applied Statistics, 45(15):2718–2740, 2018.

[23] Omer Berat Sezer, Mehmet Ugur Gudelek, and Ahmet Murat Ozbayoglu. Financial time
series forecasting with deep learning: A systematic literature review: 2005–2019. Applied
Soft Computing, 90:106181, 2020.

[24] Lance Spitzner. The honeynet project: Trapping the hackers. IEEE Security & Privacy,

1(2):15–23, 2003.

[25] Bin Wang, Tianrui Li, Zheng Yan, Guangquan Zhang, and Jie Lu. Deeppipe: A
distribution-free uncertainty quantiﬁcation approach for time series forecasting. Neuro-
computing, 397(1):11–19, 2020.

[26] Maochao Xu and Lei Hua. Cybersecurity insurance: Modeling and pricing. North American

Actuarial Journal, 23(2):220–249, 2019.

[27] Maochao Xu, Lei Hua, and Shouhuai Xu. A vine copula model for predicting the eﬀective-

ness of cyber defense early-warning. Technometrics, 59(4):508–520, 2017.

[28] Zhenxin Zhan, Maochao Xu, and Shouhuai Xu. Characterizing honeypot-captured cyber
attacks: Statistical framework and case study. IEEE Transactions on Information Forensics
and Security, 8(11):1775–1789, 2013.

[29] Zhenxin Zhan, Maochao Xu, and Shouhuai Xu. Predicting cyber attack rates with extreme
values. IEEE Transactions on Information Forensics and Security, 10(8):1666–1677, 2015.

25

