1
2
0
2

v
o
N
6
2

]

R
C
.
s
c
[

1
v
7
9
5
3
1
.
1
1
1
2
:
v
i
X
r
a

Graph-based Solutions with Residuals for Intrusion Detection:
the Modiﬁed E-GraphSAGE and E-ResGAT Algorithms

Liyan Chang
The University of Hong Kong

Paula Branco
University of Ottawa

Abstract
The high volume of increasingly sophisticated cyber threats is
drawing growing attention to cybersecurity, where many chal-
lenges remain unresolved. Namely, for intrusion detection,
new algorithms that are more robust, effective, and able to use
more information are needed. Moreover, the intrusion detec-
tion task faces a serious challenge associated with the extreme
class imbalance between normal and malicious trafﬁcs. Re-
cently, graph-neural network (GNN) achieved state-of-the-art
performance to model the network topology in cybersecurity
tasks. However, only a few works exist using GNNs to tackle
the intrusion detection problem. Besides, other promising
avenues such as applying the attention mechanism are still
under-explored.

This paper presents two novel graph-based solutions for in-
trusion detection, the modiﬁed E-GraphSAGE and E-ResGAT
algorithms, which rely on the established GraphSAGE and
graph attention network (GAT), respectively. The key idea
is to integrate residual learning into the GNN leveraging the
available graph information. Residual connections are added
as a strategy to deal with the high class imbalance, aiming at
retaining the original information and improving the minority
classes’ performance. An extensive experimental evaluation
on four recent intrusion detection datasets shows the excellent
performance of our approaches, especially when predicting
minority classes.

1 Introduction

Cybersecurity has gained increasing attention in the contem-
porary society. Due to the easy access to the global network,
individuals and organizations are faced with more complex
cyber threats that arrive with at an increasing pace. The fre-
quency and the intricacy of network attacks have signiﬁcantly
risen over the past few years posing a serious challenge.

Intrusion detection system (IDS), a model that identiﬁes
potentially malicious network trafﬁc, plays an important role
in cybersecurity [13]. Machine learning models are increas-
ingly used in this environment and the recent upsurge of deep

learning techniques resulted in major advancements for IDSs.
Neural networks are indeed playing a central role showing
vast progress concerning IDS development (e.g. [1,21,27]). In
particular, graph neural networks (GNNs) which only recently
started to be explored in cybersecurity tasks, are achieving
state-of-the-art performance in many cases [9]. Yet, the num-
ber of solutions for intrusion detection problems that rely on
GNNs is still scarce. Although this is a promising avenue, it
is still under-explored. Moreover, a critical issue remains to
be solved: the serious class imbalance present in the majority
of real-world cybersecurity problems. Most of the current
IDSs have suffered from the extreme imbalance between the
normal and malicious trafﬁc, and a good effective solution is
still missing.

GNNs are designed to encode non-Euclidean graph data,
such as social networks, biological networks, and citation
networks, to facilitate representation learning and downstream
classiﬁcation and prediction tasks. A graph-structured dataset
normally contains node and/or edge features. An effective
GNN not only learns feature embeddings, but it also captures
the spatial information hidden in the graph topology. Intrusion
detection problems are typically performed on network ﬂow
data, which is comprised of IP address, port numbers and a set
of ﬂow-related features (the ﬂow duration, transaction bytes,
and the number of transmitted packets, etc). GNNs provide a
perfect application context for intrusion detection problems
by simply mapping IPs and ports to nodes while network
ﬂows can be mapped to edges.

In this paper, we extend the idea of E-GraphSAGE [14]
to explore more alternatives of GNNs, such as graph atten-
tion networks (GATs), and propose two GNN solutions for
intrusion detection tasks that leverage the residual learning.
The ﬁrst proposal builds upon E-GraphSAGE by combining it
with residual learning, in order to tackle the high class imbal-
ance presented in the datasets. Inspired by the improvements
observed on this solution, we introduce a second novel pro-
posal named edge-based residual graph attention network (E-
ResGAT). E-ResGAT uses the attention mechanism of GAT
while also supporting edge features and embedding residual

1

 
 
 
 
 
 
learning to further enhance the performance. We demonstrate
that these graph-based models with residuals can effectively
detect malicious trafﬁc and deal with the class imbalance prob-
lem, through an extensive experimental evaluation on four
well-known intrusion detection datasets. Our results show
that our proposed models achieve better performance than the
original models and provide important improvements in the
classiﬁcation of the minority classes.

The rest of this paper is organized in the following way.
Section 2 describes the related work on previous graph-based
models in the cybersecurity tasks. Section 3 presents the
graph construction from the trafﬁc network and provides a
theoretical overview of both proposed graph-based algorithms
with residuals. Section 4 provides the experimental evalua-
tion details including a summary of four intrusion detection
datasets, the experimental settings, the results obtained and a
discussion. Finally, concluding remarks and the future work
are presented in Section 5.

2 Related Work

Graph-based deep learning is a fast evolving and promis-
ing area of research that is witnessing a growth in new so-
lutions and applications. Recently, graph neural networks
(GNNs) [19] have achieved state-of-the-art performance in
cybersecurity tasks, such as network intrusion and anomaly
detection [9]. They are suitable for cybersecurity tasks, owing
to their ability to capture the spatial information hidden in the
network topology and can be generalized to unseen topolo-
gies when the networks are dynamic. Compared to traditional
neural networks, these models take both graph topology and
ﬂow features into consideration. Usually, hosts are viewed as
nodes, while ﬂows in between are regarded as edges in graph-
based models. This section covers a wide range of graph
learning techniques used for intrusion detection, including
graph embedding, graph convolutional and graph attention
networks.

Graph embedding transforms the edges, nodes and their
features information into a vector space. Multiple approaches
involving graph embedding have been proposed, including
Structure2Vec [5] and GraphSAGE [14], among others. Struc-
ture2Vec [5] is based on the idea of embedding latent variable
models into feature spaces, and has shown to be efﬁcient and
scalable for structured data representation, although it was
not applied to the network intrusion detection problem.

A graph embedding approach that learns the representa-
tions of network ﬂows in an anomaly detection task was pro-
posed by Xiao et al. [25]. Network ﬂows begin by being
transformed into two graphs. The ﬁrst graph learns the em-
bedding by using its own IP address and port number, while
the second graph generates the embedding from all IPs and
port numbers. This method captures both local and global em-
bedding representations of network ﬂows. However, it uses a
transductive method, which learns the representation for each

individual ﬂow. Thus, this model may fail to classify unseen
network ﬂows. Our proposed models show an advantage as
both learn the neighborhood aggregation, and are thus appli-
cable to inductive tasks as well. The GraphSAGE network
proposed by Hamilton et al. [7] is an example of an inductive
approach. Although allowing the generation of low dimen-
sional vector representation of graphs, this algorithm still has
a signiﬁcant drawback as it does not consider edge features.
GraphSAGE has been applied to several problems such as
network slicing management and trafﬁc prediction. However,
its application to network intrusion detection, our target prob-
lem, was proposed after the introduction of a modiﬁcation to
the GraphSAGE algorithm that we describe next.

The extension of GraphSAGE to the intrusion detection
problem was carried out by Lo et al. [14] that proposed E-
GraphSAGE algorithm. This is an inductive model using
GraphSAGE network that also allows to take into consid-
eration the edge features. The authors ﬁrst aggregate the
neighboring ﬂows for both source and destination nodes and
then concatenate them as the ﬂow representation. This model
learns the aggregation function rather than the direct ﬂow
representation and thus, it can be easily applied to unseen
ﬂows. However, one of its disadvantages is that the ﬂow fea-
ture itself does not have enough impact on the embedding
representation. Our ﬁrst proposed model addresses this is-
sue in E-GraphSage, by proposing a modiﬁcation that allows
the algorithm to act as residual learning. We focused on E-
GraphSAGE to carry out this modiﬁcation given its suitability
for the intrusion detection problem.

Graph convolutional networks (GCN) extend to the graph
scenario with the convolution operation that is used on tra-
ditional data such as images [9]. The key idea of GCNs is
to learn a function that maps the graph information into a
new representation. In this function, a node aggregates not
only its own features but also its neighbors features in or-
der to generate the new representation of the data. Cheng et
al. [3] proposed Alert-GCN, a solution that aims at correlating
alerts that belong to the same attack using GCNs. Alert-GCN
tackles this task as a node classiﬁcation problem. It starts
by building an alert graph using the alert information from
neighbors which is then feed into GCN to perform node clas-
siﬁcation. Zhou et al. [28] performed botnet detection using
a graph convolutional network, which ﬁrst generated botnet
trafﬁc by creating botnet connections mixed with different
real large-scale network trafﬁc ﬂows. Then, they applied the
graph convolutional network for network node classiﬁcation.
The generated graph does not include any ﬂow or node fea-
tures, and only considers the topological information of the
network connectivity graph. Furthermore, the approach does
not detect individual attack ﬂows, but is limited to detecting
attack nodes, speciﬁcally in the context of botnets. In contrast,
We utilize both topology information and edge features in
our proposed models, and aim to detect a variety of attack
families.

2

Graph attention networks [23] (GAT) are widely applied in
communication network and several studies have shown their
potentials. The key distinguishing aspect of GAT models lies
on the incorporation of the attention mechanism into the prop-
agation step while using the multi-head attention mechanism
to make the learning process more stable. One of the advan-
tages of the attention mechanism is that it focuses on the most
relevant neighbors of the ﬂow to make decisions, rather than
taking the neighbors equally. Multi-scale Spatial-Temporal
Graph Neural Network (MSTNN) [26] uses the attention
mechanism on the spatial extractor for capturing the time-
varying spatial correlations of nodes in origin-destination
trafﬁc prediction task. Wan et al. proposed GLAD-PAW [24]
for anomaly detection in log ﬁles. GLAD-PAW contains a
position aware weighted graph attention layer, which encodes
the node features with position information. However, GAT
is seldom applied in intrusion detection and only a few recent
research works use it in this context. Our second proposed
method includes an attention mechanism and demonstrates
good generalization ability on three current intrusion detec-
tion datasets.

3 Our Proposed Graph-based Algorithms

with Residuals

This section ﬁrst introduces the graph construction from the
network ﬂow data and then presents our two proposed algo-
rithms: the modiﬁed E-GraphSAGE model and the E-ResGAT
model. We use the following notation: vectors are represented
with bold lowercase letters and matrices are denoted with bold
uppercase letters.

3.1 Graph Construction

Network ﬂow data consist of the following three components,
source and destination addresses, and ﬂow features, including
duration, transaction bytes, transmitted packet sizes, etc. It is
natural to construct a graph using source and destination as
the endpoints of a network ﬂow. To be more speciﬁc, source
IP addresses and port numbers are combined to identify graph
nodes, and so are the destination addresses and ports. The
remaining ﬂow data serves as the edge features. In this way,
the intrusion detection problem is encoded as an edge classiﬁ-
cation task.

Since in all of our datasets, the source and destination ad-
dresses are disjoint, we construct a bipartite graph G(S ,D;E),
where S , D, E represent the source node set, destination node
set, and edge set, respectively. Notice that the graph nodes are
featureless. We can easily convert the bipartite graph into its
line graph counterpart [12], whose nodes correspond to the
original edges and vice versa. Thus, the problem becomes a
node classiﬁcation task. Figure 1 shows a simple example of
such transformation. Our proposed modiﬁed E-GraphSAGE

and E-ResGAT models are based on the described bipartite
and line graph structures, respectively.

Figure 1: The transformation between the original bipartite
graph (left) and the corresponding line graph (right). Nodes
on the right correspond to the edges on the left. S refers to a
source node set of size 3, and D a destination node set of the
same size.

Furthermore, we augment the source (resp. destination)
node set to the size of destination (resp. source) node set, by
creating virtual nodes, if |S | < |D| (resp. |S | > |D|). Then, the
source (resp. destination) endpoints of the edges are randomly
replaced with the virtual nodes. The purpose for the augmen-
tation is two-fold. Firstly, as the full neighborhood is used in
the GAT-based models, the adjacency list of the line graph
may not ﬁt in the memory. This step deliberately increases
the number of nodes and therefore reduces the averaged node
degree, since the number of edges |E| remains unchanged.
Note that |E| in the line graph is given by ∑i∈S ∪D (di −1)di/2,
where di is the degree of node i. In this way, the memory of
line graph will be reduced, especially when there exists a
large node degree. Secondly, the node augmentation intro-
duces a random mapping, which will prevents the potential
issue of the source nodes providing an unintentional label for
malicious trafﬁcs [14].

3.2 The Modiﬁed E-GraphSAGE Algorithm

The E-GraphSAGE algorithm [14] is adapted from the Graph-
SAGE algorithm [7] to support edge classiﬁcation. The key
idea behind E-GraphSAGE is that it performs GraphSAGE
algorithm on two endpoints of an edge separately, and ﬁnally
concatenate node embeddings as the edge representation. Our
key contribution lies on a modiﬁcation on the ﬁnal concatena-
tion step that aims at overcoming the potential loss of infor-
mation of the original edge features.

Similar to the E-GraphSAGE, our proposed modiﬁed E-
GraphSAGE algorithm consists of two parts, sampling and
aggregation. Due to the memory limitation, we implemented
the mini-batch version of both the original and the modiﬁed
E-GraphSAGE algorithm. The sampling and aggregation com-
ponents of our modiﬁed E-GraphSAGE algorithm are shown
in Algorithm 1 and 2, respectively.

3

Sampling
In the modiﬁed E-GragpSAGE, we uniformly
sample a ﬁxed-size set of neighbors, instead of using full
neighborhood as in [14]. We deﬁne the neighborhood Nv of
a node set V , as a ﬁxed-size, uniform sample from the set
{u : v ∈ V , uv ∈ E}, where V ⊆ S ∪ D. For each edge batch
B, we draw different uniform samples at each iteration k as
shown in Equation 1.

B k ← B k ∪ {uv, ∀u ∈ Nv}

(1)

Without this sampling, the memory and expected runtime
of a single batch is unpredictable and in the worst case
O(|V |) [7]. In practice, the average degree of nodes is used
as the size of neighborhood and so, we sampled a 2-hop 8-
neighborhood of the batch before aggregation.

Algorithm 1: E-GraphSage minibatch sampling
Input: Graph G(V ,E);

edge minibatch B;
node set V (B) = {v : v is an end point of
uv ∈ B};
depth K; neighborhood sampling functions
Nv : v → 2V ;

Output: 2-hop neighborhood of the batch B 0

1 B K ← B;
2 for k = K, ..., 1 do
3

B k−1 ← B k;
for v ∈ V (B k) do

4

5

B k−1 ← B k−1 ∪ {uv, ∀u ∈ Nv};

end

6
7 end

Aggregation After sampling, the algorithm iteratively ag-
gregates the neighboring edge features layer by layer. We now
describe a single layer below. The input to the layer are the
edge features huv, where uv ∈ B 0. At the kth layer, the neigh-
boring edge features in the previous layer are aggregated to
the node v, which can be expressed as follows:

hk
Nv

= AGGk({hk−1

uv , ∀u ∈ Nv})

(2)

In Equation 2, various kinds of aggregation functions AGG
can be applied, such as mean, pooling, graph convolution
or Long Short-Term Memory (LSTM). For our experiments,
we selected the mean aggregation for simplicity, as adopted
in [14]. The mean aggregation function simply computes the
average of edge features over the sampled neighborhood of
the nodes.

Then, to combine the aggregated information hk
Nv

and the

node embedding in previous layer hk−1

v

, they are concatenated

4

and multiplied with a trainable weight matrix Wk. Finally, the
node embeddings are yielded after activation σ:

v = σ(Wk · [hk−1
hk

v

(cid:107)hk
Nv

]),

(3)

where (cid:107) represents concatenation. Notice that the nodes in net-
work graph are featureless. They are initialized with all-one
vectors h0
v = 1 with the dimension equal to the number of edge
features. After the aggregation, the ﬁnal edge embeddings at
the Kth layer are calculated as the concatenation of node em-
beddings from the endpoints in the original E-GraphSage, as
shown in Equation 4.

zuv = hK

u (cid:107)hK

v , ∀uv ∈ B

(4)

However, original edge features may be diluted during the
mean aggregation and are not well represented in the ﬁnal
edge embeddings. To address this issue, we propose a modi-
ﬁcation at this stage. To obtain improved ﬁnal edge embed-
dings, we propose concatenating the node embeddings from
the endpoints and the original edge features as well, as shown
in line 9 in Algorithm 2. Equation 5 shows this new concate-
nation procedure.

zuv = (cid:107)({hK

u , hK

v , euv}), ∀uv ∈ B

(5)

This represents the ﬁnal edge embeddings in our modiﬁed
E-GraphSAGE algorithm. Notice that this is very similar to a
residual learning [8], where zuv = F (euv) + euv with residual
function F , except that the summation is replaced by con-
catenation. The proposed modiﬁcation to the E-GraphSAGE
algorithm provided a signiﬁcant improvement in the experi-
ments carried out, showing the clear advantage of using this
new solution.

3.3 The E-ResGAT Algorithm

In the E-GraphSage algorithm, all the neighboring edges are
treated equally. However, it is more natural to consider dif-
ferent weights for different neighbors. Hence, we decided
to apply the graph attention network [23] with residuals to
our problem, which additionally learns the weighted aggre-
gation. Another key difference between E-ResGAT and E-
GraphSAGE algorithms is that E-ResGAT operates on the
line graph G (cid:48)(V (cid:48),E (cid:48)) instead of the original bipartite graph
G(V ,E), where V (cid:48) = E as speciﬁed in Figure 1. This way,
the model does not split the neighborhood with regard to the
endpoint, thus allowing for a neighborhood of approximately
twice the size to be sampled which means that more useful
information is aggregated.

The algorithm ﬁrst constructs a 2-hop full neighborhood
of a batch. We omit a detailed description of this step due to
its similarity to the sampling step described in Section 3.2.
The only difference is that we use the full neighborhood of
nodes in the line graph (Figure 1 on the right hand side). Then,
an E-ResGAT layer aggregates the neighboring information

, hk

N (v)));

hk
Nv

=

uvWmhk−1
αm

u

)

(8)

σ( ∑
u∈Nv

M
(cid:107)
m=1
v = hk
hk
Nv

(cid:107)W(cid:48)ev,

Algorithm 2: E-GraphSage minibatch aggregation
Input: Graph G(V ,E);

edge minibatch B; 2-hop neighborhood B 0;
node set V (B) = {v : v is an end point of
uv ∈ B};
input edge features {euv, ∀uv ∈ B};
input node features xv = 1;
depth or layer K; non-linearity σ;
weight matrices Wk, ∀k ∈ {1, ..., K};
differentiable aggregator functions AGGk;
neighborhood sampling functions
Nv : v → 2V ;

Output: Vector representation zuv, ∀uv ∈ B
uv ← euv, ∀uv ∈ B 0;
v ← xv, ∀v ∈ V (B 0);

1 h0
2 h0
3 for k = 1, ..., K do
4

for v ∈ V (B k) do

5

6

hk
v = σ(Wk · [hk−1
v
v = σ(Wk ·CONCAT (hk−1
hk

(cid:107)hk
Nv

]);

v

end

7
8 end
9 zuv = (cid:107)({hK

v , euv}), ∀uv ∈ B;
// concatenate the original features as

u , hK

well

using the attention mechanism. Moreover, inspired by the
performance improvements observed with the modiﬁcation
made in E-GraphSAGE, we also concatenated a transforma-
tion of the original node feature in each layer. Thanks to
such residual learning, the E-ResGAT can go deeper than the
vanilla GAT, and the stacked aggregation layers will gradually
reﬁne the node embeddings. The overall architecture of the
proposed E-ResGAT model is described in Figure 2, and the
mini-batch version of the E-ResGAT aggregation is shown in
Algorithm 3.

Aggregation with Residual We next describe the E-
ResGAT aggregation layer in detail. The inputs are node
features hv in a line graph, where v ∈ B 0. The layer outputs
are the aggregated node embeddings h(cid:48)

v.

For each node v, a weighted average of the neighboring
features is ﬁrst computed, and then concatenated with the
transformed node features ev. This works for the highly im-
balanced data in malware detection, as it can prevent jeopar-
dizing the performance when the neighborhood of a node is
mostly occupied by majority class and the node embedding
is not well represented. At the kth layer, the attention-based
aggregation with residual can be expressed as:

hk
v = σ( ∑
u∈Nv

αuvWhk−1

u

)(cid:107)(W(cid:48)ev),

(6)

where αuv is the attention coefﬁcient assigned to the edge
euv in the line graph, and W is a shared linear transformation
across the layer that maps the input features into a lower
dimension. The attention coefﬁcient αuv can be learned simply
by a feedforward neural network a[Whu(cid:107)Whv], where a is
a weight vector. Then, the attention coefﬁcients for all node
pairs are obtained after LeakyReLU activation [15] and a
softmax function, as shown in Equation 7.

αuv =

exp (LeakyReLU(a[Whu(cid:107)Whv]))
∑i∈Nv exp (LeakyReLU(a[Whi(cid:107)Whv]))

(7)

Similar to GAT, we can apply multi-head attentions to in-
crease the capacity of the E-ResGAT model. Thus, a multi-
head E-ResGAT aggregation is formulated as:

where there are M heads of attention, αm
uv are the mth nor-
malized attention coefﬁcients, and Wm corresponds to mth
weight matrix. Note that we concatenate the original node
features ev at the end with a transformation W(cid:48). Such transfor-
mation can be the average of all M matrices Wm. To reduce
the computation, we simply implemented a identity mapping.

Algorithm 3: E-ResGAT minibatch aggregation
Input: Graph G (cid:48)(V (cid:48),E (cid:48));

edge minibatch B; 2-hop neighborhood B 0;
input node features ev;
layer K; number of heads M;
weight matrices Wm, ∀m ∈ {1, ..., M};
non-linearity σ;
neighborhood for node v Nv : v → 2V ;

Output: Vector representation zv, ∀v ∈ B
1 h0
v ← ev, ∀v ∈ B 0;
2 for k = 1, ..., K do
3

for m = 1, ..., M do

4

5

6

7

8

euv = LeakyReLU(a[Wmhk−1
αuv = softmax(euv) = exp(euv)

∑i∈Nv exp(eiv) ;

u (cid:107)Wmhk−1

v

]);

end

M
(cid:107)
m=1

=

hk
Nv
v = hk
hk
Nv

σ(∑u∈Nv αm
(cid:107)W(cid:48)ev;

uvWmhk−1

u

);

9 end
10 zv = hK
Nv

(cid:107)W(cid:48)ev, ∀v ∈ B;

5

Figure 2: The overall architecture of E-ResGAT.

4 Experimental Evaluation

In order to evaluate the performance of the two proposed
models, we carried out our experiments on four intrusion
detection datasets. First, a brief description of all the con-
sidered datasets is presented. Then, we discuss the detailed
experimental setting, including the metrics and implementa-
tion. Subsequently, we present the results of both binary and
multiclass classiﬁcation problems, followed by an extensive
analysis and discussion on efﬁciency and learned features.

4.1 Data Sets

For our experiments we selected four intrusion detection
datasets. Their main characteristics are described in Table 1.
The datasets chosen represent multiclass problems where the
percentage of benign cases is much higher than the percentage
of attacks. For dataset CIC-DarkNet1 [6] we used the entire
data available. However, for the remaining datasets we only
used a sample of the data available on the webpage. We used
the train/test ﬁles provided for dataset ToN-IoT2 [2], while
for datasets UNSW-NB153 [16] and CSE-CIC-IDS4 [20] we
use only a sample of the data available. In particular, we se-
lected roughly 25% and 3% of the data available for these
two datasets, respectively. The motivation for this is related
with both the large size of the datasets and the generation
of different settings with regards to the percentages of the
majority class (normal) cases.

Table 2 shows the distribution obtained for all classes
in each dataset. We observe that these intrusion detection
datasets have different characteristics. All are multiclass prob-
lems with one majority class (the normal class) and multiple
minority classes (the different attacks). The majority class
has a different representation in each dataset ranging from
96,83% to 65.07%. The minority classes also exhibit differ-

1https://www.unb.ca/cic/datasets/darknet2020.html
2https://research.unsw.edu.au/projects/toniot-datasets
3https://research.unsw.edu.au/projects/unsw-nb15-dataset
4https://www.unb.ca/cic/datasets/ids-2018.html

ent distributions ranging between 13.04% (DDOS attack on
CES-CIC-IDS dataset) and 0.003% (Worms attack in UNSW-
NB15 dataset). This provides a challenging scenario for our
experiments.

4.2 Experimental Setting

Evaluation Metrics The overall accuracy performance of
the proposed models is measured through the F1-score, which
is expressed below:

F1-score = 2 ×

Recall × Precision
Recall + Precision

,

(9)

where Precision measures the ability of an intrusion detec-
tion system to identify only the attacks, while Recall can be
thought as the system’s ability to ﬁnd all the attacks. The
higher the F1-score, the better the balance between Precision
and Recall achieved by the model. We initially compare the
weighted F1-scores as overall model performance for both
binary and multi-classiﬁcation tasks. The weight coefﬁcients
used are the class ratios. However, we do not want the high
weighted F1-score in cost of the minority accuracy. Thus, a
macro F1-score is considered as well. The F1-scores of all
classes are averaged evenly in calculation of macro F1-score.
This way, we do not discriminate the classes by their percent-
age and raise the importance of minority classes. Both metrics
are computed on test sets of corresponding datasets.

The efﬁciency performance is evaluated with the time spent
on training the proposed models. The time is collected from
a Windows machine with an Intel(R) Core(TM) i5-8250U
CPU @ 1.60GHz and 8 GB RAM. The batch training time is
measured in seconds.

Models and Hyperparameters For the modiﬁed E-
GraphSAGE, a 2-layer model, followed by softmax classiﬁer,
is used for edge classiﬁcation. The original E-GraphSAGE is
compared with the modiﬁed version as a baseline model. For
all the E-GraphSAGE models, either modiﬁed or not, we used

6

Dataset

No. Examples Normal(%) No. Classes No. Features Reference

UNSW-NB15
CIC-DarkNet
CSE-CIC-IDS
ToN-IoT

700001
141530
250928
461043

96.83
82.82
70.61
65.07

10
9
7
10

43
77
77
39

[16]
[6]
[20]
[2]

Dataset

UNSW-NB15

CIC-DarkNet

Table 1: Overview of the datasets used in our experiments.

Classes (names and %)

Normal Exploits Recon. DoS Generic Shellcode Fuzzers Worms
96.83

0.167 1.07

0.722

0.032

0.003

0.773

0.251

Backd. Analysis
0.076

0.075

Normal Audio str Brows. Chat File tr. Email
82.82

0.41

3.21

1.84

0.19

9.39

CSE-CIC-IDS

Normal BruteF. DoS
1.01
7.28
70.61

DDoS Web
13.04 0.37

Bot
5.42

Video str VOIP
0.95

1.03

P2P
0.16

Inﬁltr.
2.27

ToN-IoT

Normal Scanning Dos
4.34
4.34
65.07

Inject. DDos
4.34

4.34

Password XSS
4.34
4.34

Ransomw. Backd. MITM
4.34
4.34

0.22

Table 2: Distribution of the classes of the selected datasets.

mean function as aggregation, ReLU [17] as the non-linearity,
neighborhood as 2-hop and sample size as 8 for both layers.
For E-ResGAT, a deeper model with 6-head attention at
each layer is used for node classiﬁcation. The number of lay-
ers is selected based on the best performing hyperparameter
value. To make a fair comparison, we also implemented a
vanilla GAT model as a baseline to illustrate the impact of
residual modules. In order to avoid overﬁtting, dropout is ap-
plied to attention coefﬁcients. It is also applied to the input in
original GAT model [23]. We did not adopt the dropout step,
as the number of features in the considered datasets is already
very small. Similar to the original GAT, ELU [4] is used as
the non-linearity.

All models were implemented in Pytorch [18] using Adam
optimizer [10]. In order to give a fair comparison, these mod-
els share identical minibatch iterators, loss function, learning
rates and the splitting of train, validation and test sets. In prac-
tice, we used minibatchs of size 500, cross-entropy loss and
5/2/3 for train/validation/test ratio. Learning rates (lr) were
chosen from the best setting for each dataset according to
the performance on the validation set. This way, the learning
rates used are as follows: lr = 0.007 for UNSW-NB15 and
lr = 0.003 for both CIC-DarkNet and CSE-CIC-IDS, while
lr = 0.01 for ToN-IoT. All the models were trained for two
epochs on the training set.

4.3 Results and Discussion

Classiﬁcation We ﬁrst carry out a binary classiﬁcation eval-
uation, assessing whether a ﬂow belongs to a normal or mali-
cious class and then move to a multiclass scenario where we
identify both the benign and the individual attack classes. As

mentioned above, we report F1 weighted and F1 macro scores
for binary classiﬁcation and the muticlass scenario. Table 3
displays the F1 weighted and F1 macro results obtained with
the four algorithms in the four datasetsIn order to fairly as-
sess the beneﬁts of the residual features, comparison is made
within each pair of modiﬁed and original models. As observed
from Table 3, the two models we propose clearly outperform
the original ones in three out of four datasets, in terms of both
F1 weighted and F1 macro scores. The results are different for
dataset UNSW-NB15 where the proposed alternatives do not
win every time. Still, in this case, we observe that E-ResGAT
yields better scores than GAT in the multiclass classiﬁcation
setting for both metrics. Moreover, on this dataset, for the
binary F1 Macro scenario, the differences observed between
the results of our models and the competitor models are very
small (0.2% on GraphSAGE-based models and 0.11% for
GAT-based models). We must also highlight that a similar
behaviour was observed for the UNSW-NB15 in the origi-
nal E-graphSAGE experiments [14], when it was compared
against other non-graph based start-of-the-art models. Hence,
we hypothesize that graph-based models may not be the most
suitable solution for this particular highly imbalanced dataset.
Finally, it is also noticeable the clear positive impact of em-
bedding an attention mechanism in the algorithms. In effect,
if we compare between the two groups of models, GAT-based
and GraphSAGE-based, it becomes evident that GAT-related
models generally beat GraphSAGE-related models, indicating
the power of the attention mechanism.

To provide a more detail overview of the results achieved
when tackling these problems as a multiclass task, we provide
in Table 4 the individual F1-score for each class obtained
by the four models tested in all the selected datasets. The

7

Dataset

Algorithm

F1 Weighted

F1 Macro

Binary

Multi

Binary

Multi

UNSW-NB15

CIC-DarkNet

CSE-CIC-IDS

ToN-IoT

E-GraphSAGE
0.9944
E-GraphSAGE M 0.9950*
0.9948
GAT
0.9946
E-ResGAT

0.8693
E-GraphSAGE
E-GraphSAGE M 0.9150
0.9206
GAT
0.9232*
E-ResGAT

0.9082
E-GraphSAGE
E-GraphSAGE M 0.9632
0.9634
GAT
0.9650*
E-ResGAT

E-GraphSAGE
0.9953
E-GraphSAGE M 0.9988*
0.9976
GAT
0.9988*
E-ResGAT

0.9868*
0.9837
0.9855
0.9858

0.8093
0.8790
0.8749
0.8807*

0.8774
0.9557*
0.9494
0.9531

0.9384
0.9851
0.9954
0.9970*

0.9539
0.9519
0.9589*
0.9578

0.7465
0.8519
0.8628
0.8666*

0.8657
0.9554
0.9554
0.9573*

0.9949
0.9987*
0.9973
0.9986

0.4091*
0.3631
0.3830
0.3865

0.3554
0.4248
0.4343
0.5087*

0.5505
0.7591
0.7688
0.9101*

0.7695
0.9385
0.9776
0.9930*

Table 3: Weighted and Macro F1 measure results on the four selected datasets.
Numbers in bold are the higher scores achieved in each pair of algorithms, while
those with the asterisk(*) refer to the highest scores among all algorithms.

Dataset

Algorithm

Per class F1-score

0

1

2

3

4

5

6

7

8

9

UNSW-NB15

CIC-DarkNet

CSE-CIC-IDS

ToN-IoT

E-GraphSAGE
0.9970
E-GraphSAGE M 0.9970
0.9970
GAT
0.9973
E-ResGAT

E-GraphSAGE
0.9355
E-GraphSAGE M 0.9450
0.9484
GAT
0.9511
E-ResGAT

E-GraphSAGE
0.9329
E-GraphSAGE M 0.9752
0.9740
GAT
0.9760
E-ResGAT

E-GraphSAGE
0.9931
E-GraphSAGE M 0.9949
0.9947
GAT
0.9951
E-ResGAT

0.6386
0.5832
0.6280
0.6540

0.1575
0.6684
0.6453
0.6209

0.9956
0.9964
0.9789
0.9870

0.7297
0.9103
0.9865
0.9907

0.7904
0.8383
0.4280
0.4886

0.4426
0.3158
0.4179
0.4978

0.0000
0.9010
0.9286
0.9561

0.9196
0.9645
0.9810
0.9897

0.0515
0.0316
0.0675
0.0490

0.3351
0.7424
0.4960
0.6079

0.4712
0.9839
0.9895
0.9909

0.5503
0.9072
0.9884
0.9937

0.9472
0.9227
0.9378
0.9252

0.2087
0.3093
0.2650
0.3672

0.0000
0.5817
0.5934
0.6108

0.0000
0.7028
0.8532
0.9793

0.0900
0.0000
0.0000
0.0000

0.1936
0.0794
0.2960
0.3345

0.9030
0.9925
0.9175
0.9400

0.9955
0.9992
0.9985
0.9987

0.5763
0.2578
0.5221
0.5242

0.6971
0.5328
0.6210
0.6784

0.0000
0.0549
0.0000
0.0000

0.7225
0.9557
0.9898
0.9946

0.0000
0.0000
0.0000
0.0000

0.0000
0.0000
0.0000
0.0000

0.3090
0.1809
0.0770
0.3090

0.0000
0.0000
0.2500
0.2268

0.0000
0.0492
0.1420
0.2113

0.9625
0.9844
0.9967
0.9964

0.9616
0.9887
0.9928
0.9956

0.8606
0.9771
0.9946
0.9963

Table 4: F1-score results per class on the four selected datasets. Numbers in bold refer to the highest scores achieved in each
class among all the implemented algorithms.

8

proposed E-ResGAT gives the best per-class F1 scores in most
of the cases. To be more speciﬁc, E-ResGAT outperforms all
the other models on 20 out of 34 classes in total. Class 7 and
9 in UNSW-NB15 is excluded as none of the models can
distinguish them given such a low ratio in the dataset. For
the classes where this is not the case, such as class 1 and 3 in
CIC-DarkNet and class 1, 5 and 6 in CSE-CIC-IDS, etc, the
proposed modiﬁed E-GraphSAGE achieves the best scores.
So, our results successfully demonstrate the beneﬁcial effect
of residual features in most of our considered datasets, as
expected in Section 3.3. Furthermore, it is worth noting the
improvements achieved on individual classes: there is at least
1% increment from E-ResGAT w.r.t. other models in most
classes. Moreover, our proposed models display an overall
impressive boost in the performance of the classes in which
the original E-GraphSAGE shows near zero F1-scores, once
again reﬂecting the signiﬁcance of residual features.

Efﬁciency The efﬁciency of the implemented models is
evaluated by the time spent in terms of seconds in training a
single batch of 500 network ﬂow samples. The overall average
training time of the two GraphSage-based models and the two
GAT-based models is summarised in Table 5. The efﬁciency
within each type of models (the GraphSAGE-based and the
GAT-based) does not differ very much. However, we observe
that the total time of the GAT-based models we implemented
is over 50 times that of GraphSAGE-based models. We must
highlight that the actual training time of GAT-based models
is only about 3 5 times more than that of GraphSAGE-based
models as shown by the right most value in Table 5. The
main reason that justiﬁes this is the transformation from the
original bipartite graph structure to its line graph structure,
which is required for GAT-based models as discussed in Sec-
tion 3.1. Within this transformation, we select all the edges
that share common endpoints with a batch of edges to con-
struct the full neighborhood of that batch, as opposed to the
neighborhood sampling procedure applied in E-GraphSAGE.
The time needed for the transformation is in O(|B|D2), where
|B| is the batch size and D is the averaged degree. If available
memory permits, one can store the line graph in disk and
signiﬁcantly save the training time. The optimization of the
graph structure transformation is left for future work.

Dataset

GraphSAGE-based

GAT-based

UNSW-NB15
CIC-DarkNet
CSE-CIC-IDS
ToN-IoT

0.171
0.127
0.119
0.154

5.65 (5.05 + 0.602)
4.48 (4.12 + 0.361)
5.72 (5.33 + 0.656)
7.17 (6.52 + 0.651)

Table 5: Averaged time spent (in seconds) in training a batch
of size 500 using GraphSAGE-based and GAT-based algo-
rithms. The total time of GAT models is further divided into
time for line graph transformation (left in brackets) and actual
training time (right in brackets).

clusters. We hypothesize that this behaviour may be attributed
to the sequential patterns learned by the attention mechanism
present in both algorithms. Comparing between two pairs of
models, we can see that there exists a higher class separability
in the models with residuals ((b) and (d) in Figure 3), which
also conﬁrms the model’s discriminatory ability across the
ten different classes of ToN-IoT dataset.

Figure 3: A t-SNE visualization of learned feature represen-
tations of all four models’ last hidden layer on the ToN-IoT
dataset. (a) E-GraphSAGE (b) E-GraphSAGE Modiﬁed (c)
GAT (d) E-ResGAT

Learned Feature Representation Finally, we provide a
visualization of the learned feature representations from the
last layer of all four models using the t-SNE method [22].
The results of dataset ToN-IoT are shown in Figure 3. The
representation exhibits discernible clustering in the projected
two dimensional plane. It can be observed that each pair of
models learns similar representations on ToN-IoT. Moreover,
an interesting pattern is detected by the GAT-related models
((c) and (d) in Figure 3). Instead of globular shaped clusters,
the t-SNE plots of GAT-related models yield long curved

Similar results can be found in the remaining datasets, with
the exception of UNSW-NB15. Figure 4 displays the t-SNE
results of UNSW-NB15. Due to the high class imbalance, the
normal network ﬂow (class 0) is deliberately omitted from
the ﬁgures to allow a clearer visualization of the minority
classes. The key difference of the results lies in the plots of
GAT-related models. In this dataset we no longer observe
the long curved clusters shown in the other datasets. We hy-
pothesize that the reason for this difference is associated to
the inability of the attention mechanism to learn sequential

9

patterns given a highly imbalanced neighborhood with around
95% normal network ﬂows. Furthermore, there is no signif-
icant difference in the class separability among all the four
models. While these models perform well in predicting class
4 (Generic attacks), they are less effective to distinguish the
other minority classes, which is in line with the experimental
results previously discussed.

Figure 4: A t-SNE visualization of learned feature represen-
tations on the UNSW-NB15 dataset. Class 0 is omitted for
clarity. (a) E-GraphSAGE (b) E-GraphSAGE Modiﬁed (c)
GAT (d) E-ResGAT

5 Conclusions and Future Work

In this paper, we propose two graph-based solutions for in-
trusion detection. The ﬁrst model extends the idea of E-
GraphSAGE [14], by adding residual connections to the out-
put layer of the E-GraphSAGE. The proposed modiﬁed ver-
sion of E-GrapgSAGE achieves better results when compared
against the original version. Inspired by the improved per-
formance, we present edge-based residual graph attention
networks (E-ResGATs) in an attempt to solve intrusion detec-
tion tasks and deal with class imbalance issues. E-ResGAT
introduces residual connections and attention mechanisms to
E-GraphSAGE. This results in a more robust graph-based
intrusion detection system that assigns larger weights to sim-
ilar network ﬂows, or at least remain unperturbed under an
extremely imbalanced neighborhood. The improved perfor-
mance in the classiﬁcation of minority classes validates this
point. An extensive set of experiments on four intrusion de-
tection datasets demonstrates the robustness of E-ResGAT in
most cases. Our results reveal the potential of GNNs under
intrusion detection tasks and the promising future research
in this ﬁeld. We believe that GNNs, with careful modiﬁca-

10

tions, can be widely applied to other areas or different tasks
in cybersecurity.

Regarding future explorations, ﬁrst, we plan to optimize the
construction of batch neighborhood to speed up the proposed
E-ResGAT model. Another interesting direction is to extend
the model to effectively tackle even more imbalanced datasets
in intrusion detection, possibly combining it with synthetic
data generation techniques (e.g. [11]).

Code Availability

We release a Python implementation of all the four algorithms
discussed in this paper for reproducing our work in the anony-
mous Github repository. The repository contains the four
considered datasets, all the models, a README ﬁle with
instructions and the code for t-SNE visualization.

References

[1] Tamer Aldwairi, Dilina Perera, and Mark A Novotny. An
evaluation of the performance of restricted boltzmann
machines as a model for anomaly network intrusion
detection. Computer Networks, 144:111–119, 2018.

[2] Abdullah Alsaedi, Nour Moustafa, Zahir Tari, Abdun
Mahmood, and Adnan Anwar. Ton_iot telemetry dataset:
A new generation dataset of iot and iiot for data-driven
intrusion detection systems. IEEE Access, 8:165130–
165150, 2020.

[3] Qiumei Cheng, Chunming Wu, and Shiying Zhou. Dis-
covering attack scenarios via intrusion alert correlation
using graph convolutional networks. IEEE Communica-
tions Letters, 25(5):1564–1567, 2021.

[4] Djork-Arné Clevert, Thomas Unterthiner, and Sepp
Hochreiter. Fast and accurate deep network learn-
ing by exponential linear units (elus). arXiv preprint
arXiv:1511.07289, 2015.

[5] Hanjun Dai, Bo Dai, and Le Song. Discriminative em-
beddings of latent variable models for structured data.
In International conference on machine learning, pages
2702–2711. PMLR, 2016.

[6] Arash Habibi Lashkari, Gurdip Kaur, and Abir Rahali.
Didarknet: A contemporary approach to detect and char-
acterize the darknet trafﬁc using deep image learning.
In 2020 the 10th International Conference on Commu-
nication and Network Security, pages 1–13, 2020.

[7] William L Hamilton, Rex Ying, and Jure Leskovec. In-
ductive representation learning on large graphs. In Pre-
ceedings of the 31st International Conference on Neu-
ral Information Precessing Systems, pages 1025–1035,
2017.

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 770–778, 2016.

[20] Iman Sharafaldin, Arash Habibi Lashkari, and Ali A
Ghorbani. Toward generating a new intrusion detection
dataset and intrusion trafﬁc characterization. ICISSp,
1:108–116, 2018.

[21] Imtiaz Ullah and Qusay H Mahmoud. A two-level hy-
brid model for anomalous activity detection in iot net-
works. In 2019 16th IEEE Annual Consumer Communi-
cations & Networking Conference (CCNC), pages 1–6.
IEEE, 2019.

[22] Laurens Van der Maaten and Geoffrey Hinton. Visu-
alizing data using t-sne. Journal of machine learning
research, 9(11), 2008.

[23] Peter Veliˇckovi´c, Guillem Cucurull, Arantxa Casannova,
Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph
attention networks. arXiv preprint arXiv:1710.10903,
2017.

[24] Yi Wan, Yilin Liu, Dong Wang, and Wen Yujin. Glad-
paw: Graph-based log anomaly detection by position
aware wighted graph attention network. In PAKDD (1),
pages 66–77. Springer, 2021.

[25] Qingsai Xiao, Jian Liu, Quiyun Wng, Zhengwei Jiang,
Xuren Wang, and Yepeng Yao.
Towards network
In Inter-
anomaly detection using graph embedding.
national Conference on Computational Science, pages
156–169. Springer, 2020.

[26] Chenming Yang, Zhiheng Zhou, Hui Wen, and Liang
Zhou. Mstnn: A graph learning based method for
In ICC 2020-
origin-destination trafﬁc prediction.
2020 IEEE Interntional Conference on Communications
(ICC), pages 1–6. IEEE, 2020.

[27] Chuanlong Yin, Yuefei Zhu, Jinlong Fei, and Xinzheng
He. A deep learning approach for intrusion detection
using recurrent neural networks. Ieee Access, 5:21954–
21961, 2017.

[28] Jiawei Zhou, Zhiying Xu, Alexander M Rush, and Min-
lan Yu. Automating botnet detection with graph neural
network. arXiv preprint arXiv:2003.06344, 2020.

[9] Weiwei Jiang. Graph-based deep learning for com-
arXiv preprint

munication networks: A survey.
arXiv:2106.02533, 2021.

[10] Diederik P Kingma and Jimmy Ba.
method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam: A
arXiv preprint

[11] JooHwa Lee and KeeHyun Park. Gan-based imbalanced
data intrusion detection system. Personal and Ubiqui-
tous Computing, 25(1):121–128, 2021.

[12] Philippe GH Lehot. An optimal algorithm to detect a
line graph and output its root graph. Journal of the ACM
(JACM), 21(4):569–575, 1974.

[13] Hung-Jen Liao, Chun-Hung Richard Lin, Ying-Chih Lin,
and Kuang-Yuan Tung. Intrusion detection system: A
comprehensive review. Journal of Network and Com-
puter Applications, 36(1):16–24, 2013.

[14] Wai Weng Lo, Siamak Layeghy, Mohanad Sarhan, Mar-
cus Gallagher, and Marius Portmann. E-graphsage: A
graph neural network based intrusion detection system.
arXiv preprint arXiv:2103.16329, 2021.

[15] Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al.
Rectiﬁer nonlinearities improve neural network acoustic
In Proc. icml, volume 30, page 3. Citeseer,
models.
2013.

[16] Nour Moustafa and Jill Slay. Unsw-nb15: a compre-
hensive data set for network intrusion detection systems
(unsw-nb15 network data set). In 2015 military commu-
nications and information systems conference (MilCIS),
pages 1–6. IEEE, 2015.

[17] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear
units improve restricted boltzmann machines. In Icml,
2010.

[18] Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Py-
torch: An imperative style, high-performance deep learn-
ing library. Advances in neural information processing
systems, 32:8026–8037, 2019.

[19] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
Hagenbuchner, and Gabriele Monfardini. The graph
neural network model. IEEE transactions on neural
networks, 20(1):61–80, 2008.

11

