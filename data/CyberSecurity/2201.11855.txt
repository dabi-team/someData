Accountability and Insurance in IoT Supply Chain

Yunfei Ge

Quanyan Zhu

January 31, 2022

Abstract

Supply chain security has become a growing concern in security risk analysis of the

Internet of Things (IoT) systems. Their highly connected structures have signiﬁcantly

enlarged the attack surface, making it diﬃcult to track the source of the risk posed by

malicious or compromised suppliers. This chapter presents a system-scientiﬁc frame-

work to study the accountability in IoT supply chains and provides a holistic risk

analysis technologically and socio-economically. We develop stylized models and quan-

titative approaches to evaluate the accountability of the suppliers. Two case studies are

used to illustrate accountability measures for scenarios with single and multiple agents.

Finally, we present the contract design and cyber insurance as economic solutions to

mitigate supply chain risks. They are incentive-compatible mechanisms that encourage

truth-telling of the supplier and facilitate reliable accountability investigation for the

buyer.

2
2
0
2

n
a
J

7
2

]

R
C
.
s
c
[

1
v
5
5
8
1
1
.
1
0
2
2
:
v
i
X
r
a

Department of Electrical and Computer Engineering, Tandon School of Engineering
New York University, Brooklyn, NY, 11201, USA
E-mail: {yg2047, qz494}@nyu.edu

 
 
 
 
 
 
1

Introduction

Supply chains play a critical role in the security and resilience of IoT systems and aﬀect
many users, including small- and medium-sized businesses and government agencies. An
attacker can exploit vulnerabilities of a vendor in the supply chain to compromise the IoT
system at the end-user. The recent SolarWinds attack is an example of an attack that has
resulted in a series of data breaches at government agencies. One seller of the Microsoft Cloud
services was compromised by the attacker, allowing the attacker to access the customer data
of its resellers. Once the attacker established a foothold in SolarWind’s software publishing
infrastructure after getting access to SolarWind’s Microsoft Oﬃce 365 account, he stealthily
planted malware into software updates that were sent to the users, which include customers
at US intelligence services, executive branch, and military.

The infamous Target data breach in 2013 is another example of supply-chain attacks. The
attacker ﬁrst broke into Target’s main data network through ill-protected HVAC systems.
The attacker exploited the vulnerabilities in the monitoring software of the HVAC systems,
which shared the same network with the data services.
It led to a claimed total loss of
$290 million to data breach-related fees [10, 22]. The supply-chain attacks would become
increasingly pervasive in IoT systems. Consider a next-generation industrial manufacturing
plant equipped with IoT devices that are supported by third-party vendors. The software
and the hardware of these devices can be trojanized. As a result, the attacker disrupts the
manufacturing plant, which can create a shortage of essential products (e.g., pharmaceutical
products, COVID19 vaccines, and gasoline) and lead to grave repercussions in the nation’s
supply chain.

Risk-based approaches have been studied to guide the procurement and design decision-
making process. This kind of approach oﬀers risk measurement, rating tools, and compliance
checking to identify and rank the vendors by their risk criticality. It is a useful preventive
measure that provides a transparent understanding of the security posture in the products,
systems, and services of the end-users and helps mitigate the risks prior to the procurement
contracts and continuous product development. Cyber resilience complements this measure.
It shifts the focus from prevention to recovery by creating a cyber-resilient mechanism to
reconﬁgure the IoT system adaptively to the uncertainties of adversaries and maintain critical
functions in the event of successful attacks.

Many private sectors have for years prioritized eﬃciency and low cost over security and
resilience.
In addition, they are agnostic to where these technologies are manufactured
and where the associated supply chains and inputs originate. This common practice has
resulted in enlarged attack surfaces and many unknown and unidentiﬁed threats in the IoT

1

Figure 1: Supply-chain attacks: An attacker ﬁrst attacks a vendor, who sells the users com-
promised products. They act as Trojans inside the user’s system and stealthily manipulate
it.

systems. A healthy ecosystem of vendors and suppliers is pivotal to secure and resilient IoT
systems. One challenge is that the IoT supply chain is becoming globalized. Manufacturers
and material suppliers are geographically diverse, thus increasing the uncertainties and the
vulnerabilities of the end-user IoT systems.
It is critical to check the compliance of the
products from the global supply chain to determine whether they would increase the cyber
risk of the IoT users.

One way to improve the health of the IoT supply chain is to design an IoT system with
built-in security and resilience mechanisms. For example, the integration of cyber deception
into IoT systems provides a proactive way to detect and respond to advanced and persistent
threats. Game-theoretic methods and reinforcement learning techniques have been used
to provide a clean-slate approach to designing cyber resilient mechanisms in response to
supply-chain attacks.

Apart from the technological solutions, accountability and cyber insurance are the socio-
economic ones that can be used to improve the cyber resilience of IoT end-users. Account-
ability, in general, is the ability to hold an entity, such as a person or organization, responsible
for its actions. An accountable system can identify and punish the party or the system com-
ponent that violates the policy or the contract. By creating accountable IoT supply chains,
we create an ecosystem where each supplier invests in cybersecurity to reduce the cyber risks
at each stage of the supply chain. A supplier would be held accountable if the failures of the
end-user system are attributed to it. Accountability establishes a set of credible incentives
for the suppliers and elicits desirable behaviors that mitigate the cyber risks. Accountability

2

Figure 2: The IoT supply chain can be protected using preventive measures which include
compliance checking and auditing. The supply chain resilience can be enhanced by building
real-time resilience measures (e.g., detection, adaptation, and reconﬁgurations). The residual
risk as a result of the preventive and real-time resilience measures can be further mitigated
by accountability and insurance mechanisms. Accountability is designed to attribute the
violations to the suppliers, who will be penalized based on the contract. Insurance is another
mechanism to transfer the remaining risks to a third party through an insurance contract.
The multi-tier solutions from preventive measures to insurance are interdependent and they
create consolidated protection of our IoT supply chain ecosystem.

can be viewed as part of the cyber resilience solutions succeeding the technological solutions,
especially when the technological resilience measures do not prevent the damages.

Insurance is another risk management tool to protect the end-users from cyber attacks
and failures by transferring their residual risk from an entity to a third party through an
insurance contract. It is the last resort when an IoT system cannot be perfectly accountable;
i.e., there is inadequate evidence to hold any one of the suppliers accountable, or when the
defects in the user’s design lead to unanticipated consequences. The residual risks would
be evaluated by an underwriter and the coverage can include the losses that arise from
ransomware and data theft or incidents caused by failures of IoT devices. Figure 2 shows
the relationships between preventive cyber measures and resilient cyber measures. The
cyber-resilient mechanisms include the technological real-time resilience measures as well as
accountability and insurance solutions. They constitute a holistic socio-technical solution to
protect the IoT systems from supply-chain threats.

Both accountability and insurance provide an additional layer of protection that reduces
the risks of IoT users. Accountability and insurance are system-level issues. We need to take
a system-scientiﬁc and holistic approach to understand their role in IoT systems and supply
chains, which would lead to an integrative socio-technical solution for supply chain security.

3

This chapter provides a quantitative deﬁnition to measure and assess the accountability in
the IoT supply chain that pertains to the system design, procurement contracts, as well as,
vendor description. Despite the focus of the chapter on cybersecurity issues, the deﬁnition
of accountability can be extended and used for general contexts of supply chain disruptions
caused by natural disasters and the defects in the products.

Game theory naturally provides a framework that captures the incentives and penalties
through utility functions for multiple interacting agents. In particular, mechanism design
theory explicitly provides a quantitative approach to create a reward and penalty mechanism
to elicit desirable behaviors at equilibrium. The violations from the desired behaviors would
be disincentivized or punished, while the compliance with the rules would be incentivized or
rewarded. In this chapter, we leverage these features of game theory to create computational
accountability and insurance framework for IoT systems and their supply chain.

Accountability is a system-level issue that encompasses detection and attribution of the
violations or anomalies, multi-agent interactions, asymmetric information, and feedback.
Game-theoretic methods provide a baseline for a system-scientiﬁc view for accountability.
We build a system scientiﬁc framework that bridges game theory, feedback system theory,
detection theory, and network science to provide a holistic view toward accountability in IoT
supply chains. The framework proposed here can be applied to understand accountability
in general.

One extension of this chapter is to investigate the concept of collective accountability,
where multiple agents are held accountable for the violations. One advantage of such ac-
countability mechanisms is the convenience in identifying the entities to be held accountable
and the implementation of the penalties. The disadvantage is that they are not targeted and
entities that are not directly linked to the violation of the failures would be also punished.

2 Literature Review

Accountability has been studied in many diﬀerent contexts in computer science [28, 12, 11].
K¨unnemann et al. in [19] have studied accountability in security protocols. Accountability
is deﬁned as the ability of a protocol to point to any party that causes failure with respect to
a security property. Zou et al. in [45] have proposed a service contract model that formalizes
the obligations of service participants in a legal contract using machine-interpretable lan-
guages. The formalism enables the checking of obligation fulﬁllment for each party during
service delivery and holds the violating parties for the non-performance of the obligations.
The deﬁnition of accountability in these works aligns with the deﬁnition in this chapter. An
accountable system has the ability to check and verify compliance with the requirements in

4

the agreement and identify the non-conforming behaviors and their parties.

There are several game-theoretic models that are closely related to accountability. For
example, inspection games are one class of games where the inspector determines a strategy
to examine a set of sampled items from a producer to check whether the producers of the
goods violated the standards. The producer aims to set a production strategy to minimize
the detection probability while minimizing the cost of maintaining high standards. The
inspection games have been used in many contexts such as patrolling, cybersecurity, and
auditing. Blocki et al. in [2] have studied a class of audit games in which the defender ﬁrst
chooses a distribution over n targets to audit and the attacker then chooses one of the n
targets to attack. It is better for the defender to audit the attacked target than an unattacked
target, and it is better for the attacker to attack an unaudited target than an audited one.
Rass et al. in [34, 35] have studied a multi-stage cyber inspection game between a network
system defender and an advanced persistent threat (APT) attacker. The defender needs to
choose an inspection strategy to detect anomalies at diﬀerent layers of the networks. The
attacker’s goal is to stay stealthy and ﬁnd strategies to evade the detection and compromise
the target.

Utility-theoretic approaches are useful to capture the incentives of the participants in
an agreement and their punishment. In [11], Feigenbaum et al. have formalized the notion
of punishment using a utility-theoretic, trace-based view of system executions. Violation is
determined based on the traces of the participants. When there is a violation, the participant
is punished. This punishment is captured through a decrease in the utility, relative to the
one without the violation. This approach to punishment is often seen in the literature of
mechanism design [24, 25]. The designer ﬁrst announces a resource allocation rule and
a payment or punishment rule. The participants in the mechanism know the rules and
determine the messages that they send to the designer. An incentive-compatible mechanism
is one in which the participants will truthfully reveal their private information through the
message under the allocation and the punishment rules. In other words, no participants have
incentives to lie about their private information under an incentive-compatible mechanism.
Mechanism designs have been used in many disciplines to study pricing of resources [9, 43, 44],
create security protocols [8], and design services [42, 41]. The framework that we present
in this chapter is built on the mechanism design approach. The utility-theoretic approach
conveniently captures the incentives of the suppliers and their behaviors. Furthermore, the
mechanism-design approach naturally creates a punishment mechanism to create incentives
for truthful behaviors. This type of behavior can be generalized to compliant behaviors in
supply chain agreements and contracts.

Our framework builds on this approach and bridges the accountability gap by incor-

5

porating the detection mechanism that enables the designer to detect and attribute the
non-compliant behaviors.
In addition, our framework distinguishes from prior works in
accountability by focusing on accountability in system engineering. This problem is instru-
mental in the development of large-scale IoT systems, where the building blocks of the IoT
systems are manufactured or designed by third parties. We integrate the critical component
of engineering designs into the accountability problem for IoT systems. The system designs
can contribute to accountability. A design is called transparent if it helps identify the cause
of the accidents; otherwise, a design makes the accountability inconspicuous. In some cases,
the cause of the accidents is not caused by the suppliers but the negligence in the design
process. It is important to have a framework that can consolidate multiple factors into the
framework and study accountability in a holistic manner.

3 Accountability Models in IoT Supply Chain

Figure 3: Supply chain accountability: the buyer of the product can identify the supplier of
a component who violates the policies or the contracts. The buyer can then use the contract
to penalize the identiﬁed supplier. The supplier can attribute the violation to his supplier.
It is called multi-stage accountability.

3.1 Running Examples

We introduce two running examples which will be used in later discussions for illustrations.

6

Example I: Uber Autonomous Vehicles
The Uber incident in Tempe, Arizona is another example of accountability of autonomous
vehicles. A pedestrian was struck by an Uber self-driving vehicle with a human safety
backup driver in the driving seat. The fatality is caused by the failure of the software
system which fails to recognize the pedestrian. Sensor technologies, including radar and
LiDAR, are sophisticated enough to recognize objects in the dark. Evidence has shown
that the pedestrian was detected 1.3 seconds before the incident and the system determined
that emergency braking was required but the emergency braking maneuvers were not enabled
when the vehicle is under computer control. The design of the software system is accountable
for the death of the pedestrian.

Example II: Ransomware attack on smart homes
A smart home consists of many modern IoT devices, including lighting systems, surveil-
lance cameras, autonomous appliance control systems, and home security systems. The com-
ponents of each system are supplied by diﬀerent entities. Smart home technology integrates
the components and creates a functioning system that will sense the home environment,
make online decisions, and control the system. The camera is accountable if the home se-
curity system does not respond to the burglary adequately due to a camera failure. There
is an increasing concern about ransomware attacks. Accountability enables the homeowner
to mitigate the impact of the ransomware by attributing the attack to a supplier of the IoT
devices.

Illustrated in the two examples, IoT supply chain security has a signiﬁcant impact on
the private sector and its customers. Several technologies have been proposed to track the
integrity of the supply chain to provide real-time monitoring and alerts of tampering and
disruptions. They provide a tool to monitor, trace, and audit the activities of all participants
in the supply chain and ensure that the contractually deﬁned Service Level Agreements
(SLAs) are followed. The essence of the technologies is to create transparency and situational
awareness for the companies. However, the software and hardware tampering is much harder
to monitor and track than the physical one. As a result, it creates information asymmetry
where the buyers or the systems do not have complete information about their suppliers. As
in the Target and the SolarWinds attacks, an attacker can get access to the system through a
compromised third-party vendor. It would require proactive security mechanisms to detect
and respond to the exploited vulnerabilities. We have seen the emerging applications of
cyber deception [32] and moving target defense [15] in both software and hardware to reduce
the information asymmetry and create proactive mechanisms for detection. They are tools
that contribute to real-time resilience measures as illustrated in Fig. 4 and provide inputs
for accountability in the next stage.

7

3.2 System Modeling

Figure 4: A supplier of type θ provides a description m of the product to a buyer who will
make a procurement decision a. The system designer develops a design d to integrate all
the components to form a functioning system. The system, as a result, yields an observable
performance y. The supply chain is said to be accountable if the malfunction of the system
can be attributed to the supplier who has misled the system designer. The supply chain
risk can be mitigated at three stages. The ﬁrst stage is compliance checking before the
procurement. The buyer can check whether the description of the product complies with
the standards, regulations, and requirements. The second stage is the contracting stage.
The buyer can make a contract that speciﬁes the penalty or the consequences if the supplier
does not fully disclose the product information. It will allow the buyer to hold the supplier
accountable when the root cause of the malfunction is at the supplier. The third stage is
cyber insurance. The buyer can purchase cyber insurance to mitigate the ﬁnancial impact
of the malfunction. The ﬁnancial risk is partially transferred to the insurer.

In this section, we provide a stylized model and a quantitative approach to accountability.
Fig. 4 describes three stages of interactions. At the ﬁrst stage, a supplier interacts with a
buyer to agree on an SLA contract. The supplier is characterized by the private information
θ ∈ Θ, which is a true description of the product of the supplier. For example, the supplier
is aware of the true security level and investment in the product but may not disclose the
information to the buyer. The supplier sends the buyer a message m ∈ M , which is the
informed description of the product. The description can prevaricate, hide, or sometimes lie
about the security information that would be useful in the procurement decisions. We say
that the supplier truthfully reports the product when θ = m; otherwise, we say that the
supplier misinforms the buyer. This misinformation can be unintended or intentional. In
the case of intentional behaviors, the supplier sends a manipulative message when he knows
his true type. For example, some foreign suppliers do not fully disclose the information
of their product with the aim to attract US customers due to its low cost. In the case of

8

unintended behaviors, the supplier may not be aware of the vulnerabilities of the product
and sends a description based on his perceived information. In this case, we can assume that
the private information θ is a function ρ : Θ × W (cid:55)→ Θ of the truth and uncertainties, i.e.,
θ = ρ(θt, wt), where θt ∈ Θ is the true value unobservable by the supplier and wt ∈ W is the
bias, modeled as a random variable, unknown to the supplier. This bias can be interpreted as
the uncertainties introduced by nature or a stealthy attacker that has unknowingly changed
the security attributes of the product. In both cases of unintended and intentional behaviors,
it is suﬃcient to assume that the type known to the supplier is θ.

Based on the product description m, the buyer can make purchase decisions. Let a = 1
denote the decision of adopting the product of the vendor and a = 0 otherwise. The decision
rule α : M (cid:55)→ [0, 1] yields the probability of purchase based on the received description, i.e.
α(m) = Pr(a = 1|m). This can be interpreted as the purchase preference from historical
records. If the buyer decides to adopt the product, then he determines how the product is
designed and integrated into the system. Here, we assume that the user and the designer
belong to the same organization and hence the procurement and design decisions are made
jointly. In other words, the user and the designer can be viewed as the same decision entity
who coordinates the design and procurement. In practice, the engineers design the systems
and send the procurement department the speciﬁcations and requirements for the needed
materials and components.

An IoT system consists of many components. We can classify the components into
ﬁve major categories: sensing, computation, control, communications, and hardware. The
sensing component allows the system to provide information about the environment, for
example, the LiDAR and temperature sensors. The computation units provide functions
and services for information processing and computations, for example, cloud services and
GPUs. The control components are used to instrument and actuate the physical systems,
for example, temperature adjustment and remote control. Communications provide the
information and data transmission among IoT components, e.g., LoRa and ZigBee wireless
communications. The hardware refers to the physical systems that underlie the IoT network,
for example, the manufacturing plant and the robots.

The designer builds an IoT system using a blueprint δ : M (cid:55)→ D, which yields a design
d = δ(m), d ∈ D based on the device descriptions and speciﬁcations provided by the sup-
plier. The system design leads to a performance y ∈ Y. For example, in Example I, the
designer develops a software system that integrates sensors, control algorithms, and the car.
Safety is a critical performance measure of autonomous vehicles. It can be measured by the
rate of accidents experienced by vehicles as of now. Here, we model the performance as a
random variable. Given α and δ, the distribution of the performance random variable is

9

py(y; θ, α(m), δ(m)), py : Θ × M (cid:55)→ ∆Y. Using Bayes’ rule, we arrive at

py(y; θ, α(m), δ(m)) = pθ

y(y; α(m), δ(m)|θ)pθ(θ),

(1)

where pθ(·) ∈ ∆Θ is the prior distribution of the type of product; pθ
y(y; α(m), δ(m)|θ),
pθ
y : M (cid:55)→ ∆Y, is an indication of all possible system performances given the attribute
of product θ. Note that the performance implicitly depends on m. The true performance
of the system is determined by the true attribute of the product and the procurement and
design decisions, which are made based on m. We denote pI = py(y; θ, α(θ), δ(θ)) as the ideal
system performance when the design and procurement decisions are made given a truthful
supplier, i.e., m = θ.

Without knowing the true attributes of the product θ, the performance anticipated by the
buyer is denoted by qy = py(y; m, α(m), δ(m)). When m (cid:54)= θ, there is a diﬀerence between
the observed performance py and the anticipated one qy. The buyer can perform hypothesis
testing based on the sequence of observations y1, y2, · · · , by setting up H0 as the hypothesis
that the observations follow the distribution qy and H1 otherwise. For example, in Example
I, this decision is particularly important when yi represents malfunctions or accidents for
each trial test driving. If the malfunction is not expected by the designer, then there is a
need to ﬁnd out which supplier is accountable for the accidents or, in the case of a single
supplier, whether the supplier should be held accountable.

3.3 Accountability Investigation

One critical step of accountability is the ability to attribute the performance outcomes to the
supplier. We start with the accountability of a single supplier with binary type Θ = {0, 1}
and assume the message space is the same as type space M = Θ. Consider a sequence of
repeated but independent observations Y k = {y1, y2, · · · , yk}, k ∈ N. A binary accountability
investigation is performed based on Y k. Based on the received m, hypothesis H0 is set to
be the case when the observations follow the anticipated distribution qy and H1 otherwise.
Depending on whether H0 or H1 holds, each observation yi admits the following distribution

H0 : yi ∼ fm(y|H0) = py(y; m, α(m), δ(m)),

H1 : yi ∼ fm(y|H1) = py(y; ¬m, α(m), δ(m)).

(2)

(3)

The optimum Bayesian investigation rule is based on the likelihood ratio, which is denoted

10

by

L(Y k) =

k
(cid:89)

j=1

py(yj; δ(m)|¬m)pθ(¬m)
py(yj; δ(m)|m)pθ(m)

,

(4)

where we omit the purchase decision because the performance can only be observed when
a = 1 and α(m) = P r[a = 1|m] is the same under both hypotheses. The likelihood ratio
test (LRT) provides the decision rule that H1 is established when L(Y k) exceeds a deﬁned
threshold value τk ∈ R; otherwise, H0 is established. It can be formulated by the equation

L(Y k)

H1
(cid:82)
H0

τk.

(5)

One critical component in accountability investigation is the prior distribution over hy-
potheses, which indicates the reputation of the supplier. Without knowing the true dis-
tribution of the type, we argue that reputation is suﬃcient knowledge to determine the
accountability of the supplier. Here we give the deﬁnition of reputation over a binary type
space, but the deﬁnition can be extended to multiple type space accordingly.

Deﬁnition 3.1 (Reputation) The reputation of the supplier π ∈ ∆H is a prior distribu-
tion over all hypotheses. In binary case, π0 = Pr[H0] is the prior probability that the supplier
truthfully report and π1 = Pr[H1] otherwise, with π0 + π1 = 1.

Assume the cost of the investigation is symmetric and incurred only when an error oc-
curs. In the binary case, the optimum decision rule will consequently minimize the error
probability, and the threshold value τk in LRT will reduce to

τk = π0/π1.

(6)

Deﬁnition 3.2 (Accountability)

1. Given an investigation rule, i.e., the threshold τk, the accountability PA ∈ [0, 1] is
deﬁned as the probability of correct establishment of hypothesis H1 based on the obser-
vations Y k and message m, which is given by

PA(τk) =

(cid:90)

Y1

fm(Y k|H1)dyk,

(7)

where Y1 is the observation space where Y1 = {Y k : L(Y k) ≥ τk}.

11

2. The wronged accountability PU ∈ [0, 1] is deﬁned as the probability of a false alarm
that H1 is established while the underlying truth is H0. Consider the threshold τk and
observations Y k, PU is given by

PU (τk) =

(cid:90)

Y1

fm(Y k|H0)dyk.

(8)

We call a supplier η-unaccountable if PA ≤ η, for a threshold accountability η ∈ [0, 1]
chosen by the investigator. In this case, the system does not have strong conﬁdence that the
observed accidents are caused by the supplier. We call a system (cid:15)-nontransparent if PA ≤ (cid:15),
for a given small (cid:15) ∈ [0, 1]. That is, the system is close to being unable to hold the vendor
accountable for the accidents.

The performance of the accountability investigation will be evaluated in terms of PA
and PU .
Ideally, we would like to conduct error-free accountability testing where PA is
close to one and PU is close to zero (correctly identify accountable supplier without making
mistake). However, the deﬁnition above leads to a fundamental limit on the accountability
of the supplier. Except for situations where the observations Y k under H0 and H1 are
completely separable or the number of observations k goes to inﬁnity, the performance of
the accountability testing will be restricted within a feasible region.

Deﬁnition 3.3 (Accountability Receiver Operating Characteristic) Accountability
Receiver Operating Characteristic (AROC) is a plot which describes the relationship between
achievable accountability PA and wronged accountability PU in the square [0, 1] × [0, 1].

As shown in Fig. 5, if we conduct LRT in accountability investigation, the AROC curve
depicts the testing performance with respect to diﬀerent threshold values τk. Similar to
traditional binary hypothesis testing, the AROC curve under proper design preserves the
following properties [20].

Property 3.1 (AROC) AROC curve under proper design has the following properties:

(1) (PU , PA) = (0, 0) and (1, 1) belong to the AROC.

(2) The slope of the AROC curve dPA(τk)/dPU (τk) is equal to the threshold τk.

(3) The AROC curve is concave and the feasible domain of (PU , PA) is convex.

(4) PA(τk) ≥ PU (τk), ∀τk ∈ [0, +∞).

12

Figure 5: Accountability receiver operating characteristics (AROC).

Remark 1 The likelihood ratio lies in the region between zero and inﬁnity. If we set the
threshold τk in LRT to zero, investigator will classify any performance results into hypothesis
H1 (misinformation). Both accountability PA and wronged accountability PU will approach to
one, as (PU , PA) = (1, 1). Similarly, if we set τk in LRT to inﬁnity, investigator will classify
any performance into hypothesis H0 (truthfully report), resulting in (PU , PA) = (0, 0).

Remark 2 Property (3) and (4) are satisﬁed under the proper design, i.e. the test is “good”
with PA ≥ PU . For a “bad” test with PA < PU , because of the real meaning behind the
hypothesis, we cannot simply reverse the performance distribution as in traditional hypothesis
testing.
Instead, we need to re-construct the investigation and ﬁnd another performance
metric that can properly distinguish the misinformation between the supplier and buyer.

It is worth noting that as the threshold τk increases, the accountability of the supplier
PA increases. However, according to the aforementioned properties, it would also increase
wronged accountability PU when the accidents are not caused by the vendor. There is a
fundamental trade-oﬀ between accountability PA and wronged accountability PU depending
on the accountability investigation. One way to evaluate the investigation performance is

13

the area under the AROC curve (AUC). AUC is a measure of investigation capability [40],
which provides a simple ﬁgure of merit to represent the degree of separability between two
hypotheses.

AU C(τk) =

(cid:90) 1

0

PA(τk) dPU (τk)

(9)

This value varies from 0.5 to 1. When AUC equals 0.5, the designed investigation has no
separation capability, which means the performance of the test is no better than ﬂipping
a coin. This is corresponding to the case when PA(τk) = PU (τk) for all possible threshold
τk. Ideally, an excellent test will produce an AUC equal to one. In this situation, the ac-
countability investigation can completely distinguish between two hypotheses, thus correctly
identifying the supplier who should be accountable for the accidents.

Unfortunately, in realistic investigation tasks, it is hard to obtain the exact computation
of AUC. Analyzing the upper and lower bounds of AUC will help the investigator to describe
the performance of the designed test. Shapiro in [36] provides an upper bound and lower
bound on binary testing. Consider equally likely hypotheses with τ = 1, the probability of
error Pe ∈ [0, 1] is deﬁned as

Pe =

PU (τ = 1)
2

+

1 − PA(τ = 1)
2

.

Due to the convexity of the AROC curve, the bounds of the AUC can be described as

1 − Pe ≤ AU C ≤ 1 − 2P 2
e .

(10)

(11)

3.4 Model Extensions

This framework can be extended to multiple product types and multiple suppliers. The
accountability needs to point to any suppliers that cause failures under the hypothesis.
In this section, we provide several testing frameworks and the deﬁnition of accountability
accordingly.

3.4.1 Single Supplier with Multiple Types

Consider the product from the supplier with T ∈ N possible types, Θ = {θ1, θ2, . . . , θT }.
Based on the received message m = θm, hypotheses {H1, H2, . . . , HT } can be constructed by

14

the investigator such that the performance observation y under each hypothesis Ht admits

Ht : y ∼ fm(y|Ht) = py(y; θt, α(θm), δ(θm)),

(12)

for 1 ≤ t ≤ T . The distribution under hypothesis Ht describes the system performance if
the buyer makes purchase and designs based on the message θm while the underlying true
product type is θt. In this case, the only anticipated performance by the buyer follows Hm.
Any other observation distribution Ht(cid:54)=m will attribute to the accountability of the supplier.
Investigation could be conducted through M-ary hypothesis testing. For a single supplier
with multiple product types, we can deﬁne the accountability as follows.

Deﬁnition 3.4 (Accountability with multiple types) Given a detection rule λ, the re-
ceived message m and observations Y k, the accountability for a single supplier with multiple
product types is deﬁned as

PA(λ) =

(cid:88)

(cid:90)

t(cid:54)=m,1≤t≤T

Yt

fm(Y k|Ht)dyk,

(13)

where Yt is the observation space we classify the observations as Ht.

If we assume the investigation cost is symmetric and only occurs with error, this gives
a MAP decision rule and the performance of the accountability testing can be evaluated
through the error probability as

Pe =

(cid:88)

1≤t≤T

Pr(E|Ht)π(t),

(14)

where E denote the error event and π(·) ∈ ∆Θ is the prior probability that Ht will happen,
which represents the reputation of the supplier.

3.4.2 Multiple Suppliers

In IoT system design with multiple suppliers, accountability testing needs to point to any
suppliers that cause failures under the hypothesis. To simplify the illustration, we consider
the case where the component from each supplier may have binary types θi ∈ {0, 1}, ∀i ∈
I. Consider the problem with N vendors in the supply chain. Each supplier i ∈ I =
{1, 2, . . . , N } with true product type θi will send a message mi ∈ Mi to the buyer to make
purchase decision ai ∈ {0, 1} and determine the overall design d ∈ D. The process is

15

Figure 6: Extension of the model to multiple suppliers.

illustrated in Fig. 6. We can construct hypotheses as a vector

Hj = (h1, h2, . . . , hN ),

hi = 1(mi (cid:54)= θi) ∀i ∈ I,

(15)

where each element hi is an indicator of whether supplier i truthfully reports or not, and the
subscript 0 ≤ j ≤ 2N − 1 is the decimal number of the binary combination in the vector. The
hypothesis vector indicates which supplier(s) should be accountable for the accident. When
the performance distribution under each hypothesis is distinguishable, the investigation could
be conducted through M-ary hypothesis testing. Otherwise, we can consider decentralized
investigation as described in the following.

Figure 7: Decentralized testing

16

Consider a decentralized accountability investigation with 2N hypothesis H0, .., H2N −1 and
prior reputation π(H0), . . . , π(H2N −1), respectively. Suppose we have N suppliers providing
components to the system. Each component investigator λi is inspecting the performance
related to the product provided from the vendor i. In practice, we can design the independent
tests for each component to determine the accountability of supplier i. We can control the
other parts (j (cid:54)= i) to be known and ﬁxed products in test design and focus on the binary
hypothesis testing with respect to component i.

Each component investigator receives observations yi, which is a random variable taking
values in a set Yi. The local investigator will conduct accountability testing through λi :
Yi
(cid:55)→ {0, 1} and output a binary decision variable hi = λi(yi), which indicates whether
supplier i should hold accountable for the accident. This reduces the problem to N parallel
binary hypothesis testing with each supplier, and the accountability of each supplier then
will be the same as we deﬁned in 3.2. The ﬁnal investigator determines which hypothesis will
be established based on received information, λ0 : {0, 1}N → {0, 1, . . . , 2N − 1}. It has been
shown in [38] and [27] that there exists an optimal detection rule if each testing observations
are independent or conditionally correlated under each hypothesis.

4 Case Study 1: Autonomous Truck Platooning

In the following section, we will provide a detailed case study in autonomous truck platooning
with adaptive cruise control (ACC) system. This example illustrates the case when the true
performance is unknown to the investigator. We will discuss the accountability of the ranging
sensor supplier in the case of a collision.

4.1 Background

With the rapid development of autonomous vehicles, safety is one of the main priorities
for manufacturers. As estimated by the World Health Organization (WHO), the number
of annual road deaths with collision has reached 1.35 million worldwide [30]. The recent
incident in Tempe, Arizona, has thrown a spotlight on the safety of autonomous vehicles.
The Uber self-driving test car caused the death of the pedestrian because of the failure of
braking control by the autonomous driving system. The investigation of accountability is
crucial to determine the cause of the collision and provides insights for future car design.

In this case study, we consider the task of autonomous truck platooning with Adaptive
cruise control (ACC) system. Adaptive cruise control is a driver assistance technology that
maintains a safe following distance between the vehicle and traﬃc ahead without any inter-

17

vention by the driver. If the preceding truck is detected traveling too slowly or too close, the
ACC system will react by automatically activating the brakes and mitigating potential col-
lisions. Brake control is determined based on the relative distance, relative velocity, and the
acceleration of leading and the following truck. The speed and acceleration of both vehicles
can be measured by built-in speed sensors and accelerometers. Ranging sensors, including
radar and LiDAR, are used for distance detection in the ACC system. The upper-level con-
trol system uses the measurements of the sensors to interpret the driving environment, and
trigger appropriate brake action to mitigate collision [37]. Thus, the detection range and
precision of the ranging sensor are critical in ACC design. Defective ranging sensors could
cause severe consequences and should be held accountable in case of such a collision.

4.2 Vehicle Dynamics Model

Figure 8: Host truck with ACC system following the leading truck.

To illustrate the accountability of the ranging sensor in this framework, we ﬁrst introduce
the dynamics model of the problem. Consider the testing scenario in Fig. 8, where the host
truck equipped with ACC system is approaching the preceding vehicle. The control goal
of the ACC system is to maintain the desired safe distance from the leading vehicle. The
desired distance L is normally determined by Constant time gap spacing policy in ACC
systems, which guarantees the individual vehicle stability and string stability [37].

L = vh · tgap,

(16)

where vh is the speed of the host vehicle and tgap is the constant desired time gap.

Denote xi, vi, ai as the position, velocity and acceleration of the leading (i = l) or host
(i = h) vehicle. We assume the leading vehicle is at constant speed vl(t) = v0. The system

18

state vector x(t) and control vector u(t) are deﬁned as follows [39].

x(t) =

(cid:104)
∆x(t) − L, ∆v(t)

(cid:105)T

, u(t) =

(cid:105)
(cid:104)
ah(t)

,

(17)

where ∆x(t) = xl(t) − xh(t) is the current distance and ∆v(t) = vl(t) − vh(t) is the relative
speed between the leading and following vehicles. The state space representation of the
system can be written as

˙x(t) = Ax(t) + Bu(t),

y(t) = Cx(t) + w(t),

The matrices are given by

A =

(cid:35)

(cid:34)

0 1
0 0

, B =

(cid:35)

(cid:34)

−tgap
−1

, C =

(cid:104)

(cid:105)

,

1 0

(18)

(19)

(20)

where y(t) = ∆x(t) − L + w(t) is the noisy control error between the desired distance and
current distance; w(t) is the observation noise. We assume the observation disturbance is
modeled by an additive white Gaussian noise,

w(t) = N (0, σ2).

(21)

The variance σ2 indicates the inﬂuence of the measurement environment. The intuition
behind using the Gaussian noise model is that it gives a good approximation of the natural
processes. If a speciﬁc distribution of measurement error is given, the noise model can be
changed accordingly and the accountability testing framework will still work.

The optimal control can be achieved through linear quadratic regulator (LQR) control.

We deﬁne the cost function with zero terminal cost as

J =

1
2

(cid:90) ∞

t=0

x(t)T Qx(t) + u(t)T Ru(t) dt,

where the diagonal weights

Q =

(cid:35)

(cid:34)

w1
0
0 w2

, R =

(cid:105)
(cid:104)
1

.

(22)

(23)

The goal of the controller is to regulate the state towards (0, 0)T . The feedback optimal

19

control low is given as

u(t) = −R−1BT P x(t)

where P is the solution to the following associated algebraic Riccati equation:

0 = P A + AT P + Q − P BR−1BT P.

(24)

(25)

The aforementioned vehicle dynamics model and optimal control describe the system
design δ of the ﬁnal ACC system based on the information provided by the supplier. Diﬀerent
control methods and system design can be implemented to achieve the same goal. In the
following section, we assume that this system design is not the cause of the collision and
purely focuses on the accountability of the sensor supplier.

4.3 Accountability Testing

The true product attributes play an important role in control system design. From the
previous section, the optimal control of the system depends on the correct distance detection
between the two objectives. Thus, the sensor with degraded detection result should hold
accountable if the ACC system fails to maintain the safety distance and causes a collision.
To attribute the ACC system performance to the ranging sensor supplier, we conduct the
following accountability testing with respect to the ranging sensor.

For the simplicity of the model, we consider two types of ranging sensor θ ∈ Θ =
{0, 1}, which diﬀer in the detection precision. We assume the sensor with type θ = 1 is
functioning normally, as the detection result r1(t) = ∆x(t); while the sensor with type θ = 0
is malfunctioning with detection result r0(t) = ∆x(t) + ed. The value ed is the detection
error of the ranging sensor. The damaged sensor will put the host vehicle at risk of collision,
since the actual distance is closer to the detection result.

The true property of the sensor is private information to the supplier, which is not revealed
to the system designer. The supplier should hold accountable for a collision if there exists
misinformation between the product description m and true product property θ. Note that
the misinformation can be unintended or intentional. We would like to determine whether
the ranging sensor supplier should hold accountable for such an accident.

Consider the testing scenario in Fig. 9. The distance detection result from the sensor will

be the input of the state vector as

(cid:104)

x(t) =

rθ(t) − L, ∆v(t)

(cid:105)T

.

(26)

20

Figure 9: Accountability testing with diﬀerent sensor types.

We use the ﬁnal distance control error as the performance y of the ACC system when testing.
Suppose the supplier report m = 1 when signing the contract. Consider a noisy observation
results y as described in (19), then the performance should follow

y ∼ py(y; 1, α(1), δ(1)) = N (0, σ2).

This is the anticipated distribution of the observations if the supplier truthfully report the
product type (m = θ = 1). On the other hand, if the supplier misinforms the buyer
(m (cid:54)= θ = 0), the performance should follow

y ∼ py(y; 0, α(1), δ(1)) = N (−ed, σ2)

The negative distance control error suggests that the distance between two vehicles is smaller
than the desired safety distance requirement L, which can lead to a potential collision.

We set up the following hypotheses to estimate the accountability of the supplier who
reports m = 1. Let Y = [y1, y2, . . . , yN ] ∈ RN be a vector of independent identically
distributed observations yk (1 ≤ k ≤ N ) of the aforementioned testing scenarios.

H0 : Y ∼ N (−ed, σ2IN )
H1 : Y ∼ N (0, σ2IN )

where IN is the identity matrix of size N . To keep the consistency with other studies, H1
represents the case that the supplier truthfully report. H0 suggests there exists misinforma-
tion between the reported product description m and true product type θ. The supplier will

21

be accountable if the investigator correct detected that hypothesis H0 should be established.
Assume the cost of the decision is symmetric and incurred only when an error occurs. The
reputation of the supplier follows [π0, π1]. In Bayesian binary hypothesis testing, LRT will
compare the likelihood ratio to threshold τ = π0/π1. The result suggests that the hypothesis
H0 will be established if the sample mean S is smaller than the testing threshold η, as shown
in the following

where

S =

1
N

N
(cid:88)

i=1

yi

H1
(cid:82)
H0

η

η =

ed
2

+

σ2 ln(τ )
N ed

(27)

(28)

Given the decision rule and supplier’s reputation ratio τ , the accountability and wronged
accountability of the sensor supplier who reported m = 1 is

PA(τ ) =

PU (τ ) =

(cid:90)

Y0

(cid:90)

Y0

f1(y|H0)dy = 1 − Q

f1(y|H1)dy = Q

(cid:18) d
2

ln(τ )
d
(cid:19)

+

(cid:18) d
2
ln(τ )
d

−

(cid:19)

(29)

(30)

where Q(x) is the Gaussian Q function and d = N 1/2ed/σ [20].

4.4 Parameter Analysis

The accountability of the sensor supplier helps the investigator to determine whether the
failure of the ACC system should be attributed to the sensor. Since the accountability
depends on parameters such as sampling size N , environmental observation noise variance
σ2 and sensor range diﬀerence ed.
In this section, we discussed several numerical results
under diﬀerent cases.

Figure 10 depicts the inﬂuence of the number of tests N and sensor detection error ed on
the accountability. First, we notice that the PA → 1 and PU → 0 as the number of tests N
increases. This phenomenon indicates more testing will produce a more accurate detection
of the supplier’s accountability. From equation (27), we note that the observation means
S converges almost surely to the expected mean of each hypothesis as N → ∞. Besides,
the second term in the testing threshold η vanishes, and we end up comparing the expected
mean of Y to the middle point ed/2 of two hypothesis means.

22

(a) Accountability PA

(b) Wronged Accountability PU

Figure 10: Diﬀerent sensor range diﬀerence (σ = 2, π0/π1 = 0.5/0.5)

The inﬂuences of sensor detection error ed is also illustrated in Fig 10. The prior is set to
π0 = π1 = 0.5, which means that we do not favor any hypothesis before testing. From Fig 10,
as the range diﬀerence between two types increases, the PA and PD curves are associated
with a more rapid change with respect to N . It suggests that if the qualities of the two types
of sensors have a signiﬁcant diﬀerence, it will be easier for the investigator to determine the
accountability of the supplier within a fewer number of tests.

Figure 11: Impact of supplier’s reputation (σ = 2, ed = 2, N = 30)

Fig. 11 displays the impact of supplier’s reputation on the accountability estimation.
The ratio τ = π0/π1 represents the reputation of the supplier. A larger value of τ indicates
that we have a strong belief the supplier is lying. Normally, we are more likely to suspect

23

that the supplier with a bad reputation would be accountable for the incidents. As shown
in Fig 11, when we ﬁx the testing environment, the accountability of supplier PA increases
as τ increases. However, it should be noted that the wronged accountability PU increases
as well. This is because the increase of τ will cause the testing threshold η in LTR will
increase, leading to a larger observation space Y0 where we classify the observations as H0.
Thus, both PA and PU will increase according to the deﬁnition. The wronged accountability
misattributes the incident to the supplier when they should not be accountable. We will see
more details about the trade-oﬀ between PA and PU in the following section.

4.5 Investigation Performance

4.5.1 Accountability Receiver Operating Characteristic

In the context of this ACC case study, we are interested in the relationship between account-
ability PA and wronged accountability PU . as

PA =

PU =

(cid:90)

Y0

(cid:90)

Y0

fm(y|H0)dy = 1 − PF

fm(y|H1)dy = 1 − PD

(31)

(32)

Because of the symmetric property of the Gaussian Q function, the ROC curve is invariant
under this transformation. From equations (29) and (30), if we eliminate the parameter τ ,
the relationship between PA and PU can be written as

PU = Q(d − Q−1(1 − PA))

(33)

The relationship between PA and PU is traced out as the threshold τ in LRT varies from 0
to ∞. Note that this relationship depends on the variable d = N 1/2ed/σ. We plot the ROC
curve under diﬀerent d values in the following ﬁgure.

The slope of the AROC at point (PA(τ ), PU (τ )) is equal to the supplier’s reputation
τ [20]. Ideally, we would like to conduct a hypothesis test such that PA is close to one and
PU is close to zero. As we can see from the ﬁgure, the ROC curve approached the ideal
test point when the value of d increases. This result coincides with our aforementioned
analyses. Increasing the number of test N , comparing sensor with larger sensor error ed, and
reducing the observation variance σ can all increase the value of d, leading to a more reliable
accountability test result.

24

Figure 12: ROC curve under diﬀerent d

4.5.2 Area under the AROC curve

In the ACC sensor accountability testing case, the exact AUC value and its bounds with
respect to d are shown in Fig. 13. From the ﬁgure, we can see that the performance of the
hypothesis testing increases along with the value d. In fact, in testing with the Gaussian
hypothesis, the value d indicates the Chernoﬀ distance between the two Gaussian distribu-
tions [20]. A larger value of d means the distribution of H0 and H1 have less overlap, thus
it is easier to separate between them. Since we have the exact expression of Pe, the bounds
of AUC can be expressed as

1 − Q

(cid:19)

(cid:18) d
2

≤ AU C(d) ≤ 1 − 2Q2

(cid:19)

.

(cid:18) d
2

(34)

Figure 13: Bounds of AUC under diﬀerent d

25

5 Case Study 2: Ransomware in IoT Supply Chain

In this section, we provide a second case study of supplier accountability in smart home IoT
under ransomware attacks. This example illustrates how we determine accountability in a
supply chain and sophisticated systems involving diﬀerent components.

5.1 Background

Ransomware is a type of malware that infects particular network entities to demand ransom.
This kind of attack is becoming more prevalent nowadays with the fast development of
IoT systems. The broad connections for IoT devices provide more security threats and
vulnerabilities. Besides, the massive number of IoT devices increases the risk of getting
infected by ransomware since any device could be the target. Indeed, the ransomware attack
has caused signiﬁcant economic losses in industrial domains. The estimated global damage
from ransomware reaches $20 billion in 2021 [5].

Smart home technologies integrate diﬀerent IoT-enabled components to provide advanced
services within the home environment. The components from diﬀerent suppliers contribute
to addressing various challenges to improve the quality of human life. However, their limited
processing capabilities make them vulnerable to security threats [13], including ransomware.
If the component in the home security system is taken controlled by the attacker, the end-
user may face serious economic loss and privacy leakage. The user needs to determine which
part of the IoT system should hold accountable for the accident. Our framework provides a
way to mitigate the impact of ransomware by attributing the accident to a supplier of IoT
devices.

5.2 Smart Lock and Ransomware Attack

Nowadays, smart home technologies have been widely accepted by individuals and organi-
zations to improve home security. With the development of IoT and machine learning, the
number of smart lock users are increasing in recent decades. Instead of physical keys, smart
lock utilizes face recognition and/or ﬁngerprint veriﬁcation to achieve digital authentication.
Most smart locks also are equipped with intruder alert and remote control when you are
physically away from home. This innovation avoids the threats with cloneable physical keys
and provides a front-line deterrent against potential intruders.

While the smart lock oﬀers convenience to homeowners, the transition towards digital
control brings concerns over security in cyberspace. One potential threat is the ransomware
attack. This type of attacks belongs to the family of Advanced Persistent Threats (APTs).

26

A malicious attacker attack your smart home IoT system, lock the front door of your house,
and request a ransom. The highly-connected feature of IoT provides the attacker multiple
vulnerabilities as the entry point into the network. Once building a foothold in the network,
the attacker moves laterally towards the target to achieve his goal, in this case, locking the
door and denying legitimate access. Once compromised by ransomware, the dangling par-
ticiple would be huge if someone under medical conditions is locked and requires immediate
treatment. We may be discouraged by the fact that victims simply pay the ransom in many
cases, and even the FBI once inadvertently mentioned paying the ransom if the network
device is infected [6].

Figure 14: IoT supply chain related to security lock.

To mitigate the loss under such ransomware attacks, accountability investigation provides
a way to check the responsibility of the IoT device supplier(s) regarding the attack. It is
important for the investigator to ﬁnd out the initial attack entry that poses a risk to the whole
system. Due to the tiered structure of the supply chain, the accountability investigation
needs to be constructed through a top-down layered tree analysis as shown in Fig. 14.
This structure helps the investigator to narrow down the search scope and determine the
accountability of the suppliers among diﬀerent supply chain tiers. More details will be
provided in the following section.

27

5.3 Accountability Investigation

5.3.1 Tier-1 Investigation

Face recognition and ﬁngerprint veriﬁcation are two critical parts of smart lock authenti-
cation. The failure of the smart lock could be caused by the failure of one or both of the
functions. In this case, the ﬁrst step in accountability investigation is to determine whether
the tier-1 suppliers of these two parts need to be accountable for the ransomware attack. As
described in Sec. 3.4.2, this is corresponding to the accountability investigation of multiple
suppliers.

Denote the supplier of face recognition technology as i = 1 and the supplier of ﬁngerprint
veriﬁcation technique as i = 2. We assume that each supplier may have binary types
θi ∈ {0, 1}. θi = 0 means that the provided product operates normally and θi = 1 stands
for malfunctioning. By default, each supplier sends a message mi = 0 and guarantees the
product functionality when signing the contract with the buyer. Thus, we can construct the
following hypotheses as in Table 1. Denote hi, i = {1, 2} as the accountability of supplier
i. ˆH0 indicates that both parts are operating normally as reported; ˆH1/ ˆH2 suggests that
there be misinformation from one of the suppliers; ˆH3 means both suppliers need to hold
accountable for the ransomware attack.

Hypothesis h1 = 1(θ1 (cid:54)= 0) h2 = 1(θ2 (cid:54)= 0)

ˆH0
ˆH1
ˆH2
ˆH3

0

0

1

1

0

1

0

1

Table 1: Four hypotheses in accountability investigation.

Instead of looking into the joint performance of the two components, it is practical to con-
duct independent decentralized investigations into each of the suppliers as shown in Fig 15.
We take the face recognition system h1 for example. The investigation of the ﬁngerprint
veriﬁcation h2 can be conducted in the same manner. Suppose the normal operating face
recognition system can correctly detect the registered identity with µ0 = 9% accuracy. If
this system is destructed by the ransomware attacker, we would expect a lower identiﬁcation
accuracy, i.e. µ1 < µ0. To investigate the accountability of the face recognition system,
we design the following testing scenarios. On each trial, diﬀerent photos of registered faces
are displayed randomly in front of the device. The performance yi ∈ {0, 1} at each trial

28

Figure 15: Decentralized tier-1 accountability investigation.

is an indicator of the testing results, where yi = 1 represents correct identiﬁcation and
yi = 0 otherwise. Let Y N = {y1, y2, . . . , yN } be a sequence of independent and identically
distributed trials, we consider the following hypotheses for accountability testing. For each
trail 1 ≤ i ≤ N ,

H0 : yi ∼ Bern (0.9) ,

H1 : yi ∼ Bern (µ1) ,

where µ1 < µ0 = 0.95. Bernoulli distribution is a natural model to describe events with
Boolean-valued outcomes under certain success probability. In this hypothesis model, H0
indicates that the face recognition system operates normally with 90% detection accuracy
on average. H1 suggests a degraded identiﬁcation accuracy. This investigation aims to ﬁnd
out whether hypothesis H1 should be established based on the system performance.

One limitation of Bayesian tests as described in Sec.4 is their reliance on the prior knowl-
edge π, i.e., the reputation of the supplier, and costs assigned to diﬀerent decision errors.
The choice of decision cost depends on the nature of the problem, but the prior probabilities
must be known. In many applications, the prior knowledge may not be obtained precisely;
thus, the correct value of the threshold in LRT is unknown. In the ransomware case study,
the misinformation between the supplier and buyer may be unintended. It is challenging to
determine the probability π1 that the supplier is compromised by the attacker. It is natural
to consider alternative tests that can achieve desired detection results without such prior
knowledge.

Neyman and Pearson [26] formulated a test λ that maximizes the correct detection prob-
ability PA(λ) (accountability) while ensuring the false-alarm probability PU (λ) (wronged
accountability) is subject to a upper bound constraint α. This can be formulated as

max
λ

PA(λ) =

s.t. PU (λ) =

(cid:90)

Y1

(cid:90)

Y1

fm(Y N |H0)dyN ,

fm(Y N |H1)dyN ≤ α.

29

(35)

This constrained optimization problem requires no prior knowledge about reputation and
decision cost function. The only parameter that needs speciﬁcation is the maximum accept-
able wronged accountability α. A classic result due to Neyman and Pearson shows that the
optimal solution to this type of investigation is a likelihood ratio test (LRT).

Lemma 5.1 (Neyman-Pearson) Consider the likelihood ratio test in (5) with τk > 0 cho-
sen so that PU (τk) = α. There does not exist another test λ such that PU (λ) ≤ α and
PA(λ) ≥ PA(τk). Hence, the LRT is the most powerful test with false-alarm probability
PU (λ) less than or equal to α.

In the accountability investigation of the face recognition system, both hypotheses admit

a Bernoulli distribution. The likelihood ratio is given by

L(Y k) =

(cid:81)N

(cid:81)N

i=1 µyi
i=1 µyi

1 (1 − µ1)1−yi
0 (1 − µ0)1−yi

=

(cid:18) 1 − µ0
1 − µ1

(cid:19)N (cid:18)µ0(1 − µ1)
µ1(1 − µ0)

(cid:19)(cid:80)N

i=1 yi

.

The suﬃcient statistics of such testing will be the sum of all performance results S =
i=1 yi. According to Neyman-Pearson lemma, the most powerful test will hold the supplier

(cid:80)N

accountable if S < λ for a constant threshold λ.

S =

N
(cid:88)

i=1

yi

H0
(cid:82)
H1

λ

Under H0, the detection accuracy is on average µ0, and S admits to a binomial distribution,
S ∼ Binomial(N, µ0). To ensure PU (λ) = α, the threshold λ is chosen to be the α quantile
of the Binomial(N, µ0) distribution.

λ = Q(α) = inf {x ∈ R : α ≤ FS(x)} ,

where FS(x) is the cumulative distribution function of random variable S. Note that as this
is a discrete distribution, it may not be possible to get the exact α and λ desired. One way
to address this problem is to increase the total number of trials N and approximate the
binomial with a Gaussian distribution according to the central limit theorem.

In the IoT ransomware attack case, the changes made by the stealthy attacker often
remains unknown even after investigations. Thus, it is hard to determine identiﬁcation
accuracy µ1 after the attack and ﬁnd the exact performance distribution under hypothesis H1.
We can only assume that the attack results in a degraded identiﬁcation accuracy as µ1 < µ0.
Neyman-Pearson test provides a way to investigate the accountability of the supplier with
limited prior knowledge. It guarantees that the correct detection probability PA is maximized

30

Figure 16: Neyman-Pearson test result for tier-1 investigation.

under the false-alarm constraint PU ≤ α. In the context of the IoT supply chain attack,
Neyman-Pearson test paves the way for the buyer to investigate the accountability of the
supplier with limited information.

5.3.2 Multi-stage Accountability Investigation

The tier-1 investigation examines the accountability of each tier-1 supplier. However, due to
the layered structure of the IoT supply chain and the sophisticated feature of the ransomware
attack, the true cause of the attack may lie in the suppliers in the subordinate tiers. Tier-1
suppliers can further attribute the malfunction to their suppliers following a similar fashion.
A top-down layered investigation is needed if we would ﬁnd out the origin of the attack and
obtain a holistic view of the entire supply chain. This is called a multi-stage accountability
investigation.

For instance, if the face recognition system should hold accountable for the attack accord-
ing to the tier-1 investigation, the supplier could further investigate the components that the
system consists of. There may exist diﬀerent types of vulnerabilities in the components that
are provided by tier-2 suppliers. The attacker could break into the system by compromising
the ill-protected camera and further penetrating into the system. Another possibility is that
adversaries against face recognition are performed at the detection software. If the latter
case holds true, the detection software provider can further check which part of the software
is malfunctioning. Face recognition attacks can be performed at the database, the predeﬁned
algorithm parameters, the communication channels, etc. The multi-stage accountability in-

31

vestigation aims to further ﬁgure out which among the vulnerabilities is the underlying cause
of the attack.

Figure 17: Multi-stage accountability investigation.

To analyze the accountability of the involved suppliers at each tier, we view the supply
chain as a directed graph as shown in Fig. 17. The arrows in the graph indicate the procure-
ment relationship. Multi-stage accountability starts from the top tier node, the ﬁnal product.
The accountability investigation on each supplier i produces accountability P i
A subject to an
investigation cost Ci. Whether a supplier is accountable depends on the comparison between
A and selected threshold (cid:15) ∈ (0, 1). We call a supplier accountable if P i
P i

A > (cid:15).

If the current supplier is determined to be non-accountable (P i

A < (cid:15)), there is no need to
continue investigation among its suppliers. In the ransomware example, if we determine that
the face recognition system solely should hold accountable in the tier-1 investigation, there
is no need to conduct an accountability check for the suppliers related to the ﬁngerprint
veriﬁcation system. Deductive reasoning helps reduce the investigation eﬀorts on unrelated
system components and focus on the ones that attribute the accident. It provides a way to
prioritize the contributors leading to the top event.

It should be noted that the product design of each sub-system can also be the cause of
the vulnerability that exposes the system to threats. This brings up the question that how
deep we should investigate during the process. Suppose the total investigation budget is B.
The investigator needs to decide whether to continue the investigation or simply stop and
replace the component. Replacement will be a better choice if the remaining budget cannot
support further investigation as

B −

(cid:88)

i∈I

Ci ≤ Cnext,

where I is the set of investigated suppliers and Cnext is the investigation cost of the next

32

supplier. The trade-oﬀ between investigation and replacement may be another dimension to
be considered when conducting multi-stage accountability investigations.

Multi-stage accountability investigation is an iterative analysis process to ﬁnd the cause
of the accident. The layered approach provides a way to understand how the system fails,
identify the vulnerabilities in the IoT supply chain, and determine the accountability of
If
any supplier.
the structure of the supply chain has been upgraded (e.g., component replacement), it can
provide a set of steps to design quality tests and maintenance procedures.

It also creates the foundation for any further analysis and evaluation.

6 Compliance and Cyber Insurance

6.1 Compliance Modeling

The description m ∈ M from the supplier to the buyer is a self-reporting mechanism that
requires the vendors to disclose information about their products so that the buyers can use
the NIST standards to check their compliance before they are integrated into IoT systems.
The procured products have to comply with the business or mission, organization-speciﬁc
requirements, the operational environment, risk appetite, and risk tolerance [4]. Security
requirements are an important component of compliance. They are imposed by not only the
developers in the private sectors to provide information and quality assurance but also the
law, which aims to protect the nation from cyber-attacks.

Recent legislation has been signed into law requiring IoT devices purchased with govern-
ment money to comply with security standards [18]. The Internet of Things Cybersecurity
Act of 2020 [17] requires NIST to “develop and publish under section 20 of NIST Act
(15 U.S.C. 278g-3) standards and guidelines for the federal government on the appropriate
use and management of Internet of Things devices owned or controlled by an agency and
connected to information systems owned or controlled by an agency, including minimum
information security requirements for managing cybersecurity risks associated with such de-
vice.” All IoT devices connected to IT systems owned or controlled by a federal agency must
conform to NIST standards by September 4, 2021.

The Biden executive order of May 12, 2021 [1] demands that “the federal government
must bring to bear the full scope of its authorities and resources to protect and secure
its computer systems, whether they are cloud-based, on-premises, or hybrid.” The scope
of protection and security must include systems that process data (information technology
(IT)) and those that run the vital machinery that ensures our safety (operational technology
(OT)).” The executive order requires full NIST compliance. The focus of the new rules is on

33

IoT systems that support information technologies, e.g., the power and cooling systems, such
as uninterruptible power supplies (UPSs), power distribution units (PDUs), and computer
room air conditioners and air handlers (CRAC & CRAH) that support networks, servers,
and data centers on the property of federal agencies, building management systems (BMS),
and data center infrastructure management systems (DCIM).

Besides the federal regulations, supply contracts are also useful to secure systems installed
by suppliers. The suppliers need to be informed of your security requirements and standards.
You can check whether the proposed or delivered products or services comply with them.
The contracts also play an important role in accountability. The penalty can be enforced
by contracts once non-compliance of the services is found by the buyer, which has been
discussed in the earlier section.

We can use formal methods to check whether the attributes in m satisfy the requirements
that are coded into logical formulae f . The product is compliant if m |= p, the description
satisﬁes the speciﬁcations; otherwise, it is not. There are well-established tools that can be
used to eﬃciently solve this satisﬁability problem. For example, the compliance problem can
be formulated as a satisﬁability modulo theories (SMT) problem, which can be solved using
a formalized approach and many solvers. PRISM is another tool that enables probabilistic
modeling and checking of systems. Under the assumption that the reporting of m truthfully
describes the product, i.e., m = θ, a compliant buyer or system will not acquire from suppliers
that do not satisfy the requirement. In other words, a = 0 if m (cid:54)|= p.

6.2 Contract Design

There are two economic-level solutions. One is the mechanism design between the buyer and
the supplier to induce m = θ. To achieve this, we would need to create incentives for the
supplier to truthfully reveal θ. This would rely on the design of a certain form of penalty
as a credible threat. One of such penalties is through the contract. The contract between
the supplier and the buyer would include a penalty once the supplier is accountable. The
contract will be eﬀective only when the buyer decides to purchase the product a = 1, which
happens with probability α(m) = P r(a = 1|m). We consider the following utility function
of the supplier, US : Θ × M (cid:55)→ R, given by

US(θ, m) :=Eα

(cid:2)JS(θ, m) − EP m

A

[CS(θ, m)](cid:3) .

(36)

Here, JS : Θ × M (cid:55)→ R is the proﬁt of the supplier if he reports m ∈ M when the true type
is θ ∈ Θ and under the procurement decision. The second term in the utility function is the
average penalty CS : Θ × M (cid:55)→ R for the supplier if he is held accountable. The probability

34

of being accountable is given by P m
that the penalty depends on θ and m.

A in Def. 3.2 based on the received message m. It is clear

We call a supplier is incentive-compatible if

US(θ, θ) ≥ US(θ, m),

for all m ∈ M.

(ICS)

An incentive-compatible supplier does not have incentives to misreport what he knows
when he is held accountable for his actions. Note that to achieve this, we assume that the
purchase rule and accountability testing scheme are revealed to the supplier through the
contract. The (ICS) condition gives a natural constraint when designing a procurement
contract. However, the challenge is that the proﬁt function JS and the type space of the
suppliers are often unknown to the acquirer and they need to be conjectured or learned from
experience or data.

We call a supplier is individually rational if

US(θ, m) ≥ 0,

for all m ∈ M, m (cid:54)= θ

(IRS)

The (IRS) constraint ensures the supplier will beneﬁt from participating in the contract.
This requires the buyer to design the penalty carefully so that the expected proﬁt of the
supplier is non-negative.

Example: Autonomous truck platooning

If we take a closer look at the utility function of the supplier, it can be further expressed as

US(θ, m) = α(m) · [JS(θ, m) − CS(θ, m) · P m

A ] .

(37)

The goal of contract design is to assign an appropriate penalty CS for the supplier if they
need to be held accountable for the accident. The ﬁrst consideration comes from the (IRs)
constraints. This set of constraints suggests that we should not assign a penalty that exceeds
the expected proﬁt.

The (ICS) constraints are automatically satisﬁed when the supplier truthfully report
m = θ. Consider the autonomous truck platooning example as described in Sec. 4 with the
binary sensor type space, i.e., Θ = M = {0, 1}. The contract designer need to meet the

35

following constraints

α(1) (cid:0)J 11
α(0) (cid:0)J 00

S − P 1
S − P 0

AC 11
S
AC 00
S

(cid:1) ≥ α(0) (cid:0)J 10
(cid:1) ≥ α(1) (cid:0)J 01

S − P 0
S − P 1

AC 10
S
AC 01
S

(cid:1)

(cid:1)

(38)

(39)

where we denote the proﬁt of supplier with true type θ who sends message m as J θ,m
the penalty for such supplier as C θ,m
S .

S

, and

S

From the contract designer’s viewpoint, the proﬁt of the supplier J θ,m

is beyond his
control. This value is determined by the production cost and economical nature of the
system. In the ACC system, θ = 1 is the product type corresponding to the system design.
It is natural to assume that the sensor supplier with true type θ = 1 will make more proﬁt
when he truthfully reports, as J 11
S . Similarly, we can assume misinformation will bring
more proﬁt for the supplier with θ = 0, as J 00

S < J 01
S .
In terms of misinformation penalty, it is incentive to penalize more on the supplier who
S , for every m (cid:54)= θ. If we expect same purchasing
A = PA are the same for both messages m ∈ {0, 1},

fails to truthfully report, as C θ,θ
policy α(m) and accountability P m
constraint (38) will be automatically satisﬁed and constraint (39) will be reduced to

S < C θ,m

S > J 10

J 01
S − J 00

S ≤ PA(C 01

S − C 00

S ).

(40)

This indicates for the supplier θ = 0 who has the incentive to misinform the buyer, the
expected extra penalties brings to the supplier through contract need to exceed the extra
proﬁt generated from the untruthful report. The result coincides with the intuition that the
contract needs to be designed with incentive capability.

For automakers looking at production, the prices of lidar sensors need to be cost-eﬀective
for automotive ACC use. Ranging sensors with greater abilities will be sold for higher prices.
It is reported that Lidar suppliers manage to reduce the single-unit samples price to $250 in
large volumes [14]. In the ACC supplier example, consider the following values:

S = J 01
J 11

S = 250; J 00

S = J 10

S = 200; α(1) = 0.8, α(0) = 0.5; P 1

A = 0.3, P 0

A = 0.7.

We arrive at the following constraints for contract penalty design for the supplier:

36

0.8 ∗ (250 − 0.3 ∗ C 11

0.5 ∗ (200 − 0.7 ∗ C 00

0.5 ∗ (200 − 0.7 ∗ C 10

0.8 ∗ (250 − 0.3 ∗ C 01

S ),
S ),

S ) ≥ 0.5 ∗ (200 − 0.7 ∗ C 10
S ) ≥ 0.8 ∗ (250 − 0.3 ∗ C 01
S ) ≥ 0,
S ) ≥ 0,
S < C 10
S .

C 00

S < C 01

S , C 11

(ICS)

(IRS)

By solving the feasible region of penalty under constraints as in Fig. 18, the contract designer
can select the proper penalties for the supplier and help avoid misinformation.

(a) Feasible Region for θ = 0

(b) Feasible Region for θ = 1

Figure 18: Feasible penalties under constraints.

6.3 Cyber Insurance

6.3.1 Background Introduction

In spite of the wide applications of cyber-physical systems, the cyber risks within the IoT
supply chain are considered to be the most challenging problem to handle. Cyber insurance
is the last resort for resilience to mitigate the loss of performance. It is an important risk
management tool that transfers the risks of the buyer to a third party, i.e., an insurer.
Victims of a cyber attack can reduce their ﬁnancial losses and quickly recover to restore
their business operations. According to the cyber insurance report released by the National
Association of Insurance Commissioners (NAIC) [29], the cybersecurity insurance market
in 2020 is roughly $4.1 billion reﬂecting an increase of 29.1% from the prior year. This

37

scheme particularly beneﬁts small and medium-size businesses that cannot aﬀord a major
investment in cyber protection.

Unlike traditional insurance policies, cyber insurance reimburses the buyer for the loss
incurred by data breaches, malware infections, or other cyberattacks in which the insured
entity was at fault. An incentive-compatible cyber insurance policy could help reduce the
number of successful cyber attacks by incentivizing the adoption of preventative measures
in return for more coverage [7, 21]. It can be served as an indicator of the quality of security
protection. Besides, it is believed that cyber insurance can induce greater social welfare and
encourage more comprehensive policies regarding cyber security[23].

Various frameworks have been proposed to study cyber insurance from diﬀerent perspec-
tives. Pal et al., studies the economic impact of cyber insurance by proposing a supply-
demand model. Their work showed that cyber insurance with client contract discrimination
can improve network security [31]. B¨ohme et al. proposed several market models to un-
derstand the information asymmetries between defenders and insurers [3]. Radanliev et al.
built a new impact assessment model of IoT cyber risk to better estimate cyber insurance
[33]. In our framework, we will focus on the cyber insurance policy within the IoT supply
chain and understand the impact of accountability investigation on cyber insurance.

6.3.2

Insurance Policy Design

Typically, the cyber insurance contract consists of the premium price and the coverage rate.
The key challenge in insurance policy design lies in the diﬃculty of risk evaluation due to
the complex structure of the cyber-physical systems. An insurer can make two separate
contracts with the supplier or/and the buyer. The loss of the buyer would be compensated
by the insurer when an accident or a disruption occurs. The loss of the supplier due to
accountability could be insured as well. In this section, we focus on the insurance contract
between an insurer and a buyer.

Figure 19: Information exchange between the insurer, buyer and supplier.

38

The contract is composed of the premium and the coverage of the losses. Let CI ∈ R be
the premium charged by an insurer and the coverage is modeled by the percentage r ∈ (0, 1].
They are decision variables that are determined by the insurer. A buyer has incentives to
participate in the insurance if the average utility under the coverage is higher than the one
without coverage. To quantitatively capture it, we specify the loss or payoﬀ function of the
buyer JB, given by

JB(m, δ) := (1 − r) ˆLB(m, δ(m); θ) + CB(m) + CI.

(41)

Here, the ﬁrst term ˆLB is the average loss of performance, which is the diﬀerence between
the true and the anticipated performances. The cyber insurance will cover the r portion
of the risk. Hence the residual loss is (1 − r) of the losses. The insurance can completely
compensate for the loss of the performance when r = 1. The second term is CB(m) is the
cost of procurement of the product and CI is the premium paid by the buyer.

In this framework, we focus on the potential loss due to the misinformation from the
supplier who cannot be held accountable due to the limitation of accountability investi-
gation. According to the investigation, if the supplier should be held accountable for the
malfunctioning of the system, the loss of performance should be compensated by the sup-
plier. However, if the investigation cannot hold the supplier accountable, the risk will be
transferred to the third party under the insurance contract. The latter case occurs with
probability 1 − P m
A , the probability of unaccountable. Thus, the loss of performance can be
viewed as a stochastic variable lB

lB(m, δ(m); θ) =




UB(m, δ(m)) − UB(θ, δ(m)) w.p. 1 − P m
A ,

0


w.p. P m
A ,

(42)

where UB(θ, δ(m)) is the performance utility measure under the design δ(m) and the true
product quality θ. We assume that the true performance UB(θ, δ(m)) is at best the same as
the anticipated performance when m = θ, i.e. UB(m, δ(m)). When misinformation occurs,
there will be a positive loss of performance; when the supplier truthfully report, the true
performance coincide with anticipated one and the loss is zero; in other words, the expected
loss of performance

ˆLB = (1 − P m

A )∆UB ≥ 0,

(43)

where we denote the diﬀerence in performance measure as ∆UB.

One critical aspect of cyber insurance is the bias from insurance buyers. Humans will

39

hold biased recognition concerning losses and risks, which can lead to diﬀerent decisions
compared to completely rational ones. Agents are often risk-averse, which means they prefer
lower returns with known risks rather than higher returns with unknown risks. In terms of
the expected losses ˆLB, economic literature commonly imposes the following functions for a
risk-averse agent.

• Constant Absolute Risk Aversion (CARA) [3]:

φ(x) =

eβx
β

,

(44)

where the parameter β ≤ 1 is the absolute risk aversion coeﬃcient, measuring the
degree of risk aversion that is implicit in the utility function. The biased expected loss
in this case is

Φ( ˆLB) = (1 − P m

A )φ(∆UB),

• Prospect Theory (PT) [16]:

φ(x) =




xβ

x ≥ 0



−λ(−x)β x < 0

, w(p) =

pζ
pζ + (1 − p)ζ ,

(45)

(46)

where φ(x) and w(p) are biased utility and weighted probability, respectively, and
λ, β, ζ are prospect parameters with loss aversion implying λ > 1.
In general, PT
shows that people are more averse to losses and less sensitive to gains; people inﬂate
the belief for rare events and deﬂate for high-probability ones. The biased expected
loss in this case is

Φ( ˆLB) = w(1 − P m

A )φ(∆UB),

(47)

For these types of buyer, we should replace the average loss ˆLB in equation (41) with
the biased expectation Φ( ˆLB). The risk-averse buyer has an incentive to purchase cyber
insurance if the expected cost under insurance is lower than the one without insurance:

(1 − r)Φ( ˆLB) + CB(m) + CI ≤ Φ( ˆLB) + CB(m).

(IRB)

Note that we assume that the utility of the buyer does not include the penalty payment
from the procurement contract and assume that the procurement does not involve an ac-

40

countability contract. If so, we need to design the procurement contract and the insurance
contract jointly as they are interdependent.

The mechanism design problem of the insurer is to determine the optimal premium rate
CI and the coverage r to maximize his proﬁt. The insurer provides insurance only when the
the proﬁt is non-negative. Thus, we have the following constraint.

JI := CI − r · ˆLB ≥ 0

(IRI)

We assume that the insurer is rational and risk-neutral so that they use the accurate value
of the expected loss of the system when making decisions. The insurer solves the following
optimization problem:

max
r, CI

s.t.

JI = CI − r · ˆLB

(1 − r)Φ( ˆLB) + CI ≤ Φ( ˆLB)
CI − r · ˆLB ≥ 0
r ∈ (0, 1]
CI ∈ R+

(IRB)

(IRI)

(48)

Combining the individual rationality constraints (IRB) and (IRI) with the biased utility

function, we arrive at the following proposition.

Proposition 3 The insurance contract is established between the insurer and the buyer if
the premium CI ∈ R+ and the coverage level r ∈ (0, 1] satisfy

ˆLB ≤

CI
r

≤ Φ( ˆLB)

(49)

This result shows that the ratio between the coverage level r and premium value CI
depends on the average loss of performance of the system and the risk aversion of the
pursuer. Under this constraint, a risk-averse buyer will have the incentive to purchase the
insurance. This provides a fundamental principle for designing the insurance policy.

6.3.3 Maximum Premium with Full Coverage

In this section, we discuss the maximum acceptable premium the risk-averse buyer is willing
to pay. According to Prop. 3, the ratio between the coverage level and the premium CI/r
is bounded by the expected and biased loss of performance of the system. The maximum
premium value can be achieved when the insurer is providing full coverage as r = 1.

41

Proposition 4 The maximum acceptable premium for the buyer is achieved under the fol-
lowing insurance policy:

r∗ = 1,

I = Φ( ˆLB).
C ∗

(50)

Consider the PT risk aversion in (46). Since we are considering the absolute value of
losses, the utility function need to be reﬂected over the origin. The maximum acceptable
premium can be expressed as

I = ˆΦ( ˆLB) = (1 − P m
C ∗

A ) · λ(∆UB)β.

(51)

Figure 20: Maximum acceptable premium under diﬀerent degrees of risk aversion.

Proposition 5 With full coverage r = 1, the maximum acceptable premium is higher than
the unbiased expected loss when the performance diﬀerence is relatively small, as

I ≥ ˆLB
C ∗

if

0 ≤ ∆UB ≤ λ

1
1−β .

We ﬁrst set P m

A = 0.8, apply β = 0.88, ζ = 0.69 in behaviour science literature and
discuss the inﬂuence of loss aversion level λ on the maximum acceptable premium C ∗
I , which
is depicted in Fig. 20. The dotted line served as the baseline of the risk-neutral buyer,
which represents the unbiased expected loss of performance. The larger value of λ indicates
that the buyer is more risk-averse against the losses. The biased loss function is concave in

42

∆UB because when the ∆UB in performance is too high, a small increase in losses has little
inﬂuence on the buyer’s recognition.

Risk-averse buyers are sensitive to small losses, which provides the insurer an opportunity
to take advantage of the risk aversion and charge for a higher premium. From the ﬁgure, the
biased expected loss is greater than the unbiased one when ∆UB is within the tolerable range
for the buyer. This range coincides with the insurance purchase constraint in Prop. 3. If
1−β , we have Φ( ˆLB) > ˆLB and the buyer would not have the incentive to purchase
∆UB > λ
cyber insurance anymore. This indicates that the insurer can increase the premium to
maximum acceptable value if the buyer is going to purchase the insurance.

1

Proposition 6 Cyber insurance is an incentive mechanism that encourages the buyer to
have a more reliable accountability investigation.

Another key result is that cyber insurance could increase the buyer’s incentive to estab-
lish a more valid accountability investigation method. As described in equation (51), the
maximum acceptable premium C ∗
I has a negative correlation with respect to the account-
ability P m
A . Let β = 0.88, λ = 2.25 and ζ = 0.69 as the typical values in prospect theory, the
inﬂuence of accountability investigation on the maximum acceptable premium is depicted in
the following ﬁgure.

Figure 21: Relationship between accountability and maximum acceptable premium.

Figure 21 illustrates that a more reliable accountability investigation (larger P m
A ) can
reduce the maximum premium of the insurance. The amount of reduction is higher if the
performance diﬀers more within two product types. If we consider the payoﬀ function of

43

the buyer under full insurance coverage. If the insurance company charges the maximum
acceptable premium, we have

JB(m, δ) = CB(m) + C ∗
I .

(52)

The decrease in CI will reduce the total payoﬀ JB of the buyer, resulting in a higher proﬁt.
This is the same as saying that cyber insurance provides incentives for the buyer to invest
more in accountability investigation and establish a more reliable examination method to
determine whether the supplier should be accountable for the incident.

6.3.4 Coverage Level with Given Premium

In this section, we discuss the coverage level r when the premium CI is given. As demon-
strated in Prop. 3, given a premium CI, the insurance contract will be established if

CI
Φ( ˆLB)

≤ r ≤

CI
ˆLB

.

(53)

This can be regarded as a constraint in the optimization problems for the buyer and the
insurer.

Given CI, the buyer’s problem is to ﬁnd the optimal coverage level that minimizes the

total payoﬀ under insurance.

min
r∈(0,1]

s.t.

JB = (1 − r)Φ( ˆLB) + CB(m) + CI,

CI
Φ( ˆLB)

≤ r ≤

CI
ˆLB

.

(OPB)

Note that the buyer will make decision under biased expected loss, thus we use Φ( ˆLB) in the
objective function to represent her recognition. On the other hand, the insurer’s problem is
to ﬁnd the optimal coverage level that maximizes his proﬁt.

max
r∈(0,1]

s.t.

JI = CI − r ˆLB,

CI
Φ( ˆLB)

≤ r ≤

CI
ˆLB

.

(OPI)

We assume the insurer is rational and the expected loss in the objective function is unbiased.
By solving these two optimization problems (OPB) and (OPI), the optimal coverage

44

levels for the buyer and the insurer are

r∗
B = max

(cid:111)
,

, 1

(cid:110) CI
ˆLB

r∗
I = min

(cid:110) CI

Φ( ˆLB)

(cid:111)
.

, 0

The buyer prefers a larger coverage level at the upper bound under the constraints, while the
insurer favors a lower coverage level at the lower bound. The result coincides with the fact
that the insurance company and the buyer have a conﬂict of interest in terms of the overall
payoﬀ. However, the individual preferences of both sides need to satisfy the constraint in
(53) in order to establish the insurance contract in the ﬁrst place.

Figure 22: Coverage level under diﬀerent accountability (∆UB = 6, CI = 2).

Proposition 7 Given the insurance premium CI, the acceptable range of coverage level r
will shift in the buyer’s favor with larger accountability P m
A .

Figure 22 illustrates the acceptable coverage level r when the performance diﬀerence
∆UB = 6 and given premium value CI = 2. From the ﬁgure, both bounds of the coverage
A . This is because both ˆLB and Φ( ˆLB)
level will increase with respect to the accountability P m
are decreasing functions in P m
A . The phenomenon shows that a more reliable accountability
investigation (larger P m
A ) will beneﬁt the buyer when participating in cyber insurance. Since
the insurance contract will only be established under the constraint, the acceptable range of
coverage level closer to 1 will cover more portion of the losses in the system, thus reducing
the payoﬀ that the buyer needs to pay after a system malfunction.

45

6.3.5 Trade-oﬀ Between Accountability Investment and Cyber Insurance

Lastly, we discuss the trade-oﬀ between the investment in accountability investigation and
cyber insurance. From the previous discussion, a more reliable accountability investigation
method (larger P m
A ) will reduce the maximum acceptable premium CI and increase the
coverage level r. These will result in a more favorable insurance plan for the buyer that
mitigates the losses of performance due to the supplier. However, usually, the increase
in P m
A comes with a cost. This brings up the question: how much should we invest in
accountability?

Suppose the cost to increase the accountability from P m

A is Cn. This value repre-
sents the extra funding on accountability investigation. The total payoﬀ of the buyer before
(JB) and after (J (cid:48)

B) accountability investment are

A to P m(cid:48)

JB = (1 − r)(1 − P m
B = (1 − r(cid:48))(1 − P m(cid:48)
J (cid:48)

A )∆UB + CB(m) + CI
A )∆UB + CB(m) + C (cid:48)

I + Cn

(54)

where r(cid:48) and C (cid:48)
P m(cid:48)
A > P m

A , r(cid:48) > r and C (cid:48)

I are the modiﬁed insurance plan. From previous discussion, we know that

I < CI. The problem is to ﬁnd the optimal investment such that

J (cid:48)
B − JB ≤ 0.

(55)

The optimal investment will depend on various factors such as the cost Cn, expected loss
ˆLB, the buyer’s risk aversion, etc. We will illustrate the trade-oﬀ between accountability
investment and cyber insurance in the following example.

Example: autonomous truck platooning

Consider the autonomous truck platooning example in Sec. 4.3. The accountability of the
supplier takes the form

P m

A (N ) = 1 − Q

(cid:18) d
2

+

(cid:19)

,

ln(τ )
d

(56)

where d = N 1/2ed/σ. Normally, the sensor diﬀerence ed, supplier’s reputation ratio τ and
observation variance σ2 are already given. The only variable that is completely controlled
by the investigator is the number of test N . From the analysis in the previous section, we
know that dP m
A , the buyer need to increase
the number of tests during the investigation, which is costly in general.

A /dN ≥ 0. In order to reach a higher value of P m

Consider the insurance plan with full coverage r = 1 and maximum premium C ∗

I as

46

described in Prop. 4. We assume the buyer obeys CARA risk aversion for the expected loss.
Suppose the cost to conduct one test is cn. The buyer would like to ﬁnd out the optimal
number of tests N that can minimize her payoﬀ, which is

min
N

JB =(1 − r) ˆLB + CB(m) + C ∗

I + N · cn

=CB(m) + (1 − P m

A (N ))φ(∆UB) + N · cn

(57)

Figure 23: Optimal number of test with diﬀerent test cost.

Figure 23 shows the optimal number of accountability tests with diﬀerent test costs.
When there is no cost to conduct one accountability test (cn = 0), the more test the better
for the buyer. Increasing the number of tests, in general, will increase the accountability
P m
A . As N → ∞, the accountability investigation can identify the untruthful supplier almost
surely with P m
A → 1. In this case, the supplier will be penalized for the misinformation, and
the payoﬀ of the buyer will be close to zero. When the cost of each test cn increases, the
optimal number of test N ∗ will decrease. This illustrates the trade-oﬀ between account-
ability investigation and cyber insurance. Even though increasing the number of tests will
provide a more reliable test and reduce the insurance premium, the total investment would
exceed the beneﬁt after some point, causing unnecessary payoﬀ for the buyer. Finally, if
the investigation is too costly as cn = 100, the buyer will never beneﬁt from conducting
an accountability investigation. It is better for the buyer to change to other comparatively
low-cost investigation methods. By decreasing cn, the buyer could ﬁnd the optimal number
of tests and achieve a lower payoﬀ.

47

7 Conclusion

In this chapter, we have proposed a system-scientiﬁc framework to study the accountabil-
ity in IoT supply chains and provided a holistic risk analysis technologically and socio-
economically. We have developed stylized models and quantitative approaches to evaluate
the accountability of the supplier. Two case studies have been used to demonstrate the
model of accountability in the setting of autonomous truck platooning and ransomware in
IoT supply chain.

We discuss the accountability investigation performance and design with a single sup-
plier in the autonomous truck platooning case. From the parameter analysis, the reliability
of the investigation can be improved with larger sensor error, more number of tests, and less
observation variance. We have also showed the impact of the supplier’s reputation on ac-
countability investigation. A bad reputation will increase both accountability and wronged
accountability during the investigation.

Using the smart lock case study, we have illustrated how to determine the accountability
of the supplier in the IoT supply chain under a ransomware attack. A Neyman-Pearson test
has been used to deal with suppliers with limited prior information. We have presented the
model of the multi-stage accountability investigation with multiple suppliers in the supply
chain and discussed the trade-oﬀ between detailed investigation and product replacement.

Contract design and cyber insurance are used as economic solutions to improve the cyber
resilience in IoT supply chains. By designing contracts under incentive-compatibility and
individual rationality constraints, the IoT end-user can penalize the accountable supplier
and reduce his incentive of providing misinformation in the ﬁrst place. Cyber insurance
mitigates the loss of performance by transferring the risks to a third party. We have showed
that cyber insurance is an incentive-compatible mechanism that facilitate a more reliable
accountability investigation from the buyer side. However, the investigator needs to balance
between the accountability investment and cyber insurance to achieve a higher payoﬀ.

48

References

[1] J. Biden. Executive order on improving the nation’s cybersecurity, May 2021.
[2] J. Blocki, N. Christin, A. Datta, A. D. Procaccia, and A. Sinha. Audit games.
Twenty-Third International Joint Conference on Artiﬁcial Intelligence, 2013.

In

[3] R. B¨ohme, G. Schwartz, et al. Modeling cyber-insurance: Towards a unifying framework.

In WEIS, 2010.

[4] J. Boyens, C. Paulsen, R. Moorthy, and N. Bartol. Supply chain risk management

practices for federal information systems and organizations, April 2015.

[5] D. Braue. Global ransomware damage costs predicted to exceed $265 billion by 2031,

2021. Accessed: July 20, 2021.

[6] E. Cartwright, J. Hernandez Castro, and A. Cartwright. To pay or not: game theoretic

models of ransomware. Journal of Cybersecurity, 5(1):tyz009, 2019.

[7] B. Cashell, W. D. Jackson, M. Jickling, and B. Webel. The economic impact of cyber-
attacks. Congressional research service documents, CRS RL32331 (Washington DC),
2, 2004.

[8] J. Chen and Q. Zhu. Security as a service for cloud-enabled internet of controlled things
under advanced persistent threats: a contract design approach. IEEE Transactions on
Information Forensics and Security, 12(11):2736–2750, 2017.

[9] M. J. Farooq and Q. Zhu. Optimal dynamic contract for spectrum reservation in mission-
critical unb-iot systems. In 2018 16th International Symposium on Modeling and Opti-
mization in Mobile, Ad Hoc, and Wireless Networks (WiOpt), pages 1–6. IEEE, 2018.
[10] D. L. Farris. Target to pay nearly $40 million to settle with banks over data breach;

total costs reach $290 million, Dec 2015.

[11] J. Feigenbaum, A. D. Jaggard, and R. N. Wright. Open vs. closed systems for account-
ability. In Proceedings of the 2014 Symposium and Bootcamp on the Science of Security,
pages 1–11, 2014.

[12] J. Feigenbaum, A. D. Jaggard, R. N. Wright, et al. Accountability in Computing:

Concepts and Mechanisms. NOW PUBLISHERS Incorporated, 2020.

[13] D. Geneiatakis, I. Kounelis, R. Neisse, I. Nai-Fovino, G. Steri, and G. Baldini. Security
and privacy issues for an iot based smart home. In 2017 40th International Conven-
tion on Information and Communication Technology, Electronics and Microelectronics
(MIPRO), pages 1292–1297. IEEE, 2017.

[14] J. Hecht. Lidar for self-driving cars. Optics and Photonics News, 29(1):26–33, 2018.
[15] S. Jajodia, A. K. Ghosh, V. Swarup, C. Wang, and X. S. Wang. Moving target de-
fense: creating asymmetric uncertainty for cyber threats, volume 54. Springer Science
& Business Media, 2011.

[16] D. Kahneman and A. Tversky. Prospect theory: An analysis of decision under risk.
In Handbook of the fundamentals of ﬁnancial decision making: Part I, pages 99–127.
World Scientiﬁc, 2013.

[17] R. L. Kelly. Text - h.r.1668 - 116th congress (2019-2020): Internet of things cybersecurity

improvement act of 2020, Dec 2020.

[18] D. Kovaleski. Bill that requires security standards for government purchases of iot

devices signed into law, Dec 2020.

[19] R. K¨unnemann, I. Esiyok, and M. Backes. Automated veriﬁcation of accountability

49

in security protocols. In 2019 IEEE 32nd Computer Security Foundations Symposium
(CSF), pages 397–39716. IEEE, 2019.

[20] B. C. Levy. Binary and mary hypothesis testing. In Principles of Signal Detection and

Parameter Estimation, pages 1–57. Springer, 2008.

[21] R. P. Majuca, W. Yurcik, and J. P. Kesan. The evolution of cyberinsurance. arXiv

preprint cs/0601020, 2006.

[22] N. Manworren, J. Letwat, and O. Daily. Why you should care about the target data

breach. Business Horizons, 59(3):257–266, 2016.

[23] A. Marotta, F. Martinelli, S. Nanni, A. Orlando, and A. Yautsiukhin. Cyber-insurance

survey. Computer Science Review, 24:35–61, 2017.

[24] R. B. Myerson. Optimal auction design. Mathematics of operations research, 6(1):58–73,

1981.

[25] R. B. Myerson. Perspectives on mechanism design in economic theory. American Eco-

nomic Review, 98(3):586–603, 2008.

[26] J. Neyman and E. S. Pearson.

Ix. on the problem of the most eﬃcient tests of sta-
tistical hypotheses. Philosophical Transactions of the Royal Society of London. Series
A, Containing Papers of a Mathematical or Physical Character, 231(694-706):289–337,
1933.

[27] K. C. Nguyen, T. Alpcan, and T. Basar. Distributed hypothesis testing with a fusion
center: The conditionally dependent case. In 2008 47th IEEE Conference on Decision
and Control, pages 4164–4169. IEEE, 2008.

[28] H. Nissenbaum. Computing and accountability. Communications of the ACM, 37(1):72–

81, 1994.

[29] N. A. of Insurance Commissioners (NAIC). Report on the cybersecurity insurance

market, 2021. Accessed: Oct 20, 2021.

[30] W. H. Organization et al. Global status report on road safety 2018: summary. Technical

report, World Health Organization, 2018.

[31] R. Pal, L. Golubchik, K. Psounis, and P. Hui. Will cyber-insurance improve network
security? a market analysis. In IEEE INFOCOM 2014-IEEE Conference on Computer
Communications, pages 235–243. IEEE, 2014.

[32] J. Pawlick and Q. Zhu. Game Theory for Cyber Deception: From Theory to Applications.

Springer Nature, 2021.

[33] P. Radanliev, D. De Roure, S. Cannady, R. Mantilla Montalvo, R. Nicolescu, and
M. Huth. Analysing iot cyber risk for estimating iot cyber insurance.
In Living in
the Internet of Things: Cybersecurity of the IoT-2018. IET Conference Proceedings,
pages 1–9. London: The Institution of Engineering and Technology, 2018.

[34] S. Rass, S. Schauer, S. K¨onig, and Q. Zhu. Optimal inspection plans. In Cyber-Security

in Critical Infrastructures, pages 179–209. Springer, 2020.

[35] S. Rass and Q. Zhu. Gadapt: a sequential game-theoretic framework for designing
defense-in-depth strategies against advanced persistent threats. In International con-
ference on decision and game theory for security, pages 314–326. Springer, 2016.
[36] J. H. Shapiro. Bounds on the area under the roc curve. JOSA A, 16(1):53–57, 1999.
[37] C. St¨ockle, W. Utschick, S. Herrmann, and T. Dirndorfer. Robust design of an auto-
matic emergency braking system considering sensor measurement errors. In 2018 21st
International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2018.

50

[38] J. N. Tsitsiklis et al. Decentralized detection. Massachusetts Institute of Technology,

Laboratory for Information and . . . , 1989.

[39] M. Wang, W. Daamen, S. P. Hoogendoorn, and B. van Arem. Rolling horizon control
framework for driver assistance systems. part i: Mathematical formulation and non-
cooperative systems. Transportation research part C: emerging technologies, 40:271–289,
2014.

[40] T. D. Wickens. Elementary signal detection theory. Oxford university press, 2001.
[41] R. Zhang and Q. Zhu. FlipIn: A game-theoretic cyber insurance framework for incentive-
compatible cyber risk management of internet of things. IEEE Transactions on Infor-
mation Forensics and Security, 15:2026–2041, 2019.

[42] R. Zhang, Q. Zhu, and Y. Hayel. A bi-level game approach to attack-aware cyber
insurance of computer networks. IEEE Journal on Selected Areas in Communications,
35(3):779–794, 2017.

[43] T. Zhang and Q. Zhu. Optimal two-sided market mechanism design for large-scale data

sharing and trading in massive iot networks. arXiv preprint arXiv:1912.06229, 2019.

[44] T. Zhang and Q. Zhu. On incentive compatibility in dynamic mechanism design with
exit option in a markovian environment. Dynamic Games and Applications, pages 1–45,
2021.

[45] J. Zou, Y. Wang, and K.-J. Lin. A formal service contract model for accountable saas
In 2010 IEEE International Conference on Services Computing,

and cloud services.
pages 73–80. IEEE, 2010.

51

