0
2
0
2

g
u
A
4

]

Y
S
.
s
s
e
e
[

1
v
7
9
4
1
0
.
8
0
0
2
:
v
i
X
r
a

Synthesis of Sensor Deception Attacks at the Supervisory Layer of
Cyber-Physical Systems

Rômulo Meira-Góes a, Eunsuk Kang b, Raymond H. Kwong c, Stéphane Lafortune a

aDepartment of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA

bSchool of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA

cDepartment of Electrical and Computer Engineering, University of Toronto, Toronto, ON M5S 3G4, CA

Abstract

We study the security of Cyber-Physical Systems (CPS) in the context of the supervisory control layer. Speciﬁcally, we propose a general
model of a CPS attacker in the framework of discrete event systems and investigate the problem of synthesizing an attack strategy for a
given feedback control system. Our model captures a class of deception attacks, where the attacker has the ability to hijack a subset of
sensor readings and mislead the supervisor, with the goal of inducing the system into an undesirable state. We utilize a game-like discrete
transition structure, called Insertion-Deletion Attack structure (IDA), to capture the interaction between the supervisor and the environment
(which includes the system and the attacker). We show how to use IDAs to synthesize three different types of successful stealthy attacks,
i.e., attacks that avoid detection from the supervisor and cause damage to the system.

Key words: Discrete Event Systems; Supervisory Control; Cyber-Physical Systems; Cyber-Security; Deception Attacks.

1 Introduction

Cyber-Physical Systems (CPS) are characterized by the in-
teraction of computational entities with physical processes.
These systems are found in a broad spectrum of safety-
critical applications, such as smart grids, process control
systems, autonomous vehicles, medical devices, etc. In such
applications, some undesired behavior could cause damage
to the physical system itself or to people relying on the sys-
tem. Undesired behavior of CPS may be caused by faulty
behavior or by external attacks on the system. In fact, some
attacks on CPS have already been reported, see, e.g., (Kerns
et al. 2014, Checkoway et al. 2011), including the well-
known StuxNet attack (Farwell and Rohozinski 2011).

For this reason, cyber-security of CPS became a subject
of increasing attention in the control community, see, e.g.,
(Weerakkody et al. 2019). The work in (Cardenas et al.
2008, Teixeira et al. 2012) have classiﬁed different types of

⋆

This work was supported in part by the US National Science
Foundation under grants CNS-1421122, CNS-1446298 and CNS-
1738103.

Email addresses: romulo@umich.edu (Rômulo

Meira-Góes), eskang@cmu.edu (Eunsuk Kang),
kwong@control.utoronto.ca (Raymond H. Kwong),
stephane@umich.edu (Stéphane Lafortune).

cyber attacks on control systems. These cyber attacks have
assumed attackers that have some prior knowledge of the
CPS. Our paper focuses on security issues that arise in the
feedback control of CPS for a speciﬁc class of these cyber
attacks called sensor deception attacks. In this class of at-
tacks, the sensor readings are hijacked by an attacker that is
assumed to have some prior knowledge of the control sys-
tem. More speciﬁcally, we are concerned with the problem
of synthesizing a sensor deception attack strategy at the su-
pervisory layer of a given CPS (Amin et al. 2013), an at-
tack strategy that manipulates the sensor measurements re-
ceived by the controller/supervisor1 in order to achieve the
attacker’s goals. Three different types of attacks are investi-
gated, where each attack has different capabilities regarding
manipulating the sensor measurements that are sent to the
supervisor.

Given that we are investigating cyber-attacks at the super-
visory layer of a CPS, we use the formalism of Discrete
Event Systems (DES) to model both the behavior of the
attacker as well as the behavior of CPS itself. In other
words, we assume that a discrete abstraction of the under-
lying CPS has already been performed. This allows us to
leverage the concepts and techniques of the theory of super-

1 Since our focus is the supervisory layer of the control system,
we will use the term supervisor in the remainder of the paper.

Preprint submitted to Automatica

5 August 2020

 
 
 
 
 
 
visory control of DES. Several recent works have adopted
similar approaches to study cyber-security issues in CPS;
see, e.g., (Carvalho et al. 2018, Paoli et al. 2011, Thorsley
and Teneketzis 2006, Wakaiki et al. 2018).

Previous works such as (Carvalho et al. 2018, Paoli et al.
2011, Thorsley and Teneketzis 2006) on intrusion detection
and prevention of cyber-attacks using discrete event models
were focused on modeling the attacker as faulty behavior.
Their corresponding methodologies relied on fault diagnosis
techniques.

Recently, (Su 2018) proposed a framework similar to the
one adopted in our paper, where they formulated a model
of bounded sensor deception attacks. Our approach is more
general than the one in (Su 2018), since we do not impose
a normality condition to create an attack strategy; this con-
dition is imposed to obtain the so-called supremal control-
lable and normal language under the attack model. In addi-
tion to bounded sensor deception attacks, we consider two
other attack models. An additional difference between this
paper and the approach in (Su 2018) is the way the dynam-
ical interaction between the attacker and the supervisor is
captured. This will be revisited in Section 3.

In (Wakaiki et al. 2018), the authors presented a study of
supervisory control of DES under attacks. They introduced a
new notion of observability that captures the presence of an
attacker. However, their study is focused on the supervisor’s
viewpoint and they do not develop a methodology to design
attack strategies. They assume that the attack model is given
and they develop their results based on that assumption. In
that sense, the work (Wakaiki et al. 2018) is closer to robust
supervisory control and it is complementary to our work.

Several prior works considered robust supervisory control
under different notions of robustness (Alves et al. 2014,
Lin 2014, Rohloff 2012, Xu and Kumar 2009, Yin 2017),
but they did not study robustness against attacks. In the
cyber-security literature, some works have been carried out
in the context of discrete event models, especially regard-
ing opacity and privacy or secrecy properties (Cassez et
al. 2012, Lin 2011, Saboori and Hadjicostis 2007, Wu et
al. 2018, Meira-Góes et al. 2018). These works are con-
cerned with studying information release properties of the
system, and they do not address the impact of an intruder
over the physical parts of the system.

Our approach is based on a general model of sensor decep-
tion attacks at the supervisory layer of the control system of
the CPS. In that context, we investigate the problem of syn-
thesizing successful stealthy sensor deception attacks. We
make the following assumptions about the attacker: (i) it
has knowledge of both the system and its supervisor; and
(ii) it has the ability to alter the sensor information that is
received by the supervisor. The goal of the attacker is to
induce the supervisor into allowing the system to reach a
pre-speciﬁed unsafe state, thereby causing damage to the

system. Throughout the paper, we discuss the reasoning and
the impact of these assumptions.

In this paper, we study three speciﬁc types of attacks that are
based on the interaction between the attacker and the con-
trolled system. The methodology developed to synthesize
these attacks is inspired by the work in (Wu et al. 2018, Yin
and Lafortune 2017, Yin and Lafortune 2016). As in these
works, we employ a discrete structure to model the game-
like interaction between the supervisor and the environment
(system and attacker in this paper). We call this structure
an Insertion-Deletion Attack structure (or IDA). By con-
struction, an IDA embeds all desired scenarios where the
attacker modiﬁes some subset of the sensor events without
being noticed by the supervisor. Once constructed according
to the classes of attacks under consideration, an IDA serves
as the basis for solving the synthesis problem. In fact, this
game-theoretical approach provides a structure for each at-
tack class that incorporates all successful stealthy attacks.
Different stealthy attack strategies can be extracted from this
structure. This is a distinguishing feature of our work as
compared to previous works mentioned above. By provid-
ing a general synthesis framework, our goal is to allow CPS
engineers to detect and address potential vulnerabilities in
their control systems.

The remainder of this paper is organized as follow. Sec-
tion 2 introduces necessary background and some notations
used throughout the paper. The attack model as well as the
problem statement are formalized in Section 3. Section 4
describes the IDA structure and its properties. Section 5 in-
troduces the AIDA (All Insertion-Deletion Attack structure)
and provides a construction algorithm for it. Sections 6 and
7 introduce for each attack type their stealthy IDA struc-
ture, together with a simple synthesis algorithm to extract
an attack function. Lastly, Section 8 presents concluding re-
marks. Preliminary and partial versions of some of the re-
sults in this work have appeared in (Meira-Góes et al. 2017).
All proofs are located in the Appendix.

2 Supervisory control system model

We assume that a given CPS has been abstracted as a dis-
crete transition system that is modeled as a ﬁnite-state au-
tomaton. A ﬁnite-state automaton G is deﬁned as a tuple
G = (X, Σ, δ, x0), where: X is a ﬁnite set of states; Σ is a
ﬁnite set of events; δ : X × Σ → X is a partial transition
function; and x0 ∈ X is the initial state. The function δ is ex-
tended in the usual manner to domain X ×Σ∗. The language
generated by G is deﬁned as L(G) = {s ∈ Σ∗|δ(x0, s)!},
where ! means “is deﬁned”.

In addition, ΓG(S) is deﬁned as the set of active events at
the subset of states S ⊆ X of automaton G, given by:

ΓG(S) := {e ∈ Σ|(∃u ∈ S) s.t. δ(u, e)!}

(1)

By a slight abuse of notation, we write ΓG(x) = ΓG({x})
for x ∈ X.

2

Language L(G) is considered as the uncontrolled system
behavior, since it includes all possible executions of G. The
limited actuation capabilities of G are modeled by a partition
in the event set Σ = Σc ∪ Σuc, where Σuc is the set of
uncontrollable events and Σc is the set of controllable events.

It is assumed that G is controlled by a supervisor SP that dy-
namically enables and disables the controllable events such
that it enforces some safety property on G. In the nota-
tion of the theory of supervisory control of DES initiated in
(Ramadge and Wonham 1987), the resulting controlled be-
havior is a new DES denoted by SP /G with the closed-loop
language L(SP /G) deﬁned in the usual manner (Cassandras
and Lafortune 2008). The set of admissible control decisions
is deﬁned as Γ = {γ ⊆ Σ | Σuc ⊆ γ}, where admissibility
guarantees that a control decision never disables uncontrol-
lable events.

In addition, due to the limited sensing capabilities of G, the
event set is also partitioned into Σ = Σo ∪ Σuo, where Σo is
the set of observable events and Σuo is the set of unobserv-
able events. Based on this second partition, the projection
function Po : Σ∗ → Σ∗
o is deﬁned as:

Po(ǫ) = ǫ and Po(se) =

Po(s)e
Po(s)

(cid:26)

if e ∈ Σo
if e ∈ Σuo

(2)

The inverse projection P −1
P −1
o

(t) = {s ∈ Σ∗|P (s) = t}.

o

: Σ∗

o → 2Σ

∗

is deﬁned as

Formally, a partial observation supervisor is a function SP :
Po(L(G)) → Γ. Without loss of generality, we assume that
SP is realized (i.e., encoded) as a deterministic automaton
R = (Q, Σ, µ, q0), such that, ∀q ∈ Q, if e ∈ Σuo is an
enabled unobservable event at state q by SP , then we deﬁne
µ(q, e) = q as is customary in a supervisor realization (cf.
(Cassandras and Lafortune 2008)). In this manner, given a
string s ∈ L(G) the control decision is deﬁned as SP (s) =
ΓR(µ(q0, s)).

Example 1 Consider the system G represented in Fig. 1(a).
Let Σ = Σo = {a, b, c} and Σc = {b, c}. Figure 1(b) shows
the realization R of a supervisor SP that was designed for
G. In this case, the language generated by L(SP /G) guar-
antees that state 2 is unreachable in the controlled behavior.

0

a

c,a

c

2

1

3

b

A

a

c,a

a

B

C

b

(a) G

(b) Supervisor R

Fig. 1. A system automaton along with its supervisor (Example 1)

For convenience, we deﬁne two operators that are used in this
paper together with some useful notation. The unobservable

3

reach of the subset of states S ⊆ X under the subset of
events γ ⊆ Σ is given by:

U Rγ(S) :={x ∈ X|(∃u ∈ S)(∃s ∈ (Σuo ∩ γ)∗) s.t.

x = δ(u, s)}

(3)

The observable reach (or next states) of the subset of states
S ⊆ X given the execution of the observable event e ∈ Σo
is deﬁned as:

N Xe(S) := {x ∈ X|∃u ∈ S s.t. x = δ(u, e)}

(4)

s . . . ei

For any string s ∈ Σ∗, let si denote the preﬁx of s with
s be the ith event of s, so that
the ﬁrst i events, and let ei
si = e1
s; by convention, s0 = ǫ. We deﬁne ¯s as the
set of preﬁxes of string s ∈ Σ∗. Lastly, we denote by N,
N+, and Nn = {0, . . . , n} the sets of natural numbers, pos-
itive natural numbers, and natural numbers bounded by n,
respectively.

3 The sensor deception attack problem

3.1 The general attack model

We start by deﬁning the model for sensor deception attacks,
as illustrated in Fig. 2. The attacker intervenes in the com-
munication channels between the system’s sensors and the
supervisor. We assume that the attacker observes all events
in Σo that are executed by the system. In addition, it has
the ability to edit some of the sensor readings in these com-
munication channels, by inserting ﬁctitious events or delet-
ing the legitimate events. The subset of sensor readings that
can be edited is deﬁned as the compromised event set and
denoted by Σa. For generality purposes, we assume that
Σa ⊆ Σo. To formally introduce the attack function shown

Fig. 2. Model for Sensor Deception Attacks

a = {ei|e ∈ Σa} and Σd

in Fig. 2, we ﬁrst deﬁne two new sets of events. The sets
Σi
a = {ed|e ∈ Σa} are deﬁned as
the sets of inserted and deleted events, respectively. These
sets represent the actions of the attacker, and we use the sub-
scripts to clearly distinguish them from events generated by
G. Note that, Σi
a, and Σa are disjoint. For convenience,
we deﬁne Σe
a ∪ Σd
a as the set of editable events. These
events are used to identify the insertion or deletion of an
event.

a = Σi

a, Σd

We also deﬁne the projections P S
to analyze strings consisting of events in Σe

e , P G

e , and the mask Me
a ∪ Σ. P S
e is a

a; P S

e (ei) = e, ei ∈ Σi

e (ed) = ǫ, ed ∈ Σd

natural projection that treats editable events in the following
manner: P S
a, and
P S
e (e) = e, e ∈ Σ. We use the superscript S because P S
e
describes how the supervisor observes the modiﬁed events.
Given an event, P S
e outputs the legitimate event if the event
was inserted by the attacker, it outputs the empty string if
the event was deleted by the attacker, and it outputs the
legitimate event otherwise. In addition, we deﬁne P G
to
e
describe how the modiﬁed events interact with the system
a; P G
G: P G
a, and
P G
e (e) = e, e ∈ Σ. Finally, Me is a mask that removes
the subscript {i, d} if it exists: Me(ei) = Me(ed) = e, if
ei, ed ∈ Σe

e (ed) = e, ed ∈ Σd

e (ei) = ǫ, ei ∈ Σi

a, and Me(e) = e, e ∈ Σ.

Formally, we model an attacker as a nondeterministic string
edit function. The nondeterminism model provides different
class of attack strategies when compared to the deterministic
model in (Meira-Góes et al. 2017).

a)∗ × (Σo ∪ {ǫ}) → 2(Σo∪Σe
a)

Deﬁnition 2 Given a system G and a subset Σa ⊆ Σo, an
attacker is deﬁned as a (potentially partial) function fA :
(Σo ∪ Σe
s.t. fA satisﬁes the
following constraints:
(1) fA(ǫ, ǫ) ⊆ Σi
a
fA(s, ǫ) = ∅;
(2) ∀s ∈ (Σo ∪ Σe
(3) ∀s ∈ (Σo ∪ Σe

∗;
a)∗, e ∈ Σo\Σa : fA(s, e) ⊆ {e}Σi
a
∗.
a)∗, e ∈ Σa: fA(s, e) ⊆ {e, ed}Σi
a

∗ and ∀s ∈ ((Σo ∪ Σe

a)∗\{ǫ}) :

∗

The function fA captures a general model of sensor decep-
tion attack. Given the past edited string s and observing a
new event e executed by G, the attacker may choose to edit
e based on Σa and replace e by selecting an edited sufﬁx
from the set fA(s, e). The ﬁrst case in the above deﬁnition
gives an initial condition for an attack. The second case con-
strains the attacker from erasing e when e is outside of Σa.
∗
However, the attacker may insert an arbitrary string t ∈ Σi
a
after the occurrence of e. Lastly, the third case in Deﬁni-
tion 2 is for e ∈ Σa; the attacker can edit the event to any
string in the set {e, ed}Σi
a

∗.

∗

As mentioned before, fA only deﬁnes the possible edited
sufﬁxes based on the last executed event and the edit history.
It is interesting to deﬁne a function that deﬁnes the possible
edited strings based on the executed string. Formally, the
string-based edit (potentially partial) function ˆfA : Σ∗
o →
2(Σo∪Σe
a)
a)∗|u ∈
ˆfA(s) ∧ t ∈ fA(u, e)} for any s ∈ Σ∗
o and e ∈ Σo, and
ˆfA(ǫ) = fA(ǫ, ǫ). The function ˆfA(s) returns the set of
possible edited strings for a given string s ∈ Σ∗
o. Note that,
in general, ˆfA is a partial function, and ˆfA(s) may only be
deﬁned for selected s ∈ Σ∗
o.

is deﬁned as ˆfA(se) = {ut ∈ (Σo ∪ Σe

Nevertheless, in some cases the attacker might only have in-
terception capabilities but full not transmission capabilities
since it has not taken control of all the hardware. For exam-
ple, this assumption holds in wireless sensor networks.

3.2 The controlled behavior under sensor deception attack

o → 2Γ as SA(s) = [SP ◦ P S

The presence of the attacker induces a new controlled lan-
guage that needs to be investigated. More speciﬁcally, SP ,
ˆfA, and P S
e together effectively generate a new supervisor
SA for system G, as depicted in Fig. 2. Formally, we de-
e ◦ ˆfA(s)]. Note
ﬁne SA : Σ∗
that ˆfA(s) returns the set of modiﬁed strings and SP as-
signs a control decision to each projected modiﬁed string;
SA returns the set containing all these control decisions. An
equivalent deﬁnition is SA(s) = {γ| ∃sA ∈ ˆfA(s) s.t. γ =
SP (P S
e (sA))}. Deﬁning the supervised language based on
nondeterministic control decisions is cumbersome and com-
plicated (Lin 2014). Nonetheless, we can avoid this difﬁculty
by analyzing the language generated by the events in Σ∪Σa
e .
For this reason, we deﬁne the function Sd
e to be
the deterministic part of SA. The function Sd
A is used when
the attacker has decided which modiﬁed string to send to
the supervisor. Based on fA and Sd
A the language generated
by SA/G is deﬁned recursively as follows:

A = SP ◦ P S

(1) ǫ ∈ L(SA/G)
(2)

(3)

(cid:1)

(cid:0)

∧

∧

uo(Σo ∪ {ǫ})

Po(t1) 6= ǫ ⇒ i|t1| = |t2|

∃t2 ∈ fA(ǫ, ǫ) and
t1 ∈
⇔ t1 ∈

t1 ∈ L(G) ∩ Σ∗
i1 ≤ i2 ≤ . . . ≤ i|t1| ∈ N|t2| s.t. ∀j ∈ N|t1| : ej
(cid:0)
A(tij
Sd
2 )
L(SA/G)
(cid:1)
s ∈ L(SA/G)
t1 ∈ Σ∗
(cid:0)
fA(t3, e|s|
∀j ∈ N|t1| : ej
|t2|

st1 ∈ L(G) where
∧
∃t3 ∈ ˆfA(Po(s|s|−1)), ∃t2 ∈
uo(Σo ∪ {ǫ})
(cid:1)
(cid:0)
s ) and i1 ≤ i2 ≤ . . . ≤ i|t1| ∈ N|t2| s.t.
(cid:1)
(cid:0)
A(t3tij
t1 ∈ Sd
Po(t1) 6= ǫ ⇒ i|t1| =
∧
2 )
⇔ st1 ∈ L(SA/G)

e|s|
s ∈ Σo
∧

∧

(cid:0)

(cid:1)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

The above deﬁnition captures the intricate interaction be-
tween plant, supervisor and attacker. Two important con-
cepts are applied in this deﬁnition. First, the supervisor
issues a control decision whenever it receives an observable
event. An observable event can be either a legitimate event
or a ﬁctitious event inserted by an attacker. Second, the
plant can execute any unobservable event enabled by the
current control decision. To demonstrate how to compute
L(SA/G), we illustrate condition (2) in Figure 3.

Remark: We have assumed that the attacker has the same
observable capabilities as the supervisor. This assumption is
accepted in papers, like ours, where the worst case analysis
is performed. The problem where the attacker and the su-
pervisor has incomparable observation is known to be hard.

(a) Only unobservable events (b) Last event is observable

Fig. 3. Demonstration of Condition (2)

Assume that uo1, uo2 ∈ Σuo, ob ∈ Σo and uo1uo2,
uo1uo2ob ∈ L(G). Figure 3(a) describes how t1 =

4

A(ti3

A(ti2

uo1uo2 ∈ L(SA/G). The string uo1uo2 belongs to
L(SA/G) whenever there exists a string t2 ∈ fA(ǫ, ǫ)
A(ti1
and indices i1 ≤ i2 ∈ N such that uo1 ∈ Sd
2 )
and uo2 ∈ Sd
2 ). Similarly, Fig. 3(b) describes how
uo1uo2ob ∈ L(SA/G). If we use the same indices i1, i2 as
before, then we just need to test if ob ∈ Sd
A(t2).
In the case of the last event being observable, we assume
that the attacker has ﬁnished its entire modiﬁcation t2. On
the other hand, we assume that unobservable events can be
executed based on control decisions of string preﬁxes of
t2. Condition (3) applies the same mechanism of Condition
(2), however it has nuances related to previous modiﬁca-
tions made by the attacker.
Similarly to the language L(SA/G), we recursively deﬁne
the set of reachable states of G under the supervision of Sd
A
driven by the edited string s, for any s ∈ (Σo ∪ Σe
a)∗, as
follows:

2 ) = Sd

REi

A) :=

G(s, Sd
{x ∈ X|∃u ∈ N XP G
∃t ∈ (Sd

e (ei
s)
A(si) ∩ Σuo)∗ : x = δ(u, t)},

(cid:0)

REi−1

G (s, Sd
A)

,

if ei

(cid:1)
s ∈ Σo ∪ Σd
a
,

{x ∈ X|∃u ∈
∃t ∈ (Sd

REi−1

G (s, Sd
A)
A(si) ∩ Σuo)∗ : x = δ(u, t)},
if ei

(cid:0)

(cid:1)

s ∈ Σi
a






for 1 ≤ i ≤ |s| and:

RE0

G(s, Sd

A) :={x ∈ X|∃t ∈ (Sd
x = δ(x0, t)}

A(ǫ) ∩ Σuo)∗ :

(5)

(6)

We extended the deﬁnition under Sd
A for edited strings
since SA is nondeterministic. In the above deﬁnition, we
want to deﬁne the reachable set of states after a partic-
ular edited string was selected. Note that, the deﬁnitions
of REi
G(sA, Sd
A) and of L(SA/G) share many similarities
since they are related.

The above-described dynamical interaction between the at-
tacker and the supervisor is different than the one in (Su
2018), where the supervisor reacts to strings of inserted
events produced by the attacker.

3.3 Attacker objectives

In order to formally pose the problem, we must specify the
objective of the attacker. We assume that G contains a set
of critical unsafe states deﬁned as Xcrit ⊂ X such that
∀x ∈ Xcrit, x is never reached when SP controls G and
no attacker is present. In general, not all states reached by
strings of G that are disabled by SP (when no attacker is
present) are critically unsafe. In practice, there will be certain
states among those that correspond to physical damage to the
system, such as “overﬂow” states or “collision” states, for

5

instance. Similar notions of critical unsafe states have been
used in other works, e.g., (Paoli and Lafortune 2005, Paoli et
al. 2011). Therefore, the objective of the attacker is to force
the controlled behavior under attack L(SA/G) to reach any
state in Xcrit.
The second objective of the attacker is to remain stealthy,
i.e., the attacker should feed the supervisor with normal be-
havior. By normal behavior, we mean that the supervisor
should receive strings in the language L(SP /G). We assume
that if the supervisor receives a string that is not included
in L(SP /G), then an intrusion detection module detects the
attacker.
To capture the behavior that detects the attacker, we de-
ﬁne an automaton that captures both the control decisions
of the supervisor and the normal/abnormal behavior. We
start by deﬁning the automaton H = obs(R||G), where obs
is the standard observer automaton of G (cf. (Cassandras
and Lafortune 2008)) and H = (XH , Σo, δH , x0,H ), where
XH ⊆ 2XR×XG . The automaton H captures only the nor-
mal projected behavior of the controlled system. However,
H cannot be used as an supervisor since it only could con-
tain inadmissible control decisions and it does not have all
decisions of R.
Based on H, we deﬁne ˜R = ( ˜Q, Σ, ˜µ, ¯q0), where ˜Q =
XH ∪ {dead}, and ˜µ is deﬁned to include all the transitions
in δH plus the additional transitions: (∀q ∈ XH ) (∀e ∈
[(Σuc ∩ Σo)\ΓH(q)]) ˜µ(q, e) = dead, (∀q ∈ XH ) (∀e ∈
(Σuc ∩ Σuo)) ˜µ(q, e) = q, (∀q ∈ XH) (∀e ∈ (Σc ∩
Σuo) s.t. ∃x ∈ q, e ∈ ΓR||G(x)) ˜µ(q, e) = q and (∀e ∈
Σuc) ˜µ(dead, e) = dead. In this manner, automaton ˜R em-
beds the same admissible control decisions as automaton
R and it differentiates normal/abnormal behavior. The state
dead in ˜R captures the abnormal behavior of the controlled
system. The attacker remains stealthy as long as ˜R does not
reach the state dead. Figure 4 illustrates the supervisor ˜R
computed for Example 1.

Remark: No new transition to the dead state with controllable
events are included in the deﬁnition of ˜R. For simplicity,
we assume that the supervisor does not enable controllable
events unnecessarily. In this manner, only uncontrollable
events can reach the dead state.

a

Dead

A

a

c,a

B

C

b

Fig. 4. Supervisor ˜R for Example 1

3.4 Problem formulation

Finally, we are able to formally state the problem formu-
lation of the synthesis of stealthy sensor deception attack
problem.

Problem 3 (Synt. of Stealthy Sensor Deception Attacks)
Given an attacker that has full knowledge of the models G

and ˜R, and is capable of compromising events Σa ⊆ Σo,
synthesize an attack function fA such that it generates a
controlled language L(SA/G) that satisﬁes:

1.
2.

∀s ∈ Po(L(SA/G)), ˆfA(s) is deﬁned (Admissibility);
ˆfA(Po(s))
∀s ∈ L(SA/G), P S
⊆ Po(L(SP /G))
e
(Stealthiness);
3(a). ∃s ∈ L(SA/G) s.t.

(Po(s)) ∩ L(SA/G)]

(cid:0)
∀t ∈ [P −1

(cid:1)

o

δ(x0, t) ∈ Xcrit.

(cid:0)

(cid:1)

In this case, we say that fA is a strong attack. We addition-
ally deﬁne the notion of a weak attack as follows:

3(b). ∃s ∈ L(SA/G) s.t. δ(x0, s) ∈ Xcrit.

The Admissibility condition guarantees that fA is well de-
ﬁned for all projected strings in the modiﬁed controlled lan-
guage Po(L(SA/G)). The Stealthiness condition guarantees
that the attacker stays undetected by the supervisor, mean-
ing that any string in Po(L(SA/G)) should be modiﬁed to
a string within the original controlled behavior. In this man-
ner, ˜R never reaches state “dead”. Lastly, the reachability of
critical states is stated in condition 3, where condition 3(a)
is a strong version of the problem. In the strong case, the
attacker is sure that the system has reached a critical state
if string s occurs in the system. Condition 3(b) is a relaxed
version, where the attacker might not be sure if a critical
state was reached, although it could have been reached. Both
variations of condition 3 guarantee the existence of at least
one successful attack, namely, when string s occurs in the
new controlled behavior.

Remark: Problem 3 assumes that the attacker has full knowl-
edge of the plant and the supervisor models. Although it
might be difﬁcult to achieve this assumption in practice, our
paper studies the worst attack scenario case. Moreover, this
assumption is common practice within the cyber-security
domain.

The previous scenario considers a powerful attacker, and it
might be unrealistic in many real applications. In this case,
the second scenario limits the ﬁrst one by considering only
bounded deterministic attack functions. In other words, we
assume that the system does not react up to a bounded num-
ber of modiﬁcations made by the attacker. Bounded deter-
ministic attack functions are deﬁned next.

Deﬁnition 5 Given NA ∈ N+, a bounded deterministic at-
tack function is an attack function fA s.t.
∀s ∈ (Σo ∪
Σe
(∀e ∈ Σo) [(fA(s, e)! ⇒ |fA(s, e)| = 1) ∧ (sA ∈
fA(s, e) ⇒ |sA| ≤ NA)].

a)∗

(cid:0)

(cid:1)

Finally, the last scenario is the least powerful attacker we
consider. We assume that the system can interrupt the at-
tacker’s modiﬁcation at any point. We call this function an
interruptible attack function. Formally, we deﬁne it as:

Deﬁnition 6 An attack function fA is an interruptible attack
function if (∀s ∈ (Σo ∪ Σe
a)∗)(∀e ∈ Σo) [sA ∈ fA(s, e) ⇒
¯sA\{ǫ} ⊆ fA(s, e)].

We use a simple example to provide more intuition about
the above-described scenarios.

Example 7 Let L(G) = {a}{b}∗, where Σa = Σo =
{a, b}. Assume that SP never disables an event. We partially
deﬁne three attack functions: f 1

A, and f 3
A.

A, f 2

f 1
A(ǫ, a) = {a}
A(ǫ, a) = {abn
f 2
f 3
A(ǫ, a) = {a, abi, abibi}

i }, where n ∈ N

(7)
(8)
(9)

f 1
A is a bounded deterministic attack function with NA = 1,
f 2
A is an unbounded deterministic attack function, and lastly,
f 3
A is an interruptible attack function.

3.5 Attack scenarios

4 Insertion-Deletion Attack Structure

Problem 3 is deﬁned for a general function fA, which is
deﬁned as in Deﬁnition 2. However, there exists an interac-
tion between the attacker, the system, and the supervisor that
may limit the power of the attacker. For this reason, we pro-
pose three different types of attack functions. Each of them
has different assumptions that are application-dependent.

The ﬁrst scenario assumes that the attacker has “time" to
perform any unbounded modiﬁcation. In other words, the
system does not execute any event until the attacker ﬁnishes
its modiﬁcation. Such an attack function is deﬁned as an
unbounded deterministic attack function.

Deﬁnition 4 An unbounded deterministic attack function
is an attack function fA s.t.
(∀e ∈
Σo)[fA(s, e)! ⇒ |fA(s, e)| = 1].
(cid:0)

∀s ∈ (Σo ∪ Σe

a)∗

(cid:1)

4.1 Deﬁnition

An Insertion-Deletion Attack structure (IDA) is an exten-
sion of the notion of bipartite transition structure presented
in (Yin and Lafortune 2016). An IDA captures the game be-
tween the environment and the supervisor considering the
possibility that a subset of the sensor network channels may
be compromised by a malicious attacker, whose moves will
be constrained according to various rules. In this game we
ﬁx the supervisor’s decisions as those deﬁned in the given
˜R. The environment’s decisions are those of the attacker and
the system. Therefore, in this game, environment states have
both attacker’s and system’s decisions. In this section, we
deﬁne the generic notion of an IDA. In the next sections,
we will construct speciﬁc instances of it, according to the
permitted moves of the attacker.

6

In order to build the game, we deﬁne an information state
as a pair IS ∈ 2X × ˜Q, and the set of all information states
as I = 2X × ˜Q. The ﬁrst element in an IS represents the
correct state estimate of the system, as seen by the attacker
for the actual system outputs. The second element represents
the supervisor’s state, which is the current state of its real-
ization based on the edited string of events that it receives.
As deﬁned, an IS embeds the necessary information for ei-
ther player to make a decision.

Deﬁnition 8 An Insertion-Deletion Attack structure (IDA)
A w.r.t. G, Σa, and ˜R, is a 7-tuple

A = (QS, QE, hSE, hES, Σ, Σe

a, y0)

(10)

where:

• QS ⊆ I is the set of S-states, where S stands for Su-
pervisor and where each S-state is of the form y =
(IG(y), IS(y)), where IG(y) and IS(y) denote the cor-
rect system state estimate and the supervisor’s state,
respectively;

• QE ⊆ I is the set of E-states, where E stands
for Environment; each E-state is of the form z =
(IG(z), IS(z)) deﬁned in the same way as in the
S-states case;

• hSE : QS × Γ → QE is the partial transition func-
tion from S-states to E-states, deﬁned only for γ =
Γ ˜R(IS(y)):

hSE(y, γ) := (U Rγ(IG(y)), IS(y))

(11)

• hES : QE × (Σo ∪ Σe

a) → QS is the partial transition
function from E-states to S-states, satisfying the fol-
lowing constraints: for any y ∈ QS, z ∈ QE and e ∈
Σo ∪ Σe
a, if hES(z, e) is deﬁned, then hES(z, e) := y
where:

N Xe(IG(z)), ˜µ(IS(z), e)

, if

(cid:0)

e ∈ Γ ˜R

IS(z)

∩ ΓG
(cid:1)

IG(z)

(12a)

IG(z), ˜µ(IS(z), P S

(cid:0)

(cid:1)
e (e))
Me(e) ∈ Γ ˜R
(cid:1)

(cid:0)

(cid:1)
, if e ∈ Σi
a and
IS(z)

(12b)

y =

(cid:0)

(cid:0)

N XP G

e (e)(IG(z)), IS(z)
Me(e) ∈ Γ ˜R
IS(z)
(cid:1)
• Σ is the set of events of G;
(cid:0)
• Σe
• y0 ∈ QS is the initial S-state: y0 := ({x0}, q0).

a is the set of editable events;

IG(z)

(cid:1)

(cid:0)

(cid:1)

(cid:0)
, if e ∈ Σd
∩ ΓG

(cid:1)

a and

(12c)






Since the purpose of an IDA is to capture the game between
the supervisor and the environment, we use a bipartite struc-
ture to represent each entity. An S-state is an IS containing
the state estimate of the system G and the supervisor’s state;
it is where the supervisor issues its control decision. An E-
state is an IS at which the environment (system or attacker)
selects one among the observable events to occur.

7

A transition from a S-state to an E-state represents the up-
dated unobservable reach in G’s state estimate together with
the current supervisor state. Note that hSE is only deﬁned
for y and γ such that γ = Γ ˜R(IS(y)). On the other hand,
a transition from an E-state to a S-state represents the “ob-
servable reach” immediately following the execution of the
observable event by the environment. In this case, both the
system’s state estimate and the supervisor’s state are up-
dated. However, these updates depend on the type of event
generated by the environment: (i) true system event unal-
tered by the attacker; (ii) (ﬁctitious) event insertion by the
attacker; or (iii) deletion by the attacker of an event just ex-
ecuted by the system. Thus, the transition rules are split into
three cases, described below.

The partial transition function hES is characterized by three
cases: Equations (12a),(12b), and (12c). Equation (12a) is
related to system’s actions, while Equations (12b) and (12c)
are related to attacker’s actions. In the case of Equation
(12a), the system generates a feasible (enabled) event and
the attacker lets the event reach the supervisor intact, ei-
ther because it cannot compromise that event, or because it
chooses not to make a move. In the case of Equation (12b),
the attacker only inserts events consistent with the control
decision of the current supervisor state. In the case of Equa-
tion (12c), it only deletes actual observable events generated
by the system. Equation (12c) differs from Equation (12a)
since it adds the condition that the event executed is com-
promised and that the attacker deleted it. Remark: a given
IDA will contain some attack moves (since it is a generic
structure), all of which have to satisfy the constraints in the
deﬁnition of hES.

In the remainder of this paper, we assume that all states
included in an IDA are reachable from its initial state.

Example 9 Let us consider system G and supervisor ˜R
from Example 1. Considering the compromised event set
Σa = {b}, Fig. 5 gives two IDA examples. In the ﬁgure,
oval states represent S-states and rectangular states repre-
sent E-states. Moreover, the red state indicates where the su-
pervisor reaches the “dead" state and the green state (2,A)
represents the successful reaching of a critical state.

3,C

{c,a}

c,a

{a}

a

0,A

b

1,B

{b,a}

3,C

{c,a}

c,a

3,B

a

{b,a}

bi

b

{a}

a

0,A

{b,a}

bd

bi

1,C

{c,a}

c

{a}

(a) An IDA with no attacks

(b) A second IDA example

Fig. 5. IDA Structures

Given two IDAs A1 and A2, we say that A1 is a subsystem
of A2, denoted by A1 ⊑ A2, if QA1
S ⊆ QA2
S ,

E ⊆ QA2

E , QA1

and for any y ∈ QA1
that:

S , z ∈ QA1

E , γ ∈ Γ, and e ∈ Σ, we have

(1) hA1
(2) hA1

SE(y, γ) = z ⇒ hA2
ES(z, e) = y ⇒ hA2

SE(y, γ) = z and
ES(z, e) = y.

Before, we discuss relevant properties of the IDA structure,
we deﬁne a property about E-states.

(P1) An E-state z ∈ QE is called a race-free state, if the
following condition holds:

∀e ∈ Γ ˜R(IS(z)) ∩ Σo : (∃x ∈ IG(z) : δ(x, e)!) ⇒
[hES(z, e)! ∨ hES(z, ed)!]

Property (P1) ensures the non-existence of a race condi-
tion between the attacker and the system in a given E-state.
Speciﬁcally, when (P1) holds in any E-state, the attacker has
the option of either letting the event reach the supervisor
intact or preventing the event from reaching the supervisor
(or allowing both). It means that the attacker can wait for
the system’s response to the most recent control action and
react accordingly. Note that if the event in question is a com-
promised event, then the attacker has the option of allowing
both actions since we deﬁned nondeterministic attack func-
tions. In the case of deterministic attack functions, we force
the attacker to select one of the actions. If (P1) does not hold
at a particular E-state, then the attacker must insert an event,
and this insertion must take place before the system reacts
to the most recent control action sent by the supervisor. (In
some sense, the attacker is “racing” with the system.) An
IDA that satisﬁes (P1) for all E-states is called a race-free
IDA.

Deﬁnition 10 (Induced E-state) Given an IDA A, IE(z, s)
is deﬁned to be the E-state induced by string s ∈ (Σo ∪Σe
a)∗,
when starting in the E-state z. IE(z, s) is computed recur-
sively as:

IE(z, ǫ) := z

IE(z, se) :=

hSE

y, Γ ˜R
undeﬁned
(cid:0)

(

IS(y)

if hES(IE(z, s), e)!

(cid:0)

otherwise

(cid:1)(cid:1)

where y = hES(IE(z, s), e)

We also deﬁne IE(s)
hSE(y0, Γ ˜R(IS(y0))).

:= IE(z0, s), where z0 =

We conclude this section by deﬁning the notion of embed-
ded attack function in an IDA and presenting two lemmas
about reachability in IDAs. The ﬁrst lemma shows that after
observing an edited string sA, the attacker correctly keeps
track of the supervisor state ˜R. Then we show that an IDA
correctly estimates the set of possible states of the system
after the occurrence of edited string sA.

Deﬁnition 11 (Embedded fA) An attack function fA is
said to be embedded in an IDA A if
∀s ∈ Po(L(SA/G))
∀t ∈ ˆfA(s)

, then IE(ti) is deﬁned.

∀i ∈ N|t|−1

(cid:0)

(cid:1)

(cid:1)

(cid:1)(cid:0)

(cid:0)
Intuitively, an attack function is embedded in an IDA if for
every modiﬁed string that is consistent with the behavior of
G, we can ﬁnd a path in the IDA for that string. Note that
we limit this constraint on fA to strings in Po(L(SA/G)),
since these are the ones consistent with G under a given fA.
We are not interested in the deﬁnition of any fA for strings
outside of the controlled behavior.

Lemma 12 Given a system G, supervisor ˜R, and an IDA
structure A with an embedded attack function fA, for any
string s ∈ L(SA/G) and any string sA ∈ ˆfA(Po(s)), we
have

IS

IE(sA)

IG

IE(sA)
(cid:0)

q0, P S

= ˜µ
= RE|sA|
(cid:1)
(cid:0)

G

e (sA)
sA, Sd
(cid:1)
A

(13)

(14)

(cid:1)
5 All Insertion-Deletion Attack Structure

(cid:0)

(cid:0)

(cid:1)

In this section, we deﬁne the All Insertion-Deletion Attack
Structure, a speciﬁc type of IDA abbreviated as AIDA here-
after. Given SP and Σa, the AIDA embeds all insertion-
deletion actions the attacker is able to execute. We discuss
its construction and properties.

5.1 Deﬁnition

As consequence of Lemma 12, if we construct an IDA struc-
ture based on SP and Σa that is “as large as possible", then
it will include all valid insertion and deletion actions for
the attacker. We formally deﬁne such a structure as the All
Insertion-Deletion Attack structure.

Deﬁnition 13 (AIDA(G, SP , Σa)) Given a system G, a su-
pervisor SP , and a set of compromised events Σa, the All
Insertion-Deletion Attack structure (AIDA), denoted by
AIDA(G, SP , Σa) = (QS, QE, hSE, hES, Σ, Σe
a, y0), is de-
ﬁned as the largest IDA w.r.t to G, SP and Σa s.t.

(1) For any y ∈ QS, we have |ΓAIDA(y)| = 0 ⇔

IS(y) = dead

(2) For any z ∈ QE, we have

(a) ∀e ∈ Γ ˜R(IS(z))∩ΓG(IG(z))∩Σo : (hES(z, e)!∨

hES(z, ed)!) or

(b) IG(z) ⊆ Xcrit ⇒ |ΓAIDA(z)| = 0

Condition 2(a) alone satisﬁes the non-existence of a race
condition in the AIDA. Conditions 1 guarantees that the
AIDA stops its search once the attack is detected. Similarly,
Condition 2(b) stops the search given that the attacker knows
it has reached its goal.

By “largest" structure, we mean that for any IDA A satisfy-
ing the above conditions: A ⊑ AIDA(G, SP , Σa). This no-
tion of “largest" IDA is well deﬁned. If A1 and A2 are two

8

IDA structures satisfying the above conditions, then their
union still satisﬁes the conditions, where the union A1∪A2 is
deﬁned as: QA1∪A2
= QA1
E ∪ QA2
S ∪ QA2
S ,
E
and for any y ∈ QA1∪A2
, z ∈ QA1∪A2
, γ ∈ Γ and e ∈
a, we have that hA1∪A2
Σ ∪ Σe
(y, γ) = z ⇔ ∃i ∈ {1, 2} :
hAi
SE(y, γ) = z and hA1∪A2
(z, e) = y ⇔ ∃i ∈ {1, 2} :
hAi
ES(z, e) = y.

E , QA1∪A2

= QA1

ES

SE

E

S

S

5.2 Construction

The construction of the AIDA follows directly from its def-
inition. It enumerates all possibles transitions for each state
by a breadth-ﬁrst search. Each S-state has at most one con-
trol decision, which is related to the supervisor state ˜R. On
the other hand, each E-state enumerates both system’s and
attacker’s actions, according to its system and supervisor es-
timate, respectively. In practice, we do not need to search
the entire state space; we stop the branch search when an S-
state reaches a state with the supervisor at the “dead" state or
when the system estimate of an E-state is a subset of Xcrit.

The procedure mentioned above is described in Algorithm 1
(Construct-AIDA). It has the following parameters: AIDA
is the graph structure of the AIDA we want to construct;
AIDA.E and AIDA.S are the E- and the S-state sets of
the structure, respectively; AIDA.h is its transition func-
tion; Q is a queue. We begin the procedure by initializing
AIDA.S with a single element y0 = ({x0}, q0). The breath-
ﬁrst search is then performed by the procedure DoBFS. The
transitions between S-states and E-states are dealt within
lines 7 to 11. The transitions between E-states and S-states
are deﬁned in lines 12 to 25, where each attack possibil-
ity is analyzed. These transitions are deﬁned exactly as in
Deﬁnition 8, equations (12a), (12b) and (12c). (For the sake
of readability, we employ the usual triple notation (origin,
event, destination) for the transition function.) The procedure
converges when all uncovered states (states in the queue)
are covered, meaning we have traversed the whole reachable
space of E- and S-states. Note that lines 29 and 33 impose
the stop conditions of Deﬁnition 13.

Theorem 14 Algorithm Construct-AIDA correctly con-
structs the AIDA.

Example 15 We return to system G and supervisor ˜R in
Fig. 1(b) with Σa = {b}. The IDA shown in Fig. 5(b) is its
AIDA. Note that there exists an S-state where ˜R reaches state
dead. Moreover, there exists an E-state where G reaches a
critical state, (2, A).

(cid:1)

G, ˜R, ({x0}, q0)

Algorithm 1 Construct-AIDA
Require: G, ˜R and Σa
Ensure: AIDA
1: AIDA ← DoBFS
2: procedure DOBFS(G, R, y)
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

(cid:0)
AIDA.S ← {y}, AIDA.E ← ∅, AIDA.h ← ∅
Queue Q ← {y}
while Q is not empty do
c ← Q.dequeue( )
if c ∈ AIDA.S then
γ ← Γ ˜R(IS(c))
z ←
U Rγ
AIDA.h ← AIDA.h ∪ {(c, γ, z)}
(cid:1)
Add-State-to-AIDA(z, AIDA, Q)

else if c ∈ AIDA.E then
for all e ∈ Σo ∩ Γ ˜R
if e ∈ ΓG
IG(c)
(cid:0)
IG(c)
(cid:1)

IS(c)
then

IS(c), e

, IS(c)

(cid:1)
, ˜µ

IG(c)

y ←

do

15:

(cid:0)

(cid:1)

(cid:0)

N Xe
(cid:0)
(cid:16)

(cid:1)

(cid:1)

(cid:0)

(cid:0)

(cid:0)

IG(c), ˜µ

IG(c)
(cid:1)

if e ∈ Σa then

IG(c)
N Xe
(cid:0)
(cid:0)

∧ e ∈ Σa then
, IS(c)

AIDA.h ← AIDA.h ∪ {(c, e, y)}
(cid:1)(cid:17)
Add-State-to-AIDA(y, AIDA, Q)

if e ∈ ΓG
y ←
AIDA.h ← AIDA.h ∪ {(c, ed, y)}
(cid:1)
Add-State-to-AIDA(y, AIDA, Q)

16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26: procedure ADD-STATE-TO-AIDA(c, AIDA, Q)
if c /∈ AIDA.E ∧ c is an E-state then
27:
AIDA.E ← AIDA.E ∪ {c}
28:
if IG(c) * Xcrit then
29:
Q.enqueue(c)
30:
31:
32:
33:
34:

AIDA.S ← AIDA.S ∪ {c}
if IS(c) 6= dead then
Q.enqueue(c)

y ←
AIDA.h ← AIDA.h ∪ {(c, ei, y)}
Add-State-to-AIDA(y, AIDA, Q)

else if c /∈ AIDA.S ∧ c is a S-state then

IS(c), e

(cid:1)(cid:1)

(cid:0)

(cid:0)

the supervisor. In this section, we show how to synthesize
interruptible and stealthy attack functions that solve Prob-
lem 3. First, we present a pruning process that removes
non-stealthy interruptible strategies from the AIDA. The re-
sulting pruned IDA is then used in the synthesis algorithm.
Recall that three attack types were presented in Section 3
(Deﬁnitions 4, 5, and 6). We focus our attention on the
interruptible case in this section; next, in Section 7, we
present our results for the two remaining cases.

Remark: The AIDA has at most 2|X|+1| ˜Q| states since
Q1, Q2 ⊆ I. If Σuo = ∅, then it has at most 2|X|| ˜Q| states.

6.1 Pruning Process

6 Synthesis of Stealthy IDA: Interruptible Case

The AIDA embeds all attack functions, including non-
stealthy strategies, i.e., those that lead to state dead of

The AIDA could reach state dead of supervisor ˜R. Each
time this occurs, it means that the last step of the attack is no
longer stealthy. Hence, we must prune the AIDA in order to
embed only stealthy attacks. We pose this pruning process
as a meta-supervisory-control problem, where the “plant” is

9

the entire AIDA, the speciﬁcation for that plant is to prevent
reaching state dead of the supervisor, and the controllable
events are the actions of the attacker. We assume that the
reader is familiar with the standard “Basic Supervisory Con-
trol Problem" of (Ramadge and Wonham 1987); we adopt
the presentation of that problem as BSCP in (Cassandras
and Lafortune 2008). First, we show necessary modiﬁca-
tions to the standard BSCP algorithm in order to construct
the Stealthy-AIDA.

Algorithm 2 Modiﬁed BSCP for Interruptible Attacker

(Interruptible Stealthy Deceptive Attack), is deﬁned to be
the automaton obtained after running Algorithm 2 on Atrim
with respect to AG and ΣA
c .

Example 17 We return to the AIDA in Figure 5, where we
would like to obtain the ISDA as in Deﬁnition 16. Figure
6(a) shows the resulting ISDA, where the attacker cannot
play event bd at E-state (1, B). For example, if the attacker
takes such decision, then the uncontrollable event a could
be executed, revealing the attack to the supervisor.

Require: A = (QE ∪ QS, E, fse ⊕ fes, a0 = y0), where
0), where

a ∪ Γ) and Atrim = (At, E, f t, at

E ⊆ (Σo ∪ Σe
At ⊆ QE ∪ QS

1: Step 1 Set H0 = (A0, E, g0, a0) = Atrim, and i = 0
2: Step 2 Calculate
3: Step 2.1 A′
4: Step 2.2 A∗

i = {a ∈ Ai|ΓA(a) ∩ Euc ⊆ ΓHi (a)}
i = {a ∈ A′

i|e ∈ ΓA(a) ⇒ (e ∈ ΓHi (a) ∨

ed ∈ ΓHi(a)}
i = gi|A∗
g′
5: Step 2.3 Hi+1 = T rim(A∗
6: Step 3 If Hi+1 = Hi, Stop; otherwise i ← i + 1, back

i [transition function update]
i , E, g′

i, a0)

to Step 2

The difference between the original BSCP algorithm (see,
e.g., (Cassandras and Lafortune 2008)) and its modiﬁed ver-
sion in Algorithm 2 is the addition of Step 2.2. Since the
BSCP algorithm has quadratic worst-time complexity in the
number of states of the automaton Atrim, it follows that Al-
gorithm 2 also has quadratic worst-time complexity. In order
to ensure the desired interruptibility condition of an attack
function, we need that each state that it visits in the AIDA
be race free. Therefore, at Step 2.2 we enforce the race-free
condition at every E-state of the IDA. In order to enforce
such condition, the algorithm deletes E-states where both
the “let through” transition and the “erasure” transition are
absent for a feasible system event. Therefore, the resulting
IDA from Algorithm 2 is a race-free IDA.

To compute the stealthy AIDA structure, we deﬁne as system
the AIDA constructed according to Algorithm 1. Moreover,
any event e ∈ Σa ∪ Σe
a is treated as controllable while any
control decision γ ∈ Γ and any event e ∈ Σo\Σa is treated
as uncontrollable. The speciﬁcation language, realized by
Atrim, is obtained by deleting the states where the supervisor
reaches the dead state, i.e., by deleting in the AIDA all states
of the form y = (S, dead) for any S ⊆ X.

We formalize the pruning process for obtaining all stealthy
insertion-deletion attacks as follows.

c = Σa ∪ Σe

a as the set of controllable events and ΣA

Deﬁnition 16 Given the AIDA constructed according to Al-
gorithm 1, deﬁne the system automaton AG = AIDA with
ΣA
uc =
(Σo\Σa) ∪ Γ as the set of uncontrollable events. The spec-
iﬁcation automaton is deﬁned by Atrim, which is obtained
by trimming from AG all its states of the form (S, dead), for
any S ⊆ X. The Stealthy AIDA structure, called the ISDA

3,C

{c,a}

c,a

{a}

a

0,A

{a}

b

1,B

{b,a}

bi

c

1,C

{c,a}

3,C

{c,a}

c,a

3,B

{b,a}

bi

b

{a}

a

0,A

{b,a}

bd

bi

1,C

{c,a}

c

{a}

(a) ISDA

(b) USDA

Fig. 6. Stealthy Deceptive Structures

Lemma 18 If an interruptible attack function fA satisﬁes
the admissibility and the stealthy conditions from Problem
3, then it is embedded in the ISDA.

Lemma 19 If an interruptible attack function fA is con-
structed from the ISDA, then the stealthy condition of Prob-
lem 3 is satisﬁed.

Constructing an fA from the ISDA means selecting deci-
sions at E-states and properly deﬁning fA from the selected
decisions. Remark: Lemma 19 does not guarantee the admis-
sibility condition. An inadmissible interruptible fA could be
synthesized from the ISDA. However an admissible inter-
ruptible fA can always be synthesized from the ISDA. Since
the ISDA is race-free, an inadmissible fA synthesized from
the ISDA can always be extended to make it admissible; see
Example 22 below.

Theorem 20 The ISDA embeds all possible interruptible
stealthy insertion-deletion attack strategies with respect to
Σa, ˜R and G.

6.2 Synthesis of Stealthy Interruptible Functions

Based on the ISDA, we can synthesize interruptible attack
functions that satisfy the admissibility and the stealthy con-
ditions. In order to fully satisfy Problem 3, we need to ad-
dress its last condition about the existence of strong or weak
attacks.

Theorem 21 Given the ISDA, there exists an interruptible
fA that strongly satisﬁes Problem 3 if and only if there exists
an E-state z in the ISDA s.t. IG(z) ⊆ Xcrit. On the other
hand, it weakly satisﬁes Problem 3 if and only if there exists
an E-state z in the ISDA s.t. ∃x ∈ IG(z), x ∈ Xcrit.

10

Theorem 21 gives necessary and sufﬁcient conditions for
synthesis of interruptible attack functions. The following
algorithm (Algorithm 3) synthesizes a simple interruptible
attack function that satisﬁes Problem 3. Speciﬁcally, Algo-
rithm 3 encodes an interruptible attack function in an au-
tomaton F . The encoded function simply includes one at-
tempt to reach a given critical state. First, it computes the
shortest path from the initial state to a critical state via
the function Shortest-Path(ISDA, z ∈ QISDA
) such that
IG(z) ⊆ Xcrit (strong attack) or IG(z) ∩ Xcrit 6= ∅ (weak
attack). The second step is to expand the function, based on
the shortest path, in order to satisfy the admissibility condi-
tion.

E

Example 22 Let us extract an interruptible fA from the
ISDA shown in Fig. 6(a). The shortest path starting from
the initial state (0, A) to E-state (2, A) goes through E-
states (0, A), (1, B) and (1, C). Based on this path fA is
deﬁned, for example fA(ǫ, a) = {a, abi}. However, deﬁning
fA solely based on this path makes it inadmissible. The string
ab is deﬁned in L(SA/G) but fA(a, b) is undeﬁned. For this
reason, we have to expand the function fA after deﬁning
it for the shortest path. In this example, we ﬁrst expand
it for fA(a, b) = {b}, and later on for fA(ab, a) = {a}
and fA(ab, c) = {c}. Such extensions sufﬁce to make fA
admissible.

Algorithm 3 Synthesis-fA
Require: ISDA, z ∈ QISDA

E

IG(z) ∩ Xcrit 6= ∅

s.t. IG(z) ⊆ Xcrit or

Ensure: Encoded fA in automaton F
1: path ← Shortest-Path(ISDA, z)
2: F ←Expand-Path(path, ISDA)
3: procedure EXPAND-PATH(path, ISDA)
4:
5:
6:
7:
8:

Queue Q ← ∅; F ← ∅
F.X ← {q0}; F.x0 = q0
F.δ ← ∅; F.Σ = Σ ∪ Σe
a
for all (q, e, f ) ∈ path ∧ q is an E-State do

Add-to-F(q, e, f, Q, A)

9:
10:
11:
12:
13:
14:

while Q is not empty do
q ← Q.dequeue()
for all (q, e, f ) ∈ ISDA.h ∧ (q, e, f ) /∈ A.h do
if e ∈ Σo ∧ ∄f ∗ s.t. (q, ed, f ∗) ∈ A.h then

Add-to-F(q, e, f, Q, A)

else if e ∈ Σd

a ∧ (q, M(e), f ∗) /∈ ISDA.h

then

Add-to-F(q, e, f, Q, A)

15:
16: procedure ADD-TO-F(q, e, f, Q, A)
F.δ ← F.δ ∪ {(q, e, f )}
17:
if f /∈ Q then
18:
19:
20:

Q.enqueue(f )
F.X ← F.X ∪ {f }

Remark: We presented a simple synthesis algorithm. Clearly,
one could choose another strategy to extract an interruptible
function from the ISDA. The important point here is that
the ISDA provides a representation of the desired “solution
space” for the synthesis problem. We are not focused on the

11

synthesis of minimally invasive attack strategies as studied
in (Su 2018), where minimally invasive means an attack
strategy with the least number of edits to reach a critical
state.

7 Other Attack Scenarios

In this section we present modiﬁcations to Algorithm 2 in
order to compute similar structures as the ISDA for the re-
maining two attack functions investigated in this paper. We
omit the respective synthesis algorithms since they are sim-
ilar to Algorithm 3. Although the “expand path” function of
Algorithm 3 changes for each attack function, such changes
follows the assumption of each attack function.

7.1 Deterministic Unbounded Attacks

Different from the interruptible attack, in the deterministic
unbounded (det-unb) attack case, we do not need to consider
that the system may interrupt during an attack insertion. The
attacker can insert events, possibly an arbitrarily long string
in fact, before the system reacts. As consequence, the pruned
IDA for the det-unb attack is not necessarily race free.

Algorithm 2 prunes the AIDA enforcing it to be race free
(Step 2.2); however this condition needs to be relaxed for
the det-unb case, resulting in Algorithm 4. Step 2.1 is also
modiﬁed since we also need to relax the controllability con-
dition. Speciﬁcally, Step 2.1 relaxes the controllability con-
dition because the attacker “races" with the system at states
that violate this condition.2 Algorithm 4 ﬂags states that vi-
olate the controllability condition to later analyze if they
need to be pruned or not. Note that once a state is ﬂagged,
it remains ﬂagged throughout the algorithm. Step 2.2 is di-
vided into three steps. First, deadlocks created by the prun-
ing process are deleted. Second, we ﬂag all states violating
the race-free condition. Then, the transition function is up-
dated based on the ﬂagged states. This update is such that
only insertions transitions are possible from ﬂagged states,
since the attack will not wait for a reaction of the system. We
can adapt Deﬁnition 16 to prune the AIDA for the case of
det-unb attacks by considering the modiﬁcation of Step 2.1
and Step 2.2, as presented in Algorithm 4. We name the re-
sulting stealthy IDA as the USDA, for Unbounded Stealthy
Deceptive Attack structure. Versions of Lemmas 18, 19, and
Theorem 20 are created for this speciﬁc attacker.

Lemma 23 A deterministic unbounded attack function fA
is embedded in the USDA if it satisﬁes conditions (1) and
(2) from Problem 3.

Lemma 24 If a det-unb attack function fA is synthesized
from USDA, then the stealthy condition of Problem 3 is
satisﬁed.

2 It is interesting to mention the similarity of “racing" in our
work with the work on supervisory control with forced events
(Golaszewski and Ramadge 1987).

Algorithm 4 Det-Unb Modiﬁcation

resulting in Algorithm 5. Bounded attacks include features

1: Step 2.1 Flag all a ∈ Ai s.t. ΓA(a) ∩ Euc 6⊆ ΓHi (a)
2: Step 2.2
3: Step 2.2.1 A∗
4: Step 2.2.2 Flag all a ∈ A∗
{e, ed} ∩ ΓHi (a) = ∅
5: Step 2.2.3 For a ∈ A∗
i and e ∈ E
(cid:1)
if e ∈ (Σi

a
e ∈ Σo∧e ∈ ΓA(a)
(cid:0)
(cid:0)

i = {a ∈ Ai| ΓHi
i s.t.

= ∅ ⇒ ΓA(a) = ∅}
⇒

gi(a, e)

a ∪ Γ)∧

(cid:1)

(cid:1)

(cid:0)

g′
i(a, e) =

gi(a, e)!
if e ∈ (Σd

a ∪ Σo) ∧ gi(a, e)!∧

gi(a, e)

a not ﬂagged

undeﬁned otherwise






Theorem 25 The USDA embeds all possible det-unb
stealthy insertion-deletion attack strategies with respect to
Σa, R and G.

Example 26 As we did for the ISDA, we also show the
USDA structure for the AIDA in Figure 5. Figure 6(b) shows
the USDA, where the only deleted state is (0, dead). Note
that decision bd was deleted in the ISDA at state (1, B),
however it is maintained in the USDA. It is only possible
since we know that decision bi can be played before the
execution of event a at state (3, B).

7.2 Deterministic Bounded Attacks

The AIDA structure is general enough for the interrupt-
ible and the unbounded attack scenarios; however, as con-
structed, it does not capture the bound in the case of de-
terministic bounded attacks (or det-bounded case). We now
present a simple mechanism in order to computed a bounded
version of the AIDA, that we term BAIDA. (The || operation
is the standard parallel composition of automata.)

Deﬁnition 27 Given the AIDA constructed by Algorithm 1
and the automaton shown in Figure 7, the BAIDA is deﬁned
as BAIDA = AIDA||Gbound. (For the purpose of ||, the
AIDA is treated as an automaton.)

Fig. 7. Gbound

The det-bounded attack case has similar conditions as the
previously-discussed det-unb case, however, it can only per-
form bounded modiﬁcations. We need to take into account
how many modiﬁcations the attacker has already performed
at a given E-state. Therefore, synthesis algorithms for det-
bounded attacks must use the BAIDA, as in Deﬁnition 27.

The BAIDA, as the AIDA previously, has to be pruned.
We only show the modiﬁcation of Step 2 for Algorithm 2,

12

Algorithm 5 Det-Bounded Modiﬁcation

1: Step 2.1
2: Step 2.1.1 Flag all (a, n) ∈ Ai s.t. ΓA(a) ∩ Euc 6⊆

3: Step 2.1.2 A′

i = {(a, n) ∈ Ai| n = NA ⇒ ΓA(a) ∩

ΓHi((a, n)) ∧ n < NA

Euc ⊆ ΓHi((a, n))}

4: Step 2.2
5: Step 2.2.1 A′′
ΓA(a) = ∅}
6: Step 2.2.2 A∗
ΓA(a) ⇒

i = {(a, n) ∈ A′

i| ΓHi

(a, n)

= ∅ ⇒

i = {(a, n) ∈ A′′

i | n = Na ∧ e ∈ Σa ∧e ∈

(cid:0)

(cid:1)

e ∈ ΓHi ((a, n)) ∨ ed ∈ ΓHi ((a, n))

}

7: Step 2.2.3 Flag all (a, n) ∈ A∗

(cid:0)
Σo ∧ e ∈ ΓA(a)) ⇒

(cid:1)
{e, ed} ∩ ΓHi ((a, n)) = ∅

i s.t. n < Na, (e ∈

8: Step 2.2.4 For (a, n) ∈ A∗

i and e ∈ E

(cid:0)

gi((a, n), e) if e ∈ (Σi

a ∪ Γ)∧

(cid:1)

g′
i((a, n), e) =

gi((a, n), e)!

gi((a, n), e) if e ∈ (Σd

a ∪ Σo)∧

gi((a, n), e)!∧

(a, n) is not ﬂagged

undeﬁned

otherwise






from both unbounded and interruptible attacks. E-states that
have reached the maximum allowed modiﬁcation behave as
E-states in the interruptible case, in other words, they must
satisfy the race-free condition. On the contrary, E-states that
have not reached the maximum allowed editions are similar
to E-states in the unbounded scenario. Consequently, Step
2.1 and Step 2.2 are a combination of the corresponding
steps in the previous cases. A similar structure as the ISDA
and the USDA can be introduced, however we omit such
deﬁnition given that it would follow the same steps as in the
previous cases. The same comment holds for the lemmas and
the theorems introduced in the previous cases. The details
are left for the readers to work out.

8 Conclusion

We have considered the supervisory layer of feedback con-
trol systems, where sensor readings may be compromised
by an attacker in the form of insertions and deletions. In
this context, we have formulated the problem of synthesiz-
ing stealthy sensor deception attacks, that can cause damage
to the system without detection by an existing supervisor.
We deﬁned the attacker as a nondeterministic edit function
that reacts to the plant’s output and its previous editions in a
way that guarantees stealthiness of its attack. We introduced
three different types of attacks, based on the interaction be-
tween the system and the attacker.

Our solution procedure is game-based and relies on the con-
struction of a discrete structure called the AIDA, which is
used to solve the synthesis problem for each attack scenario.

The AIDA captures the game between the environment (i.e.,
system and attacker) and the given supervisor. It embeds all
valid actions of the attacker. Based on the AIDA, we speci-
ﬁed a pruning procedure for each attack type, thereby con-
structing stealthy structures denoted as the ISDA and the
USDA. Based on each type of stealthy structure, we can
synthesize, if it exists, an attack function that leads the sys-
tem to unsafe critical states without detection, for the corre-
sponding attack scenario.

In the future, we plan to investigate how to modify a su-
pervisor that is susceptible to stealthy deception attacks. We
also plan to study the problem of directly designing super-
visors that enforce safety and liveness speciﬁcations and at
the same time are robust to deception attacks.
References

Alves, M. V. S., Joao Carlos Basilio, Antonio Eduardo C. da Cunha,
Lilian Kawakami Carvalho and Marcos Vicente Moreira (2014).
Robust supervisory control against intermittent loss of observations.
In ‘12th IFAC International Workshop on Discrete Event Systems’.
pp. 294 – 299.

Amin, S., X. Litrico, S. Sastry and A. M. Bayen (2013). ‘Cyber security
of water scada systems - Part I: Analysis and experimentation of
stealthy deception attacks’. IEEE Transactions on Control Systems
Technology 21(5), 1963–1970.

Cardenas, A. A., S. Amin and S. Sastry (2008). Secure control: Towards
survivable cyber-physical systems. In ‘2008 The 28th International
Conference on Distributed Computing Systems Workshops’. pp. 495–
500.

Carvalho, L. K., Yi-Chin Wu, Raymond Kwong and Stéphane Lafortune
(2018). ‘Detection and mitigation of classes of attacks in supervisory
control systems’. Automatica 97, 121 – 133.

Cassandras, C. G. and Stéphane Lafortune (2008). Introduction to Discrete
Event Systems. Springer-Verlag New York, Inc.. Secaucus, NJ, USA.

Cassez, F., Jérémy Dubreil and Hervé Marchand (2012). ‘Synthesis of
opaque systems with static and dynamic masks’. Formal Methods in
System Design 40(1), 88–115.

Checkoway, S., Damon McCoy, Brian Kantor, Danny Anderson, Hovav
Shacham, Stefan Savage, Karl Koscher, Alexei Czeskis, Franziska
Roesner and Tadayoshi Kohno (2011). Comprehensive experimental
analyses of automotive attack surfaces. In ‘Proceedings of the 20th
USENIX Conference on Security’. SEC’11. USENIX Association.
Berkeley, CA, USA. pp. 6–6.

Farwell, J. P. and Rafal Rohozinski (2011). ‘Stuxnet and the future of

cyber war’. Survival 53(1), 23–40.

Golaszewski, C. H. and P. J. Ramadge (1987). Control of discrete event
processes with forced events. In ‘26th IEEE Conference on Decision
and Control’. Vol. 26. pp. 247–251.

Kerns, A. J., Daniel P. Shepard, Jahshan A. Bhatti and Todd E. Humphreys
(2014). ‘Unmanned aircraft capture and control via GPS spooﬁng’.
J. Field Robot. 31(4), 617–636.

Lin, F. (2011). ‘Opacity of discrete event systems and its applications’.

Automatica 47(3), 496–503.

Lin, F. (2014). ‘Control of networked discrete event systems: Dealing
with communication delays and losses’. SIAM Journal on Control
and Optimization 52(2), 1276–1298.

Meira-Góes, R., Blake C. Rawlings, Nicholas Recker, Gregory Willett
and Stéphane Lafortune (2018). ‘Demonstration of indoor location
privacy enforcement using obfuscation’. 14th IFAC Workshop on
Discrete Event Systems WODES 2018 51(7), 145 – 151.

13

Meira-Góes, R., E. Kang, R. Kwong and S. Lafortune (2017). Stealthy
deception attacks for cyber-physical systems. In ‘2017 IEEE 56th
Annual Conference on Decision and Control (CDC)’. pp. 4224–4230.

Paoli, A. and Stéphane Lafortune (2005).

‘Safe diagnosability for
fault-tolerant supervision of discrete-event systems’. Automatica
41(8), 1335–1347.

Paoli, A., Matteo Sartini and Stéphane Lafortune (2011). ‘Active fault
tolerant control of discrete event systems using online diagnostics’.
Automatica 47(4), 639–649.

Ramadge, P. J. and W. M. Wonham (1987). ‘Supervisory control of a
class of discrete event processes’. SIAM Journal on Control and
Optimization 25(1), 206–230.

Rohloff, K. (2012). Bounded sensor failure tolerant supervisory control.
In ‘11th IFAC International Workshop on Discrete Event Systems’.
pp. 272 – 277.

Saboori, A. and C. N. Hadjicostis (2007). Notions of security and opacity
in discrete event systems. In ‘46th IEEE Conference on Decision
and Control’. pp. 5056–5061.

Su, R. (2018). ‘Supervisor synthesis to thwart cyber attack with bounded

sensor reading alterations’. Automatica 94, 35–44.

Teixeira, A., Daniel Pérez, Henrik Sandberg and Karl Henrik Johansson
(2012). Attack models and scenarios for networked control systems.
In ‘Proceedings of
International Conference on High
Conﬁdence Networked Systems’. HiCoNS ’12. ACM. New York,
NY, USA. pp. 55–64.

the 1st

Thorsley, D. and D. Teneketzis (2006). Intrusion detection in controlled
discrete event systems. In ‘Proceedings of the 45th IEEE Conference
on Decision and Control’. pp. 6047–6054.

Wakaiki, M., Paulo Tabuada and João P. Hespanha (2018). ‘Supervisory
control of discrete-event systems under attacks’. Dynamic Games
and Applications.

Weerakkody, S., Omur Ozel, Yilin Mo and Bruno Sinopoli (2019).
‘Resilient control in cyber-physical systems: Countering uncertainty,
constraints, and adversarial behavior’. Foundations and Trends in
Systems and Control 7(1-2), 1–252.

Wu, Y.-C., Vasumathi Raman, Blake C. Rawlings, Stéphane Lafortune and
Sanjit A. Seshia (2018). ‘Synthesis of obfuscation policies to ensure
privacy and utility’. Journal of Automated Reasoning 60(1), 107–131.

Xu, S. and R. Kumar (2009). Discrete event control under nondeterministic
In ‘2009 IEEE International Conference on

partial observation.
Automation Science and Engineering’. pp. 127–132.

Yin, X. (2017). ‘Supervisor synthesis for mealy automata with output
functions: A model transformation approach’. IEEE Transactions on
Automatic Control 62(5), 2576–2581.

Yin, X. and S. Lafortune (2016). ‘A uniform approach for synthesizing
property-enforcing supervisors for partially-observed discrete-event
systems’. IEEE Transactions on Automatic Control 61(8), 2140–
2154.

Yin, X. and S. Lafortune (2017). ‘Synthesis of maximally-permissive
supervisors for the range control problem’. IEEE Transactions on
Automatic Control 62(8), 3914–3929.

A Proofs

Proof of Lemma 12

Proof. We prove the result by induction on the length of sA.

Let |sA| = n. Let y0 be deﬁned as usual, and zi =
hSE(yi, Sd

A(si

A))

Proof. By contradiction, assume that we have an interrupt-
ible fA that is admissible and stealthy, but is not embed-
ded in the ISDA. There exists a string s ∈ L(SA/G),
sa ∈ ˆfA(Po(s)) where IE(sa) is not deﬁned in the ISDA.
For simplicity and without loss of generality, assume that
sa = tae where e ∈ Σo ∪ Σe
a, and IE(ta) is deﬁned but
IE(sa) is not deﬁned. There are two reasons why IE(sa)
is not deﬁned.

(1) IE(sa) is not deﬁned in the AIDA. We have that
z = IE(ta) in the AIDA, however IE(z, e) is not de-
ﬁned. Based on the construction of the AIDA, at state
z the transition function is exhaustively constructed
given Γ ˜R(IS(z)). Therefore, if IE(z, e) is not de-
ﬁned, then Me(e) /∈ Γ ˜R(IS(z)). Since ˜R is admissi-
ble, Me(e) ∈ Σc ∩ Σo and e ∈ Σi
a. As consequence of
Lemma 12 P S
e (sa) 6∈ Po(L(SP /G)), which trivially
violates stealthiness. This contradicts our assumption
of stealthy fA. Note that, this case is different than
reaching the dead state. In this case, the attacker inserts
an event that is not allowed by the supervisor.

(2) IE(sa) is deﬁned in the AIDA but it was pruned by Al-
gorithm 2. Algorithm 2 returns the “supremal control-
lable sublanguage” of the AIDA, i.e., it is maximally
permissive, under the race-free and controllability con-
ditions. It removes all sequences that are non-stealthy
or that uncontrollably lead to non-stealthiness. Simi-
larly for the race-free condition. Moreover, one cannot
deﬁne an interruptible attack decision at a state that is
not race free. Finally, the deﬁnition of the set of con-
trollable events guarantees admissibility. Thus, overall,
sa must lead to a non-stealthy, non-interruptible, or in-
admissible strategy, which makes the function fA also
either non-stealthy, non-interruptible, or inadmissible,
a contradiction.

This completes the proof.

Proof of Lemma 19

Proof. The proof follows directly by the construction of the
ISDA.

Proof of Theorem 20

Proof. The proof follows from Lemmas 18 and 19.

Proof of Theorem 21

Proof. The proof follows from Theorem 20.

for i ∈ Nn and yi+1 = hES(zi, ei+1) for i ∈ Nn−1.

Induction Basis: sA = ǫ

IE(ǫ) = hSE(y0, SP (ǫ))

We recall that the supervisor state does not change in the
IDA from S-states to E-states since it did not receive any
observable event, thus yk → zk ⇒ IS(yk) = IS(zk), then

IS(IE(ǫ)) = q0 = ˜µ(q0, ǫ)

Induction hypothesis: Assume
˜µ(q0, P S

that
A)) holds for i ∈ Nk, k < n.

e (si

IS(IE(si

A)) =

Induction step: At k + 1 we have

yk+1 = hES(zk, ek+1)

And we know that

IS(zk) = ˜µ(q0, P S

e (sk

A))

IS(yk+1) = ˜µ(IS(zk), P S

e (ek+1)) = ˜µ(q0, P S

Given that IS(yk+1) = IS(zk+1),
˜µ(q0, P S
e (sk+1
P S

A )). Consequently, IS(IE(sk+1

e (sk+1

A )).

A ))

e (sk+1
then IS(zk+1) =
A )) = ˜µ(q0,

Proof of Theorem 14

Proof. Conditions 1 and 2 of Deﬁnition 13 follow directly
by the construction of the Algorithm 1 (lines 14, 29, 30).
Thus, we only need to prove that the IDA returned by the
algorithm is the largest one. We show it by contradiction.

∗

∗

S ⊆ QA

E ⊆ QA

S or QA

Assume that A is the IDA returned by the algorithm; how-
ever assume that ∃A∗ that satisﬁes conditions 1 and 2, where
A ⊏ A∗. It means that QA
E . If A
and A∗ have the same states, then either A = A∗ or A∗
has more transitions than A. The ﬁrst case is a contradic-
tion of our arguments. The second case implies that some
transitions were not included in A, which is also a contra-
diction. Algorithm 1 applies an exhaustive BFS therefore it
cannot leave it out any transition if all states where covered.
Thus, it is the case that A∗ has more states than A. Let us
start with A∗ having one additional E-state namely z, then
QA
S such that y → z, which
means that IS(y) 6= dead. Therefore, Algorithm 1 will not
converge to A, contradicting our assumption. The same rea-
soning can be used for the case of one more S-state or when
both sets are larger.

S . Therefore, ∃y ∈ QA

S = QA

∗

Proof of Lemma 18

14

