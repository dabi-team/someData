Detecting Cybersecurity Events from Noisy Short Text

Semih Yagcioglu, Mehmet Saygin Seyﬁoglu∗, Begum Citamak, Batuhan Bardak
Seren Guldamlasioglu, Azmi Yuksel, Emin Islam Tatli
STM A.S¸ ., Ankara, Turkey
{syagcioglu, msaygin.seyfioglu, begum.citamak, batuhan.bardak,
sguldamlasioglu, azyuksel, emin.tatli} @stm.com.tr

Abstract

It is very critical to analyze messages shared
over social networks for cyber threat intel-
In this
ligence and cyber-crime prevention.
study, we propose a method that leverages
both domain-speciﬁc word embeddings and
task-speciﬁc features to detect cyber secu-
rity events from tweets. Our model employs
a convolutional neural network (CNN) and
a long short-term memory (LSTM) recurrent
neural network which takes word level meta-
embeddings as inputs and incorporates contex-
tual embeddings to classify noisy short text.
We collected a new dataset of cyber security
related tweets from Twitter and manually an-
notated a subset of 2K of them. We exper-
imented with this dataset and concluded that
the proposed model outperforms both tradi-
tional and neural baselines. The results sug-
gest that our method works well for detecting
cyber security events from noisy short text.

1

Introduction

Twitter has become a medium where people can
share and receive timely messages on about any-
thing. People share facts, opinions, broadcast
news and communicate with each other through
these messages. Due to the low barrier to tweeting,
and growth in mobile device usage, tweets might
provide valuable information as people often share
instantaneous updates such as the breaking news
before even being broadcasted in the newswire c.f .
Petrovi´c et al. (2010). People also share cyber se-
curity events in their tweets such as zero day ex-
ploits, ransomwares, data leaks, security breaches,
vulnerabilities etc. Automatically detecting such
events might have various practical applications
such as taking the necessary precautions promptly
as well as creating self-awareness as illustrated in
Fig. 1. Recently, working with the cyber security

∗Corresponding author.

Dear @AppleSupport, we noticed a *HUGE* se-
curity issue at MacOS High Sierra. Anyone can
login as “root” with empty password after click-
ing on login button several times. Are you aware
of it @Apple?

Figure 1: A cyber security event. A recently discovered se-
curity issue has been reported on Twitter which caught public
attention. A security ﬁx has been published right afterward.

related text has garnered a lot of interest in both
computer security and natural language process-
ing (NLP) communities (c.f . Joshi et al. (2013);
Ritter et al. (2015); Roy et al. (2017)). Neverthe-
less, detecting cyber security events from tweets
pose a great challenge, as tweets are noisy and of-
ten lack sufﬁcient context to discriminate cyber se-
curity events due to length limits. Recently, deep
learning methods have shown to be outperform-
ing traditional approaches in several NLP tasks
(Chen and Manning, 2014; Bahdanau et al., 2014;
Kim, 2014; Hermann et al., 2015).
Inspired by
this progress, our goal is to detect cyber secu-
rity events in tweets by learning domain-speciﬁc
word embeddings and task-speciﬁc features using
neural architectures. The key contribution of this
work is two folds. First, we propose an end-to-
end learning system to effectively detect cyber se-
curity events from tweets. Second, we propose a
noisy short text dataset with annotated cyber secu-
rity events for unsupervised and supervised learn-
ing tasks. To our best knowledge, this will be
the ﬁrst study that incorporates domain-speciﬁc
meta-embeddings and contextual embeddings for
detecting cyber security events.

2 Method

In the subsequent sections, we address the chal-
lenges to solve our task. The proposed system
overview is illustrated in Fig. 2.

9
1
0
2

n
u
J

2

]
L
C
.
s
c
[

2
v
4
5
0
5
0
.
4
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
Figure 2: System Overview. A tweet is ﬁrst pre-processed, then task-speciﬁc features and word level meta-embeddings are
extracted to represent tokens. Finally, a Bi-LSTM, CNN, and Contextual Encoder are fused to classify the encoded tweet.

2.1 Meta-Embeddings

Word embedding methods might capture differ-
ent semantic and syntactic features about the same
word. To exploit this variety without losing the
semantics, we learn meta-embeddings for words.
Word Embeddings. Word2vec (Mikolov et al.,
2013), GloVe (Pennington et al., 2014), and fast-
Text (Joulin et al., 2016; Bojanowski et al., 2016)
are trained for learning domain speciﬁc word em-
beddings on the unlabeled tweet corpus.
Meta-Encoder.
Inspired by Yin and Sch¨utze
(2015) we learn meta-embeddings for words with
the aforementioned word embeddings. We use a
Convolutional Autoencoder (Masci et al., 2011)
for encoding 3xD size embeddings to a 1xD di-
mensional latent variable and to reconstruct the
original embeddings from this latent variable.
Both encoder and decoder are comprised of 2 con-
volutional layers where 32 neurons are used on
each. The encoder part is shown in Fig. 3. We ar-
gue that this network learns a much simpler map-
ping while capturing the semantic and syntactic re-
lations from each of these embeddings, thus lead-
ing to a richer word-level representation. Another
advantage of learning meta-embeddings for words

Meta-Embedding Vector

Convolutional Features

3xD Word Embeddings

Figure 3: Convolutional encoder as a feature extractor. The
decoder is symmetric to the encoder, and in inference time
we drop the decoder and use only the encoder network.

is that the proposed architecture alleviates the Out-
of-Vocabulary (OOV) embeddings problem, as we
still get embeddings from the fastText channel, in
contrast to GloVe and word2vec, where no embed-
dings are available for OOV words.

2.2 Contextual Embeddings

To capture the contextual information, we learn
task-speciﬁc features from tweets.
LDA. Latent Dirichlet Allocation (LDA) is a gen-
erative probabilistic model to discover topics from
a collection of documents (Blei et al., 2003). LDA
works in an unsupervised manner and learns a ﬁ-
nite set of categories from a collection, thus rep-
resents documents as mixtures of topics. We train
an LDA model to summarize each tweet by using
the topic with the maximum likelihood e.g. with
the topic “vulnerability” for the tweet in Fig 1.
NER. Named Entity Recognition (NER) tags the
speciﬁed named entities from raw text into pre-
deﬁned categories. Named entities could be more
general categories such as people, organizations,
or speciﬁc entities can be learned by creating a
dataset containing speciﬁc entity tags. We em-
ploy an automatically annotated dataset that con-
tains entities from cyber security domain (Bridges
et al., 2013) to train our Conditional Random Field
model using handcrafted features, i.e., uni-gram,
bi-gram, and gazetteers. The dataset comprises of
850K tokens that contain named entities such as
‘Relevant Term’, ‘Operating System’,‘Hardware’,
‘Software’, ‘Vendor’, in the standard IOB-tagging
format. Our NER model tags “password” as ‘Rel-
evant Term’ and “Apple” as ‘Vendor’ for the tweet
in Fig 1.
IE. Uncovering entities and the relations between
those entities is an important task for detecting
In order to address this
cyber security events.

PreprocessingNormalizationTokenizationword2vecGloVefastText       huge        flaw          …          … meltdown  meltdown...LSTMhugeflawCNNMeta-EncoderNERIESee Eq. 1Contextual-EncoderLDAFusion LayerHuge Flaw Found in Intel Processors; Patch Could Hit 5-30% CPU Performance tcrn.ch/2CObCiO by @romaindillet WORSE than expected. ALERT WHOMEVERRRR!!! #meltdownTask-Specific Featureswe use Information Extraction (IE), in particu-
lar OpenIE annotator(Angeli et al., 2015) from
the Stanford CoreNLP (Manning et al., 2014).
Subsequently, we extract relations between noun
phrases with the following dependency triplet
(cid:104)arg1, rel, arg2(cid:105), where arg1, arg2 denote the ar-
guments and rel represents an implicit semantic
relation between those arguments. Hence, the fol-
lowing triplet is extracted from the tweet in Fig. 1,
(cid:104)we, noticed, huge security issue(cid:105).
Contextual-Encoder. We use the outputs of LDA,
NER and IE algorithms to obtain a combined
vector representation using meta-embeddings de-
scribed in Sec. 2.1. Thus, contextual embeddings
are calculated as follows1.

f (ϕ(τ )) +

N
(cid:80)
i=1

f (ς(τ )i) +

M
(cid:80)
j=1

f (δ(τ )j)

N + M + 1

(1)

γ(τ ) =

where γ function extracts contextual embeddings
and τ denotes a tweet, f , ϕ, ς and δ represent
meta-embedding, LDA, NER, and IE functions,
respectively. Lastly, N and M denote the output
tokens.

2.3 Event Detection

Inspired by the visual question answering task
(Antol et al., 2015), where different modalities are
combined by CNNs and RNNs, we adopt a similar
network architecture for our task. Prior to train-
ing, and inference we preprocess, normalize and
tokenize each tweet as described in Sec. 3.
CNN. We employ a CNN model similar to that of
(Kim, 2014) where we feed the network with static
meta-embeddings. Our network is comprised of
one convolutional layer with varying ﬁlter sizes,
that is 2, 3, 5. All tweets are zero padded to the
maximum tweet length. We use ReLU as activa-
tion and global max pooling at the end of CNN.
RNN. We use a bi-directional LSTM (Hochreiter
and Schmidhuber, 1997) and read the input in both
directions and concatenate forward and backward
hidden states to encode the input as a sequence.
Our LSTM model is comprised of a single layer
and employs 100 neurons.

3 Experiments

Data Collection. We collected 2.5M tweets us-
ing the Twitter’s streaming API over a period from
2015-01-01 to 2017-12-31 using an initial

set of keywords, henceforth referred as seed key-
words to retrieve cyber security related tweets.
In particular, we use the main group names of
cyber security taxonomy described in Le Sceller
et al. (2017) as seed keywords e.g. ‘denial of ser-
vice’, ‘botnet’, ‘malware’, ‘vulnerability’, ‘phish-
ing’, ‘data breach’ to retrieve relevant tweets. Us-
ing seed keywords is a practical way to ﬁlter out
noise considering sparsity of cyber security related
tweets in the whole tweet stream. After the initial
retrieval, we use langid.py (Lui and Baldwin,
2012) to ﬁlter out non-English tweets.
Data Preprocessing. We substitute user han-
dles with $mention$, and hyperlinks with $url$.
We remove emoticons and reserved keyword RT
which denotes retweets. We substitute hashtags by
removing the preﬁx # character. We limit charac-
ters that repeat more than two times, remove cap-
italization and tokenize tweets using the Twitter
tokenizer in nltk library. We normalize non-
standard forms, i.e. writing cu tmrrw instead of
see you tomorrow. Although there are several rea-
sons for that, the most prominent one is that people
tend to mimic prosodic effects in speech (Eisen-
stein, 2013). To overcome this, we use lexical nor-
malization, where we substitute OOV tokens with
in-Vocabulary (IV) standard forms, i.e. a standard
form available in a dictionary. In particular we use
UniMelb (Han et al., 2012), UTDallas (Liu et al.,
2011) datasets. Lastly, we remove identical tweets
and check the validity by removing tweets with
less than 3 non-special tokens.
Data Annotation. We instructed cyber security
domain experts for manual labelling of the dataset.
Annotators are asked to provide a binary label for
whether there is a cyber security event in the given
tweet or not. Annotators are told to skip tweets
if they are unsure about their decisions. Finally,
we validated annotations by only accepting an-
notations if at least 3 among 4 annotators agreed
on. Therefore, we presume the quality of attained
ground truth labels is dependable. Overall, 2K
tweets are annotated.
Dataset Statistics. After preprocessing, our ini-
tial 2.5M tweet dataset is reduced to 1.7M tweets
where 2K of them are labeled2. The labeled
dataset is somewhat balanced as there are 843
event-related tweets and 1157 non-event tweets.
The training and testing sets have 1600 and 400
samples, respectively.

1We used zero vectors for the non-existent relations.

2Available at https://stm-ai.github.io/

Training. We used Keras with Tensorﬂow back-
For fastText and
end in our neural models.
word2vec embeddings we used Gensim, and for
GloVe we used glove-python library. For
training the word embeddings, we use the en-
tire tweet text corpus and obtain 100 dimensional
word embeddings. We set word2vec and fastText
model’s alpha parameter to 0.025 and window size
to 5. For GloVe embedding model, we set the
learning rate to 0.01, alpha to 0.75 and maximum
count parameter to 100. For embedding mod-
els, we determined the minimum count parameter
to 5, culminating in the elimination of infrequent
words. Consequently, we have 3, 100-dimensional
word embedding tensor in which ﬁrst, second and
third channels consist of word2vec, fastText and
GloVe embeddings respectively. We then, en-
code these 3x100 dimensional embeddings into
1x128 dimensional representations by using our
Meta-Encoder. We train our two channel architec-
ture that combines both LSTM and CNN with 2
inputs: meta-embeddings and contextual embed-
dings. We use meta-embeddings for feature learn-
ing via LSTM and CNN, and their feature maps
are concatenated with contextual embeddings in
the Fusion Layer. In the end, fully connected lay-
ers and a softmax classiﬁer are added, and the
whole network is trained to minimize binary cross
entropy loss with a learning rate of 0.01 by using
the Adam optimizer (Kingma and Ba, 2014).3
Baselines. To compare with our results, we im-
plemented the following baselines: SVM with
BoW: We trained an SVM classiﬁer using Bag-
of-words (BoW) which provides a simpliﬁed rep-
resentation of textual data by calculating the oc-
currence of words in a document. SVM with
meta-embeddings: We trained an SVM clas-
siﬁer with the aforementioned meta-embeddings.
CNN-Static: We used Kim (2014)’s approach
using word2vec embeddings.
Results. Table 1 summarizes the overall perfor-
mance of each method. To compare the models,
we used four different metrics: accuracy, recall,
precision and F1-score. Each reported result is the
mean of a 5-fold cross validation experiment. It is
clear that our method outperforms various simple
and neural baselines. Also, in Table 2, we pro-
vide results of our proposed model along with the
ground-truth annotations. We also provide results
with the different combinations of contextual fea-

Models
SVM+BoW
SVM+Meta-Emcoder
CNN-static (Yoon Kim, 2014)
Human
CNN+Meta-Encoder
LSTM+Meta-Encoder
Ours (see Fig. 2)

Accuracy Precision Recall F1
0.75
0.71
0.76
0.65
0.78
0.78
0.82
Table 1: Results

0.70
0.61
0.69
0.87
0.63
0.70
0.72

0.71
0.64
0.72
0.70
0.78
0.74
0.79

0.70
0.63
0.70
0.59
0.70
0.72
0.76

tures, i.e., LDA, NER, IE4.
Human Study. 8 different subjects are thoroughly
instructed about what is considered as a cyber se-
curity event and individually asked to label 50 ran-
domly selected tweets from the test set. The re-
sults are provided in Table 3.
Error Analysis. In order to understand how our
system performs, we randomly select a set of erro-
neously classiﬁed instances from the test dataset.
Type I Errors. Our model identiﬁes this tweet
as an event “uk warned following breach in air
pollution regulation $url$” whereas it is clearly
about the a breach of a regulation. We hypothe-
size that this is due to the lack of sufﬁcient train-
ing data. Following tweet is also identiﬁed as
an event “wannacry ransomware ransomwareat-
tack ransomwarewannacry malware $url$”. We
suspect that the weights of multiple relevant terms
deceive the model.
Type II Errors. Our model fails to identify the fol-
lowing positive sample as an event. For “playsta-
tion network was the target of miraibotnet ddos at-
tack guiding tech rss news feed search” our model
fails to recognize the ’miraibotnet’ from the tweet.
We suspect this is due to the lack of hashtag de-
composition; otherwise, the model could recog-
nize ‘mirai’ and ‘botnet’ as separate words.
Discussions. Cyber security related tweets are
complicated and analysing them requires in-depth
domain knowledge. Although human subjects
are properly instructed, the results of the human
study indicate that our task is challenging and
humans can hardly discriminate cyber security
events amongst cyber security related tweets. To
further investigate this, we plan to increase the
number of human subjects. One limitation of this
study is that we do not consider hyperlinks and
user handles which might provide additional in-
formation. One particular problem we have not
addressed in this work is hashtag decomposition.
Error analysis indicates that our model might get
confused by challenging examples due to ambigu-
ities and lack of context.

3See supplementary for hyperparameter choices.

4See supplementary for feature combination details.

Tweet
that thing where you run corporation phishing test and user does’nt
click it but clicks the next message which is real phishing email sigh
march 03 the fbi investigating alleged data breach at the center
for election systems at kennesaw state university
cia malware codenames are freaking amazing
proprietary software on malware vista10 is more malicious
in huge breach of trust deidentiﬁed medical history data from
millions of australians can be matched to individuals url2
hackers take aim at your mention account with this new phishing
attack cdwsocia
wannacry ransomware ransomwareattack ransomwarewannacry
malware url

Table 2: Some Example Results

Our Model GT

0

1

1
0

0

0

1

0

1

0
1

1

1

0

Subjects Accuracy Precision Recall F1
0.7
#1
0.62
0.65
#2
0.54
0.71
#3
0.66
0.73
#4
0.66
0.77
#5
0.8
0.72
#6
0.66
0.71
#7
0.7
0.58
#8
0.6
Average
0.65
0.59
Table 3: Human Study Results

1
0.95
0.91
1
0.73
0.95
0.82
0.60
0.87

0.54
0.5
0.58
0.57
0.8
0.57
0.63
0.56
0.70

Cohen’s κ
0.43
0.33
0.42
0.46
0.28
0.41
0.31
0.28
0.36

4 Related Work

Event detection on Twitter is studied extensively
in the literature (Petrovi´c et al., 2010; Sakaki et al.,
2010; Weng and Lee, 2011; Ritter et al., 2012;
Yuan et al., 2013; Atefeh and Khreich, 2015).
Banko et al. (2007) proposed a method to extract
relational tuples from web corpus without requir-
ing hand labeled data. Ritter et al. (2012) pro-
posed a method for categorizing events in Twit-
ter. Luo et al. (2015) suggested an approach to
infer binary relations produced by open IE sys-
tems. Recently, Ritter et al. (2015) introduced the
ﬁrst study to extract event mentions from a raw
Twitter stream for event categories DDoS attacks,
data breaches, and account hijacking. Chang et al.
(2016) proposed an LSTM based approach which
learns tweet level features automatically to extract
events from tweet mentions. Lately, Le Sceller
et al. (2017) proposed a model to detect cyber se-
curity events in Twitter which uses a taxonomy
and a set of seed keywords to retrieve relevant
tweets. Tonon et al. (2017) proposed a method to
detect events from Twitter by using semantic anal-
ysis. Roy et al. (2017) proposed a method to learn
domain-speciﬁc word embeddings for sparse cy-
ber security text. Prior art in this direction (Ritter

et al., 2015; Chang et al., 2016) focuses on extract-
ing events and in particular predicting the events’
posterior given the presence of particular words.
Le Sceller et al. (2017); Tonon et al. (2017) focus
on detecting cyber security events from Twitter.
Our work distinguishes from prior studies as we
formulate cyber security event detection problem
as a classiﬁcation task and learn meta-embeddings
from domain-speciﬁc word embeddings while in-
corporating task-speciﬁc features and employing
neural architectures.

5 Conclusion

We introduced a novel neural model that utilizes
meta-embeddings learned from domain-speciﬁc
word embeddings and task-speciﬁc features to
capture contextual information. We present a
unique dataset of cyber security related noisy short
text collected from Twitter. The experimental re-
sults indicate that the proposed model outperforms
the traditional and neural baselines. Possible fu-
ture research direction might be detecting cyber
security related events in different languages.

Acknowledgments

We would like to thank Merve Nur Yılmaz and
Benan Bardak for their invaluable help with the
annotation process on this project. This research
is fully supported by STM A.S¸ . Any opinions,
ﬁndings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reﬂect the view of the sponsor.

References

Gabor Angeli, Melvin Johnson Premkumar, and
Christopher D Manning. 2015. Leveraging linguis-
tic structure for open domain information extraction.
In Proceedings of the 53rd Annual Meeting of the
ACL 2015.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. Vqa: Visual question an-
swering. In Proceedings of the IEEE ICCV, pages
2425–2433.

Farzindar Atefeh and Wael Khreich. 2015. A survey of
techniques for event detection in twitter. Computa-
tional Intelligence, 31(1):132–164.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv:1409.0473.
learning to align and translate.
Version 7.

Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
In IJ-
Open information extraction from the web.
CAI, volume 7, pages 2670–2676.

David M Blei, Andrew Y Ng, and Michael I Jor-
JMLR,

Latent dirichlet allocation.

dan. 2003.
3(Jan):993–1022.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2016. Enriching word vectors with
subword information. arXiv:1607.04606. Version
2.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Arnav Joshi, Ravendar Lal, Tim Finin, and Anupam
Joshi. 2013. Extracting cybersecurity related linked
In Semantic Computing (ICSC),
data from text.
2013 IEEE Seventh International Conference on,
pages 252–259. IEEE.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efﬁcient text
classiﬁcation. arXiv:1607.01759. Version 3.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. arXiv:1408.5882. Version
2.

Diederik Kingma and Jimmy Ba. 2014.
for

A
arXiv:1412.6980. Version 9.

stochastic

method

Adam:
optimization.

Quentin Le Sceller, ElMouatez Billah Karbab, Mourad
Debbabi, and Farkhund Iqbal. 2017. Sonar: Au-
tomatic detection of cyber security events over the
twitter stream. In Proceedings of the 12th Interna-
tional Conference on Availability, Reliability and Se-
curity, page 23. ACM.

Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution?: normal-
izing text messages without pre-categorization nor
In Proceedings of the 49th Annual
supervision.
Meeting of the ACL: Human Language Technolo-
gies: short papers-Volume 2, pages 71–76. ACL.

Robert A Bridges, Corinne L Jones, Michael D Ian-
nacone, Kelly M Testa, and John R Goodall. 2013.
Automatic labeling for entity extraction in cyber se-
curity. arXiv:1308.4941. Version 3.

Marco Lui and Timothy Baldwin. 2012. langid. py: An
In Pro-
off-the-shelf language identiﬁcation tool.
ceedings of the ACL 2012 system demonstrations,
pages 25–30. ACL.

Ching-Yun Chang, Zhiyang Teng, and Yue Zhang.
2016. Expectation-regulated neural model for event
In HLT-NAACL, pages 400–
mention extraction.
410.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
In Proceedings of the 2014 conference on
works.
EMNLP, pages 740–750.

Jacob Eisenstein. 2013. What to do about bad language
on the internet. In HLT-NAACL, pages 359–369.

Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
In Proceedings of the 2012 joint
for microblogs.
conference on EMNLP and CoNLL, pages 421–432.
ACL.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
In Advances in
chines to read and comprehend.
NIPS, pages 1693–1701.

Kangqi Luo, Xusheng Luo, and Kenny Qili Zhu. 2015.
Inferring binary relation schemas for open informa-
tion extraction. In EMNLP, pages 555–560.

Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. In ACL (System Demon-
strations), pages 55–60.

Jonathan Masci, Ueli Meier, Dan Cires¸an, and J¨urgen
Schmidhuber. 2011. Stacked convolutional auto-
encoders for hierarchical feature extraction. Ar-
tiﬁcial Neural Networks and Machine Learning–
ICANN 2011, pages 52–59.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient estimation of word represen-
tations in vector space. arXiv:1301.3781. Version
3.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. EMNLP.

Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko.
2010. Streaming ﬁrst story detection with applica-
tion to twitter. In Human Language Technologies:
The 2010 Annual Conference of the NAACL, pages
181–189. ACL.

Alan Ritter, Oren Etzioni, Sam Clark, et al. 2012. Open
In Proceed-
domain event extraction from twitter.
ings of the 18th ACM SIGKDD international con-
ference on KDD, pages 1104–1112. ACM.

Alan Ritter, Evan Wright, William Casey, and Tom
Mitchell. 2015. Weakly supervised extraction of
computer security events from twitter. In Proceed-
ings of the 24th International Conference on World
Wide Web, pages 896–905. International World
Wide Web Conferences Steering Committee.

Arpita Roy, Youngja Park, and SHimei Pan. 2017.
Learning domain-speciﬁc word embeddings from
sparse cybersecurity texts. arXiv:1709.07470. Ver-
sion 1.

Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
In Proceedings
event detection by social sensors.
of the 19th international conference on World wide
web, pages 851–860. ACM.

Alberto Tonon, Philippe Cudr´e-Mauroux, Albert
Blarer, Vincent Lenders, and Boris Motik. 2017. Ar-
matweet: Detecting events by semantic tweet anal-
ysis. In European Semantic Web Conference, pages
138–153. Springer.

Jianshu Weng and Bu-Sung Lee. 2011. Event detection

in twitter. ICWSM, 11:401–408.

Wenpeng Yin and Hinrich Sch¨utze. 2015. Learning
meta-embeddings by using ensembles of embedding
sets. arXiv:1508.04257. Version 2.

Quan Yuan, Gao Cong, Zongyang Ma, Aixin Sun, and
Nadia Magnenat Thalmann. 2013. Who, where,
when and what: discover spatio-temporal topics for
In Proceedings of the 19th ACM
twitter users.
SIGKDD international conference on KDD, pages
605–613. ACM.

Supplementary Notes

In this supplement, we provide the implementation
details that we thought might help to reproduce the
results reported in the paper.

What about the model hyperparameters?

In Table 4, we provide the hyperparameters we
used to report the results in the paper.

Can we download the data?

Yes. Along with this submission, we provide the
whole dataset we collected. Nevertheless, due
to the restriction imposed by Twitter, the dataset
only contains unique tweet IDs. However, the
associated tweets can be easily downloaded with
the provided tweet IDs. Dataset is available at
https://stm-ai.github.io/

How to reproduce the results?

Here we describe the key steps to recollect data,
retrain model and reproduce results on the test set.

• Step 1: As mentioned before, researchers can
recollect data through provided tweet IDs.

• Step 2: After recollecting data, preprocess-
ing, normalization and tokenization tasks are
implemented as detailed in Experiments.

general

LDA

w2v & fastText

GloVe

Autoencoder

CRF

Hyperparameter
vector size
num topics
update every
chunksize
passes
window size
min count
iter
alpha
window size
no components
learning rate
epoch num
nb epoch
batch size
shufﬂe
validation split
learning rate
l2 regularization

value
100
40
1
10000
1
5
5
5
0.025
5
100
0.01
10
100
100
True
0.1
0.01
1e-2

Table 4: Selected Hyperparameters

• Step 3:

In order to learn domain-speciﬁc
word embeddings on the unlabeled tweet cor-
pus, meta embedding encoders are trained by
applying word2vec, GloVe and fastText as
discussed in Section 2.

• Step 4: Contextual embedding encoder is im-
plemented in order to reveal contextual infor-
mation as mentioned in Section 2.

• Step 5: Network architecture combined by
CNNs and RNNs is implemented for detect-
ing cyber security related events as detailed
in section 2.

Have you used a simpler model?

We favor simple models over complex ones, but
for our task, detecting cyber security related events
requires tedious effort as well as domain knowl-
edge. In order to capture this domain knowledge,
we designed handcrafted features with domain ex-
perts to address some of the challenges of our
problem. Nevertheless, we also learn to extract
features using deep neural networks.

In the Section 3 of the paper, we also provide
ablations where we discuss which part of the pro-
posed method adds how much value to the overall
success.

Why did you use all of the contextual features?

At ﬁrst glance, it might seem that we threw ev-
erything that we got to solve the problem. How-
ever, we argue that providing contextual features is
somewhat yielding a better initialization, thus pro-
viding a network to converge better local minima.
We also tried out different combinations of con-
textual features, i.e., LDA, NER, IE by training 2
layered fully connected neural net with them and,
although marginally, the combination of all yield
the best results, see Table 5. We argue that NER
is more biased towards making false positives as
it does not consider the word order or semantic
meaning and only raises a ﬂag when many rele-
vant terms are apparent. However, results prove
that NER’s features could be beneﬁcial when used
in combination with IE and LDA which indicates
that NER is detecting something unique that IE
and LDA could not.

How to recollect data?

As our goal is to develop a system to detect cyber
security events, thus collecting more data is crucial
for our task. Hence, using the seed keywords as

What about hardware details?

All computations are done on a system with the
following speciﬁcations: NVIDIA Tesla K80 GPU
with 24 GB of VRAM, 378 GB of RAM and Intel
Xeon E5 2683 processor.

Features
Accuracy
0.725
All
NER & LDA 0.705
0.69
LDA & IE
0.71
NER & IE
0.68
IE
0.64
NER
0.66
LDA

Table 5: Results for Contextual Feature Combinations

described in the paper Section 3, even more data
can be collected using the Twitter’s streaming API
over a desired period.

What are the most common words?

Word cloud in Fig. 4 represents the most common
words inside the dataset without seed keys.

Figure 4: Word Cloud

How about annotations?

We expected annotators to discriminate between a
cyber security event and non cyber security event.
In that regard, we used a team of 8 annotators,
who manually annotated the cyber security related
tweets. Each annotator annotated their share of
tweets individually, and in sum, the team anno-
tated a total of 2K tweets. Following the same
procedure, it is possible to annotate more data,
which we believe to help achieve even better re-
sults.

How is the human evaluation done?

We randomly selected 50 tweets and provided this
subset to 8 human subjects for evaluation. Each
annotator evaluated the tweets independently for
his/her share of 50 tweets. Then, we compared
their annotations against ground-truth annotations.

