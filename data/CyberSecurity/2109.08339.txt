The Stackelberg Equilibrium for One-sided Zero-sum Partially
Observable Stochastic Games

Wei Zheng, Taeho Jung, Hai Lin

1
2
0
2

p
e
S
7
1

]

Y
S
.
s
s
e
e
[

1
v
9
3
3
8
0
.
9
0
1
2
:
v
i
X
r
a

Abstract— Formulating cyber-security problems with attack-
ers and defenders as a partially observable stochastic game has
become a trend recently. Among them, the one-sided two-player
zero-sum partially observable stochastic game (OTZ-POSG)
has emerged as a popular model because it allows players
to compete for multiple stages based on partial knowledge
of the system. All existing work on OTZ-POSG has focused
on the simultaneous move scenario and assumed that one
player’s actions are private in the execution process. However,
this assumption may become questionable since one player’s
action may be detected by the opponent through deploying
action detection strategies. Hence, in this paper, we propose a
turn-based OTZ-POSG with the assumption of public actions
and investigate the existence and properties of a Stackelberg
equilibrium for this game. We ﬁrst prove the existence of
the Stackelberg equilibrium for the one-stage case and show
that
the one-stage game can be converted into a linear-
fractional programming problem and therefore solved by linear
programming. For multiple stages, the main challenge is the
information leakage issue as the public run-time action reveals
certain private information to the opponent and allows the
opponent to achieve more rewards in the future. To deal with
this issue, we adopt the concept of (cid:15)-Stackelberg equilibrium
and prove that this equilibrium can be achieved for ﬁnite-
horizon OTZ-POSGs. We propose a space partition approach
to solve the game iteratively and show that the value function
of the leader is piece-wise linear and the value function of
the follower is piece-wise constant for multiple stages. Finally,
examples are given to illustrate the space partition approach
and show that value functions are piece-wise linear and piece-
wise constant.

I. INTRODUCTION

With advances of technologies in computing, communi-
cations, and control, new engineered systems require tighter
and tighter integration of cyber systems and physical sys-
tems, which increases security risks and attack surfaces, and
therefore brings new challenges to cyber-security defense
[19]. As attack surfaces increase, cyber-attacks may be
composed of multiple stages. For example, attackers may
compromise the most vulnerable device ﬁrst and then exploit
it for attacking other devices. Through observing attack
effects and behaviors of defenders, attackers may adjust
their attacking strategies dynamically. Meanwhile, due to the
limited resources for anomaly detection, information such
as infected devices is usually private for the defender. The
defender has to infer this information from other observations
of the system. The dynamic nature and the partial observ-
ability of this kind of attack require the defender to be more
reactive to the system and robust to information uncertainty.
Recently, various approaches have been proposed to mit-
igate cyber-security concerns such as the machine learning
approach [23], the data mining approach [7] and the game

theory approach [10]. In this paper, we will focus on the
game theory approach because it provides a theoretical study
of interactions among independent players. Most of the
existing games in the literature focus on static behaviors
and ignore the dynamic nature. Dynamic games such as
repeated games [12], evolutionary games [18] and stochastic
games [21] consider the dynamic behavior but assume the
full observability of game information. As a critical branch
of the game theory, the partially observable stochastic game
(POSG) attracts more and more attention in the area because
the POSG allows players to compete sequentially based on
partial knowledge of the system, which is closer to real
cyber-security problems [22], [17], [26]. Game models such
as the static game, repeated game, and stochastic game are
special cases of the POSG. However, solving the general
POSG is a nontrivial task. The dynamic programming al-
gorithm that solves the game exactly becomes inefﬁcient
quickly beyond a small horizon [13]. Although approximate
algorithms such as the (cid:15)-pruning approximation [20] and
the Bayesian game approximation [11] have been proposed,
the POSG planning algorithm is still not mature enough for
practical applications.

Sub-classes of POSGs have been proved more practical
than the general case. The one-sided two-player zero-sum
POSG assumes that one player can observe the state directly
while the other player accesses the state via a partial ob-
servation [14]. This model allows both players to maintain
a common belief over states, which enables us to design
and implement efﬁcient planning algorithms. Meanwhile,
this model reserves properties of the general POSG, such as
the dynamic nature and partial observability. A point-based
approximate algorithm [14], a heuristic search algorithm
[16], and a mixed-integer linear programming approach [2]
have been proposed to solve the game efﬁciently. Beyond
the theoretical work, the OTZ-POSG has been extensively
discussed for cyber-security problems such as the com-
puter network defense [17], [25], [24]. For a more general
setup than the OTZ-POSG [14], the two-player zero-sum
POSG with public observations assumes that each player
has private information and has a partial observation on the
other player’s private information [15]. By assuming that
observations are public, the existence of the Nash equilibrium
is guaranteed.

All aforementioned OTZ-POSGs assume that both players
move simultaneously and one player’s action is unobservable
by the other player. However, these assumptions may become
questionable for real-world applications. First, to guarantee
the simultaneous move is not always realistic, especially for

 
 
 
 
 
 
a competing scenario. Secondly, the actions of players may
become observable as opponents may deploy action detection
strategies in the execution stage. Hence, we propose the
turn-based OTZ-POSG with public actions and investigate
the ﬁnite-horizon Stackelberg equilibrium in this paper. In
this game, one player (the leader/the defender) plays ﬁrst
while the other player (the follower/the attacker) follows.
Certain statues of the environment or the attacker, such as
the attacker’s locations, are only partially observed by the
defender. The defender plays ﬁrst because he has to deploy
defending resources according to partial observations before
the attack. In each stage, both actions of the defender and
the attacker are observable at the end of the stage.

First, we prove the existence of the Stackelberg equilib-
rium for the one-stage OTZ-POSG and show that the one-
stage OTZ-POSG can be converted into a linear-fractional
programming problem, and therefore solved by linear pro-
gramming. The value function of the leader is piece-wise
linear and the value function of the follower is piece-wise
constant. These value functions are solved by enumerating
all extreme points of the linear program. For multiple stages,
the main challenge is the information leakage issue because
the follower’s policy is private-information-dependent. When
taking full advantage of the private information, the follower
reveals certain private information to the leader. Then, the
leader can infer more private information from the follower
and achieves more rewards in the following stages. To solve
this issue, we adopt the concept of (cid:15)-Stackelberg equilibrium
[5]. At this equilibrium, the follower sacriﬁces certain re-
wards in the current stage for more rewards in the following
stages. We propose a space partition approach to solve the
game through value iteration and show that value functions
for both players are piece-wise linear and piece-wise constant
respectively.

The main contribution of this paper is twofold. First, we
prove the existence of the Stackelberg equilibrium for the
one-stage OTZ-POSG and show that the one-stage game can
be solved by linear programming. Hence, value functions
of players are piece-wise linear and piece-wise constant
respectively. Secondly, we adopt the concept of (cid:15)-Stackelberg
equilibrium and prove that the (cid:15)-Stackelberg equilibrium is
achieved for ﬁnite-horizon OTZ-POSGs with public actions.
Meanwhile, we propose a dynamic programming algorithm
to solve the ﬁnite-horizon OTZ-POSG iteratively through
belief space partition.

The rest of the paper is organized as follows. Section II
deﬁnes the OTZ-POSG and formulates the problem. The ex-
istence of the Stackelberg equilibrium for the one-stage OTZ-
POSG and the policy-solving algorithm are given in Section
III. Section IV introduces the (cid:15)-Stackelberg equilibrium and
proposes the space partition approach to solve the multiple-
stage game iteratively. Section V concludes the paper.

Notations: R represents the set of real numbers and Rp×q
represents the set of real-valued matrices with p rows and
q columns. Speciﬁcally, 1p represents a vector of ones
with dimension p and 0p represents a vector of zeros with
dimension p. For a vector v ∈ Rp×1, vT stands for the

Fig. 1. The information ﬂow of the OTZ-POSG.

transpose of the vector and vi stands for the ith element
of the vector v. For two vectors v, u ∈ Rp×1, v ≤ u implies
vi ≤ ui for all i. For a ﬁnite set S, |S| represents the
cardinality of the set. P(·) denotes the probability and E
denotes the expectation.

II. PRELIMINARIES AND PROBLEM FORMULATIONS

In this section, we give the formal deﬁnition of the OTZ-

POSG and formulate the problem.

Deﬁnition 1. A OTZ-POSG model is deﬁned as a tuple G =
(I, S, O, AL, AF , T, Ξ, Υ, b0) where

• I = {leader(L), f ollower(F )} is a set of players;
• S is a ﬁnite set of states;
• O is a ﬁnite set of observations;
• Ai is a ﬁnite set of actions of player i ∈ I;
• T : S × AL × AF × S → [0, 1] is a transition function;
• Ξ : S × O → [0, 1] is an observation function;
• Υ : S × AL × AF → R is a reward function of player

F ;

• b0 : S → [0, 1] is an initial belief over states.

The game G is played in turn and actions of players
are public. The game playing process is shown in Fig. 1.
Arrows represent information dependencies. For any stage t,
the state st ∈ S is only informed to the follower. The leader
takes an action ﬁrst according to the observation ot ∈ O.
This action would not be revealed to the follower until the
follower’s action is taken. Each player achieves a reward
and this reward is not explicitly announced until the end of
the game. The state of the system transits from st to st+1
according to the transition function T (st+1|st, aL
t ) which
deﬁnes the distribution over the next state st+1 ∈ S after
taking a joint action [aL
t ] ∈ AL × AF from the state
st. An observation ot+1 ∈ O generated according to the
observation function Ξ(ot+1|st+1) is publicly observed by
players. Since actions are public, the leader’s optimal strategy
in each stage relies on the action executed by the follower
in the previous stage (see the red dotted line). The initial
belief b0 is a probabilistic distribution over states. It is used
to describe the initial knowledge of players. Initially, a state
s0 ∈ S is drawn according to this distribution and the state
is only informed to the follower. We assume that the initial
belief is common knowledge of both players.

t , aF

t , aF

Since the game is zero-sum, the reward function of the
leader is −Υ(s, aL, aF ) for all s ∈ S, aL ∈ AL and aF ∈
AF . Because the game is one-sided, the leader has to infer

the state st of the game through the initial belief b0 and the
information observed.

Deﬁnition 2. Up to stage t, the observable path of the leader
is υt = b0aL
0 aF
t−1ot and the observable path of
the follower is ωt = b0s0aL
t−1aF
0 s1o1...aL
t−1stot where
τ ∈ AF , sτ ∈ S, and oτ ∈ O for all 0 ≤ τ ≤ t.
τ ∈ AL, aF
aL

0 o1...aL

t−1aF

0 aF

Based on the observable path, the leader can reason about
the state of the game through a probability distribution over
states for which we call the belief state.

Deﬁnition 3. A belief state of a OTZ-POSG G is deﬁned as
a conditional probability distribution over states, i.e., bst
t =
t−1, aF
P(st|b0, aL

0 , ..., ot−1, aL

t−1, ot).

0 , aF

Beginning with the initial belief b0, we can calculate the
belief bt by the Bayes’ rules incrementally. The updated
belief bt+1 after taking a particular joint action at = [aL
t , aF
t ]
and observing ot+1 is

bst+1
t+1 =

Ξot+1
st+1

(cid:80)
T st+1
st,at bst
t
st
(cid:80)
T st+1
Ξot+1
st,atbst
st+1
t

st

(cid:80)

st+1

,

(1)

where T st+1
probabilities and observation probabilities.

st,at and Ξot+1

st+1 are concise notations for transition

To behave optimally, players have to plan to act according
to their observable paths. Because the belief is a sufﬁcient
statistic of the path υt,
the leader can act equivalently
according to the belief [1]. As υt is a sub-sequence of ωt,
the follower can maintain the same belief state as that of
the leader, and therefore, act equivalently according to the
belief-state pair. In this paper, we consider mixed policies
for players. A mixed policy is a probability distribution over
the action space.

Deﬁnition 4. The policy of the leader is deﬁned as a
mapping from a belief to a distribution ηt over the action
space AL, i.e., πL : bt → ηt and the policy of the follower is
deﬁned as a mapping from a belief-state pair to a distribution
δt over action space AF , i.e., πF : (bt, st) → δt where
ηt ∈ R|AL|×1 and δt ∈ R|AF |×1.

Assumption 1. In each stage, the strategy adopted by the
leader is known by the follower.

Remark 1. This assumption is usually referred to the
commitment in the literature [9]. We have this assumption
because the leader usually arrives at the site where the game
is played before the follower. For example, the defender
usually arrives at
the site before the attacker for cyber-
security defense. The strategy adopted by the leader could
be learned by the follower through long-term observations.

Once the policies of players are ﬁxed, each player is

expected to receive a reward for ﬁnite stages.

achieved by the leader is deﬁned as

πL,πF (b0) = E[
vL

h
(cid:88)

t=1

Υ(st, aL

t , aF

t )|b0, πL, πF ]

(2)

and the total reward achieved by the follower is deﬁned as

πL,πF (b0, s0) = E[
vF

h
(cid:88)

t=1

Υ(st, aL

t , aF

t )|b0, s0, πL, πF ]

(3)

Remark 2. Once the policies of players are ﬁxed, both the
state transition and state observation are stochastic. Hence,
total rewards are deﬁned as expectations of cumulative
rewards over all stages. It is noted that the total rewards
deﬁned above are for the reward function of the follower.
Maximizing the reward for the leader is equivalent to mini-
mizing the reward vL

πL,πF (b0).

Since the game is turn-based and the policy of the leader
is known by the follower, the leader has to optimize the
total reward concerning the best response of the follower.
Hence, we introduce the Stackelberg equilibrium to study
the behavior of the game [6].

Deﬁnition 6. Given the total rewards for both players, a pair
of policies [ˆπL, ˆπF ] forms a Stackelberg equilibrium if they
satisfy following conditions,

vL
ˆπL,σ(ˆπL)(b0) ≤ vL
ˆπL,ˆπF (b0, s0) ≥ vF
vF

∀πL,
πL,σ(πL)(b0),
ˆπL,πF (b0, s0), ∀πF ,

(4)

where σ(πL) is a reaction function of the follower deﬁned
by σ(πL) = arg maxπF vF

πL,πF (b0, s0).
At the Stackelberg equilibrium, neither the leader nor the
follower has the incentive to change the policy. Because of
Assumption 1, the follower always responses optimally to the
leader’s policy. Hence, the Stackelberg equilibrium deﬁnes
its ﬁrst inequality with the reaction function σ(πL).

In this paper, we will study the existence of the Stackel-
berg equilibrium and provide policy solving algorithms for
the one-stage OTZ-POSG and its (cid:15)-version for the multi-
stage OTZ-POSG.

Problem 1. Given a OTZ-POSG model G and a ﬁnite hori-
zon h, solve policies πL and πF that achieve the Stackelberg
equilibrium if the equilibrium exists.

Remark 3. Each stage of the OTZ-POSG is a two-player
zero-sum Bayesian game with one-sided information [27].
In the one-stage game, there are several normal-form games
indexed by state s ∈ S. The leader has a probability
distribution on the normal-form game while the follower
knows the exact normal-form game they are playing. As
the one-stage game builds the foundation for the multi-
stage OTZ-POSG, we ﬁrst discuss the one-stage game in
the following section.

III. ONE-STAGE OTZ-POSGS

Deﬁnition 5. Given a OTZ-POSG model G, a ﬁnite horizon
h, and a pair of policies [πL, πF ] for players, the total reward

For the convenience of notation, we use b to represent
the belief, matrix Υsi ∈ R|AL|×|AF | to represent the reward

matrix for state si ∈ S and Υ[aL,aF ]
at row aL ∈ AL and column aF ∈ AF .

si

to represent the element

Deﬁnition 7. At
function of the leader is

the Stackelberg equilibrium,

the value

vL(b) = min

η

(cid:104) (cid:88)

si

bsi

(cid:0) max
δsi

(cid:2)ηT Υsiδsi(cid:3)(cid:1)(cid:105)

,

(5)

and the value function of the follower is vF (b, si) =
(cid:2)ˆηT Υsiδsi(cid:3), ∀si ∈ S, where η ∈ R|AL|×1 and
maxδsi
δsi
∈ R|AF |×1 are policies of players, and ˆη is the solution
of Equation (5).

Remark 4. The value function vL(b) is the weighted average
of the value function vF (b, si) over state si and the weight
is the belief b. Both value functions are well-deﬁned because
the Stackelberg equilibrium always exists. To see this, we ﬁx
the policy of the leader η ﬁrst and solve the value function
vF (b, si) for any belief b and state si. Because η is in a
bounded space, we can solve the value function vL(b) by
taking the minimum value over a bounded space. The main
challenge here is how to solve the optimal policy η for
the leader and represent both value functions concisely. To
solve this issue, we ﬁrst make an assumption on the reward
function.

Lemma 1. Let Υsi ∈ Rp×q and Θsi ∈ Rp×q be reward
matrices for state si, and they are related to each other by
q , ∀si ∈ S where c ∈ R is a
the relation Θsi = Υsi + c1p1T
constant. Then, every mixed policy achieving the Stackelberg
equilibrium for the matrix set {Θs1 , ..., Θsn } also constitutes
a mixed policy at the Stackelberg equilibrium for the matrix
set {Υs1 , ..., Υsn}, and vice versa.

The proof of the lemma is straightforward and thus omitted
here. Through Lemma 1, we can assume that the reward
function is lower bounded by a positive real value.

Assumption 2. The reward function of the game G is lower
bounded by a positive real value, i.e., ∃ r > 0 such that
Υ[aL,aF ]
≥ r for all s ∈ S, aL ∈ AL, and aF ∈ AF .

s

Theorem 1. For any one-stage OTZ-POSG, the policy η that
achieves the Stackelberg equilibrium deﬁned by Equation (5)
can be solved by linear programming.

Proof. Inspired by the work for normal-form games [5],
we deﬁne a function f i(η) for each state si as f i(η) =
maxδsi [ηT Υsiδsi
]. As the function f i(η) is the maximum
value, we have f i(η) ≥ ηT Υsiδsi
for all δsi
. It is equivalent
to the inequality f i(η)1|AF | ≥ ΥT
siη. Let’s deﬁne a new
variable xi = η/f i(η). It is easy to see that xi are linearly
dependent for all i. Deﬁne the scale factor between the
vector x1 and xi to be ρi = x1/xi. The policy η and the
corresponding value function vL(b) can be solved by the

following optimization problem.

min
x1,ρ2,...,ρn

bT ρ
(x1)T 1|AL|

s.t. ΥT

si x1 ≤ ρi1|AF |, ∀i ∈ {1, ..., n},
x1 ≥ 0|AL|, ρj > 0, ∀j ∈ {2, ..., n},
(x1)T 1|AL| ≥ 1/ ¯f ,

(6)

where ρ = [ρ1, ρ2, ..., ρn]T ∈ Rn×1, ρ1 = 1, n = |S| and ¯f
is an upper bound of f i(η) for all 1 ≤ i ≤ n.

The optimization problem given by Equation (6) is a

linear-fractional program as it is equivalent to

cT z + α
dT z

min
z
s.t. Γz ≤ β,

dT z ≥ 1/ ¯f , z ≥ 0|AL|+n−1,

(7)

where α = bs1,












.

,















































z =

Γ =

, c =

, d =

, β =

1|AF |
0|AF |
· · ·
0|AF |

0|AL|
bs2
· · ·
bsn


0|AF |
...
0|AF |
...
...
· · ·
... −1|AF |

x1
1|AL|
ρ2
0
· · ·
· · ·
ρn
0
ΥT
0|AF |,
s1 ,
ΥT
s2 , −1|AF |,
· · ·
· · ·
ΥT
0|AF |,
sn ,
the linear-fractional program is
The feasible region of
nonempty as the optimal policy of the leader exists and
the value dT z = 1/f 1(η) is non-zero for all η. Hence, the
linear-fractional program has a feasible solution. The feasible
region of the linear-fractional program is not bounded. But
we can restrict variables into a bounded space without
changing the optimal solution of the original problem. To
show this, we begin with the deﬁnition of the function f i(η).
With Assumption 2, we have f i(η) ≥ r. Hence, the variable
xi ≤ (1/r)1|AL| for all i. For the variable ρi = f i(η)/f 1(η),
it is also upper bounded because f i(η) is upper bounded and
f 1(η) is lower bounded by r. Meanwhile, we have ρi > 0
for all i. Hence, we can restrict the variables into a bounded
space U = {z|0|AL| ≤ x1 ≤ (1/r)1|AL|, 0 < ρi ≤ ¯f /r, ∀i}
without changing the optimal solution.

Because the denominator dT z is lower bounded by a
positive value, we can convert the linear-fractional program
to a linear program [8].

cT µ + αλ

min
µ,λ
s.t. Γµ ≤ βλ,

dT µ = 1, 0|AL|+n−1 ≤ µ ≤ ¯µ,
0 < λ ≤ ¯f ,

(8)

|AL|, ¯f 1T

where µ = z

dT z , λ = 1

dT z and ¯µ = [1T
Remark 5. We tighten the constraint when converting the
linear-fractional program to a linear program because the
extra constraint U is too loose. The extra constraint U is only
used to guarantee that the feasible region is bounded. From

n−1]T .

the original optimization problem given by Equation (6), we
give a tighter constraint on the variable µ and λ without
changing solutions. By solving the linear program, the policy
is η = [µ1, ..., µ|AL|]T and the value vL(b) is cT µ + αλ for
any b. Because the coefﬁcient c and α is uniquely determined
by the belief b, the value function vL(b) is piece-wise linear
for belief b.

Deﬁnition 8. Let D represent the convex polyhedral set
deﬁned by the linear constraint of the linear program in
Equation (8). A point [µT , λ]T ∈ R(|AL|+n)×1 of the polyhe-
dron D is called an extreme point if there exists a coefﬁcient
[cT , α]T ∈ R(|AL|+n)×1 such that cT µ + αλ < cT µ(cid:48) + αλ(cid:48)
for all [(µ(cid:48))T , λ(cid:48)]T (cid:54)= [µT , λ]T ∈ D.

For a linear objective function cT µ + αλ deﬁned over a
polyhedral convex set D, the minimum value is taken only
at extreme points of D. Hence, a direct result from Theorem
1 is the value function representation.

Corollary 1. Let V = {[(µi)T , λi]T } denote the set of
all extreme points of the linear constraint
in Equation
(8). The value function of the leader can be represented
concisely as vL(b) = mini[bT θ(µi, λi)] where θ(µ, λ) =
[λ, µ|AL|+1, ..., µ|AL|+n]T is a vector extracting elements
from µ and λ corresponding to the nonzero entries of the
coefﬁcient [cT , α]T .

Remark 6. All extreme points of the polyhedral convex set
D can be solved using the algorithm proposed in [4]. The
value function of the leader is piece-wise linear and convex
for the belief b. Correspondingly, the value function vF (b, si)
is piece-wise constant for belief b.

1 , aL

Example 1. We consider a one-stage OTZ-POSG where the
state space is S = {s1, s2}, the action set of the leader is
AL = {aL
2 }, the action set of the follower is AF =
{aF
1 , aF
2 }, and the reward matrix is Υs1 = [4, 2; 2, 7] for
state s1 and Υs2 = [8, 6; 3, 4] for state s2. Extreme points
and the corresponding policies η derived from this game are
listed in the following table.
ˆη2

µ3

ˆη1

λ

1
2
3

0.000 1.000
0.333 0.667
0.714 0.286

7.000 4.000
5.333 4.667
3.429 6.571

The piece-wise linear value function vL(b) and piece-wise
constant value function vF (b, si) are shown in Fig. 2. Three
extreme points are found and vectors θ(µi, λi) are shown
by dotted lines. For different belief b, an extreme point is
chosen by the min operator.

IV. MULTI-STAGE OTZ-POSGS
For the OTZ-POSG with multiple stages, the equilibrium
can be solved by dynamic programming and the total rewards
can be solved through value iteration. Technically, at stage
t, the value function of the leader achieving the Stackelberg
equilibrium is

vL(bt) = min
ηt

(cid:104) (cid:88)

st

bst
t

(cid:0) max
δst
t

(cid:2)ηT

t (Υst + Φst)δst
t

(cid:3)(cid:1)(cid:105)

,

(9)

Fig. 2.
constant value function vF (b, si).

The piece-wise linear value function vL(b) and the piece-wise

where ηt ∈ R|AL|×1 and δst
t ∈ R|AF |×1 are policies. The ma-
trix Φst ∈ R|AL|×|AF | represents the future reward for each
joint action at = [aL
t and col-
= (cid:80)
Ξot+1
st+1 vF (bt+1, st+1).
umn aF
t
For any st ∈ S,
the value function of the follower is
vF (bt, st) = ˆηT
t are solutions
of Equation (9).

t ]. The element at row aL
ot+1

t where ˆηt and ˆδst

t (Υst +Φst )ˆδst

t , aF
st+1

is Φat
st

T st+1
st,at

(cid:80)

The main challenge is the information leakage issue
which also appeared in the repeated games with incomplete
information [3]. As Fig. 1 shows, the action taken by the
follower reveals the state information because the policy of
the follower is state-dependent. If the follower takes full
advantage of the private information, the follower reveals
the state information to the leader. Then, the leader can infer
more state information and achieves more rewards in the
future.

To solve this issue, we consider the (cid:15)-Stackelberg equi-
librium. The basic idea is to sacriﬁce certain rewards in the
current stage for more future rewards (from the follower’s
perspective). First, we deﬁne a sub-optimal policy for the
follower.
Deﬁnition 9. A policy ˜δst
sacriﬁce policy of ˆδst
if ˆηT
t
all state st ∈ S.

t ∈ R|AF |×1 is said to be a (cid:15)-
t (Υst + Φst)(ˆδst
t ) ≤ (cid:15) for

t − ˜δst

By adopting the (cid:15)-sacriﬁce policy, the follower guarantees
that the sacriﬁced reward is bounded by (cid:15). Meanwhile, by
keeping the (cid:15)-sacriﬁce policy private, the follower prevents
the leader from inferring the state information for future
stages. Inspired by the work [5], we ﬁt the concept of (cid:15)-
Stackelberg equilibrium to our problem as follows.

Deﬁnition 10. Given the total rewards for both players, a
pair of policies [ˆπL, ˆπF ] forms a Stackelberg equilibrium if
it satisﬁes the following conditions,

ˆπL,σ(ˆπL)(b0) ≤ vL
vL
ˆπL,ˆπF (b0, s0) ≥ vF
vF

∀πL,
πL,σ(πL)(b0) + (cid:15),
ˆπL,πF (b0, s0) − (cid:15), ∀πF ,

(10)

where σ(πL) is a reaction function of the follower deﬁned
by σ(πL) = arg maxπF vF

πL,πF (b0, s0).
To show the (cid:15)-Stackelberg equilibrium, we ﬁrst calculate
from the value function vF (bt+1, st+1).
the matrix Φst
Although the value function is piece-wise constant, its value
relies on the belief bt+1. We have to represent it as a function

of belief bt. To solve this issue, we propose a belief space
partition approach.

Deﬁnition 11. Given a belief space ∆, a partition of the
belief space ∆ is deﬁned as Λ = {∆1, ..., ∆m} where ∆ =
∪i∆i and ∆i ∩ ∆j = ∅, ∀i (cid:54)= j.

i

i

i

i

mt+1

into ¯Πi

} with ¯∆t

t+1bt+1 ≤ 0lt+1

i deﬁned as ¯∆t

at,ot+1
tbt ≤ 0lt+1

, ..., ∆t+1
mt+1
if Πi

In our proposed belief space partition approach, each
partition ∆i is represented as ∆i = {b ∈ ∆|Πib ≤ 0li}
where Πi ∈ R|li|×|S| can be constructed iteratively. To
illustrate the space partition approach, we begin with the
stage t + 1 and assume that the belief space partition Λt+1 =
{∆t+1
} is given. A belief bt+1 belongs to the set
1
∆t+1
. For each joint action at and
i
observation ot+1, plugging in the belief bt+1 from Equa-
tion (1), we can convert the linear constraint Πi
t+1bt+1 ≤
. The partition of the belief bt
0lt+1
tbt ≤ 0lt+1
i, ..., ¯∆t
is ¯Λt
= { ¯∆t
i =
{bt| ¯Πi
}. Combing partitions of all joint actions
at and observations ot+1, we have a ﬁner partition of the
belief space ¯Λt = {∩at,ot+1
¯∆t
}.
The constraint matrix for the intersection can be achieved
by concatenating the constraint matrix ¯Πi
t together. In each
i of set ¯Λt, the value vF (bt+1, st+1) is a constant
region ∆t
value. Hence, matrices {Φst, st ∈ S} are constant and can
be calculated. Based on the matrices {Φst, st ∈ S}, a new
linear program can be formulated and all extreme points can
be founded. The region ∆t
i can be further partitioned using
these extreme points. In each partition, the value function
vL(bt) is linear and the value function vF (bt, st) is constant.
Through partitioning the belief space, we can calculate
the value function iteratively. When the follower adopts a
(cid:15)-sacriﬁce policy, the (cid:15)-Stackelberg equilibrium is achieved.

∈ ¯Λt

at,ot+1

at,ot+1

at,ot+1

| ¯∆t

Assumption 3. The fact that the follower adopts a (cid:15)-sacriﬁce
policy is common knowledge for both players, but the value
(cid:15) and the (cid:15)-sacriﬁce policy are private.

Remark 7. Through Assumption 3, we assert
the
leader only updates the belief state using observations. It is
because the value (cid:15) and the (cid:15)-sacriﬁce policies are private
information. It is nontrivial for the leader to infer the (cid:15)-
sacriﬁce policy in ﬁnite stages.

that

Theorem 2. Given a ﬁnite horizon h and a positive real value
(cid:15), the OTZ-POSG achieves an (cid:15)-Stackelberg equilibrium if
the follower adopts a

(cid:15)
h+1 -sacriﬁce policy.

Proof. We prove this theorem by induction. Let ˜vL(bt) and
˜vF (bt, st) denote value functions when the follower adopts
a (cid:15)/(h + 1)-sacriﬁce policy. At stage h, it is straightfor-
ward to verify the inequality |vF (bh, sh) − ˜vF (bh, sh)| ≤
(cid:15)/(h + 1) for any state sh. At stage t + 1, we assume
that |vF (bt+1, st+1) − ˜vF (bt+1, st+1)| ≤ (h − t)(cid:15)/(h + 1)
for any state st+1. Then, at stage t, we have |Φat
−
st
¯Φat
is the matrix cal-
st
culated from value function ˜vF (bt+1, st+1). It is easy to
|vF (bt, st) − ¯vF (bt, st)| ≤ (h − t)(cid:15)/(h + 1)
check that
where ¯vF (bt, st) is the value function derived with matrices

| ≤ (h − t)(cid:15)/(h + 1) where ¯Φst

{ ¯Φst, st ∈ S}. After adopting a (cid:15)/(h + 1)-sacriﬁce policy
at stage t, the value sacriﬁced by the follower is bounded,
i.e., |¯vF (bt, st) − ˜vF (bt, st)| ≤ (cid:15)/(h + 1). Hence, the total
distance |vF (bt, st) − ˜vF (bt, st)| is bounded by |vF (bt, st) −
¯vF (bt, st)| + |¯vF (bt, st) − ˜vF (bt, st)| = (h − t + 1)(cid:15)/(h + 1)
for any state st ∈ S. By induction, we have |vF (b0, s0) −
˜vF (b0, s0)| ≤ (cid:15). Because vL(b0) = (cid:80)
bs0
0 vF (b0, s0) and
s0
˜vL(b0) = (cid:80)
bs0
0 ˜vF (b0, s0), we have |vL(b0)−˜vL(b0)| ≤ (cid:15).
As a consequence, the (cid:15)-Stackelberg equilibrium is achieved
because we have ˜vF (b0, s0) ≥ vF (b0, s0) − (cid:15) and ˜vL(b0) ≤
vL(b0) ≤ vL(b0) + (cid:15).

s0

Theorem 3. The value function of the leader is piece-wise
linear and the value function of the follower is piece-wise
constant for any stage t.

The theorem is a direct result of the belief space partition

approach and the proof is omitted here.

Remark 8. By Theorem 2 and through the belief space
partition approach, the (cid:15)-Stackelberg equilibrium is achieved
and policies at the equilibrium are solved iteratively. Hence,
Problem 1 is solved.

Remark 9. From Theorem 3, the belief space is partitioned
ﬁner and ﬁner. In the worst case, there are

κ =

(|AL| + |S| + |AF ||S|)!
(|AL| + |S| − 1)!(|AF ||S| + 1)!

extreme points for each linear program. The total number
of partitions grows double exponentially with respect to the
planning horizon h, i.e. O(κ(|AL||AF ||O|)h
), which is a po-
tential bottleneck of the value function calculation. However,
an approximation algorithm with performance guarantees
is nontrivial to develop because the value function of the
follower is piece-wise constant. To approximate this value
function, evaluating the boundary is inevitable.

Example 2. Consider a OTZ-POSG model where the state
space, the action space and the reward function are deﬁned
in Example 1. The observation set is O = {o1, o2} and the
transition function is

a1,1
s1
s2
a2,1
s1
s2

s1
0.3
0.9
s1
0.8
0.1

s2
0.7
0.1
s2
0.2
0.9

a1,2
s1
s2
a2,2
s1
s2

s1
0.0
0.8
s1
0.5
0.0

s2
1.0
0.2
s2
0.5
1.0

,

i , aF

represents the joint action [aL

where ai,j
j ]. The ob-
servation probability is Ξs1 = [0.6, 0.4] and Ξs2 =
[0.1, 0.9]. We assume that
the value functions at stage
t + 1 is given by Fig. 2. The belief space is par-
=
titioned into three regions
{bt+1|Πi
t+1 = [1.67, −0.67;
3.57, −2.57], Π2
and
Π3
t+1 = [−3.57, 2.57; −1.91, 1.91]. For each joint action
and observation, we can convert the linear constraint into
the form of ¯Πi
tbt ≤ 02. In this process, the number of

t+1 = [−1.67, 0.67; 1.91, −1.91]

t+1bt+1 ≤ 02} where Π1

represented by ∆t+1

i

all extreme points of the linear program, we have shown
that the value function of the leader is piece-wise linear and
the value function of the follower is piece-wise constant.
For the ﬁnite-horizon POSG, we have proved that the (cid:15)-
Stackelberg equilibrium is achieved. This study will pave
the way towards a formal and systematic design theory for
problems such as cyber-security defense when actions are
public. One bottleneck of the proposed approach is the high
computation complexity as the number of partitions grows
very fast for the planning horizon. To reduce the complexity
will be one of our further work.

REFERENCES

[1] Douglas Aberdeen, Olivier Buffet, and Owen Thomas.

Policy-
In Artiﬁcial Intelligence and

gradients for PSRs and POMDPs.
Statistics, pages 3–10, 2007.

[2] Mohamadreza Ahmadi, Murat Cubuktepe, Nils Jansen, Sebastian
Junges, Joost-Pieter Katoen, and Ufuk Topcu. The partially observable
games we play for cyber deception. arXiv preprint arXiv:1810.00092,
2018.

[3] Robert J Aumann, Michael Maschler, and Richard E Stearns. Repeated

games with incomplete information. MIT press, 1995.

[4] Michel L Balinski. An algorithm for ﬁnding all vertices of convex
polyhedral sets. Journal of the Society for Industrial and Applied
Mathematics, 9(1):72–88, 1961.

[5] Tamer Bas¸ar and Geert Jan Olsder. Dynamic noncooperative game

theory. SIAM, 1998.

[6] Michele Breton, Abderrahmane Alj, and Alain Haurie. Sequential
Stackelberg equilibria in two-person games. JOTA, 59(1):71–97, 1988.
[7] Anna L Buczak and Erhan Guven. A survey of data mining and
machine learning methods for cyber security intrusion detection. IEEE
Communications surveys & tutorials, 18(2):1153–1176, 2015.

[8] Abraham Charnes and William W Cooper. Programming with linear
fractional functionals. Naval Research logistics quarterly, 9(3-4):181–
186, 1962.

[9] Vincent Conitzer and Tuomas Sandholm. Computing the optimal
In Proceedings of the 7th ACM conference

strategy to commit to.
on Electronic commerce, pages 82–90, 2006.

[10] Cuong T Do, Nguyen H Tran, Choongseon Hong, Charles A
Kamhoua, Kevin A Kwiat, Erik Blasch, and et al. Game theory for
cyber security and privacy. ACM Computing Surveys (CSUR), 50(2):1–
37, 2017.

[11] Rosemary Emery-Montemerlo, Geoff Gordon, Jeff Schneider, and
Sebastian Thrun. Approximate solutions for partially observable
stochastic games with common payoffs. In Proceedings of the Third
International Joint Conference on AAMAS, 2004. AAMAS 2004., pages
136–143. IEEE, 2004.

[12] Mehran Fallah. A puzzle-based defense strategy against ﬂooding
IEEE transactions on dependable and

attacks using game theory.
secure computing, 7(1):5–19, 2008.

[13] Eric A Hansen, Daniel S Bernstein, and Shlomo Zilberstein. Dynamic
In AAAI,

programming for partially observable stochastic games.
volume 4, pages 709–715, 2004.

[14] Karel Hor´ak and Branislav Boˇsansk`y. A point-based approximate
algorithm for one-sided partially observable pursuit-evasion games. In
International Conference on Decision and Game Theory for Security,
pages 435–454. Springer, 2016.

[15] Karel Hor´ak and Branislav Boˇsansk`y. Solving partially observable
stochastic games with public observations. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, volume 33, pages 2029–2036,
2019.

[16] Karel Hor´ak, Branislav Boˇsansk`y, and Michal Pˇechouˇcek. Heuristic
search value iteration for one-sided partially observable stochastic
In Thirty-First AAAI Conference on Artiﬁcial Intelligence,
games.
2017.

[17] Karel Hor´ak, Branislav Boˇsansk`y, Petr Tom´aˇsek, Christopher Kiek-
intveld, and Charles Kamhoua. Optimizing honeypot strategies against
dynamic lateral movement using partially observable stochastic games.
Computers & Security, 87:101579, 2019.

[18] Jianming Huang, Hengwei Zhang, and Jindong Wang. Markov
IEEE
evolutionary games for network defense strategy selection.
Access, 5:19505–19516, 2017.

Fig. 3. The partition of the belief space at stage t + 1 for the joint action
a1,1
t

and observation o2

t+1.

Fig. 4. The partition of the belief space at stage t and the corresponding
value function of the leader.

t

t are ¯Π1

t+1, the constraint matrices ¯Πi

partitions may be reduced. For the joint action a1,1
and
the observation o1
t =
[0.25, 0.89; 0.46, 1.90], ¯Π2
t = [−0.25, −0.89; 0.21, 1.01] and
¯Π3
t = [−0.46, −1.90; −0.21, −1.01]. Among them, only one
3 = {bt| ¯Π3
partition ¯∆t
t bt ≤ 02} is active. For joint action
t+1, the constraint matrices are ¯Π1
a1,1
and observation o2
t =
t
[−0.22, 0.54; −1.19, 1.05], ¯Π2
t = [0.22, −0.54; −0.97, 0.51]
and ¯Π3
t = [1.19, −1.05; 0.97, −0.51]. All these partitions are
active and the corresponding regions are shown in the left
ﬁgure of Fig. 3. The value (cid:80)
Ξot+1
st+1 T st+1
st,at vF (bt+1, st+1)
when ot+1 = o2
is shown in the right ﬁgure
t
of Fig. 3. The red dash-dot lines represent the boundaries of
partitions. After combing partitions of all joint actions and
observations, we have a ﬁner partition of the belief space.
In each partition, we can calculate all candidate α-vectors
because the matrix Φst
is constant. The value function
at stage t is shown in Fig. 4. The dotted lines represent
candidate α-vectors in each partition and solid lines represent
the picked α-vectors for the value function.

st+1
t+1 and at = a1,1

Remark 10. From Fig. 4, we see that the value function of
the leader is piece-wise linear but not continuous. It is why
existing planning algorithms for the POMDP model do not
work for the OTZ-POSG. The POMDP can be treated as a
special case of the OTZ-POSG. Hence, the proposed space
partition algorithm can be applied on the POMDP model.

V. CONCLUSION

In this paper, we considered the policy design problem for
turn-based OTZ-POSGs with public actions. We proved the
existence of the Stackelberg equilibrium for the one-stage
OTZ-POSG and shown that, in each stage, the game can be
converted into a linear-fractional programming problem, and
therefore, solved by linear programming. By enumerating

[19] Abdulmalik Humayed, Jingqiang Lin, Fengjun Li, and Bo Luo. Cyber-
physical systems security - a survey. IEEE Internet of Things Journal,
4(6):1802–1831, 2017.

[20] Akshat Kumar and Shlomo Zilberstein. Dynamic programming
In Pro-
approximations for partially observable stochastic games.
ceedings of the Twenty-Second International FLAIRS Conference, page
547–552, 2009.

[21] KC Lalropuia and Vandana Gupta. Modeling cyber-physical attacks
based on stochastic game and Markov processes. Reliability Engineer-
ing & System Safety, 181:28–37, 2019.

[22] Yi Luo, Ferenc Szidarovszky, Youssif Al-Nashif, and Salim Hariri.
Game tree based partially observable stochastic game model for intru-
sion defense systems (IDS). In IIE Annual Conference. Proceedings,
page 880. IISE, 2009.

[23] Said A Salloum, Muhammad Alshurideh, Ashraf Elnagar, and Khaled
Shaalan. Machine learning and deep learning techniques for cyberse-
curity: a review. In Joint European-US Workshop on Applications of
Invariance in Computer Vision, pages 50–57. Springer, 2020.
[24] Petr Tom´aˇsek, Branislav Boˇsansk`y, and Thanh H Nguyen. Using
one-sided partially observable stochastic games for solving zero-sum
In International Conference
security games with sequential attacks.
on Decision and Game Theory for Security, pages 385–404. Springer,
2020.

[25] Olivier Tsemogne, Yezekael Hayel, Charles Kamhoua, and Gabriel
Deugoue. Partially observable stochastic games for cyber deception
In International Conference on Decision
against network epidemic.
and Game Theory for Security, pages 312–325. Springer, 2020.
[26] Xinrun Wang, Milind Tambe, Branislav Boˇsansk`y, and Bo An. When
players affect target values: Modeling and solving dynamic partially
observable security games. In International Conference on Decision
and Game Theory for Security, pages 542–562. Springer, 2019.
[27] Shmuel Zamir. Bayesian games: Games with incomplete information.

Springer, 2020.

