Accelerating Dynamic Graph
Analytics on GPUs

Technical Report
Version 2.1

Mo Sha, Yuchen Li, Bingsheng He and Kian-Lee Tan

8
1
0
2

n
u
J

7
2

]
S
D
.
s
c
[

2
v
1
6
0
5
0
.
9
0
7
1
:
v
i
X
r
a

July 15, 2017

 
 
 
 
 
 
Contents

1 Introduction

2 Related Work

2.1 Graph Stream Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Graph Analytics on GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Storage Formats on GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 A dynamic framework on GPUs

4 GPMA Dynamic Graph Processing
4.1 GPMA Graph Storage on GPUs
. . . . . . . . . . . . . . . . . . . . . . . .
4.2 Adapting Graph Algorithms to GPMA . . . . . . . . . . . . . . . . . . . . .

1

3
3
3
4

5

6
7
9

5 GPMA+: GPMA Optimization

11
5.1 Bottleneck Analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
5.2 Lock-Free Segment-Oriented Updates . . . . . . . . . . . . . . . . . . . . . . 12

6 Experimental Evaluation

15
6.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
6.2 The Performance of Handling Updates . . . . . . . . . . . . . . . . . . . . . 18
6.3 Application Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.4 Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
6.5 Overall Findings

7 Conclusion & Future Work

22

Appendices

23
A TryInsert+ Optimizations . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Additional Experimental Results For Data Transfer . . . . . . . . . . . . . . 27
B
The Performance of Handling Updates on Sorted Graphs
C
. . . . . . . . . . 27
Additional Experimental Results For Graph Streams with Explicit Deletions 28
D

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Abstract

As graph analytics often involves compute-intensive operations, GPUs have been
extensively used to accelerate the processing. However, in many applications such
as social networks, cyber security, and fraud detection, their representative graphs
evolve frequently and one has to perform a rebuild of the graph structure on GPUs
to incorporate the updates. Hence, rebuilding the graphs becomes the bottleneck of
processing high-speed graph streams. In this paper, we propose a GPU-based dynamic
graph storage scheme to support existing graph algorithms easily. Furthermore, we
propose parallel update algorithms to support eﬃcient stream updates so that the
maintained graph is immediately available for high-speed analytic processing on GPUs.
Our extensive experiments with three streaming applications on large-scale real and
synthetic datasets demonstrate the superior performance of our proposed approach.

1 Introduction

Due to the rising complexity of data generated in the big data era, graph representa-
tions are used ubiquitously. Massive graph processing has emerged as the de facto stan-
dard of analytics on web graphs, social networks (e.g., Facebook and Twitter), sensor
networks (e.g., Internet of Things) and many other application domains which involve
high-dimen-sional data (e.g., recommendation systems). These graphs are often highly
dynamic: network traﬃc data averages 109 packets/hour/router for large ISPs [23]; Twit-
ter has 500 million tweets per day [40]. Since real-time analytics is fast becoming the norm
[26, 12, 35, 42], it is thus critical for operations on dynamic massive graphs to be processed
eﬃciently.

Dynamic graph analytics has a wide range of applications. Twitter can recommend in-
formation based on the up-to-date TunkRank (similar to PageRank) computed based on
a dynamic attention graph [14] and cellular network operators can ﬁx traﬃc hotspots in
their networks as they are detected [27]. To achieve real-time performance, there is a grow-
ing interest to oﬄoad the graph analytics to GPUs due to its much stronger arithmetical
power and higher memory bandwidth compared with CPUs [43]. Although existing solu-
tions, e.g. Medusa [57] and Gunrock [48], have explored GPU graph processing, we are
aware that the only one work [29] has considered a dynamic graph scenario which is a
major gap for running analytics on GPUs. In fact, a delay in updating a dynamic graph
may lead to undesirable consequences. For instance, consider an online travel insurance
system that detects potential frauds by running ring analysis on proﬁle graphs built from
active insurance contracts [5]. Analytics on an outdated proﬁle graph may fail to detect
frauds which can cost millions of dollars. However, updating the graph will be too slow
for issuing contracts and processing claims in real time, which will severely inﬂuence legit-
imate customers’ user experience. This motivates us to develop an update-eﬃcient graph
structure on GPUs to support dynamic graph analytics.

There are two major concerns when designing a GPU-based dynamic graph storage scheme.
First, the proposed storage scheme should handle both insertion and deletion operations
eﬃciently. Though processing updates against insertion-only graph stream could be han-
dled by reserving extra spaces to accommodate the updates, this na¨ıve approach fails to
preserve the locality of the graph entries and cannot support deletions eﬃciently. Consid-
ering a common sliding window model on a graph edge stream, each element in the stream
is an edge in a graph and analytic tasks are performed on the graph induced by all edges
in the up-to-date window [49, 15, 17]. A na¨ıve approach needs to access the entire graph in

1

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

the sliding window to process deletions. This is obviously undesirable against high-speed
streams. Second, the proposed storage scheme should be general enough for supporting
existing graph formats on GPUs so that we can easily reuse existing static GPU graph
processing solutions for graph analytics. Most large graphs are inherently sparse. To max-
imize the eﬃciency, existing works [6, 32, 31, 29, 51] on GPU sparse graph processing rely
on optimized data formats and arrange the graph entries in certain sorted order, e.g. CSR
[32, 6] sorts the entries by their row-column ids. However, to the best of our knowledge,
no schemes on GPUs can support eﬃcient updates and maintain a sorted graph format
at the same time, other than a rebuild. This motivates us to design an update-eﬃcient
sparse graph storage scheme on GPUs while keeping the locality of the graph entries for
processing massive analytics instantly.

In this paper, we introduce a GPU-based dynamic graph analytic framework followed by
proposing the dynamic graph storage scheme on GPUs. Our preliminary study shows that
a cache-oblivious data structure, i.e., Packed Memory Array (PMA [10, 11]), can potentially
be employed for maintaining dynamic graphs on GPUs. PMA, originally designed for
CPUs [10, 11], maintains sorted elements in a partially contiguous fashion by leaving gaps
to accommodate fast updates with a constant bounded gap ratio. The simultaneously
sorted and contiguous characteristic of PMA nicely ﬁts the scenario of GPU streaming
graph maintenance. However, the performance of PMA degrades when updates occur in
locations which are close to each other, due to the unbalanced utilization of reserved spaces.
Furthermore, as streaming updates often come in batches rather than one single update
at a time, PMA does not support parallel insertions and it is non-trivial to apply PMA to
GPUs due to its intricate update patterns which may cause serious thread divergence and
uncoalesced memory access issues on GPUs.

We thus propose two GPU-oriented algorithms, i.e. GPMA and GPMA+, to support eﬃcient
parallel batch updates. GPMA explores a lock-based approach which becomes increasingly
popular due to the recent GPU architectural evolution for supporting atomic operations
[18, 28]. While GPMA works eﬃciently for the case where few concurrent updates con-
ﬂict, e.g., small-size update batches with random updating edges in each batch, there are
scenarios where massive conﬂicts occur and hence, we propose a lock-free approach, i.e.
GPMA+. Intuitively, GPMA+ is a bottom-up approach by prioritizing updates that occur
in similar positions. The update optimizations of our proposed GPMA+ are able to maxi-
mize coalesced memory access and achieve linear performance scaling w.r.t the number of
computation units on GPUs, regardless of the update patterns.

In summary, the key contributions of this paper are the following:

• We introduce a framework for GPU dynamic graph analytics and propose, the ﬁrst of
its kind, a GPU dynamic graph storage scheme to pave the way for real-time dynamic
graph analytics on GPUs.

• We devise two GPU-oriented parallel algorithms: GPMA and GPMA+, to support eﬃcient

updates against high-speed graph streams.

• We conduct extensive experiments to show the performance superiority of GPMA and
GPMA+. In particular, we design diﬀerent update patterns on real and synthetic graph
streams to validate the update eﬃciency of our proposed algorithms against their CPU
counterparts as well as the GPU rebuild baseline. In addition, we implement three real
world graph analytic applications on the graph streams to demonstrate the eﬃciency
and broad applicability of our proposed solutions. In order to support larger graphs, we
extend our proposed formats to multiple GPUs and demonstrate the scalability of our

2

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

approach with multi-GPU systems.

The remainder of this paper is organized as follows. The related work is discussed in
Section 2. Section 3 presents a general workﬂow of dynamic graph processing on GPUs.
Subsequently, we describe GPMA and GPMA+ in Sections 4-5 respectively. Section 6 reports
results of a comprehensive experimental evaluation. We conclude the paper and discuss
some future works in Section 7.

2 Related Work

In this section, we review related works in three diﬀerent categories as follows.

2.1 Graph Stream Processing

Over the last decade, there has been an immense interest in designing eﬃcient algorithms
for processing massive graphs in the data stream model (see [35] for a detailed survey).
This includes the problems of PageRank-styled scores [38], connectivity [21], spanners
[20], counting subgraphs e.g. triangles [46] and summarization [44]. However, these works
mainly focus on the theoretical study to achieve the best approximation solution with linear
bounded space. Our proposed methods can incorporate existing graph stream algorithms
with ease as our storage scheme can support most graph representations used in existing
algorithms.

Many systems have been proposed for streaming data processing, e.g. Storm [45], Spark
Streaming [54], Flink [1]. Attracted by its massively parallel performance, several at-
tempts have successfully demonstrated the advantages of using GPUs to accelerate data
stream processing [47, 56]. However, the aforementioned systems focus on general stream
processing and lack support for graph stream processing. Stinger [19] is a parallel solution
to support dynamic graph analytics on a single machine. More recently, Kineograph [14],
CellIQ [27] and GraphTau [26] are proposed to address the need for general time-evolving
graph processing under the distributed settings. However, to our best knowledge, existing
works focusing on CPU-based time-evolving graph processing will be ineﬃcient on GPUs,
because CPU and GPU are two architectures with diﬀerent design principles and perfor-
mance concerns in the parallel execution. We are aware that only one work [29] explores
the direction of using GPUs to process real-time analytics on dynamic graphs. However,
it only supports insertions and lacks an eﬃcient indexing mechanism.

2.2 Graph Analytics on GPUs

Graph analytic processing is inherently data- and compute-intensive. Massively paral-
lel GPU accelerators are powerful to achieve supreme performance of many applications.
Compared with CPU, which is a general-purpose processor featuring large cache size and
high single core processing capability, GPU devotes most of its die area to a large number
of simple Arithmetic Logic Units (ALUs), and executes code in a SIMT (Single Instruc-
tion Multiple Threads) fashion. With the massive amount of ALUs, GPU oﬀers orders of
magnitude higher computational throughput than CPU in applications with ample paral-
lelism. This leads to a spectrum of works which explore the usage of GPUs to accelerate
graph analytics and demonstrate immense potentials. Examples include breath-ﬁrst search

3

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Figure 1: The dynamic graph analytic framework

(BFS) [32], subgraph query [31], PageRank [6] and many others. The success of deploying
speciﬁc graph algorithms on GPUs motivates the design of general GPU graph processing
systems like Medusa [57] and Gunrock [48]. However, the aforementioned GPU-oriented
graph algorithms and systems assume static graphs. To handle dynamic graph scenario,
existing works have to perform a rebuild on GPUs against each single update. DCSR [29]
is the only solution, to the best of our knowledge, which is designed for insertion-only
scenarios as it is based on linked edge block and rear appending technique. However, it
does not support deletions or eﬃcient searches. We propose GPMA to enable eﬃcient dy-
namic graph updates (i.e. insertions and deletions) on GPUs in a ﬁne-grained manner. In
addition, existing GPU-optimized graph analytics and systems can replace their storage
layers directly with ease since the fundamental graph storage schemes used in existing
works can be directly implemented on top of our proposed storage scheme.

2.3 Storage Formats on GPUs

Sparse matrix representation is a popular choice for storing large graphs on GPUs [3, 2, 57,
48] The Coordinate Format [16] (COO) is the simplest format which only stores non-zero
matrix entries by their coordinates with values. COO sorts all the non-zero entries by the
entries’ row-column key for fast entry accesses. CSR [32, 6] compresses COO’s row indices
into an oﬀset array to reduce the memory bandwidth when accessing the sparse matrix. To
optimize matrices with diﬀerent non-zero distribution patterns, many customized storage
formats were proposed, e.g., Block COO [50] (BCCOO), Blocked Row-Column [7] (BRC)
and Tiled COO [52] (TCOO). Existing formats require to maintain a certain sorted order
of their storage base units according to the unit’s position in the matrix, e.g. entries for
COO and blocks for BCCOO, and still ensure the locality of the units. As mentioned
previously, few prior schemes can handle eﬃcient sparse matrix updates on GPUs. To
the best of our knowledge, PMA [10, 11] is a common structure which maintains a sorted
array in a contiguous manner and supports eﬃcient insertions/deletions. However, PMA is
designed for CPU and no concurrent updating algorithm is ever proposed. Thus, we are
motivated to propose GPMA and GPMA+ for supporting eﬃcient concurrent updates on all
existing storage formats.

4

3/23/20171CPUGPUActive Graph StructureGraph UpdateGraph AnalyticsGraph StreamStreaming ApplicationsGraph Stream BufferDynamic Query BufferContinuousMonitoringAccelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Figure 2: Asynchronous streams

3 A dynamic framework on GPUs

To address the need for real-time dynamic graph analytics, we oﬄoad the tasks of concur-
rent dynamic graph maintenance and its corresponding analytic processing to GPUs. In
this section, we introduce a general GPU dynamic graph analytic framework. The design
of the framework takes into account two major concerns: the framework should not only
handle graph updates eﬃciently but also support existing GPU-oriented graph analytic
algorithms without forfeiting their performance.

Model: We adopt a common sliding window graph stream model [35, 27, 44]. The sliding
1 which indicates
window model consists of an unbounded sequence of elements (u, v)t
the edge (u, v) arrives at time t, and a sliding window which keeps track of the most
recent edges. As the sliding window moves with time, new edges in the stream keep being
inserted into the window and expiring edges are deleted. In real world applications, the
sliding window of a graph stream can be used to monitor and analyze fresh social actions
that appearing on Twitter [49] or the call graph formed by the most recent CDR data
[27]. In this paper, we focus on presenting how to handle edge streams but our proposed
scheme can also handle the dynamic hyper graph scenario with hyper edge streams.

Apart from the sliding window model, the graph stream model which involves explicit
insertions and deletions (e.g., a user requests to add or delete a friend in the social net-
work) is also supported by our scheme as the proposed dynamic graph storage structure
is designed to handle random update operations. That is, our system supports two kinds
of updates, implicit ones generated from the sliding window mechanism and explicit ones
generated from upper level applications or users.

The overview of the dynamic graph analytic framework is presented in Figure 1. Given
a graph stream, there are two types of streaming tasks supported by our framework.
The ﬁrst type is the ad-hoc queries such as neighborhood and reachability queries on the
graph which is constantly changing. The second type is the monitoring tasks like tracking
PageRank scores. We present the framework by illustrating how to handle the graph
streams and the corresponding queries while hiding data transfer between CPU and GPU,
as follows:

Graph Streams: The graph stream buﬀer module batches the incoming graph streams
on the CPU side (host) and periodically sends the updating batches to the graph update
module located on GPU (device). The graph update module updates the “active” graph
stored on the device by using the batch received. The “active” graph is stored in the format
of our proposed GPU dynamic graph storage structure. The details of the graph storage

1Our framework handles both directed and undirected edges.

5

Graph	Stream	Transfer	(host	to	device)Active	Graph	Update(Summarization	&	Update)Query	Transfer	(host	to	device)Graph	Stream	Transfer	(host	to	device)Graph	Analytic	ProcessingActive	Graph	Update(Summarization	&	Update)Graph	StreamResults	Transfer	(device	to	host)Query	Transfer	(host	to	device)Results	Transfer	(device	to	host)QueryStreamStep	1Step	2Step	3Repeat…Data	transfer	on	PCIeGPU	computationAccelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

segment size
density lower bound ρ
density upper bound τ
min # of entries
max # of entries

Leaf
4
0.08
0.92
1
3

Level 1
8
0.19
0.88
2
6

Level 2
16
0.29
0.84
4
12

Level 3
32
0.40
0.80
8
24

Figure 3: PMA insertion example (Left: PMA for insertion; Right: predeﬁned thresholds)

structure and how to update the graph eﬃciently on GPUs will be discussed extensively
in later sections.

Queries: Like the graph stream buﬀer, the dynamic query buﬀer module batches ad-
hoc queries submitted against the stored active graph, e.g., queries to check the dynamic
reachability between pairs of vertices. The tracking tasks will also be registered in the
continuous monitoring module, e.g., tracking up-to-date PageRank. All ad-hoc queries
and monitoring tasks will be transferred to the graph analytic module for GPU accelerated
processing. The analytic module interacts with the active graph to process the queries and
the tracking tasks. Subsequently, the query results will be transferred back to the host. As
most existing GPU graph algorithms use optimized array formats like CSR to accelerate
the performance [18, 28, 34, 52], our proposed storage scheme provides an interface for
storing the array formats.
In this way, existing algorithms can be integrated into the
analytic module with ease. We describe the details of the integration in Section 4.2.

Hiding Costly PCIe Transfer: Another critical issue on designing GPU-oriented sys-
tems is to minimize the data transfer between the host and the device through PCIe. Our
proposed batching approach allows overlapping data transfer by concurrently running an-
alytic tasks on the device. Figure 2 shows a simpliﬁed schedule with two asynchronous
streams: graph streams and query streams respectively. The system is initialized at Step 1
where the batch containing incoming graph stream elements is sent to the device. At Step
2, while PCIe handles bidirectional data transfer for previous query results (device to host)
and freshly submitted query batch (host to device), the graph update module updates the
active graph stored on the device. At Step 3, the analytic module processes the received
query batch on the device and a new graph stream batch is concurrently transferred from
the host to the device. It is clear to see that, by repeating the aforementioned process, all
data transfers are overlapped with concurrent device computations.

4 GPMA Dynamic Graph Processing

To support dynamic graph analytics on GPUs, there are two major challenges discussed
in the introduction. The ﬁrst challenge is to maintain the dynamic graph storage in the
device memory of GPUs for eﬃcient update as well as compute. The second challenge is
that the storage strategy should show its good compatibility with existing graph analytic
algorithms on GPUs.

In this section, we discuss how to address the challenges with our proposed scheme. First,
we introduce GPMA for GPU resident graph storage to simultaneously achieve update and
compute eﬃciency (Section 4.1). Subsequently, we illustrate GPMA’s generality in terms of
deploying existing GPU based graph analytic algorithms (Section 4.2).

6

[0,15][16,19][0,31][12,15][24,27][8,11][24,31][4,7][0,7][0,3][16,23][20,23][8,15][28,31][16,31]2581316172327283134374246485162258131617232728313437424651624825813161723272831343742465162OriginalInsertedFinalLeafLevel	1Level	2Level	3non-zeroentrybalancedrebalancedunbalancedAccelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

4.1 GPMA Graph Storage on GPUs

In this subsection, we ﬁrst discuss the design principles our proposed dynamic graph
storage should follow. Then we introduce how to implement our proposal.

Design Principles. The proposed graph storage on GPUs should take into account the
following principles:

• The proposed dynamic graph storage should eﬃciently support a broad range of updat-
ing operations, including insertions, deletions and modiﬁcations. Furthermore, it should
have a good locality to accommodate the highly parallel memory access characteristic
of GPUs, in order to achieve high memory eﬃciency.

• The physical storage strategy should support common logical storage formats and the
existing graph analytic solutions on GPUs based on such formats can be adapted easily.

Background of PMA. GPMA is primarily motivated by a novel structure, Packed Mem-
ory Array (PMA [10, 11]), which is proposed to maintain sorted elements in a partially
continuous fashion by leaving gaps to accommodate fast updates with a bounded gap
ratio. PMA is a self-balancing binary tree structure. Given an array of N entries, PMA
separates the whole memory space into leaf segments with O(log N ) length and deﬁnes
non-leaf segments as the space occupied by their descendant segments. For any segment
located at height i (leaf height is 0), PMA designs a way to assign the lower and upper
bound density thresholds for the segment as ρi and τi respectively to achieve O(log2 N )
amortized update complexity. Once an insertion/deletion causes the density of a segment
to fall out of the range deﬁned by (ρi, τi), PMA tries to adjust the density by re-allocating
all elements stored in the segment’s parent. The adjustment process is invoked recursively
and will only be terminated if all segments’ densities fall back into the range deﬁned by
PMA’s density thresholds. For an ordered array, modiﬁcations are trivial. Therefore, we
mainly discuss insertions because deletions are the dual operation of insertions in PMA.
Example 1. Figures 3 presents an example for PMA insertion. Each segment is uniquely
identiﬁed by an interval (starting and ending position of the array) displayed in the corre-
sponding tree node, e.g., the root segment is segment-[0,31] as it covers all 32 spaces. All
values stored in PMA are displayed in the array. The table in the ﬁgure shows predeﬁned
parameters including the segment size, the assignment of density thresholds (ρi, τi) and
the corresponding minimum and maximum entry sizes at diﬀerent heights of the tree. We
use these setups as a running example throughout the paper. To insert an entry. i.e. 48,
into PMA, the corresponding leaf segment is ﬁrstly identiﬁed by a binary search, and the
new entry is placed at the rear of leaf segment. The insertion causes the density of the
leaf segment 4 to exceed the threshold 3. Thus, we need to identify the nearest ancestor
segment which can accommodate the insertion without violating the thresholds, i.e., the
segment-[16,31]. Finally, the insertion is completed by re-distpatching all entries evenly
in segment-[16,31].
Lemma 1 ([10, 11]). The amortized update complexity of PMA is proved to be O(log2 N )
in the worst case and O(log N ) in the average case.

It is evident that PMA could be employed for dynamic graph maintenance as it maintains
sorted elements eﬃciently with high locality on CPU. However, the update procedure de-
scribed in [11] is inherently sequential and no concurrent algorithms have been proposed.
To support batch updates of edge insertions and deletions for eﬃcient graph stream ana-
lytic processing, we devise GPMA to support concurrent PMA updates on GPUs. Note that
we focus on the insertion process for a concise presentation because the deletion process

7

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Figure 4: GPMA concurrent insertions

is a dual process w.r.t. the insertion process in PMA.

Concurrent Insertions in GPMA. Motivated by PMA on CPUs, we propose GPMA to
handle a batch of insertions concurrently on GPUs. Intuitively, GPMA assigns an insertion
to a thread and concurrently executes PMA algorithm for each thread with a lock-based
approach to ensure consistency. More speciﬁcally, all leaf segments of insertions are iden-
tiﬁed in advance, and then each thread checks whether the inserted segments still satisfy
their thresholds from bottom to top. For each particular segment, it is accessed in a mutu-
ally exclusive fashion. Moreover, all threads are synchronized after updating all segments
located at the same tree height to avoid possible conﬂicts as segments at a lower height
are fully contained in the segments at a higher level.

Algorithm 1 presents the pseudocode for GPMA concurrent insertions. We highlight the
lines added to the original PMA update algorithm in order to achieve concurrent update
of GPMA. As shown in line 2, all entries in the insertion set are iteratively tried until all
of them take eﬀect. For each iteration shown in line 9, all threads start at leaf segments
and attempt the insertions in a bottom-up fashion. If a particular thread fails the mutex
competition in line 11, it aborts immediately and waits for the next attempt. Otherwise,
it inspects the density of the current segment. If the current segment does not satisfy the
density requirement, it will try the parent segment in the next loop iteration (lines 13-14).
Once an ancestor segment is able to accommodate the insertion, it merges the new entry
in line 16 and the entry is removed from the insertion set. Subsequently, the updated
segment will re-dispatch all its entries evenly and the process is terminated.
Example 2. Figure 4 illustrates an example with ﬁve insertions, i.e. {1, 4, 9, 35, 48}, for
concurrent GPMA insertion. The initial structure is the same as in Example 1. After identi-
fying the leaf segment for insertion, threads responsible for Insertion-1 and Insertion-4
compete for the same leaf segment. Assuming Insertion-1 succeeds in getting the mu-
tex, Insertion-4 is aborted. Due to enough free space of the segment, Insertion-1 is
successfully inserted. Even though there is no leaf segment competition for Insertions-
9,35,48, they should continue to inspect the corresponding parent segments because all
the leaf segments do not satisfy the density requirement after the insertions. Insertions-
35,48 still compete for the same level-1 segment and Insertion-48 wins. For this ex-
ample, three of the insertions are successful and the results are shown in the bottom of
Figure 4. Insertions-4,35 are aborted in this iteration and will wait for the next at-
tempt.

8

[0,15][16,19][0,31][12,15][24,27][8,11][24,31][4,7][0,7][0,3][16,23][20,23][8,15][28,31][16,31]258131617232728313437424651621258913161723272831343742464851621493548…………Thread	PoolInsertion	BufferLevel	2Level	1LeafLevel	3OriginalRound1non-zeroentrybalancedrebalancedbalancedunbalancedtrylockfailedrebalancedAccelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

3:

4:

5:

6:

7:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

Algorithm 1 GPMA Concurrent Insertion
1: procedure GPMAInsert(Insertions I)
2:

while I is not empty do
parallel for i in I

Seg s ← BinarySearchLeafSegment(i)
TryInsert(s, i, I)

synchronize
release locks on all segments

8: function TryInsert(Seg s, Insertion i, Insertions I)
9:

while s (cid:54)= root do
synchronize
if fails to lock s then

return

if (|s| + 1)/capacity(s) < τ then
s ← parent segment of s

else

Merge(s, i)
re-dispatch entries in s evenly
remove i from I
return

double the space of the root segment

(cid:46) insertion aborts

(cid:46) insertion succeeds

4.2 Adapting Graph Algorithms to GPMA

Existing graph algorithms often use sparse matrix format to store the graph entries since
most large graphs are naturally sparse[5]. Although many diﬀerent sparse storage formats
have been proposed, most of the formats assume a speciﬁc order to organize the non-
zero entries. These formats enforce the order of the graph entries to optimize their speciﬁc
access patterns, e.g., row-oriented (COO2), diagonal-oriented (JAD), and block-/tile-based
(BCCOO, BRC and TCOO). It is natural that the ordered graph entries can be projected
into an array and these similar formats can be supported by GPMA easily. Among all
formats, we choose CSR as an example to illustrate how to adapt the format to GPMA.

CSR as a case study. CSR is most widely used by existing algorithms on sparse matrices
or graphs. CSR compresses COO’s row indices into an oﬀset array, which contributes to
reducing the memory bandwidth when accessing the sparse matrix, and achieves a better
workload estimation for skewed graph distribution (e.g., power-law distribution). The
following example demonstrates how to implement CSR on GPMA.
Example 3. In Figure 5, we have a graph of three vertices and six edges. The number
on each edge denotes the weight of the corresponding edge. The graph is represented as
a sparse matrix and is further transformed to the CSR format shown in the upper right.
CSR sorts all non-zero entries in the row-orient order, and compresses row indices into
intervals as a row oﬀset array. The lower part denotes the GPMA representation of this

2Generally, COO means ordered COO and it can also be column-oriented.

9

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Algorithm 2 Breadth-First Search
1: procedure BFS(Graph G, Vertex s)
for each vertex u ∈ G.V − {s} do
2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

u.visited = false

Q ← φ
s.visited ← true
ENQUEUE(Q, s)
while Q (cid:54)= φ do

u ← DEQUEUE(Q)
for each v ∈ G.Adj[u] do

if IsEntryExist(v) then

if v.visited = false then
v.visited ← true
ENQUEUE(v)

Algorithm 3 GPU-based BFS Neighbour Gathering
1: procedure Gather(Vertex frontier, Int csrOﬀset)
{r, rEnd} ← csrOﬀset[frontier, frontier + 1]
2:
for (i ← r+threadId; i<rEnd; i+=threadNum) do
if IsEntryExist(i) then ParallelGather(i)

3:

4:

graph. In order to maintain the row oﬀset array without synchronization among threads,
we add a guard entry whose column index is ∞ during concurrent insertions. That is to
say, when the guard is moved, the corresponding element in row oﬀset array will change.

Given a graph stored on GPMA, the next step is to adapt existing graph algorithms to
GPMA. In particular, how existing algorithms access the graph entries stored on GPMA
is of vital importance. As for the CSR example, most algorithms access the entries by
navigating through CSR’s ordered array[18, 28, 34, 52]. We note that a CSR stored on
GPMA is also an array which has bounded gaps interleaved with the graph entries. Thus,
we are able to eﬃciently replace the operations of arrays with the operations of GPMA. We
will demonstrate how we can do this replacement as follows.

Algorithm 2 illustrates the pseudocode of the classic BFS algorithm. We should pay
attention to line 10, which is highlighted. Compared with the raw adjacency list, the
applications based on GPMA need to guarantee the current vertex being traversed is a valid
neighbour instead of an invalid space in GPMA’s gap.

Algorithm 2 provides a high-level view for GPMA adaption. Furthermore, we present how it
adapts GPMA in the parallel GPU environment with some low-level details. Algorithm 3 is
the pseudocode of the Neighbour Gathering parallel procedure, which is a general primitive
for most GPU-based vertex-centric graph processing models [36, 18, 22]. This primitive
plays a role similar to line 10 of Algorithm 2 but in a parallel fashion in accessing the
neighbors of a particular vertex. When traversing all neighbours of frontiers, Neighbour
Gathering follows the SIMT manner, which means that there are threadNum threads as a
group assigned to one of the vertex frontier and the procedure in Algorithm 3 is executed

10

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Row Oﬀset
Column Index
Value

[0 2 3 6]
[0 2 2 0 1 2]
[1 2 3 4 5 6]

Example Graph

CSR Format

Figure 5: GPMA based on CSR

in parallel. For the index range (in the CSR on GPMA) of the current frontier given by
csrOﬀset (shown in line 2), each thread will handle the corresponding tasks according to
its threadId. For GPU-based BFS, the visited labels of neighbours for all frontiers will not
be judged immediately after neighbours are accessed. Instead, they will be compacted to
contiguous memory in advance for higher memory eﬃciency.

Similarly, we can also carry out the entry existing checking for other graph applications
to adapt them to GPMA. To summarize, GPMA can be adapted to common graph analytic
applications which are implemented in diﬀerent representation and execution models, in-
cluding matrix-based (e.g., PageRank), vertex-centric (e.g., BFS) and edge-centric (e.g.,
Connected Component).

5 GPMA+: GPMA Optimization

Although GPMA can support concurrent graph updates on GPUs, the update algorithm is
basically a lock-based approach and can suﬀer from serious performance issue when diﬀer-
ent threads compete for the same lock. In this section, we propose a lock-free approach,
i.e. GPMA+, which makes full utilization of GPU’s massive multiprocessors. We carefully
examine the performance bottleneck of GPMA in Section 5.1. Based on the issues identiﬁed,
we propose GPMA+ for optimizing concurrent GPU updates with a lock-free approach in
Section 5.2.

5.1 Bottleneck Analysis

The following four critical performance issues are identiﬁed for GPMA:

• Uncoalesced Memory Accesses: Each thread has to traverse the tree from the root
segment to identify the corresponding leaf segment to be updated. For a group of GPU
threads which share the same memory controller (including access pipelines and caches),
memory accesses are uncoalesced and thus, cause additional IO overheads.

• Atomic Operations for Acquiring Lock: Each thread needs to acquire the lock
before it can perform the update. Frequently invoking atomic operations for acquiring
locks will bring huge overheads, especially for GPUs.

• Possible Thread Conﬂicts: When two threads conﬂict on a segment, one of them
has to abort and wait for the next attempt. In the case where the updates occur on
segments which are located proximately, GPMA will end up with low parallelism. As
most real world large graphs have the power law property, the eﬀect of thread conﬂicts
can be exacerbated.

• Unpredictable Thread Workload: Workload balancing is another major concern
for optimizing concurrent algorithms [43]. The workload for each thread in GPMA is
unpredictable because: (1) It is impossible to obtain the last non-leaf segment traversed

11

021615342(0,0) (0,2) (0,∞) (1,2) (1,∞) (2,0) (2,1)(2,2)(2,∞) 123456[12,15][8,15][8,11][0,15][4,7][0,7][0,3]4814Row	OffsetAccelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Figure 6: GPMA+ concurrent insertions (best viewed in color)

by each thread in advance; (2) The result of lock competition is random. The unpre-
dictable nature triggers the imbalanced workload issue for GPMA. In addition, threads
are grouped as warps on GPUs. If a thread has a heavy workload, the remaining threads
of the same warp are idle and cannot be re-scheduled.

5.2 Lock-Free Segment-Oriented Updates

Based on the discussion above, we propose GPMA+ to lift all bottlenecks identiﬁed. The
proposed GPMA+ does not rely on lock mechanism and achieves high thread utilization
simultaneously. Existing graph algorithms can be adapted to GPMA+ in the same manner
as GPMA.

Compared with GPMA, which handles each update separately, GPMA+ concurrently pro-
cesses updates based on the segments involved.
It breaks the complex update pattern
into existing concurrent GPU primitives to achieve maximum parallelism. There are three
major components in the GPMA+ update algorithm:

(1) The updates are ﬁrst sorted by their keys and then dispatched to GPU threads for

locating their corresponding leaf segments according to the sorted order.

(2) The updates belonging to the same leaf segment are grouped for processing and GPMA+

processes the updates level by level in a bottom-up manner.

(3) In any particular level, we leverage GPU primitives to invoke all computing resources

for segment updates.

We note that, the issue of uncoalesced memory access in GPMA is resolved by compo-
nent (1) as the updating threads are sorted in advance to achieve similar traversal paths.
Component (2) completely avoids the use of locks, which solves the problem of atomic
operations and thread conﬂicts. Finally, component (3) makes use of GPU primitives to
achieve workload balancing among all GPU threads.

We present the pseudocode for GPMA+’s segment-oriented insertion in the procedure
GpmaPlusInsertion of Algorithm 4. Note that, similar to Section 4 (GPMA), we focus
on presenting the insertions for GPMA+ and the deletions could be naturally inferred. The
inserting entries are ﬁrst sorted by their keys in line 2 and the corresponding segments are
then identiﬁed in line 3. Given the update set U , GPMA+ processes updating segments level
by level in lines 4-15 until all updates are executed successfully (line 11). In each iteration,
UniqueInsertion in line 7 groups update entries belonging to the same segments into
unique segments, i.e., S∗, and produces the corresponding index set I for quick accesses of
updates entries located in a segment from S∗. As shown in lines 19-20, UniqueSegments
only utilizes standard GPU primitives, i.e. RunLenghtEncoding and ExclusiveScan.
RunLenghtEncoding compresses an input array by merging runs of an element into a

12

258131617232728313437424651621493548Update	Segments:Update	Offsets:Update	Keys:02345042428Successful	Flag:YNNN1245813161723272831343742465162124589131617232728313435374246485162OriginalRound	1Round	2Original93548Update	Segments:Update	Offsets:Update	Keys:013024Successful	Flag:NNRound	193548Update	Segments:Update	Offsets:Update	Keys:013016Successful	Flag:YYRound	2[0,15][16,19][0,31][12,15][24,27][8,11][24,31][4,7][0,7][0,3][16,23][20,23][8,15][28,31][16,31]LeafLevel	1Level	2Level	3non-zeroentrybalancedrebalancedAccelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Algorithm 4 GPMA+ Segment-Oriented Insertion
1: procedure GpmaPlusInsertion(Updates U )
2:

Sort(U )
Segs S ← BinarySearchLeafSegments(U )
while root segment is not reached do

Indices I ← ∅
Segs S∗ ← ∅
(S∗, I) ← UniqueSegments(S)
parallel for s ∈ S∗

TryInsert+(s, I, U )

if U = ∅ then
return

parallel for s ∈ S

if s does not contain any update then

remove s from S
s ← parent segment of s

r ← double the space of the old root segment
TryInsert+(r, ∅, U )

18: function UniqueSegments(Segs S)
19:

(S∗, Counts) ← RunLengthEncoding(S)
Indices I ← ExclusiveScan(Counts)
return (S∗, I)

20:

21:

22: function TryInsert+(Seg s, Indices I, Updates U )
23:

ns ← CountSegment(s)
Us ← CountUpdatesInSegment(s,I,U )
if (ns + |Us|)/capacity(s) < τ then

Merge(s, Us)
re-dispatch entries in s evenly
remove Us from U

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

24:

25:

26:

27:

28:

single element. It also outputs a count array denoting the length of each run. ExclusiveS-
can calculates, for each entry e in an array, the sum of all entries before e. Both primitives
have very eﬃcient parallelized GPU-based implementation which makes full utilization of
the massive GPU cores. In our implementation, we use the NVIDIA CUB library [4] for
these primitives. Given a set of unique updating segments, TryInsert+ ﬁrst checks if a
segment s has enough space for accommodating the updates by summing the valid entries
in s (CountSegment) and the number of updates in s (CountUpdatesInSegment).
If the density threshold is satisﬁed, the updates will be materialized by merging the in-
serting entries with existing entries in the segment (as shown in line 26). Subsequently, all
entries in the segment will be re-dispatched to balance the densities. After TryInsert+,
the algorithm will terminate if there are no entries to be updated. Otherwise, GPMA+
will advance to higher levels by setting all remaining segments to their parent segments
(lines 12-15). The following example illustrates GPMA+’s segment-oriented updates.
Example 4. Figure 6 illustrates an example for GPMA+ insertions with the same setup as

13

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

in example 2. The left part is GPMA+’s snapshots in diﬀerent rounds during this batch of
insertions. The right part denotes the corresponding array information after the execution
of each round. Five insertions are grouped into four corresponding leaf segments (denoted
in diﬀerent colours).

For the ﬁrst iteration at the leaf level, Insertions-1,4 of the ﬁrst segment (denoted as
red) is merged into the corresponding leaf segment, then its success ﬂag is marked and will
not be considered in the next round. The remaining intervals fail in this iteration and their
corresponding segments will upgrade to their parent segments. It should be noted that the
blue and the green grids belong to the same parent segment and therefore, will be merged
and then dispatched to their shared parent segment (as shown in Round1). In this round,
both segments (denoted as yellow and blue) cannot satisfy the density threshold, and their
successful ﬂags are not checked. In Round2, both update segments can be merged by the
corresponding insertions and no update segments will be considered in the next round since
all of them are ﬂagged.

In Algorithm 4, TryInsert+ is the most important function as it handles all the cor-
responding insertions with no conﬂicts. Moreover, it achieves a balanced workload for
each concurrent task. This is because GPMA+ handles the updates level by level and each
segment to be updated in a particular level has exactly the same capacity. However, seg-
ments in diﬀerent levels have diﬀerent capacities. Intuitively, the probability of updating
a segment with a larger size (a segment closer to the root) is much lower than that of
a segment with a smaller size (a segment closer to the leaf). To optimize towards the
GPU architecture, we propose the following optimization strategies for TryInsert+ for
segments with diﬀerent sizes.

• Warp-Based: For a segment with entries not larger than the warp size, the segment
will be handled by a warp. Since all threads in the same warp are tied together and
warp-based data is held by registers, updating a segment by a warp does not require
explicit synchronization and will obtain superior eﬃciency.

• Block-Based: For a segment of which the data can be loaded in GPU’s shared memory,
block-based approach is chosen. Block-based approach executes all updates in the shared
memory. As shared memory has much larger size than warp registers, block-based
approach can handle large segments eﬃciently.

• Device-Based: For a segment with the size larger than the size of the shared memory,
we handle them via global memory and rely on kernel synchronization. Device-based
approach is slower than the two approaches above, but it has much less restriction on
memory size (less than device memory amount) and is not invoked frequently.

We refer interested readers to Appendix A for the detailed algorithm of the optimizations
above.
Theorem 1. Given there are K computation units in the GPU, the amortized update
performance of GPMA+ is O(1 + log2N
K ), where N is the maximum number of edges in the
dynamic graph.

Proof. Let X denote the set of updating entries contained in a batch. We consider the
case where |X| ≥ K as it is rare to see |X| < K in real world scenarios.
In fact, our
analysis works for cases where |X| = O(K). The total update complexity consists of three
parts: (1) sorting the updating entries; (2) searching the position of the entries in GPMA;
(3) inserting the entries. We study these three parts separately.

For part (1), the sorting complexity of |X| entries on the GPU is O( |X|

K ) since parallel radix

14

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Table 1: Experimented Graph Algorithms and the Compared Approaches

Compared Approaches Graph Container

BFS

ConnectedComponent

PageRank

CPU Approaches

GPU Approaches

AdjLists
PMA [10, 11]
Stinger [19]
cuSparseCSR [3]
GPMA/GPMA+

Standard Single Thread Algorithms

Stinger built-in Parallel Algorithms

D. Merrill et al.[36]

J. Soman et al.[41]

CUSP SpMV [2]

sort is used (keys in GPMA are integers for storing edges). Then, the amortized sorting
complexity is O( |X|

K )/|X| = O(1).

For part (2), the complexity of concurrently searching |X| entries on GPMA is O( |X|·logN
)
since each entry is assigned to one thread and the depth of traversal is the same for
one thread (GPMA is a balanced tree). Thus, the amortized searching complexity is
O( |X|·logN

)/|X| = O( logN

K

K ).

K

For part (3), we need to conduct a slightly complicated analysis. We denote the total
insertion complexity of X with GPMA+ as cX
GPMA+. As GPMA+ is updated level by level,
cX
GPMA+ can be decomposed into: cX
GPMA+ = c0 + c1 + ... + ch where h is the height of the
PMA tree. Given any level i, let zi denote the number of segments to be updated by
GPMA+. Since all segments at level i have the same size, we denote pi as the sequential
complexity to update any segment si,j at level i (TryInsert+ in Algorithm 4). GPMA+
evenly distributes the computing resources to each segment. As processing each segment
only requires a constant number of scans on the segment by GPU primitives, the complexity
for GPMA+ to process level i is ci = pi·zi

K . Thus we have:

cX
GPMA+ =

(cid:88)

i=0,..,h

pi · zi
K

≤

1
K

(cid:88)

x∈X

cx
PMA

where cx
PMA is the sequential complexity for PMA to process the update of a particular
entry x ∈ X. The inequality holds because for each segment updated by GPMA+, it
must be updated at least once by a sequential PMA process. With Lemma 1, we have
GPMA+ = O( |X|·log2N
cx
PMA = O(log2N ) and thus cX
Then the amortized complexity to
update one single entry under the GPMA scheme naturally follows as O(1 + log2N

K ).
Finally, we conclude the proof by combining the complexities from all three parts.

).

K

Theorem 1 proves that the speedups of GPMA+ over sequential PMA is linear to the num-
ber of processing units available on GPUs, which showcases the theoretical scalability of
GPMA+.

6 Experimental Evaluation

In this section, we present the experimental evaluation of our proposed methods. First,
we present the setup of the experiments. Second, we examine the update costs of dif-
ferent schemes for maintaining dynamic graphs. Finally, we implement three diﬀerent
applications to show the performance and the scalability of the proposed solutions.

15

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Figure 7: Performance comparison for updates with diﬀerent batch sizes. The dashed lines
represent CPU-based solutions whereas the solid lines represent GPU-based solutions.

Table 2: Statistics of Datasets

Datasets
Reddit
Pokec

|E|/|V |

|V |

|E|
2.61M 34.4M 13.2
1.60M 30.6M 19.1
200
200

|Es|
17.2M
15.3M
100M
100M

|Es|/|V |
6.6
9.6
100
100

Graph500 1.00M 200M
1.00M 200M

Random

6.1 Experimental Setup

Datasets. We collect two real world graphs (Reddit and Pokec) and synthesize two
random graphs (Random and Graph500) to test the proposed methods. The datasets are
described as follows and their statistics are summarized in Table 2.

• Reddit is an online forum where user actions include post and comment. We collect
all comment actions from a public resource3. Each comment of a user b to a post from
another user a is associated with an edge from a to b, and the edge indicates an action
of a has triggered an action of b. As each comment is labeled with a timestamp, it
naturally forms a dynamic inﬂuence graph.

• Pokec is the most popular online social network in Slovakia. We retrieve the dataset
from SNAP [30]. Unlike other online datasets, Pokec contains the whole network over
a span of more than 10 years. Each edge corresponds to a friendship between two users.
• Graph500 is a synthetic dataset obtained by using the Graph500 RMAT generator [37]

to synthesize a large power law graph.

• Random is a random graph generated by the Erd˝os-Renyi model. Speciﬁcally, given
a graph with n vertices, the random graph is generated by including each edge with
probability p. In our experiments, we generate a Erd˝os-Renyi random graph with 0.02%
of non-zero entries against a full clique.

Stream Setup. In our datasets, Reddit has a timestamp on every edge whereas the other
datasets do not possess timestamps. As commonly used in existing graph stream algo-
rithms [55, 53, 38], we randomly set the timestamps of all edges in the Pokec, Graph500
and Random datasets. Then, the graph stream of each dataset receives the edges with
increasing timestamps.

For each dataset, a dynamic graph stream is initialized with a subgraph consisting of
the dataset’s ﬁrst half of its total edges according to the timestamps, i.e., Es in Table 2
denotes the initial edge set of a dynamic graph before the stream starts. To demonstrate
the update performance of both insertions and deletions, we adopt a sliding window setup
where the window contains a ﬁxed number of edges. Whenever the window slides, we need

3https://www.kaggle.com/reddit/reddit-comments-may-2015

16

101103105Sliding Size0100101102103104Time (ms)Uniform Random101103105Sliding Size0100101102103104Time (ms)Graph500101103105Sliding Size0100101102103104Time (ms)Reddit101103105Sliding Size0100101102103104Time (ms)PokecAdjListsPMAStingercuSparseCSRGPMAGPMA+Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

to update the graph by deleting expired edges and inserting arrived edges until there are
no new edges left in the stream.

Applications. We conduct experiments on three most widely used graph applications to
showcase the applicability and the eﬃciency of GPMA+.

• BFS is a key graph operation which is extensively studied in previous works on GPU
graph processing [24, 33, 13]. It begins with a given vertex (or root) of an unweighted
graph and iteratively explores all connected vertices. The algorithm will assign a mini-
mum distance away from the root vertex to every visited vertex after it terminates. In
the streaming scenario, after each graph update, we select a random root vertex and
perform BFS from the root to explore the entire graph.

• Connected Component is another fundamental algorithm which has been extensively
studied under both CPU [25] and GPU [41] µ environment. It partitions the graph in the
way that all vertices in a partition can reach the others in the same partition and cannot
reach vertices from other partitions. In the streaming context, after each graph update,
we run the ConnectedComponent algorithm to maintain the up-to-date partitions.
• PageRank is another popular benchmarking application for large scale graph process-
ing. Power iteration method is a standard method to evaluate the PageRank where the
Sparse Matrix Vector Multiplication (SpMV) kernel is recursively executed between the
graph’s adjacency matrix and the PageRank vector. In the streaming scenario, when-
ever the graph is updated, the power iteration is invoked and it obtains the up-to-date
PageRank vector by operating on the updated graph adjacency matrix and the PageR-
ank vector obtained in the previous iteration. In our experiments, we follow the standard
setup by setting the damping factor to 0.85 and we terminate the power iteration once
the 1-norm error is less than 10−3.

These three applications have diﬀerent memory and computation requirements. BFS re-
quires little computation but performs frequent random memory accesses, and PageRank
using SpMV accesses the memory sequentially and it is the most compute-intensive task
among all three applications.

Maintaining Dynamic Graph. We adopt the CSR [32, 6] format to represent the dy-
namic graph maintained. Note that all approaches proposed in the paper are not restricted
to CSR but general enough to incorporate any popular representation formats like COO
[16], JAD [39], HYB [9, 34] and many others. To evaluate the update performance of our
proposed methods, we compare diﬀerent graph data structures and respective approaches
on both CPUs and GPUs.

• AdjLists (CPU). AdjLists is a basic approach for CSR graph representation. As
the CSR format sorts all entries according to their row-column indices, we implement
AdjLists with a vector of |V | entries for |V | vertices and each entry is a RB-Tree
to denote all (out)neighbors of each vertex. The insertions/deletions are operated by
TreeSet insertions/deletions.

• PMA (CPU). We implement the original CPU-based PMA and adopt it for the CSR

format. The insertions/deletions are operated by PMA insertions/deletions.

• Stinger (CPU). We compare the graph container structure used in the state-of-the-art
CPU-based parallel dynamic graph analytic system, Stinger [19]. The updates are
handled by the internal logic of Stinger.

• cuSparseCSR (GPU). We also compare with the GPU-based CSR format used in the
NVIDIA cuSparse library [3]. The updates are executed by calling the rebuild function

17

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

in the cuSparse library.

• GPMA/GPMA+. They are our proposed approaches. Although insertions and deletions
could be handled similarly, in the sliding window models where the numbers of insertions
and deletions are often equal, the lazy deletions can be performed via marking the
location as deleted without triggering the density maintenance and recycling for new
insertions.

Note that we do not compare with DCSR [29] because, as discussed in Section 2.2, the
scheme can neither handle deletions nor support eﬃcient searches, which makes it incom-
parable to all schemes proposed in this paper.

To validate if using the dynamic graph format proposed in this paper aﬀects the perfor-
mance of graph algorithms, we implement the state-of-the-art GPU-based algorithms on
the CSR format maintained by GPMA/GPMA+ as well as cuSparseCSR. Meanwhile, we
invoke Stinger’s built-in APIs to handle the same workloads of the graph algorithms,
which are considered as the counterpart of GPU-based approaches in highly parallel CPU
environment. Finally, we implement the standard single-threaded algorithms for each ap-
plication in AdjLists and PMA as baselines for thorough evaluation. The details of all
compared solutions for each application is summarized in Table 1.

Experimental Environment. All algorithms mentioned in the remaining part of this
section are implemented with CUDA 7.5 and GCC 4.8.4 with -O3 optimization. All
experiments except Stinger run on a CentOS server which has Intel(R) Core i7-5820k
(6-cores, 3.30GHz) with 64GB main memory and three GeForce TITAN X GPUs (each has
12GB device memory), connected with PCIe v3.0. Stinger baselines run on a multi-core
server which is deployed 4-way Intel(R) Xeon(R) CPU E7-4820 v3 (40-cores, 1.90GHz)
with 128GB main memory.

6.2 The Performance of Handling Updates

In this subsection, we compare the update costs for diﬀerent update approaches. As pre-
viously mentioned, we start with the initial subgraph consisting of each dataset’s ﬁrst half
of total edges. We measure the average update time where the sliding window iteratively
shifts for a batch of edges. To evaluate the impact of update batch sizes, the batch size
is set to range from one edge and exponentially grow to one million edges with base two.
Figure 7 shows the average latency for all approaches with diﬀerent sliding batch sizes.
Note that the x-axis and y-axis are plotted in log scales. We have also tested sorted graph
streams to evaluate extreme cases. We omit the detailed results and we refer interested
readers to Appendix C.

We observe that, PMA-based approaches are very eﬃcient in handling updates when the
batch size is small. As batch size becomes larger, the performance of PMA and GPMA
quickly degrades to the performance of simple rebuild. Although GPMA achieves better per-
formance than GPMA+ for small batches since the concurrent updating entries are unlikely
to conﬂict, thread conﬂicts become serious for larger batches. Due to its lock-free char-
acterstic, GPMA+ shows superior performance over PMA and GPMA. In particular, GPMA+
has speedups of up to 20.42x and 18.30x against PMA and GPMA respectively. Stinger
shows impressive update performance in most cases as Stinger eﬃciently updates its
dynamic graph structure in a parallel fashion and the code runs on a powerful multi-core
CPU system. For now, multi-core CPU system is considered more powerful than GPUs
for pure random data structure maintainance but cost more (in our experimental setup,

18

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Figure 8: Streaming BFS

Figure 9: Streaming Connected Component

Figure 10: Streaming PageRank

our CPU server costs more than 5 times that of the GPU server). Moreover, we also note
that, Stinger shows extremely poor performance in the Graph500 dataset. According
to the previous study [8], the phenomenon is due to the fact that Stinger holds a ﬁxed
size of each edge block. Since Graph500 is a heavily skewed graph as the graph follows
the power law model, the skewness causes severe performance deﬁciency on the utilization
of memory for Stinger.

We observe the sharp increase for GPMA+ performance curves occur when the batch size
is 512. This is because the multi-level strategy is used in GPMA+ (which is mentioned in
Section 5.2) and shared-memory constraint cannot support batch size which is more than
512 on our hardware. Finally, the experiments show that, GPMA is faster than GPMA+
when the update batch is smaller and leads to few thread conﬂicts, because the GPMA+
logic is more complicated and includes overheads by a number of kernel calls. However,
using GPMA only beneﬁts when the update batch is extremely small and the performance
gain in such extreme case is also negligible compared with GPMA+. Hence, we can conclude
that GPMA+ shows its stability and eﬃciency across diﬀerent update patterns compared
with GPMA, and we will only show the results of GPMA+ in the remaining experiments.

6.3 Application Performance

As previously mentioned, all compared application-speciﬁc approaches are summarized in
Table 1. We ﬁnd that integrating GPMA+ into an existing GPU-based implementation
requires little modiﬁcation. The main one is in transforming the array operations in

19

0.00.050.10.150.20.250.3Time (second)1%0.1%0.01%Slide Size1514124.42.12.40.70.3Uniform Random0.00.050.10.150.20.250.3Time (second)1%0.1%0.01%1418194.81.71.4234.42.1Graph5000.00.010.020.030.040.05Time (second)1%0.1%0.01%1.61.71.71.20.70.60.40.40.4Reddit0.00.010.020.030.040.05Time (second)1%0.1%0.01%1.61.91.40.80.60.40.40.50.6PokecAdjListsPMAStingercuSparseCSRGPMA+BFS (patterned)Update (unpatterned)0.00.10.20.30.40.5Time (second)1%0.1%0.01%Slide Size1516114.41.91.91.51.31.0Uniform Random0.00.10.20.30.40.5Time (second)1%0.1%0.01%1619145.72.51.9234.91.9Graph5000.00.020.040.060.080.1Time (second)1%0.1%0.01%1.82.02.30.90.60.51.00.90.6Reddit0.00.020.040.060.080.1Time (second)1%0.1%0.01%1.51.61.70.70.40.40.50.50.5PokecAdjListsPMAStingercuSparseCSRGPMA+ConnectedComponent (patterned)Update (unpatterned)0.00.51.01.52.02.53.0Time (second)1%0.1%0.01%Slide Size4442359.24.54.03.6Uniform Random0.00.51.01.52.02.53.0Time (second)1%0.1%0.01%703730215.14.9254.8Graph5000.00.20.40.60.81.0Time (second)1%0.1%0.01%128.16.04.92.21.61.0Reddit0.00.20.40.60.81.0Time (second)1%0.1%0.01%149.76.93.01.81.2PokecAdjListsPMAStingercuSparseCSRGPMA+PageRank (patterned)Update (unpatterned)Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Figure 11: Concurrent data transfer and BFS computation with asynchronous stream

Figure 12: Multi-GPU performance on diﬀerent sizes of Graph500 datasets

the original implementation to the operations on GPMA+, as presented in Section 4.2. The
intentions of this subsection are two-fold. First, we test if using the PMA-like data structure
to represent the graph brings signiﬁcant overheads for the graph algorithms. Second, we
demonstrate how the update performance aﬀects the overall eﬃciency of dynamic graph
processing.

In the remaining part of this section, we present the performance of diﬀerent approaches
by showing their average elapsed time to process a shift of the sliding window with three
diﬀerent batch sizes, i.e., the batches contain 0.01%, 0.1% and 1% edges of the respective
dataset. We have also tested the graph stream with explicit random insertions and dele-
tions for all applications as an extended experiment. We omit the detailed results here
since they are similar to the results of the sliding window model and we refer interested
readers to Appendix D. We distinguish the time spent on updates and analytics with
diﬀerent patterns among all ﬁgures.

BFS Results: Figure 8 presents the results for BFS. Although processing BFS only ac-
cesses each edge in the graph once, it is still an expensive operation because BFS can
potentially scan the entire graph. This has led to the observation that CPU-based ap-
proach takes signiﬁcant amount of time for BFS computation whereas the update time is
comparatively negligible. Thanks to the massive parallelism and high memory bandwidth
of GPUs, GPU-based approaches are much more eﬃcient than CPU-based approaches for
BFS computation as well as the overall performance. For the cuSparseCSR approach,
the rebuild process is the bottleneck as the update needs to scan the entire group mul-
tiple times. In contrast, GPMA+ takes much shorter time for the update and has nearly
identical BFS performance compared with cuSparseCSR. Thus, GPMA+ dominates the
comparisons in terms of the overall processing eﬃciency.

We have also tested our framework in terms of hiding data transfer over PCIe by using
asynchronous streams to concurrently perform GPU computation and PCIe transfer. In
Figure 11, we show the results when running concurrent execution by using the GPMA+
approach. The data transfer consists of two parts: sending graph updates and fetching
updated distance vector (from the query vertex to all other vertices).
It is clear from
the ﬁgure that, under any circumstances, sending graph updates is overlapped by GPMA+
update processing and fetching the distance vector is overlapped by BFS computation.

20

020406080100Time (ms)1%0.1%0.01%Slide SizeUniform Random020406080100Time (ms)1%0.1%0.01%Graph5000510152025Time (ms)1%0.1%0.01%Reddit0510152025Time (ms)1%0.1%0.01%PokecGPMA+ UpdateBFSFetch BFS Distance VectorSend Updates020406080Throughput (million edges / second)600M1.2B1.8BNumber of EdgesPMA Update02468Throughput (billion edges / second)600M1.2B1.8BPageRank012345Throughput (billion edges / second)600M1.2B1.8BBFS0123Throughput (billion edges / second)600M1.2B1.8BConnected Component1 GPU2 GPUs3 GPUsAccelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Thus, the data transfer is completely hidden in the concurrent streaming scenario. As the
observations remain similar in other applications, we omit their results and explanations,
and the details can be found in Appendix B.

Connected Component Results: Figure 9 presents the results for running Connected
Component on the dynamic graphs. The results show diﬀerent performance patterns com-
pared with BFS as ConnectedComponent takes more time in processing which is caused
by a number of graph traversal passes to extract the partitions. Meanwhile, the update
cost remains the same. Thus, GPU-based solutions enhance their performance superior-
ity over CPU-based solutions. Nevertheless, the update process of cuSparseCSR is still
expensive compared with the time spent on Connected- Component. GPMA+ is very
eﬃcient in processing the updates. Although we have observed that, in the Reddit and
the Pokec datasets, GPMA+ shows some discrepancies for running the graph algorithm
against cuSparseCSR due to the “holes” introduced in the graph structure, the discrep-
ancies are insigniﬁcant considering the huge performance boosts for updates. Thus, GPMA+
still dominates the rebuild approach for overall performance.

PageRank Results: Figure 10 presents the results for Page- Rank. PageRank is a
compute-intensive task where the SpMV kernel is iteratively invoked on the entire graph
until the PageRank vector converges. The pattern follows from previous results: CPU-
based solutions are dominated by GPU-based approaches because iterative SpMV is a
more expensive process than BFS and ConnectedComponent, and GPU is designed to
handle massively parallel computation like SpMV. Although cuSparseCSR shows inferior
performance compared with GPMA+, the improvement brought by GPMA+’s eﬃcient update
is not as signiﬁcant as that in previous applications since the update costs are small
compared with the cost of iterative SpMV kernel calls. Nevertheless, the dynamic structure
of GPMA+ does not aﬀect the eﬃciency of the SpMV kernel and GPMA+ outperforms other
approaches in all experiments.

6.4 Scalability

GPMA and GPMA+ can also be extended to multiple GPUs to support graphs with size
larger than the device memory of one GPU. To showcase the scalability of our proposed
framework, we implement the multi-GPU version of GPMA+ and carry out experiments of
the aforementioned graph applications.

We generate three large datasets using Graph500 with increasing numbers of edges (600
Million, 1.2 Billion and 1.8 Billion) and conduct the same performance experiments in
section 6.3 with 1% slide size, on 1, 2 and 3 GPUs respectively. We evenly partition graphs
according to the vertex index and synchronize all devices after each iteration. For fair
comparison among diﬀerent datasets, we use throughput as our performance metric.The
experimental results of GPMA+ updates and application performance are illustrated in
Figure 12. We do not compare with Stinger because in this subsection, we focus on the
evaluation on the scalability of GPMA+. The memory consumption of Stinger exceeds
our machine’s 128GB main memory based on its default conﬁguration in the standalone
mode.

Multiple GPUs can extend the memory capacity so that analytics on larger graphs can be
executed. According to Figure 12, the improvement in terms of throughput for multiple

21

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

GPUs behaves diﬀerently in various applications. For GPMA+ update and PageRank,
we achieve a signiﬁcant improvement with more GPUs, because their workloads between
communications are relatively compute-intensive. For BFS and ConnectedComponent,
the experimental results demonstrate a tradeoﬀ between overall computing power and
communication cost with increasing number of GPUs, as these two applications incur
larger communication cost. Nevertheless, multi-GPU graph processing is an emerging
research area and more eﬀectiveness optimizations are left as future work. Overall, this
set of preliminary experiments shows that our proposed scheme is capable of supporting
large scale dynamic graph analytics.

6.5 Overall Findings

We summarize our ﬁndings in the subsection. First, GPU-based approaches (cuSparseCSR
and GPMA+) outperform CPU-based approaches thanks to our optimizations in taking ad-
vantage of the superior hardware of the GPUs, even compared with Stinger running on a
40-core CPU server. One of the key reasons is that GPMA+ and graph analytics can exploit
the superb high memory bandwidth and massive parallelism of the GPU, as many graph
applications are data- and compute-intensive. Second, GPMA+ is much more eﬃcient than
cuSparseCSR as maintaining the dynamic updates is often the bottleneck of continuous
graph analytic processing and GPMA+ avoids the costly process of rebuilding the graph
via incremental updates while bringing minimal overheads for existing graph algorithms
running its graph structure.

7 Conclusion & Future Work

In this paper, we address how to dynamically update the graph structure on GPUs in an
eﬃcient manner. First, we introduce a GPU dynamic graph analytic framework, which en-
ables existing static GPU-oriented graph algorithms to support high-performance evolving
graph analytics. Second, to avoid the rebuild of the graph structure which is a bottleneck
for processing dynamic graphs on GPUs, we propose GPMA and GPMA+ to support incre-
mental dynamic graph maintenance in parallel. We prove the scalability and complexity
of GPMA+ theoretically and evaluate the eﬃciency through extensive experiments. As the
future work, we would like to explore the hybrid CPU-GPU supports for dynamic graph
processing and more eﬀectiveness optimizations for involved applications.

22

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

Appendices

A TryInsert+ Optimizations

Based on diﬀerent segment sizes, we propose three optimizations of Function TryInsert+
in Algorithm 4. The motivation is to obtain better memory access eﬃciency and lower cost
of synchronization by balancing between problem scale and hardware hierarchy on GPU.
The key computation logic of TryInsert+ is to merge two sorted arrays, i.e., existing
segment entries and entries to be inserted. Standard approach for parallel merging needs
to identify the position in merged array by binary search and then to execute parallel map,
which requires heavy and uncoalesced memory accesses. Thus, depending on the size of
the merge, we wish to employ diﬀerent hardware hierarchies on GPU (i.e. warp, block and
device) to minimize the cost of memory accesses.

Before presenting the details of our optimizations, Algorithm 5 illustrates how to group
threads according to their positions in diﬀerent hierarchies of GPU architecture and how
to target the groups to their assigned segments. In particular, each thread is assigned with
a lane id, a block id and a global thread id to indicate the position of the thread in the
corresponding warp, block and device work group. Each thread is assigned for one GPMA+
segment and the thread will ask other threads in the same work group to cooperate for
its task. This means that each thread tries to drive a group of threads to deal with the
assigned segment. Such a strategy lifts thread divergences caused by diﬀerent execution
branches. Note that this assignment policy will be used in our warp and block based
optimizations as an initialization function.

Algorithm 6 shows the Warp-Based optimization for any segments with entries no larger
than the warp size. This implementation has high eﬃciency because explicit synchro-
nization is not needed and all data is stored in registers. For each iteration, all threads
of a particular warp will compete for the control of the warp as shown in line 11. The
winner will drive the other threads in this warp to handle its required computation steps
of the corresponding segment. As an example, line 27 counts valid entries in the segment
concurrently. Lines 32-34 omit the remaining computation steps in TryInsert+, such as
merging insertions and redistributing entries of segments, as their computation paradigm
is similar to counting entries.

Algorithm 7 shows the Block-Based optimization. It utilizes the shared memory, which
has a higher volume than registers, to store data. Even though explicit synchronization is
needed in line 12 and line 32 to guarantee consistency, synchronization in a block is highly
optimized in GPU hardware and thus it does little eﬀect to the overall performance. Both
Warp-Based and Block-Based optimizations explicitly accommodate GPU features. As
discussed in Section 5.2, although these two methods have limited memory for eﬃcient
access, they can handle most of the update requests.

Algorithm 8 shows the Device-Based implementation. The implementation is diﬀerent
from the ones in Warp and Block based approaches, because it is designed for segments
having a size larger than the shared memory size. Under this scenario, we have to handle
them in the GPU’s global memory. One possible approach is to invoke an independent
kernel for each large segment, but it will take considerable costs to initialize and schedule
for multiple kernels. Hence, we propose an approach to handle a large number segments
by only invoking a few unique kernel calls.

23

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

We illustrate the idea by showing how to perform counting segments which are valid for
insertions as an example. As shown in lines 5 and 12, all valid entries stored in GPMA+
segments are ﬁrst marked, and then all valid entry counts are calculated by SumReduce in
one kernel call. Line 16 generates valid indexes for segments which have enough free space
to receive their corresponding insertions, which is used by the rest computation steps.
Simply speaking, our approach executes in horizontal steps of the execution logic, in order
to avoid load imbalance caused by branch divergences. Finally, merging and segment
entries redistribution use the same mechanism and thus are omitted.

24

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38

Algorithm 5 TryInsert+ Initialization

inline function ConstInit( void ) {

// cuda protocol variables
WARPS = blockDim / 32;
warp_id = threadIdx / 32;
lane_id = threadIdx % 32;
thread_id = threadIdx;
block_width = gridDim;
grid_width = gridDim * blockDim;
global_id = block_width * blockIdx + threadIdx;

// infos for assigned segment
seg_beg = segments[global_id];
seg_end = seg_beg + segment_width;

// infos for insertions belong current segment
ins_beg = offset[global_id];
ins_end = offset[global_id + 1];
insert_size = ins_end - ins_beg;

// the upper number that current segment can hold
upper_size = tau * segment_size;

}

Algorithm 6 TryInsert+ Warp-Based Optimization

kernel TryInsert+(int segments[m], int offsets[m],

int insertions[n], int segment_width) {

ConstInit();

volatile shared comm[WARPS][5];
warp_shared_register pma_buf[32];

while (WarpAny(seg_end - seg_beg)) {

// compete for warp
comm[warp_id][0] = lane_id;

// winner controls warp in this iteration
if (comm[warp_id][0] == lane_id) {

seg_beg = seg_end;
comm[warp_id][1] = seg_beg;
comm[warp_id][2] = seg_end;
comm[warp_id][3] = ins_beg;
comm[warp_id][4] = ins_end;

}

memcpy(pma_buf, pma[seg_beg], segment_width);
// count valid entries in this segment
entry_num = 0;
if (lane_id < segment_width) {

valid = pma_buf[lane_id] == NULL ? 0 : 1;
entry_num = WarpReduce(valid);

}

// check upper density if insert
if (entry_num + insert_size) < upper_size) {

// merge insertions with pma_buf
// evenly redistribute pma_buf
// mark all insertions successful
memcpy(pma[seg_beg], pma_buf, segment_width);

}

}

}

25

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

Algorithm 7 TryInsert+ Block-Based Optimization

kernel TryInsert+(int segments[m], int offsets[m],

int insertions[n], int segment_width) {

ConstInit();

volatile shared comm[5];
volatile shared pma_buf[segment_width];

while (BlockAny(seg_end - seg_beg)) {

// compete for block
comm[0] = thread_id;
BlockSynchronize();

// winner controls block in this iteration
if (comm[0] == lane_id) {

seg_beg = seg_end;
comm[1] = seg_beg;
comm[2] = seg_end;
comm[3] = ins_beg;
comm[4] = ins_end;

}

memcpy(pma_buf, pma[seg_beg], segment_width);
// count valid entries in this segment
entry_num = 0;
ptr = thread_id;
while (ptr < segment_width) {

valid = pma_buf[ptr] == NULL ? 0 : 1;
entry_num += BlockReduce(valid);
thread_id += block_width;

}
BlockSynchronize();

// same as lines 30-37 in Algorithm 5

}

}

Algorithm 8 TryInsert+ Device-Based Optimization

function TryInsert+(int segments[m], int offsets[m],

int insertions[n], int segment_width) {

int valid_flags[m * segment_width];
parallel foreach i in range(m):

parallel foreach j in range(segment_width):

if (pma[segments[i] + j] != NULL) {

valid_flags[i * segment_width + j] = 1;

}

DeviceSynchronize();
int entry_nums[m];
DeviceSegmentedReduce(valid_flags, m,

segment_size, entry_nums);

DeviceSynchronize();
int valid_indexes[m];
parallel foreach i in range(m):

if (entry_nums[i] + insert_size < upper_size) {

valid_indexes[i] = i;

}

DeviceSynchronize();
RemoveIfTrue(valid_indexes);
DeviceSynchronize();
// according to valid_indexes, segmentedly to:
//
//
//

merge insertions into segments
evenly redistribute segments
mark all insertions successful

}

26

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

B Additional Experimental Results For Data Transfer

We show the experimental results for using asynchronous streams for concurrently trans-
mitting data on PCIe and running computations on the GPU. We only show the results
for GPMA+.

In ConnectedComponent, the data transferred on PCIe consists of two parts: the graph
updates and the component label vector to all vertices computed by ConnectedComponent.
In PageRank, the result vector to be fetched indicates PageRank scores, which has the
same size as ConnectedComponent’s. The results in Figures 13 and 14 have shown that
the data transfer is completely hidden by analytic processing on GPU and GPMA+ update.

Figure 13: Concurrent data transfer and Connected Component computation with asyn-
chronous stream

Figure 14: Concurrent data transfer and PageRank computation with asynchronous stream

C The Performance of Handling Updates on Sorted Graphs

Figure 15: Performance comparison for updates with diﬀerent batch sizes. The dashed
lines represent CPU-based solutions whereas the solid lines represent GPU-based solutions.

For the update results with sorted streaming orders, AdjLists performs the best among
all approaches due to its eﬃcient balanced binary tree update mechanism. Meanwhile,
a batch of sorted updates makes GPMA very ineﬃcient as all updating threads within
the batch conﬂict. Thanks to the non-locking optimization introduced, the update perfor-
mance of GPMA+ is still signiﬁcantly faster than that of the rebuild approach (cuSparseCSR)
with orders of magnitude speedups for small batch sizes.

27

0255075100125Time (ms)1%0.1%0.01%Slide SizeUniform Random0255075100125Time (ms)1%0.1%0.01%Graph500010203040Time (ms)1%0.1%0.01%Reddit0102030Time (ms)1%0.1%0.01%PokecGPMA+ UpdateConnectedComponentFetch Component Label VectorSend Updates050100150200250Time (ms)1%0.1%0.01%Slide SizeUniform Random0100200300400Time (ms)1%0.1%0.01%Graph5000255075100125Time (ms)1%0.1%0.01%Reddit0255075100125Time (ms)1%0.1%0.01%PokecGPMA+ UpdatePageRankFetch PageRank VectorSend Updates101103105Sliding Size0100101102103104105106Time (ms)Uniform Random101103105Sliding Size0100101102103104105106Time (ms)Graph500101103105Sliding Size0100101102103104105106Time (ms)Reddit101103105Sliding Size0100101102103104105106Time (ms)PokecAdjListsPMAStingercuSparseCSRGPMAGPMA+Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

D Additional Experimental Results For Graph Streams with Explicit

Deletions

We present the experimental results for graph streams which involve explicit deletions. In
this section, we use the same stream setup which is mentioned in Section 6.1. However, for
each iteration of sliding, we will randomly pick a set of edges belonging to current sliding
window insteading of the head part as edges to be deleted.

Figure 16: Streaming BFS with explicit deletions

Figure 17: Streaming Connected Component with explicit deletions

Figure 18: Streaming PageRank with explicit deletions

Figures 16, 17 and 18 illustrate the results of three streaming applications respectively.
Note that we pick sets of edges to be deleted in advance, which means that for each
independent baseline, it handles the same workload all the time. Since there is no intrinsic
diﬀerence between expiry and explicit deletions, the results are similar to sliding window’s.
The subtle diﬀerence in the results are mainly due to diﬀerent deletions which lead to
various applications’ running time.

28

0.00.050.10.150.20.250.3Time (second)1%0.1%0.01%Slide Size1514124.42.12.40.70.3Uniform Random0.00.050.10.150.20.250.3Time (second)1%0.1%0.01%1418194.81.71.4234.42.1Graph5000.00.010.020.030.040.05Time (second)1%0.1%0.01%1.61.71.71.20.70.60.40.40.4Reddit0.00.010.020.030.040.05Time (second)1%0.1%0.01%1.61.91.40.80.60.40.40.50.6PokecAdjListsPMAStingercuSparseCSRGPMA+BFS (patterned)Update (unpatterned)0.00.10.20.30.40.5Time (second)1%0.1%0.01%Slide Size1516114.41.91.91.51.31.0Uniform Random0.00.10.20.30.40.5Time (second)1%0.1%0.01%1619145.72.51.9234.91.9Graph5000.00.020.040.060.080.1Time (second)1%0.1%0.01%1.82.02.30.90.60.51.00.90.6Reddit0.00.020.040.060.080.1Time (second)1%0.1%0.01%1.51.61.70.70.40.40.50.50.5PokecAdjListsPMAStingercuSparseCSRGPMA+ConnectedComponent (patterned)Update (unpatterned)0.00.51.01.52.02.53.0Time (second)1%0.1%0.01%Slide Size4442359.24.54.03.6Uniform Random0.00.51.01.52.02.53.0Time (second)1%0.1%0.01%703730215.14.9254.8Graph5000.00.20.40.60.81.0Time (second)1%0.1%0.01%128.16.04.92.21.61.0Reddit0.00.20.40.60.81.0Time (second)1%0.1%0.01%149.76.93.01.81.2PokecAdjListsPMAStingercuSparseCSRGPMA+PageRank (patterned)Update (unpatterned)Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

References

[1] Apache ﬂink. https://flink.apache.org/. Accessed: 2016-10-18.

[2] Cusp library. https://developer.nvidia.com/cusp. Accessed: 2017-03-25.

[3] cusparse. https://developer.nvidia.com/cusparse. Accessed: 2016-11-09.

[4] CUDA UnBound (CUB) library. https://nvlabs.github.io/cub/, 2015.

[5] L. Akoglu, H. Tong, and D. Koutra. Graph based anomaly detection and description:

a survey. Data Min. Knowl. Discov., 29(3):626–688, 2015.

[6] A. Ashari, N. Sedaghati, J. Eisenlohr, S. Parthasarathy, and P. Sadayappan. Fast
In SC, pages

sparse matrix-vector multiplication on gpus for graph applications.
781–792, 2014.

[7] A. Ashari, N. Sedaghati, J. Eisenlohr, and P. Sadayappan. An eﬃcient two-
In
dimensional blocking strategy for sparse matrix-vector multiplication on gpus.
ICS, pages 273–282, 2014.

[8] D. A. Bader, J. Berry, A. Amos-Binks, D. Chavarr´ıa-Miranda, C. Hastings, K. Mad-
duri, and S. C. Poulos. Stinger: Spatio-temporal interaction networks and graphs
(sting) extensible representation. Georgia Institute of Technology, Tech. Rep, 2009.

[9] N. Bell and M. Garland. Eﬃcient sparse matrix-vector multiplication on CUDA.

Technical Report NVR-2008-004, NVIDIA Corporation, 2008.

[10] M. A. Bender, E. D. Demaine, and M. Farach-Colton. Cache-oblivious b-trees. SIAM

J. Comput., 35(2):341–358, 2005.

[11] M. A. Bender and H. Hu. An adaptive packed-memory array. ACM Trans. Database

Syst., 32(4), 2007.

[12] L. Braun, T. Etter, G. Gasparis, M. Kaufmann, D. Kossmann, D. Widmer,
A. Avitzur, A. Iliopoulos, E. Levy, and N. Liang. Analytics in motion: High perfor-
mance event-processing and real-time analytics in the same database. In SIGMOD,
pages 251–264, 2015.

[13] F. Busato and N. Bombieri. Bfs-4k: an eﬃcient implementation of bfs for kepler gpu

architectures. TPDS, 26(7):1826–1838, 2015.

[14] R. Cheng, J. Hong, A. Kyrola, Y. Miao, X. Weng, M. Wu, F. Yang, L. Zhou, F. Zhao,
and E. Chen. Kineograph: Taking the pulse of a fast-changing and connected world.
In EuroSys, pages 85–98, 2012.

[15] M. S. Crouch, A. McGregor, and D. Stubbs. Dynamic graphs in the sliding-window
model. In European Symposium on Algorithms, pages 337–348. Springer, 2013.

[16] H.-V. Dang and B. Schmidt. The sliced coo format for sparse matrix-vector multipli-

cation on cuda-enabled gpus. Procedia Computer Science, 9:57–66, 2012.

[17] M. Datar, A. Gionis, P. Indyk, and R. Motwani. Maintaining stream statistics over

sliding windows. SIAM journal on computing, 31(6):1794–1813, 2002.

[18] A. Davidson, S. Baxter, M. Garland, and J. D. Owens. Work-eﬃcient parallel gpu
In Parallel and Distributed Processing

methods for single-source shortest paths.
Symposium, 2014 IEEE 28th International, pages 349–359. IEEE, 2014.

29

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

[19] D. Ediger, R. McColl, E. J. Riedy, and D. A. Bader. STINGER - High performance

data structure for streaming graphs. HPEC, 2012.

[20] M. Elkin. Streaming and fully dynamic centralized algorithms for constructing and

maintaining sparse spanners. ACM Trans. Algorithms, 7(2):20:1–20:17, 2011.

[21] J. Feigenbaum, S. Kannan, A. McGregor, S. Suri, and J. Zhang. On graph problems

in a semi-streaming model. Theor. Comput. Sci., 348(2-3):207–216, 2005.

[22] Z. Fu, M. Personick, and B. Thompson. MapGraph: A High Level API for Fast
Development of High Performance Graph Analytics on GPUs. A High Level API for
Fast Development of High Performance Graph Analytics on GPUs. ACM, New York,
New York, USA, June 2014.

[23] S. Guha and A. McGregor. Graph synopses, sketches, and streams: A survey.

Proc. VLDB Endow., 5(12):2030–2031, 2012.

[24] P. Harish and P. Narayanan. Accelerating large graph algorithms on the gpu using
cuda. In International Conference on High-Performance Computing, pages 197–208.
Springer, 2007.

[25] D. S. Hirschberg. Parallel algorithms for the transitive closure and the connected
component problems. In Proceedings of the eighth annual ACM symposium on Theory
of computing, pages 55–57. ACM, 1976.

[26] A. P. Iyer, L. E. Li, T. Das, and I. Stoica. Time-evolving graph processing at scale.
In Proceedings of the Fourth International Workshop on Graph Data Management
Experiences and Systems, pages 5:1–5:6, 2016.

[27] A. P. Iyer, L. E. Li, and I. Stoica. Celliq : Real-time cellular network analytics at

scale. In NSDI, pages 309–322, 2015.

[28] R. Kaleem, A. Venkat, S. Pai, M. Hall, and K. Pingali. Synchronization trade-oﬀs
in gpu implementations of graph algorithms. In Parallel and Distributed Processing
Symposium, 2016 IEEE International, pages 514–523. IEEE, 2016.

[29] J. King, T. Gilray, R. M. Kirby, and M. Might. Dynamic sparse-matrix allocation on

gpus. In ISC, pages 61–80, 2016.

[30] J. Leskovec and R. Sosiˇc. Snap: A general-purpose network analysis and graph-mining

library. TIST, 8(1):1, 2016.

[31] X. Lin, R. Zhang, Z. Wen, H. Wang, and J. Qi. Eﬃcient subgraph matching using

gpus. In ADC, pages 74–85, 2014.

[32] H. Liu, H. H. Huang, and Y. Hu. ibfs: Concurrent breadth-ﬁrst search on gpus. In

SIGMOD, pages 403–416, 2016.

[33] L. Luo, M. Wong, and W.-m. Hwu. An eﬀective gpu implementation of breadth-ﬁrst

search. In DAC, pages 52–55, 2010.

[34] M. Martone, S. Filippone, S. Tucci, P. Gepner, and M. Paprzycki. Use of hybrid
recursive csr/coo data structures in sparse matrix-vector multiplication. In IMCSIT,
pages 327–335. IEEE, 2010.

[35] A. McGregor. Graph stream algorithms: A survey. SIGMOD Rec., 43(1):9–20, 2014.

30

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

[36] D. Merrill, M. Garland, and A. Grimshaw. High-Performance and Scalable GPU

Graph Traversal. TOPC, 1(2), 2015.

[37] R. C. Murphy, K. B. Wheeler, B. W. Barrett, and J. A. Ang. Introducing the graph

500. 2010.

[38] N. Ohsaka, T. Maehara, and K.-i. Kawarabayashi. Eﬃcient pagerank tracking in

evolving networks. In KDD, pages 875–884, 2015.

[39] Y. Saad. Numerical solution of large nonsymmetric eigenvalue problems. Computer

Physics Communications, 53(1):71–90, 1989.

[40] D. Sayce. 10 billions tweets, number of tweets per day. http://www.dsayce.com/

social-media/10-billions-tweets/. Accessed: 2016-10-18.

[41] J. Soman, K. Kothapalli, and P. J. Narayanan. A fast GPU algorithm for graph

connectivity. IPDPS Workshops, 2010.

[42] M. Stonebraker, U. C¸ etintemel, and S. Zdonik. The 8 requirements of real-time stream

processing. ACM SIGMOD Record, 34(4):42–47, 2005.

[43] J. A. Stratton, N. Anssari, C. Rodrigues, I.-J. Sung, N. Obeid, L. Chang, G. D. Liu,
and W.-m. Hwu. Optimization and architecture eﬀects on gpu computing workload
performance. In InPar, pages 1–10, 2012.

[44] N. Tang, Q. Chen, and P. Mitra. Graph stream summarization: From big bang to

big crunch. In SIGMOD, pages 1481–1496, 2016.

[45] A. Toshniwal, S. Taneja, A. Shukla, K. Ramasamy, J. M. Patel, S. Kulkarni,
J. Jackson, K. Gade, M. Fu, J. Donham, N. Bhagat, S. Mittal, and D. Ryaboy.
Storm@twitter. In SIGMOD, pages 147–156, 2014.

[46] C. E. Tsourakakis, U. Kang, G. L. Miller, and C. Faloutsos. DOULION: counting

triangles in massive graphs with a coin. In SIGKDD, pages 837–846, 2009.

[47] U. Verner, A. Schuster, M. Silberstein, and A. Mendelson. Scheduling processing of
In SYSTOR, page 7,

real-time data streams on heterogeneous multi-gpu systems.
2012.

[48] Y. Wang, A. Davidson, Y. Pan, Y. Wu, A. Riﬀel, and J. D. Owens. Gunrock: A high-
performance graph processing library on the gpu. SIGPLAN Not., 50(8):265–266,
2015.

[49] Y. Wang, Q. Fan, Y. Li, and K.-L. Tan. Real-time inﬂuence maximization on dynamic

social streams. In Proc. VLDB Endow., 2017.

[50] S. Yan, C. Li, Y. Zhang, and H. Zhou. yaspmv: yet another spmv framework on gpus.

In SIGPLAN Notices, volume 49, pages 107–118, 2014.

[51] X. Yang, S. Parthasarathy, and P. Sadayappan. Fast sparse matrix-vector multipli-
cation on gpus: Implications for graph mining. Proc. VLDB Endow., 4(4):231–242,
2011.

[52] X. Yang, S. Parthasarathy, and P. Sadayappan. Fast sparse matrix-vector multipli-
cation on gpus: implications for graph mining. Proc. VLDB Endow., 4(4):231–242,
2011.

31

Accelerating Dynamic Graph Analytics on GPUs

M. Sha et al.

[53] Y. Yang, Z. Wang, J. Pei, and E. Chen. Tracking inﬂuential nodes in dynamic

networks. arXiv preprint arXiv:1602.04490, 2016.

[54] M. Zaharia, T. Das, H. Li, T. Hunter, S. Shenker, and I. Stoica. Discretized streams:
Fault-tolerant streaming computation at scale. In SOSP, pages 423–438, 2013.

[55] H. Zhang, P. Lofgren, and A. Goel. Approximate personalized pagerank on dynamic

graphs. arXiv preprint arXiv:1603.07796, 2016.

[56] Y. Zhang and F. Mueller. Gstream: A general-purpose data streaming framework on

GPU clusters. In ICPP, pages 245–254, 2011.

[57] J. Zhong and B. He. Medusa: Simpliﬁed graph processing on gpus.

IEEE Trans.

Parallel Distrib. Syst., 25(6):1543–1552, 2014.

32

