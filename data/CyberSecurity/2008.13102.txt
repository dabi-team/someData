1

Blackmarket-driven Collusion on Online Media: A Survey

HRIDOY SANKAR DUTTA∗, IIIT-Delhi, India
TANMOY CHAKRABORTY, IIIT-Delhi, India

Online media platforms have enabled users to connect with individuals, organizations, and share their thoughts. Other than
connectivity, these platforms also serve multiple purposes - education, promotion, updates, awareness, etc. Increasing the
reputation of individuals in online media (aka Social growth) is thus essential these days, particularly for business owners and
event managers who are looking to improve their publicity and sales. The natural way of gaining social growth is a tedious
task, which leads to the creation of unfair ways to boost the reputation of individuals artificially. Several online blackmarket
services have developed thriving ecosystem with lucrative offers to attract content promoters for publicizing their content
online. These services are operated in such a way that most of their inorganic activities are being unnoticed by the media
authorities, and the customers of the blackmarket services are less likely to be spotted. We refer to such unfair ways of
bolstering social reputation in online media as collusion. This survey is the first attempt to provide readers a comprehensive
outline of the latest studies dealing with the identification and analysis of blackmarket-driven collusion in online media. We
present a broad overview of the problem, definitions of the related problems and concepts, the taxonomy of the proposed
approaches, description of the publicly available datasets and online tools, and discuss the outstanding issues. We believe that
collusive entity detection is a newly emerging topic in anomaly detection and cyber-security research in general and the
current survey will provide readers with an easy-to-access and comprehensive list of methods, tools and resources proposed
so far for detecting and analyzing collusive entities on online media.

Additional Key Words and Phrases: Collusion, blackmarket, Twitter, YouTube, social media analysis

INTRODUCTION

1
The prosperity of online media has attracted people and organizations to join the platform and use it for several
purposes - creating a network among commonalities, building and broadening their business, promoting/demoting
e-commerce products, etc. This has led users to choose artificial ways of gaining social growth to get benefits
within a short time. The main reason behind choosing artificial boosting is that the legitimate efforts of gaining
appraisals (followers, retweets, likes, shares, etc.) take a significant amount of time and may not meet the actual
needs of users. Such activities impose a significant threat to social media platforms as these are mostly against
the Terms of Service of many social media platforms (see Section 2.1.2 for more details). Such artificial boosting
of social reputation/growth is often known as “collusion” [28, 40].

According to a recent survey by HitSearch1, 98% of content creators admitted to having spotted collusive
followers among online influencers on Instagram. The adversarial impact of the collusive entities poses a massive
threat to online media. These entities create an atmosphere where people start trusting their information due to
the popularity they receive. For example, in the 2019 UK general election, politicians approached the blackmarket
services2 for online political campaigning in order to reach out to their potential voters. A study conducted by
LawSuit3, a law management firm in the United States, reported that around 15% of the Twitter users are non-
human accounts driving more than 66% links published to the site. It is also reported that most of the politicians
currently in contention or conversation for the 2020 US presidential election have a very high percentage or
volume of non-human followers linked to their Twitter account. The above examples show how collusive entities

∗This is the corresponding author
1https://tinyurl.com/y39qtg2r
2https://www.ics-digital.com/general-election-2019-mps-fake-twitter-followers/
3https://lawsuit.org/politics-and-fake-social-media-followers/

, Vol. 1, No. 1, Article 1. Publication date: January 2020.

0
2
0
2

g
u
A
0
3

]
I
S
.
s
c
[

1
v
2
0
1
3
1
.
8
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
boost the believability of information during events. Moreover, the limitation of humans’ potential to distinguish
between the collusive and genuine entities due to the vast amount of available information is an important
concern that motivates to design methods for automatic identification of these entities. The collusive entities
not only deceive people but also pollute the entire social space. Using the blackmarket services, the collusive
entities can perform appraisals such as improving the credibility of rumors, propagate fake news, inflate/deflate
the ratings of products in e-commerce platforms, gain popularity in video-sharing platforms, etc. Fig. 1 illustrates
how collusive entities gain artificial appraisals on various online media platforms. We request the reader to
refer to Section 4 for a thorough explanation of Fig. 1. Recent studies that tackle the problem of collusive entity
detection state that it is harder to discern these entities as they express a mixture of organic and inorganic
activities [40]. Moreover, the problem is relatively new as compared to its related studies, e.g., bot/spam detection,
fake account detection, etc. and recently has gained significant attention from diverse research communities.

Although artificial boosting has become a regular practice with the increasing popularity of different online
media platforms, there is a lack of coherent and collective attempts from different research communities to explore
the micro-dynamics controlling such malpractice and the extent of its effect in manipulating social reputation.
Most of the existing studies aim to detect spam [13, 30, 82, 85, 99, 113, 115, 116, 120–122, 131, 136], fake [20, 26,
31, 32, 43, 53, 54, 56, 65, 86, 97, 106, 137] and bots [21, 24, 25, 29, 30, 34, 38, 47, 51, 68, 70, 88, 114, 117, 130, 135],
and how these accounts are used for information propagation [12, 27, 36, 75, 78, 78, 91, 104, 119] in online media.
Studies [39, 40] have shown that collusive users are neither fake users nor bots, but are normal human beings who
express a mixture of organic and inorganic activities. Unlike bots, these users have no synchronicity across their
behaviors [40], which makes it difficult to design automated techniques to detect them. In this paper, we present
a comprehensive survey of existing literature on topics related to collusion in different online media platforms.
Recently, Kumar and Shah [74] surveyed three aspects of false information on the web and social media – fake
reviews, hoaxes, and false news. They also mentioned about the lack of publicly available datasets related to
false information and social media rumors. In comparison to [74] and other related surveys, we mention our
contributions in the remaining part of this section.

1.1 Related Surveys and Our Contributions
To our knowledge, this is the first survey to provide a detailed overview of collusive activities in online media.
The aim of this survey is to provide readers with a comprehensive overview of the major studies conducted in
detecting and reasoning collusive activities across different online media platforms. The following four aspects
make our survey unique and different from other related surveys:

(1) Existing surveys do not directly focus on the problem of “collusive activities” and are more centered
around the detection and analysis of fake users, fraudsters and spammers on the web. Previous studies
mentioned that unlike these problems, collusive activities are very different in nature [7, 39, 40] due to
the mixture of organic and inorganic activities. In this paper, we conduct a thorough analysis of past
studies dealing with collusive activities in different online platforms.

(2) Existing surveys mentioned only fraud and spam-related datasets. Here, we describe the datasets on

collusion from multiple aspects - the type of dataset and the entities present in them.

(3) We also outline the annotation guidelines and evaluation metrics used for collusive entity detection, as

mentioned in the related studies.

(4) We conclude the paper by highlighting a set of key challenges and open problems in the area of collusion

in online media.

We use the terms collusion and artificial boosting of social growth interchangeably throughout the paper. We refer
to an action/appraisal as online media activity such as retweet, follow, review, view, subscription, etc., and an
entity as a social entity such as user, tweet, post, review, etc.

2

Fig. 1. Illustration of blackmarket services with types, platforms, appraisals.

3

1.2 Survey Methodology

1.2.1

Survey Scope. Since our scope is on investigating collusive entities in online media platforms, we will
systematize the studies related to the analysis and detection of the collusive entities. As collusion is related to a
few other aspects such as fake, bot, spam, etc., we partly focus on the previous works in those domains as well.

1.2.2

Survey Organization. In this article, we survey the existing algorithms, methodologies and applications in
detecting and analyzing collusive activities in online media platforms. In Section 2, we provide a broad overview
and background of collusion in online media. We also analyze particular cases of collusion, show how collusion
is closely related to other close concepts, see how collusion has been evolving and who are the main targets of it.
Next, in Section 3, we provide some preliminary concepts on blackmarket services. In Section 4, we show how
collusion happens across multiple online media platforms and overview the state-of-the-art techniques for each
platform. Section 5 explains two broad categories of collusive activities. To identify what has been done so far
in the literature, in Section 6, we conduct a systematic literature review of the existing techniques. In Section
7, we present the annotation guidelines, related datasets and evaluation metrics that can be used for studies in
collusive entity detection. Being a under-explored research problem, there are plenty of important issues that
need further attention. Pointers to such issues are mentioned in Section 8. Section 9 concludes this survey with a
summary of the main contributions and open problems.

2 BACKGROUND AND PRELIMINARIES
2.1 Overview of Online Media

2.1.1 Online Media Platforms. Online media refers to the technologies present on the Internet that connect
people or organizations to exchange information. The reason online media has become famous is the abundant
sources of information offered by the Internet where a user can get access to the same news from several places.
To keep users engaged, online media platforms also enable them to share their opinions on the information. In
today’s world, social media is considered to be a fast, inexpensive and effective way to reach a target audience.
The history of online media started at the end of the 19th century with the arrival of Arpanet, email, blogging and
bulletin boards. Prior to this, services such as telegram, radio, telephone, etc. were used to exchange information.
However, their contributions were limited as information exchange in these platforms did not take place “online”
and was mostly used to send individual messages between two people. The rapid growth of the Internet in
early 2000 set the real stage for the emergence of online media with the arrival of knowledge sharing and
social networking sites. Fig. 2 depicts the history of online media starting from postal service in the early 90s
to live-streaming (Periscope, Meerkat) and messaging platforms (Discord) in 2020. Example of online media
platforms are social networking sites (e.g., Facebook, LinkedIn), microblogs (e.g., Twitter, Tumblr), wiki-based
knowledge-sharing sites (e.g., Wikipedia), social news sites and websites of news media (e.g., Huffington Post),
forums, mailing lists, newsgroups community media sites (e.g., YouTube, Flickr, Instagram), social Q & A sites
(e.g., Quora, Yahoo Answers), user reviews (e.g., Yelp, Amazon.com), social curation sites (e.g., Reddit, Pinterest)
and location-based social networks (e.g., Foursquare).

2.1.2 Terms of Service in Online Media Platforms. Terms of Service (ToS) in online media platforms refer
to the rules and regulations to be agreed upon by the users of the services. Each service has its own policy
against fake and spam engagements. For instance, Twitter has declared its own platform manipulation and spam
policy4, YouTube has Fake Engagement Policy5, etc. These policies ensure that violations of the rules may result
in permanent suspension of account and its content. The ToS followed by the online media platforms forbids
artificially inflating own or others’ appraisals (followers/retweets/views/likes/subscriptions). This includes selling

4https://help.twitter.com/en/rules-and-policies/platform-manipulation
5https://support.google.com/youtube/answer/3399767?hl=en

4

Fig. 2. Evolution of online media platforms.

or purchasing engagements using premium services, using or promoting third-party apps by posting content
that helps in gaining engagements, trading to exchange engagements using freemium services, etc. Some of the
terms proposed by Twitter on collusion-related activities are engagement churn (first following a large number of
unrelated Twitter accounts and then unfollowing them), indiscriminate engagement (using third-party APIs or
automated software to follow a large number of unrelated accounts in a short time period), aggressive engagement
(aggressively engaging with Tweets to drive traffic or attention to accounts).

2.2 Collusion in online media

2.2.1 Definition. In general, collusion is defined as a covert and secret conspiracy or collaboration to deceive
others. Collusion in online media is a process by which users artificially gain social reputation, which violates the
ToS of the online media platform. Online media entities involved in collusion approach blackmarket services
to artificially inflate their social status. This results in entities to appear credible and legitimate to the end-
users, thus leading to activities such as fake promotions, campaigns, misinformation, etc., thereby creating an
inadequate social space. The blackmarket services provide eminent online media services ranging from online
social networks to various other platforms such as rating/review platforms, video-sharing platforms and even
recruitment platforms. In Section 4, we discuss in detail how collusion happens in all these platforms. For
example, a boost in YouTube views can transform a small event into a big campaign or a promotional event.
Despite its apparent presence in the real world, collusion has remained as an underexplored concept. We go
deeper into the notion of collusion by discussing particular cases and examples in the next subsection.

2.2.2 Examples of Collusive Activities. In this section, we show two examples of how collusion happens in
online media. Fig. 3 shows an example of collusive activities in online media. Note that we redacted the name
of the Twitter user/YouTube channel to maintain anonymity. Fig. 3(a) shows the official Twitter account of
an organization registered in blackmarket services for collusive follower appraisals. Fig. 3(b) shows a video
posted by a verified YouTube channel registered in blackmarket services for collusive like requests. In both
examples, the accounts are marked as verified by Twitter/YouTube. The presence of verified online media entities
in the blackmarket services clearly shows that the in-house algorithms deployed by these platforms have been
unsuccessful in detecting such entities. This further motivates the problem and necessitates the development of
automated techniques to detect these entities. In Section 4.1, we discuss in detail how online media users can
request verification badge through the blackmarket services.

5

(a)

(b)

Fig. 3. Example of collusive activities in online media. (a) an official Twitter account of an organization registered in
blackmarket services for collusive follower appraisals. (b) a video posted by a verified YouTube channel registered in
blackmarket services for collusive like requests. Some profile information are blurred for the sake of anonymity.

2.3 Reasons behind Collusion
In order to keep up with the pace of today’s turbo-charged world, a large number of users like to go against
the stream. Though shortcuts to success always conflict with the enormous efforts needed to be successful, it
may also lead to some costly mistakes, eventually undermining the true goals. Online media is considered as
the perfect convergence of communication and information. It has already become the most important source
for public information, organizations and for individuals to promote their ideologies, products and services.
Manipulating such an important source of information can result in a significant gain in terms of fame and
finance for individuals/organizations. This is clearly evident in online social networking platforms like Twitter in
the form of attaining fake followers, rating platforms like Amazon in the form of posting fake reviews, video
streaming platforms like YouTube in the form of synthetic gain in viewership count and many more. With a
huge competition in online media, it is hard for online media users who spend months creating new content but
fail to reach their target audience. Blackmarket services serve as a complete social media toolkit that helps an
online media entity to gain a stronger social media presence among its competitors. It is very effective for – (i)
companies who want to hype a campaign, (ii) musicians who want to promote new projects, album and releases,
(iii) entertainment companies who want to publicize their shows, and (iv) online media moguls who need quick
online media presence.

2.4 Challenges in Collusion Detection
Detecting collusive activities is a challenging task. Dutta et al. [40] showed how collusive users show a mixed
behavior of organic and inorganic activities. In the case of Twitter, these users, on the one hand, are involved in
retweeting (following) genuine tweets (genuine users); and on the other hand, they are involved in retweeting
(following) tweets (users) submitted to the blackmarket services. This kind of mixed behavior makes it difficult
to be detected by traditional fake user detection methods [103]. Moreover, collusive users are not bots; they
are normal human beings. This makes it difficult to be flagged by bot detection methods [28]. A recent study
by Dutta et al. [42] investigated how collusion happens on YouTube. They designed web scrapers to collect a
large set of YouTube videos and channels submitted to the blackmarket services for collusive appraisals. In the
case of rating platforms like Amazon, methods to detect collusive reviewers have not been successful so far
due to the lack of enough labeled data. The dynamic nature of the propagation of collusive activities is very
complicated. Collusive activities can easily propagate and impact a large number of users in a short time by
spreading misinformation. Kim et al. [69] developed CURB, an online algorithm that leverages information from
the crowd to prevent the spread of misinformation. They also reported that fact-checking organizations like
Snope and Politifact are not able to properly limit the spread of misinformation as it requires significant
human effort. Moreover, as collusion happens in multiple online media platforms, it is also a difficult task to
understand the complete adversarial intent of the collusive entities. This raises concern on developing systems

6

Table 1. Comparison between collusion and other related concepts.

Concept
Fake

Bot

Sockpuppetry

Malicious promotion

Spam

Content polluters

Definition of the concept
Fake is to increase the visibility of others’
content [41].
Bots have synchronous fraudulent activi-
ties [49].
Sockpuppets are operated by a puppetmas-
ter who controls other user accounts [71].
Promote a specific product/topic for a tar-
get audience [81].
Consistently perform similar operations
across multiple accounts to manipulate or
undermine current trends [48].
Polluters post nearly identical contents,
sometimes by randomly adding mentions
to unrelated legitimate users [77].

Relationship Infiltrators They follow the reciprocity in the relation-
ship (follow/retweet/subscribe) to engage
in spam activities [77].
They give a false low rating with a positive
review to confuse recommender systems
[129]

Slanderous users

Difference from collusion
Collusion is self-focused i.e., to increase the
visibility of their own content.
Collusive entities exhibit asynchronous
fraudulent activities.
In collusion, every account is controlled by
the real owner of the account.
Collusion is more general and not neces-
sarily focused on a specific product/topic.
Collusive entity may contain spammy con-
tents, but not necessarily.

In collusion, content pollution never oc-
curs as every action happens through the
blackmarket services.
In collusion,
freemium services to gain credits.

reciprocity happens

in

Collusive users always give a high rating
and positive review.

for early detection of collusive entities to limit its artificial social growth. Finally, due to the restrictions of the
online media platforms on collecting the public data, the research community has very limited training data,
which does not include all the necessary information of collusive entities. Most of the previous studies [7, 42]
had to design their own scrapers due to the limitations and restrictions of the APIs provided by the online media
platforms.

2.5 Collusion and Other Related Concepts
Some related concepts to collusion are fake, bot, sockpuppetry, malicious promotion, spam, content polluters,
relationship infiltrators and slanderous users. We distinguish between these concepts and collusion in Table
1. Even though these concepts differ from collusion, they are highly related. Therefore, studying the literature
which focuses on these topics will give us a better knowledge of how to analyze and detect the collusive entities
in online media platforms.

3 A NOTE ON BLACKMARKET SERVICES
Blackmarket services are the major controlling authorities of collusive activities. Customers join these services
and contribute directly or indirectly to boost their online profiles artificially. The blackmarket services are divided
into two types based on the mode of service [103] – Premium and Freemium.
1. Premium Services: These services require the customers to pay a certain amount of money in order to obtain
the facilities (e.g., SocialShop, RedSocial). Most of the premium services provide a comprehensive range of social
media enhancement services for all purposes. These services also ensure strategic social media promotion to
maintain an edge over the competitors, in the following ways: (i) location-specific actions, (ii) time during which
users want to gain social growth, (iii) users gained from the services have eye-catching display pictures and filled

7

Fig. 4. Illustration showing the working principle of premium and freemium services.

out bios in the profile, and (iv) domain-specific actions (users gained from services can be from various domains
such as fashion, music, blogging, etc.). These services offer actions in tiers (100, 1K, 10K, etc.) and lucrative
offers to their customers and some additional facilities such as 100% money-back guarantee, retention guarantee,
complete privacy, etc. Few of these services ensure replenishment if the customer experiences any drop in the
actions. We divide this type of services into two categories:

(1) General Premium: Here, customers have to choose a plan which suits their budget.
(2) Auto Premium: These services provide daily/weekly/monthly delivery service of actions. Customers
need to select one of the auto action packages in advance for a time duration, e.g., 100 Twitter auto
retweets (2-4 days). The main advantage of these services is to automate the process of social reputation
to an extent. The basic principle of auto premium remains the same as that of general premium services
but can be considered as a faster and effective way to boost social reputation.

2. Freemium Services: These services are free to use; but they also have premium subscription plans (e.g.,
YouLikeHits, Like4Like). The main idea is to get customers familiar with the workflow of the services and motivate
them to opt for the premium plans. Freemium services operate in one of the three ways: social-share services,
auto-time freemium services, and credit-based services. More details on these types can be found in [40] and [39].
Freemium services operate in one of the three ways [40]:

(1) Social-share Services: These services require customers to perform social actions on multiple platforms
in order to get appraisals for their content. Some of the possible actions are share/like/follow on Facebook,
follow/like/view/comment on Instagram, like/view/share on YouTube, etc.

(2) Auto-time Freemium services: These services require the customers to get access tokens from the
services, after which they can request for a fixed number of actions for a time duration, e.g., 10-50 retweets
in 10 minutes window.

(3) Credit-based Services: These services are operated based on a ‘give and take’ relationship. Customers
of these services lose credits when other customers perform actions on their submitted content. Similarly,
customers gain credits when they perform actions on the content of other customers.

Fig. 4(a) shows the working of premium services. Here, two types of entities are involved – customers that ask
for appraisals and suppliers those supply appraisals. Fig. 4(b) shows the working of freemium services. Here,
customers are involved in a credit-based ecosystem; hence customers are also the suppliers. The wordclouds of the
description/bios or premium and freemium users observed by Dutta and Chakraborty [39] are shown in Fig. 5.

8

(a)

(b)

Fig. 5. Wordclouds generated from the profile description of customers in (a) premium and (b) freemium services. Reprinted
with permission from [39].

4 COMPROMISED ONLINE PLATFORMS
As discussed in previous sections, collusion happens across multiple online media platforms. In this section,
we discuss in detail how appraisals in these platforms are artificially manipulated by the collusive entities.
We will look into seven types of platforms: Social networks, rating/review platforms, video streaming platforms,
recruitment platforms, discussion platforms, music sharing platforms and development platforms. Other than
the above-mentioned platforms, we will also look into the artificial manipulation of website traffic through
blackmarket services. Fig. 1 illustrates how different types of blackmarket services provide collusive appraisals to
various online media platforms.

4.1 Social Networks
Social networks serve as a platform to build social relations among users of common interests (personal or
professional). Platforms like Twitter, Facebook and Instagram allow their users to perform actions such as post,
like, retweet, follow, share, etc. Nowadays, these platforms also serve as real-time news delivery services and a
medium for the business owners to connect with their customers and thus expand their outreach. However, the
natural way to attract users is usually a tedious task and takes significant time. This motivates users to choose an
artificial way of gaining appraisals.

Facebook. Facebook is the most popular social network to connect and share with family and friends online.
Facebook has four types of appraisals: shares, likes, comments, followers. Large number of appraisals on Facebook
acts as a form of social proof for the sign of popularity and importance.

Twitter. Twitter is a microblogging service where users write tweets about topics such as politics, sport, cooking,
fashion, etc. Twitter has three types of appraisals: retweets, likes and followers. Acquiring more appraisals on
Twitter helps to increase the user’s social signals and attract more visitors to the profile.

Instagram. Instagram is a social networking platform that enables users to share images or videos with their
audience. Users can upload photos, videos and share them with their followers or with a group of friends.
Instagram has three types of appraisals: likes, followers and views. Higher appraisals are the key to visibility on
Instagram. The more appraisals a post receives on Instagram, the higher is the post rank in search results and on
the Explore page.

9

Pinterest.: Pinterest is an image-sharing platform that is designed as a visual discovery engine for finding ideas
like recipes, home and style inspiration, etc. A message on Pinterest is known as Pin. Pinterest has two types of
appraisals: likes and followers.

Other than the common appraisals, we found few blackmarket services where social networking users can
request for a verification badge. Verifiedbadge6, StaticKing7, Prime badges8 and SocialKing9 are the most
popular platforms providing such appraisals. Verification badges are coveted checkmark badges that enhance the
user’s social media presence and improve credibility on the platform. The minimum requirements to request for
verification badges through blackmarket services are as follows:

• The user/organization should be a celebrity, journalist, popular brand, government official or sports

company.

• The user/organization should have a Wikipedia page or media coverage in leading online news portals.
• For social media platforms where subscription is an appraisal, the user should have a minimum of 100K

subscribers.

There is a vast literature on the detection and analysis of collusive entities in social networks. Shah et al.
[103] and Dutta et al. [40] are two of the first few studies that investigate collusive entities on Twitter registered
in blackmarket services. Dutta et al. [40] trained multiple state-of-the-art supervised classifiers using a set of
64 features to distinguish collusive users from genuine users. They also divided the set of collusive users into
three categories: bots, promotional customers and normal customers. Arora et al. [7] obtained better classification
performance than Dutta et al. [40] by incorporating the content-level and network-level properties of Twitter
users in a multi-task setting. De Cristofaro et al. [35] performed the first work on Facebook where the authors
presented a comparative measurement study of page promotion methods. Sen et al. [100] conducted the first
work on Instagram where they developed an automated mechanism to detect fake likes on Instagram. We discuss
in detail about these studies in Section 6.

4.2 Rating/Review Platforms
Rating/Review platforms allow users to rate or share their opinion about entities, e.g., products, applications,
food items, restaurants, movies, etc. Examples of these platforms are e-commerce platforms (e.g., Amazon), travel
platforms (e.g., TripAdvisor), business rating platforms (e.g., Yelp, Google review), etc.

Amazon. Amazon is the world’s largest e-commerce platform and is considered the ultimate hub for selling
merchandise on the web. It allows two types of appraisals: reviews and ratings. High ratings and reviews for a
product on Amazon attract customers and make the product more trustworthy.

Google. Google provides valuable information to businesses and its customers using two types of appraisals:
reviews and ratings. Higher ratings and reviews on Google help to improve the business and enhance local search
rankings.

TripAdvisor. TripAdvisor is an online travel platform that offers multiple services such as online hotel reser-
vations, travel experiences, restaurant reviews, etc. Similar to other platforms, TripAdvisor has two types of
appraisals: reviews and ratings. Positive reviews from previous occupants help the business to improve its
reputation and increase the customer base.

Yelp. Yelp is a local business review platform that collects crowdsourced data from its users. Yelp has two types
of appraisals: reviews and ratings. Getting more reviews in turn improves business reputation and gets a lot more
customers with free traffic.

6https://verifiedbadge.co/
7https://www.staticking.com/
8https://primebadges.com/
9https://www.socialking.in/

10

Nowadays, online reviews/ratings are becoming highly relevant for customers to take any purchase-related
decisions. As these appraisals play a significant role in deciding the sentiment/popularity of a product/business,
there is a massive scope for collusion among sellers/buyers to manipulate it artificially. [66, 79, 123] are a
few initial studies on detecting fake reviews in review patterns representing unusual behaviors of reviewers.
Another early work by Li et al. [80] identified fake reviews on Dianping, the largest Chinese review hosting site.
The authors proposed a supervised learning algorithm to identify fake reviews in a heterogeneous network of
users, reviews and IP addresses. Mukherjee et al. [90] performed one of the first attempts to detect fraudulent
reviewer groups in e-commerce platforms. Recently, Kumar et al. [73] identified users in rating platforms who
give fraudulent ratings for excessive monetary gains. Most of the studies in review platforms are focused on
e-commerce platforms. We believe the newer platforms such as Google reviews, Yelp and TripAdvisor would
open more research questions as these platforms contain reviews of millions of hotels, restaurants, attractions
and other tourist-related businesses.

4.3 Video Streaming Platforms
Video streaming platforms are mostly used for sharing videos and streaming live videos. These platforms allow
users to upload, view, rate, share, add to playlists, report, comment on videos and subscribe to other users.
Here, we discuss how appraisals on video streaming platforms are inflated artificially. With the increase in the
popularity of live streaming came the concept of “astroturfing” – a broader and sophisticated term referring to the
synthetic increase of appraisals in an online social network by means of blackmarket services. The consequences
of such synthetic inflation are not only restricted to increase monetary benefits, directory listings and partnership
benefits, but also expanded to better recommendation rankings thus doctoring the experiences of viewers who
are recommended all these boosted materials instead of genuine materials produced by the honest broadcasters.
YouTube. YouTube is a video sharing platform where users can create their own profile, upload videos, watch,
like and comment on other videos. YouTube has four types of appraisals: likes, comments, subscribers, views. Likes,
comments and views are for the videos and subscribers are for the channels. Higher the number of YouTube
users interacts with a video/channel, higher that video/channel will be listed to other users. These appraisals are
considered as the measure of engagement and not only make the entity popular but also help the channel with
sponsorship opportunities and monetization options.

Twitch. Twitch is the most popular live streaming platform on the web. It is mostly used by gamers to stream
their games while other users can watch them. Twitch has two types of appraisals: followers and views. The
higher views a channel has, the higher is its popularity on Twitch and to be ranked on the featured list. Twitch is
usually considered to be one of the most difficult platforms to earn quick popularity due to the presence of so
many streamers using the platform.

Tiktok. Tiktok is a video sharing social network which is primarily used to create short videos of 3 to 15
seconds. Tiktok has two types of appraisals: likes and followers. Getting more likes on Tiktok posts increases the
chance to maintain its presence and make it famous amongst the audience.

Vimeo. Vimeo is a video hosting and sharing platform that allows users to upload and promote their videos
with a high degree of customization which is not available on other competing platforms. Vimeo has two types
of appraisals: likes and followers. A higher number of appraisals helps to show up the video in the suggested
videos list created by Vimeo’s algorithm.

Few studies addressed issues of astroturfing in video streaming platforms. Shah [101] made the pioneering
attempt to combat astroturfing on live-streaming platforms. He proposed Flock, an unsupervised method to
identify botted broadcasts and their constituent botted views. Note that he didn’t disclose the name of the live
streaming corporation on which the study was performed. In a recent study, Dutta et al. [42] proposed CollATe,
an unsupervised method to detect collusive entities on YouTube. The method utilizes the metadata features,
temporal features and textual features of a video to detect whether it is collusive or not.

11

4.4 Recruitment Platforms
Recruitment platforms are employment-oriented platforms that also provide professional networking among
users. These platforms offer the opportunity to discover new professionals either locally or internationally
and help one with their professional endeavors. Hiring new employees is a crucial part of every organization,
which starts with posting new job ads and ends with recruitment. It can be thought of as a multi-step process,
which is normally very time-consuming and prone to human errors. With the advent and rise of automated
systems, the hiring process of an organization is being done in the cloud with the help of tools such as Applicant-
Tracking-System (ATS)10. ATS makes the hiring process faster and accurate by preparing job ads, posting them
online, collecting applicant resumes, making efficient communication with them and finding the best fit resumes
for the organization. However, increasing use of ATS also invokes various disadvantages such as spammers
compromising the job seekers’ privacy, slandering the reputation of the organizations and financially hurting it
by manipulating the normal flow of functioning of the system - most frequently the job ads publishing process
(often recognized as the employment scam).

LinkedIn. LinkedIn is the most popular online recruitment platform where employers can post jobs, and
job seekers can post their profiles. There are four types of appraisals on LinkedIn: followers, recommendations,
endorsements, and connections. A higher number of connections and followers on LinkedIn helps the user to gain
attention. LinkedIn endorsements help to add validity to the user’s profile by backing up his/her work experience.
Adikari and Dutta [1] identified fake profiles on LinkedIn. The authors considered state-of-the-art supervised
classifiers designed on a set of profile-based features to detect fake profiles. Prieto et al. [92] detected spammers
on LinkedIn based on a set of heuristics and their combinations using a supervised classifier. Another work
by Vidros et al. [118] tackled the problem of Online Recruitment Frauds (ORF) (see Sec. 6 for more details).
The authors also released a publicly available dataset of 17, 880 annotated job ads (17, 014 legitimate and 866
fraudulent job ads) from various recruitment platforms.

4.5 Discussion Platforms
Discussion platforms are content sharing platforms primarily used to entertain community-based discussions.
The discussions can be in the form of question-and-answers or content that other users have submitted using
links, text posts or images. Here, we explain how collusion happens on discussion platforms:

Quora. Quora is a question-and-answer based discussion forum that empowers the user to ask questions on
any subject and connect with like-minded people who contribute unique insights and high-quality answers.
Quora has four types of appraisals: followers, upvotes, downvotes and comments. Higher the number of followers
and upvotes a user gains for his/her answers, higher is the ranking factor and the more the answers are displayed
to other users. The aim of a user on Quora is to be named as a top writer in the long run. Answers written by top
writers on Quora are considered as expert opinions and are also displayed in the featured list.

ASKfm. ASKfm is another question-and-answers based discussion forum and is mostly used by users to post
questions anonymously. The platform has two types of appraisals: followers and likes. Higher likes on answers
grow its rating on ASKfm at a faster rate.

Reddit. Reddit is a social news-based discussion forum that allows users to discuss and vote on content that
other users have submitted. Reddit has five types of appraisals: subscribers, upvotes, downvotes, karma and
comments. Having more karma on Reddit allows the user to post more often on the platform and gives him/her
more reputation. More number of upvotes on posts helps users gain more exposure, which eventually pushes the
posts higher up on the targeted Reddit or Subreddit.

Studies in discussion platforms have been conducted with the goal of manipulating the visibility of political
threads on Reddit [22]. The authors measured the effect of manipulation of upvotes and downvotes on article

10https://en.wikipedia.org/wiki/Applicant tracking system

12

visibility and user engagement by comparing Reddit threads whose visibility is artificially increased. Another
work by Shen and Rose [105] investigated polarized user responses on an update to Reddit’s quarantine policy.

4.6 Music Sharing Platforms
Music sharing platforms enable users to upload, promote and share audio. Here, we explain how collusion
happens on music sharing platforms.

Soundcloud. Soundcloud is an audio sharing platform that connects the community of music creators, listeners
and curators. Soundcloud has three types of appraisals: plays, followers and likes. A higher number of followers
and likes creates a massive fan base for creators and gets more attention from the community. The popularity of
a soundtrack in the platform is driven by the number of plays it receives. Plays attract users to hear the music as
they feel to check something which is liked by most of the other users.

Reverbnation. Reverbnation is an independent music sharing platform where musicians, producers and venues
collaborate with each other. The platform has two types of appraisals: plays and fans. Higher the number of plays
in an audio or video, higher is the rank of the artist on the platform.

Some studies have been conducted on fraudulent entity detection in music sharing platforms. Bruns et al. [19]
investigated Twitter bots that help in promoting SoundCloud tracks. The authors also proposed a number of
social media metrics that help to identify bot-like behavior in the sharing of such content. Another work by [96]
proposed a method to distinguish between two groups of SoundCloud accounts – bots and humans.

4.7 Development Platforms
Interestingly, we found a few popular development platforms where collusion happens.

GitHub. GitHub is a repository hosting service that provides distributed version control and source code
management (SCM) functionality. GitHub has three types of appraisals: followers, stars and forks. User profiles
with more followers make the account more popular. Similarly, stars and forks are the metrics to show the
popularity of a repository. Blackmarket services help to deliver GitHub followers, stars and forks from real and
active people.

Hackernews. Hackernews is the most popular discussion platform for developers. It has three types of appraisals:
upvotes, karma and comments. Higher the number of comments and upvotes on a post, higher is its popularity. User
profiles with high karma can perform additional appraisals on a post: downvote, making polls etc. Blackmarket
services help to deliver upvotes and karma on Hackernews simply by adding the post link and making the
payment.

Medium. Medium is an online publishing platform that is commonly used by developers to share ideas,
knowledge and perspectives. It has two types of appraisals: followers and claps. A higher number of claps in
a post ranks it higher in feed and search results. Similarly, a higher number of followers increases the post’s
reach and view. Blackmarket services help to deliver followers and claps by adding the post link and making the
payment.

Most of the studies in development platforms are on measuring user influence and identifying unusual commit
behavior by analyzing the attributes of the platforms. Hu et al. [59] measured the user influence on GitHub
using the network structure of the following relation, star relation, fork relation and user activities. The authors
also introduced an author-level H-index (also known as H-factor) to measure users’ capability. Goyal et al. [52]
identified the unusual changes in the commit history of GitHub repositories. The authors designed an anomaly
detection model based on commit characteristics of a repository. One potential research direction is to examine
how collusive appraisals on these platforms help to popularize the reputation of the repositories/users. Another
potential research direction is to develop collusive entity detection techniques by considering the hidden relations
between the entities of the platform.

13

4.8 Other Platforms
Other than the online media, artificial boosting is also observed in other platforms. Website owners can avail
premium/freemium blackmarket services to get traffic on their website. The idea is to endow the responsibility
for the necessary amount of web traffic to some other company. In blackmarket terms, the appraisal is called
as “hits” or “web traffic”. Gaining artificial hits helps the website to gain popularity faster compared to the slow
process of working on growing organic visitors via Search Engine Optimization. Blackmarket services offer two
types of web traffic: Regional traffic where the customers can opt for traffic from a specific country or region
and Niche traffic where customers can opt for targeted traffic that focuses on a specific type of business such
as music-based, e-commerce based, etc. To activate traffic to a website, customers have to enter the URL of the
website, the number of visitors required and preferred timespan for delivery.

5 TYPES OF COLLUSIVE ACTIVITIES
Collusive activities can be categorized based on the mode of collaboration - individual collusion and group collusion.
In this section, we will discuss the two types of collusion and how the individual collusion differs from group
collusion when providing collusive appraisals.

Individual Collusion

5.1
Individual collusion happens when the collusive activities of individuals are independent of each other; however,
they are guided by centralized blackmarket authorities. Understanding individual collusion has been studied to
some extent in the literature. Most of the existing studies used supervised models based on behavioral features
and profile features [40]. Some network-based approaches infer anomaly scores for nodes/edges in the network
(tweet-user network, product-user network, etc.) and rank them to spot suspicious users [28, 58, 73, 102, 123].
Existing studies mostly focused on the behavioral dynamics of individuals. However, these approaches fail when
it comes to detecting group collusion. Group collusion can be more damaging as they can take the total control
of the appraisals for an entity due to its size.

5.2 Group Collusion
Group-level collusion takes place when a set of individuals collaborate as a group to perform collusive activities.
Such collective behavior is, therefore, more subtle than individual behavior. At an individual level, activities might
be normal; however, at the group level, they might be substantially different from the normal behavior. Moreover,
it may not be possible to understand the actual dynamics of a group by aggregating the behavior of its members
due to the complicated, multi-faceted and evolving nature of inter-personal dynamics [37]. Some properties of
collusive groups are as follows – (i) members in collusive group work in shorter time frames and create maximum
impact (e.g., flooding deceptive opinions), (ii) members of the group may or may not know each other (agreement
by a contracting agency), (iii) multiple accounts within a group can be controlled by a single master account
(existence of Sockpuppets [72]), (iv) larger the size of the group, the more damaging the group is, and (v) group
members have a high chance of performing similar activities (group members posting similar reviews or writing
same comments). Past studies in this direction mostly detected groups using Frequent Itemset Mining (FIM) and
ranked groups based on different group-level spam indicators [55, 124]. Wang et al. [125] pointed out several
limitations of FIM for group detection – high computational complexity at low minimum support, absence of
temporal information, unable to capture overlapping groups, prone to detect small and tighter groups, etc. Liu
et al. [83] proposed HoloScope to find a group of suspicious users in rating platforms based on the contrasting
behavior of fraudsters and honest users in terms of topology, temporal spikes and rating deviation.

14

6 PROGRESS IN COLLUSIVE ENTITY DETECTION
Despite the increasing interest in analyzing and designing anomalous activities on the web, there has been limited
work in collusive entity detection. Most of the previous studies in collusive entity detection are limited to social
networks and rating platforms. In this section, we provide a summary of the reviewed papers to summarize the
central idea and provide the basics of the models. To structure the descriptions, the relevant papers are grouped
by the type of approaches proposed in the paper. We categorize the methodologies followed by the previous
studies into one of the following types: (i) feature based, (ii) graph based, and (iii) deep learning based.

6.1 Feature Based Detection
The majority of the papers model the behavioral properties of the users in online media platforms. Feature-based
methods can be used to distinguish between collusive entities from genuine entities. The aim is to design a
set of features that can (well) represent and capture various behavioral characteristics of the entities. Recently,
researchers have also focused on the linguistic behavior of the collusive entities, such as deceptive information
[6, 50], lexical analysis [61] and sentiment analysis [38].

Dutta et al. [40] reported that collusive users show an amalgamation of organic and inorganic activities and there
is no synchronicity among their behaviors. They substantiated their finding by comparing collusive retweeting
behavior with that of normal retweet fraudsters. Here, normal retweet fraudsters are those retweeters who are
abnormally synchronized in some patterns. The authors then collected data from four freemium blackmarket
services (credit-based services). Three human annotators were asked to label the blackmarket customers into bots,
promotional customers and normal customers based on a set of annotation rules. The data is used to tackle two
problems - (i) a four-class classification problem (genuine, bot, promotional, normal), and (ii) a binary classification
problem (genuine and combining all types of customers into a single class). In order to detect collusive retweeters,
the authors developed ScoRe, a supervised model to detect collusive retweeters based on five sets of features:
profile features, social network features, user activity features, likelihood features and fluctuation features. The
authors also developed a chrome browser extension to detect the collusive retweeters in real-time. To study
the underlying behavior of the blackmarket services, Dutta and Chakraborty [39] extended their previous work
[40] to provide an in-depth analysis of collusive users involved in premium and freemium blackmarket services.
The authors collected the dataset from four premium and four freemium blackmarket services. They analyzed
the activities of collusive users based on retweet-centric, network-centric, profile-centric and timeline-centric
properties. They showed that unlike premium services, the credit-system in freemium services is the primary
reason behind the unusual functioning of freemium collusive users. They also developed an updated version
of the chrome extension by incorporating the features calculated from both premium and freemium users. Fig.
5 shows the wordcloud of the text present in the description of premium and freemium retweeters collected
from the blackmarket services. It can be seen that premium retweeters are associated with high profile accounts
having keywords such as ‘CEO’, ‘official’, ‘speaker’, ‘founder’. However, freemium retweeters are associated with
advertising agencies having keywords such as ‘like’, ‘agency’, ‘SocialMedia’ and ‘YouTubeMarketing’.

Dutta et al. [41] proposed HawkesEye, a classifier based on the Hawkes process and topic modeling to detect
collusive retweeters on Twitter. They collected tweets using three hashtags: #deletefacebook, #cambridgeanalytica,
#facebookdataleaks and manually annotated the retweeters as “fake” or “genuine”. The HawkesEye model takes
the temporal information of retweet objects of a user and topical information of the retweet texts as input. The
temporal information is modeled using the Hawkes process due to its self-excitation property, and the topical
information is modeled using Latent Dirichlet allocation (LDA). The authors reported highly accurate results on
the classification task. Comparisons were made with respect to well-known bot detection approaches [24] and
collusive retweeter detection approaches [40].

15

More recently, Arora et al. [7] improved the collusive retweeter detection task by considering the multifaceted
characteristics of a collusive user. They created multiple views for a user by examining representations from
the content, attributes and social network. They then proposed a multiview learning-based approach based on
Weighted Generalized Canonical Correlation Analysis (WGCCA) to combine individual representations (views)
of a user to derive the final user embeddings. The authors reported that as each view is more and less helpful for
the classification experiment (c.f. t-SNE visualizations in fig. 6), they assigned a weight to each view differently
instead of treating each view equally.

Several research studies have been conducted about the automatic detection of collusive followers on Twitter.
In an earlier study, Liu et al. [84] proposed DetectVC to detect voluntary followers (volowers). They showed how
volowers gain profit by following enough users for self and product promotion. However, do these users act as a
group to perform malicious activities? To answer this, Gupta et al. [55] proposed approaches to detect malicious
retweeter groups. They found that the activities appear normal at the individual level. However, at the group level,
they look suspicious and group activities vary across groups. Jang et al. [63] is the first work that detects collusive
followers using geographical distance. The authors reported that when the distance between the legitimate users
and collusive users increases, the legitimate users’ follower ratio between the number of followers and the total
number of followers decreases. Aggarwal et al. [2] detected users on Twitter with manipulated follower count
using an unsupervised local neighborhood detection method.

Other studies have looked at how collusion happens on Facebook and e-commerce platforms. De Cristofaro
et al. [35] conducted an in-depth analysis of Facebook pages where they collected likes via Facebook ads and other
farms. The authors monitored the “liking” activity by creating honeypot pages and crawling them after every 2
hours to check for new likes from the blackmarket services. Badri Satya et al. [11] conducted an in-depth analysis
of fake likers on Facebook collected from the blackmarket services. They reported that fake likers behave very
differently than genuine likers in terms of their liking behaviors, longevity, etc. Mukherjee et al. [90] proposed
an unsupervised approach to detect and rank collusive spammer groups in Amazon. They used Frequent Itemset
Mining (FIM) to extract the set of reviewer groups. The extracted reviewer groups were labeled by the domain
experts into spammer and genuine groups. Their proposed method GSRank considers various group-level spam
behavioral indicators, individual spam behavioral indicators and then models the inter-relationship between
reviewer groups, members of those reviewer groups and products they reviewed to find the spammer groups.

Studies have also looked at artificial manipulation of viewership/comment/subscription count in the video
streaming platforms such as YouTube, Twitch, etc. Shah [101] introduced FLOCK, an unsupervised multi-step
process for identifying botted views and botted broadcasts. FLOCK performs the following steps: (a) it models
a normal broadcast as a collection of multiple views; (b) it classifies whether a given broadcast has inflated
popularity by looking at the collective behavior of all the views in a broadcast; (c) it classifies whether individual
views of the broadcast are botted. For modeling the normal broadcast behavior, the authors included temporal
features of the constituent views of broadcast, such as start time and end time of a particular view. It is also worth
noting that they avoided using descriptive features (like browser used, country of origin) and engagement-based
features to make the proposed solution easier. In a very recent study, Dutta et al. [42] proposed three models to
detect three types of collusive entities on YouTube: (a) videos submitted for collusive like requests, (b) videos
submitted for collusive comment requests, and (c) channels submitted for collusive subscription requests. They
analyzed the videos submitted to blackmarket services for collusive appraisals (likes and comments) based on two
perspectives – propagation dynamics and video metadata. They also analyzed the collusive YouTube channels
based on location, channel metadata, and network properties. Their analysis on the structural properties of the
giant component present in the collusive channel network shows that it is a small-world. In order to detect the
collusive videos and collusive channels, they designed one-class classifiers based on features calculated from
video metadata and temporal information. The models were evaluated in terms of true positive rate, achieving
state-of-the-art results. The authors also proposed CollATe, a denoising autoencoder model that leverages the

16

Fig. 6. t-SNE visualization of representations of collusive (red) and genuine (green) users created using (a) Tweet2Vec, (b)
SCoRe[40], (c) Retweet network, (d) Quote network, (e) Follower network, (f) Followee network. Reprinted with permission
from Arora et al. [7].

power of three components: metadata feature extractor, anomaly feature extractor and comment feature extractor
to learn feature representation of videos. Note that CollATe can only detect videos submitted in blackmarket
services for collusive comment requests as unlike temporal information of like activity, the temporal information
of the comments is publicly available in the YouTube API11. Fig. 7 shows the snapshot of collusive entities
detected using their model. Fig. 7(a) shows a video where the number of likes is much higher than the number of
views. The authors claimed that the blackmarket services use YouTube API using the credentials of its users to
help these videos gain likes. Similarly, Fig. 7(c) shows the snapshot of a video submitted to blackmarket services
for collusive comment subscriptions with a higher number of comments as compared to views and likes. Fig.
7(b) shows a YouTube channel that was submitted for collusive subscriptions. The authors claimed that collusive
YouTube channels make proper use of promotional keywords to attract more subscribers to their channel.

The work by Vidros et al. [118] is the only work that deals with recruitment based fraud detection. The authors
introduced this problem as Online Recruitment Fraud (ORF), more specifically relating to employment scams.
The dataset used in this work contains numerous job ads and their corresponding annotations (legitimate or
fraudulent). They performed an in-depth analysis of the dataset by applying standard state-of-the-art machine
learning classifiers. In these cases, the trapped user(s) unknowingly become a medium for scammers to complete
their jobs, often even driving the individual to complete a direct wire transfer for them under the careful and
believable disguise of working visa or travel expenses. With this much being said about employment scams, it is
worth noting that although they share some similar characteristics with problems such as email phishing, online
opinion fraud, trolling, cyber-bullying, they are very much different in many aspects.

11https://developers.google.com/youtube/v3

17

(a)

(b)

(c)

Fig. 7. Example of (a) collusive video (for likes), (b) collusive channel (for subscriptions), and (c) collusive video (for comments)
detected by our models. Sensitive information are blurred. Reprinted with permission from [42].

Limitation. The main limitation is that in cases where user features are used, it can be restrictive to a particular
online media platform and not generalize to others. Moreover, if collusive activities are performed by new online
media accounts, it is difficult to collect user behaviors for such accounts. The drawback of using linguistic features
is the lack of generalizability across multiple languages and circumstances. It has been reported in the literature
[5, 76] that linguistic features designed for one circumstance may not work perfectly for other circumstances.

6.2 Graph Based Models
In this section, we present and discuss the graph-based models for collusive entity detection. Recently, graph-
based models have been developed particularly for spotting outliers and anomalies in networks. The advantage of
using graph techniques is to efficiently capture long-range correlations among the inter-dependent data objects
[4].

Chetan et al. [28] proposed CoReRank, an unsupervised approach to capture the interdependency between
the credibility of users and the merit of tweets using a recurrence formulation based on network, behavioral
information of users and topical diversity of tweets. The authors created a directed bipartite graph by modeling the
interactions between users and tweets in terms of retweets/quotes. They finally reported suspiciousness scores for
both users and tweets in the graph based on the recurrent formulation. By incorporating prior knowledge about
the collusive users, they proposed a semi-supervised version of CoReRank, called CoReRank+. Both CoReRank and
CoReRank+ outperformed their respective state-of-the-art methods by a significant margin.

Rayana and Akoglu [94] proposed SpEagle, a framework that considers metadata of reviews (review content,
rating, timestamp) along with relational data (reviewer-review-product graph) for detecting spam users, fake
reviews and the products targeted by spammers. SpEagle used metadata for extracting the spam features, which
were then transformed into a spam score to decide the genuineness of users and reviews. It was also extended to
a semi-supervised setting (called SpEagle+) to improve its performance. The authors also implemented a lighter
version (SpLite) that leverages a small subset of review features for the detection task to achieve a boost in
runtime. Wang et al. [125] proposed a bipartite graph projection-based approach, called GSBP to detect review
spammer groups in e-commerce websites. Unlike FIM based methods [90], their proposed approach detects
loosely connected spammer groups – each group member may not review every target product of the group.

Kumar et al. [73] developed a fraudulent reviewer detection system, called REV2 for rating platforms. They
proposed three interdependent metrics – fairness of a user in rating an item, reliability of a specific rating, and
goodness of a product. By combining network and behavioral properties of users, products and ratings, REV2
calculates fairness, reliability, goodness scores in both supervised and unsupervised settings. They evaluated the

18

(a)

(b)

Fig. 8. (a) Schematic diagram of DeFrauder to detect online fraud reviewer groups. (b) Coherent review patterns of four
spammers in a fraudulent reviewer group. Reprinted with permission from [37].

computed scores by using five real-world datasets – Flipkart, Bitcoin OTC, Bitcoin Alpha, Epinions and Amazon.
The current version of REV2 is deployed at Flipkart (an Indian e-commerce company).

Wang et al. [124] proposed a graph-based framework, called GGSpam to detect review spammer groups. It
recursively splits the entire reviewer graph into small groups. GSBC, the key component of GGSpam, finds the
spammer groups by identifying all the bi-connected components within the split reviewer graphs whose spamicity
score exceeds the given threshold. They proposed several group spam indicators to compute the spamicity score
of a given reviewer group. They conducted experiments on two real-world datasets – Amazon and Yelp.

Dhawan et al. [37] proposed DeFrauder, an unsupervised approach to detect online fraud reviewer groups.
This approach initially detects the candidate fraud groups by leveraging the underlying product review graph
and incorporating several behavioral signals which model multi-faceted collaboration among reviewers. It then
maps the reviewers into an embedding space and assigns a spam score to each group, such that groups consisting
of spammers with highly similar behavioral traits attain high spam score. They conducted experiments on
four real-world datasets – Amazon, PlayStore, YelpNYC [94] and YelpZip [95]. Fig. 8(a) shows the schematic
diagram of DeFrauder to detect online fraud reviewer groups. Fig. 8(b) shows the coherent review patterns of
four spammers in a fraudulent reviewer group.

Liu et al. [83] formulated a novel suspiciousness metric from graph topology and spikes to detect fraudulent
users in multiple rating platforms (BookAdvocate, Amazon). The authors proposed HoloScope, an unsupervised
approach that combines suspicious signals from graph topology, temporal bursts and drops, and rating deviation.
The primary goal behind the approach is to obstruct the fraudsters by increasing the time they need to perform
an attack. The evaluation was done on multiple semi-real (synthetic) and real datasets.

Shin et al. [110] proposed two algorithms – M-Zoom (Multi-dimensional Zoom) and M-Biz (Multidimensional
Bi-directional Zoom) to detect suspicious lockstep behavior in online review platforms. The task of detecting
lockstep behavior is to find out a set of accounts that give fake reviews to the same set of products/restaurants.
The objective of M-Zoom and M-Biz is to detect dense subtensors in a greedy way until it reaches a local optimum.
Though the overall structure of M-Biz and M-Zoom is the same, it differs significantly in the way they find each
dense subtensor. Using the above algorithms, the authors detected three dense subtensors that indicated the
activities of bots, which changed the same pages hundreds of thousands of times.

Mehrotra et al. [86] detected fake followers on Twitter using graph centrality measures. The authors created a
graph using the follower and friend network from a set of Twitter users. Next, six centrality-based features were
used for the classification experiment. Using this feature set with a Random Forest classifier gave an accuracy

19

of 95%. Zhang and Lu [137] proposed a graph-based approach using near-duplicates to detect fake followers in
Weibo, a Chinese counterpart of Twitter. They were able to discover 11.90 million users in Weibo who bought
followers from blackmarket services.

Shin et al. [107] detected three empirical patterns of core users in real-world networks across diverse domains.
The first pattern is the MIRROR PATTERN, which states that the coreness of a vertex is strongly related to its
degree. The second pattern CORE-TRIANGLE PATTERN states that the degeneracy (non-core component) and the
triangle-count obey a power-law with slope 13. The third pattern STRUCTURED CORE PATTERN states that the
degeneracy cores are not cliques. Using these patterns, the authors analyzed the lockstep behavior for collusive
followers on Twitter. It was reported that at least 78% of the vertices with high coreness values use blackmarket
services to boost their followers.

Limitation. Graph-based methods are computationally intensive which can restrict their ability to design

real-time tools to detect collusive entities in online media platforms.

6.3 Deep Learning Based models
The advent of deep learning has alleviated the shortcomings of the previous models with the power to learn more
complex representations that are difficult to be captured by traditional machine learning models.

Arora et al. [8] proposed a multi-task learning approach to detect collusive tweets. The authors collected
the data from two blackmarket services - YouLikeHits and Like4Like after creating honeypot accounts in these
services. The multi-task model takes two inputs – the feature representation extracted from the tweet metadata
and Tweet2vec representation of the tweets, which are passed through a cross-stitch neural network to learn
the optimal combination of the inputs via soft parameter sharing. Ahsan et al. [3] tackled the problem of fake
reviewers using an active learning approach with the TF-IDF features of the review content.

Studies have also looked at the detection of social spammers in online media platforms. Wu et al. [127] proposed
an end-to-end deep learning model based on Graph Convolution Networks (GCNs) that operates on directed
social graphs to detect social spammers. The authors built a classifier by performing graph convolution on the
social graph with different types of neighbors.

In a study to identify collusive entities on YouTube, Dutta et al. [42] proposed CollATe, a deep unsupervised
learning method using denoising autoencoders. The CollATe architecture consists of three components – metadata
feature extractor, anomaly feature extractor, and comment feature extractor to learn video feature representations.
The final representation is then fed to a collusive video detector module that returns the label – collusive or
non-collusive for the input video.

To detect collusive followers on Twitter, Castellini et al. [23] proposed a deep learning technique using a
denoising autoencoder. The denoising autoencoder is implemented as an anomaly extractor and is trained with
examples only from the features generated from the real profiles. The authors built a test set with both real
and fake profiles. Once they got the reconstruction error calculated using the autoencoder for a data point, it
was then compared to a threshold value (computed from the training set): if the error is higher, it is marked as
collusive (i.e., the autoencoder is not able to properly reconstruct the record), otherwise, it is marked as real.

Limitation. The main limitation of the deep learning methods is the requirement of vast amounts of human-
annotated data. Moreover, due to the unusual behavior of collusive entities due to the mixture of organic and
inorganic activities, it is difficult to find correlations between sets of features which makes the deep learning
techniques to not perform well.

6.4 Other Methods
In contrast to the above approaches which were largely focused on collusive activities in online media platforms,
a series of works have been conducted on detecting collusive activities in other platforms.

20

Asavoae et al. [9] developed a fully automated and effective detection system to detect colluding android apps
that violate the permissions causing data leaks or distributing malware over multiple apps. The authors first
proposed a rule-based approach to identify colluding apps. As defining rules requires expert knowledge and for
some cases explicit rules may not exist, the authors proposed a probabilistic model to calculate two likelihood
components for an app: (i) the likelihood of carrying out a threat and (ii) the likelihood of performing an inter-app
communication.

Blasco and Chen [16] proposed Application Collusion Engine (ACE), a system to automatically generate
colluding apps with a variety depending on the configuration of app component templates and code snippets.
The ACE system has two main components: colluding set engine and application engine. The first component tells
the second component how it should create apps in order to collude. The second component is responsible for
creating fully working android apps that are ready to be installed in a device. The primary aim of this work is to
create substantial colluding app sets for experimentation as representative datasets do not presently exist for
colluding apps. Kalutarage et al. [67] proposed a probabilistic model towards an automated threat intelligence
system for app collusion. The readers are encouraged to go through [15] for a detailed survey on the app detection
techniques that focus on collusive activities by investigating inter-app communication threats.

7 ANNOTATION GUIDELINES, DATASETS, APPLICATIONS AND EVALUATION METRICS
As collusive entity detection is a considerably new research area, most of the previous studies suggested few
guidelines which may be helpful to label these users. In this section, we first review the annotation guidelines for
creating annotated datasets for collusive entities. We then provide pointers to the previous studies which have
released various datasets for collusive entity detection. Next, we discuss the tools and interfaces in the analysis
and detection of collusive entities in online media. Finally, we describe the common evaluation metrics used for
collusive entity detection.

7.1 Annotation Guidelines
Annotation guidelines are usually seen in studies that create a labeled set of entities for various experiments, as
mentioned below.

The dataset of Dutta et al. [40] is one of the most widely used dataset [7, 39] on collusive retweeter detection.
The dataset is created from the blackmarket services and the annotators were asked to label each retweeter by
one of the three classes: Bots, Promotional Customers, Normal Customers. For instance, Bots can be selected if the
account is controlled by software. The promotional customers can be identified if the accounts are involved in
promoting brands using keywords such as ‘win’, ‘ad’, ‘Giveaway’. The normal customers class can be selected if
the account does not fall under any of the above categories. The annotators were also given complete freedom to
search for any information related to collusive entities on the web and apply their own intuition.

In [128], a corpus of collusive spammers in a Chinese review website is described. The task of the annotator is
to label each reviewer in a reviewer group as either colluder or non-colluder. The annotators in this work were
computer science major graduate students who had extensive online shopping experiences. The annotators were
given a set of three instructions about the colluder class and how they should reason to arrive at the colluder
class when the data is not straightforward: (i) if the reviewer has too many reviews giving opposite opinions
than other reviews about the same stores, (ii) if the reviewer has too many reviews giving opposite opinions
about the same stores as compared to the ratings from Better Business Bureaus12, an organization that focuses on
advancing marketplace trust, and (iii) if the reviewer has too many reviews giving opposite opinions about the
same stores as compared to evidence by general web search results.

12https://www.bbb.org/

21

Table 2. A summary of the publicly available datasets of collusive entities in online media.

Name

Available entities

Type

Collusive users registered in freemium blackmarket services
Collusive users and tweets registered in blackmarket services
Collusive retweeters promoting popular hashtags
Collusive tweets registered in freemium blackmarket services
Collusive users registered in freemium/premium blackmarket services
Users involved in blackmarket-based following activities

ScoRe
CoReRank
HawkesEye
MTLCollu
ScoRe+
Fame4Sale
View WGCCA Collusive users registered in freemium blackmarket services
MalReg
DetectVC
FakeLikers
GSRank
GSBP
REV2
GGSpam
SpEagle
DeFrauder
EMSCAD
CollATe

Malicious retweeter groups
Voluntary following activities
Fake likers on Facebook
Review spammer groups
Loosely connected review spammer groups
Fraudulent users in rating platforms
Review spammer groups modeled as bi-connected components
Spam users, fake reviews and the targeted products
Fraud reviewer groups
Online recruitment frauds
Collusive videos and channels on YouTube

Individual Twitter
Individual Twitter
Individual Twitter
Individual Twitter
Individual Twitter
Individual Twitter
Individual Twitter
Twitter
Group
Sina Weibo
Individual
Individual
Facebook
Individual Amazon
Individual Amazon
Individual Amazon
Amazon
Group
Individual Yelp
Group
PlayStore
Individual LinkedIn
Individual YouTube

Platform Ref.
[40]
[28]
[41]
[8]
[39]
[33]
[7]
[55]
[84]
[11]
[90]
[125]
[73]
[124]
[94]
[37]
[118]
[42]

In [89], a corpus of collusive spammers in an e-commerce website is described. Three annotators were given
the task of examining the e-commerce users and provide a label as spammer or non-spammer based on three
opinion spam signals. Note that each of the annotators was domain expert, i.e., an employee of e-commerce
platforms such as Rediff Shopping, eBay.in, etc. As the annotators were domain experts, they were also given
access to detailed information, e.g., entire profile history, demographic information, etc.

Li et al. [79] built a review spam corpus. They crawled data from a consumer review site, named Epinions13.
The task of the annotator was to annotate the review spam dataset and label each review as spam or non-spam.
The annotators used the rules specified in a platform, named Consumerist14, an independent source of consumer
news and information published by Consumer Reports, to label the reviews. The platform suggests a set of 30
rules which are helpful for annotators to spot fake online reviews. The authors employed ten college students
to annotate their reviews. Each review was annotated by two students, and conflict was resolved by another
student.

In most of the above studies, Fleiss’/Cohen’s kappa value was reported to show the reliability of agreement

between the annotators.

7.2 Datasets
In this section, we present a list of publicly-available datasets used for the detection of collusive activities in
online media. Table 2 compares these datasets and lists the available entities, as mentioned in their respective
papers.

The Fame4Sale dataset [33] is a corpus of fake followers collected from various blackmarket services –
InterTwitter (INT), FastFollowerz (FSF) and TwitterTechnology (TWT). The dataset also contains the relationships
between the user accounts (followers/friends). The ScoRe dataset [39, 40] is a medium-sized corpus of collusive
users, collected from four freemium blackmarket retweeting services – YouLikeHits, Like4Like, Traffup and

13http://www.epinions.com/
14https://consumerist.com/

22

JustRetweet. It contains an anonymized version of 1, 941 users, who were manually annotated into four categories
(bots, promotional customers, normal customers and genuine users). The authors also reported the values of 64
features for each user, which were used in their approach to distinguish between collusive users and genuine users.
The CoReRank dataset [28] is another corpus of retweeters collected from two blackmarket services – YouLikeHits
and Like4Like. It contains the details of 4, 732 collusive users and 2, 719 genuine users. Liu et al. [84] released
the DetectVC dataset of voluntary followers containing 3, 250 volower IDs and their following relations. The
MalReg dataset [55] contains annotated groups of users that collude to retweet together maliciously. It consists
of 1, 017 malicious retweeter groups collected from three political events: UK General Election 2017, Indian
banknote demonetization 2016 and Delhi legislative assembly election 2013. The FakeLikers dataset contains
13, 147 likers, 4.66M pages, 0.99M posts and 5.47M friend relations from Facebook. There is only one available
dataset for recruitment frauds (EMSCAD) [118]. The dataset consists of 17, 880 annotated job ads (17, 014 legitimate
and 866 fraudulent) published between 2012 and 2014 and annotated by people having domain expertise. Each
job ad is described by a set of fields, and a label indicates whether the entry is fraudulent or not. There are a few
publicly available datasets for rating platforms. Dhawan et al. [37] released a large-scale dataset of reviews from
different applications available on Google Playstore. The dataset contains 3, 25, 424 reviews made by 3, 21, 436
reviews on 192 apps. YelpNYC [94] and YelpZip [95] are the datasets collected from Yelp, and used for detecting
collusive users/groups. YelpNYC dataset consists of review data related to 923 restaurants in New York City. It
includes 359k reviews and 160k reviewers. YelpZip is relatively large compared to the other two datasets. It
consists of 608k reviews and 260k reviewers, for 5k restaurants located in multiple states of the United States.
Dutta et al. [42] released their dataset on collusive YouTube entities. The dataset contains a large set of YouTube
videos (45, 572 videos for like appraisals, 25, 106 videos for comment appraisals) and channels (7, 847 channels for
subscription appraisals) submitted to the blackmarket services for collusive appraisals.

Interfaces and Applications

7.3
In this section, we discuss the tools and interfaces developed to detect collusion in online media. A list of
interfaces and applications is provided below with references to the corresponding studies as well. Fig. 9 shows
the working of two such applications.

(a)

(b)

Fig. 9. (a) Working of TweetCred to assess credibility of content on Twitter. (b) Working of Analisa to find the authenticity
of followers on Instagram.

23

• Dutta et al. [40] developed a chrome extension, called ScoRe to detect collusive users involved in
blackmarket-based retweeting activities. ScoRe also supports online learning by incorporating user
feedback.

• TwitterAudit15 and FollowerWonk16 services analyze Twitter accounts to check for fake followers.
• Botometer17 assigns a score (0-5) to an account based on its bot activities. Higher the score, higher the

chance that the account is being controlled by a bot.

• TweetCred [53] measures the credibility of a tweet based on user-centric and tweet-centric properties.
• FakeCheck, FakeLikes and IGAudit18 evaluate Instagram accounts for fake followers.
• TwitterPatrol [111] is a real-time tool to detect spammers, fake and compromised accounts on Twitter.
• Modash19 analyzes brands to find influencers on Instagram. It also has the functionality to detect fake

followers on a public user account.

• Analisa20 is a tool for bloggers and agencies to check the follower authenticity for an Instagram or

TikTok account.

Finally, some of the previous works have publicly released their resources and codes. Interested readers can
refer to the following studies: [7, 8, 37, 39, 40, 55] where public links to the resources and codes for collusive
entity detection and analysis are available. We believe that such studies are worthy of attention as they encourage
the reproducibility of the experiments and enable the follow-up studies to use the models as baselines to improve
the collusive entity detection models.

7.4 Evaluation Metrics
For any classification task, metrics such as precision, recall, and F-score are commonly used. However, in most of
the studies, we observe F-score to be more popular as it is a combined metric that conveys the balance between
the precision and recall. In general, F-score is calculated as the weighted harmonic mean of precision and recall,
with a beta parameter (β) which determines the weight of recall. In collusive entity detection, the most widely
used version of F-score is calculated as the macro-average of the F-scores for collusive and genuine classifications
as follows:

F =

2 × precision × recall
Precision + Recall

F =

Fcollusive + Fдenuine
2

Fcollusive =

2 × Precisioncollusive × Recallcollusive
Precisioncollusive + Recallcollusive

Fдenuine =

2 × Precisionдenuine × Recallдenuine
Precisionдenuine + Recallдenuine

Precisioncollusive =

Correctcollusive
Correctcollusive + Spuriouscollusive

Precisionдenuine =

Correctдenuine
Correctдenuine + Spuriousдenuine

15https://www.twitteraudit.com
16https://followerwonk.com/analyze
17https://botometer.iuni.iu.edu
18https://www.fakecheck.co, https://fakelikes.info/, https://igaudit.io/
19https://www.modash.io/
20https://analisa.io/

24

Recallcollusive =

Correctcollusive
Correctcollusive + Missinдcollusive

Precisionдenuine =

Correctдenuine
Correctдenuine + Missinддenuine

where Correctcollusive and Correctдenuine indicate correctly classified collusive and genuine users, respectively.
Similarly, Spuriouscollusive (resp. Missinдcollusive ) and Spuriousдenuine (resp. Missinддenuine ) indicate false
positive (resp. false negative) for collusive and genuine classes, respectively.

Precision@k and Recall@k are another metrics used for collusive entity detection [28] and are calculated as

follows:

Precision@k =

Recall@k =

# of recommended items @k that are collusive
k
# of recommended items @k that are collusive
total # of collusive items

.

Dutta et al. [42] also reported the True Positive Rate (TPR). The reason behind choosing TPR as the evaluation
metric is because all their models are trained on one-class, and the authors are only interested in the proportion
of actual positives (data in the collusive class) that are correctly identified by the models. The evaluation strategy
can be considered when the objective is to measure the effectiveness of a single class.

8 OPEN PROBLEMS
Collusion is a considerably new topic in the area of anomaly detection which opens up a number of future research
opportunities. Some of these topics include: (i) Collective collusion detection, (ii) understanding connectivity
patterns in collusive network, (iii) event-specific studies, (iv) temporal modeling of collusive entities, (v) cross-
lingual and multi-lingual studies, (vi) core collusive user detection, (vii) cross-platform spread of collusive content,
(viii) multi-modal analysis of collusive entities, and (ix) how collusion promotes fake news. We briefly discuss
these topics in detail below.

8.1 Collective Collusion Detection
Recently, collusive entity detection has gained a lot of attention in the literature. However, most of the existing
methods only detect independent collusion; but in reality, it is often seen that the anomalous phenomena also
happens in groups. The primary aim of the collusive group detection task is to find a set of users that jointly
exhibit anomalous behavior. Detecting anomalous groups is more difficult as compared to the individual detection
task due to the inter-group dynamics. We request the reader to read Section 5 for details of previous studies in
individual and group collusive entity detection. We believe that the advent of new datasets (see Section 7 for
more details) will foster fraudulent user/entity detection (both individual and group level) with the advantage of
adding the topical as well as the temporal dimension.

8.2 Understanding Connectivity Patterns in Collusive Network
Understanding connectivity patterns of an underlying network is a well-studied problem in the literature. It
includes tasks such as inferring lockstep behavior [14], dense block detection [109], detecting core users [107, 108],
identifying the most relevant actors in a network [17], sudden appearance/disappearance of links [46], etc. One
potential research direction is to create various networks among the entities to investigate the network’s structural
patterns. Recently, several studies have modeled information diffusion for collusive entities from a topological
point of view. It includes tasks such as influence maximization (selecting a seed set to maximize the influence
spread) [64, 87], predicting information cascade [57, 93], measuring message propagation and social influence
[18, 132], etc.

25

8.3 Event-specific Studies
Event-specific studies can be employed by deeply investigating large-scale datasets. Some of the publicly available
datasets mentioned in Section 7 are obtained from multiple sources and span over a long period of time. Therefore,
it may consist of information from many major events [10], which can be easily extracted for event-centric
studies. Researchers can also check how these users/entities were involved in manipulating the popularity of
events by artificially inflating/deflating the social growth of entities in online media [138].

8.4 Temporal Modeling of Collusive Entities
A topic closely related to the previous topic is the temporal modeling of the collusive entities. The tasks in
temporal modeling include detecting time periods containing unusual activity[50], identifying repetitive patterns
in time-evolving graphs [134], etc. Recently, detecting anomalies in streams [45, 46] has gained a lot of attraction
in the research community due to the time-evolving (or dynamic) property where consecutive snapshots of
activity in a time window are monitored. Related studies for temporal modeling can be found in [44, 126, 133].

8.5 Cross-lingual and Multi-lingual Studies
The datasets in collusive entity detection are collected from various online media platforms where the entities
usually have texts written in several languages. Though we observe the lack of annotated datasets in languages
other than English, cross-lingual studies can be done by converting the English texts into the target language
using automated translation tools. The converted texts can then be used to create the training and test datasets.
For multi-lingual studies, we can consider it as a future research topic that can be performed only when sufficient
content is available in multiple languages.

8.6 Core Collusive User Detection
The underlying blackmarket collusive network comprising two types of users: (i) core users – fake accounts or
sockpuppets, which are fully controlled by the blackmarkets (puppet masters), and compromised accounts that are
temporarily hired to support the core users. These two types of users are together called as collusive users. Core
users are the spine of any collusive blackmarket; they monitor and intelligently control the entire fraudulent
activities in such a way that none of their hired compromised accounts are being suspended. Therefore, detecting
and removing core blackmarket users is of the utmost importance to decentralize the collusive network and keep
the YouTube ecosystem healthy and trustworthy.

According to the literature [60], detecting core nodes in a network is to find the influential nodes. However, in
the case of collusion, core nodes might not be influential as these are the accounts that are fully controlled by
the blackmarket authorities. The work by Shin et al. [107] is the only study that explores empirical patterns of
core users in real-world networks. [60, 139] are some of the studies on influential node detection in a network.
However, none of the existing studies attempted to detect core collusive users. One reason may be the lack of
ground-truth for training the model and evaluation. We believe that Core blackmarket user detection is highly
important to understand how these services operate and flag their behavior.

8.7 Cross-platform Spread of Collusive Content
Another future work could be to investigate the cross-platform spread of collusive entities and study its impact.
Online media platforms differ from each other in multiple ways: (i) different platforms have significantly different
language characteristics, (ii) some platforms have restrictions on the length of posts allowing users to express
themselves within less space, (iii) some platforms only allow images/videos as posts, and (iv) different platforms
have different types of appraisals. Very few studies considered analyzing cross-platform data [62, 98] in online
media platforms. Moreover, no work to date examined the cross-platform study of collusive entities. An important

26

related issue to conduct the cross-platform study is the need for ground-truth datasets collected from different
online media platforms.

8.8 Multi-modal Analysis of Collusive Entities
A recent trend in social computing research is to conduct a multi-modal analysis of online media entities. The
multi-modal investigation of collusive entities is important due to the following reasons: (i) different modalities
may exhibit different but important information about an entity (e.g., may help in detecting the authenticity of
the information), and (ii) different modalities can be manipulated differently by the blackmarket services. Note
that it is expected that multi-modal analysis will introduce a computational cost due to the operation on image
and video data. Singhal et al. [112] proposed a multi-modal framework, called SpotFake for fake news detection
by leveraging the textual and image modalities. Hence, new models can be designed to incorporate different
modalities for collusive entity detection.

8.9 How Collusion Promotes Fake News
Online social networks are a popular way of dissemination and consumption of information. However, due to
their decentralized nature, they also come with limited liability for disinformation and collusive persuasion. It is
often the experience that online users are subjected to a barrage of misinformation within a short span of time,
all of which are targeted at persuading the user to develop a particular sentiment against a person, a political
party, a system of medicine, or the cause of an event. This “echo chamber” effect influences users’ thoughts in
a manner that is unfavourable to the spirit of free speech and debate. This direction may undertake research
into the detection of fake news within online social networks, with a specific focus on understanding the nature
of collusive orchestration of misinformation. In particular, one may consider – (i) collusion in fake news, i.e.,
identifying tell-tale patterns of collusion within the context of fake news, and (iii) collusive fake news detection,
i.e., leveraging such collusive patterns in order to enhance fake news detection. In addition to improving fake
news detection, this project will make fundamental advances towards a novel direction of research in fake news
analytics; that of characterizing and exploiting collusive behavior towards improving fake news detection.

9 CONCLUSION AND OPEN CHALLENGES
In this survey paper, we presented a detailed overview of the fundamentals of collusion and detection of artificial
boosting of social growth and manipulation observed in online media. We categorized existing studies based on
the platforms which are compromised by the collusive users. The approaches were mostly developed for the
detection of collusive entities using techniques such as feature-based, graph-based and deep learning-based in
both supervised and unsupervised settings. In addition, the survey also includes the related annotation guidelines,
datasets, available applications, evaluation metrics and future research opportunities for collusive entity detection.
This area is still in its infancy and has various open challenges. The major challenge is the collection of
large-scale datasets, as crawling blackmarket services is extremely challenging and may require ethical consent.
Moreover, there are restrictions and limitations of the APIs provided by the online media platforms. Furthermore,
human annotation to label an action (retweet, share, like, etc.) as collusive is confusing and often incurs low
inter-annotator agreement [28] as collusive activities bear high resemblance to genuine activities. Therefore, it
is difficult to design efficient supervised methods. There are limited studies on the detection of such collusive
manipulation in online media platforms other than social networks, which requires detailed inspection. Detection
and evaluation of self-collusion (such as creating multiple-account deception, sockpuppets, etc.) are extremely
challenging due to the difficulty in collecting ground-truth data. We believe that this survey will motivate
researchers to dig deeper into exploring the dynamics of collusion in online media platforms.

27

REFERENCES
[1] Shalinda Adikari and Kaushik Dutta. 2020. Identifying fake profiles in LinkedIn. arXiv preprint arXiv:2006.01381 (2020).
[2] Anupama Aggarwal, Saravana Kumar, Kushagra Bhargava, and Ponnurangam Kumaraguru. 2018. The Follower Count Fallacy: Detecting

Twitter Users with Manipulated Follower Count. arXiv preprint arXiv:1802.03625 (2018).

[3] MN Istiaq Ahsan, Tamzid Nahian, Abdullah All Kafi, Md Ismail Hossain, and Faisal Muhammad Shah. 2016. Review spam detection
using active learning. In 2016 IEEE 7th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON).
IEEE, 1–7.

[4] Leman Akoglu, Hanghang Tong, and Danai Koutra. 2015. Graph based anomaly detection and description: a survey. Data mining and

knowledge discovery 29, 3 (2015), 626–688.

[5] Mohammed Ali and Timothy Levine. 2008. The language of truthful and deceptive denials and confessions. Communication Reports 21,

2 (2008), 82–91.

[6] Jalal S Alowibdi, Ugo A Buy, S Yu Philip, Sohaib Ghani, and Mohamed Mokbel. 2015. Deception detection in Twitter. Social network

analysis and mining 5, 1 (2015), 32.

[7] Udit Arora, Hridoy Sankar Dutta, Brihi Joshi, Aditya Chetan, and Tanmoy Chakraborty. 2020. Analyzing and Detecting Collusive Users

Involved in Blackmarket Retweeting Activities. ACM Trans. Intell. Syst. Technol. 11, 3, Article 35 (April 2020), 24 pages.

[8] Udit Arora, William Scott Paka, and Tanmoy Chakraborty. 2019. Multitask learning for blackmarket tweet detection. In Proceedings of

the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. 127–130.

[9] Irina Mariuca Asavoae, Jorge Blasco, Thomas M Chen, Harsha Kumara Kalutarage, Igor Muttik, Hoang Nga Nguyen, Markus Roggenbach,

and Siraj Ahmed Shaikh. 2016. Towards automated android app collusion detection. arXiv preprint arXiv:1603.02308 (2016).

[10] Farzindar Atefeh and Wael Khreich. 2015. A survey of techniques for event detection in twitter. Computational Intelligence 31, 1 (2015),

132–164.

[11] Prudhvi Ratna Badri Satya, Kyumin Lee, Dongwon Lee, Thanh Tran, and Jason Jiasheng Zhang. 2016. Uncovering fake likers in online

social networks. In ACM CIKM. 2365–2370.

[12] Eytan Bakshy, Jake M Hofman, Winter A Mason, and Duncan J Watts. 2011. Everyone’s an influencer: quantifying influence on twitter.

In Proceedings of the fourth ACM international conference on Web search and data mining. ACM, 65–74.

[13] Fabricio Benevenuto, Gabriel Magno, Tiago Rodrigues, and Virgilio Almeida. 2010. Detecting spammers on twitter. In Collaboration,

electronic messaging, anti-abuse and spam conference (CEAS), Vol. 6. 12.

[14] Alex Beutel, Wanhong Xu, Venkatesan Guruswami, Christopher Palow, and Christos Faloutsos. 2013. Copycatch: stopping group

attacks by spotting lockstep behavior in social networks. In WWW. ACM, 119–130.

[15] Shweta Bhandari, Wafa Ben Jaballah, Vineeta Jain, Vijay Laxmi, Akka Zemmari, Manoj Singh Gaur, Mohamed Mosbah, and Mauro

Conti. 2017. Android inter-app communication threats and detection techniques. Computers & Security 70 (2017), 392–421.

[16] Jorge Blasco and Thomas M Chen. 2018. Automated generation of colluding apps for experimental research. Journal of Computer

Virology and Hacking Techniques 14, 2 (2018), 127–138.

[17] Stephen P Borgatti. 2006. Identifying sets of key players in a social network. Computational & Mathematical Organization Theory 12, 1

(2006), 21–34.

[18] Phil E Brown and Junlan Feng. 2011. Measuring user influence on twitter using modified k-shell decomposition. In Fifth international

AAAI conference on weblogs and social media. 18–23.

[19] Axel Bruns, Brenda Moon, Felix Victor M¨unch, Patrik Wikstr¨om, Stefan Stieglitz, Florian Brachten, and Bj¨orn Ross. 2018. Detecting
Twitter bots that share SoundCloud tracks. In Proceedings of the 9th International Conference on Social Media and Society. 251–255.
[20] Cody Buntain and Jennifer Golbeck. 2017. Automatically identifying fake news in popular twitter threads. In 2017 IEEE International

Conference on Smart Cloud (SmartCloud). IEEE, 208–215.

[21] Chiyu Cai, Linjing Li, and Daniel Zengi. 2017. Behavior enhanced deep bot detection in social media. In 2017 IEEE International

Conference on Intelligence and Security Informatics (ISI). IEEE, 128–130.

[22] Mark Carman, Mark Koerber, Jiuyong Li, Kim-Kwang Raymond Choo, and Helen Ashman. 2018. Manipulating visibility of political and
apolitical threads on Reddit via score boosting. In 2018 17th IEEE International Conference On Trust, Security And Privacy In Computing
And Communications/12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE). IEEE, 184–190.

[23] Jacopo Castellini, Valentina Poggioni, and Giulia Sorbi. 2017. Fake twitter followers detection by denoising autoencoder. In Proceedings

of the International Conference on Web Intelligence. 195–202.

[24] Nikan Chavoshi, Hossein Hamooni, and Abdullah Mueen. 2016. DeBot: Twitter Bot Detection via Warped Correlation.. In ICDM.

817–822.

[25] Nikan Chavoshi, Hossein Hamooni, and Abdullah Mueen. 2017. Temporal patterns in bot activities. In Proceedings of the 26th International

Conference on World Wide Web Companion. International World Wide Web Conferences Steering Committee, 1601–1606.

[26] Liang Chen, Yipeng Zhou, and Dah Ming Chiu. 2015. Analysis and detection of fake views in online video services. ACM Transactions

on Multimedia Computing, Communications, and Applications (TOMM) 11, 2s (2015), 44.

28

[27] Justin Cheng, Lada Adamic, P Alex Dow, Jon Michael Kleinberg, and Jure Leskovec. 2014. Can cascades be predicted?. In Proceedings of

the 23rd international conference on World wide web. ACM, 925–936.

[28] Aditya Chetan, Brihi Joshi, Hridoy Sankar Dutta, and Tanmoy Chakraborty. 2019. CoReRank: Ranking to Detect Users Involved in

Blackmarket-Based Collusive Retweeting Activities. In ACM WSDM. 330–338.

[29] Zi Chu, Steven Gianvecchio, Haining Wang, and Sushil Jajodia. 2010. Who is tweeting on Twitter: human, bot, or cyborg?. In Proceedings

of the 26th annual computer security applications conference. ACM, 21–30.

[30] Zi Chu, Steven Gianvecchio, Haining Wang, and Sushil Jajodia. 2012. Detecting automation of twitter accounts: Are you a human, bot,

or cyborg? IEEE Transactions on Dependable and Secure Computing 9, 6 (2012), 811–824.

[31] Mauro Conti, Radha Poovendran, and Marco Secchiero. 2012. Fakebook: Detecting fake profiles in on-line social networks. In Proceedings
of the 2012 International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2012). IEEE Computer Society,
1071–1078.

[32] David M Cook, Benjamin Waugh, Maldini Abdipanah, Omid Hashemi, and Shaquille Abdul Rahman. 2014. Twitter deception and

influence: Issues of identity, slacktivism, and puppetry. Journal of Information Warfare 13, 1 (2014), 58–71.

[33] Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, and Maurizio Tesconi. 2015. Fame for sale: efficient detection

of fake Twitter followers. Decision Support Systems 80 (2015), 56–71.

[34] Clayton Allen Davis, Onur Varol, Emilio Ferrara, Alessandro Flammini, and Filippo Menczer. 2016. Botornot: A system to evaluate social
bots. In Proceedings of the 25th International Conference Companion on World Wide Web. International World Wide Web Conferences
Steering Committee, 273–274.

[35] Emiliano De Cristofaro, Arik Friedman, Guillaume Jourjon, Mohamed Ali Kaafar, and M Zubair Shafiq. 2014. Paying for likes?:

Understanding facebook like fraud using honeypots. In ACM IMC. 129–136.

[36] Michela Del Vicario, Alessandro Bessi, Fabiana Zollo, Fabio Petroni, Antonio Scala, Guido Caldarelli, H Eugene Stanley, and Walter
Quattrociocchi. 2016. The spreading of misinformation online. Proceedings of the National Academy of Sciences 113, 3 (2016), 554–559.
[37] Sarthika Dhawan, Siva Charan Reddy Gangireddy, Shiv Kumar, and Tanmoy Chakraborty. 2019. Spotting Collective Behaviour of

Online Fraud Groups in Customer Reviews. (2019). arXiv:arXiv:1905.13649

[38] John P Dickerson, Vadim Kagan, and VS Subrahmanian. 2014. Using sentiment to detect bots on twitter: Are humans more opinionated
than bots?. In Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. IEEE
Press, 620–627.

[39] Hridoy Sankar Dutta and Tanmoy Chakraborty. 2019. Blackmarket-driven Collusion among Retweeters–Analysis, Detection and

Characterization. IEEE Transactions on Information Forensics and Security (2019).

[40] Hridoy Sankar Dutta, Aditya Chetan, Brihi Joshi, and Tanmoy Chakraborty. 2018. Retweet us, we will retweet you: Spotting collusive

retweeters involved in blackmarket services. In ASONAM. 242–249.

[41] Hridoy Sankar Dutta, Vishal Raj Dutta, Aditya Adhikary, and Tanmoy Chakraborty. 2020. HawkesEye: Detecting Fake Retweeters using

Hawkes Process and Topic Modeling. IEEE Transactions on Information Forensics and Security (2020).

[42] Hridoy Sankar Dutta, Mayank Jobanputra, Himani Negi, and Tanmoy Chakraborty. 2020. Detecting and analyzing collusive entities on

YouTube. arXiv preprint arXiv:2005.06243 (2020).

[43] Ahmed El Azab, Amira M Idrees, Mahmoud A Mahmoud, and Hesham Hefny. 2016. Fake account detection in twitter based on minimum

weighted feature set. Int. Sch. Sci. Res. Innov 10, 1 (2016), 13–18.

[44] Dhivya Eswaran. 2020. Mining Anomalies using Static and Dynamic Graphs. Ph.D. Dissertation. Carnegie Mellon University Pittsburgh,

PA.

[45] Dhivya Eswaran and Christos Faloutsos. 2018. Sedanspot: Detecting anomalies in edge streams. In 2018 IEEE International Conference

on Data Mining (ICDM). IEEE, 953–958.

[46] Dhivya Eswaran, Christos Faloutsos, Sudipto Guha, and Nina Mishra. 2018. Spotlight: Detecting anomalies in streaming graphs. In

Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1378–1386.

[47] Emilio Ferrara, Onur Varol, Clayton Davis, Filippo Menczer, and Alessandro Flammini. 2016. The rise of social bots. Commun. ACM 59,

7 (2016), 96–104.

[48] Grace Gee and Hakson Teh. 2010. Twitter spammer profile detection. Available online: cs229. stanford. edu/proj2010/GeeTeh-Twitter

Spammer Profile Detection. pdf (2010).

[49] Maria Giatsoglou, Despoina Chatzakou, Neil Shah, Alex Beutel, Christos Faloutsos, and Athena Vakali. 2015. Nd-sync: Detecting

synchronized fraud activities. In PAKDD. 201–214.

[50] Maria Giatsoglou, Despoina Chatzakou, Neil Shah, Christos Faloutsos, and Athena Vakali. 2015. Retweeting activity on twitter: Signs of

deception. In Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 122–134.

[51] Zafar Gilani, Liang Wang, Jon Crowcroft, Mario Almeida, and Reza Farahbakhsh. 2016. Stweeler: A framework for twitter bot analysis.
In Proceedings of the 25th International Conference Companion on World Wide Web. International World Wide Web Conferences Steering
Committee, 37–38.

29

[52] Raman Goyal, Gabriel Ferreira, Christian K¨astner, and James Herbsleb. 2018. Identifying unusual commits on GitHub. Journal of

Software: Evolution and Process 30, 1 (2018), e1893.

[53] Aditi Gupta, Ponnurangam Kumaraguru, Carlos Castillo, and Patrick Meier. 2014. Tweetcred: Real-time credibility assessment of

content on twitter. In International Conference on Social Informatics. Springer, 228–243.

[54] Aditi Gupta, Hemank Lamba, Ponnurangam Kumaraguru, and Anupam Joshi. 2013. Faking sandy: characterizing and identifying fake
images on twitter during hurricane sandy. In Proceedings of the 22nd international conference on World Wide Web. ACM, 729–736.
[55] Sonu Gupta, Ponnurangam Kumaraguru, and Tanmoy Chakraborty. 2019. MalReG: Detecting and Analyzing Malicious Retweeter

Groups. In CODS-COMAD. ACM, 61–69.

[56] Supraja Gurajala, Joshua S White, Brian Hudson, and Jeanna N Matthews. 2015. Fake Twitter accounts: profile characteristics obtained
using an activity-based pattern detection approach. In Proceedings of the 2015 International Conference on Social Media & Society. ACM,
9.

[57] Man Aris Nur Hakim and Masayu Leylia Khodra. 2014. Predicting information cascade on Twitter using support vector regression. In

2014 International Conference on Data and Software Engineering (ICODSE). IEEE, 1–6.

[58] Bryan Hooi, Neil Shah, Alex Beutel, Stephan G¨unnemann, Leman Akoglu, Mohit Kumar, Disha Makhija, and Christos Faloutsos. 2016.
Birdnest: Bayesian inference for ratings-fraud detection. In Proceedings of the 2016 SIAM International Conference on Data Mining. SIAM,
495–503.

[59] Yan Hu, Shanshan Wang, Yizhi Ren, and Kim-Kwang Raymond Choo. 2018. User influence analysis for Github developer social networks.

Expert Systems with Applications 108 (2018), 108–118.

[60] Xinyu Huang, Dongming Chen, Dongqi Wang, and Tao Ren. 2020. Identifying Influencers in Social Networks. Entropy 22, 4 (2020), 450.
[61] Isa Inuwa-Dutse, Bello Shehu Bello, and Ioannis Korkontzelos. 2018. Lexical analysis of automated accounts on Twitter. arXiv preprint

arXiv:1812.07947 (2018).

[62] Kokil Jaidka, Sharath Chandra Guntuku, Anneke Buffone, H Andrew Schwartz, and Lyle H Ungar. 2018. Facebook vs. Twitter: Cross-
platform differences in self-disclosure and trait prediction. In Proceedings of the Twelfth International AAAI Conference on Web and Social
Media. 141–150.

[63] Boyeon Jang, Sihyun Jeong, and Chong-kwon Kim. 2019. Distance-based customer detection in fake follower markets. Information

Systems 81 (2019), 104–116.

[64] Siwar Jendoubi, Arnaud Martin, Ludovic Li´etard, Hend Ben Hadji, and Boutheina Ben Yaghlane. 2017. Two evidential data based models

for influence maximization in twitter. Knowledge-Based Systems 121 (2017), 58–70.

[65] Fang Jin, Edward Dougherty, Parang Saraf, Yang Cao, and Naren Ramakrishnan. 2013. Epidemiological modeling of news and rumors

on twitter. In Proceedings of the 7th Workshop on Social Network Mining and Analysis. ACM, 8.

[66] Nitin Jindal, Bing Liu, and Ee-Peng Lim. 2010. Finding unusual review patterns using unexpected rules. In Proceedings of the 19th ACM

international conference on Information and knowledge management. 1549–1552.

[67] Harsha Kumara Kalutarage, Hoang Nga Nguyen, and Siraj Ahmed Shaikh. 2017. Towards a threat assessment framework for apps

collusion. Telecommunication Systems 66, 3 (2017), 417–430.

[68] M¨ucahit Kantepe and Murat Can Ganiz. 2017. Preprocessing framework for twitter bot detection. In 2017 International Conference on

Computer Science and Engineering (UBMK). IEEE, 630–634.

[69] Jooyeon Kim, Behzad Tabibian, Alice Oh, Bernhard Sch¨olkopf, and Manuel Gomez-Rodriguez. 2018. Leveraging the crowd to detect and
reduce the spread of fake news and misinformation. In Proceedings of the Eleventh ACM International Conference on Web Search and Data
Mining. 324–332.

[70] Sneha Kudugunta and Emilio Ferrara. 2018. Deep neural networks for bot detection. Information Sciences 467 (2018), 312–322.
[71] Srijan Kumar, Justin Cheng, Jure Leskovec, and VS Subrahmanian. 2017. An army of me: Sockpuppets in online discussion communities.

In Proceedings of the 26th International Conference on World Wide Web. 857–866.

[72] Srijan Kumar, Justin Cheng, Jure Leskovec, and V. S. Subrahmanian. 2017. An Army of Me: Sockpuppets in Online Discussion

Communities. In WWW. 857–866.

[73] Srijan Kumar, Bryan Hooi, Disha Makhija, Mohit Kumar, Christos Faloutsos, and VS Subrahmanian. 2018. Rev2: Fraudulent user

prediction in rating platforms. In ACM WSDM. 333–341.

[74] Srijan Kumar and Neil Shah. 2018. False information on web and social media: A survey. arXiv preprint arXiv:1804.08559 (2018).
[75] Andrey Kupavskii, Liudmila Ostroumova, Alexey Umnov, Svyatoslav Usachev, Pavel Serdyukov, Gleb Gusev, and Andrey Kustarev.
2012. Prediction of retweet cascade size over time. In Proceedings of the 21st ACM international conference on Information and knowledge
management. ACM, 2335–2338.

[76] David F Larcker and Anastasia A Zakolyukina. 2012. Detecting deceptive discussions in conference calls. Journal of Accounting Research

50, 2 (2012), 495–540.

[77] Kyumin Lee, Brian David Eoff, and James Caverlee. 2011. Seven months with the devils: A long-term study of content polluters on

twitter. In Fifth international AAAI conference on weblogs and social media. 185–192.

30

[78] Kristina Lerman and Rumi Ghosh. 2010. Information contagion: An empirical study of the spread of news on digg and twitter social

networks. In Fourth International AAAI Conference on Weblogs and Social Media. 90–97.

[79] Fangtao Huang Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu. 2011. Learning to identify review spam. In Twenty-second international

joint conference on artificial intelligence. 2488–2493.

[80] Huayi Li, Zhiyuan Chen, Bing Liu, Xiaokai Wei, and Jidong Shao. 2014. Spotting fake reviews via collective positive-unlabeled learning.

In 2014 IEEE international conference on data mining. IEEE, 899–904.

[81] Huayi Li, Arjun Mukherjee, Bing Liu, Rachel Kornfield, and Sherry Emery. 2014. Detecting campaign promoters on twitter using

markov random fields. In 2014 IEEE International Conference on Data Mining. IEEE, 290–299.

[82] Po-Ching Lin and Po-Min Huang. 2013. A study of effective features for detecting long-surviving Twitter spam accounts. In 2013 15th

International Conference on Advanced Communications Technology (ICACT). IEEE, 841–846.

[83] Shenghua Liu, Bryan Hooi, and Christos Faloutsos. 2017. Holoscope: Topology-and-spike aware fraud detection. In Proceedings of the

2017 ACM on Conference on Information and Knowledge Management. 1539–1548.

[84] Yuli Liu, Yiqun Liu, Min Zhang, and Shaoping Ma. 2016. Pay Me and I’ll Follow You: Detection of Crowdturfing Following Activities in

Microblog Environment.. In IJCAI. 3789–3796.

[85] Michael Mccord and M Chuah. 2011. Spam detection on twitter using traditional classifiers. In international conference on Autonomic

and trusted computing. Springer, 175–186.

[86] Ashish Mehrotra, Mallidi Sarreddy, and Sanjay Singh. 2016. Detection of fake Twitter followers using graph centrality measures. In 2016

2nd International Conference on Contemporary Computing and Informatics (IC3I). IEEE, 499–504.

[87] Yan Mei, Weiliang Zhao, and Jian Yang. 2017. Influence maximization on twitter: A mechanism for effective marketing campaign. In

2017 IEEE International Conference on Communications (ICC). IEEE, 1–6.

[88] Fred Morstatter, Liang Wu, Tahora H Nazer, Kathleen M Carley, and Huan Liu. 2016. A new approach to bot detection: striking the
balance between precision and recall. In 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
(ASONAM). IEEE, 533–540.

[89] Arjun Mukherjee, Abhinav Kumar, Bing Liu, Junhui Wang, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. 2013. Spotting
opinion spammers using behavioral footprints. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery
and data mining. 632–640.

[90] Arjun Mukherjee, Bing Liu, and Natalie Glance. 2012. Spotting fake reviewer groups in consumer reviews. In WWW. ACM, 191–200.
[91] Yiangos Papanastasiou. 2018. Fake news propagation and detection: A sequential model. Available at SSRN 3028354 (2018).
[92] Vıctor M Prieto, Manuel Alvarez, and Fidel Cacheda. 2013. Detecting linkedin spammers and its spam nets. International Journal of

Advanced Computer Science and Applications (IJACSA) 4, 9 (2013).

[93] Geerajit Rattanaritnont, Masashi Toyoda, and Masaru Kitsuregawa. 2011. A study on characteristics of topicspecific information cascade

in Twitter. In Forum on Data Engineering (DE2011). 65–70.

[94] Shebuti Rayana and Leman Akoglu. 2015. Collective opinion spam detection: Bridging review networks and metadata. In ACM SIGKDD.

985–994.

[95] Shebuti Rayana and Leman Akoglu. 2016. Collective opinion spam detection using active inference. In SIAM. SIAM, 630–638.
[96] Bj¨orn Ross, Florian Brachten, Stefan Stieglitz, Patrik Wikstrom, Brenda Moon, Felix Victor M¨unch, and Axel Bruns. 2018. Social bots in
a commercial context-A case study on SoundCloud. In Proceedings of the 26th European Conference on Information Systems (ECIS2018).
Association for Information Systems, 1–10.

[97] Diego Saez-Trumper. 2014. Fake tweet buster: a webtool to identify users promoting fake news ontwitter. In Proceedings of the 25th

ACM conference on Hypertext and social media. ACM, 316–317.

[98] Zahra Riahi Samani, Sharath Chandra Guntuku, Mohsen Ebrahimi Moghaddam, Daniel Preot¸iuc-Pietro, and Lyle H Ungar. 2018.
Cross-platform and cross-interaction study of user personality based on images on Twitter and Flickr. PloS one 13, 7 (2018), e0198660.
[99] Igor Santos, Igor Mi ˜nambres-Marcos, Carlos Laorden, Patxi Gal´an-Garc´ıa, Aitor Santamar´ıa-Ibirika, and Pablo Garc´ıa Bringas. 2014.

Twitter content-based spam filtering. In International Joint Conference SOCO’13-CISIS’13-ICEUTE’13. Springer, 449–458.

[100] Indira Sen, Anupama Aggarwal, Shiven Mian, Siddharth Singh, Ponnurangam Kumaraguru, and Anwitaman Datta. 2018. Worth its

Weight in Likes: Towards Detecting Fake Likes on Instagram.. In WebSci. 205–209.

[101] Neil Shah. 2017. FLOCK: Combating astroturfing on livestreaming platforms. In WWW. 1083–1091.
[102] Neil Shah, Alex Beutel, Bryan Hooi, Leman Akoglu, Stephan Gunnemann, Disha Makhija, Mohit Kumar, and Christos Faloutsos. 2016.
Edgecentric: Anomaly detection in edge-attributed networks. In 2016 IEEE 16th International Conference on Data Mining Workshops
(ICDMW). IEEE, 327–334.

[103] Neil Shah, Hemank Lamba, Alex Beutel, and Christos Faloutsos. 2017. OEC: Open-Ended Classification for Future-Proof Link-Fraud

Detection. CoRR abs/1704.01420 (2017). arXiv:1704.01420 http://arxiv.org/abs/1704.01420

[104] Chengcheng Shao, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng Yang, Alessandro Flammini, and Filippo Menczer. 2018. The

spread of low-credibility content by social bots. Nature communications 9, 1 (2018), 4787.

31

[105] Qinlan Shen and Carolyn Rose. 2019. The discourse of online content moderation: Investigating polarized user responses to changes in

redditfis quarantine policy. In Proceedings of the Third Workshop on Abusive Language Online. 58–69.

[106] Yi Shen, Jianjun Yu, Kejun Dong, and Kai Nan. 2014. Automatic fake followers detection in chinese micro-blogging system. In

Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 596–607.

[107] Kijung Shin, Tina Eliassi-Rad, and Christos Faloutsos. 2016. Corescope: Graph mining using k-core analysisfi!?patterns, anomalies and

algorithms. In 2016 IEEE 16th International Conference on Data Mining (ICDM). IEEE, 469–478.

[108] Kijung Shin, Tina Eliassi-Rad, and Christos Faloutsos. 2018. Patterns and anomalies in k-cores of real-world graphs with applications.

Knowledge and Information Systems 54, 3 (2018), 677–710.

[109] Kijung Shin, Bryan Hooi, and Christos Faloutsos. 2016. M-zoom: Fast dense-block detection in tensors with quality guarantees. In

Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 264–280.

[110] Kijung Shin, Bryan Hooi, and Christos Faloutsos. 2018. Fast, accurate, and flexible algorithms for dense subtensor mining. ACM

Transactions on Knowledge Discovery from Data (TKDD) 12, 3 (2018), 1–30.

[111] Monika Singh, Divya Bansal, and Sanjeev Sofat. 2018. Who is who on twitter–spammer, fake or compromised account? a tool to reveal

true identity in real-time. Cybernetics and Systems 49, 1 (2018), 1–25.

[112] Shivangi Singhal, Rajiv Ratn Shah, Tanmoy Chakraborty, Ponnurangam Kumaraguru, and Shin’ichi Satoh. 2019. SpotFake: A
Multi-modal Framework for Fake News Detection. In 2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM). IEEE,
39–47.

[113] Jonghyuk Song, Sangho Lee, and Jong Kim. 2011. Spam filtering in twitter using sender-receiver relationship. In International workshop

on recent advances in intrusion detection. Springer, 301–317.

[114] VS Subrahmanian, Amos Azaria, Skylar Durst, Vadim Kagan, Aram Galstyan, Kristina Lerman, Linhong Zhu, Emilio Ferrara, Alessandro

Flammini, and Filippo Menczer. 2016. The DARPA Twitter bot challenge. Computer 49, 6 (2016), 38–46.

[115] Kurt Thomas, Chris Grier, Dawn Song, and Vern Paxson. 2011. Suspended accounts in retrospect: an analysis of twitter spam. In

Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference. ACM, 243–258.

[116] Kurt Thomas, Damon McCoy, Chris Grier, Alek Kolcz, and Vern Paxson. 2013. Trafficking Fraudulent Accounts: The Role of the

Underground Market in Twitter Spam and Abuse. In USENIX. 195–210.

[117] Onur Varol, Emilio Ferrara, Clayton A Davis, Filippo Menczer, and Alessandro Flammini. 2017. Online human-bot interactions:

Detection, estimation, and characterization. In Eleventh international AAAI conference on web and social media. 280–289.

[118] Sokratis Vidros, Constantinos Kolias, Georgios Kambourakis, and Leman Akoglu. 2017. Automatic detection of online recruitment

frauds: Characteristics, methods, and a public dataset. Future Internet (2017).

[119] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false news online. Science 359, 6380 (2018), 1146–1151.
[120] Alex Hai Wang. 2010. Don’t follow me: Spam detection in twitter. In IEEE SECRYPT. 1–10.
[121] Bo Wang, Arkaitz Zubiaga, Maria Liakata, and Rob Procter. 2015. Making the most of tweet-inherent features for social spam detection

on twitter. arXiv preprint arXiv:1503.07405 (2015).

[122] De Wang, Shamkant B Navathe, Ling Liu, Danesh Irani, Acar Tamersoy, and Calton Pu. 2013. Click traffic analysis of short url spam
on twitter. In 9th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing. IEEE, 250–259.
[123] Guan Wang, Sihong Xie, Bing Liu, and S Yu Philip. 2011. Review graph based online store review spammer detection. In ICDM.

1242–1247.

[124] Zhuo Wang, Songmin Gu, Xiangnan Zhao, and Xiaowei Xu. 2018. Graph-based review spammer group detection. KAIS (2018), 1–27.
[125] Zhuo Wang, Tingting Hou, Dawei Song, Zhun Li, and Tianqi Kong. 2016. Detecting review spammer groups via bipartite graph

projection. Comput. J. 59, 6 (2016), 861–874.

[126] Audrey Wilmet, Tiphaine Viard, Matthieu Latapy, and Robin Lamarche-Perrin. 2018. Degree-based outliers detection within ip traffic

modelled as a link stream. In 2018 Network Traffic Measurement and Analysis Conference (TMA). IEEE, 1–8.

[127] Yongji Wu, Defu Lian, Yiheng Xu, Le Wu, and Enhong Chen. 2020. Graph Convolutional Networks with Markov Random Field

Reasoning for Social Spammer Detection. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1054–1061.

[128] Chang Xu, Jie Zhang, Kuiyu Chang, and Chong Long. 2013. Uncovering collusive spammers in Chinese review websites. In CIKM.

ACM, 979–988.

[129] Yuanbo Xu, Yongjian Yang, Jiayu Han, En Wang, Jingci Ming, and Hui Xiong. 2019. Slanderous user detection with modified recurrent

neural networks in recommender system. Information Sciences 505 (2019), 265–281.

[130] Kai-Cheng Yang, Onur Varol, Clayton A Davis, Emilio Ferrara, Alessandro Flammini, and Filippo Menczer. 2019. Arming the public

with AI to counter social bots. arXiv preprint arXiv:1901.00912 (2019).

[131] Sarita Yardi, Daniel Romero, Grant Schoenebeck, et al. 2010. Detecting spam in a twitter network. First Monday 15, 1 (2010).
[132] Shaozhi Ye and S Felix Wu. 2010. Measuring message propagation and social influence on Twitter. com. In International conference on

social informatics. Springer, 216–231.

[133] Minji Yoon, Bryan Hooi, Kijung Shin, and Christos Faloutsos. 2019. Fast and accurate anomaly detection in dynamic graphs with
a two-pronged approach. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.

32

647–657.

[134] Hossein Rouhani Zeidanloo, Azizah Bt Manaf, Payam Vahdani, Farzaneh Tabatabaei, and Mazdak Zamani. 2010. Botnet detection

based on traffic monitoring. In 2010 International Conference on Networking and Information Technology. IEEE, 97–101.

[135] Chao Michael Zhang and Vern Paxson. 2011. Detecting and analyzing automated activity on twitter. In International Conference on

Passive and Active Network Measurement. Springer, 102–111.

[136] Xianchao Zhang, Shaoping Zhu, and Wenxin Liang. 2012. Detecting spam and promoting campaigns in the twitter social network. In

2012 IEEE 12th international conference on data mining. IEEE, 1194–1199.

[137] Yi Zhang and Jianguo Lu. 2016. Discover millions of fake followers in Weibo. Social Network Analysis and Mining 6, 1 (2016), 16.
[138] Yubao Zhang, Xin Ruan, Haining Wang, Hui Wang, and Su He. 2016. Twitter trends manipulation: a first look inside the security of

twitter trending. IEEE Transactions on Information Forensics and Security 12, 1 (2016), 144–156.

[139] Yu Zhang and Yan Zhang. 2017. Top-K influential nodes in social networks: A game perspective. In Proceedings of the 40th International

ACM SIGIR Conference on Research and Development in Information Retrieval. 1029–1032.

33

