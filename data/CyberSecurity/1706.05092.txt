7
1
0
2

n
u
J

5
1

]

R
C
.
s
c
[

1
v
2
9
0
5
0
.
6
0
7
1
:
v
i
X
r
a

Creating a Cybersecurity Concept Inventory:
A Status Report on the CATS Project

Alan T. Sherman,1 Linda Oliva,2 David DeLatte,1 Enis Golaszewski,1 Michael Neary,1
Konstantinos Patsourakos,1 Dhananjay Phatak,1 Travis Scheponik,1
University of Maryland, Baltimore County (UMBC)
Baltimore, Maryland 21250
email: {sherman, oliva, dad, golaszewski, mneary1, konpats1, phatak, tschep1}@umbc.edu

Geoﬀrey L. Herman,3 Julia Thompson,3
University of Illinois at Urbana-Champaign
Champaign, Illinois 61820
email: {glherman, jdthomp}@illinois.edu

June 8, 2017

Abstract

We report on the status of our Cybersecurity Assess-
ment Tools (CATS) project that is creating and val-
idating a concept inventory for cybersecurity, which
assesses the quality of instruction of any ﬁrst course
in cybersecurity. In fall 2014, we carried out a Del-
phi process that identiﬁed core concepts of cyber-
In spring 2016, we interviewed twenty-six
security.
students to uncover their understandings and mis-
conceptions about these concepts.
In fall 2016, we
generated our ﬁrst assessment tool–a draft Cyberse-
curity Concept Inventory (CCI), comprising approx-
imately thirty multiple-choice questions. Each ques-
tion targets a concept; incorrect answers are based on
observed misconceptions from the interviews. This
year we are validating the draft CCI using cognitive
interviews, expert reviews, and psychometric testing.
In this paper, we highlight our progress to date in
developing the CCI.

1Cyber Defense Lab, Department of Computer Science and

Electrical Engineering

2Department of Education
3Computer Science

The CATS project provides infrastructure for a rig-
orous evidence-based improvement of cybersecurity
education. The CCI permits comparisons of diﬀerent
instructional methods by assessing how well students
learned the core concepts of the ﬁeld (especially ad-
versarial thinking), where instructional methods re-
fer to how material is taught (e.g., lab-based, case-
studies, collaborative, competitions, gaming). Specif-
ically, the CCI is a tool that will enable researchers to
scientiﬁcally quantify and measure the eﬀect of their
approaches to, and interventions in, cybersecurity ed-
ucation.

Index terms— Cybersecurity Assessment Tools
cybersecurity education, Cybersecurity

(CATS),
Concept Inventory (CCI).

1

Introduction

In the coming years, America will need to educate
an increasing number of cybersecurity professionals.
But how will we know if the preparatory courses are
eﬀective? Presently there is no rigorous, research-
based method for measuring the quality of cyber-
security instruction. Validated assessment tools are

1

 
 
 
 
 
 
needed so that cybersecurity educators have trusted
methods for discerning whether eﬀorts to improve
student preparation are successful. The Cybersecu-
rity Assessment Tools (CATS) project is developing
rigorous instruments that can measure student learn-
ing and identify best practices. The ﬁrst CAT will be
a Cybersecurity Concept Inventory (CCI) that mea-
sures how well students understand basic concepts in
cybersecurity (especially adversarial thinking) after a
ﬁrst course in the ﬁeld.

Cybersecurity is a vital area of growing impor-
tance for national competitiveness, and national re-
ports reveal a growing need for cybersecurity profes-
sionals [FS15]. As educators wrestle with this de-
mand, there is a corresponding awareness that we
lack a rigorous research base that informs how to
meet that demand. Existing certiﬁcation exams, such
as CISSP [cis], are largely informational, not concep-
tual. We are not aware of any scientiﬁc studies of
any of these tests. The CATS Project is developing
rigorous assessment tools for assessing and evaluating
educational practices.

Since fall 2014, we have been following prescrip-
tions of the National Research Council for developing
rigorous and valid assessment tools [PCG01, PB14,
HCZL14, SMSR+11]. Our work is inspired in part by
the Force Concept Inventory [HWS92], which helped
transform and improve physics education to employ
more active learning methods.

We carried out two surveys using the Delphi
process to identify the scope and content of the
CATs [PDH+16]. We then used qualitative interviews
to develop a cognitive theory that can guide the con-
struction of assessment questions [SSD+16, THS+17].
Based on these interviews, we developed a prelim-
inary battery of approximately thirty assessment
items for the CCI. Each item focuses on one of the
top ﬁve rated concepts (on importance) from our CCI
Delphi process. The distractors (incorrect answers)
for each assessment item are based on student mis-
conceptions observed during the interviews.

In this paper we provide a status report on the
CATS project, highlighting our results from the Del-
phi process, student interviews, and our development
of draft multi-choice questions. Examples illustrate
our method. For more details, see our project re-

ports [PDH+16, SDH+16, SSD+16, THS+17].4

2

Indentifying
through a Delphi Process

Core

Concepts

In fall 2014, we carried out
two Delphi pro-
cesses aimed at identifying core cybersecurity top-
ics [PDH+16]. A Delphi process solicits input from
a set of subject matter experts to create consensus
about contentious decisions [GGH+10]. Topics are re-
ﬁned and prioritized over several rounds, where par-
ticipants share comments without attribution so that
the logic of a contributed remark is most signiﬁcant.
The ﬁrst process was for the CCI, which is for stu-
dents completing any ﬁrst course in cybersecurity.
The second process was for a Cybersecurity Curricu-
lum Assessment (CCA), which is for students gradu-
ating from college about to enter the workforce as cy-
bersecurity professionals. Here, we focus on the CCI.
We conducted each process electronically, through
emails between Delphi leaders and the panel of ex-
perts, and through web forms to collect survey data.5
We carried out ﬁve rounds for CCI and four for CCA.
To the authors’ knowledge, these are the ﬁrst Delphi
processes for cybersecurity to identify core concepts.
A total of thirty-six experts participated in the ini-
tial topic generation phase, including thirty-three for
the CCI process. The selected experts constitute a
diverse group of men and women from over a dozen
US states and Canada, working as cybersecurity au-
thors, educators, and professionals from industry and
government [PDH+16]. Each expert holds a PhD in a
cybersecurity-related ﬁeld and teaches cybersecurity,
or works as a cybersecurity professional. The project
website lists the experts and their aﬃliations.6

Experts rated CCI and CCA topics according to
three distinct metrics: (1) Importance, (2) Diﬃculty,
and (3) Timelessness using a 1–10 Likert scale, where
10 is the greatest. If an expert chose to rate a topic
outside the interquartile range, they were asked to
provide a written justiﬁcation for their deviation from

4Parts of this paper are drawn from these reports.
5The Delphi leaders were Sherman and Parekh, who con-
sulted with Herman who has notable experience with this pro-
cess.

6http://cisa.umbc.edu/cats/

2

Table 1: Top ﬁve reconciled CCI topics sorted by median importance (I) and then by median diﬃculty (D),
as rated by the Delphi experts using a 1-10 Likert scale where 10 is the greatest.

Topic
Identify vulnerabilities and failures
Identify attacks against CIAa triad and authentication
Devise a defense
Identify the security goals
Identify potential targets and attackers

I D
8
9
8
9
7
9
6
9
5
9

aCIA refers to conﬁdentiality, integrity, availability.

the consensus. These comments enabled dissenting
experts to sway the majority. Once the deadline
for the round passed, the Delphi leaders compiled
summary statistics for each topic. These descriptive
statistics and data visualization provided the Delphi
leaders with information about the level of consensus.
Responses from the ﬁrst rounds of CCI and CCA
were unexpectedly similar; though, adversarial think-
ing was a prevalent theme among CCI responses. To
ensure that CCI was headed in a distinct direction
from CCA, a second topic identiﬁcation round was
performed for CCI only. Delphi leaders asked partic-
ipants to provide topics focused on adversarial think-
ing, which the Delphi leaders and the experts felt con-
stitutes a vital core of cybersecurity. The restarted
CCI process produced thirty topics.

Table 1 lists the top ﬁve topics from the CCI pro-
cess sorted by median importance. The main con-
tribution of this phase of the project is a numerical
rating of the importance and diﬃculty of concepts in
cybersecurity. It is prudent to identify topics that are
diﬃcult, since those topics may provide the greatest
barriers to mastery. These ratings can be used to
identify core concepts—cross-cutting ideas that con-
nect knowledge in the discipline—which can guide
the design of curriculum, assessment tools, and other
educational materials and policies.

The results of the Delphi processes, especially the
CCA process, identiﬁed a range of specialized topics,
reﬂecting the broad, multi-faceted aspects of cyber-
security. This range of facets can make prioritizing
content in cybersecurity education diﬃcult and make
it diﬃcult for students to discern how topics connect.

The ﬁve topics rated most important by the Delphi
experts in CCI stand out as important and timeless
concepts that can create priorities in instruction and
help students organize their learning.

In addition, these results help clarify, distill, and
articulate what is cybersecurity, which this project
sees as the management of information and trust in
an adversarial cyber world.

3 Uncovering Misconceptions by In-

terviewing Students

To study how students reason about core cybersecu-
rity concepts, in spring 2016, we conducted twenty-
six think-aloud interviews with cybersecurity stu-
dents who had completed at least one course in cyber-
security [SSD+16, THS+17]. We recruited these stu-
dents from three diverse institutions: University of
Maryland, Baltimore County, Prince George’s Com-
munity College, and Bowie State University. Dur-
ing the interviews, students grappled with security
scenarios designed to probe student understanding
of cybersecurity, especially adversarial thinking. No
prior research has documented student misconcep-
tions about cybersecurity concepts nor how they use
adversarial models to guide their reasoning.

Drawing upon the ﬁve topics from the CCI Del-
phi process, we developed a set of twelve engaging
cybersecurity scenarios, organized into three semi-
structured interview protocols. These scenarios were
not formally vetted by external experts. Each in-
terview lasted about one hour during which an in-
terviewer asked the student to identify and scruti-

3

nize security concerns for each of four scenarios. We
audio- and video-recorded each interview and pro-
duced a written transcript.
In addition, we saved
any diagrams drawn by the student. We analyzed
student statements using a structured qualitative
method, novice-led paired thematic analysis, to doc-
ument student misconceptions and problematic rea-
sonings [HZL12, MHB+15].

The following excerpt illustrates a typical exchange
that took place during an interview. In the given sce-
nario, the interviewer presents evidence of a possible
SQL injection attack vulnerability, one of the most
common software security issues.

Scenario A3: Database Input Error.
Interviewer 1: When a user Mike O’Brien registered
a new account for an online shopping site, he was re-
quired to provide his username, address, ﬁrst and last
name. Immediately after Mike submitted his request,
you–as the security engineer–receive a database input
error message in the logs. What might you infer from
this error message?
Subject: It’s very costly to implement security at the
server level. But at the client side, SQL injection
methods are used by hackers to get into database.
Simple queries, like avoiding special characters, which
can be used for hacking, can be avoided.
Interviewer 1: Would it be suﬃcient for them to add
that sort of input checking at the computer?
Subject: Client level, yeah. Implementing security at
the client level is easier than in database and server
level. Because at server level the cost is insane.
. . . I would say somebody is trying to do
Subject:
SQL? When they are trying to gain access to some-
one’s account by adding in a piece of SQL that would
always return true, but there are ways to do that too.
Interviewer 2: How would you do it?
Subject: You would not allow certain characters to
say username, password ﬁeld or you would make sure
that there is enough quotations around the password,
so the if someone were to try to break out of the
quotations whatever content the user entered would
be escaped. So you wouldn’t accent a quote as a raw
quote but an escaped character, so that the query
database system would know that the quote is not

the end of a set of strings as part of the query its
data in the query. ∎

Among other issues, this exchange illustrates the
following theme that we commonly observed: incor-
rect assumptions—failure to see vulnerabilities and
limiting the adversary. The student incorrectly as-
serts that it would be suﬃcient to implement a de-
fense at the client side, apparently failing to appreci-
ate that an adversary might attack the server and/or
that the client might be malicious.

In addition, the student’s focus on the client (with
less attention to the server) suggests a user bias, per-
haps because the student’s experience may have pri-
marily been as a user. Such a limited mental perspec-
tive can blind the student to vulnerabilities in other
aspects of the system and lead to inappropriate con-
clusions.

In addition to the themes of incorrect assumptions
and biases, we also commonly observed two addi-
tional themes: over generalizations and conﬂating
concepts. For example, some students stated that no
Internet communications are secure; others seemed
to believe that use of biometrics always greatly en-
hances the strength of authentication systems. We
observed students conﬂating many concepts, includ-
ing, for example, authorization and authentication,
hashing and encryption, and threat and risk.

Biases we observed include user, physical, and per-
sonal. For example, some students assumed that
physical security is usually stronger than software se-
curity. Others assumed that data security is usu-
ally stronger in the USA than in other countries.
These biased assumptions are not necessarily true,
and there is not always a clean boundary separat-
ing the referenced categories (e.g., the security of
software systems depends in part on the security of
hardware components, and hackers often cross inter-
national boundaries electronically).

In a companion paper [SDH+16], we discuss an-
swers to six of our scenarios, to provide instructive
case studies for students and educators.

4

4 Drafting Assessment Questions

Building on the CCI Dephi process, scenarios, and
student interviews, we developed thirty draft CCI
multiple-choice questions of varying diﬃculty. For
each of the twelve scenarios, we developed one or
more questions. Each question targets one of the
top ﬁve topics identiﬁed in the CCI Delphi process.
Each question has exactly one best answer, and the
distractor (incorrect) choices are inspired, whenever
possible, by commonly observed misconceptions from
the interviews.

We developed these assessment items through a
collaborative iterative process while seated around a
conference table. Our team included experts in cy-
bersecurity and education, including professors and
graduate students. We endeavored to follow best
practices for creating multiple choice questions, in-
cluding heeding the “Vanderbilt advice” [Bra]. It was
helpful to revisit draft questions periodically to revise
and polish them. Everyone discovered that produc-
ing high-quality questions is challenging.

The representative draft question in Figure 1 il-
lustrates the results of our process. This question
targets the topic of devising a defense in the con-
text of the database input error scenario introduced
in Section 3.

Distractor B is inspired by the misconception dis-
cussed for the interview excerpt from Section 3. The
other distractors are not bad actions to include, but
they alone do not mitigate the vulnerability. Alter-
native A is clearly the best choice among incomplete
alternatives.

5 Next Steps

This year we will validate our draft CCI multiple-
choice questions using cognitive interviews, expert
reviews, and psychometric testing. First, we will
carry out a pilot study seeking feedback from at least
twenty experts and administering the CCI to at least
200 students. We will use the results to revise and
improve the draft questions. Then, applying item-
response theory [HSR91, HSJ93], we will analyze re-
sults from administering the CCI to at least 1000 stu-
dents. We expect the ﬁnal test to comprise twenty-

Scenario A3. When a user Mike O’Brien registered
a new account for an online shopping site, he was re-
quired to provide his username, address, ﬁrst and last
name. Immediately after Mike submitted his request,
you–as the security engineer–receive a database input
error message in the logs.

Question A3-3. Choose the best defense to protect
against possible security problems suggested by this
error:

A. Sanitize input at the server side.

B. Place security controls at the client side.

C. Require all characters to be from a restricted set

of characters.

D. Implement the system in a secure programming

language.

E. Test the software more thoroughly before deploy-

ing it.

Figure 1: A sample CCI assessment item associated
with Scenario A-3 (database input error).

ﬁve questions, ﬁve per topic.

In addition, we plan to continue development of
the CCA. The next step is to develop draft questions.
We envision that the CCA will target the same topics
from the CCI but at greater technical depth.

The CATS project is meeting the need for rigorous
evidence-based assessment tools to inform the devel-
opment of best practices in cybersecurity education.
In particular, the CCI will enable researchers to sci-
entiﬁcally quantify and measure the eﬀect of their
approaches to ﬁrst courses in cybersecurity. We wel-
come feedback on our work, and we would be de-
lighted to hear from any researcher who might like to
participate in our study.

6 Acknowledgments

This work was supported in part by the U.S. Depart-
ment of Defense under CAE-R grants H98230-15-1-
0294 and H98230-15-1-0273, and by the National Sci-
ence Foundation under SFS grant 1241576.

5

References

[Bra]

[cis]

[FS15]

choice

Brame.

Writing

C.
J.
multiple
https://cft.vanderbilt.edu/guides-sub
-pages/writing-good-multiple-choice
-test-questions/. [accessed 3-12-17].

good
questions.

test

Information

Certiﬁed
tems
https://www.isc2.org/cissp/default.aspx.
[accessed 3-14-17].

Sys-
Professional.

Security

2

Global
Study.

Frost and Sullivan. The 2015 (ISC)
Information
Security Workforce
https://www.isc2cares.org/uploadedFiles/
wwwisc2caresorg/Content/GISWS/
FrostSullivan-(ISC)%C2%B2-Global
-Information-Security-Workforce
-Study-2015.pdf, 2015.

+

[GGH

10] K. Goldman, P. Gross, C. Heeren, G. L.
Herman, L. Kaczmarczy, M. C. Loui, and
C. Zilles. Setting the scope of concept in-
ventories for introductory computing sub-
ject. ACM Transactions on Computing Edu-
cation,, 10(2):5:1–29, 2010.

[HCZL14] G. L. Herman, C. C. Zilles, and M. C. Loui. A
psychometric evaluation of the Digital Logic
Concept Inventory. Computer Science Edu-
cation, 24(4):277–303, 2014.

[HSJ93]

[HSR91]

R. Hambleton, K. Swaminathan, and R. J.
Jones. Comparison of classical test theory
and item response theory and their applica-
tions to test development. Educational Mea-
surement:
Issues & Practice, 12:253–262,
1993.

R. Hambleton, K. Swaminathan, and H. J.
Rogers. Fundamentals of Item Response The-
ory. Sage Publications, Newbury Park, CA,
1991.

[HWS92] D. Hestenes, M. Wells, and G. Swack-
The

Force Concept Inventory.

hamer.
Physics Teacher, 30:141–166, 1992.

[HZL12]

G. L. Herman, C. Zilles, and M. C. Loui.
Flip-ﬂops in students’ conceptions of state.
IEEE Transactions on Education, 55(1):88–
98, 2012.

[PB14]

[PCG01]

O. Adesope. Patterns of student conceptual
understanding across engineering content ar-
International Journal of Engineering
eas.
Education, 31(6A):1587–1604, 2015.

J. Pellegrino and S. Brophy. Cambridge
Handbook on Engineering Education Re-
search, chapter The science and design of
assessment in engineering education, pages
571–600. Cambridge University Press, 2014.

J. Pellegrino, N. Chudowsky, and R. Glaser.
Knowing What Students Know: The Science
and Design of Educational Assessment. Na-
tional Academy Press, 2001.

+

[PDH

16] G. Parekh, D. DeLatte, G. L. Herman,
L. Oliva, D. Phatak, T. Scheponik, and A. T.
Sherman.
Identifying core concepts of cy-
bersecurity: Results of two Delphi processes.
IEEE Transactions on Education, May 2016.
accepted.

+

[SDH

16] A. T. Sherman, D. DeLatte, G. L. Herman,
M. Neary, L. Oliva, D. Phatak, T. Scheponik,
and J. Thompson. Cybersecurity: Exploring
core concepts through six scenarios. Cryp-
tologia, May 2016. in review.

[SMSR

+

11] R. A. Streveler, R. L. Miller, A. I. Santiago-
Roman, M. A. Nelson, M. R. Geist, and
B. M. Olds. Rigorous method for concept in-
ventory development: Using the ‘assessment
triangle’ to develop and test the thermal and
transport science concept inventory (TTCI).
International Journal of Engineering Educa-
tion, 27:968–984, 2011.

+

[SSD

16] T. Scheponik, A. T. Sherman, D. DeLatte,
D. Phatak, L. Oliva, J. Thompson, and G. L.
Herman. How students reason about cyber-
security concepts. In Proceedings of the Fron-
tiers in Education Conference, October 2016.

[THS

+

17]

J. Thompson, G. L. Herman, T. Scheponik,
E. Golaszewski, A. T. Sherman, D. DeLatte,
K. Patsourakos, D. Phatak, and L. Oliva.
Student misconceptions about cybersecurity
concepts: Analysis of think-aloud interviews
with students. February 2017.
in prepara-
tion.

+
[MHB

15] D. B. Montfort, G. L. Herman, S. A. Brown,
H. M. Matusovich, R. A. Streveler, and

Appears in the proceedings of the 2017 National Cyber
Summit (June 6–8, 2017, Huntsville, AL).

6

