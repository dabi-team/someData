Argumentation Models for Cyber Attribution

Eric Nunes, Paulo Shakarian
Arizona State University
Tempe, AZ 85281, USA
Email: {enunes1, shak} @asu.edu

Gerardo I. Simari
Inst. for CS and Eng. (CONICET–UNS)
DCIC, UNS, Bahia Blanca, Argentina
Email: gis@cs.uns.edu.ar

Andrew Ruef
Trail of Bits, Inc.
New York, NY 10003, USA
Email: andrew@trailofbits.com

6
1
0
2

l
u
J

7

]
I

A
.
s
c
[

1
v
1
7
1
2
0
.
7
0
6
1
:
v
i
X
r
a

Abstract—A major challenge in cyber-threat analysis is com-
bining information from different sources to ﬁnd the person or
the group responsible for the cyber-attack. It is one of the most
important technical and policy challenges in cyber-security. The
lack of ground truth for an individual responsible for an attack
has limited previous studies. In this paper, we take a ﬁrst step
towards overcoming this limitation by building a dataset from
the capture-the-ﬂag event held at DEFCON, and propose an
argumentation model based on a formal reasoning framework
called DeLP (Defeasible Logic Programming) designed to aid
an analyst in attributing a cyber-attack. We build models from
latent variables to reduce the search space of culprits (attackers),
and show that this reduction signiﬁcantly improves the perfor-
mance of classiﬁcation-based approaches from 37% to 62% in
identifying the attacker.

I. INTRODUCTION

A major challenge in cyber-threat analysis is to ﬁnd the
person or the group responsible for a cyber-attack. This is
known as cyber-attribution [17] and it is one of the central
technical and policy challenges in cyber-security. Oftentimes,
the evidence collected from multiple sources provides a con-
tradictory viewpoint. This gets worse in cases of deception
where either an attacker plants false evidence or the evidence
points to multiple actors, leading to uncertainty. In the text
on cyber-warfare [17] the authors discuss the difﬁculties that
an intelligence analyst faces in attributing an attack to a
perpetrator given that deception might have occurred, and how
the analyst needs to explore deception hypotheses under the
given attack scenario.

However, one of the major drawbacks of the study and
evaluation of cyber-attribution models is the lack of datasets
with the ground truth available regarding the individual party
responsible for the attack—this has limited previous studies.
To overcome this, we built and leveraged a dataset from the
capture-the-ﬂag event held at DEFCON. In previous work,
this dataset was used to study cyber-attribution, framing it
as a multi-label classiﬁcation problem to predict
the at-
tacker [13]. Machine learning approaches struggle in situations
of deception, where similar attributes point towards multiple
attackers—we propose to address this issue using a formal
logical framework.

Speciﬁc contributions of this paper include:

• description of how a model for cyber-attribution can
be designed and implemented in the DeLP structured
argumentation framework;

• experiments demonstrating that using argumentation-
based tools can signiﬁcantly reduce the number of po-
tential culprits that need to be considered in the analysis
of a cyber-attack; and

• experiments showing that the reduced set of culprits,
used in conjunction with classiﬁcation, leads to improved
cyber-attribution decisions.

Related work: Adversarial machine learning is an emerging
ﬁeld of study. It uses effective machine learning techniques to
identify or defend against an adversary’s opponents. Under-
standing the limits of adversary’s knowledge and capabilities
is crucial for coming up with countermeasures, as discussed
in [9]. Here the authors propose models to study these limi-
tations to come up with evasion techniques. On the contrary,
Lowd and Meek [12] explore the problem from an adversarial
point of view. They propose strategies that an adversary can
use to reverse engineer a classiﬁer so that his attacks are
undetected by the classiﬁer. They use a real world application
in spam ﬁltering to demonstrate their method, which they call
adversarial classiﬁer evasion. In a spam ﬁltering setting an
example of such a technique is replacing feature words that
raise a red ﬂag with their synonyms to evade detection. This
feature cross substitution technique is discussed in [10]. Here
the authors offer a simple heuristic method based on mixed-
integer linear programming with constraint generation to make
the classiﬁer robust to cross substitution techniques. There is
research that looks at modeling the interaction between the
learner (adversary) and the classiﬁer in terms of a competition
using Stackelberg games [4], [3]. Most adversarial machine
learning applications deal with modeling classiﬁers to be ro-
bust against evasive techniques in real world applications like
malware detection and spam ﬁltering. Cyber-attribution falls in
the domain of adversarial learning, but looks at analyzing the
evidence in the aftermath of an attack to discover the attacker.

Currently, cyber-attribution is limited to identifying ma-
chines [2] as opposed to the hacker or their afﬁliation to a
group or a state. An example of such a technical attribution
approach is WOMBAT [5], where a clustering technique is
used to group attacks to common IP sources. A method that
combines information from different sources was proposed by
Walls [22], who considered forensic information from diverse
sources but did not account for inconsistency or uncertainty
due to deception. A less rigorous mathematical model, known
as the Q model [15], was proposed recently; the model answers

 
 
 
 
 
 
queries from an analyst, and by combining these answers the
analyst attributes an attack to a party. Unfortunately, there
are no experimental evaluations of its effectiveness. Argumen-
tation has been used for cyber reasoning [1] by leveraging
arguments to deal with incomplete and contradictory data,
allowing to derive big-picture conclusions to keep systems
secure and online in case of an attack. This is a different
application than the one we are addressing.

In [20], a tool was presented to support human deci-
sions, focusing on how user trust in the evidence inﬂuences
the process; a user study demonstrating the hypotheses was
presented in [16]. Concurrently, a formal logical framework
for reasoning about cyber-attribution has been devised [18],
[19]; it explores multiple competing hypotheses based on the
evidence for and against a particular attacker to help analysts
decide on an attribution, providing a map of the reasoning that
led to the decision.

The rest of the paper is organized as follows. We present
a description of our DEFCON capture-the-ﬂag dataset and an
analysis on the occurrence of deception within this data in
Section II. This is followed by the argumentative model based
on [8] in Section III. We then summarize results from [13] and
discuss how we built our baseline argumentation model along
with two other extended baseline models for cyber-attribution
with DeLP in Section V and Section VI, with a discussion of
the experimental results obtained with each of these models.
Conclusions are discussed in Section VII.

II. DEFCON CTF DATASET

The DEFCON security conference sponsors and hosts a
capture the ﬂag (CTF) competition every year, held on site
with the conference in Las Vegas, Nevada. DEFCON CTF is
one of the oldest and best-known competitions. The ctftime.org
site provides a ranking for CTF teams and CTF competitions,
and in this system DEFCON CTF has the highest average
weight of all other CTF competitions.

CTF competitions can be categorized by what role the
competitors play in the competition: either red team, blue
team, or a combination. In a blue team focused CTF the
competitors harden their systems against a red team played
by the organizers of the CTF. In a combined red/blue team
CTF every team plays both blue and red team simultaneously.
The NCCDC and CDX competitions are examples of a blue
team CTF, while DEFCON CTF is a combined red/blue team.
Each team is simultaneously responsible for hardening and
defending their systems as well as identifying vulnerabilities
and exploiting them in other teams’ systems.

The game environment is created primarily by the DEFCON
CTF organizers. The game focuses around programs (known in
the game as services) written by the organizers. These services
are engineered to contain speciﬁc vulnerabilities. The binary
image of the service is made available to each team at the
start of the game, but no other information about the service
is released. Part of the challenge of the game is identifying the
purpose of each service as well as the vulnerabilities present
in the service. Identiﬁcation of vulnerabilities serves both a

defensive and offensive goal. Once a vulnerability has been
identiﬁed, a team may patch this vulnerability in the binary
program. Additionally, the teams may create exploits for that
vulnerability and use them to attack other teams and capture
digital ﬂags from those teams’ systems.

Each team is also provided with a server running the
services, which contains the digital ﬂags to be defended. To
deter defensive actions such as powering off the server or
stopping the services, the white team (a third team, played
by the organizers) conducts periodic availability tests of the
services running on each team’s server. A team’s score is the
sum of the value of the ﬂags they have captured, minus the sum
of the ﬂags that have been captured from that team, multiplied
by an availability score determined by how often the white
team was able to test that team’s services. This scoring model
incentivizes teams to keep their server online, identify the
vulnerabilities in services and patch them quickly, and exploit
other teams services to capture their ﬂags. It disincentivizes
teams’ from performing host-level blocking and shutting down
services, as this massively impacts their ﬁnal score.

This game environment can be viewed as a microcosm of
the global Internet and the careful game of cat and mouse
between hacking groups and companies. Teams are free to use
different technical means to discover vulnerabilities. They may
use fuzzing and reverse engineering on their own programs,
or, they may monitor the network data sent to their services
and dynamically study the effects that network data has on
unpatched services. If a team discovers a vulnerability and uses
it against another team, the ﬁrst team may discover that their
exploit is re-purposed and used against them within minutes.
The organizers of DEFCON CTF capture all of the network
trafﬁc sent and received by each team, and publish this trafﬁc
at the end of the competition [6]. This includes IP addresses
for source and destination, as well as the full data sent and
received and the time the data was sent or received. This data
is not available to contestants; depending on the organizers’
choice from year to year, the contestants either have a real time
feed but with the IP address obscured, or a full feed delivered
on a time delay of minutes to hours.
Analysis: We use the data from the CTF tournament held at
DEFCON 21 in 2013. The CTF data set is very large, about
170 GB in compressed format. We used multiple systems
with distributed and coordinated processing to analyze the
entire dataset—fortunately, analyzing individual streams is
easy to parallelize. To analyze this data, we identiﬁed the
TCP ports associated with each vulnerable service. From
this information, we used the open source tool tcpﬂow to
process the network captures into a set of ﬁles, with each ﬁle
representing data sent or received on a particular connection.
With these data ﬁles identiﬁed, we analyzed some of them
by hand using the Interactive Disassembler (IDA) to determine
if the data contained shell-code, which in fact was the case. We
used an automated tool to produce a summary of each data
ﬁle as a JSON encoded element. Included in this summary
was a hash of the contents of the ﬁle and a histogram of the
processor instructions contained in the ﬁle. These JSON ﬁles

TABLE 1: Fields in an instance of network attack

Field

Intuition

Value

byte hist

inst hist

Histogram of byte sequences
in the payload

0×43:245,
0×3a:9, .....

0×69:8,

Histogram of
used in the payload

instructions

cmp:12,
movtmi:60 ......

subs:8,

from team

The team where the payload
originates (attacking team)

Blue Lotus

to team

time

The team being attacked by
the exploit

Indicates the date and time of
the attack

Robot Maﬁa

2013-08-03T23:45:17

were the ﬁnal output of the low-level analysis, transforming
hundreds of gigabytes of network trafﬁc into a manageable set
of facts about exploit trafﬁc in the data. Each JSON ﬁle is a list
of tuples (time-stamp, byte-histogram, instruction-histogram,
attack team and target team). The individual ﬁelds of the tuple
are listed in Table 1.

The pre-processing can be summarized in the following

steps:

• Un-tarring the archives available from the organizers. The
archives produce a large number of pcap-ng formatted
ﬁles that contain the trafﬁc captures.

• Conversion of the pcap-ng ﬁles to tcpdump format cap-
ture using the editcap utility. This is to allow tcpﬂow to
process the data.

• Use of xargs and GNU parallel to run tcpﬂow on each
pcap. This is a time-consuming process, and produced a
directory structure with ﬁles for data sent and received
on host-port socket pairs. This step of processing allows
ﬁle-based tools to process the network data.

• A tool to process each ﬁle containing data sent or received
by network ports associated with CTF challenges. These
tools produced summary statistics for each data stream,
to include a byte histogram, overall size, a hash, and an
ARM instruction histogram (we ran a linear sweep with
the Capstone instruction decoder to produce this). This
data was saved via JSON.

After this pre-processing of the network data packets, we
have around 10 million network attacks consisting of around
1 million unique exploits built and used by 20 teams in the
competition. In order to attribute an attack to a particular
team, apart from analyzing the payloads used by the team,
we also need to analyze the behavior of the attacking team
towards their adversary. For this purpose, we divide the attacks
according to the team being targeted. Thus, we have 20 such
subsets, which we represent as T-i, where i ∈ {1, 2, 3, ..., 20}.
The processed dataset is publicly available 1.

We now discuss two important observations from the
dataset, which make the task of attributing an observed net-
work attack to a team difﬁcult.
Deception: In the context of this paper we deﬁne an attack to

1http://lab.engineering.asu.edu/cysis/cyber-attribution/

be deceptive when multiple adversaries get mapped to a single
attack pattern; deception is thus a scenario in which the same
exploit is used by multiple teams to target the same team. The
number of unique deceptive attacks amount to just under 35%
of the total unique attacks in our dataset—clearly, deception
is a heavily-used technique in this domain.
Duplicate attacks: A duplicate attack occurs when the same
team uses the same payload to attack the same team at different
points in time. We group duplicates as either being non-
deceptive or deceptive. Non-deceptive duplicates are the copies
of the attacks launched by the team that ﬁrst initiated the use
of a particular payload; on the other hand, deceptive duplicates
are all the attacks from the teams that did not initiate the use.

III. ARGUMENTATION MODEL

Our approach relies on a model of the world where we can
analyze competing hypotheses in a cyber-operation scenario.
Such a model should allow for contradictory information so it
can handle inconsistency in cases of deception.

Before describing the argumentation model in detail, we
introduce some necessary notation. Variables and constant
symbols represent items such as the exploits/payloads used
for the attack, and the actors conducting the cyber-attack (in
this case, the teams in the CTF competition). We denote the
set of all variable symbols with V and the set of all constants
with C. For our model we require two subsets of C: Cact,
denoting the actors capable of conducting the cyber-operation,
and Cexp, denoting the set of unique exploits used. We use
symbols in all capital letters to denote variables. In the running
example, we use a subset of our DEFCON CTF dataset.

Example 1. Actors and cyber-operations from the CTF
data: Cact = {bluelotus, robotmaﬁa, apt8}, Cexp =
{exploit1, exploit2, ..., exploitn}.

The language also contains a set of predicate symbols that
have constants or variables as arguments, and denote events
that can be either true or false. We denote the set of predicates
with P; examples of predicates are shown in Table 2. For
instance, culprit(exploit1, apt8) will either be true or false, and
denotes the event where apt8 used exploit1 to conduct a cyber-
operation.

TABLE 2: Example predicates and explanation

Predicate

Explanation

attack(exploit1, bluelotus)

exploit1 was targeted towards the
team Blue Lotus.

replay attack(E, Y)

Exploit E was replayed by team Y.

deception(exploit1, apt8)

time diff(I, Y)

culprit(exploit1, apt8)

Team apt8 used exploit1 for decep-
tion.

Team Y was deceptive within the
given time interval I.

Team apt8 is the likely culprit for
the attack (using exploit1 on the
target team).

A ground atom is composed by a predicate symbol and
a tuple of constants, one for each argument. The set of all

ground atoms is denoted as G. A ground literal L is a ground
atom or a negated ground atom; hence, ground literals have
no variables. An example of a ground atom for our running
example is attack(exploit1, bluelotus). We denote a subset of
G with G(cid:48).

We choose a structured argumentation framework [14] for
our model; our approach works by creating arguments (in
the form of a set of rules and facts) that compete with each
other to attribute an attack to a given perpetuator. In this case,
arguments are defeated based on contradicting information in
other arguments. This procedure is known as a dialectical
process, where the arguments that are undefeated prevail.
An important result is the set of all the arguments that are
warranted (not defeated) by any other argument, which give
a clear map supporting the conclusion. Such transparency
lets a security analyst not only add new arguments based
on new evidence discovered in the system, but also get rid
of incorrect information and ﬁne-tune the model for better
performance. Since the argumentation model can deal with
inconsistent information, it draws a natural analogy to the way
humans settle disputes when there is contradictory information
available. Having a clear explanation of why one argument is
chosen over others is a desirable characteristic for both the
analyst and for organizations to make decisions and policy
changes. We now brieﬂy discuss some preliminaries on DeLP.
Defeasible Logic Programming: DeLP is a formalism that
combines logic programming with defeasible argumentation;
full details are discussed in [8]. The formalism is made up
of several constructs, namely facts, strict rules, and defeasible
rules. Facts represent statements obtained from evidence, and
are always true; similarly, strict rules are logical combinations
of elements (facts or other inferences) that can always be
performed. On the contrary, defeasible rules can be thought
of as strict rules that may be true in some situations, but
could be false if contradictory evidence is present. These three
constructs are used to build arguments, and DeLP programs
are sets of facts, strict rules and defeasible rules. We use the
usual notation for DeLP programs, denoting the knowledge
base with Π = (Θ, Ω, ∆), where Θ is the set of facts, Ω is
the set of strict rules, and ∆ is the set of defeasible rules.
Examples of the three constructs are provided with respect to
the dataset in Fig. 1. We now describe the constructs in detail.
Facts (Θ) are ground literals that represent atomic information
or its (strong) negation (¬).

Strict Rules (Ω) represent cause and effect information; they
are of the form L0 ← L1, ...Ln, where L0 is a literal and
{Li}i>0 is a set of literals.

Defeasible Rules (∆) are weaker versions of strict rules, and
are of the form L0 -≺ L1, ...., Ln, where L0, is the literal and
{Li}i>0 is a set of literals.

When a cyber-attack occurs, the model can be used to derive
arguments as to who could have conducted the attack. Deriva-
tion follows the same mechanism as logic programming [11].
DeLP incorporates defeasible argumentation, which decides
which arguments are warranted and it blocks arguments that

Θ :

Ω :

∆ :

θ1 = attack(exploit1, bluelotus)
θ2 = ﬁrst attack(exploit1, robotmaﬁa)
θ3 = last attack(exploit1, apt8))
θ4 = time diff(interval, robotmaﬁa)
θ5 = most

frequent(exploit1, pwnies)

ω1 = culprit(exploit1, pwnies) ←

frequent(exploit1, pwnies),

most
replay attack(exploit1)

ω2 = ¬ culprit(exploit1, robotMaﬁa) ←

last attack(exploit1, apt8),
replay attack(exploit1)

δ1 = replay attack(exploit1) -≺

attack(exploit1, bluelotus),
last attack(exploit1, apt8)

δ2 = deception(exploit1, apt8) -≺
replay attack(exploit1),
ﬁrst attack(exploit1, robotmaﬁa)

δ3 = culprit(exploit1, apt8) -≺

deception(exploit1, apt8),
replay attack(exploit1)

δ4 = ¬culprit(exploit1, apt8) -≺

time diff(interval, robotmaﬁa)

Fig. 1: A ground argumentation framework.

(cid:104)A1, replay attack(exploit1) (cid:105)
A1 = {δ1, θ1, θ3}
(cid:104)A2, deception(exploit1, apt8) (cid:105) A2 = {δ1, δ2, θ2}
(cid:104)A3, culprit(exploit1, apt8)(cid:105)
A3 = {δ1, δ2, δ3}
(cid:104)A4, ¬culprit(exploit1, apt8)(cid:105)
A4 = {δ1, δ4, θ3}

Fig. 2: Example ground arguments from Figure 1.

are in conﬂict and a winner cannot be determined. Fig. 1 shows
a ground argumentation framework demonstrating constructs
derived from the CTF data. For instance, θ1 indicates the fact
that exploit1 was used to target the team Blue Lotus, and θ5
indicates that team pwnies is the most frequent user of exploit1.
For the strict rules, ω1 says that for a given exploit1 the attacker
is pwnies if it was the most frequent attacker and the attack
exploit1 was replayed. Defeasible rules can be read similarly;
δ2 indicates that exploit1 was used in a deceptive attack by
APT8 if it was replayed and the ﬁrst attacker was not APT8.
By replacing the constants with variables in the predicates we
can derive a non-ground argumentation framework.

Deﬁnition 1. (Argument) An argument for a literal L is a
pair (cid:104)A, L(cid:105), where A ⊆ Π provides a minimal proof for L
meeting the requirements: (1) L is defeasibly derived from A2,
(2) Θ ∪ Ω ∪ ∆ is not contradictory, and (3) A is a minimal
subset of ∆ satisfying 1 and 2, denoted (cid:104)A, L(cid:105).

Literal L is called the conclusion supported by the ar-
gument, and A is the support. An argument (cid:104)B, L(cid:105) is a
subargument of (cid:104)A, L(cid:48)(cid:105) iff B ⊆ A. The following examples
show arguments for our scenario.

Example 2. Fig. 2 shows example arguments based on the

2This means that there exists a derivation consisting of a sequence of rules

that ends in L—that possibly includes defeasible rules.

KB from Fig. 1; here, (cid:10)A1, replay attack(exploit1)(cid:11) is a
subargument of (cid:10)A2, deception(exploit1, apt8)(cid:11) and (cid:10)A3,
culprit(exploit1, apt8)(cid:11).

For a given argument there may be counter-arguments that
contradict it. For instance, referring to Fig. 2, we can see that
A4 attacks A3. A proper defeater of an argument (cid:104)A, L(cid:105) is
a counter-argument that—by some criterion—is considered to
be better than (cid:104)A, L(cid:105); if the two are incomparable according
to this criterion, the counterargument is said to be a blocking
defeater. The default criterion used in DeLP for argument
comparison is generalized speciﬁcity [21].

A sequence of arguments is called an argumentation line.
There can be more than one defeater argument, which leads to
a tree structure that is built from the set of all argumentation
lines rooted in the initial argument. In this dialectical tree,
every child can defeat its parent (except for the root), and the
leaves represent unchallenged arguments; this creates a map
of all possible argumentation lines that decide if an argument
is defeated or not. Arguments that either have no attackers or
all attackers have been defeated are said to be warranted.

Given a literal L and an argument (cid:10)A, L(cid:11), in order to
decide whether or not a literal L is warranted, every node
in the dialectical tree T ((cid:104)A, L(cid:105)) is recursively marked as “D”
(defeated) or “U” (undefeated), obtaining a marked dialectical
tree T ∗((cid:104)A, L(cid:105)) where:

• All leaves in T ∗((cid:104)A, L(cid:105)) are marked as “U”s, and
• Let (cid:104)B, q(cid:105) be an inner node of T ∗((cid:104)A, L(cid:105)). Then, (cid:104)B, q(cid:105)
will be marked as “U” iff every child of (cid:104)B, q(cid:105) is marked
as “D”. Node (cid:104)B, q(cid:105) will be marked as “D” iff it has at
least one child marked as “U”.

Given argument (cid:104)A, L(cid:105) over Π, if the root of T ∗((cid:104)A, L(cid:105))
is marked “U”, then T ∗((cid:104)A, h(cid:105)) warrants L and that L is
warranted from Π. (Warranted arguments correspond to those
in the grounded extension of a Dung argumentation system
[7].)

In practice, an implementation of DeLP accepts as input sets
of facts, strict rules, and defeasible rules. Note that while the
set of facts and strict rules is consistent (non-contrdictory),
the set of defeasible rules can be inconsistent. We engineer
our cyber-attribution framework as a set of defeasible and
strict rules whose structure was created manually, but are
dependent on values learned from a historical corpus of data.
Then, for a given incident, we instantiate a set of facts for
that situation. This information is then provided as input
into a DeLP implementation that uses heuristics to generate
all arguments for and against every possible culprit for the
cyber attack. Dialectical trees based on these arguments are
analyzed, and a decision is made regarding which culprits are
warranted. This results in a reduced set of potential culprits,
which we then use as input into a classiﬁer to obtain an
attribution decision.

IV. BASELINE ARGUMENTATION MODEL (BM)

In [13] machine learning techniques were leveraged on the
CTF data to identify the attacker. We will now provide a sum-

ω1 = culprit(E, Y) ← last attack(E, Y), replay attack(E).
δ1 = replay attack(E) -≺ attack(E, X), last attack(E, Y).

Fig. 3: Defeasible and strict rule for non-deceptive attack.

mary of the results obtained. The experiment was performed
as follows. The dataset was divided according to the target
team, building 20 subsets, and all the attacks were then sorted
according to time. The ﬁrst 90% of the attacks were reserved
for training and the remaining 10% for testing. The byte and
instruction histograms were used as features to train and test
the model. Models constructed using a random forest classiﬁer
performed the best, with an average accuracy of 0.37. Most
of the misclassiﬁed samples tend to be deceptive attacks and
their duplicates.

When using machine learning approaches it is difﬁcult to
map the reasons why a particular attacker was predicted,
especially in cases of deception where multiple attackers were
associated with the same attack. Knowing the arguments that
supported a particular decision would greatly aid the analyst in
making better decisions dealing with uncertainty. To address
this issue we now describe how we can form arguments/rules
based on the latent variables computed from the training data,
given an attack for attribution.

We use the following notation: let E be the test attack
under consideration aimed at target team X, Y represent all
the possible attacking teams, and D be the set of all deceptive
teams (those using the same payload to target the same team)
if the given attack is deceptive in the training set. For non-
deceptive attacks, D will be empty. We note that facts cannot
to compress the
have variables, only constants (however,
program for presentation purposes, we use meta-variables in
facts). To begin, we deﬁne the facts: θ1 = attack (E, X), θ2 =
ﬁrst attack (E, Y), θ3 = last attack (E, Y); θ1 states that
attack E was used to target team X, θ2 states that team Y
was the ﬁrst team to use the attack E in the training data, and
similarly θ3 states that team Y was the last team to use the
attack E in the training data. The ﬁrst and last attacking team
may or may not be the same. We study the following three
cases:
Case 1: Non-deceptive attacks. In non-deceptive attacks, only
one team uses the payload to target other teams in the training
data. It is easy to predict the attacker for these cases, since
the search space only has one team. To model this situation,
we deﬁne a set of defeasible and strict rules.

In Fig. 3, defeasible rule δ1 checks whether the attack was
replayed in the training data. Since it is a non-deceptive attack,
it can only be replayed by the same team. The strict rule ω1
then puts forth an argument for the attacker (culprit) if the
defeasible rule holds and there is no contradiction for it.
Case 2: Deceptive attacks. These attacks form the majority
of the misclassiﬁed samples in [13]. The set D is not empty
for this case; let Di denote the deceptive teams in D. We
also compute the most frequent attacker from the training
data given a deceptive attack. Let the most frequent deceptive

θ1 = decep (E, X), θ2 = frequent (E, F )

ω1 = ¬culprit(E, Y) ← ﬁrst attack(E, Y), decep(E, X)
ω2 = culprit(E, F ) ← frequent(E, F ), deception (E, Di)

δ1 = replay attack(E) -≺ attack(E, X), last attack(E, Y)
δ2 = deception(E, Di) -≺ replay attack(E),
ﬁrst attack(E, Y)

δ3 = culprit(E, Di) -≺ deception(E, Di), ﬁrst attack(E, Y)

Fig. 4: Facts and rules for deceptive attacks.

attacker be denoted as F . The DeLP components that model
this case are shown in Figure 4; fact θ1 indicates if the attack
E was deceptive towards the team X and θ2 indicates the most
frequent attacker team F from the training set. The strict rule
ω1 indicates that in case of deception the ﬁrst team to attack
(Y) is not the attacker, ω2 states that the attacker should be
F if the attack is deceptive and F was the most frequent
deceptive attacker. For the defeasible rules, δ1 deals with the
case in which the attack E was replayed, δ2 deals with the
case of deceptive teams from the set D, δ3 indicates that all the
deceptive teams are likely to be the attackers in the absence of
any contradictory information. and δ4 states that the attacker
should be F if the attack is deceptive and F was the most
frequent attacker.
Case 3: Previously Unseen Attacks. The most difﬁcult
attacks to attribute in the dataset are the unseen ones, i.e.
attacks ﬁrst encountered in the test set and thus did not occur
in the training set. To build constructs for this kind of attack
we ﬁrst compute the k nearest neighbors from the training set
according to a simple Euclidean distance between the byte
and instruction histograms of the two attacks. In this case
we choose k = 3. For each of the matching attacks from
the training data we check if the attack is deceptive or non-
deceptive. If non-deceptive, we follow the procedure for Case
1, otherwise we follow the procedure for Case 2. Since we
replace one unseen attack with three seen attacks, the search
space for the attacker increases for unseen attacks.

Attacker Time Analysis: The CTF data provides us with time
stamps for the attacks in the competition. We can use this
information to come up with rules for/against an argument for
a team being the attacker. We compute the average time for a
team to replay its own attack given that it was the ﬁrst one to
deploy the attack (see Fig. 5). It can be observed that teams
like more smoked leet chicken (T-13) and Wowhacker-bios (T-
8) are very quick to replay their own attacks as compared to
other teams. Fig. 5 also shows the average time for a team
to perform a deceptive attack. Teams like The European (T-7)
and Blue lotus (T-10) are quick to commit deception, while
others take more time.

We use this time information to narrow down our search
space for possible attackers. In particular, for a deceptive
test sample, we compute the time difference between the test
sample and the training sample that last used the same payload.
We denote this time difference as (cid:52)t, and include it as a

Fig. 5: Average time for team to perform a deceptive attack
and replay its own attack (Log-scale).

Θ :

θ1 = timedifference (E, X)

For Y /∈ interval:

∆ :

δ1 = ¬culprit(E, Y) -≺ timedifference (E, X).

Fig. 6: Time facts and rules. Interval indicates a small portion
of the entire deceptive time (for instance < 2000 sec, > 8000
sec and so on).

fact θ1. We then divide the deceptive times from Fig. 5 into
appropriate intervals; each team is assigned to one of those
time intervals. We then check which time interval (cid:52)t belongs
to and deﬁne a defeasible rule δ1 that makes a case for all
teams not belonging to the interval to not be the culprits, as
shown in Fig. 6.

We now provide a summary of the experimental results—
the setup is similar to [13]: the dataset is sorted by time for
each target team, the ﬁrst 90% of the data is used for training
and the remaining 10% for testing. The constructs for all test
samples based on the cases discussed in the previous section
are computed, and these arguments are used as input to the
DeLP implementation. For each test sample the DeLP system
is queried to ﬁnd all possible attackers (culprits) based on
the arguments provided. If there is no way to decide between
contradicting arguments, these are blocked and thus return no
answers. Initially, the search space for each test sample is 19
teams (all except the one being attacked).

After running the queries to return the set of possible
culprits, the average search space across all target teams is
5.85 teams. This is a signiﬁcant reduction in search space
across all target teams; to gauge how much the reduced search
space can aid an analyst in predicting the actual culprit, a
metric is computed that checks if the reduced search space
contains the ground truth (actual culprit). For all the target
teams, the ground truth is present on average in almost 66%
of the samples with reduced search space. For some teams
like more smoked leet chicken (T-13) and raon ASRT (whois)
(T-17) the average reduced search space is as low as 1.82 and
2.9 teams, with high ground truth fraction of 0.69 and 0.63,
respectively.

Predictive analysis is then performed on the reduced search

110100100010000T-1T-2T-3T-4T-5T-6T-7T-8T-9T-10T-11T-12T-13T-14T-15T-16T-17T-18T-19T-20Average time (sec) -Log ScaleDeceptiveReplayspace. The experimental setup is similar to the one described
earlier; the only difference this time is instead of having a 19
team search space as in [13], the machine learning approach is
allowed to make a prediction from the reduced search space
only; a random forest is used for learning, which has been
shown to have the best performance for CTF data [13].

We report

the following average accuracies across 20
target teams; the accuracy achieved after running Random
forest without applying the argumentation-based techniques,
as reported in [13], is 0.37. This was the best performing
approach using standard machine learning techniques. The
baseline model achieves an average accuracy of 0.5, which is
signiﬁcantly better than the average accuracy of 0.37 in [13].

V. EXTENDED BASELINE MODEL I (EB1)

Previously Unseen attacks make up almost 20% of the test
samples for each target team. On analyzing the misclassiﬁca-
tion from the baseline argumentation model, we observe that
the majority of the previously unseen attacks get misclassiﬁed
(>80%). The misclassiﬁcations can be attributed to two rea-
sons: (i) the reduced search space is not able to capture the
ground truth for unseen attacks, leading the learning model to
a wrong prediction; and (ii) we represent each unseen attack
by the 3 most similar attacks in the training data; this leads
to an increase in the search space and many choices for the
learning model.

We address these issues by proposing two sets of defeasible
rules. First, for each target team we compute from the training
set the top 3 teams that come up with the most unique exploits,
as these teams are more likely to launch an unseen attack in
the test set. The intuition behind this rule is the fact that not
all teams write their own exploits, most teams just capture a
successful exploit launched by other teams and repackage it
and use it as their own (deception). The second set of rules
is proposed to avoid addition of less similar teams to the
reduced search space. In the baseline model we use 3-nearest
neighbors to represent an unseen attack. In this extended
version we consider only the nearest neighbors that are less
than a particular threshold value T , which is decided for each
target team separately. So, each attack will be represented by
k ≤ 3 teams depending upon the threshold requirement. In
addition to the baseline model rules, we propose the following
rules for deceptive attacks. Let U denote the set of teams with
the three highest numbers of unique attacks in the training
data. Also, let N denote the set of three most similar culprits
for the given unseen attack.

The extended model is shown in Fig. 7; the fact θ1 indicates
the teams present in N and whose similarity is less than a
particular threshold T , and θ2 indicates if the team ui was
one of most unique attackers from set U. For the defeasible
rules, δ1 makes use of the fact θ1 stating that the teams in N
that satisfy the threshold condition are likely to be the culprits,
and δ2 indicates that if ui is a unique attacker then it can be
the culprit unless contradictory information is available. U is
independent of the test samples and will be the same for all
unseen attacks given a target team.

For (ni ∈ N and sim < T ):

Θ :

θ1 = threshold(E, T )

For ui in U:
θ2 = unique(E, ui)

∆ :

δ1 = culprit(E, ui) -≺ threshold(E, T )

For ui ∈ U:

δ2 = culprit(E, ui) -≺ unique(E, ui)

Fig. 7: Rules for unseen attacks.

Θ :

θ1 = timedifference (E, X)

For Y ∈ interval:

∆ :

δ1 = culprit(E, Y) -≺ timedifference (E, X).

Fig. 8: Time facts and rules. Interval indicates a small portion
of the entire deceptive time (for instance < 2000 sec, > 8000
sec and so on).

On the contrary, for each of the similar payloads (three or
fewer) computed from the training data we check if the attack
is deceptive or non-deceptive. If non-deceptive, we follow the
procedure for Case 1, otherwise we follow the procedure for
Case 2 stated in the baseline argumentation model.
Experiment: We evaluate EB1 using an experimental setup
similar to the one for the baseline argumentation model.
We report the average reduced search space and prediction
to provide a
accuracy for both EB1 and baseline model
comparison. EB1 performs better than the baseline with an
average accuracy of 0.53 vs. 0.50, and signiﬁcantly better than
the machine learning model without argumentation that has an
average accuracy of 0.37. The improvement in performance
is due to the larger fraction of reduced search spaces with
ground truth present in them. Also, the search space reduced
from on average 6.07 teams to 5.025 (less teams to consider).
The results are reported in Table 3 along with a comparison
to the second extended baseline argumentation model (EB2).

VI. EXTENDED BASELINE MODEL II (EB2)

Another source of misclassiﬁcation in the baseline argu-
mentation model is the presence of unseen deceptive teams
and their duplicates. These refer to teams that did not use the
exploit in the training set but started using it in the test set. It
is difﬁcult for a machine learning approach to predict such a
team as being the culprit if it has not encountered it using the
exploit in the training set. In our dataset these attacks comprise
15% of the total, and up to 20% for some teams.

In order to address this issue we propose an extension
of EB1, where we group together teams that have similar
deceptive behavior based on the time information available to
us from the training set; for instance teams that are deceptive
within a certain interval of time (e.g., less than 2,000 secs.)
after the ﬁrst attack has been played are grouped together. For
a given test attack we compute the time difference between
the test attack and the last time the attack was used in the
training set. We then assign this time difference to a speciﬁc

group based on which interval the time difference falls in.
In order to ﬁne tune the time intervals, instead of using the
average deceptive times averaged across all target teams (as
used in the baseline model), we compute and use deceptive
times for each target team separately. We model the time rules
as stated in Fig. 8; fact θ1 states the time difference between
the test sample and the last training sample to use that attack,
defeasible rule δ1 on the other hand states that teams belonging
to that interval (in which the time difference lies) are likely to
be the culprits unless a contradiction is present. It is clear that
this rule will increase the search space for the test sample,
as additional teams are now being added as likely culprits.
We observe that for EB2 the search space is increased by an
average of almost 2.5 teams per test sample from EB1; at the
same time the presence of ground truth in the reduced search
space increased to 0.78, which is a signiﬁcant improvement
over 0.68.
Experiment: We evaluate EB2 using an experimental setup
similar to the one discussed in the baseline argumentation
model. We report the prediction accuracies for each of the
proposed baseline argumentation models for each of the target
teams and compare it with the previous accuracy reported
in [13], denoted as ML. In Table 3 the second extended
baseline model (EB2) performs the best with an average
prediction accuracy of 62% as compared to other proposed
methods. The additions of teams based on time rules not only
beneﬁts detection of unseen deceptive teams but it also helps
in predicting attackers for unseen attacks. The major reason
for the jump in performance is that for most unseen deceptive
team samples, the time rules proposed in the baseline model
block all deceptive teams from being the culprit, leading to
an empty set of culprits. The new set of rules proposed in
EB2 adds similar-behaving teams to this set based on time
information; the learning algorithm can then predict the right
one from this set.

VII. CONCLUSION

In this paper we demonstrated how an argumentation-
based framework (DeLP) can be leveraged to improve cyber-
attribution decisions by building DeLP programs based on
CTF data; this affords a reduction of the set of potential
culprits and thus greater accuracy when using a classiﬁer for
cyber attribution. We are currently looking at implementing a
probabilistic variant of DeLP [19], as well as designing our
own CTF event in order to better mimic real-world scenarios.
Our new CTF will encourage deceptive behavior among the
participants, and we are also enhancing our instrumentation of
the CTF, allowing for additional data collection (host data is
of particular interest).

VIII. ACKNOWLEDGMENTS

Authors of this work were supported by the U.S. Depart-
ment of the Navy, Ofﬁce of Naval Research, grant N00014-15-
1-2742 as well as the Arizona State University Global Security
Initiative (GSI) and by CONICET and Universidad Nacional
del Sur, Argentina.

TABLE 3: Results Summary

Team

ML [13]

T-1

T-2

T-3

T-4

T-5

T-6

T-7

T-8

T-9

T-10

T-11

T-12

T-13

T-14

T-15

T-16

T-17

T-18

T-19

T-20

0.45

0.22

0.30

0.26

0.26

0.5

0.45

0.42

0.41

0.30

0.37

0.24

0.35

0.42

0.30

0.43

0.42

0.48

0.41

0.48

BM

0.51

0.45

0.40

0.44

0.45

0.49

0.53

0.61

0.50

0.42

0.44

0.43

0.63

0.52

0.38

0.48

0.58

0.50

0.51

0.51

EB1

EB2

0.52

0.38

0.47

0.42

0.45

0.55

0.56

0.58

0.53

0.41

0.5

0.36

0.64

0.53

0.55

0.55

0.58

0.52

0.56

0.64

0.60

0.43

0.66

0.44

0.56

0.7

0.66

0.74

0.76

0.41

0.73

0.52

0.75

0.67

0.64

0.65

0.68

0.65

0.68

0.71

REFERENCES

[1] A. Applebaum, K. Levitt, Z. Li, S. Parsons, J. Rowe, and E. Sklar.
Cyber reasoning with argumentation: Abstracting from incomplete and
contradictory evidence. In Proc. of MILCOM, 2015.
[2] W. E. Boebert. A survey of challenges in attribution.

In Proc. of a

workshop on Deterring CyberAttacks, pages 41–54, 2010.

[3] M. Br¨uckner, C. Kanzow, and T. Scheffer. Static prediction games
for adversarial learning problems. The Journal of Machine Learning
Research, 13(1):2617–2654, 2012.

[4] M. Br¨uckner and T. Scheffer. Stackelberg games for adversarial predic-
tion problems. In Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 547–555.
ACM, 2011.

[5] M. Dacier, V.-H. Pham, and O. Thonnard. The wombat attack attribution
In Information Systems Security, pages 19–37.

method: some results.
Springer, 2009.

[6] DEFCON. DEFCON: Capture the ﬂag. https://media.defcon.org/, 2013.

[Online; accessed January-2015].

[7] P. M. Dung. On the acceptability of arguments and its fundamental role
in nonmonotonic reasoning, logic programming and n-person games.
Artiﬁcial intelligence, 77(2):321–357, 1995.

[8] A. J. Garc´ıa and G. R. Simari. Defeasible logic programming: An
argumentative approach. Theory and practice of logic programming,
4(1+ 2):95–138, 2004.

[9] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar.
Adversarial machine learning. In Proceedings of the 4th ACM workshop
on Security and artiﬁcial intelligence, pages 43–58. ACM, 2011.
[10] B. Li and Y. Vorobeychik. Feature cross-substitution in adversarial
classiﬁcation. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence,
and K. Q. Weinberger, editors, Advances in Neural Information Process-
ing Systems 27, pages 2087–2095. Curran Associates, Inc., 2014.
[11] J. W. Lloyd. Foundations of logic programming. Springer Science &

Business Media, 2012.

[12] D. Lowd and C. Meek. Adversarial

In Proceedings of
the eleventh ACM SIGKDD international conference on Knowledge
discovery in data mining, pages 641–647. ACM, 2005.

learning.

[13] E. Nunes, N. Kulkarni, P. Shakarian, A. Ruef, and J. Little. Cyber-
deception and attribution in capture-the-ﬂag exercises. In Proceedings
of the 2015 IEEE/ACM International Conference on Advances in Social
Networks Analysis and Mining, ASONAM 2015, Paris, France, August
25 - 28, 2015, pages 962–965, 2015.

[14] I. Rahwan, G. R. Simari, and J. van Benthem. Argumentation in artiﬁcial

intelligence, volume 47. Springer, 2009.

[15] T. Rid and B. Buchanan. Attributing cyber attacks. Journal of Strategic

Studies, 38(1-2):4–37, 2015.

[16] J. Salvit, Z. Li, S. Perumal, H. Wall, J. Mangels, S. Parsons, and E. I.
Sklar. Employing argumentation to support human decision making:
In AAMAS Workshop on Argumentation in Multiagent
A user study.
Systems, 2014.

[17] P. Shakarian, J. Shakarian, and A. Ruef. Introduction to cyber-warfare:

A multidisciplinary approach. Elsevier, 2013.

[18] P. Shakarian, G. I. Simari, G. Moores, and S. Parsons. Cyber attribution:
In Cyber Warfare: Building the

An argumentation-based approach.
Scientiﬁc Foundation, pages 151–171. Springer, 2015.

[19] P. Shakarian, G. I. Simari, G. Moores, D. Paulo, S. Parsons, M. Falappa,
and A. Aleali. Belief revision in structured probabilistic argumentation.
Annals of Mathematics and Artiﬁcial Intelligence, pages 1–43, 2015.

[20] E. I. Sklar, S. Parsons, Z. Li, J. Salvit, S. Perumal, H. Wall, and J. Man-
gels. Evaluation of a trust-modulated argumentation-based interactive
decision-making tool. Autonomous Agents and Multi-Agent Systems,
pages 1–38, 2015.

[21] F. Stolzenburg, A. J. Garc´ıa, C. I. Chesnevar, and G. R. Simari.
Computing generalized speciﬁcity. Journal of Applied Non-Classical
Logics, 13(1):87–113, 2003.

[22] R. J. Walls. Inference-based Forensics for Extracting Information from
Diverse Sources. PhD thesis, University of Massachusetts Amherst,
2014.

