This work was accepted at the 1st International Workshop on Adaptive Cyber Defense (ACD) at the 30th International Joint Conference on
Artiﬁcial Intelligence (IJCAI-21) August 19-20, 2021

A Smart and Defensive Human-Machine Approach to Code Analysis

Fitzroy D. Nembhard∗ , Marco M. Carvalho
L3Harris Institute for Assured Information,
Florida Institute of Technology, Melbourne, Florida, USA
{fnembhard, mcarvalho}@ﬁt.edu

1
2
0
2

g
u
A
6
2

]
I

A
.
s
c
[

3
v
4
9
2
3
0
.
8
0
1
2
:
v
i
X
r
a

Abstract

Static analysis remains one of the most popular ap-
proaches for detecting and correcting poor or vul-
nerable program code. It involves the examination
of code listings, test results, or other documenta-
tion to identify errors, violations of development
standards, or other problems, with the ultimate goal
of ﬁxing these errors so that systems and software
are as secure as possible. There exists a plethora
of static analysis tools, which makes it challeng-
ing for businesses and programmers to select a tool
to analyze their program code. It is imperative to
ﬁnd ways to improve code analysis so that it can be
employed by cyber defenders to mitigate security
risks. In this research, we propose a method that
employs the use of virtual assistants to work with
programmers to ensure that software are as safe as
possible in order to protect safety-critical systems
from data breaches and other attacks. The pro-
posed method employs a recommender system that
uses various metrics to help programmers select the
most appropriate code analysis tool for their project
and guides them through the analysis process. The
system further tracks the user’s behavior regarding
the adoption of the recommended practices.

1 Introduction
The human element is a signiﬁcant component of cybersecu-
rity. Throughout the software development life cycle (SDLC)
to the proper use of systems, humans are intrinsically in-
volved in ensuring that systems are designed and used with
security in mind. At the root of every network and computing
system is the software that facilitates their operation. Conse-
quently, to achieve certain guarantees of security, the under-
lying program code must be free of vulnerabilities. One way
to eliminate vulnerabilities is through constant static analy-
sis and bug ﬁxing. As researchers and engineers continue
to create and improve code analysis tools, it is also essen-
tial to develop intelligent models that work with humans to
inculcate behaviors that adaptively defend against attackers
who may possess knowledge of system properties or security

∗Contact Author

practices. Studies show that code-level vulnerabilities remain
a top cause of data breaches across the globe [OSSRA, 2021;
CISQ, 2021; Tricentis, 2018]. In the 2021 Open Source Secu-
rity Risk Analysis (OSSRA) report published by Synopsys, it
was reported that 84% of 1,546 codebases that were scanned
contained at least 1 vulnerability [OSSRA, 2021]. Nearly half
of the open source vulnerabilities found in that report were
identiﬁed as high risk [OSSRA, 2021]. Given the fact that an
attacker only knows what is perceived through observation
of the target network, it is imperative to engineer methods to
harden networks and the software that drive them to make it
harder for unauthorized users to gain access. Consequently,
defending a network from authorized access should start at
the ground level—the program code.

Static analysis is a widely recommended approach to scan
program code for unsafe practices. However, many program-
mers are skeptical of using static analysis tools due to the
sheer number of tools available as well as usability challenges
and poor presentation of vulnerability reports [Nembhard and
Carvalho, 2019; Nembhard et al., 2019]. To address some
of these limitations, researchers have proposed hybrid tools
that combine static and dynamic approaches [Aggarwal and
Jalote, 2006; Nembhard, 2018] or use data fusion to aggre-
gate and reﬁne reports from a number of static analysis tools
to provide the user with a more reﬁned and comprehensive set
of action items [Nembhard, 2018; Alenezi and Javed, 2016;
Kong et al., 2007]. However, these methods have their own
limitations in that they do not address the performance of the
tools they employ.

While scanning code for vulnerabilities is an important
step towards creating more secure software and networks,
ﬁxing the vulnerabilities found is a more critical activity.
To that end, several researchers have proposed models that
attempt to auto-ﬁx or repair code [Marginean et al., 2019;
Gupta et al., 2017; Weimer et al., 2013]. These auto-repair
systems have not yet become widespread due to their abil-
ity to ﬁx only a small subset of errors. There is also a need
for systems that work with programmers to harden their code
instead of simply scanning and displaying bug reports. We
were the ﬁrst to apply recommender systems to the secure
coding problem by providing the user with a set of candidates
from which they can choose to ﬁx vulnerabilities in their code
[Nembhard et al., 2019]. We used hand-selected features to
train a model to detect taint-style (data dependence) vulner-

 
 
 
 
 
 
This work was accepted at the 1st International Workshop on Adaptive Cyber Defense (ACD) at the
30th International Joint Conference on Artiﬁcial Intelligence (IJCAI-21) August 19-20, 2021

abilities in program code. Our system, named VulIntel (Vul-
nerability Intellisensor), monitors program code as the pro-
grammer types, checks for unsafe practices, and then makes
recommendations accordingly. We ranked the ﬁxes using
similarity metrics in order to provide the user with code that
is most similar to the one being developed. In addition, we
proposed a system that employs the use of virtual assistants
to guide programmers as they scan their code for vulnerabil-
ities [Nembhard and Carvalho, 2021]. The system features
a plug-and-play (PnP) model that can use any code analyzer
to scan a given project. The system was tested with Google
Assistant and PMD code analyzer. The results demonstrate
a seamless coordination of man and machine in an effort to
encourage programmers to produce secure code.

In this position paper, we propose further enhancements to
the model discussed in [Nembhard and Carvalho, 2021]. The
proposed improvements include adding a recommender sys-
tem that is enriched with knowledge regarding existing static
analysis tools, building behavioral models that provide in-
sights into the kinds of ﬁxes that programmers choose, and
tracking how users respond to a virtual agent that guides them
in making defensive decisions regarding the security of their
code and networks. The knowledge base will include infor-
mation regarding static analysis tools such as trends and re-
views, tool descriptions, types of projects targeted, etc, in or-
der to help the programmer select the best tool to scan their
project. The ultimate goals are: 1) to increase the adoption of
static analysis tools in the SDLC by teaming up users with
a virtual assistant and recommender system to make code
analysis more intelligent and human-centered, 2) to capture
data that will help us understand how humans respond to se-
cure recommendations, and 3) to foil insider threats that may
be introduced during coding. The rest of the paper is orga-
nized as follows: Section 2 presents background information
on techniques that will be employed in the proposed model.
We discuss our proposed approach in Section 3 and present
our conclusion in Section 4.

2 Background

In this section, we discuss brieﬂy some existing approaches
that can be used to enrich a recommender system with knowl-
edge about static analyzers as well as making the system more
adaptive in its selection of a tool for a given project.

2.1 Topic Models
A topic model is a generative model for documents, which
speciﬁes a probabilistic procedure by which documents can
be generated [Steyvers and Grifﬁths, 2007]. Inversely, topic
models can be used as an unsupervised machine learning
technique to automatically cluster word groups and similar
expressions that best characterize a set of documents. Ex-
isting research has shown that topic models can be mined
from source code [Nembhard et al., 2017; Linstead et al.,
2007]. Latent Dirichlet Allocation (LDA) is one example
of a topic model that has been known to perform well in
this domain [Linstead et al., 2007]. This is due to the fact
that programmers tend to use informative keywords to name
variables, methods, and classes, in addition to comments that

describe the purpose of their code [Nembhard et al., 2017;
Mirakhorli and Cleland-Huang, 2016]. These latent topics
can be used to provide information on the overall focus or
goal of a project.

2.2 Topic Memory Networks for Short Text

Classiﬁcation

[Zeng et al., 2018] proposed Topic Memory Networks (TMN)
for short text classiﬁcation (for example, tool descriptions,
reviews, etc). The proposed network consists of three major
components: 1) a neural topic model (NTM), which is based
on a variational auto-encoder [Kingma and Welling, 2013], to
induce latent topics, 2) a topic memory mechanism that maps
the inferred latent topics to classiﬁcation features, and 3) a
text classiﬁer, which produces the ﬁnal classiﬁcation labels
for instances [Zeng et al., 2018].

Given X = x1, x2, . . . , xM as the input with M short text
instances, each instance x is processed into two representa-
tions: 1 ) a bag-of-words (BoW) term vector xBoW ∈ RV
and 2) a word index sequence vector xSeq ∈ RL , where V is
the vocabulary size and L is the sequence length. xBoW is fed
into the neural topic model to induce latent topics. The NTM
is described in a manner similar to LDA, where each instance
x has a topic mixture θ that is represented as a K-dimensional
distribution. Each topic k is represented by a word distribu-
tion φk over the given vocabulary. However, unlike LDA, in
TMN, an encoder estimates the prior parameters and a de-
coder describes the generation story while the additional dis-
tributional vectors θ and φ yield latent topic representations
[Zeng et al., 2018]. Thus, topic memory networks are useful
for understanding the purpose of a tool given a short review
or description.

3 Proposed Human-Machine Approach to

Code Analysis

In this section, we describe the proposed approach for team-
ing humans with a virtual assistant to interactively scan and
ﬁx vulnerabilities in program code. Figure 1 shows the archi-
tecture of the proposed framework. The system has 5 main

Figure 1: Proposed human-machine approach to code analysis with
enhancements highlighted using a dashed border.

components: a virtual assistant, a system integration frame-
work, a code analyzer, a recommender system, and an ana-
lytics and insights component. The existing framework, as

Trends & ReviewsTool descriptionsPopularity ScoresTargeted Projects...Behavioral modelsVirtual AssistantSystem IntegrationAnalytics andCode AnalyzerProjectCodingFrameworkInsightsRecommenderSystemSpeechThis work was accepted at the 1st International Workshop on Adaptive Cyber Defense (ACD) at the
30th International Joint Conference on Artiﬁcial Intelligence (IJCAI-21) August 19-20, 2021

described in [Nembhard and Carvalho, 2021], involves creat-
ing and incorporating a Google Assistant app with a system
integration framework that allows a user to conversationally
scan a coding project for vulnerabilities. The user commu-
nicates with the virtual assistant using their voice, and web
services backed by Google conversation APIs and local IDE
plugins are used to contextualize the user’s request to deter-
mine which project the user would like to scan. The system
integration framework then uses a code analyzer to scan the
system under test for unsafe code. Currently, the user may
work with the virtual assistant to summarize vulnerabilities
found and even email the user a vulnerability scan report.

The additions proposed in this work are highlighted with a
dashed border in Figure 1. We now describe key components
in the framework and the proposed improvements.

3.1 The Machine: A Virtual Assistant
A virtual assistant, or voice assistant, is a software agent that
uses voice recognition, language processing algorithms, and
voice synthesis to listen to speciﬁc voice commands and re-
turn relevant information or perform speciﬁc tasks on behalf
of the user [Alan, 2021]. Voice commands are usually con-
verted to text and processed by cloud-based services to de-
termine the user’s intent. Once intent has been determined,
natural language processing (NLP) algorithms backed by ma-
chine learning and AI are used to map the user’s request to the
most applicable function. Google Assistant was chosen as
the virtual assistant for this research due to its popularity and
easy-to-use App Engine [App Engine, 2021] and Dialogﬂow
[Dialogﬂow, 2021] platforms. Google App Engine is a cloud
computing platform as a service (PaaS) for developing and
hosting web applications. Dialogﬂow is a natural language
understanding platform that allows users to design and inte-
grate a conversational user interface into a mobile app, web
application, device, bot, interactive voice response system,
etc. [Dialogﬂow, 2021].

The purpose of the virtual assistant

is to work syn-
chronously with the user to perform code analysis in a user-
friendly environment. As a cloud-based agent, the virtual as-
sistant can carry out tasks in a speedy and efﬁcient manner
as the user multitasks. Traditional code analysis frameworks
may require users to manually conﬁgure system preferences
or specify certain parameters in order for the analysis tool to
work effectively. This may reduce the programmer’s produc-
tivity and discourage use of static analysis tools altogether.
Our goal is to automate many of these manual tasks by allow-
ing the agent to perform tasks on behalf of the user. Figure
2 captures a subset of the intents already incorporated into
the system [Nembhard and Carvalho, 2021]. As shown in the
diagram, the agent can work with the user to select a repos-
itory or coding project to scan, trigger the code analyzer to
perform the scan, and summarize the results found. More in-
tents will be added to make the system more robust, active
and responsive in the way it works with humans.

3.2 Achieving Adaptability using a Recommender

System

Our existing framework [Nembhard and Carvalho, 2021] can
only use one code analyzer. We intend to integrate a recom-

Figure 2: Subset of Dialogﬂow intents already incorporated into the
framework

mender system with the model so that the system can adap-
tively work with humans to select the most appropriate code
analyzer.
Information retrieval and text mining techniques
will be used to collect data to enrich the knowledge base of
the recommender system by including performance metrics
and descriptions of existing tools so that it can recommend
a code analyzer for a given project based on factors such
as security, project size, project type, etc. Recommending a
tool for a given project will make the system more adaptable
to the unique security challenges facing a given institution,
thus enabling experts to better ward off threats to their cyber-
infrastructure.

Adaptability can be achieved by fortifying the recom-
mender system with both hand-selected and automatically se-
lected data. Most static analysis tools have websites that in-
clude short descriptions of the types of projects/errors that
the tool targets. The Gartner Magic Quadrant [Gartner Magic
Quadrant, 2021] and Gartner Peer Insights [Gartner Peer In-
sights, 2021] also include customer reviews and ratings for
several static analysis tools that can provide a wealth of in-
formation for the recommender system. Topic models and
topic networks, which were discussed in Section 2, will be
used to map tools to topics and source code to topics in order
to recommend the most appropriate tool for a given project.
In other words, topic networks will be used to augment topic
models so the recommender system can suggest the most ap-
propriate tool for a project even when very little information
about a tool is provided.

3.3 Behavioral Models
Humans play an important role in the overall security and
wellbeing of any computing system. The National Security
Agency (NSA) lists “understanding and accounting for hu-
man behavior” as one of the ﬁve hard problems of cyberse-
curity [Scala et al., 2019]. However, many designers of code
analysis systems do not take into account the behaviors of
humans regarding proper use and adoption of code analyz-
ers. As users interact with the virtual assistant and their cod-

●vulnerability-scanning●↳git-account-confirmed●↳repo-selected●↳email-scan-report●↳device-account-selected●↳other-account-selected●↳email-address-typed●↳email-address-spoken●↳email-address-spoken-no●↳summary-requested●↳vulnerability-context-provided●↳suggested-git-account-yes●↳other-git-account-providedIDE or GitBrowser tab (GitHub, etc)Other Git account●↳ide-code-scan-confirmedUser IDE (Eclipse, IntelliJ, etc)●↳auto-fix-errors-requested●↳email-address-spoken-yesThis work was accepted at the 1st International Workshop on Adaptive Cyber Defense (ACD) at the
30th International Joint Conference on Artiﬁcial Intelligence (IJCAI-21) August 19-20, 2021

ing environment, we intend to collect data that can be used
to model and understand human behavior in their response
to code analysis. This kind of data will help us identify ac-
tions taken by users that lead to weaknesses in their coding
projects. We also plan to use severity scores of vulnerabilities
to determine potential impact on software security especially
when recommendations are ignored by users. These models
will also help to identify insider threats where programmers
may intentionally reject secure practices and leave systems
open to attacks.

Modeling context and diverse speech with Dialogﬂow is
currently limited as the designer has to manually enumer-
ate all user intents using a tree-like structure. This limi-
tation can be observed in Figure 2. Further, once a user
breaks the ﬂow of a conversation, Dialogﬂow does not mem-
orize the states or trail taken by the user. Research shows
that attention and memory can be modeled using Long-short-
term memory (LSTM) networks, which are a speciﬁc recur-
rent neural network (RNN) architecture that was designed to
model temporal sequences and their long-range dependencies
more accurately than conventional RNNs [Chen et al., 2019;
Sak et al., 2014; Hochreiter and Schmidhuber, 1997].
In
our research, we will explore the feasibility of integrating
LSTMs with Dialogﬂow intents to handle more complex
state-switching to make the conversation more realistic. Fig-
ure 3 shows an example state machine that models several
paths that could be taken as users interact with the system.
For example, a user may not like the results provided by a cer-
tain code analyzer and give an excuse or ask that a different
tool be used. LSTMs will facilitate such types of deviations
in the conversation.

Figure 3: An example state machine showing the dynamic nature of
conversations between a user and a virtual assistant

The path followed by the user as they communicate with
the assistant will be used to create a behavioral model that
will provide insights into how users use the code analysis
framework. Figure 4 shows preliminary features, including
data-types, that can be used to describe human behavior as it
relates to the adoption of safe practices during coding. Some
example features include a time series of the times that a par-
ticular recommendation was given for a project, a Boolean
value to indicate whether the recommendation was adopted

into the codebase, and the transition model that represents the
steps followed by the user in the analysis process, to name a
few. Behavioral models will be created using machine learn-
ing techniques such a ensemble methods and neural networks.
We also intend to use feature selection to determine the most
appropriate features based on the collected data.

Figure 4: Subset of proposed behavioral features to be captured by
the recommender system

4 Conclusion and Future Work
In this position paper, we proposed enhancements to an exist-
ing human-centered code scanning framework that will make
it more resilient in mitigating vulnerabilities and adaptive
against insider threats. The existing system employs a vir-
tual assistant that works with programmers to scan their code
for vulnerabilities. The current system, while novel, is lim-
ited in that it uses a ﬁxed static analysis tool and it does not
adaptively defend against insider threats.

Our subsequent plans are to implement the enhancements
proposed in this paper. These enhancements include the ad-
dition of a recommender system that recommends and uses
a code analyzer based on the user’s preferences and the sys-
tem under test. The model will be enriched with information
such as tool descriptions, trends and reviews, target projects,
etc, which will allow us to use advances in topic modeling
to make appropriate recommendations. Further, as program-
mers use the proposed system, it will build behavioral mod-
els that will help analysts spot insider threats and take nec-
essary actions to eliminate or discourage practices that may
expose the network and its underlying software to unautho-
rized and malicious users. The system will be evaluated us-
ing user studies in the form of A/B testing where participants
will use various tools to scan their code and report on their
experiences.

References
[Aggarwal and Jalote, 2006] A. Aggarwal and P. Jalote. In-
tegrating static and dynamic analysis for detecting vulner-
abilities. In 30th Annual International Computer Software
and Applications Conference (COMPSAC’06), volume 1,
pages 343–350, Sept 2006.

[Alan, 2021] What is a voice assistant?

https://alan.app/
blog/voiceassistant-2/, April 2021. Accessed: 2021-04-
23.

DTN..................DT2DT1T1ST1T3T2Unrelated RequestsClarificationCode ScanningRequestExcusesUser QueryTimes Security Practice Recommended (Time series)Fix Adopted into Codebase (Boolean)Vulnerability Score/Severity (Float)NVD/CVE Reference (URL)Files/Classes/Lines Affected (Blob)Potential Side-effects of Recommendation (Blob)Time Taken to adopt a fix (Duration)Project Metrics: e.g. project type, LOC (HashTable)Transition Model (Dynamic State Diagram)...Behavioral ModelThis work was accepted at the 1st International Workshop on Adaptive Cyber Defense (ACD) at the
30th International Joint Conference on Artiﬁcial Intelligence (IJCAI-21) August 19-20, 2021

[Alenezi and Javed, 2016] Mamdouh Alenezi

and Yasir
Javed. Developer companion: A framework to produce se-
cure web applications. International Journal of Computer
Science and Information Security, 14(7):12, 2016.

[Mirakhorli and Cleland-Huang, 2016] Mehdi Mirakhorli
and Jane Cleland-Huang. Detecting, tracing, and moni-
toring architectural tactics in code. IEEE Trans. Software
Eng., 42:205–220, 2016.

[App Engine, 2021] App engine. https://cloud.google.com/

appengine, 2021. Accessed: 2021-04-22.

[Chen et al., 2019] Yu Chen, Lingfei Wu, and Mohammed J
Zaki. Graphﬂow: Exploiting conversation ﬂow with graph
neural networks for conversational machine comprehen-
sion. arXiv preprint arXiv:1908.00059, 2019.

[CISQ, 2021] The Cost of Poor Software Quality in
https://www.it-cisq.org/

the US: A 2020 Report.
the-cost-of-poor-software-quality-in-the-us-a-2020-report.
htm, January 2021. Accessed: 2021-04-22.

[Dialogﬂow, 2021] Dialogﬂow.

https://cloud.google.com/

dialogﬂow/docs, 2021. Accessed: 2021-04-22.

quadrant

[Gartner Magic Quadrant, 2021] Gartner
application

magic
https://www.microfocus.com/en-us/assets/security/
magic-quadrant-for-application-security-testing,
Accessed: 2021-04-27.

security

for

2020
testing.

2021.

[Gartner Peer Insights, 2021] Application security testing
(ast)
https://www.gartner.com/
reviews/market/application-security-testing, 2021. Ac-
cessed: 2021-04-26.

reviews and ratings.

[Gupta et al., 2017] Rahul Gupta, Soham Pal, Aditya
Kanade, and Shirish Shevade. Deepﬁx: Fixing common
c language errors by deep learning. In Thirty-First AAAI
Conference on Artiﬁcial Intelligence, 2017.

[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and
J¨urgen Schmidhuber. Long short-term memory. Neural
computation, 9(8):1735–1780, 1997.

[Kingma and Welling, 2013] Diederik P Kingma and Max
Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.

[Kong et al., 2007] Deguang Kong, Quan Zheng, Chao
Chen, Jianmei Shuai, and Ming Zhu.
Isa: A source
code static vulnerability detection system based on data fu-
sion. In Proceedings of the 2Nd International Conference
on Scalable Information Systems, InfoScale ’07, pages
55:1–55:7. ICST (Institute for Computer Sciences, Social-
Informatics and Telecommunications Engineering), 2007.
[Linstead et al., 2007] Erik Linstead, Paul Rigor, Sushil Ba-
jracharya, Cristina Lopes, and Pierre Baldi. Mining con-
cepts from code with probabilistic topic models. In Pro-
ceedings of the twenty-second IEEE/ACM international
conference on Automated software engineering, pages
461–464, 2007.

[Marginean et al., 2019] A. Marginean, J. Bader, S. Chan-
dra, M. Harman, Y. Jia, K. Mao, A. Mols, and A. Scott.
In 2019
Sapﬁx: Automated end-to-end repair at scale.
IEEE/ACM 41st International Conference on Software
Engineering: Software Engineering in Practice (ICSE-
SEIP), pages 269–278, 2019.

[Nembhard and Carvalho, 2019] Fitzroy Nembhard

and
Marco Carvalho. The impact of interface design on the
usability of code analyzers. In 2019 SoutheastCon, pages
1–6, 2019.

[Nembhard and Carvalho, 2021] Fitzroy D. Nembhard and
Marco M. Carvalho. Conversational code analysis: The
future of secure coding. arXiv preprint arXiv:2105.03502,
2021.

[Nembhard et al., 2017] Fitzroy Nembhard, Marco Car-
valho, and Thomas Eskridge. A hybrid approach to im-
proving program security. In 2017 IEEE Symposium Series
on Computational Intelligence (SSCI), pages 1–8, 2017.
[Nembhard et al., 2019] Fitzroy D. Nembhard, Marco M.
Carvalho, and Thomas C. Eskridge. Towards the applica-
tion of recommender systems to secure coding. EURASIP
Journal on Information Security, 2019(1):9, 2019.

[Nembhard, 2018] Fitzroy Nembhard. A Recommender Sys-
tem for Improving Program Security Through Source Code
Mining and Knowledge Extraction. PhD thesis, Florida In-
stitute of Technology, 2018.

[OSSRA, 2021] 2021

Open

Source

Security
and
https://www.synopsys.

Risk Analysis Report.
com/software-integrity/resources/analyst-reports/
open-source-security-risk-analysis.html, 2021.
cessed: 2021-04-22.

Ac-

[Sak et al., 2014] Hasim Sak, Andrew W Senior,

and
Franc¸oise Beaufays. Long short-term memory recurrent
neural network architectures for large scale acoustic mod-
eling. 2014.

[Scala et al., 2019] Natalie M. Scala, Allison C. Reilly,
Paul L. Goethals, and Michel Cukier. Risk and the
Risk Analysis,
ﬁve hard problems of cybersecurity.
39(10):2119–2126, 2019.

[Steyvers and Grifﬁths, 2007] M. Steyvers and T. Grifﬁths.
Probabilistic topic models. In T. Landauer, S. Dennis Mc-
Namara, and W. Kintsch, editors, Handbook of Latent Se-
mantic Analysis. Laurence Erlbaum, 2007.

[Tricentis, 2018] Software Fail Watch: The Politics of Soft-
ware Defects, Q2, 2018.
https://www.tricentis.com/
blog/software-fail-watch-q2-2018/, July 2018. Accessed:
2021-04-22.

[Weimer et al., 2013] Westley Weimer, Zachary P. Fry, and
Stephanie Forrest. Leveraging program equivalence for
adaptive program repair: Models and ﬁrst results. In 2013
28th IEEE/ACM International Conference on Automated
Software Engineering (ASE), pages 356–366, 2013.

[Zeng et al., 2018] Jichuan Zeng, Jing Li, Yan Song, Cuiyun
Gao, Michael R. Lyu, and Irwin King. Topic memory net-
works for short text classiﬁcation, 2018.

