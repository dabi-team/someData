Getting Passive Aggressive About False Positives:
Patching Deployed Malware Detectors

Edward Raff
Laboratory for Physical Science
edraff@lps.umd.edu
Booz Allen Hamilton
Raff_Edward@bah.com

Bobby Filar
Elastic
ﬁlar@elastic.co

James Holt
Laboratory for Physical Sciences
holt@lps.umd.edu

0
2
0
2

t
c
O
2
2

]

G
L
.
s
c
[

1
v
0
8
0
2
1
.
0
1
0
2
:
v
i
X
r
a

Abstract—False positives (FPs) have been an issue of extreme
importance for anti-virus (AV) systems for decades. As more
security vendors turn to machine learning, alert deluge has
hit critical mass with over 20% of all alerts resulting in FPs
and,
in some organizations, the number reaches half of all
alerts [1]. This increase has resulted in fatigue, frustration, and,
worst of all, neglect from security workers on SOC teams. A
foundational cause for FPs is that vendors must build one global
system to try and satisfy all customers, but have no method to
adjust to individual local environments. This leads to outrageous,
albeit technically correct, characterization of their platforms
being 99.9% effective. Once these systems are deployed the
idiosyncrasies of individual, local environments expose blind spots
that lead to FPs and uncertainty.

We propose a strategy for ﬁxing false positives in production af-
ter a model has already been deployed. For too long the industry
has tried to combat these problems with inefﬁcient, and at times,
dangerous allowlist techniques and excessive model retraining
which is no longer enough. We propose using a technique called
passive-aggressive learning to alter a malware detection model to
an individual’s environment, eliminating false positives without
sharing any customer sensitive information. We will show how to
use passive-aggressive learning to solve a collection of notoriously
difﬁcult false positives from a production environment without
compromising the malware model’s accuracy, reducing the total
number of FP alerts by an average of 23x.

I. INTRODUCTION

Employing machine learning to detect good/bad activity in
unknown environments will yield false positives (FPs). Security
vendors are quick to point out the latest ML-backed feature is
99.9% effective, and they are not wrong. But, these are models
trained on a global representation of data. Once operational in
a customer’s local environment, it is not uncommon for FPs
to pile up. This stems from the fact that while the model may
be 99.9% accurate globally, the customers local environment
may have very different characteristics or peculiarities. The
global vs. local dichotomy is not a new observation. Previous
studies have shown that other AV products can have different
FP and false negative (FN) rates for regional malware (e.g.,
United States vs. Brazil) and malware types (e.g., Trojan vs.
ransomware)
[2]. This can cause excessive false alerts, which
users ﬁnd unacceptable and may lead them to abandon the
malware detectors.

While Windows PE (Portable Executable) and Android APK
(Android Package) malware detection are more common in

the literature, the same is true for all domains. Our domain of
interest, detecting malicious Microsoft Ofﬁce Macros, suffers
these same issues with additional difﬁculties.

The standard approach to false positive triage, allowlisting
on ﬁle hash, works well for executable applications that update
infrequently but is mostly a futile approach for macro detection.
This is due to the hash of the Ofﬁce document changing with
each modiﬁcation to the ﬁle, a common occurrence. Security
workers are essentially left to play “whack-a-mole,” hoping that
the one-to-one hash-to-ﬁle allowlisting effectively suppresses
future FPs and lacks the tools/support needed to perform
anything more effective
[3]. Macro malware is also less
prevalent, resulting in less training data, making frequent global
retraining ineffective in ﬁxing local bursts of false positives.
The other commonly proposed option is to have users report
all false positives back to the company and wait for a new
model update to get the previous false positives correct after
training. This can takes weeks to months and does not alleviate
immediate concerns. This may also be untenable for users with
privacy concerns who are unwilling to share the ﬁles causing
false positives.

Despite the large focus placed on having low false positives,
we are aware of no prior work that has considered ﬁxing or
correcting a model’s false positives once already in production.
Our contributions are: 1) framing the need to study correcting
a deployed model in a safe (does not diverge) and effective
(still obtains meaningful true-positive rates after alteration) way
that is more general than allowlists. 2) An initial solution to
this problem. We propose using a particular type of machine
learning, Passive Aggressive (PA) learning, as an approach to
correct false positives made by a model. Following current
production deployment, analysts/users mark the false positives’
ﬁles, and we perform a local adaption of their model to
correct these errors. This ensures the false positives are ﬁxed
immediately. To prevent users from erroneously “correcting” on
mislabeled data, we introduce a method to estimate the impact
to a model’s Area Under the Curve (AUC) to detect such
situations, inform the user or perform some other intervention.
Combined, we show that we can perform these updates while
maintaining acceptable performance, ﬁxing the false positives
and reducing alerts, and detect potentially erroneous updates.
We will review the related work to our own in section II, and

 
 
 
 
 
 
the production dataset used for our experiments in section III.
A review of the Passive Aggressive approach, and how we use
it counter to the typical practice, is given in section IV. This
includes our approach to estimate the impact of a change to the
model in subsection IV-A. To show the importance of using
the PA approach speciﬁcally, we will detail several baseline
models we compare against in section V, followed by our
results showing the utility of our PA based local reﬁnement
in section VI. Given our results, some practical considerations
will be reviewed in section VII followed by our conclusions
in section VIII.

II. RELATED WORK

The focus on false positives within malware detection is
not new. Many prior works of varying different approaches,
from static features, to dynamic analysis and even contextual
information, have been built with an emphasis on achieving low
false positives in the design phase (e.g., [4]–[8]). These works
acknowledge and design around unique costs in keeping false
positives low, but we know no prior work that focuses on how to
deal with false positives once a model is already in production.
The current solution for most corporate AV providers is to
receive the false positive reports from customers, encourage
allowlist use, and provide a model update once a quarter that
hopefully resolves any troublesome false positives. This focus
on false positive resolution only during the design phase of a
model remains true for related areas like spam detection [9],
clustering malware families[10], and code duplication detection
within malware[11]. For non-machine learning-based solutions,
one would increase the size/speciﬁcity of signatures to reduce
FP [12] or in ML-based approaches, remove processing
steps/features that lead to FPs [13]. Once our model is deployed,
these are no longer options. It is also necessary that our model
adapt quickly with the minimum possible number of false
positives, as customers will not wait or tolerate hundreds or
thousands of FPs to alter a model. Our approach to alter
the model using the Passive-Aggressive algorithm gives us the
ability to tackle this issue in a manner that recognizes customer
concerns and privacy, without having to wait weeks for an
updated model.

The general theme of our approach is that we need to learn
using a minimal number of samples. Typically this would fall
into the domain of “one-shot” or “few-shot” learning, where a
model must be adapted to a new concept with a limited number
of examples. Classically few-shot learning is meant for multi-
class problems where new classes may be introduced at test
time, and limited prior work has investigated few-shot learning
for malware family detection [14], [15]. In our case, we have
only two classes, and our goal is to reﬁne the decision surface
in a small manner to ﬁx a problematic set of instances. We
compare against few-shot learning as an alternative approach
to our own but ﬁnd that the differences in scenarios lead to
poor model performance.

would prefer to test modifying the GBDT to ﬁx the false
positives found in production. While prior work like Hodefﬁng
Trees [17], [18], VFDT trees [19], and Mondrian Forests [20]
allow online learning of decision trees, none of them support
the kind of “correction” using just one or two samples that
we need. Instead, these approaches require multiple samples
before altering the tree structure or adjusting the prediction in
a leaf node. In practice, we have too few examples of the hard
false positives to alter the tree’s predictions. As such, we do
not consider these approaches in any detail.

It is important to note that adversarial attacks [21] are an
important problem, of particular relevance to malware detection.
Malware involves a real live adversary who wishes to avoid
detection [22], which makes the motivation for attacking and
defending malware detection models salient and well-motivated.
Indeed the MalConv based model we use as our foundation
has had numerous examples of adversarial attacks performed
against it [23]–[26]. At the same time, there have also been
approaches that counter these attacks at an impact to accuracy,
by exploiting the one-way nature of attacks in this space (that
only malware authors only wish to fool the detector) [27]–[29]
Indeed, there are various pros and cons to a machine learning-
based approach to detect like our own, and more classical
methods, in terms of vulnerability to attack [30]. For this work,
we consider the adversarial problem important but out-of-scope.
This is motivated in part by the fact that in real-world use,
more straightforward methods than adversarial attacks currently
satisfy the malicious actor’s desires and are most prevalent
[31], [32].

III. DATA USED AND MOTIVATION

We crafted a problem that was motivated by real issues in
deploying information security models in production environ-
ments. We base our testing and results on a representative
sample of industry data. Our corpus consisted of 1,101,407
Microsoft Ofﬁce documents that contained macros. Similar
to [33] data was collected from Common Crawl (CC) [34]
and VirusTotal [35]. All CC samples were checked against
VirusTotal to ensure identical label logic was applied across
all samples. If 5+ Vendors viewed the sample as malicious, we
assigned a malicious label. If no vendors labeled the sample
as malicious, and the sample had been submitted at least one
month before our collection date, the sample was labeled benign.
This approach is supported by a recent year-long study of AV
labeling that recommends using a threshold ≥ 2, and that when
using all available engines, the majority of ﬁle labels are stable
as AV products update their databases [36]. This label logic
yielded 651,872 benign and 449,535 malicious samples. This
means the random guessing performance would be 59.2%, the
percentage of benign ﬁles in the corpus. Will use a stratiﬁed
sample of 80% for the training set, and 20% for the test set.
We will refer to this test set as the “large” test set.

The original domain knowledge-based model used XGBoost
[16] as the learning algorithm, a popular decision tree boosting
library. Since the GBDT had the best global performance, we

A second test set will be composed of 58 hard false positives.
These ﬁles come predominantly from two larger commercial
ﬁnancial tools, widely used by many businesses, and considered

critical to business operations1. These 58 ﬁles were discovered
during the initial deployment of a production macro malware
detector and were regularly miss-classiﬁed as malicious during
multiple model iterations. These ﬁles were objectively difﬁcult
due to their large and complex macro codebases, exhibiting
many of the same behaviors as other malicious macros, but were
entirely benign. Because these ﬁles were considered mission-
critical by customers, their failure to be classiﬁed correctly
was a signiﬁcant issue, and motivate this work. We will refer
to these 58 ﬁles as the “hard FPs” or the hard benign set.

From a machine learning context, most models are built
with a target false positive rate (FPR) (e.g., 0.1%), and the
threshold on a model’s output is adjusted to meet this desired
FPR. Normally one would continue to adjust this threshold if
too many FPs were occurring. We make explicit that this is not
a possible solution for these 58 difﬁcult samples. The output
probability for all samples ranges from near 0% to near 100%.
As such, adjusting the threshold would require calling almost
every ﬁle benign to avoid these FPs, which is unacceptable.
This situation occurred with all machine learning approaches
attempted in this work.

IV. PASSIVE AGGRESSIVE REFINEMENT

Our primary approach leverages the Passive Aggressive (PA)
algorithm [37]. Under normal circumstance, the PA algorithm
works in a manner similar to the classical perceptron. At the
t’th optimization step we have a hyperplane wt ∈ Rd, a data
point xt ∈ Rd, and a label yt ∈ {−1, 1}, the goal is that
(cid:124)
w
t xt · yt ≥ 0, indicating we predicted xt to belong to class
yt. If we are successful in doing this, we make no change
(wt+1 = wt). If not, we alter it by wt+1 = wt + τtytxt,
where τt is a constant based on the learning algorithm. In the
classical perception, τt = 1.

When performing Passive Aggressive updates [37], our goal

is to solve the optimization problem given by Equation 1.

1
2

(1)

wt+1 = argmin
w∈Rd

(cid:107)w − wt(cid:107)2
2
s.t. max (0, 1 − yt · w(cid:124)x) = 0
We want to ﬁnd a new hyperplane wt+1 that is the smallest
possible distance from the current solution (wt) as possible,
while simultaneously making the hinge loss (cid:96)(s) = max(0, 1 −
s) for the current item correct. This ﬁxes the error (or “fully
corrects” the hyperplane) while minimizing the change to the
model. This problem has an exact solution [37], which is given
by Equation 2.

wt+1 = wt + τtytxt where τt =

(cid:124)
t x

1 − yt · w
(cid:107)xt(cid:107)2

(2)

Under normal circumstances, we would not actually want to
use Equation 2, because any mislabeled, noisy, or otherwise
outlier datum could destroy the accuracy of the model. Indeed
Crammer, Dekel, Keshet, et al. [37] modiﬁed this equation to

1The exact origin is regarded as sensitive information, and so unfortunately

not disclosed.

(cid:124)
t x

include a regularization term C, to limit the change to the model
(e.g., τt = min(C, 1−yt·w
) so that it did not fully correct
(cid:107)xt(cid:107)2
the label, which resulted in signiﬁcantly improved accuracy.
In our case, we will not use the PA approach to train our
model, but instead to reﬁne an already trained model that has
been deployed into production. First, we will train MalConv
style network [38] on the input features. This model would be
deployed, with a threshold selected that satisﬁes a desired false-
positive rate (e.g., 1% or 0.1%). With this MalConv model, we
remove the softmax layer and use the penultimate activation
as a featurized version of the input ﬁle.

Users will submit their false positives to the model, which
must fully correct the error so that it is no longer a false positive
using Equation 2. We hypothesize that most false positives
live near the hyperplane border, so these corrections could be
made without impairing the model’s performance on the global
population. We use the PA updates because they induce the
smallest possible change to the model to minimize the potential
impact.

To support this intuition, we used Uniform Manifold Ap-
proximation and Projection (UMAP) [39], [40] to create a 2D
visualization of the benign, malicious, and hard false positives
discussed in section III. The resulting UMAP embedding can
be found in Figure 1, where the black X marks show the hard
false positives from the evaluation set. You can see that they
exist on the edge of a larger cluster, in a region that contains
a broad mix of both benign and malicious samples.

A. Preventing Catastrophic Changes to Models

As was mentioned above, our approach using the complete
update of Equation 2 can be dangerous if a mislabeled data
point is given. For a comprehensive solution that can be used in
deployment, we need to warn users from performing updates
that may hinder the performance of the model as a whole.
However, we can not give users access to all training data,
which could cause privacy issues. We would necessitate more
signiﬁcant compute resources to check the impact against all
prior data.

We note that we can approximate the impact of our model
using the K-Means algorithm. Clusters from the training data
can be shared with users in the deployed model without risking
privacy concerns and keeping the corpus small enough that
evaluations can be run quickly with standard desktop hardware.
Our approach is outlined in Algorithm 1, where the Es-
timateAUC function takes in a hyperplane w, the K-means
computed from the training data c1, . . . , cK, and two statistics:
s(ci) ∈ [0, N ], the number of data points in cluster ci, and
l(ci) ∈ [0, 1], the fraction of positive instance (i.e., malware)
in cluster ci. This function looks at the label assigned to each
cluster based on the given hyperplane w, and tabulates an error
based on the average label of that cluster. Because the cluster
center acts as a summary of many points, it is effectively
similar to estimating AUC in buckets of probability score (or
distance to the margin), assuming each item within a cluster
would have received a similar relative classiﬁcation score.

Fig. 1: UMAP embedding of the Macro dataset using the penultimate activations of MalConv. Benign samples in blue, malicious
samples in red, and a black X marks the locations for the 58 hard FPs.The box in the lower-left shows the zoomed in view of
the area that contains the majority of hard FPs.

Using this function, we can simply receive a new ﬁle f from
the user with a correct label y, that they wish to update the
model with. We take the penultimate activations from MalConv
as our feature vector, and compute a new weight vector ˆw
using Equation 2. We can then estimate the AUC using the
original and updated solutions (w and ˆw respectively), and
take the difference. If the result is positive, we will anticipate
that the update improved the model quality. If negative, we
have degraded the quality. In deployment, we may select a
minimum threshold of allowed degradation (e.g., 0.05) before
preventing the user from performing any more updates, out of

concern that they would negate the model’s usefulness.

V. MODELS USED

Now that we have described our approach to using, against
normal consideration, the exact Passive Aggressive updates, we
will deﬁne the ﬁnal model we use plus additional baselines. We
conducted our experiments using two different sets of features,
1) the set of 328 features extracted using domain knowledge
and selected by analysts, and 2) the penultimate activations
from using a MalConv network [38] trained from the raw bytes
of the Macro code.

Algorithm 1 Estimate Impact to AUC

1: function ESTIMATEAUC(w, c1, . . . , cK, s(·), and l(·))
2:
3:
4:
5:

α ← 0
for i ∈ [1, K] do
ˆy ← w(cid:124)ci
if ˆy ≥ 0 then

6:
7:
8:

9:

α ← α + s(ci) · l(ci)

else

α ← α + s(ci) · (1 − l(ci))

return

(cid:80)K

α
i=1 s(ci)

Require: Desired number of clusters K, MalConv embedded

data points X

10: c1, . . . , cK ← K means computed by K-Means clustering

of training data X

11: Let s(cj) indicate the number data points assigned to

cluster j

j

12: Let l(cj) indicate the fraction of malicious items in cluster
//Users get access only to c1, . . . , cK, s(·), and l(·)
13: Receive new ﬁle f with label y, that needs to be corrected.
14: x ← M alConv(f ) //Extract penultimate activation from

(cid:107)x(cid:107)2

MalConv
15: ˆw ← 1−y·w(cid:124)x
16: init ← ESTIMATEAUC(w, c1, . . . , cK, s(·), l(·))
17: result ← ESTIMATEAUC( ˆw, c1, . . . , cK, s(·), l(·))
18: return estimated AUC impact result − init

· y · x

//Equation 2

For the domain knowledge features, we will train a gradient
boosted decision tree (GBDT) as our primary baseline, as this
was the original model deployed in production. We ﬁnd that
using these domain knowledge-based features, the expressive
power of a GBDT is necessary to achieve reasonable accuracy.
To show this, we will train the PA model on this data as
well, including a kernelized version using the Random Fourier
Features approximation [41] of the Radial Basis Function
kernel, K(x, x(cid:48)) = exp
. These will be
referred to as just GBDT, PA, and Kernel PA, respectively.
For PA and Kernel PA, we ﬁrst unit normalized each feature.
For Kernel PA, the γ parameter was selected by grid search
from [10−3, 10−2, 10−1, 1, 101, 102, 103].

−γ · (cid:107)x − x(cid:48)(cid:107)2
2

(cid:16)

(cid:17)

The PA and Kernel PA baselines will show the importance of
using the MalConv based feature extraction, which provides the
beneﬁt of not requiring analyst time to produce new features.
We trained MalConv with a batch size of 64, and embedding
dimension of 8, 128 ﬁlters with a ﬁlter size of 128, and a stride
of 2. We speciﬁcally use a stride of 2 because the raw Macro
code was encoded as UTF-8 due to non-ASCII characters, so
a stride of 2 corresponds to processing the string one Unicode
character at a time (though each Unicode token is broken up
into two tokens embedded separately). MalConv was trained
for 5 epochs, with a resulting AUC of 99.4% and an accuracy
of 48.27 on the hard FPs, so MalConv alone is not sufﬁcient.
For our PA model trained on top of the MalConv penultimate
activation’s, we denote it as MalConv+PA. This is our primary

approach, showing the utility of the full Passive Aggressive
updates. To show why this full correction is needed, we will
also compare with adjusting the ﬁnal activation of MalConv
using Stochastic Gradient Descent (SGD), which is the standard
approach for gradient-based learning. This MalConv+SGD
baseline allows for online learning, but can not adapt in the
small number of examples we desire to ﬁx false positives in
production.

Our last MalConv based ablation model will use Prototype-
Networks [42], a recent method for few-shot learning. It works
by using a neural network f (·) (in our case, MalConv) to
embed an input x ∈ Rd into a new feature space z ∈ Rd(cid:48)
.
The approach of the Prototype-network is to then average the
embedding zi within a class, and use the distance to the nearest
embedding as the mechanism for classiﬁcation. We train in
the style described by Snell, Swersky, and Zemel [42] using
MalConv as the network architecture with an output embedding
d(cid:48) = 128. Our results below will show that the large differences
between our scenario with two classes instead of many results
in the few-shot learning approach having limited utility.

VI. RESULTS

Now that we have described our approach MalConv+PA, and
the ﬁve baselines we will compare against, we will show the
results in two scenarios. First is the ﬁxed scenario, where
we look at each method’s performance on the larger test
set. This gives us an understanding of each model’s global
performance. Models that do not have a sufﬁciently adequate
global performance are not viable for our use under any
circumstances.

The second scenario is the adaptive one. Here we have an
initial global model, which will be exposed to a sequence of
hard false positives that are mission-critical and thus need to
be corrected, as described in section III. The mode will receive
an example xi , and, if wrong, will then perform a corrective
update. This will be done for all 58 hard FPs in random order.
This process will be repeated 200 times with a different random
ordering each time, to get a distributional understanding of the
true impact. This avoids biasing our results to an ordering that
may be particular (un)favorable to any approach. We will look
at the global metrics and how the change as more hard FPs are
presented, and the total number of errors/times the model had
to be adjusted against hard FPs. Ideally, only 1 error/adjustment
would be needed for the model to get all remaining hard FPs
correct.

A. Baseline Performance

Now that we have speciﬁed our approach to this problem
and the baseline methods we will compare against; we will ﬁrst
look at the performance on the larger test set of 276,921 macro
ﬁles. Our goal is to determine the classiﬁer’s performance
against the global population at-large. This can be found in
Table I. Algorithms prepended with “MalConv” work base of
ﬁrst training the MalConv architecture. All others used the
production domain knowledge-based feature extractor.

TABLE I: Performance of all 4 methods on the larger test set.

Algorithm

Acc

AUC

AUCF P R≤.1%

FPR

TPR

MalConv+PA
MalConv+SGD
MalConv+Prototype
GBDT
PA
Kernel PA

96.66
97.06
60.97
99.85
95.13
66.80

99.34
99.36
64.96
99.97
97.12
63.26

78.30
79.21
50.01
99.27
50.39
56.28

0.1005
0.0997
13.29
0.0930
0.1006
0.0999

58.35
66.18
86.70
99.65
2.310
14.87

Prototype-Networks, a few-shot learning approach, is most
notable for its deﬁcient performance and lack of generalization
to the test set. While the few-shot paradigm framework is an
attractive model to use for the issue of ﬁxing false positives
at deployment time, we found signiﬁcant problems in training
and using in — often resulting in network divergence during
training, or the most successful case was networks that degraded
to degenerate solutions of projecting all data onto a few sets
of points. This caused a limited number of distance values to
each class centroid, making it difﬁcult to achieve the desired
FPR and hampered overall performance.

We believe this stems from the miss-match in the underlying
assumptions used in the few-shot learning literature and our
application space. For most few-shot datasets, e.g., omniglot,
there are many classes with a limited amount of intraclass
variance. This makes the problem of recognizing a new class
and encapsulating it easier. For our data, we have only two
classes, each of which has signiﬁcant intraclass variance. In
particular, benign applications have broad diversity in their
content and use cases. This clashes with the few-shot models’
underlying assumptions, like Prototype-networks, leading to
the issues we see.

We also see that

the GBDT model using the domain
knowledge features has the absolute best performance by a
wide margin. While the PA and SGD approaches that use
MalConv as a feature extractor have close AUC scores, the tail
of the distribution we care about is an FPR ≈ 0.1%. In this
domain the GBDT, using domain knowledge features, clearly
has the best performance with an up to 70.8% improvement in
TPR at the desired threshold. MalConv+PA and MalConv+SGD
have performance similar to each other, with detection rates
considered acceptable but not as good as GBDT. However, the
MalConv+SGD combination appears to perform better initially.
We emphasize that while the GBDT model has much better
global TPR, these values are not useful if the local FPR is
too high. In such a situation, it is preferable to switch to a
model with lower but acceptable TPR to provide some level
of defense because the alternative is a user disabling the tools,
leaving them vulnerable. This is the precise situation that has
occurred in production that our work mitigates. We show this
in the next section.

B. Hard Benign Sample Performance

Given these initial results, our preference would lean
towards using domain knowledge features in a GBDT model.
This approach would allow us to provide users with some
interpretability to reduce the “black-box” feel of malware

classiﬁcation. However, the picture changes when we look at
the population of 58 samples generated by popular enterprise
software. These results are shown in Table II. Using a
ﬁxed global model, the GBDT fails to classify all of these
critical applications. This necessitates using a allowlist to
suppress alerts on these ﬁles. Because these are document, the
hash representation changes with each update/save operation
resulting in regular and persistent false positives that have lead
to signiﬁcant user frustration.

TABLE II: False Positive Rate (FPR) on the 58 hard held out
samples, lower is better. The Fixed column shows the FPR
with a static model. The Adaptive column shows FPR as the
model gets a chance to update after each error, sampled over
200 trials.

Hard FP Rate (%)

Algorithm

Fixed

Adaptive

MalConv+PA
MalConv+SGD
GBDT

4.33±1.919%
58.62%
37.93% 26.46±1.893%
100.0%

N/A

Both PA and SGD based approaches can run in the adaptive
scenario. Again, in this situation, the 58 hard benign ﬁles
are presented sequentially as if being found over time in
deployment. When the model makes an FP error (calling the
benign application malicious), we perform a model update to
hopefully resolve the issue. This makes the test order sensitive,
so we averaged the results over 200 random trials.

The PA approach, while having a 58.6% FPR under the ﬁxed
scenario, drops down to just 4.33% in the adaptive scenario.
This corresponds to making an average of just 2.51 errors
against all 58 hard benign applications. This is better than the
GBDT, which can not adapt on a single-instance basis, and
better than the SGD method. SGD, while it does reduce its
FPR from 37.9% down to 26.5%, makes smaller changes in a
model view that it is trying to optimize a larger function. This
shows how the PA model’s attempt to fully correct each error
better matches this issue in production and leads to efﬁciency
in ﬁxing FPs with a minimal number of errors.

In particular, Figure 2 shows results from all 200 random
trials for each method. The MalConv+PA approach never makes
more than 5 errors on all 58 ﬁles, and as few as just 1 error.
The MalConv+SGD approach makes at least 13 errors and up
to 18, making it several times worse on this critical dataset.

While the MalConv+PA model did not have the highest
possible accuracy on the global dataset, customers’ ability to
adapt the model to their local domain is critical. This new
approach to using the PA model is the only current method to
quickly adapt deployed models, making it preferable to deal
with such hard benign programs. We also note that the change
in TPR and FPR trade-offs can be ﬁxed easily by sending
the PA weights (a few KB) back to the AV provider for re-
calibration of the threshold to obtain an FPR of 0.1% again,

15

10

s
r
o
r
r
E

5

e
r
o
c
S

1.0

0.8

0.6

0.4

0.2

0.0

Accuracy
AUC
AUC@FPR<0.1%
TPR
Est Impact

MalConv+SGD

MalConv+PA

Method

0

10

20

40
30
Hard FPs Presented

50

60

Fig. 2: Number of errors made in the adaptive scenario. Each
dot representing the result from one trial with a different random
ordering of the benign ﬁles.

which restores TPR to 58%, and still classiﬁers all hard FPs
correctly.

C. Adaptive Impact and Estimated Impact

Now that we have shown our PA approach is the best current
method to deal with false positives in production, we consider
the global impact of these adjustments on the MalConv+PA
model, as well as our ability to estimate the impact described
using our k-means based approach from subsection IV-A. We
arbitrarily choose K = 1024 clusters based on two factors.
Any K smaller than the whole dataset accomplishes the privacy
goal, and larger values of K are expected to produce better
AUC approximations, as the limit is K equal to the size of
the corpus, resulting in the true AUC being computed.

We start by looking at all of our metrics, excluding the FPR,
in Figure 3. These results are over the average of 200 trials and
measured against the larger test set of 276,921 ﬁles. Here we
can see that the global ability of the model is not signiﬁcantly
altered. Using the standard threshold of 0, the Accuracy and
AUC see minimal impacts, and on average, actually improve
slightly as more ﬁles are presented. For example, Accuracy
has the biggest improvement from 96.95% to 97.30%.

The estimated impact, using Algorithm 1, of the model is
always measured against the original global model. This is
done so that we are not lured into a false sense of conﬁdence,
and we want to know if the model has diverged too signiﬁcantly
from the global case. If we measured impact based on changes
against previous versions of the model, the impact would
almost always be 0 because the PA approach makes ≤ 5
changes over all 58 ﬁles. Measuring against the previously
modiﬁed model could also lead to many small changes with
collectively represent a large change from the global model,
without realizing that such signiﬁcant changes have occurred.
Measuring against the original global model, Figure 3 shows
that the estimated impact to AUC is always small in magnitude

Fig. 3: Accuracy, AUC, AUCF P R ≤ 0.1%, TPR, and estimated
impact on the Macro+PA model larger test dataset as hard
benign ﬁles are presented. Average results over 200 random
trials, standard deviation shown in lighter bands around each
method. The ﬁrst three methods, standard deviation is drawn
but exceptionally small (zoom in digitally to see).

and usually positive. The actual AUC (the red line) shows
minimal impact and a slight positive trend. This indicates a
behavioral match between our estimation approach and the
test data appears to be consistent. We also note that the AUC
measured up to a maximum FPR of 0.1% also shows no
signiﬁcant impact, with only slight improvement.

We note that measuring the AUC up to a maximum FPR
involves determining the exact threshold for achieving that FPR,
requiring all the data. So these results show that the global
properties of our model are maintained, but results could differ
based on the threshold originally learned from the training data,
which may no longer be the best threshold to use in practice.
The True Positive Rate (purple) in Figure 3 shows this result
when using the original threshold, which reduces the TPR by
48.3%. This is not surprising because we are explicitly biasing
the model toward calling more things benign, but still effective
at 28.17 TPR.

The drop in TPR comes with the beneﬁt of a far more
signiﬁcant drop in the FPR of the models. This is shown in
Figure 4, where the TPR decreases monotonically down to
0.001365%, a 73× reduction in false positives. We note that
re-calibrating the threshold to achieve a 0.1% false positive
rate still results in 0 FPs on the hard benign applications, since
the PA approach guarantees that they are on the malicious side
of the margin, and do not need any special thresholding to be
classiﬁed correctly. We will talk about re-calibration more in
section VII.

These results show that the global properties of the Mal-
Conv+PA model have remained adequate and unaltered by our
approach, baring a potential re-calibration of the threshold to
achieve an FPR of 0.1% instead of 0.001%. While our testing

FPR

0.001

0.0008

R
P
F

0.0006

0.0004

0.0002

0

0

10

20

30
40
Hard FPs Presented

50

60

Fig. 4: False Positive Rate (y-axis) as the average number of
hard FPs (x-axis) are presented to the MalConv+PA model.
Because we do not alter the threshold used, and the hard FPs
are corrected in favor of calling more things benign, the FPR
goes down signiﬁcantly by 74×.

t
c
a
p
m

I

e
u
r
T

0.0

-0.1

-0.2

-0.3

-0.4

-0.5

-0.6

-0.6

-0.5

-0.4

-0.3
-0.2
Estimated Impact

-0.1

0.0

Fig. 5: Estimated Impact to AUC (x-axis) compared to actual
impact (y-axis). Black line shows a simple linear ﬁt, and dashed
lines show the 95% conﬁdence interval of the results. Results
shown for a random sample of 2,500 points for legibility.

showed that our AUC impact estimation from subsection IV-A
correctly predicted low impact to the model, it does not tell
us if our approach would successfully stop us from making a
dangerously large change to the model.

To test that, we ran an experiment where we took ﬁles
from the training set, swapped their labels, and then performed
PA updates. We then compared the estimated impact from
Algorithm 1 with the actual impact on the larger test set. These
results can be found in Figure 5. Here we can see that our
approach reliably estimates the true impact to AUC in a strong
linear relationship, with almost all points ﬁtting with a 95%

conﬁdence interval. Thus we can rely on our approach to
detect large changes to AUC (e.g., -0.5 would indicate taking
the model from accurate to near random-guessing levels of
performance) that would be detrimental to the model’s use.
A threshold of maximal AUC change can be selected such
that we either warn the users that the change may reduce their
ability to detect malware reliably or, if large enough, prevent
the users from making the change entirely — and potentially
warning that the ﬁle under consideration needs expert review.

VII. PRACTICAL CONSIDERATIONS

Now that we have described our approach to adapting a
production model based on false positives, we take a moment
to consider how this system can be used in production. The ﬁrst
concern is that, under the global model, the GBDT approach
does have better performance than MalConv+PA. For this
reason, we would deploy our approach using the GBDT based
model ﬁrst and only switch to the MalConv+PA model once a
hard FP has been presented that needs to be corrected. This is
because not all customers experience these hard FPs, and we
can retain the better global beneﬁts of GBDT in such a case.
Once a hard FP has been correct, and we have switched to
MalConv+PA as the primary model, we would still encourage
users to send the false positives back for further analysis and
enter the process for a global model update. A future GBDT
version, given many such FPs, may eventually get all hard FPs
for a client, and we can switch back to this better global model.
The MalConv+PA approach gives us the means to make sure
those customers have an immediate and still useful solution,
without causing them to forgo all protection due to excessive
alerts and multi-week to multi-month lag time of the global
model. A ﬂow diagram of this combined process is given in
Figure 6.

In such situations, we may also choose to run the GBDT
model and MalConv+PA model simultaneously on all inputs,
while relying on MalConv+PA to act as an abstainer for primary
alerts. For cases where the two models disagree, we may throw
lower-priority alerts or request that such samples be sent back
for further analysis and eventually improve the global model.
Finally, we note that the current approach results in an
effective change in the target FPR rate after MalConv+PA
performs a correction. Rather than let the customer run at their
model at this new effective FPR, we can send the corrected
model back to the corporate environment to perform a re-
calibration on the threshold. This is computationally cheap
to perform, requiring only a few seconds on our corpus, and
eliminates the reduction in TPR observed in Figure 3. Sending
the model back to corporate and returning a new threshold is
done because we do not want to distribute gigabytes of training
data to customers as part of a deployed model. This would
pose an unnecessary storage cost and privacy concerns. This
re-calibration can be done by sending only the changed model
back to corporate, protecting the customer’s privacy of samples
if they are unwilling or unable to share them for global model
improvement.

New File

Using Local
Model?

N

Y

Global Model

Local Model

PA Update
Local Model

N

New Global
Model?

Y

Done

N

Remediate

N

Malware?

Y: Alert

False Positive?

Y

Local FP
Database

Y (Low Impact):
Insert

Estimate
AUC Impact

N (High Impact)

N

Covers All
Local FP?

Y

Switch to new
Global Model

Tag for
detailed audit

Fig. 6: Flow diagram for deployed use. A new ﬁle comes in and must be classiﬁed, by default using the global model (GBDT).
Alerts are reviewed and if determined to be a false positive, the estimated impact is computed with Algorithm 1. If above some
threshold, the ﬁle is tagged for expert audit. Otherwise the model can be adaptive with low impact to performance. The datum
is added to a local database. If a new global model is available and satisﬁes all local historical FPs, switch to the new global
model. Otherwise continue using local model, and perform a Passive Aggressive update to correct local errors, and switch to
using local model.

VIII. CONCLUSION

Using machine learning to detect malicious activity will
always yield false positives in production environments. That
is the trade-off between detections that only catch the known
versus detections that also attempt to catch the unknown.
However, the current false positive triage approaches are
antiquated, often relying solely on “whack-a-mole” hash-
based allowlisting that leaves security workers drowning in
alerts. There is an opportunity to develop novel methods to
signiﬁcantly reduce local false positives by exposing intuitive
ML capabilities to security workers, so their domain expertise
can reﬁne and tailor models.

Our approach highlights how an iterative, human-in-the-loop
process can be applied to close the gap between identifying
false positives local to an environment and ensuring future
models do not repeat those mistakes. We aim to limit the set
of observations required to tailor a model while keeping an
eye on preserving user data privacy.

REFERENCES
[1] NISC, “NISC Survey Results,” Neustar International Security

Council, Tech. Rep., 2020.

[2] M. Botacin, F. Ceschin, P. de Geus, and A. Grégio, “We need to
talk about antiviruses: challenges & pitfalls of av evaluations,”
Computers & Security, vol. 95, p. 101 859, 2020.

[3] F. B. Kokulu, A. Soneji, T. Bao, Y. Shoshitaishvili, Z. Zhao,
A. Doupé, and G.-J. Ahn, “Matched and Mismatched SOCs:
A Qualitative Study on Security Operations Center Issues,”
in Proceedings of the 2019 ACM SIGSAC Conference on
Computer and Communications Security, ser. CCS ’19, New
York, NY, USA: Association for Computing Machinery, 2019,
pp. 1955–1970.

[4] B. Li, K. Roundy, C. Gates, and Y. Vorobeychik, “Large-Scale
Identiﬁcation of Malicious Singleton Files,” in 7TH ACM
Conference on Data and Application Security and Privacy,
2017.

[5] G. E. Dahl, J. W. Stokes, L. Deng, and D. Yu, “Large-
scale malware classiﬁcation using random projections and
neural networks,” in 2013 IEEE International Conference on
Acoustics, Speech and Signal Processing, IEEE, May 2013,
pp. 3422–3426.

[6] M. Z. Shaﬁq, S. M. Tabish, F. Mirza, and M. Farooq, “A
Framework for Efﬁcient Mining of Structural Information
to Detect Zero-Day Malicious Portable Executables,” 1Next
Generation Intelligent Networks Research Center (nexGIN
RC), Tech. Rep., 2009.

[7] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert, “ZOZZLE:
Fast and Precise in-Browser JavaScript Malware Detection,”
in Proceedings of the 20th USENIX Conference on Security,
ser. SEC’11, USA: USENIX Association, 2011, p. 3.
[8] Y. Fukushima, A. Sakai, Y. Hori, and K. Sakurai, “A behavior
based malware detection scheme for avoiding false positive,”
in 2010 6th IEEE Workshop on Secure Network Protocols,
IEEE, Oct. 2010, pp. 79–84.

[10]

[9] S. W.-t. Yih, J. Goodman, and G. Hulten, “Learning at Low
False Positive Rates,” in Proceedings of the 3rd Conference
on Email and Anti-Spam, CEAS, Jul. 2006.
J. Jang, D. Brumley, and S. Venkataraman, “BitShred: Feature
Hashing Malware for Scalable Triage and Semantic Analysis,”
in Proceedings of the 18th ACM conference on Computer and
communications security - CCS, New York, New York, USA:
ACM Press, 2011, pp. 309–320.

[11] A. Calleja, J. Tapiador, and J. Caballero, “The MalSource
Dataset: Quantifying Complexity and Code Reuse in Malware
Development,” IEEE Transactions on Information Forensics
and Security, vol. 14, no. 12, pp. 3175–3190, Dec. 2019.

[12] M. Christodorescu and S. Jha, “Testing Malware Detectors,”
in Proceedings of the 2004 ACM SIGSOFT International
Symposium on Software Testing and Analysis, ser. ISSTA ’04,
New York, NY, USA: ACM, 2004, pp. 34–44.

[13] C. Kolbitsch, P. M. Comparetti, C. Kruegel, E. Kirda, X. Zhou,
and X. Wang, “Effective and Efﬁcient Malware Detection at
the End Host,” in 18th USENIX Security Symposium (USENIX
Security 09), Montreal, Quebec: {USENIX} Association, Aug.
2009.

[14] T. K. Tran, H. Sato, and M. Kubo, “One-shot Learning
Approach for Unknown Malware Classiﬁcation,” in 2018 5th
Asian Conference on Defense Technology (ACDT), IEEE, Oct.
2018, pp. 8–13.

[15] S.-C. Hsiao, D.-Y. Kao, Z.-Y. Liu, and R. Tso, “Malware
Image Classiﬁcation Using One-Shot Learning with Siamese
Networks,” Procedia Computer Science, vol. 159, pp. 1863–
1871, 2019.

[16] T. Chen and C. Guestrin, “XGBoost: Reliable Large-scale Tree
Boosting System,” in Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data
Mining, 2016.

[17] A. Bifet, G. Holmes, B. Pfahringer, and E. Frank, “Fast per-
ceptron decision tree learning from evolving data streams,” in
Paciﬁc-Asia conference on Advances in Knowledge Discovery
and Data Mining - Volume Part II, 2010, pp. 299–310.
P. Domingos and G. Hulten, “Mining high-speed data streams,”
the sixth ACM SIGKDD international
in Proceedings of
conference on Knowledge discovery and data mining - KDD
’00, Boston, MA: ACM Press, 2000, pp. 71–80.

[18]

[19] G. Hulten, L. Spencer, and P. Domingos, “Mining time-
changing data streams,” in seventh ACM SIGKDD international
conference on Knowledge discovery and data mining, ACM,
2001, pp. 97–106.

[20] B. Lakshminarayanan, D. M. Roy, and Y. W. Teh, “Mondrian
Forests: Efﬁcient Online Random Forests,” in Advances in
Neural Information Processing Systems 27, Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,
Eds., Curran Associates, Inc., 2014, pp. 3140–3148.
[21] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise
of adversarial machine learning,” Pattern Recognition, vol. 84,
pp. 317–331, Dec. 2018.

[22] M. A. Rajab, L. Ballard, N. Jagpal, P. Mavrommatis, D. Nojiri,
N. Provos, and L. Schmidt, “Trends in CircumventingWeb-
Malware Detection,” Google, Tech. Rep. July, 2011.
[23] O. Suciu, S. E. Coull, and J. Johns, “Exploring Adversarial
Examples in Malware Detection,” ALEC’18 (AAAI 2018 Fall
Symposium), 2018.

[24] L. Demetrio, B. Biggio, G. Lagorio, F. Roli, and A. Armando,
“Explaining Vulnerabilities of Deep Learning to Adversarial
Malware Binaries,” in 3rd Italian Conference on Cyber Security,
ITASEC, 2019.

[25] B. Kolosnjaji, A. Demontis, B. Biggio, D. Maiorca, G. Giacinto,
C. Eckert, and F. Roli, “Adversarial Malware Binaries: Evading
Deep Learning for Malware Detection in Executables,” in 26th
European Signal Processing Conference (EUSIPCO ’18), 2018.

[26] F. Kreuk, A. Barak, S. Aviv-Reuven, M. Baruch, B. Pinkas,
and J. Keshet, “Adversarial Examples on Discrete Sequences
for Beating Whole-Binary Malware Detection,” arXiv preprint,
2018.

[27] A. Demontis, M. Melis, B. Biggio, D. Maiorca, D. Arp, K.
Rieck, I. Corona, G. Giacinto, and F. Roli, “Yes, Machine
Learning Can Be More Secure! A Case Study on Android
Malware Detection,” IEEE Transactions on Dependable and
Secure Computing, pp. 1–1, 2017.
I. Incer, M. Theodorides, S. Afroz, and D. Wagner, “Adver-
sarially Robust Malware Detection Using Monotonic Clas-
siﬁcation,” in Proceedings of the Fourth ACM International
Workshop on Security and Privacy Analytics, ser. IWSPA ’18,
New York, NY, USA: ACM, 2018, pp. 54–63.

[28]

[29] W. Fleshman, E. Raff, J. Sylvester, S. Forsyth, and M. McLean,
“Non-Negative Networks Against Adversarial Attacks,” AAAI-
2019 Workshop on Artiﬁcial Intelligence for Cyber Security,
2019.

[30] W. Fleshman, E. Raff, R. Zak, M. McLean, and C. Nicholas,
“Static Malware Detection & Subterfuge: Quantifying the
Robustness of Machine Learning and Current Anti-Virus,”
in 2018 13th International Conference on Malicious and
Unwanted Software (MALWARE), IEEE, Oct. 2018, pp. 1–10.
[31] H. S. Anderson, B. Filar, and P. Roth, “Evading Machine
Learning Malware Detection,” in Black Hat USA, 2017.
[32] H. Aghakhani, F. Gritti, F. Mecca, M. Lindorfer, S. Ortolani,
D. Balzarotti, G. Vigna, and C. Kruegel, “When Malware is
Packin’ Heat; Limits of Machine Learning Classiﬁers Based on
Static Analysis Features,” in Proceedings 2020 Network and
Distributed System Security Symposium, Reston, VA: Internet
Society, 2020.

[33] E. M. Rudd, R. Harang, and J. Saxe, “MEADE: Towards
a Malicious Email Attachment Detection Engine,” in 2018
IEEE International Symposium on Technologies for Homeland
Security (HST), IEEE, Oct. 2018, pp. 1–7.

[34] C. Crawl, Common crawl, 2018. [Online]. Available: https:

//commoncrawl.org/.

[35] VirusTotal-Free online virus, malware and URL scanner, 2018.

[Online]. Available: https://www.virustotal.com.

[36] S. Zhu, J. Shi, L. Yang, B. Qin, Z. Zhang, L. Song, and
G. Wang, “Measuring and Modeling the Label Dynamics
of Online Anti-Malware Engines,” in 29th USENIX Security
Symposium (USENIX Security 20), Boston, MA: {USENIX}
Association, Aug. 2020.

[37] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer, “Online passive-aggressive algorithms,” Journal of
Machine Learning Research, vol. 7, pp. 551–585, 2006.
[38] E. Raff, J. Barker, J. Sylvester, R. Brandon, B. Catanzaro, and
C. Nicholas, “Malware Detection by Eating a Whole EXE,” in
AAAI Workshop on Artiﬁcial Intelligence for Cyber Security,
Oct. 2018.

[39] L. McInnes, J. Healy, and J. Melville, “UMAP: Uniform Mani-
fold Approximation and Projection for Dimension Reduction,”
arXiv, 2018.

[40] C. J. Nolet, V. Lafargue, E. Raff, T. Nanditale, T. Oates,
J. Zedlewski, and J. Patterson, “Bringing UMAP Closer to the
Speed of Light with GPU Acceleration,” arXiv, 2020.
[41] A. Rahimi and B. Recht, “Random Features for Large-Scale
Kernel Machines,” in Neural Information Processing Systems,
2007.
J. Snell, K. Swersky, and R. Zemel, “Prototypical Networks
for Few-shot Learning,” in Advances in Neural Information
Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.,
Curran Associates, Inc., 2017, pp. 4080–4090.

[42]

