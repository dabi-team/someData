8
1
0
2

v
o
N
5
1

]

R
C
.
s
c
[

1
v
2
6
1
6
0
.
1
1
8
1
:
v
i
X
r
a

Plan Interdiction Games

Yevgeniy Vorobeychik and Michael Pritchard

Abstract We propose a framework for cyber risk assessment and mitigation which
models attackers as formal planners and defenders as interdicting such plans. We
illustrate the value of plan interdiction problems by ﬁrst modeling network cyber
risk through the use of formal planning, and subsequently formalizing an important
question of prioritizing vulnerabilities for patching in the plan interdiction frame-
work. In particular, we show that selectively patching relatively few vulnerabili-
ties allows a network administrator to signiﬁcantly reduce exposure to cyber risk.
More broadly, we have developed a number of scalable approaches for plan inter-
diction problems, making especially signiﬁcant advances when attack plans involve
uncertainty about system dynamics. However, important open problems remain, in-
cluding how to effectively capture information asymmetry between the attacker and
defender, how to best model dynamics in the attacker-defender interaction, and how
to develop scalable algorithms for solving associated plan interdiction games.

1 Introduction

Interdiction seems by its very nature an adversarial act, one perpetrated, if you will,
by “bad guys.” For example, an attacker may interdict the power ﬂow on an electric
power grid, resulting in widespread blackouts [15], or interdict a transportation or a
supply network [11, 8]. Consequently, it may be somewhat jarring at ﬁrst to consider
the defender —the “good guy”— as the interdictor. We would like to argue that
in cybersecurity this is precisely the ultimate goal of the defender: to interdict an

Yevgeniy Vorobeychik
Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN
e-mail: eug.vorobey@gmail.com

Michael Pritchard
Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN
e-mail: michael.j.pritchard@vanderbilt.edu

1

 
 
 
 
 
 
2

Yevgeniy Vorobeychik and Michael Pritchard

attack, whether actual or potential. In particular, in this chapter we will illustrate
that cyber attacks are naturally viewed as plans, or dynamic decision processes by
malicious agents.

At the most basic level, a plan is a sequence of actions which, if successfully
executed beginning from an appropriate initial state, accomplishes a planner’s goal.
Of course, this notion of a plan is quite restricted; for example, if uncertainty is at
all salient, an intelligent plan would involve contingencies, or a mapping from ob-
served state to action. The perspective we will take in this chapter is that the attacker
is such a planner. The initial state for the attacker includes the initial vulnerabilities
of the target network, as well as any relevant capabilities and information that the
attacker possesses. An attack is then a series of actions —perhaps, as a part of a
contingent plan of actions— aimed at accomplishing a malicious goal, or perhaps a
contingent series of goals, each with a different value to the attacker. The defender,
as we had remarked, is the interdictor: insofar as the attacker’s plan would accom-
plish goals which are contrary to the defender’s desires, the defender would wish to
prevent this from happening. A myopic defender would simply attempt to interdict
past attacks; arguably, that is what most cyber-defense is like in practice: preventing
attacks which have been identiﬁed from succeeding. We argue for a longer, proac-
tive view: the defender can, and should, reason about alternative attack plans that
the attacker could make, for particular defensive interdiction strategies, and choose
an optimal interdiction —one that proactively accounts not merely for previously
known attacks, but possible future variations which circumvent the defense as well.
Below, we begin with a simple example to illustrate the nature of attack plans
and interdictions. Subsequently, we describe several formal modeling approaches
for plan interdiction, ﬁrst when the planning environment is deterministic, and sub-
sequently when it is stochastic. We follow with an in-depth illustration of the ap-
proach by using it as a modeling framework for network cyber risk assessment.
We then suggest future research directions that consider longer term attack plans,
when the defender is able to observe, and react to, some of the attack actions. These
directions present interesting questions about optimal adaptive defense, as well as
important considerations of adversarial deception.

2 A Simple Example

As a simple illustration, consider the following example of an attack planning prob-
lem. The initial state includes the initial attacker capabilities, such as possession of
a boot disk and port scanning utilities. Actions include both physical actions (break-
ing and entering and booting a machine from disk) as well as cyber actions, such as
performing a port scan to ﬁnd vulnerabilities.1 Figure 1 shows an attack graph (with
attack actions as nodes) for this scenario, with the actual attack plan highlighted in
red.

1 The actions in our example are taken from the CAPEC database (http://capec.mitre.
org).

Plan Interdiction Games

3

Fig. 1 Example attack graph. Boxes correspond to initial attacker capabilities, ovals are attack
actions, and diamonds are attacker goals. An optimal attack plan is highlighted in red.

Figure 2, on the other hand, shows an example interdiction plan. In this example,
we interdict a subset of actions (for example, by patching speciﬁc vulnerabilities, or
changing the network architecture). A noteworthy observation about the particular

Fig. 2 Example interdiction plan: actions that are blocked are colored in blue, and the ﬁnal attack
plan (circumvention) is highlighted in red.

interdiction strategy in Figure 2 is that it still allows the attacker to attack a low-
stakes target. This is because, in this speciﬁc example, interdicting all possibilities
is not cost-effective, relative to the consequence of successful attack.

4

Yevgeniy Vorobeychik and Michael Pritchard

3 Planning in AI

We begin by formalizing the notion of planning, starting with classical planning in
deterministic domains, and then considering generalizations that allow us to capture
uncertainty.

3.1 Classical Planning

Formally, a classical (deterministic) planning problem is a tuple {X, A, s0, G, r, c},
where X is the set of literals (binary variables) which represent the state of the world
relevant for the planning problem, A is the set of actions, s0 is the set of literals which
are initially true (i.e., the initial state of the world), and G is the set of goals. A plan
action a ∈ A is characterized by a set of preconditions, that is, the set of literals that
must be true in the current state for the action to be applicable, and a set of effects,
which either add literals from current state, or delete these, thereby determining
transition from one state to another. A reward function rl assigns a value (utility) to
each goal literal l ∈ G (we assume that the total utility is additive in these). Finally,
ca is the cost of taking an action a. A solution to this planning problem is a plan π,
which is a sequence of actions. A number of effective approaches exist for solving
such planning problems at scale [5].

3.2 Planning under Uncertainty

A common formalization of planning under uncertainty makes use of Markov de-
cision processes (MDPs), deﬁned by a tuple {S, A, P, r, p0}, where S is the set of
states (of the world), A the set of actions, P(s, a, s(cid:48)) = Pr{s(cid:48)|s, a} a transition func-
tion which represents system dynamics as a function of state s and action a ∈ A
taken by the planner, R(s, a) the reward function, and p0(s) the probability distribu-
tion over initial states; below, we assume that there is a ﬁxed starting state s0 [14].
A solution to the MDP is a policy, π, which determines actions to take as a function
of relevant information. If we further assume that the MDP is discrete-time, inﬁnite-
horizon, and discounted (future is discounted exponentially, with a discount factor
γ ∈ [0, 1)), we can restrict attention to policies π(s) which map states to actions
taken in these states [14, 7].

A major problem with moving from classical planning to MDPs is represen-
tational: in MDPs the full state space S is explicitly represented, whereas it is only
implicit in classical planning, a result of joint values of state literals. Factored MDPs
(FMDPs) aim to address this gap. Speciﬁcally, instead of specifying the state space
S, FMDPs are represented by a collection X of state variables (which we take to be
binary henceforth); these are random variables, with X = x denoting a particular in-
stantiation of these to values x. Dynamics are now represented by a Dynamic Bayes

Plan Interdiction Games

5

Network for each action a; further, we assume that the reward function is additive in
state variables, i.e., r(x, a) = ∑ j r j(x j, a), where r j are the variable-speciﬁc rewards.
Well-known approaches exist for approximately solving FMDPs [9].

4 Plan Interdiction Problem

Our ultimate goal is not merely to compute an optimal plan for the attacker, but
rather to compute an optimal defender interdiction strategy. To this end, we model
the interaction between the defender and attacker as a Stackelberg game in which
the defender moves ﬁrst, choosing to deploy a set of mitigations, and the attacker
responds to these by constructing an optimal attack plan given the resulting envi-
ronment. We can formalize this game as a plan interdiction problem (PIP), deﬁned
by {M , cm, rD, φ }, where M is the set of defender mitigation actions, cm is the cost
of a mitigation m ∈ M , the defender’s reward function rD(x) = ∑ j r j(x j), which is
additive over state variables, and the underlying planning problem φ for the attacker.
The consequence of a mitigation m can be two-fold: it can modify current (initial)
state s0, and remove a subset of attacker actions. Thus, if S is a subset of mitigations
used by the defender, these modify the attacker’s planning problem φ ; we denote the
resulting modiﬁed problem by φ (S), and the associated optimal plan for the attacker
by π(S). In the PIP, the defender’s goal is to choose the optimal set of mitigations S,
balancing the defender’s utility V D(S; π(S)) (total expected discounted reward) and
cost of mitigations c(S) = ∑m∈S cm:

V D(S; π(S)) − c(S),

max
S⊆M

(1)

where π(S) is the attacker’s best response plan, that is, its optimal plan over the
restricted planning domain φ (S).

We ﬁrst discuss the special case of this problem in the context of deterministic

planning, and follow with a more general treatment when uncertainty is involved.

4.1 Interdiction of Deterministic Plans

In the deterministic setting, plan interdiction is focused on goals that can be achieved
by the attacker. Recall that G is the set of goal literals, and rl is the reward to the
attacker for achieving a goal literal l ∈ G. We now augment this with a defender’s
corresponding reward rD
l (presumably, negative). The plan interdiction problem then
becomes

S⊆M ∑
max

l∈G

zl(π(S))rD

l − c(S),

where zl is a binary indicator of whether the goal literal l is achieved by the at-
tacker. Letchford and Vorobeychik [10] show that this bi-level optimization prob-

6

Yevgeniy Vorobeychik and Michael Pritchard

lem can be solved using a combination of mixed-integer linear programming and
constraint generation, where constraints represent possible attack plans, and are it-
eratively added by computing approximately optimal plans using state-of-the-art
heuristic planning software, such as SGPlan [5].

4.2 Interdicting MDPs

The power of deterministic planning and associated interdiction is that it involves
solving highly structured problems, and we can therefore develop a scalable ap-
proach for these. The limitation is generality: often, uncertainty in the planning
problem faced by the attacker must be appropriately captured.

We model the attacker’s planning problem when it faces uncertainty by an MDP,
described above. Speciﬁcally, we can address the MDP interdiction problem, which
can be deﬁned just as its deterministic counterpart above, but using the more general
utility function V D(S; π(S)). The interdiction problem can then be described very
generally by the Equation (1).

While it is possible to solve MDP interdiction problems using a similar approach
for bi-linear optimization as developed by Letchford and Vorobeychik [10], these
require that the state space be explicitly represented. Panda and Vorobeychik [13]
addressed this technical challenge by integrating state-of-the-art approaches for ap-
proximately solving FMDPs by Guestrin et al. [9] with a bi-level framework, and
appealing to Fourier representation of a value function over a Boolean space [12].

While the approach by Panda and Vorobeychik [13] enables a considerable ad-
vance in solving MDP interdiction problems over factored MDP representations,
it is still somewhat limited in scalability. Moreover, a major challenge in plan in-
terdiction approaches to date is that the defender is typically uncertain about the
attacker’s planning problem, such as the initial vulnerabilities and attacker’s access
and capabilities, and the resulting Bayesian Stackelberg game is infeasible for cur-
rent methods for even small problem instances [10].

In recent research, we have taken an alternative approach which integrates rein-
forcement learning with function approximation. Combined with a restriction that
the defender’s mitigations only modify initial state (which is without loss of general-
ity, since we can capture action removal by adding associated preconditions to initial
state), we can now learn a general value function for the attacker over the state space,
and then solve the interdiction problem once, without having to iteratively re-solve
the planning problem. Additionally, we can introduce uncertainty about the attacker
by capturing it as uncertainty over a subset of initial state variables. In the special
case when the value function is linear (or approximated by a linear function), we
can transform the Bayesian interdiction problem into a form which can be solved
using integer linear programming, a signiﬁcant advance in scalability compared to
prior art.

Plan Interdiction Games

7

4.3 Interdicting Partially Observable MDPs

MDPs model an attacker’s uncertainty about dynamics. However, they ignore an-
other crucial consideration: attacker’s uncertainty about the true state of the system.
Insofar as this introduces information asymmetry between the defender and attacker,
it introduces a potentially very rich space of interdiction options for the defender.
One issue that arises is signaling: if the defender knows aspects of the initial state
which are unknown to the attacker, the defender’s mitigations can signal to the at-
tacker information about the true initial state. This, in turn, provides an opportunity
for deception: the defender may, through particular (costly) mitigations they deploy
signal to an attacker that they have vulnerabilities they do not actually have, thereby
deceiving the attacker into expending resources into an attack which cannot suc-
ceed. In addition, the defender may also control the observations of the attacker
about system state, a capability which can provide further leverage.

As we can see, this partially observable Markov decision process (POMDP) in-
terdiction problem is exceptionally rich. It is also an open problem from a com-
putational perspective—we would argue, the next important open problem in plan
interdiction.

5 Illustration: Threat Risk Assessment Using Plan Interdiction

Having motivated the plan interdiction problem rather abstractly, we now illustrate
its value more concretely by using the planning framework for network cyber-risk
analysis and mitigation. Speciﬁcally, we ﬁrst deﬁne a model for cyber threat as-
sessment using classical planning primitives, augmenting these with stochastic in-
formation which represents uncertainty about attacker’s access, such as which user
accounts the attacker has already compromised. We then describe an implementa-
tion of this model, and demonstrate its value through experiments. Finally, we show
that by selectively patching a small set of vulnerabilities we can dramatically reduce
cyber risk.

5.1 Model

In creating our network threat assessment model, we deﬁne six components: a set
of hosts H, the system environment T , a dictionary of vulnerabilities V , the target
ﬁle F, and the attacker A with action set S. Next, we formally describe these com-
ponents. For illustration, we assume that the attacker aims to exﬁltrate a particular
ﬁle, which may represent sensitive information such as trade secrets.

Each host in our model represents a device within the target environment, with no
inherent distinction made among personal computers, servers, mobile devices, etc.
Formally, we deﬁne a host hi ∈ H as possessing three attributes: a set of exploitable

8

Yevgeniy Vorobeychik and Michael Pritchard

vulnerabilities Vi ⊂ V , the set of neighboring hosts Ni ⊂ H, and a ﬂag Fi ∈ {0, 1},
which indicates if the target ﬁle is present on the host. Vulnerabilities are assigned
to a host stochastically, reﬂecting differences in particular host conﬁgurations, non-
deterministic exploitation outcomes, and the relative complexity of executing the
exploit. The set of neighbors Ni of a host hi is the set of all hosts which can poten-
tially be accessed from hi.

The system environment variable T contains the state of each host ti. The state is
represented by a binary vector, and contains information pertaining to the status of
the host relative to the attacker. For example, whether the attacker has read or write
access to ﬁles on the host could be a pair of binary values in the state vector.

Vulnerabilities in the dictionary V are deﬁned by two vectors R j, E j ∈ RE . The
ﬁrst of these, R j, is the exploitation requirement vector: the minimum required state
of the host (relative to the system environment vector for the host) for successful
exploitation. One example of a state requirement would be a vulnerability which
requires authentication with user credentials. After successful exploitation of a vul-
nerability, the state of the host is updated according to the exploitation effects vector,
E j.

The target ﬁle F is simply a binary valued variable that is initially set to 0. If the

ﬁle is accessed by an attacker, it is updated to 1.

The attacker A possesses three attributes. The ﬁrst of these is a set of hosts C ⊂ H
for which the attacker possesses user credentials (obtained, for example, through a
phishing attack). The other two attributes are also sets of hosts, Ha ⊂ H and Hc ⊂ H,
which are those hosts which are accessible to the attacker and have been compro-
mised by the attacker, respectively. In addition to the aforementioned attributes, the
attacker has an abstract set of knowledge and tools to assess information about the
system environment (e.g. connectivity between hosts). Further, the attacker is as-
sumed to be able to exploit any vulnerability present on an accessible host.

The action set of the attacker, S, contains ﬁve categories of deterministic ac-
tions encapsulating reconnaissance, exploitation, and data access/exﬁltration. These
action categories are summarized formally in Table 1. The ﬁrst two categories of ac-
tions, Exploration and Probing, deal with efforts by the attacker to learn information
about hosts and the network. Exploration targets a single host hi (which must in the
set of compromised hosts) and results in the union of Ni, the host’s neighborhood,
with Ha, the set of accessible hosts. Probing examines an accessible host and checks
for the presence of particular vulnerabilities.

Preconditions
hi ∈ Hc
hi ∈ Ha
hi ∈ Ha ∧ hi ∈ C

Action
Explore(hi)
Probe(hi,V )
Masquerade(hi)
hi ∈ Ha ∧ (R j − (R j ∧ ti) = 0) Exploit(hi, v j)

hi ∈ Hc ∧ ri = 1

Access(hi)

Table 1 Categories of attacker actions

Effects

Ha = Ha

(cid:83){Ni}

{v j ∈ V |R j − (R j ∧ ti) = 0}
ri, wi = 1
ti = ti ∨ E j
F = 1

Plan Interdiction Games

9

The next two categories of actions pertain to malicious behavior executed by
the attacker. Masquerading, the ﬁrst of these actions, is the act of accessing a host
through normal means (e.g. SSH) with the use of externally acquired user creden-
tials. Naturally, this requires both that the attacker possesses suitable credentials
and access to the host. The result of Masquerading is attainment of read and write
privileges (denoted ri and wi for host hi) to the target host. Exploitation, the sec-
ond malicious action available to the attacker, targets a particular accessible host hi
with a vulnerability v j. Satisfaction of vulnerability requirements R j are necessary
for execution of this action, and the effects of exploitation are according to E j. The
ﬁnal action available to the attacker is Access. This action targets a particular com-
promised host hi and checks for the presence of the target ﬁle. If the ﬁle is present
and the state of the host permits, the attacker accesses the ﬁle. Successful execution
of this action is the goal for the attacker, and indicates that the system has been
compromised successfully.

5.2 Implementing the Model

In this section we describe our implementation of the model given in Section 5.1.

5.2.1 Vulnerability Dictionary

To populate our dictionary of vulnerabilities, we turn to the National Vulnerability
Database (NVD). Originally created in 2000 by the NIST Computer Security Divi-
sion, the NVD is an extension of MITRE’s Common Vulnerability and Exposures
(CVE) dictionary that includes additional vulnerability assessment metrics. One of
these metrics, the Common Vulnerability Scoring System (CVSS), is of particular
interest to us.

Designed to communicate the characteristics and impact of vulnerabilities, the
CVSS provides both quantitative scores and vectors of characteristics. The former
indicates the holistic “severity” of the vulnerability, while the latter gives us some
indication of the conditions necessary for and the results of successful exploita-
tion. Three sets of metrics comprise the CVSS: base, temporal, and environmental
metrics. Of these we only use the base metrics, which cover intrinsic qualities of a
vulnerability. Base metrics are divided into exploitability metrics and impact met-
rics. The former captures information concerning vulnerability accessibility and has
three parameters: attack vector (AV), access complexity (AC), and authentication
(Au). Impact metrics gauge the potential effects of successful exploitation, divided
into conﬁdentiality, integrity, and availability impacts. As detailed in the next sec-
tion, each of these parameters contributes to our vulnerability model.

10

Yevgeniy Vorobeychik and Michael Pritchard

5.2.2 Vulnerability Proﬁles

Having parsed the NVD to generate a vulnerability dictionary, we next construct
proﬁles containing a subset of these vulnerabilities. To this end, we utilize the pen-
etration testing suite Nessus. Speciﬁcally, we construct proﬁles by ﬁltering Nessus
vulnerability reports to recover CVE entry IDs, then populate the entries from our
dictionary.

Based on CVSS information from the dictionary, we generate the set of exploita-
tion requirements for each vulnerability and derive a value denoting the exploitation
probability. Two base metric parameters are retained for later use: attack vector and
authentication. The attack vector can take on one of three values, each of which in-
troduces different preconditions for successful exploitation in our model. The most
restrictive value, Local, requires an attacker to have either direct physical access to
the machine or access to a shell account. A corresponding parameter for each host
determines whether this level of access is present. Adjacent Network access, the
second AV value, necessitates some kind of access to either the broadcast or colli-
sion domain of the host. In our model, this access is represented by directed edges
between an already compromised system and the target system. Finally, Network
access indicates that the vulnerability is remotely exploitable. Satisfying Adjacent
Access or the corresponding network access parameter are required for exploitation
in this case.

Authentication requirements can also take on one of three values: None, Single,
and Multiple. As the latter two values indicate user credentials must be obtained,
they introduce an additional precondition for exploitation of the vulnerability.

Exploitation probability is derived from a combination of base metrics and their
corresponding weights in the CVSS severity calculation. The initial value of 1.0 is
ﬁrst multiplied by the scoring value associated with access complexity. This attribute
captures the relative level of exploitation difﬁculty due to the presence of special-
ized access conditions (e.g. non-default software conﬁgurations). Low complexity,
indicative of no specialized access conditions, has an associated value of 0.71. A
value of 0.61 is used for medium complexity – so denoted when ”somewhat” spe-
cialized access conditions are present. High complexity carries an associated value
of 0.35. In the case that multiple authentications are required for exploitation of the
vulnerability, the probability is multiplied by an additional factor of 0.8. Finally, we
account for the non-deterministic quality of access granted by partial conﬁdentiality
impact with a multiplied factor of 0.5. Note that this value is not derived directly
from the CVSS formula.

Phishing Attack Probability

Recognizing that attackers are not limited to exploiting software vulnerabilities, we
include a phishing attack probability parameter PP in our framework. Despite the
name, the parameter is meant to encapsulate all social engineering vectors which
result in the recovery of a user’s credentials. In selecting a default value of 0.03, we

Plan Interdiction Games

11

examined the spear-phishing rate for small businesses (<250 employees) and the
overall phishing rate from the 2016 Symantec Internet Security Threat Report.

In precisely deﬁning the utilization of PP in our model, we say that it is the
probability a given host has one or more privileged user accounts whose credentials
have been compromised. In so deﬁning the parameter, we avoid the need to generate
a set of user credentials and associate them in some manner with the hosts.

Zero-Day Attack Probability

In addition to published vulnerabilities and social engineering vectors, we consider
that the attacker may have knowledge of one or more zero-day vulnerabilities which
affect hosts. Since the NVD serves as our repository of vulnerabilities, we utilize it
to determine our default probability of zero-day attacks. Speciﬁcally, we examined
all of the CVE entries between 2002 and 2016 and compared the labeled year of the
vulnerability (i.e. the year included in the CVE name) with the published date. If the
two differed, the vulnerability was considered to have been present and unknown
sufﬁciently long to qualify as a viable zero-day attack. On average across the span
of years mentioned, approximately 13% of vulnerabilities meet this criterion.

5.2.3 Host Generation

In general, a host in our model can be any device connected to the network; however,
for the initial implementation of the framework we limit ourselves to consideration
of servers and personal computers. Hosts are deﬁned fundamentally by a set of three
attributes: the vulnerability proﬁle, a list of neighboring hosts, and a set of access
levels. A unique host name as well as a boolean value which indicates whether the
host serves as a gateway are also included. The gateway indicator is used in our
generative network model discussed in Section 5.3.2.

Upon initialization of a host object, the set of vulnerabilities in the proﬁle are
iterated through, with each vulnerability being retained or discarded stochastically
according to the associated probability of exploitation. This results in each host
containing a subset of the full proﬁle and reﬂects differences in host vulnerability
due to varying software conﬁgurations.

Neighboring hosts in our model are any hosts which satisfy the properties of
adjacent access (as deﬁned in the CVSS). Viewing a particular instance of the model
as a graph, each neighbor possesses an incoming directed edge.

The access levels list contains information on host properties related to exploit
preconditions. In our initial implementation this consists of three values: network,
root, and user access. The ﬁrst indicates that the attacker can execute remote exploits
on the host. Root access implies privilege escalation for the attacker on the host has
been successfully carried out. Finally, user access indicates that the attacker pos-
sesses either user credentials for an account on the host or the means of equivalently
authenticating.

12

Yevgeniy Vorobeychik and Michael Pritchard

5.2.4 Generation of PDDL

The planning domain and problem description for a particular instance are generated
sequentially based on instances of the model described in Section 5.1. First the
domain types and constants are produced. The former consists of three entries: hosts,
vulnerabilities, and ﬁles. The numbered list of hosts and a single generic ﬁle are
produced as constants.

Finally, the three actions are constructed. The ﬁrst of these, the exploit action,
generalizes the impact of exploiting present vulnerabilities with the intent of gaining
read access to the host (to check for the desired ﬁle) and, upon failure to locate the
ﬁle, compromise a neighbor. The base precondition for the action is the presence of
a particular vulnerability on the host. Recall that this presence is probabilistic based
on the characteristics of the vulnerability and, in the case of heterogeneous host
proﬁles, the vulnerability proﬁle itself. Further, we introduce two additional sets of
preconditions that can be added in association with speciﬁc vulnerabilities. The ﬁrst
is a requirement of user access, and appears in the event that authentication (single
or multiple) is required for exploitation of the vulnerability. The second pertains
to the required access vector between the attacker and the host: local, adjacent, or
network access. Execution of this action grants read access to the attacker and marks
the host as compromised. For each vulnerability that appears on at least one host, an
instance of this action is added to the domain.

Figure 3 depicts a sample plan and the effects of each action for a small example
network. In the ﬁgure, the red arrow indicates that H1 is remotely accessible, and the
underlined text for H3 indicates that the target ﬁle is present on the host. Initially, the
attacker can only probe for vulnerabilities on H1. Finding a suitable vulnerability,
the attacker next compromises H1 by exploiting the ﬂaw. Having penetrated the
network, the attacker can explore to discover the connections between H1 and the
other hosts. As before, the attacker then probes for and then exploits a vulnerability,
this time on H3. Finally, having gained sufﬁcient privileges on the target host, the
attacker accesses the target ﬁle, indicating the goal condition has been met.

Once a host has been compromised, the number of types of access vectors avail-
able to the attacker must be updated accordingly. To this end, we introduced an
artiﬁcial action, update access, which has two preconditions: a connection between
two hosts, and that one of the hosts has been previously compromised. Note that
connections between hosts are directed. Once these conditions have been met, the
neighboring host is updated to reﬂect that the attacker now has both network and
adjacent access vectors.

The third action fulﬁlls the goal condition of the plan, namely, reading the target
ﬁle. Execution of the Access action requires that the attacker have read access to the
host which contains the target ﬁle. Taking this action indicates that the ﬁle has been
read successfully, which satisﬁes the goal conditions of the plan.

Initial generation of the problem instance follows a straightforward process. First,
the target ﬁle is randomly assigned to one of the hosts. Next, each host is examined
and corresponding predicates are added to the problem deﬁnition. An instance of the
connected predicate is generated for each neighbor (i.e. a host that is accessible from

Plan Interdiction Games

13

Fig. 3 An attack planning example on a small network.

this host) based on the state of neighboring hosts. Instances of has vuln are taken
from the speciﬁc vulnerability proﬁle of the host. Finally, the presence of initial
network, user, or root access are added accordingly. As the ﬁnal step of problem
instance generation, the goal condition of accessing the target ﬁle is added.

14

Yevgeniy Vorobeychik and Michael Pritchard

5.3 Experimental Methods

We illustrate our model of cybersecurity risk assessment using a novel experimental
framework which combines a generative model of a network with stochastic gener-
ation of hosts based on host images.

5.3.1 Generating Vulnerability Proﬁles

For our experiments we carefully crafted four vulnerability proﬁles. Two of these
proﬁles represent generic desktop system conﬁgurations, with the remaining two
representing server conﬁgurations. The operating systems for these proﬁles were
chosen based on compiled OS market share data [1, 2], and software conﬁgurations
were informed by application use data from WhatPulse [3], a computing habit in-
formation aggregator with more than 285,000 active users.

The ﬁrst desktop conﬁguration was based on a generic Windows desktop setup.
As Windows 7 holds the largest share amongst Windows distributions, it was chosen
for use with this proﬁle. In addition to the standard software packages installed with
Windows 7, the following pieces of third party software were installed:

• Chrome
• Firefox
• Skype
• Java Runtime Environment (JRE)
• Adobe Reader
• Adobe Flash Player

Though Mac OS X holds the second highest market share after Windows for
desktops, test images were not readily available for use. This being the case, the sec-
ond desktop proﬁle was constructed based on Ubuntu Linux version 14.04 (Trusty
Tahr).

For server proﬁles, we again provided both a Windows and Linux variants. The
Windows server is a stock image of Windows Server 2012, while the Linux server
is again Ubuntu 14.04, but with the addition of software necessary to run an Apache
web server.

5.3.2 Generating Network Architecture

We implemented two generative models for constructing random network topolo-
gies in the experiments we report on below. The ﬁrst is the well-known random
graph (Erdos-Renyi) model, while the second is a novel generative model aimed at
capturing aspects of networks salient in threat modeling.

Plan Interdiction Games

Erdos-Renyi

15

The ﬁrst model implemented was the Erdos-Renyi random graph model [6]. Specif-
ically, we utilize the G(n, p) formulation of the model in which a random graph G is
constructed by randomly connecting vertices. For every pair of vertices v1, v2 ∈ G,
an edge e is added between them with probability p, independently of any other
edge.

In addition to specifying arbitrary values of p, we can utilize relationships and
established thresholds between n and p to construct graphs with known, potentially
interesting characteristics. An example of a known threshold in Erdos-Renyi graphs
is p = ln(n)/n. This value represents a sharp connectedness threshold – larger values
of p will almost certainly be connected, while smaller values indicate the almost
sure certain presence of isolated vertices in the graph. Several of these thresholds
are employed in the experimental evaluations detailed in 5.4.1.

Organizational Network Generative Model

The Erdos-Renyi model offers clean insights, but is known to be overly simplistic to
realistic topological characteristics of real networks. Moreover, in modeling security
threats on networks one must also make a distinction between types of hosts (such
as servers and desktops), as that pertains to the particular OS that is likely to run
on such systems (for example, servers will typically not run Microsoft Word, and
would not be susceptible to vulnerabilities in this application). To this end, we use a
more realistic Block Two-Level Erdos-Renyi (BTER) model proposed in [16], and
extend it with host-level characteristics.

Unlike the original Erdos-Renyi model, which treats all vertices identically with
regard to edge construction, BTER treats the overall graph structure as a set of in-
terconnected communities, each of which is an Erdos-Renyi random graph. The
ﬁrst phase of BTER graph construction consists of generating a collection of Erdos-
Renyi (ER) blocks according to a user-speciﬁed degree distribution. These blocks
are interconnected in the second phase via nodes in each block that have access
degree. The Chung-Lu graph model [4] (which can be framed as a weighted Erdos-
Renyi graph) is employed over the excess degrees to form the connections among
the blocks.

Our model takes as input a desired node count n, a power law exponent α, a
remote access probability PN, and an optional set of connectivity policies ρ. Con-
struction of a network with our model then follows four primary steps, the expla-
nation of which will be aided by Figure 4. We ﬁrst sample n discrete values from
a power law sequence with exponent α. In the BTER construction process, these
values represent the desired degree of the nodes, and deﬁne the size of the commu-
nities present within the graph. The sampled values are sorted in ascending order
and grouped by value, as seen in panel (a) of Figure 4. Each entry in the sample
list with value 1 represents a device which does not belong to a restricted subnet.
As such, we don’t consider them when building communities. Starting with the ﬁrst

16

Yevgeniy Vorobeychik and Michael Pritchard

value greater than 1, groups are formed by selecting and removing the ﬁrst d + 1
entries from the list, where d is the value of the ﬁrst entry. This procedure continues
until the list is empty, at which point all of the groups have been formed. The end
result is displayed in panel (b) of Figure 4: a set of individual nodes and one or more
communities of nodes.

Fig. 4 Generative Model Network Construction

Device classiﬁcation occurs next, with two sets of distinctions made among the
devices. First, one node in each community is chosen arbitrarily to serve as the
gateway node. Next, each individual node (i.e. those not in communities) is ﬂagged
as remotely accessible with probability PN; those ﬂagged are distinguished as server
nodes. In panel (c) of Figure 4, node 6 is a gateway and node 3 is a server.

Finally, edges are drawn between the nodes as shown in panel (d). By default, all
individual nodes are bi-directionally adjacent to each other and the set of gateway
nodes. Additionally, each community forms a complete subgraph. As shown later
in Section 5.4.2, the policies deﬁned in ρ modify the manner in which edges are
added.

Plan Interdiction Games

17

5.4 Experimental Study of Network Cyber Risk

We now present an experimental study of network cyber risk using the experimental
setup in Section 5.3 which instantiates our planning-based threat analysis frame-
work.

5.4.1 Erdos-Renyi Network Model

As an initial set of experiments, we employ the Erdos-Renyi random graph model
with directed edges to generate our network topology. As mentioned in Section
5.3.2, graphs generated using the Erdos-Renyi model possess well-known charac-
teristics based on the relationship between the number of hosts n and the edge prob-
ability p. We examine thresholds corresponding to two of these relationships:

1. np = 1
2. p = ln(n)
n

The ﬁrst of these, which we will henceforth refer to as NP1, produces graphs
2
which almost always contain a maximum component of size n
3 . The second re-
lationship, referred to hereafter as LNN, is the threshold for connectedness in the
model [6]. Since the Erdos-Renyi model does not lend itself to differentiation of
vertices based on topological characteristics, we generate hosts homogeneously us-
ing the Windows desktop vulnerability proﬁle detailed in Section 5.3.1.

Fig. 5 Cyber risk for Erdos-Renyi Topologies, n = 100.

18

Yevgeniy Vorobeychik and Michael Pritchard

Figure 5 shows the effect of varying the host network access probability PN for
a range of values between 0.01 and 0.20. As we might expect, the behavior for
the two different values of p differ signiﬁcantly. For the NP1 case, the proportion
of successful attacks PS is much smaller, but increases roughly linearly with PN
(indeed, across the range of values we considered, PS ≈ PN in this case). We can
explain this behavior by examining the structure of the generated networks. Since
the attacker does not have the option of physically accessing devices, attack vectors
must originate with devices that are remotely accessible. Sparsity of edges, their
directed nature, and the inherently disconnected structure of the generated networks
limits the ability of the attacker to penetrate further into the network following the
compromise of an externally accessible device. Further, since zero-day attacks and
utilization of phished user credentials require network access to vulnerable devices,
they are of relatively little use for this topology.

With the LNN case, the effect of varying PN is completely different. Following
a sharp increase in PS across PN values between 0.01 and 0.05, the proportion of
successful attacks reaches a steady state. Given the guarantee of connectedness in-
herent to the LNN case, any remotely-accessible device will have a path to the target
device; however, no guarantees are made that any two remotely-accessible devices
will have independent paths to the target. The leveling-off of PS suggests that inde-
pendent paths to the target device are saturated beyond PN = 0.05.

5.4.2 Organizational Network Generative Model

As a follow-up to the initial experiments run with the Erdos-Renyi model, simi-
lar evaluations with the generative model proposed in Section 5.3.2 are executed.
Except where otherwise noted, default values for PP and the zero-day attack prob-
ability PZ are used (0.03 and 0.13, respectively). For the generative model, we use
directed edges and also differentiate vulnerability proﬁles across the hosts. Gate-
way and server nodes (remotely accessible nodes not in a community) are assigned
either the Linux or Windows server proﬁles with probabilities 0.664 and 0.336,
respectively. All other nodes are assigned either the Windows or Linux desktop pro-
ﬁles with probability 0.7048 and 0.2952. These values represent the proportional
ratio of Windows to Linux according to [1]. In addition to varying the network ac-
cess probability, α value, and host count, we also examine the impact of several
network policies and the effect of increased phishing attack probabilities. For the
baseline version of the generative model, phishing attacks have an additional effect:
if at least one host has compromised credentials, then the attacker is able to utilize a
VPN connection, resulting in servers, gateways, and non-community desktop nodes
being remotely accessible.

Figure 6 demonstrates the impact of varying network size for the baseline organi-
zational network generative model. For small values of PN, there are signiﬁcant dif-
ferences between PS across smaller network sizes. These differences become smaller
as the largest network sizes are reached. Moreover, risk increases with the number
of hosts. The intuition behind these observations is that for smaller networks we

Plan Interdiction Games

19

Fig. 6 Cyber risk for the organizational network generative model with varying host counts, no
connection restrictions. α = 2.5

expect that phishing attacks are less likely (a lower probability that some user ac-
count is phished), which limits attack vectors to remotely accessible nodes. Larger
networks have more remotely accessible hosts, and a higher likelihood that some
credentials are compromised through a phishing attack. Moreover, once even a sin-
gle credential is phished, the attacker can remotely access a much larger portion of
the network, signiﬁcantly increasing the threat of a successful cyber attack. Conse-
quently, increasing network size increases overall risk in this model.

We next examine the effect of introducing two network connection policies. The
ﬁrst of these is a restriction on outgoing connections from gateway nodes. If en-
abled, gateways retain outgoing connections only to nodes within their subnet, and
all other outgoing connections are removed; incoming connections are not mod-
iﬁed. The second policy restricts all outgoing connections from server nodes. In
other words, if the second policy is in place, one cannot connect (e.g., SSH) from
a server to any other device on the local network. Figure 7 displays each of the
combinations of these two policies. On the legend, the ﬁrst letter denotes gateway
restrictions (True/False), and the second denotes server restrictions. We can see that
either of these policies by itself does not appreciably improve the security of the
network compared to the baseline. Surprisingly, however, combining them results
in an average reduction of 0.15 in risk, PS. Note that because of the attacker’s ability
to employ VPN connections, there is little sensitivity to changes in PN.

Next, we study the risk associated with allowing VPN connectivity. As seen in
Figure 8, disabling VPN connections to the network has a signiﬁcant impact on
overall cyber risk. For the unrestricted model across different values of α, we see
high initial sensitivity to changes in PN. As the attacker can no longer bypass attack-

20

Yevgeniy Vorobeychik and Michael Pritchard

Fig. 7 Cyber risk for the organizational network generative model with varying connection re-
strictions, n = 100. On the legend, the ﬁrst letter denotes gateway restrictions (True/False), and the
second denotes server restrictions

Fig. 8 Cyber risk for the organizational network generative model with no VPN. On the legend,
the number denotes the value of α, ’FF’ denotes no access restrictions, and ’TT’ denotes restricted
gateways and servers.

Plan Interdiction Games

21

ing remotely accessible servers, the expected number of such servers in the network
becomes quite relevant. For a network of 100 nodes, PN = 0.08 marks the beginning
of the steady state for PS. Restricting both gateways and servers results in extremely
small values of PS, and retains only minimal sensitivity to increases in PN.

Fig. 9 Cyber risk for the organizational network generative model with varying phishing attack
probabilities. α = 2.5

In assessing the impact of increased phishing attacks on the system, we varied
PP from 0.01 to 0.20 in increments of 0.01 while keeping PN at 0.01. The results of
this experiment are shown in Figure 9. As we might expect, both the initial values
of PS and the sensitivity to increases in PP are markedly higher in larger networks.
We’ve seen that the ability to bypass attack bottlenecks on remotely accessible hosts
by utilizing VPN connections has a profound impact on security, and these results
reinforce this observation. Even the network of 25 hosts approaches the same level
of vulnerability as the 150 host network when PP becomes large.

5.5 Cyber Risk Mitigation through Plan Interdiction

Our discussion of the risk assessment approach, which leverages AI planning, thus
far focused on the attacker modeling. Although experiments demonstrated the value
of speciﬁc policy changes in mitigating some cyber risk, we only considered a small
set of such policies. We now explore systematically a common challenge faced by
organizations: of the many vulnerabilities present on the network, which should be
prioritized. While in an ideal world, all vulnerabilities are patched, in reality do-

22

Yevgeniy Vorobeychik and Michael Pritchard

ing so can be both time consuming, and costly in terms of lost productivity. Con-
sequently, in practice a natural question is whether an organization can focus on a
small subset of vulnerabilities which are truly critical for this organization, and make
these a high priority. Given that our risk analysis framework is based on attack plan-
ning, the problem of choosing an optimal subset of vulnerabilities to prioritize is
precisely the plan interdiction problem.

To formalize, let V be the set of all vulnerabilities present on a network, and let
S ⊆ V be a subset of these which the defender (network administrator) will prioritize.
Rather than imposing a cost on patching a given vulnerabilty and minimizing risk, as
in our model above, we minimized the number of vulnerabilities that are prioritized,
subject to a constraint that risk is below a target threshold θ .

Let R(S) be the risk (probability of a successful attack) faced by the defender
when the set S of vulnerabilities is chosen to be prioritized for patching. Implicitly,
R(S) depends on the attacker’s policy in response to S. However, note that in our
framework, the attacker’s policy is also a function of additional exogenously spec-
iﬁed factors, such as the assignment of (exploitable) vulnerabilities to hosts, and
phishing attack success, among others. Let η be a random variable which captures
these stochastic factors, and let π(S, η) be the associated optimal plan for the at-
tacker. If we deﬁne r(S, π(S, η)) as a binary variable indicating whether the attack
succeeds under exogenous parameters η, risk becomes R(S) = Eη [r(S, π(S, η); η)].
The defender’s optimization problem is then

|S|

min
S⊆V

s.t. : R(S) ≤ θ .

(2)

We propose a heuristic solution approach for this problem: greedily add a vulnera-
bility v ∈ V to S in the order of maximum marginal impact on reducing risk R(S),
until we satisfy the condition R(S) ≤ θ .

Next, we evaluate the impact that selective patching of vulnerabilities has on cy-
ber risk. For these evaluations, we set PN = 0.01, and used default values for PP
(probability of a successful phishing attack) and PZ (zero-day probability). Addi-
tionally, α was ﬁxed at 2.5. We used the organizational network generative model
to generate the network topologies, along with the heterogeneous vulnerability pro-
ﬁles and varied host count to assess the impact of vulnerability patching for different
network sizes, as in the earlier experiments. We set the threshold in the optimization
problem (2) to θ = 0.05.

Figure 10 shows the results of our heuristic prioritization approach as a function
of the number of vulnerabilities being prioritized (until the threshold of θ = 0.05
is reached). As we might expect, the larger networks display higher initial vulnera-
bility; however, the response of each network to the removal of vulnerabilities was
very similar in magnitude. Most of the vulnerabilities patched came from the Win-
dows Desktop vulnerability proﬁle, which contributed 81 of the 104 vulnerabilities
(after ﬁltering them by year) among all proﬁles. These initially patched vulnera-
bilities were consistent in that they possessed a higher than average probability of
being exploitable on any given host with the proﬁle. Remarkably, however, we can

Plan Interdiction Games

23

Fig. 10 Threat minimization through vulnerability prioritization. On the legend, the value of n
denotes the number of hosts in the network.

dramatically reduce cyber attack risk in this model by patching relatively few vul-
nerabilities (< 10), as long as we effectively prioritize these.

6 Dynamic Plan Interdiction Games

Thus far, our discussion (and most prior research) had focused on interdiction games
in which the defender ﬁrst makes a ﬁxed decision, and then the attacker devises a
dynamic plan, with the possibility of circumventing defender’s mitigations. This is
most relevant when the defender cannot easily observe adversarial actions as they
unfold, and only detects attacks after they have either succeeded or failed. In the
remainder of this chapter we brieﬂy discuss ideas for modeling the more general
situation in which the defender may detect portions of the attack plan, and can sub-
sequently react to it, for example, deploying additional mitigations in response (in-
cluding modiﬁcations to the observed state).

Let’s start with the following scenario: the defender has deployed mitigations,
and a detection system. Let’s view the detection system as predicting whether or
not a particular observed state x involves malicious activity, captured by a binary
classiﬁer f (x). Suppose that the detectors are binary, and we can immediately deploy
response mitigations that prevent a detected attack from succeeding. The problem
of designing such detectors can be viewed as a “basic” plan interdiction problem:
we aim to design detectors which minimize loss from undetected attacks, where
mitigation (detector design) costs can capture the costs of false positives.

24

Yevgeniy Vorobeychik and Michael Pritchard

An interesting challenge arises when detectors may detect an attack, but optimal
mitigations may be unclear. This arises when the defender is uncertain about the
attacker—for example, about the attacker’s goals. In this case, the defender may
have discovered a subplan (a partial sequence of attack actions), but is uncertain
about the full plan. This gives rise to an interesting problem which couples plan in-
terdiction with plan recognition: using partial information about the observed plan
to infer information about the attacker’s planning problem, so that mitigations can
be optimally deployed. If the attacker is myopic, this can in principle be addressed
through Bayesian updating: the defender may have a prior over the attacker’s plan-
ning problem aspects (such as the attacker’s capabilities and access), and then ob-
servations of an actual attack allows them to infer a posterior distribution over these
(potentially resolving much of the uncertainty). However, an intelligent attacker may
devise plans so as to make such inference difﬁcult, and faces an interesting tradeoff
between deceiving the defender, and achieving their goals as efﬁciently as possible.
We view the challenge of addressing such dynamic problems at a sufﬁciently high
resolution to obtain practically meaningful results as a major open problem in plan
interdiction games.

7 Conclusion

We introduced the general notion of plan interdiction games. In their most basic
variation, the defender commits to a collection of mitigations, and the attacker sub-
sequently chooses an optimal attack plan. We illustrate the value of this modeling
framework by using it to develop a network cyber threat assessment approach in
which network features, as well as vulnerability proﬁles from standard vulnerability
datasets, are used to compute a risk of successful attacks. Our experiments offered
several interesting insights into the nature of risk associated with different design
choices, such as speciﬁc choices about network connectivity and the use of VPN,
as well as different exogenous factors, such as susceptibility to phishing attacks.
Moreover, we demonstrated an important use case of plan interdiction as a means to
prioritize which vulnerabilities are immediately patched, and showed that prioritiz-
ing only a few vulnerabilities can have a signiﬁcant impact on reducing exposure to
cyber risk.

While we have made considerable progress modeling the basic plan interdiction
problem, as well as advancing technical state of the art to signiﬁcantly improve
scalability and generality of solution approaches, a number of interesting concep-
tual and technical issues arise when uncertainty is involved either on the part of
the attacker about system state, or on the part of the defender about the attacker’s
planning problem primitives. Dynamics of the problem, in addition to the different
aspects of information asymmetry, allow us to consider a very rich space of issues of
great relevance in cybersecurity, including adaptive defense and deception both on
the part of the defender, and on the part of the attacker. A great deal more research is

Plan Interdiction Games

25

needed to fully understand the space of plan interdiction problems, develop scalable
solutions, and apply these in practical cybersecurity settings.

References

1. Desktop operating system market share. URL https://www.netmarketshare.com/
2. Developer survey results 2016. URL https://insights.stackoverflow.com/

survey/2016#technology-development-environments

3. Whatpulse: Most used applications. URL https://whatpulse.org/stats/apps/
4. Aiello, W., Chung, F., Lu, L.: A random graph model for power law graphs. Experimental

Mathematics 10(1), 53–66 (2001). URL http://eudml.org/doc/227051

5. Chen, Y., Wah, B.W., wei Hsu, C.: Temporal planning using subgoal partitioning and resolu-

tion in SGPlan. Journal of Artiﬁcial Intelligence Research 26, 323–369 (2006)

6. Erdos, P., R´enyi, A.: On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci

5(1), 17–60 (1960)

7. Filar, J., Vrieze, K.: Competitive Markov Decision Processes. Springer-Verlag (1997)
8. Ghare, P., Montgomery, D., Turner, W.: Optimal interdiction policy for a ﬂow network. Naval

Research Logistics Quarterly 18(1), 37–45 (1971)

9. Guestrin, C., Koller, D., Parr, R., Venkataraman, S.: Efﬁcient solution algorithms for factored

mdps. Journal of Artiﬁcial Intelligence Research 19, 399–468 (2003)

10. Letchford, J., Vorobeychik, Y.: Optimal interdiction of attack plans. In: International Confer-

ence on Autonomous Agents and Multiagent Systems, pp. 199–206 (2013)

11. McMasters, A., Mustin, T.: Optimal interdiction of a supply network. Naval Research Logis-

tics Quarterly 17(3), 261–268 (1970)

12. O’Donnell, R.: Some topics in analysis of boolean functions. In: Proceedings of the fortieth

annual ACM symposium on Theory of computing, pp. 569–578. ACM (2008)

13. Panda, S., Vorobeychik, Y.: Near-optimal interdiction of factored mdps. In: Conference on

Uncertainty in Artiﬁcial Intelligence (2017)

14. Puterman, M.L.: Markov Decision Processes: Discrete Stochastic Dynamic Programming.

John Wiley & Sons, Inc. (1994)

15. Salmeron, J., Wood, K., Baldrick, R.: Worst-case interdiction analysis of large-scale electric

power grids. IEEE Transactions on Power Systems 24(1), 96–104 (2009)

16. Seshadhri, C., Kolda, T.G., Pinar, A.: Community structure and scale-free collections of erd˝os-

r´enyi graphs. Physical Review E 85(5), 056,109 (2012)

