1
2
0
2

p
e
S
7

]

R
C
.
s
c
[

1
v
1
3
3
3
0
.
9
0
1
2
:
v
i
X
r
a

IJCAI-21 1st International Workshop on Adaptive Cyber Defense

CyGIL: A Cyber Gym for Training Autonomous Agents over Emulated Network
Systems

Li Li1∗ , Raed Fayad2 , Adrian Taylor 1
1Defence Research and Development Canada
2Dept. of Electrical and Computer Engineering, Queens University, Canada
li.li2@ecn.forces.gc.ca, raed.fayad@queensu.ca, adrian.taylor@ecn.forces.gc.ca

Abstract

Given the success of reinforcement learning (RL)
in various domains, it is promising to explore the
application of its methods to the development of
intelligent and autonomous cyber agents. Enabling
this development requires a representative RL train-
ing environment. To that end, this work presents
CyGIL: an experimental testbed of an emulated
RL training environment for network cyber opera-
tions. CyGIL uses a stateless environment architec-
ture and incorporates the MITRE ATT&CK frame-
work to establish a high ﬁdelity training environ-
ment, while presenting a sufﬁciently abstracted in-
Its comprehensive
terface to enable RL training.
action space and ﬂexible game design allow the
agent training to focus on particular advanced per-
sistent threat (APT) proﬁles, and to incorporate a
broad range of potential threats and vulnerabilities.
By striking a balance between ﬁdelity and simplic-
ity, it aims to leverage state of the art RL algorithms
for application to real-world cyber defence.

1 Introduction
To strengthen cybersecurity of a networked system, red team
exercises are often employed to validate its defensive posture
by launching various attacks. Effective red team exercises
may incorporate adversary proﬁles to simulate the most rele-
vant Advanced Persistent Threats (APTs). In certain cases it
may also be desirable to test defences against a broad cover-
age of Tactics, Techniques, and Procedures (TTPs) from var-
ious advanced adversaries.

Red team exercises have signiﬁcant costs, in both the de-
velopment of human expertise and in the time required to
conduct them. To improve the efﬁciency of red teaming,
tools automating red team exercises such as emulators have
grown in popularity, including capabilities like staging frame-
works, enabling scripts, and execution payloads [MITRE
Corp., 2021a; Guardicore Inc., 2021; Red Canary Co., 2021;
Chris Ross, 2018]. These red team automation tools fa-
cilitate attack exercises. When using these tools, planning

∗Contact Author

and decision-making, including organizing TTPs through the
stages of the campaign, rely still on the human experts.

The recent rapid advancement of artiﬁcial intelligence (AI)
[Mnih et al., 2013; Mnih et al., 2015; Isele et al., 2018]
brings about the prospect of AI-assisted or even autonomous
AI red teaming. Superior dynamic decision-making capabil-
ity developed through AI learning and training may achieve
emerging attack Course Of Actions (COAs) across complex
networked cyber systems that are not yet anticipated or de-
In this approach,
veloped by the human red team experts.
the red agents are trained using Reinforcement Learning (RL)
and particularly, Deep RL (DRL) algorithms to learn, iden-
tify and optimize attack operations in the network. The same
analogy applies to the blue agents that may learn and optimize
a defence strategy against the red agents. In our preliminary
study on red DRL agents using DRL algorithms [Sultana et
al., April 2021], both the DQN (Deep-Q-Network) and PPO
(Proximal Policy Optimization) agents can be trained to stage
and execute optimized network attack sequences despite the
uncertain success distributions associated with their attack ac-
tions.

The ﬁrst issue of concern is the training environment. The
network cyber environment is much more complex compared
with other domains where DRL solutions are being applied.
It involves different network segments and many functional
layers where the networking devices, protocols, services, ap-
plications, users, attackers and defenders are interacting to-
gether. The environment is not only entangled with a multi-
tude of dynamic factors and potential actions, but also par-
tially observable with random distributions of action out-
comes.

At present, the few RL/DRL training environments for net-
worked cyber operations [Schwartz and Kurniawatti, 2019;
Baillie et al., 2020; Microsoft, 2021], are mostly simulation-
based. This is reasonable given the beneﬁts of a simulated en-
vironment that models cyber network and its operations using
abstractions. This approach is hardware resource-efﬁcient,
involves no real networks nor action executions, and as a re-
sult expedites training. A potential downside of a simulated
environment is its deviation from reality, which may render
the trained agent decision model less relevant. Increasing the
ﬁdelity of a simulator is difﬁcult because the cyber network
operations environment is complex and therefore hard to ab-

 
 
 
 
 
 
stract; the state-space of the simulator increases rapidly when
trying to model more details of the real environment.

In comparison,

the emulation approach requires many
more hardware resources to operate a cyber network and all
its assets. The emulated environment is especially expensive
in agent training time, because actions are physically exe-
cuted in the network. Nevertheless the time to learn can still
be low compared to a red team exercise which often takes
weeks or even months. In addition, the trained AI agents are
intended to provide augmented and emerging insights and re-
sults beyond those gathered by the human red teams. The
key appeal of the emulated training environment is its high ﬁ-
delity, resulting in an agent more applicable to the real world.
The emulated environment also avoids the difﬁcult task of es-
tablishing a correct and appropriate model for the complex
real system.
It can alternatively be used to generate real-
istic models for building the simulated environment, and to
validate the simulated environment models. In general, for
practical applications, both simulated and emulated training
environments are being used to advise and complement each
other.

Implementing an emulation based RL environment is com-
plex and challenging as it involves the real cyber networked
system. This work presents CyGIL, an emulated RL/DRL
training environment for cyber operations in complex net-
works. CyGIL takes an efﬁcient system implementation ap-
proach by leveraging open source network and cyber opera-
tions tools in constructing a modular architecture. CyGIL in-
cludes a network emulator but can also run on other emulated
or real networks. Though its ﬁrst prototype supports only red
agent training, CyGIL’s system architecture is designed for
multi-agent blue and red team training, i.e. as a complete
cyber-RANGE for AI agents. Embedding the State of the Art
(SOTA) red/blue team emulators [MITRE Corp., 2021a], Cy-
GIL inherits all enterprise adversary TTPs per the ATT&CK
framework [MITRE Corp, 2021b] in the red agent’s action
space. The action space in different games is then conﬁg-
urable for training agents either focused on speciﬁc APTs, or
with a broad coverage of emerging APTs and COAs against
the network.

The contributions presented are twofold: a novel architec-
ture approach to achieve an emulated RL/DRL training envi-
ronment for complex network cyber operations, and the inte-
gration of industry SOTA tools to maximize the action abil-
ities of the agents. To our knowledge, CyGIL may be a ﬁrst
emulated RL/DRL environment for networked cyber opera-
tions that places the SOTA attack emulation tools to the dis-
posal of AI agents.

The rest of this paper is organized as follows. Section 2
brieﬂy reviews the current network cyber RL/DRL training
environments. The CyGIL system architecture and imple-
mentation approach are presented in Section 3. Section 4
illustrates examples of preliminary AI agent training exper-
iments. Section 5 summarizes the concluding remarks and
the future research with CyGIL.

2 Current cyber RL training environments

The scarcity of RL/DRL training environments for networked
cyber operations may be ascribed to the complexity of cyber
networks. At present, when applying RL/DRL to network
cyber research, the investigation is often conﬁned to a par-
ticular cyber task or a network location [Nguyen and Reddi,
2020]. Although a few host based environments for training
AI-assisted penetration testers have been reported [Pozdni-
akov et al., 2020; Chaudhary et al., 2020], they have very
limited game goals and actions. In [Gha, 2018], a network
pentester training environment is modelled as a Partially Ob-
servable Markov Decision Process (POMDP). The details of
the environment and the RL training were however not de-
scribed.

In the OpenAI Gym repository of RL/DRL training en-
vironments [OpenAI, 2021], the Network Attack Simula-
tor (NASim) [Schwartz and Kurniawatti, 2019] supports red
agent training for network-wide penetration tests. Being a
simulated environment, NASim represents networks and cy-
ber assets, from networking elements like hosts, devices, sub-
nets, and ﬁrewalls to services and applications, in abstrac-
tions modeled with a ﬁnite state machine. The simpliﬁed ac-
tion space includes “network and host discovery”, “service
exploit” for each conﬁgured service vulnerability in the net-
work, and “privilege escalation” for each kind of hijackable
process running in the network. The agent can thus simu-
late a simpliﬁed kill chain through discovery, privilege esca-
lation, and service exploits across the network. Some critical
network-based adversary abilities like lateral movement are
combined into the general ‘exploit’. At its level of abstrac-
tion, NASim does not map concepts of TTPs and APTs for
adversary focused attack discovery and optimization.

Recently Microsoft open-sourced its network cyber RL
the “CyberBattleSim” (CBS) [Mi-
training environment,
crosoft, 2021], also built on the OpenAI Gym. CBS is for red
agent training that focuses on the lateral movement phase of a
cyber-attack in an environment that simulates a ﬁxed network
with conﬁgured vulnerabilities. The attacker uses exploits for
lateral movement while a pre-deﬁned simple blue agent seek-
ing to detect the attacker and contain the intrusion. Similar to
NASim, the CBS environment can deﬁne the network layout
and the list of vulnerabilities with their associated nodes. In
the CBS, the modelled cyber assets capture OS versions with
a focus to illustrate how the latest operating systems and up-
to-date patches can deliver improved protections. As stated
by Microsoft, the CBS environment has a highly abstract na-
ture and cannot be directly applied to real-world systems [Mi-
crosoft, 2021].

In [Baillie et al., 2020], a work-in-progress cyber gym
called CybORG is presented. CybORG is designed to sup-
port both simulated and emulated environment with the same
front-end interface. CybORG also supports competing red
and blue agents. The simulated environment of CybORG cap-
tures certain details of the cyber assets and provides a large
action space. For example, the red action space includes ex-
ploits from the Metasploit Framework (MSF) space, while
blue actions are based on Velociraptor [Velocidex., 2021] ca-
pabilities. Both red and blue also have access to shell com-

mands for low-level actions. Aiming to be a more realistic
network cyber operation environment, the state space of the
CybORG simulator is very large. When aiming for this level
of detail, an emulator offers an advantage in avoiding the cost
of building the requisitely complex state model. To this end,
CybORG also supports the emulated environment which is
still in testing and development [Baillie et al., 2020]. An in-
teresting issue raised by CybORG is the training game design
for the environment. The networked cyber environment must
support not only many different network conﬁgurations but
also different games for capturing the potential attack vectors
and objectives of the adversary.

Compared with the current solutions, the novelty of Cy-
GIL includes its stateless architecture approach that enables
a complex emulated network cyber environment. Its compre-
hensive action space allows for realistic training games. Ad-
ditionally CyGIL directly leverages the rapid advancement of
industry cyber defence frameworks [Hutchins et al., 2011;
MITRE Corp, 2021b] and automation tools to stay at the
forefront of the most relevant APTs and SOTA technologies
found in the current cyber network operations.

3 CyGil Architecture and System

Implementation

As with all RL/DRL training environments, CyGIL trains an
agent to optimize its sequential action decisions by maximiz-
ing the accumulated rewards collected from its actions. The
objective of the agent is modelled by a reward function that
returns the reward value after each action execution, when
the agent also receives its current observation space (ObS).
Different agents have their own action spaces, objectives de-
scribed by reward functions, and ObSs. As in a real network
cyber environment, while the blue agent may see most of the
network states depending on the sensors employed, the red
agent often starts from seeing nothing in its ObS and builds
it up step-by-step from the output of its executed commands.
As CyGIL currently is tested only for red agent training, the
descriptions hereafter refer only to the red agent perspective
unless explicitly stated otherwise.

3.1 Centralized control and distributed execution
Cyber operations require suitable infrastructure to deploy and
execute. CyGIL employs a command and control (C2) infras-
tructure, as shown in Figure 1 with the red agent example.
The centralized C2 decides and launches attack actions which
are then executed by the distributed implant(s), denoted as
“hand(s)” in the network. Thus, in CyGIL the red agent that
is being trained performs the decision-making C2 for the at-
tack campaign. The agent organizes the action sequence of
the attack operation and dispatches to hands the payloads for
command execution at different network locations depending
on their local conﬁgurations. An action at one step of the
action sequence may involve multiple executions by differ-
ent hands at the same time. The ObS of the agent aggregates
the information collected by all hands from their executions
of the action. The agent’s decision model therefore involves
the dimension of managing multiple hands to traverse the net-
work for carrying out actions towards the higher reward.

Figure 1: The CyGIL Red team C2 Infrastructure - centralized con-
trol and distributed execution

Figure 2: The CyGIL environment (env)

As such the agent executes actions continuously in time in
the CyGIL env, with each action counted as one action step by
the env. The attack operation starts after the initial compro-
mise where at least one hand is implanted in the network. The
process of the initial compromise, which is often achieved
through social engineering (for example email phishing, ma-
licious web link access etc.), is not included in CyGIL.

3.2 Functional Substrates
The CyGIL software architecture is illustrated in Figure 2. To
enable direct use of all SOTA algorithm libraries published
from the RL/DRL research community, CyGIL provides the
training environment in the standard OpenAI Gym python
class “env”. The scenario conﬁguration supports conﬁgura-
tions of both the network and the training game. The loading
function initializes an env instance per the conﬁguration for
agent training.

To support the training runtime, the CyGIL env embodies
three major functional substrates as depicted in Figure 2: the
Networked Cyber Physical System (NCPS), the Action Ac-
tuator (AA) and the Sensor Views (SV). They together wrap
the entire cyber network and operational space into the env
class. Among them, the NCPS comprises all networked cy-
ber assets of hardware and software and enables the SV and
AA. The AA carries out the execution of each action on the
network through the NCPS. The SV formats and presents to
the agents what they can see and know about the env.

The NCPS organizes the agent’s training on the network in
repeated game episodes, each of which is a sequence of the
agent’s actions against the network. The NCPS veriﬁes if the
current training episode is ended in order to reset the env for
the next training episode. The reset cleans up relics left in

the network by the agent and restores the hand(s) to their ini-
tial positions. The NCPS keeps for the agent an information
database, namely the “fact” database, to record its executed
actions and the results parsed from the actions’ output mes-
sages, which is used in forming the agent’s ObS.

The AA actuates on the network every action offered to the
agent, employing the hand(s), collecting the output messages,
and storing the results to the NCPS’s information database
for exposing the observation space to the agent. The reward
resulted from the action execution is also computed according
to the reward function.

Although the action space is supported and actuated by
the AA, the SV formats and presents the action space to the
agents per the standard interface [OpenAI, 2021] between the
env and agent. The SV also formats and presents to the agent
the reward, ObS and game-end ﬂag after each action execu-
tion according to the standard interface. As described above,
this information is generated and maintained by the AA and
NCPS.

3.3 Functional implementation
The CyGIL implementation builds upon SOTA open source
software frameworks and tools, aiming to keep CyGIL rele-
vant with respect to red (and blue) team techniques advanced
In particular, the
by the industry and research community.
CALDERA framework developed by MITRE [MITRE Corp.,
2021a] was selected as a basis for red actions after evaluating
red team emulation platforms for their C2 infrastructure ca-
pabilities, plug-in interfaces, and project status.

Actively developed and maintained by MITRE in open
source, CALDERA’s purpose is to enable red team emulation
using the MITRE ATT&CK framework that encompasses and
analyses reported cyber-attack TTPs. CALDERA thus sup-
ports an extensive red team action space. Additionally, the
plug-in APIs of CALDERA allow for quick add-on of new
custom abilities and other red team tools, e.g. Metasploit, to
expand the supported action space. The abilities/actions map
to the implementations of the Techniques in the ATT&CK
framework. CALDERA also supports blue agent and red vs.
blue ”brawl” games. Thus, CALDERA brings to CyGIL a
comprehensive and real threat-relevant action space, together
with the multi-agent capabilities for red vs. blue competitive
games which we plan to leverage in future work.

As an emulation platform, CALDERA is not a RL/DRL
training environment. The CyGIL implementation, as de-
scribed in Figure 3, adapts CALDERA to realize the func-
tions required for RL/DRL training env with an additional in-
terface layer. Figure 3 also describes other open source tools
used to implement the RL/DRL training env.

This implementation approach enables CyGIL env to pro-
vide agents with all the actions offered by CALDERA, and
to grow them with new CALDERA releases. The red agent
for example has more than 270 actions to apply against Win-
dows systems and more than 180 actions against Linux sys-
tems. These actions cover almost the entire ATT&CK matrix,
including execution of various techniques for initial access,
discovery, command-and-control, credential access, privilege
escalation, persistence, defense evasion, lateral movement,
collection, and exﬁltration. Additionally, CyGIL enhances

Figure 3: The CyGIL implementation approach

the agent action space by integrating Metasploit and adding
certain CyGIL custom lateral movement actions through the
CALDERA plug-in interfaces.

3.4 The red observation space
After the execution of each action, the ObS must be re-
turned to the agent as part of the result (Figure 2). To this
end, CyGIL does not maintain the entire network system
states for generating the agent’s ObS. Instead the NCPS uses
CALDERA agent’s “fact” database [MITRE Corp., 2021a]
which already gathers the information harvested by the red
agent from the output of each action command executed by
the hand(s). These are what the red agent realistically may
“see/observe,” for example a new host IP address discovered,
or the new process IDs obtained that might be used for pro-
cess hijacking. The information in this fact database in NCPS
forms the red agent’s ObS, which is inherently partially ob-
servable and very limited compared to the state space of the
env.

The SV formats the ObS of the env to a 2-dimensional ar-
ray that consists of rows to record found facts of each host and
of the entire network. The network row checks if certain net-
work wide information has been gathered, such as the fully
qualiﬁed domain name, the domain user and admin creden-
tials, the organization names, email servers, etc. Each host
row describes a host, where the columns keep the informa-
tion in the following 10 categories:

• If a hand is on the host and at what user privilege level

• The local user accounts and creds found

• The OS types and host types if it is a particular server,

e.g. the domain controller or a web server

• The modiﬁable service/process

• The local ﬁles, directories, and shares

• Host defensive conﬁgurations seen, e.g. anti-virus

• The host network info found, e.g. domain name, IP ad-

dress, subnet interfaces

• Other host system info found, e.g. system time

• Other remote system info found on this host, e.g. the ssh
command executed before or the remote IPs seen/used

• If the action executed successfully on the host

The ObS does not present the exact values or strings of the
information but rather applies 0/1 coding as much as possible
to indicate if the item is found or not. In a few columns, the
number of obtained information pieces is indicated using in-
tegers, for example the “number of local user names found.”
The content details of the information, for instance the IP ad-
dress found of a remote host, is not shown in the ObS but
is maintained in the CALDERA fact database to be used in
forming the commands and payloads for the action selected
by the agent before sending it to the hands for execution. It
is important to disassociate the agent from the very speciﬁc
information values that are irrelevant to its decision model.
Otherwise, a slight change such as a different username or a
digit in the IP address may render the trained agent incapable
of generalizing to these trivial differences.

The number of rows must not reveal the total number of
hosts in the network as the agent in reality cannot observe ac-
curately such information. One option is to use a large num-
ber of rows to support the maximum number of hosts in a
network for which this agent model may be trained. A better
alternative is to reduce the returned ObS to only the infor-
mation gathered by this action execution, excluding all what
has been seen by the agent before. With a hand potentially
located on each host in the network of the env, a composite
ObS format such as a list of arrays is needed to achieve such
a size reduction.

Although containing only a small subset of the total state
space, the ObS of the red agent can still grow exceedingly
fast. Given an ObS array of size Mr × Mc, where Mr is the
number of rows and Mc number of columns, the size of the
observation space Sob can be expressed as in Equation 1.

N
(cid:89)

Sob = (

(n)Cn )Mr ,

s.t.

N
(cid:88)

(Cn) = Mc

(1)

n=2

n=2

where n is the number of possible values in a column, Cn
is the number of the columns that take n possible values.
Reducing N towards N = 2 and reducing Cn and Mc as
much as possible will contain Sob. We are currently inves-
tigating the importance of each column empirically by ob-
serving game outputs with the goal of reducing N ,Cn and
Mc. It should be noted that this ObS size does not affect the
complexity and running of the CyGIL env, while posing chal-
lenges to the agent training. Addressing such challenge is the
purpose of CyGIL in supporting the cyber AI research.

4 Games and Agent Training
4.1 CyGIL Mini-testbed Setup
A mini-testbed CyGIL environment is shown in Figure 4.
The emulated cyber network runs on virtualized hardware
where the Mininet and ONOS controller manages all the hosts
and switches on the target network, with a CALDERA C2
on an external network. The “CyGIL env code” in the DL

Figure 4: The CyGIL system setup

server provides the env for the agent training. The “Cy-
GIL env code” initializes the emulated network using Mininet
switches and the ONOS controller, and uses the CALDERA
server to execute all actions from the agent towards the net-
work. The emulated network can also run on any other net-
work emulator or can even be a real network, as both the
CALDERA server and the CyGIL management interface are
for connecting to real networks.

In the following experiments,

the emulated network is
hosted on a Dell Laptop running on Inter(R) Core(TM) i7-
4910MQ CPU@2.90Ghz with 32GB RAM. The DL train-
ing server in Figure 4 is a HP laptop running on Inter(R)
Core(TM) i9-98880H CPU@2.30Ghz with 64GB RAM and
a NVIDIA Quadro RTX 5000 Graphical Processing Unit.
The OpenAI Gym standard interface of CyGIL env enables
support for almost all open source SOTA DRL algorithm li-
braries. Trainings are tested on both the Tensorﬂow (version
2.3.0) and Pytorch (version 1.7.0) DL frameworks.

4.2 Experiments
A CyGIL env instance includes both the network and game
conﬁgurations. Different games can be set up over the same
network to form different env instances for training agents of
different proﬁles in capabilities and objectives. CyGIL allows
ﬂexible game conﬁgurations including the selected action
space, the variables in the reward function, the game goal,
the maximum number of actions allowed per game episode,
and the initial position(s) of the hand(s). The details of the
CyGIL game deﬁnition framework is a very important topic
but beyond the scope of this paper.
In the game examples
presented in this work, the reward is simply set as −1 per ex-
ecuting hand for all actions, except the successful last action
that reaches the game goal, whose reward is 99. The positive
reward placed only on the last “goal action” pushes the agent
to reach the end goal in as few action steps and with as few
hands as possible. It also prevents the human game design-
ers from subjectively inﬂuencing the agent’s COAs. Note that
CyGIL allows more types of game setup in reward functions,
action cost, and game-end objectives than shown in the ex-
amples presented here. In the following, a training episode
ends when either the maximum number of actions is reached,
or when the goal action succeeds, that is, the agent’s reward
reaches the maximum value. Then the env is reset for a new
episode.

(a) Network scenario

Figure 6: Training the DQN agent in game 1 - x Axis in 100 steps
per unit

(a) Loss

(b) Reward

CE trains on each complete game episode. The CE is a less
powerful algorithm and takes more time to train, which re-
sults in a stress-test of the CyGIL env. With both algorithms,
the agent can indeed learn the optimized action sequence to
exﬁltrate target ﬁles solely through playing in the CyGIL env,
starting with no knowledge including hosts or actions. The
DQN training results are depicted in Figure 6. The maximum
reward of 96 with a sequence of 4 steps using a single hand is
reached by the agent after playing 4600 steps.

The second game is on the network illustrated in Fig-
ure 7 (a), which consists of 9 hosts. The action space is
shown in Figure 7(b). In the network, hosts 2 and 5 are on
Ubuntu Linux and host 9 is a Windows 2016 server. The
rest of the hosts run on Windows 10 Enterprise OS. Hosts 1
and 2 are reachable from the external “Internet” by the C2.
All hosts inside the network can reach the Active Directory
Server/Domain Controller (DC) at host 9. Hosts on the same
switch belong to the same subnet and can communicate with
each other. Between different subnets, ﬁrewall rules are put
in place through ONOS to allow host 6 to communicate with
host 2 and host 3. In the training, each host sends light trafﬁc
to at least one other host as the generic network user trafﬁc.

In Game 2, the initial breach of a single hand/implant esca-
lates to a major network compromise. The action space pro-
vides various techniques in four tactic groups of discovery,
credential access, privilege escalation and lateral movement,
which are used often by high proﬁle APTs. The goal action
that succeeds to land a hand on the DC (host 9) receives the
reward of 99 and ends the episode. A hand must have an es-
calated privilege of domain administrator in order to land on
the DC, representing potentially a major system breach. The
maximum number of action steps per game is set at 300.

The hand initially lands on host 2. The actions have dif-
ferent conditional success distributions on different hosts and
under different circumstances, even for carrying out the same
technique. The success rates of actions in this game are rel-
atively low, especially for those of lateral movement. The
agent needs to learn the network reachability paths, required
credentials, and privilege levels and where, when, and how
to obtain them. It also has to manage the best number and
placement of hands and learn to select actions among imple-
mentation variations of the same technique for higher success
rates at different network locations.

Game 2 is drastically more complicated and takes more
training time than game 1. The optimized COA’s see the agent
uses the hand on host 2 via taking on actions 3,4,7 and 13

(b) Action space

Figure 5: Training game example 1

When selecting a small subset from the total action space
in a game, the training may focus on the most relevant TTPs
associated with the selected adversary proﬁles. A larger ac-
tion space may assume an adversary with more or even all
the supported TTP abilities, generating complex games that
require longer training durations. The ample action space in
CyGIL enables the red agent to execute all the typical tactics
following the initial network breach to delivery in an end-to-
end kill-chain operation, e.g. achieving persistence and lat-
eral movement in the network to collect and exﬁltrate target
information.

The ﬁrst simple testing game is illustrated in Figure 5. The
action that succeeds in exﬁltrating the target ﬁle from the host
to the C2 gets the reward of 99 and ends the game episode.
The maximum number of actions in one training episode is
100. The action space (Figure 5(b)) includes variants of tech-
niques from two groups of adversary tactics, namely collec-
tion and exﬁltration. Depending on conﬁgurations of the
hosts, the exploits may or may not be executable. Even if
executable, the exploit may or may not succeed due to the
host states. In the network (Figure 5(a)) host 1 to host 3 are
Windows 10 machines and host 4 is on Linux Ubuntu. Data
trafﬁc is sent between the following host pairs: host 1 and
host 3, host 1 and host 2, and host 3 and 4.

Two typical DRL algorithms have been used in this test.
The ﬁrst is the Deep Q-Network (DQN) [Mnih et al., 2013]
using its implementation from the reinforcement learning
library Tf-Agent 2.3 [Guadarrama et al., 2018] developed
by Google on Tensorﬂow. The second is a classic cross-
entropy (CE) algorithm implemented on the PyTorch frame-
work. While the DQN trains on every step, the policy-based

(a) Network scenario

Figure 8: Training DQN agent in game 2

involve fewer steps. The agent training time takes from sev-
eral hours to a few days, depending on the game and the al-
gorithm. In addition to the action execution latency, the time
required to reset the env for a new episode lasts from 3s in
game 1 to 40s in game 2 with the mini-version of the hard-
ware platform, contributing also to the average episode du-
ration. To reduce latency both in action execution and game
episode/env resetting, improved hardware resource including
servers and the hyperconverged management infrastructure of
VxRail are being tested for network emulation. While imped-
ing the training time, the inherent latency in an emulated en-
vironment nevertheless contributes to its realism because as
in the real cyber networks, the agent must adapt to variable
communication delays between the hands and the C2/agent.

5 Conclusions and Future Work

As presented, CyGIL is an emulated RL/DRL training en-
vironment for network cyber agents. The CyGIL approach
enables industry SOTA red team tooling abilities to support
a comprehensive action space. The CyGIL functional archi-
tecture realizes a stateless implementation of the env that is
scalable and expandable. Being an emulated environment,
the agent trains on real network conﬁgurations to construct
relevant and applicable cyber operation decision models. Cy-
GIL provides ﬂexible network scenarios and game conﬁg-
urations to support different adversary objectives and abil-
ity proﬁles. Compliant to the OpenAI Gym standard inter-
face, almost if not all the SOTA RL/DRL algorithm libraries
developed by the research community can be used directly
with CyGIL. Some preliminary experiments have produced
expected training results and highlighted RL/DRL algorithm
challenges when applied to the autonomous cyber operation
domain.

In our current research,

the CyGIL testbed is being
improved to reduce latency in all areas, especially with
game/env resetting.
In addition to scaling up the hardware
resource, the option of switching between parallel emulated
networks for the env is being evaluated. At the same time
we are investigating how to map the cyber operation goals to
relevant training games, as well as the effective DRL algo-
rithms that reduce the training time and improve the model
generalization. Finally we are adding blue agent capability to
enable competitive multi-agent training for realizing a fully
representative red vs. blue cyber game environment.

(b) Action space

Figure 7: Training game example 2

(or 3,7,4 and 13, or 7,3,4 and 13) to discover, to elevate the
privilege and to launch a new hand on host 6. Capturing the
domain admin credential on host 6 via action 5, a hand ﬁnally
succeeds in logging into (via action 12) the DC’s domain ad-
min account on the Active Directory server residing at host
9. Using the DQN from Google’s Tf-Agent library, the agent
through training learns the optimized COA’s and reaches re-
wards ranging from 90 to 92 as depicted in Figure 8, as some
actions have low success ratio. More efﬁcient algorithms are
however desirable to improve the training efﬁciency. This
more realistic game presents a challenging problem in DRL
training, which is exactly the purpose of CyGIL.

Action execution in CyGIL runs on wall-time and results
in longer latency than in simulated environments. The latency
depends on the hardware resources offered to the emulated
network. At present in the mini-testbed, average action exe-
cution time is under 1s in game 1, and ranges from under 1s
to a couple of minutes for actions in game 2. Some actions,
such as lateral movement, may take up to 120s for the new
hand to install on the remote host and its reporting beacons to
be heard by the C2, under the network trafﬁc conditions. The
long latency is partly due to the hardware limitation when
running more VMs in game 2 on one laptop. Each training
episode lasts for about 30s on average in game 1 and about
20-30 minutes in game 2. The episode decreases in its dura-
tion when more optimized action sequences are formed that

deep reinforcement learning. nature, 518(7540):529–533,
2015.

[Nguyen and Reddi, 2020] Thanh Thi Nguyen and Vi-
jay Janapa Reddi. Deep reinforcement learning for cyber
security, 2020.

[OpenAI, 2021] OpenAI. Openai gym. https://github.com/

openai/gym, 2021.

[Pozdniakov et al., 2020] Konstantin Pozdniakov, Eduardo
Alonso, Vladimir Stankovic, Kimberly Tam, and Kevin
Jones. Smart security audit: Reinforcement learning with
a deep neural network approximator. In 2020 International
Conference on Cyber Situational Awareness, Data Analyt-
ics and Assessment (CyberSA), pages 1–8. IEEE, 2020.

[Red Canary Co., 2021] Red Canary Co.
team - document and souce code.
redcanaryco/atomic-red-team, 2021.

Atomic red
https://github.com/

[Schwartz and Kurniawatti, 2019] Jonathon Schwartz and
Hanna Kurniawatti. Nasim: Network attack simulator.
https://networkattacksimulator.readthedocs.io/, 2019.
[Sultana et al., April 2021] Madeena Sultana, Adrian Taylor,
and Li Li. Autonomous network cyber offence strat-
egy through deep reinforcement learning. Proceedings of
SPIE conference on defence and commercial sensing 2021,
April 2021.

[Sutton et al., 1999] Richard S. Sutton, David McAllester,
Satinder Singh, and Yishay Mansour. Policy gradient
methods for reinforcement
learning with function ap-
In Proceedings of the 12th International
proximation.
Conference on Neural Information Processing Systems,
NIPS’99, page 1057–1063, Cambridge, MA, USA, 1999.
MIT Press.

[Velocidex., 2021] Velocidex. Velociraptor - document and
https://github.com/Velocidex/velociraptor,

souce code.
2021.

[Williams, 1992] Ronald J. Williams.

Simple statistical
gradient-following algorithms for connectionist reinforce-
ment learning. Mach. Learn., 8(3–4):229–256, May 1992.

References
[Baillie et al., 2020] Callum Baillie, Maxwell Standen,
Jonathon Schwartz, Michael Docking, David Bowman,
and Junae Kim. CybORG: An Autonomous Cyber Oper-
ations Research Gym. arXiv:2002.10667 [cs], February
2020. arXiv: 2002.10667.

[Chaudhary et al., 2020] Sujita Chaudhary, Austin O’Brien,
and Shengjie Xu. Automated post-breach penetration test-
ing through reinforcement learning. In 2020 IEEE Con-
ference on Communications and Network Security (CNS),
pages 1–2. IEEE, 2020.
[Chris Ross, 2018] Steve

Borosh
Empire project.

Chris

Ross,
https://github.com/

Will Schroeder.
EmpireProject/Empire, 2018.

[Gha, 2018] Reinforcement Learning for Intelligent Penetra-

tion Testing, 2018.

[Guadarrama et al., 2018] Sergio Guadarrama, Anoop Ko-
rattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam
Fishman, Ke Wang, Ekaterina Gonina, Neal Wu, Eﬁ
Kokiopoulou, Luciano Sbaiz, Jamie Smith, G´abor Bart´ok,
Jesse Berent, Chris Harris, Vincent Vanhoucke, and Eu-
gene Brevdo. TF-Agents: A library for reinforcement
learning in tensorﬂow.
https://github.com/tensorﬂow/
agents, 2018. [Online; accessed 29-January-2021].

[Guardicore Inc., 2021] Guardicore Inc. Infection monkey -
document and source code. https://github.com/guardicore/
monkey, 2021.

[Hutchins et al., 2011] Eric M Hutchins, Michael J Clop-
pert, Rohan M Amin, et al. Intelligence-driven computer
network defense informed by analysis of adversary cam-
paigns and intrusion kill chains. Leading Issues in Infor-
mation Warfare & Security Research, 1(1):80, 2011.

[Isele et al., 2018] David Isele, Reza Rahimi, Akansel Cos-
gun, Kaushik Subramanian, and Kikuo Fujimura. Nav-
igating occluded intersections with autonomous vehicles
In 2018 IEEE Inter-
using deep reinforcement learning.
national Conference on Robotics and Automation (ICRA),
pages 2034–2039, 2018.
[Microsoft, 2021] Microsoft.

-
documnent and source code. https://github.com/microsoft/
CyberBattleSim, 2021.

Cyberbattlesim project

[MITRE Corp., 2021a] MITRE Corp. Caldera - document
and souce code. https://github.com/mitre/caldera, 2021.
[MITRE Corp, 2021b] MITRE Corp. Mitre att&ck knowl-

edge base. https://attack.mitre.org/, 2021.

[Mnih et al., 2013] Volodymyr Mnih, Koray Kavukcuoglu,
David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep
reinforcement learning. arXiv preprint arXiv:1312.5602,
2013.

[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu,
David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. Human-level control through

