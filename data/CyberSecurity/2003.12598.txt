A Security and Performance Driven Architecture for
Cloud Data Centers

Muhamad Felemban, Anas Daghistani, Yahya Javed, Jason Kobes, and Arif Ghafoor

1

0
2
0
2

r
a

M
7
2

]

R
C
.
s
c
[

1
v
8
9
5
2
1
.
3
0
0
2
:
v
i
X
r
a

ABSTRACT

With the growing cyber-security threats, ensuring the se-
curity of data in Cloud data centers is a challenging task.
A prominent type of attack on Cloud data centers is data
tampering attack that can jeopardize the conﬁdentiality and the
integrity of data. In this article, we present a security and per-
formance driven architecture for these centers that incorporates
an intrusion management system for multi-tenant distributed
transactional databases. The proposed architecture uses a novel
data partitioning and placement scheme based on damage
containment and communication cost of distributed transac-
tions. In addition, we present a benchmarking framework for
evaluating the performance of the proposed architecture. The
results illustrate a trade-off between security and performance
goals for Cloud data centers.

I. INTRODUCTION

The rapid growth of data volume driven by IoT, social
networks, and other data-intensive applications poses numer-
ous performance and security challenges in terms of real-
time data stores and processing [1]. Cloud computing has
emerged as a leading infrastructure that provides a pay-per-
use access to a shared pool of resources. Numerous research
efforts have aimed at designing scalable Cloud Data Centers
(CDCs) for hosting multi-tenant applications [2]. Figure 1(a)
depicts the conventional three layered architecture of a CDC.
These layers include: Application, Platform, and Infrastructure
layer. The application layer provides access to various services
to the end-user. The platform layer consists of operating
systems and software frameworks including database systems
for supporting Cloud applications. The infrastructure layer
consists of shared physical resources, e.g., CPUs, memory,
and data storage.

One of the main challenges in designing transactional
databases for CDCs is the partitioning and placement of
tenants data across a cluster of data stores. Efﬁcient solution
to this problem can result in an increase in the availability
and reliability of the CDC [3], [2]. This is especially true
for multi-tenant applications that may serve large volume of
transactions. An example of a transactional database designed
for Cloud environments is ElasTras [4]. Figure 1(b) depicts
the architecture of a typical distributed transactional database,
which consists of three components: a distributed transactions
manger (DTM), a transactions router, and a data partition
manger. The DTM is responsible for managing the commit
protocol of the distributed sub-transactions. The transaction
router maps each sub-transaction to an appropriate database

that contains the required data. The partition manger is
responsible of managing the distributed storage solutions,
e.g., Hadoop Distributed File System (HDFS), by creating
and placing the partitions across the distributed data store.
Several solutions have been proposed to address the problem
of data partitioning and placement for distributed Cloud data
stores. These solutions are primarily focused on achieving
various performance criteria such as latency and transaction
throughput. [3], [5].

On the other hand, the multi-tenancy feature provided by a
Cloud infrastructure poses unique security risks due to data
leakage, virtual machine escape, and side channel attacks.
Such risks arise from sharing of physical resources among
tenants. Another prominent intrusion attack in transactional
databases is the data tampering attack that aims at modifying
conﬁdential data with incorrect values [6]. Due to multi-
tenancy feature of CDCs, a data tampering attack can cause
catastrophic cascading failures and performance degradation as
a result of application interoperability, data dependency, and
data sharing among tenants. Such attacks can be manifested
by exploiting vulnerabilities in the application, e.g., SQL
injection, privilege escalation, and virtual machine escape.
Table I lists some known Cloud vulnerabilities along with their
scores as stipulated by the Common Vulnerability Scoring Sys-
tem (CVSS)1. Exploitation of these vulnerabilities constitutes
data tampering attacks in CDCs. Several solutions have been
proposed to address the risk of data leakage including data
isolation and enforcement of some access control mechanisms
[7]. However, no solution has been proposed to mitigate the
risk of intrusion attacks on transactional databases for CDC.
In this article, we propose a security-driven architecture
for transactional database for CDCs. Development of this
architecture entails reengineering the CDC architecture by
including a real-time Adaptive Intrusion Management Sys-
tem (AIMS) that provides intrusion detection, response and
recovery mechanisms for the database. AIMS uses an adaptive
access and admission control mechanism that responds to
intrusion attacks by selectively blocking segments of data that
are affected by these attacks. Intrusion attacks are manifested
in the form of malicious transactions. The proposed archi-
tecture uses a security-driven and performance-oriented data
partitioning and placement strategy across Cloud data stores.
We model the data partitioning and placement requirement
as an optimization problem with combined performance and
security objectives. In addition, we present a novel malicious
transaction benchmark for AIMS to illustrate its resiliency

1https://www.ﬁrst.org/cvss/

 
 
 
 
 
 
2

Vulnerability

CVE-2017-7546

CVE-2018-1058

CVE-2015-7502

CVE-2018-5985
CVE-2016-9994

CVSS
Score
9.8

8.8

5.1

9.8
7.1

TABLE I: Example of vulnerabilities in CDCs.

Description

PostgreSQL vulnerability that allows adversary to gain access to database with empty
password
PostgreSQL vulnerability that allows adversary to execute codes in with the permissions
of superuser
Operating systems vulnerability that allows adversary to obtain sensitive data and gain
privilege
SQL injection vulnerability in LiveCRM SaaS cloud component
SQL injection vulnerability in IBM Cloud

close to AIMS is given in [13], [10] that proposes an online
damage tracking and quarantine mechanism to increase the
survivability of single-tenant databases. The authors propose a
multi-pass recovery procedure to ensure that all the corrupted
data objects are recovered. However, in scenarios with high
dependency among transactions in a workload, the recovery
procedure can take a long time and hence can degrade the
overall performance. AIMS employs an admission control
mechanism that momentarily suspends running transactions
and performs the recovery procedure in a single-pass.

Previous work on data partitioning and placement has been
focused on improving the availability, reliability, and scalabil-
ity, but not on the security aspect of shared-nothing distributed
transactional databases. For example, in [14] authors study
the problem of data placement strategies that minimize the
data communication cost incurred by distributing data across
clusters of servers. In [5], authors propose an automatic data
partitioning methodology for distributed transactional memory
systems. In [3], authors propose a partitioning and place-
ment technique across large number of machines in order to
minimize the number of distributed transactions. However, in
this article the proposed architecture of CDC addresses the
challenge of data partitioning and placement with respect to
both performance and security considerations.

III. SECURITY-DRIVEN REENGINEERING DESIGN OF
CLOUD DATA CENTER
Figure 2(a) depicts a generic architecture of AIMS, which
provides intrusion detection, response and recovery mecha-
nisms for the database. In the following we brieﬂy elaborate
these mechanisms. Subsequently, in order to reengineer the
design of the CDC architecture in Figure 1(b), we integrate
AIMS functionalities in this architecture. The integration en-
tails the identiﬁcation of new components and possible re-
designing some of the existing components of Figure 1(b).
In Figure 2(b) the integrated architecture of CDC is depicted.
This architecture provides a security-driven and performance-
oriented data partitioning and placement, and distributed in-
trusion management across cluster of servers for CDC. Dis-
cussion on AIMS and each component of the new architecture
is given in the following sections.

A. Adaptive Intrusion Management System

The objective of AIMS is to mitigate the damage caused
by intrusion attacks on transactional databases. We consider

Fig. 1: CDC and distributed transactional database architec-
tures.

against various attack scenarios. We present the evaluation
results to highlight the viability of the proposed architecture
and illustrate a trade-off between security and performance in
the context of reengineering CDCs.

II. RELATED WORK

Several solutions have been proposed to prevent and miti-
gate the effect of intrusion attacks on transactional databases.
One solution is to employ an Intrusion Detection System (IDS)
with the objective of an IDS is to monitor and detect illegal
accesses and malicious actions in transactional databases [8],
[9]. However, an IDS can miss the detection of an attacks and
is not designed to repair the damage caused by late detection
of attacks. Therefore, an IDS is often integrated with response
and recovery mechanisms to alleviate the damage [10]. In
[11], a mechanism to recover from intrusion attacks for web-
applications by rolling back the database and replaying subse-
quent legitimate actions to correct the state of the database is
proposed. In [12], the authors propose an intrusion recovery
tool for database-driven applications running on Platform-
as-a-Service Clouds. AIMS can be used as a middle-layer
between the transactional database and the application to
perform automatic intrusion response and recovery indepen-
dently from the running applications. The most relevant work

UsersCloud	Data	CenterApplicationPlatformsInfrastructureDatabaseApplicationTransaction	RouterDistributed	Transaction	ManagerData	Partition	and	Placement	ManagerDatabaseDatabaseTransactional	Database	Management	for	Cloud	Data	CentersDistributed	Datastores(a)	Conventional	layered	architecture	for	cloud	data	centers.(b)	A	distributed	transactional	database	management	architecture.3

time information about tm, i.e., its commit timestamp tc
its detection timestamp td
the objects that have been updated during the period tc
td
m by adding them to COT.

m and
m. The response subsystem marks all
m and

3) Recovery Subsystem: The objectives of recovery subsys-
tem are: 1) to identify the correct and complete set of affected
transactions, and 2) to execute the compensating transactions
accordingly. The set of affected transactions is correct and
complete if and only if the set contains no transactions that
are falsely identiﬁed as affected and contains every affected
transactions caused by the attack. In order to identify this set,
the recovery subsystem temporarily blocks new transactions
to prevent
them from reading any undiscovered corrupted
objects. This subsystem performs blocking operations based
on a locking algorithm. When the lock is acquired by the
recovery subsystem, the admission controller blocks all the
incoming transactions until the recovery subsystem releases
the lock. During the time in which the lock is acquired by the
recovery subsystem, the correct and complete set of affected
transactions is identiﬁed. Subsequently, uncorrupted objects
are released from COT.

The recovery procedure is performed in two phases. First,
the recovery subsystem executes compensating transactions to
undo the effect of the malicious and affected transactions.
The recovery subsystem uses the transactions log table to
ﬁnd the correct version of the corrupted objects. In particular,
the corrupted objects are updated with the values of the
most recent versions before the execution of the malicious
transaction. Second, the recovery subsystem executes compen-
sating transactions to re-execute each transaction in the set of
affected transactions. The information required to re-execute
the transactions is maintained in the transactions log. At the
end of the recovery procedure, the recovered data objects are
removed from COT. Also, the admission controller is signaled
to resume the admission of the blocked transactions.

B. Security-Driven Transactional Database for Cloud Data
Centers

Figure 2(b) shows the modiﬁed security-driven architecture
of transactional database for CDCs. The key modiﬁcation
of the proposed architecture is the redesiging of the Data
Partitioning and Placement Manager (DPPM) in Figure 1(b)
to provide damage containment in the presence of an attack.
The new security-driven DPPM uses information about inter-
transaction dependency to generate an optimal data partition-
ing and placement plan that minimizes the communication
cost and curtails the propagation of damage among tenants.
This information is provided by the Workload Dependency
Assessment (WDA). The objective of WDA is to analyze and
capture the inter-transaction dependencies in the workload.
Note, we only consider data partitioning and placement plans
with no data replication. As a result, a distributed transaction
is divided into sub-transactions, where each sub-transaction
is executed in different data stores. In addition, four new
components are added to the architecture in Figure 1(b) to
perform the functionalities of AIMS, namely, the Distributed
Response and Recovery Manager (DRRM), the Distributed

(a) Generic AIMS Architecture.

(b) Security-driven distributed transac-
tional database architecture.

Fig. 2: Reengineering CDC by integrating AIMS with the
distributed transactional database architecture.

transaction-level intrusion attacks on the database. A transac-
tion, tm, is malicious if it tampers the database by updating
data objects with incorrect values. In this context, a malicious
transaction corrupts the data by launching an attack. A trans-
action, ta, is affected if it directly (or indirectly) depends on a
malicious or an another affected transaction. Two transactions,
ti and tj, are dependent if tj reads an object that has been
updated by ti. Malicious and affected transactions are invalid
transactions, which, if executed, take the database into an
invalid state. Consequently, the integrity and availability of
the CDCs can be affected.

A generic architecture of AIMS, as depicted in Figure 2(a),
contains four components: the IDS, the Admission Controller,
the Response Subsystem, and the Recovery Subsystem. We
assume that existing IDSs can be integrated with AIMS.
Further discussion about IDS is not be provided in this article;
we refer interested readers to [8], [9]. In the following subsec-
tions, we discuss each component in detail. The pseudocode
of the procedures performed by each component is listed in
Algorithm 1.

1) Admission Controller: The main functionality of the
admission controller is to regulate the execution of the transac-
tions. For this purpose, the admission controller checks if the
transaction is requesting to access corrupted objects, which are
stored in the Corrupted Objects Table (COT). In particular, the
admission controller extracts the read/write set, RW , of the
transaction. If RW ∩COT is not empty, then the transaction is
blocked until the requested objects are recovered and released
from COT by the recovery subsystem as discussed below.

2) Response Subsystem: The objective of the response sub-
system is to provide an initial evaluation of the damage caused
by a malicious transaction tm in order to prevent incoming
transactions from further spreading the damage. In particular,
IDS alerts the response subsystem when tm is detected as
malicious. Subsequently, the response subsystem collects the

Log	Table/	CTTTransactional	DatabaseIntrusion	DetectionResponse	SubsystemRecovery	SubsystemAdmission	ControllerAIMSTransactions	StreamTransaction	RouterSecurity-Driven	Transactional	Database	for	Cloud	CentersWorkload	Dependency	Assessment	ModuleDistributed	Transaction	ManagerSecurity-Driven	Data	Partitioning	andPlacement	ManagerApplicationDistributed	Response	and	Recovery	ManagerDatabaseDatabaseDatabaseDistributed	DatastoresIntrusion	Detection	SystemDistributed	Admission	ControllerALGORITHM 1: AIMS procedures

4

1. Function Admission Controller (ti)
2.

/* Invoked when a transaction ti is executed

RWti ← the read/write objects set ti
while RWti ∩ COT do

*/

Block ti

end
Execute ti

7.
8. Function Response Subsystem (tm, td
9.

m)

/* Invoked when IDS detects a transaction

*/

tm as malicious
tc
m ← get commit time of tm
for objects updated between tc
Add object to COT

m and td

m do

end

13.
14. Function Recovery Subsystem (tm)
15.

/* Invoked when the response subsystem is

terminated
Block new transactions
AT ← Find the complete and correct set of affected transaction
Resume new transactions
Remove uncorrupted objects from COT
Undo tm and all t ∈ AT
Redo all t ∈ AT

*/

3.

4.

5.

6.

10.

11.

12.

16.

17.

18.

19.

20.

21.

Admission Controller (DAC), the WDA, and the IDS. The
objective of the DRRM is to coordinate the response and
recovery procedures across the distributed data stores. The
objective of the DAC is to coordinate the admission of
incoming transactions based on the status of aggregated COT
of each data store. Note, the IDS, the DAC, and the WDA
operate at the transaction level, i.e., before a transaction is
divided to multiple sub-transactions, which are routed to their
respective data store. On the other hand, the DRRM operates
at the sub-transaction-level. In essence, the DRRM manages
the response and recovery procedures to recover the corrupted
objects damaged by executing the sub-transactions in each data
store. In the next section, we discuss the security-driven data
partitioning and placement scheme deployed by DPPM.

IV. REENGINEERING CHALLENGE: DATA PARTITIONING
AND PLACEMENT FOR CLOUD DATA CENTER

We perceive that the distributed data stores of CDC can
provide a better damage containment strategy using an in-
telligent data partitioning and placement scheme. Consider
for example a CDC with three data stores, i.e., M1, M2,
and M3, in Figure 3. Let D be the set of objects in the
multi-tenant workload, where Di denotes the data objects of
the ith tenant. Note, D = (cid:83)
i Di ∪ DS, where DS is the
set of shared data objects among the tenants. Let the set of
transactions executed by the tenants on D be T , which are
executed in the following order {T1, T2, T3, T4, T5, T6, T7}.
A transaction can be classiﬁed into two categories: single-
transactions2. Single-
tenant
tenant transactions access data objects belonging to a single
tenant, whereas multi-tenant transactions access shared ob-
jects among tenants and are denoted with T (cid:48). For example,

transactions and multi-tenant

2https://docs.microsoft.com/en-us/azure/sql-database/

Fig. 3: Pictorial example of data partitioning and placement
in CDCs. Each cylinder is a data store representing a data
partition.

T (cid:48) = {T4, T6, T7}. Note, T (cid:48) ⊆ T . Refer to the diagram in
the top-left corner of Figure 3. If T6, executed by Tenant 3,
is malicious, then the data of Tenant 4 will be corrupted if
T7 is executed without waiting for the decision of the IDS.
A plausible solution to contain the damage is by employing
a partitioning and placement scheme that splits and places
the data accessed by T6 to M1 and M2. Consequently, T6
becomes a distributed transaction as shown in diagram in the
bottom-right corner of Figure 3. Since the sub-transactions of
a distributed transactions need to be coordinated at the time of
commit, the transaction is held until the IDS declares if it is
a malicious or a benign transaction. Note, a naive solution for
damage containment is to wait for the IDS decision after the
execution of each transaction. However, this can substantially
reduce the transaction throughput for the CDC.

The goal of the data partitioning and placement scheme
is to help in containing the damage caused by malicious
transactions. However, data partitioning can incur some per-
formance degradation as a result in increase in the cost of
communication among distributed data stores. This commu-
nication is incurred due to the delay among data stores [3].
Therefore, we propose a scheme that jointly optimizes the
security and performance goals in terms of data partition-
ing and placement across data stores. Subsequently, we can
formulate the partitioning and placement scheme as a dual-
objective optimization problem with the joint objective to
minimize the communication cost while ensuring damage
containment. Alternatively, the problem can be transformed
into a single-objective optimization problem to minimize the
communication cost of distributed transactions. In this case,
the damage containment is modeled as a constraint function
in the optimization problem that forces a transaction with

Application(Transactions	Stream)	{"#,"%,"&,"',"(,"),"*}Distributed	Response	and	Recovery	Managero1o2o3o4o6o5o7,&T3T1T2,#o8o9o10o11o12o17o18o13o14o15o19o16,%T4T5T6T7Data	accessed	by	T1Application	DataT2o5o6T1o1o2o3o4T3o7o8Tenant	2Tenant	3Tenant	4Tenant	1SharedT5o18o17T6o19o16T4o9o10o11o12T7o15o14o13"&#"&%")%")#M13090120M3M2shared data to span multiple data stores. Let St be the span
of transaction t, i.e., the set of data stores that t is accessing
through its sub-transactions. The communication cost of t is
denoted as C(St) and is given as the following.

C(St) = max
e∈(u,v)
u,v∈St,u(cid:54)=v

(cid:96)(e)

(1)

where (cid:96)(e) is the value of the label on edge e. In other words,
the communication cost of transaction t accessing the set of
partitions (i.e. data stores) represented by St is the maximum
delay incurred by the communication between any pairs of
data stores. For example, the communication cost of T3 is 90,
since ST3 = {M1, M2} as shown in the graph in the top-right
corner in Figure 3. Accordingly, the optimization problem for
the partitioning and placement scheme of the reengineered
CDC can be given as follows.

C(St)

Minimize
P ∈P
subject to

|St(cid:48)| > 1 ∀t(cid:48) ∈ T (cid:48)
|Pi| ≤ Capacity(Mi) ∀i = 1, . . . , n

(2)

(3)

(4)

Constraint 3 captures the damage containment requirement as
it forces each multi-tenant transaction t(cid:48) ∈ T (cid:48) to access at least
two data stores, while constraint 4 ensures that the partitions
can ﬁt into an assigned data store.

V. MALICIOUS TRANSACTION WORKLOAD BENCHMARK

Several benchmarks have been developed in the literature to
evaluate the performance of transactional databases systems,
e.g., TPC-C3. However, no benchmark has been developed
to evaluate the performance of intrusion management in these
systems. To address this challenge, we have developed a novel
malicious transaction workload benchmark4 with the ability to
generate transactional workload with several parameters and
to orchestrate various attack scenarios.

A. Transactional Workload Generation

The proposed benchmark simulates a banking money trans-
fer application. In essence, the benchmark consists of a single
data table, Checking, that has two attributes: account id and
balance. The benchmark has three types of money transfer
transactions: distribute, collect, and many-to-many transfer. A
distribute transaction transfers money from a single account to
N other accounts (a fan-out transaction); a collect transaction
transfers money from M accounts to a single account (a fan-
in transaction); a many-to-many transactions transfers money
from many accounts to many accounts (a fan-in/fan-out trans-
action). A pictorial illustration of these transactions is given
in Figure 4.

The transactional workload is generated using three param-
eters: the number of transactions (n), the degree of fan-in/fan-
out (α), and inter-transaction dependency threshold (β). The

3http://www.tpc.org/tpcc/
4The benchmark is available on https://bitbucket.org/mfelemban/mtb/

5

(a) 1-to-N

(b) N -to-1

(c) N -to-M

Fig. 4: Types of money transfer transactions.

type of each transaction is chosen as distribute, collect, and
many-to-many, randomly. For each transaction, the degree of
fan-in/fan-out is determined using a uniform distribution with
a range of [2, α] and the amount of transferred money from
the source account is chosen as a percentage of the balance
with uniform distributed range of [0.01,0.1]. To simulate the
transaction inter-dependency, a random graph G of n nodes is
generated using, for example, the Erd¨os-Renyi model with β
representing the edge dependency probability. Subsequently,
we map the transactions to the nodes in G. Two transactions
are dependent if their respective nodes in G are adjacent. The
data objects are assigned to the transactions such that two
dependent transactions are assigned shared objects.

B. Benchmark Evaluation

The benchmark is implemented on an OLTP-benchmark
testbed [15] and PostgreSQL 9.5. The balance of the accounts
in the Checking table is initially set to $10,000. We evaluate
the performance of AIMS using three metrics:
i) number
of affected transactions, ii) average recovery time, and iii)
two metrics indicate the
average response time. The ﬁrst
capability of damage containment, while the third metrics
indicates the availability of the database. We assume that the
IDS has a detection delay of ∆ ms. Table II shows the values
of the parameters used in the experiments. The results are
depicted in Figure 5.

TABLE II: Description of Transactions

Parameter
Total number of transactions in the workload
α
β
Transaction arrival rate

Value
5000
0.5
10
10 Txs/sec

In this experiment, we evaluate the performance of AIMS by
choosing 100, 500, and 750 malicious transactions embedded
within the total workload consisting of 5000 transactions. We
study the effect of IDS detection delay. It can be noticed

SenderReceiver1Receiver2ReceivernSender2Sender1SendernReceiverSender1Receiver1Receiver2ReceivermSender2Sendernfrom Figure 5(a) that an IDS with poor performance, i.e.,
with a long detection delay, can result a large number of
affected transactions as compared to an IDS with a short
detection delay. The reason is that as the detection delay of
IDS increases, the number of dependent transactions arriv-
ing during the detection period increases. Consequently, the
number of compensating transactions increases which results
in a prolonged average recovery time as depicted in Figure
5(b). Furthermore, a large number of affected transactions
results in registering more data objects in COT. As a result,
the average response time of the transactions in the workload
increases because of the increase in the number of blocked
transactions. The average response time is shown in Figure
5(c). In addition, these ﬁgures illustrate that increasing the
number of malicious transactions results in increasing the
number of affected transactions, average response time, and
average recovery time. This is expected since increasing the
intensity of attacks results in more affected transactions which
can lead larger number of corrupted objects. Consequently,
new arriving transactions are more likely to get blocked and
hence results in an increase in the average response time as
depicted in Figure 5(c).

Figure 5 also plots the performance of the reengineered
architecture with a security-driven data partitioning and place-
ment strategy. For this purpose, we solve the optimization
problem given in Equations 3-4 to generate 10 and 20 par-
titions for the data stores using a randomized heuristic. Note,
the optimal solution for this type of partitioning problems
is NP-Complete [3]. In the randomized approach, we have
generated a large number of random solutions and picked
the one that has the smallest value of the objective function.
the security-
An important observation can be made that
driven data partitioning and placement strategy reduces the
number of affected transactions and the average recovery
time as we increase the number of partitions from a non-
partitioned case to a case of 10 and 20 partitions, for the same
number of malicious transactions. Note, commit delays are
incurred by all distributed transactions. This delay increases
with the number of partitions. In particular, the commit delays
associated with the distributed malicious transactions results
in the reduction of the number of affected transactions as
mentioned earlier. However, this reduction is achieved at the
cost of increasing the average response time as depicted in
Figure 5(c). Consequently, better results in terms of damage
containment are obtained. On the other hand, as observed in
Figure 5(c) increasing the number of partitions results in larger
communication overhead which subsequently leads longer
average response time. In conclusion, the results show that the
security-driven architecture with AIMS functionality achieves
a trade-off between damage containment and performance. We
expect that an improved solution for the optimization problem
can further improve this trade-off.

VI. CONCLUSION

With the rapid growth of data-intensive applications, the
demand on using secure and efﬁcient cloud data centers
increases. We have proposed a security-driven architecture for

6

(a) Number of affected transactions
with varying values of m and λ.

(b) Average recovery time
with varying values of m and λ.

(c) Average response time
with varying values of m and λ.

Fig. 5: Performance results of AIMS using various benchmark-
ing parameters.

multi-tenant distributed transactional database in CDCs. The
proposed architecture integrates the functionalities of a generic
adaptive intrusion management systems with the capability to
detect, respond, and recover from intrusion attacks. Further-
more, we have proposed a security-driven and performance-
driven data partitioning and placement technique to support
multi-tenant workload. Our future work will consider the
impact of various cloud infrastructure aspects, for example
networking and virtualization, on the performance of the dis-
tributed intrusion management. Further study on such aspects
will lead to design CDCs with improved trade-off.

ACKNOWLEDGEMENT

This research was supported by the grants from Northrop
Grumman Corporation and US National Science Foundation
(NSF) Grant IIS-0964639.

REFERENCES

[1] M. Ali, S. U. Khan, and A. Y. Zomaya, “Security and dependability of
cloud-assisted internet of things,” IEEE Cloud Computing, vol. 3, no. 2,
pp. 24–26, 2016.

[2] Z. ´A. Mann, “Allocation of virtual machines in cloud data centers: a
survey of problem models and optimization algorithms,” Acm Computing
Surveys (CSUR), vol. 48, no. 1, p. 11, 2015.

[3] K. A. Kumar, A. Quamar, A. Deshpande, and S. Khuller, “Sword:
workload-aware data placement and replica selection for cloud data
management systems,” The VLDB Journal The International Journal
on Very Large Data Bases, vol. 23, no. 6, pp. 845–870, 2014.

[4] S. Das, D. Agrawal, and A. El Abbadi, “Elastras: An elastic, scalable,
and self-managing transactional database for the cloud,” ACM Transac-
tions on Database Systems (TODS), vol. 38, no. 1, p. 5, 2013.

[5] A. Turcu, R. Palmieri, B. Ravindran, and S. Hirve, “Automated data
partitioning for highly scalable and strongly consistent transactions,”
IEEE transactions on parallel and distributed systems, vol. 27, no. 1,
pp. 106–118, 2016.

[6] D. Puthal, S. Nepal, R. Ranjan, and J. Chen, “Threats to networking
cloud and edge datacenters in the internet of things,” IEEE Cloud
Computing, vol. 3, no. 3, pp. 64–71, 2016.

 0 500 1000 1500 2000 100 500 1000 1500Number of affected transactionsIDS delay (ms)m=100_1 partitionm=500_1 partitionm=750_1 partitionm=100_10 partitionsm=500_10 partitionsm=750_10 partitionsm=100_20 partitionsm=500_20 partitionsm=750_20 partitions 100 200 300 400 500 600 100 500 1000 1500Average recovery time (ms)IDS delay (ms)m100_1 partitionm500_1 partitionm750_1 partitionm=100_10 partitionsm=500_10 partitionsm=750_10 partitionsm=100_20 partitionsm=500_20 partitionsm=750_20 partitions 0 20 40 60 80 100 100 500 1000 1500Average response time (ms)IDS delay (ms)m=100_1 partitionm=500_1 partitionm=750_1 partitionm=100_10 partitionsm=500_10 partitionsm=750_10 partitionsm=100_20 partitionsm=500_20 partitionsm=750_20 partitions[7] A. Almutairi, M. Sarfraz, S. Basalamah, W. Aref, and A. Ghafoor,
“A distributed access control architecture for cloud computing,” IEEE
software, vol. 29, no. 2, pp. 36–44, 2012.

[8] Z. Tan, U. T. Nagar, X. He, P. Nanda, R. P. Liu, S. Wang, and J. Hu,
“Enhancing big data security with collaborative intrusion detection,”
IEEE cloud computing, vol. 1, no. 3, pp. 27–33, 2014.

[9] A. Sallam, E. Bertino, S. R. Hussain, D. Landers, R. M. Leﬂer, and
D. Steiner, “Dbsafe: an anomaly detection system to protect databases
from exﬁltration attempts,” IEEE Systems Journal, vol. 11, no. 2, pp.
483–493, 2017.

[10] P. Ammann, S. Jajodia, and P. Liu, “Recovery from malicious transac-
tions,” IEEE Transactions on Knowledge and Data Engineering, vol. 14,
no. 5, pp. 1167–1185, 2002.

[11] R. Chandra, T. Kim, M. Shah, N. Narula, and N. Zeldovich, “Intrusion
recovery for database-backed web applications,” in Proceedings of the
Twenty-Third ACM Symposium on Operating Systems Principles. ACM,
2011, pp. 101–114.

[12] D. R. M. M. L. Pardal and M. Correia, “Rectify: Black-box intrusion
the 18th International

recovery in paas clouds,” in Proceedings of
Middleware Conference. ACM, 2017.

[13] K. Bai and P. Liu, “A data damage tracking quarantine and recovery
(dtqr) scheme for mission-critical database systems,” in Proceedings of
the 12th International Conference on Extending Database Technology:
Advances in Database Technology. ACM, 2009, pp. 720–731.
[14] L. Golab, M. Hadjieleftheriou, H. Karloff, and B. Saha, “Distributed
data placement to minimize communication costs via graph partitioning,”
in Proceedings of the 26th International Conference on Scientiﬁc and
Statistical Database Management. ACM, 2014, p. 20.

[15] D. E. Difallah, A. Pavlo, C. Curino, and P. Cudre-Mauroux, “Oltp-
bench: An extensible testbed for benchmarking relational databases,”
Proceedings of the VLDB Endowment, vol. 7, no. 4, pp. 277–288, 2013.

7

Muhamad Felemban (mfelemban@purdue.edu) is an Assis-
tant Professor in the Computer Engineering Department at
KFUPM, Saudi Arabia. He has a PhD Degree from the School
of Electrical and Computer Engineering at Purdue University.
His research interests include security and privacy of data, IoT,
and distributed systems. He is a student member of IEEE.
Anas Daghistani (adaghist@purdue.edu) is a PhD. candidate
in the School of Electrical and Computer Engineering, Purdue
University. His research interests include database, distributed
systems, big data management, and database security. He is a
student member of IEEE.
Yahya Javed (yjaved@purdue.edu) is a PhD student in the
School of Electrical and Computer Engineering, Purdue Uni-
versity. His research interests include intrusion tolerance and
recovery in distributed computing systems. He is a student
member of IEEE.
Jason Kobes (Jason.Kobes@ngc.com) works as a Principal
Cyber Architect & Research Scientist in Washington, DC for
Northrop Grumman Corporation. Jason has over 20 years of
experience concentrated in information systems design ana-
lytics, business/mission security architecture, enterprise risk
management,
information assurance research, and business
consulting. Jason has a Master’s of Science in Information
Assurance (MSIA) and a Bachelor’s of Science in Computer
Science from Iowa State University.
Arif Ghafoor (ghafoor@purdue.edu) is a professor in the
School of Electrical and Computer Engineering at Purdue Uni-
versity. His research interests include: multimedia information
systems, database security, and distributed computing. He is a
Fellow of IEEE.

