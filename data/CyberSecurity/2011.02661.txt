Knowledge-Base Practicality for Cybersecurity
Research Ethics Evaluation

Robert B. Ramirez1,a) Tomohiko Yano1 Masaki Shimaoka1
Kenichi Magata1

Abstract: Research ethics in Information and Communications Technology has seen a resurgence in pop-
ularity in recent years. Although a number of general ethics standards have been issued, cyber security
speciﬁcally has yet to see one. Furthermore, such standards are often abstract, lacking in guidance on spe-
ciﬁc practices. In this paper we compare peer-reviewed ethical analyses of condemned research papers to
analyses derived from a knowledge base (KB) of concrete cyber security research ethics best practices. The
KB we employ was compiled in prior work from a large random survey of research papers. We demonstrate
preliminary evidence that such a KB can be used to yield comparable or more extensive ethical analyses of
published cyber security research than expert application of standards like the Menlo Report. We extend the
ethical analyses of the reviewed manuscripts, and calculate measures of the eﬃciency with which the expert
versus KB methods yield ethical insights.a

a To appear in Computer Security Symposium 2020 (CSS 2020). Readers viewing this document after
10/19/2020 please refer to the most recent version on arXiv.org. Proceedings to be made available at:
https://www.iwsec.org/css/2020/proceedings.html.

Keywords: Ethics, Knowledge Base, Literature Review, Usable Security, Deontic Logic

1.

Introduction

Research ethics in Information and Communications
Technology (ICT) has seen a resurgence in popularity in
recent years, spurred in part by Artiﬁcial Intelligence (AI)
[6,12]. Although a number of general standards have been is-
sued in the past decade, there are currently no easily usable,
granular, or comprehensive benchmarks for evaluating the
ethics of cyber security research projects. There is also cur-
rently no method for evaluating security research ethics in a
truly systematic or reproducible manner [2, 5, 20, 23, 32, 36].
Existing frameworks for ethics in ICT research are ab-
stract in that they either focus on the ethical assessment
process as a whole, give only general advice, or do not focus
on security. In reality, researchers have to deal with concrete
ethical dilemmas on a variety of topics, as evidenced by the
prevalence of ‘ethical issues’ sections in research papers [16].
As a result, despite ethical analyses being demanded by
many top conferences [14], traditionally, committees or In-
ternal Review Boards (IRBs) composed of experts have been
necessary to comprehensively review the ethics of papers,
which has been seen as a domain requiring signiﬁcant ex-
pertise, often from senior members of the research commu-
nity [1, 8]. However, many IRBs lack experience speciﬁcally

1

a)

SECOM CO., LTD. Intelligent Systems Laboratory, Mitaka,
Tokyo, Japan
ro-ramiresu@secom.co.jp

evaluating ICT research, particularly in a cyber security con-
text [19, 36, 45]. This frequently leads to ICT research being
exempted from IRB review [25, 31, 40]. Thus, researchers
need to be able to evaluate their own research. This poses
a barrier for those with less experience with ethics reviews.
In this paper we perform a best-case scenario test of the
ability of security researchers without ethics committee ex-
perience to evaluate research ethics. We employ a knowledge
base (KB) of concrete cyber security research ethics best
practices, compiled in [30,41] from a large random survey of
research papers. We contribute 1) evidence that such a KB
can be used by non-experts to yield ethical analyses of pub-
lished cyber security research comparable to those current
benchmarks yield in the hands of experts, and 2) a novel
approach for comparing these two analysis methods.

We assess two separate ethically-condemned research pa-
pers and corresponding published ethical analyses written by
experts about them, and show that the principles collected
in the knowledge base yield, depending on the calculation
method, between 3 and 12 times as extensive an analysis
as those given by the experts, after accounting for redun-
dant and irrelevant information. We deﬁne and measure the
coverage and the eﬃciency of both the expert reports and
the KB-based analyses by systematically extracting and or-
ganizing the claims made in the expert reports and those
made with the KB, to make them comparable. The KB
analyses encompassed the vast majority of the observations

found in the expert reports, and further yielded a number of
novel insights expanding on the ethics of the original papers.

2. Background

2.1

ICT Ethics Guidelines and Related Work
There are a number of existing ICT guidelines relevant to
cyber security research ethics. Here we highlight some of
the more well known ones and their relation to our work.

The Menlo Report was released in mid-2012 as ‘guidance
for ICT researchers’ in ‘the context of ... information secu-
rity research’ [20]. It was inspired by the Belmont Report
developed in the 1970s for medical ethics. When formal
ethical discussions are included in cyber security research
papers, the Menlo Report or its principles are sometimes
referenced [22]. It includes detailed high-level guidance on
the types of considerations necessary when conducting ethi-
cal research, including stakeholder analysis, respect for per-
sons, beneﬁcence, justice, and the public interest.

The Menlo Report gives a few example applications of its
principles to cyber security research, with hypothetical ex-
amples that involve phishing, vulnerability disclosure, and
handling sensitive information; but it does not describe the
details of the ethics of those actions themselves. While the
Menlo Report and its Companion [19] provide a methodol-
ogy for ethics, they do not commit to recommending con-
crete best practices for speciﬁc activities. The Companion
does, however, include a synthetic case study that goes into
detail about how to apply its guidelines to a few speciﬁc
hypothetical research actions, but it is limited in scope, or-
ganized as prose, and lacks speciﬁc references to research.

A tool called ‘CREDS’ was at one point in development
by a disjoint set of researchers aﬃliated with the authors of
the Menlo Report and the U.S. Department of Homeland
Security [11]. The stated goal of this tool, ﬁrst proposed in
2015, seems similar to [30, 41] (the subject of this paper) in
that it seeks to somehow analyze best practices, as well as
laws, to create an online ethics tool [33].

2.2 Cyber Security Ethics Knowledge Base

In [30, 41], the authors extracted descriptions of ethical
practices from 101 relevant papers out of a collection of 943
published in the top conferences in cyber security between
2013-2017. The ﬁndings were compiled into a knowledge
base (KB) using a decision tree structure (Figure 1) [3, 41].

Fig. 1: Decision Tree Sample

2.2.1 Structure

Each branch of the tree terminates with leaves that, along
with their branches, specify the ethics of actions (A) a re-
searcher might consider taking. A given path from root to
leaf is meant to be human-readable as a full statement of
actions. There are 5 types of leaves: “Permitted”, “Prohib-
ited”, “Demanded”, “Gray,” and “Recommended,” based on
traditional Deontic Logic [39]:

( 1 ) Permitted means performing A is not in itself unethical
( 2 ) Prohibited means performing A is in itself unethical
( 3 ) Demanded means not performing A is itself unethical
( 4 ) Gray means A could be either Permitted or Prohibited
( 5 ) Recommended indicates Permitted or Demanded.

Gray and Recommended are “TBD” placeholders indicat-
ing a lack of consensus on the ethics of A.

3. Ethically-Condemned Manuscripts

In this paper we use [30, 41]’s KB to analyze the ethics
of speciﬁc research actions described in research papers that
have been condemned as unethical by the community, and
compare our analyses to the corresponding published expert
ones, which used the Menlo Report and the Association of
Internet Researchers (AoIR) Recommendations [36]. Here
we give a description of the papers and their controversies,
based in part on our own analyses (Section 6).

3.1

Internet Census 2012

Internet Census 2012 (Carna Botnet) [17] is a 2012 non-
peer-reviewed paper by an anonymous hacker about an
illegally-conducted survey of insecure machines [15, 34, 44].
The Carna Botnet creator built a port scanner to scan
Its eﬃciency was enabled
the majority of the Internet.
by piggybacking oﬀ of insecure devices to propagate their
malware-like program, exponentially increasing the number
of nodes performing scans. The paper was published before
other eﬃcient and comparatively more ethical solutions like
Zmap [27], so it may have been considered novel at the time.
Among the ethical malpractices included in Internet Cen-
sus 2012’s research are not only breaking into devices, but
also releasing all the data from the scans. Because some have
considered Carna a “nice” botnet, there has been discussion
about whether it is ethical to use its data in mainstream
research [4, 38, 43]. The Internet Census 2012 author made
some eﬀort to reduce the burden imposed on devices by lim-
iting the number of simultaneous connections the scanner
can make, avoiding probing further into networks beyond in-
secure routers, and even removing malware found on devices
if it interfered with Carna’s scans. However, the laundry list
of illegal and unethical practices in deploying Carna made
the paper an immediate target for condemnation.

3.2 Encore

Encore: Lightweight Measurement of Web Censorship
with Cross-Origin Requests [21] details an experiment in
which the authors released a program for web hosts that uses

cross-origin requests to fetch content from separate websites
that are allegedly censored in certain countries, in order to
conﬁrm what is censored to whom. Amassing data from an
array of vantage points in this way enables the authors to
perform longitudinal measurements of censorship.

The controversy in the paper lies mainly in the fact that
unsuspecting users navigating to otherwise safe websites
could be served censored content, putting them at risk with
governments. Furthermore, visitors’ information, such as
their IP address (considered personal identiﬁable informa-
tion under the GDPR [13,24]), is sent directly to the authors
of Encore, without obtaining users’ informed consent.

The oﬃcial version of the paper on SIGCOMM’s web-
site includes a statement from the program committee (PC)
stating that while it appreciates the technical contributions
of the paper, there are a number of ethical concerns with the
experiments the authors conducted that lead the PC to be
unable to endorse it, and that the controversy surrounding
the paper arose in large part because of the lack of ethics
standards for this type of research. The statement continues
that because the authors engaged with their IRB and yet the
IRB did not ﬂag the research as unethical, the PC elected
to publish the paper as a case study for the community.

4. Proposal

To enable non-experts to perform ethics analyses that
more eﬃcient and comprehensive than experts’, we propose
that it ought to be feasible to compile a knowledge base with
suﬃcient mention of ethical issues (coverage) for such a pur-
pose by sampling from research papers and ethics standards.
In this paper we use a KB compiled from peer-reviewed pa-
pers [30, 41]. If the KB has a concrete level of granularity
and much greater coverage (as compared to current abstract
standards) based on issues researchers actually encounter,
use of such a KB should yield more comprehensive and eﬃ-
cient ethical analysis results than expert reviews using cur-
rent ethics standards. This is because the KB is based on
similar cases to the research under evaluation.

Current standards require signiﬁcant extrapolation to
draw conclusions from, and despite their existence there are
still many ethical questions in the security research commu-
nity, indicating a deﬁciency in guidance [9, 10].

Presumably, committees formed from people with sig-
niﬁcant experience reviewing research papers and applying
standards to diﬀerent cases are the gold standard for ethical
review [1, 8]. These experts likely gained much of their ex-
pertise by reviewing research papers and proposals. There-
fore, we posit that a good measure of the quality of the
KB is how well it performs against such an expert armed
with accepted standards like the Menlo Report. To account
for the inﬂuence of experience, a member of our team with
no experience serving on review committees performed the
analysis that used the KB. Our comparison includes eval-
uations of both comprehensiveness and information added,
which accounts for noise and redundancy.

5. Experiment

In this section we outline the experiment we performed to
compare the coverage and eﬃciency of the KB versus expert
review. For this experiment our notion of eﬃciency includes
a large percentage of reported observations actually being
related to ethics (low noise), and a low percentage of re-
peated or rephrased observations (low redundancy). In this
paper we use the terms item and observation mostly inter-
changeably. An item is a speciﬁc identiﬁable point we use
for quantifying analyses; an observation refers more gener-
ally to the concept of an ethic issue identiﬁed in an item.

5.1 Experiment Procedure

Since an ethical analysis with more comprehensive cover-
age of ethics-related observations can be considered better,
we performed an experiment consisting of three parts in or-
der to determine how the coverage (after accounting for noise
and redundancy) of analyses yielded by the KB compares to
that obtained when using current standards (Figure 2). One
can imagine that, at a minimum, obtaining the same result
as an expert analysis would be a successful result, given that
the KB analysis is performed by an ethics amateur.

To assess coverage we compare the number of ethical ob-
servations yielded by the KB to those made by experts in
published ethical critiques for the same research projects.

To eliminate noise in the analyses, a blind majority vote
between three researchers is used to classify out-of-scope
items. We report the coverage from each approach before
and after accounting for redundant items by determining
whether an items’ observations are covered by other ones.

We reason that if the noise- and redundancy-adjusted cov-
erage obtained from using the KB matches or exceeds that
of the traditional expert reviews for all case studies, then
the KB approach is an eﬀective ethical analysis method.
5.1.1 Paper Selection

In 2018 we performed a literature review to ﬁnd appli-
cations of ICT or security ethics standards to case studies.
In our literature review we were unable to ﬁnd case studies
of any substantial scale that used ethics standards besides
the Menlo Report. As a result, we limited our search to
critiques published after 2012, which could have used the
Menlo Report in their analyses. This review turned up only
two critiques: ones of the papers in Section 3. Both used
the Menlo Report as the basis for their analysis. The Encore
Report also drew on the AoIR’s ethical recommendations.

The Internet Census 2012 critique was written by an au-
thor of the Menlo Report and is peer-reviewed. The Census
itself was published only shortly after the Menlo Report, so
the author may have been unable to refer to it, however.

The Encore critique is a federally-funded paper written by
the person who ﬁrst pointed out Encore’s ethical issues to
ACM SIGCOMM, giving it some form of peer-acceptance.
It has been cited more than the Internet Census 2012 ethi-
cal critique. In this experiment we analyze both the original

Unethical Research 
Papers (O)

Expert with our
Knowledge Base

Set of 
observations 
generated by T
DT

x2

x2

T

E

(cid:722)5.2

Redundancy 
Adjustment

Eliminate 
redundancy by 
grouping similar 
observations 
together

T

Majority vote to 
decide out-of-scope 
observations

DE

X

T

L

Noise 
Adjustment

Critiques of 
Unethical Papers (C)

Cyber Security 
Research Ethics 
Expert

Set of 
observations 
written in and 
extracted from 
reports by E

L

DT(cid:224) DE &  DE(cid:224) DT

((cid:722)5.1) Map 
corresponding 
observations

Score 
Coverage

Fig. 2: Comparing Analyses Using a Knowledge Base Versus
Existing Standards

research papers, O, and their published ethical critiques, C.
The ACM Code of Ethics (CoE) also has on its website
very short hypothetical research case studies [7]. We initially
considered using these as well, but because of their artiﬁcial
nature and unrealistic length, we determined that it would
be impossible to perform a fair comparison between the KB
and the ACM CoE through them. This is not to say that
the ACM case studies are poor examples, only that they are
not suited for our comparison.
5.1.2 Data Extraction

To assess coverage we compare the number of ethical ob-
servations made by C and the KB with respect to O. There
are three main roles members of our team play in the com-
parison. The ﬁrst role is a labeler, L, who compares the KB
approach to the “manual” expert approach. L compares the
datasets DE and DT, which are organized sets of observa-
tions extracted by analyzing C and O, respectively.

DT is the dataset, in spreadsheet format, of ethical obser-
vations regarding the ethics of the original papers O (one
spreadsheet of observations per paper). DT is created by
using the KB to assess the papers without consulting C.
The creator of DT, fulﬁlling the second main role, is called
T (Figure 2), named for the Decision Support (T)ool.
In
order to ensure a best-case comparison, T is someone who
has signiﬁcant familiarity with the KB.

In order to conduct a fair comparison between cur-
rent standards and [30,41]’s ethical best-practices gathering
methodology (Section 2.2), for this analysis we excluded the
items in the KB that were solely derived from ethics stan-
dards, such as the Menlo Report and ACM CoE, and used
only those items derived from surveying the literature.

DE is the set of observations extracted from the ethical cri-
tiques C. The creator of this dataset, called E, for (E)xpert,
has signiﬁcant experience on security ethics and review com-
mittees. E creates each DE spreadsheet by reading C and
recording the authors’ noted observations in a spreadsheet
format that resembles DT. The role of E’s is merely that of
a transcriber of the results of the standards-based methodol-
ogy already applied by the authors of C, who have a degree
of expertise in ICT or cyber security ethics. Therefore E is

instructed to refer to O only for clariﬁcation.
5.1.3 Mapping

A third party L labels the unique and shared observations
from DE with respect to DT, then repeats going from DT
to DE, for each of the two papers. In each case, the labeling
is said to go from the “primary” set to the “secondary set,”
whose items are referenced in the labels. L does not read C,
and only reads the introductions from O, for context. Our
preliminary tests showed this basic background knowledge
to be necessary for comparing items.

This two-sided mapping is done for two reasons. First,
tractability, because each set of analysis results contained
on the order of 30 to 90 results, requiring a systematic ap-
proach for consistency in labeling. This simpliﬁes the scor-
ing of the ﬁnal results. Second, to mitigate bias arising from
considering matches from the point of view of only one side.

• 1: Used if no other labels apply. The item is in-scope,
and no items in the secondary set correspond to it.
• +α: There is some similarity, shared implied meaning,
or overlap between the items in question. Speciﬁcally,
the secondary item is included in the primary one, which
adds some value. L must be able to articulate the re-
maining diﬀerence, though, or the label becomes ∅.
• ∅: The intended meaning of the primary item seems
similar to the extent that the diﬀerence between the
secondary item cannot be easily articulated. This la-
bel should be used even if the primary item is “−α”
with respect to the secondary item - that is, even if it
is entirely included in the (inferred) intended meaning
of the second item. This is because comparisons are
done one-sidedly, such that only the primary set mat-
ters for this label. The secondary set’s contributions are
enumerated when sets are reversed.
/∈ S (determined by a vote of three): The item did not
contain a description of any of: something researchers
ought to do or take into consideration, an explicit state-
ment that something is unethical or obligatory, or a risk
of a negative externality to a party. For fairness, L was
instructed to label items that appear +α or ∅ as in-
scope, regardless. For brevity we use the set notation
∈ S to mean in-scope and /∈ S for out-of-scope.

•

5.2 Scoring
5.2.1 Coverage

Each of DE and DT is evaluated based on its number of
unique ethics observations between the two datasets, by as-
signing points to it. The labeling system (above) is divided
into labels that award points, and labels that do not.

S+ (Awards Points) items include 1 and +α and contain
some unique value-added ethical observation by the primary
set. Because a brand new item (1) is more unique than a
slightly expanded item (+α), one may wish to weight these
two types of items diﬀerently. Within these two types, all
contributions are awarded 1 point equally. To avoid bias
against items with multiple observations, during the cre-
ation of DE and DT, E and T made eﬀorts to ensure that

all items are maximally subdivided, to eliminate these cases.
S0 (Does Not Award Points) labels include /∈ S and ∅,
and indicate no unique value added by the item, and do not
result in points for the approach the primary set represents.
5.2.2 Noise

To eliminate “noise” in the analyses, we had two addi-
tional researchers besides L independently label out-of-scope
nodes, and we then used the majority decision between those
three researchers to classify items as ∈ S or /∈ S.

Having this vote is necessary since coverage results depend
heavily on whether an item is labeled ∈ S. Since the KB
should theoretically have no out-of-scope items, for a given
/∈ S item in DE, if there is no corresponding ( /∈ S) item in
DT, the primary other label competing for that item would
be 1, so such items are highly likely to subsequently be cat-
egorized as 1 by L. This means DE could gain an advantage
if it presents a lot of /∈ S items hoping to get some past L.
/∈ S items were especially prevalent in the Encore critique.
This was a result of it being structured to include a general
overview of the Encore program in the ﬁrst half of the paper.
5.2.3 Redundancy

Throughout the experiment it became clear that a sub-
stantial number of items in DE, and a few items in DT, were
completely or signiﬁcantly redundant, resulting in a discon-
nect between the granularity of claims in the two sets. To
ensure we measure the amount of novel
information each
ethical analysis approach contributes, we also compared DE
and DT after accounting for redundant observations.

To eliminate this within-set redundancy and present all
items at similar level of granularity, a researcher (T in this
experiment), separate from the primary labeler L, grouped
together similar items within each of the four sets (two from
DE, two from DT). This was done after the initial labeling.
Although in many cases redundant items within a set hap-
pened to have the same labels, the following rules were then
used to select the ﬁnal label for a given set of redundant
items that had multiple labels:

( 1 )

( 2 )

If an item is a member of multiple redundant groups,
the label associated with that item is not a reliable indi-
cator of the individual group’s label, since it is unclear
which group that label actually applies to.
If one of the items was linked to an extremely large
number of items in the secondary set, its label was not
used as it could be expected to be too broad.

( 3 ) Connections between items were then emphasized,
yielding the priority (+α → ∅) → ( /∈ S → 1), where
items labeled +α and ∅ had priority because there were
items in the secondary set associated with them, unlike
/∈ S and 1. The highest priority label among a group’s
items was chosen as the ﬁnal label.
If there were an equal number of singular items labeled
as ∅ and 1, as a compromise +α was selected.

( 4 )

We prioritized +α over ∅ because if a group contributed +α
from one item solely belonging to that group, and another
item that did not contribute a +α (i.e. that was only ∅),

then we assume that ‘+α’ + ‘∅’ = ‘+α’ the same way that 1
+ 0 = 1, using the reasoning that ‘S+’ + ‘S0’ = ‘S+’. By the
same logic we treated ‘1’ + ‘∅’ = ‘S+’, but we equated this
to +α because, due to the ∅, there were still clearly some
aspects of the item that were the same, and by the deﬁni-
tion of +α given in Section 5.1.3, the focus is on whether
there is some similarity between the items (as there is with
∅). As described in Section 5.1.3, 1 was only selected as a
last resort (regardless of whether T or E was the primary
set). For /∈ S and 1, because of the risk mentioned above
regarding noise, we prioritized /∈ S.

6. Results

We report adjusted results to the calculated coverage after
accounting for noise and redundancy sequentially. By com-
paring original vs adjusted coverage we can understand the
raw amount of reported observations from each approach as
well as how eﬃcient each is at generating observations.

Table 1. Observations from Expert Versus Knowledge Base

Raw Score

No /∈ S

No Redundancy 　

E

T

T/E

E

T

T/E

E

T

T/E

Census

∈ S

1
+α
∅

/∈ S
G1

S+2
%N3
Encore

∈ S

1
+α
∅

/∈ S
G1

S+2
%N3

26

10
9
7

15
41

19
.46

41

7
13
21

40
81

20
.25

36

19
16
1

0
36

35
.97

34

20
5
9

0
34

25
.74

1.4

0
.88

1.8
2.1

.83

0
.42

1.3
3.0

23

8
8
7

18
41

16
.39

30

1
12
17

51
81

13
.16

36

19
16
1

0
36

35
.97

34

20
5
9

0
34

25
.74

1.6

0
.88

2.2
2.5

1.1

0
.42

1.9
4.6

13

4
5
4

18
31

9
.29

13

0
4
9

51
64

4
.06

25

12
12
1

0
25

24
.96

25

12
7
6

0
25

19
.76

1.9

0
.81

2.7
3.3

1.9

0
.39

4.8
12

1 T :=∈ S+ /∈ S;

2 S+ := 1 + α;

3 %N(%NEW ) := T /S+

Table 1 shows the detailed results of labeling the ethics
observations, for both the Internet Census (top) and Encore
(bottom). Results are reported to two signiﬁcant ﬁgures.
Raw Score, No /∈ S, and No Redundancy indicate the raw
number of items found by E and T (calculated from L’s la-
beling of DE and DT) for unadjusted coverage, the adjusted
numbers after accounting for noise, and the adjusted num-
bers after further accounting for redundancy, respectively.
The T/E (%) columns show a comparison of the two analy-
sis approaches. For numbers of interest, they give the ratio
T:E as a percentage of the two columns immediately to the
left of each of the respective T/E (%) columns in table 1.
The ∈ S rows show the sum of all the 1, +α, and ∅ items
for a given analysis. (G) shows the total number of items
using each of the E and T approaches. They are the sum
of their respective /∈ S and ∈ S items for each of the two

papers (light blue). The value in row (G) always remains
unchanged until redundancy is accounted for, when similar
observations are combined into a single item.

The top light-gray rows show the sums of all S+ items (1
and +α) for each stage of the calculation. These rows give
an indication of the total coverage of ethical issues yielded by
each approach. ∅ items are contributed by both approaches
and therefore do not award any points. The bottom light
gray rows (% NEW) are a measure of the eﬃciency of each
of the approaches. They show, as a percentage, the fraction
of items reported by each approach that are unique to that
approach, rather than being shared (∅) or irrelevant ( /∈ S).
Bold numbers indicate changes across columns. For ex-
ample, in No /∈ S, the number of items from T remained
unchanged, but E saw a slight decrease in their ∈ S items.
The individual changes to the 1 and +α rows are shown in
bold, but to avoid cluttering the table, the overall changes
are not repeated in ∈ S and /∈ S rows.

6.1 Summary of Results

Table 1 shows that the knowledge base approach yielded
substantially more novel information in the form of ethics
observations than the expert critiques based on existing
standards, despite yielding fewer observations overall.

Using DT, the Internet Census analysis shows gains rang-
ing from 80% (i.e. a T/E value of 180%) to 170% (i.e. a
T/E value of 270%), and the Encore analysis shows gains
ranging from 30% to 380%, depending on whether noise and
redundancy are accounted for. Gains are determined by sub-
tracting 100% from the calculated value of the ratio T/E.
In order to consider ethical observations as independent
events for statistical purposes, we must only consider unique
observations, so for that purpose it only makes sense to use
the results after accounting for redundancy. Thus, using the
KB, after accounting for redundancy we see a 170% gain
when assessing the Internet Census 2012, and a 380% gain
for Encore. Note that these gains are in the quantity of
exclusive, novel
information compared to the control ap-
proach with E. The exact new information provided diﬀers
between each approach.

A more accurate comparison between the two methods
may be to use the total number of ∈ S observations found,
including observations labeled as ∅ (i.e. observations found
by both approaches). These gains are more conservative, at
+90% for both the Census 2012 and Encore papers. Since
the KB approach T yields nearly twice as much information
as the traditional expert approach E, this can be viewed as
evidence for the eﬀectiveness of a knowledge base.

In addition, the eﬃciency (% NEW under No Redun-
dancy) was 230% and 1100% greater for the KB tool (T)
compared to the expert critiques (E), for the Internet Cen-
sus and Encore respectively. This may be the result of C in-
cluding a number of points that can not be said to be directly
related to ethics, but were nevertheless purposely included
as observations for the sake of making them more coherent
as publications. For E, in both cases %NEW dropped sig-

niﬁcantly when adjusting for noise and redundancy, from
46% to 29% for the Census, and from 25% to only 6% in the
case of Encore, whereas T showed little change in %NEW,
remaining high at 97% to 96% with the Census and increas-
ing from 74% to 76% in the case of Encore.

6.2 Contributions from the KB

DT contributed a number of novel observations to the eth-

ical analyses of O. We list the major (i.e., 1) ones below.

The KB has rules for data collection about computer sys-
tems, separate from its human subjects rules. This makes
it immune to questions about whether human subjects are
involved that could result in a committee stopping short of
In general the KB goes into
a thorough ethical analysis.
more depth on issues and the conditions surrounding them.
It also was better at pointing out Permitted actions in O.
6.2.1
• Collecting MAC addresses of devices is a Gray action.
• It is Recommended to ensure that traﬃc your assets
send during your research is not malicious. The Carna
botnet’s traﬃc can be considered malicious, but it is
unclear to what extent its creator assessed this.

Internet Census 2012

• According to Zmap’s principles, which have seen signiﬁ-
cant adoption [28,29,47], it was Demanded that Carna’s
creator coordinate closely with local network admins to
reduce risks and handle inquiries (but he did not).
• Demanded : Feasibly minimize data collected/stored
• Signing research agreements limiting the use of data to

the present experiment is Recommended

• Although it is common practice to anonymize data col-
lected from or that could potentially be linked back to
human subjects, the act of pseudonymizing, anonymiz-
ing, or aggregating collected data about computer sys-
tems being a Demanded practice is unique to the KB.

• Encrypting this data in transit is Recommended
• DT Recommends testing before deployment for bugs,
consistency, safety, etc., in software, hardware, and ser-
vices to be used on experiment participants. Even
though O implied some amount of testing for safety
before deploying their technology (“After development
of most of the code we began debugging our infrastruc-
ture. We used a few thousand devices randomly chosen
for this purpose.” [17]), C did not acknowledge this.
• DE and DT both mention that disseminating malware
is Prohibited (although the KB lists it as Gray if con-
sent is obtained). However, C and the Menlo Report it
was based on do not explicitly mention that infecting
computers with malware (i.e. installing and/or execut-
ing) is Prohibited. This point may have been taken as
a given by the authors of [26] and therefore excluded.

6.2.2 Encore
• Requiring experiment participants log into,

install
things onto, or otherwise employ their personal devices,
accounts, or systems is a Gray action in the KB.
• “Fairly compensate or pay subjects, including crowd-
sourced workers, for their contributions” is Demanded.

In O, the author did not mention compensating the true
targets of the study, the site visitors: “For further in-
centive, we could institute a reciprocity agreement for
webmasters: in exchange for installing our measurement
scripts, webmasters could add their own site to Encore’s
list of targets and receive notiﬁcation about their site’s
availability from Encore’s client population” [21].
• The Recommended practice of testing before deploy-
ment also came up in O but not C. However, in En-
core’s case, they satisﬁed this recommendation.

• C arguably touched on the fallibility of IRBs, but the
KB made explicit that adhering to REB/IRB rules but
not to other ethics standards is Prohibited.

• Data storage and deletion was not touched on in C.
DT included the observation that prioritizing any data-
driven aspects of research so as to minimize retention
time is Demanded.

• In addition, minimizing data retention by deleting data
as soon as possible after use (even if it must be collected
anew in case of error) is Recommended.

• Although a related point regarding deception was men-
tioned in C, DT added that collecting data as part of a
separate service not speciﬁcally for collecting that data
(e.g. which gives bonus incentives), is Prohibited when
dealing with human subjects, but Permitted if the data
is solely about computer systems (although not to the
exclusion of obtaining informed consent, etc.).

7. Discussion

There were some observations in C that T’s analysis of O

did not uncover. We detail them all below for completion.

Internet Census 2012 Critique

7.1
7.1.1 1
• C very interestingly noted that anonymous publication
is unethical because of transparency and accountabil-
ity [26]. This observation was not turned up by the re-
view of top conference papers used to generate the KB.
It is possible that such a “meta-insight” about publi-
cation itself can only be discovered from papers specif-
ically about ethics, such as C.

• Second, C touched on the ethics of releasing source
code, while DT noted that distributing malware is Gray.
These were deemed separate issues by L, so this obser-
vation was labeled unique to DE. Such discrepancies
can be resolved by adjusting the wording of the KB or
using multiple labelers (see Section 8.3).

7.1.2 +α
• Informed consent for releasing others’ data publicly.
Although this was also mentioned in DT separate from
consent, the KB contains a number of informed con-
sent provisions. In the future, informed consent can be
represented as meta-data for relevant nodes.

• The Census’s C noted that O excluded from their scans

“systems that could potentially harm Secondary Stake-
holders” as an example of not treating all subjects eq-
uitably. Relevant to this, the current KB only has data
on equitably (or randomly) assigning test conditions.

7.2 Encore Critique (+α only)
• C mentioned that O claimed it was infeasible to accu-
rately measure the potential risk of using Encore, and
noted that proceeding with research in this situation is
ethically questionable. The KB includes a number of
practices for minimizing risk, but does not explicitly
obligate assessing risk.

• The abstract principle of meeting users’ expectations.
Although present in spirit, this is not explicitly ad-
dressed in the concrete practices in the KB.

8. Limitations

8.1 Overlap Between Coverage and Out-of-Scope
Some items were voted /∈ S by the majority if they were
e.g. quotes from the Menlo Report that readers might infer
recommendations from – i.e. they weren’t truly novel.

The main mapping task also mixed in L’s judgment about
whether the item was out-of-scope. Because items voted ∈ S
then took L’s label, there were two cases from DE where L’s
/∈ S decision was defaulted to.

8.2 Potential to Overlook Information

As mentioned in Section 5.1.2 , observations extracted
from C were organized in a hierarchical format similar to
the KB’s. However, we did not include parent items sepa-
rately from children. Our labeling scheme therefore cannot
account for the case where both a parent and all of its chil-
dren are non-redundant and in-scope; although we do not
believe there were any observations in DE like this.

In addition, for Encore, there were observations unique to
C that referred to a FAQ on the project’s website not men-
tioned in O, which we therefore excluded from the results.

8.3 Subjectivity in Labeling

Labeling experiments necessarily involve subjectivity [35,
42]. Due to our team size, using more than a single la-
beler L was infeasible. Ideally two or three labelers would
resolve diﬀerences through discussion or an objective pro-
cess [18, 37, 46]. For example, [18] had 54 items coded by 2
coders, [37] had 3 coders for 14 questions from 15 interviews,
and [46] had 100 topic model labels and 2 coders.

9. Conclusion

We ﬁnd substantial potential in the use of an ethics knowl-
edge base sourced for research papers to provide the same
types of insights as an expert analysis using accepted ethics
standards, as well as to yield additional concrete insights
that are missed when using abstract, subjective guidelines.
In this paper we present preliminary evidence that such a
knowledge base enables one to create more comprehensive

reports of ethical issues than traditional standards do, with
greatly increased eﬃciency. We outline and implement a
process to systematically compare results generated from
using the two approaches, and ﬁnd that the knowledge base
from [30, 41] can be two to four times more eﬀective at lo-
cating ethical issues. We also identify deﬁciencies in existing
standards, including the extent of data handling recommen-
dations and interactions with computer test subjects.

Future work would involve extending this comparison be-
tween ethical analysis approaches from the two case studies
we used here to three or four, in order to improve its statis-
tical signiﬁcance; and conducting a more typical user study
to measure how various researchers perform when using ex-
isting standards versus the KB’s decision tree user interface.

References

[1]

[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

45 CFR § 46.107 IRB membership. (Jul 2018), https:
//www.law.cornell.edu/cfr/text/45/46.107, library Cata-
log: www.law.cornell.edu
ACM Code of Ethics and Professional Conduct (Jun 2018),
https://ethics.acm.org/
“A Cyber Security Research Ethics Decision Support User
Interface.” [Online]. Available: https://www.secom.co.jp/
isl/en/research/ethics/
Internet Census 2012 - Thoughts (Mar 2013), https://blog.
rapid7.com/2013/03/21/internet-census-2012-thoughts/
EC Council Code Of Ethics (Mar 2016), https://www.
eccouncil.org/code-of-ethics/
Ethically Aligned Design, First Edition (Dec 2017), https:
//ethicsinaction.ieee.org/
Case
2018),
integrity-project/case-studies/
Committee on the Use of Humans as Experimental Subjects
COUHES Committee Members and Staﬀ (2019), https:
//couhes.mit.edu/committee-members-and-staff
“Cyber-security Research Ethics Dialog & Strategy Work-
shop (CREDS 2013).” [Online]. Available: https://www.
caida.org/workshops/creds/1305/index.xml
“Cyber-security Research Ethics Dialog & Strategy Work-
shop (CREDS II - The Sequel).” [Online]. Available: https:
//www.caida.org/workshops/creds/1405/index.xml
IMPACT Cyber Trust Ethos (Feb 2018), https://www.
impactcybertrust.org/ethos

https://ethics.acm.org/

Studies

(Jul

[12] The Ethics in AI

Institute (Sep 2019), https://www.

schwarzmancentre.ox.ac.uk/Page/ethicsinai

[13] Personal data
personal-data/

(2019),

https://eugdprcompliant.com/

[14] USENIX Security ’20 Call

for Papers

(Oct 2019),

[15]

https://www.usenix.org/sites/default/files/sec20_
cfp_101519.pdf
at 23:14,
sets up illegal
420,000 node botnet for IPv4 internet map (Mar 2013),
https://www.theregister.co.uk/2013/03/19/carna_
botnet_ipv4_internet_map/

I.T.i.S.F..M..:

Researcher

[16] Akiyama, M.: 研究倫理に関して我々の置かれている状況. In:

SCIS 2017. SCIS (2017)

[17] Anonymous:

Internet Census 2012: Port scanning /0 us-
ing insecure embedded devices: Carna Botnet (Mar 2013),
http://census2012.sourceforge.net/paper.html

[18] Association for Computing Machinery: Regulators, Mount
Up! Analysis of Privacy Policies for Mobile Money Services
(2017), oCLC: 255559492

[19] Bailey, M., Dittrich, D., Kenneally, E.: Applying Ethical
Principles to Information and Communication Technology
Research: A Companion to the Menlo Report (2013)
[20] Bailey, M., Kenneally, E., Dittrich, D., Maughan, D.: The
Menlo Report. SSRN Scholarly Paper ID 2145676, Social Sci-
ence Research Network, Rochester, NY (Mar 2012), https:
//papers.ssrn.com/abstract=2145676

[21] Burnett, S., Feamster, N.: Encore: Lightweight measure-
ment of web censorship with cross-origin requests. In:
In
ACM SIGCOMM (2015)

[22] Carlini Pratyush Mishra, N., Vaidya, T., Zhang, Y., Sherr,

M., Shields, C., Wagner, D., Zhou, W.: Hidden Voice Com-
mands (2016)

[23] Chatila, R., Havens, J.C.: The IEEE Global Initiative on
Ethics of Autonomous and Intelligent Systems. In: Aldin-
has Ferreira, M.I., Silva Sequeira, J., Singh Virk, G., Tokhi,
M.O., E. Kadar, E. (eds.) Robotics and Well-Being, vol. 95,
pp. 11–16. Springer International Publishing, Cham (2019).
DOI: 10.1007/978-3-030-12524-0 2,

[24] Council of European Union: Council regulation (EU)
no 679/2016 (2014), https://eur-lex.europa.eu/legal-
content/EN/TXT/?uri=CELEX:02016R0679-20160504

[25] Dittrich, D.:

search Ethics
10.1177/1747016115583380,

The
11(4),

ethics of

192–210

social honeypots. Re-
2015). DOI:

(Dec

[26] Dittrich, D., Carpenter, K., Karir, M.: The Internet Cen-
sus 2012 Dataset: An Ethical Examination. IEEE Technol-
ogy and Society Magazine 34(2), 40–46 (Jun 2015). DOI:
10.1109/MTS.2015.2425592

[27] Durumeric, Z., Wustrow, E., Halderman, J.A.: ZMap: Fast
Internet-wide Scanning and its Security Applications p. 15
(2013). DOI: 10.5555/2534766.2534818

[28] Ensaﬁ, R., Winter, P., Mueen, A., Crandall, J.R.: Analyz-
ing the Great Firewall of China Over Space and Time. Pro-
ceedings on Privacy Enhancing Technologies 2015(1), 61–76
(2015). DOI: 10.1515/popets-2015-0005,

[30]

[29] Holz, R., Amann, J., Mehani, O., Wachs, M., Ali Kaafar,
M.: TLS in the Wild: An Internet-wide Analysis of TLS-
based Protocols for Electronic Communication. Internet So-
ciety (2016). DOI: 10.14722/ndss.2016.23055,
Inagaki, S., Ramirez, R., Shimaoka, M., Magata, K.:
In-
vestigation on Research Ethics and Building a Benchmark.
The Institute of Electronics, Information and Communica-
tion Engineers, Niigata, Japan (Jan 2018), https://www.
iwsec.org/scis/2018/program.html
Jackman, M., Kanerva, L.: Evolving the IRB: Building Ro-
bust Review for Industry Research p. 17 (2016)
Jim, T.: There is no standard of ethics in computer security
research (Aug 2014), http://trevorjim.com/there-is-no-
standard-of-ethics-in-computer-security-research/

[31]

[32]

[33] Kenneally, E., Fomenkov, M.: Cyber Research Ethics
Decision Support (CREDS) Tool. pp. 21–21. ACM Press
(2015). DOI: 10.1145/2793013.2793017, http://dl.acm.
org/citation.cfm?doid=2793013.2793017

[34] Kleinman, A.: The Most Detailed Map Of The Internet Was
Made By Breaking The Law (0400), https://www.huffpost.
com/entry/internet-map_n_2926934

[35] Lange, R.T.:

Inter-rater Reliability. In: Kreutzer, J.S.,
DeLuca, J., Caplan, B. (eds.) Encyclopedia of Clinical
Neuropsychology, pp. 1348–1348. Springer, New York, NY
(2011). DOI: 10.1007/978-0-387-79948-3 1203,

[36] Markham, A., Buchanan, E.: Ethical decision-making and
Internet research: Recommendations from the AoIR ethics
working committee (Version 2.0). AoIR (2012), http://
aoir.org/reports/ethics2.pdf

[37] McGregor, S., Charters, P., Holliday, T., Roesner, F.: In-
vestigating the Computer Security Practices and Needs of
Journalists (2015)

[38] McMillan, R.: Is It Wrong to Use Data From the World’s
First ‘Nice’ Botnet? Wired (May 2013), https://www.wired.
com/2013/05/internet-census/

[39] McNamara, P.: Deontic logic.

Zalta, E.N. (ed.)
The Stanford Encyclopedia of Philosophy. Metaphysics
Research Lab, Stanford University,
summer 2019 edn.
https://plato.stanford.edu/archives/sum2019/
(2019),
entries/logic-deontic/

In:

[40] Narayanan, A., Zevenbergen, B.: No Encore for Encore?
Ethical Questions for Web-Based Censorship Measurement.
SSRN Electronic Journal (2015). DOI: 10.2139/ssrn.2665148,
[41] R. Ramirez, S. Inagaki, M. Shimaoka, and K. Magata, “A cy-
bersecurity research ethics decision support UI.” USENIX
Association, Aug. 2020. [Online]. Available: https://www.
usenix.org/conference/soups2020/presentation/ramirez
[42] Smyth, P.: Bounds on the mean classiﬁcation error rate of
multiple experts. Pattern Recognition Letters 17(12), 1253–
1257 (Oct 1996). DOI: 10.1016/0167-8655(96)00105-5,
[43] Snoke, T.: Working with the Internet Census 2012
(Oct 2013), https://insights.sei.cmu.edu/cert/2013/10/
working-with-the-internet-census-2012.html

[44] Stocker, C., Horchert, J.: Mapping the Internet: A
Hacker’s Secret Internet Census. Spiegel Online (Mar 2013),
https://www.spiegel.de/international/world/hacker-

measures-the-internet-illegally-with-carna-botnet-
a-890413.html

[45] Thomas, D.R., Pastrana, S., Hutchings, A., Clayton,
R., Beresford, A.R.: Ethical
issues in research using
datasets of illicit origin. In: Proceedings of the 2017 In-
ternet Measurement Conference on - IMC ’17. pp. 445–
462. ACM Press, London, United Kingdom (2017). DOI:
10.1145/3131365.3131389,

[46] Weinberg, Z., Sharif, M., Szurdi, J., Christin, N.: Top-
ics of Controversy: An Empirical Analysis of Web Censor-
ship Lists. Proceedings on Privacy Enhancing Technologies
2017(1), 42–61 (2016). DOI: 10.1515/popets-2017-0004,
[47] Zhang, J., Durumeric, Z., Bailey, M., Liu, M., Karir, M.:
On the Mismanagement and Maliciousness of Networks. In:
NDSS (2014),

