Security: Doing Whatever is Needed...
and Not a Thing More!

Omer Katz
Technion
omerkatz@cs.technion.ac.il

Benjamin Livshits
Imperial College London
b.livshits@imperial.ac.uk

8
1
0
2

g
u
A
9

]

R
C
.
s
c
[

2
v
5
1
9
8
0
.
2
0
8
1
:
v
i
X
r
a

Abstract—As malware, exploits, and cyber-attacks
advance over time, so do the mitigation techniques
available to the user. However, while attackers of-
ten abandon one form of exploitation in favor of a
more lucrative one, mitigation techniques are rarely
abandoned. Mitigations are rarely retired or disabled
since proving they have outlived their usefulness is
often impossible. As a result, performance overheads,
maintenance costs, and false positive rates induced by
the diﬀerent mitigations accumulate, culminating in an
outdated, ineﬃcient, and costly security solution.

We advocate for a new kind of tunable framework
on which to base security mechanisms. This new frame-
work enables a more reactive approach to security
allowing us to optimize the deployment of security
mechanisms based on the current state of attacks. Based
on actual evidence of exploitation collected from the
ﬁeld, our framework can choose which mechanisms to
enable/disable so that we can minimize the overall costs
and false positive rates while maintaining a satisfactory
level of security in the system.

We use real-world Snort signatures to simulate the
beneﬁts of reactively disabling signatures when no evi-
dence of exploitation is observed and compare them to
the costs of the current state of deployment. Addition-
ally, we evaluate the responsiveness of our framework
and show that in case disabling a security mechanism
triggers a reappearance of an attack we can respond in
time to prevent mass exploitation.

Through a series of large-scale simulations that use
integer linear and Bayesian solvers, we discover that our
responsive strategy is both computationally aﬀordable
and results in signiﬁcant reductions in false positives,
at the cost of introducing a moderate number of false
negatives. Through measurements performed in the
context of large-scale simulations we ﬁnd that the time
to ﬁnd the optimal sampling strategy is under 2.5
minutes in the vast majority of cases. The reduction in
the number of false positives is signiﬁcant, about 20%
over traces that are about 9 years long (∼9.2 million
false positives).

I.

Introduction

Much of the focus in the security community in the
last several decades has been on discovering, preventing,
and patching vulnerabilities. While both new vulnerability
classes and new vulnerabilities are discovered seemingly
every day, the exploitation landscape often remains murky.
For example, despite buﬀer overruns, cross-site script-
ing (XSS), and SQL injection attacks (SQLIA) being her-

alded as the vulnerabilities of the decade [52], there is pre-
cious little published evidence of how commonly exploited
XSS or SQLIA might be in practice; of course, there is a
number of studies [37] on how vulnerability trends change
over time. One of the studies we present in this paper
suggests that, for example, XSS exploitation is not nearly
as common as would be suggested by the daily stream of
discovered vulnerabilities (found at www.openbugbounty.org/
or xssed.org).

Changing exploitation landscape: The security in-
dustry produces regular reports that generally present a
growing number of vulnerabilities, some in widely-deployed
software. It is exceedingly tempting to misinterpret this as
a growth trend in the number of actual exploits. However,
the evidence for the latter is scant at best. Due to a number
of defense-in-depth style measures over the last decade, in-
cluding stack canaries, ASLR, XRF tokens, automatic data
sanitization against XSS and a number of others, practical
exploitation on a mass scale now requires an increasingly
sophisticated attacker. We see this in the consolidation
trends of recent years. For example, individual drive-by
attacks have largely been replaced by exploit kits [38], [23],
[55]. In practice, mass-scale attacks generally appear to be
driven by a combination of two factors: 1) ease of exploita-
tion, and 2) whether attacks are consistently monetizable.
This is clearly diﬀerent from targeted attacks and APTs
where the upside of a single successful exploitation attempt
may be quite signiﬁcant to the attacker.

Given the growing scarcity in exploitable vulnerabili-
ties, there is some recent evidence that attackers attempt
to take advantage of publicly disclosed attacks right after
their announcement, while the window of vulnerability is
still open and many end-users are still unpatched; Bilge et
al. [17] report an increase of 5 orders of magnitude in attack
volume following public disclosures of zero-day attacks.
The situation described above leads to a long tail of at-
tacks — a period of time when attacks are still possible but
are increasingly rare. It is tempting to keep the detection
mechanism on during the long tail. However, it is debatable
whether that is a good strategy, given the downsides.
We argue that the usual human behavior in light of the
rapidly-changing landscape is inherently reactive, however,
often not reactive enough.

A. Mounting Costs of Security Mechanisms

One of the challenges of security mechanisms is that
their various costs can easily mount if unchecked over time.

 
 
 
 
 
 
• False positives. FPs have nontrivial security implica-
tions [53], [56]. According to a recent Ponemon Institute
report [49], “The average cost of time wasted responding
to inaccurate and erroneous intelligence can average
$1.27 million annually.” Futrthermore, “ Of all alerts,
19% are considered reliable but only 4% are investi-
gated.”

• Performance overhead. Our studies of scanning and
ﬁltering costs in Section I-A1 show that while the
IO overhead from opening ﬁles, etc. can be high, the
costs of scanning and ﬁltering increases signiﬁcantly as
more signatures are added to the signature set. When
many solutions are applied simultaneously within a piece
of software, the overhead of even relatively aﬀordable
mechanisms, such as stack canaries [22] and ASLR [47],
can be additive. There is a growing body of evidence that
security mechanisms that incur an overhead of 10% or
more do not tend to get widely deployed [59]. However,
several low-overhead solutions one on top of another can
clearly easily exceed the 10% mark.

The performance and false positive costs are in addition
to maintenance and update costs, which are also diﬃcult to
predict i.e. analyzing malware, testing signatures, running
honeypots, etc. [51], [50]).

We argue that as a result of the factors above, over
a long period of time, a situation in which we only add
security mechanisms is unsustainable due to the mounting
performance costs and accumulating false positives. This
is akin to performing a DOS attack against oneself; in the
limit, the end-user would not be able to do much useful
work because of the overhead and false positive burden of
existing software.

Reluctance to disable: At the same time, actively re-
moving a security mechanism is tricky, if only from a PR
standpoint. In fact, we are not aware of a recent major
security mechanism that has been oﬃcially disabled. One
of the obvious downsides of tuning down any security
mechanism is that the recall decreases as well. However,
it is important to realize that when tuning down a speciﬁc
mechanism is driven by representative measurements of its
eﬀectiveness in the wild, this is a good strategy for dealing
for mass attacks.

Targeted attacks, on the other hand, are likely to
be able to overcome most existing defenses, as we have
seen from Pwn2own competition, XSS ﬁlter bypasses [62],
[36], [15], etc. We hypothesize that sophisticated targeted
attacks are not particularly aﬀected by existing defenses.
However, reducing the level of defense may invite new
waves of mass attacks, which can be mitigated by upping
the level of enforcement once again. We therefore consider
entirely taking out a security mechanism, rather than
simply disabling or tuning it down, to be ill-advised.

1) Scanning and Filtering Costs: To demonstrate our
claims of the ineﬃciency and mounting costs of maintain-
ing a large amount of outdated security mechanism, we
turn to ClamAV [2], Snort [1], and Brave’s ad blocking
engine [7].

• We installed the latest version of the ClamAV en-
gine (0.99.2) and used it to scan a single ﬁle of 248 MB.

To understand how the running time is aﬀected by the
size of the signature set, we ran the scan several times
using diﬀerent sized subsets of the ClamAV signature
dataset. Figure 1a shows the obtained scan times, in
seconds.

• We performed similar measurements using the Snort
NIDS engine. Using the latest version of Snort (2.9.9.0),
we scanned a collection of pcap ﬁles, which we obtained
from [8], with signature sets of diﬀerent sizes. Figure 1b
shows the results of these measurements.

• Lastly, we used the ad-block engine of the Brave
browser [6], [7] to the eﬀect the ad-block rule set size
has on the average scan time per url. We used rules
from the EasyList and EasyPrivacy rule sets [3], total-
ing at 86,665 rules. Utilizing the Brave ad-block en-
gine (4.1.1), we scanned a set of URLs containing 15,507
elements. We ran the experiment using diﬀerent-sized
subsets of the rule set. Figure 1c shows the obtained
average scan times per URL, in milliseconds.

All three of the above ﬁgures clearly exhibits an approx-
imately linear correlation between the rule/signature set
size and the average scan time. These charts demonstrate
the hidden cost over time of adding signatures and rarely
removing them. This factor together with false positives
argues for removing signatures more aggressively.

B. Toward Tunable Security

In recent years we have seen growing evidence that
vulnerability statistics and exploit statistics are at odds.
Nayak et al. [45] report that despite an increase in reported
vulnerabilities in the last several years, while the amount of
exploitation goes down. Furthermore, only a portion of vul-
nerabilities (about 35%) actually gets exploited in a prac-
tical sense. Furthermore, vulnerability severity rankings
are often misleading, as they do not necessarily correlate
with the attractiveness or practicality of exploitation [11].
Barth et al. [14] advocate a reactive approach to security
where previous attack patterns are used to decide how to
spend the defender’s ﬁnite budget.

Methodology: In this paper, we experimentally prove the
advantages of reactive security. We look at widely deployed
security mechanisms, where the potential, for example, of
a single false positive is ampliﬁed by the potentially vast
installation base. A good example of such a system is an
anti-virus (AV) engine or an IDS. The reactive approach
to security is also supported by the number of zero-days
that are observed in the wild and reported by Bilge et
al. [17]. Our experimental evaluation in Section IV is based
on 9 years of Snort signatures. We argue that a well-
calibrated simulation is the best way we have to assess
the reactive security mechanism proposed in this paper.
We use an extremely valuable study of over 75 million
real-world attacks collected by Symantec [10] from 2015
to calibrate our simulation.

Tunable security mechanism: We propose an alter-
native: a tunable mechanism, guided by a strategy that
allows the defender to selectively apply the mechanism
based on internal and/or external sets of conditions. For
example, a centralized server can notify the client that

2

(a) Using ClamAV (0.99.2) to scan a sin-
gle 248 MB ﬁle.

(b) Using Snort (2.9.9.0) to scan a 4.1 GB
pcap ﬁle.

(c) Using Brave’s adblock engine (4.1.1) to
scan 15.5k URLs.

Fig. 1: Average scanning time as a function of the rule/signatures set size. The dashed lines represent linear ﬁts to the measurement.

certain categories of attacks are no longer in the wild,
causing the client to reduce their sampling rates when
it comes to suspicious traﬃc. Signatures are thus retired
when the threats they are designed to look for become less
prevalent. For this to work, there needs to be a greater level
of independence between the security mechanism and the
policy, the kind of separation that is already considered a
major design principle [9], [40].

Beyond signatures: It important to emphasize that
while we primarily experiment with signatures in this
paper, the ideas apply well beyond signatures. Speciﬁcally,
most runtime security enforcement mechanisms, albeit not
all, can be turned oﬀ, either partially or entirely. Mech-
anisms that rely on matching lists for their enforcement
include XSS ﬁlters [15] and ad blockers [16], [42], [44], to
name just a few. Similarly, one can apply XFI or CFI to
some DLLs, but not others, that have not been implicated
in recent attacks. Of course, the ability to adjust which
DLLs are CFI-protected depends greatly on having a dy-
namic software deployment infrastructure, which we claim
to be a desirable goal.

Matching today’s reality: In many ways, our proposal
is aligned with practical security enforcement practices of
adjusting the sensitivity levels for detectors depending on
the FP-averseness of the environment in which the detector
is deployed. The Insight reputation-based detector from
Symantec allows the user to do just that [57]. Our ultimate
goal here, of course, is to reduce the factors listed above,
i.e. the false positive rate and the performance overhead.

C. Contributions

Our paper makes the following contributions:

• Tunable. We point out that today’s approach to proac-
tive security leads to inﬂexible and bloated solutions over
time. We instead advocate a novel notion of tunable
security design, which allows ﬂexible and ﬁne-grained
policy adjustments on top of existing security enforce-
ment mechanisms. This way, the protection level is tuned
to match the current threat landscape and not either the

worst-case scenario or what that landscape might have
been in the past.

• Sampling-based approach. For a collection of mech-
anisms that can be turned on or oﬀ independently, we
propose a strategy for choosing which mechanisms to
enable for an optimal combination of true positives, false
positives, and performance overheads.

• Formalization. We formalize the problem of optimal
adjustment for a mechanism that includes an ensemble
of classiﬁers, which, by adjusting the sampling rates,
produces the optimal combination of true positives, false
positives, and performance overheads.

• Simulation. Using a simulation based on a history of
Snort signature updates over a period of about 9 years,
we show that we can adjust the sampling rates within
a window of minutes. This means that we can rapidly
react to a fast-changing exploit landscape.

II. Background

When it comes to malware detection, anti-virus soft-
ware has long been the ﬁrst line of defense. However, for
almost as long as AV engines have been around, they have
been recognized to be far from perfect, in terms of their
false negative rates [61], [30], false positive rates, and,
lastly, in terms of performance [39], [24].

Clearly, the choice of signature database plays a
decisive role in the success of the AV solution. This
is illustrated by the promises of SaneSecurity (http:
//sanesecurity.com) to deliver improved detection rates,
by using the free open-source ClamAV detection engine
and their own carefully curated and frequently updated
database of signatures. For example, as of August 2016,
they claim a detection rate of 97.11% vs only 13.82%
for out-of-the-box ClamAV, using a database of
little
over 4,000 signatures vs. almost 250,000 for ClamAV.

While the issue of false negatives is generally known
to industry insiders, the issue of false positives receives
much more negative press. Reluctance to remove or disable
older signatures runs the risk of unnecessarily triggering
false positives, which are supremely costly to the AV

3

vendor, as reported by AV-Comparatives [53] (http://www.
av-comparatives.org).

A. The Changing Attack Landscape

When the attacks for which some mitigation mecha-
nism was designed are no longer observed in the wild,
it might seem very alluring to remove said mechanism.
However, in most real world scenarios, we are not able to
fully retire mitigation techniques. Before disabling some
mitigation mechanism, one should examine the reason
these attacks are no longer being observed and whether
this is simply due to the observational mechanism and data
collection approach being faulty.

Today, large-scale exploitation is often run as a busi-
ness, meaning it is driven largely, but not entirely, by
economic forces. The lack of observed attacks might simply
be associated with an increased diﬃculty in monetizing the
attack. Given that the attacker is aiming to proﬁt from the
attack, if the cost of mounting a successful attack is too
high compared to either the possible gains or alternative
attack vectors, the attacker will most likely opt not to
execute it as it is no longer cost-eﬀective. We see these
forces in practice as the attack landscape changes, with
newer attacks such as ransomware becoming increasingly
popular in the last several years while older attacks leading
to the theft of account credentials becoming less common
because of two-factor authentication, geo-locating the user,
etc. To summarize, we identify two common cases where
monetizing becomes hard.

• No longer proﬁtable. The ﬁrst is the result of causes
other than the mitigation mechanism. For example,
when clients are no longer interested in the possible
product of the attack or if there are other security
mechanisms in place that prohibit the usefulness of the
attack’s outcome. In such cases, removing the mitiga-
tion mechanism in question will most likely not have a
practical negative eﬀect on the system since the attack
remains not cost-eﬀective.

• Eﬀective mitigation. The other case is when the
attack is not cost-eﬀective due to diﬃculties imposed
by the mitigation mechanism. In such cases, removing
the mechanism will result in an increase in the cost-
eﬀectiveness of the attacks it was aimed to prevent. This
might result in the reemergence of such attacks.

By sampling the relative frequency of attacks of a par-
ticular kind, we cannot always determine which case we
are currently faced with. It may also be a combination of
these two factors. However, there is growing evidence that
attackers are frequently going after the low-hanging fruit,
eﬀectively behaving lazily [18].

We therefore suggest an alternative that acts as a mid-
dle ground by introducing sampling rates for all mitigation
mechanisms. In the ﬁrst case, the mitigation mechanism is
no longer needed, therefore adding a sampling rate will not
reduce the security of the system, but will provide fewer
beneﬁts than a complete removal. On the other hand, in
the second case, the security of the system is somewhat
lowered, but the statistical nature of the sampling rate
maintains some deterrence against attackers.

B. Study of Snort Signatures

To test some of these educated guesses, we have per-
formed an in-depth study of Snort signatures. Focusing
on the dynamics of signature addition and removal, we
have mined the database of Snort signatures, starting
on 12/30/2007 and ending on 9/6/2016. Daily updates
to the Snort signature database are distributed through
the Emerging Threats mailing list archived at https:
//lists.emergingthreats.net, which we used to determine
which signatures, if any were 1) added, 2) removed, or
3) modiﬁed every single day. Figure 2 presents statistics
of Snort signatures obtained by exploring the dataset we
collected by crawling the mailing list archive.

It is evident from the ﬁgure that there are many more
signature additions than removals. These statistics support
our claim that signature dataset sizes are growing out of
control and becoming unsustainable.

Below we give several representative examples of signa-

ture addition, update, and removal.
Example 1: Signature addition. Figure 3 shows the ad-
dition of new signatures in response to observations of the
ProjectSauron [31] malware in the wild. The connection
between the malware and Emerging Threats signatures
designed to prevent it is evident from the signature de-
scription.

Example 2: Signature update.
In the case of the sig-
nature marked as 2011124, we see a false positive report
about traﬃc on port 110 on 04/04/2016, which receives a
response from the maintainers within two days:

we get quite a lot of false positives with this one due to
the POP3 protocol on port 110, it would be great if port 110
or more generally POP3 traffic could be excluded from this rule
-- JohnNaggets - 2016-04-02
Thanks, we’ll get this out today!
-- DarienH - 2016-04-04

The maintainers added port 110 the same day they re-
sponded, resulting in this signature revision 19:

alert ftp $HOME_NET ![21,25,110,119,139,445,465,475,587,902,1433,2525] ->

any any (msg:"ET MALWARE Suspicious FTP 220 Banner on Local Port (spaced)";
flow:from_server,established,only_stream; content:"220 ";
depth:4; content:!"SMTP"; within:20;
reference:url,doc.emergingthreats.net/2011124;
classtype:non-standard-protocol; sid:2011124; rev:19;)$

CVE Additions: Evaluation of the Delay: Another
question to consider is how fast signatures for known
threats are added after the threats are discovered or pub-
licly announced. To estimate that, we have correlated 176
CVEs to 40,884 Emerging Threats signatures. This
process usually involves analyzing the comments embedded
in the signature to ﬁnd CVE references (for example,
reference:cve,2003-0533). We have plotted a histogram
(Figure 4) that shows the delay in days between the CVE
(according to the NIST NVD database) and the signature
introduction date. As we can see, many signatures are
created and added the same day the CVE is disclosed.
Sadly, in quite a signiﬁcant percentage of cases, signatures
are added two weeks and more after the CVE release date.
What is perhaps most surprising is that many signatures
are created quite a bit before the disclosure, in many cases
the day before, and in some cases over a month prior to
it. This is due to other information sources that lead to
signature generation.

4

(a) Additions of signatures in the Snort emerging threats database.

(b) Removals of signatures in the Snort emerging threats database.

(c) Updates of signatures in the Snort emerging threats database.

Fig. 2: Dynamics of Snort signatures between 12/30/2007 and 9/6/2016.

2023020 - ET TROJAN ProjectSauron Remsec DNS Lookup (rapidcomments.com) (trojan.rules)
2023021 - ET TROJAN ProjectSauron Remsec DNS Lookup (bikessport.com) (trojan.rules)
2023022 - ET TROJAN ProjectSauron Remsec DNS Lookup (myhomemusic.com) (trojan.rules)
2023023 - ET TROJAN ProjectSauron Remsec DNS Lookup (flowershop22.110mb.com) (trojan.rules)
2023024 - ET TROJAN ProjectSauron Remsec DNS Lookup(wildhorses.awardspace.info) (trojan.rules)
2023025 - ET TROJAN ProjectSauron Remsec DNS Lookup (asrgd-uz.weedns.com) (trojan.rules)
2023026 - ET TROJAN ProjectSauron Remsec DNS Lookup (sx4-ws42.yi.org)(trojan.rules)
2023027 - ET TROJAN ProjectSauron Remsec DNS Lookup (we.q.tcow.eu)(trojan.rules)

Fig.
Connecting
(http : //bit.ly/2eX1O9h).

3:

signatures

to

ProjectSauron malware

III. Optimally Setting Sampling Rates

In this section we set the stage for a general approach to
selecting sampling rates in response to changes to the data.
We evaluate these ideas with practical multi-year traces in
Section IV. We start with a model in which we have a set of
classiﬁers (in the context of Snort signatures discussed in
Section II, each signature is a classiﬁer) at our disposal and
we need to assign a sampling rate to each of them, so as to
match our optimization goals. These goals include higher
true positive rates, lower false positive rates, and lower
overheads. We formalize this as problem of selecting a bit-

Fig. 4: Number of days between CVE announcement and corresponding
signature addition.

vector α, indicating the sampling rate for each classiﬁer.

5

A. Active Classiﬁer Set

We assume that our classiﬁers send some portion of
the samples they ﬂagged as malicious for further analysis.
This matches how security products, such as those from
Symantec, use user machines for in-ﬁeld monitoring of
emerging threats. In the context of a large-scale deploy-
ment, this results in a large, frequently updated dataset,
which consists solely of true positive and false positive
samples. We can use this dataset to evaluate the average
true positive, false positive, true negative, and false neg-
ative rates, induced by sampling rates for our classiﬁers.
Speciﬁcally, our aim is to choose a sampling bit-vector ¯α
that will keep the true positive and true negatives above
some threshold, while keeping the false positive rate, false
negatives, and performance costs below some maximum
acceptable values. We found this formulation to be most
useful in our evaluation.

Constraints: Formally, these goals can be speciﬁed as a
set of inequalities, one for each threshold:

T P (¯α) ≥ Xp

T N (¯α) ≥ Xn

F P (¯α) ≤ Yp

F N (¯α) ≤ Yn

Cost(¯α) ≤ Z

(1a)

(1b)

(1c)

(1d)

(1e)

Parametrization: Given a dataset D and a set of classi-
ﬁers C we deﬁne the following parametrization:

• Di is the ith entry in the dataset and Cj is the jth

classiﬁer;

• G ∈ (0/1)|D|, such that Gi is 1 iﬀ Di is a malicious entry

in the ground-truth;

• R ∈ (0/1)|D|×|C|, such that Ri,j is 1 if Di is classiﬁed as

malicious by Cj or 0 otherwise;

• P ∈ R|C|, such that Pj is the average cost of classifying

an entry from the dataset using Cj;

• α ∈ [0, 1]|C|, such that αj is the sampling rate for

classiﬁer cj.

For each set sampling rate α we can compute the average
cost of executing the entire set of classiﬁers on an entry
from the dataset as:

F P (¯α) =

Σ|D|

i=0((1 − Gi) · P ri(α))
i=1(Gi)

Σ|D|

T N (¯α) =

Σ|D|

i=0((1 − Gi) · (1 − P ri(α)))
i=1(1 − Gi)

Σ|D|

F N (¯α) =

Σ|D|

i=0((Gi · (1 − P ri(α)))
i=1(1 − Gi)

Σ|D|

(4b)

(4c)

(4d)

In practice, not all suggested goals are always necessary
and not all goals are always meaningful. Finding the
optimal sampling rate usually depends on the setting for
which it is needed. Next we discuss a few hypothetical
scenarios and which approaches might best suit them.

Prioritized objectives: When the user can state that one
objective is more important than others, a multi-leveled
optimization goal can be used. In such a solution, the
objective with the highest priority is optimized ﬁrst. If
there is more than a single possible solution, the second
objective is used to choose between them, and so on.

We note that in our scenario it is extremely unlikely
that one can reduce the sampling rate of a classiﬁer
without aﬀecting the true-positive and false-positive rates.
As a result, using strict objectives, such as maximize true-
positives, would result in a single solution, often enabling
all classiﬁers completely (or disabling all, depending the
chosen objective). Therefore it is recommended to phrase
the objectives as “maintain X% of true-positives”, so that
some ﬂexibility remains.

Budget-aware objectives: Often when assessing the
eﬀect a security mechanism has on a company’s budget,
a cost is assigned to each false positive and each false
negative produced by the mechanism. These assessments
can be used to minimize the total budgetary eﬀect of the
mechanism and expected expenses. Assuming CostF N and
CostF P are the costs of false negatives and false positives
respectively, we can express the expected expenses as:

Expenses(α) = CostF N · F N (α) + CostF P · F P (α) (5)

Using this formulation we can:

• Deﬁne a budget, Expenses(α) ≤ BU DGET , as a strict

requirement from any sampling rate.

• Deﬁne our problem as a standard optimization problem

Cost(α) = P T · ¯α

(2)

with the objective minimizeExpenses(α).

Optimization: To evaluate the true positive and false
positive rates induced by a sampling rate α, we ﬁrst need
to evaluate the probability that an entry will be classiﬁed
as malicious. Given a constant R, this probability can be
expressed as:

P ri(α) = 1 − Π|C|

j=0(1 − Ri,j · αj)

(3)

Based on this probability, we can express the true/false-
positive rates as:

T P (¯α) =

Σ|D|

i=0(Gi · P ri(α))
i=1(Gi)

Σ|D|

(4a)

6

Balancing true positives and false positives: In
this scenario, we correlate our sampling rate optimization
problem to a standard classiﬁer optimization setting, such
that our true-positive rate is equivalent to the classiﬁer’s
precision while the false-positive rate becomes the recall.

In such cases, a ROC curve induced by diﬀerent sam-
pling rates can be used to select the best rate. Taking some
inspiration from the well-known F 1-score, a similar score,
F 1sr, expressed in formula 6 can be used to transform our
problem to a single-objective optimization problem.

F 1sr = 2 ·

T P · F P
T P + F P

(6)

For eﬃciency, we split the process of classiﬁer sampling
rate optimization into two steps. Real-world data often
contains classiﬁer overlap, that is, samples that are ﬂagged
by more than one classiﬁer. We split our dataset into
batches based on the classiﬁer overlap, so that the samples
in each batch are ﬂagged by the same set of classiﬁers. Each
batch is associated with true positive and false positive
counts. The ﬁrst step consists of choosing the batches that
are cost-eﬀective.

B. 0/1 Sampling

Based on the desired optimization objective and the
estimated cost ratio between false negatives and false
positives (if applicable), we proceed to deﬁne the problem
of ﬁnding the optimal subset of sample batches as a linear
programming problem. At this stage, since each batch
is determined by a speciﬁc classiﬁer overlap, there is no
overlap between the batches. Therefore, the computation
of the true/false positives/negatives becomes a simple
summation of the associated true/false positive counts. For
example, the total true positive count is the sum of true
positives associated with batches that are determined as
enabled (meaning they should be sampled) and the total
false negative count is the sum of the true positive counts
of disabled batches.

To encode this problem, we assign each batch bi with
a boolean variable vi, representing whether or not the
batch should be active. We then encode the optimization
goal using these variables and the associated counts. We
use a linear programming solver called Pulp [5], which
ﬁnds an assignment to ¯v = {v1, v2, ...} that optimizes the
optimization objective. The output of this step is a division
of the samples into enabled (meaning the classiﬁer should
sample them) and disabled samples.

When the dataset contains no classiﬁer overlap, mean-
ing each sample is sampled by exactly one classiﬁer, the
output of the ﬁrst step can be used as the classiﬁer
sampling rates. In this setting, the batches essentially cor-
respond to single classiﬁers and therefore disabled batches
correspond to classiﬁers that are deemed not cost-eﬀective
according to the optimization objective. The optimal solu-
tion in this case would be to fully enable all cost-eﬀective
classiﬁers and fully disable the rest.

C. Inferring Sampling Rates

In the second step of our solution, given a set of
enabled samples and a set of disable samples as described
in Section III-B, we infer sampling rates for all classiﬁers
that will induce the desired separation.

The key insight we use to infer the classiﬁer sampling
rates is to express our problem in the form of factor
graphs [41]. Factor graphs are probabilistic graphical mod-
els composed of two kinds of nodes: variables and factors.
A variable can be either an evidence/observation variable,
meaning it’s value is set, or a query variable, whose value
needs to be inferred. A factor is a special node that deﬁnes
the relationships between variables. For example, given
variable A and B, a possible factor connecting them could
be A → B.

C1

F2

S2

C2

C3

F3

S3

F4

S4

F5

S5

F1

S1

Fig. 5: Example factor graph, containing three classiﬁers C1 . . . C3
and ﬁve samples S1 . . . S5. The structure of the factor graph determines
the overlap among the classiﬁers.

Example 3: Consider the example factor graph in Figure 5.
The graph consists of 8 variables: 3 named C1–C3 repre-
senting 3 diﬀerent classiﬁers and 5 variables named S1-S5
representing samples. These variable are connected using 5
factors, F1–F5, such that Fi( ¯C, Si) = ∨ ¯C → Si, where ¯C is
the set of classiﬁers with edges entering the factor.

The variables Si are treated as observations, meaning
their value is set, and the variables Ci are treated as
query variables. The inference algorithm chooses at which
probability is each Ci set to true, such that the factors
satisfy the observations. If we set all observations Si to
true, the inference of the factor graph returns the trivial
solution of always setting all Ci to true (meaning Ci
is true with probability 1.0). When we set some Si to
false, the inference algorithm is able to provide more
elaborate answers. For example, setting S4 to false, results
in probability 1.0 for C1 and probability 0.5 to both C2
and C3.

Given the sets of enabled and disabled samples from the
previous step, we translate the problem to a factor graph
as follows:

1) For each classiﬁer i we deﬁne a query variable Ci;
2) for each sample j we deﬁne an observation variable Sj

and a factor Fj;

3) if sample j was set to be enabled we set Sj to true,

otherwise to false;

4) we connect each Sj to its corresponding Fj;
5) for every pair of classiﬁer and sample (i, j), if classiﬁer

i ﬂags sample j we connect Ci to Fj.

Using this construction, we get a factor graph similar in
structure to the graph in ﬁgure 5. The inferred probabil-
ities for the query variables Ci are used as the sampling
rates for the corresponding classiﬁers.

To solve the factor graph and infer the probabilities
for Ci, we use Microsoft’s Infer.NET [4], a framework
for running Bayesian inference in graphical models1. We
evaluate the performance of the solver in Section IV.

D. Discussion

Maintaining the dataset: We intend to build our
dataset using samples ﬂagged as malicious by our classi-

1We use ExpectationPropagation as the inference algorithm,

which proved empirically fastest in our experiments.

7

ﬁers. This kind of dataset will naturally grow over time to
become very large. Two problems arise from this situation.

The ﬁrst problem is that after a while most of the
dataset will become outdated. While we usually wouldn’t
want to completely drop old samples, since they still
represent possible attacks, we would like to give precedence
to newer samples over older ones (which essentially should
result in higher sampling rates for current attacks). To
facilitate this we can assign a weight to each sample in the
dataset. We represent these weights using W ∈ [0, 1]|D|
and rewrite the formulas from 4 as:

T P (α) =

Σ|D|

i=0(Wi · Gi · P ri(α))
i=1(Wi · Gi)

Σ|D|

F P (α) =

Σ|D|

i=0(Wi · (1 − Gi) · P ri(α))
i=1(Wi · Gi)

Σ|D|

T N (α) =

Σ|D|

i=0(Wi · (1 − Gi) · (1 − P ri(α)))
i=1(Wi · (1 − Gi))

Σ|D|

F N (α) =

Σ|D|

i=0(Wi · Gi · (1 − P ri(α)))
i=1(Wi · (1 − Gi))

Σ|D|

(7a)

(7b)

(7c)

(7d)

While many diﬀerent weighting techniques can be used,
two examples are: (1) Assign weight 0 to all old sam-
ples, essentially dropping old samples from the dataset;
(2) Assign some initial weight w0 to each new sample and
exponentially decrease the weights of all samples after each
sampling rate selection. The second problem stems from
the sampling rates themselves. Given 2 classiﬁers, C1 and
C2, and their corresponding sampling rates, α1 and α2, if
α1 is higher than α2 the dataset will contain more samples
of attacks blocked by C1 than by C2. This creates a biased
dataset that, in turn, will inﬂuence sampling rates selected
in the future. This problem can also be addressed using
the weights mechanism. One possible approach will be to
assign weights in reverse ratio to the sampling rates (so
that samples matching C2 will be assigned a higher rate
than samples matching C1). Other viable approaches exist
and the most suitable approach should be chosen based on
the setting in which the classiﬁers are used.

Minimum sampling rates: In Section III-A we’ve de-
ﬁned a sampling rate as α ∈ [0, 1]|C|. This deﬁnition allows
for a complete disable of a classiﬁer by setting it’s sampling
rate to 0. In practice, since we can never be sure that an
attack has completely disappeared from the landscape, it is
unlikely that we will want to completely disable a classiﬁer.

A possible approach to address this is by setting a
minimal sampling rate for the classiﬁer. Given that the
attack for which this classiﬁer was intended is extremely
unlikely to be encountered we don’t want to apply the
classiﬁer to every sample encountered. however since the
attack is still possible we should statistically apply the
classiﬁer to some samples to maintain some chance of
blocking and noticing an attack (if one appears). Given
an inferred sampling rate S, the minimal sampling rate
can be introduced in many forms, such as

8

• a lower bound L on the sampling rate assigned to each

classiﬁer (S >= L);

• a constant value X added to the sampling rate (S + X);
• some percentage Y reduced from the non-sampled por-

tion (S + (1 − S) · Y ).

We note that the minimum sampling rate for each classiﬁer
should be proportional to the severity of the attacks for
which it was intended. If the impact of a successful attack
is minuscule, we may set a lower minimum sampling rate
because even if we miss the attack the consequences are
not severe. However, if the impact is drastic, meaning the
severity of the attack is high, then we should set a higher
minimum sampling rate as a precaution.

We can formalize the notion of minimal sampling rates
as M inSR ∈ [0, 1]|C|, which is based on a severity mapping
S ∈ N|C| (such that Sj is the severity of the attacks for
which classiﬁer Cj was intended), and use M inSRj as
either L,X, or Y from the examples above.

IV. Experimental Evaluation

In this section we ﬁrst describe our simulation design
and then discuss both how much our approach helps with
achieving optimization objectives such as reducing false
positives, and how long it takes to solve the optimization
problems on a daily or weekly basis.

A. Simulation Design

To evaluate the beneﬁts of our approach we performed
several simulations mimicking real-world anti-virus activ-
ity. For the purposes of our simulation, we collected de-
tailed information from Snort signature activity summaries
from 12/30/2007 until 9/6/2016, entailing signature addi-
tions, updates and removals, as shown in Figure 2. In total,
we’ve collected information regarding 40,884 signatures.

Generating simulated malware traﬃc: We generate
malware traﬃc traces (observed true positive and false
positive samples) based on the collected Snort signature
information. We base these traces on the following assump-
tions:

i Each signature was introduced to counteract some

speciﬁc malware.

ii Nonetheless, some signatures might unintentionally
ﬂag malware other than the one it was intended for
(resulting in classiﬁer overlap).

iii The main purpose of signature updates is to address
some false negatives, resulting in increased true posi-
tive and false positive observations.

The following design decisions are also incorporated

into the trace generation:

i The decline in true positive observations count for a
speciﬁc signature is modeled as a power law decay
curve.

ii False positive observations count is modeled as a
percentage of the true positive count (denoted as a
simulation parameter θ).

iii Amount of true positive traﬃc does not aﬀect false

positive observations.

iv Observations may be captured by more than one sig-

nature (referred to as “classiﬁer overlap”).

dates accordingly to incorporate this insight into the attack
model.

For more details, please see Section IV-A1.

Simulation scenario: We aim to simulate a real world
usage in our simulation. The scenario we are simulating
is applied to the
is when once every 3 days our tool
latest observations and updates the sampling rates for all
active signatures. New signatures might still be introduced
between sampling rate updates and are set to full sampling
until the next update. We believe this to be a reasonable
setting that is quite likely to be implemented in practice.

Additionally, under some conditions, Infer.NET’s infer-
ence algorithm might fail. Such conditions are very rare
(inference for only 1.5% of days either failed or timed-
out). However, if they occur we allow the simulation to
keep using the sampling rates computed on the last update,
which we believe to be a solution applicable in practice.

We deﬁned our optimization goal using a budget-aware
objective. Assuming a known estimated cost ratio β be-
tween false negatives and false positives, we phrase the
objective as F P + β · F N . We leave β as a parameter for
the simulation.

Essentially, we set our goal to minimize the overall cost
incurred by scanning. We note that, as Section III-A men-
tions, there are many possible goals. Based on discussions
with commercial vendors, we believe the overall incurred
cost is a measure likely to be used in practice. Another
reasonable goal is the total cost of scanning. In this setting
each signature would have to be associated with a scanning
cost the the system would look for a solution that either
minimizes it or keeps it below some given threshold.

1) Design Decisions: The following design decisions are

incorporated into the trace generation:

Simulating the decline in true positives: We use
a power law decay curve to simulate the decline of a
speciﬁc type of malware over time. Prior research on large-
scale exploitation (75 million recorded attacks detected by
Symantec sensors) in the wild suggests the power law as an
accurate way to model diminishing exploitation rates [10].

We initialize the curve for an initial true positive count
of approximately 500 observations per day and calibrate
it to ﬁt the lifespan of the signature intended for that
malware. We ﬁlter out signatures which were added or
removed outside of our sampling period, for which we
cannot determine a life span, since we are unable to
calibrate a proper decay curve for these signatures. We
also eliminate temporary short-lived signatures (less than 7
days). Out of the 40,884 collected signatures, we are left
with 3,029 signatures after ﬁltering, which we use in the
simulation. Figure 6 shows the number of active signatures
for each day of our simulation.

We note that in many real-world cases, a signature is
introduced only a few days after a malware appears in the
wild and is removed at least a few days after the relevant
malware disappeared from the attack landscape. We adjust
the overall signature lifespan, introduction and removal

Simulating false positives: We model the amount of
false positive observations as a percentage of the true
positive observations count. We denote this percentage as
θ, used as a parameter for the simulation.

As false positive observations originate solely from
legitimate traﬃc which remains mostly unchanged, we
determine that false positive observations should remain
constant as long as the signature is not changed. We
therefore update the false positive traces only when the
signature is updated. The curve in Figure 7 show an
example of the number of true positive and false positive
observation for Snort signature 2007705 for a set θ.

Simulating classiﬁer overlap: From the collected sig-
nature information, we learn that, while there exists some
overlap between signatures, most signatures only ﬂag one
kind of malware. To simulate overlap we randomly choose
for each signature and each observation with how many
other signatures it overlaps. We draw this value from the
distribution shown in Figure 8, calibrated to match our
collected signature information.

B. Experimental Setup

To compare diﬀerent simulation conditions, we ran
several simulations, each with a diﬀerent combination of
values for θ and β, both with classiﬁer overlap and without.
The simulations were executed on a Linux machine with 64
AMD Opteron(TM) 6376 processors, operating at 2.3GHz
each, and 128 GB RAM, running Ubuntu 14.04. Each
simulation was assigned the exclusive use of a single core.

C. Precision and Recall Results

By applying the sampling rates computed by our sys-
tem, one can eliminate part of the false positives previously
observed at the expense of losing part of the true positive
observations. In Figure 10, we show the percentage of true
positives remaining compared to the percentage of false
positives remaining.

The dashed line across each of the ﬁgures symbolizes
an equal loss of both false positives and true positives. The
area above the dashed line matches settings in which less
true positives are lost compared to false positives. This is
the area we should strive to be in, since it represents a
sampling which is relatively cost-eﬀective. One can clearly
see from the ﬁgures that, regardless of classiﬁer overlap,
all of our simulations reside above the dashed line.

Figure 9 show the classiﬁcation precision and recall as
a function of θ for diﬀerent values of β, both with and
without classiﬁer overlap. The ﬁgures show that, regardless
of overlap, both precision and recall drop when β and
θ rise. A rise of θ means there are more false positive
observations, which reduces the portion of observed true
positives, thus aﬀecting the overall precision and recall.
Similarly, a rise of β means that relative cost of a false
negative is higher than that of a false positive. Therefore,
based on the optimization objective we set in Section IV-A,

9

Fig. 6: Number of active signatures in the Snort archive (12/30/2007–9/6/2016). Figure 2 shows some of the update dynamics.

Fig. 7: Modeling the true positive count and the false positive count per day for Emerging Threats signature 2007705, assuming power law decay. The
dashed lines at the ends of the ﬁgure indicate malware appearance, signature introduction, malware disappearance, and signature removal. The dashed
lines in the middle of the ﬁgure indicate signature updates.

relatively cheap (as is often the case for private, user owned
desktops), we can expect our system to determine sampling
rates that are relatively low.

We believe our system is especially well suited for large-
scale services such as web-mail providers. In this scenario,
each user can set it’s own willingness to accept risk, i.e. the
subjective cost of a false negative. Doing so will allow the
service’s servers to greatly reduce their scanning workload,
which will have a tremendous eﬀect on the overall server
performance as the amount of data scanned by these
servers is huge. Such services would like obtain the greatest
beneﬁt from our approach.

Fig. 8: Probability distribution used to decide amount of overlap for each
signature.

D. Solution Times

it is only logical that the system will choose to allow for
more false positives, rather than risking a false negative,
thus again aﬀecting both precision and recall.

Adapting to the situation: From the aforementioned
ﬁgures, we learn that the eﬀectiveness of applying sampling
rates depends greatly on the operating scenario. In some
cases, where for example false negatives are extremely
expensive (as might be the case for corporate datacenters),
the sampling rates remain rather high and thus the over-
all true positive and false positive counts remain mostly
unaltered. On the other hand, when false negatives are

We recognize that for a system such as the one proposed
in this paper to be applicable to real world scenarios, it is
required that solving and computing new sampling rates be
very fast and cheap. Long solving times mean the system
would not be able to quickly adapt to changing landscapes
and respond by setting new sampling rates in a timely
fashion.

Figure 11 shows a cumulative distribution of the to-
tal solving times (both PuLP and Infer.NET) measured
during our simulations for each day. The ﬁgure shows
that over 80% of simulated days were solved in under 20
seconds. The day which took our system the longest to
solve took less than 5 minutes (285 seconds to be exact).

10

(a) precision without overlap

(b) precision with overlap

(c) recall without overlap

(d) recall with overlap

Fig. 9: Classiﬁcation precision and recall as a function of θ for diﬀerent values of β with and without classiﬁer overlap.

Fig. 12: Average daily solving times. The blue line (at the bottom)
represents PuLP solving time (≤ 1 second). The black line represent
Infer.NET solving time. The grey line in the background show number
of active signatures per day.

is no classiﬁer overlap, the solution provided by PuLP
is suﬃcient as the sampling rates for the classiﬁers. This
means that when there is no overlap, solving is extremely
fast. Also, there is a clear correlation between Infer.NET
solving time and the number of active signatures, both
following the same trends. This correlation is interesting
as it indicates that (1) solving time can be anticipated
in advance, and (2) we can accelerate the solving of days
with many active signatures using a “divide-and-conquer”
approach, meaning we can split them to smaller batches
and solve each one separately.

E. Experimental Summary

Reduction in false positives: Regardless of classi-
ﬁer overlap, when comparing the reduction in the num-
ber of false positives to that of true positives, we ﬁnd
that our responsive optimization technique removes more
false positives than true positives, both in terms of
percentages (19.23% compared to 12.39% without over-
lap; 20.13% compared to 11.96% with overlap) with over-
lap) and in terms of absolute values (9,286,530 com-
pared to 8,002,871.5 without overlap; 9,225,422.6 com-
pared to 8,065,888.6 with overlap). The reduction in abso-
lute values is surprisingly signiﬁcant, considering that the
highest value of θ in our simulations was set to 25% of
the true positive rate. In settings with classiﬁer overlap,
applying sampling rates is more beneﬁcial than in settings

Fig. 10: Percentage of true positives remaining compared to percentage
of false positives eliminated with and without classiﬁer overlap. Diﬀerent
setting for FP-threshold and cost ratio correspond to diﬀerent points on
the curves.

Fig. 11: Cumulative distribution of overall solution times (PuLP +
Infer.NET).

This tells us that using this kind of system in a responsive
manner is indeed feasible.

Figure 12 shows the average solving time needed for
each day of our simulation. We ﬁrst note that the solv-
ing times for PuLP (represented by the blue line) are
extremely low, constantly below 1 second. When there

11

without classiﬁer overlap. This can be observed from the
relevant reduction rates, 20.13% compared to 19.23% of
false positives and 11.96% compared to 12.39% of true
positives. This means that, on average, we can eliminate
more false positives at the expense of fewer true positives.

While in some settings the beneﬁts of our approach
are not as drastic as in others, our simulations clearly
demonstrate that this approach is advantageous regardless
of the operating scenario.

Solver running time: In settings without classiﬁer over-
lap, the sampling rates for all simulated days were com-
puted in mere seconds. In settings with overlap, timing
measurements indicate that our approach for setting sam-
pling rates is computationally feasible and applicable to
real-world settings. The sampling rates for over 98% of
simulated days were computed in under 2.5 minutes per
day, with an average and median of 15 seconds.

V. Limitations

We acknowledge that our approach has the following

limitations.

Code complexity: The adoptive approach may increase
development and maintenance costs to developers or secu-
rity researchers, because a new mechanism for introducing
the sampling rates will need to be implemented. We also
note that debugging might prove more diﬃcult since,
in addition to the inputs introduced to the system, the
sampling rates used would also have to be taken into
account, leading to possible non-determinism.

When it comes to the issue of long-term cost of en-
forcement, we remind the reader that our approach doesn’t
remove signatures, it merely disables them.

Adaptive attacker countermeasures: Most security
mechanism entail new opportunities for attackers and our
approach is no exception. An attacker familiar with the
working of a system utilizing our approach could attempt
to use it to her advantage. With the approach in this paper,
classiﬁers are no longer always applied and any attack has
some probability to get through the classiﬁers if it is not
sampled. Using this knowledge, the attacker can attempt
the same attack several times, hoping that at least one
instance will be overlooked.

Additionally, classiﬁers for uncommon attacks would
be sampled at lower rates. An attacker can deliberately
use outdated vulnerabilities, which are likely not sampled
frequently to increase her chances of a successful attack.
The attacker can use a variety of attacks to force a high
sampling rate on all classiﬁers. This would at most cause
the system to revert to the current default situation, in
which all classiﬁers are always on.

Using one or more of the approaches outlined above, the
attacker can temporarily increase her chances of success.
Doing so will increase the amount of observed attacks
for the relevant classiﬁers, thus triggering an increase in
respective sampling. As a result, we anticipate that the
window of vulnerability will be small, and that relatively
few users will be aﬀected.

VI. Related Work

Reactive vs. proactive Security: There has been some
interest in comparing the relative performance of proactive
and reactive security over the years. Barth et al. [14] make
the interesting observation that proactive security is not
always more cost-eﬀective than reactive security. Just like
in our paper, they support their claim using simulations.
Barreto et al. [13] formulate a detailed adversary model
that considers diﬀerent levels of privilege for the attacker
such as read and write access to information ﬂows.

Blum et al. [19] demonstrate that optimizing defences
as if attackers have unlimited knowledge is close to optimal
when confronted with a diligent attacker, with limited
knowledge regarding current defences prior to attacking.
Such attackers are more realistic, thus supporting our
results highlighting the beneﬁts of tunable security.

Classiﬁers designed to detect malicious traﬃc or sam-
ples are often aﬀected by an issue known as “Concept
drift” [65]. Researchers have proposed a number of ways
to address concept drift through retraining classiﬁers. For
instance, Soska and Christin [27] present techniques that
allow to proactively identify likely targets for attackers as
well as sites that may be hosted by malicious users.

Several researchers have proposed generating new sig-
natures automatically [46], [48], [32], [54], [66]. Signature
addition seems to largely remain a manual process, sup-
plemented with testing potential AV signatures against
known deployments, often within virtual machines. Au-
tomatic signature generation will likely improve security
but will worsen the false positive and performance issues
addressed in this paper.

Exploitation In the Wild: Nayak et al. [45] highlights
the lack of a clear connection between vulnerabilities and
metrics such as attack surface or amount of exploitation
that takes place. They use ﬁeld data to get a more com-
prehensive picture of the exploitation landscape as it is
changing. None of the products in their study had more
than 35% of their disclosed vulnerabilities exploited in
the wild. These ﬁndings resonate with the premise of our
paper.

One of the better ways to understand the exploitation
landscape is by consulting intelligence reports published
by large security software vendors. Of these, reports pub-
lished by Microsoft [43] and Symantec [58] stand out,
both published on a regular basis. A recent report from
Microsoft [63], [64] highlights the importance of focusing
on exploitation and not only vulnerabilities. The current
approach to software security at Microsoft is driven by
data. This approach involves proactive monitoring and
analysis of exploits found in-the-wild to better understand
the types of vulnerabilities being exploited and exploita-
tion techniques being used.

Bilge et al. [17] focus on the prevalence and exploitation
of zero-days. They discover that a typical zero-day attack
lasts 312 days on average and that, after vulnerabilities are
disclosed publicly, the volume of attacks exploiting them
increases by up to 5 orders of magnitude. These ﬁndings

12

website-centric and user-centric point of view. Kwon et
al. [60] analyzed approximately 43,000 malware download
URLs over a period of over 1.5 years, in which they studied
the URLs’ long-term behavior.

VII. Conclusions

Securing today’s complex systems does not come free
of charge. The most common cost is performance. Using
three security mechanism examples, anti-virus signatures,
Snort malware signatures, and ad-blocking lists, we show
that the cost of security enforcement (measured in terms
of latency) often grow linearly with the number of policies
that are involved. It is therefore imperative to devise ways
to limit the enforcement cost.

In this paper we argued for a new kind of tunable frame-
work, on which to base security mechanisms. This new
framework enables a more reactive approach to security,
thus allowing us to optimize the deployment of security
mechanisms based on the current state of attacks.

Based on actual evidence of exploitation collected from
the ﬁeld, our framework can choose which mechanisms to
enable/disable so that we can minimize the overall costs
and false positive rates, while maintaining a satisfactory
level of security in the system.

Our responsive strategy is both computationally aﬀord-
able and results in signiﬁcant reductions in false positives,
at the cost of introducing a moderate number of false neg-
atives. Through measurements performed in the context of
large-scale simulations, we ﬁnd that the time to ﬁnd the
optimal sampling strategy is mere seconds for the non-
overlap case, and under 2.5 minutes in 98% of overlap
cases. The reduction in the number of false positives is
signiﬁcant (about 9.2 million, when removed from traces
that are about 9 years long, 20.13% and 19.23%, with and
without overlap, respectively).

were important in designing credible simulations in this
paper.

Economics of Security Attacks and Defenses: Her-
ley et al. [35] point out that, while it can be claimed that
some security mechanism improves security, it is impossible
to prove that a mechanism is necessary or suﬃcient for
security, meaning there is no other way to prevent an
attack or that no other mechanism is needed. They also
make a similar observation stating that one can never prove
that a security mechanism is redundant and not needed.
These observations put into words the frame of mind that
resulted in the current overwhelming number of active
security mechanisms. We try to address this problem using
the proposed sampling rates.

A report by the Ponemon Institute [49] estimated the
costs of false positives to industry companies. The estima-
tion was based on a survey ﬁlled by 18,750 people in vari-
ous positions. While the numbers portrayed in the report
are not accurate, they do paint an interesting picture. The
average cost of false positives to a company was estimated
at 1.27 million dollars per year. These estimations include
the cost analyzing and investigating false positive reports
as well as the cost of not responding in time to other true
positive reports.

Models of Malware Propagation: Arbaugh et al. [12]
introduced a vulnerability life-cycle model supported by
case studies. The introduced model is diﬀerent than the
intuitive model one would imagine a vulnerability follows.
We relied on the insights presented in this paper in design-
ing our models for trace generation.

Many works have focused on modeling malware prop-
agation. Bose et al. [20] study several aspects crucial to
the problem, such as user interactions, network structure
and network coordination of malware (e.g. botnets). Gao et
al. [28] study and model the propagation of mobile viruses
through Bluetooth and SMS using a two-layer network
model. Fleizach et al. [26] evaluate the eﬀects of malware
self-propagating in mobile phone networks using commu-
nication services.

Garetto et al. [29] present analytical techniques to
better understand malware behavior. They develop a mod-
eling methodology based on Interactive Markov Chains
that captures many aspects of the problem, especially
the impact of the underlying topology on the spreading
characteristics of malware. Edwards et al. [25] present a
simple Markov model of malware spread through large
populations of websites and studies the eﬀect of two in-
terventions that might be deployed by a search provider:
blacklisting and depreferencing, a generalization of black-
listing in which a website’s ranking is decreased by a ﬁxed
percentage each time period the site remains infected.

Grottke et al. [33] deﬁne metrics and models for the
assessment of coordinated massive malware campaigns
targeting critical infrastructure sectors. Cova et al. [21]
oﬀer the ﬁrst broad analysis of the infrastructure un-
derpinning the distribution of rogue security software by
tracking 6,500 malicious domains. Hang et al. [34] conduct
an extensive study of malware distribution and follow a

13

References

[1] “Snort,” http://www.snort.org/, 1998.

[2] “ClamAV,” http://www.clamav.net/, 2002.

[3] “EasyList ad blocking rule set,” https://easylist.to/, 2005.

[4] “Infer.NET,”

http://research.microsoft.com/en-us/um/

cambridge/projects/infernet/, 2011.

[5] “PuLP,” https://pypi.python.org/pypi/PuLP, 2011.

[6] “Brave,” https://brave.com/, 2015.

[7] “Brave Ad Block,” https://github.com/brave/ad-block, 2015.

[8] “Training
2015,”
networkforensics virtualbox.zip, 2015.

PCAP

FIRST
https://www.ﬁrst.org/ assets/conf2015/

dataset

from

[9] “Separation

of mechanism and

policy,”

https://

en.wikipedia.org/wiki/Separation of mechanism and policy,
2016.

[30]

I. Gashi, V. Stankovic, C. Leita, and O. Thonnard, “An exper-
imental study of diversity with oﬀ-the-shelf antivirus engines,”
in NCA, 2009.

[31] GReAT,

“Projectsauron:

cyber-espionage
platform covertly extracts encrypted government comms,”
https://securelist.com/analysis/publications/75533/faq-the-
projectsauron-apt/, 2016.

level

top

[32] K. Griﬃn, S. Schneider, X. Hu, and T.-C. Chiueh, “Auto-
matic generation of string signatures for malware detection,”
in RAID, 2009.

[33] M. Grottke, A. Avritzer, D. S. Menasch´e, J. Alonso, L. Aguiar,
and S. G. Alvarez, “Wap: Models and metrics for the assess-
ment of critical-infrastructure-targeted malware campaigns,” in
ISSRE, 2015.

[34] H. Hang, A. Bashir, M. Faloutsos, C. Faloutsos, and T. Dumi-
tras, “”infect-me-not”: A user-centric and site-centric study of
web-based malware,” in IFIP Networking, 2016.

[35] C. Herley, “Unfalsiﬁability of security claims,” Proceedings of

[10] L. Allodi, “The heavy tails of vulnerability exploitation,” in

the National Academy of Sciences, 2016.

ESSoS, 2015.

[11] L. Allodi and F. Massacci, “Comparing vulnerability severity
and exploits using case-control studies,” TISSEC, 2014.

[12] W. A. Arbaugh, W. L. Fithen, and J. McHugh, “Windows of

vulnerability: A case study analysis,” Computer, 2000.

[13] C. Barreto, A. A. C´ardenas, and N. Quijano, “Controllability
of dynamical systems: Threat models and reactive security,” in
GameSec, 2013.

[14] A. Barth, B. I. P. Rubinstein, M. Sundararajan, J. C. Mitchell,
D. Song, and P. L. Bartlett, “A learning-based approach to re-
active security,” in Proceedings of the International Conference
on Financial Cryptography and Data Security, 2010.

[15] D. Bates, A. Barth, and C. Jackson, “Regular expressions

[36] V. Ivanov, “Protocol-level evasion of web application ﬁrewalls,”

2012.

[37] T. Jarrett, “The fog of war: How prevalent is sql injection?”
https://www.veracode.com/blog/2015/07/fog-war-how-
prevalent-sql-injection, 2015.

[38] V. Kotov and F. Massacci, “Anatomy of exploit kits: Prelimi-
nary analysis of exploit kits as software artefacts,” in ESSoS,
2013.

[39] L. M. L. Radvilavicius and A. Cenys, “Overview of real-time

antivirus scanning engines,” JESTR, 2012.

[40] R. Levin, E. Cohen, W. Corwin, F. Pollack, and W. Wulf,
“Policy/mechanism separation in hydra,” SIGOPS Operating
Systems Review, 1975.

considered harmful in client-side xss ﬁlters,” in WWW, 2010.

[41] H. A. Loeliger, “An introduction to factor graphs,” IEEE Signal

[16] T. U. Berlin, O. Hohlfeld, and A. Feldmann, “Annoyed Users :

Ads and Ad-Block Usage in the Wild,” IMC, 2015.

[17] L. Bilge and T. Dumitras, “Before we knew it: An empirical

study of zero-day attacks in the real world,” in CCS, 2012.

[18] J. Blasco, “The lazy attacker,” 2013. [Online]. Available: https:

//www.scmagazineuk.com/the-lazy-attacker/article/545575/

[19] A. Blum, N. Haghtalab, and A. D. Procaccia, “Lazy defenders
are almost optimal against diligent attackers,” in AAAI, ser.
AAAI’14, 2014.

[20] A. Bose and K. G. Shin, “Agent-based modeling of malware
dynamics in heterogeneous environments,” Security and Com-
munication Networks, 2013.

[21] M. Cova, C. Leita, O. Thonnard, A. D. Keromytis, and
M. Dacier, An Analysis of Rogue AV Campaigns, 2010.

[22] T. H. Dang, P. Maniatis, and D. Wagner, “The performance
cost of shadow stacks and stack canaries,” in CCS, 2015.

[23] G. De Maio, A. Kapravelos, Y. Shoshitaishvili, C. Kruegel, and
G. Vigna, “Pexy: The other side of exploit kits,” in DIMVA,
2014.

[24] N. K. Dien, T. T. Hieu, and T. N. Thinh, Memory-Based Multi-
pattern Signature Scanning for ClamAV Antivirus, 2014.

[25] B. Edwards, T. Moore, G. Stelle, S. Hofmeyr, and S. Forrest,
“Beyond the blacklist: modeling malware spread and the eﬀect
of interventions,” in NSPW, 2012.

[26] C. Fleizach, M. Liljenstam, P. Johansson, G. M. Voelker, and
A. Mehes, “Can you infect me now?: malware propagation in
mobile phone networks,” in WORM, 2007.

[27] K. Fu and J. Jung, Eds., Proceedings of the 23rd USENIX
Security Symposium, San Diego, CA, USA, August 20-22,
2014. USENIX Association, 2014.

[28] C. Gao and J. Liu, “Modeling and restraining mobile virus
propagation,” IEEE Transactions on Mobile Computing, 2013.

Processing Magazine, 2004.

[42] G. Merzdovnik, M. Huber, D. Buhov, N. Nikiforakis, S. Neuner,
M. Schmiedecker, and E. Weippl, “Block me if you can: A large-
scale study of tracker-blocking tools,” EuroS&P, 2017.

[43] Microsoft Corporation, “Microsoft intelligence report,” https:

//www.microsoft.com/security/sir/default.aspx, 2015.

[44] M. H. Mughees, Z. Qian, and Z. Shaﬁq, “Detecting Anti Ad-

blockers in the Wild,” PETS, 2017.

[45] K. Nayak, D. Marino, P. Efstathopoulos, and T. Dumitra¸s,
Some Vulnerabilities Are Diﬀerent Than Others, 2014.
[46] J. Newsome, B. Karp, and D. Song, “Polygraph: Automatically
generating signatures for polymorphic worms,” in S&P, 2005.

[47] M. Payer, “Too much pie is bad for performance,” 2012.
[48] R. Perdisci, W. Lee, and N. Feamster, “Behavioral clustering of
http-based malware and signature generation using malicious
network traces,” in NSDI, 2010.

[49] Ponemon Institute, “The cost of malware containment,”
http://www.ponemon.org/local/upload/ﬁle/Damballa%
20Malware%20Containment%20FINAL%203.pdf, 2015.
[50] N. Provos, P. Mavrommatis, M. A. Rajab, and F. Monrose, “All
your iFRAMEs point to us,” in USENIX Security Symposium,
2008.

[51] N. Provos, M. A. Rajab, and P. Mavrommatis, “Cybercrime
2.0: when the cloud turns dark,” Communications of the ACM,
2009.

[52] P. Roberts, “At the vulnerability Oscars, the winner is... buﬀer
overﬂow!!” https://www.veracode.com/blog/2013/02/at-the-
vulnerability-oscars-the-winner-is-buﬀer-overﬂow, 2013.
[53] N. J. Rubenking, “False positives sink antivirus ratings,” http:
//www.pcmag.com/article2/0,2817,2481367,00.asp, 2015.
[54] V. S. Sathyanarayan, P. Kohli, and B. Bruhadeshwar, “Signa-
ture generation and detection of malware families,” in ACISP,
2008.

[29] M. Garetto, W. Gong, and D. Towsley, “Modeling malware

[55] B. Stock, B. Livshits, and B. Zorn, “Kizzle: A signature com-

spreading dynamics,” in CCC, 2003.

piler for exploit kits,” in DSN, 2016.

14

[56] O. Sukwong, H. S. Kim, and J. C. Hoe, “Commercial antivirus

software eﬀectiveness: an empirical study,” Computer, 2011.

[57] Symantec Corporation, “Insight: Deployment best practices,”

https://support.symantec.com/en US/article.DOC5077.html,
2016.

[58] Symantec Corproration, “Internet security threat report,”
https://www.symantec.com/content/dam/symantec/docs/
reports/istr-21-2016-en.pdf, 2016.

[59] L. Szekeres, M. Payer, T. Wei, and D. Song, “Sok: Eternal war

in memory,” in S&P, 2013.

[60] Y. Tanaka and A. Goto, “Analysis of malware download sites
by focusing on time series variation of malware,” in ISCC, 2016.
[61] Tenable, “Tenable malware detection: Keeping up with
environment,”
sophisticated

an
http://www.enpointe.com/images/pdf/whitepaper-tenable-
malware-detection.pdf, 2014.

increasingly

threat

[62] D. Weston and M. Miller, “Web application ﬁrewalls: Attacking

detection logic mechanisms,” 2016.

[63] ——, “Windows 10 mitigation improvements,” 2016.
[64] D. Weston, M. Miller, and T. Rains, “Exploitation trends: From
potential risk to actual risk,” https://www.rsaconference.com/
writable/presentations/ﬁle upload/br-t07-exploitation-
trends-from-potential-risk-to-actual-risk.pdf, 2015.

[65] G. Widmer and M. Kubat, “Learning in the presence of concept

drift and hidden contexts,” Machine Learning, 1996.

[66] M. F. Zolkipli and A. Jantan, “A framework for malware detec-
tion using combination technique and signature generation,” in
ICCRD, 2010.

15

