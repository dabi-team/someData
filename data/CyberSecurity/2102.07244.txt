Resilient Machine Learning for Networked Cyber
Physical Systems: A Survey for Machine Learning
Security to Securing Machine Learning for CPS

Felix O. Olowononi, Danda B. Rawat, Senior Member, IEEE and Chunmei Liu

1

1
2
0
2

b
e
F
4
1

]

R
C
.
s
c
[

1
v
4
4
2
7
0
.
2
0
1
2
:
v
i
X
r
a

Abstract—Cyber Physical Systems (CPS) are characterized
by their ability to integrate the physical and information or
cyber worlds. Their deployment in critical infrastructure have
to transform the world. However,
demonstrated a potential
harnessing this potential is limited by their critical nature and
the far reaching effects of cyber attacks on human, infrastructure
and the environment. An attraction for cyber concerns in CPS
rises from the process of sending information from sensors to
actuators over the wireless communication medium, thereby
widening the attack surface. Traditionally, CPS security has been
investigated from the perspective of preventing intruders from
gaining access to the system using cryptography and other access
control techniques. Most research work have therefore focused on
the detection of attacks in CPS. However, in a world of increasing
adversaries, it is becoming more difﬁcult to totally prevent CPS
from adversarial attacks, hence the need to focus on making CPS
resilient. Resilient CPS are designed to withstand disruptions and
remain functional despite the operation of adversaries. One of
the dominant methodologies explored for building resilient CPS
is dependent on machine learning (ML) algorithms. However,
rising from recent research in adversarial ML, we posit that
ML algorithms for securing CPS must themselves be resilient.
This paper is therefore aimed at comprehensively surveying the
interactions between resilient CPS using ML and resilient ML
when applied in CPS. The paper concludes with a number
of research trends and promising future research directions.
Furthermore, with this paper, readers can have a thorough
understanding of recent advances on ML-based security and
securing ML for CPS and countermeasures, as well as research
trends in this active research area.

Index Terms—Adversarial attacks, Cybersecurity, Machine

Learning, Resiliency in Cyber Physical Systems

I. INTRODUCTION

The advent of the internet is a foundation for the birth
of many of the developments and technologies that have
signiﬁcantly affected human, his interactions with others and
the environment. The ability to electronically interconnect
computer systems across the world made communication,
collaboration and access to information easy and so serve as
a tool for creativity and innovation. According to the Inter-
national Telecommunication Union (ITU), broadband internet

Authors are with the Data Science and Cybersecurity Center (DSC2),
Department of Electrical Engineering and Computer Science, Howard
University, Washington DC, 20059 USA. Corresponding Author E-mail:
danda.rawat@howard.edu.

This work was supported in part by the US NSF under grants CNS/SaTC
2039583, CNS 1650831 and 1828811, and by the DoD Center of Excellence in
AI and Machine Learning (CoE-AIML) at Howard University under Contract
Number W911NF-20-2-0277 with the U.S. Army Research Laboratory and by
the U.S. Department of Homeland Security (DHS) under grant award number,
2017-ST-062-000003.

penetration is directly proportional to the employment and
economic growth rates of a nation [1]. This is because the
internet is the underlying technology for the digital revolu-
tion, which is responsible for paradigms and platforms like
online commerce/shopping, online banking, online education,
e-health and e-government. Beyond the internet, this digital
revolution is also fueled by advances in wireless communica-
tions, proliferation of high capacity mobile devices, relatively
lower cost of computing devices, alternative energy sources
and access to larger memories. The ﬁeld of parallel/distributed
computing, cloud computing, quantum computing, nanotech-
nology and microelectronics and opto-electronics have also
contributed immensely to these developments. In the early
days of the internet, access was limited to computers and later
on smart phones. However, further developments in wireless
sensors have made it possible to incorporate minute and high
capacity sensors into hardware devices used for everyday
activities,
thereby establishing connectivity to the internet.
This development known as the Internet-of-Things (IoT) has
made it possible to expand the internet from a worldwide
network of computers to a worldwide network of computer
and things, resulting in terms like Internet-of-battleﬁeld things
(IoBT), Internet-of-Vehicles (IoV) and Industrial Internet-of-
Things (IIoT). IoT and wireless sensor networks (WSN) have
pushed the frontiers of research in ﬁelds like manufacturing,
transportation, healthcare, home automation, military warfare,
entertainment and security [2]–[8].

CPS leverage on the internet and WSN to act as intelli-
gent systems that automate processes which were previously
largely dependent on human efforts. Deﬁned in so many ways
by different authors,
they fundamentally refer to physical
and engineered systems where the monitoring, coordination,
controlling and integration of the operations are done by a
computing and communication core [9]. CPS add a control
action to the computing and networking dimensions of the IoT.
As a result of the ability to use a feedback control to direct
an actuator to take action based on physical measurements
obtained from the sensors, the level of automation in CPS
exceed that obtained from IoT systems. CPS have therefore
generated a lot of interest from the industry, government
they have to
and academia due to the immense potential
revolutionize virtually every ﬁeld of human endeavor and solve
practical challenges in our world.

One of the implications of the IoT and CPS when ap-
plied in critical infrastructure in agriculture, health, military,
transportation, home automation and power systems is that a

 
 
 
 
 
 
2

underscore their impact in the present world. Furthermore,
emerging technologies like IoT, CPS, BDA and AI are the
major technologies pushing for the fourth industrial revolution
[15], [16]. As a result of these developments, a lot of research
interest is directed towards the use of big data and data science
principles to secure systems from adversarial attacks. The use
of AI and ML for cybersecurity began with its implementation
in IDS. Research in this area included malware and anomaly
detection in information and communication systems. With
the success recorded, ML was also used to achieve cyber-
security in IoT systems [17]–[21]. Furthermore, the advent
of deep learning (DL) and reinforcement learning (RL) have
contributed signiﬁcantly to the deployment of ML algorithms
to solve actual problems that posed a challenge to shallow
algorithms and the more familiar supervised and unsupervised
algorithms. Factors that support the use of DL in CPS include
the high dimensional data generated and the continual growth
of data [12]. Recently, researchers have combined DL and
RL to arrive at the deep reinforcement learning (DRL); a
development that has resulted in tremendous revolution in
CPS research and continues to demonstrate a great potential
to proffer solutions to present and imminent challenges. This
revolution is prominent in vehicular CPS like autonomous
vehicles, because of the need to continually make decisions
like lane changing and respond to trafﬁc signs autonomously
through image and pattern recognition [22]–[26].

There are however rising concerns with the use of AI in
cybersecurity. Recent research has shown that systems that
depend on ML algorithms for security are also prone to various
forms of adversarial attacks. ML algorithms are data dependent
and make inferences or predictions through data generated
from various sensors in the networked systems. A predominant
form of cyber attack in CPS and other systems therefore is
the development of strategies to tamper with the data or the
input. Consequently, the model is forced to produce wrong
outputs. This is particularly common with neural networks,
especially the deep neural networks (DNN) that have become
very popularly used to secure CPS systems. Furthermore, the
possibility of reusing the strategies meant to defend a system
to attack it has also become a source of concern. The usage
of AI and ML algorithms to defend systems can also be used
by adversaries to attack the systems and perform adversarial
attacks. Recently, it has been found that such attacks have
a great potential because they are more sophisticated, faster
and relatively cheaper since they leverage on the efforts of the
defense systems to make themselves stronger and difﬁcult to
detect or curb.

The possibility of compromising ML algorithms that are
deployed to enhance cybersecurity in networked systems is a
challenge that researchers must seek for ways to combat. In
other words, it must be accepted that ML algorithms cannot
totally prevent machines from gaining access to systems that
are to be protected. As seen from [27], [28], the concerns about
the security of ML has been of interest for over a decade.
Although many research have focused on IDS and spam e-
mail ﬁltering, not many have dwelt on what the systems must
do in the presence of adversarial attacks. The desire to ensure
that ML-enhanced systems continue to offer the services they

Figure 1. Application scenarios of CPS

large volume of data is generated. This is because the devices
are usually connected in real-time and remain continuously
powered on. The concept of big data analytics (BDA) have
therefore become applicable to IoT and CPS systems as they
enable information to be accessible from data for making
decisions such as fault prediction, diagnosis and predictive
maintenance. In order to obtain value from the generated
data, the use of data analytics to uncover hidden patterns,
correlations and insights from large amounts of data are
becoming increasingly popular as they introduce new func-
tionalities to the systems highlighted above. Presently, CPS
and other networked controlled systems are accountable for
a large amount of data produced in the world. Various types
of application that illustrate the concept of CPS are shown in
Figure 1.

Despite all the hype of CPS, their actual deployment to
solve real life challenges is hampered by safety and security
concerns [10]. CPS have very stringent requirements such
as a need to operate in real-time and sensitivity to network
challenges like delay. Moreover, the damage a failure causes
to human life and infrastructure is more grave when compared
to traditional information technology systems. Cybersecurity
has become a dominant research topic in computer science
and information technology. Initially limited to attacks on
information using techniques like malware, adware, spyware
and ransomware, security was guaranteed through the use of
anti-viruses, ﬁrewalls and intrusion detection systems (IDS).
The increase in inter-connectivity of sensors, actuators and
controllers in CPS contribute largely to the rise of cyber attacks
because they widen the attack surface and make these systems
susceptible to adversarial activities. Recently, ML and artiﬁcial
intelligence (AI) algorithms have been used to enhance the
efﬁciency of many systems [11]–[13]. The reference of data
and AI as “new oil” and “new electricity” respectively [14]

should in the presence of adversarial attack led to the idea of
resilient ML. This is a research domain that must be critically
looked into to enhance the practical deployment of ML-based
cybersecurity especially in CPS.

A. Previous works

learning competition. Also,

In the last few years, a few survey papers have sought to
discuss the issue of adversarial attacks and defense in ML
models. Ozdag et al. [29] presented a sparse study on the
subject. However, it was limited to DNN and the defenses were
only focused on the Neural Information Processing System
(NIPS) 2017 adversarial
there
was no focus on the resiliency of physical systems. A more
comprehensive survey by Biggio et al. [30] discussed the
evolution of ML for over a decade. However, it was limited to
DL and there was no attention on resiliency of CPS. Liang
et al [31] focused on the good, bad and ugly use of ML
for cybersecurity in CPS/IoT. The paper highlighted the ad-
vantages and disadvantages of ML applications in networked
systems. However, it only presented a general overview of the
subject. Finally, Yuan et al. [32] presented an extensive survey
on adversarial attacks and defense in DL. Just like the others,
the authors did not dwell on RL, a commonly used approach in
CPS. Furthermore, it was limited to methods published before
2017.
B. Contributions

This paper therefore aims to address the gaps of the pre-
viously highlighted research work by analyzing recent papers
on adversarial attacks and defense, focus on both DNN and
RL and also give insights on the subject with a focus on CPS.
In summary, we comprehensively investigate the interactions
between resilient CPS using ML and resilient ML when
applied in critical systems. Speciﬁcally, the main contributions
of this paper include:

• Identify the roles of ML algorithms in the security and
resiliency of CPS and prove from a comprehensive study
of literature why the resiliency of ML algorithms must
be a research concern.

• Present a comprehensive study of AML and generative
adversarial networks (GAN) in CPS and discuss their use
both as an attack or defense against the resiliency of CPS.
• Present a discussion of recent trends, research challenges,
insights and future research directions in the use of ML
algorithms for achieving resiliency in CPS.

The remainder of this paper is organized as follows: Section
II presents preliminary information on ML while Section
III presents an overview of ML applications in CPS and
speciﬁcally considers vehicular CPS, industrial CPS, medical
CPS and smart grids. Section IV discusses the security and
resiliency of CPS with ML while Section V focuses on AML
in CPS. In Section VI, the focus is on secure and resilient ML.
Section VII highlights open challenges and future research
directions and section VIII concludes the study.

In an attempt to make the paper easy to read and navigate

through, Figure 2 presents the structure of the survey.

3

II. OVERVIEW OF MACHINE LEARNING

In order to give the reader a good grasp of the discussion
on the role of ML in CPS and the need to make ML
models resilient to adversarial attacks, the various ML models
commonly applied in CPS are brieﬂy discussed in this section.
One of the common deﬁnitions of ML is the ability of
systems to “make intelligent decisions without being explicitly
programmed”. Despite the fact that it is used interchangeably
with AI by some people, ML is actually a subset of the
ﬁeld of AI. ML approaches are data-driven in nature. The
application of ML in CPS is therefore as a result of the
large amount of data generated from the numerous sensors.
Furthermore, ML techniques are usually categorized into three
namely supervised, unsupervised and RL. Figure 3 shows the
different types and the tasks that are carried out in each. These
will be discussed in this section. Also, some of the algorithms
that belong to each of these classes will be brieﬂy discussed.
This section will no doubt serve as a foundation to the major
subject of this paper.

A. Supervised Learning

In supervised learning, the training set contains the data
samples and the desired solution or label(s). The goal of the
ML algorithm is therefore to develop a function that maps
the input to the output. After learning has taken place, an
efﬁcient model can take an unseen input and decide what
the output should be. The most commonly used performance
metrics for ML algorithms include accuracy, precision, recall
and F1-score. The major task in this category include classiﬁ-
cation and regression. Supervised learning in CPS context are
predominantly focused on classiﬁcation tasks. A few of the
most commonly used algorithms in CPS research are brieﬂy
discussed below.

1) Artiﬁcial Neural Networks (ANN): ANN are modeled
after biological neurons in human brains. The perceptron
is one of the simplest ANN architectures. Perceptrons can
be trained to make predictions based on a ﬁxed threshold.
The multi-layer perceptron (MLP), obtained by combining
many perceptrons together achieves better results. Activation
functions like binary step, sigmoid and rectiﬁed linear unit
ReLU) play the major role of converting an input signal of a
node to an output signal. Technically speaking, they decide if a
neuron should be triggered after mathematical computations.
ANN models have been used to solve a lot of problems in
our world. In CPS research, DNN’s, which are discussed in a
later section have become a choice model for solving various
classiﬁcation and regression tasks.

2) Support Vector Machine (SVM): The capability of SVM
to be used for classiﬁcation, regression and even outlier
detection tasks, generate accurate results and conserve com-
putation power makes it an ML model of choice in CPS
research. The algorithm has a goal of ﬁnding a hyperplane
(decision boundaries) in an N-dimensional space that distinctly
classiﬁes the data points. The dimension of the hyperplane
is dependent on the number of features in the dataset. For

4

Figure 2. Organization of the tutorial

optimal results, the chosen plane should maximize the between
data points of both classes. Prior to the widespread used of
neural networks, SVM was a very popular ML algorithm for
supervised learning.

3) k-Nearest Neighbors (kNN): KNN is a basic ML algo-
rithm for classiﬁcation (and regression) tasks. In CPS, it has
been used for pattern recognition, data mining and intrusion
detection. It’s non-parametric behavior, which means that there
is no need for assumptions on the data needed is a great
attraction for its use in real-life applications. Basically, the
algorithm decides the class of a test point based on majority
voting by its K nearest neighbors. KNN makes predictions
using the training dataset directly. For a new instance, pre-
dictions can be made by searching the entire dataset for the
K most similar instances or neighbors and then summarizing
the output variable for those K instances. Similarity between
instances are measured using distance measure methods like
Euclidean distance. Although kNN is no longer as popular as
it used to be, some researchers still use it for their research.

Figure 3. Figure showing types of ML

B. Unsupervised Learning

In contrast to the supervised learning methods described
above, unsupervised learning is achieved with unlabeled train-
ing data. The algorithm aims to learn or search for hidden

insights in the unlabeled data. Tasks such as dimensionality
reduction, clustering, density estimation, anomaly detection
and visualization are achieved with this class of ML. Two of
the most commonly used algorithms in this class, employed
in CPS research are discussed below.

1) K-Means Clustering: K-means clustering is one of the
simplest and popular unsupervised ML algorithms. Clustering
refers to the task of identifying similar instances and assigning
them to a group (cluster), after which underlying patterns can
be identiﬁed. K-means clustering simply seeks to partition a
number of datapoints into K clusters. Speciﬁcally, the algo-
rithm operates by identifying K number of centroids (center
of the cluster), assign every data point in the dataset to the
nearest cluster with an ultimate aim to keep the centroids as
small as possible. Although this method has advantages of
being fast and scalable, it suffers limitations when the clusters
have varying sizes and different densities. However, it has
been used extensively in CPS applications for data analysis,
dimensionality reduction, anomaly detection and even image
segmentation.

2) Principality Component Analysis (PCA): PCA identiﬁes
the hyperplane that lies closest to the data and projects the data
onto it. Put in another way, it is an orthogonal linear transfor-
mation method that transforms the data to a new coordinate
system. The major advantage of PCA in ML research is its
ability to guarantee efﬁciency in the ML lifecycle through a
reduction in the number of features in the dataset, while still
retaining the required information necessary for training. PCA,
together with other dimensionality reduction algorithms like
linear discriminant analysis (LDA) is therefore usually used
to address the curse of dimensionality problem in ML.

C. Reinforcement Learning (RL)

In RL, the algorithm or agent learns to make decisions
through its interactions with the environment. The algorithm
performs this learning procedure in a trial and error manner,
where it receives rewards and punishments for correct and
incorrect performances respectively. The ultimate goal of the
agent is to maximize the reward in any given situation.

Figure 4 shows the interactions between the two major
elements of a RL system; the agent and environment. While
the agent depicts the algorithm itself, the environment rep-
resents the external condition or object the agent is acting
on. Four other important elements of a RL system include
the policy, reward signal, value function and the model of
the environment. The policy deﬁnes the behavior of the agent
at any given time. This is usually achieved by mapping the
states to the actions. The reward, which is the main goal of
the setup is a function of the current action of the agent and the
current state of its environment. The policy is usually changed
by the agent to maximize the reward. The value function,
though similar to the reward signal represents the long-term or
cumulative reward an agent can gather based on the states that
are likely to follow the present state and the rewards associated
to those future states. The model of the environment seeks to
predict the behavior of the agent by making inferences about
its next state and rewards based on information it has about a
given state and action.

5

Figure 4.

Interactions between the agent and environment in a RL system

A unique feature of the RL that distinguishes it from
other types of learning algorithms is the trade-off between
exploration and exploitation. This, in simple terms means that
since the goal of the agent is to maximize the reward while
interacting with the environment, it must seek to exploit the
knowledge it already has based on past interactions and the
rewards obtained. However, in order to select better actions in
the future, the agent needs to explore other actions to maximize
rewards. This occurrence is usually known as the exploration-
exploitation dilemma.

Over the years, a number of RL algorithms have been
developed. Q-learning, the classical algorithm was proposed
by C.Watkins [33]. Next, the deep Q-Network (DQN) [34],
[35], proposed by Google DeepMind in 2013 made popular
the concept of DRL. Others include the value iterative network
(VIN) [36], asynchronous advantage actor-critic algorithm
(A3C) [37], trust region policy optimization (TRPO) [38], deep
deterministic policy gradients (DDPG) [39], proximal policy
optimization (PPO) [40] and the unsupervised reinforcement
and auxiliary learning (UNREAL) [41]. Moreover, it is per-
tinent to state that with the introduction of the DQN, A3C
and UNREAL, Google DeepMind has made a lot of impact
on research in RL. In the coming sections, it will be evident
that applications of RL in real life applications have leveraged
mostly on the DQN. However, research into the defense of
RL algorithms have attempted to study the DQN, TRPO and
the A3C algorithms. Two of the most commonly applied RL
algorithms in CPS are brieﬂy discussed below.

1) Q-Learning: This is the simplest and most commonly
used RL algorithm. As the name implies, the goal of the RL
agent is to learn the Q-Value, through iterative interactions
with the environment and then use the information to take
an appropriate action. The Q-Value, sometimes referred to as
the Quality values is used to estimate the optimal state-action
values. The Q-Value refers to the discounted accumulative
rewards of an agent that starts with a state-action pair and
follows a certain policy. At any state, the goal of the agent
is to take an action with the largest Q-Value. Initially, the
Q-Value is estimated to zero and then updated using the Q-
Value iteration algorithm. However, a challenge of Q-learning
that also affects its use in CPS is its inability to scale well to
large Markov decision processes with many states and actions.

2) Deep Q Network (DQN): To solve the aforementioned
scaling challenge of Q-learning, DNN’s are often used to esti-

mate Q-Values. The DQN therefore introduced the subject of
DRL. Put concisely, DQN is able to overcome the challenges
of unstable learning with the following techniques; experi-
ence replay, target network, clipping of rewards and skipping
frames. State transition samples generated through interactions
with the environment are stored in a replay memory and
consequently used to train the DQN. Furthermore, a target
DQN is used to generate target values. The excellent results
of the DQN algorithm have made them gain prominence in
CPS research. This is especially evident in various tasks in
autonomous vehicles.

In summary, RL represents scenarios where there are in-
teractions between an active decision-making agent and its
environment, where the agent, without a knowledge of the
environment however seeks to effectively achieve a goal in
the environment. The agent relies on the fact that its actions
can affect the future state of the environment and in essence
the choices available to it in the future.

D. Deep Learning

Recent research work has focused on the application of
DL especially in the growing ﬁeld of data science. The DL
methods differ from the traditional shallow algorithms because
they have several hidden layers, perform high level feature
abstraction, generalize better on unseen samples and have
shown to improve the performance of systems in which they
have been deployed. These characteristics of DL have made
them an attraction for various tasks in CPS. DL algorithms
such as convolutional neural networks (CNN), recurrent neural
networks (RNN) and autoencoders have been used for various
tasks in CPS.

III. ML APPLICATIONS IN CPS

CPS are very important because of how they affect our daily
lives. Their applications are found in critical infrastructure
as already highlighted in the last section. Furthermore, ML
models have been used to achieve various tasks in CPS. These
include malware detection, intelligent resource allocation, de-
tection of anomalous behavior, fault prediction, preventive
maintenance and detection of attacks. Figure 5 shows the
contribution of ML algorithms to the advancement of the
major CPS. Since this paper is focused on security of CPS,
more research on this will be presented in this section. The
critical nature of these applications demand that they are safe
and secure from attacks. However, this is not presently so.
Some of the factors responsible for the challenge in securing
CPS include the heterogeneous nature of the components,
the complex interactions between the cyber and the physi-
cal sub-systems, and the widened attack surfaces generated
by such interactions. Recent attacks on CPS have shown
that the consequences are more pronounced, especially when
information technology systems.
compared with traditional
However, any attempt to secure or defend CPS must begin
with an understanding of the various vulnerabilities, threat
and attacks that the system can suffer. These are discussed
for the major CPS application scenarios presented in Figure
1. In [42], some prominent CPS cyber attacks were discussed.

6

Figure 5. Applications of ML in CPS

These include the Stuxnet attack, the RQ-170 attack, Ukraine
attack, Maroochy Attack and the Jeep Hack.

In bolstering the argument for the use of DL in the security
of CPS, the authors in [12] posited that they are able to handle
the high dimensional data obtained from a large number of
heterogeneous sensors in CPS. Furthermore, the ability to han-
dle large data makes them improve because they are exposed
to data with new vulnerabilities. In this section therefore, we
introduce further the most common CPS applications and give
an overview of the role of ML in these systems.

A. Vehicular Cyber Physical Systems (VCPS)

The quest for a solution to persistent challenges like trafﬁc
congestion, vehicular accidents and its adverse effects on the
environment has sustained the research interest in VCPS [43].
Advanced driver assistance systems like cooperative adaptive
cruise control and collision avoidance systems leverage on data
from cameras, sensor networks and geographical positioning
systems to increase the intelligence of vehicles. Vehicular
adhoc networks (VANET) play a major role in the success
of VCPS through the provision of interconnections between
vehicles and road side units through wireless communication
media. These are usually referred to as vehicle-to-vehicle
(V2V) and vehicle-to-infrastructure (V2I) communications.
Recent developments like self-driving cars and vehicular pla-
tooning are products of research advancements in vehicular
technology. Figure 6 illustrates how the components of a
VANET communicate cooperatively to ensure the safe travel
of autonomous and platooned vehicles. Since events that occur
on the road are communicated to other vehicles, travel time
and road congestion can be prevented because vehicles are able
to take proactive decisions to ensure the comfort and safety
of human and infrastructure.

The security of VCPS have also been a major source of
concern as they are subject
to various adversarial attacks
[44], [45]. Initially, attacks on VCPS were studied from
the communication perspective, with denial-of-service (DoS)
[46] and man-in-the-middle (MiTM) attacks accounting for
the majority of attacks [47]. Furthermore, another kind of
attack known as the replay attacks operates by delaying the
messages sent in order to deceive the vehicles into taking
action of belated information. However, recently, attacks have
also been launched on the control structure of VCPS. In
this type of attack, the adversary alters a subset of control
inputs, sensor measurements or control laws through replay

and false data injections [48]. In the quest to defend VCPS
from adversarial attacks, various schemes such as trust-based
[49], blockchain [50]–[52] and ML dependent [53] techniques
have been used to secure vehicles from attacks. Recently,
attention has been focused on autonomous vehicles where
image and pattern recognition have been used to control the
vehicles autonomously. Adversarial examples and GAN can be
used to craft new inputs that cause the vehicle to take wrong
decisions such as misjudging stop signs and other trafﬁc signs.
This challenge of neural networks and its effect on CPS
security is addressed comprehensively in a later section.
Dominant research in securing VCPS with ML algorithms
will continue to rise since the use of DNN’s and other DL
techniques are used to make decisions in vehicular systems
more than other CPS.

B. Industrial Cyber Physical Systems (ICPS)

CPS systems when applied in industries and manufacturing
are usually referred to as ICPS, IIoT or Industry 4.0. In
ICPS, a multitude of sensors, devices or agents deployed
ubiquitously, and to remote locations in the plant connect
over a communication network with other parts like actuators
and controllers, for the purpose of monitoring, collecting,
exchanging, analyzing and intelligently executing prompt ac-
tions on gathered data. Security and safety of ICPS is very
important due to the nature of these systems and the cost
and effects of adversarial attacks to human and infrastructure.
A major case study of attacks to the ICPS is the Stuxnet
attack on nuclear power plants in Iran. The novel manner the
malware was distributed and the extent of damage that resulted
from the attacks brings to the fore the issue of cybersecurity
in network controlled systems. The malware was introduced
into the network using a USB drive, after which the worm
propagated itself to systems running on its target operating
system. Moreover,
it was also designed to search for the
targeted component of the ICS; the high-speed centrifuges
produced by Siemens. The primary goal of Stuxnet was to
compromise the logic controllers of the system using “zero

7

day” attacks. The developers then spied on the operations of
the centrifuge for information, and launched attacks by taking
control of the centrifuge. The unique qualities of Stuxnet were
it’s ability to evade detection systems during introduction and
also remain hidden from human operators during the attack.
ML has been used for fault prediction, identiﬁcation of
anomalous behaviors and predictive maintenance in industrial
and manufacturing systems. Haghighi et al [54] proposed an
ML-based ﬁrewall for securing ICPS. The goal was to build
on other researches that focused on accuracy to achieve zero
false-positives in developed classiﬁers. The learning ﬁrewall
receives labeled samples and performed self-conﬁguration by
writing adaptive preventive rules that avoid false alarms. Simu-
lations on the KDD Cup’99 dataset showed that the proposed
classiﬁer could achieve zero false positives. Recently, edge
computing have been used to improve the efﬁciency of ICPS.
This is achieved by shifting tasks that are computationally
intensive from edge devices with limited resources to high ca-
pacity edge servers. However, challenges of limited spectrum,
low capacity batteries and lack of contextual information serve
as a barrier to full realization of its potentials in industrial
applications. ML approaches have been proposed to address
these challenges. Sun et al. [55] proposed a ML-enhanced
method for ofﬂoading in edge devices in IIoT. The method
is able to intelligently direct trafﬁc to the edge server through
the optimal path. Also, Liao et al. [56] proposed a learning-
based context aware method for resource allocation in edge
devices applied in IIoT.

it

From all of the research presented above,

is evident
that ML algorithms will continue to play a great role for
production efﬁciency in industrial systems. Moreover, in a bid
to achieve resiliency in manufacturing plants, ML also have
a great role to play through the incorporation of intelligence
into such systems. Furthermore, as attackers begin to launch
adversarial attacks in the ICPS, ML will also enhance the
adaptive learning of the attack methods and hence proffer
solutions that will mitigate such attacks. Speciﬁcally,
the
potential beneﬁts of deploying various learning agents that will
make the system self-conﬁgurable and resilient to adversarial
attacks and operational faults will continue to make them of
great research interest. However, recent studies has shown
that ML algorithms will not be applied alone for achieving
optimal results. They will be applied in hybrid with emerging
technologies like software deﬁned networks, blockchain and
edge computing. Federated learning will also become suitably
applied in ICPS due to the vast amount of sensors and hetero-
geneous devises in ICPS. Also, since industrial plants are very
delicate, the increasing roles of ML algorithms necessitates
adequate research into boosting the resilience of the algorithms
themselves. This will no doubt, enhance the push towards the
fourth industrial revolution.

Figure 6. Figure showing communications in a VANET

C. Medical Cyber Physical Systems

The ﬁeld of medicine and healthcare has beneﬁted a lot
from the developments in information and communication
technology (ICT) over the years. Some of the developments
that have come from the impact of ICT on medicine and

healthcare include advanced software enabled functionalities
in medical equipment, continuous healthcare away from the
hospitals to improve convenience both for the patient and
healthcare givers, and increased connectivity of healthcare
devices [57].

MCPS therefore refer to systems that can be used to
remotely monitor vital signs such as heart rate, blood sugar
and stress levels and automatically take actions to respond
to situations when these vital signs are out of the normal
threshold. This is usually achieved through the use of body
sensor and wireless sensor networks. The most common
examples of medical devices in MCPS include wearable
devices and implantable medical devices (IMD) such as heart
pacemakers and insulin pumps. Figure 7 shows the process of
communication between the various units of a MCPS. Sensors
like electrocardiogram (ECG), electromyography (EMG) and
blood pressure sensors implanted in the body of the patient
measure signals from the heart, muscles and blood pressure
respectively. These vital signs and readings are then sent
wirelessly to the control unit. The sensors and control unit
form a body area network (BAN), a wireless interconnection of
computing devices through a wireless communication medium
like redtooth. The control unit further relates the information to
the medical server at the end of the medical personnel, through
the access point and the internet. The medical personnel, on
receiving this information is able to take proactive steps to
prevent the occurrence of a heart attack even before it occurs.

However, from a security perspective, the wireless links
between these medical devices, their controllers and the server
make them susceptible to cyber attacks. The attacks could
be either passive or active. In the passive attack, the hacker
seeks to gain access to the data logged by the medical device,
gain knowledge of the health conditions of the patient and
use the information to blackmail or threaten such a person.
However, in the active attack, the hacker’s objective is to
disrupt the operation of the MCPS and jeopardize the health
of the patient. This is possible through jamming the wireless
signals between the medical devices, thus resulting in DoS
attacks that are dangerous to the health of the user. Moreover,
an attacker might also compromise the sensors that measure
the vital signals and cause it to give a wrong input to the
controller. Consequently, when it responds to the false input,
the controller directs the actuator to take an action such as
pumping insulin into the blood stream, thereby putting the
health of patient at risk.

In summary, medical CPS due to the wireless interface
over which they operate are susceptible to attacks like privacy
invasion,
jamming, noise, replay and false data injection
attacks [58]. Furthermore, defects from software have also
become a security concern for medical devices [59]. It is
therefore evident that either through the hardware, software or
the wireless communication through which they communicate,
MCPS are vulnerable to various threats and thus efforts to
secure them must be made a priority because of the threat
they pose to human lives.

8

Figure 7.

Illustration of Medical Cyber Physical System

D. Smart Grids

The integration of the CPS concept

to power systems
resulted in smart grids. According to the United States De-
partment of Energy, a smart grid uses digital technology to
improve reliability, security, and efﬁciency of the electrical
system from large generation, through the delivery systems to
electricity consumers and a growing number of distributed-
generation and storage resources [60]. Smart grids make a lot
of contribution to efﬁciency, and add a lot of functionalities to
the generation, transmission and distribution of electricity. The
smart grid comprises of two subsystems; the power application
where the major functions of generation, transmission and
distribution occur, and the supporting infrastructure where
intelligent monitoring and control of these operations are
carried out through the interactions of software, hardware and
communication networks.

Smart grid also suffer adversarial attacks when communi-
cations between ﬁeld devices, control center and the smart
meters are attacked through false data injection (FDI) and
DoS attacks. These attacks have largely been classiﬁed into
attacks against conﬁdentiality, integrity and availability. Smart
grids attacks usually result in blackouts that can cause a lot
of damage to other systems that depend on it for power. The
security of smart grids is therefore an active research domain.
However, the complexity of smart grids and the heterogeneity
of the CPS components have introduced signiﬁcant difﬁculties
to their security and privacy protection. The complex cyber-
physical interactions pose a challenge to the assessment of
threats and vulnerabilities in smart grids. Also, since attacks
of power grids affect the efﬁciency of other dependent systems,
hackers are not relenting in ﬁnding new loopholes to launch
cyber attacks.

Just like other CPS discussed, ML algorithms also con-
tribute to the overall efﬁciency and security of smart grids.
The data generated by the various sensors while the system is
in operation is used to learn how to react to faults and attacks.
To assert the invaluable roles of ML in smart grids, Zhang
et al. [61] presented a survey on the applications of DL, RL
and DRL in smart grids. These include load forecast/power
consumption [62], demand response [63], defect/fault detec-
tion [64], [65], stability analysis [66] and cybersecurity [67].
From the security perspective, the authors in [68] posited

that recent attacks are stealthy and cannot be detected by
traditional methods that depend on state estimation. They
therefore proposed a ML-based approach for detection of
FDI attacks in smart grids. The approach combines both
supervised (SVM) and unsupervised learning (PCA). Khoda
et al. identiﬁed that the ML algorithms used for securing
CPS need to be resilient and proposed a novel adversarial
retraining method for securing them. Without doubt, there is
a great potential for ML to boost the resiliency of CPS to
conﬁdentiality, integrity and availability attacks.

E. Reinforcement Learning Applications in CPS

The mode of operation of RL makes it very viable for
it has
improving the efﬁciency of CPS. In the last few,
become a great tool for research in CPS. Signiﬁcant impact
of RL on CPS are highlighted in this section to reinforce this
position. Kato et al. [77] while stating that quality assurance
in CPS remains a challenge as a result of factors like their
heterogeneous and black-box components, proposed the use
of RL to serve as a falsiﬁcation approach in CPS. The goal
of the trained RL agent is to learn the model behavior and
then leverage on this information to compromise it for further
investigations. Furthermore, the majority of the application of
DRL have focused on image classiﬁcation and recognition.

The application of RL in CPS have been aligned more
towards power systems and intelligent transportation systems.
In power systems, the major research focus is on consumer
cost optimization and other energy management endeavors.
The advantage of RL lie in its ability to learn the best control
policy and solve problems with a large state space. Kumar
et al. [78] proposed the used of a DRL agent to operate in
a variable pricing regime and learn to optimize the energy
cost for the consumers. The agent manages the activities of
the storage devices with a goal to maximize demand side
cost savings. Other research in this direction are presented in
[79], [80]. However, from the perspective of efﬁciency of the
grid operations, Ren et al. focused on the use of RL for load
balancing in smart grids [81]. Highlighting the importance of
developing cost-effective strategies for self-conﬁguration and
restoration of grid operations during blackouts, the authors
identiﬁed a gap of other approaches to include the penchant to
focus on maximizing the restoration efﬁciency and neglect the
reliability-load balancing trade-off. They therefore proposed
a method that uses the wolf pack algorithm (WPA), an RL
strategy to optimize the reliability of the system during the
restoration process. Moreover, in line with the context of this
survey, Liu et al. [82] differed from the other researches by
proposing a method that leverages on RL for cybersecurity
of power systems. Using a contingency analysis context, they
leveraged on the Q-learning algorithm to develop an online
learning scheme that models the activities of adversaries and
the process of maximizing the attack strategy. The effect of
the method was conﬁrmed using simulations on eleven test
cases.

In ITS, the advanced research in connected and autonomous
vehicles have leveraged a lot on the ﬁeld of ML and DL to
increase the level of automation of vehicles and make them

9

perform tasks that were previously performed by humans.
In principle, sensors and other monitoring devices are now
deployed in vehicles and other infrastructure to obtain data.
The data gathered from the sensors and devices is therefore
analyzed for information that is used to make critical decisions
on the road using DRL. Selected research on applications of
RL in autonomous vehicles is presented in Table I. From a
high level perspective, due to the level of uncertainty involved
in autonomous driving of vehicles, DRL is used to carry out
decisions such as intersection crossing, changing of lanes,
speed control, trust computation and evaluation for safety and
security. Furthermore, the Q-learning and deep Q-Network
are the most widely used RL policies applied in research in
autonomous vehicles.

Research into the application of DRL in autonomous ve-
hicles and the other form of CPS is an interesting research
area. It
is expected that other decisions beyond the ones
reported will be achieved with DRL. Furthermore, although
the researches reported are theoretical, more needs to be done
guarantee the safety and security of these systems if they will
actually be implemented in real life scenarios.

F. Summary and lessons learned

In this section, an overview of the roles of ML in CPS for
four major application scenarios was discussed. The research
trend showed that there has been a surge since 2017. We
posit that the major factor responsible for this surge is the
practical deployment of DL algorithms and their application
in RL. Most research in this area have been directed towards
VCPS and industrial applications. However, from a security
perspective, it is also evident that the state-of-the-art methods
for defending networked systems from attacks are no longer
efﬁcient due to the development of innovative attacks. Fur-
thermore, the goal of most researchers have been to achieve
optimal accuracy when simulations are carried out. However,
issues of computational complexity and delay need to be
brought to the fore due to the critical nature of CPS. The
use of test beds will enhance the application of ML in CPS.

IV. ML FOR RESILIENT CPS

In the last section, applications of ML in CPS was exten-
sively discussed. In this section, in line with the focus of
this paper, the goal will be to discuss the role of ML in
cybersecurity of CPS. Speciﬁcally, attack detection in CPS
using ML algorithms and the role of generative adversarial
networks in the resiliency CPS will be discussed.

A. Attack detection in CPS

Attack detection in CPS is a dominant research topic
because the early discovery of malicious behaviors or attacks
will improve the chances of success of a counter attack to
limit, mitigate or manage the extent of damage caused to
the system. According to [83], attack detection schemes for
CPS differ from traditional IDS for IT systems because of
the additional physical dimension present in CPS. Security of

10

Figure 8. Application of DNN in autonomous vehicles

CPS systems from malicious attacks have therefore been found
to be more effective when the physics or physical properties
of the systems are modeled and monitored. The authors in
[84] suggested that performing attack detection at the physical
domain of the CPS system serves as a last line of defense in
the occasion that the other network layer schemes for attack
detection are bypassed. The use of state space models like
the Kalman ﬁlter, although commonly used in research for
modeling dynamic systems, have been identiﬁed to suffer
challenges like the inability to achieve optimal accuracy in
complex CPS and the ineffectiveness in the detection of
stealthy attacks [85]. The above listed challenges serves as
an incentive towards the drive to applying ML schemes for
attack detection in CPS.

In [84], a ML-dependent attack detection scheme for CPS
security was proposed. The success of the scheme began with
a comprehensive feature generation scheme that
leveraged
on statistical, physical domain knowledge and DL techniques
to generate features that better represent the non-linear and
spatio-temporal relationships of the physical system. Further-
more, the combination of the generated features and the novel
use of extreme learning machine for the detection model
resulted in a high accuracy and also achieved early detection of
malicious attacks in CPS. Furthermore, a behavior-based ML
approach for detection in attacks in CPS was proposed in [86].
Speciﬁcally, the authors focused on intrusion detection in the
SWaT testbed. In [87], the importance of automatic detection
of attacks and intelligent response in complex CPS was
underscored. The authors highlighted that statistical process
control methods for anomaly detection, such as cumulative
sum (CUSUM) and exponentially weighted moving average
(EWMA) are unable to produce effective results in networked
CPS. This is due to their heterogeneous nature and the time
series data generated from multiple sensors. They also posited
that supervised ML techniques suffer from dearth of labeled
data while unsupervised methods like clustering and temporal

prediction methods have the challenge of capturing temporal
dependencies across different time series, coupled with the
presence of noise in multivariate time series data from actual
CPS operations. They therefore proposed an approach based
on statistical correlation analysis between multivariate time
series data and unsupervised DL algorithm for identiﬁcation
of adversarial operations in a complex multi-process CPS.
Speciﬁcally, the approach uses a trained CNN autoencoder
(CNN-AE) and convolutional long short term encoder-decoder
(ConvLSTM-ED) models. The performance of the method
was justiﬁed based on simulations carried out on the Swat
testbed and comparison with state-of-the-art baseline methods.
Furthermore, Wang et al. [88] developed a ML classiﬁer for
detecting time synchronization attack in CPS. Based on the
principle of “ﬁrst aware”, the results showed that the proposed
classiﬁer was able to detect direct and stealth time synchro-
nization attacks. Shin et al. [89] proposed a DL-dependent
method for detection of adversarial attacks in sensors deployed
in autonomous vehicles. They also investigated the inertial
measurement unit and wheel encoder sensors under conditions
of uncertainty and non-linearity. Also, the authors in [90]
used supervised regression as a means to detect anomalous
sensor readings in CPS. By modeling the interaction between
the CPS defender and attacker as a Stackelberg game, where
the defender chooses detection thresholds in response to
adversarial attacks, they proposed an algorithm for ﬁnding an
approximately optimal threshold for the defender and proved
that resilience can be boosted without sacriﬁcing accuracy.

Due to the critical nature of CPS, it is required that they are
dependable and secure. The dependability of a system entails
integrity and maintainability.
availability, reliability, safety,
Also, security involves the common CIA triad; conﬁdentiality,
integrity and availability. Since CPS are ubiquitous, heteroge-
neous and complex in nature, it is possible for the operational
conditions to change. The term “resilience” is therefore often
used to describe the attributes of a system when it is resistant

Table I
SUMMARY OF RESEARCH ON APPLICATIONS OF REINFORCEMENT LEARNING IN AUTONOMOUS VEHICLES

11

Paper

Automated speed and lane change
decision making using deep
reinforcement learning [69]

Policy

Deep Q-Network

Goal

Solution

Automated speed and Lane change
decision

Learning Negotiating Behavior
Between Cars in Intersections
using Deep Q-Learning [70]

Deep Q-learning

Intersection crossing

A DRL Driving Policy for
Autonomous Road Vehicles [71]

DDQN

Trust-Evaluation-Based Intrusion
Detection and Reinforcement
Learning in Autonomous Driving
[72]

Q -Learning

Path planning for autonomous
vehicle in a mixed driving
environment (comprising of
autonomous and manual driven
vehicles)

Intrusion detection in autonomous
vehicles based on trust evaluation

A DRL based trust management
scheme for software deﬁned
vehicular networks [61]

Robust Deep Reinforcement
Learning for Security and Safety
in Autonomous Vehicle Systems
[73]

Navigating Occluded Intersections
with Autonomous Vehicles Using
Deep Reinforcement Learning [74]

Dueling deep Q-network (DDQN)

Trust computation and path
learning

Q Learning

Robustness of AV to adversarial
attacks

Deep Q-Network

Intersection crossing in unsignaled
intersections with occlusions

ML for Cooperative Driving in a
Multi-Lane Highway Environment
[75]

Deep Q-Network

Investigate application of RL with
cooperative driving in a highway
environment

Scheduling the Operation of a
Connected Vehicular Network
Using Deep Reinforcement
Learning [76]

Deep Q-Network

Improve safety and QoS in a
connected vehicular network

A DQN is trained to autonomously
make decisions in self-driving
vehicles. A CNN is also applied
to high level inputs to quicken the
learning process and optimize the
agent’s performance.

The vehicle observes distance
and speed of vehicles on the
intersecting road and use a policy
that adapts its speed along its
pre-deﬁned trajectory to pass the
crossing efﬁciently

The driving policy generates a
collision-free trajectory for the
autonomous vehicle to follow
through dynamic programming.
It operates by mapping the data
from sensors on the AV and its
environment to a goal.

A Q-learning-based incentive
mechanism to encourage
autonomous vehicles report
warnings to improve their trust
values and utilities.

The SDN controller is used as an
agent to learn the most trusted
routing path and determine the
best routing policy.

A novel DRL algorithm was
proposed to maximize the
robustness of AV dynamics control
against data injection attacks.

Improve safety of AV by analysing
exploratory actions/creeping
behaviours created by occlusions
using DRL agents.

The use of information exchange
in vehicular networking to control
an AV in a multi-lane highway
environment.

Use DRL to train an agent to
realize an energy-efﬁcient and
QoS-oriented scheduling policy.

to malicious faults and persists in the delivery of its service
or functions even when facing failure or adversarial circum-
stances. Also, it also refers to the persistence of dependability
when a system is facing changes [91]. To underscore the
importance of resiliency of next generation CPS, Barbeau et
al. [92] presented a vision for these systems. While acceding to
the present case where increase in adversarial activities implies
an increase in the likelihood of disruption of the system, they
posited that by building mechanisms that leveraged on fuzzy
decisions and ML, systems can continue to operate efﬁciently
in such scenarios. However, in building resilient systems, the
possible faults that pose as security threats must be considered.
Furthermore, methodologies for detection and mitigating such
threats must also be investigated.

The concept of adversarial networks became widespread
when it was noticed that effective or competent adversaries are
beginning to use certain strategies to evade detection systems
which are designed to operate based on ML algorithms. Their
activities which are generally classiﬁed as adversarial attacks
are intended to attack the integrity, availability and privacy
of the targeted system. In vehicular CPS, activities of an
adversarial attack might include the ﬂagging of a number of
activities that are normal as an attack thus making the detection
system unnecessary busy and affecting the availability of the
system. However, the injection of false data into the system
thereby causing it to make wrong classiﬁcation is a dominant
method for adversarial attacks in CPS.

As already discussed, resiliency in CPS will enhance their

real-life deployment. Many systems have already been made
resilient using AI and ML algorithms. Kannapan et al. [93]
following in this direction proposed the incorporation of
learning modules in ICPS as a viable method for achieving
resilience. Agents monitor the activities of the system during
normal operations and are able to recover to the learned states
in the occurrence of a failure. In [53], a method for adversarial
resiliency in VCPS was proposed. Furthermore, with regards
to resiliency in CPS, Feng et al. sought to use the technique to
ensure that a system maintains is activities in the presence of
unknown cyber attacks. The authors ﬁrst proposed a novel cy-
ber state dynamics system that can dynamically and effectively
ascertain the real-time impacts of present cyber attack and
defense strategies. Next, they formulated the optimal defense
problem as a two-player zero-sum game and ﬁnally developed
a DRL algorithm to enable the scheme operate in real-time
to suit its application in CPS. Simulations results reinforced
their claim that the proposed DRL-based game theoretic actor-
critic neural network was capable of learning the optimal
defense and worst attack policies online accurately and in
real-time [94]. Lokesh et al. in [95] proposed a biologically
inspired methodology for achieving resiliency in CPS using
state awareness. Multi-agents deployed in the system are used
to achieve state awareness.

In summary, research presented in this section show that
ML algorithms have been successfully used both for attack
detection and to buoy the resiliency of CPS.

B. Generative Adversarial Networks (GAN) for resilient CPS

GAN’s, as a result of their name have been widely portrayed
as a technology that is solely used for compromising the
integrity of various systems. However, in this section, we will
show the contributions of GAN in the quest to improve the
resiliency of CPS. This succeeds an introduction to the subject
topic.

1) Generative Adversarial Networks: Generative models,
which stem from DNN’s typically operate by learning the
density functions of original samples of data. They then use
this information to craft fake samples that are not easily dis-
tinguishable from the real samples [96]. They were described
in [97] as an area of DL research that focused primarily on the
generation of realistic data. The importance of generative mod-
els include the use of training and sampling models generated
to ascertain the possibility of representing and manipulating
high-dimensionality probability distributions, the potential of
incorporating generative models into RL to improve decision
making, their ability to both be trained with missing data and
also make predictions even when their inputs have missing
data, and the ability to enable ML work with multiple outputs
[98].

GAN’s are a type of generative model that generate samples
of a training dataset. In principle, they operate by setting up a
game between the two major components; the generator and
the discriminator. These two neural networks create samples
that aim to possess the same distribution with the training data,
and examine the veracity of the samples produced respectively.
The goal of the generator is to deceive the discriminator to

12

Figure 9. Figure showing the operation of a GAN

mis-classify while the discriminator learn through supervised
learning. Figure 9 illustrates the interactions between the two
major components of a GAN. The advantages of GAN’s over
other generative models include their ability to generate sam-
ples in parallel, relatively limited restrictions for the generator
function, non-requirement of Markov chains and their ability
to produce relatively better samples. The demerit of GAN’s
is therefore the need to ﬁnd the Nash equilibrium of a game
during training [98].

GAN’s have become of interest in cybersecurity especially
as DL continues to be a ML methodology of choice in recent
years. Yinka-Banjo et al. [97] posited that the application of
GAN’s in cybersecurity is a developing research ﬁeld. They
authors in [99] also stated that beyond the possibility of GANs
to generate fake data to fool a security system, they can also be
used to defend systems. This is done by detecting the operation
of adversaries through the generation and addition of fake
samples to the training data to improve the robustness and
resiliency of the system. The following section expand on this
subject of discussion.

2) The role of GAN’s for resilient CPS: GAN’s have proven
useful not only for attacks but for also defending CPS. The
ability to use a generator and discriminator to craft adversarial
samples can also be leveraged on to defend systems against
attacks. CPS usually comprise of control loops where multiple
sensors interact with the physical process or environment,
micro-controllers that receive this data from the sensor net-
works and actuators that receive instructions from the micro-
controllers in the form of electric signals and take a control
action on the physical process. CPS therefore operate as a
feedback control system. Although research initially focused
on the communication aspect of CPS, the cybersecurity of
the control and estimation processes in CPS have become
a source of concern as adversarial attacks have also been
targeted to disrupt them. To solve this challenge, a number
of methodologies have been proposed by researchers [100],
[101]. However, ML models are now also being used to make
the control process intelligent.

The use of ML for learning and estimation in control
systems have also made them susceptible to attacks suffered
by systems that operate using ML. Adversarial samples can
also be used to fool the system and thus compromise their
integrity. To solve this problem, a novel conditional GAN
(CGAN) was proposed in [102]. The authors observe that the
present controllers are unable to detect anomalous behaviors
in the control loop, prevent such attacks or recover from an

adversarial attack on the control loop. The system proposed
therefore uses the conditional GAN to capture and learn the
normal interaction of the physical system and the controller.
The CGAN discriminator with the help of the discriminator
captures the real behavior of the control loop during normal
operation and thus able to identify anomalous behavior. The
proposed system is also able to recover from the attacks by the
prediction of the systems correct state. The results of the actual
test proved that the proposed method is able to guarantee the
detection and recovery from anomalous behaviors in vehicular
in [103], a GAN was used for modeling the
CPS. Also,
distributions of the data streams of many sensors of a CPS
operating in normal condition and another GAN to identify
anomalies in the CPS, caused by attacks. The discriminator
and generator therefore both use the multivariate time series
data obtained form the sensors during normal operation to
detect anomalies. The proposed model was validated using
data from a Secure Water Treatment (SWaT). Other tasks such
as predictive maintenance and fault diagnosis were highlighted
as future areas where the propose algorithm will be applied
to solve challenges in CPS in general. Furthermore, in [104],
the combination on a GAN with a LSTM-RNN as the base
model was used to detect anomalies in multivariate times
series data generated from CPS. The authors, after testing
their approach on the SWaT and Water Distribution (WADI)
datasets concluded that their method is effective in detective
anomalies caused by cyber attacks in CPS. The use of GAN
was also proposed for identifying security anomalies and
cyber threats in the self-organizing networks of CPS [105].
The authors as part of their future works intend to use the
proposed model to secure a self-learning VANET/MANET.
Chhetri et al. [106] proposed a conditional GAN security
model that abstracts and estimates the relations between cyber
and physical domains in ICPS and then analyses the security
of the system.

From the research presented above, it is evident that ML and
GAN, though predominantly seen as a cybersecurity concern
to systems and networks can also be used to mitigate attacks
to these systems. As research into the applications of AI in
CPS continues to expand, GAN’s will play a major role in the
developments in future.

C. Summary and lessons learned

Without doubt, the role of ML in guaranteeing the resiliency
of CPS is immense. The ability to train systems to take
actions based on intelligent inferences from data will be used
to enhance the efﬁciency and effectiveness of these systems.
Ongoing research posit that ML algorithms will have a greater
role to play in the quest to achieve real deployments of CPS.

V. ADVERSARIAL MACHINE LEARNING (AML) AND CPS

Having already discussed the contributions of ML algo-
rithms to the automation of various CPS, there are concerns
that affect their successful deployment in CPS. AML involves
the development of methods to compromise ML algorithms
and their output, consequently inﬂuencing their ability to make
right classiﬁcation or predictions. Initially, most of the research

13

on AML, especially for classiﬁcation and pattern recognition
tasks were generic in nature. However, since ML have been
proven to be very instrumental in the progress of CPS, a
number of researchers are beginning to explore the ﬁeld of
AML with focus on CPS. Rosenberg et al. [107] in their
study of adversarial learning in cybersecurity presented CPS
and industrial control systems as a case study. Cai et al. [108]
studied an advanced emergency braking system for self-driving
cars that operates by using DNN to estimate the proximity to
an obstacle. They therefore used a regression model based
on variational autoencoder to detect adversarial examples in
learning-enabled CPS and concluded that the proposed method
can detect adversarial examples effectively with a short delay.
This was an improvement on their earlier study aimed at
efﬁciently detecting out-of-distribution data capable of causing
errors and compromising safety in CPS [109]. The proposed
method used variational autoencoders and deep support vector
data description to learn models that efﬁciently identify and
compute disparity between input data during the movement of
a self-driving vehicle and the training set it was trained with.
Similarly, the authors in [110], [111] in a bid to complement
the predictions of DNN in VCPS computed trusted conﬁdence
bounds for learning-enabled CPS. Furthermore, Li et al. [112]
studied the challenge of adversarial attacks on ML models
used for energy theft detection in smart grids. Their major con-
tribution involved the development of a black-box attack that
compromised meter measurements and consequently reports
low power consumption measurements and so successfully
fools the ML algorithm used for energy theft detection. Clark
et al. [113] also investigated the impact of adversarial attacks
on the ML policies for controlling a robotic system. Finally,
Xiong et al. in [114] proposed attacks and defenses against
ML algorithms used in learning-enabled controllers.

The research presented above show that AML in CPS is
an active research ﬁeld and favorable results will facilitate the
deployment of CPS in real-life scenarios. In this section, a
brief overview of the subject, classiﬁcation of attacks peculiar
to ML, methods for crafting adversarial examples in DNN and
RL are discussed herein.

A. Overview of AML

The ﬁeld of AML brought to the fore the possibility of
attacks on ML algorithms through the crafting of adversarial
examples. The discovery that ML models (especially those
based on neural networks like DNN’s) can be fooled into
mis-classiﬁcations with a high degree of conﬁdence, by adding
some perturbations to the training samples was ﬁrst introduced
by Szegedy et al. [115]. Furthermore, Papernot et al. suggested
that the increased use of DL encouraged adversaries to deceive
such systems where they are employed [116]. In a quest
to explain this issue, Goodfellow et al. in [117] highlighted
that speculations attributed the possibility of adversarial ex-
amples in DNN’s to a combination of factors such as their
extreme non-linearity, insufﬁcient averaging of the model and
insufﬁcient regularization of the learning procedure. However,
the authors disputed these speculations and posited that the
linear behaviour of neural networks can be used to craft

14

Figure 10. Figure showing classiﬁcation of attacks

adversarial examples. On the basis of this discovery, they
therefore developed a fast method for creating adversarial
examples known as the fast gradient sign method (FGSM).

Over the years, interesting developments in the ﬁeld of
AML have followed since the discovery by Szegedy et al.
Beginning with various attempts to develop attacks which
operate stealthily and have the greatest impact in as little
time as possible, efforts are now geared towards developing
defenses to these attacks.

B. Classiﬁcation of attacks on ML

Attacks on ML models can be identiﬁed based on the goal
of the attacker, stage of the attack and the level of information
the adversary has on the targeted model. Recently, the ability
of an attack on a CPS to remain hidden has also become a
source of research focus. Figure 10 therefore shows the various
classiﬁcations of a attacks on ML algorithms.

1) Attacks based on the goal of the attacker: A study of
literature shows that there are different incentives for attacking
a ML algorithm. In [118], three of these goals were given as
conﬁdence reduction, random mis-classiﬁcation and targeted
mis-classiﬁcation. To reduce the conﬁdence of the system,
the attacker seeks to introduce an ambiguity in classiﬁcation.
However, the random classiﬁcation occurs when the attacker
changes the output classiﬁcation to a random one different
from the original. The targeted mis-classiﬁcation occurs when
the attacker seeks to supply the inputs or compel the system
to produce and output class different form the original [116].
2) Attack based on stage of the attack: Attacks on ML
models can occur either during the training or testing phases.
However, as a result of its relative simplicity, majority of
the attacks have been carried out during the training phase.
The attack strategies employed in this method include the
modiﬁcation of data through FDI and logic manipulation [31].
Attacks during the testing stage however operate after the
training has been completed. The goal of the adversary is
to inﬂuence the model
into making wrong classiﬁcations.
Figure 11 illustrates adversarial attack on a DNN in a ML-
enabled CPS. Attacks during the training and testing phases
are illustrated. Furthermore,
inference attacks on the ML
model is also shown.

3) Attack based on knowledge of model parameters: Adver-
sarial attacks have also been investigated from the perspective

Figure 11.

Illustration of adversarial attack on ML-enabled CPS

of the level of the knowledge of the internal architecture of
the ML models. To this end, the primary classiﬁcations include
white-box and black-box attacks. A hybrid of both of them,
known as the grey box attack also exists. However, the focus
here is on the white and black box attacks.

• White-box attacks: White box attacks represent a sce-
nario where the attacker has a knowledge of the model
parameters and the internal architecture of the model.
This include information on the type of neural network,
number of layers, and the number of neurons in these
layers. Along with a knowledge of the learning algorithms
and training process, the adversary seeks to modify these
parameters. This therefore makes it a targeted adversarial
method. Until recently, most studies in AML have fo-
cused on white-box attacks. However, it has been argued
that in real-world systems, it is impractical to assume that
the adversary will always have access to the information
about the parameters of the target system because of the
dynamic nature of such systems. There is therefore a high
motivation for studying the attacks on ML models in
scenarios where attackers do not have any information
about the gradient function.

• Black-box attack: In this type of attack, the adversary
has little or no knowledge of the internal structure or
architecture of the ML model. The adversary therefore
attacks the ML model by investigating the relationship
between the input and output data sets. Consequently, the
attacker either practices the attack action through the use
of an agent learning model, or a direct manipulation of
the input datasets that compels the model to perform mis-
classiﬁcations. An approach for black-box attacks was
proposed by Papernot et al. in [119]. In this approach
known as the transfer attack,
the attacker begins by
constructing a substitute model similar to the original
model. Afterwards, the substitute model is attacked using
the well known white-box attacks. The method proved not

only to be functional in DNN but other supervised learn-
ing algorithms like logistic regression, SVM, decision
trees and KNN. This approach was validated in [120],
[121].
However, according to [122], this approach of attack-
ing the substitute model to perform a black-box attack
has been faulted recently because studies has shown it
usually leads to much larger distortion and low success
rate. The authors in [123] therefore proposed the score-
based black-box setting that operates by querying the
softmax layer. For further improvement of the query-
based method, an autoencoder-based approach to reduce
query counts and an adaptive random gradient estimation
to balance query counts and distortion was introduced in
[124].
Without doubt therefore, black-box attacks give a true
picture of a real life scenario. Research into black-box
attacks in adversarial
therefore continue
to evolve in the coming years because since systems
themselves are seeking to be resilient to such attacks, it
will be very difﬁcult for the attacker to have a knowledge
of the internal architecture of the model.

learning will

C. Methods for crafting adversarial examples in DNN

In AML, the intention of the attacker is to generate a sample
as similar as possible to the normal sample by adding the
minimal perturbation to compromise the target model and
also hide the change from human eyes. Such perturbations
include fast gradient sign and other natural effects like fog
and sunlight. Adversarial examples are a major tool used by an
adversary to attack ML algorithms used in image classiﬁcation.
According to [29], studies in adversarial attacks are from three
perspectives namely non-targeted adversarial attacks, targeted
adversarial attacks and defenses against adversarial attacks.

Generally, considering an input image x, the adversary seeks
to ﬁnd a minimum perturbation η which when added to the
the input image, produces an adversarial input x = x + η that
can fool the system by causing it to mis-classify.

Studying the early works that brought

to the fore the
weakness and susceptibility of DNN’s to adversarial attacks,
it was seen that the various methods developed to detect the
adversarial examples and make the ML model resilient to such
attacks were dependent on the type of adversarial example and
the method used to craft it. It was therefore common to see
a method that proved to have a high accuracy in detecting
adversarial examples fail in the future when tested with a
newly developed adversarial example. From the foregoing, an
understanding of the methods for crafting adversarial examples
is therefore very pertinent to developing schemes for building
resiliency against them. The state-of-the-art in the generation
of adversarial examples have been classiﬁed into three namely
one-step gradient-based approaches, iterative methods and op-
timization based methods [125]. The gradient-based methods
are the most popular of the three. They are of real interest
in CPS security because autonomous vehicles and self-driving
cars rely on image classiﬁcation and pattern recognition to
autonomously drive the vehicles and make important decisions

15

as the vehicles travel on the road. A thorough understanding on
how perturbations can be generated to make the systems take
wrong decisions will enhance research in developing counter-
measures to make them resilient to such attacks. Some of
the well-known methods are brieﬂy discussed in this section.
Moreover, a summary of the peculiarities of the various
approaches is presented in Table II.

1) Fast Gradient Sign Method (FGSM): Goodfellow et al.
posited that the susceptibility of DL models to perturbations is
as a result of their linear behaviour. They therefore developed
the FGSM for crafting adversarial examples. Introduced in
[117], adversarial perturbations to input images were crafted
using the sign of the gradient or derivative of the models loss
function with respect to the input feature vector. Furthermore,
the authors in [29] described the FGSM as a method which
creates an adversarial example by adding some weak noise
to every step of optimization that approaches or moves away
from the expected class. The FGSM therefore seeks to fool
the ML model into making wrong classiﬁcation of the image
through the addition of a small vector which is usually difﬁcult
to notice.

Just like the general case highlighted above, the minimal
perturbation η is obtained by perturbing each feature of the
input image in the direction of the gradient. The mathematical
expression is given below:

η = (cid:15) ∗ sign(∇xJ(θ, x, y))

where η represents the minimal perturbation, (cid:15) is a parameter
that is used to determine the perturbation size, J(θ, x, y) the
cost or loss function for training the DNN, θ represents the
model parameters, x is the model input and y the targets to the
model. The authors evaluated the performance of the method
with MNIST and CIFAR-10 datasets. The results obtained
conﬁrmed that the method was able to fool DNN’s to make
wrong classiﬁcations.

Since the introduction of the FGSM, other methods to boost
the generation of adversarial example using FGSM have been
developed. This include iterative variants of the gradient based
method [126]. Recently, it was argued that FGSM, being a
one-step gradient-based method can only generate adversarial
examples with high transferability when applied in a white-
box model. A disadvantage is that it achieves a low success
rate when trying to fool a black-box model. Since black box
models are more practical in real world applications and sys-
tems now have their own defense mechanisms, it is important
to develop attacks that can attack such models effectively
without a prior knowledge of the internal architecture of the
model. The solution proposed was the momentum iterative
gradient-based methods to boost the examples,making it more
transferable and achieve high success rates when applied in
both white and black box models. The momentum based
methods operate by iteratively accumulating a velocity vector
in the gradient direction of the loss function with the principal
aim of stabilizing update directions and shunning poor local
maxima. This method is an improvement on the one-step
gradient methods and the iterative methods. Readers who are
more interested in this topic can refer to [125] for a detailed
understanding. Judging from the fact that the method won the

Table II
SUMMARY OF RESEARCH ON METHODS FOR CRAFTING ADVERSARIAL EXAMPLES

Paper

Method

Contributions

Comments

16

Szegedy et al. [115]

Solved penalized optimization
problems

Goodfellow et al. [117]

Fast Gradient Sign Method
(FGSM)

Kurakin et al. [126]

Basic Iterative Method (BIM)

Papernot et al. [116]

Jacobian Saliency Map Approach
(JSMA)

Carlini and Wagner [127]

Carlini & Wagner

Moosavi-Dezfooli et al. [128]

DeepFool

Baluja et al. [129]

Adversarial Transformation
Network

The method was time consuming
and did not scale well to large
data sets

The ease of generating adversarial
examples have made it very
popular. Initially used to test
the effectiveness of most users
but later found to be a relatively
weaker form of adversarial attack.

It is an improvement on the
FGSM, although the iteration
sacriﬁces some level of speed

A targeted attack, the need to
modify only a limited number of
pixels in an input image makes it
efﬁcient

Robust against the defensive
distillation method of adversarial
defense and therefore highly
recommended for testing methods
for adversarial defense

This method computes a more
optimal adversarial perturbation
and used to show that adversarial
training signiﬁcantly increases
robustness

A relatively new method that has
not been well explored but has
a great potential to be the new
direction in this research area

Introduced the concept of
adversarial instability in neural
networks; estimated adversarial
examples by solving optimization
problems

The method is fast and thus
consumes less resources thus
making the process of adversarial
training a reality

Applies the FGSM method
iteratively, with a reduced step
size and clipping of pixel values
after each step

Also gradient based, the adversary,
with a knowledge of the target
model constructs adversarial
saliency maps which it uses to
detects input features that have the
largest impact on classiﬁcation of
output and then attacks them with
large perturbations

Three targeted gradient-based
attacks; CW0, CW1 and CW2
that were based on L0, L2 and
L∞ norms respectively. More
effective that the previous attacks

Improves on other methods
by accurately computing the
robustness of deep classiﬁers to
adversarial perturbations especially
in large data sets, and thus helps
build more robust classiﬁers

A separate network is trained to
attack the target network, and
any input can be turned to an
adversarial input. The advantages
include fast and efﬁcient training
due to a single-forward pass, non-
transferability and the choice of
controlling the nature of mis-
classiﬁcation

ﬁrst place positions in the targeted and non-targeted adversarial
attack competitions in NIPS 2017, a knowledge of the method
is very important.

The FGSM since its introduction has become very popular
because it
is fast, simple and requires less computational
resources thus making it very practicable. It has also been used
to test the adversarial training method for enhancing resiliency
of ML algorithms and helped to advance the research endeav-
ors in this area. Adversarial training is discussed extensively
in a later section.

2) Basic Iterative Method (BIM): Developed by Kurakin et
al., it is an iterative variant of the FGSM [126]. The results
obtained showed that the attacks was more effective than the
FGSM attack. In this method, the adversarial noise η is applied
many times with a relatively small magnitude of the parameter
(cid:15). One of the major beneﬁts of this type of attack is the power

it gives the adversary to control the attack. Furthermore, the
BIM attack can be used by the adversary to successfully fool
the network even when adversarial training is used to make
the neural network robust. Adversarial training can increase
the robustness of neural networks against a one-step FGSM
attack but in a case where the attack is iterative, adversarial
training will need to be adaptive to defend the model against
the attack..

3) Jacobian-based Saliency Map attack (JSMA): This type
of attack also operates iteratively and focuses on targeted mis-
classiﬁcation. Proposed in [116], the attack operates by using
the forward derivative of a DNN to compel the model to
classify into a predetermined class. The iterative nature of the
attack makes it have a better success rate.

4) Carlini and Wagner: The Carlini-Wagner attack [127],
[130] attracted a lot of attention when it was introduced

because of its ability to overcome the popular defensive dis-
tillation, which was proven to have the capability to overcome
the FGSM attack. The approach has three attacks namely the
CW2 attack, CW0 attack and CW∞ attack, based on the L2,
L0 and L∞ norms respectively. These three attacks generated
adversarial examples that successfully fooled neural networks
using the defensive distillation into wrong classiﬁcations. The
authors therefore recommended their approach as the relevant
method for testing the effectiveness of any approach to be used
in building models that are resilient to adversarial examples.
5) DeepFool: The DeepFool method for crafting adversar-
ial examples was proposed by Moosavi et al. [128]. DeepFool
uses concepts from geometry to guide the search for the
minimum perturbation needed to deceive a classiﬁer to make
wrong classiﬁcations. Furthermore, it uses the L2 minimiza-
tion method to search for adversarial examples. Through an
iterative linearization of the classiﬁer, the smallest perturba-
tion needed to compromise the classiﬁcation of samples is
generated.

6) Adversarial Transformation Networks (ATNs): Baluja
and Fischer [129] proposed a novel method for developing
attacks for neural networks. The ATN operates by training
a separate network to attacks another target network. In
principle, adversarial examples can be generated by training
the network to generate the perturbation to the input or
an adversarial auto-encoding of the input. The possibility
of generating both targeted and untargeted attacks and also
executing training in a white-box or black-box manner makes
the method attractive. Furthermore, ATN’s have the advantage
of granting the attacker the power to determine the nature
of mis-classiﬁcation that occurs in the target network and
also reveal weaknesses in the target classiﬁer. It is pertinent
to state that being one of the latest attacks developed, the
ATN has not been subjected to enough discussion to prove
its efﬁciency beyond the report of the authors. A comparative
analysis of its ease of detection relative to other attacks will be
instrumental in verifying its qualities. However, the qualities of
training efﬁciency, need for single forward pass and its ability
to convert any input into an adversarial example make it have
a potential to contribute to research in AML.

D. Adversarial Attacks in RL

The applications of DNN’s in CPS, as previously studied are
typically supervised as they majorly perform image classiﬁca-
tion and pattern recognition tasks. The security and resiliency
of such ML algorithms have been discussed extensively in the
previous section. However, RL have recently become a very
active research area. The ability of RL to achieve signiﬁcant
performance in various decision making tasks that involve
uncertainty have endeared them to all. Results of endeavors by
the industry and academia show that it has solved a myriad of
problems, especially in VCPS research. The attacks on RL
algorithms differ from those on DNN’s. However, there is
some intersections because of the use of DRL. Speciﬁcally,
DNN’s are used to approximate the action-value function.
The policy, because of its role in directing the agent on the
most effective or rewarding action to take in response to

17

the state of the environment is usually the target of many
attacks. Moreover, it is pertinent to state that although not
much attention has been given to adversarial attacks in DRL
in the past, this is bound to change in the coming years as
DRL continues to extend from the initial video games to more
critical systems like robotics and autonomous vehicles.

The state-of-the-art approach in research into security and
resiliency of ML models usually begins with the develop-
ment of attack and threat models to ascertain their ability
to compromise the model. Adversarial attacks on DRL can
be classiﬁed into three. These include those that target the
reward by perturbing it directly or the reward signals through
the states; attacks that target the DRL policy by perturbing the
states, perturbing the environment, involving an adversarial
agent and model extraction attacks; attacks that target the
observation by perturbing the states; and attacks that target
the environment [131]. One of the earliest study in adversarial
attacks in RL was carried out by Behzadan et al. in [132].
The research afﬁrmed that the policy and induction in DQN, a
common type of RL technique are vulnerable when adversarial
examples are introduced into the input, and that transferability
of adversarial examples is also possible from a DQN model
to another. The attack mechanism developed by the authors
depended on previously discussed methods for crafting adver-
sarial examples like FGSM and JSMA. The policy induction
attack presented has an adversary that trains a DQN with an
adversarial reward, and then uses the trained policy to craft
targeted adversarial examples that attract the agent to take
actions leading into obtaining the adversarial reward.

Furthermore, Huang et al. [133] also towed the same line
to show that existing adversarial examples crafting techniques
can be used to limit the performance of RL policies. The
authors state that although the research presented in [132]
focused on adversarial attack at the training phase of the agent
to prevent learning, their work was an improvement because
it presented adversarial examples at test time to investigate
adversarial attacks on an RL agent. It is pertinent to also
state that the study was based on white-box and black-box
settings. The authors in [134] also used the FGSM attack in
their study on adversarial attacks in DRL policies. The major
contributions of the research include a comparative analysis
of adversarial examples and random noises in attacking DRL
policies, and the exploration of the value function of the
policy as guide in the injection of perturbations. This novel
method has the advantage of reducing the time the adversary
expends in injecting examples to record a successful attack
and therefore increases the probability of it remaining unde-
tected. Lin et al. [135] also proposed the limitation of the
time of operation of an adversary as a method to guarantee
the stealthiness and efﬁciency of adversarial attacks in DRL
agents. To this end, using the C&W method [127] for crafting
examples, they proposed the strategically-timed attack and the
enchanting attack. The strategically-timed attack operates by
determining the most effective time an adversarial example
should be crafted and selectively attacks at a subset of the
time steps as against the usual uniform attack presented in
[133]. The enchanting attacks combines a generative model
for the prediction of future states and a planning algorithm for

the generation of predetermined actions to lure the agent to a
desired target state after a number of steps. Signiﬁcant among
the results presented is that the strategically-timed attack can
achieve the same effect as the uniform step attack, despite the
agent being attacked four times less.

Sun et al. [136] also predicted the critical point attack and
antagonist attacks. Just like the attacks in [135], both attacks
operate by building a model to predict the future states of the
environment and the action of the agent and then select that in
which maximum damage can be achieved in minimum steps.
The C&W approach for generating adversarial attacks is also
used in the research. The authors in [137],while critiquing the
works in [133], [134] as unrealistic for ignoring the dynamic
nature of RL models by assuming that attacks can be generated
per state proposed the targeted attacks on DRL agents through
observations. They also argued that the general perception in
the adversarial RL community that the goal of attacks is to
largely achieve a drop in the performance of the model is
untrue as it may also be to lure the agent into an action
determined by another policy. They therefore proposed the
per-observation attack and the universal-masks attacks using
the FGSM method of crafting adversarial examples.

Deviating from the use of the FGSM and C&W methods for
crafting adversarial examples, Tretschk et al. [138] in studying
adversarial attacks in RL were the ﬁrst to use the ATN to learn
to generate the attack against the policy network. Their goal
was to show that unlike the state-of-the-art, a sequence of
attacks can be used to thrust a random adversarial reward on
the policy of the target system. The effect of this is that the
target agent can be deceived to optimise for the adversarial
agent as a result of the sequential attacks. This approach,
though similar to those in [132], [135] is unique due to the
use of the ATN and the application of the attack at test time.
With a goal to investigate the application of DRL in CPS,
Lee et al. [139] proposed the white-box Myopic Action Space
(MAS) and the white-box Look-Ahead Action Space (LAS)
attacks. In contrast to other works which have focused more on
attacking the RL agents state space, the attacks presented target
the RL agents action space which corresponds to actuators in
CPS. Based on the state-action dynamics, the MAS is formu-
lated as an optimization problem with decoupled constraints
on the attack budget while the LAS operates by spreading the
attack across the temporal and action dimensions.

Just like in DNN’s, black box attacks in RL are also more
challenging than white box attacks because of the lack of
information about the internal architecture and parameters of
the target model. Although most research works have focused
on the latter, there are recently few research that have shown
results for black box attacks in RL [140], [141]. Inkawhich et
al [142] posited that the state-of-the-art in black box attacks
in RL which involves training a proxy agent and assuming
that the adversary has a full knowledge of the environment
is unrealistic. This is because, unlike in supervised learning
where samples are from a static dataset, data generation in RL
stems from the continuous exchange of the state, action and
reward signals between the agent and state of the environment.
They therefore proposed the snooping threat model, which
operates with an assumption that the adversary, without access

18

to the environment of the target resolves to eavesdropping on
subsets of the RL signals at each time step. They therefore
prove that by training proxy models on tasks similar to that
of the target agent, adversarial samples can be crafted to
compromise the performance of an RL agent. Interestingly,
Pattanaik et al [143] proposes adversarial attacks for RL
and then leverage on the proposed attacks to improves the
robustness of DRL to parameter uncertainties.

Recently, other attacks on RL algorithms have been pro-
posed. Our focus have been on methods that have received
relatively wider recognition by the research community. For
easy reference, Table III presents a summary of the attacks
on RL. In the next section, we will look at techniques for
developing robust and resilient RL models.

E. Summary and lessons learnt

We have seen that recent trends in ML security have focused
principally on resiliency of ML models to black-box adver-
sarial attacks and the transferability of these attacks to other
supervised learning models. However, the effect of adversarial
attacks on unsupervised ML algorithms have not been well
investigated. Some of the factors responsible for this neglect
is the difﬁculty in deﬁning what constitutes an adversarial
example for clustering algorithms as a result of the absence of
labels. Moreover, the inherently ad-hoc nature of unsupervised
ML algorithms (such as clustering algorithms) also contribute
to the factors that make adversarial ML a relatively more
difﬁcult task in comparison to supervised learning algorithms.
Chhabra et al. [144] in an attempt to address these concerns
developed an iterative black-box adversarial attack whose goal
is to craft adversarial examples that fooled four clustering
algorithms. Moreover, they studied the issue of adversarial at-
tacks transferability in unsupervised ML models. Furthermore,
the same authors in [145] proposed a deﬁnition for adver-
sarial examples in clustering algorithms. Consequently, they
presented a powerful black-box adversarial attack algorithm
against clustering algorithms for linearly separable clusters.
Their simulation results showed that the proposed method
succeeded in generating adversarially perturbed samples by
changing the decision boundary and therefore ensuring that
the examples were mis-clustered.

With these recent discoveries, it is expected that research
into adversarial attacks on unsupervised learning algorithms
will be given more attention by researchers. However, whether
this attention will be as intensive as the supervised learning
algorithms remains a doubt as ML algorithms in themselves
have become very popular for their role in classiﬁcation.
We therefore posit that attention will focus on supervised
and reinforcement
learning than on unsupervised learning
algorithms, especially for CPS applications.

VI. SECURE/RESILIENT ML

Previous sections of this paper have shown that ML algo-
rithms enhance resiliency in CPS. Speciﬁcally, the communi-
cation, control and computing tasks can be been made robust
to ensure their continuous operation during adversarial attacks.
However, a recent research problem is that ML algorithms

19

Table III
SUMMARY OF RESEARCH ON ADVERSARIAL ATTACKS ON REINFORCEMENT LEARNING

Research

Name of Attack

Method of crafting adversarial
examples

Contribution

Adversarial Attacks on Neural
Network Policies [133]

White Box / Black Box

Fast Gradient Sign Method

Tactics of Adversarial Attacks on
DRL agents [135]

Strategically-timed & Enchanting
attacks

Carlini & Wagner

Delving into Adversarial Attacks
on Deep Policies [134]

Value Function (VF) attack

Fast Gradient Sign Method

Spatiotemporally Constrained
Action Space Attacks on DRL
agents [139]

White-box Myopic Action Space
(MAS)/White-box Look-Ahead
Action Space (LAS)

Similar to FGSM, with standard
gradients computed

Vulnerability of Deep Reinforce-
ment Learning to policy induction
attacks [132]

Policy induction attacks

FGSM & JSMA

Targeted Attacks on DRL Agents
through adversarial observations
[137]

Per-observation attack &
Universal-masks attacks

Fast Gradient Sign Method

Snooping Attacks on Deep
Reinforcement Learning [142]

Snooping threat models

Fast Gradient Sign Method

Stealthy and Efﬁcient Adversarial
Attacks against Deep Reinforce-
ment Learning [136]

Critical point attack and
Antagonist attack

Carlini & Wagner

Sequential Attacks on Agents for
Long-Term Adversarial Goals
[138]

Sequential attacks

Adversarial Transformer Network
(ATN)

The FGSM in [117] is extended to
the DRL domain to successfully
fool agents. They also showed that
the transferability property also
holds in RL.

Limits the duration of an attack
to achieve optimal effects thus
guaranteeing stealthiness. Also
capable of luring an agent into a
targeted state.

The frequency of injection of
adversarial examples into the agent
is signiﬁcantly reduced by guiding
the attacker to attack at crucial
moments using computations based
on VF

The adversarial attack focus on
the agent’s action space (which
represents actuators in CPS) as
against other works that have
focused more on the agent’s state
space

First to establish the possibility
of adversarial examples and
their transferability in Deep Q-
Networks. Using a game scenario,
an attack mechanism to exploit
perturbation and transferability to
attack the policy was proposed.

The attacks here are targeted and
also aimed at observation of the
environment and not the internal
state of the agent as usually
considered. The attacks are also
constant and not per-observation as
usual.

Unlike other works, the adversary
has no personal interaction with
the environment; it eavesdrops the
action and reard signals exchanged
and then launches an attack on an
agent by training proxy models on
related tasks and then transferring
the attacks.

The critical point attack can
predict the environment states
and locate the critical moments
to incur the most damage while
the antagonistic attack can
automatically identify the optimal
attack strategy using the least
attack cost.

Sequential attacks are applied
at test time with adversarial
examples crafted using ATN with
the ultimate aim of manoeuvring a
target policy network to pursue an
adversarial reward.

themselves are susceptible to attacks by the adversaries. Such
attacks include data integrity attacks during the training or
testing stages using data poisoning or the use of adversarial
examples to fool the ML model to make wrong classiﬁcations.
The use of GAN’s have also made it easier to fool ML models.
It is therefore pertinent that ML algorithms themselves must be
made resilient to such attacks in order to guarantee safety and
security of the systems where they are deployed. Therefore,
due to the importance of ML to ﬁelds of cybersecurity,
CPS and IoT, efforts must continue to ensure that these ML
models are resilient to such attacks. Research efforts to achieve
resilient ML is the focus of this section.

A. Defense against adversarial attacks in DL

in the quest

Papernot et al. [146] suggested that certain requirements
must be met
to design defense mechanisms
against adversarial perturbations. These include the need to
limit impact on the architecture of the network, maintain the
classiﬁcation accuracy and speed of the target neural network.
Although, research in this area is still at its infant stage, the
interest is expected to rise signiﬁcantly in the new decade as
stated in many forecasts and white papers on the future of
cybersecurity and AI. Various works are presently ongoing to
address this challenge and some solutions have been proposed.
This section therefore discusses the techniques or strategies
for guaranteeing resilient ML. The majority of research pre-
sented focus on supervised learning tasks such as computer
vision and image classiﬁcation tasks using DNN’s. From
Figure 12, it is evident that defense against adversarial at-
tacks can generally be achieved in three major ways. The
ﬁrst is a modiﬁcation of the input through techniques like
adversarial training [117], transformation of the input through
processes like compression and reduction of the bit-depth
[147], randomization of the data [148] and a regularization
of the input gradient [149]. Next, defense against against
adversarial attacks can also be achieved through a modiﬁ-
cation of the network structure through techniques such as
defensive distillation [146], multimodel-based defense [150],
the addition of a detector sub-network [151] and also the
addition of a high-level representative guided denoiser [152].
Finally, the objective function can also be modiﬁed by adding
a stability term [153], adding a regularization term [154] and
the stochastic activation pruning [155].

The most popular methods of adversarial defense are dis-

cussed further in this section.

1) Adversarial Training: Adversarial training is one of the
strongest methods of making ML algorithms resilient to adver-
sarial attacks. In this method, examples are generated during
the training of the ML model to harden it. Initially introduced
by Szegedy et al. [115], it was not fully implemented because
of the challenge of generation of adversarial examples at the
time. However, with the development of the FGSM, a fast
method for generating adversarial examples, more extensive
work was implemented and reported by Goodfellow et al.
[117].

The adversarial training procedure therefore has a goal of
minimizing the worst case error when an adversary introduces

20

perturbations into the input data. Moreover, it can be likened
to a form of active learning where the model has the ability
to request labels on new points. Research has also shown that
they are able to achieve high success rates. However, in a bid
to improve the method and address certain drawbacks, variants
of adversarial learning have since been developed to improve
the resilient of ML algorithms to attacks.

Tramer et al. [156] proposed ensemble adversarial training
for adversarial defense of ML models. This model differs from
the method initially proposed by separating the generation of
the examples from the trained model. The method therefore
enhances the training data and consequently the robustness of
the target model to black-box attacks. Na et al. [157], with a
focus on unknown iterative attacks also proposed the cascade
training to enhance the robustness of DNN’s.
adversarial
Building on the principle of ensemble adversarial training, the
cascade adversarial training uses already defended network for
generation of iterative FGSM images, in addition to the one-
step adversarial images crafted from the network being trained.
2) Mitigation against adversarial examples through ran-
domization: Another method proposed for securing ML mod-
els against adversarial attacks using adversarial examples is
the used of randomization [148]. According to the authors,
carrying out randomization at inference time through random
resizing and padding is capable of making systems resilient
against adversarial attacks. The advantages of this method
included the ability to make the network much more robust
to adversarial images (especially for iterative attacks including
white-box and black box attacks), non-requirement for training
or ﬁne-tuning, requirement for few computations thus having
no real effect on computational complexity, and their compati-
bility with different network structures and adversarial defense
methods.

3) Defensive Distillation: Defensive distillation is another
principle for guaranteeing resiliency of ML algorithms used
in CPS. First proposed by Papernot et al. [146] as a defense
mechanism for adversarial attacks especially within the context
of DNN’s, the principle had earlier been suggested by [158]
and formally presented by [159]. Primarily developed for
it
solving computational complexity challenges in DNN’s,
involves the transfer of knowledge from one model to another.
There are three training steps in defensive distillation. First,
a network is trained using the normal known techniques.
Then, it is evaluated by every instance of the training set to
produce soft labels. Lastly, a second network known as the
distilled network is trained on the soft labels generated in the
second step. Papernot et al. further proved the effectiveness of
defensive distillation in mitigating adversarial attacks created
using the FGSM and the Jacobian-based iterative approach by
providing experimental results [160].

However, as newer adversarial examples were crafted, the
challenges of defensive distillation were also unmasked. The
major discovery is that it did not work well in detecting certain
adversarial examples [119], [127]. The authors in [127] posited
that slightly modifying a standard attack such as FGSM,
defensively distilled networks can still fall prey to adversarial
examples. Speciﬁcally, it was observed that since distillation
operates as a defense mechanism to adversarial examples by

increasing the magnitude of the inputs to the softmax layer, a
successful attack can be achieved by reducing the magnitude
of the input. Seeking to extend the principle of defensive
distillation with the knowledge of its challenges as highlighted,
the proponents of the technique suggested that it is more
effective in white-box than in black-box attacks. They however
the method does not require
concluded that
the defender to generate adversarial examples still makes
it a method of choice. They also propose using defensive
distillation together with adversarial training to improve ts
efﬁciency. [161].

the fact

that

The use of defensive distillation as a technique for defense
against adversarial attacks is still potent. However, there is
a need for improvement so that new forms of attacks can
be addressed. Furthermore, it was stated in [127], [130] that
although this method of adversarial defense can only be
validated on available attacks, there is a need to understand the
intrinsic qualities of the mechanism that determine its success
and also investigate further by modifying other parameters
with a goal to ascertain if the defensive duties will still be
properly carried out. This serves as a proactive step to ensure
that when parameters are modiﬁed, a defensive mechanism
does not lose its power.

4) Gradient Masking: Studies have shown that most of the
methods used to construct adversarial examples depend on the
knowledge of the gradient of the model to operate effectively.
A method known as gradient masking was therefore proposed
for protecting DNN’s [162]. Like the name implies, the gra-
dient masking method is built around preventing the attacker
from gaining knowledge of the gradient of the model. After
gradient masking has been applied to a neural network, the
result is usually a model that is smooth in speciﬁc neighbor-
hoods of training points. Consequently, without a knowledge
of the gradient of the model, the attacker will not know in
what direction to perturb the input and thus succeed in making
the model mis-classify. Recently, newer methods that operate
similar to gradient masking have been referred to as obfuscated
gradients. The authors in [163] achieved this using gradi-
ent shattering, stochastic gradients and vanishing/exploding
gradients. They therefore proposed three methods; backward
pass differentiable approximation (BPDA), expectation over
training (EOT) and re-parameterization to show that majority
of the methods that depended on obfuscating gradients to
prevent the generation of adversarial examples can be broken
[164].

Furthermore, techniques earlier highlighted for launching
successful attacks against black box models also affect the
efﬁciency of gradient masking [119]. Speciﬁcally, the ability
of an adversary to train a substitute model which mimics the
target model or model defended with gradient masking, and
then use the input-output relationship of the substitute model
to attack the targeted model weakens the argument for the
use of gradient masking and other methods that only prevent
gradient descent-based attacks as a defense mechanism in
neural networks.

5) Detection Methods: Adversarial attacks against ML
models can also be detected. Methods for detecting such
attacks begin by analysing the input sample to ﬁnd out

21

variations from the normal samples. Input samples which
differ statistically from the normal examples are identiﬁed
as adversarial. Differing from other methods like adversarial
training and gradient masking that seek to harden DNN’s
by modifying the model, Xu et al. [165] proposed a new
method known as feature squeezing for detecting adversar-
the feature input spaces are
ial examples. Observing that
usually large and therefore an incentive for the construction
of adversarial samples, they suggested that it is possible to
squeeze or coalesce the unnecessary input features and so
restrict the attack surface of the adversary. A comparison of the
prediction of the model on samples before and after squeezing
helps the model infer that a sample is adversarial if outputs
are substantially different. Applying the proposed method in
image classiﬁcation, they use two feature squeezing methods
to reduce the color bit depth of each picture and also for spatial
smoothing. They concluded that the method is good because it
is inexpensive, achieves high accuracy and few false positives
and can also be used to complement other techniques.

Other squeezing methods can also be used to protect
DNN from adversarial attacks. Furthermore, Feinman et al.
[166] proposed two features known as density estimates and
Bayesian uncertainty estimates for detection of adversarial
examples in DNN. Lu et al [167] also developed SafetyNet,
a method for detecting and rejecting adversarial examples.
The detector in this case ascertains that the image and depth
map are consistent, hence identifying an adversarial example
if a contrary situation occurs. Furthermore, Roth et al. [168]
examined the change in features and log-odds when noise was
applied to input samples to detect the operation of adversaries.
They discovered that a characteristic direction is maintained
during adversarial attacks while there is no speciﬁc direction
in a normal scenario. Other methods include the use of
prediction difference [169], neural ﬁngerprinting [170], stress
response [171], feature distillation [172], incremental learning
of GAN’s [173], convolutional ﬁlter statistics [174] and Fisher
information [175].

6) Lessons learned: Despite all the research work carried
out in detecting adversarial examples and improving the re-
siliency of DNN to such attacks, there is still more work
to be done. Carlini et al. [176] posited that the detection of
adversarial examples is not an easy task. A survey of ten of
the proposed detection methods showed that they all can be
fooled when new loss functions are constructed. The authors
therefore conclude that the existing defense mechanisms have
not been thoroughly subjected to tests and that research into
the detection of adversarial examples is yet to reach a satisfac-
tory point. Furthermore, in [163], the same sentiments were
reemphasized. The authors therefore called for more intensive
threat models that do not limit activities of the attacker and
also have an idea of the structure of the defense model.

Few years after making this discovery, a study of liter-
ature still shows that there is still no defense method that
can adequately withstand all attack mechanisms. Researchers
who propose defense methods should therefore ensure to test
them with as many attacks as possible and also carry out
research into ﬁnding loopholes attackers can use to launch
attack. However, we acknowledge that attack and defense

mechanisms will continue in a game-like manner. Discovery
of new defense methods will birth the launch of new attacks.
The general consensus that attacks are relatively easier to
construct than defense mechanisms calls for intensiﬁed efforts
from researchers. We posit that researchers should focus on re-
evaluating state-of-the-art defense methods before attempting
to propose newer schemes. This will greatly enhance research
in developing resilient ML. Furthermore, the potentials of ad-
versarial training are great. However, issues of computational
complexity and the time of reaction to attacks in real-life CPS
applications need for attention if the proposed defense methods
will be deployed in such scenarios.

B. Defense against adversarial attacks in RL

Having already discussed the possibility of attacking RL,
the security of these systems is of concern to the researchers
all over the world. The reality that adversarial attacks cannot
be entirely isolated using access control methods is a call to
develop algorithms that can operate despite these attacks. Qu
et al. in [177] investigated the minimal requirements necessary
to launch an adversarial attack in DRL. They considered cases
where the attacker is only knowledgeable of the state and
action, perturbs only a small number of pixels and attacks
only some signiﬁcant frames of the RL model. The simulations
results show that attacking a single pixel which corresponds
to 0.01% of the state or attacking around 1% frames totally
fooled the trained policy using DQN. This research therefore
shows that without developing robust defense against attacks,
DRL cannot be used in critical infrastructure like in robotics
and ITS.

Research on defense of DRL against adversarial attacks
is still in its infant stage. Many of the techniques for the
resiliency of DRL stems from those developed for DNN’s.
Ilahi et al. [131] classiﬁed defenses for adversarial attacks
on DRL into adversarial training, robust learning, adversarial
detection and defensive distillation. However, some of the
researchers, in presenting their attacks recognized the chal-
lenge of defending systems against these attacks and therefore
gave recommendations. The major method recommended for
resiliency of DRL models is the adversarial training. With
regards to images classiﬁcation tasks using DNN, it has been
proven that defensive distillation is not a reliable method for
protecting ML models from attacks and thus it has not been
considered for DRL by most researchers. The different variants
of adversarial training suggested are presented ﬁrst and then
other methods are discussed.

Behzadan et al. [178] investigated the resilience of DRL to
training and test time attacks and concluded that the policies
that are learnt with adversarial training can withstand test time
attacks better. Havens et al. [179] stated that most strategies
for defending RL policies against adversarial attacks using
adversarial training are usually in an off-line method. The
disadvantage of this is that such strategies cannot adapt when
the attack is online. Also, another demerit of other techniques
is that the defense mechanisms are only effective for speciﬁc
attacks. With these concerns in mind, the authors proposed
the meta-learned advantage hierarchy (MLAH), an algorithm

22

that detects and mitigates attacks on the state of the algorithm.
The algorithm operates by using the advantage map, a metric
estimated by a comparison of the expected return of a state to
the observed return of some action to determine the presence
of an adversary. The master agent, using the advantage map
is able to switch between the nominal and adversarial sub-
policies in correspondence to the attack scenario. The ad-
vantages of the MLAH algorithm include its online nature,
ability to operate in the decision space and its effectiveness
irrespective of the type of attack. However, the nature of
the algorithm makes it computational intensive and increases
the delay in detection because the target agent has to be
fooled before the master agent can begin its defense procedure.
Furthermore, the adversarially robust policy learning (ARPL)
was proposed in [180]. This algorithm, targeted at the defense
of autonomous agents in physical domains like self-driving
cars and robots uses adversarial agents during the training
of RL agents to make them resilient to adversarial attacks in
the form of changes in the environment. The authors start by
proposing a method to generate adversarial perturbations that
are plausible in physical systems. Thereafter, they proceed to
use the ARPL to actively select the perturbations that are used
to train the policy to make it more robust. Analysis of the effect
of perturbations on performance in the presence of dynamics
noise, process noise and observation noise makes this research
well relevant to the resilient RL research. With future work
such as the development of a theoretical justiﬁcation for the
algorithm and testing it on a physical robots, there is still
enough room for research to make the algorithm deployed in
real systems.

For adversarial detection, Lin et al. [181] proposed a defense
method that both detects adversarial attacks in DRL and also
provides suggestions on actions to be taken under such attacks.
In this method, a visual foresight module is trained to detect
the presence of adversarial examples by comparing the current
action of the policy with the action generated by the same
policy using a predicted frame. The Stochastic Activation
Pruning (SAP) method proposed by Dhillon et al. [155] draws
its inspiration from game theory. The SAP method prunes a
random subset of activations and then scale up the ones left
to makeup for the loss.

Due to the challenges associated with using adversarial
examples for improving the robustness of DRL models to
adversarial attacks, researchers have now focused on methods
that seek to certify or verify the robustness bounds. These
methods depend on the approach proposed by Weng et al. and
initially used for classiﬁcation tasks [182] . However, Oikari-
nen et al. [183] posited that the reason there are fewer research
works that use this approach is the presence of challenges like
an absence of a stationary training set and distinct right action
for each state. Everett et al. [184] also towing the same line
of certiﬁed adversarial robustness developed an online method
for robustness of DRL algorithms. The proposed method;
CARRL selected the action with the highest guaranteed lower
bound Q-value during the execution process. The policy learnt
also had an advantage of providing a certiﬁcate even when
the certiﬁer is unaware of salient details. The approach was
tested with a DQN policy and conﬁrmed to improve robustness

23

Figure 12. Categorization of methods for defense against adversarial attacks

when applied to pedestrian collision avoidance scenarios and
task. Similarly, Wang et al. [185] used
a classic control
the same approach but extended the research on robustness
certiﬁcation to a dynamic setting which is similar to the CPS.
They therefore developed an algorithm for the certiﬁcation of
robustness of a DRL in a feedback control loop experiencing
persistent adversarial attacks. Their method was conﬁrmed to
perform better than the conventional Lipschitz-based robust
control approach, especially where the knowledge of the model
dynamics is unknown. Other methods in this category were
proposed in [186], [187]

Oikarinen et al. [183] however submitted that the authors
in [184], [185] did not propose techniques for training more
robust models. They therefore proposed the RADIAL-RL, a
method that improves the robustness of DRL agents through
the design of adversarial loss functions coupled with robust-
ness veriﬁcation bounds during the training process. They also
proposed the greedy worst-case reward (GWC) which provides
as a good estimate of the reward under the worst case sequence
of adversarial attacks. By accounting for the importance of
each action, GWC has an advantage over other approaches
that only evaluate whether each single action is affected by
input perturbations. This approach is similar to those proposed
in [188], [189]. Fischer et al. [188] proposed Robust Student-
DQN (RS-DQN), a method that uses both adversarial training
and certiﬁed defense for the defense of DRL. The major
contributions include splitting the DQN architecture into a
student and a Q network. While the student network is

used for exploration, the Q network is used for conventional
training. The method therefore depends on imitation learning
for a robust prediction of actions. However, the authors in
[189], while acknowledging the efﬁciency of RS-DQN stated
that it does not detail its behavior in environments without
continuous action spaces. They therefore proposed the state-
adversarial Markov decision process (SA-MDP), a method that
does not depend on imitation learning and performs better in
eleven test environments. The results showed that their method
improved the adversarial robustness in PPO, DDPG and DQN
agents.

Qu et al. [190] faulted the state-of-the-art approach of
boosting adversarial robustness in DRL through policy distil-
lation using adversarial training. They posited that by adding
adversarial examples while the student policy is being trained,
the robustness of the model suffers when it encounters a new
attack. Moreover, this approach increases the computational
cost of the model. To address this challenge,
the authors
proposed a new policy distillation method that does not include
generation of adversarial examples during training for the
defense of RL models against adversarial attacks. Speciﬁcally,
they designed a policy distillation loss function that consists of
the prescription gap maximization (PGM) loss and Jacobian
regularization (JR) loss. The PGM loss maximizes both the
probability of the action selected by the teacher policy and
the entropy of the unwanted actions while the JR minimizes
to the input state.
the norm of the Jacobian with respect
Theoretical and experimental analysis shows the accuracy of

the new policy distillation and an improvement in robustness
to attacks.

1) Summary and lessons learned: Research into adversarial
attacks and defense in RL is expected to continue to evolve in
the coming years. However, from the research trend presented
in this section, these developments will be hinged on some
research ﬁndings. The ﬁrst is the possibility of intermittent
attacks that occur over a subset of time steps thereby making
it further possible for an adversary to operate stealthily and
efﬁciently. Second, the ability to lure an agent into taking ac-
tions that direct it towards an adversarial reward will continue
to be a threat to the application of DRL models. Also, the
development of more efﬁcient methods of crafting adversarial
examples like the ATN and the development of adversarial
black-box attacks will further extend the frontier of research
in DRL. All of these factors form critical concerns for the
application of DRL in safety-critical systems like drones, self-
driving cars and other CPS. Compared to DNN’s, adversarial
defense for RL is still in its infant stage. However, as the
applications of DRL in CPS and other systems continue to
grow, the issue of defense will also attract a lot of attention.
We agree with the authors in [190] that adversarial training for
RL, aside from challenges of increased computational cost will
suffer when new attacks are encountered and posit that similar
to their research, novel defense methods that do not involve the
generation of examples during training will be more efﬁcient
and practicable for uncertain environments.

Finally, the issue of designing systems without a broad
consideration about security and adversarial attacks right from
the onset has posed as a setback to addressing the security
concerns afterwards. Therefore, it is pertinent that security
concerns must be factored into the design of systems to
enhance their resiliency to adversarial attacks.

VII. OPEN RESEARCH CHALLENGES AND FUTURE
DIRECTIONS

The ﬁelds of cybersecurity, CPS and AI will continue to
evolve. The challenges experienced in applying these tech-
nologies to solve real life problems are of continuous research
interest. A lot of investment is being made by the industry,
government and other consortia to ensure that these problems
are solved. Since research has become inter-disciplinary as
CPS is in itself an inter-disciplinary ﬁeld, there are some
emerging technologies or developments that will contribute to
actualizing AI-driven cybersecurity especially in CPS. Some
of these are at very early stages but ﬁndings from white
papers, blogs, and recent publications show that with deeper
research interest into these topics of interest, they will without
doubt inﬂuence the ﬁeld positively. Furthermore, questions and
issues of concern that should be brought to the fore as we
design these systems are being discussed in this section.

A. Adaptive Adversarial Training for resiliency in ML

Adversarial training was earlier discussed as one of the
major methods of enhancing resiliency of ML models. How-
ever, although the perturbation of all inputs during adversarial

24

training has an advantage of robustness and resiliency, it also
has the disadvantages of cost as a result of computational com-
plexity and its potential of leading to poorer generalizations.
The principle of adaptive adversarial training where the input
to be selected for perturbation are carefully selected in an
adaptive manner has become a solution to this challenge. In
[191], an instance adaptive adversarial training technique that
carries out sample-speciﬁc perturbation margins around every
training sample was proposed. Their results showed that with
a marginal drop in robustness, generalization was achieved
when the model was tested with unperturbed samples.

B. Context awareness for resiliency in ML

Context awareness is the ability of a system or system
component to gather information about its environment at any
given time and adapt behaviors accordingly. Context awareness
is also a technological driver for M2M (machine to machine)
and IoT, ubiquitous computing and event-driven computing
environments. An online context-aware ML algorithm for
5G millimeter-wave vehicular communications was proposed
in [192]. The algorithm sourced for sparse user location
information, aggregated the received data and was thus able
to learn and adapt to the environment. Resiliency of ML in
CPS applications can therefore be satisfactorily achieved if the
systems are context aware.

C. Federated Learning (FL) in Cybersecurity

Recently, as a result of the big data revolution, data sets
have become very large and the models have also become
more complex thus making the training process more difﬁcult.
Federated Learning was proposed as the solution to this
challenge. In edge devices such as mobile phones and IoT
devices, there is a constraint in resources such as memory
and processor and electric power. FL therefore gives them
the capability to learn a shared model for prediction, while
keeping the training data local. The advantages of this princi-
ple of decentralizing training models include privacy, security,
regulatory and economic beneﬁts [193]. Doku et. al in [194]
applied the principle of FL to determine the data relevance in
big data.
In the nearest future, the principle of federated learning is
therefore expected to be used to secure systems using ML.

D. Distinguishing Malicious Attacks from Faulty Systems

CPS systems comprise of a lot of components and sub-
systems that also have the potential to fail. It is possible for
some sensors to become faulty and produces wrong readings
that result
in a wrong output. However, because security
systems that have been trained to identify the right behavior
and the input of the system, and also be on the lookout for
adversarial examples or data poisoning attacks and other signs
of anomalous behavior and react, such system in seeking to
achieve resiliency of the system will classify such a hardware
fault as an attack and take appropriate moves. This would
increase the computational power consumption of the system.
The problem of distinguishing malicious attacks from failures

of sensors and other devices in the CPS must therefore be
solved even as we continue to seek to build systems that are
both resilient using ML and have resilient ML algorithms.

[13] G. Dartmann, H. Song, and A. Schmeink, Big data analytics for cyber-
physical systems: machine learning for the internet of things. Elsevier,
2019.

[14] A. Ng, “why ai is the new electricity,” Nikkei Asian Review Online,

25

VIII. CONCLUSION

In this paper, we have seen that the recent internet and
telecommunication penetration has propelled technologies
such as IoT and CPS. However, the growing interconnection of
devices and things has widened the cyber surface and therefore
led to a lot of cybersecurity concerns. With the increase in the
success of cybersecurity attacks, the effects of such attacks
ﬁnancially and the damage they can have on infrastructure,
new methodologies to complement the traditional methods of
preventing such attacks must be explored. The potentials and
challenges in applying AI and ML in cybersecurity have been
thoroughly examined and the concerns and future directions
have also been identiﬁed. Without a doubt, ML and AI will
play a long role in securing our cyber space from attackers
but there are challenges that need to be examined to ensure
the overall success. The quest to overcome these challenges
will make this interactions between ML and cybersecurity
continuous with the ultimate goal of ensuring that ML models
serve dominantly as a defense strategy and not an attack
strategy.

REFERENCES

[1] R. Katz, “The impact of broadband on the economy: Research to date

and policy issues,” Broadband Series, 2012.

[2] W. A. Jabbar, T. K. Kian, R. M. Ramli, S. N. Zubir, N. S. Zamrizaman,
M. Balfaqih, V. Shepelev, and S. Alharbi, “Design and fabrication of
smart home with internet of things enabled automation system,” IEEE
Access, vol. 7, pp. 144059–144074, 2019.

[3] S. Mahmud, S. Ahmed, and K. Shikder, “A smart home automation and
metering system using internet of things (iot),” in 2019 International
Conference on Robotics, Electrical and Signal Processing Techniques
(ICREST), pp. 451–454, IEEE, 2019.

[4] G. Ramachandran, S. Kannan, T. Sheela, A. Malarvizhi, P. Murali,
and G. Sureshkumar, “Internet of things in healthcare,” Research &
Reviews: Journal of Medical Science and Technology, vol. 8, no. 1,
pp. 10–12, 2019.

[5] C. Yang, W. Shen, and X. Wang, “Applications of internet of things
in manufacturing,” in 2016 IEEE 20th International Conference on
Computer Supported Cooperative Work in Design (CSCWD), pp. 670–
675, IEEE, 2016.

[6] S. Jeschke, C. Brecher, T. Meisen, D.

¨Ozdemir, and T. Eschert,
“Industrial internet of things and cyber manufacturing systems,” in
Industrial Internet of Things, pp. 3–19, Springer, 2017.

[7] N. Abuzainab and W. Saad, “Dynamic connectivity game for adver-
sarial internet of battleﬁeld things systems,” IEEE Internet of Things
Journal, vol. 5, no. 1, pp. 378–390, 2017.

[8] A. Azmoodeh, A. Dehghantanha, and K.-K. R. Choo, “Robust mal-
ware detection for internet of (battleﬁeld) things devices using deep
eigenspace learning,” IEEE Transactions on Sustainable Computing,
vol. 4, no. 1, pp. 88–95, 2018.

[9] R. Rajkumar, I. Lee, L. Sha, and J. Stankovic, “Cyber-physical systems:
The next computing revolution,” in Design Automation Conference,
pp. 731–736, June 2010.

[10] H. Olufowobi, C. Young, J. Zambreno, and G. Bloom, “Saiducant:
Speciﬁcation-based automotive intrusion detection using controller area
network (can) timing,” IEEE Transactions on Vehicular Technology,
vol. 69, no. 2, pp. 1484–1494, 2019.

[11] C. Li and M. Qiu, Reinforcement Learning for Cyber-Physical Systems:
with Cybersecurity Case Studies. Chapman and Hall/CRC, 2019.
[12] C. S. Wickramasinghe, D. L. Marino, K. Amarasinghe, and M. Manic,
“Generalization of deep learning for cyber-physical system security:
the IEEE
A survey,” in IECON 2018-44th Annual Conference of
Industrial Electronics Society, pp. 745–751, IEEE, 2018.

vol. 27, 2016.

[15] H. Lasi, P. Fettke, H.-G. Kemper, T. Feld, and M. Hoffmann, “Industry
4.0,” Business & information systems engineering, vol. 6, no. 4,
pp. 239–242, 2014.

[16] J. Zhou, “Intelligent manufacturing-main direction of ‘made in china
2025,” China Mechanical Engineering, vol. 26, no. 17, pp. 2273–2284,
2015.

[17] L. Xiao, X. Wan, X. Lu, Y. Zhang, and D. Wu, “Iot security techniques
based on machine learning: How do iot devices use ai to enhance
security?,” IEEE Signal Processing Magazine, vol. 35, no. 5, pp. 41–
49, 2018.

[18] A. A. Diro and N. Chilamkurti, “Distributed attack detection scheme
using deep learning approach for internet of things,” Future Generation
Computer Systems, vol. 82, pp. 761–768, 2018.

[19] R. Doshi, N. Apthorpe, and N. Feamster, “Machine learning ddos
detection for consumer internet of things devices,” in 2018 IEEE
Security and Privacy Workshops (SPW), pp. 29–35, IEEE, 2018.
[20] P. M. Shakeel, S. Baskar, V. S. Dhulipala, S. Mishra, and M. M. Jaber,
“Maintaining security and privacy in health care system using learning
based deep-q-networks,” Journal of medical systems, vol. 42, no. 10,
p. 186, 2018.

[21] A. Azmoodeh, A. Dehghantanha, and K. R. Choo, “Robust malware de-
tection for internet of (battleﬁeld) things devices using deep eigenspace
learning,” IEEE Transactions on Sustainable Computing, vol. 4, pp. 88–
95, Jan 2019.

[22] Z. Ning, P. Dong, X. Wang, J. J. Rodrigues, and F. Xia, “Deep
reinforcement learning for vehicular edge computing: An intelligent
ofﬂoading system,” ACM Transactions on Intelligent Systems and
Technology (TIST), vol. 10, no. 6, p. 60, 2019.

[23] V. Behzadan and A. Munir, “Adversarial reinforcement learning frame-
work for benchmarking collision avoidance mechanisms in autonomous
vehicles,” arXiv preprint arXiv:1806.01368, 2018.

[24] L. Yu, X. Shao, and X. Yan, “Autonomous overtaking decision making
of driverless bus based on deep q-learning method,” in 2017 IEEE Inter-
national Conference on Robotics and Biomimetics (ROBIO), pp. 2267–
2272, Dec 2017.

[25] M. P. Ronecker and Y. Zhu, “Deep q-network based decision making
for autonomous driving,” in 2019 3rd International Conference on
Robotics and Automation Sciences (ICRAS), pp. 154–160, June 2019.
[26] C. You, J. Lu, D. Filev, and P. Tsiotras, “Highway trafﬁc modeling and
decision making for autonomous vehicle using reinforcement learning,”
in 2018 IEEE Intelligent Vehicles Symposium (IV), pp. 1227–1232, June
2018.

[27] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar,
“Can machine learning be secure?,” in Proceedings of the 2006 ACM
Symposium on Information, computer and communications security,
pp. 16–25, ACM, 2006.

[28] X. Liao, L. Ding, and Y. Wang, “Secure machine learning, a brief
overview,” in 2011 Fifth International Conference on Secure Software
Integration and Reliability Improvement-Companion, pp. 26–29, IEEE,
2011.

[29] M. Ozdag, “Adversarial attacks and defenses against deep neural
networks: A survey,” Procedia Computer Science, vol. 140, pp. 152–
161, 2018.

[30] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise of
adversarial machine learning,” Pattern Recognition, vol. 84, pp. 317–
331, 2018.

[31] F. Liang, W. G. Hatcher, W. Liao, W. Gao, and W. Yu, “Machine
learning for security and the internet of things: the good, the bad, and
the ugly,” IEEE Access, vol. 7, pp. 158126–158147, 2019.

[32] X. Yuan, P. He, Q. Zhu, and X. Li, “Adversarial examples: Attacks and
defenses for deep learning,” IEEE transactions on neural networks and
learning systems, vol. 30, no. 9, pp. 2805–2824, 2019.

[33] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8,

no. 3-4, pp. 279–292, 1992.

[34] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D. Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement
learning,” arXiv preprint arXiv:1312.5602, 2013.

[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
et al., “Human-level control through deep reinforcement learning,”
Nature, vol. 518, no. 7540, p. 529, 2015.

[36] A. Tamar, Y. Wu, G. Thomas, S. Levine, and P. Abbeel, “Value iteration
networks,” in Advances in Neural Information Processing Systems,
pp. 2154–2162, 2016.

[37] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-
forcement learning,” in International conference on machine learning,
pp. 1928–1937, 2016.

[38] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust
region policy optimization,” in International conference on machine
learning, pp. 1889–1897, 2015.

[39] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforce-
ment learning,” arXiv preprint arXiv:1509.02971, 2015.

[40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
2017.

[41] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Sil-
ver, and K. Kavukcuoglu, “Reinforcement learning with unsupervised
auxiliary tasks,” arXiv preprint arXiv:1611.05397, 2016.

[42] S. M. Dibaji, M. Pirani, D. B. Flamholz, A. M. Annaswamy, K. H.
Johansson, and A. Chakrabortty, “A systems and control perspective of
cps security,” Annual Reviews in Control, 2019.

[43] F. O. Olowononi, D. B. Rawat, and C. Liu, “Dependable adaptive
mobility in vehicular networks for resilient mobile cyber physical
systems,” in 2020 IEEE International Conference on Communications
Workshops (ICC Workshops), pp. 1–6, IEEE, 2020.

[44] A. Humayed, J. Lin, F. Li, and B. Luo, “Cyber-physical systems
security—a survey,” IEEE Internet of Things Journal, vol. 4, no. 6,
pp. 1802–1831, 2017.

[45] J. Giraldo, E. Sarkar, A. A. Cardenas, M. Maniatakos, and M. Kantar-
cioglu, “Security and privacy in cyber-physical systems: A survey of
surveys,” IEEE Design & Test, vol. 34, no. 4, pp. 7–17, 2017.
[46] Y. Jie, M. Li, C. Guo, and L. Chen, “Dynamic defense strategy against
dos attacks over vehicular ad hoc networks based on port hopping,”
IEEE Access, vol. 6, pp. 51374–51383, 2018.

[47] F. Ahmad, A. Adnane, V. Franqueira, F. Kurugollu, and L. Liu, “Man-
in-the-middle attacks in vehicular ad-hoc networks: Evaluating the
impact of attackers’ strategies,” Sensors, vol. 18, no. 11, p. 4040, 2018.
[48] S. Dadras, S. Dadras, and C. Winstead, “Resilient control design for
vehicular platooning in an adversarial environment,” in 2019 American
Control Conference (ACC), pp. 533–538, IEEE, 2019.

[49] D. B. Rawat and C. Bajracharya, “Securing vanets for vehicular cps,”
in Vehicular Cyber Physical Systems, pp. 41–60, Springer, 2017.
[50] Z. Yang, K. Yang, L. Lei, K. Zheng, and V. C. M. Leung, “Blockchain-
based decentralized trust management in vehicular networks,” IEEE
Internet of Things Journal, vol. 6, pp. 1495–1505, April 2019.
[51] S. Guo, X. Hu, Z. Zhou, X. Wang, F. Qi, and L. Gao, “Trust
access authentication in vehicular network based on blockchain,” China
Communications, vol. 16, pp. 18–30, June 2019.

[52] S. Rahmadika, K. Lee, and K. Rhee, “Blockchain-enabled 5g au-
tonomous vehicular networks,” in 2019 International Conference on
Sustainable Engineering and Creative Computing (ICSECC), pp. 275–
280, Aug 2019.

[53] F. O. Olowononi, D. B. Rawat, M. Garuba, and C. Kamhoua, “Security
engineering with machine learning for adversarial resiliency in cyber
physical systems,” in Artiﬁcial Intelligence and Machine Learning
for Multi-Domain Operations Applications, vol. 11006, p. 110061O,
International Society for Optics and Photonics, 2019.

[54] M. S. Haghighi and F. Farivar, “A machine learning-based approach to
build zero false-positive ipss for industrial iot and cps with a case study
on power grids security,” arXiv preprint arXiv:2004.06432, 2020.
[55] W. Sun, J. Liu, and Y. Yue, “Ai-enhanced ofﬂoading in edge computing:
When machine learning meets industrial iot,” IEEE Network, vol. 33,
no. 5, pp. 68–74, 2019.

[56] H. Liao, Z. Zhou, X. Zhao, L. Zhang, S. Mumtaz, A. Jolfaei, S. H.
Ahmed, and A. K. Bashir, “Learning-based context-aware resource
allocation for edge computing-empowered industrial iot,” IEEE Internet
of Things Journal, 2019.

[57] I. Lee and O. Sokolsky, “Medical cyber physical systems,” in Design

automation conference, pp. 743–748, IEEE, 2010.

[58] D. Halperin, T. S. Heydt-Benjamin, B. Ransford, S. S. Clark, B. De-
fend, W. Morgan, K. Fu, T. Kohno, and W. H. Maisel, “Pacemakers
and implantable cardiac deﬁbrillators: Software radio attacks and zero-
power defenses,” in 2008 IEEE Symposium on Security and Privacy
(sp 2008), pp. 129–142, IEEE, 2008.

26

[59] S. Hanna, R. Rolles, A. Molina-Markham, P. Poosankam, J. Blocki,
K. Fu, and D. Song, “Take two software updates and see me in
the morning: The case for software security evaluations of medical
devices.,” in HealthSec, 2011.

[60] D. T. Ton and W. P. Wang, “A more resilient grid: The us department
of energy joins with stakeholders in an r&d plan,” IEEE Power and
Energy Magazine, vol. 13, no. 3, pp. 26–34, 2015.

[61] D. Zhang, F. R. Yu, R. Yang, and H. Tang, “A deep reinforcement
learning-based trust management scheme for software-deﬁned vehicu-
lar networks,” in Proceedings of the 8th ACM Symposium on Design
and Analysis of Intelligent Vehicular Networks and Applications, pp. 1–
7, ACM, 2018.

[62] J. Chou, S. Hsu, N. Ngo, C. Lin, and C. Tsui, “Hybrid machine
learning system to forecast electricity consumption of smart grid-based
air conditioners,” IEEE Systems Journal, vol. 13, no. 3, pp. 3120–3128,
2019.

[63] F. Pallonetto, M. De Rosa, F. Milano, and D. P. Finn, “Demand
response algorithms for smart-grid ready residential buildings using
machine learning models,” Applied energy, vol. 239, pp. 1265–1282,
2019.

[64] F. Lucas, P. Costa, R. Batalha, D. Leite, and I. ˇSkrjanc, “Fault detection
in smart grids with time-varying distributed generation using wavelet
energy and evolving neural networks,” Evolving Systems, pp. 1–16,
2020.

[65] S. Bodda and P. Agnihotri, “Deep learning based ac line fault classiﬁer
and locator for power system,” in 2019 Innovations in Power and
Advanced Computing Technologies (i-PACT), vol. 1, pp. 1–5, IEEE,
2019.

[66] D. Moldovan and I. Salomie, “Detection of sources of instability in
smart grids using machine learning techniques,” in 2019 IEEE 15th
International Conference on Intelligent Computer Communication and
Processing (ICCP), pp. 175–182, IEEE, 2019.

[67] E. Hossain, I. Khan, F. Un-Noor, S. S. Sikander, and M. S. H.
Sunny, “Application of big data and machine learning in smart grid,
and associated security concerns: A review,” IEEE Access, vol. 7,
pp. 13960–13988, 2019.

[68] M. Esmalifalak, L. Liu, N. Nguyen, R. Zheng, and Z. Han, “Detecting
stealthy false data injection using machine learning in smart grid,” IEEE
Systems Journal, vol. 11, no. 3, pp. 1644–1652, 2014.

[69] C.-J. Hoel, K. Wolff, and L. Laine, “Automated speed and lane change
decision making using deep reinforcement learning,” in 2018 21st
International Conference on Intelligent Transportation Systems (ITSC),
pp. 2148–2155, IEEE, 2018.

[70] T. Tram, A. Jansson, R. Gr¨onberg, M. Ali, and J. Sj¨oberg, “Learning
negotiating behavior between cars in intersections using deep q-
learning,” CoRR, vol. abs/1810.10469, 2018.

[71] K. Makantasis, M. Kontorinaki, and I. Nikolos, “A deep reinforcement
learning driving policy for autonomous road vehicles,” arXiv preprint
arXiv:1905.09046, 2019.

[72] R. Xing, Z. Su, N. Zhang, Y. Peng, H. Pu, and J. Luo, “Trust-
learning in

evaluation-based intrusion detection and reinforcement
autonomous driving,” IEEE Network, vol. 33, pp. 54–60, Sep. 2019.

[73] A. Ferdowsi, U. Challita, W. Saad, and N. B. Mandayam, “Robust
deep reinforcement learning for security and safety in autonomous
vehicle systems,” in 2018 21st International Conference on Intelligent
Transportation Systems (ITSC), pp. 307–312, IEEE, 2018.

[74] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura,
“Navigating occluded intersections with autonomous vehicles using
deep reinforcement learning,” in 2018 IEEE International Conference
on Robotics and Automation (ICRA), pp. 2034–2039, May 2018.
[75] A. Chandramohan, M. Poel, B. Meijerink, and G. Heijenk, “Machine
learning for cooperative driving in a multi-lane highway environment,”
in 2019 Wireless Days (WD), pp. 1–4, April 2019.

[76] R. F. Atallah, C. M. Assi, and M. J. Khabbaz, “Scheduling the operation
of a connected vehicular network using deep reinforcement learning,”
IEEE Transactions on Intelligent Transportation Systems, vol. 20,
pp. 1669–1682, May 2019.

[77] K. Kato, F. Ishikawa, and S. Honiden, “Falsiﬁcation of cyber-physical
systems with reinforcement learning,” in 2018 IEEE Workshop on
Monitoring and Testing of Cyber-Physical Systems (MT-CPS), pp. 5–6,
IEEE, 2018.

[78] H. Kumar, “Explainable ai: Deep reinforcement learning agents for
residential demand side cost savings in smart grids,” arXiv preprint
arXiv:1910.08719, 2019.

[79] Y. Yang, J. Hao, Y. Zheng, X. Hao, and B. Fu, “Large-scale home en-
ergy management using entropy-based collective multiagent reinforce-
ment learning framework,” in Proceedings of the 18th International

27

Conference on Autonomous Agents and MultiAgent Systems, pp. 2285–
2287, International Foundation for Autonomous Agents and Multiagent
Systems, 2019.

[80] X. Lu, X. Xiao, L. Xiao, C. Dai, M. Peng, and H. V. Poor, “Re-
inforcement learning-based microgrid energy trading with a reduced
power plant schedule,” IEEE Internet of Things Journal, vol. 6, no. 6,
pp. 10728–10737, 2019.

[81] Y. Ren, D. Fan, Q. Feng, Z. Wang, B. Sun, and D. Yang, “Agent-based
restoration approach for reliability with load balancing on smart grids,”
Applied energy, vol. 249, pp. 46–57, 2019.
[82] X. Liu and C. Konstantinou, “Reinforcement

learning for cyber-
physical security assessment of power systems,” in 2019 IEEE Milan
PowerTech, pp. 1–6, IEEE, 2019.

[83] D. I. Urbina, D. I. Urbina, J. Giraldo, A. A. Cardenas, J. Valente,
M. Faisal, N. O. Tippenhauer, J. Ruths, R. Candell, and H. Sandberg,
Survey and new directions for physics-based attack detection in control
systems. US Department of Commerce, National Institute of Standards
and Technology . . . , 2016.

[84] W. Yan, L. K. Mestha, and M. Abbaszadeh, “Attack detection for
securing cyber physical systems,” IEEE Internet of Things Journal,
vol. 6, no. 5, pp. 8471–8481, 2019.

[85] F. Miao, Q. Zhu, M. Pajic, and G. J. Pappas, “Coding schemes for
securing cyber-physical systems against stealthy data injection attacks,”
IEEE Transactions on Control of Network Systems, vol. 4, no. 1,
pp. 106–117, 2016.

[86] K. N. Junejo and J. Goh, “Behaviour-based attack detection and
classiﬁcation in cyber physical systems using machine learning,” in
Proceedings of
the 2nd ACM International Workshop on Cyber-
Physical System Security, pp. 34–43, ACM, 2016.

[87] M. Macas and W. Chunming, “Enhanced cyber-physical security

through deep learning techniques,”

[88] J. Wang, W. Tu, L. C. Hui, S.-M. Yiu, and E. K. Wang, “Detecting
time synchronization attacks in cyber-physical systems with machine
learning techniques,” in 2017 IEEE 37th International Conference on
Distributed Computing Systems (ICDCS), pp. 2246–2251, IEEE, 2017.
[89] J. Shin, Y. Baek, Y. Eun, and S. H. Son, “Intelligent sensor attack
detection and identiﬁcation for automotive cyber-physical systems,” in
2017 IEEE Symposium Series on Computational Intelligence (SSCI),
pp. 1–8, IEEE, 2017.

[90] A. Ghafouri, Y. Vorobeychik, and X. Koutsoukos, “Adversarial regres-
sion for detecting attacks in cyber-physical systems,” arXiv preprint
arXiv:1804.11022, 2018.

[91] J.-C. Laprie, “From dependability to resilience,” in 38th IEEE/IFIP Int.
Conf. On dependable systems and networks, pp. G8–G9, 2008.
[92] M. Barbeau, G. Carle, J. Garcia-Alfaro, and V. Torra, “Next generation
resilient cyber-physical systems,” arXiv preprint arXiv:1907.08849,
2019.

[93] P. Kannappan, K. Karydis, H. G. Tanner, A. Jardine, and J. Heinz,
“Incorporating learning modules improves aspects of resilience of
supervisory cyber-physical systems,” in 2016 24th Mediterranean Con-
ference on Control and Automation (MED), pp. 996–1001, IEEE, 2016.
[94] M. Feng and H. Xu, “Deep reinforecement learning based optimal
defense for cyber-physical system in presence of unknown cyber-
attack,” in 2017 IEEE Symposium Series on Computational Intelligence
(SSCI), pp. 1–8, IEEE, 2017.

[95] M. Lokesh and Y. Kumaraswamy, “State awareness towards resiliency
in cyber-physical system: a modiﬁed danger theory based deterministic
dendritic cell algorithm approach,” in 2015 IEEE International Confer-
ence on Computer Graphics, Vision and Information Security (CGVIS),
pp. 201–208, IEEE, 2015.

[96] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press,

2016.

[97] C. Yinka-Banjo and O.-A. Ugot, “A review of generative adversarial
networks and its application in cybersecurity,” Artiﬁcial Intelligence
Review, pp. 1–16, 2019.

[98] I. Goodfellow, “Nips 2016 tutorial: Generative adversarial networks,”

arXiv preprint arXiv:1701.00160, 2016.

[99] M. Barbeau and J. Garcia-Alfaro, “Faking and discriminating the
navigation data of a micro aerial vehicle using quantum generative
adversarial networks,” arXiv preprint arXiv:1907.03038, 2019.
[100] S. Z. Yong, M. Q. Foo, and E. Frazzoli, “Robust and resilient
estimation for cyber-physical systems under adversarial attacks,” in
2016 American Control Conference (ACC), pp. 308–315, July 2016.

[102] K. Vatanparvar and M. A. Al Faruque, “Self-secured control with
anomaly detection and recovery in automotive cyber-physical systems,”
in 2019 Design, Automation Test in Europe Conference Exhibition
(DATE), pp. 788–793, March 2019.

[103] D. Li, D. Chen, J. Goh, and S.-k. Ng, “Anomaly detection with
generative adversarial networks for multivariate time series,” arXiv
preprint arXiv:1809.04758, 2018.

[104] D. Li, D. Chen, B. Jin, L. Shi, J. Goh, and S.-K. Ng, “Mad-gan:
Multivariate anomaly detection for time series data with generative
adversarial networks,” in International Conference on Artiﬁcial Neural
Networks, pp. 703–716, Springer, 2019.

[105] V. Belenko, V. Chernenko, M. Kalinin, and V. Krundyshev, “Evaluation
of gan applicability for intrusion detection in self-organizing networks
of cyber physical systems,” in 2018 International Russian Automation
Conference (RusAutoCon), pp. 1–7, IEEE, 2018.

[106] S. R. Chhetri, A. B. Lopez, J. Wan, and M. A. Al Faruque, “Gan-
sec: Generative adversarial network modeling for the security analysis
of cyber-physical production systems,” in 2019 Design, Automation &
Test in Europe Conference & Exhibition (DATE), pp. 770–775, IEEE,
2019.

[107] I. Rosenberg, A. Shabtai, Y. Elovici, and L. Rokach, “Adversarial learn-
ing in the cyber security domain,” arXiv preprint arXiv:2007.02407,
2020.

[108] F. Cai, J. Li, and X. Koutsoukos, “Detecting adversarial examples in
learning-enabled cyber-physical systems using variational autoencoder
for regression,” arXiv preprint arXiv:2003.10804, 2020.

[109] F. Cai and X. Koutsoukos, “Real-time out-of-distribution detection in
learning-enabled cyber-physical systems,” in 2020 ACM/IEEE 11th In-
ternational Conference on Cyber-Physical Systems (ICCPS), pp. 174–
183, IEEE, 2020.

[110] D. Boursinos and X. Koutsoukos, “Trusted conﬁdence bounds for learn-
ing enabled cyber-physical systems,” arXiv preprint arXiv:2003.05107,
2020.

[111] D. Boursinos and X. Koutsoukos, “Assurance monitoring of cyber-
physical systems with machine learning components,” arXiv preprint
arXiv:2001.05014, 2020.

[112] J. Li, Y. Yang, and J. S. Sun, “Searchfromfree: Adversarial mea-
surements for machine learning-based energy theft detection,” arXiv
preprint arXiv:2006.03504, 2020.

[113] G. Clark, M. Doran, and W. Glisson, “A malicious attack on the
machine learning policy of a robotic system,” in 2018 17th IEEE
International Conference On Trust, Security And Privacy In Computing
And Communications/12th IEEE International Conference On Big Data
Science And Engineering (TrustCom/BigDataSE), pp. 516–521, IEEE,
2018.

[114] Z. Xiong, J. Eappen, H. Zhu, and S. Jagannathan, “Robustness to
adversarial attacks in learning-enabled controllers,” arXiv preprint
arXiv:2006.06861, 2020.

[115] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfel-
low, and R. Fergus, “Intriguing properties of neural networks,” arXiv
preprint arXiv:1312.6199, 2013.

[116] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,” in
2016 IEEE European Symposium on Security and Privacy (EuroS&P),
pp. 372–387, IEEE, 2016.

[117] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.

[118] A. Chakraborty, M. Alam, V. Dey, A. Chattopadhyay,

and
D. Mukhopadhyay, “Adversarial attacks and defences: A survey,” arXiv
preprint arXiv:1810.00069, 2018.

[119] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against machine learning,” in
Proceedings of the 2017 ACM on Asia conference on computer and
communications security, pp. 506–519, ACM, 2017.

[120] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transfer-
able adversarial examples and black-box attacks,” arXiv preprint
arXiv:1611.02770, 2016.

[121] A. N. Bhagoji, W. He, B. Li, and D. Song, “Exploring the space
of black-box attacks on deep neural networks,” arXiv preprint
arXiv:1712.09491, 2017.

[122] M. Cheng, T. Le, P.-Y. Chen, J. Yi, H. Zhang, and C.-J. Hsieh, “Query-
efﬁcient hard-label black-box attack: An optimization-based approach,”
arXiv preprint arXiv:1807.04457, 2018.

[101] Y. Li, J. Wu, and S. Li, “Controllability and observability of cpss under
networked adversarial attacks,” IET Control Theory & Applications,
vol. 11, no. 10, pp. 1596–1602, 2017.

[123] P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh, “Zoo: Zeroth
order optimization based black-box attacks to deep neural networks
without training substitute models,” in Proceedings of the 10th ACM

Workshop on Artiﬁcial Intelligence and Security, pp. 15–26, ACM,
2017.

[124] C.-C. Tu, P. Ting, P.-Y. Chen, S. Liu, H. Zhang, J. Yi, C.-J. Hsieh, and
S.-M. Cheng, “Autozoom: Autoencoder-based zeroth order optimiza-
tion method for attacking black-box neural networks,” in Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, vol. 33, pp. 742–749,
2019.

[125] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li, “Boosting
the IEEE
adversarial attacks with momentum,” in Proceedings of
conference on computer vision and pattern recognition, pp. 9185–9193,
2018.

[126] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in

the physical world,” arXiv preprint arXiv:1607.02533, 2016.

[127] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in 2017 IEEE Symposium on Security and Privacy (SP),
pp. 39–57, IEEE, 2017.

[128] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a sim-
ple and accurate method to fool deep neural networks,” in Proceedings
of the IEEE conference on computer vision and pattern recognition,
pp. 2574–2582, 2016.

[129] S. Baluja and I. Fischer, “Learning to attack: Adversarial transformation

networks.,” in AAAI, pp. 2687–2695, 2018.

[130] N. Carlini and D. Wagner, “Defensive distillation is not robust to
adversarial examples,” arXiv preprint arXiv:1607.04311, 2016.
[131] I. Ilahi, M. Usama, J. Qadir, M. U. Janjua, A. Al-Fuqaha, D. T. Hoang,
and D. Niyato, “Challenges and countermeasures for adversarial attacks
on deep reinforcement learning,” arXiv preprint arXiv:2001.09684,
2020.

[132] V. Behzadan and A. Munir, “Vulnerability of deep reinforcement
learning to policy induction attacks,” in International Conference on
Machine Learning and Data Mining in Pattern Recognition, pp. 262–
275, Springer, 2017.

[133] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, and P. Abbeel,
“Adversarial attacks on neural network policies,” arXiv preprint
arXiv:1702.02284, 2017.

[134] J. Kos and D. Song, “Delving into adversarial attacks on deep policies,”

arXiv preprint arXiv:1705.06452, 2017.

[135] Y.-C. Lin, Z.-W. Hong, Y.-H. Liao, M.-L. Shih, M.-Y. Liu, and M. Sun,
“Tactics of adversarial attack on deep reinforcement learning agents,”
arXiv preprint arXiv:1703.06748, 2017.

[136] J. Sun, T. Zhang, X. Xie, L. Ma, Y. Zheng, K. Chen, and Y. Liu,
“Stealthy and efﬁcient adversarial attacks against deep reinforcement
learning,”

[137] L. Hussenot, M. Geist, and O. Pietquin, “Targeted attacks on deep
reinforcement learning agents through adversarial observations,” arXiv
preprint arXiv:1905.12282, 2019.

[138] E. Tretschk, S. J. Oh, and M. Fritz, “Sequential attacks on agents for
long-term adversarial goals,” arXiv preprint arXiv:1805.12487, 2018.
[139] X. Yeow Lee, S. Ghadai, K. L. Tan, C. Hegde, and S. Sarkar, “Spa-
tiotemporally constrained action space attacks on deep reinforcement
learning agents,” arXiv preprint arXiv:1909.02583, 2019.

[140] Y. Zhao, I. Shumailov, H. Cui, X. Gao, R. Mullins, and R. Anderson,
“Blackbox attacks on reinforcement learning agents using approxi-
mated temporal information,” in 2020 50th Annual IEEE/IFIP Inter-
national Conference on Dependable Systems and Networks Workshops
(DSN-W), pp. 16–24, IEEE, 2020.

[141] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, and S. Russell,
“Adversarial policies: Attacking deep reinforcement learning,” arXiv
preprint arXiv:1905.10615, 2019.

[142] M. Inkawhich, Y. Chen, and H. Li, “Snooping attacks on deep
reinforcement learning,” arXiv preprint arXiv:1905.11832, 2019.
[143] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and G. Chowdhary,
“Robust deep reinforcement learning with adversarial attacks,” arXiv
preprint arXiv:1712.03632, 2017.

[144] A. Chhabra, A. Roy, and P. Mohapatra, “Strong black-box adversarial
attacks on unsupervised machine learning models,” arXiv preprint
arXiv:1901.09493, 2019.

[145] A. Chhabra, A. Roy, and P. Mohapatra, “Suspicion-free adversarial

attacks on clustering algorithms.,” in AAAI, pp. 3625–3632, 2020.

[146] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural networks,”
in 2016 IEEE Symposium on Security and Privacy (SP), pp. 582–597,
IEEE, 2016.

[147] C. Guo, M. Rana, M. Cisse, and L. Van Der Maaten, “Counter-
ing adversarial images using input transformations,” arXiv preprint
arXiv:1711.00117, 2017.

28

[148] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille, “Mitigating adversar-
ial effects through randomization,” arXiv preprint arXiv:1711.01991,
2017.

[149] A. S. Ross and F. Doshi-Velez, “Improving the adversarial robustness
and interpretability of deep neural networks by regularizing their input
gradients,” in Thirty-second AAAI conference on artiﬁcial intelligence,
2018.

[150] S. Srisakaokul, Y. Zhang, Z. Zhong, W. Yang, T. Xie, and B. Li,
“Muldef: Multi-model-based defense against adversarial examples for
neural networks,” arXiv preprint arXiv:1809.00065, 2018.

[151] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff, “On detecting

adversarial perturbations,” arXiv preprint arXiv:1702.04267, 2017.

[152] F. Liao, M. Liang, Y. Dong, T. Pang, X. Hu, and J. Zhu, “Defense
against adversarial attacks using high-level representation guided de-
noiser,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1778–1787, 2018.

[153] S. Zheng, Y. Song, T. Leung, and I. Goodfellow, “Improving the ro-
bustness of deep neural networks via stability training,” in Proceedings
of the ieee conference on computer vision and pattern recognition,
pp. 4480–4488, 2016.

[154] Z. Yan, Y. Guo, and C. Zhang, “Deep defense: Training dnns with
improved adversarial robustness,” in Advances in Neural Information
Processing Systems, pp. 419–428, 2018.

[155] G. S. Dhillon, K. Azizzadenesheli, Z. C. Lipton, J. Bernstein, J. Kos-
saiﬁ, A. Khanna, and A. Anandkumar, “Stochastic activation pruning
for robust adversarial defense,” arXiv preprint arXiv:1803.01442, 2018.
[156] F. Tram`er, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and
P. McDaniel, “Ensemble adversarial training: Attacks and defenses,”
arXiv preprint arXiv:1705.07204, 2017.

[157] T. Na, J. H. Ko, and S. Mukhopadhyay, “Cascade adversarial ma-
chine learning regularized with a uniﬁed embedding,” arXiv preprint
arXiv:1708.02582, 2017.

[158] J. Ba and R. Caruana, “Do deep nets really need to be deep?,” in
Advances in neural information processing systems, pp. 2654–2662,
2014.

[159] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a

neural network,” arXiv preprint arXiv:1503.02531, 2015.

[160] N. Papernot and P. McDaniel, “On the effectiveness of defensive

distillation,” arXiv preprint arXiv:1607.05113, 2016.

[161] N. Papernot and P. McDaniel, “Extending defensive distillation,” arXiv

preprint arXiv:1705.05264, 2017.

[162] N. Papernot, P. McDaniel, A. Sinha, and M. Wellman, “Towards the
science of security and privacy in machine learning,” arXiv preprint
arXiv:1611.03814, 2016.

[163] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated gradients give a
false sense of security: Circumventing defenses to adversarial exam-
ples,” arXiv preprint arXiv:1802.00420, 2018.

[164] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Synthesizing
robust adversarial examples,” in International conference on machine
learning, pp. 284–293, PMLR, 2018.

[165] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial
examples in deep neural networks,” arXiv preprint arXiv:1704.01155,
2017.

[166] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner, “Detecting
adversarial samples from artifacts,” arXiv preprint arXiv:1703.00410,
2017.

[167] J. Lu, T. Issaranon, and D. Forsyth, “Safetynet: Detecting and rejecting
adversarial examples robustly,” in Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pp. 446–454, 2017.

[168] K. Roth, Y. Kilcher, and T. Hofmann, “The odds are odd: A
test for detecting adversarial examples,” arXiv preprint

statistical
arXiv:1902.04818, 2019.

[169] F. Guo, Q. Zhao, X. Li, X. Kuang, J. Zhang, Y. Han, and Y.-a. Tan,
“Detecting adversarial examples via prediction difference for deep
neural networks,” Information Sciences, vol. 501, pp. 182–192, 2019.
[170] S. Dathathri, S. Zheng, R. M. Murray, and Y. Yue, “Detect-
ing adversarial examples via neural ﬁngerprinting,” arXiv preprint
arXiv:1803.03870, 2018.

[171] L. Sun, Detecting Adversarial Examples by Measuring Their Stress

Response. PhD thesis, Arizona State University, 2019.

[172] Z. Liu, Q. Liu, T. Liu, N. Xu, X. Lin, Y. Wang, and W. Wen,
“Feature distillation: Dnn-oriented jpeg compression against adversarial
examples,” in 2019 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 860–868, IEEE, 2019.

[173] Z. Yi, J. Yu, S. Li, Y. Tan, and Q. Wu, “Incremental learning of gan
for detecting multiple adversarial attacks,” in International Conference
on Artiﬁcial Neural Networks, pp. 673–684, Springer, 2019.

29

[174] X. Li and F. Li, “Adversarial examples detection in deep networks with
convolutional ﬁlter statistics,” in Proceedings of the IEEE International
Conference on Computer Vision, pp. 5764–5772, 2017.

[175] J. Martin and C. Elster, “Inspecting adversarial examples using the

ﬁsher information,” Neurocomputing, 2019.

[176] N. Carlini and D. Wagner, “Adversarial examples are not easily
detected: Bypassing ten detection methods,” in Proceedings of the 10th
ACM Workshop on Artiﬁcial Intelligence and Security, pp. 3–14, 2017.
[177] X. Qu, Z. Sun, Y. S. Ong, A. Gupta, and P. Wei, “Minimalistic attacks:
How little it takes to fool deep reinforcement learning policies,” IEEE
Transactions on Cognitive and Developmental Systems, 2020.
[178] V. Behzadan and A. Munir, “Whatever does not kill deep reinforcement
learning, makes it stronger,” arXiv preprint arXiv:1712.09344, 2017.
[179] A. Havens, Z. Jiang, and S. Sarkar, “Online robust policy learning in the
presence of unknown adversaries,” in Advances in Neural Information
Processing Systems, pp. 9916–9926, 2018.

[180] A. Mandlekar, Y. Zhu, A. Garg, L. Fei-Fei, and S. Savarese, “Ad-
versarially robust policy learning: Active construction of physically-
plausible perturbations,” in 2017 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), pp. 3932–3939, IEEE, 2017.
[181] Y.-C. Lin, M.-Y. Liu, M. Sun, and J.-B. Huang, “Detecting adversarial
attacks on neural network policies with visual foresight,” arXiv preprint
arXiv:1710.00814, 2017.

[182] T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning,
I. S. Dhillon, and L. Daniel, “Towards fast computation of certiﬁed
robustness for relu networks,” arXiv preprint arXiv:1804.09699, 2018.
[183] T. Oikarinen, T.-W. Weng, and L. Daniel, “Robust deep reinforcement
learning through adversarial loss,” arXiv preprint arXiv:2008.01976,
2020.

[184] M. Everett, B. Lutjens, and J. P. How, “Certiﬁed adversarial robustness

for deep reinforcement learning,” arXiv preprint arXiv:2004.06496,
2020.

[185] Y.-S. Wang, T.-W. Weng, and L. Daniel, “Veriﬁcation of neural network
control policy under persistent adversarial perturbation,” arXiv preprint
arXiv:1908.06353, 2019.

[186] A. Raghunathan, J. Steinhardt, and P. Liang, “Certiﬁed defenses against
adversarial examples,” arXiv preprint arXiv:1801.09344, 2018.
[187] E. Wong and Z. Kolter, “Provable defenses against adversarial examples
via the convex outer adversarial polytope,” in International Conference
on Machine Learning, pp. 5286–5295, PMLR, 2018.

[188] M. Fischer, M. Mirman, and M. Vechev, “Online robustness training for
deep reinforcement learning,” arXiv preprint arXiv:1911.00887, 2019.
[189] H. Zhang, H. Chen, C. Xiao, B. Li, D. Boning, and C.-J. Hsieh,
“Robust deep reinforcement learning against adversarial perturbations
on observations,” arXiv preprint arXiv:2003.08938, 2020.

[190] X. Qu, Y.-S. Ong, A. Gupta, and Z. Sun, “Defending adversarial
attacks without adversarial attacks in deep reinforcement learning,”
arXiv preprint arXiv:2008.06199, 2020.

[191] Y. Balaji, T. Goldstein, and J. Hoffman, “Instance adaptive adversarial
training: Improved accuracy tradeoffs in neural nets,” arXiv preprint
arXiv:1910.08051, 2019.

[192] G. H. Sim, S. Klos, A. Asadi, A. Klein, and M. Hollick, “An online
context-aware machine learning algorithm for 5g mmwave vehicular
communications,” IEEE/ACM Transactions on Networking, vol. 26,
pp. 2487–2500, Dec 2018.

[193] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated
learning with non-iid data,” CoRR, vol. abs/1806.00582, 2018.
[194] R. Doku, D. B. Rawat, and C. Liu, “Towards federated learning
approach to determine data relevance in big data,” in 2019 IEEE 20th
International Conference on Information Reuse and Integration for
Data Science (IRI), pp. 184–192, IEEE, 2019.

