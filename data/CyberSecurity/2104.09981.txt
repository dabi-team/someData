Prospective Artiﬁcial Intelligence Approaches
for Active Cyber Defence

Neil Dhira,∗, Henrique Hoeltgebaumb,∗, Niall Adamsb, Mark Briersa,b, Anthony Burkea and Paul Jonesa

1

1
2
0
2

r
p
A
0
2

]

R
C
.
s
c
[

1
v
1
8
9
9
0
.
4
0
1
2
:
v
i
X
r
a

Abstract—Cyber criminals are rapidly developing new mali-
cious tools that leverage artiﬁcial intelligence (AI) to enable new
classes of adaptive and stealthy attacks. New defensive methods
need to be developed to counter these threats. Some cyber security
professionals are speculating AI will enable corresponding new
classes of active cyber defence measures – is this realistic, or
currently mostly hype? The Alan Turing Institute, with expert
guidance from the UK National Cyber Security Centre and
Defence Science Technology Laboratory, published a research
roadmap for AI for ACD last year. This position paper updates
the roadmap for two of the most promising AI approaches –
reinforcement learning and causal inference - and describes why
they could help tip the balance back towards defenders.

Index Terms—Artiﬁcial Intelligence, Active Cyber Defence, Cy-
bersecurity, Reinforcement Learning, Causal Inference, Causal
Reinforcement Learning.

I. INTRODUCTION

C YBER criminality leads to global losses of around $600

billion per annum [1]. In principle an organisation’s
security framework is set, implemented and periodically up-
dated by domain experts. Increasing sophistication of cyber-
attacks is demanding that these update cycles shorten which
creates a need for adaptive and autonomous defences [2]. The
potential of artiﬁcial intelligence (AI) to enhance hostile cyber-
capability is a further deep concern. An AI controlled attack
could be fast, ﬂexible and adaptive and simply overwhelm
the current generation of defences. This fact generates an
asymmetry between technologies, where defences are usually
reactive and lag behind the attacker’s technology [3]. AI-
based technology is fast becoming a target of cyber-attack
in its own right, with many attacks attempting to impact the
integrity of the underlying statistical model, in order to make
the algorithm misbehave [4]. Such attacks are only going to
increase in sophistication. There is a need for a new breed of
robust AI approaches to cyber defence and an increase in the
sophistication of active defensive measures. This branch in the
cybersecurity literature is called active cyber defence (ACD)
[3], [5]–[8]. The main focus of ACD is to automate defence
capability in such a way that it is able to deal with automated
threats [5]. In short, we seek to enhance this automation with
AI approaches.

The authors would like to acknowledge funding from the UK Government,
through the Defence & Security partnership at The Alan Turing Institute.
Also the authors would like to acknowledge inputs from Chris W from UK’s
Defence Science and Technology Laboratory respectively. Also we are grateful
to four anonymous reviewers for their insightful comments. Corresponding
authors: ndhir@turing.ac.uk and hh3015@imperial.ac.uk.

a The Alan Turing Institute, UK

b Imperial College London, Departament of Mathematics, UK
∗Equal contribution.

In this paper we update a recent research roadmap [9]
that sets out recommendations for leveraging AI to develop
enhanced ACD capabilities. The roadmap was developed by
the Alan Turing Institute, in collaboration with UK’s National
Cyber Security Centre (NCSC) and Defence Science and Tech-
nology Laboratory (DSTL). In §II we describe the roadmap in
more detail and in §III we describe in more detail a particular
research path which involves reinforcement learning (RL) for
ACD. We then build on this with causal inference research in
§IV, concluding in §V.

II. AI FOR ACD

The main focus of this research initiative is to explore and
develop AI technologies which aim to reduce the asymmetry
gap between technologies in which defences just play a reac-
tive role, to provide AI-based adaptive technologies to power
autonomous cyber defence systems [9]. Challenges, which are
detailed in [2], must be overcome in the following three areas:
• AI-enabled network defence – automatic monitoring, pro-

tection and healing of systems;

• AI-enabled security planning – AI-enabled planning
systems which automatically enhance human decision-
making and action taking;

• AI-enabled penetration testing – AI agents which auto-
matically identify system weaknesses and vulnerabilities
before real-world adversaries.

The ACD concept in academic literature presents ambitious
requirements for intelligent automation capabilities. In order
to create these technologies, researchers and technologists
must overcome difﬁcult challenges in AI, network security and
human-machine teaming. Sustained research in fundamental
topics applied to realistic cyber scenarios is needed to
AI
address the ambitious aims of the ACD concept [9].

In the following two sections, we discuss our positional view
regarding AI applicability in ACD with two speciﬁc projects
that were prioritised from those presented in the full roadmap.
These projects leverage RL and causal inference. To aid the
exposition of both methodologies, a simulator of an enterprise
network was also proposed in [10].

III. REINFORCEMENT LEARNING

Reinforcement learning (RL) is based on the idea that an
agent learns by interacting with an environment on a trial-and-
error basis. The framework is illustrated in ﬁg. 1, with an agent
taking an action and reacting with the environment. Given this
action at, the agent will alter from state st to st+1 collecting
some reward rt+1 (positive or negative) in the process. The

 
 
 
 
 
 
2

process keeps repeating up to a terminal step and is index by
time t in the interim.

Action

at

Agent

Environment

Reward

State

rt
st

rt+1
st+1

Fig. 1. Schematic illustration on how RL works. Figure is an adaptation of
the one presented in [11].

The breakthrough in the RL literature appeared when Google
Deepmind made use of RL agents to beat humans in board
and video games [12]–[14]. As a natural consequence, RL
applications in the cyber domain began to emerge. To name
a few, recent applications of RL to set defence strategies,
considering an enterprise network, have been explored by [8],
[15]–[19]. Note that while [8], [15]–[18] focused on a round
based game, where both attacker and defender play a game
in which the attacker tries to conquer a target node and the
defender tries to detect it, [19] focused on the use of RL to
set honeypots across the network. Building on that, in the
sequel we detail our position with experiences acquired when
conducting the research in [10]. Note that an excellent recent
survey detailing applications in several cyber security domains
making use of deep learning methods with RL is provided in
[20].

In [10] we consider enterprise network security, proposing
a highly abstract simulation-based framework to generate
scenarios of an enterprise network. The preference for an
abstract simulator is motivated by the fact that cyber security
data is unlabelled, there is no information about the ground
truth [21]. In practice this means any information regarding
if and when an attack happened is unavailable. Moreover, if
real data is used to formulate defence strategies, we would be
facing a counterfactual scenario, speciﬁcally the question, what
would have happened if a speciﬁc defence strategy was not
applied under speciﬁc conﬁgurations? Collectively, this leads
to the need to simulate data when dealing with cybersecurity
applications. This is explicitly stated in [20] corroborated by
[8], [15]–[17], [17], [19], [22], [23]. These abstractions have
the objective of portraying several complexities from a real
cyber environment and can provide game like environments
where RL techniques can be deployed to evaluate defence
strategies.

The abstraction we proposed in [10] is a node-based sim-
ulation, where each node of the network represents a device
with parameters characterising concepts of defence strength
(which represents how protected the device is, for instance,
security patch updates or anti-virus versions), vulnerabilities
and credentials. On the other side of this cyber warfare, the
attacker prototype is modeled following lateral traversal attack
properties [24] with parameters abstracting concepts of attack
strength and spread capability. In [10] the attacker seeks to
escalate credentials in an enterprise network. The game ends
when the attacker is successful in acquiring what is designated
as the target node, which may represent, for example the active

directory server. Note that with data from the simulator, the
defender can understand different properties for controlling
defences. Using this abstraction, a stochastic Markov game
with one agent is deﬁned in [10], representing a centralised
defence controller, which acts against an attacker performing
lateral traversal. Both attacker and defender have incomplete
information and partial observability. The attacker does not
know the complete topology of the network. As more nodes
are compromised, more information about the target’s location
is acquired. On the other hand, the defender does know the
network topology but not those nodes which are compromised.
The character of the enterprise cyber-security problem, when
expressed as a game, is distinctly different from more familiar
game scenarios as we now illustrate with simple examples.

In the game of chess the rules are ﬁxed throughout the
game for both players leading to a comparison of the players’
skills to win the game; in the lateral traversal game, the set of
rules is ephemeral and not agreed by both players, speciﬁcally
the attacker is willing to ignore the rules. This means that
the way the game is deﬁned for both players is different (an
asymmetric game) in the sense that attacker and defender win
in different ways under different set of rules. The attacker
seeks to compromise the network, while the defender can only
postpone compromise with a given set of actions. Additionally,
in the cyber environment the set of rules can be adaptive. As
a consequence, rewards and optimal policies for both players
could potentially be time-varying. After observing a set of
movements from the attacker, the defender sets a defence
strategy which potentially would not be optimal when facing
another kind of attack.

As a second example, consider the famous video game, Pac-
Man, which is a maze-based game where an agent must eat
the dots inside the maze, while avoid getting caught by ghosts.
This game shares the asymmetry of the lateral traversal game
in that the players win in different ways. However the rules
are ﬁxed for both. Both the agent and the ghosts have a set
of rules ﬁxed a priori, movements are limited to left, right, up
and down. Given this set of movements, the ultimate goal for
the agent is to collect the maximum number of dots to win
the game, while not being captured. Making an analogy to the
lateral traversal game, we portray the defender as the ghosts
trying to catch Pac-Man, who is the attacker. The attacker has
a huge range of options and no obligation to follow any rules
or policies, or even behave in a consistent manner.

The defender in the lateral traversal game needs to cope with
a set of adaptive mechanisms from the attacker while providing
best utilisation of enterprise resources - the classic security
trade-off between security and usability. Actions the defender
takes have costs and consequences. In framing a defence
policy, an ideal approach would be that which absorbs these
time-varying costs enabling the defender to take decisions in
cyber relevant speed while implementing defence strategies.

One way to reason about decision making challenges for
the defender is to propose collaborative defensive agents
that are able to exchange information about learned attack
patterns. This idea is closely related to the concept of “benign
worms” (see [6] and references therein), where these agents
continuously traverse the network, interacting with devices,

and maintaining the health of the enterprise. These worms
would act as defenders taking decisions regarding a computer’s
safety in real time. However, there are a few drawbacks to
discuss, primarily that they create more attack vectors for a
malicious entity to exploit. If the worms are controlled by a
central agent, the attacker has the opportunity to compromise
this agent thereby potentially increasing it’s attack capabilities.
A decentralised approach, where each worm is autonomous,
reduces the attacker’s potential to compromise them. In either
case these worms would require a digital authentication mech-
anism to validate interacting with a device, opening a new
attack vector.

There are signiﬁcant challenges ahead to construct such
defensive agents with learned and adaptive control policies,
due to the issues described above, such as asymmetry. A
promising direction is provided by a new avenue of research
in RL and control which is already exploring the inclusion of
adversarial disturbances in the model [25]. Such approaches
could enable defensive agents to collect information from
attackers, rather than statistical noise, and then sequentially
optimise a sum of revealed cost functions over time. The
question we can pose then is, how well can defensive agents
perform when compared to rewards achieved under the best
stationary policy over time?

As in the games presented above, real-world attacks are
made up of a sequence of malicious actions with several points
at which defensive interventions could be made. To address
this, we need to supplement our approach with research from
the ﬁeld of causal inference.

IV. CAUSAL INFERENCE

In machine learning (ML) and statistics, usually when we
seek to make predictions we do so by estimating conditional
distributions, which is to say; we are interested in how variable
Y behaves given variable X (and provided we have samples
of both). In causal inference we have the same goal, but
unlike standard ML we take into account two different kinds
of conditional distributions, as indicated in ﬁg. 2.

Observational distribution

Interventional distribution

Z

Z

X

Y

X

Y

E[Y | X]

E[Y | do(X = x)]

Fig. 2. Comparing an observational and interventional distribution, under the
intervention do(X = x), where the expected value of the target value (Y )
is shown w.r.t. the given distribution. In the right panel we are using Pearl’s
do-operator [26], graphically applied to the causal graphical model (CGM).

Observational data are samples (measurements) of a system
evolving under normal operating conditions. Whereas inter-
ventional data are samples of a system evolving under the
inﬂuence of an intervention acting on the system. How does
causal inference compare to supervised ML? The observational

3

conditional p(y | x) asks “what is the distribution of Y
given that I observe that variable X takes value x?” It is a
conditional distribution which can simply be calculated as the
ratio of two of its marginals. The interventional conditional
p(y | do(X = x)) asks “what is the distribution of Y given
that I set that variable X takes value x?” This describes
the distribution of Y we would observe if we intervened in
the data generating process by artiﬁcially forcing the variable
X to take value x, but otherwise simulating the rest of the
variables according to the original process that generated
the data. This is the causal approach. Observe that the data
generating procedure is not the same as the joint distribution
p(x, y, z, . . .).

In the next two sections we will discuss how these simple
principles of causality and the do-calculus [26], can be used
for ACD. In particular our focus will be on causal inference
applied to the MITRE ATT&CK1 framework. This frame-
work provides deﬁnition and classiﬁcation for a range of cyber-
attacks, codiﬁed in a consistent and clear manner. It contains a
comprehensive matrix of tactics and techniques used by threat
hunters and defenders to better classify attacks and assess
the risk posed to an organisation from cyber crime. There is
however little research into understanding causal links between
such tactics and techniques.

The beneﬁts of incorporating the causal aspect are multitude.
For example, the causal approach would allow us to understand
if the sequence of MITRE ATT&CK tactics are consistent
just from a correlation point of view but from a
– not
causal standpoint. If a causal structure can be learned from
observational and interventional data of a particular attack
(say) then, if that pattern is recognised, the defender can amply
anticipate the attacker’s next move, since we are privy to the
causal nature of the attack [27]. Indeed the same idea can be
entertained for defence strategy.

A. Threat mitigation in the MITRE ATT&CK framework

The framework is used as a foundation for the development
of speciﬁc threat models and methodologies in the private
sector, in government, and in the cyber security product and
service community. Therein there are multiple headings under
which techniques are listed according to what an adversary is
trying to achieve, see ﬁg. 3.

We seek to analyse the larger MITRE ATT&CK frame-
work through the lens of a small subset of the available
techniques, found in ﬁg. 3, investigating, in depth, their causal
relationships. Speciﬁcally, we seek to understand the causal
relationships across time. The toy-example (a small sub-CGM
of the full CGM found in ﬁg. 3) is shown in ﬁg. 4.

There are multiple ways in which the topologies can unfold
over time, and some of these are represented in ﬁg. 4 – in (b)
the manipulative variables (interventions) Z and X are, in the
spatial setting, fully independent. One intervention does not
affect the other in that time-slice. In (a) there is a dependence
between interventions; Z naturally has to ﬂow through X in
order to affect the target variable Y . But further, in (c) the
connections change over time to simulate a scenario where

1https://attack.mitre.org

4

Initial access

Credential
access

Discovery

Collection

Exﬁltration

on us being privy to a present threat to the system. Which is
to say that this type of threat mitigation is not useful without
threat detection.

Execution

Privilige
escalation

Lateral
movement

Command
and control

Impact

B. Threat detection

Persistence

Defence
evasion

Compromise

Explore

Exploit

Impact

Fig. 3. Possible causal representations in the MITRE ATT&CK framework
– designed by domain experts at the NCSC and DSTL (note: this is not
a deﬁnite causal representation, there are others). For example, the “lateral
movement” node corresponds to the adversary trying to move through the
environment whereas ‘collection’ means that the adversary is trying to gather
data of interest to their goal. The diagram is encoded with a colour gradient
which from left to right, blue to red, signiﬁes the potential damage an intruder
will cause by engaging in those speciﬁc techniques (nodes).

some external factor(s) are inﬂuencing the unfolding of the
dynamics in the system.

Now, consider the application side of the three scenarios.
In scenario (a) lateral movement is a function of a command
and control attack. Once the malware has gained entry to the
system, the attack will typically evolve through the different
stages of the kill chain. It carries out early reconnaissance,
creates a state of persistence, seeks access to the outside world
through a C&C server and then initiates a series of lateral
movements, until it reaches its ﬁnal goal of data exﬁltration
(other impacts are possible as well of course). In (b) the
intruder is telling the malware what to collect, but the malware
moves around the network without direct control. Finally in
(c) the CGM has a confounder node that represents an attacker
who can directly control both the lateral movement and/or the
collection. Which is to say that an adversary may search local
system sources, such as ﬁle systems or local databases, to ﬁnd
ﬁles of interest and sensitive data prior to exﬁltration.

There are of course multiple scenarios we could entertain
but for the sake of brevity we settle for these three. The
graph structures that we investigate can be motivated by some
known malware structure found in [28, ﬁg. 7]. Finally, the
time-index version of these manipulative and target variables
are connected in a dynamical Bayesian network, and this
particular set of nodes are chosen because they straddle the
exploration/exploitation boundary. The analysis thus far hinges

Command
and control

Z

≡

Lateral
movement

Collection

X

Y

Z

Z

Z

(a)

X

(b)

X

(c)

X

Y

Y

Y

Fig. 4. Sub-graph (selection of tactics from ﬁg. 3) of MITRE framework
with variable labelling for tactics used in this toy example. The shaded node
representation means that this tactic is “observed” or “can be measured”. The
right column denotes possible derivative topologies.

In an ofﬂine setting where we are privy to all the obser-
vational data and have a clear understanding of causal asso-
ciations in our system (i.e. we have a good approximation or
indeed the true, causal graphical model) we seek to understand
if a suite of MITRE ATT&CK indicators of compromise (e.g.
suspicious ﬁle hashes or IP addresses) constitute malicious or
benign activity. From a signal processing stance this would
constitute a smoothing problem in which we can perform
inference from the back and front of the graph. Thus, given a
set of temporal features (where we assume that indicators of
compromise are temporal and have been logged across time),
can we classify this sequence as benign or malign? This falls
under the topic of time-series classiﬁcation. A successful threat
detection would then inform the threat mitigation approach as
shown in ﬁg. 5, and any interventions would then change the
next set of observations.

To enable true breakthroughs in threat detection, in addition
the research community needs
to the proposed simulator,
enhanced approaches to simulating data to provide realistic
scenarios for meaningful threat detection, ideally based on real
observations from enterprises or from complex cyber arenas.

[optional ] Automation

Causal inference

Threat detection

Threat mitigation

Fig. 5. Causal inference learning scheme in a cyber security context. Malware
or threat detection (left box) box precedes mitigation of the threat (right
box), with the found intervention being returned to the threat detection in the
form of feedback, to enable better detection in the future. In this abstraction
and interpretation, the scheme sits within an optional automation framework,
meaning that the user selects the desired level of autonomy.

V. FINAL REMARKS

We have discussed some interesting challenges and
prospects for the development of AI for ACD in the light of the
complexities of enterprise cyber defence. Promising work from
the literature has been discussed and we suggest directions for
further development.

A potentially rich avenue of further research arises by com-
bining §III and §IV performing causal reinforcement learning.
Such approaches have recently gained traction in the research
community [29], [30] and can provide several insights into
how to set controlling defence strategies for speciﬁc types of
attacks. We also refer to their online repository for tutorials
and references [31].

Making substantial progress on our roadmap will require
sustained multi-year investment from government and industry,
as well a range of new collaborative research initiatives
focussed on developing game-changing AI-based defence ca-
pabilities.

REFERENCES

[1] J. Lewis, “Economic impact of cybercrime - no slowing down,” Santa
Clara: McAfee & CSI (Center for Strategic and International Studies),
2018.
[2] “Robust

intelligence

defence,”

artiﬁcial

active

cyber

for

https://www.turing.ac.uk/sites/default/ﬁles/2020-08/public ai acd techreport ﬁnal.pdf,
accessed: 2021-03-02.

5

[28] J. Lee and H. Lee, “Gmad: Graph-based malware activity detection by
dns trafﬁc analysis,” Computer Communications, vol. 49, pp. 33–47,
2014.

[29] E. Bareinboim, S. Lee, and J. Zhang, “An introduction to causal
reinforcement learning,” Columbia University, Tech. Rep., 2020.
[30] J. Zhang and E. Bareinboim, “Designing optimal dynamic treatment
regimes: A causal reinforcement learning approach,” in International
Conference on Machine Learning. PMLR, 2020, pp. 11 012–11 022.

[31] Causal reinforcement learning webpage.

[Online]. Available: https://crl.causalai.net

[3] S. Xu, W. Lu, and H. Li, “A stochastic model of active cyber defense
dynamics,” Internet Mathematics, vol. 11, no. 1, pp. 23–61, 2015.
[4] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise of
adversarial machine learning,” Pattern Recognition, vol. 84, pp. 317–
331, 2018.

[5] M. J. Herring and K. D. Willett, “Active cyber defense: a vision for
real-time cyber defense,” Journal of Information Warfare, vol. 13, no. 2,
pp. 46–55, 2014.

[6] W. Lu, S. Xu, and X. Yi, “Optimizing active cyber defense,” in
International Conference on Decision and Game Theory for Security.
Springer, 2013, pp. 206–225.

[7] M. J. De Lucia, A. Newcomb, and A. Kott, “Features and operation of an
autonomous agent for cyber defense,” arXiv preprint arXiv:1905.05253,
2019.

[8] A. Ridley, “Machine learning for autonomous cyber defense,” Next Wave,

vol. 22, no. 1, pp. 7–14, 2018.

[9] A. Burke, “Robust artiﬁcial intelligence for active cyber defence,” Alan

Turing Institute, Tech. Rep., March 2020.

[10] H. Hoeltgebaum, N. Adams, and M. Briers, “A framework for exploring
automated control in enterprise cyber security,” Under review in: IEEE
Transactions in Dependable Security, 2021.

[11] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.

Cambridge, MA, USA: MIT Press, 1998.

[12] V. Mnih, K. Kavukcuoglu et al., “Human-level control through deep
reinforcement learning,” nature, vol. 518, no. 7540, pp. 529–533, 2015.
[13] D. Silver, A. Huang et al., “Mastering the game of go with deep neural
networks and tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016.
[14] D. Silver, T. Hubert et al., “A general reinforcement learning algorithm
that masters chess, shogi, and go through self-play,” Science, vol. 362,
no. 6419, pp. 1140–1144, 2018.

[15] K. Hammar and R. Stadler, “Finding effective security strategies through
reinforcement learning and self-play,” in 2020 16th International Con-
ference on Network and Service Management (CNSM).
IEEE, 2020,
pp. 1–9.

[16] R. Elderman, L. J. Pater, and A. S. Thie, “Adversarial reinforcement
learning in a cyber security simulation,” Ph.D. dissertation, University
of Groningen, 2016.

[17] J. A. Bland, M. D. Petty et al., “Machine learning cyberattack and

defense strategies,” Computers & Security, vol. 92, p. 101738, 2020.

[18] F. M. Zennaro and L. Erdodi, “Modeling penetration testing with
reinforcement learning using capture-the-ﬂag challenges and tabular q-
learning,” arXiv preprint arXiv:2005.12632, 2020.

[19] S. Dowling, M. Schukat, and E. Barrett, “Improving adaptive honeypot
functionality with efﬁcient reinforcement learning parameters for auto-
mated malware,” Journal of Cyber Security Technology, vol. 2, no. 2,
pp. 75–91, 2018.

[20] T. T. Nguyen and V. J. Reddi, “Deep reinforcement learning for cyber

security,” arXiv preprint arXiv:1906.05799, 2019.

[21] N. Heard, N. Adams et al., Data Science for Cyber-Security. World

Scientiﬁc Publishing, 2018.

[22] A. N. Zakrzewska and E. M. Ferragut, “Modeling cyber conﬂicts
using an extended petri net formalism,” in 2011 IEEE Symposium on
Computational Intelligence in Cyber Security (CICS).
IEEE, 2011, pp.
60–67.

[23] L.-X. Yang, P. Li et al., “A risk management approach to defending
against the advanced persistent threat,” IEEE Transactions on Depend-
able and Secure Computing, vol. 17, no. 6, pp. 1163–1172, 2020.
[24] B. A. Powell, “The epidemiology of lateral movement: exposures and
countermeasures with network contagion models,” Journal of Cyber
Security Technology, vol. 4, no. 2, pp. 67–105, 2020.

[25] N. Agarwal, B. Bullins et al., “Online control with adversarial distur-
PMLR,

bances,” in International Conference on Machine Learning.
2019, pp. 111–119.

[26] J. Pearl, Causality. Cambridge university press, 2009.
[27] X. Qin and W. Lee, “Attack plan recognition and prediction using causal
networks,” in 20th Annual Computer Security Applications Conference.
IEEE, 2004, pp. 370–379.

