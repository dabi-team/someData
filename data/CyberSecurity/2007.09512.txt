0
2
0
2

l
u
J

8
1

]

A
M

.
s
c
[

1
v
2
1
5
9
0
.
7
0
0
2
:
v
i
X
r
a

Active Deception using Factored Interactive POMDPs
to Recognize Cyber Attacker’s Intent

Aditya Shinde
Institute for AI
University of Georgia, Athens
GA 30602
adityas@uga.edu

Prashant Doshi
Institute for AI & Dept. of Computer Science
University of Georgia, Athens
GA 30602
pdoshi@uga.edu

Omid Setayeshfar
Dept. of Computer Science
University of Georgia, Athens
GA 30602
omid.s@uga.edu

Abstract

This paper presents an intelligent and adaptive agent that employs deception
to recognize a cyber adversary’s intent. Unlike previous approaches to cyber
deception, which mainly focus on delaying or confusing the attackers, we focus on
engaging with them to learn their intent. We model cyber deception as a sequential
decision-making problem in a two-agent context. We introduce factored ﬁnitely-
nested interactive POMDPs (I-POMDPX ) and use this framework to model the
problem with multiple attacker types. Our approach models cyber attacks on a
single honeypot host across multiple phases from the attacker’s initial entry to
reaching its adversarial objective. The defending I-POMDPX -based agent uses
decoys to engage with the attacker at multiple phases to form increasingly accurate
predictions of the attacker’s behavior and intent. The use of I-POMDPs also enables
us to model the adversary’s mental state and investigate how deception affects
their beliefs. Our experiments in both simulation and on a real host show that the
I-POMDPX -based agent performs signiﬁcantly better at intent recognition than
commonly used deception strategies on honeypots.

1

Introduction

An important augmentation of conventional cyber defense utilizes deception-based cyber defense
strategies [16]. These are typically based on the use of decoy systems called honeypots [21] with
additional monitoring capabilities. Currently, honeypots tend to be passive systems with the purpose of
consuming the attacker’s CPU cycles and time, and possibly logging the attacker’s actions. However,
the information inferred about the attackers’ precise intent and capability is usually minimal.

On the other hand, honeypots equipped with ﬁne-grained logging abilities offer an opportunity to bet-
ter understand attackers’ intent and capabilities. We may achieve this by engaging and manipulating
the attacker to perform actions that reveal his or her true intent. One way of accomplishing this is
to employ active deception. Active strategies entail adaptive deception which seeks to inﬂuence the
attackers’ beliefs and manipulates the attackers into performing desired actions [11]. We investigate
how multi-agent decision making can be used toward automating adaptive deception strategies to
better understand the attacker.

Preprint. Under review.

 
 
 
 
 
 
We represent cyber deception on a single host as a decision-making problem between a defender
and an attacker. We introduce a factored variant of the well-known interactive partially observable
Markov decision process [9], labeled as I-POMDPX , to computationally model the decision making
of the defender while reasoning about the attacker’s beliefs and capabilities as it acts and observes.
I-POMDPX exploits the factored structure of the problem, representing the dynamics and observation
function using algebraic decision diagrams, and solving the model using a method that directly
operates on these factored representations [1]. This brings some level of tractability to an otherwise
intractable framework, sufﬁcient to adequately solve the cyber deception domain. I-POMDPX
explicitly models the beliefs of the attacker and the defender throughout the interaction. This allows
for detailed inferences about how speciﬁc deceptive actions affect the attacker’s subjective view of
the system. We evaluate the performance of I-POMDPX in promoting active deception with multiple
attacker types both in simulation and on a real host. Our results show that the I-POMDP-based agent
learns the intent of the attacker much more accurately compared to baselines that do not engage the
attacker or immediately deploy all decoys en masse.

2 Background on I-POMDPs

Interactive POMDPs (I-POMDPs) are a generalization of POMDPs to sequential decision-making in
multi-agent environments [9, 4]. Formally, an I-POMDP for agent i in an environment with one other
agent j is deﬁned as,

I-POMDPi = (cid:104)ISi, A, Ti, Ωi, Oi, Ri(cid:105)
ISi denotes the interactive state space. This includes the physical state S as well as models of the
other agent Mj, which may be intentional or subintentional [3]. In this paper, we ascribe intentional
models to the other agent as they model the other agent’s beliefs and capabilities as a rational agent.
A = Ai × Aj is the set of joint actions of both agents. Ti represents the transition function, Ti:
S × A × S −→ [0, 1]. The transition function is deﬁned over the physical states and excludes the
other agent’s models. This is a consequence of the model non-manipulability assumption – an agent’s
actions do not directly inﬂuence the other agent’s models. Ωi is the set of agent i’s observations. O
is the observation function, Oi: S × A × Ω −→ [0, 1]. The observation function is deﬁned over the
physical state space only as a consequence of the model non-observability assumption – other’s model
parameters may not be observed directly. Ri deﬁnes the reward function for agent i, Ri: Si × A −→ R.
The reward function for I-POMDPs usually assigns utilities to the other agent’s physical states.
We limit our attention to a ﬁnitely nested I-POMDP, in which the interactive state space ISi,l at
strategy level l is deﬁned bottom up as,

ISi,0 = S,

ISi,1 = S × Mj,0,

Θj,0 = {(cid:104)bj,0, ˆθj(cid:105) : bj,0 ∈ ∆(ISj,0)}
Θj,1 = {(cid:104)bj,1, ˆθj(cid:105) : bj,1 ∈ ∆(ISj,1)}

...

ISi,l = S × Mj,l−1, Θj,l = {(cid:104)bj,l, ˆθj(cid:105) : bj,l ∈ ∆(ISj,l)}.

Above, ˆθj represents agent j’s frame, deﬁned as ˆθj = (cid:104)Aj, Ωj, Tj, Oj, Rj, OCj(cid:105). Here, OCj
represents j’s optimality criterion and the other terms are as deﬁned previously. Θj is the set of agent
j’s intentional models, deﬁned as θj = (cid:104)bj, ˆθj(cid:105). The interactive state space is typically restricted to a
ﬁnite set of j’s models, which are updated after every interaction to account for the belief update of
agent j. The interactive state space for agent i at level l can be then deﬁned as,

ISi,l = S × Reach(Θj,l−1, H), Θj,l = {(cid:104)bj,l, ˆθj(cid:105) : bj,l ∈ ∆(ISj,l)}.

Here, Reach(Θj,l−1, H) is the set of level l − 1 models that j could have in H steps; Reach
(Θj,l−1, 0) = Θj,l−1. We obtain Reach() by repeatedly updating j’s beliefs in the models in Θj,l−1.

3 Modeling Cyber Deception using Factored I-POMDPs

Engaging and deceiving human attackers into intruding controlled systems and accessing obfuscated
data offers a proactive approach to computer and information security. It wastes attacker resources
and potentially misleads the attacker. Importantly, it offers an untapped opportunity to understand

2

attackers’ beliefs, capabilities, and preferences and how they evolve by sifting the detailed activity
logs. Identifying these mental and physical states not only informs the defender about the attacker’s
intent, but also guides new ways of deceiving the attacker. In this section, we ﬁrst introduce our
domain of cyber deception and subsequently discuss how it can be modeled in a factored I-POMDP.

3.1 Cyber Deception Domain

The cyber deception domain models the interactions between the attacker and the defender on a
single honeypot host system. A state of the interaction is modeled using 11 state variables deﬁning
a total of 4,608 states. Table 1 brieﬂy summarizes the state space. The S_DATA_DECOYS and
C_DATA_DECOYS state variables represent the presence of sensitive data decoys and critical data
decoys. The HOST_HAS_DATA variable represents the true type of valuable data on the system. We
assume that a system cannot have two different types of valuable data simultaneously. This is a
reasonable assumption because usually different hosts on enterprise networks possess different assets.
We differentiate between sensitive_data and critical_data as distinct targets. Sensitive data,
for example, includes private data of employees, high ranking ofﬁcials, or any data that the attacker
would proﬁt from stealing. Also, in practical scenarios, honeypots never contain any real valuable data.
Consequently, in the cyber deception domain in this paper, the HOST_HAS_DATA is none. However,
the attacker is unaware of the honeypot or the data decoys and hence forms a belief over this state
variable. Thus, the HOST_HAS_DATA variable gives a subjective view of the attacker being deceived.

Table 1: The state of the cyber deception domain is comprised of 11 variables.

State Variable Name Values

Description

PRIVS_DECEPTION
S_DATA_DECOYS
C_DATA_DECOYS

HOST_HAS_DATA

DATA_ACCESS_PRIVS
ATTACKER_PRIVS
DATA_FOUND
VULN_FOUND
IMPACT_CAUSED
ATTACKER_STATUS
HOST_HAS_VULN

user, root, none
yes, no
yes, no
sensitive_data,
critical_data, none
user, root
user, root
yes, no
yes, no
yes, no
active, inactive
yes, no

Deceptive reporting of privileges
Presence of sensitive data decoys
Presence of critical data decoys
Type of valuable data
on the system
Privileges required to access or ﬁnd data
Attacker’s highest privileges
Valuable data found by the attacker
Local PrivEsc discovered by attacker
Attack successful
Presence of attacker on the host
Presence of local PrivEsc vulnerability

There are 5 observation variables for the attacker which make a total of 48 unique observations. We
include three different types of attackers; the data exﬁl attacker, data manipulator and persistent
threat. The data exﬁl attacker represents a threat that aims to steal valuable private data from the host.
The data manipulator attacker represents a threat that seeks to manipulate data that is critical for the
operation of a business or a physical target. Thus, the data exﬁl attacker targets sensitive_data in
the system and the data manipulator attacker targets critical_data. The persistent threat attacker
wants to establish a strong presence in the system at a high privilege level.

The attacker in the interaction can perform one of 9 actions to gather information about the system,
manipulate the system, or take action on objectives. Table 2 brieﬂy summarizes the actions available
to the attacker. The FILE_RECON_SDATA and FILE_RECON_CDATA actions cause the DATA_FOUND
variable to transition to yes. The FILE_RECON_SDATA action is slightly worse at ﬁnding data than the
FILE_RECON_CDATA. This reﬂects the fact that private sensitive information is slightly difﬁcult to ﬁnd
because it is often stored in user directories in arbitrary locations. On the other hand, critical data, like
service conﬁguration or database ﬁles, are stored in well-known locations on the system. The attacker
gets information about the DATA_FOUND transition through the DATA observation variable. It simulates
the data discovery phase of an attack. VULN_RECON is another action that works similarly and causes
the VULN_FOUND transition to yes. This transition depicts the attacker looking for vulnerabilities to
raise privileges. Depending on the type of the attacker, the START_EXFIL, MANIPULATE_DATA, or
PERSIST actions can be performed to achieve the attacker’s main objectives. We assume that the
attacker is unable to discern between decoy data and real data, and hence, unable to determine which
variable inﬂuences the DATA_FOUND state transition during ﬁle discovery. The attacker, however, can

3

Action name

FILE_RECON_SDATA
FILE_RECON_CDATA
VULN_RECON
PRIV_ESC
CHECK_ROOT
START_EXFIL
PERSIST
MANIPULATE_DATA
EXIT

Table 2: The actions available to the attacker.
States affected

Description

DATA_FOUND
DATA_FOUND
VULN_FOUND
ATTACKER_PRIVS
none
IMPACT_CAUSED
IMPACT_CAUSED
IMPACT_CAUSED
ATTACKER_STATUS Terminate the attack

Search for sensitive data for theft
Search for critical data for manipulation
Search for local PrivEsc vulnerability
Exploit local PrivEsc vulnerability
Check availability of root privileges
Upload critical data over network
Establish a permanent presence in the system
Manipulate stored data

distinguish between different types of valuable data. So, if the system contains data that is different
from what the attacker expects, the attacker can observe this from the DISCREPANCY observation
variable. As DATA and DISCREPANCY are separate observation variables, the attacker can observe a
discrepancy even when data has been found. When this occurs, the attacker develops a belief over the
decoy data states as the host can have only one type of data. This realistically models a situation in
which the attacker encounters multiple decoys of different types and suspects deception.

The defender in the interaction starts with complete information about the system. The defender’s
actions mostly govern the deployment and removal of different types of decoys. These actions
inﬂuence the S_DATA_DECOYS and C_DATA_DECOYS states. Additionally, the defender can inﬂuence
the attacker’s observations about his privileges through the PRIVS_DECEPTION state. The defender
gets perfect observations whenever the attacker interacts with a decoy. Additionally, the defender
gets stochastic observations about the attacker’s actions through the LOG_INFERENCE observation
variable. The attacker is rewarded for exiting the system after causing an impact. For the data
exﬁl and data manipulator attacker types, this is achieved by performing the START_EXFIL and
MANIPULATE_DATA actions respectively. The persistent threat attacker is rewarded for getting root
level persistence in the system.

Figure 1: The attacker starts with a low prior belief on the existence of decoys and an active defender.
If decoys are indistinguishable from real data, the attacker attributes his observation to the existence
of real data even when the host has none.

Figure 1 illustrates a scenario taken from an actual simulation run with the data manipulator attacker
type. Initially, the attacker has a non-zero belief over the existence of data on the system. However,
the true state of the system on the left shows that the system does not actually contain any data. In the
absence of the defender or any static data decoys, the attacker will eventually update his beliefs to
accurately reﬂect the reality by performing the FILE_RECON_CDATA action and observing the result.
However, to avoid this belief state, the defender deploys data decoys when the attacker acts. The
attacker’s inability to tell the difference between decoy data and real data and his prior belief about

4

the absence of decoys leads him to attribute his observations to the existence of real data leading to
the attacker being deceived.

(a) Dynamics compactly represented as a two time-slice
DBN for select joint actions and observation variables.

(b) An ADD representing the observation function
P NOP(S_DATA_DECOY_INTR’|X (cid:48), Aj)

Figure 2: I-POMDPX representation of the cyber deception domain.

3.2 Factored I-POMDPs for Modeling Cyber Deception

Factored POMDPs have been effective toward solving structured problems with large state and
observation spaces [7, 17]. Motivated by this observation, we extend the ﬁnitely-nested I-POMDP
reviewed in Section 2 to its factored representation, I-POMDPX . Formally, this extension is deﬁned
as:

I-POMDPX = (cid:104)IS i, A, Ti, Yi, Oi, Ri(cid:105)
IS i is the factored interactive state space consisting of physical state factors X and agent j’s models
Mj. In a ﬁnitely-nested I-POMDPX the set Mj is bounded similarly to ﬁnitely-nested I-POMDPs.
Action set A is deﬁned exactly as before. We use algebraic decision diagrams (ADDs) [1] to represent
the factors for agent i’s transition, observation, and reward functions compactly. Ti deﬁnes the
transition function represented using ADDs as P ai,aj (X (cid:48)|X ) for ai ∈ Ai and aj ∈ Aj. Yi is the
set of observation variables which make up the observation space. Oi is the observation function
represented as ADDs, P ai,aj (Y (cid:48)
i|X (cid:48)). Ri deﬁnes the reward function for agent i. The reward function
is also represented as an ADD, Rai,aj (X ).

i = {Y (cid:48)
i1

We illustrate I-POMDPX by modeling the cyber deception domain of Section 3.1 in the framework.
Figure 2a shows the DBN for select state and observation variables given that the attacker engages in
reconnaissance actions. The two slices in the DBN represent the sets of pre- and post-action state vari-
1, ..., X (cid:48)
ables, X = {X1, ..., Xn} and X (cid:48) = {X (cid:48)
n} where Xn represents a single state variable. Simi-
larly, Y (cid:48)
j = {Y (cid:48)
} and Y (cid:48)
, ..., Y (cid:48)
, ..., Y (cid:48)
} denote the sets of observation variables for agents i
j1
jn
in
and j respectively. The ADD P ai(X (cid:48)|X , Aj) = P ai(X (cid:48)
n|X , Aj)
represents the complete transition function for action Ai = ai. This is analogous to the complete
action diagram deﬁned by Hoey et al. [13] for MDPs. Similarly, the observation function is repre-
sented using the ADD (Fig. 2b), P ai(Y (cid:48)
|X (cid:48), Aj) which
i|X (cid:48), Aj) = P ai(Y (cid:48)
i1
is analogous to the complete observation diagram [7]. Additionally, in an I-POMDPX , agent i
also recursively updates the beliefs of agent j. The attacker types are modeled as frames in Mj.
Let Mj = {mj1 : (cid:104)bj1, ˆθj1(cid:105), ..., mjn : (cid:104)bjq , ˆθjr (cid:105)} be the set of all models in Reach(Θj,l−1, H).
Because neither aj nor oj are directly accessible to agent i, they are represented as ADDs
P (Aj|Mj) and P ai(Y (cid:48)
j, Aj, X (cid:48)) =
j|X (cid:48), Aj). Using these factors, we can now deﬁne the distribution
P ai(M (cid:48)
over X (cid:48) and M (cid:48)

j given action ai and observation oi as a single ADD using existential abstraction:

j|X (cid:48), Aj). The distribution over M (cid:48)

|X (cid:48), Aj) × ... × P ai(Y (cid:48)
in

n, X , Aj)×...×P ai(X (cid:48)

j is then P ai(M (cid:48)

j|Mj, Aj, Y (cid:48)

j) × P ai(Y (cid:48)

2, . . . , X (cid:48)

j|Mj, Y (cid:48)

1|X (cid:48)

P ai,oi(M (cid:48)

j, X (cid:48)|Mj, X ) =

(cid:88)

Aj ,Y (cid:48)
j

P ai,oi(Y (cid:48)

j, M (cid:48)

j, X (cid:48), Aj|Mj, X )

(cid:88)

=

Aj ,Y (cid:48)
j

P ai(X (cid:48)|X , Aj)P ai(Y (cid:48)

i|X (cid:48), Aj)P (Aj|Mj)P ai(M (cid:48)

j|Mj, Aj, Y (cid:48)

j, X (cid:48)).

5

(1)

Here, the ADD P ai(X (cid:48)|X , Aj) compactly represents Ti(st−1, at−1
represents
P ai (M (cid:48)
and
, at−1
j, bt
τθt
j
ADD P ai,oi(X (cid:48), M (cid:48)
given action ai and observation oi. The I-POMDPX belief update can then be computed as:

i|X (cid:48), Aj)
|θt−1
the probabilities Oi(st, at−1
),
j
j
i
j, X (cid:48))
transitions
represents
j)Oj(st, at−1
, ot
the constructed
j|X Mj) contains the transition probabilities for all interactive state variables

i
i), P (Aj|Mj)
recursive
j) of the original I-POMDP. Thus,

, at−1
j
represents P (at−1
update

j|Mj, Aj, Y (cid:48)
, ot

, st), P ai(Y (cid:48)

, ot
the

, at−1
j

, at−1
j

(bt−1
j

belief

i

j

bai,oi
i

(X (cid:48), M (cid:48)

j) =

(cid:88)

X ,Mj

b(X , Mj) × P ai,oi(X (cid:48), M (cid:48)

j|X , Mj)

(2)

where the ADD P ai,oi(X (cid:48), M (cid:48)

j|X , Mj) is obtained as in Eq. 1.

Symbolic Perseus [17] offers a relatively scalable point-based approximation technique that exploits
the ADD structure of factored POMDPs. Toward generalizing this technique for I-POMDPX , we
are aided by the existence of point-based value iteration for I-POMDPs [5]. Subsequently, we may
generalize the α-vectors and its backup from the latter to the factored representation of I-POMDPX :

Γai,∗ ←− αai,∗(X , Mj) =

(cid:88)

Rai(X , Aj)P (Aj|Mj)

Aj
Γai,oi ∪←− αai,oi(X , Mj) = γ

(cid:88)

X (cid:48),M (cid:48)
j

P ai,oi(X (cid:48), M (cid:48)

j|X , Mj)αt+1(X (cid:48), M (cid:48)

j),

∀αt+1 ∈ V t+1

(3)

Γai ←− Γai,∗ ⊕oi arg max

(αai,oi · bi), V t ←− arg max
Γai

αt∈(cid:83)

Γai,oi

ai

(αt · bi),

∀bi ∈ Bi

Here, V t+1 is the set of α-vectors from the next time step and bi is a belief point from the set of
considered beliefs Bi. A popular way of building Bi is to project an initial set of beliefs points
forwards for H time steps using the belief update of Eq. 2.

4 Experiments and Analysis

We modeled the full cyber deception domain described in Section 3.1 from the perspective of a level-1
defender using the I-POMDPX framework. We implemented the generalized Symbolic Perseus using
the point-based updates of the α-vectors and the belief set projection as given in Section 3.2, in order
to solve I-POMDPX . The solver has several enhancements such as cached ADD computations and
ADD approximations for additional speed up.

We evaluate the deception policy generated by I-POMDPX in simulations and on an actual system
consisting of a standalone attacker programmed via Metasploit [15] and a defender workstation. We
simulate each attacker type using the optimal policy computed by the level-0 attacker POMDP. We
show these policies for each type of attacker in the supplementary material. For the simulations,
we randomly sample the frame and the starting privileges of the attacker to simulate a threat with
unknown intentions and privileges. The defender begins knowing about the existence of decoys on
the system. The attacker, on the other hand, does not have prior knowledge about any vulnerabilities
or data on the system. The defender engages with the attacker by deploying decoys, facilitating
deceptive observations, or adding known vulnerabilities to the system. In the simulations, the state
transitions and observations for both agents are generated by sampling from the joint transition
functions and individual observation functions.

Simulations We compare the I-POMDPX policy against other passive baselines: one that does
not engage and passively observes the attacker, and another which uses deception indiscriminately
having deployed both sensitive and critical data decoys and all vulnerabilities in the honeypot at the
beginning. We label the ﬁrst baseline as NO-OP(no decoy) and the second baseline as NO-OP(all
decoys). We perform the simulations for 30 trials with an attacker type randomly picked in each trial.
During each trial, the defender begins not knowing the type of the attacker and believes that the state
is that the attacker’s privileges are not known. We set H in Reach(Θj,l−1, H) to 5. The generalized
Symbolic Perseus is then run on 200 projected belief points until convergence to obtain the policy,
which prescribes the subsequent actions for the defender until the end of the trial. It converges in
about 6 minutes with a mean time per backup of 37 secs on Ubuntu 18 with Intel i7 and 64 GB RAM.

6

The NO-OP(no decoy) and NO-OP(all decoy) yielded a mean (± std err.) of 4.30 ± 0.16 and 3.26
± 0.20 steps of engagement with the attacker, respectively. The longest engagement among these
consisted of 7 and 5 steps, respectively. With NO-OP(no decoy), the attacker spends time searching
for data and attempting to escalate his privileges but without much success, ﬁnally exiting the system.
With NO-OP(all decoys), the attacker either quickly exploits the vulnerabilities or encounters the
data decoys but quickly exits often due to the encountered data not being as expected. However, the
I-POMDPX agent engaged with the attacker for a mean duration of 5.90 ± 0.24 with the longest
interaction happening for 9 steps. It leverages the information gained by the ﬁrst few observations
to avoid using decoys that the attacker would ﬁnd suspicious. For example, the defender ﬁrst
manipulates the attacker’s observations about his privileges. This increases her chances of observing
ﬁle enumeration or vulnerability discovery activity, forming a belief over the frames. Subsequently,
the defender baits the attacker using decoys and observes the interaction to solidify her belief. This
minimizes the risk of the attacker encountering unexpected decoys or noticing discrepancies.

The simulations above are predicated on the level-1 defender believing that none of the level-0
attacker types are aware of the deception, which is the typical case. However, if the defender believes
that the attacker believes that there is a small chance at 0.1 of decoys being used, we observed that
the attacker often quickly exited the system as one would expect.

(a) When engaging a defender-unaware attacker,
I-POMDPX -based defender outperforms other pas-
sive agents in engaging the attackers and recogniz-
ing their intent

(b) On the actual host deployment, the I-POMDPX -
based agent uses implemented deception tech-
niques to engage with the attacker for longer dura-
tion than other agents

Figure 3: Cross entropy (KL divergence) of the beliefs of the I-POMDPX agent and other baselines
in simulations. Cross entropies near zero signify good intent recognition.

Do the extended engagements facilitated by the I-POMDPX agent help in intent recognition? Fig-
ure 3a(a) shows the cross-entropy between the defender’s belief of the attacker’s frame and the
attacker’s true type, as it varies across the steps of the interaction. The defender’s I-POMDPX policy
eventually yields the lowest cross-entropy values compared to the baselines, often reaching zero
in 6 steps. We show the cross-entropy for more steps because the attacker remains in the system
performing a few more actions. The sharp decrease in cross-entropy in the ﬁrst three steps is because
the attacker’s decoy interactions (if the attacker is of type data exﬁl or manipulator) are perfectly
observed by the defender (some other interactions generate noisy observations). Multiple consecutive
data reconnaissance steps ﬁlter out the persistence attacker type, and the ﬁnal step of either exﬁltrating
the data or manipulating it allows the defender to distinguish between the remaining two attacker
types. But, for the NO-OP(no decoy) with no deception, the only source of information about the at-
tacker is his general actions, which is noisy. Hence, such a defender is unable to form accurate beliefs
before the attacker leaves the interaction. For the NO-OP(all decoy) agent that indiscriminately uses
deception, observations from decoy interactions are perfect, but the risk of the attacker encountering
contradicting decoys and suspecting deception is also high leading to early exits.

Host deployment
In our next phase of experimentation, we evaluated the real-world feasibility of
deploying an operational I-POMDPX on a host system and testing its efﬁcacy. The testbed consists
of 3 separate hosts: the attacker, the adaptive honeypot and the defender. Figure 4 shows the overall
architecture of our testbed implementation. The attacker system runs a Kali Linux distribution which
is well known for the variety of offensive and defensive cybersecurity tools that are preinstalled on it.

7

Figure 4: System architecture of the testbed used to deploy the agents. The defender manipulates the
system through decoys and commonly used coreutils binaries to give deviant observations.

The adaptive honeypot on which the interaction takes place runs a Metasploitable 3 Linux distribution.
This distribution has a wide range of builtin vulnerabilities and is commonly used to simulate victim
workstations in cyber attack simulations. The adaptive honeypot also contains an attacker agent
that executes the attacks and communicates with the attacker. The attacker agent implements the
actions given by the attacker’s optimal plan located on the attacker host using realistic techniques
commonly used by real attackers. We implement real exploits to facilitate privilege escalation on the
host. The adaptive honeypot also has a defender agent that implements the defender’s actions and
gets observations.

The defender AI located on the defender workstation solves the I-POMDPX and computes the
optimal action. For implementing the observation function, the I-POMDPX agent monitors and
analyzes the system logs to get information about the attacker’s actions (i.e., observations). To
enable this, we use GrAALF [19], a graphical framework for processing and querying system call
logs. GrAALF analyzes system call logs in real-time and provides the stochastic LOG_INFERENCE
observation variable values (pertaining to ﬁle and vulnerability searches) as well as the perfectly
observed DATA_DECOY_INTERACTION variable values to the defender.

Our results in Fig. 3(b) show the adaptive deception strategy employed by the I-POMDPX agent is
better at engaging adversaries on a honeypot as compared to the passive strategies that are commonly
used. While the cross entropy does not reach zero due to the challenge of accurately inferring the
attacker’s actions from the logs (leading to noisier observations), it gets close to zero, which is
indicative of accurate intent recognition.

5 Related Work

AI methods are beginning to be explored for use in cyber deception. An area of signiﬁcant recent
interest has been game-theoretic multi-agent modeling of cyber deception, which contrasts with the
decision-theoretic modeling adopted in this paper.

Schlenker et al. [18] introduced cyber deception games based on Stackelberg games [20]. These
model deception during the network reconnaissance phase when the attacker is deceived into intruding
a honeypot. Another similar approach [6] allocates honeypots in a network using a Stackelberg game.
The game uses attack graphs to model the attacker and creates an optimal honeypot allocation strategy
to lure attackers. Jajodia et al. [12] develop probabilistic logic to model deception during network
scanning. While these efforts focus on static deployment of deception strategies at the network level,
we seek active deception at the host level – once the attacker has entered the honeypot. Further, we
model individual phases of the attack in greater detail, which allows us to employ realistic deception
techniques at each phase.

At the host level, Carroll et al. [2] models deception as a signaling game while Horak et al. [10] creates
a model for active deception using partially observable stochastic games. However, both of these
take a high-level view modeling defender actions rather abstractly. In contrast, our defender actions
are realistic and can be implemented on honeypots as demonstrated in Section 4. Ferguson-Walter
et al. [8] model possible differences between the attacker’s and defender’s perceptions toward the
interaction by modeling cyber deception as a hypergame [14]. Hypergames model different views

8

of the game being played from the perspective of the players. While this approach similar to ours
represents the attacker’s perspective of the game, we explicitly model the adversary using a subjective
decision-theoretic approach and do not solve for equilibrium.

6 Conclusion

Our approach of utilizing automated decision making for deception to recognize attacker intent
is a novel application of AI and decision making in cyber security. It elevates the extant security
methods from anomaly and threat detection to intent recognition. We introduced a factored variant of
the well-known I-POMDP framework, which exploits the environment structure and utilized it to
model the new cyber deception domain. Our experiments revealed that the I-POMDPX -based agent
succeeds in engaging various types of attackers for a longer duration than passive honeypot strategies,
which facilities intent recognition. Importantly, the agent is practical on a real system with logging
capabilities paving the way for its deployment in actual honeypots.

9

Broader Impact

On a broader scale, the I-POMDPX framework that we introduce makes I-POMDPs tractable to
be applied to larger problems. I-POMDPs are suitable for modeling multi-agent interactions due
to their ability to model opponents from the perspective of an individual. This has a multitude of
applications like negotiations, studying human behavior, cognition, etc. Through our work, we hope
to make I-POMDPs tractable to be applied to such domains. Another area that we hope to motivate
through our research is deception in human interactions. Modeling other agents explicitly will help
understand how deceptive or real information inﬂuences an individual’s beliefs. This has a wide range
of potential applications such as studying how biases can be exploited, the effect of fake news on
individuals, and how individuals can detect deception. We hope our research will eventually motivate
further research in areas like counter deception and deception resilience in agents.

At an application level, our work aims to motivate the use of AI and decision making to create
informed cyber defense strategies. Our work provides a new perspective different from the traditional
action-reaction dynamic that has deﬁned interactions between cyber attackers and defenders for years.
Our framework models the opponent’s mental states and preferences. This will aid security teams
in understanding threats at a deeper level. We hope our framework will motivate the development
of adaptive and intelligent deceptive solutions that can study and predict attackers at a deeper level.
Understanding attackers’ mental models, inherent biases, and preferences will go a long way in
forming ﬂexible cyber defense strategies that can adapt to different threats.

References

[1] R. I. Bahar, E. A. Frohm, C. M. Gaona, G. D. Hachtel, E. Macii, A. Pardo, and F. Somenzi.
Algebric decision diagrams and their applications. Formal methods in system design, 10(2-3):
171–206, 1997.

[2] T. E. Carroll and D. Grosu. A game theoretic investigation of deception in network security.
Security and Communication Networks, 4(10):1162–1172, 2011. ISSN 19390122. doi: 10.
1002/sec.242.

[3] D. Dennett. Intentional systems. brainstorms, 1986.

[4] P. Doshi. Decision making in complex multiagent settings: A tale of two frameworks. AI

Magazine, 33(4):82–95, 2012.

[5] P. Doshi and D. Perez. Generalized point based value iteration for interactive pomdps. In AAAI,

pages 63–68, 2008.

[6] K. Durkota, V. Lis`y, B. Bošansk`y, and C. Kiekintveld. Approximate solutions for attack graph
games with imperfect information. In International Conference on Decision and Game Theory
for Security, pages 228–249. Springer, 2015.

[7] Z. Feng and E. A. Hansen. Approximate planning for factored pomdps. In Sixth European

Conference on Planning, 2014.

[8] K. Ferguson-Walter, S. Fugate, J. Mauger, and M. Major. Game theory for adaptive defensive
cyber deception. In Proceedings of the 6th Annual Symposium on Hot Topics in the Science of
Security, page 4. ACM, 2019.

[9] P. J. Gmytrasiewicz and P. Doshi. A framework for sequential planning in multi-agent settings.

Journal of Artiﬁcial Intelligence Research, 24:49–79, 2005.

[10] K. Horák, Q. Zhu, and B. Bošansk`y. Manipulating adversary’s belief: A dynamic game
approach to deception by design for proactive network security. In International Conference on
Decision and Game Theory for Security, pages 273–294. Springer, 2017.

[11] S. Jajodia, V. Subrahmanian, V. Swarup, and C. Wang. Cyber deception. Springer, 2016.

[12] S. Jajodia, N. Park, F. Pierazzi, A. Pugliese, E. Serra, G. I. Simari, and V. Subrahmanian.
A probabilistic logic of cyber deception. IEEE Transactions on Information Forensics and
Security, 12(11):2532–2544, 2017.

10

[13] A. Jesse Hoey, R. S. Aubin, and C. Boutilier. Spudd: stochastic planning using decision
diagrams. Proceedings of Uncertainty in Artiﬁcial Intelligence (UAI). Stockholm, Sweden. Page
(s), 15, 1999.

[14] N. S. Kovach, A. S. Gibson, and G. B. Lamont. Hypergame theory: a model for conﬂict,

misperception, and deception. Game Theory, 2015, 2015.

[15] D. Maynor. Metasploit toolkit for penetration testing, exploit development, and vulnerability

research. Elsevier, 2011.

[16] Pingree. Emerging Technology Analysis : Deception Techniques and Technologies Create
Security Technology Business Opportunities. Trapx security, pages 1–18, 2018. doi: G0027834.

[17] P. Poupart. Exploiting structure to efﬁciently solve large scale partially observable Markov

decision processes. PhD thesis, University of Toronto, 2005.

[18] A. Schlenker, O. Thakoor, H. Xu, L. Tran-Thanh, F. Fang, P. Vayanos, M. Tambe, and Y. Vorob-
eychik. Deceiving cyber adversaries: A game theoretic approach. Proceedings of the Interna-
tional Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS, 2:892–900,
2018. ISSN 15582914.

[19] O. Setayeshfar, C. Adkins, M. Jones, K. H. Lee, and P. Doshi. Graalf: Supporting graphical

analysis of audit logs for forensics. arXiv preprint arXiv:1909.00902, 2019.

[20] M. Simaan and J. J.B. Cruz. On the stackelberg strategy in nonzero-sum games. Journal of

Optimization Theory and Applications, 11(5):533–555, 1973.

[21] L. Spitzner. The honeynet project: Trapping the hackers. IEEE Security & Privacy, 1(2):15–23,

2003.

Appendix

Attacker Policies

We model the attackers using optimal policies of their level-0 POMDPs. For our problem, we deﬁne
three distinct types of attackers which are modeled as separate frames in the I-POMDP. Below we
discuss the optimal policies for each type.

The data exﬁl attacker frame

The data exﬁl type attacker is rewarded for stealing sensitive_data on the host. We model this
type based on threats that steal private data and other sensitive data from systems. The attacker starts
with no knowledge of the existence of data on the system. We see that the optimal policy recommends
the FILE_RECON_SDATA action which simulates sensitive data discovery on computers. After failing
to ﬁnd data after the ﬁrst few attempts, the attacker attempts to escalate privileges and search again.
If the attacker encounters unexpected types of decoys, the attacker leaves since there is no reward
for stealing data that is not sensitive. Also, the observation of discrepancies when data is found will
inform the attacker about the possibility of deception. This is because the system only contains a
single type of data. On being alerted to the possibility of being deceived, the attacker leaves the
system.

The data manipulator attacker frame

The data manipulator type attacker is rewarded for manipulating critical_data on the host. This
attacker type is modeled after attackers that intrude systems to manipulate data that is critical for
a business operation. Similar to the data exﬁl type, the attacker starts with no information about
the data. The optimal policy for this attacker type recommends FILE_RECON_CDATA action in the
initial steps. Because critical data like service conﬁgurations or databases are usually stored in well-
known locations, the FILE_RECON_CDATA is modeled to ﬁnd critical_data quickly as compared
to sensitive data. In the subsequent interaction steps, the attacker escalates privileges to continue the
search if data is not found in the initial steps. Like the data exﬁl attacker, the data manipulator also
leaves the system on observing discrepancies, suspecting deception, or on failure to ﬁnd data.

11

Figure 5: Optimal policy for data exﬁl type attacker

Figure 6: Optimal policy for data manipulator type attacker

The persistent threat attacker frame

Figure 7: Optimal policy for persistent threat type attacker

The persistent threat type attacker aims to establish root level persistence on the host. Such attacks
are common. Attackers establish a strong presence in an organization’s network and stay dormant for

12

an extended duration. For this attacker type, the policy consists of vulnerability discovery actions
in the initial steps. The attacker escalates privileges by performing the PRIV_ESC action on ﬁnding
vulnerabilities. Once the attacker has the required privileges, the PERSIST action is performed to
complete the objective.

While all three attacker policies may seem signiﬁcantly different from their actions, the defender’s
observations of these actions are noisy. The errors in observation come from the noisy nature
of real-time log analysis. For example, the VULN_RECON action models vulnerability discovery
on a host. This action involves looking through the local ﬁle system for any vulnerable scripts,
enumerating system information, listing services, etc. Thus a VULN_RECON can be mistaken for
a FILE_RECON_CDATA or a FILE_RECON_SDATA in real-time log analysis. Similarly, it is difﬁcult
to tell the difference between the FILE_RECON_CDATA and FILE_RECON_SDATA from logs alone.
Hence, without baiting the attacker into performing further actions, it is challenging to infer the intent
of the attacker from the ﬁrst few actions.

13

