Setting the threshold for high throughput detectors
A mathematical approach for ensembles of dynamic, heterogeneous, probabilistic anomaly detectors

Robert A. Bridges*, Jessie D. Jamieson†, Joel W. Reed*

*Computational Sciences & Engineering Division, Oak Ridge National Laboratory, Oak Ridge, TN
{bridgesra, reedjw}@ornl.gov
†Department of Mathematics, University of Nebraska, Lincoln, NE, jdjamieson@huskers.unl.edu

Abstract—Anomaly detection (AD) has garnered
ample attention in security research, as such algo-
rithms complement existing signature-based methods
but promise detection of never-before-seen attacks. Cy-
ber operations now manage a high volume of heteroge-
neous log data; hence, AD in such operations involves
multiple (e.g., per IP, per data type) ensembles of
detectors modeling heterogeneous characteristics (e.g.,
rate, size, type) often with adaptive online models
producing alerts in near real time. Because of the high
data volume, setting the threshold for each detector
in such a system is an essential yet underdeveloped
conﬁguration issue that, if slightly mistuned, can leave
the system useless, either producing a myriad of alerts
(and ﬂooding downstream systems) or giving none.

In this work, we build on the foundations of Ferragut
et al. to provide a set of rigorous results for under-
standing the relationship between threshold values and
alert quantities, and we propose a principled algo-
rithm for setting the threshold in practice. Speciﬁcally,
we create an algorithm for setting the threshold of
multiple, heterogeneous, possibly dynamic detectors
completely a priori, in principle. Indeed, if the un-
derlying distribution of the incoming data is known
(closely estimated), the algorithm provides provably
manageable thresholds. If the distribution is unknown
(e.g., has changed over time) our analysis gives insight
into how the model distribution diﬀers from the actual
distribution, indicating a period of model reﬁtting is
necessary. We provide empirical experiments showing
the eﬃcacy of the capability by regulating the alert rate
of a system with ≈2,500 adaptive detectors scoring over
1.5M events in 5 hours. Further, we demonstrate on the
real network data and detection framework of Harshaw
et al. the alternative case, showing how the inability to
regulate alerts indicates how the detection model is not
a good ﬁt to the data.

7
1
0
2

t
c
O
5
2

]

R
C
.
s
c
[

1
v
2
2
4
9
0
.
0
1
7
1
:
v
i
X
r
a

I. Introduction

Flow Record Example

The current state of defense against cyber attacks is a lay-
ered defense, primarily of a variety of automated, signature-
based detectors and secondarily via manual investigation
by security analysts. Typically large cyber operations (e.g.,
at government facilities) have widespread collection and
query capabilities for an enormous amount of logging and
alert data. For example, at the network level, ﬁrewalls and
intrusion detection/prevention systems (IDS/IPS) such as
Snort1 produce logs, warnings, and alerts that are collected,
in addition to the collection of network ﬂow logs, and
sometimes full packet captures are stored and/or analyzed.
situational
Additionally,
such as
awareness
tools
Nessus2 provide
lists of
software, software version,
and known vulnerabilities
for each host. Host-based
IDS/IPS such as McAfee3
software
(AV)
anti-virus
and AMP4 report alerts to
cyber security operations,
in addition to situational
awareness appliances. Hence,
security analysts now have
access to multiple streams of heterogeneous sources
producing data in high volumes. As an example, a large
enterprise network operation, with which we collaborate,
monitors only a portion of their network ﬂow logs, a
volume of 4-7 GB/s, in addition to many other logging
and alerting tools employed. Consequently, manual
investigation and automated processing of data/incidents
must manage the large bandwidth.

09:58:32.912
tcp
192.168.1.100
59860
172.16.100.10
80
508526
1186562
1695088

Time
Protocol
SrcIP
SrcPort
DstIP
DstPort
SrcBytes
DstBytes
TotBytes

Table I: Flows record the meta-
data of IP-IP communications.

This is the extended version of this document including proofs to
mathematical results. Please cite the published version appearing in
the Proceedings of IEEE Big Data Conference 2017.

This manuscript has been authored by UT-Battelle, LLC under Con-
tract No. DE-AC05-00OR22725 with the U.S. Department of Energy.
The United States Government retains and the publisher, by accepting
the article for publication, acknowledges that the United States
Government retains a non-exclusive, paid-up, irrevocable, world-wide
license to publish or reproduce the published form of this manuscript,
or allow others to do so, for United States Government purposes.
The Department of Energy will provide public access to these results
of federally sponsored research in accordance with the DOE Public
Access Plan (http://energy.gov/downloads/doe-public-access-plan).

While the ﬁrst line of defense is signature-based methods
(e.g., AV, ﬁrewall), which operate by matching precise rules
that identify known attack patterns, their false negative
rate is problematic. In response there is a large body
of literature to use anomaly detection (AD) systems for

1https://www.snort.org/
2https://www.tenable.com/products/nessus-vulnerability-

scanner

3https://www.mcafee.com/us/index.html
4http://www.cisco.com/c/en/us/products/security/advanced-

malware-protection/index.html

1

 
 
 
 
 
 
protection [1–16]. AD provides a complimentary monitoring
tool that holds the promise of detecting novel attacks by
identifying large deviations from normal behavior, and
this concept has been proven in many of the previous
works. Ideally, accurate detection with a low number
of false positives is achieved. At a minimum, an AD
IDS should isolate a manageable subset of events that
are suﬃciently abnormal to warrant next-step operating
procedures, such as, passing alerts to a downstream system
(e.g., automated signature generator) or to an operator for
manual investigation.

While AD has garnered much research attention, such
algorithms are met with many challenges when used in
practice in the cyber security domain. How to design AD
models for accuracy—exploring what statistics, algorithms,
and data representations to use so that the detected events
correspond with operator-deﬁned positives—is the focus
of many previous works [1–8, 14–17] and in deployment
is likely a network-speciﬁc task leveraging both domain
expertise (understanding attacks, protocols, etc.) and tacit
environmental knowledge (understanding conﬁguration
of network appliances and their behaviors). Common
trends to increase accuracy involve the use of ensembles
of heterogeneous detectors [3–7, 14, 16, 18, 19] and/or
online detection models that adapt in real time and/or
upon observations of data [1, 5, 8, 20, 21]. In practice, the
need for multiple detectors is enhanced by the diversity
in network components (models conditioned on each host,
subnet, etc.), data types (models conditioned on ﬂow data,
system logs, etc.), and features of interest (rate, distribution
of ports used, ratio of data-in to data-out, etc.). E.g.,
patents of Ferragut et al. [22, 23] detail AD systems using
a ﬂeet of dynamic models and producing near real-time
alerts on high volume logging data.

A. Problem Addressed

In this work we do not present novel methods for accurate
detection of intrusions. Rather, we address a diﬃcult but
important question for AD in IDS applications, namely:
How should the alert threshold be set in the case of a large
number of heterogeneous detectors, possibly changing in
real time, that are producing alerts on high-volume data?
Our organization’s cyber operations’ analysts have arrived
at the problem of alert rate regulation from three scenarios
that all require real-time prioritization of alerts that can
accommodate inﬂuxes of data as well as the multitude of
evolving models, namely, (1) manual alert investigation
requires an online way to triage events; (2) data storage
limitations, e.g., storing packet captures (PCAPs) from
the most anomalous traﬃc, requires a real-time algorithm
for prioritizing events as “anomalous enough”; (3) online
automated alert processing (e.g., automated signature
generation of anomalous activity) cannot handle inﬂuxes,
i.e., downstream systems require alert rate regulation to
prevent a denial-of-service.

To illustrate the problem, consider the AD system
and Skaion data used in Section III-A. This AD system
scores each ﬂow using two evolving probability models per
internal IP, totaling about 2,500 dynamic detectors. It
is simply not feasible to manually tune the threshold for
each model, and even so, since the models are changing in
real time, reconﬁguration would be periodically necessary.
Furthermore, the consequences of misconﬁgured thresholds
are substantial. Altogether, the system produces a collective
≈2M anomaly scores in about ﬁve hours; hence, a threshold
that is only slightly too high can produce tens of thousands
of alerts per hour! Moreover, this dataset is small compared
to many networks, and the detection ensemble grows
linearly with the number of IPs to model (network size).
The speciﬁc problem of how to set the alert threshold
for detection systems in these very realistic scenarios is
diﬃcult, underdeveloped, and when not properly addressed,
leaves anomaly detectors useless, as their goal is to isolate
the most abnormal events from the sea of data. Hence, we
arrive at our problem of interest. How does an operator
set the threshold for an AD system, given that the method
must adequately accommodate a multitude of possibly
adaptive models operating on possibly variable speed, high
volume data? Second, our work contributes to the related
problem of detecting drift of adaptive models over time.

B. Background & New Contributions

This alert-rate regulation problem is ﬁrst speciﬁed by
Ferragut et al. [5], and they note that a principled notion
of quantitative comparability across detectors is necessary
to deal with multiple/dynamic models. Their relevant
contribution is twofold. (1) By assuming data is sampled
from an accessible probability distribution, they formulate
a deﬁnition equivalent to Defn. II.2. The upshot is that
anomalies are events with low p-values (see Defn. II.1),
and this technique provides a distribution-independent,
comparable anomaly score. (2) They provide a theorem
equivalent to Lemma II.3 for alert-rate regulation. No alert
rate algorithm nor experimental testing of the theorem’s
consequences are presented.

Kreigel et al. [18] have addressed the problem of compar-
ing multiple outlier detection methods by manually crafting
transformation functions that convert the given output to
a score to a comparable output in the interval [0,1]. This is
only done for a handful of outlier detection algorithms,
indicating the obvious drawback of this approach, the
necessity to manually investigate each model.

For numerical time-series data, Siﬀer et al. [24] exploit
the extreme value theorem to ﬁnd distribution-independent
bounds on the rate of extreme (large) values.

Our contributions build on the work of Ferragut et al. [5]
both in extending the mathematics and in converting
these theorems into an operationally-viable solution to
the problem. New theorems of Section II provide further
mathematical advancements pertinent to understanding
the relationship between the p-value threshold and the

2

likelihood of an alert. These results informs an operational
workﬂow. Given (1) a detection capability that uses a
probability distribution to score low p-value events as
anomalies and (2) knowledge of the data’s rate, the
operator has a principled, distribution independent method
for setting the threshold to regulate the number of alerts
produced. Hence, the algorithm can be applied to an
ensemble of possibly dynamic, heterogeneous detectors
to prevent overproduction of alerts. Notably, the system
will not suppress inﬂuxes of anomalies, but asymptotically
the operator-given bound is respected. Our math results
give hypotheses that ensure equality in the theorem;
operationally, this is the case when users can specify, rather
than just bound, the number of alerts. As the theorems hold
independent of the model (distribution) used, operators can
set the threshold a priori. In particular, it remains valid in
a streaming setting, where the detection model is updated
in real time to accommodate new observations. Because
the underlying assumption is that future observations
are sampled from the model’s distribution, the alert rate
regulation will fail if the model distribution diﬀers from the
actual distribution. Hence, the theorem’s contrapositive
gives an operational beneﬁt, namely that violations of
the operator-given alert rate indicate that the anomaly
detection model is not a good ﬁt to the data. In this case
a period of relearning the distribution is necessary for the
threshold-setting algorithm to remain eﬀective.

We present empirical experiments testing our method in
setting the alert rate in two scenarios, both using detectors
on network ﬂow data. The ﬁrst (Section III-A) shows
the eﬃcacy of setting the threshold on approximately
2,500 simultaneous, dynamically updated detectors. In
this scenario, multiple anomaly scores are computed per
ﬂow; hence, the data rate is high and varies according to
network traﬃc. The results show that the mathematics
give a method for regulating the alert rate of dynamic
detectors that is a priori (in the sense that no knowledge
of the speciﬁc distribution is necessary for threshold
conﬁguration). Our second experiment (Section III-B) uses
data from the network AD paper of Harshaw et al. [8],
which ﬁts a Gaussian to a vector describing the real network
traﬃc every 30 seconds. Hence, it is a single, ﬁxed rate
detector. The results from this application illustrate the
analytic capabilities for gauging model ﬁt that are made
possible by the theorems we develop.

Altogether our work gives new mathematical results re-
garding the p-value distribution. This informs an algorithm
that poses an alternative—Operators can accurately set
the threshold of detection ensembles to bound the expected
number of alerts or identify a misﬁt of the detection model.

II. Mathematical Results

In this section we present mathematical assumptions and
results that are the foundation for bounding the alert rate of
an anomaly detector. To proceed, we consider probabilistic
anomaly detectors, which score data’s anomalousness

according to a probability distribution describing the data.
We leverage the probabilistic description to provide a
theorem that gives sharp bounds on the alert rate in terms
of the threshold, independent of the distribution. This gives
an a priori method to manage the expected number of alerts
for any distribution. As the mathematics is presented with
the necessary but possibly abstruse formality and rigor,
we include easy-to-understand examples illustrating the
results, their implications, and limitations.

A. Setting and Notation

Our setting assumes data is sampled independently from
a distribution with probability density function (PDF)
𝑓 : 𝑋 → [0, ∞). The ambient space, (𝑋, M, 𝑚), is assumed
to be a 𝜎-ﬁnite measure space with measure 𝑚 and M the
set of measurable subsets of 𝑋. We deﬁne the probability
of a measurable set 𝑆 ∈ M to be 𝑃𝑓 (𝑆) := ∫︀
𝑆 𝑓 𝑑𝑚; i.e.,
𝑃𝑓 denotes the corresponding probability measure with
Radon-Nikodym derivative 𝑓 . For almost all applications,
𝑋 is either a subset of R𝑛 with 𝑚 Lebesgue measure, or
𝑋 is a discrete space with 𝑚 counting measure.

We say an anomaly score 𝐴(𝑥) respects the distribution
𝑓 if 𝐴(𝑥) ≥ 𝐴(𝑦) if and only if 𝑓 (𝑥) ≤ 𝑓 (𝑦)—intuitively, 𝑥
is more anomalous than 𝑦 if and only if 𝑥 is less likely
than 𝑦. While this can be attained by simply letting
𝐴 = ℎ ∘ 𝑓 for a decreasing function ℎ (for example, see
Tandon and Chan [15] where 𝐴 := − log2(𝑓 ) ), the work
of Ferragut et al. [5] notes that such an anomaly score
inhibits comparability across detectors. That is, because
such a deﬁnition puts 𝐴 in one-to-one correspondence with
the values of 𝑓 , anomaly scores can vary wildly for diﬀerent
distributions. The consequence is that setting a threshold
is dependent on the distribution, which is problematic
especially for two settings that are common. The ﬁrst is a
setting where multiple detectors (e.g., detectors for network
traﬃc velocity, IP-distribution, etc.) are used in tandem as
each require diﬀerent models. For instances in the literature
using cooperative detectors see [3–7, 14, 16, 18, 19]. The
second setting is when dynamic models (where 𝑓 is
updated upon new observations) are used, as this requires
comparison of anomaly scores over time. For examples of
streaming detection scenarios see [1, 5, 8, 20, 21].

To circumvent this problem, we follow Ferragut et al. [5]
by assuming observations are sampled independently from
𝑓 , a PDF, and deﬁne anomalies as events with low p-value
(as do many detection capabilities). For any distribution
the p-value gives the likelihood relative to the distribution.
Hence, it always takes values in [0, 1], and in the speciﬁc
case of a univariate Gaussian is just the two-sided z-score.

Deﬁnition II.1 (P-Value). The p-value of 𝑥 ∈ 𝑋 with
respect to distribution 𝑓 , is denoted 𝑝𝑣𝑓 : 𝑋 → [0, 1] and
is deﬁned as

∫︁

𝑝𝑣𝑓 (𝑥) :=

{𝑡:𝑓 (𝑡)≤𝑓 (𝑥)}

𝑓 𝑑𝑚 = 𝑃𝑓 ({𝑡 : 𝑓 (𝑡) ≤ 𝑓 (𝑥)}).

3

It is clear from the deﬁnition that 𝑝𝑣𝑓 (𝑥) > 𝑝𝑣𝑓 (𝑦) if
and only if 𝑥 is more likely than 𝑦, since 𝑓 ≥ 0. Finally,
in order to deﬁne an anomaly score, simply compose a
decreasing function, say, ℎ, with the p-value.

Deﬁnition II.2 (Anomaly Score). An anomaly score that
respects a distribution 𝑓 is of the form 𝐴 = ℎ ∘ 𝑝𝑣𝑓 (𝑥), for
strictly decreasing ℎ : [0, 1] → [0, 1].

Since ℎ can be any strictly decreasing function, ℎ(𝑥) =
(1 − 𝑥) is a natural choice for simply inverting [0,1] so that
low p-values (anomalies) get high scores and conversely.
For the theorems that follow, we use both the p-value
threshold denoted by 𝛽, and the corresponding anomaly
score threshold is simply 𝛼 := ℎ(𝛽).

B. Theorems

In this section we present the mathematical results
that make precise the relationship between an anomaly
threshold and the likelihood of an alert. Theorem II.4 and
ensuing corollaries give sharp estimates for bounding the
alert rate in terms of the threshold.

Lemma II.3. Let 𝑓 denote a probability distribution. For
all 𝛽 ≥ 0,

𝑃𝑓 ({𝑥 : 𝑝𝑣𝑓 (𝑥) ≤ 𝛽}) ≤ 𝛽
𝑃𝑓 ({𝑥 : 𝑝𝑣𝑓 (𝑥) > 𝛽}) ≥ 1 − 𝛽.

Furthermore, equality holds in both if and only if 𝛽 =
sup{𝑥 ∈ 𝑋 : 𝑝𝑣𝑓 ≤ 𝛽}.
Proof. Suppose for the moment there exists 𝑦 ∈ 𝑋 such
that 𝑝𝑣𝑓 (𝑦) = 𝛽. Then

{𝑝𝑣𝑓 ≤ 𝛽} = {𝑥 : 𝑝𝑣𝑓 (𝑥) ≤ 𝑝𝑣𝑓 (𝑦)}

= {𝑥 : 𝑓 (𝑥) ≤ 𝑓 (𝑦)}.

It follows that

𝑃𝑓 ({𝑥 : 𝑝𝑣𝑓 (𝑥) ≤ 𝛽}) = 𝑃𝑓 ({𝑥 : 𝑓 (𝑥) ≤ 𝑓 (𝑦)})

Roughly speaking, the lemma says that if we sample 𝑥
from distribution 𝑓 , and compute its p-values, 𝑝𝑣𝑓 (𝑥), the
chance that the 𝑝𝑣𝑓 (𝑥) is less than a ﬁxed number 𝛽 is less
than or equal to 𝛽. The next theorem translates this to
the AD setting.

Theorem II.4. [Alert Rate Regulation Theorem] Let ℎ
be strictly decreasing so that 𝐴𝑓 = ℎ ∘ 𝑝𝑣𝑓 is an anomaly
score that respects the distribution 𝑓 . Let 𝛼 denote the alert
threshold (so 𝑥 is called “anomalous” iﬀ 𝐴𝑓 (𝑥) ≥ 𝛼), and
set 𝛽 = ℎ−1(𝛼). Let 𝑆 ⊂ 𝑋 be a set of independent samples
from PDF 𝑓 . Then the expected number of alerts in 𝑆 is
bounded above by 𝛽|𝑆|, i.e.,

𝐸[{𝑥 ∈ 𝑆 : 𝐴𝑓 (𝑥) ≥ 𝛼}] ≤ 𝛽|𝑆|.

Proof. By deﬁnition of 𝐴𝑓 and 𝛽, we have

𝐸[{𝑥 ∈ 𝑆 : 𝐴𝑓 (𝑥) ≥ 𝛼}] =

=

∑︁

𝑥∈𝑆
∑︁

𝑃𝑓 ({𝐴𝑓 (𝑥) ≥ 𝛼})

𝑃𝑓 ({𝑝𝑣𝑓 (𝑥) ≤ 𝛽})

𝑥∈𝑆
≤ 𝛽|𝑆|,

with the last inequality provided by the Lemma.

Corollary II.5. If 𝑝𝑣𝑓 : 𝑋 → [0, 1] is surjective, then
equality holds in the preceding theorem, lemma for all 𝛽.

Corollary II.6. If 𝑋 is a connected topological space, 𝑓
is not the uniform distribution, and 𝑝𝑣𝑓 is continuous, then
𝑝𝑣𝑓 (𝑋) = [𝑎, 1] for some 𝑎 ∈ [0, 1]; hence, equality holds in
the preceding theorem and lemma for all 𝛽 ∈ [𝑎, 1).

Corollary II.7. Suppose 𝑋 is a topological space and, for
all 𝑦 > 0, 𝑚{𝑥 : 𝑓 (𝑥) = 𝑦} = 0. Then equality holds in the
preceding theorem and lemma.

Proof. Let 𝑥0, 𝑥1 ∈ 𝑋 such that 0 ≤ 𝑓 (𝑥0) ≤ 𝑓 (𝑥1). Then

= 𝑝𝑣𝑓 (𝑦) = 𝛽.

(1)

=

𝑓 𝑑𝑚

0 ≤ 𝑝𝑣𝑓 (𝑥1) − 𝑝𝑣𝑓 (𝑥0)

∫︁

Hence we have equality in this case, which shows the
inequalities are sharp, once proven.

To prove the inequality let 𝑟 = sup{𝑥 ∈ 𝑋 : 𝑝𝑣𝑓 ≤ 𝛽}.

There exists 𝑥𝑛 ∈ 𝑋 such that 𝑝𝑣𝑓 (𝑥𝑛) ↗ 𝑟, hence

{𝑥 : 𝑝𝑣𝑓 (𝑥) ≤ 𝛽} =

⋃︁

{𝑥 : 𝑝𝑣𝑓 (𝑥) ≤ 𝑝𝑣𝑓 (𝑥𝑛)}.

Since the sets on the right are a nested, increasing family,
we have

𝑃𝑓 ({𝑥 : 𝑝𝑣𝑓 (𝑥) ≤ 𝛽}) = lim
𝑛
= lim
𝑛
= 𝑟 ≤ 𝛽.

𝑃𝑓 ({𝑥 : 𝑝𝑣𝑓 (𝑥) ≤ 𝑝𝑣𝑓 (𝑥𝑛)})

𝑝𝑣𝑓 (𝑥𝑛)

by (1)

This proves the ﬁrst inequality, and establishes equality iﬀ
𝛽 = 𝑟 = sup{𝑥 ∈ 𝑋 : 𝑝𝑣𝑓 ≤ 𝛽}. Finally, 𝑃𝑓 ({𝑥 : 𝑝𝑣𝑓 (𝑥) >
𝛽}) = 1 − 𝑃𝑓 ({𝑥 : 𝑝𝑣𝑓 (𝑥) ≤ 𝛽}) ≥ 1 − 𝛽.

4

{𝑡:𝑓 (𝑡)∈(𝑓 (𝑥0),𝑓 (𝑥1)]}

≤ ‖𝑓 ‖1𝑚{𝑡 : 𝑓 (𝑡) ∈ (𝑓 (𝑥0), 𝑓 (𝑥1)]}
= 𝑚{𝑡 : 𝑓 (𝑡) ∈ (𝑓 (𝑥0), 𝑓 (𝑥1)]}

since 𝑓 a PDF.

(2)

By hypothesis, for any 𝑥1 ∈ 𝑋,

0 = 𝑚{𝑡 : 𝑓 (𝑡) = 𝑓 (𝑥1)}
⋂︁

=

𝑚{𝑡 : 𝑓 (𝑡) ∈ (𝑓 (𝑥0), 𝑓 (𝑥1)]}

{𝑥0:𝑓 (𝑥0)<𝑓 (𝑥1)}

=

lim
𝑓 (𝑥0)↗𝑓 (𝑥1)

𝑚{𝑡 : 𝑓 (𝑡) ∈ (𝑓 (𝑥0), 𝑓 (𝑥1)]}

by continuity of measures from above. Hence, for all 𝑥1 ∈ 𝑋
and 𝜖 > 0 there exists 𝑥0 ∈ 𝑋 such that 𝑚{𝑡 : 𝑓 (𝑡) ∈
(𝑓 (𝑥0), 𝑓 (𝑥1)]} ≤ 𝜖.

It follows from Inequality (2) that sup{𝑥 ∈ 𝑋 : 𝑝𝑣𝑓 (𝑥) <
𝑝𝑣𝑓 (𝑥1)} = 𝑝𝑣𝑓 (𝑥1). This establishes the condition of
Lemma II.3 for equality with 𝛽 = 𝑝𝑣𝑓 (𝑥1).

C. Examples and Explanations

D. Alert Rate Regulation Algorithm

1

a

simple

depicting

See Figure

trinomial
distribution and corresponding p-value distribution.
This distribution’s plateau forces a discontinuity in
the threshold.
the rate of alerts as a function of
In this distribution the
operator can either yield
exactly none or 1/6th of
all events as the thresh-
old changes from below
to above 𝑝𝑣𝑓 = 1/6. As
the extreme case, consider
the uniform distribution in
which all events are equally
likely/anomalous. With the
uniform distribution, the
operator can yield exactly
none or all events. Note
that the limitation is in-
dependent of the method
for choosing the threshold
and poses a general prob-
lem for AD. This limitation
appears in our experiments
with real data.

Figure 1: Trinomial distribution
with corresponding p-value dist-
ribution. P-value thresholds 1/6,
1/2, and 1 are the only values
for which equality holds in the
theorems. For these threshold val-
ues the expected percentage of
alerts are exactly 1/6, 1/2, 1, and,
moreover, these are the only per-
centages possible; e.g., using p-
value threshold 𝛽 ∈ [0, 1/6) will
yield exactly 0 events and 𝛽 ∈
[1/6, 1/2) yields an expected 1/6
of the events as alerts. This illus-
trates a fundamental limitation
of PDFs with plateaus. Note that
this phenomenon can occur with
continuous 𝑓 as well.

Corollaries II.5, II.6, and
II.7 are crafted to identify
when this limitation is not
present. As a simple ex-
ample, consider the stan-
dard normal distribution,
Figure 2. Regarding the
plot of the corresponding p-
value distribution is easy to see continuity. Since 𝑝𝑣𝑓 (𝑥) =
2𝐹 (−|𝑥|), where 𝐹 is the cumulative distribution function
(CDF), it follows from Corollary II.6 that we have equality
in Theorem II.4 for all 𝛽 ∈ [0, 1]. The same result follows
from Corollary II.7 and the fact that 𝑓 has no plateaus.
Hence, the equality condition for all threshold values
means that one can specify the expected number of alerts;
quite explicitly, if one desires exactly the most anomalous
1/1000th of the data, then simply setting the p-value
threshold to 𝛽 = 0.001 guarantees the result. Using the
contrapositive, we see that if the model 𝑓 admits equality
for some p-value threshold, 𝛽, then an average number of
alerts above/below 𝛽% indicates that the tails of 𝑓 are
too small/big, respectively. Without equality one can only
detect tails that are too thick.

Finally, we note that while these examples are simple
distributions chosen for illustrative purposes, the theorems
hold under the speciﬁed, very general hypotheses. All that
is needed is a known measure 𝑚 for which the probability
measure is absolutely continuous.

5

Under the assumption that data observations are samples
from our distribution, we are mathematically equipped
to design an algorithm that exploits the relationship
between the alert rate and the threshold to prevent an
overproduction of alerts. To illustrate this, suppose we
receive 𝑁 data points per time interval 𝛿𝑡 (e.g., per
minute), but operators only have resources to inspect
the most anomalous 𝑀 ≤ 𝑁 in each time interval.
Let 𝑓𝑛 be a PDF ﬁt
to all previous observa-
tions {𝑥1, ..., 𝑥𝑛−1}. Follow-
ing the assumption that the
next observation, 𝑥𝑛, will
be sampled according to 𝑓𝑛,
deﬁne the anomaly score,
𝐴𝑛 := ℎ ∘ 𝑝𝑣𝑓𝑛 where ℎ is a
ﬁxed, strictly decreasing bi-
jection of the unit interval.
Upon receipt of 𝑥𝑛 an alert
is issued if 𝐴𝑛(𝑥𝑛) ≥ 𝛼.
Figure 2: The standard normal
distribution PDF is depicted with
Equivalently, if 𝑝𝑣𝑓𝑛 (𝑥𝑛) ≤
corresponding p-value distribution.
ℎ−1(𝛼) =: 𝛽. Finally, we
In this case, the p-value is contin-
update 𝑓𝑛 to 𝑓𝑛+1, now in-
uous, and the issue faced by the
aforementioned trinomial distribu-
cluding observation 𝑥𝑛 and
tion is avoided. Operationally, this
repeat the cycle upon re-
means for any speciﬁed percent 𝑝,
ceipt of the next observa-
a threshold can be set to isolate
the most anomalous 𝑝% of the
tion. Leveraging the the-
distribution.
orem above, the expected
number of alerts per interval is ∑︀𝑁 −1
𝑛=0 𝑃𝑓𝑛 (𝐴𝑛(𝑥𝑛) ≥ 𝛼) ≤
𝑁 ℎ−1(𝛼). Hence, choosing 𝛼 = ℎ(𝑀/𝑁 ) (equivalently,
ﬂagging if the p-value is below 𝛽 = 𝑀/𝑁 ) ensures that
the operator’s bound on the number of alerts will hold on
average.

The method above is for a ﬁxed time interval, or for
constant rate data. As the speed of the data may vary,
we now adapt the above method to dynamically change
the alert rate to accommodate variable data speed. Let 𝑡𝑖
denote arrival time of 𝑥𝑖, and let 𝑟 (alerts per second) be the
user-desired upper bound on the alert rate (the analogue on
𝑀 ). Next, for each time interval we periodically estimate
the rate of data by letting 𝑟𝑘 = |{𝑥𝑖 : 𝑡𝑖 ∈ [(𝑘 − 1) * 𝛿𝑡, 𝑘 *
𝛿𝑡)}|/𝛿𝑡, so that 𝑟𝑘 gives the number of observed events
over the 𝑘th 𝛿𝑡-length interval. Hence 𝑟𝑘 is a periodically
computed, moving average data speed. Alternatively, one
could compute 𝑟𝑛 on each data point’s wait time, i.e.,
𝑟𝑛 := 1/(𝑡𝑛 − 𝑡𝑛−1), which could experience much more
variance. Finally, the new threshold at time 𝑛 shall be
given by 𝛼𝑘 = ℎ(𝑟/𝑟𝑘) for the 𝑘th interval, or equivalently,
the p-value threshold is 𝛽𝑘 = 𝑟/𝑟𝑘. This new threshold
is used for classifying 𝑥𝑛 as soon as 𝑥𝑛 is observed. This
algorithm can be called at each iteration of a streaming
algorithm to regulate the alert rate.

Note that this choice of threshold depends only on the
rate of the data (𝑁 or 𝑟𝑘), and the operator’s bound on the

alert rate (𝑀 or 𝑟). In particular, it is independent of the
distribution, and therefore can be set a priori, regardless of
the distribution. This is especially applicable in a streaming
setting, where 𝑓𝑛 is constantly changing. Next, this is
not a hard bound on the number of alerts per day, but
rather bounds the alert rate in expectation. The operational
impact is that if there is an inﬂux of anomalous data, all
will indeed be ﬂagged, but on average the alert rate will
be bounded as desired. Consequently, it is possible to have
greater than 𝑀 alerts in some time intervals. Finally, if
one has the luxury of performing post-hoc analysis, such
an algorithm is not needed; for example, one can prioritize
a previous day’s alerts by anomaly score, handling as many
as possible. Yet, if real time analysis is necessary, e.g., only
a fraction of the alerts can be processed, stored, etc., then
such an algorithm allows real time prioritization of alerts
with the bound preset.

The underlying assumption is that data is sampled from
the model’s probability distribution, so if this assumption
fails, for example, if there is a change of state of the
system and/or if 𝑓𝑛 poorly describes the observations
to come, then these bounds may cease to hold. This
failure gives an ancillary beneﬁt of the mathematical
machinery above, namely, that monitoring the actual versus
expected alert rate gives a quantitative measure of how
well the distribution ﬁts the data. Our work to characterize
conditions that ensure equality (Corollaries II.5, II.6, and
II.7) serve this purpose. Under these conditions, the alert
rate bound is an actual equality, and deviations from the
expected number of alerts over time (both below and above)
indicate a poor model for the data. On the other hand,
when the bound is strict (and equality does not hold), only
an on-average over-production of alerts will signal a model
that does not ﬁt the data. Our experiments on real data
illustrate this phenomenon as well.

Finally, we note that the adaptive threshold, 𝛽 = 𝑟/𝑟𝑘
with 𝑟𝑘 the data rate, is conveniently self tuning for variable
speed data, it induces a vulnerability. Quite simply, an all-
knowing adversary with the capability to increase 𝑟𝑘 at
the time of attack, can force the threshold to zero, to
mask otherwise alerted events. On the other hand, this
is easily parried with a simply ﬁxed-rate detector on the
data rate, i.e., modeling the statistic 𝑟𝑘, (e.g., a denial of
service ﬂooding detector). Note that as 𝑟𝑘 is computed
each interval, the proposed workaround is a ﬁxed rate
detector; hence, the alert rate can be regulated without
the vulnerability induced.

E. Impact on Detection Accuracy

For high volume situations, the adaptive threshold will
reduce the number of alerts during inﬂuxes of data. Conse-
quently, the true/false positive rates (deﬁned, respectively,
as the percentage of positives/negatives that are alerts,
and hereafter TPR, FPR) will drop, as the number of
alerts (numerator) will be reduced with ﬁxed denominator.
The eﬀect on the positive predictive value (PPV) also

known as precision (deﬁned as the percent of alerts that are
positives), will depend on the distribution of anomaly scores
to the positive events in the data set. In particular, if this
distribution is uniform, then precision will be unaﬀected.
In this case we note that our theorems give a sharp bound
(and ability to regulate) the false detection rate (1-PPV).

III. Empirical Experiments

We present experiments testing the ﬁxed-rate and
streaming threshold algorithms on two data sets, Skaion
(synthetic) and GraphPrints (real) ﬂow data.

A. Skaion Data & Detection System

To test the alert rate algorithm we implemented a
streaming AD system on the network ﬂow data from
the Skaion data set. See Acknowledgments IV for details
on Skaion data source information. This data was, to
quote the dataset documentation, “generated by capturing
information from a synthetic environment, where benign
user activity and malicious attacks are emulated by com-
puter programs.” There is a single PCAP ﬁle for benign
background traﬃc and a PCAP ﬁle for each of nine
attack scenarios. We utilized ARGUS5 (the Audit Record
Generation and Utilization System), an open source, real-
time, network ﬂow monitor, for converting the PCAP
information into network ﬂow data. As the diﬀerent PCAP
ﬁles had mutually disjoint time intervals, we created a
single, continuous set of ﬂows by oﬀsetting the timestamps
from the 5s20 (Skaion label) attack scenario ﬂows to
correspond with a portion of the “background” (i.e., non-
attack, 5b5 Skaion label) ﬂows, and then shuﬄing the
ambient and attack data together so they are sorted by
time.6

All together our test data set has 681,220 background
traﬃc ﬂows spanning ﬁve hours 37 minutes (337 minutes)
with 227,962 ﬂows from the attack PCAP ﬁle included
from the 227th minute onward. The attack PCAP ﬁle
includes approximately 20 minutes of data before the
attacker initiates the attack. This data set includes 6,905
IPs of which 1,246 are internal IPs (100.*.*.*). To put this
in perspective, Section III-B uses ﬂows from a real network
of (only) ≈50 researchers and created twice the Skaion ﬂow
volume in half the time. Skaion data is relatively small.

We implement a dynamic ﬂeet of detectors roughly based
on the Ferragut et al. [23] patent and currently used in
operation. Speciﬁcally, for each internal IP we implement
two detectors. The ﬁrst models the previously observed
inbound and outbound private ports, numbered 1-2048
(1-1024 for outbound, 1025-2048 for inbound traﬃc), using
a 2048-bin multinomial, and follows the recent publication

5http://www.qosient.com/
6 The 5s20 attack scenario, titled “Multiple Stepping Stones,”
begins with the attacker scanning internet-facing systems, gaining
access to one of them using an OpenSSL exploit, then leveraging this
to gain access to several systems behind the ﬁrewall. The attacker’s
initial scan of the internet-facing systems is not subtle, and therefore,
produces a large spike in the AD system.

6

of Huﬀer & Reed [25] where it is shown that the role of
a host can be characterized by the use of private ports
in ﬂow data. The second models the producer-consumer
ratio (PCR), which is deﬁned as (source bytes - destination
bytes)/(source bytes + destination bytes). Hence, PCR is
a metric describing the ratio, data in : data out, per ﬂow
and takes a value in the interval [-1,1]. This is modeled by a
10-bin multinomial. Initially, all bins (in both models) are
given a lone count; notationally, with 𝑘 bins (𝑘 = 2048 or
10) 𝑓0(𝑖) = 1/𝑘 for all 𝑖 = 1, 2, .., 𝑘. Upon receipt of the 𝑛th
observation, the p-value is computed; 𝑝𝑣𝑓𝑛 (𝑥𝑛) = ∑︀ 𝑓𝑛(𝑖),
with sum over {𝑖 ∈ 1, ..., 𝑘 : 𝑓𝑛(𝑖) ≤ 𝑓𝑛(𝑥𝑛)}. Finally, the
model is updated from 𝑓𝑛 to 𝑓𝑛+1 by simply incrementing
the count of the bin observed and the denominator. Math-
ematically, this is the maximum a posteriori multinomial
given the previous observations and a uniform prior.

Altogether, the system has 1,246 IPs × 2 detectors/IP
= 2,492 dynamic detectors and produces 1,565,596
anomaly scores (p-values) in the 337 minutes of data.
Our goal in designing these detectors was to create a
realistic
detection
capability,
and models
chosen
were
the
at
advice
of
professional
cyber security
analysts.
Although the
focus of this
paper is not
on pioneering
accurate AD, we quickly note that the current detectors
are able to easily recognize that the initial port scan
activity of the attack is anomalous (Fig. 3, min. 247).
Deeper investigation of when and how well these models
are eﬀective is out of scope for this eﬀort.

Figure 4: Rate of Skaion ﬂow data, 𝑟𝑘, (anomaly
scores per minute) plotted. Adaptive threshold
(not depicted) has inverse relationship, 𝑟/𝑟𝑘, with
𝑟 the user-given alert rate bound, (𝑟 = 1 in
experiments).

1) Skaion Data Results: We test our alert rate threshold
analysis against more traditional thresholds. Throughout
our discussion, reference Figure 3 giving the number of
alerts per minute with various thresholds. Some previous
works both within intrusion detection [26–29] and in other
domains [30] using p-value thresholds follow the “3𝜎 rule-
of-thumb” and let 0.3%= .003 serve as the threshold. This
follows from the fact that the 3𝜎-tails of a normal distribu-
tion have p-value 0.0026 or approximately 0.003. Others
simply choose a value that is near 2% for an undisclosed
reason, or because of a true positive versus false positive
analysis in light of labeled events. [8, 10, 31–34]. While such
values may be appropriate for hypothesis testing in many
situations, for AD on high volume data this unprincipled
manner of setting the threshold is inadequate. Testing these
approaches, we ﬁnd p-value threshold 𝛽 = 0.02 produces
an average of ≈ 50 alerts per minute, while 𝛽 = 0.003

7

produces on average ≈ 14 alerts per minute (top two rows
of Figure 3). The clear conclusion is that the operator will
be overwhelmed by alerts with these thresholds.

We contrast this with the a priori analysis furnished
by our theorem. Suppose ﬁrst that our operators can
realistically consider 𝑀 = 1 alert per minute on av-
erage, and that they know 𝑁 ≈ 1.5m scores will be
produced. With these two ﬁgures, we use the theorem
to compute the static p-value threshold as follows: 1 =
𝐸(alerts per minute) = (1, 565, 596/337) * 𝑃 (𝑝𝑣𝑓 (𝑋) ≤
𝛽) ≤ (1, 565, 596/337) * 𝛽. Hence, we set 𝛽 = 𝑀/𝑁 =
1/(1, 565, 596/337) = 0.0002153. This simple calculation
shows that the ad-hoc p-value thresholds of 0.003 and 0.02
are one and two orders of magnitude too large, respectively.
Moreover, testing the ﬁxed threshold 𝛽 = 0.0002153 yields
≈ 0.64 alerts per minute. See the bottom row of Figure 3.
This shows the eﬃcacy of the alert rate theorem even for
ﬁxed thresholds on variable speed data.

Next, we remove the assumption that the operator
knows the number of events (𝑁 ), and test the dynamically
changing threshold with data rate 𝑟𝑘 recomputed each
minute. To do so, we again let the user-deﬁned bound on
the average number of alerts be 𝑟 = 1 (following notation
of the last Section). Each minute we compute the moving
average of the data, 𝑟𝑘, using the previous minute of
data and set 𝛽𝑘 = 𝑟/𝑟𝑘 = 1/𝑟𝑘; i.e., 𝑟𝑘 is deﬁned as in
Section II-D with 𝛿𝑡 = 1 minute. See Figure 4 depicting 𝑟𝑘.
Consulting Figure 3, we see the adaptive threshold yields
about 0-4 alerts per minute except for the one large spike
induced by the exceptionally anomalous attack activity. On
average, the adaptive threshold produces 0.43 alerts per
minute, slightly less than the ﬁxed but informed threshold
(𝛽 = 0.0002152). Finally, consulting the the bottom-row
plots of Figure 3 shows that the distribution of alerts is
more spread out with the adaptive threshold. Overall, the
streaming alert rate regulation algorithm is eﬀective in
regulating the nearly 2,500 adaptive detectors with no a
priori information.
Lastly, we present
the accuracy met-
rics for each thresh-
old in the wrapped
table. As expected,
both TPR and FPR both drop when using the adaptive
threshold as a result of an overall decrease in alerts.
Precision (PPV) is signiﬁcantly lower in the adaptive case.
To explain this, we regard Figure 4 to see that the data
rate nearly doubles on average during the attack times.
This is caused by the creation of the data set, combining
the non-attack Skaion ﬁles with the attack Skaion ﬁles.
With the adaptive threshold, the number of alerts will
roughly halve during the times when positive examples
are included; hence, the drop in precision is an artifact of
the simulation. In this speciﬁc application the adaptive
threshold still provides nearly 50 alerts at the onset of the
attack. More generally, the eﬀect of our threshold algorithm

.003
2.152e-4
Adaptive

.0015
3.48e-5
5.33e-5

.7262
.8598
.3194

.0763
.0040
.0021

Thresh. TPR

PPV

FPR

Figure 3: All subplots depict number of alerts per minute (𝑌 -axis) against time (𝑋-axis) for the adaptive threshold (blue curve in all
plots) against a ﬁxed p-value threshold, top row: yellow curve is 𝛽 = 0.02, middle row: red curve is 𝛽 = 0.003, bottom row: green curve is
𝛽 = 0.0002152. Left column shows the whole 337 minute dataset; right column is zoomed in to show better resolution. Total, the system
has 1,246 IPs × 2 detectors/IP = 2,492 dynamic detectors and produces 1,565,596 anomaly scores (p-values). The spike in minute 247
is caused by the attacker initiating a series of unsubtle port scans against internet-facing systems, causing an increased quantity of alerts.
Fixed thresholds 𝛽 = 0.02 and 0.003 averaging ≈ 50 and ≈ 14 alerts per minute are used as baselines for comparison. The ﬁxed threshold
𝛽 = 0.0002152 is computed using our theorem to satisfy the bound of 1 alert per minute on average and averages 0.64 alerts per minute.
Considering the ﬁxed thresholds only, we conclude that improper choice of the threshold will easily ﬂood the analyst with alerts, but that
using a ﬁxed threshold informed by our mathematics will prevent overproduction. The adaptive threshold produces 0.43 alerts per minute on
average. Considering the bottom row only, we conclude that the adaptive threshold produces less overall alerts than the ﬁxed threshold with
the same alert-rate bound, and spreads the alerts more evenly.

8

on precision will depend on the distribution of anomaly
scores to the attack events.

B. GraphPrints Data & Detection System

We now present experiments of the alert rate develop-
ments on the data and network-level anomaly detector
(GraphPrints) from the publication of Harshaw et al. [8],
which was shared with us by the authors. In this publica-
tion, they chose the tightest threshold that still detects all
known positives in the data. We will show how our analysis
can inform the understanding of the AD model.

Their work used 175 minutes of network ﬂow data
collected from a small oﬃce building at our organization,
also using ARGUS ﬂow sensor. It included ﬂows from
approximately 50 researchers using 642 internal IPs plus
another ∼2,500 internal, reserved IPS (10.*.*.* addresses),
and was comprised of 1,725,150 ﬂows. The data contained
anomalous bittorrent traﬃc and occasional IP scanning
traﬃc.

The AD method proposed by Harshaw et al., called
GraphPrints, uses graph analytics and robust statis-
tics to identify anomalies in the local topology of
network traﬃc. The method proceeds in three steps.
First, a graph is created
for each 31 second in-
terval of network traﬃc
(with one second overlap)
by representing IPs by
nodes and adding a di-
rected edge from source
to destination IP nodes
of at least one such ﬂow
occurred. Moreover, edges
Figure 5: GraphPrints data’s actual
are given a binary color in-
and expected number of alerts for
small p-values. Because a Gaussian
dicating if at least one pri-
distribution was used, the corollar-
vate port occurred in the
ies indicate the two curves should be
ﬂow. Second, the graph
approximately equal. The extreme
disparity in actual vs. expected
is converted to a graphlet
alerts for small p-values indicates
count vector, where each
that the data is sampled from a
component of the vector is
distribution with much thicker tails
than the Gaussian used for the AD
the count of an observed
model. This results in an inability
graphlet, which are small,
to regulate the number of alerts, in-
node-induced subgraphs.
dicating problems with their model.
Conceptually, each graphlet can be thought of as a building
block graph, and the count vector gives the sums of each
such small graph observed in the network traﬃc graph.7
Third, AD is performed by ﬁtting a multivariate Gaussian
to the history of observed vectors, and then, given a
new vector, alerting if its p-value is below a threshold.8

7See Prvzulj et al. [35] for pioneering work on graphlets.
8We note that Harshaw et al. detected events with suﬃciently high
Mahalanobis distance. It is easy to show that Mahalanobis distance is
an anomaly score that respects the Gaussian distribution in the sense
of Defn. II.2 by proving 𝑝𝑣𝑘(𝑥0) = 1 − 𝐺𝑘(𝑚(𝑥)2), where 𝑝𝑣𝑘 denotes
the p-value of the 𝑘−variate normal distribution, 𝑚 the Mahalanobis
distance, and 𝐺𝑘 is the cumulative density of the 𝜒2 random variable
with 𝑘 degrees of freedom.

Harshaw et al. implemented this method as a streaming
detection algorithm, initially ﬁtting the Gaussian to the
ﬁrst 150 (of 350) data points, and iteratively scoring,
then reﬁtting the Gaussian to each of the subsequent
200 events. In order to prevent unknown attacks or other
anomalies in the data from aﬀecting their model, Harshaw
et al. used the Minimum Covariance Determinant (MCD)
algorithm with ℎ = 0.85. This isolates the most anomalous
15% = (1 − ℎ) * 100% of the data and ﬁts the Gaussian to
the remaining ℎ = 85%.9

C. GraphPrints Data Results

First note that because the AD is based on low p-
values of a Gaussian distribution, which has no plateaus,
Corollaries II.5,II.6, and II.7 all independently imply that
equality holds in Theorem II.4. Operationally, this means
that users can specify the expected number of alerts (not
just bound them), provided events are sampled from the
model’s distribution. Testing this for a simple example
shows alert rate regulation fails; for example, p-value
threshold 𝛽 = 0.01 = 2/200 corresponds to an expected two
alerts in the data, but produces over 60! This indicates that
our detection model is a bad ﬁt to the data. Digging deeper,
Figure 5 shows that for all small p-values the realized
alerts far exceed the expected, violating the theorem.
We can conclude that the data’s distribution has much
thicker tails than the Gaussian used for detection—in
short, the detection model is not a good ﬁt to the data.
This is perhaps unsurprising recalling the use of MCD
ﬁtting, which eﬀectively discarded the 15% most outlying
observations before computing the mean and covariance.
The result above illustrates a tradeoﬀ aﬀorded by the
mathematical framework—either accurate regulation of
the alert rate is possible, or the bound/equality on the
expected number of alerts is not obeyed but information
on the ﬁtness (or lack thereof) of the distribution is gained.

IV. Conclusion

In this work we consider the problem of setting the thresh-
old of multiple heterogeneous and/or streaming anomaly
detectors. By assuming probability models of our data and
deﬁning anomalies as low p-value events, we prove theorems
for bounding the likelihood of an anomaly. Leveraging
the mathematical foundation, we give and test algorithms
for setting the threshold to limit the number of alerts of
such a system. Our algorithmic developments rely on the
underlying assumption that observations are sampled from
the model distribution. As the theorems hold independently
of the distribution, our threshold-setting method persist
as models evolve online or for a heterogeneous collection
of models (so long as the assumption holds). Using the
Skaion synthetic network ﬂow data, we implement an
AD system of ≈ 2, 500 adaptive detectors that scores
over 1.5m events in 5 hours, and show empirically how

9See Rousseeuw [36] for algorithmic details.

9

to set the threshold and regulate the number of alerts.
The mathematical contrapositive of our main theorem
operationally provides the user with an alternative—either
the alert rate regulation is possible or the detector’s model
is a bad ﬁt for the data. We demonstrate the use of this
analytical insight by implementing the threshold algorithm
on the real network data of Harshaw et al. and proving
that their data is sampled from a distribution with much
thicker tails than their detection model’s. In summary, our
work provides a mathematical foundation and empirically
veriﬁed method for conﬁguring anomaly detector thresholds
to accommodate the hardships necessitated by modern
cyber security operations.

Acknowledgements

Thank you J. Laska, V. Protopopescu, M. McClelland,
L. Nichols, J. Gerber, and reviewers whose comments
helped polish this document. This material is based on
research sponsored by the U.S. Department of Homeland
Security (DHS) under Grant Award Number 2009-ST-
061-CI0001, DHS VACCINE Center under Award 2009-
ST-061-CI0003, and Laboratory Directed Research and
Development Program of Oak Ridge National Laboratory,
managed by UT-Battelle, LLC, for the U. S. Department
of Energy, contract DE-AC05-00OR22725. The views
and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing
the oﬃcial policies or endorsements, either expressed or
implied, of the DHS. This material is based upon work
supported by the National Science Foundation Graduate
Research Fellowship Program under Grant No. 25-0517-
0143-002. Any opinions, ﬁndings, and conclusions or
recommendations expressed in this material are those of
the authors and do not necessarily reﬂect the views of
the National Science Foundation. The data used in this
research and referenced in this paper was created by Skaion
Corporation with funding from the Intelligence Advanced
Research Project Agency, via www.impactcybertrust.org.

References

[1] T. Ahmed et al., “Multivariate online anomaly de-
tection using kernel recursive least squares,” in 26th
INFOCOM.
IEEE, 2007, pp. 625–633.

[2] S. Axelsson, “The base-rate fallacy and the diﬃculty
of intrusion detection,” ACM Trans. Inf. Syst. Secur.,
vol. 3, no. 3, pp. 186–205, 2000.

[3] M. Christodorescu and S. Rubin, “Can cooperative
intrusion detectors challenge the base-rate fallacy?” in
Malware Detection. Springer, 2007, pp. 193–209.
[4] E. Ferragut et al., “Automatic construction of anomaly
IEEE,

detectors from graphical models,” in CICS.
2011, pp. 9–16.

tion,” in ICMLA, vol. 2.

[5] ——, “A new, principled approach to anomaly detec-
IEEE, 2012, pp. 210–215.
[6] R. Fontugne et al., “Mawilab: Combining diverse
anomaly detectors for automated anomaly label-

ing and performance benchmarking,” ser. Co-NEXT.
ACM, 2010.

[7] S. Garcia et al., “An empirical comparison of botnet
detection methods,” Comp. & Sec., vol. 45, 2014.
[8] C. Harshaw et al., “Graphprints: Towards a graph
analytic method for network anomaly detection,” in
11th CISRC. ACM, 2016, pp. 15–19.

[9] A. Lakhina et al., “Diagnosing network-wide traﬃc
anomalies,” SIGCOMM, vol. 34, no. 4, pp. 219–230,
Aug. 2004.

[10] M. Moore et al., “Modeling inter-signal arrival times
for accurate detection of can bus signal injection
attacks,” in 12th CISRC. ACM, 2017.

[11] T. Pevn`y et al., “Identifying suspicious users in
corporate networks,” in Proc. Info. Forens. Sec., 2012.
[12] M. Rehak et al., “Adaptive multiagent system for
network traﬃc monitoring,” Intel. Sys., vol. 24, 2009.
[13] K. Scarfone and P. Mell, “Guide to intrusion detection
and prevention systems,” NIST, vol. 800, 2007.
[14] J. Sexton et al., “Attack chain detection,” J. SADM,

vol. 8, no. 5-6, pp. 353–363, 2015.

[15] G. Tandon and P. Chan, “Tracking user mobility to
detect suspicious behavior.” in SDM. SIAM, 2009,
pp. 871–882.

[16] A. Thomas, “Rapid: Reputation based approach for
improving intrusion detection eﬀectiveness,” in IAS.
IEEE, 2010, pp. 118–124.

[17] C. Joslyn et al., “Discrete mathematical approaches
to graph-based traﬃc analysis,” in ECSaR, 2014.
[18] H. Kriegel et al., “Interpreting and unifying outlier

scores,” in SDM. SIAM, 2011, pp. 13–24.

[19] E. Schubert et al., “On evaluation of outlier rankings
and outlier scores,” in SDM, 2012, pp. 1047–1058.
[20] R. Bridges et al., “Multi-level anomaly detection on
time-varying graph data,” in ASONAM’15. New York,
NY, USA: ACM, 2015, pp. 579–583.

[21] ——, “A multi-level anomaly detection algorithm for
time-varying graph data with interactive visualization,”
Soc. Netw. Anal. & Mining, vol. 6, no. 1, p. 99, 2016.
[22] E. Ferragut et al., “Detection of anomalous events,”

Jun. 7 2016, US Patent 9,361,463.

[23] ——, “Real-time detection and classiﬁcation of anoma-
lous events in streaming data,” Apr. 19 2016, US
Patent 9,319,421.

[24] A. Siﬀer et al., “Anomaly detection in streams with
extreme value theory,” in 23rd ACM SIGKDD, 2017.
[25] K. Huﬀer and J. Reed, “Situational awareness of
network system roles (SANSR),” in 12th CISRC.
ACM, 2017.

[26] R. Bhaumik et al., Securing collaborative ﬁltering
against malicious attacks through anomaly detection.
AAAI, 2006, vol. WS-06-10, pp. 50–59.

[27] J. Cucurull et al., Anomaly Detection and Mitigation
Berlin, Heidelberg:

for Disaster Area Networks.
Springer Berlin Heidelberg, 2010, pp. 339–359.
[28] N. Ye et al, “Multivariate statistical analysis of

10

audit trails for host-based intrusion detection,” Trans.
Comp., vol. 51, no. 7, pp. 810–820, 2002.

[29] N. Ye and Q. Chen, “An anomaly detection technique
based on a chi-square statistic for detecting intrusions
into information systems,” Qual. Reli. Eng. Int., vol. 17,
no. 2, pp. 105–112, 2001.

[30] R. Patera, “Space event detection method,” J. Space.

and Rock., vol. 45, no. 3, pp. 554–559, 2008.

[31] J. Buzen and A. Shum, “MASF - multivariate adaptive
statistical ﬁltering.” in Int. CMG Conference. Comp.
Meas. Gr., 1995, pp. 1–10.

[32] R. Campello et al., “Hierarchical density estimates for
data clustering, visualization, and outlier detection,”
TKDD, vol. 10, no. 1, p. 5, 2015.

[33] A. Lazarevic et al., A Comparative Study of Anomaly
Detection Schemes in Network Intrusion Detection.
SIAM, 2003, pp. 25–36.

[34] M. Reddy et al., “Probabilistic detection methods for
acoustic surveillance using audio histograms,” Circ.,
Sys., Sig. Proc., vol. 34, no. 6, pp. 1977–1992, 2015.
[35] N. Pržulj et al., “Modeling interactome: scale-free or
geometric?” Bioinform., vol. 20, no. 18, pp. 3508–
3515, 2004.

[36] P. Rousseeuw and K. Driessen, “A fast algorithm
for the minimum covariance determinant estimator,”
Technometrics, vol. 41, no. 3, pp. 212–223, 1999.

11

