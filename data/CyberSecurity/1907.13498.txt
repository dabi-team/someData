9
1
0
2

l
u
J

1
3

]

R
C
.
s
c
[

1
v
8
9
4
3
1
.
7
0
9
1
:
v
i
X
r
a

An Eﬃcient and Scalable Privacy Preserving Algorithm for Big Data
and Data Streams

M.A.P. Chamikaraa,b,∗, P. Bertoka, D. Liub, S. Camtepeb, I. Khalila

aSchool of Science, RMIT University, Australia
bCSIRO Data61, Australia

Abstract

The published article can be found at https://doi.org/10.1016/j.cose.2019.101570

A vast amount of valuable data is produced and is becoming available for analysis as a result

of advancements in smart cyber-physical systems. The data comes from various sources, such as

healthcare, smart homes, smart vehicles, and often includes private, potentially sensitive information

that needs appropriate sanitization before being released for analysis. The incremental and fast nature

of data generation in these systems necessitates scalable privacy-preserving mechanisms with high

privacy and utility. However, privacy preservation often comes at the expense of data utility. We

propose a new data perturbation algorithm, SEAL (Secure and Eﬃcient data perturbation Algorithm

utilizing Local diﬀerential privacy), based on Chebyshev interpolation and Laplacian noise, which

provides a good balance between privacy and utility with high eﬃciency and scalability. Empirical

comparisons with existing privacy-preserving algorithms show that SEAL excels in execution speed,

scalability, accuracy, and attack resistance. SEAL provides ﬂexibility in choosing the best possible

privacy parameters, such as the amount of added noise, which can be tailored to the domain and

dataset.

Keywords: Privacy, privacy-preserving data mining, data streams, smart cyber-physical systems,

Internet of Things (IoT), Web of Things (WoT), sensor data streams, big data.

1. Introduction

Smart cyber-physical systems (SCPS) such as smart vehicles, smart grid, smart healthcare systems,

and smart homes are becoming widely popular due to massive technological advancements in the past

∗Corresponding author
Email address: pathumchamikara.mahawagaarachchige@rmit.edu.au (M.A.P. Chamikara )

Preprint submitted to Journal of Computers & Security

August 1, 2019

 
 
 
 
 
 
few years. These systems often interact with the environment to collect data mainly for analysis,

e.g. to allow life activities to be more intelligent, eﬃcient, and reliable [1]. Such data often includes

sensitive details, but sharing conﬁdential information with third parties can lead to a privacy breach.

From our perspective, privacy can be considered as “Controlled Information Release” [2]. We can

deﬁne a privacy breach as the release of private/conﬁdential information to an untrusted environment.

However, sharing the data with external parties may be necessary for data analysis, such as data mining

and machine learning. Smart cyber-physical systems must have the ability to share information while

limiting the disclosure of private information to third parties. Privacy-preserving data sharing and

privacy-preserving data mining face signiﬁcant challenges because of the size of the data and the speed

at which data are produced. Robust, scalable, and eﬃcient solutions are needed to preserve the privacy

of big data and data streams generated by SCPS [3, 4]. Various solutions for privacy-preserving data

mining (PPDM) have been proposed for data sanitization; they aim to ensure conﬁdentiality and

privacy of data during data mining [5, 6, 7, 8].

The two main approaches of PPDM are data perturbation [9, 10] and encryption [11, 12]. Although

encryption provides a strong notion of security, due to its high computation complexity [13] it can be

impractical for PPDM of SCPS-generated big data and data streams. Data perturbation, on the

other hand, applies certain modiﬁcations such as randomization and noise addition to the original

data to preserve privacy [14]. These modiﬁcation techniques are less complex than cryptographic

mechanisms [15]. Data perturbation mechanisms such as noise addition [16] and randomization [17]

provide eﬃcient solutions towards PPDM. However, the utility of perturbed data cannot be 100%

as data perturbation applies modiﬁcations to the original data, and the ability to infer knowledge

from the perturbed data can result in a certain level of privacy leak as well. A privacy model [18]

describes the limitations to the utility and privacy of a perturbation mechanism. Examples of such

earlier privacy models include k − anonymity [19, 20] and l − diversity [21]. However, it has been

shown that older privacy models are defenseless against certain types of attacks, such as minimality

attacks [22], composition attacks [23] and foreground knowledge [24] attacks. Diﬀerential privacy (DP)

is a privacy model that provides a robust solution to these issues by rendering maximum privacy via

minimizing the chance of private data leak [25, 26, 27, 28]. Nevertheless, current DP mechanisms fail

for small databases and have limitations on implementing eﬃcient solutions for data streams and big

data. When the database is small, the utility of DP mechanisms diminishes due to insuﬃcient data

being available for a reasonable estimation of statistics [29]. At the other end of the scale, when the

2

database is very large or continuously growing like in data streams produced by SCPS, the information

leak of DP mechanisms is high due to the availability of too much information [30]. Most perturbation

mechanisms tend to leak information when the data is high-dimensional, which is a consequence of the

dimensionality curse [31]. Moreover, the signiﬁcant amount of randomization produced by certain DP

algorithms results in low data utility. Existing perturbation mechanisms often ignore the connection

between utility and privacy, even though improvement of one leads to deterioration of the other [32].

Furthermore, the inability to eﬃciently process high volumes of data and data streams makes the

existing methods unsuitable for privacy-preservation in smart cyber-physical systems. New approaches

which can appropriately answer the complexities in privacy preservation of SCPS generated data are

needed.

The main contribution of this paper is a robust and eﬃcient privacy-preserving algorithm for

smart cyber-physical systems, which addresses the issues existing perturbation algorithms have. Our

solution, SEAL (Secure and Eﬃcient data perturbation Algorithm utilizing Local diﬀerential privacy),

employs polynomial interpolation and notions of diﬀerential privacy. SEAL is a linear perturbation

system based on Chebyshev polynomial interpolation, which allows it to work faster than comparable

methods. We used generic datasets retrieved from the UCI data repository1 to evaluate SEAL’s

eﬃciency, scalability, accuracy, and attack resistance. The results indicate that SEAL performs well

at privacy-preserving data classiﬁcation of big data and data streams. SEAL outperforms existing

alternative algorithms in eﬃciency, accuracy, and data privacy, which makes it an excellent solution

for smart system data privacy preservation.

The rest of the paper is organized as follows. Section 2 provides a summary of existing related

work. The fundamentals of the proposed method are brieﬂy discussed in Section 3. Section 4 describes

the technical details of SEAL. Section 5 presents the experimental settings and provides a comparative

analysis of the performance and security of PABIDOT. The results are discussed in Section 6, and the

paper is concluded in Section 7. Detailed descriptions of the underlying concepts of SEAL are given

in the Appendices.

1https://archive.ics.uci.edu/ml/index.php

3

2. Related Work

Smart cyber-physical systems (SCPS) have become an important part of the IT landscape. Often

these systems include IoT devices that allow eﬀective and easy acquisition of data in areas such as

healthcare, smart cities, smart vehicles, and smart homes [1]. Data mining and analysis are among the

primary goals of collecting data from SCPS. The infrastructural extensions of SCPSs have contributed

to the exponential growth in the number of IoT sensors, but security is often overlooked, and the

devices become a source of privacy leak. The security and privacy concerns of big data and data

streams are not entirely new, but require constant attention due to technological advancements of the

environments and the devices used [33]. Conﬁdentiality, authentication, and authorization are just a

few of the concerns [34, 35, 36]. Many studies have raised the importance of privacy and security of

SCPS due to their heavy use of personally identiﬁable information (PII) [37]. Controlling access via

authentication [38], attribute-based encryption [39], temporal and location-based access control [38]

and employing constraint-based protocols [40] are some examples of improving privacy of SCPS.

In this paper, our primary target is maintaining privacy when sharing and mining data produced

by SCPSs, and the focus is on “controlled information release”. Literature shows diﬀerent attempts to

impose constraints on data release and analysis in order to preserve privacy [41]. Data encryption and

data perturbation-based solutions have proven to be more viable for privacy-preserving data publishing

and analysis than methods based on authentication and authorization [42]. Some recent examples for

encryption based privacy-preserving approaches for cloud computing include PPM [43], Sca-PBDA [44]

and TDPP [45], which provide scalable privacy-preserving data processing infrastructures. However,

cryptographic mechanisms are less popular in PPDM for “controlled information release” due to the

high computational complexity, hence not suitable for resource-constrained devices. Perturbing the in-

stances of the original data by introducing noise or randomization is called input perturbation [46, 41],

whereas perturbing the outputs of a query or analysis using noise addition or rule hiding is called out-

put perturbation. Unidimensional perturbation and multidimensional perturbation [46, 47, 48, 49] are

the two main types of input perturbation classes. Examples for unidimensional perturbation include

but are not limited to additive perturbation [16], randomized response [50], and swapping [51]. Con-

densation [52], random rotation [9], geometric perturbation [10], random projection [48], and hybrid

perturbation are types of multidimensional perturbation [41]. Microaggregation[53] can be consid-

ered as a hybrid perturbation mechanism that possesses both unidimensional and multidimensional

perturbation capabilities.

4

As privacy models evolved, the limits of privacy imposed by particular mechanisms were eval-

uated [18], and new models were deﬁned to overcome the issues of their predecessors. For ex-

ample, l − diversity [21] was deﬁned to overcome the shortcomings of k − anonymity [19], then

(α, k) − anonymity [54], t − closeness [55] were proposed as further improvements. However, all these

models eventually exhibited vulnerabilities to diﬀerent attacks such as minimality [22], composition [23]

and foreground knowledge [24] attacks. Moreover, these models were not scalable enough to address

the curse of dimensionality presented by big data and data streams [31, 56], hence resulted in higher

privacy leak [31].

In recent years diﬀerential privacy (DP) has drawn much attention as a power-

ful privacy model due to its inherent strong privacy guarantee. Diﬀerential privacy that is achieved

via output perturbation is known as global diﬀerential privacy (GDP), whereas diﬀerential privacy

achieved using input perturbation is known as local diﬀerential privacy (LDP). Laplacian mechanism

and randomized response are two of the most frequently employed data perturbation methods used

to achieve GDP and LDP [57, 17]. LDP permits full or partial data release allowing the analysis of

the perturbed data [58, 59], while GDP requires a trusted curator who enforces diﬀerential privacy by

applying noise or randomization on results generated by running queries or by analysis of the orig-

inal data [58]. Nevertheless, LDP algorithms are still at a rudimentary stage when it comes to full

or partial data release of real-valued numerical data, and the complexity of selecting the domain of

randomization with respect to a single data instance is still a challenge [60, 61, 29]. Consequently,

existing DP mechanisms are not suitable for diﬀerentially private data release.

Two important characteristics that determine the robustness of a particular perturbation mech-

anism are the ability to (1) protect against data reconstruction attacks and (2) perform well when

high dimensional datasets and data streams are introduced. A data reconstruction attack tries to

re-identify the individual owners of the data by reconstructing the original dataset from the perturbed

dataset. Data reconstruction attacks are normally custom built for diﬀerent perturbation methods

using the properties of the perturbation mechanisms to restore the original data systematically. Dif-

ferent perturbation scenarios are vulnerable to diﬀerent data reconstruction attacks. Principal com-

ponent analysis [62], maximum likelihood estimation [63], known I/O attack [64], ICA attack [65] and

known sample attack [64] are some examples of common data reconstruction attacks. For example,

additive perturbation is vulnerable to principal component analysis [66] and maximum likelihood esti-

mation [66], whereas multiplicative data perturbation methods are vulnerable to known input/output

(I/O) attacks, known sample attacks and ICA attacks. These reconstruction attacks exploit the high

5

information leak due to the dimensionality curse associated with high-dimensional data. In addition

to providing extra information to the attacker, high-dimensional data also exponentially increases the

amount of necessary computations [31, 21, 9, 10].

The literature shows methods that try to provide privacy-preserving solutions for data streams and

big data by addressing the dimensionality curse in data streams. Recent attempts include a method

proposing steered microaggregation to anonymize a data stream to achieve k − anonymity [67], but

the problem of information leak inherent in k − anonymity in case of high dimensional data can be a

shortcoming of the method. Xuyun Zhang et al. [68] introduced a scalable data anonymization with

MapReduce for big data privacy preservation. The proposed method ﬁrst splits an original data set

into partitions that contain similar data records in terms of quasi-identiﬁers and then locally recodes

the data partitions by the proximity-aware agglomerative clustering algorithm in parallel which limits

producing suﬃcient utility for data streams such as produced by SCPS. Further, the requirement

of advanced processing capabilities precludes its application to resource-constrained devices. Data

condensation is another contender for data stream privacy preservation [69]. The problem, in this case,

is that when the method parameters are set to achieve high accuracy (using small spatial locality),

the privacy of the data often suﬀers. P 2RoCAl is a privacy preservation algorithm that provides

high scalability for data streams and big data composed of millions of data records. P 2RoCAl is

based on data condensation and random geometric transformations. Although P 2RoCAl provides

linear complexity for the number of tuples, the computational complexity increases exponentially for

the number of attributes [70]. PABIDOT is a scalable privacy preservation algorithm for big data

[71]. PABIDOT comes with a new privacy model named Φ − separation, which facilitates full data

release with optimum privacy protection. PABIDOT can process millions of records eﬃciently as it

has linear time complexity for the number of instances. However, PABIDOT also shows exponential

time complexity for the number of attributes of a dataset [71]. Other examples include the use of a

Naive Bayesian Classiﬁer for private data streams [72], and the method to eﬃciently and eﬀectively

track the correlation and autocorrelation structure of multivariate streams and leverage it to add noise

to preserve privacy [73]. The latter methods are also vulnerable to data reconstruction attacks such

as principal component analysis-based attacks.

The complex dynamics exhibited by SCPS require eﬃcient privacy preservation methods scalable

enough to handle exponentially growing databases and data streams such as IoT stream data. Existing

privacy-preserving mechanisms have diﬃculties in maintaining the balance between privacy and utility

6

while providing suﬃcient eﬃciency and scalability. This paper tries to ﬁll the gap, and proposes a

privacy-preserving algorithm for SCPS which solves the existing issues.

3. Fundamentals

In this section, we provide some background and discuss the fundamentals used in the proposed

method (SEAL). Our approach is generating a privacy-preserved version of the dataset in question, and

allowing only the generated dataset to be used in any application. We use Chebyshev interpolation

based on least square ﬁtting to model a particular input data series, and the model formation is

subjected to noise addition using the Laplacian mechanism used in diﬀerential privacy. The noise

integrated model is then used to synthesize a perturbed data series which approximate the properties

of the original input data series.

3.1. Chebyshev Polynomials of the First Kind

For the interpolation of the input dataset, we use Chebyshev Polynomials of the First Kind. These

polynomials are a set of orthogonal polynomials as given by Deﬁnition 3 (available in Appendix A)

[74]. Chebyshev polynomials are a sequence of orthonormal polynomials that can be deﬁned recursively.

Polynomial approximation and numerical integration are two of the areas where Chebyshev polynomials

are heavily used [74]. More details on Chebyshev polynomials of the ﬁrst kind can be found in Appendix

A.

3.2. Least Squares Fitting

Least squares ﬁtting (LSF) is a mathematical procedure which minimizes the sum of squares of the

oﬀsets of the points from the curve to ﬁnd the best-ﬁtting curve to a given set of points. We can use

vertical least squares ﬁtting which proceeds by ﬁnding the sum of squares of the vertical derivations R2

(refer Equation B.1 in Appendix B) of a set of n data points [75]. To generate a linear ﬁt considering

f (x) = mx + b, we can minimize the expression of squared error between the estimated values and the

original values (refer Equation B.5), which proceeds to obtaining the linear system shown in Equation

1 (using Equations B.8 and B.9). We can solve Equation 1 to ﬁnd values of a and b to obtain the

corresponding linear ﬁt of f (x) = mx + b for a given data series.










b

m


 =




n

(cid:0)(cid:80)n

i=1 xi

(cid:1)

(cid:0)(cid:80)n



−1 

(cid:0)(cid:80)n

(cid:1)




(cid:1)

i=1 xi
i=1 x2
i




(cid:80)n

i=1 yi

(cid:80)n

i=1 xiyi






(1)

7

3.3. Diﬀerential Privacy

Diﬀerential Privacy (DP) is a privacy model that deﬁnes the bounds to how much information can

be revealed to a third party or adversary about someone’s data being present or absent in a particular

database. Conventionally, (cid:15) (epsilon) and δ (delta) are used to denote the level of privacy rendered by

a randomized privacy preserving algorithm (M ) over a particular database (D). Let us take two x and

y adjacent datasets of D, where y diﬀers from x only by one person. Then M satisﬁes ((cid:15), δ)-diﬀerential

privacy if Equation (2) holds.

Privacy Budget and Privacy Loss ((cid:15)): (cid:15) is called the privacy budget that provides an insight

into the privacy loss of a DP algorithm. When the corresponding (cid:15) value of a particular diﬀerentially

private algorithm A is increased, the amount of noise or randomization applied by A on the input data

is decreased. The higher the value of (cid:15), the higher the privacy loss.

Probability to Fail a.k.a. Probability of Error (δ): δ is the parameter that accounts for ”bad events”

that might result in high privacy loss; δ is the probability of the output revealing the identity of a

particular individual, which can happen n × δ times where n is the number of records. To minimize

the risk of privacy loss, n × δ has to be maintained at a low value. For example, the probability of a

bad event is 1% when δ = 1

100×n .

Deﬁnition 1. A randomized algorithm M with domain N |X| and range R is ((cid:15), δ)-diﬀerentially private

for δ ≥ 0, if for every adjacent x, y ∈ N |X| and for any subset S ⊆ R

P r[(M (x) ∈ S)] ≤ exp((cid:15))P r[(M (y) ∈ S)] + δ

(2)

3.4. Global vs. Local Diﬀerential Privacy

Global diﬀerential privacy (GDP) and local diﬀerential privacy (LDP) are the two main approaches

to diﬀerential privacy. In the GDP setting, there is a trusted curator who applies carefully calibrated

random noise to the real values returned for a particular query. The GDP setting is also called

the trusted curator model [76]. Laplace mechanism and Gaussian mechanism [57] are two of the

most frequently used noise generation methods in GDP [57]. A randomized algorithm, M provides

(cid:15)-global diﬀerential privacy if for any two adjacent datasets x, y and S ⊆ R, P r[(M (x) ∈ S)] ≤

exp((cid:15))P r[(M (y) ∈ S)] + δ (i.e. Equation (2) holds). On the other hand, LDP eliminates the need of a

trusted curator by randomizing the data before the curator can access them. Hence, LDP is also called

the untrusted curator model

[59]. LDP can also be used by a trusted party to randomize all records in

8

a database at once. LDP algorithms may often produce too noisy data, as noise is applied commonly

to achieve individual record privacy. LDP is considered to be a strong and rigorous notion of privacy

that provides plausible deniability. Due to the above properties, LDP is deemed to be a state-of-the-art

approach for privacy-preserving data collection and distribution. A randomized algorithm A provides

(cid:15)-local diﬀerential privacy if Equation (3) holds [60].

Deﬁnition 2. A randomized algorithm A satisﬁes (cid:15)-local diﬀerential privacy, if for all pairs of inputs

v1 and v2, for all Q ⊆ Range(A) and for ((cid:15) ≥ 0) Equation (3) holds. Range(A) is the set of all

possible outputs of the randomized algorithm A.

P r[A(v1) ∈ Q] ≤ exp((cid:15))P r[A(v2) ∈ Q]

(3)

3.5. Sensitivity

Sensitivity is deﬁned as the maximum inﬂuence that a single individual data item can have on the

result of a numeric query. Consider a function f , the sensitivity (∆f ) of f can be given as in Equation

(4) where x and y are two neighboring databases (or in LDP, adjacent records) and (cid:107).(cid:107)1 represents the

L1 norm of a vector [77].

3.6. Laplace Mechanism

∆f = max{(cid:107)f (x) − f (y)(cid:107)1}

(4)

The Laplace mechanism is considered to be one of the most generic mechanisms to achieve diﬀer-

ential privacy [57]. Laplace noise can be added to a function output (F (D)) as given in Equation (6)

to produce a diﬀerentially private output. ∆f denotes the sensitivity of the function f . In the local

diﬀerentially private setting, the scale of the Laplacian noise is equal to ∆f /(cid:15), and the position is the

current input value (F (D)).

P F (D) = F (D) + Lap(

∆f
(cid:15)

)

P F (D) =

(cid:15)
2∆f

e− |x−F (D)|

∆F

9

(5)

(6)

4. Our Approach

The proposed method, SEAL, is designed to preserve the privacy of big data and data streams

generated by systems such as smart cyber-physical systems. One of our aims was balancing privacy and

utility, as they may adversely aﬀect each other. For example, the spatial arrangement of a dataset can

potentially contribute to its utility in data mining, as the results generated by the analysis mechanisms

such as data classiﬁcation and clustering are often inﬂuenced by the spatial arrangement of the input

data. However, the spatial arrangement can be aﬀected when privacy mechanisms apply methods like

randomization. In other words, while data perturbation mechanisms improve privacy, at the same time

they may reduce utility. Conversely, an increasing utility can detrimentally aﬀect privacy. To address

these diﬃculties, SEAL processes the data in three steps: (1) determine the sensitivity of the dataset

to calibrate how much random noise is necessary to provide suﬃcient privacy, (2) conduct polynomial

interpolation with calibrated noise to approximate a noisy function over the original data, and (3)

use the approximated function to generate perturbed data. These steps guarantee that SEAL applies

enough randomization to preserve privacy while preserving the spatial arrangement of the original data.

SEAL uses polynomial interpolation accompanied by noise addition, which is calibrated according to

the instructions of diﬀerential privacy. We use the ﬁrst four orders of the Chebyshev polynomial of

the ﬁrst kind in the polynomial interpolation process. Then we calibrate random Laplacian noise to

apply a stochastic error to the interpolation process, in order to generate the perturbed data. Figure

1 shows the integration of SEAL in the general purpose data ﬂow of SCPS. As shown in the ﬁgure, the

data perturbed by the SEAL layer comes directly from the SCPS. That means that the data in the

storage module has already gone through SEAL’s privacy preservation process and does not contain

any original data.

10

Figure 1: Arrangement of SEAL in a smart system environment. In this setting, we assume that the original data are
perturbed before reaching the storage devices. Any public or private services will have access only to the perturbed data.

Figure 2 shows the ﬂow of SEAL where the proposed noisy Chebyshev model (represented by a

green note) is used to approximate each of the individual attributes of a particular input dataset or

data stream. The approximated noisy function is used to synthesize perturbed data, which is then

subjected to random tuple shuﬄing to reduce the vulnerability to data linkage attacks.

4.1. Privacy-Preserving Polynomial Interpolation for Noisy Chebyshev Model Generation

We approximate an input data series (experimental data) by a functional expression with added

randomization in order to inherit the properties of diﬀerential privacy. For approximation, our method

uses the ﬁrst four orders of Chebyshev polynomials of the ﬁrst kind. We systematically add calibrated

random Laplacian noise in the interpolation process, i.e. apply randomization to the approximation.

Then we use the approximated function to re-generate the dataset in a privacy-preserving manner. We

can denote an approximated function ˆf of degree (m − 1) using Equation 7, where the degree of (ϕk) is

k−1. For the approximation, we consider the root mean square error (RMSE) E between the estimated

values and the original values (refer to Equation C.14). We use the ﬁrst four Chebyshev polynomials

of the ﬁrst kind for the approximation, which limits the number of coeﬃcients to four (we name the

coeﬃcients as a1, a2, a3, and a4). Now we can minimize E (the RMSE) to obtain an estimated function
ˆf ∗(x), thus seeking to minimize the squared error M (a1, a2, a3, a4). For more details refer to Equation

C.15, where a1, a2, . . . , am are coeﬃcients and ϕ1(x), ϕ2(x), . . . , ϕm(x) are Chebyshev polynomials of

ﬁrst kind.

ˆf (x) = a1ϕ1(x) + a2ϕ2(x) + · · · + amϕm(x)

(7)

11

Smart SystemsFog / EdgeCloud / Third party usersSEALSEALSEALPrivacyPreservationModuleDatabase 1Database nDatabase 2StorageModuleService 1Service nService 2AnalyticalServicesCloud 1Cloud nUsers4.1.1. Introducing privacy to the approximation process utilizing diﬀerential privacy (the determination

of the sensitivity and the position of Laplacian noise)

We apply the notion of diﬀerential privacy to the private data generation process by introducing

randomized Laplacian noise to the root mean square error (RMSE) minimization process. Random

Laplacian noise introduces a calibrated randomized error in deriving the values for a1, a2, a3, and a4

with an error (refer to Equations C.22, C.25, C.28 and C.31). We add Laplacian noise with a sensitivity

of 1, as the input dataset is normalized within the bounds of 0 and 1, which restricts the minimum

output to 0 and maximum output to 1 (refer to Equation (C.20)). We select the position of Laplacian

noise to be 0, as the goal is to keep the local minima of RMSE around 0. We can factorize the noise

introduced squared error minimization equations to form a linear system which can be denoted by

Equation 8. C is the coeﬃcient matrix obtained from the factorized expressions, A is the coeﬃcient

vector obtained from M , and B is the constant vector obtained from the factorized expressions (refer

Equations C.24, C.27, C.30, and C.33 where mij denote the coeﬃcients and bi denote the constants).

CA = B

Where,



m11 m12 m13 m14

C =

m21 m22 m23 m24

m31 m32 m33 m34











m41 m42 m43 m44

A = [a1, a2, a3, a4]T

B = [b1, b2, b3, b4]T













(8)

(9)

(10)

(11)

Now we solve the corresponding linear system (formed using Equations C.35-C.37), to obtain noisy

values for a1, a2, a3, and a4 in order to approximate the input data series with a noisy function. The

results will diﬀer each time we calculate the values for a1, a2, a3, and a4 as we have randomized the

process of interpolation by adding randomized Laplacian noise calibrated using a user-deﬁned (cid:15) value.

12

Figure 2: The basic ﬂow of SEAL. The users can calibrate the level of privacy using the privacy budget parameter ((cid:15)).
The smaller the (cid:15), the higher the privacy. It is recommended to use an (cid:15) in the interval (0, 10), which is considered to
be the default range to provide a suﬃcient level of privacy.

4.2. Algorithmic Steps of SEAL for Static Data and Data Streams

Algorithm 1 presents the systematic ﬂow of steps in randomizing the data to produce a privacy-

preserving output. The algorithm accepts input dataset (D), privacy budget (cid:15) (deﬁned in Equation

C.21), window size (ws) and threshold (t) as the input parameters. The window size deﬁnes the number

13

accept thedataset/data stream(D) to be sanitizedapproximate a noisyfunction using theproposed modeldeﬁne the parameters,1. privacy budget, 2. window size,3. threshold,     eachwindowperturbed    ()      for eachattribute     ofD   eachsort      normalize   between [0,1]  normalize      within [0,1]    normalize      within [min       ,max       ]   ()  ()  attributeall attributesperturbedProposedNoisyChebyshevModel restore the original       order of         for eachwindow   merge all       togenerate               merge all       togenerate        all windows consumed           or =−1if    number of windows perturbedrandomly swapthe tuples of      TrueFalserelease      randomly swapthe tuples of      release     of data instances to be perturbed in one cycle of randomization. The window size of a data stream is

essential to maintain the speed of the post-processing analysis/modiﬁcation (e.g. data perturbation,

classiﬁcation, and clustering) done to the data stream [78]. For static data sets, the threshold is

maintained with a default value of −1. For a static dataset, t = −1 ignores that a speciﬁc number

of perturbed windows need to be released before the whole dataset is completed. In the case of data

streams, the window size (ws) and the threshold t are useful as ws can be maintained as a data buﬀer

and t can be speciﬁed with a certain number to let the algorithm know that it has to release every t

number of processed windows. Maintaining t is important for data streams because data streams are

growing inﬁnitely in most cases, and the algorithm makes sure that the data is released in predeﬁned

intervals.

According to conventional diﬀerential privacy, the acceptable values of (cid:15) should be within a small

range, ideally in the interval of (0, 9] [79]. Due to the lower sensitivity of the interpolation process,

increasing (cid:15) greater than 2 may lower privacy. It is the users’ responsibility to decrease or increase (cid:15)

depending on the requirements. We suggest an (cid:15) of 1 to have a balance between privacy and utility.

If the user chooses an (cid:15) value less than 1, the algorithm will provide higher randomization, hence

providing higher privacy and lower utility, whereas lower privacy and a higher utility will be provided

in case of an (cid:15) value higher than 1. The selection of ws depends speciﬁcally on the size of the particular

dataset. A comparably larger ws can be chosen for a large dataset, while ws can be smaller for a small

dataset. For a static dataset, ws can range from a smaller value such as one-tenth the size of the

dataset to the full size of the dataset. The minimum value of ws should not go down to a small value

(e.g. < 100) because it increases the number of perturbation cycles and introduces an extreme level

of randomization to the input dataset, resulting in poor utility. For a data stream, ws is considered

as the buﬀer size and can range from a smaller value to any number of tuples that ﬁt in the memory

of the computer. Further discussions on selecting suitable values for (cid:15) and ws is provided in Section

5.2.1.

14

Algorithm 1 Steps of the perturbation algorithm: SEAL

Inputs :

D ← input dataset (numeric)
← scale of Laplacian noise
(cid:15)
ws ← data buﬀer/window size
t

← threshold for the maximum number of windows processed

before a data release (default value of t = −1)

Outputs : Dp ← perturbed dataset
1: divide D in to data partitions (wi) of size ws
2: x = [1, . . . , ws]
3: normalize x within the bounds of [0, 1]
4: for each wi do
5:

rep=rep+1
Dp = []
normalize data of wi within the bounds of [0, 1]
for each attribute ai in wi do

sai = sort(ai)
generate M
generate B
A = B ∗ M −1
use A = [a1, a2, a3, a4] to generate perturbed data (ap
normalize ap
i within the bounds of [0, 1] to generate aN
i
normalize aN
i within the bounds of [min(ai), max(ai)]
resort aN
i

to the original order of ai to generate ao
i

i ) using

end for
i to generate wp
merge all ao
i
Dp = merge(Dp, wp
i )
if rep==t then

randomly swap the tuple of Dp
release Dp
rep=0

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

(cid:46) empty matrix

(cid:46) sorted in ascending order
(cid:46) according to Equation C.35
(cid:46) according to Equation C.37

ˆf (x)

(cid:46) refer Equation C.12

end if

24:
25: end for
26: if t==-1 then

randomly swap the tuple of Dp
return Dp

27:

28:
29: end if

End Algorithm

4.3. A use case: SEAL integration in a healthcare smart cyber-physical system

Biomedical and healthcare systems provide numerous opportunities and challenges for SCPS. Intel-

ligent operating rooms and hospitals, image-guided surgery and therapy, ﬂuid ﬂow control for medicine

and biological assays and the development of physical and neural prostheses are some of the exam-

ples for biomedical and healthcare systems which can be eﬀectively facilitated and improved using

15

SCPS [80]. However, biomedicine and healthcare data can contain a large amount of sensitive, per-

sonal information. SEAL provides a practical solution and can impose privacy in such scenarios to

limit potential privacy leak from such systems [80].

Figure 3 shows a use case for SEAL integration in a healthcare smart cyber-physical system.

Patients can have several sensors attached to them for recording diﬀerent physical parameters. The

recorded data are then transmitted to a central unit which can be any readily available digital device

such as a smartphone, a personal computer, or an embedded computer. A large variety of sensors

are available today, e.g. glucose monitors, blood pressure monitors [81]. In the proposed setting, we

assume that the processing unit that runs SEAL, perturbs all sensitive inputs forwarded to the central

unit. As shown in the ﬁgure, we assume that the central units do not receive any unperturbed sensitive

information, and the data repositories will store only perturbed data, locally or in a remote data center.

Data analysts can access and use only the perturbed data to conduct their analyses. Since the data is

perturbed, adversarial attacks on privacy will not be successful.

Figure 3: A use case: The integration of SEAL in a healthcare smart cyber-physical system. As shown in the ﬁgure,
SEAL perturbs data as soon as they leave the source (medical sensors, medical devices, etc.). In the proposed setting,
SEAL assumes that there is no trusted party.

16

             Database at the Local Edge                   Central Unit/s                Patients Medical Data Analyst/sSensor nSensor 1ActuatorCardiographyPerturbed RecordsPharmacyIntensive CareComputerMobile PhoneSEALSEALSEALThe InternetCloud Service 1Cloud Service nRemote data centerAdversary5. Experimental Results

In this section, we discuss the experimental setup, resources used, experiments, and their results.

The experiments were conducted using seven datasets retrieved from the UCI data repository2. We

compare the results of SEAL against the results of rotation perturbation (RP), geometric perturbation

(GP) and data condensation (DC). For performance comparison with SEAL, we selected GP and RP

when using static datasets, while DC was used with data streams. The main reason for selecting

GP, RP, and DC is that they are multidimensional perturbation mechanisms that correlate with the

technique used in the linear system of SEAL as given in Equation C.34. Figure 4 shows the analytical

setup which was used to test the performance of SEAL. We perturbed the input data using SEAL, RP,

GP, and DC and conducted data classiﬁcation experiments on the perturbed data using ﬁve diﬀerent

classiﬁcation algorithms to test and compare utility. We used the default settings of 10 iterations with

a noise factor (sigma) of 0.3 to perturb the data using RP and GP. Next, SEAL was tested for its

attack resistance against naive inference, known I/O attacks, and ICA-based attacks that are based on

data reconstruction. These attacks are more successful against perturbation methods that use matrix

multiplications. The attack resistance results of SEAL were then compared with the attack resistance

results of RP, GP, and DC. Subsequently, we tested and compared SEAL’s computational complexity

and scalability by using two large datasets. Finally, we tested the performance of SEAL on data

streams and compared the results with the results of DC.

Figure 4: The analytical setup used for SEAL. The ﬁgure shows the diﬀerent levels of performance analysis of SEAL.
First, the time consumption of perturbation was recorded under the eﬃciency analysis. Next, the attack resistance and
classiﬁcation accuracy were analyzed upon the perturbed data.

2https://archive.ics.uci.edu/ml/index.php

17

Original DataPerturb data using SEALConduct Analysis AttackResilience Classiﬁcation Accuracy Efﬁciency Analysis Perturbed Data5.1. Experimental Setup

For the experiments we used a Windows 10 (Home 64-bit, Build 17134) computer with Intel (R)

i5-6200U (6th generation) CPU (2 cores with 4 logical threads, 2.3 GHz with turbo up to 2.8 GHz) and

8192 MB RAM. The scalability of the proposed algorithm was tested using a Linux (SUSE Enterprise

Server 11 SP4) SGI UV3000 supercomputer, with 64 Intel Haswell 10-core processors, 25MB cache and

8TB of global shared memory connected by SGI’s NUMAlink interconnect. We implemented SEAL in

MATLAB R2016b. The experiments on data classiﬁcation were carried out on Weka 3.6 [82], which is

a collection of machine learning algorithms for data mining tasks.

5.1.1. Datasets used in the experiments

A brief description of the datasets is given in Table 1. The dimensions of the datasets used for the

performance testing vary from small to extremely large, to check the performance of SEAL in diﬀerent

circumstances. Since the current version of SEAL conducts perturbation only on numerical data; we

selected only numerical datasets in which the only attribute containing non-numerical data is the class

attribute.

Table 1: Short descriptions of the datasets selected for testing.

Abbreviation Number of Records Number of Attributes Number of Classes
Dataset
Wholesale customers3
WCDS
Wine Quality4
WQDS
Page Blocks Classiﬁcation 5 PBDS
Letter Recognition6
LRDS
Statlog (Shuttle)7
SSDS
HEPMASS8
HPDS
HIGGS9
HIDS

440
4898
5473
20000
58000
3310816
11000000

8
12
11
17
9
28
28

2
7
5
26
7
2
2

5.1.2. Perturbation methods used for comparison

Random rotation perturbation (RP), geometric data perturbation (GP), and data condensation

(DC) are three types of matrix multiplicative perturbation approaches which are considered to provide

high utility in classiﬁcation and clustering [83]. In RP, the original data matrix is multiplied using

a random rotation matrix which has the properties of an orthogonal matrix. A rotational matrix

R follows the property of R × RT = RT × R = I, where I is the identity matrix. The application

3https://archive.ics.uci.edu/ml/datasets/Wholesale+customers
4https://archive.ics.uci.edu/ml/datasets/Wine+Quality
5https://archive.ics.uci.edu/ml/datasets/Page+Blocks+Classiﬁcation
6https://archive.ics.uci.edu/ml/datasets/Letter+Recognition
7https://archive.ics.uci.edu/ml/datasets/Statlog+%28Shuttle%29
8https://archive.ics.uci.edu/ml/datasets/HEPMASS#
9https://archive.ics.uci.edu/ml/datasets/HIGGS#

18

of rotation is repeated until the algorithm converges at the desired level of privacy [9].

In GP, a

random translation matrix is added to the process of perturbation in order to enhance privacy. The

method accompanies three components: rotation perturbation, translation perturbation, and distance

perturbation [10]. Due to the isometric nature of transformations, the perturbation process preserves

the distance between the tuples, resulting in high utility for classiﬁcation and clustering. RP and

GP can only be used for static datasets in their current setting, due to their recursive approach to

deriving the optimal perturbation. DC is speciﬁcally introduced for data streams. In DC, data are

divided into multiple homogeneous groups of predeﬁned size (accepted as user input) in such a way that

the diﬀerence between the records in a particular group is minimal, and a certain level of statistical

information about diﬀerent records is maintained. The sanitized data is generated using a uniform

random distribution based on the eigenvectors which are generated using the eigendecomposition of

the characteristic covariance matrices of each homogeneous group [52].

5.1.3. Classiﬁcation algorithms used in the experiments

Diﬀerent classes of classiﬁcation algorithms employ diﬀerent classiﬁcation strategies [84]. To inves-

tigate the performance of SEAL with diverse classiﬁcation methods, we chose ﬁve diﬀerent algorithms

as the representative of diﬀerent classes, namely: Multilayer Perceptron (MLP) [82], k-Nearest Neigh-

bor (kNN) [82], Sequential Minimal Optimization (SMO) [85], Naive Bayes [82], and J48 [86], and

tested SEAL for its utility in terms of classiﬁcation accuracy. MLP uses back-propagation to classify

instances [82]. kNN is a non-parametric method used for classiﬁcation [82]. SMO is an implementa-

tion of John Platt’s sequential minimal optimization algorithm for training a support vector classiﬁer

[85]. Naive Bayes is a fast classiﬁcation algorithm based on probabilistic classiﬁers

[82]. J48 is an

implementation of the decision tree based classiﬁcation algorithm [82].

5.2. Performance Evaluation of SEAL

We evaluated the performance of SEAL with regard to classiﬁcation accuracy, attack resistance,

time complexity, scalability, and also looked at data streams. First, we generated perturbed data using

SEAL, RP, GP, and DC for the datasets: WCDS, WQDS, PBDS, LRDS, and SSDS (refer to Table 1)

under the corresponding settings. The perturbed data were then used to determine classiﬁcation accu-

racy and attack resistance for each perturbed dataset. During the classiﬁcation accuracy experiments,

k of k-nearest neighbor (kNN) classiﬁcation algorithm was kept at 1. The aggregated results were

rated using the nonparametric statistical comparison test, Friedman’s rank test, which is analogous

19

to a standard one-way repeated-measures analysis of variance [87]. We recorded the statistical signiﬁ-

cance values, and the Friedman’s mean ranks (FMR) returned by the rank test. The time consumption

of SEAL was evaluated using runtime complexity analysis. We ran SEAL on two large-scale datasets,

HPDS and HIDS, to test its scalability. Finally, the performance of SEAL was tested on data streams

by running it on the LRDS dataset, and the results were compared with those produced by DC.

5.2.1. Eﬀect of randomization on the degree of privacy

One of the main features of SEAL is its ability to perturb a dataset while preserving the original

shape of data distribution. We ran SEAL on the same data series to detect the eﬀect of randomization

in two diﬀerent instances of perturbation. This experiment is to check and guarantee that SEAL does

not publish similar perturbed data when it is applied with the same (cid:15) value to the same data on

diﬀerent occasions. This feature enables SEAL to prevent privacy leak via data linkage attacks that

are exploiting multiple data releases. As depicted in Figure 5, in two separate applications, SEAL

generates two distinct randomized data series, while preserving the shape of the original data series.

The left-hand plot of Figure 5 shows the data generated under an (cid:15) of 1, whereas the right-hand

plot shows the data generated under an (cid:15) of 0.1. The right plot clearly shows the eﬀect of high

randomization under the extreme level of privacy generated by a strict privacy budget ((cid:15)) of 0.1.

(a) Eﬀect of perturbations by SEAL with (cid:15) = 1 on the
same data series in two diﬀerent instances.

(b) Eﬀect of perturbations by SEAL with (cid:15) = 0.1 on the
same data series in two diﬀerent instances.

Figure 5: Eﬀect of perturbation by SEAL. The plot with the staircase pattern represents the original data series (ﬁrst
attribute of the LRDS dataset). The two plots that were plotted above the original data series represent two instances
of perturbation conducted by SEAL on the original data series.

20

00.20.40.60.81x00.20.40.60.8100.20.40.60.81x00.20.40.60.815.2.2. Dynamics of privacy budget ((cid:15)) and window size (ws)

As explained in Section 5.2.1, smaller (cid:15) means higher randomization, which results in decreased

utility. Figure 6a shows the change of classiﬁcation accuracy against an increasing (cid:15). As shown in the

ﬁgure, classiﬁcation accuracy increases with an increasing privacy budget ((cid:15)). Figure 6a shows a more

predictable pattern of increasing utility (classiﬁcation accuracy) against increasing (cid:15). The choice of

a proper (cid:15) depends on the application requirements: a case that needs higher privacy should have a

smaller (cid:15), while a larger (cid:15) will provide better utility. As it turns out, two-digit (cid:15) values provide no useful

privacy. Given that SEAL tries to preserve the shape of the original data distribution, we recommend

a range of 0.4 to 3 for (cid:15) to limit unanticipated privacy leaks. We showed that SEAL provides better

privacy and utility than comparable methods under a privacy budget of 1.

(a) Change of classiﬁcation accuracy of the LRDS dataset
perturbed by SEAL, where the window size (ws) was
maintained at 10, 000 tuples.

(b) Change of classiﬁcation accuracy (J48) of the LRDS
dataset perturbed by SEAL where the privacy budget ((cid:15))
was maintained at 1.

Figure 6: Classiﬁcation accuracy of SEAL. The classiﬁcation accuracy was obtained by classifying the corresponding
datasets using the J48 classiﬁcation algorithm. The orange dotted horizontal lines on the two plots represent the
classiﬁcation accuracy of the original dataset. The window size (ws) is measured in number of tuples.

Next, we tested the eﬀect of window size (ws) on classiﬁcation accuracy and the magnitude of

randomization performed by SEAL. As shown on Figure 6b, classiﬁcation accuracy increases when

ws increases. When ws is small, the dataset is divided into more groups than when ws is large.

When there is more than one group to be perturbed, SEAL applies randomization on each group

distinctly. Since each of the groups is subjected to distinct randomization, the higher the number of

groups, the larger the perturbation of the dataset. For smaller sizes of ws, SEAL will produce higher

perturbation, resulting in more noise, reduced accuracy, improved privacy, and better resistance to

data reconstruction attacks.

21

70.00%72.00%74.00%76.00%78.00%80.00%82.00%84.00%86.00%88.00%90.00%0.10.30.50.70.9246810Classification accuracyPrivacy budget, ∈50.00%55.00%60.00%65.00%70.00%75.00%80.00%85.00%90.00%Classification accuracyWindow size (ws)5.2.3. Classiﬁcation accuracy

Table 2: Classiﬁcation accuracies obtained when using the original dataset and the datasets perturned by three methods.
During the experiments conducted using SEAL, the privacy budget (cid:15) was maintained at 1. The window size ws was
maintained at full length of the corresponding dataset. For example, ws of SEAL for the LRDS dataset was maintained at
20,000 during the experiments presented in this table. The last row shows the mean ranks returned by the nonparametric
statistical comparison test (Friedman’s rank test on the classiﬁcation accuracies) of the three methods. A larger FMR
value represents better classiﬁcation accuracy.

Dataset

Algorithm MLP

IBK

SVM Naive Bayes

J48

LRDS

PBDS

SSDS

WCDS

WQDS

Original
RP
GP
SEAL
Original
RP
GP
SEAL
Original
RP
GP
SEAL
Original
RP
GP
SEAL
Original
RP
GP
SEAL

82.20% 95.96% 82.44% 64.01%
74.04% 87.19% 71.07% 48.41%
79.12% 93.05% 77.92% 59.89%
80.59% 93.67% 81.71% 63.10%
96.25% 96.02% 92.93% 90.85%
92.00% 95.52% 89.99% 35.76%
90.24% 95.67% 89.93% 43.10%
96.34% 96.73% 95.59% 86.97%
99.72% 99.94% 96.83% 91.84%
96.26% 99.80% 88.21% 69.04%
98.73% 99.81% 78.41% 79.18%
99.70% 99.21% 98.51% 89.94%
90.91% 87.95% 87.73% 89.09%
89.09% 85.00% 82.27% 84.55%
91.82% 86.59% 85.00% 84.32%
89.32% 86.82% 89.09% 88.41%
54.94% 64.54% 52.14% 44.67%
47.65% 53.29% 44.88% 32.32%
48.86% 56.88% 44.88% 32.16%
53.92% 64.02% 52.02% 47.83%

87.92%
64.89%
70.54%
85.28%
96.88%
95.61%
95.49%
96.34%
99.96%
99.51%
99.59%
99.87%
90.23%
86.82%
88.86%
86.59%
59.82%
45.53%
46.43%
84.15%

FMR Values

RP: 1.34

GP: 1.86

SEAL: 2.80

Table 2 provides the classiﬁcation accuracies when using the original dataset and the datasets

perturbed by the three methods. During the experiments for classiﬁcation accuracy, we maintained

(cid:15) at 1 and ws at the total length of the dataset. For example, if the dataset contained n number

of tuples, ws was maintained at n. After producing the classiﬁcation accuracies, Friedman’s rank

test was conducted on the data available in Table 2 to rank the three methods: GP, RP, and SEAL.

The mean ranks produced by Friedman’s rank (FR) test are presented in the last row of Table 210.

The p-value suggests that the diﬀerence between the classiﬁcation accuracies of RP, GP, and SEAL

are signiﬁcantly diﬀerent. When evaluating FMR values on classiﬁcation accuracies, a higher rank

means that the corresponding method tends to produce better classiﬁcation results. The mean ranks

indicate that SEAL provides comparatively higher classiﬁcation accuracy. SEAL is capable of providing

higher utility in terms of classiﬁcation accuracy due to its ability to maintain the shape of the original

10The FR test returned a χ2 value of 27.6566, a degree of freedom of 2 and a p-value of 9.8731e-07.

22

data distribution despite the introduced randomization. Although SEAL provides better performance

overall than the other two methods, we can notice that in a few cases (as shown in Table 2) SEAL

has produced slightly lower classiﬁcation accuracies. We assume that this is due to the eﬀect of

variable random noise applied by SEAL. However, these lower accuracies are still on par with accuracies

produced by the other two methods.

5.2.4. Attack resistance

Table 3 shows the three methods’ (RP, GP, and SEAL) resistance to three attack methods: naive

snooping (NI), independent component analysis (ICA) and known I/O attack (IO) [9, 83]. We used the

same parameter settings of SEAL ((cid:15) = 1 and ws=number of tuples) which were used in classiﬁcation

accuracy experiments for attack resistance analysis as well. IO and ICA data reconstruction attacks

try to restore the original data from the perturbed data and are more successful in attacking matrix

multiplicative data perturbation. FastICA package [88] was used to evaluate the eﬀectiveness of ICA-

based reconstruction of the perturbed data. We obtained the attack resistance values as standard

deviation values of (i) the diﬀerence between the normalized original data and the perturbed data

for NI, and (ii) the diﬀerence between the normalized original data and reconstructed data for ICA

and IO. During the IO attack analysis, we assume that around 10% of the original data is known to

the adversary. The “min” values under each test indicate the minimum guarantee of resistance while

“avg” values give an impression of the overall resistance.

We evaluated the data available in Table 3 using Friedman’s rank test to generate the mean ranks

for GP, RP, and SEAL. The mean ranks produced by Friedman’s rank test are given in the last row of

Table 311. The p-value implies that the diﬀerence between the attack resistance values is signiﬁcantly

diﬀerent. As for the FMR values on attack resistance, a higher rank means that the corresponding

method tends to be more attack-resistant. The mean ranks suggest that SEAL provides comparatively

higher security than the comparable methods against the privacy attacks.

11The test statistics: χ2 value of 14.6387, a degree of freedom of 2 and a p-value of 6.6261e-04.

23

Table 3: Attack resistance of the algorithms. During the experiments conducted using SEAL, the privacy budget (cid:15) was
maintained at 1. The window size ws was maintained at full length of the corresponding dataset. For example, ws of
SEAL for the LRDS dataset was maintained at 20,000 during the experiments presented in this table. The last row
provides Friedman’s mean ranks returned by the nonparametric statistical comparison test on the three methods.

SSDS

PBDS

Dataset Algorithm N Imin
0.8750
LRDS
1.3248
1.4061
0.7261
0.2845
1.3900
1.2820
1.4490
1.4065
1.0105
1.4620
1.3130
1.2014
1.3463
1.3834

RP
GP
SEAL
RP
GP
SEAL
RP
GP
SEAL
RP
GP
SEAL
RP
GP
SEAL

WQDS

WCDS

N Iavg
1.4490
1.6175
1.4148
1.3368
1.4885
1.4084
1.5015
1.6285
1.4119
1.3098
1.7489
1.3733
1.4957
1.6097
1.4138

ICAmin
0.4057
0.6402
0.7024
0.5560
0.1525
0.7008
0.1751
0.0062
0.7038
0.6315
0.1069
0.6775
0.4880
0.3630
0.7018

ICAavg
0.6942
0.7122
0.7062
0.6769
0.6834
0.7056
0.5909
0.3240
0.7068
0.7362
0.6052
0.7053
0.7062
0.6536
0.7053

IOmin
0.0945
0.0584
0.6986
0.0001
0.0000
0.6932
0.0021
0.0011
0.7027
0.0000
0.0000
0.6557
0.0057
0.0039
0.6859

IOavg
0.2932
0.4314
0.7056
0.1242
0.1048
0.7031
0.0242
0.0111
0.7068
0.0895
0.1003
0.6930
0.4809
0.4025
0.7026

FMR Values

RP: 1.68

GP: 1.75

SEAL: 2.57

5.2.5. Time complexity

Algorithm 1 (SEAL) has two loops. One loop is controlled by the number of data partitions

resulting from the window size (ws), and the number of attributes controls the other loop.

In a

particular instance of perturbation, these two parameters (ws and the number of attributes) remain

constants. Let us take the contribution of both loops to the computational complexity as a constant

(k). If we evaluate the steps of SEAL from step 9 to step 23, we can see that the highest computational

complexity in these steps is O(n), where n is the number of tuples. From this, we can estimate the

time complexity of Algorithm 1 to be O(kn) = O(n). We investigated the time consumption against

the number of instances and the number of attributes to determine the computational complexity

empirically. Figure 7 conﬁrms that the time complexity of SEAL is in fact O(n).

24

(a) Change of the time elapsed for the LRDS dataset with
an increasing number of instances.

(b) Change of the time elapsed for the LRDS dataset with
an increasing number of attributes.

Figure 7: Time consumption of SEAL. During the runtime analysis, the window size (ws) was maintained at a full length
of the corresponding instance of LRDS. The privacy budget ((cid:15)) was maintained at 1.

5.2.6. Time complexity comparison

Both RP and GP show O(n2) time complexity to perturb one record with n attributes. The total

complexity to perturb a dataset of m records is O(m × n2). However, both RP and GP run for r

number of iterations (which is taken as a user input) to ﬁnd the optimal perturbation instance of

the dataset within the r iterations. Therefore, the overall complexity is O(m × r × n2). Under each

iteration of r, the algorithms run data reconstruction using ICA and known IO attacks to ﬁnd the

vulnerability level of the perturbed dataset. Each attack runs another k number of iterations (which

is another user input) to reconstruct k number of instances. Usually, k is much larger than r. For one

iteration of k, IO and ICA contribute a complexity of O(n × m) [89]. Hence, the overall complexity of

RP or GP in producing an optimal perturbed dataset is equal to O(m2 × r × k × n3) which is a much

larger computational complexity compared to the linear computational complexity of SEAL. Figure 8

shows the time consumption plots of the three methods plotted together on the same ﬁgure. As shown

on the ﬁgures, the curves of SEAL lie almost on the x-axis due to its extremely low time consumption

compared to the other two methods.

25

0.100.200.300.400.500.600.700.800.901.00Time/sNumber of instaces 0.000.100.200.300.400.500.600.700.800.901.0012345678910111213141516Time/sNumber of attributes (a) Increase of time consumption of SEAL, RP, and GP
against the number of tuples.

(b) Time consumption of SEAL, RP, and GP against the
number of attributes.

Figure 8: Time consumption comparison of SEAL, RP, and GP. The time consumption plots available in Figure 7 are
plotted in comparison with the time consumption plots of RP and GP. Due to the extremely low time consumption of
SEAL, its curves lie almost on the x-axis when drawn in a plot together with the others.

5.2.7. Scalability

We conducted the scalability analysis of SEAL on an SGI UV3000 supercomputer (a detailed

speciﬁcation of the supercomputer is given in Section 5.1). SEAL was tested for its scalability on two

large datasets: HPDS and HIDS. The results are given in Table 4. It is apparent that SEAL is more

eﬃcient than RP, GP, and DC; in fact, RP and GP did not even converge after 100 hours (the time

limit of the batch scripts were set to 100 h). Both RP and GP use recursive loops to achieve optimal

perturbation, which slows down the perturbation process. Therefore, RP and GP are not suitable for

perturbing big data and data streams. DC is eﬀective in perturbing big data, but SEAL performs

better by providing better eﬃciency and utility.

Table 4: Scalability results (in seconds) of the three methods for high dimensional data.

Dataset RP

GP

DC (k=10,000) SEAL (ws=10,000)

HPDS
HIDS

NC within 100h NC within 100h
NC within 100h NC within 100h

526.1168
6.42E+03

97.8238
1.02E+03

12NC: Did not converge

5.2.8. Performance on data streams

We checked the performance of SEAL on data streams with regard to (i) classiﬁcation accuracy

and (ii) M inimum ST D(D − Dp). The latter provides evidence to the minimum guarantee of attack

resistance provided under a particular instance of perturbation. As shown in Figure 9a, the clas-

siﬁcation accuracy of SEAL increases with increasing buﬀer size. This property is valuable for the

perturbation of inﬁnitely growing data streams generated by systems such as smart cyber-physical

26

0.00500.001000.001500.002000.002500.003000.003500.00135791113151719Time consumed/ sNumber of instances in 1000sSEALRPGP0.00500.001000.001500.002000.002500.003000.003500.004000.0012345678910111213141516Time consumed/ sNumber of attributesSEALRPGPsystems. The ﬁgure indicates that when a data stream grows inﬁnitely, the use of smaller window sizes

would negatively aﬀect the utility of the perturbed data. When the window size is large, the utility

of the perturbed data is closer to the utility of the original data stream. We can also notice that DC

performs poorly in terms of classiﬁcation accuracy compared to SEAL. It was previously noticed that

DC works well only for tiny buﬀer sizes such as 5 or 10 [70]. However, according to Figure 9b, the

minimum guarantee of attack resistance drops when the buﬀer size decreases, which restricts the use

of DC with smaller buﬀer sizes. According to Figure 9b, however, SEAL still provides a consistent

minimum guarantee of attack resistance, which allows SEAL to be used with any suitable buﬀer size.

(a) Change of classiﬁcation accuracy against the buﬀer size.

(b) Change of minimum ST D(D − Dp) consumption
against the buﬀer size.

Figure 9: Dynamics of classiﬁcation accuracy and the minimum STD(D-Dp) against increasing buﬀer size. During
the experiments, the privacy budget ((cid:15)) was maintained at 1. The minimum ST D(D − Dp) represents the minimum
(attribute) value of the standard deviation of the diﬀerence between the normalized attributes of the original data (LRDS
dataset) and the perturbed data. The classiﬁcation accuracy was obtained by classifying each perturbed datasets using
the J48 classiﬁcation algorithm.

6. Discussion

The proposed privacy-preserving mechanism (named SEAL) for big data and data streams performs

data perturbation based on Chebyshev polynomial interpolation and the application of a Laplacian

mechanism for noise addition. SEAL uses the ﬁrst four orders of Chebyshev polynomials of the ﬁrst kind

for the polynomial interpolation of a particular dataset. Although Legendre polynomials would oﬀer

a better approximation of the original data during interpolation, Chebyshev polynomials are simpler

to calculate and provide improved privacy; a higher interpolation error, i.e. increased deviation from

the original data would intuitively provide greater privacy than Legendre polynomials. Moreover, we

intend to maintain the spatial arrangement of the original data, and this requirement is fully satisﬁed

27

0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%Classification accuracyBuffer sizeSEALDC0.60000.70000.80000.90001.00001.10001.20001.30001.40001.5000Minimum STD(D-DP)Buffer sizeSEALDCby Chebyshev interpolation. During the interpolation, SEAL adds calibrated noise using the Laplacian

mechanism to introduce randomization, and henceforth privacy, to the perturbed data. The Laplacian

noise allows the interpolation process to be performed with an anticipated random error for the root

mean squared error minimization. We follow the conventions of diﬀerential privacy for noise addition,

the introduction of noise is in accordance with the characteristic privacy budget (cid:15). The privacy budget

((cid:15)) allows users (data curators) of SEAL to adjust the amount of noise. Smaller values of (cid:15) (usually

less than 1 but greater than 0) add more noise to generate more randomization, whereas large values of

(cid:15) add less noise and generate less randomization. The privacy budget is especially useful for multiple

data release, where the data curator can apply proper noise in the perturbation process in consecutive

data releases. SEAL’s ability to maintain the shape of the original data distribution after noise addition

is a clear advantage, and enables SEAL to provide convincingly higher utility than a standard local

diﬀerentially private algorithm. This characteristic may come at a price, and the privacy enforced by

a standard diﬀerentially private mechanism can be a little higher than that of SEAL.

The experimental results of SEAL show that it performs well on both static data and data streams.

We evaluated SEAL in terms of classiﬁcation accuracy, attack resistance, time complexity, scalability,

and data stream performance. We tested each of these parameters using seven datasets, ﬁve classiﬁ-

cation algorithms, and three attack methods. SEAL outperforms the comparable methods: RP, GP,

and DC in all these areas, proving that SEAL is an excellent choice for privacy preservation of data

produced by SCPS and related technologies. SEAL produces high utility perturbed data in terms of

classiﬁcation accuracy, due to its ability to preserve the underlying characteristics such as the shape

of the original data distribution. Although we apply an extensive amount of noise by using a small (cid:15)

value, SEAL still tries to maintain the shape of the original data. The experiments show that even

in extremely noisy perturbation environments, SEAL can provide higher utility compared to similar

perturbation mechanisms, as shown in Section 5.1. SEAL shows excellent resistance with regard to

data reconstruction attacks, proving that it oﬀers excellent privacy. SEAL takes several steps to en-

hance the privacy of the perturbed data, namely (1) approximation through noisy interpolation, (2)

scaling/normalization, and (3) data shuﬄing. These three steps help it outperform the other, similar

perturbation mechanisms in terms of privacy.

In Section 5.1 we showed that SEAL has linear time complexity, O(n). This characteristic is crucial

for big data and data streams. The scalability experiments conﬁrm that SEAL processes big datasets

and data streams very eﬃciently. As shown in Figure 9, SEAL also oﬀers signiﬁcantly better utility and

28

attack resistance than data condensation. The amount of time spent by SEAL in processing one data

record is around 0.03 to 0.09 milliseconds, which means that SEAL can perturb approximately 11110

to 33330 records per second. We note that runtime speed depends on the computing environment, such

as CPU speed, memory speed, and disk IO speeds. The processing speed of SEAL in our experimental

setup suits many practical examples of data streams, e.g. Sense your City (CITY)13 and NYC Taxi

cab (TAXI)14 [90]. The results clearly demonstrate that SEAL is an eﬃcient and reliable privacy

preserving mechanism for practical big data and data stream scenarios.

7. Conclusion

In this paper, we proposed a solution for maintaining data privacy in large-scale data publishing

and analysis scenarios, which is becoming an important issue in various environments, such as smart

cyber-physical systems. We proposed a novel algorithm named SEAL to perturb data to maintain

data privacy. Linear time complexity (O(n)) of SEAL allows it to work eﬃciently with continuously

growing data streams and big data. Our experiments and comparisons indicate that SEAL produces

higher classiﬁcation accuracy, eﬃciency, and scalability while preserving better privacy with higher

attack resistance than similar methods. The results prove that SEAL suits the dynamic environments

presented by smart cyber-physical environments very well. SEAL can be an eﬀective privacy-preserving

mechanism for smart cyber-physical systems such as vehicles, grid, healthcare systems, and homes, as

it can eﬀectively perturb continuous data streams generated by sensors monitoring an individual or

group of individuals and process them on the edge/fog devices before transmission to cloud systems

for further analysis.

The current conﬁguration of SEAL does not allow distributed data perturbation, and it limits the

utility only to privacy-preserving data classiﬁcation. A potential future extension of SEAL can address

a distributed perturbation scenario that would allow SEAL to perturb sensor outputs individually while

capturing the distinct latencies introduced by the sensors. SEAL could then combine the individually

perturbed data using the corresponding timestamps and latencies to produce the privacy-protected

data records. Further investigation on privacy parameter tuning would allow extended utility towards

other areas such as descriptive statistics.

13Sense your City is an urban environmental monitoring project that used crowd-sourcing to deploy sensors at 7 cities

across 3 continents in 2015 with about 12 sensors per city, and it generates 7000 messages/ sec.

14NYC Taxi cab (TAXI) produces a stream of smart transportation messages at the rate of 4000 messages/sec. The

messages arrive from 2M trips taken on 20,355 New York city taxis equipped with GPS in 2013.

29

Appendix A. Chebyshev Polynomials of the First Kind

Deﬁnition 3. The Chebyshev polynomial Tn(x) of the ﬁrst kind is a polynomial in x of degree n,

deﬁned by the relation,

Tn(x) = cos nθ when x = cos θ

(A.1)

From Equation A.1, we can deduce the ﬁrst ﬁve (n = 0, 1, 2, 3, 4) Chebyshev polynomials using

Equation A.2 to Equation A.6, which are normalized such that Tn(1) = 1, and x ∈ [−1, 1].

T0(x) = 1

T1(x) = x

T2(x) = 2x2 − 1

T3(x) = 4x3 − 3x

T4(x) = 8x4 − 8x2 + 1

(A.2)

(A.3)

(A.4)

(A.5)

(A.6)

Furthermore, we can represent any Chebyshev polynomial of the ﬁrst kind using the recurrence

relation given in Equation A.7, where T0(x) = 1 and T1(x) = x.

Tn+1(x) = 2xTn(x) − Tn−1(x)

(A.7)

Appendix B. Least Square Fitting

In least square ﬁtting, vertical least squares ﬁtting proceeds by ﬁnding the sum of squares of the

vertical derivations R2 (refer Equation B.1) of a set of n data points [75].

R2 ≡

(cid:88) (cid:2)f (xi, a1, a2, . . . , an) − yi

(cid:3)2

(B.1)

Now, we can choose to minimize the quantity given in Equation B.2, which can be considered as an

average approximation error. This is also referred to as the root mean square error in approximating

(xi, yi) by a function f (xi, a1, a2, . . . , an).

30

E =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

(cid:2)f (xi, a1, a2, . . . , an) − yi

(cid:3)2

(B.2)

Let’s assume that f (x) is in a known class of functions, C. It can be shown that a function ˆf ∗

which is most likely to equal to f will also minimize Equation B.3 among all functions ˆf (x) in C. This

is called the least squares approximation to the data (xi, yi).

E =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

(cid:104) ˆf (xi, a1, a2, . . . , an) − yi

(cid:105)2

(B.3)

Minimizing E is equivalent to minimizing R2, although the minimum values will be diﬀerent. Thus

we seek to minimize Equation B.1, which result in the condition given in Equation B.4 for i = 1, . . . , n.

∂(R2)
∂ai

= 0

(B.4)

Let’s consider f (x) = mx + b for a linear ﬁt. Thus we attempt to minimize Equation B.5, where b

and m are allowed to vary arbitrarily.

R2 =

n
(cid:88)

i=1

[mxi + b − yi]2

(B.5)

Now, according to Equation B.4, the choices of b and m that minimize R2 satisfy, Equation B.6

and Equation B.7.

∂R2
∂b
∂R2
∂m

= 0

= 0

Equation B.6 and Equation B.7 result in Equation B.8 and Equation B.9.

∂R2
∂b

=

n
(cid:88)

i=1

2 [mxi + b − yi]

∂R2
∂m

=

n
(cid:88)

i=1

2

(cid:104)
mx2

i + bxi − xiyi

(cid:105)

(B.6)

(B.7)

(B.8)

(B.9)

From Equation B.8 and Equation B.9, we can generate the linear system shown in Equation B.10

31

which can be represented by the matrix form shown in Equation B.11. Now, we can solve Equation

B.12, to ﬁnd values of a and b to obtain the corresponding linear ﬁt of f (x) = mx + b.

nb +





n
(cid:88)

i=1



xi

 m =

n
(cid:88)

i=1

yi





n
(cid:88)



xi

 b +





n
(cid:88)

i=1

i=1



x2
i

 m =

n
(cid:88)

i=1

xiyi






n









(cid:0)(cid:80)n

i=1 xi
i=1 x2
i

(cid:1)




(cid:1)




b

m


 =









(cid:80)n

i=1 yi

(cid:80)n

i=1 xiyi

(cid:0)(cid:80)n

i=1 xi

(cid:1)

(cid:0)(cid:80)n

So,










b

m


 =




n

(cid:0)(cid:80)n

i=1 xi

(cid:1)

(cid:0)(cid:80)n

(cid:0)(cid:80)n

(cid:1)




(cid:1)

i=1 xi
i=1 x2
i



−1 




(cid:80)n

i=1 yi

(cid:80)n

i=1 xiyi






(B.10)

(B.11)

(B.12)

Appendix C. Privacy-Preserving Polynomial Model Generation

Consider a dataset {(xi, yi)|1 ≤ i ≤ n}, and let

ˆf (x) = a1ϕ1(x) + a2ϕ2(x) + · · · + amϕm(x)

(C.1)

where, a1, a2, . . . , am are coeﬃcients and ϕ1(x), ϕ2(x), . . . , ϕm(x) are Chebeshev polynomials of ﬁrst

kind,

ϕ1(x) = T0(x) = 1

ϕ2(x) = T1(x) = x

ϕn(x) = Tn+1(x) = 2xTn(x) − Tn−1(x)

(C.2)

(C.3)

(C.4)

Assume that the data {xi} are chosen from an interval [α, β]. The Chebyshev polynomials can be

modiﬁed as given in Equation C.5,

32

ϕk(x) = Tk−1

(cid:18) 2x − α − β
β − α

(cid:19)

(C.5)

The approximated function ˆf of degree (m − 1) can be given by Equation C.1, where the degree of (ϕk)

is k − 1. We will assume the interval [α, β] = [0, 1] and construct the model accordingly. According to

Equation C.6 when [α, β] = [0, 1], we get Equation C.6.

ϕk(x) = Tk−1

(cid:18) 2x − α − β
β − α

(cid:19)

= Tk−1(2x − 1)

(C.6)

From Equation C.1 and Equation C.6 we have the following equations for m = 4.

ϕ1(x) = T0(2x − 1) = 1

ϕ2(x) = T1(2x − 1) = 2x − 1

ϕ3(x) = T2(2x − 1) = 8x2 − 8x + 1

ϕ4(x) = T3(2x − 1) = 32x3 − 48x2 + 18x − 1

Equation C.12 deﬁnes ˆf (x) when m = 4.

ˆf (x) = a1ϕ1(x) + a2ϕ2(x) + a3ϕ3(x) + a4ϕ4(x)

ˆf (x) = a1(1) + a2(2x − 1) + a3(8x2 − 8x + 1) + a4(32x3 − 48x2 + 18x − 1)

(C.7)

(C.8)

(C.9)

(C.10)

(C.11)

(C.12)

Let the actual input be yi, where i = 1 to n. The error of the approximated input can be determined

by Equation C.13.

ei = ˆf (xi) − yi

(C.13)

We need to determine the values of a1, a2, a3, and a4 in such a way that the errors (ei) are small.

In order to determine the best values for a1, a2, a3, and a4, we use the root mean square error given in

Equation C.14.

E =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

(cid:104) ˆf (xi) − yi

(cid:105)2

33

(C.14)

Let’s take the least squares ﬁtting of ˆf (x) of the class of functions C which minimizes E as ˆf ∗(x).

We can obtain ˆf ∗(x) by minimizing E. Thus we seek to minimize M (a1, a2, a3, a4) which is given in

Equation C.15.

M (a1, a2, a3, a4) =

n
(cid:88)

i=1

(cid:104)
a1 + a2(2x − 1) + a3(8x2 − 8x + 1) + a4(32x3 − 48x2 + 18x − 1) − yi

(cid:105)2

(C.15)

The values of a1, a2, a3, and a4 that minimize M (a1, a2, a3, a4) will satisfy the expressions given in

Equation C.16, Equation C.17, Equation C.18, and Equation C.19.

∂M (a1,a2,a3,a4)
∂a1

=

(cid:16)(cid:80)n

i=1[a1+a2(2x−1)+a3(8x2−8x+1)+a4(32x3−48x2+18x−1)−yi]2(cid:17)

∂

∂a1

= 0 (C.16)

∂M (a1,a2,a3,a4)
∂a2

=

(cid:16)(cid:80)n

i=1[a1+a2(2x−1)+a3(8x2−8x+1)+a4(32x3−48x2+18x−1)−yi]2(cid:17)

∂

∂a2

= 0 (C.17)

∂M (a1,a2,a3,a4)
∂a3

=

(cid:16)(cid:80)n

i=1[a1+a2(2x−1)+a3(8x2−8x+1)+a4(32x3−48x2+18x−1)−yi]2(cid:17)

∂

∂a3

= 0 (C.18)

∂M (a1,a2,a3,a4)
∂a4

=

(cid:16)(cid:80)n

i=1[a1+a2(2x−1)+a3(8x2−8x+1)+a4(32x3−48x2+18x−1)−yi]2(cid:17)

∂

∂a4

= 0 (C.19)

Appendix C.1. Utilizing diﬀerential privacy to introduce randomization to the approximation process

To decide the amount of noise, we have to determine the sensitivity of the noise addition process.

Given that we add the noise to the approximated values of

ˆf (x), the sensitivity (∆f ) can be deﬁned

using Equation C.20, which is the maximum diﬀerence between the highest and the lowest possible

output values of ˆf (x). Since the input dataset is normalized within the bounds of 0 and 1, the minimum

possible input or output is 0 while the maximum possible input or output is 1. Therefore, we deﬁne

the sensitivity of the noise addition process to be 1.

34

∆f = (cid:107)max(yi) − min(yi+1)(cid:107)1 = (1 − 0) = 1

(C.20)

Now we add random Laplacian to each expression given in Equations C.16 - C.19, according to

Equation C.21 with a sensitivity (∆f ) of 1 and a privacy budget of (cid:15) as shown in Equations C.22,

C.25, C.28 and C.31. In the process of adding the Laplacian noise, we choose Laplacian noise with a

position (location) of 0 as the idea is to keep the local minima of RMSE around 0 during the process of

interpolation. Here we try to randomize the process of obtaining the local minima of the mean squared

error to generate the value for the coeﬃcients (a1, a2, a3, a4) with randomization. The diﬀerentially

private Laplacian mechanism can be represented by Equation C.21, where ∆f is the sensitivity of the

process, and (cid:15) is the privacy budget.

P F (D) =

(cid:15)
2∆f

e− |x−F (D)|

∆F

(C.21)

Equation C.22 shows the process of using the Laplacian mechanism to introduce noise to the

RMSE minimization of the polynomial interpolation process. Here, we try to introduce an error to the

partial derivative of Equation C.15 with respect to a1. By doing so, it guarantees that Equation C.22

contributes with an error to the process of ﬁnding the coeﬃcients for a1, a2, a3, and a4, which is given

in Equation C.36. Since the sensitivity (∆f ) of the noise addition process is 1, as deﬁned in Equation

C.20, the scale (spread) of the Laplacian noise is 1/(cid:15). We restrict the position (µ) of the Laplacian

noise at 0 as the goal is to achieve the global minima keeping the RMSE at 0 after the randomization.

∂M (a1,a2,a3,a4)
∂a1

=

∂

(cid:16)(cid:80)n

i=1[a1+a2(2x−1)+a3(8x2−8x+1)+a4(32x3−48x2+18x−1)+Lapi( ∆f

(cid:15) )−yi]2(cid:17)

∂a1

= 0 (C.22)

After applying the partial derivation on Equation C.22 with respect to a1, we can obtain Equation

C.23 which leads to obtaining Equation C.24.

(cid:80)n

i=1 2

(cid:104)
a1 + a2(2x − 1) + a3(8x2 − 8x + 1) + a4(32x3 − 48x2 + 18x − 1) + Lapi( ∆f

(cid:15) ) − yi

(cid:105)

= 0 (C.23)

Let’s use mij to denote the coeﬃcients, and bi to represent the constants in the right hand side of

35

the equal symbol in the factorised Equations C.24, C.27, C.30, and C.33.



a1 n
(cid:124)(cid:123)(cid:122)(cid:125)
m11

+a2


2





n
(cid:88)



xi

 − n

i=1

(cid:123)(cid:122)
m12






(cid:125)



+a3


8

(cid:124)





n
(cid:88)



x2
i

 − 8





n
(cid:88)



xi

 + n

i=1

i=1

+






(cid:125)

(cid:124)


a4


32

(cid:124)





n
(cid:88)



x3
i

 − 48





n
(cid:88)





x2
i

 + 18



i=1

i=1

(cid:123)(cid:122)
m14

(cid:123)(cid:122)
m13



xi

 − n

n
(cid:88)

i=1






(cid:125)

=

n
(cid:88)

i=1
(cid:124)

n
(cid:88)

yi −

Lapi

i=1

(cid:123)(cid:122)
b1

(cid:18) ∆f
(cid:15)

(cid:19)

(cid:125)

(C.24)

We repeat the same process of applying noise in Equation C.22 to apply noise to Equation C.17.

∂M (a1,a2,a3,a4)
∂a2

=

∂

(cid:16)(cid:80)n

i=1[a1+a2(2x−1)+a3(8x2−8x+1)+a4(32x3−48x2+18x−1)+Lapi( ∆f

(cid:15) )−yi]2(cid:17)

∂a2

= 0 (C.25)

After solving Equation C.25, we can obtain Equation C.26 which leads to obtaining Equation C.27.

(cid:104)

(cid:80)n

i=1 2

a1 + a2(2x − 1) + a3(8x2 − 8x + 1) + a4(32x3 − 48x2 + 18x − 1) + Lapi( ∆f

(cid:15) ) − yi

(cid:105)

(2xi − 1) = 0 (C.26)

36



a1


2

(cid:124)





n
(cid:88)



xi

 − n

i=1

(cid:123)(cid:122)
m21



a4


64

(cid:124)





n
(cid:88)



x2
i

 − 4





n
(cid:88)

i=1

i=1



xi

 + n

+






(cid:125)



x3
i

 − 24



n
(cid:88)



x2
i

 + 10





n
(cid:88)

(cid:123)(cid:122)
m22







(cid:125)



+a2


4

(cid:124)





n
(cid:88)

i=1



a3


16

(cid:124)

i=1

(cid:123)(cid:122)
m23



x3
i

 + 84





n
(cid:88)





n
(cid:88)



x4
i

 − 128





n
(cid:88)

i=1

i=1

i=1



xi

 − n

+






(cid:125)

i=1





x2
i

 − 20





xi

 + n

n
(cid:88)

i=1






(cid:125)





n
(cid:88)

i=1



xiyi

 −

n
(cid:88)

i=1



yi −


2

= 2

(cid:124)

(cid:123)(cid:122)
m24




n
(cid:88)

xiLapi

i=1

(cid:123)(cid:122)
b2



(cid:19)

 −

(cid:18) ∆f
(cid:15)

n
(cid:88)

i=1

Lapi

(cid:19)

(cid:18) ∆f
(cid:15)

(C.27)






(cid:125)

Similarly, we can obtain Equation C.30 and Equation C.33 after introducing the calibrated Lapla-

cian noise to Equation C.18 and Equation C.19 respectively.

∂M (a1,a2,a3,a4)
∂a3

=

∂

(cid:16)(cid:80)n

i=1[a1+a2(2x−1)+a3(8x2−8x+1)+a4(32x3−48x2+18x−1)+Lapi( ∆f

(cid:15) )−yi]2(cid:17)

∂a3

= 0 (C.28)

(cid:104)

(cid:80)n

i=1 2

a1 + a2(2x − 1) + a3(8x2 − 8x + 1) + a4(32x3 − 48x2 + 18x − 1) + Lapi( ∆f

(cid:15) ) − yi

(cid:105)

(8x2 − 8x + 1) = 0 (C.29)

37



a1


8

(cid:124)





n
(cid:88)



x2
i

 − 8





n
(cid:88)

i=1

i=1



xi

 + n






(cid:125)



+a2


16



x3
i

 − 24





n
(cid:88)

i=1





n
(cid:88)

i=1





x2
i

 + 10





xi

 − n

n
(cid:88)

i=1

+






(cid:125)

(cid:123)(cid:122)
m31




a3


64



(cid:124)



x4
i

 − 128

n
(cid:88)

i=1





n
(cid:88)

i=1

(cid:123)(cid:122)
m32




x2
i

 − 16



(cid:124)





x3
i

 + 80



n
(cid:88)

i=1

(cid:123)(cid:122)
m33



xi

 + n

+






(cid:125)

n
(cid:88)

i=1







n
(cid:88)



x5
i

 − 640





n
(cid:88)

i=1

i=1





x4
i

 + 560





x3
i

 − 200

n
(cid:88)

i=1





n
(cid:88)

x2
i

 + 26



i=1

i=1



n
(cid:88)



xi

 − n






(cid:125)



a4


256

(cid:124)



8



n
(cid:88)

i=1



(cid:19)

 − 8

=




8





n
(cid:88)

i=1

x2
i Lapi

(cid:18) ∆f
(cid:15)

(cid:123)(cid:122)
m34



x2
i yi

 − 8





n
(cid:88)

xiLapi

i=1

(cid:18) ∆f
(cid:15)





n
(cid:88)

xiLapi

i=1

(cid:123)(cid:122)
b3



(cid:19)

 +

(cid:18) ∆f
(cid:15)

n
(cid:88)

i=1

yi−

(C.30)



(cid:19)

 +

n
(cid:88)

i=1

Lapi

(cid:19)

(cid:18) ∆f
(cid:15)






(cid:125)

(cid:16)(cid:80)n

i=1[a1+a2(2x−1)+a3(8x2−8x+1)+a4(32x3−48x2+18x−1)+Lapi( ∆f

(cid:15) )−yi]2(cid:17)

∂a4

= 0 (C.31)

(cid:124)

∂

∂M (a1,a2,a3,a4)
∂a4

=

(cid:104)

(cid:80)n

i=1 2

a1 + a2(2x − 1) + a3(8x2 − 8x + 1) + a4(32x3 − 48x2 + 18x − 1) + Lapi( ∆f

(cid:15) ) − yi

(cid:105)

(32x3 − 48x2 + 18x − 1) = 0

(C.32)

38



xi

 − n

+






(cid:125)



a1


32





n
(cid:88)

i=1



x3
i

 − 48





n
(cid:88)



x2
i

 + 18





n
(cid:88)

i=1

i=1

(cid:124)

(cid:123)(cid:122)
m41


x4
i

 − 128



a2


64





n
(cid:88)

i=1





n
(cid:88)

i=1



a3


256

(cid:124)





n
(cid:88)

i=1





x5
i

 − 640





x4
i

 + 560

n
(cid:88)

i=1





x3
i

 + 84





x2
i

 − 20

n
(cid:88)

i=1

(cid:123)(cid:122)
m42



xi

 + n

+






(cid:125)





n
(cid:88)

i=1







n
(cid:88)



x3
i

 − 200





n
(cid:88)



n
(cid:88)

x2
i

 + 26



i=1

(cid:123)(cid:122)
m43

i=1

i=1



xi

 − n

+






(cid:125)

(cid:124)



a4


1024

(cid:124)





x6
i

 − 3072







n
(cid:88)

i=1

n
(cid:88)

i=1





x5
i

 + 3456



n
(cid:88)

i=1



x4
i

 − 1792

(cid:123)(cid:122)
m44





n
(cid:88)

i=1





x3
i

 + 420





x2
i

 − 36

n
(cid:88)

i=1





n
(cid:88)

i=1



x

 + n






(cid:125)







x3
i yi

 − 48







x2
i yi

 + 18





xiyi

 −

n
(cid:88)

i=1

n
(cid:88)

i=1

yi−

n
(cid:88)

i=1

n
(cid:88)

i=1

32







n
(cid:88)

i=1

=




32

(cid:124)

x3
i Lapi



(cid:19)



 − 48



(cid:18) ∆f
(cid:15)

n
(cid:88)

i=1

x2
i Lapi



(cid:19)



 + 18



(cid:18) ∆f
(cid:15)

n
(cid:88)

i=1

xiLapi



(cid:19)

 −

(cid:18) ∆f
(cid:15)

n
(cid:88)

i=1

Lapi

(cid:19)

(cid:18) ∆f
(cid:15)

(cid:123)(cid:122)
b4






(cid:125)

(C.33)

Let’s consider the coeﬃcients (mij) and the constants (bi) in the Equations C.24, C.27, C.30, and

C.33. Using mij and bi we can form a linear system which can be denoted by Equation C.34.

CA = B

(C.34)

Where,



C =











m11 m12 m13 m14

m21 m22 m23 m24

m31 m32 m33 m34

m41 m42 m43 m44

39













(C.35)

A = [a1, a2, a3, a4]T

B = [b1, b2, b3, b4]T

(C.36)

(C.37)

References

[1] G. De Francisci Morales, A. Bifet, L. Khan, J. Gama, W. Fan, Iot big data stream mining, in:

Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and

Data Mining, ACM, 2016, pp. 2119–2120.

[2] E. Bertino, D. Lin, W. Jiang, A survey of quantiﬁcation of privacy preserving data mining algo-

rithms, in: Privacy-preserving data mining, Springer, 2008, pp. 183–205.

[3] Q. Zhang, L. T. Yang, Z. Chen, Privacy preserving deep computation model on cloud for big data

feature learning, IEEE Transactions on Computers 65 (5) (2016) 1351–1362.

[4] Y. Wen, J. Liu, W. Dou, X. Xu, B. Cao, J. Chen, Scheduling workﬂows with privacy protection

constraints for big data applications on cloud, Future Generation Computer Systemsdoi:10.

1016/j.future.2018.03.028.

URL http://www.sciencedirect.com/science/article/pii/S0167739X17307379

[5] M. Xue, P. Papadimitriou, C. Ra¨ıssi, P. Kalnis, H. K. Pung, Distributed privacy preserving data

collection, in: International Conference on Database Systems for Advanced Applications, Springer,

2011, pp. 93–107.

[6] M. Backes, P. Berrang, O. Goga, K. P. Gummadi, P. Manoharan, On proﬁle linkability despite

anonymity in social media systems, in: Proceedings of the 2016 ACM on Workshop on Privacy in

the Electronic Society, ACM, 2016, pp. 25–35.

[7] K. Yang, Q. Han, H. Li, K. Zheng, Z. Su, X. Shen, An eﬃcient and ﬁne-grained big data access

control scheme with privacy-preserving policy, IEEE Internet of Things Journal 4 (2) (2017) 563–

571.

[8] D. Vatsalan, Z. Sehili, P. Christen, E. Rahm, Privacy-preserving record linkage for big data:

Current approaches and research challenges, in: Handbook of Big Data Technologies, Springer,

2017, pp. 851–895.

40

[9] K. Chen, L. Liu, A random rotation perturbation approach to privacy preserving data classiﬁca-

tion.

URL https://corescholar.libraries.wright.edu/knoesis/916/

[10] K. Chen, L. Liu, Geometric data perturbation for privacy preserving outsourced data mining,

Knowledge and Information Systems 29 (3) (2011) 657–695.

[11] J. Li, D. Lin, A. C. Squicciarini, J. Li, C. Jia, Towards privacy-preserving storage and retrieval

in multiple clouds, IEEE Transactions on Cloud Computing 5 (3) (2017) 499–509. doi:10.1109/

TCC.2015.2485214.

[12] F. Kerschbaum, M. H¨arterich, Searchable encryption to reduce encryption degradation in ad-

justably encrypted databases, in: IFIP Annual Conference on Data and Applications Security

and Privacy, Springer, 2017, pp. 325–336.

[13] K. Gai, M. Qiu, H. Zhao, J. Xiong, Privacy-aware adaptive data encryption strategy of big

data in cloud computing, in: Cyber Security and Cloud Computing (CSCloud), 2016 IEEE 3rd

International Conference on, IEEE, 2016, pp. 273–278.

[14] S. Agrawal, J. R. Haritsa, A framework for high-accuracy privacy-preserving mining, in: Data

Engineering, 2005. ICDE 2005. Proceedings. 21st International Conference on, IEEE, 2005, pp.

193–204.

[15] H. Xu, S. Guo, K. Chen, Building conﬁdential and eﬃcient query services in the cloud with rasp

data perturbation, IEEE transactions on knowledge and data engineering 26 (2) (2014) 322–335.

[16] K. Muralidhar, R. Parsa, R. Sarathy, A general additive data perturbation method for database

security, management science 45 (10) (1999) 1399–1415.

[17] J. A. Fox, Randomized response and related methods: Surveying Sensitive Data, Vol. 58, SAGE

Publications, 2015.

[18] A. Machanavajjhala, D. Kifer, Designing statistical privacy for your data, Communications of the

ACM 58 (3) (2015) 58–67.

[19] B. Niu, Q. Li, X. Zhu, G. Cao, H. Li, Achieving k-anonymity in privacy-aware location-based

services, in: INFOCOM, 2014 Proceedings IEEE, IEEE, 2014, pp. 754–762.

41

[20] Y. Zhang, W. Tong, S. Zhong, On designing satisfaction-ratio-aware truthful incentive mecha-

nisms for k-anonymity location privacy, IEEE Transactions on Information Forensics and Security

11 (11) (2016) 2528–2541.

[21] A. Machanavajjhala, J. Gehrke, D. Kifer, M. Venkitasubramaniam, l-diversity: Privacy beyond

k-anonymity, in: Data Engineering, 2006. ICDE’06. Proceedings of the 22nd International Con-

ference on, IEEE, 2006, pp. 24–24.

[22] L. Zhang, S. Jajodia, A. Brodsky, Information disclosure under realistic assumptions: Privacy

versus optimality, in: Proceedings of the 14th ACM conference on Computer and communications

security, ACM, 2007, pp. 573–583.

[23] S. R. Ganta, S. P. Kasiviswanathan, A. Smith, Composition attacks and auxiliary information in

data privacy, in: Proceedings of the 14th ACM SIGKDD international conference on Knowledge

discovery and data mining, ACM, 2008, pp. 265–273.

[24] R. C.-W. Wong, A. W.-C. Fu, K. Wang, P. S. Yu, J. Pei, Can the utility of anonymized data be

used for privacy breaches?, ACM Transactions on Knowledge Discovery from Data (TKDD) 5 (3)

(2011) 16.

[25] C. Dwork, The diﬀerential privacy frontier, in: Theory of Cryptography Conference, Springer,

2009, pp. 496–502.

[26] N. Mohammed, R. Chen, B. Fung, P. S. Yu, Diﬀerentially private data release for data mining,

in: Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and

data mining, ACM, 2011, pp. 493–501.

[27] A. Friedman, A. Schuster, Data mining with diﬀerential privacy, in: Proceedings of the 16th

ACM SIGKDD international conference on Knowledge discovery and data mining, ACM, 2010,

pp. 493–502.

[28] W. Wang, L. Chen, Q. Zhang, Outsourcing high-dimensional healthcare data to cloud with per-

sonalized privacy preservation, Computer Networks 88 (2015) 136–148.

[29] Z. Qin, Y. Yang, T. Yu, I. Khalil, X. Xiao, K. Ren, Heavy hitter estimation over set-valued data

with local diﬀerential privacy, in: Proceedings of the 2016 ACM SIGSAC Conference on Computer

and Communications Security, ACM, 2016, pp. 192–203.

42

[30] G. Kellaris, S. Papadopoulos, X. Xiao, D. Papadias, Diﬀerentially private event sequences over

inﬁnite streams, Proceedings of the VLDB Endowment 7 (12) (2014) 1155–1166.

[31] C. C. Aggarwal, On k-anonymity and the curse of dimensionality, in: Proceedings of the 31st

international conference on Very large data bases, VLDB Endowment, 2005, pp. 901–909.

[32] K. Mivule, C. Turner, A comparative analysis of data privacy and utility parameter adjustment,

using machine learning classiﬁcation as a gauge, Procedia Computer Science 20 (2013) 414–419.

[33] P. Kieseberg, E. Weippl, Security challenges in cyber-physical production systems, in: Interna-

tional Conference on Software Quality, Springer, 2018, pp. 3–16.

[34] E. Balandina, S. Balandin, Y. Koucheryavy, D. Mouromtsev, Iot use cases in healthcare and

tourism, in: Business Informatics (CBI), 2015 IEEE 17th Conference on, Vol. 2, IEEE, 2015, pp.

37–44.

[35] S. Sridhar, A. Hahn, M. Govindarasu, et al., Cyber-physical system security for the electric power

grid., Proceedings of the IEEE 100 (1) (2012) 210–224.

[36] R. Fernando, R. Ranchal, B. An, L. B. Othman, B. Bhargava, Consumer oriented privacy pre-

serving access control for electronic health records in the cloud, in: Cloud Computing (CLOUD),

2016 IEEE 9th International Conference on, IEEE, 2016, pp. 608–615.

[37] J. Liu, Y. Xiao, S. Li, W. Liang, C. P. Chen, Cyber security and privacy issues in smart grids,

IEEE Communications Surveys & Tutorials 14 (4) (2012) 981–997.

[38] E. Bertino, Data privacy for iot systems: concepts, approaches, and research directions, in: Big

Data (Big Data), 2016 IEEE International Conference on, IEEE, 2016, pp. 3645–3647.

[39] X. Wang, J. Zhang, E. M. Schooler, M. Ion, Performance evaluation of attribute-based encryption:

Toward data privacy in the iot, in: Communications (ICC), 2014 IEEE International Conference

on, IEEE, 2014, pp. 725–730.

[40] T. Kirkham, A. Sinha, N. Parlavantzas, B. Kryza, P. Fremantle, K. Kritikos, B. Aziz, Privacy

aware on-demand resource provisioning for iot data processing, in: International Internet of Things

Summit, Springer, 2015, pp. 87–95.

[41] Y. A. A. S. Aldeen, M. Salleh, M. A. Razzaque, A comprehensive review on privacy preserving

data mining, SpringerPlus 4 (1) (2015) 694.

43

[42] V. S. Verykios, E. Bertino, I. N. Fovino, L. P. Provenza, Y. Saygin, Y. Theodoridis, State-of-the-

art in privacy preserving data mining, ACM Sigmod Record 33 (1) (2004) 50–57.

[43] A. Razaque, S. S. Rizvi, Privacy preserving model: a new scheme for auditing cloud stakeholders,

Journal of Cloud Computing 6 (1) (2017) 7.

[44] D. Wu, B. Yang, R. Wang, Scalable privacy-preserving big data aggregation mechanism, Digital

Communications and Networks 2 (3) (2016) 122–129.

[45] A. Razaque, S. S. Rizvi, Triangular data privacy-preserving model for authenticating all key

stakeholders in a cloud environment, Computers & Security 62 (2016) 328–347.

[46] R. Agrawal, R. Srikant, Privacy-preserving data mining, in: ACM Sigmod Record, Vol. 29, ACM,

2000, pp. 439–450.

[47] S. Datta, On random additive perturbation for privacy preserving data mining, Ph.D. thesis,

University of Maryland, Baltimore County (2004).

[48] K. Liu, H. Kargupta, J. Ryan, Random projection-based multiplicative data perturbation for pri-

vacy preserving distributed data mining, IEEE Transactions on knowledge and Data Engineering

18 (1) (2006) 92–106.

[49] J. Zhong, V. Mirchandani, P. Bertok, J. Harland, µ-fractal based data perturbation algorithm for

privacy protection., in: PACIS, 2012, p. 148.

[50] W. Du, Z. Zhan, Using randomized response techniques for privacy-preserving data mining, in:

Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and

data mining, ACM, 2003, pp. 505–510.

[51] V. Estivill-Castro, L. Brankovic, Data swapping: Balancing privacy against precision in mining

for logic rules, in: DaWaK, Vol. 99, Springer, 1999, pp. 389–398.

[52] C. C. Aggarwal, P. S. Yu, A condensation approach to privacy preserving data mining, in: EDBT,

Vol. 4, Springer, 2004, pp. 183–199.

[53] J. Domingo-Ferrer, J. M. Mateo-Sanz, Practical data-oriented microaggregation for statistical

disclosure control, IEEE Transactions on Knowledge and data Engineering 14 (1) (2002) 189–201.

44

[54] R. C.-W. Wong, J. Li, A. W.-C. Fu, K. Wang, (α, k)-anonymity: an enhanced k-anonymity model

for privacy preserving data publishing, in: Proceedings of the 12th ACM SIGKDD international

conference on Knowledge discovery and data mining, ACM, 2006, pp. 754–759.

[55] N. Li, T. Li, S. Venkatasubramanian, t-closeness: Privacy beyond k-anonymity and l-diversity,

in: Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on, IEEE, 2007, pp.

106–115.

[56] J. Cao, B. Carminati, E. Ferrari, K.-L. Tan, Castle: Continuously anonymizing data streams,

IEEE Transactions on Dependable and Secure Computing 8 (3) (2011) 337–352.

[57] C. Dwork, A. Roth, et al., The algorithmic foundations of diﬀerential privacy, Foundations and

Trends R(cid:13) in Theoretical Computer Science 9 (3–4) (2014) 211–407.

[58] C. Dwork, Diﬀerential privacy: A survey of results, in: International Conference on Theory and

Applications of Models of Computation, Springer, 2008, pp. 1–19.

[59] P. Kairouz, S. Oh, P. Viswanath, Extremal mechanisms for local diﬀerential privacy, in: Advances

in neural information processing systems, 2014, pp. 2879–2887.

[60] ´U. Erlingsson, V. Pihur, A. Korolova, Rappor: Randomized aggregatable privacy-preserving or-

dinal response, in: Proceedings of the 2014 ACM SIGSAC conference on computer and commu-

nications security, ACM, 2014, pp. 1054–1067.

[61] G. Cormode, S. Jha, T. Kulkarni, N. Li, D. Srivastava, T. Wang, Privacy at scale: Local diﬀerential

privacy in practice, in: Proceedings of the 2018 International Conference on Management of Data,

ACM, 2018, pp. 1655–1658.

[62] S. Wold, K. Esbensen, P. Geladi, Principal component analysis, Chemometrics and intelligent

laboratory systems 2 (1-3) (1987) 37–52.

[63] F. W. Scholz, Maximum Likelihood Estimation, Wiley Online Library, 2006. doi:10.1002/

0471667196.ess1571.pub2.

URL https://onlinelibrary.wiley.com/doi/abs/10.1002/0471667196.ess1571.pub2

[64] C. C. Aggarwal, S. Y. Philip, A general survey of privacy-preserving data mining models and

algorithms, in: Privacy-preserving data mining, Springer, 2008, pp. 11–52.

45

[65] K. Chen, L. Liu, Privacy preserving data classiﬁcation with rotation perturbation, in: Data

Mining, Fifth IEEE International Conference on, IEEE, 2005, pp. 1–4.

[66] Z. Huang, W. Du, B. Chen, Deriving private information from randomized data, in: Proceedings

of the 2005 ACM SIGMOD international conference on Management of data, ACM, 2005, pp.

37–48.

[67] J. Domingo-Ferrer, J. Soria-Comas, Steered microaggregation: A uniﬁed primitive for anonymiza-

tion of data sets and data streams, in: Data Mining Workshops (ICDMW), 2017 IEEE Interna-

tional Conference on, IEEE, 2017, pp. 995–1002.

[68] X. Zhang, W. Dou, J. Pei, S. Nepal, C. Yang, C. Liu, J. Chen, Proximity-aware local-recoding

anonymization with mapreduce for scalable big data privacy preservation in cloud, IEEE trans-

actions on computers 64 (8) (2015) 2293–2307.

[69] C. C. Aggarwal, P. S. Yu, On static and dynamic methods for condensation-based privacy-

preserving data mining, ACM Transactions on Database Systems (TODS) 33 (1) (2008) 2.

[70] M. A. P. Chamikara, P. Bertok, D. Liu, S. Camtepe, I. Khalil, Eﬃcient data perturbation for

privacy preserving and accurate data stream mining, Pervasive and Mobile Computingdoi:10.

1016/j.pmcj.2018.05.003.

[71] M. A. P. Chamikara, P. Bertok, D. Liu, S. Camtepe, I. Khalil, Eﬃcient privacy preservation of

big data for accurate data mining, Information Sciencesdoi:10.1016/j.ins.2019.05.053.

[72] Y. Xu, K. Wang, A. W.-C. Fu, R. She, J. Pei, Privacy-preserving data stream classiﬁcation,

Privacy-Preserving Data Mining (2008) 487–510.

[73] F. Li, J. Sun, S. Papadimitriou, G. A. Mihaila, I. Stanoi, Hiding in the crowd: Privacy preservation

on evolving streams through correlation tracking, in: Data Engineering, 2007. ICDE 2007. IEEE

23rd International Conference on, IEEE, 2007, pp. 686–695.

[74] J. C. Mason, D. C. Handscomb, Chebyshev polynomials, Chapman and Hall/CRC, 2002.

[75] E. W. Weisstein, Least squares ﬁtting.

[76] T.-H. H. Chan, M. Li, E. Shi, W. Xu, Diﬀerentially private continual monitoring of heavy hit-

ters from distributed streams, in: International Symposium on Privacy Enhancing Technologies

Symposium, Springer, 2012, pp. 140–159.

46

[77] Y. Wang, X. Wu, D. Hu, Using randomized response for diﬀerential privacy preserving data

collection., in: EDBT/ICDT Workshops, Vol. 1558, 2016.

[78] M. A. Hammad, M. J. Franklin, W. G. Aref, A. K. Elmagarmid, Scheduling for shared window

joins over data streams, in: Proceedings of the 29th international conference on Very large data

bases-Volume 29, VLDB Endowment, 2003, pp. 297–308.

[79] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, L. Zhang, Deep learning

with diﬀerential privacy, in: Proceedings of the 2016 ACM SIGSAC Conference on Computer and

Communications Security, ACM, 2016, pp. 308–318.

[80] R. Baheti, H. Gill, Cyber-physical systems, The impact of control technology 12 (1) (2011) 161–

166.

[81] A. Alhayajneh, A. Baccarini, G. Weiss, T. Hayajneh, A. Farajidavar, Biometric authentication

and veriﬁcation for medical cyber physical systems, Electronics 7 (12) (2018) 436.

[82] I. H. Witten, E. Frank, M. A. Hall, C. J. Pal, Data Mining: Practical machine learning tools and

techniques, Morgan Kaufmann, 2016.

[83] B. D. Okkalioglu, M. Okkalioglu, M. Koc, H. Polat, A survey: deriving private information from

perturbed data, Artiﬁcial Intelligence Review 44 (4) (2015) 547–569.

[84] S. Lessmann, B. Baesens, H.-V. Seow, L. C. Thomas, Benchmarking state-of-the-art classiﬁcation

algorithms for credit scoring: An update of research, European Journal of Operational Research

247 (1) (2015) 124–136.

[85] B. Sch¨olkopf, C. J. Burges, A. J. Smola, Advances in kernel methods: support vector learning,

MIT press, 1999.

[86] J. R. Quinlan, C4. 5: Programming for machine learning, Morgan Kauﬀmann 38.

[87] D. C. Howell, Fundamental statistics for the behavioral sciences, Nelson Education, 2016.

[88] H. G¨avert, J. Hurri, J. S¨arel¨a, A. Hyv¨arinen, The fastica package for matlab, Lab Comput Inf Sci

Helsinki Univ. Technol.

URL https://research.ics.aalto.fi/ica/fastica/

47

[89] V. Zarzoso, P. Comon, M. Kallel, How fast is fastica?, in: 2006 14th European Signal Processing

Conference, IEEE, 2006, pp. 1–5.

[90] A. Shukla, Y. Simmhan, Benchmarking distributed stream processing platforms for iot applica-

tions, in: Technology Conference on Performance Evaluation and Benchmarking, Springer, 2016,

pp. 90–106.

48

