Intelligent Systems Design for Malware
Classiﬁcation Under Adversarial Conditions

Sean M. Devine∗1, Nathaniel D. Bastian, PhD1,2

1Department of Systems Engineering, U.S. Military Academy, West Point, New York 10996
2Army Cyber Institute, U.S. Military Academy, West Point, New York 10996

16 May 2019

Abstract

The use of machine learning and intelligent systems has become an established practice in
the realm of malware detection and cyber threat prevention. In an environment characterized
by widespread accessibility and big data, the feasibility of malware classiﬁcation without the
use of artiﬁcial intelligence-based techniques has been diminished exponentially. Also charac-
teristic of the contemporary realm of automated, intelligent malware detection is the threat
of adversarial machine learning. Adversaries are looking to target the underlying data and/or
algorithm responsible for the functionality of malware classiﬁcation to map its behavior or cor-
rupt its functionality. The ends of such adversaries are bypassing the cyber security measures
and increasing malware eﬀectiveness. The focus of this research is the design of an intelligent
systems approach using machine learning that can accurately and robustly classify malware
under adversarial conditions. Such an outcome ultimately relies on increased ﬂexibility and
adaptability to build a model robust enough to identify attacks on the underlying algorithm.

Keywords: Cybersecurity, Malware, Intelligent Systems, Adversarial Machine Learning

9
1
0
2

l
u
J

6

]

G
L
.
s
c
[

1
v
9
4
1
3
0
.
7
0
9
1
:
v
i
X
r
a

∗Email: sean.devine@westpoint.edu; Corresponding author

1

 
 
 
 
 
 
1

Introduction

Machine learning and intelligent systems seem to be at the forefront of the application of data

science and analytics to the realm of cyber security and defense. While the contemporary

cyber domain has become characterized by a reliance on big data and a widespread prevalence

of threats, so has the need for increased automation with regard to analytics and data science

techniques for active cyber defense. With regards to the classiﬁcation of malware, intelligent

systems oﬀer a solution to the diminishing feasibility of data classiﬁcation without automation.

By assessing patterns within the data, machine learning classiﬁers are able to eﬀectively label

data as malicious, thus increasing the level of security and the ability of administrators to

more eﬀectively monitor their systems. Such classiﬁers rely on an existing pool of inputs

and classiﬁcations in order to train the classiﬁer. Mathematically, and given viable training

data, machine learning classiﬁers are able to predict classiﬁcation labels with a high degree of

accuracy. Such a fact explains their applicability and eﬀectiveness in classifying malware. In

identifying data-based features of the malware, the classiﬁer can eﬀectively predict whether or

not a given set of features correspond to an identiﬁed type of malware.

From a cyber security perspective, these classiﬁers are extremely beneﬁcial to the area of

malware detection. However, the design of these classiﬁers is ultimately ﬂawed and features key

vulnerabilities that adversaries may exploit in order to diminish the classiﬁer’s eﬀectiveness.

Because these models rely on a pool of training data in order to shape its predictions, this

data serves as a target for potential adversaries who may targeting the classiﬁer’s underlying

algorithm. In perturbing, or poisoning, the training data that these classiﬁer draw upon to

shape its predictions, adversaries can lower the accuracy of these classiﬁers, or even alter the

range of acceptable data, thus facilitating the obscuration of potential malware.

This research attempts to address this issue using an intelligent systems design approach

by demonstrating a method of adding a degree of robustness to the design of classiﬁers in order

to account for the potential data poisoning. This research analyzes the eﬀect of purposefully

training a series of base classiﬁcation models on a variety of data perturbations to determine

which models are best suited for optimal performance under given adversarial conditions.

2

These models will then be combined through a linear stacking method in order to test the

accuracy of classiﬁers that incorporate varying degrees of robustness.

1.1 Literature Review

The foundation of this research is in the ﬁeld on machine learning and intelligent systems as

they stand as the target of such adversarial machine learning attacks. As such, it is important

to have an understanding of these machine learning systems within the realm of intelligent or

expert systems. In deﬁning the unique qualities of intelligent systems, Gregor and Benbasat

(1999) layout the distinguishing factor of these systems as the presence of a “knowledge com-

ponent” or a computer representation of “human tacit and explicit knowledge” [1]. In essence,

such systems seek to use the “knowledge component” as a means of automating computational

tasks. F. Hayes-Roth (1997) oﬀers a more in-depth analysis of basic elements of such systems

that involve this artiﬁcial intelligence factor. He deﬁnes the basic elements of artiﬁcial as

control, inference, and representation [2]. These elements compliment the earlier work of B.

Hayes-Roth (1995) in which she deﬁnes the three basic functions of “intelligent agents.” B.

Hayes-Roth deﬁnes these functions as a process of ﬁrst generating a perception of the dynamic

environment following by a corresponding action to aﬀect such environmental conditions, and

ending with an output based on interpreting the perceptions, drawing inferences, and deter-

mining appropriate actions [3].

In application of these principles, B. Hayes-Roth (1995) oﬀers the concept “adaptive intel-

ligent systems” to combat the notion of designing systems to meet a speciﬁc niche. In contrast,

B. Hayes-Roth proposes instead designing architectures that are ﬁt to classes of niches and

thus are much more adaptive the dynamic conditions within the environment [3]. Practically,

intelligent systems serve to automate the processes of tasks such as pattern detection, cluster

analysis, and intrusion detection [4]. Machine learning systems have even become essential to

big data analytics and data mining. However, its widespread use has led to the identiﬁcation

of various vulnerabilities and its identiﬁcation by Yu (2016) as a major privacy concern [5].

Such vulnerabilities are in part due to the emergence of the adversarial environment. Biggio

et al. (2014) point the overall design of such systems as the major ﬂaw. Essentially, the design

3

of such systems are based on classical methods and evaluation techniques that do not consider

the unique adversarial environment [6].

The concept of the adversarial environment characterizes the current challenges facing

machine learning based classiﬁcation algorithms as they are subject to attacks to degrade their

validity and eﬀectiveness. Lowd and Meek (2005) describe this condition as they introduce the

Adversarial Classiﬁer Reverse Engineering (ACRE) problem which describes the adversary’s

approach to learning about a malware classiﬁer [7]. Huang et al. would complement Lowd and

Meek with their own application of these principles to their concept of adversarial machine

learning. Huang et al. (2011) essentially apply a game theoretic approach to the adversarial

learning problem, an approach in which the adversary is actively attempting to determine

strategy of the classiﬁer in order to increase the eﬀectiveness of malware attacks [8]. They also

reference two forms of attacks within adversarial learning. The ﬁrst form is the exploratory

attack in which the adversary essentially observes the classiﬁers response to tailored instances.

The second form is the causative attack in which the adversary attempts to corrupt the training

data in order to inﬂuence a false mapping and thus produce a poor classiﬁer [8].

Khurana, Mittal, and Joshi (2018) oﬀer an analysis of such attacks in the contemporary

adversarial learning environment as they describe an approach to combating poisoning attacks.

Such attacks involve the poison of open-source intelligence (OSINT) data sources with instances

designed to produce false positives/negatives by threat defense ar) systems [9]. Their approach

consisted of modeling the credibility of OSINT prior to its incorporation into threat detection

AI training data [9]. The concept of the poisoning attack is more speciﬁcally the subject of the

online centroid detection. According to Kloft and Laskov (2012), poisoning attacks attempts

to target the centroid and radius of the area of classiﬁers range of legitimate classiﬁcations.

With the introduction of malicious training points, an adversary can force the centroid to shift

in the direction of the attack. Doing so can allow new attacks to fall within the range of what

the classiﬁer considers legitimate data [10].

The existence of these adversarial capabilities, as well as this initial understanding of the

adversarial model presents the concept of the arms race within adversarial machine learning.

Biggio et al. (2014) present this idea as a means of understanding the relationship between

4

classiﬁer and adversary [6]. The arms race is separated between the reactive arms race, in

which a classiﬁer develops counter measures following an attack, and the proactive arms race

in which classiﬁers attempt to anticipate the adversarys actions based on the understanding

of the adversarial model [6]. Berreno et al. (2006) deﬁne the adversarial model with their

concept of the attack model. The attack model looks to classify adversarial attacks based on

inﬂuence, speciﬁcity, and security violation of the attack. Within the inﬂuence, the model

deﬁnes the attack as wither causative or exploratory. Speciﬁcity expands the classiﬁcation by

identifying the attack as either a targeted attack on a speciﬁc set or point, or indiscriminate.

The ﬁnal level of classiﬁcation is the form of security violation. An integrity attack looks to

inﬂuence the classiﬁer to produce false negatives. An availability attack is broader as it looks

to produce both false positives and false negatives [11]. Such a classiﬁcation of an attack aids

in understanding what Biggio et al. (2014) refer to as the overall adversarial goal, knowledge,

capabilities, and strategy [6].

Inherent to the proactive arms race is understanding the implications the concept has on

classiﬁer design considerations. Within the design of such system, Papernot et al.

(2016)

introduces the concept of the no free lunch theory. The major beneﬁt to machine learning

based intelligent systems is the inherent simplicity of their hypotheses. While increasing the

robustness of such hypothesis would decrease their inherent vulnerabilities, doing so comes at

the price of the accuracy of the system in correctly classifying legitimate inputs [12].

1.2 Research Objectives

The central objective of this research is to respond to the need for a malware classiﬁcation

model that is robust enough to operate eﬀectively within the emerging adversarial environment.

While the development of such models has been the subject of established research, the new

diﬃculty in this ﬁeld is in testing and assessing such models for robustness. The underlying

concern is thus the development for processes of experimentally testing classiﬁcation models.

Involved in this process is the critical task of identifying and utilizing known classiﬁcation

performance metrics and potentially new metrics or measures of eﬀectiveness.

The end result of this research is ultimately a model robust enough to accurately clas-

5

sify malware within an adversarial environment. Increased model robustness will decrease the

model’s susceptibility to poisoning attacks. Such attacks are the mechanism behind which

adversaries alter a machine learning algorithm’s range of acceptability. In achieving this end

result, the notable secondary result is an eﬀective process by which models can be experi-

mentally tested and assessed for eﬀectiveness. Such a product could prove to be useful for

future machine learning model development as intelligent design techniques undergo their own

development to meet the demands of the adversarial environment.

2 Materials and Methods

This research follows the Cross Industry Standard Process for Data Mining (CRISP-DM)

methodology, the stages of which can be seen in Figure 5. The ﬁrst three phases of the

methodology (Business Understanding, Data Understanding, Data Preparation) encompass

the overall initial preparation and foundation for modeling and evaluation. The ﬁnal three

phases of Modeling, Evaluation, and Deployment occur as part of the general approach to

the development and experimental testing and assessment of AI-based malware classiﬁers. for

more details about the CRISP-DM, please refer to [13].

Figure 1: Stages of the CRISP-DM Process Model [13]

6

In an eﬀort to incorporate robustness into the intelligent systems design of malware clas-

siﬁers, this research relies on stacking as a means of building robustness into a supervised

learning-based classiﬁcation model. This approach, as applied to this research, begins with a

comparison of the accuracy of 10 base classiﬁcation models when trained on three data sets of

varying perturbation, or “poisoning”, schemes. This comparison ultimately reveals the most

accurate model per data set. In stacking the top performing models in terms of accuracy, this

research looks to reveal that a classiﬁcation model can be developed to account for potential

forms of training data perturbation with minimal eﬀect on the overall model accuracy.

2.1 Data Understanding

The modeling for this research is based on a training data set of samples previously identiﬁed

as malware. Particularly, the samples were identiﬁed by a class corresponding to its malware

type. The set of malware types used for this dat aset consist of Viruses, Worms, Packed

Malware, and AdWare. The data set is in the form of a N xM matrix in JSON format and

consists of six schema: “data”, “row index”,“column index”,“schema”,“shape”, and “labels.”

More speciﬁcally, the schema take on the following ranges and data types:

data : int [1:6317199]

row index : int [1:6317199]

column index: int [1:6317199]

schema : chr [1:106428]

shape : int [1:2]

labels : int [1:12536]

Among the schema present in the JSON ﬁle, “data”, “row index”, and “column index” cor-

respond to the contents of the feature matrix used to train potential classiﬁers. The feature ma-

trix represents the instance, i, that corresponds to the input, [row index[i], column index[i]].

As such, a particular instance in the “data” schema is represented by the following function:

f eature matrix[row index[i], column index[i]] = data[i]

(1)

7

The contents of “schema” take on the form of either unigrams, bigrams, or trigrams that

correspond to a particular Windows API call. The “shape” schema corresponds to the [N, M ]

values that dictate the dimensions of the data matrix. Finally, the “label” scheme correspond

to the class of each instance within the feature matrix. This schema is unique to the training

data as it is used as a means of assessing the accuracy of classiﬁcation models.

2.2 Data Preparation

Preparing the data for analysis and model input involved ﬁrst iterating through the range of

the “data” schema according to the function deﬁned in (1), At this point the feature matrix can

then be converted to compressed sparse row (CSR) format and saved to a npz ﬁle. Furthermore,

this initial CSR format of the data can be normalized with scaling from -1 to 1 and a ﬁxed

standard deviation.

While the described technique provides a feature matrix that can be used to train classiﬁca-

tion models, the shape of this initial matrix (12536, 6317199) poses an issue of dimensionality.

High-dimensional data in general is an issue based on in increased need in the number of

samples necessary for a estimator to eﬀectively generalize. Furthermore, in reducing the di-

mensions, the data can be analyzed and used for training classiﬁers despite limitations to

memory and processing power. For the purpose of this research, responding to the problem of

high-hdimensionality involved an implementation of singular value decomposition (SVD) on

the feature matrix. This SVD technique is the equivalent to Principle Components Analysis

in which the columnwise mean is subtracted from the feature values. Such a technique can

be tailored so as to produce new, reduced data sets based on a desired number of dimensions.

Given the initially prepared data set, the feature matrix can be reduced to 1000, 500, 250,

and 100 dimensions respectively. Similar to the initial data set, these reduced data sets are

formatted as CSR data sets and written to npz ﬁles. The approach is implemented using the

truncatedSVD class with sklearn in Python 3.7.

For the purpose of training the classiﬁers, the data sets generated thus far represent the

input (features). The output, or prediction, of such classiﬁers is ultimately the classiﬁcation

label (0-4), a categorical variable. Such labels can be extracted from the initial JSON ﬁle

8

of the training data. Analysis and comparison of the classiﬁcation models in this research is

based on the results of the models when trained on both an unperturbed, base data set, and

when trained on data sets containing a perturbed set of inputs and a perturbed set of labels,

respectively. More speciﬁcally, the classiﬁcation models are each trained on three separate

data sets and are assessed based on a constant validation data set containing inputs and the

corresponding labels that are pulled from the initial base data set.

The validation set is generated by designating 25% of the base data set, T rinput as a

constant, unaltered data set with the naming convention Vinput. The labels that correspond

to the input are similarly designated as Vlabel. These data sets serve as method of comparing

each classiﬁer training iteration (regardless of input) against an immutable validation set. The

remaining 75% of the base data set is used as the unperturbed control data set, which is

designated as C1,input with the corresponding set of labels being designated as C1,label. As the

C1,input and C1,label serve as the unperturbed training set, they act as the control training set

to determine the relative eﬀect of the perturbed training sets on each malware classiﬁer.

The overall perturbation scheme entails targeting the two aspects of training data that serve

as potential threat vectors for adversaries. In an attempt to poison the data, an adversary

could target either the input features themselves, or the corresponding training labels in order

to degrade the classiﬁcation accuracy. The ﬁrst set of perturbations on the base data set aimed

to naively emulate the poisoning of the data inputs (features). The scheme is based on applying

a uniform perturbation to a percentage of the features per input so as to eﬀectively poison

the data and alter the accuracy of a potential classiﬁcation, without being so drastic as to

easily recognizable. The perturbation is based on iterating through the base inputs of C1,input

to randomly perturb 20% of the features per input. Of the values selected, the perturbation

involves multiplying non-zero values by 1.5 and changing zero values to the product of a random

integer between 1 and 10 and 0.1. This perturbed data set is designated as C2,input. The set

of corresponding labels, C2,label, is unperturbed and, therefore, is equal to C1,input.

The second set of perturbations on the base data set aimed to naively emulate the poisoning

of training data classiﬁcation labels. Given the lower volume of data points in the training

labels data set compared to the training inputs data set, the goal with this scheme was to more

9

randomly perturb a percentage of the labels in order to degrade the classiﬁcation accuracy.

Therefore, this perturbation scheme is based on iterating through the base classiﬁcation labels

of C1,label to randomly perturb 20% of the labels within the base data set. The values selected

are changed to a random value between 0 and 4, representative of the diﬀerent potential

malware types. This data set is designated as C3,label. Meanwhile, the set of corresponding

inputs is unperturbed and, therefore, is equal to the inputs in data set C1,input.

2.3 Model Building

The foundation of this research is the implementation of the 10 most commonly used ma-

chine learning models for supervised learning (classiﬁcation): random forest, support vector

machine, gradient boosting, logistic regression, artiﬁcial neural network, linear discriminant

analysis, quadratic discriminant analysis, naive bayes, bagging, and decision tree. All models

are implemented using the Python 3.7 machine learning library, scikit-learn, or sklearn, which

facilitates simplicity and eﬃciency in machine learning for data mining and analysis. For the

sake of this research, all models are implemented with their respective default parameters;

hence, no hyper-parameter tuning was performed, serving as future work.

The support vector machine classiﬁer calculates a set of hyperplanes based on the training

data, all of which would potentially classify the inputs.

In order to identify the optimal

hyperplane, the classiﬁer then looks to maximize the functional margin as depicted in Figure

2. The margin ultimately represents the distance between the plane and any point of either

class. The classiﬁer is implemented using the svm class within sklearn. As applied to multi-

class classiﬁcation, the logistic regression classiﬁer distinguishes between classes. The classiﬁer

essentially follows a multinomial logistic function in order to model a response variable that

describes the probability prediction of the classiﬁcation label [14].

The multi-layer perceptron classiﬁer, as depicted in Figure 3, is an implementation of an

artiﬁcial neural network. It is a base implementation of the MLPClassiﬁer class in sklearn.

The artiﬁcial neural network consists of hidden layers of perceptrons. Within each layer, inputs

are transformed based on trained weight summation, or bias. Weights at each hidden layer are

trained using backpropagation, relying on gradient to minimize a cross-entropy cost function.

10

Figure 2: Optimal Functional Margin Between Two Points of Two Respective Classes [15]

The result is ultimately a set of prediction probability estimates for each input [15].

Figure 3: Feed-Forward Artiﬁcial Neural Network Containing a Hidden Layer of Perceptrons [15]

Linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA) are based

on training linear and quadratic decision boundaries within each respective model. Decision

boundaries for both models are generating according to Bayes’ rule, with the conditional

probability being modeled on a Gaussian distribution. the diﬀerence between the two models

is in the assumed covariances of the Gaussian densities for each class. In LDA, the decision

boundaries are calculated under the assumption that the densities for each class share the same

11

Figure 4: Diﬀerences Between LDA and QDA Given Data with Fixed and Varying Covariances [15]

covariance matrix. In QDA, there are no assumptions with regard to the covariance matrix

for each density. The result, therefore, is the possibility for quadratic decision boundaries as

opposed to the strictly linear decision boundaries for LDA [15]. The eﬀect of the diﬀering

treatment of covariances can be more deﬁnitively depicted in Figure 4. Figure 4 especially

highlights how this diﬀering treatment can eﬀect the decision boundaries for the LDA and

QDA classiﬁers respectively, as the QDA will produce a quadratic decision boundary when

trained on data with varying covariances. The two models are implemented using the sklearn

classes LinearDiscriminantAnalysis and QuadraticDiscriminantAnalysis, respectively.

The naive bayes classiﬁer is an application of gaussian naive bayes classiﬁcation. The

model is an inherently a naive classiﬁcation approach due to its assumption of conditional

independence between pairs of features for a given input. This particular approach assumes

the value of the likelihood of the features follows the gaussian distribution in order to calculate

its prediction probabilities [15].

The decision tree classiﬁer is essentially a non-parametric approach to classiﬁcation that

12

model a decision, or classiﬁcation, as tree-like graphs. The ultimate classiﬁcation is based

on the linear combination of explanatory variables for a given input. The model seeks the

shortest sequence of explanatory variables in order to calculate its prediction [14]. Developing

the model calls for a recursive partitioning of a training sets with respect to its explanatory

variable, so that inputs with similar features are grouped together. The recursive partitioning

creates the structure of a tree and facilitates such analysis of the shortest combination of

features that enables a prediction [15]. In this research, the model is implemented through the

DecisionTreeClassiﬁer class within sklearn.

The random forest classiﬁer expands upon the decision tree in that it is an ensemble method

consisting of a series of trees derived from randomized subsets of the training data. Similar to

the base decision tree, each tree will produce a probabilistic prediction of the overall classiﬁ-

cation. This prediction probability is then averaged across all trees, thus producing an overall

prediction for the given input features [15]. The model is implemented using the Random-

ForestClassiﬁer and this implementation consists of 100 trees derived from the corresponding

number of randomized samples from the training data. The bagging classiﬁer is another en-

semble method and for this research is implemented using a very similar approach. The key

diﬀerences in the random subset of the training data, features are drawn without replacement.

The bagging classiﬁer is implemented suing the BaggingClassiﬁer class [15].

In a similar approach, the gradient boosting classiﬁer is an ensemble method that relies

on the use of a series of decision trees as weak learners. The classiﬁer then builds a greedy,

additive model in which trees are added so as to minimize loss. The minimization is achieved

by calculating the steepest descent method. The method is ultimate a numerical approach and

involves a calculation of the gradient of the loss function for the given stage of the ensemble

[15]. The base model is implemented using the GradientBoostingClassiﬁer class within sklearn.

Finally, this research utilizes a stacking method in an attempt to generate a more robust

model. The linear stacking method involves designating models to be stacked and such stack

serves as Level 1 of the model. In “stacking” the approach. as depicted in ﬁgure 5, trains

and runs the selected models in order to capture each model’s respective predictions. The

predictions for each are then compiled to form a new data set in which the predictions become

13

the features for each input and correspond to a label in the validation or test set. The new

“stacked” data set is then pushed to Level 2 of the linear stacking model which consists of a

base classiﬁer. The Level 2 classiﬁer is trained on the new data set so that it is essentially

predicting a label based on the predictions of the Level 1 models [16].

Figure 5: Description of the Data Flow Within the Linear Stacking Method

3 Results and Discussion

3.1 Base Model Deployment

The base models serve as the foundation of this research, as their classiﬁcation accuracy when

trained on C1,C2, and C3, respectively, determines the Level 1 make up of stacked across the

top performing models per data set. Table 1 list the results of deploying the base models.

Each model was trained on each of the three data sets, and the accuracy report is based on

comparing the model prediction to the Vlabel set.

Initial analysis of the classiﬁcation model accuracy reveals the general trend of data set

C2, a perturbation of the features, causing a slight drop in the overall accuracy of the models

and C3, a perturbation of the labels, causing a more drastic decrease in accuracy. The main

take away from the base model deployment, however, is that given the respective perturbation

schemes, the results in Table 1 provide insight into the top performing models, and, thus, the

models most likely to add the desired level of robustness to the stacked model.

For data set C1, the random forest classiﬁer is the best performing model based on its

accuracy of 0.9294. It is key to note, however, that while not the top performing models relative

14

Table 1: Accuracy Scores for Base Models based on Given Input Data Sets C1, C2, and C3
Base Model Classiﬁcation Accuracy

Base Classiﬁer
Random Forest
Support Vector Machine
Gradient Boosting
Logistic Regression
Artiﬁcial Neural Network
LDA
QDA
Naive Bayes
Bagging
Decision Tree

C1
0.9294
0.9135
0.9211
0.9007
0.9167
0.8836
0.8050
0.5602
0.9226
0.8919

C2
0.9262
0.9007
0.9175
0.8923
0.9175
0.8820
0.6886
0.4944
0.9215
0.8967

C3
0.8931
0.9103
0.9111
0.8927
0.8927
0.8880
0.5060
0.5817
0.8668
0.8002

to accuracy, the bagging and gradient boosting classiﬁers produce similarly high accuracies of

0.9226 and 0.9211, respectively. In the case of the bagging classiﬁer, it appears to be the second

highest performing classiﬁer for both data sets C1 and C2, respectively. In both deployments,

the model is outperformed by the random forest classiﬁer. Given that the random forest

classiﬁer is the top performer for data set C1, a second iteration of the model should not be

included in the stacked model as doing so would only add duplicate label predictions from

the same source. Because of this, this research designates the bagging classiﬁer as the top

performer for data set C2. The bagging classiﬁer’s performance when trained on C1 also adds

a degree of relative surety that it is a strong candidate for inclusion in the stacked model.

Analysis of the gradient boosting classiﬁer demonstrates that although it is not a top

performer for data set C1 or data set C2, its accuracy of 0.9111 when trained on data set C3 is

the highest among all of the base classiﬁers. It also is key to note that while the perturbation

scheme in data set C3 seems to have had a relatively drastic eﬀect on the other base models,

its eﬀect on the accuracy of the gradient boosting classiﬁer is relatively minimal.

3.2 Stacked Model Development and Deployment

The stacked model draws upon these top performing models across the three data sets in order

to generate the Level 1 classiﬁcation. Based on each classiﬁer’s predicted label for each input,

15

the Level 2 classiﬁer in the stack will predict an overall label for a given input in the data

set. Given the Level 1 stack generated by the random forest, bagging, and gradient boosting

classiﬁers and the original 10 base classiﬁcation models, the product is ultimately 10 stacked

models all based on a constant Level 1 stack. Similar to the base models, these 10 stacked

models can be trained on C1, C2, and C3 respectively in order to compare model accuracy

relative to each perturbation scheme. Table 2 lists the accuracy of each model relative to each

data set as a means of comparing model performance.

Ultimately the key takeaway from the results of the stacked model is their performance

relative to the Table 1 results. When trained on C1 the random forest classiﬁer predicts with

an accuracy of 0.9294, when trained on C2 the bagging classiﬁer predicts with an accuracy of

0.9215, and when trained on C3 the gradient boosting classiﬁer predicts with an accuracy of

0.9111. In analyzing the Table 2 results, it is clear that the stacked models do not outperform

the top performing base model relative to data sets C1, C2, and C3, respectively. However, it

is key that the stacked models achieve the goal of increased robustness, with only a minimal

eﬀect on the accuracy of the classiﬁer. Speciﬁcally, the stacked model with a Level 2 support

vector machine classiﬁer achieves an accuracy of 0.9183, 0.9147, and 0.8900 for data sets C1,

C2, and C3, respectively.

Table 2: Accuracy for “Best of” Stacked Models Based on Given Input Data Sets C1, C2, and C3
Stacked Model Classiﬁcation Accuracy

Level 1 Stack

Random
Forest,
Bagging,
Gradient
Boosting

Level 2 Classiﬁer
Random Forest
Support Vector Machine
Gradient Boosting
Logistic Regression
Artiﬁcial Neural Network
LDA
QDA
Naive Bayes
Bagging
Decision Tree

C1
0.9091
0.9183
0.9099
0.6800
0.9163
0.7041
0.8696
0.9167
0.9111
0.9111

C2
0.9095
0.9147
0.9099
0.8297
0.9115
0.7085
0.8788
0.9135
0.9063
0.9091

C3
0.8768
0.8900
0.8856
0.6826
0.8808
0.6882
0.8589
0.8844
0.8824
0.8864

In eﬀort to analyze the eﬀects of adding a further degree of robustness to the model de-

sign, this research considers two models based on a Level 1 stack of all base models with a

16

Level 2 consisting of the gradient boosting classiﬁer and the support vector machine classiﬁer,

respectively. In analyzing the accuracy of these two additional stacked models, which is pro-

vided in Table 3, it is interesting to note that not only does this approach increase the level of

robustness, the models also outperform the stacked models based on the top performing base

models. The stack of all models with the support vector machine as the Level 2 classiﬁer even

outperforms the top performing base models for data sets C2 and C3.

Table 3: Accuracy for Stack of All Base Models Based on Given Input Data Sets C1, C2, and C3
Classiﬁcation Accuracy of a Stack Across All Models

Level 2 Classiﬁer
Gradient Boosting
Support Vector Machine

C1
0.9222
0.9274

C2
0.9222
0.9264

C3
0.9091
0.9179

Given this overall top performing model, a recommendation for subsequent implementation

would be to further expand on this intelligent systems approach with another ensemble method;

this would further increase the overall robustness of the model. Given that the stack of all

models with the Level 2 support vector machine model produced a prediction based on each

of the three data sets, C1, C2, and C3, the three models can be linearly combined relative

to their predicted probabilities. Given the three sets of predicted probabilities per class per

input, the maximum a posterior sum of probability values per class per input can be used to

determine the overall prediction per input. Speciﬁcally, the result will be a single set of vectors

corresponding to each input of the base data set. Within each vector, select the highest value,

just as the highest prediction probability is selected to produce a classiﬁcation prediction;

this is also known as a soft voting ensemble. The ultimate result is a robust classiﬁcation

that is designed intelligently to account for the potential for a base, unperturbed data set, a

perturbation of features, and a perturbation of labels.

17

4 Conclusions and Future Work

4.1 Conclusions

This research using an intelligent systems design approach for malware classiﬁcation ultimately

called for developing and training machine learning models based on a purposeful poisoning,

or perturbation, of the training data. This research ultimately looked to demonstrate that

doing so aided in identifying those base models best equipped to consider varying degrees of

data poisoning. Stacking these models created a classiﬁer that is designed with robustness in

mind and considers the potential of poisoned training data, all while limiting that eﬀect of

poisoning and increased robustness on the model’s classiﬁcation accuracy.

The results of this research provide evidence that this method of stacking as a means of

accounting for data poisoning increased robustness without a detrimental eﬀect on the overall

accuracy of the model. Furthermore, in expanding the robustness by increasing the number

of models in the Level 1 stack, the stacked model is better equipped to classify in adversarial

conditions than the base models alone. The trade oﬀ of is a slightly diminished classiﬁcation

accuracy in instances where training data may be unperturbed.

Overall, the Level 1 stack across all models with a Level 2 support vector machine can

be considered the best performing model, as it outperformed even the base models when

considering adversarial conditions and perturbations within training data sets. There of course

exists even more potential for this model to be expanded using a ﬁnal soft voting ensemble

approach designed to account for all perturbation schemes at once. This implementation can

only further expand the robust, intelligent systems design of the malware classiﬁcation model,

making it more apt to handle the potential for adversarial conditions.

4.2 Limitations and Future Work

First, the shear size of the data set and the number of features per input exposed a signiﬁcant

memory limitation of the machine used for classiﬁcation model design and implementation.

This memory limitation drove the need for a high degree of dimensionality reduction of the

18

initial base data set. The eﬀect was minimal due to the scope and objective of this research.

Future work, however, may call for analysis of the models on larger, non-dimension reduced

data sets.

Second, the data poisoning schemes used within the intelligent systems design approach

incorporated relatively naive perturbations based upon simple stochastic sampling. As a result,

future work should investigate and compare more sophisticated data poisoning schemes to

conduct the perturbations, such as genetic algorithms, particle swarm optimization, generative

adversarial networks, or other heuristic-based methods.

Third, future work calls for the optimization of the initial base classiﬁcation models. Given

the objective of this research, using the “default” status of the models was suﬃcient due to

the focus on developing a robust, intelligent systems design approach. However, future eﬀorts

should focus on reﬁning the base models prior to the stacking eﬀorts. In particular, a full- or

partial-experimental design could be used for hyper-parameter tuning of each of the malware

classiﬁers. By optimizing the hyper-parameters of the base models, this could prove to optimize

the classiﬁcation accuracy of the ﬁnal stacked model while maintaining its inherent robustness.

Finally, future work should test and operationalize the ﬁnal soft voting ensemble approach

to add the prediction probabilities of the ﬁnal stacked model for each data set. This research

provides somewhat of an experimental foundation, but further research should focus on prac-

tical implementation of the intelligent systems design within malware classiﬁcation.

References

[1] Gregor, S. & Benbasat, I. (1999). Explanations from Intelligent Systems: Theoretical

Foundations and Implications for Practice. MIS Quarterly, 23(4), 497-530.

[2] Hayes-Roth, F. (1997). Artiﬁcial intelligence: what works and what doesn’t?. AI Maga-

zine, 18(2), 99-113.

[3] Hayes-Roth, B. (1995). An architecture for adaptive intelligent systems. Artiﬁcial Intel-

ligence, 72(1-2), 329-365.

19

[4] Liu, Q., Li, P., Zhao, W., Cai, W., Yu, S., & Leung, V. C. (2018). A survey on security

threats and defensive techniques of machine learning: a data driven view. IEEE access,

6, 12103-12117.

[5] Yu, S. (2016). Big privacy: Challenges and opportunities of privacy study in the age of

big data. IEEE access, 4, 2751-2763.

[6] Biggio, B., Fumera, G., & Roli, F. (2014). Security evaluation of pattern classiﬁers under

attack. IEEE transactions on knowledge and data engineering, 26(4), 984-996.

[7] Lowd, D., & Meek, C. (2005, August). Adversarial learning. In Proceedings of the

eleventh ACM SIGKDD international conference on Knowledge discovery in data mining

(pp. 641-647). ACM.

[8] Huang, L., Joseph, A. D., Nelson, B., Rubinstein, B. I., & Tygar, J. D. (2011, October).

Adversarial machine learning. In Proceedings of the 4th ACM workshop on Security and

artiﬁcial intelligence (pp. 43-58). ACM.

[9] Khurana, N., Mittal, S., & Joshi, A. (2018). Preventing Poisoning Attacks on AI based

Threat Intelligence Systems. arXiv preprint arXiv:1807.07418.

[10] Kloft, M., & Laskov, P. (2012). Security analysis of online centroid anomaly detection.

Journal of Machine Learning Research, 13(Dec), 3681-3724.

[11] Barreno, M., Nelson, B., Sears, R., Joseph, A. D., & Tygar, J. D. (2006, March). Can ma-

chine learning be secure?. In Proceedings of the 2006 ACM Symposium on Information,

computer and communications security (pp. 16-25). ACM.

[12] Papernot, N., McDaniel, P., Sinha, A., & Wellman, M. (2016). Towards the science of

security and privacy in machine learning. arXiv preprint arXiv:1611.03814.

[13] Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., & Wirth,

R. (2000). CRISP-DM 1.0 Step-by-step data mining guide.

[14] Hackeling, G. (2014). Mastering Machine Learning with scikit-learn: Apply eﬀective

learning algorithms to real-world problems using scikit-learn. Birmingham: Packt Publ.

20

[15] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,

M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau,

D., Brucher, M., Perrot, M., Duchesnay, E. (2011). Scikit-learn: Machine Learning in

Python. Journal of Machine Learning Research, 12, 2825-2830.

[16] Wu, L. (2018). Stacking.py. Retrieved from https://github.com/WuLC/

MachineLearningAlgorithm/blob/master/python/Stacking.py.

21

