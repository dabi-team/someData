Adaptive Strategic Cyber Defense for Advanced Persistent
Threats in Critical Infrastructure Networks

Linan Huang and Quanyan Zhu
{lh2328, qz494}@nyu.edu

Department of Electrical and Computer Engineering, New York University
2 MetroTech Center, Brooklyn, NY, 11201, USA ∗

8
1
0
2

p
e
S
6

]
T
G
.
s
c
[

1
v
7
2
2
2
0
.
9
0
8
1
:
v
i
X
r
a

ABSTRACT
Advanced Persistent Threats (APTs) have created new security chal-
lenges for critical infrastructures due to their stealthy, dynamic, and
adaptive natures. In this work, we aim to lay a game-theoretic foun-
dation by establishing a multi-stage Bayesian game framework to
capture incomplete information of deceptive APTs and their multi-
stage multi-phase movement. The analysis of the perfect Bayesian
Nash equilibrium (PBNE) enables a prediction of attacker’s behav-
iors and a design of defensive strategies that can deter the adver-
saries and mitigate the security risks. A conjugate-prior method
allows online computation of the belief and reduces Bayesian up-
date into an iterative parameter update. The forwardly updated pa-
rameters are assimilated into the backward dynamic programming
computation to characterize a computationally tractable and time-
consistent equilibrium solution based on the expanded state space.
The Tennessee Eastman (TE) process control problem is used as a
case study to demonstrate the dynamic game under the information
asymmetry and show that APTs tend to be stealthy and deceptive
during their transitions in the cyber layer and behave aggressively
when reaching the targeted physical plant. The online update of
the belief allows the defender to learn the behavior of the attacker
and choose strategic defensive actions that can thwart adversarial
behaviors and mitigate APTs. Numerical results illustrate the de-
fender’s tradeoff between the immediate reward and the future ex-
pectation as well as the attacker’s goal to reach an advantageous
system state while making the defender form a positive belief.

1.

INTRODUCTION

With the integration of communication networks and informa-
tion technologies with the critical infrastructures including power
grids, transportation systems, and water distribution systems, the
direct use of the off-the-shelf technologies has made our infras-
tructure vulnerable to cyber attacks. One emerging threat is the
Advanced Persistent Threats (APTs) which are a class of multi-
phase and multi-stage hacking processes [9], initiating their infec-
tions in cyberinfrastructures yet targeting at speciﬁc physical in-
frastructures such as nuclear power stations and automated facto-
ries. Unlike the “spray-and-pray” attacks, APTs as the targeted at-
tacks, perform reconnaissance and tailor their hacking techniques
to the targeted system. As shown in Fig. 1, the APTs’ life cycle

∗This work was partially supported by NSF awards CNS-1544782,
SES-1541164, ECCS-1550000, CNS-1720230, DOE grant de-
ne0008571, and a DHS grant through Critical Infrastructure Re-
silience Institute (CIRI).

Copyright is held by author/owner(s).

Figure 1: APTs start the infection by exploiting network vul-
nerabilities or the human weakness. They aim to cause physical
damages or collect conﬁdential data.

includes a sequence of phases and stages such as the initial en-
try, privilege escalations, and lateral movements. APTs use each
stage as a stepping stone for the next one. Since APTs have a spe-
ciﬁc target at the ﬁnal stage, they receive no beneﬁts going back
to previous stages. Thus, the multi-stage attack graph bears a tree
structure without jumps or loops. Unlike the “smash-and-grab” at-
tacks, APTs behave seemingly as legitimate users, wait until the
ﬁnal stage to launch the “critical hit” on their speciﬁc targets, and
inﬂict an enormous loss.

The classical intrusion prevention techniques such as the cryp-
tography and the physical isolation can be ineffective for APTs.
An APT-type adversary can steal the full cryptographic keys by
exploiting zero-day vulnerabilities and techniques such as social
engineering. Stuxnet can bridge the air gap between local-area net-
works with the insertion of infected USB drives. Similarly, the in-
trusion detection approach [2] has proven to be insufﬁcient when
APTs acquire knowledge of the system response as well as the
detection rule with the help of insiders and the reconnaissance.
Moreover, APTs operated by human experts can analyze, learn,
and update the knowledge of the system, thus evading detection
by stealthy and strategic movements, e.g., scan the port sufﬁcient
slow to avoid the alarm and even choose the No Operation (NOP)
at some stages. Hence, it is essential to design up-to-date secu-
rity mechanisms that can mitigate the risks despite the successful
inﬁltration and the strategic response of APTs.

One way to understand the multi-stage and stealthy nature of the
APTs is through dynamic games with incomplete information. The
dynamic game frameworks capture the multi-stage movement of
the defender and the attacker in networks [4, 6]. The deceptive and
stealthy behaviors of the APTs lead to the information asymme-
try where an attacker has his own private information encapsulated
by a random variable called types [3]. The type characterizes the
essence and the objective of the user, i.e., whether the user is le-
gitimate or adversarial, which assets serve as his targets, and how

 
 
 
 
 
 
much damages he can inﬂict on the system. The user’s type de-
termines his behaviors if he is rational and aims at maximizing his
utility, which makes it possible for the defender to form and up-
date a belief of the type based on the history of user’s behaviors.
Since the attacker has to follow the network protocol and move
stealthily across the networks by hiding his footprints and evading
the detection, it is natural to view the defender as the principal who
can design security policies and the attacker as the agent who fol-
lows the policies to attain his goal. The strategic behaviors of the
defender and the attacker will lead to a perfect Bayesian Nash equi-
librium (PBNE) where no one can proﬁt from unilateral deviations
at any stage. Achieving a long-term statistic optimal is challeng-
ing since the belief updates forwardly yet the PBNE strategy pair is
computed backwardly. With the beta-binomial conjugate prior as-
sumption, we manage to unify the coupled forward and backward
processes and form the dynamic programming with an expanded
state. Tennessee Eastman process is used as a case study to illus-
trate the theoretical underpinning of our framework for the design
of strategic defense to deter the attacks and mitigate the impact of
the threats.
Related Work: FlipIt game [7] has analyzed the scenario of the
key leakage under APTs so that a system operator and APTs will
takeover the system alternately. Defenders cannot know the time
of the stealthy takeover as well as the current system status unless
taking defensive actions. FlipIt game provides high-level guide-
lines on how to allocate the limited resources to deter the APTs.
Our multi-stage Bayesian game framework, however, supports a
speciﬁcation of both adversarial and defensive actions with utilities
and enables the equilibrium analysis of the game as the prediction
of the attack moves. Signaling game, a two-stage game with the
one-sided type, has been applied to study the information asymme-
try in cyber deception [8]. However, both players receive a one-shot
utility which does not well capture the multi-stage transition of the
APTs. In our framework, each player at each stage receives feed-
backs involving his/her immediate reward and the other player’s ap-
parent activities, which enables the defender to learn the attacker’s
type during the multi-stage interactions.
Organization of the Paper: The rest of the paper is organized as
follows. Section 2 introduces the forward belief update and the
backward dynamic programming under the PBNE solution con-
cept. In Section 3, we adopt the binomial-beta conjugate prior to
turn the nonparametric update of the distribution into a parametric
one. A case study of APTs targeted at the TE process is presented
in Section 4, and Section 5 concludes the paper.

2. SYSTEM MODEL

Consider a two-person game with P1 as the system defender (pro-
noun “she”) and P2 as the user (pronoun “he”). The user has a type
θ which is the realization of a continuous random variable ˜θ with
the support Θ := [0, 1] ⊂ R. The value of the type indicates the
strength of the user in terms of damages that he can inﬂict on the
system. A user with a larger type value indicates a higher threat
level to the system.

At each stage t ∈ {0, 1, · · · , T }, each player Pi chooses an action
i ∈ A t
at
i . The user’s actions represent the apparent behaviors and
observable activities from log ﬁles such as a privilege escalation
request and sensor access. A defender cannot identify the user’s
type from observing his actions. The defender’s action at
1 repre-
sent precautions and proactive behaviors such as restricting the es-
calation request or monitoring the sensor access. Thus, the action
pair (at
1, at
2) is known to both players after stage t and forms a his-
2 }. The state xt ∈ X t shows
tory ht := {a0

1, · · · , at−1

2, · · · , at−1

, a0

1

1 and σ t

1 : X t (cid:55)→ (cid:52)A t

2 : X t × Θ (cid:55)→ (cid:52)A t

the system status such as the location of the APTs at each stage
t ∈ {0, 1, · · · , T }. Since the initial state x0 and history ht uniquely
determine the state, xt contains information of history up to t and
has the transition kernel described by xt+1 = f t (xt , at
1, at
2) with a
deterministic kernel function f t . Deﬁne (cid:52)A t
i as the probability
distribution over Pi’s action space. The behavioral mixed strate-
gies σ t
2 mean that both
players make their decisions based on the information available to
them. With a slight abuse of notation, let σ t
2|xt , θ ) be
1|xt ), σ t
i at stage t under state xt and type
the probability of taking action at
θ . The set of all behavioral mixed strategies σ t
i forms the strategy
space Σt
i.
Believe Update: To strategically gauge the user’s type, the de-
fender speciﬁes a belief Bt : X t (cid:55)→ (cid:52)Θ as a distribution over the
type space according to the state xt at stage t. Likewise, Bt (θ |xt )
is the conditional probability density function (PDF) of the type θ
and (cid:82) 1
0 Bt (θ |xt )dθ = 1, ∀t, xt . The prior distribution of the user’s
type is known to be B0 and the belief of the type updates according
to the Bayesian rule with the arrival of the action observation at
2
drawn from the mixed strategy σ t
2.

1(at

2(at

Bt+1(θ | f t (xt , at

1, at

2)) =

Bt (θ |xt )σ t
(cid:82) 1
0 Bt ( ˆθ |xt )σ t

2(at
2(at

2|xt , θ )
2|xt , ˆθ )d ˆθ

.

(1)

i : X t × A t

Utility Function: The user’s type inﬂuences Pi’s immediate payoff
2 × Θ (cid:55)→ R. For
received at each stage t, i.e., Jt
example, a legitimate user’s access to the sensor beneﬁts the sys-
tem while a pernicious user’s access can incur a considerable loss.
i}t=t(cid:48),··· ,T ∈ Σt(cid:48):T
Deﬁne σ t(cid:48):T
as a sequence of policies
from t(cid:48) to T . The defender has the objective to maximize the cu-
mulative expected utility:

1 × A t

i ∈ Σt

:= {σ t

i

i

Ut(cid:48):T
1

(σ t(cid:48):T
1

, σ t(cid:48):T
2

, xt(cid:48)

) :=

=

T
∑
t=t(cid:48)

(cid:90) 1

0

Bt (θ |xt ) ∑
1∈A t
at

1

T
∑
t=t(cid:48)

Eθ ∼Bt ,at

1∼σ t

1,at

2∼σ t
2

1(xt , at
Jt

1, at

2, θ )

1(at
σ t

1|xt ) ∑
2∈A t
at

2

2(at
σ t

2|xt , θ )Jt

1dθ .

and the user’s objective function is

Ut(cid:48):T
2

(σ t(cid:48):T
1

, σ t(cid:48):T
2

, xt(cid:48)

, θ ) =

T
∑
t=t(cid:48)

∑
1∈A t
at

1

1(at
σ t

1|xt ) ∑
2∈A t
at

2

2(at
σ t

2|xt , θ )Jt
2.

2 to best-respond to σ t

Perfect Bayesian Nash Equilibrium: We model the scenario of
APTs under the insider threat as a dynamic principal-agent prob-
lem where defender P1 as the principal chooses her policy σ t
1 ﬁrst at
each stage t. Attacker P2 as the agent perceives σ t
1 via insiders, and
then chooses his policy σ t
1, i.e., maximizes his
cumulative expected utility Ut:T
. Since APTs have to follow rules
to avoid detection, a sophisticated defender aware of the potential
policy leakage under insider threats can acquire the best response
of APTs through the attack tree or honeypots. The described secu-
rity scenario leads to the following deﬁnition of perfect Bayesian
Nash equilibrium (PBNE) where the defender chooses the most re-
warding policy to confront the attacker’s best-response policies.

2

DEFINITION 1. In the two-person multi-stage game with a se-
quence of beliefs Bt ,t ∈ {t(cid:48), · · · , T } satisfying the Bayesian update
in (1) and the cumulative utility function Ut(cid:48):T
(σ t(cid:48):T
) :=
i
1
2
(σ t(cid:48):T
(σ t(cid:48):T
, θ ), ∀σ t(cid:48):T
{γ ∈ Σt(cid:48):T
2 ∈
1
1
2
Σt(cid:48):T
, ∀xt(cid:48)
, θ ∈ Θ} is P2’s best-response set to P1’s policy
2
σ t(cid:48):T
1 ∈ Σt(cid:48):T

, the set Rθ ,xt(cid:48)
, xt(cid:48)
, σ t(cid:48):T
2

under state xt(cid:48)

: Ut(cid:48):T
2
∈ X t(cid:48)

, θ ) ≥ Ut(cid:48):T

and type θ .

, γ, xt(cid:48)

1

2

DEFINITION 2. In the two-person multi-stage Bayesian game
with P1 as the principal, the cumulative utility function Ut(cid:48):T
, the
initial state xt(cid:48)
, the type θ ∈ Θ, and a sequence of beliefs
i,t ∈ {t(cid:48), · · · , T } in (1), a sequence of strategies σ ∗,t(cid:48):T
Bt
is
called a perfect Bayesian Nash equilibrium (PBNE) for the princi-
pal, if

∈ Σt(cid:48):T
1

∈ X t(cid:48)

1

i

U ∗,t(cid:48):T
1

(xt(cid:48)

) :=

inf
2 ∈Rθ ,xt(cid:48)
σ t(cid:48) :T

2

(σ ∗,t(cid:48) :T

1

)

Ut(cid:48):T
1

(σ ∗,t(cid:48):T
1

, σ t(cid:48):T
2

, xt(cid:48)

)

= sup
1 ∈Σt(cid:48) :T
σ t(cid:48) :T

1

inf
2 ∈Rθ ,xt(cid:48)
σ t(cid:48) :T

2

(σ t(cid:48) :T
1

)

Ut(cid:48):T
1

(σ t(cid:48):T
1

, σ t(cid:48):T
2

, xt(cid:48)

).

A strategy σ ∗,t(cid:48):T
2
PBNE for the agent P2.

∈ arg maxσ t(cid:48) :T

2 ∈Σt(cid:48) :T

2

Ut(cid:48):T
2

(σ ∗,t(cid:48):T
1

, σ t(cid:48):T
2

, xt(cid:48)

, θ ) is a

Dynamic Programming: Given the type belief at every stage, we
use dynamic programming to ﬁnd the PBNE policies in a backward
fashion because of the tree structure and the ﬁnite horizon. Deﬁne
, σ ∗,t:T
2(xt , θ ) :=
1(xt ) := Ut:T
the value function V t
1
2
Ut:T
, xt , θ ) as the optimal utility-to-go for the defender
2
and the user, respectively. We have the following simultaneous
equations, i.e.,
1(xt ) = sup
V t
σ t
1

, xt ) and V t

2)) + Jt

( f t (xt , at

(σ ∗,t:T
1

, σ ∗,t:T
2

(σ ∗,t:T
1

Eθ ∼Bt ,at

1(xt , at

[V t+1
1

2∼σ ∗,t

1, at

1, at

1∼σ t

1,at

2

2)],

V t
2(xt , θ ) = sup
σ t
2

Eat

2∼σ t

2,at

1∼σ ∗,t

1

[V t+1
2

(xt+1, θ ) + Jt

2(xt , at

1, at

2, θ )],

i

where σ ∗,t
, i ∈ {1, 2} is the PBNE policy pair at stage t. The above
system equations have to be solved backwardly from stage t to
stage 0 with the boundary conditions V T +1
(xT +1, θ )
at stage t = T + 1. However, the belief Bt in (1) updates forwardly
with the boundary condition B0 at initial stage t = 0. These two
equations are coupled, and we need to ﬁnd the consistent pair of
PBNE strategies and beliefs.

(xT +1),V T +1

2

1

3. COMPUTATION

In the Bayesian update, the prior probability distribution Bt is
called a conjugate prior for the likelihood function σ t
2 if the pos-
terior distribution Bt+1 is in the same family as the prior distri-
bution Bt . Similar to our previous work [5], the defender divides
the action space of the user into K + 1 time-invariant sets C j, i.e.,
2 = {∪C j} j=0,1,··· ,K, ∀t which are mutual exclusive C j ∩ Cl =
A t
/0, ∀ j (cid:54)= l. Each set represents a category and each at
2 uniquely cor-
2|xt , θ ),
2(at
responds to one category. Then, we can transform σ t
the distribution of at
2, into a distribution of the corresponding cate-
gory ˆσ t
2(kt |xt , θ ) with the index kt ∈ {0, 1, · · · , K}. If we assume
ˆσ t
2 to be a binomial distribution with the parameter q = θ and
N = K. The probability mass function (PMF) of category k is
Pr(k) = (cid:0)N
(cid:1)qk(1 − q)N−k. The prior belief B0 is assumed to be
k
a beta-distribution with hyperparameters α 0 and β 0. Since bi-
nomial and beta-distributions are conjugate, the posterior belief
conserves to be a beta-distribution with updated hyperparameters
(αt+1, β t+1) = (αt + kt , β t + K − kt ), where kt is the category that
the user’s action at stage t falls into. Finally, we transform the be-
lief conditioned on the categories back to the belief conditioned on
the corresponding actions using the hard de-aggregation in which
actions at
2 correspond to the same category kt share the same
2, ¯at
belief distribution Bt .
Expanded State in Dynamic Programming: Since the parame-
ter update (αt+1, β t+1) = (αt + kt , β t + K − kt ) is sufﬁcient to de-
termine the belief update in (1). The original system state xt and

the belief state αt , β t compose an expanded state yt = {xt , αt , β t }.
Since αt + β t = α 0 + β 0 + tK, we only need one of the two pa-
rameters to determine the beta-distribution and the notation θ ∼ β t
means that the type is of the beta-distribution with the hyperparam-
eters (α 0 + β 0 + tK − β t , β t ).

V t
1(yt ) = sup
σ t
1∈Σt
1
2(yt , θ ) = sup
V t
σ t
2∈Σt
2

Eθ ∼β t ,at

1∼σ t

1,at

2∼σ ∗,t

2

[V t+1
1

(yt+1) + Jt

1(xt , at

1, at

2, θ )],

Eat

2∼σ t

2,at

1∼σ ∗,t

1

[V t+1
2

(yt+1, θ ) + Jt

2(xt , at

1, at

2, θ )].

(2)

t(cid:48)=1 kt(cid:48)

, β t = β 0 +tK −∑t
Since αt = α 0 +∑t
, ∀t ∈ {1, · · · , T },
the number of feasible expanded states at stage t is ﬁnite. Thus,
we can directly compute (2) from stage t = T to stage t = 0 in
a backward fashion and obtain the consistent pair of beliefs and
PBNE policies.

t(cid:48)=1 kt(cid:48)

4. CASE STUDY

We consider a four-stage transition (T = 2) of the expanded state
yt = {xt , αt , β t } as shown in Fig. 2. At the initial stage t = 0 where
no behaviors are observed, the defender forms a biased initial be-
lief that the user is more likely to be legitimate and of small threats,
i.e., B0 is a beta-distribution with hyperparameter α 0 = 1, β 0 = 2.
Starting from the initial system state x0 = 0, the user at stage t = 0
(and t = 1) chooses to escalate his privilege at
2 = 1 (resp. propa-
gate laterally) with an action cost ct
2 ≥ 0 or no operation performed
(NOP) at
2 = 0. The defender, at stage t = 0 (and t = 1), can choose
proactive actions such as restricting the privilege escalation at
1 = 1
(resp. the lateral movement) with an action cost ct
1 ≥ 0 or no oper-
ation performed (NOP) at
1 = 0. State x1 = 3, 2, 1 represents a high,
medium, and low privilege level for the user, respectively. The state
transition function f 0 shows that if the user escalates his privilege
at
2 = 1 and the defender does not restrict it at
1 = 0, the output privi-
lege level is high; if the defender restricts it at
1 = 1, the output level
is medium; otherwise the output level is low when the user takes
NOP at
2 = 0. Let K = 1, then the secure category k = 0 includes
2 = 0 and k = 1 includes at
at
2 = 1, respectively.

Figure 2: The multistage transition of the expanded state yt =
{xt , αt , β t }. The parenthesis around the arrow, e.g., (0/1, 1) de-
notes the value of the actions pair (at

1, at

2).

The traditional cyber security concerns the information protec-
tion yet APTs go beyond that. APTs can break the normal indus-
trial operation by falsifying the set point of the controller, tam-
pering the sensor reading and blocking the communication chan-
nel to cause delays in either the control message or the sensing
data. Thus, after the transition in the cyberinfrastructure, the user
will arrive at the physical plant (i.e., stage t = 2) with the system

Table 1: The nonzero-sum stage utilities (JT
1 , JT
(JT
2 )
aT
1 = 0
aT
1 = 1

aT
2 = 1
(rn − (rn − ra)θ , (rn − ra)θ − cT
2 )

aT
2 = 0
(rn, 0)
(rn − cT

1 , −cT
2 )

(rn − cT

1 , JT

1 , 0)

2 ).

state xt ∈ X t =: {1, 2, 3, 4, 5} representing different sensors un-
der user’s control. Both players take actions, obtain utilities relat-
ing to the operation of the physical plant, and arrive at the termi-
nal stage t = T + 1 with the boundary conditions V T +1
(xT +1) :=
0,V T +1
(xT +1, θ ) := 0, ∀θ ∈ Θ, i.e., terminal states share the same
2
stage utilities after the breach has happened.
Physical Threats: We consider the benchmark Tennessee Eastman
(TE) process as the targeted physical plant. The TE process in-
volves two irreversible reactions to produce two liquid (liq) prod-
ucts G, H from four gaseous (g) reactants A,C, D, E.

1

A(g) +C(g) + D(g) → G(liq),
A(g) +C(g) + E(g) → H(liq).

2 = 0 means NOP and aT

The process shuts down when the safety constraints are violated
such as a high reactor pressure, a high/low separator/stripper liq-
uid level. The control objective is to maintain a desired production
rate as well as quality, while stabilizing the whole system under
Gaussian noise to avoid violating the safety constraints. The in-
herent feedback controller for this nonlinear system performs well
and results in the utility rn derived from three performance metrics,
i.e., the product quality, the operation cost, and the shutdown time.
Attackers can compromise different sensors and lead to different
states xT ∈ X T . Then attackers can revise the reading to drive the
system away from the reference point. Deﬁne a reward function
ra : X T (cid:55)→ R, then ra(xT ) will be the operation utility of the TE
process under the state xT , which can be obtained from the simula-
tion results of the process model [1]. We rank the output value of
the function ra from high to low and index the states correspond-
ingly, e.g., xT = 1 indicates the compromise of a secondary sensor
and xT = 5 indicates the compromise of all the sensors in the TE
process. Action aT
2 = 1 means revising the
readings of the sensors under his control. Unlike the stealthy transi-
tion in the previous cyber networks, the attacker at the ﬁnal stage T
do not need to disguise as legitimate and can take detectable adver-
sarial actions. Defenders can choose to defend aT
1 = 1 with the cost
cT
1 = 0. As shown in Table 1, if aT
1 or not defend aT
i = 0, i ∈ {1, 2},
the system operates normally with a reward of rn and the user does
If the action pair is
not receive rewards incurred by the attack.
(1, 0), the defender has to pay an extra cost to monitor the sen-
sor activities. If both players take actions (1, 1), then the system
is well protected and receives a normal operation utility minus the
monitoring cost while the attack pays the action cost cT
2 without
accomplishing the compromise. Finally, if the attacker launches an
attack under no proper defenses, the system is compromised and
receives a discounted payoff rn − (rn − ra(xT ))θ . The attacker,
on the other hand, wins a reward proportional to (rn − ra(xT ))θ .
Note that the reward loss is discounted by the threat level θ , At
the extreme case θ = 0 where the legitimate user will not sabo-
tage, i.e., rn − (rn − ra(xT ))θ = rn and he receives no beneﬁts from
the attack. Let pt (yt ), qt (yt , θ ) be the probability of taking action
1 = 1, at
at
2 = 1, respectively. Thus, the value functions under the

Table 2: Equivalent stage utility ˜Jt
state yt = {3, 2, 2} at stage t = 1.

1 and ˜Jt

2 under the expanded

˜Jt
at
2 = 0
1
V t+1
at
1 = 0
({3, 2, 3})
1
1 = 1 V t+1
({3, 2, 3}) − ct
at
at
˜Jt
2 = 0
2
({3, 2, 3}, θ ) V t+1
1 = 0 V t+1
at
({3, 2, 3}, θ ) V t+1
1 = 1 V t+1
at

at
2 = 1
V t+1
({5, 3, 2})
1
1 V t+1
({4, 3, 2}) − ct
1
1
at
2 = 1
({5, 3, 2}, θ ) − ct
2
({4, 3, 2}, θ ) − ct
2

2

2

1

2

2

PBNE mixed-strategies are given as follows.
1 (yT ) = max
V T
pT

Eθ ∼β T [qT (yT , θ )(rn − ra(xT ))θ − cT

1 pT (yT )

2 (yT , θ ) = max
V T
qT (yT ,θ )

+ rn − qT (yT , θ )(rn − ra(xT ))θ ],
[(1 − pT (yT ))(rn − ra(xT ))θ − cT

2 ]qT (yT , θ ).

(3)

i

2

(pT ) = 1{(1−pT )(rn−ra(xT ))θ −cT

i to form an equivalent stage utility ˜Jt
1(xt , at

The best-response policy Rθ ,xT
2 >0} =
1{θ > ¯θ (pT ,xT )}, where ¯θ (pT , xT ) := cT
2 /[(1− pT )(rn −r2(xT ))]. Plug
the best response into the ﬁrst equation, and we solve for the de-
fender’s policy p∗,T (yT ) as well as the user’s policy q∗,T (yT , θ ) =
Rθ ,xT
(p∗,T (yT )). The user’s policy q∗,T has the threshold ¯θ (pT , xT ),
2
i.e., if his type value θ < ¯θ , he will choose NOP; otherwise he will
choose to attack. The policy q∗,T (yT , θ ) is semi-separating if the
threshold ¯θ ∈ (0, 1) and is called a pooling strategy if the threshold
¯θ (cid:54)∈ (0, 1). The defender cannot learn any knowledge about user’s
type when he adopts pooling strategies which are independent of
his type value.
Cyber Transitions: According to (2), the value functions V t
i from
stage t = 0, 1 under the PBNE can be computed in the same fashion
as in (3) if the future expectation V t+1
is assimilated into the direct
1, at
1(xt , at
stage reward Jt
2, θ ) =
V t+1
2, θ ) = V t+1
(yt+1)+Jt
(yt+1, θ )+
2(xt , at
2
1
1, at
2(xt , at
Jt
2, θ ) as shown in Table 2. Since the attacker aims to
compromise sensors and inﬂict physical damages at stage T , we
assume a petty utility for the cyber state transition, i.e., only the
action cost ct
i, i ∈ {1, 2} is taken into account. However, actions at
the cyber stage t = 0, 1 will affect the future system state xT at the
physical stage. Thus, the defender has the tradeoff of being secure
and economical, i.e., paying the defense cost to guard against the
potential compromises. On the other hand, the attacker’s action will
also affect the future belief state α T , β T , which leads to his tradeoff
of either being stealthy or reaching advantageous future expanded
states yT .
Comparisons and Insights: As shown in Fig. 3, a high value for
the defender is the result of a healthy system state xT as well as a
belief state where the user is more likely to have a low type value.
At the extreme state xT = 1 where the reward incurred by the attack
is so low that users with any type values choose not to attack. Then
the defender does not need to defend p∗,T (yT ) = 0, ∀yT and obtains
the maximum utility.

2, θ ) and ˜Jt

1, at

1, at

To investigate the effect of the defender’s belief, we ﬁx the sys-
tem state xT = 3 and change the belief state (α T , β T ) from (9, 1)
to (1, 9), which means that the defender grows optimistically that
the user is of a low threat level with a high probability. Since
players’ value functions are of different scales in terms of the at-
tacking threshold and the probability, we normalize the value func-
tions with respect to their maximum values to illustrate their trends
and make them comparable to the threshold and the probability as

Therefore, it illustrates that the attacker of a large type value will
take the risk of behaving aggressively to reach a desirable system
state in the next stage because he would obtain higher rewards once
the attack succeeds. As a countermeasure, P1 chooses to defend yet
only with a small probability.

5. CONCLUSION

In this work, we have explored a multistage incomplete informa-
tion Bayesian game framework for designing proactive and adap-
tive defensive strategies for critical infrastructure networks with the
presence of Advanced Persistent Threats (APTs). This framework
well captures the multi-stage and multi-phase structure of APTs
and their strategic nature to move stealthily within the network.
With the information asymmetry between the attacker and the sys-
tem, the defender needs to form a belief dynamically on the type of
the user using observable footprints. To enable the online computa-
tion of the belief, we have used conjugate priors to reduce Bayesian
updates into parameter updates. This approach leads to a compu-
tationally tractable extended-state dynamic programming criterion
that yields an equilibrium solution consistent with the forward be-
lief update and backward induction. Finally, we have used Ten-
nessee Eastman process as a case study to demonstrate the pro-
posed framework. The numerical experiments have shown that our
framework has signiﬁcantly improved the security of critical infras-
tructures by strategically deterring the attacker and mitigating the
APTs.

6. REFERENCES
[1] BATHELT, A., RICKER, N. L., AND JELALI, M. Revision of
the tennessee eastman process model. IFAC-PapersOnLine 48,
8 (2015), 309–314.

[2] COPPOLINO, L., D’ANTONIO, S., ROMANO, L., AND

SPAGNUOLO, G. An intrusion detection system for critical
information infrastructures using wireless sensor network
technologies. In Critical Infrastructure (CRIS), 2010 5th
International Conference on (2010), IEEE, pp. 1–8.

[3] HARSANYI, J. C. Games with incomplete information played
by bayesian players, i–iii part i. the basic model. Management
science 14, 3 (1967), 159–182.

[4] HUANG, L., CHEN, J., AND ZHU, Q. A large-scale markov
game approach to dynamic protection of interdependent
infrastructure networks. In International Conference on
Decision and Game Theory for Security (2017), Springer,
pp. 357–376.

[5] HUANG, L., AND ZHU, Q. Analysis and computation of

adaptive defense strategies against advanced persistent threats
for cyber-physical systems. In International Conference on
Decision and Game Theory for Security (2018).

[6] MANSHAEI, M. H., ZHU, Q., ALPCAN, T., BACS¸ AR, T.,

AND HUBAUX, J.-P. Game theory meets network security and
privacy. ACM Computing Surveys (CSUR) 45, 3 (2013), 25.
[7] VAN DIJK, M., JUELS, A., OPREA, A., AND RIVEST, R. L.
Flipit: The game of stealthy takeover. Journal of Cryptology
26, 4 (2013), 655–713.

[8] ZHANG, T., AND ZHU, Q. Strategic defense against deceptive

civilian gps spooﬁng of unmanned aerial vehicles. In
International Conference on Decision and Game Theory for
Security (2017), Springer, pp. 213–233.

[9] ZHU, Q., AND RASS, S. On multi-phase and multi-stage

game-theoretic modeling of advanced persistent threats. IEEE
Access 6 (2018), 13958–13971.

Figure 3: Defender’s value function V T
expanded states yT = {xT , α T , β T }.

1 (yT ) under different

shown in Fig. 4. When β T is small, the defender chooses to protect
the system with a high probability p∗,T = 0.67, which completely
deters attackers with any type values because the probability to at-
tack q∗,T = 1{θ > ¯θ } equals 0 when the attacking threshold ¯θ is 1.

As the defender trusts more about the user’s legitimacy, the de-
fending probability p∗,T (yT ) decreases to 0 when β T = 9. Since
the defender is less likely to defend, the attacker bears a smaller
threshold to launch the attack. However, the threshold will not de-
crease to 0 because the users with type values less than ¯θ = 0.33
(deﬁned as the limiting threshold) cannot receive sufﬁcient rewards
from the attack even when the defender chooses NOP. The value
of the limiting threshold depends on the expanded state yT , yet it
should always be larger than 0 because a user with type θ = 0 has
no incentive to attack. The resulted defending policy p∗,T captures
a tradeoff between security and economy and guarantees a high
value for defenders at most of the belief states.

Figure 4: The effect of the defender’s belief.

2 = 2, c1

Finally, we investigate the multi-stage effect and the PBNE strat-
egy pair for the long-term maximum utilities. To simplify the com-
putation, we choose c0
2 = 0, i.e., the expense of privilege
escalations is more than lateral movements for the user and c0
1 =
0.1, c1
1 > 2.39. Then, the optimal policy for both players is to
choose NOP for all three expanded states at stage t = 1. There-
fore, although the attacker prefers to achieve a more advantageous
system state xt+1, aggressive behaviors at
2 = 1 at stage t = 1 can
decrease the defender’s trust and result in a less favorable belief
state (αt+1, β t+1). Because of the petty stage utility assumption, it
is more beneﬁcial for the attacker to remain stealthy at the interme-
diate stage and deceive the defender into a false belief. At the initial
stage t = 0, P1 chooses a0
1 = 1 with probability 0.20. P2 chooses
a0
2 = 1 when his type is larger than 0.95 and chooses a0
2 = 0 other-
wise. The values are V 0
2 = −10 + 13.57 · 1{θ >0.95}.

1 = 59.97 and V 0

