Assessment of Cyber-Physical Intrusion Detection
and Classiﬁcation for Industrial Control Systems

Nils M¨uller, Charalampos Ziras, Kai Heussen
Department of Electrical Engineering
Technical University of Denmark
Lyngby, Denmark
{nilmu; chazi; kh}@elektro.dtu.dk

2
2
0
2

b
e
F
8
1

]

R
C
.
s
c
[

1
v
2
5
3
9
0
.
2
0
2
2
:
v
i
X
r
a

Abstract—The increasing interaction of industrial control sys-
tems (ICSs) with public networks and digital devices introduces
new cyber threats to power systems and other critical infrastruc-
ture. Recent cyber-physical attacks such as Stuxnet and Irongate
revealed unexpected ICS vulnerabilities and a need for improved
security measures. Intrusion detection systems constitute a key
security technology, which typically monitor network data for
detecting malicious activities. However, a central characteristic
of modern ICSs is the increasing interdependency of physical and
cyber network processes. Thus, the integration of network and
physical process data is seen as a promising approach to improve
predictability in intrusion detection for ICSs by accounting
for physical constraints and underlying process patterns. This
work systematically assesses real-time cyber-physical intrusion
detection and multiclass classiﬁcation, based on a comparison
to its purely network data-based counterpart and evaluation
of misclassiﬁcations and detection delay. Multiple supervised
machine learning models are applied on a recent cyber-physical
dataset, describing various cyber attacks and physical faults on a
generic ICS. A key ﬁnding is that integration of physical process
data improves detection and classiﬁcation of all attack types. In
addition, it enables simultaneous processing of attacks and faults,
paving the way for holistic cross-domain cause analysis.

Index Terms—cyber-physical,

intrusion detection,

industrial

control systems, machine learning, power systems

I. INTRODUCTION

In recent years, industrial control systems (ICSs) face an
ongoing opening to the internet [1]. Previously isolated sys-
tems relying on private networks and speciﬁcally designed
protocols increasingly use public networks and digital devices
to achieve several business beneﬁts. However, together with
advantages such as cost-efﬁciency and process ﬂexibility, new
cyber vulnerabilities emerge. In the ﬁrst half of 2021, one-third
of ICS computers around the globe were targeted by cyber
attacks [2], with an increasing trend. Besides the introduction
of new cyber threats, the ongoing integration of cyber and
physical components transforms modern ICSs to complex
cyber-physical systems. In such cyber-physical ICSs, failures
can originate from a variety of hard- or software faults, human
errors or malicious activities. Distinguishing attacks from
physical faults or human errors is a particularly challenging
task in such highly integrated and complex systems, which
complicates the identiﬁcation of causal factors.

Intrusion detection systemss (IDSs) are responsible for de-
tecting malicious activities by monitoring and analyzing either

ICSs end-device (host-based IDS) or network data (network-
based IDS). In recent years the use of machine learning (ML)
for IDS has attracted increasing interest for reasons such as
the ability to capture complex properties of ICS operation and
attacks, lower central processing unit (CPU) load compared
to conventional IDS, higher detection speed, reduced need for
expert knowledge due to generalizability, and the exploitation
of steadily increasing amounts of data in ICSs [3]. In this
context, supervised classiﬁcation comes with the advantage
that it allows distinguishing different types of attacks and
other anomalies such as physical faults, which facilitates the
identiﬁcation of causal factors.

A promising approach to improve supervised intrusion
detection and classiﬁcation is the integration of cyber network
with physical process data [4]. In this way, the underlying
physical constraints and patterns of an ICS are included into
the IDS, potentially improving predictability. A few studies in-
vestigate supervised cyber-physical detection and classiﬁcation
of cyber attacks. In [5] the authors propose a multi-layer cyber
attack detection system, which combines a supervised and
exclusively network data-based classiﬁcation step with an em-
pirical model for detecting abnormal operation in physical pro-
cess data. However, the authors consider a binary classiﬁcation
problem (normal vs. attack) which weakens the determination
of causational factors. Moreover, as the dataset was shufﬂed
and randomly divided into training and test data, samples of a
speciﬁc attack event can be found in both sets, which entails
data leakage, weakening the validity of the results. In [6]–
[8] ML-based intrusion detection and classiﬁcation in ICSs
is investigated considering multiple attack types as well as
cyber network and physical process features. However, none of
these works compares cyber-physical with network intrusion
detection, or evaluates misclassiﬁcations and detection delay.
In [9]–[11] the beneﬁt of integrating physical process data
into supervised classiﬁcation-based IDS is demonstrated for
robotic vehicles. None of the works considers a multiclass
classiﬁcation problem, again weakening cause identiﬁcation.
Moreover, only in [9] several classiﬁers are compared and
only [12] considers a scenario including both cyber attacks
and physical faults. The thematically and methodologically
closest works are [13] and [14]. In [13] the authors introduce
the dataset which also forms the foundation for the present

 
 
 
 
 
 
work. To demonstrate a use case of the dataset, they compare
several supervised classiﬁers for intrusion detection. Although
the work compares the use of cyber network and physical
process data, the integration of both information sources is not
considered. Moreover, only a binary classiﬁcation problem is
considered. Finally, models are evaluated using K-fold cross-
validation (CV), which leads to data leakage as samples from
the same attack or fault event are placed in the training and
test datasets. In [14] the authors compare several unsupervised
and supervised models for cyber-physical intrusion detection
in power systems. The study explicitly compares network to
cyber-physical intrusion detection. However, in total only 4
man-in-the-middle (MiTM) attack events are considered and
investigated individually, again leading to data leakage.

The review reveals that existing works on supervised cyber-
physical intrusion detection and classiﬁcation either i) lack
systematic assessment through comparison to a purely network
data-based approach and evaluation of misclassiﬁcations and
detection delay, or ii) do not consider multiple attack and
anomaly classes, weakening cause identiﬁcation, or iii) suffer
from methodological issues, limiting the validity of results.

This work systematically assesses real-time cyber-physical
intrusion detection and multiclass classiﬁcation based on a
comparison to its exclusively network data-based counterpart
and evaluation of misclassiﬁcations and detection delay. Vari-
ous supervised ML models are implemented and evaluated on
a recent dataset of a generic ICS [13], describing several cyber
attack and physical fault types based on physical process and
cyber network data.

A. Contribution and paper structure

The main contributions of this work are as follows:

• Systematic comparison of ML-based network and cyber-
physical intrusion detection and multiclass classiﬁcation
for ICSs, evaluating multiple supervised classiﬁers.

• Attack and fault-wise analysis of misclassiﬁcations and
detection delay for cyber-physical intrusion detection.
• Proposal and assessment of prediction post-ﬁltering for

reduction of misclassiﬁcations.

• Transferability evaluation of the investigated generic ICS

to control systems in the power sector.

The remainder of the paper is structured as follows: In
Section II the investigated dataset
is introduced, and the
transferability of the underlying ICS to power sector control
systems evaluated. Section III provides a description of the
data preparation and applied models. In Section IV results are
presented and discussed, followed by a conclusion and a view
on future work in Section V.

II. DATASET DESCRIPTION AND TRANSFERABILITY

This section ﬁrst describes the dataset under investigation
(Subsection II-A). Thereafter, Subsection II-B sheds light on
the transferability of this work’s results by comparing the
generic ICS to control systems in the power sector.

A. Dataset

The dataset used in this study was acquired from a
hardware-in-the-loop water distribution testbed and is intro-
duced in [13]. The physical process comprises eight tanks.
Water ﬂow between the tanks is realized by valves, pumps,
and pressure and ﬂow sensors. The process is controlled by
a typical supervisory control and data acquisition (SCADA)
architecture consisting of multiple sensors and actuators (ﬁeld
instrumentation control layer), four programmable logic con-
trollers (PLCs) (process control layer), and a SCADA work-
including an human-machine interface (HMI) and
station,
data historian (supervisory control layer). Communication is
conducted via the MODBUS TCP/IP protocol. An additional
Kali Linux machine is included for launching cyber attacks.
The process consists of four stages, each of which is controlled
by one of the four PLCs.

The dataset describes the normal operation of the sys-
tem as well as several types of cyber attacks and physical
faults against different components and communication links.
Among the cyber attacks are 8 different MiTM attacks, 5
denial-of-service (DoS), and 7 scanning attacks. Moreover, 3
different water leaks and 6 sensor and pump breakdowns are
included as physical events, which partly appear simultane-
ously and are either called attacks or faults by the authors. In
the present work they will be referred to as physical faults. The
dataset consists of two sub-datasets, namely a cyber network
and physical process dataset. While the physical dataset has a
constant one-second resolution, the network dataset on average
has 2633 observations per second. The raw features are listed
in Table I. For a more detailed explanation of the dataset the
reader is referred to [13].

TABLE I
RAW CYBER NETWORK AND PHYSICAL PROCESS FEATURES.

No.
1

2-9

10-15

16-19

20-21

Physical features
Timestamp
Pressure values
of tank 1-8
State of pump 1-6
Flow values
of ﬂow sensor 1-4
State of valves 1-2

No.
1
2-3
4-5
6-7
8
9
10
11
12
13-14

Network features
Timestamp
IP address (src. & dst.)a
MAC address (src. & dst.)
Port (src. & dst.)
Protocol
TCP ﬂags
Payload size
MODBUS function code
MODBUS response value
No. of packets (src. & dst.)

aSrc. and dst. refer to source and destination, respectively.

B. Transferability to control systems in the power sector

The considered ICS constitutes a generic test bed. To assess
the transferability of the results of this study, a comparison to
control systems in the power sector, such as distribution system
operator’s SCADA or substation automation systems (SASs),
is conducted. An overview is given in Table II.

An important commonality is the SCADA architecture and
its components. Thus, the investigated attack scenarios, target-
ing various SCADA components, provide realistic scenarios
for both system types. Another similarity is the well-deﬁned

TABLE II
COMPARISON OF THE EXAMINED ICS TO CONTROL SYSTEMS IN THE
POWER SECTOR.

3

SCADA architecture & components

No. Similarities
1
2 Attack types & target components
Steady network & process conﬁgu-
ration (e.g., IP addresses, protocols
& number of physical devices)
Continuous, repetitive and thus deter-
ministic network & process patterns
Continuous, discrete & categorical
features

4

5

3

No. Differences
1 Volatility & trend pattern
Physical fault types
2
External impacts (e.g.,
weather & customer
behavior)
Type of communication
protocol
Number of physical &
network components

4

5

and steady conﬁguration of the physical process and cyber
network. Moreover, both systems show continuous and repet-
itive patterns of the physical process either due to process
cycles (generic ICS) or seasonality (power systems). Such
deterministic conﬁgurations and patterns allow to model both
systems with a set of physical and network features.

Differences are mainly related to the physical process. In
contrast to the investigated ICS, power systems are subjected
to external inﬂuences such as weather and customer behavior,
increasing process volatility. Moreover, compared to a distribu-
tion system operator’s SCADA system, the number of physical
and network components is relatively small. Thus, a central
difference is system complexity due to higher volatility and
number of components in power systems.

To conclude, the present generic ICS can be considered a
valuable test case for smaller control systems in the power
sector such as SASs. However, due to a lack of system com-
plexity, investigation of intrusion detection for larger SCADA
systems will require more extensive test beds.

III. METHODOLOGY

This section ﬁrst describes the preparation of the dataset.
Thereafter, the data pipelines applied to the intrusion detection
and classiﬁcation problem are introduced.

A. Data preparation

1) Data partitioning: A good practice to test performance
and generalization of a fully speciﬁed ML model is the appli-
cation to data which come from the same target distribution but
were not previously seen [15]. The present dataset describes
attack or fault events in time series format, where a speciﬁc
event corresponds to a sequence of observations. As a speciﬁc
event can only occur once, its entire sequence must either
be placed in the training or test dataset. Placing observations
from a single event in both the training and test set will assume
information from future events during model training. Thus,
data shufﬂing or CV-based performance evaluation will result
in an overly optimistic model performance assessment.

In this work, the entire sequences of the last two events of
every class are reserved for the test dataset. Since there are
only three water leak events in the dataset, some of which
also occur simultaneously with sensor or pump breakdowns,
all events of physical faults are combined into one event

class. As a result, the training set consists of the ﬁrst 80 %
of all normal operation observations, 73.07 % of the DoS
observations, 80.91 % of the MiTM observations, 77.75 % of
the physical fault observations, and 71.42 % of the scanning
observations. In this way, the risk of data leakage is minimized,
while a typical 75/25 ratio between the training and test set
can be maintained. Thus, future works examining the present
dataset are encouraged to use the same partitioning in order
to improve validity and comparison of results.

2) Feature extraction and fusion: To reduce the number
of model executions and simultaneously fuse network and
physical process data, the network dataset is downsampled
from approx. 2633 to one observation per second. For that
purpose, network data transfer is summarized for each second
by several statistics, which are listed in Table III. The initial
type column in the table deﬁnes on which type of raw network
features (Table I) the respective statistic is applied, while the
new type column indicates the data type of extracted features.
Three data types are considered, namely continuous, discrete
and categorical. As an example, the number of anomalous
value occurrences is calculated for all categorical (e.g., source
IP address) and discrete (e.g., payload size) raw features, while
the resulting extracted features are continuous. If no initial data
type is speciﬁed, they are not extracted from individual raw
features. For class-speciﬁc statistics, information about event
classes is taken only from the training dataset.

TABLE III
EXTRACTED NETWORK AND PHYSICAL PROCESS FEATURES.

Initial type
-

No.
1
2-3
4-20
21-31 No. of anomalous value occ.
32-37 No. of normal value occ.
38-107 No. of occ. for each instance
108-117 Number of different instances
118-127 Number of NaN occ.
128-132 Mean value

New type
Extracted network features
Continuous
Number of data transfers
Categorical
MAC/IP mismatch (src. & dst.) Categorical
Anomalous value occ.
Categorical
Cat., Discr.
Continuous
Categorical
Continuous
Categorical
Categoricala
Continuous
Categorical
Continuous
Cat., Con., Dis. Continuous
Continuous
Con., Discr.

133-178

No.
179
180
181

No. of occ. of class-
speciﬁc instances
Extracted physical features
Process cycle stage
Sine transf. of the cycle stage
Cos. transf. of the cycle stage

Categorical

Continuous

Initial type
-
Continuous
Continuous

New type
Continuous
Continuous
Continuous

aSrc. and dst. port excluded due to large number of instances.

In addition, 3 physical process features are extracted. The
process cycle stage indicates the progress of the current
process cycle and takes values between 0 and 1. To account for
the cyclical and continuous behavior of the process, the cycle
stage is further encoded by sine and cosine transformation.
After combining the raw physical features (Table I) with the
extracted features (Table III), and removing the ones with
constant values and thus without information, the ﬁnal dataset
comprises 161 features and 9185 observations.

B. Intrusion detection and classiﬁcation data pipelines

Mapping the feature set (Section III-A2) of an observa-
tion to an event class comprises several data transformation

steps. In this work, these include data scaling, dimensional-
ity reduction, undersampling, oversampling, classiﬁcation and
prediction post-ﬁltering. Note that ﬁltering stems from result
evaluation in Section IV and is not considered during model
selection. Fig. 1 depicts the resulting intrusion detection data
pipeline. For each of the transformation steps, several can-
didate methods are considered. The scaling methods include
feature standardization by removing the mean and scaling to
unit variance, normalization to values between 0 and 1, and
scaling to the maximum absolute value. Principal component
analysis (PCA) with Bayesian selection of the number of prin-
ciple components is considered for dimensionality reduction
[16]. Methods for undersampling include instance hardness
threshold (IHT) and removal of Tomek Links, while synthetic
minority oversampling technique (SMOTE) and Borderline
SMOTE constitute the oversampling techniques. The clas-
siﬁcation models include a random forest (RF), k-nearest
neighbors (KNN), a support vector machine (SVM) and an
artiﬁcial neural network (ANN). Prediction ﬁltering is realized
by a moving majority ﬁlter which outputs the most frequent
label of the past six predictions. As scan attacks do not appear
in sequences, they are excluded from the ﬁltering process.
All data transformation steps, except for classiﬁcation, can
also be bypassed. For some classiﬁers speciﬁc transformation
steps, such as bypassing scaling for SVM, are excluded due to
numerical issues. In the highlighted example in Fig. 1, a DoS
attack is detected by a data pipeline consisting of maximum
value scaling, Borderline SMOTE and a SVM.

The pipeline and most of the embedded data transforma-
tion methods and classiﬁcation models are implemented in
Python using the scikit-learn library [17]. One exception is the
ANN which is implemented using the deep learning library
Keras [18]. For a detailed description of all
implemented
models and methods, the reader is referred to the respective
documentation. The selection of transformation methods and
hyperparameters is conducted based on the training set (see
Section III-A1) applying a shufﬂed and stratiﬁed 5-fold CV
grid search. Although shufﬂing introduces data leakage during
model selection, it is required to ensure observations of each
attack type in all folds. While this may result in selection of
non-optimal hyperparameters, the evaluation of the selected
models is unaffected. The method and hyperparameter selec-
tion is summarized in Table IV, where selected pipelines are
referred to as the respective classiﬁer. For hyperparameters
the library’s default values are
not deﬁned in Table IV,

TABLE IV
METHOD AND HYPERPARAMETER SELECTION RESULTS.

Step
Scaling
Dim. red. None
Unders. None
Overs.

Classif.

ANN

SMOTE

PCA
None

nestimators:
100,
nmax-features:
17

KNN
Standard.
PCA
None
None
nneighbors: 5,
Dist. func.:
manhattan,
Weight func.:
distance

Cyber-physical intrusion detection & classiﬁcation pipelines
SVM
RF
Max. val. sc. Max. val. sc.
Standard.
None
None
Bor. SMOTE Bor. SMOTE
Kernel: radial-
basis function,
Penalty para.:
10000, Kernel
coeff.: 0.0175

nhid.-layers: 2, nunits:
150, Act. func.: Re-
Lu, Dropout rate:
0.5, nepochs: 500,
Batch size: 512
Network intrusion detection & classiﬁcation pipelines
SVM
Max. val. sc. Max. val. sc.
PCA
None
Tomek Links None
None
None
Kernel: radial-
nneighbors: 5,
Dist. func.:
basis function,
Penalty para.:
manhattan,
10000, Kernel
Weight func.:
coeff.: 0.0175
uniform

PCA
None
Bor. SMOTE
nhid.-layers: 2, nunits:
100, Act. func.: Re-
Lu, Dropout rate:
0.5, nepochs: 500,
Batch size: 256

nestimators:
100,
nmax-features:
17

ANN

RF

KNN

Step
Scaling Max. val. sc. Standard.
Dim. red. None
Unders.
IHT
Overs.
None

Classif.

used. Before being applied to test data, the selected detection
pipelines are retrained on the full training set.

IV. PERFORMANCE EVALUATION

This section assesses the performance of cyber-physical
intrusion detection and classiﬁcation. Subsection IV-A intro-
duces the applied metrics. In Subsection IV-B a comparison
to network intrusion detection and classiﬁcation is conducted,
while Subsection IV-C evaluates detection delay and misclas-
siﬁcations.

A. Metrics

To evaluate the class-wise detection and classiﬁcation per-

formance, this study considers the F1 score according to

F1,i =

T Pi

T Pi + 1

2 (F Pi + F Ni)

,

(1)

where T Pi, F Pi and F Ni are the number of true positives,
false positives and false negatives of the i-th class, respectively.
The overall performance is assessed based on a macro average
of the class-wise F1 scores, given as

F m

1 =

(cid:80)Nclasses

i=1 F1,i
Nclasses

,

(2)

Network features
• Transfers: 5328
• Anom. IP: True
• ...
Physical features
• Pump state: On
• Cycle stage: 0.6
• ...

Scaling

Dimensionality
reduction

Undersampling Oversampling

Classiﬁcation

• Standardizing
• Normalizing [0,1]
• Max. value scaling
• None

• PCA
• None

• IHT
• Tomek Links
• None

• SMOTE
• Borderline SMOTE
• None

• RF
• SVM
• ANN
• KNN

Prediction
ﬁltering

• Majority ﬁlter
• None

Prediction
• Normal
• DoS
• MiTM
• Phy. fault
• Scan

Fig. 1. Data pipeline for cyber-physical intrusion detection and classiﬁcation. Underlined methods indicate an example pipeline detecting a DoS attack.

with Nclasses being the number of classes. As seen from (2),
the macro average F m
treats all classes evenly, which is
1
important given the high cost of missing observations of the
less-populated attack classes. Average detection delay of class
i and average detection delay over all classes are given by

τi =

(cid:80)Nevents

j=1 (tdet,j − tstart,j)
Nevents

, and τ =

(cid:80)Nclasses
i=1
Nclasses

τi

,

(3)

where Nevents is the number of events of the i-th class and
tstart,j and tdet,j start and ﬁrst-detection time of the j-th event.

B. Comparison of network and cyber-physical intrusion de-
tection and classiﬁcation

In Table V the F1 scores of all detection and classiﬁcation
pipelines are listed. The highest scores are in bold, while the
second best scores are underlined. For network intrusion de-
tection, all models show a similar overall performance, despite
the differences on a class level. As expected, physical faults are
barely detected with pure network data. However, most models
detect some physical fault observations, especially the ANN. It
can be concluded that network data provide some information
about the physical process, which for instance could result
from altered payload sizes or higher NaN occurrences.

The use of the cyber-physical feature set improves class-
wise and overall performance for all models, with detection
of normal observations by the RF being the only exception.
This allows the conclusions that incorporating physical process
data has the potential to improve supervised intrusion detection
and classiﬁcation in ICSs. Interestingly, most cyber-physical
pipelines perfectly detect and classify scanning attacks, despite
the very few training examples. It can be inferred that the
extracted features in Table III very well capture the distinctive
characteristics of scanning attacks. Although ANNs can cap-
ture highly non-linear and complex relationships, they achieve
only second best performance. An explanation may be the
insufﬁcient number of observations. However, as training data
of cyber attacks usually is scarce, ANNs might not be appro-
priate for the given problem. The highest overall performance
is achieved by the SVM, due to the superior exploitation of
physical process information. Moreover, SVMs are known to
be accurate also in high dimensional spaces, which may be
an advantage given the relatively large feature-to-observation
ratio. The good performance of the SVM under existence
of physical faults demonstrates that integration of physical

TABLE V
CLASS-WISE AND AVERAGE F1 SCORES FOR NETWORK AND
CYBER-PHYSICAL INTRUSION DETECTION AND CLASSIFICATION.

Network features

Cyber-physical features

Event class RF KNN SVM ANN RF KNN SVM ANN
0.93
Normal
0.50
DoS
0.92
MiTM
0.46
Phy. fault
1.00
Scanning
Avg. (F m
0.76
1 )

0.93
0.47
0.83
0.04
0.67
0.59

0.94
0.49
0.88
0.06
0.80
0.63

0.88
0.47
0.68
0.14
0.80
0.60

0.91
0.71
0.87
0.26
1.00
0.75

0.92
0.55
0.87
0.07
0.57
0.60

0.95
1.00
0.81
0.62
1.00
0.88

0.86
0.96
0.42
0.00
0.80
0.61

process data also allows simultaneous detection and classiﬁ-
cation of events of fundamentally different nature, paving the
way for holistic cross-domain cause analysis. Nevertheless,
the comparatively low F1 score for physical faults requires
further improvements such as comprehensive physical feature
extraction.

While the overall performance (F m

1 ) on average improves
by 15.5 percentage points, the models show very different
class-speciﬁc improvements. Although detection of physical
faults clearly improves for most models, as expected, KNN
sets an exception. Moreover, only the SVM and ANN show
strong improvements for MiTM detection, while the RF shows
a comparatively good improvement for DoS attacks. This
complementarity suggests further investigation of ensemble
modeling, e.g., combination of a SVM and ANN.

C. Evaluation of misclassiﬁcations and detection delay of
cyber-physical intrusion detection and classiﬁcation

Misclassiﬁcations and detection delay are investigated on
the SVM considering cyber-physical features due to superior
performance. The confusion matrix in Fig. 2 reveals that the
SVM mainly confuses physical faults and MiTM attacks with
normal operation and vice versa. Moreover, it shows almost
no confusion within the attack and fault classes.

s
s
a
l
c

t
n
e
v
e

e
u
r
T

l
a
m
r
o
N

1460
94%

S 0
o
D
0%

M 14
T
10%
M

i

l
u
a
f

t 48
39%

P

.

g
n
i
n
n
a
c
S

0
0%

Normal

0
0%

42
100%

0
0%

0
0%

0
0%

45
3%

0
0%

128
90%

1
1%

0
0%

41
3%

0
0%

0
0%

74
60%

0
0%

0
0%

0
0%

0
0%

0
0%

2
100%

DoS

P. fault
MiTM
Predicted event class

Scanning

Fig. 2. Confusion matrix of the SVM for the cyber-physical feature set.

To further evaluate the location of misclassiﬁcations, the
true and predicted labels of an excerpt of the test dataset
are depicted in Fig. 3(a) in time series format. It can be
noticed that misclassiﬁcations do not primarily appear during
transition between event classes and are rather distributed.
However, some increased emergence can be noticed at the
transition from physical fault to normal operation (15:55:56)
and at the beginning and end of the MiTM attack between
16:08:26 and 16:10:01. High misclassiﬁcation densities ex-
ist around 16:00:55 and 16:06:20 during normal operation,
which might result from unlabeled irregular process behavior
or noise. Finally, most misclassiﬁcations occur individually
and not in sequences. This ﬁnding suggests ﬁltering of the
classiﬁcation output, as indicated in Fig. 1. Fig. 3(b) depicts
the predictions after applying the majority ﬁlter described in
Subsection III-B. The implemented ﬁlter is simple and not the

Ground truth
Prediction

15:55:40

15:59:32

(a) Unﬁltered
16:03:24

16:07:16

16:11:08

Scanning
Phy. fault
MiTM
DoS
Normal

Scanning
Phy. fault
MiTM
DoS
Normal

15:55:42

15:59:34

16:03:26
(b) Filtered

16:07:18

16:11:10

Fig. 3. Unﬁltered (a) and ﬁltered (b) predicted and true test dataset labels.

result of a purely training data-based model selection process.
Thus, results are of preliminary nature and further investigation
is required. From a comparison of Fig. 3(a) and (b), it is
noticeable that individual misclassiﬁcations are ﬁltered out
and only sequences remain, which greatly reduces the number
of false positives. A quantitative assessment of the additional
ﬁltering step in terms of F1 score and detection delay for
the test set excerpt of Fig. 3 is given in Table VI. Filtering
improves overall detection performance (F m
1 ) by 3 percentage
points. While detection of physical faults and MiTM attacks
greatly improves, the performance of DoS detection decreases.
This can be explained by the ﬁltering-induced false negatives
and positives at the beginning and end of the initially perfectly
classiﬁed DoS events. From Table VI it can also be seen
that the unﬁltered SVM immediately detects all event classes
except for physical faults. Filtering increases detection delay,
since several seconds of an event need to elapse to reach
majority within the ﬁlter sequence.

TABLE VI
COMPARISON OF THE UNFILTERED AND FILTERED SVM BASED ON F1
SCORE AND DETECTION DELAY FOR THE TIME SERIES DEPICTED IN FIG. 3.

Model Metric Normal DoS MiTM P. fault
SVM
unﬁlt.
SVM
ﬁltered

0.62
3.00
0.72
6.00

F1 [-]
τi [s]
F1 [-]
τi [s]

0.85
0.00
0.98
3.00

1.00
0.00
0.90
3.00

0.94
−
0.96
−

Scan Average
1.00
0.00
1.00
0.00

0.88
0.75
0.91
3.00

V. CONCLUSION AND FUTURE WORK

This work assesses ML-based cyber-physical intrusion de-
tection and multiclass classiﬁcation for ICSs. For that pur-
pose, a systematic comparison to a purely network data-
based approach is conducted, followed by an evaluation of
misclassiﬁcations and detection delay. An average F m
im-
1
provement of 15 percentage points across several supervised
classiﬁcation models demonstrates the beneﬁt of incorporating
physical process data into intrusion detection and classiﬁ-
cation. Moreover, simultaneous processing of cyber attacks
and physical faults is demonstrated, which paves the way to
holistic cross-domain cause analysis. Based on the evaluation
of misclassiﬁcations, a post-ﬁltering of the classiﬁer output is

proposed to reduce false positives, which however comes at
the cost of an increasing detection delay. Based on the results
of this work, future directions are seen in the investigation of
more advanced techniques for classiﬁcation, feature extraction
and post-ﬁltering. Especially combining classiﬁers with com-
plementing detection and classiﬁcation skills in an ensemble
model seems a promising direction. Moreover, the concept of
cyber-physical intrusion detection and classiﬁcation should be
further assessed on larger and application-speciﬁc test beds.

REFERENCES

[1] M. R. Asghar, Q. Hu, and S. Zeadally, “Cybersecurity in industrial con-
trol systems: Issues, technologies, and challenges,” Computer Networks,
vol. 165, 2019.

[2] Kaspersky, “Threat landscape for industrial automation systems.” https:
//ics-cert.kaspersky.com/media/Kaspersky-ICS-CERT-Threat-landsca
pe-for-industrial-automation-systems-statistics-for-H1-2021-En.pdf,
2021. [Accessed: 17/01/2022].

[3] P. Mishra, V. Varadharajan, U. Tupakula, and E. S. Pilli, “A detailed
investigation and analysis of using machine learning techniques for
intrusion detection,” IEEE Communications Surveys Tutorials, vol. 21,
no. 1, pp. 686–728, 2019.

[4] A. Ayodeji, Y.-k. Liu, N. Chao, and L.-q. Yang, “A new perspective
towards the development of robust data-driven intrusion detection for in-
dustrial control systems,” Nuclear Engineering and Technology, vol. 52,
no. 12, pp. 2687–2698, 2020.

[5] F. Zhang, H. A. D. E. Kodituwakku, J. W. Hines, and J. Coble, “Mul-
tilayer data-driven cyber-attack detection system for industrial control
systems based on network, system, and process data,” IEEE Transactions
on Industrial Informatics, vol. 15, no. 7, pp. 4362–4369, 2019.

[6] J. Yeckle and S. Abdelwahed, “An evaluation of selection method in
the classiﬁcation of scada datasets based on the characteristics of the
data and priority of performance,” in Proceedings of the International
Conference on Compute and Data Analysis, pp. 98–103, 2017.

[7] M. Keshk, N. Moustafa, E. Sitnikova, and G. Creech, “Privacy preser-
vation intrusion detection technique for scada systems,” in Military
Communications and Information Systems Conference, IEEE, 2017.
[8] R. C. B. Hink, J. M. Beaver, M. A. Buckner, T. Morris, U. Adhikari,
and S. Pan, “Machine learning for power system disturbance and cyber-
attack discrimination,” in 2014 7th International symposium on resilient
control systems (ISRCS), pp. 1–8, IEEE, 2014.

[9] G. Loukas, T. Vuong, R. Heartﬁeld, G. Sakellari, Y. Yoon, and D. Gan,
“Cloud-based cyber-physical intrusion detection for vehicles using deep
learning,” Ieee Access, vol. 6, pp. 3491–3508, 2017.

[10] T. P. Vuong, G. Loukas, and D. Gan, “Performance evaluation of
cyber-physical intrusion detection on a robotic vehicle,” in 2015 IEEE
International Conference on Computer and Information Technology;
Ubiquitous Computing and Communications; Dependable, Autonomic
and Secure Computing, pp. 2106–2113, IEEE, 2015.

[11] T. P. Vuong, G. Loukas, D. Gan, and A. Bezemskij, “Decision tree-
based detection of denial of service and command injection attacks on
robotic vehicles,” in 2015 IEEE International Workshop on Information
Forensics and Security (WIFS), pp. 1–6, IEEE, 2015.

[12] A. Bezemskij, G. Loukas, R. J. Anthony, and D. Gan, “Behaviour-
based anomaly detection of cyber-physical attacks on a robotic vehicle,”
in 2016 15th international conference on ubiquitous computing and
communications and 2016 international symposium on cyberspace and
security (IUCC-CSS), pp. 61–68, IEEE, 2016.

[13] L. Faramondi, F. Flammini, S. Guarino, and R. Setola, “A hardware-
in-the-loop water distribution testbed dataset for cyber-physical security
testing,” IEEE Access, vol. 9, 2021.

[14] A. Sahu, Z. Mao, P. Wlazlo, H. Huang, K. Davis, A. Goulart, and
S. Zonouz, “Multi-source multi-domain data fusion for cyberattack
detection in power systems,” IEEE Access, vol. 9, 2021.

[15] J. Friedman, T. Hastie, R. Tibshirani, et al., The elements of statistical

learning, vol. 1. Springer series in statistics New York, 2001.

[16] T. Minka, “Automatic choice of dimensionality for pca,” Advances in

neural information processing systems, vol. 13, 2000.

[17] F. Pedregosa et al., “Scikit-learn.” https://scikit- learn.org/, 2011.

[Accessed: 25/01/2022].

[18] F. Chollet, “Keras.” https://keras.io, 2015. [Accessed: 25/01/2022].

