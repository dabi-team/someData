Date of publication xxxx 10, 2021, date of current version xxxx 10, 2021.

AxXiv.org Pre-print 02.2021/ArXiv.org

Poisoning Attacks and Defenses on
Artiﬁcial Intelligence: A Survey

Miguel A. Ramirez1 †, Song-Kyoo Kim1,2 †, Hussam Al Hamadi1, Ernesto Damiani1, Young-Ji
Byon3, Tae-Yeon Kim3, Chung-Suk Cho3 and Chan Yeob Yeun1
1Center for Cyber-Physical Systems, Khalifa University of Science and Technology, Abu Dhabi, UAE.
2School of Applied Sciences, Macao Polytechnic Institute, R. de Luis Gonzaga Gomes, Macao, SAR.
3Department of Civil Infrastructure and Environmental Engineering, Khalifa University of Science and Technology, Abu Dhabi, UAE.
†The ﬁrst two authors have equal contribution.

2
2
0
2

ABSTRACT Machine learning models have been widely adopted in several ﬁelds. However, most recent
studies have shown several vulnerabilities from attacks with a potential to jeopardize the integrity of the
model, presenting a new window of research opportunity in terms of cyber-security. This survey is conducted
with a main intention of highlighting the most relevant information related to security vulnerabilities in
the context of machine learning (ML) classiﬁers; more speciﬁcally, directed towards training procedures
against data poisoning attacks, representing a type of attack that consists of tampering the data samples
fed to the model during the training phase, leading to a degradation in the model’s overall accuracy
during the inference phase. This work compiles the most relevant insights and ﬁndings found in the
latest existing literatures addressing this type of attacks. Moreover, this paper also covers several defense
techniques that promise feasible detection and mitigation mechanisms, capable of conferring a certain level
of robustness to a target model against an attacker. A thorough assessment is performed on the reviewed
works, comparing the effects of data poisoning on a wide range of ML models in real-world conditions,
performing quantitative and qualitative analyses. This paper analyzes the main characteristics for each
approach including performance success metrics, required hyperparameters, and deployment complexity.
Moreover, this paper emphasizes the underlying assumptions and limitations considered by both attackers
and defenders along with their intrinsic properties such as: availability, reliability, privacy, accountability,
interpretability, etc. Finally, this paper concludes by making references of some of main existing research
trends that provide pathways towards future research directions in the ﬁeld of cyber-security.

INDEX TERMS Artiﬁcial intelligence, cybersecurity, data poisoning, machine learning, poisoning attacks,
robust classiﬁcation

b
e
F
2
2

]

R
C
.
s
c
[

2
v
6
7
2
0
1
.
2
0
2
2
:
v
i
X
r
a

I. INTRODUCTION

The reliability associated with modern artiﬁcial intelligence
(AI) models plays an important role in a wide range of
applications [1], [2] including Internet-of-Things [3]. Conse-
quently, over the last couple of years, these machine learning
(ML) models demand adopting additional techniques in or-
der to address security related issues since newly emerging
vulnerabilities are being discovered and capable of posing
a threat to the integrity of the ML model which is the
target of an attacker [4]. An attacker could exploit such
vulnerabilities causing a negative impact on the performance
of the ML model. It has been proven plausible to maliciously
compromise a training dataset in order to affect the decision-
making process of the model which can cause malfunctions
during testing (i.e. inference) phase [4]. The need of public

and available data is continuously on demand by most ML
models. A clear example can be seen in smart city systems
wherein large amounts of data are gathered by numerous
sensors, such as smartphones. Then it can be foreseen that the
consequences of an attack targeting smart city systems could
be devastating and such an event is prone to occur due to
the system being heavily dependent on public data. The main
objective of this survey is directed towards gathering some of
the most representative attack and defense approaches from
the perspective of data poisoning [5]. Data poisoning (DP)
attacks aim to compromise the integrity of a target model
by performing alterations to the required dataset used by
the model during the training phase. This causes the model
to misclassify samples during the testing phase, resulting in
a signiﬁcant reduction in the overall accuracy. Due to the

VOLUME 4, 2016

1

 
 
 
 
 
 
above mentioned considerations, there is an urge to develop
more advanced defense mechanisms, aiming to enhance the
robustness of the model against potential DP attacks occur-
ring while training, in order to mitigate the effects of the
data poisoning. It is expected that a profound analysis over
the latest advances in defense schemes against poisoning
attacks could serve as a guideline for developing a novel
approach that attains a certain level of immunization against
DP on smart devices feeding data to smart city systems. The
main contributions of this survey compared to other existing
review/survey papers are listed as follows:

(1) This paper reviews related works of machine learning
security mechanisms, focusing primarily on potential
threats involving poisoning attacks occurring during
the training phase, particularly towards manipulation of
mislabeled training data.

(2) Several threats and attack strategies for machine learn-
ing (ML) models are presented and analyzed, describing
capabilities of the attacks and the list of assumptions
involved in each attack of interest. Furthermore, attacks
on the ML models are classiﬁed into 2 categories:
Attacks on Non-Neural Networks (NN) and Attacks on
Neural Networks.

(3) Various defense techniques are examined, highlighting
their potential beneﬁts as well as their disadvantages or
challenges entailing details of their deployments.
(4) This paper suggests future research directions for the
ﬁeld of machine learning security with discussions of
their implications.

This survey paper is organized as follows: the related back-
ground knowledge is explained on Section II. The Various
types of data poisoning attacks and their defense mechanisms
are presented on Section III and IV. Section V discusses and
suggests research directions in the recent studies which have
been reviewed by our survey. Overall lessons and the insights
of the survey are covered in Section VI. Basic statistics of the
survey is also included in Section VI. Section VII concludes
this paper.

II. KNOWLEDGE BACKGROUND
In this section, an overview of the properties related to attacks
and defenses is presented. The fundamentals of relevant top-
ics involving the security issue in machine learning models
are discussed, mainly from the perspective of assumptions
about attackers and different types of attacks throughout the
machine learning lifecycle.

A. MANIPULATIONS
Training data manipulation [6] is one of different types of
DP attacks by corrupting (or poisoning) the training data
during a training phase with an aim of utterly jeopardizing
the integrity of the ML classiﬁer making it to become an
ineffective classiﬁer. Examples of techniques often used by

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

the attackers are the modiﬁcation of data labels, injection of
malicious samples and manipulation of the training data. As
a result, the overall damage to the target ML model can only
be prevailed at a later inference phase, with the accuracy of
the model being drastically reduced. This effect is commonly
referred to as an accuracy degradation.

An input manipulation refers to triggering a machine
learning system to malfunction by altering the input that
is fed into the system [7]. It would be in a form of an
altered image adding noises or another input that causes
the classiﬁer to perform towards making wrong predictions.
Adversarial attacks [8]–[12] take place during the inference
phase, when the previously trained ML model is considered
to be reliable and is assumed to perform with high accuracies
[13]. Depending on the goal of the attacker, an adversarial
attack can fall in one of two categories. The ﬁrst category
is referring to a targeted attack when the input in a form
of crafted adversarial examples leads to the target model to
misclassify the samples into a speciﬁc class deﬁned by the
attacker [14], [15]. In contrast, in a Non-targeted attack the
crafted adversarial examples aim to cause the target model to
misclassify. Nonetheless, there is no need nor interest from
the attacker to misclassify into a particular class apart from
the correct one. Evasion attacks are also another kind of input
manipulation and are different from adversarial attacks in a
sense that evasion attacks do not require any knowledge over
the training data [16].

B. ASSUMPTIONS OF ATTACK AND DEFENSE
Security threats to machine learning models are generally
divided into data poisoning (DP) attacks and adversarial
attacks, the former is applied during a training phase and
the latter is applied during a testing phase, this difference
is shown in Figure 1. For the purposes of this paper, data
poisoning attacks will remain as the main topic of interest.

FIGURE 1. Data poisoning attacks during training phase affecting testing
phase [17].

One of the most common schemes of the attacker is inject-
ing malicious samples into the target model’s training set,
corrupting either the feature values or labels of the training
samples, and affecting the ML model boundaries by causing
signiﬁcant deviations to a point where the model’s reliability
is completely devastated. As a result, this would leave the
model susceptible to make wrong predictions. In summary,
the main goal of a such attacker is to disrupt the training

2

VOLUME 4, 2016

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

process aiming to signiﬁcantly reduce the performance of
the target model, causing a degradation in accuracy; and
increasing misclassiﬁcation rates of the samples during the
testing phase.

Assumptions of attackers refer to the prior knowledge
(implicit or explicit) about the target model of interest of
the attacker, entailing the resources available to the attacker.
When conducting experiments, the devised attack is meant
to be evaluated against a defense, both attack and defense
states-related assumptions must be declared to determine the
conditions that guarantee either scheme’s efﬁciencies (e.g.
attaining the defense to defeat an attack, or vice versa). How-
ever, various DP attacks have been shown to be successful
in spite of having very little knowledge of the target model.
An example of this is described in the research [18], which
directs a DP attack scheme to naive Bayes email-spam ﬁlters
by simply sending ‘hamlike’ emails using a black-listed IP
address as the sender, followed by being threatened and
labeled as a spam, nonetheless these corrupt data will be
inevitably used by the spam ﬁlter for further training.

The assessment of the inﬂuence of the attacker over the
training data is commonly deﬁned as an attacker’s capability.
Primary interest of the attacker is to alter either the feature
values or labels as part of the training set. Nevertheless,
the attacker is usually restricted to poison a limited number
of samples, typically corresponding to a ratio of less than
30% of the total data samples. More optimized poisoning
algorithms have been in continuous development during the
last decade, aiming to maximize the accuracy degradation
and minimize the number of poisoning samples needed to
perform the attack.

C. SECURITY AND RELIABILITY REQUIREMENTS OF AI
MODELS
In this section, we deﬁne some of the most relevant qualita-
tive security properties present in ML models [19], each of
these properties can be associated to a security threat and can
be quantiﬁed by speciﬁc metrics [20].

• Integrity is deﬁned as an ability of a model to function
according to speciﬁed norms in an understandable and
predictable manner. Thereby, an attacker could tamper
with the model’s parameters in the training phase and in
turn affect the overall integrity of the model. This can be
seen in the number of inputs representing label-ﬂipping,
which is the ratio of the models parameters impacted
by attacking with respect to the target model during the
training or testing phase.

• Availability is associated to the ability of an ML model
to perform as expected when facing radical perturba-
tions with the potential of causing a considerable impact
on the input data distribution due to the arising of unex-
pected conditions. Analyzing the ML model’s decision
boundary represents a clear indicator of availability,

which is reﬂected in the accuracy metric as well. For
instance, the effect of a DP attack is magniﬁed following
a function of the ratio of the injected poisoning data
samples in the training set, which leads to the utter
breakdown of the decision boundary.

• Robustness is deﬁned as an ability of the model carry-
ing on procedures in a desirable way in spite of having
perturbations in the input distributions. Such perturba-
tions could be deliberately crafted by an attacker, then
performing the training of a model with poisoning sam-
ples substantially tampers with the model’s robustness.
The distance from the last best version of the model
can be used as a metric, as well as for ROC (Receiver
Operating Characteristics) curve and AUC (Area Under
the Curve).

For the purposes of this paper, the integrity is considered
as the most important property as it is discussed in nearly all
existing DP attack review literatures.

D. METRICS OF INTEREST
The success of DP attacks is measured based on the amount
of degradation shown by the target model performance dur-
ing the testing phase. This can be further veriﬁed after
computing the decision matrix, observing the overall mis-
classiﬁcation rates in each class displaying: true positive,
false positive, true negative and false negative. Moreover,
the effectiveness of the attack is shown in the form of a
signiﬁcant drop in the overall accuracy, this is referred as an
accuracy degradation. The employment of additional metrics
besides measuring the accuracy have been proposed in this
paper to reﬂect and analyze in further detail about the overall
performance of the target model and make comparisons to
other performances of models. Various metrics for artiﬁcial
intelligence have been proposed by multiple standard bodies
including the International Organization for Standardization
(ISO) [21] and the National Institute of Standards and Tech-
nology (NIST) [22]. The metrics for AI typically include the
accuracy, the precision, the recall, the ROC and its area1 [23].

Other approaches such as the one by Biggio and Roli [24]
introduces security evaluation curves as a way to characterize
the performance of a ML model against an intended attack
considering various levels of knowledge from the attackers.
Thus this approach accomplishes a comprehensive evaluation
of the overall security of the model; and by doing so, enables
another means to compare assorted defense techniques.

E. ADVERSARIAL CAPABILITIES
In the testing phase, the attacker naturally will aim to at-
tain further knowledge over the target model in order to
increase the effectiveness of adversarial attacks, the attacker
typically focuses on any of the following ﬁve factors: Feature
space, classiﬁer type (e.g. DNN or SVM), classiﬁer learning

1AROC: Area under Receiver Operating Characteristic

VOLUME 4, 2016

3

algorithm, classiﬁer learning hyperparameters, and training
dataset.

• A white-box assumption is commonly deﬁned as a
scenario in which the attacker does have complete
knowledge over all the ﬁve elements already described
previously, as well as any defense mechanism already
set on top of the model [25]–[27].

• A black-box assumption is the opposite to white-box
assumption, when no knowledge of the target model,
albeit query it can be plausible. Nonetheless, it is im-
portant to remark that, just having access to the training
data grants the upper hand to the attacker over any de-
fender, representing this training data the unadulterated
or ‘clean’ dataset, in question [28]–[32].

• A gray-box assumption is often referred as a middle
ground between white-box and black-box scenarios,
where the prior knowledge on the attacker’s side can
include the feature space, the target classiﬁer; this in-
cludes the model architecture, model parameters and the
training dataset; however, the defense mechanism on top
is unknown to the attacker. The gray-box setting usually
is used to evaluate the defense against the adversarial
attack [17].

III. POISONING ATTACKS
In the following paragraphs several examples of data poison-
ing attacks on different types of models are discussed. For
the purposes of the survey, the center of focus is directed
towards data poisoning attacks performed during the training
phase. The effects of every poisoning technique is then
analyzed for classiﬁer models only. Albeit there are several
prior works that address poisoning techniques on regression
models, these will not be considered as covered by the scope
of this survey.

A. LABEL FLIPPING ATTACKS
The most common way to generate this kind of poisoning
is by maliciously tampering the labels in the data [33], this
can be easily achieved by just ﬂipping labels, thus generating
mislabeled data as shown in Figure 2. Label ﬂipping can be
performed either randomly or speciﬁcally depending on the
aims of the attacker; the former aims to reduce the overall
accuracy of all classes, the later does not aim to perform
signiﬁcant accuracy reduction, rather it is focus on the mis-
classiﬁcation of a determined class in particular. Paudice et
al. [34] proposes an optimal label ﬂipping poisoning attacks
compromising machine learning classiﬁers. Label ﬂipping
actions are performed following an optimization formulation
focused on maximizing the loss function of the target model.
This approach is considered computationally intractable due
to the inclusion of heuristic functions enabling the label
ﬂipping attacks to downscale the computational cost.

The applications of this approach limits itself to binary
classiﬁcation problems and the assumptions of the attack
involves complete knowledge over the learning algorithm,

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

FIGURE 2. Misclassiﬁcation error caused by label-ﬂipping [33].

loss function, training data and also the set of features used
by the ML classiﬁer, turning it basically into an attack on
a White-box model. Albeit the list of assumptions appeal
to unrealistic scenarios, the analysis emphasizes on worst
case scenarios. The effectiveness of the propose method is
demonstrated in three datasets from UCI repository: MNIST,
Spambase and BreastCancer; succeeding in increasing the
classiﬁcation error by a factor of 6.0, 4.5 and 2.8, respectively
[34].

Xiao et al. [35] reports a successful attack on a SVM model
after performing label ﬂipping using an optimized framework
capable of procure the label ﬂips which maximizes potential
classiﬁcation errors, causing a signiﬁcant reduction in the
overall accuracy of the classiﬁer. As a potential drawback,
this technique naturally implies a high computational over-
head as a main requirement.

B. ATTACKS ON SUPPORT VECTOR MACHINES (SVM)
Support Vector Machines (SVMs) could be targeted for the
poisoning attacks as shown in Figure 3 [36]. These attacks
harness prior knowledge not only over the training data, but
also onto the validation data and the hyperparameters of the
SVM learning algorithm. Then the poisoning integrates an
optimization method that maximizes the objective function
based on the classiﬁer error rate obtained over the validation
data as a function of the location of the poisoning sample, this
includes the class label as well. The optimization considers
both the support and non-support vectors subsets are con-
sidered unaffected by the insertion of the poisoning samples
during training.

Biggio et at. [37] showcase a poisoning attack on SVM
that requires as little as an insertion of one single poisoning
sample to cause a considerable amount of accuracy degra-
dation, demonstrating the high vulnerability of SVM models
against DP attacks. After conducting an extensive evaluation
of the target models performing over MNIST dataset, the
reported error increased up to a range between 15% and 20%
[37]. Poisoning attacks against SVM classiﬁers to increase

4

VOLUME 4, 2016

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

FIGURE 3. (Left) SVM classiﬁer decision boundary for two classes (Right)
Impact on decision caused by the re-location of one single data point [36].

the testing errors of the classiﬁer, crafted training data is fed
to the model has been also proposed [38]. Then based on
the SVM model’s optimal solution, a gradient ascent strategy
is deployed to construct the poisoning data. In addition,
this method enables optimization formulation and allows
itself to be kernelized. Nonetheless, such poisoning strategy
requires full knowledge of both the algorithm of interest to
the attacker and its training data.

C. ATTACKS ON CLUSTERING ALGORITHMS
Biggio et al. [39] performs a poisoning attack attempting
against the clustering process with a reduced number of
poisonous samples, assessing the effectiveness of the attack
by performing evaluations of the target model on handwritten
digits and malware samples, generating poisonous data sam-
ples by relying on a behavioral approach on malware clus-
tering. The algorithms itself computes the existing distance
between two clusters and deposits the poisoning samples
right between the both of them, which in question creates
conﬂicts with their decision boundaries, almost managing
to merge both into a single cluster, as seen in [40]. As a
result, the approach in both works generalize well among
clustering models; nonetheless the approach is only valid
in white-box scenarios, since the attacker assumes not only
having access to the training data, but also depends heavily
on prior knowledge of the feature space and the clustering
model itself.

D. ATTACKS USING GRADIENT OPTIMIZATION IN NN
Muñoz-González et al. [41] use a back-gradient optimization
to perform poisoning attacks on DL models (see Figure 4).
The gradient is calculated using automatic differentiation.
Also the learning process is reversed in order to lessen the
complexity of the attack. Such poisoning attacks display a
wider capability of attack, able to be employed for multi-
class problems rather than only binary classiﬁcation. In ad-
dition, the poisoning examples entailed in the training phase
do offer an adequate generalization over diverse learning
models. This kind of approach has proven to be effective
when dealing with handwritten recognition problems, spam
ﬁltering, and malware detection. Nonetheless, by performing
back-gradient optimization to generate one poisoning data
each time, the demand of even higher computation power
increases.

FIGURE 4. Framework on a deep neural network [42].

Yang et al. [43] addresses the possibility of generate poi-
soning data targeting NN by harnessing the power of tradi-
tional gradient-based methods, or direct gradient method, via
leveraging the weights of the target model. This work pro-
poses two poisoning methods, the ﬁrst one involving a direct
gradient method and the second an autoencoder/generator of
poisoning data. The generator approach with an autoencoder
acting as a generator being updated via a reward function
resulting from the loss. Then the model being targeted for
such an attack becomes the discriminator. Once the discrimi-
nator/target model receives the poisoning data to compute the
loss with respect to the normal data.

The auto-encoder (or generator) based method exceeds in
accelerating the generation rate of poisonous data by up to
239 times faster than relying on the direct gradient method.
However, direct gradient method achieves 91.1% of accuracy
degradation while the generative approach is about 83.4%,
representing just a minimal decline in this metric. In addition,
the generative method is proven to be superior in matters that
regard poisoning larger NN models and bigger datasets. Such
a difference is noticeable after conducting tests not only on
MNIST dataset [44]; but also using CIFAR- 10 dataset [45],
in which both poisoning schemes obtained similar accuracy
degradation.

E. ATTACKS USING GAN (GENERATIVE ADVERSARIAL
NETWORKS)
In a very parallel fashion to [43], Muñoz-González et al.
[46] devices a optimal poisoning named pGAN, consisting
of a generator that crafts the poisoning data; capable of both
maximizing the error of the target classiﬁer model, to degrade
its performance, and achieving undetectability against any
potential defense mechanism. The later features allows the
proposed mechanism to be tested with ML classiﬁer and
NN, modeling the attacking strategy with varios levels of
aggressiveness, also model the attack based on different de-
tectability constraints [34] against some potential mitigation
actions from the target model, testing then the robustness of

VOLUME 4, 2016

5

the algorithm in question. The generator produces poisoning
data based on the maximization of both the discriminator’s
loss and the classiﬁer’s loss on the poisoning data points. The
labor of the discriminator entails distinguishing honest data
and the generated poisoning data. The classiﬁer purpose is
minimizing a portion of the loss function containing a minor
portion of poisoning data points throughout the training
phase, identifying regions of the data distribution with prone
to vulnerabilities and yet more challenging to detect.

Based on the interaction of these three elements: generator,
discriminator and classiﬁer, a trade-off between detectability
and attack effectiveness is achieved and measured by the
computation of a hyperparameter alpha, indicating the prob-
ability of the crafted data to evade detection, the lower its
value, the more accuracy drop but the more chances of the
attack being detected, thereby the key elements consists on
controlling the value of this hyperparameter. Experimental
tests conducted on MNIST [44] and Fashion-MNIST [47]
show the effectiveness of the proposed poisoning technique
assuming 20% of poisoning samples. In contrast, accuracy
degradation using pGAN is less comparing it with plain
label-ﬂipping techniques. Nonetheless label-ﬂipping ignores
any detectability constraints, therefore label ﬂipping can only
outperform pGAN at the non-existing presence of a defense
mechanism.

Chen et al. [48] proposes DeepPoison as stealthy feature-
based data poisoning attack, capable of generating poisoned
training samples indistinguishable from the honest samplies
for human visual inspection, then making the poisoned sam-
ples mush less identiﬁable throughout the training process.
Also the scheme proposed displays high resistance against
other defense methods, since many existing defenses account
for attack success rates deployed with patch-based poisoning
samples. The scheme is based on a GAN composed of one
generator of poisoning data and two discriminators; one of
the discriminators sets the ratio of the poisoning perturba-
tions, while the second emulates the target model to test
the effects of poisoning. After certain evaluations on public
available datasets (Labeled Faces in the Wild1 and CASIA2),
DeepPoison shows to attain a maximum attack success rate
of 91.74% during testing phase, this by deploying only 7%
of poisoned samples. The performance is also analized when
performed against two anomaly detection defense mecha-
nisms: autodecoder defense [49] and cluster detection [50]
showing no steep drop in the ASR (maximum around 10%
drop).

F. FEATURE-BASED POISONING ATTACKS
Feature-based poisoning attacks could accomplish positive
aspects in terms of privacy preservation, since the poisoned
training samples remain indistinguishable from the honest

1http://vis-www.cs.umass.edu/lfw/
2http://www.cbsr.ia.ac.cn/english/IrisDatabase.asp

6

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

samples from the human visual perspective as seen in [48],
[51]. This property surges as another topic of interest in
devising a poisoning attack that can serve against abusive
data collection which often represents a risks of user privacy
violations. A potential application of such approach is men-
tioned in TensorClog [51], an attack could be intentionally
directed to the media content of an smartphone gallery app
just before sharing this data via social media, all of this
without affecting the human perception of the media content,
this mechanism is shown in Figure 5.

FIGURE 5. Privacy protection scheme with TensorClog [51].

TensorClog poisoning attack [51] attains the degradation
of the overall accuracy of the target machine learning model
by increasing the training loss by 300% and test error by
272% while preserving a high human visual similarity of
SSIM = 0.9905, index that estimates quantitatively the
change of human visual perception when assessing an exist-
ing similarity between two images. Being the previous results
the product of an experiment conducted in a real life sce-
nario over the CIFAR-10 dataset [45]. The TensorClog attack
technique is the result of clogging the back-propagation for
gradient tensors, minimizing the gradient norm, throughout
the training phase. Such a minimization of the partial deriva-
tive associated to the lost function weights practically leads
to a deliberately caused gradient vanishing, jeopardizing the
training process obtaining a larger converged loss. Further-
more, it succeeds in regularizing the added perturbation in
the dataset without compromising the same; avoiding any
possible human imperceptible perturbations by regularizing
the distance between the poisoned and the clean samples.
The effectiveness of TensorClog attack depends strongly on
the availability of some of the target machine learning model
information such as: Input-output pairs, model architecture,
pre-trained weights, initialize function for the trainable layer,
initialized value of the trainable layer. Having full access to
all of the 5 key elements mentioned above indicates an attack
on a white-box model/assumption; otherwise described as
black box, wherein the effectiveness of the poisoning attack
declines drastically.

VOLUME 4, 2016

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

G. ATTACKS ON CROWD-SENSING SYSTEMS
Crowd-sensing systems are vulnerable to data poisoning
due to the dearth of control over the worker’s identities.
Each poisoning strategy is based on creating interference
with the collected data via fake data injection. Li et al.
[52] addresses vulnerabilities issues related to crowdsensing
systems, such as TruthFinder’s framework, by developing
a poisonous attack strategy that considers access to local
information (obtained through the attacker’s sensing devices
and the results announced by TruthFinder) rather that global
information (overall distribution of data including honest
and poisoner workers). Modeling the proposed scheme as a
partially observable data poisoning attack based on deep rein-
forcement learning, allowing the poisoner workers to tamper
with TruthFinder and remain hidden within the network, the
optimization strategy enables the poisoner workers to learn
from past attack attempts and progressively improve. A series
of conducted experiments showcase the performance of the
proposed attack strategy called refered to as ‘DeepPois’ and
tested alongside another two baseline models, the metric to
compare is in terms of the rewards yielded by each method,
the cumulative reward as per every episode in DeepPois
reaches superior values than the others. Moreover, the cu-
mulative reward increases as the number of episodes does
since DeepPois considers historical attack experiences in the
learning process.

H. ATTACKS ON DATA AGGREGATION MODELS
Zhao et al. [53] analyzes poisoning attacks occurring on
data aggregation, commonly refer as garbage in, garbage
out; as representative example, an attacker could transmit
to the aggregator a set of poisoned locations. This work
focuses on the inputs of data aggregation and by so this work
proposes a novel attack framework capable of deploying
poisoning attacks on location data aggregation (PALDA);
in addition, PALDA accomplishes to disguise the behav-
ior of each launched attack. The entailed process requires
to deﬁne the poisoning attack as a min-max optimization
problem to solve via an iterative algorithm, which has been
proven theoretically to achieve convergence. The poison
strategy entails on tampering the aggregated results at the
output of the aggregation model obtained by the aggrega-
tor; this in form of the an error, obtained when comparing
the existing similarity in terms of mobility between honest
with the poisoned locations as the aggregated results. Then
PALDA minimizes the aggregation parameters of the model
and maximizes both the error of the aggregated results at
the output of the aggregation. Also, the attack strategy of
this approach is presumed to be extended to other settings,
representing a suitable option to linear decomposable ag-
gregation models that are executed by an aggregator. The
effectiveness of the proposed poisoning attack is simulated
onto six different GPS mobility datasets. Three of them
provided by Microsoft Research Asia, featuring the cities of
Beijing, Hongkong and Aomen; also the loc-Gwalla and loc-
Brightkite datasets (location-based social networks) and the

Athens truck dataset. A benchmark evaluation is conducted
by comparing the performance of PALDA alongside with the
poisoning attack named Synthesizing [54] and Baseline, the
later consisting on poisoned locations randomly generated
being fed to the aggregator. During the evaluation it is ob-
served a drastic increase in the MSE when running PALDA
scheme over the other two poisonous schemes. Nonetheless,
the entire evaluation is conducted assuming poisonous rate of
less than 20%.

The false positive and false negative rates are experi-
mentally tested for the three schemes, observing a superior
numbers with PALDA. Attributing then to PALDA the ca-
pability of disguising the behavior of each launched attack,
feature not present in the other two schemes. Making then
even harder for a defense system to detect such threads,
being honest users more likely to be considered poisoners;
which is mainly due to the algorithm optimizing approach.
Developing in the nearest future a defense system against
PALDA attacks cannot be successful by pondering the exist-
ing physical proximity among honest users in order to detect
the attackers, because PALDA succeeds in generating users
with a higher probability of accomplishing physical proxim-
ity to honest users. Then a better suggestion for a poisoning
defense could be presented in the form of a method that
entails simultaneously both physical proximity and social
relations as a network, assuming by then no existing social
interaction processed as side information between attackers
and honest users.

I. MISCELLANEOUS ATTACKS
Attacks on principal component analysis: Rubinstein et
al. [55] proposed a series of poisoning attacks on Principal
Component Analysis (PCA), this by inserting a minimum
portion of poisoned data; and as a result, the performance
of the detector diminishes drastically. However, the success
of this approach relies on binary classiﬁcation algorithms,
leaving aside any possibility of being deployed on generic
models nor learning algorithms.

Attacks against an speciﬁc defense: Koh et al. [56] pro-
poses a DP attack against a defense mechanism considered in
[57]. This scheme considers prior knowledge of the sanitiza-
tion defense as the main assumption that allows it to tamper
the accuracy of the ML model. This approach is successful at
evading the defenses since the poisoned samples are placed
in close proximity to the honest data; therefore result, they
cannot be considered outliers. However, this attack scheme is
heavily dependent on the attacker knowing both the training
and the test dataset.

IV. DEFENSE MECHANISMS
In this section, several defense techniques for ML are re-
viewed. Covering various countermeasures and different ap-
proaches published over the most recent years with the aim of
diminish the damage/possibility of a poison attack. These can

VOLUME 4, 2016

7

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

FIGURE 6. The poisoning attacks (2019-2021).

be divided in two kinds, protection for the data and protection
for the model, this last category has been segmented based on
whether the target model is a NN model or a non-NN model.

considerable computational costs for both performing the
testing on the updates on the client side and also transmitting
the sub-models to the clients.

A. DEFENSES IN COLLABORATIVE LEARNING
With a data poisoning attack an attacker aims to tamper the
integrity of a machine learning model or cybersecurity sys-
tem by maliciously modifying its behavior by modifying any
of the data examples [58] or by simply changing the labels
of the training data [59]. These kinds of defense mechanisms
aim to mitigate the DP based on collected data [60]. Collabo-
rative Learning is especially vulnerable to poisoning attacks
due to the fact that the servers, while generating the necessary
updates, lacks of any ability to look into the process. Zhao
et al. [61] presents a novel defense approach capable of
detecting anomalous updates, the scope of this application
covers iid (independent and identically distributed) and non-
iid settings. After performing an evaluation on MNIST [44]
and KDDCup datasets [62], the proposed solution is proven
to be robust enough against two distinctive label-ﬂipping
poisoning attacks. Upon realizing client/side cross validation,
the detection task is assigned to the clients in order to evaluate
the performance resulting from each update, which at the
same time is evaluated over other’s client’s local data. When
performing aggregation, according to the evaluation results,
the server is able to modify the weights associated to the
updates. The limitation of this scheme is associated to a

B. DEFENSES IN FEDERATED LEARNING
Federated Learning (FL) in recent years has become relevant
in privacy-preserving applications. This is possible since the
data gathered from each device or worker is kept locally
stored in each device [63], then enabling the training process
of a sub-machine learning model individually. As a second
step, only the resulting gradients obtained after training are
exchanged to a centralized server instead of the raw data, then
the centralized server performs the entire training lifecycle by
multiple iterations until attaining a desirable accuracy. Due
to the nature of FL, malicious users could perform a label-
ﬂipping attack [38] by deliverately inserting crafted gradients
leading to classiﬁcation errors during the test phase. In the
past it is been proven that a single poisoner can undermine
the whole training process and as a result the integrity of
the model. Therefore, a robust FL model needs to regard on
concerns related not only to data privacy, but also rely on a
certain degree of resilience against poisoning attacks and data
manipulations. Liu et al. [33] addresses the privacy/defense
related issue in FL models by showcasing a novel framework
called privacy-enhanced FL (PEFL). PELF grants the central
server the ability to detect malicious gradients and block

8

VOLUME 4, 2016

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

poisoner workers. By comparing the malicious gradients,
submitted by the poisoner workers, as a set of parameters
to the same ones belonging to the honest workers; the dif-
ference between malign and benign gradient vectors can be
evaluated by calculating the Pearson correlation coefﬁcient
[64]. Abnormality behavior is related to a lower correlation
coefﬁcient, then the action of the defense mechanism con-
sists on simply setting the weights of the malign model to
zero. PEFL claims superiority among other similar systems
such as Trimmed Mean [63], Krum [65] and Bulyan [66].
Since the proposed scheme does not assume to have any
knowledge of the total number of poisoners, posing then a
more appropriate defense more suitable for real-case sce-
narios. Furthermore, PEFL poses a higher resilience against
accuracy drops compared to Bulyan and Trimmed Mean
due to weight adjustment performed on each gradient which
guarantes trustworthiness within the remaining parameters.
Observing in the end a maximum attack success rate of
0.04, evidence of the robustness of the model against label-
ﬂipping.

C. MISCELLANEOUS DEFENSE MECHANISMS
SVM Resistance Enhancement [67] is targeted to avoide
label-ﬂipping attacks, being SVM particularly vulnerable
against this kind of attacks, causing total misclassiﬁcation
due to the computation of erroneous decision boundaries.
Thinking ahead about the effects of suspicious data points
within the SVM decision boundary, the proposed approach
considers a weighted SVM accompanied by KLID (K-LID-
SVM). This work introduces K-LID, a new approximation
of Local Intrinsic Dimensionality (LID), metric associated to
the outliners in data samples. K-LID computation relies on
the kernel distance involved in the LID calculation, allowing
LID to be computed in high dimensional transformed spaces.
Obtaining by such means the LID values and discovering as
a result three speciﬁc label dependent variations of K-LID
capable of counter the effects of label-ﬂipping. K-LID-SVM
attains higher overall stability against ﬁve different label-
ﬂipping attack variants: Adversarial Label Flip Attack (alfa)
attack, ALFA based on Hyperplane Tilting (alfa-tilt), Farﬁrst,
Nearestl and Random label ﬂipping; using ﬁve different
real-world datasets for a benchmark test: Acoustic, Ijcnn1,
Seismic and Splice and MNIST. The defense system attains
a drop of 10% on average in misclassiﬁcation error rates,
this method can distinguish poison samples from honest
samples and then suppress the poisoning effect. Therefore, it
succeeds in decreasing the potential magnitude of the attack
signiﬁcantly and demonstrating a superior performance than
traditional LID-SVM.

Bagging classiﬁer [68] is an alternative to detect poison-
ing samples, representing this method an ensemble method
that accomplishes the negative impact of the outliers in-
ﬂuence over the training data. Then Biggio addresses the
existing similarities between DP attacks and outlier detec-
tion problems, assuming a reduced number of outliers with

shifted distribution behavior. The training of this ensemble
method requires different training samples and considers
multiple classiﬁers as well. Thus the combination of the all
the predictions obtained from the multiple classiﬁers can be
harnessed in order to mitigate the strength of the poisoning
data/outliers. This defense approach is evaluated over two
scenarios, one including a web-based IDS and a spam ﬁlter.
However, a considerable computational power is demanded
to deploy the proposed defense system.

Kernel-based SVM [69] has been proposed to combat
DP attacks that entail mislabeling actions. This approach
showcases signs of improved robustness when diminishing
the SVM slackness penalty, present on margin violations,
enabling a higher number of samples to take part in the
estimation of the SVM’s weight vector. Albeit, the most
signiﬁcant contribution in this work is in attaining the sub-
stitution of the existing dual SVM objective function with an
dual function that is rather based on probability of accounting
for mislabeling rather than on the training labels.

ANTIDOTE [55] is a defense scheme to counteract the
actions of poisoning attacks treating anomaly detector. Based
on a statistical approach, ANTIDOTE is able to weaken the
effects of the poisoning attack by discarding the outliers
present in the training data.

KUAFUDET [70] is a defense strategy to combat DP
in malware detection. This defense system employs a self-
adaptive learning framework to detect and avoid suspicious
results falling in the category of false negative to take part of
the training process.

De-Pois [71] is an attack-agnostic defense system against
poisoning attacks named De-Pois. The defense strategy de-
picted in this work is not to attack-speciﬁc, i.e. it is not
designed to combat one type of attack in speciﬁc, but assert
in attaining more generality over its deployment than other
defense techniques. Regardless previous knowledge on the
type of machine learning model being targeted, nor the type
of assumptions taken by the attacker; De-Pois scheme can
discriminate between the poisoned data samples from the
honest samples is achieved based on the results of a pre-
diction computed to compare both the target and the mimic
models.

Firstly, De-Pois generates sufﬁcient synthetic training data
that resembles a similar distribution to the honest data sam-
ples by employing a cGAN (conditional GAN) to capture the
distribution of the clean data [72] and then it is used for data
generation and the inclusion of a discriminator to monitor
the data augmentation process. Afterwards, a conditional
version of WGAN-GP (Wasserstein GAN gradient penalty)
[73] is set to learn the distribution present in the predictions
related to the augmented training data, the mimic model is
obtained by extracting the discriminator part from WGAN-
GP as shown in Figure 7. As a result, the mimic model attains

VOLUME 4, 2016

9

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

FIGURE 7. Conﬁguration of De-Pois framework: cGAN-based for synthetic data generation, WGAN-GP for mimic model construction and poisoned data
recognition [71].

a similar prediction performance to the target model. Finally,
by employing a detection boundary the poisoned samples can
be set apart from the honest samples. Then gauging the output
from the mimic model with the obtained detection boundary
a sample can be regarded as either poisoned or honest. The
effectiveness of De-Pois is tested against various poisoning
attacks including CD [57], TRIM [74], DUTI [75], Sever [76]
and Deep-kNN [77] by applying commonly known image
sets including MNIST and CIFAR-10. De-Pois succeeds in
detecting poisonous data when facing every distinctive attack
scheme; performing better than most attack-speciﬁc defense
systems, obtaining an average value of over 0.9 for both
accuracy and F1-score.

k-Nearest-Neighbours defense scheme [34] is designed
to detect malicious data and counteract the effects of the
same, being this defense referred as Label sanitization (LS).
Label sanitization (LS) bases its defense on the decision
boundary of SVM, observing the remoteness of the poisoned
samples, commending these samples to be re-labelled. Stein-
hardt et al. [57] proposes a nearest-neighbor-based mecha-
nism to detect outliers and SVM optimization right after-
wards, getting as a result a domain-dependent upper bound
associated to the estimated highest drop in accuracy due to
a DP attack. A special assumption is made for this scenario,
declaring the removal of nonattack outliers inconsequential
to the performance of the target model.

Active Learning Involvement: Active learning systems
entails the intervention of a human expert commonly referred
to as an “oracle”. As a result, the number of labeled sam-
ples is increased by preforming a selection of the samples
from unlabeled batches, supposing enhanced results by just
performing libeling, most of the times labeling the samples
associated to higher uncertainty values (closer to the deci-
sion boundary). The main concept is to allow the model
to be trained again with these set of new labeled samples.
Some researchers suggest suggests a selection scheme that
considers a mixed sample scenario; in which, during the
labeling step, the selection process is performed in a random

manner [36]. The target model is then prone to generalize
better against DP by the introduction of a certain level of
randomness. Such a strategy promises a signiﬁcant reduc-
tion in the frequency of the attacker’s labeling attempts,
diminishing the resulting accuracy degradation. Human-in-
the-Loop is targeted to improve the accurate rate detection
of DP attacks can be addressed by the inclusion of a human
expert in the loop [78]. Such approach is useful for situations
where the detected attack requires further characterization
in order to propose a set of response actions. Then, rather
than simply detecting anomalies, the aim is to categorize
these; for instance, impeding the access to the same attacker.
In the long-run, a Human-in-the-Loop defense enables the
integration of active learning towards the inclusion of an
automated classiﬁer capable of mimicking and eventually
replace the human expert.

V. BRIEFS OF OUR SURVEY STUDIES
This section is mainly summarizing the contents what the
current studies in the survey have mentioned. The selected
discussion points and/or further research considerations are
covered in this section.

A. DISCUSSION ARCHIVES
It is important to highlight the attacker’s capability in other
scenarios and the assumptions imposed by the attacker, being
sometimes to optimistic while in other scenarios represent
more realistic conditions [71]. In the literature, there are
many examples related to assumptions in regards to the
capability of the attacker. For instance, TRIM [74] assumes
the ratio of the poisoning examples declared by the attacker
as known. Deep-kNN [77] assumes access to ground-truth
labels allowing the system to compare each sample’s k neigh-
bors with the class labels. CD [57] considers the inﬂuence of
the outliers as inconsequential to the drop in performance of
the target model. Based on this assumption, an approximation
of the upper bounds about the loss is tested against poisoning
attacks with non-convex settings. It is important to make a
clear distinction on the properties of the crafted poisoning
data, poisonous data obtained by performing label-ﬂipping is

10

VOLUME 4, 2016

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

FIGURE 8. The defense techniques (2018-2021).

one. Albeit, the scheme promises high accuracy degradation,
it is far from representing the most effective option for an
attacker. This is because label-ﬂipping is considered among
the most basic poisoning techniques and most of the existing
defense mechanism can detect these ones as outliers and
reject them with relatively ease.

On the other hand, other poisoning attacks are feature-
based, focusing solely on the sample’s feature representation
and by so relies on the distribution of the training data to
generate poisoning samples. This approach is more efﬁcient
since it does not rely on attacking capability, then the number
of samples necessary to cause the target model to drop in
accuracy is minimum. A clear example of this is shown
in DeepPoison [48] where only 7% of poisoned samples
were required to cause an devastating 91% drop in accuracy,
demostrating superior robustness when compared to other
poisoning schemes such as: BadNets [58], Poison Frog [79],
Invisible Poisoning attack [50], Fault Sneaking attack [80]
and various backdoor attacks [81]–[83].

Powerful poisonous attacks data generation based on GAN
such as [46] and [48], both present an attractive option that
offers high accuracy degradation with minimum attacking
capability, in some cases promising a much reduced training
time compared to other schemes, such as the direct gradient
method. Albeit, employing a GAN implies a higher com-
putational overhead, direct gradient method attains a higher
accuracy degradation than the GAN approach, as seen in [43]
GAN-based poisoning attacks can deliver equal to results
when dealing with a signiﬁcantly bigger training set. Most of
the defenses already explored in this work propose an end-
to-end solution to counteract DP attacks based on a detection
strategy, once the poisoning samples are identiﬁed they are
rejected from the training set, thus nullifying the attack.
Another approaches succeed in making the models more
robust against the effects of outliers, considered as poisoning
samples [55], [68]. Nonetheless, signiﬁcant computational

overhead is demanded continuously, also the three methods
limit themselves to binary classiﬁcation tasks.

B. SUGGESTED RESEARCH
Recent ML models can no longer be seen as black box
systems to be assessed solely on their results, but a deep
understanding of the model is necessary to identify security
ﬂaws that could lead to critical undesirable outcomes if not
properly addressed. Any breakthrough in the ﬁeld of machine
learning will always implicitly allow the introduction of new
vulnerabilities. Such vulnerabilities will always pose an open
window of opportunity for adversarial entities to exploit,
leading as a consequence to an opportunity to develop new
defenses counterbalance the outcome in the same matter
as it occurred with the invention of the internet and the
introduction of cybersecurity systems.

Ensemble methods represent another approach of potential
interest capable of detecting poisoning attacks over different
ML model architectures (ex. smart devices on crowd sensing
systems) where a partition of the original dataset is given to
every ML model to train with. A potential solution to this
problem would imply sending the local data from one smart
device to other devices that conform a dynamic ensemble,
this in order to perform predictions on the data received and
label them as either benign or malign before submitting it to
a query device. There are other metrics that imply a more
behavioral approach and can play an important role in the
prediction process and the selection process of the devices as
part of the ensemble to indicate the existence of a poisoning
attack threatening the model, a representative example would
be a trust/conﬁdence score.

The introduction of a uniﬁed method capable of per-
forming an exhaustive security evaluation over the model’s
robustness has not been formally attained. It is believed that,
if such method were established, the effects and implications
resulting from the interaction of both attack and defense

VOLUME 4, 2016

11

strategies would be better assessed in throughout every exper-
iment [84]. Such comprehensive approach could be extended
onto privacy related evaluations, focusing specially on the
training set and the hyperparameters associated to the target
model. Albeit, [24] proposes a method that could address
partially this need of comprehensive method, there is an wide
and growing area of opportunity in this matter.

VI. SURVEY ANALYTICS AND INSIGHTS
This section is the report of the survey results which includes
the general statistics and the major insights of the data poi-
soning attacks and their defense mechanisms. The qualitative
survey analysis for broad ranges of the studies is one of major
contributions of this survey.

A. SURVEY STATISTICS
More than 80 research papers (out of 106 references) have
been carefully reviewed for this survey study. According to
our survey, the research contributions which are related with
the data poisoning and its defense mechanisms are appeared
in the mid 2000 and have been widely studied in the late 2010
(see Figure 9). The related surveys [2], [8], [17], [84], [85] in
theses research topics are also appeared after 2018 (orange
color in Figure 9).

FIGURE 9. Number of publications on the survey.

Around 65% (i.e., 54% + 11%) of the research is about
the data poisoning attack methods and around 46% (i.e.,
35%+11%) of the research is about the defense mechanisms.
It is noted that some studies covers both attack and defense
together (A/D) which is around 11% (see Figure 10). One
of interesting insight is that more than 65% of papers are
registered on ArXiv.org which is an open-access repository
of research articles in the ﬁelds of mathematics, computers,
sciences and engineering [86].

B. DEFENSE MECHANISMS FOR TESTING NODES
According to our analysis, there are no data poisoning de-
fense mechanisms (see Figure 8) in the testing phase and
this result is aligned with the previous research [17], [84].
Most of defense methods are targeted to protect the integrity
of testing nodes (i.e., single or multiple trained machine
learning models). A trusted execution environment (TEE) is

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

FIGURE 10. Category of the survey.

a secure execution environment (mostly connected hardware
components) which guarantees code and data loaded inside to
be protected with respect to conﬁdentiality and integrity [17].
Although dedicated TEEs for AI systems have been widely
studied [87]–[90], the mechanisms for constructing secured
environments are broader than just AI dedicated ones. Non-
AI system dedicated defense mechanisms could be applied
to protect trained machine learning models as the TEE in the
testing phase [91]. Even error detection method for a ML sys-
tem could be also adapted for developing a TEE as a defense
mechanism [92]. The blockchain based defense mechanisms
have been proposed for guaranteeing the integrity within
connected nodes (i.e., network system components) [93]–
[95]. These techniques could be also applied into AI systems
as the defense mechanisms. It is noted that any security
mechanisms for improving integrity (and conﬁdentiality) of
systems could be considered as AI system dedicated TEEs.
In the other hand, the malware detection for AI models in
the testing phase is also another typical defense mechanism
to protect testing nodes [15], [32]. The TEE and the malware
detection are two major categories for the defense mechanism
in the testing phase [17].

C. ATTACK COST FOR MACHINE LEARNING SYSTEMS
Training models equipped with poisoning detection tech-
niques can either increase the resilience of a classiﬁer against
the insertion of training poisoning data. As a result, new ML
models are expected to develop a certain degree of immunity.
This will open a debate on the rising cost involved in the
data poisoning process, which is predicted to be more costly
in the years to come and could even discourage potential

12

VOLUME 4, 2016

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

attack attempt to be crafted in the ﬁrst place. The overall
sophistication of the attacks has increased gradually over the
recent years as a trend that is correlated to the resources
available to the attacker; for example, employ more advance
hardware such as GPUs. Then the resources of the attacker
seen as a ‘budget’ is becoming a factor worth to be analyzed
in the future, being predicted as an increasing trend in the
years to come. Most successful attacks reviewed in this work
consider the aim of the attacker to have an increased control
over future classiﬁcation over the model, based on this a
common practice is to train a ML model that resembles and
emulates the characteristics of the target model. This will
not only serve to test the effect of the attack but also to
approximate as close as possible to the separation surfaces
of the classiﬁer, having by then a low conﬁdence point, as an
initial point instead of starting at a random location without
having any knowledge over the target model. Another exam-
ple that associates the success of an attacker with its available
resources is reﬂected on the capability of the attacker to limit
itself to target linear models only or go beyond this scope.

D. NEURAL NETWORKS AND ADVERSARIAL
LEARNING
Neural network models showcase impressive performance
across a wide number of applications, making it the algorithm
of choice. Nonetheless NN poses a major ﬂaw in terms of
interpretability, which is a term used to describe the level
of understanding over the decision process performed by
a model on each prediction. Then interpretability poses a
challenge difﬁcult to overcome due to the nature of NN
algorithms, since low interpretability impedes tracking pre-
dictive processes involving complicated logic and mathemat-
ical algorithms. High interpretability will then pose a more
desirable scenario allowing the NN working mechanism to
be effortlessly analyzed. Low interpretability leads to vulner-
abilities related to privacy issues. Thereby it is foreseen that
in the nearest future such a concern will be weighted even
higher up to the point of being treated as an urgent matter.
Nonetheless, any improvement in matters of interpretability
can be harnessed not only by the defense, but by the attacker
as well. Thereby, dept understanding of the model would
facilitate an attacker to craft more effective adversarial ex-
amples and poisonous data. At some point, exploring new
vulnerabilities could present itself as a more feasible option
rather than detecting them prior to occur; thereby, giving the
attacker the upper hand over the defender [96].

VII. CONCLUSION
This paper offers a comprehensive survey on ML classiﬁers
covering data poisonous attacks during the training phase, en-
listing various types of attack schemes and countermeasures
in forms of defense strategies. Different DP attacks and their
associated defense strategies in various scenarios have been
investigated and compared in terms of their performances,
disclosing advantages and shortcomings for each approach.

Stronger attack approaches have become a trend in the ﬁeld
of deep learning, showing remarkable potentials in recent
years, that are capable of generating feature-based poisoning
data that are found to be relatively more effective than other
attack schemes. In addition, they target a wider variety of
classiﬁers and other defenses as well, generally effective
among diverse ML models. Moreover, the deep learning
approach has shown concerning privacy related vulnerabil-
ities which need to be addressed in the near future. Security
related issues in machine learning still remain as an active
research domain, which requires continued attentions from
researchers in the area for the years to come. More complex
security threats are predicted to emerge continuously which
in turn requires developments of more advanced defense
techniques to detect and counteract such threats. Therefore,
guarantying the robustness in any ML model against DP
is foreseen to become a priority and become an industry
standard.

REFERENCES
[1] M. Barreno, B. Nelson and et al., "Can machine learning be secure?" in
Proc. ACM Symp. Inf., Comput. Commun. Secur.-ASIACCS, 2006, pp. 16-
25.

[2] Z. Xu and J. H. Saleh, “Machine learning for reliability engineering and
safety applications: Review of current status and future opportunities,”
ArXiv.org, 2021. [Online] Available: https://arxiv.org/abs/2008.08221
[3] H. Olufowobi, R. Engel and et al., “Data provenance model for internet
of things (iot) systems. In Service-Oriented Computing,” in ICSOC 2016
Work- shops, Banff, AB, Canada, 2016, pp. 85–91.

[4] B. Biggio, I. Corona and et al., “Evasion attacks against machine learning
at test time,” in Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery
Databases, 2013, pp. 387–402.

[5] A. Paudice, L. Muñoz-González and et al., “Detection of adversarial train-
ing examples in poisoning attacks through anomaly detection,” ArXiv.org,
2018. [Online] Available: https://arxiv.org/abs/1802.03041.

[6] Z. Hu, B. Tan and et al., "Learning Data Manipulation for Aug-
[Online] Available:

mentation and Weighting," ArXiv.org, 2019.
https://arxiv.org/abs/1910.12795.

[7] M. Comiter,

“Attacking Artiﬁcial

Security
It,” Belfer
for Science and International Affairs, Harvard Kennedy
[Online] Available:

Vulnerability and What Policymakers Can Do About
Center
School. Cambridge, MA, USA, Aug. 2019.
https://www.belfercenter.org/publication/AttackingAI.

Intelligence: AI’s

[8] D. Miller, Z, Xiang and et al., "Adversarial Learning Targeting Deep
Neural Network Classiﬁcation: A Comprehensive Review of Defenses
Against Attacks," Proceedings of the IEEE, vol. 108, no. 3, pp. 402-433,
2020.

[9] X. Ma, B. Li and et al., “Characterizing adversarial subspaces us-
ing local intrinsic dimensionality,” ArXiv.org, 2018. [Online] Available:
https://arxiv.org/abs/1801.02613.

[10] Y. Ma, T. Xie and et al., "Explaining Vulnerabilities to Adversarial Ma-
chine Learning through Visual Analytics," IEEE Transactions on Visual-
ization and Computer Graphics, vol. 26, no. 1, pp. 1075-1085, 2020.

[11] D. Lowd and C. Meek, “Adversarial

learning,” in Proc. 11th ACM

SIGKDD Int. Conf. Knowl. Discov. Data Mining, 2005, pp. 641–647.
[12] N. Papernot, P. D. McDaniel and et al., “The limitations of deep learning in
adversarial settings,” in Proc. IEEE Eur. Symp. Secur. Privacy (EuroSI&P),
2016, pp. 372–387.

[13] S. Moosavi-Dezfooli, A. Fawzi, and et al., “Universal adversarial perturba-
tions,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2017,
pp. 86–94.

[14] D. Meng and H. Chen, “Magnet: A two-pronged defense against ad-
versarial examples,” in Proceedings of the ACM SIGSAC Conference on
Computer and Communications Security (CCS), Dallas, TX, USA, 2017,
pp. 135–147.

[15] K. Grosse, N. Papernot and et al., “Adversarial examples for malware
detection,” in Proc. 22nd Eur. Symp. Res. Comput. Secur., 2017, pp. 62–79.

VOLUME 4, 2016

13

[16] N. Rndic and P. Laskov, “Practical evasion of a learning-based classiﬁer:

A case study,” in Proc. IEEE Symp. Secur. Privacy, 2014, pp. 197–211.

[17] X. I. Liu, L. I. Xie and et al., "Privacy and Security Issues in Deep

[43] C. Yang, Q. Wu and et al., “Generative poisoning attack method
[Online] Available:

against neural networks,” ArXiv.org, 2017.
https://arxiv.org/abs/1703.01340.

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

Learning: A Survey," IEEE Access, vol. 9, pp. 4566-4593, 2020.

[18] D. Lowd and C. Meek, "Good word attacks on statistical spam ﬁlters," in
Proceedings of the Second Conference on Email and Anti-Spam (CEAS),
2005, pp. 1-8.

[19] J. Horkoff, "Non-Functional Requirements for Machine Learning: Chal-
lenges and New Directions," in 2019 IEEE 27th International Require-
ments Engineering Conference (RE), 2019, pp. 386-391.

[44] “MNIST,” 1998, [Online] Available: http://yann.lecun.com/exdb/mnist/
[45] “CIFAR-10,”

[Online]

2009,

Available:

http://www.cs.toronto.edu/kriz/cifar.html

[46] L. Munoz-Gonzalez, B. Pﬁtzner and et al., “Poisoning attacks with
[Online] Available:

generative adversarial nets,” ArXiv.org, 2019.
http://arxiv.org/abs/1906.07773.

[47] “FMNIST,”

2017.

[Online]

Available

[20] L. O. Nweke, "Using the CIA and AAA Models to Explain Cybersecurity

https://github.com/zalandoresearch/fashion-mnist

Activities", PM World Journal, vol. 6, no. 12, pp. 1-3, 2017.
trustworthiness in artiﬁcial

intelligence,

[21] Overview of
24028:2020.

ISO/IEC TR

[22] “Artiﬁcial

Intelligence,” National
Technology. Gaithersburg, MD, USA, 2021,
https://www.nist.gov/artiﬁcial-intelligence

Institute

of
and
Standards
[Online] Available:

[23] N. Carlini and D. Wagner, "Adversarial examples are not easily detected:
Bypassing ten detection methods," in Proc. 10th ACM Workshop Artif.
Intell. Secur.-AISec, 2017, pp. 3-14.

[24] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise of
adversarial machine learning,” ArXiv.org, 2018. [Online] Available:
https://arxiv.org/abs/1712.03141 .

[25] M. Nasr, R. Shokri and et al., “Comprehensive privacy analysis of deep
learning: Passive and active white-box inference attacks against central-
ized and federated learning,” in Proc. IEEE Symp. Secur. Privacy (SP),
2019, pp. 739–753.

[26] B. Hitaj, G. Ateniese and et al., “Deep models under the GAN: Information
leakage from collaborative deep learning,” in Proc. ACM SIGSAC Conf.
Comput. Commun. Secur., 2017, pp. 603-618.

[27] L. Melis, C. Song and et al., “Exploiting unintended feature leakage in
collaborative learning,” in Proc. IEEE Symp. Secur. Privacy (SP), 2019,
pp. 691-706.

[28] R. Shokri, M. Stronati and et al., “Membership inference attacks against
machine learning models,” in Proc. IEEE Symp. Secur. Privacy (SP), 2017,
pp. 3-18.

[29] Y. Long, V. Bindschaedler and et al., “Understanding membership infer-
ences on well-generalized learning models,” ArXiv.org, 2018. [Online]
Available: https://arxiv.org/abs/1802.04889.

[30] J. Hayes, L. Melis and et al., “LOGAN: Membership inference attacks
against generative models,” Proc. Privacy Enhancing Technol., vol. 2019,
no. 1, pp. 133-152, 2019.

[31] A. Salem, Y. Zhang and et al., “ML-leaks: Model and data independent
membership inference attacks and defenses on machine learning models,”
ArXiv.org, 2018. [Online] Available: https://arxiv.org/abs/1806.01246.
[32] W. Xu, Y. Qi and et al., “Automatically evading classiﬁers,” in Proc. Netw.

Distrib. Syst. Symp., 2016, pp. 1–15.

[33] X. Liu, H. Li and et al., "Privacy-Enhanced Federated Learning Against
Poisoning Adversaries," IEEE Transactions on Information Forensics and
Security, vol. 16, pp. 4574-4588, 2021.

[34] A. Paudice, L. Muñoz-González and et al., “Label sanitization against
label ﬂipping poisoning attacks,” in Proc. ECML PKDD, 2018, pp. 5–15.
[35] H. Xiao and C. Eckert, “Adversarial label ﬂips attack on support vector
machines,” in 20th European Conference on Artiﬁcial Intelligence (ECAI),
Montpellier, France, 2012, pp. 870–875.

[36] D. Miller, X. Hu and et al., "Adversarial learning: A critical review and
active learning study," in Proc. IEEE 27th Int. Workshop Mach. Learn.
Signal Process. (MLSP), 2017, pp. 1-6.

[37] B. Biggio, B. Nelson and et al., "Support vector machines under adversar-
ial label noise," in Proc. Asian Conf. Mach. Learn., 2011, pp. 97- 112.

[38] B. Biggio, B. Nelson and et

al.,

support vector machines,” ArXiv.org, 2012.
http://arxiv.org/abs/1206.6389.

“Poisoning attacks

against
[Online] Available:

[39] B.Biggio, I. Pillai and et al., “Is data clustering in adversarial settings
secure?” in Proc. ACM Workshop Artif. Intell. Secur. AISec, 2013, pp.
87–98.

[40] B. Biggio, K. Rieck and et al., “Poisoning behavioral malware clustering,”

in Proc. Workshop Artif. Intell. Secur. Workshop AISec, 2014, pp. 27–36.

[41] L. Munoz-Gonzalez, B. Biggio and et al., “Towards poisoning of deep
learning algorithms with back-gradient optimization,” in Proc. 10th ACM
Workshop Artif. Intell. Secur., 2017, pp. 27–38.

[42] K. Melcher,

Networks,”

ral
https://www.knime.com/blog/a-friendly-introduction-to-deep-neural-networks.

2021.

“A Friendly
KNIME,

Introduction

to
[Online]

[Deep] Neu-
Available:

[48] J. Chen, L. Zhang and et al., "DeepPoison: Feature Transfer Based Stealthy
Poisoning Attack for DNNs," IEEE Transactions on Circuits and Systems
II: Express Briefs, vol. 68, no.7, pp. 2618-2622, 2021.

[49] M. Du, R. Jia and et al., “Robust anomaly detection and backdoor attack
detection via differential privacy,” ArXiv.org, 2019. [Online] Available:
https://arxiv.org/abs/1911.07116.

[50] J. Chen, H. Zheng and et al., “Invisible poisoning: Highly stealthy targeted
poisoning attack,” in Proc. Int. Conf. Inf. Security Cryptol., 2019, pp.
173–198.

[51] J. Shen, X. Zhu and et al., "TensorClog: An Imperceptible Poisoning
Attack on Deep Neural Network Applications," IEEE Access, vol. 7, pp.
41498-41506, 2019.

[52] M. Li, Y. Sun and et al., "Deep Reinforcement Learning for Partially Ob-
servable Data Poisoning Attack in Crowdsensing Systems," IEEE Internet
of Things Journal, vol. 7, no. 7, pp. 6266-6278, 2020.

[53] P. Zhao, H. Jiang and et al., "Garbage In, Garbage Out: Poisoning Attacks
Disguised with Plausible Mobility in Data Aggregation," IEEE Transac-
tions on Network Science and Engineering, vol. 8, no. 3, pp. 2679-2693,
2021.

[54] V. Bindschaedler and R. Shokri, “Synthesizing plausible privacy-
preserving location traces,” in Proc. IEEE Symp. Secur. Privacy, 2016, pp.
546–563.

[55] B. I. P. Rubinstein, B. Nelson and et al., “ANTIDOTE: Understanding
and defending against poisoning of anomaly detectors,” in Proc. 9th ACM
SIGCOMM Conf. Internet Meas. Conf. (IMC), 2009, pp. 1–14.

[56] P. W. Koh, J. Steinhardt and et al., "Stronger data poisoning attacks
break data sanitization defenses," ArXiv.org, 2018. [Online] Available:
https://arxiv.org/abs/1811.00741.

[57] J. Steinhardt, P. W. Koh and et al., “Certiﬁed defenses for data poisoning

attacks,” in Proc. NIPS, 2017, pp. 3520–3532.

[58] T. Gu, B. Dolan-Gavitt and et al., “BadNets: Identifying vulnerabilities
in the machine learning model supply chain,” ArXiv.org, 2017. [Online]
Available: https://arxiv.org/abs/1708.06733.

[59] A. N. Bhagoji, S. Chakraborty and et al., “Analyzing federated learning
through an adversarial lens,” in Proc. 36th Int. Conf. Mach. Learn., 2019,
pp. 634–643.

[60] N. Baracaldo, B. Chen and et al., "Mitigating poisoning attacks on ma-
chine learning models: A data provenance based approach," in Proceed-
ings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security,
AISec@CCS, Dallas, TX, USA, 2017, pp. 103–110.

[61] L. Zhao, S. Hu and et al., "Shielding Collaborative Learning: Mitigating
Poisoning Attacks Through Client-Side Detection," IEEE Transactions on
Dependable and Secure Computing, vol. 18, no. 5, pp. 2029-2041, 2021.

[62] “KDDCup,”

1999.

[Online]

Available:

http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html

[63] G. Xu, H. Li and et al., “VerifyNet: Secure and veriﬁable federated
learning,” IEEE Trans. Inf. Forensics Security, vol. 15, pp. 911–926, 2020.
[64] J. Benesty, J. Chen and et al., “Pearson correlation coefﬁcient,” in Noise
Reduction in Speech Processing, Springer Topics in Signal Processing, vol
2., Berlin, Germany: Springer, 2009, pp. 1-4.

[65] P. Blanchard, Rachid Guerraoui and et al., “Machine learning with adver-
saries: Byzantine tolerant gradient descent,” in Proc. NeurIPS, 2017, pp.
119–129.

[66] E. M. E. Mhamd, R. Guerraoui and et al., “The Hidden Vulnerability of
Distributed Learning in Byzantium,” in Proc. ICML, 2018, pp. 3521–3530.
[67] S. Weerasinghe, T. Alpcan and et al., "Defending Support Vector Machines
Against Data Poisoning Attacks," IEEE Transactions on Information
Forensics and Security, vol. 16, pp. 2566-2578, 2021.

[68] B. Biggio, I. Corona and et al., “Bagging classiﬁers for ﬁghting poisoning
attacks in adversarial classiﬁcation tasks,” in Proc.10th Int. Conf. Mult.
Classif. Syst., 2011, pp. 350–359.

[69] H. Xiao, B. Biggio and et al., "Support vector machines under adversarial
label contamination," Neurocomputing, vol. 160, pp. 53-62, 2015.

14

VOLUME 4, 2016

[98] B. Nelson, M. Barreno and et al., "Misleading learners: Co-opting your
spam ﬁlter," in Machine Learning in Cyber Trust: Security, Privacy, and
Reliability, Berlin, Germany: Springer, 2009, pp. 17-51.

[99] H. Dang, Y. Huang, and et al., “Evading classiﬁers by morphing in the
dark,” in Proc. ACM SIGSAC Conf. Comput. Commun. Secur. (CCS), 2017,
pp. 119–133.

[100] J. Su, D. V. Vargas and et al., “One pixel attack for fooling deep neural
networks,” IEEE Trans. Evol. Comput., vol. 23, no. 5, pp. 828–841, Oct.
2019.

[101] A. Demontis, M. Melis and et al., “Yes, machine learning can be more
secure! A case study on Android malware detection,” IEEE Trans. De-
pendable Secure Comput., vol. 16, no. 4, pp. 711–724, 2019.

[102] Z. Zhu, Y. Lu and et al., "Generating Adversarial Examples by Makeup
Attacks on Face Recognition," in 2019 IEEE International Conference on
Image Processing (ICIP), 2019, pp. 2516-2520.

[103] C. Liu, B. Li and et al., “Robust linear regression against training data
poisoning,” in Proc. 10th ACM Workshop Artif. Intell. Secur. AISec, 2017,
pp. 91–102.

[104] J. Wen, B. Z. H. Zhao and et al., “With Great Dispersion Comes Greater
Resilience: Efﬁcient Poisoning Attacks and Defenses for Linear Regres-
sion Models," IEEE Transactions on Information Forensics and Security,
vol. 16, pp. 3709-3723, 2021.

[105] Y. Chen, C. Caramanis and et al., “Robust High Dimensional Sparse
Regression and Matching Pursuit,” ArXiv.org, 2013. [Online] Available:
https://arxiv.org/abs/1301.2725.

[106] R. Zhang and Q. Zhu, “A game-theoretic defense against data poisoning
attacks in distributed support vector machines,” in Proc. IEEE 56th Annu.
Conf. Decis. Control (CDC), 2017, pp. 4582–4587.

MA Ramirez, SK Kim, et al.: PA and Defenses, ArXiv.org, 2022.

[70] S. Chen, M. Xue and et al., “Automated poisoning attacks and defenses in
malware detection systems: An adversarial machine learning approach,”
Comput. Secur., vol. 73, pp. 326–344, 2018.

[71] J. Chen, X. Zhang and et al., "De-Pois: An Attack-Agnostic Defense
against Data Poisoning Attacks," IEEE Transactions on Information
Forensics and Security, vol. 16, pp. 3412-3425, 2021.

[72] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”
ArXiv.org, 2014. [Online] Available: http://arxiv.org/abs/1411.1784.
[73] F. Gulrajani, M. A. Ahmed and et al., “Improved training of Wasserstein

GANs,” in Proc. NIPS, 2017, pp. 5767–5777.

[74] M. Jagielski, A. Oprea and et al., “Manipulating machine learning: Poison-
ing attacks and countermeasures for regression learning,” in Proc. IEEE
Symp. Secur. Privacy (SP), 2018, pp. 19-35.

[75] X. Zhang, X. Zhu and et al., “Training set debugging using trusted items,”

in Proc. AAAI, 2018, pp. 1–8.

[76] I. Diakonikolas, G. Kamath and et al., “Sever: A robust meta-algorithm for

stochastic optimization,” in Proc. ICML, 2019, pp. 1596–1606.

[77] N. Peri, N. Gupta and et al., “Deep k-NN defense against clean-
label data poisoning attacks,” ArXiv.org, 2019. [Online] Available:
https://arxiv.org/abs/1909.13374.

[78] B. Miller, A. Kantchelian and et al., “Adversarial active learning,” in Proc.

Workshop Artif. Intell. Secur. (AISec), 2014, pp. 3–14.

[79] A. Shafahi, W. R. Huang and et al., “Poison frogs! targeted clean-label
poisoning attacks on neural networks,” in Proc. Adv. Neural Inf. Process.
Syst., 2018, pp. 6103–6113.

[80] P. Zhao, S. Wang and et al., “Fault Sneaking Attack: A stealthy framework
for misleading deep neural networks,” in Proc. 56th ACM/IEEE Design
Autom. Conf. (DAC), 2019, pp. 1–6.

[81] X. Chen, C. Liu and et al., “Targeted backdoor attacks on deep learn-
ing systems using data poisoning,” ArXiv.org, 2017. [Online] Available:
https://arxiv.org/abs/1712.05526.

[82] A. Saha, A. Subramanya and et al., “Hidden trigger backdoor attacks,”

Proc. AAAI Conf. Artif. Intell., vol. 34, pp. 11957–11965, 2020.

[83] B. Wang, Y. Yao and et al., "Neural cleanse: Identifying and mitigating
backdoor attacks in neural networks," in 2019 IEEE Symposium on Secu-
rity and Privacy (SP), San Francisco, CA, USA, 2019, pp. 707–723.
[84] M. Xue, C. Yuan and et al., "Machine Learning Security: Threats, Counter-
measures, and Evaluations," IEEE Access, vol. 8, pp. 74720- 74742, 2020.
[85] Y. He, G. Meng and et al., “Towards Security Threats of Deep
Learning Systems: A Survey," ArXiv.org, 2020. [Online] Available:
https://arxiv.org/abs/1911.12562.

[86] “ArXiv,” 2021. [Online] Available https://arxiv.org/
[87] Q. Xia, Z. Tao and et al., “FABA: an algorithm for fast aggregation against
byzantine attacks in distributed neural networks,” in Proceedings of the
Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence
(IJCAI), Macao, China, 2019, pp. 4824–4830.

[88] B. Wang and N. Z. Gong, “Stealing hyperparameters in machine learning,”
in IEEE Symposium on Security and Privacy (SP), San Francisco, Califor-
nia, USA, 2018, pp. 36–52.

[89] F. Tramer, F. Zhang and et al., “Stealing machine learning models via
prediction apis.” in 25th USENIX Security Symposium, USENIX Security
16, Austin, TX, USA, 2016, pp. 601–618.

[90] M. Juuti, S. Szyller and et al., “PRADA: protecting against DNN
[Online] Available:

attacks,” ArXiv.org,

2018.

model
https://arxiv.org/abs/1805.02628.

stealing

[91] Z. Chen, N. Lv and et al., "Intrusion Detection for Wireless Edge Networks
Based on Federated Learning," IEEE Access, vol. 8, pp. 217463-217472,
2020.

[92] A. Chakarov, A. Nori and et al., "Debugging machine learning tasks,"
ArXiv.org, 2016. [Online] Available: https://arxiv.org/abs/1603.07292.
[93] S.-K Kim, "Blockchain Governance Game", Computers & Industrial

Engineering 136, 2019, pp. 373-380.

[94] S.-K Kim, "Strategic Alliance for Blockchain Governance Game," Probab.

Eng. Inf. Sci., 2020, pp. 1-17.

[95] S.-K Kim, "Enhanced IoV Security Network by Using Blockchain Gover-

nance Game," Mathematics, 9:2, 2021, 109.

[96] M. Veale, R. Binns and et al., “Algorithms that remember: Model inversion
attacks and data protection law,” Phil. Trans. R. Soc., vol.376, 20180083,
2018.

[97] B. Nelson, M. Barreno and et al., “Exploiting machine learning to subvert
your spam ﬁlter,” in Proc. USENIX Workshop Large-Scale Exploit. Emerg.
Threat., 2008, pp. 1–9.

VOLUME 4, 2016

15

