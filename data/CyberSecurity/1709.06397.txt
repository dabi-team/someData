A Deep Learning-based Framework for Conducting
Stealthy Attacks in Industrial Control Systems

7
1
0
2

p
e
S
9
1

]

R
C
.
s
c
[

1
v
7
9
3
6
0
.
9
0
7
1
:
v
i
X
r
a

Cheng Feng∗, Tingting Li∗, Zhanxing Zhu† and Deeph Chana∗
∗Institute for Security Science and Technology
Imperial College London, London, United Kingdom
Emails: {c.feng, tingting.li, d.chana}@imperial.ac.uk
†Peking University and BIBDR, Beijing, China
Email: zhanxing.zhu@pku.edu.cn

Abstract—Industrial control systems (ICS), which in many
cases are components of critical national
infrastructure, are
increasingly being connected to other networks and the wider
internet motivated by factors such as enhanced operational
functionality and improved efﬁciency. However, set in this context,
it is easy to see that the cyber attack surface of these systems is
expanding, making it more important than ever that innovative
solutions for securing ICS be developed and that the limitations of
these solutions are well understood. The development of anomaly
based intrusion detection techniques has provided capability for
protecting ICS from the serious physical damage that cyber
breaches are capable of delivering to them by monitoring sensor
and control signals for abnormal activity. Recently, the use of so-
called stealthy attacks has been demonstrated where the injection
of false sensor measurements can be used to mimic normal control
system signals, thereby defeating anomaly detectors whilst still
delivering attack objectives. To date such attacks are considered
to be extremely challenging to achieve and, as a result, they have
received limited attention.

In this paper we deﬁne a deep learning-based framework
which allows an attacker to conduct stealthy attacks with minimal
a-priori knowledge of the target ICS. Speciﬁcally, we show that
by intercepting the sensor and/or control signals in an ICS
for a period of time, a malicious program is able to auto-
matically learn to generate high-quality stealthy attacks which
can achieve speciﬁc attack goals whilst bypassing a black box
anomaly detector. Furthermore, we demonstrate the effectiveness
of our framework for conducting stealthy attacks using two real-
world ICS case studies. We contend that our results motivate
greater attention on this area by the security community as we
demonstrate that currently assumed barriers for the successful
execution of such attacks are relaxed. Such attention is likely
to spur the development of innovative security measures by
providing an understanding of the limitations of current detection
implementations and will inform security by design considerations
for those planning future ICS.

I.

INTRODUCTION

Industrial Control Systems (ICS) generally consist of a
set of supervisory control and data acquisition (SCADA)
subsystems for controlling ﬁeld devices via the monitoring and
processing of data related to industrial processes. In response
to information received from remote sensors, control com-
mands are issued either automatically, or manually, to remotely
located control devices, which are able to make physical
changes to the states of one or more industrial processes.
In the pursuit of increased communication efﬁciency and
higher throughputs, modern information and communication

technologies (ICT) have been widely integrated into ICS. For
instance, cloud computing has recently emerged as a new com-
puting paradigm for ICS, proposing the conversion of localised
data historians and control units into cloud based services [1].
Such an evolution satisﬁes the growing needs from operators,
suppliers and third-party partners for remote data access and
command execution from multiple platforms. A trend which,
consequently, exposes modern ICS to an increased risk of
cyber attacks. Unlike conventional ICT systems, ICS security
breaches have the potential to result in signiﬁcant physical
damage to national infrastructure; resulting in impacts that can
include large scale power outages, disruption to health-service
operations, compromised public transport safety, damage to the
environment and direct loss of life.

The ﬁrst well known ICS-targeted cyber virus – Stuxnet [2]
– popularised cyber security vulnerabilities of ICS and demon-
strated how external attackers might feasibly penetrate multiple
layers of security to manipulate the computer programs that
control their ﬁeld devices. Since then, according to sources
such as ICS-CERT, the number of attacks recorded against
ICS targets has grown steadily. In 2014 [3] 245 incidents
were reported to ICS-CERT by trusted industrial partners. This
ﬁgure increased to 295 in 2015 [4] and then again to 290
by 2016 [5]. More recently than Stuxnet, a major security
breach to a German steel mill was widely reported in 2014.
This attack was initiated by credential theft via spear-phishing
emails, leading to massive damage to a blast furnace [6] at the
plant and illustrating clearly the cyber-physical nature of ICS
security. Even more recently, in December 2015, Ukrainian’s
capital city Kiev reportedly lost approximately one-ﬁfth of
its required power capacity as a result of a utility-targeted
cyber attack that caused a massive blackout, affecting 225,000
citizens [7].

As an approach to implementing effective intrusion detec-
tion systems (IDS), anomaly detection has become recognised
as part of the standard toolkit for protecting ICS from cyber
attacks. In recent years many ICS-speciﬁc anomaly detection
models have been proposed [8], [9], mainly incorporating
blacklist-based or whitelist-based methods. These IDS are
designed to detect intrusions by consideration of common ICS
communication protocols (e.g. Modbus/DNP3) [10], ICS oper-
ating standards [11], [12] and the identiﬁcation of patterns and
structures of data transmitted across ICS networks. Amongst
the various underpinning technologies used in existing IDS,
machine learning (ML) techniques have enabled detecting

 
 
 
 
 
 
modalities that exploit automated learning from past experi-
ence and the construction of self-evolving models that can
adapt to evolving/future classiﬁcation problems. To date, ML
approaches that have shown promising capabilities for securing
ICS include: Support Vector Data Description (SVDD) [13],
Bloom ﬁlters [14], statistical Bayesian networks [15] and deep
neural networks [16], [17], etc.

Furthermore, anomaly detection techniques employed at
the ﬁeld network layer provide protection at a point within
ICS where security compromise has the greatest potential
for causing signiﬁcant physical damage to the system. At
present, most anomaly detection implementations designed
for this speciﬁc purpose rely on a predictive model, where
future sensor measurements predictions are generated based
on historical signals and compared with real measurements
using a thresholded residual error. Comparisons exceeding the
deﬁned threshold constitute a detection event and generate
an alert [18], [19], [20], [16]. However, recent studies have
shown that cyber attacks may be generated by the injection
of false sensor measurements that avoid the detection by such
systems [21], [22]. Needless to say, such stealthy attacks are
amongst the most harmful class of attacks against ICS that
exist, as malicious sensor measurements can directly violate
the operational safety bounds and conditions set for a given
ICS. Although efforts to understand the risks and mitigations
related to stealthy attacks on ICS have been made [23], [24],
it is still generally believed that this class of attacks has a
low likelihood of occurrence. This is due, in most part, to the
level and type of operational information of the system that is
thought to be needed by an attacker a-priori; information that
is challenging to obtain.

In this work we challenge the above strong conditions
imposed on a stealthy attacker by formulating and demon-
strating an attack framework that uses a deep learning-based
methodology. The deep learning method proposed allows the
attacker to effectively conduct stealthy attacks which lead
to a speciﬁc amount of deviation on sensor measurements
with minimal knowledge of the target ICS. Speciﬁcally, our
framework consists of set of deep neural network models,
which are trained by a recently proposed adversarial training
technique – Wasserstein GAN [25], a special variant of Gener-
ative Adversarial Net (GAN) [26] to maximize the likelihood
that generated malicious sensor measurements will success-
fully bypass an assumed black-box anomaly detector. This
technique signiﬁcantly lowers the bar for conducting stealthy
attacks as generating high-quality attacks can be automatically
achieved by intercepting the sensor and/or control signals in
an ICS for a period of time using a particularly designed real-
time learning method. In addition, the explicit relaxation of
conditions relating to speciﬁc operational knowledge within
the attack design demonstrates the potential for its general
applicability across ICS instances and industrial sectors. To
show this we demonstrate the effectiveness of our framework
by two case studies in which we conduct stealthy attacks on
a gas pipeline and a water treatment system, respectively.
Overall, our results indicate that more attention to stealthy
attacks is urgently required within the ICS community.

Fig. 1: ICS control network

II. BACKGROUND

By way of providing relevant background for the work
undertaken here, this section provides an brief overview of
the ICS control network in Section II-A. The generation and
detection of stealthy attacks is brieﬂy discussed in Section II-B.
Following this, Section II-C provides information on the key
underpinning techniques of deep neural networks employed
here.

A. ICS Control Network

We summarize the main control loops of a typical ICS
in Figure 1, showing the main communication transactions
within it. Speciﬁcally, in the ﬁeld network level, programmable
logic controllers (PLCs) or Remote Terminal Units (RTUs)
receive real-time measurements from sensors and forward
them to the Human Machine Interface (HMI) in the control
network level. HMI processed data is then again forwarded
to the Master Control Station in the supervision level. In
response to the received inputs, the Master Control Station
then issues necessary control commands to change the states
of its monitored industrial processes. Commands eventually
make their way to relevant actuators to make the physical
changes required. Intrusion detection systems (IDS) are often
deployed on the master control stations, where the sensor
measurements and control signals are monitored to secure the
physical processes under control. We refer to Section III for
the detailed description about the mechanisms for securing
physical processes in ICS.

B. Generating and Revealing Stealthy Attacks

Stealthy attacks, or more generally mimicry attacks, have
been studied in the community of conventional IT security for
decades. Such attacks can achieve speciﬁc attack goals with-
out introducing illegal control ﬂows into computer programs.
Existing IDS for computer programs were mainly relied on
short call sequence veriﬁcation, which were unable to detect
such attacks. The work in [27] proposed an IDS implemented
by two-stage machine learning algorithms to construct normal
call-correlation patterns and detect stealthy intrusions. In the
domain of monitoring the behaviours of applications, system
calls have been used extensively to construct
the normal
proﬁles for intrusion detection [28], [29]. However, it has been

2

FIELDCONTROLSUPERVISIONPLC/RTUSensorActuatorHMIHMI...IDSMaster Control StationIDSMaster Control StationmeasurementscommandsPLC/RTUSensorActuatorPLC/RTUSensorActuator...HMIshown that such IDS are inadequate to detect mimicry attacks
[29] (i.e. attacks that interleave the malicious code with legit
code) and impossible paths attacks [30] (i.e. attacks relying
on a legal yet never executed sequence of system calls). The
work presented in [29] was one of the earliest studies on
mimicry attacks against host-based IDS, where a theoretical
framework was proposed to evaluate the effectiveness of an
IDS combating mimicry attacks. A waypoint-based IDS was
introduced in [30] to detect both mimicry attacks and impos-
sible paths attacks by considering the trustworthy execution
contexts of programs and restricting system call permissions.
Besides, there were also existing work conducted on generating
mimicry attacks by using generic programming [31] or static
binary analysis [28].

In many successful

ICS attacks, physical damage is
achieved from an exploitation phase that utilises methods of
injecting false sensor measurements into the control network.
As discussed brieﬂy previously, such attacks have also been
described as Stealthy Attacks. An early practical description
of such an attack is provided in [22]. Here the ability of an
attacker to insert arbitrary errors into the normal operation
of a power system without being detected by an implemented
state estimator IDS is shown. It was pointed out that launching
such attacks do pose strong requirements for the attackers
as the conﬁguration of the targeted power system must be
known. Further aspects of stealthy attacks have been studied in
[21], [32]. Speciﬁcally, two security indices were introduced in
[32] to quantify the difﬁculty of launching successful stealthy
attacks against particular targets and the work in [21] has
proposed an efﬁcient way to compute such security indices for
sparse stealthy attacks. A stealthy deception attack against a
canal system is shown in [33] and the effect of stealthy attacks
on both the regulatory layer and the supervisory layer of an ICS
is discussed. In [23], the authors proposed to reveal stealthy
attacks in control systems through modifying the system’s
structure periodically. Recently, the authors in [24] showed
that the impact of stealthy attacks can be mitigated by the
proper combination and conﬁguration of different off-the-shelf
detection schemes.

C. Deep Neural Networks

Recently, deep learning [34] has achieved remarkable
success and improved the state-of-the-art in various artiﬁcial
intelligence applications including visual object classiﬁcation,
speech recognition and natural language understanding. It is
a powerful modeling framework in machine learning, where
multiple processing layers are used to extract features from
data with multiple levels of abstraction. In this work, several
neural network architectures are used to conduct feature ex-
traction, anomaly detection and stealthy attacks generation. We
describe these architectures below.

1) Feedforward Neural Network (FNN): The FNN, also
called multi-layer perceptron (MLP), describes the most classic
form of neural network (NN) where multiple processing nodes
are arranged in layers such that information only ﬂows in one
direction – from input to output. The architect of a typical
FNN is illustrated in Figure 2. In this architecture the j-th
node in l-th layer computes linear combination of its inputs
(i.e. the outputs of last layer) followed by a simple non-linear

Fig. 2: The architect of a typical FNN

transformation f (·),

o(l)
j = f

(cid:32)

(cid:88)

i

W (l)

ij o(l−1)

i

(cid:33)

+ b(l)
j

,

where we often use rectiﬁed linear unit (ReLU) as the non-
linear function f (z) = max(0, z), the input x = o(0). And the
model parameters θ = {W (l), b(l)}L
l=1 needs to be learned by
minimizing certain loss function

J(θ) =

1
N

N
(cid:88)

i=1

e(o(L)(xi), yi; θ),

given the training data {(xi, yi)}N
i=1, where e(·, ·) is some
chosen criterion to measure the difference between the pre-
diction and ground truth, such as squared loss for regression
and cross entropy for classiﬁcation problem. Often stochastic
gradient descent (SGD) can be used to minimize the loss
J(θ). Concretely, in each iteration, a mini-batch of training
samples are selected randomly to estimate the true gradient,
∇θ ˜J = (cid:80)m
j=1 ∇θe(o(L)(xj), yj; θ), where m is the number
of samples inside each mini-batch. Then we can update the
parameters by θ ← θ − α∇θ ˜J with some given learning rate
α.

2) Recurrent Neural Network (RNN): In contrast to FNNs,
RNNs permit cyclical connections between nodes allowing
such neural networks to exhibit dynamic temporal properties in
operation. RNNs [35] are suitable for dealing with sequential
data, such as speech, language and structured time series. An
input sequence is processed by an RNN one element at a time,
and the information is encoded into a hidden unit (i.e. a state
vector) for each time step that describes the history of all the
past elements of the sequence. The outputs of the hidden units
at different time steps can be compared with the ground truth
of the sequence such that the network can be trained. The
most commonly implemented RNNs fall into the class of long
short-time memory (LSTM [36]) neural networks. As the name
suggests, such NNs exhibit remarkable empirical performance
for extracting/preserving long-term dependencies whilst also
maintaining short-term signals. LSTM networks involve three
gates in the computation of each hidden cell to determine what
to forget, what to output and what to be provided to next hidden
cell, respectively, as shown in Figure 3. The information ﬂow

3

apply Wasserstein distance to measure the discrepancy between
the two probability distributions, hence the name, Wasserstein
GAN (WGAN). The min-max optimization problem of WGAN
can be formulated as,

min
θG

max
|θD |∞≤c

Ex∼pdata(x)[D(x; θD)] − Ez∼pz (z)[D(G(z; θG))].

Compared with original GAN,

(6)
the discriminator D(·) of
WGAN, also called the “critic”, can output any real value,
not just probabilities. The inﬁnity norm of θD is constrained
to be less than a predeﬁned positive constant c, and practically
this can be achieved by “clipping” the elements larger than c
to c (and elements smaller than −c to −c) in each iteration.
And the expectation term in Eq. (6) can be approximated by
using m random samples from training data and pz(z).

(1)

(2)

We summarize the training procedure of WGAN in Alg. 1,
where a variant of stochastic gradient descent, RMSProp [37],
is used for updating the parameters.

Algorithm 1 Training Procedure of WGAN [25]

Require: the learning rate α = 0.00005, the clipping param-
eter c = 0.01, the number of iterations of the critic per
generator iteration n = 5, the size of mini-batch, m.

D and θ0
G.

Require: the initial parameters θ0
1: while θ has not converged do
2:
3:
4:

for t = 0, . . . , n do
Sample {x(i)}m
Sample {z(i)}m
samples.
Evaluate stochastic gradient of θD:

i=1 a mini-batch from training data.
i=1 ∼ pz(z) a mini-batch of prior

δθD ← ∇θD [

m
(cid:88)

i=1

D(x(i); θD)/m −

m
(cid:88)

i=1

D(G(z(i); θG); θD)/m]

θD ← θD + α · RM SP rop(θD, δθD )
θD ← clip(θD, −c, c)

end for
Fix θD
Sample {z(i)}m
ples.
Evaluate stochastic gradient of θG:

i=1 ∼ pz(z) a mini-batch of prior sam-

δθG ← −∇θG

m
(cid:88)

i=1

D(G(z(i); θG); θD)/m

5:

6:
7:
8:
9:
10:

11:

θG ← θG + α · RM SP rop(θG, δθG )

12:
13: end while

III. ANOMALY DETECTION MECHANISM FOR SECURING
PHYSICAL PROCESSES

As discussed in Section II-A, the physical process in ICS
is directly controlled by the ﬁeld network which consists of
a number of ﬁeld devices called sensors, actuators and PLCs.
Speciﬁcally, the sensors are devices which convert physical
parameters into electronic measurements; actuators are devices
which convert control commands into physical state changes
(e.g.,
turning a pump on or off); based on measurements
received from sensors through PLC-sensor channels, PLCs
send control commands to actuators through PLC-actuator
channels.

Fig. 3: LSTM cell

of LSTM cell is as follows,

f (t) = σ

g(t) = σ

q(t) = σ

(cid:16)

(cid:16)

(cid:16)

,

bf + U f x(t) + W f h(t−1)(cid:17)
bg + U gx(t) + W gh(t−1)(cid:17)
bq + U qx(t) + W qh(t−1)(cid:17)
(cid:16)

,

,

s(t) = f (t) (cid:12) s(t−1) + g(t) (cid:12) tanh

b +

h(t) = tanh (cid:0)s(t)(cid:1) (cid:12) q(t),

(3)
U x(t) + W h(t−1)(cid:17)

,

(cid:88)

j

(4)

(5)

where σ(·) and tanh(·) represent the sigmoid and hyperbolic
tangent function, respectively, and (cid:12) denotes the element-wise
product.

3) Generative Adversarial Net (GAN): GANs [26] are an
example of generative models that aim to learn an estimate (i.e.
pmodel(x)) of the data distribution pdata(x), given the training
samples drawn from pdata(x), such that we can generate new
samples from the pmodel(x).

The general idea of GANs is to construct a game between
two players. One of them is called the generator, which
intends to create samples following the same distribution as
the training data. The other player is the discriminator which
tries to distinguish whether the samples (obtained from the
generator) are real or fake. When the discriminator cannot tell
apart generated and training samples, implying that we have
learned the data distribution, i.e. pmodel(x) = pdata(x).

The generator is simply a differentiable function x =
G(z; θG) with model parameters θG, where the input z
follows a simple prior distribution, such as uniform and
Gaussian distribution. Due to the high capacity of multi-layer
neural networks, they are often used as the G(·) function, and
its output x ∼ pmodel(x). The discriminator is a two-class
classiﬁer, D(x; θD) often designed as a neural network to
output class probability between 0 and 1. GAN aims to solve
the following min-max optimization problem,

min
θG

max
θD

Ex∼pdata(x)[log D(x; θD)]+Ez∼pz (z)[log(1−D(G(z; θG)))]

An alternative update of θD and θG by SGD can be adopted
to solve this problem. Then we can use the optimized x =
G(z; θ∗
G) to generate new samples through the random input
z.

Unfortunately, the training of the original GAN is very
instable in practice, and users have to balance both of the
capacity and training steps between the generator and discrim-
inator. To overcome this issue, the authors of [25] proposed to

4

{y(t)

1 , y(t)

2 , . . . , y(t)

To protect the control system from physical faults and
cyber attacks, anomaly detection mechanisms are often de-
ployed by monitoring the sensor measurements and control
commands in the system at discrete time steps. Concretely, let
X = {x(1), x(2), . . .} be a time-series in which each signal
x(t) = {y(t), u(t)} ∈ Rm+n is a (m + n)-dimensional vector
m , u(t)
whose elements correspond to m values representing all the
sensor measurements and n values capturing all
the con-
time t. Currently, most
trol commands in the system at
anomaly detection mechanisms for the control system rely on
a predictive model which predicts the sensor measurements
ˆy(t) = {ˆy(t)
m } based on previous signals, and an
alarm is triggered if the residual error between the predicted
measurements and the true measurements exceeds a speciﬁc
threshold.

2 , . . . , u(t)
n }

2 , . . . , ˆy(t)

1 , u(t)

1 , ˆy(t)

A. Predictive Models

The underlying predictive model can take many different
forms, among which the Auto-Regressive (AR) model [20],
[24] and the Linear Dynamic State-space (LDS) model [23],
[24] (it is often called as state estimator in power systems
[19]) are the most commonly used. Speciﬁcally, the AR model
predicts ˆy(t) by ﬁtting a linear regression model for each
sensor measurement based on its p previous values:

ˆy(t)
i =

p
(cid:88)

j=1

αjy(t−j) + α0 ∀i ∈ {1, 2, . . . , m}.

where αj, ∀j ∈ {1, . . . , p} are coefﬁcients representing the
weights of the measurement at time t − j for predicting ˆy(t),
α0 is a constant.

The LDS model assumes a vector w ∈ Rk to denote the
physical state of the system, then ˆy(t) can be inferred by the
following equations:

w(t) = A w(t−1) + B u(t−1) + (cid:15)(t−1)
ˆy(t) = C w(t) + D u(t) + ε(t)

where A, B, C, D are matrices capturing the dynamics of the
physical system, (cid:15)(t−1) and ε(t) are vectors of noise for state
variables and sensor measurements with a random process with
zero mean. In general, D = 0 because sensor measurements
only depend on the current physical state in most systems.
Then, to predict ˆy(t), one can use y(t−1) and u(t−1) to obtain
an estimate of the current system state ˆw(t), and predict ˆy(t) =
C ˆw(t).

Since the system dynamics in many ICS are highly non-
linear, recently people have found that deep learning models
can achieve better prediction accuracy than the linear models.
For example, the authors in [16] shows that the LSTM model
can be employed to predict:

h(t) = f (x(t), h(t−1))
ˆy(t) = Wyh(t−1) + by

where h(t) is a hidden vector computed iteratively by the ﬁrst
equation which encodes the previous time series signals to
provide the context for predicting the sensor measurements at

5

time t; f is a complex function as a short form of Equations 1
to 5; Wy and by represent the weight matrix and the bias vector
respectively for decoding the hidden vector to the predicted
sensor measurements.

B. Detection Methods

Based on the prediction of the predictive model, an anoma-
lous signal can be detected when the Euclidean distance
between the predicted sensor measurements and their obser-
vations exceeds a speciﬁc threshold: (cid:107)ˆy(t) − y(t)(cid:107) > τ , where
(cid:107)ˆy(t) − y(t)(cid:107) is often called the residual error at time point t.

Instead of solely relying on the residual errors at a single
time point, we could also take account of the history of
residual errors. For example, we can apply the Cumulative
Sum (CUSUM) method [38] for detecting collective anoma-
lies. Speciﬁcally, let r(t) denote the residual error at time t,
assuming residual errors at different time points are indepen-
dent and identically distributed with mean µ and variance
σ2, the CUSUM method will detect anomalies based on an
accumulated statistic H (t) such that:

H (t) = max(0, H (t−1) + r(t) − µ − ω)

with initials H (0) = 0; ω is often set to a reference value such
as σ. Then, if the cumulative sum H (t) reaches the predeﬁned
threshold τ , an alarm is triggered. After an alarm is triggered,
H (t) is set to 0 again, and a new round of detection is initiated.

Since in many cases, anomalies are not presented in the
training phase of the detection models, thus the threshold value
for the residual errors is often decided by tunning the expected
false alarm rate. For the CUSUM method, the expected time
between false alarms can also be tuned to decide its threshold
value.

IV. STEALTHY ATTACK MODEL

In this section, we formally deﬁne the stealthy attacks
which will be generated using our deep learning framework.
Speciﬁcally, we consider an ICS with l1 PLC-sensor channels,
l2 PLC-actuator channels, and an anomaly detector is monitor-
ing the sensor measurements and control commands delivered
via the channels. Without loss of generality, we denote the
anomaly detector as a function:

F(y(t) | Xt−1) =

(cid:26)1
0

if y(t) triggers an alarm
otherwise

where Xt−1 = {. . . , x(t−2), x(t−1)} represents the monitored
time series of the whole system until time t − 1; F(y(t) |
Xt−1) = 1 indicates a bad sensor measurement is detected at
time t.

Furthermore, we consider the attacker has the ability to in-
tercept k1 PLC-sensor channels and k2 PLC-actuator channels,
where k1 ≤ l1 and k2 ≤ l2. Therefore, the attacker has a partial
knowledge of the system dynamics, which can be denoted
as a time-series Xc = {x(1)
, . . .} where each signal
c
x(t)
c = {y(t)
c } is a subset of x(t), consisting of the sensor
measurements and control commands which are delivered via
the k1 + k2 compromised channels. Unlike previous works
which assume the anomaly detection function F is known or

c , u(t)

, x(2)
c

at least partially known to the stealthy attacker [21], [22], we
assume the anomaly detector as a black box to the attacker in
this paper.

The stealthy attacker’s target is to inject malicious sensor
measurements which are deviant with their real values to
a speciﬁc amount, whilst bypassing the black box anomaly
detector F. Speciﬁcally, let ˜y(t)
c denote the injected malicious
sensor measures at time t, y(t)
denote their corresponding
c
real values, we deﬁne a set of attack goals G for the stealthy
attacker. Formally, each attack goal g ∈ G is deﬁned as a target
function:

˜y(t)
g

g

vg where g ∈ {>, <, ≤, ≥, =} ∧ ˜y(t)

g ∈ ˜y(t)
c ,

and vg is the target compromising value set by the attacker. As
an illustration, the target function ˜y(t)
g − 1 denotes an
attack goal to fool the PLC with a fake sensor measurement
with more than 1 unit smaller than its real value y(t)
g . Clearly,
such stealthy attacks are very dangerous, as they can poten-
tially sabotage the ICS by implicitly putting them in a critical
condition.

g < y(t)

V. DEEP LEARNING FRAMEWORK FOR CONDUCTING
STEALTHY ATTACKS

In this section, we present the methodology to automati-
cally conduct stealthy attacks from the attackers’ perspective.
Speciﬁcally, the conducting of stealthy attacks consists of two
phases: the reconnaissance phase and the attacking phase. In
the reconnaissance phase, a deep learning model for generating
stealthy attacks is initialized and trained in real-time by re-
connoitering the compromised channels without launching any
attacks. In the attacking phase, the malicious sensor measure-
ments generated by the trained model are injected to replace
the real measurements. Let the attacker conduct stealthy attacks
by injecting a malicious program, then we outline two key parts
of our framework for implementing stealthy attacks: a powerful
model for generating stealthy attacks, and an effective method
to train the model in real time. Therefore, in the remaining
part of this section, we propose a GAN for generating stealthy
attacks as well as a real-time learning method to train the
stealthy attack GAN.

A. Stealthy Attack GAN

The stealthy attack GAN is composed by two deep learning
models: a generator model for creating malicious sensor mea-
surements and a discriminator model as a substitute anomaly
detector to provide information for training the generator
model.

1) Malicious Sensor Measurement Generator: The objec-
tive of the generator model is to generate malicious sensor
measurements which can achieve the predeﬁned attack goals
whilst bypassing the black box anomaly detector. Since the
attacker does not have the full information of the physical
process, the best strategy for the attacker is then to maximize
the information he/she can utilize, which is the time series sig-
nals in the compromised channels, Xc, to generate malicious
sensor measurements.

Concretely, to utilize the information from the compro-

mised channels, we deﬁne a sliding window:

c = {x(t−l)
St

c

, x(t−l+1)

c

, . . . x(t−1)
c

}

c which only differs from St

which contains all the time series signals obtained from the
compromised channels from time t − l to t − 1, where l > 0 is
the length of the sliding window. Moreover, we also maintain
another sliding windows ˜St
c such
that the previously generated malicious sensor measurements
are injected, and their real values are replaced. Our generator
will generate the next malicious sensor measurements ˜y(t)
at
c
time t based on both St
c captures the real
system dynamics as well as providing the information relevant
for achieving attack goals, ˜St
c provides the related context for
bypassing the black box anomaly detector with the considera-
tion of the previously generated malicious measurements.

c. Intuitively, St

c and ˜St

We approach malicious sensor measurement generation as a
sequence learning problem, hence, we propose an LSTM-FNN
as illustrated in Figure 4 to model our generator. Speciﬁcally,
the two LSTMs read in the signals in the sliding window
c and ˜St
St
c separately, learn their temporal features, and then
respectively encode them to hidden vectors h(t−1) and ˜h(t−1),
which provides the context for the generation of malicious
sensor measurements. Then, the FNN will be used to learn
their high dimensional features h(t), and then output
the
malicious sensor measurements ˜y(t)
c . Concretely, the model
can be represented by the following equations:

c

c

h(t−l) = f (x(t−l)
h(t−i) = f (x(t−i)
˜h(t−l) = f (˜x(t−l)
˜h(t−i) = f (˜x(t−i)

)
, h(t−i−1))
)
, ˜h(t−i−1))
c
h(t) = Wh1 h(t−1) + Wh2
= W h(t) + b
˜y(t)
c

c

1 ≤ i ≤ l − 1

1 ≤ i ≤ l − 1

˜h(t−1) + bh

where the ﬁrst four equations are used to iteratively encode St
c
c to hidden vectors h(t−1) and ˜h(t−1) in which f is a
and ˜St
complex function as a short form of Equations 1 to 5; Wh1 ,
Wh2 and bh are the weight matrices and the bias vector to
further encode the hidden vectors h(t−1) and ˜h(t−1) for higher
dimensional feature representation; W and b are the weight
matrix and the bias vector respectively to decode h(t) to the
output malicious sensor measurements ˜y(t)
c . For convenience,
we represent the generator model as an overall function G:

c = G(St
˜y(t)

c, ˜St

c; θG)

(7)

c, ˜St
where θG denote the parameters of the model, St
are the inputs and output of the model, respectively.

c and ˜y(t)

c

Then, let T be all the moments for generating malicious
sensor measurements, |T | be the size of T , to make the gen-
erated malicious measurements bypass the black box anomaly
detector as well as achieving the attacker’s goals is equivalent
to optimize the generator model as follows:

arg min
θG

1
|T |

subject to

(cid:88)

t∈T
˜y(t)
g

6

F(G(St

c, ˜St

c; θG) | Xt−1)

g vg ∀g ∈ G, t ∈ T

(8)

(9)

activation function will be used to transform the output η to
a probability p ∈ (0, 1) such that p = 1
1+e−η . However, since
here we will use the training method inspired by WGAN,
the sigmoid activation function is not used in our substitute
anomaly detector.

With the discriminator model, the whole procedure for

computing η can be outlined by the following equations:

h(t−l) = f (ˆx(t−l)
h(t−i) = f (ˆx(t−i)

)
, h(t−i−1))
c
h(t) = Whh(t−1) + Wy ˆy(t)

c

c + b

1 ≤ i ≤ l − 1

Fig. 4: The generator model

η = α h(t) + β

where the ﬁrst two equations iteratively encode ˆSt
c to a hidden
vector h(t−1) as similar with the generator model; the third
equation encodes h(t−1) and ˆy(t)
to a hidden vector h(t) in
c
which Wh and Wy are their weight matrices, b is a bias vector;
the last equation decode h(t) to the output singular η, in which
α is the weight vector, β is a bias singular.

Again,

for convenience, we represent

the substitute

Fig. 5: The substitute anomaly detector model

From above, we can clearly see that in order to optimize the
generator model, ﬁrstly, we have to use a substitute anomaly
detection model to approximate the black box anomaly detec-
tion function F to provide information for the training of the
generator.

2) Substitute Anomaly Detector: In order to provide in-
formation to optimize the generator of malicious sensor mea-
surements, here we propose another neural network model to
approximate the black box anomaly detector. Again, without
the ability to access the entire time series Xt−1, our strategy
for deﬁning the substitute anomaly detector is to utilize a
sliding window ˆSt
c can either be St
c depending on
whether malicious sensor measurements are injected in the
previous time steps within the sliding window) to classify
whether the sensor measurements at time t are malicious or
not.

c or ˜St

c (ˆSt

Concretely, we employ an LSTM-FNN discriminator
model whose architecture is illustrated in Figure 5 to model
the substitute anomaly detector. The model consists of two
parts: an LSTM which takes the sliding window ˆSt
c as input,
learns its temporal features, and then encodes them to a hidden
vector h(t−1); a FNN which takes h(t−1) and the sensor
measurements ˆy(t)
can either be malicious
c
c or real measurements y(t)
measurements ˜y(t)
c ), encodes them
to a hidden vector h(t) for capturing nonlinear features, and
then outputs a singular η ∈ R for classiﬁcation. With a larger
value of η, ˆy(t)
is more likely to be malicious. Note that
c
in general, for a binary classiﬁcation problem the sigmoid

as input (ˆy(t)
c

anomaly detector as a function D:
c , ˆSt

c; θD)

η = D(ˆy(t)
where θD is the parameters of the detector; ˆy(t)
c are
c
the inputs; η ∈ R is the output. Then, the learning goal of D is
to output small values for real sensor measurements and large
values for malicious sensor measurements in order to classify
them. Hence, the optimization problem can be formulated as
follows:

and ˆSt

arg min
θD

1
|T1|

(cid:88)

t∈T1

D(cid:0)y(t)

c , St

c; θD

(cid:1) −

1
|T2|

(cid:88)

t∈T2

D(cid:0)˜y(t)

c , ˜St

c; θD

(cid:1)

(10)

where T1 and T2 denote the moments for sampling real mea-
surements and generated malicious measurements for training
D, respectively; ˜y(t)
represent a generated malicious
c
measurement sample and a real measurement sample, respec-
tively.

and y(t)
c

3) The GAN: With the substitute anomaly detector, then the
optimization problem of the generator is equivalent to gener-
ating malicious sensor measurements which let the substitute
anomaly detector output as smaller value as possible whilst
achieving the attack goals. Speciﬁcally, we can replace the
black box function F in Equation 8 by the function D, then
the optimization problem can be reformulated as follows:

arg min
θG

1
|T |

subject to

(cid:88)

t∈T
˜y(t)
g

D(cid:0)G(St

c, ˜St

c; θG), ˜St

c; θD

(cid:1)

g vg ∀g ∈ G, t ∈ T

where we assume θD is ﬁxed for the time being. Note that
the above formulation requires that all the generated malicious
sensor measurements can achieve the attack goals, which is,
however, generally not feasible in practice (at some time
points, if the attack goals are too ambitious, it is impossible
to generate such measurements both bypassing the anomaly
detector and achieving attack goals). As a result, we relax
the optimization problem to allow the attack goals to fail in
some time points, but we pay a cost from each failed cases.

7

Fig. 6: The architecture of the stealthy attack GAN

To implement this, we introduce slack variables ξ(t)
Speciﬁcally, a non-zero value for ξ(t)
measurement ˜y(t)
g
proportional to the value of ξ(t)
g .

g ∈ R≥0.
g allows a generated sensor
to not satisfy an attack goal g at a cost

With slack variables, the formulation of the optimization

problem becomes:

arg min
θG

1
|T |

(cid:88)

[D(cid:0)G(St

c, ˜St

c; θG), ˜St

c; θD

(cid:88)

(cid:1) +

λgξ(t)
g ]

t∈T

g∈G

(11)
(12)

subject to

˜y(t)
g

g vg ± ξ(t)
g

∀g ∈ G, t ∈ T

where the second equation means we achieve attacks goals by
either adding or subtracting slack variables ξ(t)
g ; λg ∈ R>0
in the ﬁrst equation is a hyperparameter which controls the
trade-off between the probability of bypassing the substitute
anomaly detector and the distance to achieve the attack goal:
as λg becomes larger, the generator model is more willing to
generate sensor measurements to achieve attack goals; when
λg is small,
is more likely to generate sensor
measurements which can bypass the anomaly detector, but
the attack goals are not strictly satisﬁed. In our case, λg is
always set to a small value as we should always prioritize the
generator’s ability to bypass the anomaly detector.

the model

More importantly, we can always ﬁnd such slack variables
to satisfy the constraints in Equation 12, and the value of slack
variables can be obtained by the following equation:

ξ(t)
g =






max(0, vg − ˜y(t)
g )
|vg − ˜y(t)
g |
max(0, ˜y(t)

g − vg)

if g ∈ {>, ≥}
if g ∈ {=}
if g ∈ {<, ≤}

∀g ∈ G, t ∈ T

Replacing the above equation into Equation 11, we can ﬁnally
remove the constraints in Equation 12, then the constrained
optimization problem for the generator is converted to an
unconstrained optimization problem over θG.

With the completion of the deﬁnition of the generator and
the substitute anomaly detector, we illustrate the architecture
of the stealthy attack GAN in Figure 6, in which dashed arrows
indicate optional data ﬂow.

B. Real-time Learning Method

Here we present the learning method to train the stealthy
attack GAN. Speciﬁcally, the basic principle of our learning
method follows the training principle of WGAN, which is
to train the generator and the substitute anomaly detector

iteratively to play an adversarial game until the generated mali-
cious sensor measurements cannot be distinguished from real
measurements by the substitute anomaly detector. However,
since it is generally not feasible for the attacker to collect the
intercepted time series signals in a repository and then train the
stealthy attack GAN in an ofﬂine mode, we propose a real-time
learning method to train the stealthy attack GAN. The whole
procedure of the learning method is illustrated in Algorithm 2.

Algorithm 2 The real-time learning method of the stealthy
attack GAN
Require: the learning rate α = 0.00005, the clipping param-
eter c = 0.01, the number of time steps for training the
substitute anomaly detector per time step for training the
generator n = 5.

Require: a sliding window St

and another sliding window ˜St
sensor measurements.

c with real sensor measurements,
c with generated malicious

Require: the length of sliding window l, the total time steps
for learning N , the trade-off hyperparameter λg, ∀g ∈ G,
the probability υ for reseting ˜St

Require: the initial parameters of the substitute detector θ0
D,
the initial

the initial parameters of the generator θ0
G,
sliding windows ˜Sl
c = Sl
c

c with St
c

1: for t = l + 1, l + 2, . . . , l + N do
2:

Generate a random number γ uniformly distributed in
(0, 1)
if γ < υ then
Set ˜St
c = St
c

end if
if t mod (n + 1) (cid:54)= 0 then

Generate malicious measurements:
c = G(St
˜y(t)

c, ˜St

c; θG)

c , St

c; θD) − D(˜y(t)
δθD ← ∇θD [D(y(t)
θD ← θD + α · RM SP rop(θD, δθD )
θD ← clip(θD, −c, c)
c and ˜St
Reset St

c

c , ˜St

c; θD)]

end if
if t mod (n + 1) = 0 then

ﬁx θD
δθG ← −∇θG [D(cid:0)G(St

c, ˜St

c; θG), ˜St

c; θD

(cid:1)+(cid:80)

g∈G λgξ(t)
g ]

3:
4:
5:
6:
7:

8:
9:
10:
11:
12:
13:
14:
15:

θG ← θG + α · RM SP rop(θG, δθG )
Reset St

c and ˜St

c

16:
17:
end if
18:
19: end for

c and ˜St

Speciﬁcally, our learning method only requires to maintain
two sliding windows St
c for the time series signals in
the compromised channels with the real sensor measurements
and the corresponding generated malicious sensor measure-
ments. At each time step, either the substitute anomaly detector
is trained to minimize the objective function in Equation 10 by
using the gradient at the current sample which contains the real
as well as the generated malicious measurements for the time
step (Steps 7 to 9), or the generator is trained to minimize the
objective function in Equation 11 by the gradient at the current
sample in which the two sliding windows are used to generate

8

malicious measurements at the time step to fool the substitute
anomaly detector as well as achieving the attack goals (Steps
14 to 16). At each time step, the two sliding windows are
reseted such that new signals at time t will be appended, and
the expired signals at time t − l will be removed (Steps 11
and 17). Moreover, we also periodically reset ˜St
c at
a rate speciﬁed by υ to start a new cycle for continuously
injecting malicious measurements (Steps 2 to 5). Note that
reseting ˜St
c periodically is rather important since this
gives the chance for the stealthy attack GAN to properly learn
how to initiate the attacks.

c = St

c = St

After the training of the stealthy attack GAN has been ﬁn-
ished, the attacker can generate malicious sensor measurements
for each time step at the attacking phase using Equation 7, and
then inject them to the corresponding compromised channels to
replace the real measurements for achieving their attack goals.

VI. GAS PIPELINE CASE STUDY

In this section, we show a case study in which we conduct
stealthy attacks in a laboratory-scale gas pipeline system using
our framework. Speciﬁcally, the system in our case study con-
sists of a small airtight pipeline connected to a pressure meter,
a pump, and a solenoid-controlled relief valve. A SCADA
system is deployed to control the air pressure in the pipeline,
which contains a PLC, a sensor for pressure measurement,
and several actuators. Our attack goal is to constantly fool the
PLC with a malicious pressure measurement which is smaller
than its real value with different scales, which can potentially
lead to the explosion of the gas pipeline. All the experiments
in the section are done by exploiting a public dataset [39]
which records the network trafﬁc data log captured from the
gas pipeline SCADA system.

A. Gas Pipeline Dataset

is listed in Table I. Speciﬁcally,

The gas pipeline dataset for training and testing our stealthy
attack GAN consists of the sensor measurements and control
commands extracted every two seconds from normal network
packages in a gas pipeline network trafﬁc data log originally
described in [39]. In total, 68,803 time series signals are
collected. The detailed description of the extracted features
for each signal
the gas
pipeline includes a system control mode with 3 states; off,
manual control, or automatic control. In off mode, the pump
is forced to the off state to allow it to cool down. In manual
mode, a HMI can be used to manually change the pump state
and control the relief valve state by opening or closing the
solenoid. In automatic mode, the control scheme determines
which mechanism is used to regulate the pressure set point:
either by turning a pump on or off or by opening and closing a
relief valve using a solenoid. Moreover, a proportional integral
derivative (PID) controller is used to control the pump or
solenoid depending upon the control scheme chosen. Six PID
control parameters can be set, which are pressure set point,
gain, reset rate, rate, dead band, and cycle time.

B. Experiment Setup

In our experiments, we split the time-series dataset into
two slices. The ﬁrst slice which contains 4/5 of the data is
used for the reconnaissance phase to train our stealthy attack

Feature
setpoint
gain
reset rate
deadband
cycle time
rate
system mode
control scheme

pump

solenoid

pressure
measurement

Description
The pressure set point
PID gain
PID reset rate
PID dead band
PID cycle time
PID rate
automatic (2), manual (1) or off (0)
Either pump (0) or solenoid (1)
Pump control – open (1) or off (0)
only for manual mode

Valve control – open (1) or closed (0)
only for manual mode.

Pressure measurement

TABLE I: Extracted sensor measurements (in bold text) and
control commands from the gas pipeline dataset [39]

g = max(y(t)
˜y(t)

g − 4, 0)

Attack Goal
g = max(y(t)
˜y(t)

g − 8, 0)

Attacker’s
Abilities

PLC-Sensor
channel
Compromised
All channels
Compromised

Attack Scenario 1

Attack Scenario 2

Attack Scenario 3

Attack Scenario 4

TABLE II: Attack Scenarios of the Gas Pipeline Case Study

GAN. The other slice is used for the attacking phase. Since the
gas pipeline dataset is relatively small, we go through the ﬁrst
time-series slice for 50 passes to properly train the stealthy
attack GAN using our real-time learning method.

1) Baseline Anomaly Detector: For the baseline anomaly
detector, three predictive models, which are the AR model, the
LDS model, and the LSTM model as described in Section III-A
are ﬁtted to minimize the mean square error between the
predicted pressure measurements and their real values using
cross validation on the ﬁrst time-series slice. We pick the
model with the best prediction accuracy, which is the LSTM
model (with 10 previous signals are used for the model inputs)
as our baseline predictive model. Both the residual errors and
the CUSUM statistics as described in Section III-B are used
to detect anomalies.

g = max(y(t)

g = max(y(t)

2) Attack Scenarios: The normal air pressure range for
the gas pipeline system is about [0,40]. Let y(t)
be the real
g
pressure measurement at time t, we investigate two attack
goals, which are to inject malicious measurement ˜y(t)
such
g
that ˜y(t)
g − 4, 0) and ˜y(t)
g − 8, 0).
Speciﬁcally, the above attack goals require the malicious mea-
surements being 4 or 8 units smaller than their real values (but
not beyond the normal pressure range). Furthermore, we also
consider two cases, in which the attacker can only compromise
the PLC-sensor channel, and the attacker can compromise all
the PLC-sensor, PLC-actuator channels in the gas pipeline
system. To summarize, we consider four attack scenarios with
different attacker’s goal and ability assumptions as illustrated
in Table II.

3) Feature Processing: We normalize all continuous fea-
tures into range [0, 1] by min-max scaling. Speciﬁcally, let x(t)
i

9

be a continuous feature, e.g., the setpoint, at time t, min(xi)
and max(xi) respectively be the minimal and maximal value
of the feature, we covert x(t)
max(xi)−min(xi) . All the
categorical features (system mode, control scheme, pump,
solenoid), are one-hot encoded, e.g., automatic, manual, off
system modes are encoded to [1,0,0], [0,1,0], [0,0,1], respec-
tively.

i = x(t)

i −min(xi)

4) Model Parameters and Memory Cost: We explicitly set
the number of units in the LSTM layers and the FNN layers of
the stealthy attack GAN to four and two times of the dimension
of the processed signals, respectively. The length of sliding
window l is set to 10, which is equal to the number of previous
signals used in the baseline LSTM predictive model. The trade-
off hyperparameter λg is set to 0.01 as we ﬁnd it achieves
the best balance between bypassing anomaly detectors and
achieving attack goals. The probability υ for reseting ˜St
c with
St
c is also set to 0.01. The memory costs for the stealthy attack
GANs are about 40 kB for the ﬁrst two attack scenarios, 160
kB for the other two attack scenarios implemented using the
Keras library [40].

C. Results and Evaluation

For each attack scenario, we train the corresponding
stealthy attack GAN using our real-time learning method
during the reconnaissance phase, after which we start
to
generate malicious pressure measurement for each time point
in the attacking phase. To evaluate the quality of the malicious
pressure measurements generated by our stealthy attack GAN,
we show the trace of malicious pressure measurements during
the attacking phase in the four attack scenarios compared with
the trace of the real values in Figure 7. As can be seen from
the generated malicious pressure measurements
the ﬁgure,
successfully capture the trend of the real trace. The trace of
malicious measurements in the ﬁrst two scenarios exhibit a
minor ﬂuctuation behavior as only the PLC-sensor channel can
be compromised, thus the generated malicious measurements
are more noisy than the counterpart in the other two scenarios.

Furthermore, we also illustrate the deviation between the
generated malicious pressure measurements and their real
values at each time point in the attacking phase compared with
the target deviation as speciﬁed by the attack goals in Figure 8.
We can see that at most time points, the deviation is very
close with the target deviation, which means that the attack
goals are well achieved at these time points. We are aware
that there are a few speciﬁc time points at which the deviations
are rather far away with the target deviation. They are mostly
caused by the noisy behaviour of air pressure in the gas
pipeline system induced by human operations from HMI. The
pressure measurements at these time points are hard to capture,
and thus can be seen as outliers for the deployed anomaly
detector (note that in reality, human operations are much less
often than here in the gas pipeline dataset). As a result, our
stealthy attack GAN has to sacriﬁce the attack goal in order to
prioritize capturing the behaviour of the anomaly detector so
as to bypass it at these time points. Nevertheless, we can also
observe that the stealthy attack GAN can adjust the deviation to
the targeted value quickly after these outliers. Furthermore, to
quantitatively evaluate the quality of the generated malicious
measurements, we consider the attack goal is achieved at a

real − dt

real and dt

target| ≤ δ, where dt

time point t if |dt
target
respectively denote the real deviation and the target deviation
between the malicious pressure measurement and its real value
at time point t, δ is a threshold value. In Figure 9, we plot the
ratio of time points at which the attack goal is achieved with
different values of δ in each attacking scenario. We can observe
that at around 50% of time points, the deviation caused by our
attacks exactly matches the target deviation (δ = 0) if both the
PLC-sensor and the PLC-actuator channels are compromised.
The ﬁgure is slightly lower (about 40%) if only the PLC-
sensor channel is compromised due to the ﬂuctuation behavior
as illustrated in Figure 7. And we can see that the ratios in
all the attack scenarios become close when we increase δ by
which the standard for achieving attack goals is relaxed. This
means the generated malicious pressure measurements by our
stealthy attack GAN are rather robust in achieving attack goals
even with different scales of target deviation and different
knowledge of the system dynamics. Moreover, we can observe
that even with δ = 1, attack goals are achieved more than 80%
time points in all attacking scenarios, this ﬁgure increases to
more than 90% when δ = 2, which indicates the high quality
of malicious pressure measurements generated by our stealthy
attack GAN for achieving the targeted deviations.

To demonstrate the ability of the generated malicious pres-
sure measurements to bypass the black box anomaly detector.
We use both the residual error and the CUSUM statistic-based
methods to detect the injected malicious pressure measure-
ments in all the four attack scenarios. Speciﬁcally, we tune the
value of τ for the residual error-based method to get different
values of expected false alarm rate based on the training
dataset. The value of τ for the CUSUM statistic-based method
is also tunned to get different values of expected time between
false alarms. In Figure 10 and 11, we plot the values of alarm
rates with different values of expected false alarm rate and
the value of mean time between alarms (measured in minutes)
with different values of expected time between false alarms
in the four attack scenarios, respectively, comparing with the
corresponding values for the real pressure measurement trace.
From Figure 10, we can observe that in all cases, the alarm
rates for our injected malicious pressure measurements are
lower than the corresponding value for the measurements in
the real trace. This indicates that it is extremely inefﬁcient
to the expected false alarm rate)
and costly (with respect
to detect our generated stealthy attacks using the residual
error-based detection method. Additionally, we can also see
that with the expected false alarm rate being less than 0.002
(0.002 expected false alarm rate means that it is expected that
2 out of 1000 detections will generate a false alarm), it is
almost impossible to detect our stealthy attacks in all scenarios.
Furthermore, from Figure 11, we also observe that, in all
cases, the mean time between alarms for our injected malicious
pressure measurements are higher than the corresponding value
for the measurements in the real trace. This reﬂects that it is
also very inefﬁcient to use the CUSUM statistic-based method
to detect our stealthy attacks. The reason for all the above
phenomena is that instead of minimizing the mean square
error between the predicted pressure measurements and their
real values like traditional predictive models, our stealthy
attack GAN is conﬁgured to directly learn how to bypass the
black box anomaly detector, thus can continuously generate
malicious sensor measurements with smaller residual errors

10

Fig. 7: The generated malicious pressure measurement trace in the attacking phase compared with their real values. (Scenario 1
to 4 from left to right)

Fig. 8: The deviation between the generated malicious pressure measurements and their real values at each time point in the
attacking phase compared with the target deviation as speciﬁed by the attack goals (Scenario 1 to 4 from left to right)

Fig. 9: The ratio of time points at which the attack goal is
achieved in the attacking phase with different value of δ

Fig. 10: The values of alarm rates in the four attack scenarios
compared with the corresponding value for the real pressure
trace with different values of expected false
measurement
alarm rate

even compared with the real trace, making them extremely
difﬁcult to reveal by both the residual error and the CUSUM
statistic-based detection methods. Lastly, we also observe
from the results for the ﬁrst two attack scenarios that only
compromising the PLC-sensor channel can still generate high-
quality stealthy attacks. We believe this is because there exist
unstable delays to reﬂect the control commands from sensor
measurements, which allows the stealthy attack GAN to learn
the control commands solely from the previous real sensor
measurements in the sliding window St
c, making the generated
malicious sensor measurements more likely to bypass the black
box anomaly detector.

VII. WATER TREATMENT SYSTEM CASE STUDY

In this section, we present another case study in which we
conduct stealthy attacks using our framework onto a Secure
Water Treatment (SWaT) testbed using a simulation approach.
Speciﬁcally, the SWaT testbed [41] is a scaled down water
treatment plant which has a six-stage ﬁltration process to purify
raw water. The general description of each stage is as follows:
In the ﬁrst stage, raw water is taken in and stored in a tank. It is
then passed to the second stage for pretreatment process, where

11

Feature Description

AIT201

AIT202

AIT203

FIT201

P101

Conductivity analyzer
measures NaCl level
pH analyzer
measures HCl level
ORP analyzer
measures NaOCl level
Flow transmitter for dosing pumps
Raw water tank pump state
control water from tank to the second stage

MV201 Motorized valve state

P201
P203
P205

control water ﬂow to the third stage
NaCl dosing pump state
HCl dosing pump state
NaOCl dosing pump state

Fig. 11: The values of mean time between alarms (measured
in minutes) in the four attack scenarios compared with the
corresponding value for the real pressure measurement trace
with different values of expected time between false alarms

the conductivity, pH, and Oxidation Reduction Potential (ORP)
are measured to determine whether chemical dosing is per-
formed to maintain the water quality within acceptable limits.
In the third stage, the Ultra Filtration (UF) system will remove
undesirable materials by using ﬁne ﬁltration membranes. This
is followed by the fourth stage, where the remaining chlorines
are destroyed in the Dechlorinization process using Ultraviolet
lamps. Subsequently, the water is pumped into the Reverse
Osmosis (RO) system to reduce inorganic impurities in the ﬁfth
stage. In the last stage, the clean water from the RO system
is stored and ready for distribution. A SCADA system with
6 PLCs, 24 sensors and 27 actuators is deployed to control
the testbed. Our attacks focus on the second stage, where we
inject malicious pH and ORP measurements which are larger
and smaller than their real values simultaneously to change the
quality of produced water.

A. Dataset and Experiment Setup

The water treatment dataset for training and testing our
stealthy attack GAN consists of 51 sensor measurements and
control commands extracted every second from the SWaT
dataset [16]. In total, 496,800 signals for regular operation are
collected. Since we focus on conducting stealthy attacks in the
second stage of the water treatment process, we only illustrate
the related sensor measurements and control commands for the
second stage in Table III.

Again, we split the time-series dataset into two slices. The
ﬁrst slice for training the stealthy attack GAN contains 4/5
of the data. The other slice is used for the attacking phase.
We also use the LSTM predictive model (as it achieves the
best prediction accuracy) together with the residual error as
well as the CUSUM statistic-based method for our baseline
anomaly detector. For feature processing, we also normalize all
the continuous features in the dataset using min-max scaling.
All categorical features are one-hot encoded. All the model
parameters are also set with the same rule as in the previous
case study.

TABLE III: Extracted sensor measurements (in bold text) and
control commands from the water treatment dataset [16]

g1 , y(t)

Here we set the attack goal to simultaneously send ma-
licious HCl and NaOCl measurements to their corresponding
PLCs whose values are 0.1 (after normalization) higher and
lower than their real values, respectively, but not beyond the
normal range. Speciﬁcally, let y(t)
g2 be the real HCl and
NaOCl level measurements at time t, the attacker requires the
malicious measurements sent to the PLCs as ˜y(t)
g1 +
0.1, 1) and ˜y(t)
g2 −0.1, 0). Furthermore, we consider
two attack scenarios. In the ﬁrst scenario, the attacker can
only compromise the PLC-AIT202, PLC-AIT203 channels.
In the second scenario, the attacker is able to compromise
all the PLC-sensor, PLC-actuator channels in the stage. The
corresponding memory costs of the stealthy attack GANs in
the two scenarios are about 44 kB and 154 kB, respectively.

g2 ≤ max(˜y(t)

g1 ≥ min(y(t)

B. Simulation and Results

We test our framework for conducting stealthy attacks in
this case study using a simulation approach. Speciﬁcally, in
the SWaT testbed, the HCl and NaOCl dosing pumps are
always turned on simultaneously when the raw water tank
pump (P101) is turned on, and switched off when the raw
water tank pump is turned off. In the simulation, we also
require to turn on the HCl dosing pump when the measured
HCl level is higher than 0.99, and the NaOCl dosing pump
is turned on when the measured NaOCl level is lower than
0.01. In the attacking phase, we start to inject malicious HCl
and NaOCl measurements at 1,000 randomly selected time
points. Then, we consider an attack is successful when either
the HCl or the NaOCl dosing pump is turned on unexpectedly
by the injected malicious measurements when the raw water
tank pump is still in the off state and meanwhile the malicious
measurements must bypass the baseline anomaly detector until
then (the quality of water to the third stage is changed if this
happens).

The simulation results are shown in Figure 12 and 13,
the ratio of successful attacks among the
where we plot
1,000 attempts with different values of expected false alarm
rate and expected time between false alarms, respectively.
As can be seen from the ﬁgures, again, we ﬁnd that only
compromising the PLC-sensor channels for injecting malicious

12

Fig. 12: The successful ratio of turning on the HCl or NaOCl
dosing pump by the injected malicious measurements while
bypassing the baseline anomaly detector with different values
of expected false alarm rate

Fig. 13: The successful ratio of turning on the HCl or NaOCl
dosing pump by the injected malicious measurements while
bypassing the baseline anomaly detector with different values
of expected time between false alarms (measured in minutes)

sensor measurements can still have rather high successful
rates of conducting stealthy attacks. From Figure 12, we
can see that the attacks will be successful on almost every
attempt if the expected false alarm rate is tunned to zero
(by setting the threshold τ to the maximal residual error
in the training dataset). The ratios of successful attacks in
both attack scenarios are still rather high (about 40%) even
with 0.01 expected false alarm rate (it is expected that 1
out of 100 detections will generate a false alarm). When
using the CUSUM statistic to detect our attacks, we ﬁnd that
the ratio of successful attacks can be as high as 90% if all
channels are compromised, and about 80% if only the PLC-
AIT202, PLC-AIT203 channels are compromised, given that
the expected time between false alarms is tunned to be one
hour as illustrated in Figure 13. Even setting the expected
time between false alarms to around three minutes, we can
still get around 20% of successful attacks which is still rather
signiﬁcant considering the severe consequence once the attack
succeeded. From the above observations, we can clearly see
that such malicious sensor measurements generated by our
stealthy attack GAN can easily cause serious physical damages
to ICS in the real world.

VIII. CONCLUSION

In this paper we have developed and demonstrated a novel
GAN based stealthy attack framework, requiring a much lower
a-priori operational knowledge of a targeted ICS instance
than has been previously reported for stealthy attacks.This
feature of the framework makes it agnostic to speciﬁc ICS
types, making it applicable to a broad range of industrial
scenarios. Speciﬁcally, we have shown a real-time adversarial
learning method which provides the theoretical foundation
for allowing the attacker to inject a malicious program to
automatically conduct stealthy attacks which can bypass the
deployed anomaly detector that is assumed as a black box to
him. Furthermore, our framework can even allow the attacker
to specify the targeted amount of deviation caused by his

attacks, and let the malicious program automatically decide
the optimal amount of deviation injection at each time point.

The quality of the generated malicious sensor measure-
ments from the deep learning method employed is assessed
quantitatively by using two real-world datasets from two in-
dustrially distinct ICS instances. The results show that the gen-
erated malicious sensor measurements are extremely difﬁcult
to reveal by the current anomaly detection security measures.
This is because our stealthy attack GAN directly learns the
behaviour of the anomaly detector, thus can constantly generate
malicious measurements with a relatively small residual error
with respect to the value predicted by the anomaly detector.
Moreover, we also ﬁnd in the experiments that even with a
partial knowledge of the system dynamics, our framework can
still generate high-quality stealthy attacks that can potentially
cause physical damage to the underlying ICS processes.

To conclude, our work indicates that with recent advances
in deep learning techniques, the widely understood reliability
of existing anomaly detection techniques is overestimated and
that, by inference, the risk of physical harm to ICS from cyber
attacks is higher. In order to counter stealthy attacks generated
by our framework, we suggest that novel security mechanisms
be developed and implemented; for example, by the use of
multiple sensors for a given physical process and conduct
correlation or cross-correlation checking between multiple
sensors. Such broader consideration of methods for mitigating
cyber security risks is of importance to those operating legacy
ICS instances and to the designers of next-generation ICS
systems who are involved with creating ICS architectures that
are secure by design.

ACKNOWLEDGMENT

We thank iTrust, Centre for Research in Cyber Security,
Singapore University of Technology and Design for providing
the SWaT dataset. Cheng Feng and Deeph Chana are supported
by the EPSRC project Security by Design for Interconnected

13

Critical Infrastructures, EP/N020138/1. Tingting Li
is sup-
ported by the EPSRC project RITICS: Trustworthy Industrial
Control Systems, EP/L021013/1.

REFERENCES

[1] O. Givehchi, H. Trsek, and J. Jasperneite, “Cloud computing for
industrial automation systems – a comprehensive overview,” in 18th
IEEE Conference on Emerging Technologies & Factory Automation
(ETFA),.

IEEE, 2013, pp. 1–4.

[2] N. Falliere, L. O. Murchu, and E. Chien, “W32. stuxnet dossier,” White

paper, Symantec Corp., Security Response, vol. 5, 2011.

[3]

ICS-CERT. (2014) Ics-csrt monitor september 2014 - february 2015.
”www.ics-cert.us-cert.gov/monitors/ICS-MM201502”.

[4] ——.

(2015)

Incident

response activity november 2014 - de-
cember 2015. ”https://ics-cert.us-cert.gov/sites/default/ﬁles/Monitors/
ICS-CERT Monitor Nov-Dec2015 S508C.pdf”.

[5] ——.

(2016)

Ics-csrt monitor november 2016 - december 2016.

”https://ics-cert.us-cert.gov/sites/default/ﬁles/Monitors/ICS-CERT
Monitor Nov-Dec2016 S508C.pdf”.

[6] R. Lee, M. Assante, and T. Connway, “ICS cyber-to-physical or process
effects case study paper–german steel mill cyber attack,” Sans ICS, Dec,
2014.

[26]

[7]

ICS-CERT. (2016) Cyber-attack against Ukrainian critical infrastruc-
ture. ”www.ics-cert.us-cert.gov/alerts/IR-ALERT-H-16-056-01”.

[8] R. Mitchell and I.-R. Chen, “A survey of intrusion detection techniques
for cyber-physical systems,” ACM Computing Surveys (CSUR), vol. 46,
no. 4, p. 55, 2014.

[9] B. Zhu and S. Sastry, “Scada-speciﬁc intrusion detection/prevention
systems: a survey and taxonomy,” in Proceedings of the 1st Workshop
on Secure Control Systems (SCS), vol. 11, 2010.

[10]

I. N. Fovino, A. Carcano, T. D. L. Murel, A. Trombetta, and M. Masera,
“Modbus/dnp3 state-based intrusion detection system,” in 2010 24th
IEEE International Conference on Advanced Information Networking
and Applications.

IEEE, 2010, pp. 729–736.

[11] B. Kang, K. McLaughlin, and S. Sezer, “Towards a stateful analysis
framework for smart grid network intrusion detection,” in Proceedings
of the 4rd International Symposium for ICS & SCADA Cyber Security
Research. British Computer Society, 2016, pp. 124–131.

[12] Y. Yang, K. McLaughlin, T. Littler, S. Sezer, B. Pranggono, and
H. Wang, “Intrusion detection system for iec 60870-5-104 based scada
networks,” in Power and Energy Society General Meeting (PES), 2013
IEEE.

IEEE, 2013, pp. 1–5.

[13] P. Nader, P. Honeine, and P. Beauseroy, “lp-norms in one-class classiﬁ-
cation for intrusion detection in scada systems,” IEEE Transactions on
Industrial Informatics, vol. 10, no. 4, pp. 2308–2317, 2014.

[14] S. Parthasarathy and D. Kundur, “Bloom ﬁlter based intrusion detection
for smart grid scada,” in Electrical & Computer Engineering (CCECE),
2012 25th IEEE Canadian Conference on.

IEEE, 2012, pp. 1–6.

[15]

[16]

J. Bigham, D. Gamez, and N. Lu, “Safeguarding scada systems
with anomaly detection,” in International Workshop on Mathematical
Methods, Models, and Architectures for Computer Network Security.
Springer, 2003, pp. 171–182.

J. Goh, S. Adepu, M. Tan, and Z. S. Lee, “Anomaly detection in cyber
physical systems using recurrent neural networks,” in High Assurance
Systems Engineering (HASE), 2017 IEEE 18th International Symposium
on.

IEEE, 2017, pp. 140–145.

[17] C. Feng, T. Li, and D. Chana, “Multi-level anomaly detection in
industrial control systems via package signatures and lstm networks,” in
Dependable Systems and Networks (DSN), 2017 47th Annual IEEE/IFIP
International Conference on.

IEEE, 2017, pp. 261–272.

[18] T. Van Cutsem, M. Ribbens-Pavella, and L. Mili, “Bad data identiﬁ-
cation methods in power system state estimation-a comparative study,”
IEEE transactions on power apparatus and systems, no. 11, pp. 3037–
3049, 1985.

[19] A. Abur and A. G. Exposito, Power system state estimation: theory and

[20] D. Hadˇziosmanovi´c, R. Sommer, E. Zambon, and P. H. Hartel, “Through
the eye of the plc: semantic security monitoring for industrial pro-
cesses,” in Proceedings of the 30th Annual Computer Security Applica-
tions Conference. ACM, 2014, pp. 126–135.

[21] G. Dan and H. Sandberg, “Stealth attacks and protection schemes
for state estimators in power systems,” in Smart Grid Communica-
tions (SmartGridComm), 2010 First IEEE International Conference on.
IEEE, 2010, pp. 214–219.

[22] Y. Liu, P. Ning, and M. K. Reiter, “False data injection attacks
against state estimation in electric power grids,” ACM Transactions on
Information and System Security (TISSEC), vol. 14, no. 1, p. 13, 2011.
[23] A. Teixeira, I. Shames, H. Sandberg, and K. H. Johansson, “Reveal-
ing stealthy attacks in control systems,” in Communication, Control,
and Computing (Allerton), 2012 50th Annual Allerton Conference on.
IEEE, 2012, pp. 1806–1813.

[24] D. I. Urbina, J. A. Giraldo, A. A. Cardenas, N. O. Tippenhauer,
J. Valente, M. Faisal, J. Ruths, R. Candell, and H. Sandberg, “Lim-
iting the impact of stealthy attacks on industrial control systems,” in
Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2016, pp. 1092–1105.

[25] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,” arXiv

preprint arXiv:1701.07875, 2017.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in
Advances in neural information processing systems, 2014, pp. 2672–
2680.

[27] X. Shu, D. Yao, and N. Ramakrishnan, “Unearthing stealthy program
attacks buried in extremely long execution paths,” in Proceedings of
the 22nd ACM SIGSAC Conference on Computer and Communications
Security. ACM, 2015, pp. 401–413.

[28] C. Kruegel, E. Kirda, D. Mutz, W. Robertson, and G. Vigna, “Automat-
ing mimicry attacks using static binary analysis,” in Proceedings of the
14th conference on USENIX Security Symposium-Volume 14. USENIX
Association, 2005, pp. 11–11.

[29] D. Wagner and P. Soto, “Mimicry attacks on host-based intrusion
detection systems,” in Proceedings of the 9th ACM Conference on
Computer and Communications Security. ACM, 2002, pp. 255–264.
[30] H. Xu, W. Du, and S. J. Chapin, “Context sensitive anomaly monitoring
of process control ﬂow to detect mimicry attacks and impossible paths,”
in International Workshop on Recent Advances in Intrusion Detection.
Springer, 2004, pp. 21–38.

[31] H. G. Kayacik, A. N. Zincir-Heywood, M.

I. Heywood, and
S. Burschka, “Generating mimicry attacks using genetic programming:
a benchmarking study,” in Computational Intelligence in Cyber Security,
2009. CICS’09. IEEE Symposium on.

IEEE, 2009, pp. 136–143.

[32] H. Sandberg, A. Teixeira, and K. H. Johansson, “On security indices
for state estimators in power networks,” in First Workshop on Secure
Control Systems (SCS), Stockholm, 2010, 2010.

[33] S. Amin, X. Litrico, S. Sastry, and A. M. Bayen, “Cyber security of
water scada systems – part i: Analysis and experimentation of stealthy
deception attacks,” IEEE Transactions on Control Systems Technology,
vol. 21, no. 5, pp. 1963–1970, 2013.

[34] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[35] Z. C. Lipton, J. Berkowitz, and C. Elkan, “A critical review of
recurrent neural networks for sequence learning,” arXiv preprint
arXiv:1506.00019, 2015.

[36] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Comput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997. [Online]. Available:
http://dx.doi.org/10.1162/neco.1997.9.8.1735

[37] T. Tieleman and G. Hinton. (2014) RMSprop Gradient Optimiza-
tion. ”http://www.cs.toronto.edu/∼tijmen/csc321/slides/lecture slides
lec6.pdf”.

[38] W. H. Woodall and B. M. Adams, “The statistical design of cusum
charts,” Quality Engineering, vol. 5, no. 4, pp. 559–570, 1993.
[39] T. H. Morris, Z. Thornton, and I. Turnipseed, “Industrial control system
simulation and data logging for intrusion detection system research,” in
7th Annual Southeastern Cyber Security Summit, 2015.

implementation. CRC press, 2004.

[40] F. Chollet et al., “Keras,” https://github.com/fchollet/keras, 2015.

14

[41]

J. Goh, S. Adepu, K. N. Junejo, and A. Mathur, “A dataset to support
research in the design of secure water treatment systems,” in Inter-
national Conference on Critical Information Infrastructures Security.
Springer, 2016.

15

