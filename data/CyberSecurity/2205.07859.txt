BTP report

Dvij Kalaria
18CS10018

Under the supervision of
Prof. Partha Pratim Chakrabarti & Prof. Aritra
Hazra

DEPARTMENT OF COMPUTER SCIENCE & ENGINEERING

INDIAN INSTITUTE OF TECHNOLOGY KHARAGPUR

2
2
0
2

y
a
M
9

]

G
L
.
s
c
[

1
v
9
5
8
7
0
.
5
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Contents

1 Introduction

1.1 Adversaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Adversarial attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Adversarial Attack Models and Methods . . . . . . . . . . . . . . . . . . .
1.3.1 Random Perturbation (RANDOM) . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
1.3.2 Fast Gradient Sign Method (FGSM)
1.3.3 Projected Gradient Descent (PGD) . . . . . . . . . . . . . . . . . .
1.3.4 Carlini-Wagner (CW) Method . . . . . . . . . . . . . . . . . . . . .
1.3.5 DeepFool method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.4 Detection of adversaries

2 Early Statistical Methods

2.1 Using PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Using Softmax Distribution . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Using Input Reconstructions From logits . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Limitations

1
1
1
2
2
2
3
3
3
3

4
4
5
6
7

3 Network Based Methods

3.1 Adversary detector network . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Additional Class Node . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Limitations

8
8
9
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

4 Distribution Based ethods

11
4.1 Kernel Density and Bayesian Uncertainty Estimates . . . . . . . . . . . . . 11
4.2 Maximum Mean Discrepancy . . . . . . . . . . . . . . . . . . . . . . . . . 12
4.3 Using Distribution from Deep Layers . . . . . . . . . . . . . . . . . . . . . 13
4.4 Feature Squeezing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

5 Some special methods

16
5.1 Reverse Cross Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
5.2 Adversarial Examples Detection in Features Distance Spaces . . . . . . . . 17

i

6 Use of conditional Variational AutoEncoder (CVAE)

19
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.2 Proposed Framework Leveraging CVAE . . . . . . . . . . . . . . . . . . . . 20
. . . . . . . . . . . 20
6.2.1 Conditional Variational AutoEncoders (CVAE)
6.2.2 Training CVAE Models . . . . . . . . . . . . . . . . . . . . . . . . . 21
6.2.3 Determining Reconstruction Errors . . . . . . . . . . . . . . . . . . 22
6.2.4 Obtaining p-value . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
6.3 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
6.3.1 Datasets and Models . . . . . . . . . . . . . . . . . . . . . . . . . . 25
6.3.2 Performance over Grey-box attacks . . . . . . . . . . . . . . . . . . 25
. . . . . . . . . . . . . . . . . 28
6.3.3 Performance over White-box attacks
6.4 Comparison with State-of-the-Art using Generative Networks . . . . . . . . 30
6.4.1 Use of simple AutoEncoder (AE)
. . . . . . . . . . . . . . . . . . . 30
6.4.2 Use of Variational AutoEncoder (VAE) . . . . . . . . . . . . . . . . 31
6.4.3 Use of Generative Adversarial Network (GAN) . . . . . . . . . . . . 31

7 Use of Variational AutoEncoder (VAE) for Puriﬁcation

33
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
. . . . . . . . . . . . . . . . . 33
7.2 Training of Variational AutoEncoder (VAE)
7.3 Puriﬁcation of Input Images . . . . . . . . . . . . . . . . . . . . . . . . . . 33
34
7.4 Results on MNIST and Comparison With Other Works on MNIST dataset
. . . . . . . . . . . . . . . . . . . . . . . 34
7.5 Variations for CIFAR-10 Dataset
7.5.1 Fixed no of iterations . . . . . . . . . . . . . . . . . . . . . . . . . . 34
7.5.2 Fixed no of iterations with using ADAM optimizer for update . . . 35
7.5.3 Variable learning rate based on the current reconstruction error
. . 35
Set target distribution for reconstruction error . . . . . . . . . . . . 36
7.5.4
Set target distribution for reconstruction error with modiﬁed update
7.5.5
rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
7.5.6 Add random noise at each update step . . . . . . . . . . . . . . . . 37
7.5.7 Add random transformation at each update step . . . . . . . . . . . 37
7.6 Variations for ImageNet Dataset . . . . . . . . . . . . . . . . . . . . . . . . 38
7.6.1 Attack method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
7.6.2 Noise remover network . . . . . . . . . . . . . . . . . . . . . . . . . 39
7.6.3 No of iterations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
. . . . . . . . . . . . . . . . . . . . . . . . . 40
7.6.4 Puriﬁcation variations
7.7 Possible Counter Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
7.7.1 Counter attack A . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
7.7.2 Counter attack B . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

8 Future Work

43

ii

Future Work

References

43

47

iii

Chapter 1

Introduction

1.1 Adversaries

A deep learning model is trained on certain training examples for various tasks such
as classiﬁcation, regression etc. By training, weights are adjusted such that the model
performs the task well not only on training examples judged by a certain metric but has
an excellent ability to generalize on other unseen examples as well which are typically
called the test data. Despite the huge success of machine learning models on a wide
range of tasks, security has received a lot less attention along the years. Robustness along
various potential cyber attacks also should be a metric for the accuracy of the machine
learning models. These cyber attacks can potentially lead to a variety of negative impacts
in the real world sensitive applications for which machine learning is used such as medical
and transportation systems. Hence, it is a necessity to secure the system from such
attacks. Int this report, I focus on a class of these cyber attacks called the adversarial
attacks in which the original input sample is modiﬁed by small perturbations such that
they still look visually the same to human beings but the machine learning models are
fooled by such inputs. These modiﬁed input examples are called adversarial examples or
adversaries. This has become an active research topic since the ﬁrst adversarial example
was suspected by Amodei et al. [2]

1.2 Adversarial attacks

These are a special category of potential cyber attacks that can be made in which the
attacker modiﬁes the input examples such that they visually look the same ad appealing to
the human being but the machine learning model gets fooled into mis-classifying it or mis-
predicting the output value by a higher margin that too with very high probability. These
attacks can be divided into white box and black box attacks. In the white box attacks, the
attacker has the access to the machine learning model and its weights. In the following
case, the attacks are mostly derived from using the model weights for back propagation

1

and getting a small perturbation along the gradient direction to get the desired change
in output. The second class of attacks are called black box attacks in which the attacker
doesn’t have access to the model but only the sample inputs and outputs to the model.
These attacks typically use some characteristic properties to modify input that drives
it considerably away from the input distribution or use the input and output data to
construct the approximate parameters of the model and then devise a white box attack
for it.

1.3 Adversarial Attack Models and Methods

For a test example X, an attacking method tries to ﬁnd a perturbation, ∆X such that
|∆X|k ≤ (cid:15)atk where (cid:15)atk is the perturbation threshold and k is the appropriate order,
generally selected as 2 or ∞ so that the newly formed perturbed image, Xadv = X + ∆X.
Here, each pixel in the image is represented by the (cid:104)R, G, B(cid:105) tuple, where R, G, B ∈ [0, 1].
In this paper, we consider only white-box attacks, i.e. the attack methods which have
access to the weights of the target classiﬁer model. However, we believe that our method
should work much better for black-box attacks as they need more perturbation to attack
and hence should be more easily detected by our framework. For generating the attacks,
we use the library by [33].

1.3.1 Random Perturbation (RANDOM)

Random perturbations are simply unbiased random values added to each pixel ranging in
between −(cid:15)atk to (cid:15)atk. Formally, the randomly perturbed image is given by,

Xrand = X + U(−(cid:15)atk, (cid:15)atk)

(1.1)

where, U(a, b) denote a continuous uniform distribution in the range [a, b].

1.3.2 Fast Gradient Sign Method (FGSM)

Earlier works by [16] introduced the generation of malicious biased perturbations at each
pixel of the input image in the direction of the loss gradient ∆XL(X, y), where L(X, y)
is the loss function with which the target classiﬁer model was trained. Formally, the
adversarial examples with with l∞ norm for (cid:15)atk are computed by,

Xadv = X + (cid:15)atk.sign(∆XL(X, y))

FGSM perturbations with l2 norm on attack bound are calculated as,

Xadv = X + (cid:15)atk.

∆XL(X, y)
|∆XL(X, y)|2

2

(1.2)

(1.3)

1.3.3 Projected Gradient Descent (PGD)

Earlier works by [28] propose a simple variant of the FGSM method by applying it mul-
tiple times with a rather smaller step size than (cid:15)atk. However, as we need the overall
perturbation after all the iterations to be within (cid:15)atk-ball of X, we clip the modiﬁed X at
each step within the (cid:15)atk ball with l∞ norm.

Xadv,0 = X,

Xadv,n+1 = Clip(cid:15)atk
X

(cid:110)

Xadv,n + α.sign(∆XL(Xadv,n, y))

(cid:111)

(1.4a)

(1.4b)

Given α, we take the no of iterations, n to be (cid:98) 2(cid:15)atk
been named as Basic Iterative Method (BIM) in some works.

α + 2(cid:99). This attacking method has also

1.3.4 Carlini-Wagner (CW) Method

[6] proposed a more sophisticated way of generating adversarial examples by solving an
optimization objective as shown in Equation 1.5. Value of c is chosen by an eﬃcient
binary search. We use the same parameters as set in [33] to make the attack.

Xadv = Clip(cid:15)atk
X

(cid:110)

min
(cid:15)

(cid:107)(cid:15)(cid:107)2 + c.f (x + (cid:15))

(cid:111)

(1.5)

1.3.5 DeepFool method

DeepFool [37] is an even more sophisticated and eﬃcient way of generating adversaries.
It works by making the perturbation iteratively towards the decision boundary so as to
achieve the adversary with minimum perturbation. We use the default parameters set in
[33] to make the attack.

1.4 Detection of adversaries

There has been an active research in the direction of adversaries and the ways to
avoid them. There are many statistical, as well as machine learning based algorithms
which have been put forth for the systematic detection as well as ﬁxing them so that they
get classiﬁed into the right class or give the desired output. However, mostly the literature
and study has been into the adversaries in classiﬁcation neural networks. This report will
speciﬁcally focus on the detection of the adversaries for the classiﬁcation neural networks.
In other words, the ocus of this report is to brieﬂy discuss and review the state of the art
methods which are used to classify the adversaries from a pool of input examples

3

Chapter 2

Early Statistical Methods

Some of the early works in this domain which mark the starting of research are from
Hendrys [21] and Gimpel [20]. They investigated 3 statistical methods which can be
used to characterize adversaries from non-adversaries and hence based on these statistics
identify adversaries based on threshold or machine learning techniques

2.1 Using PCA

The ﬁrst method proposed was to analyze the principal components using the SGD
decomposition of the co-variance matrix to get the matrix U which can be used for ﬁnding
the PCA components given the input. This modiﬁed input is called the PCA whitened
input. The author proposes that for the adversaries, the magnitude of the larger PCA
coeﬃcients is very less as compared to the non adversaries. Hence by putting a threshold
or some other statistical adaptive measure on the variance of the last PCA coeﬃcients, the
adversaries can be identiﬁed. This was tested for the MNIST dataset. The PCA compo-
nent value variations can be seen from ﬁg 1. The threshold based technique demonstrated
the identiﬁcation of FGSM [17], BIM [29] adversaries on MNIST, Tiny-ImageNet datasets
when the attacker is not aware of the defense strategy

An extension to this work [32] used the PCA components of deep layers instead of the
input. In this work, a deep learning classiﬁer was used for each layer of the architecture
with the inputs as the PCA coeﬃcients, max and min values and 25%, 50% and 75%
medians of the PCA coeﬃcient values. For combining the result from each layer, they
used a cascaded boosting. With this setup, for a sample to be identiﬁed as a non adversary,
it has to be identiﬁed as non adversary at each stage. The algorithm for the following is
given in ﬁg 2.

4

Figure 2.1: Magnitudes of PCA components for normal and adversarial examples

Figure 2.2: Percentage of adversaries correctly identiﬁed using PCA method

2.2 Using Softmax Distribution

Further, Hendrycks and Gimpel [21] [20] proposed that the distribution after soft-
max on logits are diﬀerent between clean and adversarial inputs, and thus can be an-
alyzed to perform adversarial detection. The analysis measure can be done from the
Kullback-Leibler divergence between uniform distribution and the softmax distribution.
Then threshold-based detection or similar statistical diﬀerentiation can be made on it.
It was found that the softmax distribution of normal examples are usually further away
from uniform distribution compared to adversarial examples. This can be understood as
a model tends to predict an input with a high conﬁdence tend to have a uniform distri-
bution as they essentially have equal no of training samples and thus do not diﬀerentiate
on any dimension, while for adversaries generated by attack methods, do not care about
the output distribution hence there is a high chance that they form a non uniform biased
distribution

This method seems to only applicable to speciﬁc class of attacks that stop as soon as an

5

Figure 2.3: Algorithm for training process of a cascade classiﬁer

input becomes adversarial such as the JSMA [40] , which may be predicted by the model
with a low conﬁdence score. Furthermore, evaluation on ImageNet models is needed which
is currently incomplete since the softmax probability on ImageNet dataset usually is less
conﬁdent due to the large number of classes. However, it is also not clear whether or not
this strategy would work against adversarial attacks that also target speciﬁc conﬁdence
score such as the C&W attack [5]

Figure 2.4: Constraints degrade the pathology of fooling images. Value σ is standard
deviation

2.3 Using Input Reconstructions From logits

Finally, Hendrycks and Gimpel proposed that by analyzing the input reconstructions ob-
tained by adding an auxiliary decoder [233] to the classiﬁer model that takes the logits
as an input, adversarial examples can be detected . The decoder and classiﬁer are jointly
trained only on clean examples. The detection can be done by creating a detector net-
work which takes as input the reconstruction, logits, and conﬁdence score, and outputs
a probability of an input being adversarial or not. Here, the detector network is trained
on both clean and adversarial examples. They evaluated this method on MNIST model
and demonstrated that this method is successful in detecting adversarial examples gen-
erated by FGSM and BIM. They showed qualitatively how reconstruction of adversarial
examples are noisier compared to normal inputs (see ﬁg 3), allowing one to compute the

6

diﬀerences as a way to detect adversarial examples. The diﬀerences can be calculated
using smootheness loss between input and reconstructed image.

Figure 2.5: Adversarial image reconstruction are of lower quality then clean image recon-
structions

2.4 Limitations

Carlini and Wagner [7] in their work showed that all of these methods are not robust

and ineﬀective against strong attacks and also inconsistent with other datasets.

For the PCA method, they found that it can be bypassed easily if the attacker knows
the defense strategy by setting a constraint on the magnitude of the magnitude of last
PCA components. This can be achieved using C&W attacks by adding this constraint to
the adversary generation process. Also, they showed that the PCA method was eﬀective
on MNIST as most of the pixels are black on all images for eg the corner points but in
adversaries they are changed resulting into larger magnitudes at higher PCA components.
However, they argued that this was due to the aforementioned artifact in the dataset itself
which made it to succeed on the MNIST dataset but however showed poor performance
on the Cifar10 dataset.

The decoder and classiﬁer networks are fully diﬀerentiable, hence can be used to pose
a white box attack bypassing them. To summarize, these methods did show good result
for some speciﬁc datasets against FGSM and BIM adversaries but however fail to identify
complex adversary attacks like C&W

7

Chapter 3

Network Based Methods

3.1 Adversary detector network

Metzen et al. [36] proposed augmenting a pretrained neural network with a binary
detector network at each layer of the network. The detector network D is a binary classiﬁer
network that is trained to classify an example real and adversarial example. It takes the
output neurons of the pre-trained network at a certain layer as an input and outputs
the probability of an input being adversarial (i.e., D(fl(x)) = yadv or yclean, where fl(x)
denotes the output of classiﬁer f at the l-th layer). For the training, adversary eamples are
generated from the pretrained model and augmented to the dataset. In order to account
for future attacks with the assumption that attacker have access to both the classiﬁer and
detector networks, The author generated examples which speciﬁcally attempted to fool
the detector. They generated adversarial examples according to the following eqn(1) :-

i+1 = Clip(cid:15)x(cid:48)
x(cid:48)
where x(cid:48)

i + σ[(1 − α)sign(δxLclassif ier(x(cid:48)

i, y)) + αsign(δxLclassif ier(x(cid:48)

i, yadv))]

0 = x, and α denotes the weight factor that controls whether the attacker’s
objective is to attack the classiﬁer or the detector, chosen randomly at every iteration.
The training is ﬁrst done for normal examples, the adversarial training

This method was found to be giving satisfactory results against FGSM, DeepFooland
BIM attacks tested on CIFAR10 and 10-class Imagenet. Placing the detector network
at diﬀerent depths of the network gave diﬀerent results for diﬀerent attacks demonstrat-
ing the characteristic diﬀerence of the attacks. The author further proposed booging of
detector networks at each of the individual layer

Gong et al. [15] proposed a similar method by training a binary classiﬁer network to
diﬀerentiate between adversarial and clean examples. However, the binary classiﬁer here
is a completely separate network from the main classiﬁer. However, rather than gener-
ating adversarial examples against the detector, they generate adversarial examples for
a pretrained classiﬁer, and augment these adversarial examples to the original training
data to train explicitly the binary classiﬁer freezing the weights of the model. The author
observed several limitations on the generalization of this method. First, they found that
the detector network is sensitive to the (cid:15) value used to generate FGSM and BIM adver-

8

Figure 3.1: Detector network conﬁguration

saries, in the sense that detector trained on adversarial examples with (cid:15)2 cannot detect
adversarial examples with (cid:15)2, especially when (cid:15)2 ¡ (cid:15)1. They also discovered that training a
detector was even not able to generalize on other adversaries noting that adversaries gen-
erated by diﬀerent methods have diﬀerent characteristics. For example, a detector trained
on FGSM adversaries was found to not be able to detect JSMA adversaries correctly.

3.2 Additional Class Node

Another similar method was proposed by Grosse et al.
[15], a detection method that
works by augmenting a classiﬁer network with an additional class node that represents
adversarial class. Given a pretrained model, a new model with an extra class node is
trained on clean examples and adversarial examples generated for the pre-trained model
itself and setting the ground truth for the newly created adversaries as the new additional
node.

Grosse et al. showed that this method can be used to detect adversaries generated by
FGSM and JSMA robustly on MNIST. They also showed how this method can reliably
detect SBA , particularly FGSM-SBA and JSMA-SBA variants also.

Figure 3.2: Example test case labels of additional class node

9

Concurrent work from Hosseini et al. proposed a very similar method with slight
diﬀerence in the detail of the training procedure where the classiﬁer is alternatively trained
on clean and adversarial examples (i.e., via adversarial training). Furthermore, the labels
used for training the model were carefully assigned by performing label smoothing. Label
smoothing sets a probability value to the correct class and distributes the rest uniformly
to the other classes. Having a slightly diﬀerent goal than Grosse et al. [15], Hosseini et
al. [23] evaluated their method in the blackbox settings and showed how their method is
especially helpful to reduce the transferability rate of adversarial examples. Although the
detection method proposed by Hosseini et al. was not evaluated by Carlini and Wagner,
the method appears to be similar with the method proposed by Grosse et al. Since the
evaluation of this method was only done on MNIST and grayscaled GTSRB, it is doubtful
whether this method will also exhibit high false positive rate when tested on CIFAR10 or
other datasets with higher complexity and on other whitebox attacks. See the ﬁg 6 for
the visual diagram showing the algorithm

Figure 3.3: Training procedure

3.3 Limitations

First of all, detector networks are diﬀerentiable, hence if their weights are accessible to
the attacker in a whitebox attack, the constraint problem can be changed accordingly to
bypass them. Laater, Carlini and Wagner found out that these methods have a high false
positive rate against complex attacks C&W. Also, these methods are highly sensitive to
the (cid:15) values, if they are adversarially trained to avoid attacks for a particular value of (cid:15),
they fail to give similar results for a slightly higher value of (cid:15). Also, Carlini and Wagner
[5] showed that similar results are not replicable on other complex datasets like CIFAR10
as on simple datasets like MNIST, hence authenticity of these methods is questionable.

10

Chapter 4

Distribution Based ethods

4.1 Kernel Density and Bayesian Uncertainty Esti-

mates

Assuming that adversarial examples are from a diﬀerent distribution than the normal
examples i.e. they do not lie inside the non-adversarial data manifold, Feinman et al [11]
proposed 2 methods namely Kernel Density Estimates(KDE), where the objective is to
identify a data point being in class manifold and Bayesian Uncertainty Estimates (BUE),
where the objective is to ﬁnd the data points in the low conﬁdence region where the KDE
is not eﬀective. KDE works on using the logit vector distribution of clean examples to
ﬁnd the density estimate. The mathematical formulation of KDE is given below :-

(cid:80)

−||Z(x)−Z(xi)||2
σ2

e

xt∈Xt

|Xt|

KDE(x) =
where Z(x) denotes the logits vector given x as the input, Xt is a set of data with output
class t and σ is the bandwidth or the variance of the kernel. Note that the kernel function
used is the Gaussian kernel is evaluated from the logits vecor instead of the input vector.
The use of the logits is inspired from the work by Bengio et al. [3] and Gardner et al. [14],
which demonstrated how the manifold learned by the network becomes increasingly linear
and ﬂatter which makes it easier to work with the logit space than the input space. An
adversarial example x’ will have low KDE value if x’ is far from the target class manifold
in the logits space, and thus can be detected using threshold based or similar approach.
Later, the author suggested that this method fail to work when x(cid:48) is near the target class
manifold.

They proposed another similar method using dropout at inference time to measure
uncertainty. They mathematically quantify the uncertainty U(x) of the network by per-
forming N stochastic passes by applying dropouts with the same setting, and using the
average value of the logits as the estimate. The intuition here is that for the adversarial
examples, the dependence on some speciﬁc features is quite high, hence if by dropout
those deep features are dropped, a high change in value of the output is caused leading

11

N

(cid:80)N

(cid:80)N

(cid:80)N

i=1 f (x)i)

i=1 f (x)T

i f (x)i − ( 1
N

to more variance, hence a threshold can be kept on this variance to diﬀerentiate between
normal and adversarial examples. Mathematically :-
U (x) = 1
i=1 f (x)i)T ( 1
N
where f (x)i denotes the i-t stochastic prediction. Intuitively, this uncertainty measure
should be large for adversaries. The author showed that the ratio U (x(cid:48))
U (x) was signiﬁcantly
more than 1 in most case when evaluated against MNIST, CIFAR10 datasets with FGSM,
BIM, JSMA and also with complex adversaries from C&W attacks. The ratio can be used
to identify adversaries by putting up a simple threshold. KDE was also shown to be able
to detect adversaries, but especially indicative when evaluated against a variant of BIM
that stops the attack process following a ﬁxed number of iterations. This is indicated by
the ratio being less than 1 in most cases.

Based on these ﬁndings, the authors proposed a combined threshold based detection
method, using both metrics to perform adversarial detection. This is done by putting
threshold values on the uncertainty measurement and on the negative log of the kernel
density estimates of a sample. The combined method was shown to be superior than both
methods individually against FGSM, BIM, JSMA, and C&W on MNIST, CIFAR10, and
SVHN.

Carlini and Wagner [5] evaluated this method and concluded that the KDE method
alone does not work well on CIFAR10 and can be fooled with modiﬁed C&W attacks
by including an additional objective term deﬁned by max(-log(KDE(x’))-(cid:15), 0) both in
whitebox and blackbox settings. They also showed that BUE can be circumvented by
C&W attacks on the expectation values of diﬀerent models sampled during dropout.
However, they noted that the distortions required to generate adversarial examples that
fool BUE are quite larger compared to other detection methods that they evaluated. As a
result, Carlini and Wagner [5] concluded that BUE was the hardest detection method to
fool compared to the other methods known to it so far. It is also relatively straightforward
and easy to implement as an add-on to an existing network architecture.

4.2 Maximum Mean Discrepancy

This particular work [x] argued that the adversaries form a diﬀerent characteristic dis-
tribution as per the input samples. Hence the adversaries can be detected by ﬁnding
the prior mean and variance of the cluster formed on input by the adversaries and non
adversaries and using these to identify the new input sample as an adversary or non ad-
versary. The max distance along all dimensions in the means of these 2 clusters denotes
the statistical diﬀerence in the distributions of adversaries from non adversaries. This is
called the Maximum Mean Discrepancy (MMD), mathematically expressed as :-

M M Db(F, X1, X2) = supf ∈F (

(cid:80)N

i=1 f (x1i)
N

(cid:80)M

i=1 f (x2i)
M

)

−

12

Figure 4.1: Clusters formed by normal samples and adversaries

4.3 Using Distribution from Deep Layers

PixelDefend was proposed which considered the deep layers of a pretrained network
for Bayesian inference instead of input or output logit vectors. The original work is
a defense technique but is related to ﬁnding a distribution for train samples and then
bringing the input sample closer to that distribution for defense, hence it can be used for
detection as well with the same idea using thresholding on the obtained probability value.
It uses the deep representations of PixelCNN which is used for classiﬁcation of images for
deriving the distribution of the train samples. The probability of the sample to be non
adversarial is obtained using Bayesian inference from the obtained distribution assuming
it to be Gaussian. Mathematically, probability is expressed as :-

pCN N (X) = ΠipCN N (xi|x1:(i−1))
To normalize the value for thresholding, Bits Per Dimension (BPD) is used which is
obtained by scaling the negative of log of probability values by the dimension, as follows
:-

BP D(X) = −logpCN N (X)
I.J.K.log(2)
The BPD values were consistently diﬀerent and characteristic for diﬀerent adversaries,
normal examples had less values, adversaries have higher values, strong adversaries like
C&W and DeepFool had less diﬀerence in the values, hence had higher error rates as
shown in ﬁg 4.2

13

Figure 4.2: (a) BPD values obtained for diﬀerent adversaries (b) Percentage error in
results if diﬀerentiation is done on the basis of BPD threshold

4.4 Feature Squeezing

Xu et al.

[45] argued that the with the large dimensionality of input features,
yields a large attack surface. They proposed a detection strategy in which they compare
the predictions of squeezed i.e. scaled down input for eg downscaled input image and
unsqueezed inputs. As the name suggests, the goal of feature squeezing is to remove
unnecessary features from an input by reducing the dimensionality. Two feature squeezing
methods were evaluated: color bit-depth reduction and spatial smoothing, both with local
and non-local smoothing. The input is labelled as adversarial if the L1 diﬀerence (absolute
diﬀerence) between the model’s prediction on squeezed and unsqueezed inputs is larger
than a certain threshold value T. Diﬀerent levels of squeezing can be combined by bagging
i.e. taking the max of L1 diﬀerence from all of them. Figure 4.3 illustrates this method.

Figure 4.3: Procedure for squeezing model

For color depth reduction, The author experimented with various i-bit depth where 1 ≤
i ≤ 8. For local smoothing, median ﬁltering was found to be more eﬀective than Gaussian
and mean smoothing. In the case of non-local smoothing, non-local means denoising was
used. There is no strong motivation for this choice of squeezers as mentioned by the
authors, future work on the most eﬀective squeezers is still needed. They also suggested
using multiple feature squeezing methods as part of the detection model, since diﬀerent
squeezers are more eﬀective on certain types of adversaries than others and taking the
max of the diﬀerence outputs given by each of them. Given n squeezers the output of the
detector is deﬁned as follows :-

14

Detector(x) =




Adversarial,

if max(||f (x) − f (s1(x)||1, ..., ||f (x) − f (sn(x)||1) > T



Benign,

otherwise

where f(x) is the output vector given by the softmax layer of a deep neural network

The author also showed that feature squeezing can also be used to increase robustness
along adversaries.This is similar to the work from Chuan Guo [18] which also attempted
to enhance adversarial robustness through various through various input transformations.
Furthermore, the author also argued that the robustness can be increased by coupling
feature squeezing with adversarial training. Feature squeezing demonstrated excellent
results in detecting FGSM, DeepFool, JSMA and C&W attacks on MNIST, CIFAR10
and ImageNet datasetswhen the attacker is not aware of the strategy used. The author
found that this technique does well for L2 and Linf based attacks and gave poor results
on L0 derived attacks. Intuitively, L0 attacks cause perturbations that can be thought
of as salt-and-pepper noise, in which median ﬁltering has often been used to remove this
type of noise. These ﬁndings support the need of using joint squeezers conﬁguration to
take into account diﬀerent types of adversarial examples.

The author also considered a case where the attacker is aware of the detection model.
In this setting, the attacker would typically ﬁnd adversaries that fool the classiﬁer and
minimize the L1 score between squeezed and unsqueezed according to the squeezing model
used. To avoid these type of whitebox attacks, the author suggested to introduce ran-
domness for eg randomizing the threshold value as T ± rand(δT ).

15

Chapter 5

Some special methods

5.1 Reverse Cross Entropy

Pang et al. [38] proposed a novel detection method by introducing a new objective function
called the Reverse Cross-Entropy (RCE). For C no of classes, RCE loss is deﬁned as :-

LRCE = −yrlogf (x)
Where yr is called the reversed label and is deﬁned as :-

yr(i) =




0,

if i = y(truelabel)



1
C−1, otherwise

On training a model, to minimize LRCE will produce a special type of classiﬁer called
reverse classiﬁer which will output logits such the the one with lowest value will be the
[38] suggested to negate the logits such that f(x) =
predicted class. Thus, Pang et al.
softmax(-Z(x)). After the model is trained, adversarial detection on the model can be
performed based on threshold for some measures like Kernel density estimate (as discussed
in section 4.1). They also introduced a new metric called the non maximum element (non-
ME) deﬁned as :-

ˆf (x)ilog( ˆf (x)i

non − M E(x) = − (cid:80)
where ˆf (x)i denotes the normalized on-max elements in f(x), For detection, the value
is compared by a threshold T. If the value is less than threshold, it is classiﬁed as an
adversary else as a normal valid example

i(cid:54)=ˆy

This method was demonstrated to be robust when evaluated on MNIST and CIFAR10
datasets against FGSM, BIM/ILLCM [29], JSMA [4], C&W [5] and modiﬁed C&W that
takes into account the use of KDE; which is referred to as MCW. Furthermore, this
method was also evaluated in blackbox settings against MCW-SBA [39] and showcased
how the adversarial examples generated by SBA suﬀer from poor transferability. Even
when adversarial examples that fool both detector and classiﬁer models were found, the
adversarial examples usually exhibit larger distortions which are visually perceptible to a
human.

16

Finally, the author argued that the use of RCE not only allows one to perform adver-
sarial detection, but also increases robustness of a model in general compared to using
standard cross-entropy as the objective function. They also did the comparison of t-SNE
embeddings between a model trained with RCE and non-RCE objectives, and showed how
the t-SNE [34] visualization of the model that was trained with RCE objective achieved
higher separability. The use of new objective functions such as RCE which constrains
the ability of attacker to generate adversarial examples making it robust seems to be an
interesting research area to be explored in the future.

5.2 Adversarial Examples Detection in Features Dis-

tance Spaces

The authors for this work [8] put forth a very interesting way to detect adversaries. The
authors argued that the adversaries trace a certain path when considered in the feature
space through all the deep layers of network. These paths can be used as a criteria to
determine whether the input is an adversary is not. The set of all paths taken by the
normal examples are used to determine the mean paths. The mean vector values are pre-
calculated at each layer to get a 2D vector of the mean feature vectors for each sample
class. Using these means at each layer, an M dimensional vector is obtained at each layer
where M is the no of classes as the L2 distance from these M mean vectors at each layer.
These N distance vectors are passed through sequential neural networks like LSTM or
MLP. The output of this sequential network is used to predict whether the input is an
adversary or not

Figure 5.1: Intuition on distance feature paths followed by normal samples and adversaries

Basically, this method tries to ﬁnd patterns in the sequential inference through deep
layers to detect the presence of adversarial example. It tries to encode sequential ﬂow
of inference within layers into training an LSTM network which outputs the probability.

17

Hence the trained LSTM network tries to ﬁnd similarities in the inference patterns which
it fails to in case of adversarial examples, hence able to detect them. The basic procedure
can be described as :-

1. Calculate the principle centers for each class for each layer as the centroid of all

the examples:- pl

c =

(cid:80)

j=1 Kcol
Kc

c,j

2. For the input, derive C neuron embedding for each layer by taking L2 distance from

each of the principal centre :-

el = (d(ol, pl
3. Feed the L layer embedding to train a time series network like LSTM to ﬁnd

2).....d(ol, pl

1), d(ol, pl

C))

patterns in the sequential embedding

Figure 5.2: Network architecture

The model demonstrated excellent results on large no of adversary attacks including
L-BFGS, FGSM, BIM, PGD, MI-FGSM. Also it was observed that using LSTM yielded
better results than MLP due to the inherent sequential nature of the patterns which are
better recorded by the LSTM. True Positive Rate (TPR) and False Positive Rate (FPR)
ratio results can be seen from ﬁg 5.3

Figure 5.3: True positive and false positive rates for adversaries and normal samples

18

Chapter 6

Use of conditional Variational
AutoEncoder (CVAE)

6.1

Introduction

Many recent works have presented eﬀective ways in which adversarial attacks can be
avoided as we discussed in earlier chapters. To re-iterate, adversarial attacks can be
classiﬁed into whitebox and blackbox attacks. White-box attacks [1] assume access to the
neural network weights and architecture, which are used for classiﬁcation, and thereby
speciﬁcally targeted to fool the neural network. Hence, they are more accurate than
blackbox attacks [1] which do not assume access the model parameters. Methods for
detection of adversarial attacks can be broadly categorized as – (i) statistical methods, (ii)
network based methods, and (iii) distribution based methods. Statistical methods [22] [32]
focus on exploiting certain characteristics of the input images or the ﬁnal logistic-unit layer
of the classiﬁer network and try to identify adversaries through their statistical inference.
A certain drawback of such methods as pointed by [6] is that the statistics derived may be
dataset speciﬁc and same techniques are not generalized across other datasets and also fail
against strong attacks like CW-attack. Network based methods [36] [15] aim at speciﬁcally
training a binary classiﬁcation neural network to identify the adversaries. These methods
are restricted since they do not generalize well across unknown attacks on which these
networks are not trained, also they are sensitive to change with the amount of perturbation
values such that a small increase in these values makes this attacks unsuccessful. Also,
potential whitebox attacks can be designed as shown by [6] which fool both the detection
network as well as the adversary classiﬁer networks. Distribution based methods [11] [13]
[44] [46] [26] aim at ﬁnding the probability distribution from the clean examples and try
to ﬁnd the probability of the input example to quantify how much they fall within the
same distribution. However, some of the methods do not guarantee robust separation of
randomly perturbed and adversarial perturbed images. Hence there is a high chance that
all these methods tend to get confused with random noises in the image, treating them

19

as adversaries.

To overcome this drawback so that the learned models are robust with respect to
both adversarial perturbations as well as sensitivity to random noises, we propose the
use of Conditional Variational AutoEncoder (CVAE) trained over a clean image set. At
the time of inference, we empirically establish that the input example falls within a low
probability region of the clean examples of the predicted class from the target classiﬁer
network. It is important to note here that, this method uses both the input image as
well as the predicted class to detect whether the input is an adversary as opposed to
some distribution based methods which use only the distribution from the input images.
On the contrary, random perturbations activate the target classiﬁer network in such a
way that the predicted output class matches with the actual class of the input image
and hence it falls within the high probability region. Thus, it is empirically shown that
our method does not confuse random noise with adversarial noises. Moreover, we show
how our method is robust towards special attacks which have access to both the network
weights of CVAE as well as the target classiﬁer networks where many network based
methods falter. Further, we show that to eventually fool our method, we may need larger
perturbations which becomes visually perceptible to the human eye. The experimental
results shown over MNIST and CIFAR-10 datasets present the working of our proposal.

In particular, the primary contributions made by our work is as follows.

(a) We propose a framework based on CVAE to detect the possibility of adversarial

attacks.

(b) We leverage distribution based methods to eﬀectively diﬀerentiate between randomly

perturbed and adversarially perturbed images.

(c) We devise techniques to robustly detect specially targeted BIM-attacks [36] using our

proposed framework.

To the best of our knowledge, this is the ﬁrst work which leverages use of Variational
AutoEncoder architecture for detecting adversaries as well as aptly diﬀerentiates noise
from adversaries to eﬀectively safeguard learned models against adversarial attacks.

6.2 Proposed Framework Leveraging CVAE

In this section, we present how Conditional Variational AutoEncoders (CVAE), trained
over a dataset of clean images, are capable of comprehending the inherent diﬀerentiable
attributes between adversaries and noisy data and separate out both using their proba-
bility distribution map.

6.2.1 Conditional Variational AutoEncoders (CVAE)

Variational AutoEncoder is a type of a Generative Adversarial Network (GAN) having
two components as Encoder and Decoder. The input is ﬁrst passed through an encoder

20

to get the latent vector for the image. The latent vector is passed through the decoder to
get the reconstructed input of the same size as the image. The encoder and decoder layers
are trained with the objectives to get the reconstructed image as close to the input image
as possible thus forcing to preserve most of the features of the input image in the latent
vector to learn a compact representation of the image. The second objective is to get
the distribution of the latent vectors for all the images close to the desired distribution.
Hence, after the variational autoencoder is fully trained, decoder layer can be used to
generate examples from randomly sampled latent vectors from the desired distribution
with which the encoder and decoder layers were trained.

Figure 6.1: CVAE Model Architecture

Conditional VAE is a variation of VAE in which along with the input image, the class of
the image is also passed with the input at the encoder layer and also with the latent vector
before the decoder layer (refer to Figure 6.1). This helps Conditional VAE to generate
speciﬁc examples of a class. The loss function for CVAE is deﬁned by Equation 6.1.
The ﬁrst term is the reconstruction loss which signiﬁes how closely can the input X be
reconstructed given the latent vector z and the output class from the target classiﬁer
network as condition, c. The second term of the loss function is the KL-divergence (DKL)
between the desired distribution, P (z|c) and the current distribution (Q(z|X, c)) of z
given input image X and the condition c.

L(X, c) = E(cid:2) log P (X|z, c)(cid:3) − DKL

(cid:2)Q(z|X, c) || P (z|c)(cid:3)

(6.1)

6.2.2 Training CVAE Models

For modeling log P (X|z, c), we use the decoder neural network to output the reconstructed
image, Xrcn where we utilize the condition c which is the output class of the image
to get the set of parameters, θ(c) for the neural network. We calculate Binary Cross
Entropy (BCE) loss of the reconstructed image, Xrcn with the input image, X to model
log P (X|z, c). Similarly, we model Q(z|X, c) with the encoder neural network which
takes as input image X and utilizes condition, c to select model parameters, θ(c) and
outputs mean, µ and log of variance, log σ2 as parameters assuming Gaussian distribution
for the conditional distribution. We set the target distribution P (z|c) as unit Gaussian
distribution with mean 0 and variance 1 as N (0, 1). The resultant loss function would be

21

as follows,

L(X, c) = BCE(cid:2)X, Decoder(X, θ(c))(cid:3) −

(cid:104)
Encoder2

1
2

σ(X, θ(c)) + Encoder2
σ(X, θ(c))(cid:1)(cid:105)

−1 − log (cid:0)Encoder2

µ(X, θ(c))

(6.2)

The model architecture weights, θ(c) are a function of the condition, c. Hence, we
learn separate weights for encoder and decoder layers of CVAE for all the classes.
It
implies learning diﬀerent encoder and decoder for each individual class. The layers sizes
are tabulated in Table 6.1. We train the Encoder and Decoder layers of CVAE on clean
images with their ground truth labels and use the condition as the predicted class from
the target classiﬁer network during inference.

Attribute

Encoder

Mean
Variance
Project

Decoder

Layer

Conv2d

BatchNorm2d
Relu
Conv2d

BatchNorm2d
Relu
Conv2d

BatchNorm2d
Linear
Linear
Linear
Reshape
ConvTranspose2d

BatchNorm2d
Relu
ConvTranspose2d

BatchNorm2d
Relu
ConvTranspose2d

Sigmoid

Size

Channels: (c, 32)
Kernel: (4,4,stride=2,padding=1)
32

Channels: (32, 64)
Kernel: (4,4,stride=2,padding=1)
64

Channels: (64, 128)
Kernel: (4,4,stride=2,padding=1)
128
(1024,zdim=128)
(1024,zdim=128)
(zdim=128,1024)
(128,4,4)
Channels: (128, 64)
Kernel: (4,4,stride=2,padding=1)
64

Channels: (64, 32)
Kernel: (4,4,stride=2,padding=1)
64

Channels: (32, c)
Kernel: (4,4,stride=2,padding=1)

Table 6.1: CVAE Architecture Layer Sizes. c = Number of Channels in the Input Image
(c = 3 for CIIFAR-10 and c = 1 for MNIST).

6.2.3 Determining Reconstruction Errors

Let X be the input image and ypred be the predicted class obtained from the target
classiﬁer network. Xrcn,ypred is the reconstructed image obtained from the trained encoder
and decoder networks with the condition ypred. We deﬁne the reconstruction error or the
reconstruction distance as in Equation 6.3. The network architectures for encoder and

22

decoder layers are given in Figure 6.1.

Recon(X, y) = (X − Xrcn,y)2

(6.3)

Two pertinent points to note here are:

• For clean test examples, the reconstruction error is bound to be less since the CVAE
is trained on clean train images. As the classiﬁer gives correct class for the clean
examples, the reconstruction error with the correct class of the image as input is
less.

• For the adversarial examples, as they fool the classiﬁer network, passing the ma-
licious output class, ypred of the classiﬁer network to the CVAE with the slightly
perturbed input image, the reconstructed image tries to be closer to the input with
class ypred and hence, the reconstruction error is large.

As an example, let the clean image be a cat and its slightly perturbed image fools the
classiﬁer network to believe it is a dog. Hence, the input to the CVAE will be the slightly
perturbed cat image with the class dog. Now as the encoder and decoder layers are
trained to output a dog image if the class inputted is dog, the reconstructed image will
try to resemble a dog but since the input is a cat image, there will be large reconstruction
error. Hence, we use reconstruction error as a measure to determine if the input image
is adversarial. We ﬁrst train the Conditional Variational AutoEncoder (CVAE) on clean
images with the ground truth class as the condition. Examples of reconstructions for
clean and adversarial examples are given in Figure 6.2 and Figure 6.3.

(a) Input Images

(b) Reconstructed Images

Figure 6.2: Clean and Adversarial Attacked Images to CVAE from MNIST Dataset

(a) Input Images

(b) Reconstructed Images

Figure 6.3: Clean and Adversarial Attacked Images to CVAE from CIFAR-10 Dataset.

23

6.2.4 Obtaining p-value

As already discussed, the reconstruction error is used as a basis for detection of adversaries.
We ﬁrst obtain the reconstruction distances for the train dataset of clean images which is
expected to be similar to that of the train images. On the other hand, for the adversarial
examples, as the predicted class y is incorrect, the reconstruction is expected to be worse as
it will be more similar to the image of class y as the decoder network is trained to generate
such images. Also for random images, as they do not mostly fool the classiﬁer network, the
predicted class, y is expected to be correct, hence reconstruction distance is expected to be
less. Besides qualitative analysis, for the quantitative measure, we use the permutation
test from [10]. We can provide an uncertainty value for each input about whether it
comes from the training distribution. Speciﬁcally, let the input X (cid:48) and training images
X1, X2, . . . , XN . We ﬁrst compute the reconstruction distances denoted by Recon(X, y)
for all samples with the condition as the predicted class y = Classifier(X). Then,
using the rank of Recon(X (cid:48), y(cid:48)) in {Recon(X1, y1), Recon(X2, y2), . . . , Recon(XN , yN )} as
our test statistic, we get,

T = T (X (cid:48); X1, X2, . . . , XN )

=

N
(cid:88)

i=1

I(cid:2)Recon(Xi, yi) ≤ Recon(X (cid:48), y(cid:48))(cid:3)

(6.4)

Where I[.] is an indicator function which returns 1 if the condition inside brackets is true,
and 0 if false. By permutation principle, p-value for each sample will be,

p =

1
N + 1

(cid:16) N
(cid:88)

i=1

I[Ti ≤ T ] + 1

(cid:17)

(6.5)

Larger p-value implies that the sample is more probable to be a clean example. Let t be
the threshold on the obtained p-value for the sample, hence if pX,y < t, the sample X is
classiﬁed as an adversary. Algorithm 1 presents the overall resulting procedure combining
all above mentioned stages.

recon ← Train(Xtrain, Ytrain)
recon dists ← Recon(Xtrain, Ytrain)
Adversaries ← φ
for x in X do

Algorithm 1 Adversarial Detection Algorithm
1: function Detect Adversaries (Xtrain, Ytrain, X, t)
2:
3:
4:
5:
6:
7:
8:
9:
10:

ypred ← Classifier(x)
recon dist x ← Recon(x, ypred)
pval ← p-value(recon dist x, recon dists)
if pval ≤ t then

Adversaries.insert(x)

11:

return Adversaries

24

Algorithm 1 ﬁrst trains the CVAE network with clean training samples (Line 2) and
formulates the reconstruction distances (Line 3). Then, for each of the test samples
which may contain clean, randomly perturbed as well as adversarial examples, ﬁrst the
output predicted class is obtained using a target classiﬁer network, followed by ﬁnding
it’s reconstructed image from CVAE, and ﬁnally by obtaining it’s p-value to be used for
thresholding (Lines 5-8). Images with p-value less than given threshold (t) are classiﬁed
as adversaries (Lines 9-10).

6.3 Experimental Results

We experimented our proposed methodology over MNIST and CIFAR-10 datasets. All
the experiments are performed in Google Colab GPU having 0.82GHz frequency, 12GB
RAM and dual-core CPU having 2.3GHz frequency, 12GB RAM. An exploratory version
of the code-base will be made public on github.

6.3.1 Datasets and Models

Two datasets are used for the experiments in this paper, namely MNIST [31] and CIFAR-
10 [27]. MNIST dataset consists of hand-written images of numbers from 0 to 9. It consists
of 60, 000 training examples and 10, 000 test examples where each image is a 28 × 28 gray-
scale image associated with a label from 1 of the 10 classes. CIFAR-10 is broadly used for
comparison of image classiﬁcation tasks. It also consists of 60, 000 image of which 50, 000
are used for training and the rest 10, 000 are used for testing. Each image is a 32 × 32
coloured image i.e. consisting of 3 channels associated with a label indicating 1 out of 10
classes.

We use state-of-the-art deep neural network image classiﬁer, ResNet18 [19] as the
target network for the experiments. We use the pre-trained model weights available
from [25] for both MNIST as well as CIFAR-10 datasets.

6.3.2 Performance over Grey-box attacks

If the attacker has the access only to the model parameters of the target classiﬁer model
and no information about the detector method or it’s model parameters, then we call such
attack setting as Grey-box. This is the most common attack setting used in previous works
against which we evaluate the most common attacks with standard epsilon setting as used
in other works for both the datasets. For MNIST, the value of (cid:15) is commonly used between
0.15-0.3 for FGSM attack and 0.1 for iterative attacks [43] [15] [46]. While for CIFAR10,
the value of (cid:15) is most commonly chosen to be 8
255 as in [44] [46] [12]. For DeepFool [37] and
Carlini Wagner (CW) [6] attacks, the (cid:15) bound is not present. The standard parameters
as used by default in [33] have been used for these 2 attacks. For L2 attacks, the (cid:15) bound

25

is chosen such that success of the attack is similar to their L∞ counterparts as the values
used are very diﬀerent in previous works.

Reconstruction Error Distribution

The histogram distribution of reconstruction errors for MNIST and CIFAR-10 datasets
for diﬀerent attacks are given in Figure 6.4. For adversarial attacked examples, only
examples which fool the network are included in the distribution for fair comparison.
It may be noted that, the reconstruction errors for adversarial examples is higher than
normal examples as expected. Also, reconstructions errors for randomly perturbed test
samples are similar to those of normal examples but slightly larger as expected due to
reconstruction error contributed from noise.

(a) MNIST dataset

(b) CIFAR-10 dataset

Figure 6.4: Reconstruction Distances for diﬀerent Grey-box attacks

p-value Distribution

From the reconstruction error values, the distribution histogram of p-values of test samples
for MNIST and CIFAR-10 datasets are given in Figure 6.5.
It may be noted that, in
case of adversaries, most samples have p-value close to 0 due to their high reconstruction
error; whereas for the normal and randomly perturbed images, p-value is nearly uniformly
distributed as expected.

ROC Characteristics

Using the p-values, ROC curves can be plotted as shown in Figure 6.6. As can be observed
from ROC curves, clean and randomly perturbed attacks can be very well classiﬁed from
all adversarial attacks. The values of (cid:15)atk were used such that the attack is able to fool
the target detector for at-least 45% samples. The percentage of samples on which the
attack was successful for each attack is shown in Table 6.2.

26

(a) p-values from MNIST dataset

(b) p-values from CIFAR-10 dataset

Figure 6.5: Generated p-values for diﬀerent Grey-box attacks

(a) MNIST dataset

(b) CIFAR-10 dataset

Figure 6.6: ROC Curves for diﬀerent Grey-box attacks

Statistical Results and Discussions

The statistics for clean, randomly perturbed and adversarial attacked images for MNIST
and CIFAR datasets are given in Table 6.2. Error rate signiﬁes the ratio of the number
of examples which were misclassiﬁed by the target network. Last column (AUC) lists the
area under the ROC curve. The area for adversaries is expected to be close to 1; whereas
for the normal and randomly perturbed images, it is expected to be around 0.5.

Type

Error Rate (%)

Parameters
MNIST CIFAR-10 MNIST CIFAR-10 MNIST CIFAR-10

AUC

NORMAL
RANDOM
FGSM
FGSM-L2
R-FGSM
R-FGSM-L2
PGD

2.2
2.3
90.8
53.3
91.3
54.84
82.13

CW
DeepFool

100
97.3

8.92
9.41
40.02
34.20
41.29
34.72
99.17

100
93.89

-
(cid:15)=0.1
(cid:15)=0.15
(cid:15)=1.5

-
(cid:15)= 8
255
(cid:15)= 8
255
(cid:15) = 1
255 , 8
255 )
255 ,1)
255 ,n=12

(cid:15)=(0.05,0.1) (cid:15)=( 4
(cid:15)=( 4
(cid:15)=(0.05,1.5)
(cid:15)=0.1,n=12 (cid:15)= 8
(cid:15)step = 0.02 (cid:15)step= 1
255

-
-

-
-

0.5
0.52
0.99
0.95
0.99
0.95
0.974

0.98
0.962

0.5
0.514
0.91
0.63
0.91
0.64
0.78

0.86
0.75

Table 6.2: Image Statistics for MNIST and CIFAR-10. AUC : Area Under the ROC
Curve. Error Rate (%) : Percentage of samples mis-classiﬁed or Successfully-attacked

It is worthy to note that, the obtained statistics are much comparable with the state-

27

of-the-art results. Interestingly, some of the methods [44] explicitly report comparison
results with randomly perturbed images and are ineﬀective in distinguishing adversaries
from random noises, but most other methods do not report results with random noise
added to the input image. Since other methods use varied experimental setting, attack
models, diﬀerent datasets as well as (cid:15)atk values and network model, exact comparisons
with other methods is not directly relevant primarily due to such varied experimental
settings. However, the results are mostly similar to our results while our method is able
to statistically diﬀerentiate from random noisy images.

In addition to this, since our method does not use any adversarial examples for train-
ing, it is not prone to changes in value of (cid:15) or with change in attacks which network based
methods face as they are explicitly trained with known values of (cid:15) and types of attacks.
Moreover, among distribution and statistics based methods, to the best of our knowledge,
utilization of the predicted class from target network has not been done before. Most of
these methods either use the input image itself [26] [44] [46], or the ﬁnal logits layer [11]
[22], or some intermediate layer [32] [12] from target architecture for inference, while we
use the input image and the predicted class from target network to do the same.

6.3.3 Performance over White-box attacks

In this case, we evaluate the attacks if the attacker has the information of both the defense
method as well as the target classiﬁer network.
[36] proposed a modiﬁed PGD method
which uses the gradient of the loss function of the detector network assuming that it is
diﬀerentiable along with the loss function of the target classiﬁer network to generate the
adversarial examples. If the attacker also has access to the model weights of the detector
CVAE network, an attack can be devised to fool both the detector as well as the classiﬁer
network. The modiﬁed PGD can be expressed as follows :-

Xadv,0 = X,

Xadv,n+1 = Clip(cid:15)atk
X

(cid:110)

Xadv,n+

α.sign(cid:0) (1 − σ).∆XLcls(Xadv,n, ytarget)+
σ.∆XLdet(Xadv,n, ytarget) (cid:1)(cid:111)

(6.6a)

(6.6b)

Where ytarget is the target class and Ldet is the reconstruction distance from Equation
6.3. It is worthy to note that our proposed detector CVAE is diﬀerentiable only for the
targeted attack setting. For the non-targeted attack, as the condition required for the
CVAE is obtained from the target classiﬁer output which is discrete, the diﬀerentiation
operation is not valid. We set the target randomly as any class other than the true class
for testing.

28

Eﬀect of σ

To observe the eﬀect of changing value of σ, we keep the value of (cid:15) ﬁxed at 0.1. As can
be observed in Figure 6.7, the increase in value of σ implies larger weight on fooling the
detector i.e. getting less reconstruction distance. Hence, as expected the attack becomes
less successful with larger values of σ 6.8 and gets lesser AUC values 6.7, hence more
eﬀectively fooling the detector. For CIFAR-10 dataset, the detection model does get
fooled for higher c-values but however the error rate is signiﬁcantly low for those values,
implying that only a few samples get attacks on setting such value.

(a) MNIST dataset

(b) CIFAR-10 dataset

Figure 6.7: ROC Curves for diﬀerent values of σ. More area under the curve implies
better detectivity for that attack. With more σ value, the attack, as the focus shifts to
fooling the detector, it becomes diﬃcult for the detector to detect.

(a) MNIST dataset

(b) CIFAR-10 dataset

Figure 6.8: Success rate for diﬀerent values of σ. More value of σ means more focus on
fooling the detector, hence success rate of fooling the detector decreases with increasing
σ.

Eﬀect of (cid:15)

With changing values of (cid:15), there is more space available for the attack to act, hence the
attack becomes more successful as more no of images are attacked as observed in Figure
6.10. At the same time, the trend for AUC curves is shown in Figure 6.9. The initial
dip in the value is as expected as the detector tends to be fooled with larger (cid:15) bound.
From both these trends, it can be noted that for robustly attacking both the detector and
target classiﬁer for signiﬁcantly higher no of images require signiﬁcantly larger attack to
be made for both the datasets.

29

(a) MNIST dataset

(b) CIFAR-10 dataset

Figure 6.9: ROC Curves for diﬀerent values of (cid:15). With more (cid:15) value, due to more space
available for the attack, attack becomes less detectable on average.

(a) MNIST dataset

(b) CIFAR-10 dataset

Figure 6.10: Success rate for diﬀerent values of (cid:15). More value of (cid:15) means more space
available for the attack, hence success rate increases

6.4 Comparison with State-of-the-Art using Genera-

tive Networks

Finally we compare our work with these 3 works [35] [24] [43] proposed earlier which
uses Generative networks for detection and puriﬁcation of adversaries. We make our
comparison on MNIST dataset which is used commonly in the 3 works (Table 6.3). Our
results are typically the best for all attacks or are oﬀ by short margin from the best. For
the strongest attack, our performance is much better. This show how our method is more
eﬀective while not being confused with random perturbation as an adversary.

6.4.1 Use of simple AutoEncoder (AE)

MagNet [35] uses AutoEncoder (AE) for detecting adversaries. We compare the results
with our proposed CVAE architecture on the same experiment setting and present the
comparison in AUC values of the ROC curve observed for the 2 cases. Although the
paper’s claim is based on both detection as well as puriﬁcation (if not detected) of the
adversaries. MagNet uses their detection framework for detecting larger adversarial per-
turbations which cannot be puriﬁed. For smaller perturbations, MagNet proposes to
purify the adversaries by a diﬀerent AutoEncoder model. We make the relevant compari-

30

Type

AUC
MagNet PuVAE DefenseGAN CVAE (Ours)

RANDOM
FGSM
FGSM-L2
R-FGSM
R-FGSM-L2
PGD
CW
DeepFool
Strongest

0.61
0.98
0.84
0.989
0.86
0.98
0.983
0.86
0.84

0.72
0.96
0.60
0.97
0.61
0.95
0.92
0.86
0.60

0.52
0.77
0.60
0.78
0.62
0.65
0.94
0.92
0.60

0.52
0.99
0.95
0.987
0.95
0.97
0.986
0.96
0.95

Table 6.3: Comparison in ROC AUC statistics with other methods. More AUC implies
more detectablity. 0.5 value of AUC implies no detection. For RANDOM, value close to
0.5 is better while for adversaries, higher value is better.

son only for the detection part with our proposed method. Using the same architecture as
proposed, our results are better for the strongest attack while not getting confused with
random perturbations of similar magnitude. ROC curves obtained for diﬀerent adversaries
for MagNet are given in Figure 6.11

Figure 6.11: ROC curve of diﬀerent adversaries for MagNet (left), PuVAE (centre), De-
fenseGAN (right)

6.4.2 Use of Variational AutoEncoder (VAE)

PuVAE [24] uses Variational AutoEncoder (VAE) for purifying adversaries. We compare
the results with our proposed CVAE architecture on the same experiment setting. PuVAE
however, does not propose using VAE for detection of adversaries but in case if their
model is to be used for detection, it would be based on the reconstruction distance. So,
we make the comparison with our proposed CVAE architecture. ROC curves for diﬀerent
adversaries are given in Figure 6.11

6.4.3 Use of Generative Adversarial Network (GAN)

Defense-GAN [43] uses Generative Adversarial Network (GAN) for detecting adversaries.
We used L = 100 and R = 10 for getting the results as per our experiment setting. We
compare the results with our proposed CVAE architecture on the same experiment setting
and present the comparison in AUC values of the ROC curve observed for the 2 cases.

31

Although the paper’s main claim is about puriﬁcation of the adversaries, we make the
relevant comparison for the detection part with our proposed method. We used the same
architecture as mentioned in [43] and got comparable results as per their claim for MNIST
dataset on FGSM adversaries. As this method took a lot of time to run, we randomly
chose 1000 samples out of 10000 test samples for evaluation due to time constraint. The
detection performance for other attacks is considerably low. Also, Defense-GAN is quite
slow as it needs to solve an optimization problem for each image to get its corresponding
reconstructed image. Average computation time required by Defense-GAN is 2.8s per
image while our method takes 0.17s per image with a batch size of 16. Hence, our
method is roughly 16 times faster than Defense-GAN. Refer to Figure 6.11 for the ROC
curves for Defense-GAN.

32

Chapter 7

Use of Variational AutoEncoder
(VAE) for Puriﬁcation

7.1

Introduction

After detecting adversaries next net logical thing to do is to purify the images before
passing to the target classiﬁer. The idea of puriﬁcation is to neutralise or reduce the
threat of any adversary attack if made on the image. There are various existing works
doing this. MagNet .... PuVAE..... PixelDefend..... DefenseGAN....... We however
propose use of Variational AutoEncoder similar to MagNet but however with a modiﬁed
form similar to DefenseGAN and PixelDefend.

7.2 Training of Variational AutoEncoder (VAE)

The proposed Variational AutoEncoder is trained similar to MagNet on only clean images.
The network architecture consists of an Encoder and a Decoder. Encoder outputs mean,
µ and variance, σ of size, latent dimension, z. A value is sampled from the obtained value
of z and is fed to the decoder to output a reconstructed image Xrecon os the same size as
X. The training loss, L is described as follows :-

L(X) =

BCE[X, Decoder(z)] −

1

2[Encoder2

σ(X) + Encoder2

µ(X) − 1 − log(Encoder2

σ(X))]

(7.1)

(7.2)

7.3 Puriﬁcation of Input Images

For puriﬁcation of an input image, unlike MagNet [35] instead of directly taking the
reconstructed image, Xrecon as the puriﬁed image to feed to the classiﬁer, we propose
solving the following objective to achieve the same where Xpurif ied is the resultant puriﬁed

33

image :-

Xpurif ied = min

(cid:15)

c. (cid:107)(cid:15)(cid:107)2

2 + (cid:107)X + (cid:15) − recon(X + (cid:15))(cid:107)2

2

(7.3)

where c is a constant parameter chosen accordingly. We use ADAM optimizer with n
iterations for each image.

7.4 Results on MNIST and Comparison With Other

Works on MNIST dataset

We present the comparison in results for MNIST dataset [31] with the 2 methods discussed
earlier MagNet and DefenseGAN. Results for adversarial training are generated in the
same way as described in MagNet [35]. All results are with (cid:15) = 0.1.

Attack No defense Adversarial training DefenseGAN MagNet Ours
0.97
0.887
0.887
0.886
0.907

Clean
FGSM
R-FGSM
PGD
CW

0.974
0.269
0.269
0.1294
0.1508

0.909
0.875
0.875
0.83
0.656

0.822
0.651
0.651
0.354
0.28

0.95
0.63
0.63
0.65
0.85

Table 7.1: Comparison in results. Values reported are classiﬁer success rate (in fraction
out of 1)

7.5 Variations for CIFAR-10 Dataset

MNIST dataset is a simple dataset with easily identiﬁable number shapes which can be
easily and robustly learned by a neural network. It is therefore very easy to correct a
perturbed image with a simple back-propagation method with ﬁxed no of iterations. For
CIFAR-10 dataset however, due to the inherent complexity in classifying objects, it is
very easy to attack and thus simple defenses with ﬁxed no of iterations is not enough to
give a reasonably good puriﬁcation result on attacks.

7.5.1 Fixed no of iterations

First, we evaluate the results with ﬁxed no of iterations. The results are listed on Table
7.2. As can be observed with more no of iterations clean accuracy gets reduced and
adversarial accuracy improves. The drawback with such method is that it is not essentially
using any discrimination for the update rule for adversarial and non adversarial examples.
As a result of this, reconstruction errors of both adversarial and clean images (see Figure
7.1a) are reduced leading to disappearance of important features. Formally, the puriﬁed
example, Xpur can be expressed as follows where Xadv is the adversarial image, n is
the ﬁxed no of iterations, α is the constant learning rate and purif ier is the Puriﬁer
autoencoder function.

34

Xpur,0 = Xadv

Xpur,i+1 = Xpur,i − α

∂L(Xpur,i, purif ier(Xpur,i))
∂Xpur,i

Where L(X, Y ) = (X − Y )2

for i ∈ {1, 2..., n}

(7.4)

7.5.2 Fixed no of iterations with using ADAM optimizer for

update

Major drawback with using constant learning rate is that both clean and adversarial
examples are updated for ﬁxed no of pixels in the image space meaning the distance
between input and puriﬁed image is same irrespective if the input is adversarial or not.
To counter it, we use ADAM optimizer to achieve the minima quickly. with same no of
iterations. In this case, the update amount varies for adversarial and clean image as clean
image has less reconstruction error (see Figure 7.1b). Formally, the update is deﬁned as
follows where Xadv is the adversarial image, Xpur is the puriﬁed image, n is the no of
iterations, α and β are parameters for ADAM optimizer.

Xpur,0 = Xadv

Xpur,i+1 = Xpur,i − αwi

Where wi = βwi−1 + (1 − β)

Where L(X, Y ) = (X − Y )2

∂L(Xpur,i, purif ier(Xpur,i))
∂Xpur,i

for i ∈ {1, 2..., n}

(7.5)

7.5.3 Variable learning rate based on the current reconstruction

error

Usually for adversarial images, the initial reconstruction error is high as compared to
clean image. Based on the following, an advantage can be taken to purify mostly only
the adversarial images and not the clean images. No of iterations can be varied based
on the reconstruction error of the input sample. First, the mean value, µ and variance,
σ of the distribution of reconstruction errors for the clean validation samples are deter-
mined. Then, the probability of falling within the equivalent gaussian distribution of
reconstruction distances of clean validation samples is determined. The learning rate is
varied accordingly linear to this probability. Formally, update rule can be deﬁned as
follows where Xadv is the adversarial image, Xpur is the puriﬁed image, n is the no of
iterations, α and β are parameters for ADAM optimizer.

35

Xpur,0 = Xadv

Xpur,i+1 = Xpur,i − αiwi

Where αi = α(1 − exp−(

Xpur,i−µ
σ

)2)

And wi = βwi−1 + (1 − β)

∂L(Xpur,i, purif ier(Xpur,i))
∂Xpur,i

for i ∈ {1, 2..., n}

Where L(X, Y ) = (X − Y )2

(7.6)

7.5.4 Set target distribution for reconstruction error

Major drawback with earlier variations is that even though they discriminate in purifying
already clean and adversarial images, they still try to purify clean images by bringing down
the reconstruction error. One drawback to this is on doing so, the important features of
the image are lost as the puriﬁcation model sees them as perturbations to the images
giving rise to the reconstruction error. Back-propagating through them smoothens them
leading to less reconstruction error but confuses the classiﬁer leading to wrong predictions.
To avoid this, we ﬁrst ﬁnd the mean, µ and variance, σ of the clean image validation set.
The objective for the update rule is kept to increase the probability mass function value
with respect to the target distribution. Formally, the update rule is deﬁned as follows :-

Xpur,0 = Xadv

Xpur,i+1 = Xpur,i − αwi

Where wi = βwi−1 + (1 − β)

∂L(Xpur,i, purif ier(Xpur,i))
∂Xpur,i

for i ∈ {1, 2..., n}

(7.7)

Where L(X, Y ) =

|dist(X, Y ) − mu|
σ

Where dist(X, Y ) = (X − Y )2

7.5.5 Set target distribution for reconstruction error with mod-

iﬁed update rule

Drawback of the above method is that it tries to increase the reconstruction error (see
Figure 7.1c) of the samples with less reconstruction error belonging to clean image set.
Due to this, classiﬁcation model gives less accuracy. To avoid this, we change the update
rule as follows which ultimately leads to no change for clean images with reconstruction
error less than µ (see Figure 7.1c).

36

Xpur,0 = Xadv

Xpur,i+1 = Xpur,i − αwi

Where wi = βwi−1 + (1 − β)

∂L(Xpur,i, purif ier(Xpur,i))
∂Xpur,i

for i ∈ {1, 2..., n}

(7.8)

Where L(X, Y ) =

max(dist(X, Y ) − mu, 0)
σ

Where dist(X, Y ) = (X − Y )2

7.5.6 Add random noise at each update step

Just adding random noise has been observed to improve the classiﬁcation accuracy for
adversarial examples as it changes the overall perturbation to near random breaking the
targeted perturbation created by the adversarial attack to the corresponding clean sample.
This technique can be used in conjunction to our method where random perturbation is
added at each update step. This leads to slightly better results as observed in Table 7.2.
Formally the update rule is deﬁned as follows where γ is the amount of noise to be added
at each update step.

Xpur,0 = Xadv

Xpur,i+1 = Xpur,i − αwi

Where wi = βwi−1 + (1 − β)

∂L(Xpur,i, purif ier(Xnoise,pur,i))
∂Xnoise,pur,i

for i ∈ {1, 2..., n}

(7.9)

Where Xnoise,pur,i = Xpur,i + γrX, rX N (0, IX)

Where L(X, Y ) =

max(dist(X, Y ) − mu, 0)
σ

Where dist(X, Y ) = (X − Y )2

7.5.7 Add random transformation at each update step

Adding random rotate and resize transformations to the input image has also been ob-
served to improve the classiﬁcation accuracy for adversarial examples [41]. This technique
can be used in conjunction to our method where random transformation is added at each
update step. This leads to slightly better results as observed in Table 7.2. Formally the
update rule is deﬁned as follows where t is the transformation function taking the resize
factor f and rotation θ as input.

37

(a) Fixed no of iterations

(b) Fixed no of iterations with using
ADAM optimizer for update

(c) Set target distribution for recon-
struction error

(d) Set target distribution for recon-
struction error with modiﬁed update
rule

Figure 7.1: Reconstruction errors change for diﬀerent variations

Xpur,0 = Xadv

Xpur,i+1 = Xpur,i − αwi

Where wi = βwi−1 + (1 − β)

∂L(Xpur,i, purif ier(Xtrans,pur,i))
∂Xtrans,pur,i

for i ∈ {1, 2..., n}

(7.10)

Where Xtrans,pur,i = t(Xpur,i, f, θ)

Where L(X, Y ) =

max(dist(X, Y ) − mu, 0)
σ

Where dist(X, Y ) = (X − Y )2

7.6 Variations for ImageNet Dataset

ImageNet dataset is the high resolution dataset with cropped RGB images of size 256X256.
For our experiments we use pretrained weights on ImageNet train corpus available from
[9]. For test set we use the 1000 set of images available for ILSVRC-12 challenge [42]
which is commonly used by many works [46] for evaluating adversarial attack and defense
methods. This test set consists of 1000 images each belonging to a diﬀerent class. The
pre-trained weights of ResNet-18 classiﬁer are also provided by ILSVRC-12 challenge [42].

38

Attack

No
de-
fense
0.954
Clean
0.86
Random
FGSM
0.533
R-FGSM 0.528
0.002
BIM

Adversarial
training

MagNet

A
(n=12)

A
(n=15)

A
(n=18)

B

C

D

E

F

G

H
(Ours)

0.871
-
0.650
0.640
0.483

0.790
0.793
0.661
0.655
0.367

0.879
0.889
0.73
0.73
0.632

0.853
0.875
0.76
0.747
0.694

0.843
0.867
0.774
0.764
0.702

0.908 0.858 0.905
0.872 0.892 0.924 0.907
0.873
0.883 0.874 0.90
0.893 0.846 0.889
0.834 0.818 0.828 0.825
0.778 0.769 0.783
0.817 0.811 0.814 0.815
0.772 0.762 0.767
0.729 0.711 0.735
0.702
0.706 0.692 0.684

Table 7.2: Comparison of results for diﬀerent variations on Cifar-10 dataset. A : Fixed
no of iterations, B : Fixed no of iterations with using ADAM optimizer for update, C :
Variable learning rate based on the current reconstruction error, D : Set target distribution
for reconstruction error, E : Set target distribution for reconstruction error with modiﬁed
update rule, F : Add random noise at each update step, G : Add random transformation
at each update step, H : Add random noise + transformation at each update step

The 1000 images are chosen such that the classiﬁer gives correct class for all of them.

7.6.1 Attack method

The attack method for ImageNet dataset is diﬀerent as the classiﬁcation accuracy for
ImageNet dataset comprising of 1000 classes is deﬁned by the top-1 accuracy which means
a prediction is correct if any of the top 1% of total classes or 10 class predictions in this
case are correct. Hence, the iterative attack (which is used as baseline by all methods)
is deﬁned mathematically as follows where J(.) is the cross entropy loss, classif ier(.) is
the target classiﬁer neural network, α is the step size and πX,(cid:15)(.) function restricts value
within [X − (cid:15), X + (cid:15)].

Xattacked,0 = Xcln

Xattacked,i+1 = π(cid:15)[Xattacked,i + α

∂J(classif ier(Xattacked,i), y)
∂Xattacked,i

] for i ∈ {1, 2..., n}

(7.11)

7.6.2 Noise remover network

We use autoencoder with skip connections and blind spot architecture for noise removal
[30]. The network is trained as self-supervised task in N2V manner with the task of
outputting a clean image given a noisy image. For additional details regarding the training
settings for unknown noise form and magnitude, readers are referred to [30].

7.6.3 No of iterations

Eﬀect of varying the no of iterations of update for purifying adversarial examples can
be seen in Table 7.3. As can be observed, increasing no of iterations lead to better
classiﬁcation accuracy for puriﬁed adversaries but leads to drop in clean accuracy. Results

39

for adversaries created with diﬀerent no of iterations are also reported. The value of
(cid:15) = 8
255 are chosen as standard values for comparison with [46].

255 and update step, α = 1

7.6.4 Puriﬁcation variations

We further explore diﬀerent variations to the base version with ﬁxed no of iterations and
constant update rate α.

Random noise at each step

Similar to Cifar-10 dataset, adding random noise to input leads to increase in classiﬁcation
accuracy. Hence, we conjunct this by adding random noise at each update step. As
evident from Table 7.3, there is a slight surge in classiﬁcation of puriﬁed adversaries with
this addition. Mathematically, update rule is deﬁned as follows where γ is the amount of
noise to be added at each update step.

Xpur,0 = Xadv

Xpur,i+1 = Xpur,i − α

∂L(Xpur,i, purif ier(Xnoise,pur,i))
∂Xnoise,pur,i

for i ∈ {1, 2..., n}

(7.12)

Where Xnoise,pur,i = Xpur,i + γrX, rX N (0, IX)
Where L(X, Y ) = (X − Y )2

Random transformation at each step

Similar to Cifar-10 dataset, we add random rotate and resize transformation before each
update step. This too results in a subsequent increase in classiﬁcation accuracy of puriﬁed
adversaries as observed in Table 7.3. The reason for the rise in accuracy is that the
transformation leads to change in the locality of the features where targeted attack was
made. The classiﬁcation network however has been trained to act robustly if the image
is resized or rotated, hence the classiﬁed class for clean images doesn’t change, while for
attacked images due to relocation of targeted perturbations, the resultant perturbation
tends towards non-targeted random noise leading to increased accuracy. Mathematically,
update rule can be expressed as follows where t is the transformation function taking the
resize factor f and rotation θ as input.

Xpur,0 = Xadv

Xpur,i+1 = Xpur,i − α

∂L(Xpur,i, purif ier(Xtrans,pur,i))
∂Xtrans,pur,i

for i ∈ {1, 2..., n}

(7.13)

Where Xtrans,pur,i = t(Xpur,i, f, θ)
Where L(X, Y ) = (X − Y )2

40

Attack
Clean
Random
BIM ((cid:15) = 10
BIM ((cid:15) = 25

255 )
255 )

No defense Adversarial training MagNet

1.0
0.969
0.361
0.002

0.912
0.901
0.441
0.254

0.916
0.911
0.539
0.312

A
0.902
0.907
0.670
0.580

B
0.895
0.887
0.705
0.665

C
0.892
0.884
0.726
0.681

Table 7.3: Comparison of results for diﬀerent variations on Imagenet dataset. A : Ours,
B : Ours (With random noise), C : Ours (With random transformations)

7.7 Possible Counter Attacks

Based on the attack method, adaptive attacks can be developed to counter the attack.
We study the 2 types of attacks in detail. For a detailed review on how to systematically
determine an adaptive attack based on the category of defense, readers are referred to
[30].

7.7.1 Counter attack A

The ﬁrst adaptive attack possible is designed by approximating the transformation func-
tion i.e. the function of the output image obtained by modifying the input image through
the update rule by the diﬀerentiable function obtained by autoencoder end-to-end out-
put given the input. Intuitively this can be thought of as the near output obtained by
backpropagating through the reconstruction error loss between the input and end-to-end
puriﬁed output after passing through autoencoder puriﬁer network. We observe (see
Table 7.4) that on applying this counter attack, the accuracy for direct puriﬁer output
method drops drastically while ours method gives better robust results against this at-
tack. Mathematically, we can express the attacked image, Xattacked as follows where Xcln
is the original clean image, n is the no of iterations, classif ier(.) is the target classiﬁer
network, J(.) is the cross entropy loss, purif ier(.) is the puriﬁer autoencoder, α is the
update rate and πX,(cid:15)(.) function restricts value within [X − (cid:15), X + (cid:15)].

Xattacked,0 = Xcln

Xattacked,i+1 = πXcln,(cid:15)[Xattacked,i + αsign(

∂J(classif ier(Xattacked,i), y)
∂Xattacked,i

)]

(7.14)

for i ∈ {1, 2..., n}

7.7.2 Counter attack B

The second adaptive attack possible here is by modifying the loss function of the BIM
attack by including new weighted term getting less reconstruction error from the puriﬁer
This way we attack both the classiﬁer as well as puriﬁer. The puriﬁer method relies on
reconstruction error as a measure to update the input to get less reconstruction error

41

similar to clean images but if the attack is made with this consideration to fool the
puriﬁer method by giving similar reconstruction error to clean image while also fooling the
classiﬁer, the attack is successful as it bypasses the puriﬁer method. For this attack, the
attacked image seems to be attacked more at the edges as modifying those do not change
the reconstruction error for puriﬁer much. As observed from Table 7.4, the adaptive
counter attack is successful to an extent but needs a larger value of (cid:15) to make the attack
have a considerably successful attack rate. Mathematically this attack is deﬁned as follows
where Mathematically, we can express the attacked image, Xattacked as follows where Xcln
is the original clean image, n is the no of iterations, classif ier(.) is the target classiﬁer
network, J(.) is the cross entropy loss, purif ier(.) is the puriﬁer autoencoder, α is the
update rate, β is the weighing factor for the reconstruction error in the combined loss
function, and πX,(cid:15)(.) function restricts value within [X − (cid:15), X + (cid:15)].

Xattacked,0 = Xcln

Xattacked,i+1 = πXcln,(cid:15)[Xattacked,i + αsign(

∂J(Xattacked,i, y) + β(Xattacked,i − purif ier(Xattacked,i))2
∂Xattacked,i

)]

for i ∈ {1, 2..., n}

(7.15)

Attack
Counter attack A
Counter attack B

No defense Adversarial training MagNet Ours
0.680
0.507

0.122
0.151

0.199
0.275

0.004
0.009

Table 7.4: Comparison of results for counter attacks

42

Chapter 8

Future Work

Further, I plan to complete experimentation with the proposed VAE method for purrifying
adversaries and try t beat the benchmark for results on other datasets like CIFAR-10,
CIFAR-100, especially high dimensional ImageNet etc. Also, I would extend my proposed
CVAE for detection to test on ImageNet. ImageNet has been especially challenging to
get good results because of the high dimensional images, the attack perturbation is quite
less.

Apart from this, the recent work by Pang et al. [38] suggests that training the network
using RCE is much more robust than the currently use cross entropy loss. This opens
a completely new direction for research. There can be a separate RCE trained network
for the sake of robustness as the detector. There can be a corresponding research along
these lines by training a generator network from logits with this new loss function as it is
relatively robust against adversaries.

Also, the use of LSTM for detecting abnormal paths taken by adversaries along the
distance vector spaces seems to be an interesting research direction. The idea can be
further explored by using maybe transformers instead of LSTM to model the pattern in
the path taken. Also, instead of taking mean vectors from the class distribution, clustering
methods like K means clustering can be used to get mean vectors for the clusters. Also,
Instead of using distant spaces, PCA or similar method can be used to map to a low
dimensional representation from each layer.

Another interesting approach would be using representation learning for bayesian in-
ference. Currently for the distribution based methods, logits vector, deep representations
or input have been used for bayesian inference. What would be interesting is the use
of unsupervised representation learning to get mapping of the input to a distribution
which can be used for ﬁguring out whether a given sample falls within the normal sample
distribution or the adversarial examples.

43

Bibliography

[1] Naveed Akhtar and Ajmal Mian. “Threat of adversarial attacks on deep learning in

computer vision: A survey”. In: IEEE Access 6 (2018), pp. 14410–14430.

[2] Dario Amodei et al. Concrete Problems in AI Safety. 2016. arXiv: 1606 . 06565

[cs.AI].

[3] Yoshua Bengio et al. “Better Mixing via Deep Representations”. In: Proceedings of
the 30th International Conference on Machine Learning. Ed. by Sanjoy Dasgupta
and David McAllester. Vol. 28. Proceedings of Machine Learning Research 1. At-
lanta, Georgia, USA: PMLR, June 2013, pp. 552–560. url: http://proceedings.
mlr.press/v28/bengio13.html.

[4] Jacob Buckman et al. “Thermometer Encoding: One Hot Way To Resist Adversarial
Examples”. In: International Conference on Learning Representations. 2018. url:
https://openreview.net/forum?id=S18Su--CW.

[5] Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural

Networks. 2017. arXiv: 1608.04644 [cs.CR].

[6] Nicholas Carlini and David Wagner. “Towards evaluating the robustness of neural

networks”. In: IEEE symposium on security and privacy (S&P). 2017, pp. 39–57.

[7] Nicholas Carlini and David A. Wagner. “Adversarial Examples Are Not Easily
Detected: Bypassing Ten Detection Methods”. In: CoRR abs/1705.07263 (2017).
arXiv: 1705.07263. url: http://arxiv.org/abs/1705.07263.

[8] Fabio Carrara et al. “Adversarial Examples Detection in Features Distance Spaces:
Subvolume B”. In: Jan. 2019, pp. 313–327. isbn: 978-3-662-53906-4. doi: 10.1007/
978-3-030-11012-3_26.

[9] Jia Deng et al. “ImageNet: A large-scale hierarchical image database”. In: 2009
IEEE Conference on Computer Vision and Pattern Recognition. 2009, pp. 248–255.
doi: 10.1109/CVPR.2009.5206848.

[10] Bradley Efron and Robert J. Tibshirani. An Introduction to the Bootstrap. Mono-
graphs on Statistics and Applied Probability 57. Boca Raton, Florida, USA: Chap-
man & Hall/CRC, 1993.

[11] Reuben Feinman et al. “Detecting adversarial samples from artifacts”. In: arXiv

preprint arXiv:1703.00410 (2017).

44

[12] Gil Fidel, Ron Bitton, and Asaf Shabtai. “When explainability meets adversarial
learning: Detecting adversarial examples using shap signatures”. In: International
Joint Conference on Neural Networks (IJCNN). 2020, pp. 1–8.

[13] Ruize Gao et al. “Maximum Mean Discrepancy Test is Aware of Adversarial At-

tacks”. In: International Conference on Machine Learning. 2021, pp. 3564–3575.

[14] Jacob R. Gardner et al. Deep Manifold Traversal: Changing Labels with Convolu-

tional Features. 2016. arXiv: 1511.06421 [cs.LG].

[15] Zhitao Gong, Wenlu Wang, and Wei-Shinn Ku. “Adversarial and clean data are not

twins”. In: arXiv preprint arXiv:1704.04960 (2017).

[16]

[17]

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. “Explaining and har-
nessing adversarial examples”. In: arXiv preprint arXiv:1412.6572 (2014).

Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Har-
nessing Adversarial Examples. 2015. arXiv: 1412.6572 [stat.ML].

[18] Chuan Guo et al. “Countering Adversarial Images using Input Transformations”.
In: International Conference on Learning Representations. 2018. url: https : / /
openreview.net/forum?id=SyJ7ClWCb.

[19] Kaiming He et al. “Deep Residual Learning for Image Recognition”. In: arXiv

preprint arXiv:1512.03385 (2015).

[20] Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting Misclassiﬁed and Out-

of-Distribution Examples in Neural Networks. 2018. arXiv: 1610.02136 [cs.NE].

[21] Dan Hendrycks and Kevin Gimpel. Early Methods for Detecting Adversarial Images.

2017. arXiv: 1608.00530 [cs.LG].

[22] Dan Hendrycks and Kevin Gimpel. “Early methods for detecting adversarial im-

ages”. In: arXiv preprint arXiv:1608.00530 (2016).

[23] Hossein Hosseini et al. Blocking Transferability of Adversarial Examples in Black-

Box Learning Systems. 2017. arXiv: 1703.04318 [cs.LG].

[24] Uiwon Hwang et al. PuVAE: A Variational Autoencoder to Purify Adversarial Ex-

amples. 2019. arXiv: 1903.00585 [cs.LG].

[25] Yerlan Idelbayev. Proper ResNet Implementation for CIFAR10/CIFAR100 in Py-

Torch.

[26] Susmit Jha et al. “Detecting adversarial examples using data manifolds”. In: IEEE

Military Communications Conference (MILCOM). 2018, pp. 547–552.

[27] Alex Krizhevsky. Learning multiple layers of features from tiny images. Tech. rep.

University of Toronto, 2009.

[28] A. Kurakin, I. Goodfellow, and Samy Bengio. “Adversarial Machine Learning at

Scale”. In: arXiv preprint arXiv:1611.01236 (2017).

45

[29] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial Machine Learning

at Scale. 2017. arXiv: 1611.01236 [cs.CV].

[30] Samuli Laine et al. “High-Quality Self-Supervised Deep Image Denoising”. In: NeurIPS.

2019.

[31] Yann LeCun, Corinna Cortes, and CJ Burges. “MNIST handwritten digit database”.
In: ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist 2 (2010).

[32] Xin Li and Fuxin Li. “Adversarial examples detection in deep networks with convo-
lutional ﬁlter statistics”. In: Proceedings of the IEEE International Conference on
Computer Vision. 2017, pp. 5764–5772.

[33] Yaxin Li et al. DeepRobust: A PyTorch Library for Adversarial Attacks and De-

fenses. 2020. arXiv: 2005.06149 [cs.LG].

[34] Laurens van der Maaten and Geoﬀrey Hinton. “Visualizing Data using t-SNE”.
In: Journal of Machine Learning Research 9.86 (2008), pp. 2579–2605. url: http:
//jmlr.org/papers/v9/vandermaaten08a.html.

[35] Dongyu Meng and Hao Chen. MagNet: a Two-Pronged Defense against Adversarial

Examples. 2017. arXiv: 1705.09064 [cs.CR].

[36] Jan Hendrik Metzen et al. On Detecting Adversarial Perturbations. 2017. arXiv:

1702.04267 [stat.ML].

[37] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: a
simple and accurate method to fool deep neural networks. 2016. arXiv: 1511.04599
[cs.LG].

[38] Tianyu Pang et al. Towards Robust Detection of Adversarial Examples. 2018. arXiv:

1706.00633 [cs.LG].

[39] Nicolas Papernot et al. Practical Black-Box Attacks against Machine Learning. 2017.

arXiv: 1602.02697 [cs.CR].

[40] Nicolas Papernot et al. The Limitations of Deep Learning in Adversarial Settings.

2015. arXiv: 1511.07528 [cs.CR].

[41] Edward Raﬀ et al. “Barrage of Random Transforms for Adversarially Robust De-
fense”. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion (CVPR). 2019, pp. 6521–6530. doi: 10.1109/CVPR.2019.00669.

[42] Olga Russakovsky et al. “ImageNet Large Scale Visual Recognition Challenge”. In:

International Journal of Computer Vision 115 (2015), pp. 211–252.

[43] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting
Classiﬁers Against Adversarial Attacks Using Generative Models. 2018. arXiv: 1805.
06605 [cs.CV].

[44] Yang Song et al. “Pixeldefend: Leveraging generative models to understand and
defend against adversarial examples”. In: arXiv preprint arXiv:1710.10766 (2017).

46

[45] Weilin Xu, David Evans, and Yanjun Qi. “Feature Squeezing: Detecting Adversarial
Examples in Deep Neural Networks”. In: Proceedings 2018 Network and Distributed
System Security Symposium (2018). doi: 10.14722/ndss.2018.23198. url: http:
//dx.doi.org/10.14722/ndss.2018.23198.

[46] Weilin Xu, David Evans, and Yanjun Qi. “Feature squeezing: Detecting adversarial

examples in deep neural networks”. In: arXiv preprint arXiv:1704.01155 (2017).

47

