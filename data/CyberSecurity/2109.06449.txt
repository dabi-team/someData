1
2
0
2

p
e
S
4
1

]
I

A
.
s
c
[

1
v
9
4
4
6
0
.
9
0
1
2
:
v
i
X
r
a

IJCAI-21 1st International Workshop on Adaptive Cyber Defense

Deep hierarchical reinforcement agents for automated penetration testing
Khuong Tran1 , Ashlesha Akella1 , Maxwell Standen2 , Junae Kim2 , David Bowman2 , Toby
Richer2 , Chin-Teng Lin1
1University of Technology Sydney, Australia
2Defence Science and Technology Group, Australia
{Khuong.Tran | Ashlesha.Akella | Chin-Teng.Lin}@uts.edu.au, {Max.Standen | Junae.Kim |
David.Bowman | Toby.Richer}@dst.defence.gov.au

Abstract

Penetration testing – the organised attack of a com-
puter system in order to test existing defences – has
been used extensively to evaluate network security.
This is a time consuming process and requires in-
depth knowledge for the establishment of a strat-
egy that resembles a real cyber-attack. This paper
presents a novel deep reinforcement learning archi-
tecture with hierarchically structured agents called
HA-DRL, which employs an algebraic action de-
composition strategy to address the large discrete
action space of an autonomous penetration testing
simulator where the number of actions is expo-
nentially increased with the complexity of the de-
signed cybersecurity network. The proposed ar-
chitecture is shown to ﬁnd the optimal attacking
policy faster and more stably than a conventional
deep Q-learning agent which is commonly used as
a method to apply artiﬁcial intelligence in auto-
matic penetration testing.

1 Introduction

The development of effective autonomous defenses requires
sophisticated attacks to train against. Penetration Testing
(PT) has been used extensively to evaluate the security of
ICT systems. This is a time-consuming process and requires
in-depth knowledge for the establishment of a strategy that
resembles a real cyber-attack. Available automatic penetra-
tion testing tools cannot learn to develop new strategies in re-
sponse to simple defensive measures. As a result, automatic
penetration testing tools that use artiﬁcial intelligence are de-
sirable for simulating real attackers with an attacking strategy
instead of testing all possible values when hacking into a sys-
tem.

Reinforcement Learning (RL) is a paradigm of learning in
which an autonomous decision-making agent explores and
exploits the environmental dynamics and discovers a suitable
policy for acting in such environments. This makes RL a suit-
able candidate for tackling the problem of automating the PT
process. Deep reinforcement learning (DRL) with function

approximators as neural networks have achieved many state
of the art results on a variety of platforms and applications,
as demonstrated by [Mnih et al., 2015] and [Lillicrap et
al., 2015]. Value-based methods such as variants of deep Q-
learning (DQN) algorithms [Watkins and Dayan, 1992] have
become the standard in many approaches thanks to their sim-
plicity and effectiveness in learning from experienced trajec-
tory. However, one of the limitations of applying DQN in
practical applications is the complexity of action space in dif-
ferent problem domains. DQN requires an evaluation for all
actions at the output and uses a maximization operation on
the entire action space to select the best action to take in ev-
ery step. This can be problematic if the action space is large
as multiple actions may share similar values [Farquhar et al.,
2019]. PT is one such practical application where the action
space scales exponentially with the number of host computers
in different sub-networks. Applying conventional DRL to au-
tomate PT would be difﬁcult and unstable as the action space
can explode to thousands even for relatively small scenarios
[Schwartz and Kurniawati, 2019].

There are a number of approaches to address the in-
tractability problem of a large discrete action space, such as
work done by [Tavakoli et al., 2018] and [Dulac-Arnold et
al., 2015] where the action space is hierarchically structured
into branches or prior domain knowledge is applied to embed
the action space respectively. This study aims at introducing
a new action decomposition scheme which is efﬁcient and
versatile in developing PT attacking strategy in different net-
work conﬁgurations. The proposed method is inspired by the
paradigm of multi-agent RL to decompose the action space
into smaller sets and train individual RL agents in each of the
action subsets.

We begin by giving a brief overview of the available liter-
ature on the problem, followed by the background and nota-
tions. A detailed description of our proposed method is then
provided together with empirical results which demonstrate
the performance of our algorithm. We use CybORG which is
an autonomous cybersecurity simulator platform developed
by [Baillie et al., 2020] to empirically validate the learning
and convergence properties of our architecture.

 
 
 
 
 
 
2 Related Work

RL has been used recently as an AI method to model
and develop exploit attack to cybersecurity setting as done
[Schwartz and Kurniawati, 2019],
[Hu et al., 2020] and
[Zennaro and Erdodi, 2020]. In these works, PT is formulated
as a partially observable markov decision process (POMDP)
where RL or DRL is used to model the environment and learn
exploit strategy. The attacker agent only observes the com-
promised hosts and/or subnets but not the entire network con-
nections. The agent then needs to learn from trial-and-error
experiences in order to develop suitable policy or strategy to
gain access to hidden assets. These previous studies have
shown tabular RL or DRL to be a possible method to solve
small scale networks or capture the ﬂag challenges which has
relatively small action spaces. Extending DRL to networks
with a larger action space is still a standing challenge not only
in cybersecurity setting but also in RL context [Schwartz and
Kurniawati, 2019].

[Dulac-Arnold et al., 2015] proposed an architecture,
named Wolpertinger, that leverages the actor-critic frame-
work of RL. This work attempted to learn a policy in a large
action space problem with sub-linear complexity. It uses prior
domain knowledge to embed the action space and applies
nearest-neighbor method to cluster similar actions to narrow
down the most probable action selection. This method can
be unstable in a sparse reward environment as the gradient
cannot propagate back to enhance the learning of the actor
network.

Another study done by [Zahavy et al., 2018] suggested the
use of an elimination signal to teach the agents to ignore irrel-
evant actions to reduce the number of possible actions from
which to choose. A rule-based system calculates the action
elimination signal by evaluating actions and assigning nega-
tive points to irrelevant actions. Then, the signal is sent to the
agent as part of an auxiliary reward signal to help the agent
learn better. This framework is best suited to domains where
a rule-based component can easily be built to embed expert
knowledge and facilitate the agent’s learning by reducing the
size of the action set.

[Tavakoli et al., 2018] incorporates different network
branches to handle different action dimensions. A growing
action space is trained using curriculum learning to avoid
[Chan-
overwhelming the agents [Farquhar et al., 2019].
dak et al., 2019] focused on learning action representation so
the agent can infer similar actions using a single representa-
tion while work done by [de Wiele et al., 2020] targeted at
learning action distributions. All these approaches aimed at
ﬁnding a way to reduce the large number of actions. However
they cannot be applied to solve automating PT for the follow-
ing reasons. Firstly, the action space in PT is entirely discrete
without continuous parameters. This is different with other
works where the action space is parameterised and comprised
of few major actions within which contain different continu-
ous parameters. Secondly, each action in PT can have very
different effects such as attacking hosts in different subnets
or different method of exploits. This nature is again different
with large discretised action space, where group of actions
can have spatial or temporal correlation , which is addressed

in [Tavakoli et al., 2018], [de Wiele et al., 2020] or [Dulac-
Arnold et al., 2015].

To the authors’ best knowledge, this work is the ﬁrst to
propose a non-conventional DRL architecture to directly tar-
get the large action space problem in PT settings. An explicit
action decomposition scheme using an hierarchical agents re-
inforcement learning approach (where the agents are grouped
hierarchically and each agent is trained to learn within its ac-
tion subset using the same external reward signal) is used.
This proposed model-free methodology requires minimum
domain knowledge to decompose the set of actions and is
proven to be efﬁcient in scaling for more complex problems.
The solution also takes advantage of value based reinforce-
ment learning to reduce the sample complexity and it can be
modiﬁed to extend to other problem domains in which the
action space can be ﬂattened into a large discrete set.

3 Problem description

The problem to be considered in this paper is a discrete
time reinforcement learning task modelled by an MDP 1, for-
malised by a tuple (cid:104)S, A, P, R, γ(cid:105). At each time step t, the
agent receives a state observation st ∈ S from the environ-
ment. In our PT setting, this represents the observable part
of the compromised network. To interact with the environ-
ment, the agent executes an action at ∈ A, after which it
receives another state st+1 according to the environment tran-
sition function P : S × A × S → R, and a reward signal rt
given by R : S × A → R. The agent aims to maximise a util-
ity, deﬁned as the total sum of all reward gained in an episode
Rt = (cid:80)∞
τ =t γτ −trτ , where γ ∈ [0, 1] is the discount factor
used to determine the importance of long term rewards.

Value-based methods such as DQN algorithms learn a
(near) optimal policy π : S → A which maps the observed
states or observations to actions. The action value function as
deﬁned below tells each agent i the expected utility of choos-
ing the action ai
t given state st, and the expected utility for
that state:
Qπ,i(s, ai) = E[Rt|st = s, at = ai, πi]
= Es(cid:48)[r + γ E

)]|s, ai, πi]
(1)
By recursively solving Equation 1, the optimal action value
function deﬁned as Q∗(s, a) = maxπ Qπ(s, a) can be at-
tained by taking the action with highest Q-value in the next
state. This solution is shown in Equation 2 and this is the main
equation of the popular Deep Q-Learning (DQN) algorithm.

ai(cid:48) ∼πi(s(cid:48))[Qπ,i(s(cid:48), ai(cid:48)

Qi∗(s, ai) = Es(cid:48)[r + γ max
ai(cid:48)

Qi∗(s(cid:48), ai(cid:48)

)|s, ai]

(2)

In non-trivial problems, Q values are often approximated
by parameterised functions such as neural networks (e.g.
Q(s, a; θ)). If the action space |A| is large, DQN becomes

1We do not want to formulate PT as POMDP similar to other
works as to clearly demonstrate the effect of the proposed architec-
ture without compounding factor, our method can still be applied as
a feature to POMDP solution. Additionally, none of previous works
uses any POMDP speciﬁc solutions

unstable as there are many different actions with similar Q-
values and it is not clear which action should be taken. As
a result, the performance of DQN and its variants degrade
[Dulac-Arnold et al., 2015].
as the action space increases
The action space in reinforcement learning has different for-
mats and representation depending on the type of problems.
Mainly they are classiﬁed into two main categories: contin-
uous and discrete. In a discrete action problem, the action
space can be organised to have a ﬂat action space or param-
eterised into an hierarchically structured action space where
there are a few types of main actions under which there are
sub-actions [Masson and Konidaris, 2015]. It is harder, how-
ever, to learn in a parameterized action space as the agent
needs to learn two Q-functions or policies, one for choosing
the main action and another to choose the sub-action or the
parameters of each action. On the other hand, converting the
hierarchically structured action space into a ﬂat representa-
tion could result in a substantially larger action size which
also hinders the learning of DQN agents. This research pro-
poses a multi-agent learning framework where the agents are
grouped into an hierarchical structure to tackle this particu-
lar instance of a large ﬂat discrete action space, or a ﬂattened
parameterised action space.

In a ﬂat and discrete setting,

the action space is en-
coded into integer identiﬁers ranging from LOW IN T to
M AX IN T , for instance from 0 to 999 in an action space
of 1000 actions. Each number is translated into a valid ac-
tion that can be understood and executed by the simulator. In
our work, we do not assume any prior knowledge over the se-
mantics or relationships among these individual actions. The
main idea of the proposed solution is to decompose the over-
all action space A into smaller sets A1, A2, . . . , AL, where
|A|i << |A|. If these sets are combined together using pre-
deﬁned linear functions (e.g f i+1 : Ai×Ai+1 → Ai+1
out ), they
can produce a strictly increasing sets of action representation:
out| < |A2
|A1
out = A. Each
Ai
out is a set of action representation constructed up to level
i. The sets Ai and Ai+1 can be considered as the supports
of the set Ai+1
out . Intuitively, the integer identiﬁers of the ﬁnal
actions which can be large in value are built up algebraically
using smaller integer values. The growing numerical repre-
sentation of the action space is illustrated in Figure 1.

out| < · · · < ... < |AL

out| with AL

4 Hierarchical-Agent Action Decomposition
4.1 Action decomposition scheme
The main contribution of this paper is to propose an action de-
composition strategy to reduce the large discrete action space
into manageable sets for deep reinforcement learning agents,
which stabilises the learning and enhances convergence prop-
erties.

As mentioned in the previous section, the large action
space is decomposed into L sub-action spaces.
Instead of
using different neural networks’ heads to manage these ac-
tion subsets as proposed by [Tavakoli et al., 2018], sep-
arate DQN agents are used to control each of these sub-
sets. The beneﬁts of this approach will be highlighted in the
next section. The agents at levels i and i + 1 output primi-
tive action signals ai and ai+1, each of which is within the

Figure 1: Action space composition

t

outt

outt

outt

, ai+1
t

) = ai

out = f i+1(ai

outt = f i+1(ai

ranges of [0, . . . , |Ai| − 1] and [0, . . . , |Ai+1| − 1] respec-
tively. These primitive actions do not interact directly with
the environment but they are used to build up the ﬁnal ac-
tion. Each agent from level 1 to level L can use its action
signal, together with the external reward Rt, to train its own
neural network using DQN updates as in Equation 2. The
primitive action values are then combined together using a
linear function to produce the action representation at level
× βi+1 + ai+1
, ai+1
i + 1: ai+1
,
t
where βi+1 is a pre-deﬁned value used to scale up the out-
) and βi+1 = |Ai+1|. To simplify
put of f i+1(ai
notations, time step t will be removed to enhance equa-
tions readability, therefore: ai+1
out, ai+1) with
out = a1. For instance, we can decompose an action space
a1
of a given scenario with |A| = |AL
out| = 1000, using log-
arithm of the original action set with base 10, into L sets
out| = 3. A1, A2 and A3 all contain
where: L = log10 |AL
10 action identiﬁers from 0 to 9. When we combine together
A1 and A2 using the function f 2(a1
out × β2 + a2
with β2 = 10, A2
out will expand the range of actions it can
represent to be from 0 to 99 which has 100 actions in total.
Finally to arrive at the action space of 1000 actions, we apply
out × β3 + a3 and
the same procedure to A3 with a3
β3 = 10. As a result, A3
out contains action values from 0
to 999. In summary, after a few levels of function composi-
tions, the agent uses the ﬁnal action, which is computed via
out = f L(aL−1
aL
out , aL), to interact with the environment. The
working mechanism of the proposed framework is depicted in
Figure 2. We name this method as Hierarchical Agent Deep
Reinforcement Learning or HA-DRL.

out, a2) = a1

out = a2

4.2 Hierarchical Agents Deep Reinforcement

Learning (HA-DRL)

HA-DRL extends the conventional MDP framework with a
single reinforcement learning agent to having a factored ac-
tion space handled by a group of DQN agents. This means
that the original action value function Q(s, a; θ) is decom-
posed into a combination of smaller action value functions
Q(s, ai; θi) where i is in the range of 1, . . . , L. This ap-

Figure 2: HA-DRL Architecture

Figure 3: Parallel training and execution

proach has three major advantages in facilitating learning and
enhancing the convergence property.

First of all, leveraging a system of hierarchical agents in
learning action subsets instead of having a single large DQN
agent with thousands of actions in the output improves the
use of the the learning signal to update different subsets of
θ. Each agent uses the same reward signal to minimise its
own loss function as in Equation 4 and 5. The parameterised
policy πi(ai|s; θi) is updated to produce optimal abstract ac-
tions. As long as each agent learns the optimal policy to
achieve highest possible long term reward, together with sub-
stantial exploration, the combined action or the overall policy
will converge towards optimal behavior. This reasoning is
formalised as follows:

Q∗(st, a; θ) = Eπ[rt + γ max
a(cid:48)

Q∗(st+1, a

(cid:48)

)|st = s, at = a]

(3)

with a = f (a1, a2, . . . , aL).
The above optimal state-action value function is achieved
when the agents can maximise the future expected environ-
ment reward, which is the same goal for each of the agent in
the hierarchy. With enough exploration, the group of agents
can explore all possible combinations of individual actions to
achieve this global optimal policy. However with sparse re-
ward scenarios or extremely large action space, this condition
may not be practically feasible. Current empirical results in
tested scenarios showed this approach to result in faster train-
ing and better convergence properties than a single DQN with
combined neural network.

Secondly, the neural networks of HA-DRL can be con-
structed with a smaller neural network for each of the agents.
HA-DRL does not solve a complex problem by multiplica-
tively duplicating a single DQN agent into multiple agents. It
does so by breaking down the action space into small sets so
that each agent does not have to learn all the intricate features
of the state to acquire the overall optimal policy. As a result,
each agent in HA-DRL needs a smaller neural network which
is enough to map relatively abstract state features to action
representations. Additionally the required number of agents
is not large even if we have thousands of actions as we will
discuss in the following section. This justiﬁes the concerns
over network complexity of having multi-agent learning.

Lastly, the training and inference of all agents do not have
to be sequential in nature. Only the application of the lin-
ear function composition has to be constructed sequentially
which is almost a constant operation with respect to the in-
crease in complexity of the action space. With appropriate
conﬁgurations of multi-agent DQN training and the use of
parallel programming frameworks such as Pytorch, all agents
can be trained and used for inference in parallel which pro-
duces no extra cost compared to a single DQN training. This
is also true for the utilisation of a replay buffer where a single
replay buffer is used for all agents since the tuple of informa-
tion is mostly the same for all agents, except for the outputted
action of each agent (Figure 3).

The learning of all agents in this paper follows the Dueling-
DQN (DDQN) architecture as introduced by [Wang et al.,
2015] where θi is updated by optimizing the loss:

Li(θi) = E

st,ai

t,rt,s(cid:48)
t

[(yi,DDQN − Qi(st, ai

t; θi)2]

(4)

with

yi,DDQN = rt + γ max
a(cid:48)
where θ−i is the parameter of the target network which is

Q(s(cid:48), a

(5)

)

(cid:48) i

; θ−i

mentioned in the original DQN paper [Mnih et al., 2015].

Interpretability of action decomposition
4.3
The idea of decomposing a large action space into smaller sets
is similar to having a tree structure of multiple layers of ac-
tions. Each node in the tree acts like an action selection node
where it picks the corresponding action from its child nodes.
Each action node has its own range of integer identiﬁers. In
order to reach a leaf node with a speciﬁc action value, the par-
ent node needs to pick the right children to reach a speciﬁed
leaf. The Figure 4, which is an adaptation from a structured
parameterised action space in [Fan et al., 2019], illustrates the
idea of having different layers of action selection.

Instead of having a different number of agents in each level
to represent ranges of integer values, we leverage the linear
algebraic function to shift the values of action identiﬁers into
appropriate ranges. The function ai+1
out, ai+1) =
ai
out × βi+1 + ai+1 where the coefﬁcients βi+1 represent the

out = f (ai

number of action selection nodes or agents at level i + 1. The
value of βi+1 shifts the output range of the integer values of
the actions at a particular level. This results in having only
one agent per level as shown in Figure 2. This action de-
composition scheme shows that instead of having a single RL
agent explore and learn an entire action space of |A| which
can be extremely large, a few smaller RL agents can be used
to explore and learn on a much smaller version of the action
space of its own. The outputs of these smaller agents are then
chained together to produce the ﬁnal action. The complexity
of having multi-agents on different levels is similar to travers-
ing a tree in computer science. The number of levels L is in
the order of L ≈ log |A|. As a result, it requires only a hand-
ful of agents for an action space even with millions of actions.

4.4 Optimality of action value function
This subsection discusses the optimality of the learned policy
under this action decomposition scheme. The optimal action
value function Q∗(s, a; θ) is approximated by the optimal ac-
tion value function of each agent i with Q∗i(s, ai; θi). As the
ﬁnal action is actually the combination of all previous action
values a = f (a1, a2, . . . , aL), the optimal action value func-
tion is related to the combination of individual optimal action
value function via a function g(.):

Q∗(s, a; θ) = Q∗(s, f (a1, . . . , aL); θ1, . . . , θL)

= g(Q∗i(s, ai; θi))

(6)

As the action of one agent can have an effect on the re-
ward received by another agent, the global optimal policy is
not guaranteed in complex scenarios or with limited and ﬁnite
exploration. Current works are being done to implement co-
ordinator module such as QMIX proposed by [Rashid et al.,
2018] to approximate the function g(.).

However for medium-sized scenarios with frequent reward
signals, the proposed architecture can handle the learning bet-
ter than individual DRL agent.

5 Experiment Setup
5.1 CybORG simulator
We use an autonomous penetration testing simulator as our
testbed for validating the performance of HA-DRL. [Baillie
et al., 2020] developed CybORG to provide an experimen-
tal environment for conducting AI research in a cybersecurity
context and it is designed to enable an autonomous agent to
conduct a penetration test against a network. Apart from be-
ing open source and having the appropriate OpenAI gym in-
terface for applying different DRL algorithms, this environ-
ment represents a real application where we are faced with
different aspects of complexity: sparsity of reward signals,
large discrete action space and discrete state representation
mean we cannot rely on any computer vision techniques to
facilitate the learning. We summarize the details of the exe-
cution and complexity of CybORG here.

The simulation provides an interface for the agent to inter-
act with the network and provides a vector to the agent that
is based on the observed state of the network. Through this

Figure 4: Hierarchical Action Selection

interface, the agent performs actions that reveal information
about the simulated network and successfully penetrates the
network by capturing the ﬂags on the hosts within it.

The simulation can implement various scenarios. These
scenarios detail the topology of the target network, the loca-
tions of the ﬂags, and host speciﬁc information, such as the
operating system and running services. The target network
features a series of subnets that contain hosts. The hosts in
a subnet may only act upon hosts in the same subnet or ad-
jacent subnets. An illustrative representation of a possible
scenario conﬁguration with 24 hosts and one attacker agent is
presented in Figure 5. In this example scenario, there are two
hidden ﬂags in one of the hosts in subnet 5 and in one of the
hosts in subnet 7. There are 3 hosts in each of the subnets.
The scenarios differ in terms of the number of hidden ﬂags
and the locations of the hosts that contain the ﬂags, along
with the number of hosts, which determines the complexity
of the state and action spaces.

In the 24-host example, 550 actions are available to the
attacker agent. The simulation includes three action types:
host-to-host actions, host-to-subnet actions, and on-host ac-
tions. Each action may be performed on a host, and each host-
to-host action or host-to-subnet action may target another host
or subnet, respectively. These actions include the following:

• actions that reveal information about a host’s operating

system (OS),

• actions that reveal network information that is contained

on a host,

• actions that reveal information about other hosts in the
network using scans with an internet control message
protocol (ICMP) ping,

• actions that reveal information about the services on a
host with a transport control protocol (TCP) SYN sweep,

• actions that provide information about the network that

is obtained via passive observation of trafﬁc,

• actions that provide remote code execution (RCE) from
the exploitation of a vulnerable service with brute-force
guessing of SSH credentials. The agent can use RCE on
a host to pivot to other hosts.

of up to 5000 actions. Due to the complexity of CybORG
simulator, the algorithm’s performance was demonstrated on
a maximum complexity of 100 hosts which has the action
space of 4646 actions. All the training runs are conducted
using a single RTX6000 GPU. The implementation can be
shared upon requests from interested readers.

6 Results
6.1 Policy training
Each performance in Figure 6 was repeated ﬁve times with
different random seeds to ensure reproducibility. The shaded
regions on the graphs represent the variability between runs.
There are two indicators we are looking at after training the
agents: the maximum score the agents can get which is on the
left panel and the number of steps to reach the target shown
on the right panel. For each scenario, the maximum score
the agent can receive is approximately 20 points (minus some
small negative rewards on invalid action it takes to reach the
assets).

We have tested the HA-DRL algorithm on a variety of sce-
narios with different conﬁgurations of hosts and action space
as shown in Table 1. Regarding scenario complexity, with
the number of hosts varied from 6 to 100, the action space
increases from having 49 to 4646 actions. However there
is only an addition of having 2 more agents to train. In all
the tested scenarios, HA-DRL convergence results were ei-
ther similar to or superior to the DDQN agent, depending on
the complexity of the scenario and the size of action space.

Figure 6 presents the algorithms’ performance on four no-
table scenarios where the complexity is signiﬁcant enough
to showcase the superiority of the algorithm while not too
computationally expensive for repeated training. In scenar-
ios where both DDQN and HA-DRL were able to learn the
optimal policy, HA-DRL showed faster and stabler conver-
gence than DDQN. DDQN’s performance on scenarios with
60 and 70 hosts were unstable as it only successfully learned
the optimal policy in 1 out of 4 runs. HA-DRL held up its per-
formance on the scenario of 100 hosts where DDQN failed to
achieve any progress during training.

For the 100 hosts scenario, DDQN was not able to explore
the action space and learn the policy at all while HA-DRL
took approximately 4000 episodes to pick up the learning and
start to converge to the optimal policy. The policy learned by
HA-DRL in each of the tested scenarios is optimal in terms
of having minimum number of taken actions.

This is also the ﬁrst demonstration of applying deep rein-
forcement learning in an automatic penetration testing sce-
nario that can handle such a large action space [Ghanem and
Chen, 2020].

6.2 Discussion
To examine the learning of each agent in HA-DRL, we look
into the state and action representation that the trained agents
have learned and visualise the state representation using t-
SNE method [Van der Maaten and Hinton, 2008]. Figure 7
shows the state representations the agents have learned which
are quite similar across the three agents in the architecture
used for the 50-hosts scenario. Surprisingly, the learned state

Figure 5: An example of 24 hosts scenario with 1 public subset and
7 private subnets where ﬂags represent valuable assets are hidden in
private subnet 5 and 7

For a speciﬁed scenario, p hosts are grouped into q sub-
nets. When the simulation supports m types of host-to-host
actions, n types of host-to-subnet actions and o types of
of host actions, the action space for host-to-host actions is
m × p × (p − 1), the action space for host-to-subnet actions
is n × p × q, and the action space for on-host actions is o × p.
This leads to a large potential action space even in a small
network.

The success of actions depends both on the state of the sim-
ulation and the probability. The state of the simulation con-
sists of various information, such as the operating system on
each host and the services that are listening on each host. The
agent sees an unknown value for each value it does not know.
As the agent takes actions in the simulation, its awareness of
the true state of the simulation increases. An action fails if the
agent does not have remote access to a target host or subnet.
However, an action also has a probability of failing even if the
agent has access to the target. This probability represents that
attempts to exploit vulnerabilities are seldom if ever 100%
reliable.

The simulation was deliberately conﬁgured to pose a
highly sparse environment. The agent began the game with
no knowledge of the ﬂag location so ﬁnding the host with
the ﬂag or a hidden asset was completely random. A ﬁnal
high reward is given for capturing the ﬂag while a small pos-
itive reward is given for every host that was successfully at-
tacked. The ﬂag was hidden from the state space; hence, the
agent could not determine the location of the ﬂag by observ-
ing the state vector. This simulation differs signiﬁcantly from
the typical RL environments, in which either the objective or
target is visible to the agent.

5.2 Neural network architecture
Each agent in our experiments uses a 3-layer Dueling DQN
network architecture. However HA-DRL can be used with
any variants of DQN algorithms such as prioritised experi-
ence replay, double DQN etc.. The performance of HA-DRL
is compared against a single Dueling DQN agent. The ﬁrst
two layers have 2048 neurons to evaluate the advantage func-
tions while the last layer has 512 neurons to compute the
value functions. The action space of individual DQN agents
in HA-DRL is limited to be in the range of 8 to 10. As a
result, there are at most 4 DQN agents created for scenarios

Hosts

State space Action space Agents

6
9
12
18
24
50
60
70
100

39
55
71
217
285
573
685
797
1133

49
120
182
342
550
1326
1830
2414
4646

2
2
2
2
2
3
4
4
4

Table 1: Conﬁgurations of tested scenarios

Figure 6: Results of HA-DRL and single DDQN on different Cy-
bORG scenarios

representations are mapped into 9 separate clusters which
match the 9 private subnets in the 50-hosts conﬁguration,
which itself has 1 public subnet and 9 private subnets each
with 5 hosts, even though this knowledge is not presented or
observable to the agents. The colored marker represents the
identiﬁer of the action each agent would take in certain states.
The action space of 1326 is mapped into smaller subsets of
10 to 15 actions per agent. After training, each agent learns
that only 2 to 3 actions in its own subsets are needed to opti-
mally capture the hidden assets. This visualisation opens up
new research direction where further look into state and ac-
tion representation can yield better progress in applying DRL
to even more complex PT scenarios.

7 Conclusions
This paper introduced a new hierarchical agent reinforce-
ment learning architecture called HA-DRL to tackle PT sce-
narios with large discrete action space. The proposed algo-
rithm requires minimum prior knowledge on the problem do-
main while providing competitively better performance than
conventional DQN agents. We validate the algorithms on
simulated scenarios from CybORG. In all tested scenarios,
HA-DRL showed superior performance to the single DDQN
agent.

The proposed algorithm is also scalable into larger ac-
tion spaces with sub-linear increase in network computa-
tional complexity. This is because the individual networks

Figure 7: State representation of 50-hosts scenario

are trained and used for inference separately, and their out-
comes are used sequentially to compute the ﬁnal action. De-
spite HA-DRL’s promising results, various challenges remain
to be resolved. Future works will target extending the HA-
DRL into incorporating subgoals learning in order to perform
better in environments where rewards are much sparser. Ad-
ditionally efﬁcient exploration in large action spaces is still a
standing problem that deserves further research.

Acknowledgements

This work was supported by the Australian Defence Sci-
ence Technology Group (DSTG) under Agreement No: MyIP
10699.

References
[Baillie et al., 2020] Callum Baillie, Maxwell Standen,
Jonathon Schwartz, Michael Docking, David Bowman,
and Junae Kim. Cyborg: An autonomous cyber operations
research gym. arXiv preprint arXiv:2002.10667, 2020.

[Chandak et al., 2019] Yash

Georgios
Chandak,
James Kostas, Scott M. Jordan, and
Theocharous,
Philip S. Thomas. Learning action representations for
reinforcement learning. CoRR, abs/1902.00183, 2019.
[de Wiele et al., 2020] Tom Van de Wiele, David Warde-
Farley, Andriy Mnih, and Volodymyr Mnih. Q-learning in
enormous action spaces via amortized approximate maxi-
mization, 2020.

[Dulac-Arnold et al., 2015] Gabriel Dulac-Arnold, Richard
Evans, Hado van Hasselt, Peter Sunehag, Timothy Lilli-
crap, Jonathan Hunt, Timothy Mann, Theophane Weber,
Thomas Degris, and Ben Coppin. Deep reinforcement
learning in large discrete action spaces. arXiv preprint
arXiv:1512.07679, 2015.

[Fan et al., 2019] Zhou Fan, Rui Su, Weinan Zhang, and
Yong Yu. Hybrid actor-critic reinforcement learning in pa-
rameterized action space. CoRR, abs/1903.01344, 2019.
[Farquhar et al., 2019] Gregory Farquhar, Laura Gustafson,
Zeming Lin, Shimon Whiteson, Nicolas Usunier, and
CoRR,
Gabriel Synnaeve.
abs/1906.12266, 2019.

Growing action spaces.

[Ghanem and Chen, 2020] Mohamed C. Ghanem and
Thomas M. Chen. Reinforcement learning for efﬁcient
network penetration testing, 2020.

[Hu et al., 2020] Zhenguo Hu, Razvan Beuran, and Yasuo
Tan. Automated penetration testing using deep reinforce-
ment learning. In 2020 IEEE European Symposium on Se-
curity and Privacy Workshops (EuroS&PW), pages 2–10.
IEEE, 2020.

[Lillicrap et al., 2015] Timothy P Lillicrap, Jonathan J Hunt,
Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
Continuous con-
David Silver, and Daan Wierstra.
arXiv preprint
trol with deep reinforcement learning.
arXiv:1509.02971, 2015.

Masson

[Masson and Konidaris, 2015] Warwick

and
George Dimitri Konidaris. Reinforcement learning with
parameterized actions. CoRR, abs/1509.01644, 2015.
[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu,
David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529,
2015.

[Rashid et al., 2018] Tabish Rashid, Mikayel Samvelyan,
Christian Schr¨oder de Witt, Gregory Farquhar, Jakob N.
Foerster, and Shimon Whiteson. QMIX: monotonic value
function factorisation for deep multi-agent reinforcement
learning. CoRR, abs/1803.11485, 2018.

[Schwartz and Kurniawati, 2019] Jonathon Schwartz

and
Hanna Kurniawati. Autonomous penetration testing using
reinforcement learning, 2019.

[Tavakoli et al., 2018] Arash Tavakoli, Fabio Pardo, and
Petar Kormushev. Action branching architectures for deep
In Thirty-Second AAAI Confer-
reinforcement learning.
ence on Artiﬁcial Intelligence, 2018.

[Van der Maaten and Hinton, 2008] Laurens Van der Maaten
and Geoffrey Hinton. Visualizing data using t-sne. Journal
of machine learning research, 9(11), 2008.

[Wang et al., 2015] Ziyu Wang, Tom Schaul, Matteo Hes-
sel, Hado Van Hasselt, Marc Lanctot, and Nando De Fre-
itas. Dueling network architectures for deep reinforcement
learning. arXiv preprint arXiv:1511.06581, 2015.

[Watkins and Dayan, 1992] Christopher JCH Watkins and
Peter Dayan. Q-learning. Machine learning, 8(3-4):279–
292, 1992.

[Zahavy et al., 2018] Tom Zahavy, Matan Haroush, Nadav
Merlis, Daniel J. Mankowitz, and Shie Mannor. Learn
what not to learn: Action elimination with deep reinforce-
ment learning. CoRR, abs/1809.02121, 2018.

[Zennaro and Erdodi, 2020] Fabio Massimo Zennaro and
Laszlo Erdodi. Modeling penetration testing with rein-
forcement learning using capture-the-ﬂag challenges and
arXiv preprint arXiv:2005.12632,
tabular q-learning.
2020.

