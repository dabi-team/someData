9
1
0
2

v
o
N
7

]

R
C
.
s
c
[

2
v
2
8
1
2
1
.
6
0
9
1
:
v
i
X
r
a

Adaptive Honeypot Engagement through Reinforcement
Learning of Semi-Markov Decision Processes

Linan Huang and Quanyan Zhu(cid:63)

Department of Electrical and Computer Engineering, New York University
2 MetroTech Center, Brooklyn, NY, 11201, USA
{lh2328,qz494}@nyu.edu

Abstract. A honeynet is a promising active cyber defense mechanism. It reveals
the fundamental Indicators of Compromise (IoCs) by luring attackers to con-
duct adversarial behaviors in a controlled and monitored environment. The active
interaction at the honeynet brings a high reward but also introduces high imple-
mentation costs and risks of adversarial honeynet exploitation. In this work, we
apply inﬁnite-horizon Semi-Markov Decision Process (SMDP) to characterize a
stochastic transition and sojourn time of attackers in the honeynet and quantify
the reward-risk trade-off. In particular, we design adaptive long-term engagement
policies shown to be risk-averse, cost-effective, and time-efﬁcient. Numerical re-
sults have demonstrated that our adaptive engagement policies can quickly attract
attackers to the target honeypot and engage them for a sufﬁciently long period to
obtain worthy threat information. Meanwhile, the penetration probability is kept
at a low level. The results show that the expected utility is robust against attackers
of a large range of persistence and intelligence. Finally, we apply reinforcement
learning to the SMDP to solve the curse of modeling. Under a prudent choice of
the learning rate and exploration policy, we achieve a quick and robust conver-
gence of the optimal policy and value.

Keywords: Reinforcement Learning · Semi-Markov Decision Processes · Active
Defense · Honeynet · Risk Quantiﬁcation

1

Introduction

Recent instances of WannaCry ransomware attack and Stuxnet malware have demon-
strated an inadequacy of traditional cybersecurity techniques such as the ﬁrewall and
intrusion detection systems. These passive defense mechanisms can detect low-level
Indicators of Compromise (IoCs) such as hash values, IP addresses, and domain names.
However, they can hardly disclose high-level indicators such as attack tools and Tactics,
Techniques and Procedures (TTPs) of the attacker, which induces the attacker fewer
pains to adapt to the defense mechanism, evade the indicators, and launch revised at-
tacks as shown in the pyramid of pain [2]. Since high-level indicators are more effective

(cid:63) This research is supported in part by NSF under grant ECCS-1847056, CNS-1544782, and

SES-1541164, and in part by ARO grant W911NF1910041.

 
 
 
 
 
 
2

L. Huang and Q. Zhu

in deterring emerging advanced attacks yet harder to acquire through the traditional pas-
sive mechanism, defenders need to adopt active defense paradigms to learn these funda-
mental characteristics of the attacker, attribute cyber attacks [35], and design defensive
countermeasures correspondingly.

Honeypots are one of the most frequently employed active defense techniques to
gather information on threats. A honeynet is a network of honeypots, which emulates
the real production system but has no production activities nor authorized services.
Thus, an interaction with a honeynet, e.g., unauthorized inbound connections to any
honeypot, directly reveals malicious activities. On the contrary, traditional passive tech-
niques such as ﬁrewall logs or IDS sensors have to separate attacks from a ton of legiti-
mate activities, thus provide much more false alarms and may still miss some unknown
attacks.

Besides a more effective identiﬁcation and denial of adversarial exploitation through
low-level indicators such as the inbound trafﬁc, a honeynet can also help defenders to
achieve the goal of identifying attackers’ TTPs under proper engagement actions. The
defender can interact with attackers and allow them to probe and perform in the hon-
eynet until she has learned the attacker’s fundamental characteristics. More services a
honeynet emulates, more activities an attacker is allowed to perform, and a higher de-
gree of interactions together result in a larger revelation probability of the attacker’s
TTPs. However, the additional services and reduced restrictions also bring extra risks.
Attacks may use some honeypots as pivot nodes to launch attackers against other pro-
duction systems [37].

Fig. 1: The honeynet in red mimics the targeted production system in green. The hon-
eynet shares the same structure as the production system yet has no authorized services.

Access PointInternet / CloudFirewallSwitchSwitchAccess PointInternet / CloudIntrusion DetectionHoneypot192.168.1.10HoneywallGatewayRouterServerHoneypot192.168.1.45Data BaseComputer NetworkServerWork Station192.168.1.55Data Base192.168.1.90HoneywallSensorActuatorHoneypotHoneypot NetworkHoneypotHoneypotHoneynetProduction SystemsAdaptive Honeypot Engagement

3

The current honeynet applies the honeywall as a gateway device to supervise out-
bound data and separate the honeynet from other production systems, as shown in Fig.
1. However, to avoid attackers’ identiﬁcation of the data control and the honeynet, a de-
fender cannot block all outbound trafﬁcs from the honeynet, which leads to a trade-off
between the rewards of learning high-level IoCs and the following three types of risks.

T1: Attackers identify the honeynet and thus either terminate on their own or generate

misleading interactions with honeypots.

T2: Attackers circumvent the honeywall to penetrate other production systems [34].
T3: Defender’s engagement costs outweigh the investigation reward.

We quantify risk T1 in Section 2.3, T2 in Section 2.5, and T3 in Section 2.4. In
particular, risk T3 brings the problem of timeliness and optimal decisions on timing.
Since a persistent trafﬁc generation to engage attackers is costly and the defender aims
to obtain timely threat information, the defender needs cost-effective policies to lure the
attacker quickly to the target honeypot and reduce attacker’s sojourn time in honeypots
of low-investigation value.

Fig. 2: Honeypots emulate different components of the production system.

To achieve the goal of long-term, cost-effective policies, we construct the Semi-
Markov Decision Process (SMDP) in Section 2 on the network shown in Fig. 2. Nodes
1 to 11 represent different types of honeypots, nodes 12 and 13 represent the domain of
the production system and the virtual absorbing state, respectively. The attacker transits
between these nodes according to the network topology in Fig. 1 and can remain at
different nodes for an arbitrary period of time. The defender can dynamically change
the honeypots’ engagement levels such as the amount of outbound trafﬁc, to affect the

ClientsServerSwitchNormal ZoneComputerNetworkEmulatedSensorsEmulatedDatabase12111012345679813Absorbing State4

L. Huang and Q. Zhu

attacker’s sojourn time, engagement rewards, and the probabilistic transition in that
honeypot.

In Section 3, we deﬁne security metrics related to our attacker engagement problem
and analyze the risk both theoretically and numerically. These metrics answer important
security questions in the honeypot engagement problem as follows. How likely will the
attacker visit the normal zone at a given time? How long can a defender engage the at-
tacker in a given honeypot before his ﬁrst visit to the normal zone? How attractive is the
honeynet if the attacker is initially in the normal zone? To protect against the Advanced
Persistent Threats (APTs), we further investigate the engagement performance against
attacks of different levels of persistence and intelligence.

Finally, for systems with a large number of governing random variables, it is of-
ten hard to characterize the exact attack model, which is referred to as the curse of
modeling. Hence, we apply reinforcement learning methods in Section 4 to learn the
attacker’s behaviors represented by the parameters of the SMDP. We visualize the con-
vergence of the optimal engagement policy and the optimal value in a video demo1.
In Section 4.1, we discuss challenges and future works of reinforcement learning in
the honeypot engagement scenario where the learning environment is non-cooperative,
risky, and sample scarce.

1.1 Related Works

Active defenses [23] and defensive deceptions [1] to detect and deter attacks have been
active research areas. Techniques such as honeynets [30,49], moving target defense
[48,17], obfuscation [31,32], and perturbations [44,45] have been introduced as defen-
sive mechanisms to secure the cyberspace. The authors in [11] and [16] design two
proactive defense schemes where the defender can manipulate the adversary’s belief
and take deceptive precautions under stealthy attacks, respectively. In particular, many
works [10,26] including ones with Markov Decision Process (MDP) models [22,30] and
game-theoretic models [40,20,41] focus on the adaptive honeypot deployment, conﬁg-
uration, and detection evasion to effectively gather threat information without the at-
tacker’s notice. A number of quantitative frameworks have been proposed to model
proactive defense for various attack-defense scenarios building on Stackelberg games
[31,25,46], signaling games [33,27,51,29,42], dynamic games [36,15,7,47], and mech-
anism design theory [5,43,9,50]. Pawlick et al. in [28] have provided a recent survey of
game-theoretic methods for defensive deception, which includes a taxonomy of decep-
tion mechanisms and an extensive literature of game-theoretic deception.

Most previous works on honeypots have focused on studying the attacker’s break-in
attempts yet pay less attention to engaging the attacker after a successful penetration so
that the attackers can thoroughly expose their post-compromise behaviors. Moreover,
few works have investigated timing issues and risk assessment during the honeypot en-
gagement, which may result in an improper engagement time and uncontrollable risks.
The work most related to this one is [30], which introduces a continuous-state inﬁnite-
horizon MDP model where the defender decides when to eject the attacker from the
network. The author assumes a maximum amount of information that a defender can

1 See the demo following URL: https://bit.ly/2QUz3Ok

Adaptive Honeypot Engagement

5

learn from each attack. The type of systems, i.e., either a normal system or a honey-
pot, determines the transition probability. Our framework, on the contrary, introduces
following additional distinct features:

– The upper bound on the amount of information which a defender can learn is hard
to obtain and may not even exist. Thus, we consider a discounted factor to penalize
the timeliness as well as the decreasing amount of unknown information as time
elapses.

– The transition probability not only depends on the type of systems but also depends

on the network topology and the defender’s actions.

– The defender endows attackers the freedom to explore the honeynet and affects the
transition probability and the duration time through different engagement actions.
– We use reinforcement learning methods to learn the parameter of the SMDP model.
Since our learning algorithm constantly updates the engagement policy based on the
up-to-date samples obtained from the honeypot interactions, the acquired optimal
policy adapts to the potential evolution of attackers’ behaviors.

SMDP generalizes MDP by considering the random sojourn time at each state, and
is widely applied to machine maintenance [4], resource allocation [21], infrastructure
protection [13,14,13], and cybersecurity [38]. This work aims to leverage the SMDP
framework to determine the optimal attacker engagement policy and to quantify the
trade-off between the value of the investigation and the risk.

1.2 Notations

Throughout the paper, we use calligraphic letter X to deﬁne a set. The upper case let-
ter X denotes a random variable and the lower case x represents its realization. The
boldface X denotes a vector or matrix and I denotes an identity matrix of a proper
dimension. Notation Pr represents the probability measure and (cid:63) represents the con-
volution. The indicator function 1{x=y} equals one if x = y, and zero if x (cid:54)= y. The
superscript k represents decision epoch k and the subscript i is the index of a node or a
state. The pronoun ‘she’ refers to the defender, and ‘he’ refers to the attacker.

2 Problem Formulation

To obtain optimal engagement decisions at each honeypot under the probabilistic transi-
tion and the continuous sojourn time, we introduce the continuous-time inﬁnite-horizon
discounted SMDPs, which can be summarized by the tuple {t ∈ [0, ∞), S, A(sj),
tr(sl|sj, aj), z(·|sj, aj, sl), rγ(sj, aj, sl), γ ∈ [0, ∞)}. We describe each element of
the tuple in this section.

2.1 Network Topology

We abstract the structure of the honeynet as a ﬁnite graph G = (N , E). The node set
N := {n1, n2, · · · , nN } ∪ {nN +1} contains N nodes of hybrid honeypots. Take Fig. 2
as an example, a node can be either a virtual honeypot of an integrated database system

6

L. Huang and Q. Zhu

or a physical honeypot of an individual computer. These nodes provide different types
of functions and services, and are connected following the topology of the emulated
production system. Since we focus on optimizing the value of investigation in the hon-
eynet, we only distinguish between different types of honeypots in different shapes, yet
use one extra node nN +1 to represent the entire domain of the production system. The
network topology E := {ejl}, j, l ∈ N , is the set of directed links connecting node
nj with nl, and represents all possible transition trajectories in the honeynet. The links
can be either physical (if the connecting nodes are real facilities such as computers) or
logical (if the nodes represent integrated systems). Attackers cannot break the topology
restriction. Since an attacker may use some honeypots as pivots to reach a production
system, and it is also possible for a defender to attract attackers from the normal zone
to the honeynet through these bridge nodes, there exist links of both directions between
honeypots and the normal zone.

2.2 States and State-Dependent Actions

At time t ∈ [0, ∞), an attacker’s state belongs to a ﬁnite set S := {s1, s2, · · · , sN ,
sN +1, sN +2} where si, i ∈ {1, · · · , N + 1}, represents the attacker’s location at time
t. Once attackers are ejected or terminate on their own, we use the extra absorbing state
sN +2 to represent the virtual location. The attacker’s state reveals the adversary visit
and exploitation of the emulated functions and services. Since the honeynet provides
a controlled environment, we assume that the defender can monitor the state and tran-
sitions persistently without uncertainties. The attacker can visit a node multiple times
for different purposes. A stealthy attacker may visit the honeypot node of the database
more than once and revise data progressively (in a small amount each time) to evade
detection. An attack on the honeypot node of sensors may need to frequently check the
node for the up-to-date data. Some advanced honeypots may also emulate anti-virus
systems or other protection mechanisms such as setting up an authorization expiration
time, then the attacker has to compromise the nodes repeatedly.

At each state si ∈ S, the defender can choose an action ai from a state-dependent
ﬁnite set A(si). For example, at each honeypot node, the defender can conduct ac-
tion aE to eject the attacker, action aP to purely record the attacker’s activities, low-
interactive action aL, or high-interactive action aH to engage the attacker, i.e., A(si) :=
{aE, aP , aL, aH }, i ∈ {1, · · · , N }. The high-interactive action is costly to implement
yet both increases the probability of a longer sojourn time at honeypot ni, and reduces
the probability of attackers penetrating the normal system from ni if connected. If the
attacker resides in the normal zone either from the beginning or later through the pivot
honeypots, the defender can choose either action aE to eject the attacker immediately,
or action aA to attract the attacker to the honeynet by exposing some vulnerabilities
intentionally, i.e., A(sN +1) := {aE, aA}. Note that the instantiation of the action set
and the corresponding consequences are not limited to the above scenario. For example,
the action can also refer to a different degree of outbound data control. A strict control
reduces the probability of attackers penetrating the normal system from the honeypot,
yet also brings less investigation value.

Adaptive Honeypot Engagement

7

2.3 Continuous-Time Process and Discrete Decision Model

Based on the current state sj ∈ S, the defender’s action aj ∈ A(sj), the attacker tran-
sits to state sl ∈ S with a probability tr(sl|sj, aj) and the sojourn time at state sj is a
continuous random variable with a probability density z(·|sj, aj, sl). Note that the risk
T1 of the attacker identifying the honeynet at state sj under action aj (cid:54)= AE can be
characterized by the transition probability tr(sN +2|sj, aj) as well as the duration time
z(·|sj, aj, sN +2). Once the attacker arrives at a new honeypot ni, the defender dynami-
cally applies an interaction action at honeypot ni from A(si) and keeps interacting with
the attacker until he transits to the next honeypot. The defender may not change the ac-
tion before the transition to reduce the probability of attackers detecting the change and
become aware of the honeypot engagement. Since the decision is made at the time of
transition, we can transform the above continuous time model on horizon t ∈ [0, ∞)
into a discrete decision model at decision epoch k ∈ {0, 1, · · · , ∞}. The time of the at-
tacker’s kth transition is denoted by a random variable T k, the landing state is denoted
as sk ∈ S, and the adopted action after arriving at sk is denoted as ak ∈ A(sk).

2.4

Investigation Value

The defender gains a reward of investigation by engaging and analyzing the attacker in
the honeypot. To simplify the notation, we divide the reward during time t ∈ [0, ∞)
into ones at discrete decision epochs T k, k ∈ {0, 1, · · · , ∞}. When τ ∈ [T k, T k+1]
amount of time elapses at stage k, the defender’s reward of investigation

r(sk, ak, sk+1, T k, T k+1, τ ) = r1(sk, ak, sk+1)1{τ =0} + r2(sk, ak, T k, T k+1, τ ),

at time τ of stage k, is the sum of two parts. The ﬁrst part is the immediate cost of
applying engagement action ak ∈ A(sk) at state sk ∈ S and the second part is the re-
ward rate of threat information acquisition minus the cost rate of persistently generating
deceptive trafﬁcs. Due to the randomness of the attacker’s behavior, the information ac-
quisition can also be random, thus the actual reward rate r2 is perturbed by an additive
zero-mean noise wr.

Different types of attackers target different components of the production system.
For example, an attacker who aims to steal data will take intensive adversarial actions
at the database. Thus, if the attacker is actually in the honeynet and adopts the same
behavior as he is in the production system, the defender can identify the target of the
attack based on the trafﬁc intensity. We specify r1 and r2 at each state properly to
measure the risk T3. To maximize the value of the investigation, the defender should
choose proper actions to lure the attacker to the honeypot emulating the target of the
attacker in a short time and with a large probability. Moreover, the defender’s action
should be able to engage the attacker in the target honeypot actively for a longer time to
obtain more valuable threat information. We compute the optimal long-term policy that
achieves the above objectives in Section 2.5.

As the defender spends longer time interacting with attackers, investigating their
behaviors and acquires better understandings of their targets and TTPs, less new infor-
mation can be extracted. In addition, the same intelligence becomes less valuable as
time elapses due to the timeliness. Thus, we use a discounted factor of γ ∈ [0, ∞) to
penalize the decreasing value of the investigation as time elapses.

8

L. Huang and Q. Zhu

2.5 Optimal Long-Term Policy

The defender aims at a policy π ∈ Π which maps state sk ∈ S to action ak ∈ A(sk) to
maximize the long-term expected utility starting from state s0, i.e.,

u(s0, π) = E

(cid:34) ∞
(cid:88)

(cid:90) T k+1

k=0

T k

e−γ(τ +T k)(r(Sk, Ak, Sk+1, T k, T k+1, τ ) + wr)dτ

(cid:35)

.

At each decision epoch, the value function v(s0) = supπ∈Π u(s0, π) can be repre-

sented by dynamic programming, i.e.,

v(s0) = sup

E

(cid:34)(cid:90) T 1

a0∈A(s0)

T 0

e−γ(τ +T 0)r(s0, a0, S1, T 0, T 1, τ )dτ + e−γT 1

v(S1)

(cid:35)

.

(1)

We assume a constant reward rate r2(sk, ak, T k, T k+1, τ ) = ¯r2(sk, ak) for sim-

plicity. Then, (1) can be transformed into an equivalent MDP form, i.e., ∀s0 ∈ S,

v(s0) = sup

(cid:88)

a0∈A(s0)

s1∈S

tr(s1|s0, a0)(rγ(s0, a0, s1) + zγ(s0, a0, s1)v(s1)),

(2)

where zγ(s0, a0, s1) := (cid:82) ∞
0 e−γτ z(τ |s0, a0, s1)dτ ∈ [0, 1] is the Laplace transform of
the sojourn probability density z(τ |s0, a0, s1) and the equivalent reward rγ(s0, a0, s1)
:= r1(s0, a0, s1)+ ¯r2(s0,a0)
(1−zγ(s0, a0, s1)) ∈ [−mc, mc] is assumed to be bounded
by a constant mc.

γ

A classical regulation condition of SMDP to avoid the probability of an inﬁnite
number of transitions within a ﬁnite time is stated as follows: there exists constants
θ ∈ (0, 1) and δ > 0 such that

tr(s1|s0, a0)z(δ|s0, a0, s1) ≤ 1 − θ, ∀s0 ∈ S, a0 ∈ A(s0).

(3)

(cid:88)

s1∈S

It is shown in [12] that condition (3) is equivalent to (cid:80)
s1∈S tr(s1|s0, a0)zγ(s0, a0, s1) ∈
[0, 1), which serves as the equivalent stage-varying discounted factor for the associated
MDP. Then, the right-hand side of (1) is a contraction mapping and there exists a unique
optimal policy π∗ = arg maxπ∈Π u(s0, π) which can be found by value iteration, pol-
icy iteration or linear programming.

Cost-Effective Policy The computation result of our 13-state example system is illus-
trated in Fig. 2. The optimal policies at honeypot nodes n1 to n11 are represented by
different colors. Speciﬁcally, actions aE, aP , aL, aH are denoted in red, blue, purple,
and green, respectively. The size of node ni represents the state value v(si).

In the example scenario, the honeypot of database n10 and sensors n11 are the main
and secondary targets of the attacker, respectively. Thus, defenders can obtain a higher
investigation value when they manage to engage the attacker in these two honeypot

Adaptive Honeypot Engagement

9

nodes with a larger probability and for a longer time. However, instead of naively adopt-
ing high interactive actions, a savvy defender also balances the high implantation cost of
aH . Our quantitative results indicate that the high interactive action should only be ap-
plied at n10 to be cost-effective. On the other hand, although the bridge nodes n1, n2, n8
which connect to the normal zone n12 do not contain higher investigation values than
other nodes, the defender still takes action aL at these nodes. The goal is to either in-
crease the probability of attracting attackers away from the normal zone or reduce the
probability of attackers penetrating the normal zone from these bridge nodes.

Engagement Safety versus Investigation Values Restrictive engagement actions en-
dow attackers less freedom so that they are less likely to penetrate the normal zone.
However, restrictive actions also decrease the probability of obtaining high-level IoCs,
thus reduces the investigation values.

To quantify the system value under the trade-off of the engagement safety and the
reward from the investigation, we visualize the trade-off surface in Fig. 3. In the x-axis,
a larger penetration probability p(sN +1|sj, aj), j ∈ {s1, s2, s8}, aj (cid:54)= aE, decreases
the value v(s10). In the y-axis, a larger reward rγ(sj, aj, sl), j ∈ S \ {s12, s13}, l ∈ S,
increases the value. The ﬁgure also shows that value v(s10) changes in a higher rate,
i.e., are more sensitive when the penetration probability is small and the reward from the
investigation is large. In our scenario, the penetration probability has less inﬂuence on
the value than the investigation reward, which motivates a less restrictive engagement.

Fig. 3: The trade-off surface of v(s10) in z-axis under different values of penetra-
tion probability p(sN +1|sj, aj), j ∈ {s1, s2, s8}, aj (cid:54)= aE, in x-axis, and the reward
rγ(sj, aj, sl), j ∈ S \ {s12, s13}, l ∈ S, in y-axis.

10

L. Huang and Q. Zhu

3 Risk Assessment

Given any feasible engagement policy π ∈ Π, the SMDP becomes a semi-Markov
process [24]. We analyze the evolution of the occupancy distribution and ﬁrst passage
time in Section 3.1 and 3.2, respectively, which leads to three security metrics during the
honeypot engagement. To shed lights on the defense of APTs, we investigate the system
performance against attackers with different levels of persistence and intelligence in
Section 3.3.

3.1 Transition Probability of Semi-Markov Process

Deﬁne the cumulative probability qij(t) of the one-step transition from {Sk = i, T k =
tk} to {Sk+1 = j, T k+1 = tk + t} as Pr(Sk+1 = j, T k+1 − tk ≤ t|Sk = i, T k =
tk) = tr(j|i, π(i)) (cid:82) t
0 z(τ |i, π(i), j)dτ, ∀i, j ∈ S, t ≥ 0. Based on a variation of the
forward Kolmogorov equation where the one-step transition lands on an intermediate
state l ∈ S at time T k+1 = tk + u, ∀u ∈ [0, t], the transition probability of the system
in state j at time t, given the initial state i at time 0 can be represented as

pii(t) = 1 −

(cid:88)

qih(t) +

(cid:90) t

(cid:88)

l∈S

0

pli(t − u)dqil(u),

h∈S
(cid:90) t

pij(t) =

(cid:88)

l∈S

0

plj(t − u)dqil(u) =

plj(t) (cid:63)

dqil(t)
dt

(cid:88)

l∈S

, ∀i, j ∈ S, j (cid:54)= i, ∀t ≥ 0,

where 1 − (cid:80)
can easily verify that (cid:80)
pii(t), we can take Laplace transform and then solve two sets of linear equations.

h∈S qih(t) is the probability that no transitions happen before time t. We
l∈S pil(t) = 1, ∀i ∈ S, ∀t ∈ [0, ∞). To compute pij(t) and

For simplicity, we specify z(τ |i, π(i), j) to be exponential distributions with param-
eters λij(π(i)), and the semi-Markov process degenerates to a continuous time Markov
chain. Then, we obtain the inﬁnitesimal generator via the Leibniz integral rule, i.e.,

¯qij :=

¯qii :=

dpij(t)
dt
dpii(t)
dt

(cid:12)
(cid:12)
(cid:12)
(cid:12)t=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)t=0

= λij(π(i)) · tr(j|i, π(i)) > 0, ∀i, j ∈ S, j (cid:54)= i,

(cid:88)

= −

j∈S\{i}

¯qij < 0, ∀i ∈ S.

Deﬁne matrix ¯Q := [¯qij]i,j∈S and vector Pi(t) = [pij(t)]j∈S , then based on the for-
ward Kolmogorov equation,

dPi(t)
dt

= lim
u→0+

Pi(t + u) − Pi(t)
u

= lim
u→0+

Pi(u) − I
u

Pi(t) = ¯QPi(t).

Thus, we can compute the ﬁrst security metric, the occupancy distribution of any state
s ∈ S at time t starting from the initial state i ∈ S at time 0, i.e.,

Pi(t) = e ¯QtPi(0), ∀i ∈ S.

(4)

Adaptive Honeypot Engagement

11

We plot the evolution of pij(t), i = sN +1, j ∈ {s1, s2, s10, s12}, versus t ∈ [0, ∞)
in Fig. 4 and the limiting occupancy distribution pij(∞), i = sN +1, in Fig. 5. In Fig.
4, although the attacker starts at the normal zone i = sN +1, our engagement policy
can quickly attract the attacker into the honeynet. Fig. 5 demonstrates that the engage-
ment policy can keep the attacker in the honeynet with a dominant probability of 91%
and speciﬁcally, in the target honeypot n10 with a high probability of 41%. The honey-
pots connecting the normal zone also have a higher occupancy probability than nodes
n3, n4, n5, n6, n7, n9, which are less likely to be explored by the attacker due to the
network topology.

Fig. 4: Evolution of pij(t), i = sN +1.

Fig. 5: The limiting occupancy distribution.

3.2 First Passage Time

Another quantitative measure of interest is the ﬁrst passage time TiD of visiting a set
D ⊂ S starting from i ∈ S \ D at time 0. Deﬁne the cumulative probability function
(cid:82) t
iD(t) = (cid:80)
iD(t) := Pr(TiD ≤ t), then f c
f c
0 f c
lD(t − u)dqil(u).
In particular, if D = {j}, then the probability density function fij(t) :=
satisﬁes

h∈D qih(t) + (cid:80)

l∈S\D

df c
ij (t)
dt

pij(t) =

(cid:90) t

0

pjj(t − u)df c

ij(u) = pjj(t) (cid:63) fij(t), ∀i, j ∈ S, j (cid:54)= i.

Take Laplace transform ¯pij(s) := (cid:82) ∞
transform on ¯fij(s) = ¯pij (s)
¯pjj (s) , we obtain

0 e−stpij(t)dt, and then take inverse Laplace

fij(t) =

(cid:90) ∞

0

est ¯pij(s)
¯pjj(s)

ds, ∀i, j ∈ S, j (cid:54)= i.

(5)

We deﬁne the second security metric, the attraction efﬁciency as the probability of
the ﬁrst passenger time Ts12,s10 less than a threshold tth. Based on (4) and (5), the
probability density function of Ts12,s10 is shown in Fig. 6. We take the mean denoted

0510152025Time00.10.20.30.40.50.60.70.80.91Probability1: Swtich2: Server10: Database12: Normal Zone1: Swtich2: Server345678910: Database11: Sensor12: Normal Zone12%10%1%2%1%3%3%11%3%41%4%9%12

L. Huang and Q. Zhu

by the orange line as the threshold tth and the attraction efﬁciency is 0.63, which means
that the defender can attract the attacker from the normal zone to the database honeypot
in less than tth = 20.7 with a probability of 0.63.

Fig. 6: Probability density function of Ts12,s10.

Mean First Passage Time The third security metric of concern is the average engage-
ment efﬁciency deﬁned as the Mean First Passage Time (MFPT) tm
iD = E[TiD], ∀i ∈
S, D ⊂ S. Under the exponential sojourn distribution, MFPT can be computed directly
through the a system of linear equations, i.e.,

tm
iD = 0, i ∈ D,

1 +

(cid:88)

l∈S

¯qiltm

lD = 0, i /∈ D.

(6)

ij (cid:54)= tm

In general, the MFPT is asymmetric, i.e., tm

ji , ∀i, j ∈ S. Based on (6), we
compute the MFPT from and to the normal zone in Fig. 7 and Fig. 8, respectively.
The color of each node indicates the value of MFPT. In Fig. 7, the honeypot nodes
that directly connect to the normal zone have the shortest MFPT, and it takes attackers
much longer time to visit the honeypots of clients due to the network topology. Fig. 8
shows that the defender can engage attackers in the target honeypot nodes of database
and sensors for a longer time. The engagements at the client nodes are yet much less
attractive. Note that two ﬁgures have different time scales denoted by the color bar
value, and the comparison shows that it generally takes the defender more time and
efforts to attract the attacker from the normal zone.

The MFPT from the normal zone tm

s12,j measures the average time it takes to attract
attacker to honeypot state j ∈ S \ {s12, s13} for the ﬁrst time. On the contrary, the
MFPT to the normal zone tm
i,s12 measures the average time of the attacker penetrating
the normal zone from honeypot state i ∈ S \ {s12, s13} for the ﬁrst time. If the defender
pursues absolute security and ejects the attack once it goes to the normal zone, then Fig.
8 also shows the attacker’s average sojourn time in the honeynet starting from different
honeypot nodes.

���������������������������������Adaptive Honeypot Engagement

13

Fig. 7: MFPT from the normal zone tm

s12,j.

Fig. 8: MFPT to the normal zone tm

i,s12 .

3.3 Advanced Persistent Threats

In this section, we quantify three engagement criteria on attackers of different levels of
persistence and intelligence in Fig. 9 and Fig. 10, respectively. The criteria are the sta-
tionary probability of normal zone pi,s12 (∞), ∀i ∈ S \ {s13}, the utility of normal zone
v(s12), and the expected utility over the stationary probability, i.e., (cid:80)
j∈S pij(∞)v(j),
∀i ∈ S \ {s13}.

As shown in Fig. 9, when the attacker is at the normal zone i = s12 and the de-
fender chooses action a = aA, a larger λ := λij(aA), ∀j ∈ {s1, s2, s8}, of the ex-
ponential sojourn distribution indicates that the attacker is more inclined to respond to
the honeypot attraction and thus less time is required to attract the attacker away from
the normal zone. As the persistence level λ increases from 0.1 to 2.5, the stationary
probability of the normal zone decreases and the expected utility over the stationary
probability increases, both converge to their stable values. The change rate is higher
during λ ∈ (0, 0.5] and much lower afterward. On the other hand, the utility loss at the
normal zone decreases approximately linearly during the entire period λ ∈ (0, 2.5].

As shown in Fig. 10, when the attacker becomes more advanced with a larger failure
probability of attraction, i.e., p := p(j|s12, aA), ∀j ∈ {s12, s13}, he can stay in the
normal zone with a larger probability. A signiﬁcant increase happens after p ≥ 0.5.
On the other hand, as p increases from 0 to 1, the utility of the normal zone reduces
linearly, and the expected utility over the stationary probability remains approximately
unchanged until p ≥ 0.9.

Fig. 9 and Fig. 10 demonstrate that the expected utility over the stationary proba-
bility receives a large decrease only at the extreme cases of a high transition frequency
and a large penetration probability. Similarly, the stationary probability of the normal
zone remains small for most cases except for the above extreme cases. Thus, our policy
provides a robust expected utility as well as a low-risk engagement over a large range
of changes in the attacker’s persistence and intelligence.

14

L. Huang and Q. Zhu

Fig. 9: Three engagement criteria under
different persistence levels λ ∈ (0, 2.5].

Fig. 10: Three engagement criteria under
different intelligence levels p ∈ [0, 1].

4 Reinforcement Learning of SMDP

Due to the absent knowledge of an exact SMDP model, i.e., the investigation reward, the
attacker’s transition probability (and even the network topology), and the sojourn dis-
tribution, the defender has to learn the optimal engagement policy based on the actual
experience of the honeynet interactions. As one of the classical model-free reinforce-
ment learning methods, the Q-learning algorithm for SMDP has been stated in [3], i.e.,

Qk+1(sk, ak) :=(1 − αk(sk, ak))Qk(sk, ak) + αk(sk, ak)[¯r1(sk, ak, ¯sk+1)

+ ¯r2(sk, ak)

(1 − e−γ ¯τ k
γ

)

− e−γ ¯τ k

max
a(cid:48)∈A(¯sk+1)

Qk(¯sk+1, a(cid:48))],

(7)

where sk is the current state sample, ak is the current selected action, αk(sk, ak) ∈
(0, 1) is the learning rate, ¯sk+1 is the observed state at next stage, ¯r1, ¯r2 is the observed
investigation rewards, and ¯τ k is the observed sojourn time at state sk. When the learn-
ing rate satisﬁes (cid:80)∞
k=0(αk(sk, ak))2 < ∞, ∀sk ∈ S, ∀ak ∈
A(sk), and all state-action pairs are explored inﬁnitely, maxa(cid:48)∈A(sk) Qk(sk, a(cid:48)), k →
∞, in (7) converges to value v(sk) with probability 1.

k=0 αk(sk, ak) = ∞, (cid:80)∞

At each decision epoch k ∈ {0, 1, · · · }, the action ak is chosen according to the (cid:15)-
greedy policy, i.e., the defender chooses the optimal action arg maxa(cid:48)∈A(sk) Qk(sk, a(cid:48))
with a probability 1 − (cid:15), and a random action with a probability (cid:15). Note that the explo-
ration rate (cid:15) ∈ (0, 1] should not be too small to guarantee sufﬁcient samples of all
state-action pairs. The Q-learning algorithm under a pure exploration policy (cid:15) = 1 still
converges yet at a slower rate.

In our scenario, the defender knows the reward of ejection action aA and v(s13) =
0, thus does not need to explore action aA to learn it. We plot one learning trajectory of
the state transition and sojourn time under the (cid:15)-greedy exploration policy in Fig. 11,
where the chosen actions aE, aP , aL, aH are denoted in red, blue, purple, and green,
respectively. If the ejection reward is unknown, the defender should be restrictive in
exploring aA which terminates the learning process. Otherwise, the defender may need

00.20.4ProbabilityStationary Probability of Normal Zone-3-2-1ValueUtility of Normal Zone00.511.522.5Value of 6810ValueExpected Utility over Stationary Probability00.51ProbabilityStationary Probability of Normal Zone-3.5-3-2.5-2ValueUtility of Normal Zone00.10.20.30.40.50.60.70.80.91Probability of Failed Attraction0510ValueExpected Utility over Stationary ProbabilityAdaptive Honeypot Engagement

15

Fig. 11: One instance of Q-learning on SMDP where the x-axis shows
the sojourn time and the y-axis represents the state transition. The
chosen actions aE, aP , aL, aH are denoted in red, blue, purple, and
green, respectively.

kc
k{sk ,ak }−1+kc

to engage with a group of attackers who share similar behaviors to obtain sufﬁcient
samples to learn the optimal engagement policy.
In particular, we choose αk(sk, ak) =

, ∀sk ∈ S, ∀ak ∈ A(sk), to
guarantee the asymptotic convergence, where kc ∈ (0, ∞) is a constant parameter and
k{sk,ak} ∈ {0, 1, · · · } is the number of visits to state-action pair {sk, ak} up to stage k.
We need to choose a proper value of kc to guarantee a good numerical performance of
convergence in ﬁnite steps as shown in Fig. 12. We shift the green and blue lines verti-
cally to avoid the overlap with the red line and represent the corresponding theoretical
values in dotted black lines. If kc is too small as shown in the red line, the learning rate
decreases so fast that new observed samples hardly update the Q-value and the defender
may need a long time to learn the right value. However, if kc is too large as shown in
the green line, the learning rate decreases so slow that new samples contribute signiﬁ-
cantly to the current Q-value. It causes a large variation and a slower convergence rate
of maxa(cid:48)∈A(s12) Qk(s12, a(cid:48)).

We show the convergence of the policy and value under kc = 1, (cid:15) = 0.2, in the
video demo (See URL: https://bit.ly/2QUz3Ok). In the video, the color of
each node nk distinguishes the defender’s action ak at state sk and the size of the node
is proportional to maxa(cid:48)∈A(sk) Qk(sk, a(cid:48)) at stage k. To show the convergence, we
decrease the value of (cid:15) gradually to 0 after 5000 steps.

Since the convergence trajectory is stochastic, we run the simulation for 100 times
and plot the mean and the variance of Qk(s12, aP ) of state s12 under the optimal policy

2.48992.49942.50892.51842.52792.5374Time10412345678910111213State16

L. Huang and Q. Zhu

π(s12) = aP in Fig. 13. The mean in red converges to the theoretical value in about
400 steps and the variance in blue reduces dramatically as step k increases.

Fig. 12: The convergence rate under dif-
ferent values of kc.

Fig. 13: The evolution of the mean and the
variance of Qk(s12, aP ).

4.1 Discussion

In this section, we discuss the challenges and related future directions about reinforce-
ment learning in the honeypot engagement.

Non-cooperative and Adversarial Learning Environment The major challenge of
learning under the security scenario is that the defender lacks full control of the learning
environment, which limits the scope of feasible reinforcement learning algorithms. In
the classical reinforcement learning task, the learner can choose to start at any state at
any time, and repeatedly simulate the path from the target state. In the adaptive honeypot
engagement problem, however, the defender can remove attackers but cannot arbitrarily
draw them to the target honeypot and force them to show their attacking behaviors
because the true threat information is revealed only when attackers are unaware of the
honeypot engagements. The future work could generalize the current framework to an
adversarial learning environment where a savvy attacker can detect the honeypot and
adopt deceptive behaviors to interrupt the learning process.

Risk Reduction during the Learning Period Since the learning process is based on
samples from real interactions, the defender needs to concern the system safety and
security during the learning period. For example, if the visit and sojourn in the normal
zone bring a signiﬁcant amount of losses, we can use the SARSA algorithm to conduct a
more conservative learning process than Q-learning. Other safe reinforcement learning
methods are stated in the survey [8], which are left as future work.

01234567Step k104Value01002003004005006007008009001000-7-6-5-4-3-2-10123VarianceMeanTheoretical ValueAdaptive Honeypot Engagement

17

Asymptotic versus Finite-Step Convergence Since an attacker can terminate the in-
teraction on his own, the engagement time with attacker may be limited. Thus, compar-
ing to an asymptotic convergence of policy learning, the defender aims more to conduct
speedy learning of the attacker’s behaviors in ﬁnite steps, and meanwhile, achieve a
good engagement performance in these ﬁnite steps.

Previous works have studied the convergence rate [6] and the non-asymptotic con-
vergence [19,18] in the MDP setting. For example, [6] have shown a relationship be-
tween the convergence rate and the learning rate of Q-learning, [19] has provided the
performance bound of the ﬁnite-sample convergence rate, and [18] has proposed E3 al-
gorithm which achieves near-optimal with a large probability in polynomial time. How-
ever, in the honeypot engagement problem, the defender does not know the remaining
steps that she can interact with the attacker because the attacker can terminate on his
own. Thus, we cannot directly apply the E3 algorithm which depends on the horizon
time. Moreover, since attackers may change their behaviors during the long learning
period, the learning algorithm needs to adapt to the changes of SMDP model quickly.

In this preliminary work, we use the (cid:15)-greedy policy for the trade-off of the exploita-
tion and exploration during the ﬁnite learning time. The (cid:15) can be set at a relatively large
value without the gradual decrease so that the learning algorithm persistently adapts to
the changes in the environment. On the other hand, the defender can keep a larger dis-
counted factor γ to focus on the immediate investigation reward. If the defender expects
a short interaction time, i.e., the attacker is likely to terminate in the near future, she can
increase the discounted factor in the learning process to adapt to her expectations.

Transfer Learning In general, the learning algorithm on SMDP converges slower than
the one on MDP because the sojourn distribution introduces extra randomness. Thus,
instead of learning from scratch, the defender can attempt to reuse the past experience
with attackers of similar behaviors to expedite the learning process, which motivates
the investigation of transfer learning in reinforcement learning [39]. Some side-channel
information may also contribute to the transfer learning.

5 Conclusion

A honeynet is a promising active defense scheme. Comparing to traditional passive de-
fense techniques such as the ﬁrewall and intrusion detection systems, the engagement
with attackers can reveal a large range of Indicators of Compromise (IoC) at a lower rate
of false alarms and missed detection. However, the active interaction also introduces the
risks of attackers identifying the honeypot setting, penetrating the production system,
and a high implementation cost of persistent synthetic trafﬁc generations. Since the re-
ward depends on honeypots’ type, the defender aims to lure the attacker into the target
honeypot in the shortest time. To satisfy the above requirements of security, cost, and
timeliness, we leverage the Semi-Markov Decision Process (SMDP) to model the tran-
sition probability, sojourn distribution, and investigation reward. After transforming the
continuous time process into the equivalent discrete decision model, we have obtained
long-term optimal policies that are risk-averse, cost-effective, and time-efﬁcient.

18

L. Huang and Q. Zhu

We have theoretically analyzed the security metrics of the occupancy distribution,
attraction efﬁciency, and average engagement efﬁciency based on the transition prob-
ability and the probability density function of the ﬁrst passenger time. The numerical
results have shown that the honeypot engagement can engage the attacker in the target
honeypot with a large probability and in a desired speed. In the meantime, the pene-
tration probability is kept under a bearable level for most of the time. The results also
demonstrate that it is a worthy compromise of the immediate security to allow a small
penetration probability so that a high investigation reward can be obtained in the long
run.

Finally, we have applied reinforcement learning methods on the SMDP in case the
defender can not obtain the exact model of the attacker’s behaviors. Based on a prudent
choice of the learning rate and exploration-exploitation policy, we have achieved a quick
convergence rate of the optimal policy and the value. Moreover, the variance of the
learning process has decreased dramatically with the number of observed samples.

References

1. Al-Shaer, E.S., Wei, J., Hamlen, K.W., Wang, C.: Autonomous Cyber Deception: Reasoning,

Adaptive Planning, and Evaluation of HoneyThings. Springer (2019)

2. Bianco, D.: The pyramid of pain (2013), http://detect-respond.blogspot.

com/2013/03/the-pyramid-of-pain.html

3. Bradtke, S.J., Duff, M.O.: Reinforcement learning methods for continuous-time markov de-
cision problems. In: Advances in neural information processing systems. pp. 393–400 (1995)
4. Chen, D., Trivedi, K.S.: Optimization for condition-based maintenance with semi-markov

decision process. Reliability engineering & system safety 90(1), 25–29 (2005)

5. Chen, J., Zhu, Q.: Security as a service for cloud-enabled internet of controlled things under
advanced persistent threats: a contract design approach. IEEE Transactions on Information
Forensics and Security 12(11), 2736–2750 (2017)

6. Even-Dar, E., Mansour, Y.: Learning rates for q-learning. Journal of Machine Learning Re-

search 5(Dec), 1–25 (2003)

7. Farhang, S., Manshaei, M.H., Esfahani, M.N., Zhu, Q.: A dynamic bayesian security game
framework for strategic defense mechanism design. In: Decision and Game Theory for Se-
curity, pp. 319–328. Springer (2014)

8. Garcıa, J., Fern´andez, F.: A comprehensive survey on safe reinforcement learning. Journal

of Machine Learning Research 16(1), 1437–1480 (2015)

9. Hayel, Y., Zhu, Q.: Attack-aware cyber insurance for risk sharing in computer networks. In:

Decision and Game Theory for Security, pp. 22–34. Springer (2015)

10. Hecker, C.R.: A Methodology for Intelligent Honeypot Deployment and Active Engagement

of Attackers. Ph.D. thesis (2012), aAI3534194

11. Hor´ak, K., Zhu, Q., Boˇsansk`y, B.: Manipulating adversarys belief: A dynamic game ap-
proach to deception by design for proactive network security. In: International Conference
on Decision and Game Theory for Security. pp. 273–294. Springer (2017)

12. Hu, Q., Yue, W.: Markov decision processes with their applications, vol. 14. Springer Science

& Business Media (2007)

13. Huang, L., Chen, J., Zhu, Q.: Distributed and optimal resilient planning of large-scale inter-
dependent critical infrastructures. In: 2018 Winter Simulation Conference (WSC). pp. 1096–
1107. IEEE (2018)

Adaptive Honeypot Engagement

19

14. Huang, L., Chen, J., Zhu, Q.: Factored markov game theory for secure interdependent in-
frastructure networks. In: Game Theory for Security and Risk Management, pp. 99–126.
Springer (2018)

15. Huang, L., Zhu, Q.: Adaptive strategic cyber defense for advanced persistent threats in criti-
cal infrastructure networks. ACM SIGMETRICS Performance Evaluation Review (2018)
16. Huang, L., Zhu, Q.: A dynamic games approach to proactive defense strategies against ad-
vanced persistent threats in cyber-physical systems. arXiv preprint arXiv:1906.09687 (2019)
17. Jajodia, S., Ghosh, A.K., Swarup, V., Wang, C., Wang, X.S.: Moving target defense: creating
asymmetric uncertainty for cyber threats, vol. 54. Springer Science & Business Media (2011)
18. Kearns, M., Singh, S.: Near-optimal reinforcement learning in polynomial time. Machine

learning 49(2-3), 209–232 (2002)

19. Kearns, M.J., Singh, S.P.: Finite-sample convergence rates for q-learning and indirect algo-

rithms. In: Advances in neural information processing systems. pp. 996–1002 (1999)

20. La, Q.D., Quek, T.Q., Lee, J., Jin, S., Zhu, H.: Deceptive attack and defense game in
honeypot-enabled networks for the internet of things. IEEE Internet of Things Journal 3(6),
1025–1035 (2016)

21. Liang, H., Cai, L.X., Huang, D., Shen, X., Peng, D.: An smdp-based service model for inter-
domain resource allocation in mobile cloud networks. IEEE transactions on vehicular tech-
nology 61(5), 2222–2232 (2012)

22. Luo, T., Xu, Z., Jin, X., Jia, Y., Ouyang, X.: Iotcandyjar: Towards an intelligent-interaction

honeypot for iot devices. Black Hat (2017)

23. Mudrinich, E.M.: Cyber 3.0: The department of defense strategy for operating in cyberspace

and the attribution problem. AFL Rev. 68, 167 (2012)

24. Nakagawa, T.: Stochastic processes: With applications to reliability theory. Springer Science

& Business Media (2011)

25. Paruchuri, P., Pearce, J.P., Marecki, J., Tambe, M., Ordonez, F., Kraus, S.: Playing games for
security: An efﬁcient exact algorithm for solving bayesian stackelberg games. In: Proceed-
ings of the 7th international joint conference on Autonomous agents and multiagent systems-
Volume 2. pp. 895–902. International Foundation for Autonomous Agents and Multiagent
Systems (2008)

26. Pauna, A., Iacob, A.C., Bica, I.: Qrassh-a self-adaptive ssh honeypot driven by q-learning.
In: 2018 international conference on communications (COMM). pp. 441–446. IEEE (2018)
27. Pawlick, J., Colbert, E., Zhu, Q.: Modeling and analysis of leaky deception using signaling
games with evidence. IEEE Transactions on Information Forensics and Security 14(7), 1871–
1886 (2018)

28. Pawlick, J., Colbert, E., Zhu, Q.: A game-theoretic taxonomy and survey of defensive decep-
tion for cybersecurity and privacy. ACM Computing Surveys (CSUR), to appear (2019)
29. Pawlick, J., Farhang, S., Zhu, Q.: Flip the cloud: Cyber-physical signaling games in the
presence of advanced persistent threats. In: Decision and Game Theory for Security, pp.
289–308. Springer (2015)

30. Pawlick, J., Nguyen, T.T.H., Colbert, E., Zhu, Q.: Optimal timing in dynamic and robust
attacker engagement during advanced persistent threats. In: 2019 17th International Sympo-
sium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt).
pp. 1–6. IEEE (2019)

31. Pawlick, J., Zhu, Q.: A Stackelberg game perspective on the conﬂict between machine learn-
ing and data obfuscation. In: Information Forensics and Security (WIFS), 2016 IEEE In-
ternational Workshop on. pp. 1–6. IEEE (2016), http://ieeexplore.ieee.org/
abstract/document/7823893/

32. Pawlick, J., Zhu, Q.: A mean-ﬁeld stackelberg game approach for obfuscation adoption in
empirical risk minimization. In: 2017 IEEE Global Conference on Signal and Information
Processing (GlobalSIP). pp. 518–522. IEEE (2017)

20

L. Huang and Q. Zhu

33. Pawlick, J., Zhu, Q.: Proactive defense against physical denial of service attacks using pois-
son signaling games. In: International Conference on Decision and Game Theory for Secu-
rity. pp. 336–356. Springer (2017)

34. Pouget, F., Dacier, M., Debar, H.: White paper: honeypot, honeynet, honeytoken: termino-

logical issues. Rapport technique EURECOM 1275 (2003)

35. Rid, T., Buchanan, B.: Attributing cyber attacks. Journal of Strategic Studies 38(1-2), 4–37

(2015)

36. Sahabandu, D., Xiao, B., Clark, A., Lee, S., Lee, W., Poovendran, R.: Dift games: dynamic
information ﬂow tracking games for advanced persistent threats. In: 2018 IEEE Conference
on Decision and Control (CDC). pp. 1136–1143. IEEE (2018)

37. Spitzner, L.: Honeypots: tracking hackers, vol. 1. Addison-Wesley Reading (2003)
38. Sun, Y., Uysal-Biyikoglu, E., Yates, R.D., Koksal, C.E., Shroff, N.B.: Update or wait: How to
keep your data fresh. IEEE Transactions on Information Theory 63(11), 7492–7508 (2017)
39. Taylor, M.E., Stone, P.: Transfer learning for reinforcement learning domains: A survey.

Journal of Machine Learning Research 10(Jul), 1633–1685 (2009)

40. Wagener, G., Dulaunoy, A., Engel, T., et al.: Self adaptive high interaction honeypots driven
by game theory. In: Symposium on Self-Stabilizing Systems. pp. 741–755. Springer (2009)
41. Wang, K., Du, M., Maharjan, S., Sun, Y.: Strategic honeypot game model for distributed
denial of service attacks in the smart grid. IEEE Transactions on Smart Grid 8(5), 2474–
2482 (2017)

42. Xu, Z., Zhu, Q.: A cyber-physical game framework for secure and resilient multi-agent au-
tonomous systems. In: Decision and Control (CDC), 2015 IEEE 54th Annual Conference
on. pp. 5156–5161. IEEE (2015)

43. Zhang, R., Zhu, Q., Hayel, Y.: A bi-level game approach to attack-aware cyber insurance
of computer networks. IEEE Journal on Selected Areas in Communications 35(3), 779–794
(2017)

44. Zhang, T., Zhu, Q.: Dynamic differential privacy for ADMM-based distributed classiﬁcation
learning. IEEE Transactions on Information Forensics and Security 12(1), 172–187 (2017),
http://ieeexplore.ieee.org/abstract/document/7563366/

45. Zhang, T., Zhu, Q.: Distributed privacy-preserving collaborative intrusion detection systems
for vanets. IEEE Transactions on Signal and Information Processing over Networks 4(1),
148–161 (2018)

46. Zhu, Q., Bas¸ar, T.: Game-theoretic methods for robustness, security, and resilience of cyber-
physical control systems: games-in-games principle for optimal cross-layer resilient control
systems. IEEE Control Systems Magazine 35(1), 46–65 (2015)

47. Zhu, Q., Bas¸ar, T.: Dynamic policy-based ids conﬁguration. In: Decision and Control, 2009
held jointly with the 2009 28th Chinese Control Conference. CDC/CCC 2009. Proceedings
of the 48th IEEE Conference on. pp. 8600–8605. IEEE (2009)

48. Zhu, Q., Bas¸ar, T.: Game-theoretic approach to feedback-driven multi-stage moving target

defense. In: Decision and Game Theory for Security. pp. 246–263. Springer (2013)

49. Zhu, Q., Clark, A., Poovendran, R., Basar, T.: Deployment and exploitation of deceptive
honeybots in social networks. In: Decision and Control (CDC), 2013 IEEE 52nd Annual
Conference on. pp. 212–219. IEEE (2013)

50. Zhu, Q., Fung, C., Boutaba, R., Bas¸ar, T.: Guidex: A game-theoretic incentive-based mecha-
nism for intrusion detection networks. Selected Areas in Communications, IEEE Journal on
30(11), 2220–2230 (2012)

51. Zhuang, J., Bier, V.M., Alagoz, O.: Modeling secrecy and deception in a multiple-period
attacker–defender signaling game. European Journal of Operational Research 203(2), 409–
418 (2010)

