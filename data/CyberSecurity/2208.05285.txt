2
2
0
2

g
u
A
0
1

]

R
C
.
s
c
[

1
v
5
8
2
5
0
.
8
0
2
2
:
v
i
X
r
a

Explaining Machine Learning DGA Detectors from DNS
Traﬃc Data

Giorgio Piras1, Maura Pintor1,2, Luca Demetrio1,2, and Battista Biggio1,2

1 University of Cagliari, Cagliari, Italy
2 Pluribus One S.r.l., Cagliari, Italy

Abstract

One of the most common causes of lack of continuity of online systems stems from a
widely popular Cyber Attack known as Distributed Denial of Service (DDoS), in which
a network of infected devices (botnet) gets exploited to ﬂood the computational capacity
of services through the commands of an attacker. This attack is made by leveraging the
Domain Name System (DNS) technology through Domain Generation Algorithms (DGAs),
a stealthy connection strategy that yet leaves suspicious data patterns. To detect such
threats, advances in their analysis have been made. For the majority, they found Machine
Learning (ML) as a solution, which can be highly eﬀective in analyzing and classifying
massive amounts of data. Although strongly performing, ML models have a certain degree
of obscurity in their decision-making process. To cope with this problem, a branch of ML
known as Explainable ML tries to break down the black-box nature of classiﬁers and make
them interpretable and human-readable. This work addresses the problem of Explainable
ML in the context of botnet and DGA detection, which at the best of our knowledge, is the
ﬁrst to concretely break down the decisions of ML classiﬁers when devised for botnet/DGA
detection, therefore providing global and local explanations.

1 Introduction

During the last decades, our day-by-day life has been strictly connected to the usage of de-
vices and online services, therefore making their eﬃciency and continuity play a crucial role in
the technological transformation we witness. Likewise, the economic loss derived from cyber-
threats has increased exponentially in recent years [1] as the technologies continually evolve and
attackers develop their skills. One of the most common ways cybercriminals try to jeopardize
the continuity of systems and thus cause economic damage is Denial of Service (DoS), which
aims to drain the computing capabilities of the target system in both fancy and basic ways.
A case of this attack is the Distributed Denial of Service DDoS, where a network of infected
devices (bots) are commanded by an attacker (botmaster) through a Command&Control Server
(C&C) [2, 3, 4]. What happens to be erratic and thus detectable by a Machine Learning (ML)
model in this kind of attack is the DNS traﬃc, carrying Domain Names through which bots
are connected to the C&C server. This stealthy connection strategy is commonly known as
Domain Fluxing, where the algorithms used by the infected bots to generate the domain are
known as Domain Generation Algorithms (DGAs).

Although employing ML models to detect the presence of botnets within network traﬃc
has been demonstrated to be successful, almost the entirety of the relevant works have followed
a common baseline and workﬂow, presenting a partially novel feature set on which to train a
classiﬁer to obtain relevant results [5, 6, 7, 8]. The proposed approaches lack interpretability
and contextualization. First, depending on the context from which DNS traﬃc data is extracted
and the model is deployed, potential attackers might have control over some features. Second,
the model prioritization and general usage of the features in the decision process are not known
beforehand, making the process challenging to debug and protect.

 
 
 
 
 
 
Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

To make up for these problems, we ﬁrst analyze the techniques used to detect botnets/DGAs
from the DNS data (Section 2); we analyze which explainability techniques can provide insight
into how the model takes its decisions (Section 3). Upon a re-implementation of the EXPO-
SURE system [9, 5] (Section 4), we provide the following contributions: (i) we build and test the
EXPOSURE system on a newly collected dataset; (ii) we observe statistics on the features used
by the system; (iii) we train diﬀerent classiﬁers and compare their performances; (iv) we obtain
explanations from such classiﬁers; and (v) given the explanations, we develop and discuss an
analysis on the features used by the systems mentioned above. Finally, we conclude the work
by presenting related works (Section 5), limitations, and future directions (Section 6).

2 Background: DNS System and ML Techniques

From DNS to DGA. The Domain Name System (DNS) is a database responsible for mapping
domain names to IP addresses, thus answering a query made by clients in the form of a domain
name towards the IP addresses. This action is commonly known as resolution [10]. The
DNS organizes domain names into a hierarchy (through dot-separated levels), as the whole
technology itself creates a hierarchical database structure. The information stored and carried
by DNS records can be A records, returning IPv4 addresses, NS records, returning authoritative
name servers, and ﬁnally, PTR, which stands for Pointer to Record and returns a domain name
but in the reverse query format (i.e., the question started from an IP rather than a domain
It is also worth citing speciﬁc information carried by the DNS packets, such as the
name).
Time-to-live (TTL), which indicates how long the server will cache that packet [11, 10]. Being
of paramount importance for the correct functioning of basic internet activities, the DNS is the
perfect target for malicious activities having a high impact on unaware users. That is why this
technology gets exploited by attackers (botmasters) who aim to command and control a network
of infected machines, i.e., a botnet. To go as undercover as possible, having only one domain
name to which to connect would have the botnet quickly taken down by vigilant authorities.
That is why bots generate massive DNS traﬃc trying to connect to a much more concealed
C&C server. The generation of such a signiﬁcant amount of domain names happens through
Domain Generation Algorithms that, given a random seed, create a string that will possibly
establish a connection.

Botnet Detection with ML: The EXPOSURE system. Seizing the chance to detect
malicious patterns, the research community has driven its eﬀorts towards analyzing the DNS
data, extracting the features, and eventually training a ML model capable to distinguish mali-
cious and benign DNS behaviors. The EXPOSURE system [9, 5] is among the most prominent
works for its completeness in the feature set and reproducibility (in terms of feature extraction).
For this reason, we use it as a base for our explainability analysis. Table 1 shows the feature
set, listing the features extracted by the EXPOSURE system (whose extensive description can
be found in the original work [9, 5]). The entire set is subdivided into Time-Based features
(collecting temporal patterns from the queries to the domains), DNS-Answer-Based Features
(patterns from the answers records), TTL-Value-Based Features (statistical patterns from the
TTL values), and ﬁnally, Name-Based Features (statistical patterns from the Domain Name).
Given a collection of DNS packets, we can compose a training set of benign and malicious
samples to train the classiﬁer, as Bilge et al. did in EXPOSURE. Our work will focus on repro-
ducing the experiment with our newly-collected traﬃc and applying explainability techniques
to understand the patterns employed by the model for detecting malicious activities.

2

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

Feature set

Time-Based Features

DNS Answer-Based Features

TTL Value-Based Features

Domain Name-Based Features

#
2

Paper Feature Name
Short Life

1
2

2

1
1
3

1

1
1
1
1
5

1
1

Daily similarity
Repeating patterns

Access Ratio

Number of distinct IP addresses
Number of distinct countries
Reverse DNS query result

Number of domains sharing the
same IP
Average TTL
Standard Deviation of TTL
Number of distinct TTL values
Number of TTL changes
Percentage usage of TTL ranges

% of numerical characters
% of length of the LMS

Our Feature Names
glob short lived
glob life ratio
daily similarity
local numOf changes
stddev before change
idle
popular
unique ips
unique ccode
rev arec
rev nsrec
rev asnrec
shared ips

ttl avg
ttl stddev
unique ttls
ttl changes
ttl range1
ttl range100
ttl range300
ttl range900
ttl rangeinf
num chars%
%of lms

Table 1: List of features used in EXPOSURE renewed with a mnemonic. The ﬁrst column indicates the Feature
subset. The second one shows the number of features that a speciﬁc feature holds. Finally, the third and fourth
columns indicate respectively the feature name chosen by the authors and our feature names for atomic features.

3 Explaining Predictions of ML-based DNS Analysis

As pointed out by Miller et al. [12], explanations increase transparency and interpretability so
that user awareness and systems designers can jointly beneﬁt from this gain of trust. In security-
relevant scenarios, like the one we are considering, understanding the data and the model
provides the added beneﬁt of helping to see if there are problems in the system, for example
assigning high relevance to spurious features that should not inﬂuence it to that extent [13].

Analyzing the dataset’s statistics provides further insights into the separability of the fea-
tures into the two diﬀerent classes. Additionally, in the case under investigation, lots of features
come from similar sources and elaborations, which lets the statistical analysis come in handy
to highlight correlations and redundancies.

On top of that, we will use a ML model to analyze such features and categorize the samples
into the two output classes of benign and malicious domains. Model explanations can help
understand how the model is making such decisions. An explanation is said to be local if it is
made on single samples and wants to describe how a model emphasizes the features of a speciﬁc
single sample in its classiﬁcation. On the other hand, global explanations are made over entire
datasets or relevant collections of samples to describe how the model prioritizes features over
those samples [14]. This work will focus on both local and global explanations.

In [15], Lundberg and Lee proposed SHAP (SHapley Additive exPlanations), where feature
importance is computed with an additive approach, representing a uniﬁed measure of feature
importance. The basic concept behind SHAP comes from Shapley values and a game theory
setting, where the features act as players and cooperate in a coalitional game (i.e., the prediction
task) to receive a proﬁt (i.e., a gain, which is the actual prediction). The Shapley values
assign payouts to players depending on their contribution to the total payout [16]. Thus each
feature that contributes to the prediction task is computed as a sum of the expected marginal
contributions in any feature value combination. Given the computational burden for which

3

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

SHAP should ﬁnd all the possible feature combinations, Lundberg and Lee proposed a Shapley
kernel that produces estimates instead of exact values.

3.1 Data Analysis and Explainability Techniques

This section will brieﬂy explain the details and diﬀerences between the data analysis and ex-
plainability tools that will play a central role in the experiments section.

Feature Statistical Analysis. These plots show the marginal distributions of every pair of
features as density plots, describing how the distributions for the classes behave. Through the
scattered plots instead, we can assess where both benign and malicious samples lie in their ad-
hoc feature space, thus making us capable of understanding to which extent pairs of features
separate the data. Analyzing the scattered plots allows observing the distribution of the features
to get a rough idea of how they will behave/discriminate and to which extent.

Partial Dependence Plot. This plot shows the marginal eﬀect that a single feature has
on the prediction made by the model, thus providing global explanations. Taking as input
the model, the feature, and a background distribution on which to make the model learn the
feature importance, the Partial Dependence Plots (PDPs) depict the feature values on the x-
axis, whilst the y-axis represents the expected prediction contribution given the feature value.
In the background, a histogram shows the underlying data distribution of the feature values.
A horizontal line represents the expected contribution to the prediction, and a vertical one
represents the expected value of the feature. By reading this plot, we can measure how the
observed feature contributes to the classiﬁcation of the samples.

Summary Plot. The SHAP summary plot, which as the PDP is a global explanation tech-
nique, shows how the model prioritizes the features and how these contribute to steering the
classiﬁcation towards each class. This plot comprises a list of features ordered from the one
giving the higher contribution to the least powerful as interpreted by the model, showing the
magnitude for benign (in blue) and malicious (in red) samples.

Force Plot. This technique is one of the local explainability methods provided in SHAP. It
explains why a speciﬁc sample has been assigned a particular label. This can be useful for
understanding why samples are misclassiﬁed and to which extent the classiﬁer misunderstands
them. Force plots, showing the magnitude of the feature contribution on single samples, are
rendered as blue arrows indicating magnitude values towards the benign class and red vice-versa.
In the next section, we will use the presented techniques to explain predictions of our re-

implementation of the EXPOSURE system.

4 Experiments

In the experimental section of this work, we will ﬁrst describe our re-implementation of the
EXPOSURE system. Then we will discuss the DNS traﬃc data we used to make our feature
extraction, followed by a brief model selection made to improve the system’s performance.
Eventually, we will delve into the results section to show how explanations applied in this
context can bring the analysis to the next level.

4.1 Re-implementation of the EXPOSURE system

Dataset. The DNS traﬃc was collected from recursive servers on which, through sniﬀers, we
were able to save the data as .pcap ﬁles for the entire month of January 2021. Given the

4

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

massive amount of traﬃc, summing up to 15 GB of data per day, we ﬁltered out packets whose
label was not known by either black or white lists and domain names that did not resolve
(NXDOMAIN as response code). We used the list of most popular suﬃxes from the Alexa website1
to label benign domains, and the list from DGArchive [17] to ﬂag the malicious samples. The
remaining packets (203, 034 domains, of which 25, 882 benign and 177, 152 malicious - note that
benign domains re-appear much more frequently than the malicious ones, which are almost
always unique) have then been passed through the feature extractor we implemented, and are
distributed through days as shown in Figure 1.

Figure 1: Malicious and legitimate domains for every day.

Model Selection. The authors used a J48 Decision Tree to obtain overall good performances in
the original EXPOSURE work. We additionally bench-marked several models such as Decision
Tree (DT), K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Ada-Boost (ADA),
and Random Forest (RF). After estimating the hyperparameters through a Grid Search, (whose
bests overall have been reported in Section 7) we compared the best models with the best
parameters on two diﬀerent days (i.e., two diﬀerent sample balances). The ﬁrst ROC curve in
Figure 2a was obtained using a more balanced day of data (mid of January days). The second
set of curves in Figure 2b was obtained from a day of data with very few malicious samples,
showing how the performances of the classiﬁers dropped down consistently. Overall, throughout
the most balanced days, RF and ADA have shown to be the more consistent classiﬁers. For
this reason, they have been selected as classiﬁers for the rest of the experiments. We reckon
that the mid-days of capture are also more suitable for the rest of the analysis, and they have
been thereby used for all of the following experiments.

4.2 Feature and Explainability Analysis on EXPOSURE

We now present our results on the analysis of the feature statistics and our insight obtained
through applying explainability techniques to interpret the decisions taken by the machine-
learning models used in our DGA detector. These proposed plots have been implemented
through the Python libraries Seaborn [18] and SHAP [19].
Statistical Analysis. The statistical analysis shows an overview of the correlation and distri-
bution of the features. As shown in Figure 3a, some features like the %of lms and num chars%
when joined, do not separate perfectly the data collection used into the two classes. In par-
ticular, the %of lms reaches a plateau in malicious domains once over 0.8 (bottom-right plot,
depicting the distribution of the feature), which describes how algorithmically-generated do-
mains tend not to have a single meaningful word covering their entire name in most of the
cases, yet there are exceptions in any direction. This might be brought on by the diversity in

1https://www.alexa.com/topsites

5

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

(a) Classifiers trained on day 14, with class distribution un-
balanced but less evident than the other days in the dataset.

(b) The same classifiers trained on day 5, which presents a
highly-unbalanced distribution of the two classes.

Figure 2: The ROC curves obtained using features from day 14 (left) and on day 5 (right).

malware families, where some like “Gameover” DGA used to mix up numbers and characters.
In contrast, others such as “Gozi” used to mix up words from openly accessible documents, such
as the US constitution [20]. In the plot of Figure 3b, depicting time-based features, malicious
domains show a more volatile behavior, which is reasonable if we think about the diversity of
applications in which they can be used. In Figure 8, shown in the Appendix, we can observe
interesting TTL behaviors characterizing the domains. Contrary to the now old-fashioned belief
that a low TTL is only typical for malicious domains [21, 22], as it makes malicious records
stand less in caches, we show that also benign domains can present this behavior depending
on the application in which they are used, e.g., to handle critical resources [23] or for load
balancing purposes [22].

Interpreting Global Explanations. As pointed out in Section 3, diﬀerent models can use
features in diﬀerent ways. Global explanations can uncover these behaviors and let the analyst
be aware of the feature prioritization that a model brings. Some Decision-Tree based classiﬁers
such as ADA and RF, respectively in the summary plots of Figure 4a and Figure 4b, share four
out of the ﬁve top important features, which is likely to be a consequence of their “similar” tree-
based intrinsic nature. In both of them, unique ips notably brings the higher contribution.
In Section 7, we show how other classiﬁers have a low magnitude provided by the unique ips
feature, whilst prioritizing a diverse subset of TTL features. This furtherly shows how it is not
possible to solely rely on statistical analysis to foresee the utilization of the features, as class
separations that at ﬁrst glance look either weak or strong can be subverted.

Partial Dependence Plots show the marginal eﬀect of a single feature globally on the pre-
dictions. Considering a trained classiﬁer (RF in the case of this analysis) and a background
distribution, through SHAP we can assess how the considered feature contributes to classifying
the background samples over their values. An advisable security-related use of these plots can
be to employ a background distribution of malicious samples, thus analyzing to which extent
the feature values contribute to classifying the sample as malicious. The following plots (after
normalization) have been made using a background distribution of 1000 malicious samples on
the RF classiﬁer. Figure 5a shows the PDP of the strongest feature of the model. The plot

6

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

(a) Correlation between name-based features.

(b) Correlation between time-based features.

Figure 3: Density and scatter plots for two pairs of name-based (left) and time-based features (right). The plots are
based on the 12th day of capture, which counts 1324 malicious samples and 1324 benign samples (resized from 6925).

(a) SHAP summary plot of feature contributions on ADA BOOST
classifier.

(b) SHAP summary plot of feature contributions on RANDOM
FOREST classifier.

Figure 4: Global summary plots for ADA (left) and RF (right) classiﬁers.

tends to be a “gentle” step, producing the highest contribution on very low feature values and
the lowest with values going just subtly over the threshold. Very similar behavior to the one of
the unique ips feature is shown in the number of changes in the TTL, depicted in Figure 7 in
Section 7. These plots help, for example, understanding the extent to which features contribute
to the classiﬁcation of the domain as malicious and possibly setting policies and restrictions
based on simply tweakable features, such as the num chars% in Figure 5b.
In this plot, we
can understand how a high rate of numerical characters leads to a solid contribution to the
prediction of the domain as malicious. Likewise, it is surprising that a 20% rate of numerical

7

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

characters in the domain string leads to an even bigger magnitude, which can be caused by the
relevant presence of some malware families not having numbers in their “regex”. In this case,
usage of the proposed security policies for a system hosting an EXPOSURE-like system would
be to allow domain names with numerical characters comprised in between the 20%-40% range.

(a) SHAP PDP on unique ips feature.

(b) SHAP PDP on num chars% feature.

Figure 5: Partial Dependence plots for unique ips (left) and num chars% (right)

Magnifying the Behavior with Local Explanations. Force plots and local explanations
can be considered the local version of summary plots, showing how features contribute locally
to the sample. As Figure 6a shows, the features correctly lead the RF model classiﬁcation of
the domain spring.io as benign. It is not surprising to see a 0 value of unique ips forcing the
classiﬁcation towards malicious, but the rest of the features gently move the prediction towards
the benign class. For malicious samples, instead, the latter plays a major role incorrectly
classifying some malicious samples like mobile.de and qcx.nl, as additionally shown in the
Appendix. In general, the magnitude of the features sticks to what is shown by the summary
plot of Figure 4b for samples correctly classiﬁed. To understand instead which features are
leading the model to misclassify a sample, Figure 6b shows the force plot for the domain
fgc.es, blacklisted but yet misclassiﬁed by our system as benign. The unique ips and %of lms
features correctly move the prediction towards the malicious class, but the values of the TTL-
based features deceive the classiﬁer. Figure 6c, on the other hand, shows how unique ips and
TTL values deviate the prediction of the benign domain towards the malicious class.

Summary of the Results. As a result of the presented experiments, we can reﬂect on the
issues of feature management and hypothetical counteractions. We understand the features
distribution, correlation, and how the DGAs in our traﬃc tend to behave from statistical
analysis. However, the global explanations can turn the table and quantify how the model
perceives the features. Finally, we can see how features drive the sample’s prediction via local
explanations. This ensemble of analysis makes us notice how the overall feature prioritization
depends on both the model used and the considered data, which further proves how context-
dependent such systems’ behavior can be. Hence, an Explainability analysis should always
be used to better portray the big picture of both systems and employed data.
In our case,
the big picture has led to a more prominent analysis of the EXPOSURE system. The TTL
features, the subject of this analysis, sum up to 37.5% of the entire feature set, being 9 out
of 24.
It turns out from our analysis of the explanations that they contribute massively to
the misclassiﬁcation of several samples (such as Figure 6b and Figure 6c) as they cover high-
magnitude roles in the summary plots of Figure 4a and Figure 4b (which are the best classiﬁers

8

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

(a) Force plot of the benign domain sample spring.io, correctly classified as benign.

(b) Force plot of the malicious domain sample fgc.se, misclassified as benign.

(c) Force plot of the benign domain sample topeleven.com, misclassified as malicious.

Figure 6: Local explanations on three domains from the dataset.

overall). This makes this feature set highly powerful for the whole system, yet in its power also
lies a crucial problem. Namely, attackers can manipulate this feature, being completely free
to tune the TTL and balance their caching time (i.e., the likelihood of being detected) with
the chance of evading the classiﬁcation of the system. Having such a relevant portion of the
feature set reserved for values that can be somehow crafted directly by the attackers, it can
serve as a signiﬁcant stepping-stone for attackers. Furthermore, if deployed on a system devised
to manage critical resources, besides evading, some part of the feature set can be overridden
by the context. Some works like [23] point out how security-sensitive systems, e.g., banking
applications, should indeed carefully set their DNS TTL to a low value. The scope of these
assumptions is that a botnet/DGA detector cannot solely rely on accuracy metrics to establish
its eﬃciency, in that analysts also need to be aware of the model, the data, and the context
therein. Explanations can give a huge and crucial hand in this regard, helping prevent major
issues from happening and allowing debugging of the model. Considering our system, through
explanations, we have seen how dangerously inﬂuential TTL-based features are in most of the
models. And considering their extensive use in the feature set, appropriate security measures
should be taken (e.g., reducing their number like for Name-Based features, which are just easily
adjustable as well but sum up to only the 8%). We ﬁrmly believe that through explanations we
can rapidly enhance the usage and trust in AI, as companies can look at such security systems
from a human-readable perspective, and model biases can be analyzed and studied.

5 Related Work

DNS Analysis. Several promising works have striven to tackle botnet/DGA detection dur-
ing the last decade, often showing innovative DNS passive features and methodologies. Some
notable works, besides EXPOSURE, have been Notos [24], where Antonakakis et al.
cre-
ated the ﬁrst relevant and eﬃcient reputation system for domains from various data sources.
Pleiades [25], where the authors focused on NXDOMAIN records to both cluster domains and
classify DGAs by looking at the strings association. Finally, in FANCI [6], Schueppen et al.
developed a detector based on a small feature set such as the EXPOSURE one, though focusing
only on NXDOMAIN passive data. All of these works have reached comparable performances
in diﬀerent settings. None of them, though, has focused their interest on the explainability

9

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

of such a critical application. The only works to have addressed such problems focused on
multiclass classiﬁcation problems with deep learning approaches, thus classifying the malicious
domains with family pairing.
In [26], Becker et al. proposed a visual analytics system for
Deep Learning (DL) models, providing graphical insights on statistical properties of the do-
main name string. Drichel et al., in [27], brieﬂy highlighted some string-wise interpretations for
DL models starting from the misclassiﬁed samples. In contrast, in [28], Drichel et al. proposed
feature-based classiﬁers based on string features for multiclass classiﬁcation, with the purpose
of improving explainability. Firstly, none of these three works focused on passive DNS data,
choosing string-based features to ease the computational burden. Secondly, none developed
explanatory analysis, rather focusing on how the model could be made more explainable or
at most on how to visualize a few string patterns from a DL model. Our work focuses on
passive DNS traﬃc data, analyzing features from a comprehensive viewpoint and not limiting
them to the human-readable string features. Additionally, we propose both local and global
explanations, concretely enhancing the awareness of how a model behaves in such a context.

In [29], Ribeiro et al. proposed LIME (Local Interpretable
Explainability Techniques.
Model-Agnostic Explanations), an explainability method conceived as a local model learning
and approximating around the prediction. Despite its wide use, several concerns about stability
and consistency have been addressed towards LIME [15, 30]. Considering that SHAP is a more
reliable tool, we have driven our choice towards its use in our explainability work.

6 Conclusions and Future Work

In this work, we proposed an explanatory analysis of ML classiﬁers devised for botnet/DGA
detection. Starting from the implementation of the EXPOSURE feature set on our traﬃc data,
we have shown how from prior statistical assumptions on the malware behavior within the net-
work, a model can interpret features in its way globally, thus prioritizing certain features rather
than others that were prevented, also demonstrating how diﬀerent models can have a diﬀer-
ent feature conception, to which eventually we analysts should adapt and debug accordingly.
Locally, we have seen how certain features can contribute and how explanations can make the
analysts and users aware of the single decisions and motivations behind misclassiﬁed samples.
Through these analyses, we raised concerns about how the feature and model can be biased by
the context in which the systems are both trained and deployed. And our analysis makes the
comprehension of such contexts move fast forward towards favoring the employment of such
systems, as they can be ﬁrstly interpreted and adapted and subsequently accepted. In this re-
gard, several advances of this work can be developed aiming at fairness and legal regularization
of the detectors through explanations and, if possible, bringing them into debugging/pipelining
processes to obtain an eﬃcient and explainable system. Additionally, they can be instrumental
when humans want to be involved in the decision-making. All in all, this work demonstrated
how powerful explanations can be and how security, debugging, interpretability, and fairness
can be brought to the next level by the application of ML to detection, where security has to
be assessed and interpreted through the process chain.

Acknowledgments

This work has been partly supported by the PRIN 2017 project RexLearn, funded by the Italian
Ministry of Education, University and Research (grant no. 2017TWNMH2); and by the project
TESTABLE (grant no. 101019206), under the EU’s H2020 research and innovation programme.

10

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

References

[1] Z.M. Smith, E. Lostri, and McAfee (Firm). The Hidden Costs of Cybercrime. McAfee, 2020.
[2] Ramneek Puri. Bots &; Botnet: An Overview. Elsevier, page 17, 2003.
[3] William Salusky and Robert Danford. Know Your Enemy: Fast-Flux Service Networks. In The

Honeypot Project, July 2007.

[4] Brett Stone-Gross, Marco Cova, Lorenzo Cavallaro, Bob Gilbert, Martin Szydlowski, Richard
Kemmerer, Christopher Kruegel, and Giovanni Vigna. Your botnet is my botnet: analysis of a
botnet takeover. In Proceedings of the 16th ACM conference on Computer and communications
security - CCS ’09, page 635, Chicago, Illinois, USA, 2009. ACM Press.

[5] Leyla Bilge, Engin Kirda, Christopher Kruegel, and Marco Balduzzi. Exposure: Finding malicious
In Network and Distributed System Security Symposium

domains using passive dns analysis.
(NDSS), 2011.

[6] Samuel Sch¨uppen, Dominik Teubert, Patrick Herrmann, and Ulrike Meyer. FANCI : Feature-
based automated nxdomain classiﬁcation and intelligence. In 27th USENIX Security Symposium
(USENIX Security 18), pages 1165–1181, Baltimore, MD, 2018. USENIX Association.

[7] C. Zheng, Q. Qiang, T. Zang, W. Chao, and Y. Zhou. Themis: A Novel Detection Approach for
Detecting Mixed Algorithmically Generated Domains. In 2019 15th International Conference on
Mobile Ad-Hoc and Sensor Networks (MSN), pages 259–264, December 2019.

[8] Stefano Schiavoni, Federico Maggi, Lorenzo Cavallaro, and Stefano Zanero. Phoenix: Dga-based
botnet tracking and intelligence. In Detection of intrusions and malware, and vulnerability assess-
ment, pages 192–211. Springer, 2014.

[9] Leyla Bilge, Sevil Sen, Davide Balzarotti, Engin Kirda, and Christopher Kruegel. Exposure: A
passive dns analysis service to detect and report malicious domains. ACM Trans. Inf. Syst. Secur.,
16(4), apr 2014.

[10] P. V. Mockapetris. Domain names - implementation and speciﬁcation.
[11] P. V. Mockapetris. Domain names - concepts and facilities.
[12] Tim Miller.

Intelligence:
arXiv:1706.07269 [cs], August 2018. arXiv: 1706.07269.

Explanation in Artiﬁcial

Insights from the Social Sciences.

[13] Daniel Arp, Erwin Quiring, Feargus Pendlebury, Alexander Warnecke, Fabio Pierazzi, Christian
Wressnegger, Lorenzo Cavallaro, and Konrad Rieck. Dos and don’ts of machine learning in com-
puter security. In Proceedings of the 31st USENIX Security Symposium, 10 2020.
2.5 Properties of Explanations
https://christophm.github.io/interpretable-ml-book/, 2019.

Interpretable Machine Learning.

[14] Christoph Molnar.

|

[15] Scott Lundberg and Su-In Lee. A Uniﬁed Approach to Interpreting Model Predictions.

arXiv:1705.07874 [cs, stat], November 2017. arXiv: 1705.07874.

[16] Sergiu Hart. Shapley Value. In The New Palgrave Dictionary of Economics, pages 1–5. Palgrave

Macmillan UK, London, 2017.

[17] Daniel Plohmann, Khaled Yakdan, Michael Klatt, Johannes Bader, and Elmar Gerhards-Padilla.
A comprehensive measurement study of domain generating malware. In 25th USENIX Security
Symposium (USENIX Security 16), pages 263–278, 2016.

[18] Michael L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software,

6(60):3021, 2021.

[19] An introduction to explainable AI with Shapley values — SHAP latest documentation.
[20] Fraunhofer FKIE Daniel Plohmann. Dgarchive. https://dgarchive.caad.fkie.fraunhofer.de.
[21] Ricardo Villamarin-Salomon and Jose Carlos Brustoloni. Identifying botnets using anomaly de-
tection techniques applied to dns traﬃc. In 2008 5th IEEE Consumer Communications and Net-
working Conference, pages 476–481, 2008.

[22] Kamal Alieyan, Ammar ALmomani, Ahmad Manasrah, and Mohammed M. Kadhum. A survey of

11

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

botnet detection based on DNS. Neural Computing and Applications, 28(7):1541–1558, July 2017.
[23] N. Vlajic, M. Andrade, and U. T. Nguyen. The Role of DNS TTL Values in Potential DDoS
Attacks: What Do the Major Banks Know About It? Procedia Computer Science, 10:466–473,
January 2012.

[24] Manos Antonakakis, Roberto Perdisci, David Dagon, Wenke Lee, and Nick Feamster. Building a
dynamic reputation system for dns. In USENIX security symposium, pages 273–290, 2010.
[25] Manos Antonakakis, Roberto Perdisci, Yacin Nadji, Nikolaos Vasiloglou, Saeed Abu-Nimeh, Wenke
Lee, and David Dagon. From throw-away traﬃc to bots: detecting the rise of dga-based malware.
In Presented as part of the 21st USENIX Security Symposium (USENIX Security 12), pages 491–
506, 2012.

[26] Franziska Becker, Arthur Drichel, Christoph M¨uller, and Thomas Ertl. Interpretable visualizations
of deep neural networks for domain generation algorithm detection. In 2020 IEEE Symposium on
Visualization for Cyber Security (VizSec), pages 25–29, 2020.

[27] Arthur Drichel, Ulrike Meyer, Samuel Sch¨uppen, and Dominik Teubert. Analyzing the real-world
applicability of dga classiﬁers. In Proceedings of the 15th International Conference on Availability,
Reliability and Security, pages 1–11, 08 2020.

[28] Arthur Drichel, Nils Faerber, and Ulrike Meyer. First step towards explainable dga multiclass
classiﬁcation. In Proceedings of the 16th International Conference on Availability, Reliability and
Security, pages 1–13, 08 2021.

[29] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”Why Should I Trust You?”: Explaining
the Predictions of Any Classiﬁer. arXiv:1602.04938 [cs, stat], February 2016. arXiv: 1602.04938
version: 1.

[30] Zhengze Zhou, Giles Hooker, and Fei Wang. S-LIME: Stabilized-LIME for Model Explanation.
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages
2429–2438, August 2021. arXiv: 2106.07875.

12

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

7 Appendix

In this additional section, we show several plots that have been cited in the previous sections
and that we believe can support the comprehension of the work.

Grid Search Results. Using the Scikit-Learn Python suite, we optimized the parameters
through the GridSearchCV API. The results of the optimization have been reported for com-
pleteness in Listing 1.

TTL Features plots. Having focused the discussion of the explainability analysis almost
entirely on the TTL features, there are some additional plots that can point out interesting
behaviors, such as Figure 8, which shows the statistical analysis of the ﬁrst 4 TTL-based
In Figure 7 instead, we can see how low changes in the TTL values mean a low
features.
contribution to the classiﬁcation of the sample as malicious and vice versa.
Additional Summary Plots. Figure 9 shows how unique ips are much less considered than
the TTL-based features by the KNN classiﬁer, which again shows how models are as diverse as
they are. The same goes for the SVC classiﬁer in Figure 10, which once again does not employ
the unique ips feature as much as the Decision-Tree based classiﬁers do.
Additional Force Plots. The plots of Figure 11 show a variety of samples either correctly
classiﬁed or misclassiﬁed by the RF model, demonstrating practically how the most relevant
features can play a major role in any classiﬁcation scenario, either in the wrong or correct way.

Listing 1: Grid Search Results

1

2

3

4

5

6

7

8

9

10

# RANDOM FOREST parameters
{’criterion’: ’entropy’, ’max_depth’: 20, ’n_estimators’: 125}
# ADA-BOOST parameters
{’algorithm’: ’SAMME’, ’n_estimators’: 175}
# K-NEAREST NEIGHBORS parameters
{’n_neighbors’: 13, ’weights’: ’distance’}
# DECISION TREE parameters
{’criterion’: ’gini’, ’max_depth’: 5}
# SVC RBF parameters
{’C’: 297.63514416313194, ’gamma’: 0.6951927961775591}

Figure 7: SHAP PDP on ttl changes feature.

13

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

Figure 8: Correlation between the ﬁrst four TTL-based features.

14

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

Figure 9: SHAP summary plot of feature contributions on KNN classiﬁer.

15

Explaining Machine Learning DGA Detectors from DNS Traﬃc Data

Piras et al.

Figure 10: SHAP summary plot of feature contributions on SVC classiﬁer.

(a) Force plot of the benign domain sample mobile.de, correctly classified as benign.

(b) Force plot of the malicious domain sample qcx.nl, correctly classified as malicious.

(c) Force plot of the malicious domain sample szx.pw, misclassified as benign.

Figure 11: Local explanations on other three domains from the dataset.

16

