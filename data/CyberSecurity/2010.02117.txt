0
2
0
2

t
c
O
5

]

C
H
.
s
c
[

1
v
7
1
1
2
0
.
0
1
0
2
:
v
i
X
r
a

Statistical Reliability of 10 Years of Cyber
Security User Studies(cid:63)
[Extended Version]

Thomas Groß

School of Computing
Newcastle University, UK
thomas.gross@newcastle.ac.uk

Abstract. Background. In recent years, cyber security security user
studies have been appraised in meta-research, mostly focusing on the
completeness of their statistical inferences and the ﬁdelity of their statis-
tical reporting. However, estimates of the ﬁeld’s distribution of statistical
power and its publication bias have not received much attention.
Aim. In this study, we aim to estimate the eﬀect sizes and their stan-
dard errors present as well as the implications on statistical power and
publication bias.
Method. We built upon a published systematic literature review of 146
user studies in cyber security (2006–2016). We took into account 431
statistical inferences including t-, χ2-, r-, one-way F -tests, and Z-tests.
In addition, we coded the corresponding total sample sizes, group sizes
and test families. Given these data, we established the observed eﬀect
sizes and evaluated the overall publication bias. We further computed
the statistical power vis-`a-vis of parametrized population thresholds to
gain unbiased estimates of the power distribution.
Results. We obtained a distribution of eﬀect sizes and their conversion
into comparable log odds ratios together with their standard errors. We,
further, gained funnel-plot estimates of the publication bias present in
the sample as well as insights into the power distribution and its conse-
quences.
Conclusions. Through the lenses of power and publication bias, we shed
light on the statistical reliability of the studies in the ﬁeld. The upshot
of this introspection is practical recommendations on conducting and
evaluating studies to advance the ﬁeld.

Keywords: User studies · SLR · Cyber security · Eﬀect Estimation ·
Statistical Power · Publication Bias · Winner’s Curse

1

Introduction

Cyber security user studies and quantitative studies in socio-technical aspects
of security in general often rely on statistical inferences to make their case that

(cid:63) Open Science Framework: osf.io/bcyte. The deﬁnitive short version of this paper
is to appear in Proceedings of the 10th International Workshop on Socio-Technical
Aspects in Security (STAST 2020), LNCS 11739, Springer, 2020.

 
 
 
 
 
 
observed eﬀects are not down to chance. They are to separate the wheat from
the chaﬀ. Indeed, null hypothesis signiﬁcance testing and p-values indicating
statistical signiﬁcance hold great sway in the community. While the studies in
the ﬁeld have been appraised in recent years on the completeness and ﬁdelity of
their statistical reporting, we may still ask how reliable the underlying statistical
inferences really are.

“To what extent can we rely on reported eﬀects? ” This question can take
multiple shapes. First, we may consider the magnitude of observed eﬀects. While
a statement of statistical signiﬁcance is dependent on the sample size at which
the inference was obtained, the magnitude of an eﬀect, its eﬀect size, informs
us whether an eﬀect is practically relevant—or not. While small eﬀects might
not make much diﬀerence in practice and might not be economical to pursue,
large eﬀects estimated with conﬁdence can guide us to the interventions that are
likely carrying considerable weight in socio-technical systems.

Indeed, a second dimension of reliability pertains to the conﬁdence we have in
observed eﬀects, typically measured with 95% conﬁdence intervals. Here, we are
interested how tightly the conﬁdence interval envelops the eﬀect point estimate.
The rationale behind such a conﬁdence interval is that if an experiment were
repeated many times, we would expect 95% of the observed eﬀect estimates to
be within the stated conﬁdence intervals. Wide intervals, thereby, give us little
conﬁdence in the accuracy of an estimation procedure.

This consideration is exacerbated if a study conducted many tests in the same
test family. Given the risk of multiple comparisons to amplify false-positive rates,
we would need to adjust the conﬁdence intervals accounting for the multiplicity
and, hence, be prepared to gain even less conﬁdence in the ﬁndings.

Third, we may consider statistical power, the likelihood of ﬁnding an eﬀect
that is present in reality. To put it in precise terms, it is the likelihood of rejecting
a null hypothesis when it is, in fact, false—the complement of the false negative
rate. At the same time, statistical power also impacts the likelihood that a posi-
tive report is actually true, hence further impacts the reliability of a ﬁnding. The
power distribution, further, oﬀers a ﬁrst assessment on the statistical reliability
of the ﬁeld.

Finally, we expand on the reliability of the ﬁeld in terms of evaluating re-
search biases that could undermine results. Three biases of interest are (i) the
publication bias [27,21,12,30], (ii) the related winner’s curse [3], and (iii) sig-
niﬁcance chasing [20]. The publication bias, on the one hand, refers to the phe-
nomenon that the outcome of a study determines the decision to publish. Hence,
statistically signiﬁcant positive results are more likely to be published, than null
results—even if null results live up to the same scientiﬁc rigor and possibly
carry more information for falsiﬁcation. Furthermore, researchers might be in-
centivized to engage in research practices that ensure reporting of statistically
signiﬁcant results, introducing biases towards questionable research practices.

The winner’s curse, on the other hand, refers to the phenomenon that under-
powered studies tend to report more extreme eﬀects with statistical signiﬁcance,
hence tend to introducing a bias in the mean eﬀect estimates in the ﬁeld.

Signiﬁcance chasing, on the other hand, considers the phenomenon of authors
producing excess signiﬁcant results, especially when they their results are close
to a stated signiﬁcance level.

To the best of our knowledge, these questions on reliability of statistical in-
ferences in cyber security user studies have not been systematically answered, to
date. Coopamootoo and Groß [6] oﬀered a manual coding of syntactic complete-
ness indicators on studies sampled in a systematic literature review (SLR) of 10
years of cyber security user studies, while also commenting on post-hoc power
estimates for a small sub-sample. Groß [15] investigated the ﬁdelity of statistical
test reporting along with an overview of multiple-comparison corrections and
the identiﬁcation of computation and decision errors. While we chose to base
our analysis on the same published SLR sample, we close the research gap by
creating a sound empirical foundation to estimate eﬀect sizes, their standard er-
rors and conﬁdence intervals, by establishing power simulations vs. typical eﬀect
size thresholds, by investigating publication bias, winner’s curse, and signiﬁcance
chasing.

Our Contributions. We are the ﬁrst to estimate a large number (n = 431) of
heterogenous eﬀect sizes from cyber security user studies with their conﬁdence
intervals. Based on this estimation, we are able to show that a considerable
number of tests executed in the ﬁeld are underpowered, leaving results in ques-
tion. This holds especially for small studies which computed a large number of
tests at vanishingly low power. Furthermore, we are able to show that the re-
ported eﬀects of underpowered studies are especially susceptible to falter under
Multiple-Comparison Corrections (MCC), while adequately powered studies are
robust to MCC.

We are the ﬁrst to quantify empirically that a publication bias is present in
the ﬁeld of cyber security user studies. We can further evidence that the ﬁeld
suﬀers from the over-estimated eﬀect sizes at low power, the winner’s curse. We
are the ﬁrst to explore this ﬁeld for signiﬁcance chasing, ﬁnding indications of
it being present especially in the presence or absence of MCC. We conclude our
study with practical and empirically grounded recommendations for researchers,
reviewers and funders.

2 Background

2.1 Statistical Inferences and Null Hypothesis Signiﬁcance Testing

Based on a—necessarily a priori —speciﬁed null hypothesis (and alternative hy-
pothesis) and a given signiﬁcance level α, statistical inference with null hypothesis
signiﬁcance testing [18, pp. 163] sets out to establish how surprising an obtained
observation D is, assuming the null hypothesis being true. This is facilitated
by means of a test statistic that relates observations to appropriate probability
distributions. It is inherent to the method that the statistical hypothesis must
be ﬁxed, before the sample is examined.

The p-value, then, is the likelihood of obtaining an observation as extreme
as or more extreme than D, contingent on the null hypothesis being true, all
assumption of the test statistic being fulﬁlled, the sample being drawn randomly,
etc. Indeed, not heeding the assumptions of the test statistic is one of the more
subtle ways how the process can fail. Null hypothesis signiﬁcance testing and
p-values are often subject to fallacies, well documented by Nickerson [24].

Statistical inferences carry the likelihood of a false positive or Type I error [18,
pp. 168]. They are impacted, hence, by multiplicity, that is, the phenomenon that
computing multiple statistical tests on a test family inﬂates the family-wise error
rate. To mitigate this eﬀect, it is prudent practice to employ multiple-comparison
corrections (MCC) [18, pp. 415]. The Bonferroni correction we use here is the
most conservative one, adjusting the signiﬁcance level α by dividing it by the
number of tests computed in the test family.

2.2 Eﬀect Sizes and Conﬁdence Intervals

We brieﬂy introduce estimation theory [10] as a complement to signiﬁcance test-
ing and as a key tool for this study. An observed eﬀect size (ES) is a point
estimate of the magnitude of an observed eﬀect. Its conﬁdence interval (CI) is
the corresponding interval estimate [18, pp. 313]. For instance, if we consider
the popular 95% conﬁdence interval on an eﬀect size, it indicates that if an ex-
periment were repeated inﬁnitely many times, we would expect that the point
estimate on the population eﬀect were within the respective conﬁdence interval
95% of the cases. The standard error of an ES is equally a measure of the ef-
fects uncertainty and monotonously related to the width of the corresponding
conﬁdence interval.

Notably, conﬁdence intervals are often misused or misinterpreted [17,23]. For
instance, they do not assert that the population eﬀect is within a point estimate’s
CI with 95% likelihood.

However, used correctly, eﬀect sizes and their conﬁdence intervals are useful in
establishing the practical relevance of and conﬁdence in an eﬀect [14]. They are,
thereby, recommended as minimum requirement for standard reporting [11], such
as by the APA guidelines [1,26]. Whereas a statement of statistical signiﬁcance or
p-value largely gives a binary answer, an eﬀect size quantiﬁes the eﬀect observed
and, thereby, indicates what its impact in practice might be.

2.3 Statistical Power

In simple terms, statistical power (1−β) [4] is the probability that a test correctly
rejects the null hypothesis, if the null hypothesis is false in reality. Hence, power
is the likelihood not to commit a false negative or Type II error.

It should not go unnoticed that power also has an impact on the probability
whether a positively reported result is actually true in reality, often referred to
as Positive Predictive Value (PPV) [19]. The lower the statistical power, the less
likely a positive report is true in reality. Hence, a ﬁeld aﬀected by predominately
low power is said to suﬀer from a power failure [3].

Statistical power is largely determined by signiﬁcance level, sample size, and
the population eﬀect size θ. A priori statistical power of a test statistic is esti-
mated by a power analysis [18, pp. 372] on the sample size employed vis-`a-vis of
the anticipated eﬀect size, given a speciﬁed signiﬁcance level α and target power
1 − β.

Post-hoc statistical power [18, p. 391], that is, computed on observed eﬀect
sizes after the face, is not only considered redundant to the p-value and conﬁ-
dence intervals on the eﬀect sizes, but also cautioned against as treacherously
misleading: It tends to overestimate the statistical power because it discounts the
power lost in the study execution and because it is vulnerable to being inﬂated
by over-estimated observed eﬀect sizes. Hence, especially small under-powered
studies with erratic eﬀect size estimates tend to yield a biased post-hoc power.
Hence, post-hoc power statements are best disregarded.

We oﬀer a less biased alternative approach in power simulation. In that, we
specify standard eﬀect size thresholds, that is, we parametrize the analysis on
assumed average eﬀect sizes found in a ﬁeld. We then compute the statistical
power of the studies given on their reported sample size against those thresh-
olds. As the true average eﬀect sizes of our ﬁeld are unknown, we oﬀer power
simulations for a range of typical eﬀect size thresholds.

2.4 Research Biases

Naturally, even well-executed studies can be impacted by a range of biases on
per-study level. A cursory spotlighting of biases might include: (i) sampling
bias, from inappropriately selected sampling frames and non-random sampling
methods, (ii) systematic and order bias, from ﬂawed or non-random assignment,
(iii) measurement bias, from ﬂawed instruments and their application, (iv) ex-
perimenter and conﬁrmation bias, from insuﬃcient blinding, and (v) a whole
host of biases from poor study and statistical analysis execution as well as ques-
tionable research practices. In this study, we consider biases of a ﬁeld, instead.
We consider three biases, speciﬁcally: (i) the publication bias, (ii) the winner’s
curse, and (iii) signiﬁcance chasing.

The publication bias [27,21,12,30] refers to the phenomenon that the publi-
cation of studies may be contingent on their positive results, hence condemning
null and unfavorable results to the ﬁle-drawer [27,28].

The winner’s curse [3] is a speciﬁc kind of publication bias referring to the
phenomenon that low-power studies only reach statistically signiﬁcant results on
large eﬀects and, thereby, tend to overestimate the observed eﬀect sizes. They,
hence, perpetuate inﬂated eﬀect estimates in the ﬁeld.

We chose them as lens for this paper because they both operate on the
interrelation between sample size (impacting standard error and power) and
eﬀects observed and emphasize diﬀerent aspects of the overall phenomenon. The
publication bias is typically visualized with funnel plots [21], which pit observed
eﬀect sizes against their standard errors. We shall follow Sterne and Egger’s
suggestion [31] on using log odds ratios as best suited x-axis. If no publication
bias were present, funnel plots would be symmetrical. Hence, an asymmetry is

an indication of bias. This asymmetry is tested with the non-parametric rank
correlation coeﬃcient Kendall’s τ [2]. We note that funnel-plots as analysis tools
can be impacted by the heterogeneity of the eﬀect sizes investigated [32] and,
hence, need to be taken with a grain of salt.

Signiﬁcance chasing [20] refers to the presence of an excess of statistically
signiﬁcant results especially around a threshold signiﬁcance level, such as α =
.05. The excess of statistically signiﬁcant results is established as follows:

A :=

(cid:20) (O − E)2
E

+

(O − E)2
(n − E)

(cid:21)

,

where A is approximated by a chi2 distribution with df = 1, O is the number
of observed signiﬁcant results, n is the overall number of tests, and

n
(cid:88)

(1 − βi).

E =

i=1

For a scenario in which a sound estimate of the population eﬀect ˆθ is unknown,
Ioannidis and Trikalinos [20] proposed to evaluate the signiﬁcance chasing over
a range of eﬀect size thresholds, which we shall adopt here. Signiﬁcance chasing
is shown graphically by plotting the p-value of the signiﬁcance chasing χ2-test
by the signiﬁcance level α. Apart from the research question not having been
registered, the analysis is exploratory because of the nature of the method it-
self as explained by Ioannidis and Trikalinos and because our sample exhibits
considerable heterogeneity.

3 Related Works

3.1 Appraisal of the Field

Usable security, socio-technical aspects in security, human dimensions of cyber
security and evidence-based methods of security are all young ﬁelds. The System-
atic Literature Review (SLR) by Coopamootoo and Groß [6], hence, zeroed in on
cyber security user studies published in the 10 years 2006–2016. The ﬁeld has un-
dergone some appraisal and self-reﬂection. The mentioned Coopamootoo-Groß
SLR considered completeness indicators for statistical inference, syntactically
codeable from a study’s reporting [8]. These were subsequently described in a
reporting toolset [9]. The authors found appreciable weaknesses in the ﬁeld, even
if there were cases of studies excelling in their rigor. Operating from the same
SLR sample, Groß [15,16] investigated the ﬁdelity of statistical reporting, on
completeness of the reports as well as the correctness of the reported p-values,
ﬁnding computation and decision errors in published works relatively stable over
time with minor diﬀerences between venues.

3.2 Guidelines

Over the timeframe covered by the aforementioned SLR, a number of authors
oﬀered recommendations for dependable, rigorous experimentation pertaining
to this study. Peisert and Bishop [25] considered the scientiﬁc design of security
experiments. Maxion [22] discussed dependable experimentation, summarizing
classical features of sound experiment design. Schechter [29] spoke from experi-
ence in the SOUPS program committee, oﬀering recommendations for authors.
His considerations on multiple-comparison corrections and adherence to statisti-
cal assumptions foreshadow recommendations we will make. Coopamootoo and
Groß [7] summarized research methodology, largely focusing on quantitative and
evidence-based methods, discussing null hypothesis signiﬁcance testing, eﬀect
size estimation, and statistical power, among other things.

4 Aims

Eﬀect Sizes. As a stepping stone, we intend to estimate observed eﬀect sizes and
their standard errors in a standardized format (log odds ratios).

RQ 1 (Eﬀect Sizes and their Conﬁdence) What is the distribution of ob-
served eﬀect sizes and their 95% conﬁdence intervals? How are the conﬁdence
intervals aﬀected by multiple-comparison corrections?

Hmcc,0: The marginal proportions of tests’ statistical signiﬁcance are equal irre-

spective of per-study family-wise multiple-comparison corrections.

Hmcc,1: Per-study family-wise multiple-comparison corrections impact the marginal

proportions of tests’ statistical signiﬁcance.

Statistical Power. We inquire about the statistical power of studies independent
from their possibly biased observed eﬀect size estimates.

RQ 2 (Statistical Power) What is the distribution of statistical power vis-`a-
vis parameterized eﬀect size thresholds? As an upper bound achievable with given
sample sizes as well as for the actual tests employed?

Given the unreliability of post-hoc power analysis, we pit the sample sizes em-
ployed by the studies and individual tests against the small, medium, and large
eﬀect size thresholds according to Cohen [4]. The actual thresholds will diﬀer
depending on the type of the eﬀect size.

Publication Bias. We intend to inspect the relation between eﬀect sizes and
standard errors with funnel plots [21], asking the question:

RQ 3 (Publication Bias) To what extent does the ﬁeld exhibit signs of publi-
cation bias measured in terms of relation between eﬀect sizes and standard errors
as well as asymmetry?

We can test statistically for the presence of asymmetry [2] as indicator of

publication bias, yielding the following hypotheses:
Hbias,0: There is no asymmetry measured as rank correlation between eﬀect sizes

and their standard errors.

Hbias,1: There an asymmetry measured as rank correlation between eﬀect sizes

and their standard errors.

The Winner’s Curse. We are interested whether low-powered studies exhibit
inﬂated eﬀect sizes and ask:

RQ 4 (Winner’s Curse) What is the relation between simulated statistical
power (only dependent on group sizes) and observed eﬀect sizes?

Hwc,0: Simulated power and observed eﬀect size are independent.
Hwc,1: There is a negative correlation between simulated power and observed

eﬀect size.

Hence, the winner’s curse is evidenced iﬀ greater power correlates with smaller
eﬀects.

Signiﬁcance Chasing. As an exploratory analysis not pre-registered, we will eval-
uate the presence of a phenomenon Ioannidis and Trikalinos [20] called “signiﬁ-
cance chasing.”

RQ 5 (Signiﬁcance Chasing) To what extent is there an excess of signiﬁcant
results? To what extent is this excess pronounced around a signiﬁcance level of
α = .05?

The former question will be tested with the statistical hypothesis:
Hesr,0: The observed number O of “positive” results is equal to the expected

number of signiﬁcant results E = (cid:80)n

i=1(1 − βi).

Hesr,1: The observed number O of “positive” results is diﬀerent from the ex-

pected number E.

Due to the low power of the test, we follow the recommendation [20] to test these
hypotheses with a signiﬁcance level of αesr = .10. We will answer this question
with and without multiple-comparison corrections.

5 Method

This study was registered on the Open Science Framework (OSF)1, before its
statistical inferences commenced. Computations of statistics, graphs and tables
are done in R with the packages statcheck, metafor, esc, compute.es, pwr. Their
results are woven into this report with knitr. Statistics are computed as two-tailed
with α = .05 as reference signiﬁcance level. Multiple-comparison corrections are
computed with the Bonferroni method, adjusting the signiﬁcance level used with
the number of members of the test family.

1 https://osf.io/bcyte/

5.1 Sample

The sample for this study is based on a 2016/17 Systematic Literature Re-
view (SLR) conducted by Coopamootoo and Groß [6]. This underlying SLR, its
search, inclusion and exclusion criteria are reported in short form by Groß [15]
are included in this study’s OSF Repository. We have chosen this SLR on the
one hand, because its search strategy, inclusion and exclusion criteria are explic-
itly documented supporting its reproducibility and representativeness; the list
of included papers is published. On the other hand, we have chosen it as sample,
because there have been related analyses on qualitatively coded completeness in-
dicators as well as statistical reporting ﬁdelity [15] already. Therefore, we extend
a common touchstone for the ﬁeld. The overall SLR sample included N = 146
cyber security user studies. Therein, Groß [15] identiﬁed 112 studies with valid
statistical reporting in the form of triplets of test statistic, degrees of freedom,
and p-value. In this study, we extract eﬀect sizes for t-, χ2-, r-, one-way F -tests,
and Z-tests, complementing automated with manual extraction.

5.2 Procedure

We outlined the overall procedure in Figure 1 and will describe the analysis stages
depicted in dashed rounded boxes in turn. Figure 10 in Appendix B illustrates
how sampled and parametrized data relate to the diﬀerent analyses.

Fig. 1. Flow chart of the analysis procedure

PAPERpwrWithMCCstatcheckTest Statistic ExtractionPower SimulationEffect SizeThresholdsValidly ReportedTest StatisticsPowerDistributionstatcheckAutomatedExtractionManual CodingTest Statistics dfs, p-valuesMeansSDsEffect SizeStandard ErrorSample SizesGroup SizesTestFamiliesWithMCCEstimationConfidence IntervalsFunnelPlotsmetaforBias AnalysisranktestAsymmetryEvaluationPublicationBiasBias Scatter PlotsWinner’s Curse AnalysisPower/ESCorrelation/RegressionWinner’sCurseES/CIDistributionTable 1. Eﬀect size thresholds for various statistics and eﬀect size (ES) types [4]

ES Type

Statistic

Cohen’s d
Pearson’s r
Cohen’s w
Cohen’s f
log(OR)

t
r
χ2
F

Threshold

Small Medium Large

0.20
0.10
0.10
0.10
0.36

0.50 0.80
0.30 0.50
0.30 0.50
0.25 0.40
0.91 1.45

Automated Test Statistic Extraction. We analyzed the SLR sample with R pack-
age statcheck proposed by Epskamp and Nuijten [13]. We obtained cases on
test statistic type, degrees of freedom, value of the test statistic and p-value
along with a correctness analysis. This extraction covered correctly reported
test statistics (by APA guidelines) and t-, χ2-, r-, F -tests, and Z-tests at that.

Manual Coding. For all papers in the SLR, we coded the overall sample size, use
of Amazon Mechanical Turk as sampling platform, and the presence of multiple-
comparison corrections. For each statistical test, we also coded group sizes, test
statistics, degrees of freedom, p-values, means and standard deviations if appli-
cable as well as test families. For the coding of test families, we distinguished
diﬀerent studies reported in papers, test types as well as dimensions investi-
gated. For instance, if a study computed multiple One-Way ANOVAs on the
same sample across the same dimensions but possibly on diﬀerent variables, we
would consider that a test family. We would consider pair-wise post-hoc com-
parisons a separate test family.

Test Exclusion. To put all eﬀect sizes on even footing, we excluded tests violating
assumption and ones not constituting one-way comparisons.

Power Simulation. We conducted a power simulation, that is, we speciﬁed eﬀect
size thresholds for various eﬀect size types according to the classes proposed by
Cohen [5]. Table 1 summarizes corresponding thresholds.

Given the sample sizes obtained in the coding, we then computed the a
priori power against those thresholds with the R package pwr, which is inde-
pendent from possible over-estimation of observed eﬀect sizes. We further com-
puted power analyses based on group sizes for reported tests, including a power
adjusted for multiple comparisons in studies’ test families with a Bonferroni
correction. We reported those analyses per test statistic type.

Estimation. We computed a systematic estimation of observed eﬀect sizes, their
standard errors and conﬁdence intervals. This estimation was either based on
test statistics, their degrees of freedom and group sizes used for the test or on
summary statistics such as reported means, standard deviations and group sizes.
We conducted the estimation with the R packages esc and compute.es for cases

in which only test statistics were available and with the package metafor if we
worked with summary statistics (e.g., means and standard deviations). As part
of this estimation stage, we also estimated 95% conﬁdence intervals (with and
without multiple-comparison corrections).

Publication Bias Analysis. We used the R package metafor to compute analyses
on the publication bias. In particular, we produced funnel plots on eﬀect sizes
and their standard errors [21]. For this analysis, we converted all eﬀect sizes and
standard errors irrespective of their origin to log odds ratios as the predomi-
nant eﬀect-size form for funnel plots [31]. Following the method of Begg and
Mazumdar [2], we evaluated a rank correlation test to ascertain the presence of
asymmetry.

Winner’s Curse Analysis. To analyze for the winner’s curse, we created scat-
terplots that pitted the simulated power of reported tests against the observed
eﬀect sizes extracted from the papers. We applied a Loess smoothing to illustrate
the bias in the distribution. Finally, we computed a Kendall’s τ rank correlation
to show the relationship between absolute eﬀect size and power. We employed
a robust linear regression of the R package MASS using an iterated re-weighted
least squares (IWLS) ﬁtting to estimate the mean expected eﬀect size of the ﬁeld
at 100% power.

Signiﬁcance Chasing Analysis. We analyzed for signiﬁcance chasing by comput-
ing the expected number of signiﬁcant results E from the simulated power 1 − β
of the respective tests against speciﬁed eﬀect size thresholds deﬁned in Table 1.
The observed number of “positive” results was drawn from the signiﬁcance of
the eﬀects with respect to a parametrized signiﬁcance level α. We computed the
χ2 signiﬁcance chasing test with df = 1 and αesr as deﬁned in Section 2.4.

6 Results

6.1 Sample

The sample was reﬁned in multiple stages, ﬁrst establishing papers that are
candidates for eﬀect size extraction, their reﬁnement shown in Table 2. In total,
we retained a sample of Nstudies = 54 studies suitable for eﬀect size extraction.

Secondly, we set out to extract test statistics and eﬀect sizes with statcheck
and manual coding. Table 3 gives an overview how these extracted tests were ﬁrst
composed and then pruned in an exclusions process focused on statistical validity.
After exclusion of tests that would not yield valid eﬀect sizes, we Nes = 454 of
usable eﬀect sizes and their standard errors.

We include the descriptives of the complete sample of extracted eﬀect sizes
grouped by their tests in Table 4. The table standardizes all eﬀect sizes as log
odds ratios, irrespective of test statistic of origin.

Table 2. Sample Reﬁnement on Papers

Phase

Source SLR [6]

Excluded Retained

Search results (Google Scholar)
Inclusion/Exclusion

—
1011

1157
146

Reﬁnement in this study

Empirical studies
With sample sizes
With extractable tests

2
21
69

144
123
54

Table 3. Sample Reﬁnement on Extracted Eﬀect Sizes

Phase

Total eﬀects extracted

statcheck automated extraction
Test statistic manual coding
Means & SD manual coding

Reﬁnement in this study
Independent-samples test on dependent sample
Treated proportion as t-distribution
Reported dependent-samples test w/o correlation
Reported χ2 without df
χ2 with df > 1 without contingency
Multi-way F -test
Yielded inﬁnite ES or variance
Duplicate of other coded test

Excluded Retained

0

46
8
62
5
72
22
3
1

650

252
89
309

604
596
534
529
457
435
432
431

6.2 Eﬀect Size Estimates and their Conﬁdence

In Figure 2, we analyze the eﬀect size estimates of our sample with their conﬁ-
dence intervals in a caterpillar plot: estimated eﬀect sizes are plotted with error
bars representing their 95% conﬁdence intervals and ordered by eﬀect size. Two
thirds of the observed eﬀects did not pass the medium threshold: (i) 37% were
trivial, (ii) 28% were small, (iii) 15% were medium, and (iv) 20% were large.

The ﬁgure underlays the uncorrected conﬁdence intervals (gray) with the
multiple-comparison-corrected conﬁdence intervals in red. While 54% of 431 tests
were statistically signiﬁcant without MCC, only 38% were signiﬁcant after ap-
propriate MCC were applied. Table 5 contains the corresponding contingency
tables. The multiple-comparison correction signiﬁcantly impacted the signiﬁ-
cance of the tests, FET p < .001, OR = 0.53, 95% CI [0.4, 0.7] We, thereby,
reject the null hypothesis Hmcc,0.

Fig. 2. Caterpillar forest plot of n = 431 log odds ratios and their 95% conﬁdence
intervals, ordered by log(OR). Note: The dark-blue error bar represents the unmodiﬁed
95% CI; the light-yellow error bar represents the Bonferroni-corrected 95% CI.

0510log odds ratio (with 95% CI)Statistical TestMultiple−Comparison CorrectionsWith MCCWithout MCCTable 4. Descriptives of the observed absolute eﬀect sizes as log odds ratios.

Statistic

n Min Q:25 Mdn Q:75 Max IQR M SD

Chi2
F
r
t
Z

102 0.07 1.05 1.38 1.74 3.73 0.69 1.40 0.90
42 0.04 0.31 0.73 1.57 7.71 1.26 1.43 1.81
6 -1.03 1.02 1.33 1.86 2.15 0.84 1.40 0.60
317 -4.58 0.21 0.43 0.99 8.35 0.78 0.85 1.18
33 0.07 0.31 0.66 1.59 7.17 1.29 1.37 1.57

(a) Upper-Bound Power by Sample Size

(b) Upper-Bound Power Density

Fig. 3. Upper-bound of power against Standardized Mean Diﬀerence (SMD) eﬀects
and 112 observed study sample sizes N in SLR. (Note: Only studies with N < 1250
are shown for visual clarity, excluding 14 from the view)

6.3 Upper Bounds of Statistical Power

We estimate the upper-bound statistical power studies can achieve had they used
their entire sample for a single two-tailed independent-samples t-test versus a
given standardized mean diﬀerence eﬀect size. Thereby, Figure 3 oﬀers us a ﬁrst
characterization of the ﬁeld in a beaded monotonously growing power plot. The
corresponding descriptive statistics are included in Table 6 in Appendix C.

Let us unpack what we can learn from the graph. Regarding the sample size
density on the top of Figure 3a, we observe that the sample sizes are heavily
biased towards small samples (N < 100). Considering the ridge power plot in
Figure 3b, the middle ridge of power versus medium eﬀects shows the ﬁeld to
be bipartite: There is there is a peak of studies achieving greater than 80%
power against a medium eﬀect. Those studies match the proﬁle of studies with a
priori power analysis seeking to achieve the recommended 80% power. However,
roughly the same density mass is in smaller studies failing this goal. The bottom

20%40%60%80%100%025050075010001250Sample Size NPowerES Threshold0.20.50.8MTurkNoYes(small) 0.2(medium) 0.5(large) 0.80%20%40%60%80%100%PowerES ThresholdFig. 4. Histogram-density plot comparing statistical power for all tests in the sample
by MCC. Note: The histogram is with MCC and square-root transformed.

ridge line tells us that almost no studies achieve recommended power against
small eﬀects.

6.4 Power of Actual Tests

Figure 4 illustrates the power distribution of all tests and all ES thresholds
investigated taking into account their respective group sizes, comparing between
scenarios with and without MCC. Notably, the studies tests designed to have 80%
power largely retain their power under MCC. We observe a considerable number
of tests with power of approx. 50%, that is, just statistically signiﬁcant, which
falter under MCC. The descriptive statistics Table 7 in Appendix C conﬁrms
this observation. In Figure 5, we compare the power distribution by eﬀect size
thresholds and MCC.

Distinguishing further between diﬀerent test types, we consider independent-
samples t- and 2×2 χ2-tests as the most prevalent test statistics. Their respective
power simulations are included in Figures 11 and 12 in Appendix D, respectively.
Distinguishing further between diﬀerent test types, we consider independent-
samples t-, 2 × 2 χ2-tests, and one-way F -tests as test statistics. Their respective
power simulations are included in Figures 11, 12, and 13 in Appendix D, respec-
tively. In both cases, we observe the following phenomena: (i) The density mass
is on smaller sample sizes. (ii) The ridge-density plots show characteristic “two-
humped” shapes, in exhibiting a peak above 80% power, but also a density mass
at considerably lower power. (iii) The three classes of t-, χ2-, and F -tests were

01230%25%50%75%100%Statistical PowerDensityMultiple−Comparison CorrectionsWith MCCWithout MCC(a) small

(b) medium

(c) large

Fig. 5. Histogram-density plot comparing statistical power by eﬀect size thresholds
without MMC and with MCC. Note: The histogram square-root transformed for visual
clarity.

0240%25%50%75%Statistical PowerDensityMultiple−Comparison CorrectionsWith MCCWithout MCC01230%25%50%75%100%Statistical PowerDensityMultiple−Comparison CorrectionsWith MCCWithout MCC01234525%50%75%100%Statistical PowerDensityMultiple−Comparison CorrectionsWith MCCWithout MCC(a) Per paper (Mean ES and SE)

(b) Per test

Fig. 6. Funnel plots of log(OR) eﬀect sizes and their standard errors

largely ill-equipped to detect small eﬀect sizes. Overall, we see a self-similarity
of the MCC-corrected power of actual tests vis-`a-vis of the upper-bound power
considered in the preceding section.

6.5 Publication Bias

The funnel plots in Figure 6 shows the results for 47 papers and a total of 431
statistical tests. For the aggregated plot 6a, we computed the mean log odds
ratio and mean standard error per paper. We observe in both plots that with
greater standard errors (that is, smaller samples), the eﬀect sizes become more
extreme. Hence, we conjecture that smaller studies which did not ﬁnd signiﬁcant
eﬀects were not published.

By the Begg-Mazumdar rank-correlation test [2], there is a statistically sig-
niﬁcant asymmetry showing the publication bias in the per-paper aggregate,
Kendall’s τ (N = 47) = .349, p < .001., Pearson’s r = .52, 95% CI [.52, .52]. We
reject null hypothesis Hbias,0.

6.6 The Winner’s Curse

In Figure 7 depicts the winner’s curse phenomenon by pitting the simulated
power against a threshold medium eﬀect against the observed eﬀect sizes. We
observe that at low power, extreme results were more prevalent. At high power,
the results were largely clustered closely around the predicted mean log odds.

There was a statistically signiﬁcant negative correlation between power and
observed eﬀect size, that is with increasing power the observed eﬀect sizes de-
crease, Kendall’s τ (N = 396) = −.338, p < .001, corresponding to an ES of
Pearson’s r = −.51, 95% CI [−.51, −.51] using Kendall’s estimate, a large eﬀect

Outcome log(OR)Standard Error1.30.9750.650.3250−4−2024680.10<p£1.000.05<p£0.100.01<p£0.050.00<p£0.01StudiesOutcome log(OR)Standard Error1.30.9750.650.3250−4−202468Fig. 7. ES Bias by Power illustrating the Winner’s Curse. Note: Entries with more
than 1000% bias were removed for visual clarity without impact on the result.

explaining R2
Hwc,0.

τ = .11 of the variance. We reject the winner’s curse null hypothesis

We evaluated an iterated re-weighted least squares (IWLS) robust linear re-
gression (RLM) on the ES ∼ power relation mitigating for outliers, statistically
signiﬁcant at F (1, 394) = 114.135, p < .001. We obtained an intercept of 1.6 95%
CI [1.44, 1.76], F (1, 394) = 331.619, p < .001. For every 10% of power, the mea-
sured eﬀect size decreased by -0.11; 95% CI [-0.13, -0.09], F (1, 394) = 114.135,
p < .001. The simulated-power regression explained approximately R2 = .08 of
the variance; the standard error of the regression was S = 0.19. Figure 8 shows
the regression-residuals plot.

We can extrapolate to the expected mean log odds ratio at 100% power
log(OR) = 0.47, 95% CI [0.21, 0.72]. This corresponds to an SMD estimate in
Cohen’s d = 0.26, 95% CI [0.12, 0.4].

6.7 Signiﬁcance Chasing

By the signiﬁcance-chasing χ2-test of Ioannidis and Trikalinos [20] for ﬁxed
α = .05, we found a statistically signiﬁcant excess of signiﬁcant ﬁndings when
considering the eﬀects without multiple-comparison corrections (MCCs), for
small, medium and large eﬀect thresholds, with χ2(1) = 41.791, p < .001,
χ2(1) = 9.638, p = .002, and χ2(1) = 15.973, p < .001, respectively. Conse-
quently, we reject the null hypothesis Hesr,0.

0%300%600%900%25%50%75%100%PowerRelative bias of findingFig. 8. Robust regression of absolute log odds by power and its residuals

While not present for the medium eﬀect threshold under MCCs, χ2(1) =
0.451, p = .502, the statistically signiﬁcant excess of signiﬁcant ﬁndings persisted
vis-`a-vis of small and large thresholds, χ2(1) = 400.137, p < .001 and χ2(1) =
68.369, p < .001, respectively.

In Figure 9, we evaluated the tested eﬀects for signiﬁcance chasing by sig-
niﬁcance level, once without MCC and once with MCC. In Figure 9a without
MCC, we found a characteristic signiﬁcance-chasing dip in the χ2 p-value be-
low the α = .05 threshold. This phenomenon was highlighted by Ioannidis and
Trikalinos [20] as an indicator of authors “chasing signiﬁcance” by making de-
sign decisions biasing towards a signiﬁcant outcome when their results are just so
not signiﬁcant. Figure 9b with MCC does not show a comparable phenomenon,
allowing for the conjecture that authors made the use of multiple-comparison
corrections contingent on still getting a signiﬁcant result.

01234525%50%75%100%PowerAbsolute log odds(a) Uncorrected

(b) With MCC

Fig. 9. Signiﬁcance chasing χ2 p-value by signiﬁcance level α, with and without MCC

7 Discussion

The power distribution is characteristically two-humped. We found empirical
evidence that a substantive number of studies and half the tests extracted were
adequate for 80% power at a medium target eﬀect size. Hence, it is plausible to
conjecture an unspoken assumption in the ﬁeld that the population eﬀect sizes in
cyber security user studies are medium (e.g., Cohen’s d ≥ .50). The good news
here is that studies that were appropriately powered, that is, aiming for 80%
power, retained that power also under multiple-comparison corrections. Studies
which were under-powered in the ﬁrst place, got entangled by MCCs and ended
up with negligible power retained (cf. Figure 4, Section 6.4).

Having said that, the power distribution for the upper bound as well as
for actual tests (Figures 3, 11, 12, and 13) came in two “humps.” While we
consistently observed peaks at greater than 80% power for medium eﬀect sizes,
there was a density mass of under-powered tests, where the distribution was
roughly split half-half. Typically, tests were altogether too under-powered to
detect small eﬀect sizes. Overall, we believe we have evidence to attest a power
failure in the ﬁeld.

Population eﬀect sizes may be smaller than we think. The problem of power
failure is aggravated by the mean eﬀect sizes in the SLR having been close
to small, shown in the caterpillar forest plot (Figure 2) and the ES descriptives
(Table 4). In fact, our winner’s curse analysis estimated a mean Cohen’s d = 0.26,
95% CI [0.12, 0.4]. Of course, it is best to obtain precise eﬀect size estimates for
the eﬀect in question from prior research, ideally from systematic meta-analyses
deriving the estimate of population eﬀect size ˆθ. Still, the low eﬀect size indicated

1.2.1.01.001.0001.20.10.05.01.005.001Significance LevelSignificance Chasing p−ValueES Thresholdsmallmediumlarge1.2.1.01.001.0001.20.10.05.01.005.001Significance LevelSignificance Chasing p−ValueES Thresholdsmallmediumlargehere should give us pause: aiming for a medium eﬀect size as a rule of thumb
might be too optimistic.

Cyber security user studies suﬀer from a host of biases. We showed the pres-
ence of an appreciable publication bias (cf. Figure 6, Section 6.5), that is, the
phenomenon that the publication of studies was contingent on their positive
outcomes, and found evidence of the winner’s curse, that is, the phenomenon
that under-powered studies yielded exaggerated eﬀect estimates (cf. Figure 7,
Section 6.6).

Taking as a grain of salt the heterogeneity of the studies and of the ex-
ploratory nature of the method involved, we found in Section 6.7 that there is a
statistically signiﬁcant excess of signiﬁcant results in cyber security user studies,
as well. We have further seen indications of signiﬁcance chasing around a signif-
icance level of α = .05 which are absent when multiple-comparison corrections
are taken into account.

Taken together with the likely close-to-small population eﬀect sizes and the
diagnosed power failure, we need to conclude that the ﬁeld is prone to accept
publications that are seemingly “positive” results, while perpetuating biased
studies with over-estimated eﬀect sizes. These issues could be resolved with a
joint eﬀort by ﬁeld’s stakeholders—authors, gatekeepers and funders: paying
greater attention to statistical power, point and interval estimates of eﬀects, and
adherence to multiple-comparison corrections.

7.1 Limitations

Generalizability. We observe that we needed to exclude a considerable num-
ber of studies and statistical tests. This is consistent with the observations by
Coopamootoo and Groß [6] on prevalent reporting completeness, ﬁnding that
71% of their SLR sample did not follow standard reporting guidelines and only
31% combinations of actual test statistic, p-value and corresponding descrip-
tives. Similarly, Groß [15] found that 69 papers (60%) did not contain a single
completely reported test statistic. Hence, we also observe that meta research is
severely hamstringed by the reporting practices found in the ﬁeld.

We note, further, that we needed to exclude 104 extracted statistical tests
and eﬀect sizes due to problems in how these tests were employed, leading to
17 less represented. Studies that inappropriately used independent-sample tests
in a dependent-sample research designs or violated other assumptions by, e.g.,
using diﬀerence-between-means test statistics (expecting a t distribution) to test
diﬀerences between proportions (z-distribution), needed to be excluded to pre-
vent perpetuation of those issues. Finally, we needed to exclude 74 tests because
papers reported tests with degrees of freedom df > 1 without the summary
statistics to establish the eﬀect sizes. Even though those studies contained com-
plete reports the auxiliary data to estimate the eﬀects were missing.

These exclusions on empirical grounds limit generalizability. The retained
sample of 431 tests is focused on the studies that were most diligent in their

reporting. This fact, however, makes our investigation more conservative rather
than less so.

This is not a meta-analysis. Proper meta-analysis combines eﬀect sizes on sim-
ilar constructs to summary eﬀects. Given that studies operating on the same
constructs are few and far between in cyber security user studies, we standard-
ized all eﬀects to log odds ratios to gain a rough overall estimate of the ﬁeld.

There are a number of consequences of this approach. First, this study does
not attempt to create an estimate of the population eﬀect ˆθ for the research
questions investigates as a meta-analysis would. The extrapolation of the linear
regression of eﬀect size by simulated power only oﬀers a rough estimate of the
mean eﬀect sizes typically present.

Second, with the only common ground of the sample is that they constitute
user studies in cyber security, the analysis faces considerable heterogeneity. This
heterogeneity, in turn, can impact the tests on publication bias, winner’s curse,
and signiﬁcance chasing. While the analysis of Terrin et al. [32] cautions against
the use of funnel plots in face of such heterogeneity, Ioannidis and Trikalinos [20]
make similar observations about their signiﬁcance chasing detection method.
In essence, Terrin et al. [32] argue that “an inverse relation between treatment
beneﬁt and precision would result [because of researchers’ design decisions], even
when there is no publication bias.” For instance, researchers who investigate
eﬀects with larger well-assured estimates ˆθ or who have intense interventions
with strong eﬀect sizes at their disposal, could have made a conscious decision
to work with lower—but still suﬃcient—sample sizes. However, in the ﬁeld of
cyber security user studies assured estimates of population eﬀects, e.g., as gained
from systematic meta-analyses, or highly eﬀective validated interventions are few
and far between. Hence, we believe we can rule out this alternative explanation
in this case. What will likely still apply to this work is the observation that
“low quality studies may report exaggerated treatment beneﬁt, and may also
tend to be small;” [32] we observed exaggerated eﬀect sizes of small studies in
our winner’s curse analysis of Section 6.6. However, needing to choose between
the community being biased toward publication contingent on “positive” results
(publication bias) or prone to accept under-powered studies with exaggerated
results (winner’s curse), is being stuck between a rock and a hard place.

8 Concluding Recommendations

We are the ﬁrst to evaluate the statistical reliability of this ﬁeld on empirical
grounds. While there is a range of possible explanations of the phenomena we
have found—including questionable research practices in, e.g., shirking multiple-
comparison corrections in search of signiﬁcant ﬁndings, missing awareness of sta-
tistical power and multiplicity, or limited resources to pursue adequately pow-
ered studies—we believe the evidence of power failure, possibly close-to-small
population eﬀect sizes, and biased ﬁndings can lead to empirically underpinned
recommendations. We believe that these issues, however, are systemic in na-
ture and that the actions of diﬀerent stakeholders are, thereby, inter-dependent.

Hence, in the following we aim at oﬀering recommendations to diﬀerent stake-
holder, making the assumption that they aim at advancing the knowledge of the
ﬁeld to the best of their ability and resources.

Researchers. The most important recommendation here is: plan ahead with the
end in mind. That starts with inquiring typical eﬀect sizes for the phenomena
investigated. If the reported conﬁdence intervals thereon are wide, it is prudent
to choose a conservative estimate. It is tempting to just assume a medium eﬀect
size (e.g., Cohen’s d = 0.5) as aim, but there is no guarantee the population
eﬀect sizes are that large. Our study suggests they are not.

While it is a prudent recommendation to conduct an a priori power analysis,
we go a step further and recommend to anticipate multiple comparisons one
might make. Adjusting the target signiﬁcance level with a Bonferroni correction
for that multiplicity can prepare the ground for a study retaining suﬃcient power
all the way. This kind of foresight is well supported by a practice of spelling out
the research aims and intended statistical inferences a priori (e.g., in a pre-
registration). Taken together these measures aim countering the risk of a power
failure.

Speaking from our experience of painstakingly extracting eﬀects from a body
of literature, we are compelled to emphasize: One of the main points of strong
research is that it is reusable by other scientists. This goal is best served by
reporting eﬀect sizes and their conﬁdence intervals as well as full triplets of test
statistic, degrees of freedom and exact p-values, while also oﬀering all summary
statistics to enable others to re-compute the estimates. It is worth recalling that
all tests undertaken should be reported and that rigorous, well-reported studies
have intrinsic value, null result or not. This line of recommendations aims at
enabling the gatekeepers of the ﬁeld to do their work eﬃciently.

Gatekeepers. It bears repeating that the main goal of science is to advance the
knowledge of a ﬁeld. With reviewers, program chairs and editors being gatekeep-
ers and the arbiters of this goal, it is worthwhile to consider that the goal is
not served well in pursuing shiny signiﬁcant results or valuing novelty above all
else. Such a value system is prone to fail to ward against publication and related
biases. A well-powered null result or replication attempt can go a long way in ini-
tiating the falsiﬁcation of a theory in need of debunking. Because empirical epis-
temology is rooted in falsiﬁcation and replication, we need the multiple inquiries
on the same phenomena. We should strive to include adequately-powered stud-
ies of suﬃcient rigor irrespective of the “positiveness” of the results presented,
exercising the cognitive restraint to counter publication bias.

Reviewers can support this by insisting on systematic reporting and on get-
ting to see a priori speciﬁcations of aims, research designs, tests conducted, as
well as sample size determinations, hence creating an incentive to protect against
power failure. This recommendation dovetails with the fact that statistical in-
ference is contingent on fulﬁlling the assumptions of the tests used, where the
onus of proof is with the researchers to ascertain that all assumptions were satis-

ﬁed. Those recommendations are in place to enable the gatekeepers to eﬀectively
ascertain the statistical validity and reliability of studies at hand.

Funders. With signiﬁcant investments being made in putting cyber security
user studies on an evidence-based footing, we recall: “Money talks.” On the one
hand, we see the responsibility with the funders to support studies with suﬃ-
cient budgets to obtain adequately powered samples—not to speak of adequate
sampling procedures and representativeness. On the other hand, the funders
are in a strong position to mandate a priori power analyses, pre-registrations,
strong reporting standards geared towards subsequent research synthesis, pub-
lished datasets, and open-access reports. They could, furthermore, incentivize
and support the creation registered-study databases to counter the ﬁle-drawer
problem.

Acknowledgment

We would like to thank the anonymous reviewers of STAST 2020 for their com-
ments. Early aspects of this study were in parts funded by the UK Research
Institute in the Science of Cyber Security (RISCS) under a National Cyber Secu-
rity Centre (NCSC) grant on “Pathways to Enhancing Evidence-Based Research
Methods for Cyber Security.” Thomas Groß was funded by the ERC Starting
Grant CASCAde (GA no716980).

References

1. American Psychological Association (ed.): Publication Manual of the American
Psychological Association (6th revised ed.). American Psychological Association
(2009)

2. Begg, C.B., Mazumdar, M.: Operating characteristics of a rank correlation test for

publication bias. Biometrics pp. 1088–1101 (1994)

3. Button, K.S., Ioannidis, J.P., Mokrysz, C., Nosek, B.A., Flint, J., Robinson, E.S.,
Munaf`o, M.R.: Power failure: why small sample size undermines the reliability of
neuroscience. Nature Reviews Neuroscience 14(5), 365–376 (2013)
4. Cohen, J.: A power primer. Psychological bulletin 112(1), 155 (1992)
5. Cohen, J.: Statistical power analysis. Current directions in psychological science

1(3), 98–101 (1992)

6. Coopamootoo, K., Groß, T.: Systematic evaluation for evidence-based methods in

cyber security. Technical Report TR-1528, Newcastle University (2017)

7. Coopamootoo, K.P., Groß, T.: Evidence-based methods for privacy and identity
management. In: IFIP International Summer School on Privacy and Identity Man-
agement. pp. 105–121. Springer (2016)

8. Coopamootoo, K.P., Groß, T.: A codebook for experimental research: The nifty
nine indicators v1.0. Tech. Rep. TR-1514, Newcastle University (November 2017)
9. Coopamootoo, K.P., Groß, T.: Cyber security and privacy experiments: A design
and reporting toolkit. In: IFIP International Summer School on Privacy and Iden-
tity Management. pp. 243–262. Springer (2017)

10. Cumming, G.: Understanding the new statistics: Eﬀect sizes, conﬁdence intervals,

and meta-analysis. Routledge (2013)

11. Cumming, G., Fidler, F., Kalinowski, P., Lai, J.: The statistical recommenda-
tions of the american psychological association publication manual: Eﬀect sizes,
conﬁdence intervals, and meta-analysis. Australian Journal of Psychology 64(3),
138–146 (2012)

12. Dickersin, K.: The existence of publication bias and risk factors for its occurrence.

Jama 263(10), 1385–1389 (1990)

13. Epskamp, S., Nuijten, M.: statcheck: Extract statistics from articles and recompute
p-values (R package version 1.0.0.). https://cran.r-project.org/web/packages/
statcheck/index.html (2014)

14. Gardner, M.J., Altman, D.G.: Conﬁdence intervals rather than p values: estimation
rather than hypothesis testing. Br Med J (Clin Res Ed) 292(6522), 746–750 (1986)
15. Groß, T.: Fidelity of statistical reporting in 10 years of cyber security user studies.
In: In proceedings of the 9th International Workshop on Socio-Technical Aspects
in Security (STAST’2019). LNCS, vol. 11739, pp. 1–24. Springer Verlag (2019)
16. Groß, T.: Fidelity of statistical reporting in 10 years of cyber security user studies

[extended version]. arXiv Report arXiv:2004.06672, Newcastle University (2020)

17. Hoekstra, R., Morey, R.D., Rouder, J.N., Wagenmakers, E.J.: Robust misinterpre-
tation of conﬁdence intervals. Psychonomic Bulletin & Review 21(5), 1157–1164
(2014)

18. Howell, D.C.: Fundamental Statistics for the Behavioral Sciences (8th international

ed). Cengage Learning (2014)

19. Ioannidis, J.P.: Why most published research ﬁndings are false. PLoS Med 2(8),

e124 (2005)

20. Ioannidis, J.P., Trikalinos, T.A.: An exploratory test for an excess of signiﬁcant

ﬁndings. Clinical trials 4(3), 245–253 (2007)

21. Light, R.J., Pillemer, D.B.: Summing up; the science of reviewing research. Cam-

bridge, MA (USA) Harvard Univ. Press (1984)

22. Maxion, R.: Making experiments dependable. In: Dependable and Historic Com-

puting, pp. 344–357. Springer (2011)

23. Morey, R.D., Hoekstra, R., Rouder, J.N., Lee, M.D., Wagenmakers, E.J.: The fal-
lacy of placing conﬁdence in conﬁdence intervals. Psychonomic bulletin & review
23(1), 103–123 (2016)

24. Nickerson, R.S.: Null hypothesis signiﬁcance testing: a review of an old and con-

tinuing controversy. Psychological methods 5(2), 241 (2000)

25. Peisert, S., Bishop, M.: How to design computer security experiments. In: Fifth
World Conference on Information Security Education. pp. 141–148. Springer (2007)
26. Publications, APA and on Journal, Communications Board Working Group: Re-
porting standards for research in psychology: Why do we need them? what might
they be? The American Psychologist 63(9), 839 (2008)

27. Rosenthal, R.: The ﬁle drawer problem and tolerance for null results. Psychological

bulletin 86(3), 638 (1979)

28. Scargle, J.D.: Publication bias (the” ﬁle-drawer problem”) in scientiﬁc inference.

arXiv preprint physics/9909033 (1999)

29. Schechter, S.: Common pitfalls in writing about security and privacy human sub-
jects experiments, and how to avoid them. https://www.microsoft.com/en-us/
research/wp-content/uploads/2016/02/commonpitfalls.pdf (2013)

30. Sterling, T.D., Rosenbaum, W.L., Weinkam, J.J.: Publication decisions revisited:
The eﬀect of the outcome of statistical tests on the decision to publish and vice
versa. The American Statistician 49(1), 108–112 (1995)

31. Sterne, J.A., Egger, M.: Funnel plots for detecting bias in meta-analysis: guidelines
on choice of axis. Journal of clinical epidemiology 54(10), 1046–1055 (2001)
32. Terrin, N., Schmid, C.H., Lau, J., Olkin, I.: Adjusting for publication bias in the

presence of heterogeneity. Statistics in medicine 22(13), 2113–2126 (2003)

A Underlying Systematic Literature Review

This meta-analytic study is based on a Systematic Literature Review (SLR),
which was conducted in 2016/17 for the UK Research Institute in the Science
of Cyber Security (RISCS). We adapted this description of the SLR’s search
from its technical report [6]. It was previously published in the supplementary
materials of the analysis of statistical reporting ﬁdelity [16].

A.1 Search Strategy of the SLR Sample

The SLR included security and privacy papers published between 2006 and 2016
(inclusive).

The search was restricted to the following security and privacy venues:

– journals: IEEE Transactions on Dependable & Secure Computing (TDSC),

ACM Transactions on Information and System Security (TISSEC),

– ﬂagship security conferences: IEEE S&P, ACM CCS, ESORICS, and PETS

or

– specialized conferences and workshops: LASER, SOUPS, USEC and WEIS.
The search was conducted on Google Scholar. Each query extracts articles
mentioning “user study” and at least one of the words “experiment,” “evidence”
or “evidence based.” The described query was executed for each of the 10 pub-
lication venues. In the advanced search option of Google Scholar, each of the
following ﬁelds were set:

– with all words = user study
– at least one of the words = experiment evidence “evidence based”
– where my words occur = anywhere in the article
– return articles published in = [publication venue]
– return articles dated between = 2006–2016

The search yielded 1157 publications.

A.2 SLR Inclusion/Exclusion Criteria

We adapt the inclusion/exclusion criteria of the 2017 SLR [6] for this pre-
registration. The SLR focused on human factors studies including a human sam-
ple. The following Inclusion Criteria were applied to its overall pool of 1157
publications:

– Studies including a user study with human participants.
– Studies concerned with evidence-based methods or eligible for hypothesis

testing and statistical inference.

– Studies that lend themselves to quantitative evaluation, quoting statements

of statistical signiﬁcance, p-values or eﬀect sizes.

Fig. 10. Data dependency diagram for analyses included in the paper.

– Studies with true experiments, quasi-experiments or observational analysis.
Of the papers included, the ones fulﬁlling the following Exclusion Criteria were
excluded:

– Papers that were not subject to research peer-review, key note statements,

posters and workshop proposals.

– Position papers or informal arguments.d R
– Papers not including a study with human participants,
– Theoretical papers.
– Studies with qualitative methodology.

This inclusion/exclusion process yielded a ﬁnal sample of 146 publications.

B Data Dependencies

This work explicitly steps away from post-hoc power analysis and focuses on
statistical power simulation. Given how inﬂated ES estimates lead to inﬂated—
and thereby unreliable—post-hoc power estimates, this distinction is crucial. To
make clear how diﬀerent analyses depend on sampled data from papers and
externally inputted parameters as well as to support reproducibility, we provide
a data dependency diagram in Figure 10.

C Descriptive Statistics

We include a number of descriptive statistics complementing plots in the body
of the paper. Table 5 oﬀers contingency tables on the diﬀerence made by using
family-wise multiple-comparison corrections. Table 6 provides the characteristics
of the extracted overall sample sizes and upper-bound power simulation. Finally,
Table 7 includes the power simulation for actual tests.

ObservedAsymmetryFunnelmetaforTest#MCTest FamilySample Size NUpper-BoundPowerSimulationpwr:a priori powerGroup Size niTestp-ValueTestStatisticTestSE, CIObservedEffectSizeEffect SizeThresholdSMDwfrActual-TestPower SimulationMMCCorrection~~ObservedESDistributionES estimationObservedWinner’sCurseCorrelationSampled from SLR StudiesExternalParameters~~Table 5. Absolute and before-after matched contingency tables of signiﬁcance by
multiple-comparison corrections (MCC)

(a) Overall Frequencies

(b) Matched-Pairs Before/After MCC

Signiﬁcant Not Signiﬁcant

Signiﬁcant Not Signiﬁcant

MCC
No MCC

165
232

266
199

Signiﬁcant
Not Signiﬁcant

165
0

67
199

Table 6. Descriptives of sample size and upper-bound power distribution

(a) Sample size

Min Q:25 Mdn Q:75 Max

IQR

M SD

1 9.00 41.25 138.25 340.75 1627.00 299.50 264.19 339.13

(b) Upper bounds of statistical power

smd Min Q:25 Mdn Q:75 Max IQR M SD

1 0.20 0.06 0.10 0.21 0.45 0.98 0.36 0.31 0.26
2 0.50 0.10 0.35 0.83 1.00 1.00 0.65 0.68 0.33
3 0.80 0.18 0.71 1.00 1.00 1.00 0.29 0.84 0.24

Table 7. Descriptives of statistical power of actual tests with and without MCC

(a) Without MMC

threshold Min Q:25 Mdn Q:75 Max IQR M SD

1 large
0.40 0.80 0.98 1.00 1.00 0.20 0.86 0.20
2 medium 0.19 0.41 0.68 1.00 1.00 0.59 0.66 0.31
0.07 0.10 0.12 0.53 1.00 0.44 0.34 0.34
3 small

(b) With MCC

threshold Min Q:25 Mdn Q:75 Max IQR M SD

1 large
0.15 0.62 0.94 1.00 1.00 0.38 0.79 0.27
2 medium 0.05 0.22 0.53 1.00 1.00 0.77 0.57 0.37
0.01 0.04 0.08 0.28 1.00 0.24 0.24 0.33
3 small

D Power of Actual Tests

Figures 11, 12, and 13 include the power estimates for actual independent-
samples t-tests, 2 × 2 χ2 tests, one-way F -tests, respectively.

(a) Power by Sample Size

(b) Power Density

Fig. 11. Power of independent-samples t tests vs. SMD (Hedges’ g) thresholds

(a) Power by Sample Size

(b) Power Density

Fig. 12. Power of χ2 tests vs. Cohen’s w thresholds

0%20%40%60%80%100%025050075010001250Sample Size NPowerES Threshold0.20.50.8(small) 0.20(medium) 0.50(large) 0.800%20%40%60%80%100%PowerES Threshold0%20%40%60%80%100%025050075010001250Sample Size NPowerES Threshold0.10.30.5(small) 0.10(medium) 0.30(large) 0.500%20%40%60%80%100%PowerES Threshold(a) Power by Sample Size

(b) Power Density

Fig. 13. Power of one-way F tests vs. Cohen’s f thresholds

0%20%40%60%80%100%025050075010001250Sample Size NPowerES Threshold0.10.250.4(small) 0.10(medium) 0.25(large) 0.400%20%40%60%80%100%PowerES Threshold