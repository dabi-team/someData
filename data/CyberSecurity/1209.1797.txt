3
1
0
2

n
u
J

5

]

R
C
.
s
c
[

3
v
7
9
7
1
.
9
0
2
1
:
v
i
X
r
a

1

Securing Your Transactions: Detecting
Anomalous Patterns In XML Documents

Eitan Menahem, Alon Schclar, Lior Rokach, Yuval Elovici

Abstract—XML transactions are used in many information systems to store data and interact with other systems. Abnormal
transactions, the result of either an on-going cyber attack or the actions of a benign user, can potentially harm the interacting systems
and therefore they are regarded as a threat. In this paper we address the problem of anomaly detection and localization in XML
transactions using machine learning techniques. We present a new XML anomaly detection framework, XML-AD. Within this framework,
an automatic method for extracting features from XML transactions was developed as well as a practical method for transforming XML
features into vectors of ﬁxed dimensionality. With these two methods in place, the XML-AD framework makes it possible to utilize
general learning algorithms for anomaly detection. Central to the functioning of the framework is a novel multi-univariate anomaly
detection algorithm, ADIFA. The framework was evaluated on four XML transactions datasets, captured from real information systems,
in which it achieved over 89% true positive detection rate with less than a 0.2% false positive rate.

Index Terms—XML Anomaly Detection, XML Security, Machine-Learning, Outliers Detection, Anomaly-Detection

(cid:70)

1 INTRODUCTION
Today, many information systems communicate and in-
teract through XML transactions. These transactions may
fall victim to cyber-attacks or even benign mistakes,
which can alter the structure and content of their in-
teraction media, i.e., the XML documents. Regardless of
whether the origin of these alterations is malicious or
benign, the altered XMLs, especially those that adhere
to the XSD schema, can potentially exploit vulnerabilities
of the interacting information systems. Since the altered
XMLs can be render as anomalous with respect to the
majority of the XML transactions in the same domain,
detecting anomalous XMLs is an important means to
increase the security of many information systems. Un-
fortunately, state-of-art, end-to-end security protocols
for XML transactions, i.e., XML encryption [16], XML
signature [17], and XML-canonicalization [15] provide
little protection against such a threat. This is mostly
because the alteration actions, which deform the XMLs,
take place before such protective measures are applied
at the endpoint’s systems. It follows that in addition to
the abovementioned security protocols, XML documents
should be subjected to an anomaly detection prior to
being processed by the endpoint information systems.

Extensible Markup Language [6] is a framework that
facilitates the deﬁnition of structured markup languages.
Data in such languages is described by documents in
which every datum is encapsulated by tags. Since
XML is very ﬂexible it is especially suitable for: (a)

• E. Menahem, L. Rokach and Y. Elovici are with the Department of
Information Systems Engineering, Ben-Gurion University of the Negev,
Be’er Sheva, 84105, Israel.
E-mail: {eitanme, liorrk, elovici}@post.bgu.ac.il

• A. Schclar is with the School of Computer Science, The Academic College
of Tel-Aviv-Yafo, Tel Aviv, 61083, Israel. E-Mail: alonschc@mta.ac.il

data storage in a structured format; (b) data transfer
and sharing both in local organizational networks and
over the Internet; and (c) data serialization. The XML
ﬁles, bound to their deﬁnition in an XSD schema can
vary considerably and two XMLs that adhere to the
same XSD schema, can have very different attributes
regarding both their content and structure.

1.1 XML Anomalies

Anomalies are data patterns, which are either very rare
or novel. In the scope of this paper, the anomalous
patterns are related to both the structure and content of
an XML document. Such patterns can be generated by ei-
ther actions of which intentions are malicious (i.e., cyber-
attack), or benign (user mistake or a technical error).
Next, we describe the two most prominent anomalous
pattern generators.

XML attacks

Applications that interact through XML messages, such
as various Web-services, are essentially vulnerable to a
wide range of malicious attacks. These attacks exploit
various vulnerabilities in the XML processing mecha-
nism, such as the vulnerability of XML parsers or the
weak points of input veriﬁcation in the target server
application. Among the prominent attacks of this type
are input validation attacks [27]; probing [40]; malware
inﬁltration; buffer overﬂow [40], [27]; XML parameter
poisoning [40], [36]; CDATA ﬁeld attacks [40], [36]; SQL
injection [40], [36], [27]; cross-site scripting [27]; schema
poisoning [24]; denial of service (DoS); DDoS XML
bombardment; DOM parser DoS attacks; XML Bomb [37]
and repetition attack. These XML attacks are expected to
result in producing XML anomalies, since the attacks are

 
 
 
 
 
 
expressed through string expressions (or by other data
types) that, with respect to the normal XML transaction
collections, are inherently very unlikely to be obtained.
Another threat to modern information systems arises
from data leakage. Among the causes of data leakage are
Trojan attacks, SQL injection attacks, or simple human
error. There are many ways in which outgoing XML
transactions can lead to data leaks in the system. The
simplest way results from putting all the data, as it is,
in one ﬁeld that is not properly constrained by a regular
expression. A simple variation of this scheme is dividing
the data into several parts and embedding it into many
different ﬁelds of the XML ﬁle.

Benign Anomalies
Not all XML anomalies are a product of a cyber-attack or
a malicious action. There are many ways in which XML
documents might become anomalous. User mistakes,
application errors and communication errors are typical
examples of how benign anomalies might be manifested
in XMLs.

1.2 Problem Statement and Applicability

The present work focuses on the problem of detecting
and localizing anomalies in readable XML documents at
computer endpoints, but will not address XML parser
attacks since those can be efﬁciently detected at the
network level, by technologies such as Anagram [41].

The algorithms, which are presented in this paper
aim to detect anomalies that stem from either malicious
or benign actions. In our opinion, it is important to
detect both types of anomalies. The rationale behind
this approach is the assumption that XML anomalies,
regardless of their nature, have the potential to result in
unwanted effects in the information processing system.
We would like to stress that the present work does not
try to infer the nature of the detected anomalies, since
this require elaborate forensic work and an understand-
ing of the data domain and semantics.Consequently, we
use XML-AD only as an indicator for what could be
a network or system attack, which is being delivered
by XML documents. As such, XML-AD can be appli-
cable, for example, for endpoint anomaly-based XML-
Firewalls.

1.3 Anomaly Detection

Anomaly detection [2], [3], [4], [22], [26], [30] is a process
aimed at discovering patterns in datasets that deviate
from the behavior or the expect behavior of the majority
of the data. Anomaly detection can be found in a broad
spectrum of applications such as intrusion detection,
cyber-security, fraud detection, ﬁnancial systems, and
military surveillance, to name a few. Anomaly detec-
tion methods employ a wide range of techniques that
are based on statistics, classiﬁcation, clustering, nearest
neighbor search, information theory and spectral analy-
sis.

2

There might be an inﬁnite number of anomalous pat-
terns, some of which are very rare and hard to obtain.
In such cases, the most conventional learning approach,
i.e., supervised-learning, is impractical since training a
supervised classiﬁer demands at least a single example
from each of the patterns that must be classiﬁed. More-
over, in many real-life domains, normal state examples
are inherently easier to obtain than anomalous state
examples. In such domains, researchers take the semi-
supervised anomaly detection approach (also known as
one-class learning), in which only the normal class is
being taught but the algorithm learns to detect abnormal
patterns [22].
Semi-supervised anomaly detection is a suitable ap-
proach for training anomaly detection for XML trans-
actions when one assumes that at the time of training,
only normal XML examples exist.

1.4 Similarity-Based Anomaly Detection

In order to classify a new XML example as normal or as
anomalous, most anomaly detection algorithms consider
the similarity of the new example to a group of XML
documents, labeled normal. The general idea is that if
the new example is similar to the normal instances, it
should be labeled normal; otherwise, it should be labeled
anomalous. Such a similarity function, denoted as S(·),
takes two parameters: a new instance, xnew, and a group
of XML documents, labeled as normal, X {normal}. One
way to calculate such a similarity score is by computing
the distance from xnew to the closest normal XML doc-
ument, i.e., S(xnew, X) ≡ argminx∈X {normal} d(xnew, x),
where a distance function, d(·) , is a pairwise distance
function. The distance function, d(xnew, x), computes a
scalar that reﬂects the XMLs‘ mutual similarity in the
feature space. Finding such a distance function, which is
able to separate the normal documents from the anoma-
lous documents in the feature space, is a fundamental
challenge in XML anomaly detection [25].

Most similarity-based anomaly detection algorithms
use a multivariate vector distance function, which is a
plausible approach in many domains, especially those
in which data is represented by low-dimensional vectors.
However, multivariate distance functions are inherently
susceptible to the ”curse of dimensionality” [5]. Con-
sequently, the functions become much less accurate as
the number of dimensions grows, since between any
two points in the given dataset the distance converges,
rendering the concept of distance meaningless. Another
weak point one ﬁnds in similarity-based anomaly de-
tection algorithms that use multivariate vector distance
functions is that they are unable to indicate the speciﬁc
dimension(s) in the new vector that incurs the anoma-
lous pattern. As a result, the algorithms do not allow
localization of the anomaly pattern source, i.e., detect
the cause of the anomaly.

To avoid the weaknesses of

the abovementioned
similarity-based anomaly detection algorithms, the ap-
proach taken in this study is to use multiple, univariate

3

Fig. 1. The conceptual architecture of XML-AD

distance functions. This approach was previously used
in several domains, including intrusion detection [35],
[39], [29], [8], however, it has not been applied to detect
XML anomalies so far. We show that our approach
results in a very accurate XML anomaly detection and
makes it possible to localize the dimensions (i.e., the
XML features), which are the source of the anomalous
patterns.

Paper outline

The rest of this paper is organized as follows. In Section 2
we present related work and discuss the need for a new
anomaly detection framework for XML transactions. In
Section 3 we present XML-AD, the proposed anomaly
detection framework. In Section 4 we discuss the meth-
ods, classiﬁers, datasets and performance metrics used
for evaluating XML-AD. In Section 5, we present sev-
eral evaluation experiments and discuss their results. In
Section 6, we summarize the contributions of this paper
and discuss future directions.

2 RELATED WORK

Despite the risk XML documents anomalies impose, very
little relevant research has been done in this area. In the
following paragraphs, we describe available methods for
the anomaly detection of XML forms. All these methods
take the semi-supervised anomaly detection approach.
Bruno et al. propose a method for detecting frequently
occurring relationships in datasets, which correspond
to the normal behavior of the data [10], [11]. The de-
tection method uses association rules and the relation-
ships are represented as quasi-functional dependencies.
Anomalies are discovered by querying either the original
database or the previously mined association rules to
indicate the presence of erroneous data or novel infor-
mation that represents the outliers of frequent rules. The
method is independent of the considered database and
directly infers rules from the data. In [9], an incremental
approach is used to extend the method in [10], [11] to
handle dynamic databases where the anomalies must be
updated according to changes that the data undergoes.

Premalatha and Natarajan [32] mine negative associ-
ation rules [42], which are used to describe those rela-
tionships between item sets that indicate the occurrence
of some item sets by the absence of others. The chi-
square test is used to identify independent attributes
and the anomalies are identiﬁed as a negative association
rule whose conﬁdence value is greater than a minimum
conﬁdence threshold. Unfortunately, domain knowledge
of the data sets is incorporated into ﬁlter rules, a step
that does not contribute to the detection process.

H´ev´ızi et al. [21] use probabilistic inference for classi-
ﬁcation and anomaly detection of structured documents
which they test on XML documents. Speciﬁcally, they
extract a feature vector from every XML document ac-
cording to the number of attributes each tag can have.
The features are learned and represented in a factorized
form as a product of pairwise joint probability distri-
bution functions according to a method introduced by
Chow and Liu [12]. Anomaly is detected by applying
an acceptance threshold to the probability values. The
authors indicate that this threshold should be trained
and adapted for databases that are subject to frequent
changes.

Raz et al. [33] detect anomalies in dynamic data
feeds. Speciﬁcally,
invariants such as value interval
and arithmetic expressions are extracted and used as
proxies to detect anomalies. The detection method is
demonstrated for semantic anomalies, i.e., values that
are syntactically correct but have unreasonable values.
the
Two types of
mean statistics and invariants that are produced by
an adjusted version of a software for the detection of
invariants in computer programs.

invariants are extracted, namely,

In light of the above mentioned works, the original
contribution of this work is three-fold: (1) it presents a
general and automatic method for extracting structural
and content features from XML transactions, (2) it pro-
vides a practical method for transforming XML features
into vectors of ﬁxed dimensionality and hence, enables
the use of non-proprietary machine-learning algorithms
for the XML anomaly detection task, (3) it presents a
novel anomaly detection algorithm, ADIFA, which is
based on the multi-univariate model approach.

XML Features ExtractorXSD ExtractorTransaction ProcessorXSDElements Definition (VXSD)XMLsTransactions CorpusTransactions Features ExtractorDataset GeneratorAnomaly Detection Model LearnerFeatures matrix (Fm)DatasetFeatures FlatteningLeaning AlgorithmInstancesAnomaly Detection Model123Feature Extraction Method 1 (F.E.M.)F.E.M.2F.E.M.m3 XML-AD: AN AUTOMATED, CONTEXT-
LEARNING OF XML DOCUMENTS
We propose a new framework, XML-AD, for training a
classiﬁer for detecting and localizing XML anomalies.
The framework is composed of three stages: feature
extraction, dataset generation and training of machine-
learning model. The input for the training process is a
corpus of XML transactions and a single XSD ﬁle, which
deﬁnes the transactions at hand.
In the ﬁrst stage, the raw transaction features are ex-
tracted. In order to do so, the XSD is parsed and the
meta data it contains, i.e., XML elements deﬁnition and
constraints, are extracted. Next, the entire training XML
corpus is put through a feature extraction procedure.
The meta-data extracted from the XSD is used to select
the suitable feature extraction methods for each of the
available XML elements. The second stage is dataset
generation, in which the transaction features are aggre-
gated and arranged in tuples, each containing multiple
computed (aggregated), features of a single transaction.
These tuples are then added to a dataset (the train-set). In
the third and ﬁnal stage, the anomaly detector is trained
by applying the ADIFA algorithm (section 3.3), on the
train-set, which was generated in the previous stage. The
above mentioned process is depicted in Figure 1.

3.1 Transactions Feature Extraction

The feature extraction process starts with the acquisition
of the deﬁnitions of the XML elements. This is achieved
by parsing the XSD ﬁle. The XML elements’ deﬁnitions
are then stored in a data structure that we denote as
VXSD. In Figure 2 we show an XSD ﬁle that deﬁnes
three variables: PaymentAmount, PyValue and Name. The
ﬁrst variable is deﬁned as XSD:double, which means that
a numerical value may be assigned to it. PyValue was
deﬁned as enumeration, so only a pre-deﬁned (not visible
in this example), integer may be assigned to it. Last, the
Name variable is deﬁned as XSD:String indicating that
any textual symbols may be assigned to it. In Figure 3
we can see that VXSD contains one descriptive object for
each deﬁned element in the XSD ﬁle shown in Figure
2. A descriptive object is a simple container that carries
the type of the XML element (i.e. numeric, date, binary
etc.). In case of enumeration the descriptive object also
contains value ranges (as with PyValue above). To avoid
the complexity of handling many XSD data-types, we
found that it was sufﬁcientto deal with only a few
abstract data types: Numerical, Enumeration, String and
Date.

Next, the XML transactions are processed and their
features are extracted and stored in the features matrix
(Fm). The Fm matrix will contain a single row for
each XML transaction. Each row is comprised of mul-
tiple complex features, one for each element deﬁnition
stored in VXSD. Since the XML structure allows repeti-
tion of elements within the same document, a complex
feature may contain multiple occurrences of the same

4

element. Therefore, each complex feature contains a
list, {mv1, mv2, . . . }, which we denote as ’measurement-
vector’ (mv), which stores information involving the
occurrences of the related element. Each term in the
’measurement-vector’ corresponds to a single element
occurrence. Finally, each mv contains l scalar measure-
ments, {m1, m2, . . . , ml} that corresponds to l attributes
of each XML element occurrence.

Fig. 2. An example of an XSD ﬁle

Figure 4 exemplify the Fm matrix related to the above

XSD example.

Fig. 3. A part of the XSD vector, VXSD, produced by the
XSD parser. Three objects are Visible: P ntAmt, P yV alue
and N ame of type Numerical, Enumeration and String,
respectively.

3.2 Dataset Compilation

(1) all

Traditionally, most machine-learning algorithms require
that two conditions regarding their input datasets be
instances must have the same number
met:
of features, and (2) all features must be scalars (as
opposed to the complex features, which have an inner
structure). While transaction instances in Fm contain
the same number of complex features, they may contain
a different number of inner data items (e.g., measure-
vectors). Thus, the second condition is not met.

To overcome this problem, Fm should be ﬂattened.
The ﬂattening process can be made in either a lossless

           <?xml version="1.0" encoding="UTF-8" ?> - <xsd:schema xmlns:xsd="…">     - <xsd:element name="TXLife"> …        - <xsd:element name="PntAmt">             - <xsd:complexType>                - <xsd:simpleContent>                  <xsd:extension base="xsd:double" />                </xsd:simpleContent>            </xsd:complexType>         </xsd:element> …        - <xsd:element name="PyValue">             - <xsd:complexType>                - <xsd:simpleContent>                  <xsd:extension base="xsd:enumeration"/>                </xsd:simpleContent>            </xsd:complexType>         </xsd:element> …        - <xsd:element name="Name">             - <xsd:complexType>                - <xsd:simpleContent>                  <xsd:extension base="xsd:String"/>                </xsd:simpleContent>            </xsd:complexType>         </xsd:element> …  </xsd:schema    VXSD = {…,  PntAmt, PyValue, Name,…}                              Numerical  Enumeration {0,1,3,5}  String   (cid:1)(cid:2) = (cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:5)(cid:8)(cid:3)(cid:9)(cid:10)(cid:6)(cid:11),  {(cid:13)(cid:11),(cid:11),(cid:13)(cid:15),(cid:11),,…,(cid:13)(cid:16),(cid:11)}   transaction(cid:15),               {(cid:13)(cid:11),(cid:15),(cid:13)(cid:15),(cid:15),,…,(cid:13)(cid:16),(cid:15)} ··· ··· ··· ··· transaction(cid:26), {(cid:13)(cid:11),(cid:2),(cid:13)(cid:15),(cid:2),,…,(cid:13)(cid:16),(cid:2)}   {(cid:27)(cid:28)(cid:11), (cid:27)(cid:28)(cid:15), …, (cid:27)(cid:28)(cid:29)}                       {(cid:27)(cid:11), (cid:27)(cid:15), …, (cid:27)(cid:30)}                VXSD = {…,  PntAmt, PyValue, Name,…}                              Numerical  Enumeration {0,1,3,5}  String   (cid:1)(cid:2) = (cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:5)(cid:8)(cid:3)(cid:9)(cid:10)(cid:6)(cid:11),  {(cid:13)(cid:11),(cid:11),(cid:13)(cid:15),(cid:11),,…,(cid:13)(cid:16),(cid:11)}   transaction(cid:15),               {(cid:13)(cid:11),(cid:15),(cid:13)(cid:15),(cid:15),,…,(cid:13)(cid:16),(cid:15)} ··· ··· ··· ··· transaction(cid:26), {(cid:13)(cid:11),(cid:2),(cid:13)(cid:15),(cid:2),,…,(cid:13)(cid:16),(cid:2)}   {(cid:27)(cid:28)(cid:11), (cid:27)(cid:28)(cid:15), …, (cid:27)(cid:28)(cid:29)}                       {(cid:27)(cid:11), (cid:27)(cid:15), …, (cid:27)(cid:30)} Figure 7: The extracted features matrix, Fm. Parameters m and n are the number of instances and the number of complex features respectively.  <?xml version="1.0" encoding="UTF-8" ?> - <xsd:schema xmlns:xsd="…">     - <xsd:element name="TXLife"> …        - <xsd:element name="PntAmt">             - <xsd:complexType>                - <xsd:simpleContent>                  <xsd:extension base="xsd:double" />                </xsd:simpleContent>            </xsd:complexType>         </xsd:element> …        - <xsd:element name="PyValue">             - <xsd:complexType>                - <xsd:simpleContent>                  <xsd:extension base="xsd:enumeration"/>                </xsd:simpleContent>            </xsd:complexType>         </xsd:element> …        - <xsd:element name="Name">             - <xsd:complexType>                - <xsd:simpleContent>                  <xsd:extension base="xsd:String"/>                </xsd:simpleContent>            </xsd:complexType>         </xsd:element> …  </xsd:schema 5

features. For Enumeration complex-
simple (scalar),
the Sum aggregation function.
features we use
By summing up every occurrence of each possible
enumeration value, the number of generated simple-
features becomes dependent on the number of elements
in the enumeration. String complex-features are treated
slightly differently. First, we compute the number of
words and the length of the string for each string
occurrence in a String complex-feature. Then, we
apply the Maximum and Minimum aggregate function
to produce four simple features:
over these values,
the maximal and minimal number of words, and the
maximal and minimal string length. Additionally, to
learn the textual context of the XML transactions, a
dictionary of words is produced from the training
transaction, from which we then compute the TF-IDF
(word term frequency inverse document
frequency)
value for every word in the dictionary. Lastly, according
to their TF-IDF value, only the k most prominent words
are chosen for extracting simple features, i.e., TF-IDF
values. This way, each transaction instance will contain
k simple text contextual features.

Finally, when the ﬂattening process is over, the derived
dataset meets both of the required conditions mentioned
above: ALL instances have the same dimensionality and
all the features are scalars. Evidently, as we illustrate
in Section 5, the proposed ﬂattening approach was very
effective for detecting XML anomalies.

3.3 The Anomaly Detection Model

Due to the ﬂattening process, a derived dataset has
two essential properties that make it difﬁcult to learn.
First, its dimensionality is high, because the ﬂattening
process translates each XSD element into several simple
features, and standard XSDs contain thousands of
such elements. Moreover, since the learning task is
anomaly detection, its dimensionality cannot be reduced
by feature selection because anomalies can appear in
any of the dataset’s features, thus, removing any feature
can result in a missed anomaly detection.
the dataset
is assumed that
In addition, since it
is composed of normal
the dataset
instances only,
represents a one-class classiﬁcation problem, which is
inherently a more difﬁcult task compared to supervised
or unsupervised anomaly detection. While there is
wealth of anomaly detection algorithms to choose from,
only very few are capable of a one-class classiﬁcation,
from which none, as far as we know, can cope with
datasets whose dimensionality is very high.
To close this gap, this work proposes a new algorithm,
ADIFA (Attribute DensIty Function Approximation),
compared to three very known
which is
unsupervised anomaly detection
that
we transformed into one-class anomaly detection (see
Section 4.1). In the rest of this section, we focus on the
ADIFA algorithm.

algorithms

later

Fig. 4. The extracted features matrix, Fm. m and n are
the number of transaction instances and the number of
complex features, respectively.

or a lossy manner. In the lossless ﬂattening process the
instances information is preserved, however, the number
of features per instance is not constant and consequently,
the derived dataset will not be rectangular. Therefore, the
ﬁrst condition mentioned above is not met. In the lossy
ﬂattening process, some information is sacriﬁced, while
new properties can be gained, for example, determining
the number of features per instance. In our case, the
beneﬁt of choosing the lossy ﬂattening alternative is that
many usable generic machine-learning algorithms can be
used later to train anomaly detection models.

We chose to ﬂatten the Fm matrix using a lossy process
due to the above mentioned advantages. The ﬂatten-
ing was achieved by applying a group of aggregate
functions, Ψ = {ψ1, . . . , ψk}, to each of Fm’s tuples
separately, where ψi : {f1,...,n} → R. Speciﬁcally, let fi,j
be a complex feature where i denotes the feature and j
denotes the tuple. A set of k aggregated values ψl(fi,j) is
calculated where l = 1, . . . , k. Since the same aggregate
functions are applied to all the tuples in Fm, the derived
ﬂattened dataset is rectangular, as Figure 5 shows. Next,
we discuss the aggregate functions.

Aggregate Features

Using aggregate functions to ﬂatten the complex feature
matrix, Fm, inevitably results in the loss of some infor-
mation. However, to minimize the loss of information,
the aggregate functions should only discard information
that would have as little impact as possible on the ability
to detect anomalies.

Speciﬁcally, for Numerical and Time complex-features,
we apply the Maximum and Minimum aggregation
functions. Accordingly, only the maximal and minimal
values of the complex features data-times are used as

Fig. 5. The derived ﬂattened dataset

              Fm =  {f1,1, f2,1, …, fn,1} {f1,2, f2,2, …, fn,2} ··· ··· {f1,m, f1,m, …, fn,m}        {mv1, mv2, …, mvk}             {m1, m2, …, ml}      Fm =  f1,1 , …, PntAmt1, PyValue1, Name1, …, fn,1            {mv1, mv2,… }                                                 … … f1,m , …, PntAmtm, PyValuem, Namem,…, fn,m    Flattened Dataset  (cid:2)(cid:3)(cid:4)(cid:5)(cid:3),(cid:7)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:3),(cid:7)(cid:8),…,(cid:2)(cid:3)(cid:4)(cid:5)(cid:7),(cid:7)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:7),(cid:7)(cid:8),…,(cid:2)(cid:10)(cid:4)(cid:5)(cid:11),(cid:3)(cid:8)   … (cid:2)(cid:3)(cid:4)(cid:5)(cid:3),(cid:12)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:3),(cid:12)(cid:8),…,(cid:2)(cid:3)(cid:4)(cid:5)(cid:7),(cid:12)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:7),(cid:12)(cid:8),…,(cid:2)(cid:10)(cid:4)(cid:5)(cid:11),(cid:12)(cid:8)   (cid:2)(cid:3)(cid:4)(cid:5)(cid:3),(cid:3)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:3),(cid:3)(cid:8),…,(cid:2)(cid:3)(cid:4)(cid:5)(cid:7),(cid:3)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:7),(cid:3)(cid:8),…,(cid:2)(cid:10)(cid:4)(cid:5)(cid:11),(cid:3)(cid:8)     {m1, m2, … } =               Fm =  {f1,1, f2,1, …, fn,1} {f1,2, f2,2, …, fn,2} ··· ··· {f1,m, f1,m, …, fn,m}        {mv1, mv2, …, mvk}             {m1, m2, …, ml}      Fm =  f1,1 , …, PntAmt1, PyValue1, Name1, …, fn,1            {mv1, mv2,… }                                                 … … f1,m , …, PntAmtm, PyValuem, Namem,…, fn,m    Flattened Dataset  (cid:2)(cid:3)(cid:4)(cid:5)(cid:3),(cid:7)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:3),(cid:7)(cid:8),…,(cid:2)(cid:3)(cid:4)(cid:5)(cid:7),(cid:7)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:7),(cid:7)(cid:8),…,(cid:2)(cid:10)(cid:4)(cid:5)(cid:11),(cid:3)(cid:8)   … (cid:2)(cid:3)(cid:4)(cid:5)(cid:3),(cid:12)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:3),(cid:12)(cid:8),…,(cid:2)(cid:3)(cid:4)(cid:5)(cid:7),(cid:12)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:7),(cid:12)(cid:8),…,(cid:2)(cid:10)(cid:4)(cid:5)(cid:11),(cid:12)(cid:8)   (cid:2)(cid:3)(cid:4)(cid:5)(cid:3),(cid:3)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:3),(cid:3)(cid:8),…,(cid:2)(cid:3)(cid:4)(cid:5)(cid:7),(cid:3)(cid:8),(cid:2)(cid:7)(cid:4)(cid:5)(cid:7),(cid:3)(cid:8),…,(cid:2)(cid:10)(cid:4)(cid:5)(cid:11),(cid:3)(cid:8)     {m1, m2, … } = T = {A1, A2, . . . , An}

dj(x∗

j |Aj) ≡ φj(x∗) =

ADIFA is inspired by the Parzen-Window density esti-
mation method [28]. Similar to Parzen-Window, ADIFA
takes the approach of the non-parametric density esti-
mation and assumes a probabilistic generative model for
the observed data. However, unlike the Parzen-Window,
ADIFA is a semi-supervised, meta-learning based algo-
rithm. The algorithm learns multiple univariate mod-
els, each of which is responsible for approximating
the density-function of a single related attribute, hence,
permits per-attribute anomaly detection. Given a test
XML document , the univariate density-function models
compute a series of per-attribute normality scores. These
scores are then combined via yet another univariate
density-function model (i.e., the meta-classiﬁer), which
outputs the ADIFA prediction. We begin by discussing
some preliminaries and then give a formal deﬁnition of
ADIFA.

Preliminaries
Let T = {x1, x2, . . . , xm} be a training set containing
m instances, drawn i.i.d. from X {normal}, the group of
normal instances. We assume that the instances in T
well represent the normal class. For our purpose, it is
convenient to represent T as a bag of n independent
features (attributes):

where Aj = {aj,1, aj,2, . . . , aj,m}, j = 1, . . . , n is the set
of values of the jth attribute in T . We assume that
Aj is drawn i.i.d. from an unknown density function
dj. Let x ∈ X be an instance to be classiﬁed. The
ADIFA tasks are: (1) approximate the density functions
{dj}j = 1, . . . , n; and (2) use these approximate models
to calculate the likelihoods (or probability), that the
instance x was drawn from X {normal}.
To safely use a set of univariate models for anomaly
detection in multivariate vectors, as proposed in this
algorithm, the next proposition should hold with high
probability: ∀x1, x2 ∈ X

D(x1|T ) > D(x2|T ) ⇒
Ψ(d1(x1,1|A1), d2(x1,2|A2) . . . ) > Ψ(d1(x2,1|A1), d2(x2,2|A2) . . . )

Where D(·) is a multivariate anomaly detection model;
d(·) is a per-attribute univariate anomaly detection
model; and Ψ(·) is an aggregation function (e.g., arith-
metic mean, geometric mean, and harmonic mean).

D(x1|T ) is the normality score of instance x1, with
respect to the train-set T , whereas di(x1,i|A1) is the
normality score of the ith attribute of instance x1.
In other words, we assume that anomalous patterns
can be effectively detected by aggregating attribute-wise
normality scores. Our experimental results show that
in many domains, especially in computer and network
security, this assumption holds. Notice that in order
to cover all anomalous patterns, one should also ad-
dress uncommon situations, in which examples have an
anomalous combination of normal values. This can be
done, for example, by learning association-rules.

6

3.4 Training Process
First, a set of n density-function models D =
{d1, d2, . . . , dn} are learned. A model, dj : R → [0, 1]
is responsible for approximating the density-function of
the corresponding attribute, Attrj, where j = 1, . . . , n. To
calculate the attribute-wise normality scores of a given
test instance, x∗ =< x∗
1, . . . , x∗
n >, we use the Gaussian
radial-basis function (RBF), with a normalization factor
bj = (2π (cid:98)σj

2)− 1
2 :

φj(x∗) =

1
m

m
(cid:88)

i=1

bj ∗ ρj((cid:13)

(cid:13)ai,j − x∗
j

(cid:13)
(cid:13)), j = 1, . . . , n

Where ai,j is the jth attribute of train instance i, and m
is the cardinality of Aj. We chose the exponential-decay
distance function for the RBF function:
(cid:13)ai,j − x∗
j

(cid:13)
j )2
(cid:13)) = e−τj (ai,j −x∗

ρj((cid:13)

The coefﬁcient τj controls the similarity decay speed,
which also controls the smoothness of the density func-
tion and therefore, its generalization power. The per-
attribute, density approximation model, d(·) is deﬁned
as follows:

1
m

m
(cid:88)

i=1

1

(cid:113)

2π (cid:98)σj

2

∗ e−τj (ai,j −x∗

j )2

(1)

To compute the instance-wise normality score, s(x),
we use an aggregate function Ψ(·) on the weighted per-
attribute normality scores as follows:

s(x)=Ψ (α1 ∗ d1(x1|A1), α2 ∗ d2(x2|A2), . . . , αn ∗ dn(xn|An))

(2)

Where the weights, α1,...,n, reﬂect the complexity of
learning the corresponding approximation models. The
weights can be obtained by using methods such as
entropy-based weighting

αj = 1 −

H(Aj)
i=1H(Ai)

Σn

(3)

or by using techniques such as[20] to efﬁciently esti-

mate the per-attribute doubling dimension.

The notation H(Aj) in Equation 3 speciﬁes the
entropy of attribute Aj. Note that
the
attribute’s entropy is,
is to accurately
the easier it
approximate (i.e. to learn) its density function. Since
H(Aj )
i=1H(Ai) ∈ [0, 1], the weights, αj, calculated by this
Σn
technique are also in the range of [0, 1].

the lower

Lastly, the training applies Equation 2 to each of the m
instances to compute the normality scores of the training
instances. The training employs a leave-one-out scheme
i.e. each training instance is used only once as a test
instance, while the rest m − 1 instances are used as the
training set T . Let S be the normality scores group of
the training instances, i.e., S = {s(x1), s(x2), . . . , s(xm)}.

7

Fig. 6. ADIFA training process

Fig. 7. ADIFA on-line classiﬁcation process

3.5 Classiﬁcation with ADIFA

The classiﬁcation of a test sample is done in two steps.
First, the test instance’s normality score, s(x∗), is cal-
culated using the technique presented in the previous
section. Then, the likelihood of obtaining such a normal-
ity score, according to the density function, p(x), of the
normal instances, is computed.

Assuming that the normality scores in S are drawn
i.i.d. from p(x), the density function p(x) can be approx-
imated by a single univariate model. This is done using
the same technique presented in Equation 1.

In order to calculate the likelihood of obtaining s(x∗),
the algorithm computes how anomalous s(x∗) is with
respect to S. This is done by approximating the density
function p(x) by using an additional univariate density
approximate, with the parameters s(x∗) and S:

d(s(x∗)|S) =

1
m

m
(cid:88)

i=1

(cid:112)

1
2π (cid:99)σS

2

∗ e−τS (S(xi)−S(x∗))2

(4)

The value obtained by Equation 4 is the output of
ADIFA. In case a classiﬁcation is needed, this value can
be thresholded by a user deﬁned value 0 < C ≤ 1. In
this case, ADIFA predicts anomaly if d(s(x∗)|S) < C and
normal otherwise.

3.6 Anomaly Localization Strategy

Identifying the location of an anomaly within the trans-
action document is a task that the system or service
security ofﬁcer has to deal with when a transaction is
suspected of being anomalous. Locating the anomaly
can be very important, particularly when the anomaly
source may indicate an ongoing attack. Since the location
of an attack-related anomaly is known, some mitigation
measures can be employed. It is also important for the
post-attack analysis, since the location information may
provide the forensic expert with helpful facts regarding
the attacker’s sources, attack methods and propagation.

Although such localization will not provide a complete
operational meaning, such as the anomaly semantic, it
provides valuable information towards this goal.

Once an anomaly is detected, the anomaly localization
is straightforward. Since it is a multi-univariate classiﬁer,
the ADIFA classiﬁer has full knowledge regarding the
normality score of each of the test instance dimensions.
When an anomalous transaction is detected, the ADIFA
classiﬁer identiﬁes the features with the lowest likeli-
hoods. Additionally, the classiﬁer can produce a list of
all features, ordered by their likelihood values.

4 METHODS
In this Section we specify the methods for evaluating the
algorithms proposed in this paper. First, in Section 4.1,
we describe the classiﬁers we used. Then in Section 4.2,
the performance metrics are detailed. Next, in Section
4.3, we present the datasets used in our experiments.
Lastly, in section 4.4 we present an application for in-
jecting real attacks in XML transactions.

4.1 Classiﬁers

We made use of six classiﬁers trained by four anomaly
detection (one-class) algorithms: OC-GDE, OC-PGA, 1-
SVM [34], and ADIFA1. We selected these classiﬁers,
as they represent the prominent branches of one-class
algorithms: nearest neighbor (OC-PGA), density estima-
tion (OC-GDE) and boundary (1-SVM). The ﬁrst three
algorithms are our own adaptations of two well-known
unsupervised algorithms to one-class learning. Table 1
speciﬁes the setup parameters of all seven classiﬁers
used for the experiments. Note that for ADIFA classiﬁers,
we used equation 3 to compute the individual features
weights. We used a classiﬁcation threshold, C = 0.5 for
all of the classiﬁers, i.e., instance x ∈ normal if p(x) ≤ 0.5,
where p(x) ∈ [0, 1] is the classiﬁer output.

1. The implementation of the mentioned algorithms can be down-
loaded from: http://sourceforge.net/projects/xml-ad/ﬁles/AD.rar/
download

Test instance x*Train-set TTrain-set TTraining-set TPer-attribute Normality Scored1(x*1|A1)d2(x*2|A2)dn(x*n|An)Normality scores (per-attribute)d(s(x*)|S)Prediction:  l(x*∈X{normal}|S)s(x*)Ψ(•)Aggregation FunctionMeta-ClassifierS = {s(x1), s(x2),…, s(xm)}Per-attribute Normality Scored1(xi,1|A1)d2(xi,2|A2)dn(xi,n|An)Normality scores (per-attribute)s(xi)Ψ(•)Aggregation FunctionNormality scoresxix*Train instances’ normality scoresInstance normality scoreRepeat for all m train instancesS = {…}Test instance x*Train-set TTrain-set TTraining-set TPer-attribute Normality Scored1(x*1|A1)d2(x*2|A2)dn(x*n|An)Normality scores (per-attribute)d(s(x*)|S)Predictions(x*)Ψ(•)Aggregation FunctionMeta-ClassifierS = {s(x1), s(x2),…, s(xm)}Per-attribute Normality Scored1(xi,1|A1)d2(xi,2|A2)dn(xi,n|An)Normality scores (per-attribute)s(xi)Ψ(•)Aggregation FunctionNormality scoresxix*Train instances’ normality scoresInstance normality scoreRepeat for all m train instancesS = {…}One-Class Peer Group Analysis

The One-Class Peer Group Analysis (OC-PGA), is an
adaptation of
the unsupervised Peer-Group-Analysis
method (PGA), proposed by Eskin et al. [18] for the one-
class learning domain. The algorithm identiﬁes anoma-
lies as points in low-density regions of the feature space.
An anomaly score is computed at point x as a function
of the distances from x to its k nearest neighbors. Al-
though PGA is actually a ranking technique applied to
a clustering problem, we implemented it as a one-class
classiﬁer. Given a training set S, a test point x is classiﬁed
as follows. For each xi ∈ S, we pre-compute the distance
from xi to its nearest neighbor in S, and denote it by:
dnn(xi, S \ {xi}). To classify x, the distance to its nearest
neighbor in S, dnn(x, S) is computed. The test point x is
classiﬁed as an anomaly if dnn(x, S) is within a percentile
α or higher among {dnn(xi, S \ {xi})}xi∈S ; otherwise, it
is classiﬁed as normal.

One-Class Global Density Estimation

The Global Density Estimation (GDE), proposed in [23],
is also an unsupervised density-estimation technique,
which uses the nearest-neighbor technique. Given a
training set S and a real value r, one computes the
anomaly score of test point x by comparing the number
of training points falling within the r-ball Br(x) about x
to the average of |Br(xi) ∩ S| over all xi ∈ S. We set
r to be twice the set average of dnn = (xi, S \ {xi})
to ensure that the average number of neighbors is at
least one. To adapt the GDE into the one-class domain
(OC-GDE), we used the following heuristic function to
threshold the anomaly scores, since it achieved a low
classiﬁcation error on the data: x is labeled as normal if
e(−(nr(x) − Nr)/σr) > 1/2 where nr(x) is the number
of neighbors of x in Br(x) ∩ S (points in S at a distance
no higher than r from x), Nr and σr are the average and
standard deviation of {nr(xi)}xi∈S, respectively.

ADIFA-AM AD-AM Ψ = Arithmetic Mean
ADIFA-HM AD-HM Ψ = Harmonic Mean
ADIFA-GM AD-GM Ψ = Geometric Mean

Algorithm Classiﬁer Acronym
ADIFA
ADIFA
ADIFA
OC-GDE
OC-PGA
1-SVM

OC-GDE
OC-PGA
1-SVM

GDE −
PGA k = 1 (1-nearest neighbor), α = 0.1
SVM Kernel = RBF (Gaussian), ν = 0.05

Parameters

TABLE 1
Classiﬁer’s setup parameters. The parameters shown
are only those that are non-default.

4.2 Performance Metrics

The main measure used to evaluate the classiﬁcation
performance of XML-AD was the area under the receiver
operating characteristic (ROC) curve, which is a graphi-
cal plot of the speciﬁcity vs. 1-sensitivity for a classiﬁer
system since its discrimination threshold is varied. The
ROC can also when represented equivalently by plotting

8

the fraction of true positives (TPR = true positive rate)
vs. the fraction of false positives (FPR = false positive
rate). ROC analysis provides tools for selecting possible
optimal models and discarding suboptimal ones inde-
pendently from (and prior to specifying) the cost context
or the class distribution. ROC analysis is strongly related
to cost/beneﬁt analysis in diagnostic decision-making.
Widely used for many decades in medicine, radiology,
psychology, and other areas,
it has been introduced
rather recently into machine-learning and data mining.
In order to estimate the area under ROC (AUC), a 5x2
cross-validation procedure was performed [14]. In each
of the cross-validation iterations, the training set was
randomly partitioned into two disjoint instance subsets.
In the ﬁrst fold, the ﬁrst subset was utilized as the
training set, while the second subset was utilized as the
testing set. In the second fold, the role of the two subsets
was reversed. This process was repeated ﬁve times. The
same cross-validation folds were implemented for all
algorithms in all experiments.

The one-tailed paired t-test with a conﬁdence level
of 95% veriﬁed whether the differences in AUC be-
tween tested classiﬁers were statistically signiﬁcant. To
conclude which classiﬁer performed best over multiple
datasets, we followed the procedure proposed in [13].
We ﬁrst used the adjusted Friedman test to reject the
null hypothesis, followed by the Bonferroni-Dunn test
to examine whether a speciﬁc classiﬁer produces signif-
icantly better AUC results than the reference method.

4.3 XML transactions Collections

To evaluate the XML-AD framework we used three
transaction collections.The ﬁrst two, Insurance and In-
ventory, are collections of XML transactions of real in-
formation system. They contain only transactions related
to the normal class. i.e., they do not contain any attacks
or anomalies of interest. The third collection, ARP-D,
contains XML transactions derived from ARP packets,
which were captured on the local network at the BGU
university (2011). This collection contains ARP transac-
tions related to an actual ARP attack.

Insurance & Inventory

The Insurance collection contains 3,340 insurance trans-
actions, taken from a real insurance information system
that follows the ACORD standard [1]
all of which
are labeled as normal. Since the insurance XML trans-
actions contain many private data items, a pre-process
of anonymization was employed to ensure the privacy
of the insurees. The second collection, Inventory, is com-
prised of transactions that were extracted from a logistic
information system which mainly consists of supply
data. The collection contains 4,000 transaction, all labeled
as normal.
Both XML transactions collections were put through the
processes of feature extraction and dataset-ﬂattening, de-
scribed in Sections 3.1 and 3.2. These processes extracted

9

Fig. 8. Five types of XML attacks, embedded in target XML ﬁle, shown in (a), using the AI module

1,021 features from each transaction in the Insurance
collection, and 285 features from each transaction in the
Inventory collection, regardless of the original transac-
tion size or the number of elements.

ARP-D

The ARP abbreviation stands for Address Resolution
Protocol [31]. The ARP-D dataset contains actual ARP
spooﬁng attacks directed against the computer network
of Ben-Gurion University. During an ARP attack, the
attacker temporarily steals the IP addresses of its victims
and as a result all of their trafﬁc was redirected to
the attacker without the knowledge or consent of the
victims. The collection is constructed from 9,039 ARP
packets, captured, and converted to XML using the
Wireshark tool [38] at the local-network, for the duration
of 10 minutes. The ARP spooﬁng attack began 5 minutes
after the Wireshark tool had began snifﬁng the network
trafﬁc and lasted for 5 additional minutes. During the
recording time period, there were 173 active computers
on the local network, 27 of which were attacked. The
XML ﬁles in this collection were put through a process
of feature extraction and dataset-ﬂattening, similar to the
process that was applied to the Insurance and Inventory
collections. This process extracted 24 attributes from each
ARP XML transaction. In order to examine different fea-
ture aggregation methods, two distinct datasets ARP-D1
and ARP-D2, which differ by their feature aggregation
properties, were constructed.

4.4 Anomalous XML Transactions

To properly evaluate XML-AD, it was necessary to exper-
iment with both normal and anomalous XML transaction
documents from real systems. However, in many real-
life systems, as is the case with the Insurance and In-
ventory systems, XML documents containing anomalies
of interest (i.e., anomalies that can potentially harm the
system), are extremely rare, to the point where entire
data collections can be safely presumed to be normal.

To overcome this problem, we implemented an XML
anomaly injection (AI) module, which embeds instances
of real XML attacks into target XML transactions, which
are otherwise normal XML transactions. The AI module
implements ﬁve different attack classes: Value poison-
ing, Cross-site scripting (XSS), an XML injection using
CDATA, an XPAth injection, and Data leakage in text
elements.

The Value poisoning attack is performed by changing
a single numerical element in the target XML transaction
to a random value. The XSS and the XPath attacks are
represented by strings (either as clear or encoded text)
and are embedded into a randomly chosen textual XML
element. The CDATA XML injection attack embeds four
different malicious encoded java scripts into CDATA
elements. A CDATA XML attacks is injected between
two randomly selected adjacent XML elements of the
target XML transaction. Lastly, the data-leakage attack is
performed by replacing the content of a textual XML ele-
ment with randomly selected sentences from the known
children book: The wonderful Wizard of Oz. The mentioned
XML attack are demonstrated in Figure 8.

The AI module accepts two parameters: a target XML
transaction and an anomaly index, which is the ratio
between the number of the anomalies to inject and the
number of XML elements in the target XML transaction.
When constructing an anomalous XML transaction,
the AI module embeds one or more instances of the
mentioned attack classes into the target XML transaction
at random. In order to avoid being ruled out by the XML
parser, the AI module embeds the attacks only where
it is allowed according to the XSD schema, .i.e., XPath
injections are only embedded into textual XML elements,
Value poisoning is only applied to numeric elements, etc.
Notice that the above mentioned attacks are bound
to introduce anomalies into the target XML transactions
because they represent very rare events, with respect to
the content of the normal transactions. These anomalies
may or may not maliciously affect the consumer infor-
mation system. Either way, such transactions should be

  … <TxRequest>     <TransRefGUID>D640836E-E124</TransRefGUID>     <TransType type="0103"/>     <ProcessID>1003</ProcessID>     <TransMode mode="2"/>     <TransRefText>SM097125</TransRefText>  ...  </TxRequest> …  … <TxRequest>     <TransRefGUID>D640836E-E124</TransRefGUID>     <TransType type="0103"/>     <ProcessID> -1 </ProcessID>     <TransMode mode="2"/>     <TransRefText>SM097125</TransRefText>  ...  </TxRequest> …  … <TxRequest>     <TransRefGUID>D640836E-E124</TransRefGUID>     <TransType type="0103"/>     <ProcessID> 1003 </ProcessID>     <TransMode mode="2"/>     <TransRefText> a secret message </TransRefText>  ...  </TxRequest> … (a) Source transaction XML  (b)  Value poisoning  (c) Using text elements for leaking information   … <TxRequest>     <TransRefGUID>D640836E-E124</TransRefGUID>     <TransType type= %3c%73%63%72%69%70%74%3e%77… />     <ProcessID>1003</ProcessID>     <TransMode mode="2"/>     <TransRefText>SM097125</TransRefText>  ...  </TxRequest> …  … <TxRequest>     <TransRefGUID>D640836E-E124</TransRefGUID>     <TransType type="0103"/>     <ProcessID>1003</ProcessID>     <TransMode mode="2"/>     <TransRefText> <![CDATA[<IMG SRC=http://…/logo.gif onmouseover=javascript:alert('Attack');>]]> </TransRefText>     ...  </TxRequest> …  … <TxRequest>     <TransRefGUID>D640836E-E124</TransRefGUID>     <TransType type="0103"/>     <ProcessID>1003</ProcessID> ...       <uid> ' or 1=1 or uid=' </uid>        <password>1234</password> …  </TxRequest> … (d)  Cross Site Script injection (encoded to hex) (e) XML Injection using CDATA  (f) XPath Injection  detected as anomalous due to the reasons mentioned in
Section 1.2. Finally, we would like to stress that in order
to train the anomaly detection classiﬁers, only normal-
labeled XML documents were used. The anomalous
XML transactions were only used for testing.

5 EVALUATION
The evaluation of the XML-AD framework (Fig. 1) fo-
cused on (a) determine whether the proposed approach
can effectively detect anomalies in XML documents and
(b) which machine-learning approach is best suited for
that task. Regarding the ADIFA learning algorithm, we
examined which of the proposed aggregation functions
produces the best classiﬁcation performance. Addition-
ally, we examined the anomaly detection capabilities of
ADIFA in other domains and compared it to similar
algorithms. To examine the abovementioned aspects, we
implemented an evaluation framework according to the
scheme shown in Figure 1 and the classiﬁers in Section
4.1. The following sections describe the experiments and
results.

5.1 Anomaly Detection Framework

In this section we present our evaluation of the XML-AD
framework in detecting anomalous XML transactions.
First, we investigate the performance of the available
one-class classiﬁers when applied to our XML transac-
tions datasets. Next, we examine the sensitivity of the
classiﬁers to the level of abnormality, within the XML
attacks. Finally, we show the learning curves of our one-
class classiﬁers.

5.1.1 XML Anomaly Detection
Our ﬁrst experiment evaluates the dataset ﬂattening and
the anomaly detection aspects of the XML-AD frame-
work. First, we examine whether the dataset ﬂattening
process damages the ability to detect XML attacks, due
to the lossy nature of the ﬂattening process. Next, we
compared the two classiﬁcation approaches, i.e., the mul-
tivariate and multi-univariate approaches, to determine
which one, if any, achieves a better detection of XML
anomalies.

The 1-SVM, OC-GDE and OC-GDE classiﬁers be-
long to the multivariate approach, whereas the ADIFA-
GM, ADIFA,HM and ADIFA-AM classiﬁers are multi-
univariate. The experiment results are presented in Table
2. The classiﬁers‘ ROC curves, which are depicted in
Figure 9, show that it is possible to obtain a detection
rate of 89%, with only 0.2% false positive rate.

These results show that for each tested dataset, there
was at least one classiﬁer that successfully distinguished
between the anomalous XML transactions and the nor-
mal XML transactions. This leads to the conclusion that
the lossy ﬂattening process preserves the information
needed for detecting XML anomalies.

Examining the classiﬁers performance, we notice that
ADIFA-GM achieves the highest average AUC among all

10

Anomaly
index
1%
5%
10%
Average
1%
5%
10%
Average
n/a
n/a
Average

Datasets

Insurance

Inventory

ARP-D1
ARP-D2

Total

Classiﬁers
AD-AM AD-GM AD-HM GDE 1-SVM PGA
0.944 0.498 0.501 0.524
0.995 0.574 0.498 0.532
0.999 0.589 0.504 0.539
0.979 0.554 0.501 0.532
0.884 0.500 0.537 0.527
0.954 0.501 0.562 0.642
0.982 0.501 0.575 0.744
0.940 0.501 0.558 0.638
0.799 0.634 0.643 0.929
0.926 0.635 0.635 0.936
0.949 0.639 0.634 0.933
0.957 0.555 0.556 0.672

0.570
0.692
0.818
0.693
0.590
0.683
0.757
0.677
0.873
0.900
0.980
0.759

0.951
0.996
0.998
0.981
0.887
0.956
0.984
0.943
0.871
0.910
0.969
0.964

TABLE 2
Average AUC result for the XML transactions datasets

the tested classiﬁers. The second best result was achieved
by ADIFA-HM (AD-HM). Both results are substantially
better than the other four classiﬁers. The GDE and 1-
SVM classiﬁers performed the worst, with an average
AUC close to 0.5, a value that represents a random
classiﬁer. By further analyzing the results, it is clear that
the choice of aggregation functions plays a crucial part in
ADIFA. In datasets where the dimensionality is relatively
low (ARP-D1 and ARP-D2), the arithmetic-mean yields
better results, whereas in higher-dimensionality datasets,
such as with Insurance and Inventory, ADIFA performs
better when it used with the geometric-mean aggre-
gation function. Interestingly, the results show that all
three multivariate classiﬁers, i.e., GDE, PGA, and 1-SVM,
performed very poorly on Insurance and Inventory, the
datasets which contain genuine XML transactions. These
results support our hypothesis regarding the uselessness
of multivariate anomaly detection models (models that
use multivariate distance function) for detecting anoma-
lies in XML transactions.

5.1.2 Multivariate Vs. Multi-Univariate Classiﬁers

The previous experiment showed that the ADIFA clas-
siﬁers performed substantially better than the other
multivariate classiﬁers, especially when applied to high
dimensionality datasets. To explain these results, we
examine the following hypotheses:

Fig. 9. ROC plot for Insurance dataset with 5% anoma-
lous XML elements in the abnormal transactions

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91True positiveFalse PositiveADIFA-GMADIFA-AMGDE1-SVMADIFA-HMPGA• h1 - multi-univariate classiﬁers are inherently more
accurate than multivariate classiﬁers, when deal-
ing with anomaly detection in high-dimensional
datasets.

• h2 - the classiﬁcation performance depends on the
data modeling method, i.e., compared to the other
the ADIFA algorithm better modeled
classiﬁers,
the information, which resides within the tested
datasets, and hence, it produced a superior anomaly
detection classiﬁer.

To examine the hypotheses, we conducted an exper-
iment in which we transformed the multivariate clas-
siﬁers into multi-univariate systems by training each
multivariate classiﬁer with a single dimension and then
aggregating their results using the geometric-mean. Fi-
nally, we compared the new univariate systems with
ADIFA-GM, which also uses the geometric-averaging
aggregation. We denote the new systems as MUSV M ,
MUGDE and MUP GA, where the subscript indicates the
base classiﬁcation algorithm and MU stands for multi-
univariate.

In this experiment we also measured the time it takes
to classify the datasets. Classiﬁcation time (CT) is an
important feature of the anomaly detector, since it dic-
tates the rate in which transactions can be classiﬁed
in real-time. To avoid being a bottleneck for the XML
transaction consuming process, the classiﬁcation time
should be as low as possible.

Datasets

Anomaly
index

Classiﬁers
AD-GM MUSV M MUGDE MUP GA

11

classiﬁers.
Inspecting the classiﬁcation time, we notice that the clas-
siﬁcation systems produce a wide range of results. The
average classiﬁcation time of the slowest classiﬁcation
system, i.e., the MUP GA, was about 40 seconds, which is
about two orders of magnitude higher than the minimal
classiﬁcation time, achieved by the ADIFA algorithm.
Lastly, the presented results validate both h1 and h2.

5.1.3 Classiﬁer Performance Vs. Anomaly Index

In the following experiment, we tested the classiﬁers’
responsiveness to XML anomalies. This experiment had
two goals: 1) to learn how responsive each classiﬁer was
to different levels of transaction anomalies and 2) to ﬁnd
interesting trends, such as, which classiﬁer was more
effective for detecting anomalies in datasets containing a
small percentage of anomalies and which was preferable
for higher rates of anomalies. Such a scenario is indeed
possible since the classiﬁer improvement rate, as a func-
tion of the anomaly index, can differ from one classiﬁer
to the other. If such trend were to be found, an ensemble
of classiﬁers would probably offer the best anomaly
detection solution. To accomplish this experiment our six
classiﬁers were applied to ten variations of the Insurance
and Inventory datasets, each with a distinct anomaly
index (1 to 10).

1%
5%
10%

AUC CT
0.951 0.8
0.996 0.8
0.998 0.7
Average 0.981 0.8
0.888 0.4
0.960 0.4
0.984 0.4
Average 0.945 0.4
0.966 0.2
0.973 0.2
Average 0.969 0.2
0.964 0.5

1%
5%
10%

n/a
n/a

Insurance

Inventory

ARP-D1
ARP-D2

Total

AUC CT
0.477 2.5
0.497 3.0
0.521 2.3
0.498 2.6
0.474 1.4
0.486 1.6
0.496 1.4
0.485 1.5
0.758 1.8
0.610 1.6
0.684 1.7
0.540 2.0

AUC CT
0.506 32.1
0.506 34.9
0.506 29.0
0.506 31.9
0.530 15.0
0.530 18.7
0.533 16.8
0.531 16.9
0.840 2.2
0.873 2.6
0.857 2.4
0.603 18.9

AUC CT
0.861 41.3
0.905 69.9
0.905 33.7
0.890 47.6
0.820 49.3
0.885 84.1
0.910 33.8
0.872 56.7
0.959 2.1
0.963 2.5
0.961 2.3
0.901 39.6

Fig. 10. AUC vs. XML anomaly index (Insurance dataset)

TABLE 3
Average classiﬁcation AUC and classiﬁcation time (sec.)
for the multi-univariate systems on the XML transactions
datasets

The evaluation results, shown in table 3,

indicate
that, on average, transforming a multi-variate classiﬁer
into multi-univariate system increases its classiﬁcation
effectiveness. However, the results are not consistent; the
classiﬁcation performance improvements were +34.2%
, +8.6% and -2.9% for the MUP GA, MUGDE and the
MUSV M ,respectively, compared to their multivariate
versions. The average performance boost, however, was
not enough to outperform the ADIFA algorithm, which,
as in the previous experiment, achieved the highest
classiﬁcation efﬁciency (AUC= 0.964) among the tested

Fig. 11. AUC vs. XML anomaly index (Inventory dataset)

The experimental results, presented in Figures 10 and
11, show that the classiﬁcation effectiveness of ADIFA-
GM and ADIFA-HM is consistently better than the rest

0.40.50.60.70.80.91.012345678910Area Under ROC (AUC)Anomaly index of the abnormal transactionsADHMADGMADAM1-SVMGDEPGA0.40.50.60.70.80.91.012345678910Area UnderROC (AUC)Anomaly index of the abnormal transactionsADHMADGMADAM1-SVMGDEPGAof the tested classiﬁers across all the tested anomaly in-
dexes. Interestingly, if the classiﬁers where to be ranked
according to their relative AUC performance, (i.e., the
best classiﬁer is ranked “1”, the second best is ranked
“2”, and so on) the resulted ranking would be the
same for every anomaly index. This result indicates that
the dataset’s anomaly index has no inﬂuence on the
responsiveness of the classiﬁers to anomalies. Thus, the
responsiveness must be an intrinsic feature of the induc-
ers, which were used to construct the tested classiﬁers.
We also observed that the multivariate classiﬁers (i.e.,
PGA, 1-SVM and GDE) did not show any improvement
in the Insurance dataset as the anomaly index increased.
This, however, was not the case in the Inventory dataset,
which has a much lower dimensionality in comparison.
This further supports our claim that anomaly classiﬁers,
which are based on multivariate distance functions, are
less suitable for high dimensional datasets, particularly,
for XML anomaly detection.

5.2 Learning Curves

A learning curve reﬂects the relation between the efforts
to learn and the classiﬁcation performance of the trained
classiﬁer. Thus, a learning curve is an indication of
the effectiveness of the learning algorithm. Additionally,
since the learning curve indicates the rate in which the
learning process converges, it can be used to determine
the optimal amount of data for training (i.e., the optimal
“learning effort”).

In the current experiment we derived ten new datasets
from each of our original XML datasets. Each derived
dataset contained a ﬁx number of anomalous XML trans-
actions, but a different amount of normal instances, of
which half were used for training the classiﬁers. In order
to create the derived dataset, we applied the following
process to each of the original XML transaction datasets.
First, we divided the original dataset into two subsets:
D− and D+, where D− contained only the normal XML
transactions, while D+ consisted of only the anoma-
lous XML transactions. Next, D− was randomly parti-
tioned into ten equal sized groups: D−
10. Then,
the following datasets were constructed: D1=D+ (cid:83) D−
1 ,
D2=D1
10. Finally, we evaluated
the tested learning algorithms on D1, . . . , D10. The re-
sults are depicted in Figures 12 - 14.

2 , . . . , D10=D9

1 , . . . , D−

(cid:83) D−

(cid:83) D−

The learning curves of the Insurance and Inventory
datasets show that only ADIFA-GM and ADIFA-HM
signiﬁcantly improved their classiﬁcation performance
as the number of training instances increased. In con-
trast, the other classiﬁers improved by a much lower
rate, if any. Among which, the 1-SVM classiﬁer achieved
the best performance improvement, although its learning
curve was not monotonic.

The situation in the ARP-D dataset was quite different
from the two previously discussed datasets. The clas-
siﬁcation performance of ADIFA-AM was on par with
ADIFA-GM and better than that of ADIFA-HM. These

12

Fig. 12. Learning Curves for Insurance dataset

Fig. 13. Learning Curves for Inventory dataset

three classiﬁers reached their optimal performance when
they were applied to the smallest training-set, i.e., 113
instances. The GDE classiﬁer has a negative learning
curve, were its classiﬁcation performance deteriorated as
the number of training instances increased. This trend
was somewhat restraint when the training set contained
more than 400 instances.

5.3 Can ADIFA Generalize to Other Domains?

In this section we examine whether the ADIFA algo-
rithm can detect anomalies in other domains as well
as it did within the XML transaction domain, i.e., does
the multi-univariate approach of ADIFA work in other
classiﬁcation domains. To answer this question, in the
following experiment we compare the performance of
ADIFA with the performance of the anomaly detection
algorithms in Table 1, and also, with the Local Outlier
Factor algorithm [7] 2. The Local Outlier Factor (LOF)
is a density-based anomaly detection algorithm, which
is usually applied for unsupervised anomaly detection
(i.e., when none of the training instances are labeled). In
order to apply LOF in the one-class paradigm, we apply
the following rule to the LOF output. We ﬁrst use the
LOF algorithm to calculate a local outlier factor value,
lof (x), for each instance x in the training set, X. Let
LOFmax = maxx∈X lof (x) be the highest calculated local
outlier factor. A test instance x‘ is labeled as anomalous if

2. The LOF algorithm was considered also for the XML-AD frame-
work evaluation, but was ruled out due to the algorithm’s inability
to handle high dimensionality data, such as with the Insurance and
Inventory dataset

0.40.50.60.70.80.91.0Area under ROC curve (AUC)Number of Training InstancesADAMADGMADHMGDE1-SVMPGA0.40.50.60.70.80.91.0Area under ROC curve (AUC)Number of Training InstancesADAMADGMADHM1-SVMGDEPGA13

Classiﬁers
AD-AM AD-GM AD-HM OC-GDE OC-PGA 1-SVM

AD-GM
AD-HM
OC-GDE
OC-PGA
1-SVM
LOF

=
=
+
+
+
=

=
+
+
+
+

+
+
+
=

=
=
−

=
−

−

TABLE 5
The signiﬁcance of the difference between the classiﬁers
using the AUC metric. Row i and column j contain: a ‘+’
in case the method in the column is signiﬁcantly better
than the method in the row; a ‘-’ indicates the opposite,
and a ‘=’ indicates that the difference is insigniﬁcant

The results show that, on average, the three ADIFA
variations perform better, than the multivariate classi-
ﬁers: OC-GDE, OC-PGA, 1-SVM, and LOF. The non-
parametric Bonferroni Dunn test shows that while there
is no signiﬁcant difference between the ADIFA varia-
tions, only the ADIA-GM is signiﬁcantly better than all
the multivariate classiﬁers. The comparable performance
of the three ADIFA variations may be attributed to the
low-dimensionality of the selected UCI datasets. Specif-
ically, in low-dimensionality, the aggregation functions
which are used by the different ADIFA variations, are
highly correlated, leading to similar classiﬁcations.

6 CONCLUSIONS AND FUTURE WORK

This paper presented a new framework for detecting
XML anomalies. Our experiments showed that the ap-
proach taken in XML-AD is very useful for detecting
various types of anomalies, some of which originate
from possible attacks on the structure and content of
XML transaction documents.

One of the foundational challenges we faced in this
research was ﬁnding general and efﬁciently predictive
XML features that could be extracted from any XML
transaction. We developed an automatic feature extrac-
tion process in which both the XML content and struc-
ture features were addressed.

A key feature of

the proposed framework is its
unique method for transforming complex XML features
into ﬁxed-length feature-vectors (i.e., instance ﬂattening).
This feature makes it possible to use general anomaly de-
tection algorithms, which are readily available. The price
for this XML features transformation was relatively low,
both in computation time and in information-loss, since
the most critical feature values (for deciding whether
the instance is abnormal) were preserved during the
ﬂattening process.

Most of the prominent existing XML anomaly de-
tection algorithms are based on the association-rules
or multivariate distance function, which both perform
poorly in high dimensions. We therefore proposed a new
algorithm, ADIFA, that is comprised of multiple uni-
variate models. Our evaluation demonstrated ADIFA’s

Fig. 14. Learning Curves for the ARP-D dataset

LOFmax ≤ lof (x‘), where lof (x‘) is the calculated local
outlier factor of the test instance x‘. This rule is suitable
for our purpose since LOFmax is the maximal observed
local outlier factor value in the entire training set, and
therefore, approximately bounds the maximal acceptable
local outlier factor value for normal test instances.

We selected 30 popular datasets from the widely used
UCI repository [19]. The datasets vary across characteris-
tics such as the number of target classes, instances, input
features, and feature types (nominal, numeric). In order
to have only two classes in each dataset, we only selected
the instances of the two most prominent classes. Similar
to the previous experiment, only instances of a single
class (the ﬁrst of the deﬁned classes), where used for
training, while the instances of the second class were
used strictly for evaluation. The results are displayed in
Table 4 and their statistical signiﬁcance is presented in
Table 5.

Classiﬁers

LOF

PGA

1-SVM

AD-AM AD-GM AD-HM GDE
Datasets
0.865 (4) 0.909 (2) 0.938 (1) 0.803 (5) 0.522 (7) 0.627 (6) 0.883 (3)
Audiology
0.946 (2) 0.940 (3) 0.921 (4) 0.741 (5) 0.652 (6) 0.638 (7) 0.968 (1)
Balance-Scale
0.498 (5) 0.522 (3) 0.537 (2) 0.416 (6) 0.508 (4) 0.541 (1) 0.394 (7)
Breast-cancer
0.803 (1) 0.766 (2) 0.707 (3) 0.681 (4) 0.601 (6) 0.491 (7) 0.671 (5)
C.H. Disease
0.832 (1) 0.832 (2) 0.807 (3) 0.668 (5) 0.605 (6) 0.500 (7) 0.763 (4)
Credit-rating
0.940 (4) 0.963 (1) 0.963 (2) 0.819 (6) 0.634 (7) 0.879 (5) 0.942 (3)
Ecoli
0.815 (1) 0.781 (2) 0.722 (3) 0.701 (5) 0.577 (6) 0.492 (7) 0.709 (4)
Heart-statlog
0.811 (3) 0.825 (2) 0.831 (1) 0.701 (5) 0.606 (6) 0.484 (7) 0.714 (4)
Hepatitis
0.781 (2) 0.794 (1) 0.771 (3) 0.537 (5) 0.523 (6) 0.512 (7) 0.693 (4)
Horse-colic
0.816 (1) 0.786 (2) 0.739 (4) 0.761 (3) 0.690 (5) 0.503 (7) 0.640 (6)
H. Disease
0.799 (6) 0.901 (3) 0.955 (1) 0.696 (7) 0.895 (4) 0.812 (5) 0.926 (2)
Ionosphere
0.998 (4) 1.000 (1) 1.000 (1) 0.894 (7) 0.980 (5) 0.931 (6) 1.000 (1)
Iris
0.943 (1) 0.910 (2) 0.873 (3) 0.500 (7) 0.611 (5) 0.568 (6) 0.856 (4)
Labor
0.810 (1) 0.785 (3) 0.756 (5) 0.774 (4) 0.606 (6) 0.563 (7) 0.803 (2)
Lymphography
0.985 (4) 0.985 (3) 0.995 (1) 0.915 (5) 0.765 (6) 0.725 (7) 0.990 (2)
Mfeat
0.925 (2) 0.910 (3) 0.888 (4) 0.666 (5) 0.628 (6) 0.618 (7) 0.995 (1)
Mushroom
0.858 (4) 0.909 (3) 0.936 (2) 0.684 (6) 0.844 (5) 0.583 (7) 0.973 (1)
Page-blocks
1.000 (2) 1.000 (1) 0.999 (3) 0.826 (6) 0.978 (5) 0.749 (7) 0.998 (4)
Pen-Digits
0.522 (2) 0.512 (3) 0.509 (4) 0.447 (7) 0.490 (5) 0.583 (1) 0.469 (6)
Diabetes
0.664 (1) 0.587 (3) 0.535 (7) 0.564 (5) 0.567 (4) 0.547 (6) 0.636 (2)
Primary Tumor
0.998 (1) 0.995 (4) 0.997 (2) 0.849 (6) 0.975 (5) 0.824 (7) 0.996 (3)
Segment
0.825 (1) 0.823 (2) 0.820 (3) 0.705 (5) 0.616 (6) 0.517 (7) 0.779 (4)
Sick
0.804 (3) 0.837 (1) 0.827 (2) 0.607 (4) 0.582 (5) 0.555 (6) 0.494 (7)
Spam-base
0.986 (3) 0.997 (2) 0.997 (1) 0.575 (5) 0.549 (6) 0.484 (7) 0.968 (4)
Splice
0.724 (4) 0.757 (3) 0.785 (2) 0.679 (5) 0.626 (6) 0.482 (7) 0.802 (1)
Vehicle
0.959 (3) 0.972 (2) 0.983 (1) 0.721 (5) 0.488 (7) 0.757 (4) 0.672 (6)
Vote
0.695 (4) 0.731 (3) 0.758 (2) 0.797 (1) 0.627 (6) 0.582 (7) 0.680 (5)
Vowel
Waveform
0.832 (4) 0.849 (2) 0.840 (3) 0.627 (7) 0.710 (6) 0.831 (5) 0.861 (1)
W-Breast-Cancer 0.991 (2) 0.992 (1) 0.984 (3) 0.797 (6) 0.923 (5) 0.610 (7) 0.969 (4)
0.990 (4) 0.999 (1) 0.999 (2) 0.896 (6) 0.974 (5) 0.500 (7) 0.994 (3)
Mfeat-Factors
Average
0.85 (2.67) 0.85 (2.2) 0.85 (2.6) 0.70 (5.3) 0.68 (5.6) 0.62 (6.1) 0.81 (3.5)

TABLE 4
Average AUC result for the UCI datasets. Inside the
parenthesis is the AUC rank of the tested classiﬁer.

0.40.50.60.70.80.91.0Area under ROC curve (AUC)Number of Training InstancesADAMADGMADHM1-SVMGDEPGA14

performance superiority over four related algorithms (1-
SVM, OC-PGA, OC-GDE and LOF) both in detecting
anomalies in XML transactions and in other domains.

Future work may include evolving our XML-AD
framework towards a transaction ﬁltering system, which,
in addition to performing anomaly detection, could also
prevent system attacks, i.e., a machine-learning based
XML-ﬁrewall.

REFERENCES

[1] Acord Insurance Data Standards. Website. http://www.acord.

org.

[2] M. Agyemang, K. Barker, and R. Alhajj. A comprehensive survey
of numeric and symbolic outlier mining techniques. Intell. Data
Anal., 10(6):521–538, 2006.

[3] Z. Bakar, R. Mohemad, A. Ahmad, and M. Deris. A comparative
study for outlier detection techniques in data mining. In Cyber-
netics and Intelligent Systems, 2006 IEEE Conference on, pages 1 –6,
june 2006.

[4] R. J. Beckman and R. D. Cook. Outlier..........s. Technometrics,

25(2):pp. 119–149, 1983.

[23] E. M. Knorr and R. T. Ng. A uniﬁed notion of outliers: Properties
In In Proc. of the International Conference on
and computation.
Knowledge Discovery and Data Mining, pages 219–222. AAAI Press,
1997.

[24] P. Lindstrom. Attacking and defending web services. A Spire

Research Report, 2004.

[25] J. Long, D. G. Schwartz, and S. Stoecklin. An xml distance

measure. In DMIN, pages 119–125, 2005.

[26] M. Markou and S. Singh. Novelty detection: a review - part 1:

statistical approaches. Signal Processing, 83(12):2481–2497, 2003.

[27] E. Moradian and A. H˚akansson. Possible attacks on xml web

services. 2006.

[28] E. Parzen. On estimation of a probability density function and
mode. The annals of mathematical statistics, 33(3):1065–1076, 1962.
[29] A. Patcha and J. Park. An overview of anomaly detection
techniques: Existing solutions and latest technological trends.
Computer Networks, 51(12):3448–3470, 2007.

[30] A. Patcha and J.-M. Park. An overview of anomaly detection
techniques: Existing solutions and latest technological trends.
Computer Networks, 51(12):3448–3470, 2007.

[31] D. Plummer. Rfc 826: An ethernet address resolution protocol–or–
converting network protocol addresses to 48. bit ethernet address
for transmission on ethernet hardware. 1982.

[32] K. Premalatha and A. M. Natarajan. Chi-square test for anomaly
detection in xml documents using negative association rules.
Computer and Information Science, 2(1):35–42, 2009.

[33] O. Raz, P. Koopman, and M. Shaw. Semantic anomaly detection

[5] R. Bellman. Dynamic Programming-Code. Princeton University

in online data sources. In ICSE, pages 302–312, 2002.

[34] B. Schlkopf, J. C. Platt, J. Shawe-taylor, A. J. Smola, and R. C.
Williamson. Estimating the support of a high-dimensional distri-
bution, 1999.

[35] A. Sebyala, T. Olukemi, and L. Sacks. Active platform security
through intrusion detection using naive bayesian network for
In London Communications Symposium. Cite-
anomaly detection.
seer, 2002.

[36] A. Stamos and S. Stender. Attacking web services: The next
generation of vulnerable enterprise apps. BlackHat2005, 2005.
[37] B. Sullivan. Xml denial of service attacks and defenses, 2009.
http://msdn.microsoft.com/en-us/magazine/ee335713.aspx.

[38] T. W. team. Wireshrk. www.wireshark.org.
[39] A. Valdes and K. Skinner. Adaptive, model-based monitoring for
cyber attack detection. In RAID, pages 80–93. Springer, 2000.
[40] A. Vorobiev and J. Han. Security attack ontology for web services.

In SKG, page 42, 2006.

[41] K. Wang, J. J. Parekh, and S. J. Stolfo. Anagram: A content
In RAID, pages

anomaly detector resistant to mimicry attack.
226–248, 2006.

[42] X. Wu, C. Zhang, and S. Zhang. Efﬁcient mining of both positive
and negative association rules. ACM Trans. Inf. Syst., 22(3):381–
405, 2004.

Press, 1957.

[6] T. Bray, C. Frankston, and A. Malhotra. Document content de-
scription for xml. WWW Consortium, Note NOTE-dcd-19980731,
July 1998.

[7] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander. Lof:
Identifying density-based local outliers. In SIGMOD Conference,
pages 93–104, 2000.

[8] A. Bronstein, J. Das, M. Duro, R. Friedrich, G. Kleyner, M. Mueller,
S. Singhal, and I. Cohen. Self-aware services: Using bayesian
networks for detecting anomalies in internet-based services.
In
Integrated Network Management Proceedings, 2001 IEEE/IFIP Inter-
national Symposium on, pages 623–638. IEEE, 2001.

[9] G. Bruno, P. Garza, and E. Quintarelli. Mining rare association rules
by discovering quasi-functional dependencies: an incremental approach.
Information Science Reference (IGI Global), 2010.

[10] G. Bruno, P. Garza, E. Quintarelli, and R. Rossato. Anomaly
detection in xml databases by means of association rules.
In
DEXA Workshops, pages 387–391, 2007.

[11] G. Bruno, P. Garza, E. Quintarelli, and R. Rossato. Anomaly
JDIM,

detection through quasi-functional dependency analysis.
5(4):191–200, 2007.

[12] C. I. Chow, S. Member, and C. N. Liu. Approximating discrete
probability distributions with dependence trees. IEEE Transactions
on Information Theory, 14:462–467, 1968.

[13] J. Demsar. Statistical comparisons of classiﬁers over multiple data

sets. JMLR, 7:1–30, 2006.

[14] T. G. Dietterich. Approximate statistical test for comparing
supervised classiﬁcation learning algorithms. Neural Computation,
10(7):1895–1923, 1998.

[15] D. Eastlake and P. S. Inc. Canonical XML. 2001. W3C Recommen-

dation.

[16] D. Eastlake, J. Reagle, and D. Solo. XML Encryption Syntax and

Processing. 2002. W3C Recommendation.

[17] D. Eastlake, J. Reagle, and D. Solo. XML-Signature Syntax and

Processing. 2002. W3C Recommendation.

[18] E. Eskin, A. Arnold, M. Prerau, L. Portnoy, and S. Stolfo. A geo-
metric framework for unsupervised anomaly detection: Detecting
In Applications of Data Mining in
intrusions in unlabeled data.
Computer Security. Kluwer, 2002.

[19] A. Frank and A. Asuncion. UCI machine learning repository, 2010.
[20] L.-A. Gottlieb and R. Krauthgamer. Proximity algorithms for
nearly-doubling spaces. In APPROX-RANDOM, pages 192–204,
2010.

[21] G. H´ev´ızi, T. Marcinkovics, and A. L ¨orincz. Improving recognition
accuracy on structured documents by learning structural patterns.
Pattern Anal. Appl., 7(1):66–76, 2004.

[22] V. J. Hodge and J. Austin. A survey of outlier detection method-

ologies. Artif. Intell. Rev., 22(2):85–126, 2004.

