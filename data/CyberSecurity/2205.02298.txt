2
2
0
2

y
a
M
4

]

R
C
.
s
c
[

1
v
8
9
2
2
0
.
5
0
2
2
:
v
i
X
r
a

Zero Day Threat Detection Using Graph and Flow
Based Security Telemetry

Christopher Redino, Deloitte∗ Dhruv Nandakumar, Deloitte∗ Robert Schiller, Deloitte∗

Kevin Choi, Deloitte∗

Abdul Rahman, Deloitte∗

Edward Bowen, Deloitte∗

Matthew Weeks, Deloitte∗

Aaron Shaha, Deloitte∗

Joe Nehila, Deloitte∗

Abstract

Zero Day Threats (ZDT) are novel methods used by malicious actors to attack
and exploit information technology (IT) networks or infrastructure. In the past
few years, the number of these threats has been increasing at an alarming rate and
have been costing organizations millions of dollars to remediate. The increasing
expansion of network attack surfaces and the exponentially growing number of
assets on these networks necessitate the need for a robust AI-based Zero Day
Threat detection model that can quickly analyze petabyte-scale data for potentially
malicious and novel activity. In this paper, the authors introduce a deep learning
based approach to Zero Day Threat detection that can generalize, scale, and effec-
tively identify threats in near real-time. The methodology utilizes network ﬂow
telemetry augmented with asset-level graph features, which are passed through a
dual-autoencoder structure for anomaly and novelty detection respectively. The
models have been trained and tested on four large scale datasets that are repre-
sentative of real-world organizational networks and they produce strong results
with high precision and recall values. The models provide a novel methodology to
detect complex threats with low false-positive rates that allow security operators
to avoid alert fatigue while drastically reducing their mean time to response with
near-real-time detection. Furthermore, the authors also provide a novel, labelled,
cyber attack dataset generated from adversarial activity that can be used for valida-
tion or training of other models. With this paper, the authors’ overarching goal is to
provide a novel architecture and training methodology for cyber anomaly detectors
that can generalize to multiple IT networks with minimal to no retraining while
still maintaining strong performance.

1

Introduction

Increased complexity in organizational attack surfaces and data volumes lead to a constant improve-
ment in attack techniques and artiﬁcial intelligence (AI)-driven processes for cyber attacks [1]. Cyber
Threat Detection Solutions play a pivotal role in proactively identifying and responding to these
threats [2] in a manner that can prevent costly damages to organizations [3], that could cost a business
an average of over $1 billion in long-term cost [4]. Today’s petabyte-scale security telemetry and
attack sophistication quickly overcome limitations of traditional Signal-based Intrusion Detection
Systems (SIDS) [2], and require more and more robust AI-based detection mechanisms to iden-
tify hard-to-detect threats. In addition, these systems are largely limited to rules based alerting
mechanisms constructed against indicators of compromise (IoC) gathered from threat research and

∗Deloitte and Touche LLP

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
intelligence. A well known limitation of rules based detections is their inability to match to new
threats (i.e. zero day threats) that may not match the speciﬁc rule. This increasing complexity
coupled with the inherent weakness in native rules based IDS engines allows attackers to continuously
discover novel ways to exploit systems. Currently, ZDTs have an average growth in number of 125%
year-over-year since 2015 [5].

Henceforth, ZDTs are novel Tactics or Techniques under the MITRE ATT&CK Framework
(https://attack.mitre.org) that have not been encountered in threat research / intelligence previously
(i.e. no known rule presently exists to match for the threat). In this paper, we propose a novel
dual-autoencoder approach to ZDT detection utilizing network ﬂow telemetry augmented with asset-
speciﬁc graph features to contextualize said ﬂows. The overarching goal of the proposal is to build a
robust anomaly detection mechanism that can identify ZDTs in near real-time while simultaneously
being able to generalize across different network topologies with minimal retraining. In this work we
also place a heavy emphasis on producing models with a very low false positive rate and high recall
in order to minimize Alert Fatigue on the security operator while delivering actionable results. The
key contributions of this paper are:

• Proposing and implementing a dual-autoencoder approach for network anomaly detection

and attack novelty detection successively.

• Utilizing asset-level graph features in our deep learning architecture alongside ﬂow-level

features.

• Comparing the performance of traditional single-model methods without ﬂow features with

our method to evaluate the models’ ability to generalize across networks.

• Providing a novel, labelled attack dataset consisting of network ﬂow information for use in

model development, benchmarking, and further research.

The paper begins with a literature review discussing previous work in the ZDT detection ﬁeld so
far, followed by a description of training methodology, implementation details, and results. We then
conclude with remarks on some of our key design decisions, and description of future work.

2 Review and Shortcomings in the Literature

2.1 Review of the Literature

Our literature review will focus on three distinct components - proposed methods for ZDT detection,
Autoencoders (AEs) for cyber anomaly detection, and the use of graph features and metrics in
accomplishing the same.

While cyber threat detection for known attacks such as Botnet infections [6] and other such attacks
has been extensively studied in literature, ZDTs have only recently gained traction in terms of
research. Zhao et al. [7] and Sameera et al. [8] utilized Transfer Learning to identify ZDTs, and
Abri et al. [9] and Kim et al. [10] also experimented with Deep Learning for ZDT detection utilizing
methods such as Generative Adversarial Networks. There has also been other work in detecting ZDTs
using methods such as, but not limited to, rule-based approaches as well as unsupervised clustering
methods using malware opcode sequences [11], [12] and Bayesian Classiﬁers for ZDT detection [13].
However, there has also been a predominant focus on identifying ZDTs using network ﬂow telemetry,
similar to our paper. Lobato, Lopez, et al. [14] utilize supervised machine learning approaches such
as Support Vector Machines (SVM) and Stochastic Gradient Descent (SGD) Algorithms to classify
network ﬂow telemetry as malicious or benign using an adaptive data modeling and pipeline approach
with promising results, achieving precision and recall values between 65.7% and 97.3%. Blaise
et al. [15] utilized network ﬂow telemetry to identify ZDTs using an unsupervised approach that
identiﬁes anomalous port usage. Sarhan et al. [16] also utilized network ﬂow telemetry to identify
ZDTs using a Zero-Shot Learning approach with Random Forest (RF) and Multi-layer Perceptron
(MLP) models using a novel data splitting approach to hold out attack classes as ZDTs to measure
performance. While utilizing supervised or statistical approaches to ZDT detection has been shown
to have some success, we believe that the lack of accurately labeled training datasets and the vast
differences between network topologies of various organizations could cause difﬁculties in either
successful model training or future generalization.

2

Autoencoders are powerful tools in anomaly detection, particularly in the cyber domain, due to
the fact that they can be trained solely on benign data and identify anomalous examples using a
threshold on an erroneous reconstruction loss. Hindy et al. [17], Youseﬁ-azar et al. [18], and An
and Cho [19] demonstrate the effectiveness of Network Intrusion Detection using Autoencoders
trained on network ﬂow telemetry in papers,and achieve strong precision and recall values compared
to traditional methods such as RF models. Zhang et al. [20] also utilize a modiﬁed Autoencoder
approach with semantic segmentation of attack types in natural language in to identify ZDTs with
strong accuracy values up to 88.3% as compared to other methods. However, the above studies are all
limited to single Autoencoder models performing anomaly detection after being trained on benign
data, and only utilize ﬂow-based features or semantic representations of said features.

We believe that graph-based features could help augment network ﬂow telemetry and allow anomaly
and ZDT detectors to generalize better across networks. By extracting expressive representations
such that graph anomalies and normal objects can be easily separated, or the deviating patterns of
anomalies can be learned directly through deep learning techniques, graph anomaly detection with
deep learning (GADL) is starting to take the lead in the forefront of anomaly detection [21]. Yu,
Cheng, et. al [22] had success utilizing deep learning to produce dynamic network embeddings
for anomaly detection. However, their approach required utilizing embeddings for entire network
topoligies over time to detect anomalies rather than our proposed approach for utilizing graph-based
features only for assets involved in a network ﬂow. Graph-based methods that do not utilize deep
learning have also been seen to yield strong results, e.g. Chen et al. [23] utilize Multi-Centrality Graph
Spectral Decomposition on Intrusion Detection (ID) data from the University of New Brunswick
along with k-means clustering to demonstrate that graph features can be utilized to identify network
intrusions. Furthermore, Sadreazami et al. [24], Eric Dull [25], and Noble and Cook [26] also utilized
graph based statistical techniques to identify cyber anomalies in network ﬂow information. We
wanted to extend the applicability of graph-based cyber anomaly detection by using asset-level graph
features in AEs along with network ﬂow features to enhance anomaly detection capabilities and
improve our models’ ability to generalize.

2.2 Shortcomings in Testing Methodology and Data in Literature

The majority of studies conducted previously rely on one or two primary datasets of CICIDS2017,
NSL-KDD, or KDD-99 for training and evaluation of models [14] [17], [18], [20]. Some studies
also utilize other open-source datasets such as the MAWI Wide Archive [15] or datasets such as
the UNSW-NB15 [16], and have aimed to create sophisticated machine learning methods for cyber
anomaly detection on a per-network basis. However, we believe that adopting the approach of having
to retrain our ZDT detector during deployment on new industry architecture will prove to be infeasible
due to the scale of the networks and the volume of security telemetry. Furthermore, retraining our
models on production data could also involve training on anomalous network ﬂows from Advanced
Persistent Threats (APTs) that could be learned by our models, and consequently not identiﬁed as
ZDTs. Our hypothesis is that in order to improve the ability of our models to generalize across
networks, we would need to encode information about not only the network ﬂows themselves, but
also about the assets in the network involved in those ﬂows. As a consequence, our team did not
utilize commonly available datasets such as NSL-KDD or CICDS2017 due to the absence of asset
identiﬁcation information (source and destination IP addresses). Instead, we utilized larger datasets
with asset identiﬁers such as the National Collegiate Cyber Defence Competition (NCCDC) 2020
and 2021 dataset and the MAWI Archive as detailed below.

3 Methodology

In this section, we will cover the key concepts related to the datasets we utilized for training and
evaluation, our high-level approach to ZDT detection, and our feature engineering.

3.1 Datasets

Our approach to dataset selection was based on three criteria. Namely, the datasets must include ﬂow
source and destination information such as IP addresses, they must include basic ﬂow-level features
such as ﬂow duration, destination port, timestamp, etc., and we must be able to collect a variety
of datasets that vary in size and time-frames. Consequently, we relied on four primary datasets for

3

Table 1: Datasets Used For Training and Evaluation

Dataset
MAWI
Archive
(MAWI)

Description
Real network PCAP data from hun-
dreds of devices across 12 universi-
ties in Japan.

Consists of labelled blue-team and
red-team data from real attack sim-
ulations on a cyber range.

Notes
Flow data for 7 consecutive days from 2021,
and 1 day from 2016 were used from a dataset
of over 14 years. Dataset is not labelled but is
considered benign due to the extreme class im-
balance of anomalies in real-world networks.
Data spanned two consecutive days in 2020
and in 2021. Attack types consist of Network
Scanning, Interrogation, Botnet, and Com-
mand and Control.

Data consists of real Deloitte inter-
nal network trafﬁc from 12 Deloitte
US and US-India locations.

Data consists of network data of
over 30 million ﬂows over 1 year
from hundreds of real malware sam-
ple detonations in Deloitte’s inter-
nal malware cyber-range.

Flow data for 7 consecutive days was used.
Dataset is not labelled but is considered be-
nign due to the extreme class imbalance of
anomalies in real-world networks.
Dataset is labelled with malware class name,
and was correlated with threat intelligence to
extract higher level attack-type labels includ-
ing Botnets, Ransomware, Infostealer, etc.
Data did not contain non-malicious ﬂows.

Ntnl. Col-
legiate
Cyber
Defence
(NC-
CDC)
Deloitte
Internal
Flow
(DIF)
Deloitte
Codex
Malware
Lab
(Codex)

training and evaluation, listed in 1. As seen in 1, we are also introducing a novel dataset (NCCDC)
for the purposes of model development, benchmarking, and further research. NCCDC was generated
using packet captures from the United States National Collegiate Cyber Defence Competition [27].
The dataset was captured during red-team and blue-team activities where the red-teams conducted one
of four attacks: Scanning, Interrogation, Botnet, or Command and Control. The dataset we provide
consists of network ﬂow features labelled as one of the four attack types mentioned previously or a
benign ﬂow.

3.2 Feature Engineering

Our ZDT detector relies only on network ﬂow data to identify potentially malicious activity on
organizational networks. Our base dataset was speciﬁcally chosen to maximize compatibility with
most security logging and monitoring (L&M) infrastructures and requires only connection level
information such as Connection Source and Destination IP addresses, connection duration and
timestamp, port, protocol, and the total size of information exchanged between source and destination.
We also utilize all of the network ﬂow data to construct a network interaction graph of the network
which we then use to derive graph-features for the various ﬂows. The network graph is constructed
by assigning a node to every unique IP address present in the dataset and an edge with weight 1 for
every unique ﬂow between two nodes. If there exist multiple ﬂows, the edge weight is incremented
by 1 for every ﬂow present. Once the graph is constructed, we extract the graph features for the
source and destination IP address involved in a ﬂow, for every ﬂow including Degree Centrality and
Pagerank [28], Clustering Coefﬁcient [29] and Betweenness Centrality [30], In-degree, Out-degree, In-
weight, Out-weight, Hub and Authority values using the HITS algorithm [31], and Cross-community
ﬂow detection using Label Propagation [32]. Once the graph features are calculated, each ﬂow is
represented by 25 unique features (before preprocessing steps) per ﬂow, which is inclusive of the
graph features and ﬂow level features. It is important to note that the source and destination IP
addresses are not used as features to feed our model, and are only used to construct our network
graph.

3.3 ZDT Model Architecture

Our ZDT detection architecture is a workﬂow that consists of two independently trained AEs that
perform the following functions:

4

Figure 1: High-Level Architecture

1. Anomaly Detector (AD): An AE trained solely on benign network trafﬁc, responsible for
identifying anomalous and potentially malicious ﬂows, indicated by a high reconstruction
loss.

2. Novelty Detector: An AE trained only on ’known’ or previously seen attack types, re-
sponsible for identifying novel or zero-day attacks, indicated by a high reconstruction
loss.

During inference, network ﬂows are normalized and passed through the AD to identify if the network
ﬂow is suspicious. If the reconstruction losses are high, the original ﬂows are then re-normalized
using the normalization parameters for the Novelty Detector and passed through it to identify if the
network anomaly is novel or not. Novel threats are then sent to security operators for review and
slated to be used for retraining our models based on operator review. The workﬂow is also depicted
in 1.

4 Experimental Design

4.1 Performance Metrics and Baseline Modeling

Due to the highly imbalanced nature of real-world cybcersecurity datasets and their tendency to be
heavily skewed towards benign events, we utilized Precision, Recall and the area under a Receiver
Operating Characteristics (ROC) curve (AUC) to evaluate our modes. Furthermore, we also worked
with the Deloitte Detect and Respond cyber unit and threat hunt teams to establish broad benchmarks
we believe our models should accomplish to be useful in a practical setting. Our models should
be able to make predictions in near-real-time with a high precision (above 0.9), a recall as high as
possible (above 0.8) and maximize the AUC. Based on our PMs, our team, in collaboration with other
data scientists at Deloitte’s A.I. Center of Excellence created the following model implementation
and evaluation framework. To establish baseline peformance, we ﬁrst trained a Random Forest
Multi-Class Classiﬁer (RF) [33] and a a Multi-Class Support Vector Classiﬁer [34] to classify network
ﬂow data as benign, or as a distinct attack type. Although they cannot be used to detect novel threats in
this manner, we wanted to be able to estimate problem difﬁculty, which showed low performance with
both the RF and SVM struggling to classify labels with low support (0.7 and 0.17 AUC respectively).
Next, we trained a single Dense AE on benign network trafﬁc to observe whether it would be able
to identify anomalous network trafﬁc on the basis of poor reconstruction loss. This method has had
wide success in literature and we wanted to ensure we could recreate similar results with our datasets.
Similarly, we then trained a single Dense AE on benign network trafﬁc as well as network trafﬁc of
three distinct attack types. We then computed reconstruction loss on a fourth independent attack type
to observe if this fourth attack would be ﬂagged as anomalous by the AE. The goal would be to test if
the model would be able to identify ZDTs effectively, which in this case would be the distinct fourth
attack type.

5

4.2 Anomaly Detection

The goal of the model is to separate benign and malicious trafﬁc. We trained the AD on benign
network trafﬁc, with only 84 network ﬂow features extracted from packet capture ﬁles using the
CICFlowMeter tool [35], from a single network and assess reconstruction loss on malicious trafﬁc
from that network, as well as benign trafﬁc from two other networks. This would test if the model can
generalize across multiple networks but still identify anomalous trafﬁc. Next, we trained the AD on
benign trafﬁc from two networks, with only network ﬂow features, and validate on malicious trafﬁc
from network, and benign trafﬁc from the third to see if there is a boost in performance. We repeat
this experiment while training with benign trafﬁc with three networks as well.Finally we performed
the above two sets of experiments with a Dense AE, but this time, we included not only network ﬂow
features but also asset-based graph features discussed in III B during training and validation. Based
on these experiments, we could then assess if graph features provide the necessary asset level context
to allow our models to generalize between networks.

4.3 Novelty Detection

The goal of the Novelty Detector would be to analyze anomalous or malicious trafﬁc and identify
if the anomaly is of a class that has been previously seen or if it is ’novel’, i.e. a zero-day attack.
Consequently, the Novelty Detector is trained solely on malicious network trafﬁc and evaluated based
on how it reconstructs examples of attack types similar to ones it has been trained on and ’holdout’
attack types that are fundamentally different from any examples seen during training time. Holdout
attack types are curated to be tactically and technically different from attack types shown to the
model during training to ensure that they are indeed representative of ZDTs rather than different
variations of a parent attack type. Furthermore, we also set aside examples of different attacks that are
part of the same attack family to ones that are seen during training, e.g. different Botnet infections,
to determine that our Novelty Detector does not incorrectly classify different kinds of procedures
under the same tactic or technique as novel attacks. Experiments were also conducted to determine
that novelty detection could be generalized across networks by determining that an attack type in a
previously unseen network is not ﬂagged as novel if the model has seen a similar example from a
network during training. We also conducted experiments to determine the converse applies, where
similar novel attack types are ﬂagged as novel regardless of what network they originated in.

4.4 End-to-End Pipeline Performance

Once the AD and Novelty Detector were benchmarked and tested individually, we then passed a
mixture of benign and anomalous examples through an end-to-end pipeline containing both models
sequentially in order to measure performance. The data were ﬁrst passed through the AD, and
malicious events ﬂagged by that model were then passed through the Novelty Detector to identify if
they were ZDTs. Overall precision, recall, and AUC values were then computed.

5

Implementation and Results

5.1 Autoencoder Trained on Benign Data

Training and validation of these models was also performed using data from the NCCDC and MAWI
datasets. The NCCDC dataset was ﬁrst split into benign and malicious data based on the label
of the example. The benign data was then split into training (NCC-train) and testing (NCC-test)
datasets with the training dataset containing 70% of randomly sampled benign examples. NCC-train
was normalized using Min-Max normalizaiton, and NCC-test was normalized using NCC-trains
normalization parameters. The malicious data from the NCCDC dataset (NCC-holdout) was also
normalized using NCC-train’s normalization parameters.

The AE was then trained on NCC-train and performance was evaluated on NCC-test and NCC-
holdout. A strong model will produce lower reconstruction loss for Benign examples than for
Malicious examples. Our experiments with NCCDC supported this hypothesis, with our trained
model able to distinguish between Benign and Malicious events without error (2a) at a reconstruction
threshold of 0.022. This yielded a perfect AUC of 1 which provided a signiﬁcant boost in performance
over the RF and SVM.

6

Figure 2: Baseline Loss Histograms

(a) AE Reconstruction Loss - Trained on Benign

(b) AE ZDT - Loss Histogram

5.2 AE Trained on Benign and Malicious Data

Similar to the previous experiment, NCC-train and NCC-test sets were created using Benign, Scanning,
Interrogation, and Command and Control examples from NCCDC. Exﬁltration examples were used
as the NCC-holdout dataset. Our hypothesis was that, if the AE is a strong ZDT detector, it would have
produced higher reconstruction losses on NCC-holdout than on NCC-test given that NCC-holdout
contained a novel, fundamentally different, attack type.

However, results from the experiment indicated that a single AE architecture could not effectively
distinguish between novel and previously seen attack types with an AUC of 0.72 and signiﬁcant
overlap in the loss histogram2b. These results supported our hypothesis that AEs, while strong
anomaly detectors, would not be effective ZDT detectors in practice, especially as the number of
known attack types starts to rise. These experimentation results formed the basis for our decision to
move to a two AE structure: the AD and the Novelty Detector.

5.3 The AD

Training of the AD was performed using only benign data from three datasets from distinct and
independent networks: NCCDC, MAWI 2021, and MAWI 2015. MAWI 2015 and MAWI 2021 are
assumed to be independent as both datasets have different characteristics in network ﬂow values. In
each of the three experiments outlined below, each dataset was normalized independently of other
datasets using Min-Max normalization. Furthermore, malicious data from the NCCDC dataset were
treated as holdouts during training and testing, and were normalized separately using normalization
parameters acquired from normalizing NCCDC benign data. It is also worth noting that, unless
otherwise mentioned, training and test splits for training data are randomly sampled with a 70%/30%
split respectively.

The ﬁrst experiment tested the AD’s ability to generalize to multiple networks using only network
ﬂow features, and consisted of three runs. In the ﬁrst run, the model was trained on benign data
from the NCCDC dataset and tested on NCCDC and the two MAWI datasets. Next, the training data
from NCCDC was supplemented with benign trafﬁc from MAWI 2015, and the model was evaluated
against all three networks. Finally, the model was trained and evaluated using data from all three
networks. The models trained on data from one, two, and three networks generated AUC scores of
0.72, 0.77, and 0.79, respectively, indicating stronger generalization with additional training data
from distinct networks (3a).

The second experiment followed a similar setup, but with the addition of asset-level graph features
and only utilizing 6 foundational network ﬂow features identiﬁed in III B instead of 84 features.
Subsetting the network ﬂow features while seeing if we can still maintain performance will allow our
models to be more ﬂexible with deployment without the need for specialized sensors on networks.
In the ﬁrst run, the network was trained on NCCDC and evaluated on NCCDC, MAWI, and DIF.
Next, the training data was supplemented with MAWI and the model was again evaluated on all
three networks. Finally, the model was trained and evaluated on all three networks. In all three runs
the models demonstrated AUC greater than 0.99, with small but consistent performance increases
from training with additional networks. The ROC curves are not shown because they are nearly

7

Figure 3: AD Performance

(b) AUC Scores for AD Trained on Different Numbers
of Networks

No. of Networks
1
2
3

Flow Only
0.72
0.77
0.79

Flow and Graph
0.993
0.999
0.999

(a) AD ROC for Different Numbers of Train-
ing Networks

identical, but a summary of the results of the AD experiments can be found in 3b. This highlights that
our AE approach combined with asset-level graph features demonstrated near perfect performance
in generalize to new networks as an anomaly detector; signiﬁcantly higher than an AE with only
network ﬂow features.

5.4 The Novelty Detector

The novelty detector was trained on the NCCDC and Codex datasets using 6 ﬂow-level features along
with asset-level graph features. In each training run, one attack class was held out and treated as
a ZDT at evaluation, and the loss threshold was selected to maximize precision while maintaining
some sensitivity to positive examples (recall generally greater than 0.50). The models were able to
discriminate between novel and known attacks, though the performance varied signiﬁcantly with
the choice of holdout (4a). In the test datasets, holdout attacks represented less than 2% of all
events, mimicking the rarity of ZDTs. This class imbalance accounts for the discrepancy between the
measured AUC and precision-recall.

Figure 4: Novelty Detector and End-to-End Performance

(a) Performance of Novelty Detector

(b) End to End Performance of Two-Model Approach

Attack
rat
infostealer
command/control
ransomware
botnet
interrogation
worm
downloader
scanning
Average

AUC Precision Recall
0.45
0.69
0.84
0.29
0.56
0.94
0.83
0.97
0.91
0.48
0.80
0.95
0.89
1.00
0.95
0.92
0.91
0.89
0.53
0.83
0.97
0.54
0.71
0.86
0.91
0.93
0.84
0.65
0.82
0.91

Attack
rat
infostealer
command/control
ransomware
botnet
interrogation
worm
downloader
scanning
Average

AUC Precision Recall
0.45
0.71
0.84
0.28
0.60
0.94
0.83
0.98
0.91
0.40
0.78
0.87
0.89
1.0
0.96
0.92
0.92
0.90
0.50
0.83
0.97
0.53
0.74
0.86
0.89
0.93
0.84
0.63
0.83
0.90

5.5 End to End Testing

To evaluate the overall performance of the two-model approach, a separate test was conducted for
each attack class, treating it as a novel threat. Each test used the same strongest AD, and a subset
of the data was used to tune the loss threshold for an event to be labelled anomalous; this subset of
data was not used in the ﬁnal evaluation of the two-model detector’s performance. Events labelled
as anomalous were then passed to the appropriate novelty detector given the choice of held-out
attack, and the overall precision, recall, and AUC were computed taking into account the events
rejected by the AD (4b). The results closely track the results from the novelty detector due to the
near-perfect performance of the AD. Averaged evenly over all attack classes, two-model approach

8

demonstrated 83% precision and 63% recall. However, it is worth noting here that almost none of the
wrongly classiﬁed events were benign events due to the strong performance of our AD, but rather
were anomalous events that were mistakenly identiﬁed as novel. Furthermore, 5 shows the signiﬁcant
boost in performance our approach provides compared to traditional approaches discussed previously.

6 Feature Selection and Hyperparameter Tuning

We found that a larger feature set containing 6
ﬂow-level features and all graph features was
required to obtain strong performance with the
AD, whereas only a subset of those features were
required to achieve similar performance with
the Novelty Detector. Feature selection for each
model was conducted using a grid search over all
possible feature combinations and performance
was judged by the AUC of a holdout attack type.
Furthermore, we found that normalization was
essential to allowing our models to generalize
between different networks.

Figure 5: Average results for the single autoen-
coder, dual autoencoder, and dual autoencoder with
graph features, trained on MAWI and NCCDC and
tested on NCCDC. Note: The proprietary Codex
and DIF datasets are excluded.

Single
Dual
Dual with Graph

AUC Precision Recall
0.64
0.63
0.69
0.76
0.84
0.83
0.93
0.96
0.93

We also conducted several experiments to deter-
mine a high performing structure for each of our AE involving modiﬁcations to the depth of the
Autoencoder, the width of its latent space layer, activation functions for the AE layers, dropout, and
batch normalization layers. Ultimately, both our AEs utilized a symmetric structure where successive
layer widths increased or decreased by a fraction of 1.4 and the latent space layer contained 6
perceptrons. Our experiments also found that the addition of dropout or batch-normalization layers
between fully-connected layers led to a signiﬁcant drop in performance and the Rectiﬁed Linear Unit
(ReLU) activation function yielded the strongest performance.

7 Conclusion and Future Work

The novel Dual AE Approach, augmented with asset-level graph features, to ZDT detection proposed
in this paper offers a viable approach to identifying unknown threats effectively. Compared with
single-AE based approaches or modeling approaches based on just network ﬂow features, our models
demonstrate a superior ability to generalize detection to unseen network topologies without retraining.
This provides the distinct beneﬁts of being extremely cost effective to deploy and drastically reducing
time to value for security operators using our models. The strong performance of our models and
low false positive rates offer operators reliable detections and reduced alert fatigue. Furthermore,
the features used in our models, such as ﬂow duration, source and destination features, have been
speciﬁcally chosen for maximum compatibility with existing security monitoring infrastructure and
should be easily available on client networks without the overhead of installing specialized sensors.
This ﬂexibility allows for greater versatility of our models. The AEs and pipeline have also been tested
with large-scale datasets from real organizational networks in addition to attack-range datasets to
determine that our model can produce reliable detections based on security telemetry from networks
that have thousands of hosts. These factors combined make our architectures and methodology
outperform similar approached to ZDT detection while maintaining feasibility to deploy and operate.

The ZDT detection pipeline that has been proposed has strong performance in differentiating benign
from anomalous data. However, the performance of the model drops as the number of attack types
increases (as seen by comparing the results of ﬁgures 4 and 5, with and without the Codex data),
so we will continue to improve the performance of the Novelty Detector independently in order to
improve the effectiveness of detections overall. Possible improvements could include changes to the
model architectures or training methodology. We will also continue to train and test our models on
datasets and networks with a large diversity in topologies, behaviours, and most importantly, attack
types. We will also focus on creating architectures for continuous improvement of our models based
on real-time operator feedback to improve the value our models bring to real-world detection and
response for Security Operations Centers.

9

References

[1] N. Kaloudi and J. Li, “The ai-based cyber threat landscape: A survey,” ACM Computing Surveys

(CSUR), vol. 53, pp. 1–34, 02 2020.

[2] A. Khraisat, I. Gondal, P. Vamplew, and J. Kamruzzaman, “Survey of intrusion detection

systems: techniques, datasets and challenges,” Cybersecurity, vol. 2, 12 2019.

[3] “How much does a data breach cost?” https://www.ibm.com/security/data-breach, accessed:

2022-02-28.

[4] E. W. Powers, D. J. Fincher, and J. Silber. [Online]. Available: https://www2.deloitte.com/

content/dam/Deloitte/us/Documents/risk/us-risk-beneath-the-surface-infographic.pdf

[5] “Internet security threat report, i. symante, mountain view, ca, usa, 2014.” accessed: 2022-02-28.

[6] Y. Xing, H. Shu, H. Zhao, D. Li, and L. Guo, “Survey on botnet detection techniques: Classiﬁ-
cation, methods, and evaluation,” Mathematical Problems in Engineering, vol. 2021, pp. 1–24,
04 2021.

[7] J. Zhao, S. Shetty, J. Pan, C. Kamhoua, and K. Kwiat, “Transfer learning for detecting unknown

network attacks,” EURASIP Journal on Information Security, vol. 2019, 02 2019.

[8] N. Sameera and M. Shashi, “Deep transductive transfer learning framework for zero-day attack

detection,” ICT Express, vol. 6, 03 2020.

[9] J. Kim, S. Bu, and S. Cho, “Zero-day malware detection using transferred generative adversarial
networks based on deep autoencoders,” Information Sciences, vol. 460-461, pp. 83–102, Sep.
2018.

[10] F. Abri, S. Siami-Namini, M. A. Khanghah, F. M. Soltani, and A. S. Namin, “The performance
of machine and deep learning classiﬁers in detecting zero-day vulnerabilities,” Nov 2019.
[Online]. Available: https://arxiv.org/abs/1911.09586

[11] K. Sornalakshmi, “Detection of dos attack and zero day threat with siem,” in 2017 International

Conference on Intelligent Computing and Control Systems (ICICCS), 2017, pp. 1–7.

[12] M. Zolotukhin and T. Hämäläinen, “Detection of zero-day malware based on the analysis of
opcode sequences,” in 2014 IEEE 11th Consumer Communications and Networking Conference
(CCNC), 2014, pp. 386–391.

[13] X. Sun, J. Dai, P. Liu, A. Singhal, and J. Yen, “Using bayesian networks for probabilistic
identiﬁcation of zero-day attack paths,” IEEE Transactions on Information Forensics and
Security, vol. 13, no. 10, pp. 2506–2521, 2018.

[14] A. G. P. Lobato, M. A. Lopez, I. J. Sanz, A. A. Cardenas, O. C. M. B. Duarte, and G. Pujolle,
“An adaptive real-time architecture for zero-day threat detection,” in 2018 IEEE International
Conference on Communications (ICC), 2018, pp. 1–6.

[15] A. Blaise, M. Bouet, V. Conan, and S. Secci, “Detection of zero-day attacks: An
unsupervised port-based approach,” Jul 2020. [Online]. Available: https://hal.archives-ouvertes.
fr/hal-02889708

[16] M. Sarhan, S. Layeghy, M. Gallagher, and M. Portmann, “From zero-shot machine learning to

zero-day attack detection,” 09 2021.

[17] H. Hindy, R. Atkinson, C. Tachtatzis, J.-N. Colin, E. Bayne, and X. Bellekens, “Utilising deep
learning techniques for effective zero-day attack detection,” Electronics, vol. 9, p. 1684, 10
2020.

[18] M. Youseﬁ-Azar, V. Varadharajan, L. Hamey, and U. Tupakula, “Autoencoder-based feature
learning for cyber security applications,” in 2017 International Joint Conference on Neural
Networks (IJCNN), 2017, pp. 3854–3861.

[19] J. An and S. Cho, “Variational autoencoder based anomaly detection using reconstruction

probability,” 2015.

[20] Z. Zhang, Q. Liu, S. Qiu, S. Zhou, and C. Zhang, “Unknown attack detection based on zero-shot

learning,” IEEE Access, vol. 8, pp. 193 981–193 991, 2020.

[21] X. Ma, J. Wu, S. Xue, J. Yang, Q. Z. Sheng, and H. Xiong, “A comprehensive survey on graph

anomaly detection with deep learning,” ArXiv, vol. abs/2106.07178, 2021.

10

[22] W. Yu, W. Cheng, C. C. Aggarwal, K. Zhang, H. Chen, and W. Wang, “Netwalk: A ﬂexible
deep embedding approach for anomaly detection in dynamic networks,” in Proceedings of the
24th ACM SIGKDD international conference on knowledge discovery & data mining, 2018, pp.
2672–2681.

[23] P.-Y. Chen, S. Choudhury, and A. O. Hero, “Multi-centrality graph spectral decompositions
and their application to cyber intrusion detection,” in 2016 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 4553–4557.

[24] H. Sadreazami, A. Mohammadi, A. Asif, and K. N. Plataniotis, “Distributed-graph-based
statistical approach for intrusion detection in cyber-physical systems,” IEEE Transactions on
Signal and Information Processing over Networks, vol. 4, no. 1, pp. 137–147, 2018.

[25] E. Dull, “Cyberthreat analytics using graph analysis,” 05 2020.
[26] C. Noble and D. Cook, “Graph-based anomaly detection,” 01 2003, pp. 631–636.
[27] D. Williams. [Online]. Available: https://www.nationalccdc.org/
[28] W. Xing and A. Ghorbani, “Weighted pagerank algorithm,” in Proceedings. Second Annual
IEEE, 2004, pp.

Conference on Communication Networks and Services Research, 2004.
305–314.

[29] D. J. Watts and S. H. Strogatz, “Collective dynamics of ‘small-world’ networks,” Nature, vol.

393, no. 6684, pp. 440–442, 1998.

[30] L. Freeman, “A set of measures of centrality based on betweenness,” Sociometry, vol. 40, pp.

35–41, 03 1977.

[31] J. M. Kleinberg, “Authoritative sources in a hyperlinked environment,” JOURNAL OF THE

ACM, vol. 46, no. 5, pp. 604–632, 1999.

[32] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data with label propagation,”

2002.

[33] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp. 5–32, 2001.
[34] B. E. Boser, I. M. Guyon, and V. N. Vapnik, “A training algorithm for optimal margin classiﬁers,”
in Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory. ACM
Press, 1992, pp. 144–152.

[35] A. Habibi Lashkari, “Cicﬂowmeter-v4.0 (formerly known as iscxﬂowmeter) is a network trafﬁc
bi-ﬂow generator and analyser for anomaly detection. https://github.com/iscx/cicﬂowmeter,” 08
2018.

11

