Computers & Graphics (2021)

Contents lists available at ScienceDirect

Computers & Graphics

journal homepage: www.elsevier.com/locate/cag

Data-driven insight into the puzzle-based cybersecurity training

Karol´ına Doˇckalov´a Bursk´aa,∗, V´ıt Rusˇn´akb, Radek Oˇslejˇseka

aFaculty of Informatics, Masaryk University, Brno, Czech republic
bInstitute of Computer Science, Masaryk University, Brno, Czech republic

1
2
0
2

p
e
S
8
2

]

C
H
.
s
c
[

1
v
8
7
7
3
1
.
9
0
1
2
:
v
i
X
r
a

A R T I C L E I N F O

Article history:
Received 30 July 2021
Accepted 27 September 2021

Keywords: Visual analytics, learning an-
alytics, cybersecurity education, hand-
s-on training, design study

A B S T R A C T

Puzzle-based training is a common type of hands-on activity accompanying formal
and informal cybersecurity education, much like programming or other IT skills. How-
ever, there is a lack of tools to help the educators with the post-training data analysis.

Through a visualization design study, we designed the Training Analysis Tool that
supports learning analysis of a single hands-on session. It allows an in-depth trainee
comparison and enables the identiﬁcation of ﬂaws in puzzle assignments. We also
performed a qualitative evaluation with cybersecurity experts and students. The par-
ticipants apprised the positive inﬂuence of the tool on their workﬂows. Our insights
and recommendations could aid the design of future tools supporting educators, even
beyond cyber security.

© 2021 Elsevier B.V. All rights reserved.

1. Introduction

Higher-order thinking has become one of the essential skills
for the 21st century. The best way to develop and strengthen
these abilities is through practical hands-on courses [1, 2]. One
commonly used learning method for training problem-solving
or various IT skills (e.g., programming) is puzzle-based learn-
ing. Michalewicz et al. [3] introduced a game-based learning
method that uses puzzles as a metaphor for getting students
to think about how to frame and solve unstructured problems.
In IT education, the puzzle-based learning approach has been
prevalent for many years [4, 5, 6]. Even programming courses
consist of basic concepts such as recursion with assignments
like “Write a program to calculate the factorial of a given num-
ber.”

Multiple studies conﬁrmed the usefulness of puzzle-based
learning also for cybersecurity education [7, 8, 9]. However,
while hands-on training produces a tangible output in many

learning areas, e.g., a code that can be checked, analyzed, and
evaluated, cybersecurity training is process-oriented. Puzzles
are tasks like “search for a vulnerability on server X” that are
diﬃcult to track. Tutors have only a limited view of what
trainees are doing in the computer network and how they deal
with the task, making the post-training evaluation challenging.
This paper presents results of cooperation with cybersecurity
education experts that led to the design of a visualization tool
supporting the follow-up learning analysis of the training ses-
sions.

Regardless of the education subject, tutors make intensive
eﬀorts to create, organize, and continually improve these so-
called blended courses1. Trainees’ assessment, which usually
follows the training session, is integral to the teaching process.
The focus lies on comparing individual trainees and analyzing
their progress or discovering weaknesses in the training design.
We contribute to the state of the art of applying visualizations
in education practice with: (a) a user requirement deﬁnition on
support tools for tutors of the hands-on puzzle-based learning

∗Corresponding author.
E-mail addresses: burska@mail.muni.cz (Karol´ına Doˇckalov´a
Bursk´a), rusnak@ics.muni.cz (V´ıt Rusˇn´ak), oslejsek@fi.muni.cz
(Radek Oˇslejˇsek)

1Blended courses combine computer-supported learning activities with tra-

ditional face-to-face interaction during training sessions.

 
 
 
 
 
 
2

Accepted manuscript / Computers & Graphics (2021)

activities (in the cybersecurity education context); (b) design
and implementation of the visualization tool for the post hoc
analysis of data from the training session; and (c) an evaluation
with domain experts resulting in design recommendations for
future work.

2. Related work

Assessing the eﬀectiveness of game-based learning poses a
signiﬁcant challenge in the learning analytics research domain.
Loh [10] distinguishes between ”assessment for learning” and
”assessment of learning.” The former is designed to assess a
learner’s understanding at the course end. The latter is more
helpful to educators because it helps them to improve the learn-
ing processes. This paper deals with educators’ insight into the
learning process. A considerable eﬀort has been made in the
past to conceptualize data mining and digital assessment for se-
rious games so that generic learning analytics principles can
be researched and applied regardless of the speciﬁc game con-
tent [11, 12, 13]. Our solution deals with event logs and the
score-based assessment that represent broadly accepted types
of telemetry and evaluation data for serious games.

Our work lies at the intersection of education, visualization,
and HCI research. According to the classiﬁcation provided
in [14], this paper addresses visual data analysis tasks of or-
ganizing participants (referred to as tutors). Using information
technologies in blended courses enables us to collect metadata
produced by learners. Tutors can use them for a post hoc analy-
sis of learners’ progression and content revision. Nevertheless,
the design and deployment of eﬃcient support tools remain a
challenging problem [15]. There are general tools that could
be used for speciﬁc post-training tasks, e.g., comparing score-
based assessment settings via the LineUp application [16]. Our
tool aims to reﬂect the well-deﬁned requirements of training de-
signers and tutors, providing them with a domain-speciﬁc com-
prehensible analytical dashboard.

The purpose of the post-training learning analysis is to under-
stand and optimize learning processes. Previous works [17, 18,
19, 20, 21] address using visual dashboards for learning analy-
sis and conﬁrm the need for insight exceeding simple summa-
tive feedback [22]. Apart from focusing on the learning pro-
cess, learning analytics in higher education also provide valu-
able teaching or research resources [23]. Analytical tools can
support decision-making and improve pedagogical approaches.
Most of these learning analysis tools focus on the high-level
perspective evaluation of students’ performance. Existing sur-
veys overview and analyze learning dashboards either for tu-
tors [24, 25, 26] or students [27]. Most of them are related
to the uptake of massive online open courses. These tools focus
on visualizing learning activity, tracking speciﬁc learning goals,
and providing a high-level perspective on learners’ progress.
Moodleboard [28] is a decision support tool for pedagogical
engineers and administrators providing both course statistics
and detection of ﬂaws or misuses for an open-source learning
management system Moodle. LISSA [29] aims at improving
student-advisor dialogue during face-to-face consultations. The
tool provides an overview of study progress or peer compar-
ison among multiple students. SAM [30] is a general-purpose

web-based environment visualizing learners’ activities, improv-
ing awareness, and supporting self-reﬂection. Such high-level
tools represent domain-independent systems to gather, process,
and report the collected and derived data while overlooking dis-
ciplinary knowledge practices.

In contrast, tools for lower-level data analysis from practical
courses often require considering insight from domain experts
because the input data driving the analytical tools are domain-
speciﬁc. Examples can be found for math [31], where the sys-
tem tackles the understanding of selected math functions, pro-
gramming tools [32] that utilize compilation processes and soft-
ware quality metrics for assessment, or penetration testing [33]
based on knowledge graphs. Figure 1 categorizes these tools in
two axes: x-axis – single or multiple training sessions; y-axis
– data speciﬁcity, i.e., from the domain-speciﬁc data to derived
data and metadata.

Fig. 1. Categorization of learning analytics tools based on their focus (on
single or multiple sessions) and the input data types (from domain-speciﬁc
to derived meta-data). TAT position is highlighted.

We propose the Training Analysis Tool (TAT) – a dashboard-
like tool for tutors providing data-driven insight into a training
session through several linked visualizations. The TAT supports
tutors in low-level learning analytics tasks such as inspection
and comparison of trainees or identifying training design ﬂaws
based on the data from single training sessions.

3. Background

The puzzle-based learning in the cybersecurity domain is pri-
marily represented by Capture the Flag (CTF) games [34, 35,
36]. CTF training scenarios serve as puzzle-based templates
structuring the content into levels focused on solving cyberse-
curity tasks, e.g., scan the network, identify a server, ﬁnd the
server vulnerability, exploit it, and gain the root privileges. CTF
games can be organized in diverse ways. Very popular are un-
supervised online games when a trainee can access the game or
interrupt it anytime. Tutored (or supervised) training sessions
for small groups are often practiced in a formal cybersecurity
education or professional training. The supervised training ses-
sions share the principles of blended courses popular in primary
and secondary education.

Accepted manuscript / Computers & Graphics (2021)

3

CTF games contain a short background story, task assign-
ments, their evaluation, hints, and solutions for each level. A
typical scenario consists of up to ten levels. Finding a level
solution is necessary to proceed to the next one. Training sce-
narios use multiple gamiﬁcation characteristics such as scoring,
level-based approach, or scoreboards. Trainees are penalized
when taking hints or solutions and reach score points for suc-
cessful solutions.

Hands-on cybersecurity training is often organized in so-
called cyber ranges. The KYPO Cyber Range Platform2 (here-
after referred to as KYPO CRP)that we use for development
and evaluation is a cloud-based environment providing features
for the virtualization of computer systems and networks [37].
It serves as a platform for practical training of various cyber-
security skills in university courses as well as for the training
of practitioners from institutions outside. The KYPO Cyber
Range allows us to create so-called sandboxes – isolated com-
puter networks consisting of multiple virtual machines for sev-
eral dozens of trainees (the exact number depends on the cloud
capacity and resource requirements). The web portal provides a
user interface for the management of sandboxes, users, training
scenarios, and organizing training sessions.

A typical training session is organized for 15–20 participants
in the IT classroom. Trainees log in to the web portal and launch
a training scenario consisting of a sequence of cybersecurity
puzzles. Trainees solve the puzzles individually in their private
sandboxes without aﬀecting others’ work. A successful solu-
tion of the puzzle yields a short string (called ﬂag). Entering
the ﬂag in the web portal opens the next level. Trainees who
are struggling can use hints speciﬁc for each level. When help-
less, they can see the correct solution (a list of steps leading to
the ﬂag). Time for solving all the levels is usually limited to
the class length (one or two hours). Tutors walk around and
help trainees either on request or when they realize that some-
one signiﬁcantly lacks behind (typically by quick peek on their
displays or asking them directly). In the end, the scoreboard
shows individual scores, and tutors hold a short debrieﬁng to
present correct solutions.

Figure 2 illustrates the principal elements and actions of the

whole workﬂow.

There are two broad use cases for the post-training analysis:
(a) a comparison of trainees and (b) training scenario improve-
ments. The former is essential when the CTF games are part of
the competitions or exams. The rank or grade is then based on
the ﬁnal score and time. However, the tutor cannot understand
the subtle diﬀerence in the trainee’s behavior or expose cheat-
ing. Likewise, training scenario improvements were usually
based on error-prone manual processing of the logged data and
anecdotal evidence from training sessions, making revisions in-
eﬃcient.

3.1. Data description

Hands-on CTF games provide two datasets available for vi-
sual analysis: a training scenario and timestamped trainees’

2https://kypo.cz

Fig. 2. The generalized training workﬂow. The tutor uses the visual inter-
face to get insight into the training session (to help trainees in trouble) and
to revise and improve the training scenario. The data sources are activ-
ity logs of the trainees and training scenario description which provides
context.

events recorded during the training session. The KYPO CRP
provides REST API to access these data on-demand in JSON
format.

The training scenario contains attributes related to the con-
tent. Namely, a background story, puzzle assignments, hints,
hint penalties, solutions, solution penalties, correct ﬂags, ﬂag
score points, and level time limits. These attributes do not
change during the training session. However, tutors might edit
them afterward based on trainees’ feedback or outcomes from
training session analysis. Typical changes include ﬁxing typos
and improving the clarity of puzzle assignments, or adjusting
level duration estimate, score, and penalty points.

The trainees’ events are automatically collected when
trainees interact with the web portal. Example events are: train-
ing started, training ended, level started, level ended, correct
ﬂag entered, incorrect ﬂag entered, hint taken, solution taken.
Each event contains a standard set of attributes (timestamp,
event type, training description ID, training session ID, user
ID). Three event types (an incorrect ﬂag entered, a hint used,
a solution displayed) contain speciﬁc attributes – an incorrect
ﬂag string and penalty points.

Although the input data is domain-speciﬁc, we can ﬁnd sim-
ilarities also in other forms of puzzle-based gaming. Data types
are either integers (score and penalty points, level duration esti-
mate – representing minutes) or text strings (plain-text for ﬂags,
markdown markup for all the rest).

4. Process and methods

We closely collaborate with domain experts (cybersecurity
educators) from our university who represent target users. They
provided initial requirements, gave us feedback on proposed de-
signs, and participated in both evaluations. Our goal was to im-
prove the workﬂow of tutors and organizers of hands-on cyber-
security training sessions through the design and deployment of
the Training Analysis Tool that processes data from the KYPO
Cyber Range.

In this project, we applied the user-centered approach guided
by the design study methodology framework [38], reﬂecting its
core stages: discover, design, implement, deploy. Our iterative

LogsTraineesVisual InterfaceTutorTraining  ScenarioFeedProduceFeedbackInsightDesign/RevisionFeed4

Accepted manuscript / Computers & Graphics (2021)

process has four phases. Each phase reﬂects one or more of
these stages:

Problem characterization (discover): We conducted semi-
structured interviews with three domain experts from the uni-
versity cybersecurity team. All of them partake in educational
activities as seminar tutors or lecturers, and they also partici-
pated later on in the evaluation. Each interview lasted about an
hour. We also did four ﬁeld observations during training ses-
sions to gather user requirements and complement our notes,
each lasting up to two hours. From these data, we elicited func-
tional requirements and design decisions for both tools.

Early prototype and formative evaluation (design, imple-
ment, deploy): We created the early prototype and performed a
qualitative formative evaluation with ﬁve collaborating cyberse-
curity educators and one student familiar with the CTF games.
Late prototype and summative evaluation (design, imple-
ment, deploy): We added new features and redesigned the user
interface based on received feedback. A qualitative summative
evaluation with eight participants served us for the validation of
the ﬁnal designs.

Final deployment (implement, deploy): The last phase in-
cludes the integration of TAT into the KYPO CRP. We also
plan to collect further feedback from its routine usage. Unfortu-
nately, due to the COVID-19 pandemic, the number of training
sessions has been severely limited.

5. User requirements

Post-training session evaluation provides many opportunities
for tutors to perform a detailed analysis of a training scenario
and assessment of the trainees. The interviews and ﬁeld obser-
vations revealed that tutors struggle with analyzing the training
data from the individual sessions. They expressed the need for
an overview of the data collected during the training session,
which enables them to: analyze trainees’ behavior, compare
their performance, and revise the content and conﬁguration of
the training scenario.

We organized the requirements into the four main categories:
R1 – Trainee behavior analysis: Tutors should examine
trainees’ behavior and identify outliers – e.g., those who are
extremely slow/fast or gave up the training. They should as-
sess the trainees by comparing their results (e.g., ﬁnal time and
score, taken hints, number of entered incorrect ﬂags). It is also
relevant when the training session is a part of some competition.
Further, reviewing the trainees’ actions, such as many partially
correct ﬂags submitted by several trainees, can point out ﬂaws
in the puzzle assignment.

R2 – Assessment revision: Correctly set scores and penal-
ties are crucial for the gameplay and trainees’ motivation to
complete the training. Setting the penalties for hints too small,
for instance, can demotivate trainees in attempting to ﬁnd the
solution by themselves. Instead, they could take all hints imme-
diately, which would even result in a better ﬁnal score. There-
fore, the tutors should be able to review the assessment criteria
of the training session.

R3 – Timing revision: Proper estimation of time require-
ments for cybersecurity puzzles is tricky. Short time allocated

for a challenging puzzle can delay the whole session, put un-
necessary pressure on trainees to take hints early, or force tutors
to intervene prematurely. During the interviews, even the most
experienced tutors admitted that they do not have a proper ﬁrst
estimate of mapping puzzle diﬃculty to time limits. Therefore,
tutors should be able to review the time limits of the training
session.

R4 – Training content revision: Tutors should be able to
analyze problematic parts of the training content to improve its
quality iteratively. The trouble can be hidden either in indi-
vidual puzzles (e.g., unclear puzzle assignment, useless hint)
or their interconnection (e.g., the unbalanced diﬃculty of two
successive levels).

6. Early design

The main goal of the Training Analysis Tool (TAT) is to dis-
play data from a single training session in the context of the cor-
responding training scenario (e.g., puzzle assignments, scoring,
timing). The tool is designed as a dashboard combining sev-
eral linked views. Its design follows principles formulated by
Oslejsek et al. [14]:

• Analyze the impact of tutor’s supervision: The tool con-
sists of temporal views of trainees’ actions and the score
development at various levels of detail. Tutors can analyze
the impact of both individual and class-wide interventions
by focusing on the time of intervention.

• Analyze quality of training exercise: All views display the
score and time limits that form the primary assessment cri-
teria and delimit the training session’s diﬃculty. These vi-
sual artifacts help tutors to analyze the quality of training.
Moreover, predeﬁned parameters (penalties, time limits,
tasks) are available in the dashboard together with run-
time data, enabling tutors to reveal possible weaknesses
in training scenarios by comparing expected versus actual
development.

• Analyze behavior analysis of trainees: The training ses-
sion is captured from several perspectives: temporal view
on trainees’ activities, a static preview of ﬁnal results, and
detailed dynamic score development. By combining these
coordinated views, tutors can interactively analyze indi-
vidual trainees’ behavior, compare them mutually or con-
cerning expected behavior, and visually identify outliers.
The early prototype of the Training Analysis Tool (TAT)
(Fig. 3) is a web application consisting of three interactive vi-
sualizations: time-score overview, training overview, and in-
dividual training walkthrough.

The former two are based on visualizations proposed by [39]
for player-centered reﬂection and CTF game results. Since their
input data is similar (timestamped events), we used its core de-
sign principles and visual encoding, but our visualizations pro-
vide extended interaction capabilities. We further elaborate on
the design of individual TAT components in detail.

All three visualizations of the early prototype use a ﬁxed
color scheme. The colors were meant to distinguish individ-
ual levels of training and were selected in diﬀerent intensities
to be distinguishable for people with the most common forms
of color vision deﬁciencies.

Accepted manuscript / Computers & Graphics (2021)

5

Fig. 3. The early prototype of the Training Analysis Tool (TAT) consists of three interconnected visualizations. The time-score overview (top-left) presents
the distribution of achieved scores (ﬁnal and per-level) for each trainee. The training overview (top-right) displays the overall training duration for each
trainee and their activities (e.g., taking hints, inserting incorrect ﬂags). The individual walkthrough (bottom) is suitable for a detailed comparison of two or
more trainees.

6.1. Time-score overview

Total duration and the ﬁnal score are two main factors used
for measuring the performance of the trainees. The time-score
overview (Fig. 3, top-left) helps identify the correlations be-
tween these factors, providing a view on the score distribution,
pinpoint the outliers, or allocate clusters.

Using simple standard statistical views, such as boxplots,
would be inconvenient because we need to put in the context
multiple metrics (average, and estimate times, ﬁnal scores).
Therefore, the visualization combines bar charts with scatter
plots to incorporate time and score data into a single view. The
top bar shows the total time (x-axis) and each trainee’s ﬁnal
score (y-axis). The smaller bars below represent individual lev-
els (i.e., tasks). Each bar’s length expresses the maximum time
for the given level (i.e., the time of the slowest trainee). The
average time is on the border of two color shades. Although the
scoring span can diﬀer in each level, the bars have ﬁxed heights.
The vertical space is suﬃcient to display and analyze achieved
score distribution regardless of the scoring span. The maximal
level or game score is on the y-axis, and the exact score numbers
are provided on-demand as tooltips of individual dots together
with a trainee’s name.

Hovering the mouse cursor over the dot highlights the corre-
sponding results of the trainee in the remaining levels highlight
and the exact time and the achieved score for the level display.
A mouse click on the dot highlights the corresponding data in
the training overview and displays detailed score development

in the individual training path at the bottom. Dot clusters can
visually indicate the correlations between time and score, which
is particularly helpful when the tutor aims to identify the train-
ing design issues such as a level diﬃculty compared to its dura-
tion.

The tutors can use it to analyze the results of individual
trainees and put them in the context of the training group (R1)
or to review score-based assessment (R2). Bar charts also help
the tutors review time requirements (R3). Dot clusters may help
in the identiﬁcation of problematic levels in the training sce-
nario (R4).

6.2. Training overview

The training overview (Fig. 3, top-right) provides a detailed
yet compact and uncluttered view of the trainees’ progressions
and activities. It is based on a stacked bar chart where each row
corresponds to one trainee. Segments represent training levels
and encompass related game events as glyphs. A user can ﬁlter
the data based on the level duration and zoom the view to unfold
the aggregated events (numbered circles) performed quickly.

The visualization shows the relative time of the training. The
stacked bars are aligned to the left, so it is possible to compare
the time requirements regardless of the delays caused by indi-
vidual trainees’ various starting times (R3). Level labels above
the bars support sorting by the duration of the corresponding
levels. The related vertical lines indicate the expected level du-
ration. When sorted, they also reveal the deviation of the actual
and estimated time for each trainee.

6

Accepted manuscript / Computers & Graphics (2021)

The glyphs indicate events. In this view, they help the tutors
to recognize possible problems in the design of training deﬁ-
nition (R4) or analyze the behavior of the trainees (R1). For
example, multiple incorrect ﬂags submitted by diverse trainees
can indicate unclear or ambiguous instructions; many hints
taken in quick succession may suggest a lack of eﬀort caused
by improper diﬃculty.

6.3. Individual walkthrough

The individual walkthrough (Fig. 3, bottom) is based on a
step chart with glyphs representing trainees’ actions. It enables
the tutors to track outliers’ behavior (R1) and explore the cause
of recognized problems in the training session (R2, R4). It pro-
vides a detailed insight into a trainee’s advancement and ac-
tions or allows comparing two or three trainees selected from
the training overview list or the time-score overview. The y-
axis represents gained score. The horizontal dashed lines imply
the maximal level score. The striped background outlines the
estimated level times.

A zoom function allows adjusting the view on a selected por-
tion of the chart, which is useful when the events are clustered.
On mouse hover, a tooltip shows details for each action. A con-
text view frame below the main chart helps the tutor to get ori-
ented in the zoomed area and shift the time range when needed.
Furthermore, the checkboxes in the bottom right corner allow
ﬁltering the event types.

7. Formative evaluation

The main goal was to gain feedback on the TAT’s usefulness

in four areas:

• Trainees – Is it possible to identify trainees who struggled
(e.g., lacking behind, stuck with the task/level)? Can tutors
recognize any unusual behavior of trainees (e.g., cheating,
prolonged inactivity)?

• Training session – Is it possible to recognize when the
training is running out of schedule? Can tutors identify
scenario design issues?

• Visual encoding – Is the visualization easy to understand?

What type of information is redundant or missing?

• Interaction – How do tutors interact with the visualiza-

tion? Are the interaction capabilities suﬃcient?

We further evaluated the usability and usefulness of the visu-
alizations and gathered remarks on visualization improvements
for the following design process iteration.

7.1. Participants

Due to the necessary background knowledge of hands-on cy-
bersecurity training, we conducted a qualitative user study with
ﬁve domain experts (P1–P5) and one student (P6). All of them
were members of the university cybersecurity team who partake
in hands-on training on diﬀerent positions. Table 1 shows their
demographic information.

Table 1. Demographic summary of the participants and their involvement
in the design study. TE – teaching experience (in years), OE – organized
hands-on exercises (in sessions). Participation in individual stages: PC
– problem characterization; FE – formative evaluation; SE – summative
evaluation.
ID

Position

Age

OE

TE

PC

FE

SE

P1
P2
P3
P4
P5
P6
P7
P8

33
27
31
27
35
24
22
21

Lecturer, Manager
Seminar tutor
Seminar tutor
Seminar tutor
Senior lecturer
CTF Course graduate
CTF Course graduate
CTF Course graduate

4
7
3
5
5
0
0
0

>20
<20
>20
<10
>20
1
1
0

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

7.2. Procedure

In September 2019, we held the formative evaluation ses-
sions in person using 27” iMac with the resolution 2560×1440
and Google Chrome browser version 76. The experimenter
took notes and audio recorded the participants’ opinions and
thoughts.

The user study had two parts, and the participants were asked
to think aloud. The sessions lasted about an hour. In the ﬁrst
part, the experimenter outlined the procedure. The participant
consented and ﬁlled the demography questionnaire. The experi-
menter presented the TAT and situated the participant in the role
of a tutor using the tool. Next, the participant spent 2–3 minutes
familiarizing with it using dummy data followed by completing
three tasks addressing requirements R1–R4:

• T1: Identify an unusual behavior of trainees and name the

potential issues.

• T2: Find and compare a pair of trainees who: a) have the
same score; b) were the best and the worst; c) were the
slowest and the fastest. How do they diﬀer?

• T3: Identify problems caused by the poor design of the

training scenario and propose improvements.

Participants performed the tasks on two data sets DS1 and
DS2. We chose the genuine data since they contain various ac-
tions observable during training sessions (e.g., guessing the cor-
rect ﬂag, prolonged inactivity, varying trainees’ performance).
Their diﬀerent size, number of trainees, and duration show two
distinct yet ordinary real-world circumstances.

DS1 is from the tutorial on computer forensic skills and con-
sists of six game levels. The goal is to identify and examine ma-
licious software running in the computer system. The trainees
learn how to identify a suspicious application, dissect its exe-
cutable, and process memory. The session lasted 55 minutes,
and 16 trainees generated 374 events, making the 23.4 events
per trainee on average. DS2 is an attack-oriented training sce-
nario that consists of four game levels with the following puz-
zles: exploit server vulnerability, gain the root privileges, access
a protected data ﬁle, and cover the traces after the attack. Six
trainees generated 146 events over 90 minutes, averaging 24.8
events per trainee.

Finally, the participant ﬁlled two usability questionnaires and
was debriefed. We chose the SUS – System Usability Scale [40]
and the SEQ – Single Ease Question [41], two widely used
questionnaires for measuring various products’ usability. The

Accepted manuscript / Computers & Graphics (2021)

7

former is a widely used method for assessing the usability of
the systems. The latter is considered a robust measure to quan-
tify the usability for tasks that are too complex for metrics like
task duration time or completion rate3 and when the number of
participants is low, as in our case.

7.3. Results

scored 5 (T1: Unusual behavior of trainees) and 6 (T2: Com-
parison of trainees).

The formative evaluation revealed weaknesses in the early
design and helped us understand tutors’ work after the training
session.

The most acclaimed feature of the training overview visual-
ization is the ability to sort trainees by the time spent at some
level and compare them to the estimated level duration (deﬁned
in the training scenario). Participants also used the visualization
to identify the trainees who signiﬁcantly exceeded the estimated
level duration time.

For most of the participants, the score overview visualization
was a starting point when solving all the tasks. They used it to
identify outlying trainees (P2, P4, P6), to assess the diﬃculty of
each level based on the time/score distribution of trainees (P1,
P2, P4, P6), or to compare it with the maximum score per level
(P3, P4). P3 also used the score overview to assess the concep-
tual design of the training scenarios (the ﬁrst levels should be
manageable and short compared to the ﬁnal ones). Participants
lacked information about estimated level duration (P1–P4, P6).
P6 wanted even more details, such as medians of time and score
for each level.

Participants often used score overview visualization to high-
light trainees in training overview and vice versa.
Score
overview was also often used in T2 as a selector for trainees
to compare. We did not observe any other extensive mutual use
of two or all three visualizations. On the other hand, the individ-
ual training walkthrough visualization was generally consid-
ered ”useful only in a speciﬁc case when the training session is
organized as a competition to decide the ﬁnal order of trainees”
(P4).

The main complaint (mentioned by all) was the absence of a
tabular view showing various details of all trainees such as their
ﬁnal score, scores per level, number of taken hints, or incorrect
ﬂags.

Other frequent issues were: the absence of ﬁltering features
(P1–P5); a missing overview of the training scenario allowing
the users to skim through the texts of tasks, set penalties, and
ﬂags (P2–P4, P6); insuﬃcient integration of the visualizations
(P1, P2, P4, P5); and the visual encoding (P1, P3, P4, P6) con-
sidered by P3 as ”disturbing due to many colors without proper
meaning.”

The SUS score was 65.4 points (out of 100). It corresponds to
the good rating, according to the adjective ratings [42]. Fig. 4
summarizes the SUS questionnaire responses. With the SEQ
score of 6.5 (out of 7), the TAT showed to be well-suited for
training design analysis (T3: Identify training design issues.).
The two tasks focused on identifying and comparing trainees

3The user responds to a single precisely-worded question (“Overall, how
diﬃcult or easy did you ﬁnd this task?”), using a scale from 1 (Very diﬃcult) to
7 (Very easy).

Fig. 4. Formative evaluation: The SUS questionnaire responses.

While these results conﬁrmed the overall usability and use-
fulness of the TAT, we had to address the main issues raised by
the study participants.

8. Final design

We revised the ﬁnal design rationale, visual encoding, and
interaction capabilities of the current version of the TAT based
on the formative evaluation. The prototype, implemented us-
ing Angular and D3.js library, is available at https://tat.
surge.sh.

The main principles of the three visualizations remain the
same. However, we signiﬁcantly redesigned the layout mak-
ing the training overview the most prominent visualization.
We also added more ﬁltering options for selecting individual
trainees and revised the use of colors. The formative evaluation
also revealed that the coloring of levels is not essential for the
users, so we have changed it in the late prototype: the platform
on which the training sessions take place generates a unique
avatar for each trainee. Therefore, we decided to emphasize the
trainees based on the avatar’s color instead. Now, each trainee
has a unique color in all three visualizations. These colors are
not intended as the exclusive means of trainee identiﬁcation but
as complementary visual support (to accompany the ability to
highlight or ﬁlter the trainees). To distinguish training levels,
we used gray color shades in the late prototype.

Finally, we added additional information regarding the train-
ing deﬁnition, such as the task descriptions, correct ﬂags, and
contextualized trainees’ data with individual levels. Fig. 6 dis-
plays the ﬁnal layout, with the collapsed training definition
summary and visualization filters sections.

8.1. Training deﬁnition summary and visualization ﬁlters

The TAT’s upper part (Fig. 5 – A) contains a collapsible
panel with the training deﬁnition details, visualization ﬁlters,
and avatar-based trainees ﬁlter. The training definition sum-
mary serves for the conﬁguration of the tool and synopsis of
the training. It provides training scenario parameters (i.e., task
assignments, hints, penalties, correct ﬂags). The tabs show data

423121311142131123221312121411111I think that I would like to use the visualization frequently.I found the visualization unnecessarily complex.I thought the visualization was easy to use.I think that I would need the support of a technical person to be able to use this I found the various functions in this visualizations were well integrated.I thought there was too much inconsistency in this visualization.I would imagine that most people would learn to use this visualization very quickly.I found the visualization very awkward to use.I felt very confident using the visualization.I needed to learn a lot of things before I could get going with this visualization.0246Strongly disagreeDisagreeNeutralAgreeStrongly agree8

Accepted manuscript / Computers & Graphics (2021)

Fig. 5. The Training Analysis Tool (TAT) consists of the upper panel for training deﬁnition summary and ﬁlters (A) and three visualizations: the training
overview (B) displays the overall training duration for each trainee and their activities (e.g., taking hints, inserting incorrect ﬂags). The time-score overview
(C) presents the distribution of achieved scores (ﬁnal and per-level) for each trainee. The individual walkthrough (D) directly compares of two or three
trainees and is subordinate to the training overview.

for individual levels (Fig. 6 – A). For each game level, a table
summarizing data of individual trainees provides an overview
of the gained score, taken hints, incorrect ﬂags, and time spent
in the level (R2 and R3). Comparing the results shown in the ta-
ble with the level content and parameters (e.g., the comparison
of incorrect ﬂags with the correct ﬂag or scheduled time allo-
cation with the average or median values) can help the tutors
identify problematic parts of the content (R4).

The Visualization Filters (Fig. 6 – B) are global ﬁltering
options to show or hide glyphs representing hints or ﬂags and
switch between trainees’ avatars and names (IDs). The avatars
(Fig. 6 – C) are switches for ﬁltering out the trainees from the
training overview and time-score overview.

8.2. Training overview

We extended the training overview (Fig. 5 – B) with the table
summarizing total game duration, achieved score, number of

taken hints, and submitted incorrect ﬂags for each trainee. We
also added the legend for quicker orientation.

The training overview interacts with two complementary
views. By clicking on the stacked bar, the individual walk-
through visualization appears, showing score polyline and
events of the corresponding trainee. The level bars highlight the
corresponding dots in the time-score overview and the polyline
in the individual walkthrough on mouseover.

8.2.1. Time-score overview

Unlike the early prototype version, we added the dashed ver-
tical line to indicate the actual average completion time of the
trainees. The striped segments delimit the time estimate for
each level. Therefore, the tutors can quickly identify the diﬀer-
ences between the expected and the actual (and averaged) time
for each level, as shown in Fig. 7.

Accepted manuscript / Computers & Graphics (2021)

9

basic concepts and only have hands-on experience with their
design.

9.2. Procedure

Due to the COVID-19 pandemic restrictions, we held it re-
motely using Google Meet, which we also used to record au-
dio and screen. The participants used their computers or lap-
tops with the 13.3”–27” screens and resolutions ranging from
FullHD to UHD. The procedure was almost the same as in the
formative evaluation (see Sec. 7.2). The only diﬀerence was a
new data set that we used for the tasks.

DS3 uses data from a training session held as the introductory
lecture of the CTF game design course of Fall 2019. It is an
attack-oriented four-level training scenario similar to DS2, in
this case, tested on nine trainees who generated 281 events in
the session lasting 110 minutes. On average, each participant
performed 31.3 events.

9.3. Results

The participants completed all the tasks without struggle.
Despite minor diﬃculties, the immediate feedback was more
positive than in the previous evaluation. Since the tasks are
complex and depend on the tutor’s knowledge and experience
we sought qualitative input rather than measuring user perfor-
mance.

Participants mostly worked with the training overview since
it contains most of the necessary information. The time-score
overview serves well to identify timing issues and assess level
diﬃculty. The training definition summary supports ﬁnding
ﬂaws in the puzzle assignments (e.g., misleading texts, wrong
instructions for ﬂag format). Further, we did an inductive qual-
itative analysis [43] of the video recordings, which is summa-
rized below.

Visualizations usage. Figure 8 shows the usage of visualiza-
tions to solve the tasks by participants. The most preferred was
the training overview. All but P5 used the training overview
as a starting point when solving all the tasks (P5 preferred the
time-score overview). Its most acclaimed feature is the ability
to sort trainees by the time spent in individual levels and com-
pare them to the estimated level duration (deﬁned in the training
scenario). Participants also used the visualization to identify
the trainees who signiﬁcantly exceeded the estimated level du-
ration time. All the sorting options (by time spent in a level,
ﬁnal time, score, hints, and incorrect ﬂags) were used at least
once by each participant. On the other hand, the zooming func-
tion was used only rarely (P1, P6). The participants used the
time-score overview to identify outlying trainees (P2, P4, P5),
assess each level’s diﬃculty based on their time/score distribu-
tion (P1, P2, P4, P5), or compare it with the maximum score
per level (P3, P4). The individual walkthrough was still con-
sidered the least usable (P1, P2, P5, P6, P7). P1 and P5 did not
work with it at all. Others used it only for a direct comparison
of two trainees (T2).

The TAT allows comparison of trainees beyond time and
score. To identify non-standard trainees’ behavior (T1), we ob-
served that all the participants revealed all or almost all occur-
rences of the most common types, such as taking all hints at

Fig. 6. TAT – details of the training definition summary (A), configuration
(B), and the trainees (C) sections.

Fig. 7. The Time-Score Overview combines bar charts with scatter plots to
show relationships between the score and time of the game levels.

8.2.2. Individual walkthrough

In the ﬁnal version, the individual walkthrough (Fig. 5 – D)
displays upon selecting a trainee in the training overview. The
selected trainees are indicated as avatars next to the title. We
also reﬂected the main complaints regarding the clutteredness
and simpliﬁed the visualization layout. Only the total training
duration estimate is shown instead of the estimate for each level.
We also added the vertical dashed line to indicate the actual
average time, similarly to the time-score overview.

9. Summative evaluation

The summative evaluation was held in April 2020. We in-
tended to validate the ﬁnal design concerning the user require-
ments R1–R4, assess the usability and usefulness of the TAT,
and identify possible reﬁnements for the ﬁnal integration into
the KYPO CRP.

9.1. Participants

We asked the same six people who participated in the for-
mative evaluation. We also recruited two more students who
passed the CTF design course taught at our university (see Ta-
ble 1). They represent novice users familiar with CTF games’

10

Accepted manuscript / Computers & Graphics (2021)

Fig. 8. Gray cells indicate visualization usage (Vis) when solving tasks T1–
T3 for each participant (P1–P8). Visualizations: Training Overview (TO),
Time-Score Overview (TS), Individual Walkthrough (IW).

once shortly after they entered a new level or guessing the ﬂags
in each dataset. The participants found those with the lowest
score/largest time, followed by a detailed inspection of the num-
ber of taken hints and inserted incorrect ﬂags. The procedure
was the same for all. The diﬀerence was only in the starting
visualization. While P4, P5, and P7 started with the time-score
overview, the rest used the training overview solely. The par-
ticipants also intensively used the trainee ﬁlter combined with
the training overview sorting capabilities to ﬁlter out unwanted
trainees quickly, especially for the second task (T2). Despite the
individual walkthrough received mixed reactions, most partici-
pants (except P1 and P5) used it for a head-to-head comparison.
The TAT helps to identify training scenario shortcomings.
When dealing with the identiﬁcation of training scenario short-
comings (T3), the participants mainly focused on three areas:
correcting the time estimates and maximal score of individual
levels, the perceived level diﬃculty, and instructions for a cor-
rect ﬂag format. All the participants proposed changing the
time estimates or the assigned maximum of points based on
the trainees’ overdue in the ﬁrst two levels of D3. Moreover,
seniors (P1, P3–P5) also identiﬁed the confusion with the ﬂag
formatting instructions in the second level. P3–P5 analyzed the
data even more profoundly and revealed the ﬂaw in the game
design based on the observation that some trainees used the cor-
rect ﬂag for the fourth level in the third one.

Except for P1, P2, and P4, the participants used the train-
ing definition summary since it clearly shows the diﬀerence be-
tween the estimate and real-time. The size of each level allows
for a quick comparison of their perceived diﬃculty (the longer
it took, the problematic the level was). The glyphs visualizing
incorrect ﬂags in the training overview proved to be good indi-
cators for potential issues with the puzzle assignments, includ-
ing the technical instructions. All the experts (P1–P5) greeted
the training definition summary as a convenient way to search
for problematic parts of the training deﬁnition.

Gaps and drawbacks of the TAT. We received several sug-
gestions for further improvements to the TAT visualizations. P5
suggested adding “the horizontal line also showing the average
score per level” in the time-score overview to improve compar-
ing level scores. The two-level ﬁltering (avatars → trainees in
the training overview) received mixed feedback. Only three
participants (P1, P2, P5) used both to ﬁlter out speciﬁc trainees,
while others preferred to keep all of them visible. The eval-
uation also revealed that with the grayscale for the training

Fig. 9. Single Ease Question scores of the tasks in both evaluations.

Fig. 10. Summative evaluation: The SUS questionnaire responses.

overview, highlighting of selected trainees is not very pro-
nounced and will be revised in future development.

The main beneﬁt of the individual walkthrough is that the
polyline visualizing score development better informs the tutor
whether there are similarities in the trainees’ gameplay. Since
this is useful only in a speciﬁc use case, we will reconsider its
integration in the subsequent design iterations simplifying the
user interface.

The average SUS score raised to 77.5 (compared to 65.4 for
the early prototype), which still equals to good rating. We as-
sume that it is mainly due to the higher complexity of the tool
and the remaining issues with the individual walkthrough. The
data plot of the SUS questionnaire responses is in Fig. 10. How-
ever, the medians 6.0 of SEQ score (Fig. 9) for all the tasks (T1–
T3) further supports our statement that the TAT is well-suited
for the post-training analysis.

10. Discussion

In this section, we discuss the ﬁndings and limitations of the

studies.

10.1. Lessons learned

The summative evaluation validated our design decisions.
The verbal feedback from participants and the SEQ and SUS

1234567Task 1Task 2Task 3Formative Evaluation (N=6)Summative Evaluation (N=8)Overall, how difficult or easy was the task to complete?2455431223111212111211514232611243I think that I would like to use the visualization frequently.I found the visualization unnecessarily complex.I thought the visualization was easy to use.I think that I would need the support of a technical person to be able to use this I found the various functions in this visualizations were well integrated.I thought there was too much inconsistency in this visualization.I would imagine that most people would learn to use this visualization very quickly.I found the visualization very awkward to use.I felt very confident using the visualization.I needed to learn a lot of things before I could get going with this visualization.02468Strongly disagreeDisagreeNeutralAgreeStrongly agreeAccepted manuscript / Computers & Graphics (2021)

11

scores conﬁrmed that the tools address the elicited require-
ments. We also revealed three notable ﬁndings regarding the
presentation of summaries, sorting and ﬁltering capabilities,
and domain speciﬁcity.

Summaries. Extending the visualization with pertinent sum-
mary data could help tutors to overview the situation and iden-
tify anomalies quickly. Especially in analytical tools, even el-
ementary statistics and simple charts are helpful. Although we
did not implement such charts in the TAT, some participants
asked for them as feature requests.

Sorting and ﬁltering. The evaluation revealed that we
should work with the sorting and ﬁltering options even more
thoughtfully so that tutors can better focus their attention. There
must be a real usage scenario for each ﬁlter type. Particular at-
tention should be paid to carefully selecting items for ﬁltering
and the batch selecting and ﬁltering shortcuts (e.g., “deselect
all”).

Domain-speciﬁc insight over universality and scalabil-
ity. Puzzle-based learning represents a vast area where tutors’
support tools diﬀer vastly among various application domains.
Since there are no guidelines or best practices and the user re-
quirements are often contradictory, they have to be considered
carefully, and the tools should be tailored to speciﬁc uses. Fur-
thermore, the amount of data from a single session is usually
relatively small.

10.2. Limitations

Both user studies had two main limitations to the external
validity:
low number of participants and qualitative focus of
the evaluation in the controlled environment instead of the in-
the-wild evaluation.

To ensure the evaluation’s ecological validity, we needed
users with practical experience with organizing hands-on train-
ing sessions and knowledge of cybersecurity education. These
demands notably restrict our choice of suitable candidates. Our
collaborating cybersecurity educators are, no doubt, the primary
users of the developed tools. Therefore, they provided relevant
feedback, which will serve as a source for further thoughts on
both tools’ improvements. We also asked students of the cyber-
security degree program who successfully passed the university
course on CTF games design. They represent novice users un-
familiar with analytical visualizations.

Due to the qualitative nature of the evaluations, we did not fo-
cus on ﬁnding the limits in terms of the total number of trainees
and their events since the events with more than 16 participants
are literally none due to the space limits of the training facility at
our university. We originally planned to perform the case stud-
ies to assess the TAT’s ﬁnal design in the actual deployment.
Unfortunately, due to the COVID-19 pandemic, the scheduled
hands-on training sessions had been canceled, and the only fea-
sible option was to perform the evaluation remotely, using the
same procedure as in the summative evaluation.

In this work, we restrict ourselves to the case study of hands-
on cybersecurity courses focused on system hacking and cyber-
attacks.
In particular, puzzle-based capture the ﬂag games
where the structure and data are well-deﬁned in advance. These

restrictions allowed us to provide the tutors with a more in-
depth insight into this speciﬁc application sub-domain through
a pair of visualization tools.

Despite these limitations, the provided feedback has been
guiding our work and feature requests for the deployment into
the KYPO CRP.

11. Conclusion and future work

We introduced the visual analytics tool that, based on the
qualitative feedback, improves the tutors’ insight into the train-
ing sessions and allows them to assess the quality of the training
scenarios and evaluate the training session results. We focused
on low-level learning analysis (i.e., analyzing data from a sin-
gle training session). As we pointed out in Sec. 2, this particular
area is often overlooked since the main focus in support tools
for tutors and educators is on high-level analysis for MOOC
e-learning.

We have presented a design study on applying visual analyt-
ics to data from hands-on cybersecurity training in the form of
CTF games. We introduced two iterations of the Training Anal-
ysis Tool, allowing tutors to assess the quality of the training
scenarios and gain insight into the trainees’ progress beyond
the completion time and ﬁnal score. The summative evalua-
tion validated our design decisions. The verbal feedback from
participants and the SEQ and SUS scores conﬁrmed that the
tools address the elicited requirements. We gradually learned
more about what information tutors would like to display in
the visualization and how they interact with the data during the
design study. Based on this experience, we believe that a data-
driven insight into the training courses could provide surpris-
ing insights and knowledge about the design and behavior of
trainees.

Focusing on puzzle-games principles enabled us to concep-
tualize the data and visualizations beyond the cybersecurity do-
main. If we look closely at the information we used, we realize
that it is a quadruple: timestamp, the ID of the trainee, type
of event, content (arbitrary). Therefore, we believe that our
approach can be easily applied in other areas where hands-on
training becomes common. We admit that there are further re-
quirements, such as automated processing of user inputs, but
even basic logging can provide suﬃcient data. The level of de-
tail depends mainly on the expressiveness of the content com-
ponent.

Consider the university programming course as another ap-
plication area. The tutors often evaluate students’ assignments
using automated compilation and validation tools against pre-
deﬁned unit tests and datasets. The summary of code diﬀs,
compiler error logs, and output of the automated tests can be
logged. Similar to the cybersecurity domain, these events can
be mapped to assessment events (e.g., penalties for unsuccess-
ful unit tests), player actions (e.g., the submission of a piece of
code), and progress events (e.g., successful compilation and test
of a programming task). Visualizing these events on the time-
lines (one per student) or further text analysis of the code can be
as valuable as our analogy with the cybersecurity CTF games.
The support tools for a category of so-called blended class-
rooms and hands-on courses are still mostly unexplored. Our

12

Accepted manuscript / Computers & Graphics (2021)

work addresses only a tiny part of this broad research area.
Despite our focus on cybersecurity education, we consider our
ﬁndings applicable in other areas of puzzle-based learning and
analyzing data from a single training session (i.e., low-level
learning analysis). We want to encourage others to explore
novel methods for visual analysis of puzzle-based learning
courses in diﬀerent areas.

The TAT is integrated into the user interface of the KYPO
CRP. We also work on additional data integration from sand-
boxes (e.g., resource usage, executed commands, running pro-
cesses). Enhancing the current level of event processing with
this information will further improve the insight and enable a
more detailed analysis of the training and its scenario. Our next
goal is to explore the possibilities for visual analysis of multi-
ple training sessions and analyze and assess trainees’ long-term
progress. Extending the analysis with automatic highlighting of
anomalies or ﬂaws in the training design is another direction of
research that needs further study.

References

[1] Medeiros, RP, Ramalho, GL, Falc˜ao, TP. A Systematic Literature Re-
view on Teaching and Learning Introductory Programming in Higher Ed-
ucation. IEEE Trans on Education 2018;62(2):77–90.

[2] McMurtrey, ME, Downey, JP, Zeltmann, SM, Friedman, WH. Critical
Skill Sets of Entry-level IT Professionals: An Empirical Examination of
J of Inf Tech Education: Research
Perceptions from Field Personnel.
2008;7:101–120.

[3] Michalewicz, Z, Michalewicz, M. Puzzle-based learning. Ormond,

Australia: Hybrid Publishers; 2008.

[4] Yoneyama, Y, Matsushita, K, Mackin, KJ, Ohshiro, M, Yamasaki,
K, Nunohiro, E. Puzzle Based Programming Learning Support System
with Learning History Management. In: Proc. of the 16th Int. Conf. on
Computers in Education. New York: IEEE Press; 2008, p. 623–627.
[5] Merrick, KE. An Empirical Evaluation of Puzzle-based Learning as an
Interest Approach for Teaching Introductory Comp. Science. IEEE Trans
on Education 2010;53(4):677–680.

[6] Harms, KJ, Rowlett, N, Kelleher, C. Enabling Independent Learning of
Programming Concepts Through Programming Completion Puzzles. In:
2015 IEEE Symp. on Visual Languages and Human-Centric Computing
(VL/HCC). IEEE; New York: IEEE Press.; 2015, p. 271–279.

[7] Gondree, M, Peterson, ZN, Denning, T. Security Through Play. IEEE

Security & Privacy 2013;11(3):64–67.

[8] Hendrix, M, Al-Sherbaz, A, Victoria, B. Game-based Cyber Security
Training: Are Serious Games Suitable for Cyber Security Training? Int J
of Serious Games 2016;3(1):53–61.

[9] Dasgupta, D, Ferebee, DM, Michalewicz, Z. Applying Puzzle-Based
Learning to Cyber-Security Education. In: Proc. of the 2013 on InfoS-
ecCD ’13: Information Security Curriculum Development Conf. New
York, NY, USA: ACM; 2013, p. 20:20–20:26.
Information trails:

In-process assessment of game-based
In: Assessment in game-based learning. Berlin, Heidelberg:

[10] Loh, CS.

learning.
Springer; 2012, p. 123–144.

[11] Chung, GK. Guidelines for the design and implementation of game
telemetry for serious games analytics. In: Serious games analytics. Berlin,
Heidelberg: Springer; 2015, p. 59–79.

[12] Alonso-Fernandez, C, Calvo, A, Freire, M, Martinez-Ortiz,

I,
Fernandez-Manjon, B. Systematizing game learning analytics for serious
games. In: 2017 IEEE global engineering education Conf. (EDUCON).
IEEE; New York: IEEE; 2017, p. 1111–1118.

[13] Owen, VE, Baker, RS. Fueling prediction of player decisions: Founda-
tions of feature engineering for optimized behavior modeling in serious
games. Technology, Knowledge and Learning 2020;25(2):225–250.
[14] Oˇslejˇsek, R, Rusˇn´ak, V, Bursk´a, K, ˇSv´abensk´y, V, Vykopal, J, ˇCegan,
J. Conceptual model of visual analytics for hands-on cybersecurity train-
ing. IEEE Trans on Vis and Comp Graphics 2020;:1–13.

[15] Rodr´ıguez-Triana, M, Prieto, L, et al. Monitoring, Awareness and Re-
ﬂection in Blended Technology Enhanced Learning: a Systematic Re-
view. Int J of Tech Enhanced Learning 2016;9.

[16] Gratzl, S, Lex, A, Gehlenborg, N, Pﬁster, H, Streit, M. Lineup: Visual
analysis of multi-attribute rankings. IEEE transactions on visualization
and computer graphics 2013;19(12):2277–2286.

[17] Matcha, W, Gaˇsevi´c, D, Uzir, NA, Jovanovi´c, J, Pardo, A. Analytics of
Learning Strategies: Associations with Academic Performance and Feed-
back. In: Proc. of the 9th Int. Conf. on Learning Analytics & Knowledge.
New York, NY, USA: ACM; 2019, p. 461–470.

[18] Jivet,

I, Scheﬀel, M, Specht, M, Drachsler, H. License to Evaluate:
Preparing Learning Analytics Dashboards for Educational Practice. In:
Proc. of the 8th Int. Conf. on Learning Analytics and Knowledge. New
York, NY, USA: ACM; 2018, p. 31–40.

[19] Oˇslejˇsek, R, Vykopal, J, Bursk´a, K, Rusˇn´ak, V. Evaluation of cyber
defense exercises using visual analytics process. In: 2018 IEEE Frontiers
in Education Conference (FIE). IEEE; 2018, p. 1–9.

[20] de Freitas, S, Gibson, D, et al. How to Use Gamiﬁed Dashboards and
Learning Analytics for Providing Immediate Student Feedback and Per-
formance Tracking in Higher Education. In: Proc. of the 26th Int. Conf.
on World Wide Web Companion. Geneva, Switzerland: Int. World Wide
Web Conf.s Steering Committee; 2017, p. 429–434.

[21] Loh, CS, Sheng, Y, Ifenthaler, D. Serious Games Analytics: Method-
ologies for Performance Measurement, Assessment, and Improvement.
Springer International Publishing; 2015.

[22] Macfadyen, LP, Dawson, S. Mining LMS Data to Develop an “Early
Warning System” for Educators: A Proof of Concept. Computers & Edu-
cation 2010;54(2):588–599.

[23] Siemens, G, Long, P. Penetrating the Fog: Analytics in Learning and

Education. EDUCAUSE Review 2011;46(5):30.

[24] Verbert, K, Govaerts, S, Duval, E, Santos, JL, Van Assche, F, Parra,
G, et al. Learning Dashboards: An Overview and Future Research Op-
portunities. Personal and Ubiquitous Computing 2013;18:1499–1514.

[25] Verbert, K, Duval,

JL.
Learning Analytics Dashboard Applications. Am Behavioral Scientist
2013;57(10):1500–1509.

J, Govaerts,

E, Klerkx,

S, Santos,

[26] Schwendimann, BA, Rodr´ıguez-Triana, MJ, Vozniuk, A, Prieto, LP,
Boroujeni, MS, Holzer, A, et al. Perceiving Learning at a Glance: A
IEEE
Systematic Literature Review of Learning Dashboard Research.
Trans on Learning Tech 2017;10(1):30–41.

[27] Bodily, R, Verbert, K. Review of Research on Student-Facing Learn-
ing Analytics Dashboards and Educational Recommender Systems. IEEE
Trans on Learning Tech 2017;10(4):405–418.

[28] S´ebastien, V, S´ebastien, D, Timol,

I, Gay, D, Cucchi, A, Porlier,
C. Moodleboard: Dynamic and Interactive Indicators for Teachers and
Pedagogical Engineers. In: 2019 Conf. on Next Generation Computing
Applications (NextComp). New York: IEEE; 2019, p. 1–5.

[29] Charleer, S, Moere, AV, Klerkx, J, Verbert, K, De Laet, T. Learning
Analytics Dashboards to Support Adviser-Student Dialogue. IEEE Trans
on Learning Tech 2018;11(3):389–399.

[30] Govaerts, S, Verbert, K, Klerkx, J, Duval, E. Visualizing Activities
for Self-reﬂection and Awareness. In: Int. Conf. on Web-based Learning.
Berlin, Heidelberg: Springer Berlin Heidelberg; 2010, p. 91–100.

[31] Jacobs, KL.

Investigation of Interactive Online Visual Tools for the
Int J of Mathematical Education in Sci and

Learning of Mathematics.
Technology 2005;36(7):761–768.

[32] Fu, X, Shimada, A, Ogata, H, Taniguchi, Y, Suehiro, D. Real-time
Learning Analytics for C Programming Language Courses. In: Proc. of
the Seventh Int. Learning Analytics & Knowledge Conf. New York, NY,
USA: ACM; 2017, p. 280–288.

[33] Falah, A, Pan, L, Abdelrazek, M. Visual Representation of Penetration
Testing Actions and Skills in a Technical Tree Model. In: Proc. of the
Australasian Comp. Sci. Week Multiconference. New York, NY, USA:
ACM; 2017, p. 8:1–8:10.

[34] Werther, J, Zhivich, M, Leek, T, Zeldovich, N. Experiences in Cyber
Security Education: The MIT Lincoln Laboratory Capture-the-ﬂag Exer-
cise. In: Proc. of the 4th Conf. on Cyber Security Experimentation and
Test. Berkeley, CA, USA: USENIX Association; 2011, p. 1–12.

[35] Davis, A, Leek, T, Zhivich, M, Gwinnup, K, Leonard, W. The Fun
and Future of CTF. In: 2014 USENIX Summit on Gaming, Games, and
Gamiﬁcation in Security Education (3GSE 14). San Diego, CA: USENIX
Association; 2014, p. 1–9.

Accepted manuscript / Computers & Graphics (2021)

13

[36]

[37]

ˇSv´abensk´y, V, Vykopal, J, Cermak, M, Laˇstoviˇcka, M. Enhancing
Cybersecurity Skills by Creating Serious Games. In: Proc. of the 23rd
Annual ACM Conf. on Innovation and Tech. in Comp. Sci. Education.
ACM; New York, NY, USA: ACM; 2018, p. 194–199.
ˇCeleda, P, ˇCegan, J, Vykopal, J, Tovarˇn´ak, D. KYPO – A Platform
for Cyber Defence Exercises.
In: STO-MP-MSG-133: M&S Support
to Operational Tasks Including War Gaming, Logistics, Cyber Defence.
Munich (Germany): NATO Science and Technology Organization; 2015,
p. 12.

[38] Sedlmair, M, Meyer, M, Munzner, T. Design Study Methodology: Re-
ﬂections from the Trenches and the Stacks. IEEE Transactions on Visand
Comp Graphics 2012;18(12):2431–2440.

[39] Oˇslejˇsek, R, Rusˇn´ak, V, Bursk´a, K, ˇSv´abensk´y, V, Vykopal, J. Vi-
sual Feedback for Players of Multi-Level Capture the Flag Games: Field
Usability Study. In: Proc. of the IEEE Symp. on Vis.for Comp. Security
(VizSEC). New York: IEEE Press.; 2019, p. 1–11.

[40] Sauro, J. A Practical Guide to the System Usability Scale: Background,
Benchmarks & Best Practices. USA: CreateSpace Independent Publish-
ing Platform; 2011.

[41] Sauro, J, Dumas, JS. Comparison of Three One-question, Post-task Us-
ability Questionnaires. In: Proc. of the SIGCHI Conf. on Human Factors
in Computing Systems. New York, NY, USA: ACM; 2009, p. 1599–1608.
[42] Bangor, A, Kortum, P, Miller, J. Determining What Individual SUS
J Usability Studies

Scores Mean: Adding an Adjective Rating Scale.
2009;4(3):114–123.

[43] Thomas, DR. A General Inductive Approach for Analyzing Qualitative

Evaluation Data. Am J of Evaluation 2006;27(2):237–246.

