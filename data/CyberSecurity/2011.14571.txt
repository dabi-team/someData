2
2
0
2

n
a
J

8
1

]

C
O
.
h
t
a
m

[

3
v
1
7
5
4
1
.
1
1
0
2
:
v
i
X
r
a

A reputation game on cyber-security and cyber-risk calibration

Kookyoung Han∗ and Jin Hyuk Choi†

January 20, 2022

Abstract

To analyze strategic interactions arising in the cyber-security context, we develop a new reputation

game model in which an attacker can pretend to be a normal user and a defender may have to announce

attack detection at a certain point of time without knowing whether he has been attacked. We show the

existence and uniqueness of sequential equilibrium in Markov strategies, and explicitly characterize the

players’ equilibrium strategies. Using our model, we suggest empirical and theoretical ways of calibrating

the attack probability, which is an important element of cyber-risks.

Keywords: stochastic control, cyber-security, reputation game, Bayesian learning, optimal stopping.

1

Introduction

Modern technologies have been relying more and more on networks such as the Internet of Things, mobile

networks, and cyber-physical systems. At the same time, many organizations and institutions have been

suﬀering from bigger threats of cyber-attacks such as advanced persistent threats (APTs). Recent examples

of APTs are the SolarWinds hack in 2020, Microsoft Exchange Server data breach in 2021, and Double

Dragon (APT41). The ﬁrst two examples are known to operate over a few months, and the last example is

known to operation over a few years.

There are several characteristics of APTs. First, it is diﬃcult to prevent APTs because APT actors

utilize various tools such as zero-day attacks, unﬁxed vulnerabilities of a system, and even social engineering.

Second, it is not easy to detect APTs. APT actors steal a small amount of data, pretending to be normal

users. Third, APTs are carried out for a long period of time. Since it is diﬃcult to prevent and detect

APTs, it takes a long time to become aware of APTs. Fourth, APT actors adjust their activities based on

circumstances. These characteristics of APTs have two implications. First, it is diﬃcult to discern malicious

hackers from innocent users. Especially, this diﬃculty is persistent over time. Second, false alarms have to

be involved in detecting APTs since it is too late if a security manager waits until she collects hard evidences

and fully realizes cyber-attacks as a fact.

APTs consist of multiple stages. At the reconnaissance stage, ATP actors lure users or employees of a
targeted company. Once some are lured and infected, APT actors use the infected hosts as a foothold, and

escalate their privileges to obtain an access to servers of the target company. Then, APT actors continuously

and slowly steal data from the servers. A security manager estimates the likelihood of being attacked using

∗Ulsan National Institute of Science and Technology, Email: kyhan@unist.ac.kr
†Ulsan National Institute of Science and Technology, Email: jchoi@unist.ac.kr

1

 
 
 
 
 
 
a certain countermeasure. The security manager take actions such as shutting down the servers once she is

suﬃciently suspicious of cyber-attacks.

Motivated by the characteristics of APTs, we consider a dynamic game in which a defender tries to

detect cyber-attacks, suﬀering from persistent private information and false alarm costs. Persistent private

information has been studied in reputation games and asset pricing models. Firms can pretend to be
In [11], it is shown that in a large class of
a commitment type to threaten potential entrants [15, 18].

repeated games, reputation eﬀect eventually disappears. In [13], reputation game in continuous time setting

is studied when a group of small players faces a large commitment type player. Regarding asset pricing

models, [1, 2, 3, 4, 5, 8, 9, 16] investigate how equilibrium asset price dynamics is derived by informed
trader’s trading strategies.1

The defender in our model plays against a suspect who can be either an attacker who dynamically chooses

actions or an innocent user who repeats the same action over time. Observing noisy signals of the suspect’s

actions, the defender can decide to see whether the suspect is the attacker or innocent by inspecting the

suspect. The defender expects to incur potential damages due to cyber-attacks before inspection, but he

incurs the false alarm cost when the suspect turns out to be innocent after inspection. This aspect is a

diﬀerence between our model and the studies mentioned in the previous paragraph because there is no way

that players in the studies reveal private information. However, the defender in our model can reveal private

information although he is penalized for false detection.

We explicitly solve for sequential equilibrium in Markov strategies with the suspicion level, which is the

posterior probability that the suspect is the attacker based on noisy observations of the suspect’s actions.

Our analysis shows that the attacker’s actions are weakened as the suspicion level increases and that the

defender begins inspection in equilibrium only if the suspicion level exceeds a certain threshold. In addition

to characterization of equilibrium, we propose an empirical way of estimating the initial probability of the

suspect being the attacker based on data that only indicate whether a user is inspected or not. We also

propose a theoretical estimation on the initial probability of cyber-attacks, assuming that the attacker can

choose the attack probability right before the game. Two methods that we propose would be useful because

actual estimation may be neither available nor reliable due to lack of actual data.

The rest of this paper is organized as follows. In Section 2, we formally describe the model. In Section 3,

we explicitly characterize sequential equilibrium of the model and provide comparative statics of equilibrium

strategies. The proof of the main theorem is provided in Section 4.

In Section 5, we propose empirical

and theoretical estimations on the initial probability of cyber-attacks. In Section 6, we calibrate the model

parameters using a report on data breach. Section 7 illustrates graphical results and numerical simulations.

Section 8 summarizes this paper and suggests several extensions of our model for future research.

2 The Model

We consider a continuous-time game between two risk-neutral players, a suspect and a defender. The

suspect’s type is a random variable θ taking value in {0, 1}. The suspect is an attacker (θ = 1) with
probability q0 ∈ (0, 1) or innocent (θ = 0) with the complementary probability 1 − q0. The suspect knows
the true value of θ, whereas the defender does not.

The attacker chooses attack intensity 0 ≤ ∆t ≤ M at every moment t ≥ 0 in time, where the constant

1Asset pricing has also been studied in models without private information. For instance, [7, 19, 20, 22] study dynamic

asset pricing in Radner equilibrium.

2

M > 0 is the upper bound of attack intensities. The innocent type always chooses zero attack intensity.

One interpretation of attack intensities is the amount of data that the attacker steals at every moment in

time. The defender chooses whether to block the suspect or not at every moment in time. Once the defender
blocks the suspect, the game ends and the true value of θ is publicly revealed.2

The defender does not directly observe the suspect’s attack intensity. Instead, the defender observes the
signal process (Yt)t≥0, which is noisy observations of the suspect’s attack intensities. We assume that the
signal process obeys the following stochastic diﬀerential equation (SDE):

dYt = ∆t1{θ=1}dt + σ dWt,

(2.1)

where 1{θ=1} is the indicator function, σ is a strictly positive constant, and (Wt)t≥0 is a standard Brownian
motion independent of θ. The signal process (Yt)t≥0 is public information. That is, the players observe the
signal process.

The defender is a Bayesian learner. Based on observations of the signal process Y up to time t, the

defender calculates suspicion level qt, the probability of the suspect being the attacker at time t:

qt = P (cid:0)θ = 1(cid:12)

(cid:12)F Y
t

(cid:1) ,

(2.2)

where (F Y
t )t≥0 is the ﬁltration generated by the signal process (Yt)t≥0. We derive the SDE for qt that
describes the change in the suspicion level given the attack intensity process (∆t)t≥0, using Theorem 8.1 of
[17]:

E[θ ∆t1{θ=1}|F Y

t ] − E[θ|F Y

t ] · E[∆t1{θ=1}|F Y
t ]

dqt =

=

(cid:16)

1
σ2
qt(1 − qt)∆t
σ2

(cid:16)

dYt − qt∆tdt

(cid:17)

.

(cid:17)

(cid:16)

·

dYt − E[∆t1{θ=1}|F Y

t ]dt

(cid:17)

(2.3)

We assume that the game can be over by a random time T that is independent of θ and (Wt)t≥0 and has

an exponential distribution

P(T > t) = e−rt,

(2.4)

with a constant r > 0. Note that T is an exogenously given random variable that the players cannot

control. The true value of θ is also publicly revealed if the game ends due to the random termination. One

interpretation of the random termination time T is that the suspect’s identity θ can be revealed to the

defender due to other independent factors.

The attacker’s strategy (∆t)t≥0 is assumed to be a progressively measurable process with respect to the
ﬁltration (F Y
t )t≥0. The attacker obtains proﬁts of ∆tdt during inﬁnitesimal time interval dt until the game
is over at T ∧ τ := min{T, τ }, where τ is the time the defender blocks the suspect. The attacker seeks the
optimal attack intensity to maximize her expected proﬁts:3

max
0≤(∆t)t≥0≤M

E

(cid:34)(cid:90) T ∧τ

0

(cid:35)
(cid:12)
(cid:12)
(cid:12) θ = 1

.

∆tdt

(2.5)

2This assumption can be thought of as a circumstance in which after blocking a suspected user, a defender begins a thorough

inspection that results in hard evidence on the identity of the suspected user.

3If τ is a ﬁxed random time, then the obvious optimal strategy in (2.5) is ∆t = M for all t ≥ 0. However, the defender
bases his decision τ on observations of the signal process Y . Equations (2.3) and (2.6) imply that τ will depend on the attack
intensity ∆, and thus the optimization in (2.5) is not obvious at all.

3

Figure 1

As one can see from the expression above, the attacker’s proﬁt is larger if she steals a larger amount of data

for a longer period of time.

If the suspect is the attacker, the defender incurs costs of ∆tdt during inﬁnitesimal time interval dt until
the defender blocks the suspect. If the suspect is innocent, the defender incurs zero costs during the game,

but incurs a one-time false alarm cost l > 0 if the defender blocks the innocent user before the random

termination. Even though we use the phrase ‘false alarm cost’, l does not necessarily represent penalty for

wrong detection only. The term l represents the defender’s opportunity costs when he disables the innocent

suspect. For instance, l can include foregone proﬁts during system downtimes, claims from customers because

of inconvenience, and actual costs of inspecting the suspect’s identity.

The defender’s strategy is when to block the suspect, and his strategy is represented by a stopping time
t )t≥0.

t )t≥0. We denote T as the set of all stopping times with respect to (F Y

with respect to the ﬁltration (F Y
The defender’s goal is to ﬁnd the optimal stopping time to minimize expected costs:

(cid:34)

E

min
τ ∈T

1{θ=1} ·

(cid:32)(cid:90) T ∧τ

0

(cid:33)

(cid:35)

∆tdt

+ 1{θ=0, τ <T } · l

.

(2.6)

The defender’s cost is larger if a larger amount of data is stole or if the false alarm cost is larger, which

seems reasonable to some extent.

In this paper, we restrict our attention to sequential equilibrium in Markov pure strategies that depend
only on the suspicion level qt in (2.2). To be more speciﬁc, the attacker’s equilibrium strategy is represented
by a function α : [0, 1] → [0, M ] of the suspicion level q, and the defender’s equilibrium strategy is represented

by a closed set S ⊂ [0, 1] which is the collection of suspicion levels at which the defender stops the game.
For convenience, we denote τS as the ﬁrst time the suspicion level process hits the closed set S,

τS = inf{t ≥ 0 : qt ∈ S}.

(2.7)

4

Clearly, τS is a stopping time with respect to (F Y

t )t≥0.

Now, we introduce the deﬁnition of our Markov equilibrium.

Deﬁnition 2.1. Consider a process (qt)t≥0, a closed set S ⊂ [0, 1], and a Lipschitz continuous function
α : [0, 1] → [0, M ]. We say that the triplet ((qt)t≥0, S, α) is a Markov equilibrium if the following conditions
hold:

(1) (Consistency) The process (qt)t≥0 satisﬁes Bayes’ rule (2.2), given the initial value q0 and the attack

intensity ∆t = α(qt).

(2) (Attacker’s optimality) The process (cid:0)α(qt)(cid:1)

t≥0 is the solution to the attacker’s proﬁt maximization

problem (2.5) for given τ = τS,

(cid:0)α(qt)(cid:1)

t≥0 ∈ arg max

0≤(∆t)t≥0≤M

E

(cid:34)(cid:90) T ∧τS

0

(cid:35)
(cid:12)
(cid:12)
(cid:12) θ = 1

.

∆tdt

(2.8)

(3) (Defender’s optimality) The stopping time τS solves the defender’s cost minimization problem (2.6)

for given attack intensity ∆t = α(qt),

τS ∈ arg min

τ ∈T

(cid:34)

E

1{θ=1} ·

(cid:32)(cid:90) T ∧τ

0

(cid:33)

(cid:35)

α(qt)dt

+ 1{θ=0, τ <T } · l

.

(2.9)

The ﬁrst condition in the deﬁnition above implies that the suspicion level is calculated as if the initial
value is q0 and the attack intensity is α, given the observation of the signal process. The second and third
conditions are typical, meaning that every player’s equilibrium strategy is the best response to everyone

else’s equilibrium strategy.

3 Equilibrium Analysis

3.1 Heuristic Derivation of Diﬀerential Equations When S = [p, 1]

We ﬁrst present heuristic derivation of the diﬀerential equations that the players’ value functions satisfy in
Markov equilibrium ((qt)t≥0, [p, 1], α). Indeed, in Proposition 4.2, we provide the result that the defender’s
equilibrium strategy should be the form of S = [p, 1]. For simpler presentation, we slightly abuse our notation

of stopping times as

τk = inf {t ≥ 0 : qt ∈ [k, 1]}

for k ∈ [0, 1].

The attacker’s expected proﬁt can be written as

(cid:104) (cid:90) T ∧τp

E

0

∆tdt

(cid:12)
(cid:105)
(cid:12)
(cid:12) θ = 1

(cid:104) (cid:90) τp

= E

0

1{T >t}∆tdt

(cid:12)
(cid:105)
(cid:12)
(cid:12) θ = 1

(cid:104) (cid:90) τp

= E

0

e−rt∆tdt

(cid:12)
(cid:105)
(cid:12)
,
(cid:12) θ = 1

(3.1)

where the second equality is from the independence of T and the other random variables. Using the expression

in (3.1), we deﬁne the value function V as

V (q) :=

max
0≤(∆t)t≥0≤M

(cid:104) (cid:90) τp

E

0

e−rt∆tdt

(cid:12)
(cid:12)
(cid:12) θ = 1, q0 = q

(cid:105)
.

(3.2)

We derive the Hamilton-Jacobi-Bellman (HJB) equation for V in (3.2). The ﬁrst condition in the deﬁnition
of sequential equilibrium speciﬁes how the suspicion level qt is calculated given the observation of the signal

5

process up to time t. In our equilibrium concept, it is a common knowledge for both players that the attacker
chooses attack intensities according to α in equilibrium. Therefore, when the attacker actually chooses ∆t
at time t, the inﬁnitesimal change in the suspicion level is:

dqt =

qt(1 − qt)α(qt)
σ2

(∆tdt + σdWt − qtα(qt)dt) .

(3.3)

Given equation (3.3), the attacker’s proﬁt maximization problem (3.2) produces the following HJB equation:

− rV (q) − V (cid:48)(q) q2(1−q)α(q)2

σ2

+ 1

2 V (cid:48)(cid:48)(q) q2(1−q)2α(q)2

σ2

+ max

∆∈[0,M ]

(cid:0)V (cid:48)(q) q(1−q)α(q)

σ2

+ 1(cid:1)∆ = 0,

(3.4)

for q ∈ (0, p). If V (cid:48)(q) · q(1−q)α(q)
maximizer in (3.4) is supposed to be α(q) in equilibrium, we rewrite (3.4) for the case of V (cid:48)(q) ≥ − σ2

+ 1 ≥ 0, then ∆ = M maximizes the left hand side of (3.4). Since the
q(1−q)M ,

σ2

V (cid:48)(cid:48)(q)

2 + V (cid:48)(q)

q − rσ2V (q)

M 2q2(1−q)2 +

σ2

M q2(1−q)2 = 0.

(3.5)

For the case of V (cid:48)(q) < − σ2
Then any ∆ ∈ [0, M ] maximizes (3.4), and we rewrite (3.4) as

q(1−q)M , we set α(q) = − σ2

q(1−q) ·

1

V (cid:48)(q) to match the maximizer in (3.4) with α(q).

V (cid:48)(cid:48)(q)

2 − V (cid:48)(q)

1−q − r

σ2 V (cid:48)(q)2 V (q) = 0.

(3.6)

To ﬁnd the boundary condition for the attacker’s value function in (3.2), we observe that q0 = 0 implies
τp = ∞ because 0 is an absorbing state (see SDE (2.3)). Then the attacker chooses the highest intensity M
all the time and the corresponding value is V (0) = (cid:82) ∞

0 e−rtM dt = M
r .

Below are the diﬀerential equation and boundary conditions that the attacker’s value function satisﬁes.






If q ∈ (0, p) and V (cid:48)(q) ≥ − σ2

M (1−q)q , then

If q ∈ (0, p) and V (cid:48)(q) < − σ2

M (1−q)q , then










If q ∈ [p, 1], then V (q) = 0.

V (0) = M
r .

V (cid:48)(cid:48)(q)

2 + V (cid:48)(q)

q − rσ2V (q)

M 2q2(1−q)2 +

σ2

M q2(1−q)2 = 0,

α(q) = M.
V (cid:48)(cid:48)(q)

2 − V (cid:48)(q)

α(q) = −

1−q − r
σ2
q(1−q)V (cid:48)(q) .

σ2 V (cid:48)(q)2 V (q) = 0,

(3.7)

Now we derive a diﬀerential equation from the defender’s optimal stopping problem (2.9). Using (2.2),

the defender’s expected cost at time 0 can be written as

(cid:104) (cid:90) τ

E

0

(cid:105)
e−rtα(qt)qt dt + e−rτ l(1 − qτ )

.

We deﬁne the value function U as

U (q) := min
τ ∈T

(cid:104) (cid:90) τ

E

0

e−rtα(qt)qt dt + e−rτ l(1 − qτ )

(cid:12)
(cid:12)
(cid:12) q0 = q

(cid:105)

.

(3.8)

(3.9)

Since the defender’s perception of the attack intensity is ∆t = α(qt) in equilibrium, the corresponding

6

SDE for the suspicion level is:

dqt =

qt(1 − qt)α(qt)
σ2

(α(qt)dt + σdWt − qtα(qt)dt) .

(3.10)

Given equation (3.10), the optimal stopping problem (3.9) produces the following variational inequality and
the characterization of the optimal stopping time τp in equilibrium:

(cid:110)

0 = min

− rU (q) + 1

2 U (cid:48)(cid:48)(q) · q2(1−q)2α(q)2

σ2

τp = inf{t ≥ 0 : U (qt) = l(1 − qt)}.

+ q α(q) , l(1 − q) − U (q)

(cid:111)
,

(3.11)

(3.12)

If q0 = 0, then qt ≡ 0 all the time (see SDE (2.3)). Therefore, in case q0 = 0, the maximizer in (3.9) is
τ = ∞ and we obtain U (0) = 0. With this boundary condition and the smooth-ﬁt principle, we rewrite the
variational inequality (3.11) as






If q ∈ (0, p), then

If q ∈ [p, 1], then








−rU (q) + q2(1−q)2α(q)2U (cid:48)(cid:48)(q)

2σ2

+ q α(q) = 0,

U (q) < l(1 − q).
−rU (q) + q2(1−q)2α(q)2U (cid:48)(cid:48)(q)

2σ2

+ q α(q) ≥ 0,

(3.13)



U (q) = l(1 − q).

U (0) = 0,

limq↑p U (q) = l(1 − p),

limq↑p U (cid:48)(q) = −l.

In summary, (3.7) and (3.13) constitute the system of diﬀerential equations for the Markov equilibrium. To
describe the explicit solution of the system, we ﬁrst deﬁne the functions ϕ, y and constants a, b, c, q∗ as

(cid:90) x

e−t2

ϕ(x) := 2√
π
0
(cid:16)(cid:113)
1 + 8rσ2

dt,

(cid:17)
M 2 − 1

a := 1
2
b := (1−a)M
r ,
2σ
√
re−b2
√

c := 2σ

√

M

π + ϕ(b),

,

(3.14)

q∗ := p(c−ϕ(b))
c−p ϕ(b) ,
y(x) := ϕ−1(cid:0) c(p−x)

p(1−x)

(cid:1).

Note that the constant q∗ and the function y depend on the constant p that will be determined later.

Lemma 3.1. Let the functions ϕ, y and constants a, b, c, q∗ be as in (3.14). If rσ2
hold.

M 2 < 1, then the followings

(1) a ∈ (0, 1), b > 0, c ≥ 1, q∗ ∈ (0, p) and y(q∗) = b.
(2) 0 < c

π (cid:0) (1−p)x

M for x ∈ (q∗, p].

(cid:1)ey(x)2

− 2σ

p(1−x)

√

√

r

Proof. Elementary calculations produce (1). To obtain (2), we ﬁrst observe that z (cid:55)→ ϕ(z) + 2σ
strictly increasing function on z ∈ [0, b]. Then, for x ∈ (q∗, p], we observe that y(x) < b and

√

M

r e−z2
√
π

is a

ϕ(b) + 2σ

r e−b2
√

√

M

π > ϕ(y(x)) + 2σ

√

r e−y(x)2
π
M

√

− c(1−p)x
p(1−x)

(cid:17)

+ c.

√

r e−y(x)2
π
M

√

(cid:16) 2σ

=

7

The above inequality, together with the expression of c, produces (2).

The unique explicit solution of the system (3.7) and (3.13) is provided in the next proposition.

Proposition 3.2. The unique solution of the system (3.7) and (3.13) that satisﬁes V, U ∈ C 2([0, p)) has the
following expression:
(1) If rσ2
M 2 ≥ 1, then

p = (1+a)rl

(1+a)rl+aM

α(q) = M for



M
r

V (q) =

q ∈ [0, 1]

(cid:0)1 − (cid:0) 1−p

p

(cid:1)a(cid:0) q

1−q

(cid:1)a(cid:1)

for q ∈ [0, p)





U (q) =

0
(cid:16) M

q

r − (cid:0) M

r − (1−p)l

p



(1 − q)l

(2) If rσ2

M 2 < 1, then

for q ∈ [p, 1]
(cid:1)a(cid:17)

(cid:1)a(cid:0) q

1−q

(cid:1)(cid:0) 1−p

p

for q ∈ [0, p)

for q ∈ [p, 1]

for q ∈ [0, q∗],

for q ∈ (q∗, p),

for q ∈ [p, 1].

α(q) =

√

r

√

√

M

r
π

√
√

πr
πr+σ

π (1−p)q e−y(q)2

2p(1−q)σ
c
2σ
c

p = c l
√
c l

M





0

(cid:16) M
r − (cid:0) σ2
aM − c
q

r q y(q) + l(1 − q)(cid:0)e−y(q)2
σ√

(1 − q)l

r − σ2
aM
σ√
r y(q)

πr(1−p)lσ
(1+a)M p

(cid:0) 1−q∗
q∗

(cid:1)a(cid:0) q

1−q

(cid:1)a

√

V (q) =

U (q) =

for q ∈ [0, q∗]

for q ∈ (q∗, p)

for q ∈ [p, 1]

(cid:1)(cid:0) 1−q∗

q∗
− c

√

(cid:1)a(cid:0) q

1−q

(cid:1)a(cid:17)

π(1−p)q y(q)

p(1−q)

(cid:1)

for q ∈ [0, q∗]

for q ∈ (q∗, p)

for q ∈ [p, 1]

(3.15)

(3.16)

(3.17)

(3.18)

(3.19)

(3.20)

(3.21)

(3.22)

Proof. Lemma 3.1 and explicit computations produce the proposition.

3.2 Characterization of the Unique Equilibrium

We further restrict our attention to Markov equilibria in which the value functions of the attacker and the

defender are smooth enough (twice diﬀerentiable), and show that there exists a unique Markov equilibrium

that induces the smooth enough value functions.

Theorem 3.3. There exists a unique4 Markov equilibrium ((qt)t≥0, S, α). And, there exists a unique p ∈ [0, 1]
such that S = [p, 1]. The equilibrium stopping threshold p and the equilibrium attack intensity α have the

4It turns out that in the Markov equilibrium ((qt)t≥0, S, α), α|(p,1] has little to no impact on the stopping threshold p. That
is, for another Lipschitz continuous function ˆα such that ˆα|[0,p] = α|[0,p], the defender optimally chooses p as long as ˆα|(p,1]
is not too small. This means that ((qt)t≥0, S, ˆα) is another Markov equilibrium. However, the pair of the optimal stopping
threshold p and the optimal attack intensity α over the interval [0, p] is uniquely determined by the exogenous parameters. In
this sense we say that our Markov equilibrium is unique. For simplicity, we set α(q) = α(p) for q ∈ (p, 1].

8

following form:

In case

rσ2
M 2 ≥ 1 :

In case

rσ2
M 2 < 1 :

p = (1+a)rl

(1+a)rl+aM , α(q) = M for q ∈ [0, 1].

M



√
πr
πr+σ , α(q) =

π (1−p)q e−y(q)2

2p(1−q)σ
c
2σ
c

√
√

r
π

√

√

r

p = c l
√
c l

for q ∈ [0, q∗],

for q ∈ (q∗, p),

for q ∈ [p, 1].

(3.23)

Proof. We postpone the proof to Section 4. The result is the direct consequence of Proposition 4.1 and

Proposition 4.2.

This theorem shows that in equilibrium, the defender blocks the suspect once the suspicion level exceeds a

certain threshold p, allowing us to use the stopping threshold instead of the set of suspicion levels at which

the defender blocks the suspect.

The expression of the equilibrium in (3.23) implies that if rσ2

M 2 ≥ 1, it is the dominant strategy for the
attacker to choose the highest attack intensity M all the time. To understand this, it is helpful to imagine
three extreme cases in which rσ2
M 2 is very large. A high probability r of random termination implies that the
game is more likely to end due to random termination, which in turn implies that the attacker has a weaker
incentive to slow down the defender’s learning. A large noise σ enables the attacker to hide behind the

noise. If the upper bound M of attack intensity is low, then the stopping threshold would be high because

the aggregate running costs are low compared to the false alarm costs. In these cases, the attacker has an
incentive to set the highest attack intensity. If rσ2
M 2 < 1, the attacker chooses the highest attack intensity
M when the suspicion level qt is suﬃciently low (qt ≤ q∗). As the suspicion level qt increases above q∗, the
attacker gradually decreases the attack intensity to lower the rate at which the suspicion level is updated.

Remark 3.4. Let us discuss the stopping threshold. Intuitively, if p is the equilibrium stopping threshold,

the defender is indiﬀerent between stopping the game and continuing the game when the suspicion level is

p. The defender incurs the expected cost of (1 − p)l if she stops the game at p. If the defender waits until

the suspicion level becomes p + dp, the expected running cost increases and the expected false alarm cost

decreases. The defender incurs aggregate running costs that she would not have incurred if she had stopped

the game immediately. However, since the defender stops the game at a higher threshold, the expected false

alarm cost decreases. Up to the ﬁrst order of dp, it should be true that:

(cid:34)(cid:90) τp+dp∧T

E

0

α(qt)dt · 1{θ=1} + l · 1{τp+dp<T,θ=0}

(cid:35)
(cid:12)
(cid:12)
(cid:12) q0 = p

= E[l · 1{θ=0} | q0 = p].

(3.24)

Rearranging this equation, up to the ﬁrst order of dp, we obtain:

(cid:20)(cid:90) τp+dp

E

0

e−rtα(qt)dt · 1{θ=1}

(cid:21)
(cid:12)
(cid:12)
(cid:12) q0 = p

= E

(cid:20)(cid:18)(cid:90) τp+dp

(cid:19)

r e−rtdt

l · 1{θ=0}

0

(cid:12)
(cid:12)
(cid:12) q0 = p

(cid:21)

.

(3.25)

This equation makes sense because it basically implies that marginal beneﬁt equals marginal cost. The left
hand side of the equation is the marginal increase in the expected cost and the right hand side is the marginal

reduction in the false alarm cost.

Remark 3.5. Based on the arguments in Remark 3.4, we can infer a possible impact of other types of
running costs on the equilibrium. For instance, let Z be a constant that represents a running cost such as

9

monitoring cost. To be speciﬁc, we add the term E

(cid:104)(cid:82) T ∧τ
0

(cid:105)
Zdt

to the defender’s cost.

Similar to equation (3.25), up to the ﬁrst order of dp, we obtain:

(cid:20)(cid:90) τp+dp

E

0

e−rt(cid:0)α(qt) + Z(cid:1)dt · 1{θ=1}

(cid:21)

(cid:12)
(cid:12)
(cid:12) q0 = p

= E

(cid:20)(cid:90) τp+dp

0

e−rt(rl − Z)dt · 1{θ=0}

(cid:21)
(cid:12)
(cid:12)
(cid:12) q0 = p

.

(3.26)

As we can see from the equation above, other types of running costs decrease the stopping threshold. Intu-

itively, if the defender incurs a larger amount of running costs (due to the monitoring cost Z), he is more
willing to stop the game earlier to save costs.5

Equation (3.26) has another important implication. As one can infer from the previous paragraph, other

types of running costs or income ﬂows do change quantitative properties of equilibrium, but do not alter

qualitative properties of equilibrium. To be more speciﬁc, in the model with the additional cost term Z, one

can check that Theorem 3.3 still holds with a diﬀerent expression of p that depends on Z.

The explicit expression in (3.23) allows us to describe how the equilibrium threshold and attack intensity

change as the exogenous parameter changes.

Proposition 3.6. (1) The equilibrium stopping threshold p increases in l and r and decreases in σ and M .

(2) The equilibrium attack intensity α increases in l, r, σ and M .

Proof. (1) We prove that p is a decreasing function of M . Other cases can be proved similarly. In case
rσ2
M 2 ≥ 1, we have p = (1+a)rl
(1+a)rl+aM . We substitute a in (3.14) into the expression of p and compute the
derivative,

∂p
∂M = −

√

8lr2σ2(
M 2+8rσ2(cid:0)−M 2+lr
√

√

M 2+8rσ2−2M )

M 2+8rσ2+M (lr+

√

M 2+8rσ2)(cid:1)2 < 0,

where the inequality is due to rσ2

M 2 < 1, we have p = cl

M 2 ≥ 1.
√
πr
πr+σ . We substitute c in (3.14) into the expression of p and compute

√

cl

In case rσ2
the derivative,

e−b2

∂p
∂M = −

l(cid:0)(M 2+2rσ2)
M 2

√

√

M 2+8rσ2−M 3−6M rσ2(cid:1)
πrc)2

√

M 2+8rσ2(σ+l

< 0,

where the inequality is due to (M 2 + 2rσ2)2(M 2 + 8rσ2) − (M 3 + 6M rσ2)2 = 32r3σ6 > 0.

(2) It is enough to check the monotonicity of α(q) in the parameters, for rσ2

M 2 < 1 and q ∈ (q∗, p]. We

substitute p in (3.23) to the expression of α in (3.23) and obtain

α(q) = 2(1−q)

q

l r exp (cid:0) − ϕ−1(c −

πrl(1−q) )2(cid:1)
qσ√

for

q ∈ (q∗, p].

(3.27)

(i) (α increases in l): As we know that p increases in l by part (1), the expression of α in (3.23) for

q ∈ (q∗, p] implies that it is enough to check that α increases in p. We ﬁrst check that

e−y(q)2

√

− c

π( (1−p)q

p(1−q) )y(q) ≥ 0

for q ∈ (q∗, p],

(3.28)

5Note that Z does not have to be interpreted as costs only. The term Z can be considered as income ﬂow if it is negative, in
which case the stopping threshold increases. If the defender earns positive proﬁts, he is willing to take more risk of cyberattacks
and tries to stop the game later.

10

where y(q) is deﬁned in (3.14). Indeed, we observe that

(e−y(q)2

√

− c

(cid:0)e−y(q)2

d
dq

− c

p(1−q) )y(q)(cid:1)(cid:12)
π( (1−p)q
√
π( (1−p)q

(cid:12)q=q∗ = ae−b2
> 0,
p(1−q) )y(q)(cid:1) = c2πq(1−p)2ey(q)2

2p2(1−q)3

> 0,

and conclude the inequality (3.28). Then, we observe that α increases in p due to (3.28) and the following

expression:

∂

∂p α(q) = 2

c

rσ(1−q)
π(1−p)2q

√
√

(cid:0)e−y(q)2

√

− c

π(cid:0) (1−p)q

p(1−q)

(cid:1)y(q)(cid:1).

(3.29)

(ii) (α increases in σ): Due to the expression of α in (3.27), it is enough to show that
πrl(1−q) decreases in σ, because the map x (cid:55)→ e−ϕ−1(x)2
qσ√
c −
q ∈ (q∗, p],

decreases in x for x > 0. We observe that for

(cid:0)c −

∂
∂σ

qσ√

πrl(1−q)

(cid:1) = ∂c

∂σ −

q√
πrl(1−q) < ∂c

∂σ −

√

q∗

πrl(1−q∗) = e−b2

(M

√
M 2+8rσ2−M 2−6rσ2)
√

σ2

πr(M 2+8rσ2)

The last inequality above is from M 2(M 2 + 8rσ2) − (M 2 + 6rσ2)2 = −4rσ2(M 2 + 9rσ2) < 0.

(iii) (α increases in r): We ﬁrst observe that

∂c

∂r = ((2A2+1)

√
1+8A2−(6A2+1))e−b2
√
1+8A2
πrA

√

2

> 0, where A = σ

√
r
M .

The inequality above is from ((2A2 + 1)

√

1 + 8A2)2 − (6A2 + 1)2 = 32A6. We also observe

(cid:16)

∂
∂q

e−y(q)2

√

c(1−p)

=

√

2p(1−q) + r ∂c

πy(q)(cid:0) c(1−p)q
−
π(cid:0)2p(1−q)y(q)+ey(q)2 √
4p2(1−q)3

∂r

(cid:1)(cid:17)

π(c(1−p)q+2pr(1−q)

∂c

∂r )(cid:1)

> 0,

for q ∈ (q∗, p],

< 0.

(3.30)

(3.31)

where we use (3.30) for the inequality. Using the expression of α in (3.27) and the inequalities (3.30) and
(3.31), we observe that for q ∈ (q∗, p],

∂

∂r α(q) = 2(1−q)l
> 2(1−q)l
q

q

(cid:16)

(cid:16)

e−y(q)2

−

√

πy(q)(cid:0) c(1−p)q
√

2p(1−q) + r ∂c

∂r

πy(q∗)(cid:0) c(1−p)q∗

(cid:1)(cid:17)

e−y(q∗)2

−
2(1−q)le−b2(cid:0)16A4+15A2+2−(5A2+2)
√
4qA2

1+8A2

√

2p(1−q∗) + r ∂c
1+8A2)(cid:1)

∂r

=

(cid:1)(cid:17)

(3.32)

, where A = σ

√
r
M .

The above expression and the inequality (16A4 + 15A2 + 2)2 − ((5A2 + 2)
13A4 + A2) > 0 produce ∂

∂r α(q) > 0.

√

1 + 8A2)2 = 8(32A8 + 35A6 +

(iv) (α increases in M ): Due to the expression of α in (3.27), it is enough to show that c decreases in M ,

because the map x (cid:55)→ e−ϕ−1(x)2

decreases in x for x > 0. Indeed,

∂c

∂M = e−b2

(M 3+6M rσ2−(M 2+2rσ2)
M 2+8rσ2
πrM 2σ

√

√

√

M 2+8rσ2)

< 0,

where the last inequality is by (M 3 + 6M rσ2)2 − ((M 2 + 2rσ2)

√

M 2 + 8rσ2))2 = −32r3σ6 < 0.

11

4 Proof of Theorem 3.3

This section is devoted to the proof of Theorem 3.3. In Proposition 3.2, we provide the unique solution of the

system of the diﬀerential equations (3.7) and (3.13). In Proposition 4.1, we verify that the unique solution
In Proposition 4.2, we show that if ((qt)t≥0, S, α) is a Markov
equilibrium, then the set S should be of the form [p, 1] for a constant p > 0. All in all, Proposition 4.1 and

indeed constitutes a Markov equilibrium.

Proposition 4.2 complete the proof of Theorem 3.3.

Proposition 4.1. ((qt)t≥0, [p, 1], α) deﬁned in (3.23) is a Markov equilibrium in Deﬁnition 2.1, and V and
U in Proposition 3.2 are the value functions of the attacker and defender.

Proof. Checking (1) in Deﬁnition 2.1

The Lipschitz continuity of α ensures that the SDE (2.3) has a unique solution when ∆t = α(qt). Theorem

8.1 of [17] implies that the solution of the SDE (2.3) satisﬁes (2.2).

Checking (2) in Deﬁnition 2.1
We prove the optimality of τp in (2.9). For any τ ∈ T and U in Proposition 3.2, Ito’s formula produces

e−r(t∧τ )U (qt∧τ ) +

(cid:90) t∧τ

e−rsα(qs)qs ds

= U (q0) +

≥ U (q0) +

(cid:90) t∧τ

0
(cid:90) t∧τ

0

0

e−rs(cid:16)

− rU (qs) + q2

s (1−qs)2α(qs)2
2σ2

U (cid:48)(cid:48)(qs) + qs α(qs)

(cid:90) t∧τ

(cid:17)

ds +

0

e−rsU (cid:48)(qs)dqs

(4.1)

e−rsU (cid:48)(qs)dqs,

where the inequality is due to the fact that U satisﬁes (3.13).

If we consider the stopping time τp, the

inequality becomes an equality:

e−r(t∧τp)U (qt∧τp ) +

(cid:90) t∧τp

0

e−rsα(qs)qs ds = U (q0) +

(cid:90) t∧τp

0

e−rsU (cid:48)(qs)dqs.

(4.2)

We apply the Fubini’s theorem and the iterated conditioning to obtain

(cid:104) (cid:90) t∧τ

E

0

e−rsU (cid:48)(qs)dqs

(cid:90) ∞

(cid:105)

=

0

(cid:104)
1{0≤s≤t∧τ }e−rsU (cid:48)(qs) qs(1−qs)α(qs)2
E

σ2

E(cid:2)1{θ=1} − qs

(cid:12)
(cid:12)F Y
s

(cid:3)(cid:105)

ds

(cid:104) (cid:90) t∧τ

+ E

0

e−rsU (cid:48)(qs) qs(1−qs)α(qs)

σ

dWs

(cid:105)

= 0,

(4.3)

where the last equality holds because E[1{θ=1} − qs
square-integrable martingale with respect to the ﬁltration (F Y

(cid:12)
(cid:12)F Y

s ] = 0 (see (2.2)) and the stochastic integral part is a

t )t≥0. We combine (4.1)-(4.3) and obtain

(cid:104)
E
e−r(t∧τ )U (qt∧τ ) +

(cid:90) t∧τ

0

(cid:105)
e−rsα(qs)qs ds

(cid:104)
≥ U (q0) = E
e−r(t∧τp)U (qt∧τp ) +

(cid:90) t∧τp

0

(cid:105)
e−rsα(qs)qs ds

.

(4.4)

Since U and α are bounded, as t → ∞, the dominated convergence theorem produces

(cid:104)
E

e−rτ U (qτ ) +

(cid:90) τ

0

(cid:105)
e−rsα(qs)qs ds

(cid:104)
≥ E

e−rτp U (qτp ) +

(cid:90) τp

0

(cid:105)
e−rsα(qs)qs ds

.

12

The above inequality, together with (3.13), implies that

(cid:104)
E

e−rτ l(1 − qτ ) +

(cid:90) τ

0

(cid:105)
e−rsα(qs)qs ds

(cid:104)
≥ E
e−rτp l(1 − qτp ) +

(cid:90) τp

0

(cid:105)
e−rsα(qs)qs ds

.

(4.5)

It remains to derive (2.9) from (4.5). Since the process (qt)t≥0 is uniformly bounded, we apply the optional
sampling theorem6 to the martingale qt = E[1{θ=1}|F Y

t ] and obtain

qτ = E[1{θ=1}|F Y
τ ]

for all τ ∈ T .

(4.6)

Using (4.6) and the independence of T , we obtain the following equalities:

(cid:104)
E

e−rτ l(1 − qτ ) +

(cid:90) τ

0

(cid:105)
e−rsα(qs)qs ds

(cid:104)
= E

l · e−rτ 1{θ=0}

(cid:105)

+

(cid:90) ∞

(cid:104)
E

1{s<τ }e−rsα(qs)1{θ=1}

(cid:105)

ds

(cid:104)
= E

l · 1{θ=0}1{T >τ }

0

(cid:105)

+

(cid:90) ∞

0

(cid:104)
E
1{s<τ }1{s<T }α(qs)1{θ=1}

(cid:105)

ds

(4.7)

(cid:104)(cid:16) (cid:90) T ∧τ

= E

(cid:17)

α(qs)ds

0

· 1{θ=1} + l · 1{θ=0, τ <T }

(cid:105)

,

where we apply the Fubini’s theorem for the ﬁrst and third equality, and use the iterated conditioning for
the ﬁrst and second equality. Since (4.5) and (4.7) hold for any τ ∈ T , we conclude that τp is optimal in
(2.9).

Checking (3) in Deﬁnition 2.1

In this part of the proof, we use notation q(∆)
instead of qt and τp, to emphasize their dependence
on the attacker’s (possibly oﬀ-equilibrium) strategy ∆. To be speciﬁc, for attack intensity process (∆t)t≥0,
let the process (q(∆)
To verify that the function V in Proposition 3.2 is indeed the optimal value of the attacker, we apply Ito’s
formula, conditioned on θ = 1: For q0 ∈ [0, p],

)t≥0 be the solution of SDE (2.3) and τ (∆)

p = inf{t ≥ 0 : q(∆)

and τ (∆)

t ≥ p}.

p

t

t

e−r(t∧τ (∆)

p

)V (q(∆)

t∧τ (∆)
p

) +

(cid:90) t∧τ (∆)

p

0

e−rs∆sds

= V (q0) +

(cid:90) t∧τ (∆)

p

e−rs(cid:16)

0

− rV (q) − q2(1−q)α(q)2V (cid:48)(q)

σ2

+ q2(1−q)2α(q)2V (cid:48)(cid:48)(q)
2σ2

+ (cid:0) q(1−q)α(q)V (cid:48)(q)

σ2

+ 1(cid:1)∆s

(cid:17)(cid:12)
(cid:12)
(cid:12)q=q(∆)

s

ds +

(cid:90) t∧τ ∆

p

0

e−rs q(1−q)α(q)V (cid:48)(q)

σ

(cid:12)
(cid:12)
(cid:12)q=q(∆)

s

dWs

(4.8)

≤ V (q0) +

(cid:90) t∧τ ∆

p

0

e−rs q(1−q)α(q)V (cid:48)(q)

σ

(cid:12)
(cid:12)
(cid:12)q=q(∆)

s

dWs,

where the inequality above is due to (3.4). Indeed, V in Proposition 3.2 satisﬁes (3.7), and (3.7) implies
(3.4). Since q(1−q)α(q)V (cid:48)(q)
martingale (with respect to (F Y

is bounded on q ∈ [0, p), the stochastic integral term in (4.8) is a square-integrable
t )t≥0) and has mean zero. Since the maximum is achieved at ∆ = α(q) in

σ

6See, for example, [14] Theorem 3.22 in Chapter 1.

13

(3.4), the inequality (4.8) implies

(cid:104)
e−r(t∧τ (∆)
E

p

)V (q(∆)

t∧τ (∆)
p

) +

(cid:90) t∧τ (∆)

p

0

e−rs∆sds

(cid:12)
(cid:105)
(cid:12)
(cid:12) θ = 1

(cid:104)
e−r(t∧τ (α)
≤ V (q0) = E

p

)V (q(α)

t∧τ (α)
p

) +

(cid:90) t∧τ (α)

p

0

e−rsα(q(α)

s

)ds

(cid:12)
(cid:105)
(cid:12)
,
(cid:12) θ = 1

where we denote q(α)
p
(4.9), we let t → ∞, and the boundedness of V produces

and τ (α)

t

as the suspicion level process and the stopping time with ∆t = α(q(α)

t

(4.9)

). In

(cid:104)
e−rτ (∆)
E

p V (qτ (∆)

p

(cid:90) τ (∆)

p

) +

0

e−rs∆sds

(cid:12)
(cid:105)
(cid:12)
(cid:12) θ = 1

(cid:104)
e−rτ (α)
≤ V (q0) = E

p V (q(α)
τp

) +

(cid:90) τ (α)

p

0

e−rsα(q(α)

s

)ds

(cid:12)
(cid:105)
(cid:12)
.
(cid:12) θ = 1

(4.10)

V (p) = 0 implies that e−rτ (∆)

p V (qτ (∆)

p

) = 0. Therefore, (4.10) implies

(cid:104) (cid:90) τ (∆)

p

E

0

e−rs∆sds

(cid:12)
(cid:105)
(cid:12)
(cid:12) θ = 1

≤ V (q0) = E

(cid:104) (cid:90) τ (α)

p

0

e−rsα(q(α)

s

)ds

(cid:12)
(cid:105)
(cid:12)
.
(cid:12) θ = 1

(4.11)

Finally, we conclude the optimality of α in (2.8) by (3.1) and (4.11).

The following proposition shows that in equilibrium, S (the set of suspicion levels at which the defender

stops the game) should have the form of S = [p, 1] for a constant p > 0.

Proposition 4.2. Suppose that ((qt)t≥0, S, α) is a Markov equilibrium. Then there exists a constant p ∈ (0, 1]
such that S = [p, 1].

Proof. We can easily see that if q0 = 0 (q0 = 1), then τ ≡ ∞ (τ ≡ 0) is the defender’s optimal stopping
time. This implies that 0 /∈ S and 1 ∈ S. Due to this observation and the closedness of S, to prove the

proposition, it is enough to show that the set S is connected. We prove it by contradiction. Suppose that

there exist constants 0 < p < ¯p < 1 such that p, ¯p ∈ S and (p, ¯p) ∩ S = ∅. As in Subsection 3.1, we derive

the diﬀerential equation and the variational inequality for the value functions of the attacker and defender:

If V (cid:48)(q) ≥ − σ2

M (1−q)q , then

If V (cid:48)(q) < − σ2

M (1−q)q , then








V (cid:48)(cid:48)(q)

2 + V (cid:48)(q)

q − rσ2V (q)

M 2q2(1−q)2 +

σ2

M q2(1−q)2 = 0,

α(q) = M,
−rU (q) + q2(1−q)2α(q)2U (cid:48)(cid:48)(q)

2σ2

+ q α(q) = 0.

σ2 V (cid:48)(q)2 V (q) = 0,

V (cid:48)(cid:48)(q)

2 − V (cid:48)(q)

1−q − r
σ2
α(q) = −
q(1−q)V (cid:48)(q) ,
−rU (q) + q2(1−q)2α(q)2U (cid:48)(cid:48)(q)

2σ2

+ q α(q) = 0.

(4.12)

(4.13)

14

with the boundary conditions

V (p) = V (¯p) = 0,

U (p) = l(1 − p), U (¯p) = l(1 − ¯p),

U (cid:48)(p) = U (cid:48)(¯p) = −l,

U (q) ≤ l(1 − q) for q ∈ (p, ¯p),

U (q) ≤ M

r q, V (q) ≥ 0 for q ∈ (p, ¯p),

(4.14)

(4.15)

(4.16)

(4.17)

(4.18)

where the condition (4.16) is from the smooth-ﬁt condition, and (4.18) is from the form of the optimization

problems in Deﬁnition 2.1.

Now our goal is to show that there is no solution to the above system (4.12)-(4.18). Suppose that there

exist V, α, U satisfying (4.12)-(4.18). Then, the following four steps lead us to a contradiction.

Step 1: The set {q ∈ (p, ¯p) : V (cid:48)(q) ≥ − σ2
proof of Step 1. Suppose that the set is not connected. Then, there exist constants p1 and p2 such that

M (1−q)q } is a connected set.

p < p1 < p2 < ¯p and

V (cid:48)(q) < − σ2

M (1−q)q for q ∈ (p1, p2),

V (cid:48)(p1) = −

σ2
M (1−p1)p1

,

V (cid:48)(p2) = −

σ2
M (1−p2)p2

.

(4.19)

The equalities in (4.19) and the diﬀerential equation for V in (4.13) produce

V (cid:48)(cid:48)(p1) = − 2σ2(M p1−rV (p1))

M 2(1−p1)2p2
1

,

V (cid:48)(cid:48)(p2) = − 2σ2(M p2−rV (p2))

M 2(1−p2)2p2
2

.

The equalities and inequality in (4.19) imply that

d
dq

(cid:0)V (cid:48)(q) + σ2

M (1−q)q

(cid:1)(cid:12)
(cid:12)q=p1

≤ 0,

d
dq

(cid:0)V (cid:48)(q) + σ2

M (1−q)q

(cid:1)(cid:12)
(cid:12)q=p2

≥ 0.

(4.20)

(4.21)

Combining (4.20) and (4.21), we obtain − σ2(M −2rV (p1))
M 2(1−p1)2p2
1
imply V (p1) ≥ M

2r ≥ V (p2), but this contradicts to V (cid:48)(q) < − σ2

≤ 0 and − σ2(M −2rV (p2))
M 2(1−p2)2p2
2
M (1−q)q < 0 for q ∈ (p1, p2).

≥ 0. These inequalities

Step 2: There exists a constant p∗ ∈ (p, ¯p) such that






V (cid:48)(q) ≥ − σ2
V (cid:48)(q) < − σ2

M (1−q)q ,
M (1−q)q ,

q ∈ [p, p∗]

q ∈ (p∗, ¯p]

.

(4.22)

proof of Step 2. We ﬁrst show that V (cid:48)(q) ≥ − σ2
1 implies that V (cid:48)(q) < − σ2
in (4.13) with the boundary condition V (p) = 0 in (4.14) is V (q) = σ√
we reach a contradiction: V ≥ 0 implies c1 ≥ 0, but

M (1−q)q for q ≥ p close enough to p. Suppose not. Then, Step
M (1−q)q for q > p close enough to p. The solution of the diﬀerential equation for V
q−p
1−q ) for a constant c1. Then

r ϕ−1(c1 ·

√

c1(1−p)σ
2

r(1−q)2 eϕ−1(c1·

√

r

q−p

1−q )2

= V (cid:48)(q) < − σ2

M (1−q)q < 0

implies c1 < 0. Therefore, we conclude that V (cid:48)(q) ≥ − σ2

M (1−q)q for q ≥ p close enough to p

Suppose that V (cid:48)(q) ≥ − σ2

M (1−q)q for q ∈ [p, ¯p]. Then the diﬀerential equation for U in (4.12) and the
inequality (4.18) imply that U (cid:48)(cid:48)(q) ≤ 0 for q ∈ [p, ¯p]. Since U is concave, (4.15) and (4.17) imply that

15

U (q) = l(1 − q) for q ∈ [p, ¯p], which does not satisﬁes the diﬀerential equation for U in (4.12). Therefore, we
conclude that there exists p∗ ∈ (p, ¯p) such that V (cid:48)(q) ≥ − σ2
M (1−q)q for
q > p∗ close enough to p∗. Furthermore, Step 1 implies that V (cid:48)(q) < − σ2

M (1−q)q for q ∈ [p, p∗], and V (cid:48)(q) < − σ2

M (1−q)q for q ∈ (p∗, p].

Step 3: U (cid:48)(cid:48)(q) ≤ 0 for q ∈ (p, p∗) and U (cid:48)(cid:48)(q) < 0 for q ∈ [p∗, ¯p).

proof of Step 3. We ﬁrst observe that (4.15) and (4.18) imply

rl(1 − p∗) − M p∗ < rl(1 − p) − M p ≤ 0.

We rewrite the diﬀerential equation for U in (4.12) and (4.13) as

U (cid:48)(cid:48)(q) = 2σ2(rU (q)−qα(q))

(1−q)2q2α(q)2

.

(4.23)

(4.24)

Therefore, it is enough to check that rU (q) − qα(q) ≤ 0 for q ∈ (p, p∗) and rU (q) − qα(q) < 0 for q ∈ [p∗, ¯p).
(i) For q ∈ (p, p∗), by the result in Step 2 and (4.12) and (4.18), we have rU (q) − qα(q) ≤ 0.
(ii) For q ∈ [p∗, ¯p), by the result in Step 2 and (4.13) and (4.15), we have rU (q) − qα(q) ≤ rl(1 − q) − qα(q).
To prove rl(1 − q) − qα(q) < 0 for q ∈ [p∗, ¯p), it is enough to show that the function rl(1−q)−qα(q)
decreases in
q on (p∗, ¯p), because rl(1 − p∗) − p∗α(p∗) < 0 by (4.23). Indeed, for q ∈ (p∗, ¯p), the solution of the diﬀerential

1−q

equation for V in (4.13) with (4.14) is the form of V (q) =

σϕ−1(c2· ¯p−q
1−q )
√

r

for a constant c2 > 0, so using this,

we obtain

(cid:0) rl(1−q)−qα(q)
1−q

(cid:1)(cid:48)

= −

√

2

rσϕ−1(c2· ¯p−q
1−q )
(1−q)2

< 0

for q ∈ (p∗, ¯p).

Step 4: The system (4.12)-(4.18) does not have a solution.

proof of Step 4. In Step 3, we concluded that if there exists a solution to the system (4.12)-(4.18), then U

should be a concave function on [p, ¯p]. The conditions (4.15) and (4.17), together with the concavity of U ,
imply that U (q) = l(1 − q) for q ∈ [p, ¯p]. This contradicts the result in Step 3: U (cid:48)(cid:48)(q) < 0 for q ∈ [p∗, ¯p).

5 Estimation of the Initial Suspicion Level q0

In this section, we present empirical and theoretical estimations regarding the initial suspicion level q0,
interpreting it as the fraction of infected users among a whole population of users whose traﬃcs are observed

by the defender. Estimation of the initial suspicion level is essential in several industries related to cyber-

security. For instance, an actuary may have to estimate how many users will be infected in the future
when designing a cyber-insurance contract. However, estimation of q0 using actual data may not be reliable
due to insuﬃcient amount of actuarial data [12]. In this case, our proposed methods can complement the

estimation.

5.1 Empirical Estimation of q0

In this subsection, we propose a method of empirically estimating the actual fraction of infected users using

data that contain whether a user is blocked by the defender before random termination.

We assume that the defender’s perception q0 might be diﬀerent from the actual fraction x. To be speciﬁc,

we consider a situation that

Pdef ender(θ = 1) = q0 (cid:54)= Ptrue(θ = 1) = x,

(5.1)

16

where Pdef ender is the probability measure describing the defender’s belief and Ptrue is the actual probability
measure. The proposition below provides an unbiased estimator of Ptrue(θ = 1) = x in terms of the ratio of
the blocked suspects when the defender has the belief Pdef ender(θ = 1) = q0.

Proposition 5.1. Suppose that there are N ∈ N suspects. For suspect i ∈ {1, 2, . . . , N }, θ(i) indicates
whether suspect i is an attacker or not, (W (i)
)t≥0 represents the noise in (2.1), and T (i) is the random
termination time in (2.4). We assume that these random variables and Brownian motions are all independent
under Pdef ender and Ptrue, and

t

Ptrue(θ(i) = 1) = x and Pdef ender(θ(i) = 1) = q0 ∈ (0, p),

1 ≤ i ≤ N.

(5.2)

Each suspect and the defender play the game under the probability measure Pdef ender, and we denote by τ (i)
p
the ﬁrst hitting time for suspect i. Then, µθ deﬁned below is an unbiased estimator of x:

µθ :=

p(1 − q0)
(p − q0)u(q0)

·

1
N

N
(cid:88)

i=1

1(cid:110)

p <T (i)(cid:111) −
τ (i)

q0(1 − p)
p − q0

,

where

u(q0) =





(cid:1)a

(cid:0) q0(1−p)
p(1−q0)
√
√

acM
2σ

π
r

,
(cid:0) q0(1−q∗)
q∗(1−q0)

(cid:1)a

p(1−q0)

q0(1−p) e−y(q0)2

− c

,
√

if

if

if

rσ2
M 2 ≥ 1
rσ2
M 2 < 1 & q0 ∈ (0, q∗]
rσ2
M 2 < 1 & q0 ∈ (q∗, p)

π y(q0),

(5.3)

(5.4)

and the constants a, c, and q∗ and the function y are deﬁned in (3.14).

Proof. By straightforward computations, we check that u in (5.4) satisﬁes u ∈ C 2([0, p)) and solves the
following diﬀerential equation:




−ru(q) + q2(1−q)2α(q)2

2σ2

u(cid:48)(cid:48)(q) + q(1−q)2α(q)2

σ2

u(cid:48)(q) = 0



u(0) = 0,

u(p) = 1.

(5.5)

Conditioned on θ(i) = 1 and the initial suspicion level q0, Ito’s formula and (5.5) produce the following:

e−r(t∧τ (i)

p )u(qt∧τ (i)

p

) = u(q0) +

(cid:90) t∧τ (i)

p

0

e−rs qs(1−qs)α(qs)

σ

u(cid:48)(qs)dW (i)
s .

(5.6)

In (5.6), the stochastic integral is a square integrable martingale since the integrand is bounded. Therefore,

u(q0) = lim
t→∞
= Edef ender

= Pdef ender

(cid:104)

e−r(t∧τ (i)

Edef ender
(cid:104)
e−rτ (i)
· 1{τ (i)
(cid:16)
p < T (i) (cid:12)
τ (i)

p

p <∞}
(cid:17)
(cid:12) θ(i) = 1

,

(cid:105)
) (cid:12)
p )u(qt∧τ (i)
(cid:12) θ(i) = 1
(cid:105)
(cid:12)
(cid:12) θ(i) = 1

p

(5.7)

where the second equality is due to the dominated convergence theorem and u(p) = 1, and the third equality
holds since T (i) is exponentially distributed and independent of other random variables. By the same way,

17

we also obtain the expression of Pdef ender(τ (i)

p < T (i) | θ = 0), and the result is summarized below:

Pdef ender(τ (i)
Pdef ender(τ (i)

p < T (i) | θ = 1) = u(q0),
p < T (i) | θ = 0) = q0(1−p)

p(1−q0) u(q0).

Since Ptrue(θ(i) = 1) = x, the probability of user i being blocked before the random termination is

Ptrue(τ (i)

p < T (i)) = Ptrue(τ (i)

p < T (i) | θ(i) = 1) · x + Ptrue(τ (i)

p < T (i) | θ(i) = 0) · (1 − x)

= Pdef ender(τ (i)

p < T (i) | θ(i) = 1) · x + Pdef ender(τ (i)

p < T (i) | θ(i) = 0) · (1 − x),

(5.8)

(5.9)

where the second equality is due to the observation that Ptrue = Pdef ender once the value of θ(i) is realized.

Finally, we combine (5.8) and (5.9) to conclude that Etrue[µθ] = x.

The empirical estimation method presented in Proposition 5.1 does not require inspection results. That

is, the data set does not need to indicate whether a blocked suspect is actually an attacker or an innocent

user. This method is useful in the sense that a defender can adjust the probability of cyber-attacks before

inspection results come out.

We can actually relax the assumption that all suspects and the defender share the same suspicion level
q0 at the beginning of the game. Even if suspect i is an attacker with probability q(i)
0 , one can still utilize
the expression in (3.23). The defender chooses a certain q0, and construct observation data starting from
q0. In this case, the estimator µθ in Proposition 5.1 is the ratio of infected users to the whole population of
users.

5.2 Theoretical Estimation of q0

We move onto a theoretical estimation on the probability of cyber-attacks. So far, we have assumed that
the initial suspicion level q0, which can be interpreted as the probability of cyber-attacks, is taken as given.
However, we can imagine cases in which the attacker chooses the attack probability q0 as in [6, 21]. With the
probability q0, the attacker actually launches cyber-attacks, whereas with the complementary probability
1 − q0, the attacker leaves the game. For example, one may imagine a bot herder, a malicious hacker who
controls a botnet (many bot-infected devices) to attack a target and chooses the proportion of active bots.
In this case, q0 represents the proportion of bots that actually launch attacks.

Suppose that the attacker chooses to attack with the probability q0 ∈ [0, 1]. When the attacker actually

attacks, her expected proﬁt V (q0) in equilibrium is

V (q0) = E

(cid:34)(cid:90) T ∧τp

0

(cid:35)
(cid:12)
(cid:12)
α(qt)dt
(cid:12)θ = 1

.

Then, q0V (q0) is the attacker’s expected proﬁt when she chooses to attack with the probability q0. Therefore,
the optimal attack probability ˆq,

ˆq ∈ arg max
q0∈[0,1]

q0V (q0),

(5.10)

can be a reasonable theoretical estimation of q0 when we interpret q0 as the attack probability. The propo-
sition below characterizes the optimal attack probability.

18

Proposition 5.2. There exists a unique optimal attack probability ˆq ∈ (0, p) in (5.10).

Proof. We ﬁrst observe the sign of ∂2
∂q2

(cid:0)qV (q)(cid:1), whose expression is





(cid:0) M
r

− a(1+a)
(1−q)2q
− (1+a)σ2
√
σ(1−p)c

M (1−q)2q ( 1−q∗

(cid:1) ( 1−p
q∗ )a( q

1−q )a,

p )a( q
1−q )a,
2p(1−q)e−y(q)2
2p2(1−q)4

√

r

πe2y(q)2 (cid:16)

−

√

−c

π(1−p)qy(q)

(cid:17)

if rσ2
if rσ2

M 2 ≥ 1 and q ∈ [0, p)
M 2 < 1 and q ∈ [0, q∗]

.

,

if rσ2

M 2 < 1 and q ∈ (q∗, p)

(5.11)

The inequalities in Lemma 3.1 ensure that the above three expressions are all strictly negative. Therefore,

the map q (cid:55)→ qV (q) on q ∈ [0, p] is strictly concave, and we conclude that there exists unique ˆq such that

ˆq = arg max

qV (q).

q∈[0,p]

Since qV (q) = 0 for q ∈ [p, 1] and qV (q) > 0 for q ∈ (0, p), we conclude that ˆq above also satisﬁes (5.10).

In many studies, expected losses due to cyber-attacks are taken as given [12, 21]. However, the expected

losses are related to the probability of cyber-attacks in dynamic environments. This is because if the

probability of cyber-attacks is lower, the attacker can gain more due to longer duration of the game. Our

model suggests a relationship between the attack probability and the expected losses. Given the optimal

attack probability ˆq, the expected losses would be V (ˆq).

6 Calibration of Model Parameters

We calibrate the model parameters based on the report conducted by Ponemon Institute and sponsored and
published in 2020 by IBM Security.7 The report collected the information on data breach from more than
500 organizations. To name a few items in the report, there are components of costs, detection times by

industry and by nation, and root causes of data breaches. In the report, a main reason for data breach

falls into one of the three categories: system glitches, human errors, and malicious attacks. Among these

categories, 50% of data breaches are due to malicious attacks, 13% of which are carried out by nation state

attackers. Even though our model is inspired by APTs, our model is intended to capture long-term data

breach including some characteristics of APTs. For the calibration purpose, we use the aggregate data as

the report does not provide detailed information on single data breaches. Nevertheless, we believe that the

report would ﬁt into our model to some extent.

Before we describe our calibration, it would be worth discussing an issue on discrepancy between our

model and the report. For instance, let us consider security automation deployment in the report. It refers

to enabling augment or replace human intervention in the identiﬁcation and containment of cyber exploits

or breaches. According to the report, average security automation deployment by industry ranges from

49% to 68%. This means that a certain fraction of industry does not deploy security automation at all. No

deployment of security automation can be a reason for a longer average detection time because ﬁrms without

security automation deployment possibly detect data breaches at later times. The report does not provide

details of data composition and the defender in our model can be thought of as a security automation system,

7https://www.ibm.com/security/digital-assets/cost-data-breach-report. This report is referred to as “the report” through-

out this section.

19

which means that our calibration is likely to result in inaccurate values to some extent. It would be a future
project to develop a model that can handle the discrepancy issues.8

There are ﬁve model parameters to calibrate:

l, M, q0, r and σ. We substitute the false alarm cost l
with the lost business cost of $1.52M in the report. Lost business includes business disruption, opportunity
cost during system downtimes, lost costumers, and reputation losses. These events can happen when the
defender falsely shuts down servers, and thus the lost business cost is used as the false alarm cost. For the

upper bound of the attack intensity M , we set a suﬃciently large number M = 100 as it has little impacts
on equilibrium.9 For the initial attack probability q0, we use the optimal attack probability ˆq in equation
(5.10), which is a function of the remaining two undetermined parameters r and σ.

Among information contained in the report, we focus on the average cost of a data breach and average
detection time, by industry. We set V (ˆq) as the average cost of a data breach, and E[τp ∧ T |θ = 1, q0 = ˆq] as
the average detection time. The expression of the function V is given in Proposition 3.2, and the expression

in equation (5.7) provides

E[τp ∧ T |θ = 1, q0 = ˆq] = E

(cid:20)(cid:90) τp

0
1 − u(ˆq)
r

,

=

(cid:21)
(cid:12)
e−rtdt
(cid:12)
(cid:12)θ = 1, q0 = ˆq

=

1 − E[e−rτp |θ = 1, q0 = ˆq]
r

where the function u is deﬁned in equation (5.4). We calibrate r and σ to match the values of V (ˆq) and
E[τp ∧ T |θ = 1, q0 = ˆq].

Table 1 presents the calibrated parameters of the model for 17 industries used in the report and provides

basic building blocks for discussions about the calibration of the model parameters.

For discussion of calibrated r, we would like to extend the meaning of r. As mentioned in Section 2, r

represents how often random termination occurs. However, it is worth mentioning that r can represent the

overall time preference. That is, r can also represent how fast values of data decay over time or foregone

values of other alternatives. For instance, a large value of r can imply a high frequency of detecting cyber-

attacks (or equivalently, a high risk of the attacker being detected), a fast decay of current data value, and

a large proﬁts of the attacker doing other things rather than cyber-attacks. For this reason, we refer to r as

time preference factor in this section.

According to our calibration, two industries with the highest time preference factors are ﬁnancial in-

dustry and research industry. Intuitively, one can think of high time preference factors in these industries.

For research industry, frontier researches and state-of-the-art technologies are rapidly devalued over time.

For ﬁnancial industry, personally identiﬁable information of important customers would depreciate quickly,

compared to other industries. In addition, hackers would take larger risks in ﬁnance and research industries

as information and data are protected by better systems in these industries.

Three industries with the lowest time preference factors are healthcare industry, entertainment industry,

and manufacturing industry. Information on patients in healthcare industry, consumers in manufacturing

8One might be wondering why the report is chosen. A main reason is because the report deals with data breaches in the
long run as our model deals with persistent data leakage over time. Another good candidate set of data for calibration would
be the PRC database. Although the PRC database provides information on types of data breaches, its composition may not
be perfectly suited to our setting. According to the PRC database, the most frequent type of data breach is “PHYS”, paper
documents that are lost or stolen, accounting for almost 1,400 incidents. The second most frequent type of data breach is
“DISC”, unintended disclosure that is not involved with hacking and intentional breaches, accounting for slightly more than
1,000 incidents. “HACK” type, which means ‘hacked by an outside party or infected by malware’, is the third most frequent
type of data breach and accounts for around 900 incidents. It seems that the two most frequent types PHYS and DISC are
possibly related to one-shot events.

9See the graph of α for varying M in Figure 3 and the graph of p as a function of M in Figure 4.

20

Industry
Healthcare
Energy
Financial
Pharmaceuticals
Technology
Manufacturing
Services
Entertainment
Education
Transportation
Communication
Consumer
Retail
Hospitality
Media
Research
Public
Global average

Average cost Average detection time

$7.13M
$6.39M
$5.85M
$5.06M
$5.04M
$4.99M
$4.23M
$4.08M
$3.90M
$3.58M
$3.01M
$2.59M
$2.01M
$1.72M
$1.65M
$1.53M
$1.08M
$3.86M

329 days
254 days
233 days
257 days
246 days
302 days
286 days
314 days
283 days
275 days
251 days
307 days
311 days
275 days
281 days
244 days
324 days
280 days

r
0.32
0.42
0.46
0.42
0.44
0.35
0.38
0.35
0.39
0.40
0.44
0.36
0.37
0.43
0.42
0.49
0.39
0.39

σ
7.1
7.3
6.9
5.7
5.8
5.1
4.4
4.1
4.1
3.8
3.3
2.5
1.9
1.7
1.6
1.6
0.9
4.1

α(ˆq)/σ
1.09
1.26
1.32
1.26
1.29
1.16
1.21
1.16
1.23
1.25
1.33
1.21
1.25
1.36
1.35
1.47
1.36
1.23

Table 1: Calibration of Model Parameters for 17 industries

and entertainment industries is important, and values of information may not change quickly over time.

Therefore, one can anticipate low time preference factors in those industries.

We close this section by discussing the noise intensity σ and the informativeness α(ˆq)
σ

of the signal

process. The noise intensity represents the volatility of normal activities, and Table 1 shows that industry

with the lowest noise intensity is the public sector, as one may expect. Financial, healthcare, and energy

industries have the highest noise intensity. This would be because ﬁnancial, healthcare, and energy services

are necessary in daily lives and thus there are many transactions and emergency cases in those industries. For
α(ˆq)
σ , we ﬁnd that it is rather stable across all industries. This would be a model property as we conjecture

that there is an optimal range of α

σ for the attacker.

7 Numerical Illustrations

In this section, we graphically illustrate the results in the previous sections for varying parameters. Based

on the discussions in Section 6 and the “Global average” row in Table 1, we set the model parameters as

M = 100,

l = 1.52,

r = 0.39,

σ = 4.1.

(7.1)

For these parameters, the equilibrium threshold is p = 0.29 and the optimal attack probability is ˆq = 0.14.
The left graph in Figure 2 presents a sample path of the noise (Wt)t≥0, and the right graph presents the
corresponding paths of the suspicion level process (qt)t≥0, when θ = 1 (solid line) and θ = 0 (dashed line),
respectively. As the deﬁnition of qt and the SDE (2.3) indicate, for a given sample path of W , we observe
that qt for θ = 1 case is always higher than qt for θ = 0 case. Therefore, the defender blocks the suspect
earlier (qt hits the stopping threshold p earlier) when the suspect is the attacker, as the ﬁgure shows.

The equilibrium attack intensity function α in (3.23) is illustrated in Figure 3. One can observe in the
ﬁgure that the attacker chooses the highest attack intensity M when the suspicion level qt is low enough.

21

Figure 2: The left graph is a sample path of a standard Brownian motion. The right graph is the corresponding paths of the
suspicion level process when θ = 1 (solid line) and θ = 0, with parameters q0 = 0.14, M = 100, σ = 4.1, r = 0.39, l = 1.52. In
this case, the equilibrium stopping threshold p is 0.29.

The attack intensity function decreases as the suspicion level qt increases. The intuition behind this is the
following. As the suspicion level increases, the duration of the game decreases. The attacker decreases the
attack intensity to slow down the increase of qt (see the form of the SDE (2.3)) and induces the defender
to stop the game later. This strategic behavior of our attacker can be interpreted as reducing the current

proﬁt to extend the duration of the game.

The comparative statics in Proposition 3.6 is demonstrated in Figures 3 and 4. For the parameters we

chosen, the graphs show that the equilibrium stopping threshold p increases in l and r and decreases in σ

and M , and the equilibrium attack intensity α increases in l, r, σ and M .

Figure 5 is a simulation result illustrating the empirical estimator µθ in Proposition 5.1. For given
x = 0.14, we generate N = 50, 000 sample paths of standard Brownian motion, N realizations of the random

termination time T (from exponential distribution in (2.4)), and N realizations of the suspect types (from
Bernoulli distribution with parameter x = 0.14). For these N realizations, we pick q0 = 0.1 and calculate
the suspicion level processes qt starting at q0 = 0.1. If a suspicion level process ceases due to the random
termination time or hits the equilibrium threshold p, then the suspicion level process is frozen. In these N
samples, we count the number of cases that qt hits p before the random termination. Then we use (5.3)
to infer the actual initial suspicion level. The left graph describes the ratio of the blocked suspects, as a

function of t:

BR(t) :=

1
N

N
(cid:88)

i=1

1(cid:110)

τ (i)
p <T (i)∧t

(cid:111),

The right graph is the corresponding empirical estimation, as a function of t:

EE(t) :=

p(1 − q0)
(p − q0)u(q0)

·

1
N

N
(cid:88)

i=1

1(cid:110)

τ (i)
p <T (i)∧t

(cid:111) −

q0(1 − p)
p − q0

,

(7.2)

(7.3)

In Figure 5, we can see that the empirical estimation EE(t) approaches to the true value x = 0.14 as time

goes on.

Lastly, Figure 6 graphically illustrates Proposition 5.2. As we check in the proof of the proposition, the

map q (cid:55)→ qV (q) is strictly concave on [0, p]. The unique maximizer ˆq in (5.10) is marked in the ﬁgure.

22

0.20.40.60.81.0t-2-112Wt0.00.20.40.60.81.0t0.10.20.30.4qtσ = 2 (—), σ = 4 (- -),
σ = 6 (-·-), σ = 8, (-··-),
M = 100, r = 0.39, l = 1.52

r = 0.2 (—), r = 0.4 (- -),
r = 0.6 (-·-), r = 0.8 (-··-),
M = 100, σ = 4.1, l = 1.52

l = 0.5 (—), l = 1.0 (- -),
l = 1.5 (-·-), l = 2.0 (-··-),
M = 100, r = 0.39, σ = 4.1

M = 5 (—), M = 10 (- -),
M = 20 (-·-), M = 100 (-··-),
r = 0.39, σ = 4.1, l = 1.52

Figure 3: Graphs of equilibrium attack intensities for varying l, r, σ, and M .

8 Concluding Remark

In the cyber-security context, we develop a reputation game model between a suspect and a defender, and

fully analyze the equilibrium interaction between them. As far as we know, our game model is the ﬁrst

to include the optimal termination of the game with asymmetric information, imperfect monitoring, and

continuous-time Bayesian updates. Using the game model, we provide an empirical and theoretical methods
of estimating the initial suspicion level.

As a future research, we plan to generalize our cyber-security game model by incorporating time-

dependent noise size (periodic patterns of noise) and multidimensional signal processes (traﬃcs from multiple

channels).

Acknowledgement

This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea

government (MSIT) (No. 2020R1C1C1A01014142, No. 2021R1I1A1A01050679, and No. 2021R1A4A1032924).

References

[1] Axel Anderson and Lones Smith. Dynamic deception. The American Economic Review, 103(7):2811–

2847, 2013.

[2] Kerry Back and Shmuel Baruch. Information in securities markets: Kyle meets Glosten and Milgrom.

Econometrica, 72(2):433–465, 2004.

23

0.00.10.20.30.40.5q5101520α0.00.10.20.30.40.5q5101520α0.00.10.20.30.40.5q5101520α0.00.10.20.30.40.5q5101520αM = 100, r = 0.39, l = 1.52

M = 100, r = 0.39, σ = 4.1

M = 100, σ = 4.1, l = 1.52

r = 0.39, σ = 4.1, l = 1.52

Figure 4: The equilibrium threshold p for varying l, r, σ, and M .

Figure 5: The left graph is ratio of the blocked suspects as time goes (see (7.2) for BR(t)) with N = 50, 000 simulations where
the solid red line is the desired ratio computed by (5.3). The right graph is the corresponding empirical estimation of x (see
(7.3) for EE(t)), where the solid red line is the true value of x = 0.14. The parameters are M = 100, r = 0.39, σ = 4.1, l = 1.52,
and the solution qt of (2.2) is computed with the initial prior q0 = 0.1. In each graph, the dashed line and the dotted line
correspond to 95% and 99% conﬁdence intervals, respectively.

[3] Ren´e Caldentey and Ennio Stacchetti. Insider trading with a random deadline. Econometrica, 78(1):245–

283, 2010.

[4] Luciano Campi and Umut Cetin. Insider trading in an equilibrium model with default: a passage from

reduced-form to structural modelling. Finance and Stochastics, 11(4):591–602, 2007.

[5] Umut C¸ etin. Financial equilibrium with asymmetric information and random horizon. Finance and

Stochastics, 22(1):97–126, 2018.

[6] Lin Chen and Jean Leneutre. A game theoretical framework on intrusion detection in heterogeneous

networks. IEEE transaction on information forensics and security, 4(2), June 2009.

[7] Jin Hyuk Choi and Kasper Larsen. Taylor approximation of incomplete Radner equilibrium models.

Finance and Stochastics, 19(3):653–679, 2015.

[8] Jin Hyuk Choi, Kasper Larsen, and Duane J. Seppi. Information and trading targets in a dynamic

market equilibrium. Journal of Financial Economics, 132(3):22 – 49, 2019.

24

012345σ0.20.40.60.81.0p0.00.51.01.52.0l0.20.40.60.81.0p0.00.10.20.30.40.5r0.20.40.60.81.0p020406080100M0.20.40.60.81.0p0123456t0.190.200.210.220.230.240.25BR(t)01234560.060.080.100.120.140.16Figure 6: Graph of qV (q) as a function of q. Parameters are M = 100, r = 0.39, σ = 4.1 and l = 1.52. The unique maximum
is obtained at ˆq = 0.14.

[9] Pierre Collin-Dufresne and Vyacheslav Fos. Insider trading, stochastic liquidity, and equilibrium prices.

Econometrica, 84(4):1441–1475, July 2016.

[10] Thomas F. Cooley. Calibrated models. Oxford Review of Economic Policy, 13(3):55–69, 1997.

[11] Martin W. Cripps, George J. Mailath, and Larry Samuelson. Imperfect monitoring and impermanent

reputations. Econometrica, 72(2):407–432, March 2004.

[12] Wanchun Dow, Wenda Tang, Xiaotong Wu, Lianyoung Qi, Xiaolong Xu, Xuyun Zhang, and Chunhua

Hu. An insurance theory based optimal cyber-insurance contract against moral hazard. Information

Sciences, 527:576–589, 2020.

[13] Eduardo Faingold and Yuliy Sannikov. Reputation in continuous-time games. Econometrica, 79(3):773–

876, May 2011.

[14] Ioannis Karatzas and Steven Shreve. Brownian Motion and Stochastic Calculus. Springer, 1998.

[15] David M. Kreps and Robert Wilson. Reputation and imperfect information. Journal of Economic

Theory, 27(2):253–279, August 1982.

[16] Albert S. Kyle. Continuous auctions and insider trading. Econometrica, pages 1315–1335, 1985.

[17] Robert Liptser and Albert Shiryaev. Statistics of Random Processes. Springer, 2001.

[18] Paul Milgrom and John Roberts. Predation, reputation, and entry deterrence. Journal of Economic

Theory, 27(2):280–312, August 1982.

[19] Kim Weston. Existence of a Radner equilibrium in a model with transaction costs. Mathematics and

Financial Economics, 12(4):517–539, 2018.

[20] Kim Weston and Gordan ˇZitkovi´c. An incomplete equilibrium with a stochastic annuity. Finance and

Stochastics, 24:359–382, 2020.

[21] Hao Wu, Wei Wang, Changyun Wen, and Zhengguo Li. Game theoretical security detection strategy

for networked systems. Information Sciences, 453:346–363, 2018.

[22] Gordan ˇZitkovi´c. An example of a stochastic equilibrium with incomplete markets. Finance and Stochas-

tics, 16(2):177–206, 2012.

25

0.20.40.60.81.0q-0.10.10.20.30.40.50.6qV(q)q