Detecting ‘Cyber-Related’ Discussions in 

Online Social Platforms 

Ikwu .E. Ruth1 

School of Physical Sciences and Design  

Department of Computer Sciences 

Brunel University London 

UB8 3PN, United Kingdom 

Panos Louvieris2 

panos.louvieris@brunel.ac.uk 

School of Physical Sciences and Design  

Department of Computer Sciences 

Brunel University London 

UB8 3PN, United Kingdom 

1 Cyber Security Researcher Brunel University London, ruth.ikwu@brunel.ac.uk 
2 Panos Louvieris is Professor of Information Systems and leads the Defence & Cyber Security (DCS) research 
group, panos.louvieris@brunel.ac.uk 

 
 
 
                                                           
Abstract 

As  the  use  of  social  platforms  continues  to  evolve,  in  areas  such  as  cyber-security  and 

defence, it has become imperative to develop adaptive methods for tracking, identifying and 

investigating  cyber-related  activities  on  these  platforms.  This  paper  introduces  a  new 

approach  for  detecting  “cyber-related”  discussions  in  online  social  platforms  using  a 

candidate set of terms that are representative of the cyber domain. The objective of this paper 

is  to  create  a  cyber  lexicon  with  cyber-related  terms  that  is  applicable  to  the  automatic 

detection  of  cyber  activities  across  various  online  platforms.  The  method  presented  in  this 

paper  applies  natural  language  processing  techniques  to  representative  data  from  multiple 

social  platform  types  such  as  Reddit,  Stack  overflow,  twitter  and  cyberwar  news  to  extract 

candidate  terms  for  a  generic  cyber  lexicon.  In  selecting  the  candidate  terms,  we  introduce 

the APMIS Aggregated Pointwise Mutual Information Score in comparison with the Term 

Frequency-Term  Degree  Ratio  (FDR  Score)  and  Term  Frequency-Inverse  Document 

Frequency Score (TF-IDF Score).  These scoring mechanisms are robust to account for term 

frequency,  term  relevance  and  mutual  dependence  between  terms.  Finally,  we  evaluate  the 

performance of the cyber lexicon by measuring its precision of in classifying discussions as 

'Cyber-Related' or 'Non-Cyber-Related'. 

This paper theoretically provides a methodology for creating custom cyber-related lexicons 

that capture specific domains of analytical interest. The results presented are most applicable 

to cyber threat monitoring and detection of malicious cyber activities in online social 

platforms. 

Keywords 

Please provide 7-10 key terms related to the topic of your chapter and clear, concise definitions (in 

your  own  words)  for  each  term.    Place  your  terms  and  definitions  after  the  references  section  of 

your chapter. 

 
 
Introduction 

The  evolution  of  the  World  Wide  Web  provides  new  methods  for  communication  in  the 

hyper-connected  world.  These  communications  include  sharing  of  texts,  images  and  videos 

between  geographically  distributed  cyber-personas.  Security  experts  usually  apply  manual 

techniques  for  monitoring  and  detecting  cyber-related  activities  on  social  platforms. 

However, given the ever-increasing volume of data generated on these social platforms, these 

manual techniques have become labour intensive, inefficient, and a significant challenge for 

security experts who hope to stay ahead. 

The  volume  of  text  data  currently  generated  daily  online  and  the  resultant  exponential 

increase  in  the  number  of  potential  cyber-related  activities  requires  robust  techniques  in 

analytics  to  identify  cyber  threats  for  early  mitigation.    One  such  technique  is  Natural 

Language  processing  of  text  data.    Natural  Language  Processing  (NLP)  is  rooted  in  many 

disciplines, such as computer science and computational linguistics. It is yet another branch 

of Artificial Intelligence (AI) which gives computers a better understanding of, interpretation 

of, as well as the ability to manipulate human language [1], [2]. The primary aim of NLP is a 

step towards bridging the communication gap between human and machine.  

Language  processing  techniques  that  support  cyber  situational  awareness  on  the  social 

dimension of cyberspace need more domain-specific lexicon than what is available today. For 

example,  several  studies  and  historical  events  have  demonstrated  the  ability  of  individuals 

and  groups  to  use  social  platforms  as  enablers  for  perpetrating  cyber-incidents  [3]–[5].  In 

July  2006,  the  hacker  group  anonymous  recruited  new  members,  planned,  organised  and 

facilitated the famous raid on Habbo Hotel using the 4chan microblogging platform [6], [7]. 

Similarly,  between  2008  and  2012  the  hacker  groups  Anonymous  and  its  spin-off  LulzSec 

used social chat rooms to recruit, train new hackers, plan and coordinate cyber-attacks with 

hashtag trends such as #Operation Payback [8], [9]. Furthermore, in popular and open social 

platforms like Twitter, the use of trending hashtags such as #OpLibtard, #OpISIS, #OpPedos, 

#OpIsreal  are  open  tags  for  tracking  discussions  related  to  certain  cyber-activities  being 

carried  out.  A  gap  exists  in  automatically  spotting  these  sort  of  conversations  on  these 

platforms, given the volume of data generated. 

This  paper  addresses  this  gap  by  providing  a  methodology  for  automatically  identifying 

cyber-related discussions in online social platforms. The goal is to create a simple method for 

monitoring  conversations  on  social  platforms  and  detecting  texts  that  are  related  to  cyber 

activities. We aim to develop a generic lexicon of candidate terms that capture the context of 

analytical  interest.  We  achieve  this  by  creating  a  representative  corpus  of  cyber-related 

discussions using data from popular cyber microblogging platforms such as Reddit, Twitter, 

stack  overflow,  hacker  news  and  cyberwar  news.  We  build  a  lexicon  of  cyber-related  or 

context-specific terms that are known to appear frequently with a higher degree of relevance 

to  the  context  of  analysis,  in  these  discussions.  We  rank  each  term  based  on  scoring 

mechanisms  that  emphasise  the  frequency  of  term  occurrence,  the  relevance  of  terms  in 

sentences  and  the  mutual  dependence  amongst  terms.  To  evaluate  the  performance  of  the 

lexicon, we apply it to a ‘cyber-related’ quantification task on a set of new labelled (Cyber-

related  and  Non-cyber-related)  discussions  from  a  collection  of  social  platforms.  A 

quantification  algorithm  estimates  the  degree  of  ‘cyber-relatedness’  of  random  text  and 

classifies each text into one of two classes – Cyber-related or Non-Cyber-Related -- based on 

an  optimally  selected  threshold.  Given  the  increased  cost  of  deploying  cyber  mitigation 

strategies and a low tolerance for false positives,  we focus on maximising the classification 

precision  of  the  lexicon.  We  compare  the  performance  of  each  scoring  mechanism  in 

maximising classification precision. The aggregated pointwise information score (APMIS)  

is seen to return a higher prediction precision alongside a lower false positive rate.  

The main contributions of this paper are of two folds; firstly, it presents a novel approach for 

building  a  cyber  domain-specific  lexicon  and  offers  a  curated  list  of  context-specific  terms 

that  are  representative  of  the  cyber  domain.  The  lexicon  contributes  to  already  existing 

methods for monitoring and detecting cyber-related activities on social platforms [10]–[12]. 

Mining Social Platforms for Cyber-Related Discussions 

The first challenge in building a lexicon in the context of the cyber domain is to generate a 

representative set of “cyber-related” discussions from these social platforms. Any strategy for 

gathering such information must include relevant texts from the appropriate social channels. 

Social  media  platforms  are  characterised  by  the  type  of  content  generated,  user  experience 

and policies that guide interactions on these platforms [13]. To accurately identify all kinds of 

sources  of  relevant  data,  we  define  three  types  of  social  platforms  based  on  the  level  of 

moderation of user-generated content [14].  

The categorisation of Social Platforms 

The level of moderation of user content is crucial as it defines the manner of conversations 

happening  on  a  social  platform.  We  observe  such  categorisation  from  the  degree  to  which 

users can freely express their views on these platforms (unhindered) and therefore, the extent 

to which they can use them for personal activities. 

  Pre-Moderated Social Platforms: Administrators of these platforms highly moderate 

user-generated  content  before  it  appears  online.  Pre-moderation  ensures  that  posts 

generated  by  users  but  deemed  “inappropriate”  by  site  administrators  never  make  it 

online. These sites do not allow users to create personalised topics for discussions but 

rather,  contribute  to  topic  categories  created  by  the  site  administrators.  Therefore, 

these  types  of  social  platforms  have  highly  regulated  topic  forums  where  users  can 

participate. Additionally, due to the need for highly regulated online communications 

on these platforms, discussions do not  occur in  real-time. While these platforms  are 

useful  for  controlling  inappropriate  use  and  stamping  out  cybercrimes,  such  rules 

have been known to lead to the death of online communities [14].  

  Post-Moderated  Social  Platforms:  Administrators  of  these  sites  are  minimally 

involved in the moderation of its content. As opposed to pre-moderated content, user-

generated  content  appears  online  immediately  after  posting  but  queued  for 

moderation.  This  level  of  moderation  allows  for  real-time  communication  in  online 

communities.  Eventually,  user-generated  content  deemed  inappropriate  by  site 

moderators  is  either  filtered,  hidden  or  deleted.  Post-moderation  filters  displayed 

posts  generated  by  users,  only  when  deemed  “inappropriate”  by  site  administrators. 

On these platforms, users are sometimes allowed to create their topics of discussion 

and contribute to topics created by other users. While these platforms offer flexibility 

as  opposed  to  pre-moderated  platforms,  they  are  at  risk  of  accommodating 

inappropriate content if response site moderators do not respond quickly. Examples of 

social  networks  within  these  categories  include  Stack  Overflow,  Reddit,  Stack 

Exchange, Quora [14]. 

  Reactively-Moderated  Social  Platforms:  Administrators  of  these  sites  are  rarely 

involved  in  the  moderation  of,  user-generated  content.  As  a  result,  content  appears 

online  immediately,  and  precisely  as  created.  Reactive  moderation  means  site 

moderators  rely  on  users  to  report  inappropriate  content  when  they  see  it.  User-

generated  content  is  therefore  only  checked  if  a  complaint  is  made  about  them  by 

other users. Reported posts are removed if deemed necessary by site moderators. This 

level  of  moderation  ensures  that  users  create  and  engage  in  topics  they  want.  Users 

with similar ideologies can belong to the same sub-group where they share thoughts 

and  beliefs  without  restrictions.    Users  are  usually  allowed  to  create  their  topics  of 

discussion, monitor and contribute to  topics created by other users. Therefore, these 

types  of  social  platforms  encourage  highly  effective  real-time  communication  with 

minimal  restrictions.  While  these  platforms  offer  flexibility  and  freedom,  they  are 

known  to  be  safe  havens  for  cybercriminals.  Examples  of  social  platforms  in  this 

category include Twitter, Facebook, Snap Chat, Tumblr, 4chan [14]. 

Identifying text in online platforms that are relevant to a specific topic of interest is usually 

done with a keyword-based approach [15], [16]. In a keyword-based approach, a set of terms 

are  used  to  filter  through  text,  and  only  texts  containing  any  word  in  the  set  of  terms  are 

returned [17]. However, with no prior assumptions of context-related keywords, it is useful to 

target forums where users are actively engaged in related topics of interest.   

On real-time microblogging platforms like Twitter and Facebook, a keyword-based approach 

may  be  appropriate  to  filter  tweets  on  specific  topics  or  from  specific  user  accounts  [18], 

[19]. However, specific users and hashtags that are known to be related to events on the cyber 

domain are also a good source of related discussions. With content-organised platforms like 

Reddit and Stack Overflow, identifying a handful of context-related channels where users are 

actively engaged in relevant types of discussions to have shown to return better samples for 

context analysis [10]. 

Lexicon Building 

The aim of building a lexicon is to extract a set of terms that capture the context of lingual 

analytical interest. From an existing sample of ‘domain-related’ texts, we hope to obtain a set 

of terms that are seen to appear frequently these texts.  In addition to the frequency of term 

occurrence, we aim to discriminate terms based on their level of importance and associative 

mutual dependence on other terms in the sentence.  

Typically,  when  building  domain-specific  lexicons,  two  design  approaches  are  considered 

and  most  often  combined:  selecting  and  grouping  terms  within  some  predefined  categories 

[20] and weighting terms based on the domain of analysis [21] and usefulness of terms to the 

topic of interest [22]. 

The  first  step  in  most  lexical  creation  tasks  is  the  candidate  terms  generation.  This  step 

involves  creating  a  wordlist  or  dictionary  of  terms  that  are  known  to  appear  frequently  in 

discussions of interest. Keyword-based extraction of terms is most common in this step and 

has  been  instrumental  to  creating  traditional  lexicons  [15],  [23]  for  natural  language  and 

information  retrieval  tasks  such  as  sentiment  analysis  [21].  For  example,  [24]  creates  a 

wordlist of positive and negative words for ranking text documents on a scaled range of -1 to 

1 (-1 indicating a strongly negative text document and +1 indicating a strongly positive text 

document). Allahyari et  al. [25] apply a similar approach to a multi-class text classification 

task. Similarly, Rose et  al. [26]  keyword-based  approach provide  context  to  the terms  used 

for  lexical  analysis  by  creating  a  network  of  lexical-semantic  relations  between  words  in  a 

document corpus, where the meaning of each term in the lexicon is defined within the context 

of its relationship with other terms. 

Simple  keyword-based  candidate  term  selection  approach  is  further  extended  to  include 

methods  for  term  scoring.  For  each  term  that  makes  it  into  the  candidate  set,  scoring 

techniques  evaluate  the  importance  of  that  term  relative  to  other  terms  in  the  candidate  set 

[16]  —  for  example,  quantifying  the  relationship  between  the  terms  ‘vulnerability’  and 

‘malware’  in  a  document  set.  Popularly  in  research,  terms  are  numerically  scored  by  two 

main  techniques:  frequency-based  scoring  techniques  [26]–[28]  and  scores  based  on 

associative  dependence  [22],  [29],  [30].  Frequency-based  scoring  techniques  rank  scores 

based  on  the  number  of  times  they  occur  within  sentences.  These  methods  usually  address 

issues with stop words such as ‘I’, ‘is’, ‘then’, ‘that’, ‘have’, ‘has’. – that have no contextual 

meaning  but  have  a  high-frequency  score  due  to  the  nature  of  their  usage  in  the  English 

language.  Scoring  techniques  based  on  associative  dependence  further  simple  frequency-

based scoring to address issues of term importance in the context they occur.  

However,  since  the  basis  of  most  of  these  term  scoring  techniques  is  on  semantic 

relationships  between  words  in  a  text  document  (independent  of  externally  perceived 

meanings), most of these techniques are blind to the domain of analysis in which they occur. 

For example, consider these two phrases that belong to the same text corpora taken from two 

different  subreddits:  ‘MySQL Database developer needed urgently  for a 2-month project  in 

Belfast.’ and ‘New MySQL database vulnerability found on Windows operating system. Yet 

again!!’  Assuming  a  single  domain  of  analysis,  the  term  ‘database’  in  the  midst  of  other 

domain-related terms such as: [‘vulnerability’], [‘operation’ and ‘system’], [‘MySql’], should 

have a higher-ranking score than the same word in the previous phrase. 

This paper aims to develop a lexicon that is usable on most social platforms to automatically 

detect  cyber-related  messages.  The  data  comprises  of  a  standard  set  of  discussions  from 

various social platforms, representative of these cyber-related discussions. 

Data collection and pre-processing 

The  datasets  used  in  this  study  were  collected  from  five  social  platforms;  Twitter 

(twitter.com),  Reddit  (reddit.com),  Stack  Overflow  (stackoverflow.com),  Cyber  War  News 

(cyberwarnews.info)  and  The  Hacker  News  (thehackernews.com).  These  platforms  were 

selected as they represent the types of social networks and content generated discussed in this 

paper. 

Twitter  represents  a  self-moderated  real-time  microblogging  platform  where  users 

communicate uninterrupted and unedited. Twitter is  a real-time reactively moderated social 

network  platform  where  moderators  rely  on  users  to  report  content  deemed  inappropriate. 

Users populate twitter timelines with short (maximum of 140 characters) non-curated posts. 

Hashtags  identify  tweets  on  a  single  topic,  ‘@’  represents  users  involved  in  a  discussion 

thread. Cyber discussions on twitter cover a wide range of cyber event types from individuals 

and groups on cyber hacktivism, cyber warfare, cyber-crimes and cyber terrorism activities. 

Reddit  is  a  collection  of  minimally  moderated  sub-forums  called  subreddits  with  multiple 

topic threads and comment discussions in each subreddit. Users populate Reddit forums with 

short  to  medium-length  posts  and  comments.  Individuals  are  allowed  to  freely  start  and 

participate  in  conversations  on  their  topic  of  interest.  Sub-reddits  dedicated  to  cyber 

discussions  cover  a  wide  range  of  cyber-related  issues  such  as  cyber-crime,  cyber-warfare, 

cyber-hacktivism and cyber-terrorism. 

Stack overflow is a highly-moderated question and answer community with sub-communities 

a  wide  range  of  technical  content.  Questions  and  answers  on  stack  exchange  can  be  long, 

medium  or  sometimes  short  text.  Discussion  forums  are  highly  moderated,  and  content  is 

filtered based on what forum moderators classify ‘inappropriate’ or ‘irrelevant’ to the forum. 

Cyberwar  news  is  an  archive  of  articles  on  cyber  events.  Cyberwar  news  provides  long 

curated, moderated and edited news articles with details on cyber events. Cyber event details 

include details of attackers, details of the victim and technical aspects of the attack. 

Similar to  Cyberwar news, the Hacker News is  a widely-acknowledged  cybersecurity news 

platform,  with  over  8  million  active  readers  monthly.  Readers  include  IT  Professionals, 

academic  researchers,  hackers  and  technology  experts.  The  platform  features  news  on  the 

latest  events  in  cybersecurity  and  extensive  coverage  of  current  and  future  trends  in 

information security.  

Data Collection 

This work aims to build a lexicon with a set of candidate terms to automatically identify texts 

that  are  related  to  the  cyber  domain.  After  identifying  potential  data  sources  for 

representative cyber-related discussions, we create a generic text corpus that is a collection of 

documents from all data sources. 

Data Source 

Social Platform Type 

Cyber Discussion Type  Number 

of 

% Corpus 

Documents 

1 

2 

3 

4 

5 

Question 

and  Answer 

Long, Medium or Short, 

190,970 

17.8% 

Forum 

Minimally-moderated 

user-generated content. 

Discussion 

/Community 

333,882 

31.2% 

Forum 

Microblogging Site 

Short  

507,348 

47.4% 

Blog/News Site 

Long 

Heavily 

10,568 

1% 

Moderated, 

Edited 

News-like Articles 

Blog/News Site 

Long 

Heavily 

28,114 

2.6% 

Moderated, 

Edited 

News-like Articles 

1,070,882 

Table 1: Data Sources For Lexicon Development 

Total 

Number 

of 

Documents 

From blogging platforms such  as  cyberwar news  (cyberwarnews.info) and the hacker news 

(thehackernews.com),  we  collect  a  total  of  1550  cyber  event  news  articles.  Each  article  is 

sentence tokenised, and each sentence is a single document in the final corpus. Cyber news 

articles such as on cyberwar news and hacker news, provide details of cyber events on social 

media such as the name of the operation, target, hacker(s) (if available), date of the event and 

additional social media details. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To identify relevant users and therefore construct a useful keyword-based filter for Twitter, 

we  extract  547  twitter  hashtags  (189)  and  mentions  (358)  from  the  curated  cyber  news 

articles collected from cyberwar news and the hacker news. For example, cyber operations on 

Twitter  are  tagged  with  the  naming  convention  “#Op[Name  of  Operation]”  (#OpPayback).  

This  list  of  extracted  handles,  hashtags  and  mentions  are  further  used  as  filters  to  collect 
historical twitter discussions from the 18th of June 2018. Each tweet is also added as a single 

document to the final corpus. 

All user comments on each post from thirty-eight cyber-related subreddits were also gathered 

and  processed.  Each  post  and  user  comment  is  sentence-tokenised  and  added  as  a  single 

document to the final corpus. 

Furthermore, questions, answers and posts from  ~900 stack exchange question threads with 

at least a thousand (1000) votes were also collected and added to the corpus. Non-text lines 

were excluded from the collection process. 

The search space for data collection in this paper is significantly reduced to only sources of 

cyber-related discussions online. Therefore, tweets, comments and posts from all data sources 

were collected under the assumption of being a post about cyber-related activities. 

Data processing 

We  combined  tweets,  Reddit  comments,  stack  overflow  posts  and  blog  articles  from 

cyberwar news and hacker news into a single text corpus.  We remove duplicate tweets and 

retweets.  We ensure that each data source contributes a significant number of documents to 

the final corpus to achieve an even distribution of document types. A document, represents a 

single  tweet,  a  question,  an  answer,  a  sentence,  a  post  or  a  comment  from  any  of  our  data 

sources.  Long  curated  articles  (text)  such  as  from  cyberwar  news  are  split  into  individual 

sentences,  where  each  sentence  is  a  document  in  our  corpus.  Splitting  long  documents 

ensures that the length of each document in the final corpus remains within a same range of 

word count. 

We use speech tagging to identify and remove entities such as names, places, people, things, 

events and time from each document. Speech tagging splits each document into samples of 

parts-of-speech  identification,  which  act  as  markers  to  find  people,  places,  dates,  time  and 

other  related  entities.  Parts  of  speech  tagging  identifies  the  function  a  word  plays  in  a 

sentence. Table 2 below shows an example of a document with speech-tagged words. 

“Excerpt  from  Michael  Hastings’  “The  Operators”  Re:  Death  Threats  http://bit.ly/198uVBl 

#Anonymous #OpIsrael #OpPalestine #Gaza” 

SN 

TERM 

TAG 

DESCRIPTION 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

Excerpt 

NN 

Noun Singular Or Mass 

From 

IN 

Preposition or subordinating conjunction 

Michael 

NNP 

Proper Noun, Singular 

Hastings 

NNP 

Proper Noun, Singular 

The 

DT 

Determiner 

Operators 

NNPS 

Proper Noun, Plural 

death 

NN 

Noun, Singular Or Mass 

threats 

NNS 

Noun, Plural 

anonymous 

JJ 

Adjective 

opIsreal 

NNP 

Proper Noun Singular 

OpPalestine  NNP 

Proper Noun Singular 

Gaza 

NNP 

Proper Noun Singular 

Table 2: Sample of Speech-Tagged Tweet 

“Always assume the adversary knows the method, see Kerckhoffs' principle linked in our sidebar.” 

SN 

TERM 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

Always 

Assume 

The 

Adversary 

Knows 

The 

Method 

See 

Kerchoffs 

Principle 

Linked 

in 

TAG 

NNS 

VBP 

DT 

NN 

VBZ 

DT 

NN 

VBP 

NNP 

NN 

VBD 

IN 

DESCRIPTION 

Noun, Plural 

Verb, non-3rd person singular present 

Determiner 

Noun, singular or mass  

Verb, 3rd person singular present 

Determiner 

Noun, singular or mass 

Verb, non-3rd person singular present 

Proper noun, singular 

Noun, singular or mass 

Verb, past tense 

Preposition or subordinating conjunction 

 
Our 

Sidebar 

PRP$ 

NN 

pronoun, possessive 

Noun, singular or mass 

Table 3: Sample of Speech-Tagged Reddit Comment 

We use the compendium of Peen Treebank [31] as a standard for speech tagging terms in our 

corpus. We remove all types of nouns (NN, NNS, NNP, NNPS, POS), all forms of pronouns 

(PRP, PRP$, WP, WP$), prepositions (IN, EX), conjunctions (CC), determiners (DT, PDT, 

WDT),  articles  (TO,  RP)  and  other  unwanted  terms  (FW,  MD,  SYM)  leaving  only  verbs, 

adverbs  and  adjectives.  Additionally,  we  also  remove  URLs,  user  mentions,  hashtags  and 

emotion  encodings  from  each  document.  Finally,  we  remove  forum  specific  words  from 

corresponding  documents.  For  example,  the  word  “post”  from  twitter  documents,  “votes”, 

“upvotes” and “downvotes” from stack overflow documents, the words “subreddit”, “Reddit” 

and “forum” from Reddit documents. 

Figure 1: Lexicon Development Process 

Building The Lexicon 

Figure 1 above shows the steps involved in  developing our  generic ‘cyber-related’ lexicon. 

Our  process  starts  by  selecting  a  set  of  candidate  terms  for  our  generic  lexicon.  Each 

candidate term is assigned three scores; an FDR score, a TF-IDF score and an APMIS score. 

The  FDR  score  represents  the  term’s  frequency  of  occurrence  across  all  documents  in  the 

corpus.  The  TF-IDF  score  represents  the  average  of  the  term’s  importance  across  all 

documents in which it occurred. The APMIS score represents the associative dependence of 

the  term  based  on  its  point-wise  mutual  information  with  other  terms  in  the  document. 

Finally, a cutoff at the Nth percentile is used to select  the Top (k) terms from each scoring 

mechanism. 

 
 
 
 
Candidate Term Selection 

Our candidate terms are selected based on term frequency, term unigrams and term entropy in 

across all documents. First, each document is term tokenised across the corpus producing a 

unique set  of 2,434 initial  candidate terms.  After term  tokenising sentences, we perform an 

initial curation step to remove words that are less than two characters long, greater than 15 

characters  and  with  term  entropy  of  0.  The  entropy  of  terms  is  estimated  using  the  text 

perfection method described in [32]. We estimate the entropy of each term as 𝑻𝒆𝒓𝒎 𝑬𝒏𝒕𝒓𝒐𝒑𝒚 =

 −(𝒑 ∗ 𝒍𝒐𝒈(𝒑)) where  p  is  the  probability  of  occurrence  for  each  individual  character  in  term, 

estimated as: 

𝑵𝒖𝒎𝒃𝒆𝒓 𝒐𝒇 𝒖𝒏𝒊𝒒𝒖𝒆 𝒄𝒉𝒂𝒓𝒂𝒄𝒕𝒆𝒓𝒔 𝒊𝒏 𝒕𝒆𝒓𝒎
𝑵𝒖𝒎𝒃𝒆𝒓 𝒐𝒇 𝑪𝒉𝒂𝒓𝒂𝒄𝒕𝒆𝒓𝒔 𝒊𝒏 𝒕𝒆𝒓𝒎

Using  the  term  entropy  eliminates  terms  with  no  substance-for  example  ‘aaaaa’,  ‘ahhhh’, 

‘hahaha’.  We  also  remove  text  noise  in  the  form  of  numbers,  punctuation  and  stop  words. 

Finally,  we  stem  the  remaining  words  using  Porter’s  Stemmer  [33].  The  candidate  term 

selection step returns 1149 candidate terms. 

Term Scoring 

Our term scoring strategy is robust in that it includes terms ranked by three different scoring 

mechanisms. In other to ensure a robust lexicon, we rank terms based on their co-occurring 

frequency  with  other  terms,  relevance  and  mutual  association  with  other  terms  across  all 

documents.  For  each  term  in  our  candidate  set,  we  estimate  its  Term  Frequency-Inverse 

Document  Frequency  (TF-IDF)  [34]  to  measure  its  relevance,  Frequency-Degree  Ratio 

(FDR) [26] to quantify its overall co-occurring frequency and Aggregated Pointwise Mutual 

Information  Score  (APMIS)  to  estimate  its  associative  dependence.  The  frequency  degree 

ratio scoring ensures that the lexicon includes terms, frequently occurring with other terms. 

Similarly, the Term-Frequency Inverse Document Frequency scoring mechanism ensures that 

the  lexicon  includes  rare  but  relevant  contextual  terms.  Finally,  the  Aggregated  Pointwise 

Information  scoring  ensures  that  the  lexicon  includes  terms  with  greater  informative 

association with other terms across documents. 

The term  scoring mechanisms used in  this paper  starts  with  an initial  estimation  of a term-

document  matrix.  A  term-document  matrix, 𝒕𝒇𝒕,𝒅 is  a  sparse  matrix  representation  of  the 

terms’ weights in each document in the corpus [22].We start by creating the term-document 

 
matrix 𝒕𝒇𝒕,𝒅 ,  where  each  row  is  a  document  d,  each  column  is  a  term  t  and  each  cell 

represents the number of times the term t occurs in the document d. Note that the columns in 

a  term-document  matrix  represent  terms  across  all  documents  in  the  corpus,  therefore 

creating a highly sparse matrix representation of term weightings of zeros across documents. 

To address the sparsity of the term-document matrix, we remove terms that only appear in at 

least 90% of documents in the combined corpus. 

Term-Frequency Inverse Document Matrix (TF-IDF Score) 

While term frequency of a term t refers to the number of occurrences of the term t across all 

documents  in  the  corpus,  the  document  frequency  of  a  term  t,  refers  to  the  number  of 

documents in which t occurs. Dividing the term-frequency of each term by its corresponding 

document  frequency  produces  a  matrix  that  is  directly  proportional  to  the  frequency  of 

occurrence  of  each  term  in  a  document  but  inversely  proportional  to  the  number  of 

documents  it  occurs.  The  inverse  document  frequency  score  diminishes  the  weights  of 

frequently occurring terms  and increases the weights  of terms  that occur rarely.  The term-

frequency  inverse  document  frequency  of  terms,  therefore,  assigns  weights  to  terms  that 

signify  their  quantified  relevance  in  documents.    The  Inverse  Document  Frequency  (IDF) 

[35] is estimated as: 

𝒊𝒅𝒇𝒕 = 𝒍𝒐𝒈(

𝑵
𝒅𝒇𝒕

) 

We,  therefore,  estimate  the  term-frequency  inverse-document  frequency  score  (TF-IDF 

score)  for  each  term  as  a  product  of  the  term  frequencies  and  the  inverse  document 

frequency. 

𝑻𝑭𝒕,𝒅𝒊𝒅𝒇𝒕 = (𝑻𝑭𝒕,𝒅)(𝒍𝒐𝒈 (

𝑵
𝒅𝒇𝒕

)) 

Where the total term-frequency of each term ( 𝑻𝑭𝒕,𝒅) is estimated as the sum of raw counts of 

a term t in each document d divided by the total number of terms in d. 

Frequency Degree Ratio (FDR Score) 

The summed term frequency, freq(t), refers to the number of times each term appears across 

all documents while term degree is a measure of co-occurrence of each term with other terms 

across all documents in a given corpus [26].  The term degree is a term-term first-order co-

occurrence  matrix  [36] 𝒕𝒕𝒕,𝒕  which  represents  the  number  of  times  a  term  𝒕𝒊  occurs  with 

another term   𝒕𝒋 within the same context  across all documents in  the corpus. Co-occurrence 

expresses  links  of  relationships  between  texts,  therefore  acting  as  an  indicator  of  cohesion 

between individual terms in a document [37]. In a co-occurrence matrix or term-term matrix, 

each term is represented as a numeric vector of the number of occurrences with other terms. 

Each term in our candidate set is represented as a potential context term. Our co-occurrence 

matrix is an |𝑴| 𝑿 |𝑴|  matrix where M = number of terms in the candidate set. The summed 

term  degree,  degree(t),  is  the  sum  of  term-term  co-occurrence  of  term  t  with  all  other 

candidate  terms.  The  term  degree,  therefore,  measures  the  importance  of  each  term  in  a 

document relative to other documents in the entire corpus. 

For  the  Frequency-degree  ratio  scoring,  each  term  in  the  resulting  candidate  set  is  initially 

weighted on these two basic scores: the summed term frequency and the summed term degree 

of terms. Note that our measure of term degree, degree(t) subtracts the number of self-term 

co-occurrence (i.e. the number of times a term co-occurs with itself) from the summed total 

of  term  degree  (by  equating  the  diagonals  of  the  term-term  matrix  M  to  0).  Similarly,  we 

normalise our measure of term frequency of each term with the total number of terms in the 

corresponding document. The ratio of term frequency to term degree 

in our candidate set is estimated to give the FDR term score. 

𝒅𝒆𝒈𝒓𝒆𝒆 (𝒕)

𝒇𝒓𝒆𝒒(𝒕)

 for each term 

Aggregated Pointwise Information Score 

Frequency-based  algorithms,  however,  are  not  the  best  measures  for  associations  between 

terms as they are not very discriminative to terms with a superior level of relevance given the 

context of analysis. To extract a measure for the degree of context shared between individual 

pairs  of  terms,  we  replace  the  frequency  with  the  pointwise  mutual  information  score 

between the two terms. We estimate the informative dependence between two terms as their 

pointwise  mutual  information.  The  pointwise  mutual  information  between  two  terms 

𝒕𝟏 𝑎𝑛𝑑 𝒕𝟐 measures  the  amount  of  informative  association  between  them  [38]  i.e.  the 

probability  of  observing  a  term  𝒕𝟏  with  another  term  𝒕𝟐  as  opposed  to  observing  them 

independently. In its simplest application, a set of terms in a candidate set is measured against 

a  set  of  ‘context-terms’,  usually  representing  a  given  context  of  analysis.  It  can  also  be 

described as the logged independent joint probability of occurrence  between 𝒕𝟏 𝑎𝑛𝑑 𝒕𝟐. The 

PMI between two terms 𝒕𝟏 𝑎𝑛𝑑 𝒕𝟐 is estimated as: 

𝑷𝑴𝑰(𝒕𝟏, 𝒕𝟐) = 𝒍𝒐𝒈𝟐 (

𝑷(𝒕𝟏, 𝒕𝟐)
𝑷(𝒕𝟏)𝑷(𝒕𝟐)

) 

We  use  our  candidate  terms  as  context  terms  for  analysis.  Therefore,  our  PMI  Matrix  is  a 

term-term representation of the pointwise mutual information between each pair of candidate 

terms.  The  PMI  estimate  ranges  from  -∞  to  +∞.  To  compute  the  PMI  matrix  from  a  term-

term  matrix  M  with  n  rows  (terms)  and  n  columns  (context-terms;  candidate  terms  in  our 

case), we estimate each cell as: 

𝒑𝒎𝒊𝒊𝒋 =   𝒍𝒐𝒈𝟐 (

𝒑𝒊𝒋
𝒑𝒊 ∗   𝒑𝒋

) 

The  pointwise  mutual  information  (PMI)  matrix  shows  the  PMI  score  for  each  ‘term-term’ 

combination of terms in  our candidate set.  For example, given the term-term matrix below, 

the aggregated sum of co-occurrence of the term ‘attack’ with all other terms in the matrix is 

3182. Similarly, the aggregated sum of co-occurrence of the term ‘target’ with all other terms 

in the matrix is 1398. 

Figure 2: Term-term Matrix with term-term co-occurrence scores 

If  the  sum  of  the  ‘term-term’  matrix  is  8182,  we  can  estimate  the  PMI  between  the  term 

“attack” and the context-term “target” as: 

𝑷𝑴𝑰𝒂𝒕𝒕𝒂𝒄𝒌,𝒕𝒂𝒓𝒈𝒆𝒕 =   𝒍𝒐𝒈𝟐  (

𝒑(𝒂𝒕𝒕𝒂𝒄𝒌, 𝒕𝒂𝒓𝒈𝒆𝒕)
𝒑(𝒂𝒕𝒕𝒂𝒄𝒌) ∗ 𝒑(𝒕𝒂𝒓𝒈𝒆𝒕)

) 

Where “attack” is the term and “target” is the context term. 

P(attack) = 3182/8182 = 0.39 

P(target) = 1398/8182 = 0.17 

P(attack, target) = 731/8182 = 0.09 

𝑷𝑴𝑰𝒂𝒕𝒕𝒂𝒄𝒌,𝒕𝒂𝒓𝒈𝒆𝒕 =   𝒍𝒐𝒈𝟐  (

𝟎. 𝟎𝟗
𝟎. 𝟑𝟗 ∗ 𝟎. 𝟏𝟕

) 

 
 
Therefore, there is a 14% probability of observing the terms ‘attack’ and ‘target’ together in 

the  document  corpus.  Estimating  this  value  for  each  term-term  (context)  combination 

produces the figures shown below. 

Figure 3: Term-term Matrix with term-term co-occurrence PMI Scores 

The  PMI  scores  for  term-based  analysis  is  known  to  be  discriminative  to  infrequent  terms 

where  infrequent  terms  have  very  high  PMI  values  [22].  Jurasky  &  Martin  proposes  two 

solutions  to  this  problem:  a)  higher  probability  assignments  by  raising  context-term 

probabilities  to  =0.75  and  b)  using  the  Laplace  [add-2]  smoothed  values  of  the  co-

occurrence matrix. Therefore, the Aggregated Pointwise Mutual Information Score (APMIS) 

for each term is  the simple sum  of PMI scores  of a single term  with  all context-terms.  For 

example, using the figure above, 

𝑨𝑷𝑴𝑰𝑺𝒂𝒕𝒕𝒂𝒄𝒌 = 𝟎. 𝟎𝟎𝟎 + 𝟎. 𝟗𝟐𝟓 + 𝟎. 𝟒𝟎𝟏 − 𝟎. 𝟎𝟒𝟗 − 𝟎. 𝟎𝟔𝟗 − 𝟎. 𝟔𝟑𝟐 = 𝟎. 𝟓𝟕𝟔 

Therefore, there is  a 57% probability of observing the term ‘attack’ with other terms in the 

matrix in figure 3 above.  

Finally,  the  table  below  shows  the  top  20  cyber-related  terms  using  each  of  the  scoring 

criteria. 

SN 

TF-IDF 

1. 

  http 

2. 

  enter 

3. 

  run 

4. 

  data 

5. 

  free 

6. 

  find 

7. 

  window 

8. 

  actual 

FDR 

administr 

agenc 

action 

analyz 

amount 

aim 

breach 

access 

APMIS 

actual 

allow 

account 

back 

activ 

address 

addit 

access 

 
9. 

  differ 

10.    file 

11.    user 

12.    call 

13.    prize 

14.    ticket 

15.    open 

16.    chang 

17.    read 

18.    creat 

19.    start 

20.    key 

aspect 

account 

address 

autom 

applic 

attempt 

attack 

capabl 

admin 

advis 

associ 

alert 

app 

basic 

assum 

base 

avail 

applic 

appear 

attack 

affect 

android 

anonym 

break 

Table 4: Top 20 Terms (FDR, TF-IDF, APMIS Scoring). 

The difference in term ranks for each scoring mechanism, in summary, the FDR word score 

favours frequently co-occurring terms, the TF-IDF score will favour useful terms that often 

occur  across  documents  while  the  APMIS  will  favour  terms  with  greater  association  and 

dependence with other terms across documents. 

 Term Curation And Top-terms Selection 

The  curation  steps  further  remove  unwanted  terms  to  yield  better-filtered  results.  We  also 

remove  terms  contextually  associated  with  specific  cyber  events,  users,  Reddit  forums  and 

hashtags.  For  example,  terms  such  as  the  name  of  Reddit  forums  or  twitter  usernames  and 

handles.  Additionally,  names  of  specific  cyber  events  on  Twitter  such  as  tagged  cyber 

operations  e.g.  #OpPayback  or  specific  users  such  as  @anonr00t  were  removed  from  the 

resulting  word  list.  After  candidate  terms  have  been  selected,  we  select  terms  whose  term 

scores meets an optimal threshold K. 'K' represents a threshold of terms scores for including 

corresponding terms in the lexicon. 

Figure 4: Distribution of Term Scores 

Terms Score Cutoff Selection 

We determine the cutoff 'K' by observing the changes in the term scores at various percentiles 

as shown in figure 4. Figure 4(a), 4(b) and 4(c) plot the density distribution of term scores. 

The red lines indicate the cutoff percentile at which there is at least an increase twice as much 

as the previous increase. The right-skewed distributions show that about 10% (above the 90th 

percentile) of all terms from each scoring mechanism meet the threshold.  

Figure 5: Term Score Cut-offs 

Additionally, as seen in table 3 above, the terms whose scores meet the 90th percentile cutoff, 

returned  by  each  scoring  mechanism  significantly  contain  terms  that  are  representative  of 

specific cyber event categories [39]. For example, the Top(K) terms for the term frequency – 

term degree ratio returns terms that are characteristic of cybercrimes while the Top(K) terms 

for APMIS returns terms that are characteristic of cyber warfare and cyber espionage. To get 

 
 
a representative sample of terms that characterise various types of cyber incidents, we include 

all terms whose scores meet the optimal cutoff in the lexicon. 

𝑪𝒖𝒕𝒐𝒇𝒇(𝑲)𝒃𝒂𝒔𝒊𝒄 𝒔𝒄𝒐𝒓𝒆, 𝑪𝒖𝒕𝒐𝒇𝒇(𝑲)𝑻𝑭−𝑰𝑫𝑭 𝒂𝒏𝒅 𝑪𝒖𝒕𝒐𝒇𝒇(𝑲)𝑨𝑷𝑴𝑰𝑺. 

In  the  end,  this  cyber  lexicon  should  be  able  to  track  various  types  of  cyber-related 

discussions  on  social  platforms.  The  application  of  this  lexicon  depends  on  the  kind  of 

analysis  conducted.  For  example,  the  lexicon  can  be  used  in  quantifying  a  random  text  to 

estimate  its  degree  of  cyber-relatedness  or  used  in  querying  and  tracking  discussions  on 

social forums. The final  Cyber Lexicon is a set of 217  terms with their corresponding term 

scores for each scoring criteria. 

Evaluating The Lexicon 

To test the real-world application of our generic cyber lexicon, we create a new text corpus 

with  documents  collected  from  various  social  media  platforms  and  measure  the  degree  of 

‘cyber-relatedness’ of documents in  the corpus. Social media platforms  included in  the test 

sample cases include some of and more social media platforms included in the corpus used in 

developing  the  lexicon.  We  start  off  by  building  the  sample  text  corpus  of  cyber  and  non-

cyber related documents (texts) obtained from the web and apply our generic lexicon to each 

document  in  the  corpus.  The  sources  for  our  test  corpus  collection  are  based  on  a 

representative  sample  of  all  types  of  cyber-related  discussions  on  various  social  platforms. 

The table below shows the data sources, amount of cyber-related and non-cyber-related texts 

collected for our test text corpus. 

Data Source 

Social Platform Type 

Number 

of 

Number  of 

Number 

% 

of 

Cyber-related 

Non-Cyber-

of 

Test 

Texts 

related 

Docume

Corpus 

News Site (Long edited articles) 

15 

Texts 

15 

nts 

30 

13.7% 

Microblogging 

Comments 

15 

13 

28 

12.8% 

(Short  –  Medium  minimally 

moderated user comments) 

Blogging/News 

Site 

(Long 

14 

0 

14 

6.4% 

edited articles) 

1 

2 

3 

 
 
 
 
 
 
 
4 

5 

6 

7 

8 

Microblogging 

Comments 

15 

14 

29 

13.3% 

(Short  –  Medium  minimally 

moderated user comments) 

Discussion Forum 

Discussion Forum 

Question and Answer Forum 

15 

15 

14 

Blogging  Comments  (Short  – 

14 

Medium  minimally  moderated 

user comments) 

15 

30 

13.7% 

15 

14 

15 

30 

28 

29 

13.7% 

12.8% 

13.3% 

TOTAL 

117 

101 

218 

≈ 100% 

Table 5: Data Sources for Evaluation Data 

Evaluation Strategy 

For each data source identified in the table above, we collect a set of ‘cyber-related’ texts and 

non-cyber-related  texts.  Cyber-related  texts  were  collected  from  comments  in  forums  or 

sections tagged as being related to cyber-attacks or incidents. The non-cyber-related text was 

collected from random sections of forums such as fashion, entertainment, economy, finance 

etc. All posts under cyber-related topics were labelled as ‘cyber’ (or 1), and all posts on other 

topics were labelled as  ‘non-cyber’ (or 0). A total  of 117 cyber-related and 101 non-cyber-

related  texts  were  collected  from  8  social  platforms-as  shown  in  table  7-  to  create  the  test 

corpus. Each document in the corpus is then assigned to one of two classes: a) Cyber-related 

and  b)  Non-Cyber-Related.  Note  that  there  is  a  fairly  balanced  number  of  documents 

assigned to each class. 

To calculate the degree of cyber-relatedness of a document, we rely on a bag-of-words-based 

algorithm that is a function of the document’s terms and terms in the cyber lexicon. For this 

evaluation, our estimate of the ‘cyber-relatedness’ of a given text is a measure of the summed 

APMIS scores of individuals words in the text matched to terms in our generic lexicon. The 

APMIS scores in the lexicon are re-scaled on a scale of 0 through 100. The cyber-relatedness 

of text is estimated as the average of the APMIS scores of all cyber-related terms in the given 

text. 

Given that all APMIS term scores are within a range of  0 through 100, the expected estimate 

for the cyber-relatedness of any given text should also be on a similar scale. Therefore, these 

estimates  can  be  represented  as  a  percentage.  Note  that  each  word  in  a  text  is  scored 

 
 
 
 
 
 
 
 
regardless  of  its  frequency  of  occurrence;  therefore,  a  word  W  with  a  word  frequency  of  3 

will add (𝑾𝒂𝒑𝒎𝒊𝒔) ∗ 𝟑   to the total text score. 

Given a random text or sentence and our generic lexicon with corresponding term scores, the 

‘cyber-relatedness’ is measured by the ‘Cyber-Relatedness’ algorithm as presented below: 

Pseudo-Code: Estimating the Cyber-Relatedness (CR) of  a Random Text 

Input: L < Cyber Lexicon [Term: Score] >, S < A Random Text or Sentence >     

Output: CR < Numeric Quantity for the Cyber-relatedness of S > 

set match = 0;                                                                                                 

<< (a) 

1: 

2: 

3: 

4: 

5: 

6: 

7: 

8: 

9: 

set sum_scores = 0; 

set words = SentenceTokenise(S); 

set wordcount = length(words); 

            For each word in words: 

                         If word in L.Terms: 

                                    set sum_scores = sum_scores + L.Term.Score  

                                     set match = match+1 

                          End 

<< (b) 

<< (c) 

<< (d) 

10: 

            End 

11: 

set scalar =  match ÷ wordcount  

12: 

set CR =  sum_scores * scalar 

13: 

return CR 

Table 6: Pseudocode-Estimating Cyber-Relatedness of a Random Text 

The  pseudocode  above  demonstrates  the  steps  taken  to  quantify  the  degree  of  cyber-

relatedness for a text or sentence. The expected output is a numeric quantity of how ‘cyber-

related’ the sentence is. The Pseudocode above has two inputs: a) a cyber lexicon with cyber-

related terms and respective term scores and b) a sentence of which is to be quantified. The 

pseudocode has four case points:  

a)  match: this tracks the number of words in sentence matched to terms in cyber lexicon,  

b)  sum_scores: this tracks the sum of term scores for matched terms,  

c)  wordcount: this is the number of words in the given sentence,  

 
 
 
 
 
 
 
 
 
 
d)  scalar:  this  scales  the  total  matches  found  by  the  total  number  of  words  in  the 

sentence.   

The  process  splits  the  sentence  into  single  words  and  searches  the  lexicon  for  a  match  on 

each word. It sums up the term scores of each matched term and scales it by a scalar quantity. 

The final CR score is a sum of the term scores scaled by the scalar quantity.  

Classifying Documents 

Document  classification  is  the  task  of  grouping  documents  in  our  test  corpus  as  ‘cyber-

related’  or  ‘non-cyber-related’  based  on  their  content.  The  content  of  each  document  is 

assumed  to  contain  words  that  match  terms  in  our  generic  lexicon,  and  we  estimate  the 

percentage  degree  of  cyber-relatedness  as  a  function  of  matched  terms  scores  and  the 

frequency  of  occurrence  of  each  matched  term.  After  pre-processing  each  document,  using 

the  algorithm  above,  we  estimate  the  degree  of  cyber-relatedness  of  each  test  in  the  new 

sample set. After re-scaling the term scores, the expected estimate for the ‘cyber-relatedness’ 

of  each  text  returns  a  percentage  with  a  value  between  0  and  100.  These  percentages, 

however,  do  not  represent  the  probabilities  of  documents  belonging  to  either  of  the  two 

classes-  ‘Cyber-related’  and  ‘Non-Cyber-Related’.  Classifying  each  document  given  its 

percentage degree of ‘cyber-relatedness’ is treated here as  a two-class classification (binary 

classification) task in a supervised learning environment. The true class for each document is 

the  assigned  classes  (‘Cyber-related’  or  ‘Non-Cyber-Related’)  from  the  previous  document 

labelling phase. 

Applying  our  algorithm  to  each  document  in  our  text  corpus  produces  a  numeric  vector  of 

percentage  ‘cyber-relatedness’  ranging  from  0  through  100.  Therefore,  the  classifier 

boundary (a threshold) between the two identified classes must be determined by a threshold. 

To select an optimal classification threshold, we use the ‘Receiver Operating Characteristic’ 

curve  (ROC)  analysis  [40].  Typically,  the  ROC  curve  is  created  by  plotting  the  recall  or 

sensitivity against the false positive rate given a set of estimated and actual values. 

 
Figure 6: Receiver Operating Curve 

We  select  a  threshold  value  that  maximises  the  ‘area  under  the  ROC  curve'  [40]  as  the 

optimal boundary for our classification task. The intersection of the two red lines in the figure 

above indicates the optimal value (0.19) for a threshold. A threshold of 0.19 maximises the 

probability (0.77) that our algorithm will score a randomly selected cyber-related document 

higher than a randomly chosen non-cyber-related document. 

We create a confusion matrix from comparing our predicted classes produced by applying the 

lexicon  and  the  actual  classes  of  each  document.  The  confusion  matrix  in  the  table  below 

shows  the  number  of  Cyber-related  documents  our  generic  lexicon  correctly  classifies  as 

‘Cyber-related’ (𝑇𝑃𝑠),  the  number  of  Cyber-related  documents  it  incorrectly  classifies  as 

‘Non-Cyber-Related’  (𝐹𝑁𝑠),,    the  number  of  ‘Non-Cyber-related’  documents  it  correctly 

classifies  as  ‘Non-Cyber-Related’  (𝑇𝑁𝑠) ,    and  the  number  of  ‘Non-Cyber-Related’ 

documents it incorrectly classifies as ‘Cyber-related’ (𝐹𝑃𝑠),. 

FDR Scoring 

Cyber 

31 (TP) 

80 (FP) 

 
Non-Cyber 

5 (FN) 

96 (TN) 

TF-IDF Scoring 

Cyber 

79 (TP) 

Non-Cyber 

48 (FN) 

APMIS Scoring 

Cyber 

86 (TP) 

Non-Cyber 

63 (FN) 

32 (FP) 

53 (TN) 

25 (FP) 

38 (TN) 

Table 7: Score Performance Comparison 

Table  5  above  shows  the  performance  of  term  weights  for  each  scoring  mechanism  in 

correctly  identifying  cyber-related  discussions.  Each  confusion  matrix  above  is  an  nXn 

matrix  tool  used  for  performance  evaluation.  The  diagonals  of  the  confusion  (𝑇𝑃𝑠)  and 

(𝑇𝑁𝑠), represent the total number of documents our generic lexicon correctly classifies. On 

the  other  hand,  the  off-diagonals  of  the  confusion  matrix, (𝐹𝑁𝑠), and (𝐹𝑃𝑠), ,  represents  the 

total number of documents our generic lexicon incorrectly classifies. 

The  term  scores  of  the  frequency-degree  ratio  scoring  are  seen  to  maximize  the  ability  to 

correctly  identify  non-cyber  related  documents  while  also  minimizing  the  probability  of 

classifying  a  cyber-related  document  as  non-cyber  related.  On  the  other  hand,  the  terms 

weights from the APMIS scoring algorithms are seen to maximize the ability of the lexicon to 

correctly  identify  all  cyber-related  documents  but  minimally  truly  identifies  all  non-cyber 

related documents. 

We  determine  various  performance  evaluation  metrics  from  the  confusion  matrices  above. 

The  set  of  metrics  we  estimate  are  the  Error  rate,  Performance  accuracy,  Precision,  Recall 

and the F1 Score.  

FDR Scores 

TF-IDF 

APMIS Scores 

Precision 

Recall 

Accuracy 

F1 Score 

Error Rate 

85% 

31% 

61% 

46% 

38% 

61% 

72% 

62% 

66% 

38% 

58% 

78% 

60% 

67% 

40% 

Table 8: Performance Evaluation 

 
Table  6  above  is  a  cross  evaluation  table  of  the  lexicon  using  each  scoring  criteria.  The 

APMIS scores are seen to maximize the ability of the lexicon to correctly identify all cyber-

related texts with an acceptable error level. The APMIS score is also seen to provide a model 

with  an  optimal  balance  of  identifying  positive  cases  and  ignoring  non-positive  cases  by 

maximizing the F1 score. 

Conclusion 

This paper describes a methodology for creating a generic lexicon that captures the analytical 

context of the cyber domain. Our method produces an effective generic cyber lexicon of 745 

terms  based  on  their  TF-IDF,  FDR  and  APMIS  scores  within  the  sample  data  used  in  this 

study.  Our  generic  lexicon  classifies  random  documents  using  a  frequency-based  algorithm 

with an accuracy of approximately 80%. Our results are based on a balanced representation 

of all types of cyber-related discussions in online social platforms. In its generalist form, this 

study  provides  researchers  and  cyber  analysts  with  an  integrated  approach  for  detecting 

‘cyber-related’ discussions in online platforms. However, there are extensive theoretical and 

practical impacts of this study. Firstly, the theoretical methodology outlined in this study can 

be  applied  to  develop  custom  cyber-lexicons  based  on  different  evidence  sources  or  social 

platforms.  The  methodology  can  be  used  in  a  combination  of  various  social  platforms  to 

extract  keywords  that  capture  the  context  of  cyber  discussions  taking  place  on  these 

platforms.  Likewise,  the  generic  lexicon  provided  in  this  study  can  be  applied  to  some 

information retrieval tasks on social platforms. The terms in our generic lexicon can be used 

as filters for sampling cyber-related documents or discussions. Lastly, the results of this study 

can be used to monitor cyber activities online. This study is also useful for cyber analysts to 

detect  malicious  cyber  activities  and  detect  early  warning  signs  of  cyber  threats  on  social 

platforms. To further this study, cyber analysts are usually interested in identifying keywords 

on social platforms that characterise the proliferation of various types of cyber events. We are 

currently  working  on  classifying  filtered  ‘cyber-related’  discussions  from  online  platforms 

based on the classifications of cyber incidents presented by Hathaway [39]. 

 
 
References 

[1] 

F. Halper, “Advanced Analytics: Moving Toward AI, Machine Learning, and Natural Language 

Processing,” TDWI Best Pract. Rep., 2017. 

[2] 

SAS Institute, “Natural Language Processing - What it is and why it matters,” 2018. [Online]. 

Available: https://www.sas.com/en_us/insights/analytics/what-is-natural-language-

processing-nlp.html. 

[3] 

P. Olsen, We Are Anonymous. London: William Heinemann, 2013. 

[4] 

K. Zetter, Countdown to Zero Day. United States: Crown Publishing Group, 2014. 

[5] 

A. Hernández, V. Sanchez, G. Sánchez, and H. Pérez, “Security Attack Prediction Based on 

User Sentiment Analysis of Twitter Data,” Proc. 2016 IEEE Int. Conf. Ind. Technol., pp. 610–

617, 2016. 

[6]  M. Bernstein, A. Monroy-Hernández, D. Harry, P. André, K. Panovich, and G. Vargas, “4chan 

and /b/: An Analysis of Anonymity and Ephemerality in a Large Online Community,” Proc. 

Fifth Int. AAAI Conf. Weblogs Soc. Media, no. Coleman, pp. 50–57, 2011. 

[7] 

L. Knuttila, “User unknown: 4chan, anonymity and contingency,” First Monday, vol. 16, no. 

10, 2011. 

[8] 

R. Mackey, “‘Operation Payback’ Attacks Target MasterCard and PayPal Sites to Avenge 

WikiLeaks,” New York Times, 2010. 

[9] 

A. Pras, A. Sperotto, G. C. M. Moura, I. Drago, R. Barbosa, R. Sadre, R. Schmidt, and R. 

Hofstede, “Attacks by ‘Anonymous’ WikiLeaks Proponents not Anonymous,” CTIT Tech. Rep., 

no. 10.41, pp. 1–10, 2010. 

[10]  R. P. Lippmann, W. M. Campbell, D. J. Weller-Fahy, A. C. Mensch, G. M. Zeno, and J. P. 

Campbell, “Finding Malicious Cyber Discussions in Social Media,” Lincoln Lab. J., vol. 22, no. 1, 

pp. 203–209, 2016. 

[11]  R. P. Khandpur, T. Ji, S. Jan, G. Wang, C.-T. Lu, and N. Ramakrishnan, “Crowdsourcing 

Cybersecurity: Cyber Attack Detection using Social Media,” 2017. 

[12]  M. S. Sri, L. Yellari, and M. S. Rao, “Identifying Malicious Data in Social Media,” Int. Res. J. Eng. 

Technol., vol. 4, no. 3, 2017. 

[13] 

J. Chen, H. Xu, and A. B. Whinston, “Moderated Online Communities and Quality of User-

Generated Content,” Ssrn, vol. 1222, 2009. 

[14] 

P. McKenzie, J. Burkell, L. Wong, C. Whippey, S. E. Trosow, and M. B. McNally, “User-

generated online content 1: Overview, current state and context,” First Monday, vol. 17, no. 

4, 2012. 

[15] 

 a. Ntoulas, P. Pzerfos, and J. C. J. Cho, “Downloading textual hidden web content through 

keyword queries,” Proc. 5th ACM/IEEE-CS Jt. Conf. Digit. Libr. (JCDL ’05), pp. 100–109, 2005. 

[16]  N. Kaji and M. Kitsuregawa, “Building Lexicon for Sentiment Analysis from Massive Collection 

of HTML Documents.,” EMNLP-CoNLL, vol. 43, no. June, pp. 1075–1083, 2007. 

[17]  A. Bruns and L. Yuxian Eugene, “Tools and methods for capturing Twitter data during natural 

disasters,” First Monday, vol. 17, no. 4, 2012. 

[18] 

K. Starbird, G. Muzny, and L. Palen, “Learning from the crowd: Collaborative filtering 

techniques for identifying on-the-gGround Twitterers during mass disruptions,” Proc. 9th Int. 

Conf. Inf. Syst. Cris. Response Manag. ISCRAM, vol. 2011, no. April, pp. 1–10, 2012. 

[19] 

J. Bian, Y. Yang, H. Zhang, and T. S. Chua, “Multimedia summarization for social events in 

microblog stream,” IEEE Trans. Multimed., vol. 17, no. 2, pp. 216–228, 2015. 

[20] 

K. Kipper, A. Korhonen, N. Ryant, and M. Palmer, “Extending VerbNet with novel verb 

classes,” Proc. Lr., vol. 2006, no. 2.2, p. 1, 2006. 

[21] 

S. Baccianella, A. Esuli, and F. Sebastiani, “SentiWordNet 3.0 : An Enhanced Lexical Resource 

for Sentiment Analysis and Opinion Mining SentiWordNet,” Analysis, vol. 10, no. January 

2010, pp. 1–12, 2010. 

[22]  D. Jurafsky and J. Martin, “Vector Sematics,” in Speech and Language Processing, 2nd ed., no. 

c, Prentice Hall, 2017, pp. 99–124. 

[23]  A. Olteanu, C. Castillo, F. Diaz, and S. Vieweg, “CrisisLex: A Lexicon for Collecting and Filtering 

Microblogged Communications in Crises,” Proc. 8th Int. Conf. Weblogs Soc. Media, p. 376, 

2014. 

[24] 

F. Å. Nielsen, “A new ANEW: Evaluation of a word list for sentiment analysis in microblogs,” in 

CEUR Workshop Proceedings, 2011, vol. 718, pp. 93–98. 

[25]  M. Allahyari, S. Pouriyeh, M. Assefi, S. Safaei, E. D. Trippe, J. B. Gutierrez, and K. Kochut, “A 

Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques,” 2017. 

[26] 

S. Rose, D. Engel, N. Cramer, and W. Cowley, “Automatic keyword extraction,” Text Min. Appl. 

Theory, pp. 1--277, 2010. 

[27] 

J. E. Blumenstock, “Size matters: word count as a measure of quality on wikipedia,” Proc. 

17th Int. Conf. World Wide Web, pp. 1095–1096, 2008. 

[28] 

Y. Zhang, R. Jin, and Z. H. Zhou, “Understanding bag-of-words model: A statistical 

framework,” Int. J. Mach. Learn. Cybern., vol. 1, no. 1–4, pp. 43–52, 2010. 

[29] 

F. Debole and F. Sebastiani, “Supervised term weighting for automated text categorization,” 

Proc. 2003 ACM Symp. Appl. Comput.  - SAC ’03, p. 784, 2003. 

[30] 

F. H. Khan, U. Qamar, and S. Bashir, “SentiMI: Introducing point-wise mutual information 

with SentiWordNet to improve sentiment polarity detection,” Appl. Soft Comput. J., vol. 39, 

pp. 140–153, 2016. 

[31]  B. Santorini, “Part-of-Speech Tagging Guidelines for the Penn Treebank Project (3rd 

Revision),” Univ. Pennsylvania 3rd Revis. 2nd Print., vol. 53, no. MS-CIS-90-47, p. 33, 1990. 

[32]  B. Revovna Ospanova, “Calculating Information Entropy of Language Texts,” World Appl. Sci. 

J., vol. 22, no. 1, pp. 41–45, 2013. 

[33] 

P. Willett, “The Porter stemming algorithm: Then and now,” Program, vol. 40, no. 3, pp. 219–

223, 2006. 

[34] 

J. Ramos, “Using TF-IDF to Determine Word Relevance in Document Queries,” Proc. first Instr. 

Conf. Mach. Learn., pp. 1–4, 2003. 

[35] 

K. Spärck Jones, “A statistical interpretation of term specifcity and its application in retrieval,” 

J. Doc., vol. 28, no. 1, pp. 11–21, 1972. 

[36]  H. Schütze and J. Pedersen, “A Vector Model for Syntagmatic and Paradigmatic Relatedness,” 

Mak. Sense Words Proc. Conf., pp. 104–113, 1993. 

[37]  R. Mihalcea and P. Tarau, “TextRank: Bringing Order into Texts,” Proc. EMNLP, pp. 404–411, 

2004. 

[38]  C. Kenneth and P. Hanks, “Word Association Norms, Mutual Information, And Lexicography,” 

Comput. Linguist., vol. 16, no. 1, 1990. 

[39]  O. A. Hathaway, R. Crootof, P. Levitz, H. Nix, A. Nowlan, W. Perdue, and J. Spiegel, “The law of 

cyber-attack,” Calif. Law Rev., vol. 100, no. 4, pp. 817–885, 2012. 

[40] 

J. Hanley and B. McNeil, “The Meaning and Use of the Area Under The a Reciever Operating 

Characteristic Curve,” Radiology, vol. 143, no. 1, pp. 29–36, 1982. 

 
 
