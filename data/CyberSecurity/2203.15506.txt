Trojan Horse Training for Breaking Defenses against
Backdoor Attacks in Deep Learning

Arezoo Rajabi1, Bhaskar Ramasubramanian2, Radha Poovendran1

2
2
0
2

r
a

M
5
2

]

R
C
.
s
c
[

1
v
6
0
5
5
1
.
3
0
2
2
:
v
i
X
r
a

Abstract— Machine learning (ML) models that use deep neu-
ral networks are vulnerable to backdoor attacks. Such attacks
involve the insertion of a (hidden) trigger by an adversary. As
a consequence, any input that contains the trigger will cause
the neural network to misclassify the input to a (single) target
class, while classifying other inputs without a trigger correctly.
ML models that contain a backdoor are called Trojaned models.
Backdoors can have severe consequences in safety-critical cyber
and cyber physical systems when only the outputs of the model
are available. Defense mechanisms have been developed and
illustrated to be able to distinguish between outputs from a
Trojaned model and a non-Trojaned model in the case of a
single-target backdoor attack with accuracy > 96%.

Understanding the limitations of a defense mechanism re-
quires the construction of examples where the mechanism fails.
Current single-target backdoor attacks require one trigger per
target class. We introduce a new, more general attack that will
enable a single trigger to result in misclassiﬁcation to more than
one target class. Such a misclassiﬁcation will depend on the true
(actual) class that the input belongs to. We term this category
of attacks multi-target backdoor attacks. We demonstrate that
a Trojaned model with either a single-target or multi- target
trigger can be trained so that the accuracy of a defense
mechanism that seeks to distinguish between outputs coming
from a Trojaned and a non-Trojaned model will be reduced.
Our approach uses the non-Trojaned model as a ‘teacher’
for the Trojaned model and solves a min-max optimization
problem between the Trojaned model and defense mechanism.
Empirical evaluations demonstrate that our training procedure
reduces the accuracy of a state-of-the-art defense mechanism
from > 96% to 0%. We also discuss possible approaches to
improve defense mechanisms to ensure resilience to backdoor
attacks for a broader category of ML models.

I. INTRODUCTION

The recent advances in cost-effective storage and com-
puting has resulted in the wide use of deep neural net-
works (DNN) across multiple data-intensive applications
such as face-recognition [1], mobile networks [2], computer
games [3], and healthcare [4]. The large amounts of data
and extensive computing resources required to train these
deep networks has made online machine learning (ML)
platforms [5]–[7] increasingly popular. However, these plat-
forms only provide access to input-output information from
the models, and not parameters of the models themselves.
This is termed black-box access. Recent research [8] has
demonstrated that online ML models can be trained in a
manner so that the presence of a speciﬁc perturbation, called
a trigger, in the input will result in an output that is different

1Network Security Lab, Department of Electrical and Computer Engi-

neering, University of Washington, Seattle, WA 98195, USA.
{rajabia, rp3}@uw.edu

2Electrical and Computer Engineering, Western Washington University,

Bellingham, WA 98225, USA. {ramasub}@wwu.edu

from the correct or desired output. At the same time, outputs
of the model for clean inputs- i.e., inputs without trigger- is
not affected. This can result in severe consequences when
such platforms are used in safety-critical cyber and cyber-
physical systems [9]. The insertion of a trigger into inputs
to a model is called a backdoor attack and the model that
misclassiﬁes such inputs is termed Trojaned.

Backdoor attacks can have severe implications in safety-
critical cyber and cyber-physical systems where only the
outputs of a model are available. For example, autonomous
navigation systems that depend on DNNs for decision mak-
ing using reinforcement learning models have been shown to
be vulnerable to backdoor attacks [10]. DNN models have
also been used for trafﬁc sign detection, and these models
can be trained to identify the signs correctly, but results in
an incorrect output when the sign has a trigger [8] (e.g.,
a ‘stop’ sign with a small sticker on it is identiﬁed as a
‘speed-limit’ sign). Such threats necessitate the development
of defense mechanisms. However, most defenses assume that
hyper-parameters of the model are available [11]–[14] or
that a pre-processing module can be added to the target
model [15]. These may not be practical for applications
where only outputs of the model are available and users
cannot control inputs provided to the model.

A defense mechanism against backdoor attacks called
meta-neural Trojan detection (MTND) was proposed in [16].
This method leverages an insight that the distribution of
outputs from a Trojaned model might be different to those
from a non-Trojaned model, even though both models have
similar accuracies. MTND learns a discriminator (a classiﬁer
with two outputs, YES or NO) using outputs from a Trojaned
and a non-Trojaned model as training data,
in order to
distinguish between the models. This approach was shown
to identify Trojaned models with > 96% accuracy, when
only black-box access to them was available [16]. Despite
the current success of MTND, examples can be constructed
that demonstrate some of its limitations.

In this paper, we identify a new class of backdoor attack
called multi-target backdoor attack. Unlike existing single-
target trigger backdoors, a trigger from a multi-target back-
door attack can cause misclassiﬁcation to different output
labels depending on the true class of the input. Speciﬁcally,
we demonstrate that a model can be trained so that
its
is a function of the true class of the input and
output
the presence of a trigger in that input. We also propose
a two-step methodology to bypass the MTND defense
mechanism. Figure 1 demonstrates our approach: (i) we
use a non-Trojaned model as a teacher for the Trojaned

 
 
 
 
 
 
Fig. 1: Schematic of our two-step methodology. Knowledge Distillation: we use a non-Trojaned model as a teacher for the
Trojaned model in order to learn indistinguishable outputs for clean images (green dashed lines). Min-max Optimization:
we optimize the Trojaned model against a discriminator in order to ensure that the discriminator cannot distinguish between
outputs from the Trojaned and non-Trojaned models (red dashed lines).

model (Knowledge Distillation), then (ii) we use min-max
optimization between a discriminator and Trojaned model
to ensure that the discriminator is not able to distinguish
between outputs from a Trojaned and a non-Trojaned model
(Min-max Optimization).

We make the following contributions:
• We introduce a new class of multi-target backdoor
attacks. Such an attack has the property that a single
trigger can result in misclassiﬁcation to different output
labels, based on true input label.

• We design two algorithms- a training procedure that
combines knowledge distillation (Algorithm 1) and min
- max optimization (Algorithm 2) to reduce the accuracy
of a defense mechanism designed to distinguish between
Trojaned and non-Trojaned models.

• We evaluate the trained Trojaned model from the pre-
vious step by examining the effect of a multi-target
backdoor attack on a state-of-the-art meta-neural Trojan
defense (MTND). Our empirical evaluations demon-
strate that our training procedure is able to bypass the
MTND defense 100% of the time.

The remainder of this paper is organized as follows:
Section II presents a tutorial introduction to DNNs with
backdoors and describes our system model. An overview of
related literature on backdoor attacks in deep learning and
state-of-the-art defense mechanisms is provided in Section
III. We introduce our solution approach in Section IV and
report results of empirical evaluations in Section V. Section
VI discusses methods to extend our solution to a broader
class of problems, and Section VII concludes the paper.

II. PRELIMINARIES

This section provides a brief introduction to classiﬁcation
using deep neural networks, and single-target backdoor at-
tacks using a set of poisoned inputs. Finally, we introduce
the system model that we use for our algorithms.

A. Deep Neural Networks

Deep Neural Network (DNN) classiﬁers are trained to
predict the most relevant class among C possible classes
for a given input. The output of a DNN is called a logit,
which gives a weight to each class, z := [z1, · · · , zC].
The output of the model is fed to the softmax function to
generate a probability vector where each element i is the
conditional probability of class i for a given input x. The
softmax function is deﬁned as:

p(zi, T ) =

exp zi/T
j exp zj/T

(cid:80)C

,

(1)

where T is a temperature parameter (typically = 1). A DNN
classiﬁer is a function z := F (x; θ), where x ∈ [0, 1]d is
an input and θ represents hyperparameters of the DNN. We
will write p(z, T ) to denote the probability vector determined
through the softmax function. In order to train the DNN (i.e.,
determine values of θ), we minimize the difference between
the output of softmax function p(F (xk; θ), T = 1), and the
true class of the input, y∗
k for a sample xk. This is quantiﬁed
by a loss function L(p, y∗), and parameters θ are iteratively
updated using stochastic gradient descent as:

θt+1 ← θt − α

1
|D|

∂
∂θ

(cid:88)

k

L(p(F (xk; θ)), y∗
k)

(2)

where D, F , α and L are training set, DNN’s function with
hyper-parameter θ, a positive coefﬁcient and loss function
respectively.

One way of

into the model
introducing a backdoor
is through poisoning the training set with a set of in-
puts stamped with a pre-deﬁned trigger and labeled with
the desired output [17], [18]. The trigger has a single-
target
to
return a speciﬁc output.
to re-
require one trig-
turn multiple target classes, we will

i.e., any input with trigger causes the model
In order

for a model

ger per target class to be inserted into the input. Let
D = {(x1, y1), (x2, y2), · · · , (xN , yN )} be the origi-
(a set of clean samples) and D(cid:48) =
nal
{(x(cid:48)
n, yd)} (n (cid:28) N ) be a set of
perturbed samples. Suppose each sample in D(cid:48) is perturbed
using a pre-deﬁned trigger as:

training set
1, yd), (x(cid:48)

2, yd), · · · , (x(cid:48)

x(cid:48)
ij = mij ∗ ∆ + (1 − mij)xij i.e.,

i ∈ [1, W ], j ∈ [1, H]

where ∆ is the perturbation that we term a Trojan trigger and
m is a mask that indicates the location where the perturbation
is applied. In this paper, we assume that each sample is an
image of resolution W × H. The trained Trojaned model
on both clean and poisoned datasets would return a desired
output in the presence of a speciﬁc trigger in the input while
keeping the accuracy unchanged for clean samples.

B. Our System Model

In this paper, we assume that the Trojaned model is trained
by an adversary who does not share the hyper-parameters of
her model. The Trojaned model can be shared through an
ML platform or can be a built-in model in a smart device.
Therefore, only the outputs of the model are available to
users/defenders for any given input. This is termed black-
box access. The defender aims to learn a discriminator (a
classiﬁer with two classes of YES/NO) to determine whether
a model is Trojaned or not. The defender can learn several
non-Trojaned and Trojaned models locally and use their
outputs to train the discriminator (See Figure 1). We also
assume the defender and adversary have access to the same
training sets to train their local models. Given an arbitrary
set of inputs that is provided to both a Trojaned and non-
Trojaned model, the discriminator uses the outputs from
these two models to learn a (binary) classiﬁer. After training,
the discriminator is used to evaluate an unknown model, to
determine whether it is Trojaned or not. Our contribution in
this paper is the design of a methodology to demonstrate that
such a discriminator can be fooled (i.e., cannot say whether
a model is Trojaned with probability > 0.5).

III. RELATED WORK

This section summarizes recent

literature on backdoor

attacks and defense mechanisms against such attacks.

A backdoor attack results in DNN models misclassifying
inputs that contain a trigger [19]. An input containing a trig-
ger can be viewed as an adversarial example [20], but there
are differences in the ways in which an adversarial example
attack and a backdoor attack are carried out. Adversarial
examples are generated by learning a quantum of adversarial
noise that needs to be added to an input in order to cause a
pre-trained DNN model to misclassify the input [20]–[22].
Backdoor attacks, in comparison, aim to inﬂuence weights
of parameters of neural networks that describe the target
model during the training phase either through poisoning the
training data [19] or poisoning the weights themselves [23],
[24]. Consequently, any input stamped with a pre-deﬁned
trigger will be able to cause the DNN to misclassify the
input. Moreover, the trigger for a backdoor attack might be

invisible [25]–[27] It has been demonstrated that adversarial
example attacks and backdoor attacks can both be trained
to be transferable [8], [28]. When access to a target model
was not available, a black-box backdoor attack was proposed
in [29]. In this case, the adversary had to generate training
samples, since the training dataset was not available.

The vulnerability of DNN models to backdoor attacks have
been examined in multiple applications. A class of semantic
backdoor attacks was proposed in [19], [30] for image and
video models. In this case, the target label was determined
based on features of the input- for e.g., inputs featuring green
colored cars would result in the model misclassifying it as
a bicycle. The authors of [19] and [31] designed backdoor
attacks for code poisoning and natural language processing
models respectively. Recently, an untargeted backdoor attack
for deep reinforcement learning models was proposed in
[10]. Backdoor attacks have also been used to determine
ﬁdelity- speciﬁc examples include model watermarking [32]
and verifying that a request by a user to delete their data was
actually carried out by a server [33].

Backdoor attacks typically specify one target output class
for each trigger. An N-to-one trigger backdoor attack was
proposed in [34], where an adversary used a single trigger
at a speciﬁc location, but with different intensities for each
target. A procedure to train different models to output differ-
ent target classes using the same trigger was demonstrated
in [35]. The authors of [36] introduced an input-based
misclassiﬁcation procedure by learning adversarial pertur-
bations [37] for pairs of target classes. Different from [36]
where one perturbation was required for each pair of classes,
our methodology in this paper uses a single trigger that can
cause misclassiﬁcation to more than one target class.

Defense mechanisms against backdoor attacks can focus
on removing backdoors from Trojaned models [11]–[13] or
detecting/suppressing poisoned data during training [38]–
[40]. Our focus in this paper is on a third type- defense
mechanisms for pre-trained models, since we assume that
the target model has already been trained, and that the user
will only have ‘black-box’ access to it.

Pre-processing defense mechanisms deploy a module that
will remove or reduce the impact of a trigger present in
the input. For example, an auto-encoder was used as a pre-
processor in [15]. A generative adversarial network (GAN)
was used to identify ‘inﬂuential’ portions of an image/ video
input in [41]. This approach was leveraged by [42] to use
the dominant color of the image as a trigger-blocker. Style-
transfer was used as a pre-processing module in [43] to
mitigate the impact of trigger present in the input. Modifying
the location of the trigger using spatial transformations was
deployed as a defense mechanism in [18].

In contrast, post-training defense mechanisms aim to de-
termine whether a given model
is Trojaned or not, and
then refuse to deploy a Tojaned model. The authors of [14]
proposed a technique based on learning adversarial pertur-
bations [37] to locate a trigger using an insight that triggers
constrain the magnitude of the learned perturbation. Thus,
the learning process would identify a model as Trojaned if

the learned perturbation was below a threshold. An outlier
detector was used in [44] to explain outputs of a model and
features extracted using a saliency map were used to identify
if the model was Trojaned. A defense mechanism against
backdoor attacks when working with limited amounts of data
was proposed in [45].

All

the approaches described above require access to
hyperparameters of the model. There is a relatively smaller
body of work focused on designing defenses in the ab-
sence of such access. A mechanism called DeepInspect was
proposed in [46], which learned the probability distribution
of triggers using a generative model. DeepImpact assumed
that a trigger had ﬁxed patterns with a constant-valued
distribution. In comparison, we consider a trigger that can
have an arbitrary location within the input, and can result in
misclassiﬁcation to more than one target class. The authors
of [16] proposed meta neural Trojan detection (MTND).
MTND used a discriminator which took a target model as
input, and performed a binary classiﬁcation on the output of
the model to identify if it was Trojaned or not. We evaluate
our methodology using MTND as a benchmark, since MTND
does not make any assumptions about the trigger.

Fig. 2: Two steps of our learning procedure. In the ﬁrst step
(left chart), we use a non-Trojaned model as a teacher for
the Trojaned model. Then in the second step (right chart),
we update the weights of model to fool the discriminator.

the indistinguishability between outputs of the two models
by solving a min-max optimization problem. The remainder
of this section explains these steps in detail.

IV. SOLUTION APPROACH

A. Using a non-Trojaned Model as Teacher

Backdoor attacks aim to preserve the accuracy of a model
on inputs without a trigger (clean samples) while misclas-
sifying inputs that are stamped with a trigger. We denote
the (Trojan) trigger by ∆. Different from existing backdoor
attacks that result in the model producing a single, unique
target class for inputs with a trigger, we propose a new class
of multi-target backdoor attacks. A multi-target backdoor
attack can result in a model producing a different output
based on the true class of the input that contains a trigger.
Consequently, an adversary can trigger a desired output by
selecting a sample from the corresponding source class.

In order to train a multi-target backdoor, an adversary
poisons the training input set with a new set of samples
perturbed with the trigger, and labeled using a map-function:

D(cid:48) = {(xi1 + m∆, g(y∗
i1

)), (xi2 + m∆, g(y∗
i2

)),

· · · , (xin + m∆, g(y∗
in

))},

where ∆, m, g(·) are the trigger, a mask which denotes where
the trigger is deployed, and a function that maps a source
class to target class (that is, g(i) = j, i, j ∈ {1, 2, . . . C}, i (cid:54)=
j) respectively. However, a backdoor attack can result in
output distributions (i.e., probabilities that the output belongs
to a speciﬁc class) from a Trojaned model being different to
that from a non-Trojaned model, even though both models
will have similar accuracy on clean samples.

Our objective is to demonstrate the limitations of defense
mechanisms that seek to distinguish between outputs from a
Trojaned model and a non-Trojaned model can be bypassed.
To this end, we seek to learn a Trojaned model that has an
output distribution on clean samples which is similar to that
from a non-Trojaned model. The two-stage setup that we use
is shown in Figure 2. We ﬁrst learn such a Trojaned model
using a non-Trojaned model as a teacher. Then, we maximize

The process of transferring knowledge from a ‘teacher’
model that has high accuracy to a smaller ‘student’ model is
called knowledge distillation [47]. Unlike ‘hard’ labels which
assign a sample to exactly one class with probability 1, ‘soft’
labels assign a probability distribution over output classes.
The non-zero probabilities provide information about inter-
class similarities. Knowledge distillation improves accuracy
of the student model using ‘soft’ labels provided by the
teacher. In order to ensure similarities in behaviors of a
Trojaned model and a non-Trojaned model, we use the non-
Trojaned as a teacher for the Trojaned model, and minimize
the difference between their outputs for a given input.

Let the DNNs that comprise the Trojaned model be param-
eterized by θ, and those comprising the non-Trojaned model
be parameterized by θ(cid:48)(cid:48). Assume that zT = FT (.; θ) and
zN = F (.; θ(cid:48)(cid:48)) denote the logits from the two models, and
C be the number of output classes. Knowledge distillation
techniques minimize the distance between the outputs of
teacher and student models for clean samples. The authors
of [47] established that when using gradient-based techniques
and soft labels as an input to a loss function, the gradients
were scaled by 1/T 2. For e.g., when using the L2-norm to
measure the distance between the models’ outputs, following
Eqn (1) and assuming that logits are zero mean without loss
of generality, the gradient of the loss function is:

0.5(p(zN , T ) − p(zT , T ))2

∂
∂zi
T
1
T

=

≈

1
T

T /T

(cid:80)

ezi
T /T
j ezj
1 + zi
C + (cid:80)

(

(cid:80)

ezi
N /T
j ezj

N /T

−

ezi
T /T
j ezj

T /T

(cid:80)

)

T /T
j zj

T /T

(

1 + zi
C + (cid:80)

N /T
j zJ

N /T

−

1 + zi
C + (cid:80)

T /T
j zj

T /T

)

≈

1
C 2T 2 zi

T (zi

N − zi

T )

To compensate for the 1/T 2 factor obtained in the above
gradient-calculation, we will scale by T 2 when updating
parameters θ of the DNN (see Line 12 of Algorithm 1). The
gradients for other loss functions like the Kullback-Liebler
(KL)-divergence, LKL, and cross-entropy loss, LCE can be
computed in a similar manner.

The Kullback-Leibler divergence loss function quantiﬁes
the distance between the distributions of the outputs of two
models and is deﬁned as:

LKL(p(zN , T ), p(zT , T )) = p(zN , T ) log

p(zN , T )
p(zT , T )

(3)

Knowledge distillation techniques also train the Trojaned
model on hard labels together with soft labels. The cross
entropy loss function measures the difference between the
the softmax of the logit of the training (Trojaned) model,
denoted T = 1, and the true label of the input as follows:

LCE(y∗, p(zT , T = 1)) = −

y∗
j log pj(zT , T = 1)

(cid:88)

j

(4)

= − log pi(zT , T = 1)

where y∗ is the true class of the input and pi(zT , 1) is the
ith element of the softmax function’s output. Since a hard
label (y∗) assigns an input to one class, all elements have
value of 0, except the ith element, which has a values of
1 if the input belongs to class i. Algorithm 1 explains the
knowledge distillation step in detail.

(x, y) ← random-selection(D)
D(cid:48) ← D(cid:48) ∪ (x + m∆, g(y))

Algorithm 1 Knowledge Distillation
Require: D, ∆, m, g(.), T ∗, FN , α1 > 0, α2 > 0
1: D(cid:48) ← D
2: for j = 1 : n do
3:
4:
5: end for
6: for i = 1 : itr do
7:
8:
9:
10:
11:

for (xk, yk) ∈ D do
zT ← FT (xk; θ)
zN ← FN (xk; θ(cid:48)(cid:48))
q1 ← p(zN , T = T ∗)
q2 ← p(zT , T = T ∗)
L1 ← L1 + T 2 × ∂

∂θ LKL(q1, q2)

12:
13:
14:
15:
16:
17:
18:
19: end for

end for
for (xk, yk) ∈ D(cid:48) do
zk
T ← FT (xk; θ)
L2 ← L2 + × ∂

end for
θ ← θ − α1L1 − α2L2

∂θ LCE(yk, p(zT , T = 1))

B. Min-Max Optimization

We assume that a defender has access to only the outputs
of a Trojaned model. This is termed black-box access, and
is a reasonable assumption when machine learning-enabled

cyber and cyber-physical systems are deployed in the real
world. In order to determine whether a model is Trojaned
or not using only outputs of the model, the defender uses
a discriminator, D. The discriminator is a classiﬁer with
two classes- YES (s = 1) and NO (s = 0). Learning
a discriminator (parameterized by φ) involves taking a set
of outputs of a model corresponding to a set of random
inputs and assigning it
is
non-Trojaned, and s = 1 if it
is Trojaned. We deﬁne
ˆD := ∪j{(p(FN (xj; θ(cid:48)(cid:48)), T = 1), 0), (p(FT ;θ(xj), T =
1), 1)}, where FN (·, θ(cid:48)(cid:48)) and FT (·, θ) are functions of non-
Trojaned and Trojaned models parameterized by θ(cid:48)(cid:48) and θ,
respectively. The discriminator minimizes a loss function
derived from the cross-entropy loss LCE, given by:
(cid:88)

to class s = 0 if the model

LCE(D(q; φ), s)

(5)

L =

1
2N (cid:48)

(q,s)∈ ˆD
(q=pN ,s=0) or(q=pT ,s=1)

The objective of the adversary, on the other hand, is to
ensure that the backdoor in the Trojaned model remains
undetectable (i.e., fool the discriminator). Consequently, she
updates hyperparameters of the Trojaned model in a manner
that will maximize the loss of the discriminator:

max
θ

min
φ

LCE(D(F (x; θ); φ), 1)

(6)

Algorithm 2 explains the min-max step in detail. In the
min step, we ﬁrst generate a set of arbitrary inputs (images,
in our case that are generated from a N (µ, σ) distribution)
and provide them to both non-Trojaned and Trojaned mod-
els. The outputs from these models are used to train the
discriminator. In the max step, we update parameters of the
Trojaned model to maximize the loss of the discriminator
by generating outputs that are similar to outputs of the non-
Trojaned model for the arbitrarily generated inputs. In order
to preserve accuracy of the model on any input (clean or
triggered), we consider a set D(cid:48) that contains both types of
inputs, and minimize a cross-entropy loss (Lines 10-14).

Algorithm 2 Min-Max Optimization for Discriminator
Require: FN (., θ(cid:48)(cid:48)), FT (.; θ), µ, σ, D(cid:48), α1, α2, α3 > 0
1: for i = 1 : itr do
2:

for i = 1 : N (cid:48) do

img ← N (µ, σ)
ˆD ← ˆD ∪ {p((FN (img; θ(cid:48)(cid:48)), T = 1), 0)}
ˆD ← ˆD ∪ {(p(FT (img; θ), T = 1), 1)}

3:
4:
5:
6:
7:
8:

9:

10:
11:

(cid:80)

(q,s)∈ ˆD

end for
L1 = 1
2N (cid:48)
φ ← φ − α1L1
(cid:80)
L2 = 1
xi∼N (µ,σ)
N (cid:48)
for (xk, yk) ∈ D(cid:48) do
zk
T ← FT (xk; θ)
L3 = L3 + ∂

∂
∂φ LCE(D(q; φ), s)

∂
∂θ LCE(D(p(FT (x; θ)); φ), 1)

∂θ LCE(yk, p(zk

T , T = 1))

end for
θ ← θ − α1L1 + α2L2 − α3L3

12:
13:
14:
15: end for

Model
Non-Trojaned model
Trojaned model
Trojaned model with
Knowledge Distillation
Our approach

Accuracy
on clean
99.3%99.3%99.3%
91.92%

backdoor
success rate
-
81.06%

Discriminator
Detection Rate
100%
96.3%

98.74%
90.21%

87.39%
96.82
96.82
96.82

99.40%
0.0%0.0%0.0%

TABLE I: Comparison of models: non-Trojaned, Trojaned
trained with hard labels, Trojaned trained using only knowl-
edge distillation, and Trojaned trained using knowledge
distillation and max-min optimization (ours). Non-Trojaned
models have the highest accuracy on inputs without a trigger
(clean). Our approach results in the highest success rate for
a multi-targeted backdoor attack, and completely bypasses a
discriminator that aims to distinguish between outputs from
a Trojaned and a non-Trojaned model (0.0% in last row).

V. EVALUATION

This section introduces our simulation setup and then

explains results of our empirical evaluations.

A. Simulation Setup

We use the MNIST dataset [48] to evaluate Algorithms 1
and 2. This dataset contains 60000 images of hand-written
digits ({0, 1, · · · , , 9}), of which 50k are used for training
and 10k for testing, and each image is of size 28 × 28. A
square of size 4 × 4 at an arbitrary location in the image is
used as the trigger (shown in Figure 3).

In order to learn a multi-target backdoor, we select a
random subset of images from the training data that have
been stamped with the trigger. Let i denote the true class of
the input that is stamped with the trigger, and C denote the
total number of classes (C = 10 for MNIST). Then, these
inputs are labeled according to g(i) := (i + 1) mod C.

We use the recently proposed MTND defense [16] as
a benchmark. MTND learns a discriminator that takes the
output of a target model to return a ‘score’. If this score
exceeds a pre-deﬁned threshold, the model is identiﬁed as
Trojaned, and is identiﬁed as non-Trojaned otherwise.

The DNNs used to learn a classiﬁer for the MNIST dataset
layers, each constaining 5
consists of two convolutional
kernels, and channel sizes of 16 and 32 respectively. This is
followed by maxpooling and fully connected layers of size
512. For learning the discriminator, similar to [16], we use a
network with one (fully connected) hidden layer of size 20.

B. Experiment Results

To demonstrate the limitations of existing defense mecha-
nisms against backdoor attacks, we train the following mod-
els: (i) Trojaned, with multi-target backdoor using knowl-
edge distillation and min-max optimization (Our Trojaned
Model), (ii) Trojaned, with multi-target backdoor using only
hard labels (Traditional Trojaned Model), and (iii) non-
Trojaned, (Non-Trojaned Model).

Table I indicates the accuracy of these three models on
clean inputs (i.e., images that are not stamped with a trigger),
success rates of a multi-target backdoor attack, and detection

(0,1)

(1,2)

(2,3)

(3,4)

(4,5)

(5,6)

(6,7)

(7,8)

(8,9)

(9,0)

Fig. 3: The MNIST dataset that contains 10 classes, corre-
sponding to the 10 digits. Each image is stamped with the
trigger at a random location (yellow square). The caption
below each image shows (predicted label from non-Trojaned
model, predicted label from our Trojaned model). The non-
Trojaned model predicts the image labels correctly.

Fig. 4: Detection rates of the discriminator for a non-
Trojaned model (red) and our Trojaned model (green) during
30 rounds of Algorithm 2. The discriminator is not able to
optimize its hyper-parameters to detect both non-Trojaned
model and our Trojaned model beyond 20 rounds.

rates of a discriminator. The non-Trojaned model has the
highest accuracy on clean inputs, since backdoor attacks
decrease the accuracy of the models. Knowledge distillation
is seen to improve the accuracy of Trojaned models on
clean samples and success rates of a backdoor attack, but
a Trojaned model trained using knowledge distillation alone
can be detected by a discriminator with 99.4% accuracy.
When knowledge distillation is combined with min-max
optimization, we see that the accuracy on clean inputs is
reduced, but the success rate of the backdoor attack is higher.
At the same time, the discriminator is not able to distinguish
between outputs from a Trojaned and a non-Trojaned model
(0.0% in righmost column of last row). This demonstrates
that the state-of-the-art MTND defense can be bypassed.

Figure 4 shows detection rates of the discriminator during
different rounds of Algorithm 2. The discriminator was not
able to achieve a high detection rate on both Trojaned
and non-Trojaned models when min-max optimization was
deployed for more than 20 rounds.

To evaluate the impact of the (pre-deﬁned) threshold

1) Extension to other domains: Our solution approach
focused on the setup where inputs to a DNN was images,
and we showed that the state-of-the-art MTND defense [16]
could be bypassed. Recent research has demonstrated that
DNNs designed for other tasks such as text classiﬁcation and
generation [31], code completion [49], and decision making
in reinforcement learning and cyber-physical systems [10]
are also vulnerable to backdoor attacks. We believe that our
solution methodology can be applied to domains where the
inputs and outputs of a DNN are continuous valued such as
speech [50] and deep reinforcement learning [3]. We will
evaluate our two-step approach on backdoor attacks carried
out on these types of systems, and examine the limitations
of defenses against backdoor attacks in settings where inputs
to a DNN classiﬁer are discrete-valued (e.g., text).

2) Provable Guarantees: The nested and inherently non-
linear structure of DNNs makes it challenging to explain
the decision-making procedures for given input data [51].
This challenge also holds true when investigating the explain-
ability of defense mechanisms and their limitations against
backdoor attacks on DNNs. We believe that developing a
principled approach that enables establishing provable guar-
antees on the accuracy of certain classes of DNNs- e.g., when
the activation function is a rectiﬁed linear unit (ReLU) [52]
will be a promising step in this direction.

3) Building Better Defenses: In this paper, the discrim-
inator used outputs to an arbitrary set of image inputs
to determine whether the model was Trojaned or not. An
interesting question to answer is if the similarity between
outputs from a Trojaned and non-Trojaned model can be
characterized for any input that does not contain a trigger.
Quantifying the change in the trigger/ trigger pattern that
will result in a change in the decision of a Trojaned model
compared to a non-Trojaned model is a possible solution
approach. This will help learn and reason about the dynamics
of the decision boundaries of DNN classiﬁers to enable
building better defenses against backdoor attacks.

VII. CONCLUSION

This paper studied machine learning models that were
vulnerable to backdoor attacks. Such a model is called a
Trojaned model. We identiﬁed a limitation of a state-of-the-
art defense mechanism that was designed to protect against
backdoor attacks. We proposed a new class of multi-target
backdoor attacks in which a single trigger could result in
misclassiﬁcation to more than one target class. We then
designed a two-step procedure that used knowledge distil-
lation and min-max optimization to ensure that outputs from
a Trojaned model were indistinguishable to a non-Trojaned
model. Through empirical evaluations, we demonstrated that
our approach was able to completely bypass a state-of-the art
defense mechanism, MTND. We demonstrated a reduction
in detection accuracy of the discriminator of MTND from
> 96% without our method to 0% when using our approach.
We also discussed ways to extend our methodology to other
classes of DNN models beyond those that use images,
establish provable guarantees, and build better defenses.

Fig. 5: The ROC curve for different threshold values of
the discriminator. There is no threshold that simultaneously
returns low false positive and high true positive rates for
detecting our Trojaned model. In comparison, for traditional
Trojaned models, a very small threshold returns a low false
positive rate and high true positive rate simultaneously.

values associated with the discriminator on the detection
rate, we plot and receiver operating characteristic (ROC)
curve that compares the true positive rate (TPR) versus false
positive rate (FPR) at different classiﬁcation thresholds:

T P R :=

T P
T P + F N

, F P R :=

T P
T P + F N

where T P denotes the number of Trojaned models that
are correctly identiﬁed as Trojaned and F N is the number
of non-Trojaned models that are incorrectly identiﬁed as
Trojaned by the discriminator. The discriminator returns a
score for each input. Inputs with a score exceeding a pre-
deﬁned threshold will be assigned to class s = 1 (Trojaned).
Figure 5 demonstrates that for any value of the threshold,
the discriminator is not able to simultaneously return low
F P R and high T P R for Trojaned models trained using our
approach. However, for a traditional Trojaned model, the
discriminator is able to simultaneously return a low F P R
and high T P R for small threshold values.

We also computed the area under the ROC curve (AUC)
to measure the quality of the discriminator’s predictions,
independent of the chosen threshold value. The AUC is a
number in the range [0, 1], and a model with an accuracy
of 100% has AU C = 1. We determined that our Trojaned
model had AU C = 0.495, while the traditional Trojaned
model had AU C = 0.994. These AUC values indicate that
our two-step approach results in a discriminator performance
in distinguishing between outputs from a Trojaned and non-
Trojaned model that is as good as a ‘coin-toss’ guess (i.e.,
selecting one of two possibilities, with probability 0.5).

VI. DISCUSSION

In this section, we identify how our approach can be
extended to domains where inputs might not be images,
and highlight some open questions and challenges that are
promising future research directions.

REFERENCES

[1] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “DeepFace: Closing
the gap to human-level performance in face veriﬁcation,” in IEEE
Conference on Computer Vision and Pattern Recognition, 2014.
[2] C. Zhang, P. Patras, and H. Haddadi, “Deep learning in mobile and
wireless networking: A survey,” IEEE Communications Surveys &
Tutorials, vol. 21, no. 3, pp. 2224–2287, 2019.

[3] V. Mnih et al., “Human-level control through deep reinforcement

learning,” Nature, vol. 518, no. 7540, 2015.

[4] A. Esteva, A. Robicquet, B. Ramsundar, V. Kuleshov, M. DePristo,
K. Chou, C. Cui, G. Corrado, S. Thrun, and J. Dean, “A guide to
deep learning in healthcare,” Nature medicine, vol. 25, no. 1, 2019.

[5] “Amazon, Machine learning at AWS, 2018.”
[6] “BigML Inc. bigml, 2018.”
[7] “Caffe, Caffe Model Zoo, 2018.”
[8] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “BadNets: Evaluating
backdooring attacks on deep neural networks,” IEEE Access, vol. 7,
pp. 47 230–47 244, 2019.

[9] F. Ullah, H. Naeem, S. Jabbar, S. Khalid, M. A. Latif, F. Al-Turjman,
and L. Mostarda, “Cyber security threats detection in internet of things
using deep learning approach,” IEEE Access, vol. 7, 2019.

[10] K. Panagiota, W. Kacper, S. Jha, and L. Wenchao, “TrojDRL: Trojan
attacks on deep reinforcement learning agents,” in ACM/IEEE Design
Automation Conference, 2020.

[11] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending
against backdooring attacks on deep neural networks,” in Interna-
tional Symposium on Research in Attacks, Intrusions, and Defenses.
Springer, 2018, pp. 273–294.

[12] K. Yoshida and T. Fujino, “Disabling backdoor and identifying poison
data by using knowledge distillation in backdoor attacks on deep
neural networks,” in ACM Workshop on Artiﬁcial Intelligence and
Security, 2020, pp. 117–127.

[13] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma, “Neural attention
distillation: Erasing backdoor triggers from deep neural networks,”
International Conference on Learning Representations, 2021.
[14] S. Kolouri, A. Saha, H. Pirsiavash, and H. Hoffmann, “Universal
litmus patterns: Revealing backdoor attacks in CNNs,” in IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2020.
[15] Y. Liu, Y. Xie, and A. Srivastava, “Neural Trojans,” in International

Conference on Computer Design.

IEEE, 2017, pp. 45–48.

[16] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, and B. Li, “Detecting
AI Trojans using meta neural analysis,” in IEEE Symposium on
Security and Privacy.

IEEE, 2021, pp. 103–120.

[17] Y. Liu, X. Ma, J. Bailey, and F. Lu, “Reﬂection backdoor: A natural
backdoor attack on deep neural networks,” in European Conference
on Computer Vision. Springer, 2020, pp. 182–199.

[18] Y. Li, T. Zhai, B. Wu, Y. Jiang, Z. Li, and S. Xia, “Rethinking the
trigger of backdoor attack,” arXiv preprint arXiv:2004.04692, 2020.
[19] E. Bagdasaryan and V. Shmatikov, “Blind backdoors in deep learning
models,” in USENIX Security Symposium, 2021, pp. 1505–1521.
[20] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in
the physical world,” International Conference on Learning Represen-
tations, 2017.

[21] S. M. Moosavi Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: A
simple and accurate method to fool deep neural networks,” in IEEE
Conference on Computer Vision and Pattern Recognition, 2016.
[22] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and har-
nessing adversarial examples,” International Conference on Learning
Representations, 2015.

[23] J. Dumford and W. Scheirer, “Backdooring convolutional neural
networks via targeted weight perturbations,” in International Joint
Conference on Biometrics.

IEEE, 2020, pp. 1–9.

[24] A. S. Rakin, Z. He, and D. Fan, “TBT: Targeted neural network attack
with bit trojan,” in IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2020, pp. 13 198–13 207.

[25] A. Turner, D. Tsipras, and A. Madry, “Label-consistent backdoor

attacks,” arXiv preprint arXiv:1912.02771, 2019.

[26] S. Li, M. Xue, B. Z. H. Zhao, H. Zhu, and X. Zhang, “Invisible
backdoor attacks on deep neural networks via steganography and regu-
larization,” IEEE Transactions on Dependable and Secure Computing,
vol. 18, no. 5, pp. 2088–2105, 2020.

[27] C. Liao, H. Zhong, A. Squicciarini, S. Zhu, and D. Miller, “Backdoor
embedding in convolutional neural network models via invisible
perturbation,” arXiv preprint arXiv:1808.10307, 2018.

[28] S. Wang, S. Nepal, C. Rudolph, M. Grobler, S. Chen, and T. Chen,
transfer learning with pre-trained deep

“Backdoor attacks against
learning models,” IEEE Transactions on Services Computing, 2020.

[29] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang,
“Trojaning attack on neural networks,” Network and Distributed
Systems Security (NDSS) Symposium, 2018.

[30] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to
backdoor federated learning,” in International Conference on Artiﬁcial
Intelligence and Statistics. PMLR, 2020, pp. 2938–2948.

[31] J. Dai, C. Chen, and Y. Li, “A backdoor attack against LSTM-based

text classiﬁcation systems,” IEEE Access, vol. 7, 2019.

[32] Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, “Turning your
weakness into a strength: Watermarking deep neural networks by
backdooring,” in USENIX Security Symposium, 2018, pp. 1615–1631.
[33] D. M. Sommer, L. Song, S. Wagh, and P. Mittal, “Towards
probabilistic veriﬁcation of machine unlearning,” arXiv preprint
arXiv:2003.04247, 2020.

[34] M. Xue, C. He, J. Wang, and W. Liu, “One-to-N & N-to-one:
Two advanced backdoor attacks against deep learning models,” IEEE
Transactions on Dependable and Secure Computing, 2020.

[35] Y. Xiao, L. Cong, Z. Mingwen, W. Yajie, L. Xinrui, S. Shuxiao,
M. Yuexuan, and Z. Jun, “A multitarget backdooring attack on deep
neural networks with random location trigger,” International Journal
of Intelligent Systems, vol. 37, no. 3, pp. 2567–2583, 2022.

[36] A. Rajabi and R. B. Bobba, “Adversarial Proﬁles: Detecting out-
distribution & adversarial samples in pre-trained cnns,” arXiv preprint
arXiv:2011.09123, 2020.

[37] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Uni-
versal adversarial perturbations,” in IEEE Conference on Computer
Vision and Pattern Recognition, 2017, pp. 1765–1773.

[38] M. Du, R. Jia, and D. Song, “Robust anomaly detection and backdoor
attack detection via differential privacy,” International Conference on
Learning Representations, 2020.

[39] B. Tran, J. Li, and A. Madry, “Spectral signatures in backdoor attacks,”

Neural Information Processing Systems, vol. 31, 2018.

[40] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee,
I. Molloy, and B. Srivastava, “Detecting backdoor attacks on deep
neural networks by activation clustering,” AAAI Workshop, 2019.
[41] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh,
and D. Batra, “Grad-CAM: Visual explanations from deep networks
via gradient-based localization,” in IEEE International Conference on
Computer Vision, 2017, pp. 618–626.

[42] S. Udeshi, S. Peng, G. Woo, L. Loh, L. Rawshan, and S. Chattopad-
hyay, “Model agnostic defence against backdoor attacks in machine
learning,” arXiv preprint arXiv:1908.02203, 2019.

[43] M. Villarreal-Vasquez and B. Bhargava, “ConFoc: Content-focus pro-
trojan attacks on neural networks,” arXiv preprint

tection against
arXiv:2007.00711, 2020.

[44] X. Huang, M. Alzantot, and M. Srivastava, “NeuronInspect: Detecting
backdoors in neural networks via output explanations,” arXiv preprint
arXiv:1911.07399, 2019.

[45] R. Wang, G. Zhang, S. Liu, P.-Y. Chen, J. Xiong, and M. Wang,
“Practical detection of Trojan neural networks: Data-limited and data-
free cases,” in European Conference on Computer Vision. Springer,
2020, pp. 222–238.

[46] H. Chen, C. Fu, J. Zhao, and F. Koushanfar, “DeepInspect: A black-
box Trojan detection and mitigation framework for deep neural net-
works.” in International Joint Conference on Artiﬁcial Intelligence,
vol. 2, no. 5, 2019, p. 8.

[47] G. Hinton, O. Vinyals, J. Dean et al., “Distilling the knowledge in a
neural network,” arXiv preprint arXiv:1503.02531, vol. 2, no. 7, 2015.
[48] Y. LeCun, “The MNIST database of handwritten digits,” http://yann.

lecun. com/exdb/mnist/, 1998.

[49] R. Schuster, C. Song, E. Tromer, and V. Shmatikov, “You autocomplete
me: Poisoning vulnerabilities in neural code completion,” in USENIX
Security Symposium, 2021, pp. 1559–1575.

[50] T. Zhai, Y. Li, Z. Zhang, B. Wu, Y. Jiang, and S.-T. Xia, “Backdoor
attack against speaker veriﬁcation,” in International Conference on
Acoustics, Speech and Signal Processing, 2021, pp. 2560–2564.
[51] W. Samek, T. Wiegand, and K.-R. M¨uller, “Explainable artiﬁcial
intelligence: Understanding, visualizing and interpreting deep learning
models,” arXiv preprint arXiv:1708.08296, 2017.

[52] R. Arora, A. Basu, P. Mianjy, and A. Mukherjee, “Understanding deep
neural networks with rectiﬁed linear units,” International Conference
on Learning Representations, 2018.

