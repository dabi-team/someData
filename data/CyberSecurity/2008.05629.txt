A Differentially Private Game Theoretic Approach
for Deceiving Cyber Adversaries

Dayong Ye, Tianqing Zhu*, Sheng Shen and Wanlei Zhou

0
2
0
2

g
u
A
3
1

]

R
C
.
s
c
[

1
v
9
2
6
5
0
.
8
0
0
2
:
v
i
X
r
a

Abstract—Cyber deception is one of the key approaches used
to mislead attackers by hiding or providing inaccurate system
information. There are two main factors limiting the real-world
application of existing cyber deception approaches. The ﬁrst
limitation is that the number of systems in a network is assumed
to be ﬁxed. However, in the real world, the number of systems
may be dynamically changed. The second limitation is that
attackers’ strategies are simpliﬁed in the literature. However,
in the real world, attackers may be more powerful than theory
suggests. To overcome these two limitations, we propose a novel
differentially private game theoretic approach to cyber deception.
In this proposed approach, a defender adopts differential privacy
mechanisms to strategically change the number of systems and
obfuscate the conﬁgurations of systems, while an attacker adopts
a Bayesian inference approach to infer the real conﬁgurations of
systems. By using the differential privacy technique, the proposed
approach can 1) reduce the impacts on network security resulting
from changes in the number of systems and 2) resist attacks re-
gardless of attackers’ reasoning power. The experimental results
demonstrate the effectiveness of the proposed approach.

I. INTRODUCTION

Network security is one of the most important problems
faced by enterprises and countries today [1]. Before launching
a network attack, malicious attackers often scan systems in
a network to identify vulnerabilities that can be exploited to
intrude into the network [2]. The aim of this scanning is
to understand the conﬁgurations of these systems, including
the operating systems they are running, and their IP/MAC
addresses on the network. Once these questions are answered,
attackers can efﬁciently formulate plans to attack the network.
In order to prevent attackers from receiving true answers to
these questions and thus reduce the likelihood of successful
attacks, cyber deception techniques are employed [3].

Instead of stopping an attack or identifying an attacker,
cyber deception techniques aim to mislead an attacker and
induce him to attack non-critical systems by hiding or lying
about the conﬁgurations of the systems in a network [4].
For example, an important system s with conﬁguration k
is obfuscated by the defender to appear as a less important
system with conﬁguration k(cid:48). Thus, when an attacker scans
the network, he observes system s with conﬁguration k(cid:48) rather
than k, as shown in Fig. 1. Since conﬁguration k(cid:48)
is less
important than k, the attacker may skip over system s.

To model such an interaction between an attacker and
a defender, game theory has been adopted as a means of

*Tianqing Zhu is the corresponding author. D. Ye, T. Zhu, S. Shen and
W. Zhou are with the Centre for Cyber Security and Privacy and the School
of Computer Science, University of Technology, Sydney, Australia. Email:
{Dayong.Ye, Tianqing.Zhu, 12086892, Wanlei.Zhou}@uts.edu.au

Fig. 1. System s with conﬁguration k is obfuscated to and appears as
conﬁguration k(cid:48)

studying cyber deception [5], [6]. Game theory is a theoretical
framework to study the decision-making strategies of compet-
ing players, where each player aims to maximize her or his
own utility. In cyber deception, defenders and attackers can
be modelled as players. Game theory, thus, can be used to
investigate how a defender reacts to an attacker and vice versa.
Game theoretic formulation overcomes traditional solutions to
cyber deception in many aspects, such as proven mathematics,
reliable defense mechanisms and timely actions [7].

Existing game theoretic approaches, however, have two
common limitations. The ﬁrst limitation is that the number of
systems in a network is assumed to be ﬁxed. This assumption
hinders the applicability of existing approaches to real-world
applications, because in the real world, the number of systems
in a network may be dynamically changed. Although some
existing approaches, like [8], are computationally feasible to
recompute the strategy when new systems are added, this
change in the number of systems may affect the security
of a network [9], [10]. For example, a network has three
systems: s1, s2 and s3. The three systems are associated
with two conﬁgurations, where s1 and s2 are associated
with conﬁguration k1, and s3 is associated with conﬁguration
k2. The attacker knows that the network has three systems
and two conﬁgurations. He also knows that conﬁguration
k1 has two systems and conﬁguration k2 has one system.
He, however, is unaware of which system is associated with
which conﬁguration. Now when system s3 is removed by the
defender, the attacker knows that conﬁguration k1 has two
systems and conﬁguration k2 has zero system. By comparing
the two results, the attacker can deduce that system s3 is
associated with conﬁguration k2. After deducing system s3’s
conﬁguration, the attacker can also deduce that systems s1
and s2 are associated with conﬁguration k1. Similarly, when
a new system s4 is added and associated with conﬁguration
k2, the attacker can immediately deduce the conﬁguration of
system s4 because he realizes that the number of systems in
conﬁguration k2 increases by 1. Therefore, the problem of how
to strategically change the number of systems in a network

Conf k -[os] Windows-[Web] Nginx-[file] InternalSystem s hasSystem s appears asObfuscatedConf k-[os] Linux-[Web] Tomcat-[file] Sensitivty 
 
 
 
 
 
without sacriﬁcing its security is challenging.

The second limitation is that attackers’ approaches are
simpliﬁed in the literature through the use of only greedy-
like approaches. Greedy-like approaches create deterministic
strategies which are highly predictable to defenders. Hence,
using these approaches may underestimate the ability of real-
world attackers who can use more complex approaches that are
much less predictable to defenders. If a defender’s approach
is developed based on simpliﬁed attackers’ approaches, the
defender may be easily compromised by real-world attackers.
Thus, the problem of how to develop an efﬁcient defender’s
approach against powerful attackers is also challenging [11].
Accordingly, to overcome the above two limitations, in this
paper, we propose a novel differentially private game theoretic
approach to cyber deception. By using the differential privacy
technique [12], the change in the number of systems does
not affect the security of a network, as an attacker cannot
determine whether a given system is inside or outside of
the network. Thus, the attacker cannot deduce each system’s
true conﬁguration. Moreover, by using the differential privacy-
based approach, a defender’s strategy is unpredictable to an
attacker, irrespective of his reasoning power.

In summary, the contribution of this paper is two-fold.
1) To the best of our knowledge, we are the ﬁrst to apply
differential privacy to the cyber deception game in order to
overcome the two common limitations mentioned above.

2) We theoretically illustrate the properties of the proposed

approach, and experimentally demonstrate its effectiveness.

II. RELATED WORK

In this section, we ﬁrst review related works about game
theory for cyber deception and next discuss their limitations.
A thorough survey on game theory for general network and
cyber security can be found in [13], [14], [7], [5].

A. Review of related works

Albanese et al. [15] propose a deceptive approach to de-
feat an attacker’s effort to ﬁngerprint operating systems and
services. For operating system ﬁngerprinting, they manipulate
the outgoing trafﬁc to make it resemble trafﬁc generated by
a different system. For service ﬁngerprinting, they modify
the service banner by intercepting and manipulating certain
packets before they leave the network.

Albanese et al. [4] present a graph-based approach to
manipulate the attacker’s view of a system’s attack surface.
They formalize system views and deﬁne the distance between
these different views. Based on this formalization and deﬁ-
nition, they develop an approach to confuse the attacker by
manipulating responses to his probes in order to induce an
external view of the system, which can minimize the costs
incurred by the defender.

Jajodia et al. [8] develop a probabilistic logic of deception
and demonstrate that these associated computations are NP-
hard. Speciﬁcally, they propose two algorithms that allow the
defender to generate faked scan results in different states to
minimize the damage caused by the attacker.

Wang and Zeng [16] propose a two-stage deception game
for network protection. In the ﬁrst stage, the attacker scans
the whole network and the defender decides how to respond
these scan queries to distract the attacker from high value
hosts. In the second stage, the attacker selects some hosts
for further probing and the defender decides how to answer
these probes. The defender’s decisions are formulated as
optimization problems solved by heuristic algorithms.

Schlenker et al. [17] propose a game theoretic approach to
deceive cyber adversaries. Their approach involves two types
of attackers: a rational attacker and a naive attacker. The ra-
tional attacker knows the defender’s deception strategy, while
the naive attacker does not. Two optimal deception algorithms
are developed to counter these two types of attackers.

Bilinski et al. [18] propose a simpliﬁed cyber deception
game model. In their model, there are only two machines,
where one is real and the other is fake. At each round, the
attacker is allowed to ask one machine about its type, and the
machine can either lie or tell the truth about its type. After
N rounds, the attacker chooses one machine to attack. The
attacker will receive a positive payoff if he attacks the real
machine. Otherwise, he will receive a negative payoff.

Other game theoretic approaches have also been developed
[19], [20]. These, however, are not closely related to our work,
as they do not particularly focus on obfuscating conﬁgurations
of systems. Some game theoretic approaches [21], [22] mainly
focus on dealing with the interactions between defenders and
attackers, while other approaches [23], [24] aim at inves-
tigating the behaivors of defenders and attackers. Another
branch of related work deals with the repeated interactions of
defender and attacker over time following the “FlipIt” model
[25], [26], [27]. Their research focuses on stealthy takeovers,
where both the defender and the attacker want to take control
of a set of resources by ﬂipping them. By comparison, our
research focuses on how a defender modiﬁes the conﬁguration
of systems to deceive an attacker.

B. Discussion of related works

The above-reviewed works have two common limitations:
1) the number of systems in a network is assumed to be ﬁxed,
and 2) the attacker’s approaches are often simpliﬁed.

1) Fixed number of systems: In the real-world networks,
the number of systems is often changed in a dynamic manner.
Most of existing works, however, are conducted on the basis
of ﬁxed number of systems, though some works are computa-
tionally feasible to accommodate the increase of the number of
systems. An intuitive solution is to extend existing works by
enabling systems to be dynamically introduced or removed.
However, arbitrarily introducing or removing systems may
compromise the security of a network.

2) Simpliﬁed attackers:

In existing works, attackers are
considered to be either naive [8] or to use only greedy-like
approaches [17]. A naive attacker always attacks the observed
conﬁguration which has the highest utility. A greedy attacker
knows the defender’s deception scheme and attacks the conﬁg-
uration which has the expected highest utility. The drawback
of naive and greedy-like approaches is that the selection of

a conﬁguration is deterministic and highly predictable to a
defender. As real-world attackers are more powerful than the
simpliﬁed naive and greedy attackers, defenders’ approaches
developed based on simpliﬁed attackers’ approaches may not
be applicable to the real world.

In this paper, we develop a novel differentially private
game theoretic approach which can strategically change the
number of systems in a network without sacriﬁcing its se-
curity. Moreover, to model a powerful attacker, we adopt a
Bayesian inference approach. By using Bayesian inference,
the attacker can infer the probability with which an observed
conﬁguration k(cid:48) could be a real conﬁguration k, and selects
a conﬁguration to attack based on the probability distribution
over the observed conﬁgurations. Therefore, the selection of
a conﬁguration using the Bayesian inference approach is non-
deterministic and hardly predictable to the defender.

III. PRELIMINARIES

A. Cyber deception game

The cyber deception game (CDG)

is an imperfect-
information Stacklberg game between a defender and an
attacker [17], where a player cannot accurately observe the
actions of the other player [7]. The defender takes the ﬁrst
actions. She decides how the systems should respond to scan
queries from an attacker by obfuscating the conﬁgurations of
these systems. The attacker subsequently follows by choosing
which systems to attack based on his scan query results.
The game continues in this alternating manner, until a pre-
deﬁned number of rounds is reached. We use the imperfect
information game because in this cyber deception game, the
aim of the defender is to hide the conﬁgurations of systems
from the attacker rather than stopping an attack or identifying
the attacker. If we use a perfect information game, the de-
fenders previous strategies of obfuscating conﬁgurations can
be accurately observed by the attacker. Then, the attacker can
immediately deduce the real conﬁgurations of all the systems.
In this game, we use N to denote the set of systems in a
network protected by the defender. The number of systems
is denoted by |N |. Each system has a set of attributes: an
operating system, a ﬁle system, services hosted, and so on.
These attributes constitute a system conﬁguration. We use
K to denote the set of conﬁgurations, and the number of
conﬁgurations is denoted by |K|. According to the conﬁg-
urations, the system set N can be partitioned into a number
of subsets: N1, ..., N|K|, where Ni ∩ Nj = ∅ for any i (cid:54)= j,
and N1 ∪ ... ∪ N|K| = N . Each of the systems in subset Nk
has conﬁguration k and an associated utility uk. If a system
with conﬁguration k is attacked by the attacker, the attacker
receives utility uk and the defender loses utility uk. The utility
of a system depends on not only the importance of the system
but also on its security level. A well-chronicled ﬁnding of
information security is that attackers may go for the weakest
link, i.e., the system with the weakest security level, to have
a foothold and then try to progress from there via privilege
escalation [28]. Detailed theoretical discussion on the weakest
link can be found in [29], [30].

To mislead the attacker, the defender may obfuscate the
conﬁgurations of these systems. Obfuscating a system with

conﬁguration k to appear as k(cid:48) incurs the defender a cost
c(k(cid:48), k). When scanning, the attacker observes the obfuscated
conﬁgurations of these systems. If a system with conﬁguration
k is obfuscated to appear as k(cid:48), the system’s real conﬁguration
is still k rather than k(cid:48). Thus, when this system is attacked by
the attacker, the attacker receives utility uk instead of uk(cid:48).

After obfuscation, the system set N becomes N (cid:48), which
can be partitioned into: N (cid:48)
1, ..., N (cid:48)
|K|. Since |N | may not be
equal to |N (cid:48)|, there may be void systems (|N (cid:48)| > |N |), or
some systems may be taken ofﬂine (|N (cid:48)| < |N |). Void systems
can be interpreted as honeypots1. Deploying a honeypot with
conﬁguration k incurs the defender a cost hk. If the attacker
attacks the honeypot, he receives a negative utility −uk.
Usually, uk > hk, as the defender would not otherwise have
the motivation to deploy a honeypot. By taking a system
ofﬂine, the defender loses utility lk, but this system will not
be attacked by the attacker. Again, generally, uk > lk, as the
defender would not otherwise have the motivation to take a
system ofﬂine. While the defender aims to minimize her utility
loss, the attacker aims to maximize his utility gain.

The attacker is aware of the defender’s obfuscation ap-
proach, and knows the statistical information pertaining to
the number of systems |N |,
the number of
the network:
conﬁgurations |K|, and the number of the systems associated
with each conﬁguration |N1|, ..., |N|K||. The attacker, how-
ever, does not know which system is associated with which
conﬁguration. Moreover, as the aim of the defender is to hide
the conﬁgurations of systems, the defender’s strategies, i.e.,
when and how to make k appear as k(cid:48), are exactly what the
defender wants to hide and cannot be accurately observed by
the attacker. However, the attacker can partially observe the
defenders previous actions. As in the above example, during
each round, the attacker attacks a system which appears as
conﬁguration k(cid:48), but after the attack, the attacker receives
utility uk. The attacker, thus, knows that the attacked system
was obfuscated from k to k(cid:48)
in this round. The attacker
accumulates this information and uses Bayesian inference to
make decisions in future rounds. Speciﬁcally, in Bayesian
inference, the posterior probability q(k(cid:48)|k) takes this partial
observation into account by computing how many times k
appears as k(cid:48) in previous rounds. The details will be given in
Section IV.

B. Differential privacy

Differential privacy is a prevalent privacy model capable
of guaranteeing that any individual record being stored in
or removed from a dataset makes little difference to the
analytical output of the dataset [12]. Differential privacy has
been successfully applied to cyber physical systems [31],
machine learning [32] and artiﬁcial intelligence [33], [34].

In differential privacy, two datasets D and D(cid:48) are neigh-
boring datasets if they differ by only one record. A query
f is a function that maps dataset D to an abstract range R,
f : D → R. The maximal difference in the results of query

1Although our work also uses honeypots, these do not permanently exist in
the network, but are rather randomly introduced by our differentially private
approach. Our main aim is still to focus on the cyber deception.

f is deﬁned as sensitivity ∆S, which determines how much
perturbation is required for the privacy-preserving answer. The
formal deﬁnition of differential privacy is presented as follows.

Deﬁnition 1 ((cid:15)-Differential Privacy [35]). A mechanism M
provides (cid:15)-differential privacy for any pair of neighboring
datasets D and D(cid:48), and for every set of outcomes Ω, if M
satisﬁes:

P r[M(D) ∈ Ω] ≤ exp((cid:15)) · P r[M(D(cid:48)) ∈ Ω]

(1)

Deﬁnition 2 (Sensitivity [35]). For a query f : D → R, the
sensitivity of f is deﬁned as

∆S = max
D,D(cid:48)

||f (D) − f (D(cid:48))||1

(2)

Two of the most widely used differential privacy mecha-
nisms are the Laplace mechanism and the exponential mecha-
nism [36]. The Laplace mechanism adds Laplace noise to the
true answer. We use Lap(b) to represent the noise sampled
from the Laplace distribution with scaling b.

Deﬁnition 3 (The Laplace Mechanism [35]). Given a function
f : D → R over a dataset D, Equation 3 is the Laplace
mechanism.

(cid:98)f (D) = f (D) + Lap(

)

(3)

∆S
(cid:15)

3, respectively: N1 = {s1}, N2 = {s2} and N3 = {s3}. Each
conﬁguration is associated with a utility, such that systems
with the same conﬁguration have the same utility. Once a
system is attacked, the defender loses a corresponding utility,
while the attacker gains this utility. The aim of our approach is
to minimize the defender’s utility loss by using the differential
privacy technique to hide the real conﬁgurations of systems.
At each round of the game, our approach consists of four
steps: Steps 1 and 2 are carried out by the defender, while
Steps 3 and 4 are conducted by the attacker.

Step 1: The defender obfuscates the conﬁgurations. The
obfuscation is conducted on the statistical information of the
network according to Algorithm 1. In this example, the number
of systems in each conﬁguration is 1, i.e., |N1| = |N2| =
|N3| = 1, while the total number of systems in the network is
|N | = 3. After obfuscation, the number of systems in each
conﬁguration could be: |N (cid:48)
3| = 1,
while the total number of systems in the network becomes
|N (cid:48)| = |N (cid:48)

2| = 1, |N (cid:48)

1| = 2, |N (cid:48)

1| + |N (cid:48)

2| + |N (cid:48)

3| = 4.

Step 2: As |N (cid:48)|−|N | = 1, an extra system is introduced into
the network. This extra system is interpreted as a honeypot,
denoted as hp4. The defender uses Algorithm 3 to deploy
the systems and the honeypot
to the conﬁgurations. The
deployment result could be: N (cid:48)
2 = {s1} and
N (cid:48)
3 = {s2}, which is shown to the attacker as demonstrated
in Fig. 2.

1 = {s3, hp4}, N (cid:48)

Deﬁnition 4 (The Exponential Mechanism [35]). The expo-
nential mechanism ME selects and outputs an element r ∈ R
with probability proportional to exp( (cid:15)u(D,r)
2∆u ), where u(D, r)
is the utility of a pair of dataset and output, and ∆u =
|u(D, r) − u(D(cid:48), r)| is the sensitivity of
max
r∈R
utility.

max
D,D(cid:48):||D−D(cid:48)||1≤1

Table I presents the notations and terms used in this paper.

Fig. 2. The defender obfuscates conﬁgurations of systems

TABLE I
THE MEANING OF NOTATIONS USED IN THIS PAPER

Notations Meaning
N
K
N (cid:48)
uk
c(k(cid:48), k)

a set of systems
a set of conﬁgurations
a set of obfuscated systems
the utility of a system with conﬁguration k
cost to the defender of obfuscating a system from
conﬁguration k to appear as k(cid:48)
cost to the defender’s of deploying a honeypot
with conﬁguration k
utility loss incurred by the defender to take a
system with conﬁguration k ofﬂine
the defender’s total expected utility loss
the defender’s total cost
the defender’s deploy budget
the attacker’s total expected utility gain
the sensitivity of a query
the sensitivity of utility
privacy budget

hk

lk

Ud
Cd
Bd
Ua
∆S
∆u
(cid:15)

IV. THE DIFFERENTIALLY PRIVATE APPROACH

A. Overview of the approach

We provide an example here to describe the workﬂow
of our approach. In a network,
there are three systems
N = {s1, s2, s3} and three conﬁgurations K = {k1, k2, k3}.
System 1, 2 and 3 are associated with conﬁguration 1, 2 and

Step 3: The attacker estimates the probability with which
an observed conﬁguration k(cid:48) could be a real conﬁguration k
using Bayesian inference (Equation 10), i.e., estimating:

q(k1|k1), q(k2|k1) and q(k3|k1);
q(k1|k2), q(k2|k2) and q(k3|k2);
q(k1|k3), q(k2|k3) and q(k3|k3).
Step 4: Based on the estimation, the attacker uses Equation
11 to calculate the expected utility gain of selecting each
conﬁguration. Finally, the attacker selects a speciﬁc conﬁg-
uration as the target based on a probability distribution over
conﬁgurations (Equation 12).

B. The defender’s strategy

At each round of the game, the defender ﬁrst obfuscates the
conﬁgurations using the differentially private Laplace mecha-
nism, and deploys systems according to these conﬁgurations.
1) Step 1: obfuscating conﬁgurations: The obfuscation
is described in Algorithm 1. In Line 5, Laplace noise is
added to each |Nk| to obfuscate the number of systems with
conﬁguration k. In Line 5, ∆S denotes the sensitivity of the
maximum number of systems with a speciﬁc conﬁguration.
As this maximum number has a direct impact on the conﬁg-
uration arrangement of these systems, the sensitivity ∆S is

determined by the maximum number. Based on the deﬁnition
of sensitivity, ∆S = 1 in this algorithm.

The rationale underpinning Algorithm 1 is as follows.
Differential privacy is originally designed to preserve data
privacy in datasets. It can guarantee that an individual data
record in or out of a dataset has little impact on the analytical
output of the dataset. In other words, an attacker cannot infer
whether a data record is in the dataset by making queries to the
dataset. Here we use differential privacy to preserve the privacy
of systems in networks. The privacy of systems in this paper
means the conﬁgurations of systems. To map from differential
privacy to its meaning in the network conﬁguration, we treat a
network as a dataset and treat a system in the network as a data
record in the dataset. Since differential privacy can guarantee
that an attacker cannot infer whether a given data record is in
a dataset, it can also guarantee that an attacker cannot infer
whether a given system is in a network. Speciﬁcally, as we add
differentially private Laplace noise on the number of systems
in each conﬁguration, the attacker cannot infer whether a given
system is really associated with the shown conﬁguration. Thus,
the system’s real conﬁguration is preserved.

Algorithm 1: The obfuscation of conﬁgurations
1 Input: |N1|, ..., |N|K||;
2 Partition the system set N into N1, ..., N|K| based on the

conﬁgurations;

3 Calculate the size of each subset: |N1|, ..., |N|K||;
4 for k = 1 to |K| do

k| ← |Nk| + (cid:100)Lap( ∆S·|K|

(cid:15)

)(cid:101);

|N (cid:48)
5
6 Output: |N (cid:48)

1|, ..., |N (cid:48)

|K||;

1|, ..., |N (cid:48)

After Algorithm 1 is executed, the defender receives an
|K||. Let |N (cid:48)| = (cid:80)
output: |N (cid:48)
k|, so that
there are three possible situations: |N (cid:48)| = |N |, |N (cid:48)| > |N |
or |N (cid:48)| < |N |. When |N (cid:48)| > |N |, there are |N (cid:48)| − |N |
void systems, which can be interpreted as honeypots. When
|N (cid:48)| < |N |, there are |N | − |N (cid:48)| systems taken ofﬂine.

1≤k≤|K| |N (cid:48)

2) Step 2: deploying systems: System deployment will be
conducted according to three possible situations: |N (cid:48)| = |N |,
|N (cid:48)| > |N | or |N (cid:48)| < |N |. This deployment is based not only
on the utility of these systems, but also on the attacker’s attack
strategy in the previous rounds. The aim of this deployment
is to minimize the defender’s expected utility loss while
satisfying the defender’s budget constraint Bd.

Situation 1: When |N (cid:48)| = |N |, the defender’s expected

utility loss is shown in Equation 4,

Ud =

(cid:88)

(p(k) ·

(cid:88)

ui),

(4)

1≤k≤|K|

1≤i≤|Nk|

where p(k) is the estimated probability that the attacker will
attack the systems with conﬁguration k in the next round.
The estimated probability is the ratio between the number of
times that k was attacked to the total number of game rounds.
For example, if the game has been played for 10 rounds and
systems with conﬁguration k have been attacked 3 times, the
estimated probability is p(k) = 3
10 = 0.3. Initially, before the
ﬁrst round, the probability distribution over the conﬁgurations

is uniform. The probability distribution is then updated every
10 rounds using the method described in the example. Since
the game is an imperfect information game, the defender is
unaware of the real attack probability of the attacker. The
defender can use only her past experience to make a strategy,
i.e., past observation of the attackers strategies.

During the deployment, the defender’s cost is shown in

Equation 5,

(cid:88)

Cd =

Ii · (c(k(cid:48), k)),

(5)

1≤i≤|N |

where Ii = 1 if system i is obfuscated from conﬁguration k
to be perceived as k(cid:48), and Ii = 0 otherwise.

The problem faced by the defender is to identify the
deployment conﬁguration that can minimize her utility loss Ud
while satisfying her budget constraint: Cd ≤ Bd. Algorithm 2
is developed to solve this problem.

1|, ..., |N (cid:48)

Algorithm 2: Deployment of conﬁgurations, |N (cid:48)| = |N |
1 Input: N and |N (cid:48)
|K||;
|K|| = n(cid:48)
2 Let |N (cid:48)
1| = n(cid:48)
3 Initialize N (cid:48)
|K| = ∅;
4 Initialize Cd = 0;
5 Rank u1, ..., u|N | in an increasing order, and supposing

1, ..., |N (cid:48)
1 = ... = N (cid:48)

|K|;

that the result is u1 ≤ ... ≤ u|N |;

6 for i = 1 to |N | do
7

Select a conﬁguration k(cid:48) with probability
(cid:15)
|N | Uk(cid:48)
proportional to exp(
);
2∆Ud
if Cd + c(k(cid:48), k) > Bd then

8

9

10

11

12

13

14

continue;
k(cid:48)| < n(cid:48)
if |N (cid:48)
N (cid:48)
k(cid:48) ← N (cid:48)
Cd ← Cd + c(k(cid:48), k);

k(cid:48) then
k(cid:48) ∪ {i};

else

goto Line 7;

15 Output: N (cid:48)

1, ..., N (cid:48)

|K|;

The idea behind Algorithm 2 involves preferentially obfus-
cating low-utility systems, with speciﬁc probabilities, to be
perceived as those conﬁgurations that have a high probability
of being attacked. To a large extent, this idea can prevent high-
utility systems from being attacked. It seems that the high-
utility systems may not be protected by the exponential mech-
anism, because 1) the exponential mechanism preferentially
obfuscates low-utility systems and 2) the budget constraint
Bd may limit the number of obfuscated systems. However,
the exponential mechanism still gives high-utility systems the
probability to be obfuscated, since the exponential mechanism
is probabilistic rather than deterministic. In addition,
the
budget constraint is a parameter which is set by users. A
larger budget constraint means a larger number of obfuscated
systems.

The algorithm starts by ranking the utility of the systems in
increasing order (Line 5). Next, each of the ranked systems i
is obfuscated to appear as a conﬁguration, k(cid:48), selected using

9

10

11

12

13

14

17

18

19

20

21

22

23

the exponential mechanism, until all systems are obfuscated
or the defender’s budget is used up (Lines 6-14).

In the exponential mechanism in Line 7, Uk(cid:48) = p(k(cid:48)) ·
(cid:80)
1≤i≤|Nk(cid:48) | ui
is the defender’s expected utility loss on
the systems with conﬁguration k(cid:48). Moreover, ∆Ud =
max1≤i≤|N |ui is the sensitivity of the defender’s expected
utility loss, which is used to to obfuscate system conﬁgura-
tions. A system conﬁguration with a higher expected utility
loss has a larger probability to be obfuscated. The expected
utility loss is used only as a parameter in the exponential
mechanism, and it does not change any properties of the
exponential mechanism. Therefore, the use of the expected
utility loss does not violate differential privacy.

The rationale of using the exponential mechanism is de-
scribed as follows. According to the deﬁnitions of differential
privacy, an obfuscated network N (cid:48) is interpreted as a dataset
D, while a conﬁguration k(cid:48) in network N (cid:48) is interpreted as
an output r of dataset D. Thus, the utility of a pair of dataset
and output, u(D, r), is equivalent to the defender’s expected
utility loss of selecting conﬁguration k(cid:48) from network N (cid:48), i.e.,
U (N (cid:48), k(cid:48)) denoted as Uk(cid:48). By comparing Deﬁnition 4 to Line
7 in Algorithm 2, we can conclude that as the exponential
mechanism can guarantee the privacy of the output of a
dataset, it can also guarantee the privacy of the conﬁgurations
of the systems in a network.

Situation 2. When |N (cid:48)| > |N |,

there are |N (cid:48)| − |N |
honeypots in the network. The defender’s expected utility loss
is shown in Equation 6,

Ud =

(cid:88)

(p(k)·

(cid:88)

ui)−

(cid:88)

p(k)·uj, (6)

1≤k≤|K|

1≤i≤|Nk|

1≤j≤|N (cid:48)|−|N |

where the second part, (cid:80)
expected utility gain obtained by using honeypots.
The defender’s cost is shown in Equation 7,

1≤j≤|N (cid:48)|−|N | p(k) · uj, indicates the

(cid:88)

Cd =

Ii · (c(k(cid:48), k)) +

(cid:88)

Ij · hk(cid:48),

(7)

1≤i≤|N |

1≤j≤|N (cid:48)|−|N |

where the second part, (cid:80)
1≤j≤|N (cid:48)|−|N | Ij · hk(cid:48), indicates the
extra cost incurred by deploying honeypots. Moreover, the
privacy budget (cid:15) is proportionally increased to |N (cid:48)|
· (cid:15) to
cover the extra honeypots. We use this proportional change of
privacy budget, because in Algorithm 3, the amount of privacy
budget is consumed identically in each iteration.

|N |

The problem faced by the defender is the same as in
Situation 1. Algorithm 3 is developed to solve this problem.
In Algorithm 3, the defender ﬁrst creates honeypots (Lines
7-14) and then obfuscates systems (Lines 15-23). In Line 8,
Uk(cid:48) = p(k(cid:48))uk(cid:48) while in Line 16, Uk(cid:48) = p(k(cid:48))·(cid:80)
1≤i≤|Nk(cid:48) | ui.
In both lines, ∆Ud = max1≤i≤|N |ui. The defender favors
honeypots over obfuscation, as the use of the former may
result in her incurring a utility gain. Honeypots are created
based on the estimated probability with which a conﬁguration
will be attacked. Conﬁgurations with the highest estimated
probability of being attacked will take priority for use to
conﬁgure honeypots.

1| = n(cid:48)

1, ..., |N (cid:48)

1|, ..., |N (cid:48)

Algorithm 3: Deployment of conﬁgurations, |N (cid:48)| > |N |
1 Input: N and |N (cid:48)
|K||;
2 Let |N (cid:48)
|K|| = n(cid:48)
3 Let xk = p(k) · (cid:80)
1≤i≤|Nk| ui;
4 Initialize N (cid:48)
|K| = ∅;
5 Initialize Cd = 0;
6 Rank u1, ..., u|N | in increasing order, and supposing that

1 = ... = N (cid:48)

|K|;

the result is u1 ≤ ... ≤ u|N |;

7 for i = 1 to |N (cid:48)| − |N | do
8

Select a conﬁguration k(cid:48) with probability
proportional to exp(
Cd ← Cd + hk(cid:48);
if Cd ≤ Bd then

(cid:15)
|N (cid:48)|−|N |
2∆Ud

Uk(cid:48)

);

Create a honeypot i with conﬁguration k(cid:48);
N (cid:48)

k(cid:48) ← N (cid:48)

k(cid:48) ∪ {i};

else

goto Line 8;

15 for i = 1 to |N | do
16

Select a conﬁguration k(cid:48) with probability
(cid:15)
|N | Uk(cid:48)
proportional to exp(
);
2∆Ud
if Cd + c(k(cid:48), k) > Bd then

continue;
k(cid:48)| < n(cid:48)
if |N (cid:48)
N (cid:48)
k(cid:48) ← N (cid:48)
Cd ← Cd + c(k(cid:48), k);

k(cid:48) then
k(cid:48) ∪ {i};

else

goto Line 16;

24 Output: N (cid:48)

1, ..., N (cid:48)

|K|;

Situation 3. When |N (cid:48)| < |N |, there are |N | − |N (cid:48)| systems
taken ofﬂine. The defender’s expected utility loss is shown in
Equation 8,

(cid:88)

(cid:88)

Ud =

[(p(k) · Ji · ui) + (Ji − 1)2 · lk],

(8)

1≤k≤|K|

1≤i≤|Nk|

where Ji = 0 if system i is taken ofﬂine, and Ji = 1 otherwise.
The second part, (cid:80)
1≤i≤|N |−|N (cid:48)|(Ji − 1)2 · lk, indicates the
extra utility loss incurred by taking systems ofﬂine. The
defender’s cost is shown in Equation 9,

(cid:88)

Cd =

Ii · (c(k(cid:48), k)).

(9)

1≤i≤|N |

Moreover, akin to Situation 2, the privacy budget (cid:15) is propor-
tionally decreased to |N (cid:48)|
|N | · (cid:15), as the systems taken ofﬂine need
not be protected. Thus, this part of the privacy budget can be
conserved.

The problem for the defender is the same as in Situation 1.
Algorithm 4 is developed to solve this problem. In Algorithm
4, the defender ﬁrst takes |N | − |N (cid:48)| systems ofﬂine (Lines
7-9) and next obfuscates the remaining systems (Lines 10-18).
In Line 11, Uk(cid:48) = (cid:80)
1≤i≤|Nk|[(p(k) · Ji · ui) + (Ji − 1)2 · lk]
and ∆Ud = max1≤i≤|N |ui. The defender will be inclined to

take those systems ofﬂine that will incur the highest utility
loss if attacked.

According to Equation 11, the attacker uses an epsilon-greedy
strategy to distribute selection probabilities over the observed
conﬁgurations:

1| = n(cid:48)

1, ..., |N (cid:48)

1|, ..., |N (cid:48)

Algorithm 4: Deployment of conﬁgurations, |N (cid:48)| < |N |
1 Input: N and |N (cid:48)
|K||;
2 Let |N (cid:48)
|K|| = n(cid:48)
3 Let xk = p(k) · (cid:80)
1≤i≤|Nk| ui;
4 Initialize N (cid:48)
|K| = ∅;
5 Initialize Cd = 0;
6 Rank u1, ..., u|N | in increasing order, and supposing that

1 = ... = N (cid:48)

|K|;

the result is u1 ≤ ... ≤ u|N |;
7 for j = 1 to |N | − |N (cid:48)| do
8

s ← argM axi[p(k) · ui − lk];
Take system s ofﬂine;

9
10 for i = 1 to |N (cid:48)| do
11

Select a conﬁguration k(cid:48) with probability
(cid:15)
Uk(cid:48)
|N (cid:48) |
proportional to exp(
2∆Ud
if Cd + c(k(cid:48), k) > Bd then

);

12

13

14

15

16

17

18

continue;
k(cid:48)| < n(cid:48)
if |N (cid:48)
N (cid:48)
k(cid:48) ← N (cid:48)
Cd ← Cd + c(k(cid:48), k);

k(cid:48) then
k(cid:48) ∪ {i};

else

goto Line 11;

19 Output: N (cid:48)

1, ..., N (cid:48)

|K|;

C. The attacker’s strategy

In each round of the game, the attacker ﬁrst estimates the
probability with which an observed conﬁguration k(cid:48) is a real
conﬁguration k using Bayesian inference. Next, he decides
which observed conﬁguration k(cid:48) should be attacked, i.e., he
will attack those systems associated with conﬁguration k(cid:48).

1) Step 3: probability estimation: To estimate the probabil-
ity that an observed conﬁguration k(cid:48) is a real conﬁguration k:
q(k|k(cid:48)), the attacker uses Bayesian inference:

q(k|k(cid:48)) =

.

q(k) · q(k(cid:48)|k)
q(k(cid:48))
|N | , and q(k(cid:48)) = |N (cid:48)
k(cid:48) |

(10)

In Equation 10, q(k) = |Nk|

|N (cid:48)| , where q(k)
is calculated using the attacker’s prior knowledge and q(k(cid:48)) is
computed with reference to the attacker’s current observations.
In addition, q(k(cid:48)|k) can be obtained from the attacker’s experi-
ence, i.e., posterior knowledge. For example, in previous game
rounds, the attacker attacked 10 systems with conﬁguration
k. Among these 10 systems, four systems are obfuscated to
appear as conﬁguration k(cid:48). Thus, q(k(cid:48)|k) = 4

10 = 0.4.

2) Step 4: target selection: Based on Equation 10, the
attacker can calculate the expected utility gain of attacking
each conﬁguration k(cid:48):

U (k(cid:48))

a =

(cid:88)

[q(k|k(cid:48)) · uk].

(11)

1≤k≤|K|

(cid:40)

Pk(cid:48) =

|K| , if U (k(cid:48))

a

(1 − e) + e
e
|K| , otherwise

is the largest

.

(12)

where e is a small positive number in [0, 1]. The reason
for using the epsilon-greedy strategy will be described in
Remark 2 in Section V-B. Finally, the attacker selects a target
k(cid:48) based on the probability distribution P = {P1, ..., P|K|}.

D. Potential application of our approach

Our approach can be applied against powerful attackers
in the real world. An attacker may execute a suite of scan
queries on a network using tools such as NMAP [37]. Our
DP-based approach returns a mix of true and false results to
the attackers scan to confuse him. The false results include not
only obfuscated conﬁgurations of systems but also additional
fake systems, i.e., honey pots. Moreover, our approach can
also take vulnerable systems ofﬂine to avoid the attackers
scan. For example, if an attacker makes a scan query on
one system many times, he may combine the query results
to reason the true conﬁguration of this system. Thus, this
system should be taken ofﬂine temporarily. In summary, our
approach can signiﬁcantly increase the complexity on attackers
to formulate their attack, irrespective of their reasoning power,
so that administrators can have additional time to build defense
policies to interdict attackers.

A speciﬁc real-world example is an advanced persistent
threat (APT) attacker [38]. Our approach can be adopted to
model interactions between a system manager and an APT
attacker. Typically, an APT attacker continuously scans the
vulnerability of the target systems and studies the defense
policy of these systems so as to steal sensitive information
from these systems [39]. Accordingly, the system manager
misleads the APT attacker by obfuscating the true information
of these systems, and formulates a complex defense policy for
these systems which is hardly predictable to the APT attacker.

V. THEORETICAL ANALYSIS

Our theoretical analysis consists of two parts: analysis of
the defender’s strategy and analysis of the attacker’s strategy.
In the analysis of the defender’s strategy, we ﬁrst analyze
the privacy preservation of the defender’s strategy. Then,
we analyze the defender’s expected utility loss in different
situations. After that, the complexity of the defender’s strategy
is also analyzed. In the analysis of the attacker’s strategy, we
analyze the attacker’s optimal strategy and the lower bound of
his expected utility gain. Finally, we give a discussion on the
equalibria between the defender and the attacker.

A. Analysis of the defender’s strategy

1) Analysis of privacy preservation: We ﬁrst prove that
the defender’s strategy satisﬁes differential privacy. Then, we
give an upper bound on the number of rounds of the game.

Exceeding this bound, the privacy level of the defender cannot
be guaranteed.

Lemma 1 (Sequential Composition Theorem [40]). Suppose
a set of privacy steps M = {M1, ..., Mm} are sequentially
performed on a dataset, and each Mi provides (cid:15)i privacy
guarantee, then M will provide (cid:80)
1≤i≤m (cid:15)i-differential pri-
vacy.

Lemma 2 (Parallel Composition Theorem [40]). Suppose we
have a set of privacy step M = {M1, ..., Mm}, if each
Mi provides (cid:15)i privacy guarantee on a disjoint subset of the
entire dataset, then the parallel composition of M will provide
max1≤i≤m (cid:15)i-differential privacy.

Theorem 1. Algorithm 1 satisﬁes (cid:15)-differential privacy.

Proof. Algorithm 1 consumes a privacy budget (cid:15). The Laplace
noise sampled from Lap( ∆S·|K|
) is added to |K| steps. At
(cid:15)
each step, the average allocated privacy budget is
|K| . Thus,
(cid:15)
|K| -differential privacy. As
for each step, Algorithm 1 satisﬁes
there are |K| steps, based on Lemma 1, Algorithm 1 satisﬁes
(cid:15)-differential privacy.

(cid:15)

Theorem 2. Algorithms 2, 3 and 4 satisfy (cid:15)-differential
privacy.

Proof. We prove that Algorithm 2 satisﬁes (cid:15)-differential pri-
vacy. The proof of Algorithms 3 and 4 is similar.

In Line 7 of Algorithm 2, the selection of a conﬁguration
takes place |N | times. The privacy budget (cid:15) is, thus, consumed
in |N | steps. At each step,
the average allocated privacy
(cid:15)
budget is
|N | . Hence, for each step, Algorithm 2 satisﬁes
(cid:15)
|N | -differential privacy. As there are |N | steps, based on
the sequential composition theorem (Lemma 1), Algorithm 2
satisﬁes (cid:15)-differential privacy.

2

exp(−(cid:15) × |A⊕B|

The processes of Algorithms 2, 3 and 4 are similar to
the NoisyAverage sampling developed in McSherrys PINQ
framework [41]. They, however, are different. The output value
of the NoisyAverage sampling is: P r[N oisyAvg(A) = x] ∝
), where A and B are two datasets

max
avg(B)=x
and ⊕ is for symmetric difference. In our algorithms, the
output value is: P r[output = k(cid:48)] ∝ exp(
). The major
difference between the NoisyAverage sampling and ours is
that in the NoisyAverage sampling, the output of a value x is
based on an operation of max
, while in our algorithm, the
avg(B)=x
output of a value k(cid:48) is based on its utility Uk(cid:48). The operation
of max
may violate the deﬁnition of the exponential

(cid:15)
|N | Uk(cid:48)
2∆Ud

avg(B)=x

mechanism and thus lead the NoisyAverage sampling to fail
to satisfy differential privacy. By comparison, our algorithms
strictly follow the deﬁnition of the exponential mechanism
(ref. page 39, Deﬁnition 3.4 in [35]). Thus, our algorithms
satisfy differential privacy.

Theorem 3. The defender is guaranteed (cid:15)-differential privacy.

Proof. The defender uses Algorithms 2, 3 and 4 for de-
ployment of conﬁgurations. The three algorithms are disjoint
because they are applied to three mutually exclusive situations:
|N (cid:48)| = |N |, |N (cid:48)| > |N | and |N (cid:48)| < |N |. Therefore, the

parallel composition theorem (Lemma 2) can be used here.
Since all of these algorithms satisfy (cid:15)-differential privacy, the
defender is guaranteed (cid:15)-differential privacy.

Corollary 1. The attacker cannot deduce the exact conﬁgu-
rations of the systems.

Proof. According to Theorem 3, the defender is guaranteed
(cid:15)-differential privacy. Based on the description of differential
privacy in Section III-B, differential privacy guarantees that the
attacker cannot tell whether a particular system is associated
with conﬁguration k, because whether or not the system is
associated with conﬁguration k makes little difference to the
output observations. Since the attacker cannot deduce the
conﬁguration of each individual system, he cannot deduce the
exact conﬁgurations of the systems in the network.

Remark 1. Corollary 1 guarantees the privacy of the
defender in a single game round. However, if no bound on the
number of rounds is set, the defender’s privacy may be leaked
[35], i.e., that the attacker ﬁgures out the real conﬁgurations of
the systems. This is because in differential privacy, a privacy
budget is used to control the privacy level. Every time the
system conﬁguration is obfuscated and released, the privacy
is
budget
used up, differential privacy cannot guarantee the privacy of
the defender anymore. To guarantee the privacy level of the
defender, we need to set a bound on the number of rounds.

is partially consumed. Once the privacy budget

Deﬁnition 5 (KL-Divergence [35]). The KL-Divergence be-
tween two random variables Y and Z taking values from the
same domain is deﬁned to be:

D(Y ||Z) = Ey∼Y

(cid:20)

ln

P r(Y = y)
P r(Z = y)

(cid:21)

.

(13)

Deﬁnition 6 (Max Divergence [35]). The Max Divergence
between two random variables Y and Z taking values from
the same domain is deﬁned to be:

D∞(Y ||Z) = max

S⊆Supp(Y )

(cid:20)

ln

P r(Y ∈ S)
P r(Z ∈ S)

(cid:21)

.

(14)

Lemma 3 ([35]). A mechanism M is (cid:15)-differentially private
if and only if on every two neighboring datasets x and x(cid:48),
D∞(M(x)||M(x(cid:48))) ≤ (cid:15) and D∞(M(x(cid:48))||M(x)) ≤ (cid:15).

Lemma 4 ([35]). Suppose that random variables Y and Z
satisfy D∞(Y ||Z) ≤ (cid:15) and D∞(Z||Y ) ≤ (cid:15). Then, D(Y ||Z) ≤
(cid:15) · (e(cid:15) − 1).

Theorem 4. Given that the defender’s privacy level is (cid:15) at
each single round, to guarantee the defender’s overall privacy
level to be (cid:15)(cid:48), the upper bound of the number of rounds is
(cid:15)(cid:48)·(e(cid:15)(cid:48)
−1)
(cid:15)·(e(cid:15)−1) .
Proof. Let the upper bound of the number of rounds be k and
the view of the attacker in the k rounds be v = (v1, ..., vk). We
have D(Y ||Z) = ln
=
(cid:105)
(cid:80)k

i=1
i=1 D(Yi||Zi). As the defender is
guaranteed (cid:15)-differential privacy at each single round, based
on Lemma 3, we have D∞(Yi||Zi) ≤ (cid:15) and D∞(Zi||Yi) ≤

(cid:104) P r(Y =v)
P r(Z=v)
= (cid:80)k

(cid:104) P r(Yi=vi)
P r(Zi=vi)

P r(Yi=vi)
P r(Zi=vi)

i=1 ln

(cid:104)(cid:81)k

= ln

(cid:105)

(cid:105)

(cid:15). Based on this result, according to Lemma 4, we have
D(Yi||Zi) ≤ (cid:15) · (e(cid:15) − 1). Thus, we have D(Y ||Z) =
(cid:80)k

i=1 D(Yi||Zi) ≤ k · (cid:15) · (e(cid:15) − 1).
To guarantee the defender’s overall privacy level

to be
(cid:15)(cid:48), according to Lemma 3, we have D∞(Y ||Z) ≤ (cid:15)(cid:48) and
D∞(Z||Y ) ≤ (cid:15)(cid:48). By using Lemma 4, we have D(Y ||Z) ≤
(cid:15)(cid:48) · (e(cid:15)(cid:48)
− 1). Finally, since D(Y ||Z) ≤ k · (cid:15) · (e(cid:15) − 1) and
D(Y ||Z) ≤ (cid:15)(cid:48) · (e(cid:15)(cid:48)
− 1), the upper bound k is limited by
(cid:15)(cid:48)·(e(cid:15)(cid:48)
−1)
(cid:15)·(e(cid:15)−1) .

2) Analysis of the deployment algorithms: Here, we com-
pute the expected utility loss of the defender in three situations:
|N (cid:48)| = |N |, |N (cid:48)| > |N | and |N (cid:48)| < |N |.

Theorem 5. In Situation 1 (|N (cid:48)| = |N |), the defender’s ex-
1≤i≤|N |[ui · (cid:80)
pected utility loss is (cid:80)
1≤k(cid:48)≤|K| pk(cid:48)Pk(cid:48)], where

pk(cid:48) =

(cid:80)

exp(

U

(cid:15)
k(cid:48)
|N |
2∆Ud

1≤k≤|K| exp(

)
(cid:15)
Uk
|N |
2∆Ud

, if the defender uses Algorithm 2

)

to deploy conﬁgurations and the attacker uses Equation 12 to
select the target.

Proof. In Algorithm 2,
the defender deploys the systems
based on 1) the utility of each system and 2) the estimated
probability that the attacker will select each conﬁguration to
attack. By using the exponential mechanism, each system i
is obfuscated to appear as conﬁguration k(cid:48) with probability

U

(cid:15)
k(cid:48)
|N |
2∆Ud

exp(

)
(cid:15)
Uk
(cid:80)
|N |
2∆Ud
k(cid:48) as a target with probability Pk(cid:48).

1≤k≤|K| exp(

)

. The attacker thus selects conﬁguration

System i is attacked only when the defender obfuscates i
to appear as conﬁguration k(cid:48) and the attacker selects k(cid:48) as a
target. As there are |K| conﬁgurations, the defender’s expected
utility loss on system i is ui · (cid:80)
1≤k(cid:48)≤|K| pk(cid:48)Pk(cid:48). Since there
are |N | systems, the defender’s total expected utility loss is
(cid:80)

1≤i≤|N |[ui · (cid:80)

1≤k(cid:48)≤|K| pk(cid:48)Pk(cid:48)].

Corollary 2. In Situation 1, the upper bound of the defender’s
utility loss is (cid:80)
l | ui, where |N (cid:48)
k(cid:48)|.
(cid:80)
l | systems, which

l | ui represents the utility sum of |N (cid:48)
have the highest utility among all the systems.

l | = max1≤k(cid:48)≤|K||N (cid:48)

1≤i≤|N (cid:48)

1≤i≤|N (cid:48)

Proof. In Algorithm 2, the defender deploys each system in
order of increasing utility. Therefore, it is possible for the
defender to deploy the |N (cid:48)
l | systems in conﬁguration l, where
k(cid:48)| and the |N (cid:48)
l | = max1≤k(cid:48)≤|K||N (cid:48)
|N (cid:48)
l | systems have the
highest utility among all the systems. When conﬁguration l
is selected by the attacker as a target, the utility of all systems
in N (cid:48)
l | ui. As this is the
l
worst case for the defender in Situation 1, the utility loss in
this case is the upper bound.

is lost; this amounts to (cid:80)

1≤i≤|N (cid:48)

Similarly, we can also draw the following conclusions.

Theorem 6.
fender’s expected utility loss is (cid:80)

In Situation 2 (|N (cid:48)| > |N |),

1≤i≤|N (cid:48)|[ 2|N |−|N (cid:48)|

the de-
(ui ·

|N (cid:48)|

(cid:80)

1≤k(cid:48)≤|K| pk(cid:48)Pk(cid:48))], where pk(cid:48) =

exp(

U

(cid:15)
k(cid:48)
|N |
2∆Ud

(cid:80)

1≤k≤|K| exp(

)
(cid:15)
Uk
|N |
2∆Ud

, if the

)

defender uses Algorithm 3 to deploy conﬁgurations and the
attacker uses Equation 12 to select the target.

|N (cid:48)|

Proof. By comparing the results of Theorems 5 and 6, the
difference is that in Theorem 6, there is an extra coefﬁcient,
2|N |−|N (cid:48)|
. In Situation 2, there are |N (cid:48)| − |N | honeypots.
|N (cid:48)|
Hence, a system i could be a honeypot, with probabil-
ity |N (cid:48)|−|N |
, or a genuine system, with probability |N |
|N (cid:48)| .
When system i is attacked,
the expected utility loss on
system i is |N |
(ui ·
|N (cid:48)|
(cid:80)
1≤k(cid:48)≤|K| pk(cid:48)Pk(cid:48))
means a utility gain if system i is a honeypot. As there are
|N (cid:48)| systems, including honeypots, the expected utility loss is
(cid:80)

|N (cid:48)| [ui · (cid:80)
1≤k(cid:48)≤|K| pk(cid:48)Pk(cid:48)), where |N (cid:48)|−|N |

1≤k(cid:48)≤|K| p(k(cid:48))P(k(cid:48))] − |N (cid:48)|−|N |
(ui · (cid:80)

|N (cid:48)|

1≤i≤|N (cid:48)|[ 2|N |−|N (cid:48)|

|N (cid:48)|

(ui · (cid:80)

1≤k(cid:48)≤|K| pk(cid:48)Pk(cid:48))].

Corollary 3. In Situation 2, the upper bound of the defender’s
utility loss is (cid:80)
l | ui, where |N (cid:48)
k(cid:48)|.
(cid:80)
l | ui represents the utility sum of the |N (cid:48)
l | systems
that have the highest utility among all the systems.

l | = max1≤k(cid:48)≤|K||N (cid:48)

1≤i≤|N (cid:48)

1≤i≤|N (cid:48)

Proof. Although Situation 2 is different from Situation 1, the
worst case in Situation 2 is the same as that in Situation 1.
This is because in Situation 2, all genuine systems are still in
the network. In the worst case, the attacker attacks all of the
systems with the highest utility. The result, thus, is the same
as that in Situation 1.

Theorem 7. In Situation 3 (|N (cid:48)| < |N |), the defender’s
expected utility loss is
(cid:80)
1≤i≤|N (cid:48)|(ui ·(cid:80)

1≤k(cid:48)≤|K| pk(cid:48)Pk(cid:48))−(cid:80)

1≤j≤|N |−|N (cid:48)|(uj −lj),

U

(cid:15)
k(cid:48)
|N |
2∆Ud

exp(

where pk(cid:48) =

)
(cid:15)
Uk
|N |
2∆Ud
gorithm 4 to deploy conﬁgurations and the attacker uses
Equation 12 to select the target.

, if the defender uses Al-

1≤k≤|K| exp(

(cid:80)

)

Proof. By comparing the results of Theorems 5 and 7,
we can see that
in Theorem 7,
there is an extra part
(cid:80)
1≤j≤|N |−|N (cid:48)|(uj − lj). This part means that |N | − |N (cid:48)|
systems are taken ofﬂine. Thus, the utility of these systems
will not be lost.

Corollary 4. In Situation 3, the upper bound of the defender’s
utility loss is (cid:80)
l | ui, where |N (cid:48)
k(cid:48)|.
(cid:80)
l | ui represents the utility sum of the |N (cid:48)
l | systems
that have the highest utility among the remaining |N (cid:48)| systems.

l | = max1≤k(cid:48)≤|K||N (cid:48)

1≤i≤|N (cid:48)

1≤i≤|N (cid:48)

Proof. In Situation 3, |N | − |N (cid:48)| systems are taken ofﬂine.
These systems, therefore, will not be attacked. Hence, in the
worst case, the |N (cid:48)
l | systems with the highest utility among
the remaining |N (cid:48)| systems are attacked.

3) Analysis of the complexity of the algorithms: The pro-
posed approach includes four algorithms. The analysis of their
complexity is given as follows.

Theorem 8. The computational complexity of Algorithm 1 is
O(|N | · |K|), where |N | is the number of systems and |K| is
the number of conﬁgurations.

Proof. In Line 3 of Algorithm 1, |N | systems are partitioned
into |K| categories. This partition implicitly involves a nested
loop, where the number of iterations is |N |·|K|. In Lines 4-5 of
Algorithm 1, the number of iterations in this loop is |K|. Thus,

the overall number of iterations is |N | · |K| + |K| = (|N | +
1) · |K|. The complexity of Algorithm 1 is O(|N | · |K|).

Theorem 9. The computational complexity of Algorithm 2 is
O(|N |2), where |N | is the number of systems.

Proof. In Line 5 of Algorithm 2, |N | systems are ranked in
an increasing order based on their utilities. The number of
iterations in this ranking is |N |·(|N |−1)
. Moreover, in Lines 7
to 14, the number of iterations in this loop is |N |. Thus, the
overall number of iterations is |N |·(|N |−1)
+|N | = |N |·(|N |+1)
.
The complexity of Algorithm 2, therefore, is O(|N |2).

2

2

2

Theorem 10. The computational complexity of Algorithm 3 is
O(|N |2), where |N | is the number of systems.

Proof. In Line 6 of Algorithm 3, |N | systems are ranked in
an increasing order based on their utilities. The number of
iterations in this ranking is |N |·(|N |−1)
. Moreover, in Lines
7 to 14, the number of iterations in this loop is |N (cid:48)| − |N |,
where |N (cid:48)| is the number of systems after obfuscation. Also,
in Lines 15 to 23, the number of iterations in this loop is |N |.
Thus, the overall number of iterations is |N |·(|N |−1)
+ |N (cid:48)|.
Since |N (cid:48)| and |N | are in the same scale, the complexity of
Algorithm 2 is O(|N |2).

2

2

Theorem 11. The computational complexity of Algorithm 4 is
O(|N |2), where |N | is the number of systems.

Proof. In Line 6 of Algorithm 4, |N | systems are ranked in
an increasing order based on their utilities. The number of
iterations in this ranking is |N |·(|N |−1)
. Moreover, in Lines 7
to 9, the number of iterations in this loop is |N | − |N (cid:48)|, where
|N (cid:48)| is the number of systems after obfuscation. Also, in Lines
10 to 18, the number of iterations in this loop is |N (cid:48)|. Thus, the
overall number of iterations is |N |·(|N |−1)
+|N | = |N |·(|N |+1)
.
The complexity of Algorithm 2, therefore, is O(|N |2).

2

2

2

B. Analysis of the attacker’s strategy

We ﬁrst analyze the attacker’s optimal strategy and then

compute the lower bound of his expected utility gain.

Remark 2 (the attacker’s expected utility gain). As the
cyber deception game is a zero-sum game in Situations 1
(|N (cid:48)| = |N |) and 2 (|N (cid:48)| > |N |), the defender’s expected
utility loss is the attacker’s expected utility gain. In Situation
3 (|N (cid:48)| < |N |), since the attacker does not attack the ofﬂine
systems, his expected utility gain is based only on the utility
of the remaining |N (cid:48)| systems, which is (cid:80)
1≤i≤|N (cid:48)|(ui ·
(cid:80)

1≤k(cid:48)≤|K| pk(cid:48)Pk(cid:48)).

Theorem 12. The attacker’s optimal strategy is the solution
to the following problem: given U (1)
, ﬁnd a
a ,
probability distribution, P1,
..., P|K|, which can maximize
1≤k(cid:48)≤|K| Pk(cid:48) · U (k(cid:48))
(cid:80)

..., U (|K|)
a

a

.

Proof. According to the discussion in Remark 2, the ma-
the attacker’s expected utility gain is
jor component of
(cid:80)
1≤i≤|N (cid:48)|(ui · (cid:80)
1≤k(cid:48)≤|K| pk(cid:48)Pk(cid:48)). In this component, the
only uncertain part is P(k(cid:48)). The calculation of P(k(cid:48)) is based
on Equations 10 and 11.

.

a

q(k(cid:48))

In Equation 10, q(k|k(cid:48)) = q(k)·q(k(cid:48)|k)

, q(k) and q(k(cid:48)) are
known to the attacker, while q(k(cid:48)|k) can be calculated by the
attacker based on the defender’s strategy. Hence, Equation 11,
U (k(cid:48))
a = (cid:80)
1≤k≤|K|[q(k|k(cid:48)) · uk], can also be computed by the
attacker. Since the attacker’s aim is to maximize his expected
utility gain, he needs to identify the appropriate probability
distribution, P1, ..., P|K|, for use as his attack strategy in
1≤k(cid:48)≤|K| Pk(cid:48) · U (k(cid:48))
order to maximize (cid:80)
Remark 3 (The attacker’s risk). According to Theorem 12,
the attacker’s optimal strategy should be to attack conﬁgu-
ration k(cid:48) with probability 1, where U (k(cid:48))
is the maximum
among U (1)
. However, such a deterministic strategy
is highly predictable to the defender. The attacker has to use a
mixed strategy that is as close as possible to the deterministic
strategy. The epsilon-greedy strategy is a good solution, as
it is similar to the deterministic strategy while also exhibits
a reasonable degree of randomness. In the epsilon-greedy
strategy, the maximum U (k(cid:48))
is given a high probability, while
the remainder shares the remaining probability.
a ≥ U (2)
Theorem 13. Let U (1)
of the attacker’s utility gain is

a ≥ ...U (|K|)
(cid:80)

a , ..., U (|K|)

, the lower bound

1≤k(cid:48)≤|K| U k(cid:48)
a .

1
|K|

a

a

a

a

(cid:80)

a [(1 − e) + e

Proof. By using the epsilon-greedy strategy, the attacker’s
expected utility gain is
U (1)
= (1 − e)U (1)

1≤k(cid:48)≤|K| U (k(cid:48))
This expected utility gain decreases monotonically with the
increase of the value of e. Thus, when e = 1, the expected
1
utility reaches the minimum:
|K|

2≤k(cid:48)≤|K| U (k(cid:48))

|K| ] + e
(cid:80)

1≤k(cid:48)≤|K| U k(cid:48)
a .

a + e
|K|

(cid:80)

|K|

a

a

.

C. Analysis of the equilibria

We provide a characterization of equilibria and leave the
deep investigation of equilibria in this highly dynamic security
game as one of our future studies.

In our approach, the number of systems may change at each
round due to the introduction of Laplace noise (recall Algo-
rithm 1). Therefore, the available strategies to the defender also
change at each round, where each strategy is interpreted as an
assignment of systems to conﬁgurations, i.e., obfuscation. By
comparison, the available strategies to the attacker is ﬁxed in
the game, because 1) the number of conﬁgurations is ﬁxed;
and 2) the attacker selects only one conﬁguration to attack
at each round, i.e., attacking the systems associated with a
conﬁguration.

Formally, let S be the set of all potentially attainable strate-
gies of the defender, and St ⊂ S be the set of the defender’s
strategies at round t. Correspondingly, let At ∈ Rmt×n be
the payoff matrix at round t, where mt = |St| is the number
of the defender’s strategies at round t and n is the number
of the attacker’s strategies. Let ∆t
d be the set of probability
distributions over the defender’s available strategies at round
t and ∆a be the set of probability distributions over the
attacker’s available strategies. Then, at round t, if the defender
uses a probability distribution x ∈ ∆t
d and the attacker uses a
probability distribution y ∈ ∆a, the defender’s utility loss is
xAty. The defender wishes to minimize the quantity of xAty,

while the attacker wants to maximize it. This is a minimax
problem. The solution of this problem is an equilibrium of
the game [42]. However, the defender’s strategy set St at each
round t is randomly generated using the differentially private
Laplace mechanism and St is unknown to the attacker. Hence,
the attacker is difﬁcult to precisely compute the solution which
can force the defender not to unilaterally change her strategy.
This ﬁnding has also been demonstrated in our experimental
results which will be given in detail in Section VI.

VI. EXPERIMENT AND ANALYSIS

A. Experimental setup

In the experiments, three defender approaches are evaluated,
which are our approach, referred to as DP-based, a determin-
istic greedy approach [17], referred to as Greedy, and a mixed
greedy approach, referred to as Greedy-Mixed. The Greedy
approach obfuscates systems using a greedy and deterministic
strategy to minimize the defender’s expected utility loss. The
Greedy-Mixed approach obfuscates systems using a greedy
and mixed strategy, where a high utility system is obfuscated
to a low utility system with a certain probability.

Two metrics are used to evaluate the three approaches: the
attacker’s utility gain and the defender’s cost. For the ﬁrst
metric, the attacker’s utility gain, its amount is the same as
the amount of the defender’s utility loss. We use the metric
attackers utility gain by following reference [17]. Certainly,
using the defenders utility loss as a metric will reach the same
results. For the second metric, the defender’s cost, it is the
defenders deployment cost used to obfuscate conﬁgurations
of systems.

Against the three defender approaches, two attacker ap-
proaches are adopted, which are the Bayesian inference ap-
proach and the deep reinforcement learning (DRL) approach.
The Bayesian inference approach has been described in Sec-
tion IV-C. The DRL approach involves a deep neural network
which consists of an input layer, two hidden layers and an
output layer. The input is the state of the attacker, which is
deﬁned as the selected conﬁgurations in the last eight rounds
and the corresponding utility received by attacking the systems
in these conﬁgurations. The output is the selected conﬁguration
that the attacker will attack in this round. Moreover, each
of the hidden layer has ten neurons. The hyper-parameters
used in DRL are listed as follows: learning rate α = 0.1,
discount factor γ = 0.9, epsilon-greedy selection probability
epsilon = 0.9, batch size = 32 and memory size = 2000.
These two techniques, Bayesian inference and deep re-
inforcement learning, are powerful enough and prevalent in
cybersecurity [43], [44], [45]. By comparison, deep reinforce-
ment learning is more powerful than Bayesian inference, but
it is more complex than Bayesian inference and it needs a
long training period. Therefore, we take both of them into our
evaluation.

The experiments are conducted in four scenarios.

Scenario 1: The numbers of systems and conﬁgurations vary,
but
the average number of systems associated with each
conﬁguration is ﬁxed. This scenario is used to evaluate the
scalability of the three approaches. It simulates the real-world

situation that different organizations have different scales of
networks.
Scenario 2: The number of systems is ﬁxed, but the number
of conﬁgurations varies. Thus, the average number of systems
associated with each conﬁguration also varies. This scenario
is used to evaluate the adaptivity of the three approaches. It
simulates the real-world situation that an organization updates
the conﬁgurations of systems from time to time.
Scenario 3: The number of both systems and conﬁgurations
is ﬁxed, but the value of the defender’s deployment budget Bd
varies. This scenario is used to evaluate the impact of deploy-
ment budget value on the performance of the three approaches.
It simulates the real-world situation that different organizations
have different deployment budgets or an organization adjusts
its deployment budget sometimes.
Scenario 4: The number of both the systems and conﬁg-
the value of the privacy budget (cid:15)
urations is ﬁxed, but
varies. This scenario is used to evaluate the impact of the
privacy budget value on the performance of our approach. It
simulates the real-world situation that different organizations
have different requirements of privacy level and thus requires
different privacy budgets.

In a given experimental setting, as arbitrarily changing
the number of systems may incur security issues, in each
round we use the differential privacy mechanism (Algorithm
1) to compute the number of systems in each conﬁguration.
Therefore, the number of systems is strategically determined
and their privacy can be guaranteed.

The value of ∆S is set to 1. The utility of a conﬁgu-
ration, uk, is uniformly drawn from [1, 20]. The defender’s
obfuscation cost, c(k(cid:48), k), is set to 0.1 · u(cid:48)
k. The defender’s
honeypot deployment cost, hk, is set to 0.2·uk. The defender’s
utility loss, lk, for taking a system with conﬁguration k ofﬂine
to 0.15 · uk. These settings are experimentally set
is set
up to yield the best results. Here, “best” means the most
representative. Increasing these costs will increase both the
cost and utility loss of the defender. On one hand, these
costs are used by the defender to obfuscate conﬁgurations of
systems and deploy honeypots. Thus, increasing these costs
will increase the defenders cost. On the other hand, increasing
these costs will result in the defender using up her deployment
budget too early. The defender then cannot perform obfusca-
tion or deployment. Thus, her utility loss will increase. On
the contrary, decreasing these costs will decrease both the
cost and utility loss of the defender. However, excessively
decreasing these costs will render the experimental results
meaningless. Therefore, we set representative parameter values
to balance the defenders performance and the meanings of the
experimental results. The experimental results are obtained by
averaging 1000 rounds of the game.

B. Experimental results

1) Scenario 1: Fig. 3 demonstrates the performance of the
three approaches in Scenario 1. The number of systems varies
from 50 to 250, and the number of conﬁgurations varies from
5 to 25, accordingly. The privacy budget (cid:15) is ﬁxed at 0.3. The
cost budget Bd is ﬁxed at 1000 for each round.

(a) The attacker’s utility in dif-
ferent scales of networks

(b) The defender’s cost in dif-
ferent scales of networks

(a) The attacker’s utility as game
progresses in DP-based

(b) The defender’s cost as game pro-
gresses in DP-based

Fig. 3. The three approaches’ performance in Scenario 1

As the number of systems increases, across the three
approaches, the attacker’s utility gain increases slightly and
linearly while the defender’s cost increases sub-linearly. Along
with the increase in the number of systems,
the number
of conﬁgurations also increases. More conﬁgurations gives
the attacker more choices. By using Bayesian inference, the
attacker can choose a conﬁguration with high utility, to attack.
Thus,
the attacker’s utility gain increases linearly. As the
number of systems increases, according to Algorithms 2, 3 and
4, the defender deals with more systems. Thus, the defender’s
cost inevitably increases provided that the budget Bd is large
enough.

Comparing our DP-based approach to the Greedy and
Greedy-Mixed approaches, the attacker in the DP-based ap-
proach achieves about 30% and 12% less utility than in the
Greedy and Greedy-Mixed approaches, respectively. Moreover,
the defender in the DP-based approach incurs about 3%
and 5% more cost than in the Greedy and Greedy-Mixed
approaches, respectively. In the DP-based approach, the de-
fender can not only obfuscate systems, but can also deploy
honeypots to attract the attacker. The cost of deploying a
honeypot exceeds that of obfuscating a system. However, a
honeypot results in a negative utility to the attacker, while
obfuscating a system can only reduce the attacker’s utility gain.
By comparison, the defender in the Greedy and Greedy-Mixed
approaches only obfuscates systems. The difference between
defender’s costs among the three approaches is negligible. This
demonstrates that in our DP-based approach, the defender uses
almost the same cost as the other two approaches to achieve
much better results, i.e., lowering the attackers utility gain.

Fig. 4 shows the variation in the attacker’s utility gain
and the defender’s cost in the three approaches as the game
progresses in Scenario 1. The number of systems is ﬁxed at
100, and the number of conﬁgurations is ﬁxed at 10.

In Fig. 4(c), which depicts the Greedy approach, there are
a number of plateaus. These plateaus indicate the steadiness
of the attacker’s utility gain, which implies that the attacker
has predicted the defender’s strategy. The attacker can thus
adopt an optimal strategy to maximize his utility gain. By
comparison, in Fig. 4(a) and Fig. 4(e), which depicts the
DP-based and Greedy-Mixed approaches, respectively, there
is no plateau. This means that the attacker cannot predict the
defender’s strategy and adopts only random strategies. This
ﬁnding also demonstrates that equilibria may not exist between
the defender and attacker if the defender uses our DP-based
approach, as the attackers utility ﬂuctuates all the time with

(c) The attacker’s utility as game
progresses in Greedy

(d) The defender’s cost as game pro-
gresses in Greedy

(e) The attacker’s utility as game
progresses in Greedy-Mixed

(f) The defender’s cost as game pro-
gresses in Greedy-Mixed

Fig. 4. The three approaches’ performance as game progress in Scenario 1

no equilibrium. However, when the defender uses the Greedy
approach which does not change the available strategies of the
defender, the attacker can predict the defenders strategy and
may reach an equilibrium. Particularly, by comparing Figs.
4(a), 4(c) and 4(e), we can see that the shape of Fig. 4(e)
is more similar to Fig. 4(c) than Fig. 4(a). This implies that
although the defender in the Greedy-Mixed approach uses a
mixed strategy, the attacker can still predict the defender’s
strategy to some extent.

that

By comparing Figs. 4(b), 4(d) and 4(f), we can see that
the variation of the defender’s cost in the Greedy and Greedy-
Mixed approaches is more stable than the DP-based approach.
in the DP-based approach,
This is due to the fact
the defender can deploy honeypots which incurs extra cost.
However, as shown in Fig. 3(b), the defender’s average cost
in the DP-based approach still stays at a relatively low level
in comparison with the Greedy and Greedy-Mixed approaches.
Fig. 5 demonstrates the performance of the three de-
fender approaches against the DRL attacker. Compared to
the Bayesian inference attacker (Figs. 3 and 4), the DRL
attacker can obtain more utility, when the defender adopts

50100150200250The number of systems5060708090The attacker's utilityDP-basedGreedyGreedy-Mixed50100150200250The number of systems20406080100The defender's costDP-basedGreedyGreedy-Mixedthe tendency shown in Fig. 5. For simplicity and clarity, they
are not included in the paper.

(a) The attacker’s utility with
different numbers of conﬁgura-
tions

(b) The defender’s cost with dif-
ferent numbers of conﬁgurations

Fig. 6. The three approaches’ performance in Scenario 2

2) Scenario 2: Fig. 6 demonstrates the performance of the
three approaches in Scenario 2. The number of systems is ﬁxed
at 150, and the number of conﬁgurations varies from 5 to 25.
The privacy budget (cid:15) is ﬁxed at 0.3. The cost budget Bd is
ﬁxed at 1000 for each round.

With the increase of the number of conﬁgurations,
the
attacker’s utility gain decreases while the defender’s cost
increases. In Scenario 2, since the number of systems is ﬁxed,
as the number of conﬁgurations increases, the average number
of systems associated with each conﬁguration decreases. As
discussed in Fig. 3, the attacker’s utility gain is based on the
number of systems associated with the attacked conﬁguration.
Thus, the attacker’s utility gain decreases. Moreover, as the
number of conﬁgurations increases, the defender is more likely
to obfuscate a system. Thus, the defender’s cost increases.
For example, in our DP-based approach, when there are two
conﬁgurations: 1 and 2, the defender may obfuscate a system
from conﬁguration 1 to appear as 2 with probability 0.5.
However, when there are three conﬁgurations: 1, 2 and 3,
the defender may obfuscate a system from conﬁguration 1
to appear as 2 or 3 with the same probability of 0.33, or
0.66 altogether. This example shows that as the number of
conﬁgurations increases, the probability that the defender will
obfuscate a system will increase.

Fig. 7 shows the variation in the attacker’s utility gain
and the defender’s cost in the three approaches as the game
progresses in Scenario 2. The number of systems is ﬁxed at
150, and the number of conﬁgurations is ﬁxed at 25. Fig. 7
has a similar trend to Fig. 4, because Scenario 2 has a similar
setting to Scenario 1, where budget Bd is large enough to
cover the whole obfuscation process.

3) Scenario 3: Fig. 8 demonstrates the performance of
the three approaches in Scenario 3. The numbers of systems
and conﬁgurations are ﬁxed at 100 and 10, respectively. The
privacy budget (cid:15) is ﬁxed at 0.3. The cost budget Bd for each
round varies from 30 to 150.

When the defender’s budget Bd is tight (less than 90),
the defender’s cost can be properly controlled (Fig. 8(b)).
However, the attacker will gain high utility (Fig. 8(a)). Due to
the budget limitation (less than 90), the defender obfuscates
the attacker can easily
only a few systems, meaning that
select high-utility systems. By contrast, when the defender’s

(a) The attacker’s utility with DRL
in different scales of networks

(b) The attacker’s utility with DRL
as game progresses in DP-based

(c) The attacker’s utility with DRL
as game progresses in Greedy

(d) The attacker’s utility with DRL
as game progresses in Greedy-Mixed

Fig. 5. The three approaches against the DRL attacker in Scenario 1

either the Greedy or the Greedy-Mixed approach. This is
because the Greedy approach is deterministic and thus the
Greedy defender’s strategies are easy to be learned by the
DRL attacker. Although the Greedy-Mixed approach intro-
duces randomization to some extent, the major component of
the approach is still greedy. Therefore, it is still not difﬁcult for
a powerful DRL attacker to learn the Greedy-Mixed defender’s
strategies. However, when the defender employs our DP-
based approach, the two types of attackers obtain almost the
same utility. To explain, our DP-based approach introduces
differentially private random noise into the conﬁgurations. As
analyzed in Section V-A, with the protection of differential
privacy, it is hard for an attacker to deduce the DP-based
defender’s strategies irrespective of the attacker’s reasoning
power.

Against the two types of attackers, the defender uses almost
the same cost. This is because in the Greedy and Greedy-
Mixed approaches, defender’s strategies are independent of
the attacker’s strategies. Thus, the defender’s cost is inde-
pendent of the attacker’s types. In our DP-based approach,
the defender’s strategies do take the attacker’s strategies into
consideration. However, due to the use of differential privacy
mechanisms, the utility loss of the defender against the two
attacker approaches is almost the same as shown in Figs. 3(a)
and 5(a), given that the defender’s utility loss is identical to the
attacker’s utility gain. Hence, according to Equations 4, 6, 8
and Algorithms 2, 3, 4, as the utility loss of the defender stays
steady, the defender’s strategies are not affected much. Thus,
the defender’s cost remains almost the same. For simplicity,
the ﬁgures regarding the defender’s cost spent against the
DRL attacker are not presented. Moreover, in the remaining
scenarios, the performance variation tendency of the three
defender approaches against the DLR attacker is similar to

50100150200250The number of systems50100150200250The attacker's utility with DRLDP-basedGreedyGreedy-Mixed510152025The number of configurations6080100120140The attacker's utilityDP-basedGreedyGreedy-Mixed510152025The number of configurations5060708090100The defender's costDP-basedGreedyGreedy-Mixed(a) The attacker’s utility as game
progresses in DP-based

(b) The defender’s cost as game pro-
gresses in DP-based

(a) The attacker’s utility as
game progresses in DP-based,
Bd = 60

(b) The attacker’s utility as
in Greedy,
game progresses
Bd = 150

(c) The attacker’s utility as game
progresses in Greedy

(d) The defender’s cost as game pro-
gresses in Greedy

(e) The attacker’s utility as game
progresses in Greedy-Mixed

(f) The defender’s cost as game pro-
gresses in Greedy-Mixed

Fig. 7. The three approaches’ performance as game progress in Scenario 2

(c) The defender’s cost as game
progresses in DP-based, Bd =
60

(d) The defender’s cost as game
progresses in Greedy, Bd =
150

Fig. 9. Performance of the DP-based approach with different cost budgets

ing the defender’s budget from 60 to 150. Moreover, in Fig.
9(c), the defender’s cost is conﬁned to 60, while in Fig. 9(d),
there is no conﬁnement on the defender’s cost. This is because
budget Bd = 60 is insufﬁcient for the defender to obfuscate
all the necessary systems in most rounds. Therefore, budget
Bd = 60 becomes a conﬁnement for the defender. When the
budget is increased to 150, budget Bd = 150 is enough to
cover the obfuscation of all the necessary systems. Thus, the
conﬁnement disappears.

(a) The attacker’s utility with
different values of budget Bd

(b) The defender’s cost with
different values of budget Bd

Fig. 8. The three approaches’ performance in Scenario 3

(a) The attacker’s utility with
different values of privacy bud-
get (cid:15)

(b) The defender’s cost with dif-
ferent values of privacy budget (cid:15)

budget Bd is large enough (greater than 90), she can obfuscate
almost all necessary systems, which increases the difﬁculty
experienced by the attacker when attempting to select high-
utility systems.

Fig. 9 shows the details of the situations of Bd = 60 and
Bd = 150 in our DP-based approach. The details in the
Greedy and Greedy-Mixed approaches are similar to the DP-
based approach, which thus are not presented.

By comparing Fig. 9(a) and Fig. 9(b), the attacker’s utility
signiﬁcantly reduces from 88 to 57 on average, when increas-

Fig. 10. Our approach’s performance in Scenario 4

4) Scenario 4: Fig. 10 demonstrates the performance of
our approach in Scenario 4. The number of systems is ﬁxed
at 150, and the number of conﬁgurations is ﬁxed at 15. The
privacy budget (cid:15) various from 0.1 to 0.5. The cost budget Bd
is ﬁxed at 1000 for each round.

According to Deﬁnition 3 in Sub-section III-B, a smaller
(cid:15) denotes a larger Laplace noise. In Line 5 of Algorithm 1,
the number of systems associated with each conﬁguration is
adjusted by adding Laplace noise. Therefore, a larger Laplace

306090120150The defender's budget Bd507090110130150The attacker's utilityDP-basedGreedyGreedy-Mixed306090120150The defender's budget Bd20406080The defender's costDP-basedGreedyGreedy-Mixed02004006008001000The number of rounds0255075100125150175The attacker's utilityDP-based02004006008001000The number of rounds0255075100125150175The attacker's utilityDP-based02004006008001000The number of rounds2530354045505560The defender's costDP-based02004006008001000The number of rounds60708090100110120The defender's costDP-based0.10.20.30.40.53040506070The attacker's utilityDP-based0.10.20.30.40.550100150200250The defender's costDP-basednoise implies a larger change in the number of systems. On one
hand, if the Laplace noise is positive, the number of systems
increases (|N (cid:48)| > |N |). The defender thus incurs extra cost
to deploy the added systems, i.e., honeypots. These honeypots
will attract the attacker and decrease his utility gain. On the
other hand, if the Laplace noise is negative, the number of
systems decreases (|N (cid:48)| < |N |). The defender sacriﬁces some
utility to take |N | − |N (cid:48)| systems ofﬂine. These systems can
avoid the attacker, and the attacker’s utility gain reduces.

(a) The attacker’s utility as
game progresses

(b) The defender’s cost as game
progresses

Fig. 11. Our approach with different number of game rounds in Scenario 4

Fig. 11 demonstrates the performance of our approach with
different number of game rounds, where the privacy budget
(cid:15) is ﬁxed at 0.3. Based on the discussion in Theorem 4, to
guarantee a given privacy level, an upper bound of the number
of rounds must be set. In Fig. 11(a), with the increasing num-
ber of rounds, the attacker’s utility gain increases gradually,
especially after 2000 rounds. This implies that when the game
is played in a large number of rounds, the attacker can infer
the true conﬁgurations of systems. This experimental result
empirically prove our theoretical result. In Fig. 11(b), the
defender’s cost is not affected by the number of rounds, since
both the number of systems and conﬁgurations is ﬁxed.

C. Summary

According to the experimental results, due to the adoption
of the differential privacy technique, our DP-based approach
outperforms the Greedy and Greedy-mixed approaches in
various scenarios by signiﬁcantly reducing the attacker’s utility
gain by about 30% ∼ 40%. In terms of the overhead resulting
from adopting the differential privacy technique, the defender
in our DP-based approach incurs about 5% ∼ 8% more cost
than in the Greedy and Greedy-mixed approaches. Moreover,
the privacy budget (cid:15) can be used to balance the attacker’s
utility gain and the defender’s cost. A small value of (cid:15) means a
low utility gain for the attacker, but a high cost to the defender.
The setting of the (cid:15) value is left to users.

Furthermore, the running times of the three approaches are
almost the same. Thus, they are not shown graphically in the
experiments. Based on the theoretical analysis in Section V,
the complexity of our DP-based approach is O(|N |2), where
|N | is the number of systems. In the Greedy and Greedy-
mixed approaches,
there is a ranking process to rank the
utility of systems. Since we use bubble sort to implement the
Greedy and Greedy-mixed approaches, the complexity of both
approaches is O(|N |2) which is the same as our DP-based
approach.

VII. CONCLUSION AND FUTURE WORK

This paper proposes a novel differentially private game the-
oretic approach to cyber deception. Our approach is the ﬁrst to
adopt the differential privacy technique, which can efﬁciently
handle any change in the number of systems and complex
strategies potentially adopted by an attacker. Compared to
the benchmark approaches, our approach is more stable and
achieves much better performance in various scenarios with
slightly higher cost.

In the future, as mentioned in Section V, we will deeply
investigate the equilibria in highly dynamic security games.
We also intend to improve our approach by introducing multi-
ple defenders and multiple attackers. In this paper, we consider
only one defender and one attacker. Future games will be very
interesting when multiple defenders and attackers are involved.
Moreover, we will develop a general prototype of our approach
to allow the state of the art to progress faster. Speciﬁcally,
to develop a prototype, we intend to follow Jajodia et al.s
work [8]. We will ﬁrst use Cauldron software [46] in a real
network environment to scan for vulnerabilities on each node
in the network. Then we will replicate the network into a larger
network with the NS2 network simulator [47]. After that, our
approach will be implemented on the simulated network.

REFERENCES

[1] V. Goel and N. Perlroth, “Yahoo Says 1 Billion User Accounts Were

Hacked,” www.nytimes.com, 2016.

[2] T. E. Carroll and D. Grosu, “A Game Theoretic Investigation of
Deception in Network Security,” Security and Communication Networks,
vol. 4, no. 10, pp. 1162–1172, 2011.

[3] M. H. Almeshekah and E. H. Spafford, Cyber Deception, 2016, ch.

Cyber Security Deception, pp. 25–52.

[4] M. Albanese, E. Battista, and S. Jajodia, Cyber Deception, 2016, ch.
Deceiving Attackers by Creating a Virtual Attack Surface, pp. 167–199.
[5] C. Kiennert, Z. Ismail, H. Debar, and J. Leneutre, “A Survey on Game-
Theoretic Approaches for Intrusion Detection and Response Optimiza-
tion,” ACM Computing Surveys, vol. 51, no. 5, pp. 90:1–90:31, 2018.

[6] E. Al-Shaer, J. Wei, K. W. Hamlen, and C. Wang, Autonomous Cyber

Deception. Springer, 2019.

[7] C. T. Do and et al., “Game Theory for Cyber Security and Privacy,”

ACM Computing Surveys, vol. 50, no. 2, pp. 30:1–37, 2017.

[8] S. Jajodia, N. Park, F. Pierazzi, A. Pugliese, E. Serra, G. I. Simari, and
V. S. Subrahmanian, “A Probabilistic Logic of Cyber Deception,” IEEE
Trans. on Infor. Foren. and Secu., vol. 12, no. 11, pp. 2532–2544, 2017.
[9] C. Modi, D. Patel, B. Borisaniya, H. Patel, A. Patel, and M. Rajarajan,
“A survey of intrusion detection techniques in Cloud,” Journal of
Network and Computer Applications, vol. 36, pp. 42–57, 2013.
[10] B. B. Zarpelaoa, R. S. Mianib, C. T. Kawakania, and S. C. de Alvarenga,
“A survey of intrusion detection in Internet of Things,” Journal of
Network and Computer Applications, vol. 84, pp. 25–37, 2017.
[11] D. Ding, Q. Han, Y. Xiang, X. Ge, and X. Zhang, “A survey on security
control and attack detection for industrial cyber-physical systems,”
Neurocomputing, vol. 275, pp. 1674–1683, 2018.

[12] C. Dwork, “Differential privacy,” in Proc. of ICALP, 2006, pp. 1–12.
[13] M. H. Manshaei, Q. Zhu, T. Alpcan, T. Basar, and J.-P. Hubaux, “Game
Theory Meets Network Security and Privacy,” ACM Computing Surveys,
vol. 45, no. 3, pp. 25:1–39, 2013.

[14] X. Liang and Y. Xiao, “Game Theory for Network Security,” IEEE
Communications Surveys and Tutorials, vol. 15, no. 1, pp. 472–486,
2013.

[15] M. Albanese, E. Battista, and S. Jajodia, “A Deception based Approach
for Defeating OS and Service Fingerprinting,” in Proc. of IEEE Confer-
ence on Communications and Network Security, 2015, pp. 317–325.

[16] W. Wang and B. Zeng, “A Two-Stage Deception Game for Network

Defense,” in Proc. of GameSec, 2018, pp. 569–582.

[17] A. Schlenker, O. Thakoor, H. Xu, F. Fang, M. Tambe, L. Tran-Thanh,
P. Vayanos, and Y. Vorobeychik, “Deceiving Cyber Adversaries: A Game
Theoretic Approach,” in Proc. of AAMAS, 2018, pp. 892–900.

1000200030004000The number of rounds3540455055606570The attacker's utilityDP-based1000200030004000The number of rounds404244464850The defender's costDP-based[18] M. Bilinski, K. Ferguson-Walter, S. Fugate, R. Gabrys, J. Mauger, and
B. Souza, “You only Lie Twice: A Multi-round Cyber Deception Game
of Questionable Veracity,” in Proc. of GameSec, 2019, pp. 65–84.
[19] L. Huang and Q. Zhu, Autonomous Cyber Deception. Springer, 2019,
ch. Dynamic Bayesian Games for Adversarial and Defensive Cyber
Deception, pp. 75–97.

[20] J. Cho, M. Zhu, and M. Singh, Autonomous Cyber Deception. Springer,
2019, ch. Modeling and Analysis of Deception Games Based on
Hypergame Theory, pp. 49–74.

[21] T. H. Nguyen, Y. Wang, A. Sinha, and M. P. Wellman, “Deception in
Finitely Repeated Security Games,” in AAAI, 2019, pp. 2133–2140.
[22] J. Pawlick, E. Colbert, and Q. Zhu, “Modeling and Analysis of Leaky
Deception Using Signaling Games With Evidence,” IEEE Trans. on
Information Forensics and Security, vol. 14, no. 7, pp. 1871–1886, 2019.
[23] O. Thakoor, M. Tambe, P. Vayanos, H. Xu, C. Kiekintveld, and
F. Fang, “Cyber Camouﬂage Games for Strategic Deception,” in Proc.
of GameSec, 2019, pp. 525–541.

[24] J. Gan, H. Xu, Q. Guo, L. Tran-Thanh, Z. Rabinovich, and
M. Wooldridge, “Imitative Follower Deception in Stackelberge Games,”
in Proc. of EC, 2019, pp. 639–657.

[25] M. van Dijk, A. Juels, A. Oprea, and R. L. Rivest, “FlipIt: The Game of
“Stealthy Takeover”,” Journal of Cryptology, vol. 26, no. 4, pp. 655–713,
2013.

[26] K. D. Bowers, M. van Dijk, R. Grifﬁn, A. Juels, A. Oprea, R. L. Rivest,
and N. Triandopoulos, “Defending against the Unknown Enemy: Ap-
plying FlipIt to System Security,” in Proc. of International Conference
on Decision and Game Theory for Security, 2012, pp. 248–263.
[27] A. Laszka, G. Horvath, M. Felegyhazi, and L. Buttyan, “FlipThem:
Modeling Targeted Attacks with FlipIt for Multiple Resources,” in Proc.
of International Conference on Decision and Game Theory for Security,
2014, pp. 175–194.

[28] I. Arce, “The Weakest Link Revisited,” IEEE Security and Privacy,

vol. 1, no. 2, pp. 72–76, 2003.

[29] H. R. Varian, Economics of Information Security. Springer, 2003, ch.

System Reliability and Free Riding, pp. 1–15.

[30] R. Bohme and T. Moore, “The Iterated Weakest Link,” IEEE Security

and Privacy, vol. 8, no. 1, pp. 53–55, 2010.

[31] T. Zhu, P. Xiong, G. Li, W. Zhou, and P. S. Yu, “Differentially
private model publishing in cyber physical systems,” Future Generation
Computer Systems, vol. 108, pp. 1297–1306, 2019.

[32] D. Ye, T. Zhu, W. Zhou, and P. S. Yu, “Differentially Private Malicious
Agent Avoidance in Multiagent Advising Learning,” IEEE Transactions
on Cybernetics, p. DOI: 10.1109/TCYB.2019.2906574, 2019.

[33] T. Zhu and P. S. Yu, “Applying Differential Privacy Mechanism in
Artiﬁcial Intelligence,” in Proc. of IEEE 39th International Conference
on Distributed Computing Systems (ICDCS), 2019, pp. 1601–1609.
[34] T. Zhu, D. Ye, W. Wang, W. Zhou, and P. S. Yu, “More Than Privacy:
Applying Differential Privacy in Key Areas of Artiﬁcial Intelligence,”
IEEE Transactions on Knowledge and Data Engineering, p. DOI:
10.1109/TKDE.2020.3014246, 2020.

[35] C. Dwork and A. Roth, “The Algorithmic Foundations of Differential
Privacy,” Foundations and Trends in Theoretical Computer Science,
vol. 9, no. 3-4, pp. 211–407, 2014.

[36] T. Zhu, G. Li, W. Zhou, and P. S. Yu, “Differentially private data
publishing and analysis: A survey,” IEEE Transactions on Knowledge
and Data Engineering, vol. 29, no. 8, pp. 1619–1638, 2017.

[37] G. F. Lyon, Nmap network scanning: The ofﬁcial Nmap project guide

to network discovery and security scanning.

Insecure, 2009.

[38] C. Tankard, “Advanced Persistent Threats and How to Monitor and Deter

Them,” Network Security, vol. 8, no. 8, pp. 16–19, 2011.

[39] L. Yang, P. Li, Y. Zhang, X. Yang, Y. Xiang, and W. Zhou, “Effective
Repair Strategy Against Advanced Persistent Threat: A Differential
Game Approach,” IEEE Transactions on Information Forensics and
Security, vol. 14, no. 7, pp. 1713–1728, 2019.

[40] F. McSherry and K. Talwar, “Mechanism Design via Differential Pri-
vacy,” in Proceedings of the 48th Annual IEEE Symposium on Founda-
tions of Computer Science.
IEEE Computer Society, 2007, pp. 94–103.

[41] F. McSherry, “Privacy Integrated Queries,” in SIGMOD, 2009.
[42] A. Gilpin and T. Sandholm, “Solving Two-person Zero-sum Repeated
Games of Incomplete Information,” in Proc. of AAMAS, 2008, pp. 903–
910.

[43] P. Xie, J. H. Li, X. Ou, P. Liu, and R. Levy, “Using Bayesian Networks
for Cyber Security Analysis,” in Proc. of IEEE/IFIP International
Conference on Dependable Systems and Networks, 2010, pp. 211–220.
[44] T. T. Nguyen and V. J. Reddi, “Deep Reinforcement Learning for Cyber

Security,” https://arxiv.org/abs/1906.05799, 2019.

[45] A. Chivukula, X. Yang, W. Liu, T. Zhu, and W. Zhou, “Game
Theoretical Adversarial Deep Learning with Variational Adversaries,”
IEEE Transactions on Knowledge and Data Engineering, p. DOI:
10.1109/TKDE.2020.2972320, 2020.

[46] S. Jajodia, S. Noel, P. Kalapa, M. Albanese, and J. Williams, “Cauldron
mission-centric cyber situational awareness with defense in depth,” in
Proc. of MILCOM, 2011, pp. 1339–1344.

[47] T. Issariyakul and E. Hossain, Introduction to Network Simulator NS2.

Springer, 2008.

Dayong Ye received his MSc and PhD degrees both
from University of Wollongong, Australia, in 2009
and 2013, respectively. Now, he is a research fel-
low of Cyber-security at University of Technology,
Sydney, Australia. His research interests focus on
differential privacy, privacy preserving, and multi-
agent systems.

A/Prof. Tianqing Zhu received the BEng and MEng
degrees from Wuhan University, China, in 2000 and
2004, respectively, and the PhD degree in computer
science from Deakin University, Australia, in 2014.
Dr Tianqing Zhu is currently an associate professor
at the School of Computer Science in University of
Technology Sydney, Australia. Her research interests
include privacy preserving, data mining and network
security.

Sheng Shen is pursuing his PhD at
the School
of Computer Science, University of Technology
Sydney. He received the Bachelor of Engineering
(Honors) degree in Information and Communication
Technology from the University of Technology Syd-
ney in 2017, and Master of Information Technology
degree in University of Sydney in 2018. His current
research interests include data privacy preserving,
differential privacy and federated learning.

Prof. Wanlei Zhou received the BEng and MEng
degrees from Harbin Institute of Technology, Harbin,
China in 1982 and 1984, respectively, and the PhD
degree from The Australian National University,
Canberra, Australia, in 1991, all in Computer Sci-
ence and Engineering. He also received a DSc degree
from Deakin University in 2002. He is currently the
Head of School of School of Computer Science in
University of Technology Sydney, Australia. His re-
search interests include distributed systems, network
security, and privacy preserving.

