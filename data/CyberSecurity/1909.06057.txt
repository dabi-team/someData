Strategic Inference with a Single Private Sample

Erik Miehling, Roy Dong, C´edric Langbort, and Tamer Bas¸ar

9
1
0
2

p
e
S
3
1

]
T
G
.
s
c
[

1
v
7
5
0
6
0
.
9
0
9
1
:
v
i
X
r
a

Abstract— Motivated by applications in cyber security, we
develop a simple game model for describing how a learning
agent’s private information inﬂuences an observing agent’s
inference process. The model describes a situation in which one
of the agents (attacker) is deciding which of two targets to attack,
one with a known reward and another with uncertain reward.
The attacker receives a single private sample from the uncertain
target’s distribution and updates its belief of the target quality.
The other agent (defender) knows the true rewards, but does
not see the sample that the attacker has received. This leads
to agents possessing asymmetric information: the attacker is
uncertain over the parameter of the distribution, whereas the
defender is uncertain about the observed sample. After the at-
tacker updates its belief, both the attacker and the defender play
a simultaneous move game based on their respective beliefs. We
offer a characterization of the pure strategy equilibria of the
game and explain how the players’ decisions are inﬂuenced by
their prior knowledge and the payoffs/costs.

I. INTRODUCTION

As our societal systems become increasingly populated
with smart devices, there is an increasing number of learning
agents collecting data and drawing inferences. Furthermore,
as more and more decisions are being made by (automated)
learning agents, an increasing number of secondary agents
will arise to monitor their decision making processes.

The associated problem that one faces when learning about
the relevant environment becomes much more complex in
such settings. Agents no longer learn about their environment
in isolation; an agent’s learning process (the act of collecting
information) is subject to observation from others who are
attempting to infer their private information. Since beliefs
inﬂuence actions, and actions inﬂuence payoffs, each agent
must be mindful of how its learning actions inﬂuence what
others believe about its private information. This is especially
a concern when the interests of the agents are misaligned.1
As a result, a complete analysis of how an agent should learn
in such environments must explicitly take into account how
the learning process itself inﬂuences players’ beliefs and their
subsequent choices.

The general problem of learning while under observa-
tion arises in many real-world scenarios. Examples range
from problems in e-commerce and online marketplaces, e.g.,
where a merchant estimates a user’s willingness to pay

This work was supported by the U.S. Ofﬁce of Naval Research (ONR)

MURI grant N00014-16-1-2710.

All

authors

at Urbana–Champaign, Urbana,
of
{miehling,roydong,langbort,basar1}@illinois.edu

are with the Coordinated Science Lab, University
61801, USA.

Illinois

IL

1The inﬂuence of one’s actions/strategies on the beliefs of others is
known in the economics literature as signaling. Signaling is present in
both competitive (game) settings [1] and cooperative (decentralized control)
settings [2], [3].

based on their browsing patterns in order to set revenue-
maximizing prices, to cyber security, e.g., where a defender
partially observes an attacker’s reconnaissance of various
target
locations in order to determine where to allocate
defensive resources.

For the purposes of this paper, our focus is on cyber
security. As a motivating scenario, consider a hacker carrying
out some preliminary reconnaissance on potential attack
vectors, e.g., sending out commands to a particular server to
see which ports are open, before launching an attack. Such
recon actions not only provide information to the hacker
about the structure of the network but also, due to monitoring
systems deployed throughout the system, reveal information
to the defender about the hacker’s knowledge and true intent.
The defender thus faces a problem of attempting to infer
the hacker’s state of knowledge based on the recon actions.
This is further complicated by the fact that the defender
may only partially observe the recon actions, for instance,
while the defender can determine which server received a
malicious command, it may not know what information the
hacker had received. Consequently, the defender does not
know the hacker’s updated state of knowledge. Conversely,
the hacker would be unsure of the value of various attack
vectors/targets, and thus does not know how its private
information inﬂuenced the defender’s view. The interaction
is thus one of asymmetric information: each agent possesses
information not available to the other. For the remainder
of the paper, we will present the results in the context of
cyber security and refer to the agents as the attacker and the
defender.

A ﬁrst step in analyzing this problem is understanding
how the learning agent’s private information inﬂuences the
observing agent’s inference process. In this paper, we intro-
duce a stylized game-theoretic model to capture the basic
strategic components of this interaction in the context of
the previously described cyber security scenario. Speciﬁcally,
we consider an attacker and a defender who simultaneously
choose among two targets, a and b. If the attacker chooses to
attack a, it receives a randomized reward with an uncertain
distribution; if the attacker chooses to attack b, it receives
the defender
a ﬁxed and known reward. In both cases,
loses the same amount. If the defender chooses to defend
the same target that the attacker attacked, then the attacker
incurs a cost of capture and the defender gains the same
amount (a reward for the capture). The interesting part of
this model is what happens beforehand. Before the attacker
and the defender play the game, the attacker receives a single
private sample from the randomized reward distribution.
While the attacker’s prior before this observation is common

 
 
 
 
 
 
knowledge, the posterior after the observation is known only
to the attacker. The defender knows the true distribution of
the reward, but does not see the attacker’s sample and thus
does not know the posterior of the attacker. The fundamental
problem we investigate is the inﬂuence of the attacker’s
private sample on beliefs and equilibria of the game.

A. Background & Related Work

The structure of our model bears similarity to some ex-
isting models in the economics and learning literatures. One
such model, termed strategic experimentation [4], describes
a setting where agents learn from the outcomes of other’s
experiments. In the original model [4], each experiment,
deﬁned as the selection of an alternative, yields an outcome
that is commonly observed by the agents. The model is
a multi-agent extension of the two-armed bandit2 problem
[6], [7] where agents choose between a “risky” alternative
(with unknown statistics) and a “known” alternative, with the
objective of maximizing expected payoff. The main insight
from the classical model is that, due to free-riding, any
equilibrium in which players only use the common belief
is socially suboptimal.

The original strategic experimentation model has been
extended in many directions. The most relevant to our setting
are the models of [8], [9] that consider the case of private
information. Under private information,
the outcomes of
experiments are no longer publicly observed, rather they are
assumed to be private to the players. The model of [8] studies
a two player problem where each player faces a bandit
problem consisting of a risky arm (of two possible types)
and a known arm. Similarly, [9] study a related problem
but allow for communication (via cheap talk) among the
players and for the players to reverse their decisions to stop
experimenting, distinguishing it from the setup of [8].

A related setting is the Bayesian exploration model of
[10]. The model considers a problem where a principal is
attempting to incentivize (a series of) myopic agents to
explore, rather than be greedy and exploit. This is similar
in spirit to the motivation found in strategic experimentation
settings.

Given the motivation for our study, it is useful to also
discuss some models from the security literature. The general
problem of allocating defenses in the presence of a strategic
attacker is a well-studied topic. One prominent model is
the Colonel Blotto game [11], [12], [13] consisting of two
players who aim to allocate resources (troops) to alternatives
(battleﬁelds) in order to maximize wins (by majority rule).
Another popular model is that of Stackelberg security games
[14] where a defender (leader) allocates defenses to a col-
lection of targets before an attacker (follower) launches its
attack. While differing in the timing and informational as-
sumptions, both the Colonel Blotto and Stackelberg security
game settings provide insight into how defensive resources
should be allocated in order to minimize the chance of a
successful attack.

Compared to the aforementioned work, the model pre-
sented in this paper studies a simple setting in which the
learning agent receives a single sample of private information
from a distribution privately known by another (observing)
agent. Our model aims to isolate the effect of this single
private sample on the inference process of the observing
agent. This structured information asymmetry has some
natural applications (some of which were discussed earlier),
especially in the context of cyber security. To the best of
our knowledge, models possessing this structure have not
yet been investigated in the economics, learning, or security
literatures.

Comparing our model to the more general strategic exper-
imentation setting, our model is one of private information;
however, we note that the learner is not actively deciding to
receive the sample, it receives the sample regardless of its
choice. As a result, we refer to the model of this paper as
strategic inference rather than strategic experimentation.

B. Contribution

In summary,

the contribution of our work is a game
formulation, motivated by the aforementioned foundational
security setting, that isolates a structured class of asymmetric
information games that admits tractable analysis. The model
provides insight into how an observing agent reasons about
the private information revealed to a learning agent and its
subsequent impact on decisions.

II. THE GAME MODEL

Consider the following two-player game where an attacker
(A) and defender (D) choose among two potential targets a
and b. The reward of target a is uncertain to the attacker,
dictated by an unknown distribution, whereas target b has
a known reward. The attacker is assumed to possess a prior
over the unknown distribution. For the purposes of this paper,
we assume a Bernoulli reward distribution (for the unknown
target a) and a beta prior.3 The Bernoulli parameter, denoted
by p, is only known to the defender, whereas the beta prior,
denoted by (α0, β0) is assumed to be common knowledge.
The game proceeds as follows. First, the attacker receives
a sample from the unknown target a, which it uses to form an
updated (posterior) belief. Since the prior is assumed to be
beta and the trials Bernoulli, the updated posterior is simply
Beta(α0 + θ, β0 + 1 − θ). The defender does not see the real-
ized sample θ. Players then act simultaneously, the attacker
choosing which target to attack and the defender choosing
which target to defend. Fig. 1 represents the timeline of this
interaction.

Fig. 1: Order of events in the game.

2See [5] for an overview of multi-armed bandits and some fundamental

3This choice is for mathematical simplicity (conjugate priors). The model

results.

can also consider alternative distributions, such as Gaussian.

The objectives of the attacker and the defender are mis-
aligned; the attacker wishes to avoid the defender whereas
the defender wishes to catch the attacker. These preferences
are encoded using payoffs as follows. If the attacker selects
target a, it receives a payoff of R > 0 with probability p. If
the attacker picks b, it receives a known payoff of γR where
0 < γ < 1 is a constant. The attacker incurs a capture cost
of c > 0 if the defender picks the same target. Payoffs are
zero-sum where the attacker is assumed to be maximizing.

A. Subjective Payoffs & Costs

Before describing payoffs, we deﬁne the players’ types.
The attacker’s type is the sample it receives from target
a, denoted by θ ∈ {0, 1}. The defender’s type is the true
(reward) parameter of the reward distribution of target a,
denoted by p ∈ [0, 1]. Note that while the defender has
private knowledge of the true p, it does not know the sample
that the attacker received. In other words, neither player
knows the other player’s true type.

The attacker’s uncertainty of the defender’s type leads to it
being uncertain of the true payoffs of the game. The attacker
can only compute its expected payoff using its prior and the
information it gathers from the received sample. Speciﬁcally,
the attacker’s best estimate of the true parameter p is given
by the mean of its posterior after the sample is received, thus
α0+β0+1 R. The
its expected reward for selecting target a is
defender on the other hand, knows the true expected costs,
since it knows the reward parameter p. Furthermore, since γ
is known, the payoff for selecting target b is known to both
players. The players’ subjective payoffs/costs are illustrated
below in Table I.

α0+θ

TABLE I: SUBJECTIVE PAYOFFS/COSTS FOR PLAYERS.

D

a

b

A

a

b

α0+θ

α0+β0+1 R − c

α0+θ
α0+β0+1 R

γR

γR − c

(a) Subjective payoffs for the attacker, denoted uA.

D

b

a

A

a

b

pR − c

pR

γR

γR − c

(b) Costs for the defender, denoted uD.

B. Best Response Functions

The strategies of the attacker and the defender, denoted by
σA(a; θ) and σD(a; θ), respectively, represent the probabili-
ties that target a will be chosen. The best response functions

of the players are given by the following optimizations.
D(a; p)) | θ(cid:3)(cid:111)
A(a; θ), σD) | p(cid:3)(cid:111)

σ∗
A(a; θ) ∈ argmax
σA∈[0,1]

(cid:2)uA(σA, σ∗

(cid:2)uD(σ∗

Ep

Eθ

(cid:110)

(cid:110)

σ∗
D(a; p) ∈ argmin
σD∈[0,1]

Note that the attacker is maximizing and the defender is
minimizing, reﬂecting the fact that the attacker wants to
avoid the defender but
the defender wants to catch the
attacker. Substitution of the subjective payoffs/costs from
Table I yields the following best response functions.

Lemma 1: The best response functions of the players are

σ∗
A(a; θ) ∈ argmax
σA∈[0,1]

(cid:40)

σA

(cid:18)(cid:18) α0 + θ

α0 + β0 + 1

(cid:19)

− γ

R

+ c(cid:0)1 − 2Ep

(cid:2)σ∗

(cid:19)(cid:41)

D(a; p) | θ(cid:3)(cid:1)
A(a; θ) | p(cid:3)(cid:1)(cid:111)

,

(1)

(2)

σ∗
D(a; p) ∈ argmin
σD∈[0,1]

(cid:110)

σD

(cid:0)1 − 2Eθ

(cid:2)σ∗

(cid:2)σ∗

D(a; p) | θ(cid:3) is the attacker’s expectation of the
A(a; θ) | p(cid:3) is the defender’s

where Ep
defender’s strategy and Eθ
expectation of the attacker’s strategy.
Proof: See Appendix I-A.

(cid:2)σ∗

(cid:2)σ∗

The form of the expectations of the strategies in Lemma
1 reﬂects the information asymmetry among the players:
the attacker sees θ but does not know p;
the defender
knows p but does not see θ. The attacker computes the
D(a; p) | θ(cid:3),
expectation of the defender’s strategy, Ep
using its private sample. Given a beta prior, Beta(α0, β0),
the attacker’s distribution over the true reward parameter as
a function of the private sample is dictated by a beta distri-
bution, Beta(α0 + θ, β0 + 1 − θ). The defender computes the
A(a; θ) | p(cid:3), given
expectation of the attacker’s strategy, Eθ
its knowledge of the reward parameter p. The defender’s
distribution over the attacker’s private sample is given by a
Bernoulli distribution, Bernoulli(p). Since the attacker’s type
(sample) is generated from a probability distribution dictated
by the defender’s type (parameter of the reward distribution),
the game is one with correlated types [15]. This structure is
core to our analysis; under correlation, knowledge of one’s
type is informative for inferring the other player’s type.

(cid:2)σ∗

III. PURE STRATEGY EQUILIBRIA

As mentioned in the introduction, we are interested in
studying pure strategy equilibria. The following lemma
shows that, for a given set of parameters, R, c, γ, (α0, β0),
the resulting game has a (unique) pure strategy equilibrium
which can take three possible forms.

Lemma 2: The private sampling game has at most one
pure strategy saddle-point equilibrium, characterized by the
following three disjoint regions:

1) σA(a; θ) = σ∗

D(a; p) = 1 for all θ, p if and only if

α0 + θ
α0 + β0 + 1

R − c ≥ γR

2) σA(a; θ) = σ∗

D(a; p) = 0 for all θ, p if and only if
α0 + θ
α0 + β0 + 1

γR − c ≥

R

(a) Region where both
players choose target a.

(b) Region where both
players choose target b.

Fig. 2: Regions (blue/shaded) for (α0, β0) such that (a) both
players choose target a; (b) both players choose target b.

a, and vice versa. The defender defends a if and only if
p > 1/2. This equilibrium is best interpreted by separating
the interval condition into its two inequalities. From the proof
of Lemma 2, the condition for σ∗
A(a; θ = 1) = 1 (the lower
bound in the statement of the lemma) is

I1/2(α0 + 1, β0) ≥

(cid:18)

1
2

1 +

(cid:16)

R
c

γ −

α0 + 1
α0 + β0 + 1

(cid:17)(cid:19)

.

This region, illustrated in Fig. 3, is further subdivided into
four subregions with the following interpretations:

(i) In this subregion, α0 is high relative to β0 so target a
looks sufﬁciently desirable even if it gets caught.
(ii) In this subregion, α0 is still sufﬁciently larger than β0,
so the uncertain reward appears better to the attacker
than the certain reward. However, the attacker believes
it to be very likely that p > 1/2 (due to a higher
α0 + β0), and therefore would get caught should it
choose a, outweighing its gain in reward. The capture
cost c deters the attacker from choosing a at this point.
the attacker is sufﬁciently
conﬁdent that the defender will defend target b, and
thus the attacker chooses target a, even though the
expected reward of the uncertain target is lower than
the certain target.

(iii) Analogous to case (ii),

(iv) Analogous to (i), the attacker is conﬁdent that a will
yield no reward, and thus it does not choose a.

A(a; 0) = 0, σ∗
3) σ∗
D(a; p) = I{p > 1/2} if and only if
σ∗

A(a; 1) = 1, and

I1/2(α0, β0) ∈

(cid:34)

(cid:18)

1 +

(cid:18)

1 +

1
2

1
2

(cid:16)

R
c

γ −

α0 + 1
α0 + β0 + 1

(cid:17)(cid:19)

+

(cid:16)

R
c

γ −

α0
α0 + β0 + 1

(cid:17)(cid:19)

−

(cid:17)α0+β0

(cid:16) 1
2

α0B(α0, β0)
(cid:16) 1
2

(cid:17)α0+β0

β0B(α0, β0)

,

(cid:35)

where B(α, β) and Ix(α, β) are the beta function and
regularized incomplete beta function, respectively.
Proof: See Appendix I-B.

A. Equilibrium Discussion

Our discussion proceeds in two steps. First, for ﬁxed
parameters R, c, and γ, we describe the (α0, β0) region
where each equilibrium condition of Lemma 2 holds. Second,
we vary the capture cost c to investigate how the equilibrium-
supporting regions change.

We study the parameter regime in which R −c > γR. The
rationale for this choice is as follows. The quantity R − c
is the highest possible reward the attacker can receive from
the uncertain target if it gets caught. However, the attacker
does not always receive R from a, it may get unlucky and
get a zero reward (since the reward from a is stochastic)
in addition to getting caught. Due to the attacker’s prior on
the true value of p, the attacker’s view of the reward from
playing a (when the defender also plays a), regardless of the
received sample, is strictly less than R − c. By analyzing the
equilibria in the R − c > γR regime, we can investigate the
speciﬁc level of uncertainty (quantiﬁed by the prior) such
that the attacker would prefer to play b over a. As such, we
ﬁx R = 1, c = 0.1, and γ = 1/2 for the following analysis.
the equilibrium conditions of
Lemma 2 describe regions in the space of prior parameters
(α0, β0). In particular, the ﬁrst two regions in Lemma 2,
namely 1) σA(a; θ) = σ∗
D(a; p) = 1 for all θ, p, and
2) σA(a; θ) = σ∗
D(a; p) = 0 for all θ, p are illustrated
in Figure 2. The interpretation of the ﬁrst two regions is
straightforward. Recall from Lemma 2 the condition for the
ﬁrst region σA(a; θ) = σ∗
α0+β0+1 R−c ≥ γR.
D(a; p) = 1 is
The equilibrium condition states that this must hold true for
all received samples. Thus, regardless of the sample that the
attacker receives, it must believe with a sufﬁciently high
conﬁdence that target a will yield reward R. This is only
true when α0 is much larger than β0, characterized by the
shaded region in Fig. 2a. The reasoning for the second region
follows similarly (see Fig. 2b).

For ﬁxed R, c, and γ,

α0+θ

In the third region, the attacker follows its observation and
the defender defends the more valuable target. That is, if the
attacker receives a positive sample, θ = 1, it attacks target

Fig. 3: Region for (α0, β0) such that σ∗
D(a; p) = I{p > 1/2}.
σ∗

A(a; θ = 1) = 1 when

The reasoning for σ∗
A(a; θ = 0) = 0 (derived from the upper
bound condition in Lemma 2) follows identically. Combining
the lower and upper bounds, the equilibrium holds in the
intersection of the two regions, as illustrated by Fig. 4.

(a)

(b)

(c)

Fig. 4: Region (shaded/blue area in (c)) for (α0, β0) such that
D(a; p) = I{p > 1/2}, is
A(a; 0) = 0, σ∗
σ∗
a pure strategy equilibrium.

A(a; 1) = 1, and σ∗

We now vary the capture cost to see how the equilibrium-
supporting regions change. In particular, we look at the three
regions in Lemma 2 as we increase c. As c is increased,
see Figs. 5a – 5c, the attacker is increasingly concerned
with getting caught. This requires the attacker to have
an increasing level of conﬁdence that it will get R from
choosing a in order to outweigh the higher risk of getting
caught. For large enough c, such that R − c < γR, all pure
strategy equilibrium regions vanish. Committing to a choice
of either a or b (a pure strategy) becomes too risky, causing
the attacker to want to (either partially or fully) mix between
targets. In other words, a sufﬁciently large c will partially
deter an attack on either of the targets. A full characterization
of these mixed strategies is left for future work.

IV. CONCLUDING REMARKS

Motivated by cyber security settings, we have introduced
a simple asymmetric information game model for describing
the inﬂuence of a learner’s (the attacker) single private
sample on the inference process of an observing agent (the
defender). The subsequent game admits at most one pure
strategy equilibrium which, depending on the parameters of
the game, takes different forms. We illustrated that, even in
the case where the attacker is conﬁdent that target a will
produce a reward higher than the known reward associated
with the other target, it is not necessarily optimal for the
attacker to attack target a as it has an increased chance
of being caught. Future work includes considering multiple
unknown targets and allowing the attacker to make sampling
decisions. If these choices (but not the received samples)
are observable to the defender, then the game becomes a
signaling game.

REFERENCES

[1] H. Tavafoghi, Y. Ouyang, and D. Teneketzis, “A uniﬁed approach
to dynamic decision problems with asymmetric information-part II:
Strategic agents,” arXiv preprint arXiv:1812.01132, 2018.

[2] Y.-C. Ho, M. Kastner, and E. Wong, “Teams, signaling, and informa-
tion theory,” IEEE Transactions on Automatic Control, vol. 23, no. 2,
pp. 305–312, 1978.

[3] H. Tavafoghi, Y. Ouyang, and D. Teneketzis, “A uniﬁed approach to
dynamic decision problems with asymmetric information-part I: Non-
strategic agents,” arXiv preprint arXiv:1812.01130, 2018.

[4] P. Bolton and C. Harris, “Strategic experimentation,” Econometrica,

vol. 67, no. 2, pp. 349–374, 1999.

[5] A. Mahajan and D. Teneketzis, “Multi-armed bandit problems,” in
Foundations and Applications of Sensor Management. Springer, 2008,
pp. 121–151.

[6] M. Woodroofe, “A one-armed bandit problem with a concomitant
variable,” Journal of the American Statistical Association, vol. 74,
no. 368, pp. 799–806, 1979.

[7] D. A. Berry and B. Fristedt, “Two-armed bandits with a goal, I. One
arm known,” Advances in Applied Probability, vol. 12, no. 3, pp. 775–
798, 1980.

[8] D. Rosenberg, E. Solan, and N. Vieille, “Social learning in one-arm
bandit problems,” Econometrica, vol. 75, no. 6, pp. 1591–1611, 2007.
[9] P. Heidhues, S. Rady, and P. Strack, “Strategic experimentation with
private payoffs,” Journal of Economic Theory, vol. 159, pp. 531–551,
2015.

[10] Y. Mansour, A. Slivkins, V. Syrgkanis, and Z. S. Wu, “Bayesian explo-
ration: Incentivizing exploration in Bayesian games,” in Proceedings
of the 2016 ACM Conference on Economics and Computation. New
York, NY, USA: ACM, 2016, p. 661.
´E. Borel, “The theory of play and integral equations with skew
symmetric kernels,” Econometrica, pp. 97–100, 1953.

[11]

[12] A. Ferdowsi, A. Sanjab, W. Saad, and T. Bas¸ar, “Generalized Colonel
IEEE,

Blotto game,” in 2018 American Control Conference (ACC).
2018, pp. 5744–5749.

[13] A. Gupta, T. Bas¸ar, and G. A. Schwartz, “A three-stage Colonel
Blotto game: When to provide more information to an adversary,” in
International Conference on Decision and Game Theory for Security.
Springer, 2014, pp. 216–233.

[14] A. Sinha, F. Fang, B. An, C. Kiekintveld, and M. Tambe, “Stackelberg
security games: Looking beyond a decade of success,” in IJCAI, 2018,
pp. 5494–5501.

[15] D. Fudenberg and J. A. Tirole, Game Theory. MIT Press, 1991.

APPENDIX I
PROOFS

A. Proof of Lemma 1

Denote by uA(a, a) the attacker’s subjective payoff when
both players select a as dictated by the subjective payoffs
in Table I (a), similarly for uA(a, b), uA(b, a), and uA(b, b).
D(a; p) | θ(cid:3) by E[σ∗
Also, denoting Ep
D] and substituting
in subjective payoffs, the attacker’s expected payoff is
Ep

(cid:2)σ∗

(cid:2)uA(σA, σ∗
(cid:18)

D) | θ(cid:3)
uA(a, a)E[σ∗

= σA

D] + uA(a, b)(cid:0)1 − E[σ∗

D](cid:1)

(cid:19)

(cid:18)

+ (1 − σA)
(cid:18)(cid:18) α0 + θ

uA(b, a)E[σ∗
(cid:19)

R − c

E[σ∗
D]

= σA

D] + uA(b, b)(cid:0)1 − E[σ∗

D](cid:1)

(cid:19)

α0 + β0 + 1

+

α0 + θ
α0 + β0 + 1
+ (1 − σA)(cid:0)γRE[σ∗

R(cid:0)1 − E[σ∗

D](cid:1)

(cid:19)

D] + (γR − c)(cid:0)1 − E[σ∗

D](cid:1)(cid:1)

= γRE[σ∗

D] + (γR − c)(1 − E[σ∗
(cid:18)(cid:18) α0 + θ
(cid:19)

− γ

D])
R + c(cid:0)1 − 2E[σ∗

(cid:19)

D](cid:1)

+ σA

α0 + β0 + 1

(a) Regions under c = 0.15.

(b) Regions under c = 0.30.

(c) Regions under c = 0.45.

Fig. 5: Sensitivity analysis for the pure-strategy equilibrium regions of Lemma 2 as a function of c (for R = 1, γ = 1/2).

The ﬁrst two terms do not inﬂuence the argmax and thus
α0+β0+1 − γ(cid:1)R +
one can equivalently maximize σA
c(cid:0)1 − 2E[σ∗
D](cid:1)(cid:1). Similarly, using Table I (b) and denoting
Eθ
Eθ

A], the defender’s expected cost is

(cid:0)(cid:0) α0+θ

(cid:2)σ∗
(cid:2)uD(σ∗
= σD

A(θ) | p(cid:3) by E[σ∗
A, σD) | p(cid:3)
(cid:0)uD(a, a)E[σ∗
+ (1 − σD)(cid:0)uD(a, b)E[σ∗
(cid:0)(pR − c)E[σ∗
+ (1 − σD)(cid:0)pRE[σ∗

= σD

A] + uD(b, a)(1 − E[σ∗

A])(cid:1)

A] + uD(b, b)(1 − E[σ∗

A])(cid:1)

A] + γR(1 − E[σ∗

A])(cid:1)

A] + (γR − c)(1 − E[σ∗

A])(cid:1)
(cid:0)1 − 2E[σ∗

= γR − c + E[σ∗

A](pR − γR + c) + cσD

A](cid:1)
Again, the constants do not inﬂuence the argmin and one
can equivalently minimize σD
B. Proof of Lemma 2

(cid:0)1 − 2E[σ∗

A](cid:1).

Throughout the proof, let qθ = σA(a; θ). The attacker’s
(cid:2)qθ |

expected strategy from the defender’s perspective is Eθ
p(cid:3) = (1 − p)q0 + pq1. There are four possible cases:
1.i) q0 = q1 = 1: Assume the attacker plays a regardless
of its type, i.e., qθ = 1 ∀ θ. Thus Eθ[qθ | p] = 1 and by
Lemma 1, the defender’s best response is σ∗
D(a; p) = 1 ∀ p.
The attacker does not deviate iff

(cid:18) α0 + θ

α0 + β0 + 1

− γ

(cid:19)
R + c(cid:0)1 − 2Ep
(cid:18) α0 + θ

⇔

(cid:2)σ∗

D(p) | θ(cid:3) ≥ 0
(cid:19)

− γ

R ≥ c

α0 + β0 + 1

1.ii) q0 = q1 = 0: Assume the attacker plays b regardless
of its type. Then Eθ[qθ | p] = 0 and the best response of
the defender is σ∗
D(a; p) = 0 ∀ p. Similar to case (1.i), the
attacker does not deviate iff (cid:0)γ − α0+θ
α0+β0+1
1.iii) q0 = 1, q1 = 0: Assume the attacker plays q0 = 1, q1 =
0. Then Eθ[qθ | p] = 1−p. The best response (almost surely)
D(a; p) = I{p < 1/2}. By Lemma 1,
for the defender is σ∗
the attacker plays q0 = 1 iff
(cid:18)

(cid:1)R ≥ c.

1
2

1 +

(cid:16)

R
c

α0
α0 + β0 + 1

(cid:17)(cid:19)

− γ

Ep

(cid:2)σ∗

D(a; p) | 0(cid:3) <

or equivalently, using the fact that Ep
E[I{p < 1/2} | θ = 0] = P (p < 1/2 | θ = 0),
1
2

α0
α0 + β0 + 1

I1/2(α0, β0 + 1) ≤

R
c

1 +

(cid:18)

(cid:16)

(cid:2)σ∗

D(a; p) | θ = 0(cid:3) =

By a similar argument, the attacker plays q1 = 0 iff
(cid:18)

(cid:16) α0 + 1

I1/2(α0 + 1, β0) ≥

1 +

(cid:17)(cid:19)

− γ

(cid:18)

1 +

=

1
2

(cid:16)

R
c

α0
α0 + β0 + 1

− γ

α0 + β0 + 1
R
2c(α0 + β0 + 1)

+

(4)

1
2

R
c
(cid:17)(cid:19)

Conditions (3) and (4) imply that,

I1/2(α0 + 1, β0) − I1/2(α0, β0 + 1) ≥

R
2c(α0 + β0 + 1)

Using consecutive neighbor identities of the regularized
incomplete beta function, the above inequality becomes

−

1
B(α0 + 1, β0 + 1)

(cid:16) 1
2

(cid:17)α0+β0−1

≥

R
c

which can never be satisﬁed, thus at least one of Eqs. (3)
and (4) is violated.
1.iv) q0 = 0, q1 = 1: Assume the attacker plays q0 = 0,
q1 = 1. Then Eθ[qθ | p] = p. The defender’s best response
D(a; p) = I{p > 1/2}. The attacker plays q0 = 0 iff
is σ∗
(cid:17)(cid:19)

(cid:18)

(cid:16)

1
2

1 +

R
c

α0
α0 + β0 + 1

− γ

Ep

(cid:2)σ∗

D(a; p) | 0(cid:3) >

Since E[σ∗
equivalent condition is

D(a; p) | θ = 0] = 1 − P (p ≤ 1/2 | θ = 0), an

I1/2(α0, β0 + 1) ≤

(cid:18)

1
2

1 +

(cid:16)

R
c

γ −

α0
α0 + β0 + 1

(cid:17)(cid:19)

Similarly, the attacker plays q1 = 1 iff

(cid:18)

I1/2(α0 + 1, β0) ≥

α0 + 1
α0 + β0 + 1
Or equivalently, by consecutive neighbor identities,

R
c

γ −

1 +

1
2

(cid:16)

(cid:17)(cid:19)

.

.

I1/2(α0, β0) ∈

(cid:34)

(cid:18)

1 +

(cid:18)

1 +

1
2

1
2

(cid:16)

γ −

R
c

α0 + 1
α0 + β0 + 1

(cid:17)(cid:19)

+

(cid:16)

R
c

γ −

α0
α0 + β0 + 1

(cid:17)(cid:19)

−

(cid:17)α0+β0

(cid:16) 1
2

α0B(α0, β0)
(cid:16) 1
2

(cid:17)α0+β0

β0B(α0, β0)

,

(cid:35)

(cid:17)(cid:19)

− γ

.

(3)

which can be seen to be non-empty by selecting R and c
appropriately.

