CELEST: Federated Learning for Globally
Coordinated Threat Detection

Talha Ongun∗, Simona Boboila, Alina Oprea, Tina Eliassi-Rad
Northeastern University
Jason Hiser, Jack Davidson
University of Virginia

2
2
0
2

t
c
O
3
1

]

R
C
.
s
c
[

2
v
9
5
4
1
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—The cyber-threat landscape has evolved tremen-
dously in recent years, with new threat variants emerging daily,
and large-scale coordinated campaigns becoming more prevalent.
In this study, we propose CELEST (CollaborativE LEarning
for Scalable Threat detection), a federated machine learning
framework for global threat detection over HTTP, which is one of
the most commonly used protocols for malware dissemination and
communication. CELEST leverages federated learning in order
to collaboratively train a global model across multiple clients
who keep their data locally. Through a novel active learning
component integrated with the federated learning technique, our
system continuously discovers and learns the behavior of new,
evolving, and globally-coordinated cyber threats. We show that
CELEST is able to expose attacks that are largely invisible to
individual organizations. For instance, in one challenging attack
scenario with data exﬁltration malware, the global model achieves
a three-fold increase in Precision-Recall AUC compared to the
local model. We also design a poisoning detection and mitigation
method, DTrust, speciﬁcally designed for federated learning in
the collaborative threat detection domain. We deploy CELEST
on two university networks and show that it is able to detect
the malicious HTTP communication with high precision and low
false positive rates. Furthermore, during its deployment, CELEST
detected a set of previously unknown 42 malicious URLs and
20 malicious domains in one day, which were conﬁrmed to be
malicious by VirusTotal.

*Authors are ordered by contribution.

I.

INTRODUCTION

Modern cyber attacks have become sophisticated, coordi-
nated, and are operating at global scale. We have witnessed
globally-coordinated campaigns with the ability to spread to
hundred of thousands of victim machines on the Internet [73],
[5]. While attackers exploit a wide range of vulnerabilities
in various protocols, HTTP has become one of the prevalent
communication protocols for malware dissemination [34]. To
attackers’ advantage, malicious communication over the HTTP
protocol can easily blend in with the large volumes of benign
trafﬁc and is rarely blocked. Existing defenses for HTTP mal-
ware include network intrusion detection systems, as well as
machine learning (ML) methods applied to domain names [11],
[27], URLs [45], [46] or HTTP logs [8], [16], [33], [53],
[57], [58]. However, these methods are usually used inside
a single organizational network and have limited capabilities
of detecting attacks not seen at training time.

An important open question for thwarting global mal-
ware on the Internet is how to leverage defender collabora-
tion and enable coordinated cyber defenses. To date, inter-
organizational cooperation is used primarily for sharing threat

intelligence in the form of Indicators of Compromise (IoC),
such as IP addresses, domain names, and URL patterns used
during an attack [35], [78]. However, this approach has well-
known limitations as it relies on detection of ongoing attacks
and their associated IoCs, while attackers can change their
infrastructure and behavior to make the detected IoCs obso-
lete [29], [47], [77]. This observation leads to the natural ques-
tion: Are there other, more proactive and reliable approaches
to global defense coordination that could be effective against
evolving, sophisticated cyber threats?

In this work, we answer the above question afﬁrmatively by
presenting CELEST, a federated machine learning framework
for global HTTP-based threat detection. CELEST enables
collaboration among defenders to globally train neural network
models for HTTP malware detection. The main insight in
designing CELEST is to facilitate knowledge transfer among
participants through a collectively trained model. A global
machine learning model captures data diversity and thus be-
comes a powerful tool for uncovering a wider range of malware
behavior characteristics. This approach enables clients to detect
malware seen for the ﬁrst time in their networks, which a
locally trained ML model will not identify.

In real-world deployments of ML threat detection systems,
the vast majority of data is unlabeled, and the ground truth of
malicious samples is limited. Additionally, supervised defenses
often fail to detect novel attacks not encountered in training,
and it is difﬁcult to get accurate labels of malicious activity on
a network. To address these concerns, we propose the design of
a new active learning component into the CELEST federated
learning framework with the goal of increasing the labeled
set during the training phase and, eventually, enhancing the
global model’s detection capabilities. In our design, we use
active learning to identify a small set of anomalous samples
for investigation in each round of training, and augment the
training set with maliciously labeled samples.

We evaluate CELEST using a large dataset of HTTP logs
collected at the border of two university networks. We also use
three public datasets from different malware families (Mirai,
Gafgyt, and the data exﬁltration malware dataset from [16]),
as well as attack recreation data generated on the university
networks. We show that, across all
the malware families
we considered, the global model outperforms local models
trained on a single client’s data. The improvements obtained
by global models are signiﬁcant: For instance,
the global
model is able to detect a data exﬁltration malware family
with three times higher Precision Recall AUC (PR-AUC) than
the local model. Importantly, global defenses enable clients

 
 
 
 
 
 
to detect new malware in their environments (i.e., malware
that they have not seen in training), which is learned from the
models shared by other clients participating in the federated
protocol. We further demonstrate that when the active learning
is enabled, CELEST is able to detect completely new malware,
for which labels are not available in training at any of the
clients. In particular, the key intuition to enable new malware
discovery is to include an anomaly detection module trained
on each client network with the goal of detecting and labeling
anomalies in the network. Often, new attack instances will
result in anomalous trafﬁc relative to the benign trafﬁc. Once
the anomalies are conﬁrmed as attack instances, they are added
to the next round of federated learning training, and the global
model will learn to recognize these newly discovered attacks.

One of the important threats against federated learning is
the adversarial manipulation by malicious or compromised
clients to poison the model [7], [72], [28], [68], [79]. We
design a new defense technique speciﬁc to threat detection,
called DTrust (Distributed Trust), with the goal of training in
the presence of poisoning attacks by identifying and removing
the malicious clients. The benign clients evaluate locally if
the global model received from the server can be trusted,
and notify the server if a large performance degradation is
observed. The server investigation consists of inspection of
individual model updates and identifying the clients that sent
”bad” updates to remediate the attack. DTrust relies on the
insight that the most client organizations in a collaborative
threat detection system act in good faith, and they are incen-
tivized to actively participate in defense and validate the model
during the training process to protect against poisoning attacks.
We evaluate DTrust in three poisoning scenarios in which a
number of compromised clients inject different attack patterns
to be misclassiﬁed by the global model. In one scenario, the
global model’s PR-AUC degrades from 0.93 to 0.11 when
no defense is deployed, but DTrust restores the PR-AUC to
0.93 after identifying the malicious clients and removing them
from training. Finally, we deploy CELEST on two university
networks using three attack recreation exercises performed at
intervals of several months. We show that CELEST detects
the malicious communication carried out during the attack
recreation with high precision and false positive rates lower
than 3.3×10−5 on both networks. Moreover, the model trained
during the ﬁrst experiment still has strong performance on
an evasive attack exercise performed four months later. In
addition, CELEST detected a set of 42 previously-unknown
malicious URLs and 20 domains on the two networks, which
were conﬁrmed malicious by VirusTotal. To summarize, our
contributions are:

• Federated learning for global cyber defense:

We design CELEST, a scalable and privacy-preserving
framework for federated training of global neural network
models, which enables early-stage HTTP malware detec-
tion at participating organizations.

• Active learning for limited ground truth: We introduce
a novel active learning component in our federated design,
which selects samples for investigation and labeling us-
ing an anomaly detection module, and thus enables the
discovery of completely new attacks.

• Poisoning mitigation: We design DTrust, a new poison-
ing detection and mitigation method speciﬁcally designed
for federated learning in the collaborative threat detection

domain. DTrust successfully detects poisoning clients
using the feedback from participating clients to investigate
and remove them from the training process.

• Comprehensive evaluation: We evaluate CELEST using
large-scale datasets from two university networks, public
traces from three malware families, and three attack
recreation exercises. We show that global models trained
in CELEST have high precision and recall, and low false
positive rates, improving signiﬁcantly upon locally trained
models. We further demonstrate the impact of poisoning
attacks and effective mitigation with DTrust.

• Deployment and detection of unknown malware: We
deploy CELEST on two university networks and ﬁnd
instances of previously-unknown malware (42 malicious
URLs and 20 malicious domains).

II. PROBLEM STATEMENT AND THREAT MODEL

In this section, we discuss the problem of HTTP malware
detection, the limitations of existing solutions, and introduce
our system requirements and threat model.

HTTP-based malware. HTTP is one of the most widely used
protocols by adversaries to perpetrate malicious activities, such
as data exﬁltration [6], vulnerability exploitation [51], and
covert channel communication [54]. Command-and-control
(C2) servers often use HTTP to send commands to com-
promised systems and receive stolen data [70]. Prior work
proposed a variety of methods for detecting malicious network
activity over HTTP, including URL-based detection [46], [48],
detection using web-proxy logs [8], [33], [53], [57], [58],
and application ﬁngerprinting [16], [17], [60]. We focus on
malicious activity detection using HTTP logs for several
reasons. First, while malicious URL detection is shown to
be successful in mitigating certain attacks (e.g., phishing),
HTTP-based detection methods are able to address a variety
of attack surfaces used by different malware families. Second,
the HTTP-based methods avoid deep inspection of the payload
which could be obfuscated by attackers, and rely only on the
HTTP headers for increased scalability and lower computation
costs. Lastly, these approaches utilize common HTTP logs
collected by web proxies installed at the border of enterprise
networks, which most enterprises’ use as part of their perimeter
control.

Most of the existing machine learning detection techniques
on HTTP logs attempt
to detect malware activity within
a single network by training local models on labeled data
available in that network [33], [8], [57], [58]. As attackers
employ various techniques to evade detection, such as chang-
ing the C2 protocol, or changing the domain names and the
IP addresses of the C2 infrastructures, local models will fail
at detecting these. In this work, we address the problem of
designing global detection of HTTP malware by coordination
among multiple participating organizations. We believe that
collaboration among multiple defenders is critical in enabling
a better and more resilient cyber defense strategy and will lead
to more effective threat detection methods. Unfortunately, the
state of the art in collaboration among multiple defenders is
based on threat intelligence sharing platforms, which enable
sharing of indicators of compromise (IoCs) after attacks are
detected. IoCs have limited utility at detecting cyber threats,

2

particularly as IoCs become stale with small changes to at-
tackers’ malicious infrastructures or communication protocols.
We are interested in more proactive, coordinated approaches
among defenders that can resist evolving cyber attacks.

In designing our system, we identiﬁed several requirements

for real-world deployment in participating organizations:

• Globally coordinated detection: We are interested in
global methods that expose attacks promptly, when they
are still largely invisible to individual organizations.
• Real-time processing: We require real-time processing
of HTTP logs to generate predictions on individual HTTP
logs when the model is deployed. This approach enables
early-stage attack detection and thus minimizes the dam-
age inﬂicted by an attack.

• Scalability: We would like to run the system on multiple

networks with large volumes of logs.

• Log privacy: The system should require minimal data
sharing across the participating clients, in order to ensure
the privacy of sensitive security logs.

• High accuracy: To be used in production, the system
should have high accuracy, precision, and recall, as well
as low false positive rates across different classes of
HTTP-based malware attacks.

Challenges. Detecting malicious communication over
the
HTTP protocol is challenging for multiple reasons: First, the
malicious trafﬁc transmitted on the HTTP protocol is blending
in with large volumes of legitimate trafﬁc generated by users,
making detection very difﬁcult. Second, there is a high im-
balance among the malicious and benign samples on a single
network, making it challenging to train supervised learning
methods with high accuracy and low false positive rates [69].
Third, most existing systems for HTTP malware detection
employ some form of aggregation of events from multiple
logs. For instance, beaconing detection methods apply time
series analysis on multiple communication events to the C2
server to detect periodic communication [33]. It is challenging
to detect malicious HTTP communication in real-time, as it
requires generating a prediction on individual log events.

Threat Model. We aim to design a globally coordinated
system with multiple participating clients for detection of
malicious activity over the HTTP protocol. The malicious
communication could be part of multiple stages of a malware
campaign, including malware delivery, the C2 communication
with the malicious server, and data exﬁltration activities.

We assume that the attacker compromises one or several
victim machines within the participating networks. Hosts in
the network can be infected in a variety of ways, such as vul-
nerability exploits, social-engineering, and drive-by download
attacks. The infection vector might not occur over HTTP and
therefore, our system might not be able to detect the initial
infection. However, our goal is to detect any subsequent ma-
licious communication over HTTP, which is a common com-
munication channel once attackers have established foothold
in a network.

Our system analyzes HTTP logs collected at the border of

the monitored networks.

The participating organizational networks run local compu-
tation on the collected logs for training local models, and share

those with a central server for aggregating a global model.

We assume that the central aggregator correctly follows the
protocol for aggregating the local updates into a global model.
However, the clients participating in the protocol might be
subject to data poisoning attacks (in which the logs they collect
are under the control of an adversary), or model poisoning
attacks (in which the adversary controls the updates sent to the
central aggregator). Our goal is to train models that identify
the malicious activity over HTTP, while providing resilience
against poisoning.

III. SYSTEM OVERVIEW AND METHODOLOGY

In this section, we present an overview of CELEST, fol-
lowed by a detailed description of each system component.
CELEST is a federated framework for cyber defense, where a
set of participating clients collaboratively learn a global model
G from HTTP logs collected at their network border. The
goal of the global model is to learn a classiﬁer that generates
predictions regarding whether individual HTTP logs are Mali-
cious or Benign. We consider the cross-silo federated learning
setting, suitable for a relatively small number of clients, in
which all clients participate in each round of training [37].
Following the federated learning paradigm [49], in a training
iteration t ∈ {1, ..., T }, each client i ∈ {1, ..., n} locally trains
a local model W t
i based on the previous global model Gt−1,
by performing stochastic gradient descent (SGD) updates on a
subset of its local data, Dt
i. The clients send their local model
updates to the server, which aggregates them to produce the
updated global model Gt and distribute it to the clients. The
process continues iteratively until the global model converges.

The training data (HTTP logs) Di maintained by each
client i locally is labeled as Malicious or Benign. We discuss
in Section IV how we generate ground truth for data labeling.
Before training, the data undergoes a feature extraction phase
where individual HTTP logs are processed to generate a set of
5862 features in three categories – embedded, numerical and
categorical (Section III-A). We propose the use of embedded
features from text-based HTTP ﬁelds (URL, domain and web
referer) adapting word embedding representations from natural
language processing (NLP). Word embeddings are generated
with deep learning models such as Word2Vec [52] and Fast-
Text [14] that are trained to capture contextual and semantic
similarities in the text, such that similar words will also be
closer in the embedded vector space.

CELEST uses federated learning techniques for two tasks:
(1) generating embedded feature representations, and (2) train-
ing a neural network classiﬁer for detection of malicious HTTP
trafﬁc. For the ﬁrst task, we introduce a federated method in
which each client updates sequentially a shared embedding
model using their own data corpus. For the second task, clients
use their local HTTP feature vectors to collectively train a
global model that is able to detect various attack patterns.
CELEST introduces a novel Active Learning component that
uses a local anomaly detection module to augment the ground
truth of malicious activities through sample selection (Sec-
tion III-B). In addition, CELEST employs a novel distributed
defense mechanism against poisoning attacks, in which the
clients themselves help identify malicious attempts to corrupt
the global model (Section III-C). We detail each component
of CELEST in the following sections.

3

Fig. 1: Federated embedding model training for URL representation. We generate embedded features for URL, Domain and
Referer. In addition to embedded features, we also include numerical features and categorical features.

A. Feature Representation

Our system is unique in its requirement for real-time
processing of HTTP log events. Thus, timing features and
aggregated features over multiple HTTP logs or ﬂows are
not applicable in our setting. We leverage all the available
ﬁelds in HTTP logs, except the source IP address (since our
detection methods are not speciﬁc to the host machine), and
source port (which does not carry information related to our
detection task). We include three feature categories: embedding
features for domain, URL, and web referer representation,
categorical features for external IP subnet, port, user agent
string, method, status code, and content type, and numerical
features for request and response size, transaction depth, and
browser version. We summarize all features in Table IX from
Appendix A.

URLs are one of the HTTP ﬁelds particularly susceptible
to adversarial manipulation; they are often used by malware
to communicate information with C2 servers, through various
parts of the URLs, including sub-domain, parameters, query
string, ﬁle name, and fragment. Hence, it is important to have
an effective, semantics-aware feature representation of URLs.
Different from prior work on URL feature representation [13],
[45], [46], [39], [65], [40], we propose an embedding model
which preserves the semantic structure of the URL, handles
time, and can be trained in a
new tokens at deployment
federated fashion.

In Figure 1, we present our design for creating the em-
bedding representation in a federated unsupervised manner.
Initially, we parse the URLs into tokens representing different
categories (e.g., domain, path, query string) to preserve the
structure of an URL. Each token is considered a word, and each
URL is viewed as a sentence of words. Our contribution is the
design of a more scalable and privacy-preserving approach for
training the embeddings model among multiple participants
via distributed sequential client updates. In this approach,
each client has access to its own data and updates the global
embeddings model locally, using its entire corpus. The client
sends the updated global model back to the server, who

acts as a trusted central coordinator. The clients apply their
updates sequentially, in a round robin fashion, over multiple
iterations, as illustrated in the top part of Figure 1. CELEST
uses federated FastText as the preferred embedding method
for URL representation. Compared to Word2Vec, FastText has
the advantage that the dictionary is a set of n-grams, hence
it is well known to all participants in the protocol and does
not need to be shared. Additionally, FastText embeddings can
handle new tokens encountered at deployment time. Other
considerations on our design choices for feature representation
are described in Appendix A.

B. Active Federated Learning

In this work, we employ the commonly used Federated
Averaging (FedAvg) [49] training method, where the client
models are weighted by their dataset size and averaged to
||Dt
update the global model as follows: Gt = (cid:80)n
||Dt|| × W t
i ,
where ||Dt|| = (cid:80)n
i=1 ||Dt
i||. The local training on each client
is carried out using a Feed-Forward Neural Network (FFNN)
model based on our feature representation.

i=1

i ||

Training supervised models such as FFNN requires labeled
data, a known challenge in cyber security [69] where the
vast majority of data available in real-world deployments is
unlabeled. Furthermore, existing defenses that might work well
on previously seen attacks often fail at detecting new emerging
threats, making it particularly challenging to get accurate labels
of malicious activity on a network. To address these concerns,
we integrate an active learning component into CELEST with
the goal of augmenting the ground truth of malicious activities
incrementally, at each iteration of the training phase. Thus,
the global model’s detection capabilities are improved in a
streaming fashion, similar to online learning where true class
labels of new incoming samples are usually unknown [44].
This approach ﬁts particularly well with federated learning,
which is designed for continuous training over time, and thus
can account for malware evolution over time. To the best of
our knowledge, CELEST is the ﬁrst threat detection system

4

HTTP LogsReceive ModelSend Updated ModelServerUpdate EmbeddingsParse URLs into tokensfrom Common VocabularyNetwork -1. . .  HTTP LogsReceive ModelSend Updated ModelUpdate EmbeddingsParse URLs Network -n. . .  . . .  Embeddings Training(Global Embeddings Model)Feature Vector GenerationHTTP LogsprotocolfragmentTokenizationhttp://www.abc.co.uk/test/test1?q=val#fragsubdomaindomainsuffixpathqueryhttpfrag…Sequence of TokensGlobal Embeddings ModelNetwork -iProtocol TokensSubdomain TokensFragmentTokens…URL Feature VectorNumerical FeaturesCategorical Featuresusing active learning in a federated method of training with
limited labeled data.

The Active Federated Learning mechanism is illustrated in
Figure 2. At time step t, the global model Gt is used to rank
the unlabeled data of client i. In addition, an anomaly detector
is used to rank the remaining unlabeled data samples by their
anomaly score. The top ranked samples are investigated and
labeled by a security analyst and used in next iterations. We
consider a small budget b of samples that are investigated
and labeled by a human expert at one of the participating
organizations i, and time step t. Several strategies for selecting
instances to be labeled in active learning have been previously
explored, such as random sampling, uncertainty sampling [41],
expected error reduction [64], and variance reduction [25].
In our case, we are interested in expanding the malicious
ground truth, therefore we propose two strategies in a hybrid
approach for sampling the most suspicious HTTP log events to
be investigated. The ﬁrst strategy consists of using the global
model Gt, represented by a Feed-Forward Neural Network
classiﬁer, to evaluate and rank the unlabeled dataset; the most
suspicious b/2 samples are then investigated and labeled by
a security analyst. These are likely similar to previously seen
malicious samples. The second strategy consists of using a
local anomaly detection module to rank the remaining unla-
beled data according to an anomaly scoring method; the most
anomalous b/2 samples are then investigated and labeled by
a security analyst. The second strategy identiﬁes anomalous
logs that could augment the ground truth with new attack
samples not seen in training before. The combination of the
two strategies is critical for CELEST’s ability to detect attacks
with very limited ground truth. The samples labeled malicious
are added back to the training dataset of client i and used in
the next iterations of federated learning. We focus on labeling
malicious samples only, as some adversarial actions might look
benign during stealthy attacks. We assume the human analysts
make the correct labeling decision, and note that learning from
noisy labels is an extensively studied ML topic on its own, not
speciﬁc to our system [30], [36].

We demonstrate in our evaluation section that active fed-
erated learning can be very powerful in some scenarios, even
when starting the FL training process with zero labels. Our
anomaly detection module uses the well-known Isolation For-
est algorithm [43], however other anomaly detection methods
such as local outlier factor (LOF) [18], one-class support
vector machines (SVM) [67], and clustering[19] can also be
employed. To increase resilience, we train an ensemble of
Isolation Forest models on multiple time windows, where each
time window corresponds to one FL iteration (time step).
Speciﬁcally, given a client i, the anomaly detector is trained
on unlabeled data from the previous k time windows (i.e., the
datasets Dt−k
), while anomaly detection is carried
out on the current time window t (i.e., the dataset Dt
i). For
instance, a good trade-off between speed and performance was
reached in our experiments at k = 3.

, ...Dt−1
i

i

C. Resiliency to Poisoning Attacks

Due to its distributed nature, federated learning is vulnera-
ble to adversarial manipulation by malicious or compromised
clients. Poisoning attacks that corrupt the training data [76] or
the client model updates [7], [72], [28], [68], [79] have been

5

Fig. 2: The Active Federated Learning framework. At time step
t, the global model Gt is used to rank the unlabeled data of
client i. In addition, an anomaly detector is used to rank the
remaining unlabeled data samples by their anomaly score. The
top ranked samples are investigated and labeled by a human
expert and used in the next iterations.

limit

an important attack vector studied in recent years. Existing
defenses can be classiﬁed as: (i) defenses that
the
contribution of each client to the global model [72] or perform
anomaly detection to remove speciﬁc client updates [12], [50],
[82]; (ii) defenses against backdoor poisoning that detect and
mitigate the presence of a trigger backdoor [63], [56]; (iii)
defenses that leverage a trusted dataset at the server to ﬁlter
malicious updates [22]. We are interested in developing a
general defense against both data and model poisoning attacks,
that leverages the speciﬁcs of our threat detection setting.

DTrust algorithm: We propose a novel defense algorithm
called DTrust (Distributed Trust), with the goal of training a
resilient federated model in the presence of poisoning attacks.
In essence, DTrust is a distributed algorithm where the benign
clients locally evaluate if the global model received from
the server can be trusted, and notify the server if a large
performance degradation is observed. We introduce a novel
distributed scheme of bootstrapping trust, in which the clients
take an active role in detecting poisoning attacks. Each client
i interested in participating in CELEST’s DTrust defense
maintains a small dataset DT rust
called the trust dataset,
i
which they share with the server in case performance degra-
dation is noticed. Our insight is that multiple clients having
individual trust datasets cover a wider data range with more
opportunity for attack detection; intuitively, one attack that is
undetected by one client might stand out on another client’s
trust dataset. DTrust is particularly suited for our threat model,
where the clients are organizations (rather than IoT devices
or individual machines), hence they have enough resources to
actively participate in defense.

DTrust fully exploits the distributed nature of FL,
in
contrast to centralized trust-based schemes that rely on the
server to detect adversarial attempts [28], [22]. In CELEST
clients can maintain and share their trust datasets with the
server, and the server can leverage publicly available tools
(e.g., VirusTotal, third-party threat intelligence platforms, etc.)
to ensure that the trust datasets are clean. For instance, the trust
dataset could contain command-and-control trafﬁc to a domain,

Classifier EvaluationUnlabeled HTTP LogsAnomaly        DetectionSampleSelectionInvestigate and LabelLocal TrainingHTTP LogsFeature ExtractionBenign  MaliciousRankedSamplesActive Learning ComponentFederated TrainingGlobalModelAggregate UpdatesFeature ExtractionClient iServerwhere the domain has a high score on VirusTotal. Such tools
may not be readily available in other application domains (e.g.,
image classiﬁcation, human activity recognition, etc.) where
trust-based defenses have been explored [22], making this
approach particularly feasible for our cyber security setting.

A single iteration of DTrust at time step t for client i
and the server can be summarized as follows. When a client
receives the global model Gt from the server, it evaluates
the model on its trust dataset. In our implementation, we use
the cross-entropy loss metric L, although other performance
metrics could also be used (e.g., error rate, PR-AUC score, F1-
score). The loss impact at client i is deﬁned as L−Lmin
, where
Lmin
L, Lmin are the current loss and the minimum loss across
previous iterations, respectively. If the loss impact exceeds a
threshold T , then the client notiﬁes the server and sends the
iteration number tbest of the previous best-performing global
model and its trust dataset (along with its model updates) to
the server for investigation.

The server maintains a history database H, containing: (1)
the previous global models G1, ..., Gt−1, and (2) the most
recent model updates W t−1
received from each client i at time
step t − 1. When the server is notiﬁed of a large performance
degradation by one of the clients, it uses that client’s trust
dataset to investigate which update from the previous round of
training t − 1 has caused the performance drop, as follows:

i

1) Conﬁrm that the trust dataset sent by the client is legiti-
mate, i.e., contains malicious samples that can be veriﬁed
using threat intelligence tools.

2) Validate that the global model at tbest performs well on
the trust dataset, whereas the current model is signiﬁ-
cantly worse.

3) Leave one client m out and aggregate a global model

G(cid:48)

t−1 without m’s updates.
4) Evaluate the global model G(cid:48)

received.

t−1 on the trust dataset

5) Compute the loss
t−1

Lt−1−L(cid:48)
L(cid:48)

t−1

, where Lt−1, L(cid:48)

impact caused by client m as
t−1 represent the loss of the

global model with and without client m, respectively.
6) If the loss impact exceeds a threshold, conclude that client
m may be poisoned; remove it from the set of clients, and
revert the model to exclude its updates.

7) Communicate the observation with client m, to investigate

the incident.

DTrust can be extended to account for performance degra-
dation across multiple rounds of training. Instead of evaluating
the loss impact of client m’s last update (t − 1), the server
would evaluate the loss impact of m’s last k updates, which
have been stored in history. This approach increases DTrust’s
resilience to stealthy poisoning attacks that happen across
multiple training time steps.

IV. EVALUATION

Datasets. We evaluate our system on a number of data sources.
Each client organization participating in CELEST maintains a
labeled set of malicious and benign samples. We use the two
university networks’ trafﬁc as benign data, and use various
malicious data sources for different experiments as explained
below.

6

(A) University Network Dataset: We obtained access to
HTTP logs from two large university networks. The networks
contain an average of 20 and 9 million HTTP log events per
day, respectively, and were used actively during the period of
data collection. Sensitive ﬁelds in the logs (such as internal
IP addresses and URL parameter values) are anonymized in a
consistent manner to protect user personal information. We
performed all our analysis on servers within the university
network. The IRB ofﬁce at one of the universities reviewed our
data collection process and determined that this research does
not qualify as Human Subject Research. However, our team
members participated in IRB training and used best practices
to handle the network data. We ﬁlter this dataset based on
domain popularity using Tranco [61] to exclude top 10,000
domains. We further remove external domains contacted more
than 10,000 times. Both of these sets of domains are unlikely to
serve malware, and similar methodology was used in previous
work for data reduction [33], [57]. After ﬁltering the trafﬁc,
we have between 1,900 and 31,000 logs per 30-minute time
interval.

(B) Network IDS logs (NIDS) In one of the university
networks, a commercial off-the-shelf network IDS is available,
and alerts are collected in real-time for potentially malicious
events. We have access to the historical samples, and we
collect malicious domains and IP addresses from the malware-
callback alerts. We process 28,985 alerts, containing 9,124
unique requests and 687 unique hosts (domain and IP ad-
dresses). We use the collected indicators to match and label
connections in each network as malicious during the training
period.

(C) IoT Malware Dataset (Mirai, Gafgyt) We collect a
dataset of IoT malware (Mirai and Gafgyt) from CyberIoCs,
VirusTotal, VirusShare, and malicious samples shared by Al-
rawi et al. [4]. We build a dynamic analysis framework to
execute the malware in a sandboxed environment to collect
the HTTP logs for each malware family. In total, we use 300
malware samples from each family, containing 20,627 and
13,889 HTTP events.

(D) Data Exﬁltration Malware Dataset (DEM) Borto-
lameotti et al. [16] collected HTTP logs for data exﬁltration
malware samples in a sandbox and released the dataset. This
dataset contains multiple malware families: Shakti, FareIT,
CosmicDuke, Ursnif, Pony, Spyware, and SpyEye, and has
more variability. In total, 79 malware samples containing
8,843 HTTP events are used in the experiments. In several
experiments, we merge the trafﬁc of IoT and DEM malware
at training and testing time to create controlled experiments
with known ground truth of malicious activities.

(E) Attack Recreation Dataset (ARD) To conduct a realistic
experiment for evaluation, we set up an attack recreation
infrastructure and run controlled attacks on the two networks.
We vary the C2 infrastructure, including the C2 domains and
IP addresses, to emulate adversarial evasion capabilities and
test the performance of CELEST. During attack recreation, we
record malicious labels on individual HTTP logs. Therefore,
this setup offers ﬁne-grained ground truth, which enables us to
evaluate the performance of CELEST in a real environment.
The details of the exercises are given in Section V.

Implementation. We implemented the system in Python 3.8,

URL
Representation
Word2vec (federated)
FastText (federated)
Word2vec (centralized)
FastText (centralized)
Lexical Features

Precision

Recall

F1

PR-AUC

0.96
0.96
0.96
0.95
0.94

0.78
0.79
0.78
0.78
0.69

0.86
0.87
0.87
0.86
0.80

0.88
0.88
0.88
0.88
0.83

TABLE I: Feature representation comparison for detecting
malicious URLs. Our federated approach using Word2Vec
and FastText is similar to centralized embeddings, and they
outperform the baseline lexical features.

and used Keras [23] and TensorFlow Federated [74] for feder-
ated training. We use the Gensim framework [62] for training
embedding models. There are several hyper-parameters which
need to be selected before training. Federated learning updates
happen within rounds, and we set the duration of a round
to t = 30 minutes. Parameter t is tuned to balance the
trade-off between communication cost and convergence speed:
increasing the training time of each round results in smaller
communication cost overall, but slows down the convergence.
The neural network hyper-parameters are selected after a grid-
search: we use a single hidden layer of size 128, a dropout
layer with rate 0.1, and learning rate of 0.01.

Evaluation Metrics. We evaluate our system using Precision
and Recall metrics, as our dataset is also imbalanced. We
measure the performance with Precision-Recall Area Under
Curve (PR-AUC), a single metric that captures the performance
across all thresholds. We also use False Positive Rate (FPR)
as an important metric for the models to be deployed in a
real-world setting.

A. Feature Set Comparison

URL Representation. We evaluate our embedding-based ap-
proach for malicious URL detection in both centralized and
federated settings. We also evaluate a previous approach using
lexical features [46], [39],
in order to compare with the
embedding representation.

Embedding Model Training. We train both Word2Vec and
FastText models in centralized and federated settings. We
use the domain and URL ﬁelds of HTTP logs to evaluate
different representations. We train on one day containing
537,448 background samples, together with 18,853 malicious
samples from the three malware families (Mirai, Gafgyt, and
DEM). We test on the next day with 89,139 background
samples, and 4,267 malicious samples. The training corpus
consists of 7,060,904 URL instances, and a vocabulary of
995,059 unique tokens. The size of the embedding vectors
is 32, and we use Continuous-Bag-Of-Words, with a context
window of size 5. We selected these hyper-parameters after
experimenting with several choices. We compare the following
embedding methods: (1) Centralized Embeddings: The training
is performed at the server using the data shared by the clients,
and (2) Federated Embeddings: Each university network is a
client, which gets access to its own data and performs the
federated embedding protocol with the server.

Lexical Features. We use the code by Khramtsova et al. [39]
to generate 72 lexical features (e.g., length and entropy of each
URL category, number of letters in each category, etc.).

Feature Groups
All Features
Domain URL
UA Referer
External Host
HTTP Metadata

PR-AUC
0.95
0.88
0.53
0.63
0.52

Training Time
36m 55s
13m 23s
24m 6s
8m 4s
8m 1s

TABLE II: Feature group comparison for detecting malicious
HTTP events. We use four feature groups and compare to all
our extracted features using PR-AUC. All combined features
achieve highest PR-AUC at detecting malware.

Table I shows the comparison of different feature repre-
sentations in terms of precision, recall, F1 scores, and PR-
AUC. We make three observations: First, we note that FastText
provides similar performance to Word2Vec, and the training
time on our corpus is comparable. Since FastText provides
better generalization by handling out-of-dictionary words, we
choose FastText as the default URL embedding method in
our framework. Second, we note that the federated embedding
models provide similar performance to the centralized models,
and we used this approach due to its increased privacy as-
surances. Third, we see that the lexical feature representation
provides worse performance on our data, with 10% lower
recall and similar precision compared to the federated FastText
method. This result implies that the embedding method gener-
alizes better at detecting a larger number of malware samples,
which can evade the lexical features. The cost of training the
federated embedding is 75 minutes in our setup.

Feature Groups. We investigate here the need for additional
features extracted from HTTP logs to augment
the URL
embeddings. URLs are not always explicitly used in malicious
communication, and other HTTP ﬁelds, such as IP address,
referer and user agent string could also be attack indicators.
We compare several feature groups, as deﬁned below:

• Domain URL: Embeddings of domain and URL.
• UA Referer: User agent categorical features and referer

with embedding representation.

• External Host: External IP and port represented as cat-

egorical features.

• HTTP Metadata: Method, status, content type as cate-
gorical features, and trans depth, request length, response
length as numerical features.

• All Features: Full feature vector from Table IX.

For this experiment, we used the same dataset as for URL
detection, but with full HTTP logs. Table II shows the PR-
AUC for these feature groups. The results demonstrate that
the domain and URL features have the highest performance
in the 4 considered feature groups (PR-AUC of 0.88), but
using all HTTP features improves PR-AUC to 0.95. There is
an increase in the training time using all features, due to the
larger dimension of the neural network model.

B. Federated Models vs Local Models

In this section, we investigate how a network can ben-
eﬁt from federated training when a new malware has been
previously detected in one of the collaborating networks, but
is seen locally for the ﬁrst time. We design experiments to
use different malware families in the two networks during
training, and show how the global model helps detection of a

7

Training Malware

UNI-1

UNI-2

Test

Mirai
Mirai
Gafgyt
Gafgyt
DEM
DEM

Gafgyt
DEM
Mirai
DEM
Mirai
Gafgyt

Gafgyt
DEM
Mirai
DEM
Mirai
Gafgyt

Deployed Network: UNI-1
Local
PR-AUC
0.8751
0.2647
0.8952
0.2429
0.7231
0.6013

Global
PR-AUC
0.934
0.7522
0.9308
0.7548
0.8508
0.7983

Global FPR
at Recall 0.9
0.002
0.01
0.003
0.01
0.003
0.005

Deployed Network: UNI-2
Local
PR-AUC
0.8806
0.5483
0.9055
0.4464
0.5483
0.4736

Global
PR-AUC
0.9591
0.8774
0.9568
0.8016
0.7958
0.7939

Global FPR
at Recall 0.9
0.002
0.002
0.001
0.002
0.009
0.009

Test

Mirai
Mirai
Gafgyt
Gafgyt
DEM
DEM

TABLE III: Federated model comparison with the locally trained models for detecting malware families in multiple scenarios
with different training sets. Each row shows the training malware family in each network, and the PR-AUC for detecting the
malware that is not locally seen during training. The results show the PR-AUC improvements of the global models compared to
locally trained models and the low FPR of the global models.

malware family seen for the ﬁrst time in one of the networks.
In particular, we generate a set of controlled experiments with
the two university networks as clients. We merge malware
traces from the three families (Mirai, Gafgyt, and DEM) in
both networks to determine the performance of the local and
global models under multiple scenarios. We split each of the
malware family into 80% samples for training and 20% for
testing, and distribute them to the networks across rounds.

Table III shows the PR-AUC of the global models trained
with federated learning and the local model in various settings.
The results show the improvements achieved by the global
model, which successfully transfers the detection from one
network to other participating clients. For example, when Mirai
and DEM malware families are used for training at UNI-1 and
UNI-2, respectively, the federated model achieves a PR-AUC
of 0.75 at detecting DEM in UNI-1, a signiﬁcant improvement
over the local model’s 0.26 PR-AUC. Similarly, at UNI-2, the
global model achieves a PR-AUC of 0.87, compared to the
local model’s PR-AUC of 0.54 at detecting Mirai. In other
cases, malware families share some common characteristics,
which aids the local detection. In the case of Mirai and
Gafgyt, a local model that trains on Mirai and later attempts
to detect Gafgyt (or the other way around) performs relatively
well (at 0.87-0.9 PR-AUC). Nonetheless, the global model
outperforms it consistently, achieving a PR-AUC of about 0.93-
0.95. False positive rates are consistently low for the global
models, between 0.002 and 0.01, at a recall of 0.9.

Figure 3 shows the progress during federated training for
two of the experiments. As more training data is consumed,
the PR-AUC increases and the global model outperforms the
local model in less than 10 iterations (5 hours), illustrating a
clear performance gain with the federated model.

C. Scaling to Multiple Clients

CELEST is designed for multiple clients, but in cyber
settings it is challenging to obtain data from multiple orga-
nizations. We have access to two large university network
logs, and we use subnet-level information in each network
to create multiple clients. For this experiment, we generate all
the subnets at each university network, and create 15 clients in
each network, each using data from several subnets. We then
consider a subset of these clients, varied between 2 and 15
at each network, to test the impact of the number of clients
participating in the FL training protocol. We split the malware
samples from the three families equally across the 30 clients
in both networks.

Fig. 3: Federated model progress for detecting malware fami-
lies over multiple iterations evaluated on the two networks. The
two sites are seeing a new malware for the ﬁrst time (DEM at
UNI-1 and Gafgyt at UNI-2) and attempt to detect it with their
own local model and with the federated model. Each iteration
processes 30 minutes of data, and the global model exceeds
the local model performance in less than 10 iterations.

Table IV shows the global model PR-AUC at detecting
the three malware families on the two university networks.
We observe that for Mirai, four clients is enough to obtain
good performance (PR-AUC above 0.94), whereas for DEM
and Gafgyt families the performance improves as more clients
participate in the protocol and more malicious data is available.
The improvements are particularly large for DEM, which
consists of multiple malware families and has more variability.
The experiment shows that the system can scale to multiple
clients and its detection performance improves with more
clients participating in the FL protocol. Small variations in
performance are due to differences in data distributions of
clients joining the training process.

D. Active Federated Learning

We showed how federated learning can transfer knowledge
about malware observed in some networks to detect attacks
seen for the ﬁrst time in other networks. However, the global
model will not detect a completely new attack that has not been

8

01020304050Iterations0.00.20.40.60.8Precision-Recall AUCUNI-1 - Local: Gafgyt - Tested: DEMGlobal Model ProgressLocal Model AUC : 0.2401020304050Iterations0.00.20.40.60.8Precision-Recall AUCUNI-2 - Local: DEM - Tested: GafgytGlobal Model ProgressLocal Model AUC : 0.44Global
Model
4 Clients
10 Clients
16 Clients
20 Clients
24 Clients
30 Clients

Network: UNI-1

Network: UNI-2

Mirai

Gafgyt

DEM Mirai

Gafgyt

DEM

0.96
0.96
0.96
0.95
0.96
0.95

0.90
0.93
0.94
0.90
0.95
0.89

0.71
0.72
0.74
0.70
0.74
0.81

0.94
0.95
0.95
0.95
0.95
0.94

0.88
0.91
0.92
0.93
0.94
0.91

0.59
0.64
0.69
0.64
0.66
0.70

TABLE IV: Federated model PR-AUC as more clients par-
ticipate during training. Each row shows the total number
of clients, and the PR-AUC for detecting different malware
families in the two networks. As the number of clients in-
creases and more data is used during training, overall detection
performance generally improves.

Active
Learning
Budget
0
10
100
200
300
400
500
Fully Labeled

Network: UNI-1

Network: UNI-2

Mirai

Gafgyt

DEM Mirai

Gafgyt

DEM

0.19
0.48
0.70
0.79
0.83
0.84
0.87
0.97

0.13
0.38
0.60
0.73
0.76
0.77
0.81
0.96

0.21
0.59
0.66
0.69
0.66
0.68
0.70
0.82

0.18
0.40
0.63
0.76
0.81
0.83
0.84
0.96

0.06
0.28
0.49
0.69
0.73
0.75
0.76
0.93

0.22
0.38
0.52
0.59
0.61
0.68
0.69
0.72

TABLE V: Active federated learning at different budgets
from 0 to 500 in a challenging scenario in which none of
the malware families is labeled in training. We show PR-
AUC for detecting each malware family at each university
networks. The federated model without active learning (budget
0) cannot identify new malware (top row). The active learning
component achieves higher PR-AUC at larger budgets, and
gets close to the model trained with all labeled malicious data
(last row).

observed in either of the participating clients in training. To
overcome this challenge, we propose extending the federated
learning framework with an active learning component that
integrates a local anomaly detection module (Section III-B).

We design a challenging scenario to test

the detection
capabilities of active federated learning. The two university
networks start training the global model using NIDS alerts
known to be malicious, but without using the malicious labels
to simulate a situation when clients are infected with unknown
malware. Thus, we merge the unlabeled malware logs from
the three malware families (Mirai, Gafgyt, and DEM) into the
client networks. We would like to test if the anomaly detection
module identiﬁes some of the malicious samples in the local
networks, and, furthermore, if the federated model with active
learning is able to detect these unknown malware attacks when
no labeled data from these families is initially available in
training.

For the anomaly detection module, we train Isolation Forest
models on HTTP logs, on k previous time windows of 30
minutes each, and use an ensemble of these models to detect
anomalies within the current time window. We set k = 3, after
experimenting with multiple values of k. We vary the budget
used in investigation between 0 (which results in the federated

Fig. 4: Poisoning Attacks: Global model performance progress
with various attack strategies, and no defense. We compare
the label ﬂipping attack with
the ‘no poisoning’ baseline,
m malicious clients, and a stronger ‘weight boosting’ attack.
Poisoning starts at iteration 20. Increasing the number of helper
clients h improves detection: h = 1 (left) and h = 5 (right).

Fig. 5: Defenses: Global model performance progress when
various defenses have been employed against the strongest
attack from Figure 4 (weight boosting). DTrust successfully
recovers from the poisoning attack, outperforming the ‘no
defense’ baseline and the ‘clipping’ defense signiﬁcantly.

model without active learning) and 500.

Table V shows the PR-AUC of detecting each of the three
malware families as a function of the investigation budget.
As expected, for a budget of 0, the global model performs
poorly without active learning at detecting these completely
new malware families (e.g., PR-AUC 0.19 at detecting Mirai at
UNI-1). Active learning used in federated training signiﬁcantly
helps detect new malware: as we increase the budget for
investigation, the PR-AUC also increases, and with a budget
of 500 samples it reaches PR-AUC of 0.87 in the same
experiment for detecting Mirai.

We also trained the active learning system with sample
selection using only the anomaly detection module (without the
classiﬁer). This setup identiﬁed fewer samples per round (since
it did not incorporate any samples selected by the classiﬁer),
and the detection performance was slightly worse. For instance,
using both anomaly detection and the highly ranked samples
generated by the classiﬁer resulted in an increase of 11% PR-
AUC compared to using only the anomaly detection module
for detecting DEM. This result shows that both components for
sample selection contribute to the success of active federated
learning.

E. Resiliency Against Poisoning

In this section, we evaluate CELEST’s resiliency against
poisoning attacks, where a number of malicious clients try
to impact the global model to evade detection of a speciﬁc

9

malware pattern. In our threat model, clients belong to one of
the following three categories: (1) poisoning clients that try
to inject a speciﬁc malicious pattern; (2) helper clients, which
have observed this speciﬁc malicious behavior before; their
dataset contains correctly labeled data related to the malicious
patterns; (3) other benign clients that have not seen the speciﬁc
poisoning activity before. The helper clients are instrumental in
sharing their knowledge through the global model to increase
resilience against adversarial actions.

In these experiments, we consider a total of c = 30
clients, and vary the number m of malicious clients, as well
as the number h of helper clients. We run the training for
48 iterations (one day of data, with 30-min long iterations),
and start the poisoning activity at the 20th iteration. We study
three poisoning scenarios with speciﬁc HTTP attacks: (1) Data
exﬁltration activity to the C2 server from CosmicDuke (part
of our DEM dataset) [3]; (2) ThinkPHP exploit observed in
Mirai [2]; and (3) a home-router attack with device update
behavior from Gafgyt [1].

In Figure 4, we show the impact of poisoning during the
data exﬁltration attack, when no defense is employed. We
compare the ‘no poisoning’ baseline with: (1) a label ﬂipping
attack, where a number of malicious clients m provide corrupt
updates trained on malicious data labeled as benign; (2) a
stronger poisoning attack using weight boosting [10], which
directly scales up the weight updates sent by the poisoning
clients to the server. We make the following observations: First,
we note that the success of the label ﬂipping attack increases
with more poisoning clients m, as expected. Second, the global
model is more resilient against poisoning attacks when more
helper clients are contributing with locally seen attack patterns.
With only one helper, a single malicious client is sufﬁcient to
decrease model accuracy over time substantially (left ﬁgure,
with boosting); however, when ﬁve helpers participate in the
global model, the attacker needs to compromise as many as
ﬁve clients for an equivalent performance degradation (right
ﬁgure, with boosting). Third, we note that weight boosting
attacks are more effective than label ﬂipping attacks, reaching
PR-AUC of 0.11 in both experimental settings (i.e., left and
right ﬁgures) in only 10 and 6 iterations (after poisoning had
started), respectively.

the
In Figure 5, we explore possible defenses against
weight boosting attack presented before. We compare the ‘no
defense’ baseline with: (1) the weight clipping technique [72];
(2) DTrust algorithm; and (3) a hybrid defense consisting of
DTrust + clipping. We used clean training updates to determine
the clipping bound parameter as 0.1, after observing that lower
values hinder the main task accuracy signiﬁcantly. The plot
on the right (m = 5, h = 5) demonstrates that the clipping
technique alone is not sufﬁcient to mitigate the impact of
poisoning, as the PR-AUC still drops to 0.11 eventually during
this strong attack. Our proposed defense, DTrust, starts to
investigate the client updates after the global model PR-AUC
deteriorates by more than 10%. DTrust identiﬁes the malicious
clients and removes their updates from the global model. The
training continues with the benign clients, and the performance
recovers over time. Eventually, DTrust reaches 0.93 PR-AUC,
similar to the ‘no poisoning’ case. DTrust can be used as a
standalone defense, as well as in combination with clipping
(giving similar results). CELEST is also effective in other

Training
Family
Mirai
DEM
DEM

Poisoning
Behavior
Data Exﬁltration
HomeRouter Attack
ThinkPHP Exploit

No
Defense
0.11
0.08
0.06

DTrust
Defense
0.93
0.89
0.65

Clean
Training
0.93
0.92
0.70

TABLE VI: Overview of the poisoning experiments for three
scenarios after one day of training in a 30-client setting. The
training family is the main task that all clients train on. We run
the weight boosting attack with m = 5 malicious clients, and
h = 5 helpers. Our DTrust defense reaches PR-AUC scores
close to the clean model by detecting the poisoning activity in
less than 4 iterations in all three cases.

Fig. 6: PR-AUC of the global model trained at various time
intervals. Training data includes labeled attack recreation sam-
ples used at UNI-1, but not at UNI-2, in Exp 1. Testing is
done on Exp 2 and Exp 3 at UNI-2. Federated model reaches
almost perfect PR-AUC in Exp 2, after the model is trained
with sufﬁcient data, due to the similarity of Exp 1 and Exp
2. In Exp 3, the PR-AUC is lower and the model needs more
training data, due to adversarial evasion.

poisoning scenarios with different HTTP attacks. Table VI
summarizes the poisoning impact for the three attacks after
one day of training, and presents the mitigation results. DTrust
detects the poisoning activity in less than 4 iterations in each
case. Furthermore, DTrust enables poisoned models to recover
and reach close to the accuracy of the models trained on clean
data.

V. MODEL DEPLOYMENT

In this section, we discuss results of our system deployment
on two university networks to demonstrate its performance and
feasibility in a real-world setting. We design several attack
recreation exercises to run real malware on the two networks
in a controlled manner, and then evaluate the detection capa-
bilities of CELEST.

Attack Recreation Exercises. We deploy a set of vulnerable
virtual machines to recreate a malware infection and propa-
gation on the two networks. We use a modiﬁed instance of
Mirai [5], which scans and propagates to other vulnerable
virtual machines under our control. To ensure there were
no unintended side effects or damage to other nodes on
the internet, the Mirai malware was modiﬁed by updating
the dictionary used to brute-force passwords to include only
randomly selected user names and passwords. Mirai’s infection
component was enhanced to verify that the IP to infect was on
the controlled IP list before malware propagates and infects the

10

Training
Time (Days)
3
6
10

Detection on UNI-2
Exp #2
0.1601
0.8592
1.0

Exp #1
0.2449
0.8258
0.9272

Exp #3
0.0006
0.4025
0.6746

TABLE VII: PR-AUC at UNI-2 during the 3 attack recreation
experiments for the global model trained with 3, 6, and 10 days
of Exp 1 data. Training data includes labeled attack recreation
samples used at UNI-1, but not at UNI-2. The model trained
after 3 days has relative low PR-AUC, but this increases as
more training data is used. Results on Exp 1 and Exp 2 have
higher PR-AUC than Exp 3, which employs evasion.

Top-10
Top-30
Top-50
Top-80
Top-100

Top-10
Top-30
Top-50
Top-80
Top-100

ARD
8
28
39
46
59

9
28
31
42
42

IDS
2
2
7
19
19

1
2
12
14
14

VT
0
0
2
10
12

Prec
1.0
1.0
0.96
0.93
0.90

Deployed Network: UNI-1
Unk
0
0
2
5
10
Deployed Network: UNI-2
0
0
6
9
14

1.0
1.0
0.98
0.88
0.86

0
0
1
16
30

FPR
0
0
2.7 × 10−6
6.9 × 10−6
1.3 × 10−5

0
0
1.4 × 10−5
2.1 × 10−5
3.3 × 10−5

TABLE VIII: Detection statistics for global model deployment
in the two networks on a test day part of Exp 1 (June 20). The
majority of detected samples are attack recreation (ARD) logs
(59 out of 100 at UNI-1), and several samples are conﬁrmed
malicious by IDS (19 at UNI-1 and 14 at UNI-2). The system
detected 42 new malicious samples with unique URLs (a
total of 20 malicious domains), conﬁrmed by VirusTotal (VT),
which are not part of ARD and not detected by IDS. The
model detects all ARD samples with high precision and low
false positive rates in both networks. (Unk = Unknown)

target host. We also host a C2 server on the Chameleon [38]
service outside the university networks. This deployment al-
lows the network data to be collected at the edge of the
university networks and we observe the C2 communication in
the HTTP logs. We further enhanced Mirai to deploy a reverse
shell component, called Sandcat, from Mitre Caldera [26], to
maintain a permanent C2 connection with beaconing between
700 and 900 seconds on port 2323.

We start an experiment by infecting a few of the machines
with the Mirai malware. The lateral movement process scans
public IPs across the two universities, and takes approximately
two weeks to infect approximately half of the vulnerable nodes.
We run three exercises with different conﬁgurations, each over
a two-week interval:

• Exp 1: A total of 282 end points were deployed, and the
C2 server IP was conﬁgured to change every 30 minutes.
The new server IP was detected by the infected end points
by querying a DNS server for a single domain name. This
experiment occurred in June of 2021.

• Exp 2: Similar to Exp 1, but with 429 end points
deployed. This experiment occurred in August of 2021.
• Exp 3: Both the C2 server IP and domain change
every 30 hours. The new domain name is found by

11

using a domain generation algorithm (DGA) conﬁgured
to psuedo-randomly select 100 domains from 10,000
possible domain names. This experiment used 364 end
points, and occurred in October of 2021.

Exp 2 is designed to be similar to Exp 1, but is run
after two months to test if the global model maintains its
detection performance over time. Exp 3 is designed to emulate
an adversarial evasion strategy, in which the adversary rotates
both its C2 domain and IP infrastructure to evade detection.

Deployment and Evaluation. We deploy the federated learn-
ing framework in the two networks, and perform training over
ten days in June 2021, starting two days before Exp 1 began
to account for some intervals without any attack recreation
samples. Commercial off-the-shelf network IDS (NIDS) logs
which are commonly available as part of the organization’s
security infrastructure are used to label trafﬁc in both net-
works during training. In this scenario, we have two client
organizations, and one of the networks (UNI-1) has resources
to detect and label the attack recreation data as malicious. The
second network (UNI-2) has no attack recreation labels during
training. These two networks participate in federated training,
and try to detect the various attack recreation experiments.

We are interested in measuring global model detection at
UNI-2, for which the local model has 0 PR-AUC (as there is no
labeled attack recreation data). Table VII shows the PR-AUC
results at UNI-2 for three models trained on Exp 1, using 3, 6,
and 10 days of training data, respectively. Figure 6 shows the
increase in PR-AUC on Exp 2 and Exp 3 testing days (August
15 and October 15) for the same model trained on Exp 1 data,
at various time intervals. Initially, there is no infected host at
UNI-1 (we start the training 2 days before Exp 1), so the model
has no malicious detections until the third day. In the third day,
the model has relatively low PR-AUC (0.24 on Exp 1 testing),
but the performance increases steadily with more training data
(and reaches a perfect PR-AUC on Exp 2 after 10 days).
Interestingly, the global model trained on Exp 1 generalizes
very well on Exp 2, which occurs two months later. This
outcome is explained by the similarity in the attack recreation
data in the two experiments, which has a higher relevance than
the changes in the benign data distribution over time. Results
on Exp 3 are surprisingly good (PR-AUC of 0.67), considering
the evasion performed by malware, which changes both the
C2 server domain and IP address. These results suggest that,
while IP and domain features are important, CELEST relies
on a larger number of features extracted from HTTP logs and
provides some resilience to evasion.

We further analyze the highest ranked HTTP logs on the
test day, June 20, in Exp 1 to determine what the model
considers suspicious in addition to attack recreation events.
Table VIII includes the results of our investigation of top
ranked samples in both networks. We observe that most of
the detections are attack recreation samples (28 in top 30, and
46 in top 80 samples), and a large number (19 on UNI-1 and
14 on UNI-2) are veriﬁed as malicious by the commercial IDS.
We query the remaining samples on VirusTotal and determine
12 samples at UNI-1 and 30 at UNI-2 with positive scores
(a total of 42 malicious URLs and 20 malicious domains).
These detections are interesting as they are new malicious
detections identiﬁed by CELEST, which were not part of the

attack recreation exercises and the IDS failed to detect. The
rest (10 URLs at UNI-1 and 14 at UNI-2) have an unknown
status at the time of the investigation. We report high precision
and FPR lower than 3.3 × 10−5 in both networks, considering
the unknown samples as false positives.

in defense. We note that federated learning with privacy-
preserving aggregation does not allow inspection of individual
updates from the clients [15]. Our defense can be modiﬁed
to simply revert to a previous model, but identiﬁcation of
malicious clients would not be possible in this case.

We also deployed our model for real-time detection during
attack recreation experiments. We ran a network monitoring
service that consumes the HTTP log streams from the live
network trafﬁc. Features are generated in real-time from raw
log samples, and evaluated on the global model to generate
a score. For high-certainty detections with scores above 0.85,
we trigger a ﬁrewall rule to prevent future communications
with the detected C2 server. Our model was able to detect
the C2 malicious communications, and deployed the ﬁrewall
successfully to block the attack in progression.

VI. DISCUSSION AND EXTENSIONS

In this paper we showed that global models that leverage
coordinated detection across multiple participating organiza-
tions perform better at detecting malware threats compared
to local models that only rely on limited local data. Feder-
ated learning is often able to expose new attacks which are
largely invisible to individual organizations. FL is particularly
effective when used in conjunction with active learning, which
progressively enhances the labeled dataset used in training
with new malicious samples. When deployed on two university
networks, CELEST was able to detect new malicious activity
and identify evasive adversarial activity in real time.

Real-world Deployment. Active learning adds a major boost
to the malware detection capabilities of a federated learning
framework. Through the anomaly detection module integrated
with the classiﬁer that progressively improves its detection,
CELEST can discover and learn the behavior of completely
new attacks for which no labeled data is available. Our current
framework uses a generic anomaly detector (Isolation Forest);
however, in order to tap into its full potential, a more sophis-
ticated anomaly detector speciﬁcally designed for HTTP logs
can be used. We showed in the evaluation that active learning
can be effective even at small budgets of samples investigated
and labeled by security analysts. We note that periodic training
is necessary in order to learn changing patterns in background
trafﬁc, as well as to discover and incorporate new attack
behaviors in the model. While CELEST is currently designed
to detect malware communication from HTTP logs, incorpo-
rating other types of security logs (e.g., authentication logs,
DNS logs, ﬁrewalls logs) would be beneﬁcial. CELEST would
have less information if the malware trafﬁc is encrypted over
HTTPS, but we have shown that it still performs well when
only a subset of features are available (see Appendix IV-A).
An interesting extension is to support P2P architectures, where
a group of autonomous peers jointly train a common model
without using a trusted server. A decentralized FL approach
avoids having a single point-of-failure and several designs have
recently been proposed [24], [32], [80].

Resilience to Poisoning. Other studies have proposed to use
a trust dataset [22] which resides on the server and is used
to remove outlier updates or correct the model. In contrast,
we employ a distributed trust bootstrapping approach where
each client has its own trust dataset and actively participates

Resilience to Evasion. In Section V we presented a case study
(Exp 3) designed to emulate an adversarial evasion strategy,
in which the adversary rotates both its C2 domain and IP
infrastructure to evade detection. We showed that CELEST
is still able to detect the malware attack when faced with this
evasion method. CELEST’s resilience can be attributed to two
main factors: the large number of features used for training the
federated model, and the participation of multiple clients. Still,
a motivated attacker might be able to coordinate its behavior
and generate trafﬁc that looks normal in all participating clients
to evade CELEST’s detection. We believe evasion is more
difﬁcult in a federated setting than in a local system trained
within a single organization.

VII. RELATED WORK

Malicious URL and domain detection. A large body of
work has looked at URL-based detection [21], [40], [45], [46],
[65], [83], [75] and domain-based detection [11], [20], [27],
[42], [85] of malicious trafﬁc. Some of these studies focus
on one type of malware like phishing [83] or spam [21],
[75] in social media contexts, while others attempt to capture
a larger set of malware behaviors, e.g, Mamun et al. [46]
group malicious URLs into categories like spam, phishing,
malware, and defacement URLs using public datasets. Previous
studies employ various techniques to detect malicious URL and
domains. These include lexical features generated from bag-of-
words representation [45], behavioral analysis [21], DNS data
analysis [11], [85], webpage content analysis combined with
some URL and domain properties [20], [75], and deep-learning
methods [40], [42], [46], [65], [83].

Malicious HTTP-based detection. Efforts on malware detec-
tion in this area have focused on HTTP-based detection using
web-proxy logs [8], [33], [48], [53], [57], [58], [71] and HTTP-
based application ﬁngerprinting [16], [17], [60]. Machine
learning detection methods on HTTP logs attempt to detect
malware activity within a single network by training supervised
classiﬁcation models locally on labeled data available in that
network [8], [33], [57], [58]. In other approaches, Nelms et
al. [53] use control protocol templates derived from labeled
samples to detect new C2 domain names, while Bortolameotti
et al. [16] use application ﬁngerprinting techniques for clus-
tering HTTP connections in order to detect anomalous trafﬁc.

Federated learning for cyber security. Federated learning
has been proposed for enhancing cyber security in various
settings including mobile phones [31], Internet-Of-Things [55],
[66], cloud ecosystems [59]. Closer to our work, Khramtsova
et al. [39] study federated learning approaches on malicious
URL detection to show the beneﬁt of sharing information about
local detections. Zhao et al. [84] propose multi-task network
anomaly detection using federated learning. Fereidooni et
al. [29] proposed federated learning to enable effective cyber-
risk intelligence sharing for mobile devices. Several studies
have looked at poisoning attacks against federated learning [7],
[72], [76], [28], [81], [68], [79]. Fang et al. [28] proposed a

12

general framework of local model poisoning attacks, which can
be applied to optimize the attacks for any given aggregation
rule. Bagdasaryan et al. [7] has formulated adversarial poison-
ing as a two-task optimization problem that has high accuracy
on both the main and the backdoor tasks (constrain-and-scale
method). Previously proposed defenses against poisoning in
FL perform anomaly detection on client updates [12], [50],
[82], are speciﬁc to backdoor attacks [63], [56], or leverage a
trusted dataset at the server [22].

VIII. CONCLUSION

In this paper, we present CELEST, a federated learning
framework for collaborative threat detection. CELEST lever-
ages a distributed machine learning architecture in which
multiple participating organizations train a global model used
for HTTP-based malware detection. Using a novel active learn-
ing component, CELEST progressively improves its detection
capabilities. In addition, we propose DTrust, a new resilient
algorithm aimed at defending against data and model poisoning
attacks in distributed settings. We evaluate our system using
a variety of malware datasets and demonstrate the power of
knowledge transfer through the globally trained model, which
enables individual organizations to detect attacks that were
largely invisible locally. We deploy the model on two large
university networks and show that CELEST is able to detect
real-world malicious trafﬁc (42 malicious URLs and 20 mali-
cious domains). Overall, CELEST is a scalable and effective
proactive threat detection solution that leverages collaboration
across multiple networks to detect emerging cyber threats.

ACKNOWLEDGEMENTS

We thank Afsah Anwar, Alastair Nottingham, Molly
Buchanan, Mark Gardner, Jeffry Lang, and Jeffrey Collyer for
their help throughout the project. This research was sponsored
by contract number W911NF-18-C0019 with the U.S. Army
Contracting Command - Aberdeen Proving Ground (ACC-
APG) and the Defense Advanced Research Projects Agency
(DARPA), W911NF-21-10322, and by the U.S. Army Combat
Capabilities Development Command Army Research Labora-
tory under Cooperative Agreement Number W911NF-13-2-
0045 (ARL Cyber Security CRA). The views contained in
this document are those of the authors and should not be in-
terpreted as representing the ofﬁcial policies, either expressed
or implied, of the ACC-APG, DARPA, Combat Capabilities
Development Command Army Research Laboratory or the
is authorized to
U.S. Government. The U.S. Government
reproduce and distribute reprints for Government purposes
notwithstanding any copyright notation here on. This project
was also funded by NSF under grant CNS-1717634.

REFERENCES

[1]

[2]

[3]

“New wave of attacks attempting to exploit Huawei home routers,”
https://securitynews.sonicwall.com/xmlpost/new-wave-of-attacks-
attempting-to-exploit-huawei-home-routers/, July 2019.
“ThinkPHP Vulnerability Abused by Botnets,” https://www.trendmicro.
com/en us/research/19/a/thinkphp-vulnerability-abused-by-botnets-
hakai-and-yowai.html, Dec 2019.
“CosmicDuke Malware Analysis,” https://www.cyﬁrma.com/outofband/
cosmicduke-malware-analysis/, Aug 2022.

[4] O. Alrawi, C. Lever, K. Valakuzhy, R. Court, K. Snow, F. Monrose,
and M. Antonakakis, “The circle of life: A large-scale study of the IoT
malware lifecycle,” in USENIX Security, 2021.

[5] M. Antonakakis, T. April, M. Bailey, M. Bernhard, E. Bursztein,
J. Cochran, Z. Durumeric, J. A. Halderman, L. Invernizzi, M. Kallitsis
et al., “Understanding the Mirai botnet,” in USENIX Security, 2017.

[6] Azeria-Labs, “Data Exﬁltration Techniques,” https://azeria-labs.com/

data-exﬁltration, [accessed May 2022].

[7] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How
to backdoor federated learning,” in AISTATS, 2020, pp. 2938–2948.
[8] K. Bartos, M. Sofka, and V. Franc, “Optimized invariant representation
of network trafﬁc for detecting unseen malware variants,” in USENIX
Security, 2016, pp. 807–822.

[9] D. G. Bernal, L. Giaretta, S. Girdzijauskas, and M. Sahlgren, “Federated
word2vec: Leveraging federated learning to encourage collaborative
representation learning,” arXiv, 2021.

[10] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, “Analyzing feder-
ated learning through an adversarial lens,” in International Conference
on Machine Learning. PMLR, 2019, pp. 634–643.

[11] L. Bilge, S. Sen, D. Balzarotti, E. Kirda, and C. Kruegel, “Exposure: A
passive DNS analysis service to detect and report malicious domains,”
TISSEC, vol. 16, no. 4, pp. 1–28, 2014.

[12] P. Blanchard, E. M. E. Mhamdi, R. Guerraoui, and J. Stainer, “Machine
Learning with Adversaries: Byzantine Tolerant Gradient Descent,” in
NeurIPS, 2017.

[13] A. Blum, B. Wardman, T. Solorio, and G. Warner, “Lexical feature
based phishing url detection using online learning,” in AISec, 2010, pp.
54–60.

[14] P. Bojanowski,

E. Grave, A.

T. Mikolov,
“Enriching word vectors with subword information,” 2016, cite
arxiv:1607.04606Comment: Accepted to TACL. The two ﬁrst authors
contributed equally. [Online]. Available: http://arxiv.org/abs/1607.04606

Joulin,

and

[15] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan,
S. Patel, D. Ramage, A. Segal, and K. Seth, “Practical secure aggrega-
tion for privacy-preserving machine learning,” in CCS, 2017, pp. 1175–
1191.

[16] R. Bortolameotti, T. van Ede, M. Caselli, M. H. Everts, P. Hartel,
R. Hofstede, W. Jonker, and A. Peter, “Decanter: Detection of anoma-
lous outbound http trafﬁc by passive application ﬁngerprinting,” in
ACSAC, 2017, pp. 373–386.

[17] R. Bortolameotti, T. Van Ede, A. Continella, T. Hupperich, M. H.
Everts, R. Rafati, W. Jonker, P. Hartel, and A. Peter, “Headprint:
detecting anomalous communications through header-based application
ﬁngerprinting,” in ACM SAC, 2020, pp. 1696–1705.

[18] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander, “LOF: iden-
tifying density-based local outliers,” in SIGMOD, vol. 29, 2000, pp.
93–104.

[19] R. J. G. B. Campello, D. Moulavi, A. Zimek, and J. Sander, “Hierar-
chical Density Estimates for Data Clustering, Visualization, and Outlier
Detection,” ACM Trans. Knowl. Discov. Data, vol. 10, no. 1, 2015.

[20] D. Canali, M. Cova, G. Vigna, and C. Kruegel, “Prophiler: A fast ﬁlter
for the large-scale detection of malicious web pages,” in WWW, 2011,
pp. 197–206.

[21] C. Cao and J. Caverlee, “Detecting spam URLs in social media via

behavioral analysis,” in ECIR, 2015, pp. 703–714.

[22] X. Cao, M. Fang, J. Liu, and N. Z. Gong, “FLTrust: Byzantine-robust
federated learning via trust bootstrapping,” in NDSS, 2021. [On-
line]. Available: https://www.ndss-symposium.org/ndss-paper/ﬂtrust-
byzantine-robust-federated-learning-via-trust-bootstrapping/

[23] F. Chollet et al., “Keras: Deep learning for humans,” https://github.com/

fchollet/keras, [accessed Oct 2022].

[24] L. Chou, Z. Liu, Z. Wang, and A. Shrivastava, “Efﬁcient and less
centralized federated learning,” ArXiv, vol. abs/2106.06627, 2021.
[25] D. Cohn, Z. Ghahramani, and M. Jordan, “Active learning with sta-
tistical models,” Journal of Artiﬁcial Intelligence Research, vol. 4, pp.
129–145, 1996.

[26] T. M. Corporation, “Sandcat agent plugin,” 2022. [Online]. Available:

https://github.com/mitre/sandcat

13

[27] R. De Silva, M. Nabeel, C. Elvitigala, I. Khalil, T. Yu, and C. Keppi-
tiyagama, “Compromised or attacker-owned: A large scale classiﬁcation
and study of hosting domains of malicious URLs,” in 30th USENIX
Security Symposium, 2021.

[51] T. Micro,

“New Mirai Variant Uses Multiple

Exploits,”

https://www.trendmicro.com/en us/research/19/e/new-mirai-variant-
uses-multiple-exploits-to-target-routers-and-other-devices.html, 2019,
[accessed May 2022].

[28] M. Fang, X. Cao, J. Jia, and N. Gong, “Local model poisoning attacks
to Byzantine-Robust federated learning,” in USENIX Security, 2020, pp.
1605–1622.

[29] H. Fereidooni, A. Dmitrienko, P. Rieger, M. Miettinen, A.-R. Sadeghi,
and F. Madlener, “FedCRI: Federated Mobile Cyber-Risk Intelligence,”
in NDSS, 2022.

[30] B. Frenay and M. Verleysen, “Classiﬁcation in the presence of label
noise: A survey,” IEEE Transactions on Neural Networks and Learning
Systems, vol. 25, no. 5, pp. 845–869, 2014.

[31] R. G´alvez, V. Moonsamy, and C. Diaz, “Less is more: A privacy-
respecting android malware classiﬁer using federated learning,” Pro-
ceedings on Privacy Enhancing Technologies, vol. 4, pp. 96–116, 2021.
[32] A. Gholami, N. Torkzaban, and J. S. Baras, “Trusted decentralized

federated learning,” in IEEE CCNC, 2022, pp. 1–6.

[33] X. Hu, J. Jang, M. P. Stoecklin, T. Wang, D. L. Schales, D. Kirat,
and J. R. Rao, “BAYWATCH: Robust beaconing detection to identify
infected hosts in large-scale enterprise networks,” in DSN.
IEEE, 2016.
[34] L. Invernizzi, S. ju Lee, S. Miskovic, M. Mellia, R. Torres, C. Kruegel,
S. Saha, and G. Vigna, “Nazca: Detecting malware distribution in large-
scale networks,” in NDSS, 2014.

[35] C. Johnson, L. Badger, D. Waltermire, J. Snyder, C. Skorupka et al.,
“Guide to cyber threat information sharing,” NIST, vol. 800, no. 150,
2016.
J. M. Johnson and T. M. Khoshgoftaar, “A survey on classifying big
data with label noise,” JDIQ, 2022.

[36]

[37] P. Kairouz et al., “Advances and Open Problems in Federated Learning,”
Foundations and Trends in Machine Learning, vol. 14, no. 1–2, 2021.
[38] K. Keahey, J. Anderson, Z. Zhen, P. Riteau, P. Ruth, D. Stanzione,
M. Cevik, J. Colleran, H. S. Gunawi, C. Hammock, J. Mambretti,
A. Barnes, F. Halbach, A. Rocha, and J. Stubbs, “Lessons learned from
the chameleon testbed,” in USENIX ATC, 2020.

[39] E. Khramtsova, C. Hammerschmidt, S. Lagraa, and R. State, “Federated
learning for cyber security: SOC collaboration for malicious URL
detection,” in ICDCS.

IEEE, 2020, pp. 1316–1321.

[40] H. Le, Q. Pham, D. Sahoo, and S. C. Hoi, “URLNet: Learning a URL
representation with deep learning for malicious URL detection,” arXiv,
2018.

[41] D. D. Lewis and W. A. Gale, “A sequential algorithm for training text

classiﬁers,” in SIGIR, 1994, pp. 3–12.

[42] Y. Li, K. Xiong, T. Chin, and C. Hu, “A machine learning framework for
domain generation algorithm-based malware detection,” IEEE Access,
vol. 7, pp. 32 765–32 782, 2019.

[43] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation forest,” in 8th IEEE

Int’l. Conf. on Data Mining.

IEEE, 2008, pp. 413–422.

[44] E. Lughofer, “Single-pass active learning with conﬂict and ignorance,”

[45]

Evolving Systems, vol. 3, no. 4, pp. 251–271, 2012.
J. Ma, L. K. Saul, S. Savage, and G. M. Voelker, “Beyond blacklists:
learning to detect malicious web sites from suspicious URLs,” in ACM
SIGKDD, 2009, pp. 1245–1254.

[46] M. S. I. Mamun, M. A. Rathore, A. H. Lashkari, N. Stakhanova, and
A. A. Ghorbani, “Detecting malicious URLs using lexical analysis,” in
International Conference on Network and System Security, 2016, pp.
467–482.

[47] V. Mavroeidis and S. Bromander, “Cyber threat intelligence model:
an evaluation of taxonomies, sharing standards, and ontologies within
cyber threat intelligence,” in EISIC.
J. McGahagan, D. Bhansali, M. Gratian, and M. Cukier, “A comprehen-
sive evaluation of http header features for detecting malicious websites,”
in EDCC.

IEEE, 2017, pp. 91–98.

IEEE, 2019, pp. 75–82.

[48]

[49] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” in AISTATS, 2017, pp. 1273–1282.

[50] E. M. E. Mhamdi, R. Guerraoui, and S. Rouault, “The Hidden Vulner-
ability of Distributed Learning in Byzantium,” in ICML, 2018.

[52] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of

word representations in vector space,” 2013.

[53] T. Nelms, R. Perdisci, and M. Ahamad, “Execscent: Mining for new
c&c domains in live networks with adaptive control protocol templates,”
in USENIX Security, 2013, p. 589–604.

[54] H. News, “HTTP Status Codes Command This Malware How to Con-
trol Hacked Systems,” https://thehackernews.com/2020/05/malware-
http-codes.html, 2020, [accessed May 2022].

[55] T. D. Nguyen, S. Marchal, M. Miettinen, H. Fereidooni, N. Asokan,
and A.-R. Sadeghi, “D¨ıot: A federated self-learning anomaly detection
system for iot,” in ICDCS, 2019, pp. 756–767.

[56] T. D. Nguyen, P. Rieger, H. Chen, H. Yalame, H. M¨ollering, H. Fer-
eidooni, S. Marchal, M. Miettinen, A. Mirhoseini, S. Zeitouni et al.,
“FLAME: Taming backdoors in federated learning,” in USENIX Secu-
rity, 2022, pp. 1415–1432.

[57] A. Oprea, Z. Li, R. Norris, and K. Bowers, “MADE: Security analytics
for enterprise threat detection,” in ACSAC, 2018, pp. 124–136.

[58] A. Oprea, Z. Li, T.-F. Yen, S. H. Chin, and S. Alrwais, “Detection of
early-stage enterprise infection by mining large-scale log data,” in DSN,
2015, pp. 45–56.

[59]

J. Payne and A. Kundu, “Towards deep federated defenses against
malware in cloud ecosystems,” in TPS-ISA.
IEEE, 2019, pp. 92–100.

[60] R. Perdisci, W. Lee, and N. Feamster, “Behavioral clustering of
HTTP-based malware and signature generation using malicious network
traces.” in NSDI, 2010.

[61] V. L. Pochat, T. Van Goethem, S. Tajalizadehkhoob, M. Korczy´nski,
and W. Joosen, “Tranco: A research-oriented top sites ranking hardened
against manipulation,” arXiv, 2018.

[62] R. Rehurek,

“Gensim: Topic modelling for humans,” https://

radimrehurek.com/gensim/, 2019, [Online; accessed 2021-05-30].

[63] P. Rieger, T. D. Nguyen, M. Miettinen, and A.-R. Sadeghi, “Deepsight:
Mitigating backdoor attacks in federated learning through deep model
inspection,” arXiv preprint arXiv:2201.00763, 2022.

[64] N. Roy and A. McCallum, “Toward optimal active learning through

sampling estimation of error reduction,” in ICML, 2001.

[65]

J. Saxe and K. Berlin, “eXpose: A character-level convolutional neural
network with embeddings for detecting malicious URLs, ﬁle paths and
registry keys,” arXiv preprint arXiv:1702.08568, 2017.

[66] W. Schneble and G. Thamilarasu, “Attack detection using federated
learning in medical cyber-physical systems,” in ICCCN, 2019, pp. 1–8.

[67] B. Sch¨olkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C.
Williamson, “Estimating the support of a high-dimensional distribu-
tion,” Neural Computation, vol. 13, no. 7, pp. 1443–1471, 2001.

[68] V. Shejwalkar and A. Houmansadr, “Manipulating the byzantine: Op-
timizing model poisoning attacks and defenses for federated learning,”
in NDSS, 2021.

[69] R. Sommer and V. Paxson, “Outside the closed world: On using machine
IEEE, 2010, pp.

learning for network intrusion detection,” in S&P.
305–316.

[70] A. K. Sood, S. Zeadally, and R. J. Enbody, “An empirical study of http-
based ﬁnancial botnets,” IEEE Transactions on Dependable and Secure
Computing, vol. 13, no. 2, pp. 236–251, 2014.

[71]

J. W. Stokes, J. Platt, J. Kravis, and M. Shilman, “Aladin: Active
learning of anomalies to detect intrusions,” Microsoft Research, Tech.
Rep., 2008.

[72] Z. Sun, P. Kairouz, A. T. Suresh, and H. B. McMahan, “Can you really

backdoor federated learning?” arXiv, 2019.

[73] Symantec,

“What

ransomware,”
intelligence/wannacry-ransomware-attack, 2017.

you

need

the wannacry
know about
https://symantec-blogs.broadcom.com/blogs/threat-

to

[74] Tensorﬂow, “Tensorﬂow federated: Machine learning on decentralized
data,” https://www.tensorﬂow.org/federated, [accessed Aug 2022].

[75] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song, “Design and

14

evaluation of a real-time URL spam ﬁltering service,” in S&P, 2011,
pp. 447–462.

[76] V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data poisoning attacks

against federated learning systems,” in ESORICS, 2020, p. 480–501.

[77] W. Tounsi and H. Rais, “A survey on technical threat intelligence in
the age of sophisticated cyber attacks,” Computers & security, vol. 72,
pp. 212–233, 2018.

[78] C. Wagner, A. Dulaunoy, G. Wagener, and A. Iklody, “MISP: The
design and implementation of a collaborative threat intelligence sharing
platform,” in WISCS, 2016, pp. 49–56.

[79] H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal,
J.-y. Sohn, K. Lee, and D. Papailiopoulos, “Attack of the tails: Yes,
you really can backdoor federated learning,” in Advances in Neural
Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell,
M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020,
pp. 16 070–16 084. [Online]. Available: https://proceedings.neurips.cc/
paper/2020/ﬁle/b8ffa41d4e492f0fad2f13e29e1762eb-Paper.pdf
[80] T. Wink and Z. Nochta, “An approach for peer-to-peer federated

learning,” in DSN-W, 2021, pp. 150–157.

[81] C. Xie, K. Huang, P.-Y. Chen, and B. Li, “DBA: Distributed backdoor

attacks against federated learning,” in ICLR, 2020.

[82] D. Yin, Y. Chen, R. Kannan, and P. Bartlett, “Byzantine-Robust Dis-
tributed Learning: Towards Optimal Statistical Rates,” in ICML, 2018.
[83] H. Yuan, Z. Yang, X. Chen, Y. Li, and W. Liu, “URL2Vec: URL mod-
eling with character embeddings for fast and accurate phishing website
detection,” in ISPA/IUCC/BDCloud/SocialCom/SustainCom, 2018, pp.
265–272.

[84] Y. Zhao, J. Chen, D. Wu, J. Teng, and S. Yu, “Multi-task network
anomaly detection using federated learning,” in SoICT, 2019, pp. 273–
279.

[85] Y. Zhauniarovich, I. Khalil, T. Yu, and M. Dacier, “A survey on mali-
cious domains detection through DNS data analysis,” ACM Computing
Surveys (CSUR), vol. 51, no. 4, pp. 1–36, 2018.

APPENDIX

Table IX presents the features used in this study.

Embedding model design. We considered several approaches
for generating URL embeddings. The ﬁrst option is a central-
ized training of the word models using either Word2Vec or
FastText architectures. The training of the embedding model
is carried out on the server using the data collected from all
the clients. Thus, a single entity (i.e., the server) has access
to the entire data, and uses it to learn the vector embeddings
from the URLs (or domain, referer). This approach suffers
from scalability and privacy issues, as all the raw URL data
has to be collected in a central server to perform the training.

We are interested in a more privacy-preserving and scalable
approach for distributed training of the embeddings among
multiple participants. In theory, it is possible to apply the
Federated Averaging algorithm to train Word2Vec or FastText
models. However, in practice, there are a number of challenges
that prevented us for adopting this solution. First, existing
libraries for word embedding generation such as GenSim are
highly optimized (e.g., they have custom C implementations
and parallelize gradient updates using multiple cores), and it
is not immediately clear how to leverage these libraries in
combination with the Federated Averaging algorithm (which
requires server-side aggregation of local model updates). Sec-
ond, Federated Averaging for Word2Vec has been shown to
exhibit slow convergence due to the large size of updates sent
in each iteration [9].

Based on these considerations, we developed a distributed
approach using integrated sequential client updates. In this

Type

Fields

Description

Embedded
features

Numerical
features

URL, Domain
Name and
Referer

Request and
Response Size,
Transaction
Depth, Version

UA Browser
Information

Binary Features
for host, URI,
referer, user
agent, method

Categorical
features

External IP

Port

User Agent

HTTP Method,
Status Code,
Content Type

Based on embedding
models that preserve
URL and domain
structure. (2176 features)

Represented as ﬂoat. (4
features)

Browser version from
UA represented as ﬂoat.
(2 features)
Binary features to
indicate the existence of
certain ﬁelds in the
HTTP log. (5 features)
IP subnets are
represented as a
sequence of three tokens
corresponding to the
octets, and then
one-hot-encoded. (768
features)
Destination port,
one-hot-encoded. (100
features)
User agents are parsed to
retrieve device, browser,
and OS, which are
one-hot encoded. (2542
features)
Represented as
one-hot-encoded vectors.
(267 features)

TABLE IX: Features extracted from HTTP logs. Each HTTP
log record is represented as a ﬁxed-size vector containing
numerical representations of these features.

approach, each client has access to its own data and updates the
global embedding model locally, using its entire corpus. The
client sends the updated global model back to the server, who
acts as a trusted central coordinator. The clients apply their
updates sequentially, in a round robin fashion, over multiple
iterations.

When instantiated with Word2Vec, this method still re-
quires a common word (i.e., token) vocabulary. To address
this requirement, the clients send their token frequencies to
the server in a pre-processing phase. The server aggregates
them, deciding on a lower bound frequency (e.g., all tokens
that appear more than once), and sends the global vocabulary
back to each client. The privacy of the system can be further
enhanced with FastText embeddings, which use n-grams (i.e.,
sequences of n characters) as tokens. This method will alleviate
the need to share tokens in advance, as we can consider all
the possible n-grams to constitute the vocabulary. FastText
has the additional advantage of supporting new tokens not
observed at training time by generating n-gram representations
for them. This property is an important one, as adversaries can
change parts of the URLs to evade detection (for example, the
query string, path, and parameters can be easily updated). As
we show in our evaluation, FastText and Word2Vec perform
similarly in performance (and have higher performance than
lexical features, as expected). For these reasons, we select
federated FastText as the preferred embedding method for
CELEST URL representation.

15

