7
1
0
2

t
c
O
1
2

]
L
M

.
t
a
t
s
[

2
v
0
0
6
3
0
.
2
0
6
1
:
v
i
X
r
a

Data-Driven Online Decision Making with Costly Observations

Onur Atan
Electrical Engineering Department
University of California, Los Angeles
oatan@ucla.edu

Mihaela van der Schaar
Department of Engineering Science
University of Oxford
mihaela.vanderschaar@eng.ox.ac.uk

Abstract

In most real-world settings such as recom-
mender systems, ﬁnance, and healthcare, col-
lecting useful information is costly and re-
quires an active choice on the part of the
decision maker. The decision-maker needs
to learn simultaneously what observations to
make and what actions to take. This pa-
per incorporates the information acquisition
decision into an online learning framework.
We propose two diﬀerent algorithms for this
dual learning problem: Sim-OOS and Seq-
OOS where observations are made simultane-
ously and sequentially, respectively. We prove
that both algorithms achieve a regret that is
sublinear in time. The developed framework
and algorithms can be used in many appli-
cations including medical informatics, recom-
mender systems and actionable intelligence in
transportation, ﬁnance, cyber-security etc., in
which collecting information prior to making
decisions is costly. We validate our algorithms
in a breast cancer example setting in which
we show substantial performance gains for our
proposed algorithms.

1

Introduction

In numerous real-world settings, acquiring useful infor-
mation is often costly. In many applications such as rec-
ommender systems, ﬁnance, or healthcare, the decision-
maker performs costly research/experimentation to
learn valuable information. For instance, a website
must pay costs to observe (e.g. through cookies) the
contextual information of its online users. In doing so it
must decide the best information to observe in order to
minimize informational costs while also achieving high

Preliminary work. Under review by AISTATS 2018. Do not
distribute.

rewards. However, classical contextual Multi-Armed
Bandit (MAB) formulations (Chu et al. [2011], Slivkins
[2011], Lu et al. [2010], Dudik et al. [2011], Langford
and Zhang [2007]) have not previously considered these
important informational costs and are thus unable to
provide satisfactory performance in such settings. This
paper presents new and powerful methods and algo-
rithms for Contextual MAB with Costly Observations
(CMAB-CO). We show numerically that our algorithms
achieve signiﬁcant performance gains in breast cancer
setting, and we note that the methods and algorithms
we develop are widely applicable (perhaps with some
modiﬁcations) to an enormous range of other settings
as well, from recommender systems to ﬁnance.

A major challenge in these settings is the learning of
both optimal observations and actions. Current MAB
methods could potentially be modiﬁed to address this
issue by combining the choice of the context to observe
and the action to be taken as a single meta-action
and folding the costs of observations in the rewards.
However, the regret of such an approach can be shown
to be exponential in the number of actions and the
number of possible context states; therefore, it is so
ineﬃcient as to be impractical for any realistic problem.
Therefore there is a strong need for the development
of new algorithms that achieve better performance.

To overcome the limitations and challenges discussed
above, we propose an alternative approach. We formal-
ize the CMAB-CO problem and show that this problem
can be reduced to a two stage Markov Decision Pro-
cess (MDP) problem with a canonical start state. We
propose two diﬀerent algorithms for this dual learning
problem: Sim-OOS and Seq-OOS where observations
are made simultaneously and sequentially, respectively.
These algorithms build upon the UCRL2 algorithm
of (Jaksch et al. [2010]) to eﬃciently learn optimal
observations and actions. We show that both Sim-OOS
and Seq-OOS algorithms achieve a regret that is sublin-
ear in time. These algorithm thus perform well when
the number of observations is small, and it represents a
signiﬁcant improvement over existing algorithms, which

 
 
 
 
 
 
Manuscript under review by AISTATS 2018

would be exponential in the number of observations as
well as actions.

Our main contributions can be summarized as follows:

1. We formalize the CMAB-CO problem as a two-

stage MDP.

2. We propose two algorithms under two assumptions:
simultaneous and sequential observation selection.
We show sublinear in time regret bounds for both
algorithms.

3. We use a breast cancer dataset and show that
we can achieve up to signiﬁcant improvement in
performance with respect to an important bench-
mark.

As we have noted, our algorithms apply in many set-
tings with diﬀerent observations, actions and rewards.
In the medical context, the observations might consist
of diﬀerent types of (costly) medical tests (e.g., blood
tests, MRI, etc.), actions might consist of choices of
treatment, and rewards might consist of 5 year survival
rates. Hence, an important aspect of the decision-
making is which medical tests to conduct and which
treatment option to recommend.
In the recommen-
dation system context, the observations might consist
of (costly) information about the user (e.g., previous
search records, likes in social media, etc.), actions might
consist of item choices and rewards might consist of
click rates. In ﬁnancial applications, the observations
might represent (costly) research and information gath-
ering about speciﬁc assets (stocks, loans, IPOs, etc.),
actions might represent investment decisions, and re-
wards might represent investment returns. Indeed, the
ﬁnancial literature has studied the costs (and incen-
tives) associated with information gathering in a variety
of settings (Campbel and Kracaw [1980], Chemmanur
[1993]).

actions (observations and real actions) are selected and
the rewards of all selected actions (observation cost
and real action rewards) are selected in our setting.
However, combinatorial semi-bandits do not utilize the
observed states when taking the action.

Our work is also very related to online probing (Zol-
ghadr et al. [2013]). However, the goal in (Zolghadr
et al. [2013]) is to learn the optimal observations and
a single best function that maps observed features to
labels in order to minimize the loss and the observation
cost jointly. Unlike in the considered CMAB-CO set-
ting, an adversarial setup is assumed and a complete
loss feedback (the loss associated with all the various
actions) is obtained at each stage.

2.2 MDP literature

The CMAB-CO problem which we consider can be
formalized as a two-stage MDP (Jaksch et al. [2010],
Ortner and Auer [2007], Osband et al. [2016]) with a
canonical start state. The action set available in the
start state is the set of observations. Following an ob-
servation action in the start state, the decision-maker
moves to a new state (which consists of the realized
states of the selected observations) from which the
decision-maker selects a real action and moves back
to the start state. The reward in the ﬁrst step is
the observation cost (negative) and the second step is
the random reward obtaind by taking the real action.
Stemming from this and building upon the UCRL2
algorithm of (Ortner and Auer [2007], Jaksch et al.
[2010]), we construct eﬃcient algorithms by exploit-
ing the structure of the CMAB-CO problem: sparse
observation probabilities, known costs.

2 Related Work

2.3 Budgeted Learning

Our paper contributes to multiple strands of literature,
including MAB, MDP and budgeted learning. We
describe the contributions of our work to each topic in
turn.

2.1 MAB Literature
This work relates to various strands of research in
the MAB literature (Chu et al. [2011], Slivkins [2011],
Lu et al. [2010], Dudik et al. [2011], Langford and
Zhang [2007], Tekin and Van Der Schaar [2014]). For
example, Tekin and Van Der Schaar [2014] focuses on
learning the optimal actions by discovering relevant
information. However, this work does not consider
the costs associated with gathering information and
is thus unable to provide satisfactory performance in
the considered setting. The CMAB-CO problem is
similar to combinatorial semi-bandits since multiple

The CMAB-CO problem is also similar to budgeted
learning as the decision-maker’s goal there is to adap-
tively choose which features to observe in order to
minimize the loss. For example, (Cesa Bianchi et al.
[2011], Hazan and Koren [2012]) adaptively choose the
features of the next training example in order to train
a linear regression model while having restricted ac-
cess to only a subset of the features. However, these
problems do not consider information costs and are
restricted to batch learning.

related work is adaptive

submodular-
Another
ity (Golovin and Krause [2010]) which aims to maximize
rewards by selecting at most m observations/actions.
However, their approach assumes that observation
states are statistically independent and rewards have a
submodular structure in observations.

Manuscript under review by AISTATS 2018

3 Contextual Multi-armed Bandits

with Costly Observations

We consider a MAB setting with costly observations
where the following sequence of the events is taking
place at each time t:

3.1 Problem Formulation

Next, we present our problem formulation and illus-
trate it with a speciﬁc example from in the medical
context. Let D = {1, 2, . . . , D} be a ﬁnite set of ob-
servations (types of medical tests such as MRI, mamo-
gram, ultrasound etc.). Each observation i ∈ D is in
a (initially unknown) particular state from a ﬁnite set
of Xi of possible values (describing the outcomes of
the medical tests such as the BIRADS score associ-
ated with a mamogram). Let X = ∪i∈DXi represent
the set of all possible state vectors.. The state vector
is φ = (φ[1], φ[2], . . . , φ[D]) , where φ[i] is the state
of observation i, which represents the context in the
CMAB formulation. We assume that the state vector
is drawn according to a ﬁxed but unknown distribu-
tion. We write Φ to denote a random state vector
and p(φ) = Pr(Φ = φ) to denote the probability of
state vector φ being drawn. In the medical context,
p(·) models a joint probability over the results of the
medical tests.

We assume that only the states of the observations
that are selected by the decision-maker are revealed
in each time instance. Let ψ denote a partial state
vector, which only contains the state of a subset of
the selected observations. For example, for selected
observations I ⊆ D, the partial state vector is ψ =
(ψ[1], ψ[2], . . . , ψ[D]) with

ψ[i] =

(cid:40)

φ[i]
?

if i ∈ I
if i /∈ I

where ? denotes our symbol for missing observation
states. We use the notation dom(ψ) = {i ∈ D : ψ[i] (cid:54)=
?} to refer to the domain of ψ (i.e., the set of the
medical test outcomes realized in ψ). Let Ψ+(I) =
{ψ : dom(ψ) = I} denote the set of all possible partial
state vectors with observations from I (i.e., the set
of all possible medical test outcomes of I). Let Ψ =
∪I⊆DΨ+(I) denote the set of all possible partial state
vector states. We say ψ is consistent with φ if they are
equal everywhere in the domain of ψ, i.e., ψ[i] = φ[i]
for all i ∈ dom(ψ).
In this case, we write φ ∼ ψ.
If ψ and ψ(cid:48) are both consistent with some φ, and
dom(ψ) ⊆ dom(ψ(cid:48)), we say ψ is a substate of ψ(cid:48). In
this case, we write ψ(cid:48) (cid:23) ψ.

We illustrate these deﬁnitions on a simple example. Let
φ = (−1, 1, 1) be a state vector, and ψ1 = (−1, ?, −1)
and ψ2 = (−1, ?, ?) be partial state vectors. Then, all
of the following claims are true:

1. The environment draws a state vector φt according
to unknown distribution p(·). The state vector is
initially unknown to the decision-maker.

2. The decision-maker is allowed to select at most m
observation at time t, denoted as It, with paying
a known cost of ci ∈ [0, 1] for each observations i
in the set It. We assume that the decision-maker
has an upper bound m on the maximum number
of observations that can be made at each time t.
Let P≤m(D) denote the subset of the observations
with cardinality less than m, i.e., P≤m(D) = {I ⊆
D : |I| ≤ m}. The partial state vector ψt from the
observations It is revealed to the decision-maker,
while the remainder of the states remain unknown
to the decision-maker.

3. Based on its available information ψt, the decision-
maker takes an action at from a ﬁnite set of actions
A = {1, 2, . . . , A} and observes a random reward
rt with support [0, 1] and E [rt] = ¯r(at, φt) where
¯r : A × X → [0, 1] is an unknown expected reward
function.

We overload the deﬁnition of p and ¯r to denote marginal
probabilities and expected rewards of partial state
vectors. We write p(ψ) = Pr(Φ ∼ ψ) to denote
the marginal probability of ψ being realized and
¯r(a, ψ) = E [¯r(a, Φ)|Φ ∼ ψ] to denote the marginal
expected reward of action a when the partial state
vector is ψ. Observe that (cid:80)

ψ∈Ψ+(I) p(ψ) = 1.

The policy π for selecting observations and associated
actions consists of a set of observations I and an adap-
tive action strategy h : Ψ+(I) → A, which maps each
possible partial state vectors from I to actions (e.g., a
policy consists of a subset of medical tests I and treat-
ment recommendation for each possible test results
from I). The expected gain of the policy π = {I, h} is
given by

ρ(π) = β

(cid:88)

p(ψ)¯r(h(ψ), ψ) −

ψ∈Ψ+(I)

(cid:88)

i∈I

ci,

(1)

where β > 1 is the gain parameter, which balances the
trade-oﬀ between the rewards and observation costs.
For example, β represents the revenue made by one click
in the recommendation system context. The expected
gain of the policy π is the expected reward of π minus
the observation cost incurred by π. Without loss of
generality, we assume that decision-maker is allowed
to make at most m observations. Let Π denote the set
of all possible policies. The oracle policy is given by
π∗
m = arg maxπ=(I,h)∈Π:|I|≤m ρ(π).

φ ∼ ψ2, ψ1 (cid:23) ψ2, dom(ψ1) = {1, 3}.

The expected gain of the oracle policy is given by

Manuscript under review by AISTATS 2018

m = ρ(π∗
ρ∗
m). Note that our oracle is diﬀerent than
the oracle used in the contextual bandit literature. To
illustrate the diﬀerence, deﬁne ¯r∗(ψ) = ¯r(a∗(ψ), ψ) =
maxa∈A ¯r(a, ψ) to be the expected reward of the best
action when the partial state vector is ψ. We refer
to the policy that selects observations I and the best
actions a∗(ψ) for all ψ ∈ Ψ+(I) as the ﬁxed I-oracle
policy. The expected reward of the ﬁxed I-oracle policy
is given by

V (I) = β

(cid:88)

p(ψ)¯r∗(ψ) −

ψ∈Ψ+(I)

(cid:88)

i∈I

ci.

m = V (I ∗

m, h∗)
m = (I ∗
It can be shown that the oracle policy π∗
is given by h∗(ψ) = arg maxa∈A ¯r(a, ψ) and I ∗
m =
arg maxI∈P≤m(D) V (I).. Note that ρ∗
m).
Therefore, the oracle deﬁned in our setting achieves
the best expected reward among all the ﬁxed I-oracle
policies.
Consider an adaptive policy π1:T = [It, ht]T
t=1, which
takes observation-action It, observes ψt, uses this ob-
servation to take an action at = ht(ψt) and receives
the reward of rt. The cumulative reward of π1:T is
(cid:1). The T -time regret of the pol-
(cid:0)βrt − (cid:80)
(cid:80)T
i∈It
icy π1:T = [It, ht]T

t=1 is given by

t=1

ci

Regπ1:T

T = T ρ∗

m −

(cid:32)

βrt −

T
(cid:88)

t=1

(cid:33)

ci

.

(cid:88)

i∈It

Algorithm 1 Simultaneous Optimistic Observation
Selection (Sim-OOS)

Input: m, [ci]i∈D, conf 1(n, t), conf 2(n, t), β
Initialize: E(dom(ψ), ψ) ← ∅ for all ψ ∈ Ψ.
Initialize: E(I) ← ∅ for all I ∈ P≤m(D).
Initialize: E(a, ψ) ← ∅ for all a ∈ A and ψ ∈ Ψ.
for rounds k = 1, 2, . . . do

(cid:80)

Nk(dom(ψ))

for all ψ ∈ Ψ.

τ ∈Ek(a,ψ) rτ for all a ∈ A

conf 1,k(a, ψ) ← conf 1(Nk(a, ψ), tk).
conf 2,k(I) ← conf 2,k(Nk(I), tk).
1
(cid:98)rk(a, ψ) =
Nk(a,ψ)
and ψ ∈ Ψ.
(cid:98)pk(ψ) = Nk(dom(ψ),ψ)
(cid:98)hk(ψ) ← arg maxa∈A (cid:98)rk(a, ψ) + conf 1,k(a, ψ)
Solve the convex optimization problem given in
(3) for all I ∈ P≤m(D)
Set (cid:98)Vk(I) as the maximizer.
(cid:98)Ik ← arg maxI∈P≤m(D) (cid:98)Vk(I).
νk(a, ψ) ← 0 for all a and ψ ∈ Ψ.
while ∀(a, ψ) : νk(a, ψ) < max(1, Nk(a, ψ)) do

Select observations (cid:98)Ik, observe the partial state
vector ψt,
Select action at = (cid:98)hk(ψt), observe reward rt.
Update νk(at, ψt) ← νk(at, ψt) + 1.
for ψ : ψt (cid:23) ψ do

E(dom(ψ), ψ) ← Ek+1(ψ, dom(ψ)) ∪ t.
E(dom(ψ)) ← Ek+1(dom(ψ)) ∪ t.

end for
E(at, ψt) ← Ek+1(a, ψ) ∪ t.
t ← t + 1.

The goal here is to compute the policy π1:T to minimize
this regret by selecting at most m observations.

end while

end for

Current online learning methods could be modiﬁed to
address the CMAB-CO problem by deﬁning a set of
meta-actions that comprises all the combinations of
observation subsets and actions taken based on these
observations, and then applying a standard MAB algo-
rithm (such as the UCB algorithm Auer et al. [2002])
by considering these meta-actions to be the action
space. While this algorithm is straightforward to im-
plement, it scales linearly with the total number of
policies |Π| = (cid:80)
I∈P≤m(D) A|Ψ+(I)|. This is exponen-
tial in the number of state vectors. This makes such
algorithms computationally infeasible and suboptimal
(compared to the lower bound) even when the numbers
of actions and partial states is small. This poor scaling
performance is due to the fact that the algorithm does
not take into account that selecting an action yields
information for many policies.

3.2 Simultaneous Optimistic Observation
Selection (Sim-OOS) Algorithm

To address the above mentioned limitations of such
MAB algorithms, we develop a new algorithm, which
we refer to as Simultaneous Optimistic Observation

Selection (Sim-OOS). Sim-OOS operates in rounds
k = 1, 2, . . .. Let tk denote time at the beginning
of round k. The decision-maker keeps track of the
estimates of the mean rewards and the observation
probabilities. Note that when the partial state vector
ψt from observation set It is revealed, the decision-
maker can use this information to not only update the
observation probability estimate of ψt but also update
the observation probability estimate of all substates
of ψt. However, the decision-maker cannot update
the mean reward estimate of pairs of at and substates
of ψt since this would result in a bias on the mean
reward estimates. Therefore, at each round k, we deﬁne
Ek(a, ψ) = {τ < tk : aτ = a, ψτ = ψ}, Ek(I) = {τ <
tk : I ⊆ Iτ } and Ek(ψ, I) = {τ < tk : I ⊆ Iτ , ψτ (cid:23)
ψ} if ψ ∈ Ψ+(I) and Ek(ψ, I) = ∅ if ψ /∈ Ψ+(I).

We deﬁne the following counters: Nk(I, ψ) =
|Ek(I, ψ)|, Nk(I) = |Ek(I)|, Nk(a, ψ) = |Ek(a, ψ)|. In
addition to these counters, we also keep counters of
partial state-action pair visits in a speciﬁc round k. Let
νk(a, ψ) denote the number of times action a is taken

Manuscript under review by AISTATS 2018

when partial state ψ is observed in round k. Further-
more, we can express the mean reward estimate and
observation probability estimates as follows:

of the following optimization problem:

maximize
[ ˜p(ψ)]ψ∈Ψ+(I)

β

(cid:88)

ψ∈Ψ+(I)

˜p(ψ)(cid:98)r∗

k(ψ) −

(cid:88)

i∈I

ci

(cid:98)rk(a, ψ) =

1
Nk(a, ψ)

(cid:88)

rτ ,

τ ∈Ek(a,ψ)

(cid:98)pk(ψ) =

Nk(dom(ψ), ψ)
Nk(dom(ψ))

provided that Nk(a, ψ) > 0 and Nk(dom(ψ)) > 0.
Since these estimates can deviate from their true mean
values, we need to add appropriate conﬁdence intervals
when optimizing the policy. In the beginning of each
round k, the Sim-OOS computes the policy of round k
by solving an optimization problem given in (2). The
optimization problem with the mean reward estimate
and observation probability estimates is given by

maximize
π={I,h}, ˜p,˜r

β

(cid:88)

ψ∈Ψ+(I)

˜p(ψ)˜r(h(ψ), ψ) −

(cid:88)

i∈I

ci

subject to |˜r(a, ψ) − (cid:98)rk(a, ψ)| ≤ conf1,k(a, ψ), ∀(a, ψ),

|˜p(ψ) − (cid:98)pk(ψ)| ≤ conf2,k(I),

˜p(ψ) = 1, ∀I ∈ P≤m(D),

(2)

(cid:88)

ψ∈Ψ+(I)
(cid:88)

ψ∈Ψ+(I)

where conf1,k(a, ψ) and conf2,k(I) are the conﬁdence
bounds on the estimators at time tk. We will set these
conﬁdence bounds later in order to achieve provable
regret guarantees with high probability. Let (cid:98)πk =
{(cid:98)Ik, (cid:98)hk} denote the policy computed by the Sim-OOS.
The Sim-OOS follows policy (cid:98)πk in round k. At time t
in round k (tk ≤ t ≤ tk+1), the Sim-OOS selects (cid:98)Ik and
observes the partial state vector ψt from observations
Ik and on the basis of this, it takes an action (cid:98)hk(ψt).
Round k ends when one of the visits to the partial state
vector-action pair in round k is the same as Nk(a, ψ)
(the total observations of the partial state-action pair
from previous rounds k(cid:48) = 1, . . . , k − 1). This ensures
that the optimization problem given in (2) is only
solved when the estimates and conﬁdence bounds are
improved.

The optimization problem in (2) can be reduced to
a set of convex optimization problems which can be
solved eﬃciently in polynomial time complexity (Boyd
and Vandenberghe [2004]) (the details of this reduction
are discussed in the supplementary material). In round
k, let (cid:98)r∗
k(ψ) = maxa∈A (cid:98)rk(a, ψ) + conf 1,k(a, ψ) be the
optimistic reward of value of the partial state vector ψ
in round of k. The optimistic gain of a ﬁxed I-oracle in
round k, denoted by (cid:98)Vk(I), is deﬁned as the maximizer

subject to

(cid:88)

(cid:88)

ψ∈Ψ+(I)

˜p(ψ) = 1.

ψ∈Ψ+(I)

|˜p(ψ) − (cid:98)pk(ψ)| ≤ conf2,k(I),

(3)

At any time t of round k,
it can be shown that
the optimization in (2) can be solved as: (cid:98)hk(ψ) =
arg maxa∈A
(cid:98)rk(a, ψ) + conf 1,k(a, ψ) and (cid:98)Ik =
arg maxI∈P≤m(D) (cid:98)Vk(I). The pseudocode for the Sim-
OOS is given in Algorithm 1. It can be easily shown
that the computational complexity of the Sim-OOS
algorithm for T instances is O (A poly(Ψtot) log T ).

3.3 Regret Bounds for the Sim-OOS

algorithm

In this subsection, we provide distribution-independent
regret bounds for the Sim-OOS algorithm. Let ψtot =
(cid:80)
I∈P≤m(D) |Ψ+(I)| denote the number of all possible
states (all possible results from at most m distinct
medical tests).

Theorem 1. Suppose β = 1. For any 0 < δ < 1, set

(cid:32)

(cid:115)

conf 1(n, t) = min

1,

(cid:33)

log (20ΨtotAt5/δ)
2 max (1, n)

and

(cid:32)

(cid:115)

conf 2(n, t) = min

1,

10Ψtot log (4t/δ)
max (1, n)

(cid:33)

.

Then, with probability at least 1 − δ, the regret of the
Sim-OOS satisﬁes

RegSim-OOS

T

= O

(cid:18)(cid:18)√

A +

(cid:113)

|P≤m(D)|

(cid:19)

(cid:112)ΨtotT log (T /δ)

(cid:19)

.

The proof of Theorem 1 and all the other results can
be found in the supplementary material. The UCRL2
(Jaksch et al. [2010]) is designed for general MDP prob-
lems and achieves a regret of ˜O
. Hence,
these regret results are better than those obtained by
UCRL2. This is an important result since it demon-
strates that the Sim-OOS can eﬀectively exploit the
structure of our CMAB-CO problem to achieve eﬃcient
regret bounds which scale better than these that can
be obtained for general MDP problems.

(cid:16)(cid:112)Ψ2

totAT

(cid:17)

We illustrate this bound using the same example above.
Suppose |Xi| = X for all i ∈ D and m = D. The
upper bound given in Theorem 1 is in the order of
(cid:113)(cid:80)D
˜O

(cid:18)(cid:113)(cid:80)D

m=1 X m2DT +

m=1 X mAT

(cid:19)

.

Manuscript under review by AISTATS 2018

The Sim-OOS algorithm performs well for smaller val-
ues of which is the case in the medical setting, as it
is for instance the case in breast cancer screening, in
which imaging tests are limited to a small set: mam-
mogram, MRI and ultrasound (Saslow et al. [2007]).
In this context, the observations are usually selected
sequentially. To address such settings, we next pro-
pose the Seq-OOS algorithm that selects observations
sequentially.

4 Multi-armed Bandits with

Sequential Costly Observations

4.1 Problem Formalism

Our current setting assumes that decision-maker makes
all the observations simultaneously.
If the decision-
maker is allowed to make observations sequentially, she
can use the partial state from already selected observa-
tions to inform the selection of future observations. For
example, in the medical settings, although a positive
result in a medical test is usually followed by additional
medical test for validity, a negative result in a medical
test is not usually followed by additional medical tests.
Since any resulting simultaneous observation policy
can be achieved by a sequential observation policy, the
oracle deﬁned with sequential observations achieves
higher expected reward than that with simultaneous
observations. At each time t, the following sequence of
events is taking place:

i The decision-maker has initially no observations.
In phase 0, we denote the empty partial state as
ψ0,t = ψ0 where dom(ψ0) = ∅.

ii At each phase l ∈ L = {1, . . . , m}, if the partial
state is ψl,t and observation il,t ∈ (D \ dom(ψl,t))∪
∅ is made, the resulting partial state is ψl+1,t where
ψl+1,t = ψl,t ∪ (il,t, φt(il,t)) if il,t (cid:54)= ∅ and ψl+1,t =
ψl,t otherwise.

iii The decision-maker takes an action at when either
observation il,t = ∅ is made or the ﬁnal phase m is
reached and observes a random reward rt.

Let Ψ+(ψ, i) be the set of resulting partial state when
observation i is made at previous partial state of ψ,
i.e., Ψ+(ψ, i) = {ψ(cid:48) : ∃x, ψ(cid:48) = ψ ∪ (i, x)}.
In this
section, we deﬁne p(ψ(cid:48)|ψ, i) as the probability of result-
ing partial state ψ(cid:48) when the observation i is made at
previous partial state of ψ, which is referred to as par-
tial state transition probability. For all ψ(cid:48) ∈ Ψ+(ψ, i),
the partial state transition probability is deﬁned as
p(ψ(cid:48)|ψ, i) = Pr(Φ(i) = ψ(cid:48)(i)|Φ ∼ ψ) if i ∈ D \ dom(ψ)
and p(ψ(cid:48)|ψ, i) = 0 otherwise. In the medical example,
this is the probability of observing test i’s result as

ψ(cid:48)(i) given the previous test results (records) ψ. We
deﬁne p(ψ|ψ, ∅) = 1 and p(ψ(cid:48)|ψ, ∅) = 0 for all ψ(cid:48) (cid:54)= ψ.
Let P = [p(ψ(cid:48)|ψ, i)] denote partial state transition
probability matrix.

A sequential policy π = {g, h} consists of observation
function g and action function h where g : Ψ → D ∪ ∅
and h : Ψ → A (e.g., g(ψ) refers to the next medical test
applied on a patient with previous records (test results)
ψ and h(ψ) refers to treatment recommendation for
a patient with previous records(test results) ψ). A
sequential policy π = {g, h} works as follows. Decision-
maker keeps making observations g(ψ) until either m
observations are made or an empty observation g(ψ) =
∅ is picked and takes an action h(ψ) in a terminal state
ψ where terminal partial states of policy π is the state
with either cardinality m or with g(ψ) = ∅.

We illustrate these deﬁnitions in a medical example.
Assume that there are 2 diﬀerent tests with possible
outcomes of positive (+) and negative (−) result and
3 diﬀerent possible treatments. Suppose that a sequen-
tial policy π = (g, h) with g(∅) = {1}, g({(1, +)}) =
{2}, g({(1, −)}) = ∅, h({(1, +), (2, +)}) = a1,
h({(1, +), (2, −)}) = a2, h({(1, −)}) = a3. Basically,
this policy initially picks the medical test 1 for all
If the result of the medical
patients (g(∅) = {1}).
test 1 is positive (+), the policy picks medical test 2
(g({(1, +)}) = {2}). On the other hand, if the result
of medical test 1 is negative (−), the policy does not
make any additional test. In this example, terminal
partial states of policy π are ψ3, ψ4, ψ5.

Given a sequential policy π, let ψl denote the random
partial state in phase l and cl = cg(ψl) denote the
random cost in phase l by making observation g(ψl).
Note that cl is random since partial state in phase l
is random. Similarly, let rm denote random reward
revealed by taking action am = h(ψm) in terminal
partial state. Then, for each sequential policy π =
(g, h), we deﬁne a value function for l = 0, . . . , m:

(cid:104)
l (ψ) = E
F π

βrm −

m−1
(cid:88)

τ =l

(cid:12)
(cid:12)
(cid:12)
(cid:12)

cτ

ψl = ψ, π

(cid:105)

,

(4)

where expectation is taken with respect to randomness
of the states and rewards. In the terminal phase, we
deﬁne value function as F π
m(ψ) = ¯r(h(ψ), ψ). The opti-
mal value function is deﬁned by F ∗
l (ψ).
A policy π∗ is said to be optimal if F π∗
0 (ψ). It
is also useful to deﬁne partial state-observation optimal
value function for l = 0, . . . , m − 1 :
l+1(ψl+1)|ψl = ψ, il = i(cid:3)
l (ψ, i) = E (cid:2)−ci + F ∗
Q∗
(cid:88)
l+1(ψ(cid:48)).
p(ψ(cid:48)|ψ, i)F ∗
= −ci +

l (ψ) = supπ F π
0 (ψ) = F ∗

ψ(cid:48)∈Ψ+(ψ,i)

A sequential policy π∗ = (g∗, h∗) is optimal if and only

Manuscript under review by AISTATS 2018

if g∗(ψ) = arg maxi∈(D∪∅) Q∗
arg maxa∈A ¯r(a, ψ).

| dom(ψ)|(ψ, i), h∗(ψ) =

learning algorithm π1:T =
Consider a sequential
(gt, ht)T
t=1. The algorithm makes observation il,t =
g(ψl,t) and realizes a cost cl,t in phase l of time t and
then selects action at = ht(ψm,t) and realizes a random
reward rt, which realizes a reward of rt − (cid:80)m−1
l=0 cl,t.
To quantify the performance of sequential learning
algorithm, we deﬁne cumulative regret of sequential
learning algorithm π1:T up to time T as

Regπ1:T

T = T F ∗

0 (ψ0) −

T
(cid:88)

t=1

(cid:32)

rt −

(cid:33)

cl,t

m−1
(cid:88)

l=0

pair ψ ∈ Ψm−1 and i ∈ D \ dom(ψ). Let ˆQm−1,k(ψ, i)
denote optimistic value function for making observation
i in partial state ψ in round k of phase m − 1, which
is the solution of the following convex optimization
problem :

maximize
[ ˜p(·|ψ,i)]

− ci +

(cid:88)

˜p(ψ(cid:48)|ψ, i) ˆFm,k(ψ(cid:48))

ψ(cid:48)∈Ψ+(ψ,i)

subject to

(cid:88)

|˜p(ψ(cid:48)|ψ, i) − ˆpk(ψ(cid:48)|ψ, i)| ≤ conf 2,k(ψ, i),

ψ(cid:48)∈Ψ+(ψ,i)
(cid:88)

˜p(ψ(cid:48)|ψ, i) = 1.

ψ(cid:48)∈Ψ+(ψ,i)

(5)

where ψ0 = ∅ denotes empty state. In the next subsec-
tion, we propose a sequential learning algorithm, which
aims to minimize regret.

4.2 Sequential Optimistic Observation

Selection (Seq-OOS)

In addition to observation sets that are tracked by
Sim-OOS, Seq-OOS keeps track of the following sets
at each round k : Ek(ψ, i) = {τ < tk : ∃l ∈ L, ψl,τ =
ψ, il,t = i}, Ek(ψ, i, ψ(cid:48)) = {τ < tk : ∃l ∈ L, ψl,τ =
ψ, il,τ = i, ψl+1,τ = ψ(cid:48)}. Let Nk(ψ, i) = |Ek(ψ, i)|
and Nk(ψ, i, ψ(cid:48)) = |Ek(ψ, i, ψ(cid:48))|. In addition to these
counters, we also keep counters of visits in partial state-
action pairs and state-observation pairs in a particular
round k. Let νk(ψ, i) denote the number of times
observation i is made when partial state ψ is realized
in round k. We can express the estimated transition
probabilities as ˆpk(ψ(cid:48)|ψ, i) = Nk(ψ,i,ψ(cid:48))
, provided that
Nk(ψ,i)
Nk(ψ, i) > 0.

The Seq-OOS works in rounds k = 1, . . .. In the be-
ginning of round k (tk denotes time of beginning of
round k), the Seq-OOS solves Optimistic Dynamic Pro-
gramming (ODP), which takes the estimates ˆP k =
[ˆpk(ψ(cid:48)|ψ, i)] and ˆRk = [ˆrk(a, ψ)] as an input and out-
puts a policy πk. The ODP ﬁrst orders the partial
states with respect to size of their domains. Let Ψl
denote partial states with l observations, which is de-
ﬁned by Ψl = {ψ : | dom(ψ)| = l} (e.g., all possible
results from l distinct medical tests). Since the decision-
maker is not allowed to make any more observations for
any state ψ ∈ Ψm, estimated value of state ψ is com-
puted by ˆFm,k(ψ) = maxa∈A ˆrk(a, ψ) + conf 1,k(a, ψ)
where conf 1,k(a, ψ) is the conﬁdence interval for par-
tial state-action pair in round k. The action and
observation functions on partial state ψ ∈ Ψm com-
puted by ODP is given by ˆgk(ψ) = ∅ and ˆhk(ψ) =
arg maxa∈A ˆrk(a, ψ) + conf 1,k(a, ψ). After computing
value and policy in partial states ψ ∈ Ψm, the ODP
solves convex optimization problem to compute opti-
mistic value function for each partial state-observation

(cid:104) ˆQm,k(ψ, i)

ˆFm−1,k(ψ) = maxi∈(D\dom(ψ))∪∅

Note that the variables ( ˆFm,k(ψ(cid:48))) used in the convex
optimization problem given in (5) is computed in the
previous step by the ODP. The optimistic value of
the empty observation ∅ in partial state ψ in round
k is computed by ˆQm−1,k(ψ, ∅) = maxa∈A βˆrk(a, ψ) +
conf 1,k(a, ψ). Based on the optimistic value of partial
(cid:105)
state-observation pairs
, the ODP com-
putes the optimistic value of partial state ψ and action
and observation function of partial state ψ ∈ Ψm−1
ˆQm−1,k(ψ, i),
as
ˆhk(ψ) = arg maxa∈A βˆrk(a, ψ)+conf 1,k(a, ψ), ˆgk(ψ) =
ˆQm−1,k(ψ, i). These computa-
arg maxi∈(D\dom(ψ))∪∅
tions are repeated for l = m − 2, . . . , 0 to ﬁnd the
complete policy ˆπk.
Given ˆπk = (ˆgk, ˆhk), at each time t of round k (tk−1 ≤
t ≤ tk), the Seq-OOS follows the policy ˆπk. Basically,
if the state at phase l is ψl,t, the Seq-OOS decides
to make the observation il,t = ˆgk(ψl,t) and observes
the state ψl+1,t. If the state is ψl,t at phase l < m
and observation il,t = ˆgk(ψl,t) computed by the ODP
is empty set, i.e., ˆgk(ψl,t) = ∅, then Seq-OOS takes
action ˆhk(ψl,t). If it is a terminal phase, i.e., l = m,
Seq-OOS takes an action ˆhk(ψm,t).

4.3 Regret Bounds of the Seq-OOS

The analysis of the regret of the Seq-OOS exhibits sim-
ilarities to the analysis of the regret of the Sim-OOS.
The Seq-OOS has at most m + 1 phases in which it
makes observations sequentially followed by an action
while Sim-OOS has 2 phases in which it makes simulta-
neous observations at once followed by an action. The
diﬀerence is that we need to decompose the regret of
the Seq-OOS into regret due to phases with suboptimal
observations and regret due to suboptimal actions. Let
Ψmax = maxψ maxi∈D |Ψ+(ψ, i)|. The next theorem
bounds the distribution-independent regret.

Manuscript under review by AISTATS 2018

Theorem 2. Suppose β = 1. For 0 < δ < 1, set

(cid:32)

(cid:115)

conf 1(n, t) = min

1,

(cid:33)

log (20ΨtotAt5/δ)
2 max (1, n)

and

(cid:32)

(cid:115)

conf 2(n, t) = min

1,

10Ψmax log (4DΨtott/δ)
max (1, n)

(cid:33)

.

Then, with probability at least 1 − δ, regret of the Seq-
OOS satisﬁes

RegSeq-OOS
T

= O

(cid:16)(cid:16)

(cid:112)

m

ΨmaxD +

√

(cid:17) (cid:112)ΨtotT log (T /δ)

(cid:17)

A

.

of the following information about the patient: age,
estrogen receptor, tumor stage, WHO score. The
treatment is a choice among four chemotherapy
regimes AC, ACT, CAF, CEF. The outcomes for these
regimens were derived based on 32 references from
PubMed Clinical Queries; this is a medically accepted
procedure. Hence, the data contains the feature vector
and all derived outcomes for each treatment. The
details are given in ["removed for anonymous submis-
sion"]. We generate 200000 instances by randomly
selecting a sample from the breast cancer dataset.
In each instance, we set the observations as D =
{age, estrogen receptor, tumor stage, W HOScore},
and the rewards as 1 if the treatment with the highest
outcome is given to the patient and 0 otherwise. For
the experimental results, we set β = 100 and m = 3.

The diﬀerence in the regret bounds of Sim-OOS and
Seq-OOS is because Sim-OOS estimates the observation
probabilities p(ψ) for each ψ ∈ Ψ whereas Seq-OOS
estimates observation transition probabilities p(·|ψ, i)
for each ψ ∈ Ψ and i ∈ D.

Now, we illustrate and compare the regret bounds on
our algorithms. Suppose that |Xi| = X for all i ∈ D
and m = D. In this case, we have the distribution
for Sim-
independent regret of O
(cid:16)

(cid:16)
2D(cid:112)AX D log T /δ

OOS and
for Seq-OOS
with probability at least 1 − δ. Our algorithms become
computationally feasible when X D is small.

D(cid:112)D2DX D+1AT log T /δ

(cid:17)

(cid:17)

ci

t=1

(cid:3).

(cid:80)T

(cid:2)βrt − (cid:80)

We compare Sim-OOS and Seq-OOS algorithms with
a contextual bandit algorithm that observes realiza-
tion of all observation states φ by paying cost of
(cid:80)D
i=1 ci, referred to as Contextual-UCB. We deﬁne
the following metric of Gain of our algorithms ,which
make observations It and receives reward of rt by tak-
ing action at at each time t, over T time steps by
Gain = 1
T

i∈It
Performance of the Sim-OOS and Seq-OOS
with Diﬀerent Costs: We consider that the cost
of each observation ci = c. We illustrate gain of Sim-
OOS, Seq-OOS and Contextual-UCB algorithms for
increasing values of cost c. As Figure 1 illustrate, the
gain of the Sim-OOS and Seq-OOS algorithm decreases
as the observation cost increases. However, it should
be noted that these algorithms learn the best simul-
taneous and sequential policies while simultaneously
taking actions irrespective of the costs of observation.
These ﬁgures show that when the observation cost is
increasing, the Sim-OOS and Seq-OOS achieves better
gains than Contextual-UCB by observing less infor-
mation, hence paying less cost. Therefore, the slope
of the gain-cost curve of the Sim-OOS and Seq-OOS
illustrated in Figure 1 decreases as the observation cost
increases.

Figure 1: Comparison of Sim-OOS, Seq-OOS and All-
Context UCB

5

Illustrative Results

We evaluate the Sim-OOS and Seq-OOS on a
dataset of 10,000 records of breast cancer patients
participating in the National Surgical Adjuvant
Breast and Bowel Project (NSABP) by ["removed
for anonymous submission"]. Each instance consists

6 Conclusions

In this paper, we introduced the novel, yet ubiquitous
problem of contextual MAB with costly observations:
selecting what information (contexts) to observe to
inform the decision making process. To address this
problem, we developed two diﬀerent algorithms: Sim-
OOS and Seq-OOS, and prove that these algorithms
achieve distribution-independent regret bounds that
are sublinear in time. Future work will be dedicated
to exploring algorithms with regret bounds that are
polynomial on the number of observations.

234566570758085Cost (c)Gain  Sim−OOSSeq−OOSContextual−UCBManuscript under review by AISTATS 2018

for breast screening with mri as an adjunct to mam-
mography. CA: a cancer journal for clinicians, 57
(2):75–89, 2007.

A. Slivkins. Contextual bandits with similarity infor-
mation. In 24th Annual Conference On Learning
Theory, 2011.

C. Tekin and M. Van Der Schaar. Discovering, learning
and exploiting relevance.
In Advances in Neural
Information Processing Systems, pages 1233–1241,
2014.

N. Zolghadr, G. Bartók, R. Greiner, A. György, and
C. Szepesvári. Online learning with costly features
and labels. In Advances in Neural Information Pro-
cessing Systems (NIPS), pages 1241–1249, 2013.

References

P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time
analysis of the multi-armed bandit problem. Machine
Learning, 47:235–256, 2002.

S. Boyd and L. Vandenberghe. Convex optimization.

Cambridge university press, 2004.

T. S. Campbel and W. A. Kracaw. Information produc-
tion, market signalling, and the theory of ﬁnancial
intermediation. The Journal of Finance, 35(4):863–
882, 1980.

N. Cesa Bianchi, S. Shalev Shwartz, and O. Shamir.
Eﬃcient learning with partially observed attributes.
The Journal of Machine Learning Research, 12:2857–
2878, 2011.

T. J. Chemmanur. The pricing of initial public oﬀerings:
A dynamic model with information production. The
Journal of Finance, 48(1):285–304, 1993.

W. Chu, L. Li, L. Reyzin, and R. E. Schapire. Con-
textual bandits with linear payoﬀ functions. In In-
ternational Conference on Artiﬁcial Intelligence and
Statistics, pages 208–214, 2011.

M. Dudik, D. Hsu, S. Kale, N. Karampatziakis, J. Lang-
ford, L. Reyzin, and T. Zhang. Eﬃcient optimal
arXiv preprint
learning for contextual bandits.
arXiv:1106.2369, 2011.

D. Golovin and A. Krause. Adaptive submodularity:
A new approach to active learning and stochastic
optimization. In COLT, pages 333–345, 2010.

E. Hazan and T. Koren. Linear regression with limited
observation. In Proc. 29th Int. Conf. on Machine
Learning, pages 807–814, 2012.

T. Jaksch, R. Ortner, and P. Auer. Near-optimal
regret bounds for reinforcement learning. Journal of
Machine Learning Research, 11:1563–1600, 2010.

J. Langford and T. Zhang. The epoch-greedy algorithm
for contextual multi-armed bandits. Advances in
Neural Information Processing Systems (NIPS), 20:
1096–1103, 2007.

T. Lu, D. Pál, and M. Pál. Contextual multi-armed
bandits. In International Conference on Artiﬁcial
Intelligence and Statistics, pages 485–492, 2010.

P. Ortner and R. Auer. Logarithmic online regret
bounds for undiscounted reinforcement learning. In
Advances in Neural Information Processing Systems,
2007.

I. Osband, B. Van Roy, and Z. Wen. Generalization
and exploration via randomized value functions. In
International Conference on Machine Learning, 2016.

D. Saslow, C. Boetes, W. Burke, S. Harms, M. O. Leach,
C. D. Lehman, E. Morris, E. Pisano, M. Schnall,
S. Sener, et al. American cancer society guidelines

