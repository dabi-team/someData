2
2
0
2

y
a
M
0
1

]

G
L
.
s
c
[

1
v
6
1
8
4
0
.
5
0
2
2
:
v
i
X
r
a

Reconstruction Enhanced Multi-View Contrastive Learning for
Anomaly Detection on Attributed Networks

Jiaqiang Zhang1,2 , Senzhang Wang3 and Songcan Chen1,2∗
1College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics
2MIIT Key Laboratory of Pattern Analysis and Machine Intelligence
3Central South University
{zhangjq, s.chen}@nuaa.edu.cn, szwang@csu.edu.cn

Abstract

Detecting abnormal nodes from attributed networks
is of great importance in many real applications,
such as ﬁnancial fraud detection and cyber security.
This task is challenging due to both the complex in-
teractions between the anomalous nodes with other
counterparts and their inconsistency in terms of
attributes. This paper proposes a self-supervised
learning framework that jointly optimizes a multi-
view contrastive learning-based module and an at-
tribute reconstruction-based module to more ac-
curately detect anomalies on attributed networks.
Speciﬁcally,
two contrastive learning views are
ﬁrstly established, which allow the model to bet-
ter encode rich local and global information re-
lated to the abnormality. Motivated by the attribute
consistency principle between neighboring nodes, a
masked autoencoder-based reconstruction module
is also introduced to identify the nodes which have
large reconstruction errors, then are regarded as
anomalies. Finally, the two complementary mod-
ules are integrated for more accurately detecting
the anomalous nodes. Extensive experiments con-
ducted on ﬁve benchmark datasets show our model
outperforms current state-of-the-art models.

1 Introduction
Attributed networks are ubiquitous in many real-world sce-
narios, such as social networks and citation networks. Re-
cently, anomaly detection on attributed networks is of great
signiﬁcance in many security-related applications, such as
social spam detection [Jain et al., 2019], ﬁnancial fraud de-
tection [Wang et al., 2019] and network intrusion detection
[Shone et al., 2018], and has attracted rising research interest.
Though it has been extensively studied, detecting anomalies
on attributed networks is still a challenging task [Ding et al.,
2019]. One major challenge of this problem is that the abnor-
mal patterns of nodes are related to not only their interactions
with other nodes on the topological structure, but also their
inconsistency in terms of node attributes [Ding et al., 2019;
Peng et al., 2018].

∗Corresponding author

The early anomaly detection techniques, such as matrix
factorization [Li et al., 2017] and OC-SVM [Erfani et al.,
2016] have been widely used in many applications. A pri-
mary limitation of such methods is that they largely rely
on feature engineering constructed by domain experts. Re-
cently, deep learning techniques, especially Graph Neural
Networks, have been widely used in various graph mining
tasks (e.g. link prediction and node classiﬁcation), and have
achieved considerable performance gains. GNN-based tech-
niques [Ding et al., 2021] have also been adopted to anomaly
detection which aim to learn the anomaly-aware node repre-
sentations. Due to the high cost of obtaining anomaly sam-
ples, anomaly detection is usually performed in an unsuper-
vised manner. [Ding et al., 2019] proposed an unsupervised
autoenconder-based method to detect the abnormal patterns
effectively, which can capture the abnormal nodes accord-
ing to the reconstruction errors. [Liu et al., 2021b] for the
ﬁrst time proposed a contrastive learning based framework
for anomaly detection on graphs. Their model takes the lo-
cal contextual information as supervision signals, and learns
the representative features from the node-subgraph instance
pairs. The ﬁnal discriminative scores are used to detect abnor-
mal nodes. [Jin et al., 2021] performed patch- and context-
level contrastive learning via two GNN-based models to fur-
ther improve the performance.

Although considerable research efforts have been devoted
to this problem recently, we argue that the current approaches
still have shortcomings due to the following reasons. First,
local and global structural information are not well utilized
and integrated. Existing SOTA approach [Liu et al., 2021b]
typically constructed contrastive instance pair based on node-
subgraph to efﬁciently focus on the local information of a
node for anomaly detection. They adopted one-layer GCN
to extract the local structure information from the one-hop
neighbors, while the global structure information beyond
one-hop neighbors cannot be effectively captured and uti-
lized. Previous studies show that higher-order neighboring
information is also beneﬁcial for graph mining tasks. [Ding
et al., 2019] proposed to use the multi-layer GCN to capture
the high-order interactions for anomaly detection on graphs,
but the local and global structure information is modeled uni-
formly and coupled together, leading to information redun-
dancy in feature representation learning. Meanwhile, GCN
can be seen as a special form of low-pass ﬁlter, which has the

 
 
 
 
 
 
effectiveness in smoothing signals [Bo et al., 2021]. Stack-
ing multiple layers of GCN will more likely smooth out the
abnormal signals. Therefore, a more effective and decou-
pled local-global structure information extraction and inte-
gration method is needed to provide high quality structure
features. Second, how to effectively fuse the node attributes
with network topological structures to further boost the de-
tection performance is not well explored. Although GCN can
handle both topological structure and attributes, the learned
representations are not well suitable for anomaly detection
under the unsupervised learning scenario [Liu et al., 2021b].
Anomalous nodes usually present inconsistency in terms of
node attributes with their neighboring nodes, which provides
us with additional self-supervised signals. How to fully uti-
lize such signals and build an attribute reconstruction model
based on the attribute consistency principle to further improve
the anomaly detection performance is not well studied.

To alleviate the above drawbacks, we present a Subgraph-
based multi-view self-supervised framework which contains
a Contrastive-based module and a Reconstruction-based
module (Sub-CR for convenience). Sub-CR can separate the
local information from the global information instead of di-
rectly fusing them together, and alleviate the risk of smooth-
ing out abnormal signals in GNN. Speciﬁcally, the original
graph is ﬁrst augmented by both graph diffusion and sub-
graph sampling to obtain local and global view subgraphs
for each node. Then a two-view (intra- and inter-view) con-
trastive learning module is proposed. The intra-view con-
trastive learning aims to maximize the agreement between
the target node and its subgraph-level representations of both
views, here the agreement can be quantiﬁed as a discrimina-
tive score. The inter-view contrastive learning aims to make
the discriminative score of the two views closer, which al-
lows the model to encode both local and global information.
In order to further utilize the self-supervised signals of the at-
tribute consistency between the neighboring nodes, a masked
autoencoder is also introduced to reconstruct the original at-
tributes of the target node based on its neighboring nodes in
both views. Finally, the two complementary modules are inte-
grated for anomalous node detection. Our main contributions
are summarized as follows:

• A novel self-supervised learning framework is proposed
to address the problem of anomaly detection on attributed net-
works, which can effectively integrates contrastive learning-
based and attribute reconstruction-based models.

• A multi-view learning model is proposed. Local and
global information are modeled separately ﬁrst and then in-
tegrated together, which can provide high quality features for
detecting anomalous nodes.

• We conduct extensive experiments on ﬁve benchmark
datasets for model evaluation. The results verify the effec-
tiveness of our proposal and it outperforms existing state-of-
the-art methods.

2 Related Work
2.1 Anomaly Detection on Attributed Networks
The early work [Perozzi and Akoglu, 2016] proposed a qual-
ity measure called normality, which utilized structure and at-

tributes together on attributed networks. [Li et al., 2017] pro-
posed a learning framework to describe the residual of at-
tributes and their consistency with network structure to detect
anomalies in attributed networks. With the rapid develop-
ment of deep neural networks, researchers also tried to adopt
deep learning techniques to detect anomalies in attributed net-
[Ding et al., 2019] proposed to use GCN and au-
works.
toencoder to measure the reconstruction error of nodes from
the perspective of structure and attributes to identify anoma-
lies, which can alleviate the network sparsity and data non-
linearity issues. [Yu et al., 2018] adopted a clustering-based
method through the learned dynamic network representation
to dynamically identify network anomalies based on cluster-
[Li et al., 2019] proposed
ing in an incremental manner.
SpecAE to detect global anomalies and community anoma-
[Liu et al., 2021b;
lies via a density estimation approach.
Jin et al., 2021] proposed a contrastive self-supervised learn-
ing framework for graph anomaly detection, effectively cap-
turing the relationship between nodes and their subgraphs to
capture the anomalies.

2.2 Graph Contrastive Leaning
Recently, contrastive learning as an effective mean of self-
supervised learning [Liu et al., 2021a], has gained rising re-
search interest, which aims to learn representations from su-
pervised signals derived from the data itself. The core idea
is that it considers two augmented views of the same data in-
stance as positive sampler to be pulled closer, and all other in-
stances are considered as the negative samplers to be pushed
farther apart. Due to the great success of self-supervised
learning on images [Chen et al., 2020], contrastive learn-
ing methods are also adopted to various graph learning tasks.
[Velickovic et al., 2019] aimed to learn the node represen-
tations by maximizing the mutual information between the
patch representation and the corresponding high-level graph
[Peng et al., 2020] directly maximized the mu-
summary.
tual information between the input and output of the graph
neural encoder according to the node features and topology
to improve the DGI. [Zhu et al., 2020] proposed to generate
two data views and then pull the representation of the same
node in the two views closer, push the representation of all
other nodes apart. [Zhu et al., 2021] proposed a contrastive
framework for unsupervised graph representation learning
with adaptive data augmentation.

3 Problem Formulation
In this paper, for the convenience of presentation, the calli-
graphic fonts (e.g. G) represent sets, bold lowercase letters
and bold uppercase letters represent vectors (e.g. x) and ma-
trices (e.g. X), respectively. The terminology and problem
deﬁnitions are given as follows. Let G = (V, E, X) denote
an attributed network , where V ={v1, ..., vN } represents the
set of nodes, N is the number of the nodes in the graph, E
is the set of edges, X ∈ RN ×F is the attribute matrix, where
xi ∈ RF (i = 1, ..., N ) is the attributes for the ith node, and
A∈ RN ×N is the adjacency matrix.
Problem Deﬁnition. Given an attributed network G =
(V, E, X) whose adjacency matrix is A, we aim to learn an

Figure 1: The framework of the proposed model.

anomaly score function score(·) to measure the degree of the
abnormality for each node in G.

4 Methodology
As shown in Figure.1,
the proposed Sub-CR contains
a contrastive learning-based module and an attribute
reconstruction-based module. We ﬁrst construct two types of
data augmentations: the subgraph sampling (left upper part
in Figure.1) and the graph diffusion (left lower part in Fig-
ure.1), to get local and global view subgraphs for each node.
For each view, both the subgraph and the target node are in-
put into a GNN encoder to learn the embedding as shown
in the middle part in Figure.1. Then we establish the intra-
and inter-view contrastive learning (right part in Figure.1) to
encode local and global structure information. Meanwhile,
we further introduce the masked autoencoder to identify the
anomalous nodes through the attribute reconstruction errors
in two views. Finally, the two complementary modules are
integrated to give a ﬁnal anomaly score.

4.1 Multi-View Graph Generation with Graph

Diffusion and Subgraph Sampling

The effectiveness of the contrastive learning paradigm largely
relies on the choice of the pretext task and the data augmenta-
tions [Liu et al., 2021a]. We adopt the node-subgraph match-
ing pattern based pretext task which is proved useful for the
anomaly detection on attributed networks [Liu et al., 2021b].
Meanwhile, the graph diffusion augmentation [Hassani and
Khasahmadi, 2020] is introduced to get the global view of
the structure, which allows each node to match with its local-
and global-view subgraphs respectively. Next, we introduce
how to generate the two views of graph.
Graph Diffusion. We propose to use the graph diffusion as
the ﬁrst augmentation to obtain the global view graph. Graph
diffusion can effectively capture the global structure informa-
tion [Hassani and Khasahmadi, 2020; Klicpera et al., 2019].

This process is formulated as follows:

S =

∞
(cid:88)

k=0

θkTk ∈ RN ×N

(1)

where θk is the weighting coefﬁcient to control the propor-
tion of local and global structure information and T ∈ RN ×N
denotes the generalized transition matrix to transfer the adja-
cency matrix. Note that θk ∈ [0, 1] and (cid:80)∞
k=0 θk = 1. In
this paper, Personalized PageRank (PPR) [Page et al., 1999]
is adopted to power the graph diffusion. Given the adjacency
matrix A ∈ RN ×N , the identity matrix I and its degree matrix
D, the transition matrix and the weight can be formulated re-
spectively as T = D−1/2AD−1/2 and θk = α(1 − α)k. Then
the graph diffusion S can be reformulated as:

S = α(I − (1 − α)D−1/2AD−1/2)−1

(2)

where α denotes the teleport probability in a random walk.
The graph diffusion S is the global view of the initial graph.
Subgraph Sampling. We adopt random walk with restart
(RWR) [Tong et al., 2006] to sample the local subgraph. In
both views, a subgraph with the size P is sampled for each
node. In particular, we sample nodes and their edges from the
original view to form the local-view subgraph, and then select
the exact nodes and edges from the other view to form the
global-view subgraph. In each view, for node vi, its subgraph
is regarded as its positive pair, and a subgraph corresponding
to the other node is regarded as the negative pair.

4.2 Local & Global Graph Contrastive Learning
As mentioned before, the representation of a normal node is
similar to that of the neighboring nodes in its subgraph while
this does not hold for an abnormal node. Therefore, we deﬁne
the intra-view contrastive learning in the local and the global
views, which maximizes the agreement between the node and
the corresponding subgraph-level representations. As shown

(a) Graph diffusion & Subgraphsampling(b) Contrastive learning& Attribute reconstruction Inter-View Contrastivet0101Reconstruct𝑺𝒄𝒐𝒓𝒆𝒄𝒐𝒏𝟏GNNweight𝑺𝒄𝒐𝒓𝒆𝒄𝒐𝒏𝟐𝑺𝒄𝒐𝒓𝒆𝒓𝒆𝒔𝟏𝑺𝒄𝒐𝒓𝒆𝒓𝒆𝒔𝟐Intra-View...PositiveNegative...PositiveNegativeGNN LayerAvePool∑AvePool∑ReconstructtGNNweightIntra-View𝑮𝒍𝒐𝒄𝒂𝒍=(𝑋,𝐴)Graph diffusion𝑮𝒈𝒍𝒐𝒃𝒂𝒍=(𝑋,𝑆)PositiveNegativeSubgraphsamplingttttPositiveNegativeSubgraphsamplingin the right part of Figure.1, for node vi, the discriminative
score of its positive pair should be close to 1, while the nega-
tive one should be close to 0. Meanwhile, the inter-view con-
trastive learning is deﬁned between the two views. It aims
to encourage the discriminative score that the same node be-
tween its subgraph in two views to be close.
Intra-View Contrastive Learning. This contrastiveness is
deﬁned in each view. Taking the local view in Figure.1 as an
example, we obtain the corresponding positive and negative
subgraphs Gi, ˜Gi for the target node vi. Note that in order
to make the obtained representation more discriminative, the
attributes of the target node in the subgraph are masked. Then
the subgraph are fed into a shared GCN encoder to learn a
low-dimensional representation, which can be formulated as
follows:

Hl

i = φ( ˆD

−1/2
i

(cid:48)
i

ˆD

−1/2
i

A

H(l−1)

i W(l−1))

(3)

(cid:48)

(cid:48)

where ˆDi ∈ RN ×N is the degree matrix of A
i = Ai + IN ,
A
i is the subgraph adjacency matrix with self-loop, IN is the
identity matrix and W0 ∈ RF ×d is a learnable weight matrix.
Since the target node is masked in the subgraph, we employ
the weight matrix of GCN to map the features into the same
embedding space. It can be formulated as:

(4)

i = φ(hl−1
hl

i Wl−1)
where h0
i = xi, φ is the activation function such as ReLU.
So we obtain the representations Hi and ˜Hi for the two-view
subgraphs Gi, ˜Gi, hi for the target node vi. In order to ap-
ply the node-subgraph matching pattern pretext task (the rep-
resentation of node and its corresponding subgraph is more
consistent), we take the average pooling function as the read-
out module to obtain the subgraph-level embedding vector ei:

ei = Readout(Hi) =

ni(cid:88)

k=1

(Hi)k
ni

(5)

where ni is the number of nodes in the subgraph Gi. So the
discriminative scores for the positive pair can be deﬁned as:

si = σ(hiWseT
i )

(6)

where Ws is a learnable matrix. Similarly, we can calculate
˜si for the nagetive pair. Hence, the local view contrastive loss
for the instance pair (vi, Gi, ˜Gi) can be formulated as:
1
2

(log(si) + log(1 − ˜si))

intra(vi) = −

L1

(7)

Besides, we can similarly calculate L2
intra for the global view.
Finally, by combining the two losses, we have the intra-view
contrastive learning objective function deﬁned below:

Lintra =

1
2N

N
(cid:88)

i=1

(L1

intra(vi) + L2

intra(vi))

(8)

Inter-View Contrastive Learning. This contrastiveness is
deﬁned between the two views. Following the core idea in
contrastive learning, we make the discriminative scores from
node-subgraph pair of the two views closer. So the inter-view

contrastive loss between the local- and global-view can be
formulated as:

Linter = (||s1 − s2||2
(9)
where s1, s2 are the vectors that consist of positive pair dis-
criminative scores in the two views. By combining the intra-
and the inter-view contrastive learning, the overall loss func-
tion of the multi-view contrastive learning module is:

F )

Lcon = Linter + Lintra

(10)

4.3 Attribute Reconstruction Based on Neighbors
The self-supervised learning method based on masked au-
toencoders (MAE) has been proved to be effective in many
ﬁelds. For example, the BERT [Devlin et al., 2018]-based
pre-training model has achieved remarkable results in many
NLP tasks, and similar works are also proposed in CV [He et
al., 2021]. The basic idea of these methods is to mask a part
of a sentence or a picture and then input the unmasked part
into the encoder and decoder for predicting the masked part.
Motivated by the masked autoencoder method, we propose
to design an attribute reconstruction based on neighbors mod-
ule to boost the performance. We adopt an asymmetric de-
sign. The encoder operates on the nodes in the subgraph that
are not masked. The latent representations of these nodes are
concatenated as the input of the lightweight decoder to re-
construct the masked node’s raw attributes. Taking the local
view as an example, for the subgraph Gi of node vi, we can
get the representation of Gi through the GCN encoder. The
reconstruction loss of the local view can be deﬁned as:

L1

res(vi) = ||g(Zi) − xi||2
(11)
where g(·) is the multilayer perceptron, Zi is the concatena-
tion of neighboring nodes’ representations in the subgraph.
Similarly, we can calculate L2
res for the global view. The
overall loss function of this module can be formulated as:

Lres =

1
2N

N
(cid:88)

i=1

(L1

res(vi) + L2

res(vi))

(12)

4.4 Anomaly Score Inference
To jointly train the contrastive learning module and the at-
tribute reconstruction module, we optimize the following ob-
jective function:

L = Lcon + γLres
(13)
where γ is a controlling parameter which balances the impor-
tance of the two modules.

By minimizing the above objective function, we can com-
pute the anomaly score of each node. Inspired by [Liu et al.,
2021b], taking the node vi as an example, the anomaly score
of the contrastive-based module can be calculated by:

scorecon(vi) =

[score1

1
2
con(vi) = s1−
(s2−
i

con(vi) + score2
, score2

con(vi)]
con(vi) = s2−

(14)

i −s1+
i −s2+
where score1
,
s1+
(s2+
) are the local (global) view discrim-
i
i
inantive scores of positive and negative pairs by Eq.(6). The
anomaly score from the reconstruction-based module is:

) and s1−

i

i

i

scoreres(vi) =

1
2

2
(cid:88)

k=1

scorek

res(vi)

(15)

Datasets

Nodes

Edges

Features Anomalies

BlogCatalog
Flickr
Cora
CiteSeer
Pubmed

5196
7575
2708
3327
19717

171743
239738
5429
4732
44338

8189
12407
1433
3703
500

300
450
150
150
600

Table 1: The statistics of the datasets.

i ) − xi||2

scorek
i and Z2

res(vi) = ||g(Zk
where Z1
i are the concatenated neighboring node rep-
resentations in local and global subgraphs for node vi, re-
spectively. The two scores are ﬁrst normalized, and then in-
tegrated by the following formula:

2, k = 1, 2

(16)

score(vi) = scorecon(vi) + γscoreres(vi)

(17)

5 Experiments
5.1 Datasets
We evaluate the proposed model on ﬁve benchmark datasets
that are widely used in anomaly detection on attributed
networks [Liu et al., 2021b; Zheng et al., 2021]. These
datasets include two social network datasets BlogCatalog and
Flickr and three citation network datasets Cora, Citeseer, and
Pubmed. Due to the shortage of ground truth anomalies, the
anomalies are injected by the perturbation scheme [Ding et
al., 2019]. The statistics of the datasets is shown in Table 1.

5.2 Experimental Settings
Baselines and evaluation metrics.
In the experiments, we
compare the proposed model with seven baselines. [Perozzi
and Akoglu, 2016] evaluates the attribute correlation of node
to detect anomalies. Speciﬁcally, it analyzes the abnormal-
[Li
ity of each node from the ego-network point of view.
et al., 2017] is a residual analysis-based method. It detects
anomalies whose behaviors are singularly different from the
majority [Peng et al., 2018] proposes a novel joint frame-
work to conduct attribute selection and anomaly detection.
[Ding et al., 2019] conducts the investigation on the problem
of anomaly detection on attributed networks. [Velickovic et
al., 2019] is an approach for learning node representations.
A bilinear function is provied to score abnormality. [Liu et
al., 2021b] is the ﬁrst contrastive learning based method for
graph anomaly detection. [Jin et al., 2021] is a recent method
which performes patch- and context-level contrastive learning
for anomaly detection on attributed networks. We use ROC
and AUC to measure the performance.
Implementation Details.
In Sub-CR, we set the node size
P of subgraph to 4 for all datasets by considering both the
efﬁciency and performance. Following [Liu et al., 2021b], we
employ a one-layer GCN as the encoder, and the embedding
dimension is set to 64. The model is optimized with the Adam
optimizer during training. The bacth size is set to 300 for all
datasets. The learning rate is set to 0.001 for Cora, Citeseer,
Pubmed and Flickr, set to 0.003 for BlogCatalog. We train
the model 400 epochs for BlogCatalog and Flickr, and 100

Figure 2: ROC curves on four benchmark datasets.

epochs for Cora, Citeseer and Pubmed. The parameter γ is
set to 0.6 for BlogCatalog, Flickr, Cora and Citeseer and 0.4
for Pubmed. In the inference phase, we average the anomaly
scores in Eq. (17) over 300 rounds to get the ﬁnal anomaly
score for each node.

5.3 Result and Analysis
Table 2 shows the comparison result of these methods in
terms of AUC value. Due to space limitation, Figure.2 shows
the ROC curves on Cora, BlogCatalog, Citeseer, and Flickr
with two competitive baselines. As shown in the table, one
can ﬁrst see that our proposed model is superior to its coun-
terparts on ﬁve benchmark datasets, which demonstrates the
effectiveness of Sub-CR. To be speciﬁc, Sub-CR improves
the performance by 3.38% and 1.61% over the best base-
line ANEMONE on Flickr and Pubmed datasets, respectively.
In details, the shallow methods including AMEN, Radar and
ANOMALOUS perform worse than other models due to their
less effectiveness in modeling the high-dimension node at-
tributes and complex tological structures. The DOMINANT
and DGI also do not show the competitive performance be-
cause they focus on modeling the whole graph structure in-
stead of directly explore the abnormal patterns. DGI takes
the node vs full-graph intstance pair contrastive, and DOM-
INANT aims to reconstruct the whole graph structure or at-
tributes for each node, which do not decouple local and global
information for anomaly detection. CoLA and ANEMONE
achieve the suboptimal performance due to the considera-
tion of sorrounding substructures. But they cannot fully
catch the high-order structure information and ignore the self-
supervised signal of the original attribute reconstruction er-
ror. The superior performence of Sub-CR veriﬁes the effec-
tiveness of intergating the multi-view contrastive-based and
reconstruction-based modules, which can decouple local and
global information to better capture the attribution and the
topological structure of nodes for anomaly detection.

5.4 Ablation Study
We next conduct an ablation study to verify the effective-
ness of each component in Sub-CR. Sub-R and Sub-C de-
note the model without reconstruction-based and contrastive-
based module, respectively. The variants Sub-weight and
Sub-global are deﬁned as that we remove the balance weight (
γ is set to 1) and the global view, respectively. The results are
presented in Table 3. One can see that Sub-CR outperforms
all the variants consistently on all the datasets, which demon-
strates the components are all helpful to the studied task.
Especially, Sub-CR outperforms Sub-C by 6.81%, 5.41%,

Methods

Blogcatalog

AMEN[2016]
Radar[2017]
ANOMALOUS [2018]
DOMINANT[2019]
DGI [2019]
CoLA[2021]
ANEMONE [2021]

Sub-CR

0.6392
0.7401
0.7237
0.7468
0.5827
0.7854
0.8067

0.8141

Flickr

0.6573
0.7399
0.7434
0.7442
0.6237
0.7513
0.7637

0.7975

Cora

0.6266
0.6587
0.5770
0.8155
0.7511
0.8779
0.9057

0.9132

Citeseer

Pubmed

0.6154
0.6709
0.6307
0.8251
0.8293
0.8968
0.9189

0.9303

0.7713
0.6233
0.7316
0.8081
0.6962
0.9512
0.9548

0.9709

Table 2: The AUC values comparison on ﬁve benchmark datasets.

BlogCatalog

Flickr

Cora

CiteSeer

Pubmed

Sub-R
Sub-C
Sub-weight
Sub-global

0.7943
0.7460
0.8083
0.8090

0.7609
0.7434
0.7928
0.7923

0.9002
0.8220
0.9041
0.8975

0.9017
0.7892
0.9275
0.9195

0.9553
0.8006
0.9491
0.9625

Table 3: The AUC values of ablation study.

Figure 4: Performance with different subgraph sizes and embedding
dimensions.

achieved at P = 2, and for BlogCatalog, Cora and PubMeb
it is achieved at P = 4. When the size is too large, the sub-
graph will contain redundant information, which will hurt the
performance. A suitable embedding dimension is d = 64 for
most datasets as shown in the ﬁgure. A too small or large em-
bedding dimension will both degrade the model performance.

6 Conclusion

In this paper, we propose a novel multi-view self-supervised
learning framework for anomaly detection on attributed net-
works. The proposed contrastive learning-based module con-
sists of two carefully designed contrastive views to better cap-
ture local and global structure information related to anomaly
patterns. The attribute reconstruction module adopts the rep-
resentation of those neighbors based on the subgraph to re-
build the raw attributes of the target node of the two views.
Finally, the two complementary modules are integrated for
more effective anomaly detection. Extensive experiments
conducted over ﬁve benchmark datasets demonstrate the ef-
fectiveness of our proposed model.

Acknowledgements

This work was supported by NSFC under granted No.
62076124, partially ﬁnancially supported by the National Sci-
ence and Technology Major Project (J2019-IV-0018-0086),
NSFC (No. 62172443), and the Fundamental Research Funds
for the Central Universities (No. NZ2020014).

Figure 3: Performance with different γ.

9.12%, 14,11%, 17.03% on ﬁve datasets, respectively, which
demonstrates the contrastive-based module is crucially im-
portant to the model. The self-supervised signal of the match-
ing pattern on the node-subgraph pair is more effective for
capturing anomalies. As for the remaining three components,
Sub-R performs worse than Sub-weight and Sub-global on
BlogCatalog, Flickr, and CiteSeer datasets. On the Cora, the
component of the global view is more important. Sub-weight
has a more signiﬁcant effect on Pubmed. The reason may be
that the effect of different modules on different datasets varies
due to the different data characteristics.

5.5 Parameter Study
We ﬁnally conduct the model sensitivity analysis on critical
hyper-parameters in Sub-CR, which are the size P of sub-
graph, the embedding dimension in GCN and the balance fac-
tor γ. Figure.3 shows the effect of different γ values on the
model performance. One can see that the most datasets can
achieve the best result and not sensitive to the balance factor
when γ ≥ 0.4. Figure.4 shows the AUC values under dif-
ferent subgraph sizes and embedding dimensions. One can
observe that the best performance for Flickr and Citeseer is

758085909510000.20.40.60.8AUC（%）FlickrBlogCatalogPubMedCoraCiteseer758085909510023456AUC（%）Subgraph size BlogCatalogCiteSeerPubMedFlickrCora7580859095100163264128256AUC（%）Embedding dimension BlogCatalogCiteSeerPubMedFlickrCoraReferences
[Bo et al., 2021] Deyu Bo, Xiao Wang, Chuan Shi, and
Huawei Shen. Beyond low-frequency information in graph
convolutional networks. In AAAI, 2021.

[Chen et al., 2020] Ting Chen, Simon Kornblith, Moham-
mad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In ICML,
2020.

[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-
ton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805, 2018.

[Ding et al., 2019] Kaize Ding,

Rohit
Bhanushali, and Huan Liu. Deep anomaly detection
on attributed networks. In SDM, 2019.

Jundong Li,

[Ding et al., 2021] Kaize Ding, Jundong Li, Nitin Agarwal,
and Huan Liu. Inductive anomaly detection on attributed
networks. In IJCAI, 2021.

[Erfani et al., 2016] Sarah M Erfani,

Sutharshan Ra-
jasegarar, Shanika Karunasekera, and Christopher Leckie.
High-dimensional and large-scale anomaly detection
using a linear one-class svm with deep learning. Pattern
Recognition, 58:121–134, 2016.

[Hassani and Khasahmadi, 2020] Kaveh

Amir Hosein Khasahmadi.
representation learning on graphs. In ICML, 2020.

Hassani
and
Contrastive multi-view

[He et al., 2021] Kaiming He, Xinlei Chen, Saining Xie,
Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked
autoencoders are scalable vision learners. arXiv preprint
arXiv:2111.06377, 2021.

[Jain et al., 2019] Gauri Jain, Manisha Sharma, and Basant
Agarwal. Spam detection in social media using convolu-
tional and long short term memory neural network. An-
nals of Mathematics and Artiﬁcial Intelligence, 85(1):21–
44, 2019.

[Jin et al., 2021] Ming Jin, Yixin Liu, Yu Zheng, Lianhua
Chi, Yuan-Fang Li, and Shirui Pan. Anemone: Graph
anomaly detection with multi-scale contrastive learning. In
CIKM, 2021.

[Klicpera et al., 2019] Johannes Klicpera, Stefan Weißen-
berger, and Stephan G¨unnemann. Diffusion improves
graph learning. arXiv preprint arXiv:1911.05485, 2019.
[Li et al., 2017] Jundong Li, Harsh Dani, Xia Hu, and Huan
Liu. Radar: Residual analysis for anomaly detection in
attributed networks. In IJCAI, pages 2152–2158, 2017.

[Li et al., 2019] Yuening Li, Xiao Huang,

Jundong Li,
Mengnan Du, and Na Zou. Specae: Spectral autoencoder
In CIKM,
for anomaly detection in attributed networks.
2019.

[Liu et al., 2021a] Xiao Liu, Fanjin Zhang, Zhenyu Hou,
Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-
IEEE
supervised learning: Generative or contrastive.
Transactions on Knowledge and Data Engineering, 2021.

[Liu et al., 2021b] Yixin Liu, Zhao Li, Shirui Pan, Chen
Gong, Chuan Zhou, and George Karypis. Anomaly detec-
tion on attributed networks via contrastive self-supervised
IEEE Transactions on Neural Networks and
learning.
Learning Systems, 2021.

[Page et al., 1999] Lawrence Page, Sergey Brin, Rajeev
Motwani, and Terry Winograd. The pagerank citation
ranking: Bringing order to the web. Technical report, Stan-
ford InfoLab, 1999.

[Peng et al., 2018] Zhen Peng, Minnan Luo, Jundong Li,
Huan Liu, and Qinghua Zheng. Anomalous: A joint mod-
eling approach for anomaly detection on attributed net-
works. In IJCAI, 2018.

[Peng et al., 2020] Zhen Peng, Wenbing Huang, Minnan
Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Jun-
zhou Huang. Graph representation learning via graphical
mutual information maximization. In WWW, 2020.

[Perozzi and Akoglu, 2016] Bryan Perozzi

and Leman
Akoglu. Scalable anomaly ranking of attributed neighbor-
hoods. In SDM, 2016.

[Shone et al., 2018] Nathan Shone, Tran Nguyen Ngoc,
Vu Dinh Phai, and Qi Shi. A deep learning approach
IEEE transactions on
to network intrusion detection.
emerging topics in computational intelligence, 2(1):41–
50, 2018.

[Tong et al., 2006] Hanghang Tong, Christos Faloutsos, and
Jia-Yu Pan. Fast random walk with restart and its applica-
tions. In ICDM, 2006.

[Velickovic et al., 2019] Petar Velickovic, William Fedus,
William L Hamilton, Pietro Li`o, Yoshua Bengio, and
R Devon Hjelm. Deep graph infomax. In ICLR, 2019.
[Wang et al., 2019] Daixin Wang, Jianbin Lin, Peng Cui,
Quanhui Jia, Zhen Wang, Yanming Fang, Quan Yu, Jun
Zhou, Shuang Yang, and Yuan Qi. A semi-supervised
graph attentive network for ﬁnancial fraud detection.
In
ICDM, 2019.

[Yu et al., 2018] Wenchao Yu, Wei Cheng, Charu C Aggar-
wal, Kai Zhang, Haifeng Chen, and Wei Wang. Netwalk:
A ﬂexible deep embedding approach for anomaly detec-
tion in dynamic networks. In KDD, 2018.

[Zheng et al., 2021] Yu Zheng, Ming Jin, Yixin Liu, Lianhua
Chi, Khoa T Phan, and Yi-Ping Phoebe Chen. Generative
and contrastive self-supervised learning for graph anomaly
detection. IEEE Transactions on Knowledge and Data En-
gineering, 2021.

[Zhu et al., 2020] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang
Liu, Shu Wu, and Liang Wang. Deep graph contrastive
representation learning. arXiv preprint arXiv:2006.04131,
2020.

[Zhu et al., 2021] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang
Liu, Shu Wu, and Liang Wang. Graph contrastive learning
with adaptive augmentation. In WWW, 2021.

