Epistemic Signaling Games
for Cyber Deception with Asymmetric Recognition

Hampei Sasahara and Henrik Sandberg

1
2
0
2

y
a
M
1
1

]

R
C
.
s
c
[

2
v
7
8
2
3
0
.
3
0
1
2
:
v
i
X
r
a

Abstract— This study provides a model of cyber deception
with asymmetric recognition represented by private beliefs.
Signaling games, which are often used in existing works,
are built on the implicit premise that the receiver’s belief is
public information. However, this assumption, which leads to
symmetric recognition,
is unrealistic in adversarial decision
making. For a precise evaluation of risks arising from cognitive
gaps, this paper proposes epistemic signaling games based on
the Mertens-Zamir model, which explicitly quantiﬁes players’
asymmetric recognition. Equilibria of the games are analytically
characterized with an interpretation.

I. INTRODUCTION

Cyber deception [1], [2], which can obscure important data
such as customer information or system architecture, is an
emerging defense technology. Examples of cyber deception
include honeypots [3], moving target defense [4], and ob-
fuscation [5]. Game theory offers mathematical models for
strategic decision making [6]–[11]. In particular, signaling
games are often used for representing asymmetric players’
knowledge, which arises especially in cyber deception [12]–
[14]. Signaling games are two-player games between a
sender and a receiver, in which the sender’s type is not known
to the receiver. At the beginning of the game, the receiver
forms her prior belief on the sender’s type. Subsequently,
the sender transmits her message, and the receiver updates
her belief according to the message and chooses her action.
Using signaling games, reasonable actions of the attacker and
the defender can mathematically be represented as equilibria.
The consequences of the game, such as the attack’s impact
and the deployed defense strategy’s effectiveness, can be
assessed in a quantitative manner by analyzing the equilibria.
An implicit assumption of traditional signaling games is
that there exists a common prior, i.e., the receiver’s prior
belief is public information. In the context of cyber decep-
tion, this assumption means that the defender exactly knows
what the attacker believes. Moreover, the attacker knows that
the defender knows the attacker’s belief. This process repeats
indeﬁnitely, and their mutual beliefs are shared. In this sense,
the players’ recognition is symmetric in traditional signaling
games. However, this assumption is obviously unrealistic.
As suggested by behavioral economics, recognition plays an
important role in human’s decision making [15], and we may

Fig. 1. Example of cyber deception: the defender may deploy a honeypot,
spending cost for its disguise, while the attacker decides whether to execute
an intrusion by analyzing the available information.

underestimate security risks without an adequate model that
can describe asymmetric recognition.

This study proposes epistemic signaling games to re-
solve this issue. The problem of asymmetric recognition has
been pointed out in the general context of economics, and
the Mertens-Zamir model has been proposed to represent
asymmetric recognition in epistemic game theory [16]–[18].
Using this model, we incorporate asymmetric recognition in
signaling games. We characterize equilibria of the proposed
epistemic signaling games with an interpretation.

This paper is organized as follows. In Sec. II, we brieﬂy re-
view traditional signaling games. Sec. III provides epistemic
signaling games and analyze the equilibria. Finally, Sec. IV
draws the conclusion. The appendix contains the proofs.

II. BRIEF REVIEW: TRADITIONAL SIGNALING GAMES

A. Example: Cyber Deception using a Honeypot

An example is unauthorized access to a workstation that
may be a honeypot, which is a system placed on a network
to attract the attention of attackers. A honeypot does not
store any valuable data and collects information about the
intruder’s identity by alluring attackers [3]. Once an attacker
compromises a honeypot, the defender analyzes the adversar-
ial actions in detail and utilizes the information to improve
the network protection. The adversarial decision making in
the honeypot example is illustrated by Fig. 1. The defender
may deploy a honeypot, while the attacker may be able
to identify the system by analyzing information caused by
unusual behavior [19], [20]. In this example, the sender is
a model of the defender who decides whether to spend cost
for disguising a honeypot, while the receiver is a model of
the attacker who decides whether to execute an intrusion by
analyzing the information.

This work was supported by Swedish Research Council grant 2016-

00861.

B. Traditional Signaling Game Model

H. Sasahara and H. Sandberg are with Division of Decision and
Control Systems, School of Electrical Engineering and Computer Sci-
ence, KTH Royal Institute of Technology, Stockholm SE-100 44, Sweden
{hampei,hsan}@kth.se

In signaling games, the sender’s private information is
referred to as its type, which is denoted by θs ∈ Θ s. For
simplicity, we assume the type to be binary, i.e., Θ s =

legitimate systemserver to be attackedhoneypot1. deployment2. analysis of information3. decision on intrusionor 
 
 
 
 
 
0, θs

1}. For example, θs

{θs
0 and θs
1 represent the legitimate
system and the honeypot, respectively. The true type is
known to the sender but unknown to the receiver.

Given the type, the sender chooses a message m ∈ M,
which is assumed to be binary, i.e., M = {m0, m1}. We
refer to mi as an honest message when the type is θs
i and
the other as a deceptive message for any i ∈ {0, 1}. We
consider mixed strategies and denote the sender’s strategy by
σs ∈ S s such that σs(m|θs) gives the probability with which
the sender sends the message m when her type is θs. We
refer to m ∈ M as an on-path message when σs(m|θs) > 0
for some θs ∈ Θ s, and as an off-path message otherwise. In
the honeypot example, m represents the service provided by
the server. The defender’s choice of m is a decision whether
to disguise the system’s behavior and to pay a cost, or not.
An example of the disguise is to replace a cheap honeypot
that provides only an open service port with a sophisticated
one that provides full functional support [20].

After receiving the message, the receiver chooses an action
a ∈ A, which is assumed to be binary, i.e., A = {a0, a1}. In
the honeypot example, a0 and a1 represent execution of the
intrusion and withdrawal, respectively. The receiver’s mixed
strategies are denoted by σr ∈ S r.

Traditional signaling games assume existence of a common
prior on the type, i.e., the type is determined by nature ac-
cording to a probability distribution over Θ s, which is known
to both players, at the beginning of the game. The determined
type is informed to the sender and the receiver updates her
belief based on the prior distribution and the transmitted
message. The prior and posterior beliefs are denoted by
πr(θs) and πr(θs|m), respectively. In the next section we
will revisit and discuss this assumption on existence of a
common prior, which is standard in Harsanyi’s incomplete
information games [18].

Let us : Θ s × M × A → R denote a utility function for
the sender. Similarly, let ur : Θ s × A → R denote a utility
function for the receiver. Note that the receiver’s utility is
independent of the message because the messaging cost is
not owned by the receiver. Throughout this paper, we assume
that the utilities satisfy
(cid:26)us(θs
0, m, a0) < us(θs
1, m, a1) < us(θs
us(θs

0, a1),
1, a0),
for any m ∈ M. This assumption means that, the attacker
0 and a1 if θs = θs
prefers a0 if θs = θs
1, and the defender
prefers the opposite if the message is ﬁxed. Moreover, we
also assume that the sender’s utility is symmetric with respect
to her type for honest messaging, i.e., us(θs
0, m0, a0) =
us(θs
1, m1, a0) to sim-
plify the results. Without this assumption, similar results are
available by dividing the cases.

0, a0) > ur(θs
1, a1) > ur(θs

1, m1, a1) and us(θs

0, m0, a1) = us(θs

0, m, a1),
1, m, a0),

(cid:26)ur(θs
ur(θs

We suppose that deceptive messaging requires a cost. The

sender’s utility function is assumed to be represented as

(cid:26) us(θs
us(θs

0, m1, a) = us(θs
1, m0, a) = us(θs

0, m0, a) − C,
1, m1, a) − C,

∀a ∈ A

where C > 0 is the deceptive messaging cost. In the honey-
pot example, C may represent the cost for disguising the hon-

(a) Symmetric recognition in the honeypot example: The attacker believes
that the system of interest is legitimate with probability 0.8 and it is a
honeypot with probability 0.2. The defender knows the attacker’s belief,
and the attacker knows that the defender knows the attacker’s belief. This
is the model used in the traditional signaling games.

(b) Asymmetric recognition in the honeypot example: The attacker
believes that the system of interest is legitimate with probability 0.2 and it
is a honeypot with probability 0.8. On the other hand, the defender believes
that the attacker believes that the system of interest is legitimate with
probability 0.8 and it is a honeypot with probability 0.2 with probability
0.3 and that the attacker believes the system of interest is legitimate with
probability 0.2 and it is a honeypot with probability 0.8 with probability
0.7. The attacker knows the defender’s belief on the attacker’s belief. This
is the proposed model used in the epistemic signaling games.

Fig. 2. Symmetric and asymmetric recognitions in the honeypot example.

eypot as a legitimate system. This disguise can be achieved
by replacing a cheap honeypot with a sophisticated, but
expensive, one that can provide full functional support [20],
for example. Signaling games without deceptive messaging
cost are referred to as cheap-talk signaling games [12]. Our
analysis can be extended to the cheap-talk case.

Reasonable strategies in signaling games are perfect
Bayesian equilibria (PBE) where the players maximize their
expected utilities and the receiver’s belief is rationally up-
dated according to Bayes’ rule. By investigating resulting
PBE, as studied in [12], [13], we can assess security risks in
a quantitative manner.

III. EPISTEMIC SIGNALING GAMES WITH ASYMMETRIC
RECOGNITION
A. Implicit Assumption on Recognition in Traditional Model

An important feature of traditional signaling games is
the existence of a common prior, under which the players
precisely know what the opponent believes. In other words,
the existence of a common prior implicitly assumes sym-
metric recognition. However, in the security domain, the
attacker’s belief is not necessarily shared by the players, i.e.,
the attacker and the defender may possess asymmetric recog-
nition. Figs. 2a and 2b illustrate symmetric and asymmetric
recognitions in the honeypot example, respectively.

We brieﬂy review the existing approach to resolving this
issue used in the general context. The most fundamental
notion is belief hierarchy, which has been introduced in epis-
temic game theory [16]–[18]. A belief hierarchy is formed
as follows. Let ∆(X ) denote the set of probability measures
on X . The ﬁrst-order belief is given as π1 ∈ ∆(Θ s), which
describes the attacker’s belief on the system architecture.

legitimate system: 80%honeypot: 20%attackerdefenderattackerlegitimate system: 80%honeypot: 20%defender: 30%: 70%attackerlegitimate system: 20%honeypot: 80%legitimate system: 20%honeypot: 80%The second-order belief is given as π2 ∈ ∆(∆(Θ s)), which
describes the defender’s belief on the attacker’s ﬁrst-order
belief. In a similar manner, the belief at any level is given,
and the tuple of the beliefs at all levels is referred to as a
belief hierarchy.

To handle incomplete information games without common
prior and the resulting belief hierarchy, the Mertens-Zamir
model has been introduced [16]–[18]. The model considers
type structure, in which a belief hierarchy is embedded. A
type structure consists of players, sets of types, and beliefs.
In particular, for signaling games, a type structure can be
given by

T = ((s, r), (Θ s, Θ r), (πs, πr))

(1)

where (s, r) represents the sender and the receiver, Θ s and
: Θ r ×
Θ r represent the sets of players’ types, and πs
Θ s → [0, 1] and πr
: Θ s × Θ r → [0, 1] represent the
beliefs. The value πs(θr|θs) denotes the sender’s belief of
the receiver’s type θr when the sender’s type is θs, and
πr(θs|θr) denotes the corresponding receiver’s belief. The
ﬁrst-order belief is given by π1(θs) = πr(θs|θr) for the true
receiver’s type θr ∈ Θ r, and the second-order belief is given
by π2(πr(·|θr)|θs) = πs(θr|θs) for the true sender’s type
θs ∈ Θ s as long as there is a one-to-one correspondence
between the receiver’s types and her beliefs. The higher-
order beliefs are illustrated in Fig. 2b. In a similar manner,
the belief at any level of the belief hierarchy can be derived
from the type structure. An important fact is that, for any
reasonable belief hierarchy there exists a type structure that
can generate the belief hierarchy of interest [16]–[18]. In this
sense, the Mertens-Zamir model has the sufﬁcient capability
of describing any situation with asymmetric recognition. For
a formal discussion, see [17], [18].

B. Epistemic Signaling Games with Asymmetric Recognition

In this subsection, we propose epistemic signaling games
using the Mertens-Zamir model for describing adversarial
decision making with asymmetric recognition.

sys × Θ s

sys and Θ s

rec where Θ s

In the honeypot example, the sender’s type set is given by
Θ s = Θ s
rec represent the sets
of the system’s attribute, namely, a legitimate system or a
honeypot, and of the defender’s recognition, respectively. For
simplicity, we assume that Θ s
rec is singleton, i.e., there is only
one possible defender’s recognition. This implies that the
defender’s recognition is public information. In this sense,
this assumption corresponds to the worst case where the
defender’s belief is perfectly known to the attacker. Under
this assumption, the sender’s belief is independent of her
type. We denote the sender’s belief by πs(θr) instead of
πs(θr|θs). We also assume Θ s and Θ r in (1) to be binary,
i.e., Θ s := {θs

1} and Θ r := {θr

0, θs

0, θr

1}.

The contrasting ingredients of the traditional and our
epistemic signaling games are listed in Table I. Accordingly,
the sender’s expected utility becomes

¯us(σs, σr|θs)
(cid:88)
:=

a∈A,m∈M,θr∈Θ r

σr(a|m, θr)πs(θr)σs(m|θs)us(θs, m, a),

TABLE I
CONTRASTING INGREDIENTS OF EPISTEMIC SIGNALING GAMES

receiver’s strategy
sender’s belief
receiver’s belief

Traditional
σr(a|m)
none
πr(θs)

Epistemic
σr(a|m, θr)
πs(θr|θs)
πr(θs|θr)

which depends on the sender’s belief in contrast
to the
traditional one. Similarly, the receiver’s expected utility is
given by

¯ur(σr|m, θr) :=

(cid:88)

σr(a|m, θr)πr(θs|m, θr)ur(θs, a),

a∈A,θs∈Θ s

0) > ¯ur(a1|θr

where πr(θs|m, θr) denotes the posterior belief. For simplic-
0 and θr
ity, we assume that the receivers with the type θr
1
prefer a0 and a1, respectively, when the sender’s strategy is
independent of her type, i.e., the message does not possess
any information on sender’s type. In a mathematical form, we
1) < ¯ur(a1|θr
assume ¯ur(a0|θr
1)
where ¯ur(a|θr) := (cid:80)

0) and ¯ur(a0|θr
θs∈Θ s πr(θs|θr)ur(θs, a).
The solution concept is deﬁned as follows.
Deﬁnition 1: A PBE of the epistemic signaling game is a
strategy proﬁle (σs∗, σr∗) and posterior belief πr(θs|m, θr)
such that
(cid:26) σs∗ ∈ arg maxσs∈S s ¯us(σs, σr∗|θs),
σr∗ ∈ arg maxσr∈S r ¯ur(σr|m, θr),

∀θs ∈ Θ s,
∀m ∈ M, ∀θr ∈ Θ r

and

πr(θs|m, θr) =

σs∗(m|θs)πr(θs|θr)
φs∈Θ s σs∗(m|φs)πr(φs|θr)

(cid:80)

if (cid:80)

φs∈Θ s σs∗(m|φs)πr(φs|θr) (cid:54)= 0.

0) = σs(m|θs

There are three categories of PBE: separating, pool-
ing, and partially-separating PBE in traditional signaling
games [12], [13]. At separating PBE, the senders having
different types transmit opposite messages, i.e., σs(m|θs
0) =
1 − σs(m|θs
1) ∈ {0, 1} for any m ∈ M. At pooling PBE,
the sender’s strategies are independent of the type,
i.e.,
σs(m|θs
1) for any m ∈ M. Otherwise, the PBE
are referred to as partially-separating PBE. The separating
and pooling PBE describe two extreme cases. In the honeypot
example, the separating PBE mean that the legitimate system
always provides full service while the honeypot always
responds nothing. In contrast, the pooling PBE mean that the
system provides the same service regardless of its attribute,
although the honeypot may record the malicious intrusion.

C. Equilibrium Analysis of Epistemic Signaling Games

For the equilibrium analysis, we introduce some notation.

Deﬁne

(cid:26) ∆us
∆us

10(θs
10(θs

0) := us(θs
1) := us(θs
10(θs

0, m0, a1) − us(θs
1, m1, a1) − us(θs
1) < 0 < ∆us

0, m0, a0),
1, m1, a0),
10(θs

10(θs

0) + ∆us

which satisfy ∆us
0). Note that
∆us
1) = 0 holds from the assumption
on symmetry of the sender’s utility. We deﬁne the con-
stant C (cid:48)
0), which is the cost normalized

:= C/∆us

10(θs

10(θs

TABLE II
PARTIALLY-SEPARATING PBE OF EPISTEMIC SIGNALING GAMES

case (A): (i,vi)

case (B): (i,ii,iii,vi)

case (C): (iii,vi)

case (D): (i,iv,v,vi)

case (E): (i,ii,iii,iv,v,vi)

case (F): (iii,iv,v,vi)

case (G): (i,iv)

case (H): (i,ii,iii,iv)

case (I): (iii,iv)

10(θs

by ∆us
ur(θs
satisfy ∆ur

0). Moreover, deﬁne ∆ur

10(θs

0) := ur(θs

0, a0), and ∆ur
10(θs

1) := ur(θs
10(θs
1).
We ﬁrst characterize best responses to a given opponent’s

10(θs
0) < 0 < ∆ur

1, a1) − ur(θs

0, a1) −
1, a0), which

strategy.

Lemma 1: Deﬁne γs : S r → R and γr : S s × M × Θ r →

R by

γs(σr) := (cid:80)
γr(σs, m, θr) := (cid:80)

θr∈Θ r ∆σr
θs∈Θ s σs(m|θs)πr(θs|θr)∆ur

10(a1|θr)πs(θr),

10(θs),

(2)
10(a|θr) := σr(a|m1, θr) − σr(a|m0, θr). For
where ∆σr
a given receiver’s strategy σr, the sender’s best response
σs∗(m0|θs

0) is given by

σs∗(m0|θs

0) =






if γs(σr) < C (cid:48),
1
α if γs(σr) = C (cid:48),
otherwise,
0

Fig. 3. Regions of the sender’s belief. The partially-separating PBE are
determined from the region which the sender’s belief belongs to.

for any α ∈ [0, 1]. For θs
1, the best response is given as
the message opposite to the one by θs
0 when γs(σr) (cid:54)= C (cid:48)
and otherwise given as an arbitrary number in [0, 1]. For
a given sender’s strategy σs, the receiver’s best response
σr∗(a0|m, θr) is given by



if γr(σs, m, θr) < 0,
1
α if γr(σs, m, θr) = 0,
0

otherwise,



σr∗(a0|m, θr) =

when the message is on-path.

The separating PBE and pooling PBE are given as follows.
Theorem 1: If C ≥ ∆us
0), all PBE are separating PBE

10(θs

given by
(cid:26) σs∗(m0|θs

0) = σs∗(m1|θs
1) = 1,
σr∗(a0|m0, θr) = σr∗(a1|m1, θr) = 1,

∀θr ∈ Θ r

(3)

0|m0, θr) = πr(θs
with the posterior belief πr(θs
0) and πs(θr
10(θs
for any θr ∈ Θ r. If C < ∆us
exist pooling PBE characterized by

1|m1, θr) = 1
0) ≥ C (cid:48), there




∀θs ∈ Θ s,

σs∗(m0|θs) = 1,
(cid:26) σr∗(a0|m0, θr
σr∗(a1|m1, θr

0) = 1,
0)πs(θr

σr∗(a1|m0, θr
0) − σr∗(a0|m1, θr

1) = 1,
1)πs(θr



1) = C (cid:48)
(4)
10(θs
with a suitable off-path posterior belief. If C < ∆us
0)
1) ≥ C (cid:48), there exists PBE pooling at m1, which
and πs(θr
can be characterized in a similar manner. Furthermore, if
1) < C (cid:48), then the game admits no
πs(θs
pooling PBE.

0) < C (cid:48) and πs(θs

In Theorem 1, the claim on separating PBE implies that
the sender is always honest when the cost C is too high.
The other claim on pooling PBE implies that giving no
information can become a reasonable sender’s strategy when
the cost C is not too high.

Subsequently, we characterize partially-separating PBE.

As a preparation, we state the following lemma.

Lemma 2: The function γr deﬁned in (2) satisﬁes

γr(σs, m, θr

0) < γr(σs, m, θr

1),

∀σs ∈ S s, ∀m ∈ M (5)

and

(cid:26) γr(σs, m0, θr
γr(σs, m0, θr

0) + γr(σs, m1, θr
1) + γr(σs, m1, θr

0) < 0,
1) > 0,

∀σs ∈ S s.

(6)

Owing to Lemma 2, we can reduce the number of possible

cases.

Lemma 3: The sender’s optimal strategy σs∗ ∈ S s at any
0) < 0 <

partially-separating PBE satisﬁes γr(σs∗, m0, θr
γr(σs∗, m1, θr

1).

Using Lemma 3, we can characterize the partially-

separating PBE.

10(θs

Theorem 2: Assume C < ∆us

0). In epistemic signal-
ing games, there exist partially-separating PBE independent
of the sender’s belief characterized by

(cid:26) γr(σs∗, m0, θr

γr(σs∗, m0, θr

γr(σs∗, m1, θr
γr(σs∗, m1, θr

0) = 0,
1) > 0,

0) < 0,
1) = 0,

σs∗ :



σr∗ : γs(σr∗) = C (cid:48),

(7)
and the other partially-separating PBE dependent on the
sender’s belief are characterized by Table II, where the cases
are speciﬁcally given in Fig. 3 and the equilibrium candidates
are given in Table III.

An interpretation of the derived PBE can be given as
follows. First, the receiver’s strategy is determined from the
condition γs(σr∗) = C (cid:48) at any PBE, which means that
the receiver always balances the sender’s expected utilities
corresponding to the messages m0 and m1 independently
of her type. On the other hand, the sender’s equilibrium
strategy regions are illustrated in Fig. 4 where the solid
line segments depict the conditions γs(σs∗, m1, θs
0) = 0 and
γs(σs∗, m0, θs
1) = 0. The endpoints (1, 0) and (0, 1) corre-
spond to the pooling PBE, and the intersection corresponds
to the PBE independent of the sender’s belief given by (7).
It can be observed that the region at the PBE (i) and (vi)
are connected to that at the PBE pooling at m1. This PBE
is taken when the sender believes that the receiver’s type is
θr
1 with a high probability. In this sense, the PBE (i) and
(vi) are reasonable consequences when the sender believes
θr
1. Similarly, the PBE (iii) and (iv) can be reasonable when
the sender believes θr
0.

(A)(B)(C)(D)(E)(F)(G)(H)(I)(A)(D)(G)(B)(C)(E)(F)(H)(I)(i)(ii)(iii)(iv)(v)(vi)TABLE III
EQUILIBRIUM CANDIDATES












(i)

(iii)

(v)

σs∗ :

σr∗ :

σs∗ :

σr∗ :

(cid:26) γr(σs∗, m0, θr
γr(σs∗, m0, θr
(cid:26) σr∗(a0|m0, θr
σr∗(a1|m0, θr
(cid:26) γr(σs∗, m0, θr
γr(σs∗, m, θr
(cid:26) σr∗(a0|m0, θr
σr∗(a1|m, θr

0) < 0, γr(σs∗, m1, θr
1) = 0, γr(σs∗, m1, θr
0) = 1, σr∗(a1|m1, θr
1) = C(cid:48)

1, σr∗(a1|m1, θr

0) > 0,
1) > 0,
0) = 1,
1) = 1

0) < 0, γr(σs∗, m1, θr
1) > 0, ∀m ∈ M,
0) = 1, σr∗(a0|m1, θr
1) = 1, ∀m ∈ M

0) = 0,

0) = C(cid:48)
3,

σs∗ :

σr∗ :

σs∗ :

σr∗ :

(cid:26) γr(σs∗, m0, θr
γr(σs∗, m, θr
(cid:26) σr∗(a0|m0, θr
σr∗(a1|m, θr
(cid:26) γr(σs∗, m0, θr
γr(σs∗, m0, θr
(cid:26) σr∗(a0|m0, θr
σr∗(a0|m0, θr

0) < 0, γr(σs∗, m1, θr
1) > 0, ∀m ∈ M,
0) = 1, σr∗(a1|m1, θr
1) = 1, ∀m ∈ M

0) > 0,

0) = 1,

0) < 0, γr(σs∗, m1, θr
1) < 0, γr(σs∗, m1, θr
0) = 1, σr∗(a0|m1, θr
1) = 1, σr∗(a1|m1, θr

0) = 0,
1) > 0,
0) = C(cid:48)
4,
1) = 1

σs∗ :

(cid:26) γr(σs∗, m, θr
γr(σs∗, m0, θr
(cid:26) σr∗(a0|m, θr
σr∗(a0|m0, θr
1 := (1 − C(cid:48))/πs(θr

0) < 0, ∀m ∈ M,
1) < 0, γr(σs∗, m1, θr
0) = 1, ∀m ∈ M,
1) = 1, σr∗(a1|m1, θr
3 := 1 − C(cid:48)/πs(θr
1), C(cid:48)

σr∗ :


Constants: C(cid:48)

1) > 0,

1) = 1
0), C(cid:48)


4 := (1 − C(cid:48))/πs(θr

(cid:26) γr(σs∗, m, θr
0) < 0, ∀m ∈ M,
γr(σs∗, m0, θr
1) = 0, γr(σs∗, m1, θr
(cid:26) σr∗(a0|m, θr
0) = 1, ∀m ∈ M,
1) = C(cid:48)
σr∗(a1|m0, θr
6 := 1 − C(cid:48)/πs(θr
1)

6, σr∗(a1|m1, θr

σs∗ :

σr∗ :

0), C(cid:48)

1) > 0,

1) = 1












(ii)

(iv)

(vi)

0 and θr

Fig. 5 illustrates the transition of the sender’s equilibrium
strategy regions for increasing πs(θr
0). Starting with πs(θr
0)
close to zero, the resulting PBE are (i) and (vi). When
0) = C (cid:48) and C (cid:48) < 1/2, the PBE (ii) and (iii) are
πs(θr
additionally admitted. When πs(θr
0) slightly increases, the
possible strategy region switches from the PBE (i) to (iii)
through (ii). It can be observed that, for such a moderate
belief, the sender takes both θr
1 into account. When
πs(θr
0) increases more, the region switches again and reaches
the PBE (iii) and (iv). A similar transition can be observed
when C (cid:48) > 1/2. Note that the separating PBE at the top right
of Fig. 4 is the most honest strategy, and thus, the closer to
the bottom left the strategy is, the more deceptive it is. In this
sense, the strategies at the PBE (iii) can be regarded as more
deceptive than those at (iv), although both are reasonable
when the sender believes θr
0. Indeed, the PBE (iii) is taken
when the deception cost is low. A similar interpretation is
obtained for the PBE (i) and (vi). Finally, the PBE (ii) and
(v) “bridge” the other PBE.

IV. CONCLUSION

This study proposes epistemic signaling games, a novel
model of cyber deception with asymmetric recognition, based
on the Mertens-Zamir model. The equilibria are analytically
characterized.

APPENDIX

Proof of Lemma 1: With a slight abuse of notation, the
difference between the sender’s expected utilities using m0
and m1 is

¯us(m0, σr|θs
= − (cid:80)
= −γs(σr)∆us

0) − ¯us(m1, σr|θs
0)
0, m0, a)∆σr
a∈A,θr∈Θ r πs(θr)us(θs
10(θs

0) + C,

10(a|θr) + C

which leads to the given best response for θs
same criterion can be obtained for θs
1.

0. Similarly, the

For the receiver, with an on-path message m, we have

¯ur(a0|m, θr) − ¯ur(a1|m, θr)
= (cid:80)
θs∈Θ s πr(θs|m, θr)∆ur
= γr(σs, m, θr)/ (cid:80)

10(θs)

φs∈Θ s{σs(m|φs)πs(φs|θr)},

0 and θs

Fig. 4.
Possible equilibrium strategies of the sender, as given by
Theorem 2. The horizontal and vertical axes are the sender’s mixed strategies
of honest messaging for θs
1, respectively. The points (1, 1), (1, 0),
and (0, 1) are the separating PBE, and the PBE pooling at m0 and m1,
respectively. The red and blue lines depict the regions of sender’s strategies
that satisfy γs(σs∗, m0, θr
0) = 0, respectively.
The intersection is associated with the PBE in (7). The upper left segment of
the red line is associated with the PBE (i), and the lower right is associated
with the PBE (vi). The upper left segment of the blue line is associated
with the PBE (iii), and the lower right is associated with the PBE (iv).

1) = 0 and γs(σs∗, m1, θr

which leads to the given best response.

Proof of Theorem 1: Assume C ≥ ∆us

0), i.e., C (cid:48) ≥
1. Since γs(σr) ≤ 1 for any σr ∈ S r and θs ∈ Θ s, sending
an honest message is always optimal. Hence, the separating
PBE is given by (3).

10(θs

0) and πs(θr

Next, assume C < ∆us

0) < 0 and γr(σs, m0, θr

0) ≥ C (cid:48). Take
10(θs
the sender’s strategy σs(m0|θs) = 1 for any θs ∈ Θ s.
Then γr(σs, m0, θr
1) > 0. Hence
the receiver’s best response to σs for m0 is given by σr∗
in (4). Now σs becomes the best response to σr∗ if and
only if γs(σr∗) = C (cid:48), i.e., the off-path strategy satisﬁes the
equation in (4). There exists an off-path strategy that satisﬁes
0) ≥ C (cid:48). Therefore, the claim
the condition if and only if πs(θr
holds. For the case where C < ∆us
1) ≥ C (cid:48),
0) and πs(θr
the pooling PBE can be derived in a similar manner. Finally,
if both of πs(θr
1) are less than C (cid:48), then there
exist no off-path strategies that satisfy the necessary equation
in (4). Hence there exist no pooling PBE.

0) and πs(θr

10(θs

Proof of Lemma 2: We have ∆γr

10(σs, m) =

PBE (ii)Pool. PBE (at      )PBE (v)PBE (iv)PBE (i)PBE (iii)PBE (vi)PBE in (7)Pool. PBE (at      )Sep. PBEFig. 5.
line when πs(θr

Transition of the sender’s equilibrium strategies for increasing πs(θr

0). The sender’s strategies are given by the region depicted by the red solid

0) is small. As πs(θr

0) increases, the region shifts to the one depicted by the blue solid line.

(cid:80)
γr(σs, m, θr
πr(θs|θr
∆γr
10(σs, m)
= {σs(m|θs

(cid:124)

θs∈Θ s σs(m|θs)∆ur

10(θs)∆πr

10(θs) with ∆γr

1) − γr(σs, m, θr

0). Since ∆πr

10(θs

0) and ∆πr
10(θs

1) = −∆πr

10(θs) := πr(θs|θr
0), we have

10(σs, m) :=
1) −

1)∆ur
(cid:123)(cid:122)
>0

10(θs
1)
(cid:125)

−σs(m|θs
(cid:124)

0)∆ur
(cid:123)(cid:122)
>0

} ∆πr
10(θs
0)
(cid:124)
(cid:125)

10(θs
,
1)
(cid:125)
(cid:123)(cid:122)
>0

which leads to (5). Further, γr(σs, m0, θr)+γr(σs, m1, θr) =
¯ur(a1|θr) − ¯ur(a0|θr), which leads to (6).

Proof of Lemma 3: Since there exists m ∈ M and θs ∈
Θ s such that σs∗(m|θs) ∈ (0, 1), we have γs(σr∗) = C (cid:48).
If γr(σs∗, m0, θr
0) ≥ 0, then γr(σs∗, m1, θr
0) < 0 from (6).
Thus σr∗(a1|m1, θr
0) ≤ 0. In addition,
if γr(σs∗, m0, θr
1) > 0 from (5).
Thus σr∗(a1|m0, θr
1) ≤ 0. Therefore
γs(σr∗) ≤ 0 (cid:54)= C (cid:48), which leads to a contradiction. The other
claim can be proven in a similar manner.

0) ≥ 0, then γr(σs∗, m0, θr
10(a1|θr

1) = 1 and ∆σr∗

0) = 0 and ∆σr∗

10(a1|θr

Proof of Theorem 2: From Lemma 3, the possible
combinations of the criterion for the best response are given
1) (cid:82) 0
by the nine cases: γr(σs∗, m1, θr
with γs(σr∗) = C (cid:48), γr(σs∗, m0, θr
1) >
0.

0) (cid:82) 0, γr(σs∗, m0, θr
0) < 0, γr(σs∗, m1, θr

We ﬁrst show that the two cases
(cid:26) γr(σs∗, m1, θr
γr(σs∗, m1, θr

0) < 0
1) > 0

,

(cid:26) γr(σs∗, m1, θr
γr(σs∗, m1, θr

0) > 0
1) < 0

10(a1|θr

do not happen. Assume that the former one holds. Then
1) = 0, and hence γs(σr∗) = 0 <
∆σr∗
C (cid:48). Similarly, the latter one implies that γs(σr∗) = 1 > C (cid:48).
Those conditions lead to contradictions.

0) = ∆σr∗

10(a1|θr

For the other cases, the PBE and their existence conditions
are derived by a routine calculations. For example, for the
PBE (7), it sufﬁces to ﬁnd σs∗ that satisfy the equations.
This can be done using a standard linear algebra.

REFERENCES

[1] C. Wang and Z. Lu, “Cyber deception: Overview and the road ahead,”

IEEE Security & Privacy, vol. 16, no. 2, pp. 80–85, 2018.

[2] J. Pawlick, E. Colbert, and Q. Zhu, “A game-theoretic taxonomy and
survey of defensive deception for cybersecurity and privacy,” ACM
Computing Surveys, vol. 52, no. 4, 2019.

[3] T. H. Project, Know Your Enemy: Learning about Security Threats.

Addison-Wesley Professional, 2004.

[4] R. Zhuang, S. A. DeLoach, and X. Ou, “Towards a theory of moving
target defense,” in Proc. ACM Workshop on Moving Target Defense,
2014, p. 31–40.

[5] Q. Zhu, A. Clark, R. Poovendran, and T. Bas¸ar, “Deceptive routing
games,” in Proc. 51st Conference on Decision and Control, 2012, pp.
2704–2711.

[6] F. Farokhi, A. Teixeira, and C. Langbort, “Estimation with strategic
sensors,” IEEE Trans. Autom. Control, vol. 62, no. 2, pp. 724–739,
2017.

[7] S. Sarıtas¸, S. Y¨uksel, and S. Gezici, “Quadratic multi-dimensional
signaling games and afﬁne equilibria,” IEEE Trans. Autom. Control,
vol. 62, no. 2, pp. 605–619, 2017.

[8] E. Miehling, R. Dong, C. Langbort, and T. Bas¸ar, “Strategic inference
with a single private sample,” in Proc. 58th Conference on Decision
and Control, 2019, pp. 2188–2193.

[9] N. Heydaribeni and A. Anastasopoulos, “Linear equilibria for dynamic
LQG games with asymmetric information and dependent types,” in
Proc. 58th Conference on Decision and Control, 2019, pp. 5971–5976.
[10] Y. Nugraha, A. Cetinkaya, T. Hayakawa, H. Ishii, and Q. Zhu,
“Dynamic resilient network games considering connectivity,” in Proc.
59th Conference on Decision and Control, 2020, pp. 3779–3784.
[11] M. Pirani, J. A. Taylor, and B. Sinopoli, “Strategic sensor placement

on graphs,” Systems & Control Letters, vol. 148, 2021.

[12] T. E. Carroll and D. Grosu, “A game theoretic investigation of de-
ception in network security,” Security and Communication Networks,
vol. 4, no. 10, pp. 1162–1172, 2011.

[13] H. C¸ eker, J. Zhuang, S. Upadhyaya, Q. D. La, and B.-H. Soong,
“Deception-based game theoretical approach to mitigate DoS attacks,”
in Proc. Decision and Game Theory for Security, 2016, pp. 18–38.

[14] J. Pawlick, E. Colbert, and Q. Zhu, “Modeling and analysis of leaky
deception using signaling games with evidence,” IEEE Trans. Inf.
Forensics Security, vol. 14, no. 7, pp. 1871–1886, 2019.

[15] C. F. Camerer, T.-H. Ho, and J.-K. Chong, “A Cognitive Hierarchy
Model of Games*,” The Quarterly Journal of Economics, vol. 119,
no. 3, pp. 861–898, 2004.

[16] J. Mertens and S. Zamir, “Formulation of Bayesian analysis for games
with incomplete information,” International Journal of Game Theory,
vol. 14, pp. 1–29, 1985.

[17] E. Dekel and M. Siniscalchi, “Epistemic game theory,” in Handbook

of Game Theory. Elsevier, 2015, ch. 12, pp. 619–702.

[18] S. Zamir, “Bayesian games: Games with incomplete information,” in
Springer, 2009,

Encyclopedia of Complexity and Systems Science.
pp. 426–441.

[19] X. Fu, W. Yu, D. Cheng, X. Tan, K. Streff, and S. Graham, “On recog-
nizing virtual honeypots and countermeasures,” in IEEE International
Symposium on Dependable, Autonomic and Secure Computing, 2006,
pp. 211–218.

[20] N. Krawetz, “Anti-honeypot technology,” IEEE Security & Privacy,

vol. 2, no. 1, pp. 76–79, 2004.

(i)(vi)large(iii)(iv)small(ii)(i)(iii)(vi)(v)(iv)(i)(vi)(iii)(vi)(iv)(i)(vi)(v)(iv)(iii)(ii)(iv)(i)(iii)