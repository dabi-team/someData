9
1
0
2

v
o
N
5

]
T
G
.
s
c
[

2
v
7
8
6
9
0
.
6
0
9
1
:
v
i
X
r
a

A Dynamic Games Approach to Proactive Defense Strategies
against Advanced Persistent Threats in Cyber-Physical Systems

Linan Huanga, Quanyan Zhua

aDepartment of Electrical and Computer Engineering, New York University, Brooklyn, NY, 11201

A R T I C L E I N F O

A B S T R A C T

Keywords:
advanced persistent threats
defense in depth
proactive defense
industrial control system security
cyber deception
multi-stage Bayesian game
perfect Bayesian Nash equilibrium
Tennessee Eastman process

Advanced Persistent Threats (APTs) have recently emerged as a signiï¬cant security challenge for
a cyber-physical system due to their stealthy, dynamic and adaptive nature. Proactive dynamic
defenses provide a strategic and holistic security mechanism to increase the costs of attacks
and mitigate the risks. This work proposes a dynamic game framework to model a long-term
interaction between a stealthy attacker and a proactive defender. The stealthy and deceptive
behaviors are captured by the multi-stage game of incomplete information, where each player
has his own private information unknown to the other. Both players act strategically according to
their beliefs which are formed by the multi-stage observation and learning. The perfect Bayesian
Nash equilibrium provides a useful prediction of both playersâ€™ policies because no players beneï¬t
from unilateral deviations from the equilibrium. We propose an iterative algorithm to compute
the perfect Bayesian Nash equilibrium and use the Tennessee Eastman process as a benchmark
case study. Our numerical experiment corroborates the analytical results and provides further
insights into the design of proactive defense-in-depth strategies.

1. Introduction

The recent advances in automation technologies, 5G networks, and cloud services have accelerated the development
of cyber-physical systems (CPSs) by integrating computing and communication functionalities with components in
the physical world. Cyber integration increases the operational eï¬ƒciency of the physical system, yet it also creates
additional security vulnerabilities. First, the increased connectivity and openness have expanded the attack surface
and enabled attackers to leverage vulnerabilities from multiple system components to launch a sequence of stealthy
attacks. Second, the component heterogeneity, the functionality complexity, and the dimensionality of cyber-physical
systems have created many zero-day vulnerabilities, which make the defense arduous and costly.

Advanced Persistent Threats (APTs) are a class of emerging threats for cyber-physical systems with the following
distinct features. Unlike opportunistic attackers who spray and pray, APTs have speciï¬c targets and suï¬ƒcient knowl-
edge of the system architecture, valuable assets, and even defense strategies. Attackers can tailor their strategies and
invalidate cryptography, ï¬rewalls, and intrusion detection systems. Unlike myopic attackers who smash and grab,
APTs are stealthy and can disguise themselves as legitimate users for a long sojourn in the victimâ€™s system.

A few security researchers and experts have proposed APT models in which the entire intrusion process is divided
into a sequence of phases, such as Lockheed-Martinâ€™s Cyber Kill Chain (see Hutchins, Cloppert and Amin (2011)),
MITREâ€™s ATT&CK (see Corporation (2019)), the NSA/CSS technical cyber threat framework (see Department of
Homeland Security (2018)), and the ones surveyed in Messaoud, Guennoun, Wahbi and Sadik (2016). Fig. 1 illustrates
the multi-stage structure of APTs. During the reconnaissance phase, a threat actor collects open-source or internal
intelligence to identify valuable targets. After the attacker obtains a private key and establishes a foothold, he escalates
privilege, propagates laterally in the cyber network, and eventually either accesses conï¬dential information or inï¬‚icts
physical damage. Static standalone defense on a physical system cannot deter attacks originated from a cyber network.
The multi-phase feature of APTs results in the concept of Defense in Depth (DiD), i.e., multi-stage cross-layer
defense policies. A system defender should adopt defensive countermeasures across the phases of APTs and holistically
consider interconnections and interdependencies among these layers. To formally describe the interaction between an
APT attacker and a defender with the defense-in-depth strategy, we map the sequential phases of APTs into a game of
multiple stages. Each stage describes a local interaction between the attacker and the defender where the outcome leads

lh2328@nyu.edu (L. Huang); qz494@nyu.edu (Q. Zhu)

ORCID(s): 00000003-15918749 (L. Huang); 00000002-00082953 (Q. Zhu)

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 1 of 24

 
 
 
 
 
 
Proactive Defense against Advanced Persistent Threats

Figure 1: An illustrate example of the multi-stage structure of APTs. The multi-stage attack is composed of reconnaissance,
initial compromise, privilege escalation, lateral movement, and mission execution. An attack originated from an early-stage
cyber network can lead to damage in a physical system.

to the next stage of interactions. The goal of the attacker is to stealthily reach the targeted physical or informational
assets while the defender aims to take defensive actions at multiple phases to thwart the attack or reduce its impact.

Detecting APTs timely (i.e., before attackers have reached the ï¬nal stage) and eï¬€ectively (i.e., with a low rate of
false alarms and missed detections) is still an open problem due to their stealthy and deceptive characteristics. As
reported in LLC (2018), US companies in 2018 have taken an average of 197 and 69 days, respectively, to detect
and contain a data breach. Stuxnet-like APT attacks can conceal themselves in a critical industrial system for years
and inconspicuously increase the failure probability of physical components. Due to the insuï¬ƒciency of timely and
eï¬€ective detection systems for APTs, the defender remains uncertain about the userâ€™s type, i.e., either legitimate or
adversarial, throughout stages. To prepare for the potential APT attacks, the defender needs to adopt precautions and
proactive defense measures, which may also impair the user experience and reduce the utility of a legitimate user.
Therefore, the defender needs to strategically balance the tradeoï¬€ between security and usability when the userâ€™s type
remains private.

In this work, we model the private information of the userâ€™s type as a random variable following the work of
Harsanyi (1967). Under the same defense action, the behavior and the utility of a user depend on whether his type is
legitimate or adversarial. To make secure and usable decisions under incomplete information, the defender forms a
belief on the userâ€™s type and updates the belief via the Bayesian rule based on the information acquired at each stage.
For example, throughout the phases of an APT, detection systems can generate many alerts based on suspicious user
activities. Although these alerts do not directly reveal the userâ€™s type, a defender can use them to reduce the uncertainty
on the userâ€™s type and better determine her defense-in-depth strategies at multiple stages.

Defensive deception provides an alternative perspective to bring uncertainty to the attacker and tilt the information
asymmetry. We classify a defender into diï¬€erent levels of sophistication based on factors such as her level of security
awareness, detection techniques she have adopted, and the completeness of her virus signature database. A sophisti-
cated defender has a higher success rate of detecting adversarial behaviors. Thus, the behavior of an attacker depends
on the type of defender that he interacts with. For example, the attacker may remain stealthy when he interacts with a
sophisticated defender but behaves more aggressively when interacting with a primitive defender. As the attacker has
incomplete information regarding the defenderâ€™s type, he needs to form a belief and continuously updates it based on
his observation of the defenderâ€™s actions. In this way, the attacker can optimally decide whether, when, and to what
extent, to behave aggressively or conservatively.

To this end, we also use a random variable to characterize the private information of the defenderâ€™s type. As both
players have incomplete information regarding the other playerâ€™s type and they make sequential decisions across multi-
ple stages, we extend the classical static Bayesian game to a multi-stage nonzero-sum game with two-sided incomplete
information. Both players act strategically according to their beliefs to maximize their utilities. The Perfect Bayesian
Nash Equilibrium (PBNE) provides a useful prediction of their policies at every stage for each type since no players can
beneï¬t from unilateral deviations at the equilibrium. Computing the PBNE is challenging due to the coupling between
the forward belief update and the backward policy computation. We ï¬rst formulate a mathematical programming
problem to compute the equilibrium policy pair under a given belief for the one-stage Bayesian game. For multi-stage
Bayesian games, we compute the equilibrium policy pair under a given sequence of beliefs by constructing a sequence

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 2 of 24

Initial compromisePhysical accessWeb phishingPrivilege escalationLateral movementMission completenessData collection and exfiltration Physical damage Social engineeringPrivate keyDatabaseSensorReconnaissance Insider threatsOSINTControllerProactive Defense against Advanced Persistent Threats

of nested mathematical programming problems. Finally, we combine these programs with the Bayesian update and
propose an eï¬ƒcient algorithm to compute the PBNE.

The proposed modeling and computational methods are shown to be capable of hardening the security of a broad
class of supervisory control and data acquisition (SCADA) systems. This work leverages the Tennessee Eastman pro-
cess as a case study of proactive defense-in-depth strategies against the APT attackers who can inï¬ltrate into the cyber
network through phishing emails, escalate privileges through the process injection, tamper the sensor reading through
malicious encrypted communication, and eventually decrease the operational eï¬ƒciency of the Tennessee Eastman
process without triggering the alarm. The dynamic game approach oï¬€ers a quantitative way to assess the risks and
provides a systematic and computational mechanism to develop proactive and strategic defenses across multiple cyber
and physical stages. Based on the computation result of the case study, we obtain the following insights to guide the
design of practical defense systems.

â€¢ Defense at the ï¬nal stage is usually too late to be eï¬€ective when APTs have been well-prepared and ready to
attack. We need to take precautions and proactive responses in the cyber stages when the attack remains â€œunder
the radar" so that the attacker becomes less dominant when they reach the ï¬nal stage.

â€¢ The online learning capability of the defender plays an important role in detecting the adversarial deception and
tilting the information asymmetry. It increases the probability of identifying the hidden information from the
observable behaviors, threatens the stealthy attacker to take more conservative actions, and hence reduces the
attack loss.

â€¢ Third, defensive deception techniques are shown to be eï¬€ective to introduce uncertainty to attackers, increase
their learning costs, and hence reduce the probability of successful attacks. Those techniques may introduce a
negative impact on legitimate users. However, a delicate balance between security and usability can be achieved
under proper designs.

1.1. Related Work

One well-known industrial solution to APT defense is the ATT&CK matrix (see Corporation (2019)). It illustrates
disclosed attack methods and possible detection and mitigation countermeasures at diï¬€erent phases of APTs. However,
as argued in Dufresne (2018), it lacks a prioritization to list all possible attack methods in one matrix. A lot of false
alarms can arise as legitimate users can also generate a majority of activities in the ATT&CK matrix. Besides, despite
a persistent update, the matrix is far from complete and can lead to miss detection.

Many papers have attempted to deal with the above two challenges, i.e., false alarms and miss detection. To
prevent security specialists from overwhelming alarms, Marchetti, Pierazzi, Colajanni and Guido (2016) has analyzed
high volumes of network traï¬ƒc to reveal weak signals of suspect APT activities and ranked these signals based on
the computation of suspiciousness scores. To identify attacks that exploit zero-day vulnerabilities or other unknown
attack techniques, Friedberg, Skopik, Settanni and Fiedler (2015) has managed to learn and maintain a white-list of
normal system behaviors and report all actions that are not on the white-list. There is also a rich literature on detecting
essential components of an APT attack such as malicious PDF ï¬les in phishing emails (see Nissim, Cohen, Glezer
and Elovici (2015)), malicious SSL certiï¬cate during command and control communications (see Ghaï¬r, Prenosil,
Hammoudeh, Han and Raza (2017)), and data leakage at the ï¬nal stage of the APT campaign (see Sigholm and Bang
(2013)). These works have focused on a static detection of abnormal behaviors in one speciï¬c stage but had not taken
into account the correlation among multiple phases of APTs. Ghaï¬r, Hammoudeh, Prenosil, Han, Hegarty, Rabie and
Aparicio-Navarro (2018) has managed to build a framework to correlate alerts across multiple phases of APTs based on
machine learning techniques so that all those alerts can be attributed to a single APT scenario. Ghaï¬r, Kyriakopoulos,
Lambotharan, Aparicio-Navarro, AsSadhan, BinSalleeh and Diab (2019) has constructed a correlation framework to
link elementary alerts to the same APT campaign and applied the hidden Markov model to determine the most likely
sequence of APT stages.

An alternative perspective from the aforementioned APT detection frameworks is to address how to respond to and
mitigate potential attacks. Li, Yang, Xiong, Wen and Tang (2018) has captured the dynamic state evolution through a
network-based epidemic model and provided both prevention and recovery strategies for defenders based on optimal
control approaches. Since APTs are controlled by human experts and can act strategically, the defenderâ€™s response
should adapt to the potential change of APT behaviors. Thus, decision and game theory becomes a natural quantitative
framework to capture constraints on defense actions, attack consequences, and attackersâ€™ incentives. Van Dijk, Juels,

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 3 of 24

Proactive Defense against Advanced Persistent Threats

Oprea and Rivest (2013) has proposed FlipIt game to model the key leakage under APTs as a private takeover between
the system operator and the attacker. Many works have integrated FlipIt with other components for the APT defense
such as the signaling game to defend cloud service (see Pawlick, Chen and Zhu (2018)), an additional player to model
the insider threats (see Feng, Zheng, Hu, Cansever and Mohapatra (2015)), and a system of multiple nodes under limited
resources (see Zhang, Zheng and Shroï¬€ (2015)). The FlipIt has described a high-level abstraction of the attackerâ€™s
behavior to understand optimal timing for resource allocations. However, for our purpose of developing multi-stage
defense policies, we need to provide a ï¬ner-grained model that can capture the dynamic interactions between players of
diï¬€erent types across multiple stages. Our game framework models heterogeneous adversarial and defensive behaviors
at multiple stages, allowing the prediction of attack moves and the estimation of losses using the equilibrium analysis.
Other security game models such as Zhu and Rass (2018); Yang, Li, Zhang, Yang, Xiang and Zhou (2018); Huang,
Chen and Zhu (2017) have provided dynamic risk management frameworks that allow the defender to response and
repair eï¬€ectively. In particular, to model the multi-stage structure of APTs, Zhu and Rass (2018) has developed a
sequence of heterogeneous game phases, i.e., a static Bayesian game for spear phishing, a nested game for penetration,
and a ï¬nite zero-sum game for the ï¬nal stage of physical-layer infrastructure protection. However, most of these
security game frameworks have assumed complete information. Our framework explicitly models the incomplete
information across the entire phases of APTs and introduces their belief updates based on multi-stage information for
making long-term strategic decisions.

Cyber deception is an emerging research area. Games of incomplete information are natural frameworks to model
the uncertainty and misinformation introduced by cyber deceptions. Previous works mainly focus on adversarial de-
ceptions where the deceiver is the attacker. For example, strategic attackers in Nguyen, Wang, Sinha and Wellman
manipulate the attack data to mislead the defender in ï¬nitely repeated security games. A defender, on the other hand,
can also initiate defensive deception techniques such as perturbations via external noises, obfuscations via revealing
useless information, or honeypot deployments as shown in Pawlick, Colbert and Zhu (2017). HorÃ¡k, Zhu and BoÅ¡ansk`y
(2017) proposes a framework to engage with attackers strategically to deceive them against the attack goal without their
awareness. A honeypot which appears to contain valuable information can lure attackers into isolation and surveil-
lance. La, Quek, Lee, Jin and Zhu (2016) has used a Bayesian game to model deceptive attacks and defenses in a
honeypot-enabled network in the envisioned Internet of Things. Besides detection, a honeypot can also be used to ob-
tain high-level indicators of compromise under a proper engagement policy as shown in Huang and Zhu (2019a) where
several security metrics are investigated and the optimal engagement policy is learned by reinforcement learning. A
system can also disguise a real asset as a honeypot to evade attacks as shown in Rowe, Custy and Duong (2007). Our
work considers a dynamic Bayesian game with double-sided incomplete information to incorporate both adversarial
and defensive deceptions.

The preliminary versions of this work (see Huang and Zhu (2018, 2019b)) have considered a dynamic game with
one-sided incomplete information where attackers disguise themselves as legitimate users. This work extends the
framework to a two-sided incomplete information structure where primitive systems can also disguise themselves as
sophisticated systems. The new framework enables us to jointly investigate deceptions adopted by both attackers and
defenders, and strategically design defensive deceptions to counter adversarial ones. We also develop new method-
ologies to address the challenge of the coupled belief update in a generalize setting without the previous assumption
of the beta-binomial conjugate pair. In the case study, we investigate heterogeneous actions and cyber stages such as
web phishing and privilege escalation, whose utilities are no longer negligible. Moreover, we leverage the Tennessee
Eastman process with new performance metric and attack models to validate the eï¬ƒcacy of the proposed proactive
defense-in-depth strategies, the Bayesian learning, and the defensive deception.

1.2. Organization of the Paper

We summarize notations, variables, and acronyms in Table. 1 for readersâ€™ convenience. We use pronoun â€˜heâ€™ for
the user and â€˜sheâ€™ for the defender throughout this paper. The rest of the paper is organized as follows. Section 2
introduces the multi-stage game with incomplete information and three equilibrium concepts are deï¬ned in Section 3.
To compute these equilibria, we construct constrained optimization problems and an iterative algorithm in Section 4.
A case study of Tennessee Eastman process under APTs is presented in Section 5 with results in Section 6. Section 7
concludes the paper.

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 4 of 24

Proactive Defense against Advanced Persistent Threats

Table 1
Summary of notations, variables, and acronyms.

General Notation
ğ´ âˆ¶= ğµ
Pr
ğ‘“ âˆ¶ ğ´ â†¦ ğµ
ğ”¼ğ‘âˆ¼ğ´[ğ‘“ (ğ‘)]
â„
|ğ´|
ğ‘ âˆ¼ ğ´
ğŸ{ğ‘¥=ğ‘¦}
{ğ‘1, â‹¯ , ğ‘ğ‘›}
ğµ â§µ ğ´

Meaning
ğ´ is deï¬ned as ğµ
Probability
A function or a mapping ğ‘“ from domain ğ´ to codomain ğµ
Expectation of ğ‘“ (ğ‘) over random variable ğ‘ whose distribution is ğ´
Set of real numbers
The cardinality of set ğ´
Random variable ğ‘ follows probability distribution ğ´
Indicator function which equals one when ğ‘¥ = ğ‘¦, and zero otherwise
Set with ğ‘› elements ğ‘1, â‹¯ , ğ‘ğ‘›
Set of elements in ğµ but not in ğ´

ğ‘–

Variable
ğ‘–, ğ‘— âˆˆ {1, 2}
Î˜ğ‘–
Î”(Î˜ğ‘–)
ğœƒğ‘– âˆˆ Î˜ğ‘–
ğœƒğ»
1 (resp. ğœƒğ¿
1 )
2 (resp. ğœƒğ‘”
ğœƒğ‘
2 )
ğ¾
ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾}
ğ‘˜0 âˆˆ {0, 1, â‹¯ , ğ¾}
ğ´ğ‘˜
ğ‘–
Î”(ğ´ğ‘˜
ğ‘– )
ğ‘ğ‘˜
ğ‘– âˆˆ ğ´ğ‘˜
â„ğ‘˜, ğ» ğ‘˜
ğ‘¥ğ‘˜, ğ‘‹ğ‘˜
ğ‘“ ğ‘˜
ğ‘™ğ‘˜
ğ‘– , ğ¿ğ‘˜
ğœğ‘˜
ğ‘– , Î£ğ‘˜
ğœğ‘˜
ğ‘– (ğ‘ğ‘˜
ğœğ‘˜0âˆ¶ğ¾
ğ‘–
ğœâˆ—,ğ‘˜0âˆ¶ğ¾
(ğœâˆ—,ğ¾
ğ‘–
ğ‘–
ğ‘– âˆ¶ ğ¿ğ‘˜
ğ‘ğ‘˜
ğ‘– â†¦ Î”(Î˜ğ‘—)
ğ‘– (ğœƒğ‘—|ğ‘™ğ‘˜
ğ‘ğ‘˜
ğ‘– )
Ì„ğ½ ğ‘˜
1, ğ‘ğ‘˜
ğ‘– (ğ‘¥ğ‘˜, ğ‘ğ‘˜
1, ğ‘ğ‘˜
ğ‘– (ğ‘¥ğ‘˜, ğ‘ğ‘˜
ğ½ ğ‘˜
(ğœğ‘˜0âˆ¶ğ¾
ğ‘ˆ ğ‘˜0âˆ¶ğ¾
ğ‘–
ğ‘–

ğ‘– |ğ‘™ğ‘˜
ğ‘– )

ğ‘–

ğ‘–

âˆ¶= ğœâˆ—,ğ¾âˆ¶ğ¾
ğ‘–

)

2, ğœƒ1, ğœƒ2, ğ‘¤ğ‘˜
ğ‘– )
2, ğœƒ1, ğœƒ2)
, ğœğ‘˜0âˆ¶ğ¾
ğ‘—

, ğ‘¥ğ‘˜0 , ğœƒğ‘–)

ğ‘– (ğ‘¥ğ‘˜, ğœƒğ‘–)
ğ‘‰ ğ‘˜

Acronym
APT(s)
SBNE
DBNE
PBNE

Meaning
Index for players in the game: ğ‘–, ğ‘— = 1 for the defender and ğ‘–, ğ‘— = 2 for the user
Set of all possible types of player ğ‘– âˆˆ {1, 2}
Space of probability distributions over type set Î˜ğ‘– of player ğ‘– âˆˆ {1, 2}
Type of player ğ‘– âˆˆ {1, 2}
The defender is sophisticated (resp. primitive)
The user is adversarial (resp. legitimate)
Total number of stages
Stage index
Index for the initial stage
Set of all possible actions of player ğ‘– âˆˆ {1, 2} at stage ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾}
Space of probability distributions over the action set ğ´ğ‘˜
ğ‘–
Action of player ğ‘– âˆˆ {1, 2} at stage ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾}
Action history and the set of all possible action histories at stage ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾}
State and the set of all possible states at stage ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾}
State transition function at stage ğ‘˜, i.e., ğ‘¥ğ‘˜+1 = ğ‘“ ğ‘˜(ğ‘¥ğ‘˜, ğ‘ğ‘˜
Available Information and set of all available information for player ğ‘– at stage ğ‘˜
Behavioral strategy and the set of all behavioral strategies for player ğ‘– at stage ğ‘˜
Probability of player ğ‘– taking action ğ‘ğ‘˜
Player ğ‘–â€™s behavioral strategies from stage ğ‘˜0 to ğ¾
Player ğ‘–â€™s behavioral strategies from stage ğ‘˜0 to ğ¾ at the equilibrium
Player ğ‘–â€™s belief on the other player ğ‘—â€™s type at stage ğ‘˜ based on the available information
Probability of player ğ‘— being type ğœƒğ‘— when player ğ‘– observes information ğ‘™ğ‘˜
Player ğ‘–â€™s stage utility received at stage ğ‘˜ when the state is ğ‘¥ğ‘˜, player ğ‘– takes action ğ‘ğ‘˜
ğ‘– ,

ğ‘– at stage ğ‘˜ based on the available information ğ‘™ğ‘˜

ğ‘– at stage ğ‘˜

1, ğ‘ğ‘˜
2)

ğ‘–

player ğ‘–â€™s type is ğœƒğ‘–, and the noise is ğ‘¤ğ‘˜
ğ‘–

Player ğ‘–â€™s expected stage utility received at stage ğ‘˜ with the input of ğ‘¥ğ‘˜, ğ‘ğ‘˜
Player ğ‘–â€™s expected cumulative utility received from stage ğ‘˜0 to ğ¾ when the initial state
is ğ‘¥ğ‘˜0 , his/her type is ğœƒğ‘–, and the multi-stage strategies of player ğ‘– are ğœğ‘˜0âˆ¶ğ¾

2, ğœƒ1, ğœƒ2

1, ğ‘ğ‘˜

ğ‘–

Player ğ‘–â€™s value function at state ğ‘¥ğ‘˜ when his/her type is ğœƒğ‘–

Meaning
Advanced persistent threat(s)
Static Bayesian Nash equilibrium
Dynamic Bayesian Nash equilibrium
Perfect Bayesian Nash equilibrium

2. Dynamic Game Modelling of APT Attacks

There are two players in the game, player 1 is the user and player 2 is the defender. The stealthy, persistent, and
deceptive features of APTs result in incomplete information of the userâ€™s type to the defender. We use a ï¬nite set Î˜2
to accommodate all possible types of the user. For example, we consider a binary type set for the case study in Section

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 5 of 24

Proactive Defense against Advanced Persistent Threats

Figure 2: A block diagram of applying the defense-in-depth approach against multi-stage APT attacks. We denote the
user, the defender, and the system states in red, blue, and black, respectively. The defender interacts with the user from
stage 0 to stage ğ¾ in sequence where the output state of stage ğ‘˜ âˆ’ 1 becomes the input state of stage ğ‘˜. At each stage
ğ‘˜, the user observes the defenderâ€™s actions at previous stages, forms a belief on the defenderâ€™s type, and takes an action.
At the same time, the defender makes decisions based on the output of an imperfect detection system. The dotted line
means that the observation is not in real time, i.e., both players can only observe the previous-stage actions of the other
player.

5 and 6 where the userâ€™s type ğœƒ2
. The APT attacker, i.e., the adversarial user,
disguises himself as the legitimate user, thus the defender does not know the type of the user. The set of the userâ€™s type
can also be non-binary and incorporate diï¬€erent APT groups when their attack tools and targeted assets are diï¬€erent
(see FireEye (2017)).

is either adversarial ğœƒğ‘
2

or legitimate ğœƒğ‘”
2

The Defender can also be classiï¬ed into diï¬€erent levels of sophistication based on various factors such as her level
of security awareness, detection techniques she adopted, and the completeness of her virus signature database. The
distinguishes defenders of diï¬€erent sophistication levels and all the possible type values constitute the
discrete type ğœƒ1
or primitive
defenderâ€™s type set Î˜1
. The defender can apply defensive deception techniques and keep her type private to the user. We assume that both
ğœƒğ¿
1
playersâ€™ type sets are commonly known. Each player knows his/her own type, yet not the other playerâ€™s type. Thus,
each player ğ‘– should treat the other playerâ€™s type as a random variable with an initial distribution ğ‘0
and update the
ğ‘–
distribution to ğ‘ğ‘˜
when obtaining new information at each stage ğ‘˜. We present the above belief update formally in
ğ‘–
Section 2.3.

. For example, in our case study, the defenderâ€™s type ğœƒ1

is either sophisticated ğœƒğ»
1

2.1. Multi-stage Transition

We formulate the interaction between the multi-stage APT attack and the cross-stage proactive defense into ğ¾
stages of sequential games with incomplete information, as shown in Fig. 2. At each stage ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾}, player
. An intrusion detection system generates alerts
ğ‘– âˆˆ {1, 2} takes an action ğ‘ğ‘˜
based on the userâ€™s actions. However, since legitimate users can also trigger these alerts, each alert itself does not reveal
the userâ€™s type. For example, an APT attacker uses the Tor network connection for data exï¬ltration, yet a legitimate
user can also use it legally for the traï¬ƒc conï¬dentiality as shown in Milajerdi and Kharrazi (2015). Another example

from a ï¬nite and discrete set ğ´ğ‘˜
ğ‘–

ğ‘– âˆˆ ğ´ğ‘˜
ğ‘–

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 6 of 24

LegitimateUserâ€™s	TypeAdversarialorPrimitiveDefenderâ€™s	TypeSophisticatedInteraction	at	Stage	!orImperfect	Detection	SystemSuspicious	user	actionsDefenderâ€™s	actionUserâ€™s	actionBelief	update	and	decision	makingBelief	update	and	decision	makingInteraction	at	Stage	!âˆ’1Interaction	at	Stage	!+1%&'(%&Delayed	observation%&)(%&'*Interaction	at	Stage	0%,%-Interaction	at	Stage	.%(Proactive Defense against Advanced Persistent Threats

is that code obfuscation can be either used legitimately to prevent reverse engineering or illegally to conceal malicious
JavaScript code from being recognized by signature-based detectors or human analysts as shown in Nissim et al. (2015).
We assume that the user can observe the defenderâ€™s stage-ğ‘˜ action at stage ğ‘˜ + 1. The observation of the defenderâ€™s
action at a single stage also does not reveal the defenderâ€™s type.

In this paper, each player obtains a one-stage delayed observation of the other playerâ€™s actions, i.e., at each stage
ğ´Ì„ğ‘˜
ğ‘˜, the action history available to both players is â„ğ‘˜ = {ğ‘0
. Given
ğ‘–
1
2} after the
history â„ğ‘˜ at the current stage ğ‘˜, players at stage ğ‘˜ + 1 obtain an updated history â„ğ‘˜+1 = â„ğ‘˜ âˆª {ğ‘ğ‘˜
1
observation of both playersâ€™ actions at stage ğ‘˜. At each stage ğ‘˜, we further deï¬ne a state ğ‘¥ğ‘˜ âˆˆ ğ‘‹ğ‘˜ which summarizes
information about both playersâ€™ actions in previous stages so that the initial state ğ‘¥0 âˆˆ ğ‘‹0 and the history at stage ğ‘˜
uniquely determine ğ‘¥ğ‘˜ through a known state transition function ğ‘“ ğ‘˜, i.e., ğ‘¥ğ‘˜+1 = ğ‘“ ğ‘˜(ğ‘¥ğ‘˜, ğ‘ğ‘˜
2), âˆ€ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾ âˆ’1}.
1
States at diï¬€erent stages can have diï¬€erent meanings such as the reconnaissance outcome, the userâ€™s location, the
privilege level, and the sensor status.

} âˆˆ ğ» ğ‘˜ âˆ¶= âˆ2

âˆğ‘˜âˆ’1
Ì„ğ‘˜=0
, ğ‘ğ‘˜

, â‹¯ , ğ‘ğ‘˜âˆ’1

, â‹¯ , ğ‘ğ‘˜âˆ’1

, ğ‘0
2

, ğ‘ğ‘˜

ğ‘–=1

2

1

2.2. Behavioral Strategy

A defender should behave diï¬€erently when interacting with adversarial users and legitimate ones. The defensive
measure should also vary for attackers who adopt diï¬€erent code families and tools. However, since the defender is
uncertain about the userâ€™s type throughout the entire stages of games, she has to make judicious decisions at each stage
to balance usability versus security. The userâ€™s action should also adapt to the type of the defender. For example, if
the defender is primitive, an attacker prefers to take aggressive adversarial actions to achieve a quicker and low-cost
compromise. However, if the defender is sophisticated and can detect the malware with better accuracy, an attacker
has to take conservative actions to remain stealthy. Since the proactive defense actions across the entire stages can
aï¬€ect legitimate users, they also need to be designed to avoid collateral damage.

ğ‘– âˆ¶ ğ¿ğ‘˜

ğ‘– âˆˆ Î£ğ‘˜

ğ‘– âˆˆ ğ¿ğ‘˜
ğ‘–

Thus, the decision-making problem of the defender or the user boils down to the determination of a behavioral
ğ‘– ), i.e., player ğ‘– at each stage ğ‘˜ needs to decide which action to take or take an action
strategy ğœğ‘˜
ğ‘– â†¦ Î”(ğ´ğ‘˜
with what probability based on the information ğ‘™ğ‘˜
available to him/her at stage ğ‘˜. We present two diï¬€erent
information structures in Section 2.3.1 and 2.3.2. The strategy is called â€˜behavioralâ€™ as the strategy depends on the
information available at the time the players make their decisions. In this work, players are allowed to take mixed
strategies, thus the co-domain of the strategy function ğœğ‘˜
.
ğ‘– ), a probability distribution over the action space ğ´ğ‘˜
ğ‘–
ğ‘–
With a slight abuse of notation, we denote ğœğ‘˜
given the
available information ğ‘™ğ‘˜
, is a realization of the behavioral
are not observable for player ğ‘– at stage
strategy ğœğ‘˜
ğ‘–
are
ğ‘– ). Therefore, ğœğ‘˜
ğ‘˜, thus do not aï¬€ect player ğ‘–â€™s behavioral strategy ğœğ‘˜
ğ‘–
conditionally independent, i.e., Pr(ğ‘ğ‘˜
ğ‘– (ğ‘ğ‘˜
ğ‘— ) = ğœğ‘˜

. The actual action of player ğ‘– taken at stage ğ‘˜, i.e., ğ‘ğ‘˜
ğ‘–

ğ‘– ) as the probability of player ğ‘– taking action ğ‘ğ‘˜

. Note that the values of the other playerâ€™s type ğœƒğ‘—

and action ğ‘ğ‘˜
ğ‘—
ğ‘— , ğœƒğ‘—, ğ‘™ğ‘˜
ğ‘– |ğ‘ğ‘˜
ğ‘— ).
ğ‘— |ğ‘™ğ‘˜

, i.e., Pr(ğ‘ğ‘˜
ğ‘— (ğ‘ğ‘˜
ğ‘– )ğœğ‘˜
ğ‘– |ğ‘™ğ‘˜

ğ‘– ) = ğœğ‘˜

ğ‘– âˆˆ ğ´ğ‘˜
ğ‘–

and ğœğ‘˜
2

ğ‘– âˆˆ ğ¿ğ‘˜
ğ‘–

is Î”(ğ´ğ‘˜

ğ‘– , ğ‘ğ‘˜

ğ‘– (ğ‘ğ‘˜

ğ‘– (ğ‘ğ‘˜

ğ‘– , ğ‘™ğ‘˜

ğ‘– |ğ‘™ğ‘˜

ğ‘– |ğ‘™ğ‘˜

ğ‘— |ğ‘™ğ‘˜

1

2.3. Belief and Bayesian Update

ğ‘– (ğœƒğ‘—|ğ‘™ğ‘˜

ğ‘– â†¦ Î”(Î˜ğ‘—), ğ‘— â‰  ğ‘–. Likewise, ğ‘ğ‘˜

To quantify the uncertainty of the other playerâ€™s type throughout the entire stages, each player ğ‘– forms a belief
ğ‘– ) means that given information ğ‘™ğ‘˜
at stage ğ‘˜, player ğ‘– forms a belief
ğ‘ğ‘˜
ğ‘– âˆ¶ ğ¿ğ‘˜
with probability ğ‘ğ‘˜
that the other player ğ‘— is of type ğœƒğ‘— âˆˆ Î˜ğ‘—
ğ‘– ). At the initial stage ğ‘˜ = 0, the only information
based
available to player ğ‘– is his/her own type, i.e., ğ‘™0
on the past experiences with the other player. If no previous experiences are available to player ğ‘–, player ğ‘– can take the
uniform distribution as an unbiased prior belief. As each player ğ‘– obtains new information when arriving at the next
stage, his or her belief can be updated using the Bayesian rule. We present the Bayesian update under two diï¬€erent
information structures ğ¿ğ‘˜
ğ‘–

. We assume that player ğ‘– has a prior belief distribution ğ‘0
ğ‘–

at stage 0 < ğ‘˜ â‰¤ ğ¾ in the following two subsections.

ğ‘– âˆˆ ğ¿ğ‘˜
ğ‘–

ğ‘– (ğœƒğ‘—|ğ‘™ğ‘˜

ğ‘– = ğœƒğ‘–

2.3.1. Timely Observations

The most straightforward information structure is ğ¿ğ‘˜

, i.e., the information available to player ğ‘– at stage

ğ‘– = ğ» ğ‘˜ Ã— Î˜ğ‘–

ğ‘˜ is the action history â„ğ‘˜ and player ğ‘–â€™s own type ğœƒğ‘–

, which leads to the belief update in (1).

ğ‘ğ‘˜+1
ğ‘–

(ğœƒğ‘—|â„ğ‘˜ âˆª {ğ‘ğ‘˜

ğ‘– , ğ‘ğ‘˜

ğ‘— }, ğœƒğ‘–) =

âˆ‘

ğ‘– |â„ğ‘˜, ğœƒğ‘–)ğœğ‘˜
ğ‘– (ğ‘ğ‘˜
ğœğ‘˜
ğ‘– (ğ‘ğ‘˜
ğœğ‘˜

ğ‘— (ğ‘ğ‘˜
ğ‘– |â„ğ‘˜, ğœƒğ‘–)ğœğ‘˜

ğ‘— |â„ğ‘˜, ğœƒğ‘—)ğ‘ğ‘˜
ğ‘— (ğ‘ğ‘˜

ğ‘— |â„ğ‘˜, Ì„ğœƒğ‘—)ğ‘ğ‘˜

Ì„ğœƒğ‘— âˆˆÎ˜ğ‘—

ğ‘– (ğœƒğ‘—|â„ğ‘˜, ğœƒğ‘–)

ğ‘– ( Ì„ğœƒğ‘—|â„ğ‘˜, ğœƒğ‘–)

, ğ‘–, ğ‘— âˆˆ {1, 2}, ğ‘— â‰  ğ‘–.

(1)

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 7 of 24

Proactive Defense against Advanced Persistent Threats

Here, player ğ‘– updates the belief ğ‘ğ‘˜
ğ‘–
â„ğ‘˜+1 is not reachable from â„ğ‘˜, and the Bayesian update does not apply. In this case, we let ğ‘ğ‘˜+1
ğ‘– (ğœƒğ‘—|ğœƒğ‘–).
ğ‘0

based on the observation of the action ğ‘ğ‘˜

. When the denominator is 0, the history
(ğœƒğ‘—|â„ğ‘˜ âˆª{ğ‘ğ‘˜
ğ‘— }, ğœƒğ‘–) âˆ¶=

ğ‘– , ğ‘ğ‘˜
ğ‘—

ğ‘– , ğ‘ğ‘˜

ğ‘–

2.3.2. Markov Belief

If the information available to player ğ‘– at stage ğ‘˜ is the state value ğ‘¥ğ‘˜ and player ğ‘–â€™s own type ğœƒğ‘–

set is taken to be ğ¿ğ‘˜
ğ‘– = ğ‘‹ğ‘˜ Ã— Î˜ğ‘–
the Bayesian update between two consequent states is

. With the Markov property that Pr(ğ‘¥ğ‘˜+1

|ğœƒğ‘—, ğ‘¥ğ‘˜, â‹¯ , ğ‘¥1, ğ‘¥0, ğœƒğ‘–) = Pr(ğ‘¥ğ‘˜+1

, then the information
|ğœƒğ‘—, ğ‘¥ğ‘˜, ğœƒğ‘–),

ğ‘ğ‘˜+1
ğ‘–

(ğœƒğ‘—|ğ‘¥ğ‘˜+1, ğœƒğ‘–) =

Pr(ğ‘¥ğ‘˜+1

|ğœƒğ‘—, ğ‘¥ğ‘˜, ğœƒğ‘–)ğ‘ğ‘˜

ğ‘– (ğœƒğ‘—|ğ‘¥ğ‘˜, ğœƒğ‘–)

âˆ‘

Ì„ğœƒğ‘— âˆˆÎ˜ğ‘—

Pr(ğ‘¥ğ‘˜+1|

Ì„ğœƒğ‘—, ğ‘¥ğ‘˜, ğœƒğ‘–)ğ‘ğ‘˜

ğ‘– ( Ì„ğœƒğ‘—|ğ‘¥ğ‘˜, ğœƒğ‘–)

With the conditional independence of ğœğ‘˜
1

and ğœğ‘˜
2

,

Pr(ğ‘¥ğ‘˜+1

|ğœƒğ‘—, ğ‘¥ğ‘˜, ğœƒğ‘–) =

âˆ‘

{ğ‘ğ‘˜
1

,ğ‘ğ‘˜
2

}âˆˆ Ì„ğ´ğ‘˜

1 (ğ‘ğ‘˜
ğœğ‘˜

1|ğ‘¥ğ‘˜, ğœƒ1)ğœğ‘˜

2 (ğ‘ğ‘˜

2|ğ‘¥ğ‘˜, ğœƒ2),

, ğ‘–, ğ‘— âˆˆ {1, 2}, ğ‘— â‰  ğ‘–.

(2)

(3)

1

, ğ‘ğ‘˜

2 âˆˆ ğ´ğ‘˜

1 âˆˆ ğ´ğ‘˜

2|ğ‘¥ğ‘˜+1 = ğ‘“ ğ‘˜(ğ‘¥ğ‘˜, ğ‘ğ‘˜

where Ì„ğ´ğ‘˜ âˆ¶= {ğ‘ğ‘˜
2)} contains all the action pairs that change the system state
from ğ‘¥ğ‘˜ to ğ‘¥ğ‘˜+1. Equation (3) shows that the Bayesian update in (2) can be obtained from (1) by clustering all the
action pairs in set Ì„ğ´ğ‘˜. Thus, the Markov belief update (2) can also be regarded as an approximation of (1) using action
aggregations. Unlike the history set ğ» ğ‘˜, the dimension of the state set,
, does not grow with the number of stages.
|
Hence, the Markov approximation signiï¬cantly reduces the memory and computational complexity. The following
sections adopt the Markov belief update.

|ğ‘‹ğ‘˜

, ğ‘ğ‘˜

1

2.4. Stage and Cumulative Utility

The playerâ€™s utility can vary under the same action taken by diï¬€erent types of users or defenders. For example,
the remote access from a legitimate teleworker brings a reward to the defender while the one from an adversarial user
inï¬‚icts a loss. Therefore, at each stage ğ‘˜, player ğ‘–â€™s stage utility Ì„ğ½ ğ‘˜
2 Ã— Î˜1 Ã— Î˜2 Ã— â„ â†¦ â„ can depend
on both playersâ€™ types and actions, the current state ğ‘¥ğ‘˜ âˆˆ ğ‘‹ğ‘˜, and an external noise ğ‘¤ğ‘˜
ğ‘– âˆˆ â„ with a known probability
density function ğœ›ğ‘˜
. The noise term models unknown or uncontrolled factors that can aï¬€ect the value of the stage
ğ‘–
utility. The existence of the external noise makes it impossible for player ğ‘–, after reaching stage ğ‘˜ + 1, to infer the value
of the other playerâ€™s type ğœƒğ‘—
, together with the output of
the utility function Ì„ğ½ ğ‘˜
ğ‘–

based on the knowledge of the input parameters ğ‘¥ğ‘˜, ğ‘ğ‘˜
1

ğ‘– âˆ¶ ğ‘‹ğ‘˜ Ã— ğ´ğ‘˜

at stage ğ‘˜.

1 Ã— ğ´ğ‘˜

, ğ‘ğ‘˜
2

, ğœƒğ‘–

We denote the expected stage utility as ğ½ ğ‘˜

ğ‘– (ğ‘¥ğ‘˜, ğ‘ğ‘˜
1

ğ‘– (ğ‘¥ğ‘˜, ğ‘ğ‘˜
, ğ‘ğ‘˜
1
2
, the initial state ğ‘¥ğ‘˜0 âˆˆ ğ‘‹ğ‘˜0, and both playersâ€™ strategies ğœğ‘˜0âˆ¶ğ¾
to ğ¾, we can determine the expected cumulative utility ğ‘ˆ ğ‘˜0âˆ¶ğ¾

, ğœƒ1, ğœƒ2) âˆ¶= ğ”¼ğ‘¤ğ‘˜

, ğœƒ1, ğœƒ2, ğ‘¤ğ‘˜
, ğ‘ğ‘˜
2
âˆ¶= [ğœğ‘˜
ğ‘– (ğ‘ğ‘˜
for player ğ‘–, i.e.,

ğ‘– )], âˆ€ğ‘¥ğ‘˜, ğ‘ğ‘˜
1
ğ‘– |ğ‘¥ğ‘˜, ğœƒğ‘–)]ğ‘˜=ğ‘˜0,â‹¯,ğ¾ âˆˆ

ğ‘– âˆ¼ğœ›ğ‘˜
ğ‘–

, ğ‘ğ‘˜
2

ğ‘–

[ Ì„ğ½ ğ‘˜

, ğœƒ1, ğœƒ2

.

ğ‘–

Given the type ğœƒğ‘– âˆˆ Î˜ğ‘–
âˆğ¾
from stage ğ‘˜0

Î£ğ‘˜
ğ‘–

ğ‘˜=ğ‘˜0

ğ‘ˆ ğ‘˜0âˆ¶ğ¾
ğ‘–

(ğœğ‘˜0âˆ¶ğ¾
ğ‘–

, ğœğ‘˜0âˆ¶ğ¾
ğ‘—

, ğ‘¥ğ‘˜0, ğœƒğ‘–) âˆ¶=

ğ¾
âˆ‘

ğ‘˜=ğ‘˜0

ğ”¼ğœƒğ‘— âˆ¼ğ‘ğ‘˜

ğ‘– ,ğ‘ğ‘˜

ğ‘– âˆ¼ğœğ‘˜

ğ‘– ,ğ‘ğ‘˜

ğ‘— âˆ¼ğœğ‘˜
ğ‘—

[ğ½ ğ‘˜

ğ‘– (ğ‘¥ğ‘˜, ğ‘ğ‘˜

1, ğ‘ğ‘˜

2, ğœƒ1, ğœƒ2)]

ğ¾
âˆ‘

âˆ‘

=

ğ‘˜=ğ‘˜0

ğœƒğ‘— âˆˆÎ˜ğ‘—

ğ‘– (ğœƒğ‘—|ğ‘¥ğ‘˜, ğœƒğ‘–)
ğ‘ğ‘˜

âˆ‘

ğ‘– âˆˆğ´ğ‘˜
ğ‘ğ‘˜
ğ‘–

ğ‘– (ğ‘ğ‘˜
ğœğ‘˜

ğ‘– |ğ‘¥ğ‘˜, ğœƒğ‘–) â‹…

âˆ‘

ğ‘— âˆˆğ´ğ‘˜
ğ‘ğ‘˜
ğ‘—

ğ‘— (ğ‘ğ‘˜
ğœğ‘˜

ğ‘— |ğ‘¥ğ‘˜, ğœƒğ‘—)ğ½ ğ‘˜

ğ‘– (ğ‘¥ğ‘˜, ğ‘ğ‘˜

1, ğ‘ğ‘˜

2, ğœƒ1, ğœƒ2), ğ‘— â‰  ğ‘–.

(4)

3. PBNE and Dynamic Programming

The user and the defender use the Bayesian update to reduce their uncertainties on the other playerâ€™s type. Since
their actions aï¬€ect the belief update, both players at each stage should optimize their expected cumulative utilities
concerning the updated beliefs at the future stages, which leads to the Perfect Bayesian Nash Equilibrium (PBNE) in
Deï¬nition 1.

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 8 of 24

Proactive Defense against Advanced Persistent Threats

Deï¬nition 1. Consider the two-person ğ¾-stage game with double-sided incomplete information (i.e., each playerâ€™s
type is not known to the other player), a sequence of beliefs ğ‘ğ‘˜
ğ‘– , âˆ€ğ‘˜ âˆˆ {0, â‹¯ , ğ¾}, an expected cumulative utility ğ‘ˆ 0âˆ¶ğ¾
in (4), and a given scalar ğœ€ â‰¥ 0. A sequence of strategies ğœâˆ—,0âˆ¶ğ¾
ğ‘– is called ğœ€-dynamic Bayesian Nash
ğ‘–
equilibrium for player ğ‘– if condition (C2) is satisï¬ed. If condition (C1) is also satisï¬ed, ğœâˆ—,0âˆ¶ğ¾
is further called ğœ€-
perfect Bayesian Nash equilibrium.
(C1) Belief consistency: under strategy pair (ğœâˆ—,0âˆ¶ğ¾

), each playerâ€™s belief ğ‘ğ‘˜

ğ‘˜=0 Î£ğ‘˜

âˆˆ âˆğ¾

ğ‘– at each stage ğ‘˜ = 0, â‹¯ , ğ¾

, ğœâˆ—,0âˆ¶ğ¾
2

1

ğ‘–

ğ‘–

satisï¬es (2).

(C2) Sequential rationality: for all given initial state ğ‘¥ğ‘˜0 âˆˆ ğ‘‹ğ‘˜0 at every initial stage ğ‘˜0 âˆˆ {0, â‹¯ , ğ¾},

ğ‘ˆ ğ‘˜0âˆ¶ğ¾
1

(ğœâˆ—,ğ‘˜0âˆ¶ğ¾
1

, ğœâˆ—,ğ‘˜0âˆ¶ğ¾
2

, ğ‘¥ğ‘˜0, ğœƒ1) + ğœ€ â‰¥ ğ‘ˆ ğ‘˜âˆ¶ğ¾

1

(ğœğ‘˜0âˆ¶ğ¾
1

, ğœâˆ—,ğ‘˜0âˆ¶ğ¾
2

, ğ‘¥ğ‘˜0, ğœƒ1), âˆ€ğœğ‘˜0âˆ¶ğ¾

1

âˆˆ

ğ‘ˆ ğ‘˜0âˆ¶ğ¾
2

(ğœâˆ—,ğ‘˜0âˆ¶ğ¾
1

, ğœâˆ—,ğ‘˜0âˆ¶ğ¾
2

, ğ‘¥ğ‘˜0, ğœƒ2) + ğœ€ â‰¥ ğ‘ˆ ğ‘˜âˆ¶ğ¾

2

(ğœâˆ—,ğ‘˜0âˆ¶ğ¾
1

, ğœğ‘˜0âˆ¶ğ¾
2

, ğ‘¥ğ‘˜0, ğœƒ2), âˆ€ğœğ‘˜0âˆ¶ğ¾

2

âˆˆ

ğ¾
âˆ

ğ‘˜=0
ğ¾
âˆ

ğ‘˜=0

Î£ğ‘˜
1;

Î£ğ‘˜
2.

(5)

When ğœ€ = 0, the two ğœ€-equilibria are called Dynamic Bayesian Nash Equilibrium (DBNE) and Perfect Bayesian Nash
Equilibrium (PBNE), respectively.

The belief consistency emphasizes that when strategic players make long-term decisions, they have to consider
the impact of their actions on their opponentâ€™s beliefs at future stages. The PBNE is a reï¬nement of the DBNE
with the additional requirement of the belief consistency property. When the horizon ğ¾ = 0, the multi-stage game
of incomplete information deï¬ned in Section 2 degenerates to a one-stage (static) Bayesian game with the one-stage
2 ) and the solution concept of the DBNE/PBNE degenerates to the Static Bayesian Nash Equilibrium
belief pairs (ğ‘ğ¾
1
(SBNE) in Deï¬nition 2.

, ğ‘ğ¾

The sequential rationality property in (5) guarantees that unilateral deviations from the equilibrium at any states do
not beneï¬t the deviating player. Thus, the equilibrium strategy can be a reasonable prediction of both playersâ€™ multi-
stage behaviors. DBNE strategies have the property of strongly time consistency because (5) holds for any possible
initial states, even for states that are not on the equilibrium path, i.e., those states would not be visited under DBNE
strategies. The strongly time consistency property makes the DBNE adapt to unexpected changes. Solutions obtained
by dynamic programming naturally satisfy strongly time consistency. Hence, in the following, we introduce algorithms
based on dynamic programming techniques.

Deï¬ne the value function ğ‘‰ ğ‘˜0

(ğ‘¥ğ‘˜0, ğœƒğ‘–) âˆ¶= ğ‘ˆ ğ‘˜0âˆ¶ğ¾

ğ‘–

ğ‘–

ğ‘˜0 âˆˆ {0, â‹¯ , ğ¾} under the DBNE strategy pair (ğœâˆ—,ğ‘˜0âˆ¶ğ¾
player ğ‘– âˆˆ {1, 2} with type ğœƒğ‘–

at state ğ‘¥ğ¾ is

1

(ğœâˆ—,ğ‘˜0âˆ¶ğ¾
1

, ğœâˆ—,ğ‘˜0âˆ¶ğ¾
2
, ğœâˆ—,ğ‘˜0âˆ¶ğ¾
2

, ğ‘¥ğ‘˜0, ğœƒğ‘–) as the utility-to-go from any initial stage
). Then, at the ï¬nal stage ğ¾, the value function for

ğ‘‰ ğ¾
ğ‘–

(ğ‘¥ğ¾ , ğœƒğ‘–) = sup
ğ‘– âˆˆÎ£ğ¾
ğœğ¾
ğ‘–

ğ”¼ğœƒğ‘— âˆ¼ğ‘ğ¾

ğ‘– ,ğ‘ğ¾

ğ‘– âˆ¼ğœğ¾
ğ‘–

ğ‘— âˆ¼ğœâˆ—,ğ¾
,ğ‘ğ¾

ğ‘—

[ğ½ ğ¾

ğ‘– (ğ‘¥ğ¾ , ğ‘ğ¾

1 , ğ‘ğ¾

2 , ğœƒ1, ğœƒ2)].

(6)

For any feasible sequence of belief pairs (ğ‘ğ‘˜
1

, ğ‘ğ‘˜

tions for player ğ‘– to ï¬nd the equilibrium strategy pairs (ğœâˆ—,ğ‘˜
i.e., âˆ€ğ‘˜ âˆˆ {0, â‹¯ , ğ¾ âˆ’ 1}, âˆ€ğ‘–, ğ‘— âˆˆ {1, 2}, ğ‘— â‰  ğ‘–,

1

2), ğ‘˜ = 0, â‹¯ , ğ¾ âˆ’ 1, we have the following recursive system equa-
2 ) backwardly from stage ğ¾ âˆ’ 1 to the initial stage 0,

, ğœâˆ—,ğ‘˜

ğ‘– (ğ‘¥ğ‘˜, ğœƒğ‘–) = sup
ğ‘‰ ğ‘˜
ğ‘– âˆˆÎ£ğ‘˜
ğœğ‘˜
ğ‘–

ğ”¼ğœƒğ‘— âˆ¼ğ‘ğ‘˜

ğ‘– ,ğ‘ğ‘˜

ğ‘– âˆ¼ğœğ‘˜

ğ‘– ,ğ‘ğ‘˜

ğ‘— âˆ¼ğœâˆ—,ğ‘˜

ğ‘—

[ğ‘‰ ğ‘˜+1
ğ‘–

(ğ‘“ ğ‘˜(ğ‘¥ğ‘˜, ğ‘ğ‘˜

1, ğ‘ğ‘˜

2), ğœƒğ‘–) + ğ½ ğ‘˜

ğ‘– (ğ‘¥ğ‘˜, ğ‘ğ‘˜

1, ğ‘ğ‘˜

2, ğœƒ1, ğœƒ2)].

(7)

If we assume a virtual termination value ğ‘‰ ğ¾+1
2 ), ğœƒğ‘–) â‰¡ 0, we can obtain (6) by letting stage ğ‘˜ = ğ¾
(ğ‘“ ğ¾ (ğ‘¥ğ¾ , ğ‘ğ¾
1
in (7). The second term in (7) represents the immediate stage utility and the ï¬rst term represents the expected utility
aï¬€ects both terms, players should adopt a
under the future state ğ‘¥ğ‘˜+1 = ğ‘“ ğ‘˜(ğ‘¥ğ‘˜, ğ‘ğ‘˜
2), ğ‘˜ âˆˆ {0, â‹¯ , ğ¾ âˆ’ 1}. Since ğ‘ğ‘˜
1
long-term perspective and avoid myopic behaviors to balance between the immediate utility and the expected future
utility.

, ğ‘ğ¾

, ğ‘ğ‘˜

ğ‘–

ğ‘–

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 9 of 24

Proactive Defense against Advanced Persistent Threats

4. Computational Algorithms

In 4.1, we formulate a constrained optimization problem to compute the SBNE and ğ‘‰ ğ¾
for the one-stage game. In
ğ‘–
4.2, we use the proposed optimization problem as building blocks to compute the DBNE and ğ‘‰ ğ‘˜
ğ‘– , âˆ€ğ‘˜ âˆˆ {0, â‹¯ , ğ¾ âˆ’ 1}.
Finally, we propose an iterative algorithm to solve for the PBNE. Eï¬ƒcient algorithms to compute the PBNE lay a solid
foundation to quantify the risk of cyber-physical attacks and guide the design of proactive defense-in-depth strategies.

4.1. One-Stage Bayesian Game and SBNE

Since both playersâ€™ actions at the ï¬nal stage ğ‘˜ = ğ¾ only aï¬€ect the immediate utility ğ½ ğ¾
ğ‘–

and there is no future state
transition, we can treat the ï¬nal-stage game at each state ğ‘¥ğ¾ âˆˆ ğ‘‹ğ¾ as an equivalent one-stage Bayesian game with the
belief ğ‘ğ¾
ğ‘–

and obtain the SBNE.

Deï¬nition 2. A pair of mixed-strategies (ğœâˆ—,ğ¾
librium (SBNE) under the given belief pair (ğ‘ğ¾
1

1 âˆˆ Î£ğ¾
, ğ‘ğ¾

1

, ğœâˆ—,ğ¾

2 âˆˆ Î£ğ¾

2 ) is said to constitute a Static Bayesian Nash Equi-

2 ) and the state ğ‘¥ğ¾ âˆˆ ğ‘‹ğ¾ , if âˆ€ğœƒ1 âˆˆ Î˜1, ğœƒ2 âˆˆ Î˜2,

1

ğ”¼ğœƒ2âˆ¼ğ‘ğ¾
ğ”¼ğœƒ1âˆ¼ğ‘ğ¾

2

,ğ‘ğ¾
1

âˆ¼ğœâˆ—,ğ¾
1

,ğ‘ğ¾
2

âˆ¼ğœâˆ—,ğ¾
2

,ğ‘ğ¾
1

âˆ¼ğœâˆ—,ğ¾
1

,ğ‘ğ¾
2

âˆ¼ğœâˆ—,ğ¾
2

[ğ½ ğ¾

[ğ½ ğ¾

1 (ğ‘¥ğ¾ , ğ‘ğ¾
2 (ğ‘¥ğ¾ , ğ‘ğ¾

1 , ğ‘ğ¾
1 , ğ‘ğ¾

2 , ğœƒ1, ğœƒ2)] â‰¥ ğ”¼ğœƒ2âˆ¼ğ‘ğ¾
2 , ğœƒ1, ğœƒ2)] â‰¥ ğ”¼ğœƒ1âˆ¼ğ‘ğ¾

1

2

,ğ‘ğ¾
1

âˆ¼ğœğ¾
1

,ğ‘ğ¾
2

âˆ¼ğœâˆ—,ğ¾
2

,ğ‘ğ¾
1

âˆ¼ğœâˆ—,ğ¾
1

,ğ‘ğ¾
2

âˆ¼ğœğ¾
2

[ğ½ ğ¾

1 (ğ‘¥ğ¾ , ğ‘ğ¾
2 (ğ‘¥ğ¾ , ğ‘ğ¾

1 , ğ‘ğ¾
1 , ğ‘ğ¾

2 , ğœƒ1, ğœƒ2)], âˆ€ğœğ¾
2 , ğœƒ1, ğœƒ2)], âˆ€ğœğ¾

1 âˆˆ Î£ğ¾
1 ;
2 âˆˆ Î£ğ¾
2 .

[ğ½ ğ¾

(8)

In Theorem 1, we propose a constrained optimization program ğ¶ ğ¾ to compute the SBNE. We suppress the superscript
of ğ¾ without any ambiguity in one-stage games.

Theorem 1. A strategy pair (ğœâˆ—
2 âˆˆ Î£2) constitutes a SBNE to the one-stage bi-matrix Bayesian game (ğ½1, ğ½2)
under private type ğœƒğ‘– âˆˆ Î˜ğ‘–, âˆ€ğ‘– âˆˆ {1, 2}, belief ğ‘ğ‘–, âˆ€ğ‘– âˆˆ {1, 2}, and a given state ğ‘¥, if and only if the strategy pair is a
solution to ğ¶ ğ¾ :

1 âˆˆ Î£1, ğœâˆ—

[ğ¶ ğ¾ ] âˆ¶ max

ğœ1,ğœ2,ğ‘ 1,ğ‘ 2
âˆ‘

+

+

ğœƒ1âˆˆÎ˜1
âˆ‘

âˆ‘

ğ›¼1(ğœƒ1)ğ‘ 1(ğ‘¥, ğœƒ1) +

âˆ‘

ğ›¼2(ğœƒ2)ğ‘ 2(ğ‘¥, ğœƒ2)

ğœƒ1âˆˆÎ˜1
ğ›¼1(ğœƒ1)ğ”¼ğœƒ2âˆ¼ğ‘1,ğ‘1âˆ¼ğœ1,ğ‘2âˆ¼ğœ2

ğœƒ2âˆˆÎ˜2
[ğ½1(ğ‘¥, ğ‘1, ğ‘2, ğœƒ1, ğœƒ2)]

ğ›¼2(ğœƒ2)ğ”¼ğœƒ1âˆ¼ğ‘2,ğ‘1âˆ¼ğœ1,ğ‘2âˆ¼ğœ2

[ğ½2(ğ‘¥, ğ‘1, ğ‘2, ğœƒ1, ğœƒ2)]

s.t.

[ğ½2(ğ‘¥, ğ‘1, ğ‘2, ğœƒ1, ğœƒ2)] â‰¤ âˆ’ğ‘ 2(ğ‘¥, ğœƒ2), âˆ€ğœƒ2, âˆ€ğ‘2,

ğœ1(ğ‘1|ğ‘¥, ğœƒ1) = 1, ğœ1(ğ‘1|ğ‘¥, ğœƒ1) â‰¥ 0, âˆ€ğœƒ1,

ğœƒ2âˆˆÎ˜2
(ğ‘) ğ”¼ğœƒ1âˆ¼ğ‘2,ğ‘1âˆ¼ğœ1
âˆ‘
(ğ‘)

ğ‘1âˆˆğ´1
(ğ‘) ğ”¼ğœƒ2âˆ¼ğ‘1,ğ‘2âˆ¼ğœ2
âˆ‘
(ğ‘‘)

[ğ½1(ğ‘¥, ğ‘1, ğ‘2, ğœƒ1, ğœƒ2)] â‰¤ âˆ’ğ‘ 1(ğ‘¥, ğœƒ1), âˆ€ğœƒ1, âˆ€ğ‘1,

ğœ2(ğ‘2|ğ‘¥, ğœƒ2) = 1, ğœ2(ğ‘2|ğ‘¥, ğœƒ2) â‰¥ 0, âˆ€ğœƒ2.

ğ‘2âˆˆğ´2

The dimensions of decision variables ğœ1(ğ‘1|ğ‘¥, ğœƒ1), âˆ€ğœƒ1 âˆˆ Î˜1, and ğœ2(ğ‘2|ğ‘¥, ğœƒ2), âˆ€ğœƒ2 âˆˆ Î˜2, are |ğ´1| Ã— |Î˜1| and |ğ´2| Ã—
|Î˜2|, respectively. Besides, ğ‘ 1(ğ‘¥, ğœƒ1), âˆ€ğœƒ1 and ğ‘ 2(ğ‘¥, ğœƒ2), âˆ€ğœƒ2 are scalar decision variables for each given ğœƒğ‘–, ğ‘– âˆˆ {1, 2}.
The non-decision variables ğ›¼1(ğœƒ1), âˆ€ğœƒ1 and ğ›¼2(ğœƒ2), âˆ€ğœƒ2, can be any strictly positive and ï¬nite numbers. The solution
to ğ¶ ğ¾ exists and is achieved at the equality of constraints (ğ‘), (ğ‘), i.e., ğ‘ âˆ—
1(ğ‘¥, ğœƒ1) = âˆ’ğ‘‰1(ğ‘¥, ğœƒ1).
PROOF. The ï¬niteness and discreteness of the action and the type spaces guarantee the existence of the SBNE in mixed
strategies as shown in Shoham and Leyton-Brown (2008), which further guarantee that program ğ¶ ğ¾ has solutions. To
show the equivalence between the solution to ğ¶ ğ¾ and the SBNE, we ï¬rst show that every SBNE is a solution of
ğ¶ ğ¾ . If (ğœâˆ—
1(ğ‘¥, ğœƒ1) =
âˆ’ğ‘‰1(ğ‘¥, ğœƒ1), âˆ€ğœƒğ‘– âˆˆ Î˜ğ‘–, âˆ€ğ‘– âˆˆ {1, 2}, is feasible because it satisï¬es constraints (ğ‘), (ğ‘), (ğ‘), (ğ‘‘). Constraints (ğ‘) and (ğ‘)
imply a non-positive objective function of ğ¶ ğ¾ . Since the value of the objective function achieved under this quadruple

2 âˆˆ Î£2) is a SBNE pair, then the quadruple ğœâˆ—

2(ğ‘¥, ğœƒ2) = âˆ’ğ‘‰2(ğ‘¥, ğœƒ2), ğ‘ âˆ—

2(ğ‘¥, ğœƒ2) = âˆ’ğ‘‰2(ğ‘¥, ğœƒ2), ğ‘ âˆ—

1 âˆˆ Î£1, ğœâˆ—

1 (ğœƒ1), ğœâˆ—

2 (ğœƒ2), ğ‘ âˆ—

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 10 of 24

Proactive Defense against Advanced Persistent Threats

is 0, this quadruple is also optimal. Second, we show that ğœâˆ—
SBNE. The solution of ğ¶ ğ¾ should satisfy all the constraints, i.e.,

1 (ğœƒ1), ğœâˆ—

2 (ğœƒ2), ğ‘ âˆ—

2(ğ‘¥, ğœƒ2), ğ‘ âˆ—

1(ğ‘¥, ğœƒ1), the result of ğ¶ ğ¾ is a

ğ”¼ğœƒ1âˆ¼ğ‘2,ğ‘1âˆ¼ğœâˆ—
,ğ‘2âˆ¼ğœ2
ğ”¼ğœƒ2âˆ¼ğ‘1,ğ‘1âˆ¼ğœ1,ğ‘2âˆ¼ğœâˆ—

1

2

[ğ½2(ğ‘¥, ğ‘1, ğ‘2, ğœƒ1, ğœƒ2)] â‰¤ âˆ’ğ‘ âˆ—
[ğ½2(ğ‘¥, ğ‘1, ğ‘2, ğœƒ1, ğœƒ2)] â‰¤ âˆ’ğ‘ âˆ—

2(ğ‘¥, ğœƒ2), âˆ€ğœƒ2, âˆ€ğœ2 âˆˆ Î£2,
1(ğ‘¥, ğœƒ1), âˆ€ğœƒ1, âˆ€ğœ1 âˆˆ Î£1.

(9)

In particular, if we pick ğœğ‘–(ğœƒğ‘–) = ğœâˆ—
0, the inequality turns out to be an equality and equation (9) becomes (8), which shows that (ğœâˆ—
SBNE.

ğ‘– (ğœƒğ‘–), âˆ€ğœƒğ‘–, âˆ€ğ‘– âˆˆ {1, 2}, and combine the fact that the optimal value is achieved at
2 âˆˆ Î£2) is a
â–¡

1 âˆˆ Î£1, ğœâˆ—

Theorem 1 focuses on the double-sided Bayesian game where each player player ğ‘– has a private type ğœƒğ‘– âˆˆ Î˜ğ‘–

accommodate the one-sided Bayesian game where player ğ‘–â€™s type ğœƒğ‘– âˆˆ Î˜ğ‘–
remains unknown to player ğ‘–, we can modify program ğ¶ ğ¾ by letting ğ›¼ğ‘–(ğœƒğ‘–) > 0 and ğ›¼ğ‘–( Ìƒğœƒğ‘–) = 0, âˆ€ Ìƒğœƒğ‘– âˆˆ Î˜ğ‘– â§µ {ğœƒğ‘–}.

. To
is known by both players and player ğ‘—â€™s type

4.2. Multi-stage Bayesian Game and PBNE

and the
From (7), we can see that at stages ğ‘˜ < ğ¾, each player optimizes the sum of the immediate utility ğ½ ğ‘˜
ğ‘–
in program ğ¶ ğ‘˜ to

in program ğ¶ ğ¾ with ğ‘‰ ğ‘˜

utility-to-go ğ‘‰ ğ‘˜
ğ‘–
compute the DBNE in a multi-stage Bayesian game.

. Thus, we can replace the original stage utility ğ½ ğ¾
ğ‘–

ğ‘– + ğ½ ğ‘˜
ğ‘–

, ğœâˆ—,0âˆ¶ğ¾âˆ’1

Theorem 2. Given a sequence of beliefs ğ‘ğ‘˜
(ğœâˆ—,0âˆ¶ğ¾âˆ’1
with the expected cumulative utility ğ‘ˆ 0âˆ¶ğ¾
solutions to the following constrained optimization problem ğ¶ ğ‘˜ for each ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾ âˆ’ 1}:

ğ‘– for each player ğ‘– âˆˆ {1, 2} at each stage ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾âˆ’1}, a strategy pair
) constitutes a DBNE of the ğ¾-stage Bayesian game under double-sided incomplete information
2 (ğ‘¥ğ‘˜, ğœƒ2) are the optimal

in (4), if, and only if ğœâˆ—,ğ‘˜

1 (ğ‘¥ğ‘˜, ğœƒ1), ğ‘ âˆ—,ğ‘˜

, ğœâˆ—,ğ‘˜
2

, ğ‘ âˆ—,ğ‘˜

2

1

1

ğ‘–

[ğ¶ ğ‘˜] âˆ¶ max
,ğ‘ ğ‘˜
ğœğ‘˜
1
1

,ğœğ‘˜
2

,ğ‘ ğ‘˜
2

2
âˆ‘

âˆ‘

ğ›¼ğ‘–(ğœƒğ‘–){ğ‘ ğ‘˜

ğ‘– (ğ‘¥ğ‘˜, ğœƒğ‘–) +

âˆ‘

ğ‘– (ğœƒğ‘—|ğ‘¥ğ‘˜, ğœƒğ‘–)
ğ‘ğ‘˜

â‹… [ğ½ ğ‘˜

ğ‘–=1
1, ğ‘ğ‘˜
ğ‘– (ğ‘¥ğ‘˜, ğ‘ğ‘˜
âˆ‘
2(ğœƒ1|ğ‘¥ğ‘˜, ğœƒ2)
ğ‘ğ‘˜

ğœƒğ‘— âˆˆÎ˜ğ‘—
ğœƒğ‘–âˆˆÎ˜ğ‘–
(ğ‘“ ğ‘˜(ğ‘¥ğ‘˜, ğ‘ğ‘˜
1, ğ‘ğ‘˜
2, ğœƒ1, ğœƒ2) + ğ‘‰ ğ‘˜+1
1|ğ‘¥ğ‘˜, ğœƒ1) â‹… [ğ½ ğ‘˜
1 (ğ‘ğ‘˜
ğœğ‘˜

âˆ‘

ğ‘–

2), ğœƒğ‘–)]}
2 (ğ‘¥ğ‘˜, ğ‘ğ‘˜

1, ğ‘ğ‘˜

2, ğœƒ1, ğœƒ2) + ğ‘‰ ğ‘˜+1

2

âˆ‘

ğ‘ğ‘˜
1

âˆˆğ´ğ‘˜
1

1 (ğ‘ğ‘˜
ğœğ‘˜

1|ğ‘¥ğ‘˜, ğœƒ1)

âˆ‘

ğ‘ğ‘˜
2

âˆˆğ´ğ‘˜
2

2 (ğ‘ğ‘˜
ğœğ‘˜

2|ğ‘¥ğ‘˜, ğœƒ2)

s.t. (ğ‘)

(ğ‘)

ğœƒ1âˆˆÎ˜1
â‰¤ âˆ’ğ‘ ğ‘˜
âˆ‘

ğœƒ2âˆˆÎ˜2
â‰¤ âˆ’ğ‘ ğ‘˜

ğ‘ğ‘˜
1

âˆˆğ´ğ‘˜
1

ğ‘ğ‘˜
2

âˆˆğ´ğ‘˜
2

1(ğ‘¥ğ‘˜, ğœƒ1), âˆ€ğœƒ1 âˆˆ Î˜1, âˆ€ğ‘ğ‘˜

1 âˆˆ ğ´ğ‘˜
1.

2 âˆˆ ğ´ğ‘˜
2(ğ‘¥ğ‘˜, ğœƒ2), âˆ€ğœƒ2 âˆˆ Î˜2, âˆ€ğ‘ğ‘˜
2,
2|ğ‘¥ğ‘˜, ğœƒ2) â‹… [ğ½ ğ‘˜
2 (ğ‘ğ‘˜
ğœğ‘˜
1(ğœƒ2|ğ‘¥ğ‘˜, ğœƒ1)
ğ‘ğ‘˜

âˆ‘

1 (ğ‘¥ğ‘˜, ğ‘ğ‘˜

1, ğ‘ğ‘˜

2, ğœƒ1, ğœƒ2) + ğ‘‰ ğ‘˜+1

1

(ğ‘“ ğ‘˜(ğ‘¥ğ‘˜, ğ‘ğ‘˜

1, ğ‘ğ‘˜

2), ğœƒ2)]

(ğ‘“ ğ‘˜(ğ‘¥ğ‘˜, ğ‘ğ‘˜

1, ğ‘ğ‘˜

2), ğœƒ1)]

Similarly, ğ›¼1(ğœƒ1), ğ›¼2(ğœƒ2) can be any strictly positive and ï¬nite numbers, and (ğ‘ ğ‘˜
2(ğ‘¥ğ‘˜, ğœƒ2)) is a sequence of
scalar variables for each ğ‘¥ğ‘˜ âˆˆ ğ‘‹ğ‘˜, ğœƒğ‘– âˆˆ Î˜ğ‘–, ğ‘– âˆˆ {1, 2}. The optimum exists and is achieved at the equality of constraints
(ğ‘), (ğ‘), i.e., ğ‘ âˆ—,ğ‘˜

1(ğ‘¥ğ‘˜, ğœƒ1), ğ‘ ğ‘˜

ğ‘– (ğ‘¥ğ‘˜, ğœƒğ‘–), âˆ€ğœƒğ‘– âˆˆ Î˜ğ‘–, âˆ€ğ‘– âˆˆ {1, 2}.

(ğ‘¥ğ‘˜, ğœƒğ‘–) = âˆ’ğ‘‰ ğ‘˜

ğ‘–

ğ‘–

The proof is similar to the one for Theorem 1. The decision variables ğœğ‘˜
ğ‘–

stage ğ‘˜ = ğ¾ and ğ‘‰ ğ¾+1
Bayesian game. We can solve program ğ¶ ğ‘˜+1 to obtain the DBNE strategy pair (ğœğ‘˜+1
Then, we apply ğ‘‰ ğ‘˜+1
ğ‘–
sequences of type belief pairs ğ‘ğ‘˜
to obtain the DBNE pair (ğœâˆ—,0âˆ¶ğ¾âˆ’1

. By letting
= 0, program ğ¶ ğ¾ for the static Bayesian game is a special case of ğ¶ ğ‘˜ for the multi-stage
) and the value of ğ‘‰ ğ‘˜+1
.
. Thus, for any given
, ğœğ‘˜
ğ‘– , âˆ€ğ‘– âˆˆ {1, 2}, âˆ€ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾}, we can solve ğ¶ ğ‘˜ from ğ‘˜ = ğ¾ to ğ‘˜ = 0 recursively

in program ğ¶ ğ‘˜ to obtain a DBNE strategy pair (ğœğ‘˜
1

2 ) and the value of ğ‘‰ ğ‘˜

ğ‘– | Ã— |ğ‘‹ğ‘˜

, ğœâˆ—,0âˆ¶ğ¾âˆ’1

are of size

| Ã— |Î˜ğ‘–|

, ğœğ‘˜+1
2

|ğ´ğ‘˜

).

1

ğ‘–

ğ‘–

1

2

4.2.1. PBNE

Given a sequence of beliefs, we can obtain the corresponding DBNE via ğ¶ ğ‘˜ in a backward fashion. However,
given a sequence of policies, both players forwardly update their beliefs at each stage by (2). Thus, we need to ï¬nd a
consistent pair of belief and policy sequences as required by the PBNE. As summarized in Algorithm 1, we iteratively

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 11 of 24

Proactive Defense against Advanced Persistent Threats

Algorithm 1: Numerical Solution of ğœ€-PBNE
1 Initialization beliefs ğ‘ğ‘˜
ğ‘–
2 while the ğ‘¡ <ITERNUM do
3

ğ‘¡ âˆ¶= ğ‘¡ + 1;
for each ğ‘¥ğ¾ âˆˆ ğ‘‹ğ¾ do

at each stage ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾}, ITERNUM> 0, ğœ€ â‰¥ 0.

4

5

6

7

8

9

10

11

12

13

14

15

Compute SBNE strategy ğœâˆ—,ğ¾

ğ‘–

and ğ‘‰ ğ¾
ğ‘–

(ğ‘¥ğ¾ , ğœƒğ‘–) via ğ¶ ğ¾ .

end
for ğ‘˜ â† ğ¾ âˆ’ 1 to 0 do

for each ğ‘¥ğ‘˜ âˆˆ ğ‘‹ğ‘˜ do

Compute DBNE strategy ğœâˆ—,ğ‘˜

ğ‘–

and ğ‘‰ ğ‘˜

ğ‘– (ğ‘¥ğ‘˜, ğœƒğ‘–) via ğ¶ ğ‘˜.

end

end
for ğ‘˜ â† 0 to ğ¾ âˆ’ 1 do

Update ğ‘ğ‘˜
ğ‘–

with ğœâˆ—,0âˆ¶ğ¾âˆ’1
ğ‘–

via (2).

end
if ğœâˆ—,0âˆ¶ğ¾âˆ’1
ğ‘–
Terminate

, âˆ€ğ‘– âˆˆ {1, 2}, satisfy (5) then

16
17 end
18 Output ğœ€-PBNE strategy pair (ğœâˆ—,0âˆ¶ğ¾âˆ’1

1

, ğœâˆ—,0âˆ¶ğ¾âˆ’1

2

) and consistent beliefs ğ‘ğ‘˜

ğ‘– , âˆ€ğ‘˜ âˆˆ {0, â‹¯ , ğ¾}.

alternate between the forward belief update and the backward policy computation to ï¬nd the PBNE. We resort to
ğœ€-PBNE solutions when the existence of PBNE is not guaranteed.

Algorithm 1 provides a computational approach to ï¬nd ğœ€-PBNE with the following procedure. First, both players
for every state ğ‘¥ğ‘˜ at stage ğ‘˜ âˆˆ {0, 1, â‹¯ , ğ¾}, according to their types. Then, they compute
initialize their beliefs ğ‘ğ‘˜
ğ‘–
the DBNE strategy pair ğœâˆ—,0âˆ¶ğ¾
, âˆ€ğ‘– âˆˆ {1, 2}, under the given belief sequence at each stage by solving program ğ¶ ğ‘˜
from stage ğ¾ to stage 0 in sequence. Next, they update their beliefs at each stage according to the strategy pair
ğœâˆ—,0âˆ¶ğ¾âˆ’1
, âˆ€ğ‘– âˆˆ {1, 2}, satisï¬es (5) under the
ğ‘–
updated belief, we ï¬nd the ğœ€-PBNE and terminate the iteration. Otherwise, we repeat the backward policy computation
in step two and the forward belief update in step three.

, âˆ€ğ‘– âˆˆ {1, 2}, via the Bayesian update (2). If the strategy pair ğœâˆ—,0âˆ¶ğ¾âˆ’1

ğ‘–

ğ‘–

5. Case Study

The model presented in Section 2 can be applied to various APT scenarios. To illustrate the framework, this
section presents a speciï¬c attack scenario where the attacker stealthily initiates infection and escalates privileges in the
cyber network, aiming to launch attacks on the physical plant as shown in Fig. 3. Three vertical columns in the left
block illustrate the state transitions across three stages: the initial compromise, the privilege escalation, and the sensor
compromise of a physical system. The red squares at each column represent possible states at that stage. The right
block illustrates a simpliï¬ed ï¬‚ow chart of the Tennessee Eastman Process. We use the Tennessee Eastman process as
a benchmark of industrial control systems to show that attackers can strategically compromise the SCADA system and
decrease the operational eï¬ƒciency of a physical plant without triggering the alarm.
2 } and Î˜1 = {ğœƒğ»

1 } for the user and the defender,
denote the
and ğœƒğ¿
respectively. In particular, ğœƒğ‘
1
2
sophisticated and primitive defender, respectively. The bi-matrices in Table 2, 3, and 4 represent both playersâ€™ expected
utilities at three stages, respectively. In these matrices, the defender is the row player and the user is the column player.
Each entry of the matrix corresponds to playersâ€™ payoï¬€s under their action pairs, types, and the state. In particular, the
two red numbers in the parenthesis before the semicolon are the payoï¬€s of the defender and the user, respectively, under
, while the parenthesis in blue after the semicolon presents the payoï¬€ of the defender and the user, respectively,
type ğœƒğ‘
2
under type ğœƒğ‘”
2

In this case study, we adopt the binary type space Î˜2 = {ğœƒğ‘
2
and ğœƒğ‘”
2

denote the adversarial and legitimate user, respectively; ğœƒğ»
1

, ğœƒğ¿

, ğœƒğ‘”

1

.

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 12 of 24

Proactive Defense against Advanced Persistent Threats

Figure 3: The diagram of the cyber state transition (denoted by the left block in orange) and the physical attack on
Tennessee Eastman process via the compromise of the SCADA system (denoted by the right block in blue). APTs
can damage the normal industrial operation by falsifying controllersâ€™ setpoints, tampering sensor readings, and blocking
communication channels to cause delays in either the control message or the sensing data.

5.1. Initial Stage: Phishing Emails

We use a binary set to represent whether the reconnaissance is eï¬€ectual ğ‘¥0 = 1 or not ğ‘¥0 = 0. Eï¬€ectual recon-
naissance collects essential intelligence that can better support APTs for an initial entry through phishing emails. To
penalize the adversarial exploitation of the open-source intelligence (OSINT) data, the defender can create avatars
(fake personal proï¬les) on the social network or the company website as shown in Molok, Chang and Ahmad (2010).
At the initial stage of interaction, a user can send emails with non-executable attachments and shortened URLs to
the accounts of entry-level employees, managers, or avatars. These three action options of the user are represented
by ğ‘0
2 = 0, 1, 2, respectively. Non-executable ï¬les such as PDF and MS Oï¬ƒce are widely used in organizations
yet an APT attacker can exploit them to execute malicious actions on the victimâ€™s computer. The shortened URL
is created by legitimate service providers such as Google URL shortener yet can redirect to malicious links. The
existing email security mechanisms are not completely eï¬€ective for identifying malicious PDF ï¬les (see Nissim et al.
(2015)) and malicious links behind shortened URLs (see Sahoo, Liu and Hoi (2017)). As a supplement to technical
countermeasures, security training should be emphasized to increase employeesâ€™ security awareness and protect them
from web phishing. For example, after receiving suspicious links or attachments with strange names at unexpected
times, the entry-level employee and the manager should be aware of the potential risk and apply extra security measures
such as a digital signature request from the sender before clicking the link or opening the attachment. They should
also be suï¬ƒciently alert and report immediately if a PDF does not contain the information that it claims to have. Then
isolation can be applied to prevent the attacker from the potential lateral movement. Since employeesâ€™ awareness and
alertness diminish over time, the security training needs to be repeated at reasonable intervals as argued in Mitnick and
Simon (2011), which can be costly. With a limited budget, the defender can choose to educate entry-level employees,
manager-level employees, or no training to avoid the prohibitive training cost ğ‘0. These three action options of the
1 = 1, 2, 0, respectively. The utility matrix of the initial infection is given in Table 2.
defender are represented by ğ‘0
If the user is legitimate, i.e., ğœƒ2 = ğœƒğ‘”
if he
, then as denoted in the blue color, he receives an immediate reward ğ‘Ÿ0
1
successfully communicates with the employee or the manager by email, but receives a substantial penalty ğ‘Ÿ0
ğ‘”,ğ‘“ < 0 if
he emails the avatars because he should not contact a non-existing person. If the user is adversarial, i.e., ğœƒ2 = ğœƒğ‘
, then
2
if the email receiver does not have proper security
as denoted in the red color, he receives an immediate attack reward ğ‘Ÿ0
2
training, but an additional attack cost ğ‘Ÿ0 if the receiver has been trained properly. The adversarial user receives a faked
ğ‘,ğ‘“ > 0 when contacting the avatar, yet arrives at an unfavorable state at stage ğ‘˜ = 1 and receives few rewards
reward ğ‘Ÿ0
in the future stages. The training cost and the attack cost are both diï¬€erent for the primitive and the sophisticated

2

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 13 of 24

!"=0!"=1!&=1!&=2!&=3!&=0Attack	Physical	Plant	via	Cyber	ComponentsInitial	StageFinal	StageIntermediate	StageStripperReactorCondenserSeparatorCompressorG&HSensorADE)(+)+.(+)+/(+)	â†’	2(345)	)(+)+.(+)+6(+)	â†’	7(345)	CRecycleflowControllerTennessee	Eastman	Challenge	ProcessCyberNetwork	Transition	(K=2)!8=1!8=2!8=09:9;Privilege	Level	IntelligenceValidityLocationProactive Defense against Advanced Persistent Threats

Table 2
The expected utilities of the defender and the user at the initial stage, i.e., ğ½ 0

1 and ğ½ 0

2 , respectively.

2;ğœƒğ‘”
ğœƒğ‘

2

No
Training

Train
Employees

Train
Managers

Email
Employees

Email Managers

Email Avatars

(âˆ’ğ‘Ÿ0

2, ğ‘Ÿ0

2);(0, ğ‘Ÿ0
1)

(âˆ’ğ‘Ÿ0

2, ğ‘Ÿ0

2);(0, ğ‘Ÿ0
1)

(âˆ’ğ‘0, âˆ’ğ‘Ÿ0);(âˆ’ğ‘0, ğ‘Ÿ0
1)

(âˆ’ğ‘0, ğ‘Ÿ0

2);(âˆ’ğ‘0, ğ‘Ÿ0
1)

(âˆ’ğ‘0, ğ‘Ÿ0

2);(âˆ’ğ‘0, ğ‘Ÿ0
1)

(âˆ’ğ‘0, âˆ’ğ‘Ÿ0);(âˆ’ğ‘0, ğ‘Ÿ0
1)

(0, ğ‘Ÿ0

ğ‘,ğ‘“ );(0, ğ‘Ÿ0

ğ‘”,ğ‘“ )

(âˆ’ğ‘0, ğ‘Ÿ0

ğ‘,ğ‘“ );(âˆ’ğ‘0, ğ‘Ÿ0

ğ‘”,ğ‘“ )

(âˆ’ğ‘0, ğ‘Ÿ0

ğ‘,ğ‘“ );(âˆ’ğ‘0, ğ‘Ÿ0

ğ‘”,ğ‘“ )

Table 3
The expected utilities of the defender and the user at the intermediate stage, i.e., ğ½ 1

1 and ğ½ 1

2 , respectively.

2;ğœƒğ‘”
ğœƒğ‘

2

NOP

Escalate Privilege

Permit Escalation

(0, 0);(0, 0)

(âˆ’ğ‘Ÿ1

2, ğ‘Ÿ1

2);(ğ‘Ÿ1

1, ğ‘Ÿ1
1)

Restrict Escalation

(0, 0);(0, 0)

(ğ‘Ÿ1, âˆ’ğ‘Ÿ1);(âˆ’ğ‘Ÿ1

1, âˆ’ğ‘Ÿ1
1)

defender, i.e., ğ‘0 âˆ¶= ğ‘0
ğ» â‹… ğŸ{ğœƒ1=ğœƒğ»
holds the security training with a higher frequency, which incurs a higher cost, i.e., ğ‘0
.
in mitigating web phishing, i.e., ğ‘Ÿ0

and ğ‘Ÿ0 âˆ¶= ğ‘Ÿ0

ğ¿ â‹… ğŸ{ğœƒ1=ğœƒğ¿

ğ¿ â‹… ğŸ{ğœƒ1=ğœƒğ¿

} + ğ‘0

} + ğ‘Ÿ0

ğ» â‹… ğŸ{ğœƒ1=ğœƒğ»

}
ğ» > ğ‘0
ğ¿

}

1

1

1

1

ğ» > ğ‘Ÿ0
ğ¿

. The sophisticated defender

, but is also more eï¬€ective

5.2. Intermediate Stage: Privilege Escalation

The state at the intermediate stage can be interpreted as the location of the user where ğ‘¥1 = 1 refers to the employeeâ€™s
computer, ğ‘¥1 = 2 refers to the managerâ€™s computer, and ğ‘¥1 = 0 refers to the quarantine area. After the initial access,
the user operates within a process of low privilege. To access certain resources, the user needs to gain higher-level
privileges. An attacker can utilize the process injection to execute malicious code in the address space of a live process
and masquerade as legitimate programs to evade detection as shown in Team (2017). A mitigation method for the
defender is to prevent certain endpoint behaviors that can occur during the process injection. Table 3 presents this
game of privilege escalation.

2 = 1 and ğ‘1

The user can choose to escalate his privileges, or choose â€˜no operation performed (NOP)â€™. The two action options
are denoted by ğ‘1
2 = 0, respectively. The defender can choose to either restrict or permit an escalation,
which are denoted by ğ‘1
1 = 1 and ğ‘1
1 = 0, respectively. If the legitimate user escalates his privilege and the defender
. If the legitimate user escalates his privilege and the defender
permits escalation, then both players obtain a reward of ğ‘Ÿ1
1
restricts escalation, then the eï¬ƒciency reduction brings a loss of ğ‘Ÿ1
to both players. On the other hand, if the adversarial
1
. If the adversarial
user escalates his privilege and the defender permits escalation, the defender receives a loss of ğ‘Ÿ1
2
user escalates his privilege and the defender restricts escalation, then the adversarial user has to resort to other attack
techniques which lead to a higher rate of detection. Thus, the defender obtains a reward while the attacker receives an
additional cost. We assume that the reward and the additional cost are both ğ‘Ÿ1
if
ğ¿
the defender is sophisticated, i.e., ğ‘Ÿ1 = ğ‘Ÿ1

if the defender is primitive, and ğ‘Ÿ1
ğ»

.

ğ¿ â‹… ğŸ{ğœƒ1=ğœƒğ¿

1

} + ğ‘Ÿ1

ğ» â‹… ğŸ{ğœƒ1=ğœƒğ»

}

1

5.3. Final Stage: Sensor Compromise

The state at the ï¬nal stage represents four possible privilege levels, denoted by ğ‘¥2 = {0, 1, 2, 3}, respectively. The
privilege level aï¬€ects the result of the physical attack at the ï¬nal stage. The defenderâ€™s and the userâ€™s actions, and the
state at the intermediate stage determine the state at the ï¬nal stage. For example, if the user is at the quarantine area
during the intermediate stage, then he ends up with a level-zero privilege regardless of actions taken by the defender
and himself. Users who take control of the managerâ€™s computer at the intermediate stage can obtain a higher privilege
level than those who start from the entry-level employeeâ€™s computer, yet the degree of escalation is reduced if the

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 14 of 24

Proactive Defense against Advanced Persistent Threats

defender chooses to restrict escalation.

We modify the Simulink model in Bathelt, Ricker and Jelali (2015) to quantify the monetary loss of the Tennessee
Eastman process under sensor compromises. Our attack model of sensor compromise is presented in Section 5.3.2. A
new performance metric to quantify the operational eï¬ƒciency of the Tennessee Eastman process is proposed in Section
5.3.1 and applied in the game matrix in Section 5.3.3.

5.3.1. Performance Metric

The Tennessee Eastman process involves two irreversible reactions to produce two liquid (liq) products ğº, ğ» from
four gaseous (g) reactants ğ´, ğ¶, ğ·, ğ¸ as shown in the right block of Fig. 3. The control objective is to maintain a desired
production rate as well as quality while stabilizing the whole system under the Gaussian noise to avoid violating safety
constraints such as a high reactor pressure, a high reactor temperature, and a high/low separator/stripper liquid level.
Previous studies on the security of the Tennessee Eastman process have mostly focused on how an attacker can cause
the shortest shutdown time (see Krotoï¬l and CÃ¡rdenas (2013)), or a serious violation of a setpoint, e.g., the reactor
pressure exceeds 3, 000 kpa (see CÃ¡rdenas, Amin, Lin, Huang, Huang and Sastry (2011)). These attacks successfully
cause the shutdown of the plant and a few days of shutdowns can incur a considerable ï¬nancial loss. However, the
shutdown also discloses the attack and leads to an immediate patch and a defense strategy update. Thus, it becomes
harder for the same kind of attacks to succeed after the plant recovers from the shutdown.

In our APT scenario, the attacker aims to stealthily decrease the operational eï¬ƒciency of the plant, i.e., deviate the
normal operation state of the plant without triggering the safety alarm or shutting down the plant. By compromising
the SCADA system and generating fraudulent sensor readings, the attacker can stealthily make the plant operates at a
non-optimal state with reduced utilities. The following economic metrics aï¬€ect the operational utility of the Tennessee
Eastman process:

â€¢ Hourly operating cost ğ¶ğ‘œ

costs, and stripper steam costs.

with the unit ($âˆ•â„) is taken as the sum of purge costs, product stream costs, compressor

â€¢ Production rate ğ‘…ğ‘
â€¢ Quality of products ğ‘„ğ‘

with the unit (ğ‘š3âˆ•â„) is the volume of total products per hour.

with the unit (ğº ğ‘šğ‘œğ‘™ğ‘’%), is the percentage of ğº among total products.

with the unit ($âˆ•ğ‘š3) is the price of product ğº.

â€¢ ğ‘ƒğº
We propose a new performance metric ğ‘ˆğ‘‡ ğ¸

nessee Eastman process as follows:

, the per-hour utility to quantify the operational eï¬ƒciency of the Ten-

ğ‘ˆğ‘‡ ğ¸ = ğ‘…ğ‘ Ã— ğ‘„ğ‘ Ã— ğ‘ƒğº âˆ’ ğ¶ğ‘œ.

(10)

5.3.2. Attack Model

An attack model is characterized by two separate parts, information and capacity. First, the information available
to the attacker such as readings of diï¬€erent sensors can aï¬€ect the performance of the attack diï¬€erently. For example,
observing the input rate of the raw material in the Tennessee Eastman process is less beneï¬cial for the attacker than
the direct measurements of ğ‘ƒğº, ğ‘…ğ‘, ğ‘„ğ‘, ğ¶ğ‘œ
that aï¬€ect the utility metric in (10). Second, attackers can have diï¬€erent
capacities in accessing and revising controllers and sensors. An attacker may change the parameters of the proportional-
integral-derivative controller, directly falsify the controller output, or indirectly deviate the setpoint by tampering,
blocking or delaying sensor readings.

1(ğ‘¥2) to the defender if the user is adversarial. Fig. 4 shows the variation of ğ‘ˆğ‘‡ ğ¸

In this experiment, we assume a reading manipulation of sensor XMEAS(40) and XMEAS(17) in loop 8 and loop
13 of Tennessee Eastman process (see Ricker (1996)), respectively. Sensor XMEAS(40) measures the composition
of component ğº and sensor XMEAS(17) measures the stripper underï¬‚ow. A higher privilege state ğ‘¥2 âˆˆ {0, 1, 2, 3}
means that the user can access more sensors for a longer time, which results in a larger loss and thus a smaller utility
of ğ‘Ÿ2
versus the simulation time under
four diï¬€erent privilege states. We use the time average of these utilities to obtain the normal operational utility ğ‘Ÿ2
4
and compromised utilities ğ‘Ÿ2
1(ğ‘¥2) under four diï¬€erent privilege states ğ‘¥2 âˆˆ {0, 1, 2, 3}. The attacker compromises the
sensor and generates fraudulent readings. The fraudulent reading can be a constant, denoted by the blue line, or a
double of the real readings, denoted by the red or green lines. The pink line represents a composition attack with a
limited control time. Initially, the attacker manages to compromise both sensors by doubling their readings. After the

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 15 of 24

Proactive Defense against Advanced Persistent Threats

Figure 4: The economic impact of sensor compromise in the Tennessee Eastman process. The black line represents the
utility of Tennessee Eastman process under the normal operation while the other four lines represent the utility of Tennessee
Eastman process under attacks with four possible privilege levels. We use the time average of these utilities to obtain
the normal operational utility ğ‘Ÿ2
1(ğ‘¥2), âˆ€ğ‘¥2 âˆˆ {0, 1, 2, 3}, under four diï¬€erent states of privilege
levels in Table 4.

4 and compromised utilities ğ‘Ÿ2

attacker loses access to XMEAS(40) at the 6ğ‘¡â„ hour, the system is suï¬ƒciently resilient to recover partially in about
16 hours and achieve the same level of utility as the single attack in green. When the attacker also loses access to
XMEAS(17) at the 36ğ‘¡â„ hour, the utility goes back to normal in about 13 hours.

5.3.3. Utility Matrix

Attacks against SCADA system can apply command injection attacks to inject false control and compromise sensor
readings as shown in Morris and Gao (2013). Encryption can be introduced to conceal these malicious commands.
However, a legitimate user may also encrypt his communication with the sensor to avoid eavesdropping and enhance
privacy.

Therefore, at the ï¬nal stage, the user has two options, sends commands to the sensor with or without encryption,
2 = 0, respectively. The defender chooses to apply either a complete or selective
1 = 0, respectively. The complete monitoring stores all sets of communication

which are denoted by ğ‘2
monitoring, denoted by ğ‘2

1 = 1 and ğ‘2

2 = 1 and ğ‘2

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 16 of 24

010203040506070Time (hrs)01234Utility ($)105Sensor Compromise in Loop 13Normal OperationTwofold ReadingConstant Reading010203040506070Time (hrs)1.522.533.54Utility ($)105Sensor Compromise in Loop 8 and 13Normal OperationTwofold ReadingComposition AttackAttackers lose accessto sensor XMEAS(17)Attackers lose accessto sensor XMEAS(40)Proactive Defense against Advanced Persistent Threats

Table 4
The expected utilities of the defender and the user at the ï¬nal stage, i.e., ğ½ 2

1 and ğ½ 2

2 , respectively.

2;ğœƒğ‘”
ğœƒğ‘

2

Unencrypted Command (UC)

Encrypted Command (EC)

Selective Monitoring (SM)

Complete Monitoring (CM)

(ğ‘Ÿ2

4, 0);(ğ‘Ÿ2

4, ğ‘Ÿ2

4âˆ•2)

(ğ‘Ÿ2

4 âˆ’ ğ‘2, 0);(ğ‘Ÿ2

4 âˆ’ ğ‘2, ğ‘Ÿ2

4âˆ•2)

(ğ‘Ÿ2

1(ğ‘¥2), ğ‘Ÿ2

4 âˆ’ ğ‘Ÿ2

1(ğ‘¥2));(ğ‘Ÿ2

4, ğ‘Ÿ2
4)

(ğ‘Ÿ2 âˆ’ ğ‘2, âˆ’ğ‘Ÿ2);(ğ‘Ÿ2

4 âˆ’ ğ‘2, ğ‘Ÿ2
4)

data and analyzes them elaborately to identify malicious commands despite encryption. The selective monitoring
cannot identify malicious commands if they are encrypted. The implementation of the complete monitoring incurs
an additional cost ğ‘2 compared to the selective one. The last-stage utility matrix of both players is deï¬ned in Table
4. If the user is legitimate, as denoted in blue, both the defender and the user can receive a reward of ğ‘Ÿ4 when the
Tennessee Eastman process operates normally. Legitimate users further receive a utility reduction of ğ‘Ÿ4âˆ•2 for the
potential privacy loss if they choose unencrypted commands. For adversarial users, they send malicious commands
only when the communication is encrypted to evade detection. Thus, if they choose not to encrypt the communication,
they receive 0 utility and the defender receives a reward of ğ‘Ÿ4 for the normal operation. However, if they choose to
send encrypted malicious commands, both playersâ€™ rewards depend on whether the defender chooses the selective
or complete monitoring. If the defender chooses the selective monitoring, then the adversarial user can successfully
compromise the sensor, which results in a reduced utility of ğ‘Ÿ2
1(ğ‘¥2). In the meantime, the attacker beneï¬ts from the
1(ğ‘¥2). If the defender chooses the complete monitoring, then the adversarial user suï¬€ers a
reward reduction of ğ‘Ÿ2
loss of ğ‘Ÿ2 for being detected. The detection reward and the implementation cost for two types of defenders are ğ‘Ÿ2
ğ¿, ğ‘Ÿ2
ğ»
and ğ‘2
ğ» â‹… ğŸ{ğœƒ1=ğœƒğ»

, respectively. Let ğ‘Ÿ2 âˆ¶= ğ‘Ÿ2

ğ» â‹… ğŸ{ğœƒ1=ğœƒğ»

and ğ‘2 âˆ¶= ğ‘2

ğ¿ â‹… ğŸ{ğœƒ1=ğœƒğ¿

ğ¿ â‹… ğŸ{ğœƒ1=ğœƒğ¿

4 âˆ’ ğ‘Ÿ2

} + ğ‘2

} + ğ‘Ÿ2

ğ¿, ğ‘2
ğ»

}

}

.

1

1

1

1

6. Computation Results

In this section, we apply the algorithms introduced in Section 4 to compute both playersâ€™ strategies and utilities at
the equilibrium. We implement our algorithms in MATLAB and use YALMIP (see LÃ¶fberg (2004)) as the interface
to call external solvers such as BARON (see Tawarmalani and Sahinidis (2005)) to solve the optimization problems.
We present elaborate results from the concrete case study and provide meaningful insights of the proactive cross-layer
defense against multi-stage APT attacks that are stealthy and deceptive.

For the static Bayesian game at the ï¬nal stage in Section 6.1, we focus on illustrating how two playersâ€™ private types
aï¬€ect their policies and utilities under diï¬€erent information structures. We further apply sensitivity analysis to show
how the value of the key parameter aï¬€ects the defenderâ€™s and the attackerâ€™s utilities. For the multi-stage Bayesian game
in 6.2, we focus on the dynamic of the belief update and state transition under the interaction of the stealthy attacker
and the proactive defender. Moreover, we investigate how the adversarial and defensive deception, and how the initial
state can aï¬€ect the stage utility and the cumulative utility of the user and the defender.

6.1. Final Stage and SBNE

Playersâ€™ beliefs aï¬€ect their policies and expected utilities at the ï¬nal stage. We discuss three diï¬€erent scenarios as
follows. In Fig 5a, the defender does not know the userâ€™s type. In Fig. 6, the user does not know the defenderâ€™s type.
In Fig. 5b, both the user and the defender do not know the otherâ€™s type. In all three scenarios, the ğ‘¥-axis represents
the belief of either the user or the defender. The ğ‘¦-axis of the upper ï¬gure represents the probability of either the user
taking action â€˜selective monitoring (SM)â€™ or the defender taking action â€˜unencrypted command (UC)â€™. Fig. 5a shows
the following trends as the user becomes more likely to be adversarial. First, two black lines show that the expected
utility of the defender decreases and the defender is more inclined to apply action â€˜complete monitoringâ€™ after her belief
exceeds a threshold. Second, two red lines show that the adversarial user takes action â€˜unencrpted commandâ€™ with a
higher probability and only gains a reward when the probability of adversarial users is suï¬ƒciently small. Thus, we
conclude that when the probability of the adversarial user increases, the defender tends to invest more in cyber defense
so that the attacker behaves more conservatively and inï¬‚icts fewer losses. Third, the two blue lines show that the
legitimate user always chooses â€˜encrypt commandâ€™ and receives a constant utility, which indicates that the proactive
defense does not aï¬€ect the behavior and the utility of legitimate users at this stage.

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 17 of 24

Proactive Defense against Advanced Persistent Threats

(a) The user knows that the defender is primitive, yet the de-
fender only knows the probability of the user being adversarial.

(b) Both playersâ€™ types are private, and each player only knows
the probability of the other playerâ€™s type.

Figure 5: The SBNE strategy and the expected utility of the primitive defender and the user who is either legitimate or
adversarial. The ğ‘¥-axis represents the probability of the user being adversarial. The ğ‘¦-axis of the upper ï¬gure represents
the probability of either the user taking action â€˜selective monitoring (SM)â€™ or the defender taking action â€˜unencrypted
command (UC)â€™.

Figure 6: The SBNE strategy and the expected utility of the adversarial user and the defender who is either primitive
or sophisticated. The defender knows that the user is adversarial while the adversarial user only knows the probability of
the defender being primitive. The ğ‘¥-axis represents the probability of the defender being sophisticated. The ğ‘¦-axis of the
upper ï¬gure represents the probability of either the user taking action â€˜selective monitoring (SM)â€™ or the defender taking
action â€˜unencrypted command (UC)â€™.

Fig. 6 shows that the defender beneï¬ts from introducing defensive deception. When the defender becomes more
likely to a sophisticated one, both types of defenders can have a higher probability to apply the selective monitoring and
save the extra surveillance cost of the complete monitoring. The attacker with incomplete information has a threshold
policy and switches to a lower attacking probability after reaching the threshold of 0.5 as shown in the black line.
When the probability goes beyond the threshold, the primitive defender can pretend to be a sophisticated one and
take action â€˜selective monitoringâ€™. Meanwhile, a sophisticated defender can reduce the security eï¬€ort and take action

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 18 of 24

00.20.40.60.8100.51Probability of UC/SM00.20.40.60.81Probability of the Adversarial User024Expected Utility105DefenderLegitimate UserAdversarial User00.20.40.60.8100.51Probability of UC/SM00.20.40.60.81Probability of the Adversarial User024Expected Utility105DefenderLegitimate UserAdversarial User00.20.40.60.8100.51Probability of UC/SMAttackerPrimitive DefenderSophisticated Defender00.20.40.60.81Probability of the Sophisticated Defender3.243.263.283.3Expected Utility105Proactive Defense against Advanced Persistent Threats

Figure 7: Utilities of the primitive defender and the attacker versus the value of ğ‘Ÿ2

ğ¿ under diï¬€erent states ğ‘¥2 âˆˆ {0, 1, 2, 3}.

â€˜selective monitoringâ€™ with a higher probability since the attacker becomes more cautious in taking adversarial actions
after identifying the defender as more likely to be sophisticated. It is also observed that the sophisticated defender
receives a higher payoï¬€ before the attackerâ€™s belief reaches the 0.5 threshold. After the belief reaches the threshold,
the attacker is threatened to take less aggressive actions, and both types of defenders share the same payoï¬€.

Finally, we consider the double-sided incomplete information where both playersâ€™ types are private information,
and each player only has the belief of the other playerâ€™s type. Compared with the defender in Fig. 5a who takes action
â€˜selective monitoringâ€™ with a probability less than 0.5 and receives a decreasing expected payoï¬€, the defender in Fig.
5b can take â€˜selective monitoringâ€™ with a probability closed to 1 and receive a constant payoï¬€ in expectation after
the userâ€™s belief exceeds the threshold. Thus, the defender can spare defense eï¬€orts and mitigate risks by introducing
uncertainties on her type as a countermeasure to the adversarial deception.

6.1.1. Sensitivity Analysis

As shown in Fig. 7, if the value of the penalty ğ‘Ÿ2
ğ¿

is close to 0, i.e., the defense at the ï¬nal stage is ineï¬€ective,
then an arrival at state ğ‘¥2 = 3, the highest privilege level can signiï¬cantly increase the attackerâ€™s payoï¬€ and cause the
most damage to the defender. As more eï¬€ective defensive methods are employed at the ï¬nal stage, i.e., the value of ğ‘Ÿ2
ğ¿
increases, the attacker becomes more conservative and strategic in taking adversarial behaviors. Then, the state with
the highest privilege level may not be the most favorable state for the attacker.

6.2. Multi-stage and PBNE

We show in Fig. 8 that the Bayesian belief update leads to a more accurate estimate of usersâ€™ types. Without the
belief update, the posterior belief is the same as the prior belief in red and is used as the baseline. As the prior belief
increases in the ğ‘¥-axis, the posterior belief after the Bayesian update also increases in blue. The blue line is in general
above the red line, which means that with the Bayesian update, the defenderâ€™s belief becomes closer to the right type.
Also, we ï¬nd that the belief update is the most eï¬€ective when an inaccurate prior belief is used as it corrects the
erroneous belief signiï¬cantly.

In Fig. 9, we show that the proactive defense, i.e., defensive methods in intermediate stages can aï¬€ect the state
transition and reduce the probability of attackers reaching states that can result in huge damage at the ï¬nal stage. As
the prior belief of the user being adversarial increases, the attacker is more likely to arrive at state ğ‘¥2 = 0 and ğ‘¥2 = 1,
and reduce the probability of visiting ğ‘¥2 = 2 and ğ‘¥2 = 3.

6.2.1. Adversarial and Defensive Deception

Fig. 10 investigates the adversarial deception where the attacker takes full control of the defense system and
manipulates the defenderâ€™s belief. As shown in the ï¬gure, the defenderâ€™s utilities all increase when the belief under the
deception approaches the correct belief that the user is adversarial. Also, the increase is stair-wise, i.e., the defender
only alternates her policy when the manipulated belief is beyond certain thresholds. Under the same manipulated
belief, a sophisticated defender beneï¬ts no less than a primitive one. The defender receives a lower payoï¬€ when the

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 19 of 24

00.511.522.510500.511.522.5310500.511.522.51050.511.522.533.5105Proactive Defense against Advanced Persistent Threats

Figure 8: The defenderâ€™s prior and posterior beliefs of the user being adversarial.

Figure 9: The probability of diï¬€erent states ğ‘¥2 âˆˆ {0, 1, 2, 3}.

reconnaissance provides eï¬€ectual intelligence.

Incapable of revealing the adversarial deception completely, the defender can alternatively introduce defensive
deceptions, e.g., a primitive defender can disguise himself as a sophisticated one to confuse the attacker. Defensive
deceptions introduce uncertainties to attackers, increase their costs, and increase the defenderâ€™s utility. Fig. 11 investi-
gates the defenderâ€™s and the attackerâ€™s utilities under three diï¬€erent scenarios. The complete information refers to the
scenario where both players know the other playerâ€™s type. The deception with the ğ»-type or the ğ¿-type means that the
attacker knows the defenderâ€™s type to be sophisticated or primitive, respectively, yet the defender has no information
about the userâ€™s type. The double-sided deception indicates that both players do not know the other playerâ€™s type.

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 20 of 24

00.10.20.30.40.50.60.70.80.9100.51Bayeisan Belief UpdateWithout Belief Update00.10.20.30.40.50.60.70.80.9100.5100.10.20.30.40.50.60.70.80.9100.5100.10.20.30.40.50.60.70.80.91Prior Belief of Adversarial User00.51Posterior Belief of Adversarial User00.10.20.30.40.50.60.70.80.9100.51Bayeisan Belief UpdateWithout Belief Update00.10.20.30.40.50.60.70.80.9100.5100.10.20.30.40.50.60.70.80.9100.5100.10.20.30.40.50.60.70.80.91Prior Belief of Adversarial User00.51State ProbabilityProactive Defense against Advanced Persistent Threats

Figure 10: The defenderâ€™s utility under deceived beliefs.

The results from Fig. 11 are summarized as follows. First, the sophisticated defenderâ€™s payoï¬€s can increase as much
as 56% than those of the primitive defender. Also, a prevention of eï¬€ectual reconnaissance increases the defenderâ€™s
utility by as much as 41% and reduces the attackerâ€™s utility by as much as 38%. Second, the defender and the attacker
receive the highest and the lowest payoï¬€, respectively, under the complete information. When the attacker introduces
deceptions over his type, the attackerâ€™s utility increases and the defenderâ€™s utility decreases. Third, when the defender
adopts defensive deceptions to introduce double-sided incomplete information, we ï¬nd that the decrease of the sophis-
ticated defenderâ€™s utilities is reduced by at most 64%, i.e., changes from $55, 570 to $35, 570 when the reconnaissance
is eï¬€ectual. The double-sided incomplete information also brings lower utilities to the attacker than the one-sided
adversarial deception. However, the defenderâ€™s utility under the double-sided deception is still less than the complete
information case, which concludes that acquiring complete information of the adversarial user is the most eï¬€ective
defense. However, if the complete information cannot be obtained, the defender can mitigate her loss by introducing
defensive deceptions.

7. Discussions and Conclusions

Advanced Persistent Threats (APTs) are emerging security challenges for cyber-physical systems as the attacker
can stealthily enter, persistently stay in, and strategically interact with the system. In this work, we have developed a
game-theoretic framework to design proactive and cross-layer defenses for cyber-physical systems in a holistic manner.
Dynamic games of incomplete information have been used to capture the long-term interaction between users and
defenders who have private information unknown to the other player. Each player forms a belief on the unknowns and
uses the Bayesian update to learn the private information and reduce uncertainty. The analysis of the Perfect Bayesian
Nash Equilibrium (PBNE) has provided the defender with an eï¬€ective countermeasure against the stealthy strategic
attacks at multiple stages. To compute the PBNE of the dynamic games, we have proposed a nested algorithm that
iteratively alternates between the forward belief update and the backward policy computation. The algorithm has been
shown to quickly converge to the ğœ€-PBNE that yields a consistent pair of beliefs and policies.

Using the Tennessee Eastman process as a case study of industrial control systems, we have shown that the proactive
multi-stage defense in cyber networks can successfully mitigate the risk of physical attacks without reducing the payoï¬€s
of legitimate users. In particular, experiment results show that a sophisticated defender receives a payoï¬€ up to 56%
higher than a primitive defender does. Also, it has been illustrated that by preventing eï¬€ectual reconnaissance, the
defender increases her utility and reduces the attackerâ€™s utility by at most 41% and 38%, respectively. On one hand, the

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 21 of 24

00.10.20.30.40.50.60.70.80.9112310500.10.20.30.40.50.60.70.80.9112310500.10.20.30.40.50.60.70.80.9112310500.10.20.30.40.50.60.70.80.91Belief of Adversarial User under Deception123105System Utility under DeceptionProactive Defense against Advanced Persistent Threats

Figure 11: The cumulative utilities of the attacker and the defender under the complete information, the adversarial
deception, and the defensive deception. In the legend, the left three represent the utilities for a sophisticated defender and
the right three represent the ones for a primitive defender.

attacker receives a higher payoï¬€ after introducing the adversarial deception as it increases the defenderâ€™s uncertainties
on the userâ€™s type. On the other hand, by creating uncertainties for attackers, the defender can successfully threaten
them to take more conservative behaviors and become less motivated to launch attacks. It has been shown that the
defender can signiï¬cantly beneï¬t from the mitigation of attack losses when he adopts defensive deceptions.

The main challenge of our approach is to identify the utility and feasible actions of defenders and users at each stage.
One future direction to reduce the complexity of the model description is to develop mechanisms that can automate
the synthesis of veriï¬ably correct game-theoretic models. It would alleviate the workload of the system defender and
operator. Nevertheless, game theory provides a quantitative and explainable framework to design the proactive defen-
sive response under uncertainty compared to rule-based and machine-learning-based defense methods, respectively.
Besides, the rule-based defense is static, thus an attack can circumvent it through suï¬ƒcient eï¬€ort. Machine learning
methods require a lot of labeled data sets which may be hard to obtain in the APT scenario. Second, we have proposed
the belief to quantify the uncertainty which results from playersâ€™ private types. The belief is continuously updated to
reduce uncertainties and provide a probabilistic detection system as a byproduct of the APT response design. Third,
our approach enables the defender to evaluate the multi-stage impact of her defense strategies to both legitimate and
adversarial users when adversarial and defensive deceptions present at the same time. Based on the evaluation, defend-
ers can further ï¬nd revised countermeasures and design new game rules to achieve a better tradeoï¬€ between security
and usability. Our model can be broadly applied to scenarios in artiï¬cial intelligence, economy, and social science
where multi-stage interactions occur between multiple agents with incomplete information. Multi-sided non-binary
types can be deï¬ned based on the scenario, and our iteration algorithm of the forward belief update and the backward
policy computation can be extended for eï¬ƒcient computations of the perfect Bayesian Nash equilibrium. The future

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 22 of 24

150000170000190000210000230000250000270000290000310000330000Ineffectual	ReconnaissanceEffectual	ReconnaissanceDefender's	Utility	($)Complete	Information	with	the	H-TypeComplete	Information	with	the	L-TypeDeception	with	the	H-TypeDeception	with	the	L-TypeDouble	Deception	with	the	H-TypeDouble	Deception	with	the	L-Type400006000080000100000120000140000160000180000Ineffectual	ReconnaissanceEffectual	ReconnaissanceAttacker's	Utility	($)Complete	Information	with	H-TypeComplete	Information	with	L-TypeDeception	with		H-TypeDeception	with		L-TypeDouble	Deception	with		H-TypeDouble	Deception	with		L-TypeProactive Defense against Advanced Persistent Threats

work would extend the framework to an ğ‘-person game to characterize the simultaneous interactions among multiple
users and model composition attacks. We would also consider scenarios where playersâ€™ actions and the system state
are partially observable.

References
Bathelt, A., Ricker, N.L., Jelali, M., 2015. Revision of the Tennessee Eastman process model. IFAC-PapersOnLine 48, 309 â€“ 314. doi:https:

//doi.org/10.1016/j.ifacol.2015.08.199. 9th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2015.

CÃ¡rdenas, A.A., Amin, S., Lin, Z.S., Huang, Y.L., Huang, C.Y., Sastry, S., 2011. Attacks against process control systems: risk assessment, detection,

and response, in: Proceedings of the 6th ACM symposium on information, computer and communications security, ACM. pp. 355â€“366.

Corporation, T.M., 2019. Enterprise matrix. URL: https://attack.mitre.org/matrices/enterprise/.
Dufresne, M., 2018. Putting the MITRE ATT&CK evaluation into context. URL: https://www.endgame.com/blog/technical-blog/

putting-mitre-attck-evaluation-context.

Feng, X., Zheng, Z., Hu, P., Cansever, D., Mohapatra, P., 2015. Stealthy attacks meets insider threats: a three-player game model, in: MILCOM

2015-2015 IEEE Military Communications Conference, IEEE. pp. 25â€“30.

FireEye, 2017. Advanced Persistent Threat Groups | FireEye. URL: https://www.fireeye.com/current-threats/apt-groups.html.
Friedberg, I., Skopik, F., Settanni, G., Fiedler, R., 2015. Combating advanced persistent threats: From network event correlation to incident

detection. Computers & Security 48, 35â€“57.

Ghaï¬r, I., Hammoudeh, M., Prenosil, V., Han, L., Hegarty, R., Rabie, K., Aparicio-Navarro, F.J., 2018. Detection of advanced persistent threat

using machine-learning correlation analysis. Future Generation Computer Systems 89, 349â€“359.

Ghaï¬r, I., Kyriakopoulos, K.G., Lambotharan, S., Aparicio-Navarro, F.J., AsSadhan, B., BinSalleeh, H., Diab, D.M., 2019. Hidden markov models

and alert correlations for the prediction of advanced persistent threats. IEEE Access 7, 99508â€“99520.

Ghaï¬r, I., Prenosil, V., Hammoudeh, M., Han, L., Raza, U., 2017. Malicious ssl certiï¬cate detection: A step towards advanced persistent threat

defence, in: Proceedings of the International Conference on Future Networks and Distributed Systems, ACM. p. 27.

Harsanyi, J.C., 1967. Games with incomplete information played by â€œBayesianâ€ players, iâ€“iii part i. the basic model. Management science 14,

159â€“182.

Department of Homeland Security, D., 2018. NSA/CSS Technical Cyber Threat Framework v2 A REPORT FROM: CYBERSECURITY OPERA-
TIONS THE CYBERSECURITY PRODUCTS AND SHARING DIVISION. Technical Report. URL: https://www.nsa.gov/Portals/70/
documents/what-we-do/cybersecurity/professional-resources/ctr-nsa-css-technical-cyber-threat-framework.pdf.
HorÃ¡k, K., Zhu, Q., BoÅ¡ansk`y, B., 2017. Manipulating adversaryÃ¢Ä‚Å¹s belief: A dynamic game approach to deception by design for proactive

network security, in: International Conference on Decision and Game Theory for Security, Springer. pp. 273â€“294.

Huang, L., Chen, J., Zhu, Q., 2017. A large-scale markov game approach to dynamic protection of interdependent infrastructure networks, in:

International Conference on Decision and Game Theory for Security, Springer. pp. 357â€“376.

Huang, L., Zhu, Q., 2018. Analysis and computation of adaptive defense strategies against advanced persistent threats for cyber-physical systems,

in: International Conference on Decision and Game Theory for Security, Springer. pp. 205â€“226.

Huang, L., Zhu, Q., 2019a. Adaptive honeypot engagement through reinforcement learning of semi-markov decision processes, in: International

Conference on Decision and Game Theory for Security, Springer. pp. 196â€“216.

Huang, L., Zhu, Q., 2019b. Adaptive strategic cyber defense for advanced persistent threats in critical infrastructure networks. ACM SIGMETRICS

Performance Evaluation Review 46, 52â€“56.

Hutchins, E.M., Cloppert, M.J., Amin, R.M., 2011. Intelligence-driven computer network defense informed by analysis of adversary campaigns

and intrusion kill chains. Leading Issues in Information Warfare & Security Research 1, 80.

Krotoï¬l, M., CÃ¡rdenas, A.A., 2013. Resilience of process control systems to cyber-physical attacks, in: Nordic Conference on Secure IT Systems,

Springer. pp. 166â€“182.

La, Q.D., Quek, T.Q., Lee, J., Jin, S., Zhu, H., 2016. Deceptive attack and defense game in honeypot-enabled networks for the internet of things.

IEEE Internet of Things Journal 3, 1025â€“1035.

Li, P., Yang, X., Xiong, Q., Wen, J., Tang, Y.Y., 2018. Defending against the advanced persistent threat: An optimal control approach. Security and

Communication Networks 2018.

LLC, P.I., 2018. 2018 cost of data breach study.
LÃ¶fberg, J., 2004. Yalmip : A toolbox for modeling and optimization in MATLAB, in: In Proceedings of the CACSD Conference, Taipei, Taiwan.
Marchetti, M., Pierazzi, F., Colajanni, M., Guido, A., 2016. Analysis of high volumes of network traï¬ƒc for advanced persistent threat detection.

Computer Networks 109, 127â€“141.

Messaoud, B.I., Guennoun, K., Wahbi, M., Sadik, M., 2016. Advanced persistent threat: New analysis driven by life cycle phases and their
challenges, in: 2016 International Conference on Advanced Communication Systems and Information Security (ACOSIS), IEEE. pp. 1â€“6.
Milajerdi, S.M., Kharrazi, M., 2015. A composite-metric based path selection technique for the tor anonymity network. Journal of Systems and

Software 103, 53â€“61.

Mitnick, K.D., Simon, W.L., 2011. The art of deception: Controlling the human element of security. John Wiley & Sons.
Molok, N.N.A., Chang, S., Ahmad, A., 2010. Information leakage through online social networking: Opening the doorway for advanced persistence

threats .

Morris, T.H., Gao, W., 2013. Industrial control system cyber attacks, in: Proceedings of the 1st International Symposium on ICS & SCADA Cyber

Security Research, pp. 22â€“29.

Nguyen, T.H., Wang, Y., Sinha, A., Wellman, M.P., . Deception in ï¬nitely repeated security games.
Nissim, N., Cohen, A., Glezer, C., Elovici, Y., 2015. Detection of malicious pdf ï¬les and directions for enhancements: A state-of-the art survey.

Computers & Security 48, 246â€“266.

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 23 of 24

Proactive Defense against Advanced Persistent Threats

Pawlick, J., Chen, J., Zhu, Q., 2018. istrict: An interdependent strategic trust mechanism for the cloud-enabled internet of controlled things. arXiv

preprint arXiv:1805.00403 .

Pawlick, J., Colbert, E., Zhu, Q., 2017. A game-theoretic taxonomy and survey of defensive deception for cybersecurity and privacy. arXiv preprint

arXiv:1712.05441 .

Ricker, N.L., 1996. Decentralized control of the tennessee eastman challenge process. Journal of Process Control 6, 205â€“221.
Rowe, N.C., Custy, E.J., Duong, B.T., 2007. Defending cyberspace with fake honeypots .
Sahoo, D., Liu, C., Hoi, S.C., 2017. Malicious url detection using machine learning: a survey. arXiv preprint arXiv:1701.07179 .
Shoham, Y., Leyton-Brown, K., 2008. Multiagent systems: Algorithmic, game-theoretic, and logical foundations. Cambridge University Press.
Sigholm, J., Bang, M., 2013. Towards oï¬€ensive cyber counterintelligence: Adopting a target-centric view on advanced persistent threats, in: 2013

European Intelligence and Security Informatics Conference, IEEE. pp. 166â€“171.

Tawarmalani, M., Sahinidis, N.V., 2005. A polyhedral branch-and-cut approach to global optimization. Mathematical Programming 103, 225â€“249.
Team, M.D.A.R., 2017. Detecting stealthier cross-process injection techniques with windows defender atp: Process hollowing and atom bombing.

URL: https://bit.ly/2nVWDQd.

Van Dijk, M., Juels, A., Oprea, A., Rivest, R.L., 2013. Flipit: The game of â€œstealthy takeoverâ€. Journal of Cryptology 26, 655â€“713.
Yang, L.X., Li, P., Zhang, Y., Yang, X., Xiang, Y., Zhou, W., 2018. Eï¬€ective repair strategy against advanced persistent threat: A diï¬€erential game

approach. IEEE Transactions on Information Forensics and Security 14, 1713â€“1728.

Zhang, M., Zheng, Z., Shroï¬€, N.B., 2015. A game theoretic model for defending against stealthy attacks with limited resources, in: International

Conference on Decision and Game Theory for Security, Springer. pp. 93â€“112.

Zhu, Q., Rass, S., 2018. On multi-phase and multi-stage game-theoretic modeling of advanced persistent threats. IEEE Access 6, 13958â€“13971.

L. Huang and Q. Zhu: Preprint submitted to Elsevier

Page 24 of 24

