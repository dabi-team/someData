manuscript 00-00

Boost-R: Gradient Boosted Trees for Recurrence Data

Xiao Liu
Department of Industrial Engineering, University of Arkansas
University of Arkansas, xl027@uark.edu

Rong Pan
School of Computing, Informatics, Decision Systems Engineering
Arizona State University, rong.pan@asu.edu

Recurrence data arise from multi-disciplinary domains spanning reliability, cyber
security, healthcare, online retailing, etc. This paper investigates an additive-tree-
based approach, known as Boost-R (Boosting for Recurrence Data), for recurrent
event data with both static and dynamic features. Boost-R constructs an ensemble
of gradient boosted additive trees to estimate the cumulative intensity function of
the recurrent event process, where a new tree is added to the ensemble by minimizing
the regularized L2 distance between the observed and predicted cumulative intensity.
Unlike conventional regression trees, a time-dependent function is constructed by
Boost-R on each tree leaf. The sum of these functions, from multiple trees, yields
the ensemble estimator of the cumulative intensity. The divide-and-conquer nature
of tree-based methods is appealing when hidden sub-populations exist within a
heterogeneous population. The non-parametric nature of regression trees helps to
avoid parametric assumptions on the complex interactions between event processes
and features. Critical insights and advantages of Boost-R are investigated through
comprehensive numerical examples. Datasets and computer code of Boost-R are
made available on GitHub. To our best knowledge, Boost-R is the ﬁrst gradient
boosted additive-tree-based approach for modeling large-scale recurrent event data
with both static and dynamic feature information.

Key words : Additive Trees, Recurrent Event Data, Gradient Boosting, Reliability,

Feature Selection

1
2
0
2

l
u
J

3

]

G
L
.
s
c
[

1
v
4
8
7
8
0
.
7
0
1
2
:
v
i
X
r
a

1

 
 
 
 
 
 
2

1.

Introduction

TITLE: Boost-R
manuscript no. 00-00

1.1. Background
The rapid penetration of IoT technologies gives rise to large-scale recurrent event data from
multidisciplinary domains, including reliability, asset management, clinical trials, cyber
security, etc. A typical recurrent event dataset has two deﬁning characteristics: (i ) the
event occurrence times experienced by individuals are recorded; (ii ) each individual is
characterized by both static and dynamic feature/covariate information. By learning the
relationship between event processes and features, statistical approaches are needed to
better understand why critical events happened in the past, when events of interest will
recur in the future, and how one could optimize the event processes through proactive
interventions.

For example, Liu and Pan (2020) considered an asset management problem of a large ﬂeet
of oil and gas wells. An individual well, throughout its production life, requires repeated
maintenance due to failures over time. Because oil and gas wells are often located in vast
and remote spatial areas, the capabilities of predicting failures greatly facilitate mainte-
nance planning and reduce total production cost. For individual wells, times of failures
(i.e., repairs) are well documented. As a motivating example, the left panel of Figure 1
shows the geo-locations of 8232 oil and gas wells installed between 2007 and 2017, while
the right panel of Figure 1 shows the failure processes of a pump-related failure mode for
40 selected wells.

In this example, eight static well attributes are available, x1, x2, ..., x8. In particular,
x1, x2, ..., x6 are the static well attributes, including the dimensions of two critical com-
ponents (x1 and x2), average stroke length (x3), mean-time-between-failure (x4), mean
polished rod horse power (x5), and mean load on pump plunger against position (x6). The
last two attributes, x7 and x8, are the well latitude and longitude. Figure 2 shows the
(standardized) values of x1, x2, ..., x6 for all 8232 systems. It is seen that these systems have
diﬀerent static well attributes.

Figure 1

Left panel: geo-locations of 8232 wells with the legend showing the average number of failures per
year; Right panel: illustration of the failure processes of the ﬁrst 40 wells.

TITLE: Boost-R
manuscript no. 00-00

3

Figure 2

Standardized values of x1, x2, ..., x6 for all 8232 systems.

In addition to the static well attributes, critical well operating conditions are moni-
tored by sensors, including gear box torque, stroke length, polished rod horse power, peak
surface load, pump card area and cycles. For illustrative purposes, Figure 3 shows the
(standardized) gear torque for ﬁve chosen systems. It is seen from Figure 3 that these
systems experience diﬀerent operating conditions, which are rarely synchronized and lead
to further heterogeneity in how wells fail over time.

Figure 3

Gear torque for 5 well systems

More application examples of recurrent event data can be found from other application
domains. In personalised healthcare, it is crucial to understand the interaction between
the recurrence of a chronic medical condition and a patient’s risk factor; In cyber security,
of interest are often the prediction of recurrent fraudulent activities on susceptible systems
within a dynamic cyber environment; In transportation safety, it is important to reveal the
connection between the recurrences of accidents and the traﬃc/visibility/weather patterns
on particular highway sections; In online retail business, of interest are often the times when
customers, with diverse background, re-visit the online platforms and place their orders; In

020004000600080000.00.20.40.60.81.0system IDstandardized valuex1020004000600080000.00.20.40.60.81.0system IDstandardized valuex2020004000600080000.00.20.40.60.81.0system IDstandardized valuex3020004000600080000.00.20.40.60.81.0system IDstandardized valuex4020004000600080000.00.20.40.60.81.0system IDstandardized valuex5020004000600080000.00.20.40.60.81.0system IDstandardized valuex60.000.250.500.751.0005001000150020002500timeprocessed sensor signalindividual46421212534385645464

TITLE: Boost-R
manuscript no. 00-00

video streaming quality control, of interest are often the modeling of the recurrent buﬀering
processes during video streaming in a highly dynamic internet environment, and so on.

1.2. Literature, Gaps and Contributions
Statistical methods for event data have been investigated in survival analysis, reliability
engineering and bioinformatics (Fleming 1991, Anderson et al. 1993, Nelson 1995, Meeker
and Escobar 1998). In Liu and Pan (2020), the authors provided a comprehensive discus-
sions on the common challenges for modeling recurrence data from a large population of
heterogeneous individuals with diverse feature information. These challenges include model
speciﬁcation, data heterogeneity due to diﬀerent operating conditions, between-individual
variation (e.g., the relationship between event process and feature information may vary
among individuals) and feature selection. Hence, the divide-and-conquer nature of tree-
based methods is appealing when hidden sub-populations exist within a heterogeneous
population. The non-parametric nature of regression trees also helps to avoid parametric
assumptions on the complex interactions between event processes and features; see Liu
and Pan (2020) for detailed discussions. In fact, for time-to-event data (i.e., non-recurrent
events), the advantages of tree-based methods have been investigated in Huo et al. (2006),
Hothorn et al. (2006), Fan et al. (2006, 2009), Chipman et al. (2010), Ishwaran et al.
(2008), Ishwaran and Kogalur (2010), Ishwaran et al. (2010) and Bou-Hamad (2011).

In recent years, machine learning and Deep Recurrent Neural Network (DRNN) have
received considerable attention for their advantages in capturing complex non-linear event-
feature interactions (Ranganath et al. 2016, Lao et al. 2017, Wang et al. 2017, Katzman
et al. 2018, Grob et al. 2018, Lee et al. 2018). These methods integrate statistical sur-
vival analysis models into the framework of deep learning without speciﬁcally addressing
the existence of sub-populations with the event-feature relationship varying across sub-
populations. Finally, as the volume of data grows, so does the amount of noise (irrelevant
features, sampling error, measurement error, etc.); see Jordan (2019). Many features could
be redundant from either the statistical modeling or domain knowledge perspective (Guyon
and Elisseeﬀ 2003, Reunanen 2003, Yuan and Lin 2005, Nilsson et al. 2007, Witten and
Tibshirani 2010, Paynabar et al. 2015).

To address the challenges above, this paper proposes an additive-tree-based method for
modeling recurrent event data with static and dynamic feature information. By assum-
ing that the event process of an individual constitutes a counting process, we seek an
ensemble of binary regression trees to estimate the cumulative intensity function that
fully characterizes the recurrent event process. The ensemble trees are obtained under
the framework of XGBoost (Chen and Guestrin 2016), and the proposed method is
called Boost-R (Boosting for Recurrence Data). To our best knowledge, Boost-R is
the ﬁrst gradient boosted additive-tree-based statistical learning approach for model-
ing recurrence data with feature information. The R code is made available on GitHub
(https://github.com/dnncode/Boost-R).

Note that, a recently proposed additive-tree-based method, known as RF-R, leverages
the idea of Random Forest (RF) for modeling recurrence data (Liu and Pan 2020). The
Boost-R proposed in this paper can be seen as a competitor of RF-R, just like how XGBoost
is often seen as an alternative to RF (Chipman et al. 2010, Chen and Guestrin 2016).
The two competitors are based on exactly opposite and independent ideas: RF involves an
ensemble of de-correlated and fully-grown trees, while Boosting ﬁts a sequence of correlated
simple trees (“weaker learners”) and each tree explains only a small amount of variation

TITLE: Boost-R
manuscript no. 00-00

5

not captured by previous trees (Freund and Schapire 1997, Chipman et al. 2010, Hastie
et al. 2009). From this perspective, a natural and meaningful question to be answered is
whether the gradient boosted trees can be leveraged to achieve a better modeling and
prediction performance for recurrent event data. As shown in Figure 4, the relationship
between Boost-R and RF-R is parallel, and the development of Boost-R in the present
paper makes both RF-R and Boost-R available in practice; just like how RF and XGBoost
co-exist as the two most popular oﬀ-the-shelf methods.

Figure 4

The relationship between Boost-R and RF-R is similar to that between XGBoost and RF.

The paper is organized as follows. Section 2 presents the technical details of Boost-R.
Comprehensive numerical examples, including a case study, are provided in Section 3 which
generate critical insights on the Boost-R algorithm and demonstrate its advantages. Section
4 concludes the paper. The paper also includes supplementary material that provides
additional discussions and comparison studies using diﬀerent application examples.

2. Boost-R: Boosting for Recurrence Data

2.1. The Problem Setup
Consider a population of n individuals. An individual i (i = 1, 2, ..., n) experiences a
sequence of ri events at times yi = (yi,1, ..., yi,ri, ci), where yi,1, ..., yi,ri are the event times
and ci is the right censoring time. Associated with individual i there exists p static features,
xi = (xi,1, xi,2, ..., xi,p), and q time-varying dynamic features represented by a q-dimensional
time series, zi(t) = (zi,1(t), zi,2(t), ..., zi,q(t)). Hence, the recurrent event data are denoted by
D = (yi, xi, zi(t)), where |D| = n, xi ∈ Rp, yi ∈ Rri+1 and zi(t) : [0, ∞) → Rq. Conditioning
on xi and zi(t), events arising from an individual constitute a counting process, Λi(t) :
[0, ∞) → N+, with the cumulative intensity µ(t; xi, zh
i (t) = {zi(τ ); 0 ≤ τ ≤ t}
is the history of the dynamic feature information associated with individual i up to t.

i (t)) where zh

We seek an additive-tree-based model with K binary regression trees, T (1), T (2), ..., T (K),

such that

ˆµ(K)(t; xi, zh

i (t)) =

K
(cid:88)

k=1

T (k)(xi, zh

i (t))

(1)

where ˆµ(K)(t; xi, zh
i (t)) is the ensemble estimator of the time-dependent cumulative inten-
sity function (for individual i) constructed from K trees. Here, the output from a tree is a
time-dependent function given xi and zh

i (t).

Regression problemsRandom Forest *Breiman(2001)XGBoost*Chen and Guestrin(2016)  Regression problems for recurrence dataRF-R* Liu and Pan (2020)Boost-R*this paperextensionextensionparallelparallel6

TITLE: Boost-R
manuscript no. 00-00

Note that, there is a major diﬀerence between conventional binary regression trees and
regression trees for recurrence data. For conventional trees, a constant is found on each
tree leaf and used as the predicted value for the sub-population represented by that tree
leaf. When dealing with recurrent event data, on the other hand, an individual tree per-
forms a binary partition of the feature space, and a time-dependent function (instead of a
constant) needs to be found on each tree leaf. The sum of these time-dependent functions,
from multiple trees, yields the ensemble estimator of the cumulative intensity which fully
characterizes the recurrent event process given feature information. This key diﬀerence
requires us to devise new computationally eﬃcient algorithms for growing the ensemble
trees, T (1), T (2), ..., T (K), and the idea of Gradient Boosting is leveraged in this paper.

2.2. Boost-R with Static Features
We ﬁrst consider a recurrent event data set with only static features, D = (yi, xi), where
|D| = n, xi ∈ Rp and yi ∈ Rri+1. A conventional binary regression tree divides the feature
space into D disjoint “rectangular” subspaces, Rd, d = 1, 2, ..., D, and each subspace is
represented by a tree leaf. For any tree leaf d, a constant µd is found as the predicted value
for individuals associated with that tree leaf. Hence, a conventional regression tree can be
expressed by the linear combination of indicator functions, T (x; θ) = (cid:80)D
d=1 µdI(x ∈ Rd),
where I is an indicator function and θ = (Rd, µd)D

For recurrent event data, on the other hand, each individual i contains a sequence of
event times yi rather than a single response value as in the classical setting of regression
trees. Hence, a time-dependent function is established on each tree leaf. The sum of these
functions (from multiple trees) yields the ensemble estimator of the cumulative intensity
of the event process, conditioning on the features. Following this idea, we search for an
additive model with K trees:

d=1.

ˆµ(K)(t; x) =

K
(cid:88)

k=1

T (k)(x) =

K
(cid:88)

D(k)
(cid:88)

k=1

d=1

d (t)I(x ∈ R(k)
f (k)
d ).

(2)

In (2), ˆµ(K)(t; x) is the ensemble estimator of the cumulative intensity function obtained
d ), where D(k)
d (t) is a function of time. The space of trees is

from K trees. An individual tree is given by, T (k)(x) = (cid:80)D(k)
is the number of leaves of tree k and f (k)
given by {T (x; θ) = fr(x)(t)} with r denoting the mapping from x to a tree leaf.

d (t)I(x ∈ R(k)

d=1 f (k)

The idea of boosting involves sequentially ﬁtting a sequence of correlated simple trees
(“weaker learners”), and each tree explains a small amount of variation not explained by
previous trees. Let ˜µi(t) be the empirical Mean Cumulative Function (MCF) estimator of
the cumulative intensity function for individual i, then, a set of K trees, T (k), k = 1, ..., K,
can be obtained by minimizing a regularized objective function L(K):

minimize L(K) =

n
(cid:88)

i=1

l(˜µi(t), ˆµ(K)(t; xi)) +

K
(cid:88)

k=1

Ω(T (k)).

(3)

where l is a diﬀerentiable convex loss function which measures the distance between ˜µi(t)
and ˆµ(K)(t; xi). Although other choices are possible, we consider the following loss function

l(˜µi(t), ˆµ(K)(t; xi)) =

1
2

||˜µi(t) − ˆµ(K)(t; xi)||2
2

(4)

TITLE: Boost-R
manuscript no. 00-00

7

which is proportional to the squared L2 distance of two time-dependent functions.

The regularization Ω(·) in (3) controls the complexity of individual trees because the
central idea of Boosting involves sequentially ﬁtting a sequence of “weak learners”. Hence,
we consider the following regularization:

Ω(T (k)) = γ1D(k) +

1
2

γ2

D(k)
(cid:88)

d=1

||f (k)

d (t)||2
2.

(5)

The ﬁrst term in (5) controls the depth (i.e., the number of leaves) of individual trees.
From the perspective of Analysis of Variance (ANOVA), the value for D reﬂects the level
of dominant interaction eﬀects (of the features) on the recurrent event processes (Hastie
et al. 2009). Hence, regularizing the depth of a tree implies that only the important main
eﬀects and lower-order interaction eﬀects are captured. The second term in (5) adopts the
shrinkage strategy in statistical learning, and penalizes the contributions from individual
trees (i.e., f (k)

d (t)) to the ensemble estimate (2).

The optimization problem (3) is a formidable combinatorial problem, but can be solved
d (t) at equally

using the stagewise gradient boosting. We ﬁrst discretize ˜µi(t), ˆµ(K)
spaced times {tj}m

(t) and f (k)

j=1, and let

• {˜µi,j}m
j=1 be the values of ˜µi(t) at {tj}m
j=1;
• {ˆµ(k−1)
j=1 be the ensemble estimates from the ﬁrst (k − 1) trees at {tj}m
}m
i,j
j=1 be the values of f (k)
• {f (k)
d,j }m
j=1.
Then, given the ﬁrst (k − 1) trees in the ensemble, the kth tree is found by minimizing

d (t) at {tj}m

j=1;

i

the following objective function (i.e., stagewise gradient boosting):

minimize

n
(cid:88)

m
(cid:88)

i=1

j=1

l(˜µi,j, ˆµ(k−1)

i,j + f (k)

r(xi),j∆) + γ1D(k) +

1
2

γ2

D(k)
(cid:88)

d=1

||f (k)

d,· ||2

2

(6)

where f (k)
d,·

is a vector {f (k)

d,j }m

j=1, and ∆ is the spacing of equally-spaced times {tj}m

j=1.

Approximating the function l in (6) by a smoother function is essential in obtaining
computationally eﬃcient gradient boosting algorithms (Hastie et al. 2009). Hence, the
second-order approximation of l at ˆµ(k−1)
leads to

i,j

minimize

n
(cid:88)

(cid:40) m
(cid:88)

i=1

j=1

l(˜µi,j, ˆµ(k−1)

i,j

) + gi,jf (k)

r(xi),j +

+ γ1D(k) +

1
2

γ2

D(k)
(cid:88)

d=1

||f (k)

d,· ||2

2

(cid:41)

hi,j(f (k)

r(xi),j)2

∆

1
2

).

where gi,j = ∂ˆµ(k−1)

l(˜µi,j, ˆµ(k−1)

) and hi,j = ∂2

i,j
Let ∆ → 0 and drop the constant term l(˜µi,j, ˆµ(k−1)

ˆµ(k−1)
i,j

i,j

i,j

l(˜µi,j, ˆµ(k−1)

i,j

) in (7), we have

minimize

n
(cid:88)

i=1

(cid:90) (cid:18)

gi(t)f (k)

r(xi)(t) +

1
2

hi(t)(f (k)

r(xi)(t))2

(cid:19)

dt + γ1D(k) +

1
2

γ2

D(k)
(cid:88)

d=1

||f (k)

d,· (t)||2

2

(7)

(8)

8

TITLE: Boost-R
manuscript no. 00-00

where gi(t) = ∂ˆµ(k−1)
the ﬁrst (k − 1) trees have been determined, the kth tree can be found by solving (8).

(t)) and hi(t) = ∂2

(t)l(˜µi(t), ˆµ(k−1)

l(˜µi(t), ˆµ(k−1)

(t)). Hence, once

ˆµ(k−1)
i

(t)

i

i

i

Growing the kth tree requires iteratively ﬁnding the optimal split feature and splitting
points for each tree node. Naturally, the node splitting process can be terminated when
the objective function (8) cannot be further reduced by splitting any of the tree nodes. For
any given tree topology, let Id = {i|r(xi) = d} be a set that contains all individuals within
leaf d. Then, the contribution to the objective (8) from tree node d is:

(cid:90) (cid:18)

g·(t)fd(t) +

(h·(t) + γ2)f 2

d (t)

(cid:19)

dt + γ1

1
2

(9)

where g·(t) = (cid:80)
out causing confusion.

i∈Id

gi(t) and h·(t) = (cid:80)

i∈Id

hi(t). Here, the superscript ·(k) is dropped with-

Given any candidate split feature and splitting point, tree node d can be split into two
daughter nodes. Let I (L)
respectively contain the individuals in the left and right
daughter nodes of d, the amount of reduction of (8) achieved by this splitting is given by:

and I (R)

d

d

(cid:90) (cid:18)

g·(t)fd(t) +

G1 =

(h·(t) + γ2)f 2

d (t)

(cid:19)

dt

1
2

(cid:90) (cid:18)

g(L)
·

(t)f (L)
d

(t) +

(h(L)
·

(t) + γ2)(f (L)

d

(t))2

(cid:19)

dt

(10)

(cid:90) (cid:18)

g(R)
·

(t)f (R)
d

(t) +

(h(L)
·

(t) + γ2)(f (R)

d

(t))2

(cid:19)

dt − γ1

−

−

1
2
1
2

(t) = (cid:80)

(t) = (cid:80)

gi(t), h(L)

where g(L)
(cid:80)

i∈I (L)
d

i∈I (R)
d

(t) =
·
hi(t). Hence, if maxG1 > 0, the optimal split feature and splitting point are the
ones that maximize G1. If maxG1 ≤ 0, no gain can be achieved by further splitting the tree
node d, making this node a terminal node. The tree growing process is terminated when
no further node splitting is possible.

i∈I (R)
d

i∈I (L)
d

gi(t) and h(R)

hi(t), g(R)

(t) = (cid:80)

·

·

·

For a high-dimensional feature space with a large p, there is always a need arising from
practice to perform feature selection. Excluding irrelevant features greatly helps to develop
more accurate predictive models with improved model interpretability. By design, tree-
based methods have a natural advantage in terms of feature selection. Recall that, at each
tree node splitting, the optimal split variable is chosen to achieve the maximum gain in
(10). Hence, the importance of a feature can be measured as the total gain achieved by
splitting tree nodes based on this feature.

To make this idea formal, for any tree k in the ensemble, let G(k)

d(cid:48),i be the gain achieved by
splitting an internal node d(cid:48) by feature i. Note that, d(cid:48) = 1, 2, · · · , D(k) − 1 where D(k) − 1 is
the number of internal nodes of a binary tree with D(k) leaves. Let s(k)
d(cid:48),i = 1 if the internal
node d(cid:48) is split by feature i; otherwise s(k)
d(cid:48),i = 0. Then, the importance of feature i can be
computed as (Hastie et al. 2009):

wi =

1
K 2

K
(cid:88)

D(k)−1
(cid:88)

k=1

d(cid:48)

G(k)

d(cid:48),is(k)
d(cid:48),i,

for i = 1, 2, · · · , p.

(11)

TITLE: Boost-R
manuscript no. 00-00

9

If we let G(k)
d(cid:48),i = 1, the importance of feature i is measured by the number of times this
feature is used for splitting a node, and such a strategy appeared in Chipman et al. (2010).

Finally, the Boost-R algorithm is summarized in Algorithm 1.

Data: (xi, yi) for i = 1, 2, ..., n
For all i = 1, ..., n:
initialize µ(0)(t) = 0,
choose γ1 and γ2,
calculate gi(t) = ∂ˆµ(0)

(t)l(˜µi(t), ˆµ(0)

i

i (t)) and hi(t) = ∂2

l(˜µi(t), ˆµ(0)

i (t)).

(t)

ˆµ(0)
i

for k = 1, ..., K do

for d = 1, ..., D(k) do

if d is not a terminal node then

construct Id
calculate g·(t) = (cid:80)
for i = 1, ..., p do

i∈Id

gain ← 0
for j = 1, ..., m do

gi(t) and h·(t) = (cid:80)

i∈Id

hi(t).

split the node by xi,j
calculate G1 from (10)
gain ← max(gain, G1)

end
if gain ≤ 0 then

node d is a terminal node

end

end

end

update the tree topology
update ˆµ(k)(t) = ˆµ(k−1)(t) + T (k)(x)
update gi(t) = ∂ˆµ(k)

(t)l(˜µi(t), ˆµ(k)

i

i (t)) and hi(t) = ∂2

l(˜µi(t), ˆµ(k)

i (t)).

(t)

ˆµ(k)
i

end

end

Algorithm 1: the Boost-R algorithm

2.3. Boost-R with Dynamic Features
Next, we extend Boost-R to handle both static and dynamic features. As discussed in
Section 2.1, with both static and dynamic features, the recurrent event data can be denoted
by D = (yi, xi, zi(t)), where |D| = n, xi ∈ Rp, yi ∈ Rri+1 and zi(t) : [0, ∞) → Rq.

Incorporating dynamic features into a tree-based method is challenging especially when
dynamic features have cumulative eﬀects on the event process (Bacchetti and Segal 1995,
Bou-Hamad et al. 2009). In other words, for individual i, its cumulative intensity depends
on the entire history of the dynamic feature information, zh
i (t) = {zi(τ ); 0 ≤ τ ≤ t}, asso-
ciated with that individual. One approach, proposed in Liu and Pan (2020), is to split a
tree node based on static features, while the data on each node are modeled by a sepa-
rate model that explains the eﬀects of dynamic features. In other words, dynamic features

10

TITLE: Boost-R
manuscript no. 00-00

are nested with static features. At each tree leaf, diﬀerent individuals are associated with
diﬀerent time-dependent functions. This approach can be justiﬁed when sub-populations
are mainly characterized by static attributes, while dynamic features are used for explain-
ing the between-individual variation within a tree node (i.e., the variation between event
processes for individuals sharing similar attributes).

Following the idea above, an additive-tree-based model with both static and dynamic

features can be expressed as:

ˆµ(K)(t; xi, zh

i (t)) =

K
(cid:88)

k=1

T (k)(xi, zh

i (t)) =

K
(cid:88)

D(k)
(cid:88)

k=1

d=1

f (k)
d (t; zh

i (t))I(xi ∈ R(k)
d )

(12)

where ˆµ(K)(t; xi, zh
i (t)) is the ensemble estimator of the cumulative intensity for individual
d=1 f (k)
d ) with D(k) denoting
i based on K trees; and T (k)(xi, zh
the number of leaves of tree k and f (k)
i (t)) being a time-dependent function depending
on zh
i (t). It is easy to see how the idea of Liu and Pan (2020) is embedded into (12): a tree
node is split based on static features, and the data on each node are modeled by a second
model that incorporates the dynamic features.

i (t)) = (cid:80)D(k)
d (t; zh

i (t))I(xi ∈ R(k)

d (t; zh

The same idea behind Algorithm 1 can be adopted and modiﬁed to construct the boost-
ing trees (12) with dynamic features. However, before we present the extended algorithm,
some necessary modiﬁcations are needed:

• Because it is less realistic to assume a parametric form for f (k)

d (t; zh

i (t)) in (12), we

adopt the non-parametric approach and model f (k)

d (t; zh

i (t)) as

f (k)
d (t; zh

i (t)) =

bl(zi,l(τ ); β(d)

l

)dτ

q
(cid:88)

(cid:90) t

0

l
q
(cid:88)

=

u+v
(cid:88)

(cid:18)

β(d)
j,l

l

j

(cid:19)

Bj,v(zi,l(τ ))dτ

(cid:90) t

0

(13)

where bl is a linear combination of B-splines bases, βl are the control or de Boor points,
Bj,v is the jth B-splines basis function of order v, and u is the number of internal knots.
• The regularization term (5) needs to be modiﬁed. Note that, for Boost-R with only
static features, all individuals on a node d share the same function fd(t). However, when
dynamic features are included, individuals on the same node (i.e., individuals share the
same static feature x) are associated with diﬀerent fd(t; zh(t)) because these individuals
typically possess diﬀerent history of dynamic feature information. Hence, it is no longer
meaningful to use the shrinkage strategy by penalizing ||fd(t; zh(t))||2
2, which depends on
dynamic features. This consideration motivates us to adopt the idea of Group Lasso, and
obtain a set of K trees by minimizing

minimize L(K) =

n
(cid:88)

i=1

l(˜µi(t), ˆµ(K)(t; xi, zh

i (t))) + γ1

K
(cid:88)

k=1

D(k) +

1
2

γ2

D(k)
(cid:88)

q
(cid:88)

d=1

l

||β(d)

·,l ||2.

(14)

1,l , β(d)

·,l = (β(d)

2,l , · · · , β(d)

where β(d)
u+v,l) for l = 1, 2, · · · , q. The last term in (14) borrows the
idea from Group Lasso, which has been widely used for model selection with grouped
variables (Yuan and Lin 2007). In (14), ||β(d)
·,l ||2 measures the size of β·,l which corresponds
to dynamic feature l, i.e., the lth group in Group Lasso. If the lth feature turns out to be
less important, it is necessary to drop the entire group vector, β·,l, on a tree node.

(15)

(16)

.

TITLE: Boost-R
manuscript no. 00-00

11

• The node splitting procedure needs to be modiﬁed. In Section 2, (9) and (10) are
obtained from (8) because all individuals on node d (i.e. for all i ∈ Id) share the same
function fd(t). For the same reason, g·(t) = (cid:80)
hi(t) can be prop-
erly deﬁned. When dynamic features are included, individuals on the same node are not
associated with the same fd(t; zh(t)) as individuals are associated with diﬀerent dynamic
features. As a result, g·(t) and h·(t) cannot be deﬁned and the “gain” in (10) needs to be
modiﬁed as:

gi(t) and h·(t) = (cid:80)

i∈Id

i∈Id

G2 = F2(Id) − F2(I (L)

) − F2(I (R)

) − 2γ1

d

d

where

F2(I) =

(cid:26)(cid:90) ci

0

(cid:88)

i∈I

gi(t)fd(t; zh

i (t)) +

1
2

hi(t)f 2

d (t; zh

i (t))dt

(cid:27)

Based on the discussions above, the extended Boost-R algorithm with both static and

dynamic features is summarized in Algorithm 2.

Data: (xi, yi, zi(t)) for i = 1, 2, ..., n
For all i = 1, ..., n:
initialize µ(0)(t) = 0,
choose γ1 and γ2,
calculate gi(t) = ∂ˆµ(0)

(t)l(˜µi(t), ˆµ(0)

i

i (t)) and hi(t) = ∂2

l(˜µi(t), ˆµ(0)

i (t)).

(t)

ˆµ(0)
i

for k = 1, ..., K do

for d = 1, ..., D(k) do

if d is not a terminal node then

construct Id
for i = 1, ..., p do

gain ← 0
for j = 1, ..., m do

split the node by xi,j
calculate G2 from (15)
gain ← max(gain, G2)

end
if gain ≤ 0 then

node d is a terminal node

end

end

end

update the tree topology
update ˆµ(k)(t) = ˆµ(k−1)(t) + T (k)(x)
update gi(t) = ∂ˆµ(k)

(t)l(˜µi(t), ˆµ(k)

i

i (t)) and hi(t) = ∂2

l(˜µi(t), ˆµ(k)

i (t)).

(t)

ˆµ(k)
i

end

end

Algorithm 2: Boost-R algorithm with both static and dynamic features

12

TITLE: Boost-R
manuscript no. 00-00

3. Numerical Examples, R Code and Applications
Numerical studies, including a case study, are presented in this section to generate some
critical insights on how Boost-R performs and illustrate the applications of Boost-R.

3.1. Computer Code
Boost-R has been implemented in R and leverages the parallel computing capabilities of
R. The code is available at GitHub (https://github.com/dnncode/Boost-R), and the
use of the R code is demonstrated throughout this section.

Investigate the basic properties using DATASET A and DATASET B

3.2.
To develop some basic understanding of how Boost-R performs, we start with a simple
numerical example involving only 200 individuals. For each individual, two static features
are respectively sampled from the unit interval [0, 1]. Let xi,j denote the value of feature j
associated with individual i (i = 1, 2, ..., 200, j = 1, 2), the recurrent events of individual i
are simulated from a homogeneous Poisson process with the following intensity:

λi =






0.01
0.10
0.05

if 0 ≤ xi,1, xi,2 ≤ 0.5
if 0.5 < xi,1, xi,2 ≤ 1
otherwise

(17)

This data set is referred to as DATASET A in this paper.

To illustrative the Boost-R algorithm, we start with some arbitrarily chosen values:
K = 50, γ1 = 300 and γ2 = 100. In the R code, the boosting trees are grown using the
function BoostR:

BoostR.out = BoostR(data, X, K.value=50,

gamma1.value=300, gamma2.value=100, D.max=4)

(18)

where data contains the recurrent event data, X is a matrix that contains feature informa-
tion, K.value, gamma1.value and gamma2.value are the speciﬁed values for K, γ1 and γ2,
and D.max triggers the termination of the tree growing process once the number of leaves of
a tree is not smaller than D.max. The function BoostR returns an object BoostR.out. Four
R functions have been created to visualize the output of Boost-R. These functions include
Plot Partition, Plot Individual, Plot Leaf, and Plot Imp, which will be discussed
next.

The function, Plot Partition(BoostR.out), visualizes the binary partitions by indi-
vidual trees, as well as f (·)
d (t) on each partition. Figure 5 shows the ﬁrst 10 trees obtained
from the Boost-R algorithm. Columns 1 and 3 of this ﬁgure present the binary partitions
of the feature space [0, 1]2 by individual trees. Columns 2 and 4 show the contribution (i.e.,
f (·)
d (t)) to the estimated cumulative intensity function from each tree leaf.

TITLE: Boost-R
manuscript no. 00-00

13

Figure 5

Columns 1 and 3 show the binary partitions of the feature space [0, 1]2 by the K (= 10) trees from the
ensemble. Columns 2 and 4 show the contribution (i.e., f (·)
d (t)) to the estimated cumulative intensity
function from each tree leaf of a tree.

0.00.40.80.00.40.8x1x2tree:1123050150012345timecontribution from leafs1230.00.40.80.00.40.8x1x2tree:21230501500123timecontribution from leafs1230.00.40.80.00.40.8x1x2tree:3123050150−0.50.51.52.5timecontribution from leafs1230.00.40.80.00.40.8x1x2tree:4123050150−0.50.51.5timecontribution from leafs1230.00.40.80.00.40.8x1x2tree:5123050150−0.50.00.51.01.5timecontribution from leafs1230.00.40.80.00.40.8x1x2tree:61234050150−0.50.00.51.0timecontribution from leafs12340.00.40.80.00.40.8x1x2tree:712345050150−0.50.00.51.0timecontribution from leafs123450.00.40.80.00.40.8x1x2tree:8123050150−0.40.00.40.8timecontribution from leafs1230.00.40.80.00.40.8x1x2tree:9123050150−0.40.00.4timecontribution from leafs1230.00.40.80.00.40.8x1x2tree:10123456050150−0.40.00.4timecontribution from leafs12345614

TITLE: Boost-R
manuscript no. 00-00

It is seen from Figure 5 that each tree performs a binary partition of the feature space,
which divides the 200 individuals into several sub-populations represented by tree leaves.
For each sub-population, the contribution to the cumulative intensity function f (·)
d (t) is
computed. Three critical observations are obtained:

• The idea behind boosting suggests that a new tree is added to explain only a small
amount of variation not captured by previous trees. Hence, the newly added tree is used
to perform some necessary adjustments to the output generated from previous trees (i.e.,
performance boosting). By examining the scale of the vertical axis in columns 2 and 4, it is
easy to see that the amount of adjustment by a newly added tree becomes smaller as more
trees have already been included in the ensemble. Using Plot Individual(BoostR.out),
Figure 6 shows how the ﬁnal ensemble estimates for individuals 1 and 2 are obtained by
aggregating outputs from individual trees.

Figure 6

The ensemble estimate of cumulative intensity functions by adding the contributions from individual
boosting trees. The blue arrow shows how the initial cumulative intensity function (the horizontal dash
line) converges to the ensemble estimate (the thick red curve) as more trees are grown.

• The amount of adjustment made by a newly added tree is not necessarily positive;
for example, on leaves 2 and 4 of tree #7. Typically, this happens when the estimated
cumulative intensity is getting closer to the true function. Under such a circumstance, a
newly added tree may suggest either increase or decrease the estimated cumulative intensity
for certain sub-populations.

• The key idea behind boosting trees is that each individual tree must be kept simple to
form weak learners. In Boost-R, two mechanisms are used to regularize the tree complexity,
i.e., the regularization term (5) and D.max in (18) speciﬁed by users. Figure 7, generated by
Plot Leaf(BoostR.out), shows the number of leaf nodes for the 50 trees. It is interesting
to see that: (i) Since the maximum number of tree leaves, D.max, is set to 4 in this example,
the tree growing process is forced to stop once the number of leaves has exceeded 4 (note
that, the ﬁnal number of leaves is not necessarily 4 in this case if two or more nodes are
split simultaneously before the algorithm ends). As shown in Figure 7, this rule applies
to tree #7, #10, #15 and #17; (ii) For all other trees, the tree growing processes are
terminated before the number of leaves has reached D.max. This observation justiﬁes the
eﬀectiveness of the regularization (5) on tree complexity; (iii) After tree #28, the remaining
trees consist of only the root node. In this example, as most of the variation has been

05010015005101520timecumulative eventsinitialindividual 1final ensembleintermidate ensembleinitial0501001502000246812timecumulative eventsinitialindividual 2final ensembleintermidate ensembleinitialTITLE: Boost-R
manuscript no. 00-00

15

eﬀectively explained by the ﬁrst 28 trees, the gain of adding a new tree to the ensemble is
outweighed by the penalty incurred by adding that new tree. Note that, it would not be
possible to observe such a phenomenon if the regularization term (5) was removed (as there
would be no penalty associated with increasing tree complexity). From this perspective,
Figure 7 also provides some insights on the choice of K, which will be investigated later.

Figure 7

Number of leaf nodes for the 50 trees. Here, we purposely overﬁt the data in order to generate some
critical insights on how Boost-R performs.

The boosting trees obtained from Boost-R are capable of accurately capturing the inter-
actions between the recurrent event process and features. Figure 8 shows both the actual
(left panel) and estimated (right panel) relationship between the cumulative intensity func-
tion and features. Note that, since the Boost-R algorithm does not assume that the intensity
function is time-invariant, the intensity shown on the right panel is the average intensity
over time over the feature space. Figure 8 clearly demonstrates the potential of Boost-R in
capturing the relationship between recurrent event processes and features. More complex
scenarios are investigated in Section 3.3.

Figure 8

Learning the relationship between the cumulative intensity function and features. Left panel: the true
relationship; Right panel: the estimated relationship by Boost-R.

01020304050123456trees# of leafsx1x2intensityactual0.020.040.060.080.10x1x2intensityestimated0.020.040.060.080.1016

TITLE: Boost-R
manuscript no. 00-00

Boost-R requires one to specify the parameters, γ1, γ2 and K. The ﬁrst two parameters
γ1 and γ2 control the complexity of individual trees, while K determines the number of
trees in the ensemble. In general, K needs to be suﬃciently large so that there will be
enough number of trees, but not too large which causes overﬁtting.

As a common strategy in practice, we explore the suitable values for γ1 and γ2, leaving
K as the primary parameter (Hastie et al. 2009). Although it is theoretically possible to
perform a grid search for the best combinations of γ1 and γ2 on a two-dimensional space,
such an approach may not be practical nor necessary in practice when it is computationally
intensive to run Boost-R on big datasets. Hence, we resort to a powerful tool in computer
experiments—the space-ﬁlling designs (Joseph 2016). The idea of space-ﬁlling designs is
to have points everywhere in the experimental region with as few gaps as possible, which
serves our purpose very well. The top left panel of Figure 9 shows the Maximum Projection
Latin Hypercube Design (MaxProLHD, Joseph et al. (2015)) of 15 runs with diﬀerent
combinations of γ1 and γ2, where the experimental ranges for these two parameters are
respectively [0, 600] and [0, 200]. The top right panel of Figure 9 shows the box plot of the
number of tree leaves per tree in an ensemble, for each combination of γ1 and γ2. Since the
key idea behind boosting trees is that each individual tree needs to be kept simple with 4
to 8 leaves (Hastie et al. 2009), we quickly identify that Designs #3, #4 and #6 provide
the most suitable combinations of γ1 and γ2. From the top left panel of Figure 9, these
three design points are adjacent to each other, indicating that the appropriate choices for
γ1 and γ2 are respectively within [75, 220] and [75, 175]. If necessary, a more reﬁned search
can be perform in a much smaller experimental region.

TITLE: Boost-R
manuscript no. 00-00

17

Figure 9 Model improvements and comparison. Top left panel: the MaxProLHD design for 15 combinations of
γ1 and γ2; Top right panel: the number of tree leaves for trees in an ensemble; Bottom: comparison of
C-index for diﬀerent models

We compare the performance of Boost-R with that of existing approaches, including
1) RF-R: Random Forest for Recurrence Data; 2) MCF : the nonparametric estimation
for Mean Cumulative Function (MCF) without using feature information; 3) MCF-K : the
nonparametric estimation for MCF utilizing only the data from the K nearest neighbors
of an individual (the distance between two individuals is deﬁned by the Euclidean distance
in the feature space); and 4) HPP : the HPP model with a log-linear intensity of static
features.

Cross-validation is used to evaluate the performance of all candidate approaches. For
each approach, the data set is randomly divided into a training set (150 individuals) and a
testing set (50 individuals). The model is trained using the training set, and the prediction
C-index is used as the performance measure. The above procedure is repeated for 500 times
in order to generate the boxplot of C-indices at the bottom of Figure 9. It is seen that,
the Boost-R (based on the three best designs, Designs #3, #4 and #6), yields a higher
C-index than other competing methods. Here, the C-index, or Harrell’s concordance index,
was ﬁrstly proposed in Harrell et al. (1982) for evaluating the amount of information a
medical test provides about individual patients. For the problem considered in this paper,
the C-index can be interpreted as the empirical probability of correctly ranking any two
individuals in terms of their cumulative number of failures, and can be calculated in the

0100200300400500600050100150200MaxProLHDg1g2123456789101112131415lllBoost_R_D3Boost_R_D4Boost_R_D6RF_RMCFMCF_KHPP0.30.40.50.60.70.8different approachesC−index13579111315051015design ID# of leafs18

TITLE: Boost-R
manuscript no. 00-00

following way: 1) form all pairs of individuals from the testing data set. 2) for each pair,
rank the two individuals based on the cumulative number of events up to a given time.
3) for each pair, rank the two individuals based on the predicted cumulative number of
events up to the same time. If the predicted rankings are consistent with the observed
rankings, let Ci = 1 for pair i, otherwise Ci = 0; and 4) the C-index for a testing data set
is the empirical probability of correctly ranking any two individuals.

Finally, to illustrate the feature selection capability of Boost-R, we include eight ran-
domly generated redundant covariates, {x}10
i=3, in DATASET A. The new data set with redun-
dant covariates is referred to as DATASET B. We re-run Boost-R using DATASET B, and the
function, Plot Imp(BoostR.out, standardize=TRUE), shows the importance for the 10
features as deﬁned in (11); see Figure 10. Here, we standardize the importance measure
(11) such that the highest and lowest importance are respectively 1 and 0. It is immediately
seen that the algorithm successfully identify the correct features, x1 and x2.

Figure 10

Standardized feature importance measured by the total gain achieved by splitting tree nodes based
on a given feature; see (11)

3.3. More complicated scenarios using DATASET C and DATASET D
In the previous illustrative example, DATASETS A and B are simulated from a simple HPP
with intensity (17). To investigate the learning capabilities of Boost-R, we consider more
complicated interactions between event processes and features. In particular, DATASET C
and DATASET D respectively consist of the simulated recurrent event data from 1000 indi-
viduals. For both datasets, two features, x1 and x2, are sampled from the unit interval
[0, 1]. In addition,

• For DATASET C, the event times for individual i are simulated from a non-homogeneous
Poisson process using the thinning method (Lewis and Shedler 1979) with the following
intensity function:

λi(t) =






1.5t−0.5
t−0.5
0.5t−0.5

if (xi,1 − 0.5)2 + (xi,2 − 0.5)2 ≤ 0.04
if 0.04 < (xi,1 − 0.5)2 + (xi,2 − 0.5)2 ≤ 0.16
otherwise

(19)

12345678910featuresfeature importance (standardized)0.00.20.40.60.81.0TITLE: Boost-R
manuscript no. 00-00

19

• For DATASET D, the event times for individual i are simulated from a non-homogeneous

Poisson process with the following intensity function:

λi(t) = 0.01t0.5 exp(0.5(xi,1 − 0.5)2 + 2(xi,2 − 0.5)2).

(20)

Figure 11

Actual (left) and estimated (right) cumulative intensity at time 50 based on DATASET C

Figure 12

Actual (left) and estimated (right) cumulative intensity at time 100 based on DATASET D

We run the Boost-R algorithms using both data sets. For DATASET C, we let K = 500,
γ1 = 10 and γ2 = 5. For DATASET D, we let K = 300, γ1 = 100 and γ2 = 100. Based on
DATASET C, Figure 11 shows both the actual cumulative intensity (left panel) and estimated
cumulative intensity (right panel) at time 50 over the feature space. Based on DATASET D,
Figure 12 shows both the actual cumulative intensity (left panel) and estimated cumulative
intensity (right panel) at time 100 over the feature space. It is seen that, Boost-R suc-
cessfully captures the complex relationship between cumulative intensity and features, as
speciﬁed in (19) and (20). Such complicated and highly nonlinear relationships can hardly
be speciﬁed (unless they are known in advance) when traditional parametric approaches
are used.

x1x2cumulative intensityactual8101214161820x1x2cumulative intensityestimated5101520x1x2cumulative intensityactual789101112x1x2cumulative intensityestimated67891020

TITLE: Boost-R
manuscript no. 00-00

3.4. Modeling the Failure Processes of Oil and Gas Wells
To run Boost-R with both static and dynamic features (i.e., Algorithm 2), we model
f (k)
d (t; zi(t)) in (13) by cubic splines with two internal knots, and let K = 300, γ1 = 300,
γ2 = 100, v = 3 and u = 2. In our R code, the boosting trees are grown using the function
BoostR2:

BoostR.out = BoostR2(data, X, Z=z.list, K.value=300,

gamma1.value=300, gamma2.value=100, u.value=2, v.value=3, D.max=4)

(21)

where data contains the recurrent event times, X is a matrix that contains the static fea-
ture information, Z contains the dynamic feature information, K.value, gamma1.value,
gamma2.value, u.value and v.value are respectively the speciﬁed values for K, γ1, γ2, u
and v, and the last input D.max determines the termination of the tree growing process once
the number of leaves per tree reaches or exceeds D.max. The function BoostR2 returns an
objective that contains the output of Boost-R with both static and dynamic features, i.e.,
Algorithm 2. The output generated by BoostR2 can be visualized by the following func-
tions: Plot Partition, Plot Individual, Plot Leaf, Plot Imp, and Plot Interaction.
Figure 13 is generated by Plot Imp(BoostR.out, standardize=TRUE), and shows the
importance of the eight static system attributes as the total gain respectively achieved
by splitting tree nodes based on each feature. The algorithm clearly identiﬁes x7 and x8,
the geo-locations, as the two most important system attributes. Because wells at simi-
lar geographical locations share common, but unknown, environmental conditions (e.g.,
temperature and humidity variation, soil type, contamination, etc.), geo-locations serve as
important proxies in capturing those unknown environmental factors which may lead to
some important spatial patterns such as trend and clustering. The results shown in Figure
13 conﬁrm that these unknown environmental factors signiﬁcantly inﬂuence the system
failure processes, leading to diﬀerent failure patterns among these well systems.

Figure 13

Feature importance for the 8 static well attributes, which is measured by the total gain achieved by
splitting tree nodes based on a given feature; see (11)

Next, we re-run the Boost-R algorithm by retaining the two static features, x7 and x8,
and the dynamic gearbox torque. Although one might as well include x3, x4, ..., x6, keeping
only x7 and x8 allows us to eﬀectively visualize the interesting interaction between the
estimated B-splines coeﬃcients β·,1 and spatial locations x7 and x8.

12345678featuresfeature importance (standardized)0.00.20.40.60.81.0TITLE: Boost-R
manuscript no. 00-00

21

Figure 14

Aggregated B-splines coeﬃcients over the spatial domain. Subplots (a) to (e) respectively correspond
to the coeﬃcients β1,1, β2,1, · · · , β5,1.

Because cubic splines with two internal knots are used in this example, we have β·,1 =
(β1,1, β2,1, · · · , β5,1). Figure 14, which is generated by Plot Interaction(BoostR.out),
provides a spatially aggregated view of the ﬁve estimated B-splines coeﬃcients over the
spatial domain. Note that, each boosting tree partitions the spatial domain [0, 1]2 into
several rectangular areas and the estimated value of β·,1 is obtained for each area. Because
β·,1 can be viewed as the eﬀects of the dynamic feature on the recurrent event processes,
it is immediately seen that such eﬀects vary over the spatial domain. In other words, the
cumulative failure intensities at diﬀerent geo-locations are inﬂuenced by the operational
conditions. For example, the aggregated values of β1,1 appears to be lower in the area where
0.2 < x7 < 0.25 and x8 > 0.6, while the aggregated value of β2,1 is larger in approximately
the same area. This observation strongly demonstrates the eﬀectiveness of Boost-R in cap-
turing the interactions between static and dynamic features, by leveraging the advantages
of binary tree structures.

Using Plot Individual(BoostR.out), Figure 15 shows the estimated cumulative fail-
ure intensity and cumulative failure counts of four selected well systems (left column). The
observed gearbox torque for these well systems are also shown in the right column. We see
that, Boost-R successfully estimates the cumulative failure intensity for heterogeneous indi-
viduals with diverse system attributes (static features) and operating conditions (dynamic
feature). Such an observation is encouraging and demonstrates the potential of Boost-R
for recurrent event data analytics: the system heterogeneity is addressed by the “divide-
and-conquer” structure of binary trees, and the non-parametric approaches (including the

0.00.20.40.60.81.00.00.20.40.60.81.0(a)x7x8<−0.049−0.034−0.02−0.00470.010.025>0.040.00.20.40.60.81.00.00.20.40.60.81.0(b)x7x8<−0.063−0.034−0.00430.0250.0550.084>0.110.00.20.40.60.81.00.00.20.40.60.81.0(c)x7x8<−0.094−0.068−0.042−0.0170.00880.034>0.060.00.20.40.60.81.00.00.20.40.60.81.0(d)x7x8<−0.029−0.016−0.00360.0090.0220.034>0.0470.00.20.40.60.81.00.00.20.40.60.81.0(e)x7x8<−0.76−0.61−0.45−0.29−0.140.022>0.1822

TITLE: Boost-R
manuscript no. 00-00

Figure 15

Estimated cumulative intensity (left) and gearbox torque (right) for selected well systems.

binary tree and B-splines) are used to capture the complex, often non-linear, interactions
between recurrent event processes and feature information without imposing parametric
assumptions.

In the Appendices, we provide additional application examples and comparison studies
between Boost-R and other methods. In particular, we also provide some discussions on
the potential use of XGBoost for recurrent event data.

4. Conclusions
This paper proposed an additive-tree-based statistical learning approach, known as Boost-
R (Boosting for Recurrence Data), for modeling recurrent event data with both static and

05001000150020000246dayscumulative intensityindividual 24605001000150020000.00.40.8dayssensor measurementsindividual 24602004006008001000120002468dayscumulative intensityindividual 2690200400600800100012000.00.40.8dayssensor measurementsindividual 26905001000150020000.01.02.03.0dayscumulative intensityindividual 28605001000150020000.00.40.8dayssensor measurementsindividual 2860500100015002000250002468dayscumulative intensityindividual 320050010001500200025000.00.40.8dayssensor measurementsindividual 320TITLE: Boost-R
manuscript no. 00-00

23

dynamic feature information. The technical details behind Boost-R have been presented.
Gradient boosting algorithms have been developed to obtain an ensemble of correlated
trees that generate the estimated cumulative intensity functions characterizing the recur-
rent event processes given feature information. To our best knowledge, Boost-R is the
ﬁrst gradient boosted additive-tree-based model for recurrence data with both static and
dynamic features.

The advantages of Boost-R are due to three salient features behind this approach:
(i) Boost-R leverages the “divide-and-conquer” structure of binary trees to address the
inevitable heterogeneity among a large population of individuals; (ii) the non-parametric
nature of the algorithm (e.g., binary tree and B-splines) enables us to capture the com-
plex and non-linear relationship between event processes and features, which may not be
adequately captured by parametric approaches; (iii) Boost-R is built into the framework
of gradient boosted trees, which has proven to be one of the most successful statistical
learning approaches over the past decade. Comprehensive numerical studies, including a
case study, have been performed to demonstrate the advantages of Boost-R. R code has
been made available on GitHub to facilitate the adoption of this new approach.

References
Anderson, P. K., Borgan, O., Gill, R. D., and Keiding, N. (1993), Statistical Models based on Counting

Processes, New York, NY: Springer-Verlag.

Bacchetti, P. and Segal, M. (1995), “Survival Trees with Time-Dependent Covariates: Application to Esti-

mating Changes in the Incubation Period of AIDS,” Lifetime Data Analysis, 1, 35–47.

Bou-Hamad, I. (2011), “A Review of Survival Trees,” Statistics Surveys, 5, 44–71.

Bou-Hamad, I., Larocque, D., Ben-Ameur, H., Masse, L., Vitaro, F., and Tremblay, R. (2009), “Discrete-Time

Survial Trees,” Canadian Journal of Statistics, 37, 17–32.

Chen, T. and Guestrin, C. (2016), “XGBoost: A Scalable Tree Boosting System,” arXiv:, 1603.02754v3.

Chipman, H. A., George, E. I., and McCulloch, R. E. (2010), “BART: Bayesian Additive Regression Trees,”

The Annals of Applied Statistics, 4, 266–298.

Fan, J., Nunn, M., and Su, X. G. (2009), “Multivariate Exponential Survival Trees and Their Application

to Tooth Prognosis,” Computational Statistics and Data Analysis, 53, 1110–1121.

Fan, J., Su, X. G., Levine, R., Nunn, M., and Leblanc, M. (2006), “Trees for Censored Survival Data by Good-
ness of Split, with Application to Tooth Prognosis,” Journal of the American Statistical Association,
101, 959–967.

Fleming, T. R. (1991), Counting Processes and Survival Analysis, New York, NY: John Wiley & Sons.

Freund, Y. and Schapire, R. E. (1997), “A Decision-Theoretic Generalization of On-line Learning and an

Application to Boosting,” Journal of Computer and System Sciences, 55, 119–139.

Grob, G. L., Cardoso, A., Liu, C., Little, D. A., and Chamberlain, B. P. (2018), “A recurrent neural network
survival model: predicting web user return time,” in Proceedings of the Joint European Conference on
Machine Learning and Knowledge Discovery in Databases (ECML-PKDD 2018), pp. 152–168.

Guyon, I. and Elisseeﬀ, A. (2003), “An Introduction to Variable and Feature Selection,” Journal of Machine

Learning Research, 3, 1157–1182.

Harrell, F., Caliﬀ, R., Pryor, D., Lee, K., and Rosati, R. (1982), “Evaluating the Yield of Medical Tests,”

Journal of American Medicine Association, 247, 2543–2546.

Hastie, T., Tibshirani, R., and Friedman, J. (2009), The Elements of Statistical Learning, 2nd Edition, New

York: Springer.

24

TITLE: Boost-R
manuscript no. 00-00

Hothorn, T., B¨uhlmann, P., Dudoit, S., Molinaro, A., and Van Der Laan, M. (2006), “Survival Ensembles,”

Biostatistics, 7, 355–373.

Huo, X. M., Kim, S. B., Tsui, K. L., and C., W. S. (2006), “FBP: A Frontier-Based Tree-Pruning Algorithm,”

INFORMS Journal on Computing, 18, 494–505.

Ishwaran, H. and Kogalur, U. B. (2010), “Consistency of Random Survival Forests,” Statistics Probability

Letter, 80, 1056–1064.

Ishwaran, H., Kogalur, U. B., Blackstone, E. H., and Lauer, M. S. (2008), “Random Survival Forests,” The

Annals of Applied Statistics, 2, 841–860.

Ishwaran, H., Kogalur, U. B., Gorodeski, E. Z., Minn, A. J., and Lauer, M. S. (2010), “High-Dimensional

Variable Selection for Survival Data,” Journal of the American Statistical Association, 105, 205–217.

Jordan, M. (2019), “Artiﬁcial Intelligence–The Revolution Hasn’t Happened Yet,” Harvard Data Science

Review, 1, https://hdsr.mitpress.mit.edu/pub/wot7mkc1.

Joseph, V. R. (2016), “Space-ﬁlling designs for computer experiments: A review,” Quality Engineering, 28,

28–35.

Joseph, V. R., Gul, E., and Ba, S. (2015), “Maximum Projection Designs for Computer Experiments,”

Biometrika, 102, 371–380.

Katzman, J. L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., and Kluger, Y. (2018), “Deepsurv: person-
alized treatment recommender system using a cox proportional hazards deep neural network,” BMC
medical research methodology, 18:24.

Kelly, P. J. and Lim, L. (2000), “Survival Analysis for Recurrent Event Data: An Application to Childhood

Infectious Diseases,” Statistics in Medicine, 19, 13–33.

Lao, J., Chen, Y., Li, Z.-C., Li, Q., Zhang, J., Liu, J., and Zhai, G. (2017), “A deep learning-based radiomics

model for prediction of survival in glioblastoma multiform,” Scientiﬁc Report, 7, 10353.

Lee, C., Zame, W. R., Yoon, J., and van der Schaar, M. (2018), “A deep learning approach to survival
analysis with competing risks,” in Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence
(AAAI-18).

Lewis, P. and Shedler, G. (1979), “Simulation of Nonhomogenous Poisson Processes by Thinning,” Naval

Research Logistics Quarterly, 26, 403–413.

Liu, X. and Pan, R. (2020), “Analysis of Large Heterogeneous Repairable System Reliability Data with Static
System Attributes and Dynamic Sensor Measurement in Big Data Environment,” Technometrics, 62,
206–222.

Meeker, W. Q. and Escobar, L. A. (1998), Statistical Methods for Reliability Data, New York, NY: John

Wiley & Sons.

Nelson, W. (1995), “Conﬁdence Limits for Recurrence Data: Applied to Cost or Number of Product Repairs,”

Technometrics, 37, 147–157.

Nilsson, R., Pena, J. M., Bjorkegren, J., and Tegner, J. (2007), “Consistent Feature Selection for Pattern

Recognition in Polynomial Time,” Journal of Machine Learning Research, 8, 589–612.

Paynabar, K., Jin, J., and Reed, M. P. (2015), “Informative Sensor and Feature Selection via Hierarchical

Nonnegative Garrote,” Technometrics, 57, 514–523.

Ranganath, R., Perotte, A., Elhadad, N., and Blei, D. (2016), “Deep survival analysis,” Machine Learning

for Healthcare Conference, 101–114.

Reunanen, J. (2003), “Overﬁtting in Making Comparisons Between Variable Selection Methods,” Journal of

Machine Learning Research, 3, 1371–1382.

Wang, P., Li, Y., and Reddy, C. K. (2017), “Machine Learning for Survival Analysis: A Survey,” arXiv:,

1708.04649.

Witten, D. M. and Tibshirani, R. (2010), “A Framework for Feature Selection in Clustering,” Journal of the

American Statistical Association, 105, 713–726.

TITLE: Boost-R
manuscript no. 00-00

25

Yuan, M. and Lin, Y. (2005), “Eﬃcient Empirical Bayes Variable Selection and Estimation in Linear Models,”

Journal of the American Statistical Association, 100, 1215–1225.

— (2007), “Model selection and estimation in regression with grouped variables,” Journal of the Royal

Statistical Society, Series B, 68, 49–67.

Appendix A: Discussions on the Ad-Hoc Use of XGBoost for

Recurrence Data

[*Appendix A can be moved to supplementary materials if necessary]

If time t is treated as an additional feature, XGBoost is sometimes used in industry to
model recurrent event data. Although the oﬀ-the-shelf XGBoost can learn the relationship
between the cumulative number of events and time and other features, such an ad hoc use
of XGBoost may have a few major limitations:

(cid:46) When time is treated as another feature, the predicted cumulative intensity is typically
“bumpy” because the cumulative events at diﬀerent times are learned independently. To
illustrate this point, Figure 16 below shows how the predicted cumulative intensity from
XGBoost and the proposed Boost-R typically look like. Because XGBoost models the
number of cumulative failures at diﬀerent times independently, the predicted intensity at a
given time depends on the number of observations available at that time and the observed
cumulative events from those available samples at that time. As a result, it is not surprising
that the predicted cumulative intensity from XGBoost is rarely smooth. Boost-R, on the
other hand, learns a smooth time-dependent function at each tree leaf, and the ensemble
prediction of the cumulative intensity is also smooth.

Figure 16

An example that shows how the predicted cumulative intensity from XGBoost and the proposed
Boost-R typically look like. The output from XGBoost appears to be bumpy because it models the
cumulative events at diﬀerent times independently.

0204060801001200123456timecumulative intensityBoost−RXGBoostMCF26

TITLE: Boost-R
manuscript no. 00-00

(cid:46) More importantly, if the training data is censored in time (say, at time tc), the ad hoc
use of XGBoost (which treats time t as another feature) is incapable of event predictions at
a time beyond tc (i.e., extrapolation in time). This is due to the non-parametric nature of
tree-based methods which are ineﬀective in making extrapolations outside the range of the
feature in the training dataset. As a result, although XGBoost can learn the relationship
between the cumulative intensity and time (treated as a feature) over the interval [0, tc],
the tree-based method cannot predict the future event process over a time interval beyond
the censoring time tc (i.e., outside the range of “time” in the training dataset).

As clearly shown in the Figure 17 below (more details are provided in the Appendix of
the revised manuscript), the predicted cumulative intensity remains a constant if XGBoost
is used with time being treated as a feature. In other words, such a use of XGBoost (after
treating time t as another feature) is incapable of making predictions beyond the censor-
ing tc. In the proposed Boost-R, however, because each terminal node contains a time-
dependent function (rather than a constant), the predicted cumulative intensity function
from Boost-R is smooth and we are able to make extrapolations in time.

Figure 17

The ad hoc use of XGBoost is not capable of predicting the event process at a future time interval
beyond the censoring time (i.e., extrapolation in time).

Similarly, if the training data is under Type-II censoring, say, the observation of the
process is stopped after observing nc number of events, the ad-hoc use of XGBoost cannot
generate predictions that go beyond nc. For example, after training the XGBoost with a
training data under Type-II censoring (censored at nc), the model cannot answer common
questions such as when the (nc + 1)th failure will occur. This is because no sample in
the training dataset has more than nc failures under Type-II censoring, and the non-
parametric nature of the conventional tree-based methods prevents the algorithm from
making predictions beyond the largest number of failures in the training dataset.

0501001500123456timecumulative intensityBoost−RXGBoostMCFcensoring timeTITLE: Boost-R
manuscript no. 00-00

27

Appendix B: Additional Insights on Boost-R and Comparison Studies
[*Appendix B can be moved to supplementary materials if necessary]

We provide additional discussions and comparison studies of the proposed Boost-R using

another application example.

Application. In Kelly and Lim (2000), the authors investigated the recurrent event data
modeling for childhood infectious acute respiratory illness (ARI). The goal of the study was
to understand the eﬀect of childhood MORbidity from supplementation of VITamin A—
the MORVITA trial. This was a randomized double-blinded placebo-controlled trial with
1405 subjects aged 6-47 months. Once a child was randomized (to vitamin A or placebo)
they received the same treatment throughout the study.

Each subject has a maximum of four events (i.e., Type-II censoring), and the events are
censored if the additive total time since the start of the study is greater than 120 days
(i.e., Type-I censoring).

Model. Kelly and Lim (2000) considered a random-eﬀect model. For a subject i, let λik

be the intensity between the (k − 1)th event and the kth event (k = 1, 2, 3, 4), and

log λik(t) = β0 + βkZik + vi

(22)

where β0 and βk are the eﬀects, Zik = 1 if the subject receives treatment after the (k − 1)th
event otherwise Zik = 0, and vi ∼ N (0, σ2) is a random eﬀect covariate that introduces the
within-subject correlation.

Diﬀerent subjects respond to the treatment diﬀerently. For example, if the treatment is
constantly eﬀective, β1 = β2 = β3 = β4 = −1. If the treatment is only eﬀective for the ﬁrst
event, β1 = −1 and β2 = β3 = β4 = 0.

Data. In our experiment, we simulate the data for 1000 subjects based on the model
in Kelly and Lim (2000). For each subjects, we randomly simulate two features x1 and x2
from a uniform distribution on [0, 1], and consider four potential sub-populations as follows
• if xi,1, xi,2 < 0.5, the treatment is only eﬀective for the ﬁrst event, i.e., β1 = −1 and

β2 = β3 = β4 = 0;

• if xi,1 < 0.5 and xi,2 ≥ 0.5, the treatment is eﬀective for the ﬁrst two events, i.e.,

β1 = β2 = −1 and β3 = β4 = 0;

• if xi,1 ≥ 0.5 and xi,2 < 0.5, the treatment is eﬀective for the ﬁrst three events, i.e.,

β1 = β2 = β3 = −1 and β4 = 0.

• if xi,1 ≥ 0.5 and xi,2 ≥ 0.5, the treatment is eﬀective for the all four events, i.e., β1 =

β2 = β3 = β4 = −1.

Hence, diﬀerent subjects respond to treatment diﬀerently. Even for subjects from the
same sub-group, the random eﬀect, vi ∼ N (0, σ2), further introduces the within-sample
correlation. In the subsequent comparison studies, we consider three diﬀerent values for σ,
i.e., 0, 0.1 and 0.4, which were also considered in Kelly and Lim (2000).

All datasets are available on GitHub (https://github.com/dnncode/Boost-R).

Comparison. Three methods are included in the comparison study: Boost-R, RF-R
and XGBoost (described in Appendix A). For each method, data from 500 subjects are
used to train the model, and data from the remaining 500 subjects are used to test the
model performance. Figures 18, 19 and 20 below show the box plot of the squared L2
distance between the predicted cumulative intensity and the observed cumulative intensity

28

TITLE: Boost-R
manuscript no. 00-00

over the time interval from 0 to 120 days, respectively for three datasets assuming diﬀerent
values for σ, i.e., 0, 0.1 and 0.4.

In each ﬁgure, four combinations of the tuning parameters (γ1 and γ2) are used for Boost-
R models. In particular, Boost-R-1, Boost-R-2, Boost-R-3 and Boost-4 are respectively
based on the following combinations of γ1 and γ2: (10, 10), (10, 50), (50, 10) and (50, 50).
Three diﬀerent choices of the learning rate, including 0.1, 0.5 and 1, are respectively used for
XGBoost-1, XGBoost-2 and XGBoost-3. RF-F does not involve major tuning parameters.
Figures 18, 19 and 20 all indicate that the proposed Boost-R provides the best perfor-
mance for all three datasets. In fact, considering the additional limitations of the ad-hoc
use of XGBoost (discussed in Appendix A), Boost-R appears to be a good choice for such
an application. In addition, the performance of all methods deteriorates when σ becomes
larger, as expected.

Figure 18

Comparison between Boost-R, RF-R and XGBoost: box plot of the squared L2 distance between the
predicted cumulative intensity and the observed cumulative intensity over 0 to 120 days (σ = 0)

Boost−R−1Boost−R−2Boost−R−3Boost−R−4XGBoost−1XGBoost−2XGBoost−3RF−R050100150200250300squared errorTITLE: Boost-R
manuscript no. 00-00

29

Figure 19

Comparison between Boost-R, RF-R and XGBoost: box plot of the squared L2 distance between the
predicted cumulative intensity and the observed cumulative intensity over 0 to 120 days (σ = 0.1)

Figure 20

Comparison between Boost-R, RF-R and XGBoost: box plot of the squared L2 distance between the
predicted cumulative intensity and the observed cumulative intensity over 0 to 120 days (σ = 0.4)

Boost−R−1Boost−R−2Boost−R−3Boost−R−4XGBoost−1XGBoost−2XGBoost−3RF−R0100200300squared errorBoost−R−1Boost−R−2Boost−R−3Boost−R−4XGBoost−1XGBoost−2XGBoost−3RF−R0100200300400squared error30

TITLE: Boost-R
manuscript no. 00-00

We further compare the extrapolation capabilities between Boost-R and the ad-hoc use
of XGBoost. In particular, we train the model using the data from 500 subjects over the
time interval between 0 and 120 days, and use the model to predict the cumulative number
of events at 240 days for the 500 subjects in the testing dataset (i.e., extrapolation in time).
Figures 21, 22 and 23 shows the box plot of the MSE of the predicted cumulative number
of events at 240 days. All three ﬁgures well illustrate the advantage of Boost-R in terms
of extrapolating the number of events beyond the censoring time. Of course, as already
discussed in Appendix A, the non-parametric nature of tree-based methods prevents the
ad-hoc use of XGBoost to predict the future event process beyond the censoring time (i.e.,
outside the range of “time” in the training dataset).

Figure 21

Box plot of the MSE of the predicted cumulative number of events at 240 days (σ = 0)

Boost−R−1Boost−R−2Boost−R−3Boost−R−4XGBoost−1XGBoost−2XGBoost−30200400600800squared errorTITLE: Boost-R
manuscript no. 00-00

31

Figure 22

Box plot of the MSE of the predicted cumulative number of events at 240 days (σ = 0.1)

Figure 23

Box plot of the MSE of the predicted cumulative number of events at 240 days (σ = 0.4)

Boost−R−1Boost−R−2Boost−R−3Boost−R−4XGBoost−1XGBoost−2XGBoost−30200400600800squared errorBoost−R−1Boost−R−2Boost−R−3Boost−R−4XGBoost−1XGBoost−2XGBoost−3020040060080010001200squared error