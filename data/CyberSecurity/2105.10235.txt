Resilience in the Cyber World: 
Deﬁnitions, Features and Models 

Elisabeth Vogel1, Zoya Dyka1, Dan Klann1  and Peter Langendörfer1,2 

1 IHP  -  Leibniz-Institut  fu¨r  innovative  Mikroelektronik,  Frankfurt  (Oder),  Germany 
BTU  Cottbus-Senftenberg,  Cottbus,  Germany 

2 

{vogel,  dyka,  klann,  langendoerfer

}

@ihp-microelectronics.com 

Abstract 

Resilience  is  a  feature  that  is  gaining  more  and  more  attention  in  computer 

science  and  computer  engineering.  However,  the  deﬁnition  of  resilience  for  the 

cyber  landscape,  especially  embedded  systems,  is  not  yet  clear.  This  paper 

discusses deﬁnitions of diﬀerent authors, years and diﬀerent application areas the 

ﬁeld of computer science/computer engineering.  We identify the core statements 

that  are  more  or  less  common  to  the  majority  of  the  deﬁnitions  and  based  on 

this we give a holistic deﬁnition using attributes for (cyber-) resilience.  In order 

to  pave  a  way  towards  resilience-engineering  we  discuss  a  theoretical  model  of 

the life cycle of a (cyber-) resilient system that consists of key actions presented 

in the literature.  We  adapt this model for embedded (cyber-) resilient systems. 

Keywords:  cyber-resilience, security, redundancy, resilience 

engineering 

1 .  Introduction 

The cyber landscape of the 21st century is constantly growing and becoming 

increasingly  complex  covering  areas  such  as  telemedicine,  autonomous  driving 

etc.  Our societies as well  as individuals are highly dependent on these systems 

5 

working  correctly  and  24/7.  In  order  to  be  able  to  cope  with  the  increasing 

complexity and the unprecedented importance of cyber systems, new and inno- 

vative  methods  and  technologies  have  to  be applied.  The  concept  of  resilience 

  
 
 
 
 
is getting increasing attention in this respect, which is reﬂected above all by the 

steadily  growing  number  of  publications  on  the  topic.  Figure  1  shows  how  the 

1 0 

number of publications has increased since 2005.  The diagram in Figure 1 shows 

only  publications  with  the  keyword  Cyber-Resilience.  Beneath  its  attention  in 

science the concept of resilience already reached industry.  US-American stream- 

ing provider Netﬂix is considered a pioneer in the application of resilience in the 

form of highly redundant infrastructure.  But the principles of resilience are not 

1 5 

only  found  in  the  hardware  components  of  Netﬂix.  The  software  architecture 

also demonstrates the application of various methods to increase resilience.  The 

example of Netﬂix shows how important resilience is with increasing complexity 

[ 1]. 

000 
8 

000 
7 

000 
6 

000 
5 

000 
4 

000 
3 

000 
2 

000 
1 

0 

Figure 1:  Number of publications  with  the keyword  Cyber-Resilience  from 2005 to  2019  [2]. 

Year 

2 

  
 
 
But the term “resilience” is used in many ways in IT. In some cases, resilience 

2 

0 

is described as “extreme reliability” [3] or used as a synonym for fault tolerance 

[ 4],  [5].  In  [6]  it  is  described  that  resilience  is  fault  tolerance  with  the  key 

attribute robustness.  Anderson in [6] extended the deﬁnition of fault tolerance 

by  the  property  robustness  and  called  the  new  deﬁnition  resilience.  In  recent 

publications, however, resilience is deﬁned several times as an independent term 

2 

5 

[7] [8]. 

This  clearly  shows  that  the  deﬁnition  of  resilience  in  IT  as  well  as  in  other 

areas is certainly extensive and varied.  While we admit that the term resilience 

is diﬃcult to grasp we are also convinced that as a central concept of upcoming 

new IT-systems it needs a clear commonly accepted deﬁnition, metrics, etc.  In 

3 

0 

order  to  achieve  this,  we  give  an  overview  of  the  deﬁnitions  and  properties  of 

resilience and resilient systems in IT. For  the purpose of a consistent presenta- 

tion,  we  selected  the  publications  as  representative  as  possible.  Table  1  shows 

the  publications  considered  in  this  publication.  This  list  of  publications  is  of 

course only a very small selection, this is for a better overview.  However, it also 

3 

5 

shows  the  diﬀerent  approaches  of  the  authors,  when  deﬁning  resilience.  Fur- 

ther literature with similar perspectives is also noted at the appropriate places. 

From  the publications  given  in Table  1,  the following information  on  resilience 

was  extracted  (where  available):  deﬁnitions,  attributes,  models.  Furthermore, 

we discuss our model, which describes the structure of a resilient system.  With 

4 

0 

the  help  of  this  model  the  development  of  a  resilient  system  should  be  facili- 

tated.  These  approaches  are  critically  analysed  and  contrasted  with  our  own 

holistic  understanding  of  resilience.  Finally,  an  example  is  used  to  show  how 

approaches  of  resilience  are  already  being  implemented  and  in  which  direction 

the development could go in the future. 

3 

  
 
 
 
 
Table  1:  The  columns  title,  author(s),  year  show  the  papers  we  used  as  the  main  sources 

for this work, the column other relevant  sources indicate additional publications that use the 

term  resilience  in a similar  way  as the  main source  in  the  same row. 

Year  Title  &  Author(s) 

Further 

Sources 

976  A  Principle  for  Resilient  Sharing  of  Distributed  Re- 
1 

sources  [3] 

Peter  A. Alsberg; John D. Day 

007  Release it!  Design and Deploy Production-Ready Soft- 
2 

[10] 

ware  [9] 

Michael  T. Nygard 

008  From  Dependability  to  Resilience  [11] 
2 

Jean-Claude Laprie 

[5], [12], 

[13] 

011  Prologue:  The  scope  of  resilience  engineering  [14] 
2 

[15], [16] 

Erik Hollnagel 

013  On  the  Constituent  Attributes  of  Software  and 
2 

[4], [18] 

Organisational  Resilience  [17] 

Vincenzo De Florio 

2 015  Quantifying coastal system resilience for the US Army 

[19], [20], 

Corps  of  Engineers  [7] 

[21], [22] 

Julie Dean Rosati; Katherine  F. Touzinsky; 

W. Jeﬀ Lillycrop 

016  What’s  the  Diﬀerence  between  Reliability  and 
2 

[24], [25] 

Resilience?  [23] 

Aaron Clark-Ginsberg 

018  Systems  Security  Engineering:  Cyber  Resiliency  Con- 
2 

[26], [27], 

siderations  for  the  Engineering  of  Trustworthy  Secure 

[28], [29] 

Systems  (NIST  Special  Publication  800-160,  Volume 

2 )  [8] 

Ron Ross; Richard Graubard; Deborah J. Bodeau; 

Rosalie McQuaid 

4 

  
 
 
4 

5 

Furthermore,  we  discuss  our  model,  which  describes  the  structure  of  a  re- 

silient system.  With the help of this model the development of a resilient system 

should be facilitated. 

The  rest  of  the  paper  is  structured  as  follows.  Section  2  shows  diﬀerent 

deﬁnitions  of  diﬀerent  authors,  years  and  application  areas  for  resilience.  In 

5 

0 

section  3  the  same  authors  as  in  section  2  are  considered  again,  but  this  time 

under  the  aspect  of  attributes  describing  resilience.  Section  4  describes  the 

model  of  key  actions,  which  was  already  brieﬂy  mentioned  in  section  2.  This 

model of key actions will be reinterpreted in section 4 according to our ideas. 

In section 5 we show current applications of resilience and the transferability 

5 

5 

to  embedded  systems.  In  addition,  we  discuss  how  to  model  and  implement 

resilience. 

2 .  Deﬁnitions 

In the literature of recent years, there are many deﬁnitions of resilience, some 

of  which  diﬀer  considerably.  As  described  in  [4],  the  content  of  the  deﬁnitions 

6 0 

strongly  depends  on  the  respective  ﬁelds  of  application.  Resilience  is  derived 

from the Latin resilire and can be translated as “bouncing back”  or  “bouncing 

oﬀ”.  In essence, the term is used to describe a particular form of resistance. 

How the term resilience is used in diﬀerent disciplines (material science, en- 

gineering, psychology, ecology) is described in [30].  Also in computer science the 

6 5 

term resilience has been deﬁned several times from diﬀerent points of view.  As 

described in [4] for example, resilience is often used as a synonym for fault toler- 

ance.  However,  recent  publications  show  that  this  approach  has  been replaced 

by  the view that resilience is much more than error tolerance (see [8]). 

One of the ﬁrst deﬁnitions was presented in [3] and describes the concept of 

resilience as follows: 

” He  (remark:  the  user)  should  be  able  to  assume  that  the  system  will 

make a ”best-eﬀort” to continue service  in the event that perfect  service 

7 0 

5 

  
 
 
 
 
cannot be  supported; and that the system will not fall apart when he does 

something  he  is  not  supposed  to.” 

In  addition,  [3]  mentions  attributes  that  constitute  resilience  as  part  of  its 

deﬁnition.  The  attributes  are  the  following:  error  detection,  reliability, 

development  capability  and  protection  against  misuse in the sense that 

the  misuse  of  a  system  by  individual  users  has  only  negligible  eﬀects  on  other 

7 5 

users.  According  to  Alsberg  [3],  these  four  attributes  of  a  resilient  system  can 

be summarized as the attempt to describe extreme reliability and serviceability. 

In  summary,  a  partial  failure  of  a  system  should  not  have  any  eﬀect  on  an 

individual  user,  so  the  system  can  be  assumed  to  be  highly  reliable.  Should 

nevertheless a partial failure or a defect occur, the best possible continuation of 

8 0 

the services provided should be guaranteed.  In extreme cases, this continuation 

can also be achieved by  performing graceful degradation of services. 

The approach of continuing a service of a system even under transient eﬀects, 

permanent load or failures is also described in [9]: 

” A  resilient  system  keeps  processing  transactions,  even  when  there  are 

transient  impulses,  persistent  stresses,  or  component  failures  disrupting 

normal  processing.  This  is  what  most  people  mean  when  they  just  say 

stability.  It’s  not  just  that  your  individual  servers  or  applications  stay 

up  and  running  but  rather  that  the  user  can  still get  work  done.” 

According to Nygard [9], a system must remain stable in case of tensions or 

8 5 

stress situations or failures.  As consequence, involved (sub-) systems or possibly 

also users can still continue their work.  The system must also be able to continue 

fulﬁlling  at  least  its  rudimentary  functions  despite  any  restrictions  that  may 

occur.  The scope of these rudimentary functions may have been deﬁned as part 

of  the  Risk  Management,  for  example.  Risk  management  also  shows  at  what 

9 0 

level of functional loss the entire system can no longer function according to its 

speciﬁcations. 

6 

  
 
 
 
 
In  [11]  resilience  is  described  as  the  persistence  of  service  delivery  when 

changes occur that  have  system-wide eﬀects.  These changes can be functional, 

environmental  or  technological.  In  addition,  changes  can  either  be  planned 

9 5 

(for  example:  initialized  by  an  update),  the  timing  of  their  occurrence  can  be 

unpredictable, or they can be completely unexpected.  The duration of changes 

is also taken into account:  short-term, medium-term, long-term.  This refers to 

the duration of the impact of the change on the system or a subsystem.  Figure 

2 shows the classiﬁcation of changes schematically. 

Figure 2:  Change classiﬁcation (source:  [11]). 

1 00 

The  paper  by  Laprie  [11]  proposes  two  deﬁnitions  of  resilience.  The  ﬁrst 

deﬁnition is as follows: 

” The  persistence  of  service  delivery  that  can  justiﬁably  be  trusted,  when 

facing  changes.” 

According  to  Laprie,  this  deﬁnition  corresponds in  principle  to  the  original 

deﬁnition of reliability.  In a second deﬁnition, Laprie oﬀers an alternative which 

provides a more detailed description: 

” The  persistence  of  the  avoidance  of  failures  that  are  unacceptably  fre- 

quent  to  severe,  when  facing  changes.” 

1 05 

In  [14],  resilience  is  described  as  the  ability  to  react  to  an  event.  This 

capability includes continuous observation of  the performance of the system or 

7 

  
 
 
 
a  service  provided  by  the  system  (self-monitoring),  the  recognition  of  future 

threats and learning from past failures. 

In the collection of papers from 1985 [6], robustness was already mentioned 

1 

10 

in  connection  with  resilience.  About  30  years  later,  in  [17]  this  connection  is 

concretized.  Resilience is deﬁned as the trustworthiness of a software system to 

adapt  to  adverse  conditions.  The  software  system  should  accept  and  tolerate 

the consequences of failures, attacks and changes inside and outside the system 

boundaries.  This is deﬁned as an approach for robustness: 

” Software resilience refers to the robustness of the software infrastructure 

and may be  deﬁned  as the trustworthiness  of  a software system to adapt 

itself  so  as  to  absorb  and  tolerate  the  consequences  of  failures,  attacks, 

and  changes  within  and  without  the  system  boundaries.” 

1 

15 

The deﬁnition of resilience was further speciﬁed in [17].  Florio [17] refers to 

the deﬁnition already mentioned in [4] and another deﬁnition in [31].  This deﬁ- 

nition states that resilience can be characterized as a measure of the persistence 

of  both  functional  and  non-functional  features  of  a  system  under  certain  and 

unpredictable disturbances.  After analyzing these two  deﬁnitions, according to 

20  Florio, resilience is the ability to act and balance between two  main behaviors: 
1 

1 )  Continuous  readjustment  with  the  aim  of  improving  the  system  environ- 

ment ﬁt and compensating for both foreseeable and unforeseeable changes 

in the system environment. 

2 )  Ensure  that  the  said  changes  and  adjustments  from  1)  do  not  aﬀect  the 

1 

25 

identity  of  the  system.  This  means  that  its  speciﬁc  and  distinctive  func- 

tional and non-functional features should not be aﬀected. 

[ 7] deals with the management of water resources and was written from the 

perspective of the USACE1. According to Rosati [7], resilience is a cycle consist 

of anticipation, resistance, recovery and adaptation.  Anticipation is the starting 

1 US  Army  Corps  of  Engineers 

8 

  
 
 
1 30  point of the cycle, while adaptation marks the end.  The cycle is started by  the 

occurrence of an event that aﬀects the system in some way.  This event is called a 

disruption.  Speciﬁcally, Rosati deﬁnes resilience (in this case coastal resilience) 

as follows: 

” (Coastal)  resilience  is  deﬁned  as  the  ability  of  a  system  to  prepare, 

resist,  recover,  and  adapt  to  disturbances  in  order  to  achieve  successful 

functioning  through  time.” 

A  disturbance  occurs  here  as  an  eﬀect  of  a  hazard  on  the  infrastructure, 

1 

35 

system, etc.  A hazard is an environmental or adverse anthropogenic condition. 

The  article  by  Clark-Ginsberg  published  in  2016  [23]  deﬁnes  the  ability  of 

system to reduce the extent and the duration of disruptions as resilience.  Disrup- 

tive events  are  not  always  predictable, but when they occur they are supposed 

to lead to a learning and adaptation eﬀect of the system.  Adaptation is crucial 

40  when  it  comes  to  realizing  resilience  against  cyber  accidents,  since  the  cyber 
1 

landscape  is  developing  very  rapidly.  Clark-Ginsberg  says  in  his  article  that 

errors must  be detected and understood.  It must be possible for the system to 

adapt  to  the  errors  or  the  error  situation  and  a  fast  recovery  must  be guaran- 

teed.  The system must  recover  quickly after the occurrence of an error.  If this 

1 

45 

is  not  possible,  the  error  and  the  resulting  faulty  system  environment  must  be 

dealt with appropriately. 

The  U.S.  National  Institute  of  Standards  and  Technology  (NIST)  [8]  coins 

the  term  cyber-resilience  to  clearly  distinguish  its  approach  from  the  general 

deﬁnitions of resilience.  Cyber-resilience is the following property: 

” Cyber  Resilience  is  deﬁned  in  this  publication  as  “the  ability  to  antici- 

pate,  withstand, recover  from,  and adapt to adverse conditions, stresses, 

attacks,  or  compromises  on  systems  that  include  cyber  resources.” 

1 50 

According  to  NIST,  the  deﬁnition  of  cyber-resilience  refers  speciﬁcally  to 

all  entities  that  contain  cyber-resources.  A  cyber-resource  is  an  information 

9 

  
 
 
 
 
resource  that  creates,  stores,  processes,  manages,  transmits,  or  disposes  infor- 

mation in electronic form and that can be accessed over a network or by network 

methods.  The deﬁnition of NIST can therefore be applied to a system, a mech- 

55  anism,  a  component  or  a  system  element,  a  common  service,  an  infrastructure 
1 

or a system of systems, etc. 

The  publications  selected  here  show  that  the  type  and  scope  of  the  deﬁni- 

tions of resilience depend very much on the respective (informatics) application 

area.  However, some key actions can be ﬁltered out, which appear at least par- 

1 

60 

tially in all the publications considered here:  Anticipating, resisting, recovering, 

adapting  (to  threats  of  any  kind).  In  some  publications,  such  as  [7],  [8],  [32] 

these key actions are even mentioned explicitly.  Table 2 shows which key action 

is mentioned in which of the publications considered here. 

Table  2:  Sources mentioning key  actions 

No.  Publication 

key-action 

1 

2 

3 

4 

5 

6 

7 

8 

Alsberg, 1976 

Nygard, 2007 

Laprie, 2008 

Hollnagel, 2011 

Florio, 2013 

Rosati, 2015 

Clark-Ginsberg, 2016 

NIST, 2018 

Anticipation 

Recovery 

Resistance 

Adaptation 

These key actions and our understanding of their interaction with each other 

65  are  described  in  more  detail  in  section  4.  Each  of  the  four  key  actions  can  be 
1 

assigned diﬀerent attributes and behaviors.  They are described in the following 

section. 

1 0 

  
 
 
3 .  Attributes 

Section  2  introduced  the  key  actions  anticipation,  resistance,  recovery  and 

70  adaptation.  Each  key  action  comprises  several  attribute2.  These  attributes 
1 

can be derived directly from the deﬁnitions or were explicitly mentioned in the 

publications.  Table  3  shows  the  publications  discussed  in  detail  here  and  the 

key actions as well as the related attributes mentioned or deduced, respectively. 

[ 3]  describes  the  four  main  attributes  of  a  resilient  service.  First  of  all,  a 

1 

75 

resilient service must be able to detect and correct errors.  Further, the resilient 

service must be so robust and reliable that a user expects the service not to fail. 

If the service is capable of always detecting n errors and recovering from those 

errors, the (n+1) error is not catastrophic.  This only applies under the condition 

that  the  system  oﬀers  perfect  detection  and  recovery  of  n errors.  The  resilient 
service is therefore able  to anticipate the (n + 1)th  error in such a way  that  its 

80 

1 

negative  consequences  for  the  service  can  be  minimized.  This  corresponds  to 

a  simple  deﬁnition  of  evolvability.  As  a  fourth  key  attribute,  Alsberg  [3]  cites 

the ability of a resilient service to tolerate abuse by  a single user in such a way 

that this abuse has negligible impact on the other users of the service.  Alsberg 

85  does  not  specify  misuse,  but  if  a  malicious  and  intentional  action  is  assumed, 
1 

then  this  misuse  protection  corresponds  to  the  security  feature  of  availability. 

Alsberg summarizes the following attributes:  robustness, reliability, evolvability 

and security. 

2 In  the  publications  presented  here,  the  terms  attribute,  feature  and  measure  were  used 

synonymously.  For  the  sake  of  clarity,  only  the  term  attribute  will  be  used  in  this  article, 

representing  Feature  and  Measure. 

1 1 

  
 
 
Table  3:  Key actions and attributes  from  the  publications 

Publication 

Key  actions 

Attributes 

1 976 Alsberg [3] 

Anticipation 

Robustness 

Security 

007 Nygard [9] 
2 

008 Laprie [11] 
2 

Resistance 

Recovery 

Resistance 

Resistance 

Reliability 

Evolvability 

Reliability 

Stability 

Reliability 

Diversity 

Adaptation 

Stability 

Assessability 

Evolvability 

011 Hollnagel [14] 
2 

Anticipation 

Reliability 

Adaptation 

Evolvability 

013  Florio [17] 
2 

Resistance 

Reliability 

Integrity 

Adaptation 

Evolvability 

015 Rosati [7] 
2 

Anticipation 

Robustness 

Diversity 

2 
016 Clark-Ginsberg [23] 

Resistance 

Reliability 

Assessability 

2 
018 NIST [8] 

Recovery 

Adaptation 

Evolvability 

Integrity 

Security 

Safety 

Stability 

[ 9] claims that stability under all conditions is the most important property 

90  of  a  resilient  system.  According  to  Nygard  [9],  this  stability  is  directly  related 
1 

to the reliability of a resilient system.  This connection is obvious, since a system 

that is not stable cannot be reliable either. 

This understanding of stability and reliability is also illustrated in [11].  As- 

sessability is also an important property, because a resilient system must be able 

1 

95 

to  validate  the  correctness  or  plausibility  of  sensor  data,  for  example.  Laprie 

[ 11] also mentions diversity as another important basic property.  Diversity can 

be understood here as a basic idea of redundancy, because according to Laprie, 

diversity in a system (of hardware components, for example) should prevent the 

occurrence of single point of failure. 

2 

00 

In [14] it is  also described that reliability is a key  feature of  resilience.  The 

1 2 

  
 
 
ability  to  detect  a  fault  before  it  occurs  is  also  essential.  However,  this  only 

refers to faults  that  can be anticipated  on the basis of  existing information.  A 

resilient system  must  be able  to  minimise  the  negative eﬀects  of  a  disturbance 

by  anticipating  it.  This  is  done  by  constantly  updating  information  about  the 

05  disturbances  that  have  already  occurred  and  treated.  This  process  can  be un- 
2 

derstood as the ability to evolve.  According to [17], the following attributes are 

essential for a resilient system:  reliability, evolvability and integrity.  Reliability 

and evolvability are related to resilience, as described in the previous deﬁnitions. 

Integrity,  according  to  Florio  [17],  means  that  a  resilient  system  does  not  lose 

2 

10 

its intention after adaptation or application of changes regarding a failure.  This 

refers mainly to its functional and non-functional characteristics. 

In the publications [7], [8], [23], the abilities to anticipate, resist, recover and 

adapt  are  directly  mentioned  as  the  four  basic  attributes  or,  as  in  NIST,  the 

four basic goals of resilience. 

2 

15 

Castano [33] assigns a large number of diﬀerent attributes to resilience. 

Figure 3 shows  this in a schematic representation. 

Figure 3:  Attributes of resilience  (image based  on  the following  source:  [33]). 

Some of the attributes shown in Figure 3 and discussed in [33] build on each 

other or share identical approaches.  An obvious example is the ability to evolve, 

which  clearly lives from the ability to adapt.  For  an adaptation, in turn, some 

2 20 

form of reconﬁguration must always take place. 

1 3 

  
 
 
In the following section the key actions are described in more detail according 

to [7]. 

4 .  Model  of  key  actions 

The  key  actions  of  resilient  systems  were  introduced  in  section  2.  In  this 

2 

25 

section their dynamic relationships are shown and explained. 

The key actions are mentioned completely or partially in many publications 

but  Rosati  in  [7]  establishes  a  direct  connection  between  them  in  the  form  of 

a cycle  as shown  in  Figure  4.  Rosati is not  about implementing  an IT system, 

but  about  the  ability  of  diﬀerent  departments,  components  and  participants 

2 

30 

to  respond  when  a  disaster  occurs.  This  responsiveness  must  be  constantly 

improved  in  the  sense  of  Rosati’s  publication  in  order  to  keep  (permanent) 

damage  and  loss  of  life  as  low  as  possible.  The  concept  presented  by  Rosati  is 

intended to implement a system-wide approach that will support the challenges 

of  managing  the  United  States’  water  resource  infrastructure.  However,  this 

35  abstract concept can be transferred to IT systems.  This will be discussed later 
2 

in this section. 

The  cycle  shown  in  Figure  4  is  started  when  a  disturbance  occurs  and  is 

considered successful once it has been completed for a disturbance.  A learning 

eﬀect  is  considered  to  have  occurred  when  there  is  a  measurable  improvement 

2 

40 

if the same fault occurs again in the future. 

A  disturbance  is  considered  to  be an  adversity  that  has  negative  eﬀects  on 

the system.  Negative eﬀects are consequences that interfere with the intention 

of  the  system,  i.e.  in  a  certain  way  with  its  task.  In  the  worst  case,  the  com- 

promised  system  can  no  longer  fulﬁll  its  tasks.  Disturbances  can  be  caused 

45  by  malicious,  non-malicious,  anthropogenic  3,  non-anthropogenic,  internal  and 
2 

external inﬂuences. 

3 inﬂuenced by  humans,  caused by 

1 4 

  
 
 
Figure 4:  Presentation of key  actions in a cycle  (image based on the following  source:  [7]). 

The  four  key  actions  shown  in  Figure  4  and  the  resulting  cycle  are  to  be 

understood according to Rosati as follows: 

1 .  Prepare/Anticipate/Plan 

2 50 

This key action includes a natural process or, under certain circumstances, 

an  anthropogenic  activity  with  the  aim  of  preparing  the  system  for  a 

disturbance. 

.  Resist/Absorb/Withstand 
2 

This is the ability to withstand a disturbance while maintaining a certain 

2 55 

level of functionality. 

.  Recover/Bounce Back 
3 

The  lost  functionality  must  be restored.  If  it  is  not  possible  to  maintain 

functionality, the system shall be able to return to its original state. 

.  Adapt/Transform/Bounce Forward 
4 

2 60 

The ability to adapt involves  putting a  system into  a state that is better 

able to withstand or recover from the disruption.  Ideally, this adaptation 

leads to reduced loss of functionality and a shorter recovery time.  However, 

the process of adaptation only occurs when the cycle has been completed 

and  applies  only  to  this  type  of  disturbance.  Figure  5  shows  the  process 

1 5 

  
 
 
2 65 

of increasing resilience schematically. 

Figure 5:  Schematic representation  of the  learning  eﬀect.  A  system  is  inﬂuenced  by  a distur- 

bance and loses  functionality.  This loss  must  be resisted.  Through  the process of restoration 

and adaptation, a similar disruption will trigger a less severe loss of functionality in the future 

and  the  time to restore  the  system  is  shortened  (source:  [7]). 

In the following discussion we  present our  idea of extending this concept of 

key actions to IT systems and critical infrastructure systems. 

Our  model  of  key  actions  for  Cyber-Resilience 

The  process  of  increasing  resilience  can  be  interpreted  as  the  capacity  for 

2 70  evolution.  The system ”learns” how to better deal with a disturbance that has 

already occurred at least once.  This process is called evolvability. 

The term ”evolvability” originally comes from evolutionary biology and de- 

scribes the ability of a living organism to bring about a change in its character- 

istics (attributes) by changing its genes with the aim of improving its (survival) 

75  abilities. 
2 

Two approaches are therefore important for evolvability, which build on each 

other: 

.  Modiﬁcation of genes 
1 

.  Changes in attributes resulting from 1. 
2 

1 6 

  
 
 
2 

80 

The  relation  between  genes  and  attributes  can  be transferred  to  a  resilient 

system  as  follows:  genes  contain  among  other  things  the  basic  information  for 

the  evolution  of  the  attributes  of  a  living  organism.  The  key  actions  represent 

the  ”genes”  of  resilience  and  the  resilience  itself  has  a  multitude  of  attributes 

(reliability,  safety,  integrity,  etc.).  By  modifying  or  improving  the  key  actions 

2 

85 

(according to Figure 5) the properties of a resilient system can be improved. 

The model of the four key actions from [7] is very well suited for the purpose 

described  in  [7].  It  may  also  be  applicable  to  an  IT  system,  eventually  in  the 

area of critical infrastructure.  For the IT area, however, we believe that modiﬁ- 

cations of the  model are necessary.  First, another essential key  action must  be 

90  added:  error  analysis.  Furthermore,  from  our  point  of  view,  the  key  action 
2 

resistance  must  be considered  diﬀerently  for  a  (distributed)  IT  system.  An  IT 

system must  be able to continuously  resist current  and future disturbances.  A 

disturbance does not necessarily have to be an event that immediately has neg- 

ative consequences for the system.  A disturbance can also be an attack with the 

95  aim of stealing  information  (e.g.  private  keys).  Such  actions,  often carried out 
2 

as  side  channel  attacks  [34],  [35]  do  not  cause  any  direct,  immediately  visible 

damage.  However, if the extraction of the private key is successful, the system is 

considered broken from a security perspective and can no longer be described as 

resilient.  This eliminates the key action resistance from the cycle.  In our model 

3 

00 

resistance  is  divided  into  permanent  and  newly  learned  methods  of  resistance. 

This key action is continuously active, whereby ”newly learned” methods of re- 

sistance are added to the permanent methods of resistance after the disturbance 

has been eliminated.  Figure 6 shows our model of the key actions. 

1 7 

  
 
 
 
 
Figure  6:  Schematic  representation  of  the  key  actions  anticipation,  resilience  (permanent 

methods  and  new  (learned)  methods),  recovery  and  adaptation  and  the  added  key  action 

error analysis  (source:  [36]). 

The  components  of  our  model  are  described  below.  Please  keep  in  mind 

3 

05 

that  an  IT  system  is  under  constant  change  i.e.  its  state  concerning  the  key 

actions  is  also  constantly  changing.  The  boundaries  between  the  key  actions 

are not strict.  There is hardly a rapid transition between the states.  Thus, the 

individual  methods  of  e.g.  error  analysis  or  recovery  must  also  be considered. 

They interact with each other and partly also build on each other. 

3 10 

1.  Anticipation 

Anticipation  or  the  ability  to  anticipate  is  generally  understood  as  the 

capability  to  predict  future  conditions,  actions  or  events,  taking  into  ac- 

count  only  information  already  available.  Anticipation  is  executed  while 

the functionality of the system is within normal and expected parameters. 

3 15 

The system should be able to anticipate possible disturbances from already 

known  and/or collected information.  This ”previous knowledge” empow- 

ers  the  system  to  minimize  the  negative  consequences  of  an  anticipated 

disturbance  in  advance  or  even  to  prevent  the  disturbance  completely. 

1 8 

  
 
 
Whether  or not  it is  possible to  prevent  a disturbance  naturally  depends 

3 20 

on the type of disturbance. 

2 .  Error analysis 

The  error  analysis  basically  covers  four  points:  Error  detection,  error  lo- 

calization,  error  cause  and  error  type  determination.  This  key  action  is 

added  to  the  four  formerly  used  ones  especially  when  disturbances  oc- 

3 

25 

cur  which  could  not  be  anticipated  in  this  form  because  no  information 

was available (yet).  In this way,  error analysis correlates in a special way 

with anticipation.  If a disturbance passes through the cycle of key actions 

several times, this disturbance can be better and better anticipated later 

on,  which  supports  the  process  of  evolvability  considerably  and  shortens 

3 

30 

the  time  span  for  error  analysis.  This  can  be  traced  back  to  the  stored 

information  about  the  disturbance  from  previous  runs.  How  good  a  dis- 

turbance  can  be  anticipated  depends  on  the  type  and  complexity  of  the 

disturbance.  For error analysis it is essential that the disturbance has been 

detected, understood, localized and possibly even predicted by the system. 

3 

35 

Additionally  the  cause  of  the  disturbance  can  be  helpful.  However,  the 

cause often cannot be identiﬁed or eliminated. 

3 .  New (learned) methods of/for Resilience 

This  key  action  includes  methods  to  resist,  which  are  not  permanently 

active.  There is a pool of methods that are known to the system but inac- 

3 40 

tive.  If a disturbance occurs that is unknown to the system, i.e.  could not 

be anticipated,  additional  methods  to  resist  become  active.  If  this  is  the 

case, the required method is added to the Permanent Resilience Methods 

(see point 4.)  after the disturbance has been dealt with. 

A  corresponding  algorithm  decides  on  the  basis  of  various  parameters 

3 45 

(type  of  disturbance,  current  operating  environment,  dangerousness  of 

the  disturbance,  probability  of  the  reoccurrence  of  the  disturbance,...) 

whether  and  how  long  a  method  is  added  to  the  Permanent  Resilience 

methods.  It  also  decides  when  a  method  can  become  inactive  again,  e.g. 

1 9 

  
 
 
 
 
to save memory.  Such an algorithm can be realized with methods of arti- 

3 50 

ﬁcial intelligence. 

4 .  Robustness & Resistance 

Robustness  &  Resistance  is  the  comprehensive  key  action  that  is  always 

active.  In  general,  this  key  action  includes  the  ability  to  resist  the  neg- 

ative  consequences  of  a  known  disturbance.  When  a  disturbance  occurs, 

3 55 

a  loss  of  functionality  must  be  expected.  This  loss  of  functionality  as  a 

negative  consequence  of  a disturbance  must  be counteracted  by  methods 

of permanent resilience.  This means, for example, the prevention of data 

loss or the forwarding of faulty data. 

5 .  Recovery 

3 

60 

During  the  recovery  process,  the  disturbance  that  has  occurred  must  be 

remedied  as  far  as  possible.  It  should  be  noted  that  the  correction  of  a 

disturbance is not a process that necessarily only has to take place during 

recovery.  Rather,  the  correction  of  a  disturbance  and  the  elimination  of 

the  negative  eﬀects  of  the  disturbance  is  a  continuous  process  that  can 

3 

65 

also be started during the execution of resistance methods. 

However, the process of eliminating a disturbance must be deﬁnitely com- 

pleted  with  the  completion  of  the  recovery.  Any  lost  functionality  will, 

if  possible,  be  reintegrated  into  the  system  according  to  the  possibilities 

and  mechanisms  used  for  recovery.  Irreparably  damaged  hardware,  for 

3 

70 

example, can of course no longer be used, but a message could be sent to 

a  maintenance  team.  This  maintenance  team  can  replace  the  hardware. 

It should  be possible to replace hardware while the system is running. 

6 .  Adaptation 

According  to  Figure  5,  Rosati  [7]  indicates  that  during  the  adaptation 

3 75 

phase  the system is still in  a state in which  full functionality has not yet 

been restored.  We  believe that for eﬀective adaptation, the system should 

be in a state in which it is fully functional. 

Adaptation essentially means that the system puts itself by self-modiﬁcation 

2 0 

  
 
 
 
 
in a new state, in that it can react more eﬃciently to this type of distur- 

3 80 

bance in the future.  This means less loss of functionality and/or a shorter 

recovery time. 

However, the process of adaptation is by no means trivial.  The system has 

to  take  into  account  previous  adaptations  to  other  disturbances.  These 

must  not  be  signiﬁcantly  negatively  aﬀected  by  the  new  phase  of  adap- 

3 85 

tation.  Furthermore,  the  adaptation  of  the  system  should  be  checked 

beforehand.  This  check  can  be  made,  for  example,  in  a  backup  of  the 

system. 

According  to  this  modiﬁcation  of  the  model  of  key  actions,  the  schematic  rep- 

resentation  of  the  learning  eﬀect  must  be  adapted.  Figure  7  shows  the  new 

schematic representation. 

3 90 

3 95 

Figure  7:  Schematic  representation  of  the  learning  eﬀect  with  the  added  key  action  error 

analysis.  The  two  key  actions  of  resistance  are  to  be  regarded  as  decoupled  and  cover  the 

other 4 key  actions  of anticipation, error analysis, recovery  and  adaptation  completely. 

Based  on  the  key  actions  and  their  relationships  to  each  other,  a  resilient 

system  can  be  designed.  For  this  purpose,  in  addition  to  the  consideration  of 

the  key  actions,  information  from  the  following  areas,  among  others,  must  be 

taken into account: 

  Mechanisms for error detection 
•

2 1 

  
 
 
Mechanisms for attack detection (e.g.  IDS  4) 

Mechanisms for error correction 

Mechanisms for fault localization 

•

•

•

4 

00 

Types of errors 

•
  Mechanisms for pattern recognition 
•

These mechanisms are to be applied according to the requirements of the system 

and  the  available  options.  Particular  attention  is  required  in  the  area  of  error 

detection.  As  a  rule,  errors  are  only  detected  after  they  have  caused  (nega- 

tive) consequences or accidents.  Thus it is obvious that the cycle of key actions 

05  consists of partially interlocking key actions that are directly or indirectly inter- 
4 

dependent.  For  example,  at  the  moment  when  an  error  has  a  negative  impact 

on the system,  the system must be able to resist this impact. 

Thus, in order to achieve resilience Cyber-Physical System of Systems5 needs 

to  be  capable  to  handle  diﬀerent  adverse  conditions,  of  which  some  might  be 

10  expected while  others are not. 
4 

CPS(oS)  must  be  robust  to  speciﬁed  disturbances.  This  includes  speciﬁed 

manipulation  and  attacks.  Speciﬁed  disturbances  are  disturbances  that  could 

be  expected  during  the  development  phase.  This  robustness  can  be  realized 

e.g.  with  special  materials,  which  guarantee  the  robustness  of  the  design  in 

4 

15 

the speciﬁed working area (the expected working conditions).  These can be, for 

example, wires made of metal that have a higher melting point and can therefore 

meet the set requirements. 

In  addition,  this  robustness  can  be  achieved  by  using  redundancy.  Redun- 

dancy is used according to the speciﬁed working conditions to ensure fault tol- 

20  erance. 
4 

CPS(oS)  must  also  be  resistant  to  unspeciﬁed  disturbances.  Unspeciﬁed 

disturbances are disturbances that had to be expected but for which no reaction 

scenario was deﬁned by the system.  CPS(oS) must be able to resist unspeciﬁed 

4 
Intrusion  Detection System 
5 
CPS(oS) 

2 2 

  
 
 
 
 
 
 
disturbances  (at  least  partially).  It  is  essential  that  unspeciﬁed  disturbances 

25  can be detected, minimized, predicted or even avoided at an early stage if they 
4 

occur  repeatedly.  Unspeciﬁed  disturbances  can  be  triggered  by  the  following 

causes, for example: 

Excessive  deviation  of  the  physical  parameters  of  the  environment  from 

•

the speciﬁed working range 

  Too  frequent  (even  not  strong)  deviations  of  the  physical  parameters  of 
•
the environment from the speciﬁed working range 

Too short reaction time 

(dynamic)  changes  in  the  speciﬁed  working  or  analysis  range  (e.g. 

in- 

•

•

creased operating voltage) 

  Individualized work or analysis areas 
•

In short, we  deﬁne resilience for CPS(oS) as follows: 

4 

30 

4 

35 

A CPS(oS) is resilient if it has the ability to react to speciﬁed and unspec- 

iﬁed disturbances in a way that preserves its function and reacts quickly. 

This reaction  also includes  the early  detection,  minimization, prediction 

or  even  avoidance  of  disturbances. 

According to our deﬁnition we discuss in the following four diﬀerent types of 

systems i.e.  primitive  system without error  handling (1),  fault-tolerant system 

(2), resilient system (3) and total-resilient system (4) shown in Figure 8. 

4 40 

A system as the one shown in Figure 8.1 without any form of error detection 

should not be in use  today,  especially not in  the  area of critical infrastructure. 

This system is not able to deal with a fault, understand it or resist its negative 

eﬀects.  Figure  8.2  shows  a  fault-tolerant  system.  These  systems  are  currently 

the  ones  most  commonly  used.  If  this  system  detects  an  error,  this  error  can 

4 45  often be corrected.  If the error cannot be corrected, the user is informed.  If an 

error is not detected, e.g.  because it has no direct eﬀect on the functionality of 

the system, it is a corrupt system that can, among other things, supply incorrect 

data.  Figure 8.3 shows our idea of a resilient system. 

2 3 

  
 
 
 
 
 
 
 
Figure  8:  Schematic  representation  of  four  diﬀerent  systems:  (1)  primitive  system  without 

error handling, (2) fault-tolerant  system,  (3) resilient system,  (4) total-resilient  system. 

This  idea  was  developed  using  our  model  of  key  actions.  The  probability 

4 

50 

that  an  error  was  not  detected  (corrupt  system)  or  that  an  error  was  detected 

but  cannot  be  handled  (inform  users)  is  given  but  should  be very  low.  Figure 

8 .4  shows  a  very  extreme  case  of  a  resilient  system:  an  absolute,  i.e.  totally 

resilient  system.  In  a  totally  resilient  system,  every  error  is  detected  without 

exception and these errors can be handled.  The system will also be able to make 

4 

55 

intelligent adjustments to an error, just like in a resilient system, so that it can 

react better to the error in the future. 

In  the  current  development  process  for  highly  reliable  systems  in  critical 

infrastructure,  it  makes  sense  to  develop  increasingly  resilient  systems  and  to 

make  fault-tolerant  systems  resilient.  However,  resilience  is  an  extremely  dy- 

60  namic  concept,  as  the  many  diﬀerent  deﬁnitions  show.  Ultimately,  however, 
4 

2 4 

  
 
 
resilience is not only the ability to deal with errors, but also the realization that 

errors will deﬁnitely happen.  It is irrelevant whether these errors are symptoms 

of  a  malicious  attack,  environmental  inﬂuences  or  wear  and  tear.  In  order  to 

eﬀectively  implement  resilience,  a  holistic  understanding  of  the  system  design 

65  and the threat situation is necessary.  Also, such a system architecture must oﬀer 
4 

possibilities for expansion (during normal operation), since resilience is subject 

to constant change due to its dynamics. 

5 .  Conclusion  and  Future  Work 

Cyber-Resilience  encompasses  more  than  security  and  reliability.  Cyber- 

70  Resilience also deals with the ability of a system to make autonomous decisions 
4 

according  to  the  situation  in  which  the  system  is  located.  These  autonomous 

decisions  can  be  modeled  using  artiﬁcial  intelligence  methods.  However,  these 

methods of artiﬁcial intelligence must also satisfy the conditions of security and 

reliability. 

4 

75 

Currently Netﬂix is the most known example of a resilient system.  Netﬂix’s 

infrastructure  and  applications  use  a  high  degree  of  redundancy  to  implement 

the  idea  of  Cyber-Resilience  in  the  best  possible  way.  In  addition  to  redun- 

dancy,  Netﬂix  uses  other  mechanisms  to  ensure  that  high  availability  is  main- 

tained.  These include extensive empirical checks of the resilience (the checks are 

80  performed live and during normal operation), rapid isolation of errors, the abil- 
4 

ity to quickly perform fallback, rollback and failover, and constant logging and 

monitoring  of  all  activities in the system.  All these mechanisms enable Netﬂix 

to guarantee almost continuous availability to users and to react to unexpected 

(negative) events [1]. 

4 

85 

Redundancy  is  a  very  powerful  tool  to  achieve  Cyber-Resilience,  but  it  is 

not  universally  applicable.  The  level  of  redundancy  used  by  Netﬂix  to  ensure 

Cyber-Resilience  is  unthinkable  for  embedded  systems,  for  example.  Redun- 

dancy  requires  a  lot  of  physical  space,  especially  in  the  area  of  hardware,  but 

embedded  systems  cannot  be  extended  at  will.  This  means  that  one  has  to 

2 5 

  
 
 
 
 
90  work  with  the  given  form  factor  and  cost  limitations.  At  the  same  time,  how- 
4 

ever, it must also be determined what Cyber-Resilience means for an embedded 

system.  The  objectives  are  the  same  as  those  of  Netﬂix,  but  the  requirements 

are  limited.  The  embedded  system  must  also  be  capable  of  self-observation 

(logging  and  monitoring)  and  it  must  be able  to  predict  and  detect  (negative) 

95  changes/events  in  order  to  react.  The  ﬁrst  big  challenge  is  the  (correct  and 
4 

complete) detection of a negative change, because if a  system is not able to  do 

so, it cannot take appropriate countermeasures. 

If  negative  events  were  detected  correctly,  the  system  must  react  to  these 

events.  The way the system reacts to such an event is another major challenge. 

5 

00 

It  is  not  suﬃcient  to  say  that  a  system  must  always  react  to  error  X  with 

countermeasure Y. Embedded systems are highly complex and components are 

sometimes  highly  interdependent.  The  system  must  react  in  a  way  that  the 

negative  event  can  be countered  and  at  the  same  time  own  functionalities  are 

not or only temporarily impaired.  Also (sensor) data must not be corrupted or 

5 

05 

lost.  In  addition,  various  negative  inﬂuences  can  occur  almost  simultaneously. 

The diﬀerent countermeasures must not hinder or even prevent each other under 

any circumstances. 

The  third  major  challenge  is  the  optimal  selection  of  mechanisms  that  can 

be  used  preventively  against  negative  events.  Redundancy  would  be  one  such 

10  mechanism, but it is only of limited use in embedded systems.  A further mech- 
5 

anism  would  be  the  isolation  of  diﬀerent  components  of  an  embedded  system, 

so  that  negative  events  from  which  errors/disruptions  arise  are  limited  to  one 

component.  In  this  way,  cascading  to  other  components  can  be prevented.  Of 

course  a  complete  isolation  is  not  possible,  but  there  are  mechanisms  that  are 

5 

15 

summarized  under  the  term  loose coupling  that  help  to  keep  the  interrelation- 

ships as small as possible. 

So there is a very wide range of diﬀerent methods that can be used to achieve 

Cyber-Resilience.  Especially for embedded systems it has to be planned exactly 

which  methods  should  be  used.  The  reasons  for  this  are,  among  others,  the 

5 

20 

limited  space,  the  limited  storage  and  computing  capacity  (also  with  regard 

2 6 

  
 
 
 
 
to  the  use  of  software  solutions  for  Cyber-Resilience)  as  well  as  the  available 

ﬁnancial  means.  In  addition,  the  location  and  the  degree  of  criticality  of  the 

system (critical infrastructure) play an important role. 

The  great  challenge  of  Cyber-Resilience  is  the  planning  and  development 

25  of  systems  that  fulﬁll  selected  aspects  of  security  and  reliability.  In  addition, 
5 

a  system  must  be  able  to  make  intelligent  decisions  in  order  to  defend  itself 

eﬃciently against negative eﬀects. 

Future  Work 

One  approach  would  be to  develop  a  design  kit  for  cyber-resilient  systems. 

30  To  develop such a design kit it is ﬁrst necessary to collect and classify diﬀerent 
5 

methods for achieving Cyber-Resilience.  A classiﬁcation only makes sense if the 

methods  have  been  proven  to  have  a  positive  eﬀect  on  a  system.  Theoretical 

methods must ﬁrst be tested. 

The  classiﬁcation  of  the  methods  could  then  look  as  follows:  eﬀort  of  im- 

35  plementation,  type  and  extent  of  eﬀect  on  the  system,  possibilities  of  imple- 
5 

mentation,  etc.  The  next  step  is  to  examine  what  dependencies  exist  between 

these  methods.  This  refers  to  how  these  methods  inﬂuence  each  other  within 

a  system.  This  allows  negative  dependencies  to  be  taken  into  account.  This 

analysis of the dependencies can be implemented e.g.  with the help of artiﬁcial 

5 

40 

intelligence. 

The idea would be a semi-automatic design kit that combines diﬀerent meth- 

ods  for  Cyber-Resilience  and  can  select  and  combine  them  with  the  help  of 

artiﬁcial  intelligence  in  a  way  that  is  suitable  for  the  system.  The  result  is  a 

theoretical  model  that  can  be  used  for  the  practical  development  of  a  cyber- 

5 

45 

resilient system. 

2 7 

  
 
 
 
 
References 

[ 1]  A. Tseitlin, Resiliency through failure:  Netﬂix’s approach to extreme avail- 

ability in the cloud (2013). 

[ 2]  Web  of science. 

5 50 

URL 

https://clarivate.com/webofsciencegroup/solutions/ 

web-of-science/ 

[ 3]  Alsberg,  Peter  A.  and  Day,  John  D.,  A  principle  for  resilient  sharing  of 

distributed  resources,  Proceedings  of  the  2nd  International  Conference  on 

Software Engineering (ICSE ’76) (1976) 562–570. 

5 55 

URL 

http://pages.cs.wisc.edu/~remzi/Classes/739/Spring2003/ 

Papers/p562-alsberg.pdf 

[ 4]  J.  F.  Meyer,  Deﬁning  and  evaluating  resilience:  a  performability  perspec- 

tive,  International  workshop  on  performability  modeling  of  computer  and 

communication systems (PMCCS) 2009. 

5 60 

[5]  A.  Avizienis,  J.-C.  Laprie,  B.  Randell,  C.  Landwehr,  Basic  concepts  and 

taxonomy of dependable and secure computing, IEEE Transactions on De- 

pendable  and Secure  Computing 1 (1)  (2004) 11–33.  doi:10.1109/TDSC. 

2 004.2. 

[ 6]  T. Anderson (Ed.), Resilient Computing Systems, Wiley, New York,  1985. 

5 65 

[7]  J. D. Rosati, K. F. Touzinsky, W. J. Lillycrop, Quantifying coastal system 

resilience  for  the  us  army  corps  of  engineers,  Environment  Systems  and 

Decisions 35 (2) (2015) 196–208.  doi:10.1007/s10669-015-9548-3. 

[ 8]  Ron  Ross,  Richard  Graubart,  Deborah  Bodeau,  Rosalie  McQuaid,  Draft 

sp 800-160 vol.  2, systems security engineering:  Cyber resiliency consider- 

5 70 

ations for the engineering of trustworthy  secure systems (2018). 

[ 9]  M.  T.  Nygard,  Release  it!  Design  and  deploy  production-ready  software, 

second edition Edition, The pragmatic programmers, Pragmatic Bookshelf, 

2 8 

  
 
 
 
 
Place of publication not identiﬁed, 2018. 

URL http://proquest.tech.safaribooksonline.de/9781680504552 

5 

75 

[10]  A. Farraj, E. Hammad, D. Kundur, A cyber-physical control framework for 

transient stability in smart grids, IEEE Transactions  on Smart Grid  9 (2) 

(2018) 1205–1215.  doi:10.1109/TSG.2016.2581588. 

[ 11]  Jean-Claude  Laprie,  From  dependability  to  resilience,  DSN,  Anchorage, 

AK, USA 8. 

5 

80 

[12]  G. Hamel, L. V¨alikangas, The quest for resilience, Harvard Business Review 

9 

(Sept. 2003) 52–63, 131. 

[ 13]  J.N. Gray, Why do computers stop and what can be done about it?, Proc. 

5 
th  Symp.  on  Reliability  in Distributed  Software  Los  Angeles  (Jan.  1986) 

pp. 3–12. 

5 

85 

[14]  E. Hollnagel, Prologue:  The scope of resilience engineering, Resilience En- 

gineering in Practice:  A Guidebook E. Hollnagel, J. Pari`es,  D.Woods, and 

J. Wreathall, Eds., Aldershot (U.K.:  Ashgate) (2011) xxix–xxxix. 

[ 15]  E.  Hollnagel,  Safety-II  in  practice:  Developing  the  resilience  potentials, 

Routledge, London and New York,  2018. 

5 

90 

[16]  E.  Hollnagel,  J.  Leonhardt,  L.  Macchi,  B.  Kirwan,  White  paper  on  re- 

silience engineering (eurocontrol).  doi:10.13140/RG.2.1.3676.7206. 

URL  https://www.eurocontrol.int/sites/default/files/2019-07/ 

white-paper-resilience-2009.pdf 

[ 17]  V. de Florio, On the constituent attributes of software  and organizational 

5 

95 

resilience,  Interdisciplinary  Science  Reviews  38  (2)  (2013)  122–148.  doi: 

1 0.1179/0308018813Z.00000000040. 

[ 18]  V.  de  Florio,  Robust-and-evolvable  resilient  software  systems, 

in: 

J.  C´amara,  R.  de  Lemos,  C.  Ghezzi,  A.  Lopes  (Eds.),  ASAS  ’11,  As- 

sociation  for  Computing  Machinery,  New  York,  NY,  2011,  p.  10.  doi: 

6 

00 

10.1145/2024436.2024440. 

2 9 

  
 
 
 
 
[ 19]  M.  Herrera,  E.  Abraham, 

I.  Stoianov,  A  graph-theoretic 

frame- 

work 

for  assessing  the  resilience  of  sectorised  water  distribution 

networks,  Water  Resources  Management  30  (5)  (2016)  1685–1699. 

doi:10.1007/s11269-016-1245-6. 

6 05 

URL 

https://link.springer.com/content/pdf/10.1007% 

2 Fs11269-016-1245-6.pdf 

[ 20]  Bakkensen,  Laura  and  Fox-Lent,  Cate  and Read,  Laura  and  Linkov,  Igor, 

Validating  resilience  and  vulnerability  indices  in  the  context  of  natural 

disasters:  Validating resilience and vulnerability indices, Risk Analysis (37) 

6 10 

(2016) n. s.  doi:10.1111/risa.12677. 

21]  S.  L.  Cutter,  C.  G.  Burton,  C.  T.  Emrich,  Disaster  resilience  indicators 
[ 

for  benchmarking  baseline  conditions,  Journal  of  Homeland  Security  and 

Emergency Management 7 (1) (2010) n. s. doi:10.2202/1547-7355.1732. 

22]  S.  L.  Cutter,  K.  D.  Ash,  C.  T.  Emrich,  The  geographies  of  community 
[ 

6 

15 

disaster  resilience,  Global  Environmental  Change  29  (2014)  65–77.  doi: 

1 0.1016/j.gloenvcha.2014.08.005. 

23]  A. Clark-Ginsberg, What’s the diﬀerence between reliability and resilience? 
[ 

(2016) n. s.doi:10.13140/RG.2.2.20571.46885. 

24]  D.  E.  Alexander,  Resilience  and  disaster  risk  reduction:  an  etymological 
[ 

6 

20 

journey, Natural Hazards and Earth System Sciences 13 (11) (2013) 2707– 

2 716.  doi:10.5194/nhess-13-2707-2013. 

[ 25]  J.  S.  Mayunga,  Understanding  and  applying  the  concept  of  community 

disaster  resilience:  a  capital-based  approach,  ummer  academy  for  social 

vulnerability and resilience building (1, 16) (2007) n. s. 

6 

25 

[26]  S. Mitra, K. Brelsford, P.  N. Sanda, Cross-layer resilience challenges:  Met- 

rics and  optimization, in:  IEEE (Ed.), Design,  Automation  & Test  in  Eu- 

rope Conference & Exhibition (DATE), 2010, IEEE, Piscataway, NJ, 2010, 

pp. 1029–1034.  doi:10.1109/DATE.2010.5456961. 

3 0 

  
 
 
 
 
[ 27]  D.  Bodeau,  R.  Graubart,  Cyber  resiliency  design  principles:  Selective  use 

6 

30 

throughout the lifecycle and in conjunction with related disciplines (2017) 

n. s. 

URL 

https://www.mitre.org/sites/default/files/publications/ 

PR%2017-0103%20Cyber%20Resiliency%20Design%20Principles% 

2 0MTR17001.pdf 

6 

35 

[28]  D. J. Bodeau, R. D. Graubart, Cyber resilience metrics:  Key observations 

(2016) n. s. 

URL 

https://www.mitre.org/sites/default/files/publications/ 

1 6-0779-cyber-resilience-metrics-key-observations.pdf 

29]  S. Hukerikar,  C. Engelmann,  Resilience design patterns - a structured ap- 
[ 

6 

40 

proach to resilience at extreme scale (version 1.0) (08.11.2016) n. s. 

URL https://arxiv.org/pdf/1611.02717.pdf 

30]  Z. Dyka,  E. Vogel,  I. Kabin, M. Aftowicz, D. Klann, P.  Langendörfer, Re- 
[ 

silience more than the sum of security and dependability:  Cognition is what 

makes the diﬀerence, in:  R. Stojanovic (Ed.), 2019 8th Mediterranean Con- 

6 

45 

ference  on  Embedded  Computing  (MECO),  IEEE,  Piscataway,  NJ,  2019, 

pp. 1–3.  doi:10.1109/MECO.2019.8760139. 

31]  E.  Jen,  Stable  or  robust?  what’s  the  diﬀerence?,  Complexity  8  (3)  (2003) 
[ 

1 2–18.  doi:10.1002/cplx.10077. 

[ 
32]  Deborah  Bodeau,  Richard  Graubart,  Jeﬀrey  Picciotto,  Rosalie  McQuaid, 

6 50 

Cyber resiliency engineering framework (2011). 

33]  V.  Castano,  I.  Schagmayaev,  Resilient  computer  system  design,  Springer, 
[ 

Cham, 2015. 

URL  http://search.ebscohost.com/login.aspx?direct=true&scope= 

site&db=nlebk&AN=980100 

6 55 

[34]  I.  Kabin,  Z.  Dyka,  D.  Kreiser,  P.  Langendoerfer,  Horizontal  Address-Bit 

DEMA against ECDSA, IEEE, 9th IFIP International Conference on New 

3 1 

  
 
 
 
 
Technologies,  Mobility  &  Security,  Piscataway,  NJ,  2018.  doi:10.1109/ 

NTMS.2018.8328695. 

35]  I.  Kabin,  Z.  Dyka,  D.  Kreiser,  P.  Langendoerfer,  Horizontal  Address-Bit 
[ 

6 60 

DPA  against Montgomery kP Implementation, IEEE,  ReConFig ’17, New 

York,  2017.  doi:10.1109/RECONFIG.2017.8279800. 

36]  Z.  Dyka,  E.  Vogel,  I.  Kabin,  D.  Klann,  O.  Shamilyan,  P.  Langendörfer, 
[ 

No  resilience  without  security,  in:  2020  9th  Mediterranean  Conference  on 

Embedded Computing (MECO), 2020, pp. 1–5. 

3 2 

  
 
 
