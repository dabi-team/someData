Improving DGA-Based Malicious Domain
Classiﬁers for Malware Defense with Adversarial
Machine Learning

Ibrahim Yilmaz, Ambareen Siraj, Denis Ulybyshev
Department of Computer Science
Tennessee Technological University
Cookeville, USA
yilmaz42, asiraj, dulybyshev@tntech.edu

1
2
0
2

n
a
J

2

]

R
C
.
s
c
[

1
v
1
2
5
0
0
.
1
0
1
2
:
v
i
X
r
a

Abstract—Domain Generation Algorithms (DGAs) are used
by adversaries to establish Command and Control
(C&C)
server communications during cyber attacks. Blacklists of
known/identiﬁed C&C domains are often used as one of the
defense mechanisms. However, since blacklists are static and
generated by signature-based approaches, they can neither keep
up nor detect never-seen-before malicious domain names. Due to
this shortcoming of blacklist domain checking, machine learning
algorithms have been used to address the problem to some extent.
However, when training is performed with limited datasets, the
algorithms are likely to fail in detecting new DGA variants. To
mitigate this weakness, we successfully applied a DGA-based
malicious domain classiﬁer using the Long Short-Term Memory
(LSTM) method with a novel feature engineering technique. Our
model’s performance shows a higher level of accuracy compared
to a previously reported model from prior research. Additionally,
we propose a new method using adversarial machine learning to
generate never-before-seen malware-related domain families that
can be used to illustrate the shortcomings of machine learning
algorithms in this regard. Next, we augment the training dataset
with new samples such that it makes training of the machine
learning models more effective in detecting never-before-seen
malicious domain name variants. Finally, to protect blacklists
of malicious domain names from disclosure and tampering, we
devise secure data containers that store blacklists and guarantee
their protection against adversarial access and modiﬁcations.

Index Terms—Domain Generation Algorithms, Adversarial

Machine Learning, Long Short-Term Memory, Data Privacy

I. INTRODUCTION

Security researchers have developed many different defense
mechanisms in order to protect computer systems against
malware and malicious botnet C&C communications. Black-
lists of malicious websites are one of the most commonly
used defense mechanisms where lists of domain names or
IP addresses that are ﬂagged as harmful are maintained. Any
messages to/from these listed sites are feared to host potential
C&C servers and hence blocked to prevent any further com-
munications. As a counterattack, attackers developed DGAs
as a measure to thwart blacklist detection [1].

In recent years, hacker communities have been utilizing
DGAs as the primary mechanism to produce millions of ma-
licious domain names automatically through pseudo-random
domain names in a very short time period [2]. Subsets of

these malicious domain names are utilized to map to the C&C
servers. These dynamically created domain names successfully
evade static blacklist-checking mechanisms. Additionally, as
one domain gets recognized and blocked, the C&C server can
easily switch to another one.

To overcome the limitations of static domain blacklists,
machine learning (ML) techniques have been developed to
detect malicious domain names and these techniques have
yielded mostly promising results [3], [4], [5]. However, the
ML models do not perform well with never-seen-before DGA
families when an unrepresentative or imbalanced training
dataset is used. To address this problem, we propose a novel
approach to generate a rich set of training data representing
malicious domain names using a data augmentation technique.
Data augmentation of an existing training dataset is one
way to make ML models learn better and, as a result, perform
more robustly. Nevertheless, classic data augmentation merely
creates a restricted reasonable alternative. In our approach,
as illustrated in Figure 1, an adversarial machine learning
technique is used to generate a diverse set of augmented
data by means of data perturbation. The generated adversarial
domain names are extremely difﬁcult to differentiate from
benign domain names. As a result,
the machine learning
classiﬁer misclassiﬁes malicious domains as benign ones.
Afterwards, these adversarial examples are correctly re-labeled
as malicious and reintroduced into the existing training dataset
(see Figure 1). In this way, we augment the blacklist with
diverse data to effectively train the machine learning models
and increase the robustness of the DGA classiﬁers.

In addition, we devise a secure container to store and trans-
fer the blacklists of malicious domain names in encrypted form
as a Protected Spreadsheet Container with Data (PROSPECD),
presented in [6]. PROSPECD provides conﬁdentiality and
integrity of the blacklists so that they can be used as training
data to build a secure model. In addition to data integrity,
PROSPECD provides origin integrity. This container protects
the adversarial samples, used to teach the model, from un-
known adversarial perturbations. The protected blacklist can
be marketed commercially [7] to internet service providers and
companies who need to maintain their own internal blacklists.

 
 
 
 
 
 
Fig. 1: Overview of the Proposed Methodology.

A. Our Contributions

Blacklist is a security strategy that keeps network ﬂow
and computer environments secure [8]. Typical network trafﬁc
blacklists include malicious IP addresses or domain names,
which are blocked from communication attempts in both
directions. However, the coverage of blacklists is insufﬁcient
and unreliable because adversarial hacker communities can
compromise a system by generating malware domains dynami-
cally using DGAs that easily bypass the static blacklist. Kuhrer
et al. [9] evaluated 15 public malware blacklists as well as
4 blacklists, served by antivirus vendors. Their ﬁndings show
that blacklists fail to protect systems against prevalent malware
sites generated by DGAs.

In order to address the shortcomings of the above problem,
researchers have mostly proposed solutions based on reverse
engineering techniques to identify and block bot malware
[10]. However, such solutions are not always feasible due to
the obfuscation of underlying algorithms, as hackers adapt
their algorithms swiftly to exploit the vulnerabilities in the
system. Other alternative solutions require auxiliary contextual
information. One of these alternative solutions focuses on the
network trafﬁc analysis [11], [12] or broad network packet
examination [13], [14]. However, these techniques may not be
able to keep up with large-scale network trafﬁc. Therefore,
there is a need for a sophisticated network trafﬁc analysis tool
for effective blacklisting.

In response to the above issues, detection of malicious
domains has increasingly evolved towards usage of machine
learning techniques. Performance of the solutions proposed to
automatically detect malicious domains mostly suffers from
never-seen-before malicious domains. This is due to the lack
of generalization when a model is not trained effectively with
representative or balanced training dataset. For this reason,
blacklists must be constantly updated in order to identify and
prevent DGA generated malicious domain connections.

Data augmentation is an approach where more data is
created from existing data in a way that can enhance the
usefulness of the application. Anderson et al. [15] demon-
strated how data can be augmented more effectively by using
an adversarial machine learning technique. The researchers
enhanced adversarial domain names using Generative Adver-
sarial Network (GAN) methodology. In their approach, two
neural networks are trained simultaneously, and later trained
with a dataset which includes adversarial samples to harden the
DGA classiﬁer. However, the main drawback of this approach

is unpredictability of desirable results because of difﬁculties
of controlling both classiﬁers at the same time, even though
a good optimization algorithm is used. As a result, it fails
to always converge to a point of equilibrium to generate
new domain names. Additionally, controlling the diversity
of produced samples is challenging with GAN models [16],
[17]. In such cases, the newly generated data do not add
to the diversity of the current data. Hence,
this solution
alone cannot increase the malicious detection capabilities of
blacklists against never-before-seen DGA families.

To improve the accuracy of such detection mechanisms, we
propose a new technique based on data perturbation without
relying on a fresh public blacklist or external reputation
database. In our approach, we observe how the model works
and use the knowledge to mislead the DGA classiﬁer. To do
this, a noise, carefully calculated from the observation is added
to the DGA based malicious domains to appear non-malicious.
These adversarial samples are then predicted as benign by the
machine learning (ML) model. Such adversarial attack can be
addressed with an adversarial training [18]. Therefore, after
correctly labeling these seemingly benign adversarial samples,
the ML model is trained with the augmented dataset. The
experimental results demonstrate that the retrained ML model
is able to detect never-before-seen DGA malwares better than
any other similar approaches.

Our work has the following contributions:
• Using a machine learning technique based on the Long
Short-Term Memory (LSTM) model for automatic detec-
tion of malicious domains using a DGA classiﬁer that
analyses a massive labeled DGA dataset, character by
character over.

• To the best of our knowledge it is the ﬁrst study to propose
the generation of malicious domain names using a data
perturbation approach in order to expand the training
dataset with adversarial samples.

• Demonstrating that, as expected, the LSTM model fails
to recognize newly introduced adversarial samples in
augmented training dataset.

• Applying an adversarial training to train the model with
correct labelling of the adversarial samples in the training
dataset to increase the model’s generalization ability.
• Demonstrating that the augmented training dataset can
help the LSTM model to detect not only never-seen-
before DGAs, but also novel DGA families.

The rest of this paper is organized as follows: the literature

review in the context of our work is discussed in section II.
The necessary background for DGA-based malicious domain
models is reviewed in Section III. The core design of our
system, including the adversarial machine learning models
to generate malicious domain names and data containers to
store them, are presented in Section IV. We discuss the
implementation of the adversarial machine learning models in
Section V. The evaluation results of our study are presented
in Section VI. Section VII concludes the paper.

II. RELATED WORK

Domain Generation Algorithms and detection of malicious
domain names have been previously analyzed by different
researchers for a number of years. Daniel et al. [19] presented
a taxonomy of DGA types by analyzing characteristics of
43 different DGA-based malware families and compared the
properties of these families. They also implemented previous
studies with 18 million DGA domain data that was created to
identify malicious domains. They reported further progress in
DGA detection.

Detection of DGA botnets became feasible with the im-
plementation of powerful machine learning models. Lison
et al. [20] implemented a recurrent neural network model
for the detection of DGAs. Their empirical study detected
malicious domain names with high accuracy. Justin et al. [21]
deﬁned several models to detect malicious web sites, including
a logistic regression, support vector machine, and bayesian
model. They used the DMOZ dataset for benign websites
while PhishTank and Spamscatter were used for malicious
websites. Bin et al. [22] addressed the same issue using dif-
ferent machine learning classiﬁers for the detection of DGAs.
They created a convolutional neural network (CNN) and a
recurrent neural network (RNN) machine learning model for
the classiﬁcation of malicious and benign domain names. They
compared the results of both models in terms of their perfor-
mance and reported that both models performed comparably.
Duc et al. [23] dealt with the multiclass imbalance problem
of LSTM algorithms for the detection of malicious domain
names by generating DGAs. The authors claimed that the
LSTM algorithms performed poorly with imbalanced datasets.
To tackle this imbalanced dataset problem, they proposed a
Long Short-Term Memory Multiclass Imbalance (LSTM.MI)
algorithm and showed that their proposed algorithm provided
more accurate results by implementing different case studies.
In addition, Mayana et al. [24] introduced a WordGraph
method to recognize dictionary based malicious domains. The
authors asserted that more sophisticated DGAs are able to
avoid detection of conventional machine learning classiﬁers.
They carried out their experiments by extracting dictionary
information without using reverse engineering. Bin et al. [25]
deﬁned a deep neural network model as an inline DGA
detector. They caution about most of the available datasets not
being good representations for malicious domains or outdated.
Hence, machine learning models’ perform poorly when trained
using such datasets. Furthermore, they explained that reverse
engineering was a difﬁcult method for training models. To

tackle these problems, the researchers offered a novel detector
for malicious domains without the need for reverse engineer-
ing. Their proposed technique was based on real trafﬁc and
reported to detect malicious domains in real time. Woodbridge
et al. [3] created a machine learning classiﬁer based on the
LSTM network to detect malicious domain names in real time.
Their classiﬁer detected multiclass domains by categorizing
them into particular malware families. The model predicted a
domain name as malicious or benign according to this domain
name without any additional information.

However, although these studies achieved high detection
rates for particular DGA families, the performance of machine
learning based detection systems are poor with new DGA
variants when the models are trained with unrepresentative
or imbalanced training datasets. To handle this issue, Ander-
son et al. [15] offered a GAN algorithm to generate new
domain names. In their GAN approach, they implemented
two different deep neural network classiﬁers named discrim-
inator and generator. According to this GAN methodology,
new malicious domain names are generated by the generator,
which evades the discriminator’s detection. Their case studies
demonstrated that new malicious domain names also bypass
a random forest classiﬁer. Once the model was trained with
adversarial samples, it hardened the model against new DGA
families. However, the authors did not test it on DGA families
created using a dictionary. Additionally, implementation of
this approach is challenging due to the need of controlling
two machine learning models, which might be unsuitable
for detecting malware-related domain names. Unlike this ap-
proach, we propose to augment data by using an efﬁcient
data perturbation technique that generates hard-to-detect DGA
families and identiﬁes DGA types that are created either
randomly or using a dictionary.

To overcome limitations of machine learning models in
aforementioned circumstances, Curtin et al. [26] proposed
to combine a neural network model with domain registra-
tion supplementary information. This additional information,
which is known as WHOIS data, helped the neural network
model to efﬁciently identify the most difﬁcult samples that
were generated using English words. However, cybercriminals
take advantage of bulk registration services by registering
thousands of domain names in a short time, several months
before the start of nefarious activities [27]. In addition, unau-
thorized people can access this information and falsify it by
impersonating as legitimate users. This makes the information
questionable. Compared to this, our approach efﬁciently de-
tects DGA families solely based on the domain names, without
relying on any supplementary information.

PROSPECD container used to store and transfer blacklisted
malicious domain names, is presented in [6]. Compared to the
privacy-preserving data dissemination concept as proposed by
Lilien and Bhargava [28], it has the following features:

• Detection of several types of data leakages that can be

made by authorized entities to unauthorized ones;

• Enforcement of access control policies either on a central

server or locally on a client’s side in a Microsoft Excel®1
Add-in or in a cross-platform application [6].

• Container implementation as a digitally signed water-
marked Microsoft Excel®-compatible spreadsheet ﬁle
with hidden and encrypted data and access control poli-
cies worksheets.

• On-the-ﬂy key derivation mechanism for data worksheets.

The primary difference between PROSPECD and an Active
Bundle [30], [31], [32] is that PROSPECD does not store an
embedded policy enforcement engine (Virtual Machine).

In contrast with a solution to encrypt the desired cells in
a spreadsheet ﬁle, proposed by Tun and Mya in [33],
in
PROSPECD all the data worksheets are encrypted with the
separate keys, generated on-the-ﬂy. PROSPECD supports role-
based and attribute-based access control. Furthermore, digital
and visual watermarks are embedded in PROSPECD to enable
detection of data leakages. A Secure Data Container, pro-
posed in [34] to store device state information, only supports
centralized policy enforcement. PROSPECD supports both
centralized and local policy enforcement mechanisms [6].

III. BACKGROUND

In this section, we review background information related

to our research.

1) Domain Generation Algorithm (DGA): Domain gen-
eration algorithms are primary means to connect various
families of malware with new or never-before-seen domains
to avoid detection. There are many such DGA-based malware
families (malware that connect to DGA generated domain
names). According to a study, the ﬁve most known families
are Conﬁcker, Murofet, BankPatch, Bonnana, and Bobax [35].
Although many DGA-based domain names are produced ran-
domly, some are generated using a dictionary. The detection
of these types of domain names is more difﬁcult because of
their similarity to legitimate domains.

2) Gradient Descent Algorithm: The gradient descent al-
gorithm is the most popular optimization method for using
machine learning classiﬁers to minimize errors. It takes into
account the ﬁrst derivative when modifying all parameters
under considerations [36]. Gradient descent always strives to
ﬁnd the most appropriate way to minimize errors. The learning
process starts with randomly producing weight values. Most of
the time, these values are set to an initial value of zero and are
used to calculate the lost function value. It then uses the gradi-
ent descent algorithm to ﬁnd a way to reduce the lost function.
All weights are updated through the backpropagation process
based on the gradient descent algorithm. We generate new
adversarial domain names in our data augmentation method
by utilizing and modifying the gradient descent algorithm’s
behavior. In Section V, explanation of how such adversarial
samples are created is discussed in detail.

1This paper is an independent publication and is neither afﬁliated with, nor

authorized, sponsored, or approved by, Microsoft Corporation [29]

3) Long Short-Term Memory (LSTM) Model: The LSTM
model, a specialized Recurrent Neural Network (RNN), is
used in our approach for automatic detection of malicious
domains. RNNs are known as supervised machine learning
models that are commonly used to handle the processing of
sequential data [37]. An RNN takes the previous and current
inputs into account while the traditional networks consider all
inputs independently. In our study, we implement the model
on a character by character basis, so that
it captures the
importance of the order of the characters’ occurrence in the
word. Essentially, the model learns the occurrences of the
characters in a sequential way. For example, for the domain
name google, without the top-level domain, the model ﬁrst
learns the character g, then o, predicting that it succeeds g.
However, traditional neural networks do not take the position
the traditional
of the characters into account. In addition,
neural networks process ﬁxed-sized input. Similarly, they can
create ﬁxed-size output, whereas RNN does not have such
limitations.

When input data like domain names have long term se-
quences, traditional RNN struggles to learn such long data
dependencies, which is known as the vanishing gradient prob-
lem [38]. In order to avoid this problem, LSTM, which is a
special kind of RNN, was introduced in [37].

LSTM relies on the gating mechanisms, where the in-
formation can be read, written or erased via a set of
programmable gates. This allows recurrent nets to keep
track of information over many time-steps and gain the
ability to preserve long term dependencies. For example,
47faeb4f1b75a48499ba14e9b1cd895a is a malicious domain
name, which has a 32 character length. LSTM keeps track of
the relevant information about these characters throughout the
process. Furthermore, a study has shown LSTMs outperform
previous RNNs for the solution of both context-free language
(CFL) and regular language problems [39]. Researchers have
reported that LSTMs generalize better and faster, leading to
the creation of more effective models. In our study, the LSTM
model learns how to detect DGA-based malicious domains in
a similar way to what is mentioned above.

IV. CORE DESIGN

A. Generating New Malicious Domain Names

Machine learning models’ performances substantially rely
on the training dataset crucial for building effective classi-
ﬁers. However, one of the biggest challenges with any ML
model is accumulating a training dataset that is representative
and balanced enough to enable the creation of an effective
machine learning model. This process might be costly or
time-consuming or both. A restrictive training dataset can
lead to poor performance of the ML model and that
is
the primary reason DGA classiﬁers do not work well with
automated malicious domain name detection. With traditional
blacklists used in training, ML classiﬁers cannot detect never-
before-seen DGA families. The model needs to be readjusted
constantly with new variations of training data for effective
threat detection. To address this issue, we propose to create a

blacklist with domain names using a novel adversarial machine
learning technique.

Our adversarial approach is based on data perturbation tech-
niques inspired by [18], where domain names are inﬂuenced
based on the gradient descent algorithm of a targeted DGA
classiﬁer with regards to the classiﬁer loss. Changing the
gradient of the classiﬁer maximizes loss function instead of
minimizing it can mislead the malware detection classiﬁers.
Even though these domains are malicious, the DGA classiﬁer,
based on the LSTM model, predicts them as benign. As a
result, new adversarial domain are generated that can seem
benign by not matching the blacklist data. Our method is
clariﬁed mathematically below [18].

Let x be a given malicious domain name, y be labeled as
malicious, M be a DGA classiﬁer that M(x): x → y, ˆx is
crafted domain name using our adversarial attack and ˆy is the
class label that M(x): ˆx → ˆy

objective max l(M, ˆx, y)

y (cid:54)= ˆy

subject to

ˆx = x + δx

(1)

(2)

(3)

Here l(M.x,y) is the loss function of the DGA classiﬁer in
(1). A newly created adversarial domain name is predicted
as benign by the DGA classiﬁer in (2). δx in (3) represents
perturbation added to a vector form of the given domain name.
δx is calculated below [18]:

Fig. 2: Generation of Adversarial Samples Using Character-
Level Transformations.

ALGORITHM 1: Pseudocode of our proposed ad-
versarial domain name generation approach

1 Input:
2 - Train data pair {Xi, Yi} where Xi = Each

domain name and Yi = Corresponding
ground-truth label

3 - X =(x1 || x2 ||...|| xn) where xi = Each
character of a given input domain name
4 - Training iteration number Nitr, Number
of adversarial examples Nadv, Number of
training samples Ntrain, Number of
character of a given input domain name
Ntotal

5 - Test data pair {Xj, Yj}
6 Function generate domain names
7
8

for iteration = 0, ..., Nitr do

Update all parameters based on

δx = (cid:15) sign(∇x l(M, x, y))

(4)

gradient descent

Here sign (∇x l(M,x,y) ) represents the direction of the
loss function, which minimizes the loss function of the DGA
classiﬁer, and (cid:15) controls the expansion of the noise in (4).
The smaller epsilon value perturbs the original feature vector
slightly, while the larger one perturbs the original feature
vector signiﬁcantly. This misleads the DGA classiﬁer to a great
extent. On the other hand, a larger perturbation can be more
easily detected than a smaller one by human eyes.

The calculated noise is included in each character embed-
ding of input data. The resulting embedding characters with
noisy calculations are compared to each character’s embedding
using cosine similarity [40] to measure each distance. The ﬁnal
character is chosen based on this operation. The design of the
model is demonstrated in Figure 2. According to the example
in the ﬁgure, character g turns to c with our use of adversarial
learning. Our adversarial domain name generation algorithm
is summarized in Algorithm 1.

The DGA detectors can be seen as black-box devices in real-
world settings. Since, in the black-box scenario, an adversary
does not have any knowledge about the inner workings of
the target model. Nevertheless, for the sake of simplicity,
we implement our proposed technique under the white-box
assumption, where we obtain optimum perturbation by ac-
cessing the target model so that we can compute gradients.
Although the black-box assumptions can be perceived as more
realistic for this work, it is important to keep in mind that

9
10
11

12

13

14
15

end
for iteration = 0, ..., Nadv do

for iteration = 0, ..., Ntotal do

δxi = (cid:15) × sign(∇xi l(M, xi, yi))
# Calculate penetration for each
character

zi= xi + δxi
= zi•x√
ˆx
zi
max

2×xi

2

# New characters are generated
through our proposed method

end
Output: ˆX =(ˆx1 || ˆx2 ||...|| ˆxn)
# New malicious samples are
generated

16
17 end

end

previous studies showed that adversarial samples have the
transferarability property [41]. This means that an adversarial
example generated for one DGA model is more likely to be
misclassiﬁed by another DGA detector as well, since when
different ML models are trained with the similar dataset from
the same source, they learn similar decision boundaries. We
leave testing adversarial examples the under black-box settings

 Transpose	of	thenew	embedding ⋅    ( ( , , ))∇ . 1,1 1,256 256,1 256,256......⋮Embedding	of	character	 CalculatedNoiseNew	Embedding 1,1 2,1 3,1⋮⋮1*256256*256256*11*256Maximum	element	of	the	arraywhich	corresponds	to	the	mostsimilar	character	to		is	selected  + ⋅    ( ( , , ))= ∇ 	EmbeddingMatrix==+for future work.

B. Protected Spreadsheet Container with Data (PROSPECD)
for Domain Names Blacklists

We propose to use a PROSPECD data container, presented
in [6], to securely store and transfer blacklisted malicious
domain names. In our use case, PROSPECD, implemented
as an encrypted and digitally signed spreadsheet ﬁle, contains
the following watermarked data worksheets:

• ”Domain Blacklist” to store encrypted malicious domain

names, detected by our classiﬁer;

• ”Metadata” to store encrypted metadata, which include

access control policies;

• ”General Info” to store encrypted information about the
classiﬁer used to detect the malicious domain names and
its execution details.

PROSPECD provides data conﬁdentiality and integrity,
origin integrity, role-based and attribute-based access control
and centralized and decentralized enforcement of access
control policies. Digital and visual watermarks, embedded
into a PROSPECD spreadsheet ﬁle, enable detection of several
types of data leakages that can be made behind-the-scenes by
authorized parties to unauthorized ones [6].

PROSPECD Generator. The malicious domain names clas-
siﬁer runs on a trusted server. Once the blacklist of domain
names is generated, the dedicated process writes it, as well
as the relevant information, in a spreadsheet ﬁle. Then the
PROSPECD generator, currently implemented as a command
line utility, is called. It takes a spreadsheet ﬁle with the ”Do-
main Blacklist” worksheet and two other worksheets (”General
Info” and ”Metadata”) in a plaintext form, as an input, and
generates a separate spreadsheet ﬁle with encrypted work-
sheets. Each separate worksheet is encrypted with a separate
symmetric 256-bit AES key, generated on-the-ﬂy [6].

PROSPECD Data Access on a Trusted Server. The
PROSPECD container, stored on a trusted server, can be
accessed remotely from a web viewer. The client opens the
Authentication Server (AS)’s URL in a web browser, selects
the data subset to retrieve (”Domain Blacklist”, ”General Info”
or ”All”) and enters their credentials: username (role) and
password. The accessible worksheets from PROSPECD are
decrypted, using the on-the-ﬂy AES key derivation scheme,
based on the client’s role and attributes. These attributes
include versions of a web browser and an operating system,
as well as the type of device the client uses. Decrypted
worksheets are sent to the client as a JSON object via https
communication channel [6]. PROSPECD supports RESTful
API. Table I shows the access control policies. The role
”User” can only access blacklisted domain names from the
”Domain Blacklist” worksheet. The role ”Administrator” is
allowed to access all the worksheets and also to download the
PROSPECD ﬁle from the server to their local device, to access
the data locally or transfer it to other parties.

Local PROSPECD Data Access. Authorized parties can
access PROSPECD data locally, either from the Microsoft

TABLE I: PROSPECD Access Control Policies

Domain Blacklist

General Info Metadata

Administrator
User

YES
YES

YES
NO

YES
NO

Excel® Add-in or from the standalone cross-platform appli-
cation. For the ﬁrst option, a user needs to ”download and
install the Microsoft® Excel Add-in” [6], written in C#®, open
a PROSPECD container and enter valid credentials. Then the
PROSPECD’s digital signature is veriﬁed. If it is valid, the
”Metadata” worksheet is decrypted and access control policies,
stored in this worksheet and shown in Table I, are evaluated.
Then the decryption keys for accessible data worksheets are
derived [6]. To prevent unauthorized data disclosures,
the
authenticated user is not allowed to print or save the opened
spreadsheet ﬁle once the Add-in has been launched. When a
user closes an application, all the data are encrypted back to
their original values and the visibility of all the worksheets is
reset back to VeryHidden®, after which the application closes.
In addition to the Microsoft® Excel Add-in, a cross-platform
application was developed to view PROSPECD data [6]. This
application provides a graphical user interface and does not
allow the user to store the decrypted PROSPECD ﬁles locally,
to prevent possible data leakages.

V. EXPERIMENTAL METHODOLOGY

This section describes the dataset used to build a DGA
classiﬁer based on an LSTM model, along with the explanation
of the model implementation.

A. Dataset

The experimental dataset includes one non-DGA (benign
domain names) and 68 DGA families (malicious domain
names). Data is collected from two different sources, which
are publicly available [42], [19].

For benign domains, we use the Majestic top 1 million
dataset [42]. This dataset includes the top one million web-
site domain names all over the world and the dataset
is
updated daily. For malicious domains, we obtain data from
the DGArchive, which is a web repository of DGA-based
malware families [19]. This repository has over 18 million
DGA domains. We have worked on 68 DGA malware fam-
ilies with some being generated by traditional DGAs. The
remaining families were produced by dictionary DGAs. We
used both traditional-based DGAs and dictionary-based DGAs
with over a hundred thousand malicious domains. To ensure a
fair comparison, we used a subset of 135K samples from the
Majestic top 1 million dataset so that the classiﬁer does not
bias towards the majority class and thus, prevent occurrence
of overﬁtting.

B. ML Model Implementation

We implement our LSTM model in Python using Pytorch
[43]. We use the LSTM at character level with application of
character embeddings (vector forms of the characters). This

means that we every character is encoded to a representative
vector. We convert from the word spaces to vector spaces to
extract better features for the machine learning classiﬁer. Each
ASCII character (total number is 256) represents a vector
whose size is set to 256. In this way, we create a 256 by
256 embedding matrix where each row represents a character
and character embeddings are represented by column vectors
in the embedding matrix. Once the perturbation technique is
employed, embedded vectors of the character of malicious
domains are transformed into word space again.

We divided the dataset into training and test data. We use
90% of the dataset for training and remaining is reserved for
testing. The training is used by the model to learn detection
of DGA-based malicious domains. In our implementation,
only the domain names are considered by the model, and
the characters are pulled from the domain names character by
character. At each time interval, one character’s corresponding
vector is fed into the LSTM model. The character embeddings
are randomly initialized at the beginning. The model is able to
learn through the dependencies that is has with each other and
the conditional probabilities of the aforementioned characters.
Thus, each character’s embedding is learned by the LSTM
model itself and the matrix is ﬁlled with these embeddings. In
the test phase, the unseen data is predicted by the model as
malicious or benign. The model’s performance is analyzed in
section VI in detail.

In this work, our main goal is to augment the training dataset
to increase the model’s resiliency and improve performance
for detection of never-before-seen or yet-to-be-observed DGA
families. To do this, an optimally calculated noise is added
to each character embedding of the input data by the data
perturbation technique. The newly created embedding with the
addition of noise may not be assigned to any character. There-
fore, the model looks for the closest embedding character and
assigns it as the character to that corresponding embedding.
Here we use approximate similarity search [44] by applying
cosine similarity [40] that takes dot products between newly
created embedding and each row of the embedding matrix
to calculate the similarity. The new character is assigned to
the row matrix’s corresponding character, yielding maximum
similarity value.

Technically speaking, the LSTM model consists of two
hidden layers along with the input and output layers. The drop
out, a known regularization technique, is used with a rate of
0.5 in order to avoid overﬁtting. The ﬁne-tuned parameters are
found by using a batch size of 128 and a learning rate of 0.001
along with an epoch number of 6. To achieve learning rates
lower than this, more iterations may be needed.

In addition, we use the adam optimization algorithm an
extension to stochastic gradient descent,
to minimize the
error by adjusting network hyper parameters in an iterative
way. Furthermore, binary cross entropy utilized for binary
classiﬁcation, is used as the loss function in order to measure
the cost over probability distribution of malicious and benign
domain names.

VI. EVALUATION
The results of our experiments are divided into two sections.
Firstly, we evaluate the performance of the newly proposed
LSTM model and compare it with a previous work known as
DeepDGA in terms of model accuracy [15]. In addition, we
report on how the model accuracy changes against tempering
of input samples in order to generate adversarial instances.
Finally, we analyze the DGA classiﬁer before and after adver-
sarial augmentation of training data.

At ﬁrst, a binary classiﬁcation, which simply predicts be-
tween DGA-based malicious or benign (Alexa top 135K)
samples are applied. Table II demonstrates a comparison
between the performance of the Deep-DGA model and our
LSTM-based model. The detection rates of Cryptolocker and
Dicrypt is higher with DeepDGA than our DGA classiﬁer with
the available samples. On the other hand, Locky V2, Pykspa,
Ramdo and Simda are detected with better accuracy by our
classiﬁer, and the rest of the cases show the same detection
rate for both. Even though the results demonstrated that the
improvement was not substantial, the results could have turned
out to be different, since we used a different dataset than
DeepDGA.

TABLE II: Deep-DGA and the Proposed Model Comparison

Deep-DGA

The Proposed DGA Detector

Corebot
Cryptolocker
Dircrypt
Locky V2
Pykspa
Qakbot
Ramdo
Ramnit
Simda
Average

1.0
1.0
0.99
0.97
0.85
0.99
0.99
0.98
0.96
0.97

1.0
0.98
0.98
0.99
0.90
0.99
1.0
0.98
0.98
0.98

A. LSTM model results

Table III shows the resulting detection rates for our model
for 68 DGA families. Our ﬁndings show that our method
performed with high accuracy (usually above the 0.97 accuracy
margin) for most of the DGA-based malware families.

We also evaluate the DGA classiﬁer performance consid-
ering standard metrics such as precision, recall, F1-score,
false positive rate (FPR), false negative rate (FNR) and area
under the receiver operating characteristic curve (AUC). These
evaluation metrics are widely used to measure the quality
of the ML model. Using the proposed algorithm, we craft
domain names from both benign (Alexa 135K) and DGA-
based malicious samples. Based on different epsilon values,
the changes in these evaluation metrics of the model can be
viewed in Table VI. Initially, we set the epsilon value to
zero to observe the actual performance of the model. Our
ﬁndings show that the model performs well in terms of the
aforementioned metrics. In case of the adversarial samples that
were generated from malicious domain names, the accuracy
rate of the LSTM model degrades with increase in epsilon
value until it stabilizes at an equilibrium, because, at that

TABLE III: Detection Rate of DGA Malware Families Using
the LSTM Model.

TABLE IV: Detection Rate of DGA Malware Families Before
and After Training Data Augmentation.

TABLEI:DetectionrateofeachDGAmalwarefamilyusingtheLSTMmodelDetectionRateNumberofSamplesBamital1.01780Banjori1.01129beebone1.0150Blackhole1.01129Bobax1.0260Chir1.098Corebot1.01129Darkshell1.048Dyre1.01189Ebury1.0650Emotet1.01401Feodo1.0180Gameover1.08498GameoverP2P1.01933Gspy1.041Infy1.01050Modpack1.0200Murofetweekly1.01200Murofet1.01201Pandabanker1.01198Ramdo1.01198Ranbyus1.02000Redyms1.030Rovnix1.01200Sisron1.02000Sutra1.02000Tsiﬁri1.050Ud21.02000Ud31.060Vidrotid1.0300Wd1.02000Xshellghost1.035Xxhex1.02000Chinad0.991129Diamondfox0.99829LockyV20.993549Oderoor0.991200Padcry0.991220Qadars0.992000Qakbot0.994000Sphinx0.992000Tinba0.991998Cryolocker0.984129Dircrypt0.98500Ramnit0.981200Simda0.982000Szribi0.981200Volatilecedar0.98498Bedep0.971028Ekforward0.97578Fobber0.97725Pushdotid0.97900Pykspa20.971200Urlzone0.972000Necurs0.964201Dnschanger0.941228Suppobox0.9412000Tempedrevetdd0.941000Proslikefan0.931098Torpig0.931200Vidro0.931276Mirai0.92500Pykspa2S0.921200Pykspa0.901201Ud40.90100Hesperbot0.87150Shifu0.872000Nymaim0.851200TABLEII:DetectionrateofeachDGAmalwarefamilybeforeandaftertrainingdataaugmentationDetectionRatewithInjectedAttackSamples(Epsilon=11)DetectionRatewithRe-labelledAttackSamplesImprovementRate%Beebone0.01.0100Tsiﬁri0.01.0100Ramdo0.041.096Sisron0.041.096Redyms0.071.093Ebury0.050.9792Szribi0.081.092Simda0.080.9991Ud40.00.9090Ranbyus0.141.086Vidrotid0.141.086Pykspa20.090.9384Ud30.171.083Cryptolocker0.170.9881Fobber0.191.081Pykspa2S0.170.9679Darkshell0.040.8076Suppobox0.220.9876Urlzone0.301.070Corebot0.321.068LockyV20.280.9668Necurs0.310.9867Volatilecedar0.030.7067Vidro0.310.9564Hesperbot0.170.8063Padcrypt0.350.9863Bedep0.370.9962Emotet0.381.062Pykspa0.290.9162Tempedrevetdd0.350.9661Dnschanger0.380.9860Oderoor0.370.9760Proslikefan0.350.9459Pushdotid0.300.8959Qadars0.421.058Sphinx0.410.9958Bobax0.431.057Diamondfox0.310.8453Dircrypt0.440.9652Feodo0.481.052Sutra0.491.051Ramnit0.480.9850Qakbot0.521.048Mirai0.490.9647Modpack0.531.047Shifu0.430.8946Nymaim0.420.8745Xshellghost0.310.7544Torpig0.390.7940Rovnix0.641.036Blackhole0.661.034Gameover0.681.032Murofet0.691.031Tinba0.751.025Chinad0.761.024Banjori0.771.023Pandabanker0.781.022Dyre0.811.019Ekforward0.821.018GameoverP2P0.831.017Infy0.841.016Xxhex0.851.015Chir0.981.02Gspy0.981.02Bamital0.991.01Murofetweekly0.991.01Ud20.991.01Wd0.991.01TABLE V: Transformation of Alexa Domain Name Samples.

TABLE VI: Performance of the DGA Classiﬁer vs. Penetration Coefﬁcient for Both Benign and Malicious Domains.

point, the model has been trained well enough to recognize
malicious domains. In addition, the dissimilarities between the
benign class and the malicious class drastically increase. This
indicates the limit of misclassiﬁcation, even with increasing
epsilon values.

We also consider benign instances as an input to corrupt
the benign samples for creating the adversarial domain names.
Subtle perturbations do not decrease an accuracy much, since
the injected epsilon values do not manipulate the original data
sufﬁciently to cause misclassiﬁcation. When we continue to in-
crease the penetration coefﬁcient that causes slight differences
to the original benign data, the model fails to recognize these
changes. Therefore, the model performance is dramatically
impaired. As we further scale up the noise, the model starts
to predict these drastic modiﬁcations more accurately, due to
severe degradation of the input. Table V shows how the domain
name samples are transformed by the epsilon values.

It is noteworthy to observe the decreasing accuracy of our
DGA classiﬁer as we add perturbations because adversarial
examples mislead the model into making incorrect decisions
that increase the number of false positives and false negatives.
The various adversarial samples that are created by injecting
noise has the potential
to deceive the LSTM model even
more than the adversarial samples generated by GAN. In
the study of Anderson et al. [15],
the
detection rate of the model is 48.0%, which means that they
achieved an attack success rate of about 53%. Table VI shows
instances of LSTM model’s accuracy to be around 45% with
different perturbation coefﬁcients. We achieved the highest
attack success rate of 56%, which is higher than the GAN
approach by 3%, indicating that our model generated DGA
families are able to deceive the ML model more effectively.

it was found that

B. Improving the LSTM Model with Augmented Training Data

As discussed above, we are able to successfully produce
adversarial domain names that can bypass detection by the
LSTM model. We show that successful augmentation of
training data samples can be done with our proposed method.

Changes in penetration coefﬁcients can impact
the DGA
classiﬁer to different extents in terms of the model accuracy.
We later modify the dataset by injecting correctly labelled
adversarial domains. We replace every malicious training
samples with its adversarial counterpart
including the top
Alexa 135K in the training set, and re-train the model. Table
IV illustrates the differences before and after training with
adversarial samples when the epsilon value is 11. Our reason
for selecting the value 11 for the epsilon is to illustrate the
maximum damage to the well-trained LSTM model and how
training the model with augmented data performs much better.
When the model is trained with adversarial samples, the
model
is able to detect unseen malicious samples in the
training set to a larger extent. The hardened classiﬁer increases
the model’s detection ability for each DGA family, as can be
seen from Table IV. As noted for some family groups, such
as Bamital, Gspy, and Ud2, the adversarial manipulation did
not have any signiﬁcant impact on model accuracy (within
1%). However, for most others, the training with augmented
data boosted accuracy immensely, on some occasions reaching
up to 100%. As a result, the model trained with adversarial
samples has shown to perform much more accurately, close to
the performance of the model before adversarial manipulation.

VII. CONCLUSIONS

In this paper, we presented a novel detection system based
on an LSTM model for the detection of both traditional and
dictionary-based DGA generated domains using the character-
by-character approach. Our experimental ﬁndings show that
the LSTM-based model can detect malicious domains with a
trivial margin of error.

However, machine learning models are unable to learn the
characteristic behaviors of DGA-based malicious domains if
there are new or never-seen-before data in the testing dataset.
In this study, we highlight this issue with an adversarial ma-
nipulation using different data perturbation cases. According
to our ﬁndings, newly generated domains using the proposed
perturbation approach could not be detected by the DGA clas-
siﬁer. After we trained the model with the augmented training

MaliciousBenignEpsilonAccuracyPrecisionRecallF1FPRFNRAUCEpsilonAccuracyPrecisionRecallF1FPRFNRAUC00.980.980.990.980.030.010.9900.980.980.970.970.010.020.9810.950.950.950.950.130.090.940.70.700.690.650.650.400.150.702.50.860.840.880.850.180.120.890.80.570.580.550.550.490.200.5850.820.780.770.770.290.140.821.00.460.480.420.440.550.300.46100.760.700.660.660.350.160.771.20.460.480.420.440.550.300.46110.760.690.650.650.350.170.771.40.480.520.500.500.500.240.48dataset, including adversarial samples, the experimental results
show that the LSTM model was able to detect previously
unobserved DGA families.

We store malicious domain names, detected by our model,
in a Protected Spreadsheet Container with Data (PROSPECD).
It provides data conﬁdentiality and integrity, as well as ori-
gin integrity, role-based and attribute-based access control.
PROSPECD protects the domain names in transit and at rest
against adversarial access and modiﬁcations.

REFERENCES

[1] (2020, September) Domain generation algorithm. [Online]. Available:

https://en.wikipedia.org/wiki/Domain generation algorithm

[2] S. Yadav, A. K. K. Reddy, A. Reddy, and S. Ranjan, “Detecting
algorithmically generated malicious domain names,” in Proc. of the 10th
ACM SIGCOMM Conf. on Internet measurement. ACM, 2010, pp. 48–
61.

[3] J. Woodbridge, H. S. Anderson, A. Ahuja, and D. Grant, “Predicting
domain generation algorithms with long short-term memory networks,”
arXiv preprint arXiv:1611.00791, 2016.

[4] I. Yilmaz, R. Masum, and A. Siraj, “Addressing imbalanced data
problem with generative adversarial network for intrusion detection,”
in 2020 IEEE 21st Intl. Conf. on Information Reuse and Integration for
Data Science (IRI).

IEEE Computer Society, 2020, pp. 25–30.

[5] I. Yilmaz, “Practical fast gradient sign attack against mammographic

image classiﬁer,” arXiv preprint arXiv:2001.09610, 2020.

[6] D. Ulybyshev, C. Bare, K. Bellisario, V. Kholodilo, B. Northern,
A. Solanki, and T. O’Donnell, “Protecting electronic health records in
transit and at rest,” in 2020 IEEE 33rd Intl. Symp. on Computer-Based
Medical Systems (CBMS), 2020, pp. 449–452.

[7] (2018,

September)

a
spam blacklist? [Online]. Available: https://www.pinpointe.com/blog/
how-do-i-know-if-im-on-a-spam-blacklist

i’m on

know

How

do

if

i

[8] (2020, September) Blacklisting vs whitelisting – understanding
the security beneﬁts of each. [Online]. Available: https://blog.ﬁnjan.com/
blacklisting-vs-whitelisting-understanding-the-security-beneﬁts-of-each/
[9] M. K¨uhrer, C. Rossow, and T. Holz, “Paint it black: Evaluating the effec-
tiveness of malware blacklists,” in Intl. Workshop on Recent Advances
in Intrusion Detection. Springer, 2014, pp. 1–21.

[10] M. Ligh, S. Adair, B. Hartstein, and M. Richard, Malware analyst’s
cookbook and DVD: tools and techniques for ﬁghting malicious code.
Wiley Publishing, 2010.

[11] J. Zhang, R. Perdisci, W. Lee, U. Sarfraz, and X. Luo, “Detect-
ing stealthy p2p botnets using statistical trafﬁc ﬁngerprints,” in 2011
IEEE/IFIP 41st Intl. Conf. on Dependable Systems & Networks (DSN).
IEEE, 2011, pp. 121–132.

[12] T.-F. Yen and M. K. Reiter, “Are your hosts trading or plotting? telling
p2p ﬁle-sharing and bots apart,” in 2010 IEEE 30th Intl. Conf. on
Distributed Computing Systems.

IEEE, 2010, pp. 241–252.

[13] J. Manni, A. Aziz, F. Gong, U. Loganathan, and M. Amin, “Network-
based binary ﬁle extraction and analysis for malware detection,” Jan. 13
2015, uS Patent 8,935,779.

[14] A. Aziz, H. Uyeno, J. Manni, A. Sukhera, and S. Staniford, “Electronic
message analysis for malware detection,” Jul. 17 2018, uS Patent
10,027,690.

[15] H. S. Anderson, J. Woodbridge, and B. Filar, “Deepdga: Adversarially-
tuned domain generation and detection,” in Proc. of the 2016 ACM
Workshop on Artiﬁcial Intelligence and Security.
ACM, 2016, pp.
13–21.

[16] D. Berthelot, T. Schumm, and L. Metz, “Began: Boundary equilib-
rium generative adversarial networks,” arXiv preprint arXiv:1703.10717,
2017.

[17] I. Yilmaz and R. Masum, “Expansion of cyber attack data from
unbalanced datasets using generative techniques,” arXiv preprint
arXiv:1912.04549, 2019.

[18] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing

adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.

[19] D. Plohmann, K. Yakdan, M. Klatt, J. Bader, and E. Gerhards-Padilla,
“A comprehensive measurement study of domain generating malware,”
in 25th {USENIX} Security Symp. ({USENIX} Security 16), 2016, pp.
263–278.

[20] P. Lison and V. Mavroeidis, “Automatic detection of malware-
generated domains with recurrent neural models,” arXiv preprint
arXiv:1709.07102, 2017.

[21] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker, “Beyond blacklists:
learning to detect malicious web sites from suspicious urls,” in Proc. of
the 15th ACM SIGKDD Intl. Conf. on Knowledge discovery and data
mining. ACM, 2009, pp. 1245–1254.

[22] B. Yu, J. Pan, J. Hu, A. Nascimento, and M. De Cock, “Character level
based detection of dga domain names,” in 2018 Intl. Joint Conf. on
Neural Networks (IJCNN).

IEEE, 2018, pp. 1–8.

[23] D. Tran, H. Mac, V. Tong, H. A. Tran, and L. G. Nguyen, “A lstm based
framework for handling multiclass imbalance in dga botnet detection,”
Neurocomputing, vol. 275, pp. 2401–2413, 2018.

[24] M. Pereira, S. Coleman, B. Yu, M. DeCock, and A. Nascimento, “Dic-
tionary extraction and detection of algorithmically generated domain
names in passive dns trafﬁc,” in Intl. Symp. on Research in Attacks,
Intrusions, and Defenses. Springer, 2018, pp. 295–314.

[25] B. Yu, D. L. Gray, J. Pan, M. De Cock, and A. C. Nascimento, “Inline
dga detection with deep networks,” in 2017 IEEE Intl. Conf. on Data
Mining Workshops (ICDMW).

IEEE, 2017, pp. 683–692.

[26] R. R. Curtin, A. B. Gardner, S. Grzonkowski, A. Kleymenov, and
A. Mosquera, “Detecting dga domains with recurrent neural networks
and side information,” in Proc. of the 14th Intl. Conf. on Availability,
Reliability and Security, 2019, pp. 1–10.

[27] Y. Zhauniarovich, I. Khalil, T. Yu, and M. Dacier, “A survey on ma-
licious domains detection through dns data analysis,” ACM Computing
Surveys (CSUR), vol. 51, no. 4, pp. 1–36, 2018.

[28] L. Lilien and B. Bhargava, “A scheme for privacy-preserving data
dissemination,” IEEE Trans. on Systems, Man, and Cybernetics-Part A:
Systems and Humans, vol. 36, no. 3, pp. 503–506, 2006.

[29] (2020,

September)

seminars, & conferences
guidelines. [Online]. Available: https://www.microsoft.com/en-us/legal/
intellectualproperty/trademarks/usage/publications.aspx

Publications,

[30] L. B. Othmane, “Active bundles for protecting conﬁdentiality of sensitive

data throughout their lifecycle,” 2010.

[31] L. B. Othmane and L. Lilien, “Protecting privacy of sensitive data
dissemination using active bundles,” in 2009 World Congress on Privacy,
Security, Trust and the Management of e-Business.
IEEE, 2009, pp.
202–213.

[32] R. Ranchal, “Cross-domain data dissemination and policy enforcement,”

2015.

[33] C. N. Tun and K. T. Mya, “Secure spreadsheet data ﬁle transferring

system.”

5th Local Conf. on Parallel and Soft Computing, 2010.

[34] M. R. A. Mithu, V. Kholodilo, R. Manicavasagam, D. Ulybyshev, and
M. Rogers, “Secure industrial control system with intrusion detection,”
in The 33rd Intl. Flairs Conf., 2020.

[35] (2019, September) Dgas in the hands of cyber criminals examining the
state of the art in malware evasion techniques. [Online]. Available: https:
//web.archive.org/web/20160403200600/https://www.damballa.com/
downloads/r pubs/WP DGAs-in-the-Hands-of-CyberCriminals.pdf-

[36] (2020, September) Gradient descents.
//en.wikipedia.org/wiki/Gradient descent

[Online]. Available: https:

[37] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[38] S. Hochreiter, “The vanishing gradient problem during learning recur-
rent neural nets and problem solutions,” Intl. Journal of Uncertainty,
Fuzziness and Knowledge-Based Systems, vol. 6, no. 02, pp. 107–116,
1998.

[39] F. A. Gers and E. Schmidhuber, “Lstm recurrent networks learn simple
context-free and context-sensitive languages,” IEEE Transactions on
Neural Networks, vol. 12, no. 6, pp. 1333–1340, 2001.

[40] (2020, September) Cosine similarity.
//en.wikipedia.org/wiki/Cosine similarity

[Online]. Available: https:

[41] N. Papernot, P. McDaniel, and I. Goodfellow, “Transferability in ma-
chine learning: from phenomena to black-box attacks using adversarial
samples,” arXiv preprint arXiv:1605.07277, 2016.

[42] (2020, September) Majestic million.

[Online]. Available: https:

//majestic.com/reports

[43] (2020, September) From research to production. [Online]. Available:

https://pytorch.org/

[44] M. Patella and P. Ciaccia, “Approximate similarity search: A multi-
faceted problem,” Journal of Discrete Algorithms, vol. 7, no. 1, pp.
36–48, 2009.

