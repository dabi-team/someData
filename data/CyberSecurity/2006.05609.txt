LEARNING WITH DIFFERENTIAL PRIVACY 

Poushali Sengupta[1], Sudipta Paul[2,3], Subhankar Mishra[2,3] 

[1] University of Kalyani, Kalyani, Nadia, West Bengal – 741235 

[2] National Institute of Science, Education and Research Bhubaneswar 

Odisha, India – 752050 

[3] Homi Bhaba National Institute, Anushaktinagar, Mumbai – 400094, India 

[1] tua.poushalisengupta@gmail.com , [2] sudiptapaulvixx@niser.ac.in , [3] smishra@niser.ac.in 

INTRODUCTION 

Humans gain “knowledge” by inference from raw events, incidents or structured phenomenon. That implies, 
as  long  as  it  is  not  meaningful  or  inferred  properly,  this  “raw  data”  doesn’t  become  “information”  to  be 
inferred that help humans to grab “knowledge” from. Our chapter refers to the definition of knowledge given 
by  (Davenport  et  al.,  1998).  It  states  that  from  the  perspective  of  an  expert  with  respect  to  the  particular 
experiences and principles in the right context give a proper structure to asses and integrate new raw data and 
information. This structure helps to make the transition of the data to “knowledge” - inside a proper intelligent 
mind. This phenomenon is equally applicable in the daily routine of any organization, processes, norms and 
practices. 
Raw data goes through processes to add contextual meaning in background to become  “information”. These 
processes are heavily prone to defect and danger depending on the nature of the data, its sensitivity and its 
usefulness towards the organizations. One of the biggest dangers that these “information” and “raw data” can 
face  is  “leakage”.  Leakage  defined  as  -  when  an  adversary  knowingly  disclose  sensitive  information  for 
business  purpose  to  harm  an  individual,  a  community  or  a  particular  target  for  his/her  own  personal 
satisfaction.  The  “leakage”  can  happen  in  the  pre-processing  as  well  as  in  the  post-  processing  of 
“information”.  

These situations are not desired by any means. Some of the probable solutions are - 

•  Make sure all endpoints have basic Cyber security systems. 
•  Use a data backup and recovery solution using encryption-decryption system. 
•  Clean up the data Storage on the IT Assets after a certain time - window  
•  Limit user access privileges to only what is absolutely necessary 
•  Provide Cyber security awareness training to the employees 
•  Build security system inherited from the data itself that doesn’t need any kind of third-party affiliation, but 

robust and fast enough to provide enough privacy and security promise simultaneously. 

All of the promises except the last one needs some audit from a 3rd party who or which can be a potential attacker. 
Also,  these  promises  need  extensive  monitoring  from  a  human  perspective  all  the  time  which  is  a  tough  and 
cumbersome  work.  The  last  point  in  the  above  solutions  is  formally  known  as  “differential  privacy”  that  is 
currently the default trend of privacy solution. 
The definition of “differential privacy” (DP) will be discussed in section 4 thoroughly. But as a promise, DP might 
be  thought  of  as  a  restriction  which  filters  the  leakage  of  sensitive  statistics  at  the  time  of  the  publication  of 
aggregated information, with respect to a database, in the algorithmic level. To elaborate the above promise some 
examples are discussed below (Wikipedia – Differential Privacy), 
Government departments and agencies use DP algorithms at the time of publishing demographic and other types of 
statistical aggregated analysis report with the assurance of  the confidentiality of the survey takers and responses, 

 
 
 
 
 
 
 
Companies use DP algorithms at the time of collecting user behavior, in every step to stop leakage. This measure is 
also applicable to the internal analysts. 
In whatever way the sales numbers of a business are covered in the process of hiding, those numbers might appear 
when  the  same  process  will  be done in the  total  calculation  of a  vast  region  that the  business  belongs  to with a 
combinations of addition and subtraction. DP algorithms diminish those possibilities from the root itself even if the 
attackers use robust, interactive query system. 
The research and finding timeline that results in differential privacy is following - 
1950s  -1960s:  Statistical  agencies  started  using  electronic  information  processing  system  that  resulted  in  the 
increase of number of data tables as well as potential attack. 

•  1977: Statistician Tore Dalenius proposedcell suppression mathematics for statistical disclosure control. 
•  2003:Computer  scientists  Kobbi  Nissim  and  Irit  Dinur  presented  the  Fundamental  Law  of  Information 

Recovery, and its key characteristics. 

•  2006: Theoretical computer scientists Cynthia Dwork, McSherry, Kobi Nissim and Smith introduced the 
concept of DP, with a mathematical definition for the privacy loss and utility associated with any result 
related to a statistical database (Dwork, 2006; Dwork & Roth, 2014). They proposed (Î, d) DP where the 
whole  processes  of  statistical  functions  running  on  the  database  are  not  overly  dependent  on  an 
individual’s data.  

The  different  attack  models,  reasons  to  justify  them,  probable  solutions  in  academia  and  industry  are  the  main 
agenda of this chapter which are discussed thoroughly in the following sections. 

DIFFERENTIAL PRIVACY 

From the above discussion in the Introduction section it is evident that differential privacy comes with respect 
to  the  continuous  development  in  the  hope  of  reduction  of  information  leakage.  In  the  following  three 
subsections,  we  will  discuss  about  what  differential  privacy  does  not  promise,  different  promises  of 
differential privacy, and some useful definitions. 

Problems Regarding Privacy - Preserving Data Analysis 
There are some important problems for privacy-preserving data analysis which will be discussed here: 

•  Anonymization 

Only anonymization of datasets cannot create the strong privacy. In an anonymized rich database, data 
enables  “naming”  an  individual  by  the  combination  of  the  birth  date,  zip  code,  age  etc.  to  create  the 
uniqueness  of  the  individuals.  This  “naming”  can  be  used  for  linkage  attack  to  match  an  anonymized 
dataset  to  a  non-anonymized  data  set.  In  the  year  1997,  a  linkage  attack  on  Massachusetts  hospital 
discharge  dataset  with  public  voter  dataset  was  done  by  Prof.  Latanya  Sweeney  and  name,  address, 
phone number of each persons were leaked. 

•  Re-identification 

Re-identification is undesirable and risky because it reveals not only the membership of the database but 
also the compromising data records of the individuals of the dataset. 

•  Large Query 

It cannot be predictable how large query would be. A set of queries or a large query can leak the personal 
information of individual of a particular database. For example, let M and N are two events and the a set 
of queries is “How many people are involved with M?”; “How many people are involved with both M 
and  N?”;  “How  many  people  are  involved  with  M  for  last  2  weeks?”  and    “How  many  People  are 
involved  with  both M  and  N  for  last 3  weeks?”.  By asking  these  questions,  one  can  get the  particular 
answer that he/she wants to know. This situation can compromise the privacy of the individuals. 

•  Query Auditing 

In some cases, one can send two or more than two such queries whereby the difference of the answers of 
these queries, he/she can get the actual answer that he/she wants to know. Privacy can be compromises in 
this way. 
•  Just a few 

In some cases, the data holds privacy in such a way that it does not compromises the private data of all 
individuals except “just a few” information of individuals. Here, the data base is not fully protected. 

 
 
 
 
 
 
 
•  Ordinary fact 

Revealing  ordinary  fact  like  number  of  times  a  person  goes  to  the  supermarket  in  a  week,  can 
compromises sensitive information if it is followed over time. 

In order to give a plausible solution to the above issues with the existing privacy-preserving data analysis, DP 
as  a  concept  was  plotted  by  Cynthia  Dwork  et  al.  in  their  2006  work  (Dwork,  2006).  “Differential 
privacy”(Dwork  &  Roth,  2014)  is  a  methodology  by  which  public  sharing  of  information  regarding  any 
dataset  is  restricted  to  describe  the  groups  in  the  dataset  but  not  any  information  about  the  individuals.  DP 
addresses  a  paradox  of  learning  where  one  can  know  about  the  useful  information  of  the  given  database 
without  accessing  the  particular  sensitive  information  of  individuals.  More  fundamentally,  this  provides  a 
facility of learning overall forest data without knowing the individual trees containing the private information.  

The Promise of Differential Privacy 
The Fundamental Law of Information Recovery from introduction gives a start point to understand DP where 
they posed a restriction on the amount of query that might be asked by the analyst to not to reveal any private 
information by giving an overly accurate answer. DP possess an assurance inherently by the data curator that 
an individual is not prone to attack, adversely or otherwise, by providing their data for the purpose of use in 
any kind of research or survey, irrespective of connection to any other databases or data sources available at 
any corner of the world. 

If  a  medical  database  is  considered  for  analysis,  an  insurance  company’s  view  of  an  alcoholic’s  long-term 
medical  costs  can  be  improved  as  they  can  know  from  the  database  without  accessing  individuals’  data 
records  that  drinking  too  much  alcohol  causes  various  liver  disease,  most  importantly  cancer.  As  a  result, 
one’s  insurance  premium  may  rise,  if  the  insurer  knows  that  the  particular  person  drinks  alcohol.  Here  the 
medical  database  is deferentially  private as the information  of individuals  are not  “leaked”  to the insurance 
company and it makes an impact on the participants in the survey for giving answer independently whether 
he/she drinks alcohol or not. 

DP  is  not  an  algorithm,  it  is  a  theory  that  ensures,  a  stream  of  outputs  in  arbitrary  sequences  of  choice 
(responses to queries) is “essentially” equally likely to occur where this term “essentially” is maintained by a 
privacy parameter. The smaller the value of this parameter the better privacy occurs. This parameter is denoted 
by𝜖 . 

Different Types of Differential Privacy 
There are two types of DP. They are discussed below : 

•  Local Differential Privacy (LDP): 

A technique introduced in 1965 (Warner, 1965) named as - “Randomized Response” is the basis of this kind 
of DP. The users answer the queries using only coin toss probability set. The main plus point of this simple 
model  is  that  the  distribution  of  the  data  is  always  stable enough even  when  a user  suddenly  changes  the 
response  out  of  the  blue.  There  is  no  need  to  have  the  affiliation  of  a  third  party  authorization  too.  As  it 
always gives the answer for a forest of data instead of a single tree maintaining the above assumption, it is 
highly adopted to the industry as well as in the academia research.     

•  Central Differential Privacy (CDP):  

This whole technique is depended on the trust and proficiency of the data curator (DC). The DC add random 
noise to the aggregated answer in the central part after collecting all the data from the local servers. In the 
local servers the answer to the queries are independent of each other and do not know other servers’ identity. 
Therefore all the answers are individually part of the centralized dataset. As the answers are independent and 
the DC is adding external random noise, the whole phenomenon leads to CDP.  

 
 
 
Therefore, an easy comparison can be inferred from the above discussion which is given in the Table 2. 

Table 2. Comparative discussion on LDP and CDP 

LDP 

CDP 

No place for a trusted curator, or any data storage. 

Assurance of report production entirely depended on 
the trusted individual clients. They give their 
responses using the randomized response technique 
on the coin toss set.   

The data is collected from servers which are 
independent to each other and stored in a trusted 
centralized database. 
The curator releases the final report applying the 
Laplace noise randomly drawn from Laplace 
distribution for hiding the presence or absence of an 
individual. 

Example: RAPPOR in google chromium project for 
google chrome search engine. 

PROCHLO implementation of ESA, ESA revisited, 
Amplification by shuffling etc. techniques. 

A short timeline regarding the most important inventions and discoveries in DP is given in the following figure 1. 

Figure 1. Short timeline of DP 

STATE OF THE ART DP TECHNIQUES ACCORDING TO TIMELINE 

Differential privacy is a concept which possesses a huge promise towards security. Here we will discuss about 
some related works in the industry as well as in the academia using differential privacy such as “what can we 
learn privately”, RAPPOR, PROCHLO, OUTIS, ESA, ARA etc. and compare them with each other. 

 
 
 
 
 
 
 
 
 
 
 
 
 
Learning with limit 
Learning  problems  form  an  important  section  of  various  computational  tasks  that  are  applied  by  a  large 
number of computational researchers in real life. To recover these types of problems and to find the answer of 
“what can we learn privately”(Kasiviswanathan et. al, 2011), in this work they explored different approaches 
of learning concept classes as background theory, that eventually let them  propose an algorithm in terms of 
samples, computations, time and interaction that holds the following promises of improvement. 

•  Occam’s razor’s private version. 
•  Learning privately with an efficient learner. 
•  Local (randomised response) and SQ learning are equivalent. 
•  Local learning in two different forms i.e. interactive and non-interactive. 

In the following subsection we will go through the preliminary background theories and theoretical result they 
derived in the process. 

Preliminary Background Theories 
The  preliminary  background  theories  of  this  work  have  explored  differential  privacy,  agnostic  learning, 
Probabilistically  Approximately  Correct  (PAC)  learning,  SQ  learning  and  their  efficiency  measure.  The 
background of differential privacy will be discussed in the section 4. PAC model is the learning model that has 
the capability to access a polynomial number of labeled answers. SQ model do not access the answers directly 
whereas the learner can specify some properties on the example for which an estimate is given up to additive 
small  error,  of  the  probability  that  a  randomly  chosen  example  which  follow  the  distribution  D,  satisfy  the 
property.  The  PAC  learning  is  stringently  stronger  than  SQ  learning.  A  brief  discussion  of  PAC,  SQ  and 
Agnostic learning is given in the Table 3 below. 

Table 3: Description and mathematical formulation of different learning models. 

Learning Theories 

Description 

PAC (Probabilistically 
Approximately Correct)  

A concept class depending on 
some definite distribution will be 
PAC learnable using a 
hypothesis class if there exists 
such algorithm maintaining a 
polynomial time, whose output 
will produce a definite 
hypothesis from the aforesaid 
hypothesis class maintaining the 
bounds for probability of failure. 

Agnostic Learning 

It is identical to PAC theory with 
two more addition regarding 
distribution and bounds for 
probability of failure. 

Formulas and mathematical 
expressions 

∀𝑑𝜖ℕ, ∀𝑐𝜖𝐶𝑑 , ∀𝒳𝜖𝑋𝑑, an algorithm 
𝒜 maintaining polynomial in 
𝑑, 1 𝛼⁄ , log⁡(1 𝛽⁄ ), will give an output 
from the hypothesis class ℎ𝜖⁡ℋ , 
satisfying  

Pr[𝑒𝑟𝑟𝑜𝑟(ℎ) ≤ ⁡𝛼] ⁡ ≥ 1 − ⁡𝛽 

over the drawn examples 𝑧𝑖 =
(𝑥𝑖, 𝑐(𝑥𝑖)) 

∀𝑑𝜖ℕ, ∀𝑐𝜖𝐶𝑑 , ∀𝒳𝜖𝑋𝑑 × {0,1}, an 
algorithm 𝒜 maintaining polynomial 
in 𝑑, 1 𝛼⁄ , log⁡(1 𝛽⁄ ), , will give an 
output from the hypothesis class ℎ𝜖⁡ℋ 
, satisfying  

Pr[𝑒𝑟𝑟𝑜𝑟(ℎ) ≤ ⁡𝑂𝑃𝑇 + 𝛼] ⁡ ≥ 1 − ⁡𝛽 

over the drawn examples 𝑧𝑖 =
(𝑥𝑖, 𝑐(𝑥𝑖)) 

 
 
 
 
Private PAC Learning 

Same as PAC learning with the 
addition of privacy parameter 
∀𝜀 > 0 on the algorithm 𝒜 and 
polynomial runtime of⁡𝒜. 

Private Agnostic Learning 

Same as Agnostic Learning with 
the addition of privacy parameter 
∀𝜀 > 0 on the algorithm 𝒜 and 
polynomial runtime of⁡𝒜. 

SQ (Statistical query)oracle  

SQ learning 

Local Learning 

Efficient private learner for 
PARITY 

It takes a statistical query as an 
input over some distribution on 
labeled data with a promise to 
maintain the output within the 
tolerance of the expectations of 
the statistical query drawn from 
the distribution.  

A class of functions over a 
definite dimension will be SQ 
learnable using SQ oracle if there 
exists such algorithm 
maintaining a polynomial time, 
whose output will be produced as 
an element of the hypothesis 
class by asking a limited number 
of queries in the limit of 
tolerance parameter with a 
definite probability. 

It is identical to PAC learning 
with the addition of the SQ 
learning. 

It maintains PAC learn ability.  
The new introduction here is a 
class of PARITY functions 
instead of a class of concepts 
with a definite failure probability 
1
of
2

+ ⁡𝛽 

∀𝑑𝜖ℕ, ∀𝑐𝜖𝐶𝑑 , ∀𝒳𝜖𝑋𝑑, ∀𝜀 > 0 ,an 
algorithm 𝒜 maintaining polynomial 
in 𝑑, 1 𝛼⁄ , 1 𝜀⁄ , log⁡(1 𝛽⁄ ), , will give 
an output from the hypothesis class 
ℎ𝜖⁡ℋ , satisfying  

Pr[𝑒𝑟𝑟𝑜𝑟(ℎ) ≤ ⁡𝛼] ⁡ ≥ 1 − ⁡𝛽 

over the drawn examples 𝑧𝑖 =
(𝑥𝑖, 𝑐(𝑥𝑖)) 

∀𝑑𝜖ℕ, ∀𝑐𝜖𝐶𝑑 , ∀𝒳𝜖𝑋𝑑 × {0,1}, ∀𝜀 >
0 , an algorithm 𝒜 maintaining 
polynomial in 𝑑, 1 𝛼⁄ , 1 𝜀⁄ , log⁡(1 𝛽⁄ ), 
, will give an output from the 
hypothesis class ℎ𝜖⁡ℋ , satisfying  

Pr[𝑒𝑟𝑟𝑜𝑟(ℎ) ≤ ⁡𝑂𝑃𝑇 + 𝛼] ⁡ ≥ 1 − ⁡𝛽 

over the drawn examples 𝑧𝑖 =
(𝑥𝑖, 𝑐(𝑥𝑖)) 

|⁡𝒪𝒟

𝜏 (𝜙) − ⁡ 𝔼(𝑥,𝑦)~𝒟[𝜙(𝑥, 𝑦)]| ≤ ⁡𝜏 

With probability:1 − ⁡𝛽,  

Output a hypothesis 𝑓𝜖𝐶maintaining: 
𝑒𝑟𝑟(𝑓, 𝒟) ≤ ⁡ 𝑚𝑖𝑛𝑓∗𝜖𝐶𝑒𝑟𝑟(𝑓∗, 𝒟) + ⁡⁡𝛼 

Tolerance parameter 𝜖⁡(0,1) 

Input function: 
𝑆𝑄𝑐,𝒳 

Output v maintains the SQ learning 
promise.  

Given, 𝑐𝑟, 𝑟𝜖{0,1}𝑑, satisfies: 

Pr[𝒜(𝑛, 𝑧, 𝜀) = 𝑒𝑟𝑟𝑜𝑟(ℎ) ≤ 𝛼] ⁡

≥

1
2

+ ⁡𝛽 

 
 
 
MASKED-PARITY 

It maintains the weak SQ learn 
ability with polynomial number 
of queries, error bounded below 

1

2

This learning concept class is 
considered when the distribution  
𝑋𝑑 is uniform over binary strings with 
length𝑑 + log 𝑑 + 1 

In the following table 4 the symbols and description for all the aforementioned learning models is given. 

Table 4. Symbols and Descriptions for the different learning models 

Symbols 

Description 

C 
𝐶𝑑 
𝐷 
𝒳 
𝑋𝑑 
N 
X 
H 
h 
error(h) 
c 
β 
α 
z 
c(x) 
A 

𝑐𝑟 
Cr(x) 
r 
β0 
z0 
A∗ 
SQD 
𝑆𝑄𝑐,𝒳 

𝜏 

𝜙 

𝜏  
𝒪𝒟

n 

𝜖 

Universal concept class:{𝐶𝑑}𝑑𝜖ℕ 
The class of concepts from 𝑋𝑑to {0,1} 
𝑋𝑑× {0,1} 
{𝑋}𝑑𝜖ℕ 
The distribution of sample𝑑𝜖ℕ 
Set of natural number 
Set of all distributions on 𝑋𝑑 
A class of hypothesis. 
A hypothesis that belongs to H 
𝑃𝑥,𝑦⁡~𝒟[⁡ℎ(𝑥) ≠ 𝑐(𝑥)] 
A specific concept that belongs to𝑐𝜖𝐶𝑑 
Bounds of the probability of failure 
Desired error 
Learning algorithm’s input. 
A particular element of a particular concept. 
An algorithm 
privacy budget. 
A class of parity functions {0,1}𝑑 → {0,1} 
A set of cross products of random x 
{0,1}𝑑 
Probability failure bound for z0 
Neighboring input of z 
An algorithm is found by PAC learning. 
A statistical query over the distribution D 
The statistical query oracle that takes as a input function g:𝒳 × {+1, −1} → {+1, −1} 

Tolerance Parameter and  𝜏 ∈ (0,1) 

Statistical Query function:𝜙 ∶ ⁡𝒳 × {0,1} → [0,1] 

Statistical query oracle 

Ο(

log⁡(1 𝛽)⁄

𝜖𝛼

(𝑑 + log

1

𝛽

Privacy Budget 

)) no. of examples 

For the purpose of better understanding the above discussion the classification of the learning approaches  is 
given in table 5 below. The  star (*) marked approaches are the authors own contribution towards different 
learning approaches.  

 
 
 
 
 
 
 
 
Table 5. Associated Works with PAC and SQ learning 

Learning Approach 

PAC learning 

SQ learning 

Related Learning Theories 

PAC learning, Agnostic learning, 
Private PAC learning, Private 
Agnostic learning, Efficient private learner for PARITY *. 

Statistical Query (SQ) oracle, 
SQ learning, Local learning, 
MASKED-PARITY* 

RAPPOR (Randomized Aggregatable Privacy-Preserving Ordinal Response) 
RAPPOR (Erlingsson et. al., 2014) allows the overall client data to be studied without giving permission for 
the  possibility  to  access  the  individual  information,  with  strong  privacy  and  great  utility  guarantee  using 
randomized response technique in bloom filter. The main contributions of this work are: 

•  RAPPOR provides a local deferentially private model. 
•  It generates report by creating noise by applying bloom filter. 
•  It is the first industrialized implementation of differential privacy. 
•  it is a fast framework that provides strong privacy with great utility. 

The RAPPOR algorithm is in the following flow-chart in figure 2: 

Figure 2. Flow chart of RAPPOR 

The  three  types  of  RAPPOR  are  explained  in  the  Table  6.  In  Table  7  all  the  symbols  and  description  of 
RAPPOR is defined. 

Table 6. Explanations of three types of RAPPOR 

One-Time RAPPOR 

As the client itself imposed the one-
time collection in this case, the 
instantaneous randomized response 
step can be skipped here where the 
direct randomization on the client’s 
true value is robust enough to give 
the protection against longitudinal 
privacy. 

Basic RAPPOR 
It is a special case of RAPPOR. 
If the report collected are small 
in size and well-defined 
enough it can be mapped to a 
single bit without using a 
Bloom Filter, reducing the 
number of hash functions in 
exchange. An easy example 
can be data on clients’ binary 
preferences where an “yes” 
means 1 and a “no” means 2. 
Here the number of effective 
bloom filters is 1. 

Basic One-Time RAPPOR 
This is the combination of One-
time RAPPOR and Basic 
RAPPOR. A randomization step 
followed by a settled mapping to 
unique bits. 

 
 
 
 
 
Table 7. Symbols and Description for RAPPOR 

Description 

Symbols 
ℎ 
𝐵 
𝑘 
𝐵′ 
𝑆 
𝑓, 𝑝, 𝑞 

Client Value 
First Applied Bloom filter 
Size of the response bit string 
Permanent Randomized Response 
Instantaneous Randomized Response 
User tunable parameters 

Experiments in RAPPOR 
By recall precision graph it is seen that using of only two  hash functions (when k and m are fixed) result in 
better utility as decrease in hash function is the cause for increase in expected recall. 

For the experiment, two simulated and two real world data were used to apply RAPPOR algorithm. The first 
one of simulated data was used for Basic One-time RAPPOR and the underlying distribution was found to be 
normal distribution with enough noise. The second simulated example was used to apply modified RAPPOR 
for collecting strings and it showed the exponential distribution of string frequencies. The third example was 
real world data on processes running on windows machines. In this case, the frequency of a particular process 
”BADAPPLE.COM” was estimated with 128 bit size bloom filter, 2 hash functions, 8 cohorts where q = 0.75, 
p  =  0.5  and  f  =  0.5.  The  estimated  frequency  of  BADAPPLE.COM  was  found  to  be  2.6%.  The  data  on 
Chrome  Homepages  were  used  for  last  example  and  it  was  observed  that  the  analysis  of  RAPPOR  can 
discover  URL  domains  of  homepage,  with  statistical  confidence,  if  their  frequencies  exceed  0.1%  of  the 
responding population. 

PROCHLO 
It’s  a  common  practice  now  a  day  to  monitor  the  user’s  software  activities  thoroughly to  provide good and 
elevated  service  by  the  companies.  PROCHLO  (Bittau  et.  al.,  2017)  describes  architecture  of  a  principled 
system  -  Encode,  Shuffle,  analyze  (ESA)  to  perform  such  activities  by  maintaining  higher  utility  through 
preserving  strong  privacy  guarantee.  Here  a  pipeline  is  used  where  a  randomly  drawn  sample  from  users’ 
input  is  (Encoding  :)  encrypted  first  by  pushing  noise  randomly,  encoded  by  breaking  the  whole  encrypted 
string into data fragments, then (Shuffling:) shuffled by a secret shuffler and at last (Analyzing:) the report is 
generated from the sample. 

Architecture: PROCHLO  
It uses the Google SGX as the secret shuffler. SGX has small memory that cannot hold the millions of  data, 
and that’s why rather transferring the whole sample of users’ input directly to the shuffler, PROCHLO creates 
crowd IDs. Let, the total number of users’ input is N and the number of crowd ID is B+. Then the number of 
𝑁
input having the same crowd ID is⁡𝐷+ = ⁡
𝐵+. After getting the bunch of inputs belonging to same crowd ID, 
they are divided into fragments for encoding. 

• Step 1: Encoding 

The  encoding  can  be  done  by t∗ secret  shares  where t∗  secret  shares  split  a  secret  S∗  in  a  filed  F into 
arbitrary S∗1, S∗2,.... so that any t∗ −1 shares can hide all information about S∗. A secret share of S∗ is 
tuple  [x,ρ(x)]  for  randomly  chosen  nonzero  x    F  where  S∗=  ρ(x).  Let  t  secret  sharing  encode  of  an 
arbitrary string m with parameter t∗ is a pair as (c,aux) where c is the cipher text which is a deterministic 
encryption under a  key 𝐾𝑚= H(m)  and  aux  be the  t∗  secret  shares  of 𝐾𝑚.  But  the m is recoverable by 
using robust decryption attack with the help of aux1, aux2, ....auxt to find the 𝐾𝑚 and by using 𝐾𝑚 he/she 
can decrypt c and in this way one can create an attack to disclose the private data of individuals. 

 
 
 
 
 
 
 
So,  PROCHLO  use  the  data  fragmentation  for  encryption.  For  example,  consider  an  analysis  on  a 
database  based  upon  the  audiences’  ratings  on  movies.  In  this  database  each  entryis  supposed  to  be 
“individual” and a concern of privacy. To protect privacy, fragmentation is used here. consider a movie 
rating set is {(𝑚0, 𝑟0),((𝑚1, 𝑟1),(𝑚2, 𝑟2)}. Where 𝑚i (i=0:2) is the name of the ith movie and 𝑟i (i=0:2)is 
the  rating  of  the  ith  movie.  Now,  the  elements  of  this  rating  set  can  be  encoded  as  their  pairwise 
combinations like〈(𝑚0, 𝑟0), (𝑚1, 𝑟1)〉, 〈(𝑚0, 𝑟0), (𝑚2, 𝑟2)〉and 〈(𝑚1, 𝑟1), (𝑚2, 𝑟2)〉 whereeach pair  will  be 
transmitted to the secret shuffler for the independent shuffling. The shuffler is trusted but always curious 
about the private data of individuals. Here, for generating the crowd IDs and encryption, it is hard for the 
shuffler to guess about the private information of any particular individual. 

• Step 2: Shuffling 

In the next part of this pipeline, this shuffler performs four tasks and those are anonymization, shuffling, 
thresholding and batching. 

o  Anonymization - Anonymization is created by striping metadata which is not enough for privacy. 

𝑁
𝐵∗

o  Shuffling-  For  this,  the  SGX  do  the  oblivious  shuffling.  Based  on  Melbourne  shuffle  technique, 
PROCHLO  introduces  the  Stash  shuffle  that  actually  improves  the  all  previous  problems.  Here, 
consider  B∗  input  and  outputs  buckets  each  having𝐷”⁡ =
 items.  At  first, 𝐷”⁡ items  of  each  input 
buckets are random shuffled with B  ∗ −1 bucket separator. The shuffles determine which items will 
fall into which target output bucket. Then for every output bucket, as long as there is still room in the 
maximum  C  items  to  output,  the  items  from  input  bucket  are  read,  decrypted  and  deposited  to  the 
output bucket. If the bucket is full of C items, then the next items will go to the Stash and start waiting 
for the next target bucket. Finally, if some output bucket still not filled with C quota, then they are 
filled  with  dummy  variables.  After  all  input  buckets  has  been  processed,  the  Stash  may  leave  over 
with some items. These items are drained by filling extra K items per output buckets. At the end of 
this  phase,  K  is  set  to  be 
 where  S∗=  stash  space  and  B∗=  bucket  number.  Now,  all  the  items  in 
output buckets are re-encrypted, shuffled and dummy items are filtered out from every bucket. Each 
bucket drops 𝑑′’ item in this phase. After that, 𝐷”⁡ items are forwarded as output at a time. 

𝑆∗
𝐵∗

▪  The  phase  proceeds  in  a  sliding  window  w  buckets  of  intermediate  items.  At  the  time  of 
𝑟, 𝜇)  where  (g,h∗) is 
shuffling,  there  are  two  secret  shufflers.  The  encoder  computes (𝑔𝑟, ℎ∗
the key of shuffler 2 and r is a random value. Shuffler 1 is a blind shuffler that creates a secret 
 α′for each tuple where α′𝜖Ζ𝑝and compute (𝑔𝑟𝛼,

𝑟⁡.⁡⁡𝜇)𝛼′

, (ℎ∗

) 

𝑢

▪  After that, the shufflers 1 batch the data, shuffles and forward the blinded items to shuffler 2. 
Shuffler 2 uses its private key that is the secret x∗ such that h∗= gx∗ on input (u,v) to compute 
𝑥∗and recover µα0 = H(𝑐𝑟𝑜𝑤𝑑𝐼𝐷)𝛼′
It works with crowd ID that are already hashed and has 
𝑣
raised  to  a  secret  power𝛼′.  With  the  blind  crowd  ID,  shuffler  1  cannot  do  the  dictionary 
attack, since it does not know the secret key of the shuffler 2, whereas shuffler2 also cannot 
do  such  attack  as  it  has  not  any  idea  about  the  secret 𝛼′ of  shuffler  1.  So,  at  the  time  of 
shuffling, privacy is maintained. 

o  Thresholding- Even stripped and shuffled data may identify a client by uniqueness and to prevent this 

the shuffler do thresholding by generating crowdID. 

o  Batching- After that, the shuffler forwards the data to the analyser infrequently, in batches. 

• Step 3: Analyzing 

The analyzer decrypts, stores and aggregate those data comes from the shuffler. The analyzer’s output is 
considered public. Analyzer uses the PINQ, FLEX or the systems such as Airavat to maintain the final 
protection of user’s privacy. 

All the symbols and description of PROCHLO are given in the following table 8:  

 
 
 
 
 
Table 8: Symbols and Description of PROCHLO 

Symbols 

Description 

𝑆∗ 
𝑡∗ 
F 
𝐾𝑚 
 c 

𝑎𝑢𝑥𝑖 

N 
B* 
𝐷" 
C 
𝑆∗ 
K 
𝑑′ 
(u, v) 
𝐵+ 
𝐷+ 

Secret string. 
Number of secret shares. 
Field contains all the secret strings. 
Key. 
Cipher text. 
𝑖𝑡ℎ Number secret sharing among all t shares. 
Total number of user inputs. 
Number of  buckets 
Number of outputs each output bucket has. 
Maximum quota of each target bucket. 
Stash. 
Number of extra elements per output bucket. 
Number of items dropped by each output bucket. 
Input from users. 
Number of crowdID. 
Number of inputs having same crowdID. 

Experiments: PROCHLO  
It performs four experiments with Crowd, Secret Crowd, No-Crowd and Blinded-Crowd to privately learn word 
frequencies on sample of size 10K,100K and 1M. It is seen that, if data with No-Crowd are transferred to the 
pipeline, it will have no privacy and highest utility whereas if we pass the data with Crowd , then the privacy will 
be improved but not so much. By creating secret Crowd, this problem can be recover, but still the result is not 
satisfactory so much. all of this problem can be removed by applying blinded Crowd to the data which provides 
strong privacy guarantee with great utility. 

PROCHLO provides much better utility than RAPPOR while maintaining strong privacy guarantee. Here, data 
have less noise than RAPPOR reports, so the utility becomes higher. PROCHLO not only offers good balance 
between utility and privacy, it introduces both new cryptographic primitives and a new algorithm of oblivious 
shuffling.  PROCHLO  is  relatively  simple,  easy  to  understand  system  and  also  has  the  straightforward 
realization of the ESA structure that minimizes trust issues. 

Amplification by Shuffling 
A  important  task  in  data  analysis is  the  monitoring  of  the  statistical  properties of  the  data in  a  manner  that 
requires repeated computation the entire data set which is involved and that type of monitoring can directly or 
indirectly  harm  the  data  by  exposing  private  information  about  sensitive  attribute  of  user.  In  case  of 
centralized  differential  model,  the  response  should  be  stable  while  the  neighboring  data  set  differs  from  n 
rows. So, the centralized differential model provides better privacy than the LDP model. 

Inspired by differential privacy under continuous observation, Amplification  by Shuffling (Erlingsson et. al., 
2019) provides an algorithm that gives high accuracy online monitoring on users’ input in LDP model whose 
privacy cost is poly-logarithmic in number of changes in users’ input and it shows how LDP guarantees about 
privacy  protection  while  doing  online  monitoring  even  when  the  users  report  repeatedly,  over  multiple 
timesteps, and whether they report on the highly correlated value, on the same value or independently drawn 
values.  This  privacy  amplification  technique  shows  that  any  permutation-invariant  algorithm  satisfying 

1
differential privacy will satisfy O(𝜖√log (
𝛿

) , 𝑛 ) central differential privacy. 

 
 
 
 
 
Mathematical Model and Lower bound  
Let, we want to know the number of times a software is used by n users at the time horizon d and the input is 
taken repeated time at the time point t [d]. Consider a population of n users reporting a boolean value about 
their state at each time period t𝜖⁡[𝑑] where [d]= (d1, d2,.....) and d is the power of 2. Sti = {Sti[1], Sti[2], ......., 
Sti[d]} denotes the ith user’s state at the time point t𝜖⁡[𝑑]and they can change their input at most k times. Let, 
𝑥𝑖 ={ 𝑥𝑖[1], 𝑥𝑖[2],  ....., 𝑥𝑖[d]}  denotes  the  changes  by  the  ith  user  at  the  time  point  t𝜖⁡[𝑑].  Then,  Sti[d]  = 
∑
will  be  the  running  count  or  marginal  sum.  Now,  h  is  the  hth    level  of  the 
balanced  binary  tree  and Hi  be the  number  of  nodes  of  the  hth level  of the ith  user’s  tree  and  it is  denoted 
by𝐻(ℎ𝑖)𝑓𝑜𝑟ℎ𝜖[log2 d+1]. after that, k and c are initialised as k=0 and c=0 , d and k∗ is set up and the input xt 
is taken. Update𝑥𝑡⁡, 𝑡, 𝜖at each time step and modify k and c where the process is  differential private and𝑡 ≤
𝑑, 𝑥𝑡 ∈ {−1,0,1}. If 𝑥𝑡 ≠ 0, k= k+1, and when k reaches k∗, c takes the value of xt. Otherwise, if c=0 and t is 
divisible by 2h-1, u take random value from {-1,1}, else b takes 

 and   𝑓𝑡 = ⁡ ∑ 𝑥𝑖[𝑡]

𝑥𝑖[𝑙]

𝑛
𝑖=1

𝑙𝜖[𝑡]

value  from  2× [𝐵𝑒𝑟 (

) − 1] and  u  takes  (b  ×  c).  Now  for  response,  thealgorithm  creates  a𝑇𝑠𝑢𝑚 [ℎ,

𝜖
2

𝑒

𝜖
2

1+𝑒

[h,i]

𝑇𝑠𝑢𝑚

[ℎ, 𝑖].Here 𝑇[ℎ, 𝑖]is  the  sum  of  independent  random 

𝑡

2ℎ−1] = ⁡ ∑
variables that come from the range [-1, 1]. Now, 

) . 𝑘𝑙𝑜𝑔2𝑑.∑

𝑡 = (

𝑖,ℎ,𝑡=ℎ

𝑢𝑖,𝑡

𝑒

and𝑓̅

𝑒

𝜖
2+1
𝜖
2−1

∀[ℎ, 𝑖]:⁡|𝑇[ℎ′𝑖] − 𝐸[𝑇[ℎ, 𝑖]]| ≤ ⁡ 𝑐𝜖

√𝑛

𝑙𝑜𝑔

2𝑑
𝛽
𝑙𝑜𝑔2𝑑

, where, 𝑐𝜖 = (

𝜖
2+1
𝜖
2−1

𝑒

𝑒

) 

Byscaling with  𝑐𝜖

′ 𝑘𝑙𝑜𝑔2𝑑and multiplying by log2 d we get, 

∀𝑡𝜖[𝑑]: |𝑓𝑡 − ⁡ 𝑓̅

3
2√𝑛 log2
𝑡| ⁡ ≤ ⁡ 𝑐𝜖𝑘(log2 𝑑)

2𝑑
𝛽

(𝑖)  and 𝐷 = ⁡ 𝑥𝑖:𝑛are used where𝐴𝑙𝑑𝑝

(𝑖) : 𝑆(1)× 𝑆(2)× ........ × D → 𝑆(𝑖)be the input from ith user. 
Now, the algorithm𝐴𝑙𝑑𝑝
After taking input 𝑥1, 𝑥2,,......, 𝑥𝑛,  a random number I is chosen from [1:n] and then the first element of the data is 
swapped with the Ith element. Now, the local randomiser is operated with the data set. Let π be any permutation of 
the  set [1:n], then  π(D)  ←  {xπ(1),  xπ(2), .....,  xπ(n)}.  After  Local  Randomisation,  again  a shuffling  is done  where zi 
takes the value (z1:(i-1) , xi) and the generated report is {zπ(1), zπ(2), ....., zπ(n)}. 
Let, T is sampled from the distribution of I conditioned on 𝑍1:𝑛 =𝑆1:𝑛. Then, 

𝑃(𝑍1:𝑛=⁡𝑆1:𝑛|𝑇=𝑖)
𝑃(𝑍1:𝑛=⁡𝑆1:𝑛|𝑇=𝑗)

⁡ ≤ ⁡ 𝑒2.𝜖 

Now, we can write 

𝑃(𝑇 = 𝑖|𝑍1:𝑛 = ⁡ 𝑆1:𝑛) = ⁡

𝑃(𝑍1:𝑛 = ⁡ 𝑆1:𝑛|𝑇 = 𝑖). 𝑝(𝑇 = 𝑖)
𝑃(𝑍1:𝑛 = ⁡ 𝑆1:𝑛)|

⁡ ≤ ⁡

1
𝑛

𝑒2.𝜖 

Because  T  is  uniform  over  [n]  and𝑝 = ⁡ 𝑝′ = ⁡
composition theorem we get 

1
𝑛

.  Applying  privacy  amplification  by  shuffling  and  advanced 

𝑛𝜖(𝑒𝜖 − 1) ≤

65
64

𝑛(𝜖1)2 ≤

1
𝛿

2
3

𝜖0

√𝑙𝑜𝑔
𝑛

Where , 𝜖 < 𝜖1√2𝑛𝑙𝑜𝑔

1
𝛿

+ 𝑛𝜖1(𝑒𝜖 − 1) and 𝜖1 ≤

8𝜖
𝑛

⁡, 𝜖0 ≤ ⁡

1
2

 , 𝛿 is the small distance between two datasets. 

Each set  of 𝑛′users  for  which  the result is  applicable are still  be guaranteed  by  a  factor √𝑛′reduction in the 
centralised privacy model for the worst possible case. Here shuffling after the local randomisation technique 

satisfies(𝜖, 𝛿)differential  privacy  at  index  i  in  the  central  model  where 𝜖 = 12𝜖0√𝑙𝑜𝑔

1
𝛿
|𝑆|

 ; 𝑆 ⊆ |𝑛| such  that 

𝑖, 𝑗𝜖𝑆, 𝐴𝑙𝑑𝑝

(𝑗)  
(𝑖) = ⁡ 𝐴𝑙𝑑𝑝

 
 
 
 
 
 
 
 
 
 
 
 
Table 9. Symbols and Description for Amplification of Shuffling 

Symbols 

Description 

t 
d 
Sti 
𝑥𝑖 
𝑓𝑡 
𝑓̅
𝑡 
𝐻(ℎ𝑖) 
𝑇(ℎ, 𝑖) 
D 
𝜋 
𝜖 
𝜖1 
𝜖0 
𝑆(𝑖) 

(𝑖)  
𝐴𝑙𝑑𝑝

Time Point 
Time Horizon 
𝑖𝑡ℎ user’s state at the time point t 
Changes by 𝑖𝑡ℎ user. 
Running count 
Expected running count 
Number of nodes at the ℎ𝑡ℎ level of tree T 
sum of independent random variables that come from the range [-1, 1] 
Database 
Any permutation of the set [1: n] 
Privacy budget o the data set. 
Privacy budget of neighboring data set D’  
(𝑖)  
Privacy budget of  𝐴𝑙𝑑𝑝
(𝑖)  
Range space of 𝐴𝑙𝑑𝑝
ith local differentially private algorithm 

The  result  of  Amplification  by  Shuffling  is  encouraging,  and  it  secures  the  differential  privacy  in  central 
differential model and make the stable response for differing single input in two data sets. But this algorithm 
gives very poor response for longitudinal data. This formalisation assumes that the user population to be static 
which does not compared to real world. 

OUTIS 
This paper provides a framework that provides an accuracy guarantee just like CDP model (Chowdhury et. al., 
2019)  without  any  trusted  data  collector.  Here  crypt  employs  two  non-  colluding  semi-trusted  server  AS 
(Analytic  Server)  and  CSP  (Cryptographic  Service  Provider  )that  run  differential  privacy  on  the  encrypted 
data given by the data owner. 

Cryptographic Primitives: OUTIS 

In their paper they used four cryptography schemes. They are explained in the following –  

•  Linear  Homomorphic  Encryption  (LHE)  -  This  scheme  consists  of  three  stages  i.e.  Key  generation, 
Encryption  and  Decryption  on  the  basis  of  the  group (ℳ, +).  In  the  first  stage  a  user  tunable  security 
parameter K is being passed as input in the pipeline which outputs a pair of keys. Those are secret and 
public keys - (𝑆𝑘, 𝑃𝑘).  In the second stage, using the public key  𝑃𝑘 a randomized algorithm encrypts a 
message 𝑚 ∈ ℳ to 𝑐.  The  third  and  the  last  part  decrypt 𝑐 from  the  last  stage  using 𝑆𝑘 to  recover  the 
plaintext message 𝑚 deterministically. 

•  Labelled  Homomorphic  Encryption  (LabHE)  –  By  the  introduction  of  pseudo-random  function  every 
LHE scheme can be changed to LabHE. The LabHE scheme also has the ability to multiply two LabHE 
ciphers.  

•  They  have  used  two  more  primitives  too.  Those  are  “𝑜𝑝𝑒𝑟𝑎𝑡𝑜𝑟⁡⨁ “and  “Secure  computation  using 
garbled  circuit”. 𝑜𝑝𝑒𝑟𝑎𝑡𝑜𝑟⁡⨁⁡is  mainly  used  for  the  repetition  purpose.  In  the  later  scheme,  for  two 
private inputs from two parties, no party can learn more than 𝑓(𝑖𝑛𝑝𝑢𝑡1, 𝑖𝑛𝑝𝑢𝑡2)  for a function 𝑓. This is 
an inherent generator scheme using garbled input in every steps. 

Architecture: OUTIS 
The algorithm is given below- 

•  Setup Phase:  

The owner initializes the privacy budget 𝜖𝛽 for CSP and stores it in CSP’s privacy engine module. Then the 
CSP key manager produces a pair of keys(𝑆𝑘, 𝑃𝑘).  For labHE and publishes 𝑃𝑘 stores𝑆𝑘. 

 
 
 
 
 
•  Data Collection Phase: 

 In this phase, data owner encrypts and encodes his data with the help of data encryption technique and data 
encoder  and  then  sends  the  encrypted  and  encoded  data  to  As  and  after  that,  data  owner  gets  offline 
completely. As, aggregates the data into a singleencrypted database D by aggregator module. 

•  Program Execution Phase: 

 Here  AS  executes  data  analyst  provided  crypt  programs  that  accesses  sensitive  data  by  various 
transformation  operators  like  Cross  Product,  Project,  Filter,  Count,  Group  By  Count,  Group  By  Count 
Encoded,  Count  Distinct  etc  and  measurement  operators  like  Laplace,  Noisy  Max  etc  which  are  DP 
operations to create noisy answer. Measurement operators require interaction with CSP as they need to 
decrypt the data and also to check whether the privacy budget is exceeded or not. These functions can be 
done by the CSP’s data decryption and privacy budget modules. 

Table 10. Symbols and Descriptions for subsection OUTIS 

Symbol 

Description 

Privacy budget for cryptographic service provider 
Secret key 
Private key 
Cipher text 
Message to be encrypted 
Aggregated data 

𝜖𝛽 
𝑆𝑘 
𝑃𝑘 
c 
m 
𝐷̅ 

The first  two  phases  occur  exactly once  at the  beginning  and  every  subsequent  program  are  handled  by  the 
corresponding program execution phase. 

Experiments: OUTIS 
For  experiments  they  use  a  schema  database  <Age,  Gender,  Native  Country,  Race>  and  shows  7 
cryptographic  examples  over  it.  They  uses  the  crypt  program  p1  to  find  the  cumulative  distribution  of  age 
where the first step is to compute 100 ranges queries and the ith query computes the number of persons having 
the age 𝜖 [0,i] in 𝐷̅with privacy guarantee 𝜖𝑖 then they applied a sequence of transformation operators in each 
range  of  queries  and  all  of  these  operators  have  stability  bound  1.  For  this,  the  resultant  range  query  has  a 
sensitivity upper bound 1. In this way, the subsequent measurement operator Laplace takes the privacy budget 
𝜖𝑖and sensitivity ∆ = 1. After looping over 100 ranges, p1 gives a noisy plain text 𝑉̂= {𝑐1̂ ,𝑐2̂ ,....,𝑐100̂  } and at 
the end of the program the privacy budget is ∑

. 

100
𝑖=1

𝜖𝑖

ARA (Aggregated RAPPOR and Analysis for Central Differential Privacy) 
The main aim of ARA (Paul et. al., 2020) is trying to produce a bridge to keep best of both the worlds in terms 
of LDP and CDP, which is a software approach, comparatively less expensive, fast with less complexity in the 
analysis and overall method.  
The main contributions of ARA are following – 
•  The promises of DP are maintained. 
•  Fast enough analysis phase (Around 1.5 hours for 1000 users times 100 loops of analysis) 
•  Correct identification of the highest occurred true value every time with varying achievement. 
•  Simple probabilistic analysis method in comparison to OUTIS or PROCHLO. 

Methodology and Experiments: ARA  
ARA collects data after cloning the RAPPOR code from the Google repository of GitHub by running it 100 
times  to  make  their  own  database  using  the  generated  RAPPOR  reports.  It  only  uses  ten  true  values  for 
convenience where RAPPOR has used 100 true values.  

 
 
 
 
 
 
 
 
 
In the first step the sampling is done in 100, 1000, 10000, 20000, 25000 samples taking at a time randomly 
from the created dataset without any repetition. Then the TF-IDF value is calculated with the formula given in 
the table 11. The two achievements from this step are – 

•  No more than ⌈𝑘 2⁄ ⌉ + 1 positions can grab “on bit”s. Here, 𝑘 = 32. 
•  Constant values for 17 bit positions are calculated for the “on” bits. 

These values are now predefined for the next step and securely preserved by the trusted data curator. 

Table 11. The 17 constants value retrieved using continuous sampling on the RAPPOR reports 

Number of ‘on bit’ in the string 

Constant Value 𝓒𝒗(v ranged from 1 - 
17) 

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

13 

14 

15 

16 

17 

1.20201279 

1.0927389 

0.993399 

0.90309 

0.80618 

0.727 

0.660052 

0.60206 

0.550907 

0.50515 

0.4637573 

0.425969 

0.3912066 

0.3590219 

0.329059 

0.30103 

0.274701 

Source: Paul et. al., 2020 

In the second step, the weighted sum for the prr and irr strings is being calculated using the formulas given in 
the table. These sums are stored for a minimum time for the purpose of experiment . 
The last step is the testing phase, where the RAPPOR reports are being drawn again randomly from the testing 
portion of the created database and the weighted sum is being calculated and matched against the preserved 
results in the second step. The formulas of each steps can be consulted from the original ARA (Paul et. al., 
2020) paper and the description of the terms used in those formulas are given in the following table 12. 

Table 12. Symbols and Descriptions for ARA 

Symbol 

𝑇𝐹(𝑡", 𝑑∗) 
t” 
d* 
𝑓𝑡",𝑑∗ 
𝐼𝐷𝐹(𝑡", 𝑑∗) 
1 + |𝑑∗ϵ𝐷∗ ∶ ⁡ 𝑡"𝜖𝑑∗| 

Description 

Term Frequency 

Term 
Document in which the term t” has occurred 
Frequency of occurrence t” in d* 
Inverse document frequency 
Total number of documents in the corpus 

 
 
 
 
 
 
 
N 
W 
𝐶𝑝𝑟𝑟 
𝐶𝑖𝑟𝑟 
V 
𝐶𝐶𝑝𝑟𝑟 
𝐶𝐶𝑖𝑟𝑟 

Number of documents where t” has appeared 
Weighted sum 
The count of ‘on bit’ in the prr string 
The count of ‘on bit’ in the irr string 
Cohort value 
Value taken from the 𝒞𝑣 in the Table 11 
Value taken from the 𝒞𝑣 in the Table 11 

The main drawbacks of ARA are that the accuracy is not more than 52.28% on an average and the detection 
ability of the second major true value is poor. 

Encode, Shuffle, Analyze, Privacy Revisited: Formalization and Empirical Evaluation 
In  this  paper  (Erlingsson et.  al.,  2020)  they tried  to  bridge  all  their  previous  works starting  from  RAPPOR, 
PROCHLO – an implementation of ESA (encode, shuffle, analysis) till  Amplification by shuffling, with the 
aim  to  give  a  proper  algorithm  that  works  equally  good  in  both  the  LDP  and  CDP  environment  with  an 
extensive  experimental  results  using  MNIST  and  CIFER-10  datasets.  The  key  techniques  they  talked  about 
here are the following -   

•  Anonymity 
•  Reporting with the preservation of privacy 
•  Sketch based encryption and its limitation 
•  Cons of fragmentation 
•  Cons of shuffling 

Background Theorem 
In the proposed algorithm here, data are collected from the users first and then they are encoded by one of the 
techniques such as one-hot encoding, attribute fragmentation, report fragmentation and sketch-based encoding 
and  all  of  these  schemes  produce  reports  with  localized  differential  privacy  guarantee.  Each  and  every 
encoding system have their own pros and cons too. After completing encoding, the LDP reports then are being 
shuffled by k-shuffler to aggregate in the next step. After that the algorithm of amplification by shuffling is 
applied here and the final reports are generated which hold centralized and local differential privacy guarantee 
equally. The parts of this algorithm is discussed below -  

•  One-Hot Encoding 

The  type  of  encoding  has  strong  impact  on  utility  of  the  differentially  private  algorithm.  Let  D  be 
dictionary of elements and id D is not too large, then a data input x can be encoded by one-hot encoding 
where each data record x will hold an element in D.But when the D is large enough, it is not a suitable 
idea to apply one-hot encoding whereas sketching algorithm can be used. 

•  Sketch-Based Reports 

The main idea of this scheme is to reduce a given domain {0,1}𝐾′
, 𝑘′ < 𝐾′via hashing and then 
use  locally  private  protocols  to  operate  over  the  domain  of  size 𝑘′.  To  avoid  significant  loss  due  to 
hashing, it is performed by multiple independent hash function. But it is observed that sketching is not a 
requirement for practical deployment in regimes with local differential privacy. 

to {0,1}𝑘′

•  Attributes Fragments 

In this case, each data record x is encoded as a binary vector with k or fewer bit set and each k vector 
coordinate is called attributes where x = Σxi, xi is one-hot encoded vector. Now, either each xi splitting 
privacy budget accordingly go through the  randomizer R∗ or one of the xi is drawn as the sample and 
spend the entire privacy budget to send is to R∗ which is a randomly chosen randomizer. There are two 

 
 
 
 
 
types of local randomizer, one holds the algorithm for replacement privacy guarantee and another holds 
the algorithm for removal privacy guarantee. 

o  Replacement LDP 

An algorithm R*:D → S is a replacement (𝜖, 𝛿) deferentially private local randomiser if for 
all S ⊆ S and for all 𝑥, 𝑥′𝜖𝐷: 𝑃[𝑅∗(𝑥)𝜖𝑆] ≤ 𝑒𝜖𝑃[𝑅∗(𝑥′)𝜖𝑆] + ⁡𝛿 

o  Generalized Removal (𝜖, 𝛿) DP 

A  randomized  algorithm  M:𝐷𝑛 →  S  is  a  replacement  satisfies  removal (𝜖, 𝛿)-  differential 
privacy if there exist an algorithm 𝑀′: 𝐷𝑛 × 2[𝑛] ⟶ 𝑆with the following properties: 

▪ 
▪ 
▪ 

for all 𝐷𝜖𝐷𝑛, 𝑀′(𝐷, [𝑛]) is identical to M(D) 
for all 𝐷𝜖𝐷𝑛and 𝐼𝜖[𝑛], 𝑀′(𝐷, 𝐼)⁡⁡depends only on elements D with indices I. 
for all S⊆ S,𝐷𝜖𝐷𝑛and I, I’⊆ [n]where we have that |I∆I’|=1, 
P [ 𝑀′(𝐷, 𝐼)⁡⁡𝜖S] ≤ ⁡ 𝑒𝜖𝑃[⁡𝑀′(𝐷′, 𝐼′)⁡⁡𝜖𝑆]⁡+ δ 

o  Removal LDP 

An algorithm R*:D → S is a removal (𝜖, 𝛿) differentially private local randomiser if exist a 
random  variable  R0  such  that  for  all  S  ⊆  S  and  for  all 𝑥𝜖 D  , 𝑒−𝜖 P[ 𝑀′(𝐷, 𝐼)⁡⁡𝜖 S] ≤
⁡𝑒𝜖𝑃[⁡𝑀′(𝐷′, 𝐼′)⁡⁡𝜖𝑆]⁡+ δ 

After  sampling  each  z  from{𝑥𝑖 ∶ 𝑖𝜖⁡[𝑘]} ,  it  is  seen  that  sending  each  attribute  of  z  independently  to  LDP 
randomiser that produce anonymous reports, is advantageous. 

•  Report Fragments 

Here,  a  sequence  of  LDP  reports  which  is  generated  by  multiple  independent  applications  of  the 
randomizer R∗ to x has come and each such report is called report fragment containing less information 
than the entire LDP report sequence. Thus, it improves privacy guarantee more. 

After completing the encoding parts, the fragments are sent to the shuffler. There are k secret shufflers. Each 
fragment chooses one shuffler randomly and each attribute go to the separate channel of the selected shuffler. 
Then the secret shuffling is done and shuffle data came where one cannot guess which data fragments come 
from  which  shuffler.  In  this  way,  the  data  remains  deferentially  private.  After  shuffling  the  aggregate  is 
calculated  and  at  last  amplification  by  shuffling  technique  is  applied  to  make  the  whole  process  centrally 
deferentially private. 

USEFUL BACKGROUND ON DIFFERENTIAL PRIVACY 
To understand the fundamental theory of DP one need to goes through proper steps. These steps are following: 

•  Understanding the background probability theorems to define DP 

This  step  consists  of  learning  about  probability  simplex,  randomized  algorithm,  distance 
between adjacent databases, definition of differential privacy derived from the afore mentioned 
concepts  and  finally  the  intuition  of  post  processing  through  the  definitions  and  proposition 
mentioned. All of these are explained in the Appendix 1.  

•  Understanding the tools to build the DP algorithm 

This  step  provides  the  definition,  remarks,  theorems  and  claims  about  probabilistic  tools  like 
additive  Chernoff  bound,  multiplicative  Chernoff  bound,  Azuma’s  inequality,  Stirling’s 
approximation. It also helps to understand how to introduce noise at the time of data collection 
or  structured  surveys  by  enlightening  us  on  the  concepts  of  Randomized  response,  Laplace 
mechanism, Exponential mechanism etc. All of these are explained in the Appendix 2.  

 
 
 
 
 
 
 
 
 
 
 
•  Understanding the way to use those tools to build a DP algorithm 

The final step helps us to actually build a DP algorithm for real purpose by letting us understand 
about  the  Composition  theorem,  its  technicalities,  definition,  advancement,  Sparse  vector 
technique, its algorithm, theorems, definition etc. All of these are explained in the Appendix 3. 

To understand if a proposed DP  algorithm is good or bad, we need to calculate its accuracy and utility 
(Alvim  et.  al.,  2011).  The  flow  of  utility  and  accuracy  inside  the  DP  system  is  given  in  the  following 
figure 3. The definitions are explained in Appendix 3. 

Figure 3. Leakage and utility workflow using differential privacy 

Source: Alvim et. al., 2011 

CONCLUSION AND FUTURE SCOPE 
Industries are becoming heavily dependent and invested in data. In a lot of situations, user experience can be 
improved based on the meta-data we learn from what other users are doing. Few examples are trending words, 
relevant  suggestions,  battery  hungry  websites,  popular  emojis.  However,  answering  these  questions  needs 
personal data of the users. 
Differential  privacy  technique  helps  us  comprehend  what  users  are  doing,  while  protecting  the  privacy  of 
individual users. It helps us learn about the user groups without learning about the members of the group. It 
makes sure that data cannot be reproduced even by the organization that is collecting the data. It achieves so 
with the idea that a biased statistical noise is enough to disguise a user’s data and when aggregated over large 
users, can still give us insight on the community. With increase in cyber-attacks as well as data harvesting; 
differential privacy is the only ethical way of data collection. 
In this chapter, we discuss differential privacy with its history and origin as well as the current state of the art 
methods. We discuss the architecture or the model of the various algorithms adopting the differential privacy 
technique  along  with  the  required  theorems,  proofs  and  experiments  to  justify  their  privacy  and  utility.  We 
believe  this  chapter  will  communicate  the  audience  about  the  current  status  of  this  research  area  as  well  as 
engage the readers with exploring research directions in the area of differential privacy. 

ACKNOWLEDGEMENT 
This research was supported by Dept. of Science and Technology (Govt. of India). 
 Grant no:NRDMS/UG/S.Mishra/Odisha/E-01/2018  

REFERENCE 

Davenport, T. H., &Prusak, L. (1998). Working knowledge: How organizations manage what they know. Harvard Business 
Press. 
Larson,  S.  (2017,  February  2).  Facebook  loses  $500  million  Oculus  lawsuit.  Retrieved  March  11,  2020,  from 
https://money.cnn.com/2017/02/01/technology/zenimax-oculus-lawsuit-500-million/ 
Reuters. (2017, March 16). To Avoid a Trial, Uber to Push for Arbitration in Waymo Lawsuit. Retrieved March 11, 2020, 
from https://fortune.com/2017/03/16/uber-arbitration-waymo/ 

 
 
 
 
 
 
 
 
 
2018  Data  Breaches  -  The  Worst  Breaches:  IdentityForce®.  (2020,  February  20).  Retrieved  March  11,  2020,  from 
https://www.identityforce.com/blog/2018-data-breaches 
Armerding, T. (2018, December 20). The 18 biggest data breaches of the 21st century. Retrieved March 11, 2020, from 
https://www.csoonline.com/article/2130877/the-biggest-data-breaches-of-the-21st-century.html 
Dell  end-user  security  survey-2017.  (2017).  Https://Www.dell.com/Learn/Us/En/84/learn_docs/Dell-End-User-Security-
Survey-2017.Pdf.  Retrieved 
from  https://i.dell.com/sites/csdocuments/Learn_Docs/en/dell-end-user-security-survey-
2017.pdf 
Differential privacy. (2020, March 8). Retrieved March 11, 2020, from https://en.wikipedia.org/wiki/Differential_privacy 
Cynthia, D. (2006). Differential privacy. Automata, languages and programming, 1-12. 
Dwork,  C.,  &  Roth,  A.  (2014).  The  algorithmic  foundations  of  differential  privacy.  Foundations  and  Trends®  in 
Theoretical Computer Science, 9(3–4), 211-407. 
Warner,  S.  L.  (1965).  Randomized  response:  A  survey  technique  for  eliminating  evasive  answer  bias.  Journal  of  the 
American Statistical Association, 60(309), 63-69. 
Kasiviswanathan,  S.  P.,  Lee, H.  K.,  Nissim,  K.,  Raskhodnikova,  S.,  &  Smith,  A. (2011).  What  can  we  learn privately?. 
SIAM Journal on Computing, 40(3), 793-826. 
Erlingsson, Ú., Pihur, V., &Korolova, A. (2014, November). Rappor: Randomized aggregatable privacy-preserving ordinal 
response.  In  Proceedings  of  the  2014  ACM  SIGSAC  conference  on  computer  and  communications  security  (pp.  1054-
1067). 
Bittau, A., Erlingsson, U., Maniatis, P., Mironov, I., Raghunathan, A., Lie, D., ... &Seefeld, B. (2017, October). Prochlo: 
Strong  privacy  for  analytics  in  the  crowd.  In  Proceedings of  the  26th  Symposium  on  Operating  Systems  Principles  (pp. 
441-459). 
Erlingsson, Ú., Feldman, V., Mironov, I., Raghunathan, A., Talwar, K., &Thakurta, A. (2019). Amplification by shuffling: 
From local to central differential privacy via anonymity. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on 
Discrete Algorithms (pp. 2468-2479). Society for Industrial and Applied Mathematics. 
Chowdhury, A. R., Wang, C., He, X., Machanavajjhala, A., & Jha, S. (2019). Outis: crypto-assisted differential privacy on 
untrusted servers. arXiv preprint arXiv:1902.07756. 
Paul,  S.,  &  Mishra,  S.  (2020).  ARA:  Aggregated  RAPPOR  and  Analysis  for  Centralized  Differential  Privacy.  SN 
Computer Science, 1(1), 22. 
Erlingsson, Ú., Feldman, V., Mironov, I., Raghunathan, A., Song, S., Talwar, K., &Thakurta,  A. (2020). Encode, Shuffle, 
Analyze Privacy Revisited: Formalizations and Empirical Evaluation. arXiv preprint arXiv:2001.03618. 
Alvim, M. S., Andrés, M. E., Chatzikokolakis, K., Degano, P., &Palamidessi, C. (2011, September). Differential privacy: 
on  the  trade-off  between  utility  and  information  leakage.  In  International  Workshop  on Formal  Aspects  in  Security  and 
Trust (pp. 39-54). Springer, Berlin, Heidelberg. 
McMahan,  H.  B.,  Andrew,  G.,  Erlingsson,  U.,  Chien,  S.,  Mironov,  I.,  Papernot,  N.,  &  Kairouz,  P.  (2018).  A  general 
approach to adding differential privacy to iterative training procedures. arXiv preprint arXiv:1812.06210. 
Hanzely, F., Konečný, J., Loizou, N., Richtarik, P., & Grishchenko, D. (2019). A privacy preserving randomized gossip 
algorithm via controlled noise insertion. arXiv preprint arXiv:1901.09367. 
Wang, T., Chen, J. Q., Zhang, Z., Su, D., Cheng, Y., Li, Z., ... & Jha, S. (2020). Continuous Release of Data Streams under 
both Centralized and Local Differential Privacy. arXiv preprint arXiv:2005.11753. 
Sengupta P., Paul S. and Mishra S. (2020). BUDS: Balancing Utility and Differential Privacy by Shuffling. arXiv preprint 
arXiv:2006.04125. 
Chaudhuri,  K.,  Imola,  J.,  &  Machanavajjhala,  A.  (2019).  Capacity  bounded  differential  privacy.  In Advances  in  Neural 
Information Processing Systems (pp. 3469-3478). 
Dwork,  C.,  Naor,  M.,  Pitassi,  T.,  &  Rothblum,  G.  N.  (2010,  June).  Differential  privacy  under  continual  observation. 
In Proceedings of the forty-second ACM symposium on Theory of computing (pp. 715-724). 

ADDITIONAL READING: 

This Section contains some of the current developments on Differential Privacy.  They are following: 

•  Continuous Release of Data Stream Under Both Centralized and Local Differential Privacy: 
In their work (Wang et al.,2019) propose an exponential mechanism with quality function, along with two 
algorithms ToPS (threshold optimizer, perturber and smoother) and ToPL ( ToPS applied in LDP) which 
provides the solution regarding the publishing of a stream of real valued data satisfying DP. ToPL is the 
first  LDP  algorithm  for  streaming  data  and  it  is  the  hierarchical  version  of  traditional  PAK  algorithm 
(Dwork  et al.,  2010).  ToPL  is a method designed  with the framework of  ToPS  for  outputting  streaming 
data in LDP. Here, as user perturbs their values before sending to the server, there is no need to trust the 
server. Threshold optimizer uses frequency estimation instead of applying exponential mechanism (as user 
has  local  view)  which  is  less  accurate  than  EM  (exponential  mechanism).  After  achieving  optimal 
threshold parameter, the user report is first truncated then the consistent noise is applied. As the reports are 
unbiased themselves, to answer a range of queries, this algorithm doesn't use the smoother here. But they 

 
 
 
use  the  PAK  setting  in  threshold  optimizer  assuming  that  the  distribution  stays  same  with  further 
information like data changes slowly or regularly, which is a drawback. 

•  A Privacy Preserving Randomized Gossip Algorithm via Controlled Noise Insertion: 

This  work  (Hanzely  et  al.,  2019)  introduces  a  duel  algorithm  based  on  average  consensus  (AC)  that 
provides iteration  complexity  bound  and  performs  extensive  numerical  experiments  while  protecting  the 
information  regarding  initial  value stored  in  the  node.  For  a  particular  graph structure, at  first the  initial 
value is provided after adding noise to the system for propagating across the network, but in the following 
iteration , the previously added noise is removed and the new noise with smaller magnitude is applied to 
ensure the convergence to the true average that maximise the utility of the system obviously.  
•  A General Approach to Adding Differential Privacy to Iterative Training Procedures: 

This  work  (McMahan  et  al.,  2018)  introduces  an  algorithm  with  the  modular  approach  to  minimize  the 
changes on training algorithm by various configuration strategies of privacy mechanism which is applied 
to heterogeneous set of vectors (like gradients of different layers of DNN, matrices, batch normalization 
parameters  with  the  different  properties).  The  algorithm  provides  an  integrated  privacy  mechanism  that 
differs in granular privacy guarantee offered and the method of privacy accounting, where, after choosing 
the  subset  from  the  training data  set  with  a  probability  parameter,  using  statistical queries  -  the  privacy 
mechanism containing noises is applied to each collected aggregates and the accounting procedure is used 
to compute a final (ε, δ)- differential privacy guarantee with a good balance between privacy and utility. 
The major drawbacks are, no utility bound is given to justify the good balance between privacy and utility, 
the distribution is defined over a finite domain, the source of randomness is assumed to be guaranteed and 
computationally secured and lastly they leave open the task of developing a provable floating point 
implementation using Laplace mechanism on DP-SGD and iterating it into a ML Library. 

•  Capacity Bounded Differential Privacy: 

This paper (Chaudhuri et al.,2019) introduces the capacity bounded differential privacy where the 
adversary that distinguishes the output distribution is assumed to be not bounded in computational power. 
This work models adversaries using restricted f divergence between the probability distribution and study 
various properties of the definition and algorithm that satisfy them. They also show that the Laplace and 
Gaussian mechanisms are also giving better privacy by applying this new algorithm. 

•  BUDS: Balancing Utility and Differential Privacy:  

This work (Poushali Sengupta et al.,2020) introduces a mechanism along with Iterative Shuffling that 
ensures a good balance between utility and privacy guarantee of the dataset. The whole mechanism can be 
decomposed into two parts: Application of Query analysis and Iterative Shuffling (IS). The main drawback 
of this work is: one hot encoding is used for encoding which is a problem for big dimensional data set. 
Also proper architecture of Query Function is not provided here. 

Key terms and Definitions: 

This  section  is  divided  into  four  parts.  The  first  part  contains  the  key  terms  and  the  following  three  parts 
contains the necessary definitions and theorems for the learning of differential privacy. 

Key terms: 
PAC Learning, SQ Learning, Private PAC Learning, Private SQ Learning, Randomized Response, Differential 
Privacy, RAPPOR, ESA, ARA, BUDS 

Definitions and Theorems for the first step: 

In  the  second  part  we  will  discuss  some  useful  definitions  and  background  theorems  that  work  for  the  first 
learning step of DP discussed in the useful background on differential privacy section in alphabetical order as 
instructed by the publisher (Dwork et. al., 2014). 

Differential Privacy  
A randomized algorithm ℳ with domain ℕ|𝒳|𝑖𝑠⁡(𝜖, 𝛿) - deferentially private if for all S ⊆ Range(ℳ) and for all 
𝑥, 𝑦𝜖ℕ|𝒳| such that ||𝑥 − 𝑦||1 ≤ 1 will follow: 

𝑃(ℳ(𝑥)𝜖𝑆) ≤ exp(𝜖) 𝑃(ℳ(𝑦)𝜖𝑆) + 𝛿 

 
 
 
 
where  the  probability  space  is  over  the  coin  flips  of  the  mechanism ℳ .  If  δ  =  0,  we  say  that⁡ℳ⁡𝑖𝑠𝜖 - 
deferentially private. 

Distance between two adjacent databases 
The ℓ1norm of a database 𝑥 is denoted by ||𝑥||1 and is defined to be  

|𝜒|

||𝑥||1 = ⁡ ∑ |𝑥|𝑖

𝑖=1

The ℓ1 distance  between  two  databases 𝑥𝑎𝑛𝑑𝑦is ||𝑥 − 𝑦||1 where  the  later  term  denotes  the  measure  of  how 
many records differ between 𝑥𝑎𝑛𝑑𝑦. 

Explanation of Different DP 

The values of δ that are less than the inverse of any polynomial in the size of the database are preferable. The 
values of δ which are O(1/ ||x||1) are dangerous as it allows to show all private information of individuals from a 
very small portion of the database like ”just a few” case that we are already discussed in previous subsection. If δ 
is negligible, there are differences between (𝜖,0) and (𝜖, 𝛿)differential privacy. (𝜖,0)-differential privacy says that, 
for  every  run  of  the mechanism ℳ(𝑥),  the  output  observed  is  (almost) equally  likely  to  be  observed  on  every 
neighboring  database,  simultaneously.  Whereas,  (𝜖,δ)-differential  privacy  provide  us  a  mechanism  where,  for 
every  pair  of  neighboring  databases  x,  y,  it  is  extremely  unlikely  that,  ‘ex  post  facto’  the  observed  value 
ℳ(𝑥)will be much more or much less likely to be generated when the database is x than when the database is y. 

Intuition for Post-processing 
For an output 𝒵∼ℳ(𝑥), a database y is may be possible to find such that 𝒵 is much more likely to be produced on 
y than it is produced when in database x. Then, privacy loss can be calculated which is given below: 

𝒵

ℒℳ(𝑥)||ℳ(𝑦)

=

𝑃(ℳ(𝑥)=𝒵)
𝑃(ℳ(𝑦)=𝒵)

This loss can be negative or positive. 

Probability Simplex   
Given a discrete set ℬ, the probability simplex over ℬ, denoted ∆(⁡ℬ) is defined to be: 

∆(ℬ) = {𝑥𝜖ℝ|ℬ| ∶ ⁡ 𝑥𝑖 ≥ 0⁡∀𝑖𝑎𝑛𝑑 ∑ 𝑥𝑖 = 1

|ℬ|
𝑖=1

} 

Randomized Algorithm  
A  randomized  algorithm ℳ with  domain 𝒜 and  discrete  range ℬ is  associated  with  a  mapping ℳ : 𝒜 →  ∆ 
(⁡ℬ).  On  input  a𝜖𝒜,  the  algorithm ℳ  outputs ℳ (a)  = 𝑏with  probability   (ℳ⁡(𝑎))𝑏  for  each  b𝜖⁡ℬ .  The 
probability space is over the coin flips of the algorithmℳ. 

Theorem 

Any (𝜖,δ)-deferentially private mechanism ℳis (k𝜖 ,δ) differentially private for groups of size k. That is, for 
all ||x − y||1 ≤ k and allS ⊆Rangeℳ] 

where the probability space is over the coin flips of the mechanism ℳ. 

𝑃[ℳ(𝑥)𝜖𝑆] ≤ exp⁡(𝑘𝜖)𝑃[ℳ(𝑦)𝜖𝑆] 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Definitions and Theorems for the second step: 

Here we will discuss some useful definitions and background theorems that work for the second learning step 
of DP discussed in the useful background on differential privacy section in alphabetical order as instructed by 
the publisher (Dwork et. al., 2014). 

Theorems 

Additive Chernoff’s bound 
Let 𝒳1, …⁡⁡, 𝒳𝑚 be  independent  random  variables  bounded  such  that 0 ≤ 𝒳𝑖 ≤ 1 for  all  i.  Let 𝑆 = ⁡
denote their mean and let 𝜇 = ⁡𝔼[𝑆] denote their expected mean. Then: 

1
𝑚

𝑚
∑ 𝒳𝑖
𝑖=1

𝑃[𝑆 > 𝜇 + 𝜀] ≤ ⁡ ℯ−2𝑚𝜀2
𝑃[𝑆 < 𝜇 − 𝜀] ≤ ⁡ ℯ−2𝑚𝜀2

Azuma’s Inequality 
Let  f  be  a  function  of  m  random  variables 𝒳1, …⁡⁡, 𝒳𝑚 each 𝒳𝑖 taking  values  from  a  set 𝐴𝑖  such  that 𝔼[𝑓] is 
bounded. Let 𝑐𝑖 denote the maximum effect of 𝒳𝑖 on f , then for all 𝑎𝑖, 𝑎𝑖́ ⁡𝜖⁡𝐴𝑖 :  

|𝔼[𝑓⁡|⁡𝒳1, … , 𝒳𝑖−1⁡⁡, 𝒳𝑖 = ⁡ 𝑎𝑖] − ⁡𝔼[𝑓⁡|⁡𝒳1, … , 𝒳𝑖−1⁡⁡, 𝒳𝑖 = ⁡ 𝑎𝑖́ ]| ≤ ⁡ 𝑐𝑖 

Then: 

𝑃[𝑓(𝒳1, …⁡⁡, 𝒳𝑚) ≥ ⁡𝔼[𝑓] + 𝑡] ≤ ⁡ ℯ

−(

2𝑡2
2𝑚
𝑐𝑖
𝑖=1

∑

)

Exponential Mechanism  

The  exponential  mechanism ℳ𝐸(𝑥, 𝑢, ℛ) selects  and  outputs  an  element  r𝜖ℛ⁡with  probability  proportional  to 
exp(

). Where, sensitivity of the utility score u: ℕ|𝒳| × ℛ → ℝ and   

𝜖𝑢(𝑥,𝑟)
2∆𝑢

∆𝑢 = max
𝑟∈ℛ

max
𝑥,𝑦∈||𝑥−𝑦||≤1

||𝑢(𝑥, 𝑟) − 𝑢(𝑦, 𝑟)|| 

Claim 

The Exponential Mechanism holds (𝜖,0) differential privacy. 

Gaussian Mechanism  
 Just like Laplace Mechanism, it adds noise drawn from the Gaussian distribution whose variance is calibrated 
according to the sensitivity and privacy parameters. 

ℳ𝐺𝑎𝑢𝑠𝑠(𝑥, 𝑓, 𝜖, 𝛿) = 𝑓(𝑥) + 𝒩 𝑑 (𝜇 = 0, 𝜎2 =

Claim 
ℳ𝐺𝑎𝑢𝑠𝑠 holds (𝜖,δ) differential privacy 

2 ln (

1.25
𝛿

) . (∆2𝑓)2
𝜖2

) 

Multiplicative Chernoff’s bound 
Here the background assumption is same as the additive Chernoff’s bound. The bound is following: 

𝑃[𝑆 > 𝜇(1 + 𝜀)] ≤ ⁡ ℯ−2𝑚𝜀2/3 
𝑃[𝑆 < 𝜇(1 − 𝜀)] ≤ ⁡ ℯ−2𝑚𝜀2/2 

Noisy Max Report  

 
 
 
 
 
 
 
 
 
 
 
 
 
 
If we consider a simple algorithm where it determines which m counting queries has highest value, and then it 
adds generated Laplace Noise from Lap(1/𝜖) to each count and return the index of the largest noisy count. We 
can ignore the possibilities of tie. This is called Noisy Max Report. 

Claim 

The Report Noisy Max algorithm is (𝜖,0)-differentially private. 

Report One-Sided Noisy Arg-Max 

When  run  with  parameter  𝜖/2𝛿𝑢  yields  the  same  distribution  on  outputs  as  the  exponential 
mechanism. 

Randomized Response 
It’s a simple survey process where binary responses are considered i.e. the answers should be in ‘yes’ and ‘no’ 
only. The steps are following: 

•  Toss a coin 
• 
• 

If the coin shows ‘tails’, then give the ‘true’ answer 
If the coin shows ‘heads’, then again toss it and give ‘yes’ if the result is ‘heads’ or ‘no’ if the result is 
‘tails’. 

For (𝜖,δ) differentially private mechanism, to find the value of 𝜖, we obtain:        

RR = 

𝑃[𝑅𝑒𝑠𝑝𝑜𝑛𝑠𝑒=YES|𝑇𝑟𝑢𝑡ℎ="𝑌𝐸𝑆"]
𝑃[𝑅𝑒𝑠𝑝𝑜𝑛𝑠𝑒="𝑌𝐸𝑆"|𝑇𝑟𝑢𝑡ℎ="𝑁𝑂"]

where, 𝜖 = ln⁡(𝑅𝑅). 

Claim 

Randomized Response Technique holds (,0) differential privacy where 𝜖 = ln⁡(3). For more analysis, 
remarks and corollary please refer to (Dwork et. al., 2014). 

Stirling’s Approximation 
𝑛
𝑛! Can be approximated by √2𝜋𝑛(
𝑒

)𝑛. 

The Laplace Mechanism 
Given any function f: ℕ|𝒳| → ℝ𝑘, the Laplace mechanism is defined as: 

ℳ𝐿(𝑥, 𝑓(. ), 𝜖) = 𝑓(𝑥) + (𝑌1, 𝑌2, … … , 𝑌𝑘) 

Where Yi ;i = 1:k, are i.i.d random variables drawn from Lap(∆𝑓/𝜖). 

Definitions and Theorems for the third step: 

Here we will discuss some useful definitions and background theorems that work for the third learning step of 
DP discussed in the useful background on differential privacy section in alphabetical order as instructed by the 
publisher (Dwork et. al., 2014). 

Theorems 

Accuracy  
We will say that an algorithm which outputs a stream of answers a1 , . . . , 𝜖(⊺, ⊥)∗ in response to a stream of k 
queries  f1  ,...,  fk  is  (α,β)-accurate  with  respect  to  a  threshold  ⊥  if  except  with  probability  at  most  β,  the 
algorithm does not halt before fk , and  
∀ai = ⊺: 

and, ∀ai = ⊥: 

fi(D) ≥ T − α. 

 
 
 
 
 
 
 
 
 
fi(D) ≤ T + α 

Advanced Composition  
If 𝒜1, … . , 𝒜𝑘are randomised algorithm satisfying (,δ) differential privacy, then their composition, defined as 
𝒜1(𝐷), … … , 𝒜1(𝐷), 𝑓𝑜𝑟𝐷𝜖𝒟 satisfies (𝜖′, 𝑘𝛿 + 𝛿′) differential privacy where𝜖′ = ⁡𝜖√2𝑘𝑙𝑜𝑔(1/𝛿′) +
𝑘𝜖(exp(𝜖) − 1). Moreover 𝒜i can be chosen adaptivelydepending on the outputs of 𝒜1, … . , 𝒜𝑘. 
Let, two distribution are given µ and 𝜇′, then we can say that they are (𝜖, 𝛿)- deferentially close, denoted by 
𝜇 ≅(𝜖,𝛿) 𝜇′, if for all measurable 𝒜, we have 

exp⁡(−𝜖)(𝜇′(𝒜 − 𝛿)) ≤ 𝒜 ≤ exp⁡(𝜖)(𝜇′(𝒜 + 𝛿)) 

Composition 
Let ℳ𝑖: ℕ|X| → ℛ𝑖 be an (𝜖𝑖, 𝛿𝑖) differentially private algorithm for i 𝜖 [k]. Then if⁡ℳ[𝑘] ∶ ⁡ ℕ|𝒳| → ∏ ℛ𝑖
defined 
𝑘
(∑ 𝜖𝑖
𝑖=1

tobe  ℳ[𝑘](𝑥)
𝑘
, ∑ 𝛿𝑖
𝑖=1

𝑘
𝑖=1
then  ℳ[𝑘](𝑥)

 ,…………..., ℳ𝑘(𝑥)

) −differentially private. 

( ℳ1(𝑥), ℳ2(𝑥)

), 

= 

 is 
is 

Corollary 1  
Let ℳ𝑖 : ℕ|X|  →ℛi  be  an  (𝜖𝑖, 0)  differentially  private  algorithm  for  i 𝜖 [k].  Then  if ⁡ℳ[𝑘] ∶ ⁡ ℕ|𝒳| →
𝑘
∏ ℛ𝑖
 is  defined  to  be  ℳ[𝑘](𝑥) =  ( ℳ1(𝑥), ℳ2(𝑥)  ,……………,  ℳ𝑘(𝑥) ),  then  ℳ[𝑘](𝑥) is 
𝑖=1
𝑘
(∑ 𝜖𝑖
, 0) − differentially private. 
𝑖=1

Properties of Differential Privacy 
The notion of (𝜖,δ)differential private satisfies the following properties: 

•  Monotonicity:Let. 𝜇 ≅(𝜖,𝛿) 𝜇′Then for𝜖′ > 𝜖and𝛿′ > 𝛿, 𝜇 ≅(𝜖′,𝛿′) 𝜇′. 
•  Triangle Inequality : Let, 𝜇1 ≅(𝜖1,𝛿1) 𝜇2and  𝜇2 ≅(𝜖2,𝛿2) 𝜇3, then𝜇1 ≅(𝜖1+𝜖2,𝛿1+𝛿2) 𝜇3 
′  and, 𝜇2 ≅(𝜖,𝛿) 𝜇2
•  Quasi-Convexity:Let,𝜇1 ≅(𝜖,𝛿) 𝜇1

′  then  for  any  a 𝜖  [0,1],  it  holds  that (1 − 𝑎)𝜇1 +

𝑎𝜇2⁡ ≅(𝜖,𝛿) (1 − 𝑎)𝜇1

′ + 𝑎𝜇2

′ ⁡. 

Lemma 1  
Let  𝑞 <
that𝜇 ≅(𝜖′,𝑞𝛿) 𝜇0, where𝜖′ = log⁡(𝑞(𝑒𝜖 − 1) + 1) ≤ 𝑞(𝑒𝜖 − 1). 

1
2

 and  let  𝜇0 , 𝜇1  be  distributions  such  that 𝜇1 ≅(𝜖,𝛿) 𝜇0 .  For  𝜇 = (1 − 𝑞)𝜇0 + 𝑞𝜇1 ,  it  holds 

Max Divergence   
The Max Divergence between two random variables Y and Z taking values from the same domain is defined 
to be: 

𝐷∝(𝑌||𝑍) = max

[𝑙𝑛

𝑆⊆sup(𝑌)

𝑃[𝑌∈𝑆]
𝑃{𝑍∈𝑆}

] 

The δ-Approximate Max Divergence between Y and Z is defined to be: 

𝛿(𝑌||𝑍) = ⁡

𝐷∝

max
𝑆⊆sup(𝑌):𝑃(𝑌∈𝑆)≥𝛿

[ln⁡

𝑃[𝑌∈𝑆]−𝛿
𝑃[𝑍∈𝑆]

] 

Remark 1 
For a mechanism ℳ, it is to be noted: 

•  It will be  differentially private necessary and sufficiently for two neighbouring database 𝑥 and 𝑦, 

𝐷∝(ℳ(𝑥)||ℳ(𝑦)) ≤ 𝜖𝑎𝑛𝑑𝐷∝(ℳ(𝑦)||ℳ(𝑥)) ≤ 𝜖; and 

•  It  will  be  ( 𝜖 ,δ)  private  necessary  and  sufficiently  for  two  neighbouring  data  set  𝑥  and 𝑦 , 

𝐷∝

𝛿(ℳ(𝑥)||ℳ(𝑦)) ≤ 𝜖and𝐷∝

𝛿(ℳ(𝑦)||ℳ(𝑥)) ≤ 𝜖. 

𝑙1-Sensitivity  

 
 
 
 
 
 
 
 
 
 
 
 
 
 
The 𝑙1 sensitivity of a function f : ℕ|𝒳| → ℝ𝑘, then:  ∆𝑓 = ⁡

max

𝑥,𝑦∈ℕ|𝒳|,||𝑥−𝑦||
1

||𝑓(𝑥) − 𝑓(𝑦)||1 

=1

𝑙2-Sensitivity  
The 𝑙2 sensitivity of a function f : ℕ|𝒳| then: ∆2= max
𝑥,𝑦∈ℕ|𝒳|

||𝑓(𝑥) − 𝑓(𝑦)||2. 

Utility  
How much information about the real answer can be obtained from the reported one by a specific model[18], 
is called the utility. How the utility works can be defined by the figure 2. 

 
 
 
 
 
 
 
 
 
 
 
 
 
