An RL-Based Adaptive Detection Strategy to Secure
Cyber-Physical Systems

1st Ipsita Koley
Dept. of Computer Science and Engineering
Indian Institute of Technology
Kharagpur, India
ipsitakoley@iitkgp.ac.in

2nd Sunandan Adhikary
Dept. of Computer Science and Engineering
Indian Institute of Technology
Kharagpur, India
sunandana@iitkgp.ac.in

3rd Soumyajit Dey
Dept. of Computer Science and Engineering
Indian Institute of Technology
Kharagpur, India
soumyajit@iitkgp.ac.in

1
2
0
2

r
a

M
4

]

R
C
.
s
c
[

1
v
2
7
8
2
0
.
3
0
1
2
:
v
i
X
r
a

Abstractâ€”Increased dependence on networked, software based control
has escalated the vulnerabilities of Cyber Physical Systems (CPSs).
Detection and monitoring components developed leveraging dynamical
systems theory are often employed as lightweight security measures for
protecting such safety critical CPSs against false data injection attacks.
However, existing approaches do not correlate attack scenarios with
parameters of detection systems. In the present work, we propose a
Reinforcement Learning (RL) based framework which adaptively sets the
parameters of such detectors based on experience learned from attack
scenarios, maximizing detection rate and minimizing false alarms in the
process while attempting performance preserving control actions.

Index Termsâ€”Cyber-physical systems, security, adaptive threshold,

reinforcement learning, formal methods

I. INTRODUCTION

While security is of paramount importance in CPS [1], the real
time hard deadlines of the safety critical functions and availability
of limited computing resources constitute the major constraints to
secure CPS design. Traditional heavy-weight cryptographic encryp-
tion techniques (like RSA, AES) along with MACs cannot be
used in most of the cases as they lead to both computation and
communication overhead [2]. One alternative is to design light-
weight attack detectors by exploiting control-theoretic properties.
Such detectors leverage the features of various state-observers like
Luenberger or Kalman ï¬lter that the controller unit of almost every
CPS comes with. These observers estimate the state of the plant
by computing residue as the difference between actual and observed
sensor data. The detector compares this residue with a predeï¬ned
threshold to identify an anomaly [3]. Though observers can be
useful in monitoring the systemâ€™s behavior, they cannot always be
adjusted to detect unmodeled disturbances, like faults and attacks.
Reducing observer gain may render it
insensitive towards small
changes in system states, while increased observer gain will lead
to increased estimation error covariance, which in turn will degrade
the systemâ€™s control performance. To overcome this limitation, the
statistical change detection methods, like Ï‡2-test, Cumulative Sum
(CUSUM) [4], [5], etc. are applied on the residue, before it
is
compared to the threshold. However, the constant threshold used
by these residue based detectors may increase the false alarm rates
(FAR) by misinterpreting measurement noise as attack, leading to
unnecessary degradation in control performance. Moreover, recent
research [6], [7] have shown how a stealthy attacker can fool such
detectors by crafting perturbation sequences which create residues
that are small enough (i.e. below threshold) to be classiï¬ed as
noise. Therefore, the fundamental question that arises is, whether the
detection threshold for such monitors in CPS implementations can be
dynamically adjusted based on the deployment environment so that
FAR is minimized while even small attack efforts can be detected.

In the present work we propose an intelligent detection scheme that
strives to achieve this goal.

Some recent research efforts, for example [7], [8], [9], have
addressed this problem of balancing between FAR and detectability in
the CPS context. However, they have the following limitations that we
have addressed in the current work. Unlike [8], the proposed detector
focuses on identifying attacks on CPS rather than faults. In [8], the
authors have formulated a non-linear programming problem to syn-
thesize adaptive thresholds, considering operating regions. When the
system is online, a threshold is selected from the pre-calculated ones
to detect transient faults, based on which operating region the system
is currently working in. In case of attack, things are more difï¬cult
as an attacker can be smart enough to modify its action to remain
stealthy. In [7], the authors have presented two greedy algorithms
based on formal methods to synthesize monotonically decreasing
variable threshold based detectors to thwart targeted performance
degrading attacks while minimizing FAR. However, they have deï¬ned
the CPS system requirement in terms of settling time. In such works
it is often straightforward to design attack vectors through constraint
solving such that the settling time property is satisï¬ed while some
other safety property gets violated causing critical damage to the
system. On the contrary, our approach interprets system safety in
terms of a safe operating region. Considering that an attacker may
force the system to migrate beyond this safety boundary at any time
instant, our proposed detector would try to detect such attack efforts
as early as possible.

CPSs are usually designed as closed loop feedback systems with
both controller and observer in place, both working together to ensure
stability while minimizing the effect of noise. Aim of an attacker
would be to remain stealthy and destabilize the closed loop dynamics
by failing the efï¬cacy of the controller and the estimator. Therefore,
while designing a detector we must also consider such an attack
model. Moreover, in the view of real time constraint of the safety
critical CPSs, the adaptive threshold generation process must be
efï¬cient in terms of timing overhead. Though, the authors of [9] have
drawn similar motivation like ours and presented an attacker-defender
game to solve the adaptive threshold selection problem, their work
lacks these considerations.

Another important aspect while designing an intelligent secure
CPS is the mitigation of an attackâ€™s effect, i.e. when an attacker
is detected by the detector system and the system is still within the
safety boundary, how can its effect be mitigated at the earliest ?
The approach in [10] proposes a Reinforcement Learning (RL) based
robust control strategy for autonomous vehicle (AV) control in the
presence of an attacker who modiï¬es the spacing information between
vehicles. The method leverages the fact that measurement information
(velocity of other vehicles) is drawn from multiple sensors and learns

 
 
 
 
 
 
the optimal weights of these sensors that mitigate the attackâ€™s effect.
However, this work is applicable to a speciï¬c CPS and also the
authors do not consider any active security primitive like detection
systems. In [11], the authors formulate a secure state estimation
problem followed by RL based optimal controller design to void
the effects of detected attacks. Though the approach is based on a
general CPS model, they have used a constant threshold based attack
detector which may suffer from high FAR. In general, the fact that a
well-trained RL agent is realizable for safety critical CPSs with real
time requirements has already been established in other contexts like
energy efï¬ciency[12].

In this work, we consider that the communication channel between
i.e. an attacker can gain
the plant and controller is vulnerable,
access to the network and add spurious data to every communication
between plant and controller. Such an attack is called false data
injection (FDI) attack. Assuming that
the attacker has complete
knowledge of the system and associated detector, it can intelligently
craft an attack to induce maximum damage to the system while
remaining stealthy.

II. SECURE CPS MODEL

In the absence of an adversary, the closed loop dynamics of a CPS

can be presented as a discrete linear time-invariant (LTI) system,

xk+1 = Axk + Buk + wk, yk = Cxk + Duk + vk, uk = âˆ’K Ë†xk,
Ë†xk+1 = AË†xk + Buk + Lrk, rk = yk âˆ’ C Ë†xk + vk,
(1)
where, xk âˆˆ Rn is the system state vector, yk âˆˆ Rm is the
measurement vector obtained from available sensors at k-th time
stamp; A, B, C, D are the system matrices. We consider that the
initial state x0 âˆˆ N (Â¯x0, Î£), the process noise wk âˆˆ Rn âˆ¼ N (0, Î£w)
and the measurement noise vk âˆˆ Rm âˆ¼ N (0, Î£v) are independent
Gaussian random variables. Further, in every k-th sampling instant,
the observable system state Ë†xk is estimated using system output
yk while minimizing the effect of noise, and used for computing
the control input uk âˆˆ Rl. The symbol rk denotes the residue i.e.
the difference between the measured and the estimated outputs. The
observer gain L and controller gain K ensures that both (A âˆ’ LC)
and (A âˆ’ BK) are stable. The system has a detector unit (Fig. 1)
which computes a function f (rk) and compares it with a threshold
T h to identify any anomalous behavior of the system. Considering
an FDI attack, where the attacker injects false data ay
k (Fig. 1)
to the sensor data and control signal respectively, the equation of the
system dynamics will become,

k and au

i=1

(2)

k, ya

k, Ë†xa

k + DËœua

(Oiâˆ’Ei)2
Ei

k + vk + ay
k;
k + Lra
k

k+1 = Axa
xa
k = ya
ra

k = Cxa
k+1 = AË†xa
k + Bua
k + au
k = ua
k ;

k + B Ëœua
k âˆ’ C Ë†xa
k = âˆ’K Ë†xa
ua
k, Ëœua
k, ua
k , ra

k + wk; ya
k + vk; Ë†xa
k; Ëœua
Here, xa
k represent plant state, estimated plant
state, forged sensor data, residue, control signal, and forged control
signal respectively in an attack scenario. In the present work we
consider f as the popular Ï‡2-test commonly employed in existing
works on secure CPS [3].
Ï‡2-Test and Ï‡2-Distribution : The Ï‡2-test
is one of the most
widely used statistical tests for examining the independence of two
or more categorical variables. Given the observed count Oi and the
expected count Ei of each category i, Ï‡2 statistics can be computed
as Ï‡2 = (cid:80)k
. Here, k is the number of categories.
Smaller value of Ï‡2 signiï¬es more correlation between the categories.
Now, consider n = k âˆ’ 1 random variables that follow standard
Gaussian distribution. Sum of the squares of these random variables
follow a Ï‡2 distribution of n degrees of freedom (dof) deï¬ned as,
P (x) = x
. Here, Î“ denotes the Gamma function and mean
of this distribution is n. Given the Ï‡2-distribution of n dof and the
Ï‡2 statistics, we can decide whether to accept or reject a speciï¬ed
null hypothesis. For example, a null hypothesis can be whether
a random Gaussian vector has the expected mean and variance.
Therefore, Ï‡2-test can be used to detect anomalies in dynamical
systems [13], like CPSs[3]. Ï‡2-Test on an m dimensional random
variable z(i) âˆ¼ N (0, V ) gives Ï‡2
j=iâˆ’l+1 z(i)T V âˆ’1z(i).
z(i) follows a Ï‡2-distribution with dof = ml. With respect to a
Ï‡2
given threshold Î¸, we say an anomaly is detected if Ï‡2
z(i) â‰¥ Î¸ at
some time stamp i.

z(i) = (cid:80)i

âˆ’1eâˆ’ x
Î“( k
2

n
2

k
2

2

2

)

III. PROPOSED METHODOLOGY

In this section, we elaborately discuss the three principal com-
ponents of our proposed adaptive secure CPS model: i) an adap-
tive threshold synthesis method, ii) an intelligent attack generation
method, and iii) a robust control strategy. Finally, we present a multi-
agent RL framework that binds the above three components to ensure
intelligent attack detection and mitigation.

Fig. 1: Reinforcement learning based adaptive monitoring strategy

We present an intelligent secure CPS model that consists of: i) an
adaptive attack monitor that thwarts an FDI attack, and ii) a robust
controller that mitigates the effect of such attacks. Considering an
attack as unmodeled disturbance that does not follow any deï¬ned
distribution; we propose an RL based baseline framework as shown
in Fig. 1 that leverages the following RL agents.

1) We present a novel detector for CPS attacks which leverages
RL based adaptive threshold selection (Fig. 1). The choice of
the threshold is based on minimization of FAR while keeping it
below a predeï¬ned upper bound.

2) We present an RL based robust control strategy (Fig. 1) that
would strive for preserving control performance in the presence
of FDI attacks. When no attack takes place, the system operates
with an optimal controller that ensures high performance by
restricting the trajectory inside a preferable operating region.
When an attack effort is detected with the system still within
its safety boundary, the proposed robust controller brings the
system back to its preferable operating region.

3) For the RL based detector and controller to learn from expe-
rience, one needs to provide FDI attack vectors that are both
stealthy and able to steer the system away from safe operations.
Our third RL agent mimics such attack behavior in the training
phase of the system.

4) We establish the usefulness of our multi-agent secure CPS
model by considering attack scenarios for well known CPS
benchmarks and achieving signiï¬cant performance improvement
w.r.t. baseline.

NetworkAttackerSensor dataTampered sensor dataControl signalğ‘¢ğ‘˜Tampered control signal ğ‘¢ğ‘ğ‘˜Generate robust control strategy to mitigate effect of detected attacksController Agent: Attack MitigationGenerate threshold adaptively to detect an attackDetector Agent:Attack DetectionMimics actual stealthy attacker to generate optimal stealthy attack during trainingAttacker Agent: Attack GenerationSensorPlantğ‘¥ğ‘˜EstimatorControllerDetectorğ‘Ÿğ‘˜=ğ‘¦ğ‘ğ‘˜âˆ’à·œğ‘¦kğ‘“ğ‘Ÿğ‘˜â‰¥ğ‘‡â„Alarmà·œğ‘¥ğ‘˜ğ‘¢ğ‘˜ğ‘¦ğ‘˜ğ‘¦ğ‘ğ‘˜ağ‘¦ğ‘˜ağ‘¢ğ‘˜EnvironmentRL Agentsğ‘Ÿğ‘œğ‘ğ‘¢ğ‘ ğ‘¡ğ‘ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ğ‘ ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿğ‘‘ğ‘ğ‘¡ğ‘ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘A. Optimal Threshold Synthesis

We present a residue based attack detection system where we
consider the Kalman ï¬lter as the estimator. The proposed detector
will adaptively select a threshold T hk at every k-th sample. Let,
for the discrete LTI system shown earlier, the estimation error ek
be deï¬ned as ek = (xk âˆ’ Ë†xk). The Gaussian assumptions of noise
and initial states ensure that ek follows a normal distribution with 0
mean. We denote the steady state covariance matrix of this estimation
error with Î£e. So the system residue rk, calculated in the Kalman
estimator can be expressed as rk = Cek + vk (see Eq. 1). Given that
both estimation error and measurement noise are Gaussian distributed
with zero mean and are independent of each other,
the residue
is normally distributed with 0 mean and covariance matrix given
Î£r = E[rkrT
k ] =
CÎ£eC T + Î£v.

k ] âˆ’ E[rk]E[rk]T = E[(Cek)(Cek)T ] + E[vkvT

i=kâˆ’l+1 rT

We use Ï‡2-test on rk to ï¬nd out how much the distribution of actual
plant state xk and its estimate Ë†xk vary from each other. Let gk denote
the Ï‡2-test result at k-th sample and gk = (cid:80)k
i Î£âˆ’1
r ri.
Here, l is the window size of Ï‡2-test. In this case,
the degree
of freedom is ml, where m is the number of available sensors
in the plant. In a normal scenario (no attack), gk follows Ï‡2
distribution with mean ml (Fig. 2). Let T hk be the threshold
is currently (at k-th sampling instance) being used by our
that
variable threshold based detector unit. Then, gkâ€™s probability density
function (PDF) along with its cumulative distribution function with
respect to T hk can be deï¬ned as, P (gk) = g

; P (gk â‰¤

ml
2

gk
2

eâˆ’

âˆ’1

k

ml
2 Î“( ml
2 )

2

T hk

2 )

T hk) = Î³( ml
2 ,
. Here, Î“ and Î³ are ordinary and lower
Î“( ml
2 )
is a false
incomplete gamma functions respectively. We say it
alarm when gk â‰¥ T hk even in the absence of an attacker.
So false alarm rate
is the ratio of
(FAR)
times
the number of
when the alarm has been
raised falsely and the
total number of alarms
raised.
the
blue curve and the ma-
roon curve represent the
distribution of gk under
no attack and attack respectively. Therefore, FAR should be the
fraction of area under the probability distribution curve of un-attacked
gk that is contained by the part beyond gk = T hk and computed as
1 âˆ’ P (gk â‰¤ T hk).

Fig. 2: Ï‡2-Distribution

In Fig. 2,

k under attack as established next.

Now, in presence of an FDI attack, where spurious data ay

k and au
k
are added to the sensor and the actuator data respectively, this leads
to non-centrality of the Ï‡2-test (the maroon curve in Fig. 2) result
k obtained for the residue ra
ga
Theorem 1: Considering an FDI attack on an LTI system as
k follows a non-central

speciï¬ed in Eq. 2, the Ï‡2-test on residue ra
Ï‡2-distribution.
Proof: Under FDI attack, the estimation error at k-th sample is given
by ea
k âˆ’ ek be the difference between
the estimation error under attack and no attack scenarios. Thus, ra
k =
Cea
k. Considering that the mean
of the estimation error is 0 and measurement noise is independent
of the estimation error and sensor attack, the covariance Î£ra of the
residue ra
k generated due to an FDI attack can be computed as Î£ra =

k = Cek + C(cid:52)ek + vk + ay

k. Let (cid:52)ek = ea

k + vk + ay

k = xa

k âˆ’ Ë†xa

E[ra

kraT

k ] âˆ’ E[ra

k]E[ra

k]T where,

E[ra

k C T + Cek(cid:52)eT

kraT
k ] = E[CekeT
C(cid:52)ek(cid:52)eT

k C T + C(cid:52)ekayT
k]T = (CE[(cid:52)ek] + E[ay
(cid:52)eC T + CÂµ(cid:52)eÂµT

= CÂµ(cid:52)eÂµT

E[ra

k]E[ra

k C T + C(cid:52)ekeT
k + ay

k(cid:52)eT

k + vkvT

k C T +
k C T + ay
k]T )
(cid:52)eC T + Âµay ÂµT

kayT
k ]

ay C T

k])(E[(cid:52)ek]T C T + E[ay
ay C T + Âµay ÂµT

Here, the notations Âµi and Î£i denote the mean and variance respec-
tively of any variable i, and Î£i,j denotes the covariance of i, j for
any i, j. Using the expressions of E[ra
k]T we
get,

k ] and E[ra

k]E[ra

kraT

Î£ra = CÎ£eC T + CÎ£e,(cid:52)eT C T + CÎ£e,ayT + CÎ£(cid:52)e,eT
+ CÎ£(cid:52)eC T + CÎ£(cid:52)e,ay + Î£ay ,eT + Î£ay ,(cid:52)eC T
+ Î£v + Î£ay = Î£r + P

(3)

Here, P = CÎ£e,(cid:52)eT C T + CÎ£e,ayT + CÎ£(cid:52)e,eT + CÎ£(cid:52)eC T +
CÎ£(cid:52)e,ay + Î£ay ,eT + Î£ay ,(cid:52)eC T + Î£ay , Î£r = CÎ£eC T + Î£v.
Since, by deï¬nition, covariance is positive semi deï¬nite and variance
is positive, both Î£r, P and consequently Î£ra are positive deï¬nite.
Considering an FDI attack on the both control signal and sensor out-
k = (cid:80)k
i Î£âˆ’1
r ra
put, we apply Ï‡2-test on ra
k which gives ga
i .
the mean Âµ of the Ï‡2 statistics ga
k over an
Therefore,
observation window of length l can be computed as Âµ = E[ga
k ] =
E[(cid:80)k
r ] i.e.,

i=kâˆ’l+1 raT
k of ra

i=kâˆ’l+1 trace[Î£ra Ã— Î£âˆ’1

i=kâˆ’l+1 raT

i ] = (cid:80)k

i Î£âˆ’1

r ra

k
(cid:88)

Âµ =

trace(Î£rÎ£âˆ’1

r ) +

k
(cid:88)

trace(P Î£âˆ’1
r )

i=kâˆ’l+1

i=kâˆ’l+1

= ml +

k
(cid:88)

i=kâˆ’l+1

trace(P Î£âˆ’1

r ) > ml

(4)

The last inequality follows from the fact that P is positive deï¬nite.
Hence, the mean of ga
k is strictly greater than the mean ml of gk. This
makes the distribution of ga
k a non-central one with non-centrality
parameter Î» = (cid:80)k
(cid:3)
Theorem 2: Leveraging the non-central distribution of Ï‡2 statistics

i=kâˆ’l+1 trace(P Î£âˆ’1

r ).

k

2 eâˆ’

(ga
k +Î»)
2

k ) = 1

increases the detectability of a false data injection attack.
Proof: The non-central distribution of ga
k with mean Âµ and non-
centrality parameter Î» can be deï¬ned in terms of the following
( ga
Î» )Âµ/4âˆ’1/2IÂµ/2âˆ’1((cid:112)Î»ga
PDF[14], P (ga
k ) with I
denoting Bessel function. With respect to T hk, we say an FDI attack
is detected if ga
k > T hk. This is a true positive case. Therefore, the
true positive rate (TPR) of detecting an attack is computed as T P R =
1 âˆ’ P (ga
Î», T hk).
Essentially this is fraction of area under the distribution curve (Fig. 2)
of ga
k = T hk. Here, Q is Marcum Q-function [14]. In
Theorem 1, we have proved that Âµ > ml where ml is the mean
of gk. This causes the non-central Ï‡2 distribution of ga
k to be more
shifted towards the right than the Ï‡2 distribution of gk.

k â‰¤ T h) where P (ga

k â‰¤ T hk) = 1 âˆ’ QÂµ/2(

k beyond ga

âˆš

Moreover, the variance of gk is Ïƒ = 2ml and variance of ga
k is
Ïƒa = 2(ml + 2Î»), where Î» > 0. Clearly, Ïƒa > Ïƒ. Therefore, the
expected deviation of ga
k from Âµ is more than the expected deviation
of gk from ml which makes the distribution of P(ga
k ) wider and
thereby ï¬‚atter (since the area under both curves is unity). Hence, the
fraction of area under the curve beyond ga
k ) is more
than that in case of P(gk) as shown in Fig. 2. So, the non-central
Ï‡2 distribution improves TPR i.e. attack detectability thus leading to
(cid:3)
T P R > F AR for a properly chosen threshold parameter T h.

k = T hk of P(ga

the

dependence

Given
parameters
l ( window length), T h the problem of synthesizing an optimal
detector can be formulated as following optimization problem:

of T P R, F AR on

the

Jt = max
l,T h

w1 Ã— T P R âˆ’ w2 Ã— F AR s.t. F AR < (cid:15), l < lmax

aimed at minimizing F AR and maximizing T P R. Here w1, w2 âˆˆ
[0, 1] are the constant weights of TPR and FAR respectively, (cid:15) is the
maximum allowable FAR, and lmax is maximum allowed sequence
length. Given ya
k , the current sensor measurement vector, the solution
of the above optimization problem is a pair < lâˆ—, T hâˆ— >, where lâˆ—
and T hâˆ— are the optimal Ï‡2 window length and threshold respectively
with respect to current measurement of the system states.

B. Intelligent Attack Generation

Considering the discrete LTI sys-
tem speciï¬ed in Eq. 1, we classify
the operating region of the system, as
demonstrated in Fig. 3, in two primary
subregions: i) safe region XS, and ii)
preferable operating region XR where
XR âŠ‚ XS. The system becomes un-
safe when it goes beyond the outer-
most region XS. The middle region
XR deï¬nes the set of possible states
in which system operation is preferred
due to performance consideration. We
formally deï¬ne such preferable oper-
ating regions as robust invariance sets

Fig. 3: Operating regions
of the system: Safe region
XS, Preferable Operating
Region XR

as follows.

Deï¬nition 1: Preferable Operating Region: Considering the
discrete LTI system speciï¬ed in Eq. 1 and controller with gain K,
the preferable operating region of the system is deï¬ned as:

XR = {x | âˆ€ w âˆ¼ N (Î£w, 0), v âˆ¼ N (Î£v, 0), and x âˆˆ
XR, f n(x, K) âˆˆ XR âˆ€n âˆˆ N}
where, XR âŠ‚ XS and â€™fâ€™ implements the state transition process of
(cid:3)
the LTI system (Eqs. 1) in no attack scenario.
By preferable operating region, we mean that starting from anywhere
within âˆˆ XR, the controller ensures that the system will always
remain within XR in the absence of any FDI attack. Note the choice
of XS can be exercised depending upon system description and safety
criteria. The invariant set based preferable operating region XR is
chosen in practice as some i-step invariant set within XS for which
the controller guarantees satisfactory performance.
Synthesis of Preferable Operating Region XR: We present a Sat-
isï¬ability Module Theory (SMT) aided constraint solving approach
for computing XR for a given safety-critical CPS in Algo. 1. The
GETPERFORMANCEREGION( ) function takes as input the system
matrices A, B, C, controller gain K, observer gain L, safety region
XS, and the forward step count i for an i-step invariant set. The
preferable operating region XR is deï¬ned as a fraction of XS i.e.
XR = depth Ã— XS, depth âˆˆ (0, 1). For an n dimensional system
XS âˆˆ I n with I representing any real interval. Initially, we consider
depth = dÎ´ where dÎ´ âˆˆ (0, 1) and compute XR accordingly in
line 2. Then, the plant state x0 is initiated symbolically from XR
(lines 2-3). We unroll the state progression i times following Eq. 1
in lines 7-9. For XR to be the desired preferable region, after i steps,
plant state must reside within XR i.e., xi âˆˆ XR. This symbolic
constraint is provided as an assertion Ï† (line 10). Negation of this
assertion, i.e. Â¬Ï†, is passed to the SMT solver Z3 [15]. Z3 tries to ï¬nd
a value of x0 for which Â¬Ï† can be satisï¬ed. If such an assignment
of x0 is found, it implies that there exists an initial state of the

system âˆˆ the current candidate XR, starting from which the system
does not converge back to XR in i steps. Note that we consider the
reference point of the system is 0. The optimal LQG controller with
gain K guarantees to keep the system close to the reference point at
steady state. Therefore, the algorithm retries by increasing depth by
a step dÎ´ and looks for a larger XR (line 11-12). Otherwise, current
depth Ã— XS is returned as ï¬nal XR (line 13). We set dÎ´ as 0.1 in

Algorithm 1 Region Synthesis for Assured Performance

Require: Closed Loop System Matrices (cid:104)A, B, C, K, L(cid:105), system
Safety Region XS, forward step count i in i-step invariant set
computation

Ensure: Preferable Operating region XR
1: function GETPERFORMANCEREGION((cid:104)A, B, K, L(cid:105),XS,i)
depth â† dÎ´; XR â† depth Ã— XS; x0 âˆˆ XR;
2:
y0 â† Cx0; Ë†x0 â† 0; u0 â† âˆ’K Ë†x0; r0 â† y0 âˆ’ C Ë†x0;
3:
repeat
4:
5:
6:
7:
8:
9:

xk â† Axkâˆ’1 + Bukâˆ’1 + wk;
Ë†xk â† AË†xkâˆ’1 + Bukâˆ’1 + Lrkâˆ’1;
uk â† âˆ’K Ë†xk; yk â† Cxk + vk; rk â† yk âˆ’ C Ë†xk;

XR â† depth Ã— XS;
for k = 1 to i do

10:
11:
12:
13:

Î¦ â†assert((x0 âˆˆ XR) â‡’ (xi âˆˆ XR));
depth â† depth + dÎ´;

until Â¬Î¦ is unsatisf iable âˆ§ depth â‰¥ 1
return XR

our experiments. Thus, it is formally guaranteed that the system will
always remain within XR when no attack is taking place provided
it has been initiated from XR itself. In this work, we design K as a
Linear-Quadratic-Gaussian (LQG) controller. In general, any optimal
control framework is applicable though.

In the CPS context, the attackerâ€™s motive is to steer the system
beyond the safe set XS while trying to remain stealthy by reducing
the TPR. Given the sensor measurement ykâˆ’1, we present this attack
estimation problem as the following optimization problem:

Ja = maxay

[âˆ’w1 Ã— T P R + w2 Ã— F AR + (cid:80)âˆ
i âˆˆ (cid:15)y, ua

|XS|)T W3(|xi+1| âˆ’ |XS|)] s.t. ya

k,au
k

i=k(|xi+1| âˆ’
i âˆˆ (cid:15)u

Here, w1 and w2 are the same weights used in Jt for the optimal
threshold cost function Jt. Since the attack generation method will
be used for experience learning of threshold tuner and robust control
RL agents, it is imperative that Ja assumes knowledge about Jt
and tries to negate its cost objective. This is captured in the ï¬rst
two component terms of Ja. The last component of Ja accounts for
deviation of the current system state from the safety boundary XS
using a quadratic weighted distance metric where W3 is a diagonal
matrix consisting of relative weights corresponding to criticality of
each dimension. Also, (cid:15)y, (cid:15)u indicate the allowable sensor range, and
actuation saturation range respectively. The solution (cid:104)ayâˆ—
k (cid:105) of
the above optimization problem is a possible attack vector that can
breach the safety barrier while being stealthy, i.e. by nullifying the
detector objective function Jt. In both Jt, Ja implicit constraints are
system and detector dynamics.

k , auâˆ—

C. Robust Controller Design

The LQR controller gain K is designed to provide optimal control
action with respect to control cost under no attack scenario. However,
it may not guarantee robustness against FDI attacks. To mitigate effect
of an FDI attack, we propose a robust control strategy that will be

XSXRxkxk+1xk+2xk+nxakxak+1xak+2xak+nxak+3(Î›a)

k, acta

k, obsa

The Attacker Agent

is designed to intelligently in-
false data into the system. We design a reward function
ject
Ra(obsa
k+1) for Î›a, that is built with the components of
Ja. The FDI attacker agent tries to solve the optimization problem
Ja in every sampling iteration during training, by exploiting several
actions while exploring the action space. These transitions are then
stored as experiences. The training algorithm learns the highest
expected return from the experiences and updates the RL policy to
earn it. This helps it eventually choose the optimal action i.e. the
optimal false data to inject into the sensors and actuators (cid:104)ay
k (cid:105),
that generates the maximum value of Ja as the reward Ra for the
FDI attacker agent. Similarly for Threshold-based Detector Agent
(Î›d), a reward function Rd(obsd
k+1) is designed with
the components of Jt. The agent intelligently chooses the optimal
change detection parameters like threshold T h and Ï‡2 detection
window length l by optimizing the objective function Jt i.e. aiming
k, actd
the maximum reward Rd(obsd
k+1). The reward function
k+1) is also designed with Jc for Î›c to choose
Rc(obsd
k, obsd
an optimal control input ua
k that generates the maximum reward. The
Attack mitigating Controller Agent (Î›c) is activated when an attack
is detected in the considered closed loop system (refer Algo. 2).

k, obsd

k, obsd

k, actd

k, actd

k, au

Given a secure CPS model, we train these model-free DDPG
agents so that they act according to their designated roles in the
environment, which is a simulated secure CPS model. Using standard
DDPG training algorithm, how these agents (Î›a, Î›d and Î›c) learn
to interact with our multi-agent RL environment collaboratively and
competitively, is depicted in Fig. 4. For example, from the reward
the detector agent Î›d
and objective functions we can see that
always tries to win against the attacker agent Î›a by competitively
choosing an action(Ï‡2-detection threshold and observation window)
to detect its (Î›aâ€™s) actions (FDIs). Once the attack is detected, the
controller agent Î›c then tries to nullify the effects of these actions
(attacks/FDIs output by the attacker Î›a) by choosing an optimal
action (attack mitigating control input) from the training experiences.
The attacker agent Î›a also tries to contest these two agents by
optimally launching a hard-to-detect yet successful FDI in every
iteration. In a way, Î›a is collaborating with Î›d and Î›c to help them

Algorithm 2 RL Based Adaptive FDI Attack Monitoring Framework

1: XR â† GETPERFORMANCEREGION(system, safety region XS,
forward step count i in i-step invariant set computation)
(cid:46)
Compute the preferable performance region for given closed loop
system

2: obsini â† rand(obs âˆˆ Obs) (cid:46) Initialize the environment/system

with a random state from XR

3: [Î›a, Î›c, Î›d]

TRAINAGENTS(system,
[Î›a, Î›c, Î›d],training specs) (cid:46) Competitive and Collaborative
ofï¬‚ine training of the multi-agent setup

â†

4: Put the system online
5: for every sampling iteration k âˆˆ [1, T ] do
6:

Collect the system observable states [obsc

k, obsd

k] in current

iteration
[actd
detector agent
if actd

[actc
from controller agent

7:

8:
9:

10:

k] â† [Î›d(obsd

k)] (cid:46) Update the system with actions from

kâˆ’1 ï¬‚ags an FDI & system state xk âˆˆ XS âˆ’ XR then
k)] (cid:46) Update the system with actions

k] â† [Î›d(obsd

Simulate the system with current actions and generate next
set of observable states [obsc

k+1, obsd

k+1]

Fig. 4: RL Based Adaptive Monitoring Framework

triggered only when the adaptive threshold based detector (Sec. III-A)
detects an FDI attack and the system âˆˆ XS \ XR at the moment of
attack detection. Given the forged sensor data ya
k , we compute such a
robust control action by solving the following optimization problem:

Jc = minua
k

(cid:80)âˆ

i=k(|Ë†xa

i+1| âˆ’ |XR|)T W3(|Ë†xa

i+1| âˆ’ |XR|) s.t. ya

i âˆˆ

(cid:15)y, ua

i âˆˆ (cid:15)u

k

Naturally, system dynamics is an implicit constraint here. We have
used the same weight matrix W3 from Ja to nullify the attackâ€™s effect
(assuming the knowledge of the attack generation module about all
other cost functions). The solution of the above optimization problem
is a control input uaâˆ—
that minimizes the damage induced by the
attacker by bringing the system back inside XR (thereby, inside XS).
Note that uk is not optimal w.r.t. performance unlike an optimal
controller; being robust it prioritizes safety. Thus, we are allowing
higher control effort as long as it does not exceed the actuation
saturation limit as the objective of this controller would be bringing
the system back to XR in minimum time. Once inside XR, the system
switches to the optimal controller.

D. The Reinforcement Learning Framework

Since an attacker may exhibit unknown dynamics, the central idea
of the work is to learn an adaptive attack detection scheme along with
a robust controller for attack mitigation whenever possible. Due to
scarcity of system speciï¬c labeled false data and the requirement of
learning parameters in dense domains, we employ the popular Deep
Deterministic Policy Gradient (DDPG) algorithm [16] which outputs
deterministic actions instead of optimized action distribution over the
continuous action space. The overall RL framework is shown in Fig.
4. A Deep Q-Network to criticize and update the actor policy by
calculating the Q value against the state and chosen action.

We ï¬rst describe the environment and agent speciï¬cations to un-
derstand the learning process that helps derive the design parameters.
Since a plant-controller closed loop system, equipped with a Ï‡2-
based detector, as shown in Fig. 1) is the system under test here,
we design our environment by modeling such a closed loop system.
Our methodology uses three DDPG agents (Î›) that interact with this
environment. By observing certain parameters from the environment
(obs), the agents learn how to intelligently choose an action (act) to
inï¬‚uence it as they want. The following table lists the observation
and action variables.

RL Agent
Attacker Agent Î›a
Detector Agent Î›d
Controller Agent Î›c

Observations
obsa = (cid:104)yk, uk, T hk, XS (cid:105)

obsd = (cid:104)ra
k , XS (cid:105)
obsc = (cid:104)yk, XR, XS (cid:105)

k , ya

Actions
acta = (cid:104)ay
kau
k (cid:105)
actd = (cid:104)T hk, l(cid:105)
actc = (cid:104)ua
k(cid:105)

TABLE I: RL Agent Details

(a) Threshold under no attack

(b) Variable threshold synthesis under FDI attack

(c) Attack detection

Fig. 5: Performance of adaptive threshold based detector

learn the system characteristics under FDI attack. Algo. 2 represents
the overall methodology.

IV. RESULTS

Systems and Framework Speciï¬cations: Automotive systems
have heterogeneous communication protocols for internal commu-
nications between the Electronic Control Units (ECUs) that execute
real time control tasks. Vulnerability in any of those protocols (eg.
Controller Area Network) can grant an easy access to the attacker to
manipulate majority of the system communications. This motivates us
to apply our RL based monitoring framework to one such automotive
CPS, namely Trajectory Tracking Controller (TTC). TTC regulates
the deviation of a vehicle from a given trajectory (D) and a reference
velocity (V ) by applying proper acceleration [17]. Our RL based
framework is built on MATLAB Reinforcement Learning Toolbox.
As mentioned earlier we employ DDPG agents having a policy
gradient based actor network coupled with a DQN based critic
network. Both the actor and critic networks have 3 hidden layers with
rectiï¬ed liner activation units (ReLU) for better training considering
the complexity of CPSs. We train this RL based monitoring frame-
work with TTC for 3000 episodes with three RL agents. The system
matrices (A, B, C), sampling period (h), controller and estimator
gains (K, L) along with corresponding preferable operating region
(XR) and safety region (XS) of the systems are given in Tab. II.

Sys.

TTC

TABLE II: System Speciï¬cations

Speciï¬cations

XS

XR

A = [1.0000, 0.1000;0, 1.0000];
B = [0.0050;0.1000]; C = [1 0];
h = 0.1sec; K = [16.0302, 5.6622];
L = [1.8721;9.6532]

D âˆˆ [-25, 25]
V âˆˆ [-30, 30]

D âˆˆ [-7.5, 7.5]
V âˆˆ [-9, 9]

Experimental Results: As per our system speciï¬cations the
TTC is equipped with a Ï‡2 detector. To promise optimal resilience,
while training, we assume the attacker is aware of the adaptive
threshold based detector speciï¬cations (the currently chosen T hk and
l). We have trained the detector agent in the presence and absence
of the attacker agent to reinforce the learning that differentiates an
attacked and un-attacked situation. Given the safety speciï¬cations of
the system we ï¬rst derive the preferable operating region of TTC
using Algo .1. Intializing the system states from this region, we train
the RL agents for 3000 episodes, each with 100 simulation instances
in order to train them. As we can see in Fig. 5b , the detector agent
explores and exploits different threshold values for different detection
windows. This is an FDI attack scenario where the attacker agent
injects optimal false data to make the system unsafe. For this, the
Ï‡2-test value on the system residue changes as we see in Fig. 5c.
The detector agent starts by selecting T h = 1 for l = 4 and changes
the threshold to successfully detect most of the attack efforts. If we
consider the maximum non-centrality induced by the optimal FDI

attacker agent, TPR achieved by the adaptive detector agent is 0.91.
Note that, as shown in Fig. 5c under the considered optimal attack
scenario, the attacker is detected even before it is able to send the
observed system state outside the preferable operating region XR
(=7.5 for the output state, refer Tab. II). Hence the robust controller
does not kick off and our adaptive detection system promises a cost
effective control by early detection of attack.

In Fig. 5a as can be seen, our variable threshold based detector
selects T h = 12 when there is no FDI attack in place. The average
FAR achieved by the designed adaptive detection system during no
attack situation is 0.04. As we can observe in Fig. 5a, it manages
to place the threshold above the Ï‡2-test values of system residues
due to noises (under no FDI attack). Targeting the minimum FAR,
consider a constant threshold based Ï‡2 detector with T h = 12 is
placed to detect FDI attacks. Then, it is clear from the Ï‡2 statistics
of residues under attack in Fig. 5c that many of the attack attempts
would have remained undetected.

V. CONCLUSION

The present work proposed a RL based secure CPS model and
studied its usefulness through simulation using an automotive CPS
benchmark. In future, we plan to create an automotive hardware-in-
loop simulation infrastructure which will help us simulate automotive
control loops and vehicle dynamics in a real time platform in order
to check the timing performance of the proposed scheme in a more
realistic setting.

REFERENCES

[1] A. Humayed, J. Lin, F. Li, and B. Luo, â€œCyber-physical systems
securityâ€”a survey,â€ IEEE Internet of Things Journal, vol. 4, no. 6, pp.
1802â€“1831, 2017.

[2] A. Munir and F. Koushanfar, â€œDesign and analysis of secure and depend-
able automotive cps: A steer-by-wire case study,â€ IEEE Transactions on
Dependable and Secure Computing, 2018.

[3] Y. Mo and B. Sinopoli, â€œFalse data injection attacks in cyber physical

systems,â€ in SCS, Stockholm, 2010.

[4] A. S. Willsky, J. J. Deyst, and B. S. Crawford, â€œTwo self-test methods
applied to an inertial system problem,â€ Journal of Spacecraft and
Rockets, vol. 12, no. 7, pp. 434â€“437, 1975.

[5] J. Giraldo, D. Urbina, A. Cardenas, J. Valente, M. Faisal, J. Ruths, N. O.
Tippenhauer, H. Sandberg, and R. Candell, â€œA survey of physics-based
attack detection in cyber-physical systems,â€ ACM Computing Surveys
(CSUR), vol. 51, no. 4, pp. 1â€“36, 2018.

[6] A. Teixeira et al., â€œSecure control systems: A quantitative risk manage-
ment approach,â€ IEEE Control Systems Magazine, vol. 35, no. 1, pp.
24â€“45, 2015.

[7] I. Koley, S. K. Ghosh, S. Dey, D. Mukhopadhyay, A. K. KN, S. K.
Singh, L. Lokesh, J. N. Purakkal, and N. Sinha, â€œFormal synthesis of
monitoring and detection systems for secure cps implementations,â€ in
2020 Design, Automation & Test in Europe Conference & Exhibition
(DATE).

IEEE, 2020, pp. 314â€“317.

[8] Y. Baek and M. Jo, â€œAdaptive threshold generation for fault detection
with high dependability for cyber-physical systems,â€ Appl. Sci., vol. 8,
no. 11, 2018.

[9] A. Ghafouri, W. Abbas, A. Laszka, Y. Vorobeychik, and X. Koutsoukos,
â€œOptimal thresholds for anomaly-based intrusion detection in dynamical
environments,â€ Lect. Notes Comput. Sci. (including Subser. Lect. Notes
Artif. Intell. Lect. Notes Bioinformatics), vol. 9996 LNCS, pp. 415â€“434,
2016.

[10] A. Ferdowsi, U. Challita, W. Saad, and N. B. Mandayam, â€œRobust
deep reinforcement
learning for security and safety in autonomous
vehicle systems,â€ in 2018 21st International Conference on Intelligent
Transportation Systems (ITSC).

IEEE, 2018, pp. 307â€“312.

[11] Y. Zhou, K. G. Vamvoudakis, W. M. Haddad, and Z.-P. Jiang, â€œA Secure
Control Learning Framework for Cyber-Physical Systems under Sensor
Attacks,â€ in 2019 Am. Control Conf.
IEEE, jul 2019, pp. 4280â€“4285.
[12] Y. Wang, C. Huang, and Q. Zhu, â€œEnergy-efï¬cient control adaptation
with safety guarantees for learning-enabled cyber-physical systems,â€
arXiv preprint arXiv:2008.06162, 2020.

[13] R. Da, â€œFailure detection of dynamical systems with the state chi-square
test,â€ Journal of guidance, control, and dynamics, vol. 17, no. 2, pp.
271â€“277, 1994.

[14] A. F. Siegel, â€œThe noncentral chi-squared distribution with zero degrees
of freedom and testing for uniformity,â€ Biometrika, vol. 66, no. 2, pp.
381â€“386, 1979.

[15] L. De Moura et al., â€œZ3: An efï¬cient smt solver,â€ in TACAS. Springer,

2008.

[16] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, â€œContinuous control with deep reinforcement
learning,â€ 2019.

[17] V. Lesi et al., â€œIntegrating security in resource-constrained cyber-

physical systems,â€ ACM TCPS, vol. 4, no. 3, pp. 1â€“27, 2020.

