9
1
0
2

n
a
J

6

]

R
C
.
s
c
[

1
v
8
9
5
1
0
.
1
0
9
1
:
v
i
X
r
a

Toward a Theory of Cyber Attacks

Saeed Valizadeh(cid:63) and Marten van Dijk

Secure Computation Laboratory,
Computer Science and Engineering Department,
University of Connecticut, Storrs CT 06269, USA
{saeed.val,marten.van_dijk}@uconn.edu

Abstract. We provide a general methodology for analyzing defender-attacker based
“games” in which we model such games as Markov models and introduce a capacity
region to analyze how defensive and adversarial strategies impact security. Such a
framework allows us to analyze under what kind of conditions we can prove state-
ments (about an attack objective k) of the form “if the attacker has a time budget
Tbud, then the probability that the attacker can reach an attack objective ≥ k is
at most poly(Tbud)negl(k)”. We are interested in such rigorous cryptographic se-
curity guarantees (that describe worst-case guarantees) as these shed light on the
requirements of a defender’s strategy for preventing more and more the progress
of an attack, in terms of the “learning rate” of a defender’s strategy. We explain
the damage an attacker can achieve by a “containment parameter” describing the
maximally reached attack objective within a speciﬁc time window.

Keywords: Stochastic games · Network security games · Cyber attacks · Security
region · Intrustion detection and prevention system · Honeypots · Attack modeling
· Markov models · Security capacity region.

1

Introduction

Cyber attacks targeting individuals or enterprises have become a predominant part of
the computer/information age. Such attacks are becoming more sophisticated (qualitative
aspects) and prevalent (quantitative aspects) on a daily basis [6]. The exponential growth
of cyber plays and cyber players necessitate the inauguration of new methods and research
for better understanding the “cyber kill chain”, particularly with the rise of advanced and
novel malware (e.g., Stuxnet [10], WannaCry ransomware crypto worm [6], the Mirai [2]
and its variants [3]) and the extraordinary growth in the population of Internet residents,
especially connected Internet of Things (IoT) devices.

Mathematical models can help the security research community to better understand
the threat and therefore being able to analyze the attacker’s conducts during the lifetime
of a cyber attack. The sparse amount of research on modeling and evaluating a defensive
systems’ eﬃciency (especially from a security perspective), however, warrants the need for
constructing a proper theoretical framework. Such a framework allows the community to
be able to evaluate the defensive technologies’ eﬀectiveness from a security standpoint. In

(cid:63) This work was funded by NSF grant CNS-1413996 “MACS: A Modular Approach to Cloud

Security”.

 
 
 
 
 
 
2

this regard, a proper model is needed to capture the interactions between the two famous
players of network security games i.e., a defender (taking advantage of common security
tools and technologies such as Intrusion Detection and Prevention Systems (IDPSes), Fire-
walls, and Honeypots (HPs)) and an attacker (and possibly its agents) who takes actions
to reach its attack objective(s) in the game. Modeling only one player by itself (e.g., oppor-
tunistic/targeted attacks, or a defensive system such as an IDPS) without acknowledging
the other party’s capabilities, set of actions, and strategies would be unjustiﬁable. Hence,
a realistic model should take both parties’ characteristics, objectives, and actions into con-
sideration, as they are typically oppositional. Game theoretic methods have been proposed
to model the interactions between an attacker and a defender in a network environment
[1, 4, 5, 17, 28]. Although insightful, as ﬁguring out the best attack-response strategy is in
the focal point of a game-theoretic analysis, such models suﬀer from a lack of general
applicability due to the case-speciﬁc assumptions made to reduce the complexity of the
problem leading to a case-speciﬁc game with a corresponding set of players’ actions and
payoﬀs. Therefore, the developed models become ineﬀective in the representation of general
settings.
Our Approach. We introduce a Markov Game (MG) framework and methodology for
modeling various computer security scenarios prevailing today, including opportunistic,
targeted and multi-stage attacks. We are particularly interested in situations in which each
players’ progress in the game can be viewed and modeled as an incremental process. From
the viewpoint of the attacker, a progressive adversarial move associated with a probabil-
ity distribution can bring it one step closer to its desired winning state (i.e., the attack
objective). Similarly, by monitoring the system and capturing the adversarial moves, the
defender can also get one step closer to its objective which could be diﬀerent based on
various attack/defense scenarios. For instance, the defender’s goal could be generating a
detection signal that indicates the presence of a coordinated set of activities that are part of
an Advanced Persistent Threat (APT) campaign [21], developing a signature for a malware
attack to halt next adversarial moves [22], or shuﬄing its resources after enough number
of samples (evidence) is obtained as in a Moving Target Defense strategy (MTD) [18].
Contributions & Results. Our contributions are summarized as follows:

– We introduce the notion of learning in cybersecurity games and describe a general “game
of consequences” meaning that each player’s (mainly the attacker) chances of making
a progressive move in the game depends on its previous actions. More speciﬁcally, as a
consequence of the adversarial imperfect moves in the game, the other party, i.e., the
defender has the opportunity to incrementally learn more and more about the attack
technology. This learning enables the defender to be able to seize and halt following
adversarial moves in the game, in other words, containing the attackers’ progress in the
game. We argue that such learning is possible since the actions that need to be taken
by an adversary in each cycle of a cyber attack’s life inevitably entails abnormal and
suspicious events and activities on both host and network levels.

– Unlike game theoretic methods which commonly focus on ﬁnding the best attack-
response strategies for the players, we, however, with a cryptographic mindset, mainly
focus on the most signiﬁcant and tangible aspects of sophisticated cyber attacks: (1)
the amount of time it takes for the adversary to accomplish its mission and (2) the
success probabilities of fulﬁlling the attack objectives. Therefore, our goal is to trans-

Toward a Theory of Cyber Attacks

3

late attacker-defender interactions into a well-deﬁned game so that we can provide
rigorous cryptographic security guarantees for a system given both players’ tactics and
strategies. We generalize the basic notion of computational security of a cryptographic
scheme [16] to a system which must be defended and introduce a computationally se-
cure system in which a “given system is (Tbud, k, (cid:15))-secure (for Tbud, k > 0, and (cid:15) ≤ 1),
if any adversary limited with a (time) budget at most Tbud succeeds in reaching the
attack objective k, with probability at most (cid:15)”. We study under which circumstances
the defense system can provide an eﬀective response that makes (cid:15) = poly(Tbud)negl(k).
– By modeling the learning rate of the defender as a function f (l) (which is the probability
of detecting and halting a next adversarial move) where l represents the number of
adversarial moves and actions so far observed and collected, we present the following
results:
− If f (l) does not converge to 1, then we call such a learning rate stagnating, and
the adversary can reach its winning state (attack objective) k within poly(k) time
budget.

− If f (l) → 1 and more speciﬁcally 1 − f (l) = O(l−2), then the probability of the
adversary winning the game is proven to be polynomial in the attacker’s time
budget and negligible in k.

− The above results hold for time budget measured in logical Markov model transi-
tions or physical time (seconds), and also hold for learning rates that remain equal
to f (l) = 0 until a certain number l = L∗ adversarial moves have been collected
after which learning starts.

− The adversary may not reach its desired winning state of k, but will reach a much
smaller kc (called containment parameter) of the order O(L∗ + log Tbud), where
Tbud is the attacker’s time budget, for 1 − f (l) → 0.

− We conclude that at a metalevel the adversary needs to ﬁnd one attack exploit/vec-
tor for which the defender cannot ﬁnd a fast enough increasing learning rate and
the defender needs to have a learning system/methodology in place which should
be able to reach fast enough learning rates for any possible attack exploit/vector.
In practice infrastructures are being compromised and this means that, as given
by our general framework, and presented theoretical and numerical analysis, ef-
fort is needed in order to understand associated learning rates and why these are
≤ 1 − Ω(l−a) for some ‘a’ too small or have L∗ = O(k) (i.e., learning starts too
late).

– The presented adversarial/defender game in terms of a Markov model is general and
ﬁts most practical scenarios as our extensive overview shows. We walk through diﬀerent
case studies to show how the presented modeling can be applied to real-world cyber
attacks.

Based on our theoretical analysis and its applicability to practical scenarios we un-
derstand when an adversary with time budget Tbud can be contained and prevented from
reaching its attack objective k with probability ≥ 1 − poly(Tbud)negl(k). As a consequence,
if such containment is in place, then this allows one to trust cryptographic protocols/sys-
tems which assume (for proving their security) attackers that have not reached k (and have
a key renewal / refresh operation that can be called every Tbud seconds in order to push
adversaries back to their initial state).

4

To the best knowledge of the authors, there has been no work so far considering the
notion of learning in adversarial games and its impact on players chances of prosperity
in the game. Moreover, through this presented framework, we are able to extend the idea
of security capacity introduced in [18] and provide the deﬁnition of a security capacity
region as a metric for gauging a defensive system’s eﬃciency from a security perspective
(by presenting rigorous cryptographic security guarantees in terms of the security capacity
region).
Paper Organization. The rest of this paper is organized as follows: Section 2 brieﬂy
reviews related work, especially modeling cyber attacks as a game between an attacker
and a defender. The interactions of the defender and the adversary are described as a
stochastic network security game in section 3. The security analysis of the introduced
game is presented in section 4 followed by the corresponding parametric analysis of the
game for various learning functions and system parameters to explore the eﬀects of such
variables on attack-defense objectives. In Section 5, we walk through a few attack-defense
scenarios to show how our presented framework can be applied to real-world examples. We
ﬁnally conclude the paper and present future work in section 6.

2 Background and Related Work

Modeling cyber attacks as a game between a defender and an attacker is a classical research
problem. Here, we review some of the research that has been done in this area which we
ﬁnd the most relevant (current state-of-the-art) and connected to this presented work.

The intrusion detection problem in heterogeneous networks consisting of nodes with
various security assets is studied in [5]. The expected behaviors of rational attackers in
addition to the optimal defender strategy are derived by formulating the attacker-defender
interaction as a non-cooperative game. The paper concludes that suﬃcient resources for
monitoring the environment and proper system conﬁguration at the defender side are two
necessary conditions of eﬃciently protecting the network. Similarly, the interaction of play-
ers is modeled as a general-sum stochastic game in [17] in which Lye and Wing studied
three diﬀerent attack-response scenarios including defacing a website, stealing conﬁdential
data, and launching a Denial of Service (DoS) attack. The authors though left the richer
and more complex scenarios for future works including a more capable defender with a
more extensive action set for attack detection and prevention purposes. Such a stronger
defender could potentially lure the attacker and learn the attack technology by setting up
defensive agents such as honeypots within the environment. Carroll and Grosu [4] con-
sider such stronger defense strategies, i.e., taking advantage of camouﬂage techniques (e.g.,
disguising a regular system as a honeypot or vice versa) by investigating the eﬀects of
deception on players’ interaction using a signaling game.

Motivated by the rise of advanced persistent threats, van Dijk et al. [28] introduced the
FLIPIT game to model the interaction of two players competing in a race of maximizing
the amount of time each is in control of a shared computing resource while minimizing
their total cost (associated with each player move). Strongly dominant strategies for both
players (if there exist such strategies) are determined based on diﬀerent employed attack
strategies. Also, they provided general guidance on how and when to implement a cost-
eﬀective defense strategy.

Toward a Theory of Cyber Attacks

5

Valizadeh et al. [18] introduced the concept of “security capacity” as a metric for gauging
the eﬀectiveness of an MTD strategy. The interactions of attacker and defender in dynamic
environments1 is modeled by probabilistic algorithms and characterized by a Markov chain.
In particular, they showed how the probability of a successful adversary defeating an MTD
strategy is related to the amount of time/cost spent by the adversary. The relationship
between the attack success probability and the time it takes to reach the attack objectives
(i.e., the winning state for the adversary) is then translated into the security capacity
concept: “the security capacity of an MTD game (a defense system) is at least c if the
probability that the attacker wins in the ﬁrst T = 2t time steps is ≤ 2−s for all t + s = c”.
Connell et al. [7] used a similar approach as [18] to model the attacker-defender interactions
via a Markov chain in dynamic environments. In particular, a quantitative analytic model is
proposed for evaluating the performance of MTD schemes, and the availability of resources
in the environment and a method is recommended for maximizing a utility function that
takes the tradeoﬀs between security and performance into consideration.

For a review of existing game-theory based solutions for network security problems
see [14, 19]. For instance, Liang et al. [14] summarized the presented game models’ ap-
plication scenarios (both cooperative and non-cooperative games) under two categories:
attack-defense analysis, and security measurement. Manshaei et al. [19] however, surveyed
the use of game theory in addressing diverse forms of privacy and security problems in
mobile and network applications. The studied works are organized in six main categories:
physical and MAC layer security, security of self-organizing networks, intrusion detection
systems, anonymity and privacy, network security economics, and cryptography.

3 Game Modeling

In this section, we introduce a general network security game based on the interactions
between an adversary, its agents (e.g., bots controlled by a botmaster), and a defender who
is equipped with a logically centralized defense system2 implemented in the network. We
explain how this game can be directly mapped to an equivalent Markov model and in the
next section, we provide the security analysis for this Markov model as well as presenting
compelling interpretations of the role of both players’ strategies in their probability of
winning the introduced game.

3.1 The Game of Consequences

[Game Setup] In almost any sophisticated and persistent cyber attack, the adversary
continues the attack until its attack objective is satisﬁed. However, due to the incomplete
and imperfect information of the players (in this case the attacker), and the probabilistic
nature of an attack’s success rates, the attacker’s objective usually cannot be achieved
in only a few numbers of moves/attempts. Adding a defender to this picture leads to an
unceasing game unless the attacker decides not to play anymore (i.e., dropping out of the

1 Those with changing system conﬁgurations and therefore attack surfaces
2 For instance, network/host-based intrusion detection and prevention systems, honeypots, etc.
One motivation for our work is the advent of Software-Deﬁned Networking (SDN) in which the
entire network infrastructure can be controlled from a centralized software controller.

6

game, whether the attack objective is satisﬁed or the chances of making a progressive move
become overwhelmingly small). We model the interactions of players in such scenarios as
a stochastic game.

To create a realistic mathematical model, we make reasonable simpliﬁed assumptions
from both attack and defense perspectives. This is due to the signiﬁcant level of freedom
in attack design and technology and the complexity and diversity of defense mechanisms
and systems. For instance, when it comes to evaluating a defense system’s eﬃciency (espe-
cially an IDPS), the accuracy, performance, completeness, fault tolerance, and timeliness
properties should be taken into considerations as the top ﬁve criteria [8]. However, for this
study, we believe considering all the playing factors in modeling such systems makes the
model excessively complicated (and possibly inaccurate). For this reason, we mainly focus
on security-related concerns and speciﬁcations, i.e., the accuracy and completeness (dealing
with false alarms and detection rates) of the system. This means the ability of the sys-
tem in detecting malicious behaviors and parties in the environment, and taking eﬀective
actions to foil such incidents, regardless of its architecture, used methods, or its impact
on the system performance. Therefore, assuming that the non-security related traits are
ideal (e.g., no latency, high performance, unlimited bandwidth, and fault tolerance), we
are dealing with a defense system which is neither entirely accurate nor complete, as every
information technology system suﬀers from security shortcomings and deﬁciencies.

In this regard, we consider a general system state (i, l) which only captures the security-
related parameters. The players start the game at the state (i = 0, l = 0) in which it repre-
sents the inauguration of the attack and the zero-knowledge of the defender regarding the
attack technology at time zero. From the adversary’s perspective, its view of the system
state i will change if only it makes a progressive move in the game. On the other hand,
by noticing that any adversarial move (whether fruitful or fruitless) can potentially be
observed and captured by the defender (for instance via the defense system’s agents and
sensors implemented in the environment), we model the defender as an incremental online
learning process3, meaning that the defender’s view l gets updated as a consequence of
discovering an adversarial move (i.e., an attack sample). This increasing knowledge of the
attack technology enables the defender to correctly detect and halt a new incoming mali-
cious action with some probability f (l) (true positive). It is also possible that the system
fails in detecting an adversarial move (or it falsely labels it as benign) with probability
1 − f (l) (false negative). Note that we do not care about the occurrence of false positives
(categorizing a benign activity as malicious or abnormal), as they only play a role in the
system performance and the defender’s detection cost meaning that such incidents do not
change the security state of the system (neither the attacker’s nor the defender’s view).

[Attacker] The attacker’s objective is to reach a winning state of the form (k, ∗) within
a limited time budget Tbud. We emphasize that k or in other words the attack objective
diﬀers from scenario to scenario. For instance, a malware propagator or a botmaster desires

3 Another motivation for this work is the introduction and widespread embrace of Automated
machine learning (AutoML) which provides the opportunity for automatic (and possibly dis-
tributed) attack learning, detection, and prevention.

Toward a Theory of Cyber Attacks

7

to push k (in this case, the total number of infected nodes) as high as possible4. On the
contrary, in an advanced persistent threat, the attacker might prefer to stop playing after
reaching a much smaller k to minimize the attack detection probability since once the
target machine has been identiﬁed, the attacker should use as few infections as possible to
decrease the chance of exposing the operation. An adversary trying to access distributed
information on a set of nodes within the network terminates the attack as soon as its mission
is accomplished. In a similar fashion, to construct a hitlist, the attacker will conclude its
reconnaissance after a compiled list of vulnerable nodes is constructed.

To reach the winning state, at any time step, the attacker issues a move associated
with a success probability p (purely depends on attack strategy and not the defender’s
maneuvering) that can potentially lead to incremental progress in the game, i.e., getting
one step closer to the ﬁnal attack objective (i = i + 1). This enables us to represent the
adversarial move in a single transition in the Markov model which will be explained in
section 3.2. For generality, we consider a state-dependent success probability, i.e., pi, since,
intuitively, there could be cases in which the chances of making a progressive move depends
on the current state of the attacker in the game. For instance, consider an attacker who
is trying to locate vulnerable nodes within an address space of size N , while i out of
K vulnerable hosts are already found, this means that the probability of hitting a new
vulnerable node is pi = (K − i)/(N − i). Also, we assume the adversary is aware of its
current state in the game i in (i, l) for some l ≥ 0 but it does not know the defender’s
knowledge of the attack expressed as l.

Moreover, we assume that at each time step, the interaction of the attacker (or an
attacker agent) with the system which is encapsulated as an adversarial move, can poten-
tially leak some information to the defender (if observed) regarding the attack technology
and methodology. This is because regardless of the mastery and skillfulness of an attacker,
its taken actions during any phase of the attack, inevitably lead to abnormal and suspi-
cious incidents and events on both host and network levels. Hence, almost all security tools
and technologies rely on the existence of such attack signal indicators for detection and
prevention purposes. For instance, unusual port usage [9], irregular system call sequences
[12], and the occurrence of pointer value corruption on a host’s process memory [15] are
amongst a few events that can happen as a result of adversarial moves on host levels. On
the network levels, such conducts include but are not limited to an increase in the network
latency, high bandwidth usage, suspicious traﬃc on exotic ports, irregular scan activity,
simultaneous identical domain name system (DNS) requests, and Internet relay chat (IRC)
traﬃc generated by speciﬁc ports.

However, a skillful attacker leveraging diﬀerent anti-malware and IDPS evasion tech-
niques5 is capable of minimizing the detection possibilities and therefore reducing the
chances of leaking attack information at each adversarial move by sneaking through the
defensive system. This skillfulness can provide the adversary more time to play the game

4 Or at least 5% of the total number of vulnerable hosts, as [24] shows via simulation that in
order to have hopes that no more than 50% of the total vulnerable hosts ever become infected,
patching process must begin before 5% of such population become ever infected.

5 Obfuscation methods, fragmentation and session splicing, application/protocol violations, and
DDoS attacks [20] are among the most common techniques used by the adversaries to decrease
the attack detection probability or to increase the chances of attack prosperity.

8

as each attack sample has a lower probability of being discovered and in extreme cases, it
can ideally be diﬀerent for each adversarial move making the defender’s job surely diﬃcult.
For instance, an attacker taking advantage of a polymorphic or metamorphic malware (in
contrast with a monomorphic one) will disclose minimum information by encrypting the
content and obfuscating the instruction sequences for each connection respectively lead-
ing to more propagation time available to the attacker. Therefore, dealing with a skillful
adversary, if the defense system captures an attack sample, this property intuitively leads
to maximizing the time and eﬀort to push out a solution to stop the attack and generate
an attack signature. More importantly, it means that only a few samples are not enough
for attack signature generation. Note that this property helps to minimize the information
leakage for the suspicious traﬃc ﬂow detection, signature generation, malware analysis,
and reverse engineering processes.

Fig. 1: Defense system architecture

[Defender] The defender’s objective is to learn more and more regarding the attack tech-
nology and methodologies to be able to cease a next incoming adversarial move. In this
regard, it monitors the environment for attack detection and prevention purposes via com-
mon security tools and technologies. Intrusion detection (and possibly prevention) systems

Defender's Learning EngineFlowClassification,Traffic AnalysisSuspicious flow,  Attack trafficAttackInformation&SignatureDatabaseClient      Client with HIDSEnterpriseNetworkNetwork TapHoneypot  IDPSToward a Theory of Cyber Attacks

9

are amongst the most common type of defense technologies used for monitoring, incident
identiﬁcation, attack detection, and prevention on both host and network levels6. Virtual
and physical sensors are a common component of such systems used for data collection and
analysis. Also, the defender can take advantage of electronic decoys known as honeypots
for attack information assembly.

We give the defender the opportunity to learn from observed adversarial moves and
actions. The idea is that in the early phases of the game, the defender’s knowledge of the
attack is limited (almost non-existent). As the game proceeds, the defender’s detection
rates will be enhanced by witnessing enough number of attack samples. In order to study
the learning rate of the defender or the time that the system is ﬁnally trained and is able
to detect the adversary’s actions with probability almost 1, we consider a cumulative time-
varying function 0 ≤ f (.) ≤ 1 as the detection rate, which is a function of the total number
of times an adversary agent’s activity is captured or in other words, an attack sample
is given to the defender. The function f indicates the probability that given l samples
so far, the system detects and halts a new incoming adversarial move. The idea is the
more samples are given to the defender, the more accurate would be its attack signatures
which immediately reﬂects in the detection rate which is used by the system to ﬁlter future
adversarial moves.

In this regard, we assume that all incoming traﬃc passes through an inline NIDS im-
plementation7 which gives the defender the ability to be able to block an adversarial move
with some probability f (l) in which l represents the current realization of the attack by the
defender (i.e., based on so far observed and collected adversarial moves and attack infor-
mation). Moreover, we assume that a copy of all incoming traﬃc is given to the defender’s
traﬃc analysis and classiﬁcation engine via a network tap or a spanning port which pro-
vides the opportunity of detecting suspicious ﬂows with some probability. For simplicity,
however, we model this oﬄine analysis as a probabilistic sampling process [26] in which
each incoming adversarial move can be correctly sampled/labeled as suspicious with prob-
ability γ (therefore l = l + 1), and with 1 − γ, the defender misses the adversarial move or
it falsely labels it as benign. Note that γ reﬂects the classiﬁer’s accuracy in which it is the
probability that an incoming packet will be marked as suspicious traﬃc conditioned on the
fact that it is indeed malicious. If the defender fails in bringing an adversarial move to a
halt and therefore the attacker proceeds within the system with probability 1 − f (l), the
defender still has the opportunity to learn (i.e., l = l + 1) regarding the attack technology
with probability h via the defense agents (e.g., HPs, HIDS) implemented in the environ-
ment. Hence, if the adversary deals with an electronic decoy, or as a consequence of attack
activities, on an endpoint device equipped with a HIDS, the defender can learn regarding
the attack meaning that l = l + 1.

In summary, a progressive move by the defender (l = l + 1) leads to an increase in its
future detection rates f . This function gets updated in two diﬀerent manners: (1) if the
attack traﬃc is correctly labeled as suspicious on the network level with probability γ or

6 The ﬁrst one is known as host-based intrusion detection system (HIDS), and the latter is known

as network intrusion detection system (NIDS).

7 Meaning that network traﬃc directly passes through the IDPS sensors, and the system is capable
of session snipping, dropping/rejecting suspicious network activities and altering and sanitizing
the malicious content within a packet.

10

(2) if the attack activity leaks information on host levels with probability h (whether via
a honeypot or a host-based intrusion detection system installed on a fraction of defender
systems8). This incremental learning of the attack enables the defender to be able to bring
a new incoming adversarial move to a halt with probability f (l + 1). Notice that the
adversary’s chance of making a progressive move in the future time steps decreases as the
f function’s value increases over time.

3.2 Markov Model of the Game

The above description of the adversary-defender interactions can immediately be translated
into a Markov model. In summary, at each time step, the state of the system can be
identiﬁed with a tuple (i, l) in which i represents the adversary’s progress in the game,
and l delineates the defender’s knowledge of the attack methodologies and technology. The
adversary (or one of its agents) makes a move associated with a success probability pi. Note
that pi is indeed the attack success probability while there exists no opponent player in the
environment (i.e., the defender). In the meantime, the defender can block an adversarial
move, if it can detect it correctly or it can learn the attack technology if the attack is
observed via the defense system or the attacker is in touch with a defense system agent
(e.g., a honeypot) implemented in the environment.

i, l + 1

i + 1, l + 1

m4(i, l)

m3(i, l)

m1(i, l)

i, l

m2(i, l)

i + 1, l

Fig. 2: Possible transition probabilities at each state (i, l) in the Markov model of the game

Let us denote the occurrence of the event that an adversarial move comes to a halt
by the defense system with F and the occurrence of the event that it gets sampled (or

8 Notice that, in case of a honeypot, any adversarial move will lead to an increase in the defender’s
knowledge of the attack l since all incoming communications with an HP is suspicious, while for a
HIDS, not all the adversarial moves might be observed by the defense system. For simplicity, we
use a single parameter h as the probability of gaining information on host levels (mainly HPs).
However, one can easily separate the learning from HPs and the HIDS agents by considering
another parameter for HIDS agents h(cid:48) or in general, a cumulative function fh(cid:48) for the host-level
attack information and signatures (observed and gained from HIDS agents).

Toward a Theory of Cyber Attacks

11

labeled as suspicious) by the defense system with S. Notice that these two events are
independent, as we are assuming that the straining and ﬁltering happens online for each
detected incident whereas sampling deals with a copy of the actual traﬃc for attack analysis
and classiﬁcation. Therefore the following outcomes are possible at each timestep:
– (F ∧ sS): meaning that the adversasry’s move comes to a standstill by the defender
(based on current knowledge of the attack technology) but not sampled which happens
with probability f (l)(1 − γ), as a result, the Markov chain stays at state (i, l)

– ( sF ∧ sS): The adversasry’s move gets neither ﬁltered nor sampled, there exist three

possible scenarios for this case:
− if the adversary is dealing with an electronic decoy (i.e., a honeypot), the defender
has the oppurtunity to learn from this interaction meaning that the Markov model
transits from state (i, l) to (i, l + 1) with probability (1 − f (l))(1 − γ)h

− if the attacker makes a progressive move with probability pi then the Markov chain
transits to state (i + 1, l) with probability (1 − f (l))(1 − γ)pi. Note that this is a
“perfect move” for the adversary as in which the attacker gets one step closer to
the attack objective while the defensive system was not able to detect and locate
the adversary’s activities or it falsely labeled the activity as benign (false negative)
meaning that no information and knowledge regarding the attack technology is
leaked to the defender

− and if the attacker is not dealing with a defensive agent and not also make a
progressive move with probability (1−pi −h) the Markov chain stays at the current
state with probability (1 − f (l))(1 − γ)(1 − pi − h)

– (F ∧ S): If the attacker’s move gets both ﬁltered and sampled, the chain transits to the

state (i, l + 1) with probability γf (l)

– ( sF ∧ S): If the attacker’s action does not get ﬁltered but it gets sampled

− for a successful move with probability pi, the Markov chain transits to the state

(i + 1, l + 1) with probability (1 − f (l))γpi

− and for an unsuccessful move with probability (1 − pi), the Markov chain transits

to the state (i, l + 1) with probability (1 − f (l))γ(1 − pi)

In summary, the transition probabilities can be expressed by:

m1(i, l) = P rob((i, l) → (i, l)) = f (l)(1 − γ) +

(1 − f (l))(1 − γ)(1 − pi − h)

m2(i, l) = P rob((i, l) → (i + 1, l)) =

(1 − f (l))(1 − γ)pi
m3(i, l) = P rob((i, l) → (i + 1, l + 1)) =

(1 − f (l))γpi
m4(i, l) = P rob((i, l) → (i, l + 1)) =

(1 − f (l))(1 − γ)h + f (l)γ +
(1 − f (l))γ(1 − pi)

(1)

(2)

(3)

(4)

Fig. 2 depicts the possible transitions at each state of the Markov model. The initial
state of the game is (0, 0) which it shows the starting point of the attacker in the game, and

12

the zero knowledge of the defender at the beginning (e.g., a zero-day vulnerability/exploit).
As the game evolves, reaching state k is in favor of the adversary which can happen via
horizontal m2 or diagonal m3 transitions. In the meantime, the defender’s knowledge of
the attack technology and signatures improves through transitions via m3 and m4 in which
the number of attack samples provided to the defender, i.e., l increases and the function f
gets updated consequently.

4 Security Analysis

4.1 Adversarial Containment

The goal of the adversary is to reach a winning state (k, l) for some l ≥ 0 within a limited
time budget Tbud in which k represents the attack objective which is deﬁned based on
various attack scenarios9, and time budget Tbud expresses the number of transitions in the
Markov model, i.e., the total number of adversarial moves.

The objective of the defender is to learn more and more regarding the attack tech-
nologies (e.g., malicious attack payloads), in order to be able to recognize and halt future
moves by the attacker. That is, the defender hopes to push level l – the number of so
far discovered adversarial moves– higher and higher, leading to a higher probability f (l)
of successfully detecting and ﬁltering a next adversarial action. The ultimate goal of the
defender is to have a good enough learning mechanism in place such that the probability
of reaching k is negligible in k (i.e., exponentially small in k) and only polynomial in Tbud.
In other words, the adversary cannot reach a winning state in practice.

We deﬁne w(k, Tbud) as the probability of the adversary to reach a state (k, l) for some
l ≥ 0 within time budget Tbud. In the next subsections we analyze this probability. We
assume a parameter p = pi for all i, i.e., we simply give the adversary the advantage of
using p = supi pi instead of the smaller individual pi values. We want to ﬁnd a tight upper
bound and determine for which model parameters γ, h, p, and learning rate f (l) the upper
bound proves the desired asymptotic

w(k, Tbud) = poly(Tbud)negl(k).

(5)

We notice that in this case, even though the adversary cannot reach a large k because
w(k, Tbud) is negligible in k, the adversary may reach a (much) smaller k. We are interested
in the order of magnitude of this value as a sub-objective the adversary is able to reach.
For this reason, we deﬁne kc as the solution of

w(kc, Tbud) = 1/2.

For instance, kc could reﬂect the order of magnitude of the number of compromised nodes
to which the adversary is contained. By using w(k, Tbud) = poly(Tbud)negl(k) we can show10
‘containment parameter’ kc = O(log Tbud).

9 For instance, k could be the total number of nodes the adversary wishes to compromise before

using the collective of these nodes in the next attack phases (e.g., botnet applications).

10 This implies that with signiﬁcant probability the adversary can reach an attack objective at
most O(log Tbud) (e.g., compromising at most O(log Tbud) nodes). Therefore, if the adversary

Toward a Theory of Cyber Attacks

13

Our analysis will show that for a stagnating learning rate (i.e., 1 − f (l) remains larger
than some constant τ > 0) the adversary cannot be contained (w(k, Tbud) does not follow
the desired asymptotic). For a learning rate f (l) ≥ 1 − O(1/l2) which approaches 1 fast
enough, we are able to prove the desired asymptotic. We also develop tight lower and
upper bounds on w(k, Tbud) and show an eﬃcient algorithm which is able to compute these
bounds with O(k · Tbud) complexity.

4.2 Time budget measured in real time

We ﬁrst discuss the eﬀect of translating the time budget Tbud, measured in Markov model
transitions, to a time budget which is measured in real time (seconds) as this makes sense
in practice. If the adversary wins as soon as k nodes are compromised and if each Markov
model transition takes ∆ seconds, then at most k moves can be made by each of the
compromised nodes in parallel in ∆ seconds. With a time budget of T seconds, this implies
Tbud ≤ T k/∆. This means that the desired asymptotic (5) translates into

w(k, Tbud) ≤ poly(T k/∆)negl(k).

This is poly(T )negl(k) and again implies that in practice the adversary cannot reach k
since the probability of reaching k is negligible in k and only polynomial in the physical
time budget T .

In some scenarios each adversarial move costs a certain amount, say C USD, for example
due to having to rent a virtual machine in order to remotely launch or execute an attack
move. In such a case the time budget Tbud, measured in Markov model transitions, directly
translates into C · Tbud, the amount of USD the adversary is restricted to. Such conversion
allows one to reason about economically constrained adversaries.

4.3 Stagnating learning rate

Suppose that the learning rate stagnates in that 1 − f (l) ≥ τ for some τ > 0. This means
that f (l) ≤ 1 − τ and as a result at least a fraction τ goes undetected.

Theorem 1 (Stagnating learning rate). The attacker’s probability w(k, Tbud) of win-
ning a game in which the defender’s learning rate stagnates in that 1 − f (l) ≥ τ is given
by

w(k, Tbud) ≥ 1 − k(1 − τ p)Tbud/k.

(6)

For a stagnating learning rate, the attacker almost always wins the security game, i.e.,
w(k, Tbud) ≈ 1 even for relatively small time budgets Tbud = c · k for some multiplicative
factor c ≥ 1. This is because 1 − w(k, Tbud) ≤ k(1 − τ p)Tbud/k is exponentially small in
Tbud/k = c.

In order to have the desired asymptotics (5), the learning rate should not stagnate, i.e.,

we need 1 − f (l) → 0 for l → ∞.

would not have the advantage of using p = supi pi, then it would (compromise less nodes and)
only use parameters p0, . . . , pO(log Tbud) in the Markov game. If these ﬁrst few pi values do not
diﬀer much, then a constant p becomes a realistic assumption which does not give the adversary
that much advantage.

14

4.4 Formula for w(k, Tbud)

The theorem below characterizes w(k, Tbud) in a closed form expression. In the next sub-
section, we show that neglecting the sum over T of B values leads to tight upper and
lower bounds. The remaining sum over A values turns out to correspond to a simpler
Markov model which can be used to compute this sum in O(k · Tbud) time using dynamic
programming.

Theorem 2 (Closed Form Winning Probability). The probability w(k, Tbud) of win-
ning is equal to

Tbud−1
(cid:88)

min{L,(k−1)}
(cid:88)

L=0

B=0

m2(L) + m3(L)
1 − m1(L)

· A ·

(Tbud−1)−[(k−1)+L−B]
(cid:88)

B

T =0

with A and B substituted by

(cid:88)

L−1
(cid:89)

(cid:16)

A =

b1,...,bL∈{0,1}s.t.
l=1 bl=B

(cid:80)L

l=0

·

(cid:88)

bl+1

m3(l)
1 − m1(l)

+ (1 − bl+1)

(cid:17)

m4(l)
1 − m1(l)

L
(cid:89)

l=0

(cid:16) m2(l)

(cid:17)gl−1

1 − m1(l)

, and

(cid:80)L

g0≥1,...,gL≥1s.t.
l=0(gl−1)=(k−1)−B
L
(cid:89)

(cid:88)

(cid:19)

(cid:18)tl + gl − 1
tl

t0,...,tLs.t.
(cid:80)L
l=0 tl=T

l=0

B =

m1(l)tl (1 − m1(l))gl .

4.5 Upper and Lower Bounds on w(k, Tbud)

In the notation of Theorem 2, we deﬁne ¯w as

¯w(k, Tbud) =

Tbud−1
(cid:88)

min{L,(k−1)}
(cid:88)

L=0

B=0

m2(L) + m3(L)
1 − m1(L)

· A.

(7)

Theorem 3 (Upper and Lower Bounds on w). For v ≥ k, we have

¯w(k, Tbud/v) · (1 − (k +

Tbud
v

)(1 − γ)

v

k ) ≤ w(k, Tbud) ≤ ¯w(k, Tbud).

(8)

In particular for v = k ln((Tbud +k)/(cid:15))/ln(1/(1−γ)), we have w(k, Tbud) ≥ ¯w(k, Tbud/v)·
(1−(cid:15)). Upper bound (7) proves that if ¯w(k, Tbud) has the desired complexity, i.e., ¯w(k, Tbud) =
poly(Tbud)negl(k), then w(k, Tbud) inherits this desired asymptotics. And vice versa, the
lower bound in (8) for the special choice of v proves that if w(k, Tbud) has desired asymp-
totics w(k, Tbud) = poly(Tbud)negl(k), then also ¯w(k, Tbud) ≤ w(k, Tbud · v)/(1 − (cid:15)) =
poly(Tbud · v) · negl(k) = poly(Tbud)negl(k). We conclude that in order to ﬁnd out which

Toward a Theory of Cyber Attacks

15

learning rates lead to the desired asymptotics for the defender, we only need to study which
learning rates lead to ¯w(k, Tbud) = poly(Tbud)negl(k).

The advantage of studying ¯w(k, Tbud) is having eliminated expression B in Theorem 2
which represents the eﬀect of self-loops (and turns out to be very ineﬃcient to evaluate as an
exact expression). Probability ¯w(k, Tbud) corresponds to a Markov model without any self-
loops and with horizontal transition probabilities m2(l)/(1 − m1(l)) = (1 − γ)α(l), diagonal
transition probabilities m3(l)/(1 − m1(l)) = γα(l), and vertical transition probabilities
m4(l)/(1 − m1(l)) = 1 − α(l), where

α(l) =

p(1 − f (l))
γ + (1 − γ)(p + h)(1 − f (l))

.

(9)

Algorithm 1 depicts how ¯w(k, Tbud) can be computed in O(k · Tbud) time using dynamic
programming – and this is what we use in section 4.9 to evaluate ¯w(k, Tbud) for diﬀerent
learning rates.

Algorithm 1 ¯w Computation

pr ← zeros(k − 1, Tbud − 1)
pr[0, 0] ← 1
for i ← 0, Tbud − 1 do
if i! = 0 then

pr[0, i] ← (1 − α(i − 1))pr[0, i − 1]

end if
for j ← 1, k − 1 do
if i == 0 then

pr[j, i] ← [(1 − γ)α(0)]j

1: function ¯w(k, Tbud)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end function

end for
return ¯w

end for

end if

else

end for
¯w ← 0
for i ← 0, Tbud − 1 do

¯w ← ¯w + α(i)pr[k − 1, i]

pr[j, i] ← (1 − γ)α(i)pr[j − 1, i] + γα(i − 1)pr[j − 1, i − 1] + (1 − α(i − 1))pr[j, i − 1]

4.6 Analyzing Learning Rate f (l) = 1 − d

(l+2)2

We further upper bound ¯w(k, Tbud) in the next theorem which shows that small enough
learning rates f (l) ≥ 1 − d/(l + 2)2 will attain our desired asymptotic (as explained as a
consequence after the theorem). In Section 4.7 we will discuss what happens if there is an
initial period during which the adversary already starts making progress in the game (e.g.,

16

compromising nodes) without the defender being aware or not having learned anything.
For example, f (l) = 0 for l ≤ L∗ and f (l) ≥ 1 − d
(l−L∗)2 for l > L∗. Here, L∗ represents this
initial period; only after a suﬃcient level L∗ is reached, i.e., a suﬃcient number of attack
samples (e.g., malware payloads) have been collected, the learning rate increases.

Theorem 4 (Upper Bound on ¯w). Assume that there exists a d ≥ 0 such that 1−f (l) ≤

d for all l ≥ 0, or equivalently, β(l) =

d ≤ 1 for all l ≥ 0. Let θ ≥ (1 − γ)/γ. Then,

(cid:113) 1−f (l)

¯w(k, Tbud) ≤ (1 + θ−1)

Tbud−1
(cid:88)

[θpd]k

L
(cid:89)

(cid:18)

L=0

l=0

1 + (1 + θ−1)

β(l)
1 − β(l)

(cid:19)

.

Let

(cid:90) Tbud−1

c =

l=0

β(l)
1 − β(l)

dl +

β(0)
1 − β(0)

.

Then, minimizing the above upper bound with respect to θ proves

c ≤

1 − γ
γ

k ⇒ ¯w(k, Tbud) ≤ (1 − γ)−1Tbude(1−γ)−1c[

1 − γ
γ

pd]k.

Applying this statement for

gives

1 − f (l) ≤

d
(l + 2)2

Tbud ≤ e−1+k(1−γ)/γ

⇒ ¯w(k, Tbud) ≤

e1/(1−γ)
1 − γ

· T 1+1/(1−γ)

bud

· [

1 − γ
γ

pd]k.

For f (l) ≥ 1 − d

p(1−γ) , the theorem shows that if Tbud < exp(k),
then w(k, Tbud) ≤ ¯w(k, Tbud) = poly(Tbud)negl(k) satisfying the desired asymptotics. For
analyzing other learning rates, the more general upper bound of the theorem can be used.

(l+2)2 with d <

γ

4.7 Delayed learning

In practice, learning only starts after seeing a suﬃcient number of attack samples. That is,
f (l) = 0 for l < L∗ for some L∗ after which f (l) starts to converge to 1. What would be
the attacker state in the game during this initial phase, where no learning takes place (for
instance, how many nodes will be compromised)?

Theorem 5 (Delayed learning). Suppose f (l) = 0 for l < L∗. Deﬁne u(k∗, L∗) as the
probability that L∗ is reached by a state (i, L∗) with i ≤ k∗, where we give the adversary
the advantage of an unlimited time budget. Then, for k∗ ≥ L∗ + 1,

u(k∗, L∗) ≥ 1 − L∗[

(1 − γ)p
1 − (1 − γ)(1 − (p + h))

k∗−1
L∗ .

]

Toward a Theory of Cyber Attacks

17

Let w(cid:48)(k, Tbud) correspond to learning rate f (l + L∗) as a function of l (e.g., 1 − f (l +

L∗) ≤ d

(l+2)2 ). Then, for k∗ ≥ L∗ + 1,

w(k, Tbud) ≤ (1 − u(k∗, L∗)) + w(cid:48)(k − k∗, Tbud),

(10)

where 1 − u(k∗, L∗) is the probability that > k∗ is achieved during the initial phase and
where w(cid:48)(k − k∗, Tbud) is the probability that another k − k∗ progressive adversarial moves
happen after the initial phase with time budget Tbud.

The lower bound on u(k∗, L∗) can be further simpliﬁed by using p ≤ 1−h which implies

u(k∗, L∗) ≥ 1 − L∗[(1 − γ)(1 − h)]

k∗−1
L∗ .

Since 1 − u(k∗, L∗) is exponentially small in k∗/L∗, this shows that very likely k∗ =
O(L∗). This makes common sense because initially, every adversarial move has a signiﬁcant
probability of success since the defender’s knowledge of the attack is very limited in the
beginning (the attack is still unknown to the defender, in other words, no valid attack
signature is formed yet). If we achieve desired asymptotic (5) for w(cid:48)(k, Tbud), then the
resulting containment parameter for w(k, Tbud) (see Section 4.1) is kc = O(L∗ + log Tbud).
It is very important for the defender to keep L∗ small. A larger L∗ leads to a longer initial
phase during which the adversary keeps on making progressive moves of the order of O(L∗).

4.8 Capacity Region

In this subsection, we show that as a result of our analysis we are able to prove statements
like the one in the following deﬁnition which deﬁnes a ‘capacity region’ of combinations
of parameters deﬁning the adversarial time budget, the probability of winning, and k
which characterizes what it means to be in a winning state. The deﬁnition describes what
was previously called ‘desired asymptotics’ since a non-empty capacity region shows that
an exponentially small probability of winning O(2−s) is achieved if k = O(s + log Tbud)
implying that the probability of winning is poly(Tbud)negl(k).

Deﬁnition 1. Suppose there exist vectors δ, µ, ξ, and the all-one vector 1, such that, for
all k ≥ 0 (characterizing a winning state for the adversary) and for all s ≥ 0 and t ≥ 0
satisfying

sδ + tµ ≤ k1 + ξ,

we have the following security guarantee: If the attacker has a time budget ≤ 2t, then his
probability of reaching a winning state is at most ≤ 2−s.

We say that (δ, µ, ξ) describes a capacity region for {(s, t) : s, t ≥ 0}. Its deﬁnition
implies that for a ﬁxed k and time budget 2t, the probability of winning is at most ≤ 2−s(k,t),
where

s(k, t) = max{s ≥ 0 : sδ + tµ ≤ k1 + ξ}.

(11)

Similarly, for a time budget 2t, the probability of winning is ≤ 2−s if a winning state is
characterized by some k ≥ k(s, t), where

k(s, t) = min{k ≥ 0 : sδ + tµ ≤ k1 + ξ}.

(12)

18

This deﬁnition generalizes the deﬁnition of security capacity used in MTD games [18]
where only a single equation of the form s + t ≤ δ · k needs to be satisﬁed and where
δ is deﬁned as the “security capacity”. The deﬁnition of capacity region may be of inter-
est for other more general defender-adversarial games where a Markov model is used to
characterize defender and adversarial moves.11

The deﬁnition of capacity region is of interest for a couple of reasons. First, it clearly
describes the limitation of the adversary: The capacity region explicitly characterizes the
probability of winning as a function of t and k, see (11). In practice, the attacker’s time
budget is ≤ 2t, limited to some “small” t, e.g., t = 80, 128, and using such a ﬁxed t makes
the probability of winning only a function of k.

Second, the capacity region can also be used to compute the containment parameter
kc, deﬁned as the solution k(s, t) in (12) where s is set to s = 1 which corresponds to a
probability of winning equal to 1/2. This allows us to understand how

kc = k(1, t) = O(t) = O(ln Tbud)

depends on the parameters that describe the overall Markov model.

Third, the deﬁnition allows us to trivially compose capacity regions of Markov models
corresponding to diﬀerent learning rates. For instance, in practice, an attacker may use
several exploits or attack vectors and use each attack vector to advance its footprint and
increase the number of compromised nodes. For each attack vector i, the defender develops a
learning rate fi(l). In essence, the defender plays individual games with each attack vector,
and the adversary wins the overall game if the individual games lead to ki compromised
nodes such that k = (cid:80)

i ki. The following theorem makes this argument precise.

Theorem 6. Let fi(l), 1 ≤ i ≤ a, be learning rates corresponding to Markov models
describing diﬀerent defender-adversary games that are played simultaneously. Let ki(s, t)
for the i-th game be deﬁned as in (12). Let Tbud ≤ 2t be the overall time budget of the
adversary. Then the probability of reaching a state with ki(s + ln a, t) compromised nodes in
the i-th game is at most ≤ 2−(s+ln a) = 2−s/a. Since the transition probabilities in each of
the Markov models are independent from the number of compromised nodes, the probability
of reaching k = (cid:80)a

i=1 ki(s + ln a, t) is at most

a
(cid:89)

≤ 1 −

(1 − 2−s/a) ≤ 2−s.

i=1

Notice that for a simultaneous adversarial games an exponentially small probability of
winning O(2−s) is achieved for k = O(s + ln a + ln Tbud). In this parallel game, if the time
budget is measured in real time T , then (as before) Tbud ≤ T k/∆ for k = (cid:80)a
i=1 ki(s +
ln a, t) and we require T ≤ ∆
i=1 ki(s + ln a, t) is dominated by
maxa
i=1 ki(s + ln a, t) ≥ k/a. In other words, the attacker keeps on searching for an attack
vector which can be used to achieve a large fraction12 of the desired k, and the defender

k 2t. As a ﬁnal note, k = (cid:80)a

11 A more general deﬁnition which captures winning states characterized by parameter vectors k,

rather than a single k, can also be formulated.

12 Due to the defender either starting learning after a too long initial period or, once learning

starts, not learning fast enough (in order to attain the desired asymptotic).

Toward a Theory of Cyber Attacks

19

tries to learn how to recognize attack vectors as fast as possible in order to contain these
suﬃciently.

As an example of a capacity region, we translate Theorem 4 for 1 − f (l) ≤ d

(l+2)2 in

terms of a capacity region:

Corollary 1. For learning rate 1−f (l) ≤ d
by 2-dimensional vectors

(l+2)2 we have a capacity region (δ, µ, ξ) deﬁned

δ = (0, q ln 2), where q =

ln(

(cid:20)

γ
pd(1 − γ)

(cid:21)−1
)

,

µ = (

, q(ln 2)(1 +

γ ln 2
1 − γ
γ
1 − γ

ξ = (−

, q(−

1
1 − γ

+ ln

1
1 − γ

).

1
1 − γ

)),

As a second example of working with capacity regions, we translate (10) in Theorem 5

for delayed learning:

Corollary 2. Suppose f (l) = 0 for l < L∗ and let (δ(cid:48), µ(cid:48), ξ(cid:48)) be the capacity region corre-
sponding to learning rate f (l + L∗) as a function of l. Suppose that

(cid:20)

z =

ln(

1 − (1 − γ)(1 − (p + h))
(1 − γ)p

)

(cid:21)−1

≥ 0.

Then, ﬁrst, for all s ≥ max{0, 1/z ln 2 − ln(2L∗)ln 2} and t ≥ 0 satisfying

s[δ(cid:48) + L∗(ln 2)z1] + tµ(cid:48) ≤ k1 + [ξ(cid:48) − δ(cid:48) + (L∗(ln 2L∗)z − 1)1]

and, second, for all13 0 ≤ s ≤ 1/z ln 2 − ln(2L∗)ln 2 and t ≥ 0 satisfying

sδ(cid:48) + tµ(cid:48) ≤ k1 + [ξ(cid:48) − δ(cid:48) − (L∗ + 1)1],

we have the following security guarantee: If the attacker has a time budget ≤ 2t, then his
probability of reaching a winning state is at most ≤ 2−s.

The corollary conﬁrms that the containment parameter for delayed learning (for both

cases) is kc = O(L∗ + log Tbud).

4.9 Parametric analysis of Learning Rate f (l) = 1 − d

(l+2)a

In this subsection, we use Algorithm 1 to evaluate ¯w(k, Tbud) based on diﬀerent learning
rates and Tbud = 10, 000 logical timesteps. Fig. 3 depicts ¯w(k, Tbud) as a function of k for
several γ and several learning rates of the form α(l) = (l + 1)−a. As it can be seen from
this plot, a proper defense mechanism (with suﬃciently fast converging learning rates and

13 Notice that 1/z ln 2 − ln(2L∗)ln 2 ≥ 0 if and only if L∗ ≤ 1

2 e1/z = 1−(1−γ)(1−(p+h))

2(1−γ)p

, which is

large for small (1 − γ)p.

20

large enough γ values) substantially decreases the adversary’s chances of making progress
in the game.

Fig. 4 depicts how the containment parameter kc (computed using ¯w(k, Tbud)) depends
on α(l) = (l + 1)−a. For a ≥ 0.9, kc ≤ 20 is very small making it impossible for the
adversary to reach high-value attack objectives (e.g., total number of infected nodes in a
malware propagation game).

Fig. 3: ¯w(k, Tbud) based on diﬀerent values of γ and learning functions α(l) = (l + 1)−a
where Tbud = 10, 000 logical timesteps

Fig. 5 depicts the chances of attack prosperity in two diﬀerent cases p = 0.1, and p = 1.0
(see (9)) while γ = 0.5 meaning that half of the adversarial moves will be observed by the
defender at the network levels (i.e., will be marked as suspicious on the ﬂow classiﬁer).

The impact of delayed learning on the adversarial containment parameter kc is shown
in Fig. 6 for two diﬀerent cases. In the ﬁrst scenario, the learning starts immediately (with
no delay, i.e., L∗ = 0), and in the second case, the learning commences after missing the
ﬁrst 100 attack samples (i.e., L∗ = 100).

𝛾 = 0.2𝛾 = 0.8𝛾 = 0.6𝛾 = 0.4Toward a Theory of Cyber Attacks

21

Fig. 4: kc for diﬀerent classes of learning functions α(l) = (l + 1)−a where Tbud = 10, 000
logical timesteps

Fig. 5: ¯w(k, 10000) for two cases p = 0.1, and p = 1.0 with γ = 0.5 and learning function
1 − f (l) = (l + 1)−a where a = 0.6 and a = 1 respectively.

0.600.650.700.750.800.850.900.951.00a020406080100120kcγ=0.2γ=0.8𝛾 = 0.522

Fig. 6: The impact of delayed learning on kc for diﬀerent classes of learning functions
α(l) = (l + 1)−a where Tbud = 10, 000 logical timesteps

5 Case Studies

In this section, we walk through two examples of attack-defense games as case studies to
show how real-world cyber attack scenarios can be translated into our presented framework.
Their security analsyis is worked out in the appendix.

5.1 Malware propagation and Botnet construction

Consider a statically addressed network of size N in which a fraction of the hosts K suﬀers
from a zero-day vulnerability only known to an adversary. The attacker’s objective is to
locate such hosts in the network, based on a target discovery strategy σ (e.g., a random scan
scheme, or a hitlist) associated with a probability pi, and infect them during the Window
of Vulnerability (WoV) time as the number of vulnerable systems is not yet shrunken to
insigniﬁcance and the attacker’s exploit is useful in this period. The game starts with one
infected machine as the “Patient Zero”. The attacker desires to take control of at least k
out of K vulnerable hosts, meaning that the winning state for the adversary is deﬁned as
(i, l) = (k, ∗). In the meantime, the defender’s objective is to generate an attack signature
to be able to ﬁlter next adversarial attack traﬃc (see Fig. 1). The defender’s knowledge
of the attack can be increased in two manners: (1) if an adversary agent hits a honeypot
with probability h (assuming that there exist in total H honeypots in the environment,
therefore, h = H/N for a memoryless blind scan strategy), or (2) if the attack traﬃc is
correctly labeled as suspicious on the network (classiﬁer) level with probability γ 14. In
either of these cases, the defender’s detection rate f (l) in which l is the number of so far

14 This is a typical architecture in automatic signature generation schemes (e.g., see [11, 13, 22, 27]).

𝛾 = 0.5Toward a Theory of Cyber Attacks

23

collected attack samples will be enhanced. This function represents the probability that
given l samples so far, the system detects a new incoming malicious packet and ﬁlters it on
the ﬂy. We also consider a skillful attacker (for instance taking advantage of a polymorphic
worm) as deﬁned in previous sections, and therefore, having only a few attack samples is
not enough for signature generation.

Algorithm 2 Malware propagation game simulator
1: function Simulator(N , K, H, k, σ, ε, γ, f (.), ϑ)
2:
3:
4:
5:
6:

I = ∅; l = 0;
droppingOut = False;
winningState = False;
while not droppingOut and not winningState do

pi←− probe(N , σ);

targetHost

7:

ﬁltered,sampled

f (l),γ
←−−−transmit(ε,targetHost);

if (not ﬁltered and targetHost ∈ H) or sampled then

l += 1;
update f (l);
if f (l) ≥ 1 − ϑ then

Output "Game is ﬁnished. Attacker dropped out! Defender won!";
droppingOut = True;

end if
if targetHost ∈ K and not ﬁltered then

I = I ∪ targetHost;
if |I|≥ k then

Output "Game is ﬁnished. Adversary won!";
winningState = True;

end if

8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
end if
22:
23:
end while
24: end function

end if

The system state (i, l) represents the total number of infected machines and the total
number of captured attack traﬃc (i.e., malware samples) so far by the defense system.
The attacker’s view of the system state will change if an agent’s eﬀort in infecting a new
node is successful (or if it possibly loses its control over an agent). The defender’s view
gets updated as a consequence of discovering an adversarial move (i.e., an attack/malware
sample). Algorithm 2 shows the above description of the game in which the game simulator
takes N , K, H, k, and an adversarial target discovery strategy σ and an exploit ε, in addition
to the detection probability f (l), the sampling rate γ, and an acceptable “threshold” 1 − ϑ
as the input and outputs if the attacker wins the game. The termination rule is whether
the attacker compromises its desired number of hosts k or if the attacker decides not to

24

play anymore15, that is f (l) ≥ 1 − ϑ. Note that the transmit method returns a tuple, i.e.,
if the transmitted packet by the attacker got ﬁltered by IDPS or got sampled at the ﬂow
classiﬁer level.

5.2 A Moving Target Defense Game

As another example, we consider the “Multiple-Target Hiding” (MTH) game introduced in
[18] with minor modiﬁcations to the game. In the MTH game studied in [18], the adversary
is interacting with a probabilistic defender taking advantage of a moving target defense
strategy in which it reallocates/shuﬄes its resources at each time step of the game with
some probability λ (for instance, consider an IP hopping strategy). In order to win the
game, the adversary needs to locate k out of K sensitive targets/resources distributed in
the environment while there exist in total N “locations”. The defender moves by reallocating
a target causing the attacker to redo its search for the locations. The attacker moves by
selecting one of the N possible locations and examining whether it corresponds to one of
the targets or not. Notice that MTD strategies are usually costly for the defender as they
have an immediate impact on the availability of the resources and system performance.
Therefore, instead of a randomized defense strategy (i.e., a probabilistic move at each time
step), we consider that the defender issues a move if the number of observed adversarial
actions reaches a threshold L∗ based on which an attack detection signal is being generated.
The system state (i, l) represents the total number of located target machines by the
attacker and the total number of recognized adversarial moves so far by the defender. The
attacker’s view of the system state will change if it successfully ﬁnds a sensitive target based
on its target discovery strategy σ, while the defender’s view gets updated as a consequence
of discovering an adversarial move (whether on network levels with probability γ or on host
levels with probability h via a defense agent). Algorithm 2 shows the above description of
the game in which the game simulator takes N , K, H, k, and an adversarial target discovery
strategy σ, in addition to the sampling rate γ, and a “threshold” L∗ as the input and
outputs if the attacker wins the game. The termination rule is whether the attacker ﬁnds
its desired number of sensitive targets k or if the defender reallocates its resources as it
discovers enough amount of attack evidence.

6 Concluding remarks and future directions

This presented work is an attempt at constructing a theory of security and developing
a general framework for modeling cyber attacks prevalent today, including opportunistic,
targeted and multi-stage attacks while taking practical constraints and observations into
consideration. To this end, we have modeled the interactions of an adversary (and possibly
its agents) with a defensive system during the lifecycle of an attack as an incremental online
learning game. In comparison with the available works in this area, which are too simple,
speciﬁc and static to be used in almost any practical situation, our presented framework, to
the best of our knowledge, is the most comprehensive and realistic one which can be used

15 For instance, the attacker concludes that not enough gain can be made in a reasonable time
because of not being able to make a progressive move in the game due to defender’s high-value
detection rates that keep on improving.

Toward a Theory of Cyber Attacks

25

Algorithm 3 MTD game simulator
1: function Simulator(N , K, H, k, σ, γ, L∗)
2:
3:
4:
5:

I = ∅; l = 0;
reallocateResources = False;
winningState = False;
while not reallocateResources and not winningState do

6:

observed,targetHost

(γ,h),pi←−−−−− locate(N , σ);

if observed then

l += 1;
if l ≥ L∗ then

7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
end if
21:
22:
end while
23: end function

end if

end if

end if
if targetHost ∈ K then
I = I ∪ targetHost;
if |I|≥ k then

reallocateResources = True;
I = ∅;
Output "Game is ﬁnished. Defender won!";

winningState = True;
Output "Game is ﬁnished. Adversary won!";

26

to represent the dynamic interplay between the attacker and the defender. Unlike most of
the available research in this area which ignores entirely one player’s actions and strategies
(usually the defender), we have shown how the game evolves by taking both parties set of
available actions and strategies into consideration. More speciﬁcally, instead of considering
a “dummy defender” in our modeling, we gave it the opportunity to learn regarding the
attack technology incrementally. As time elapses, and the defender captures more attack
samples, it can reach better detection rates and accuracy. This learning rate indeed reﬂects
into higher quality attack signatures and detection rates which can be used to bring a next
adversarial move to a halt and hence to contain the adversary meaning that the adversary’s
probability of making a progressive move in the game decreases consequently.

By focusing on the most signiﬁcant and tangible aspects of sophisticated cyber attacks
i.e., (1) the amount of time it takes for the adversary to accomplish its mission and (2)
the success probabilities of fulﬁlling the attack objectives, we were able to study under
which circumstances the defense system can provide an eﬀective response that makes the
probability of reaching an attack objective k to be poly(Tbud)negl(k) in which Tbud is the
attacker time budget. This led us to the deﬁnition of a security capacity region as a metric
for gauging a defensive system’s eﬃciency from a security perspective. In particular, we
show that a stagnating learning rate allows the attacker to win meaning that being able
to reach its attack objective within a limited budget (e.g., time, US dollars), whether it is
constructing a botnet of any speciﬁc size or locating information on a distributed number of
nodes. The defender cannot wait for too long learning about a used attack vector/exploit,
and once learning starts it must continue learning with an associated detection probability
converging fast enough to 1. Our security analysis gives precise recommendations for the
defender, i.e., 1 − f (l) ≤ O(l−a) for some a ≥ 2 with a proof for a = 2 in our framework.
The attacker needs to ﬁnd just one attack vector/exploit for which the defender is too slow
to react or too slow in learning.

An essential venue of future work is to estimate the learning rate f (l) based on the
number of observed malicious acts given a “worst-case adversary”. Our framework lays the
foundation for such work and allows to give a worst-case probabilistic bound on the maximal
reached attack objective based on the estimated f (l). This, in turn, will give guidance to
the defender in how to allocate its resources. In addition, we notice that the learning rate
might not always be positive, for instance, when dealing with a delusive adversary who
maliciously engineers the training data to prevent a learner from generating an accurate
classiﬁer, even if the training data is correctly labeled [23]. Therefore, in case of noise
injection attacks, such as deliberately crafted attack samples to mislead the defender’s
learning engine, and in general a delusive adversary, the learning engine’s false positive
rates should be taken into consideration, and we leave this problem for future studies.

Toward a Theory of Cyber Attacks

27

References

1. Alpcan, T., Basar, T.: An intrusion detection game with limited observations. In: 12th Int.

Symp. on Dynamic Games and Applications, Sophia Antipolis, France. vol. 26 (2006)

2. Bertino, E., Islam, N.: Botnets and internet of things security. Computer 50(2), 76–79 (2017)
3. Avast blog: Seven new Mirai variants and the aspiring cybercriminal behind them (2018),

https://bit.ly/2GVoY2c, [Accessed Nov-2018]

4. Carroll, T.E., Grosu, D.: A game theoretic investigation of deception in network security.

Security and Communication Networks 4(10), 1162–1172 (2011)

5. Chen, L., Leneutre, J.: A game theoretical framework on intrusion detection in heterogeneous
networks. IEEE Transactions on Information Forensics and Security 4(2), 165–178 (2009)
6. Cisco: Annual Cybersecurity Report (2018), https://bit.ly/2ul3dOM, [Accessed October-2018]
7. Connell, W., Menasce, D.A., Albanese, M.: Performance modeling of moving target defenses
with reconﬁguration limits. IEEE Transactions on Dependable and Secure Computing (2018)
8. Debar, H.: An introduction to intrusion-detection systems. Proceedings of Connect 2000

(2000)

9. Inoue, D., Yoshioka, K., Eto, M., Hoshizawa, Y., Nakao, K.: Automated malware analysis
system and its sandbox for revealing malware’s internal and external activities. IEICE trans-
actions on information and systems 92(5), 945–954 (2009)

10. Jin, C., Valizadeh, S., van Dijk, M.: Snapshotter: Lightweight intrusion detection and preven-
tion system for industrial control systems. In: 2018 IEEE Industrial Cyber-Physical Systems
(ICPS). pp. 824–829. IEEE (2018)

11. Kim, H.A., Karp, B.: Autograph: Toward automated, distributed worm signature detection.

In: USENIX security symposium. vol. 286. San Diego, CA (2004)

12. Kolosnjaji, B., Zarras, A., Webster, G., Eckert, C.: Deep learning for classiﬁcation of malware
system call sequences. In: Australasian Joint Conference on Artiﬁcial Intelligence. pp. 137–149.
Springer (2016)

13. Li, Z., Sanghi, M., Chen, Y., Kao, M.Y., Chavez, B.: Hamsa: Fast signature generation for
zero-day polymorphic worms with provable attack resilience. In: Security and Privacy, 2006
IEEE Symposium on. pp. 15–pp. IEEE (2006)

14. Liang, X., Xiao, Y.: Game theory for network security. IEEE Communications Surveys &

Tutorials 15(1), 472–486 (2013)

15. Liang, Z., Sekar, R.: Fast and automated generation of attack signatures: A basis for build-
ing self-protecting servers. In: Proceedings of the 12th ACM conference on Computer and
communications security. pp. 213–222. ACM (2005)

16. Lindell, Y., Katz, J.: Introduction to modern cryptography. Chapman and Hall/CRC (2014)
17. Lye, K.w., Wing, J.M.: Game strategies in network security. International Journal of Infor-

mation Security 4(1-2), 71–86 (2005)

18. Maleki, H., Valizadeh, S., Koch, W., Bestavros, A., van Dijk, M.: Markov modeling of moving
target defense games. In: Proceedings of the 2016 ACM Workshop on Moving Target Defense.
pp. 81–92. ACM (2016)

19. Manshaei, M.H., Zhu, Q., Alpcan, T., Bacşar, T., Hubaux, J.P.: Game theory meets network

security and privacy. ACM Computing Surveys (CSUR) 45(3), 25 (2013)

20. Marpaung, J.A., Sain, M., Lee, H.J.: Survey on malware evasion techniques: State of the art
and challenges. In: Advanced Communication Technology (ICACT), 2012 14th International
Conference on. pp. 744–749. IEEE (2012)

21. Milajerdi, S.M., Gjomemo, R., Eshete, B., Sekar, R., Venkatakrishnan, V.: Holmes: real-
time apt detection through correlation of suspicious information ﬂows. arXiv preprint
arXiv:1810.01594 (2018)

28

22. Newsome, J., Karp, B., Song, D.: Polygraph: Automatically generating signatures for poly-
morphic worms. In: Security and privacy, 2005 IEEE symposium on. pp. 226–241. IEEE (2005)
23. Newsome, J., Karp, B., Song, D.: Paragraph: Thwarting signature learning by training mali-
ciously. In: International Workshop on Recent Advances in Intrusion Detection. pp. 81–105.
Springer (2006)

24. Provos, N., et al.: A virtual honeypot framework. In: USENIX Security Symposium. vol. 173,

pp. 1–14 (2004)

25. Rohloﬀ, K.R., Basar, T.: Stochastic behavior of random constant scanning worms. In: Com-
puter Communications and Networks, 2005. ICCCN 2005. Proceedings. 14th International
Conference on. pp. 339–344. IEEE (2005)

26. Sperotto, A., Schaﬀrath, G., Sadre, R., Morariu, C., Pras, A., Stiller, B.: An overview of ip
ﬂow-based intrusion detection. IEEE Communications Surveys and Tutorials 12(3), 343–356
(2010)

27. Tang, Y., Xiao, B., Lu, X.: Signature tree generation for polymorphic worms. IEEE transac-

tions on computers 60(4), 565–579 (2011)

28. Van Dijk, M., Juels, A., Oprea, A., Rivest, R.L.: Flipit: The game of “stealthy takeover”.

Journal of Cryptology 26(4), 655–713 (2013)

29. Zou, C.C., Gong, W., Towsley, D.: Code red worm propagation modeling and analysis. In:
Proceedings of the 9th ACM conference on Computer and communications security. pp. 138–
147. ACM (2002)

A Security Analysis of Case studies

A.1 Security Analysis of Case study 5.1

As in section 4, we investigated the role of the convergence rate of f (.) on the attacker’s
chances of winning the game, we know that for a stagnating learning rate, i.e., f (.) = 1 − τ ,
for some τ > 0, the adversary will always win. Therefore, f must not stagnate unless the
attacker decides to drop out of the game. To show how eﬀective a learning mechanism
could be with respect to containing the adversary’s progress in the game, we consider the
infamous CodeRed1v2 worm’s actual attack settings and parameters [25] as an example.
In the codeRed1v2 epidemic, we consider the address space to be N = 232 (the entire
IPv4 address space), the approximate number of nodes susceptible to the malware as
K = 350, 000 (i.e., p = 8.15 × 10−5), and the number of scans performed by an infected
machine to be 10, 188 scans per hour, and an initially one infected node at time zero.

Fig. 7 compares the total number of infections in the ﬁrst few hours of the epidemic (1)
using the well-accepted simple deterministic epidemic model [29] and (2) by simulating the
game using Algorithm 2 with no learning. Notice that this plot shows simulations and in
this sense, it depicts the average case; no information about the probability of a worst-case
can be extracted. The plot indicates that giving the attacker the advantage of p = supi pi
in our analysis is a reasonable assumption as it does not boost its progress in the game
signiﬁcantly.

Based on the simple deterministic epidemic model (see Fig. 8 (a)), we know that the
total number of infected machines reaches its maximum (i.e., 350,000) in less than 30 hours,
while with a learning function of the form f (l) = 1 − 1/(l/10, 000 + 1), the attacker has
control of less than 350 nodes (on average) and its progress is almost contained due to high
value detection rate of the defender (see Fig. 8 (b)).

Toward a Theory of Cyber Attacks

29

Fig. 7: Studying the role of p on reaching attack objective based on simple deterministic
epidemic model and simulations of Algorithm 2 with no learning, and p = supi pi

Fig. 8: (a) Simple deterministic epidemic model with the actual codeRed1v2 parameters.
(b) Simulation of the codeRed1v2 with the same parameters while the learning function is
of the form f (l) = 1 − 1/(l/10, 000 + 1) and γ = 0.01 as the classiﬁer’s accuracy

012345678Time (hours)0100200300400500600700800900kNumber of Infected Hosts vs. TimeClassical simple epidemic modelSimulated (No learning)051015202530Time (hours)0.00.51.01.52.02.53.03.5k1e5(a)051015202530Time (hours)050100150200250300350k(b)Number of Infected Hosts vs. Time30

Let us assume that the defender is interested in ﬁguring out the highest number of
infected nodes during a long window of time (e.g., a month) for a designated insigniﬁcant
(e.g., 2−128 ≈ 10−38) probability of reaching the attack objective by the adversary. Fig.
9 depicts ¯w(k, Tbud) (as an upper bound of w(k, Tbud)) when considering a ﬁxed learning
function of the form f (l) = 1 − 1/(l/1000 + 1), while p = 8.15 × 10−5, and γ = 0.05
as the classiﬁer’s accuracy. Table 1 shows how a simple extrapolation of k can be done
for a time window of a month (i.e., Tbud ≤ 31 × 24 × 10, 188 adversarial moves for the
CodeRed example), and setting ¯w to 10−38. By deﬁning k(t) as the ‘fractional’ k when
10−38 is reached for diﬀerent t of the form Tbud = 2t, it can be seen that the diﬀerences i.e.,
k(t + 1) − k(t) is decreasing for t > 11. We can conclude that by taking the last diﬀerence,
say κ, and computing the log2 of the one month time budget t(cid:48), the value of k is computed
to be at most 130 infected nodes using k = (t(cid:48) − 14) · κ + k(14). This means that in practice,
with these attack-defense parameters, the probability that the attacker constructs an army
of 130 infected nodes within a one-month time window would be at most 2−128 which
depicts how vital the learning function is when it comes to containing the adversary.

Fig. 9: ¯w(k, Tbud) for diﬀerent values of Tbud = 2t, 8 ≤ t ≤ 14 with a ﬁxed learning function
of the form f (l) = 1 − 1/(l/1000 + 1), p = 8.15 × 10−5, and γ = 0.05 as the classiﬁer’s
accuracy

A.2 Security Analysis of Case study 5.2

In this case, the defender is not trying to generate an attack signature and is therefore not
able to bring a next adversarial move to a halt. On the other hand, the defender defense
strategy is to reallocate/shuﬄe its resources (or at least a fraction of sensitive ones) when a

0102030405060k10-3610-3310-3010-2710-2410-2110-1810-1510-1210-910-610-3100¯w(k,Tbud)Tbud=28Tbud=29Tbud=210Tbud=211Tbud=212Tbud=213Tbud=214Toward a Theory of Cyber Attacks

31

Table 1: Extrapolating k for a One-month Time Window and ¯w ≈ 10−38

t (Tbud = 2t) k(t)
8

25.61

9

10

11

12

13

14

29.62

34.26

39.29

44.42

49.44

54.22

k(t + 1) − k(t)

4.01

4.64

5.03

5.13

5.02

4.78

-

suﬃcient number of attack evidence is captured via the defense system (or in other words,
the defender’s objective is to generate a proper attack detection ﬂag). Notice that the
presented analysis in section 4.7 can immediately be applied to this case study. As f (l)
remains 0 during this game, meaning the defender is not capable of taking actions to bring
an adversarial move to a halt, therefore, the delayed learning analysis (see Theorem 5) can
be used to study how many targets will be discovered during the game by the adversary,
before a threshold L∗ is reached by the defender after which the defender shuﬄes its
resources pushing the adversary to the beginning state of the game in the Markov model.
The probability that > k targets with unlimited time budget are discovered is equal to
1 − u(k, L∗) which is exponentially small in k. (A more exact upper bound for limited time
budgets can be found by applying Theorem 3.)

B Proofs

B.1 Stagnating Learning Rate – Proof of Theorem 1

In order to prove a lower bound on w(k, Tbud) we give a beneﬁt to the defender and assume
the learning rate f (l) is as large as possible given 1 − f (l) ≥ τ , i.e., 1 − f (l) = τ with
equality.

If 1 − f (l) = τ , then m1, m2, m3 and m4 are independent of l and the Markov model
reduces to Fig. 10 where each state i represents the collection of states (i, l) for l ≥ 0 in
the original Markov model.

Within time budget Tbud we reach i = k with probability

w(k, Tbud) =

Tbud−k
(cid:88)

(cid:88)

(cid:89)

(m1 + m4)fj (m2 + m3)k,

T =0

f0,...,fk−1s.t.
(cid:80) fj =T

j

where the time budget is distributed over k steps from each state to the next until state k is
reached and T ≤ Tbud − k transitions consisting of fi self-loops in state i for 0 ≤ i ≤ k − 1.

32

m1 + m4

i

m2 + m3

i + 1

Fig. 10: Reduced Markov model of the game for a constant 1 − f (l) = τ > 0

This probability is equal to

(cid:88)

(cid:89)

(m1 + m4)fj (m2 + m3)k

f0,...,fk−1s.t.
(cid:80) fj ≤Tbud−k

j

(Tbud−k)/k
(cid:88)

(Tbud−k)/k
(cid:88)

. . .

(cid:89)

(m1 + m4)fj (m2 + m3)k

f0=0

fk−1=0

j

k−1
(cid:89)

(Tbud−k)/k
(cid:88)

(m1 + m4)fj (m2 + m3)k

j=0

k−1
(cid:89)

j=0

fj =0

1 − (m1 + m4)Tbud/k
1 − (m1 + m4)

(m2 + m3)k

≥

=

=

= [1 − (m1 + m4)Tbud/k]k ≥ 1 − k(m1 + m4)Tbud/k,

where the last equality follows from 1−(m1+m4) = m2+m3. Now we substitute m1+m4 =
1 − τ p which yields Theorem 1.

B.2 Closed Form Winning Probability – Proof of Theorem 2

Before proving any bounds on w(k, Tbud) we ﬁrst provide a closed form for w(k, Tbud) itself.
To this purpose we introduce p(k − 1, L, Tbud) deﬁned as the probability that the adversary
reaches state (k − 1, L) and is ready to leave state (k − 1, L) (after zero or more self-loops
at state (k − 1, L)) within time budget Tbud. Any possible path from state (0, 0) to state
(k − 1, L) in the Markov model with self-loops along the way in each of the visited states
(including (k − 1, L)) contributes to this probability. In our notation we use k − 1 rather
than k because after a path to (k − 1, L) for some L ≥ 0 only a single horizontal or diagonal
move is needed to reach (k, L) for the ﬁrst time, and this probability is what we will need
for our characterization of w(k, Tbud):

The formula for w(k, Tbud) considers all the possible paths towards a state (k − 1, L)
within Tbud − 1 transitions and a single ﬁnal horizontal or diagonal transition (via m2(L)
or m3(L)) towards the winning state (k, L). We are interested in the probability of en-
tering state (k, L) for some L ≥ 0 and the time it takes to reach there for the ﬁrst time.
Appropriately summing over probabilities p(k − 1, L, Tbud − 1) gives

Toward a Theory of Cyber Attacks

33

w(k, Tbud) =

(cid:88)

L≥0

p(k − 1, L, Tbud − 1) ·

(cid:16) m2(L)

1 − m1(L)

+

m3(L)
1 − m1(L)

(cid:17)

,

(13)

where each path represented by p(k−1, L, Tbud) is ready to leave (k−1, L) implying that the
horizontal and diagonal transition probabilities are conditioned on “not having a self-loop”
and this explain the division by 1 − m1(L).

A path to (k −1, L) till the moment it is ready to leave (k −1, L) is uniquely represented

by

– gl; the number of states on the path that have the same level l,
– bl; if level l is entered via a vertical transition, then bl = 0; if level l is entered via a

diagonal transition, then bl = 1,

– t(i, l) counts the number of self-loops on the path in state (i, l).

Notice that b0 is undeﬁned and gl ≥ 1 for all 0 ≤ l ≤ L. Fig. 11 depicts a typical path from
(0, 0) to (k − 1, L) without showing any self-loops, where

– il+1 = il + bl+1 + gl+1 and i0 = 0; when the path enters level l for the ﬁrst time, k = il.

The probability of having exactly t(i, l) self-loops in state (i, l) after which the path

exits (i, l) is equal to

P rob(t(i, l) = u) = m1(l)u(1 − m1(l))

and describes a Poisson process. The probability that the path exits (i, l) along a horizontal,
diagonal or vertical transition given no more self-loops in (i, l) is equal to

P rob(horizontal transition) = P rob((i, l) → (i + 1, l)|no self-loop)

=

m2(l)
1 − m1(l)
P rob(diagonal transition) = P rob((i, l) → (i + 1, l + 1)|no self-loop)
m3(l)
1 − m1(l)

, and

=

,

P rob(vertical transition) = P rob((i, l) → (i, l + 1)|no self-loop)

=

m4(l)
1 − m1(l)

.

Combination of the probabilities describing diagonal and vertical transitions shows that

P rob((il + bl+1 + gl+1, l) → (il+1, l + 1)|no self-loop) = bl+1

m3(l)
1 − m1(l)

+ (1 − bl+1)

m4(l)
1 − m1(l)

.

34

Fig. 11: A sample path toward the winning state without depicting the self-loops at each
state (i, l) in the path

(k-1,L)K-1Lli(0,0)i1i2i3iL-1iLL-1blgl0212......0101020101110312-1The above analysis proves that the probability of having the Markov model transition along
a path which is represented by gl, bl, t(i, l), and il is equal to

Toward a Theory of Cyber Attacks

35

L−1
(cid:89)

(cid:16)

l=0

bl+1

m3(l)
1 − m1(l)

+ (1 − bl+1)

(cid:17)

m4(l)
1 − m1(l)

·

·

L
(cid:89)

l=0

(cid:16) m2(l)

(cid:17)gl−1

1 − m1(l)

L
(cid:89)

gl−1
(cid:89)

l=0

i=0

m1(l)t(il+i,l)(1 − m1(l)),

(14a)

(14b)

(14c)

where (14a) corresponds to diagonal and vertical transitions, (14b) corresponds to hori-
zontal transitions, and (14c) corresponds to self-loops.

The length of the path, i.e., total number of transitions without self-loops, is equal to

(k − 1) + L − B, where B =

L
(cid:88)

l=1

bl

is the total number of diagonal transitions. The total number of vertical transitions is equal
to L − B and the number of horizontal transitions is equal to (cid:80)L
l=0(gl − 1) = (k − 1) − B.
Summing the above probability over all possible combinations of gl, bl, and t(., .) for
paths that reach state (k − 1, L) and are ready to leave (k − 1, L) such that the number of
self-loops is equal to T and the total number of transitions including self-loops is equal to
(k − 1) + L − B + T ≤ Tbud leads to an expression for p(k − 1, L, Tbud):

p(k − 1, L, Tbud) =

(cid:88)

B,T s.t.
(k−1)+L−B+T ≤Tbud

(cid:88)

(14a)

(cid:80)L

b1,...,bL∈{0,1}s.t.
l=1 bl=B
(cid:88)

(14b)

(15a)

(15b)

(cid:80)L

g0≥1,...,gL≥1s.t.
l=0(gl−1)=(k−1)−B
(cid:88)

(cid:88)

. . .

(cid:88)

(14c)

(15c)

t0,...,tLs.t.
(cid:80)L
l=0 tl=T

t(i0,0),...,t(i0+g0−1,0)
s.t. (cid:80)g0−1
t(i0+i,0)=t0

i=0

t(iL,L),...,t(iL+gL−1,L)
s.t. (cid:80)gL−1
t(iL+i,L)=tL

i=0

36

Notice that (14a) = 0 if B > L and (14b) = 0 if B > k − 1. So, the main sum only

needs to consider B ≤ min{L, (k − 1)}. Plugging the above formula into (13) gives

w(k, Tbud) =

Tbud−1
(cid:88)

min{L,(k−1)}
(cid:88)

L=0

B=0

m2(L) + m3(L)
1 − m1(L)

· (15a) · (15b)

(16a)

(Tbud−1)−[(k−1)+L−B]
(cid:88)

·

(15c).

T =0

(16b)

This closed form together with

(15c) =

(cid:88)

(cid:88)

. . .

(cid:88)

t0,...,tLs.t.
(cid:80)L
l=0 tl=T

t(i0,0),...,t(i0+g0−1,0)
s.t. (cid:80)g0−1
i=0 t(i0+i,0)=t0

t(iL,L),...,t(iL+gL−1,L)
s.t. (cid:80)gL−1
t(iL+i,L)=tL

i=0

L
(cid:89)

gl−1
(cid:89)

l=0

i=0

m1(l)t(il+i,l)(1 − m1(l))

=

=

=

(cid:88)

(cid:88)

. . .

(cid:88)

t0,...,tLs.t.
(cid:80)L
l=0 tl=T

t(i0,0),...,t(i0+g0−1,0)
s.t. (cid:80)g0−1
i=0 t(i0+i,0)=t0

t(iL,L),...,t(iL+gL−1,L)
s.t. (cid:80)gL−1
t(iL+i,L)=tL

i=0

L
(cid:89)

l=0

m1(l)tl (1 − m1(l))gl

L
(cid:89)

l=0

L
(cid:89)

l=0

(cid:88)

t0,...,tLs.t.
(cid:80)L
l=0 tl=T

(cid:88)

t0,...,tLs.t.
(cid:80)L
l=0 tl=T

(cid:88)

m1(l)tl (1 − m1(l))gl

t(il,l),...,t(il+gl−1,l)
s.t. (cid:80)gl−1
i=0 t(il+i,l)=tl
(cid:32)

(cid:33)

tl + gl − 1
tl

m1(l)tl (1 − m1(l))gl

proves Theorem 2.

B.3 Upper and Lower Bounds – Proof of Theorem 3

By noticing that (16b) for Tbud = ∞ is equal to 1, a straightforward upper bound on
w(k, Tbud) is given by just formula (16a): We have w(k, Tbud) is at most equal to ¯w(k, Tbud).

In order to prove a lower bound we ﬁrst analyze

Toward a Theory of Cyber Attacks

37

(16b) =

(Tbud−1)−[(k−1)+L−B]
(cid:88)

(15c)

=

=

T =0

(Tbud−k−L+B)
(cid:88)

(cid:88)

(cid:88)

. . .

(cid:88)

T =0

t0,...,tLs.t.
(cid:80)L
l=0 tl=T

t(i0,0),...,t(i0+g0−1,0)
s.t. (cid:80)g0−1
t(i0+i,0)=t0

i=0

t(iL,L),...,t(iL+gL−1,L)
s.t. (cid:80)gL−1
t(iL+i,L)=tL

i=0

L
(cid:89)

gl−1
(cid:89)

l=0

i=0

(cid:88)

m1(l)t(il+i,l)(1 − m1(l))

(cid:88)

. . .

(cid:88)

(cid:80)L

t0,...,tLs.t.
l=0 tl≤(Tbud−k−L+B)

t(i0,0),...,t(i0+g0−1,0)
s.t. (cid:80)g0−1
t(i0+i,0)=t0

i=0

t(iL,L),...,t(iL+gL−1,L)
s.t. (cid:80)gL−1
t(iL+i,L)=tL

i=0

L
(cid:89)

gl−1
(cid:89)

l=0

i=0

m1(l)t(il+i,l)(1 − m1(l)).

Notice that (16b) is used within another sum with B ≤ min{L, (k − 1)}. This implies
that B ≤ (k − 1)L or equivalently −(L + 1)k ≤ −k − L + B. Let v ≥ 1. For now we
only consider L ≤ (Tbud/v) − 1, or equivalently v ≤ Tbud/(L + 1). Then (L + 1)(v − k) ≤
Tbud − (L + 1)k ≤ Tbud − k − L + B, hence,

v − k ≤

Tbud − k − L + B
L + 1

.

38

This allows us to lower bound the above sums and obtain

(16b) ≥

v−k
(cid:88)

. . .

v−k
(cid:88)

t0=0

tL=0

(cid:88)

. . .

(cid:88)

t(i0,0),...,t(i0+g0−1,0)
s.t. (cid:80)g0−1
t(i0+i,0)=t0

i=0

t(iL,L),...,t(iL+gL−1,L)
s.t. (cid:80)gL−1
t(iL+i,L)=tL

i=0

L
(cid:89)

gl−1
(cid:89)

l=0

i=0

m1(l)t(il+i,l)(1 − m1(l))

(cid:88)

. . .

(cid:88)

L
(cid:89)

gl−1
(cid:89)

l=0

i=0

m1(l)t(il+i,l)(1 − m1(l))

≥

≥

=

=

=

t(i0,0),...,t(i0+g0−1,0)

t(i0+i,0)≤v−k

s.t. (cid:80)g0−1
i=0
(cid:88)

t(iL,L),...,t(iL+gL−1,L)
s.t. (cid:80)gL−1

t(iL+i,L)≤v−k

i=0

. . .

(cid:88)

. . .

(cid:88)

. . .

(cid:88)

t(i0,0)≤t0/g0

t(i0+g0−1,0)≤(v−k)/g0

t(iL,L)≤tL/gL

t(iL+gL−1,L)≤(v−k)/gL

L
(cid:89)

gl−1
(cid:89)

l=0

i=0

(cid:88)

L
(cid:89)

gl−1
(cid:89)

m1(l)t(il+i,l)(1 − m1(l))

m1(l)t(il+i,l)(1 − m1(l))

l=0

i=0

t(il+i,l)≤(v−k)/gl

L
(cid:89)

gl−1
(cid:89)

1 − m1(l)

v−k
gl

+1

l=0

i=0

1 − m1(l)

(1 − m1(l))

L
(cid:89)

(1 − m1(l)

v−k
gl

+1)gl ≥

L
(cid:89)

(1 − glm1(l)

v−k
gl

+1).

l=0

l=0

We observe that gl ≤ k within the larger sum that contains (16b). If v ≥ k, then this

implies v/k ≤ 1 + (v − k)/gl proving

(16b) ≥

L
(cid:89)

(1 − glm1(l)v/k)

l=0

Since m1(l) = [(1 − γ)(1 − (p + h)(1 − f (l)))] ≤ 1 − γ, we can further lower bound this

to

(16b) ≥

L
(cid:89)

(1 − gl(1 − γ)v/k)

l=0

L
(cid:88)

≥ 1 − (

l=0

gl)(1 − γ)v/k.

Within the larger sum that contains (16b), we have (cid:80)L
(L + 1) + (k − 1 − B) = k + L − B ≤ k + Tbud/v. This yields

l=0 gl = (L + 1) + (cid:80)L

l=0(gl − 1) =

(16b) ≥ 1 − (k +

Tbud
v

)(1 − γ)v/k.

Toward a Theory of Cyber Attacks

39

The obtained lower bound on (16b) = (cid:80)(Tbud−1)−[(k−1)+L−B]

(15c) is independent of any
T =0
of the other summing variables used in (16a) but requires L + 1 ≤ Tbud/v. By restricting L
to be ≤ (Tbud/v) − 1 in (16a), i.e., we substitute Tbud by Tbud/v, we obtain a lower bound
on w(k, Tbud):

w(k, Tbud) =

Tbud−1
(cid:88)

min{L,(k−1)}
(cid:88)

L=0

B=0

m2(L) + m3(L)
1 − m1(L)

· (15a) · (15b) ·

(Tbud−1)−[(k−1)+L−B]
(cid:88)

(15c)

T =0

· (15a) · (15b) ·

(Tbud−1)−[(k−1)+L−B]
(cid:88)

(15c)

m2(L) + m3(L)
1 − m1(L)

≥

≥

(Tbud/v)−1
(cid:88)

min{L,(k−1)}
(cid:88)

L=0

B=0

Tbud/v−1
(cid:88)

min{L,(k−1)}
(cid:88)

L=0

B=0

m2(L) + m3(L)
1 − m1(L)

· (15a) · (15b)(1 − (k +

T =0

Tbud
v

)(1 − γ)v/k)

= ¯w(k, Tbud/v) · (1 − (k +

Tbud
v

)(1 − γ)v/k),

where the last equality follows from the fact that (15a) and (15b) do not depend on Tbud
(they depend on L). This completes the proof.

B.4 Analyzing the Learning Rate – Proof of Theorem 4

Theorem 4 has a couple of statements and we start by proving the ﬁrst most general claim
that upper bounds ¯w(k, Tbud) in terms of the general parameters of the Markov model.

We deﬁne and assume

This allows us to bound

(cid:114)

β(l) =

1 − f (l)
d

≤ 1.

α(l) =

α(l) =

p(1 − f (l))
γ + (1 − γ)(p + h)(1 − f (l))
p(1 − f (l))
γ + (1 − γ)(p + h)(1 − f (l))

≤

p(1 − f (l))
γ

=

pdβ(l)2
γ

,

≥ p(1 − f (l)) = pdβ(l)2.

Since β(l) ≤ 1, β(l)2 ≤ β(l). Let θ be such that (1 − γ)/γ ≤ θ. Then the above inequalities
allow us to bound

m2(l)
1 − m1(l)

m3(l)
1 − m1(l)
m4(l)
1 − m1(l)

= (1 − γ)α(l) ≤

≤ θpdβ(l),

(1 − γ)pd
γ

β(l)2 ≤ θpdβ(l)2

= γα(l) ≤ pdβ(l)2, and

= 1 − α(l) ≤ 1 − pdβ(l)2.

40

Plugging the above bounds in expression (7) for ¯w(k, Tbud) with m2(L)
1−m1(l) ≤ θpdβ(l) yields

and m2(l)

1−m1(L) ≤ θpdβ(L)2

¯w(k, Tbud) ≤

Tbud−1
(cid:88)

(1 + θ)pdβ(L)2

L=0

L
(cid:88)

B=0

·

·

(cid:88)

b1,...,bL∈{0,1}
s.t. (cid:80)L
l=1 bl=B

(cid:88)

L−1
(cid:89)

l=0

bl+1pdβ(l)2
+(1 − bl+1)(1 − pdβ(l)2)

L
(cid:89)

(cid:16)

(cid:17)gl−1

θpdβ(l)

(cid:80)L

g0,...,gL≥1s.t.
l=0(gl−1)=(k−1)−B

l=0

(17a)

(17b)

Let bL+1 = 0 and deﬁne

Z =

L
(cid:89)

(cid:16)

l=0

(cid:17)bl+1

θpdβ(l)

.

We will multiply (17b) with Z and show an upper bound of the product which is
independent of bl and B. We will multiply (17a) with Z −1 and show that it behaves like a
product of 1 + β(l)

θ , which (as we will see) remains ‘small enough’:

We choose d such that θ ≤ (pd)−1 (if d does not satisfy this inequality, then the to be
derived upper bound will be larger than 1 and will therefore trivially hold for ¯w(k, Tbud)).
Then, since B = (cid:80)L

l=0 bl+1,

(17b) · Z =

(cid:88)

L
(cid:89)

(cid:16)

(cid:17)(gl−1)+bl+1

θpdβ(l)

(cid:80)L

g0,...,gL≥1s.t.
l=0(gl−1)+bl+1=(k−1)

l=0

(cid:88)

L
(cid:89)

(cid:16)

≤

g(cid:48)
0,...,g(cid:48)
(cid:80)L
l=0 g(cid:48)

L≥0s.t.
l=(k−1)

l=0

(cid:17)g(cid:48)

l

θpdβ(l)

≤ (θpd)k−1 (cid:88)
0,...,g(cid:48)
g(cid:48)
(cid:80)L
l=0 g(cid:48)

L≥0s.t.
l=(k−1)

L
(cid:89)

l=0

β(l)g(cid:48)

l

. . .

(cid:88)

L
(cid:89)

g(cid:48)
L≥0

l=0

β(l)g(cid:48)

l

≤ (θpd)k−1 (cid:88)
g(cid:48)
0≥0
L
(cid:89)

= (θpd)k−1

(cid:88)

β(l)g(cid:48)

l = (θpd)k−1

L
(cid:89)

l=0

1
1 − β(l)

.

l=0

g(cid:48)
l≥0

Now notice that each bl+1 is either equal to 0 or 1 and therefore

(bl+1pdβ(l)2 + (1 − βl+1)(1 − pdβ(l)2) · (θpdβ(l))−bl+1 = bl+1

β(l)
θ

+ (1 − βl+1)(1 − pdβ(l)2.

We derive

Toward a Theory of Cyber Attacks

41

(17a) · Z −1 =

L
(cid:88)

B=0

(cid:88)

b1,...,bL∈{0,1}
s.t. (cid:80)L
l=1 bl=B

L−1
(cid:89)

l=0

β(l)
θ

bl+1
+(1 − bl+1)(1 − pdβ(l)2)

=

=

=

(cid:88)

L−1
(cid:89)

b1,...,bL∈{0,1}

l=0

β(l)
θ

bl+1
+(1 − bl+1)(1 − pdβ(l)2)

L−1
(cid:89)

(cid:88)

l=0

bl+1∈{0,1}

β(l)
θ

bl+1
+(1 − bl+1)(1 − pdβ(l)2)

L−1
(cid:89)

l=0

(cid:16) β(l)
θ

+ (1 − pdβ(l)2)

(cid:17)

≤

L−1
(cid:89)

l=0

(cid:16) β(l)
θ

(cid:17)

.

+ 1

Combination of the above results proves

¯w(k, Tbud) ≤

Tbud−1
(cid:88)

[θpd]k(1 +

L=0

Tbud−1
(cid:88)

[θpd]k(1 +

≤

L=0

1
θ

1
θ

)

β(L)2
1 − β(L)

L−1
(cid:89)

l=0

1 + β(l)/θ
1 − β(l)

)

L
(cid:89)

l=0

1 + β(l)/θ
1 − β(l)

,

where the last inequality uses β(L)2 ≤ 1 ≤ 1 + β(L)/θ. The argument in the resulting
product is equal to (1 + β(l)/θ)/(1 − β(l)) = 1 + (1 + θ−1)β(l)/(1 − β(l)). This completes
the proof of the ﬁrst statement.

In order to prove the second bound in Theorem 4 we deﬁne

B(cid:48)(l) =

β(l)
1 − β(l)

and y = 1 + θ−1

and apply the next lemma. Notice that B(cid:48)(l) ≥ 0 and, since f (cid:48)(l) ≥ 0 (because learning
only increases),

B(cid:48)(cid:48)(l) = β(cid:48)(l)

2 − β(l)
(1 − β(l))2 =

−f (cid:48)(l)
2β(l)

2 − β(l)
(1 − β(l))2 ≤ 0.

Lemma 1. For diﬀerentiable continuous functions B(.), if B(cid:48)(l) ≥ 0 and B(cid:48)(cid:48)(l) ≤ 0 for
l ≥ 0, then, for y ≥ 0,

L
(cid:89)

(1 + yB(cid:48)(l)) ≤ ey(B(cid:48)(0)−B(0)+B(L)).

l=0

42

Proof. We ﬁnd an upperbound of ln (cid:81)L
w.r.t. y gives

l=0(1+yB(cid:48)(l)) = (cid:80)L

l=0 ln(1+yB(cid:48)(l)). Diﬀerentiating

L
(cid:88)

l=0

1
1 + yB(cid:48)(l)

· B(cid:48)(l) ≤

L
(cid:88)

l=0

B(cid:48)(l) = B(cid:48)(0) +

L
(cid:88)

l=1

B(cid:48)(l)

≤ B(cid:48)(0) +

(cid:90) L

l=0

B(cid:48)(l)dl

= B(cid:48)(0) + B(L) − B(0)

We also have

L
(cid:88)

l=0

ln(1 + yB(cid:48)(l))|y=0= 0 = y · [B(cid:48)(0) − B(0) + B(L)]|y=0.

We conclude that (cid:80)L

l=0 ln(1 + yB(cid:48)(l)) ≤ y · [B(cid:48)(0) − B(0) + B(L)] for y ≥ 0.
For our B(cid:48)(l) and y, application of the lemma to our upper bound yields

(cid:117)(cid:116)

¯w(k, Tbud) ≤ (1 + θ−1)

Tbud−1
(cid:88)

[θpd]ke(1+θ−1)(B(cid:48)(0)−B(0)+B(L))

≤ Tbud · (1 + θ−1)[θpd]ke(1+θ−1)(B(cid:48)(0)−B(0)+B(Tbud−1)).

L=0

Minimizing θke(1+θ−1)c for

c = B(Tbud − 1) − B(0) + B(cid:48)(0)

gives θ = c/k if c/k ≥ (1 − γ)/γ and gives θ = (1 − γ)/γ if c/k ≤ (1 − γ)/γ. Substituting
this in our upper bound (e denotes the natural number) yields

¯w(k, Tbud) ≤
(cid:40)
Tbud(1 + k
Tbud(1 − γ)−1[ 1−γ

c )[ c

k pd]kek+c = Tbud(1 + k
γ pd]ke(1−γ)−1c,

c )[epd c

k ec/k]k,

if c/k ≥ (1 − γ)/γ

if c/k ≤ (1 − γ)/γ.

(18)

The ﬁrst case of the upper bound is less interesting as c
k ec/k may be large yielding a bad
upper bound. The second case of the upper bound gives the most insight and proves the
second statement of Theorem 4.
As an example, suppose that

1 − f (l) ≤

d
(l + 2)2 .

To get an upper bound, we give the adversary the advantage of having the defender play
with the smallest possible learning rates. i.e., 1 − f (l) = d
(l+2)2 . This gives B(cid:48)(l) = 1/(l + 1),
hence, B(Tbud − 1) = ln(Tbud), B(0) = 0, and B(cid:48)(0) = 1. This implies c = 1 + ln(Tbud). The
second case of the upper bound translates into: If

Tbud ≤ e−1+k(1−γ)/γ,

(19)

Toward a Theory of Cyber Attacks

43

then the probability of winning for the adversary is at most

w(k, Tbud) ≤ ¯w(k, Tbud) ≤

e1/(1−γ)
1 − γ

· T 1+1/(1−γ)

bud

· [

1 − γ
γ

pd]k.

This proves the last statement of the theorem.

B.5 Delayed Learning – Proof of Theorem 5

In practice, we may not immediately start learning at a rate 1 − f (l) ≈ d/(l + 2)2. This will
only happen after reaching for example L∗ samples. During such a ﬁrst phase the defender
is not yet able to increase f (l) and f (l) remains 0, i.e., 1 − f (l) = 1. The adversary tries
to compromise as many, say k∗, nodes as possible before reaching L∗ levels.

For 1 − f (l) = 1, we have:

m1(l) = (1 − γ)(1 − (p + h)),
m2(l) = (1 − γ)p,
m3(l) = γp, and
m4(l) = [h − γ(h + p)] + γ = (1 − γ)h + γ(1 − p),

which are all independent of l. For this reason we use m1, m2, m3, and m4 where suited
in our derivations below.

We are interested in k∗ as a function of (cid:15) for which

u(k∗, L∗) =

k∗−1
(cid:88)

k=0

p(k, L∗ − 1, Tbud − 1)(

p(k∗, L∗ − 1, Tbud − 1)

+

m4(L∗ − 1)
1 − m1(L∗ − 1)
m4(L∗ − 1)
1 − m1(L∗ − 1)

m3(L∗ − 1)
1 − m1(L∗ − 1)

) + (20)

≥ 1 − (cid:15).

Here u(k∗, L∗) is equal to the probability that ≤ k∗ nodes will be compromised before level
l = L∗ is reached for the ﬁrst time: The sum in (20) considers all paths reaching a state
(k, L∗ − 1) for some k < k∗ after which a single vertical or diagonal transition reaches level
L∗ for the ﬁrst time. The additional term considers paths that reach a state (k∗, L∗ − 1)
after which only a vertical transition reaches level L∗ without increasing the number of
compromised nodes beyond k∗. If (20) holds, then the probability of winning is

≤ (cid:15) + (1 − (cid:15))w(k − k∗((cid:15)), Tbud),

(21)

where w(k − k∗((cid:15)), Tbud) is deﬁned for learning rate f (l + L∗) as a function of l. In other
words, during the ﬁrst phase a node with i < k∗((cid:15)) is reached with probability ≥ 1 − (cid:15)
after which the learning rate increases according to f (l + L∗) leading to a winning state if
the Markov model increments i at least another k − k∗((cid:15)) steps.

44

In order to ﬁnd a lower bound on u(k∗, L∗) we give the adversary the beneﬁt of an

unlimited time budget (implying (15c) = 1) giving

u(k∗, L∗) ≥

k∗−1
(cid:88)

p(k, L∗ − 1, ∞)(

m4(L∗ − 1)
1 − m1(L∗ − 1)

+

m3(L∗ − 1)
1 − m1(L∗ − 1)

)

k=0
min{L∗−1,(k∗−2)}
(cid:88)

=

k∗−1
(cid:88)

(15a) · (15b) ·

B=0

k=B+1

m3(L∗ − 1) + m4(L∗ − 1)
1 − m1(L∗ − 1)

.

(22)

(23)

We simplify this lower bound by noticing that (15a) does not depend on k and

k∗−1
(cid:88)

(15b) =

k∗−1
(cid:88)

k=B+1

k=B+1

(cid:88)

g0≥1,...,gL∗−1≥1s.t.

(cid:80)L∗−1
l=0

(gl−1)=k−1−B

L∗−1
(cid:89)

l=0

(cid:16) m2(l)

(cid:17)gl−1

1 − m1(l)

(cid:88)

0,...,g(cid:48)
g(cid:48)
L∗−1≥0s.t.
(cid:80)L∗−1
g(cid:48)
l≤k∗−2−B
l=0

L∗−1
(cid:89)

l=0

(cid:16) m2(l)

(cid:17)g(cid:48)

l

1 − m1(l)

(k∗−2−B)/L∗
(cid:88)

. . .

(k∗−2−B)/L∗
(cid:88)

L∗−1
(cid:89)

(cid:16) m2(l)

(cid:17)g(cid:48)

l

1 − m1(l)

g(cid:48)
0=0
(k∗−2−B)/L∗
(cid:88)

L∗−1
(cid:89)

g(cid:48)
L∗ =0

(cid:16) m2(l)

l=0

(cid:17)g(cid:48)

l

g(cid:48)
l=0

1 − m1(l)

1
1 − m2(l)
1−m1(l)

(1 − (

m2(l)
1 − m1(l)

k∗−2−B

L∗ +1)

)

1 − m1(l)
m3(l) + m4(l)

(cid:16)

1 − (

m2(l)
1 − m1(l)

)

k∗−2−B

L∗ +1(cid:17)

l=0
(cid:16) 1 − m1
m3 + m4

(cid:17)L∗ (cid:16)

1 − (

m2
1 − m1

)

k∗−2−B

L∗ +1(cid:17)L∗

.

l=0

L∗−1
(cid:89)

l=0
L∗−1
(cid:89)

=

≥

=

=

=

=

Since B ≤ L∗ − 1, this can be further lower bounded as

k∗−1
(cid:88)

(15b) ≥

k=B+1

(cid:16) 1 − m1
m3 + m4

(cid:17)L∗ (cid:16)

1 − (

m2
1 − m1

k∗−1
L∗

)

(cid:17)L∗

.

Notice that this lower bound is independent from B, hence,

Toward a Theory of Cyber Attacks

45

u(k∗, L∗) ≥

min{L∗−1,(k∗−2)}
(cid:88)

(15b) ·

m3(L∗ − 1) + m4(L∗ − 1)
1 − m1(L∗ − 1)
(cid:17)L∗

m2
1 − m1

k∗−1
L∗

)

(cid:17)L∗ (cid:16)

1 − (

B=0
(cid:16) 1 − m1
m3 + m4

·

min{L∗−1,(k∗−2)}
(cid:88)

=

(15b) ·

B=0
(cid:16) 1 − m1
m3 + m4

·

(cid:17)L∗−1(cid:16)

1 − (

m2
1 − m1

k∗−1
L∗

)

(cid:17)L∗

.

We assume k∗ ≥ L∗ + 1 and derive

min{L∗−1,(k∗−2)}
(cid:88)

(15b) =

B=0

L∗−1
(cid:88)

B=0

L∗−2
(cid:89)

(cid:16)

l=0

bl+1

m3(l)
1 − m1(l)

+ (1 − bl+1)

(cid:17)

m4(l)
1 − m1(l)

(cid:88)

b1,...,bL∗−1∈{0,1}s.t.

(cid:80)L∗−1
l=1

bl=B

L∗−2
(cid:89)

(cid:16)

l=0

=

=

(cid:88)

b1,...,bL∗−1∈{0,1}
L∗−2
(cid:89)
(

m3(l)
1 − m1(l)

l=0

bl+1

m3(l)
1 − m1(l)

+ (1 − bl+1)

(cid:17)

m4(l)
1 − m1(l)

+

m4(l)
1 − m1(l)

) =

(cid:16) m3 + m4
1 − m1

(cid:17)L∗−1

.

Substituting this in the lower bound for u(k∗, L∗) yields

u(k∗, L∗) ≥

(cid:16)

1 − (

m2
1 − m1

(cid:17)L∗

k∗−1
L∗

)

≥ 1 − L∗(

k∗−1
L∗

)

= 1 − L∗[

(1 − γ)p
1 − (1 − γ)(1 − (p + h))

]

m2
1 − m1
k∗−1
L∗ .

B.6 Capacity Region – Proofs of Corollaries 1 and 2

Proof of Corollary 1. As an example of computing a capacity region, we translate
(l+2)2 in terms of a capacity region: We have the security
Theorem 4 for 1 − f (l) ≤
guarantee

d

Tbud ≤ e−1+k(1−γ)/γ ⇒ ¯w(k, Tbud) ≤

e1/(1−γ)
1 − γ

· T 1+1/(1−γ)

bud

· [

1 − γ
γ

pd]k.

(24)

If

2t ≤ e−1+k(1−γ)/γ, and

2−s ≥

e1/(1−γ)
1 − γ

· (2t)1+1/(1−γ) · [

1 − γ
γ

pd]k,

(25)

46

then Tbud ≤ 2t implies the condition on the left hand side of the implication in (24), hence,

¯w(k, Tbud) ≤

≤

e1/(1−γ)
1 − γ
e1/(1−γ)
1 − γ

≤ 2−s.

· T 1+1/(1−γ)

bud

· [

pd]k

1 − γ
γ
1 − γ
γ

· (2t)1+1/(1−γ) · [

pd]k

This shows that the capacity region is characterized by (25), or equivalently after tak-
ing logarithms, assuming pd(1−γ
< 1, and reordering terms, δ, µ, and ξ are given by
2-dimensional vectors

γ

δ = (0, q ln 2), where q =

ln(

(cid:20)

γ
pd(1 − γ)

)

(cid:21)−1

,

µ = (

, q(ln 2)(1 +

γ ln 2
1 − γ
γ
1 − γ

ξ = (−

, q(−

1
1 − γ

+ ln

1
1 − γ

).

1
1 − γ

)),

Proof of Corollary 2. Delayed learning shows that for all k∗ ≥ L∗ + 1,

w(k, Tbud) ≤ (1 − u(k∗, L∗)) + w(cid:48)(k − k∗, Tbud)
k∗−1
L∗ + w(cid:48)(k − k∗, Tbud).

(1 − γ)p
1 − (1 − γ)(1 − (p + h))

≤ L∗[

]

If

L∗[

(1 − γ)p
1 − (1 − γ)(1 − (p + h))

k∗−1
L∗ ≤ 2−(s+1) and

]

w(cid:48)(k − k∗, Tbud) ≤ 2−(s+1),

(26)

(27)

then w(k, Tbud) ≤ 2−s.

We assume

(1−γ)p

1−(1−γ)(1−(p+h)) < 1. Then, after taking logarithms, substituting k∗ =

ˆk + L∗ + 1 with ˆk ≥ 0, and reordering terms, condition (26) is equivalent to

(s + 1)δ(cid:48)(cid:48) ≤ ˆk + ξ(cid:48)(cid:48), where

δ(cid:48)(cid:48) = L∗(ln 2)z and ξ(cid:48)(cid:48) = L∗[1 − (ln L∗)z] for z =

(cid:20)

ln(

1 − (1 − γ)(1 − (p + h))
(1 − γ)p

)

(cid:21)−1

.

Suppose that w(cid:48)(·, ·) corresponds to capacity region (δ(cid:48), µ(cid:48), ξ(cid:48)). Then, condition (27) is

implied by Tbud ≤ 2t and

(s + 1)δ(cid:48) + tµ(cid:48) ≤ (k − (ˆk + L∗ + 1))1 + ξ(cid:48).

If we assume learning is delayed suﬃciently long such that

(s + 1)δ(cid:48)(cid:48) ≥ ξ(cid:48)(cid:48)

Toward a Theory of Cyber Attacks

47

(e.g., L∗ ≥ z−1/2 for s = 0), then we may choose

ˆk = (s + 1)δ(cid:48)(cid:48) − ξ(cid:48)(cid:48) ≥ 0.

This satisﬁes condition (26). Condition (27) (after substituting ˆk) is now equivalent to
Tbud ≤ 2t and

(s + 1)(δ(cid:48) + δ(cid:48)(cid:48)1) + tµ(cid:48) ≤ k1 + ξ + (ξ(cid:48)(cid:48) − (L∗ + 1))1.

In other words,

is a capacity region.

(δ(cid:48) + δ(cid:48)(cid:48)1, µ(cid:48), ξ(cid:48) − δ(cid:48) + (ξ(cid:48)(cid:48) − δ(cid:48)(cid:48) − (L∗ + 1))1)

If we assume learning is moderately delayed such that

(s + 1)δ(cid:48)(cid:48) ≤ ξ(cid:48)(cid:48),

then we may choose ˆk = 0 and

(δ(cid:48), µ(cid:48), ξ(cid:48) − δ(cid:48) − (L∗ + 1)1).

is a capacity region.
Substituting

s ≥ ξ(cid:48)(cid:48)/δ(cid:48)(cid:48)−1 = (L∗[1−(ln L∗)z])/L∗(ln 2)z−1 = (L∗[1−(ln 2L∗)z])/L∗(ln 2)z = 1/z ln 2−ln(2L∗)ln 2

and

ξ(cid:48)(cid:48) − δ(cid:48)(cid:48) − (L∗ + 1) = L∗[1 − (ln L∗)z] − L∗(ln 2)z − (L∗ + 1) = L∗(ln 2L∗)z − 1

proves the corollary.

