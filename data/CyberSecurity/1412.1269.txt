5
1
0
2

v
o
N
0
2

]

C
O
.
h
t
a
m

[

2
v
9
6
2
1
.
2
1
4
1
:
v
i
X
r
a

The evolutionary game of pressure (or interference),
resistance and collaboration ∗

Vassili N. Kolokoltsov†

November 23, 2015

Abstract

In this paper we extend the framework of evolutionary inspection game put
forward recently by the author and coworkers to a large class of conﬂict interac-
tions dealing with the pressure executed by the major player (or principal) on the
large group of small players that can resist this pressure or collaborate with the
major player. We prove rigorous results on the convergence of various Markov de-
cision models of interacting small agents (including evolutionary growth), namely
pairwise, in groups and by coalition formation, to a deterministic evolution on the
distributions of the state spaces of small players paying main attention to situa-
tions with an inﬁnite state-space of small players. We supply rather precise rates
of convergence. The theoretical results of the paper are applied to the analysis of
the processes of inspection, corruption, cyber-security, counter-terrorism, banks and
ﬁrms merging, strategically enhanced preferential attachment and many other.

Mathematics Subject Classiﬁcation (2010): 91A22, 91A40, 91A80, 91F99, 60J20

Key words: inspection, corruption, cyber-security, crime prevention, geopolitics, coun-
terterrorism, optimal allocation, evolutionary game, major player, coalition growth, pres-
sure and resistance, social norms, networking, law of large numbers, strategically enhanced
preferential attachment

1

Introduction

1.1 Objectives and content of the study

The inspection games represent an important class of games with various applications
from the arms race control to the study of tax evasion, see e. g. [10] for a general survey,
as well as [6], [8], [9] and references therein. In [59] the author with coworkers initiated
the study of the inspection games from the evolutionary perspective, aimed at analysis of
the class of games with large number of inspectees.

∗http://arxiv.org/abs/1412.1269
†Department
of

Statistics, University
v.kolokoltsov@warwick.ac.uk and associate member of IPI RAN RF

of Warwick, Coventry CV4

7AL UK, Email:

1

 
 
 
 
 
 
The aim of the present paper is two-folds: 1) To widen the range of applicability of this
research by introducing a uniﬁed methodology for the analysis of a large class of conﬂict
interactions of social, economic or military character (that turn out to be mathematically
similar, but are often discussed in disjoint sets of subject speciﬁc journals) describing the
pressure executed by a big player (or principal) on a large group of small players that
resist the pressure or collaborate, that is the class of games of an agent immersed into
a pool of evolutionary and mean-ﬁeld interacting small players; 2) to build the rigorous
mathematical theory of the law of large number limits for the latter conﬂicts by proving
that the controlled deterministic evolutionary equation (kinetic equation) describing the
dynamics of interaction can be obtained as the limiting behavior of the controlled Markov
models of kth order and/or mean-ﬁeld interaction (with the number of agents tending to
inﬁnity) and thus extending the corresponding theory for the justiﬁcation of the usual
replicator dynamics (see e.g.
[16] or Section 11.9 of textbook [56] for the latter). The
practical usefulness of this limit is that it provides much more tractable limiting models
where carrying out a traditional Markov decision analysis for a large state space is often
unfeasible.

The paper is organized as follows. In the next introductory subsections we discuss the
related literature on the dynamic law of large numbers and then motivate our analysis
by invoking certain real life conﬂict interactions that can be analyzed via our general
model providing social, economic, historic, geopolitical and literary perspectives. The
next section is devoted to the simple case of a ’short-sighted’ principal with the direct
best response strategy. We deduce rather precise convergence rates in terms of the av-
erages of smooth functions (rather than more developed estimates for trajectories, see
[16]) and provide the crucial link between the ﬁxed point of the limiting dynamics and
the Nash equilibria of the corresponding N-player game (which is quite diﬀerent from the
usually discussed link with the underlying two player game of the standard evolutionary
setting, which is not deﬁned in our setting). This simplest framework presents a handy
opportunity to discuss in the most transparent way our basic examples of payoﬀs related
to various contexts thus leading to a uniﬁed theory of various subject areas. Subsection
2.4 is devoted to more-or-less straightforward (from the mathematical point of view) ex-
tensions of the basic model that include the possibility of simultaneous interactions of
more than two players (kth order interaction), as well as of diversiﬁed strategies of the
principle solving the optimal allocation problem on evolutionary background. Section
3 provides the convergence (with the rates) of N-player games to a deterministic limit
for a more sophisticated (but more realistic) setting of a forward looking major player.
Section 4 initiates the analysis of the controlled law of large numbers for processes with
unbounded intensities deﬁned on a countable (rather than ﬁnite or compact) state space,
which leads to modeling processes of evolutionary growth with variable population size
of small players. This includes various processes of birth, death, migration and coalition
formation, which are strategically enhanced in the sense that their evolutions are sub-
ject to controlled external pressure. In Appendix we explain some auxiliary facts about
variational derivatives, ODEs in Banach spaces and the comparison of semigroups.

Let us indicate further steps (in addition to those outlined in Subsection 4.3 and at
the ends of most of the sections) that are worth being exploited in the future work on the
models discussed here. (1) It should be of interest to analyze next order approximation
to the dynamic law of large numbers studied here, which can be carried out in two similar
(but diﬀerent) ways: by including in the generator the second order (diﬀusive) terms of

2

order 1/N (as is done in paper [72] for standard evolutionary games or in [42], [43] for
the chemical kinetics setting) or by systematic study of ﬂuctuations as dynamic central
limit theorem (as done in [54] for classical models). (2) It is natural to include possible
[86]) aiming
spatial distributions (which can lead to quite remarkable eﬀects, see e.g.
at the analysis of various models of crime detection and relating to the well developed
theory of patrolling games, see [3], [4], [5] and references therein.
(3) We consider a
single major player in the pool of small players; it is natural to extend the model to the
general ﬁnite player game on the evolutionary background. (4) Allowing the principal to
withdraw from the interaction (to retire) would lead to the optimal stopping problem on
the evolutionary background and, in particular, to the evolutionary extension of the well
[33] and [40] for the background on the
studied multi-armed bandit problem (see e. g.
latter).

All notations for the norms and spaces used are carefully introduced in the Appendix.

1.2 Related work on dynamic law of large numbers

In this section we discuss the papers that are relevant more to our methodology itself
rather than its concrete applications. Roughly speaking, this methodology concerns the
rigorous derivation of the dynamic law of large numbers for Markov dynamics with control,
competition and/or cooperation. The literature on the topic is quite abundant and keeps
growing rapidly.

First of all, our model of evolutionary type behavior of species in reaction to the actions
of the distinguished major player bears similarity with the recently developed models of
mean-ﬁeld games with a major player (see [47], [74], [85], [61]), where also the necessity
to consider various classes of players is well recognized, see also [20] and [21]. However,
unlike the mean-ﬁeld game setting, (see e. g. [18], [65], [48]), our species do not rationally
optimize the strategies based on the observed environment, but rather mechanically copy
(myopic hypothesis) better strategies of randomly chosen neighbors.

The paper [39] proves the convergence (after a natural scaling) of a centrally con-
trolled discrete-time Markov chain of large number of constituents to the deterministic
continuous-time dynamics given by ordinary diﬀerential equations. Similar results are
obtained in [55] for continuous-time Markov chains with possibly competitive control.

The derivation of various evolutionary dynamics as the dynamic law of large number
for Markov models of binary or mean-ﬁeld interaction is well developed in the literature
on evolutionary games. For instance, paper [26] proves the convergence to a deterministic
ODE of the Markov model, where the pairwise interaction is organized in discrete time
so that at any moment a given fraction α(N) of a homogeneous population of N species
is randomly chosen and decomposed into matching pairs, which afterwards experience
simultaneous transformations into other pairs according to a given distribution. Paper
[30] extends this setting to include several types of species and the possibility of diﬀerent
scaling that may lead, in the limit N
, not only to ODE, but to a diﬀusion process. In
[52] the general class of stochastic dynamic law of large number is obtained from binary
or more general kth order interacting particle systems (including jump-type and L´evy
processes as a noise). The study of [16] concentrates on various subtle estimates for the
deviation of the limiting deterministic evolution from the approximating Markov chain
for the evolution that allows a single player (at any random time) to change her strategy
to the strategy of another randomly chosen player.

→ ∞

3

→

A related trend of research analyzes various choices of Markov approximation to re-
peated games and their consequences to the question of choosing a particular Nash equi-
librium amongst the usual multitude of them. Seminal contribution [50] distinguishes
speciﬁcally the myopic hypothesis, the mutation or experimentation hypothesis and the
inertia hypothesis in building a Markov dynamics of interaction. As shown in [50] (with
similar result in [87]), introducing mutation of strength λ and then passing to the limit
λ
0 allows one to choose a certain particular Nash equilibrium, called a long run
equilibrium (or statistically stable, in the terminology of [38]) that for some coordination
games turns out to coincide with the risk-dominant (in the sense of [46]) equilibrium. Fur-
ther important contributions in this direction include [34], [23], [24] showing how diﬀerent
equilibria could be obtained by a proper ﬁddling with noise (for instance local or uniform
as in [34]) and discussing the important practical question of ’how long’ is the ’long-run’
In particular paper [24] discusses in
(for a recent progress on this question see [63]).
detail the crucial question of the eﬀect of applying the limits t
, τ
0 (the limit
from discrete to continuous replicator dynamics), N
0 in various order.
Further development of the idea of local interaction leads naturally to the analysis of the
corresponding Markov processes on large networks, see [68] and references therein. Some
recent general results of the link between Markov approximation to the mean ﬁeld (or
ﬂuid) limit can be found in [66] and [17]. Though in many papers on Markov approxima-
tion, the switching probabilities of a revising player depends on the current distribution of
strategies used (assuming implicitly that this distribution is observed by all players) there
exist also interesting results (initiated in [80], see new developments in [81]) arising from
the assumption that the switching of a revising player is based on an observed sample of
given size of randomly chosen other payers.

→ ∞
→

and λ

→ ∞

→

In the abundant literature on the models of evolutionary growth (see [84] for a review),
the discussion usually starts directly with the deterministic limiting model, with the
underlying Markov model being just mentioned as a motivating heuristics.

1.3

Informal description of the model

The models we discuss here in laymen terms will be given precise mathematical meaning
in Subsection 2.2.

In the inspection game with a large number of inspectees, see [59], any one from a large
group of N inspectees has a number of strategies parametrized by a ﬁnite or inﬁnite set of
nonnegative numbers r indicating the level at which she chooses to break the regulations
(r = 0 corresponds to the full compliance). These can be the levels of tax evasion, the
levels of illegal traﬃc through a check point, the amounts at which the arms production
exceeds the agreed level, etc. On the other hand, a speciﬁc player, the inspector, tries to
identify and punish the trespassers. Inspector’s strategies are real numbers b indicating the
level of her involvement in the search process, for instance, the budget spent on it, which
is related in a monotonic way to the probability of the discovery of the illegal behavior
of trespassers. The payoﬀ of an inspectee depends on whether her illegal behavior is
detected or not. If social norms are taken into account, this payoﬀ will also depend on the
overall crime level of the population, that is, on the probability distribution of inspectees
playing diﬀerent strategies. The payoﬀ of the inspector may depend on the ﬁnes collected
from detected violators, on the budget spent and again on the overall crime level (that
she may have to report to governmental bodies, say). As time goes by, random pairs of

4

inspectees can communicate in such a way that one inspectee of the pair can start copying
the strategy of another one if it turns out to be more beneﬁcial. Then one can argue that
this evolution (or more precisely, its limit as N
) eventually settles down to one of
its stable equilibria. The analysis of such equilibria was the main objective of [59].

→ ∞

This model naturally extends to a more general setting where a distinguished ’big’
player exerts certain level b of pressure on (or interference into the aﬀairs of) a large group
of N ’small’ players that can resist this pressure on a level r. The term ’small’ reﬂects
the idea that the inﬂuence of each particular player becomes negligible as N
. As
an example of this general setting one can mention the interference of humans on the
environment (say, by hunting or ﬁshing) or the use of medications to ﬁght with infectious
bacteria in a human body, with resisting species having the choice of occupying the areas
of ample foraging but more dangerous interaction with the big player (large resistance
levels r) or less beneﬁcial but also less dangerous areas (low r). Another example can be
the level of resistance of the population on a territory occupied by military forces.

→ ∞

A slightly new twist to the model presents the whole class of games modeling corrup-
tion (see [1], [49], [64], [70] and [57] and references therein for a general background). For
instance, developing the initial simple model of [15], a large class of these games studies
the strategies of a benevolent principal (representing, say, a governmental body that is
interested in the eﬃcient development of economics) that delegates a decision-making
power to a non-benevolent (possibly corrupt) agent, whose behavior (legal or not) de-
pends on the incentives designed by the principal. The agent can deal, for example, with
tax collection of ﬁrms. The ﬁrms can use bribes to persuade a corrupted tax collector to
accept falsiﬁed revenue reports. In this model the set of inspectors can be considered as
a large group of small players that can choose the level of corruption (quite in contrast
to the classical model of inspection) by taking no bribes at all, or not too much bribes,
etc. The strategy of the principal consists in ﬁddling with two instruments: choosing
wages for inspectors (to be attractive enough, so that the agents should be afraid to loose
it) and investing in activities aimed at the timely detection of the fraudulent behavior.
Mathematically these two types are fully analogous to preemptive and defensive methods
discussed in the literature on counterterrorism (described in detail below in Subsection
2.2).

Another ’linguistic twist’ that changes ’detected agents’ to ’infected agents’ brings us
directly to the (seemingly quite diﬀerent) setting of cyber-security or biological attack-
defence games. Yet another ’turn of the screw’ that extends the setting (more-or-less
straightforwardly) to possibly diﬀerent classes of small players, brings us to the domain
of optimal allocation games, but now in the competitive evolutionary setting, where the
principal (say an inspector) has the task to distribute limited resources as eﬃciently as
possible. As another related area let us stress the analysis of terrorism and counterterrorist
measures, where it is natural to consider terrorists or terrorists organizations as small
players against a principal representing a government of a target country.

Furthermore, in many situations, the members of the pool of small players have an
alternative class of strategies of collaborating with the big player on various levels c. The
creation of such possibilities can be considered as a strategic action of the major player
(who can thus exert some control on the rules of the game). In biological setting this
is, for instance, the strategy of dogs joining humans in hunting their ’relatives’ wolves
or foxes (nicely described poetically as the talk between a dog and a fox in the famous
novel [79]). Historical examples include the strategy of slaves helping their masters to

5

terrorize and torture other slaves and by doing this gaining for themselves more beneﬁcial
conditions, as described e.g. in the classics [14]. As a military example one can indicate
the strategy of the part of the population on a territory occupied by foreign militaries that
joins the local support forces for the occupants, for US troops in Iraq this strategy being
well discussed in Chapter 2 of [71]. Alternatively, this is also the strategy of population
helping police to ﬁght with criminals and/or terrorists. In the world of organized crime it
is also a well known strategy to play simultaneously both resistance (committing crime)
and collaboration (to collaborate with police to get rid of the competitors), the classic
presentation in ﬁction being novel [36].

It is worth stressing the existence of a large number of problems, where it is essential
in particular, with the state-space
to work with inﬁnite state-space of small players,
being the set of all natural numbers. Mathematical results are much rare for this case,
as compared with ﬁnite state-spaces, and we pay much attention to it. This inﬁnite-
dimensional setting is crucial for the analysis of models with growth, like merging banks
or ﬁrms on the market (see [75] and [78]) or the evolution of species and the development
of networks with preferential attachment (the term coined in [13]), for instance scientiﬁc
citation networks or the network of internet links (see a detailed discussion in [62]). Models
of growth are known to lead to power laws in equilibrium, which are veriﬁed in a variety of
real life processes, see e.g. [78] for a general overview and [76] for particular applications in
crime rates. Here we are interested in the response of such system to external parameters
that may be set by the principal (say, by governmental regulations) who has her own
agenda (may wish to inﬂuence the growth of certain economics sectors). Apart from the
obvious economic examples mentioned above, similar process of the growth of coalitions
under pressure can be possibly used for modeling the development of human cooperation
(forming coalitions under the ’pressure’ exerted by the nature) or the creation of liberation
armies (from the initially small guerillas groups) by the population of the territories
oppressed by an external military force. Of course these processes have a clear physical
analogs, say the formation of dimers and trimers by the molecules of gas with eventual
condensation under (now real physical) pressure. The relation with the Bose-Einstein
condensation is also well known, see e. g. [22] and [84].

2 The best response principal

2.1 Discrete setting

We shall consider a game of a major ’big’ player P (the principal) with a group of small
(indistinguishable) players. The strategies of the big player are points b in a compact
convex subset of a Euclidean space. In the simplest examples points b belong to a closed
interval and can be interpreted as the level of involvement in the actions of the group
(say, a budget of a big player). In general, its multidimensional character is natural as
describing possible various instruments that can be used to inﬂuence other players or
various allocations to groups of small players with various strategies.
Let us start with the case of a ﬁnite number of strategies

of each small
· · ·
player. Thus the state space of the group is Zd
+, the set of sequences of d non-negative
integers n = (n1, ..., nd), where each ni speciﬁes the number of players in the state i. Let
= j and
N denote the total number of players in the state n: N = n1 + ... + nd. For i

, d

1,

}

{

6

6
a state n with ni > 0 denote by nij the state obtained from n by removing one agent of
1 and nj + 1
type i and adding an agent of type j, that is ni and nj are changed to ni −
respectively. Let the payoﬀ Ri(x, b) of the strategy i against the player P be a continuous
function of the strategy b of P and the overall distribution

x = (x1,

, xd) = (n1,

, nd)/N

Σd

∈

· · ·

· · ·

of the strategies applied, where Σd is the standard simplex of vectors with non-negative
coordinates summing up to 1 (that is, the set of probability laws on

, d

1,

).

Assuming that P has some strategy b(x, N) let us consider the following Markov
model of the interaction of the group. With some rate κ/N any pair of agents can meet
and discuss their payoﬀs. This discussion may result in the player with lesser payoﬀ Ri
switching to the strategy with the better payoﬀ Rj, which may occur with probability
α(Rj −
Ri), where α > 0 is a proportionality constant. In future we set α = 1, as it can
be directly incorporated in κ.

{

· · ·

}

Remark 1. We are working here with a pure myopic behavior for simplicity. Introduction
of random mutation on global or local levels (see e. g.
[50] for standard evolutionary
games) would not aﬀect essentially the convergence result below, but would lead to serious
changes in the long run of the game, which are worth being exploited.

|

{

Rj −

More rigorously, the process is described as follows. At the initial moment to any pair
(where Ai and Aj are in the state i and j respectively) is attached a
Ai, Aj}
of agents
/N-exponential waiting time (the expec-
random clock, which will click after α
Ri|
tation of this time is N/α
Ri|
1)
exponential waiting times is of course also an exponential waiting time. If this minimum
, then the agent with the lower R, say Ai, changes her
is realized on the pair
state to the one with higher R, say Aj, and the process continues analogously from the
new state (all clocks are set to zero). (Alternatively, the same process is described by
one exponential clock such that, when it clicks, the updating pair (i, j) is chosen with
probability proportional to the product ninj of sizes of each strategy and the diﬀerence of
their payoﬀs.) This process is a continuous-time Markov chain on Zd
+ with the generator

). The minimum of all these independent N(N

Rj −
|
Ai, Aj}

−

{

Lb,N f (n) =

1
N

i,j:Rj(n/N,b(n/N,N ))>Ri (n/N,b(n/N,N ))
X

κninj

[Rj(n/N, b(n/N, N))

Ri(n/N, b(n/N, N))][f (nij)

×

−

In terms of distributions x = n/N it becomes

f (n)].

−

Lb,N f (x) = N

κxixj

[Rj(x, b(x, N))

×

−

i,j:Rj(x,b(x,N ))>Ri(x,b(x,N ))
X
Ri(x, b(x, N))][f (x

ei/N + ej/N)

−

f (x)],

−

where e1, ..., ed denotes the standard basis in Rd.

(1)

(2)

We are interested in the asymptotic behavior of the chains generated by Lb,N , as
. As will be shown, the limiting process turns out to be a deterministic one

N
governed by the system of ODE

→ ∞

˙xj =

κxixj[Rj(x, b(x))

i
X

−

7

Ri(x, b(x))],

j = 1, ..., d,

(3)

which is the system of kinetic equations generalizing (and modifying) the usual repli-
cator dynamics. At the end of this section we shall discuss some consequences to the
corresponding game with ﬁnite number of players.

Remark 2. The heuristic arguments leading to the equations of type (3) are well presented
in the literature (see e. g. [25] or [59]) and will not be reproduced here. The general context
of deterministic limit is discussed in [55].

To go further we have to model the behavior of the major player. As a warm-up,
we start in this section with a simpler case of a short-sighted major player that can
make instantaneous adjustments to her strategy without additional costs. Namely, let
us assume that the payoﬀ of P playing against the group of small players is given by a
function B(x, b, N), which is smooth and concave in b, so that for all x, N the maximum
point

is uniquely deﬁned, and that P chooses b∗(x, N) as her strategy at any time.

b∗(x, N) = argmax B(x, b, N)

Let us denote by X ∗

N (t, x) the Markov chain generated by (2) and starting in x

at the initial time t = 0, with b∗ used instead of b.

(4)

Zd

+/N

∈

We use the (standard) notations for norms, Lipschitz norms and functional spaces

speciﬁed in Appendix 5.1.

Theorem 2.1. Assume

b∗(x, N)

b∗(x)

|

0, as N

(5)
and some function b∗(x), and let the functions Ri(x, b),
with some ǫ(N)
b∗(x, N) and b∗(x) belong to CbLip in all variables with norms uniformly bounded by some
ω > 0. Suppose the initial data x(N) of the Markov chains X ∗
N (t, x(N)) converge to a
certain x in Rd, as N
. Then these Markov chains converge in distribution to the
→ ∞
deterministic evolution Xt(x) solving the equation

→ ∞

ǫ(N),

| ≤

→

−

˙xj =

κxixj[Rj(x, b∗(x))

Ri(x, b∗(x))],

j = 1, ..., d,

(6)

−

i
X

with initial condition x. This equation is globally well-posed: for any initial x
solution Xt(x) exists and belongs to Σd for all times t.

Σd, the

∈

For smooth or Lipschitz g, the following rates of convergence are valid:

|

−

Eg(X ∗

N (t, x(N))

g(Xt(x(N)))

d
√N
(cid:19)
dt2/3
N 1/3 + tǫ(N)
(cid:19)
x
x(N)
|
with constants C(ω, t) uniformly bounded for bounded sets of ω and t.

g(Xt(x(N)))

g(Xt(x(N))

N(t, x(N))

g(Xt(x))

Eg(X ∗

tC(ω, t)

+ ǫ(N)

C(ω, t)

C(ω, t)

kbLip|

(cid:18)
g

| ≤

| ≤

| ≤

−

−

−

(cid:18)

k

|

|

k

g

kC2(Σd),

g

kbLip,

k

(7)

(8)

(9)

Remark 3. (i) We separate (9) from (7) to stress that (7) holds without the assumption
x. The dependence on t and d is not essential here, but the
of the convergence x(N)
latter becomes crucial for dealing with inﬁnite state-spaces, while the former for dealing
with a forward looking principal. (ii) The convergence result of (i) follows more-or-less
directly from the general theory (the settings of [16] or Section 11.9 of [56] are just slightly
diﬀerent). We give an analytic proof aiming at the eﬀective rates of weak convergence,
improving essentially the results of [55] that dealt with smooth coeﬃcients R.

→

8

Proof. The well-posedness of (6) is more or less obvious, and it is a particular case of
more general Theorem 6.1 of [54] or Lemma 5.5 of Appendix (with the barrier L being
identically 1). Once the well-posedness is established, the Lipshitz continuity (9) of the
solutions is a standard fact from the theory of ODEs.

Next, since any function g

C(Rd) can be approximated by functions from C 2(Rd),
the convergence of Markov chains from Statement (i) follows from (7) and (9). Thus it
remains to show (7) and (8).

∈

Let us start with some calculations concerning Lb,N assuming that limN→∞ b(x, N) =

b(x) exists and that f

∈

C 1(Σd). Then we ﬁnd, expanding f in Taylor series, that

lim
N→∞, n/N→x

Lb,N f (n/N) = Λbf (x),

where

Λbf (x) =

i,j:Rj(x,b(x))>Ri(x,b(x))
X

or equivalently

κxixj[Rj(x, b(x))

Ri(x, b(x))]

−

∂f
∂xj −

∂f
∂xi (cid:21)

(cid:20)

(x),

(10)

d

Λbf (x) =

κxixj[Rj(x, b(x))

i,j=1
X

Ri(x, b(x))]

−

∂f
∂xj

(x).

(11)

Thus the limiting operator Λbf is the ﬁrst-order PDO with characteristics solving the
equations (3), which turn to the required equations (6) when b = b∗. What is left is the
rigorous proof that the convergence of the generators Lb∗,N to Λb∗ on smooth functions f
implies the convergence of the corresponding semigroups.

The main idea is to approximate all Lipschitz continuous functions involved by the
smooth ones. Namely, choosing an arbitrary molliﬁer χ (non-negative inﬁnitely smooth
χ(w) dw = 1) and the corresponding
even function on R with a compact support and
χ(yj) on Rd−1, let us deﬁne, for any function V on Σd, its approximation
molliﬁer φ(y) =

R

Q
Φδ[V ](x) =

1
δd−1 φ

y
δ

(cid:16)

(cid:17)

ZRd−1

V (x

−

y) dy =

1
δd−1 φ

x

y

−
δ

(cid:18)

(cid:19)

Rd−1

Z

V (y) dy.

Notice that Σd is (d
function of ﬁrst (d
continuous way). It follows that

−

1)-dimensional object, so that any V on it can be considered as a
Σd (continued to Rd−1 in an arbitrary

−
1) coordinates of a vector x

∈

Φδ[V ]

kC1 =

k

|

Φδ[V ]

kbLip ≤ k

V

kbLip

(12)

for any δ and

Φδ[V ](x)

|

V (x)

−

| ≤

Z

1
δd−1 φ

y
δ

(cid:16)

|

(cid:17)

V (x

y)

V (x)

dy

|

−

−

V

kLip

1
δd−1 φ

y
δ

y

|1 dy

≤ k

≤
Remark 4. We care about dimension d in the estimates only for future use (here it is
irrelevant). By a diﬀerent choice of molliﬁer φ one can get rid of d in (13), but then it
would pop up in (14), which is avoided with our φ.

−

(cid:16)

(cid:17)

Z

Z

k

|

|

|

δ(d

1)

V

w

χ(w) dw.

(13)

kLip

9

Next, the norm

Φδ[V ]

k

kC2 does not exceed the sum of the norm

Φδ[V ]

kC1 and the

k

supremum of the Lipschitz constants of the functions

∂
∂xj

Φδ[V ](x) =

1
δd

(cid:18)

∂
∂xj

φ

y
δ

(cid:17)

(cid:19) (cid:16)

Z

V (x

−

y) dy.

Hence

1
kC2
δ
|
N (t, x): U t
N denote the semigroup of the chain X ∗

Φδ[V ]

kbLip

≤ k

1 +

χ′(w)

(cid:18)

V

Z

k

|

Let U t

(cid:19)
N g(x) = Eg(X ∗

dw

.

(14)

the semigroup of the deterministic process generated by (6): U tg(x) = g(Xt(x)). Let U t
and U t

δ be the same semigroups but built with respect to the functions
1
δd φ

Φδ[Rj](x) =

y, b∗(x

y)) dy

Rj(x

y
δ

−

−

(cid:17)
rather than Rj(x, b∗(x, N)) and Rj(x, b∗(x)) respectively. Similarly we denote by Lδ
and Λδ
instead of Rj.
Then

b∗,N
t (x) the solution of (6) with Φδ[Rj] used

b∗ the corresponding generators and by X δ

(cid:16)

Z

N (t, x)), and U t
N,δ

d
dt

|
Xt(x)

|
U tg(x)

X δ

t (x)

−

|1 ≤
=

(Xt(x)

X δ

t (x))

|1 ≤

−

2δ + 4ω

Xt(x)

|

X δ

t (x)

|1,

−

implying that

δtC(ω, t) and hence

kbLipδtC(ω, t).
Moreover, by Lemma 5.1 (its simplest ﬁnite dimensional version) and (14)

g(Xt(x)

δg(x)

t (x))

| ≤ k

−

−

g

|

|

|

g(X δ

U t

U t

δg(x)

|

|C2

≤

C(ω, t)

g

kC2 +

k
(cid:18)

1
δ k

g

kbLip

.

(cid:19)

Next we use (102) to get

U t

N g

k

−

U t
δg

k ≤

t sup
s∈[0,t] k

(Lb∗,N −

Λδ

b∗)U s
δ g

k

t sup
s∈[0,t]

≤

(Lb∗,N −

k

b∗,N )U s
Lδ
δ g

+

k

k

(Lδ

b∗,N −

Λδ

b∗)U s
δ g

.

k

(cid:1)

Then

(Lb∗,N −
and (using (16))

k

b∗,N )U s
Lδ
δ g

(cid:0)

k ≤

C(ω)(ǫ(N) + dδ)

U s
δ g

k

kbLip ≤

C(ω, s)(ǫ(N) + dδ)

g

kbLip,

k

(Lδ

1
N k
Thus choosing δ = 1/√N, makes the decay rate of δ and 1/(Nδ) equal yielding (7).

kC2(1 + 1/δ).

b∗)U s
δ g

b∗,N −

1
N k

C(ω, t)

C(ω, t)

U s
δ g

kC2

k ≤

Λδ

≤

k

g

Finally, if g is only Lipschitz, we approximate it by Φ˜δ[g], so that the second derivative

of Φ˜δ[g] is bounded by

g

k

kbLip/˜δ. Thus the rates of convergence for g become of order
[d˜δ + t(ǫ(N) + δd + 1/(Nδ˜δ))]
g
k

kbLip.

Choosing δ = (tN)−1/3, ˜δ = t2/3N −1/3 makes the decay rate of all terms (apart from ǫ(N))
equal yielding (8) and completing the proof.

10

(15)

(16)

(17)

Theorem 2.1 suggests that eventually the evolution will settle down near some stable
equilibrium points of dynamic systems (6). Analysis of stability of these equilibria will be
carried out elsewhere. As was mentioned, for a particular case of evolutionary inspection
games it was worked out in [59]. Let us observe only that system (6) is quite speciﬁc in the
sense that its singular points can be easily identiﬁed. In fact, for a subset I
,
let

⊂ {

· · ·

, d

1,

}

ΩI =

x

{

∈

Σd : xk = 0

k

I, and Rj(x, b∗(x)) = Ri(x, b∗(x)) for i, j /
∈

I

.

}

∈

⇐⇒

Theorem 2.2. A vector x with non-negative coordinates is a singular point of (6), that
is, it satisﬁes the system of equations

κxixj[Rj(x, b∗(x))

Ri(x, b∗(x))] = 0,

j = 1, ..., d,

(18)

−

i
X
ΩI for some I

if and only if x

∈

}
Proof. Since for any I such that xk = 0 for k
system but with coordinates k /
∈
this situation, system (18) reduces to

⊂ {

· · ·

1,

, d

.

I the system (18) reduces to the same
I, it is suﬃcient to show the result for the empty I. In

∈

xi[Rj(x, b∗(x))

−

i
X

Ri(x, b∗(x))] = 0,

j = 1, ..., d.

(19)

Subtracting jth and kth equations of this system yields

(x1 +

· · ·

+ xd)[Rj(x, b∗(x))

Rk(x, b∗(x))] = 0,

−

Rj(x, b∗(x)) = Rk(x, b∗(x)),

and thus

as required.

So far we have deduced the dynamics arising from a certain Markov model of inter-
action. As it is known, the internal (not lying on the boundary of the simplex) singular
points of the standard replicator dynamics of evolutionary game theory correspond to
the mixed-strategy Nash equilibria of the initial game with a ﬁxed number of players (in
most examples just two-player game). Therefore, it is natural to ask whether a similar
interpretation can be given to ﬁxed points of Theorem 2.2. Because of the additional
nonlinear mean-ﬁeld dependence of R on x the interpretation of x as mixed strategies is
not at all clear. However, consider explicitly the following game ΓN of N + 1 players (that
was tacitly borne in mind when discussing dynamics). When the major player chooses the
strategy b and each of N small players chooses the state i, the major player receives the
payoﬀ B(x, b, N) and each player in the state i receives Ri(x, b), i = 1,
, d (as above,
with x = n/N and n = (n1,
, nd) the realized occupation numbers of all the states).
Thus a strategy proﬁle of small players in this game can be speciﬁed either by a sequence
of N numbers (expressing the choice of the state by each agent), or more succinctly, by
the resulting collection of frequencies x = n/N.

· · ·

· · ·

As usual one deﬁnes a Nash equilibrium in ΓN as a proﬁle of strategies (xN , bN ) such

that for any player changing its choice unilaterally would not be beneﬁcial, that is

bN = b∗

N (xN ) = argmax B(xN , b, N)

11

and for any i, j

1,

∈ {

· · ·

, d

}
Rj(x

−

ei/N + ej/N, bN )

Ri(x, bN ).

≤

(20)

A proﬁle is an ǫ-Nash if these inequalities hold up to an additive correction term not
exceeding ǫ. It turns out that the singular points of (6) describe all approximate Nash
equilibria for ΓN in the following precise sense:
Theorem 2.3. Let R(x, b) be Lipschitz continuous in x uniformly b. Set ˆR = supi,b k
For I

, let

, d

1,

Ri(., b)

kLip.

⊂ {

· · ·

}
ˆΩI =

ΩI : Rk(x, b∗(x))

x

{

∈

≤

Ri(x, b∗(x)) for k

I, j /
∈

I

.

}

∈

Then the following assertions hold.

(i) The limit points of any sequence xN such that (xN , b∗(xN )) is a Nash equilibrium
for ΓN belong to ˆΩI for some I. In particular, if all xN are internal points of Σd, then
any limiting point belongs to Ω∅.

(ii) For any I and x

N (xN )) to ΓN
such that the diﬀerence of any coordinates of xN and x does not exceed 1/N in magnitude.

ΩI there exists an 2 ˆRd/N-Nash equilibrium (xN , b∗

∈

Proof. (i) Let us consider a sequence of Nash equilibria (xN , b∗(xN )) such that the coor-
dinates of all xN in I vanish. By (20) and the deﬁnition of ˆR,

Rj(xN , b∗

N (xN ))

|

−

Ri(xN , b∗

N (xN ))

2
N

ˆR

| ≤

for any i, j /
∈

I and

Rk(xN , b∗

N (xN ))

≤

Ri(xN , b∗

N (xN )) +

2
N

ˆR,

k

I, i /
∈

∈

I.

(21)

(22)

Hence x

ˆΩI for any limiting point (x, b).

∈
ˆΩI one can construct its 1/N-rational approximation, namely a sequence
(ii) If x
xN ∈
+/N such that the diﬀerence of any coordinates of xN and x does not
Σd ∩
exceed 1/N in magnitude. For any such xN , the proﬁle (xN , b∗(xN )) is an 2 ˆRd/N-Nash
equilibrium for ΓN .

∈
Zd

Theorem 2.3 provides a game-theoretic interpretation of the ﬁxed points of dynamics

(6), which is independent of any myopic hypothesis used to justify this dynamics.

Of course, the set of ’almost equilibria’ Ω may be empty or contain many points.
Thus one can naturally pose here the analog of the question which is well discussed in
the literature on the standard evolutionary dynamics (see [23] and references therein),
namely which equilibria can be chosen in the long run (the analogs of stochastically
stable equilibria in the sense of [38]) if small mutations are included in the evolution of
the Markov approximation.

2.2 Basic examples

In the standard setting of inspection games with a possibly tax-evading inspectee (ana-
lyzed in detail in [59] under some particular assumptions), the payoﬀ R looks as follows:

Rj(x, b) = r + (1

−

pj(x, b))rj −
12

pj(x, b)f (rj),

(23)

where r is the legal payoﬀ of an inspectee, various rj denote various amounts of not
declared proﬁt, j = 1,
, d, pj(x, b) is the probability for the illegal behavior of an
inspectee to be found when the inspector uses budget b for searching operation and f (rj)
is the ﬁne that the guilty inspectee has to pay when being discovered.

· · ·

In the standard model of corruption ’with benevolent principal’, see e. g. [1], one sets

the payoﬀ of a possibly corrupted inspector (now taking the role of a small player) as

−

(1

f ),

p)(r + w) + p(w0 −
where r is now the bribe an inspector asks from a ﬁrm to agree not to publicize its proﬁt
(and thus allowing her not to pay tax), w is the wage of an inspector, f the ﬁne she has to
pay when the corruption is discovered and p the probability of a corrupted behavior to be
discovered by the benevolent principal (say, governmental oﬃcial). Finally it is assumed
that when the corrupted behavior is discovered the agent not only pays ﬁne, but is also
ﬁred from the job and has to accept a lower level activity with the reservation wage w0.
In our strategic model we make r to be the strategy of an inspector with possible levels
r1,
, rd (the amount of bribes she is taking) and the probability p of discovery to be
dependent on the eﬀort (say, budget b) of the principal and the overall level of corruption
x, with ﬁne too depending on the level of illegal behavior. This natural extension of the
standard model leads to the payoﬀ

· · ·

Rj(x, b) = (1

pj(x, b))(rj + w) + pj(x, b)(w0 −

−

f (rj)),

(24)

which is essentially identical to (23).

In the more general pressure and resistance games, the payoﬀ Rj(x, b) has the following
special features: R increases in j and decreases in b. The dependence of R and b∗ on x
is more subtle, as it may take into account social norms of various character. In case of
the pressure game with resistance and collaboration, the strategic parameter r of small
players naturally decomposes into two coordinates r = (r1, r2), the ﬁrst one reﬂecting the
level of resistance and the second the level of collaboration. If the correlation between
these activities are not taken into account the payoﬀ R can be decomposed into the sum
j (x, b) with R1 having the same features as R above, but with
of rewards R = R1
R2 increasing both in j and b.

j (x, b) + R2

As another set of examples let us look at the applications to the botnet defense (for
example, against the famous conﬂicker botnet), widely discussed in the contemporary
literature, since botnets (zombie networks) are considered to pose the biggest threat to
the international cyber-security, see e. g. review of the abundant bibliography in [19]. The
comprehensive game theoretical framework of [19] (that extends several previous simpliﬁed
models) models the group of users subject to cybercriminal attack of botnet herders as a
diﬀerential game of two players, the group of cybercriminals and the group of defenders.
Our approach adds to this analysis the networking aspects by allowing the defenders to
communicate and eventually copy more beneﬁcial strategies. More concretely, our general
model of inspection or corruption becomes almost directly applicable in this setting by
the clever linguistic change of ’detected’ to ’infected’ and by considering the cybecriminal
as the ’principal agent’ ! Namely, let rj (the index j being taken from some discrete set
here, though more advanced theory of the next sections allows for a continuous parameter
j) denote the level of defense applied by an individual (computer owner) against botnet
herders (an analog of the parameter γ of [19]), which can be the level of antivirus programs

13

installed or the measures envisaged to quickly report and repair a problem once detected
(or possibly a multidimensional parameter reﬂecting several defense measures). Similarly
to our previous models, let pj(x, b) denote the probability for a computer of being infected
given the level of defense measures rj, the eﬀort level b of the herder (say, budget or time
spent) and the overall distribution x of infected machines (this ’mean-ﬁeld’ parameter is
crucial in the present setting, since infection propagates as a kind of epidemic). Then, for
a player with a strategy j, the cost of being (inevitably) involved in the conﬂict can be
naturally estimated by the formula

Rj(x, b) = pj(x, b)c + rj,

(25)

where c is the cost (inevitable losses) of being infected (thus one should aim at minimizing
this Rj, rather then maximizing it, as in our previous models). Of course, one can extend
the model to various classes of customers (or various classes of computers) for which
values of c or rj may vary and by taking into account more concrete mechanisms of virus
spreading, as described e. g. in [67] and [69].

Yet another set of examples represent the models of terrorists’ attacks and counterter-
rorism measures, see e. g. [7], [82], [83], [28] for the general background on game -theoretic
models of terrorism, and [35] for more recent developments. We again suggest here a nat-
ural extension to basic models to the possibility of interacting large number of players and
of various levels of attacks, the latter extension being in the line with argument from [29]
advocating consideration of ’spectacular attacks’ as part of a continuous scale of attacks of
various levels. In the literature, the counterterrorists’ measures are usually decomposed
into two groups, so called proactive (or preemptive), like direct retaliation against the
state-sponsor and defensive (also referred to as deterrence), like strengthening security at
an airport, with the choice between the two considered as the main strategic parameter.
As stressed in [77] the ﬁrst group of action is ’characterized in the literature as a pure
public good, because a weakened terrorist group poses less of a threat to all potential
targets’, but on the other hand, it ’may have a downside by creating more grievances
in reaction to heavy-handed tactics or unintended collateral damage’ (because it means
to ’bomb alleged terrorist assets, hold suspects without charging them, assassinate sus-
pected terrorists, curb civil freedoms, or impose retribution on alleged sponsors’), which
may result in the increase of terrorists’ recruitment. Thus, the model of [77] includes the
recruitment beneﬁts of terrorists as a positively correlated function of preemption eﬀorts.
A direct extension of the model of [77] in the line indicated above (large number of players
and the levels of attacks) suggests to write down the reward of a terrorist, or a terrorist
group, considered as a representative of a large number of small players, using one of the
levels of attack j = 1,
, d (in [77] there are two levels, normal and spectacular only),
to be

· · ·

Rj(x, b) = (1

pj(x, b))rf ail

j

(b) + pj(x, b)(Sj + rsucc

j

(b)),

(26)

where pj(x, b) is the probability of a successful attack (which depends on the level b
of preemptive eﬀorts of the principal b and the total distribution of terrorists playing
diﬀerent strategies), Sj is the direct beneﬁts in case of a success and rf ail
(b)
are the recruitment beneﬁts in the cases of failure or success respectively. The costs of
principal are given by

(b), rsucc

j

j

−

B(x, b) =

xj [(1

j
X

pj(x, b))b + pj(b)(b + Sj)] .

−

14

It is seen directly that we are again in the same situation as described by (24) (up to
constants and notations). The model extends naturally to account for possibility of the
actions of two types, preemption and deterrence. Of importance should be its extension
to several major players (for instance, USA and EU are considered in [7]).

As was mentioned in introduction, there exists a large class of problems, where the
state space of small players become inﬁnite. We shall pay most of our attention to
the major particular case (possibly the mostly relevant one for practical purposes) of a
countable state space arising in the analysis of the models of evolutionary growth. For this
class of models the number N of agents become variable (and usually growing in the result
of the evolution) and the major characteristics of the system becomes just the distribution
x = (x1, x2,
) of the sizes of the groups. The analysis of the evolution of these models
is well -developed and has a long history, see [84]. Mathematically the analysis is similar
to ﬁnite state spaces, though serious technical complications may arise. We develop the
’strategically enhanced model’ in Section 4 analyzing such evolutions under the ’pressure’
of strategically varying parameters set by the principal.

· · ·

2.3 Compact state-space

Let us extend the analysis given above to the case of continuous state space of small
players, assuming it to be a compact convex subset Z, of a Euclidean space Rn. Let
(Z)
denote the set of probability laws on Z equipped with its weak topology. For each N the
state space of N agents becomes Z N . However, assuming agents to be indistinguishable,
the state space is better described as the set of equivalence classes of Z N with respect to
all permutations that can be naturally identiﬁed with the set MN of the normalized sums
of N Dirac measures

P

1
N

(δx1 +

· · ·

+ δxN ).

· · ·

, xN ) let us use shorter notation δx for the sum δx1 +

+δxN . Assume that
For x = (x1,
continuous functions R(x, µ, b) on (Z
N)
×M
+ δxN )/N, b) is the payoﬀ for xj in the group x =
are given such that R(xj, (δx1 +
Rr of the major player, and B((δx1 +
(x1,
+
δxN )/N, b, N) is the payoﬀ of the major player applying the eﬀort level b to the the group
x = (x1,
, xN ). Assume again that B is a smooth and strictly concave function of b,
so that

, xN ) given the level of eﬀorts b

Rr) and B(µ, b, N) on (

+(Z)

+(Z)

Rr

M

· · ·

· · ·

· · ·

· · ·

· · ·

×

×

×

∈

is well deﬁned and that the limit

b∗(δx/N, N) = argmaxb B(δx/N, b, N)

lim
N→∞

b∗(µ, N) = b∗(µ)

(Z).

exists uniformly in µ

∈ P

The direct analog of the generator (2) with b = b∗ (describing the Markov chain
produced by pairwise exchange of information) to the continuous state-space is clearly
the operator

(27)

(28)

Lb∗,N f (δx/N) =

κ

N

X(i,j)

[f (δx/N

−

δxi/N + δxj /N)

f (δx/N)]

−

[R(xj, δx/N, b∗(δx/N, N))

R(xi, δx/N, b∗(δx/N, N))],

(29)

−

×

15

where x = (x1,
way that

· · ·

, xN ) and the sum is over all pairs (i, j) of indices ordered in such a

R(xj, δx/N, b∗(δx/N, N)) > R(xi, δx/N, b∗(δx/N, N))

(the order is irrelevant if the corresponding values of R coincide).

Let us denote by X ∗
In order to see what happens with generator (29) in the limit N

N (t, δx/N) the Markov chain on MN generated by (29).

, take a linear

→ ∞

function f on measures given by the integration, that is,

f (µ) = Fg(µ) =

g(x)µ(dx).

(30)

Then

Z

Lb∗,N Fg(δx/N) =

κ

N 2

[R(xj, δx/N, b∗(δx/N, N))

×

−

X(i,j)
R(xi, δx/N, b∗(δx/N, N))][g(xj)

g(xi)].

−

Since the product of the square brackets is invariant under the change of the order of
(i, j), this rewrites in a simpler form as

R(xi, δx/N, b∗(δx/N, N))](g(xj)

−

g(xi)],

−

(31)

Lb∗,N Fg(δx/N) =

[R(xj, δx/N, b∗(δx/N, N))

κ

N

2N 2

and consequently as

i,j=1
X

Lb∗,N Fg(δx/N) =

κ

2

[g(z2)

−

g(z1)]

Z Z

R(z1, δx/N, b∗(δx/N, N))]

1
N
M(Z) this turns to

−
with any µ

∈

δx(dz1)

1
N

δx(dz2).

[R(z2, δx/N, b∗(δx/N, N))

×

Thus if δx/N

µ as N

→ ∞

→
κ

Lb∗Fg(µ) =

2

ZZ ZZ

[g(z2)

−

g(z1)][R(z2, µ, b∗(µ))

−

R(z1, µ, b∗(µ))]µ(dz1)µ(dz2),

(32)

or equivalently

Lb∗Fg(µ) = κ

g(z2)[R(z2, µ, b∗(µ))

ZZ ZZ

R(z1, µ, b∗(µ))]µ(dz1)µ(dz2).

(33)

−

These calculations make the following result plausible. Unlike ﬁnite-state-space case,
we give two diﬀerent convergence rates depending basically on whether weak or strong
regularity is assumed on the coeﬃcients. We use the notations for the spaces of functions
on measures introduced in Appendices 5.1 and 5.3. Assume for deﬁniteness that Z belongs
to the cube [0, K]n of Rn.

Theorem 2.4. (i) Suppose the functions R(x, µ, b) and b∗(µ) are bounded weakly Lipschitz
with respect to all their variables with the bounds and Lipschitz constants bounded by some
ω. Suppose the initial data δx(N )/N of the Markov chains X ∗
N (t, δx(N )/N) converge weakly
to a certain µ
. Then these Markov chains converge in distribution
∈ P
(Z) solving the kinetic equation
to the deterministic evolution on

(Z), as N

→ ∞
P

˙µt(dz) = κ

[R(z, µt, b∗(µt))

Zy∈Z

R(y, µt, b∗(µt))]µt(dy)µt(dz),

(34)

−

16

g(z)µt(dz) = κ

g(z)[R(z, µt, b∗(µt))

R(y, µt, b∗(µt))]µt(dy)µt(dz).

(35)

or equivalently in the weak form

d
dt

Z

ZZ 2
This equation is globally well-posed: for any initial µ
the solution µt(µ) exists and belongs to

−

Moreover, if g

valid:

∈

C 2

weak(

+
1 (Z))

M
C bLip
weak(

∩

M

M
Eg(X ∗

|

tC(ω, t)

≤

If g

∈

C bLip
weak(

M

+
1 (Z)), then

(cid:18)

+(Z) (respectively,

∈ M

∈ P
(Z)) for all times t.

+(Z) (in particular µ

(Z)),

+
1 (Z)), the following rate of convergence is

P

N (t, δx(N )/N))

−
N 1/(2+n) + ǫ(N)

1

(cid:19)

g(µt(δx(N )/N))

g

(

k

kC2

weak

+

g

k

|
kweakLip).

(36)

Eg(X ∗

N(t, δx(N )/N))

|

g(µt(δx(N )/N))

C(ω, t)

| ≤

−

t

(tN)1/(2n+3) + tǫ(N)

(cid:19)

(cid:18)

g(µt(δx(N )/N))

|

g(µt(µ))

C(ω, t)

g

k

kbLip|

| ≤

−

dbLip∗(δx(N )/N, µ),

with constants C(ω, t) uniformly bounded for bounded ω and t.

g

kweakLip,

k

(37)
(38)

(ii) Not assuming weak Lipschitz continuity, but assuming instead that R and b are

strongly twice continuously diﬀerentiable, one has the following rate of convergence

Eg(X ∗

N (t, δx(N )/N)

|

−

g(µt(δx(N )/N))

tC(ω, t)

| ≤

1
N k

g

kC2(M1(Z)).

(39)

Remark 5. (i) A probabilistic proof of convergence is again well known (via the tightness
of the related martingale problems), see e.g. similar argument in Theorem 4.1. of [53],
but it does not supply the rates that are crucial for applications to optimal control. (ii)
All estimates reduce to the estimates of Theorem 2.4 by setting n = 0, as expected (the
dimension of a ﬁnite set is zero).

Proof. Well-posedness of (34) is a consequence of Lemma 5.5 (with the barrier L being
identically 1). Then estimate (38) is the standard Lipschitz continuity of the solutions of
ODE with Lipschitz coeﬃcients with respect to initial data. Estimate (39) is obtained
analogously to (36) using strong derivatives instead of weak ones, but much simpler indeed,
as the assumption of smoothness allows one to avoid any additional approximations.

Let us concentrate on (36) and (37).
The generator above is calculated only for linear functionals on measures. To calculate
it for arbitrary smooth functionals, one has to use the technique of variational derivatives
(recalled in Appendix). Namely, for a smooth f the value of Lb∗,N f (µ) is given by Lemma
5.4, that is, it coincides with

b∗ f (µ) = κ
Llim

δf (µ)
δµ(z2)

ZZ 2

[R(z2, µ, b∗(µ))

−

R(z1, µ, b∗(µ))]µ(dz1)µ(dz2)

up to an additive correction of order 1/N depending on the second derivatives of f .

To deduce the convergence of processes from the convergence of generators on f

∈
(Z)) we follow the same strategy of approximation as above for Theorem 2.1. An

C 2

weak(

M

17

additional ingredient is the approximation of a weakly Lipschitz function F on
the weak Lipschitz constant
I in [54]). Namely, for j
the lattice of (j + 1)n points in [0, K]n and φk
Rn given by

(Z), with
kweakLip, by ﬁnite-dimensional functionals (see Appendix
, kn) with kl ∈ {
, be
j be the collection of (j + 1)n functions on

j = (K/j)k, k = (k1,

F
N, let xk

M

· · ·

· · ·

, j

0,

∈

}

k

φj
k(x) =

n

χ

i=1
Y

(cid:18)

j
K

(xi −

ki

K
N

)

, χ(z) =

(cid:19)

(

1
0,

z

,

z
|
|
z
| ≥

− |
|

1,

| ≤
1.

This choice of functions φj

negative functions satisfying the following conditions: for any j,
an arbitrary x can belong to the supports of not more than 2n of functions φj

k is not at all unique. It is just a concrete example of non-
k = 1 and

k=(k1,··· ,kn) φj

k; and

P

φj
k(x)

|

−

φj
k(y)

| ≤

j
K |

x

y

|1.

−

(40)

Then one deﬁnes the ﬁnite-dimensional projections in the spaces of functions and

measures on Z

[0, K]n:

⊂

Pj(f ) =

f (xk

j )φk

j , P ∗

j (µ) =

(φl

j, µ)δxl

,

j

Xk
and the corresponding ﬁnite-dimensional projections on Cweak(

Xl

+
1 (Z))

M

F (µ)

7→

Fj(µ) = F (P ∗

j (µ)).

The projections Pj have the following properties:

Pjk ≤ k

f

k

k

,

Pjf

k

f

−

k ≤

2nn

K
j k

f

kLip,

Pjf

k

kLip ≤

2n+1n
k

f

kLip.

(41)

The ﬁrst one is obvious. The second one follows from the estimate

Pjf

k

f

k

−

=

|

Xk

(f (xk
j )

f (x))φk

j (x)

2d max

(f (xk
j )

|

f (x)

,

|

−

| ≤

−

where max is over those k that x belongs to the support of φk
nK/j. Then
inequality of (41), take arbitrary x, y with

x

y

j . To prove the third

Pjf (x)

|

Pjf (y)

=

|

−

k)φj

k(x)

f (xj

k)φj

k(y)].

−

|1 ≤

|
−
[f (xj

Xk

Here the sum is over not more than 2n+1 lattice points (maximum 2n for either x or y).
Let k0 be one of these points. Then

Pjf (x)

|

Pjf (y)

=

|

|

−

Xk6=k0

[f (xj

k)φj

k(x)

f (xj

k)φj

k(y)] + f (xj

k0)(

−

φj
k(y))

φj
k(y))

|

−

Xk6=k0

(f (xj
k)

−

=

|

Xk6=k0

f (xj

k0))(φj

k(x)

φj
k(y))

2n+1

f

k

| ≤

−

18

Xk6=k0
K
j

kLip

n

j
K |

x

y

|1,

−

yielding the third estimate of (41). From (41) it follows that

dbLip∗(P ∗

j µ1, P ∗

j µ2)

2n+1n dbLip∗(µ1, µ2),

≤

k

2n+1n
k

F

kweakLip,

(42)

dbLip∗(P ∗

j µ, µ)

2nn

≤

K
j

,

Fj(µ)

k

−

Now Fj(µ) can be written as some function Fj(µ) = fj(

FjkweakLip ≤
2nn

F (µ)

k ≤

K
j k

F

(φj

k, µ)

}

{

kweakLip.
) of (j + 1)n variables

(43)

k = (φj
uj

k, µ)

such that

}

uj =

{
fj(uj1)

fj(uj2)

2n+1n
k

F

−

| ≤

k1.
k −
|
Thus f is Lipschitz in u and we can apply the same smooth approximation as in the proof
of Theorem 2.1 above. Here the dimension becomes essential. Namely, using literally the
same argument as in Theorem 2.1 we obtain

k kbLip∗ ≤

kweakLipk

kweakLipk

X

−

F

2n+1n
k

uj2
k )δxj

(uj1

uj2

uj1

Eg(X ∗

N (t, δx(N )/N))

g(µt(δx(N )/N))

|

−

|
1
j

(cid:18)

tC(ω, t)

≤

+ ǫ(N) + δ(j + 1)n +

1
δN

g

(

k

kC2

weak

+

g

kweakLip).

k

(44)

(cid:19)

Choosing j = N β and δ = N −(1−β) with β = 1/(2 + n) makes the rates of decay of 1/j,
δjn and 1/(Nδ) equal yielding (36).

Finally, if g is assumed to be only weakly Lipschitz, we approximate it by the smooth

one, as above. Thus the rates of convergence for g become of order

Choosing

[jn˜δ + t(ǫ(N) + 1/j + δjn + 1/(Nδ˜δ))]

g

kbLip.

k

j = (tN)1/(2n+3),

δ = j−(n+1),

˜δ = tδ = tj−(n+1)

makes the decay rate of all terms (apart from ǫ(N)) equal yielding (37) and completing
the proof.

The extension of Theorem 2.2 to the present case is as follows.

Theorem 2.5. A (non-negative) measure µ is a singular point of (34), that is, it satisﬁes

[R(z, µ, b∗(µ))

−

Zy∈Z

R(y, µ, b∗(µ))]µ(dy)µ(dz) = 0,

(45)

if and only if the function R(., µ, b∗(µ)) is constant on the support of µ.

Proof. Denoting

=

µ

k

k

ZZ

µ(dy),

(R, µ) =

R(y, µ, b∗(µ))µ(dy),

ZZ

equation (45) rewrites as

and the result follows.

R(z, µ, b∗(µ))µ(dz) =

(R, µ)
µ

k

k

µ(dz),

(46)

The corresponding extension of Theorem 2.3 is now also straightforward.

19

2.4 Optimal allocation and group interaction

So far our small players were indistinguishable. However, in many cases the small players
can belong to diﬀerent types. These can be inspectees with various income brackets,
the levels of danger or overﬂow of particular traﬃc path, or the classes of computers
susceptible to infection. In this situation the problem for the principal becomes a policy
problem, that is, how to allocate eﬃciently her limited resources. Our theory extends to
a setting with various types more-or-less straightforwardly. We shall touch it brieﬂy.

ZA

Let our players, apart from being distinguished by states i

, can be also
classiﬁed by their types or classes α
. The state space of the group becomes
A}
Zd
+, the set of matrices n = (niα), where niα is the number of players of type α in the
state i (for simplicity of notation we identify the state spaces of each type, which is not
at all necessary). One can imagine several scenarios of communications between classes,
two extreme cases being as follows:

+ ×

∈ {

∈ {

· · ·

· · ·

, d

1,

1,

}

,

(C1) No-communication: the players of diﬀerent classes can neither communicate nor
observe the distribution of states in other classes, so that the interaction between types
arises exclusively through the principal;

(C2) Full communication: the players can change both their types and states via
pairwise exchange of information, and can observe the total distribution of types and
states.

There are lots of intermediate cases, say, when types form a graph (or a network) with
edges specifying the possible channels of information. Let us deal here only with cases
(C1) and (C2). Starting with (C1), let Nα denote the number of players in class α and
nα the vector

, d. Let xα = nα/Nα,

, i = 1,

niα}

{

· · ·

x = (xiα) = (niα/Nα)

(Σd)A,

∈
, bA) be the vector of the allocation of resources of the principal, which

and b = (b1,
may depend on x. Assuming that the principal uses the optimal policy

· · ·

b∗(x) = argmax B(x, b)

(47)

arising from some concave (in the second variable) payoﬀ function B on (Σd)A
generator (2) extends to

×

RA, the

A

Lb∗,N f (x) =

Nακα

xiαxjα

[Rα

j (xα, b∗(x))

i (xα, b∗(x))][f (x

α=1
X
Rα

−

i,j:Rα

j (xα,b∗(x))>Rα

i (xα,b∗(x))
X
i /Nα + eα
eα

−

j /Nα)

f (x)],

−

is now the standard basis in Rd

RA. Passing to the limit as N

×

(48)

under

→ ∞

×

where eα
i
the assumption that

with some constants ωα we obtain a generalization of (6) in the form

lim
N→∞

Nα/N = ωα

˙xjα = καωα

xiαxjα[Rα

j (xα, b∗(x))

for j = 1, ..., d and α = 1,

· · ·

i
X
,

A

, coupled with (47).

20

Rα

i (xα, b∗(x))],

−

(49)

In case (C2), x = (xiα)

∈

Σdα, the generator becomes

Lb∗,N f (x) =

A

Nκ

xiαxjα

Xα,β=1
Rβ
i (x, b∗(x))][f (x
j (x, b∗(x))
and the limiting system of diﬀerential equations

[Rα

i,j:Rα

−

×

j (x,b∗(x))>Rβ

i (x,b∗(x))

X
eβ
i /Nα + eα

−

j /Nα)

f (x)],

−

˙xjα = κ

xiβxjα[Rα

j (x, b∗(x))

Xi,β

Rβ

i (x, b∗(x))].

−

(50)

(51)

So far we have assumed that the propagation of strategies is due to pairwise interac-
tion (say, exchange of opinions). Let us now extend the model by allowing simultaneous
interactions in groups of arbitrary size, with appropriate scaling that makes the contri-
bution of simultaneous group interaction comparable with the contribution of pairwise
exchange. For humans this kth order interaction seems to be even more realistic than
in chemistry, where similar considerations leads to the so-called mass-action law for the
rates of chemical reactions, see [42] for the latter. Equations (55) below can be considered
as a performance of the ’mass action law for agents’ playing against the principal.

{

i1,

· · ·

ik}

Assume that any collection of k small players

, with k not exceeding certain
level K, can be formed randomly with uniform distribution (any collection of k players
is equally likely) and exchange opinions with the eﬀect that all members of the group
will accept the strategy j of the member with the highest payoﬀ, so that Rj(x, b) =
maxl Ril(x, b), with some rates ΠI = Π(Ri1,
, Rik), which are symmetric functions of
their arguments that vanish whenever all Ril(x, b) are equal. If there are several members
of the group with the same payoﬀ, the choice can be ﬁxed arbitrary, say by choosing the
member with the highest index i. For simplicity (to shorten the formulas below) let us
assume that only the players from diﬀerent states can interact. Therefore, instead of a
Markov chain with generator (2), we obtain the chain with the generator

· · ·

Lb,N f (n) = Nκ

xilΠI[f (x + kej(I)/N

K

k

Yl=1
where I are now all possible subsets of

Xk=2 XI={i1,··· ,ik}

1,

{

· · ·

}

, d

of size k.

ei/N)

−

f (x)],

(52)

−

i∈I
X

Assuming again that limN→∞ b(x, N) = b(x) exists and that f

C 1(Σd), we ﬁnd now,

analogously to the calculations with (2) (that is by expanding f in Taylor series), that

∈

lim
N→∞, n/N→x

Lb,N f (n/N) = Λbf (x),

where

or equivalently

K

Λbf (x) = κ

Xk=2 XI={i1,··· ,ik}

∂f

∂xj(I) −

ΠI

k

"

∂f
∂xi #

k

(x)

xil,

Yl=1

i∈I
X

(53)

Λbf (x) =

K

d

κ

Xk=2

m=1
X

∂f
∂xm

(x)xm 
k


′

ΠmI

XI={i1,··· ,ik−1}

Yi∈I

xi −

ΠmI

XI={i1,··· ,ik−1}:m /∈I

Yi∈I

,

xi
(54)


21

where
Rm or Ril = Rm and il < m. The corresponding system of ODEs becomes

′ denotes the sum over subsets I = (i1 · · ·

ik−1) such that for each l either Ril <

P

K

′

κxm 
k


ΠmI

XI={i1···ik−1}

Yi∈I

xi −

ΠmI

XI={i1···ik−1}:m /∈I

Yi∈I

,

xi


(55)

˙xm =

Xk=2
, d.

with m = 1,

· · ·

The analog of Theorem 2.1 can now be easily given with the limiting deterministic

dynamics being (55).

It would be of course desirable to get some empirical data on the transition probabil-

ities for kth order interactions.

3

Introducing a forward-looking principal

3.1 Discrete time

Here we start exploiting another setting for the major player behavior. We shall assume
that changing strategies bears some costs, so that instantaneous adjustments of policies
become unfeasible and that the major player has some planning horizon with both running
and (in case of a ﬁnite horizon) terminal costs. For instance, running costs can reﬂect real
spending and terminal cost some global objective, like reducing the overall crime level by
a speciﬁed amount. This setting will lead us to the class of problem that can be called
Markov decision (or control) processes (for the principal) on the evolutionary background
(of permanently varying proﬁles of small players).

We shall conﬁne ourselves to the case of a ﬁnite-state-space of small players, so that
the state space of the group is given by vectors x = (n1,
+/N
(see Subsection 2.1). The extension to an arbitrary compact state-space is straightforward
via Theorem 2.4.

, nd)/N from the lattice Zd

· · ·

Starting with a discrete time case, we denote by XN (t, x, b) the Markov chain generated

by (2) with a ﬁxed b, that is by the operator

Lb,N f (x) = N

Xi,j:Rj(x,b)>Ri(x,b)
Zd

κxixj[Rj(x, b)

−

Ri(x, b)]

f

x

h

(cid:16)

ei
N

+

ej
N

−

−

(cid:17)

f (x)

, (56)

i

and starting in x
∈
updating her strategy in discrete times
n

..., n
N aiming at ﬁnding a strategy π maximizing the reward

+/N at the initial time t = 0. We assume that the principal is
1, with some ﬁxed τ > 0,

, k = 0, 1,

· · ·

kτ

−

{

}

∈

V π,N
n

(x(N)) = EN,x(N ) [τ B(x0, b0) +

· · ·

+ τ B(xn−1, bn−1) + V0(xn)] ,

(57)

where B and V0 are given functions (the running and the terminal payoﬀ), x0 = x(N)
Zd

+/N also given,

∈

xk = XN (τ, xk−1, bk−1),

k = 1, 2,

,

· · ·

and bk = bk(xk) are speciﬁed by the strategy π as some functions depending on the current
state x = xk (EN,x(N ) denotes the expectation speciﬁed by such process). By the basic dy-
namic programming (see again [45]) the maximal rewards V N
(x(N))

n (x(N)) = supπ V π,N

n

22

at diﬀerent times k are linked by the optimality equation V N
k−1, where the
Shapley operator S[N] (sometimes referred to as the Bellman operator) is deﬁned by the
equation

k = S[N]V N

S[N]V (x) = sup

[τ B(x, b) + EV (XN (τ, x, b))] ,

b
so that Vn can be obtained by the nth iteration of the Shapley operator:

n = S[N]V N
V N

n−1 = Sn[N]V0.

(58)

(59)

Alternatively, in the inﬁnite-horizon version, the principal can be interested in maxi-

mizing the discounted sum

V π,N (x(N)) = EN,x(N )

βkB(xk, bk),

(60)

∞

Xk=0
(0, 1), or any other criterion on the inﬁnite horizon path. Recall also that we

with a β
assume b to belong to a certain convex compact subset of a Euclidean space.

∈

We are again interested in the law of large numbers limit N

, where we expect

the limiting problem for the principal to be the maximization of the reward

→ ∞

or respectively

V π
n (x0) = τ B(x0, b0) +

· · ·

∞

+ τ B(xn−1, bn−1) + V0(xn),

in the discounted inﬁnite-horizon problem, where

Xk=0

V π(x) =

βkτ B(xk, bk)

(which is supposed to exist) and

x0 = lim
N→∞

x(N)

xk = X(τ, xk−1, bk−1),

k = 1, 2,

,

· · ·

(61)

(62)

(63)

(64)

with X(t, x, b) denoting the solution to the characteristic system (or kinetic equations)

˙xj =

κxixj[Rj(x, b)

i
X

Ri(x, b)],

j = 1, ..., d,

−

(65)

with the initial condition x at time t = 0. Again by dynamic programming, the max-
imal reward in this problem Vn(x) = supπ V π
n (x) is obtained by the iterations of the
corresponding Shapley operator, Vn = SnV0, with

SV (x) = sup

[τ B(x, b) + V (X(τ, x, b))] .

(66)

b

Especially for the application to the continuous time models it is important to have

estimates of convergence uniform in n = t/τ for bounded total time t = nτ .

23

Theorem 3.1. (i) Assume the functions Ri(x, b) and B(x, b) belong to CbLip(Σd) as the
functions of the ﬁrst variable with the norm uniformly bounded with respect to the second
variable. Assume also (63) holds. Then, for any Lipschitz function V0 on Σd, τ > 0 and
n

N,

∈

V N
n (x(N))

Vn(x)

C(t)(

x(N)

+ t2/3(n/N)1/3)

|

| ≤
where t = nτ is the total time. In particular, for n = N ω with ω
on the r.h.s. of (67) becomes of order N −(1−ω)/3.

−

−

|

∈

x
|

k

V0kbLip,
(0, 1), the last term

(67)

(ii) If there exists a Lipshitz continuous optimal policy π =

, n,
for the limiting optimization problem, then π is approximately optimal for the N-agent
problem, in the sense that for any ǫ > 0 there exists N0 such that, for all N > N0,

, k = 1,

bk(x)

· · ·

}

{

V N
n (x(N))

|

−

V N,π
n

(x(N))

ǫ.

| ≤

Proof. (i) Assume V0 is Lipschitz with some Lipschitz constant κ. This implies that all
functions Vk(x) are uniformly Lipschitz continuous. In fact,

SV (x1)

|

SV (x2)

sup
b

|

| ≤

−

τ B(x1, b) + V (X(τ, x1, b))

τ B(x2, b)

−

−

V (X(τ, x2, b))

|

κBτ

x1 −

|

x2|

≤

+ κeτ F

x1 −

|

,

x2|

where κB is the Lipschitz constant for B and F is the Lipschitz constant of the function
on the r.h.s. of (65) (as a functions of x). Thus the Lipschitz constant of Vk = SkV0 is
bounded by a constant C(t). Notice also that, since the function B is uniformly bounded,
all V N

k and Vk are uniformly bounded, say by some constant v.
Next we can write

Consequently,

Sn[N]V0 −

SnV0 =

n−1

j=0
X

Sj[N](S[N]

−

S)Sn−(j−1)[N]V0.

Sn[N]V0 −

SnV0k ≤

k

n sup

k=1,·,n k

(S[N]

S)SkV0k

.

−

Since the uniform estimate of the diﬀerence of two functions of b implies the same estimate
for the diﬀerence of the maxima, it follows from Theorem 2.1 that

Sn[N]V0 −

SnV0k ≤

k

nC(t)τ 2/3N −1/3

V0kbLip.

k

yielding (67).

(ii) One shows as above that for any Lipschitz continuous policy π, the corresponding

value functions V π,N converge. Combined with (i), this yields Statement (ii).

Remark 6. For a compact state space being a subset of Rn one would get for the last
term of the r.h.s. of (67) the decay estimate of order t1−1/(2n+3)(n/N)1/(2n+3).

Since the tails of series (62) and (60) tend to zero uniformly, the following fact is a

consequence of Theorem 3.1.

Theorem 3.2. Under the assumptions of Theorem 3.1 the discounted optimal rewards
(60) converge, as N

, to the discounted reward (62).

→ ∞

24

Analyzing long time behavior of the optimal dynamics given by Theorem 3.1 leads one
naturally to the analysis of the ﬁxed points of equation (65) and their turnpike properties.
Namely, let X[b] denote the set of ﬁxed points of (65) for given b. If

sup B(x, b) = max

b

max
x∈X[b]

B(X[b], b),

(68)

the points of maximum on the r. h. s. can be expected to serve as turnpikes (introduced
in economics by [32], see recent reviews e. g. in [88] and [60]) for long time behavior of
optimal problems arising from the limiting evolution of (65). How this fact is recast in
terms of the Markov decision process with N players is an interesting problem for what one
can characterize as the turnpike theory for Markov control on evolutionary background.
We shall not touch it here.

3.2 Continuous time

Here we initiate the analysis of the optimization problem for a forward-looking principal
in continuous time choosing the most transparent deterministic evolution of the principal.
Namely, let the eﬀorts (budget) b of the major player evolve according to the equation
˙b = u with control u from a compact convex set U
Rr. The state space of the group
being again given by vectors x = (n1,
+/N, the payoﬀ of the
major player will be given by

∈
, nd)/N from the lattice Zd

· · ·

T

t
Z

J(x(s), (b(s), u(s)) ds + ST (x(T ), (b(T ))

where J, ST are some continuous functions uniformly Lipschitz in all their variables. The
optimal payoﬀ of the major player is thus

SN (t, x(N), b) = sup
u(.)∈ ˜U

EN

x(N ),b

T

t
(cid:26)Z

J(x(s), (b(s), u(s)) ds + ST (x(T ), (b(T ))

,

(69)

(cid:27)

where EN
x,b is the expectation of the corresponding Markov process starting at the position
(x, b) at time t, and ˜U is the class of controls that are piecewise constant in t and Lipschitz
in x, b (so that the equations ˙b = u(x, b) are trivially well-posed). We are now in the
standard Markov decision setting of a controlled Markov process generated by the operator
Lb,N from (2), or more precisely

Lb,N SN (t, x, b) = N

κxixj

Xi,j:Rj(x,b)>Ri(x,b)

[Rj(x, b)

Ri(x, b)][SN (t, x

ei/N + ej/N, b)

SN (t, x, b)].

×
, the dimension of vectors x tends to inﬁnity making direct calculations

−

−

−

(70)

As N
→ ∞
complicated.

As seen from (11), the operators Lb,N tend to a simple ﬁrst order PDO, so that the
limiting optimization problem of the major player turns out to be the problem of ﬁnding

T

S(t, x, b) = sup

u(.)∈ ˜U (cid:26)Z

t

J(x(s), b(s), u(s))ds + ST (x(T ), (b(T ))

,

(71)

(cid:27)

25

where (x(s), (b(s)) (depending on u(.)) solve the system of equations ˙b = u and

˙xj =

κxixj[Rj(x, b)

i
X

Ri(x, b)],

j = 1, ..., d.

−

The well-posedness of this system is a straightforward extension of the well-posedness of
equations (6).

Instead of proving the convergence SN (t, x(N), b)

S(t, x, b), we shall concentrate
on a more practical issue comparing the corresponding discrete time approximations, as
these approximations are usually exploited for practical calculations of SN or S.

→

The discrete-time approximation to the limiting problem of ﬁnding (71) is the problem

of ﬁnding

Vt,n(x, b) = sup

π

V π
t,n(x, b) = sup
π

[τ J(x0, b0, u0) +

· · ·

+ τ J(xn−1, bn−1, un−1) + V0(xn, bn)] ,

where τ = (T

−

t)/n, (x0, b0) = (x, b), V0(x, b) = ST (x, b) and

bk = bk−1 + uk−1τ,

xk = X(τ, xk−1, bk−1),

k = 1, 2,

,

· · ·

(72)

(73)

with X(t, x, b) solving equation (65) with the initial condition x at time t = 0. The
discrete-time approximation to the initial optimization problem is the problem of ﬁnding

V N
t,n(x0, b0) = sup
π

V π,N
t,n (x0, b0)

= sup

π

EN,x(N ),b [τ J(x0, b0, u0) +

· · ·

+ τ J(xn−1, bn−1, un−1) + V0(xn, bn)] ,

(74)

where xk = XN (τ, xk−1, bk−1) with XN (t, x, b) denoting the Markov process with generator
(56). The strategies π here specify the choice of control parameters uk based on the
previous information.

Remark 7. It is well known that Vn(x, b) and V N
solutions S(T
t, b, x) and SN (T
Theorem 4.1 of [37] or Theorem 3.4 of [58].

−

−

n (x, b) with V0 = ST approach the optimal
t, x, b) given by (71) and (69) respectively, see e. g.

Theorem 3.3. Recall that J, ST are uniformly Lipschitz in all their variables. Then, for
any x and t

[0, T ]

∈

V N
t,n(x)

|

−

Vt,n(x)

| ≤

C(T )(T

−

t)2/3(n/N)1/3

V0kbLip.

k

(75)

Proof. This is a direct consequence of Theorem 3.1. The only diﬀerence is the use of
control parameter that is distinct from the state b, but this does not aﬀect the proof.

4 Models of growth under pressure

4.1 General convergence result for evolutions in l1

Here we extend the results of Subsection 3.1 in two directions, namely, by working with
a countable (rather than ﬁnite or compact) state-space and unbounded rates, and with
more general interactions allowing in particular for a change in the number of particles.

26

· · ·

Thus we take the set of natural numbers

as the state space of each small
player, the set of ﬁnite Borel measures on it being the Banach space l1 of sumable real
sequences x = (x1, x2,

· · · }

1, 2,

).

{

+ of sequences of integers n = (n1, n2,

Thus the state space of the total multitude of small players will be formed by the set
Zf in
) with only ﬁnite number of non-vanishing
· · ·
ones, with nk denoting the number of players in the state k, the total number of small
players being N =
k nk. As we are going to extend the analysis to processes not
preserving the number of particles, we shall work now with a more general scaling of the
states, namely with the sequences

P

x = (x1, x2,

· · ·

...) = hn = h(n1, n2,

...)

∈

· · ·

hZf in
+

with certain parameter h > 0, which can be taken, for instance, as the inverse number
k nk at the initial moment of observation. The necessity
to the total number of players
to distinguish initial moment is crucial here, as this number changes over time. Working
with the scaling related to the current number of particles N may lead, of course, to
diﬀerent evolutions.

P

The general processes of birth, death, mutations and binary interactions that can
+ speciﬁed by the

occur under an inﬂuence b of the principle are Markov chains on hZf in
generators of the following type

Lb,hF (x) =

1
h

j
X

βj(x, b)[F (x + hej)

F (x)] +

−

1
h

j
X

αj(x, b)[F (x

hej)

−

−

F (x)]

+

1
h

i,j
X

α1

ij(x, b)[F (x

hei+hej)

−

F (x)]+

−

1
h

Xi,(j1,j2)

α1

i(j1j2)(x, b)[F (x

hei+hej1+hej2)

−

F (x)]

−

α2

(i1i2)j(x, b)[F (x

hei1 −

−

hei2 + hej)

F (x)]

−

(i1i2)(j1j2)(x, b)[F (x

hei1 −

−

hei2 + hej1 + hej2)

−

F (x)],

(76)

where brackets (i, j) denote the pairs of states. Here the terms with βj and αj describe
the spontaneous injection (birth) and death of agents, the terms with α1 describe the
multiplication or mutations of single agents (including fragmentation and splitting), the
terms with α2 describe the binary interactions, with all terms including possible mean-ﬁeld
interactions. Say, our model (2) was an example of binary interaction.

as j

→ ∞

→ ∞

Let L be a positive increasing function on N such that L(j)

. We shall
refer to such functions as Lyapunov functions. Notations from Appendix B will be used
here for diﬀerent norms and notions related to a Lyapunov function L (see (127) and the
discussion around it). We say that the generator Lb,h with βj = 0 and the corresponding
process do not increase L if for any allowed transition the total value of L cannot increase,
that is if α1
= 0,
ij 6
≤
then L(j)
L(i1) + L(i2). If this
≤
is the case, then the chains generated by Lb,h always remain in a ball B+(L, R), if they
were started there. Hence for any h and R, Lb,h generates a well-deﬁned Markov chains
Xb,h(t, x) in any of the ﬁnite state-spaces hZf in
B+(L, R) (the corresponding Kolmogorov
Q-matrices are transpose to the matrices representing Lb,h).

= 0, then L(j)
L(i1) + L(i2), if α2

= 0, then L(j1) + L(j2)

= 0, then L(j1)+L(j2)

(i1i2)(j1j2) 6

L(i), if α1

L(i), if α2

i(j1,j2) 6

(i1i2)j 6

+ ∩

≤

≤

27

+

1
h

X(i1,i2),j
α2

X(i1,i2) X(j1,j2)

+

1
h

+

Xi,(j1,j2)

Moreover,

i
X

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
operator

A generator Lb,h is called L-subcritical if Lb,h(L)

0. Of course, if Lb,h does not
increase L, then it is L-subcritical. Though the condition to not increase L seems to be
restrictive, many concrete models satisfy it, for instance the celebrated merging-splitting
(Smoluchovskii) process considered below. On the other hand, models with spontaneous
injections may increase L, so that one is conﬁned to work with the weaker property of
sub-criticality.

≤

We shall denote by C(B+(L, R)

l1) the spaces of continuous
and diﬀerentiable functions on B+(L, R) with B+(L, R) considered as a subset of l1, that
is, equipped with the topology of l1, where these sets are easily seen to be compact.
Similar notations for Banach-space valued functions will be used.

l1) and C k(B+(L, R)

⊂

⊂

By Taylor-expanding F in (76) one sees that if F is suﬃciently smooth, the sequence

Lb,hF converges to

ΛbF (x) =

(βj(x, b)

α1

i(j1j2)(x, b)[

j
X
∂F
∂xj1

+

∂F
∂xj2 −

∂F
∂xi

+

α2

(i1i2)(j1j2)(x, b)[

X(i1,i2) X(j1,j2)

αj(x, b))

−

∂F
∂xi

+

i,j
X

α1

ij(x, b)[

∂F
∂xj −

∂F
∂xi

]

∂F
∂xj −

∂F
∂xi1 −

∂F
∂xi2

]

] +

α2

(i1i2)j(x, b)[

j
X(i1,i2) X
∂F
+
∂xj1

∂F
∂xj2 −

∂F
∂xi1 −

∂F
∂xi2

].

(Lb,h −
with κ(L, R) being the supb of the norms

kC(B+(L,R)⊂l1) ≤

Λb)F

k

8hκ(L, R)

F

kC2(B+(L,R)⊂l1),

k

(αi + βi) +

α1

ij +

α1

i(j1j2) +

α2

(i1i2)j +

i,j
X

Xi,(j1,j2)

X(i1,i2),j

X(i1,i2),(j1,j2)

α2

By regrouping the terms of Λb, it can be rewritten in the form of the general ﬁrst order

(i1i2)(j1j2)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

C(B+(L,R)⊂l1)

where

fi = βi −

αi +

Xk
+

ΛbF (x) =

fj(x)

j
X

∂F
∂xj

,

(α1

ki −

α1

ik) +

[α1

k(ii) +

Xk

Xj6=i

(α1

k(ij) + α1

k(ji)]

(79)

α1

i(j1j2)

−

X(j1,j2)

α2

(j1j2)i −

Xk
(j1j2)(ii) +

[α2

[α2

(ii)k +

Xj6=i

(α2

(ij)k + α2

(ji)k)]

(α2

(j1j2)(ji) + α2

(j1j2)(ij))]

X(j1,j2)
+

X(j1,j2)

Xj6=i

[α2

(ii)(j1j2) +

−

X(j1,j2)

Xj6=i

(α2

(ij)(j1j2) + α2

(ji)(j1j2))].

Its characteristics solving the ODE ˙x = f (x) can be expected to describe the limiting

behavior of the Markov chains Xb,h(x, t) for h

0.

→

28

(77)

(78)

.

1
N 1/3 k
t4/5
N 1/5 k

F

Theorem 4.1. Assume the operators Lb,h are L non-increasing for a Lyapunov function
l1 is uniformly Lipschitz on
L on Z such that L(j)
B+(R, L) and κ(Λ, R) <
B+(R, L)
converge in distribution to the deterministic evolution X(t, x) solving equation ˙x = f (x)
and moreover

as j
. Then the Markov chains Xh(t, x(h)) with x(h)

, the function f : l1

→ ∞
∞

→ ∞

+ →

∈

EF (Xh(t, x(h)))

|

−

F (X(t, x(h)))

| ≤

tC(R, t)

F

kC2(B+(L,R)⊂l1)

(80)

EF (Xh(t, x(h)))

F (X(t, x(h)))

C(R, t)

|

| ≤
with constants C(R, t). If f is uniformly twice continuously diﬀerentiable, then the same
estimates hold with the improved rates 1/N and 1/N 1/3 respectively.

−

kCbLip(B+(L,R)⊂l1)

(81)

Proof. The proof is similar to the proof of Theorem 2.4, though the lack of compact-
ness is dealt with by L-subcritical condition that again allows one to use eﬀective ﬁnite-
dimensional approximations. Moreover, discrete setting allows one not to bother about
weak topology.

If f is smooth and

f

k

kC2(B+(L,R)⊂l1;l1) ≤

D(R),

then the solutions X(t, x) to the equation ˙x = f (x) are twice diﬀerentiable with respect to
initial data and the corresponding mapping U t : F (x)
F (X(t, x)) are twice continuously
diﬀerentiable by Lemma (5.1). Hence the estimate

7→

EF (Xh(t, x(h)))

|

−

F (X(t, x(h)))

tC(R, t)

| ≤

1
N k

F

kC2(B+(L,R)⊂l1),

(82)

claimed by the last statement of the Theorem, follows directly by (102) and (78).

If f is only Lipschitz continuous we again use a ﬁnite-dimensional approximation
j is just the projection on the ﬁrst j coordinates,

j (x)), where now P ∗

F (x)
that is [P ∗

→

Fj(x) = F (P ∗
j (x)]k = xk for k

≤

j and [P ∗

j (x)]k = 0 otherwise. For x
R
L(j)

x
kl1

−

≤

,

P ∗

j (x)

k

B+(L, R),

∈

and hence one can further use the smooth approximation Φδ(F
as in Theorem 2.4. The dimension of the image of P ∗
apply with n = 1 yielding (80) and (81).

j ) with the same eﬀect
j is j, so the results of Theorem 2.4

◦

P ∗

−

· · ·

..., n

Assume now that the principal is updating her strategy in discrete times

, k =
N aiming at ﬁnding a strategy π maximizing
0, 1,
1, with some ﬁxed τ > 0, n
hZf in
the reward (57), but now with x0 = x(h)
B+(L, R). Using Theorem 4.1, It is
straightforward to extend Theorem 3.1 to the present setting of a countable state-space.
Using the same notations as in Theorem 3.1 for rewards and Shapley operators yields the
following result.

∈
∈

kτ

∩

}

{

Theorem 4.2. Assume the conditions of Theorem 4.1 hold and the function B(x, b)
is uniformly Lipschitz on B+(R, L) as a function of the ﬁrst variable. Then, for any
N,
continuous V0 on B+(R, L), τ > 0 and n

V N
n (x(N))

|

Vn(x)

−

| ≤

C(t)(

|

x
|

−

+ t4/5(n/N)1/5)

V0kbLip,

k

(83)

where t = nτ is the total time. In case when f and B are twice continuously diﬀerrentiable,
the rates of convergence improve to N −1/3.

29

∈
x(N)

4.2 Evolutionary coalition building under pressure

As a direct application of Theorem 4.2, let us discuss the model of evolutionary coalition
building. Namely, so far we talked about small players that occasionally and randomly
exchange information in small groups (mostly in randomly formed pairs) resulting in copy-
ing the most successful strategy by the members of the group. Another natural reaction
of the society of small players to the pressure exerted by the principal can be executed
by forming stable groups that can confront this pressure in a more eﬀective manner (but
possibly imposing certain obligatory regulations for the members of the group). Analy-
sis of such possibility leads one naturally to models of mean-ﬁeld-enhanced coagulation
processes under external pressure. Coagulation-fragmentation processes are well studied
in statistical physics, see e. g. [73]. In particular, general mass-exchange processes, that
in our social environment become general coalition forming processes preserving the to-
tal number of participants, were analyzed in [51] and [53] with their law of large number
limits for discrete and general state spaces. Here we add to this analysis a strategic frame-
work for a major player ﬁtting the model to the more general framework of the previous
section. Instead of coagulation and fragmentation we shall use here the terms merging
and splitting or breakage.

+ of sequences of integers n = (n1, n2,

For simplicity, we ignore here any other behavioral distinctions (assuming no strategic
space for an individual player) concentrating only on the process of forming coalitions.
Thus the state space of the total multitude of small players will be formed by the set
Zf in
...) with only ﬁnite number of non-vanishing
ones, with nk denoting the number of coalition of size k, the total number of small players
being N =
k knk and the total number of coalitions (a single player is considered to
k nk. Also for simplicity we reduce attention to
represent a coalition of size 1) being
binary merging and breakage only, extension to arbitrary regrouping processes from [51]
(preserving the number of players) is more-or-less straightforward.

· · ·

P

P

As previously, we will look for the evolution of appropriately scaled states, namely the

sequences

x = (x1, x2,

· · ·

...) = hn = h(n1, n2,

...)

∈

· · ·

hZf in
+

with certain parameter h > 0, which can be taken, for instance, as the inverse number to
the total number of coalitions

k nk at the initial moment of observation.

P

If any randomly chosen pair of coalitions of sizes j and k can merge with the rates
Ckj(x, b), which may depend on the whole composition x and the control parameter b of
the major player, and any randomly chosen coalition of size j can split (break, fragment)
into two groups of sizes k < j and j
k with rate Fjk(x, b), the limiting deterministic
evolution of the state is known to be described by the system of the so-called Smoluchovski
equations

−

˙xk = fk(x) =

Cj,k−j(x, b)xjxk−j−
2

Ckj(x, b)xjxk+2

Xj<k

j
X

Fjk(x, b)xj−

Fkj(x, b)xk.

Xj>k

Xj<k

(84)
In addition to the well known setting with constant Cjk and Fjk (see e. g.
[11]) we
added here the mean ﬁeld dependence of these coeﬃcients (dependence on x) and the
dependence on the control parameter b.

30

As one easily checks, equations (84) can be written in the equivalent weak form

d
dt

gjxj =

j
X

Xj,k

(gj+k −

gj −

gk)Cjk(x, b)xjxk +

(gj−k + gk −

gj)Fjk(x, b)xj, (85)

j
X

Xk<j

which should hold for a suitable class of test functions g. For instance, under the assump-
tion of bounded coeﬃcients (see (90) below), the class of test functions is the class of all
functions from l∞ =
. This implies, in particular, that the correspond-
ing semigroups (104) on the space of continuous functions, that is U tG(x) = G(X(t, x)),
have the generator

g : supj |

gj|

∞}

<

{

Xk

+

ΛbG(x) =

fk(x)

∂G
∂xk

(x) =

∂G

∂xk+j −

∂G
∂xj −

∂G
∂xk (cid:19)

Xj,k (cid:18)

Cjk(x, b)xjxk

∂G

∂xj−k −

∂G
∂xj

+

∂G
∂xk (cid:19)

j
X

Xk<j (cid:18)

Fjk(x, b)xj

(86)

of type (77).

Let Rj(x, b) be the payoﬀ for the member of a coalition of size j.

In our strategic
setting, the rates Cjk(x, b) and Fjk(x, b) should depend on the diﬀerences of these rewards
before and after merging or splitting. For instance, the simplest choices can be

Ckj(x, b) = aj+k,k1Rk+j≥Rk(Rk+j −

Rk) + aj+k,j1Rk+j ≥Rj (Rk+j −
with some constants alk ≥
0 reﬂecting the assumption that merging may occur whenever it
is beneﬁcial for all members concerned but weighted according to the size of the coalitions
involved, where by 1M here and in what follows we denote the indicator function of the
set M. Similarly

Rj),

(87)

Fkj(x, b) = ˜akj1Rj ≥Rk(Rj −

Rk) + ˜ak,k−j1Rk−j ≥Rk(Rk−j −

Rk).

(88)

A Markov approximation to dynamics (84) is constructed in the standard way, which
is analogous to the constructions of approximating Markov chains described in the previ-
ous section (for coagulation - fragmentation processes this Markov approximation is often
referred to as the Markus-Lushnikov process, see e.g.
[73]), namely, by attaching expo-
nential clocks to any pair of coalitions that can merge with rates Ckj and to any coalition
that can split with rates Fkj. This leads to a Markov chain Xh(t, x, b) on hZf in
+ with the
generator

Λb,hG(x) =

Cij(x, b)xixj[G(x

hei −

−

hej + hei+j)

G(x)]

−

i,j
X

+

Fij(x, b)xi[G(x

i
X

j<i
X

hei + hej + hei+j)

G(x)],

−

−

(89)

· · ·

denote the standard basis in R∞. There exists an extensive literature
where e1, e2,
showing the well -posedness of inﬁnite-dimensional dynamics (84) and proving the con-
vergence, as h
0, of Markov chains generated by (89) under various assumptions on
the coeﬃcients C and F (see e. g.
[73] and [53] and references therein). However, to

→

31

deal with a forward -looking principal, some uniform rates of convergence are needed, like
those of Theorem 4.1.

We shall propose here only the simplest result in this direction assuming that the
intensities of individual transition are uniformly bounded and uniformly Lipschitz, that
is

C = sup
j,k

Cjk(x, b) <

∞

, F = sup

j

Xk<j

Fkj(x, b) <

,

∞

C(1) = sup

b,j,k k

Cjk(., b)

F (1) = sup
b,j

k

Xk<j

Fkj(., b)

,

kCbLip(B+(R,L)⊂l1) <

∞
kCbLip(B+(R,L)⊂l1) <

.

∞

(90)

(91)

Notice however that the overall intensities are still unbounded (quadratic), so that we are
still quite away from the assumptions of Section 3.

Choosing the function L(j) = j we see that Markov chains Xh(t, x, b) do not increase

L. Moreover, (90) implies

f (., b)

f (., b)

sup

b k

sup

b k

kC(B+(R,L)⊂l1);l1
≤
kCbLip(B+(R,L)⊂l1);l1

3CR2 + 3F R,

6CR + 3F + 3(C(1)R + F (1))R

≤

(92)

and hence the following result.

Theorem 4.3. For a model of strategically enhanced coalition building subject to (90)
and (91) the conditions of Theorem 4.1 and consequently its assertions are satisﬁed.

4.3 Strategically enhanced preferential attachment on evolu-

tionary background

A natural and useful extension of the theory presented above can be obtained by the in-
clusion in our pressure-resistance evolutionary-type game the well known model of linear
growth with preferential attachment (Yule, Simon and others, see [84] for review) turn-
ing the latter into a strategically enhanced preferential attachment model that includes
evolutionary-type interactions between agents and a major player having tools to control
(interfere into) this interaction. Since the proper exposition of the corresponding rigor-
ous convergence result requires an extension of Theorem 4.2 to L-subcritical (rather than
L-non-increasing) processes, we shall not present it here, but only indicate the expected
outcomes leaving details to another publication.

We shall work with the general framework of Theorem 4.2, having in mind that the
basic examples of the approximating Markov chains Xh(t, x(h)) can arise from the merging
and splitting coalition model of the previous section (with generator (89)) or from setting
(56), where now the number of possible states j becomes inﬁnite and hence, assuming for
simplicity that the agents are identical so that the parameter j denotes the size of the
coalition, generator (56) becomes

Lb,hG(x) =

1
h

Xi,j:Rj(x,b)>Ri(x,b)

κxixj[Rj(x, b)

Ri(x, b)][G (x

hei + hej)

−

−

−

G(x)],

(93)

32

where Rj(x, b) is the payoﬀ to a member of a coalition of size j = 1, 2,
. The Markov
chain with generator (93) describes the process where agents can move from one coalition
to another choosing the size of the coalition that is more beneﬁcial under the control
b of the principal. Of course one can work also with various combinations of genera-
tors (93) and Λb,h from (89), as well as with their various extensions including, say, kth
order interactions, see (52), or various classes (for instance, levels of activity) of agents,
where coalitions get another interpretation as groups of agents following certain particular
strategy.

· · ·

The most studied form of preferential attachment evolves by the discrete time injec-
tions of agents (see [13], [31], [84] and references therein). Along these lines, we can
assume that with time intervals τ a new agent enters the system in such a way that with
some probability α(x, b) (which, unlike the standard model, can now depend on the dis-
tribution x and the control parameter b of the principal) she does not enter any of the
α(x, b)
existing coalitions (thus forming a new coalition of size 1), and with probability 1
she joins one of the coalitions, the probability to join a coalition being proportional to its
size (this reﬂects the notion of preferential attachment coined in [13]). Thus if V (x) is
some function on the state space hZf in
+ , its expected value after a single entry changing
x to ˆx is descried by the following operator Th:

−

ThV (x) = EV (ˆx) = αV (x + he1) + (1

α)

−

∞

Xk=1

knk
L(n)

V (x

−

hek + hek+1),

(94)

where L(n) =

knk, x = nh.

P

A continuous time version of these evolutions can be modeled by a Markov process,
where the injection occurs with some intensity λ(x, b) (that can be inﬂuenced by the
principal subject to certain costs).
In other words, it can be included by adding to
generator (93) or (89) the additional term of the type

Λatt

b,hG(x) =

αλ(b, x)
h

[G(x+he1)

−

G(x)]+

(1

−

h

Xk=1
The limiting evolution will then be given by the equation

α)λ(b, x)

∞

kxk[G(x

−

hek +hek+1)

G(x)].

−

˙x = f (x) + αλ(b, x)

∂G
∂x1

+ (1

−

∞

α)λ(b, x)

kxk

Xk=1

∂G

∂xk+1 −

(cid:20)

∂G
∂xk (cid:21)

,

(95)

where f (x) is obtained from the limit of (93) or (89). A strategically enhanced preferential
attachment model on the evolutionary background will thus be described, in the dynamic
law of large number limit, by the controlled inﬁnite-dimensional ODEs (95) (via discrete
or continuous-time choice of parameter b by the principal).

As we mentioned, a rigorous proof of the convergence is beyond the scope of this paper.
Apart from sorting out this problem, an important issue is to understand the controlla-
bility of the limiting (now in the sense t
) stationary solutions, which may lead to
→ ∞
the possibility to develop tools for inﬂuencing the power tails of distributions (Zipf’s law)
appearing in many situations of practical interest, as well as the proliferation or extinction
of certain desirable (or undesirable) characteristics of the processes of evolution.

33

5 Appendix

5.1 Notations for functional spaces and measures

Notations introduced here are used in the main text systematically without further re-
minder.

For a metric space Z with a metric ρ, let C(Z) denote the space of bounded contin-
, CbLip(Z) the subspace of
f

f (x)

uous functions equipped with the sup-norm:
bounded Lipschitz functions with the norm

= supx |

k

k

|

f

kbLip =

k

f

k

k

+

f

kLip,

k

f

kLip = sup

x6=y

k

|

f (x)

f (y)

−
ρ(x, y)

|

.

(96)

We may write shortly C k or CbLip if it is clear which Z we are working with.

Since we often interpret our vectors as measures, for Euclidean space Z, it is convenient
Z, so that for functions on Rn we deﬁne

for vectors x

to use the l1-norm

x
|1 =

|

xj|

j |

∈

P
kLip = sup

x6=y

f

k

|

f (x)
x

|

f (y)
y

|1

−
−

|

= sup

sup |

j

f (x)

−
xj −

|

|

,

f (y)
yj|

(97)

where the last sup is the supremum over the pairs x, y that diﬀer only in its jth coordinate.
For Z a closed convex subset of Rn, let C k(Z) denote the space of k times continuously
diﬀerentiable functions on Z with uniformly bounded derivatives equipped with the norm

f

kCk(Z) =

k

f

k

k

+

k

j=1
X

f (j)

,

k

k

f (j)

where
In particular, for a diﬀerentiable function,

k

k

is the supremum of the magnitudes of all partial derivatives of f of order j.

For Z a closed convex subset of a Banach space B, the directional derivative of a real

f

k

f

k

kbLip.
kC1 =
x is deﬁned as

function F on Z at x in the direction ξ

Z

∈

−

DξF (x) = DF (x)[ξ] = lim
h→0+

F (x + hξ)
h

−

F (x)

,

(98)

and higher order derivatives are deﬁned recursively, for instance the second derivative is

D2F (x)[ξ, η] = D (DF (x)[ξ]) [η],

ξ, η

Z

∈

−

x.

The spaces C k(Z), k

N of continuously diﬀerentiable functions are the subsets of
functions from C(Z) with the derivatives of order up to k well deﬁned and continuous
with respect to all their variables and having ﬁnite norms

∈

F

kCk(Z) =

k

F

k

k

+

k

Xl=1

sup
x∈Z

sup
ξj:kξj k=1 |

DlF (x)[ξ1,

,

, ξl]
|

· · ·

Similarly the diﬀerentiability of the Banach-space-valued functionals F : Z

B1 and
the corresponding spaces C(Z; B1), C k(Z; B1) are deﬁned for any other Banach space B1.

→

34

For instance, if B = l1, then

F

kC1(Z) =

k

F

+ sup
x∈Z

k

k

and

F

kC2(Z) =

k

F

k

kC1(Z) + sup

x∈Z

(99)

(100)

,

∂F
∂xk (cid:12)
(cid:12)
(cid:12)
(cid:12)
∂2F
∂xk∂xl (cid:12)
(cid:12)
(Z) (resp.
(cid:12)
(cid:12)

sup
k (cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
k,l (cid:12)
(cid:12)
(cid:12)
(cid:12)
M
Mλ(Z)

∩ M

.

For a locally compact metric space Z we denote by

+(Z)) the Banach
space of signed ﬁnite Borel measures on Z (resp. its subset of non-negative measures), by
+
+(Z). According to the
λ (Z) =
Mλ(Z) the ball of radius λ there, with
(Z) is the Banach dual to the space C∞(Z),
Riesz-Markov Theorem, the Banach space
which is the subspace of functions from C(Z) vanishing at inﬁnity.

M

M

M

product notations (f, µ) =

For a function f on Z and a measure µ (not necessarily bounded) we use the scalar-
f (z)µ(dz) for the natural pairing, whenever it is well deﬁned.
+(Z) can be metri-

By the celebrated Kantorovich theorem, the weak topology on

cized via the duality relation with the space CbLip(Z), that is, via the metric

M

R

dbLip∗(µ, ν) =

µ

k

−

ν

kbLip∗ = sup

f :kf kbLip≤1 ZZ

f (z)(µ

−

ν)(dz).

For a closed convex subset S of

+(Z) we shall denote by Cweak(S) the closed subset
M
of C(S) consisting of weakly continuous functions. We shall denote by C bLip
weak(S) the space
of weakly Lipschitz functions F on S (which are Lipschitz with respect to dbLip∗). We
+
shall denote

kweakLip the corresponding Lipschitz constant and

kweakbLip =

F

F

k

k

k

k

F
kweakLip the norm in C bLip

F

weak(S).

φ(z)µ(dz) on

k
Remark 8. Linguistically counterintuitive, the weak continuity is a stronger requirement
than just continuity. For any bounded measurable φ, the linear functional F (µ) = (φ, µ) =
(Z) is continuous and continuously diﬀerentiable of all orders in the
norm topology with DF (x)[ξ] = (φ, ξ), D2F (x) = 0. On the other hand, this F (µ) is
R
weakly continuous only if φ is continuous and weakly-
0
for z
CbLip(Z). Only for discrete countable Z, the
. It is weakly Lipschitz, if φ
(Z) = l1 are continuous in the norm if and only if they
linear functionals on the space
are weakly continuous. This often allows one to avoid using weak topology for l1.

continuous if additionally φ(z)

→ ∞

M

M

→

∈

∗

We recall for reference the following simple and standard general formula for the com-
parison of arbitrary operator semigroups UN and U with generators LN and L respectively:

U T −t
N g

−

U T −t = U s−t

N U T −s

T
s=t =

|

T

t
Z

U s−t
N (LN −

L)U T −s ds.

When UN as a contraction in a space of bounded functions, it implies

U T −t
N g

k

−

U T −tg

(T

t) sup

s∈[t,T ] k

(LN −

−

k ≤

L)U T −sg

.

k

(101)

(102)

35

(103)

(104)

5.2 Sensitivity of ODEs in Banach spaces

Here we put together, in a concise way, certain basic facts on the sensitivity of ODEs in
Banach space with an unbounded (in particular quadratic) r.h.s., the main example of
interest for us being the Banach space l1 and the evolutions satisfying (130).

Let B be a Banach space equipped with the norm
We shall write shortly
for
k
of radius R in B centered at the origin and B+(R) = B+ ∩
A : B

kB and B+ its certain convex cone.
.
kB when no confusion arises. Let B(R) denote the ball
.
B(R). For a linear operator

B we denote by

.
k

k

k

A
kB→B its operator norm.

k

Let us consider an ordinary diﬀerential equation (ODE) ˙x = f (x) in B with a locally
B+(R) the global solution

Lipschitz, but generally unbounded f such that for any x
X(t, x) is uniquely deﬁned with

∈

→

∈
for some constants a, b. Lemma 5.5 below motivates the use of condition (103).

k

X(t, x)

B+(eat(

+ bt))

x0k

Under (103), the linear operators U t:

U tF (x) = F (X(t, x)),

t

0,

≥

are well deﬁned contractions in C(B+) forming a semigroup.
operators U t form a semigroup of contractions also in C(B+(R)) for any R.
Lemma 5.1. Under (103) assume additionally that f is twice continuously diﬀerentiable
as a mapping on B+ such that for any R and all x

In case a = b = 0, the

B+(R),

f

k

kC1(B+(R);B) ≤

D1(R),

f

k

kC2(B+(R);B) ≤

D2(R),

(105)

with some continuous functions D1(R), D2(R). Then the solutions to ˙x = f (x) are twice
continuously diﬀerentiable with respect to initial data and

∈

X(t, .)

k

Moreover,

X(t, .)

k
kC2(B+(R);B) ≤

kC1(B+(R);B) ≤

exp
tD2(eat(R + bt)) exp

(cid:8)

tD1(eat(R + bt))

,

3tD1(eat(R + bt))
(cid:9)

.

(106)

k
kC2(B+(R)) ≤

U tF

k

U tF

exp
kC1(B+(R)) ≤
(1 + tD2(eat(R + bt))) exp

(cid:8)

tD1(eat(R + bt))

(cid:8)

F
k
3tD1(eat(R + bt))

(cid:9)

(cid:9)
kC1(B+(eat(R+bt))),
)
k

F

(107)
kC2(B+(eat(R+bt))).
(108)

(cid:8)

(cid:9)

Proof. Diﬀerentiating the equation ˙x = f (x) with respect to initial conditions yields

d
dt

DX(t, x)[ξ] = Df (X(t, x))[DX(t, x)[ξ]] = Df (X(t, x))

DX(t, x)[ξ]

(109)

◦

d
dt

D2X(t, x)[ξ, η] = D2f (X(t, x))[DX(t, x)[ξ], DX(t, x)[η]]+Df (X(t, x))[D2X(t, x)[ξ, η]].
(110)
Since the initial conditions to these equations are DX(0, x)[ξ] = ξ, D2X(0, x)[ξ, η] = 0,
one deduces (106) from (105).
Diﬀerentiating (104) yields

D(U tF )(x)[ξ] = DF (X(t, x))[DX(t, x)[ξ]],
(111)
D2(U tF )(x)[ξ, η] = DF (X(t, x))[DX(t, x)[ξ], DX(t, x)[η]] + DF (X(t, x))[D2X(t, x)[ξ, η]]
(112)

implying (107) and (108).

36

5.3 Variational derivatives

We recall here some facts about variational derivatives of the functionals on measures. As
a consequence, we deduce the asymptotic formula for the generator of our basic model.

For a function F on a convex closed subset S of

(Z) with a locally compact metric
δY (x) is deﬁned as the directional derivative of F (Y )

M

space Z the variational derivative δF (Y )
in the direction δx:

δF (Y )
δY (x)

= DδxF (Y ) = lim
s→0+

1
s

(F (Y + sδx)

F (Y )).

−

(113)

The higher derivatives δlF (Y )/δY (x1)...δY (xl) are deﬁned inductively.

As it follows from the deﬁnition, if δF (Y )/δY (.) exists for x

Z and depends contin-
uously on Y (in weak or norm topology), then the function F (Y + sδx) of s
R+ has a
continuous right derivative everywhere and hence is continuously diﬀerentiable implying

∈

∈

F (Y + δx)

F (Y ) =

−

0
Z

1

δF (Y + sδx)
δY (x)

ds.

(114)

weak(S), k = 1, 2, . . . , if δlF (Y )/δY (x1) . . . δY (xl)
We shall say that F belongs to C k
S, and represents a continuous
exists for all l = 1, ..., k, all x1, . . . , xk ∈
mapping of k + 1 variables (when measures equipped with the weak topology) uniformly
bounded on the sets of bounded Y . When deﬁned on a bounded set S, these spaces
become Banach when equipped with the norm

Z k and Y

∈

δkF (Y )

k

F

kCk

weak(S) = sup
x1,··· ,xk

sup
Y ∈S (cid:12)
(cid:12)
Remark 9. Again counterintuitive, the weak diﬀerentiability does not imply the weak
(cid:12)
(cid:12)
Lipschitz continuity. For φ
φ(z)µ(dz)
(Z) is weakly continuously diﬀerentiable of all orders, but it is weakly Lipschitz only
on
if φ is Lipschitz, with

C(Rn), the linear functional F (µ) = (φ, µ) =

δY (xk)

δY (x1)

M

· · ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)

F

φ

R

.

∈
kweakLip =

k

kLip.

k

The following facts are basic formulas of the calculus for functionals on measures.

They are easy to deduce (the details are given in [54]).
Lemma 5.2. (i) One has the inclusion C 1

weak(S)

C 1(S) and

DξF (Y ) =

Z

⊂
δF (Y )
δY (x)

1

ξ(dx)

δF (Y + sξ)
δY (.)

, ξ

ds

(cid:19)

0 (cid:18)

Z

C 2(S) and

F (Y + ξ)

F (Y ) =

−

for F

C 1

S
weak(S) and Y
∈
−
(ii) One has the inclusion C 2
weak(S)

S, ξ

∈

∈

Y .

⊂

F (Y + ξ)

−

F (Y ) = (

δF (Y )
δY (.)

, ξ) +

0
Z

Y .

S, ξ

S

∈

−

∈

for F

C 2
∈
(iii) If t
C 1

weak(S)

weak(S) and Y
µt ∈

7→

F

∈

1

(1

s)

−

(cid:18)

δ2F (Y + sξ)
δY (.)δY (.)

, ξ

⊗

ξ

ds,

(117)

(cid:19)

S is continuously diﬀerentiable in the weak topology, then for any

d
dt

F (µt) = (δF (µt;

), ˙µt).

·

37

(118)

(115)

(116)

Though the variational derivatives are well deﬁned for the general space C 1(S) of
strongly diﬀerentiable functions, they may not be continuous and hence are not very
C 1(S) and
handy to work with. The analogs of equations (116) and (117) for F
F

C 2(S) respectively are the formulas

∈

∈

F (Y + ξ)

−

1

F (Y ) =

DF (Y + sξ)[ξ] ds,

(119)

0
Z

F (Y + ξ)

−

F (Y ) = DξF (Y ) +

1

(1

−

0
Z

s)D2F (Y + sξ)[ξ, ξ] ds,

(120)

These rules extend to measure-valued functions on
+(Z)

(Z). Namely, a mapping Φ :
(Z ′) with another set Z ′ has a weak variational derivative δΦ/δY (x), if for

M

valid for ξ

S

∈

−

x.

M
any Y

7→ M

+(Z), x

∈ M

Z the limit

∈

δΦ
δY (x)

= lim
s→0+

1
s

(Φ(Y + sδx)

Φ(Y ))

−

(Z ′) and is a ﬁnite signed measure on Z ′. Higher
exists in the weak topology of
M
derivative are deﬁned inductively. We shall say that Φ belongs to C l
(Z ′)),
l = 1, 2, . . . , if the weak variational derivatives δkΦ(Y ; x1, . . . , xk) exist for all k = 1, ..., l,
(Z), and represent continuous in the sense of the weak
all x1, . . . , xk ∈
(Z ′), which is bounded on the bounded subsets of
topology mapping
Y .

Z k and Y
(Z)

∈ M
Z k

7→ M

weak(

(Z);

M

M

M

×

Remark 10. Unlike real functions, the inclusion C l
does not hold anymore. For instance, if Z is a one-point set, we have the opposite inclu-
sion C l(R;

weak(

(Z)).

(Z))

(Z))

(Z);

(Z);

C l(

M

M

M

M

C l

⊂

(Z))

weak(R;

M

⊂

M

The following chain rule is straightforward (details of the proof see e. g. [54]).

Lemma 5.3. (i) Let Φ
sition F

∈
Φ(Y ) = F (Φ(Y )) belongs to C 1

weak(

(Z);

M

C 1

(Z)) and F

M
weak(

∈
(Z)) and

C 1

weak(

M

(Z)), then the compo-

◦

M

δF
δY (x)

(Φ(Y )) =

ZZ

δF (W )
δW (y) |W =Φ(Y )

δΦ
δY (x)

(Y, dy).

(121)

(Z);
(ii) Similarly, if Φ
Φ(Y ) = F (Φ(Y )) belongs to C 1(

M

∈

C 1(

(Z)) and F
(Z)) and

C 1(

M

∈

(Z)), then the composition

M
M

F

◦

Dξ(F

◦

Φ)(Y ) = DF (Φ(Y ))[DξΦ(Y )],

(122)

for any ξ. This turns to (121) for ξ = δx.

The following technical result is the key ingredient in the proof of Theorem 2.4.

Lemma 5.4. Let a measurable function R(b, µ) on R
diﬀerent points z1, z2 of Z and a measure µ
’large’ and s for ’small’) denote the same pair, but ordered in such a way that R(zl, µ)
R(zs, µ) (if the values are equal, the choice of ordering is irrelevant). Let

(Z) be given. For a pair of
(Z), let zl(µ), zs(µ) (with l standing for

× M

∈ M

≥

LN f (δx/N) =

κ

N

X(i,j)

[R(xj, δx/N)

−

R(xi, δx/N)][f (δx/N

δxi/N + δxj /N)

f (x)] (123)

−

−

38

· · ·

where x = (x1,
, xN ) and the sum is over all pairs (i, j) of distinct indices ordered in
such a way that R(xj, δx/N) > R(xi, δx/N) (the order is irrelevant if the corresponding
values of R coincide).
C 2

Then, for f

(Z)),

weak(

M

∈

LN f (µ) = κ

δf (µ)
δµ(z2)

[R(z2, µ)

−

ZZ ZZ

R(z1, µ)]µ(dz1)µ(dz2)

1

(1

0
Z

κ

+

2N
δ2f

×

δµ(z2)δµ(z2) −

(cid:18)
with µ = δx/N.

−

2

ZK ZK
δ2f
δµ(z2)δµ(z1)

s)

µ(dz1)µ(dz2) ds[R(zl(µ), µ)

R(zs(µ), µ)]

−

+

δ2f
δµ(z1)δµ(z1)

s
N

µ +

(cid:19) (cid:16)

(δzl(µ) −

δzs(µ))

(cid:17)

(124)

Proof. Applying (117) one gets

LN f (µ) =

κ

N

[R(xj, µ)

R(xi, µ)]

−

Xi,j:R(xj,µ)>R(xi,µ)

δxi

,

δxj −
N

1

(1

+

(cid:19)

0
Z

s)

−

(cid:18)

δ2f (µ + (δxj −
δµ(.)δµ(.)

δxi)/N)

,

δxi)⊗2

(δxj −
N 2

ds

,

(cid:19)

(cid:21)

δf (µ)
δµ(.)

×

(cid:20)(cid:18)
or equivalently

LN f (µ) =

+

κ

N 2

κ

N 3

1

(1

0

XI={i,j} Z
δ2f
δµ(xi)δµ(xj)

δ2f

×

(cid:18)

δµ(xi)δµ(xi) −

2

[R(xj, µ)

XI={i,j}

R(xi, µ)]

−

(cid:18)

δf (µ)
δµ(xj) −

δf (µ)
δµ(xi)

(cid:19)

s)ds[R(xl, µ)

R(xs, µ)]

−

−

+

δ2f
δµ(xj)δµ(xj)

µ +

s
N

(δxl(µ) −
, N

· · ·

}

(cid:19) (cid:16)
1,
{

δxs(µ))

,

(cid:17)

. It is seen directly

where the summation is over the two-point subsets I of
that this rewrites as (124).

5.4 On measure-valued ODEs with the Lyapunov condition

Let Z be a locally compact space. Here we recall the basic facts on the growth of positivity
(Z) with an unbounded r.h.s.
preserving ordinary diﬀerential equations (ODEs) in
satisfying the Lyapunov condition.

M

Let us consider again an ODE ˙x = f (x) in

(Z) with a continuous, but generally
unbounded f . We are interested here in evolutions preserving positivity, that is, such that
+(Z) for all t. This implies that
for any initial x
+(Z), the negative part
f must be conditionally positive, in the sense that for any x
+(Z) = l1
of f (x) is absolutely continuous with respect to f (x). In case
+ this means
l1
+ with xk = 0 one has fk(x)
that for any x

+(Z) the solution x(t) belongs to

∈ M
M

∈ M

M

M

0.

∈

≥

39

Remark 11. By Theorem 6.21 of [54], conditionally positive bounded f have the following
structure: there exist a family of stochastic kernels ν(x, y, dz) in Z, x
(Z), and a
non-negative function a(x, z) on

Z such that

∈ M

(Z)

M

×

f (x)(dy) =

x(dz)ν(x, z, dy)

ZZ

a(x, y)x(dy).

−

(125)

In particular, if Z = N, this means the existence of nonnegative functions ν(j, x, k) and
a(j, x) on N

l1 respectively such that

N and on N

l1

×

×

×

fk(x) =

xjν(x, j, k)

j
X

a(x, k)xk.

−

(126)

A continuous function L on Z, bounded below by a positive constant, will be referred
to as a Lyapunov function or a barrier. For any such function, let us deﬁne the subset

(Z, L) of

M

M

(Z) of measures x such that

x
kL =

k

Z

L(z)

x
|

|

(dz) = (

, L) <

x
|

|

,

∞

(127)

which is itself a Banach space with the norm
in

(Z, L) of radius R and let

(Z, L)+ =

kL. Let us denote by B(L, R) the ball
.
k
(Z, L)
∩ M
∩
M
(Z, L). In particular, l1(1) = l1 =

+(Z), B+(L, R) = B(L, R)

M
+(Z). For the case Z = N let us write l1(L) for
(Z), where 1 denotes of course the function that equals 1 everywhere.
Let us say that the equation ˙x = f (x) and the function f (x) are L-subcritical (respec-

M

M

M
M

tively, satisfy the Lyapunov condition for L) if f :

(Z, L)+ → M

M

(Z, L) and

( respectively

(L, f (x)) =

L(z)f (x)(dz)

Z

0

≤

(L, f (x))

≤

a(L, x) + b

(128)

(129)

for all x

∈ M

(Z, L)+ and some constants a, b).

(Z, L) or

Lemma 5.5. (i) Suppose the function f is conditionally positive, satisﬁes the Lyapunov
condition for a Lyapunov function L on Z and is Lipschitz either weakly or in the norm
(Z, L)+,
of
the Cauchy problem of equation ˙x = f (x) with initial condition x at time s
0 has a
unique global (that is deﬁned for all times) solution X(t, x) in
(Z, L)+ with derivative
understood with respect to the corresponding topology. Moreover,

(Z) on any bounded subset of

(Z, L)+. Then, for any x

∈ M
≥

M

M

M

M

X(t, x)

∈

B+(L, eat(

x0kL + bt)).

k

In particular, any ball B+(L, R) is invariant under an L-subcritical evolution.

(ii) If additionally to (129), one has

with a constant a1, then

(L, f (x))

a1(L, x)

≥ −

e−a1t(L, x).

(L, X(t, x))

≥

40

(130)

(131)

(132)

(iii) Finally, if instead of (129) one has

then

(L, f (x)) = a(L, x) + b,

(L, X(t, x))) = eat[(L, x) +

b
a

(1

−

e−at)].

(133)

(134)

Proof. (i) By local Lipschitz continuity and conditional positivity, equation ˙x = f (x) is
locally well-posed and preserves positivity. Moreover, by the Lyapunov condition

(L, x(t))

≤

t

(L, x) + a

(L, x(s)) ds + bt,

0
Z

so that by Gronwall’s lemma (and the preservation of positivity)

(L, x(t))

0

≤

≤

eat[(L, x) + bt]

implying that the solution can be extended to all times with required bounds.

(ii) This is clear, as (129) implies

d
dt

(L, x(t))

a1(L, x(t)).

≥ −

(iii) Equation (133) implies

d
dt

(L, x(t)) = a(L, x(t)) + b,

leading to (134).

Acknowledgements. I am grateful to Alain Bensoussan, Mark Kilgour, Oleg Malafeyev

and Didier Sornette for very useful comments to the initial drafts of this manuscript.

References

[1] T. S. Aidt. Economic Analysis of corruption: a survey. The Economic Journal 113:

491 (2009), F632-F652.

[2] M. Aizenman and Th. A. Bak. Convergence to equilibrium in a system of reacting

polymers. Comm. Math. Phys. 65:3 (1979), 203 - 230.

[3] S. Alpern and Th. Lidbetter. Searching a variable speed network. Math. Oper. Res.

39:3 (2014), 697 - 711.

[4] S. Alpern and Th. Lidbetter. Mining coal or ﬁnding terrorists: the expanding search

paradigm. Oper. Res. 61:2 (2013), 265 - 279.

[5] S. Alpern, A. Morton and K. Papadaki. Patrolling games. Oper. Res. 59:5 (2011),

1246 - 1257.

41

[6] L. Andreozzi. Inspection games with long-run inspectors. European Journal of Ap-

plied Mathematics, 21:4-5 (2010), 441-458.

[7] D. Arce and T. Sandler. Counterterrorism: A Game-Theoretic Analysis. Journal of

Conﬂict Resolution 49 (2005), 183-200.

[8] R. Avenhaus, M.J. Canty. Playing for time: a sequential inspection game. European

Journal of Operational Research, 167:2 (2005), 475-492.

[9] R. Avenhaus, D. Kilgour. Eﬃcient distributions of arm-control inspection eﬀort.

Naval Research Logistics, 51:1(2004), 1-27.

[10] R. Avenhaus, B. Von Stengel, S. Zamir (2002). Inspection games. In: R. Aumann, S.
Hart (Eds.) Handbook of Game Theory with Economic Applications, Vol. 3 North-
Holland, Amsterdam, 1947- 1987.

[11] J. M. Ball and J. Carr. The Discrete Coagulation-Fragmentation Equations: Exis-
tence, Uniqueness, and Density Conservation. Journ. Stat. Phys. 61: 1/2, 1990.

[12] J. M. Ball, J. Carr and O. Penrose. The Becker-Dring cluster equations: basic prop-
erties and asymptotic behaviour of solutions. Comm. Math. Phys. 104:4 (1986), 657
- 692.

[13] A.-L. Barab´asi and R. Albert. Emergence of Scaling in Random Networks. Science

286, 509-512.

[14] H. Beecher Stowe. Uncle Tom’s cabin. Blackie, 1963.

[15] G. S. Becker and G. J. Stigler. Law enforcement, Malfeasance, and Compensation of

Enforces. The Journal of Legal Studies 3:1 (1974), 1-18.

[16] M. Benaim, J. Weibull. Deterministic approximation of stochastic evolution in games.

Econometrica 71:3 (2003), 873 - 903.

[17] M. Benaim and J.-Y. Le Boudec, A class of mean ﬁeld interaction models for com-
puter and communication systems. Performance Evaluation 65 (2008), 823 - 838.

[18] A. Bensoussan, J. Frehse, P. Yam. Mean Field Games and Mean Field Type Control

Theory, Springer, 2013.

[19] A Bensoussan, S. Hoe , M. Kantarcioglu, A Game-Theoretical Approach for Find-
ing Optimal Strategies in a Botnet Defense Model. Decision and Game Theory for
Security First International Conference, GameSec 2010, Berlin, Germany, November
22-23, 2010. Proceeding, T. Alpcan, L. Buttyan, and J. Baras (Eds.), Vol. 6442 pp.
135-148.

[20] A. Bensoussan, J. Frehse and P. Yam. The Master equation in mean ﬁeld theory.
Journal de Math´ematiques Pures et Appliqu´ees, (9) 103:6 (2015), 1441 - 1474.

[21] R. Carmona and F. Delarue. The master equation for large population equilibri-
ums. Stochastic Analysis and Applications. Springer Proc. Math. Stat. 100, Springer,
Cham, 2014, pp. 77-128.

42

[22] G. Bianconi and A.-L. Barabasi. Bose-Einstein Condensation in Complex Network.

Physical Review Letters 86:24 (2001), 5632-5635.

[23] K. Binmore and L. Samuelson. Muddling through: noisy equilibrium selection. J.

Econom. Theory 74:2 (1997), 235 - 265.

[24] K. Binmore, L. Samuelson and R. Vaughan. Musical chairs: modeling noisy evolution.

Games Econom. Behav. 11:1 (1995), 1 - 35.

[25] S. Bowles. Microeconomics. Behavior, Institutions and Evolution. Russell Sage Foun-

dation, 2004.

[26] R. Boylan. Continuous approximation of dynamical systems with randomly matched

individuals. J. Econom. Theory 66:2 (1995), 615 - 625.

[27] S. J. Brams and M. Kilgour. Kingmakers and Leaders in Coalition Formation. Social

Choice and Welfare 41:1 (2013), 1-18.

[28] S. J. Brams and M. Kilgour. National Security Games. Synthese 76 (1988), 185-200.

[29] A. Clauset, M. Young and K. S. Gleditsch. On the Frequency of Severe Terrorist
Events. arXiv:physics/0606007v3 Journal of Conﬂict Resolution February 2007 51
(2007), 58-87.

[30] V. Corradi, R. Sarin. Continuous approximations of stochastic evolutionary game

dynamics. J. Econom. Theory 94:2 (2000), 163 - 191.

[31] St. Dereich and P. M¨orters. Random networks with sublinear preferential attachent:

the giant component. The Annals of Probability 41:1 (2013), 329 - 384.

[32] R. Dorfman, P. Samuelson and R. Solow. Linear programming and economic analysis.

McGraw-Hill, New York, 1958.

[33] N. El Karoui, I. Karatzas. Dynamic allocation problems in continuous time. Ann.

Appl. Probab. 4:2 (1994), 255286.

[34] G. Ellison. Learning, local interaction, and coordination. Econometrica 61:5 (1993),

1047 - 1071.

[35] J. R. Faria and D. Arce. A Vintage Model of Terrorist Organizations. Journal of

Conﬂict Resolution 56:4 (2012), 629-650.

[36] H. Fielding. The history of the life of the late Mr Jonathan Wilde the Great. H.

Hamilton, 1947.

[37] W. H. Fleming, H. M. Soner. Controlled Markov Processes and Viscosity Solutions.

Sec. Ed. Springer 2006.

[38] D. Foster and P. Young. Stochastic evolutionary game dynamics. Theoret. Population

Biol. 38:2 (1990), 219 - 232.

43

[39] N. Gast, B. Gaujal and J.-Y. Le Boudec. Mean Field for Markov Decision Processes:
From Discrete to Continuous Optimization. IEEE Trans. Automat. Control 57:9
(2012), 2266-2280.

[40] J. C. Gittins. Multi-Armed Bandit and Allocation Indices. Wiley, 1989.

[41] D. A. Gomes, J. Mohr, R. Souza. Discrete time, ﬁnite state space mean ﬁeld games.

J. Math. Pures Appl. (9) 93:3 (2010), 308 - 328.

[42] A. Gorban and M. Shahzad. The Michaellis-Menten-Stueckelberg theorem. Entropy

13: 5 (2011), 966-1019.

[43] A. Gorban and V. Kolokoltsov. Generalized Mass Action Law and Thermodynam-
ics for Generalized Nonlinear Markov Processes. To appear in: The Mathematical
Modelling of Natural Phenomena (MMNP), 2015.

[44] R. Gunther, L. Levitin, B. Schapiro and P. Wagner. Zipf’s Law and the Eﬀect of
Ranking on Probability Distributions. International Journal of Theoretical Physics
35:2 (1996), 395-417.

[45] O. Hernandez-Lerma and J. B. Lasserre. Discrete-Time Markov Control Processes.

Springer, New York, 1996.

[46] J. Harsanyi and R. Selten. A General Thery of Equilibrium Selection in Games.

Cambridge MA, MIT Press, 1988.

[47] M. Huang. Large-population LQG games involving a major player: the Nash certainty

equivalence principle. SIAM J. Control Optim., 48 (2010), 3318-3353.

[48] M. Huang, R. Malham´e, P. Caines. Large population stochastic dynamic games:
closed-loop Mckean-Vlasov systems and the Nash certainty equivalence principle.
Communications in information and systems 6 (2006), 221-252.

[49] A. K. Jain. Corruption: a review. Journal of Economic Surveys 15: 1 (2001), 71-121.

[50] M. Kandori, G. J. Mailath and R. Rob. Learning, mutation, and long run equilibria

in games. Econometrica 61:1 (1993), 29 - 56.

[51] V. N. Kolokoltsov. Hydrodynamic limit of coagulation-fragmentation type models
of k-nary interacting particles. Journal of Statistical Physics 115, 5/6 (2004), 1621-
1653.

[52] V. Kolokoltsov. Measure-valued limits of interacting particle systems with k-nary
interaction II. Finite-dimensional limits. Stochastics and Stochastics Reports 76:1
(2004), 45-58.

[53] V. N. Kolokoltsov. Kinetic equations for the pure jump models of k-nary interacting

particle systems. Markov Processes and Related Fields 12 (2006), 95-138.

[54] V. N. Kolokoltsov. Nonlinear Markov processes and kinetic equations. Cambridge

Tracks in Mathematics 182, Cambridge Univ. Press, 2010.

44

[55] V. N. Kolokoltsov. Nonlinear Markov games on a ﬁnite state space (mean-ﬁeld and
binary interactions). International Journal of Statistics and Probability 1:1 (2012),
77-91. http://www.ccsenet.org/journal/index.php/ijsp/article/view/16682

[56] V. N. Kolokoltsov and O. A. Malafeyev. Understanding Game Theory. World Scien-

tiﬁc, Singapore, 2010.

[57] V. N. Kolokoltsov and O. A. Malafeyev. Mean ﬁeld game model of corruption (2015).
http://arxiv.org/abs/1507.03240. To appear in Dynamics Games and Applications.

[58] V. N. Kolokoltsov and V. P. Maslov. Idempotent Analysis an its Applications. Kluwer

Academic, 1987.

[59] V. N. Kolokoltsov, H. Passi, W. Yang. Inspection and crime prevention: an evolu-

tionary perspective (2013). http://arxiv.org/abs/1306.4219

[60] V. Kolokoltsov and W. Yang. The turnpike theorems for Markov games. Dynamic

Games and Applications 2: 3 (2012), 294-312.

[61] V. N. Kolokoltsov and W. Yang. Inspection games in a mean ﬁeld setting. Manuscript

in preparation.

[62] P. L. Krapivsky and S. Redner. Organization of growing random networks. Phys.

Rev. E 63 (2001), 066123.

[63] G. E. Kreindler and H. P. Young. Fast convergence in evolutionary equilibrium se-

lection. Games Econom. Behav. 80 (2013), 39 - 67.

[64] A. Lambert-Mogiliansky, M. Majumdar and R. Radner. Strategic analysis of petty

corruption with an intermediary. Rev. Econ. Des. 13: 1-2 (2009), 45 - 57.

[65] J-M. Lasry, P-L. Lions. Jeux `a champ moyen, I. Le cas stationnaire. C.R. Math.

Acad. Sci. Paris 343:9 (2006), 619-625.

[66] J.-Y. Le Boudec. The stationary behaviour of ﬂuid limits of reversible processes is
concentrated on stationary points. Netw. Heterog. Media 8:2 (2013), 529 - 540.

[67] Zh. Li, Q. Liao and A. Striegel. Botnet Economics: Uncertainty Matters.

http://weis2008.econinfosec.org/papers/Liao.pdf

[68] D. L´opez-Pintado. Contagion and coordination in random networks. Internat. J.

Game Theory 34:3 (2006), 371 - 381.

[69] K-W. Lye, J. M. Wing. Game strategies in network security. Int J Inf Secur 4 (2005),

71 - 86.

[70] O. A. Malafeyev, N. D. Redinskikh and G. V. Alferov. Electric circuits analogies
in economics modeling: Corruption networks. Proceedings of ICEE-2014 (2nd Inter-
national Conference on Emission Electronics), DOI: 10.1109/Emission.2014.6893965,
Publisher: IEEE

[71] B. B. De Mesquita. The Predictioneer’s Game. Random House 2010.

45

[72] M. Mobilia, I.T. Georgiev, U.C. T¨auber and C. Uwe. Phase transitions and spatio-
temporal ﬂuctuations in stochastic lattice Lotka-Volterra models. J. Stat. Phys. 128
(2007), no. 1-2, 447-483.

[73] J. Norris. Cluster Coagulation. Comm. Math. Phys. 209 (2000), 407-435.

[74] M. Nourian and P. E. Caines. ǫ-Nash mean ﬁeld game theory for nonlinear stochastic
dynamical systems with major and minor agents. SIAM J. Control Optim. 51:4
(2013), 3302 - 3331.

[75] D. O. Pushkin and H. Aref. Bank mergers as scale-free coagulation. Physica A 336

(2004) 571 - 584.

[76] L. F. Richardson. Variation of the frequency of fatal quarrels with magnitude. Journ.

Amer. Stat. Ass. 43 (1948), 523.

[77] B. P. Rosendorﬀ and T. Sandler. Too Much of a Good Thing?: The Proactive Re-

sponse Dilemma. Journal of Conﬂict Resolution 48 (2005), 657-671.

[78] A. Saichev, Ya. Malvergne and D. Sornette. Theory of Zipf’s Law and Beyond. Lec-
ture Notes in Economics and Mathematicl Systems 632, Springer, Berlin 2010.

[79] F. Salten. Bambi, A life in the Woods. Engl. Transl. Simon and Schuster, 1928.

[80] W. Sandholm. Almost global convergence to p-dominant equilibrium. Internat. J.

Game Theory 30:1 (2001), 107 - 116.

[81] W. Sandholm. Stochastic imitative game dynamics with committed agents. J.

Econom. Theory 147:5 (2012), 2056 - 2071.

[82] T. Sandler and D. Arce. Terrorism and Game Theory. Simulation and Gaming 34:3

(2003), 319 - 337.

[83] T. Sandler and H. E. Lapan. The Calculus of Dissent: An Analysis of Terrorists’

Choice of Targets. Synthese 76:2 (1988), 245-261.

[84] M.V. Simkin and V.P. Roychowdhury. Re-inventing Willis. arXiv:physics/0601192v3,

Physics Reports 502 (2011), 1-35.

[85] B.-Ch. Wang and J.-F. Zhang. Distributed output feedback control of Markov jump

multi-agent systems. Automatica J. IFAC 49:5 (2013), 1397 - 1402.

[86] G. Yaari, A. Nowak, K. Rakocy and S. Solomon. Microscopic study reveals the singu-
lar origins of growth. Eur. Phys. J. B 62, 505 - 513 (2008) DOI: 10.1140/epjb/e2008-
00189-6

[87] H. P. Young. The evolution of conventions. Econometrica 61:1 (1993), 57 - 84.

[88] A. J. Zaslavski. Turnpike properties in the calculus of variations and optimal control.

Springer, New York, 2006.

46

