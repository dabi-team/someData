2
2
0
2

g
u
A
9
2

]

R
C
.
s
c
[

1
v
4
2
5
3
1
.
8
0
2
2
:
v
i
X
r
a

Lateral Movement Detection Using User
Behavioral Analysis

Deepak Kushwahaa, Dhruv Nandakumara,
Akshay Kakkara, Sanvi Guptaa,
Kevin Choia, Christopher Redinoa, Abdul Rahmana,
Sabthagiri Saravanan Chandramohana, Edward Bowena,
Matthew Weeksa, Aaron Shahaa, Joe Nehilaa
aDeloitte & Touche LLP
∗Corresponding author: kevchoi@deloitte.com

August 30, 2022

Abstract

Lateral Movement refers to methods by which threat actors gain initial
access to a network and then progressively move through said network col-
lecting key data about assets until they reach the ultimate target of their
attack. Lateral Movement intrusions have become more intricate with the
increasing complexity and interconnected nature of enterprise networks,
and require equally sophisticated detection mechanisms to proactively de-
tect such threats in near real-time at enterprise scale. In this paper, the
authors propose a novel, lightweight method for Lateral Movement detec-
tion using user behavioral analysis and machine learning. Speciﬁcally, this
paper introduces a novel methodology for cyber domain-speciﬁc feature
engineering that identiﬁes Lateral Movement behavior on a per-user basis.
Furthermore, the engineered features have also been used to develop two
supervised machine learning models for Lateral Movement identiﬁcation
that have demonstrably outperformed models previously seen in litera-
ture while maintaining robust performance on datasets with high class
imbalance. The models and methodology introduced in this paper have
also been designed in collaboration with security operators to be relevant
and interpretable in order to maximize impact and minimize time to value
as a cyber threat detection toolkit. The underlying goal of the paper is
to provide a computationally eﬃcient, domain-speciﬁc approach to near
real-time Lateral Movement detection that is interpretable and robust to
enterprise-scale data volumes and class imbalance.

1

Introduction

The increasing size, complexity, and interconnected nature of enterprise net-
works is accompanied by an increasingly complex attack surface. Malicious
actors leverage this connected nature by inﬁltrating weaker parts of networks
and then using elevated credentials to move between interconnected hosts in
the search for sensitive data or other high value assets. Lateral Movement (LM)

1

 
 
 
 
 
 
attacks are a key diﬀerentiator responsible for many of today’s Advanced Per-
sistent Threats (APTs) [Crowdstrike, 2022]. Detecting instances of malicious
LM is paramount to maintaining eﬀective enterprise security posture and being
able to proactively mitigate breaches that could cost hundreds of millions of US
dollars [Mossburg et al., 2016].

In this paper, we introduce a method to detect LM in remote authentication
events by utilizing user-focused feature engineering created speciﬁcally to cap-
ture LM behaviors and supervised classiﬁers trained to accommodate for highly
imbalanced datasets. The goal of the methodology is to enable robust and in-
terpretable LM detection in near real-time using lightweight classiﬁers and a
highly generalized feature engineering pipeline while simultaneously prioritizing
extremely low False Positive Rates (FPR) to prevent alert fatigue of end users.
The key contributions of this paper are:

• Providing a novel feature set designed to speciﬁcally identify LM behaviors

in user authentication data using user-based behavioral analysis.

• Benchmarking supervised classiﬁer performance on our novel feature set

on a realistic, and highly imbalanced dataset.

• Exploring a framework for model interpretability that allows security op-
erators to eﬀectively understand model decision criteria on a per-example
basis.

The paper begins with a literature review to explore key eﬀorts toward LM
detection that exist today, followed by sections that discuss our feature engi-
neering, modeling approach, evaluation, interpretability, and ﬁnally a conclusion
and discussion of future work.

2 Background

The objective of this section is to outline previous work conducted in the iden-
tiﬁcation of LM in enterprise networks using Machine Learning (ML) based
approaches. Most of the approaches seen in literature thus far belong predomi-
nantly to either rules-based methods or unsupervised learning methods with a
few studies utilizing supervised ML. One such rule-based study was conducted
by [Bowman et al., 2020] wherein the authors classiﬁed all ﬁrst time login events
between a user and device as anomalous. Similarly, other rules-based approaches
could also be conducted such as classifying all failed login events as anomalous.
However, the impact of scale and FPR are readily apparent when using the
above methods, especially as the size of enterprise networks grows. Utilizing
more advanced ML based methods would not only drastically improve eﬃciency
and reduce FPR but also allow for more complex APTs to be captured from
authentication logs.

Unsupervised learning approaches to LM detection allow for detection of
more complex anomalous behavior in authentication data as compared to rule-
based approaches. The notion is to be able to group/cluster users based on
patterns of similar behavior to identify if any particular users are anomalous
insofar as they do not belong to a large enough cluster.
[Bohara et al., 2017]
implemented a graph-based approach to LM detection utilizing Network and
Host Process logs, wherein the authors extracted graph-based features from the

2

aforementioned logs and implemented Principal Component Analysis (PCA)
[Wold et al., 1987] and K-means clustering [Hartigan and Wong, 1979] to iden-
tify outliers in the dataset. [Bowman et al., 2020] also utilized an unsupervised
graph-based technique using authentication data by implementing a logistic re-
gression based method to predict low-probability links between assets. While
this approach demonstrated strong results with an FPR of only 0.9%, the actual
dataset used for evaluation was signiﬁcantly smaller than those expected in en-
terprise networks.
[Chen et al., 2018] also utilized host-communication graphs
as a basis for feature extraction using a denoising autoencoder followed by a
K-Nearest-Neighbors (KNN) classiﬁer [Short and Fukunaga, 1981] for LM clas-
siﬁcation. The approach produced very strong results with a precision of 91%
and an FPR of 0.01%, but training and evaluation was conducted on a dataset
wherein 6.8% of the data was malicious and performance is anticipated to de-
grade as the class imbalance intensiﬁes to enterprise scale. Furthermore, graph
based approaches in general may be harder for end-users (security operators)
to interpret, making results signiﬁcantly less usable [Du et al., 2018]. It is also
worth noting that the eﬃcacy of unsupervised methods depends heavily on the
characteristics of the underlying data and require continuous per-network hy-
perparameter tuning to ensure that the results of the model are meaningful -
which make such models less feasible to deploy in enterprise networks eﬃciently.
Supervised ML approaches tend to overcome the hurdle of deployment eﬃ-
ciency by utilizing a pre-trained model for LM detection on Information Tech-
nology (IT) networks. Supervised methods can also be trained to account for
heavy class imbalance and complex data distributions.
[Bai et al., 2021] uti-
lized a variety of supervised ML methods on user authentication and host
process data from the Los Alamos National Laboratory (LANL) uniﬁed and
comprehensive datasets [Kent, 2015, Kent, 2016] including Logistic Regression
[Hosmer Jr et al., 2013], Random Forest (RF) classiﬁers [Breiman, 2001], Log-
itBoost (LB) [Friedman et al., 2000], and Light Gradient Boosting Machine
(LightGBM) [Ke et al., 2017] methods. The results of the study demonstrated
very strong performance with over 99% accuracy, precision, and recall but
the models were trained and evaluated on a dataset containing 56,837 events,
which will likely not be representative of enterprise scale class imbalances.
[Uppstr¨omer and R˚aberg, 2019] highlights the performance of diﬀerent super-
vised classiﬁers such as Decision Tree (DT) [Song and Lu, 2015], RF classiﬁer,
KNN, Support Vector Machines (SVM) [Boser et al., 1992] on a more imbal-
anced dataset containing 1% malicious activity with the RF model achiev-
ing 98% precision and 86% recall, which is already signiﬁcantly lower than in
[Bai et al., 2021].

Clearly, the development of a model that can operate with extremely high
precision, recall, and low FPR using highly imbalanced data representative of
enterprise networks is imperative. Furthermore, it is also important for the said
model to be interpretable to maximize its usability while simultaneously being
computationally eﬃcient. Through the rest of this paper, we will explore the
creation of such a model.

3

3 Methodology

The primary motivation for this work is to be able to build a supervised ML
model that can identify LM behaviors in enterprise networks eﬀectively. This
entails being able to capture malicious authentication activity in near real-time
reliably while simultaneously avoiding alert fatigue on security operators by re-
ducing the number of false positives. We believe that we can achieve strong
results with user-based behavioral analysis of authentication and process events
using domain-speciﬁc features designed to mimic indicators that security oper-
ators use while investigating potential LM events. Herein, we will deﬁne and
describe a comprehensive methodology for creating such domain-speciﬁc fea-
tures while minimizing computational overhead. The feature engineering aims
to construct data such that all normal user/authentication data will have similar
feature values which will also vary signiﬁcantly from feature values exhibit by
malicious actors. In subsequent sections we are going to explore the datasets we
utilized for modeling, our feature engineering process, and the complete viability
of derived features using Exploratory Data Analysis (EDA).

3.1 Dataset

In this paper, we used the LANL Comprehensive, Multi-Source Cyber-Security
Events dataset [Kent, 2015] to train and evaluate our models and approach.
It is a high-quality, publicly available assessment dataset with labelled cases
targeting LM behavior. It contains real network traﬃc data from a live exercise
at LANL, where red team hackers ran an attack campaign on the network to
simulate an adversarial event. The dataset contains security logs for 58 days,
with only 0.0000713% of all occurrences ﬂagged as malicious, resulting in an
highly imbalanced dataset. However, we believe that this is representative of
realistic probabilities of ﬁnding such events in enterprise data and consequently
prioritized modeling and feature engineering eﬀorts to maintain performance
despite the imbalance.

The LANL dataset includes network ﬂow telemetry, authentication logs, sys-
tem process execution logs, and Domain Name System (DNS) logs. However, we
only utilize the authentication and process logs from the dataset which contains
data1 as described below.

The process dataset contains 62,947 events with each event containing the

following ﬁelds:

• time: The time at which the process event occurred, measured in seconds

since the ﬁrst event in the dataset.

• user@domain: The user identity and domain of the user who’s computer

executed said process.

• computer: The identiﬁer for the device that executed said process.

• process name: The name of the process that was executed.

• start/end: Flag indicating whether a process execution began or termi-

nated.

1In some cases, missing data ﬁelds are indicated using ’ ?’

4

The authentication dataset contains 1.05 billion authentication events, which

are comprised of 9 ﬁelds:

• time: The time at which the authentication event occurred, measured in

seconds since the ﬁrst event in the dataset.

• source user@domain: The source user logging in and the associated

domain the user is using.

• destination user@domain: The username and the associated domain

after the user has performed an authentication event.

• source computer: The origin computer used for the authentication

event.

• destination computer: The destination computer where the authenti-

cation event terminates.

• authentication type: The type of Windows-based authentication used.

It contains values like NTLM, Kerberos, etc.

• logon type: The type of authentication occurring includ- ing cross-
network authentication, an interactive keyboard session, a batch event,
a system service, a screen saver lock or unlock, and several others.

• authentication orientation: This includes values indicating whether
the event is for granting a Kerberos, TGT or TGS, a log on or log oﬀ
event, or an authentication credential mapping.

• success/failure: A ﬂag indicating whether the authentication event is
successfully completed or failed. Failure could happen for several reasons
including incorrect passwords, a locked out account, or an authorization
failure.

3.2 Feature Engineering

While the LANL dataset is, indeed, realistic in scale and quality, we believe
that domain-speciﬁc feature engineering is necessary to facilitate ML models
to distinguish between malicious and benign events. These features are eﬃ-
cient in simultaneously capturing user usage patterns and also the complicated
anomalies that emerge within the network due to an adversary’s intrusion, fur-
ther resulting in an LM attack. The following features are derived from the
existing ﬁelds in the authentication and process event logs and are designed
speciﬁcally to identify LM behaviors in a manner similar to how Security Op-
erations Center (SOC) personnel would identify LM. Furthermore, given LM
occurs on a per-user basis, our feature engineering is focused primarily on fea-
tures that aim to capture information about user-behavior while avoiding the
eﬀect of authentication activity carried out by other users within the network in
the process. This has the added beneﬁt of reducing the computational overhead
of feature engineering in enterprise networks while still maintaining consistently
strong performance as an anomaly detector. The following features were de-
signed based on feedback and rationale provided by our organization’s cyber
Subject Matter Experts (SME) team:

5

• Remote: Whether the event is a remote login or a local login. A remote
login event will have an authentication activity originating from a source
computer and terminate on a diﬀerent destination computer. While inter-
active events will be used during feature engineering, only remote events
will be considered during model training. This is due to the fact that
LM events will always have to be remote in nature. Successive interactive
(physical) logins are not, by deﬁnition, LM and can be classiﬁed as benign
a priori.

• Duration: Duration of login session between a successive login and lo-
gout by a user. For every session, after the logoﬀ event, the duration is
calculated by identifying the most recent login event from the user to the
same source computer. Intrusion activity may result in very short, near
instantaneous sessions when used as part of an automated attack, while
interactive operators using a login session to maintain remote access to a
system may extend the session much longer than ordinary activity.

• Was source logged on: Whether source user account is already logged
into source computer at the time of performing a remote authentication
event. This can be identiﬁed as any interactive login event within the past
24 hours with no logout event afterward. This speciﬁc feature can capture
behavior where a malicious actor is trying to perform a remote login on a
destination within the network, without having any record of interactive
source login on the same computer, a scenario that often happens when
employing stolen credentials.

• How long ago: In incidents where the source computer is logged on
interactively within past 24 hours, this feature captures the time span
between the current remote event and the last interactive login from same
source computer. It is calculated by ﬁnding the time diﬀerence between
the most recent interactive login event and the current remote event from
the same source computer.

• How many other interactive logins: The number of other active in-
teractive logins to other computers that this user had at the time of the
remote event. Typically, a source user is expected to be logged in from
their credentials to 1 or at most 2 source computers interactively and
having a higher count higher could indicate suspicious behavior.

• Was process run: A ﬂag indicating whether a new process gets executed
on the destination computer within 60 seconds of a remote login. This
characteristic identiﬁes behaviors where an adversary can start a process
soon after performing LM and gaining a foothold of destination computer.

• Previous login fraction: Fraction of source account’s previous logins
within an hour of the current time of day (i.e., business hours). The feature
captures behavior of attackers where the authentication event occurs out
of the typical business hours for the source user. This feature will have
abnormal values if, for example, the credential of a user is used to login
a computer around midnight, who usually logs on in the ﬁrst half of the
day.

6

• Number of failed: Number of failed remote logins from the same source
computer within past 1 hour. In many instances, malicious LM activity
occurs with reused credentials the attackers obtained elsewhere or guessed
and there is a high probability of failed events as the attacker tries other
passwords before ﬁnding the correct one. A remote event captured after
multiple failed attempts by any user on same source computer could be
more indicative of a malicious actor.

• Fraction from same box: This captures a user’s inherent behavior of
logging in from the same source computer. It is measured as the fraction of
source account’s previous logins have been from the same source computer.
Any deviation of user’s behavior of logging in from other source computer
will be captured and represented by low percentage.

3.3 Exploratory Data Analysis

After completing the process of feature engineering, our goal is to perform spe-
ciﬁc EDA and determine how valuable domain-speciﬁc features are in deﬁning a
clear separation boundary between malicious and benign remote login activity.
To depict the variance in feature values for malicious and non-malicious users,
standard statistical measures with aggregated values for each feature are com-
pared. The goal of this statistical analysis is to decide whether the new features
provide visibility in users having malicious authentication events and diﬀeren-
tiate them from benign users only based on statistical values. As a result, the
investigation revealed that aggregated values for individual user data had less
inﬂuence than individual authentication events and hence, future techniques will
only employ individual authentication occurrences to construct an ML solution.
That is, the ML models in this paper are trained on each of the authentication
events for each user individually rather than in an aggregated form.

Next, we explore spatial separation between benign and malicious authen-
tication events in the LANL dataset with our new features to understand if
they can be clearly discerned by an ML model. Given the scale of data, it is
computationally infeasible and impractical to visualize all data points. Hence,
the visualization is performed by using the Uniform Manifold Approximation
and Projection (UMAP) technique [McInnes et al., 2018] for individual authen-
tication events from a random stratiﬁed subset of the data having 100,000 au-
thentication data points (574 malicious events + 99,426 normal events) in Fig.
1. The normal authentication events are shown in lighter shade of grey, while
the malicious ones representing LM are shown in black.

As shown Fig. 1, it is evident that benign and malicious data are fairly dis-
cernible in a latent space using the features we constructed, which is a promising
result. From here, we can focus on using this diﬀerence in data distributions
between classes to create a classiﬁer that distinguishes malicious from non-
malicious activity. A signiﬁcant challenge in designing a binary class classiﬁer is
handling the extreme data imbalance and approaches to do so will be explored
henceforth.

7

Figure 1: UMAP with a distribution of benign and malicious events

4 Experimental Design

Our feature set is bench-marked on supervised ML models that are designed
as binary classiﬁers. This is done in an eﬀort to facilitate near real-time de-
tection using authentication and process data with minimal hyper-parameter
optimization post training. The goal of these models will be to look at each
event independently and classify them as malicious or non-malicious based on
the values of user-behavior-based domain-speciﬁc features. Furthermore, as dis-
cussed in Methodology section, interactive (non-remote) authentication events
are not used during model training or evaluation and are discarded after being
used for feature generation. This section will next discuss our data preparation
process, how we train using imbalanced data and the bench-marking models we
apply.

4.1 Data Preparation

Post feature engineering Eight features are selected for model training, seven
of which are derived and one from raw data ﬁelds: remote, was source logged
on, how long ago, how many other interactive logins, was process run, previous
login fraction, number of failed, fraction from the same box and authentication
type respectively. Two of the nine features derived through feature engineering
were dropped: remote and duration. Since only remote events are used to train
the model, this binary characteristic is unnecessary for training. Similarly, the
duration value is only generated upon session logoﬀ, whereas the goal of this
method is to identify anomalies in real time for every remote login event. The
authentication type, which is a categorical raw feature, needed to be transformed
using label encoding. All other features are included without any modiﬁcations.

8

Training and validation sets are captured from the existing data in a ratio of
80:20. The data split has been very carefully achieved on a user basis and
stratiﬁed by the number of malicious activities within those user authentication
activities. Therefore, all of the authentication events of an individual user re-
side in either the training set or the test set. This distinction is important to
accurately validate the model to new users and determine that our models can
generalize eﬀectively. It is also important to note here that aside from focusing
on only remote events, the training and testing data are not over-sampled or
under-sampled in any way and the original class imbalance is maintained to
make sure our model performance accurately reﬂects real-world performance.
After excluding local interactive and logoﬀ events, the training dataset contains
a total of 263,716,042 authentication events, with only 485 events tagged as
malicious. The test set contains 66,530,377 events, out of which 89 events are
labelled as malicious.

4.2 Model Selection and Imbalanced Classiﬁcation

Given that the objective of our models will be to accurately classify LM at-
tempts in enterprise network data, we need to choose modeling approaches
that can accommodate both complex non-linear separations in data as well
as extreme imbalance. Consequently, we chose to implement two in- depen-
dent models to classify LM events: Extreme Gradient Boosting (XGBoost)
[Chen and Guestrin, 2016] and Fully Connected Deep Neural Networks (FCDN)
[Hecht-Nielsen, 1992]. We selected XGBoost as our ﬁrst model due to the
fact that DTs generally perform well with imbalanced data and ensemble tech-
niques with DTs (RF, XGBoost, etc.) almost always outperform singular DTs
[Zhang and Ma, 2012]. FCDNs were utilized because of their ability to perform
well in highly complex decision spaces. However, we augmented the training pro-
cesses for both models with cost-sensitive training that utilizes class weighting
while computing model loss to assign examples of the minority class a higher
weight. Furthermore, we also utilized ﬁt for purpose evaluation metrics that
gave us a clear understanding of the performance of our models on classifying
imbalanced data: Recall scores and FPR.

4.3 Model Architecture

4.3.1 The XGBoost Approach

XGBoost is a scalable tree ensemble boosting system algorithm, which is used
widely by data scientists to achieve state-of-the-art results on many ML tasks
[Chen and Guestrin, 2016]. It facilitates training capabilities for in-memory as
well as out of memory datasets. The tree-based algorithm can ingest data for
training without the need of feature scaling/normalization and are treated as
one of the best suited algorithms for imbalance classiﬁcation problem in sta-
tistical ML domain. Given the size of the training data, a distributed version
of XGBoost is used to train and validate the models. To handle the highly
imbalanced distribution of malicious and benign activities within the training
data, the parameter scale pos weight which controls the balance of positive and
negative weights is modiﬁed. As the class imbalance is quite high, we used a
more conservative value (scale pos weight=1000) for this parameter to strike

9

a balance between recall scores and FPR. This scaling parameter was identi-
ﬁed by rapid hyper-parameter tuning and simultaneously maintaining a balance
between recall scores and FPR.

4.3.2 The Neural Network Approach

An FCDN is trained as a binary classiﬁer using a feed-forward neural network
followed by a single node output, which outputs the probability of an authenti-
cation event being an instance of LM. Hyperparameter tuning and optimization
is conducted to optimize various parameters in the model including activation
functions, dropout probabilities, layer depth and width. Finally, the utilized
model architecture comprises 4 layers, with an input layer of N neurons that
handles the input data format following data transformation and feature selec-
tion. The two subsequent hidden layers have 14 and 7 neurons respectively with
Rectiﬁed Linear Unit (ReLU) activation functions, while the output layer has
one neuron which is sigmoid activated. The model is trained using a custom loss
function namely, Weighted Binary Cross Entropy (WBCE) loss. WBCE loss is
used to apply the notion of class weighting and penalizing in order to address
class imbalance. This loss function penalizes the class with more data points by
using the weights associated with each class. Below is a custom weighted binary
cross entropy loss function, with a value of 1000 applied to the class weights of
the minority class in order to balance the classiﬁer.

Algorithm 1 Weighted Binary Cross Entropy Loss
Require: weights ← dictionary
Require: labels ← y true
Require: logits ← y pred

loss ← labels ∗ −log(sigmoid(logits)) ∗ weights + (1 − labels) − log(1 −
sigmoid(logits))

Ensure: loss

4.4 Experimental Setup

The experiment results presented in this paper are the result of exhaustive fea-
ture selection. In order to attain the ﬁnal results, the experimental technique
employs a greedy approach to feature selection, selecting the most representa-
tive or signiﬁcant feature from the remaining features at each iteration while
evaluating the results. An identical set of training and testing data is utilized by
both models however, the set of input features for each model diﬀers depending
on the feature selection process. The reported ﬁndings are best obtained after
extensive hyper-parameter optimization and comprehensive feature selection.
Given that feature generation for our training and evaluation datasets needs
to be performed only once, features were pre-computed and stored on disc in
a partitioned format for faster read access during training. For the XGBoost
approach, model training and evaluation are performed in a distributed envi-
ronment utilizing Dask [Rocklin, 2015] on a single machine. Given the quantity
of the data, Dask-distributed XGBoost trains in parallel on all machine cores
to provide the best performance results. FCDNs can handle huge amounts of

10

data due to their support for in-memory, batch-based training and their train-
ing on Graphics Processing Unit (GPU) support for faster training; however,
their overall training time is signiﬁcantly longer than that of XGBoost. Both
models were trained on instances having at least 256 GB of RAM and 24 GB
of V-RAM.

4.5 Success Criteria

Given the highly imbalanced nature of the dataset and the highly severe nature
of an LM attack, we optimize for two metrics in particular: higher recall to
prioritize the detection of malicious activity and a lower FPR to reduce operator
alert fatigue. Our success criteria were to achieve a recall greater than 70% and
a FPR less than 0.005%. It should also be noted that predictions are treated as
LM events if the probability of their prediction is greater than 50%.

5 RESULTS AND DISCUSSION

Figure 2: XGBoost Approach

Figure 3: Deep Learning Approach

11

5.1 Results

The experiments with XGBoost achieved an average recall score or malicious
activity detection score of 86.51% and FPR of 0.0013% for our experimental
setups. The results obtained appear to maintain strong performance despite
higher class imbalance compared to other work in [Du et al., 2018, Uppstr¨omer and R˚aberg, 2019,
Bai et al., 2021], which makes the model more suitable for enterprise threat de-
tection. The results achieved by the FCDN have a recall score of 92.13% and
an FPR of 0.0029%. Both the models show a strong performance in predicting
either of the classes with Recall vs Threshold and FPR vs Threshold as shown
in Fig. 2 and Fig. 3 for XGBoost model and the FCDN respectively.

Algorithm Recall

FPR

XGBoost
FCDN

86.51% 0.0013%
92.13% 0.0029%

Table 1: Recall and FPR scores achieved

It is evident that the XGBoost results are comparatively better if lower
FPRs are the priority. However, both models exhibit very low FPRs in general,
with FCDNs providing good detective capabilities for LM in general. Con-
sequently, the two models demonstrated here can be used in conjunction to
provide stronger detection capabilities. Particularly, the XGBoost model can
be utilized by SOC personnel where operators triage alerts due to their lower
FPR and the FCDN can be utilized by threat hunters, where stronger recall
would be the priority. These results also give us a promising indication that our
domain-speciﬁc features provide a strong foundation for LM detection.

6 Model Interpretability

We believe that our models’ performance is only strengthened by the fact that
the underlying feature set used for prediction is inherently explainable to se-
curity operators. Consequently, we set out to build a framework for model
interpretability that can validate our assumptions about feature engineering ex
post facto as well as provide per-sample explanations of model predictions which
end users can utilize as trail-heads for threat hunts. In general, model inter-
pretability aims at giving precise explanations on the outcome of the model
and speciﬁc features which inﬂuence the model output either positively or
negatively. We utilize the popular model explainability tool Shapley (SHAP)
[Lundberg and Lee, 2017] for the detailed analysis of model outputs, which as-
signs SHAP values to features in order to accurately explain feature importance.
It’s novel components include:

• The identiﬁcation of a new class of additive feature importance measures.

• Theoretical results showing there is a solution in this class with a set of

desirable properties.

SHAP values provide per-feature global and local interpretability and serve

as the basis for our LM model interpretability.

12

Figure 4: Model Local Interpertability

6.1 Local Interpretability

SHAP provides a deﬁned value (Shapley value) for every feature employed while
creating the model, which explains the positive or negative eﬀect on the model
output of malicious or non-malicious events and is only provided when a model
prediction is malicious. This design choice was taken since benign activity would
never be reported to security operators. The local interpretability as seen in
our models allows threat hunters to understand why model determinations were
made on a per sample basis as shown in Fig. 4, in context to domain-speciﬁc
features they can understand. Fig. 4 can be understood as follows :

• f(x): The output value is the prediction for that observation (Conﬁdence

score/Probability).

• The base value: The value that would be predicted if no features for
the current output were known. i.e., it is the mean prediction conﬁdence
on the test dataset.

• Directed arrow bars: Features that push the prediction probability
higher (to the right) are represented by arrow bars pointing to the right,
and those pushing the prediction lower are shown with arrow bars pointing
left.

Our hope is that these per-sample explanations of feature importance can help
end users contextualize their threat investigations and ultimately lead to dra-
matically reduced time to detection as well as response.

6.2 Global Interpretability

Global interpretability describes the independent feature importance that inﬂu-
ence our models across all training samples. This enables us to evaluate how
eﬀective our domain-speciﬁc features are at identifying LM .Furthermore, ex-
plore feature importance to determine if they correlate with the signiﬁcance a
security operator would assign them during threat hunts. Fig. 5 provides an
overview of the aggregate global feature importance as computed by our models.
As shown in Fig. 5, all of the domain-speciﬁc features proved to be im-
portant in providing discriminatory power to our models during prediction.
Furthermore, review from our organization’s cyber SME team conﬁrmed that
the feature importance learned by our models are in line with importance they
would have assigned to the features during a threat hunt, which is a promising
indicator of model performance and interpretability.

13

Figure 5: Model Global Interpretability

7 Conclusion and Future Direction

We propose implementing a novel method for LM detection using user-based
behavioral analysis and domain-speciﬁc feature engineering using authentica-
tion and process logs. Our feature engineering approach was also bench-marked
using two supervised ML approaches to conﬁrm their eﬃcacy, and it is evident
that the features described provide stronger performance than other approaches
seen in literature despite maintaining realistic, high class-imbalance while sup-
porting near real-time, versatile detection capabilities. We provide a robust,
computationally eﬃcient methodology for LM detection with high recall and
very low FPR. We have further demonstrated that utilizing both deep learning
and tree-based approaches in a security workﬂow provides the added beneﬁt of
allowing threat hunters and SOC analysts to utilize diﬀerent models for diﬀerent
priorities. This approach also provides the added beneﬁt of being explainable
to end users because of the security-ﬁrst of the underlying features themselves.
We utilize this aspect of our data and models to build a robust interpretability
framework for LM detection that conﬁrms our feature engineering assumptions
and provides a per-sample explanation of results to end users for threat hunting.
Although our LM detectors maintain strong performance on imbalanced
LANL dataset, the dataset only captures malicious activity on a Windows based
system. Consequently, we would like to explore modeling approaches from var-
ious other operating systems such as Linux. We also plan to benchmark our
models on multiple enterprise networks to measure and discern the models’ abil-
ity to generalize to novel networks without retraining. Furthermore, we would
like to build and incorporate real-time end user feedback into our model hy-
perparameter optimization to allow them to adapt to a continually changing
enterprise environment.

References

[Bai et al., 2021] Bai, T., Bian, H., Salahuddin, M. A., Abou Daya, A., Limam,
N., and Boutaba, R. (2021). Rdp-based lateral movement detection using
machine learning. Computer Communications, 165:9–19.

[Bohara et al., 2017] Bohara, A., Noureddine, M. A., Fawaz, A., and Sanders,
W. H. (2017). An unsupervised multi-detector approach for identifying ma-
licious lateral movement.
In 2017 IEEE 36th Symposium on Reliable Dis-
tributed Systems (SRDS), pages 224–233.

14

[Boser et al., 1992] Boser, B. E., Guyon, I. M., and Vapnik, V. N. (1992).
A training algorithm for optimal margin classiﬁers.
In Proceedings of the
Fifth Annual Workshop on Computational Learning Theory, COLT ’92, page
144–152, New York, NY, USA. Association for Computing Machinery.

[Bowman et al., 2020] Bowman, B., Laprade, C., Ji, Y., and Huang, H. H.
(2020). Detecting lateral movement in enterprise computer networks with
unsupervised graph AI. In 23rd International Symposium on Research in At-
tacks, Intrusions and Defenses (RAID 2020), pages 257–268, San Sebastian.
USENIX Association.

[Breiman, 2001] Breiman, L. (2001). Random forests. Machine learning,

45(1):5–32.

[Chen et al., 2018] Chen, M., Yao, Y., Liu,

Jiang, B., Su, L.,
J.,
A novel approach for identifying lateral move-
and Lu, Z. (2018).
ment attacks based on network embedding.
In 2018 IEEE Inter-
national Conference on Parallel Distributed Processing with Applica-
tions, Ubiquitous Computing Communications, Big Data Cloud Comput-
ing, Social Computing Networking, Sustainable Computing Communications
(ISPA/IUCC/BDCloud/SocialCom/SustainCom), pages 708–715.

[Chen and Guestrin, 2016] Chen, T. and Guestrin, C. (2016). Xgboost: A scal-
able tree boosting system.
In Proceedings of the 22nd ACM SIGKDD In-
ternational Conference on Knowledge Discovery and Data Mining, KDD ’16,
page 785–794, New York, NY, USA. Association for Computing Machinery.

[Crowdstrike, 2022] Crowdstrike
lateral movement?

is

(2022).

Lateral movement

explained:
https://www.crowdstrike.com/

What
cybersecurity-101/lateral-movement/. Accessed: 2022-08-13.

[Du et al., 2018] Du, M., Liu, N., Song, Q., and Hu, X. (2018). Towards expla-
nation of dnn-based prediction with guided feature inversion. In Proceedings
of the 24th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD ’18, page 1358–1367, New York, NY, USA. Associa-
tion for Computing Machinery.

[Friedman et al., 2000] Friedman, J., Hastie, T., and Tibshirani, R. (2000). Ad-
ditive logistic regression: a statistical view of boosting (With discussion and
a rejoinder by the authors). The Annals of Statistics, 28(2):337–407.

[Hartigan and Wong, 1979] Hartigan, J. A. and Wong, M. A. (1979). Algorithm
as 136: A k-means clustering algorithm. Journal of the Royal Statistical
Society. Series C (Applied Statistics), 28(1):100–108.

[Hecht-Nielsen, 1992] Hecht-Nielsen, R. (1992). Theory of the backpropagation
neural network. In Neural Networks for Perception, pages 65–93. Academic
Press.

[Hosmer Jr et al., 2013] Hosmer Jr, D. W., Lemeshow, S., and Sturdivant, R. X.
(2013). Applied Logistic Regression. Wiley Series in Probability and Statistics.
Wiley.

15

[Ke et al., 2017] Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W.,
Ye, Q., and Liu, T.-Y. (2017). Lightgbm: A highly eﬃcient gradient boost-
ing decision tree.
In Advances in Neural Information Processing Systems,
volume 30, pages 3146–3154.

[Kent, 2015] Kent, A. D. (2015). Comprehensive, Multi-Source Cyber-Security

Events. Los Alamos National Laboratory.

[Kent, 2016] Kent, A. D. (2016). Cyber security data sources for dynamic net-

work research, volume 1, chapter 2, pages 37–65. World Scientiﬁc.

[Lundberg and Lee, 2017] Lundberg, S. M. and Lee, S. (2017). A uniﬁed ap-
proach to interpreting model predictions. In Advances in Neural Information
Processing Systems, pages 4765–4774.

[McInnes et al., 2018] McInnes, L., Healy, J., and Melville, J. (2018). Umap:
Uniform manifold approximation and projection for dimension reduction.

[Mossburg et al., 2016] Mossburg, E., Gelinne,

J.,
Beneath the surface of a cyberattack.

(2016).
deloitte.com/content/dam/Deloitte/us/Documents/risk/
us-risk-beneath-the-surface-of-a-cyber-attack.pdf.
cessed: 2022-08-13.

and Calzada, H.
https://www2.

Ac-

[Rocklin, 2015] Rocklin, M. (2015). Dask: Parallel computation with blocked
algorithms and task scheduling. In Proceedings of the 14th Python in Science
Conference, pages 126 – 132.

[Short and Fukunaga, 1981] Short, R. and Fukunaga, K. (1981). The optimal
distance measure for nearest neighbor classiﬁcation. IEEE Transactions on
Information Theory, 27(5):622–627.

[Song and Lu, 2015] Song, Y. and Lu, Y. (2015). Decision tree methods: ap-
plications for classiﬁcation and prediction. Shanghai Archives of Psychiatry,
27(2):130–135.

[Uppstr¨omer and R˚aberg, 2019] Uppstr¨omer, V. and R˚aberg, H. (2019). De-
tecting lateral movement in microsoft active directory log ﬁles: A supervised
machine learning approach. Master’s thesis, Faculty of Computing, Blekinge
Institute of Technology, SE-371 79 Karlskrona, Sweden.

[Wold et al., 1987] Wold, S., Esbensen, K., and Geladi, P. (1987). Princi-
pal component analysis. Chemometrics and Intelligent Laboratory Systems,
2(1):37–52.

[Zhang and Ma, 2012] Zhang, C. and Ma, Y. (2012). Ensemble Machine Learn-

ing: Methods and Applications. Springer, New York, NY, USA.

16

