Optimal Thresholds for Anomaly-Based
Intrusion Detection in Dynamical Environments

Amin Ghafouri1, Waseem Abbas1, Aron Laszka2, Yevgeniy Vorobeychik1, and
Xenofon Koutsoukos1

1 Institute for Software Integrated Systems,
Vanderbilt University, USA
{firstname.lastname}@vanderbilt.edu
2 Department of Electrical Engineering and Computer Sciences,
University of California, Berkeley, USA
laszka@berkeley.edu

Abstract. In cyber-physical systems, malicious and resourceful attack-
ers could penetrate a system through cyber means and cause signiﬁcant
physical damage. Consequently, early detection of such attacks becomes
integral towards making these systems resilient to attacks. To achieve
this objective, intrusion detection systems (IDS) that are able to detect
malicious behavior early enough can be deployed. However, practical
IDS are imperfect and sometimes they may produce false alarms even
for normal system behavior. Since alarms need to be investigated for any
potential damage, a large number of false alarms may increase the op-
erational costs signiﬁcantly. Thus, IDS need to be conﬁgured properly,
as oversensitive IDS could detect attacks very early but at the cost of a
higher number of false alarms. Similarly, IDS with very low sensitivity
could reduce the false alarms while increasing the time to detect the at-
tacks. The conﬁguration of IDS to strike the right balance between time
to detecting attacks and the rate of false positives is a challenging task,
especially in dynamic environments, in which the damage caused by a
successful attack is time-varying.
In this paper, using a game-theoretic setup, we study the problem of
ﬁnding optimal detection thresholds for anomaly-based detectors imple-
mented in dynamical systems in the face of strategic attacks. We formu-
late the problem as an attacker-defender security game, and determine
thresholds for the detector to achieve an optimal trade-oﬀ between the
detection delay and the false positive rates. In this direction, we ﬁrst
provide an algorithm that computes an optimal ﬁxed threshold that re-
mains ﬁxed throughout. Second, we allow the detector’s threshold to
change with time to further minimize the defender’s loss, and we provide
a polynomial-time algorithm to compute time-varying thresholds, which
we call adaptive thresholds. Finally, we numerically evaluate our results
using a water-distribution network as a case study.

Keywords: cyber-physical systems, security, game theory, intrusion de-
tection system

7
1
0
2

b
e
F
8

]
T
G
.
s
c
[

2
v
7
0
7
6
0
.
6
0
6
1
:
v
i
X
r
a

 
 
 
 
 
 
2

1

Ghafouri et al.

Introduction

In recent years, we have seen an increasing trend of malicious intruders and
attackers penetrating into various cyber-physical systems (CPS) through cyber
means and causing severe physical damage. Examples of such incidents include
the infamous Stuxnet worm [13], cyber attack on German steel plant [17], and
Maroochy Shire water-services incident [1] to name a few. To maximize the
damage, attackers often aim to remain covert and avoid getting detected for
an extended duration of time. As a result, it becomes crucial for a defender to
design and place eﬃcient intrusion and attack detection mechanisms to mini-
mize the damage. While attackers may be able to hide the speciﬁc information
technology methods used to exploit and reprogram a CPS, they cannot hide
their ﬁnal intent: the need to cause an adverse eﬀect on the CPS by sending
malicious sensor or controller data that will not match the behavior expected
by an anomaly-based detection system [7]. Anomaly-based detection systems
incorporate knowledge of the physical system, in order to monitor the system
for suspicious activities and cyber-attacks. An important design consideration
in such detection systems is to carefully conﬁgure them in order to satisfy the
expected monitoring goals.

A well-known method for anomaly-based detection is sequential change de-
tection [11]. This method assumes a sequence of measurements that starts under
the normal hypothesis and then, at some point in time, it changes to the attack
hypothesis. Change detection algorithm attempts to detect this change as soon
as possible. In a sequential change detection, there is a detection delay, that is, a
time diﬀerence between when an attack occurs and when an alarm is raised. On
the other hand, detection algorithms may induce false positives, that is, alarms
raised for normal system behavior. In general, it is desirable to reduce detec-
tion delay as much as possible while maintaining an acceptable false positive
rate. Nevertheless, there exists a trade-oﬀ between the detection delay and the
rate of false positives, which can be controlled by changing the sensitivity of the
the detector. A typical way to control detector sensitivity is through a detec-
tion threshold: by decreasing (increasing) detection threshold, a defender can
decrease (increase) detection delay and increase (decrease) false positive rate.
Consequently, the detection threshold must be carefully selected, since a large
value may result in excessive losses due to high detection delays, while a small
value may result in wasting operational resources on investigating false alarms.
Finding an optimal threshold, that is, one that optimally balances the de-
tection delay-false positive trade-oﬀ, is a challenging problem [14]. However, it
becomes much more challenging when detectors are deployed in CPS with dy-
namic behavior, that is, when the expected damage incurred from undetected
cyber-attacks depends on the system state and time. As a result, an attack on a
CPS which is in a critical state is expected to cause more damage as compared to
an attack in a less critical state. For example, in water distribution networks and
electrical grids, disruptions at a high-demand time are more problematic than
disruptions at a low-demand time. Hence, defenders need to incorporate time-

Optimal Thresholds for Intrusion Detection in Dynamical Environments

3

dependent information in computing optimal detection thresholds when facing
strategic attackers.

We study the problem of ﬁnding optimal detection thresholds for anomaly-
based detectors implemented in dynamical systems in the face of strategic at-
tacks. We model rational attacks against a system that is equipped with a detec-
tor as a two-player game between a defender and an attacker. We assume that
an attacker can attack a system at any time. Considering that the damage is
time-dependent, the attacker’s objective is to choose the optimal time to launch
an attack to maximize the damage incurred. On the other hand, the defender’s
objective is to select the detection thresholds to detect an attack with minimum
delay while maintaining an acceptable rate of false positives. To this end, ﬁrst
we present an algorithm that selects an optimal threshold for the detector that is
independent of time (i.e., ﬁxed ). We call it as a ﬁxed threshold strategy. Next, we
allow the defender to select a time-varying threshold while associating a cost with
the threshold change. For this purpose, we present a polynomial time algorithm
that computes thresholds that may depend on time. We call this approach the
adaptive threshold strategy. We present a detailed analysis of the computational
complexity and performance of both the ﬁxed and adaptive threshold strate-
gies. Finally, we evaluate our results using a water distribution system as a case
study. Since expected damage to the system by an attack is time-dependent, the
adaptive threshold strategy achieves a better overall detection delay-false posi-
tive trade-oﬀ, and consequently minimize the defender’s losses. Our simulations
indicate that this is indeed the case, and adaptive thresholds outperform the
ﬁxed threshold.

The remainder of this paper is organized as follows. In Section 2, we intro-
duce our system model. In Section 3, we present our game-theoretic model and
deﬁne optimal ﬁxed and adaptive detection thresholds. In Section 4, we ana-
lyze both strategies and present algorithms to obtain optimal ﬁxed and adaptive
thresholds. In Section 5, we evaluate these algorithms using numerical example.
In Section 6, we discuss related work on detection threshold selection in the face
of strategic attacks. Finally, we oﬀer concluding remarks in Section 7.

2 System Model

In this section, we present the system model. For a list of symbols used in this
paper, see Table 1.

2.1 Attack Model

Let the system have a ﬁnite discrete time horizon of interest denoted by {1, ..., T }.
Adversaries may exploit threat channels by compromising the system through a
deception attack that starts at time ka and ends at ke, thus spanning over the
interval [ka, ke]. Deception attacks are the ones that result in loss of integrity of
sensor-control data, and their corresponding danger is especially profound due
to the tight coupling of physical and cyber components (see [5] for details). If

4

Ghafouri et al.

Table 1. List of Symbols

Symbol Description

D(k)
δ(η)
FP (η)
C

expected damage caused by an attack at timestep k
expected detection delay given detection threshold η
false positive rate given detection threshold is η
cost of false alarms

P(η, ka) attacker’s payoﬀ for threshold η and attack time ka
L(η, ka) defender’s loss for threshold η and attack time ka

Adaptive Threshold
P(η, ka) attacker’s payoﬀ for adaptive threshold η = {ηk} and attack time ka
L(η, ka) defender’s loss for adaptive threshold η = {ηk} and attack time ka

an attack remains undetected, it will enable the attacker to cause physical or
ﬁnancial damage. In order to represent the tight relation between the CPS’s
dynamic behavior and the expected loss incurred from undetected attacks, we
model the potential damage of an attack as a function of time.

Deﬁnition 1. (Expected Damage Function): Damage function of a CPS is a
function D : {1, ..., T } → R+, which represents the expected damage D(k) in-
curred to a system from an undetected attack at time k ∈ {1, ..., T }.

The deﬁnition above describes instant damage at a time k ∈ {1, ..., T }. Fol-
lowing this deﬁnition, expected total damage resulting from an attack that spans
over some interval is deﬁned as follows.

Deﬁnition 2. (Expected Total Damage): Expected total damage is denoted by
a function ¯D : {1, ..., T } × {1, ..., T } → R+, which represents the expected total
damage ¯D(ka, ke) incurred to a system from an undetected attack in a period
[ka, ke]. Formally,

¯D(ka, ke) =

ke(cid:88)

k=ka

D(k) .

(1)

2.2 Detector

We consider a defender whose objective is to protect the physical system, which
is equipped with a detector. The detector’s goal is to determine whether a se-
quence of received measurements generated through the system corresponds to
the normal behavior or an attack. To implement a detection algorithm, we utilize
a widely used method known as sequential change detection [11]. This method
assumes a sequence of measurements that starts under the normal hypothesis,
and then, at some point in time, changes to the attack hypothesis. Change de-
tection algorithm attempts to detect this change as soon as possible.

Optimal Thresholds for Intrusion Detection in Dynamical Environments

5

Example (CUSUM). The Cumulative sum (CUSUM) is a statistic used for
change detection. The nonparametric CUSUM statistic S(k) is described by

S(k) = (S(k − 1) + z(k))+,

where S(0) = 0, (a)+ = a if a ≥ 0 and zero otherwise, and z(k) is generated by
an observer, such that under normal behavior it has expected value of less than
zero [7]. Assigning η as the detection threshold chosen based on a desired false
alarm rate, the corresponding decision rule is deﬁned as

d(S(k)) =

(cid:26) Attack if S(k) > η
Normal otherwise

Detection Delay and False Positive Rate. In detectors implementing change
detection, there might be a detection delay, which is the time taken by the de-
tector to raise an alarm since the occurrence of an attack.3 Further, there might
be a false positive, which means raising an alarm when the system exhibits nor-
mal behavior. In general, it is desirable to reduce detection delay as much as
possible while maintaining an acceptable false positive rate. But, there exists a
trade-oﬀ between the detection delay and the rate of false positives, which can
be controlled by changing the detection threshold. In particular, by decreasing
(increasing) the detection threshold, a defender can decrease (increase) detection
delay and increase (decrease) false positive rate. Finding the optimal trade-oﬀ
point and its corresponding optimal threshold is known to be an important prob-
lem [14], however, it is much more important in CPS, since expected damage
incurred from undetected attack directly depends on detector’s performance.

We represent detection delay by the continuous function δ : R+ → N ∪ {0},
where δ(η) is the detection delay (in timesteps) when threshold is η. Further, we
denote the attainable false positive rate by the continuous function FP : R+ →
[0, 1], where FP (η) is the false positive rate when the detection threshold is η.
We assume that δ is increasing and FP is decreasing, which is true for most
typical detectors including the CUSUM detector.

3 Problem Statement

In this section, we present the optimal threshold selection problem. We con-
sider two cases: 1) Fixed threshold, in which the defender selects an optimal
threshold and then keeps it ﬁxed; and 2) Adaptive threshold, in which the de-
fender changes detection threshold based on time. We model this problems as
conﬂicts between a defender and an attacker, which are formulated as two-player
Stackelberg security games.

3 Note that any desired deﬁnition of detection delay may be considered, for example,

stationary average delay [21,22].

6

Ghafouri et al.

3.1 Fixed Threshold

Strategic Choices. The defender’s strategic choice is to select a detection
threshold η. The resulting detection delay and false positive rate are δ(η) and
FP (η), respectively. We consider the worst-case attacker that will not stop the
attack before detection in order to maximize the damage. Consequently, the
attacker’s strategic choice becomes to select a time ka to start the attack. Note
that we consider damage from only undetected attacks since the mitigation of
non-stealthy attacks is independent of detector.

Defender’s Loss and Attacker’s Payoﬀ. As an alarm is raised, the defender
needs to investigate the system to determine whether an attack has actually
happened, which will cost him C. When the defender selects threshold η and
the attacker starts its attack at a timestep ka, the defender’s loss (i.e., inverse
payoﬀ) is

L(η, ka) = C · FP (η) · T +

ka+δ(η)
(cid:88)

k=ka

D(k) ,

(2)

that is, the amount of resources wasted on manually investigating false positives
and the expected amount of damage caused by undetected attacks.

For the strategies (η, ka), the attacker’s payoﬀ is

P(η, ka) =

ka+δ(η)
(cid:88)

k=ka

D(k) .

(3)

that is, the total damage incurred to the system prior to the expected detection
time. The idea behind this payoﬀ function is the assumption of a worst-case
attacker that has the goal of maximizing the damage.

Best-Response Attack and Optimal Fixed Threshold. We assume that
the attacker knows the system model and defender’s strategy, and can thus
compute the detection threshold chosen by the defender. Hence, the attacker will
play a best-response attack to the defender’s strategy, which is deﬁned below.

Deﬁnition 3. (Best-Response Attack): Taking the defender’s strategy as given,
the attacker’s strategy is a best-response if it maximizes the attacker’s payoﬀ.
Formally, an attack starting at ka is a best-response attack given a defense strat-
egy η, if it maximizes P(η, ka).

Further, the defender must choose his strategy expecting that the attacker
will play a best-response. We formulate the defender’s optimal strategy as strong
Stackelberg equilibrium (SSE) [12], which is commonly used in the security lit-
erature for solving Stackelberg games.

Optimal Thresholds for Intrusion Detection in Dynamical Environments

7

Deﬁnition 4. (Optimal Fixed Threshold): We call a defense strategy optimal
if it minimizes the defender’s loss given that the attacker always plays a best-
response. Formally, an optimal defense is

arg min
η,
ka∈bestResponses(η)

L(η, ka),

(4)

where bestResponses(η) is the set of best-response attacks against η.

3.2 Adaptive Threshold

Although the optimal ﬁxed threshold minimizes the defender’s loss considering
attacks at critical periods (i.e., periods with high damage), it imposes a high
false alarm rate at less critical periods. Adaptive threshold strategies directly
address this issue. The idea of adaptive threshold is to reduce the detector’s
sensitivity during less critical periods (via increasing the threshold), and increase
the sensitivity during more critical periods (via decreasing the threshold). As it
will be shown, this signiﬁcantly decreases the loss corresponding to false alarms.
However, the defender may not want to continuously change the threshold, since
a threshold change requires a reconﬁguration of the detector that has a cost.
Hence, the rational defender needs to ﬁnd an optimal adaptive threshold, which
is a balance between continuously changing the threshold and keeping it ﬁxed.
k=1. The number of threshold
changes is described by N = |S|, where S = {k | ηk (cid:54)= ηk+1, k ∈ {1, ..., T − 1}}. If
the system is under an undetected attack, the detection delay for each timestep
k is the delay corresponding to its threshold, i.e., δ(ηk). We deﬁne detection time
of an attack ka as the time index at which the attack is ﬁrst detected. It is given
by

The adaptive threshold is deﬁned by η = {ηk}T

σ(η, ka) = {min k | δ(ηk) ≤ k − ka} .

(5)

Note that the equation above represents the time index at which the attack is
ﬁrst detected, and not the detection delay. The detection delay for an attack ka
can be obtained by δ(η, ka) := σ(η, ka) − ka.

Strategic Choices. The defender’s strategic choice is to select the threshold
for each time index, given by η = {η1, η2, ..., ηT }. We call η to be the set of
adaptive threshold. Since we consider a worst-case attacker that will not stop
the attack before detection, the attacker’s strategic choice is to select a time ka
to start the attack.

Defender’s Loss and Attacker’s Payoﬀ Let Cd be the cost associated with
each threshold change. When the defender selects adaptive threshold η, and the
attacker starts its attack at a timestep ka, the defender’s loss is

L(η, ka) = N · Cd +

T
(cid:88)

k=1

C · FP (ηk) +

σ(η,ka)
(cid:88)

k=ka

D(k) ,

(6)

8

Ghafouri et al.

that is, the amount of resources spent on changing the threshold, operational
costs of manually investigating false alarms, and the expected amount of damage
caused by undetected attacks.

For the strategies (η, ka), the attacker’s payoﬀ is the total damage prior to

the expected detection time,

P(η, ka) =

σ(η,ka)
(cid:88)

k=ka

D(k) .

(7)

Best-Response Attack and Optimal Adaptive Threshold. The deﬁni-
tions presented in this part are analogous to the ones discussed above for the
case of optimal ﬁxed threshold. We assume the attacker can compute the adap-
tive threshold, and will play a best-response to the defender’s strategy, as deﬁned
below.

Deﬁnition 5. (Best-Response Attack): Taking the defender’s strategy as given,
the attacker’s strategy is a best-response if it maximizes the attacker’s payoﬀ.
Formally, an attack ka is a best-response given a defense strategy η, if it maxi-
mizes P(η, ka) as deﬁned in (7).

Further, the defender must choose its strategy expecting that the attacker

will play a best-response.

Deﬁnition 6. (Optimal Adaptive Threshold): We call a defense strategy opti-
mal if it minimizes the defender’s loss given that the attacker always plays a
best-response with tie-breaking in favor of the defender. Formally, an optimal
defense is

arg min
η,
ka∈bestResponses(η)

L(η, ka),

(8)

where bestResponses(η) is the best-response attack against η.

4 Selection of Optimal Thresholds

In this section, we present polynomial-time algorithms to compute optimal thresh-
olds, both for the ﬁxed and adaptive cases.

4.1 Fixed Threshold

To compute an optimal ﬁxed threshold, we present Algorithm 1. Here, we con-
sider that any detection delay can be achieved by selecting a speciﬁc threshold
value. Therefore, the algorithm ﬁnds an optimal detection delay, from which the
optimal threshold value can be selected. To ﬁnd the optimal detection delay, the
algorithm iterates through all possible values of detection delay and selects the
one that minimizes the defender’s loss considering a best-response attack. To
ﬁnd a best-response attack ka, given a delay δ, the algorithm iterates through
all possible values of ka, and selects the one that maximizes the payoﬀ.

Optimal Thresholds for Intrusion Detection in Dynamical Environments

9

ka

D(k)

ka ← 1, P (cid:48) ← 0
while ka < T do

P (δ, ka) ← (cid:80)ka+δ
if P (δ, ka) > P (cid:48) then
P (cid:48) ← P (δ, ka)
L(cid:48) ← P (cid:48) + C · FP (δ) · T

Algorithm 1 Algorithm for Optimal Fixed Threshold
1: Input D(k), T , C
2: Initialize: δ ← 0, L∗ ← ∞
3: while δ < T do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end while
19: return δ∗

end while
if L(cid:48) < L∗ then
L∗ ← L(cid:48)
δ∗ ← δ

end if
ka ← ka + 1

end if
δ ← δ + 1

Proposition 1. Algorithm 1 computes an optimal ﬁxed threshold in O(T 2) steps.

Proof. The obtained threshold is optimal since the algorithm evaluates all pos-
sible solutions through exhaustive search. Given a pair (δ, ka), when computing
the attacker’s payoﬀ P (δ, ka) in Line 6, we use the payoﬀ computed in previous
iteration, and write P (δ, ka) = P (δ, ka − 1) + D(ka − 1) + D(ka + δ), which takes
constant time. Therefore, the running time of the algorithm is subquadratic in
(cid:117)(cid:116)
the total number of timesteps T .

4.2 Adaptive Threshold

We present Algorithm 2 for ﬁnding optimal adaptive thresholds for any instance
of the attacker-defender game, which is based on the SSE. The approach com-
prises 1) a dynamic-programming algorithm for ﬁnding minimum-cost thresholds
subject to the constraint that the damage caused by a worst-case attack is at
most a given value and 2) an exhaustive search, which ﬁnds an optimal damage
value and thereby optimal thresholds. For ease of presentation, we use detection
delays δk and the corresponding maximal thresholds ηk interchangeably (e.g., we
let FP (δk) denote the false-positive rate of the maximal threshold that results
in detection delay δk), and we let ∆ denote the set of all attainable detection
delay values.

Theorem 1. Algorithm 2 computes an optimal adaptive threshold.

Proof (Sketch.). First, we prove that our dynamic-programming algorithm, called
MinimumCostThresholds in Algorithm 2, ﬁnds minimum-cost thresholds

10

Ghafouri et al.

Algorithm 2 Algorithm for Optimal Adaptive Thresholds
1: Input D(k), T , C

(cid:110)(cid:80)ke

k=ka

D(k)

T C(P ), δ∗

2: SearchSpace ←
3: for all P ∈ SearchSpace do
4:
5: end for
6: P ∗ ← arg minP ∈SearchSpace T C(P )
1 (P ∗), . . . , δ∗
7: return δ∗

1 (P ), . . . , δ∗

T (P ∗)

(cid:12)
(cid:12)
(cid:12) ka ∈ {1, . . . , T − 1}, ke ∈ {n + 1, . . . , T }

(cid:111)

T (P ) ← MinimumCostThresholds(P )

S(δn) ← Cost(n + 1, m + 1, δn) + C · FP (δn)

k=n−m D(k) ≤ P then

S(δn) ← CostP (n + 1, δn, δn) + C · FP (δn)

∀ m ∈ {0, . . . , T − 1}, δ ∈ ∆ : Cost(T + 1, m, δ) ← 0
for n = T, . . . , 1 do

else

S(δn) ← ∞

else if (cid:80)n

for all m ∈ {0, . . . n − 1} do
for all δn−1 ∈ ∆ do
for all δn ∈ ∆ do
if δn > m then

8: function MinimumCostThresholds(P )
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36: end function

end for
m ← 0, δ∗
0 ← arbitrary
for all n = 1, . . . T do
δ∗
n ← δ∗(n, m, δ∗
n−1)
m ← min{m + 1, δ∗
n}

end for
return Cost(1, 0, arbitrary), δ∗

end if
if δn−1 (cid:54)= δn ∧ n > 1 then
S(δn) ← S(δn) + Cd

1 , . . . , δ∗
T

end for

end for

end for
δ∗(n, m, δn−1) ← arg minδn S(δn)
Cost(n, m, δn−1) ← minδn S(δn)

end if

subject to any damage constraint P . Then, we show that our exhaustive search
ﬁnds an optimal damage constraint P , which minimizes the defender’s loss given
that the attacker plays a best response.

1) Minimum-Cost Thresholds In the ﬁrst part, we assume that we are given
a damage value P , and we have to ﬁnd thresholds that minimize the total cost of
false positives and threshold changes, subject to the constraint that any attack
against these thresholds will result in at most P damage. In order to solve this

Optimal Thresholds for Intrusion Detection in Dynamical Environments

11

problem, we use a dynamic-programming algorithm. We will ﬁrst discuss the
algorithm without a cost for changing thresholds, and then show how to extend
it to consider costly threshold changes.

For any two variables n and m such that n ∈ {1, . . . , T } and 0 ≤ m < n, we
deﬁne Cost(n, m) to be the minimum cost of false positives from n to T subject
to the damage constraint P , given that we only have to consider attacks that
start at ka ∈ {n−m, . . . , T } and that attacks are not detected prior to n. If there
are no thresholds that satisfy the damage constraint P under these conditions,
we let Cost(n, m) be ∞.4

We can recursively compute Cost(n, m) as follows. For any n < T and m,
iterate over all possible detection delay values δn, and choose the one that results
in the lowest cost Cost(n, m). If δn > m, then no attack could be detected at
timestep n, and Cost(n, m) would be the cost at timestep n plus the minimum
cost for timesteps {n+1, . . . , T } given that attacks may start at {n−m, . . . , T } =
{(n+1)−(m+1), . . . , T }. On the other hand, if δn ≤ m, then some attacks could
be detected at timestep n, and the worst of these attacks would start at n − m.
Hence, if (cid:80)n
k=n−m D(k) ≤ P , then Cost(n, m) would be the cost at timestep
n plus the minimum cost for timesteps {n + 1, . . . , T } given that attacks may
start at {n + 1 − δn, . . . , T }. Otherwise, there would be an attack that could
cause more than P damage, so Cost(n, m) would be ∞ by deﬁnition since there
would be no feasible thresholds for the remaining timesteps. Formally, we let

Cost(n, m) = min
δn






Cost(n + 1, m + 1) + FP (δn),

if δn > m

Cost(n + 1, δn) + FP (δn),

else if

D(k) ≤ P

.

∞

k=n−m

otherwise

n
(cid:88)

(9)
Note that in the equation above, Cost(n, m) does not depend on δ1, . . . , δn−1,
it depends only on the feasible thresholds for the subsequent timesteps. There-
fore, starting from the last timestep T and iterating backwards, we are able to
compute Cost(n, m) for all timesteps n and all values m. Note that for n = T
and any δT , computing Cost(T, m) is straightforward: if (cid:80)T
T −m D(k) ≤ P , then
Cost(T, m) = FP (δT ); otherwise, Cost(T, m) = ∞.

Having found Cost(n, m) for all n and m, Cost(1, 0) is by deﬁnition the
minimum cost of false positives subject to the damage constraint P . The min-
imizing threshold values can be recovered by iterating forwards from n = 1 to
T and again using Equation (9). That is, for every n, we select the threshold
n that attains the minimum cost Cost(n, m),
corresponding to the delay value δ∗
where m can easily be computed from the preceding delay values δ∗

1, . . . , δ∗

n.5

4 Note that in practice, ∞ can be represented by a suﬃciently high natural number.
5 Note that in Algorithm 2, we store the minimizing values δ∗(n, m) for every n and
m when iterating backwards, thereby decreasing running time and simplifying the
presentation of our algorithm.

12

Ghafouri et al.

Costly Threshold Changes Now, we show how to extend the computation of
Cost to consider the cost Cd of changing the threshold. Let Cost(n, m, δn−1) be
the minimum cost for timesteps starting from n subject to the same constraints
as before but also given that the detection delay at timestep n − 1 is δn−1. Then,
Cost(n, m, δn−1) can be computed similarly to Cost(n, m): for any n < T ,
iterate over all possible detection delay values δn, and choose the one that results
in the lowest cost Cost(n, m, δn−1). If δn−1 = δn or n = 1, then the cost would be
computed the same way as in the previous case (i.e., similarly to Equation (9)).
Otherwise, the cost would have to also include the cost Cd of changing the
threshold. Consequently, similarly to Equation (9), we deﬁne

(cid:92)Cost(n, m, δn−1) =






Cost(n + 1, m + 1, δn) + FP (δn)

Cost(n + 1, δn, δn) + FP (δn)

∞

if δn > m
n
(cid:88)

if

k=n−m
otherwise

D(k) ≤ P

,

and then based on the value of δn−1, we can compute Cost(n, m, δn−1) as
(cid:40)(cid:92)Cost(n, m, δn−1)
(cid:92)Cost(n, m, δn−1) + Cd

if δn = δn−1 ∨ n = 1
otherwise

Cost(n, m, δn−1) = min
δn

(10)

.

(11)
Note that for n = 1, we do not add the cost Cd of changing the threshold.
Similarly to the previous case, Cost(1, 0, arbitrary) is the minimum cost subject
to the damage constraint P , and the minimizing thresholds can be recovered by
iterating forwards.

2) Optimal Damage Constraint For any damage value P , using the above
dynamic-programming algorithm, we can ﬁnd thresholds that minimize the total
cost T C(P ) of false positives and threshold changes subject to the constraint that
an attack can do at most P damage. Since the defender’s loss is the sum of its
total cost and the damage resulting from a best-response attack, we can ﬁnd
optimal adaptive thresholds by solving

min
P

T C(P ) + P

(12)

and computing the optimal thresholds η∗ for the minimizing P ∗ using our
dynamic-programming algorithm.

To show that this formulation does indeed solve the problem of ﬁnding op-
timal adaptive thresholds, we use indirect proof. For the sake of contradiction,
suppose that there exist thresholds η(cid:48) for which the defender’s loss L(cid:48) is lower
than the loss L∗ for the solution η∗ of the above formulation. Let P (cid:48) be the
damage resulting from the attacker’s best response against η(cid:48), and let T C (cid:48) be
the defender’s total cost for η(cid:48). Since the worst-case attack against η(cid:48) achieves
at most P (cid:48) damage, we have from the deﬁnition of T C(P ) that T C (cid:48) ≥ T C(P (cid:48)).

Optimal Thresholds for Intrusion Detection in Dynamical Environments

13

It also follows from the deﬁnition of T C(P ) that L∗ ≤ T C(P ∗) + P ∗. Combining
the above with our supposition L∗ > L(cid:48), we get

T C(P ∗) + P ∗ ≥ L∗ > L(cid:48) = T C (cid:48) + P (cid:48) ≥ T C(P (cid:48)) + P (cid:48).

However, this is a contradiction since P ∗ minimizes T C(P ) + P by deﬁnition.
Therefore, η∗ must be optimal.

It remains to show that Algorithm 2 ﬁnds an optimal damage value P ∗. To
this end, we show that P ∗ can be found in polynomial time using an exhaustive
search. Consider the set of damage values ¯D(ka, ke) from all possible attacks
ka ≤ ke, that is, the set

(cid:40) ke(cid:88)

k=ka

D(k)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ka ∈ {1, . . . , T }, ke ∈ {ka, . . . , T }

.

(cid:41)

Let the elements of this set be denoted by P1, P2, . . . in increasing order. It is
easy to see that for any i, the set of thresholds that satisfy the constraint is the
same for every P ∈ [Pi, Pi+1). Consequently, for any i, the cost T C(P ) is the
same for every P ∈ [Pi, Pi+1). Therefore, the optimal P ∗ must be a damage value
(cid:117)(cid:116)
Pi from the above set, which we can ﬁnd by simply iterating over the set.

Proposition 2. The running time of Algorithm 2 is O(T 4 · |∆|2).

Note that since possible detection delay values can be upper-bounded by T ,

the running time of Algorithm 2 is also O(T 6).

Proof. In the dynamic-programming algorithm, we ﬁrst compute Cost(n, m,
δn−1) for every n ∈ {1, . . . , T }, m ∈ {1, . . . , n − 1}, and δn−1 ∈ ∆, and each
computation takes O(|∆|) time. Then, we recover the optimal detection delay for
all timesteps {1, . . . , T }, and the computation for each timestep takes a constant
amount of time. Consequently, the running time of the dynamic-programming
algorithm is O(T 2 · |∆|2).

In the exhaustive search, we ﬁrst enumerate all possible damage values by
iterating over all possible attacks (ka, ke), where ka ∈ {1, . . . , T } and ke ∈
{ka, . . . , T }. Then, for each possible damage value, we execute the dynamic-
programming algorithm, which takes O(T 2 · |∆|2) time. Consequently, the run-
ning time of Algorithm 2 is O(T 4 · |∆|2).
(cid:117)(cid:116)

Finally, note that the running time of the algorithm can be substantially
reduced in practice by computing Cost in a lazy manner: starting from n = 1
and m = 0, compute and store the value of each Cost(n, m, δn−1) only when it
is referenced, and then reuse it when it is referenced again. Unfortunately, this
does not change the worst-case running time of the algorithm.

5 Numerical Results

In this section, we evaluate our approach numerically using an example. In par-
ticular, we consider the anomaly-based detection of deception attacks in water

14

Ghafouri et al.

distribution networks. In such networks, an adversary may compromise pressure
sensors deployed to monitor the leakages and bursts in water pipes. By com-
promising sensors, adversary may alter their true observations, which can then
result in physical damage and ﬁnancial losses. Next, we present the system model
and the simulations of our results.

System Model. Figure 1 presents hourly water demand for a water network
during a day [9]. Since demand is time-dependent, the expected physical dam-
age and ﬁnancial loss caused by an attack on sensors is also time-dependent.
That is, the expected disruptions at a high-demand time would be more prob-
lematic than the disruptions at a low-demand time. Therefore, for each timestep
k ∈ {1, ..., 24}, we can deﬁne the expected damage as D(k) = α·d(k) where d(k)
is the demand at time k, and α ∈ R+ is a ﬁxed value for scaling (for example,
water price rate). In our experiments, we let α = 2.

Fig. 1. Hourly water demand during a day [9].

To discover attacks, we use anomaly-based detection systems implementing
sequential change detection. Based on the results presented in [7], we derive
the attainable detection delays and false alarm rates for the detector as shown
in Figure 2. We observe that for the detection delay δ = 0, the false positive
rate is FP (δ) = 0.95, and for δ = 23, the false positive rate is FP (δ) = 0.02.
As expected, the detection delay is proportional to the threshold, and the false
positive rate is inversely proportional to the threshold [6].

Fixed Threshold. In the case of ﬁxed threshold, the objective is to select the
strategy that minimizes the defender’s loss (2) while assuming the attacker will
respond using a best-response attack. Letting C = 7 and using Algorithm 1,

Optimal Thresholds for Intrusion Detection in Dynamical Environments

15

Fig. 2. Trade-oﬀ between the detection delay and the false positive rate.

we obtain δ∗ = 5, and the optimal loss L∗ = 171.30. Figure 3 shows the best-
response attack corresponding to this threshold value. The best-response attack
starts at k∗
k=10 D(k) = 91. Note that if
the attacker starts the attack at any other timestep, the damage caused before
detection is less than P ∗.

a = 10 and attains the payoﬀ P ∗ = (cid:80)15

Fig. 3. Best-response attack corresponding to the optimal ﬁxed threshold δ∗ = 5.

0123456789101112131415161718192021222300.10.20.30.40.50.60.70.80.91Delay [δ]False Positive Rate [FP(δ)]05101520250510152025Timestep [k]Expected Damage [D(k)]  Normal BehaviorBest−Response Attack16

Ghafouri et al.

Next, letting C = 8, we obtain δ∗ = 6 as the optimal defense strategy, which
leads to the optimal loss L∗ = 181.86, and best-response attack k∗
a = 9, with the
payoﬀ P ∗ = 99. We observe that, as expected, the optimal delay is higher for
the case of false alarms with higher costs.

Adaptive Threshold. Using the same setting, we use Algorithm 2 to ﬁnd
an optimal adaptive threshold. We let C = 8 and Cd = 10. As shown in Fig-
ure 4, we obtain the optimal adaptive threshold δ(k) = 23 for k ∈ {1, .., 11},
δ(k) = 1 for {12, .., 15}, and δ(k) = 3 for {17, ..., 23}. The resulting optimal loss
is L∗ = 138.88 . Figure 4 shows the corresponding best-response attack, which
starts at ka = 13 and, attains the payoﬀ P ∗ = 39. This ﬁgure demonstrates
that the detection threshold decreases as the system experiences high-demand,
so that the attacks can be detected early enough. On the other hand, as the sys-
tem experiences low-demand, the threshold increases to have fewer false alarms.

Fig. 4. Best-response attack corresponding to the optimal adaptive threshold. The
yellow points indicate the times at which the threshold change occurs.

Comparison. Keeping C = 8 ﬁxed, Figure 5 shows the optimal loss as a func-
tion of cost of threshold change Cd. For small values of Cd, the optimal losses
obtained by the adaptive threshold strategy are signiﬁcantly lower than the loss
obtained by the ﬁxed threshold strategy. As the cost of threshold change Cd
increases, the solutions of adaptive and ﬁxed threshold problems become more
similar. In the current setting, the adaptive threshold solution converges to a
ﬁxed threshold when Cd ≥ 45.

05101520250510152025δ1=23δ2 = 1δ3 = 3Timestep [k]Expected Damage [D(k)]  Normal BehaviorBest−Response AttackOptimal Thresholds for Intrusion Detection in Dynamical Environments

17

Fig. 5. The defender’s loss as a function of cost of threshold change.

Furthermore, letting Cd = 8, Figure 6 shows optimal loss as a function of
cost of false positives for ﬁxed and adaptive threshold strategies. It can be seen
that in both cases, the optimal loss increases as the cost of false alarms increases.
However, in the case of adaptive threshold, the change in loss is relatively smaller
than the ﬁxed threshold.

Fig. 6. The defender’s loss as a function of cost of false alarms.

05101520253035404580100120140160180Cost of Threshold Change [Cd]Defender’s Loss [L]  Fixed ThresholdAdaptive Threshold68101214161820Cost of False Alarms [C]100120140160180200220Defender's Loss [L]Fixed ThresholdAdaptive Threshold18

Ghafouri et al.

6 Related Work

The problem of threshold selection for anomaly detection systems has been
widely studied in the literature. Nevertheless, prior work has not particularly
addressed the optimal threshold selection problem in the face of strategic at-
tacks when the damage corresponding to an attack depends on time-varying
properties of the underlying physical system.

Laszka et al. study the problem of ﬁnding detection thresholds for multi-
ple detectors while considering time-invariant damages [14]. They show that the
problem of ﬁnding optimal attacks and defenses is computationally expensive,
thereby, proposing polynomial-time heuristic algorithms for computing approx-
imately optimal strategies. Cardenas et al. study the use of physical models for
anomaly detection, and describe the trade-oﬀ between false alarm rates and the
delay for detecting attacks [7]. Pasqualetti et al. characterize detection limita-
tions for CPS and prove that an attack is undetectable if the measurements due
to the attack coincide with the measurements due to some nominal operating
condition [18].

Signaling games are also used to model intrusion detection [8,10]. Shen et
al. propose an intrusion detection game based on the signaling game in order to
select the optimal detection strategy that lowers resource consumption [20]. Fur-
ther, Alpcan and Basar study distributed intrusion detection as a game between
an IDS and an attacker, using a model that represents the ﬂow of information
from the attacker to the IDS through a network [3,4]. The authors investigate
the existence of a unique Nash equilibrium and best-response strategies.

This work is also related to the FlipIt literature [24,16,15]. FlipIt is an
attacker-defender game that studies the problem of stealthy takeover of con-
trol over a critical resource, in which the players receive beneﬁts proportional to
the total time that they control the resource. In [19], the authors present a frame-
work for the interaction between an attacker, defender, and a cloud-connected
device. They describe the interactions using a combination of the FlipIt game
and a signaling game.

In the detection theory literature, Tantawy presents a comprehensive discus-
sion on design concerns and diﬀerent optimality criteria used in model-based
detection problems [23]. Alippi et al. propose a model of adaptive change detec-
tion that can be conﬁgured at run-time [2]. This is followed by [25], in which
the authors present a procedure for obtaining adaptive thresholds in change
detection problems.

7 Concluding Remarks

In this paper, we studied the problem of ﬁnding optimal detection thresholds
for anomaly-based detectors implemented in dynamical systems in the face of
strategic attacks. We formulated the problem as an attacker-defender security
game that determines thresholds for the detector to achieve an optimal trade-oﬀ
between the detection delay and the false positive rates. To this end, ﬁrst we pre-
sented an algorithm that computes optimal ﬁxed threshold that is independent

Optimal Thresholds for Intrusion Detection in Dynamical Environments

19

of time. Next, we deﬁned adaptive threshold, in which the defender is allowed
to change the detector’s threshold with time. We provided a polynomial time
algorithm to compute optimal adaptive threshold. Finally, we evaluated our re-
sults using a case study. Our simulations indicated that the adaptive threshold
strategy achieves a better overall detection delay-false positive trade-oﬀ, and con-
sequently minimize the defender’s losses, especially when the damage incurred
by the successful attack varied with time.

In future work, we aim to extend this work by considering: 1) Multiple sys-
tems with diﬀerent time-varying damage for each subsystem; 2) Sequential hy-
pothesis testing detectors, in which there exits a trade-oﬀ between false alarm
rate, missed detection rate, and detection delay; and 3) Moving target defense
techniques based on randomized thresholds.

Acknowledgment

This work is supported in part by the the National Science Foundation (CNS-
1238959), Air Force Research Laboratory (FA 8750-14-2-0180), National Insti-
tute of Standards and Technology (70NANB15H263), Oﬃce of Naval Research
(N00014-15-1-2621), and by Army Research Oﬃce (W911NF-16-1-0069).

References

1. M. Abrams and J. Weiss. Malicious control system cyber security attack case study
– Maroochy Water Services, Australia.
http://csrc.nist.gov/groups/SMA/
fisma/ics/documents/Maroochy-Water-Services-Case-Study_report.pdf, Jul
2008.

2. C. Alippi and M. Roveri. An adaptive CUSUM-based test for signal change detec-
tion. In Proceedings of the 2006 IEEE International Symposium on Circuits and
Systems (ISCAS), pages 5752–5755. IEEE, 2006.

3. T. Alpcan and T. Basar. A game theoretic approach to decision and analysis
in network intrusion detection. In Proceedings of the 42nd IEEE Conference on
Decision and Control (CDC), volume 3, pages 2595–2600. IEEE, 2003.

4. T. Alpcan and T. Ba¸sar. A game theoretic analysis of intrusion detection in access
In Proceedings of the 43rd IEEE Conference on Decision and

control systems.
Control (CDC), volume 2, pages 1568–1573. IEEE, 2004.

5. S. Amin, G. A. Schwartz, and A. Hussain. In quest of benchmarking security risks

to cyber-physical systems. IEEE Network, 27(1):19–24, 2013.

6. M. Basseville, I. V. Nikiforov, et al. Detection of abrupt changes: Theory and

application, volume 104. Prentice Hall, Englewood Cliﬀs, 1993.

7. A. A. C´ardenas, S. Amin, Z.-S. Lin, Y.-L. Huang, C.-Y. Huang, and S. Sastry.
Attacks against process control systems: risk assessment, detection, and response.
In Proceedings of the 6th ACM Symposium on Information, Computer and Com-
munications Security (ASIACCS), pages 355–366. ACM, 2011.

8. W. Casey, J. A. Morales, T. Nguyen, J. Spring, R. Weaver, E. Wright, L. Met-
calf, and B. Mishra. Cyber security via signaling games: Toward a science of cyber
security. In International Conference on Distributed Computing and Internet Tech-
nology, pages 34–42. Springer, 2014.

20

Ghafouri et al.

9. B. Durin and J. Margeta. Analysis of the possible use of solar photovoltaic energy

in urban water supply systems. Water, 6(6):1546–1561, 2014.

10. M. Estiri and A. Khademzadeh. A theoretical signaling game model for intrusion
detection in wireless sensor networks.
In Proceedings of the 14th International
Telecommunications Network Strategy and Planning Symposium (NETWORKS),
pages 16. IEEE, 2010, pages 1–6. IEEE, 2010.

11. T. Kailath and H. V. Poor. Detection of stochastic processes. IEEE Transactions

on Information Theory, 44(6):2230–2231, 1998.

12. D. Korzhyk, Z. Yin, C. Kiekintveld, V. Conitzer, and M. Tambe. Stackelberg vs.
Nash in security games: An extended investigation of interchangeability, equiv-
alence, and uniqueness. Journal of Artiﬁcial Intelligence Research, 41:297–327,
2011.

13. D. Kushner. The real story of stuxnet. Spectrum, IEEE, 50(3):48–53, 2013.
14. A. Laszka, W. Abbas, S. S. Sastry, Y. Vorobeychik, and X. Koutsoukos. Opti-
mal thresholds for intrusion detection systems. In Proceedings of the 3rd Annual
Symposium and Bootcamp on the Science of Security (HotSoS), pages 72–81, 2016.
15. A. Laszka, G. Horvath, M. Felegyhazi, and L. Buttyan. FlipThem: Modeling tar-
geted attacks with FlipIt for multiple resources. In Proceedings of the 5th Con-
ference on Decision and Game Theory for Security (GameSec), pages 175–194,
November 2014.

16. A. Laszka, B. Johnson, and J. Grossklags. Mitigating covert compromises: A game-
theoretic model of targeted and non-targeted covert attacks. In Proceedings of the
9th Conference on Web and Internet Economics (WINE), pages 319–332, Decem-
ber 2013.

17. R. M. Lee, M. J. Assante, and T. Conway. German steel mill cyber attack. Tech-

nical report, SANS Industrial Control Systems, December 2014.

18. F. Pasqualetti, F. Dorﬂer, and F. Bullo. Attack detection and identiﬁcation in
cyber-physical systems. IEEE Transactions on Automatic Control, 58(11):2715–
2729, 2013.

19. J. Pawlick, S. Farhang, and Q. Zhu. Flip the cloud: Cyber-physical signaling
games in the presence of advanced persistent threats. In Proceedings of the 6th
International Conference on Decision and Game Theory for Security (GameSec),
pages 289–308. Springer, 2015.

20. S. Shen, Y. Li, H. Xu, and Q. Cao. Signaling game based strategy of intrusion
detection in wireless sensor networks. Computers & Mathematics with Applications,
62(6):2404–2416, 2011.

21. A. Shiryaev. The problem of the most rapid detection of a disturbance in a sta-

tionary process. Soviet Math. Dokl, 2(795-799), 1961.

22. M. Srivastava and Y. Wu. Comparison of ewma, cusum and shiryayev-roberts
procedures for detecting a shift in the mean. The Annals of Statistics, pages 645–
670, 1993.

23. A. M. Tantawy. Model-based Detection in Cyber-Physical Systems. PhD thesis,

Vanderbilt University, 2011.

24. M. Van Dijk, A. Juels, A. Oprea, and R. L. Rivest. Flipit: The game of stealthy

takeover. Journal of Cryptology, 26(4):655–713, 2013.

25. G. Verdier, N. Hilgert, and J.-P. Vila. Adaptive threshold computation for cusum-
type procedures in change detection and isolation problems. Computational Statis-
tics & Data Analysis, 52(9):4161–4174, 2008.

