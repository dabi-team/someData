Qualitative Planning in Imperfect Information Games with Active Sensing
and Reactive Sensor Attacks: Cost of Unawareness

Abhishek N. Kulkarni, Shuo Han, Nandi O. Leslie, Charles A. Kamhoua, and Jie Fu∗

1
2
0
2

y
a
M
2

]

R
C
.
s
c
[

2
v
6
7
1
0
0
.
4
0
1
2
:
v
i
X
r
a

Abstract— We consider the probabilistic planning problem
where the agent (called Player 1, or P1) can jointly plan the
control actions and sensor queries in a sensor network and
an attacker (called player 2, or P2) can carry out attacks on
the sensors. We model such an adversarial interaction using a
formal model–a reachability game with partially controllable
observation functions. The main contribution of this paper is
to assess the cost of P1’s unawareness: Suppose P1 misinter-
prets the sensor failures as probabilistic node failures due to
unreliable network communication, and P2 is aware of P1’s
misinterpretation in addition to her partial observability. Then,
from which states can P2 carry out sensor attacks to ensure,
with probability one, that P1 will not be able to complete
her reachability task even though, due to misinterpretation,
P1 believes that she can almost-surely achieve her task. We
develop an algorithm to solve the almost-sure winning sensor-
attack strategy given P1’s observation-based strategy. Our
attack analysis could be used for attack detection in wireless
communication networks and the design of provably secured
attack-aware sensor allocation in decision-theoretic models for
cyber-physical systems.

Index Terms— Cyber-physical security; Formal methods;

Games on graphs; Sensor Attack.

I. INTRODUCTION

Security of Cyber-Physical Systems (CPS) has been stud-
ied extensively in the systems and control community (see
a survey in [1]). In these control-theoretic approaches, the
system under consideration is modeled as a linear or nonlin-
ear dynamical system. The malicious attacker on sensors and
actuators can inject errors into the sensor measurement (such
as spooﬁng attacks or false data injection attacks), block or
delay signals (as in denial of services and jamming attacks)
[2], [3]. Detection and secured estimation and control under
malicious attacks have been investigated to ensure resilience,
stability, and robustness of the control system. Secured state
estimation under sensor and/or actuator attacks has been
studied for linear time-invariant systems [4]–[6].

While a plethora of work analyze CPS security from a
systems and control perspectives, discrete event systems and
stochastic games have also been employed to analyze secured
control under attacks. The authors in [7] model the attacker-
defender interaction as a turn-based stochastic Stackelberg

A. Kulkarni and J. Fu (∗ corresponding author) are with the Robotics
Engineering Program and Dept. of Electrical and Computer Engi-
neering, Worcester Polytechnic Institute, Worcester, MA 01609 USA.
{ankulkarni,jfu2}@wpi.edu

S. Han is with the Department of Electrical and Computer Engineering,

University of Illinois, Chicago, IL 60607. hanshuo@uic.edu

N.

Leslie

is

with

Raytheon

Technologies.

nandi.o.leslie@raytheon.com
is

C. Kamhoua

with U.S. Army

charles.a.kamhoua.civ@mail.mil

Research

Laboratory.

game, where the attacker observes the defender’s strategy
before deciding how to carry out attacks and the defender
is to maximize the probability of satisfying a temporal logic
formula. They assume that both players have complete obser-
vations and the game state is controlled by the joint attacker
and defender actions. In supervisory control, the supervisor
aims to control the behaviors of a DES to be within a
desired language while an attacker can add, delete, or replace
symbols (sensor inputs or actuator outputs). The authors in
[8] propose ﬁnite-state transducers as attack models, which
are known to the supervisor. A composition of the plant
and the attack transducers is generated to evaluate, whether
the supervisor can still control the plant’s outputs within the
desired language despite of the malicious attacks. In [9], the
authors do not assume that attacker’s policy is known but
instead introduce a class of multi-adversary games where the
supervisor is played against multiple possible attackers on
sensors. They proposed the condition for controllability and
observability under such attacks and the supervisory control
design to achieve the control objective robustly. Besides
mitigating sensor/actuator attacks, opacity in DESs [10]–[12]
is to mitigate attacks on conﬁdentiality of the systems.

Our work is motivated by the development of networked
robotic systems and the security issues. Networked robots
are a class of CPSs that focus on the seamless integration
of robots to assist humans in difﬁcult tasks such as security
paroling and contested search and rescue. For these appli-
cations, the robots operate in an uncertain and potentially
adversarial environment, subject to malicious attacks in cyber
and physical spaces. In this scope, we focus on sensor attack
in networked robots and employ decision-theoretic models
to nvestigate how the sensor attack can affects an intelligent
agent’s information states, beliefs, and consequentially its
decisions. Consider a robot is to achieve a task in a stochastic
environment with partial observations. At run-time, the robot
can actively query a subset of sensors in a wireless network
to update his belief about the state and the progress with
respect to the task. However, the attacker can choose unsafe
sensor nodes and carry out reactive jamming attacks [3].
Such an attack will target the active communication and
block the sensor information from reaching the agent. Our
key questions are, if the robot mistakes sensor failures as
probabilistic node failures in the wireless network [13], how
would the robot compute its active sensing and control policy
to reach a goal state with probability one, i.e.almost-surely
win? How shall the attacker exploit the robot’s misinter-
pretation of the sensor failures for carrying out successful
sensor attacks that manipulate the robot’s information states?

 
 
 
 
 
 
With the knowledge about attacker’s almost-sure winning
region given the naive robot, we can understand how the
physical task completion is coupled with the vulnerabilities
in the cyber network and gain critical insight about hard-
ening network security with task-oriented sensor allocation
and attack-aware active sensing strategies. Our technical
approach employs a formal model called stochastic game on
graphs [14]–[16]. The key contributions are the following:
1) We formulate a class of partially observable Markov
decision process (POMDP)s with reachability objectives. The
model generalize from regular POMDPs to include both
actions and sensor queries as the decisions for the robot.
2) We extend probabilistic model checking in POMDPs
to answer, from which set of information states, the robot
has an observation-based control and active sensing strategy
that ensures the task can be achieved with probability one
given probabilistic action outcomes and probabilistic sensor
failures. 3) We analyze the adversary’s game to compute,
given the naive robot’s strategy, when to attack which sensors
so that the robot cannot achieve the task with probability
one, even from the robot believes it can do so. The solution
approach is illustrated using a running example.

Our modeling and design approaches differ from existing
literature on secured control design in three key aspects: 1)
In our model, the system dynamics is probabilistic instead
of nondeterministic (as studied in DESs) or linear, time-
invariant (as studied in secured control in CPSs). 2) Our
objective is to synthesize the almost-sure winning strategy
for the robot and use it to assess the almost-sure winning
region for the attacker who exploits the robot’s unawareness
of attacks. This criteria of performance is different from
controllability and observability in DESs. Due to the model-
ing and objective differences, our solution approaches differ
greatly from that being used in DESs and linear systems. 3)
We do not assume the knowledge of attack strategies. Instead,
we analyze the game and compute the attack strategy given
the attacker’s information and assumption of the agent’s
perception. This formal synthesis provides security insights
considering the worst-case attacker strategy. 4) In relation
in CPSs,
to other game-theoretic based secured control
we focus on sensor attacks given partial observations and
active sensor queries, instead of actuator attacks with perfect
observations [7].

II. PROBLEM FORMULATION

We consider the probabilistic planning of an autonomous
agent (called Player 1, or P1, pronoun ‘she’) in an adversarial
environment, where an attacker (called player 2, or P2,
pronoun ‘he’) can carry out attacks on the sensors of P1.
The objective of P1 is to reach a set of goal states. Such
an interaction between P1 and P2 is captured using the
following model, where P1’s observation function is partially
controllable.

Deﬁnition 1 (Zero-sum Stochastic Reachability Game with
Partially Controllable Observation Function). A two-player
stochastic game with partially controllable observation func-

Algorithm 1 GETOBSERVATION
Inputs: s, σ, β
Outputs: V
1: V = S
2: for all i ∈ σ \ β do
if vi = ⊤ then
3:

4:

V ← V ∩ γ(i)

else

5:
6:
7: return V

V ← V ∩ (S \ γ(i))

tion in which P1 has a reachability objective is a tuple

G = hS, A, P, s0, Γ, Σ × A2, O, Obs, o0, F i

where

• hS, A, P, s0i is a Markov decision process (MDP) where
S is a ﬁnite set of states; A is a ﬁnite set of actions; s0
is an initial state; P : S × A × S → [0, 1] is a transition
probability function such that Ps′∈S P (s, a, s′) = 1 for
any state s ∈ S and action a ∈ A;

• Γ = {0, 1, . . . , N } is a set of indexed sensors. Sensor
i covers a subset of states. The function γ : Γ → 2S
maps a sensor to a set of states covered by that sensor.
• Σ ⊆ 2Γ is a set of sensor query actions of P1, each of
which requests a sensor reading from a unique subset
of sensors from Γ;

• A2 ⊆ 2Γ is a set of sensor attack actions of P2, each
of which jams a sensor reading of a subset of sensors
from Γ;

• O ⊆ 2S is a ﬁnite set of observations;
• Obs : S × Σ × A2 → O is a deterministic observation
function, which maps a state s ∈ S, a sensor query
action σ and a sensor attack action β into an obser-
vation o = Obs(s, σ, β) ∈ O. Two states s, s′ ∈ S
are said to be observation equivalent given the sensing
action and sensor attack action σ, β if Obs(s, σ, β) =
Obs(s′, σ, β). The observation o includes the set of
observation-equivalent states.

• o0 = {s0} ∈ O is an initial observation;
• F ⊆ S is a set of ﬁnal states that P1 must reach to

complete her task.

To simplify the exposition of concepts in this paper, we
consider the sensors in Γ to be Boolean sensors. That is,
each sensor i returns a Boolean value, denoted vi: vi = ⊤
if the states covered by the sensor contains the current state
of G and vi = ⊥ otherwise. Whenever a sensor is attacked,
it returns a special value vi =? denoting a sensor failure.
Given a set Γ of sensors, the set of observations O can be
constructed by using GETOBSERVATION in Alg. 1 for each
state s ∈ S, sensing action σ ∈ Σ and β ∈ A2.

It is noted that in the absence of sensing attacks, the game
is a POMDP with active sensing actions. With sensor attacks,
the observation of P1 is partially controllable.

Game Play. The game play in G is constructed as follows.
From the initial state s0, P1 obtains the initial observation

o0. Based on the observation, P1 selects a control action
a0 ∈ A and a sensor query action σ0 ∈ Σ. The system
moves to state s1 with probability P (s0, a0, s1). At state
s1, P2 selects an attack action β0 ∈ A2. The system
generates a new observation o1 = Obs(s1, σ0, β0). This
process repeats until P1 knows, with probability one, that
a state in F is reached. We denote the resulting play as
ρ = s0a0σ0β0s1a1σ1β1 . . .. The set of inﬁnite plays in G
is denoted by Plays(G) and the set of ﬁnite preﬁxes of
plays is denoted by Prefs(G). We say a state s ∈ S to be
reachable in G if there exists a preﬁx ρ ∈ Prefs(G) such
that s ∈ ρ. We denote the set of possible reachable states
given a state-action pair (s, a) by PostG(s, a). Formally,
PostG(s, a) = {s′ ∈ S | P (s, a, s′) > 0}. Abusing the
notation, we deﬁne PostG(B, a) = Ss∈B PostG(s, a) where
B ⊆ S is a subset of states.

Strategies. Given the fact that P1 must determine, simul-
taneously, an action a ∈ A and a sensor query action σ ∈ Σ,
we denote A1 = A×Σ as the action space of P1 and A2 = A2
as the action space of P2. A history-dependent, randomized
(resp., deterministic) strategy for player j ∈ {1, 2} is a
function πj : Prefs(G) → D(Aj ) (resp., πj : Prefs(G) →
Aj). We say that player j follows strategy πj if for any preﬁx
ρ ∈ Prefs(G) at which πj is deﬁned, player j takes the action
πj(ρ) if πj is deterministic, or an action a ∈ Supp(πj(ρ))
with probability πj(ρ, a) if πj is randomized. The set of all
strategies of player j is denoted by Πj.

A strategy π1 ∈ Π1 is said to be almost-sure winning
for P1 over a reachability objective to visit F if, for any
π2 ∈ Π2, P1 is guaranteed to visit F with probability 1.
Similarly, a strategy π2 is almost-sure winning for P2 against
P1 if, for any π1 ∈ Π1, P2 ensures with probability one that
no state in F can be visited. A state is called an almost-
sure winning state for a player, if there exists an almost-sure
winning strategy for the player at that state.

Observation Equivalence. Given the observation func-
tion Obs, an observation of a play ρ is deﬁned as
Obs(ρ) = o0(a0, σ0, β0)o1(a1, σ1, β1) . . . where oi+1 =
Obs(si+1, σi, βi) for all i ≥ 0 and o0 is the initial ob-
servation. Two plays (or play preﬁxes) ρ, ρ′ are said to be
observation-equivalent, denoted by ρ ∼ ρ′. A strategy is said
to be observation-based if πj(ρ) = πj(ρ′) whenever ρ ∼ ρ′.
We denote the set of all observation-based strategies of P1
by ΠO
1 .

Information Structure. In this paper, we consider a
game with one-sided partial observation, in which P1 has
partial and P2 has perfect observations. Thus, the adversarial
interaction in game G is characterized as follows. During her
turn, P1 uses the sensing action to reduce the uncertainty in
her belief about the current state. Whereas P2, who knows
the current state, uses attack actions to control how much
information P1 gains about the current state of the game. To
make the problem nontrivial, we also consider that the attack
actions can be limited.

We assume the information about the game is asymmetric
and incomplete for P1. Speciﬁcally, P1 considers the failures
of a subset of sensors β ∈ A2 to be probabilistic failures.

a1

s4

a1

s2

a3

a0

start

s1

a1

s0

a0

a2

a0

a1

a0, a1

s5

a0

a0

a1

s3

Fig. 1. A two-player stochastic game with partially controllable observation
function. The dashed region represent the sensors: 0 (red), 1 (blue), 2
(green), 3 (violet).

Only P2 has the correct, complete information of the game.
Hereafter, we refer to such P1 as a na¨ıve P1. Our main goal is
to understand the cost of such unawareness of sensor attacks:
whether it is possible that P2 can win from a state that P1
believes to be almost-surely winning for her? We formalize
our problem as follows.

Problem 1. Given the stochastic game G with partially
controllable observation function in which P1 has partial
and P2 has perfect observability, determine when (for which
preﬁx in Prefs) there exists an observation-based strategy
π1 ∈ ΠO
1 using which the na¨ıve P1 believes that she can
satisfy a reachability objective over F with probability one,
no matter which strategy P2 plays. Also, determine when
there exists a P2’s best attack strategy that prevents the na¨ıve
P1 from satisfying her reachability objective.

III. MAIN RESULT

In this section, we present a qualitative analysis of game
G,
in which we show how to compute the almost-sure
winning strategies of na¨ıve P1 who has partial observability.
We start by introducing a running example to illustrate the
advantage of sensing actions and the effect of sensor attacks.

Example 1 (Part I). Fig. 1 represents a two-player stochastic
game with partially controllable observation function in
which s0 is the initial state and s4 is the ﬁnal state. The set
Γ consists of 4 sensors 0 (red), 1 (blue), 2 (green), 3 (violet)
covering states {s0, s1}, {s1, s2}, {s0, s2, s3} and {s4, s5},
respectively. P1 has three control actions, A = {a0, a1, a2}
and three sensing actions σ0, σ1 and σ2 which query the
sensors {0, 1}, {0, 2} and {2}, respectively. P2 has three
attack actions, β0, β1 and β2 that jam the sensors 0, 1 and
2, respectively. The initial observation o0 = {s0}. Given our
focus is on qualitative analysis, the probabilistic transitions
are labeled with actions only. For instance, at state s0, the
action a0 can be taken and reach any of the states s1, s2,
and s0 with a positive probability. The edges from s0 labeled
a0 are the probabilistic outcomes given that action.

To illustrate the advantage of sensing actions, we ﬁrst
analyze the game in Fig. 1 when P1 has no sensing actions
and P2 has no attack actions, P1 has no almost-sure winning
strategy at s0. This is because choosing the action a2 at s0
is unsafe as it may lead the game to the losing state s5 with
positive probability. And, by choosing a0 or a1, P1’s new
belief state would be B = {s0, s1, s2}, at which she does
not have a consistent winning action, i.e. if the true game
state is s1 then P1 should choose action a0 and consider
action a1 to be unsafe. However, if the true game state is s2
then action a0 is unsafe. Not knowing which state is the true
state, P1 cannot select any action without risking running
into the sink state s5.

On the contrary, when P1 can query sensors but P2 cannot
attack any of them (the game is still a POMDP), she has an
almost-sure winning strategy at s0 by selecting, say, a0 as
control action and σ0 as sensing action. This is because the
sensing action σ0 allows P1 to determine the resulting state
after choosing the action a0.

Next, we analyze how P1 can synthesize her almost-sure
winning active sensing and control strategies when she thinks
the sensors may have probabilistic failures.

A. The Game Perceived by P1

We are interested to synthesize P1’s active sensing and
control strategies to reach F with probability one, when she
mistakes sensor failures as probabilistic node failures. Due
to P1’s incorrect interpretation of sensor failures, from P1’s
perspective, G is a POMDP with active sensing actions.

Deﬁnition 2. The game G as perceived by na¨ıve P1 is the
tuple,

G1 = hS, A1 = A × Σ, P, O, Obs1, o0, F i,

where P is the same probabilistic transition function as in
game G. The probabilistic observation function Obs1(s, σ, o)
is deﬁned such that, given a state s ∈ S and P1’s sensing
action σ ∈ Σ, the probability of obtaining an observation
o ∈ O is strictly positive, if there exists an attack action
β ∈ A2 enabled at the state s, such that Obs(s, σ, β) = o.

To derive an almost-sure winning strategy for P1 in G1,
we construct an MDP with perfect observation, in which
the belief of P1 about the current state is made explicit. Our
construction is adopted from reachability analysis in POMDP
[17]. Also, for qualitaitive planning, we only need to know
the support of a distribution for the next state given a state-
action pair, but need not to know the exact distribution.
Deﬁnition 3. Given G1, the perfect-observation MDP of P1
is a tuple,

H = hQ ∪ {qF }, A1, δ, q0, qF i,

where

• Q = {(s, B) | ∃o ∈ O s.t. B ⊆ o and s ∈ B} is the

set of states;

• qF is a single ﬁnal state. It is also a sink state.
• q0 = (s0, {s0}) is the initial state;

(s1, {s1, s2})

(a1, ·), ·

(s5, {s4, s5})

(a0, σ0), β0

start

(s0, {s0})

(a0, σ0), β0

(a0, σ0), β0

(s2, {s1, s2})

(a 0,·),·

(

a

0,·),·

(a1, ·), ·

⊤⊤

qF

Fig. 2. A subset of perfect-observation MDP constructed by P1 in her mind.
The ﬁgure shows the relevant states and edges when P1 chooses (a0, σ0)
at the initial state (s0, {s0}) and sensor 0 fails.

• Given a state (s, B) and action (a, σ), the transition

function δ is deﬁned by,
– If

PostG(s, a) ∩ F

then
δ((s, B), (a, σ), (s′, B′)) > 0 if and only if
s′ ∈ PostG(s, a) and there exists an o ∈ O with
Obs1(s′, σ, o) > 0 such that B′ = PostG(B, a) ∩ o;

=

∅

– If PostG(s, a) ⊆ F then δ((s, B), (a, σ), qF ) = 1;
– If PostG(s, a) ∩ F

6= ∅ and PostG(s, a) \
then δ((s, B), (a, σ), qF ) > 0 and
F 6= ∅,
δ((s, B), (a, σ), (s′, B′)) > 0 for each s′ ∈
PostG(s, a) such that there exists an o ∈ O with
Obs1(s′, σ, o) > 0 such that B′ = PostG(B, a) ∩ o.

The transition function can be understood as follows.
Given a state (s, B) and action (a, σ), if PostG(s, a) ⊆ F ,
then the game reaches the ﬁnal state qF with probability
one. If none of the states s′ ∈ PostG(s, a) is a ﬁnal state,
then with a sensing action σ and an observation o, the game
reaches each state (s′, B′) with a positive probability for
which P (s, a, s′) > 0 and the belief B′ is consistent with
the observation o. Lastly, if there exists s′ ∈ PostG(s, a) \ F
and PostG(s, a) ∩ F 6= ∅, then the game reaches qF and all
(s′, B′) where s′ is reachable from s with a positive proba-
bility, and the belief B′ is consistent with some observation.
As the belief is constructed only using observations, P1,
after observing two observation-equivalent play ρ and ρ′ will
generate the same belief B. Thus, we say two states q =
(s, B) and q′ = (s′, B′) are (observation-)equivalent to P1
whenever B = B′. We denote the equivalence of two states
by q ∼ q′ and the set of all states equivalent to q by [q]∼. A
memoryless1 randomized strategy of P1 π : Q → D(A × Σ)
is said to be equivalence-preserving if and only if π(q) =
π(q′) whenever q ∼ q′.

Example 1 (Part II). Fig. 2 shows a subset of perfect-
observation MDP that P1 constructs in her mind when she
chooses the action (a0, σ0) at initial q0 = (s0, {s0}) and
the sensor 0 (corresponding to attack action β0) fails. The
game may reach any of the states (s0, {s0}), (s1, {s1, s2}) or
(s2, {s1, s2}) with a positive probability. For instance, if the
true state transitions from s0 to s1, then P1 would obtain

1Memoryless strategies are sufﬁcient for qualitative analysis of POMDPs

[17].

an observation o = {s1, s2} as sensor 0 is attacked and
the belief of P1 is updated to B′ = PostG(s0, a0) ∩ o =
{s0, s1, s2} ∩ {s1, s2} = {s1, s2}. When P1 chooses an
action (a0, ·) at (s1, {s1, s2}), where · can be any sensing
action, the game is ensured to reach the ﬁnal state regardless
of any sensor failure (because PostG(s1, a0) = {qF }).
Similarly, when P1 chooses an action (a1, ·) at (s1, {s1, s2}),
the game reaches the state (s5, {s4, s5}).

However, it is noted that P1 cannot distinguish whether
she is in (s1, {s1, s2}) or (s2, {s1, s2}). We observe that
the action (a0, σ0), which was winning for P1 at the initial
state (s0, {s0}) when no sensor failures are considered (see
Example 1 Part I), is no longer winning for her. This is
because the sensor failure results in P1’s belief state to be
{s1, s2}, at which she does not have an action that almost-
surely reaches qF —as P1 cannot distinguish whether the
state is (s1, {s1, s2}) or (s2, {s1, s2}).

belief-based

Next, we present Alg. 2 to synthesize P1’s almost-
by
sure winning
PostH (q, (a, σ)) = {q′
| δ(q, (a, σ)) > 0}. Given a
state q ∈ Q ∪ {qF }, the set of its predecessors state-action
pairs is denoted by

strategy. We

denote

Pre(q) = {(p, (a, σ))

|

q ∈ PostH (p, (a, σ))}.

and generalize this operator to subsets of states, Pre(X) =
∪q∈X Pre(q). The algorithm starts by identifying the set
L0 of states from which there does not exist a path (a
sequence of transitions with positive probabilities) to reach
the ﬁnal state qF . That is, the states in L0 are clearly not
almost-sure winning for P1. L0 can be computed using
standard graph algorithms over H. Subsequently, in k-th
iteration, the algorithm identiﬁes and eliminates those actions
at predecessors q of some u ∈ Lk which visit Lk in one step
with a positive probability. As P1 cannot distinguish between
equivalent states, if she removes an action (a, σ) from the
set of allowable actions π(q) at q then she must also remove
that same action for any state p that is equivalent to q. As P1
does not have an almost-sure winning strategy from any state
q whose allowable actions set π(q) is empty, such a state is
added to Lk+1. The k increments by one. The process repeats
until for some k, Lk+1 = ∅.

Theorem 1. P1 has an almost-sure winning strategy in G1
to visit F if and only if q0 ∈ Win1, where Win1 is the set of
almost-sure winning states computed by Alg. 2.

Proof (Sketch). It is known from [17] that P1 has an almost-
sure winning strategy in G1 to visit F if and only if she has
an almost-sure winning strategy in H to visit qF . Thus, for
the statement to hold, it must be the case that Alg. 2 must
identify every almost-sure winning state in H. This must be
true because Alg. 2 removes q from MDP only if there is no
safe action given P1’s belief B at q = (s, B), i.e. for each
enabled action, there exists a state p ∈ [q]∼ such that by
taking that action, P1 may reach a state from p from which
qF is not reachable with a positive probability.

Algorithm 2 Belief-based Strategy in POMDP Reachability
Inputs: H
Outputs: Win1: Almost-sure winning region of P1, π:

belief-based almost-sure winning strategy of P1

1: L0 ← {q ∈ Q | qF is not reachable from q}
2: For all q ∈ L, π(q) = ∅ and for all q ∈ Q \ L, π(q) =

{(a, σ) | δ(q, (a, σ)) is deﬁned.}

3: k ← 0
4: while Lk 6= ∅ do
5:

Lk+1 ← ∅
for all u ∈ Lk do

6:
7:

8:

9:

10:

11:

for all (q, (a, σ)) ∈ Pre(u) do

for all p ∈ [q]∼ do

Remove (a, σ) from π(p)
if π(p) = ∅ and p /∈ Sk
Add p to Lk+1

i=0 Li then

k ← k + 1.
12:
13: return Win1 = Q \ Sk

i=0 Li, π.

ASW Strategy. Given π : Win1 → 2A1 maps each state
q ∈ Win1 to a set of allowed actions (see Alg. 2), P1’s
almost-sure winning strategy selects each action in π(q)
randomly with a positive probability. It is noted that P1’s
strategy π is indeed a multi-strategy, or an inﬁnite set of
randomized strategies, because different choices of the prob-
abilistic distributions given the set of states yield different
randomized strategies. Again, for qualitative analysis, no
matter which randomized strategy π1 P1 selects, as long as
the support Supp(π1(q)) = π(q) for any q ∈ Win1, P1 can
ensure almost-sure winning in her perceived POMDP with
probabilistic sensor failures.

Example 1 (Part III). Consider the subset of perfect-
observation MDP in Fig. 2. As s4 is unreachable from
s5, all states (s5, B) for any B ⊆ S are contained in
L0. Thus, while investigating the state (s1, {s1, s2}), Alg. 2
identiﬁes action (a1, σ0) to lead to L0. Consequently, the
action (a1, σ0) is removed from states (s1, {s1, s2}) and
(s2, {s1, s2}) as they are equivalent. Similarly, the action
(a0, σ0) is removed from both states, thereby eliminating
them from set of almost-sure winning states. In the next
iteration, the initial state is also removed from set of almost-
sure winning states—concluding that action (a0, σ0) is not
almost-sure winning for P1.

Next, consider Fig. 3, which shows a subset of perfect-
observation MDP when P1 chooses the action (a0, σ1) at ini-
tial q0 = (s0, {s0}) and either the sensor 0 or 2 (correspond-
ing to attack action β0 and β2) fails. These states are not
eliminated by Alg. 2. To understand the winning strategy, ob-
serve the equivalent states (s2, {s0, s2}), at which the action
(a0, σ1) reaches qF with positive probability or visits one
of the states among (s1, {s1}), (s0, {s0, s2}), (s0, {s0, s1}).
For ease of reading, Fig. 3 does not show the outgoing edges
from (s0, {s0, s2}), (s0, {s0, s1}), (s1, {s0, s1}), (s1, {s1})
and (s2, {s2}). However, it can be veriﬁed that their outgoing
edges visit qF or a state among the same set of states. Thus,

(s1, {s0, s2})

(s1, {s1})

(a0, σ1), β0

0

1), β

qF

0

1), β

( a 0, σ

(a0, σ1), β0

(a0, σ1), β2

(s2, {s0, s2})

(s0, {s0, s2})

(a 0,σ 1),β 2

( a 0, σ

start

(s0, {s0})

(a

0,

σ

(a0, σ1), β0

(a0, σ1), β2

1),

β
0

(a0, σ1), β0

(s2, {s2})

(s0, {s0, s1})

Fig. 3. A subset of perfect-observation MDP constructed by P1 in her
mind. The ﬁgure shows the relevant states and edges when P1 chooses the
action (a0, σ1) at initial q0 = (s0, {s0}) and either the sensor 0 or 2 fails.

by choosing the action (a0, σ0) within this subset of MDP,
P1 is guaranteed to visit qF with probability 1 [18].

B. The Game Perceived by P2

Unlike P1, P2 has perfect observability and is also aware
of P1’s misinterpretation of the sensor failures. Thus, in
addition to keeping track of the true state of the game, he
can also track P1’s belief state. The resulting game model
for P2 can be represented as follows.

Deﬁnition 4. Given P1’s almost-sure winning, observation-
based strategy π in H such that P1 selects any action in
π(s, B) with a positive probability at a belief state B, P2’s
sensor attack strategy can be computed from qualitative
planning in the following MDP given as the tuple,

G2 = hWin1, A2, δA, q0, Win1 ∩ ((S \ F ) × 2S)i,

where Win1 is the set of almost-sure winning states of P1
in H. Given states q = (s, B), q′ = (s′, B′) ∈ Win1 and
an attack action β ∈ A2, we have δA(q, β, q′) > 0 if and
only if there exists (a, σ) ∈ π(q) such that s′ ∈ PostG(s, a)
and B′ = PostG(B, a) ∩ Obs(s′, σ, β). P2’s objective is to
ensure the game staying in the set Win1 ∩ ((S \ F ) × 2S)
with probability one.

A transition in G2 is interpreted as follows. For a given
state (s, B) ∈ Win1, P1 thinks that this belief B is almost-
sure winning for her. Therefore, she chooses any action
(a, σ) ∈ π(s, B) with a non-zero probability. However, a
state (s′, B′) in G2 will be reached probabilistically but
partially controlled by P2: state s′ will be reached with
a positive probability if s′ ∈ PostG(s, a), the belief B′
is decided jointly by the sensing action and attack action,
B′ = PostG(B, a) ∩ Obs(s′, σ, β). It is important to note
that this belief update is controlled partially by P2 given
his choice of attack action. As P2’s game G2 is a perfect-
observation game, the set of his almost-sure winning states

start

a0

a0

a0

s0

a1

s3

s1

a1

s2

Fig. 4. A two-player stochastic game with partially controllable observation
function. The dashed regions represent the sensors: 0 (red), 1 (blue), 2
(green).

to prevent P1 from reaching F can be computed using the
classical algorithm [19, Alg. 46].

Our interest in the P2’s almost-sure winning region is to
identify if there exist any states which P1 misinterprets to
be almost-sure winning for her, but they are in fact almost-
sure winning for P2. This analysis also provides a way to
detect adversarial attacks, for example, policy inference or
behavior cloning algorithms [20] from the observed sensor
failure data can be used to infer the “rule” behind the sensor
failures and compare it with the attack policy.

We illustrate the existence of such a state using a simpler
example shown in Fig. 4. The example consists of 4 states
covered by three sensors indexed 0 (blue), 1 (red) and 2
(green) covering the states γ(0) = {s1}, γ(1) = {s0, s1},
and γ(2) = {s2, s3}, respectively. P1 has two sensing
actions: σ0 and σ1 which query the sensors {1, 2} and
{1, 3}, respectively. P2 has three attack actions: βj which
jams the sensor j, for j = 0, 1, 2. The corresponding P2’s
game is shown in Fig. 5 (edges corresponding to σ2, β2
are omitted for clarity). Consider the states (s0, {s0}) and
(s0, {s0, s1}), at which P1’s strategy is to select the action
(a0, σ0). If P2 always selects β0 at these states, then the
game is restricted within the states (s0, {s0}), (s0, {s0, s1})
and (s1, {s0, s1}), indeﬁnitely. In other words, the states
(s0, {s0}), (s0, {s0, s1}) and (s1, {s0, s1}), which P1 con-
siders almost-sure winning for her, are in fact almost-surely
winning for P2. P1’s task failure is because she mistakes that
sensor failure caused by β1 is possible and sensor failure
caused by β0 cannot be persistent.

IV. DISCUSSION AND CONCLUSION

For the class of stochastic games with partially control-
lable observation function in which P1 has partial and P2
has perfect observation, we presented a method to identify
conditions under which the attacker has an almost-sure
winning, sensor-attack strategy but when the system (a na¨ıve
player) considers to be her almost-sure winning state due to
incorrectly treat sensor failures as probabilistic events.

With such an understanding of sensor-attack strategy, we
investigate the design of secured sensing and con-
will
trol strategy for a smart agent (P1) who is aware of the
adversarial sensor attacks. We will also be interested in
strategic sensor design which ensures, despite a na¨ıve P1, the
attacker’s almost-sure winning region is minimized, or does

[14] K. Chatterjee and L. Doyen, “Partial-observation stochastic games:
How to win when belief fails,” Proceedings of the Annual ACM/IEEE
Symposium on Logic in Computer Science, pp. 175–184, 2012.
[15] K. Chatterjee and T. A. Henzinger, “A survey of stochastic ω-regular
games,” Journal of Computer and System Sciences, vol. 78, no. 2, pp.
394–413, Mar. 2012.

[16] R. Bloem, K. Chatterjee, and B. Jobstmann, “Graph Games and
Reactive Synthesis,” in Handbook of Model Checking, E. M. Clarke,
Cham: Springer
T. A. Henzinger, H. Veith, and R. Bloem, Eds.
International Publishing, 2018, pp. 921–962.

[17] C. Baier, N. Bertrand, and M. Gr¨oßer, “On Decision Problems for
Probabilistic B¨uchi Automata,” in Foundations of Software Science
and Computational Structures, R. Amadio, Ed. Berlin, Heidelberg:
Springer Berlin Heidelberg, 2008, vol. 4962, pp. 287–301.

[18] M. L. Puterman, Markov decision processes: discrete stochastic dy-

namic programming.

John Wiley & Sons, 2014.

[19] C. Baier and J.-P. Katoen, Principles of model checking. MIT press,

2008.

[20] F. Torabi, G. Warnell, and P. Stone, “Behavioral cloning from ob-
servation,” in Proceedings of the Twenty-Seventh International Joint
Conference on Artiﬁcial Intelligence, IJCAI-18.
International Joint
Conferences on Artiﬁcial Intelligence Organization, 7 2018, pp. 4950–
4957.

(a0, σ0), β1

(a0, σ0), β0

start

s0, {s0}

(a0, σ0), β0

s1, {s0, s1}

(

a

0,

σ

(

a

0

,

σ

0

)
,

β

1

0

)
,

β

0

(a 0,σ 0),β 0

(a0, σ0), β1

s0, {s0, s1}

(a0, σ0), β0

(a0, σ0), β1

s1, {s1}

(a1, ·), ·

qF

Fig. 5.
P2’s perfect-observation MDP game corresponding to the game
in Fig. 4. All states in this ﬁgure are considered by P1 to be almost-sure
winning. But the shaded, red states are P2’s almost-sure winning states.

not include the initial game state. Quantitative planning under
sensor attacks with formal methods will also be investigated.

REFERENCES

[1] S. M. Dibaji, M. Pirani, D. B. Flamholz, A. M. Annaswamy, K. H.
Johansson, and A. Chakrabortty, “A systems and control perspective
of CPS security,” Annual Reviews in Control, vol. 47, pp. 394–411,
Jan. 2019.

[2] K. Xing, S. S. R. Srinivasan, M. J. M. Rivera, J. Li, and X. Cheng,
“Attacks and countermeasures in sensor networks: A survey,” in
Network Security, S. C.-H. Huang, D. MacCallum, and D.-Z. Du, Eds.
Boston, MA: Springer US, 2010, pp. 251–272.

[3] A. Cetinkaya, H. Ishii, and T. Hayakawa, “An Overview on Denial-
of-Service Attacks in Control Systems: Attack Models and Security
Analyses,” Entropy, vol. 21, no. 2, p. 210, Feb. 2019.

[4] Y. Shoukry, P. Nuzzo, A. Puggelli, A. L. Sangiovanni-Vincentelli,
S. A. Seshia, and P. Tabuada, “Secure State Estimation for Cyber-
Physical Systems Under Sensor Attacks: A Satisﬁability Modulo
Theory Approach,” IEEE Transactions on Automatic Control, vol. 62,
no. 10, pp. 4917–4932, Oct. 2017.

[5] Y. H. Chang, Q. Hu, and C. J. Tomlin, “Secure estimation based
Kalman Filter for cyber–physical systems against sensor attacks,”
Automatica, vol. 95, pp. 399–412, Sep. 2018.

[6] M. Showkatbakhsh, Y. Shoukry, S. N. Diggavi, and P. Tabuada,
“Securing state reconstruction under sensor and actuator attacks:
Theory and design,” Automatica, vol. 116, p. 108920, Jun. 2020.
[7] L. Niu and A. Clark, “Optimal Secure Control With Linear Temporal
Logic Constraints,” IEEE Transactions on Automatic Control, vol. 65,
no. 6, pp. 2434–2449, Jun. 2020.

[8] Y. Wang and M. Pajic, “Supervisory Control of Discrete Event Systems
in the Presence of Sensor and Actuator Attacks,” in 2019 IEEE 58th
Conference on Decision and Control (CDC), Dec. 2019, pp. 5350–
5355.

[9] M. Wakaiki, P. Tabuada, and J. P. Hespanha, “Supervisory Control
of Discrete-Event Systems Under Attacks,” Dynamic Games and
Applications, vol. 9, no. 4, pp. 965–983, Dec. 2019.

[10] J. Dubreil, P. Darondeau, and H. Marchand, “Supervisory Control for
Opacity,” IEEE Transactions on Automatic Control, vol. 55, no. 5, pp.
1089–1100, May 2010.

[11] A. Saboori and C. N. Hadjicostis, “Opacity-Enforcing Supervisory
Strategies via State Estimator Constructions,” IEEE Transactions on
Automatic Control, vol. 57, no. 5, pp. 1155–1165, May 2012.
[12] Y.-C. Wu and S. Lafortune, “Synthesis of insertion functions for
enforcement of opacity security properties,” Automatica, vol. 50, no. 5,
pp. 1336–1348, May 2014.

[13] Y. Wang and L. Xing, “Reliability of wireless sensor networks subject
to phase-dependent probabilistic competing failures,” in 2017 Second
International Conference on Reliability Systems Engineering (ICRSE),
Jul. 2017, pp. 1–9.

