SoK: A Survey of Open-Source Threat Emulators

Polina Zilberman1 and Rami Puzis1 and Sunders Bruskin1 and Shai Shwarz1 Yuval Elovici1

0
2
0
2

t
c
O
2

]

R
C
.
s
c
[

2
v
8
1
5
1
0
.
3
0
0
2
:
v
i
X
r
a

Abstract— Threat emulators are tools or sets of scripts that
emulate cyber attacks or malicious behavior. They can be used
to create and launch single procedure attacks and multi-step
attacks; the resulting attacks may be known or unknown cyber
attacks The motivation for using threat emulators varies and
includes the need to perform automated security audits in
organizations or reduce the size of red teams in order to lower
pen testing costs; or the desire to create baseline tests for
security tools under development or supply pen testers with
another tool in their arsenal.

In this paper, we review and compare various open-source
threat emulators. We focus on tactics and techniques from the
MITRE ATT&CK Enterprise matrix and determine whether
they can be performed and tested with the emulators. We
develop a comprehensive methodology for our qualitative and
quantitative comparison of threat emulators with respect to
general features, such as prerequisites, attack deﬁnition, clean
up, and more. Finally, we discuss the circumstances in which
one threat emulator is preferred over another.

This survey can help security teams, security developers, and
product deployment teams examine their network environment
or products with the most suitable threat emulator. Using the
guidelines provided, a team can select the threat emulator that
best meets their needs without evaluating all of them.

I. INTRODUCTION

It is essential for IT security professionals to identify
weaknesses in security systems before cyber criminals ex-
ploit them. Red teams are groups of white-hat hackers that
perform pen testing by assuming an adversarial role. In
addition to ﬁnding unpatched vulnerabilities, one of the
most important objectives of a red team is evaluating the
organization’s security readiness, active controls, and coun-
termeasures by emulating a full attack lifecycle. Extensive
what-if analysis is necessary to evaluate an organization’s
response to an attack that may penetrate its premises [27],
[23], [4], [29], [28], [20].

Threat emulation platforms, such as Red-Team Automa-
tion (RTA) [17] and Atomic Red Team [9], signiﬁcantly
accelerate and simplify what-if analysis in diverse scenarios.
Threat emulators also make it easier for IT professionals
without red team qualiﬁcations to test an organization’s se-
curity controls and countermeasures. A good threat emulator
makes it is easy to challenge the organization’s security
controls with a wide variety of realistic multi-step attacks
in a controlled manner. IT Professionals cannot afford to
explore and use multiple threat emulators. Thus, there is a
need to review existing emulators and develop a methodology
to support threat emulator selection; doing so will assist

1Department of Software

and Cyber@BGU, Ben-Gurion University
653
Israel.
{polinaz,puzis,elovici}@bgu.ac.il

Beer-Sheva,

and Information Systems Engineering
the Negev, P.O.B.
{sundersb}@post.bgu.ac.il,

of

IT professionals in choosing the threat emulator that best
meets organizational needs. To the best of our knowledge, no
rigorous comparison of threat emulators has been published.
In this paper, we survey general purpose open-source
threat emulators suitable for post-compromise analysis, ex-
cluding tools designed for speciﬁc types of systems. For
example, security assessment tools such as sqlmap [1] and
w3af [2], respectively Web application attack and audit
frameworks, are not included in this survey, because they
are too speciﬁc and are not general purpose threat emulators.
Various fuzzing tools [8], [33], [14], as well as network and
vulnerability scanners such as Nmap [19] or Nessus [30], are
also excluded from this survey, because they are not used to
emulate complex multi-step attack scenarios.

The threat emulators are reviewed and compared with
respect to tactics and techniques from the MITRE ATT&CK
matrix and criteria such as prerequisites, attack scenario
deﬁnition, and clean up. The contributions of this study are
fourfold:

1) We present well-deﬁned criteria and a methodology for
the evaluation and comparison of threat emulators.
2) We present a thorough review of the following threat
emulators: APTSimulator, Red Team Automation,
Uber’s Metta, CALDERA, Atomic Red Team, Infec-
tion Monkey, PowerSploit, DumpsterFire, Metasploit,
BT3 and Invoke-Adversary.

3) We present a taxonomy that illustrates the qualities of

the threat emulators.

4) We provide actionable guidelines for choosing the
best threat emulator given a speciﬁc organizational
environment and a set of security assessment tasks.

The rest of the paper is structured as follows. Section II
provides background on the attack capabilities matrix. In
Section III, we deﬁne the threat emulation ecosystem. Sec-
tion IV presents the criteria used to review and compare the
threat emulators. A comprehensive methodology for evaluat-
ing threat emulators is presented in Section V, followed by
highlights of our review of the 11 threat emulators. Next, in
Section VI, we introduce a comprehensive emulator taxon-
omy and guidelines for threat emulator selection. Section VII
summarizes the paper.

II. BACKGROUND: MITRE ATT&CK

MITRE’s ATT&CK is an open knowledge base of adver-
sarial tactics and techniques which is continuously updated
based on real-world observations [31]. MITRE’s ATT&CK
is considered state-of-the-art when it comes to describing
common behavior of adversaries. Many companies, such as

 
 
 
 
 
 
D3 Security and AlienVault, rely on this knowledge base in
threat intelligence that they consume or provide.

Many threat emulators are designed with respect to the
techniques listed in the MITRE ATT&CK Enterprise matrix1.
Threat emulators implement a variety of attack scenarios in
which different tactics and techniques are employed. The
larger are the number of tactics supported by an emulator and
the number of techniques achieving each tactic, the higher is
the diversity of the attack scenarios supported by an emulator.
Diversity of supported attack scenarios aids the red team in
discovering security ﬂaws. MITRE ATT&CK matrix helps
systematically evaluating the diversity of attack scenarios that
can be created by an emulator.

In this paper,

the threat emulators are reviewed and
compared with respect to tactics and techniques from the
MITRE ATT&CK matrix and criteria, we introduce the
MITRE terminology used in the rest of this paper. Tactics
represent the adversary’s technical goals, such as achieving
persistence, privilege escalation, defense evasion, and more.
An attack’s lifecycle, also referred to as the kill chain [15],
[25], [18], is typically a series of goals achieved during
the course of the attack. Every tactic includes a variety of
techniques, also known as attack patterns [6], which achieve
similar goals. A technique may achieve multiple goals, thus
being associated with a number of tactics. Techniques are
abstract descriptions that distill the essence of many similar
attack implementations. The speciﬁc implementations of the
techniques are referred to as procedures. For example, in
order to achieve persistence (a tactic), an adversary creates
a scheduled task (a technique) using the following Windows
schtasks /create /sc /daily /tn
command:
<folder path>\<task name> /tr (a procedure).

A brief explanation of the tactics listed in MITRE’s
ATT&CK matrix and their contribution to the success of
an attack is provided below. These tactics will be used as
attributes in the detailed threat emulator review in Section V.
Initial Access tactic includes all of the techniques that
enable malware to inﬁltrate a machine or a network envi-
ronment. Pen testing focuses on this tactic.

Execution tactic includes all of the techniques that result
in the execution of attacker controlled code on a victim’s
machine. The more execution techniques the attacker has in
his/her toolbox, the more obstacles, such as the PowerShell
execution policy, he/she can overcome.

Persistence assures that even if the compromised machine
restarts, the malware will persist. The fact that initial access
is risky for the adversary and requires signiﬁcant effort makes
persistence crucial in the attack’s lifecycle.

Privilege escalation is required in order to access im-
portant assets only accessible with superuser permissions.
Superuser (or administrative) permissions also facilitate the
achievement of other goals, such as defense evasion.

Defense evasion includes techniques that enable the attack

to stay under the radar of security defense tools.

Credential access is a goal pursued by most attackers.

1https://attack.mitre.org/techniques/enterprise/

Having multiple user credentials facilitates privilege escala-
tion, persistence, and defense evasion. For example, speciﬁc
user credentials may be required to access a key system.

Discovery includes techniques used by malware to dis-
cover assets, network topology, security controls, vulnerabil-
ities, etc.

Lateral movement includes techniques are used to reach
the target assets spreading out from the initial entry point.
Emulating lateral movement
threat emulator
capability when evaluating network security systems.

is a crucial

Collection includes techniques for collecting the target
data assets. In addition to espionage and intelligence, collec-
tion techniques also facilitate defense evasion and credential
access tactics.

Exﬁltration includes techniques for transferring the col-
lected data from the compromised environment to the at-
tacker.

Command and control

includes techniques that enable
the attackers’ interactions with their tools installed on the
compromised network.

III. THREAT EMULATION ECOSYSTEM

Threat emulators are an integral component of compre-
hensive red teaming. In this section, we deﬁne the threat
emulation ecosystem (see Fig. 1), including the stakeholders
and relevant software processes, and their relationships in
the threat emulation ecosystem. We map the MITRE tactics
(see Section II) to the red teaming software components.
The ecosystem elements are framed with respect to four
threat emulation use cases: training, security tool assessment,
organizational security assessment, and what-if analysis.

A. Stakeholders

The sponsor is the organization that employs the red team
to perform a security assessment.
The threat actor is the adversarial entity that targets the
sponsor. The threat actor’s motivations may vary depending
on the sponsor’s assets. His/her attacks can compromise
the sponsor’s operational environment. Information about
to the assessed
the threat actors that are most relevant
organization will enable the red team to take on the role
of the attacker [7], [10] more realistically and mimic the
behavior of the threat actors [21], [24], [20].
The red team is hired to challenge the security of an
organization and may include pen testers, social engineers,
reverse engineers, and intrusion detection specialists; in rare
cases, the team may also include language specialists [3].
The sponsor and red team agree on the assessment target
and scope in light of the organization’s critical assets and
possible attack vectors [7], [24]. The scope is also limited
by ethical considerations and legislation [20].
The blue team defends the organization’s systems by de-
ploying and using security tools to detect and mitigate the
activities of the threat actors.
The white team is responsible for the coordination, execution,
and analysis of the red team-blue team exercises [22], [12].
White teams are most important during training.

Fig. 1. The threat emulation ecosystem

B. Software and Tools

The operational environment is the hardware and software
that supports the organization’s operation. The security needs
arise from the type of assets within the operational environ-
ment, the threat actors, and the regulations the organization
is subject to [34]. The operational environment also affects
the type of security assessment and training that can be
performed in the operational environment itself and the type
of training and assessment that must be performed in a non-
operational environment, such as a cyber range [21].
Cyber range is a term used for a virtual environment created
for cyber security training. Some operational environments
are too critical to risk malfunction during training or security
assessment. In such cases, a safe environment that imitates
the real operational environment is required [21], [24]. Such
operational environments include critical infrastructure, e.g.,
supervisory control and data acquisition (SCADA) systems,
where a single glitch resulting from red team activities could
lead to a disastrous malfunction. Another reason for using a
cyber range is the fact that threat emulation can trigger false
alarms which are hard to distinguish from real attacks [34].
Security tools represent the collection of intrusion detection,
antivirus, ﬁrewall, SIEM (security information and event
management), EDR (endpoint detection and response), and
other systems deployed within the operational environment to
protect the organizational assets. Security tools are operated

by the blue team.
Vulnerability scanners, reverse engineering tools, exploits,
fuzzers, etc. are part of the red team’s toolset. They are used
by red teams to actively probe the operational environment
and security tools for weaknesses and vulnerabilities. Fol-
lowing Applebaum et al. [4] and Adkins [3], we consider all
tools used to identify opportunities for unsolicited access or
code execution, including social engineering, as pen testing.
Today, cyber security defense paradigms assume that the
organization is already compromised [11]. Consequently,
perimeter defense is insufﬁcient, and the defenses deployed
need to identify and mitigate malicious activities throughout
the entire attack lifecycle. Security assessment of the orga-
nization in post-compromise scenarios is critical. Repeated
assessment using red teams is expensive due to the expertise
required. Manual red teaming also lacks the ability to repli-
cate the security assessment in a fully repeatable manner.

Threat emulators, the software tools that are the main fo-
cus of this survey, aid in the automation of post-compromise
red team activities [4], compensating for any expertise the red
team may lack. Although, human white-hat (ethical) hackers
cannot be replaced by automatic red teaming tools, the use of
threat emulators can increase the productivity of red teams or
reduce the level of expertise required of the red team [24].
Challenging the security array in many different scenarios
rapidly and repeatedly is especially important during training

and what-if analysis. When emulating attack techniques that
may endanger the operational environment or personnel (e.g.,
persistence, credential access, lateral movement, collection,
impact), threat emulators usually operate by leaving forensic
evidence that mimics the execution of speciﬁc attack proce-
dures. Emulators can execute real attack procedures without
causing actual damage or compromising the organization.

Vulnerability scanners and threat emulators can interfere
with the network and systems in place. Therefore,
they
should be used with caution within the operational environ-
ment. The extent to which pen testing tools are employed
(frequency, intensity, volume, etc.) is deﬁned by the sponsor
and red team within the scope of red teaming.

Red team-blue team exercises should be used in a ded-
icated environment, like a cyber range, where there is no
interference in the operational environment. The cyber range
should imitate the operational environment
to the extent
possible in order to increase the suitability of the security
tools and their operators (the blue team) to the organization.

C. Red Teaming Use Cases

Training. The terms red team and blue team originate from
military exercises where a blue team is responsible for
protecting an asset, and the red team’s role is to challenge
the blue team’s plan of defense iteratively [21]. The result
of training should be a comprehensive understanding of the
effectiveness of the asset’s security mechanisms/protocols,
as well as improved response in terms of mitigation and
efﬁciency. In some cases, a white team oversees the training
exercise(s), coordinating between teams and analyzing their
performance. Often the training takes place in a controlled
environment like a cyber range.

Threat emulators can be used to train the blue teams and
test their capabilities in handling security events. Emulators
can create predeﬁned and well-designed scenarios that pro-
vide full coverage of essential detection and response cases.
In the absence of a qualiﬁed red team, an operator without
extensive security expertise can use a threat emulator to train
the blue team members in typical scenarios.
Security Tool Assessment. Such assessment and evaluation
is required to correctly position a security product within
the cyber security landscape. Organizations’ security needs
can vary. The security requirements may depend on the type
of assets an organization has, the regulation it is subject
to, the organization’s size, and other characteristics of the
operational environment.

Security tool assessment encompasses two perspectives:
(1) assessing the contribution of a tool to the security pre-
paredness of the organization, and (2) testing the resilience
of a tool (or a new version of it), also known as subversive
exploitation [24], [20]. Assessment of security products
should never compromise an organization [22]. Therefore
it is preferable to evaluate a security tool within a cyber
range that mimics the environment of the target organiza-
tion. Alternatively, an organization can use threat emulators
to perform comparisons and assessments of security tools
without compromising the organization’s security.

The ability to create and reproduce simulated attacks helps
when comparing and contrasting security. MITRE ATT&CK
Evaluation2 is an example of a service that assesses the
capabilities of different types of security tools. In particular,
it speciﬁes the adversarial techniques that each tool evaluated
can mitigate.
Organizational Security Assessment. Security regulations and
standards, such as the EU General Data Protection Reg-
ulation (GDPR) [32], ISO/27001, or the NIST catalog of
security and privacy controls [16], deﬁne a set of security
controls as well as standard security assessment activities.
For example, section CA-8 in NIST Special Publication 800-
53 on security controls deﬁnes the need for independent
security assessment using red teams and red team exercises.
Due to the constant development of the cyber threat
landscape, organizations must routinely review, document,
and update all aspects of security. The red team’s role
is to identify weak points and security breaches through
interaction with different planes of the organization: systems,
users, applications, etc. [24], [20]. In-house or outsourced red
teams assess the organization’s security by simulating attacks
that do not compromise the security but thoroughly test
it [26]. Launching a wide range of emulated attacks allows
the organization to better understand its vulnerabilities and
weaknesses; as a result, the organization can take measures
to improve security.
What-If Analysis This type of analysis assesses potential
damage under different threat assumptions. What-if analysis
disregards why and how a change in the operational en-
vironment has occurred and focuses on the consequences.
Such analysis is important for evaluating cyber defenses’
robustness to changes in both the course of the attack (red
team perspective) and the operational environment (blue
team perspective) [5]. Assessment of the potential impact
of security events using what-if analysis helps prioritize the
respective mitigation activities.

By using threat emulators, the security team can automate
large parts of the what-if analysis process. Threat emulators
allow the reproduction of attack scenarios while only chang-
ing speciﬁc attack parameters.

IV. CRITERIA FOR COMPARISON
One of the objectives of threat emulation is to accelerate
and simplify the what-if analysis of organizational security
controls and countermeasures in diverse scenarios. Suitability
of a threat emulation framework depends on the capabilities
of the red/blue teams, as well as on various organization
constraints. This section presents criteria for comparing and
evaluating threat emulators (see Fig. 2).

A. Environment

Determining whether or not the emulator can operate in
the target environment is the ﬁrst and most basic criteria for
assessing the emulator’s compatibility. This includes oper-
ating system compatibility, required changes in the security
array, special privileges, etc.

2https://attackevals.mitre.org/

3) Prerequisites: This criterion speciﬁes whether there are
any special prerequisites for the threat emulator to work. It
may include third party software tools (such as mimikatz3),
hardware components (such as a dedicated CNC server),
special privileges (if the emulator’s agent requires superuser
privileges), etc. Similar to any redistributed package, a good
threat emulator should be self-contained. The value of this
criterion can consist of one or more of the following:
∅ – The emulator is self-contained and does not require
special privileges.
3rd – The emulator requires the installation of third party
tools on the endpoints.
Priv – The emulator requires special privileges (e.g., supe-
ruser or running in a kernel mode).

We consider an emulator to be self-contained if it includes
everything that is required to operate on the endpoints. We
exclude any prerequisites for the dedicated servers from this
criterion, because their impact on the deployment of the
emulator is minimal compared to any required installations
on the endpoints. In addition, since we focus on open-source
threat emulators, we assume that all prerequisites are open-
source as well.
B. Scenario Deﬁnition

When evaluating security solutions it is often necessary
to mimic complex and sophisticated real attacks. A threat
emulator should be able to deﬁne all steps of the emulated
attacks and produce realistic artifacts. Mimicking sophisti-
cated real attacks includes both the ability of the emulator
to leave detectable traces, as well as its ability to cover its
tracks when emulating defense evasion techniques.

1) Adding new procedures: Cyber criminals constantly
develop new exploits and perform unknown attacks. Hence,
the ability to add new procedures is crucial for a future-
proof threat emulator. A good emulator should be able to
handle the addition of new procedures for techniques it
already implements as well as procedures that introduce new
techniques. This criterion is binary:
Yes – The emulator supports adding new custom procedures.
No – The emulator does not provide interfaces or infrastruc-
ture for adding new custom procedures.

2) Procedures’ conﬁguration: Different organizations
have different assets, network structures, and security con-
trols. Consequently,
to be able to easily
is important
modify the operation of an emulated attack in order to
ﬁt the organization. For example, one should be able to
conﬁgure the IP range, ports, and protocols of a procedure
that implements network scanning.

it

On the one hand, if the emulator does not support the
addition of new procedures or the conﬁguration of built-in
procedures, its applicability is very limited. On the other
hand, adding a new procedure instead of conﬁguring an
existing procedure is extremely ineffective. Moreover, the
ability to conﬁgure procedures is required when emulating
a large number of attack variants in a row. A good threat

3https://github.com/gentilkiwi/mimikatz,
//www.varonis.com/blog/what-is-mimikatz/

https:

Fig. 2.

Criteria hierarchy for comparing threat emulators

1) Operating system compatibility: Usually organizations
work with different operation systems (OSs). Ideally a threat
emulator should support all operation systems used by the
organizational endpoints. We do not include requirements
of the CNC component of the emulator in the OS compat-
ibility criterion, because it is not part of the organizational
environment tested.

The OS compatibility criterion may include one or more
of the following OSs: Windows, Linux, MacOS, Android,
iOS. Due to the ubiquitous nature of mobile devices, it
is very important to include them in the attack scenarios
tested. Thus, we include mobile platforms in this threat em-
ulator evaluation criterion, although, none of the emulators
reviewed support them.

2) Changes in the security array: Every organization has
various security tools that provide some level of protection.
Antivirus (AV) software and ﬁrewalls (FWs) are elements
of the organizational security array. Some emulators require
disabling AV and FWs to operate successfully. For example,
a threat emulator that relies on remote administration tools
(RATs) may require disabling the real-time antivirus protec-
tion in order to operate successfully. Such an emulator is not
suitable for evaluating antivirus protection effectiveness.

A good threat emulator should operate without requiring
changes in the organizational security array. In the ideal
case, the security controls and countermeasures evaluated
are capable of detecting the emulated attacks but not the
emulator’s agent. The value of this criterion represents the
set of security controls that must be disabled in order to
execute the emulator. There are three possible values of this
criterion:
∅ – The emulator can operate successfully without disabling
any security controls.
AV – Antivirus software should be disabled.
FW – The ﬁrewall should be disabled.

emulation framework should have the capacity to conﬁgure
both built-in procedures and new custom procedures. For
example, an emulator that executes remote code on an
endpoint should at least ensure that the parameters of the
executed procedures are set before transferring the code
to the endpoint. We consider three possible values for the
conﬁgurability of the threat emulator:
Low – The emulator doesn’t support repeated execution of
the same attack scenario with different parameters.
Med. – The emulator does support repeated execution of the
same attack scenario with different parameters.
High – Like the medium value, but the emulator also has the
ability to easily conﬁgure new procedures.

We assign a low conﬁgurability value to emulators with
hardcoded procedure parameters or when procedures can
only be conﬁgured manually, e.g., through a GUI. If the
emulator supports conﬁguration ﬁles or can be conﬁgured
through a CLI or API, it is possible to automate the execution
of attack scenarios with different parameters.

3) Multi-procedure attacks: An attack can be referred to
as a set of procedures launched along a speciﬁc timeline. In
order to emulate a realistic attack, it is necessary to com-
bine multiple procedures together. Threat emulators should
both provide multiple built-in attack scenarios and support
the creation of new multi-procedure attacks. Built-in attack
scenarios may assist red teams that are rapidly testing new
security solutions, IT professionals who are not qualiﬁed red
team practitioners, and novice security personnel undergo-
ing training. The ability to create custom attack scenarios
that recombine tactics, techniques, and procedures is very
important for challenging the organizational security array
and performing diverse what-if analysis. This criterion may
include one or more of the following values:
∅ – No support of multi-procedure attacks.
Built-in – Includes ready to run multi-procedure attacks.
Custom – Multi-procedure attacks can be added.

4) Tactics, techniques, and procedures (TTP) coverage:
The diversity of a threat emulator is derived from the
assortment of attack techniques it can emulate. To map the
capabilities of a threat emulator, we refer to the set of
tactics, techniques, and procedures in the MITRE ATT&CK
knowledge base. The more tactics a threat emulator covers,
the more complete with respect
to an attack’s lifecycle
the emulated attacks can be. The more techniques a threat
emulator implements, the more comprehensive the resulting
assessment is. In an ideal case a threat emulator should
contain multiple procedures implementing every known ad-
versarial technique.

At least one procedure/technique is required to claim that
a technique/tactic (respectively) is supported by an emulator.
We deﬁne the following three levels of TTP coverage based
on the tactics and techniques supported:
Low – The number of supported tactics is less than six or
the number of supported techniques is less than 20.
Med. – The emulator supports six or more tactics and 20 or
more techniques.
High – The emulator supports more than eight tactics and

over 40 techniques.

The thresholds above are quite low given the 12 tactics and
266 techniques currently listed by MITRE in the Enterprise
ATT&CK matrix. Unfortunately, the overall TTP coverage
provided by current threat emulators is very low. The thresh-
olds were set such that they partition the set of emulators into
three non-trivial groups.

C. Scenario Execution

1) Stopping an attack mid-runtime: When examining an
operational environment during the execution of a simulated
attack, an operator may want to stop the attack for various
reasons. For example, a simulated attack might consume
too many computational resources or otherwise harm the
system’s operation, or the endpoints may require crucial
updates. There may also be a need to change the attack
scenario after the evaluation process has started. For exam-
ple, the red team may ﬁnd that the scenario is missing a
crucial component required for an effective assessment, a
realization that would necessitate stopping and restarting the
evaluation. Consequently, it is important that the emulator
has the ability to stop a simulated attack at any stage before it
ﬁnishes running. Note that changing an attack scenario while
it is being executed results in a new attack scenario that is
different from both the new one and the old one. Thus, we
do not consider it a useful capability. This criterion is binary:
Yes – The emulator supports stopping an attack scenario
while it is running without far-reaching consequences.
No – The emulator does not provide such functionality, and
the operator must manually stop all of the attack components.
2) Cleanup: After an emulated attack scenario has been
completed or stopped in the middle, the machines must be
rolled back to their previous state, and all traces of the
attack must be cleaned up and removed. This functionality
is especially important when emulating a large number of
attacks. Cleanup may include, for example, changing registry
values, ﬁles, etc.

During cleanup a threat emulator should remove artifacts
that it has created. However, send and forget actions per-
formed by the emulator, such as pinging a remote server,
may be logged by the monitoring system. As a result, an alert
may be created and stored within the SIEM. An emulator is
unaware of the security, logging, or monitoring tools in the
network. In cases in which threat emulation is performed on
a dedicated cyber range, it is possible to roll the machinery
back to its previous state and emulate additional attacks.

However, in an operational network, reverting the response
of the security tools to the activity of the threat emulator is
not a reasonable requirement. This will create too strong
coupling between the red and blue teams’ instrumentation.
Nevertheless, the operational staff should be aware of the
possible consequences of the threat emulation process, in-
cluding possible collateral damage and false alarms [34].
This criterion is ternary:
No – The emulator does not support cleanup.
Proc – Cleanup of individual procedures is supported.
Attk – Cleanup of multi-procedure attacks is supported.

Although, none of

the emulators reviewed facilitate
cleanup for new custom procedures, it is important to en-
courage such implementation at the API level. The teardown
functions in unit tests are a good example of cleanup facili-
tated by a framework. In the case of multi-procedure attacks,
both procedure level and attack level cleanup (teardown)
implementation is required.

If the emulator supports stopping an attack mid-runtime,
then we expect the cleanup functionality to be invoked when
the attack is interrupted. Cases in which cleanup of an
interrupted attack is not possible are considered to be the
result of a bug rather than an assessment criterion.

3) Logs: Threat emulator logs can help the operator
understand whether the attack was executed successfully,
which stages of the attack were performed, whether the
target-assets were acquired, etc. This criterion refers to the
capability of a threat emulator to automatically produce and
store log ﬁles. It can be of the following values:
No – There are no logging capabilities implemented. If
needed, the operator can log the attack steps.
Base – Log entries are created at the beginning and end of
the emulated attack execution.
Adv. – A log entry is generated for every executed procedure.
Note that if an emulator does not support multi-procedure
attacks, the value for this criterion is set to [Base] by default
(brackets are used to denote criteria values within the text).

D. Operators

1) Required security expertise: One of the goals of using
threat emulators is to simplify the security assessment pro-
cess. Ease of use and execution of attacks is an important
factor when evaluating a threat emulator. Threat emula-
tors that demand expert knowledge, complicated setups, or
extensive conﬁguration are counterproductive. We consider
a good threat emulator as one that allows an operator
without security expertise to execute and conﬁgure all built-
in attacks. Of course, the ability to add new procedures
requires cyber security expertise. This criterion refers only
to the expertise required to conﬁgure and execute attacks
provided in the emulator’s original package. We deﬁne two
levels of required security expertise as follows:
Low – Built-in attacks can be executed out of the box
or by following step-by-step instructions provided with the
emulator.
High – Provided instructions are not sufﬁcient for an opera-
tor, who lacks cyber security knowledge, for conﬁguring and
executing attacks provided with the emulator.

Note that this criterion is different from the criterion of
documentation described below, which considers the cover-
age and the level of detail of the documentation provided.
Here we only consider the prior cyber security knowledge
required to launch attacks.

2) Documentation: In addition to the need for an intuitive
user interface,
the emulator’s
documentation. While some emulators consist of a poorly
documented collection of scripts, some are well documented.

is important

to inspect

it

Good documentation shortens the learning curve and fa-
cilitates full utilization of the emulator’s capabilities. Even
an operator that is unfamiliar with the emulator should be
able to successfully create and execute attack scenarios and
interpret the results relying only on his/her cyber security
knowledge and the documentation provided.

This criterion refers to the completeness of the documen-

tation. It may be one of the following values:
Full – The documentation is sufﬁcient for setting up all of the
emulator’s components, executing built-in attack scenarios,
creating new ones, and interpreting the results.
Miss – Documentation of some of the critical functionality
is missing.
None – The documentation is insufﬁcient, making it difﬁcult
to set up the emulator or execute built-in attack scenarios.

3) Interfaces: It is important to map an emulator’s inter-
faces in order to better suit the capabilities of the operators
and organizational needs, such as the desired level of attack
customization or the frequency of security assessment. Threat
emulators can be operated through a simple command-line
interface (CLI) or a graphical user interface (GUI). Novice
operators may ﬁnd a GUI more accessible and intuitive.
Command-line tools are usually required for automating
security assessment, but they have a steeper learning curve.
Some emulators also support scripting (further denoted as
SRP) to produce custom attack scenarios. A few emulators,
such as Atomic Red Team and Metasploit, provide an appli-
cation interface (API) to manage the attack scenarios. Ideally
an emulator will provide all the above interfaces. The value
of this criterion may consist of one or more of the following:
GUI – Some functionality is accessible through a GUI.
CLI – Some functionality is accessible through a CLI.
API – Some functionality is accessible through an API.
SRP – The emulator supports scripting of attack scenarios
or experiments.

When all functionality relevant to the scenario’s deﬁnition
and execution is accessible through a GUI, CLI, or API, we
denote this with an asterisk (*).

V. DETAILED EVALUATION OF THREAT EMULATORS

In this section, we present a comprehensive evaluation
methodology and review 11 open-source threat emulators
based on the criteria described in Section IV.

A. Evaluation Methodology

In this subsection, we describe the methodology for evalu-
ating threat emulators with respect to the criteria presented in
Section IV. The expert questionnaire for emulator assessment
is described in Appendix VIII. The step-by-step evaluation
process is depicted in Fig. 4 in Appendix IX.

First, we list the emulator components (such as the CNC
server and agents) and prerequisites (such as third party
tools, libraries, and required privileges) needed to launch the
threat emulator. If there is a need for third party tools or
special privileges to be set up on the endpoints on which
the emulated adversarial activity will be executed, then the
values [3rd] or [Priv], respectively, will be selected for the

prerequisites criterion; otherwise, the value of this criterion
is set to [∅].

Next, we record whether or not the emulator uses agents
to emulate adversarial activity. The OS compatibility is
determined by the emulator’s agents if it uses them. In the
absence of agents, the OS compatibility is determined by the
built-in procedures and third party tools required. The CNC
server, tools for analyzing the results, and other emulator
components that can run on dedicated machines that are
not a part of the original operational environment are not
considered for OS compatibility.

Then, we determine whether changes in the security array
are required to successfully set up the agents and install third
party tools. For a threat emulator that works with agents we
examine whether the agent can be installed without disabling
the local ﬁrewall and antivirus software. At this stage we also
check whether there are any antivirus alerts on third party
tools or emulator ﬁles that should run on the endpoints.

After setting up the emulator, we examine the available
interfaces using which the emulated attacks can be deﬁned
and executed. We check whether the emulator has a GUI, API
or CLI, and whether it supports scripting (SRP). For each
interface, we determine whether all functionality required
for the attack scenario deﬁnition and execution is available
via the interface. If so, we add asterisk (*) to the interface’s
respective value.

To evaluate the threat emulator with respect to required
security expertise, we start by launching a built-in attack. If
it is possible to launch the attack by using the documentation,
without the need for arguments or conﬁguration, this criterion
will receive the value [Low]. Simple, meaningful names of
procedures contribute to a [Low] required security expertise
criterion value. However, if the conﬁguration of the attack
(including choosing the relevant procedures) requires knowl-
edge in cyber security beyond the documented conﬁguration
steps, this criterion will receive the value [High].

We also assess whether the emulator includes built-in
multi-procedure attacks. If it does, the multi-procedure attack
criterion receives the value [Built-in]. We execute any attack
of the built-in multi-procedure attacks, and try to stop the
attack mid-runtime. We search for the stopping functionality
in the available interfaces. For example, in cases in which
the threat emulator has a GUI, we look for a “stop” button.
If such functionality is provided and if, by activating it, all
of the attack components are stopped, this criterion receives
the value [Yes]; otherwise, it receives [No].

After the attack has stopped, we examine the logs created,
if such logs exist. If the log describes each procedure that
has been executed, this criterion will receive the value [Adv].
However, if only the attack metadata, such as start time, end
time, endpoint, etc., was logged, without information about
the execution of procedures, this criterion will receive the
value [Base]. If none of the above exists, this criterion will
receive the value [No].

After the attack has stopped, we also examine the emu-
lator’s cleanup functionalities. If cleanup is available, we
the attack
verify that

is executed correctly,

that

i.e.,

it

traces/artifacts are removed. For example,
if a ﬁle was
created, it should be deleted. If a registry key was modiﬁed, it
should be restored to the previous value, etc. As noted earlier,
send and forget actions cannot be cleaned up. In addition, we
examine the timing of the cleanup. If the cleanup is executed
after each individual procedure, this criterion will receive the
value [Proc]. If the cleanup is executed after the execution
of the entire attack scenario, this criterion will receive the
value [Attk].

Next, we examine whether the emulator supports adding
new procedures and creating custom multi-procedure attacks.
First, we try adding a new procedure, using the available
interfaces. If the emulator is script-based, we add the new
script to the relevant folder. We conclude that an emulator
does not support adding new procedures if it requires chang-
ing the emulator’s source code; for example, inserting a new
procedure into an existing script. To conﬁrm whether the new
procedure has been added successfully, we execute a simple
attack that contains the procedure.

Then, we try creating a new multi-procedure attack, either
by using the available interfaces or by combining available
scripts. If a new procedure was added in the previous step,
the new multi-procedure attack should include it. If the new
custom multi-procedure attack was created and executed
successfully, we assign the value [Custom] for the multi-
procedure attack criterion.

Using the new multi-procedure attack, or any built-in
attack, we then examine the procedures’ conﬁguration before
execution. For each procedure that may require arguments,
such as an endpoint’s IP for lateral movement or data
exﬁltration, or a ﬁle name to search, we examine how these
arguments can be speciﬁed. If the arguments are speciﬁed
using a conﬁguration ﬁle or the emulator’s interfaces before
execution of the attack, the criterion value is set to [Med].
Otherwise, if the arguments are hardcoded, we change them
in the code and set the criterion value to [Low]. Finally,
if, when adding a new
we set
procedure, the emulator enforces an API that supports future
conﬁguration changes.

the criterion to [High],

To evaluate the TTP coverage of an emulator, we are ﬁrst
required to list all of the built-in procedures the emulator
provides. We map each procedure to the techniques that it
implements. Next, each technique is mapped to the tactics
that can be achieved by using it. The mapping is performed
according to the names of the procedures, techniques, and
tactics using mainly the MITRE ATT&CK Enterprise matrix.
Note, that the names of the emulator’s built-in procedures
might not match the names on the MITRE ATT&CK web-
site. In such cases, we map the procedures to techniques
according to their functionality. We summarize the mapping
results in a vector of 10 dimensions, one for each tactic,
except for the initial access and execution tactics. The value
of each dimension is the total number of techniques that
the threat emulator supports for the respective tactic. Initial
access and execution are not included in our evaluation of
threat emulators, because they are mainly covered by pen
testing tools which are outside the scope of this survey.

Last but not least, we conclude with an assessment of
the documentation quality. By documentation we refer to
comments in a procedure’s implementation, README ﬁles,
text on the respective GitHub page or website,
tooltips
and help options in the emulator’s interface, manuals, etc.
Throughout the emulator evaluation process we were assisted
by the available documentation. If every operation that
we were required to perform was well documented, this
criterion received the value [Full]. If one of the critical
functions required for attack deﬁnition or execution was not
well documented, this criterion received the value [Miss].
If the documentation does not include helpful instructions
for setting up the emulator and executing built-in attack
scenarios, we set the value of the documentation criterion
to [None].

B. Highlights

Each threat emulator was manually examined to deter-
mine: (1) which of the MITRE ATT&CK tactics and tech-
niques are implemented, and (2) how each of the criteria
discussed in Section IV are addressed. In this subsection we
present the highlights for each threat emulator reviewed. A
table containing a review of the emulators according to the
discussed criteria is provided as supplementary material to
the paper. 4

1) Red Team Automation (RTA): Red Team Automation5
is a framework comprised of Python scripts that blue teams
can use to test their security mechanisms. It has no central
components such as a CNC. Since RTA is written in Python,
it can only be used on Python supported OSs and on
endpoints that have Python installed on them. py2exe can
be used to operate RTA on Windows OS without Python. To
use all of the built-in functionality, RTA requires additional
third party software. For example, in order to simulate lateral
movement, psexec or xcopy are required. RTA includes min-
imal documentation, with setup and execution instructions
on its GitHub page6 and in comments within its scripts. The
scripts can be combined to create a multi-procedure attack.
Additionally, it is possible to edit procedures or add new ones
using Python. Hence, to be used effectively, RTA requires
Python knowledge. The emulated attack execution and the
changes made to the endpoints are displayed onscreen, but
there is no log ﬁle. RTA has a cleanup process which is
useful for reverting the changes made in each procedure.

RTA scripts implement a reasonable number of techniques
from MITRE’s ATT&CK matrix. However, the coverage of
the tactics is not uniform. For example, persistence and
defense evasion tactics are well covered, with 10 and 20
techniques respectively, while lateral movement has only
three techniques implemented in RTA. RTA can also generate
network trafﬁc that mimics communication with the at-
tacker’s CNC. RTA provides both procedures that antiviruses
usually do not detect and common procedures that can

4https://docs.google.com/spreadsheets/d/

be detected by antiviruses. RTA does not require disabling
security tools in order to launch most of the procedures.
Overall, RTA is suitable for testing an entire network and is
capable of simulating real-life threats, due to the diversity of
the tactics and techniques. The fact that this tool is Python-
based makes it easier to add advanced techniques and more
sophisticated attacks.

2) Metasploit: Metasploit7 is one of the most commonly
used pen testing toolkits. It focuses on initial access and
execution tactics and contains a large library of vulnera-
bilities and exploits [13]. Metasploit can also be used as
a threat emulator for security assessment thanks to the post-
compromise tools it contains. Metasploit attack modules may
deliver a payload to the compromised endpoint in order to
perform post-compromise activities. Meterpreter is Metas-
ploit’s most well-known payload. Meterpreter provides good
coverage of post-compromise tactics, including discovery,
collection, persistence, privilege escalation, and more. It
employs third party tools, such as psexec and BruteForce, for
lateral movement. Metasploit does not have a library of built-
in multi-procedure attacks, but extensive attack scenarios that
mimic real APTs can be scripted. For example, AutoTTP8
is a scripting package that enables generating and executing
multi-procedure attack scenarios using the APIs of Metas-
ploit and Empire9. AutoTTP organizes the attack procedures
along an attack lifecycle suggested by its developer Jym
Cheong. Most of the attacks performed by Metasploit are
not detected by standard antivirus tools.

Metasploit server components are part of the Kali Linux
OS, but they can also be installed on the Windows OS.
Metasploit target endpoints may run the Windows OS, the
Linux OS, and MacOS. An operator needs to have extensive
cyber security knowledge and some knowledge of Ruby in
order to both launch built-in attacks and add new attack
procedures. To make it easier for the operator, a GUI version
of Metasploit, called Armitage, has been developed10.

3) Invoke-Adversary: Invoke-Adversary11 is a PowerShell
script used by blue teams to test their security mechanisms.
Since Invoke-Adversary is written in PowerShell, it can be
used only on PowerShell supported OSs and on endpoints
that have PowerShell installed and enabled on them.

Invoke-Adversary’s main menu is based on MITRE’s
ATT&CK tactics list. For each attack, the menu allows the
operator to choose a single technique for each tactic. Invoke-
Adversary implements six to 10 procedures for each of the
following tactics: persistence, discovery, credential access,
defense evasion, command and control, and execution. How-
ever, Invoke-Adversary provides only one procedure for the
collection tactic. Lateral movement is not supported. Three
of the six procedures provided by Invoke-Adversary for the
credential access tactic are detected by an antivirus. The other

7https://www.metasploit.com/
8https://github.com/jymcheong/AutoTTP
9Empire is a PowerShell and Python post-compromise agent https:

1zUq1QDHtZRxjde91S31_InMK8alwf5_xdruOR3qttQ8/

//github.com/EmpireProject/Empire

5https://github.com/endgameinc/RTA
6https://github.com/endgameinc/RTA

10http://www.fastandeasyhacking.com/
11https://github.com/CyberMonitor/Invoke-Adversary

three (LSASS dump-based) procedures are rarely detected by
antiviruses. Other procedures provided are common and can
be detected by antiviruses.

Because it is a PowerShell script, Invoke-Adversary can
be modiﬁed by an operator that has sufﬁcient knowledge
in PowerShell and cyber security. Such an operator can
edit procedures, add new custom procedures, and compose
multi-procedure attacks. Invoke-Adversary does not provide
cleanup functionality. It outputs a log to the console at the
end of attack execution, including procedures’ results (if
successful) and the reason for failures (if unsuccessful).

Overall, Invoke-Adversary is suitable for testing a stan-
dalone endpoint, but operators are required to have cyber
security expertise in order to execute attacks that combine
more than one procedure.

4) Blue Team Training Toolkit (BT3): BT312 is a platform
that generates network trafﬁc that mimics malicious activity.
It is primarily used as blue team training kit. BT3 offers free
components and components that must be paid for. The free
components include 14 network attack simulations and 10
methods for generating dummy malicious ﬁles using hash
collision. Some BT3 components require additional third
party tools such as LHOST, SSL, etc. The attack components
cannot be combined into a multi-procedure attack scenario.
BT3 must be installed on a Linux server, however the
network simulation can target endpoints with any OS. Hash
collision can only be performed on the Linux server. BT3
does not require Linux knowledge. It provides install.sh
script for easy setup. However, in order to execute a ma-
licious network trafﬁc simulation, the operator should be
able to recognize attack procedures and the required con-
ﬁgurations based on the procedures’ names. Consequently,
cyber security knowledge is required to correctly operate the
network simulations. The BT3 website provides sufﬁcient
documentation on all BT3 components. BT3 does not have a
cleanup functionality. Files created from hash collision need
to be manually removed by the operator.

Overall, BT3 is suitable for testing an IDS or hash-based
antivirus. The fact that BT3 does not contain multi-procedure
attacks and does not support various tactics and techniques
makes it difﬁcult to use for threat emulation.

5) Advanced Persistent Threat (APT) Simulator: APT-
Simulator13 is a Windows batch script that aims to be as
simple as possible with little user interaction. APTSimulator
runs on Windows with no additional setup requirements, but
it doesn’t support other OSs.

The procedures’ documentation, including execution in-
structions, appears in comments inside the code. However,
with sufﬁcient expertise, it is possible to add new procedures
and create new attacks by editing and combining batch
scripts. Sophisticated procedures and techniques can be cre-
ated by adding third party tools to APTSimulator’s folder and
writing an appropriate batch script that utilizes them. Most of
the procedures implemented in APTSimulator were detected

by an antivirus. The procedures detected by an antivirus are
listed on the tool’s GitHub page. Currently, APTSimulator
implements techniques for persistence and discovery tactics
(eight and seven techniques, respectively), but it lacks lateral
movement capabilities and CNC component. Every proce-
dure executed provides on-screen logging, but there is no
log ﬁle or cleanup.

Overall, APTSimulator is suitable for endpoint testing. It
can be deployed and ran in minutes on Windows environment
and it provides a basic testing for the security of an endpoint.
6) CALDERA: CALDERA14 is a threat emulator devel-
oped by the MITRE organization. It is designed to test the
security of a Windows system [4]. CALDERA includes a
remote access agent, a database, and server components.
CALDERA’s server can be installed on a Windows server
or Windows 10 platforms. CALDERA’s agents can run
on Windows. The installation and conﬁguration process of
CALDERA’s components takes longer than that of most
of the other threat emulators reviewed. In order to use
CALDERA’s remote agent, the antivirus software running
on the endpoint must be disabled.

Once installed, CALDERA’s graphical interface allows
the operator to control all of the agents and obtain visual
feedback about an ongoing attack. All of the attacks in
CALDERA are taken from the MITRE ATT&CK matrix, and
information about the procedures can be found in the MITRE
documentation. Attack procedures can be chosen from the
CALDERA library and combined together to create custom
multi-procedure attacks. CALDERA gives the operator the
option to import his/her own RAT to perform additional pro-
cedures or have different CNC communication. CALDERA’s
lateral movement capabilities allow choosing the ﬁrst com-
promised machine (“patient zero”) out of the endpoints with
CALDERA’s agents. Other tactics that CALDERA covers
are persistence, privilege escalation, and discovery with 7,
6, and 11 techniques respectively. In addition, CALDERA
provides a built-in cleanup functionality.

Overall, CALDERA provides a user friendly interface and
functionality to compile new attacks from existing proce-
dures. CALDERA is most suitable for those who search for
a threat emulator with a large variety of built-in features,
lateral movement, and CNC communication using a RAT,
and a large set of discovery procedures.

7) Infection Monkey: Infection Monkey15 is a threat em-
ulator used for testing and assessing the defenses against lat-
eral movement and discovery tactics (mostly initial access).
Infection Monkey has a CNC component, and a RAT must
be installed on the endpoints. Since antivirus tools consider
the RAT used by Infection Monkey as a threat, antivirus tools
must be disabled for Infection Monkey to function properly.
Infection Monkey’s server can be installed on any OS, and
no additional knowledge is required to launch an attack; its
RAT component can be installed on Windows and Linux OSs
(but not on MacOS).

12https://www.bt3.no/
13https://github.com/NextronSystems/APTSimulator

14https://github.com/mitre/caldera
15https://github.com/guardicore/monkey

The Infection Monkey RAT can be used to choose the
ﬁrst compromised machine from the endpoints. Infection
Monkey reports about machines that were infected in the
process of lateral movement. Infection Monkey was designed
with the aim of testing defenses against lateral movement,
but it lacks a diverse range of tactics and supports few
procedures. The techniques and exploits used by Infection
Monkey come from real-world scenarios, so it closely sim-
ulates a real attacker’s behavior when it comes to lateral
movement. Notably, most of the procedures,
if executed
separately without the RAT, would have remained undetected
by antiviruses. The progress of each executed attack is logged
in the interface, and a detailed review of the results is
provided. Infection Monkey provides a cleanup option which
is useful when examining live systems.

Overall, Infection Monkey is a good threat emulator for
testing network defenses. Moreover,
its easy installation
process together with its graphical user interface, cleanup,
logging, and OS compatibility make it a suitable choice for
novice operators that are looking for an easy way to evaluate
the network environment.

PowerSploit16
aimed
scripts

8) PowerSploit:
of
security
PowerShell-based
assessment tasks. Since PowerSploit is based on PowerShell,
it runs natively on any Windows OS with PowerShell
installed. In addition, there are add-ons that can be installed
on MacOS and Linux OS to enable using PowerShell.

a
various

collection

is
at

PowerSploit does not support adding new procedures, but
PowerSploit’s scripts can be combined into a custom multi-
procedure attack. PowerSploit supports a high number of so-
phisticated procedures. As a result it requires cyber security
expertise in addition to PowerShell knowledge. PowerSploit
supports persistence, privilege escalation, defense evasion,
and discovery tactics with 18, 9, 18, and 18 techniques
respectively. It also supports credential access, collection,
tactics with seven, seven, and six
and lateral movement
techniques, respectively. Every executed procedure provides
on-screen logging, but there is no log ﬁle. PowerSploit lacks
cleanup and logging functionalities.

Overall, PowerSploit can be a quick and easy baseline
assessment tool. It is well documented, and it contains a
large number of procedures, and more speciﬁcally, many
procedures that try to avoid getting caught by antiviruses.

9) DumpsterFire: DumpsterFire17 is a platform for build-
ing attacks using techniques that are largely not part of the
MITRE ATT&CK matrix. For example, the creation of 5,000
ﬁles, searching for hacking tools in Google, and running
a video in a loop on YouTube. DumpsterFire can only be
executed on Linux systems. DumpsterFire does not have a
cleanup option, and documentation only appears on their
GitHub page and within the procedure scripts in the form
of comments. Logs of the execution of each procedure are
shown on the CLI.

A multi-procedure attack is built by choosing techniques

from a menu. DumpsterFire provides a set of built-in multi-
procedure attacks packed with a well-built CLI. In addition,
DumpsterFire implements a variety of discovery and creden-
tial access techniques (13 and 6 techniques, respectively).
However, DumpsterFire lacks CNC and lateral movement
techniques. Overall, DumpsterFire supports a small number
the
of techniques covering just a few tactics. However,
techniques it contains and its simple interface make Dump-
sterFire useful for examining the basic security of Linux
endpoints in an easy and accessible manner.

10) Uber’s Metta: Metta18 is a threat emulator developed
by Uber Technologies. Its main purpose is assessing endpoint
security, but it also includes a set of network security testing
procedures. Uber’s Metta can be executed on endpoints with
the Linux OS, the Windows OS, and MacOS. In order to use
Metta, an organization needs to install a Redis server, Python
2.7, and Vagrant, which makes the installation process a bit
more complicated than that of other threat emulators.

Metta provides built-in attacks, each of which executes all
of the techniques that achieve a speciﬁc tactic. For example,
an attack that consists of all techniques that collect a user’s
data. Such an attack does not emulate the complete attack
lifecycle, but rather focuses on a wide assessment of speciﬁc
defense targets. Metta also provides the operator with the
ability to create custom multi-procedure attacks. However,
the operator should have coding experience in order to add,
launch, and manage attacks. Metta supports CNC and lateral
movement tactics. But lateral movement procedures are only
implemented for Linux OS. Metta also provides various
techniques for achieving discovery, credential access, and de-
fense evasion tactics (10, 7, and 7 techniques, respectively).
Persistence, privilege escalation, collection, and exﬁltration
tactics are supported by Metta with just a few techniques.
Most of the procedures implemented in Metta were not
detected by antivirus tools. For each attack there is on-screen
logging, and a log ﬁle is generated at the end of the attack.
Metta has no cleanup functionality.

Overall, Metta is suitable for operators who need to test

speciﬁc parts of endpoint security across many platforms.

11) Atomic Red Team: Atomic Red Team is a threat
emulation library of lightweight security tests that can be
rapidly executed by security teams19. Attack scenarios can
be executed using command line, PowerShell, and Shell. It
is compatible with all major operating systems. Atomic Red
Team also provides an API written in Ruby.

Atomic Red Team supports multiple techniques for achiev-
ing persistence, privilege escalation, defense evasion, cre-
dential access, and discovery tactics (20, 9, 24, 9, and 16
techniques, respectively). It also includes procedures for
lateral movement and simulates CNC communication. All
procedures are mapped to the MITRE ATT&CK matrix. It
is possible to add new custom procedure scripts. In addition,
Atomic Red Team uses third party tools, such as mimikatz,
for performing credential dumping techniques. Most of the

16https://github.com/PowerShellMafia/PowerSploit
17https://github.com/TryCatchHCF/DumpsterFire

18https://github.com/uber-common/metta
19https://atomicredteam.io/,https://github.com/

redcanaryco/atomic-red-team

procedures are undetected by antivirus tools. Atomic Red
Team has cleanup at the procedure level. It also provides on-
screen logging which outputs information about the currently
running procedure during attack execution. Documentation is
provided only as comments in the procedures’ scripts.

One of the main objectives of Atomic Red Team is
allowing the operator to execute procedures quickly, with
minimal installation, however Atomic Red Team may require
cyber security knowledge in order to be used effectively.

VI. TAXONOMY OF THREAT EMULATORS
In this section, we summarize the ﬁndings from our review
of the emulators in a taxonomy that highlights their qualities.
The emulator review questionnaire (see Appendix VIII) was
completed by a review panel, based on factual assessment
of the emulators. The criteria values were inferred from the
questionnaire (see supplementary material) and aggregated
according to the criteria hierarchy (Fig. 2).

Fig. 3 presents the strengths and weaknesses of the em-
ulators with respect to four criteria dimensions: operators,
environment, scenario deﬁnition, and scenario execution.
Inspecting the highest aggregation level (functionality vs.
operation) in Fig. 3a, we can identify four outstanding threat
emulators in the top right corner: RTA, Atomic Red Team,
CALDERA, and Metasploit, which form the Pareto frontier.
These emulators exhibit the best trade-off between the ease
of setting up and operating the emulator in a wide range
of environments (operation) and the ability to deﬁne and
execute diverse attack scenarios (functionality).

Further inspecting the operator dimension using Fig. 3b,
we can identify Infection Monkey, CALDERA, and BT3 as
the emulators that are most operator friendly. Metasploit is
outstanding in terms of environment criteria, supporting all
operating systems with minimal prerequisites.

Atomic Red Team, RTA, CALDERA, and Infection Mon-
key provide the most features related to controlling and
logging attack execution (Fig. 3c).

With respect to the diversity and ﬂexibility of attack sce-
nario deﬁnition, it is also important to consider the summary
of tactics as presented in Table I. Higher diversity means
that the red team is able to create more attack scenarios,
and the blue team may gain better understanding of the
security array’s limitations. The top emulators according to
the scenario deﬁnition criteria also have the highest diversity
of supported techniques (except for DumpsterFire).

It

is apparent from Table I that Metasploit and RTA
pay much less attention to the discovery tactic than their
competitors. On the other hand CALDERA, Uber’s Metta,
and DumpsterFire emulators implement many discovery
techniques (which are usually noisy), while providing less
support for persistence and defense evasion. These three
emulators may be a good choice when evaluating anomaly
detection methods.

A. Selecting a Threat Emulator

Based on the taxonomy above and the threat emulation use
cases, we formulate guidelines for choosing the appropriate
threat emulator.

Fig. 3. Qualities of the threat emulators

TABLE I

SUMMARY OF TACTICS

Training. Since training often takes place in a controlled envi-
ronment, like a cyber range, the environmental requirements
are looser than, for example, in the organizational security
assessment use case. Because emulators are used to train blue
teams in handling a variety of security events, they should
be scored high in the attack scenario deﬁnition criteria. As
a case in point, emulators that support a variety of tactics
and techniques are preferred. When training exercises are
managed by experienced instructors, the operators criteria
do not need to be considered. As a result, Metasploit, RTA,
Atomic Red Team, and CALDERA are the top candidates
for training exercises (see Fig. 3c). An experienced red team
may prefer DumpsterFire over PowerSploit despite the lower
number of tactics, because DumpsterFire supports better
attack customization. However, in the case of a blue team
self-training, a qualiﬁed red team may be absent; in that
scenario, an operator without extensive security expertise will
prefer emulators scoring high in both the scenario deﬁnition
and operators criteria, such as CALDERA (see Fig. 3).
Security Tool Assessment. Emulators scoring high on the
environment criteria widen the range of tools that can be
assessed using the emulator, and ease of operation increases
the productivity of the assessment. Attack deﬁnition crite-
ria and in particular the variety of supported tactics and
techniques are highly important in challenging the security
tool in as many scenarios as possible. Conﬁgurability is
also important for supporting the repeated execution of
simulated attacks with different security products to compare
and contrast them. Although emulators’ advanced logging
capabilities are beneﬁcial during security tool assessment,
stopping the attack mid-runtime and cleanup are not required,
making the execution criteria less important. Of Atomic
Red Team, RTA, CALDERA, Metasploit, DumpsterFire and
PowerSploit which all score high on the scenario deﬁnition
criteria, Metasploit and CALDERA exhibit the best operator
vs. environment trade-off.
Organizational Security Assessment implies the execution
of a threat emulator within the operational organizational
environment. In such settings, all scenario execution criteria,
especially cleanup and the ability to stop an emulated attack
mid-runtime, are crucial for troubleshooting the assessment
process. All environment criteria (OS compatibility, special
privileges, etc.) are important for seamless integration in the
existing organizational environment and reduce the disrup-
tion of the organization’s operations. Based on the scenario
execution and environment criteria, Atomic Red Team and
RTA score the highest, because they support cleanup and
have loose environment constraints (see Figs. 3c and 3b).
What-If Analysis requires repeated, preferably automated, ex-
ecution of an attack scenario with slight changes in either the
security array or attack deﬁnition. One of the main objectives
of what-if analysis is assessment of the potential impact
of a security event on the organization. As such it is best
performed on the organizational premises, resulting in threat
emulators that intertwine with the operational organizational
environment. Overall, the most important criteria for what-if
analysis are the environment criteria, as well as the cleanup

and procedures’ conﬁguration criteria. Atomic Red Team
and RTA support cleanup and also have the highest score
with respect to the procedures’ conﬁguration (see criteria
values in the supplementary material) and are second only
to Metasploit with respect to the environment criteria (see
Fig. 3b).
General Considerations. As can be seen in Fig. 3b, a novice
red team member who is technology savvy but doesn’t have
cyber security expertise would prefer the Infection Monkey
or CALDERA threat emulator.

If the purpose is to determine whether a blue team
can handle advanced persistence threats (APT), the threat
emulator should support a high level of attack scenario
customization, including adding new procedures and custom
multi-procedure attacks. APT emulators should have high
TTP coverage in general and in particular support many
lateral movement and defense evasion techniques; as can
be seen in Fig. 3c,
the most appropriate emulators are
Metasploit, Atomic Red Team, RTA, and CALDERA. When
the focus is testing advanced adversarial techniques, such
as process injection, ﬁle encryption, or credential dumps,
PowerSploit is a better choice than CALDERA.

VII. SUMMARY

Threat emulators are valuable tools for training security
personnel, security assessment, and what-if analysis. In this
article we reviewed 11 open-source threat emulators and pre-
sented a detailed and comprehensive evaluation methodology
for qualitatively and quantitatively comparing them.

The review is based on well-deﬁned criteria organized
in four dimensions as discussed in Section IV. The re-
sults identify four open-source threat emulators (Atomic
Red Team, RTA, CALDERA and Metasploit) that lead the
way according to our summary of the criteria. Also worth
mentioning is Infection Monkey, which stands out as an
operator friendly threat emulator. Based on the taxonomy
presented, we formulate guidelines for selecting the most
appropriate threat emulator in a given use case. Finally, our
evaluation methodology and taxonomy highlight the need to
address the following aspects which are crucial for the further
development of threat emulators:

• Cleanup and conﬁgurability are important in order to
repeat and automate the execution of attack scenarios
during security tool assessment and what-if analysis.
• An emulator should support cleanup after the comple-
tion of the attack scenario, like CALDERA, Atomic Red
Team, and Infection Monkey do, rather than after each
individual procedure.

• An API, currently provided by Atomic Red Team,
CALDERA, and Metasploit, facilitates integration be-
tween the threat emulators and organizational security
array, thus enabling periodic and systematic security
assessment.

• It is important to provide a GUI and ready to execute
multi-procedure attacks for novice operators as well as a
CLI to support automation and advanced customization
capabilities.

REFERENCES

[1] sqlmap: Automatic

sql

injection and database

takeover

tool.

http://sqlmap.org/. Accessed on: 2019-11-10.

[2] w3af: Web application attack and audit framework. http://w3af.org/.

Accessed on: 2019-11-10.

[3] Gary Adkins. Red teaming the red team: Utilizing cyber espionage to
combat terrorism. Journal of Strategic Security, 6(3):1–9, 2013.
[4] Andy Applebaum, Doug Miller, Blake Strom, Chris Korban, and Ross
Wolf. Intelligent, automated red team emulation. In Proceedings of the
32Nd Annual Conference on Computer Security Applications, ACSAC
’16, pages 363–373, New York, NY, USA, 2016. ACM.

[5] Fabrizio Baiardi. Avoiding the weaknesses of a penetration test.

Computer Fraud & Security, 2019(4):11–15, 2019.
Common

[6] Sean Barnum.
classiﬁcation
http://capec.mitre.org/documents/documentation/CAPEC Schema Descr
iption v1, 3, 2008.

pattern
description.

enumeration
Cigital

and
Inc,

schema

(capec)

attack

[7] M. Bishop. About penetration testing. IEEE Security Privacy, 5(6):84–

87, Nov 2007.

[8] Marcel B¨ohme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik
Roychoudhury. Directed greybox fuzzing. In Proceedings of the 2017
ACM SIGSAC Conference on Computer and Communications Security,
CCS ’17, pages 2329–2344, New York, NY, USA, 2017. ACM.
security incidents

in
[9] Red Canary.
https://atomicredteam.io/,https://github.
a box!
com/redcanaryco/atomic-red-team. Accessed on: 2020-
04-14.

Dumpsterﬁre

toolset

-

[10] Patrick Engebretson. The basics of hacking and penetration testing:

ethical hacking and penetration testing made easy. Elsevier, 2013.

[11] Mike Gault. The cia secret to cybersecurity that no one seems to get,

Dec 2015.

[12] D. S. Henshel, G. M. Deckard, B. Lufkin, N. Buchler, B. Hoffman,
P. Rajivan, and S. Collman. Predicting proﬁciency in cyber defense
team exercises. In MILCOM 2016 - 2016 IEEE Military Communica-
tions Conference, pages 776–781, Nov 2016.

[13] Filip Holik, Josef Horalek, Ondrej Marik, Sona Neradova, and
Stanislav Zitta. Effective penetration testing with metasploit frame-
work and methodologies. In 2014 IEEE 15th International Symposium
on Computational Intelligence and Informatics (CINTI), pages 237–
242. IEEE, 2014.

[14] Christian Holler, Kim Herzig, and Andreas Zeller. Fuzzing with
In Presented as part of the 21st USENIX Security
code fragments.
Symposium (USENIX Security 12), pages 445–458, Bellevue, WA,
2012. USENIX.

[15] Eric M Hutchins, Michael

J Cloppert, and Rohan M Amin.
Intelligence-driven computer network defense informed by analysis
of adversary campaigns and intrusion kill chains. Leading Issues in
Information Warfare & Security Research, 1(1):80, 2011.

[16] Joint Task Force Transformation Initiative.

Security and privacy
controls for federal information systems and organizations. NIST
Special Publication, 800(53):8–13, 2013.

[17] Devon Kerr.

Introducing

endgame

red

team automation.

www.endgame.com/blog/technical-blog/introducing-endgame-red-
team-automation, March 2018. Accessed on: 2019-11-10.

[18] Robert M. Lee, Michael J. Assante, and Tim Conway. Analysis of
the cyber attack on the ukrainian power grid. Electricity Information
Sharing and Analysis Center (E-ISAC), 2016.

[19] Gordon Fyodor Lyon. Nmap Network Scanning: The Ofﬁcial Nmap
Project Guide to Network Discovery and Security Scanning. Insecure,
USA, 2009.

[20] Steve Mansﬁeld-Devine. The best form of defence – the beneﬁts of

red teaming. Computer Fraud & Security, 2018(10):8 – 12, 2018.

[21] Robin Mejia. Red team versus blue team: how to run an effective

simulation. CSO Online-Security and Risk, 2008.

[22] J. Mirkovic, P. Reiher, C. Papadopoulos, A. Hussain, M. Shepard,
M. Berg, and R. Jung. Testing a collaborative ddos defense in
IEEE Transactions on Computers,
a red team/blue team exercise.
57(8):1098–1112, Aug 2008.

[23] Michael Muehlberghuber, Frank K. G¨urkaynak, Thomas Korak,
Philipp Dunst, and Michael Hutter. Red team vs. blue team hardware
trojan analysis: Detection of a hardware trojan on an actual asic. In
Proceedings of the 2Nd International Workshop on Hardware and
Architectural Support for Security and Privacy, HASP ’13, pages 1:1–
1:8, New York, NY, USA, 2013. ACM.

[24] Jacob G. Oakley. Professional Red Teaming: Conducting Successful

Cybersecurity Engagements. APress, 1st edition, 2019.

[25] Paul Pols. The uniﬁed kill chain: designing a uniﬁed kill chain for
analyzing, comparing and defending against cyber attacks. Master’s
thesis, Cyber Security Academy, The Hague, December 2017.
[26] Shirley Radack. Guide to information security testing and assessment.
Technical report, National Institute of Standards and Technology,
2008.

[27] J. Rajendran, V. Jyothi, and R. Karri. Blue team red team approach
In 2011 IEEE 29th International
to hardware trust assessment.
Conference on Computer Design (ICCD), pages 285–288, Oct 2011.
[28] S. Rastegari, P. Hingston, Chiou-Peng Lam, and M. Brand. Testing
a distributed denial of service defence mechanism using red teaming.
In 2013 IEEE Symposium on Computational Intelligence for Security
and Defense Applications (CISDA), pages 23–29, April 2013.
[29] Aunshul Rege and Joe Adams. The need for more sophisticated
cyber-physical systems war gaming exercises. In ECCWS 2019 18th
European Conference on Cyber Warfare and Security, page 403.
Academic Conferences and publishing limited, 2019.

[30] Russ Rogers. Nessus network auditing. Elsevier, 2011.
[31] Blake E Strom, Joseph A Battaglia, Michael S Kemmerer, William
Kupersanin, Douglas P Miller, Craig Wampler, Sean M Whitley, and
Ross D Wolf. Finding cyber threats with att&ck-based analytics.
Technical report, Technical Report MTR170202, MITRE, 2017.
[32] Paul Voigt and Axel Von dem Bussche. The EU general data protection
regulation (GDPR). A Practical Guide, 1st Ed., Cham: Springer
International Publishing, 2017.

[33] T. Wang, T. Wei, G. Gu, and W. Zou. Taintscope: A checksum-aware
directed fuzzing tool for automatic software vulnerability detection.
In 2010 IEEE Symposium on Security and Privacy, pages 497–512,
May 2010.
[34] JD Work.

In wolf’s clothing: Complications of threat emulation
In 2019 International
in contemporary cyber intelligence practice.
Conference on Cyber Security and Protection of Digital Services
(Cyber Security), pages 1–8. IEEE, 2019.

VIII. APPENDIX: EMULATOR EVALUATION
QUESTIONNAIRE

The following is a conditional questionnaire where the
answers to certain questions may affect the relevance of
other questions. The term attack may refer to single or
multi-procedure attack scenarios. Explicit reference to multi-
procedure attacks or to procedures is provided where rele-
vant.

A. Environment

Questions in this section relate to the interactions between
the emulator and the environment in which it is deployed.
The following questions relate to the emulator’s main com-
ponents and dependencies.

Yes No
(cid:50)
(cid:50)
(cid:50)
(cid:50)

Q1: Does the emulator use agents?
Q2: Does the emulator execute scripts on the
endpoints?
Q3: Does the emulator execute third party
tools on the endpoints?
1) OS compatibility: W, L, and M refer to the Windows,

(cid:50)

(cid:50)

Linux, and Mac operating systems respectively.

Q4: [Q1=Yes] Are the emulator’s agents
compatible with the following operating
systems?
Q5: [Q2=Yes] Does the emulator contain
procedures compatible with the following
operating systems?
Q6: [Q3=Yes] Does the emulator use third
party tools compatible with the following
operating systems?
2) Changes to the security array:
questions refer to built-in attacks only.

W L M
(cid:50) (cid:50) (cid:50)

(cid:50) (cid:50) (cid:50)

(cid:50) (cid:50) (cid:50)

The following

Q7: Did the system’s ﬁrewall interrupt the
emulator’s workﬂow?
Q8: [Q7=Yes] Did the system’s ﬁrewall block
the connection between the CNC and the
RAT?
Q9: Did the system’s real-time antivirus in-
terrupt with the emulator’s workﬂow?
Q10: [Q9=Yes] Did the system’s real-time
antivirus interrupt any RAT functionality?
Q11: [Q9=Yes][Q1=Yes] Did the system’s
real-time antivirus interrupt the agents’ func-
tionality?
Q12: [Q9=Yes][Q2=Yes] Did the system’s
real-time antivirus interrupt the scripts’ func-
tionality? (i.e., delete PowerShell scripts)
Q13: [Q9=Yes][Q3=Yes] Did the system’s
real-time antivirus interrupt
the third party
tools’ functionality?
3) Prerequisites:

in attacks only.

Q14: [Q1=Yes] Do the emulator’s agents re-
quire special privileges from the systems to
take advantage of their full functionality?
Q15: [Q2=Yes] Do the emulator’s scripts re-
quire special privileges from the systems to
take advantage of their full functionality?
Q16: [Q3=Yes] Do the third party tools re-
quire special privileges from the systems to
take advantage of their full functionality?

Yes No
(cid:50)
(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

The following questions refer to built-

B. Operators

Questions in this section assess the ease of operating the

emulator.

1) Required security expertise: Consider a novice red
team member, such as a software engineer or computer
science student, who is technology savvy but doesn’t have
cyber security expertise.

Q17: Can the novice red team member exe-
cute built-in attacks by following the instruc-
tions provided with the emulator?
2) Documentation:

Yes No
(cid:50)
(cid:50)

Q18: Does the emulator have any kind of
documentation?
Q19: [Q18=Yes] Is the documentation suf-
ﬁcient for setting up all of the emulator’s
components?
Q20: [Q18=Yes] Is the documentation sufﬁ-
cient for launching built-in attacks?
Q21: [Q18=Yes][Q36=Yes] Is the documen-
tation sufﬁcient for creating new custom pro-
cedures?
Q22: [Q18=Yes][Q43=Yes] Is the documen-
tation sufﬁcient
for creating new multi-
procedure attack scenarios?
Q23: [Q18=Yes] Does the documentation de-
scribe how to interpret attack execution re-
sults?
3) Interface:

Yes No
(cid:50)
(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

In the following matrix indicate whether
or not the functionality (row) is available through the UI
(column).

Q24: Executing attacks
Q25: Conﬁguring procedures
Q26: Stopping attacks mid-runtime
Q27: [Q34=Yes] Accessing logs
Q28:
new custom procedures
Q29: [Q43=Yes] Adding new multi-
procedure attacks

[Q36=Yes∨37=Yes] Adding

GUI CLI API
(cid:50)
(cid:50)
(cid:50)
(cid:50)
(cid:50)
(cid:50)
(cid:50)
(cid:50)
(cid:50)
(cid:50)

(cid:50)
(cid:50)
(cid:50)
(cid:50)
(cid:50)

(cid:50)

(cid:50)

(cid:50)

Q30: Does the emulator support attack script-
ing?

Yes No
(cid:50)
(cid:50)

Yes No
(cid:50)
(cid:50)

C. Scenario execution

Questions in this section assess the emulator’s functional-

ity related to attack scenario execution.

(cid:50)

(cid:50)

1) Cleanup:

Q31: Does the emulator have cleanup func-
tionality?
Q32: [Q31=Yes] Does cleanup occur imme-
diately after the relevant attack procedure?
[Q31=Yes][Q42=Yes∨Q43=Yes] Can
Q33:
cleanup of the relevant procedures be exe-
cuted only at the end of a multi-procedure
attack?
2) Logs:

Q34: Does the emulator have logging capa-
bilities?
Q35: [Q34=Yes] Is every executed procedure
logged during an attack?

Yes No
(cid:50)
(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

Yes No
(cid:50)
(cid:50)

(cid:50)

(cid:50)

D. Scenario deﬁnition

Questions in this section assess the functionality related

to deﬁning and conﬁguring attack scenarios.

1) Adding new procedures:

Q36: Does the emulator support adding new
custom procedures through any of the inter-
faces?
Q37: [Q36=No] Are procedures implemented
using scripts?
Q38: [Q36=No][Q37=Yes] Can a new script
be added to the collection of procedures?
2) Procedures’ conﬁguration:

Q39: Can the emulator procedures be conﬁg-
ured through conﬁguration ﬁles?
Q40: [Q39=No] Does the emulator support
repeated execution of the same attack with
different parameters?
Q41: [Q36=Yes∨Q38=Yes] Can new custom
procedures be conﬁgured using the same
methods as built-in procedures (e.g., various
interfaces or conﬁguration ﬁles)?
3) Multi-procedure attacks:

Yes No
(cid:50)
(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

Yes No
(cid:50)
(cid:50)

(cid:50)

(cid:50)

(cid:50)

(cid:50)

Yes No
(cid:50)
(cid:50)

(cid:50)

(cid:50)

include built-in

Q42: Does the emulator
multi-procedure attacks?
Q43: Does the emulator support adding new
custom multi-procedure attacks?
4) TTP coverage:

In order
questions, ﬁll-in the MITRE ATT&CK matrix20
the

the following
for
procedures.

considering

to answer

emulator

built-in

only

Q44: The number of tactics is

Q45: The number of techniques
is

<6
(cid:50)
<20
(cid:50)

6 to 8
(cid:50)

>8
(cid:50)

20 to 40 >40

(cid:50)

(cid:50)

IX. APPENDIX: THREAT EMULATORS EVALUATION
PROCESS

20https://attack.mitre.org/matrices/enterprise/

Fig. 4. Threat emulator evaluation process

