Crafting Adversarial Examples For Speech Paralinguistics
Applications

Yuan Gong
Department of Computer Science and Engineering
University of Notre Dame
ygong1@nd.edu

Christian Poellabauer
Department of Computer Science and Engineering
University of Notre Dame
cpoellab@nd.edu

9
1
0
2

n
a
J

1
1

]

G
L
.
s
c
[

2
v
0
8
2
3
0
.
1
1
7
1
:
v
i
X
r
a

ABSTRACT
Computational paralinguistic analysis is increasingly being used in
a wide range of cyber applications, including security-sensitive ap-
plications such as speaker veriﬁcation, deceptive speech detection,
and medical diagnostics. While state-of-the-art machine learning
techniques, such as deep neural networks, can provide robust and
accurate speech analysis, they are susceptible to adversarial at-
tacks. In this work, we propose an end-to-end scheme to generate
adversarial examples for computational paralinguistic applications
by perturbing directly the raw waveform of an audio recording
rather than speciﬁc acoustic features. Our experiments show that
the proposed adversarial perturbation can lead to a signiﬁcant per-
formance drop of state-of-the-art deep neural networks, while only
minimally impairing the audio quality.

KEYWORDS
Paralinguistics, adversarial examples, speech processing, computer
security

ACM Reference Format:
Yuan Gong and Christian Poellabauer. 2018. Crafting Adversarial Examples
For Speech Paralinguistics Applications. In DYnamic and Novel Advances in
Machine Learning and Intelligent Cyber Security Workshop (DYNAMICS ’18),
December 3–4, 2018, San Juan, Puerto Rico, USA. ACM, New York, NY, USA,
8 pages. https://doi.org/10.1145/3306195.3306196

1 INTRODUCTION
1.1 Background
Computational speech paralinguistic analysis is rapidly turning
into a mainstream topic in the ﬁeld of artiﬁcial intelligence. In
contrast to computational linguistics, computational paralinguis-
tics analyzes how people speak rather than what people say [24].
Studies have shown that human speech not only contains the basic
verbal message, but also paralinguistic information, which can be
used (when combined with machine learning) in a wide range of
applications, such as speaker veriﬁcation [10, 18, 31], speech emo-
tion recognition [8, 29], conversation analysis [16, 17], speech de-
ception detection [26], and medical diagnostics [2, 7, 20, 23]. Many

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
DYNAMICS ’18, December 3–4, 2018, San Juan, Puerto Rico, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-6218-4/18/12. . . $15.00
https://doi.org/10.1145/3306195.3306196

(cid:14)(cid:3)

(cid:104)0.02(cid:3)

Speaker  
Verification  
System(cid:3)

Reject(cid:3)

Accept(cid:3)

Figure 1: Illustration of an attack on a speaker veriﬁcation
system: the attacker can make the system accept an illegal
input by adding a well-designed small perturbation.

of these applications are cloud-based, security sensitive, and must
be very reliable, e.g., speaker veriﬁcation systems used to prevent
unauthorized access should have a low false positive rate, while
systems used to detect deceptive speech should have a low false
negative rate. A threat to these types of systems, which has not yet
found widespread attention in the speech paralinguistic research
community, is that the machine learning models these systems rely
on can be vulnerable to adversarial attacks, even when these mod-
els are otherwise very robust to noise and variance in the input
data. That is, such systems may fail even with very small, but well-
designed perturbations of the input speech, leading the machine
learning models to produce wrong results.

An adversarial attack can lead to severe security concerns in
security-sensitive computational paralinguistic applications. Since
it is diﬃcult for humans to distinguish adversarial examples gener-
ated by an attacking algorithm from the original legitimate speech
samples, it is possible for an attacker to impact speech-based au-
thentication systems, lie detectors, and diagnostic systems (e.g.,
Figure 1 illustrates an example of attacking a speaker veriﬁcation
system, which could allow a burglar to enter a house by deceiv-
ing a voice-based smart lock). In addition, with rapidly growing
speech databases (e.g., recordings collected from speech-based IoT
systems), machine learning techniques are increasingly being used
to extract hidden information from these databases. However, if
these databases are polluted with adversarial examples, the con-
clusions drawn from the analysis can be false or misleading.

As a consequence, obtaining a deeper understanding of this prob-
lem will be essential to learning how to prevent future adversarial
attacks from impacting speech analysis. Toward this end, this pa-
per proposes a method to generate adversarial speech examples
from original examples for speech paralinguistics applications. Specif-
ically, instead of manipulating speciﬁc speech acoustic features,
our goal is to perturb the raw waveform of an audio directly. Our

 
 
 
 
 
 
DYNAMICS ’18, December 3–4, 2018, San Juan, Puerto Rico, USA

Yuan Gong and Christian Poellabauer

experiments show that the resulting adversarial speech examples
are able to impact the outcomes of state-of-the-art paralinguistic
models, while aﬀecting the sound quality only minimally.

1.2 Related Work
1.2.1 Computer Vision. The vulnerability of neural networks to
adversarial examples, particularly in the ﬁeld of computer vision,
was ﬁrst discovered and discussed in [27]. In [9], the authors ana-
lyzed the reasons and principles behind the vulnerability of neural
networks to adversarial examples and proposed a simple yet eﬀec-
tive gradient based attack approach, which has been widely used
in later studies. In recent years, properties of adversarial attacks
have been studied extensively, e.g., in [14], the authors found that
machine learning models may make wrong predictions even when
using images that are fed to the model via a camera instead of di-
rectly applying the adversarial examples as input. Further, in [21],
the authors found that it is possible to attack a machine learning
model even without knowing the details of the model.

1.2.2
Speech and Audio Processing. In [4, 30], the authors proposed
an approach to generate hidden voice commands, which can be
used to attack speech recognition systems. These hidden voice com-
mands are reconstructed from Mel-frequency cepstral coeﬃcients
(MFCC). They are unrecognizable by the human ear and not simi-
lar to any legitimate speech. Hence, strictly speaking, they are not
machine learning adversarial examples. In [12], the authors pro-
posed an approach to generate adversarial examples of music by
applying perturbations on the magnitude spectrogram. However,
rebuilding time-domain signals from magnitude spectrograms is
diﬃcult, because of the overlapping windows used for analysis,
which makes adjacent components in the spectrogram dependent.
Similarly, in [6], the authors add perturbations to the magnitude
spectrograms and make the adversarial examples fail to be recog-
nized by both known or unknown automatic speech recognition
systems. In [11], the authors also proposed an approach to gener-
ate adversarial speech examples, which can mislead speech recog-
nition systems. They ﬁrst extract MFCC features from the origi-
nal speech, add perturbations to the MFCC features, and then re-
build speech from the perturbed MFCC features. While the rebuilt
speech samples are still recognizable by the human ear, they are
very diﬀerent from the original samples, because extracting MFCC
from audio is a lossy conversion. In [1, 5, 28], the authors proposed
targeted attacks on deep neural network based speech recognition
system.

adversarial examples sound strange to the human ear (i.e., they
become recognizable). In comparison, in the computer vision ﬁeld,
adversarial examples are perturbed directly on the pixel value level
and only contain very minor amounts of noise that are diﬃcult or
even impossible to detect by the human eye. Therefore, in order to
avoid the downsides of performing the inverse conversions from
features to audio, in this work we propose an end-to-end approach
to crafting adversarial examples by directly modifying the original
waveforms.

1.3 Contributions of this Paper

• To the best of our knowledge, this is the ﬁrst work on ad-
versarial attacks in the ﬁeld of computational speech par-
alinguistics. We expect that our results and discussions will
bring useful insights for future studies and eﬀorts in build-
ing more robust machine learning models.

• We propose an end-to-end speech adversarial example gen-
eration scheme that does not require an audio reconstruc-
tion step. The perturbation is added directly to the raw wave-
form rather than the acoustic features or spectrogram fea-
tures so that no lossy conversion from the features back to
the waveform is needed.

• We describe the vanishing gradients problem when using
a gradient based attack approach on recurrent neural net-
works. We address this problem by using a substitution net-
work that replaces the recurrent structure with a feed-forward
convolutional structure. We believe that this solution is not
limited to our application, but can be used in other applica-
tions with sequences of inputs.

Feature Extraction(cid:3)

Acoustic Features(cid:3)

• We provide comprehensive experiments with three diﬀer-
ent speech paralinguistics tasks, which empirically prove
the eﬀectiveness of the proposed approach. The experimen-
Audio Reconstruction(cid:3)
tal results also indicate that the adversarial examples can be
generalized to other models.
(a) Feature level Attack(cid:3)

Perturbed Features(cid:3)

Perturbation(cid:3)

Attack Model(cid:3)

Original Audio(cid:3)

End-to-end  
Hypothetical 
Model (cid:3)

Perturbation(cid:3)

FGSM(cid:3)

Attack Model(cid:3)

Adversarial Example(cid:3)

Original Audio(cid:3)

Adversarial Example(cid:3)

To the best of our knowledge, all prior speech-related eﬀorts aim
to attack speech recognition (i.e., speech to text) systems. Speech
recognition (typically a transcription task) is very diﬀerent from
computational paralinguistic tasks (which are typically classiﬁca-
tion or regression tasks). As mentioned in Section 1.1, adversarial
attacks on computational paralinguistic applications can also lead
to severe security concerns, but they have not been studied before.
Therefore, it will be interesting to study if adversarial attacks are
also eﬀective in computational paralinguistics applications. Fur-
ther, most prior eﬀorts add perturbations at the feature level and re-
quire a reconstruction step using acoustic and spectrogram features,
which will further modify and impact the original data (in addition
to the actual perturbation). This additional modiﬁcation can make

Figure 2: The proposed end-to-end adversarial attack
scheme.

2 SPEECH ADVERSARIAL EXAMPLES
(b) Proposed End to end Attack(cid:3)
The block diagram of the proposed speech adversarial attack scheme
is shown in Figure 2. In this section, we ﬁrst provide a formal def-
inition of the adversarial example generation problem. We then
brieﬂy review the gradient based approach used in this work and
discuss its use in our application. We explain the limitations of
acoustic-feature-level adversarial attacks and why building an end-
to-end speech adversarial attack scheme is essential. Further, we

Crafting Adversarial Examples For Speech Paralinguistics Applications DYNAMICS ’18, December 3–4, 2018, San Juan, Puerto Rico, USA

discuss the selection of a hypothetical model for the attack. Finally,
we describe the vanishing gradient problem in gradient based at-
tacks on recurrent neural networks and address this problem using
a model substitution approach.

2.1 Deﬁnitions
We describe a classiﬁer that maps raw speech audio waveform vec-
tors into a discrete label set as f : Rn → {1...k } and the param-
eters of f as θ. We further assume that there exists a continuous
loss function associated with f described as J : Rn × {1...k } → R+
.
We describe a given speech audio waveform using x ∈ Rn, the
ground-truth label of x using y, and a perturbation on the vector
using η ∈ Rn. Our goal is to address the following optimization
problem:

Minimize kηk

s.t. f (x + η) , f (x)

(1)

2.2 Gradient Based Adversarial Attacks
Due to the non-convexity of many machine learning models (e.g.,
deep neural networks), the exact solution of the optimization in
Equation 1 can be diﬃcult to obtain. Therefore, gradient based
methods [9] have been proposed to ﬁnd an approximation of such
an optimization problem. The gradient based method under the
max-norm constraint is referred to as Fast Gradient Sign Method
(FGSM). The FGSM is then used to generate a perturbation η:

η = ϵsign(∇xJ (θ, x, y))

(2)

Let w be the weight vector of a neuron. Consider the situation
when the neuron takes the entire waveform x as input, then, when
we add a perturbation η, the activation of the neuron will be:

activation = wT(x + η) = wTx + wTη

The change of the activation is then expressed as:

∆ activation = wTη

= ϵwTsign(∇xJ (w, x, y))
= ϵwTsign(w)

(3)

(4)

Assume that the average magnitude of w is expressed as m, then

note that w has the same dimension with x of n:

∆ activation = ϵmn

(5)

Since kηk∞ does not grow with n, we ﬁnd that even when kηk∞
is ﬁxed, the change of the activation of the neuron grows linearly
with the dimensionality n, which indicates that for a high dimen-
sional problem, even a small change in each dimension can lead to
a large change of the activation, which in turn will lead to a change
of the output, because even for non-linear neural networks, the ac-
tivation function primarily operates linearly in the non-saturated
region. We call this the accumulation eﬀect. Speech data processing,
when done in an end-to-end manner, is an extremely high dimen-
sional problem. Using typical sampling rates of 8kHz, 16kHz, or
44.1kHz, the dimensionality of the problem can easily grow into
the millions for a 30-second audio. Thus, taking the advantage of
the accumulation eﬀect, a small average well-designed perturba-
tion for each data point can cause a large shift in the output deci-
sion.

Original Audio(cid:3)

Feature Extraction(cid:3)

Acoustic Features(cid:3)

Attack Model(cid:3)

Perturbed Features(cid:3)

Perturbation(cid:3)

Audio Reconstruction(cid:3)

Adversarial Example(cid:3)

Figure 3: A feature-level attack scheme.

(a) Feature level Attack(cid:3)

2.3 End-to-end Adversarial Example

Generation

End to end 
Hypothetical 

Original Audio(cid:3)
(b) Proposed End to end Attack(cid:3)

As discussed in the related work section, most previous approaches
to audio adversarial example generation are on the acoustic feature
level. As shown in Figure 3, acoustic features are ﬁrst extracted
from the original audio, the perturbation is added to the acoustic
features, and then audio is reconstructed from the perturbed acous-
tic features. In recent years, end-to-end machine learning models
that do not explicitly rely on hand-crafted acoustic features have
become a mainstream technology. When targeting these models,
adversarial example generation schemes using acoustic features
might still work (because end-to-end models may implicitly rely
on such acoustic features), but they are less eﬃcient due to the
following reasons:

Perturbation(cid:3)

FGSM(cid:3)

• Acoustic feature extraction is usually a lossy conversion and
reconstructing audio from acoustic features cannot recover
this loss. For example, in [11], an adversarial attack is con-
ducted on the MFCC features, which typically represent each
audio frame of 20-40ms (160-320 data points if the sample
rate is 8kHz) using a vector of 13-39 dimensions. The conver-
sion loss can be signiﬁcant and even be larger than the ad-
versarial perturbation due to the information compression.
Thus, the ﬁnal perturbation on the audio waveform consists
of the adversarial perturbation plus an extra perturbation
caused by the lossy conversion:

ηoverall = ηconversion loss + ηadversarial

(6)

• Adversarial attacks on acoustic features and on raw audio
waveform are actually two diﬀerent optimization problems:

Minimize kηacoustic featurek , Minimize kηaudiok

(7)

Since most acoustic features do not have a linear relation-
ship with the audio amplitude, it is possible that a small
perturbation on acoustic features will lead to large pertur-
bations on the audio waveform and vice versa. Thus, an ad-
versarial attack on acoustic features might not even be an
approximation of an adversarial attack on the raw audio.
In order to overcome the above-mentioned limitations, we pro-
pose an end-to-end adversarial attack scheme that directly perturbs
the raw audio waveform. Compared to the feature-level attack scheme,
the proposed scheme completely abandons the feature extraction
and audio reconstruction steps to avoid any possible conversion
loss. The optimization is then directly targeted at the perturbation

DYNAMICS ’18, December 3–4, 2018, San Juan, Puerto Rico, USA

Yuan Gong and Christian Poellabauer

WaveRNN  Back-end Layers(cid:3) WaveCNN  Back-end Layers(cid:3)

output(cid:3)

output(cid:3)

recurrent layers (cid:3)

convolutional layers (cid:3)

concatenation(cid:3)

maxpool(cid:3)

maxpool(cid:3)

conv(cid:3)

conv(cid:3)

(cid:17)(cid:17)(cid:17)(cid:3)

(cid:17)(cid:17)(cid:17)(cid:3)

maxpool(cid:3)

maxpool(cid:3)

conv(cid:3)

conv(cid:3)

maxpool(cid:3)

conv(cid:3)

(cid:17)(cid:17)(cid:17)(cid:3)

maxpool(cid:3)

conv(cid:3)

Front-end  
Layers(cid:3)

(cid:17)(cid:17)(cid:17)(cid:3)

Frame 1(cid:3)

Frame 2(cid:3)

Frame n(cid:3)

sequences (such as speech signals), most elements in g go to zero
except the last few elements. This means that the gradient of the
early input in the sequence is not calculated eﬀectively, which will
cause the perturbation η = ϵsign(∇xJ (θ, x, y)) to only have mean-
ingful values in the last few elements. In other words, the pertur-
bation is only correctly added to the end of the input sequence.
Since this problem has not been reported before (calculating the
gradients with respect to the input ∇xJ (θ, x, y) is not typical, while
∇θ J (θ, x, y) is), we formalize and describe it mathematically in the
following paragraphs.

We assume one single output (rather than a sequence of out-
puts) for each input sequence, which is the typical case for par-
alinguistic applications. Consider the simplest one-layer RNN and
let st = tanh(Uxt + Wst −1) be the state of neurons at time step
t, y be the ground-truth label, ˆy be the prediction, J (y, ˆy) be the
loss function, and n be the number of time steps of the sequence.
The vanishing gradient problem in gradient based attacks can be
described as follows:

Figure 4: Network topology of WaveRNN and WaveCNN.

on the raw audio. A key component of the proposed scheme is to
use an end-to-end machine learning model that is able to directly
map the raw audio to the output as our hypothetical model. A good
choice of such a hypothetical model is the model proposed in [29];
we refer to this model as WaveRNN in the remainder of this paper.
WaveRNN is the ﬁrst network that learns directly from raw au-
dio waveforms to paralinguistic labels using recurrent neural net-
works (RNNs) and has become a state-of-the-art baseline approach
for multiple paralinguistic applications due to its excellent perfor-
mance compared to previous models [25]. As shown in Figure 4,
WaveRNN ﬁrst segments the input audio sequence into frames of
40ms and processes each frame separately in the ﬁrst few layers
(front-end layers). The output of the front-end layers is then fed,
in order of time, to the back-end recurrent layers.

Previous studies have shown that adversarial examples are able
to generalize, i.e., examples generated for attacking the hypothet-
ical model are then often able to attack other models, even when
they have diﬀerent architectures [9]. Further, the more similar the
structures of the hypothetical model and a practical model are, the
better the practical attack performance will be. Thus, the adver-
sarial examples generated for the WaveRNN model are expected
to have good attack performance with a variety of state-of-the-
art WaveRNN-like networks widely used in diﬀerent paralinguistic
tasks. However, in our work, we also discovered the vanishing gra-
dient problem, which prevents us from using WaveRNN directly
as the hypothetical model. This problem is discussed in the next
section.

2.4 The Vanishing Gradient Problem of

Gradient Based Attacks on RNNs

One basic prerequisite of gradient based attacks is that the required
gradients g = ∇xJ (θ, x, y) can be computed eﬃciently [9]. While,
in theory, the gradient based method applies as long as the model
is diﬀerentiable [22], even if there are recurrent connections in the
model, we observe that when we use RNNs to process long input

∂J
∂xi

=

∂J
∂ ˆy

∂ ˆy
∂sn

We can use the chain rule to expand

∂si
∂xi

∂sn
∂si
∂sn
∂si :

∂sn−1
∂sn−2

· · ·

∂si +1
∂si

∂sn
∂si

=

=

=

∂sn
∂sn−1
n−1

∂sj+1
∂sj

Ö
j=i
n−1

Ö
j=i

∂tanh(Uxt + Wsj )
∂sj

(8)

(9)

Since the derivative of the tanh function is in [0, 1], we have:
∂tanh(Uxt + Wsj )
∂sj

≤ 1

(10)

0 ≤ (cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

When the time interval between i and n becomes larger, espe-
cially when (n − i) → ∞, the continuous multiplication in Equa-
tion 9 will approach zero:

lim
(n−i )→∞

∂sn
∂si

(cid:13)
(cid:13)
(cid:13)
(cid:13)

= 0

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

We then substitute Equation 11 into Equation 8:

lim
(n−i )→∞

∂J
∂xi

= 0

(11)

(12)

Equation 12 indicates that for a long input sequence vector, par-
tial derivatives of the loss function with respect to the inputs at
earlier time steps are disappearing, which will make the gradient
based method fail. We believe it is an inherent problem of RNNs.
Interestingly, using Long Short Term Memory networks (LSTM)
cannot ﬁx the problem, potentially because LSTM will still forget
some input information. Figure 5 (upper graph) shows the partial
derivative of the loss function of a trained WaveRNN model with
respect to each input audio data point xi . Even when the WaveRNN
uses the LSTM recurrent layers, the partial derivative of the ﬁrst
60,000 data points are close to zero.

Crafting Adversarial Examples For Speech Paralinguistics Applications DYNAMICS ’18, December 3–4, 2018, San Juan, Puerto Rico, USA

10-6

10-7

5

0

-5

-10

0

4

2

0

-2

-4

0

1

2

3

4

5

6

7

8

9

1

2

3

4

5

6

7

8

9

104

104

Figure 5: The vanishing gradient problem: partial deriva-
tives of the RNN’s loss function with respect to inputs at
earlier time steps in a long sequence tend to disappear. Up-
per graph: the partial derivative of the loss function of Wa-
veRNN (with LSTM layers) with respect to each input audio
data point xi . Lower graph: the partial derivative of the loss
function of WaveCNN with respect to each input audio data
point xi .

2.5 The Substitution Model Approach
According to the discussion in the last section, it is diﬃcult to ef-
fectively calculate the gradient of the loss function with respect
to the input sequence for RNNs. While RNNs are most commonly
used in processing such kinds of sequence input problems, they
are not indispensable. In order to ﬁx the vanishing gradient prob-
lem, we propose a new network with a complete feedforward struc-
ture, which can be referred to as WaveCNN. Similar to WaveRNN,
WaveCNN also ﬁrst divides the input audio sequence into frames
of 40ms and processes each frame separately in the front-end lay-
ers. But after that, instead of feeding the output of the front-end
layers of each frame into recurrent layers, the WaveCNN approach
concatenates the outputs of the front-end layers of all frames and
feeds them to the following convolutional layers. The WaveCNN
approach uses convolutional structures as a substitution for the
recurrent structures. This modiﬁcation eliminates the recurrent
structures in the network and hence ﬁxes the vanishing gradient
problem. As shown in Figure 5, WaveCNN can calculate the gradi-
ent over all input data points eﬀectively. The design of WaveCNN
still retains the local receptive ﬁelds arithmetic of WaveRNN and
the back-end convolutional layers are also able to capture tempo-
ral information. Actually, in our experiments, WaveCNN performs
almost the same as WaveRNN for a series of tasks. We therefore
use WaveCNN as the hypothetical model in our work.

3 EXPERIMENTAL EVALUATION
3.1 Dataset and Test Strategy
We use the audio part of the IEMOCAP dataset [3] for our experi-
ments, which is a commonly used database in speech paralinguis-
tic research. The IEMOCAP database consists of 10,039 utterances
(average length: 4.46s) of 10 speakers (5 male, 5 female). In order to
make our experiments without loss of generality, we perform the
experiments with three diﬀerent speech paralinguistic tasks using
IEMOCAP:

• Gender Recognition: A binary classiﬁcation task of pre-
dicting the gender of the speaker. The IEMOCAP database
consists of 5239 utterances from male speakers and 4800 ut-
terances from female speakers.

• Emotion Recognition: A binary classiﬁcation task to dis-
tinguish sad speech from angry speech. The IEMOCAP data-
base consists of 1103 utterances annotated as angry and 1084
utterances annotated as sad.

• Speaker Recognition: A four-class classiﬁcation task of
predicting the identity of the speaker. We use the data of
four speakers in IEMOCAP sessions 1&2, which consist of
two male speakers and two female speakers. The number of
utterances of each speaker are 946, 873, 952, and 859, respec-
tively.

Note that we simpliﬁed the emotion recognition task and the
speaker recognition task in our experiment by limiting them to
a two-class and four-class problem, respectively. This simpliﬁca-
tion makes the model have a better performance before being at-
tacked. It is more meaningful to attack a model that originally has
good performance. The simpliﬁcation also makes the class distri-
bution balanced in our experiments, therefore, accuracy is an ef-
fective metric for evaluating the performance of the model. All ex-
periments are conducted in a hold-out test strategy, i.e., 75%, 5%,
and 20% of the data is used for training, validation, and test, re-
spectively. Hyperparameters are tuned only on the validation set.
All utterances are padded with zero or cut into 6-second audio and
further scaled into [0,1]. The audio sample rate is 16kHz, thus each
utterance is represented by a 96000-dimensional vector.

3.2 Network Training
The WaveRNN and WaveCNN models are trained separately for
each paralinguistic task. The network structure is ﬁxed for all ex-
periments; the only hyperparameter we tune during training is the
learning rate. We conduct a binary search for the learning rate in
the range [1e-5, 1e-2]. The maximum number of epochs is 200. The
best model and learning rate is selected according to the perfor-
mance on the validation set.

Interestingly, we ﬁnd that the WaveRNN and WaveCNN models
perform very similarly. In the gender recognition task, both mod-
els have an accuracy of 88%; in the emotion recognition task, the
WaveRNN has an accuracy of 84%, while the WaveCNN has an ac-
curacy of 85%; in the speaker recognition task, the WaveRNN has
an accuracy of 69%, while the WaveCNN has an accuracy of 73%.
This indicates that the convolutional back-end layers can also pro-
cess audio sequences well.

DYNAMICS ’18, December 3–4, 2018, San Juan, Puerto Rico, USA

Yuan Gong and Christian Poellabauer

0.4

0.35

0.3

0.25

0.2

0.15

e
t
a
R

r
o
r
r

E

0.1

0

Attack on WaveRNN
Attack on WaveCNN

0.02
0.06
0.04
Perturbation Factor (   )

0.08

e
t
a
R

r
o
r
r

E

0.55

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0

Attack on WaveRNN
Attack on WaveCNN

0.01
0.03
0.02
Perturbation Factor (   )

0.04

e
t
a
R

r
o
r
r

E

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0

Attack on WaveRNN
Attack on WaveCNN

0.02
0.06
0.04
Perturbation Factor (   )

0.08

Figure 6: The error rate with diﬀerent perturbation factors for the gender recognition task (left), emotion recognition task
(middle), and speaker recognition task (right).

1

0

e
d
u
t
i
l

p
m
A

-1

0

1

0

e
d
u
t
i
l

p
m
A

-1

0

)
z
H
k
(

y
c
n
e
u
q
e
r
F

)
z
H
k
(

y
c
n
e
u
q
e
r
F

8

6

4

2

0

8

6

4

2

0

0.5

1

1.5
Time (secs)

2

2.5

0.5

1

1.5
Time (secs)

2

2.5

/

)
z
H
B
d
(

y
c
n
e
u
q
e
r
f
/
r
e
w
o
P

/

)
z
H
B
d
(

y
c
n
e
u
q
e
r
f
/
r
e
w
o
P

-40

-60

-80

-100

-120

-40
-60
-80
-100
-120
-140

0.5

1

1.5
Time (secs)

2

2.5

3

0.5

1

1.5
Time (secs)

2

2.5

3

Figure 7: Comparison of the waveform (left), spectrogram (middle), and enlarged vocal spectrogram (right) between the adver-
sarial example with ϵ = 0.02 (upper graphs) and original example (lower graphs).

Table 1: Results of subjective human hearing test.

Proposed Adversarial Examples
MFCC-based Reconstructed Examples
White-Noise Added Examples

100%
84%
100%

98%
58%
100%

100%
6%
100%

Emotion Recognition Accuracy Gender Recognition Accuracy Naturality

3.3 Adversarial Attack Evaluation
In this work, we aim to attack WaveRNN, a state-of-the-art and
widely used model for speech paralinguistic applications. However,
due to the reasons mentioned in the previous section, it is not ap-
propriate to set WaveRNN as the hypothetical model in the attack
and we therefore use WaveCNN as our hypothetical model and
expect that the attack can be generalized to the WaveRNN model.
We perform the attack using FGSM described in Equation 2 on all
three paralinguistic tasks with diﬀerent perturbation factors ϵ and
apply it twice (i.e., basic iterative FGSM [14]). We can observe the
following from the results shown in Figure 6:

• The proposed attack approach is eﬀective in all paralinguis-
tic tasks. With a perturbation factor of 0.02, the gender recog-
nition error rate increases from 12% to 31%, and from 12%
to 25% for WaveRNN and WaveCNN, respectively. With a

perturbation factor of 0.015, the emotion recognition error
rate increases from 16% to 48%, and from 15% to 42.5% for
WaveRNN and WaveCNN, respectively. With a perturbation
factor of 0.032, the speaker recognition error rate increases
from 31% to 62%, and from 29% to 44% for WaveRNN and
WaveCNN, respectively. Note that 50% and 75% are the up-
per bound error rates for 2-class and 4-class classiﬁcation
tasks.

• The proposed attack approach can be generalized, e.g., while

the adversarial examples are designed to attack the WaveCNN
as the hypothetical model, they can also cause a signiﬁcant
performance drop for the WaveRNN model. Note that the at-
tack aﬀects WaveCNN and WaveRNN in diﬀerent ways; the
error rate of the WaveCNN model increases linearly with
respect to the perturbation factor, while the error rate of

 
 
 
 
 
 
 
Crafting Adversarial Examples For Speech Paralinguistics Applications DYNAMICS ’18, December 3–4, 2018, San Juan, Puerto Rico, USA

the WaveRNN model does not change with small perturba-
tion factors, but changes rapidly when the perturbation fac-
tor exceeds a speciﬁc level. We observe a performance ﬂuc-
tuation of the WaveRNN attack in the gender recognition
task, which we believe is due to the loss function coinciden-
tally reaching a sub-optimal solution with the perturbation
rather than a ﬂaw of the attack. The error rate still increases
with the perturbation factor after the ﬂuctuation.

• The performance when there is no attack is not an indica-
tor of the vulnerability of the model. In our experiments,
the error rate of the emotion recognition model is similar to
the gender recognition model when there is no attack, but
increases much faster with larger perturbation factors than
the gender recognition model. This indicates that even high
performing models can be very vulnerable to adversarial ex-
amples.

3.4 Perturbation Analysis
Our goal is to successfully fool the machine learning model, while
also keeping the perturbation to be so small that it cannot be de-
tected by a human. Toward this end, an adversarial example (gen-
erated for the emotion recognition task with ϵ = 0.02) is com-
pared to an original example in Figure 7. When ϵ = 0.02, both
the WaveRNN and WaveCNN models are close to random guesses
with an error rate of 51% and 46.5%, respectively. Comparing the
spectrograms of the proposed adversarial example and the adver-
sarial example generated by the MFCC feature-level attack (i.e.,
Figure 1 in [11]), we observe that the proposed perturbation is
much smaller, especially in the vocal parts. The feature-level at-
tack greatly obscures the vocal spectrogram, while the proposed
attack barely changes it. Quantitatively, we reproduced the MFCC
based attacks in [11] and measured its minimal distortion (i.e., only
the distortion caused by the audio reconstruction) is being equiv-
alent to the distortion caused by our proposed perturbation when
ϵ = 0.101. Note that this is only the initial distortion level of the
work in [11] (i.e., distortion due to the use of features) and no ad-
versarial perturbation has been added yet. In our approach, where
ϵ = 0.08 < 0.101, all classiﬁers already predict like random guesses
and the perturbation of the proposed approach is smaller than that
of MFCC feature-level approaches for successful attacks.

In addition, we also performed a subjective human hearing test
to evaluate the perturbation with respect to human perception. In
this test, 10 subjects independent from this research were asked
to listen to 5 proposed adversarial examples (ϵ = 0.02), 5 exam-
ples with equivalent levels of random white noise added, and 5
examples reconstructed from MFCC features as in [11] (i.e., no ad-
versarial perturbation has been added and only the reconstruction
distortion is present in the examples). The subjects are then asked
to determine the gender, emotion, and naturality of each audio ex-
ample. The speaker identiﬁcation task was not included in this test,
because the human subjects would need prior knowledge to iden-
tify the speaker. As shown in Table 1, 100% and 98% of the proposed
adversarial samples are classiﬁed correctly in terms of emotion and
gender, while 100% of the proposed samples are also identiﬁed as
natural speech. The results are 100%, 100%, and 100% for white

noise samples and 84%, 58%, and 6% for MFCC reconstructed sam-
ples, respectively. This shows that the proposed adversarial exam-
ples are more comparable to natural speech than MFCC feature-
based adversarial examples. In fact, listening to the proposed ad-
versarial audio reveals that the vocal parts are unchanged, while
the perturbations sound the same as “normal” noise. With such
perturbations (i.e., added noise), humans have it diﬃcult to detect
an attack.

Finally, by comparing the spectrograms of the original example
and the adversarial example in Figure 7, we observe that the per-
turbation covers a broad spectrum, which means that it would be
diﬃcult to eliminate the attack through simple ﬁltering.

4 DISCUSSION
Since WaveCNN and WaveRNN have completely diﬀerent back-
end layers, we believe that one reason of the generalizability of ad-
versarial examples between WaveCNN and WaveRNN is that they
use the same front-end layers, which have a high probability of
learning similar representations and therefore having similar con-
volutional kernel weights. Considering Equation 4, the activation
change of neurons is still very large. If this assumption is correct,
then a variety of end-to-end speech processing models that use
similar front-end layer structures might also suﬀer from the same
attack.

More generally, the phenomenon that deep neural network based
end-to-end speech paralinguistic models are robust to variance and
noise in naturally occurring data, but vulnerable to man-made per-
turbation is somewhat counter-intuitive, but actually not surpris-
ing. End-to-end models experience performance improvements by
processing problems in a much higher dimensional space in or-
der to obtain a more accurate approximation function of the prob-
lem, but this also leaves blind spots in the space where data is
distributed sparsely. Therefore, when manual perturbed data en-
ters these blind spots, the model is not able to make correct pre-
dictions. Nonetheless, the problem is not unsolvable. If the exact
type of attack is known, it can help with building a defense model
by mixing adversarial examples into the training data to make the
deep neural network see such adversarial examples and reﬁne its
approximation functions so that they can withstand adversarial ex-
amples [11, 15, 19]. To the best of our knowledge, current end-to-
end audio processing algorithms (not just limited to speech par-
alinguistics) barely pay attention to this type of risk. Therefore,
our goal is to provide a better understanding of such attacks to
help design adversarial-robust models in the future.

5 CONCLUSIONS
Adversarial attacks on computational paralinguistic systems pose
a critical security risk that has not yet received the attention it de-
serves. In this work, we propose an end-to-end adversarial example
generation scheme, which directly perturbs the raw audio wave-
form. Our experiments with three diﬀerent paralinguistic tasks
empirically show that the proposed approach can eﬀectively at-
tack WaveRNN models, a state-of-the-art deep neural network ap-
proach that is widely used in paralinguistic applications, while the
added perturbation is much smaller compared to previous feature-
based audio adversarial example generation techniques.

DYNAMICS ’18, December 3–4, 2018, San Juan, Puerto Rico, USA

Yuan Gong and Christian Poellabauer

[12] Corey Kereliuk, Bob L Sturm, and Jan Larsen. 2015. Deep learning and music

adversaries. IEEE Transactions on Multimedia 17, 11 (2015), 2059–2071.

[13] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980 (2014).

[14] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial examples

in the physical world. arXiv preprint arXiv:1607.02533 (2016).

[15] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial machine

learning at scale. arXiv preprint arXiv:1611.01236 (2016).

[16] Kornel Laskowski, Mari Ostendorf, and Tanja Schultz. 2008. Modeling vocal in-
teraction for text-independent participant characterization in multi-party con-
versation. In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue.
Association for Computational Linguistics, 148–155.

[17] Chi-Chun Lee, Athanasios Katsamanis, Matthew P Black, Brian R Baucom,
Panayiotis G Georgiou, and Shrikanth Narayanan. 2011. An analysis of PCA-
based vocal entrainment measures in married couples’ aﬀective spoken interac-
tions. In Twelfth Annual Conference of the International Speech Communication
Association.

[18] Yun Lei, Nicolas Scheﬀer, Luciana Ferrer, and Mitchell McLaren. 2014. A novel
scheme for speaker recognition using a phonetically-aware deep neural network.
In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Con-
ference on. IEEE, 1695–1699.

[19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2017. Towards deep learning models resistant to adversarial at-
tacks. arXiv preprint arXiv:1706.06083 (2017).

[20] Nicolas Malyska, Thomas F Quatieri, and Douglas Sturim. 2005. Automatic dys-
phonia recognition using biologically-inspired amplitude-modulation features.
In Acoustics, Speech, and Signal Processing, 2005. Proceedings.(ICASSP’05). IEEE
International Conference on, Vol. 1. IEEE, I–873.

[21] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Ce-
lik, and Ananthram Swami. 2016. Practical black-box attacks against deep learn-
ing systems using adversarial examples. arXiv preprint arXiv:1602.02697 (2016).
[22] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang.
2016. Crafting adversarial input sequences for recurrent neural networks. In
Military Communications Conference, MILCOM 2016-2016 IEEE. IEEE, 49–54.
[23] Jean Schoentgen. 2006. Vocal cues of disordered voices: an overview. Acta Acus-

tica united with Acustica 92, 5 (2006), 667–680.

[24] Björn Schuller and Anton Batliner. 2013. Computational paralinguistics: emotion,
aﬀect and personality in speech and language processing. John Wiley & Sons.
[25] Björn Schuller, Stefan Steidl, Anton Batliner, Elika Bergelson, Jarek Krajewski,
Christoph Janott, Andrei Amatuni, Marisa Casillas, Amdanda Seidl, Melanie
Soderstrom, et al. 2017. The INTERSPEECH 2017 computational paralinguistics
challenge: Addressee, cold & snoring. In Computational Paralinguistics Challenge
(ComParE), Interspeech 2017.

[26] Björn W Schuller, Stefan Steidl, Anton Batliner, Julia Hirschberg, Judee K Bur-
goon, Alice Baird, Aaron C Elkins, Yue Zhang, Eduardo Coutinho, and Keelan
Evanini. 2016. The INTERSPEECH 2016 Computational Paralinguistics Chal-
lenge: Deception, Sincerity & Native Language.. In INTERSPEECH. 2001–2005.

[27] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Er-
han, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural net-
works. arXiv preprint arXiv:1312.6199 (2013).

[28] Rohan Taori, Amog Kamsetty, Brenton Chu, and Nikita Vemuri. 2018. Tar-
arXiv preprint

geted Adversarial Examples for Black Box Audio Systems.
arXiv:1805.07820 (2018).

[29] George Trigeorgis, Fabien Ringeval, Raymond Brueckner, Erik Marchi, Mihalis A
Nicolaou, Björn Schuller, and Stefanos Zafeiriou. 2016. Adieu features? End-to-
end speech emotion recognition using a deep convolutional recurrent network.
In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Con-
ference on. IEEE, 5200–5204.

[30] Tavish Vaidya, Yuankai Zhang, Micah Sherr, and Clay Shields. 2015. Cocaine
noodles: exploiting the gap between human and machine speech recognition.
Presented at WOOT 15 (2015), 10–11.

[31] Ehsan Variani, Xin Lei, Erik McDermott, Ignacio Lopez Moreno, and Javier
Gonzalez-Dominguez. 2014. Deep neural networks for small footprint text-
dependent speaker veriﬁcation. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on. IEEE, 4052–4056.

6 APPENDIX
6.1 Details of WaveRNN and WaveCNN
The details of the network architectures for WaveCNN and Wav-
eRNN are shown in Table 2. The model training uses the following:
Adam optimizer [13], cross entropy loss function, learning rate de-
cay of 0.1, max number of epochs of 200, and batch size of 100.
The initial learning rate is selected within the range of [1e-5,1e-2]
using a binary search.

Table 2: Network architecture of WaveCNN and WaveRNN

Layer In-
dex

Reshape
1-16

Reshape
17-28

Reshape
29
30

Network Parameters and Explanation

Common Front-end Layers
Divide audio into frames of 40ms
8×(convolutional layers + max pooling layer), kernel
size=[1,40], feature number=32, pool size=(1,2), zero
padding

WaveCNN Back-end Layers
Concatenate output of each frame
6×(convolutional layers + max pooling layer), kernel
size=[1,40], feature number=32, pool size=(1,2), zero
padding
Flatten
Fully-connected layer with 64 units
Fully-connected layer with softmax output

WaveRNN Back-end Layers

17
18

LSTM recurrent layer with 64 units
Fully-connected layer with softmax output

REFERENCES
[1] Moustafa Alzantot, Bharathan Balaji, and Mani Srivastava. 2018. Did you hear
that? adversarial examples against automatic speech recognition. arXiv preprint
arXiv:1801.00554 (2018).

[2] Tobias Bocklet, Elmar Nöth, Georg Stemmer, Hana Ruzickova, and Jan Rusz.
2011. Detection of persons with Parkinson’s disease by acoustic, vocal, and
prosodic analysis. In Automatic Speech Recognition and Understanding (ASRU),
2011 IEEE Workshop on. IEEE, 478–483.

[3] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower,
Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan. 2008.
IEMOCAP: Interactive emotional dyadic motion capture database. Language
resources and evaluation 42, 4 (2008), 335.

[4] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
Clay Shields, David Wagner, and Wenchao Zhou. 2016. Hidden Voice Com-
mands.. In USENIX Security Symposium. 513–530.

[5] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted

attacks on speech-to-text. arXiv preprint arXiv:1801.01944 (2018).

[6] Moustapha Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. 2017. Hou-
dini: Fooling deep structured prediction models. arXiv preprint arXiv:1707.05373
(2017).

[7] Louis Daudet, Nikhil Yadav, Matthew Perez, Christian Poellabauer, Sandra
Schneider, and Alan Huebner. 2017. Portable mTBI assessment using temporal
and frequency analysis of speech.
IEEE journal of biomedical and health infor-
matics 21, 2 (2017), 496–506.

[8] Yuan Gong and Christian Poellabauer. 2017. Continuous Assessment of Chil-
dren’s Emotional States Using Acoustic Analysis. In Healthcare Informatics
(ICHI), 2017 IEEE International Conference on. IEEE, 171–178.

[9] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and

harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).

[10] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. 2016. End-to-
end text-dependent speaker veriﬁcation. In Acoustics, Speech and Signal Process-
ing (ICASSP), 2016 IEEE International Conference on. IEEE, 5115–5119.

[11] Dan Iter, Jade Huang, and Mike Jermann. 2017. Generating Adversarial Exam-

ples for Speech Recognition. (2017).

