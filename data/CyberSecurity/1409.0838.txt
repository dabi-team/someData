4
1
0
2

p
e
S
0
1

]

Y
S
.
s
c
[

2
v
8
3
8
0
.
9
0
4
1
:
v
i
X
r
a

A Supervisory Control Approach to Dynamic
Cyber-Security

Mohammad Rasouli, Erik Miehling, and Demosthenis Teneketzis

Department of Electrical Engineering and Computer Science
University of Michigan, Ann Arbor, MI

Abstract. An analytical approach for a dynamic cyber-security prob-
lem that captures progressive attacks to a computer network is presented.
We formulate the dynamic security problem from the defender’s point of
view as a supervisory control problem with imperfect information, mod-
eling the computer network’s operation by a discrete event system. We
consider a min-max performance criterion and use dynamic program-
ming to determine, within a restricted set of policies, an optimal policy
for the defender. We study and interpret the behavior of this optimal
policy as we vary certain parameters of the supervisory control problem.

Keywords: Cyber-Security, Computer Networks, Discrete Event Systems, Fi-
nite State Automata, Dynamic Programming

1

Introduction

Cyber-security has attracted much attention recently due to its increasing im-
portance in the safety of many modern technological systems. These systems
are ubiquitous in our modern day life, ranging from computer networks, the in-
ternet, mobile networks, the power grid, and even implantable medical devices.
This ubiquity highlights the essential need for a large research eﬀort in order
to strengthen the resiliency of these systems against attacks, intentional and
unintentional misuse, and inadvertent failures.

The study of cyber-security problems in the existing literature can be divided

into two main categories: static and dynamic.

Static problems concern settings where the agents, commonly considered to
be an attacker and a defender, receive no new information during the time hori-
zon in which decisions are made. Problems of this type in the security literature
can largely be classiﬁed under the category of resource allocation, where both the
defender and attacker make a single decision as to where to allocate their respec-
tive resources. The main bodies of work involve infrastructure protection [3,7,9]
and mitigation of malware and virus spread in a network [5,6,8,16]. Some of the
above works consider settings where the agents are strategic [3, 9]. The presence
of strategic agents results in a game between the attacker and defender. The
strategic approaches in the above works are commonly referred to as allocation

 
 
 
 
 
 
games. The survey by Roy et al. [18], as well as [20], provide useful outlines of
some static game models in security.

Dynamic security problems are those that evolve over time, with the defender
taking actions while observing some new information from the environment.1
The formulation of a security problem as a dynamic problem, instead of a static
one, oﬀers numerous advantages. The ﬁrst advantage is clear; since real-world
security problems have an inherently dynamic aspect, dynamic models can more
easily capture realistic security settings, compared to static models. Also, most
attacks in cyber-security settings are progressive, meaning more recent attacks
build upon previous attacks (such as denial-of-service attacks, brute-force at-
tacks, and the replication of viruses, malware, and worms, to name a few). This
progressive nature is more easily modeled in a dynamic setting than in a static
setting.

The literature within the dynamic setting can be further subdivided into two
areas: models based on control theory [10, 13, 14, 17, 19] and models based on
game theory [11, 18, 21, 22].

The control theory based security models in the literature diﬀer in the ways
in which the dynamics are modeled. The work by Khouzani et al. [10] studies the
problem of a malware attack in a mobile wireless network; the dynamics of the
malware spread are modeled using diﬀerential equations. A large part of the lit-
erature on control theory based models focuses on problems where the dynamics
are modeled by ﬁnite state automata. The works of [13, 14, 19] implement spe-
ciﬁc control policies (protocols) for security purposes. The work of Schneider [19]
uses a ﬁnite state automaton to describe a setting where signals are sent to a
computer. Given a set of initial possible states, the signals cause the state of
the computer to evolve over time. An entity termed the observer monitors the
evolution of the system and enforces security in real-time. Extensions of Schnei-
der’s model are centered around including additional actions for the observer.
Ligatti et al. [13] extend Schneider’s model by introducing a variety of abstract
machines which can edit the actions of a program, at run-time, when deviation
from a speciﬁed control policy is observed. More recent work [14] develops a
formal framework for analyzing the enforcement of more general policies. An-
other category of dynamic defense concerns scenarios where the defender selects
an adaptive attack surface 2 in order to change the possible attack and defense
policies. A notion termed moving target defense (a term for dynamic system
reconﬁguration) is one class of such dynamic defense policies. The work of Rowe
et al. [17] develops control theoretic mechanisms to determine maneuvers that
modify the attack surface in order to mitigate attacks. The work involves ﬁrst
developing algorithms for estimation of the security state of the system, then
formalizing a method for determining the cost of a given maneuver. The model
uses a logical automaton to describe the evolution of the state of the system;

1 This new information could consist of the attacker’s actions, events in nature, or the

state of a some underlying system.

2 For example, changing the network topology.

however, it does not propose an analytical approach for determining an optimal
defense policy.

The next set of security models in the literature are based on the theory of
dynamic games. The work in [15] considers a stochastic dynamic game to model
the environment of conﬂict between an attacker and a defender. In this model,
the state of the system evolves according to a Markov chain. This paper has
many elements in common with our model; however, it assumes the attacker and
defender have perfect observations of the system state. In our paper, we consider
the problem from the defender’s point of view and assume that the defender has
imperfect information about the system state. The work by Khouzani [11] studies
a zero-sum two-agent (malware agent and a network agent) dynamic game with
perfect information. The malware agent is choosing a strategy which trades
oﬀ malware spread and network damage while the network agent is choosing
a counter-measure strategy. The authors illustrate that saddle-point strategies
exhibit a threshold form. The work of Yin et al. [22] (dynamic game version
of [3]) studies a Stackelberg game where the defender moves ﬁrst and commits
to a strategy. The work addresses how the defender should choose a strategy
when it is uncertain whether the attacker will observe the ﬁrst move. Van Dijk
et al. [21] propose a two player dynamic game, termed Flipit, which models a
general setting where a defender and an attacker ﬁght (in continuous time) over
control of a resource. The results concern the determination of scenarios where
there exist dominant strategies for both players. We refer the reader to Roy et
al. [18], and references therein, for a survey on the application of dynamic games
to problems in security.

While models based on game theory have generated positive results in the
static setting, there has been little progress in the dynamic setting. We believe
this is for two reasons; ﬁrst, dynamic security has not been fully investigated in
a non-strategic context and second, the results in the theory of dynamic games
are limited.

In this paper, we develop a (supervisory) control theory approach to a dy-
namic cyber-security problem and determine the optimal defense policy against
progressive attacks. We consider a network of K computers, each of which can
be in one of four security states, as seen in Figure 1. The state of the system
is the K-tuple of the computer states and evolves in time with both defender
and attacker actions. We use a ﬁnite state logical automaton to model the dy-
namics of the system. The defender adjusts to attacks based on the information
available.

Our model takes a diﬀerent approach than the existing papers in the lit-
erature. One fundamental diﬀerence of our work from the existing literature
that make use of automata is the development of an analytical framework for
determining optimal defense policies within a restricted set of policies. Other
works involving automata propose methods for enforcing a predetermined policy,
rather than determining an optimal policy. Also, our control theoretic approach
considers imperfect information regarding attacker actions, which we feel is an
aspect that is engrained into security problems.

Fig. 1: An instance of the problem that we consider. Computers are connected through
a routing layer. Each computer can be in one of four security states: normal (N),
compromised (R), fully compromised (W), or remote compromised (F).

1.1 Contribution

The contribution of this paper is the development of a formal model for an-
alyzing a dynamic cyber-security problem from the defender’s point of view.
Our approach has the following desirable features: (i) It captures the progressive
nature of attacks; (ii) It captures the fact that the defender has imperfect knowl-
edge regarding the state of the system; this uncertainty is a result of the fact
that all attacks are uncontrollable and most are unobservable, by the defender;
(iii) It allows us to quantify the cost incurred at every possible state of the sys-
tem, as well as the cost due to every possible defender action; (iv) It allows us
to quantify the performance of various defender policies and to determine the
defender’s optimal control policy, within a restricted set of policies, with respect
to a min-max performance criterion.

1.2 Organization

The paper is organized as follows. In Section 2 we discuss our dynamic defense
model. This is done by introducing the assumptions on the computer network and
corresponding state, as well as the events which drive the evolution of the system
state. In Section 3, we model the defender’s problem of keeping the computer
network as secure as possible while subjected to progressive attacks. We provide
a simpliﬁed problem formulation that is tractable. In Section 4, we determine
an optimal control policy for the defender based on dynamic programming. We
discuss the nature of the optimal policy in Section 5. We oﬀer conclusions and
reﬂections in Section 6.

si=Nsi=Rsi=Wsi=FRoutingLayerComputerLayer2 The Dynamic Defense Model

The key features of our model are characterized by assumptions (A1) – (A6).
We ﬁrst describe the assumptions related to the computer network, discussed in
assumption (A1). In assumption (A2) we introduce the notion of the computer
network system state. Next, in assumptions (A3) – (A5), we discuss the events
that can occur within the system. We describe how the events cause the system
state to evolve, as well as specify which events are controllable and observable
by the defender. In (A6) we discuss an assumption on the rules of interaction
between the attacker and the defender. As mentioned in the introduction, we
consider the cyber-security problem from the defender’s viewpoint; the model
we propose reﬂects this viewpoint.

Assumption 1 - Computer Network : We assume a set of networked com-
puters, N = {1, 2, . . . , K}. Each computer, i ∈ N , can be at security level
zi ∈ M = {N, R, W, F } where M is the set of security states.

Each computer, i ∈ N , is assumed to have three security boundaries, denoted
by B = {B1, B2, B3}, representative of a layered structure to its security. These
security boundaries partition the set of security states M. Throughout this pa-
per, we assume that the set of security states M = {N, R, W, F } is deﬁned as
follows.

Normal (zi = N ): Computer i is in the normal state if none of the security
boundaries have been passed by the attacker.
Compromised (zi = R): Computer i is compromised when security boundary
B1 has been passed by the attacker. In this state, the attacker has exploited
some vulnerability on the computer and has managed to obtain user-level
access privilege to the computer.
Fully Compromised (zi = W ): Computer i is fully compromised when both
boundaries B1 and B2 have been passed by the attacker. The attacker has
exploited some additional vulnerability on the computer and has managed
to obtain root level or execute privilege to the computer.
Remote Compromised (zi = F ): Computer i is remote compromised when
all security boundaries B1, B2, and B3 have been passed by the attacker.
The attacker has managed to obtain enough privileges to attack another
computer and obtain user-level access privilege on that computer.

Assumption 2 - System State: We assume that the computer network oper-
ates over an inﬁnite time horizon, T = {0, 1, 2, . . .}. The state of the computer
network, Zt, which evolves with time t ∈ T , is the combination of the states of
all the computers at time t. Each state Zt has a corresponding cost.

The state of the network, denoted Zt = (z1

t ) ∈ Z, is a K-tuple
of all of the computer states.3 The set Z denotes the set of all possible states,

t , . . . , zK

t , z2

3 For example, a three computer network could have a network state of Z (cid:48)

t =

(N, R, W ). Notice that state Z (cid:48)

t is distinct from state Z (cid:48)(cid:48)

t = (R, N, W ).

Z = {Z 1, Z 2, . . . , Z |M|K
where |M|K is the number of system states.

} = {(N, N, . . . , N ), (N, N, . . . , R), . . . , (F, F, . . . , F )},

The cost of the network state Zt is deﬁned by the costs of the states of the
t), to each computer i depending upon its state

computers. We assign a cost, c(zi
zi
t ∈ M. This cost is deﬁned as follows

c(zi

t) =






cN if zi
cR if zi
cW if zi
if zi
cF

t = N
t = R
t = W
t = F

with 0 ≤ cN < cR < cW < cF < ∞. The cost of state Zt is then deﬁned as

CZt =

(cid:88)

i∈N

c(zi
t)

(1)

(2)

The state of the network, Zt, evolves in time due to events, which we discuss in
the next set of assumptions.

Assumption 3 - Events: There is a set of events, E = A ∪ D, where A are
the attacker’s actions and D are the defender’s actions.

We assume that the attacker has access to three types of actions. The set of
n}i∈N ,n∈B, {H ij}i,j∈N

attacker actions, A = (cid:8)N a, {P i

(cid:9), is deﬁned as follows.

N a, null : The attacker takes no action. The null action does not change the
system state and is admissible at any state of a computer.
n, security boundary attack : Attacking the nth security boundary of com-
P i
puter i causes the security state of computer i to transition across the nth
security boundary. Speciﬁcally, P i
B1 causes computer i to transition from
B2 from zi = R to zi = W ; and
normal, zi = N , to compromised, zi = R; P i
B2 , and P i
B1 , P i
B3 from zi = W to zi = F . Actions P i
P i
B3 are only admissible
from states zi = R, zi = W , and zi = F , respectively.
H ij, network attack : Using a computer i in state zi = F to attack any
other normal or compromised computer j in the network that is in state
zj = {N, R} to bring computer j to state zj = W . The action H ij is
admissible at state zi = F for zj ∈ {N, R, W }.

We assume that the defender knows the set A as well as the resulting state
transitions due to each action in A.

The defender has access to three types of costly actions. These actions are
admissible at any computer state. The set of defender actions, denoted by D =
(cid:8)N d, {Ei}i∈N , {Ri}i∈N

(cid:9), is deﬁned as follows.

N d, null : The defender takes no action. The null action does not change the
system state.
Ei, sense computer i: The sense action, Ei, reveals the state of computer i
to the defender. The sense action does not change the system state.

Ri, re-image computer i: The re-image action, Ri, brings computer i back
to the normal state from any state that it is currently in. For example, R3
applied to state (N, R, F ) results in (N, R, N ).

The costs of the actions in D are deﬁned by ˆC(N d), ˆC(Ei), ˆC(Ri), where
0 ≤ ˆC(N d) < ˆC(Ei) < ˆC(Ri) < ∞ for all i ∈ N .

Assumption 4 - Defender’s Controllability of Events: The actions in A
are uncontrollable whereas the actions in D are controllable.

Since the problem is viewed from the perspective of the defender, all actions
in D are controllable. For the same reason, the defender is unable to control any
of the attacker’s actions A.

Assumption 5 - Defender’s Observability of Events: All actions in D and
some actions in A are assumed to be observable.

Again, due to taking the defender’s viewpoint, all actions in D are observ-
able. Although we assume that the defender knows the set A, we assume that
it cannot observe N a or any P i
n actions; it can only observe actions of the type
H ij. One justiﬁcation for this is that the the network attack H ij involves pass-
ing sensitive information of computer j through the routing layer of the system
to computer i.4 We assume that the routing layer is able to detect the transfer
of sensitive data through the network, and thus the defender is aware when an
action of the form H ij occurs.

Assumption 6 - Defender’s Decision Epochs: The defender acts at regular,
discrete time intervals. At these time intervals, the defender takes only one action
in D. The attacker takes one action in A between each defender action.

We require that the defender should consider taking a single action in D at
regular time instances. We assume that between any two such instances, the
attacker can only take one action in A. This order of events is illustrated in
Figure 2 for a given time t = τ . We introduce intermediate states, denoted by
˜Z = ( ˜Z 1, ˜Z 2, . . . , ˜Z |M|K
), which represent the system states at which events
from A are admissible (that is, the states in which the attacker takes an action).
The system states, denoted by Z = (Z 1, Z 2, . . . , Z |M|K
), are the states at which
actions from D are admissible.

Assumption (A6) is, in our opinion, reasonable within the security context.
Since time has value in security problems,5 the defender should take actions
at regular time intervals (note that at these instances the defender may choose
N d, that is, choose to do nothing). In general, a ﬁnite number of events in A
may occur between any two successive defender actions; however, to reduce the

4 This sensitive information could be the login credentials of computer j.
5 A computer that is compromised by the attacker for two time steps is more costly

to the defender than a computer that is compromised for one time step.

Fig. 2: Order of events for a given time-step. At time t = τ , the cost of the current state
CZτ is realized. At τ +, the defender takes an action in D (the cost of which is realized
immediately). The resulting system state due to the defender’s action is denoted by
˜Zτ + ∈ ˜Z. At τ ++ the attacker takes an action in A. At τ + 1, the resulting system
state is denoted by Zτ +1 ∈ Z.

dimensionality of the problem, we assume that only one event in A can occur.

One important implication of assumption (A6) is related to the defender’s
observability of events in A. By (A6), the defender is aware when an event in A
occurs. Since the event H ij is observable, if the defender does not observe H ij
when an event in A is known to occur, then it knows that one of the unobservable
events, N a or one of {P i
n}i∈N ,n∈B, has occurred. To incorporate this fact into
the defender’s knowledge about the system’s evolution, we group the above men-
tioned unobservable events into one event, denoted X = (cid:8)N a, {P i
(cid:9).
This philosophy is used in constructing the system automaton from the de-
fender’s point of view, as well as in deﬁning the defender’s information state
(discussed in Section 3). As a result of the above grouping, the set of events
(cid:9) is observable by the defender. Notice, however, that by
A(cid:48) = (cid:8)X, {H ij}i,j∈N
performing this grouping, we have introduced non-determinism into the system;
that is, the event X can take the system to many possible system states. All
unobservable events in the problem have been eliminated due to Assumption
(A6) and the grouping of unobservable events in A.

n}i∈N ,n∈B

As a result of assumptions (A1) – (A6), the evolution of the system state,
Zt, from the defender’s viewpoint, can be modeled by a discrete event system
represented by a ﬁnite state automaton, which we term the system automaton.
Due to assumption (A6), we duplicate the system states by forming the set of
intermediate states, denoted by ˜Z = ( ˜Z 1, ˜Z 2, . . . , ˜Z |M|K
). The set of interme-
diate states represents the states at which an event from A can occur. The set
of system states, denoted by Z, are the states at which the defender takes an
action d ∈ D. The resulting automaton has 2|M|K states. The set of events that
can occur is described by the set E (cid:48) = A(cid:48) ∪ D; the transitions due to these events
follow the rules discussed in assumption (A3). The system automaton takes the
form of a bipartite graph, as seen in Figure 3. Notice that, like the null action,
the sense actions, Ei, for all i ∈ N , do not change the underlying system state.
The purpose of sense is to update the defender’s information state, which will
be deﬁned and explained in the following section.

AttackerStatesDefenderStates(S,S,S)(S,S,Q)(CA,CA,CA)ττ+τ++τ+1t1AttackerStatesDefenderStates(S,S,S)(S,S,Q)(CA,CA,CA)ττ+τ++τ+1t1AttackerStatesDefenderStates(S,S,S)(S,S,Q)(CA,CA,CA)ττ+τ++τ+1t1AttackerStatesDefenderStates(S,S,S)(S,S,Q)(CA,CA,CA)ττ+τ++τ+1t1AttackerStatesDefenderStates(S,S,S)(S,S,Q)(CA,CA,CA)ττ+τ++τ+1t1Fig. 3: The system automaton represented as a bipartite graph of intermediate states,
˜Z = ( ˜Z 1, ˜Z 2, . . . , ˜Z |M|K
), with events
E (cid:48) = A(cid:48) ∪ D. Notice the non-determinism of the event X ∈ A(cid:48).

), and system states, Z = (Z 1, Z 2, . . . , Z |M|K

3 The Defender’s Problem

We now formulate the defender’s problem – protecting the computer network.
The defender must decide which costly action to take, at each time step, in order
to keep the system as secure as possible given that it has imperfect knowledge
of the network’s state.

3.1 The Defender’s Optimization Problem

Let g := {gt, t ∈ T }, denote a control policy of the defender, where

gt : Dt−1 × A(cid:48)t−1 → D,

(3)

and Dt−1 and A(cid:48)t−1 denote the space of the defender’s actions and observations
up to t − 1, respectively. Let G := {g | gt : Dt−1 × A(cid:48)t−1 → D for all t ∈ T }
denote the space of admissible control policies for the defender.

The defender’s optimization problem is

min
g∈G

max
t ∈Z,t∈T }

{Zg

(cid:20)
CZg

t

βt

+ ˆC(cid:0)dt

(cid:1)

(cid:21)(cid:41)

(cid:40)

(cid:88)

t∈T

subject to Assumptions (A1) – (A6)

(PD)

where {Z g
t ∈ Z, t ∈ T } denotes a sequence of states generated by control policy g
and dt is the defender’s action at t generated according to Equation (3). Problem
(PD) is a supervisory control problem with imperfect observations.

IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH121IntermediateStatesSystemStates˜Z1˜Z2˜Z|M|KZ1Z2Z|M|KEi,NdXR3XR1R2XH1213.2 Discussion of Problem (PD)

The notion of an information state [12] is a key concept in supervisory (and gen-
eral) control problems with imperfect information. Because of the nature of the
performance criterion and the fact that the defender’s information is imperfect,
an appropriate information state for the defender at time t is σ(Dt−1, A(cid:48)t−1),
the σ-ﬁeld generated by the defender’s actions and observations, respectively, up
to t − 1. Using such an information state, one can, in principle, write the dy-
namic program for Problem (PD). Such a dynamic program is computationally
intractable. For this reason, we formulate another problem, called (P (cid:48)
D), where
we restrict attention to a set of defense policies that have a speciﬁc structure; in
this problem we can obtain a computationally tractable solution.

3.3 Speciﬁcation of Problem (P (cid:48)

D)

We deﬁne the defender’s observer as follows. The defender’s observer is built
using the defender’s observable events, A(cid:48), and its actions, D. The observer’s
state at time t, denoted by St ⊆ Z, consists of the possible states that the
network can be in at time t from the defender’s perspective. We denote by S the
space to which St belongs, for any t ∈ T .

The evolution of the observer’s state is described by the function f : S × D ×

A(cid:48) → S. The observer’s state St follows the update

St+1 = f (St, dt, a(cid:48)
t)

where dt ∈ D is the realization of the defender’s action and its eﬀect at time t+,
t ∈ A(cid:48) is the realization of the defender’s observation at t++. The precise
and a(cid:48)
form of the function f is determined by the dynamic defense model of Section
2. Thus, the dynamics of the defender’s observer are described by a ﬁnite state
automaton with state space S and transitions that obey the dynamics deﬁned
by the function f (St, dt, a(cid:48)

t).

Using the defender’s observer we formulate Problem (P (cid:48)

D) as follows.

min
g∈G(cid:48)

max
t ∈Z,t∈T

Zg

(cid:20)
CZg

t

βt

+ ˆC(cid:0)dt

(cid:1)

(cid:21)(cid:41)

(cid:40)

(cid:88)

t∈T

subject to Assumptions (A1) – (A6),

dt = gt(St), t ∈ T ,
Z g
t ∈ St, t ∈ T ,
St+1 = f (St, dt, a(cid:48)

t), t ∈ T .

(P (cid:48)

D)

where G(cid:48) := {g | g := {gt, t ∈ T }, gt : S → D for all t ∈ T }.

4 Dynamic Programming Solution for the Defender’s

Problem

4.1 The Dynamic Program

We solve Problem (P (cid:48)
corresponding to Problem (P (cid:48)

D) using dynamic programming. The dynamic program

D) is
(cid:20)
CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

V (S) = min
d∈D

max
Z∈S

(cid:21)
βV (S(cid:48))

.

(4)

for every S ∈ S (see [2, 12]), where Q(S, d, Z) is the set of observer states that
can be reached by S when the defender’s action is d and the true system state
in S is Z. The set Q(S, d, Z) is determined as follows. If at time t the observer’s
state is S and the defender takes action d then, before the eﬀect of d at time
t+ and the observation at time t++ are realized, there will be several potential
candidate observer states at t + 1. Only a subset of these possible observer states
can occur when the true state of the system at time t is Z ∈ S. This subset is
Q(S, d, Z). We illustrate the form of the set Q(S, d, Z) by the following example.
Example 1. Assume a network of three computers and a current observer state
of

St = {(F, N, N ), (F, N, R), (F, R, N )}.

If the defender takes action E2 then, before the eﬀect of E2 and the observation
H 1,2 at t++ are realized, the possible observer states St+1 are

(cid:8){(F, W, N ), (F, W, R)}, {(F, W, N )}(cid:9).

If the true system state is Zt = (F, N, R) then

Q(St, E2, Zt) = {(F, W, N ), (F, W, R)}.

(cid:52)

4.2 Solution of the Dynamic Program

We obtain the solution of the dynamic program, Equation (4), via value iteration
[2, 12]. For that matter, we deﬁne the operator T by

T V (S) := min
d∈D

max
Z∈S

(cid:20)
CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

(cid:21)
βV (S(cid:48))
.

(5)

We prove the following result.

Theorem 1. The operator T , deﬁned by Equation (5), is a contraction map.

Proof. We use Blackwell’s suﬃciency theorem (Theorem 5, [4]) to show that T
is a contraction mapping. We show:

i) Bounded value functions: First, note that |S|, |D| < ∞, and that we have
bounded costs, CZ ≤ M1 < ∞, ∀ S ∈ S; ˆC(d) ≤ M2 < ∞, ∀ d ∈ D. Starting
from any bounded value function, V (S) ≤ M3 < ∞ with M3 > M1+M2
1−β we
have

T V (S) = min
d∈D

max
Z∈S

(cid:20)
CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

(cid:21)
βV (S(cid:48))

≤ M1 + M2 + βM3 < M3 < ∞

for all S ∈ S.

ii) Monotonicity: Assume V2(S) ≥ V1(S) ∀ S ∈ S. Then, for all S ∈ S, Z ∈ S

and d ∈ D,

CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

βV2(S(cid:48)) ≥ CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

βV1(S(cid:48))

Therefore, for all S ∈ S and d ∈ D

(cid:20)

CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

(cid:21)
βV2(S(cid:48))

≥

(cid:20)
CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

(cid:21)
βV1(S(cid:48))

max
Z∈S

max
Z∈S

Hence,

T V2(S) = min
d∈D

max
Z∈S

≥ min
d∈D

max
Z∈S

(cid:20)
CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

(cid:20)
CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

(cid:21)
βV2(S(cid:48))

(cid:21)
βV1(S(cid:48))

= T V1(S).

iii) Discounting: Assume V2(S) = V1(S) + a. Then, for all S ∈ S

T V2(S) = min
d∈D

max
Z∈S

= min
d∈D

max
Z∈S

(cid:20)
CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

(cid:20)
CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

(cid:21)
β(V1(S(cid:48)) + a)

(cid:21)
βV1(S(cid:48))

+ βa

= T V1(S) + βa.

By Blackwell’s suﬃciency theorem, the operator T is a contraction mapping. (cid:3)

Since T is a contraction mapping, we can use value iteration to obtain the
solution to Equation (4), which we term the stationary value function, V ∗(S).
From the stationary value function, we can obtain an optimal policy, g∗, as
follows

g∗(S) = arg min

d∈D

max
Z∈S

(cid:20)
CZ + ˆC(d) + max

S(cid:48)∈Q(S,d,Z)

(cid:21)

βV (S(cid:48))

The optimal policy, g∗(S), is not always unique. That is, for a given observer state
S ∈ S, there could be multiple d ∈ D which achieve the same minimum value of
(cid:21)
(cid:20)
CZ + ˆC(d) + maxS(cid:48)∈Q(S,d,Z) βV (S(cid:48))
. We denote by D∗(S) the

mind∈D maxZ∈S
set of optimal actions for a given observer state S. In the event that D∗(S) is not
a singleton for a given state S, we choose a single action d∗(S) ∈ D∗(S) based
on a quantity we deﬁne as the conﬁdentiality threat. The conﬁdentiality threat
is a measure of the degree to which computer i is presumed (by the defender) to
be compromised and is deﬁned as follows

˜Ti =

(cid:88)

Z∈S

c(zi), i ∈ N

where c(zi), zi ∈ M, is the cost of the state, as deﬁned in Equation (1), of
the ith computer in the candidate system state Z ∈ S. Summing over all can-
didate system states in the observer state S for a given computer i, we obtain
the conﬁdentiality threat ˜Ti. Next, we compare the conﬁdentiality threat of each
computer and choose the action d∗(S) ∈ D∗(S) that corresponds to the highest
conﬁdentiality threat. In the case of equal conﬁdentiality threats (which arise
when the observer state is symmetric), we choose the action in D∗(S) corre-
sponding to the computer with the lower index i ∈ N .6

5 Optimal Defender’s Policy

We now discuss the characteristics of the optimal policy for Problem (P (cid:48)
D),
henceforth referred to as the optimal policy. We illustrate sensitivity analysis
via numerical results for both a two computer and a three computer network.
We also discuss some qualitative observations of the optimal policy.

First we note that determining the set of observer states and its associated
dynamics is not a trivial computational task, even for moderately sized networks.
Our calculations show for the case of a two computer network, the defender’s
observer automaton consists of 87 states and 1207 transitions. Extending the
system to a three computer network results in 1423 states with 65602 transitions.
To automate the procedure, we have developed a collection of programs which
makes use of the UMDES-LIB software library [1]. The speciﬁc procedure is
discussed in Appendix A.

The sensitivity analysis studies how the cost of re-imaging aﬀects the optimal
policy. For both the two computer and three computer networks, we increase the
re-image cost, ˆC(Ri) = r, ∀i ∈ N , and observe how the optimal policy behaves.
Since the number of observer states in the two computer network, denoted |S2|,
is modest, |S2| = 87, we are able to plot the behavior for each observer state
S ∈ S2, as seen in Figure 4(a).7 In the three computer network, the size of
observer state space, |S3| = 1423, is much larger than that of the two computer

6 This choice is arbitrary; we could randomize the choice as well.
7 The “ordering” of these states is arbitrary.

network. As a result, we plot the percentage of observer states that have the
optimal action d, for all d ∈ D, and analyze how the percentage changes as we
increase r, as seen in Figure 4(b).

(a) Two computer network. Optimal
actions for each observer state as a
function of increasing re-image cost,
r = 3, . . . , 30.

(b) Three computer network. Percentage
of observer states that have optimal ac-
tions d ∈ D as a function of increasing
re-image cost, r = 0.2, . . . , 60.

Fig. 4: Sensitivity analysis for varying re-image cost r, where r = ˆC(Ri) for all i ∈ N .
Other parameters are ˆC(Nd) = 0, ˆC(Ei) = 0.1 ∀ i ∈ N , cN = 0, cR = 1, cW = 2,
cF = 8, and β = 0.9.

The behavior of the optimal policy due to increasing re-image costs, r, is
intuitive. As r increases, the optimal policy exhibits a threshold form,8 switching
from specifying more expensive actions to less expensive actions. For very low
re-image costs, the optimal policy speciﬁes Ri in the majority of the observer
states. As r increases, observer states for which Ri was optimal, switch to either
sense, Ei, or null, N d. Once the optimal action is null, it remains null for all
higher values of r. For the observer states where the action switched to sense,
a further increase in r may result in a switch to null; however, there exist some
observer states where the optimal action is sense for all higher values of r. This
threshold behavior is clearly depicted in Figure 4(a).

As a result of the aforementioned threshold behavior, for high enough values
of r, the optimal policy eventually speciﬁes N d or Ei for all states S ∈ S. The
argument to see why there is no re-image action for high values of r is straight-
forward; at these values of r the cost of re-imaging is prohibitively expensive and
the defender would rather incur the cost of being in a poor system state (see
Equation (2)).

An interesting (related) observation can be seen by analyzing the charac-
teristics of the observer states and how these characteristics inﬂuence when the
policy undergoes a switch as r increases. Consider Figure 4(a), and observe the

8 In the simulations that we have performed.

05914182327301020304050607080 Reimage cost  States R1R2R3E1E2E3Nd1R1R2R3E1E2E3Nd1R1R2R3E1E2E3Nd1R1R2R3E1E2E3Nd1R1R2R3E1E2E3Nd10.213253850600255075100 Reimage cost % of states each action is chosen by optimal policyR1R2R3E1E2E3Nd1R1R2R3E1E2E3Nd1R1R2R3E1E2E3Nd1R1R2R3E1E2E3Nd1R1R2R3E1E2E3Nd1R1R2R3E1E2E3Nd1R1R2R3E1E2E3Nd1behavior of the optimal policy around the re-image cost of r = 20. There is a col-
lection of observer states (with indices 74 – 87) that contain the (F, F ) element
(both computers are in the remote compromised state) where the optimal policy
speciﬁes a switch from re-image to null. In these observer states, the defender
believes that the true system state is so poor that, even if the a computer were
to be re-imaged, the events in A would cause the system to transition back to
a poor state in so few iterations that the defender would just be wasting its
resources by re-imaging. That is, the number of time steps that it takes for the
system to return to a poor state is not high enough to justify the cost that the
defender must incur to keep the system in a secure operating mode. For this
reason, in these observer states, the defender exhibits the passive behavior of
giving up by choosing the cheapest action, N d. An interesting related observa-
tion is that for other observer states in the system (the observer states that do
not contain the element (F, F )) the optimal policy speciﬁes a switch away from
re-image at a higher re-image cost (around r ∈ [25 26]). In these observer states
the defender views the process of securing the system as economically eﬃcient
because it can be returned to a secure operating mode in a small enough num-
ber of iterations (compared to the observer states that contain the system state
(F, F )). This observed behavior reﬂects the fact that attacks are progressive and
that time has value in our model.

Another observation is that there are sets of parameters for which the sense
action is useful (as seen starting in Figure 4(a) around r = 2 and peaking in
Figure 4(b) around r = 25). In these cases the act of sensing a computer results
in a split observer state that has a lower future cost than if the defender were to
choose either null or re-image. Thus, paying the cost to sense can result in the
defender having a better idea of the underlying system state and thus make a
wiser decision on which future action to take. However, for low values of r, we
can see that the defender prefers to re-image over obtaining a better estimate
of the system (and similarly for high values of r, the defender prefers to take
the null action). This behavior highlights the duality between estimation and
control.

Interestingly, sensing remains an optimal action even for high values of r
when there is no re-image action prescribed in the optimal defense policy. In
these cases, even though sensing does not change the state of the network, it
reﬁnes the defender’s information which then results in a lower future cost for
the defender. Even though the sense action is more expensive than the null
action, this lower future cost causes the defender to choose sense over null.

The intent of determining an optimal policy is to oﬀer a set of procedures
for the defender such that the network is able to be kept as secure as possible.
After the defender speciﬁes its costs for actions and costs for states, the optimal
policy speciﬁes a procedure that the defender should follow. For each action the
defender takes, d ∈ D, and for each event it observes, a(cid:48) ∈ A(cid:48), the resulting
observer state is known through the dynamics of the observer state. For each
of these observer states resulting from the sequence of defender actions and
observed events, the optimal policy speciﬁes whether to sense or re-image a

particular computer, or to wait and do nothing. The resulting defender behavior
will keep the network as secure as possible under the min-max cost criterion.

6 Conclusion and Reﬂections

In this paper we have proposed a supervisory control approach to dynamic cyber-
security. We have taken the viewpoint of the defender whose task is to defend
a network of computers against progressive attacks. Some of the attacker ac-
tions are unobservable by the defender, thus the defender does not have perfect
knowledge of the true system state. We deﬁne an observer state for the defender
to capture this lack of perfect knowledge.

We have assumed that the defender takes a conservative approach to preserv-
ing the security of the system. We have used the min-max performance criterion
to capture the defender’s conservative approach.

Dynamic programming was used to obtain an optimal defender policy to
Problem (P (cid:48)
D). The numerical results show that the optimal policy exhibits a
threshold behavior when the cost of actions are varied. We have also observed
the duality of estimation and control in our optimal policy.

We believe that our approach is suitable for modeling interactions between
an attacker and a defender in general security settings. In general, we can use our
approach to study dynamic defense against attacks in a network of N resources
each with M (orderable) security levels and M − 1 security boundaries. The
attack actions can penetrate through some of these boundaries to compromise a
resource, or use a compromised resource to attack other resources in the network.
Some of these actions can be unobservable to the defender. On the other hand,
the defender can take actions to change the state of resources to a more secure
operating mode or sense the system state to obtain more reﬁned information
about the system’s status.

The model we have deﬁned is rich enough to be extended to capture more
complicated environments. Some examples of such environments can be hetero-
geneity of the network’s computers9 or the introduction of a dummy computer10
into the system so as to increase the network’s resiliency to attacks.

One bottleneck of our approach is that the number of states and transitions
grows exponentially with the number of computers. One solution to this is to
use a hierarchical decomposition for the system. For example an Internet Service
Provider (ISP) can model a collection of nodes in their network as one region
(resource). Once a non-secure region is observed in the system, the ISP can more
carefully analyze the nodes within that region and take appropriate actions. Ap-
proximate dynamic programming methods could also be useful in dealing with
systems with a large number of computers.

9 Placing an importance weight on each computer.
10 The dummy computer contains no sensitive information and is meant to mislead the

attacker.

Acknowledgement

This work was supported in part by NSF grant CNS-1238962 and ARO MURI
grant W911NF-13-1-0421. The authors are grateful to Eric Dallal for helpful dis-
cussions.

A Appendix – UMDES-LIB

The UMDES-LIB library [1] is a collection of C-routines that was built to
study discrete event systems that are modeled by ﬁnite state automata. Through
speciﬁcation of the states and events of a system automaton (along with the
controllability and observability of events), the library can construct an entity
termed the observer automaton. In our problem the observer automaton is the
defender’s observer automaton, since we take the viewpoint of the defender.
Thus, the observer automaton consists of the defender’s observer states.

In this appendix we describe an automated process11 for extracting the de-
fender’s observer state from the system automaton that makes use of UMDES-
LIB. This requires ﬁrst constructing the system automaton in an acceptable
format for the library while preserving all the features of our model. After run-
ning the library on the provided system automaton, we extract the defender’s
observer state from the observer automaton output. This method allows one to
construct the defender’s observer state for any number of computers.12

Constructing the System Automaton. The input that we provide to UMDES-
LIB is the system automaton from the defender’s viewpoint, as illustrated earlier
in Figure 3.

In order to preserve all features of our model in the resulting observer au-
tomaton, we need to introduce additional sensing actions. Recall that the sense
action, {Ei}i∈N , causes the system automaton to transition to the same state
as the null action, N d (see Figure 3). However, as stated in Section 2, the sense
action updates the information state of the defender. In order to ensure that
UMDES-LIB captures this functionality, we expand the sense action Ei for each
computer i into |M| distinct actions, denoted by Ezi
i , which represent sensing
computer i when it is in state zi ∈ M. This results in a reduced level of uncer-
tainty for the defender as it splits the observer state into, at most, |M| possible
sets of observer states. The admissible actions from {Ezi
i }zi∈M, at a given sys-
tem state, are the sense actions that correspond to the true system state. For
example, from the system state Zt = (N, R, W ), the admissible sense actions are
EN
3 . The above example of the expanded sense action is perhaps
worrisome at ﬁrst glance – if the only admissible sense actions from the current
state are the ones that correspond to the current state of the computer, then
the defender will know what the current state of each computer is, eliminating
the need for a sense action. However, the observer state that is obtained from

2 , and EW

1 , ER

11 Source code is available upon request.
12 The only bottleneck being the (potentially large) dimensionality of the problem.

each expanded sense action is the same as the observer state that is obtained if
the defender were to observe the true, unknown state of a computer.

Running UMDES-LIB on the system automaton with the expanded sense

actions results in the observer automaton.

Extracting the Defender’s Observer State. The output of UMDES-LIB
is the observer automaton, from which we must extract the defender’s observer
state. First, since the defender does not have the ability to choose the expanded
sense actions, Ezi
i , we re-group them into a single, non-deterministic action,
Ei ∈ D, for each i ∈ N . Next, we need to extract the function, f : S ×D×A(cid:48) → S
from the observer automaton. The observer automaton, generated by UMDES-
LIB, takes the form of a bipartite graph; one collection of states of the bipartite
graph is observer states over system states Z, denoted S, whereas the other
collection is observer states over intermediate states ˜Z, denoted ˜S. Defender
actions, D, are the only admissible actions from observer states S. The defense
action d ∈ D causes a transition13 to an observer state in ˜S, where only events
in A(cid:48) are admissible. Each event a(cid:48) ∈ A(cid:48) causes a transition back to an observer
state in S. Repeating this process for all observer states in S, actions d ∈ D,
and events a(cid:48) ∈ A(cid:48), the function f : S × D × A(cid:48) → S is deﬁned. To construct the
set Q(S, d, Z) we follow the approach described in Section 4.1 and illustrated by
Example 1.

References

[1] Umdes-lib. https://www.eecs.umich.edu/umdes/toolboxes.html. Aug., 2000.
[2] D. P. Bertsekas. Dynamic programming and optimal control, volume 1. Athena

Scientiﬁc Belmont, MA, 1995.

[3] V. Bier, S. Oliveros, and L. Samuelson. Choosing what to protect: Strategic
defensive allocation against an unknown attacker. Journal of Public Economic
Theory, 9(4):563–587, 2007.

[4] D. Blackwell. Discounted dynamic programming. The Annals of Mathematical

Statistics, pages 226–235, 1965.

[5] M. Bloem, T. Alpcan, and T. Ba¸sar. Optimal and robust epidemic response for

multiple networks. Control Engineering Practice, 17(5):525–533, 2009.

[6] M. Bloem, T. Alpcan, S. Schmidt, and T. Basar. Malware ﬁltering for network
security using weighted optimality measures. In Control Applications, 2007. CCA
2007. IEEE International Conference on, pages 295–300. IEEE, 2007.

[7] R. B¨ohme and M. F´elegyh´azi. Optimal information security investment with
In Decision and Game Theory for Security, pages 21–37.

penetration testing.
Springer, 2010.

[8] T. M. Chen and N. Jamil. Eﬀectiveness of quarantine in worm epidemics.

In
Communications, 2006. ICC’06. IEEE International Conference on, volume 5,
pages 2142–2147. IEEE, 2006.

[9] S. Hart. Discrete colonel blotto and general lotto games. International Journal

of Game Theory, 36(3-4):441–460, 2008.

13 This transition may be non-deterministic due to the sense action.

[10] M. Khouzani, S. Sarkar, and E. Altman. Maximum damage malware attack in
mobile wireless networks. Networking, IEEE/ACM Transactions on, 20(5):1347–
1360, 2012.

[11] M. Khouzani, S. Sarkar, and E. Altman. Saddle-point strategies in malware attack.

Selected Areas in Communications, IEEE Journal on, 30(1):31–43, 2012.

[12] P. R. Kumar and P. Varaiya. Stochastic systems: estimation, identiﬁcation and

adaptive control. Prentice-Hall, Inc., 1986.

[13] J. Ligatti, L. Bauer, and D. Walker. Edit automata: Enforcement mechanisms
for run-time security policies. International Journal of Information Security, 4(1-
2):2–16, 2005.

[14] J. Ligatti, L. Bauer, and D. Walker. Run-time enforcement of nonsafety poli-
cies. ACM Transactions on Information and System Security (TISSEC), 12(3):19,
2009.

[15] K. Lye and J. M. Wing. Game strategies in network security.

International

Journal of Information Security, 4(1-2):71–86, 2005.

[16] L. Mastroleon. Scalable resource control in large-scale computing/networking in-

frastructures. ProQuest, 2009.

[17] J. Rowe, K. N. Levitt, T. Demir, and R. Erbacher. Artiﬁcial diversity as maneuvers
in a control theoretic moving target defense. In National Symposium on Moving
Target Research, 2012.

[18] S. Roy, C. Ellis, S. Shiva, D. Dasgupta, V. Shandilya, and Q. Wu. A survey of
game theory as applied to network security. In System Sciences (HICSS), 2010
43rd Hawaii International Conference on, pages 1–10. IEEE, 2010.

[19] F. B. Schneider. Enforceable security policies. ACM Trans. Inf. Syst. Secur.,

3(1):30–50, Feb. 2000.

[20] G. Schwartz. Blotto games for security, review and directions. Presented at
NetEcon Meeting, University of California, Berkeley (Private communication),
2013.

[21] M. Van Dijk, A. Juels, A. Oprea, and R. L. Rivest. Flipit: The game of stealthy

takeover. Journal of Cryptology, 26(4):655–713, 2013.

[22] Z. Yin, D. Korzhyk, C. Kiekintveld, V. Conitzer, and M. Tambe. Stackelberg
vs. nash in security games: Interchangeability, equivalence, and uniqueness. In
Proceedings of the 9th International Conference on Autonomous Agents and Mul-
tiagent Systems: volume 1-Volume 1, pages 1139–1146. International Foundation
for Autonomous Agents and Multiagent Systems, 2010.

