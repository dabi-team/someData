Learning to Speed Up Query Planning in Graph Databases

Mohammad Hossain Namaki†, F A Rezaur Rahman Chowdhury†
Md Rakibul Islam, Janardhan Rao Doppa, and Yinghui Wu
{mnamaki, fchowdhu, mislam1, jana, yinghui}@eecs.wsu.edu
School of EECS, Washington State University, Pullman, WA, USA, 99163
† Equal contribution from ﬁrst two authors

8
1
0
2

n
a
J

1
2

]

B
D
.
s
c
[

1
v
6
6
7
6
0
.
1
0
8
1
:
v
i
X
r
a

Abstract

Querying graph structured data is a fundamental opera-
tion that enables important applications including knowledge
graph search, social network analysis, and cyber-network se-
curity. However, the growing size of real-world data graphs
poses severe challenges for graph databases to meet the
response-time requirements of the applications. Planning
the computational steps of query processing – Query Plan-
ning – is central to address these challenges. In this pa-
per, we study the problem of learning to speedup query
planning in graph databases towards the goal of improving
the computational-efﬁciency of query processing via training
queries. We present a Learning to Plan (L2P) framework that
is applicable to a large class of query reasoners that follow
the Threshold Algorithm (TA) approach. First, we deﬁne a
generic search space over candidate query plans, and iden-
tify target search trajectories (query plans) corresponding to
the training queries by performing an expensive search. Sub-
sequently, we learn greedy search control knowledge to im-
itate the search behavior of the target query plans. We pro-
vide a concrete instantiation of our L2P framework for STAR,
a state-of-the-art graph query reasoner. Our experiments on
benchmark knowledge graphs including DBpedia, YAGO,
and Freebase show that using the query plans generated by
the learned search control knowledge, we can signiﬁcantly
improve the speed of STAR with negligible loss in accuracy.

Introduction
Database technology has been successfully leveraged to im-
prove the scalability and efﬁciency of artiﬁcial intelligence
(AI) and machine learning (ML) algorithms (Niu et al. 2011;
Sarkhel et al. 2016; Das et al. 2016a; Zhang, Kumar, and
R´e 2016). This paper focuses on the opposite direction of
this successful cross-fertilization. We investigate ways to
improve the computational-efﬁciency of querying databases
by leveraging the advances from AI search, planning, and
learning techniques. In this work, we study the following
problem: how can we automatically generate high-quality
query plans, for minimizing the response time to ﬁnd correct
answers, by analyzing training queries drawn from a target
distribution?. This problem can be seen as an instance of
learning to speed up structured prediction (Fern 2010; Xu,

Copyright c(cid:13) 2017, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Fern, and Yoon 2009; Doppa, Fern, and Tadepalli 2014a;
Doppa et al. 2014; Lam et al. 2015).

Graph querying is the most primitive operation for infor-
mation access, retrieval, and analytics over graph data that
enables applications including knowledge graph search, and
cyber-network security. We consider the problem of query-
ing a graph database, where the input is a data graph and
a graph query, and the goal is to ﬁnd the answers to the
given query by searching the data graph. For example, to
detect potential threats, a network security expert may want
to “ﬁnd communication patterns matching the attack pattern
of a newly discovered Worm” over a cyber network (Chin Jr
et al. 2014). This natural language query is issued using a
formal graph query language like SPARQL. Speciﬁcally, we
study the general top-k graph querying problem as illus-
trated in Example 1.

Figure 1: Illustration of Top-k graph querying problem.

Example 1. Consider a graph query Q on DBpedia knowl-
edge graph, shown in Figure 1. We want to ﬁnd the artists
who work with “J.Lo” in a Band. Each of these ambiguous
nodes in Q can have excessive number of candidate matches.
For example, there are 31,771 nodes with label “Band” and
82,853 “Artists” in DBpedia. In addition, “J.Lo” can match
to different people whose ﬁrst name starts with “J” or last-
name starts with “Lo”. While “Jennifer Lopez” is the perfect
match for this query, users may want to get other similar
people like “Jennifer Hudson”, etc. However, the matching
score for each candidate node can be different due to dif-
ferent node context. It is computationally expensive to ﬁrst
expand all of those matches to ﬁnd the query structure and
then rank them based on their matching scores.

The growing size of real-world data graphs poses severe
challenges for graph databases to meet the response-time re-
quirements of the applications. Planning the computational
steps of query processing – Query Planning – is central to

ArtistBandJ.Lo(Artist)Jennifer LealandTim & Bob(Band)Canela Cox(Artist)(Artist)Jennifer LopezQuery QGraph Ga top 1 match...0.80.71.0 
 
 
 
 
 
address these challenges. In this work, we answer the main
research question in the context of Threshold Algorithm
(TA) framework (Fagin, Lotem, and Naor 2003) for query
processing that is widely popular in both graph and rela-
tional databases. The TA framework works as follows. First,
the given query is decomposed into sub-queries (e.g., star
shaped queries for graph query). Subsequently, it follows a
ﬁxed fetch-and-verify strategy iteratively until a termination
criterion (via estimates of lower and upper bound) is met.
This framework has two major drawbacks: 1) It is very con-
servative in the upper bound estimation. This often leads to
more computation without improvement in results; and 2) It
applies static fetch and join policy for all the queries that will
likely degrade its performance over a heterogeneous query
set. In their seminal G¨odel prize-winning work, Fagin and
colleagues suggested the usefulness of ﬁnding good heuris-
tics to further improve the computational-efﬁciency of TA
framework as an interesting open problem (Fagin, Lotem,
and Naor 2003). Our work addresses this open problem with
strong positive results. To the best of our knowledge, this
is the ﬁrst work that tightly integrates learning and search
to improve the computational-efﬁciency of query processing
over graph databases.

We develop a general Learning to Plan (L2P) framework
that is applicable to a large class of query reasoners that fol-
low the TA approach. First, we deﬁne a generic search space
over candidate query plans, where the query plan of TA
framework can be seen as a greedy search trajectory. Second,
we perform a computationally-expensive search (heuristic
guided beam search) in this search space, to identify target
search trajectories (query plans) corresponding to the train-
ing queries, that signiﬁcantly improve over the computation-
time of query plans from the TA framework. Third, we
learn greedy policies in the framework of imitation learn-
ing to mimic the search behavior of these target trajectories
to quickly generate high-quality query plans. We also pro-
vide a instantiation of our L2P framework for STAR, a state-
of-the-art graph query reasoner and perform comprehensive
empirical evaluation of L2P-STAR on three large real-world
knowledge graphs. Our results show that L2P-STAR can sig-
niﬁcantly improve the computational-efﬁciency over STAR
with negligible loss in accuracy.

Background
In this section, we provide the background on graph query
processing and the general TA framework as applicable to
both graph and relational databases.
Data Graph. We consider a labeled and directed data graph
G=(V, E, L), with node set V and edge set E. Each node
v ∈ V (edge e ∈ E) has a label L(v) (L(e)) that speciﬁes
node (edge) information, and each edge represents a rela-
tionship between two nodes. In practice, L may specify het-
erogeneous attributes, entities, and relations (Lu et al. 2013).
Graph Query. We consider query Q as a graph
(VQ, EQ, LQ). Each query node in Q provides infor-
mation/constraints about an entity, and an edge between
two nodes speciﬁes the relationship posed on the two
nodes. Formal graph query languages including SPARQL
(Prud’Hommeaux, Seaborne, and others 2008), Cypher, and

Algorithm 1 TA Framework for Graph Search
Input: a graph query Q, a data graph G, integer k
Output: top-k match set Q(G, k)
1: Decompose Q into a set of sub-queries Q
2: repeat
3:
4:
5:
6: until UB > LB
7: return Q(G, k)

fetch k partial matches for sub-queries in round-robin way
join to assemble new matches;
update lowerbound (LB) and upperbound (UB);

Gremlin can be used to issue graph queries to a database
such as Neo4j(neo4j.com). Since existing graph query
languages are essentially subgraph queries, we can invoke
a query transformer to work with different query languages
(Kim et al. 2015). Therefore, our work is general and is not
tied to any speciﬁc query language.
Subgraph Matching. Given a graph query Q and a data
graph G, a match of Q in G is a mapping φ ⊆ VQ × V , such
that: 1) φ is an injective function, i.e., for any pair of distinct
nodes ui and uj in VQ, φ(ui) (cid:54)= φ(uj); and 2) for any edge
(vi, vj) ∈ EQ, (φ(vi), φ(vj)) ∈ E. The match φ(Q) is a
complete match if |Q| = |φ(Q)|, where |Q| denotes the size
of Q, i.e., the sum of the number of nodes and edges in Q
(similarly for |φ(Q)|). Otherwise, φ(Q) is a partial match.
Matching Score. Given Q and a match φ(Q) in G, we as-
sume the existence of a scoring function F (·) which com-
putes, for each node v ∈ VQ (resp. each edge e ∈ EQ),
a matching score F (φ(v)) (resp. F (φ(e))). The matching
score of φ is computed by a function F (φ(Q)) as

F (φ(Q)) =

(cid:88)

v∈VQ

F (v, φ(v)) +

(cid:88)

e∈EQ

F (e, φ(e))

(1)

One can use any similarity function such as acronym, ab-
breviation, edit distance etc. We adopt Levenshtein func-
tion (Navarro 2001) to compute node/edge label similarity,
and employ the R-WAG’s ranking function that incorporates
both label and structural similarities (Roy, Eliassi-Rad, and
Papadimitriou 2015) to compute matching score F (·).
Top-k Graph Querying. Given Q, G, and F (·), the top-
k subgraph querying problem is to ﬁnd a set of k matches
Q(G, k), such that for any match φ(Q) /∈ Q(G, k), for all
φ(cid:48)(Q) ∈ Q(G, k), F (φ(cid:48)(Q)) ≥ F (φ(Q)).
Top-k Search Paradigm. Top-k graph querying problem
is known to be intractable (NP-hard). The common prac-
tice in existing top-k graph search techniques is to fol-
low a conventional Threshold Algorithm (TA) style ap-
proach (Ding et al. 2014; Zeng et al. 2012; Zou et al. 2014;
Yang et al. 2016).
TA Framework for Query Processing. The TA framework
was originally developed for relational databases (Fagin,
Lotem, and Naor 2003), and has been extended to graph
databases as well. It consists of three main steps (see Al-
gorithm 1): Step 1: Query Decomposition. A query Q is
decomposed into a set of sub-queries {Q1, . . . , QT } with-
out loss of generality. The procedure then initializes one
list for each sub-query to store the partial answers. For re-
lational databases, sub-queries can correspond to individ-

straints (e.g., memory, response time, accuracy, I/O cost).

Motivated by the above observations, we explore plan-
ning and learning techniques to automatically induce adap-
tive data-driven search strategies within the TA framework
to improve the efﬁciency of top-k graph query processing.

Problem Setup
We assume the availability of a training set of query-answer
pairs (cid:8)(Q, AP )(cid:9) drawn from an unknown target distribution
D, where Q is a graph query and AP is the corresponding
output answer produced by a TA style algorithm P on data
graph G. The accuracy of a query plan (sequence of compu-
tational steps for query processing) can be measured using
a non-negative loss function L such that L(Q, ˆA, AP ) is the
loss associated with processing a particular input query Q to
produce answer ˆA when the correct answer is AP . The goal
of learning is to quickly produce high-quality query plans
for minimizing the response time to ﬁnd correct answers for
input queries drawn from the target distribution D.

In this work, we formulate the graph query planning prob-
lem in a search-based learning framework. There are three
key elements in this framework: 1) the Search space Sp
whose states correspond to candidate computational states
within the TA framework; 2) the Selection policy Πselect
that is used to select the sub-query to fetch partial matches
at each state; and 3) the Fetching policy Πf etch that is used
to decide how many partial matches to fetch for the selected
sub-query at each state.
Generic Search Space. Sp is a 2-tuple (cid:104)I, A(cid:105), where I is
the initial state function, A gives the set of possible actions
in a given state. In our case, s0 = I(Q), the initial state,
corresponds to a state with one empty list for each sub-
query decomposition of Q (say T decompositions). A(si)
consists of actions that correspond to candidate computa-
tional steps (or search operators for query plans) at state
si. Each action a is of the form (i, δ) and corresponds to
fetching δ additional partial matches for sub-query i, where
i ∈ {1, 2, · · · , T } ∪ HALT and δ ∈ [δmin, δmax]. When
HALT action is chosen at state si (terminal state), we stop
the search and return the top-k answers A(si).

We focus on greedy search (see Algorithm 2). The de-
cision process for producing query plans corresponds to
choosing a sequence of actions leading from the initial
state using both Πselect and Πf etch, until Πselect selects
the HALT action (terminal state). Πselect and Πf etch are
parametrized by feature functions ψ1 and ψ2. We want to
learn the parameters of both Πselect and Πf etch using the
training queries, with the goal of quickly producing query
plans for minimizing the response time to produce correct
answers on unseen queries drawn from D.

Learning to Plan (L2P) Framework
Inspired by the recent success of imitation learning ap-
proaches for solving sequential decision-making tasks (He,
Daum´e III, and Eisner 2012; He, Daum´e III, and Eisner
2014; He, Daum´e III, and Eisner 2013; Pinto and Fern
2014), we formulate and solve the problem of learning poli-

Figure 2: Illustration of TA via STAR query processing.

Figure 3: Illustration of query plan from TA framework.

ual attributes (Fagin, Lotem, and Naor 2003); and for graph
databases, sub-queries can correspond to nodes or edges or
stars (Yang et al. 2016); Step 2: Partial Answer Generation.
For each sub-query Qi, it performs an iterative exploration
as follows. a) It computes and fetches k partial matches of
Qi; and b) It veriﬁes if the partial matches can be joined with
other “seen” partial matches to be able to improve the top-k
matching score; and Step 3: Early Termination. TA dynami-
cally maintains a) an lower bound (LB) as the smallest top-k
match score so far; and b) an upper bound (U B) to estimate
the largest possible score of a complete match from unseen
matches. If U B < LB, TA terminates. Fig 2 illustrates TA
framework for the query Q in Fig 1 and Fig 3 shows the
query plan of TA.
Limitations. The main limitations of TA framework are as
follows: 1) Applying ﬁxed fetch-and-verify strategy for all
the queries (e.g., always fetch a ﬁxed amount of partial an-
swers) may not provide robust performance due to diversity
in queries; 2) The performance of TA framework highly de-
pends on a good estimation of the upper bound. Enforcing a
ﬁxed estimation scheme can be loose, leading to “redundant
fetches” with little improvement in quality of answers (see
Figure 2). More adaptive stopping criterion is desirable; and
3) It is hard to adapt TA framework to different resource con-

J.LoArtistBandArtistBandJenniferLopezCanelaCoxTim &BobCanelaCoxTim&BobQ1Q2SortedAccess partialanswersranked...............top-1JenniferLopezCanelaCoxTim &BobJoin(Fetch)Score=2.5redundantfetchesAnswers**Algorithm 2 L2P for TA Framework
Input: Q = graph query, G = data graph, Πselect = selection
policy, Πf etch = fetching policy
1: s ← I(Q) // initial state
2: TERMINATE ← F alse
3: while not TERMINATE do
4:
5:
6:
7:
8:
9:
10:
11: end while
12: return Top-k answers A(s) corresponding to s

aselect ← Πselect(s) // select the sub-query
if aselect == HALT then
TERMINATE = T rue

af etch ← Πf etch(s, aselect) // how many to fetch
s ← Apply (aselect, af etch) on s

end if

else

cies to produce high-quality query plans in the framework of
imitation learning.
Overview of Imitation Learning. In traditional imitation
learning, expert demonstrations are provided as training data
(e.g., demonstrations of a human expert driving a car), and
the goal of the learner is to learn to imitate the behavior of an
expert performing a task in a way that generalizes to similar
tasks or situations. Typically this is done by collecting a set
of trajectories of the expert’s behavior on a set of training
tasks. Then supervised learning is used to ﬁnd a policy that
can replicate the decisions made on those trajectories. Often
the supervised learning problem corresponds to learning a
classiﬁer or regressor to map states to actions, and off-the-
shelf tools can be used.

The two main challenges in applying imitation learn-
ing framework to query planning are: 1) Obtaining a high-
quality oracle policy that will provide the supervision for
the imitation learning process (oracle construction); and 2)
Learning search control policies that can make search deci-
sions in a computationally-efﬁcient manner (fast and accu-
rate decision-making). We provide solutions for these two
challenges below.

Oracle Construction
In this section, we describe a generic procedure to compute
high-quality query plans within the TA framework. We will
start by deﬁning the needed terminology.
Deﬁnition 1. For a given query-answer pair (Q, AP ), a state
s in the search space Sp is called a terminal state if the cor-
responding top-k answers A(s) is same as AP (output of TA
style algorithm P for query Q).
Deﬁnition 2. For a given query Q, a sequence of actions
from initial state to terminal state, (s0, a0), · · · , (sN , ∅), is
called a target query plan (TQP), where s0 is the initial state
and sN is the terminal state.
Deﬁnition 3. For a given query Q, the quality of a TQP is
deﬁned as the computation time taken to execute the query
plan to produce the top-k answers. In other words, quality
corresponds to speedup w.r.t. computation time of TA style
algorithm P on the same query.

The goal of oracle construction is to compute high-quality
target query plans (Doppa, Fern, and Tadepalli 2014b) for
each training query-answer pair (Q, AP ). A naive approach

would be to perform depth-bounded exhaustive search in
search space Sp instantiated for each training query. Since
the naive approach is impractical, we resort to heuristic
guided beam search and deﬁne effective heuristics to make
the search efﬁcient.
Heuristics. We deﬁne multiple heuristics that can poten-
tially guide the search to uncover high-quality target query
plans. These heuristics are deﬁned as a function of the given
search state s and the terminal state sP of the TA style algo-
rithm P. We deﬁne the following three concrete heuristics
noting that our approach allows to add additional heuristics
as needed: H1) Total computation time to reach state s from
initial state s0 = I(Q) normalized w.r.t. time taken by the
TA style algorithm P; H2) The cost (or score) of answer
A(s) normalized w.r.t. the score of AP (Arai et al. 2007);
and H3) The cumulative difference between the sizes of lists
(with partial matches) for each sub-query from states s and
sP normalized w.r.t. the total size of all lists for sP . Intu-
itively, H1, H2, and H3 correspond to speed, accuracy, and
distance to goal in terms of most primitive action respec-
tively. We seek to balance the accuracy and computational
cost by appropriately weighting the heuristics.
Target Query Plan Computation via Beam Search. We
propose to combine multiple heuristics by their weighted
linear combination: H(s)=(cid:80)m
i=1 wi · Hi(s). For a given
weight vector w ∈ (cid:60)m, the heuristic function is fully spec-
iﬁed. For a given training query-answer pair (Q, AP ), we
perform breadth-ﬁrst beam search with beam width b start-
ing at initial state s0 = I(Q), until we uncover a terminal
state (i.e., A(s) is same as AP ). The sequence of actions
leading from s0 to terminal state s is identiﬁed as the target
query plan for Q (see Algorithm 3).
Computing Weights via Bayesian Optimization. We don’t
know the appropriate weights w1, w2, · · · , wm that will im-
prove the effectiveness of the weighted heuristic function H.
We deﬁne the value of a candidate weight vector w ∈ (cid:60)m
over a set of training queries T , V(w, T ), as the average
quality (speedup) of the identiﬁed target query plans (via
Algorithm 3) for all queries in T with the corresponding
heuristic H. Our goal is to ﬁnd the weight vector w ∈ (cid:60)m
that maximizes V(w, T ):

w∗ = arg maxw∈(cid:60)m V(w, T )

(2)

The main challenge in solving this optimization problem
is that evaluating the value of a candidate weight vector is
computationally-expensive. In recent years, Bayesian Op-
timization (BO) (Shahriari et al. 2016) has emerged as a
very popular framework for solving optimization problems
where the function evaluations are computationally expen-
sive (e.g., hyper-parameter tuning of machine learning algo-
rithms). Therefore, we propose to ﬁnd the best weights via
BO tools. In short, BO algorithms build a statistical model
based on the past function evaluations; and execute a se-
quential decision-making process that intelligently explores
the solution space guided by the statistical model, to quickly
reach a near-optimal solution.

Algorithm 3 Target Query Plan Computation
Input: (Q, AP ) = query and answer pair, (I, A) = Search space,
b = beam width, H = heuristic function
1: B ← s0 = I(Q) // Initial state
2: TERMINATE ← F alse
3: while not TERMINATE do
C ← ∅ // Candidate set
4:
for each state s ∈ B do
5:
if A(s) == AP then
6:
7:
8:
9:
10:
11:
12:
13:
14: end while
15: return state-action pair sequence from s0 to s∗

end if
end for
B ← Top-b scoring states in C via heuristic // Pruning

Expand s and add all next states to C // Expansion

TERMINATE = T rue
s∗ = s

else

Learning Greedy Policies via Imitation Learning
Our goal is to learn a greedy policy (Πselect, Πf etch) that
maps states to actions in order to imitate the target query
plans computed via oracle construction. We assume that for
any training query-answer pair (Q, AP ), we can get the ora-
N , ∅) via Algorithm
cle query plan (s∗
3, where s∗
N is the terminal state.
The goal is to learn the parameters of Πselect and Πf etch
such that at each state s∗

0), (s∗
0 is the initial state and s∗

1), · · · , (s∗

1, a∗

0, a∗

i ) is selected.

i ∈ A(s∗

i , a∗

Algorithm 4 Learning Greedy Policy via Exact Imitation
Input: T = Training data
1: Initialize the set of classiﬁcation examples D1 = ∅
2: Initialize the set of regression examples D2 = ∅
3: for each training example (Q, AP ) ∈ T do
Compute the target query plan (s∗
4:
for each search step t = 0 to N do
5:
6:

0), · · · , (s∗

N , ∅)

0, a∗

Generate classiﬁcation example Ct and regression exam-
ple Rt to imitate (s∗
Aggregate training data: D1 = D1 ∪ Ct and D2 = D2 ∪
Rt
end for

t , a∗
t )

7:

8:
9: end for
10: Πselect = Classiﬁer-Learner(D1)
11: Πf etch = Regression-Learner(D2)
12: return greedy policy (Πselect, Πf etch)

t , a∗

t ) as input and a∗

t (1)) as input and a∗

Exact Imitation Approach. At each state s∗
t on the target
path of a training example (Q, AP ), we create one clas-
siﬁcation example with ψ1(s∗
t (1) (selec-
tion action) as output; and one regression example with
ψ2(s∗
t (2) (fetching action) as output.
The sets of aggregate classiﬁcation and regression imitation
examples collected over all the training queries are then fed
to a classiﬁer and regression learner pair, to learn the param-
eters of Πselect and Πf etch (see Algorithm 4). This reduc-
tion allows us to leverage powerful off-the-shelf classiﬁca-
tion and regression learners. In theory and practice, policies
learned via exact imitation can be prone to error propaga-
tion: errors in the previous state contributes to more errors.

DAgger Algorithm. DAgger is an advanced imitation ap-
proach (Ross, Gordon, and Bagnell 2011) that generates ad-
ditional training data so that the learner is able to learn from
its mistakes. It is an iterative algorithm, where each itera-
tion adds imitation data to an aggregated data set. The ﬁrst
iteration follows the exact imitation approach. After each it-
eration, we learn policy (Πselect, Πf etch) using the current
data. Subsequent iterations perform query planning using
the learned policy to generate a trajectory of states for each
training query. At each decision along this trajectory, we add
a new imitation example if the search decision of the learned
policy is different from the oracle policy. In the end, we se-
lect the best policy over all the iterations via performance on
validation data. To select among several imperfect policies
with varying speed and accuracy performance, we pick the
policy with the highest accuracy. This principle is aligned
with our learning objective.

Handling General Query Planning
In this section, we provide some discussion on ways to ex-
tend our L2P framework to more general query planning go-
ing beyond the TA style query processing.

There are three key elements in the L2P framework: 1)
Search space over query plans; 2) Policy for producing query
plans; and 3) Oracle query plans to drive the learning pro-
cess. The search operators for query plans are speciﬁc to
each query processing approach. We need to consider ad-
ditional search operators (e.g., different δ values in the TA
framework) to be able to construct high-quality candidate
query plans in the search space. The form of the policy will
depend on the search operators or actions at search states
(e.g., classiﬁer-regressor pair to select the sub-query and
number of partial matches to fetch in the TA framework).
For computing the oracle plans needed to learn the policy,
heuristic-guided beam search is a generic approach, but we
need to deﬁne effective heuristics as applicable for the given
query evaluation approach. A more generic off-the-shelf al-
ternative is to consider Approximate Policy Iteration (API)
algorithm (Fern, Yoon, and Givan 2006). API starts with a
default policy and iterates over the following two steps. Step
1: Generate trajectories of current rollout policy from ini-
tial state; and Step 2: Learn a fast approximation of rollout
policy via supervised learning to induce a new policy. In-
deed, each iteration of API can be seen as imitation learning,
where trajectories of current rollout policy correspond to ex-
pert demonstrations and the new policy is induced using the
exact imitation algorithm (Fern 2016).

L2P Instantiation for STAR
In this section, we provide a concrete instantiation of our
L2P framework for STAR (Yang et al. 2016), a state-of-the-
art graph query reasoner based on the TA framework.
Overview of STAR Query Processing. STAR is an instan-
tiation of TA framework, where the graph query is decom-
posed into a set of star-shaped queries. It was shown both
theoretically and empirically that star decomposition pro-
vides a good trade-off between sub-query evaluation cost
and the number of candidate partial answers. Figure 2 illus-
trates STAR query processing to ﬁnd the top-1 answer for

Yago DBPedia
590s

680s

Freebase
1120s

Table 1: Average runtime of Algorithm 3 with b=10.

1 and Q∗

1, . . . , Q∗

the query Q shown in Figure 1. It ﬁrst decomposes Q into
two stars (Q∗
2), i.e., graphs with a unique pivot and
one or more adjacent nodes. It then fetches partial answers
for each star query in a sorted manner, and joins the partial
answers whenever possible, until it ﬁnds a complete match
and termination criteria is met.
Search Space. Suppose the given graph query Q is decom-
posed into a set of star-queries {Q∗
T } without loss
of generality. Recall that each action a (i.e., search oper-
ator for query plan) is of the form (i, δ) and corresponds
to fetching δ additional partial matches for star query i,
where δ ∈ [δmin, δmax]. We employed δmin = 10 and
δmax = 200 (candidate number of partial matches to fetch),
and considered candidate choices in the multiples of δmin,
i.e., δmax
discrete values. This choice is mainly driven by
δmin
the computational complexity of ﬁnding oracle query plans
via breadth-ﬁrst beam search. The branching factor at each
search step is b × T × δmax
δmin
Features. To drive the learning process, we deﬁne feature
functions ψ1 and ψ2 over search states. Our features can be
categorized into three groups: 1) Static features that are com-
puted from the query topology, decomposed star queries,
and initial partial matches. Some examples include the num-
ber of nodes and edges in each star query, the number of
candidate nodes in data graph, and the number of joinable
nodes in a star decomposition; 2) Ranking features that are
computed from the lower bound of current top-k answers
and upper bound for each star query; and 3) Context fea-
tures that are computed based on the current state of L2P-
STAR like the selected star query and the total number of
fetches for each star query. See https://goo.gl/MD51S2 for
complete details of features. Alternatively, we can employ a
deep learning model to learn appropriate representation.

, where b is the beam width.

Experiments and Results

In this section, we present our empirical evaluation.

Experimental Setup
Datasets. We employ three real-world open knowledge
graphs: 1) YAGO (mpi-inf.mpg.de/yago) contains
2.6M entities (e.g., people, companies, cities), 5.6M rela-
tionships, and 10.7M nodes and edge labels, extracted from
several knowledge bases including Wikipedia; 2) DBpedia
(dbpedia.org) consists of 3.9M entities, 16.8M edges,
and 14.9M labels; and 3) Freebase (freebase.com), a
more diversiﬁed knowledge graph that contains 40.3M en-
tities, 180M edges, and 81.6M labels.
Query Workload. We developed a query generator to pro-
duce both training and testing queries following the DBPSB
benchmark (Morsey et al. 2011). We ﬁrst generate a set of
query templates, each has a topology sampled from a graph
category, and is assigned with labels (speciﬁed as “entity

types”) sampled from top 20% most frequent labels in the
real-world graphs. We created 20 templates to cover com-
mon entity types, and generated a total of 2K queries by in-
stantiating the templates. We employ 50% queries for train-
ing, 20% for validation, and 30% for testing respectively.
STAR Implementation. We implemented the STAR query
processing framework (Yang et al. 2016) in Java using the
Neo4j (neo4j.com) graph database system.
Oracle Policy Implementation. We performed heuristic
guided breadth-ﬁrst beam search with different beam widths
b = {1, 5, 10, 20, 50} to compute high-quality target query
plans for each training query (see Algorithm 3). BayesOpt
software (Martinez-Cantin 2014) was employed to ﬁnd the
weights of heuristics with expected improvement as the ac-
quisition function. We did not see noticeable performance
improvement beyond 100 iterations. Since we didn’t get sig-
niﬁcant speedup improvements beyond b = 10, we em-
ployed target query plans (aka Oracle policy) obtained with
b = 10 for all our training and testing experiments.
L2P-STAR
Implementation. We
employed XG-
Boost (Chen and Guestrin 2016), an efﬁcient and scalable
implementation of functional gradient
tree boosting for
classiﬁcation and regression, as our base learner. All hyper-
parameters (boosting iterations, tree depth, and learning
rate) were automatically tuned based on the validation data
using BayesOpt (Martinez-Cantin 2014), a state-of-the-art
BO software. L2P-STAR (Exact) and L2P-STAR (DAgger)
corresponds to policy learning via exact
imitation and
DAgger algorithms respectively. We performed 5 iterations
of DAgger with β = 0.8 and exponential decay. We selected
the policy with the highest accuracy on the validation data.
L2P-STAR needs to additionally store the learned policy
(classiﬁer and regressor pair). However, this overhead is
negligible when compared to the memory usage of Neo4j.
Code and Data. All the code and data is available on a
GitHub repository: https://goo.gl/MD51S2.
Evaluation Metrics. We evaluate STAR, Oracle, and L2P-
STAR using two metrics: 1) Speedup. For a given query
the response time of an algorithm P, denoted as
Q,
Time(P, Q), refers to the total CPU time taken from receiv-
ing the query to ﬁnding the top-k answer. The speedup fac-
tor τ (P, Q) is computed as the ratio of Time(STAR, Q) to
Time(P, Q); and 2) Accuracy. We employ Levenshtein dis-
tance function (Navarro 2001) to compute node/edge label
similarity, and R-WAG’s ranking function(Roy, Eliassi-Rad,
and Papadimitriou 2015) to compute the matching score
F (·). For a given query Q, the accuracy of an algorithm P is
deﬁned as the ratio of the matching score of correct answer
AP to the matching score of predicted answer ˆA.

We conducted all our experiments on a Windows server
with 3.5 GHz Ci7 CPU and 32GB RAM conﬁguration. All
experiments are conducted 3 times and averaged results are
presented. We report the average metrics (speedup, accuracy,
and computation time) over all testing queries.

Results and Analysis
Oracle Policy Computation Time. Table 1 shows the aver-
age time to compute the oracle query plan via beam search
with beam width b=10 (see Algorithm 3). The runtime is

STAR
Oracle
Random
L2P-STAR (Exact)
L2P-STAR (DAgger)

speedup
1.00
4.53
3.37
3.66
3.71

YAGO

accuracy
100%
100%
61%
93%
94%

DBpedia

run-time (ms)
1925.78
757.70
1033.6
764.00
804.61

speedup
1.00
5.62
10.40
5.00
5.00

accuracy
100%
100%
66%
97%
97%

run-time (ms)
6401.48
1192.26
549.30
1310.26
1310.26

speedup
1.00
62.53
40.20
47.69
47.71

Freebase

accuracy
100%
100%
54%
95%
96%

run-time (ms)
13932.26
1478.60
2555.59
1550.03
1508.03

Table 2: Speedup, accuracy, and query run-time results comparing STAR, Oracle, Random, and L2P-STAR.

DBpedia

Freebase

Analytical Ops
# Join
13,106
43
13
134

# Fetch
3,876
28
7
80

Computation Time (ms)

Fetch
3,101.51
1,208.89
549.29
1,238.89

Join
3,283.58
6.32
0.05
49.69

ML-Overhead
N/A
N/A
N/A
23.01

Analytical Ops
# Join
20,048
65
15
260

# Fetch
5,732
35
8
133

Computation Time (ms)

Fetch
3,868.65
1,487.12
2,554.76
1,440.39

Join
10,044.39
7.64
0.35
46.52

ML-Overhead
N/A
N/A
N/A
18.34

STAR
Oracle
Random
L2P-STAR

Table 3: Statistics of different analytical operations (e.g., fetch and join calls) and the corresponding computation time.

high and it increases for denser graphs (e.g., Freebase).
Hence, we cannot use this computationally expensive pro-
cedure in real-time and need L2P to quickly generate high-
quality query plans.
STAR vs. Oracle. To ﬁnd out the overall room for improve-
ment via L2P framework, we compare STAR with the ora-
cle policy. Recall that the oracle policy for a given query Q,
corresponds to the target query plan obtained by heuristic-
guided breadth-ﬁrst beam search, and relies on the knowl-
edge of correct answer AP . Table 2 shows the speed, accu-
racy, and run-time; and Table 3 shows the statistics of differ-
ent analytical operations (e.g., fetch and join calls) and the
corresponding computation time.

We make the following observations: 1) There is signiﬁ-
cant room for improving STAR via L2P framework as noted
by speedup of oracle policy for different datasets (4.53 for
YAGO, 5.62 for DBpedia, and 62.53 for Freebase); 2) The
speedup of oracle policy is higher for large and dense data
graphs, e.g.,Freebase. Indeed, for dense graphs, we expect
more candidate matches for each star-query, which can make
the STAR very slow and L2P-STAR would be more beneﬁ-
cial. In fact, there is a growing evidence that the real-world
graphs become denser over time and follow a power-law
pattern (Leskovec, Kleinberg, and Faloutsos 2007); and 3)
The number of fetch/join calls can be reduced by two orders
of magnitude by following the plan from oracle policy. The
number of fetch calls show the number of search steps to
reach the correct terminal state, which is much smaller for
oracle when compared to STAR. As pointed out earlier, one
of the major drawbacks of TA style STAR is that it fetches
many “useless” partial answers that do not contribute to top-
k answers. The performance of oracle policy shows that it
is possible to signiﬁcantly improve STAR if the learner can
successfully imitate the oracle plans.
L2P-STAR vs. Oracle. We compare L2P-STAR with the
oracle policy to understand how well the learner is able to
mimic the search behavior of oracle. We make the following
observations from Table 2. The speed and accuracy of L2P-
STAR in general is very close to the oracle policy across

all the datasets. L2P-STAR loses at most 6% accuracy and
signiﬁcantly improves the speed when compared to STAR.
For example, L2P-STAR improves the speed of STAR by
47 times with an accuracy loss of 5% for Freebase. L2P-
STAR has to learn when to stop the search (i.e., select HALT
action). If L2P-STAR stops the search early, it will lose ac-
curacy. Similarly, it will lose speed when stopping of the
search is delayed. From Table 3, we can see that the over-
head of L2P-STAR to make search decisions (i.e., comput-
ing features and executing the classiﬁer/regressor) is very
small when compared to the query processing time (2% of
query run-time on an average).

We saw small performance improvement in L2P-STAR
by training with DAgger when compared to training with
exact-imitation (except for DBpedia). This is due to our
principle of selecting among multiple imperfect policies:
pick the policy with highest accuracy. We were able to un-
cover policies with higher speedup over exact imitation, but
their accuracy was relatively low.
Random Policy. We also compared with a baseline, where
both Πselect and Πf etch are random. Table 2, 3 show the
results averaged over 20 different runs. Random policy has
better speedup than STAR, but with a signiﬁcant drop in ac-
curacy as it selects the HALT action prematurely.
Ablation Analysis. STAR, and L2P-STAR have their cor-
responding selection and fetching policies. To understand
how the learned selection and fetching policies Πselect and
Πf etch affect STAR individually, we plug them one at a
time. Table 4 shows the results of this analysis for all the
three datasets. If we employ Πf etch for adaptive fetching
inside STAR, we get a speedup of 1.86, 2.19, and 3.36 for
YAGO, DBpedia, and Freebase respectively, without losing
any accuracy. Therefore, when practitioner requires 100%
accuracy, this combination can be deployed. This also shows
the usefulness of Πf etch alone. However, the overall perfor-
mance of STAR with Πf etch alone is much worse than L2P-
STAR and shows the importance of Πselect.
Transfer Learning. The learned policy can be seen as a
function that maps search states to appropriate actions via

YAGO

Expansion
DBpedia

Freebase

Selection

STAR
L2P-STAR

STAR
(1, 100%)
(4.08, 87%)

L2P-STAR
(1.86, 100%)
(3.71, 94%)

STAR
(1, 100%)
(5.81, 90%)

L2P-STAR
(2.19, 100%)
(5.00, 97%)

STAR
(1, 100%)
(34.04, 91%)

L2P-STAR
(3.36,100%)
(47.71, 96%)

Table 4: Results of ablation analysis.

n
i
a
r
T

YAGO
DBpedia
Freebase
Combined

YAGO
(3.71, 0.93)
(4.30, 0.90)
(4.31, 0.90)
(3.87, 0.92)

Test dataset
DBpedia
(3.88, 0.96)
(5, 0.97)
(5.64, 0.90)
(4.27, 0.97)

Freebase
(22, 0.92)
(55.36, 0.89)
(47.71, 0.96)
(55.84, 0.96)

Table 5: Transfer results (speedup, accuracy).

features. Since the feature deﬁnitions are general, one could
hypothesize that the learned knowledge is general and can
be used to query different data graphs. To test this hypoth-
esis, we learned policies on each of the three datasets, an-
other policy using the combination of all the three datasets;
and tested each policy on both individual datasets and the
combined dataset. We make the following observations from
Table 5. The learned policies generalize reasonably well to
datasets that are not used for their training. We get the best
accuracy when training and testing are done on the same
dataset. The only exception is that the policy trained on the
combined dataset gave better performance on Freebase.

Change % (Speedup, Accuracy)

0%
5%
10%

(4.12, 95.9%)
(5.26, 93.2%)
(7.03, 93.1%)

Table 6: Performance with changes to the trained data graph.

Changing Data Graph. To investigate the stability of the
learned policy with changes to the training data graph, we
performed some experiments on DBpedia graph. We do
not have access to the temporal data graph. Therefore, we
created multiple samples of original data graph with vary-
ing sizes by employing the well-studied Forest Fire (FF)
approach (Leskovec and Faloutsos 2006). We train on the
smallest data sample (90% of the original graph) and test
on larger samples. This setup is based on the fact that real-
world graphs are known to become large and dense over
time (Leskovec, Kleinberg, and Faloutsos 2007). From Ta-
ble 6, we can see that by changing 10% of the training data
graph (i.e., 1.6M additional edges), L2P-STAR loses at most
3% accuracy (similar to transfer learning results). Addition-
ally, the speedup of L2P-STAR improves as the data graph
evolves. As explained before, it is natural to expect more
speedup with large and dense graphs: increased candidate
matches for each sub-query can make STAR slower and
L2P-STAR can be more beneﬁcial. Indeed, Table 2 corrob-
orates this hypothesis e.g., Freebase.

In general, we need to update the policy whenever the dis-
tribution of queries and/or data graph changes signiﬁcantly.

Related Work
Our work is related to a sub-area of AI called speedup
learning (Fern 2010). Speciﬁcally, it is an instance of inter-
problem speedup learning. Reinforcement learning (RL),
imitation learning (IL), and hybrid approaches combining
RL and IL have been explored to learn search control knowl-
edge from training problems in the context of diverse appli-
cation domains. Some examples include job shop scheduling
(Zhang and Dietterich 1995), deterministic and stochastic
planning (Xu, Fern, and Yoon 2009; Pinto and Fern 2014),
natural language processing (Jiang et al. 2012; He, Daum´e
III, and Eisner 2012; He, Daum´e III, and Eisner 2013; Ma et
al. 2014), computer vision (Weiss, Sapp, and Taskar 2013;
Weiss and Taskar 2013), hardware design optimization (Das
et al. 2015; Das et al. 2016b), sensor data analysis (Mi-
nor, Doppa, and Cook 2015), and mixed-integer program-
ming solvers (He, Daum´e III, and Eisner 2014; Khalil et al.
2016). Our work explores this speedup learning problem for
query planning in graph databases, a novel application do-
main. We formalized and solved this problem in a learning to
plan framework. There is some work on applying learning in
the context of query optimization (Hasan and Gandon 2014;
Ganapathi et al. 2009; Gupta, Mehta, and Dayal 2008), but
we are not aware of any existing work that tightly integrates
learning and search for graph query processing.

Knoblock and Kambhampati has done seminal work on
applying automating planning techniques for information in-
tegration on the web (Knoblock and Kambhampati 2007).
Their work leverages the relationship between query plan-
ning in information integration and automated planning with
sensing (i.e., information gathering) actions. Our work is
different from theirs as we explore query planning in the
context of traditional (graph) databases and rely heavily on
advanced learning techniques.

Summary and Future Work
We developed a general learning to plan (L2P) framework
to improve the computational efﬁciency of a large-class of
query reasoners that follow the Threshold Algorithm frame-
work. We showed that our concrete instantiation L2P-STAR
can achieve signiﬁcant speedup over STAR with negligi-
ble loss in accuracy across multiple knowledge graphs. Fu-
ture work includes scaling up L2P framework to handle
large number of batch/streaming queries by exploring ac-
tive learning techniques; and deploying L2P instantiations
for real-world relational and graph databases.

Acknowledgements. Jana Doppa would like to thank Tom
Dietterich, Alan Fern, and Prasad Tadepalli for useful discussions.
This work was supported in part by a Google Faculty Research
award, and in part by the NSF grant #1543656.

References
[Arai et al. 2007] Arai, B.; Das, G.; Gunopulos, D.; and Koudas, N.

2007. Anytime measures for top-k algorithms. In VLDB.

[Chen and Guestrin 2016] Chen, T., and Guestrin, C. 2016. Xg-

boost: A scalable tree boosting system. In KDD.

[Chin Jr et al. 2014] Chin Jr, G.; Choudhury, S.; Feo, J.; and Holder,
L. 2014. Predicting and detecting emerging cyberattack patterns
using streamworks. In Proceedings of the 9th Annual Cyber and
Information Security Research Conference, 93–96.

[Das et al. 2015] Das, S.; Doppa, J. R.; Kim, D.; Pande, P. P.; and
Chakrabarty, K. 2015. Optimizing 3D NoC design for energy efﬁ-
ciency: A machine learning approach. In ICCAD, 705–712.

[Das et al. 2016a] Das, M.; Wu, Y.; Khot, T.; Kersting, K.; and
Natarajan, S. 2016a. Scaling lifted probabilistic inference and
learning via graph databases. In SDM.

[Das et al. 2016b] Das, S.; Doppa, J. R.; Pande, P. P.; and
Chakrabarty, K. 2016b. Design-space exploration and optimiza-
tion of an energy-efﬁcient and reliable 3D small-world network-
on-chip. TCAD.

[Ding et al. 2014] Ding, X.; Jia, J.; Li, J.; Liu, J.; and Jin, H. 2014.
Top-k similarity matching in large graphs with attributes. In DAS-
FAA.

[Doppa et al. 2014] Doppa, J. R.; Yu, J.; Ma, C.; Fern, A.; and Tade-
palli, P. 2014. HC-Search for Multi-Label Prediction: An Empirical
Study. In AAAI.

[Doppa, Fern, and Tadepalli 2014a] Doppa, J. R.; Fern, A.; and
Tadepalli, P. 2014a. HC-Search: A learning framework for search-
based structured prediction. JAIR 50:369–407.

[Doppa, Fern, and Tadepalli 2014b] Doppa, J. R.; Fern, A.; and
Tadepalli, P. 2014b. Structured prediction via output space search.
JMLR 15:1317–1350.

[Fagin, Lotem, and Naor 2003] Fagin, R.; Lotem, A.; and Naor, M.
2003. Optimal aggregation algorithms for middleware. Journal of
Computer and System Sciences 66(4):614–656.

[Fern, Yoon, and Givan 2006] Fern, A.; Yoon, S. W.; and Givan, R.
2006. Approximate policy iteration with a policy language bias:
Solving relational markov decision processes. JAIR 25:75–118.
[Fern 2010] Fern, A. 2010. Speedup learning. In Encyclopedia of

Machine Learning.

[Fern 2016] Fern, A. 2016. Personal Communication.
[Ganapathi et al. 2009] Ganapathi, A.; Kuno, H.; Dayal, U.;
Wiener, J. L.; Fox, A.; Jordan, M.; and Patterson, D. 2009. Pre-
dicting multiple metrics for queries: Better decisions enabled by
machine learning. In ICDE.

[Gupta, Mehta, and Dayal 2008] Gupta, C.; Mehta, A.; and Dayal,
U. 2008. Pqr: Predicting query execution times for autonomous
workload management. In ICAC.

[Hasan and Gandon 2014] Hasan, R., and Gandon, F. 2014. A ma-
chine learning approach to sparql query performance prediction. In
IEEE/WIC/ACM.

[He, Daum´e III, and Eisner 2012] He, H.; Daum´e III, H.; and Eis-

ner, J. 2012. Imitation learning by coaching. In NIPS.

[He, Daum´e III, and Eisner 2013] He, H.; Daum´e III, H.; and Eis-
ner, J. 2013. Dynamic feature selection for dependency parsing. In
EMNLP.

[He, Daum´e III, and Eisner 2014] He, H.; Daum´e III, H.; and Eis-
ner, J. 2014. Learning to search in branch and bound algorithms.
In NIPS.

[Jiang et al. 2012] Jiang, J.; Teichert, A. R.; Daum´e III, H.; and Eis-
ner, J. 2012. Learned prioritization for trading off accuracy and
speed. In NIPS.

[Khalil et al. 2016] Khalil, E. B.; Bodic, P. L.; Song, L.;
Nemhauser, G. L.; and Dilkina, B. N. 2016. Learning to branch in
mixed integer programming. In AAAI.

[Kim et al. 2015] Kim, J.; Shin, H.; Han, W.-S.; Hong, S.; and
Chaﬁ, H. 2015. Taming subgraph isomorphism for rdf query pro-
cessing. VLDB 1238–1249.

[Knoblock and Kambhampati 2007] Knoblock, C., and Kambham-
pati, S. 2007. Tutorial on information integration on the web. In
AAAI.

[Lam et al. 2015] Lam, M.; Doppa, J. R.; Todorovic, S.; and Diet-
terich, T. 2015. HC-Search for structured prediction in computer
vision. In CVPR.

[Leskovec and Faloutsos 2006] Leskovec, J., and Faloutsos, C.

2006. Sampling from large graphs. In SIGKDD.

[Leskovec, Kleinberg, and Faloutsos 2007] Leskovec,

J.; Klein-
berg, J.; and Faloutsos, C. 2007. Graph evolution: Densiﬁcation
and shrinking diameters. TKDD 2.

[Lu et al. 2013] Lu, J.; Lin, C.; Wang, W.; Li, C.; and Wang, H.
In

2013. String similarity measures and joins with synonyms.
SIGMOD.

[Ma et al. 2014] Ma, C.; Doppa, J. R.; Orr, J. W.; Mannem, P.; Fern,
X. Z.; Dietterich, T. G.; and Tadepalli, P. 2014. Prune-and-score:
Learning for greedy coreference resolution. In EMNLP.

[Martinez-Cantin 2014] Martinez-Cantin, R. 2014. Bayesopt: A
bayesian optimization library for nonlinear optimization, experi-
mental design and bandits. Journal of Machine Learning Research
15:3915–3919.

[Minor, Doppa, and Cook 2015] Minor, B.; Doppa, J. R.; and Cook,
D. J. 2015. Data-driven activity prediction: Algorithms, evaluation
methodology, and applications. In KDD.

[Morsey et al. 2011] Morsey, M.; Lehmann, J.; Auer, S.; and
Ngonga Ngomo, A.-C. 2011. DBpedia SPARQL Benchmark – Per-
formance Assessment with Real Queries on Real Data. In ISWC.
[Navarro 2001] Navarro, G. 2001. A guided tour to approximate

string matching. CSUR 33(1):31–88.

[Niu et al. 2011] Niu, F.; R´e, C.; Doan, A.; and Shavlik, J. W. 2011.
Tuffy: Scaling up statistical inference in markov logic networks
using an RDBMS. PVLDB 4(6):373–384.

[Pinto and Fern 2014] Pinto, J., and Fern, A. 2014. Learning partial

policies to speedup MDP tree search. In UAI.

[Prud’Hommeaux, Seaborne, and others 2008] Prud’Hommeaux,

E.; Seaborne, A.; et al. 2008. Sparql query language for rdf. W3C
recommendation 15.

[Ross, Gordon, and Bagnell 2011] Ross, S.; Gordon, G. J.; and
Bagnell, D. 2011. A reduction of imitation learning and structured
prediction to no-regret online learning. In AISTATS.

[Roy, Eliassi-Rad, and Papadimitriou 2015] Roy, S. B.; Eliassi-
Rad, T.; and Papadimitriou, S. 2015. Fast best-effort search on
graphs with multiple attributes. TKDE 27(3):755–768.

[Sarkhel et al. 2016] Sarkhel, S.; Venugopal, D.; Pham, T. A.;
Singla, P.; and Gogate, V. 2016. Scalable training of markov logic
networks using approximate counting. In AAAI.

[Shahriari et al. 2016] Shahriari, B.; Swersky, K.; Wang, Z.;
Adams, R. P.; and de Freitas, N. 2016. Taking the human out
of the loop: A review of bayesian optimization. Proceedings of the
IEEE 104(1):148–175.

[Weiss and Taskar 2013] Weiss, D. J., and Taskar, B. 2013. Learn-
In

ing adaptive value of information for structured prediction.
NIPS.

[Weiss, Sapp, and Taskar 2013] Weiss, D. J.; Sapp, B.; and Taskar,

B. 2013. Dynamic structured model selection. In ICCV.

[Xu, Fern, and Yoon 2009] Xu, Y.; Fern, A.; and Yoon, S. W. 2009.
Learning linear ranking functions for beam search with application
to planning. JMLR 10:1571–1610.

[Yang et al. 2016] Yang, S.; Han, F.; Wu, Y.; and Yan, X. 2016. Fast

top-k search in knowledge graphs. In ICDE.

[Zeng et al. 2012] Zeng, X.; Cheng, J.; Yu, J.; and Feng, S. 2012.

Top-k graph pattern matching: A twig query approach. WAIM.

[Zhang and Dietterich 1995] Zhang, W., and Dietterich, T. G. 1995.
A reinforcement learning approach to job-shop scheduling. In IJ-
CAI.

[Zhang, Kumar, and R´e 2016] Zhang, C.; Kumar, A.; and R´e, C.
2016. Materialization optimizations for feature selection work-
loads. ACM TODS 41(1):2.

[Zou et al. 2014] Zou, L.; Huang, R.; Wang, H.; Yu, J. X.; He, W.;
and Zhao, D. 2014. Natural language question answering over
RDF: a graph data driven approach. In SIGMOD.

