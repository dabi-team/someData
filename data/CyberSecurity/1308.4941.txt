4
1
0
2

n
u
J

9

]

R

I
.
s
c
[

3
v
1
4
9
4
.
8
0
3
1
:
v
i
X
r
a

Automatic Labeling for Entity Extraction in Cyber Security

Robert A. Bridges1, Corinne L. Jones2, Michael D. Iannacone3, Kelly M. Testa4, John R. Goodall5
1bridgesra@ornl.gov, 2corinne.jones180@gmail.com, 3iannaconemd@ornl.gov, 4testakm@ornl.gov,
5jgoodall@ornl.gov
Cyber & Information Security Research Group
Oak Ridge National Laboratory
Oak Ridge, TN 37830

ABSTRACT

unstructured text sources.

Timely analysis of
cyber-security information
necessitates automated information extraction from
unstructured text. While state-of-the-art extraction
methods produce extremely accurate results, they
require ample training data, which is generally
unavailable for specialized applications, such as
detecting security related entities; moreover, manual
annotation of corpora is very costly and often not
a viable solution.
In response, we develop a very
precise method to automatically label text from
several data sources by leveraging related, domain-
speciﬁc, structured data and provide public access
to a corpus annotated with cyber-security entities.
Next, we implement a Maximum Entropy Model
trained with the average perceptron on a portion
of our corpus (∼750,000 words) and achieve near
perfect precision, recall, and accuracy, with training
times under 17 seconds.

I

INTRODUCTION

Online security databases, such as the National Vul-
nerability Database (NVD), the Open Source Vulner-
ability Database (OSVBD), and Exploit DB are impor-
tant sources of security information, in large part be-
cause their well deﬁned structure facilitates quick ac-
quisition of information and allows integration with
various automated systems.1 On the other hand,
newly discovered information often appears ﬁrst in
unstructured text sources such as blogs, mailing lists,
and news sites. Hence, in many cases there is a
time delay, sometimes months, between public dis-
closure of information and appropriate classiﬁcation
into structured sources (as noted in [1]). Addition-
ally, many of the structured sources include a text
description that provides important details (e.g., Ex-
ploit DB). Timely use of this information, both by se-
curity tools and by the analysts themselves, neces-
sitates automated information extraction from these

1http://nvd.nist.gov/, http://www.osvdb.org/, http://

www.exploit-db.com/

For identifying more general entity types, many “oﬀ-
the-shelf” software packages give impressive results
using proven supervised methods trained on enor-
mous corpora of labeled text. Because the training
data is only annotated with names, geo-political enti-
ties, dates, etc., these general entity recognition tools
are inadequate when expected to extract the rela-
tively foreign entities that occur in domain-speciﬁc
documents, simply because they are not trained to
handle such jargon. Exempliﬁed by our need for en-
tity extraction in the cyber-security domain, there are
many domain-speciﬁc applications for which entity
extraction will be very beneﬁcial. As evidenced by
the near perfect results of sequential labeling tech-
niques, for example [2], the machine learning is thor-
oughly developed. Rather, what is lacking is la-
beled training data tailored to domain speciﬁc needs.
Moreover, manual annotation of a suﬃciently large
amount of text is generally too costly to be a viable
solution.

This paper describes an automated process for cre-
ating an annotated corpus from text associated with
structured data that can produce large quantities of
labeled text relatively quickly (compared to man-
ual annotation) by writing a script which labels text
with related structured sources. More speciﬁcally,
the wealth of structured data available in the cyber-
security domain is leveraged to automatically label
associated text descriptions and made publicly avail-
able online.2 While labeling these descriptions may
be useful in itself, the intended purpose of this cor-
pus is to serve as training data for a supervised learn-
ing algorithm that accurately labels other text docu-
ments in this domain, such as blogs, news articles,
and tweets.

Next, we use a portion of the data to train a history-
based Maximum Entropy Model with the averaged
perceptron and greedy decoding, and exhibit preci-
sion, recall, and accuracy that are consistently above

2https://github.com/stucco/auto-labeled-corpus

Page 1 of 11
c(cid:13)ASE 2012

 
 
 
 
 
 
97%; moreover, the algorithm runs extremely eﬃ-
ciently, training on over 750, 000 labeled words in un-
der 17 seconds. In Section VI, we compare our work
to a previous similar attempt ( [3] ) at supervised
entity extraction within the cyber-security domain,
which produced scores under 80% when trained on
a hand-labeled corpus. While this is not a direct com-
parison, the increase in performance is evidently in
part due to the vast increase in training data as facil-
itated by our automated labeling process.

II BACKGROUND

1 ENTITY EXTRACTION IN CYBER-SECURITY

OVERVIEW

Our overall goal of automatically labeling cyber se-
curity entities is similar to a few previous eﬀorts.
In order to instantiate a security ontology, More et
al. [4] attempt to annotate concepts in the Common
Vulnerability Enumeration (CVE)3 descriptions and
blogs with OpenCalais, an “out-of-the-box” entity
extractor [5]. Mulwad et al. [6] expand this idea by
ﬁrst crawling the web and training a decision clas-
siﬁer to identify security relevant text. Then us-
ing OpenCalais along with the Wikipedia taxonomy,
they identify and classify vulnerability entities.

While the two sources above rely on standard en-
tity recognition software, such tools are not trained
to identify domain speciﬁc concepts, and they un-
surprisingly give poor results when applied to more
technical documents (as shown in Figure 2). This
is due to the general nature of their training cor-
pus; for example, the Stanford Named Entity Recog-
nizer4 is trained on the CoNLL, MUC-6, MUC-7 and
ACE named entity corpora, consisting of news doc-
uments annotated mainly with the names of people,
places, and organizations [7, 8]. Similar ﬁndings are
noted in Joshi et al.’s recent work [3], where Open-
Calais, The Stanford Named Entity Recognizer, and
the NERD framework [9] were all generally unable
to identify cyber security domain entities. Because
these tools do not use any domain speciﬁc training
data, domain entities are either unrecognized or are
labeled with descriptions that are too general to be
of use (e.g., “Industry Term”). The Joshi et al. paper
later supplies the Stanford Named Entity Recognizer
framework with domain speciﬁc, hand labeled train-
ing data, and it is then able to produce better results
for most of their domain speciﬁc entity types.

3http://cve.mitre.org/
4http://nlp.stanford.edu/software/CRF-NER.shtml

More speciﬁcally, the Joshi et al. [3] work also ad-
dresses the problem of entity extraction for cyber-
security with a similar solution, namely, by training a
supervised learning algorithm to identify desired en-
tities. Unlike our approach, which introduces an au-
tomated way to generate an arbitrarily large training
corpus, their approach, involves painstakingly hand-
annotating a small corpus that is then fed into the
Stanford Named Entity Recognizer’s “oﬀ-the-shelf”
template for training a conditional random ﬁeld en-
tity extractor [7]. In all, they label a training corpus of
350 short text descriptions, mostly from CVE records,
with categories surprisingly similarly to ours. While
their work has identiﬁed the same cyber-security
problem, they do not furnish a data set labeled for
this domain, nor do they address the more general
problem of how to automate the labeling process
when no training data exists. See Section VI for de-
tailed comparisons of the results, and [10] for more
speciﬁcs on the entity extraction implementation as
used in the Joshi paper.

Given this general lack of domain speciﬁc training
data, there has been some work considering semi-
supervised methods instead of supervised methods
because they are designed to do the best possible
with very little training data. Although a thorough
discussion of semi-supervised methods for entity ex-
traction is outside the scope of the current paper,
such techniques have yielded worthwhile results; for
example see [11–15], and [16]. To our knowledge
only one such eﬀort focuses on cyber-security; recent
work by McNeil et al. [1] develops a novel bootstrap-
ping algorithm and describes a prototypical imple-
mentation to extract information about exploits and
vulnerabilities. Using known databases for seeding
bootstrapping methods is also not uncommon; for
example, see [17].

2 AUTOMATIC LABELING OVERVIEW

Previous work has incorporated variations of auto-
labeling in several diﬀerent contexts where NLP is
needed and no training data exists. “Distant label-
ing” generally refers to the process of producing a
gazetteer (comprehensive list of instances) for each
database ﬁeld and performing a dictionary look-up
to label text that is not directly associated with a
given database record. While gazetteers give poor
results in an unconstrained setting [18], accurate re-
sults can be achieved when the text has little varia-
tion. An example is Seymore et al. [19] who use a
database of BIBTEX entries and some regular expres-

Page 2 of 11
c(cid:13)ASE 2012

sions to produce training data for a Hidden Markov
Model (HMM) by labeling headers of academic pa-
pers.

In general, more accurate labels are possible if there
is a direct relationship between a given database
record and the text entry to be labeled, such as if a
text description occurs as a ﬁeld of a database, or a
separate text document is referenced for each record,
as is the case for our setting. Here we describe known
instances of using an automated process for creating
labeled training data. Craven and Kumlien [20] train
a naive Bayes classiﬁer to identify sentences contain-
ing a desired pair of entities via “weak labeling”.
Speciﬁcally, given a database record that includes a
pair of entity names along with a reference to an aca-
demic publication, sentences occurring in the arti-
cle’s abstract are automatically labeled positively if
that entity pair occurs in them. This is shown to yield
better precision and recall scores than using a smaller
hand-annotated training corpus and obviates the te-
dious manual labor.

More recently, Bellare and McCallum [21] also use a
BIBTEX database to label corresponding citations and
then train a classiﬁer to segment a given citation into
authors, title, date, etc. Because their goal is to create
a text segmentation tool, they rely on the implicit as-
sumption that every token will receive a label from
the given database ﬁeld names. As our goal is to
identify and classify speciﬁc entities in text, no such
assumption can be leveraged.

While a few instances of automated labeling have oc-
curred in the literature, to our knowledge no previ-
ous work has addressed the accuracy of the auto-
matically prescribed labels. Rather, an increase in
accuracy of the supervised algorithm is usually at-
tributed to the increase in training data, which is fa-
cilitated by the automated process. We note that the
precision and recall of an algorithm’s output is de-
termined by comparison against the training data,
which may or may not have correct labels. In order
to address the quality of our auto-labeling, we have
randomly sampled sentences for manual inspection
(see Auto-Labeling Results Subsection III.3).

III AUTOMATIC LABELING

1 DATA SOURCES

To build a corpus with security-relevant labels, we
seek text that has a close tie to a database record
and use its ﬁeld names to label matching entries in

Figure 1: NVD text description of CVE-2012-0678
with automatically generated labels.

Figure 2: NVD text description of CVE-2012-0678
with labels from OpenCalais.

the text. When a vulnerability is initially discov-
ered, the Common Vulnerability Enumeration (CVE)
is usually the ﬁrst structured source to ingest the
new information and it provides, most importantly,
a unique identiﬁcation number (CVE-ID), as well
as a few sentence overview. Shortly afterward, the
National Vulnerability Database (NVD) incorporates
the CVE record and adds additional information
such a classiﬁcation of the vulnerability using a sub-
set of the Common Weakness Enumeration (CWE)
taxonomy5, a collection of links to external refer-
ences, and other ﬁelds. Hence, the NVD provides
both informative database records and many struc-
tured ﬁelds to facilitate auto-labeling. All NVD de-
scriptions from January 2010 through March 2013
have been auto-labeled and comprise the lion’s share
of our corpus.

While our main source for creating an auto-labeled
corpus is the NVD text description ﬁelds, the univer-
sal acceptance of the CVE-ID allows text from other
sources to be unambiguously linked to a speciﬁc vul-
nerability record in the database. The Microsoft Se-
curity Bulletin provides patch and mitigation infor-
mation and gives a wealth of pertinent text related

5http://cwe.mitre.org/

Page 3 of 11
c(cid:13)ASE 2012

to a speciﬁc vulnerability identiﬁed by the CVE-ID.6
Speciﬁc text ﬁelds include an “executive summary”
as well as “revision”, “general”, “impact”, “target
set”, “mitigation”, “work around”, and “vendor ﬁx”
descriptions; moreover, while not all text ﬁelds are
populated for a given record, many times a single
text ﬁeld will have multiple descriptions. Every de-
scription for the previous year’s MS-Bulletin entries
was added to our corpus.

Lastly,
the Metasploit Framework contains a
database of available exploits that includes a text
description, several categorizations and properties,
and a reference to the associated vulnerability,
usually the CVE-ID.7 By linking these text sources
to the NVD via CVE-IDs we are able to leverage
the structured data for very precise labeling of the
unstructured data.

Overall, a corpus of over 850,000 tokens with au-
tomatic annotations are available online at https:
//github.com/stucco/auto-labeled-corpus.

2 AUTO-LABELING DETAILS

Given a database record and a block of associated
text, our algorithm assigns labels to the entities in the
text as follows:

• Database Matching. Any string in the text that
exactly matches an entry of the database record
is labeled with a generalization of the name
of the database ﬁeld. For example, the label
“software product” is assigned to a string in
the text description if it also occurs in the re-
lated database record ﬁeld “os” or “applica-
tion”. Similarly, instances of “version”, ”up-
date”, and “edition” occurring in the associ-
ated text are labeled ”software version”.

• Heuristic Rules. A variety of heuristic rules
are used for identifying entities in text that are
not direct matches of database ﬁelds. For ex-
ample, the database lists every version number
aﬀected by a vulnerability, but such a list is al-
most never written in text; rather, short phrases
such as “before 2.5”, “1.1.4 through 2.3.0”, and
“2.2.x” usually appear after a software applica-
tion name; consequently, a few regular expres-
sions combined with rules identifying both la-
bels and features of previous words give pre-

6http://technet.microsoft.com/en-us/security/

bulletin

7http://www.metasploit.com/

cise identiﬁcation of version entities. Similarly,
source code ﬁle names, functions, parameters,
and methods, although not in the database, are
often referenced in text. As ﬁle names end in a
ﬁle extension (e.g., “.dll”) and the standards of
camel- and snake-case (e.g., camelCaseExam-
ple, snake_case_example) are universal, such
entities are easily distinguishable by their fea-
tures.

• Relevant Terms Gazetteer.

In order to ex-
tract short phrases that give pertinent informa-
tion about a vulnerability, a gazetteer of rele-
vant terms is created, and phrases in the text
matching the gazetteer are labeled “relevant
term”. As mentioned above, each record in
the NVD includes one (of twenty) CWE clas-
siﬁcations, which gives the vulnerability type
(e.g., SQL injection, cross-site scripting, buﬀer
errors). As the goal of CWE is to provide a
common language for discussing vulnerabili-
ties, many phrases indicative of the vulnera-
bility’s characteristics occur regularly. To con-
struct the gazetteer of relevant terms, the NVD
is sorted by CWE type, and statistical analy-
sis of the text descriptions for a given CWE
classiﬁcation is used to ﬁnd the most prevalent
unigrams, bigrams, and trigrams. Commonly
occurring but uninformative phrases (e.g., “in
the”, “is of the”) are discarded manually. We
note that Python’s Natural Language Toolkit
(NLTK) facilitated tokenization and computa-
tion of frequency distributions of n-grams [18].
Examples of relevant terms include “remote at-
tackers”, “buﬀer overﬂow”, “execute arbitrary
code”, “XSS”, and “authentication issues”.

All together, the following is the comprehensive
list of labels used: “software vendor”, “software
product”, “software version”, “software language”,
“vulnerability name” (these are CVE-IDs), “software
symbol” (these are ﬁles, functions or methods, or pa-
rameters), and “vulnerability relevant term”.

Because many multi-word names are commonplace,
standard IOB-tagging is used; speciﬁcally, the ﬁrst
word of an identiﬁed entity name is labeled with a
“B” (for “beginning”) followed by the entity type,
and any word in an entity name besides the ﬁrst is
tagged with an “I” (for “inside”) followed by the en-
tity name. Unidentiﬁed words are labeled as “O”. An
example of an automatically labeled NVD descrip-
tion is given in Figure 1.

Page 4 of 11
c(cid:13)ASE 2012

3 AUTO-LABELING RESULTS

IV ENTITY EXTRACTION VIA SEQUENTIAL

As the overall goal is to produce a machine learn-
ing algorithm that will identify entities in a much
broader class of documents, thereby aiding security
analysts, the accuracy of the algorithm, and there-
fore the training data, is very important. While both
high precision and recall are ideal, precision is more
important for our purposes as reliable information is
mandatory. More speciﬁcally, in a high recall but low
precision setting, nearly all desired entities would be
returned along with many incorrectly labeled ones;
hence, the quality of the data returned to the user
would suﬀer. On the other hand, if all information
extracted from text sources is correct, anything re-
turned is an immediate value-add. In general, this
is guaranteed by high precision in the auto-labeling
process, which we ensure by constructing precise
heuristics and using a speciﬁc database record to la-
bel closely related text.

NVD
MS-Bulletin
Metasploit

F1

Precision Recall
99%
99.4%
95.3%

77.8% .875
75.3% .778
54.3% .691

Table 1: Precision, Recall, and F1 Scores for the au-
tomatically labeled corpus are calculated by hand la-
beling a random sample.

In order to test the accuracy of the auto-labeling,
about 30 randomly sampled text descriptions from
each source were manually labeled. Because the la-
bel “relevant term” is applied by a direct dictionary
look up against a list of terms we created, we know
each and every exact match in the text is labeled;
hence, they are not included in the accuracy scores to
prevent artiﬁcial score inﬂation. In other words, the
Precision, Recall, and F1 Score results of Table 1 are
with respect to only those labels matching an entry
of a database ﬁeld or from a hand-crafted heuristic.

To our knowledge, similar work has assumed correct
automatically generated labels and ignored investi-
gating the accuracy of the labels. In total over 850,000
tokens have been labeled relatively quickly (with re-
spect to manual annotation) and with high accuracy,
and increasing the corpus size as necessary is both
expedient and easy. We hope the proposed method
can facilitate labeling data in many other domains.

LABELING

As is common in the literature, our approach to su-
pervised entity extraction is treating the task as a se-
quential labeling problem, similar to previous work
on part-of-speech tagging, noun phrase chunking,
and parsing. This section gives an overview of ma-
chine learning techniques for such a task and reviews
the mathematical foundation for Maximum Entropy
(or Log-Linear) Models in preparation for our imple-
mentation, described in the Section V.

1 SEQUENTIAL TAGGING MODELS

Used widely in sequential tagging problems, Hid-
den Markov Models (HMMs) are generative models
that estimate the joint probability of a given sentence
and corresponding tag sequence by ﬁrst estimating
an emission parameter, that is, the probability of a
word given its label, and secondly, by estimating a
prior distribution on the set of labels using a Markov
Process [22].

While HMMs are computationally eﬃcient, the sub-
class of discriminative models known as Maximum
Entropy Models (MEMs) are perhaps a more popular
choice for sequential tagging problems as they gener-
ally outperform Hidden Markov Models by virtue of
their accommodation of a much larger set of features;
for example, see [23, 24]. Two varieties of MEMs
are common in the literature, namely, those using
“history-based” features (whose features depend on
the current word as well as previous word(s) and la-
bel(s)) and those using “global features” (whose fea-
tures depend on both the words and labels before
and after a given word). More commonly referred to
as Conditional Random Fields (CRFs), global models
treat each sentence as an object to be labeled with a
corresponding set of word tags (rather than labeling
individual words sequentially) and have achieved
better performance than history-based MEMs, but
at the price of greater computational expense [25].
More speciﬁcally, with k possible word labels and a
sentence of length n, the search space for sentence
tags is of order nk. Because of the dependence only
in the reverse direction, history-based MEMs admit
use of the Viterbi algorithm for ﬁnding the most
probable tag sequence eﬃciently (with order nkm for
features depending on the previous m labels); fur-
thermore, one has the option of a greedy algorithm,
which inductively chooses the highest probability
tag for each word and ignores the overall probabil-

Page 5 of 11
c(cid:13)ASE 2012

ity of the sequence. As no such options exist for de-
coding with CRFs, eﬀorts include incorporating an
algorithm for narrowing the search space or using
probabilistic means for ﬁnding the best tag sequence
[7, 22]. Because of the observed performance of the
history-based MEM with a greedy tagging algorithm
in our setting (see Subsection VI), use of more com-
putationally expensive algorithms, such as CRFs or
even Viterbi decoding, was unwarranted.

2 MATHEMATICAL OVERVIEW

A brief mathematical overview of a history-based
MEM is followed by the implementation details used
in our experiment.

Derived by maximizing Shannon’s Entropy in the
presence of constraint equations, MEMs provide
a principled mathematical foundation that ensures
only the observed features design the probability
model. For a given sentence w = (w1, . . . , wn) and
corresponding tag sequence t = (t1, . . . , tn), the con-
ditional probability of t given w is estimated as

p(t|w) ≡

n
(cid:89)

i=1

p(ti|ti−2, ti−1, wi−2, wi−1, wi)

(1)

with t0, t−1, w0, w−1 deﬁned to be distinguished start
symbols. Hence the probability of tag tj being as-
signed to word wj is conditioned on the previous two
tags (in our implementation), as well as the current
word and previous two words. For notational ease
we let ¯ti = (ti−2, ti−i, ti), and similarly for ¯w. As pre-
scribed by the MEM,

p(ti|ti−2, ti−1, wi−2, wi−1, wi) ≡

ef (¯ti, ¯wi)·v
z(¯ti, ¯wi)

(2)

where f = (f1, . . . , fm) denotes a feature vector,
v = (v1, . . . , vm) the parameter vector (or feature
weights) to be learned from the training data, and

z(¯ti, ¯wi) ≡

(cid:88)

ˆt

exp[f (ti−2, ti−1, ˆt, ¯wi) · v],

i.e., z is the appropriate constant to ensure the sum of
Equation 2 over the sample space is one. An example
of a feature (i.e., a component of the feature vector)
is

f1(¯ti, ¯wi) =






1

0

if

ti = B: SW Vendor,
wi−1 = “the”
else.

(3)

After ﬁxing a set of features, one must decide on
the “best” parameter vector v to use and many tech-
niques for ﬁtting the model to the training data (i.e.,

learning v) exist [26]. Perhaps the most principled
approach for ﬁtting the model is maximum likeli-
hood estimation (MLE), which assumes each (sen-
tence, tag sequence)-pair is independent and uses a
prior on v (or regularization parameter) to prevent
over-ﬁtting. Speciﬁcally, the argument maximum of

v (cid:55)→ p(v|{(w, t)}) ∝

(cid:89)

(w,t)

p((w, t)|v)p(v)

is generally found by maximizing the log-likelihood,
usually by a numerical algorithm such as L-BFGS, or
OWL-QN [27]. We note that the function in question
is concave, and has a unique maximum.

Initially introduced in [28], the perceptron algorithm
and its modern variants are a class of online methods
for ﬁtting parameters that have produced competi-
tive results in accuracy and are often more eﬃcient
than MLE techniques [22, 29]. After initializing the
parameter vector v (usually setting v = 0), percep-
tron algorithms cycle through the training set a ﬁxed
number of times. At each training example the algo-
rithm predicts the “best” label with the current pa-
rameter v and compares it to the ground-truth value.
In the case of a mis-assigned label, the parameter v
is updated so that the probability of the correct label
increases. As perceptron algorithms depend on de-
coding at each step, their computational expense can
vary, but in the case of greedy or Viterbi decoding,
they are relatively fast.

V ENTITY EXTRACTION IMPLEMENTATION

While the auto-labeled corpus may be useful in its
own right, the overall goal is to train a classiﬁer
that can apply domain-appropriate labels to a wider
class of documents including news articles, security
blogs, and tweets. Our choices for such implemen-
tation follow Mathew Honnibal’s persuasive results
and documentation of greedy tagging using the aver-
aged perceptron for part-of-speech tagging,8 where
he shows impressive results with respect to a balance
of accuracy, speed, and simplicity. To our knowl-
edge no publication of the results exists. Here we
give a brief synopsis of possible tagging algorithms,
and describe our implementation of a history-based
Maximum Entropy Model trained with the averaged
perceptron. Finally, we present performance results
from a simple greedy model for tagging.

8http://honnibal.wordpress.com/2013/09/11/a-good-part-

of -speechpos-tagger-in-about-200-lines-of-python/

Page 6 of 11
c(cid:13)ASE 2012

1 AVERAGED PERCEPTRON

We chose to use a modern perceptrion variant,
namely, the averaged perceptron, which has exhib-
ited exceptional results in many natural language
processing tasks [22, 29–31]. The averaged percep-
tron algorithm is presented in detail in Algorithm 1,
and explained below.

Algorithm 1: Averaged Perceptron
Input: {(w, t)} = training set
Niter = number of iterations
Output: vave = trained parameter vector
Initialize iter = 1
Initialize i = 0
Initialize v = (0, . . . , 0)
Initialize vt−stamp = (0, . . . , 0)
Initialize vtot = (0, . . . , 0)
while iter ≤ Niter do

for (w, t) in training set do

Set y = argmaxˆt p(ˆt|w, v)
if y! = t then

vtot+ = [(i, . . . , i) − vt−stamp] ∗ v
v+ = f (w, t) − f (w, y)
for j = 1 . . . length(v) such that
f (w, y)[j]! = 0 do

Set vt−stamp[j] = i

i+ = 1

else

i+ = 1

iter+ = 1

vtot+ = [(i, . . . , i) − vt−stamp] ∗ v
Set vave = vtot/i
return vave

The averaged perceptron uses the same online al-
gorithm to tweak the parameter vector as it iterates
through the training set, although this updated vec-
tor from the “vanilla” perceptron training is not re-
Instead, we now keep track of how many
turned.
successful labels are predicted by each intermedi-
ate parameter vector and return the weighted aver-
age of the vectors observed in training. Rather than
storing every intermediate vector along with a tally
of each vector’s success, the implementation below
keeps two auxiliary vectors, a time-stamp (vt−stamp),
which records when it was last changed, along with
a running weighted sum (vtot). Upon encounter-
ing a mislabeled instance, v is updated (as required
by the “vanilla” perceptron), vtot updates to include
the weighted sums before the components of v are

changed, and the time-stamp vector is set to the cur-
rent counter for all vector components that ﬁred. Fi-
nally, to obtain the averaged vector, vtot is divided
by the number of examples encountered and is re-
turned. Hence, the algorithm requires minimal stor-
age, and runs eﬃciently provided the decoding, that
is, the labeling algorithm, is quick. In our case, we
employed a simple greedy model, which labels each
word inductively.

As an intuitive but informal justiﬁcation for the aver-
aged perceptron, consider a scenario where the per-
ceptron vector is initialized and succeeds on labeling
the ﬁrst 9,999 of 10,000 training examples correctly,
but then mis-labels the last example and therefore
changes the weight vector. Unfortunately, a vector
that has achieved at least 99.99% accuracy has been
deselected! The averaged perceptron is designed to
prevent overﬁtting and in particular to counter-act
the perceptron’s seeming over-weighting of the ﬁnal
training examples9. While formal justiﬁcation, such
as convergence theorems, and theorems bounding
the expectation of success on test data exist for the
“vanilla” perceptron and voted perceptron [22,29], to
our knowledge, and as noted here [32], no formal re-
sults have been proven for the averaged perceptron.

2 FEATURE SELECTION

Recall that our goal is to use ‘IOB’-tagging to collec-
tively identify multi-word phrases, in addition to ap-
plying the appropriate domain labels; for example, a
correct labeling of an instance of “Internet Explorer”
is “B: Software Product” for “Internet” and “I: Soft-
ware Product” for “Explorer”. Hence we view this as
an iterative labeling process, ﬁrst applying ‘IOB’ la-
bels, and secondly applying the domain labels; con-
sequently, we train two averaged perceptron classi-
ﬁers.

To develop robust features, regular expressions are
used to identify words that begin with a digit, con-
tain an interior digit, begin with a capital letter, are
camel-case, are snake-case, or contain punctuation,
and part-of-speech tags are applied to each word us-
ing NLTK and used as features for tagging. Simi-
larly, once the ‘IOB’-labels have been applied, they
are used as features for the domain speciﬁc labeling.
We then generate binary features as follows:

9This intuitive explanation is attributed to Hal Daumé III,

http://ciml.info/dl/v0_8/ciml-v0_8-ch03.pdf.

Page 7 of 11
c(cid:13)ASE 2012

Features for ‘IOB’-tagging

• Gazetteer features for

• Unigram features for

– previous two, current, and following two

words

– previous two, current and following one

part of speech tags
– previous two ‘IOB’-tags

• Bigram features for

– previous two ’IOB’-tags
– previous ’IOB’-tag & current word
– previous part of speech tag & current

word

• Regular expressions as listed above for

– previous two, current, and following two

words

Following observations made in [10], we include
gazetteer features for the labels “Software Vendor”
and “Software Product”; that is, sets of Software
Vendor and Software Products are collected during
training. Upon an occurrence of such a word, the ap-
propriate gazetteer feature ﬁres.

Features for domain-tagging

• Unigram features for

– previous two, current, and following two

words

– previous two, current and following one

part of speech tags

– previous two, current, and following

‘IOB’-tags

– previous two domain labels

• Bigram features for

– previous two domain tags
– previous domain tag & current word
– previous ’IOB’-tag & current word
– previous part of speech tag & current

word

• Regular expressions as listed above for

– Software Product
– Software Vendor

VI RESULTS

In order to examine the performance of the tag-
gers, ﬁve-fold random sub-sampling validation is
performed on the automatically labeled corpus of
NVD text, which is comprised of 15,192 text descrip-
tions averaging about 50 words each. For various
sizes of data samples (n), ﬁve random samples of
n text descriptions are split 80/20 % into training
and testing sets. For experimentation with both fea-
ture and model selection, a prototype was coded in
Python, and subsequently, a faster implementation,
which relied on the Apache OpenNLP library10, was
developed. We provide both the Python code and
the OpenNLP conﬁguration details online for those
interested,11 and report the performance results of
the OpenNLP runs in Tables 2 ,3. In particular, pre-
cision, recall, accuracy, F1-score, and training time,
that is, actual clock time in seconds as observed on
a Macbook Pro with 2.3Ghz Intel quad-core i7, 8GB
memory, 256GB ﬂash storage. We note that treating
the ‘IOB’-tagging and the domain labeling separately
allowed unambiguous analysis of the performance;
that is, trying to judge accuracy of both labels at once
results in cases where, for example, the ‘IOB’-tag is
correct but the domain speciﬁc labels are incorrect,
and no principled treatment exists.

Both the Python and OpenNLP implementations
performed with almost perfect accuracy, with
slightly better performance by the OpenNLP im-
plementation on the domain-speciﬁc
labeling,
although, as expected, the OpenNLP implemen-
tation is much faster. Perhaps the most satisfying
observation in light of the result is that as the data
size increases, training time seems to be growing
only linearly, and as expected, precision, recall, and
accuracy are monotone increasing. Hence, in the
abundance of training data, as furnished by our
auto-labeling technique, state-of-the-art entity ex-
tractors can perform exceptionally in both accuracy
and speed.

The Joshi et. al. work, [3], which trained the Stan-
ford NER (using a CRF, a global model) for extract-
ing very similar entities reported much more modest

– previous two, current, and following two

words

10https://opennlp.apache.org
11https://github.com/stucco/auto-labeled-corpus

Page 8 of 11
c(cid:13)ASE 2012

Table 2: OpenNLP ‘IOB’-Labels

P

R

F1

A

T (sec)

0.906
0.921
0.926
0.947
0.963

0.929
0.935
0.966
0.950
0.968

0.917
0.928
0.944
0.948
0.965

0.944
0.950
0.962
0.965
0.976

1.192
1.396
3.023
5.468
15.265

n
500
1000
2500
5000
15192

Note: In both Tables 2 and 3, n refers to the number of
NVD descriptions, which contain about 50 words on aver-
age. For each n, ﬁve random samples are divided 80/20%
into training and test sets. Precision, recall, F1-score, ac-
curacy, and training time are reported.

Table 3: OpenNLP Domain Labels

n
100
500
1000
2500
5000
15192

P

R

F1

A

T (sec)

0.938
0.965
0.972
0.980
0.981
0.989

0.918
0.965
0.979
0.986
0.988
0.993

0.928
0.965
0.975
0.983
0.984
0.991

0.952
0.976
0.983
0.989
0.989
0.994

0.361
0.890
1.996
4.792
9.530
28.527

results, namely, precision = .837, recall = .764, for an
F1 score = .799. (Accuracy and training time were not
recorded.) Moreover, we recall that Joshi et. al. used
a hand-labeled training corpus of 240 CVE descrip-
tions, 80 Microsoft or Adobe security bulletins, and
30 security blogs, a corpus of approximately one thir-
tieth of our full NVD data set. As CRFs have also es-
tablished themselves in the literature as state-of-the-
art entity extractors, we conjecture that there are two
reasons for the relatively lower performance in the
Joshi paper, namely that their training set is substan-
tially smaller than ours, and also more varied in the
types of text it included.

VII CONCLUSION

Our auto-labeling technique gives an expedient way
to annotate unstructured text using associated struc-
tured database ﬁelds as a step towards deploying the
machine learning capabilities for entity extraction to
diverse and tailored applications. With respect to
automating extraction of security speciﬁc concepts,
we provide a publicly available corpus labeled with
security entities and a trained MEM for identiﬁca-
tion and classiﬁcation of appropriate entites, which

exhibited extremely accurate results. Additionally,
since many sources for our auto-labeling (NVD,
CVE, ...) provide RSS feeds, we seek to automate the
process of acquiring and auto-labeling the new data
to provide an ever growing corpus, which hopefully
will help extraction methods adapt to changing lan-
guage trends. As the overall telos of this work is to
accurately label “real world” documents containing
timely security information, future work will include
making the technique operationally eﬀective by test-
ing and tweaking the method on desired input texts.
Lastly, upon suﬃcient progress towards an entity ex-
traction system, we plan to incorporate the extrac-
tion technique into a larger architecture for acquiring
documents from the web and populating a database
with the domain speciﬁc concepts as an aid to secu-
rity analysts.

VIII ACKNOWLEDGMENTS

This material is based on research sponsored by the
following:
the Department of Homeland Security
(DHS) Science and Technology Directorate, Cyber
Security Division (DHS S&T/CSD) via BAA 11-02;
the Department of National Defence of Canada, De-
fence Research and Development Canada (DRDC);
the Kingdom of the Netherlands; and the Depart-
ment of Energy (DOE). The views and conclu-
sions contained herein are those of the authors and
should not be interpreted as necessarily represent-
ing the oﬃcial policies or endorsements, either ex-
pressed or implied, of the following:
the Depart-
ment of Homeland Security; the Department of En-
ergy; the U.S. Government; the Department of Na-
tional Defence of Canada, Defence Research and De-
velopment Canada (DRDC); or the Kingdom of the
Netherlands.

References

[1] N. McNeil, R. A. Bridges, M. D. Iannacone,
B. Czejdo, N. Perez, and J. R. Goodall, “PACE:
Pattern accurate computationally eﬃcient boot-
strapping for timely discovery of cyber-security
concepts,” in Machine Learning and Applications
(ICMLA), 2013 11th International Conference on.
IEEE, 2013.

[2] C. D. Manning, “Part-of-speech tagging from
97% to 100%: is it time for some linguistics?” in
Computational Linguistics and Intelligent Text Pro-
cessing. Springer, 2011, pp. 171–189.

Page 9 of 11
c(cid:13)ASE 2012

[3] A. Joshi, R. Lal, T. Finin, and A. Joshi, “Extract-
ing cybersecurity related linked data from text,”
in Proceedings of the 7th IEEE International Con-
ference on Semantic Computing.
IEEE Computer
Society Press, 2013.

[4] S. More, M. Matthews, A. Joshi, and T. Finin,
“A knowledge-based approach to intrusion de-
tection modeling,” in Security and Privacy Work-
shops (SPW), 2012 IEEE Symposium on Semantic
IEEE, 2012, pp. 75–81.
Computing and Security.

[5] T. Reuters, “OpenCalais,” 2009.

[6] V. Mulwad, W. Li, A. Joshi, T. Finin, and
K. Viswanathan, “Extracting information about
security vulnerabilities from web text,” in Pro-
ceedings of the 2011 IEEE/WIC/ACM International
Conferences on Web Intelligence and Intelligent
Agent Technology - Volume 03, ser. WI-IAT ’11.
Washington, DC, USA: IEEE Computer So-
ciety, 2011, pp. 257–260. [Online]. Available:
http://dx.doi.org/10.1109/WI-IAT.2011.26

[7] J. R. Finkel, T. Grenager, and C. Manning,
“Incorporating non-local information into in-
formation extraction systems by Gibbs sam-
pling,” in Proceedings of
the 43rd Annual
Meeting on Association for Computational Lin-
guistics,
Stroudsburg, PA,
USA: Association for Computational Linguis-
tics, 2005, pp. 363–370. [Online]. Available:
http://dx.doi.org/10.3115/1219840.1219885

ser. ACL ’05.

[8] E. F. Tjong Kim Sang and F. De Meulder,
“Introduction to the CoNLL-2003 shared task:
Language-independent named entity recogni-
tion,” in Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003-
Volume 4. Association for Computational Lin-
guistics, 2003, pp. 142–147.

[9] G. Rizzo and R. Troncy, “NERD: A framework
for unifying named entity recognition and dis-
ambiguation extraction tools,” in Proceedings of
the Demonstrations at the 13th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics. Association for Computational Lin-
guistics, 2012, pp. 73–76.

[10] R. Lal, “Information extraction of security re-
lated entities and concepts from unstructured
text.” Master’s thesis, May 2013.

[11] S. Brin, “Extracting patterns and relations from
the world wide web,” in The World Wide Web and
Databases. Springer, 1999, pp. 172–183.

[12] R. Jones, “Learning to extract entities from la-
beled and unlabeled text,” Ph.D. dissertation,
University of Utah, 2005.

[13] J. Betteridge, A. Carlson, S. A. Hong, E. R. Hr-
uschka Jr, E. L. Law, T. M. Mitchell, and S. H.
Wang, “Toward never ending language learn-
ing.” in AAAI Spring Symposium: Learning by
Reading and Learning to Read, 2009, pp. 1–2.

[14] A. Carlson, J. Betteridge, R. C. Wang, E. R.
Hruschka Jr, and T. M. Mitchell, “Coupled
semi-supervised learning for information ex-
traction,” in Proceedings of the Third ACM Interna-
tional Conference on Web Search and Data Mining.
ACM, 2010, pp. 101–110.

[15] A. Carlson, S. A. Hong, K. Killourhy, and
S. Wang, “Active learning for information ex-
traction via bootstrapping,” 2010.

[16] R. Huang and E. Riloﬀ, “Bootstrapped training
of event extraction classiﬁers,” in Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics. Asso-
ciation for Computational Linguistics, 2012, pp.
286–295.

[17] J. Geng and J. Yang, “Autobib: Automatic
extraction of bibliographic information on the
web,” in Proceedings of the International Database
Engineering and Applications Symposium, ser.
IEEE
IDEAS ’04. Washington, DC, USA:
Computer Society, 2004, pp. 193–204. [Online].
Available: http://dx.doi.org/10.1109/IDEAS.
2004.14

[18] S. Bird, E. Klein, and E. Loper, Natural language

processing with Python. O’Reilly, 2009.

[19] K. Seymore, A. Mccallum, and R. Rosenfeld,
“Learning hidden markov model structure for
information extraction,” in In AAAI 99 Workshop
on Machine Learning for Information Extraction,
1999, pp. 37–42.

[20] M. Craven and J. Kumlien, “Constructing
biological knowledge bases by extracting in-
formation from text sources,” in Proceedings
of the Seventh International Conference on In-
telligent Systems for Molecular Biology. AAAI
Press, 1999, pp. 77–86. [Online]. Available: http:
//dl.acm.org/citation.cfm?id=645634.663209

[21] K. Bellare and A. McCallum, “Learning ex-
tractors from unlabeled text using relevant
databases,” in Sixth International Workshop on In-
formation Integration on the Web, 2007.

Page 10 of 11
c(cid:13)ASE 2012

[30] M. Collins and B. Roark, “Incremental parsing
with the perceptron algorithm,” in Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics. Association for Compu-
tational Linguistics, 2004, p. 111.

[31] Y. Zhang and S. Clark, “Chinese segmen-
tation with a word-based perceptron algo-
rithm,” in ANNUAL MEETING-ASSOCIATION
FOR COMPUTATIONAL LINGUISTICS, vol. 45,
no. 1, 2007, p. 840.

[32] Y. Goldberg and M. Elhadad, “Learning sparser
[Online].
http://www.cs.bgu.ac.il/~yoavg/

perceptron models,” Tech. Rep.
Available:
publications/

[22] M. Collins, “Discriminative training methods
for hidden markov models:
theory and ex-
periments with perceptron algorithms,” in
Proceedings of the ACL-02 Conference on Em-
pirical Methods in Natural Language Processing
- Volume 10, ser. EMNLP ’02.
Stroudsburg,
PA, USA: Association for Computational Lin-
guistics, 2002, pp. 1–8. [Online]. Available:
http://dx.doi.org/10.3115/1118693.1118694

[23] A. McCallum, D. Freitag, and F. C. N. Pereira,
“Maximum entropy markov models for in-
formation extraction and segmentation,” in
Proceedings of the Seventeenth International Con-
ference on Machine Learning, ser. ICML ’00.
San Francisco, CA, USA: Morgan Kaufmann
Publishers Inc., 2000, pp. 591–598. [Online].
Available: http://dl.acm.org/citation.cfm?id=
645529.658277

[24] A. Ratnaparkhi, “A maximum entropy model
for part-of-speech tagging,” in Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, April 16 1996. [Online].
Available: http://citeseer.ist.psu.edu/581830.
html

[25] J. D. Laﬀerty, A. McCallum, and F. C. N.
Pereira, “Conditional random ﬁelds: Prob-
abilistic models for segmenting and label-
ing sequence data,” in Proceedings of
the
Eighteenth International Conference on Machine
Learning,
San Francisco,
CA, USA: Morgan Kaufmann Publishers Inc.,
2001, pp. 282–289. [Online]. Available: http:
//dl.acm.org/citation.cfm?id=645530.655813

ICML ’01.

ser.

[26] C. Elkan, “Log-linear models and conditional
random ﬁelds,” Tutorial notes at CIKM, vol. 8,
2008.

[27] G. Andrew and J. Gao, “Scalable training of L1-
regularized log-linear models,” in Proceedings
of the 24th International Conference on Machine
Learning, ser. ICML ’07. New York, NY, USA:
ACM, 2007, pp. 33–40.
[Online]. Available:
http://doi.acm.org/10.1145/1273496.1273501

[28] F. Rosenblatt, “The perceptron: a probabilistic
model for information storage and organization
in the brain.” Psychological review, vol. 65, no. 6,
p. 386, 1958.

[29] Y. Freund and R. E. Schapire, “Large margin
classiﬁcation using the perceptron algorithm,”
Machine learning, vol. 37, no. 3, pp. 277–296,
1999.

Page 11 of 11
c(cid:13)ASE 2012

