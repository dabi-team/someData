Privacy-Preserving Reinforcement Learning Beyond Expectation

Arezoo Rajabi1, Bhaskar Ramasubramanian2, Abdullah Al Maruf1, Radha Poovendran1

2
2
0
2

r
a

M
8
1

]

G
L
.
s
c
[

1
v
5
6
1
0
1
.
3
0
2
2
:
v
i
X
r
a

Abstract— Cyber and cyber-physical systems equipped with
machine learning algorithms such as autonomous cars share
environments with humans. In such a setting, it is important
to align system (or agent) behaviors with the preferences of
one or more human users. We consider the case when an
agent has to learn behaviors in an unknown environment. Our
goal is to capture two deﬁning characteristics of humans: i) a
tendency to assess and quantify risk, and ii) a desire to keep
decision making hidden from external parties. We incorporate
cumulative prospect theory (CPT) into the objective of a rein-
forcement learning (RL) problem for the former. For the latter,
we use differential privacy. We design an algorithm to enable an
RL agent to learn policies to maximize a CPT-based objective
in a privacy-preserving manner and establish guarantees on
the privacy of value functions learned by the algorithm when
rewards are sufﬁciently close. This is accomplished through
adding a calibrated noise using a Gaussian process mechanism
at each step. Through empirical evaluations, we highlight a
privacy-utility tradeoff and demonstrate that the RL agent is
able to learn behaviors that are aligned with that of a human
user in the same environment in a privacy-preserving manner.

I. INTRODUCTION

Complex cyber and cyber-physical systems (CPS),
in-
cluding autonomous cars and drones, rely on the seamless
integration of computation and physical components. A CPS
might depend on machine learning algorithms for decision
making due to the large amounts of data generated during its
operation and limited access to models of its environment.
Reinforcement learning (RL) [1] and optimal control [2] are
two paradigms that have been commonly leveraged to choose
actions to maximize an expected reward over the horizon
of system operation when dynamic system behaviors are
represented as Markov decision processes (MDP) [3].

A risk-neutral approach to decision making involves
learning strategies (sequence of actions) to maximize an
expected reward, where the reward signal is provided by
the environment. Learning strategies to satisfy risk-neutral
objectives have been successfully implemented in multiple
domains, including robotics, autonomous vehicles, games,
and mobile networks [4]–[10]. Although these methods to
learn strategies are tractable and efﬁcient, rational and risk-
neutral decision making using the expected utility is often
not sufﬁcient to model decision making in real-world CPS.
Decision making in realistic settings is often risk-sensitive.
It is becoming increasingly common for learning-based CPS
to share an environment with human users [11]–[13]. In such

1Network Security Lab, Department of Electrical and Computer Engi-

neering, University of Washington, Seattle, WA 98195, USA.
{rajabia, maruf3e, rp3}@uw.edu

2Electrical and Computer Engineering, Western Washington University,

Bellingham, WA 98225, USA. {ramasub}@wwu.edu

a situation, it will become important for the system to be
aware of risk-sensitive and irrational behaviors of users. Due
to various cognitive and emotional biases, human users can
exhibit both risk-seeking and risk-averse behaviors. In these
cases, expected utility-based frameworks are not adequate to
describe human decision making, since humans might have a
different perception of both, the utility and the probabilistic
outcome as a consequence of their decisions [14].

To effectively capture preferences of humans for certain
outcomes over certain others, we use insights from empirical
models of human behavior from the social sciences. These
models have shown that humans derive utility relative to a
reference point [15]. There is also a tendency to be more
sensitive to losses than gains, and rather than using actual
probabilities to assess outcomes, very small and very large
probabilities are distorted [16]. As an illustration, human
drivers on the road i) are more sensitive to changes in speed,
than its absolute value; ii) are more averse to being passed
(loss) than passing another car (gain); iii) overestimate small
probabilities of engine failures and underestimate large prob-
abilities of running out of gas. Cumulative prospect theory
(CPT), introduced in [17], is a framework that incorporates
the above properties. CPT uses a non-linear utility function
to transform outcomes and a non-linear weighting function to
distort probabilities in the cumulative distribution function.
Utility and weighting functions corresponding to gains and
losses can be different to model the possibility that these are
often interpreted in different ways by a human.

Our recent work in [18] developed RL algorithms for
CPT-based decision making when a model of the system
was not known. We established theoretical guarantees on
convergence and demonstrated that behaviors of agents using
CPT-based policies closely mimicked those of a human user
in the same environment. In this paper, we focus on another
deﬁning characteristic of human behavior: a desire to keep
decision-making and its outcomes hidden from external, and
possibly adversarial parties. We use the foundations and
tools of differential privacy [19], a security property that
makes it difﬁcult for an adversary to discern information
about a system by providing probabilistic guarantees on the
indistinguishability of its observations. This makes it unlikely
that an adversary will learn anything of speciﬁc interest or
meaningful about sensitive data [20].

To assimilate this characteristic within an RL framework,
it is important to identify the sensitive information that might
be gleaned through knowledge of individual components
of the decision-making procedure. Rewards received by an
agent are an important descriptor of the task that needs to be
completed; transition probabilities from an MDP represen-

 
 
 
 
 
 
tation reveal information about the consequence of taking a
particular action; a trajectory of states visited by the agent
can disclose tracking information [21]. Previous research
has established that the rewards are the most vulnerable
component [22], [23]. Consequently, we propose a method to
incorporate differential privacy into risk-sensitive reinforce-
ment learning, to ensure that rewards which are ‘sufﬁciently
close’ to each other will be indistinguishable.

To the best of our knowledge,

the incorporation of a
privacy-preserving mechanism into prospect-theoretic rein-
learning has not been previously investigated.
forcement
Such a framework will ensure that an autonomous agent can
learn behaviors consistent with two deﬁning characteristics
of humans: i) a propensity to assess and quantify risk, and
ii) a desire to keep decision making hidden from external
parties. We make the following contributions:
• We design an algorithm, PP-CPT-RL,

to enable an
RL agent to learn policies to maximize a CPT-based
objective in a privacy-preserving manner. Our algorithm
adds noise to the CPT-value of a state-action pair at each
step through a Gaussian process mechanism.

• We establish guarantees on the privacy of value func-
tions learned by PP-CPT-RL when rewards are sufﬁ-
ciently close to each other using differential privacy.
• We evaluate the PP-CPT-RL algorithm in a continuous-
state environment where an agent has to reach a target
while avoiding obstacles. Our experiments highlight a
privacy-utility tradeoff, and we demonstrate that PP-
CPT-RL allows agents to learn optimal policies while
maintaining indistinguishability of rewards they receive.
Moreover, agent behaviors are aligned with those of a
human who might be placed in the same environment.

The remainder of this paper is organized as follows:
Section II provides an overview of related literature on risk-
sensitive RL and differential privacy. Section III establishes
notation and describes necessary background material. We
summarize our CPT-based RL framework from [18] in Sec-
tion IV. Section V details the design of the PP-CPT-RL al-
gorithm, and establishes guarantees on its privacy-preserving
nature. We report results of our empirical evaluations in
Section VI, and Section VII presents our conclusions.

II. RELATED WORK

This section summarizes related work in risk-sensitive and

differentially private reinforcement learning.

Incorporating risk into decision-making has been typically
carried out by replacing the average utility with the average
over a function of the utility [24], [25]. Examples include
a mean-variance tradeoff [26]–[28], exponential utility [29]–
[31], and conditional value at risk (CVaR) [32]. CVaR is the
average cost, given that the cost takes sufﬁciently large val-
ues, and has strong theoretical justiﬁcation for its use [33]–
[35]. An axiomatic characterization of risk considerations for
a robot was presented in [36]. Risk-sensitivity has also been
represented as a constraint to be satisﬁed while maximizing
an average utility [37], [38]. Cumulative prospect theory

(CPT) [17] models behaviors of a decision-maker that is risk-
averse with gains and risk-seeking with losses, and distorts
extremely high and low probability events. Optimization
of a CPT-based cost for MDPs was studied in [39], [40].
Our previous work [18] optimized a CPT-based cost in an
RL framework, and designed CPT-RL algorithms; a similar
paradigm was concurrently proposed in [41]. While the
above works provide promising solutions for risk-sensitive
decision makers, incorporating CPT-based objectives into an
RL framework has been relatively less studied. Further, the
consideration of a desire of an RL agent to maintain privacy
of its decision making has not been explored using CPT.

Differential privacy has been used to reason about indis-
tinguishability of trajectories of a dynamical system in [42],
[43]. An overview of results that use differential privacy in
control is presented in the survey [44]. A characterization
of differential privacy for discrete-state Markov chains was
presented in [45], and this was extended to trajectories
of discrete MDPs in our previous work [46]. Algorithmic
guarantees on the differential privacy of policies in RL using
Monte-Carlo techniques were provided in [47]. An algorithm
to synthesize privacy-preserving exploration policies that
simultaneously achieved strong regret bounds for episodic
RL in discrete environments was proposed in [48]. A lower
bound for regret minimization in ﬁnite horizon MDPs with
strong privacy-preserving guarantees was presented in [49].
The authors of [50] developed a policy synthesis algorithm
that protected the privacy of transition probabilities of MDPs.
Guarantees on the privacy of value functions of a deep Q-
learning algorithm in environments with continuous states
using differential privacy were established in [21]. Although
the above works integrate differential privacy into an MDP or
an RL framework, these examine the case where an expected
reward needs to be maximized. In comparison, we establish
privacy guarantees on value functions of a CPT-based deep
RL algorithm in environments with continuous states.

III. PRELIMINARIES

This section introduces background on reinforcement
learning, cumulative prospect theory, and differential privacy.

A. MDPs and RL

Let (Ω, F, P) denote a probability space, where Ω is a
sample space, F is a σ−algebra of subsets of Ω, and P is
a probability measure on F. A random variable (r.v.) is a
map Y : Ω → R. We assume that the environment of the RL
agent is described by a Markov decision process (MDP) [3].
Deﬁnition 1. An MDP is a tuple M := (S, A, ρ0, P, r, γ),
where S is a ﬁnite set of states, A is a ﬁnite set of actions,
and ρ0 is a probability distribution over the initial states.
P(s(cid:48)|s, a) is the probability of transiting to state s(cid:48) when
action a is taken in state s. r : S × A → R is the reward
obtained by the agent when it takes action a in state s. γ ∈
(0, 1] is a discounting factor.

An RL agent typically does not have knowledge of the
transition function P. Instead, it obtains a (ﬁnite) reward r

for each action that it takes. Through repeated interactions
with the environment, the agent seeks to learn a policy π
in order to maximize an objective Eπ[(cid:80)
t γtr(st, at)] [1]. A
policy is a probability distribution over the set of actions at a
given state, and is denoted π(·|s). In realistic scenarios, the
expected reward might not be an adequate representation of
decision-making. This will necessitate the incorporation of
risk-sensitivity into the RL framework.

B. Risk Measures and Cumulative Prospect Theory

For a set Y of random variables on Ω, a risk measure or

risk metric is a map ρ : Y → R [36].

Deﬁnition 2. A risk metric is coherent if it satisﬁes the
following properties for all Y, Y1, Y2 ∈ Y, d ∈ R, m ∈ R≥0:
1) Monotonicity: Y1(ω) ≤ Y2(ω) for all ω ∈ Ω ⇒

ρ(Y1) ≤ ρ(Y2);

2) Translation invariance: ρ(Y + d) = ρ(Y ) + d;
3) Positive homogeneity: ρ(mY ) = m · ρ(Y );
4) Subadditivity: ρ(Y1 + Y2) ≤ ρ(Y1) + ρ(Y2).

The last two properties together ensure that a coherent risk
metric will also be convex. The risk metric that we adopt
in this paper is informed from cumulative prospect theory
[17], and is not coherent. Human players or operators have
been known to demonstrate a preference to play safe with
gains and take risks with losses. Further, they tend to deﬂate
high probability events, and inﬂate low probability events.
Cumulative prospect theory (CPT) is a risk measure that has
been empirically shown to capture human attitude to risk
[17], [39]. This risk metric uses two utility functions u+ and
u−, corresponding to gains and losses, and weight functions
w+ and w− that reﬂect the fact that value seen by a human
subject is nonlinear in the underlying probabilities [16].

Deﬁnition 3. The CPT-value of a continuous r.v. Y is:

ρcpt(Y ) :=

(cid:90) ∞

0

w+(P(u+(Y ) > z))dz

(cid:90) ∞

−

0

w−(P(u−(Y ) > z))dz,

(1)

where utility functions u+, u− : R → R≥0 are continuous,
have bounded ﬁrst moment such that u+(x) = 0 for all
x ≤ 0, and monotonically non-decreasing otherwise, and
u−(x) = 0 for all x ≥ 0, and monotonically non-increasing
otherwise. The probability weighting functions w+, w− :
[0, 1] → [0, 1] are Lipschitz continuous and non-decreasing,
and satisfy w+(0) = w−(0) = 0 and w+(1) = w−(1) = 1.

When Y is a discrete r.v. with ﬁnite support, let pi denote
the probability of incurring a gain or loss yi, where y1 ≤
· · · ≤ yl ≤ 0 ≤ yl+1 ≤ . . . yK, for i = 1, 2, . . . , K. Deﬁne
i=1 pi for k ≤ l and Fk := (cid:80)K
Fk := (cid:80)k
Deﬁnition 4. The CPT-value of a discrete r.v. Y is:

i=k pi for k > l.

ρcpt(Y )

(cid:18) K−1
(cid:88)

:=

i=l+1

u+(yi)(cid:0)w+(Fi) − w+(Fi+1)(cid:1) + u+(yK )w+(pK )

(2)
(cid:19)

(cid:18)

−

u−(y1)w−(p1) +

l
(cid:88)

i=2

u−(yi)(cid:0)w−(Fi) − w−(Fi−1)(cid:1)

(cid:19)

The function u+ is typically concave on gains, while −u−
is typically convex on losses [17]. The distortion of extremely
low and extremely high probability events by humans can
be represented by a weight function that takes an inverted S-
shape- i.e., it is concave for small probabilities, and convex
for large probabilities (e.g., w(κ) = exp(−(− ln κ)η), 0 <
η < 1) [17], [51]. The CPT-value generalizes other risk
metrics for appropriate choices of weighting functions. For
example, when w+, w− are identity functions, and u+(x) =
x, x ≥ 0, u−(x) = −x, x ≤ 0, we obtain ρcpt(Y ) = E[Y ].

C. Gaussian Processes

A Gaussian vector-valued random variable Y is denoted
N (µ, Σ), where µ is the mean vector, and Σ is a symmetric,
positive deﬁnite covariance matrix. Consider a partition of
the Gaussian random vector into two sets such that Y =
(cid:2)Y1 Y2
with corresponding partitions of their means and
covariances as µ = (cid:2)µ1 µ2
the following properties hold:

(cid:20)Σ11 Σ12
ΣT
12 Σ22

and Σ =

, then

(cid:3)T

(cid:3)T

(cid:21)

22 (Y2 −µ2), Σ11 −Σ12Σ22ΣT
11 (Y1 −µ1), Σ22 −ΣT

1) Y1 ∼ N (µ1, Σ11) and Y2 ∼ N (µ2, Σ22)
2) Y1|Y2 ∼ N (µ1 +Σ12Σ−1
12)
12Σ−1
3) Y2|Y1 ∼ N (µ2 +ΣT
12Σ11Σ12)
Gaussian processes (GPs) generalize the concept of a
Gaussian distribution over discrete random variables to the
idea of a Gaussian distribution over continuous functions
[52]. GPs are particularly useful in uncertainty quantiﬁcation
due to their ability to simultaneously track the evolution of
the mean and the covariance of a distribution.

Deﬁnition 5. A Gaussian process (GP) is a collection of
r.v., any ﬁnite number of which have a joint Gaussian
distribution. A GP f (x) is speciﬁed by its mean m(x) and
covariance function K(x, x(cid:48)), and we write f ∼ G(m, K).

let fn0

For f ∼ G(g, σ2K),

:= {f (x0), f (x2),
. . . , f (x2n)} and fn1
:= {f (x1), f (x3), . . . , f (x2n−1)}
where xi = i/2n, i = 0, . . . , 2n. Let βn := β/2n. Then,
from Deﬁnition 5, fn1|fn0 ∼ N (gn1 + K10K −1
00 (fn0 −
gn0), σ2(K11 − K10K −1
00 K T

10)).

D. Differential Privacy

Differential privacy is a property that ensures that private
data of an agent is protected, while allowing for statistical
inferences from aggregates of the data [19]. This makes it
unlikely that an adversary will learn anything meaningful
about sensitive data. Attractive features of differential privacy
include compositionality, resilience to post-processing, and
robustness to side information. The notion of differential pri-
vacy is mathematically deﬁned using a notion of neighboring
data points to characterize data points that are sufﬁciently
close to each other according to some metric/ norm. Let
d, d(cid:48) ∈ D be neighboring inputs.
Deﬁnition 6. A randomized mechanism M : D → U satisﬁes
((cid:15), δ)−differential privacy if for any two neighboring inputs

d, d(cid:48) and for any subset of outputs Z ⊆ U, P(M(d) ∈ Z) ≤
exp((cid:15))P(M(d(cid:48)) ∈ Z) + δ.

The sensitivity of a mechanism M is deﬁned as ∆M :=

sup{d, d(cid:48) ∈ D : ||M(d) − M(d(cid:48))||} for some norm on U.

An example of M is the Gaussian mechanism. When U =
Rn, N (0, σ2I) is added to the output M(d), and the norm on
U is the (cid:96)2−norm. When U is a reproducing kernel Hilbert
space (RKHS), the norm is the RKHS norm [53], and a
Gaussian process noise G(0, σ2K) is added to M(d).

IV. CPT-BASED REINFORCEMENT LEARNING

This section introduces a reinforcement learning paradigm
that maximizes a CPT-based reward. We direct the reader to
[18] for proofs of the results. Note that while [18] minimized
the sum of CPT-based costs, in this paper, we maximize the
sum of CPT-based rewards.

In order to assess the quality of taking an action a at a
state s, we introduce the notion of the CPT-value of state-
action pair at time t and following policy π subsequently.
We denote this by Qπ
cpt(s, a) and will refer to it as CPT-Q.
CPT-Q is deﬁned in the following manner:

Qπ

cpt(st, at) := ρcpt(r(st, at)
P(st+1|st, at)
+ γ

(cid:88)

(cid:88)

π(at+1|st+1)Qπ

cpt(st+1, at+1)).

(3)

st+1

at+1

Qπ

cpt(s, a) will be bounded when |r(s, a)| < ∞ and
γ ∈ (0, 1). In reinforcement learning, transition probabilities
and rewards are typically not known apriori. In the absence
of a model, the agent will have to estimate Qπ
cpt(s, a) and
learn ‘good’ policies by exploring its environment. Since
Qπ
cpt(s, a) is evaluated for each action in a state, this quantity
can be estimated without knowledge of the transition prob-
abilities. This is in contrast to [40], where a model of the
system was assumed to be available, and costs were known.
The CPT-value of a state s when following policy π is
π(at|st)Qπ
cpt(st, at). We will
cpt(s) as CPT-V. We observe that CPT-V satisﬁes:

deﬁned as V π
refer to V π

cpt(st) := (cid:80)

at

cpt(st) = ρcpt(r(st, aπ
V π
t )
(cid:88)

+ γ

st+1

P(st+1|st, aπ

t )V π

cpt(st+1)).

(4)

cpt(s).

cpt(s). Then,

i=0 γir(si, aπ

Denote the maximum CPT-V at a state s by V ∗
cpt(s) = supπ V π
V ∗
Remark 1. To motivate the construction of this framework,
let the random variable R(s0) = (cid:80)∞
i ) denote
the inﬁnite horizon cumulative discounted reward starting
from state s0. The objective in a typical RL problem is
to determine a policy π to maximize the expected reward,
denoted Eπ[R(s0)]. The linearity of the expectation operator
allows us to write Eπ[R(s0)] = E[r(s0, a0) + γE[r(s1, a1) +
. . . |s1]|s0]. In this work, we are interested in maximizing the
sum of CPT-based rewards over the horizon of interest. This
will correspond to replacing the conditional expectation at
each time-step with ρcpt(·).

We deﬁned a CPT-Q-iteration operator in [18]:

(TπQπ

cpt)(s, a) := ρcpt(r(s, a)
+ γ

P(s(cid:48)|s, a)

(cid:88)

s(cid:48)

(5)
cpt(s(cid:48), a(cid:48))),

π(a(cid:48)|s(cid:48))Qπ

(cid:88)

a(cid:48)

which was instrumental in establishing the convergence of
CPT-Q-learning in Equation (3). We state a result from [18].

cpt(s) ≥ V π

Deﬁnition 7. A policy π(cid:48) is said to be improved compared
to policy π if and only if for all s ∈ S, V π(cid:48)
cpt(s).
[18] Let the functions w+, w−, u+, u− be
Proposition 1.
according to Deﬁnition 3. Assume that u+, u− are invertible
and differentiable with monotonically non-increasing deriva-
tives. Consider policies π and π(cid:48) such that Qπ(cid:48)
cpt(s, a) ≥
cpt(s, a) for all (s, a) ∈ S × A, and π(cid:48) is improved com-
Qπ
pared to π. Then, (TπQπ
cpt) ≥
(TπQπ
cpt|| ≤
γ||Q1

cpt)) and a contraction (i.e., (cid:107)|TπQ1
cpt − Q2

cpt) is monotone (i.e., (Tπ(cid:48)Qπ(cid:48)
cpt − TπQ2

cpt||).

The focus of this paper is to develop techniques to ensure
that observation of Qπ
cpt(s, a) for any (s, a) does not pro-
vide any meaningful or distinguishing information between
rewards r(s, a) and r(cid:48)(s, a) as long as ||r − r(cid:48)||∞ ≤ 1.

V. PRIVACY-PRESERVING CPT-BASED RL

This section presents our main results that demonstrate
that incorporating a privacy-preserving mechanism into a
prospect-theoretic framework ensures that an agent can learn
behaviors to enable it to assess and quantify risk, and ensure
that its decision making is hidden from external parties.
the design of a privacy-preserving CPT-
We ﬁrst detail
based reinforcement learning (PP-CPT-RL) algorithm that
will allow the agent to learn policies in a manner such that
differential privacy of value functions is preserved. We then
analyze the guarantees on privacy provided by PP-CPT-RL.

A. Algorithm

We present an algorithm based on temporal difference
(TD) techniques for PP-CPT-RL. TD techniques seek to learn
value functions using episodes of experience. An experience
episode comprises a sequence of states, actions, and rewards
when following a policy π. The predicted values at any time-
step is updated in a way to bring it closer to the prediction of
the same quantity at the next time-step. In order to support
environments with large, possibly continuous state spaces,
our approach is based on deep Q-learning [5], where Q-
values are parameterized by a deep neural network1.

From Equations (1) and (3), we observe that ρcpt is deﬁned
in terms of a weighting function applied to a cumulative
probability distribution. We ﬁrst use a technique proposed in
[39] to estimate the CPT-value ρcpt to use TD-methods.

1This parameterization is analogous to the Q-table that is typically seen in
Q-learning [1]. Updating entries of the Q-table is then equivalent to updating
the values of the parameter θ using a gradient-based method.

Algorithm 1 CPT-PP-Estimation
Require: State s, action a, current policy π, max. samples Nmax
1: Initialize n = 1; X0 := 0; s∗ ← s
2: repeat
3:
4:
5: Xn

Take action a, observe r(s, a) and next state s(cid:48)
Determine P rivP resb(s(cid:48)) for each b using Algorithm 3

r(s, a) + γ (cid:80)

cpt(s(cid:48), b) +

b π(b|s(cid:48))(Qθ

:=

P rivP resb(s(cid:48)))
if Xn > X0 then

s∗ ← s(cid:48)
X0 ← Xn

6:
7:
8:
9:
10:
11: until n > Nmax
12: Arrange samples {Xi} in ascending order: X[1] ≤ X[2] ≤ . . .

end if
n ← n + 1

13: Let:

ρ+
cpt : =

Nmax(cid:88)

i=1

u+(X[i])(w+(

Nmax + i − 1
Nmax

)

− w+(

Nmax − i
Nmax

))

ρ−
cpt : =

Nmax(cid:88)

i=1

u−(X[i])(w−(

i
Nmax

) − w−(

i − 1
Nmax

))

14: ρcpt(r(s, a) + γ (cid:80)

cpt − ρ−
ρ+
15: return ρcpt(·), s∗

cpt

b π(b|·)(Qθ

cpt(·, b) + P rivP resb(·))) :=

Algorithm 2 PP-CPT-RL
Require: Learning rate α; horizon Tmax; discount γ; target privacy

1: Initialize Parameters θ; value functions Qθ

cpt(s, a), T = 1,

level ((cid:15), δ); batch size B

linked list P rivP res = []

cpt = 0

Initialize s ∈ S, ¯b = 1, BatchLossθ
repeat

2: Determine noise level σ based on ((cid:15), δ) using Theorem 1
3: repeat
4:
5:
6:
7:
8:
9:
10:

Add s to linked list P rivP res
Determine P rivP resa(s) for each a using Algorithm 3
Choose a from arg maxa[Qθ
cpt(s, a) + P rivP resa(s)]
Obtain ρcpt(·), s∗ from Algorithm 1; Qθ
T arg := ρcpt(·)
minθ Lossθ
:=
P rivP resa(s))]2
BatchLossθ
s ← s∗
¯b ← ¯b + 1
until ¯b > B
Update parameters θ ← θ − α 1
T ← T + 1

11:
12:
13:
14:
15:
16:
17: until T > Tmax
18: return Qθ

B ∇θBatchLossθ

cpt := BatchLossθ

T arg − (Qθ

cpt + Lossθ

0.5[Qθ

cpt(s, a)

cpt

cpt

cpt

cpt(s, a) +

1) Calculating ρcpt

from samples: Algorithm 1 is a
procedure to obtain multiple samples of the random vari-
able r(s, a) + γVcpt(s(cid:48)). These samples are then used
to estimate ρcpt(r(s, a) + γVcpt(s(cid:48))) (since Vcpt(s) =
(cid:80)
a π(·|s)Qcpt(s)). This way to estimate the CPT-value of
a random variable was proposed in [39], and was shown
to be asymptotically consistent. In order to obtain these
estimates in a privacy-preserving manner, we use Algorithm

Algorithm 3 PPLevel-Action
Require: State s, noise level σ, list P rivP res
1: for each action a do
µa = K10K −1
2:
00
da = K11 − K10K −1
3:
P rivP resa(s) ∼ N (µa, σda)
4:
5: end for
6: return P rivP res(s) := [P rivP resa(s)]m

00 K T

10

a=1

3 to determine the quantum of noise that needs to be added
for each action at any state.

2) Privacy-preserving CPT-Q-Learning: Algorithm 2 is
a technique to learn policies for an RL-agent with a CPT-
based objective in a privacy-preserving manner. The target
privacy level ((cid:15), δ) determines the noise level that will be
needed to ensure differential privacy (Line 2). The linked-
list P rivP res maintains a record of states visited along
sample trajectories in the replay buffer, since parameters
of the Gaussian noise added to the value functions at each
step will depend on the ‘chain’ of states in this list (Line
7). The training process consists of working with batches
of sample trajectories that have been collected in a replay
buffer. A quadratic loss function that measures the TD-error
with respect to a target Q−network (whose parameters are
kept ﬁxed) is minimized in Line 10. The parameters of the
target network are updated after examining all the samples
from a batch using a gradient-based method (Line 15) [5].
Lines 10 and 15 together comprise the TD-update.

3) Determining noise level for each action: Algorithm
3 is used to determine the quantum of noise that needs
to be added for each action at state s. The noise is added
using a Gaussian process mechanism. Following the notation
established in Sec. III-C, and I denoting the identity matrix,
we deﬁne µa and da (Lines 2-3) as:

K10K −1

00 =

exp(−βn)
1 + exp(−2βn)

K11 − K10K −1

00 K T

10 =

1
1
...
0
0


1
0
0
1


...
...



0
0

0
0
1 − exp(−2βn)
1 + exp(−2βn)

. . .
. . .
. . .
. . .
. . .










0 0
0 0
...
...
1 0
1 1

I

(6)

(7)

B. Privacy Analysis

Our objective is to ensure that observing or querying the
value function in Algorithm 2 will not reveal useful infor-
mation about rewards received by an agent. This is formally
stated in Theorem 1, which establishes a guarantee on the
privacy of Algorithm 2. We make an assumption to ensure
that the neural networks used to learn Qθ
cpt(s, a) deﬁne a
complete vector space of functions equipped with a norm that
is a combination of Lp−norms of the functions along with
some of its derivatives. Then, we state intermediate results
which establish i) conditions for ((cid:15), δ)−differential privacy
of a mechanism M [19], [21], [53], ii) that sample paths of a
Gaussian process generated on a reproducing kernel Hilbert

space (RKHS) are bounded with high probability [21], and
iii) an expression for the RKHS norm [21], [54].

Assumption 1. The neural network used to approximate
Qπ
cpt(s, a) in Algorithm 2 has a ﬁnite number of parameters,
a ﬁnite number of layers, and the gradients of its activation
functions are bounded.

Proposition 2. Let ∆M be the sensitivity of a mechanism M.
The following statements hold:

1) If (cid:15) ∈ (0, 1) and σ ≥ (cid:112)2 ln(1.25/δ)∆M/(cid:15),

then
M(d) + y is ((cid:15), δ)−differentially private, where y ∼
N (0, σ2I).

2) If (cid:15) ∈ (0, 1) and σ ≥ (cid:112)2 ln(1.25/δ)∆M/(cid:15),

then
M(d) + g is ((cid:15), δ)−differentially private, where g ∼
G(0, σ2K), and U is an RKHS with kernel function
K.

Fig. 1: The 10 × 10 square region on which the PP-CPT-
RL algorithm is evaluated. The agent needs to learn a policy
to reach target T from start S. There are obstacles in the
environment (red squares), and the agent incurs a different
cost when encountering each obstacle.

Lemma 1. Consider a sample path f generated by a GP
G(0, σ2K) on an RKHS with kernel K(x, y) := exp(−β||x−
y||1). Then, f ∗ := max f (x) exists almost surely, and for any
u > 0, P(f ∗ ≥ 8.68

βσ + u) ≤ exp(−u2/2).

√

Lemma 2. The RKHS associated with kernel K(x, y) :=
exp(−β||x − y||1) consists of all continuous functions φ
with ﬁnite L2−norm whose derivatives φ(cid:48) also have ﬁ-
nite L2−norm. Further, the RKHS norm ||φ||2
H ≤ (1 +
β/2)(φ(x))2 + L2/2β, where L is the Lipschitz constant.

We are now ready to establish our main result that estab-
lishes a privacy guarantee on value functions learned using
Algorithm 2 when rewards are sufﬁciently close to each other
using differential privacy.

Theorem 1. Let the functions u+, u− in Deﬁnition 3 be in-
vertible and differentiable with monotonically non-increasing
√
derivatives. Let 2k > 8.68
βσ, L be the Lipschitz con-
stant of the value function approximation yielded by the
neural network parameterized by θ, B be the batch size,
and α be the learning rate of Algorithm 2. Assume that
(cid:104) α
σ ≥ (cid:112)2 ln(1.25/δ)(L2(1/β2 + 1/β))/(cid:15) and β =
B (4k +

(cid:105)−1

2cmax + 1)

, where cmax ≥ (r + γQcpt(s∗) − Qcpt(s)).
Then, each iteration of Algorithm 2 is ((cid:15), δ + exp(−(2k −
βσ)2/2)− differentially private with respect to neigh-

8.68
boring reward functions r, r(cid:48) such that ||r − r(cid:48)||∞ ≤ 1.

√

Proof. Let Qcpt and Q(cid:48)
cpt be the state-action value functions
learned using Algorithm 2 corresponding to rewards r and
r(cid:48)
respectively. Consider the update-step in Line 15 of
Algorithm 2. If Qold
cpt denotes the (ﬁxed) value function before
the update and L is the Lipschitz constant of the value
function approximation, we have:

||Qcpt − Qold

cpt|| ≤

||Q(cid:48)

cpt − Qold

cpt|| ≤

αL
B

αL
B

||r + γQold

cpt(s∗) − Qold

cpt(s)

+ P rivP resa(s∗) − P rivP resa(s)||,

||r(cid:48) + γQold

cpt(s∗) − Qold

cpt(s)

+ P rivP resa(s∗) − P rivP resa(s)||.

√

Set u = 2k − 8.68
1. Then
||P rivP resa(s∗) − P rivP resa(s)|| ≤ 2k with probability
βσ)2/2). When ||r − r(cid:48)||∞ ≤ 1, and
1 − exp(−(2k − 8.68
cpt(s∗) − Qold
(r + γQold

cpt(s)) ≤ cmax, we have:

βσ in Lemma

√

||Qcpt − Q(cid:48)

cpt|| + ||Q(cid:48)

cpt − Qold
cpt||

cpt|| ≤ ||Qcpt − Qold
αL
B

≤

(4k + 2cmax + 1)

with probability 1 − exp(−(2k − 8.68

√

βσ)2/2).

Deﬁning φ := Qcpt − Q(cid:48)

cpt, from Lemma 2, we have:

||φ||2

H ≤ (1 +

β
2

(cid:104) αL
)
B

(4k + 2cmax + 1)

(cid:105)2

+

L2
2β

Choose β =
L2(1/β2 + 1/β).

(cid:104) α
B (4k + 2cmax + 1)

(cid:105)−1

. Then ||φ||2

H ≤

Using Proposition 2, if σ ≥ (cid:112)2 ln(1.25/δ)||φ||H/(cid:15), then
adding P rivP res(s) ∼ G(0, σ2K) to the value function
Qcpt ensures that each iteration of Algorithm 2 is ((cid:15), δ +
βσ)2/2)−differentially private whenever
exp(−(2k − 8.68
||r − r(cid:48)||∞ ≤ 1.

√

VI. EXPERIMENTAL EVALUATION

This section presents an evaluation of the PP-CPT-RL
algorithm. We compare behaviors learned when the agent
adds different amounts of a calibrated noise to its value
function in order to maintain privacy of its decision making
with a baseline when this noise is not added.

We assume that the agent starts from the state ‘S’ at the
bottom left corner, and the target state ‘T’ is at the top
right corner of a 10 × 10 square region shown in Fig. 1. At
each state, the agent can take one of four possible actions,
{lef t, right, up, down}. If an action is allowed at a state
then the transition to the intended next state happens with
probability 0.9 and with a probability of 0.1 to another state.
Suppose the current state of the agent is the position (x, y),
and the agent takes the action right. Then intended next
state is determined as (x(cid:48), y(cid:48)) such that Int(x(cid:48)) = Int(x)+1

CPT

No CPT

method
Q DP-5
DP-1
x
a
M
No DP
Q DP-5
DP-1
d
n
a
No DP
R

obs1
3.74
1.31
1.125
7.58
1.595
0.54

obs2
0.74
2.63
4.26
0.115
1.825
3.49

obs3
0.02
0.85
0.055
0.365
0.575
1.695

obs4
0
0.2
0.81
0.165
0.815
3.605

obs1
7.995
4.14
4.57
8.595
3.615
2.415

obs2
0.63
1.49
2.59
0.705
1.56
4.43

obs3
0.06
0.165
0.055
0.28
0.525
1.47

obs4
0.08
2.375
0.05
0.215
1.045
3.385

TABLE I: Average number of visits to an obstacle for different privacy levels. We compare cases when the agent chooses
an action according to the highest Q-value (M ax.Q) and when an action is chosen at random (RandQ). The agent visits an
obstacle fewer times when maximizing a CPT-based reward (CP T ) than an expected reward (N o CP T ). When penalties
for {obs1, obs2, obs3, obs4} are {50, 25, 10, 5}, that CPT-based objective is more effective in avoiding visits to obstacles
with higher penalties. The number of visits to an obstacle is also higher for a higher privacy level.

higher magnitude of loss. This demonstrates that there is an
inherent trade-off between privacy and utility.

In order to reason about agent behavior when learning
policies using Algorithm 2, we assume that each obstacle
has a different penalty (to represent the relative severity of
encountering the obstacle). Table I compares the number of
visits to the obstacle regions. We compare cases when the
agent chooses an action according to the highest Q-value
(M ax.Q) and when an action is chosen proportional to its
probability (RandQ). We make the following observations:

• the agent visits an obstacle fewer times when maximiz-
ing a CPT-based objective than when maximizing an
expected utility (N o CP T );

• the CPT-based objective is more effective in avoiding

visits to obstacles with highest penalties;

• the number of visits to an obstacle is higher for a higher
privacy level- this is consistent with the higher loss
values seen in Fig. 2.

These results are consistent with intuition, and will be
aligned with that of a human user placed in the same
environment. Developing a mathematical characterization of
these properties is a promising direction of future research.

VII. CONCLUSION

This paper presented a way to enable a reinforcement
learning (RL) agent to learn behaviors that are consistent
with human traits of assessing risk and a desire to keep
decision making private. We used cumulative prospect theory
(CPT) to quantify risk-sensitive behavior, and differential
privacy to characterize privacy-preserving decision making.
We designed an algorithm, PP-CPT-RL, to enable the agent
to learn policies to maximize a CPT-based objective. Through
adding a calibrated noise to CPT-based value functions
we established guarantees on privacy when rewards were
sufﬁciently close to each other. Experimental evaluation of
PP-CPT-RL showed that agents can learn optimal policies in
a privacy-preserving manner. Our experiments also revealed
a privacy-utility tradeoff, and showed that agent behavior is
consistent with a human placed in the same environment.

Future work will seek to develop mathematically rigorous
characterizations of the privacy-utility trade-off, establish
privacy guarantees for other classes of RL algorithms (e.g.,

Fig. 2: Loss values (Lines 10-11 of Algorithm 2), averaged
over 20 runs. Shaded regions indicate variance of the loss.
When σ = 1 (red curve, denoted DP − 1), the average
loss and its variance is comparable to the setting without
differential privacy (blue curve, denoted N oDP ). Choosing
σ = 5 (green curve, denoted DP − 5) provides improved
guarantees on privacy according to Theorem 1, but results in
a higher loss, thus demonstrating a privacy-utility tradeoff.

and Int(y(cid:48)) = Int(y), where Int(·) denotes the integer part
of the argument. The intended next state for other actions
is similarly determined. If an action is not available in a
state (e.g., down at the Start) the agent remains in that state.
The agent will have to avoid obstacles in order to reach the
target. The discount factor γ is set to 0.9, and the utility and
weighting functions are chosen as:

u+(x) = |x|0.88; ω+(κ) =

u−(x) = |x|0.88; ω−(κ) =

κ0.61
(κ0.61 + (1 − κ)0.61) 1
κ0.69
(κ0.69 + (1 − κ)0.69) 1

0.61

0.69

;

.

Figure 2 compares the performance of Algorithm 2 for
different privacy levels. We observe that choosing σ = 1
in Line 2 results in a performance that
is as good as
without differential privacy (i.e., no noise added to the value
function), since the loss values and is variance is almost
identical in both cases. A higher value of σ = 5, while
providing improved guarantees on privacy, also results in a

actor-critic), and examine extensions to the multi-agent
case.

REFERENCES

[1] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduc-

tion. MIT Press, 2018.

[2] D. P. Bertsekas, Dynamic Programming and Optimal Control, Vol. 1,

4th Ed. Athena Scientiﬁc, 2017.

[3] M. L. Puterman, Markov decision processes: Discrete stochastic

dynamic programming.

John Wiley & Sons, 2014.

[4] R. Hafner and M. Riedmiller, “Reinforcement learning in feedback

control,” Machine Learning, vol. 84, pp. 137–169, 2011.

[5] V. Mnih et al., “Human-level control through deep reinforcement

learning,” Nature, vol. 518, no. 7540, 2015.

[6] D. Silver et al., “Mastering the game of Go with deep neural networks

and tree search,” Nature, vol. 529, no. 7587, 2016.

[7] C. Zhang, P. Patras, and H. Haddadi, “Deep learning in mobile and
wireless networking: A survey,” IEEE Communications Surveys &
Tutorials, vol. 21, no. 3, pp. 2224–2287, 2019.

[8] D. Sadigh, S. Sastry, S. A. Seshia, and A. D. Dragan, “Planning for
autonomous cars that leverage effects on human actions.” in Robotics:
Science and Systems, 2016.

[9] Z. Yan and Y. Xu, “Data-driven load frequency control for stochastic
power systems: A deep reinforcement learning method with continuous
action search,” IEEE Transactions on Power Systems, vol. 34, no. 2,
2018.

[10] C. You, J. Lu, D. Filev, and P. Tsiotras, “Advanced planning for
autonomous vehicles using reinforcement learning and deep inverse
RL,” Robotics and Autonomous Systems, vol. 114, pp. 1–18, 2019.

[11] S. A. Seshia, D. Sadigh, and S. S. Sastry, “Formal methods for
semi-autonomous driving,” in ACM/EDAC/IEEE Design Automation
Conference.

IEEE, 2015, pp. 1–5.

[12] N. Nikolakis, V. Maratos, and S. Makris, “A cyber physical system
approach for safe human-robot collaboration in a shared workplace,”
Robotics and Computer-Integrated Manufacturing, vol. 56, pp. 233–
243, 2019.

[13] B. Xiao, Q. Lu, B. Ramasubramanian, A. Clark, L. Bushnell,
and R. Poovendran, “FRESH: Interactive reward shaping in high-
dimensional state spaces using human feedback,” in International
Conference on Autonomous Agents and MultiAgent Systems, 2020, pp.
1512–1520.

[14] D. Kahneman and A. Tversky, “Prospect

theory: An analysis of
decision under risk,” Econometrica, vol. 47, no. 2, pp. 263–292, 1979.
[15] U. Schmidt, “Reference dependence in cumulative prospect theory,”
Journal of Mathematical Psychology, vol. 47, no. 2, pp. 122–131,
2003.

[16] N. C. Barberis, “Thirty years of prospect theory in economics: A
review and assessment,” Journal of Economic Perspectives, vol. 27,
no. 1, pp. 173–96, 2013.

[17] A. Tversky and D. Kahneman, “Advances in prospect theory: Cumu-
lative representation of uncertainty,” Journal of Risk and uncertainty,
vol. 5, no. 4, pp. 297–323, 1992.

[18] B. Ramasubramanian, L. Niu, A. Clark, and R. Poovendran, “Rein-
forcement learning beyond expectation,” in Conference on Decision
and Control (CDC).

IEEE, 2021.

[19] C. Dwork, A. Roth et al., “The algorithmic foundations of differential
privacy,” Foundations and Trends® in Theoretical Computer Science,
vol. 9, no. 3–4, pp. 211–407, 2014.

[20] G. Liao, X. Chen, and J. Huang, “Prospect

theoretic analysis of
privacy-preserving mechanism,” IEEE/ACM Transactions on Network-
ing, vol. 28, no. 1, pp. 71–83, 2019.

[21] B. Wang and N. Hegde, “Privacy-preserving Q-learning with func-
tional noise in continuous spaces,” Advances in Neural Information
Processing Systems, vol. 32, 2019.

[22] A. Y. Ng and S. J. Russell, “Algorithms for inverse reinforcement
learning,” in International Coference on Machine Learning, 2000, pp.
663–670.

[23] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse rein-
forcement learning,” in International Coference on Machine Learning,
2004.

[24] Y. Shen, W. Stannat, and K. Obermayer, “Risk-sensitive Markov con-
trol processes,” SIAM Journal on Control and Optimization, vol. 51,
no. 5, pp. 3652–3672, 2013.

[25] Y. Shen, M. J. Tobia, T. Sommer, and K. Obermayer, “Risk-sensitive
reinforcement learning,” Neural computation, vol. 26, no. 7, pp. 1298–
1328, 2014.

[26] H. Markowitz, “Portfolio selection,” The Journal of Finance, vol. 7,

no. 1, pp. 77–91, 1952.

[27] A. Tamar, D. Di Castro, and S. Mannor, “Policy gradients with
variance related risk criteria,” in International Coference on Machine
Learning, 2012, pp. 1651–1658.

[28] S. Mannor and J. N. Tsitsiklis, “Algorithmic aspects of mean–variance
optimization in Markov decision processes,” European Journal of
Operational Research, vol. 231, no. 3, pp. 645–653, 2013.

[29] R. A. Howard and J. E. Matheson, “Risk-sensitive Markov decision
processes,” Management Science, vol. 18, no. 7, pp. 356–369, 1972.

[30] P. Whittle, Risk-sensitive optimal control. Wiley, 1990.
[31] V. S. Borkar, “Q-learning for risk-sensitive control,” Mathematics of

Operations Research, vol. 27, no. 2, pp. 294–311, 2002.

[32] R. T. Rockafellar and S. Uryasev, “Conditional value-at-risk for
general loss distributions,” Journal of banking & ﬁnance, vol. 26, no. 7,
pp. 1443–1471, 2002.

[33] M. Ahmadi, U. Rosolia, M. D. Ingham, R. M. Murray, and A. D.
Ames, “Constrained risk-averse Markov decision processes,” in AAAI
Conference on Artiﬁcial Intelligence, 2021.

[34] M. P. Chapman, R. Bonalli, K. M. Smith, I. Yang, M. Pavone, and
C. J. Tomlin, “Risk-sensitive safety analysis using conditional value-
at-risk,” IEEE Transactions on Automatic Control, 2021.

[35] L. Lindemann, G. J. Pappas, and D. V. Dimarogonas, “Control barrier
functions for nonholonomic systems under risk signal temporal logic
speciﬁcations,” in IEEE Conference on Decision and Control (CDC).
IEEE, 2020, pp. 1422–1428.

[36] A. Majumdar and M. Pavone, “How should a robot assess risk?
Towards an axiomatic theory of risk in robotics,” in Robotics Research.
Springer, 2020, pp. 75–84.

[37] Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone, “Risk-
constrained reinforcement learning with percentile risk criteria,” The
Journal of Machine Learning Research, vol. 18, pp. 6070–6120, 2017.
[38] L. A. Prashanth and M. Fu, “Risk-sensitive reinforcement learning: A
constrained optimization viewpoint,” arXiv:1810.09126, 2018.
[39] C. Jie, L. A. Prashanth, M. Fu, S. Marcus, and C. Szepesv´ari,
“Stochastic optimization in a cumulative prospect theory framework,”
IEEE Transactions on Automatic Control, vol. 63, no. 9, 2018.
[40] K. Lin, C. Jie, and S. I. Marcus, “Probabilistically distorted risk-
sensitive inﬁnite-horizon dynamic programming,” Automatica, vol. 97,
pp. 1–6, 2018.

[41] V. S. Borkar and S. Chandak, “Prospect-theoretic Q-learning,” Systems

& Control Letters, vol. 156, no. 10, p. 105009, 2021.

[42] J. Cort´es, G. E. Dullerud, S. Han, J. Le Ny, S. Mitra, and G. J.
Pappas, “Differential privacy in control and network systems,” in IEEE
Conference on Decision and Control, 2016, pp. 4252–4272.

[43] K. Yazdani, A. Jones, K. Leahy, and M. Hale, “Differentially private
LQ control,” IEEE Transactions on Automatic Control, 2022.
[44] S. Han and G. J. Pappas, “Privacy in control and dynamical systems,”
Annual Review of Control, Robotics, and Autonomous Systems, vol. 1,
pp. 309–332, 2018.

[45] D. Chistikov, A. S. Murawski, and D. Purser, “Asymmetric distances
for approximate differential privacy,” in International Conference on
Concurrency Theory, 2019.

[46] B. Ramasubramanian, L. Niu, A. Clark, L. Bushnell, and R. Pooven-
dran, “Privacy-preserving resilience of cyber-physical systems to ad-
versaries,” in IEEE Conference on Decision and Control, 2020, pp.
3785–3792.

[47] B. Balle, M. Gomrokchi, and D. Precup, “Differentially private policy
evaluation,” in International Conference on Machine Learning, 2016.
[48] G. Vietri, B. Balle, A. Krishnamurthy, and S. Wu, “Private rein-
forcement learning with PAC and regret guarantees,” in International
Conference on Machine Learning, 2020, pp. 9754–9764.

[49] E. Garcelon, V. Perchet, C. Pike-Burke, and M. Pirotta, “Local
differential privacy for regret minimization in reinforcement learning,”
Advances in Neural Information Processing Systems, vol. 34, 2021.

[50] P. Gohari, M. Hale, and U. Topcu, “Privacy-preserving policy synthesis
in markov decision processes,” in IEEE Conference on Decision and
Control, 2020, pp. 6266–6271.

[51] D. Prelec, “The probability weighting function,” Econometrica, pp.

497–527, 1998.

[52] C. K. Williams and C. E. Rasmussen, Gaussian processes for machine

learning. MIT Press, 2006.

[53] R. Hall, A. Rinaldo, and L. Wasserman, “Differential privacy for
functions and functional data,” The Journal of Machine Learning
Research, vol. 14, no. 1, pp. 703–727, 2013.

[54] S. Lalley, “Introduction to Gaussian processes,” https:// galton.
uchicago.edu/ ∼lalley/ Courses/ 386/ GaussianProcesses.pdf , 2013.

